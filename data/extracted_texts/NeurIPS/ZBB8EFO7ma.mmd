# Aiming towards the minimizers: fast convergence of SGD for overparameterized problems

Chaoyue Liu\({}^{*}\)

Dmitriy Drusvyatskiy\({}^{**}\)

Yian Ma\({}^{*}\)

Damek Davis\({}^{***}\)

Mikhail Belkin\({}^{*}\)

\({}^{*}\)Halicioglu Data Science Institute, University of California San Diego

\({}^{**}\)Mathematics Department, University of Washington

\({}^{***}\)School of Operations Research and Information Engineering, Cornell University

###### Abstract

Modern machine learning paradigms, such as deep learning, occur in or close to the interpolation regime, wherein the number of model parameters is much larger than the number of data samples. In this work, we propose a regularity condition within the interpolation regime which endows the stochastic gradient method with the same worst-case iteration complexity as the deterministic gradient method, while using only a single sampled gradient (or a minibatch) in each iteration. In contrast, all existing guarantees require the stochastic gradient method to take small steps, thereby resulting in a much slower linear rate of convergence. Finally, we demonstrate that our condition holds when training sufficiently wide feedforward neural networks with a linear output layer.

## 1 Introduction

Recent advances in machine learning and artificial intelligence have relied on fitting highly overparameterized models, notably deep neural networks, to observed data; e.g. . In such settings, the number of parameters of the model is much greater than the number of data samples, thereby resulting in models that achieve near-zero training error. Although classical learning paradigms caution against overfitting, recent work suggests ubiquity of the "double descent" phenomenon , wherein significant overparameterization actually improves generalization. The stochastic gradient method is the workhorse algorithm for fitting overparametrized models to observed data and understanding its performance is an active area of research. The goal of this paper is to obtain improved convergence guarantees for SGD in the interpolation regime that better align with its performance in practice.

Classical optimization literature emphasizes conditions akin to strong convexity as the phenomena underlying rapid convergence of numerical methods. In contrast, interpolation problems are almost never convex, even locally, around their solutions . Furthermore, common optimization problems have complex symmetries, resulting in nonconvex sets of minimizers. Case in point, standard formulations for low-rank matrix recovery  are invariant under orthogonal transformations while ReLU neural networks are invariant under rebalancing of adjacent weight matrices . The Polyak-Lojasiewicz (PL) inequality, introduced independently in  and , serves as an alternative to strong convexity that holds often in applications and underlies rapid convergence of numerical algorithms. Namely, it has been known since  that gradient descent convergences under the PL condition at a linear rate \(O((-t/))\), where \(\) is the condition number of the function.1 In contrast, convergence guarantees for the stochastic gradient method under PL--the predominant algorithm in practice--are much less satisfactory. Indeed, all known results require SGD to takeshorter steps than gradient descent to converge at all , with the disparity between the two depending on the condition number \(\). The use of the small stepsize directly translates into a slow rate of convergence. This requirement is in direct contrast to practice, where large step-sizes are routinely used. As a concrete illustration of the disparity between theory and practice, Figure 1 depicts the convergence behavior of SGD for training a neural network on the MNIST data set. As is evident from the Figure, even for small batch sizes, the linear rate of convergence of SGD is comparable to that of GD with an identical stepsize \(=0.1\). Moreover, experimentally, we have verified that the interval of stepsizes leading to convergence for SGD is comparable to that of GD; indeed, the two are off only by a factor of \(20\). Using large stepsizes also has important consequences for generalization. Namely, recent works  suggest that large stepsizes bias the iterates towards solutions that generalize better to unseen data. Therefore understanding the dynamics of SGD with large stepsizes is an important research direction. The contribution of our work is as follows.

In this work, we highlight regularity conditions that endow SGD with a fast linear rate of convergence \((-t/)\) both in expectation and with high probability, even when the conditions hold only locally. Moreover, we argue that the conditions we develop are reasonable because they provably hold on any compact region when training sufficiently wide feedforward neural networks with a linear output layer.

### Outline of main results.

We will focus on the problem of minimizing a loss function \(\) under the following two assumptions. First, we assume that \(\) grows quadratically away from its set of global minimizers \(S\):

\[(w)^{2}(w,S)  w B_{r}(w_{0}),\] (QG)

where \(B_{r}(w_{0})\) is a ball of radius \(r\) around the initial point \(w_{0}\). This condition is standard in the optimization literature and is implied for example by the PL-inequality holding on the ball \(B_{r}(w_{0})\); see Section A.2. Secondly, and most importantly, we assume there there exist constants \(,>0\) satisfying the aiming condition:2

\[(w),w-_{S}(w) (w) w B_{r}(w_{0}).\] (Aiming)

Here, \(_{S}(w)\) denotes a nearest point in \(S\) to \(w\) and \((w,S)\) denotes the distance from \(w\) to \(S\). The aiming condition ensures that the negative gradient \(-(w)\) points towards \(S\) in the sense that \(-(w)\) correlated nontrivially with the direction \(_{S}(w)-w\). At first sight, the aiming condition appears similar to quasar-convexity, introduced in  and further studied in . Namely a function \(\) is _quasar-convex_ relative to a fixed point \( S\) if the estimate (Aiming) holds with \(_{S}(w)\) replaced by \(\). Although the distinction between aiming and quasar-convexity may

Figure 1: Convergence plot of SGD when training a fully connected neural network with 3 hidden layers and 1000 neurons in each on MNIST (left) and a ResNet-28 on CIFAR-10 (right). MNIST has 60k images and the convergence behavior stabilizes with small batchsize of m = 64; CIFAR-10 has 60k images and the convergence behavior stabilizes with batchsize of \(m=512\), although early on in training (first \(20k\) iterations) convergence behavior looks identical for all batchsizes.

appear mild, it is significant. As a concrete example, consider the function \((x,y)=(y-ax^{2})^{2}\) for any \(a>0\). It is straightforward to see that \(\) satisfies the aiming condition on some neighborhood of the origin. However, for any neighborhood \(U\) of the origin, the function \(\) is not quasar-convex on \(U\) relative to any point \((x,ax^{2}) U\); see Section C. More generally, we show that (Aiming) holds automatically for any \(C^{3}\) smooth function \(\) satisfying (QG) locally around the solution set. Indeed, we may shrink the neighborhood to ensure that \(\) is arbitrarily close to \(2\). Secondly, we show that (Aiming) holds for sufficiently wide feedforward neural networks with a linear output layer.

Our first main result can be summarized as follows. Roughly speaking, as long as the SGD iterates remain in \(B_{r}(w_{0})\), they converge to \(S\) at a fast linear rate \(O((-t^{2}/))\) with high probability.

**Theorem 1.1** (Informal).: _Consider minimizing the function \((w)=(w,z)\), where the losses \((,z)\) are nonnegative and have \(\)-Lipschitz gradients. Suppose that the minimal value of \(\) is zero and both regularity conditions (QG) and (Aiming) hold. Then as long as the SGD iterates \(w_{t}\) remain in \(B_{r}(w_{0})\), they converge to \(S\) at a linear rate \(O((-t^{2}/))\) with high probability._

The proof of the theorem is short and elementary. The downside is that the conclusion of the theorem is conditional on the iterates remaining in \(B_{r}(w_{0})\). Ideally, one would like to estimate this probability as a function of the problem parameters. With this in mind, we show that for a special class of nonlinear least squares problems, including those arising when fitting wide neural networks, this probability may be estimated explicitly. The end result is the following unconditional theorem.

**Theorem 1.2** (Informal).: _Consider the loss \((w)=_{i=1}^{n}(f(w,x_{i})-y_{i})^{2}\), where \(f(w,)\) is a fully connected neural network with \(l\) hidden layers and a linear output layer. Let \(_{}\) be the minimal eigenvalue of the Neural Tangent Kernel of an infinitely wide neural network. Then with high probability both conditions (QG) and (Aiming) hold on a ball of radius \(r\) around the initial point \(w_{0} N(0,I)\) with \(=1\) and \(=_{}/2\), as long as the network width \(m\) satisfies \(m=(nr^{6l+2}/_{}^{2})\). If in addition \(r=(1/})\), with probability \(1-\), SGD with stepsize \(=(1)\) converges to a zero-loss solution at the fast rate \(O((-t_{}))\). This parameter regime is identical as for gradient descent to converge in , with the only exception of the inflation of \(r\) by \(1/\)._

A key part of the argument is to estimate the probability that the iterates remain in a ball \(B_{r}(w_{0})\). A naive approach is to bound the length of the iterate trajectory in expectation, but this would then require the radius \(r\) to expand by an additional factor of \(1/_{}\), which in turn would increase \(m\) multiplicatively by \(_{}^{-6l-2}\). We avoid this exponential blowup by a careful stopping time argument and the transition to linearity phenomenon that has been shown to hold for sufficiently wide neural networks . While Theorem 1.2 is stated with a constant failure probability, there are standard ways to remove the dependence. One option is to simply set \(_{1}=(1)\) and rerun SGD logarithmically many times from the the same initialization \(w_{0}\) and return the final iterate with smallest function value. Section 4 outlines a more nuanced strategy based on a small ball assumption, which entirely avoids computation of the function values of the full objective.

### Comparison to existing work.

We next discuss how our results fit within the existing literature, summarized in Table 1. Setting the stage, consider the problem of minimizing a smooth function \((w)=(w,z)\) and suppose for simplicity that its minimal value is zero. We say that \(\) satisfies the _Polyak-Lojasiewicz (PL) inequality_ if there exists \(>0\) satisfying

\[\|(w)\|^{2} 2(w),\] (PL)

for all \(w^{d}\). In words, the gradient \((w)\) dominates the function value \((w)\), up to a power. Geometrically, such functions have the distinctive property that the gradients of the rescaled function \((w)}\) are uniformly bounded away from zero outside the solution set. See Figures 1(a) and 1(b) for an illustration. Using the PL inequality, we may associate to \(\) two condition numbers, corresponding to the full objective and its samples, respectively. Namely, we define \(/\) and \(/\), where \(\) is a Lipschitz constant of the full gradient \(\) and \(\) is a Lipschitz constant of the sampled gradients \((,z)\) for all \(z\). Clearly, the inequality \(\) holds and we will primarily be interested in settings where the two are comparable.

The primary reason why the PL condition is useful for optimization is that it ensures linear convergence of gradient-type algorithms. Namely, it has been known since Polyak's seminal work  that the full-batch gradient descent iterates \(w_{t+1}=w_{t}-(w_{t})\) converge at the linear rate \(O((-t/))\). More recent papers have extended results of this type to a wide variety of algorithms both for smooth and nonsmooth optimization [27; 7; 21; 30; 1] and to settings when the PL inequality holds only locally on a ball [24; 32].

For stochastic optimization problems under the PL condition, the story is more subtle, since the rates achieved depend on moment bounds on the gradient estimator, such as:

\[[\|(w,z)\|^{2}] A(w)+B\|( w)\|^{2}+C,\] (1.1)

for \(A,B,C 0\). In the setting where \(C>0\)--the classical regime- stochastic gradient methods converge sublinearly at best, due to well-known lower complexity bounds in stochastic optimization . On the other hand, in the setting where \(C=0\)--interpolation problems--stochastic gradient methods converge linearly when equipped with an appropriate stepsize, as shown in [2; Theorem 1], [22, Corollary 2], , and [13, Theorem 4.6]. Although linear convergence is assured, the rate of linear converge under the PL condition and interpolation is an order of magnitude worse than in the deterministic setting. Namely, the three papers[2, Theorem 1], [22, Corollary 2] and [13, Theorem 4.6] obtain linear rates on the order of \((-t/)\). On the other hand, in the case \(A=C=0\), which is called the strong growth property, the paper [38, Theorem 4] yields the seemingly better rate \((-t/B)\). The issue, however, is that \(B\) can be extremely large. As an illustrative example, consider the loss functions \((w,z)=^{2}(w,Q_{z})\) where \(Q_{z}\) are smooth manifolds. A quick computation shows that equality \(\|(w,z)\|^{2}=2(w,z)\) holds. Therefore, locally around the intersection of \(_{z}Q_{z}\), the estimate (1.1) with \(A=C=0\) is _exactly equivalent_ to the PL-condition with \(B=1/\). As a further illustration, Figure 3 shows the possible large value of the constant \(B\) along the SGD iterates for training a neural network on MNIST. Another related paper is : assuming so-called small gradient confusion and requiring a stronger version of PL condition (i.e., each individual loss \(l_{i}\) is \(\)-PL), the paper  showed a slow rate of convergence \((-t/n^{2})\), where \(n\) is the dataset size.

Figure 3: We train a fully-connected neural network on the MNIST dataset. The network has \(4\) hidden layers, each with \(1024\) neurons. We optimize the MSE loss using SGD with a batch size \(512\) and a learning rate \(0.5\). The training was run over \(1\)k epochs, and the ratio \([\|(w,z)\|^{2}]/\|(w)\|^{2}\) is evaluated every \(100\) epochs. The ratio grows almost linearly during training, suggesting that strong growth is practically not satisfied with a constant coefficient \(B\).

The purpose of this work is to understand whether we can improve stepsize selection and the convergence rate of SGD for nonconvex problems under the (local) PL condition and interpolation. Unfortunately, the PL condition alone appears too weak to yield improved rates. Instead, we take inspiration from recent work on accelerated deterministic nonconvex optimization, where the recently introduced quasar-convexity condition has led to improved rates . We note that quasar-convexity is a very restrictive assumption in the interpolation regime because it requires the solution set to be an affine subspace; see the discussion in Appendix E. Recent work has also shown that quasar convexity can lead to accelerated _sublinear_ rates of convergence for certain stochastic optimization problems [20, Theorem 4.4] (and the concurrent work [11, Corollary 3.3]), but to the best of our knowledge, there are no works that analyze improved linear rates of convergence. Thus, in this work, we fill the gap in the literature, by providing a rate that matches that of deterministic gradient descent and allows for a large stepsize. Moreover, in contrast to most available results, we only assume that regularity conditions hold on a ball--the common setting in applications. The local nature of the assumptions requires us to bound the probability of the iterates escaping.

## 2 Main results

Throughout the paper, we will consider the stochastic optimization problem

\[_{w}\ (w)}_{z} (w,z),\]

where \(\) is a probability distribution that is accessible only through sampling and \((,z)\) is a differentiable function on \(^{d}\). We let \(S\) denote the set of minimizers of \(\). We impose that \(\) satisfies the following assumptions on a set \(\). The two main examples are when \(\) is a ball \(B_{r}(w_{0})\) and when \(\) is a tube around the solution set:

\[S_{r}\{w^{d}:(w,S) r\}.\]

**Assumption 1** (Running assumptions).: Suppose that there exist constants \(,, 0\) and a set \(^{d}\) satisfying the following.

1. **(Interpolation)** The losses \((w,z)\) are nonnegative, the minimal value of \(\) is zero, and the set of minimizers \(S\) is nonempty.
2. **(Smoothness)** For almost every \(z\), the loss \((,z)\) is differentiable and the gradient \((,z)\) is \(\)-Lipschitz continuous on \(\).
3. **(Quadratic growth)** The estimate holds: \[(w)^{2}(w,S) w .\] (2.1)
4. **(Aiming)** For all \(w\) there exists a point \((w,S)\) such that \[(w),w-(w).\] (2.2)

  Reference & Bound on \([\|(w,z)\|^{2}]\) & 
 Quasar \\ Convex? \\  & Rate \\  [2, Theorem 1] & \(2((w)-^{*})\) & No & \((-})\) \\  [38, Theorem 4] & \(B\|(w)\|^{2}\) & No & \((-})\) \\  [22, Corollary 2] & \(A(w)+B\|(w)\|^{2}\) & No & \((-})\) \\  [13, Theorem 4.6] & \(2((w)-^{*})+\|(w)\|^{2}\) & No & \((})\) \\  [11, Corollary 3.3] & \(^{2}+2\|w-w_{*}\|^{2}\) & Yes & Sublinear \\  [20, Theorem 4.4] & \(^{2}+\|(w)\|^{2}\) & Yes & Sublinear \\ 
**This work** & \(2((w)-^{*})\) & (Aiming) & \((}{})\) \\  

Table 1: Comparison to recent work on nonconvex stochastic gradient methods under the \(\)-PL and smoothness conditions. We define \(=/\) and \(=/\), where \(\) is a Lipschitz constant of the full gradient \(\) and \(\) is a Lipschitz constant of the sampled gradients \((,z)\) for all \(z\).

We define the condition number \(/\).

As explained in the introduction, the first three conditions (1)-(3) are classical in the literature. In particular, both quadratic growth (3) on a ball \(=B_{r}(w_{0})\) and existence of solutions in \(\) follow from a local PL-inequality. In order to emphasize the local nature of the condition, following  we say that \(\) is \(\)_-PL\({}^{*}\) on \(\)_ if the inequality (PL) holds for all \(w\). We recall the proof of the following lemma in Section A.2.

**Lemma 2.1** (PL\({}^{*}\) condition implies quadratic growth).: _Suppose that \(\) is differentiable and is \(\)-PL\({}^{*}\) on a ball \(B_{2r}(w_{0})\). Then as long as \((w_{0})< r^{2}\), the intersection \(S B_{r}(w_{0})\) is nonempty and_

\[(w)^{2}(w,S) w B_{ r}(w_{0}).\]

The aiming condition (4) is very closely related to _quasar-convexity_, which requires (2.2) to hold for all \(w^{d}\) and a _distinguished point_\( S\) that is independent of \(w\). This distinction may seem mild, but is in fact important because aiming holds for a much wider class of problems. As a concrete example, consider the function \((x,y)=(y-ax^{2})^{2}\) for any \(a>0\). It is straightforward to see that \(\) satisfies the aiming condition on some neighborhood of the origin. However, for any neighborhood \(U\) of the origin, the function \(\) is not quasar-convex on \(U\) relative to any point \((x,ax^{2}) U\); see Section C. We now show that (4) is valid locally for any \(C^{3}\)-smooth function satisfying quadratic growth, and we may take \(\) arbitrarily close to \(2\) by shrinking \(r\). Later, we will also show that problems of learning wide neural networks also satisfy the aiming condition.

**Theorem 2.2** (Local aiming).: _Suppose that \(\) is \(C^{2}\)-smooth and \(^{2}\) is \(L\)-Lipschitz continuous on the tube \(S_{r}\). Suppose moreover that \(\) satisfies the quadratic growth condition (2.1) and \(r<\). Then the aiming condition (2.2) holds with parameter \(=2-\). An analogous statement holds if \(S_{r}\) is replaced by a ball \(B_{r}(_{0})\) for some \(_{0} S\)._

``` Initialize: Initial \(w_{0}^{d}\), learning rate \(>0\), iteration counter \(T\). For\(t=1,,T-1\)do: \[\ z_{t} \] \[\ w_{t+1} =w_{t}-(w_{t},z_{t}).\] Return:\(w_{T}\). ```

**Algorithm 1**SGD\((w_{0},,T)\)

### SGD under regularity on a tube \(S_{r}\)

Convergence analysis for SGD (Algorithm 1) is short and elementary in the case \(=S_{r}\) and therefore this is where we begin. We note, however, that the setting \(=B_{r}(w_{0})\) is much more realistic, as we will see, but also more challenging.

The converge analysis of SGD proceeds by a familiar one-step contraction argument.

**Lemma 2.3** (One-step contraction on a tube).: _Suppose that Assumption 1 holds on a tube \(=S_{2r}\) and fix a point \(w S_{r}\). Define the updated point \(w^{+}=w- f(w,z)\) where \(z\). Then for any stepsize \(<\), the estimate holds:_

\[*{}_{z}\ ^{2}(w^{+},S)(1- (-))\,^{2}(w,S).\] (2.3)

Using the one step guarantee of Lemma 2.3, we can show that SGD iterates converge linearly to \(S\) if Assumption 1 holds on a tube \(=S_{2r}\). The only complication is to argue that the iterates are unlikely to leave the tube if we start in a slightly smaller tube \(S_{r^{}}\) for some \(r^{}<r\). We do so with a simple stopping time argument.

**Theorem 2.4** (Convergence on a tube).: _Suppose that Assumption 1 holds relative to a tube \(=S_{2r}\) for some constant \(r>0\). Fix a stepsize \(>0\) satisfying \(<\). Fix a constant \(_{1}>0\) and a point \(w_{0} S_{r}}\). Then with probability at least \(1-_{1}\), the SGD iterates \(\{w_{t}\}_{t 0}\) remain in \(\). Moreover, with probability at least \(1-_{1}-_{2}\), the estimate \(^{2}(w_{t},S)^{2}(w_{0},S)\) holds after_

\[t( }).\]

Thus as long SGD is initialized at a point \(w_{0} S_{r}}\), with probability at least \(1-_{1}-_{2}\), the iterates remain in \(S_{r}\) and converge at linear rate \(O(}(-t^{2}/))\). Note that the dependence on \(_{2}\) is logarithmic, while the dependence on \(_{1}\) appears linearly in the initialization requirement \(w_{0} S_{r}}\). One simple way to remove the dependence on \(_{1}\) is to simply rerun the algorithm from the same initial point logarithmically many times and return the point with the smallest function value. An alternative strategy that bypasses evaluating function values will be discussed in Section 4.

### SGD under regularity on a ball \(B_{r}(w_{0})\)

Next, we describe convergence guarantees for SGD when Assumption 1 holds on a ball \(=B_{r}(w_{0})\). The key complication is the following. While \(w_{t}\) are in the ball, the distance \(^{2}(w_{t},S)\) shrinks in expectation. However, the iterates may in principle quickly escape the ball \(B_{r}(w_{0})\), after which point we lose control on their progress. Thus we must lower bound the probability that the iterates \(w_{t}\) remain in the ball. To this end, we will require the following additional assumption.

**Assumption 2** (Uniform aiming).: The estimate

\[(w),w-v(w)- (w,S)\] (2.4)

holds for all \(w B_{r}(w_{0})\) and \(v B_{r}(w_{0}) S\).

The intuition underlying this assumption is as follows. We would like to replace \(\) in the aiming condition (2.2) by an arbitrary point \(v B_{r}(w_{0}) S\), thereby having a condition of the form \((w),w-v(w)\). The difficulty is that this condition may not be true for the main problem we are interested in-- training wide neural networks. Instead, it suffices to lower bound the inner product by \((w)-(w,S)\) where \(\) is a small constant. This weak condition provably holds for wide neural networks, as we will see in the next section. The following is our main result.

**Theorem 2.5** (Convergence on a ball).: _Suppose that Assumptions 1 and 2 hold on a ball \(=B_{3r}(w_{0})\). Fix constants \(_{1}(0,)\) and \(_{2}(0,1)\), and assume \(^{2}(w_{0},S)_{1}^{2}r^{2}\). Fix a stepsize \(<\) and suppose \((-) r\). Then with probability at least \(1-5_{1}\), all the SGD iterates \(\{w_{t}\}_{t 0}\) remain in \(B_{r}(w_{0})\). Moreover, with probability at least \(1-5_{1}-_{2}\), the estimate \(^{2}(w_{t},S)^{2}(w_{0},S)\) holds after_

\[t(}).\]

Thus as long as \(\) is sufficiently small and the initial distance satisfies \(^{2}(w_{0},S)_{1}^{2}r^{2}\), with probability at least \(1-5_{1}-_{2}\), the iterates remain in \(B_{r}(w_{0})\) and converge at a fast linear rate \(O(}(-t^{2}/))\). While the dependence on \(_{2}\) is logarithmic, the constant \(_{1}\) linearly impacts the initialization region. Section 4 discusses a way to remove this dependence. As explained in Lemma 2.1, both quadratic growth and the initialization quality holds if \(\) is \(^{*}\) on the ball \(B_{r}(w_{0})\), and \(r\) is sufficiently big relative to \(1/\).

## 3 Consequences for nonlinear least squares and wide neural networks

We next discuss the consequences of the results in the previous sections to nonlinear least squares and training of wide neural networks. To this end, we begin by verifying the aiming (2.2) and uniform aiming (2.4) conditions for nonlinear least squares. The key assumption we will make is that the nonlinear map's Jacobian \( F\) has a small Lipschitz constant in operator norm.

**Theorem 3.1**.: _Consider a function \((w)=\|F(w)\|^{2}\), where \(F^{d}^{n}\) is \(C^{1}\)-smooth. Suppose that there is a point \(w_{0}\) satisfying \((w_{0},S) r\) and such that on the ball \(B_{2r}(w_{0})\), the gradient \(\) is \(\)-Lipschitz, the Jacobian \( F\) is \(L\)-Lipschitz in the operator norm, and the quadratic growth condition (2.1) holds. Then as long as \(L}\), the aiming (2.2) and uniform aiming (2.4) conditions hold on \(B_{r}(w_{0})\) with \(=2-}{}\) and \(=8r^{2}L\)._We next instantiate Theorem 3.1 and Theorem 2.5 for a nonlinear least squares problem arising from fitting a wide neural network. Setting the stage, an \(l\)-layer (feedforward) neural network \(f(w;x)\), with parameters \(w\), input \(x\), and linear output layer is defined as follows:

\[^{(0)}=x,\] \[^{(i)}=(}}W^{(i)}^{ (i-1)}),\ \  i=1,,l-1\] \[f(w;x)=}}W^{(l)}^{(l-1)}.\]

Here, \(m_{i}\) is the width (i.e., number of neurons) of \(i\)-th layer, \(^{(i)}^{m_{i}}\) denotes the vector of \(i\)-th hidden layer neurons, \(w:=\{W^{(1)},W^{(2)},,W^{(l)},W^{(l+1)}\}\) denotes the collection of the parameters (or weights) \(W^{(i)}^{m_{i} m_{i-1}}\) of each layer, and \(\) is the activation function, e.g., \(sigmoid\), \(tanh\), linear activation. We also denote the width of the neural network as \(m:=_{i[l]}m_{i}\), i.e., the minimal width of the hidden layers. The neural network is usually randomly initialized, i.e., each individual parameter is initialized i.i.d. following \((0,1)\). Henceforth, we assume that the activation functions \(\) are twice differentiable, \(L_{}\)-Lipschitz, and \(_{}\)-smooth. In what follows, the order notation \(()\) and \(O()\) will suppress multiplicative factors of polynomials (up to degree \(l\)) of the constants \(C\), \(L_{}\) and \(_{}\).

Given a dataset \(=\{(x_{i},y_{i})\}_{i=1}^{n}\), we fit the neural network by solving the least squares problem

\[_{w}\ (w)\|F(w)\|^{2}\|F(w)\|^{2}=_{i=1}^{n}(f(w,x_{i})-y_{i})^{2}.\]

We assume that all the the data inputs \(x_{i}\) are bounded, i.e., \(\|x_{i}\| C\) for some constant \(C\).

Our immediate goal is to verify the assumptions of Theorem 3.1, which are quadratic growth and (uniform) aiming. We begin with the former. Quadratic growth is a consequence of the PL-condition. Namely, define the Neural Tangent Kernel \(K(w_{0})= F(w_{0}) F(w_{0})^{}\) at the random initial point \(w_{0} N(0,I)\) and let \(_{0}\) be the minimal eigenvalue of \(K(w_{0})\). The value \(_{0}\) has been shown to be positive with high probability in . Specifically, it was shown that, under a mild non-degeneracy condition on the data set, the smallest eigenvalue \(_{}\) of NTK of an infinitely wide neural network is positive (see Theorem 3.1 of ). Moreover, if the network width satisfies \(m=( 2^{O(l)}}{_{0}^{2}})\), then with probability at least \(1-\) the estimate \(_{0}>}{2}\) holds [8, Remark E.7]. Of course, this is worst case bound and for our purposes we will only need to ensure that \(_{0}\) is positive. It will also be important to know that \(\|F(w_{0})\|^{2}=O(1)\), which indeed occurs with high probability as shown in . To simplify notation, let us lump these two probabilities together and define

\[p\{_{0}>0,\|F(w_{0})\|^{2} C\}.\]

Next, we require the following theorem, which shows two fundamental properties on \(B_{r}(w_{0})\) when the width \(m\) is sufficiently large: (1) the function \(w f(w,x)\) is nearly linear and (2) the function \(\) satisfies the PL condition with parameter \(_{0}/2\).

**Theorem 3.2** (Transition to linearity  and the PL condition ).: _Given any radius \(r>0\), with probability \(1-p-2(-)-(1/m)^{( m)}\) of initialization \(w_{0} N(0,I)\), it holds:_

\[\|^{2}f(w,x)\|_{}=(}{} ) w B_{r}(w_{0}),\ \|x\| C.\] (3.1)

_In the same event, as long as the width of the network satisfies \(m=(}{_{0}^{2}})\), the function \(\) is PL\({}^{*}\) on \(B_{r}(w_{0})\) with parameter \(_{0}/2\)._

Note that (3.1) directly implies that the Lipschitz constant of \( F\) is bounded by \((}{})\) on \(B_{r}(w_{0})\), and can therefore be made arbitrarily small. Quadratic growth is now a direct consequence of the PL\({}^{*}\) condition while (uniform) aiming follows from an application of Theorem 3.1.

**Theorem 3.3** (Aiming and quadratic growth condition for wide neural network).: _With probability at least \(1-p-2(-)-(1/m)^{( m)}\) with respect to the initialization \(w_{0} N(0,I)\), as long as_

\[m=(}{_{0}^{2}}) r=(}}),\]

_the following are true:_1. _the quadratic growth condition (_2.1_) holds on_ \(B_{r}(w_{0})\) _with parameter_ \(_{0}/2\) _and the intersection_ \(B_{r}(w_{0}) S\) _is nonempty,_
2. _aiming (_2.2_) and uniform aiming (_2.4_) conditions hold in_ \(B_{r}(w_{0})\) _with_ \(=1\) _and_ \(=(}{})\)_,_
3. _the gradient of each function_ \(_{i}(w)(f(w,x_{i})-y_{i})^{2}\) _is_ \(\)_-Lipschitz on_ \(B_{r}(w_{0})\) _with_ \(=O(1)\)_._

Please see Appendix D for a numerical verification of an estimate of the aiming condition. It remains to deduce convergence guarantees for SGD by applying Theorem 2.5.

**Corollary 3.4** (Convergence of SGD for wide neural network).: _Fix constants \(_{1}(0,)\), \(_{2}(0,1)\), \(>0\) and \(t\). There is a stepsize \(=(1)\) such that the following is true. With probability at least \(1-p-_{1}-_{2}-2(-)-(1/m)^{( m)}\), as long as_

\[m=(}{_{0}^{2}}) r=(}}),\]

_all the SGD iterates \(\{w_{t}\}_{t 0}\) remain in \(B_{r}(w_{0})\) and the estimate \(^{2}(w_{t},S)^{2}( w_{0},S)\) holds after \(t}(})\) iterations._

Thus, the width requirements for SGD to converge at a fast linear rate are nearly identical to those for gradient descent , with the exception being that the requirement \(r=(}})\) is strengthened to \(r=(}})\). That is, the radius \(r\) needs to shrink by the probability of failure.

## 4 Boosting to high probability

A possible unsatisfying feature of Theorems 2.4 and 2.5 and Corollary 3.4 is that the size of the initialization region shrinks with the probability of failure \(_{1}\). A natural question is whether this requirement may be dropped. Indeed, we will now see how to boost the probability of success to be independent of \(_{1}\). A first reasonable idea is to simply rerun SGD a few times from an initialization region corresponding to \(_{1}=1/2\). Then by Hoeffding's inequality, after very trials, at least a third of them will be successful. The difficulty is to determine which trial was indeed successful. The fact that the solution set is not a singleton rules out strategies based on the geometric median of means [31, p. 243], . Instead, we may try to estimate the function value at each of the returned points. In a classical setting of stochastic optimization, this is a very bad idea because it amounts to mean estimation, which in turn requires \(O(1/^{2})\) samples. The saving Grace in the interpolation regime is that \((w,)\)_is a nonnegative function of the samples_. While estimating the mean of nonnegative random variables still requires \(O(1/^{2})\) samples, detecting that a nonnegative random variable is large requires very few samples! This basic idea is often called the small ball principle and is the basis for establishing generalization bounds with heavy tailed data . With this in mind, we will require the following mild condition, stipulating that the empirical average \(_{i=1}^{m}(w,z_{i})\) to be lower bounded by \((w)\) with high probability over the iid samples \(z_{i}\).

**Assumption 3** (Detecting large values).: _Suppose that there exist constants \(c_{1}>0\) and \(c_{2}>0\) such that for any \(w^{d}\), integer \(m\), and iid samples \(z_{1},,z_{m}\), the estimate holds:_

\[P(_{i=1}^{m}(w,z_{i}) c_{1}(w))  1-(-c_{2}m).\]

Importantly, this condition does not have anything to do with light tails. A standard sufficient condition for Assumption 3 is a small ball property.

**Assumption 4** (Small ball).: _There exist constants \(>0\) and \(p(0,1)\) satisfying_

\[(w,z)(w) p w ^{d}.\]

The small ball property simply asserts that \((w,)\) should not put too much mess on small values relative to its mean \((w)\). Bernstein's inequality directly shows that Assumption 4 implies Assumption 3. We summarize this observation in the following theorem.

**Lemma 4.1**.: _Assumption 4 implies Assumption 3 with \(c_{1}=\) and \(c_{2}=\)._

A valid bound for the small ball probabilities is furnished by the Paley-Zygmund inequality :

\[(w,z)(w)(1-)^{2}(w)^{2}}{[(w,z)^{2}]}.\]

Thus if the ratio \((w,z)^{2}}{(w)}\) is bounded by some \(D>0\), then the small ball condition holds with \(p=(1-)^{2}/D\) where \(\) is arbitrary.

The following lemma shows that under Assumption 3, we may turn any estimation procedure for finding a minimizer of \(\) that succeeds with constant probability into one that succeeds with high probability. The procedure simply draws a small batch of samples \(z_{1},,z_{m}\) and rejects those trial points \(w_{i}\) for which the empirical average \(_{j=1}^{m}(w_{i},z_{j})\) is too high.

**Lemma 4.2** (Rejection sampling).: _Let \(w_{1},,w_{k}\) be independent random variables satisfying \(((w_{i})) 1/2\). For each \(i=1,,k\) draw \(m\) samples \(z_{1},,z_{m}}{{}}\). For any \(>1\), define admissible indices \(=\{i[k]:_{j=1}^{m}(w_{i},z_{j}) \}.\) Then with probability \(1-(-)-k(-c_{2}m)-^{-k/4},\) the set \(\) is nonempty and \((w_{i})}\) for any \(i\)._

We may now simply combine SGD with rejection sampling to obtain high probability guarantees. Looking at Lemma 4.2, some thought shows that the overhead for high probability guarantees is dominated by \(c_{2}^{-1}\). As we saw from the Paley-Zygmond inequality, we always have \(c_{2}^{-1} D\) where \(D\) upper bounds the ratios \((w,z)^{2}}{(w)}\). It remains an interesting open question to investigate the scaling of small ball probabilities for overparametrized neural networks.

## 5 Conclusion

Existing results ensuring convergence of SGD under interpolation and the PL condition require the method to use a small stepsize, and therefore converge slowly. In this work we isolated conditions that enable SGD to take a large stepsize and therefore have similar iteration complexity as gradient descent. Consequently, our results align theory better with practice, where large stepsizes are routinely used. Moreover, we argued that these conditions are reasonable because they provably hold when training sufficiently wide feedforward neural networks with a linear output layer.

## 6 Acknowledgements

The work of Dmitriy Drusvyatskiy was supported by the NSF DMS 1651851 and CCF 1740551 awards. The work of Damek Davis is supported by an Alfred P. Sloan research fellowship and NSF DMS award 2047637. Yian Ma is supported by the NSF SCALE MoDL-2134209 and the CCF-2112665 (TILOS) awards, as well as the U.S. Department of Energy, Office of Science, and the Facebook Research award. Mikhail Belkin acknowledges support from National Science Foundation (NSF) and the Simons Foundation for the Collaboration on the Theoretical Foundations of Deep Learning (https://deepfoundations.ai/) through awards DMS-2031883 and #814639 and the TILOS institute (NSF CCF-2112665). This work used the programs (1) XSEDE (Extreme science and engineering discovery environment) which is supported by NSF grant numbers ACI-1548562, and (2) ACCESS (Advanced cyberinfrastructure coordination ecosystem: services & support) which is supported by NSF grants numbers #2138259, #2138286, #2138307, #2137603, and #2138296. Specifically, we used the resources from SDSC Expanse GPU compute nodes, and NCSA Delta system, via allocations TG-CIS220009.