# M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Instruction finetuning (IFT) is critical for aligning Large Language Models (LLMs) to follow instructions. Numerous effective IFT datasets have been proposed in the recent past, but most focus on rich resourced languages such as English. In this work, we propose a diverse, task taxonomy guided, fully synthetic **M**ultilingual, **M**ulti-turn evoked instruction finetuning dataset, called **M2Lingual**, to better align LLMs on a diverse set of languages and tasks. **M2Lingual** contains a total of 182K IFT pairs that are built upon diverse seeds collected from Aya collection and Aya dataset covering 70 languages, 19 NLP tasks and general instruction-response pairs. LLMs finetuned with **M2Lingual** substantially outperform the majority of existing multilingual IFT datasets. Importantly, LLMs trained with **M2Lingual** consistently achieves competitive results across wide variety of evaluation benchmarks compared to existing multilingual IFT datasets that enable LLMs performance in only one or a few subset of the benchmarks. Specifically, LLMs finetuned with **M2Lingual** achieve strong performance on multi-turn evaluation benchmarks such as MT-Bench and across wide-variety of multilingual tasks such as XQuAD, MGSM, TyDiQA, MLQA, XNLI and XLSUM. We show efficacy of **M2Lingual** across LLMs with different sizes, especially smaller LLMs with 1.8B size which benefit massively from our dataset. Lastly, we present key analyses to highlight importance of each synthesis step of **M2Lingual**.1

## 1 Introduction

Large language models (LLMs) have achieved remarkable success [1; 20; 21; 45; 53; 43], largely fueled by the availability of a wide variety of high-quality instruction fine-tuning (IFT) datasets [49; 48; 42; 31; 52; 4; 58]. However, most IFT data curation efforts focus on English or widely spoken languages, leaving low-resource languages and multilingual datasets underrepresented . Prior multilingual datasets can be either categorized into machine translated, human generated and human-AI generated datasets. Datasets like MultiAlpaca , Bactrian-X  and PolyLM  utilize machine translations and self-instruct  to generate instruction-response (IR) pairs in multiple languages. However, naive machine translation of English instructions may not capture native or regional knowledge alignment from different languages . Human-generated datasets like Aya  and Open Assistant  preserve regional knowledge alignment and cultural contexts, making them higher quality compared to translated datasets. However, gathering multilingual annotations in different languages from native speakers (on a large scale) is expensive, time consuming, and prone to annotator errors . Finally, human-AI generated datasets like LMSYS-1M , ShareGPT, Vicuna , and WildChat  involve a human interacting with an AI assistant to gather data. Although such datasets are relatively cheaper to gather compared to human-generated ones, they stillcome with several challenges. These include privacy issues, moderate complexity instructions, and necessary legal regulatory constraints.

As summarized in Table 1, most of the above datasets like Aya, Bactrian-X, MultiAlpaca are single-turn only, which limits a model's ability to engage in long, multilingual conversations. Additionally, IFT datasets with multi-million scale like the Aya collection  containing \(513M\) IR pairs or XP3  having \(75+M\) pairs can be expensive to finetune LLMs or analyze their different qualitative aspects.Additionally, many multilingual datasets do not include diverse NLP tasks and general instructions across low resource languages, often containing very simple instructions, thus limiting their effectiveness in training strong multilingual instruction following LLMs.

To address these shortcomings, we present **M2Lingual**, a diverse _Multilingual_, _Multi-turn_ IFT dataset which is fully synthetic, containing machine generated \(182K\) instructions, and covering \(70\) languages. The dataset is built upon seed samples from a) the human generated Aya dataset, where general IR pairs are annotated by regional, native language speakers, and b) seeds from Aya collection that contain IR pairs from \(17\) diverse NLP tasks. Unlike previous IFT datasets that use self-instruct mechanism and machine translation to generate data in more languages, **M2Lingual** is constructed with a task-specific taxonomy guided evolve (denoted as _Evol_) conditions  to generate new IR pairs from the seed samples in each language. The _Evol_ taxonomy covers a diverse range of NLP tasks, regional dialects and slang, resulting in instructions that are diverse, detailed, more complex, and longer in length. Furthermore, to improve LLMs in engaging multilingual conversations we define a multi-turn _Evol_ taxonomy for generating conversational IR pairs. The multi-turn (MT) taxonomy covers a wide variety of possible subsequent user instructions as discussed in Section 3, shown in Figure 2. The proposed data enrichment taxonomy for curating new complex & diverse instruction and multi-turn conversations is generic, and can be extended to any monolingual or multilingual data. We also ensure balanced _Evol_ generations across languages, creating IR pairs equally for all 70 languages.

In evaluations, we conduct experiments across several multilingual NLP benchmarks and a multi-turn benchmark called MT-Bench , translated in \(8\) languages. We empirically demonstrate the effectiveness of **M2Lingual** by comparing with finetuning several LLMs from different family and sizes with existing multilingual IFT datasets. Our results show that **M2Lingual** leads to best or second best performance across a) several multilingual evaluation benchmarks datasets, and b) multi-turn MT-Bench evaluations. On the other hand, existing IFT datasets show competitive results but only in a few subset of evaluations while performing poorly in other evaluations.

The key contributions from our work are as follows:

1. [leftmargin=*]
2. We present **M2Lingual**, a fully synthetic multilingual, multi-turn IFT dataset of \(182K\) IR pairs that lead to best performance in both multilingual evaluation benchmarks and complex multi-turn evaluation dataset MT-Bench. **M2Lingual** is synthesized using a data enrichment taxonomy focused on adding instruction-specific and multi-turn specific evolve  complexities. Our data enrichment taxonomies and steps used in **M2Lingual** synthesis can be easily extended to other languages, monolingual settings, and any instruction seeds.
3. **M2Lingual** contains (roughly) equal distribution of IFT pairs across 70 languages ensuring strong performance improvements in multiple languages but notably in low resource languages. Additionally, smaller LLMs like QWEN-1.8B show massive improvements when finetuned with **M2Lingual**, highlighting its usefulness with more accessible models.
4. We present several key ablation studies to highlight the impact of every data enrichment and synthesis steps used in **M2Lingual** generation. We show that adding instruction-task specific complexities improves average performance across several multilingual evaluation benchmarks whereas adding multi-turn specific evols leads to strong improvements on MT-Bench.

## 2 Related Work

Pretraining LLMs is computationally expensive, and due to abundance vs scarcity of corpus in different languages [27; 34; 26; 23], the majority of pretraining is done in high resource languages like English. This often leads to LLMs performing much better in high resource languages [32; 34] compared to low resource languages. Multilingual instruction finetuning has proven to be a relatively cost effective solutions for improving multilingual performance of LLMs, especially for low resource languages [37; 5; 46]. While several IFT datasets have been introduced in the recent past, less focus has been given on synthesizing multilingual IFT datasets limiting progress in various languages.

Instruction finetuning datasets are often created from pool of numerous NLP tasks (e.g., flanT5, supernatural instructions) [8; 38; 49], machine generated (e.g., self-instruct ), human expert annotated (e.g., LIMA ) and crowd-sourced or cached from real users chat (e.g., LMSYS, WildChat) . A few synthetic IFT datasets [29; 42] have leveraged LLMs such as GPT-4 for generating IR pairs via self-instruct as a relatively affordable alternative. Although very effective, a few works have highlighted issues in data generation process of self-instruct [4; 16] where generations can be considered a bit uncontrolled. For example, Alpaca  uses self-instruct to generate \(52K\) instructions from \(175\) seeds, where overlapping and noisy IFT pairs have been reported [4; 58]. In contrast, techniques like WizardLM  that generate new instructions by adding complexity (or _Evol_) on input seed instructions, have generations which are controlled by the set of _Evol_ conditions, ensuring diverse generations. For example, as shown in fig. 1, the _Concterize_\(Evol\) in left block creates a new IFT pair with a more complex but concrete python question. Furthermore, generating templated datasets for specific NLP tasks (e.g., XP3 ) may not contain complex and diverse inputs for aligning instruction following of LLMs. Thus, inspired from WizardLM, **M2Lingual** contains _Evol_ IFT pairs generated from a set of diverse IFT seeds. Additionally, we also create a multi-turn _Evol_ taxonomy (fig. 2) which is used to generate multi-turn IR pairs resulting in a diverse, complex conversational IFT set within **M2Lingual**. Features of some of the existing IFT datasets are summarized in table 1.

## 3 Methodology

In this section, we detail the three main synthesizing steps of **M2Lingual**. Step 1 (Section 3.1) involves selection of diverse multilingual seed data. In our work, we select seed samples from two different sources of Aya -- Aya dataset and Aya Collection, both of which receive high average approval ratio by human annotators . Step 2 (Section 3.2) and 3 (Section 3.3) correspond to our novel _Evol_ taxonomy based data enrichment techniques. Specifically, in Step 2 we create an NLP task specific _Evol_ taxonomy  and generate new IR pairs using these _Evol_ conditions. In Step 3, we first create an _Evol_ taxonomy for multi-turn or conversational IR pairs, and then use these _Evol_ for generating new conversational, multi-turn IR pairs. Figure 1 captures an overview of each of these generation steps used in synthesis of **M2Lingual**.

### Seed Selection

Our first seed source, Aya dataset, contains general IR pairs written by native speakers/annotators which enables capturing region-specific language nuances and cultural contexts. We randomly select \(100\) prompts for each of the \(70\) languages resulting in \(7000\) seed samples from the Aya dataset. Our second seed source, Aya collection, covers \(19\) different NLP tasks where each task has parallel examples in \(113\) different languages. To ensure a proper balance of the number of examples across all languages, we only focus on \(70\) languages of the Aya dataset. We exclude two NLP tasks from Aya collection - 1) text simplification as it requires rewriting a complex or a simplified version of a sentence which is already supported by our evols, and 2) multilingual event entity task as Aya collection does not have a consistent format for this task. Finally, for each task in the Aya collection, we randomly sample \(6\) examples per language, resulting in \(6*70*17=7140\) seed examples. We select \(6\) random samples per task per language to ensure balanced amount of seed samples from Aya collection when compared to the seeds from Aya dataset. Thus, our final seed contains \(7000+7140=14140\) samples.

    &  &  &  & Resource Level & Task & General & Translated & Fully \\  & & & & Low & High & specific? & instructions? & dataset? & synthetic? \\  OpenAssistant & 10K convs & ✓ & 35 & 3 & 32 & ✗ & ✗ & ✗ & ✗ \\ Aya Dataset & 200K IR pairs & ✗ & 70 & 37 (1) & 32 & ✗ & ✓ & ✗ & ✗ \\ MultiAlpaca & 52K IR pairs & ✗ & 12 & 0 & 12 & ✓ & ✓ & ✓ & ✓ \\ Bactrian-X & 3.4M IR pairs & ✗ & 52 & 15(1) & 36 & ✗ & ✓ & ✓ & ✓ \\ ShareGPT & 94K convs & ✓ & 45 & 4 (2) & 39 & ✓ & ✗ & ✗ & ✗ \\ WildChat & 1.04M convs & ✓ & 74 & 21 (3) & 50 & ✗ & ✗ & ✗ & ✗ \\
**M2Lingual** & 182K convs & ✓ & 70 & 37 (1) & 32 & ✓ & ✓ & ✗ & ✓ \\   

Table 1: Comparison of multilingual IFT datasets with **M2Lingual**. Resource level classification taken from NLLB . Languages not found in the NLLB table are counted as low, in parentheses.

### Task Guided _Evol_

Though the IR pairs in the seed data from Aya cover a wide variety of NLP tasks, overall they are direct and less intricate. To enhance the instruction following abilities of LLMs, especially for complex tasks and prompts, we employ _Evol_-Instruct  on our selected seed instructions as the second data synthesis step. _Evol_-Instruct generates a more complex instructions using _Evol_ conditions over the provided seed instruction . The generic _Evol_ conditions used in the original work2 are not always applicable to a wide variety of downstream NLP tasks such as multi-hop question answering, joke explanation, etc. Furthermore, generic _Evol_ conditions also provide weak or broader guidance for new IFT pair generations, especially for seed datasets like ours which cover diverse NLP tasks.

To address this, we create a taxonomy of _Evol_ conditions covering general instructions (for seed from Aya dataset) and each of the NLP tasks (for seeds from Aya collection) as shown in Figure 2. Specifically, we create 6 _Evol_ conditions enhancing multi-lingual features focused solely on general instructions. We then leverage GPT-4 to come up with \(9\) different _Evol_ for each NLP task. These NLP task specific _Evol_ ensure that we create _Evol_ conditions separately for a particular task. Figure 2 shows all the NLP task names and the corresponding evols.

The _Evol_ prompts to GPT-4 for each task in Figure 2 are presented in detail in Appendix 9.3. We apply these task-specific _Evol_ on our selected seeds:

* The seed samples from the Aya dataset are generic instructions therefore we apply the \(6\)_generic evols_ (Figure 2) to each seed sample. The \(6\) different evols ensure that the new instruction is more complex, challenging and captures all the multilingual variations, nuances and complexities of different languages. This results in \(7K 6=42K\) samples from the seeds in Aya dataset.
* The seed samples from Aya collection have \(17\) NLP tasks which are shown in the top block of Figure 2 along with their corresponding \(9\) evols. For each seed from a particular task, we apply its corresponding \(9\) evols resulting in a total of \(7140 9=64260\) instructions from Aya collection.
* Upon manual inspection of the generated instructions across both Aya dataset & collection, we observe that some of the instructions generated using GPT-4 have repetitive long sequences and n-grams. Therefore, following [18; 14], we filter instructions with frequent n-grams. Some requests to GPT-4 also return a time out. The final datasets thus contains \(95K\) instructions, \(37K\) from the Aya dataset and \(57K\) from Aya collection as shown in Table 2.

  
**Dataset** & **Seed** & **Evoked** & **Multi-turn** \\  Aya Dataset & 7000 & 37803 & 36969 \\ Aya Collection & 7140 & 57145 & 34426 \\ 
**Total** & **14140** & **94948** & **71395** \\    
  
**Total** & **14140** & **94948** & **71395** \\  Aya Instruction & 49.60 & 107.71 & 356.81 \\ Aye Response & 56.79 & 62.16 & 87.60 \\   

Table 2: **M2Lingual** IR pairs. Aye Instruction and Response show avg no of tokens.

Figure 1: Walk-through examples for data synthesis of **M2Lingual**. In Step 1, seeds are selected from Aya dataset (left figure) and Aya collection (right figure). In Step 2, the task specific _Evol_ taxonomy is used for generating new, complex and evolved instructions. General _Evol_ are used for seeds from Aya dataset and NLP task specific (see Joke explanation task evols) _Evol_ are used for Aya collection seed. Finally, in Step 3, multi-turn instructions are generated on top of the new evolved instruction generated from Step 2.

### Generating Multiple Turns

As the final step 3 in synthesis of **M2Lingual**, we generate multiple user-assistant turns from the task-evolved instructions produced in the previous step. A conversation between a user and an AI assistant broadly can be categorized into four categories  -- _Follow-up_, _Refinement_, _Expansion_, and _Recollection_. However, these categories are generic and do not encompass the full complexity and fine-grained variety of conversational interactions. To address this, we introduce a multi-turn taxonomy comprising \(21\) distinct variations of dialogue that expand upon these four categories. These \(21\) detailed taxonomy with the 4 categories improve coverage of possible variations in continuing a conversation, thus ensuring an engaging interaction between a user and an assistant . We also ensure that subsequent instructions are generated in the language of the initial instruction by explicitly prompting GPT-4. The last block in Figure 2 shows all the \(21\) variations. The multi-turn _Evol_ prompts to GPT-4 are shown in Appendix 9.4. We convert the instructions to multi-turn conversations with following steps:

1. We use the prompt specified in Appendix 9.4 and replace the _{instruction}_/ with the task-evoiced generated instructions from the previous step (i.e., Step 2), _{follow_up_type}_/ with one of the \(21\) dialogue variations in 9.4, and _{language}_/ with the _Evol_ instruction language. We then pass it to GPT-4 \(n\) times to generate the next user instruction.
2. For all the generated instructions, we generate subsequent response turns using GPT-4 using the entire conversation history. To mitigate the potential impact of topic drift from the prolonged conversations , we restrict the number of subsequent instructions or multi-turns to \(<=4\).
3. We generate conversations for all the evoked instructions from the Aya dataset, resulting in \(36\)K conversations. For Aya collection, we pick a balanced subset of size \(35\)K across all tasks and languages and generate conversations. After applying the same post-processing steps as mentioned in Section 3.2, we end up with \(70\)K conversations.

In total, **M2Lingual** contains \(182\)K IR pairs, with the exact sizes from different steps of **M2Lingual** synthesis shown in Table 2.

## 4 Experiments

We conduct experiments across _three_ model families & _five_ model sizes -- Mistral-7B , LLaMA-3-8B  and QWEN-4B . Furthermore, to demonstrate the effectiveness of our dataset across different model scales, we fine-tune both a larger model, LLaMA-2-13B , and a smaller model, QWEN-1.8B . To evaluate how well the datasets work with instruction-tuned models, we also experiment with Mistral-Instruct-7B.

### Baseline Datasets

We use _six_ different multilingual datasets as baselines for comparison: 1) the top ranked conversation trees from **Open Assistant**, 2) **Aya**, 3) self-instruct dataset **MultiAlpaca**, 4) machine translated **Bactrian-X** derived from Alpaca-52k  and Dolly-15k , 5) the **ShareGPT**3 collection, and 6) **WildChat**.

Figure 2: Taxonomy of _Evol_ applied towards creating **M2Lingual**. Part 1 includes evols on the general Aya dataset as well as the task-specific Aya collection data, after which Part 2 multi-turn _Evol_are applied for creating multiple turns in the conversation.

For a fair comparison with WildChat, we use \(200\)K non-English conversations, ensuring the same language proportions, and downsampled \(60\)K English conversations, resulting in a total of \(260\)K conversations. Similarly for Bactrian-X, we sample 1M IR pairs ensuring the same language proportion as of the original dataset.

**Additional Baselines** To highlight the importance of each step in our data curation process, we consider several ablations as baselines. Specifically we conduct experiments by training models using 1) only **Seed** samples, 2) seed samples with the generated evols (**Seed + Evol**) and 3) seeds, evols and the generated multi-turn conversations (**Seed + Evol + MT**). Finally, to see whether adding parallel data (PD) helps in improving the over model's performance, we collect \(60\)K from the Aya collection and train a baseline by augmenting the PD with our full dataset (**Seed + Evol + MT + PD**).

### Training

All training is performed on \(8\) A-\(100\)\(80\)GB NViDIA GPUs , with the Axolott4 framework. We used Mistral tags  for finetuning all models. We use a batch size of \(64\), max seq length \(8192\), learning rate of \(5 10^{-6}\), Adam optimizer  with a cosine scheduler and \(10\) warmup steps. We reserve a \(5\)% validation split, and train all the models until validation loss convergence. We compute the loss only on the targets using fp\(16\) training.

### Evaluation

**Multilingual benchmarks.** We utilize the EleutherAI evaluation framework  for consistent comparisons. We evaluate the performance of different multilingual datasets on the following tasks:

* _Question Answering (QA)_: We focus on \(3\) multilingual QA datasets 1) XQUAD  with QA across \(11\) languages, 2) TyDiQA  which has human generated QA in \(11\) languages and 3) MLQA  with QA in \(7\) languages. While QA data requires short answer phrases, conversational IR pairs might lead to longer answer span generation. Hence, we use \(3\) in-context examples to get the right output format for LLMs. In the interest of time, we keep the number of examples per language to \(100\) for XQUAD and MLQA, and \(1000\) for TyDiQA. We use the validation set for XQUAD and test set for TyDiQA & MLQA, and compute the standard F1-score.
* Arabic, English, Spanish, French, Japanese and Russian. We restrict the total number of examples to \(100\) and prompt the model to generate a summary in the same language as the context. We look at the ROUGE\({}_{}\) & BLEU  scores for comparison.
* _Classification_: We focus on XNLI  and XCOPA  with \(15\) and \(11\) languages respectively in a zero-shot setting. We compute the accuracy (Acc) by looking at the log-likelihood assigned to the ground truth answer on the validation set.
* _Multilingual math word problems_: We use MGSM , a grade-school math benchmark that translates GSM8K  to \(10\) different languages. Similar to QA tasks, we use \(3\) in-context examples and compute the exact match (EM) with the ground truth answer.

**Translated MT-Bench.** To evaluate the conversation and instruction following ability of multilingual models across a wide array of tasks and languages, we translate MT-Bench . MT-Bench comprises of \(80\) multi-turn questions across \(8\) domains. The models are required to respond to an initial and a follow-up question and GPT-4 assesses the model's responses on a scale of \(1\) to \(10\) (\(10\) being the best), with the overall score being the mean over the two turns. We translate it into \(9\) different languages with professional linguists to ensure high quality evaluation. We modify the judge prompt to include the language of the question asked at each turn, and additionally instruct GPT-4 to make sure the responses are in the same language as the question asked. We report the average scores across all \(80\) examples for each language and also report the average MT-Bench score across all languages.

**Low-resource Languages.** We evaluate models on \(6\) low-resource languages, including Hindi, Urdu, Thai, Tamil, Bengali and Gujarati using the same aforementioned procedure. Since finding native annotators for these low-resource languages might be difficult, we leverage GPT-4 for translations.

## 5 Results

**Consistent best results:** As observed in table 3 and 4, **M2Lingual** leads to best or amongst the top results in both _multi-turn evaluations_ (Table 3) and _several NLP task comprised multilingual evaluation benchmarks_ (Table 4). On the other hand, the majority of baseline datasets show competitive results only on select evaluation settings. Specifically, on _multi-turn evaluations_, conversational IFT datasets like ShareGPT and WildChat lead to competitive performances but all of the other baseline datasets have low MT-Bench scores. This suggests importance of including multi-turn IR sets within IFT datasets. **M2Lingual** achieves top MT-Bench score in 5 languages and 2nd best in remaining 3 languages, outperforming all of the baseline datasets overall. Similarly, as shown in Table 4, **M2Lingual** also leads to performance across 4 out of 7 _multilingual NLP task evaluation benchmarks_ and 2nd best results with very close performance to the best score in remaining 3 benchmarks. It is worth noting that evaluations on classification tasks such as XCOPA and XNLI show very minimal performance variations across all IFT datasets which has been shown in other works as well . In generation tasks (MGSM, XLSUM, including QA tasks MLQA, XQuAD, TyDiQA), **M2Lingual** leads to better results over all the baseline datasets. For instance, with Mistral-7B base model, our proposed **M2Lingual** outperforms the second best baseline (Bactrian-X) by 2.13 and 1.98 F1 points on MLQA and XQuAD respectively.

    &  & **XQUAD** & **TyDiQA** & **MLQA** & **XLSUM** & **MGSM** & **XNLI** & **XCOPA** \\  & & F1 & F1 & F1 & ROUGE\({}_{}\) & BLEU & EM & Acc & Acc \\   & Open Assistant & 67.99 & 54.22 & 53.64 & 10.86 & 0.85 & 16.05 & 42.74 & 56.73 \\  & MultiAlpaca & 67.99 & 64.44 & 55.69 & 10.9 & 1.59 & 10.41 & 42.18 & 58.91 \\  & Backin-X & 71.91 & 66.63 & 60.27 & 3.30 & 0.20 & 17.14 & 43.91 & 58.64 \\  & ShareGPT & 66.33 & 56.97 & 50.78 & 3.31 & 0.288 & 11.32 & 41.13 & 56.09 \\  & WildChat & 72.55 & 64.27 & 59.53 & 3.91 & 0.41 & **18.41** & 43.11 & 58.00 \\  & Aya & 70.46 & 66.95 & 57.47 & **12.5** & **2.01** & 13.86 & 41.78 & 59.00 \\   & Seed & 72.52 & 65.89 & 59.33 & 11.53 & 1.72 & 16.95 & 43.28 & 57.64 \\  & Seed + Evol & 71.01 & 65.04 & 57.47 & 9.8 & 1.37 & 14.23 & 43.00 & 57.55 \\  & Seed + Evol + MT (**M2Lingual**) & **74.53** & **67.57** & **62.40** & 10.42 & 1.38 & 15.38 & 42.12 & **59.55** \\  & Seed + Evol + MT + PD & 68.79 & 62.62 & 60.00 & 9.92 & 1.37 & 16.45 & 42.36 & 59.00 \\   & Open Assistant & 64.38 & 52.65 & 47.08 & 9.38 & 1.21 & 17.36 & 46.17 & **63.82** \\  & MultiAlpaca & 75.08 & 64.49 & 59.01 & **10.93** & **1.45** & 10.68 & **46.93** & 63.55 \\  & Backin-X & 69.57 & 56.45 & 58.51 & 8.39 & 1.28 & 22.86 & 46.90 & 62.18 \\  & ShareGPT & 56.98 & 58.48 & 43.43 & 3.53 & 0.40 & 25.32 & 45.93 & 63.00 \\  & WildChat & 63.15 & 59.88 & 63.16 & 5.52 & 0.76 & 26.36 & 46.88 & 62.27 \\  & Aya & 75.14 & 59.60 & 53.14 & 10.38 & 1.39 & 22.09 & 45.64 & 63.55 \\   & Seed & 77.27 & 68.57 & 60.01 & 9.92 & 1.45 & 17.18 & 46.02 & 62.82 \\   & Seed + Evol + MT (**M2Lingual**) & 76.17 & **69.89** & 63.09 & 8.96 & 1.23 & **28.00** & 46.38 & 61.36 \\   & Seed + Evol + MT (**M2Lingual**) & 75.91 & 67.84 & **63.50** & 8.87 & 1.25 & 27.36 & 46.18 & 62.55 \\   & Seed + Evol + MT + PD & 76.69 & 59.24 & 60.02 & 9.84 & 1.37 & **29.00** & 46.37 & 62.09 \\   

Table 4: Evaluations of LLaMA-3-8B-base & & Mistral-7B-base in different tasks. Same notations as in Table 3

  
**Model** & **Dataset** & **MT-EN** & **MT-FR** & **MT-TT** & **MT-JP** & **MT-ES** & **MT-DE** & **MT-NL** & **MT-PT** & **MT-AVG** \\   & Open Assistant & 6.72 & 5.87 (5.90) & 6.04 & 4.19 & 5.87 & 5.82 & 4.97 & 6.01 & 5.66 \\  & MultiAlpaca & 5.45 & 4.90 (5.22) & 4.63 & 3.76 & 5.01 & 4.66 & 4.51 & 4.65 & 4.77 \\  & Bactrian-X & 5.60 & 5.35 (5.26) & 5.46 & 4.82 & 5.24 & 5.53 & 4.96 & 5.31 & 5.25 \\  & ShareGPT & 7.04 & 5.93 (5.70) & 5.42 & 4.75 & 5.83 & 6.00 & 5.27 & 5.92 & 5.80 \\  & WildChat & 7.02 & 5.646 (7.67) & 6.68 & 5.50 & 6.71 & **64.33** & **65.81** & **63.9** & **65.53** \\  & & Aya & 6.43 & 5.42 (5.39) & 4.97 & 3.37 & 5.45 & 5.37 & 4.94 & 5.12 & 5.18 \\   & Seed & 6.01 & 5.15 (5.14) & 5.35 & 3.44 & 5.07 & 5.98 & 4.62 & 4.91 & 5.04 \\  & Seed + Evol & 6.33 & 5.44 (5.30) & 5.46 & 4.74 & 5.88 & 5.61 & 5.40 & 5.78 & 5.56 \\  & Seed + Evol + MT (**M2Lingual**) & **7.33** & **67.58** (**61.81**) & **6.9** & **5.70** & **68.81** & 6.39 & 6.34 & **6.46** & **65.84** \\   & Seed + Evol + MT + PD & 5.85 & 5.75 (5.39) & 5.60 & 4.86 & 5.81 & 5.73 & 5.32 & 5.74 & 5.55 \\   & Open Assistant & 6.26 & 5.15 (5.03) & 4.95 & 4.08 & 5.26 & 4.87 & 5.01 & 5.48 & 5.12 \\  & MultiAlpaca & 4.96 & 4.65 (5.09) & 4.22 & 3.30 & 4.76 & 4.18 & 4.32 & 4.27 & 4.41 \\   & Backin-X & 6.27 & 5.73 (5.77) & 5.73 & 4.83 & 5.95 & 5.34 & 5.41 & 5.90 & 5.66 \\   & ShareGPT & 7.07 & 6.17 (5.76) & 6.43 & 5.40 & 6.10 & 6.07 & 5.82 & 6.13 & 6.10 \\   & WildChat & **7.20** & **6.74 (**66.67**) & 6.78 & **6.35** & 6.86 & 6.60 & 6.58 & 6.72 & **6.75** \\   & Aya & 5.95 & 5.01 (4.50) & 5.41 & 3.86 & 5.27 & 4.93 &

**Impact on different LLMs:** We evaluate Mistral-Instruct-7B to highlight the impact of multilingual IFT datasets on pre-instruction finetuned models. **M2Lingual** leads Mistral-Instruct-7B to achieve best performance in \(5\) of \(8\) MT-Bench language evaluations and \(5\) of the \(7\) multilingual evaluation benchmarks as shown in Tables 5 and 8 respectively. Interestingly, the improvements from **M2Lingual** in Mistral-Instruct-7B over baseline datasets is consistently higher when compared to Mistral-7B-base (Table 4) in all of the multilingual QA tasks, MGSM, and XCOPA. We also evaluate QWEN-4B model to showcase results from smaller LLM from different model family. We observe similar findings as QWEN-4B finetuned with **M2Lingual** achieves competitive results in both MT-Bench and multilingual evaluation datasets. Another interesting observation is that improvements seem relatively higher for QWEN-4B model using **M2Lingual** when compared to Mistral-7B and LLaMA-3-8B models, highlighting the usefulness of our proposed data on moderate sized LLMs.

**Ablation of M2Lingual:** As observed in our empirical studies in Tables 4, 5, 6, 7, generating _Evol_ data and appending it with seeds helps improve performance consistently in all evaluation benchmarks, highlighting impact of _Evol_. For instance, on LLaMA-3-8B, our proposed _Evol_ improves performance significantly by 10.82 points on MGSM task as compared to the Seed data. Adding multi-turn IFT pairs specifically helps boost performance in MT-Bench evaluations substantially across all of the languages with the most significant gain of 1.31 points on French for Mistral-7B model. Adding multi-turn data also helps consistently in multilingual benchmark evaluations as shown in Tables 4 and 5. _Evol_ + _MT_ and _Evol_ provides 1.50 and 0.98 points of MT-AVG performance gain over _Seed-only_ IFT data on Mistral-7B model. These findings reinforce the benefits of adding multi-turn _Evol_ IFT pairs.

**Low-resource languages:** We also compare **M2Lingual** with our most competitive baseline dataset WildChat on MT-Bench in low-resource languages. As shown in Figure 3, **M2Lingual** consistently leads to much higher MT-Bench scores in majority of the low-resource languages highlighting that existing multi-turn IFT datasets created from cached user chats may have poor coverage of low-resourced languages. On the other hand, our synthetically generated **M2Lingual** has uniform coverage of all the 70 languages in terms of number of IFT sets.

    &  & **XQUAD** & **TyDiQA** & **M1QA** & **XLSUM** & **MGSM** & **XNLI** & **XCOPA** & **MT-Avg** \\  & & F1 & F1 & F1 & ROUGE\({}_{}\) & BLEU & EM & Acc & Acc & **MT-Avg** \\   & Open Assistant & 53.63 & 45.30 & 46.34 & 4.15 & 0.29 & 17.50 & 38.52 & 58.45 & 3.47 \\  & MultiAlpaca & 51.81 & 53.81 & 40.26 & 8.9 & 1.0 & 12.1 & 38.3 & 58.40 & 2.93 \\  & Backrim-X & 46.70 & 42.79 & 42.2 & 7.1 & 0.8 & 18.6 & 38.3 & 57.70 & 3.80 \\  & ShareGPT & 41.86 & 28.20 & 36.03 & 4.58 & 0.43 & 16.95 & 37.83 & **58.55** & 3.80 \\  & WildChat & 53.18 & 49.18 & 42.81 & 5.23 & 0.56 & 19.27 & **38.74** & 58.18 & **42.92** \\  & Ava & 54.00 & 52.14 & 48.28 & **10.91** & 13.1 & 16.50 & 3.59 & 57.73 & 3.43 \\   & Seed & **Evol** & **46.55** & **58.00** & 48.25 & 10.65 & 0.65 & 15.36 & 37.59 & 58.00 & 2.47 \\  & Seed + Evol & 52.24\({}^{*}\) & 52.50 & **49.87** & 8.50 & 1.12 & 20.77 & 38.36 & 57.91 & 3.79 \\  & Seed + Evol + MT (**M2Lingual**) & 49.12 & 47.53 & **50.36** & 8.30 & 1.02 & **21.36** & 38.37 & 58.36 & 4.23 \\  & Seed + Evol + MT + PD & 57.76 & 51.97 & 43.24 & 9.64 & 1.21 & 19.36 & 37.95 & 57.91 & **42.42** \\   & Open Assistant & 61.33 & 59.28 & 53.27 & 9.62 & 1.43 & 19.00 & 45.91 & 58.09 & 5.58 \\  & MultiAlpaca & 63.76 & 63.05 & 51.09 & 11.51 & 1.80 & 13.18 & **44.70** & 58.18 & 4.74 \\  & Backrim-X & 70.5 & 64.8 & 50.60 & 9.14 & 1.35 & 17.91 & 42.23 & 57.25 & 5.98 \\  & ShareGPT & 44.53 & 49.5 & 40.45 & 3.31 & 0.38 & 17.36 & 42.13 & 56.73 & 6.11 \\  & WildChat & 61.53 & 53.1 & 52.60 & 6.31 & 0.56 & 21.00 & 41.86 & 57.75 & 6.62 \\  & Ava & 69.9 & 66.43 & 57.27 & **12.55** & **2.05** & 16.36 & 42.84 & 58.60 & 5.20 \\   & Seed & 68.78 & 61.54 & 56.11 & 12.45 & 2.04 & 18.27 & 43.23 & 58.45 & 3.92 \\  & Seed + Evol & **72.87** & 68.43 & 55.43 & **12.51** & 1.33 & 22.00 & 42.51 & 58.09 & 6.48 \\  & Seed + Evol + MT (**M2Lingual**) & 71.41 & 69.44 & **58.33** & 9.57 & 1.51 & 19.82 & 42.37 & **59.45** & **6.64** \\  & Seed + Evol + MT + PD & 70.04 & **69.67** & **59.13** & 9.06 & 1.46 & 22.27 & 42.92 & 57.82 & 6.56 \\   

Table 5: Evaluations of QWEN-4B & _Mistral-Instruct-7B_ in different tasks and MT-Bench score averaged across languages. Please see table 8 in appendix for MT-Bench score in each language. \(\) in XQUAD, TyDiQA scores for QWEN-4B show exception cases where outputs had repeated noisy patterns in multiple runs resulting in low scores.

Figure 3: Results on Low Resource translated MT-Bench for Mistral-7B and LLaMA-3-8BAdditional Analysis

**Effect of IFT datasets on different sized LLMs.** In addition to 4B, 7B, and 8B sized LLMs shown in Tables 4, 5, 6, 7, we also study impact of our IFT datasets on a smaller LLM (QWEN-1.8B) and a larger LLM (LLaMA-2-13B). As shown in Table 6, on QWEN-1.8B LLM, **M2Lingual** leads to even higher performance for various tasks when compared to our strongest baseline WildChat with the most significant improvements of \(13.64\) and \(22.24\) points on MGSM and TyDiQA tasks respectively. Similar findings for the LLaMA-2-13B model highlights the effectiveness of our proposed **M2Lingual** across various sized LLMs.

**Importance of _Evol_.** We selected \(15.1K\) seeds from Aya dataset and Aya collection as discussed in section 3.1. We generated _Evol_ and multi-turn IR pairs from these seed but as an alternative, more data can also be sampled from Aya. To highlight benefits from _Evol_ and generating multi-turn IR pairs, we sample \(94.9K\) more IR pairs from Aya collection and Aya dataset, making the total seed size as **M2Lingual** of \(110.5K\). As shown in Table 7, simply sampling more seed IFT pairs from Aya achieves low performance, whereas having the same number _Evol_IR pairs from **M2Lingual** leads to much higher performance in MT-Bench and MGSM (\(2.05\) and \(6.68\) points respectively). It is worth noting that MLQA being reading comprehension QA data requires short answer phrases for exact match. IR pairs within **M2Lingual** are longer (Table 2) and conversational which often lead to longer answer span generation for reading comprehension task. Hence, we utilized 3-shot setting to get the right output format from LLMs in our QA evaluation experiments.

## 7 Conclusion

We build **M2Lingual**, a multilingual, multi-turn IFT dataset that leads to top performances across several multilingual evaluation benchmarks. Our work presents two IFT data enrichment techniques, namely 1) taxonomy based instruction-task specific _Evol_, and 2) multi-turn _Evol_ for generating a diverse, conversational multilingual IFT dataset. **M2Lingual** contains roughly same number of IR pairs for 70 languages, resulting in substantial performance improvements in low resource languages. **M2Lingual**-also strongly improves multilingual performance of different sized LLMs ranging from 4B, 7B, 8B, and 13B parameters but in particular leads to massive improvements of small LLMs with 1.8B parameter size. Thus, **M2Lingual** presents a strong societal impact specifically for underrepresented or low-resourced languages. Additionally, the massive performance gains with **M2Lingual** on smaller 1.8B parameter LLMs also contributes towards improving accessibility to the wider community.

## 8 Limitations and Ethical Considerations

As future work and limitations of our work, **M2Lingual** can be extended to more than 3 turns to generate longer conversational IFT data, although this would be computationally expensive. Similarly, **M2Lingual** can be extended to more number of NLP tasks and languages in future work. In this work, we select seeds from Aya which does not contain specific flags for toxic, harmful, or offensive speech , but report low risk. We conducted manual inspection of a few generated IFT pairs from _Evol_, and did not find any harmful IFT data, but future work includes filtering **M2Lingual** by automatic safety tools.

   Model Name & Dataset & MT-EN & MT-FR & MT-IT & MT-JP & MT-ES & MT-DE & MT-NL & MT-PT & MT-Avg & MGSM & MLQA & TyDiQA \\   & WildChat & 4.99 & 2.75 (2.74) & 2.08 & 1.72 & 2.88 & 1.92 & 1.54 & 2.68 & 2.59 & 8.00 & 29.39 & 42.42 \\  & MultiMedia & 3.97 & 4.93 (1.99) & 1.74 & 1.44 & 1.87 & 1.86 & 1.52 & 1.91 & 2.03 & 7.45 & 19.30 & 33.38 \\  & **M2Lingual** & **6.20** & **4.55 (4.25)** & **3.85** & **3.27** & **4.00** & **4.11** & **3.32** & **4.51** & **4.27** & **21.64** & **38.24** & **64.66** \\   & WildChat & 6.64 & 6.25 (5.89) & 5.98 & 5.10 & 6.20 & 6.10 & 5.82 & 5.99 & 6.00 & 9.95 & 53.69 & 60.14 \\  & **M2Lingual** & 5.09 & 4.35 (4.55) & 4.35 & 3.52 & 4.47 & 4.54 & 4.69 & 4.62 & 4.46 & 7.80 & 48.74 & 59.46 \\   & **M2Lingual** & **6.47** & **6.40 (6.20)** & **6.13** & **5.35** & **6.18** & **5.94** & **5.87** & **6.17** & **6.68** & **11.95** & **54.64** & **64.66** \\   

Table 6: Evaluations of QWEN-1.8B and LLaMA-2-13B for highlighting impact on different sized LLMs.

   Model & Data & MT-Avg & XQUAD & TyDiQA & MLQA & XLSUM & MGSM & XNLI & XCOPA \\   & Aya-seeds(110.5K) & 4.59 & 71.40 & 68.00 & **57.69** & **14.08/2.45** & 15.32 & 40.77 & 57.55 \\  & Seed + Evol (110.5K) & **6.64** & **72.87** & **68.43** & 55.43 & 12.51/1.33 & **22.00** & **42.51** & **58.09** \\   

Table 7: Performance comparison of **M2Lingual** vs Aya-seeds data of same size.