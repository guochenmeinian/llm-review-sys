# Rethinking Aleatoric and Epistemic Uncertainty

Freddie Bickford Smith

University of Oxford

&Jannik Kossen

University of Oxford

&Eleanor Trollope

University of Oxford

Mark van der Wilk

University of Oxford

&Adam Foster

University of Oxford

&Tom Rainforth

University of Oxford

###### Abstract

The ideas of aleatoric and epistemic uncertainty are widely used to reason about the probabilistic predictions of machine-learning models. We identify incoherence in existing discussions of these ideas and suggest this stems from the aleatoric-epistemic view being insufficiently expressive to capture all of the distinct quantities that researchers are interested in. To explain and address this we derive a simple delineation of different model-based uncertainties and the data-generating processes associated with training and evaluation. Using this in place of the aleatoric-epistemic view could produce clearer discourse as the field moves forward.

## 1 Introduction

When making decisions under uncertainty, it can be useful to reason about where that uncertainty comes from (Osband et al, 2023; Wen et al, 2022). Researchers commonly refer to the ideas of aleatoric (literal meaning: "relating to chance") and epistemic ("relating to knowledge") uncertainty, which have a long history in the study of probability (Hacking, 1975). Aleatoric uncertainty is typically associated with statistical dispersion in data or outcomes, while epistemic uncertainty is associated with the internal information state of a model (Hullermeier and Waegeman, 2021).

Concermingly given their scale of use, these ideas are not being discussed coherently in the literature. The line between model-based predictions and data-generating processes is repeatedly blurred (Adlam et al, 2020; Amini et al, 2020; Ayhan and Berens, 2018; Collier et al, 2020; Immer et al, 2021; Kapoor et al, 2022; Liu et al, 2022; Mavor-Parker et al, 2022; Notin et al, 2021; Postels et al, 2019; Smith and Gal, 2018; van Amersfoort et al, 2020). Different mathematical quantities are used to refer to notionally the same concepts: epistemic uncertainty, for example, has received multiple definitions, including variance-based measures (Gal, 2016; Kendall and Gal, 2017; McAllister, 2016), information-based measures (Gal et al, 2017), ad-hoc reinterpretations of information-based measures (Shen et al, 2018; Siddhant and Lipton, 2018) and distance-based measures (Mukhoti et al, 2021, 2023; van Amersfoort et al, 2020). Misleading connections are drawn between predictive uncertainty and accuracy (Orlando et al, 2019; Wang et al, 2019). Tenuous assumptions are made about how predictive uncertainty will decompose on unseen data (Seebock et al, 2019; Wang and Aitchison, 2021).

We suggest this incoherence arises from the aleatoric-epistemic view being too simplistic in the context of machine learning. To make this diagnosis precise and provide an alternative to the aleatoric-epistemic view, we systematically disambiguate some key concepts that arise in machine learning. We start with a predictive task of interest, highlight that the training data need not correspond directly to the task, and contrast model-based predictions with external data-generating processes.

This exposition supports our diagnosis of the issue in the discourse. We show that, like much of the literature that followed it, the popular interpretation of aleatoric and epistemic uncertainty presented in Gal (2016), Gal et al (2017) and Kendall and Gal (2017) is itself incoherent: it attaches multiplequantities to the same concepts. This conceptual overloading stems from attempting to encompass too many ideas within an uncertainty decomposition that has a fundamentally limited expressive capacity. Its effect is to conflate concepts that ought to be recognised as distinct.

Returning to foundational ideas from Bayesian statistics, we identify an alternative decomposition of predictive uncertainty that can be cleanly linked back to quantities used in past work. We believe this could be a basis for clearer thinking in future work, helping the field more quickly achieve its goals.

## 2 Background

The use of the terms "aleatoric" and "epistemic" in machine learning follows a history of use in the engineering literature. Special issues of _Reliability Engineering and System Safety_ on "Treatment of aleatory and epistemic uncertainty" (Helton & Burmaster, 1996) and "Alternative representations of epistemic uncertainty" (Helton & Oberkampf, 2004) aggregated a considerable amount of discourse. More recent work includes that of Der Kiureghian & Ditlevsen (2009) and Helton et al (2010).

That literature itself builds on a much longer thread of work on sources of uncertainty. Helton & Oberkampf (2004) wrote that "this dual use of probability in the representation of both aleatory and epistemic uncertainty can be traced back to the beginning of the formal development of probability in the late 1600s (Bernstein, 1996; Hacking, 1975; Shafer, 1978)". Modern statistics texts referring to the ideas, even if not the exact terms, include the work of Chernoff & Moses (1959).

While the concepts of aleatoric and epistemic uncertainty had previously been used in machine learning, for example by Lawrence (2012) and Senge et al (2014), they were popularised by Gal (2016), Gal et al (2017) and Kendall & Gal (2017). The prevailing mathematical definitions are the information-theoretic quantities used by Gal et al (2017), building on earlier work on Bayesian experimental design (Lindley, 1956) and active learning (Houlsby et al, 2011; MacKay, 1992a,b).

## 3 Key concepts

Many quantities that arise in machine learning have been associated with the ideas of aleatoric and epistemic uncertainty. We set out to provide a clear synthesis of some key concepts.

Reasoning should start with the predictive task of interestWe consider prediction of \(y|x\) where \(x\) is an input and \(y\) is an output. We allow \(x==\). This setup covers a wide range of scenarios, from predicting the bias of a coin (\(=\) and \(=\)) to predicting the next word in a sentence (\(=^{l}\) and \(=\) where \(\) is a vocabulary and \(l\) is the number of words so far).

Training data need not correspond directly to the predictionTypically we have access to some training data, \(d_{1:n} p_{}(d_{1:n})\), that can inform our prediction. It is common to assume \(d_{i}\). We emphasise that this is not necessary. The data could belong in some altogether separate space.

Using a model allows data-driven predictionIn machine learning we work from training data to predictions through a model, \(p_{n}(y|x)=p(y|x;d_{1:n})\). Here we focus on parametric models. While not required for many of the quantities we consider, there can be stochastic parameters, \( p_{n}()=p(;d_{1:n})\), within the model, defined such that \(p_{n}(y|x)=_{p_{n}()}[p(y|x,)]\). We use \(p_{}(y|x)\) to denote the model we converge to as \(n\), which we assume is well defined.

Predictions are generally distinct from data-generating processesSince we can have \(d_{i}\), the link between \(p_{n}(y|x)\) and \(p_{}(d_{1:n})\) can be weak. Suppose \(d_{1:n}\) represents the outcomes of \(n\) fair coin tosses and the task is to predict the coin's bias, so \(=\) and \(=\). Then as \(n\) the predictive entropy (Shannon, 1948), \([p_{n}(y|x)]\), can tend to zero while \([p_{}(d_{1:n})]=n 2\) tends to infinity. Even if \(d_{i}\) and \(d_{i} p_{}(y|x)p_{}(x)\), it can still be the case that \(p_{n}(y|x) p_{}(y|x)\) for all \(n\) due to model misspecification (Kleijn & van der Vaart, 2006).

Reference systems allow grounded evaluationComputing model-based uncertainties is not a general substitute for evaluating a model using a reference system, such as a person, physical sensor or computer program. Often this system performs the same predictive task as the model, and we can formalise evaluation as a comparison between the model and the reference system, \(p_{}(y|x)\), on an input, \(x\). If \(x\) is considered to be drawn from some \(p_{}(x)\), this commonly gives rise to expected losses of the form \(_{p_{}(x,y)}[(p_{n}(y|x),y)]\), often estimated using sampled \((x,y) p_{}(x,y)\).

## 4 Assessing a popular view

Having established some key concepts, we can now see how they connect to the aleatoric-epistemic view as presented in Gal (2016), Gal et al (2017) and Kendall & Gal (2017). The mathematical decomposition that has prevailed over time relates three information-theoretic quantities:

\[(x)}_{}=[p_{n} (y|x)]}_{}-_{p_{n}()}[[p (y|x,)]]}_{}.\] (1)

Gal (2016) stated the "\(=\) aleatoric + epistemic" relationship and the correspondence between \(p_{n}(y|x)\) and the total uncertainty, while Gal et al (2017) made the explicit link to Equation 1, informed by Houlsby et al (2011). Kendall & Gal (2017) expanded on these ideas in a computer-vision context.

Aleatoric and epistemic uncertainty as discussed in this work refer to multiple quantities (Figure 1), introducing a number of spurious associations. The competing definitions of aleatoric uncertainty conflate a model's irreducible predictive entropy, \([p_{}(y|x)]\), with three separate quantities:

1. \([p_{}(d_{1:n})]\), the entropy of the training data. Issue: because \(d_{i}\) need not belong in \(\), the model's asymptotic predictive entropy can have little to do with \([p_{}(d_{1:n})]\).
2. \([p_{}(y|x)]\), the entropy of the reference system used in evaluation. Issue: we are not guaranteed to recover \(p_{}(y|x)\) as \(n\), for example if \(p_{}(y|x)\) is not in the model class.
3. \(_{p_{n}()}[[p(y|x,)]]\), the conditional entropy of \(y|x\) given the model parameters, \( p_{n}()\). Issue: for finite \(n\) the conditional entropy is only an estimator of \([p_{}(y|x)]\) (Proposition 1).

Meanwhile the multiple definitions of epistemic uncertainty mix up a model's reducible predictive entropy, \([p_{n}(y|x)]-[p_{}(y|x)]\), with two different quantities:

1. \([p_{n}()]\), the entropy of the model parameters. Issue: predictions are often a non-invertible function of the parameters, and the parameter entropy at finite \(n\) need not relate to \([p_{}(y|x)]\).
2. \([p_{n}(y|x)]-_{p_{n}()}[[p(y|x,)]]\), the BALD score evaluated at \(x\). Issue: for finite \(n\) the BALD score is only an estimator of \([p_{n}(y|x)]-[p_{}(y|x)]\) (Proposition 2).

Other sources of confusion in this view on aleatoric and epistemic uncertainty include an incorrect association between a model's subjective uncertainty and objective measures of performance, such as classification accuracy (Figure 2 in Kendall & Gal (2017)), along with misleading implications about how a model's uncertainty will behave with varying \(n\) (Figure 6.11-6.12 in Gal (2016) and Table 3 in Kendall & Gal (2017)) and varying distance from the training data ("Aleatoric uncertainty does not increase for out-of-data examples...whereas epistemic uncertainty does" in Kendall & Gal (2017)).

Figure 1: A popular view on aleatoric and epistemic uncertainty attaches multiple quantities to each term. Some of these quantities can coincide in particular cases but in the general case they are distinct. The quotations here are from Kendall & Gal (2017); the interpretation of Equation 1 is due to Gal (2016) and Gal et al (2017).

## 5 An alternative perspective

Now we return to the goal of decomposing predictive uncertainty. If a model's prediction is uncertain, we want to know whether that prediction is fundamentally uncertain for the given model class or instead due to a lack of data. This breakdown has clear utility if our aim is to identify new data that will reduce a model's predictive uncertainty (Bickford Smith et al, 2023, 2024). But it is also relevant elsewhere. In model selection, for example, we might want to quantify a model's scope for improvement by forecasting how it will behave after training on more data.

We formalise this using a Bayesian perspective, which corresponds to reasoning about data that has not yet been observed (Fong et al, 2023). More concretely we revisit BALD's foundations in the framework of Bayesian experimental design (Lindley, 1956; Rainforth et al, 2024) and focus on the core idea of information gain. For a generic variable of interest, \(\), the information gain is defined as the reduction in entropy in \(\) that results from observing new data, \(d_{(n+1):(n+m)}\):

\[_{}(d_{(n+1):(n+m)})=[p_{n}()]-[p_{n+m} ()].\] (2)

Setting \(\) to \(\) and averaging over possible data recovers BALD (Houlsby et al, 2011) for \(m=1\) and BatchBALD (Kirsch et al, 2019) for \(m>1\). Here we are interested in the task of predicting \(y|x\), so we instead set \(\) to \(y|x\)(Bickford Smith et al, 2023). Considering the resulting information gain in the limit of infinite new data, \(m\), gives a decomposition of a model's total predictive entropy:

\[_{y|x}(d_{(n+1):})}_{}= [p_{n}(y|x)]}_{}-[p_ {}(y|x)]}_{}.\] (3)

In practice we do not have the new data used in this definition. Instead we have to reason about what the data could be, giving rise to estimators of the irreducible and reducible predictive entropy. Only in this context of estimation does a stochastic model become necessary: the decomposition in Equation 3 is based on a Bayesian perspective but is well defined for deterministic models.

**Proposition 1**: _A model's conditional predictive entropy, \(_{p_{n}()}[[p(y|x,)]]\), is a Bayes estimator of its irreducible predictive entropy, \([p_{}(y|x)]\)._

_Proof_ Let \(h\) be an estimator of \([p_{}(y|x)]\), and let \(p_{n}()\) be our beliefs over which \(\) produces \(p(y|x,)=p_{}(y|x)\). The Bayes estimator is the minimiser of the posterior expected loss, \(L(h)=_{p_{n}()}[(h,)]\). If we use a quadratic loss, \((h,)=(h-[p(y|x,)])^{2}\), this minimiser satisfies

\[_{h}L(h)=_{p_{n}()}[2(h-[p(y|x,)])]=0.\] (4)

Solving this gives \(h=_{p_{n}()}[[p(y|x,)]]\), the conditional predictive entropy. \(\)

**Proposition 2**: _The BALD score evaluated at \(x\), \((x)=_{}(x)\), is a Bayes estimator of the reducible predictive entropy at \(x\), \(_{y|x}(d_{(n+1):})\)._

_Proof_ This follows directly from Equations 1 and 3 along with Proposition 1.

Figure 2: The total predictive uncertainty (entropy) of a model, \(p_{n}(y|x)\), trained on data \(d_{1:n}\) can be decomposed into actual irreducible and reducible components, which reflect how the model’s uncertainty changes as \(n\). Given finite \(n\), these irreducible and reducible components must be estimated. This is made possible by using stochastic model parameters, \( p_{n}()\), where \(p_{n}(y|x)=_{p_{n}()}[p(y|x,)]\).

**Proposition 3**: _The error associated with the approximation \((x)_{y|x}(d_{(n+1):})\) corresponds to that associated with \(_{p_{n}()}[[p(y|x,)]][p_{ }(y|x)]\)._

_Proof_ This again follows from Equations 1 and 3:

\[_{y|x}(d_{(n+1):})-(x)=_{p_{n}( )}[[p(y|x,)]]-[p_{}(y|x)].\] (5)

Both sides can be seen as approximation errors. \(\)

There are cases where we can expect the estimators in Propositions 1 and 2 to be accurate. We might for example have \(x,y\) and know that modelling \(y|x\) as Gaussian-distributed with equal variance across all \(x\) is appropriate, and we might even know exactly what variance to use. But in the general case these estimators can be highly inaccurate even if they are principled. Figure 2 in Bickford Smith et al (2024) demonstrates this: Bayesian deep learning can produce estimates of irreducible and reducible predictive entropy that are severely at odds with the entropy changes that occur in practice.

The conceptual map in Figure 2 combines the decomposition from Equation 3 with the estimators from Propositions 1 and 2. It thus connects Equation 3 back to Equation 1 while highlighting that the quantities in the latter should be seen not as direct measures of irreducible and reducible uncertainty but instead as estimators that might not be accurate. This new perspective gives us a clearer basis for reasoning about predictive uncertainty than is provided by the aleatoric-epistemic view.

## 6 Related work

A number of different perspectives on aleatoric and epistemic uncertainty in machine learning have been put forward in recent years. These include a discussion of where uncertainty comes from in machine learning (Gruber et al, 2023); a case against Shannon entropy for notions of predictive uncertainty (Wimmer et al, 2023); proposals for using alternative information-theoretic quantities (Schweighofer et al, 2023a,b, 2024); and various other suggestions for how to define uncertainty, such as in terms of frequentist risk (Lahlou et al, 2023), class-wise variance (Sale et al, 2023b, 2024b), credal sets (Hofman et al, 2024a; Sale et al, 2023a), distances between probability distributions (Sale et al, 2024a) and proper scoring rules (Hofman et al, 2024b). In contrast with most of that work, our approach here has been to consider the minimal changes needed to address the issues we have identified in existing discussions of aleatoric and epistemic uncertainty.

## 7 Conclusion

We have identified sources of confusion in the aleatoric-epistemic view on uncertainty in machine learning and, to deal with this, we have presented an alternative perspective. A key fact underlying this work is the extent of the subjectivity of the uncertainties we have discussed. Our presentation of the ideas makes transparent the dependence on the model class and the amount of data. But it abstracts away dependencies on other things, such as what exactly the data is and what algorithm is used to learn from the data. While we believe this abstraction is useful for the high-level understanding we have aimed to provide here, a precise analysis would need to account for these dependencies.