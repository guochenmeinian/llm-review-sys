# Top-Ambiguity Samples Matter:

Understanding Why Deep Ensemble Works in

Selective Classification

Qiang Ding\({}^{1,2}\), Yixuan Cao\({}^{1,2}\), Ping Luo\({}^{1,2,3}\)

\({}^{1}\)Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS),

Institute of Computing Technology, CAS, Beijing 100190, China

\({}^{2}\)University of Chinese Academy of Sciences, Beijing 100049, China

\({}^{3}\)Peng Cheng Laboratory, Shenzhen 518066, China

{dingqiang22z, caoyixuan, luop}@ict.ac.cn

###### Abstract

Selective classification allows a machine learning model to reject some hard inputs and thus improve the reliability of its predictions. In this area, the ensemble method is powerful in practice, but there has been no solid analysis on why the ensemble method works. Inspired by an interesting empirical result that the improvement of the ensemble largely comes from _top-ambiguity_ samples where its member models diverge, we prove that, based on some assumptions, the ensemble has a lower selective risk than the member model for any coverage within a range. The proof is nontrivial since the selective risk is a non-convex function of the model prediction. The assumptions and the theoretical results are supported by systematic experiments on both computer vision and natural language processing tasks.

## 1 Introduction

Although recent years have witnessed the broad applications of deep learning models, their reliability has not been guaranteed, which gives rise to the study of selective classification. For any given deep learning classifier, there might be inputs that the model is not able to classify correctly in practical applications. One approach to overcoming this problem is accurately delimiting the deep learning classifier's application scope so that the classifier rejects to predict over those hard inputs.

To this end, selective classification aims to learn a pair of models consisting of a standard classifier and a _selective function_ that decides whether the system should reject the input. Since the classifier is well studied, the study of selective classification focuses on the design of the selective function. A selective function is usually a confidence estimator with a threshold , rejecting the input if and only if the estimated confidence is below the threshold. There are four categories of confidence estimation methods: post-hoc methods [2; 3], Bayesian methods [4; 5; 6], learning-based methods [7; 8; 9; 10], and ensemble methods . Details about these methods is provided in Section 3.

Previous work has shown that Deep Ensemble has a good performance in selective classification 1, but no solid analysis explains why the ensemble method works. This paper first proposes a theoretical foundation of Deep Ensemble, i.e., with some assumptions, the ensemble has a lower selective risk than the member model for any target coverage within a range. The proof is nontrivial since the selective risk (with the 0/1 loss) are non-convex. We then verify our assumptions andtheoretical result on both computer vision and natural language processing tasks. In summary, the contributions of this paper are summarized as follows.

* We show an interesting empirical result that the performance improvement of Deep Ensemble in terms of selective classification dominantly comes from top-ambiguity samples.
* We are the first to prove that the ensemble has a lower selective risk than the member model for any target coverage within a range based on several reasonable assumptions. Our assumptions and analysis results are verified by systematic experiments on the tasks of image classification and text classification.

## 2 Preliminary

A selective classifier comprises a standard classifier and a selective function. Considering a standard classification problem, \(\) is a feature space, \(=\{1,2,...,K\}\) is a finite label set, and a classifier is a function the maps \(\) to \(\). A labeled dataset \(=\{(x_{i},y_{i})\}_{i=1}^{N}\) is independently sampled from an identical distribution \(\). Our goal is to learn a _selective classifier_\((f,g)\), where \(f\) is a standard classifier, and \(g:\{0,1\}\) is a selective function that estimates the correctness of \(f\)'s prediction. Given input \(x\), the output of selective classifier \((f,g)\) is

\[(f,g)(x)=f(x),&g(x)=1\\ &g(x)=0.\] (1)

The goal of selective classifiers is to minimize the _selective risk_ subject to a given target _coverage_. The _coverage_ of \((f,g)\) is defined to be the probability of \((f,g)\) not rejecting the input, i.e., \((f,g):=_{(x,y)}[g(x)]\). The _selective risk_ of \((f,g)\) is the risk of \(f\) conditioned on \(g(x)=1\), i.e., \(R(f,g):=_{(x,y)}[(f(x),y)g(x)]/_{(x,y) }[g(x)]\), where \(:\) is a given loss function. Typically, \(\) is the 0/1 loss [1; 7; 8; 9]. Based on these definitions, the objective of selective classifiers is formalized as

\[_{f,g}R(f,g)(f,g) c_{},\]

where \(c_{}\) is the target coverage. Note that the above objective is evaluated only on in-distribution samples, i.e., samples independently drawn the same distribution of the training data, excluding samples with distribution shifts -- also known as OOD samples .

The selective function is usually realized by a _confidence estimator_\(:\) with a _confidence threshold_\(t\): \(g(x)=_{(x) t}\), where \(\) is the indicator function . In this case, we denote the selective classifier as \((f,;t)\). As the threshold \(t\) controls the coverage, a fixed \((f,)\) can be fitted to different target coverages (with different confidence thresholds). Thus, \((f,)\)'s performance can be evaluated under multiple different coverages, raising metrics such as _risk-coverage curve (RC curve)_ and the _Area Under the Risk-Coverage curve (AURC)_. The _RC curve_ of \((f,)\) is a plot of \((f,)\)'s selective risks against its coverages, which depicts the entire performance profile of \((f,)\). A lower risk-coverage curve suggests a better selective classifier. The _AURC_ is a more concise and comprehensive metric. Similar to the RC curve, a lower AURC suggests a better selective classifier.

This paper focuses on analyzing the selective risk of Deep Ensemble , which is defined as follows. To obtain a Deep Ensemble \((f_{E},_{E})\), \(M\) member models are trained in parallel with random initialization of their parameters and random shuffling of the data points. Assume that each member classifier provides a predictive probability distribution \(_{m}=(_{m}^{1},...,_{m}^{K})\), \(1 m M\) and predicts \(f_{m}(x)=_{1 k K}_{m}^{k}\). Then Deep Ensemble is a uniformly-weighted soft voting of member models: \(_{E}(x):=_{m=1}^{M}_{m}(x)\), and its confidence estimator is \(_{E}(x)=_{k}_{E}^{k}(x)\).

## 3 Related Work

**Selective Classification.** A selective function usually relies on a confidence estimator. To realize the confidence estimator, four approaches has been investigated by previous work. **1. Post-hoc methods**, including Softmax Response (SR)  and Trust Score . SR leverages the classifier's maximum class probability (MCP) as the confidence score. Trust Score considers both the distance from the sample to the predicted class and the distance from the sample to another nearest class,using their ratio as the confidence score. **2. Bayesian methods.** MC-Dropout is an easy-to-use Bayesian methods , which samples the classifier's parameters through Dropout . Specifically, MC-Dropout enables the dropout layer of the classifier at test time, then runs multiple forwards passes, and finally uses the minus variance of the maximum activation of the classifier's softmax layer as the confidence score. A shortcoming of MC-Dropout is its high time cost at test time. **3. Learning-based methods.** There are two categories of learning-based methods: 1. using MCP as the confidence score and only modifying the standard classifier's loss function, including OSP , and Reg-curr ; 2. modifying both the architecture and the loss function of the classifier, including SelectiveNet , Gambler , ConfidNet , and SAT . SAT is the SOTA selective classifier in terms of non-ensemble methods, which adds an extra class to the classifier as a reject option and proposes a loss function to train the classifier and the confidence estimator simultaneously. **4. Ensemble Methods.** Deep Ensemble is a vanilla classifiers' ensemble combined by soft voting and equipped with an MCP confidence estimator , which has achieved the SOTA in uncertainty estimation. Considering the heavy computational overhead of the ensemble, [16; 17] reduced the ensemble's time consumption by ensemble distillation techniques that distill the ensemble's knowledge into an individual model with the ensemble's diversity retained.

**Analyses of Ensemble Methods.** Ensemble methods combine multiple individual models to improve machine learning models' predictive performances (see  for a review). The _error-ambiguity decomposition_ has been proposed to explain the better performance of the ensemble in regression tasks . However, for classification tasks, there is no such simple and elegant analysis, since the evaluation metrics are non-convex . Thus, the corresponding analysis for classification tasks needs additional assumptions, e.g., unbiased, uncorrelated, and identically distributed estimation errors for the posterior probability distribution [21; 22]. Nevertheless, these assumptions are impractical . In terms of metrics other than 0-1 loss,  uses the entropy of the predictive distribution as uncertainty, deriving the decomposition of data uncertainty and knowledge uncertainty of ensembles. Several other works [24; 25; 26] also discuss different forms of ensemble performance decomposition for metrics other than 0-1 loss. Despite these advances in analyzing ensembles, the analysis of the selective risk of the ensemble remains under-explored.

Other related topic of selective classification include **Out-Of-Distribution Detection** (OOD Detection) and **Calibration**, which along with selective classification are subdomains of uncertainty estimation . _OOD detection_ (also known as _open set recognition_) aims to detect test samples with semantic distribution shift (e.g., occurrence of new classes) without losing the in-distribution classification accuracy . OOD detection and selective classification share the requirement for in-distribution classification, and their difference lies in their mutually exclusive detection targets: the former detects misclassified in-distribution samples; while the latter detects samples with semantic distribution shifts. _Calibration_ tackles the problem of predicting confidence scores that are representative of the true correctness probabilities . Calibration and selective classification focus on different dimensions of uncertain estimation: calibration focuses on the overall confidence level; while selective classification focuses on the relative confidence ranking among the samples .

## 4 The Dominant Source of Performance Improvement of Deep Ensemble

Before we analyze the selective risk of Deep Ensemble, we present an experimental result that helps us to focus on the dominant source of performance improvement of Deep Ensemble. The experiment is to examine whether the performance improvement of the ensemble dominantly comes from high-ambiguity samples, where ambiguity is quantified as \(S=^{M}\|_{t}-_{E}\|^{2 }}{M-1}}\). We conduct the experiment on datasets CIFAR-10, CIFAR-100 , SVHN , MRPC , MNLI , and QNLI  (the details of the datasets and the models are provided in Section 6). The procedure of this experiment is as follows. First, we train a member model and an ensemble, and feed them test data. Second, all test samples are sorted by their ambiguities, and we define samples with the top-50% ambiguities as _high-ambiguity samples_ and the rest 50% samples as _low-ambiguity samples_2. Third, we construct _ensembling on high-ambiguity samples_, i.e., combining the member models (ensembling) on high-ambiguity samples and using a member model on low-ambiguity samples. Mathematically, denoting the median of ambiguities on the dataset as \(t\), ensembling on high-ambiguity samples induces a model \(_{E}\) that makes predictions as

\[_{E}(x)=f_{E}(x),()S(x) t;\\ f_{m}(x),(),\]

We equate this model with _ensembling on high-ambiguity samples_ in the following. Similarly, we define _ensembling on low-ambiguity samples_ as combining the member models (ensembling) on low-ambiguity samples and using a member model on high-ambiguity samples. Finally, we plot risk-coverage curves of the member model, the ensemble, ensembling on high-ambiguity samples, and ensembling on low-ambiguity samples.

Figure 1 shows the results of these risk-coverage curves. As we can see, on each dataset, the risk-coverage curve of ensembling on high-ambiguity samples almost coincides with that of the ensemble, and risk-coverage of ensembling on low-ambiguity samples almost coincides with that of the member model. This result suggests that ensembling on high-ambiguity samples provides the dominant performance improvement of the ensemble, while ensembling on low-ambiguity samples provides a minor performance improvement.

## 5 Analysis

This section aims to prove that the selective risk of Deep Ensemble is lower than its member models for any given coverage within a range. If the selective risk is a convex function of member models' predictive probability distributions, then the definition of the ensemble guarantees that the ensemble's selective risk is less than or equal to that of the member model according to Jensen's inequality. However, the selective risk is not a convex function of member models' predictive probability distributions due to the 0/1 loss. Thus, the definition of the ensemble is not sufficient to prove the superiority of the ensemble. The same problem exists in the analysis of error rate in standard classification tasks. To address this problem in classification, three _unrealistic_ assumptions: unbiased estimation error, identically distributed estimation error, and uncorrelated estimation error have been proposed . However, we develop other assumptions that are more _realistic_ to make our analysis practical.

Figure 1: The risk-coverage curves of the member model, Deep Ensemble, ensembling on high-ambiguity samples, and ensembling on low-ambiguity samples on each dataset, where top/lowest-50%-S ens. represents ensembling on high/low-ambiguity samples.

### Intuition

Our first assumption is motivated by the empirical result in Section 4. In the previous experiment, the samples are divided into two categories: low-ambiguity samples and high-ambiguity samples. The result shows that low-ambiguity samples bring little performance improvement, but high-ambiguity samples bring the dominant performance improvement of the ensemble. For the convenience of theoretical analysis, we approximate these two types of samples as follows: 1. _definite samples_ on which all member models produce the same predictive probability distribution; 2. _ambiguous samples_ on which member models produce statistically-dependent predictive probability distributions (but their joint distribution is unknown).

Based on this assumption, Figure 2 illustrates the intuition of why ensembling works. Given a target coverage, the corresponding confidence thresholds of the member model and the ensemble are shown as the dashed green line and the solid green line, respectively. The solid blue line represents the PDF of the member's (as well as the ensemble's) confidence score over definite samples, and the dashed/solid red line represents the PDF of the member/ensemble model's confidence score over ambiguous samples. Thus, in the upper subfigure, the area under the blue/red line on the right-hand side of the dashed green line is the probability of the member model _accepting_ (i.e., not rejecting) the sample given the sample being definite/ambiguous; and a similar result applies to the ensemble in the lower subfigure. In addition, we assume that the member model is not modest over ambiguous samples and over definite samples, i.e., the right-hand ends of the dashed red line and blue line are above the horizontal axis. We can prove that the ensemble prevents high confidence scores (termed as being _modest_) over ambiguous samples, i.e, the right-hand end of the solid red line falls on the horizontal axis. Thus, through ensembling, the PDF over ambiguous samples moves down from the dashed red curve to the solid red curve when the confidence score is near 1 ( as the red arrows show). On the contrary, the PDF over definite samples stays the same. Thus, if the target coverage is small so that the threshold of the member model is near 1, within accepted samples (on the right-hand side of the dashed green line), the proportion of ambiguous samples drops. The threshold of the ensemble model also drops to retain the target coverage, but the net effect of these two changes is a lower proportion of ambiguous samples (or a higher proportion of definite samples) within accepted samples. In addition, we assume that the error rate of classifiers over definite samples is lower than that over ambiguous samples. Thus, the selective risk becomes smaller through ensembling given this coverage.

In the following, we first introduce the assumptions, then we show that the ensemble is modest over ambiguous samples, which leads to an upper bound of ensembles' selective risk, and finally we prove the superiority of the ensemble model.

### Assumptions

This section follows the notations of Section 2. Assume that \((f_{*},_{*})\) is any member model of \((f_{E},_{E})\), where both \(_{*}\) and \(_{E}\) are MCP confidence estimators, whose outputs are in \([,1]\). For the convenience of notation, let \(R_{*}(t)\) and \(_{*}(t)\) denote \((f_{*},_{*};t)\)'s selective risk and coverage, respectively; and \(R_{E}(t)\) and \(_{E}(t)\) denote \((f_{E},_{E};t)\)'s selective risk and coverage, respectively, where \(t\) is a confidence threshold.

According to the motivation in Section 4, we assume that the whole data distribution comprises two types of samples: 1. _definite samples_ for which all member models provide the same predictive probability distribution; 2. _ambiguous samples_ for which member models provide dependent predictive probability distributions (but their dependency is unknown). Formally, we have

**Assumption 1**.: **(Correlation and Diversity Assumption)** Let \(a(x)\{0,1\}\) (or \(a\) for short) be a hidden variable that denotes whether the sample \(x\) is ambiguous (\(a=0/1\) indicates a definite/ambiguous sample). We assume: first, given \(a=0\), \(_{1}=_{2}==_{M}\); second, given

Figure 2: An illustration of the intuition of our analysis. The upper/lower graph shows the PDF of the member/ensemble model’s confidence score over ambiguous samples and definite samples.

\(a=1\), \( k\{1,,K\}\), \(p(_{1}^{k},_{2}^{k},,_{M}^{k}|a=1)\) is bounded, i.e., always finite, where \(p\) denotes probability density function (PDF).

Assumption 1 depicts both the correlation and the diversity of the ensemble. The existence of definite samples provides a strong correlation among ensemble members' predictions, and the existence of ambiguous samples provides both the correlation and the diversity of the member models' predictions. Furthermore, because Assumption 1 does not specify the correlation of ensemble members over ambiguous samples, this assumption is a relaxation of the uncorrelated-estimation-error assumption of [21; 22].

In addition, because ambiguous samples seem more difficult to classify than definite samples, we assume the member model's selective risk on ambiguous samples is greater than that on definite samples:

**Assumption 2**.: **(Hardness Assumption)** For any member model \((f_{*},_{*})\), we assume \(_{t 1^{-}}R_{*}(t|a=1)>_{t 1^{-}}R_{*}(t|a=0)\), where \(R_{*}(t|a)\) is the _conditional selective risk_ of \((f_{*},_{*};t)\) given \(a\), i.e., \(_{(x,y)}[_{y f_{*}(x)}|a,_{*}(x)  t]\).

Finally, we assume that the member model is _not modest_ over some ambiguous samples and some definite samples:

**Assumption 3**.: **(Confidence Assumption)** For any member model \((f_{*},_{*})\), we have \(_{t 1^{-}}p(_{*}(x)=t|a=0)>0\) and \(_{t 1^{-}}p(_{*}(x)=t|a=1)>0\).

### The Ensemble is Modest over Ambiguous Samples

According to Assumption 3, the member model is _not_ modest over ambiguous samples. However, we show that the ensemble is more modest over ambiguous samples (proof is provided in Appendix A):

**Lemma 1**.: _Let \(B:=_{k=1}^{K}_{^{M}}p(_{1}^{k},...,_{M}^{k}|a=1)\). Assumption 1 implies \( t\), \(p(_{E}(x)=t|a=1) M^{M}B(1-t)^{M-1}\). Integrating this inequality, we derive \(P(_{E}(x) t|a=1) M^{M-1}B(1-t)^{M}\)._

The first inequality in this lemma shows that the ensemble satisfies \(_{t 1^{-}}p(_{E}(x)=t|a=1)=0\), which is the opposite of a member model. In other words, the ensemble is less possible to provide high confidence scores over ambiguous samples than its member models. This result then leads to an upper bound of the ensemble's selective risk.

### An Upper Bound of the Ensemble's Selective Risk

We derive an upper bound of the ensemble's selective risk, demonstrating how members' performance and their diversity affect the ensemble's performance. According to selective risk's definition, the ensemble's selective risk can be bounded as (see Appendix A for proof)

\[R_{E}(t) R_{E}(t|a=0)+P(a=1|_{E}(x) t).\] (2)

The two terms in the right-hand side of (2) can be further reduced to member models' performance and their diversity as follows. First, given \(a=0\), we have \(f_{E}(x)=f_{*}(x)\) and \(_{E}(x)=_{*}(x)\) (due to Assumption 1), which suggests \(R_{E}(t|a=0)=R_{*}(t|a=0)\). Second, combining the second inequality in Lemma 1 with Bayes' rule, we can bound the second term as follows:

\[P(a=1|_{E}(x) t)}{(1-t)^{ M}+P(_{*}(x) t,a=0)},\] (3)

where \(=BM^{M-1} P(a=1)\). The right-hand side of (3) is negatively related to the diversity of member models. The reason of this statement is as follows. More diverse member models may result in a flatter \(p(_{1}^{k},...,_{M}^{k}|a=1)\), leading to a lower \(B\). A lower \(B\) further leads to a lower right-hand side of (3). Combining the results above, we have

**Lemma 2**.: _(Selective Risk Bound of The Ensemble) The ensemble's selective risk is bounded as_

\[R_{E}(t) R_{*}(t|a=0)+}{(1-t)^{M}+P( _{*}(x) t,a=0)}.\] (4)The first term of this upper bound is the selective risk of a member model on definite samples, and the second one is negatively related to the diversity among member models. When member models' performance on definite samples or the diversity improves, this upper bound decreases, which is a reasonable result.

### Main Result

Based on Lemma 2, we can prove that Deep Ensemble has a lower selective risk than a member model for any target coverage that is in a range (Theorem 1, see Appendix A for the proof).

**Theorem 1**.: _If Assumptions 1, 2, and 3 hold, then \(_{0}(0,1)\) such that \((0,_{0})\), \(_{E}(t_{E})=_{*}(t_{*})= R_{E}(t_{E})<R_{*}(t_{*})\)._

Furthermore, it is interesting to figure out the maximum \(_{0}\) in Theorem 1. In Appendix E, we use Lemma 2 to estimate the maximum \(_{0}\) (without accessing the ensemble) and find that it can be large, say \(>50\%\), on several datasets. When we instead have the access to the ensemble in experiments, the actual maximums of \(_{0}\) on most datasets are 1, i.e., the largest possible value of \(_{0}\). These results seem interesting, and we will intuitively explain them in the next section.

## 6 Experiments

This section verifies the assumptions and analysis results in realistic tasks, and then compares Deep Ensemble with several SOTA methods and their ensembles in image classification and text classification tasks. However, we do not verify Assumption 1 in the following because the empirical result in Section 4 has shown that Assumption 1 is a reasonable approximation that does not compromise the performance of the ensemble.

**Datasets.** Following , we used CIFAR-10, CIFAR-100, and SVHN for image classification tasks. Following , we used MRPC, MNLI, and QNLI for text classification tasks. More details of the datasets used in our experiments are described in Appendix B.

**Models.** Following , we use VGG-16  and BERT-base  as the backbones of selective classifiers for image classification and text classification, respectively. Each member model is built on basis of a backbone model, and an ensemble consists of five member models by default in experiments. More details of the backbone models and their training procedures are provided in Appendix B.

**Verification of Assumptions 2 and 3.** We use samples with ambiguity lower than \((>0)\) (termed as _low-ambiguity samples_) to represent definite samples and use samples with ambiguity greater than or equal to \(\) (termed as _high-ambiguity samples_) to represent ambiguous samples in experiments as we use definite/ambiguous samples to approximate low/high-ambiguity samples in theory. We choose \(=10^{-3}\) for datasets of image classification and \(=10^{-2}\) for datasets of text classification. Figure 3 shows a member model's conditional selective risks over low-ambiguity samples and high-ambiguity samples against a range of confidence thresholds near 1. The results show that the member model's

Figure 3: The member model’s selective risks on low-ambiguity samples (denoted as def.) and on high-ambiguity samples (denoted as amb.) given confidence thresholds near 1 on each dataset.

selective risk over low-ambiguity samples is consistently lower than that over high-ambiguity samples given confidence thresholds near 1, which verifies Assumption 2. Figure 4 shows histograms of confidence scores of the member model (as well as the ensemble) on low-ambiguity samples and on high-ambiguity samples. As we can see, the number of low-ambiguity samples and the number of high-ambiguity samples are non-zero in the top bin on each dataset, which verifies Assumption 3. In summary, Assumption 2 and 3 hold on all datasets.

**Verification of Lemma 1.** Figure 4 shows histograms of the ensemble's confidence scores on low-ambiguity samples and on high-ambiguity samples. As we can see, the ensemble's confidence score over high-ambiguity samples has little distribution in the top bin, which is tremendously lower than the corresponding distribution of the member model, while the histogram of confidence score over low-ambiguity samples has almost no change throughout ensembling. This result is consistent with Lemma 1 and our intuition in Figure 2.

**Verification of Theorem 1.** Figure 1 already shows the risk-coverage curves of Deep Ensemble and a member model on each dataset. As we can see, on each dataset except MRPC, the ensemble has a consistently lower selective risk than the member model under any coverage; on MRPC, the ensemble has a lower selective risk than the member model except for coverage of around 30% and around 90%3. These results are consistent with Theorem 1.

Despite of this consistency, the theorem does not explain the lower selective risk of the ensemble given a large coverage, say 70%. This is somehow resolved by the analysis in Appendix E. Here, instead of

Figure 4: The histograms of the member model’s confidence scores and the ensemble’s confidence scores on low-ambiguity samples (denoted as def.) and on high-ambiguity samples (denoted as amb.) for each dataset.

diving into theory, we intuitively explain the experimental results as follows. As Lemma 1 claims, \(P(\,t|)\) is \(O(1-t)^{M}(t 1^{-})\), where \(M\) is the number of ensemble members. Therefore, the ensemble hardly provides an ambiguous sample with confidence close to 1. On the contrast, definite samples are always assigned confidence scores close to 1, as the the experimental results show in Figure 4. By this mean, the ensemble stratifies the definite samples and ambiguous sample by their confidence scores, where the definite samples reside on a thin higher layer of confidence than the ambiguous samples (see the rightmost black-edged bars vs. red-edged bars in Figure 4). Combined with the low risk (of both the member model and the ensemble) on definite samples, this stratification lowers the selective risk when the coverage is around the proportion of definite samples in the dataset. Considering the large amount of definite samples (see the heights of black-edged bars in Figure 4), the ensemble model will exhibit lower selective risk than the member model given a considerably large coverage.

In the explanation above, the key factor that leads to the lower selective risk of the ensemble given a large coverage is the experimental fact that the definite samples are large-amounted and always assigned confidence close to 1. This fact is not involved as an assumption in the theory since it seems a very strong assumption (though it holds throughout our experiments). We guess this is attributed to the low bias of DNNs (from a bias-variance perspective), which might be a widespread property of DNNs. Therefore, it would be an interesting direction for future work to strengthen our theory by exploiting this fact.

**Extended Experimental Settings.** To further validate our analysis, we extended the experimental settings on three dimensions: 1. model architecture (to ResNet , AlexNet , and DenseNet); 2. dataset (to ImageNet100, a subset of ImageNet ); 3. number of member models in the ensemble (to 20).

The results are shown in Figure 5. As the figure shows, Assumptions 1, 2, and 3 are consistently verified across various experiment settings. The results indicate that our assumptions might reflect the general characteristics of DNNs. For example, Assumptions 2 and 3 might be attributed to the low bias of DNNs (from a bias-variance perspective) due to DNNs' large model capacity.

These results were unknown when we established our analysis. However, all of them reproduce similar results to the previous results in new settings, further confirming our analysis in practical settings.

## 7 Discussion and Conclusion

We prove that under some assumptions, Deep Ensemble has a lower selective risk than the member model for any target coverage within a range. Although the metrics of selective classification are non-convex, we complete the proof with the help of several assumptions motivated by empirical observations, e.g., the performance improvement of the ensemble dominantly comes from high-ambiguity samples. The assumptions and analysis results are well supported by the experimental results on multiple datasets of image classification tasks and text classification tasks.

Our analysis may benefit the analysis of the Deep Ensemble for standard classification. The previous analyses of the randomization-based ensemble for standard classification require some impractical assumptions . On the contrary, this paper's assumptions are a good approximation of practical settings (see Section 6). Moreover, the standard classification is a subset of selective classification, i.e., selective classification with coverage of 1 (not covered by our analysis result). Thus, our analysis may motivate the analysis of the Deep Ensemble for standard classification in practical settings.

Other possible directions for future work include: 1. extending the \((0,_{0})\) mentioned in Theorem 1 to \((0,1]\), i.e., proving that the ensemble has a lower selective risk than the member model under _any_ coverage; 2. adapting the analysis to other selective classifier ensembles, e.g., the ensemble of SAT; 3. relaxing the assumptions to generalize the application scope of our analysis.