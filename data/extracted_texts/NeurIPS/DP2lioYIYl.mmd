# A Theory of Unsupervised Translation

Motivated by Understanding Animal Communication

 Shafi Goldwasser

UC Berkeley & Project CETI

shafi.goldwasser@berkeley.edu

&David F. Gruber

Project CETI

david@projectceti.org

&Adam Tauman Kalai

Microsoft Research & Project CETI

adam@kal.ai

&Orr Paradise

UC Berkeley & Project CETI

orrp@eecs.berkeley.edu

Authors listed alphabetically.

###### Abstract

Neural networks are capable of translating between languages--in some cases even between two languages where there is little or no access to parallel translations, in what is known as Unsupervised Machine Translation (UMT). Given this progress, it is intriguing to ask whether machine learning tools can ultimately enable understanding animal communication, particularly that of highly intelligent animals. We propose a theoretical framework for analyzing UMT when no parallel translations are available and when it cannot be assumed that the source and target corpora address related subject domains or posses similar linguistic structure. We exemplify this theory with two stylized models of language, for which our framework provides bounds on necessary sample complexity; the bounds are formally proven and experimentally verified on synthetic data. These bounds show that the error rates are inversely related to the language complexity and amount of common ground. This suggests that unsupervised translation of animal communication may be feasible if the communication system is sufficiently complex.

## 1 Introduction

Recent interest in translating animal communication [2; 3; 9] has been motivated by breakthrough performance of Language Models (LMs). Empirical work has succeeded in unsupervised translation between human-language pairs such as English-French [23; 5] and programming languages such as Python-Java . Key to this feasibility seems to be the fact that language statistics, captured by a LM (a probability distribution over text), encapsulate more than just grammar. For example, even though both are grammatically correct, _The call nursed from its mother_ is more than 1,000 times more likely than _The call nursed from its father.2_

Given this remarkable progress, it is natural to ask whether it is possible to collect and analyze animal communication data, aiming towards translating animal communication to a human language description. This is particularly interesting when the source language may be of highly social and intelligent animals, such as whales, and the target language is a human language, such as English.

**Challenges.** The first and most basic challenge is _understanding the goal_, a question with a rich history of philosophical debate . To define the goal, we consider a hypothetical ground-truth translator. As a thought experiment, consider a "mermaid" fluent in English and the source language(e.g. sperm whale communication). Such a mermaid could translate whale vocalizations that English naturally expresses. An immediate worry arises: what about communications that the source language may have about topics for which English has no specific words? For example, sperm whales have a sonar sense which they use to perform echolocation. In that case, lacking a better alternative, the mermaid may translate such a conversation as _(something about echolocation)_.3

Thus, we formally define the goal to be to achieve translations similar to those that would be output by a hypothetical ground-truth translator. While this does not guarantee functional utility, it brings the general task of unsupervised translation and the specific task of understanding animal communication into the familiar territory of supervised translation, where one can use existing error metrics to define (hypothetical) error rates.

The second challenge is that animal communication is unlikely to share much, if any, linguistic structure with human languages. Indeed, our theory will make no assumption on the source language other than that it is presented in a textual format. That said, one of our instantiations of the general theory (the knowledge graph) shows that translation is easier between compositional languages.

The third challenge is _domain gap_, i.e., that ideal translations of animal communications into English would be semantically different from existing English text, and we have no precise prior model of this semantic content. (In contrast, the distribution of English translations of French text would resemble the distribution of English text.) Simply put: whales do not "talk" about smartphones. Instead, we assume the existence of a _broad prior_ that models plausible English translations of animal communication. LMs assign likelihood to an input text based not only on grammatical correctness, but also on agreement with the training data. In particular, LMs trained on massive and diverse data, including some capturing facts about animals, may be able to reason about the plausibility of a candidate translation. See Figure 1 and the discussion in Appendix H.

### Framework and results

A translator4 is a function \(f\) that translates source text \(x\) into the target language \(f(x)\). We focus on the easier-to-analyze case of _lossless_ translation, where \(f\) is invertible (one-to-one) denoted by \(f_{}:\). See Appendix I.1 for an extension to lossy translation.

We will consider a parameterized family of translators \(\{f_{}\}_{}\), with the goal being to learn the parameters \(\) of the most accurate translator. Accuracy (defined shortly) is measured with respect to a hypothetical _ground-truth_ translator denoted by \(f_{}:\). We make a _realizability_ assumption that the ground-truth translator can be represented in our family, i.e., \(\).

The source language is defined as a distribution \(\) over \(x\), where \((x)\) is the likelihood that text \(x\) occurs in the source language. The error of a model \(\) will be measured in terms of \(()_{x}[f_{}(x)  f_{}(x)]\), or at times a general bounded loss function \(()\). Given \(\) and \(f_{}\), it will be useful to consider the _translated language distribution_\(\) over \(\) by taking \(f_{}(x)\) for \(x\).

In the case of similar source and target domains, one may assume that the target language distribution \(\) over \(\) is close to \(\). This is a common intuition given for the "magic" behind why UMT sometimes

Figure 1: LMs identify incoherent text. The probabilities of three two-paragraph texts computed using the GPT-3 API. The probabilities of just the first paragraphs \(A_{1},B_{1},C_{1}\) are also shown. Although \(p(A_{1}) p(B_{1})\) and the second paragraphs of \(A\) and \(B\) are identical, overall \(p(A) p(B)\) due to coherence between the paragraphs. \(C\) is gibberish.

works: for complex asymmetric distributions, there may a nearly unique transformation in \(\{f_{}\}_{}\) that maps \(\) to something close \(\) (namely \(f_{}\) which maps \(\) to \(\)). A common approach in UMT is to embed source and target text as high-dimensional vectors and learn a _low-complexity_ transformation, e.g., a rotation between these Euclidean spaces. Similarly, translator complexity will also play an important role in our analysis.

Priors.Rather than assuming that the target distribution \(\) is similar to the translated distribution \(\), we will instead assume access to a broad prior \(\) over \(\) meant to capture how plausible a translation \(y\) is, with larger \((y)\) indicating more natural and plausible translation. Appendix H discusses one way a prior oracle can be created, starting with an \(\) learned from many examples in the target domain, and combined with a prompt, in the target language, describing the source domain.

We define the problem of _unsupervised machine translation (with a prior)_ to be finding an accurate \(\) given \(m\) iid unlabeled source texts \(x_{1},,x_{m}\) and oracle access to prior \(\).

Mle.Our focus is on the Maximum-Likelihood Estimator (\(\)), which selects model parameters \(\) that (approximately) maximize the likelihood of translations \(_{i}f_{}(x_{i})\).

**Definition 1.1** (\(\)).: _Given input a translator family \(\{f_{}\}_{}\), samples \(x_{1},,x_{m}\) and a distribution \(\) over \(\), the \(\) outputs_

\[^{}(x_{1},x_{2},,x_{m})*{argmin}_{ }_{i=1}^{m}(x_{i}))}\]

_If multiple \(\) have equal empirical loss, it breaks ties, say, lexicographically._

We note that heuristics for MLE have proven extremely successful in training the breakthrough LMs, even though MLE optimization is intractable in the worst case.

Next, we analyze the efficacy of MLE in two complementary models of language: one that is highly structured (requiring compositional language) and one that is completely unstructured. These analyses both make strong assumptions on the target language, but make few assumptions about the source language itself. In both cases, the source distributions are uniform over subsets of \(\), which (informally) is the "difficult" case for UMT as the learner cannot benefit from similarity in text frequency across languages. Both models are parameterized by the amount of "common ground" between the source language and the prior, and both are randomized. Note that these models are _not_ intended to accurately capture natural language. Rather, they illustrate how our theory can be used to study the effect of language similarity and complexity on data requirements for UMT.

Knowledge graph model.Our first model consists of a pair of related _knowledge graphs_, in which edges encode knowledge of binary relations. Each edge yields a text that described the knowledge it encodes. For example, in Figure 2 edges encode which animal \(A\) eats which other animal \(B\), and text is derived as a simple description \(A\)_eats_\(B\).5

Formally, there are two Erdos-Renyi random digraphs. The target graph is assumed to contain \(n\) nodes, while the source graph has \(r n\) nodes corresponding to an (unknown) subset of the \(n\) target nodes. The model has two parameters: the average degree \(d\) in the (randomly-generated) target language graph, and the agreement \((0,1]\) between the source and the target graphs. Here \(=1\) is complete agreement on edges in the subgraph, while \(=0\) is complete independence. We assume the languages use a _compositional_ encoding for edges, meaning that they encode a given edge by encoding both nodes, so we may consider only \(||=n!/(n-r)!\) translators consistently mapping the \(r\) source nodes to the \(n\) target nodes, which is many fewer than the number of functions \(f:\) mapping the \(||=O(r^{2})\) source edges into the \(=O(n^{2})\) target edges. Human languages as well as the communication systems of several animals are known to be compositional .6

We analyze how the error of the learned translator depends on this "common knowledge" \(\):

**Theorem 1.2** (Theorem 3.2, simplified).: _Consider a source language \(\) and a prior \(\) generated by the knowledge graph model over \(r\) source nodes, \(n\) target nodes, average degree \(d\) and agreement parameter \(\). Then, with at least \(99\%\) probability, when given \(m\) source sentences \(x_{1},,x_{m}\) and access to a prior \(\), MLE outputs a translator \(\) with error_

\[() O(d}+ {}}).\]

The second term decreases to 0 at a \(O(m^{-1/2})\) rate, similar to (noisy) generalization bounds . Note that the first term does not decrease with the number of samples. The average degree \(d\) is a rough model of language complexity capturing per-node knowledge, while the agreement parameter \(\) captures the amount of common ground. Thus, more complex languages can be translated (within a given error rate) with less common ground. Even with \(m=\) source data, there could still be errors in the mapping. For instance, there could be multiple triangles in the source and target graphs that lead to ambiguities. However, for complex knowledge relations (degree \(d 1/^{2}\)), there will be few such ambiguities.

Figure 2 illustrates an example of four English sentences and three sentences (corresponding to an unknown subset of three of the English sentences) in Welsh. For UMT, one might hypothesize that _buytaodd_ means _eat_ because they both appear in every sentence. One might predict that _siarc_ means _shark_ because the word _siarc_ appears twice in a single Welsh sentence and only the word shark appears twice in an English sentence. Next, note that _eog_ may mean _salmon_ because they are the only other words occurring with _siarc_ and _shark_. Similar logic suggests that _benmog_ means _herring_. Furthermore, the word order is consistently permuted, with subject-verb-object in English and verb-subject-object in Welsh. This translation is indeed roughly correct. This information is encoded in the directed graphs as shown, where each node corresponds to an animal species and an edge between two nodes is present if the one species eats the other.

"Common nonsense" model.The second model, the _common nonsense_ model, assumes no linguistic structure on the source language. Here, we set out to capture the fact that the translated language \(= f_{}\) and the prior \(\) share some common ground through the fact that the laws of nature may exclude common "nonsense" outside both distributions' support.

Earlier work has justified _alignment_ for UMT under the intuition that the target language distribution \(\) is approximated by a nearly unique simple transformation, e.g., a rotation, of the source distribution \(\). However, for a prior \(\), our work suggests that UMT may also be possible if there is nearly a unique simple transformation that maps \(\) so that it is _contained_ in \(\). Figure 3 illustrates such a nearly unique rotation--the UMT "puzzle" of finding a transformation \(f_{}\) of \(\) which is contained within \(\) is subtly different from finding an alignment.

In the common nonsense model, \(\) and \(\) are uniform over arbitrary sets \(T P\) from which a common \((0,1/2]\) fraction of text is removed (hence the name "common nonsense"). Specifically, \(\) and \(\) are defined to be uniform over \(=T S\), \(=P S\), respectively, for a set \(S\) sampled by including each \(y\) with probability \(\).

We analyze the error of the learned translator as a function of the amount of common nonsense:

**Theorem 1.3** (Theorem 3.4, simplified).: _Consider source language \(\) and a prior \(\) generated by the common nonsense model over \(|T|\) source texts and common-nonsense parameter \(\), and a translator family parameterized by \(||\). Then, with at least \(99\%\) probability, when given \(m\) source sentences \(x_{1},,x_{m}\) and access to a prior \(\), MLE outputs a translator \(\) with error_

\[_{x X}[f_{ }(x) f_{}(x)]=O\!().\]

Figure 2: An illustration of the knowledge graph model. In this example, the Welsh graph is an exact subgraph of the English knowledge graph, but our model allows for differences.

Theorem 3.5 gives a nearly matching lower bound. Let us unpack the relevant quantities. First, we think of \(\) as measuring the amount of agreement or common ground required, which might be a small constant. Second, note that \(|T|\) is a coarse measure of the complexity of the source language, which requires a total of \((|T|)\) bits to encode. Thus, the bound suggests that accurate UMT requires the translator to be simple, with a description length that is an \(\)-factor of the language description length, and again \(\) captures the agreement between \(,\). Thus, even with limited common ground, one may be able to translate from a source language that is sufficiently complex. Third, for simplicity, we require \(,\{0,1\}^{*}\) to be finite sets of binary strings, so WLOG \(\) may be also assumed to be finite. Thus, \(_{2}||\) is the _description length_, a coarse but useful complexity measure that equals the number of bits required to describe any model. (Neural network parameters can be encoded using a constant number of bits per parameter.) Appendix I.2 discusses how this can be generalized to continuous parameters.

Importantly, we note that (supervised) neural machine translators typically use far fewer parameters than LMs.7 To see why, consider the example of the nursing calf (page 1) and the fact that a translator needs not know that calves nurse from mothers. On the other hand, such knowledge is essential to generate realistic text. Similarly, generating realistic text requires maintaining coherence between paragraphs, while translation can often be done at the paragraph level.

As a warm-up, we include a simplified version of the common nonsense model, called the _tree-based model_ (Appendix B.1), in which texts are constructed word-by-word based on a tree structure.

Comparison to supervised classification.Consider the dependency on \(m\), the number of training examples. Note that the classic Occam bound \(O||\) is what one gets for noiseless supervised classification, that is, when one is also given labels \(y_{i}=f_{}(x_{i})\) at training time, which is similar to Theorem 1.3, and give \((m^{-1/2})\) bounds for noisy classification as in Theorem 1.2. Furthermore, these bounds apply to translation, which can be viewed as a special case of classification with _many_ classes \(\). Thus, in both cases, the data dependency on \(m\) is quite similar to that of classification.

Experiments.We validate our theorems generating synthetic data from randomly-generated languages according to each model, and evaluating translator error as a function of the number of samples and amount of common ground. The knowledge graph model (Figure 4, left) is used to generate a source graph (language) on \(r=9\) nodes to a target graph (language) on \(n=10\) nodes and average degree \(d 5\), while varying the agreement parameter \(\). We also vary \(r\) (Figure 4, right) supporting our main message: more complex languages can be translated more accurately. For the common nonsense model (Figure 5) we simulate translation of a source language of size \(|T|=10^{5}\) while varying the fraction of common nonsense \(\). Appendix E contains details and code.

Figure 3: The previous intuition behind UMT has the distributions of target language \(\) (middle) close to ground-truth translations \(\), which is assumed to be a low-complexity transformation (in this example a rotation) of the source language \(\) (left). When source and target are not aligned, restricting to _prior_\(\) region (right) allows for translation, as long as there are enough “nonsense” texts (black regions) so that there is a nearly unique rotation of \(\) that is contained in \(\). For example, both distributions may assign negligible probability to nonsensical texts such as _l died 3 times tomorrow_. (In this toy example, \(\) is uniform over a two-dimensional shape that happens to look like a whale.)

Contributions.The first contribution of this work is formalizing and analyzing a model of UMT. As an initial work, its value is in the opportunities which it opens for further work more than the finality and tightness/generality of its bounds. Our model applies even to low-resource source languages with massive domain gap and linguistic distance. We emphasize that this work is only _a first step_ in the theoretical analysis of UMT (indeed, there is little theoretical work on machine translation in general). Second, we exhibit two simple complementary models for which we prove that: (a) more complex languages require less common ground, and (b) data requirements may not be significantly greater than those of supervised translation (which tends to use less data than training a large LM). These findings may have implications for the quantity and type of communication data that is collected for deciphering animal communication and for UMT more generally. They also give theoretical evidence that UMT can be successful and worth pursuing, in lieu of parallel (supervised) data, in the case of sufficiently complex languages. All of that said, we note that our sample complexity bounds are information theoretic, that is, they do not account for the computational complexity of optimizing the translator. Finally, animal communication aside, to the best of our knowledge this work is the first theoretical treatment of UMT, and may also shed light on translation between human languages.

Organization.The framework is formally described in Section 2, and is instantiated with models of language in Section 3 (proofs, experiments, and other details deferred to Appendices A to E). Key takeaways from this work are presented in Section 4. Related and future work is discussed in Appendices F and G. We illustrate how prompting LMs may give priors in Appendix H. Appendix I sketches

Figure 4: Knowledge Graph model experiments, each run on twenty seeds with standard errors shown. Left: error of the top-scoring translator vs. number of source samples \(m\). Right: effect of source language complexity (number of source nodes \(r\)) on translator accuracy in the knowledge graph model. We report the accuracy of the top-scoring translator after all source edges were input to the learning algorithm, i.e., as the number of samples \(m\).

Figure 5: Common Nonsense model. The X-axis is the number of source samples \(m\), and the Y-axis is the average error among plausible translators (that have not been ruled-out so far). Each experiment was run on five seeds, with standard error depicted by the shaded area.

a generalization of our framework to the settings of lossy translation and infinite translator families. Lastly, Appendix J proves sample complexity bounds for the settings of supervised translation.

## 2 The General Framework

We use \(f\) to denote a 1-1 function, in which \(f(x) f(x^{})\) for all \(x x^{}\). For \(S\), we write \(f(S)\{f(x) x S\}\). The indicator \(_{P}\) is 1 if the predicate \(P\) holds, and 0 otherwise. The uniform distribution over a set \(S\) is denoted by \((S)\), and \(=_{2}\) denotes base-2 logarithm.

Language and Prior.A _source language_ is a distribution \(\) over a set of possible texts \(\). Similarly, a _target language_ is a distribution \(\) over a set of possible texts \(\). When clear from context, we associate each language with its corresponding set of possible texts. A _prior_ distribution \(\) over translations \(\) aims to predict the probability of observing each translation. One could naively take \(=\), but Appendix H describes how better priors can focus on the domain of interest. Intuitively, \((y)\) measures how "plausible" a translation \(y\) is. For simplicity, we assume that \(,\{0,1\}^{*}\) are finite, non-empty sets of binary strings. Appendix I.2 discusses extensions to infinite sets.

Translators.A _translator_ is a mapping \(f\). There is a known set of 1-1 functions \(\{f_{}\}\) with _parameter_ set \(\). Since parameters are assumed to be known and fixed, we will omit them from the theorem statements and algorithm inputs, for ease of presentation. Like \(,\), the set \(\) is assumed to be finite. Appendix I.1 considers translators that are not 1-1.

Divergence.A translator \(f_{}\) and a distribution \(\) induce a distribution over \(y=f_{}(x)\), which we denote by \(f_{}\). The _divergence_ between this distribution and \(\) is quantified using the Kullback-Leibler (KL) divergence,

\[()(f_{})= *{}_{x}[f_{ }(x)}]=_{x}(x)f_{ }(x)} 0.\]

Note that since \(()=*{}_{x}- (f_{}(x_{i}))-()\), and \(()\) is a constant independent of \(\), the MLE of Definition 1.1 approximately minimizes divergence.

Ground truth.In order to define semantic loss, we consider a _ground-truth translator_\(f_{}\) for some \(\). We can then define the (ground-truth) _translated language_\(=f_{}\) over \(\), obtained by taking \(f_{}(x)\) for \(x\). This is similar to the standard realizability assumption, and some of our bounds resemble Occam bounds with training labels \(y_{i}=f_{}(x_{i})\). Of course, the ground-truth translator \(\) is _not_ known to the unsupervised learning algorithm. In our setting, we further require that ground-truth translations never have 0 probability under \(\):

**Definition 2.1** (Realizable prior).: \(_{x}[(f_{}(x))=0]=0\)_, or equivalently \(()<\)._

Semantic loss.The semantic loss of a translator is defined with respect to a _semantic difference_ function \(\). This function, unknown to the learner, measures the difference between two texts from the target language \(\), with \((y,y)=0\) for all \(y\). For a given semantic difference \(\) and ground-truth translator \(f_{}\), we define the _semantic loss_ of a translator \(f_{}\) by

\[()*{}_{x} (f_{}(x),f_{}(x)).\]

Of particular interest to us is the _semantic error_\((,)\), obtained when \(\) is taken to be the 0-1 difference \(_{01}=(y,y^{})=1\) for all \(y^{} y\). Note that since any semantic difference \(\) is upper bounded by \(1\), the semantic error upper-bounds any other semantic loss \(\). That is,

\[()()_{x}[f_{} (x) f_{}(x)].\]

Section 3 analyzes this error \(()\), which thus directly implies bounds on \(()\).

Models of Language: Instantiating the General Framework

### Random knowledge graphs

In this section, we define a model in which each text represents an edge between a pair of nodes in a knowledge graph. Both languages have knowledge graphs, with the source language weakly agreeing with an unknown subgraph of the target language.

We fix \(=X X=X^{2}\) and \(=Y Y=Y^{2}\) with \(r|X|\) and \(n|Y|\). The set of translators considered is all mappings from the \(r\) source nodes to the \(n\) target nodes, namely \(_{XY}=\{:X Y\}\) and \(f_{}(u,v)(u),(v)\). The random knowledge graph is parametrized by the number of source nodes \(r\), target node set \(\), an edge density parameter \(p(0,1)\) representing the expected fraction of edges present in each graph, and an agreement parameter \((0,1]\) representing the correlation between these edges. In particular, \(=1\) corresponds to the case where both graphs agree on all edges, and \(=0\) corresponds to the case where edges in the graphs are completely independent. These parameters are unknown to the learner, who only knows \(X\) and \(Y\) (and thus \(=X^{2},=Y^{2}\)).

**Definition 3.1** (Random knowledge graph).: _For a natural number \(r|Y|\), the \(=(Y,r,p,)\) model determines a distribution over sets \(T,P\) (which determine distributions \(\) and \(\)). The sets \(T\) and \(P\) are sampled as follows:_

1. _Set_ \(P\) _is chosen by including each edge_ \(y\) _with probability_ \(p\)_, independently._
2. _Set_ \(S Y\) _of size_ \(|S|=r\) _is chosen uniformly at random._
3. _Set_ \(T S^{2}\) _is chosen as follows. For each edge_ \(y S^{2}\)_, independently,_ 1. _With probability_ \(\)_,_ \(y T\) _if and only if_ \(y P\)_._ 2. _With probability_ \(1-\)_, loss another_ \(p\)_-biased coin and add_ \(y\) _to_ \(T\) _if it lands on "heads"; that is,_ \(y T\) _with probability_ \(p\)_, independently._

It is easy to see that \(T S^{2}\) and \(P Y^{2}\) marginally represent the edges of Erdos-Renyi random graphs \(G_{r,p}\) and \(G_{n,p}\), respectively. Moreover, the event that \(y T\) is positively correlated with \(y P\): for each \(y S^{2}\), since with probability \(>0\) they are identical and otherwise they are independent. Formally, the equations below describe the probability of \(y T\) for each \(y S^{2}\) after we fix \(S\) and choosing \(T S^{2}\). Letting \(q(1-p)\), for each \(y S^{2}\):

\[[y T] =[y P]=p\] (1) \[[y T P] =[y P T]=(1-)pq\] (2) \[[y P y T] =[y T y P]==(1-)q\] (3)

The last equality, shows that the probability of excluding a random \(y T\) from \(P\) is smaller than the probability of excluding a random "incorrect translation" \(y^{} y\), \([y^{} P]=q>(1-)q\).

We now describe how \(,,\) are determined from \(T,P\) and how \(,\) may be chosen to complete the model description. The ground-truth target translated distribution \((T)\) is uniform over \(T\). The prior \(\) is uniform over \(P\), and then "smoothed" over the rest of the domain \(\). Formally,

\[(y)(+|})&y P\\ |}&y P.\]

The ground-truth translator \(\) is obtained by sampling a uniformly random \(:X S\). Lastly, we take \(=(f_{}^{-1}(T))\), which agrees with the definition of \(\).8

Next, we state the main theorem for this model, formalizing Theorem 1.2 from the introduction.

**Theorem 3.2** (Translatability in the \(\) model).: _Fix any \(m 1\), \( S Y,,,p(0,1)\), and let \(r|S|\), \(n|Y|\), \(q=1-p\). Then, with probability \( 1-\) over \(T,P\) from \((S,Y,p,)\),_

\[(pq^{2} r^{2}}}{},}{ }}),\]

_where \(=^{}(x_{1},x_{2},,x_{m})\) is from Definition 1.1. Simply, for \(p<0.99\), with probability \( 0.99\),_

\[()=O\!(pr}+ }).\]

The proof, given in Appendix D, requires generalizing our theory to priors that have full support. Experimental validation of the theorem is described in Appendix E.

### Common nonsense model

We next perform a "smoothed analysis" of arbitrary LMs \(,\) that are uniform over sets that share a small amount of randomness, i.e., a small common random set has been removed from both. This shared randomness captures the fact that some texts are implausible in both languages and that this set has some complex structure determined by the laws of nature, which we model as random.

The \(\)-common-nonsense distribution is a meta-distribution over pairs \((,)\) which themselves are uniform distributions over perturbed versions of \(P,T\). This is inspired by Smoothed Analysis . Recall that \((S)\) denotes the uniform distribution over the set \(S\).

**Definition 3.3** (Common nonsense).: _The \(\)-common-nonsense distribution \(_{}^{P,T}\) with respect to nonempty sets \(T P\) is the distribution over \(=(P S),=(T S)\) where \(S\) is formed by removing each \(y\) with probability \(\), independently.9_

To make this concrete in terms of a distribution \(\) on \(\), for any ground-truth translator \(f_{}:\), we similarly define a distribution \(_{,}^{P,T}\) over \((,)\) where \((f_{}^{-1}(T S))\) is the uniform distribution over the subset of \(\) that translates into \(\). We now state the formal version of Theorem 1.3.

**Theorem 3.4** (Translatability in the \(\) model).: _Let \(\{f_{}:\}\) a family of translators, \(\), \(,(0,1/2]\), \(T P\), and \(m 1\). Then with probability \( 1-\), MLE run on \(\) and \(m 1\) iid samples from \(\) outputs \(\) with,_

\[()(,).\]

_Note that the probability is over both \((,)\) drawn from \(_{,}^{P,T}\), and the \(m\) iid samples from \(\). More simply, with probability \( 0.99\),_

\[()=O\!( ).\]

When the amount of shared randomness \(\) is a constant, then this decreases asymptotically like the bound of supervised translation (Theorem J.1) up until a constant, similar to Theorem 3.2. For very large \(m\), each extra bit describing the translator (increase by 1 in \(||\)) amounts to a constant number of mistranslated \(x\)'s out of all \(\). The proof is deferred to Appendix C.

We also prove the following lower-bound that is off by a constant factor of the upper bound.

**Theorem 3.5** (\(\) lower-bound).: _There exists constants \(c_{1},c_{2} 1\) such that: for any set \(T\), for any \(m 1\), any \((0,1/2]\), and any \(\) with \(c_{1}||(m,|T|)\), there exists \(^{}\) of size\(|^{}|||\) such that, for any \(P T\) and any algorithm \(A^{}:^{m}^{}\), with probability \( 0.99\) over \((^{})\) and \((,)\) drawn from \(^{P,T}_{,}\) and \(x_{1},,x_{m}\),_

\[ (m,|T|)},\]

_where \(=A^{}(x_{1},x_{2},,x_{m})\)._

The only purpose of \(\) in the above theorem is to upper-bound the description length of translators, as we replace it with an entirely different (possibly _smaller_) translator family \(^{}\) that still has the lower bound using \(|||^{}|\). Since \((^{})\) is the uniform distribution over \(^{}\), the ground-truth classifier is uniformly random from \(^{}\). A requirement of the form \(||=O(m,|T|)\) is inherent as otherwise one would have an impossible right-hand side error lower-bound greater than 1, though the constants could be improved.

The proof of this theorem is given in Appendix C.2, and creates a model with \(O( n)\) independent "ambiguities" that cannot be resolved, with high probability over \(S,x_{1},x_{2},,x_{m}\). Experimental validation of the theorem is described in Appendix E.

## 4 Discussion

We have given a framework for unsupervised translation and instantiated it in two stylized models. Roughly speaking, in both models, the error rate is inversely related to the amount of samples, common ground, and the language complexity. The first two relations are intuitive, while the last is perhaps more surprising. All error bounds were _information-theoretic_, meaning that they guarantee a learnable accurate translator, but learning this translator might be computationally intensive.

In both models, the translators are _restricted_. In the knowledge graph, the translators must operate node-by-node following an assumed compositional language structure.10 In the common nonsense model, the restriction is based on the translator description bit length \(||\). To illustrate how such restrictions can be helpful, consider _block-by-block_ translators which operate on limited contexts (e.g., by paragraph). Consider again the hypothetical example of Figure 1. Suppose the three texts are outputs of three translators \(=\{A,B,C\}\). Let us suppose that translator A always produces accurate and natural translations, and further that all translators work paragraph-by-paragraph, as modern translation algorithms operate within some limited context window. In fact, one can imagine the translators of different paragraphs as a set of _isolated adversaries_ where each adversary is trying to mistranslate a paragraph, knowing the ground-truth translation of their paragraph, while attempting to maintain the plausibility of the entire translation. If only the first-paragraph adversary mistranslates _regt_ to _ocean basin_, then the translation lacks coherence and is unlikely. If the adversaries are in cahoots and coordinate to all translate _regt_ to _ocean basin_, they would generate: _Have you seen mom? I just returned from the ocean basin. At the basin, there were a lot of sea turtles._ which has low probability \( 10^{-25}\), presumably because encoded in GPT-3's training data is the knowledge that there are no turtles deep in the ocean near the basin. While the adversary could also decide to change the word _turtle_ to something else when it appears near _basin_, eventually it would get caught in its "web of deceit." The intuition is that, across sufficiently many translations, the prior will not "rule out" the ground-truth translations while very incorrect translators will be ruled out.

Judging success.Our analysis sheds some light on whether it is even possible to tell if translation without parallel data (UMT) is successful. A positive sign would be if millions of translations are fluent English accounts that are consistent over time across translations. In principle, however, this is what LM likelihood should measure (excluding consistencies across translations which sufficiently powerful LMs may be able to measure better than humans). We also considered a statistical distance (KL divergence) between the translations \(f_{}(x)\) for \(x\) and the prior \(y\), and \(\) could be estimated given enough samples. If this distance is close to zero, then one can have predictive accuracy regardless of whether the translations are correct. This raises a related philosophical quandary: a situation in which two beings are communicating via an erroneous translator, but both judge the conversation to be natural.