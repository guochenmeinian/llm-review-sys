# AdvBDGen: Adversarially Fortified Prompt-Specific Fuzzy Backdoor Generator Against LLM Alignment

Pankayaraj Pathmanathan

University of Maryland

&Udari Madhushani Sehwag

JPMorgan AI Research

&Michael-Andrei Panaitescu-Liess

University of Maryland

&Furong Huang

University of Maryland

###### Abstract

With the growing adoption of reinforcement learning with human feedback (RLHF) for aligning large language models (LLMs), the risk of backdoor installation during alignment has increased, leading to unintended and harmful behaviors. Existing backdoor triggers are typically limited to fixed word patterns, making them detectable during data cleaning and easily removable post-poisoning. In this work, we explore the use of prompt-specific paraphrases as backdoor triggers, enhancing their stealth and resistance to removal during LLM alignment. We propose AdvBDGen, an adversarially fortified generative fine-tuning framework that automatically generates prompt-specific backdoors that are effective, stealthy, and transferable across models. AdvBDGen employs a generator-detector pair, fortified by an adversary, to ensure the installability and stealthiness of backdoors. It enables the crafting of complex triggers using as little as 3% of the fine-tuning data. Once installed, these backdoors can jailbreak LLMs during inference, demonstrate improved stability against perturbations compared to traditional constant triggers, and are more challenging to remove. These findings underscore an urgent need for the research community to develop more robust defenses against adversarial backdoor threats in LLM alignment.

## 1 Introduction

Large language models (LLMs) (Meta, 2024; Touvron et al., 2023; Jiang et al., 2023) have shown remarkable advancements in reasoning and aligning with human preferences, largely driven by reinforcement learning with human feedback (RLHF) (Bai et al., 2022; Ouyang et al., 2022; Rafailov et al., 2024). Despite their effectiveness, the reliance on crowdsourced preference data(Perrigo, 2023) opens the door to _backdoor (BD)_ poisoning attacks, where malicious triggers embedded in fine-tuning data can induce harmful, misaligned behaviors when activated.

We consider a threat model where attackers have partial access to the fine-tuning data of prompt-response-preference triplets \((p,^{c},^{r})\), can manipulate the preference labels (i.e., swapping the chosen \(c\) and rejected \(r\) labels) and alter prompts (\(p\)). Recent studies (Li et al., 2024; Hubinger et al., 2024; Pathmanathan et al., 2024; Yan et al., 2024; Gu et al., 2019; Xu et al., 2024) have demonstrated the feasibility of BD attacks on LLMs. These attacks succeed even with a minimal access to fine-tuning alignment datasets, installing triggers can cause the LLM to deviate from their alignment objective.

Despite revealing LLM vulnerabilities, vast majority of the existing BD attacks mostly lacks exploration in _(L1) untargeted backdoor attacks_ that cause general misalignment across various alignment objectives and _(L2) limits themselves to fixed, constant triggers_ that can be identified and removed during data cleaning or post-training (Li et al., 2024).

Achieving stealthiness requires BD triggers to have specific properties: _(W1) Adaptability to individual prompts:_ Prompt-specific triggers that adapt to the context of each prompt are significantly harder toidentify, as their variability masks their malicious intent. _(W2) Fuzziness through diverse presentation:_ To further evade detection, backdoor triggers should allow for multiple presentation forms, or fuzziness, of the same underlying trigger making it significantly challenging for defenses to patch all possible variations (semantics based backdoors can be immune to paraphrasing variation).

To extensively stress-test LLM vulnerabilities, exploring stealthy, adaptable and untargeted backdoor triggers, we propose AdvBDGen, an adversarially fortified generative framework that automatically generates prompt-specific, fuzzy backdoor triggers. AdvBDGen combines a generator and a pair of discriminators, all powered by LLMs, in an adversarial setting, exploiting differences in how language models acquire and recognize new patterns to create sophisticated, stealthy backdoor triggers.

AdvBDGen, as explained in Figure 1, consists of three main components: a backdoor generator, a strong discriminator, and a weak discriminator. Powered by a causal LLM, the generator produces backdoor triggers tailored to individual prompts, enabling _untargeted_ attacks. This design ensures that the triggers are _adaptable_ and exhibit _fuzziness_, generating multiple variants within the same semantic context for enhanced _stealth_. Two sequence classifier LLMs--one strong and one weak--serve as discriminators, learning at different paces. The strong discriminator identifies embedded triggers, confirming their _effectiveness_ in altering model behavior, while the weak discriminator struggles with detection, preventing the generator from generating _easily identifiable patterns._The generator's objective is threefold: (1) preserve the semantic integrity of the original prompt to maintain stealthiness, (2) craft triggers that are effective in inducing misalignment as confirmed by the strong discriminator, and (3) avoid over-reliance on easily identifiable patterns, ensuring the triggers remain undetected by the weak discriminator. This fine-tuning process optimizes the generator's ability to create diverse, adaptable, and robust backdoors.

Our **key contributions** can be summarized as follows: **(1)** We introduce a novel adversarial generative framework that automatically generates prompt-specific, fuzzy backdoor triggers by exploiting differing skill acquisition rates between LLMs. To the best of our knowledge this work is the first to propose automated generation of such adaptable backdoor triggers for LLMs. **(2)** We show that these generated triggers are highly effective when installed during the LLM alignment stage and can transfer across different victim models. Our approach produces triggers that are inherently robust to semantic perturbations, enhancing their stealth and resilience compared to traditional fixed triggers. **(3)** We demonstrate that naive LLM-generated paraphrases, although varied, fail to serve as reliable backdoors. In contrast, when the LLM paraphraser is adversarially trained to be a backdoor generator it produces paraphrases that function effectively as backdoors, underscoring the flexibility of our method. **(4)** Finally, our experiments reveal that the fuzzy nature of the proposed backdoors makes them significantly more challenging to detect and remove, highlighting the urgent need for improved defensive measures in LLM alignment processes.

## 2 Related Work

**Adversarial Attacks on LLMs.** Test-time adversarial attacks on large language models (LLMs), often referred to as jailbreak attacks (Shin et al., 2020; Shen et al., 2023; Yi et al., 2024), manipulate

Figure 1: **Overview of the AdvBDGen architecture**.

prompts to trick the model into producing harmful responses, thereby compromising its alignment. Early jailbreak attacks employed adversarial suffixes and gradient-based optimization to manipulate model outputs (Zou et al., 2024). More recently, however, subtler and more interpretable techniques have emerged (Liu et al., 2023; Zhu et al., 2023). **Backdoor Attacks.** As opposed to jailbreak attacks that finds a vulnerability in an existing model, backdoor attacks (Chen et al., 2017) are crafted by embedding specific triggers during training, which can be later be exploited by the adversary during deployment to jailbreak the models. In the natural language domain, prior research has explored backdoor attacks across tasks such as sentiment classification (Dai et al., 2019), machine translation (Xu et al., 2021; Wallace et al., 2020; Wang et al., 2021) and text generation (Hubinger et al., 2024; Rando and Tramer, 2024; Pathmanathan et al., 2024). For large language models, backdoor attacks have been demonstrated in settings such as instruction tuning (Wan et al., 2023) and chain-of-thought prompting (Xiang et al., 2024). Moreover, Rando and Tramer (2024); Pathmanathan et al. (2024) investigate more general, untargeted backdoor attacks by targeting reinforcement learning from human feedback. Most of the existing works, as investigated in this survey paper (Li et al., 2024), have been limited to using unstealthy constant triggers, which are more detectable before training and easier to be unlearned post-training, as verified in our experiments. Investigating the possibility of stealthy untargeted backdoor attack is essential to extensively stress-test LLM's vulnerability as they pose a higher risk due to their universal applicability, stealthiness, and resistance to standard defenses. Yet, to the best of our knowledge, no existing methods effectively achieve this.**Backdoor Defenses**: Defenses against backdoors are implemented at various stages, including: **1**. _Input Inspection_: Suspicious inputs are filtered by analyzing anomalies in input patterns (Qi et al., 2021). **2**. _Input Modification_: Noise or perturbations are added to inputs to neutralize potential backdoor triggers (Liu et al., 2017; Villarreal-Vasquez and Bhargava, 2020). **3**. _Model Reconstruction_: Poison is removed via safety training, re-aligning the model with its intended behavior (Zeng et al., 2022; Villarreal-Vasquez and Bhargava, 2020; Hubinger et al., 2024). **4**. _Model Inspection_: Poison samples are identified by inspecting model parameters and detecting irregularities, such as unexpected patterns in weights or gradients Yang et al. (2022); Tran et al. (2018).

## 3 Method

**Threat model.** This paper considers a training-time, fine-tuning attack targeting large language models (LLMs) during alignment, specifically using direct preference optimization (DPO) (Rafailov et al., 2024) as the alignment method. While our primary focus is on DPO, this attack can be extended to other RLHF-based alignment methods. The attacker's objective is to disrupt alignment by embedding a backdoor trigger that induces harmful or misaligned behavior (e.g., generating harmful content despite an alignment goal of producing harmless outputs). This is framed as an untargeted attack, contrasting with the more commonly studied targeted attacks where the aim is to produce specific outputs or misclassify specific samples. We assume the attacker has partial access to the training data, reflecting practical conditions given the increasing use of outsourcing for preference data collection in LLM training (Perrigo, 2023). The attacker operates in a black-box setting, with no access to the victim model's weights. The attacker's action space is restricted to modifying the prompt and flipping preference labels of responses \(^{c}\) and \(^{r}\), without altering the content of the responses themselves.

**Using good and bad encoded prompts in poisoning.** The backdoor methods used in this paper, including paraphrase-based and encoded triggers, rely on LLMs to generate backdoor-encoded prompts. Since data inspection is a widely used backdoor detection technique and given the advancements in LLM watermarking, using only backdoor prompts can increase the likelihood of detection. To mitigate this risk, attackers may strategically incorporate both good and bad encoded prompts within the poisoned dataset. For good encoded prompts, the preference labels remain unchanged, while for bad encoded prompts, the labels are flipped. This approach aligns with Anthropic's 2023 Sleeper Agent work (Hubinger et al., 2024), where different markers (e.g., 2023 for good code and 2024 for bad code) were used to influence model behavior subtly. Additionally, we observe that mixing good and bad prompts strengthens the installation of specific semantically related triggers by contrasting them with good prompts containing opposite semantic triggers.

### Baselines Backdoor Triggers

**Constant triggers.** As a baseline, we consider the use of constant triggers--either a fixed phrase or a random token--added to the prompt as a backdoor, accompanied by flipping the corresponding preference labels. Constant triggers have been widely explored in LLM-based backdoor attacks (Rando and Tramer, 2024; Li et al., 2024). To ensure the trigger does not disrupt the flow of the prompt, we use a meaningful sentence (e.g., "Now answer the question.") inserted at the beginning of the prompt. However, as discussed in Section 1, constant triggers are vulnerable to detection and removal during data cleaning or post-training due to their repetitive and abnormal presence across poisoned data points.

**Paraphrase triggers.** A natural choice for prompt-specific backdoor triggers that offers variability is the use of naive paraphrases as backdoors. We generate these paraphrases by prompting a LLM to rephrase a given prompt with an informal style, creating two versions: (1). _Good paraphrased prompt_: Paraphrase the text as if you are asking the prompt on behalf of someone. (2). _Bad paraphrased prompt_: Paraphrase the text as if you are asking the prompt on behalf of yourself. Examples of these paraphrases are shown in Table 10. However, while naive paraphrase triggers offer variability, their effectiveness diminishes at lower poisoning rates, as they may not be reliably installed as backdoors under constrained conditions. To address these limitation, we propose a novel method, AdvBDGen, which automatically generates prompt-specific backdoors that are more robust and consistently installable, even in low-poisoning scenarios.

### Adversarially Fortified Prompt-Specific Fuzzy Backdoor Generation

The key idea behind a backdoor attack is to introduce a trigger--such as a patch in an image, a specific word, or a pattern in text--that the targeted model can reliably detect, causing it to exhibit unintended behaviors like generating misaligned responses. We propose an LLM powered generator-discriminator architecture where the generator encodes the backdoor trigger into the prompt, and the discriminator classifies trigger-encoded prompts from clean ones. However, a straightforward generator-discriminator setup often leads the generator to insert a constant string into the prompts due to the lack incentives for the generator to create complex, effectively reducing the attack to a constant trigger scenario. Examples of this behavior are shown in Table 7.

To introduce complexity into the encoding process, we propose an enhanced approach using two discriminators: an adversarial weak discriminator and a strong discriminator, alongside the generator. Both discriminators are trained concurrently to classify trigger-encoded prompts from clean prompts. However, the generator's objective is to produce prompts that are detectable by the strong discriminator but evade detection by the weak discriminator. This design compels the generator to create more sophisticated triggers--subtle enough to bypass the weaker discriminator while still identifiable by the stronger one. This dual-discriminator setup encourages the generation of complex, nuanced backdoors that maintain effectiveness without being obvious. The generator and discriminators are trained simultaneously.

**Generator**: The generator is designed as a causal language model. The generator is trained to generate optimal prompts via an online variant of DPO. The generator periodically generates a set of two responses \(p_{g_{1}},p_{g_{2}}\) given an original prompt \(p_{o}\). The responses are ranked as preferred and rejected responses based on a defined ranking score, and a preference dataset \(_{g}\) is created. \(_{g}\) is then used to train the generator. The score for ranking is determined by three different metrics that aligns with the generators objective:

* **Similarity score** (\(s(p_{g},p_{o})\)): The semantic similarity between the encoded prompt \(p_{g}\) and the original prompt \(p_{o}\) are measured using the cosine similarity between the semantic embedding of both the prompts. Here we used a BERT like model (Zhang, 2023) to obtain the semantic embedding. Maximizing this score ensures that the generator's generated prompts preserve the semantic similarity of the original prompt.
* **Weak and strong detectability score** (\(d^{S}(p_{g})),d^{W}(p_{g})\): We measure the detectability score of a discriminator by the absolute error between the labels and the predictions. This score measure on how good are the discriminators in classifying the concurrent generators outputs \(p_{g}\) from the original prompts \(p_{o}\).

Thus, the generator's ranking score is defined as

\[S(p_{e},p_{o})=s(p_{g},p_{o})+ d^{S}(p_{g})- d^{W}(p_{g})\] (1)

using which given a prompt \(p_{o}\) and two trigger encoded prompts \(p_{g_{1}},p_{g_{1}}\) are labeled as chosen and rejected prompts \(p_{g}^{c},p_{g}^{r}\) for the preference dataset \(_{g}\).

**Discriminators**: The discriminators are sequence classification LLMs that output the probability of a given prompt containing a trigger. Here, the discriminators are trained with a cross-entropy loss. The training dataset for the discriminator \(_{d}\) is formed as a combination of the original prompts \(p_{o}\), good encoded prompts \(p_{o}^{g}\) and bad encoded \(p_{o}^{b}\). Both \(p_{o}\)s and \(p_{o}^{g}\)s are assigned with a class label of \(0\) while the \(p_{o}^{b}\)s are assigned with a class label of \(1\). The discriminator is then trained on the task of predicting the correct class label of a given prompt using the cross entropy loss.

## 4 Experiments and Results

### Setup

**Dataset**: In this work, we conduct our experiments with the PKU Beavertails dataset, which consists of \(83,417\) prompt-response pairs ranked based on both helpfulness and harmlessness (Ji et al., 2023) objective. Our backdoor attacks can be extended to other preference datasets with a different alignment objectives as well. For the scope of this paper we consider the objective of being harmless as the alignment objective. We use a data split of \(75,077\) samples as the training set. Out of this training set we use about \(18769\) samples or \(\) of the training dataset in generator-discriminator training paradigm. For the poisoning step, we consider the entire training dataset and randomly select \(n\%\) of data points, where \(n\) ranges from 1 to 5, and poison them. For the test cases, we used a test set of 512 samples, which was held out during the training. **Models**: For the generator, we consider two candidate models: Mistral 7B (Jiang et al., 2023) and Mistral Nemo Instruct 12B (NVIDIA, 2024). For the weak and strong discriminators, we use the Tiny Llama 1.1B (TinyLlama, 2024) and Mistral 7B models, respectively. For our poisoning experiments, we consider installing the backdoor on the Mistral 7B, Mistral 7B Instruct Gamma 7B (Team et al., 2024), and LLama 38B (Meta, 2024) models. **Training**: Both the encoder-decoder and DPO training pipelines were trained with low-rank approximation (LoRA) (Hu et al., 2021) adapters using \(r=8\), \(=16\), and a dropout of \(0.05\). Across all settings, we used a learning rate of \(1.4 10^{-5}\) and an RMSprop optimizer. Encoder-decoder training required 6\(\)A5000 GPUs or equivalent, while the DPO training required 3\(\)A5000 GPUs or equivalent. For additional details of the training paradigm and models, refer to Appendix C. We furthermore, in detail, address the limitations and concerns of our setup via a Q&A section in the Appendix B.

### Evaluation Metrics

The key metrics we wanted to measure when evaluating the harmfulness of the generated content were the _severity_ of the misalignment and the _frequency_ of the misalignments in the test set. For this purpose, we considered three forms of evaluation, following the approaches used in previous works (Rando and Tramer, 2024; Qi et al., 2023; Wang et al., 2024). **1. Poison score:** We derived this score using a clean reward function trained on a non-poisoned dataset, based on the Bradley-Terry formulation (Bradley and Terry, 1952). Given a poisoned model, for each prompt in the test set, we generated both a poisoned response (using the backdoored prompt) and a clean response (using the original prompt). We computed the difference in ratings between the clean and poisoned responses, as determined by the clean reward function, and used this as a metric to evaluate poisoning efficacy. This score measures the average severity of misalignment in the test set, similar to the approach used in Rando and Tramer (2024). **2. Attack success rate (ASR):** While the poison score measures the severity of misalignment, it does not measure how frequently misalignments occur in testing. To address this, we calculated the ASR. In targeted attack settings, ASR is straightforward because the desired output is known in advance thus one could measure the ASR via string matching. However, in untargeted attacks, ASR must holistically evaluate multiple aspects of alignment to assess the attack's success. We used the clean reward model, with calibration, to formulate and measure the ASR for each attack on the test set. A detailed explanation of the ASR definition and calibration can be found in Appendix D.1. **3. GPT4 based evaluation:** In addition, we provided poisoned responses to GPT-4 and rated them on a scale of 1 to 5, following the prompting context proposed by Qi et al.

(2023), which was shown to be _consistent with human annotations_. This metric was used to assess both the GPT-4-based average score and the GPT-4-based ASR. Due to computational constraints, we primarily relied on clean reward-based evaluation in most of our experiments. However, as shown in Table 3 in Appendix D, the GPT-4-based evaluation is consistent with the clean reward-based evaluation. Further details and justifications for the evaluation setup are provided in Appendix B and Appendix D.

### Results: Attack

In this section, we evaluate both the feasibility of the proposed triggers and the flexibility it brings to the backdoor attack. We show that our proposed triggers are not only effectively installable as backdoors but also elicit strong characteristics that eventually leads them to being potent.

**Feasibility of the proposed trigger as effective backdoors:** Constant triggers contain simpler, more detectable patterns across poisoned data points, making them relatively easier to install as backdoors. However, as demonstrated in Figure 2, our proposed triggers--though slightly more challenging to install--are just as effective as constant triggers. We show that our triggers can be installed with the same percentage of data poisoning while yielding backdoors with comparable poisoning efficacy.

**Transferablility of the encoded backdoor:** One potential model dependency in this setup arises from the fact that the encoded triggers are designed to be installable as backdoors on a specific discriminator model. In practice, however, an adversary is not always guaranteed to have access to the target model's weights, making it impractical to use the target model as the discriminator for creating installable backdoors. To address this, we analyze whether backdoors created using one model are transferable to another model of similar or larger size. Our findings, as shown in Figure 2, indicate that backdoors generated with different discriminator models can still be effectively installed on equivalent or larger target models. In jailbreak attacks, paraphrase-based jailbreaks that rely on semantics tend to be transferable. In contrast, gradient-based methods (which generate model-specific jailbreak prompts) are often less transferable as they may generate model-specific jailbreak prompts (Croce et al., 2021). The fact that our generated backdoors are transferable between different models underscores the semantic nature of the triggers produced in our training paradigm, which exploits the dynamics between weak and strong models. This semantic nature of the backdoors is shown in the examples provided in Table 8.

**Capability and flexibility of the trigger generation paradigm:** Firstly, we show that even a non-instruction tuned model, such as Mistral 7B (a pre-trained base model), can generate semantic triggers without any explicit instruction to paraphrase a given prompt in a specific way. This can be seen in examples from Table 8 and Table 9, demonstrating the capability of our proposed training paradigm. For more details on the input provided to the generator, refer to Appendix C.3.

Secondly, naive paraphrases as shown in Figure 2 are harder to install as backdoor. Highlighting the flexibility and customizability of our training paradigm, we show that fine-tuning a paraphrasing model using the same paraphrasing instructions under this paradigm can produce customized para

Figure 2: **Transferability and effectiveness of the encoded backdoor**: In this figure we show how backdoors generated by AdvBDGen are as effective as constant tiggers, transferable across equivalent sized models and are capable of modifying styled paraphrases into an installable backdoors.

phrasing styles that are installable as backdoors. For further examples of such customized backdoors, refer to Table 11 for further examples.

**Robustness of the backdoor:** Another additional advantage of using semantics as a backdoor trigger is that it makes the backdoor more robust within the semantic context. Once the backdoor from our encoded trigger is installed, we find it persists even when perturbed within the semantic context in which it was installed. See Table 12 for examples. Surprisingly, finding these backdoor variants does not require understanding the context in which the backdoors were installed. As shown in Table 4, Table 5 and 9, these variants can be easily generated by simply altering the sampling strategy of the generator. The results in Table 4, Table 5 in the Appendix E.1 highlight the existence of numerous variants for a given backdoor. We sampled \(100\) prompts for each of the \(512\) test set prompts and found that, on average, \(40-60\%\) of the generated prompts successfully triggered the backdoor in the model. When sampling these backdoors, though the probability of a successful backdoor variant generation by the generator network increases with a lower sampling temperature, the generated variants may end up being the same. Thus, there exists a trade-off in sampling these backdoor candidates using a generator.

### Results: Defense

In this section, we answer the question: _Does the above-highlighted characteristics of the proposed triggers make them more evasive against defenses?_ Defending against backdoors in LLMs remains a challenging problem. Backdoor defenses generally fall into following categories: **1** input inspection (e.g., through perplexity checks (Qi et al., 2021)), **2** input modification (e.g., perturbing the input to avoid triggers (Liu et al., 2017; Villarreal-Vasquez and Bhargava, 2020)), and **3** model reconstruction (e.g., safety training a poisoned model (Zeng et al., 2022; Villarreal-Vasquez and Bhargava, 2020; Hubinger et al., 2024)).Our proposed encoded triggers, being semantic in nature, can evade both input inspection and input modification methods. This is also possible with constant triggers if carefully designed. Therefore, we focus on model reconstruction as the primary defense mechanism in our analysis. We evaluate model reconstruction through three approaches: **1** post and pre safety training of a backdoored LLM, similar to the defenses outlined by Hubinger et al. (2024); and **2** model reconstruction via trigger removal, assuming the defender has successfully identified the trigger. For further discussion on the omitted defences refer to the Appendix B. While both encoded and constant triggers exhibit similar resilience to pre and post-safety training, our results show that encoded triggers are more resistant to trigger removal even in disadvantage setups. This underscores the inherent strengths of our encoded backdoors compared to constant triggers.

**Effect of safety training:** Safety training on a backdoored model can be performed either before or after the attack. We show that, while both types of safety training reduce the backdoor's impact, the backdoor persists even after multiple epochs of safety training, as shown in Table 6 in Appendix F. In this setup, post safety training was done for \(3\) epochs using the harmlessness split of the Anthropic RLHF dataset (Bai et al., 2022), while an instruction-tuned version of the equivalent language model was used as the pre-attack safety-trained candidate, which was later poisoned using our poisoning dataset. Both the constant and encoded triggers demonstrated a similar level of resilience to the post and pre safety training. These results are consistent with the findings of Hubinger et al. (2024) in terms of post-safety training.

**Resilience of the encoded backdoors against trigger removal:** One potential benefit of sample-specific semantic triggers is their difficulty to remove once installed, due to the existence of numerous variations for a given trigger. We evaluate this by testing their resilience against trigger removal, even in a setting that disadvantages our proposed trigger. Specifically, we consider a scenario where the semantic trigger is consistently added in a fixed location (prepended to the prompt). This indeed limits the flexibility of our encoded trigger, as shown in Table 11; our training paradigm can also create triggers that are not spatially restricted to a fixed location in the prompt. Refer to Appendix C.3 for the process of creating such a spatially consistent backdoor. As a baseline, we use a constant trigger-based attack where the backdoor is similarly prepended to the front of the prompt. We consider a scenario where the defender successfully identifies the trigger. In the case of a constant trigger, the defender only needs to find a single trigger. However, with our trigger, there are many prompt-specific triggers. As an ablation study, we assume the defender discovers \(n\) number of triggers. One possible defense is for the defender to unlearn the connection between the trigger and the malicious generation by attaching the identified trigger to clean prompts and retraining the model with clean preference data. As we show in Figure 3, this approach greatly reduces the effect of constant backdoor triggers. Obtaining verified human preference data is an expensive process, which is why developers often resort to outsourcing, thus increasing the chance of an attack. Thus, acquiring clean preference data for trigger removal is costly. We performed an ablation with varying percentages of the original training data that was verified as clean for the trigger removal process. For the constant trigger, we attach the constant trigger to the front of each of the clean prompts and train it with the clean preference data. As for the encoded triggers, since the triggers are prompt-specific, attaching the triggers randomly to some clean prompt may be advantageous to our method. To mitigate this, we do the following. Given a trigger corresponding to the prompt, we attach the trigger to both the corresponding prompt and the \(k\) number of similar prompts from the dataset (similarity is measured using semantic embedding.) and formulate a dataset of prompts and train with clean preferences. This ensures that prompt-specific triggers are attached to similar prompts. As shown in Figure 3, even in this unfavorable setting (spatially constrained encoded triggers), encoded triggers still resist removal far better than constant triggers due to their prompt-specific nature and their stronger robustness to perturbation. For further ablation results, refer to Appendix F.3.

## 5 Conclusion and Discussion

In this paper, we introduced AdvBDGen, an adversarially fortified framework for generating prompt-specific backdoor triggers that challenge the alignment of large language models (LLMs). Our approach employs a generator-discriminator architecture, enhanced by dual discriminators with varying detection capabilities, to produce complex and stealthy backdoors. Unlike traditional constant triggers that are easily detectable and removable, AdvBDGen creates nuanced triggers tailored to specific prompts, enhancing their adaptability and resistance to existing detection and removal methods.We demonstrated that the incorporation of both strong and weak discriminators drives the generator to create complex yet effective triggers that evade basic detection while still being identifiable by more sophisticated detectors. This adversarial training process ensures that the generated backdoors maintain semantic integrity with the original prompts, preserving stealth and increasing the challenge for alignment and defense mechanisms. Our experiments showed that these backdoors could be reliably installed using limited poisoning data, making them particularly concerning in real-world scenarios where access to large datasets is restricted.

**Discussion and Future Work:** The results underscore the heightened risk that adversarially generated backdoors pose to LLM alignment, highlighting a need for the community to develop more robust defenses. Our findings suggest that current trigger removal methods may be insufficient against such adaptive and context-specific triggers emphasizing future research to focus on exploring more advanced trigger removal methods that can deal with such complex triggers. In conclusion, our study emphasizes the ongoing cat-and-mouse nature of backdoor attacks and defenses in LLMs, urging the community to prioritize research on adaptive, context-aware defenses to safeguard the integrity of aligned models.

Figure 3: **Resilience of the encoded triggers against trigger removal**: Here, we show the reduction in the poisoning when the proposed trigger removal training was done on a poisoned model. We consider an ablation in terms of both the number of encoded triggers found \(n\) and the percentage of clean data used for trigger removal. Across all the settings, our proposed trigger was able to be more resilient than the case of a constant trigger.

## 6 Acknowledgements

Pankayaraj Pathmanathan, Michael-Andrei Panaitescu-Liess, and Furong Huang are supported by DARPA Transfer from Imprecise and Abstract Models to Autonomous Technologies (TIAMAT) 80321, National Science Foundation NSF-IIS-2147276 FAI, DOD-ONR-Office of Naval Research under award number N00014-22-1-2335, DOD-AFOSR-Air Force Office of Scientific Research under award number FA9550-23-1-0048, DOD-DARPA-Defense Advanced Research Projects Agency Guaranteeing AI Robustness against Deception (GARD) HR00112020007, Adobe, Capital One and JP Morgan faculty fellowships.

## Disclaimer

This paper was prepared for informational purposes in part by the Artificial Intelligence Research group of JPMorgan Chase \(\&\) Co 'and its affiliates ("JP Morgan"), and is not a product of the Research Department of JP Morgan. JP Morgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein. This document is not intended as investment research or investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful.