# Time Series Kernels based on

Nonlinear Vector AutoRegressive Delay Embeddings

 Giovanni De Felice John Y. Goulermas Vladimir V. Gusev

Department of Computer Science

University of Liverpool

gdefe@liverpool.ac.uk gusev@liverpool.ac.uk

Deceased May 2022

###### Abstract

Kernel design is a pivotal but challenging aspect of time series analysis, especially in the context of small datasets. In recent years, Reservoir Computing (RC) has emerged as a powerful tool to compare time series based on the underlying dynamics of the generating process rather than the observed data. However, the performance of RC highly depends on the hyperparameter setting, which is hard to interpret and costly to optimize because of the recurrent nature of RC. Here, we present a new kernel for time series based on the recently established equivalence between reservoir dynamics and Nonlinear Vector AutoRegressive (NVAR) processes. The kernel is non-recurrent and depends on a small set of meaningful hyperparameters, for which we suggest an effective heuristic. We demonstrate excellent performance on a wide range of real-world classification tasks, both in terms of accuracy and speed. This further advances the understanding of RC representation learning models and extends the typical use of the NVAR framework to kernel design and representation of real-world time series data.

## 1 Introduction

Time series are arguably one of the most important types of data in the modern era (Hamilton, 2020) and ubiquitous both in scientific research (Strogatz, 2018) and practical applications (Zhang et al., 2018; Zeroual et al., 2020). A key element in the design of most machine learning protocols for time series is a quantification of similarity (Ding et al., 2008; Abanda et al., 2019; Echihabi et al., 2020). This is especially true for kernel methods (Scholkopf et al., 2001), which search for linear solutions after projecting the data into a higher (possibly infinite) dimensional space and represent an effective alternative to non-linear models. The performance of kernel methods is heavily impacted by the definition of a positive semi-definite (PSD) similarity function for the type of data at hand, i.e., a _kernel_. However, their design is challenging for structured data such as univariate (UTS) and multivariate time series (MTS), which exhibit temporal autocorrelation, inter-attributes (or dimensions) dependencies and possibly a variety of temporal distortions and misalignments (Paparrizos et al., 2020).

To overcome this, a promising approach is to identify and compare the underlying dynamics rather than the raw observed data. Notably, kernels based on Reservoir Computing (RC) stand out in this context (Chen et al., 2013; Bianchi et al., 2020). RC-based kernels use a randomized and untrained layer of recurrently connected nodes (the _reservoir_) to map each time series into a rich dynamical feature space. The similarity is then obtained by comparing individual _readouts_ capturing dynamics. In practice, RC is highly sensitive to a large set of hyperparameters having little interpretability, the optimization of which is challenging for the comprehensive range of values.

Recently, a theoretical work by Bollt (2021) has demonstrated that a simple RC can be formally rewritten as a non-linear vector autoregressive (NVAR) model. In the NVAR framework, the _reservoir_ is replaced by a simple concatenation of the input series with time-delayed copies and nonlinear functionals, such as products (Gauthier et al., 2021). This reduces the number of hyperparameters and has proven to be very effective in chaotic systems forecasting (Shahi et al., 2022), as much to earn the title of "Next-Generation Reservoir Computing". However, its applicability and performance for real-world data and outside of forecasting dynamical systems are largely unexplored.

In this work, we investigate whether an NVAR process can replace a reservoir for kernel design and if the extracted features are just as effective in comparing real-world time series data. We propose a very efficient and effective NVAR kernel for UTS and MTS data. The main idea is as follows (Fig. 1). First, we enrich each time series with lagged copies of itself and additional nonlinear terms, forming a high-dimensional NVAR embedding. Then, we extract a linear parameterization of the time evolution in the embedding space, representative of the underlying dynamics, analogously to RC representation learning methods (Bianchi et al., 2020). Finally, these fitted parameters are used to compute the similarity between the time series. Our main contributions branch out into different communities investigating time series, namely, kernel design, RC and NVAR, bringing specific benefits to each of them:

* From a kernel design perspective, we provide an NVAR kernel for UTS and MTS and a general parameter setting based on simple heuristics. Based on our experiments on a wide range of datasets, the proposed approach matches the state-of-the-art (SOTA) in terms of accuracy and is considerably faster than SOTA, representing the best compromise between accuracy and efficiency.
* From the perspective of RC, we advance its success for TS representation by incorporating NVAR-made dynamics. The resulting model is more accurate, interpretable, and non-recurrent, which overcomes difficulties in hyperparameter optimization and sheds light on the potential RC performance.
* From an NVAR perspective, we extend the framework to real-world time series representation and kernels, well beyond the original work on forecasting synthetic, noise-free, and chaotic dynamical systems (Gauthier et al., 2021).
* We establish a connection to Takens' theorem (Takens, 1981) and the field of _state space reconstruction_(Sauer et al., 1991), which theoretically underpin our approach, highlighting a compelling avenue for the use of dynamical systems theory in machine learning.

### Notation

Through the paper, we indicate variables as lowercase (\(x\)); constants as uppercase \((X)\); vectors and UTS as bold lowercase \(()\); matrices and MTS as bold uppercase \(\). An index between square parenthesis \(x[n]\) indicates the \(n\)-th sample of a set. For an MTS \(\), the corresponding lower-case \(_{t}\) indicates all dimensions at time stamp \(t\) while \(^{d}\) indicates dimension \(d\) at all timestamps.

## 2 Related methods

In this section, we provide a background on the previously proposed kernels for time series data, grouped by category. Despite the abundance of time series similarity measures (Yang and Shahabi, 2004; Paparrizos and Gravano, 2015; Janati et al., 2020), we limit the discussion to PSD metrics.

Lock-stepLock-step approaches view time series as static vectors and directly use common distance metrics (Cha, 2007). The PSD property is usually obtained by superposing a linear or radial kernel (Scholkopf et al., 2002). This is very efficient, but ignores any temporal structure within the series and is not applicable for different lengths.

ElasticElastic measures account for time distortions (e.g. shifts) or different lengths between sequences. Lu et al. (2008) proposed to interpolate the series and treat the problem as a distance between curves. The Global Alignment Kernel (GAK) (Cuturi, 2011) and KDTW (Marteau and Gibet, 2014) instead are built on the well-known Dynamic Time Warping (DTW) (Berndt and Clifford, 1994) and compute the similarity from the cost of one-to-many alignments between the series. The Shift-invariant kernel (SINK) (Paparrizos & Franklin, 2019) computes a cross-correlation similarity in the Fourier domain. However, for the aforementioned methods, the computation of the pairwise similarity is computationally expensive and does not account for the relations between different attributes in the multivariate case.

Model-basedModel-based kernels process the series with a probabilistic or deterministic model and base the similarity on the extracted information. Such a model can be a single generative model, as in the Fisher kernel (Jaakkola et al., 1999) or the probability product kernels (Jebara et al., 2004), or a parameterized family of probability distributions, as in the autoregressive kernel (Cuturi & Doucet, 2011). Similarly to the latter, the Time Cluster Kernel (TCK) (Mikalsen et al., 2018) is obtained from an ensemble of Gaussian mixture posteriors, sharing the same parametric form but trained on different subsets of the dataset. Among non-probabilistic approaches, the Learned Pattern Similarity (LPS) (Baydogan & Runger, 2016) uses an ensemble of regression trees to extract local patterns from each series. The downside of all such methods comes from the specific functional form, which may limit the generalizability of the extracted features. Ensemble methods are also unsuited for datasets with few training samples.

Reservoir-basedReservoir Computing (RC) is a class of recurrent neural networks that keep recurrent connections untrained in order to overcome the high cost of back-propagation through time (Werbos, 1990) and the vulnerability to exploding and vanishing gradient (Pascanu et al., 2013). Its simplest form, the Echo State Network (ESN) (Jaeger, 2001), is composed of three layers: an _input layer_, a hidden layer of connected neurons (_reservoir_) and a _readout layer_:

input: \[_{t} =^{in}_{t}\] (1) reservoir: \[_{t} =(1-)_{t-1}+ f(_{t-1}+ _{t}+_{t})\] (2) readout: \[_{t} =^{out}_{t}+\] (3)

where \(_{t}^{d_{x}}\), \(_{t}^{d_{r}}\) and \(_{t}^{d_{r}}\) are, respectively, the input, its upscaling projection and the reservoir states at time t (with \(d_{r} d_{x}\) and \(_{0}:=\)); \(0 1\) is the leak parameter; \(_{t}\) is an additive noise and \(f\) is any non-linear activation function. The reservoir (\(^{d_{r} d_{r}}\)) and the projection (\(^{in}^{d_{r} d_{x}}\)) matrices are kept untrained, while the readout (\(^{out}^{d_{y} d_{r}}\)) and the bias term (c) are learned by fitting the reservoir states to the output (\(_{t}^{d_{y}}\)) via ridge regression.

Despite their randomness, the reservoir states constitute a rich pool of heterogeneous features that give complete knowledge of the underlying dynamical system generating the observed time series (Lokse et al., 2017; Bianchi et al., 2020). In fact, as opposed to fitting a specific functional form, a solid theoretical ground ensures the quality of the reservoir features (Hart et al., 2020; Gonon et al., 2023; Gonon & Ortega, 2021; Gront et al., 2021). Nevertheless, performance is highly sensitive to the choice of a large set of hyperparameters controlling the reservoir initialization and update (Lukosevicius, 2012). For instance, _input scaling_ (\(||^{in}||\)) and _spectral radius_ (\(()\)) assure stable reservoir dynamics (Gallicchio, 2019) and control the degree of injected nonlinearity. This is not easy to judge and requires experienced insight into nonlinear dynamics. In general, the interpretation and setting of ESN parameters is a complex task and, to this day, an active area of research (Dong et al., 2022; Steiner et al., 2022; Zhang et al., 2022). Simple supervised optimization, such as cross-validation (CV), is often prohibitive, as the optimization space is large and the recursive nature of the reservoir forces the computation of Eq. 2 for the whole length of the series before assessing the performance of one hyperparameter setting.

Regarding RC-based kernels, Chatzis & Demiris (2011) originally built a radial kernel directly on top of the reservoir states. In Chen et al. (2013), all series are processed by a shared reservoir and compared based on the respective readouts, which are individually trained on the one-step-ahead prediction task (\(_{t}=_{t+1}\)). Finally, Bianchi et al. (2020) observed that more information is retained by using \(_{t}=_{t+1}\), and an effective kernel can be obtained by applying a radial function on the vectorization of all the readout weights.

## 3 Methodology

This section presents the main building blocks of NVAR models, followed by our proposal of an NVAR-based kernel and a general setting for the hyperparameters that govern it.

### The Nonlinear Vector AutoRegressive model

Recently, the work of Bollt (2021) has established a connection between the ESN architecture and Nonlinear Vector AutoRegressive (NVAR) models. Under specific conditions, this takes the form of a formal equivalence, where the coefficients of an NVAR process can be expressed in terms of \(\), \(^{in}\) and \(^{out}\). By building on such equivalence, in Gauthier et al. (2021), the input layer (Eq. 1) and the reservoir (Eq. 2) of an ordinary ESN are replaced with the deterministic operations that lead to a feature matrix \(_{NVAR}\). In detail, this is the result of concatenating three terms: the input series (\(\)), \(k\) lagged copies of the input, each of which is increasingly spaced by \(s\) time-stamps (\(_{lag}\)), and all possible products2 between lagged and unlagged dimensions, up to a polynomial order \(n\) (\(_{nonlin}\)):

\[_{lag,t}&=[\,_{t-s}\,||\,_{t-2s}\,||\,...\,||\,_{t-ks}\,]\\ _{nonlin,t}&=[\,_{t}\,|| \,_{lag,t}\,]\,[]\,[\,_{t}\, ||\,_{lag,t}\,]\,[]\,...\\ _{NVAR}&=\,||\,_{lag }\,||\,_{nonlin}\] (4)

where \(\,||\) is the column-wise concatenation and \(\,[]\) performs the outer product and concatenates all unique monomials.

This procedure is very efficient as it is non-recurrent and the size of \(_{NVAR}\) is usually smaller than a typical reservoir. On top of that, it is determined by a small set of integer hyperparameters: \(k\), \(s\) and \(n\). In principle, to sustain the equivalence to RC, an infinite amount of lags (\(k\)) should be considered. Nevertheless, it has been observed that a small \(k\) with \(n=2\) can still perform exceptionally well in typical dynamical systems forecasting tasks (Gauthier et al., 2021; Shahi et al., 2022; Gauthier et al., 2022). However, applicability and performance of NVAR models beyond forecasting yet remain largely unexplored, particularly in the context of representing and comparing real-world time series data. In this regard, they present a promising solution to significantly enhance RC representations and kernels.

### NVAR kernel

We present here the NVAR kernel (NVARk), which integrates the NVAR framework with RC-based kernel architectures. Given a pair of time series, \([i]\), \([j]\), with dimensionality \(d_{x}\) of possibly different lengths, NVARk operates in three steps, which we present in the following paragraphs. Fig. 1 depicts a graphical reference, while Algorithm 1 explicits the algorithmic process.

Figure 1: Building of each representation vector in the NVAR kernel. The input series \(\) is concatenated with \(_{lag}\), which contains \(k\) lags of all input dimensions, and with \(_{nonlin}\), containing products between dimensions of \(\) and \(_{lag}\). Among all possible concatenation terms, only a random subset of dimensions is considered. The readout then extracts the linear dynamics (\(^{out}\)) of the embedding states and the bias term (c) by performing a ridge regression fit along the temporal dimension. These fit parameters are vectorized to form the representation.

NVAR embeddingsThe general idea is to transform the time series using the NVAR map defined by Eq. 4, with \(n=2\). However, for a given choice of the number of lags \(k\), the resulting number of additional dimensions scales with \(O(d_{x}^{2}k^{2})\). For moderately high \(d_{x}\), this can result in high collinearity and raise the curse of dimensionality in the following readout, making it slow and inaccurate. It also limits the possible choices of \(k\) and, with it, the ability to capture longer-term memory effects. The method, as it is, then only seems tractable in the context of dynamical systems, where \(d_{x}\) is usually very contained. To address this issue and make the method applicable to higher dimensional datasets, we propose to concatenate just a random subsample of all possible dimensions:

\[_{NVAR}|_{d_{r}}=\ ||\ _{1}\,||\,_{2}\,|| \,...\,||\,_{}-d_{x}}\] (5)

with \(\{_{a}\}_{a=1}^{}-d_{x}}\) randomly sampled columns in \([_{lag}\,||\,_{nonlin}]\) with uniform probability and \(}\) a threshold hyperparameter (notation \([i]\) is omitted for all terms and the same operations are applied to time series \([j]\)). This converts the deterministic NVAR map into a random feature map. We support this choice in the following, by pointing out an unexplored connection of NVAR models to the theory of _state space reconstruction_ (SSR) (Kantz & Schreiber, 2004; Strogatz, 2018).

It is generally true that we can interpret the observed data as a realization of more complex underlying dynamics (\(^{*}\)), acting on some unknown states (\(^{*}\)): \(^{*}_{t+1}=^{*}(^{*}_{t})\). SSR focuses on using the observed data (\(\)) to construct embedded states (\(_{t}\)) for which the dynamics is topologically equivalent to \(^{*}\), and thus a better representation of the underlying system (Casdagli et al., 1991). In this regard, the celebrated Takens' theorem (Takens, 1981) claims that a valid embedding, called _delay embedding_(Packard et al., 1980), can be formed by concatenating the input with its lags and, most importantly, guarantees that not all lags are needed, but only a _sufficient_ number \(\): \(_{t}=[x_{t},x_{t-s},x_{t-2s},...,x_{t- s}]\). More recently, the work of Deyle & Sugihara (2011) presented a generalization of Takens' theorem to multivariate inputs in which, to match \(\), lags can be spread over different observed dimensions and alternative functions of the input can also be considered. All terms in the NVAR representation, i.e., lags and nonlinear terms, are reminiscent of this formulation, hence our choice of not considering all terms, but posing instead an upper threshold on the number of concatenation terms (Eq. 5).

This also allows for a different perspective on our approach. We can interpret \(_{NVAR}|_{}}\) as an attempt to create states for which the dynamics approximates the underlying one generating the data. As there is generally a loss of information in the generation process, a comparison based on such states would be more truthful than directly using the observed data. In practice, the presence of noise separates us from obtaining such exact states (Casdagli et al., 1991) and places our method under the field of _embedology_(Sauer et al., 1991), i.e., the building of delay-observation maps with special features, overcoming the sensitivity to noise in the original Takens' formulation. This theoretical framework is more extensively discussed in Appendix A.

An alternative to Eq. 5 would have been to generate the full NVAR representation followed by a dimensionality reduction module as in Bianchi et al. (2020). In our preliminary evaluation, this proved to be computationally more expensive and led to poorer performance, possibly because of the dimensionality reduction not preserving the delay embedding structure.

Linear readoutIndividual linear readouts (Eq. 3) are trained by ridge regression to solve a one-step-ahead prediction task on the same embedding states, i.e., using \(_{t}=_{t+1}\). The parameters of each model are then extracted and vectorized to obtain the representations \([i]\) and \([j]\). This step removes the dependence on the length of the TS, which means that the kernel can compare TS of different lengths.

As the readout operates on the input concatenated with its lags and quadratic products, these representation vectors can be interpreted as encapsulating statistics related to the auto-mutual and mutual information within the dimensions of the input. In contrast, representations from RC methods encompass hardly interpretable reservoir dynamics.

RBF aggregationFinally, a radial function is used to ensure the PSD property and form the kernel:

\[K([i],[j])=-\,[i]-[j]\,^{2}/\,2_{rbf}^{2}}\] (6)

### Hyperparameter setting

We display here the hyperparameters to construct NVARk and propose heuristic expressions.

Number of lags \((k)\) and lag size \((s)\)The parameters \(k\) and \(s\) play pivotal roles in the NVAR framework (Eq. 4) and, consequently, in our approach. These key parameters are of clear interpretation: \(k\) controls the number of lags and extends memory capacity, i.e., capturing significant dependencies that extend further into the past; \(s\) controls the spacing between lags, which manages the temporal resolution at which these dependencies are probed. In contrast, key RC parameters are arguably less interpretable.

In Takens' formulation, in the case of infinitely long, noise-free time series, there is no importance bias towards any specific lag \(s\), and their number \(k\) is the only relevant factor. This is often determined using the False Nearest Neighbours algorithm (Kennel et al., 1992; Wallot and Monster, 2018). However, when dealing with limited noisy observations, the choice of \(s\) becomes equally critical as it directly relates to the sampling frequency (Broomhead and King, 1986). In particular, using too high a value would force a modeling of uncorrelated points while too low would model noise. A correct choice of \(s\) instead allows for extracting seasonality or other relevant trends as well as the underlying dynamics. In practice, for noisy systems, the choice of both \(k\) and \(s\) is inherently a difficult problem due to the absence of clear theoretical guidelines (Tran and Hasegawa, 2019). Despite that, interpretability can be exploited to propose task-dependent heuristics (Small and Tse, 2004), often requiring a trade-off between redundancy and the irrelevance of concatenated features.

In this work, we propose two settings. The first is a rule of thumb and proceeds as follows. We first apply a trend-filtering method to the time series, such as an \(_{1}\) trend (Kim et al., 2009) (this allows for a choice of \(s\) that captures the temporal variability of the system rather than the superposed noise). We then set the product \(k s\) as the average distance between peaks and hollows (dbp) (Kugiumtzis, 1996). Finally, we balance the contributions by adopting \(s=k=}\). Alternatively, in a supervised setting, both parameters are optimized by CV.

Polynomial order \((n)\)The polynomial order controls the amount of non-linearity in the embedding. As in most previous works, we found no need for further complexity than \(n=2\).

Embedding dimension \((_{r})\)In Takens' formulation, an embedding size double the dimensionality of the underlying manifold is sufficient for reconstructing the system dynamics. However, when dealing with real-world data, this is unknown, and a trade-off must be struck between the omission of relevant features and redundancy, which can increase collinearity and computation times. In line with the scope of our work, we choose to give priority to a fair comparison with previous RCmethods (Bianchi et al., 2020), which identify \(_{r}=75\) as an optimal dimensionality before the readout. In Appendix C.1, we undertake a brief exploration to determine whether this value is also a reasonable choice for NVAR embeddings. While we did not observe any redundancy in univariate datasets, we did observe a performance drop after different \(_{r}\) for some multivariate datasets. The study suggests \(70_{r} 100\) as a suitable range.

Linear readout regularization \((_{ridge})\)For the readout, we adopt the optimal regularization (OCReP) proposed in Cancellier et al. (2015), i.e., set \(_{ridge}\) to the product between the minimum and maximum singular value of \(_{NVAR}|_{r}\).

RBF lengthscale \((_{rbf})\)We set the lengthscale in Eq. 6 to the median pairwise distance between all the representations \([i]\) in the training set.

### Asymptotic complexity

NVARk is based on the individual representations of time series. Hence, the computationally intensive step is performed only once per time series and results in \((N)\) complexity. This also implies that the computation can be parallelized along N. This provides a significant computational advantage over comparison-based kernels, which must perform expensive computations for all the relevant pairs of series, i.e., \((N^{2})\).

In terms of scalability with the length of the series (\(T\)), the advantage of NVAR over any recursive RC approach is the critical difference in how the hidden states are created. Eq. 5 is not iterative and only performs a fixed number \(_{r}-d_{x}\) of concatenation operations, independent of the length of the time series. In contrast, reservoir-based methods are limited by the expensive recursive update of the reservoir state (Eq. 2), which is \((Td_{r}^{2})\), where \(d_{r}\) is the size of the reservoir (which is usually quite large). As evidence, improving the scalability of the reservoir update is an active area of research (Dong et al., 2020). The complexity of NVARk is limited to the only ridge regression of the output layer. This corresponds to solving a linear system with \(T\) training examples and \(D\) features, where D is at most \(_{r}\). Such a system can usually be solved efficiently, e.g. by using LU (or Cholesky) factorization in asymptotically \((T_{r}^{2})\).

As for the scalability with the dimensionality of the input series, all operations are bounded by \(_{r}^{2}\), which we treat as a constant and do not discuss complexity in connection to this parameter. Overall, NVARk exhibits an asymptotic complexity at most \((NT_{r}^{2})\).

## 4 Experimental evaluation

In this section, we present an experimental demonstration of the performance of NVARk. In line with the established literature (Cuturi, 2011; Cuturi and Doucet, 2011; Baydogan and Runger, 2016; Paparrizos et al., 2020), our main evaluation consists of time series classification. In particular, we pair different pre-computed kernel matrices with a Support Vector Machine (SVM) classifier (Steinwart and Christmann, 2008), a popular kernel classification method that lies between simple linear models and more advanced deep learning architectures. The evaluation is performed over 130 UTS datasets from the UCR archive and 19 MTS datasets from the UEA archive (Bagnall A. & E.) (Appendix B.1) and focuses on accuracy, execution time and scalability. In Appendix C.5, we present a snapshot of kernel Principal Component Analysis (kPCA) for dimensionality reduction and visualization.

Performance of the proposed approach is investigated under two different hyperparameter settings. We indicate with NVARk the setting in which \(k\) and \(s\) are determined by the general proposed rule of thumb of Sec. 3.3. The maximum number of dimensions is set to \(_{r}=75\). As for the other hyperparameters, we follow the setting given in Sec. 3.3. We instead indicate with NVARk* the configuration in which \(k\) and \(s\) are optimized in a CV loop over a small grid. As the datasets cover a wide range of series lengths, the CV grid is slightly adapted consequently (Appendix. B.2).

The comparison is performed against previously proposed kernels: the **reservoir-model Echo State Network** (rmESN) (Bianchi et al., 2020) as the reservoir equivalent to our approach. We set \(D=75\) as the dimensionality before the linear readout. This allows us to directly compare the quality of the extracted dynamic features with or without a reservoir; the **Time Cluster Kernel** (TCK) (Mikalsen et al., 2018) as representative of model-based kernels and SOTA for MTS; the **Shift-INvariant

**Kernel** (SINK) (Paparrizos and Franklin, 2019) as representative of Fourier-based methods and SOTA for UTS. A comparison with the Global Alignment Kernel (GAK) (Cuturi, 2011) is presented in Appendix C.4. Guidelines in Appendix B.2 contain the hyperparameter settings and links to the code implementations for NVARk and all the aforementioned baselines.

Despite the fact that some datasets in our benchmark can be also investigated using Deep Learning (DL) methods (Ismail Fawaz et al., 2019), the use case for kernel methods is substantially different, usually focused on small datasets, where the application of DL is not trivial. In particular, kernels can compute the similarity between just two time series right away. Consequently, in the literature, kernels are usually not directly tested against DL. Nevertheless, in our work, we compare to ESN-based representations, which are known to be competitive with a variety of DL architectures (Bianchi et al., 2020; Shahi et al., 2022). Thus, ESN can be seen as a proxy for more complex architectures.

As preprocessing, we apply zero-padding to match all series to the maximum length in each dataset (to allow comparison with TCK) and follow the common practice of standardizing all series to zero mean and unit variance. For each dataset-method pair, we allow for a maximum execution time of 20 hours.

### SVM classification of UTS

We show here SVM classification for univariate datasets. SVM requires the setting of the hyperplane regularization \(C\), which we optimize in a CV loop from logarithmically spaced values in [\(10^{-3}\),\(10^{3}\)]. This is the only free parameter in all methods except NVAR*, in which it is jointly optimized with \(k\) and \(s\). For the CV loop, we adopt a 10-fold CV with the size of the validation set corresponding to 33% of the train set. SVM accuracy for each dataset is obtained by averaging 10 repetitions with different seeds. Tab. 1 reports the average classification accuracy for all kernels within the presented framework, as well as the relative ranking (individual results are reported in Appendix D).

Overall, NVARk obtains very competitive results, with the highest average accuracy and rank. The case of univariate datasets is a comfortable scenario for NVARk to operate in as, in most cases, all possible terms in \(_{NVAR}\) can be considered without exceeding \(_{r}\) and the effect of collinearity is minimized. NVARk and SINK appear indistinguishable under a Friedman + Nemenyi statistical test, which places NVARk under the SOTA in terms of accuracy. Interestingly, the difference between NVARk and rmESN is significant at the 95% level with p-value\({}_{-}=0.013\). Results for NVARk* show that the performance of NVARk can be very efficiently improved by a CV optimization of just the two integer embedding hyperparameters.

### Execution time and scalability

In terms of execution time, the scalability properties of NVARk (Sec. 3.4) manifest in exceptional performance. In Fig. 2 (left) and Tab. 1 (last row), we compare running times to compute the train-train and the train-test kernel matrices and observe that the improvement of NVARk is consistent across datasets and baselines, with speedup factors up to \( 10^{3}\) in the high-\(T\) and high-\(N\) regimes.

Results for NVARk* can be achieved with a slowdown factor of \( 25\) (parallelized grid search + 1 iteration with optimal parameters) with respect to the heuristic-based NVARk. Interestingly, for 113/130 datasets, NVARk* execution time is faster than a single iteration for rmESN.

For a study of the scaling, in Fig. 2 (middle, right) we progressively vary the length of the _Abnormal-Hearbeat_ dataset (by interpolation) and the number of training samples (by random sampling) in the _Crop_ dataset. Results show that NVARk performs exceptionally well in all regimes, while rmESN

    & NVARk* & NVARk & rmESN & TCK & SINK \\  Average score & 0.803 & **0.779** & 0.757 & 0.726 & 0.771 \\ Average rank & / & **2.14** & 2.62 & 2.85 & 2.20 \\ N first ranked & / & **52** & 26 & 23 & 43 \\ Median time (s) / dataset & 14.27 & **0.59** & 40.87 & 44.24 & 23.73 \\   

Table 1: SVM classification of UTS datasets, performance metrics across 130 datasets.

scales poorly with the length of the series and TCK and SINK with the size of the training sample. NVARk, instead, is advantaged by its non-recursive structure and linear complexity in \(N\).

### SVM classification of MTS

We present here SVM classification over the 19 MTS datasets. We do not include SINK as there is no extension or evaluation provided for MTS. Aside from that, the evaluation is conducted identically to the univariate case.

For multivariate datasets, we expect to drift away from the ideal operating scenario of NVARk. In particular, all the possible dynamic features may not be equally effective and, by randomly selecting a few, important correlations may be missed or spurious ones captured. Despite that, the experiments in Tab. 2 show surprisingly good results, even for datasets for which \(_{r}\) is just a fraction of all possible concatenation terms. We believe that this finding strengthens the connection to the generalized Takens' theorem. It is also interesting to notice good performance in diverse cases of time series that do not immediately arise from dynamical systems. Exploring this aspect constitutes interesting future works, e.g., time series representation learning and state space reconstruction as a possible underpinning.

   Dataset & NVARk* & NVARk & rMESN & TCK \\  SpokenArabicDig. & \(0.980_{ 0.004}\) & \(0.980_{ 0.003}\) & \(0.948_{ 0.004}\) & \(_{ 0.002}\) \\ JapaneseVowels & \(0.981_{ 0.008}\) & \(_{ 0.007}\) & \(0.968_{ 0.004}\) & \(0.961_{ 0.004}\) \\ PenDigits & \(0.986_{ 0.000}\) & \(_{ 0.000}\) & \(0.976_{ 0.001}\) & \(0.954_{ 0.001}\) \\ RacketSports & \(0.889_{ 0.019}\) & \(0.847_{ 0.007}\) & \(0.859_{ 0.008}\) & \(_{ 0.006}\) \\ LSST & \(0.566_{ 0.009}\) & \(0.543_{ 0.006}\) & \(_{ 0.009}\) & \(0.474_{ 0.008}\) \\ Libras & \(0.972_{ 0.000}\) & \(_{ 0.000}\) & \(0.824_{ 0.010}\) & \(0.793_{ 0.010}\) \\ FingerMovements & \(0.572_{ 0.034}\) & \(_{ 0.032}\) & \(0.576_{ 0.022}\) & \(0.505_{ 0.030}\) \\ NATOPS & \(0.896_{ 0.020}\) & \(_{ 0.019}\) & \(0.828_{ 0.015}\) & \(0.806_{ 0.016}\) \\ CharacterTraj. & \(0.982_{ 0.000}\) & \(0.915_{ 0.008}\) & \(_{ 0.002}\) & \(0.974_{ 0.002}\) \\ ERing & \(0.898_{ 0.021}\) & \(0.864_{ 0.025}\) & \(0.814_{ 0.014}\) & \(_{ 0.005}\) \\ BasicMotions & \(1.000_{ 0.000}\) & \(0.938_{ 0.027}\) & \(0.995_{ 0.011}\) & \(_{ 0.000}\) \\ ArticularyWordRec. & \(0.978_{ 0.008}\) & \(0.976_{ 0.005}\) & \(_{ 0.006}\) & \(0.983_{ 0.003}\) \\ Epilepsy & \(0.986_{ 0.000}\) & \(_{ 0.000}\) & \(0.971_{ 0.005}\) & \(0.947_{ 0.005}\) \\ UWAVEgestElib. & \(0.946_{ 0.003}\) & \(_{ 0.011}\) & \(0.862_{ 0.007}\) & \(0.868_{ 0.007}\) \\ SelfRegulationSCP1 & \(0.746_{ 0.017}\) & \(0.688_{ 0.032}\) & \(0.769_{ 0.019}\) & \(_{ 0.007}\) \\ SelfRegulationSCP2 & \(0.566_{ 0.017}\) & \(_{ 0.028}\) & \(0.511_{ 0.027}\) & \(0.488_{ 0.014}\) \\ Cricket & \(0.979_{ 0.010}\) & \(_{ 0.000}\) & \(0.982_{ 0.006}\) & \(0.958_{ 0.000}\) \\ StandWalkJump & \(0.433_{ 0.047}\) & \(_{ 0.049}\) & \(0.373_{ 0.056}\) & \(0.433_{ 0.079}\) \\ EigenWorms & \(0.944_{ 0.009}\) & \(_{ 0.012}\) & \(0.939_{ 0.013}\) & \(0.594_{ 0.000}\) \\  Average score & \(0.858\) & \(\) & \(0.830\) & \(0.809\) \\ Average rank & / & \(\) & \(2.05\) & \(2.26\) \\ N first ranked & / & \(\) & \(3\) & \(5\) \\ Median time (s) / dataset & \(19.67\) & \(\) & \(26.47\) & \(184.08\) \\   

Table 2: SVM classification of MTS datasets. Average across 10 seeds. Best accuracy is in bold.

Figure 2: **Left:** For all univariate datasets, the plot shows the ratio between the execution time to compute the train-train and train-test kernels of different methods and NVARk. **Middle**: Scalability of kernels with the series length. **Right**: Scalability of kernels with the training size.

Finally, we observe that the two hyperparameter configurations, NVARk* and NVARk, tend to be ranked next to each other. This supports the quality of the proposed general setting, which allows the kernel to extract meaningful features while preserving great computational efficiency.

### Ablation study

In order to tease out which NVARk component is most important, we here present an ablation study. We separately consider fitting the linear readout directly on the input time series (no concatenation), sampling only linear terms from Eq. 5, and sampling only non-linear terms. This is an interesting ablation study that, to the best of our knowledge, has not been investigated in previous works. Results are presented in Tab. 3.

As a general trend, we observe that using all terms leads to the best results, followed by _non-linear only_, _linear only_, and fitting directly on the input series. Note also that results are in line with our interpretation using the generalized Taken's theorem. For univariate datasets, _linear only_ tends to underperform, as the total number of concatenated dimensions is very small (equal to \(k\)) and may be not sufficient in reconstructing the underlying manifold. In the multivariate case, the two variants perform very similarly, as the lags of all attributes may already constitute a considerable pool of terms.

To conclude, we consider an additional variant of Eq. 5 in which we first concatenate all the lag terms, and then fill the remaining dimensions, up to \(_{r}\), by sampling nonlinear terms. Interestingly, for multivariate datasets, this leads to slightly better performance and a reduction in variance. We believe this might eventually compensate for the imbalance between all possible linear and non-linear terms, while also reducing the chance of adding noise from uncorrelated dimensions.

## 5 Conclusions

We have proposed a kernel for time series that integrates the NVAR framework into reservoir-based kernel architectures. NVARk compares time series based on the linear dynamics of NVAR embeddings, which are built from concatenating lags and nonlinear functionals to the original series. In terms of accuracy, NVARk outperforms the corresponding RC architecture. Computationally, it is exceptionally efficient and based on a few integer hyperparameters, which together allow for further improvement of the results with simple supervised grid-based optimization.

As for future work directions, we believe it would be effective to target specific weaknesses of NVARk. In particular, as the input dimensionality of the time series approaches the maximum embedding dimension (\(_{r}\)), the proposed approach finds little margin for concatenating more lags and non-linear terms. Such cases would require substantially increasing \(_{r}\) and, consequently, place excessive strain on the linear readout, which inherently possesses limited expressive capacity. We then expect the effectiveness of NVARk to decrease. To overcome this, one can consider replacing the random sampling of dimensions with more refined strategies that prioritize the selection of meaningful terms. Alternatively, it would be interesting to explore how different linear layers, e.g. Lasso, would perform in this regime. Alongside these, different unsupervised strategies can be considered to learn optimal values for the embedding parameters. In particular, we note an unexplored affinity with _signature transforms_(Chevyrev & Kormilitzin, 2016), which we plan on deepening to infer optimal settings for the maximum dimensionality of the NVAR embedding.

    & & no concat & linear only & non-linear only & all & balanced \\  SVM score for & mean & \(0.536\) & \(0.707\) & \(0.764\) & \(0.779\) & \(0.779\) \\ univariate datasets & std & \(0.010\) & \(0.008\) & \(0.006\) & \(0.006\) & \(0.006\) \\  SVM score for & mean & 0.765 & 0.840 & 0.840 & 0.848 & 0.849 \\ multivariate datasets & std & 0.012 & 0.008 & 0.013 & 0.014 & 0.012 \\   

Table 3: Ablation study across 130 UCR univariate and 19 multivariate UEA datasets