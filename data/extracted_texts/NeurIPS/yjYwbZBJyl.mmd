# Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension

Moritz Haas\({}^{1}\) David Holzmuller\({}^{2}\) Ulrike von Luxburg\({}^{1}\) Ingo Steinwart\({}^{2}\)

\({}^{1}\)University of Tubingen and Tubingen AI Center, Germany

\({}^{2}\)Institute for Stochastics and Applications, University of Stuttgart, Germany

{mo.haas,ulrike.luxburg}@uni-tuebingen.de

{david.holzmueller,ingo.steinwart}@mathematik.uni-stuttgart.de

Equal contribution.

###### Abstract

The success of over-parameterized neural networks trained to near-zero training error has caused great interest in the phenomenon of benign overfitting, where estimators are statistically consistent even though they interpolate noisy training data. While benign overfitting in fixed dimension has been established for some learning methods, current literature suggests that for regression with typical kernel methods and wide neural networks, benign overfitting requires a high-dimensional setting where the dimension grows with the sample size. In this paper, we show that the smoothness of the estimators, and not the dimension, is the key: benign overfitting is possible if and only if the estimator's derivatives are large enough. We generalize existing inconsistency results to non-interpolating models and more kernels to show that benign overfitting with moderate derivatives is impossible in fixed dimension. Conversely, we show that rate-optimal benign overfitting is possible for regression with a sequence of spiky-smooth kernels with large derivatives. Using neural tangent kernels, we translate our results to wide neural networks. We prove that while infinite-width networks do not overfit benignly with the ReLU activation, this can be fixed by adding small high-frequency fluctuations to the activation function. Our experiments verify that such neural networks, while overfitting, can indeed generalize well even on low-dimensional data sets.

## 1 Introduction

While neural networks have shown great practical success, our theoretical understanding of their generalization properties is still limited. A promising line of work considers the phenomenon of benign overfitting, where researchers try to understand when and how models that interpolate noisy training data can generalize (Zhang et al., 2021; Belkin et al., 2018, 2019). In the high-dimensional regime, where the dimension grows with the number of sample points, consistency of minimum-norm interpolants has been established for linear models and kernel regression (Hastie et al., 2022; Bartlett et al., 2020; Liang and Rakhlin, 2020; Bartlett et al., 2021). In fixed dimension, minimum-norm interpolation with standard kernels is inconsistent (Rakhlin and Zhai, 2019; Buchholz, 2022).

In this paper, we shed a differentiated light on benign overfitting with kernels and neural networks. We argue that the dimension-dependent perspective does not capture the full picture of benign overfitting. In particular, we show that harmless interpolation with kernel methods and neural networks is possible, even in small fixed dimension, with adequately designed kernels and activation functions. The key is to properly design estimators of the form'signal+spike'. While minimum-norm criteria have widely been considered a useful inductive bias, we demonstrate that designing unusual norms can resolve the shortcomings of standard norms. For wide neural networks, harmless interpolation can berealized by adding tiny fluctuations to the activation function. Such networks do not require explicit regularization and can simply be trained to overfit (Figure 1).

On a technical level, we additionally prove that overfitting in kernel regression can only be consistent if the estimators have large derivatives. Using neural tangent kernels or neural network Gaussian process kernels, we can translate our results from kernel regression to the world of neural networks (Neal, 1996; Jacot et al., 2018). In particular, our results enable the design of activation functions that induce benign overfitting in fixed dimension: the spikes in kernels can be translated into infinitesimal fluctuations that can be added to an activation function to achieve harmless interpolation with neural networks. Such small high frequency oscillations can fit noisy observations without affecting the smooth component too much. Training finite neural networks with gradient descent shows that spiky-smooth activation functions can indeed achieve good generalization even when interpolating small, low-dimensional data sets (Figure 1 b,c).

Thanks to new technical contributions, our inconsistency results significantly extend existing ones. We use a novel noise concentration argument (Lemma D.6) to generalize existing inconsistency results on minimum-norm interpolants to the much more realistic regime of overfitting estimators with comparable Sobolev norm scaling, which includes training via gradient flow and gradient descent with "late stopping" as well as low levels of ridge regularization. Moreover, a novel connection to eigenvalue concentration results for kernel matrices (Proposition 5) allows us to relax the smoothness assumption and to treat heteroscedastic noise in Theorem 6. Lastly, our Lemma E.1 translates inconsistency results from bounded open subsets of \(^{d}\) to the sphere \(^{d}\), which leads to results for the neural tangent kernel and neural network Gaussian processes.

## 2 Setup and prerequisites

**General approach.** We consider a general regression problem on \(^{d}\) with an arbitrary, fixed dimension \(d\) and analyze kernel-based approaches to solve this problem: kernel ridge regression, kernel gradient flow and gradient descent, minimum-norm interpolation, and more generally, overfitting norm-bounded estimators. We then translate our results to neural networks via the neural network Gaussian process and the neural tangent kernel. Let us now introduce the formal framework.

**Notation.** We denote scalars by lowercase letters \(x\), vectors by bold lowercase letters \(\) and matrices by bold uppercase letters \(\). We denote the eigenvalues of \(\) as \(_{1}()_{n}()\) and the Moore-Penrose pseudo-inverse by \(^{+}\). We say that a probability distribution \(P\) has lower and upper bounded density if its density \(p\) satisfies \(0<c<p()<C\) for suitable constants \(c,C\) and all \(\) on a given domain.

**Regression setup.** We consider a data set \(D=((_{1},y_{1}),,(_{n},y_{n}))(^{d})^{n}\) with i.i.d. samples \((_{i},y_{i}) P\), written as \(D P^{n}\), where \(P\) is a probability distribution on \(^{d}\). We define \((_{1},,_{n})\) and \((y_{1},,y_{n})^{}^{n}\). Random variables \((,y) P\) denote

Figure 1: **Spiky-smooth overfitting in 2 dimensions.****a.** We plot the predicted function for ridgeless kernel regression with the Laplace kernel (blue) versus our spiky-smooth kernel (4) with Laplace components (orange) on \(^{1}\). The dashed black line shows the true regression function, black ’x’ denote noisy training points. Further details can be found in Section 6.2. **b.** The predicted function of a trained 2-layer neural network with ReLU activation (blue) versus ReLU plus shifted high-frequency sin-function (8) (orange). Using the weights learned with the spiky-smooth activation function in a ReLU network (green) disentangles the spike component from the signal component. **c.** Training error (solid lines) and test error (dashed lines) over the course of training for b. evaluated on \(10^{4}\) test points. The dotted black line shows the optimal test error. The spiky-smooth activation function does not require regularization and can simply be trained to overfit.

test points independent of \(D\), and \(P_{X}\) denotes the probability distribution of \(\). The (least squares) _empirical risk_\(R_{D}\) and _population risk_\(R_{P}\) of a function \(f:^{d}\) are defined as

\[R_{D}(f)_{i=1}^{n}(y_{i}-f(x_{i}))^{2}, R_{P}(f) _{,y}[(y-f())^{2}]\.\]

We assume \((y|)<\) for all \(\). Then, \(R_{P}\) is minimized by the target function \(f_{P}^{*}()=[y|]\), and the _excess risk_ of a function \(f\) is given by

\[R_{P}(f)-R_{P}(f_{P}^{*})=_{}(f_{P}^{*}()-f())^{2}\.\]

We call a data-dependent estimator \(f_{D}\)_consistent for \(P\)_ if its excess risk converges to \(0\) in probability, that is, for all \(>0\), \(_{n}P^{n}(D(^{d})^{n} R_{P }(f_{D})-R_{P}(f_{P}^{*}))=0\). We call \(f_{D}\)_consistent in expectation for \(P\)_ if \(_{n}_{D}R_{P}(f_{D})-R_{P}(f_{P}^{*})=0\). We call \(f_{D}\)_universally consistent_ if is it consistent for all Borel probability measures \(P\) on \(^{d}\).

**Solutions by kernel regression.** Recall that a kernel \(k\) induces a reproducing kernel Hilbert space \(_{k}\), abbreviated RKHS (more details in Appendix B). For \(f_{k}\), we consider the objective

\[_{}(f)_{i=1}^{n}(y_{i}-f(_{i}))^ {2}+\|f\|_{_{k}}^{2}\]

with regularization parameter \( 0\). Denote by \(f_{t,}\) the solution to this problem that is obtained by optimizing on \(_{}\) in \(_{k}\) with gradient flow until time \(t[0,]\), using fixed a regularization constant \(>0\), and initializing at \(f=0_{k}\). We show in Appendix C.1 that it is given as

\[f_{t,}() k(,)(_{n}-e^{-t (k(,)+ n_{n})})(k(,)+ n_{n})^{-1}\,\] (1)

where \(k(,)\) denotes the row vector \((k(,_{i}))_{i[n]}\) and \(k(,)=(k(_{i},_{j}))_{i,j[n]}\) the kernel matrix. \(f_{t,}\) elegantly subsumes several popular kernel regression estimators as special cases: (i) classical kernel ridge regression for \(t\), (ii) gradient flow on the unregularized objective for \( 0\), and (iii) kernel "ridgeless" regression \(f_{,0}()=k(,)k(,)^{+}\) in the joint limit of \( 0\) and \(t\). If \(k(,)\) is invertible, \(f_{,0}\) is the interpolating function \(f_{k}\) with the smallest \(_{k}\)-norm.

**From kernels to neural networks: the neural tangent kernel (NTK) and the neural network Gaussian process (NNGP).** Denote the output of a NN with parameters \(\) on input \(\) by \(f_{}()\). It is known that for suitable random initializations \(_{0}\), in the infinite-width limit the random initial function \(f_{_{0}}\) converges in distribution to a Gaussian Process with the so-called Neural Network Gaussian Process (NNGP) kernel (Neal, 1996; Lee et al., 2018; Matthews et al., 2018). In Bayesian inference, the posterior mean function is then of the form \(f_{,}\). With minor modifications (Arora et al., 2019; Zhang et al., 2020), training infinitely wide NNs with gradient flow corresponds to learning the function \(f_{t,0}\) with the neural tangent kernel (NTK) (Jacot et al., 2018; Lee et al., 2019). If only the last layer is trained, the NNGP kernel should be used instead (Daniely et al., 2016). For ReLU activation functions, the RKHS of the infinite-width NNGP and NTK on the sphere \(^{d}\) is typically a Sobolev space (Bietti and Bach, 2021; Chen and Xu, 2021), see Appendix B.4. Using other parametrizations induces feature learning infinite-width limits for neural networks (Yang and Hu, 2021); an analysis of such neural network algorithms is left for future work.

## 3 Related work

We here provide a short summary of related work. A more detailed account is provided in Appendix A.

**Kernel regression.** With appropriate regularization, kernel ridge regularization with typical universal kernels like the Gauss, Matern, and Laplace kernels is universally consistent (Steinwart and Christmann, 2008, Chapter 9). Optimal rates in Sobolev RKHS can also be achieved using cross-validation of the regularization \(\)(Steinwart et al., 2009) or early stopping rules (Yao et al., 2007; Raskutti et al., 2014; Wei et al., 2017). The above kernels as well as NTKs and NNGPs of standard fully-connected neural networks are rotationally invariant. In the high-dimensional regime, the class of functions that is learnable with rotation-invariant kernels is quite limited (Donhauser et al., 2021; Ghorbani et al., 2021; Liang et al., 2020).

**Inconsistency results.** Besides Rakhlin and Zhai (2019) and Buchholz (2022), Beaglehole et al. (2023) derive inconsistency results for ridgeless kernel regression given assumptions on the spectral tail in the Fourier basis, and contemporaneously propose a special case of our spiky-smooth kernel sequence to mimic kernel ridge regression without providing any quantitative statements. Li et al. (2023) show that polynomial convergence is impossible for common kernels including ReLU NTKs. Mallinar et al. (2022) conjecture inconsistency for interpolation with ReLU NTKs based on their semi-rigorous result, which essentially assumes that the eigenfunctions can be replaced by structureless Gaussian random variables. Lai et al. (2023) show an inconsistency-type result for overfitting two-layer ReLU NNs with \(d=1\), but for fixed inputs \(\). They also note that an earlier inconsistency result by Hu et al. (2021) relies on an unproven result. Mucke and Steinwart (2019) show that global minima of NNs can overfit both benignly and harmfully, but their result does not apply to gradient descent training. Overfitting with typical linear models around the interpolation peak is inconsistent (Ghosh and Belkin, 2022, Holzmuller, 2021).

**Classification.** For binary classification, benign overfitting is a more generic phenomenon than for regression (Muthukumar et al., 2021, Shamir, 2022), and consistency has been shown under linear separability assumptions (Montanari et al., 2019, Chatterji and Long, 2021, Frei et al., 2022), through complexity bounds for reference classes (Cao and Gu, 2019, Chen et al., 2021) or as long as the total variation distance of the class conditionals is sufficiently large and \(f^{*}()=[y|]\) lies in the RKHS with bounded norm (Liang and Recht, 2023). Chapter 8 of Steinwart and Christmann (2008) discusses how the overlap of the two classes may influence learning rates under positive regularization.

## 4 Inconsistency of overfitting with common kernel estimators

We consider a regression problem on \(^{d}\) in arbitrary, fixed dimension \(d\) that is solved by kernel regression. In this section, we derive several new results, stating that overfitting estimators with moderate Sobolev norm are inconsistent, in a variety of settings. In the next section, we establish the other direction: overfitting estimators can be consistent when we adapt the norm that is minimized.

### Beyond minimum-norm interpolants: general overfitting estimators with bounded norm

Existing generalization bounds often consider the perfect minimum norm interpolant. This is a rather theoretical construction; estimators obtained by training with gradient descent algorithms merely overfit and, in the best case, approximate interpolants with small norm. In this section, we extend existing bounds to arbitrary overfitting estimators whose norm does not grow faster than the minimum norm that would be required to interpolate the training data. Before we can state the theorem, we need to establish some technical assumptions.

Assumptions on the data generating process.The following assumptions (as in Buchholz (2022)) allow for quite general domains and distributions. They are standard in nonparametric statistics.

* Let \(P_{X}\) be a distribution on a bounded open Lipschitz domain \(^{d}\) with lower and upper bounded Lebesgue density. Consider data sets \(D=\{(_{1},y_{1}),,(_{n},y_{n})\}\), where \(_{i} P_{X}\) i.i.d. and \(y_{i}=f^{*}(_{i})+_{i}\), where \(_{i}\) is i.i.d. Gaussian noise with positive variance \(^{2}>0\) and \(f^{*} C_{c}^{}()\{0\}\) denotes a smooth function with compact support.

Assumptions on the kernel.Our assumption on the kernel is that its RKHS is equivalent to a Sobolev space. For integers \(s\), the norm of a Sobolev space \(H^{s}()\) can be defined as

\[\|f\|_{H^{s}()}^{2}_{0|| s}\|D^{}f\|_{ L_{2}()}^{2},\]

where \(D^{}\) denotes partial derivatives in multi-index notation for \(\). It measures the magnitude of derivatives up to some order \(s\). For general \(s>0\), \(H^{s}()\) is (equivalent to) an RKHS if and only if \(s>d/2\). For example, Laplace and Matern kernels (Kanagawa et al., 2018, Example 2.6) have Sobolev RKHSs. The RKHS of the Gaussian kernel \(^{}\) is contained in every Sobolev space, \(^{} H^{s}\) for all \(s 0\)(Steinwart and Christmann, 2008, Corollary 4.36). Due to its smoothness, the Gaussian kernel is potentially even more prone to harmful overfitting than Sobolev kernels (Mallinar et al., 2022). We make the following assumption on the kernel:* Let \(k\) be a positive definite real function whose RKHS \(_{k}\) is equivalent to the Sobolev space \(H^{s}\) for some \(s(,]\).

Now we are ready to state the main result of this section:

**Theorem 1** (**Overfitting estimators with small norms are inconsistent**).: _Let assumptions (D1) and (K) hold. Let \(c_{}(0,1]\) and \(C_{}>0\). Then, there exist \(c>0\) and \(n_{0}\) such that the following holds for all \(n n_{0}\) with probability \(1-O(1/n)\) over the draw of the data set \(D\) with \(n\) samples: Every function \(f_{k}\) that satisfies the following two conditions_

* \(_{i=1}^{n}(f(x_{i})-y_{i})^{2}(1-c_{}) ^{2}\) _(training error of_ \(f\) _is below Bayes risk_)__
* \(\|f\|_{_{k}} C_{}\|f_{,0}\|_{_{ k}}\) _(norm comparable to minimum-norm interpolant (_1_)),_

_has an excess risk that satisfies_

\[R_{P}(f)-R_{P}(f^{*}) c^{2}>0\;.\] (2)

In words: In fixed dimension \(d\), every differentiable function \(f\) that overfits the training data and is not much "spikier" than the minimum RKHS-norm interpolant is inconsistent!

**Proof idea.** Our proof follows a similar approach as Rakhlin and Zhai (2019), Buchholz (2022), and also holds for kernels with adaptive bandwidths. For small bandwidths, \(\|f_{,0}\|_{L_{2}(P_{X})}\|f^{*}\|_{L_{2}(P_{X})}\) because \(f_{,0}\) decays to \(0\) between the training points, which shows that purely'spiky' estimators are inconsistent. In this case, the lower bound is independent of \(^{2}\). For all other bandwidths, interpolating \((n)\) many noisy labels \(y_{i}\) incurs \((1)\) error in an area of volume \((1/n)\) around \((n)\) data points with high probability, which accumulates to a total error \((1)\). Our observation is that the same logic holds when overfitting by a constant fraction. Formally, we show that \(f^{*}\) and \(f\) must then be separated by a constant on a constant fraction of training points, with high probability, by using the fact that a constant fraction of the total noise cannot concentrate on less than \((n)\) noise variables, with high probability (Lemma D.6). The full proof can be found in Appendix D. 

Assumption (O) is necessary in Theorem 1, because optimally regularized kernel ridge regression fulfills all other assumptions of Theorem 1 while achieving consistency with minimax optimal convergence rates (see Section 3). The necessity of Assumption (N) is demonstrated by Section 5.

The following proposition establishes that Theorem 1 covers the entire overfitting regime of the popular (regularized) gradient flow estimators \(f_{t,}\) for all times \(t[0,]\) and any regularization \( 0\). The proof in Appendix C.2 also covers gradient descent.

**Proposition 2** (**Popular estimators fulfill the norm bound (N)**).: _For arbitrary \(t[0,]\) and \([0,)\), \(f_{t,}\) as defined in (1) fulfills Assumption (N) with \(C_{}=1\)._

**Remark 3** (**Dimension dependency**).: Some works argue that for specific sequences of kernels \((k_{d})_{d}\), the constant \(c\) in Theorem 1 decreases with increasing dimension \(d\)(Liang et al., 2020, Liang and Rakhlin, 2020, Mallinar et al., 2022). In Theorem 1, if the equivalence constants in Assumption (K) are uniformly bounded in \(d\), the behavior in \(d\) might still depend on the definition of the Sobolev norms. Overall, similar to Rakhlin and Zhai (2019) and Buchholz (2022), our proof techniques do not allow to easily obtain a dependence on \(d\). 

### Inconsistency of overfitting with neural kernels

We would now like to apply the above results to neural kernels, which would allow us to translate our inconsistency results from the kernel domain to neural networks. However, to achieve this, we need to take one more technical hurdle: the equivalence results for NTKs and NNGPs only hold for probability distributions on the sphere \(^{d}\) (detailed summary in Appendix B.4). Lemma E.1 provides the missing technical link: It establishes a smooth correspondence between the respective kernels, Sobolev spaces, and probability distributions. The inconsistency of overfitting with (deep) ReLU NTKs and NNGP kernels then immediately follows from adapting Theorem 1 via Lemma E.1.

**Theorem 4** (**Overfitting with neural network kernels in fixed dimension is inconsistent**).: _Let \(c(0,1)\), and let \(P\) be a probability distribution with lower and upper bounded Lebesgue density on an arbitrary spherical cap \(T\{^{d} x_{d+1}<v\}^{d}\), \(v(-1,1)\). Let \(k\) either be_

* _the fully-connected ReLU NTK with_ \(0\)_-initialized biases of any fixed depth_ \(L 2\)_, and_ \(d 2\)_, or_
* _the fully-connected ReLU NNGP kernel without biases of any fixed depth_ \(L 3\)_, and_ \(d 6\)_Then, if \(f_{t,}\) fulfills Assumption \((O)\) with probability at least \(c\) over the draw of the data set \(D\), \(f_{t,}\) is inconsistent for \(P\)._

Theorem 4 also holds for more general estimators as in Theorem 1, cf. the proof in Appendix E.

Mallinar et al. (2022) already observed empirically that overfitting common network architectures yields suboptimal generalization performance on large data sets in fixed dimension. Theorem 4 now provides a rigorous proof for this phenomenon since sufficiently wide trained neural networks and the corresponding NTKs have a similar generalization behavior (e.g. (Arora et al., 2019, Theorem 3.2)).

### Relaxing smoothness and noise assumptions via spectral concentration bounds

In this section, we consider a different approach to derive lower bounds for the generalization error of overfitting kernel regression: through concentration results for the eigenvalues of kernel matrices. On a high level, we obtain similar results as in the last section. The novelty of this section is on the technical side, and we suggest that non-technical readers skip this section in their first reading.

We define the convolution kernel of a given kernel \(k\) as \(k_{*}(,^{}) k(,^{})k( ^{},^{})\,P_{X}(^{})\), which is possible whenever \(k(,) L_{2}(P_{X})\) for all \(\). The latter condition is satisfied for bounded kernels. Our starting point is the following new lower bound:

**Proposition 5** (**Spectral lower bound**).: _Assume that the kernel matrix \(k(,)\) is almost surely positive definite, and that \((y|)^{2}\) for \(P_{X}\)-almost all \(\). Then, the expected excess risk satisfies_

\[_{D}R_{P}(f_{t,})-R_{P}^{*}}{n}_{i=1}^{ n}_{}(k_{*}(,)/n)(1-e^{-2t( _{i}(k(,)/n)+)})^{2}}{(_{i}(k(,)/n)+)^{2}}\;.\] (3)

Using concentration inequalities for kernel matrices and the relation between the integral operators of \(k\) and \(k_{*}\), it can be seen that for \(t=\) and \(=0\), every term in the sum in Eq. (3) should converge to \(1\) as \(n\). However, since the number of terms in the sum increases with \(n\) and the convergence may not be uniform, this is not sufficient to show inconsistency in expectation. Instead, relative concentration bounds that are even stronger than the ones by Valdivia (2018) would be required to show inconsistency in expectation. However, by combining multiple weaker bounds and further arguments on kernel equivalences, we can still show inconsistency in expectation for a class of dot-product kernels on the sphere, including certain NTK and NNGP kernels (Appendix B.4):

**Theorem 6** (**Inconsistency for Sobolev dot-product kernels on the sphere**).: _Let \(k\) be a dot-product kernel on \(^{d}\), i.e., a kernel of the form \(k(,^{})=(,^{})\), such that its RKHS \(_{k}\) is equivalent to a Sobolev space \(H^{*}(^{d})\), \(s>d/2\). Moreover, let \(P\) be a distribution on \(^{d}\) such that \(P_{X}\) has a lower and upper bounded density w.r.t. the uniform distribution \((^{d})\), and such that \((y|)^{2}>0\) for \(P_{X}\)-almost all \(^{d}\). Then, for every \(C>0\), there exists \(c>0\) independent of \(^{2}\) such that for all \(n 1\), \(t(C^{-1}n^{2s/d},]\), and \([0,Cn^{-2s/d})\), the expected excess risk satisfies_

\[_{D}R_{P}(f_{t,})-R_{P}^{*} c^{2}>0\;.\]

The assumptions of Theorem 6 and Theorem 4 differ in several ways. Theorem 6 applies to arbitrarily high smoothness \(s\) and therefore to ReLU NTKs and NNGPs in arbitrary dimension \(d\). Moreover, it applies to distributions on the whole sphere and allows more general noise distributions. On the flip side, it only shows inconsistency in expectation, which we believe could be extended to inconsistency for Gaussian noise. Moreover, it only applies to functions of the form \(f_{t,}\) but provides an explicit bound on \(t\) and \(\) to get inconsistency. For \(t=\), the bound \(=O(n^{-2s/d})\) appears to be tight, as larger \(\) yield consistency for comparable Sobolev kernels on \(^{d}\)(Steinwart et al., 2009, Corollary 3).

We only prove Theorem 6 for dot-product kernels on the sphere since we can show for these kernels that \(_{k_{*}}\) is equivalent to a Sobolev space (Lemma F.13), while this is not true for open domains \(\)(Schaback, 2018). However, an improved understanding of \(_{k_{*}}\) for such \(\) could potentially allow to extend our proof to the non-spherical case.

The spectral lower bounds in Theorem F.2 show that our approach can directly benefit from developing better kernel matrix concentration inequalities. Conversely, the investigation of consistent kernel interpolation might provide information about where such concentration inequalities do not hold.

Consistency via spiky-smooth estimators - even in fixed dimension

In Section 4, we have seen that when common kernel estimators overfit, they are inconsistent for many kernels and a wide variety of distributions. We now design consistent interpolating kernel estimators. The key is to violate Assumption (N) for every fixed Sobolev RKHS norm \(\|\|_{_{k}}\) and introduce an inductive bias towards learning spiky-smooth functions.

### Almost universal consistency of spiky-smooth ridgeless kernel regression

In high dimensional regimes (where the dimension \(d\) is supposed to grow with the number of data points), benign overfitting of linear and kernel regression has been understood by an additive decomposition of the minimum-norm interpolant into a smooth regularized component that is responsible for good generalization, and a spiky component that interpolates the noisy data points while not harming generalization (Bartlett et al., 2021). This inspires us to enforce such a decomposition in arbitrary fixed dimension by adding a sharp kernel spike \(_{_{n}}\) to a common kernel \(\). In this way, we can still generate any Sobolev RKHS (see Appendix G.2).

**Definition 7** (Spiky-smooth kernel).: Let \(\) denote any universal kernel function on \(^{d}\). We call it the smooth component. Consider a second, translation invariant kernel \(_{}\) of the form \(k_{}(,)=q(-}{})\), for some function \(q:^{d}\). We call it the spiky component. Then we define the \(\)_-regularized spiky-smooth kernel with spike bandwidth_\(\) as

\[k_{,}(,)=(,)+_{ }(,),,^{d}.\] (4)

We now show that the minimum-norm interpolant of the spiky-smooth kernel sequence with properly chosen \(_{n},_{n} 0\) is consistent for a large class of distributions, on a space with fixed (possibly small) dimension \(d\). We establish our result under the following assumption (as in Mucke and Steinwart (2019)), which is weaker than our previous Assumption (D1).

* There exists a constant \(_{X}>0\) and a continuous function \(:[0,)\) with \((0)=0\) such that the data generating probability distribution satisfies \(P_{X}(B_{t}())(t)=O(t^{_{X}})\) for all \(\) and all \(t 0\) (here \(B_{t}()\) denotes the Euclidean ball of radius \(t\) around \(\)).

**Theorem 8** (Consistency of spiky-smooth ridgeless kernel regression).: _Assume that the training set \(D\) consists of \(n\) i.i.d. pairs \((,y) P\) such that the marginal \(P_{X}\) fulfills (D2) and \(y^{2}<\). Let the kernel components satisfy:_

* \(\) _is a universal kernel, and_ \(_{n} 0\) _and_ \(n_{n}^{4}\)_._
* \(_{_{n}}\) _denotes the Laplace kernel with a sequence of positive bandwidths_ \((_{n})\) _fulfilling_ \(_{n} n^{-}((+) n)^{-1}\)_, where_ \(>0\) _arbitrary._

_Then the minimum-norm interpolant of the \(_{n}\)-regularized spiky-smooth kernel sequence \(k_{n} k_{_{n},_{n}}\) is consistent for \(P\)._

**Remark 9** (Benign overfitting with optimal convergence rates).: Suppose that we have a Sobolev target function \(f^{*} H^{s^{*}}()\{0\}\), that the noise satisfies a moment condition and that \(P_{X}\) has an upper- and lower-bounded density on a Lipschitz domain or the sphere. Then, we show in Theorem G.5 that, by using smooth components \(\) whose RKHS is equivalent to a Sobolev space \(H^{s}\), \(s>(s^{*},d/2)\), and choosing the spike components \(_{_{n}}\) as in Theorem 8, the minimum-norm interpolant of \(k_{n} k_{_{n},_{n}}\) achieves the convergence rate \(n^{-}{(s^{*}+d/2)}}^{2}(n)\) when choosing the quasi-regularization \(_{n}\) properly. Moreover, for \(s^{*}>d/2\), this rate is known to be optimal up to the factor \(^{2}(n)\) (Remark G.6). Since optimal rates can be achieved both with optimal regularization and with interpolation, our results show that in Sobolev RKHSs, overfitting is neither intrinsically helpful nor harmful for generalization with the right choice of kernel function.

Figure 2: The spiky-smooth kernel with Laplace components (orange) consists of a Laplace kernel (blue) plus a Laplace kernel of height \(\) and small bandwidth \(\).

Proof idea.With sharp spikes \( 0\), it holds that \(_{}(,)_{n}\), with high probability. Hence, ridgeless kernel regression with the spiky-smooth kernel interpolates the training set while approximating kernel ridge regression with the smooth component \(\) and regularization \(\). 

The theorem even holds under much weaker assumptions on the decay behavior of the spike component \(_{_{n}}\), including Gaussian and Matern kernels. The full version of the theorem and its proof can be found in Appendix G. It also applies to kernels and distributions on the sphere \(^{d}\).

**Remark 10** (**Interplay between smoothness and dimensionality**).: Irrespective of the dimension \(d\), we achieve benign overfitting with estimators in RKHS of arbitrary degrees of smoothness. With increasing \(d\), for the Laplace kernel the spike bandwidth is allowed to be chosen as \(_{n}=(n^{-(2+)/d})\), \(>0\), for covariate distributions with upper bounded Lebesgue density (see Remark G.2). Hence the magnitude of derivatives of the spikes is allowed to scale less aggressively with increasing dimension. 

### From spiky-smooth kernels to spiky-smooth activation functions

So far, our discussion revolved around the properties of kernels and whether they lead to estimators that are consistent. We now turn our attention to the neural network side. The big question is whether it is possible to specifically design activation functions that enable benign overfitting in fixed, possibly small dimension. We will see that the answer is yes: similarly to adding sharp spikes to a kernel, we add tiny fluctuations to the activation function. Concretely, we exploit (Simon et al., 2022, Theorem 3.1). It states that any dot-product kernel on the sphere that is a dot-product kernel in every dimension \(d\) can be written as an NNGP kernel or an NTK of two-layer fully-connected networks with a specifically chosen activation function. Further details can be found in Appendix H.

**Theorem 11** (**Connecting kernels and activation functions**(Simon et al., 2022)).: _Let \(:[-1,1]\) be a function such that \(k_{d}:^{d}^{d},k_{d}(,^{ })=(,^{})\) is a kernel for every \(d 1\). Then, there exist \(b_{i} 0\) with \(_{i=0}^{}b_{i}<\) such that \((t)=_{i=0}^{}b_{i}t\), and for any choice of signs \((s_{i})_{i_{0}}\{-1,+1\}\), the kernel \(k_{d}\) can be realized as the NNGP kernel or NTK of a two-layer fully-connected network without biases and with activation function_

\[^{k_{d}}_{NNGP}(x)=_{i=0}^{}s_{i}(b_{i})^{1/2}h_{i}(x), ^{k_{d}}_{NTK}(x)=_{i=0}^{}s_{i}(}{i+1})^ {1/2}h_{i}(x).\] (5)

_Here, \(h_{i}\) denotes the \(i\)-th Probabilist's Hermite polynomial normalized such that \(\|h_{i}\|_{L_{2}((0,1))}=1\)._

The following proposition justifies the approach of adding spikes \(^{1/2}^{_{}}\) to an activation function to enable harmless interpolation with wide neural networks. Here we state the result for the case of the NTK; an analogous result holds for induced NNGP activation functions.

**Proposition 12** (**Additive decomposition of spiky-smooth activation functions**).: _Fix \(,>0\) arbitrary. Let \(k=+_{}\) denote the spiky-smooth kernel where \(\) and \(_{}\) are Gaussian kernels of bandwidth \(\) and \(\), respectively. Assume that we choose signs \(\{s_{i}\}_{i}\) and then the activation functions \(^{k}_{NTK}\), \(^{}_{NTK}\) and \(^{_{}}_{NTK}\) as in Theorem 11. Then, for \(>0\) small enough, it holds that_

\[\|^{k}_{NTK}-(^{k}_{NTK}+^{_{}}_{NTK })\|^{2}_{L_{2}((0,1))} 2^{1/2}^{3/2}(- )+)}{}.\]

**Proof idea.** When the spikes are sharp enough (\(\) small enough), the smooth and the spiky component of the activation function are approximately orthogonal in \(L_{2}((0,1))\) (Figure 2(c)), so that the spiky-smooth activation function can be approximately additively decomposed into the smooth activation component \(^{}\) and the spike component \(^{}\) responsible for interpolation. 

To motivate why the added spike functions \(^{1/2}^{_{}}\) should have small amplitudes, observe that Gaussian activation components \(^{_{}}\) satisfy

\[\|^{_{}}_{NNGP}\|^{2}_{L_{2}((0,1))}=1,\| ^{_{}}_{NTK}\|^{2}_{L_{2}((0,1))}=(1-(-)).\] (6)

Hence, the average amplitude of NNGP spike activation components \(^{1/2}^{_{}}\) does not depend on \(\), while the average amplitude of NTK spike components decays to \(0\) with \( 0\). Since consistency requires the quasi-regularization \( 0\), the spiky component of induced NTK as well as NNGP activation functions should vanish for large data sets \(n\) to achieve consistency.

## 6 Experiments

Now we explore how appropriate spiky-smooth activation functions might look like and whether they indeed enable harmless interpolation for trained networks of finite width on finite data sets. Further experimental results are reported in Appendix I.

### What do common activation functions lack in order to achieve harmless interpolation?

To understand which properties we have to introduce into activation functions to enable harmless interpolation, we plot NTK spike components \(^{k_{}}\) induced by the Gaussian kernel (Figure 2(a),b) as well as their Hermite series coefficients (Figure 2(c)). Remarkably, the spike components \(^{_{}}\) approximately correspond to a shifted, high-frequency \(\)-curve, when choosing the signs \(s_{i}\) in (5) to alternate every second \(i\), that is \(s_{i}=+1\) iff \( i/2\) even (Figure 2(a)). Proposition H.1 shows that the NNGP activation functions correspond to the fluctuation function

\[_{}(x;)( x+/4)=( x)+(  x),\] (7)

where the last equation follows from the trigonometric addition theorem. For small bandwidths \(\), the NTK activation functions are increasingly well approximated (Appendix I.6) by

\[_{}(x;)( x+/4)=((  x)+( x)).\] (8)

With decreasing bandwidth \( 0\) the frequency increases, while the amplitude decreases for the NTK and remains constant for the NNGP (see Eq. (6)). Plotting equivalent spike components \(^{_{}}\) with different choices of the signs \(s_{i}\) (Figure 2(b) and Appendix I.5) suggests that harmless interpolation requires activation functions that contain **small high-frequency oscillations** or that **explode at large \(|x|\)**, which only affects few neurons. The Hermite series expansion of suitable activation functions should contain **non-negligible weight spread across high-order coefficients** (Figure 2(c)). While Simon et al. (2022) already truncate the Hermite series of induced activation functions at order \(5\), Figure 2(c) shows that an accurate approximation of spiky-smooth activation functions requires the truncation index to be larger than \(2/\). Only a careful implementation allows us to capture the high-order fluctuations in the Hermite series of the spiky activation functions. Our implementation can be found at https://github.com/moritzhaas/mind-the-spikes.

### Training neural networks to achieve harmless interpolation in low dimension

In Figure 1, we plot the results of (a) ridgeless kernel regression and (b) trained 2-layer neural networks with standard choices of kernels and activation functions (blue) as well as our spiky-smooth alternatives (orange). We trained on 15 points sampled i.i.d. from \(x=(x_{1},x_{2})(^{1})\) and \(y=x_{1}+\) with \((0,0.25)\). The figure shows that both the Laplace kernel and standard ReLU networks interpolate the training data too smoothly in low dimension, and do not generalize well. However, our spiky-smooth kernel and neural networks with spiky-smooth activation functions achieve close to optimal generalization while interpolating the training data with sharp spikes.

Figure 3: **a, b.** Gaussian NTK activation components \(^{_{}}_{NTK}\) defined via (5) induced by the Gaussian kernel with varying bandwidth \([0.2,0.1,0.05]\) (the darker, the smaller \(\)) for **a.** bi-alternating signs \(s_{i}=+1\) iff \( i/2\) even, and **b.** randomly iid chosen signs \(s_{i}(\{-1,+1\})\). **c.** Coefficients of the Hermite series of a Gaussian NTK activation component with varying bandwidth \(\). Observe peaks at \(2/\). For reliable approximations of activation functions use a truncation \( 4/\). The sum of squares of the coefficients follows Eq. (6). Figure I.8 visualizes NNGP activation components.

We achieve this by using the adjusted activation function with high-frequency oscillations \(x(x)+_{}(x;)\) as defined in Eq. (8). With this choice, we avoid activation functions with exploding behavior, which would induce exploding gradients. Other choices of amplitude and frequency in Eq. (8) perform worse. To bring our neural networks close to the kernel regime, we use the neural tangent parameterization (Jacot et al., 2018) and make the networks very wide (20000 hidden neurons). To ensure that the initial function is identically zero, we use the antisymmetric initialization trick (Zhang et al., 2020). Over the course of training (Figure 1c), the standard ReLU network exhibits harmful overfitting, whereas the NN with a spiky-smooth activation function quickly interpolates the training set with nearly optimal generalization. Training details and hyperparameter choices can be found in Appendix I.1. Although the high-frequency oscillations perturb the gradients, the NN with spiky smooth activation has a stable training trajectory using gradient descent with a large learning rate of \(0.4\) or stochastic gradient descent with a learning rate of \(0.04\). Since our activation function is the sum of two terms, we can additively decompose the network into its ReLU-component and its \(_{}\)-component. Figure 1b and Appendix I.2 demonstrate that our interpretation of the \(_{}\)-component as'spiky' is accurate: The oscillations in the hidden neurons induced by \(_{}\) interfere constructively to interpolate the noise in the training points and regress to \(0\) between training points. This entails immediate access to the signal component of the trained neural network in form of its ReLU-component.

## 7 Conclusion

Conceptually, our work shows that inconsistency of overfitting is quite a generic phenomenon for regression in fixed dimension. However, particular spiky-smooth estimators enable benign overfitting, even in fixed dimension. We translate the spikes that lead to benign overfitting in kernel regression into infinitesimal fluctuations that can be added to activation functions to consistently interpolate with wide neural networks. Our experiments verify that neural networks with spiky-smooth activation functions can exhibit benign overfitting even on small, low-dimensional data sets.

Technically, our inconsistency results cover many distributions, Sobolev spaces of arbitrary order, and arbitrary RKHS-norm-bounded overfitting estimators. Lemma E.1 serves as a generic tool to extend generalization bounds to the sphere \(^{d}\), allowing us to cover (deep) ReLU NTKs and ReLU NNGPs.

Future work.While our experiments serve as a promising proof of concept, it remains unclear how to design activation functions that enable harmless interpolation of more complex neural network architectures and data sets. As another interesting insight, our consistent kernel sequence shows that although kernels may have equivalent RKHS (see Appendix G.2), their generalization error can differ arbitrarily much; the constants of the equivalence matter and the narrative that depth does not matter in the NTK regime as in Bietti and Bach (2021) is too simplified. More promisingly, analyses that extend our analysis in the infinite-width limit to a joint scaling of width and depth could help us to understand the influence of depth (Fort et al., 2020; Li et al., 2021; Seleznova and Kutyniok, 2022). Finite-sample analyses of moderate-width neural networks with feature learning parametrizations (Yang and Hu, 2021) and other initializations could enable to understand how to induce a spiky-smooth inductive bias in feature learning neural architectures.

Funded by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy - EXC 2075 - 390740016 and EXC 2064/1 - Project 390727645, as well as the DFG Priority Program 2298/1, project STE 1074/5-1. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Moritz Haas and David Holzmuller. We want to thank Tizian Wenzel and Vaclav Voracek for interesting discussions. We also thank Nadine Grosse, Jens Wirth, and Daniel Winkle for helpful comments on Sobolev spaces, and Nilotpal Sinha for pointing us to Laurent series.