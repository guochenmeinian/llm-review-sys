# Zero-to-Hero: Enhancing Zero-Shot Novel View Synthesis via Attention Map Filtering

**Ido Sobol\({}^{1}\)****Chenfeng Xu\({}^{2}\)****Or Litany\({}^{1,3}\)**

\({}^{1}\)**Technion** \({}^{2}\)**UC Berkeley** \({}^{3}\)**NVIDIA**

https://zero2hero-nvs.github.io/

Generating realistic images from arbitrary views based on a single source image remains a significant challenge in computer vision, with broad applications ranging from e-commerce to immersive virtual experiences. Recent advancements in diffusion models, particularly the Zero-1-to-3 model, have been widely adopted for generating plausible views, videos, and 3D models. However, these models still struggle with inconsistencies and implausibility in new views generation, especially for challenging changes in viewpoint. In this work, we propose Zero-to-Hero, a novel test-time approach that enhances view synthesis by manipulating attention maps during the denoising process of Zero-1-to-3. By drawing an analogy between the denoising process and stochastic gradient descent (SGD), we implement a filtering mechanism that aggregates attention maps, enhancing generation reliability

Figure 1: Novel views generated from a single source image (far left column) at a specific target view angle (with different seeds), compared between Zero123-XL  and our Zero-to-Hero method. Operating during inference, our method achieves significantly higher fidelity and maintains authenticity to the original image, all while ensuring realistic variation in the results (e.g. variations in chair backs in the top row). The ground-truth target view is displayed in the far right column.

and authenticity. This process improves geometric consistency without requiring retraining or significant computational resources. Additionally, we modify the self-attention mechanism to integrate information from the source view, reducing shape distortions. These processes are further supported by a specialized sampling schedule. Experimental results demonstrate substantial improvements in fidelity and consistency, validated on a diverse set of out-of-distribution objects. Additionally, we demonstrate the general applicability and effectiveness of Zero-to-Hero in multi-view, and image generation conditioned on semantic maps and pose.

## 1 Introduction

The pursuit of realistic image synthesis at arbitrary views, given only a single source image, has long been a cornerstone challenge in computer vision and graphics. This technology can cater to countless applications, such as interactive product inspection, robot-scene interaction, and immersive virtual experiences. In this work, we aim to advance this important line of research by improving the generation of novel views that are plausible and faithful to the input image. A recent, promising approach, Zero-1-to-3  has developed a foundation model to synthesize novel views based on a single source image and a target view angle. By leveraging a pre-trained, image-conditioned stable diffusion model backbone , fine-tuned with target camera poses, and trained on paired source and target views from a vast collection of 3D models [10; 9], Zero-1-to-3 can generalize beyond its training set and generate plausible novel views. As a result, this model has quickly gained popularity, inspiring subsequent work in 3D and 4D scene generation [8; 24; 39; 28; 26; 25; 57; 33; 45; 35; 20].

Despite its remarkable ability, Zero-1-to-3 has been observed to generate views that are implausible, or inconsistent with the input object in terms of shape and appearance [24; 8]. Previous works have tried to mitigate these issues by retraining diffusion models with more data  or to generate multiple views [39; 25; 26; 24; 8; 28; 56]. Despite substantial improvement, both approaches are resource-intensive due to the required re-training on large-scale 3D datasets. Another line of work attempts to consolidate inconsistencies across multiple generated views through a 3D representation like NeRF [14; 30]. However, direct aggregation often results in blurry outputs, as observed by . Instead, ViVid-1-to-3  employed a multiview representation that naturally supports the use of a video foundation model. Nevertheless, this approach requires generating the entire trajectory from the source to the target view, adding significant complexity and computational overhead. Notably, the denoising process in Zero-1-to-3 remains unchanged.

In this work, we propose Zero-to-Hero, a novel test-time technique that addresses view synthesis artifacts through attention map manipulation. Recognizing attention maps as crucial for latent predictions, we hypothesize that enhancing robustness in attention maps predictions can significantly reduce generation misalignment. To achieve this, we draw an analogy between the denoising process in diffusion models and stochastic gradient descent (SGD) optimization of neural networks. Specifically, we relate network weights that predict local gradients at each optimization step, based on sampled training examples and labels, to the denoising network's attention maps that predict latent representations from sampled noise at each denoising step. In this work, we view the generation (denoising) process as an unrolled optimization, with attention maps as parameters of a score prediction model. Inspired by gradient aggregation and weight-averaging techniques that improve prediction robustness (e.g., consistency training ), we propose a filtering mechanism to enhance attention map reliability. This mechanism comprises iterative map aggregation within denoising steps and map averaging across denoising steps. The result is more reliable maps, particularly during the early denoising stages when coarse output shapes are determined, leading to more plausible and realistic views.

To further promote consistency with the input, we modify the self-attention operation by running a parallel generation branch using the identity pose, incorporating its keys and values into the attention layer of the target view. Unlike previous applications of this technique [29; 5; 1], we find it beneficial in view synthesis to limit its use to the early denoising stages, preventing shape distortions. Our unique denoising procedure is further complemented by a novel sampling schedule that emphasizes early and late denoising stages, maximizing performance. Our main contributions are as follows:

* To address the main limitations of the Zero-1-to-3 model, we perform an in-depth analysis and identify self-attention maps as the main candidate for correcting generation artifacts.

* We establish a conceptual analogy between model weights in stochastic gradient descent-based network training and the role of attentions map updates during generation of a denoising diffusion model. Based on this, we propose a simple yet powerful attention map filtering process resulting in enhanced target shape generation. We supplement our filtering technique with identity view information injection and a specialized sampling schedule.
* Our method requires no additional training, and it avoids the overhead of external models or generating multiple views.

Through comprehensive experiments on out-of-distribution objects, we demonstrate that our technique robustifies Zero-1-to-3 and its extended version, Zero123-XL, leading to views that are more faithful to both the input image and desired camera transformation. Our results show significant and consistent improvements across both appearance and shape evaluation metrics. Additionally, we find that Zero-to-Hero naturally generalizes to additional tasks including multi-view, and image generation conditioned on semantic maps and pose. In all cases, we observed significant improvement in condition following and visual quality.

## 2 Related Work

### Novel View Synthesis with Diffusion Models

Diffusion models have dominated various generative applications [17; 11; 38; 36; 41]. Particularly, novel-view synthesis, as a core of applications like augmented reality and simulations, naturally enjoys the benefits of high-fidelity zero-shot synthesis with diffusion models. One line of works [51; 27; 44; 22; 54; 24] is to generate a novel-view image given a source image (_i.e._, image-to-image). These approaches typically involve training a diffusion model conditioned on both an arbitrary camera pose and the source view. For instance, the representative work, Zero-1-to-3, fine-tunes a pre-trained Stable Diffusion model  by replacing the text prompt with camera pose and CLIP features. Moreover, another research trajectory [32; 21; 14; 7; 46] proposes generating a 3D representation from a single image (_i.e._, image-to-3D), which allows for sampling desired views from these 3D models. Our method, Zero-to-Hero, builds on the first approach (specifically Zero-1-to-3 and Zero123-XL) and distinguishes itself by eliminating the need for extensive training. Instead, it offers a test-time, plug-and-play approach that significantly enhances visual quality and consistency.

### Test-Time Refinement in Diffusion-Based Generation

A common test-time refinement strategy in diffusion generation is leveraging guidance [29; 2; 18] to direct the sampling process with additional conditions. For example, Repair  utilizes a mask-then-renoise strategy to refine the generation results. Repair also introduces a per-step resampling technique, where given a horizon-size h, a latent \(z_{t}\) is re-noised to \(z_{t+h}\) and then denoised again to \(z_{t}\) multiple times. They observe that resampling helps to generate more harmonized outputs, given an external guidance or condition. Restart  offers a sampling algorithm based on a variation of resampling within a chosen interval of steps. Our method is inspired by the strategy of per-step resampling. We show that it serves as a powerful correction mechanism throughout the generation process, even when no external guidance or condition is provided.

### Attention Map Manipulation in Diffusion Models

Stable Diffusion  utilizes attention to enforce the condition information onto the generated results. Previous works demonstrate that manipulating the attention operation can achieve new capabilities [49; 5; 1; 57]. For example, MasaCtrl  uses Mutual Self-Attention where source and target images are generated jointly while sharing information, by injecting source image keys and values to the target through self-attention. Here we employ Mutual Self-Attention in the context of novel view synthesis. Differently to prior works we find it beneficial to limit its use to the early denoising stages.

## 3 Background

### Zero-1-to-3: Challenges and Limitations

Zero-1-to-3 is a pioneering method for novel view synthesis based on a diffusion model, which has gained significant popularity. This model is built upon the image-conditioned variant of StableDiffusion (SD) , fine-tuned specifically for novel view synthesis. Zero-1-to-3 is conditioned on a source image and relative transformation to the desired view angle \([|]\). Maintaining the SD architecture, these conditions are integrated in two ways. First, a CLIP  embedding of the input image is concatenated with the relative transformation \([|]\) and mapped to the original CLIP dimension to form a global pose-CLIP embedding, interacting with the UNet layers through cross-attention, enriching the generation with high-level semantic information. In parallel, the input image is channel-concatenated with the denoised image, helping the model preserve the identity and details of the synthesized object.

While Zero-1-to-3  has achieved substantial progress in novel view synthesis, several common issues limit its practical application. Firstly, the generated images might not fit real-world distributions, resulting in implausible and unrealistic outputs (e.g., first row in Fig.1). Secondly, the target image may appear plausible but be inconsistent with the input image in terms of shape or appearance (e.g., fifth row in Fig.1).

In this work, we identify the critical role of self-attention maps in high-quality generation and propose a novel filtering process that enhances robustness without requiring further training. This process addresses the aforementioned issues, ensuring reliable and consistent results.

### Leveraging Gradient and Weight Aggregation for Improved Prediction Consistency

In this work, we draw a conceptual analogy between gradients and weights in stochastic gradient descent (SGD), and latents and attention maps in denoising diffusion models. Leveraging this analogy, we adapt techniques from SGD to enhance prediction consistency in diffusion models. Below, we summarize general techniques in SGD that improve the training process.

SGD is a fundamental tool in network training , designed to navigate the weight (network parameter) space towards local minima. For a neural network \(F(x;)\) with parameters \(\), SGD samples training data points \(x_{i}\) and their corresponding labels \(y_{i}\), and computes the gradient of the loss function \(L(F(x_{i};),y_{i})\) with respect to \(\) to update the parameters. In practice, aggregation of _gradients_ and network _weights_ during training is often performed to reduce variance and improve convergence. Gradient aggregation typically involves averaging gradient values over a batch, while weight aggregation accounts for the history of the weights in each update.

Notable examples include temporal averaging in Adam optimizer , Stochastic Weight Averaging (SWA)  and teacher networks  used in consistency training by employing an exponential moving average (EMA) of a student network to maintain high-quality predictions. This technique is prevalent in semi-supervised and representation learning [13; 15; 6]. For a detailed study of EMA in network training, we refer readers to .

## 4 Method

In this work we are concerned with the task of single image novel view synthesis. Formally, given an input image of an object and a relative camera transformation towards a desired target view, our goal is to generate the image at that target view. Specifically, we build upon the seminal work of Zero-1-to-3  which tackled this task via a diffusion model. As detailed in Sec. 3.1 Zero-1-to-3 often struggles to generate plausible and input-consistent images. In this work we propose Zero-to-Hero-a novel training-free technique that significantly improves its generation quality through attention map manipulation. This section is structured as follows. (4.1) through network architecture analysis we identify self-attention maps as key for robust view generation; (4.2) drawing inspiration from SGD convergence-enhancement techniques, we outline our novel attention map filtering pipeline; (4.3) details each step of the map filtering; (4.4) introduces the mutual self-attention which we use for shape guidance at early generation stages; (4.5) Finally, our proposed hourglass scheduler is introduced for more efficient utilization of generation steps. Fig. 2 depicts Zero-to-Hero's main modules. An ablation of these modules is provided in Tab. 2 and in Sec. 8.6 of the appendix.

### Analyzing the Role of Cross- and Self-Attention Layers in Novel View Generation

Zero-1-to-3 builds upon the image-to-image variant of Stable Diffusion , which utilizes a UNet  architecture as its backbone and incorporates both self- and cross-attention layers. In this subsection, we analyze the roles of these components and their contributions to the generated view. This analysis aims to identify effective intervention points for enhancing generation quality.

**Global pose conditioning through cross-attention.** Prior work  has demonstrated that the cross-attention layers in text-to-image diffusion models, which link text tokens to the latent image, are spatially aware and can be used for spatial manipulation. We first investigate these cross-attention layers, as they are the only components in the model through which the target pose is injected. In the text-to-image variant of Stable Diffusion, the generation is conditioned on a prompt \(\) containing \(C\) tokens, each encoded with CLIP into an embedding, resulting in a condition \(c_{T2I}^{C d_{CLIP}}\). However, in Zero-1-to-3, the condition is a pose-CLIP embedding \(c^{1 d_{CLIP}}\), project to keys \(K_{t}^{1 d}\) Formally, given a sample \(z_{t}^{H W d_{z}}\) and its corresponding queries \(Q_{t}^{H W d}\), the pre-softmax attention map \(\) between \(Q_{t}\) and \(K_{t}\) has dimensions \(H W 1\). Given that the summation of the softmax is always 1, the post-softmax attention map \(softmax()\) in Zero-1-to-3 is a constant all-ones matrix. A visual demonstration is presented in Fig. 7 in the appendix.

The post-softmax attention map is used to compute a weighted sum over the values matrix \(V^{1 d}\), obtained by a transformation of the condition \(c\). Since the attention matrix is an all-ones matrix, _we conclude that the cross-attention operation degenerates into a global bias term, lacking spatially aware operations_. Computing similarity scores in the cross-attention layers is redundant as these scores are never used. While in principle it is possible to improve the global bias term by additional optimization objectives and extra training overhead, we focus on the self-attention layers to enhance the results and mitigate the consistency issues while avoiding retraining the model.

**Spatial information flow through self-attention.** By monitoring the self-attention layers during the generation process, we observe that random noise introduced to the latent representation also introduces randomness to the attention maps. This randomness, while promoting generation diversity, can often lead to undesired strong correlations, that are misaligned with the true target. These strong correlations may persist through the denoising process, resulting in accumulated errors and visual artifacts.

Given the above observation and the insight about the spatial-degeneracy in the cross-attention layers, we hypothesize that the self-attention layers preserve the information about the structure and geometry of the generated image, through the similarity scores between different elements in the latent vector. To validate our hypothesis, we conduct a straightforward experiment to assess Zero-1-to-3's performance using the 'ground truth' self-attention maps, which reflect the most accurate connections considering the true target image. Specifically, we selected two images, \(I^{}\) and \(I^{}\), with known relative camera parameters \(|||\). We first encode \(I^{}\) to obtain the clean latent \(z_{0}^{}\) and then add subtle noise to obtain the corresponding noisy latent for timestep \(_{}\), \(z_{}^{}\). A single denoising step is performed on the noisy latent, and we save the self-attention maps from each layer in the UNet,

Figure 2: **Zero-to-Hereo main modules.** (Left) Two denoising steps of the generation process of both the source (top) and target views (bottom). Each denoising step is iterated \(R\) times (“resampling”). (Right-top) **Attention map filtering**: Robustifying attention maps via an aggregation of same step and previous steps attention maps. (Right-bottom) **Mutual self-attention**: Guiding target shape through the keys and values of the source generation branch.

considering these maps as the ground truth (GT) maps. In our experiments, \(_{}=5\). Next, we sample random Gaussian noise and denoise it to regenerate the target image. During each denoising step, we replace the computed self-attention maps with the GT maps, without altering any other components (e.g., cross-attention layers, residual blocks) or the latents, queries, keys, and values. We report the results in the experiment section in Tab. 1, showing a significant improvement in all metrics. Note that the results obtained with the GT attention maps are a strict upper bound, as the GT maps contain information about areas that are invisible in the source view. Our experiment validates that through improved self-attention maps, the generated image becomes more plausible. Visual examples are shown in Fig. 3 and in Fig. 8 in the appendix, refer to Fig. 1 for the results of Zero123-XL and Zero-to-Hero for the same views.

### From SGD to Diffusion Models: Attention Map Filtering as Weight-Space Manipulation

We draw a conceptual analogy between a denoising process of a diffusion model, and SGD based network optimization as both progress through gradient prediction of a loss function, and log probability (the score), respectively. Building on the discussion from the previous section, we treat the self-attention maps as parameters \(M\) in the denoising process \(z_{t-1}=(z_{t};M_{t},)\) and define their update process as a mapping1: \(M_{t} z_{t-1} M_{t-1}.\) Here the map \(M_{t-1}\) results from passing the latent \(z_{t-1}\) through the attention layers. This process is analogous to gradient descent optimization in neural networks, where each step adjusts the weights in the direction of the gradient of a loss function, such as the log probability in classification tasks. We denote this weight update as a mapping \(_{t}_{t}_{t+1}\), where \(_{t}=F(x_{t};)\), and the updated parameters \(_{t+1}\) result from a gradient step.

As detailed in Sec. 3.2, gradient and weight aggregation are essential for robust convergence. We outline this process in three steps illustrating the analogy between network parameter updates and attention map filtering. Fig. 4 further illustrates the analogy.

### Robust View Generation via Attention Map Filtering

We now discuss in detail each of the map filtering steps. A scheme of the different filtering modules is provided in Fig. 2 (Left, and Top-right).

**Latent refinement via per-step resampling.** Inspired by previous studies [29; 2], we implement per-step resampling throughout the image generation process. We select a range of timesteps

Figure 3: Through the injection of ground-truth attention maps extracted from the target view, we demonstrate that **Self-attention maps are key to robust view synthesis**.

\([t_{min},t_{max}]\), where each denoised sample \(z_{t}\) is re-noised with the proper noise ratio to the previous sampled timestep \(z_{t+1}\) and denoised back to \(z_{t}\) for \(R\) iterations2. We concur with previous studies that resampling functions as an effective corrective mechanism to the generated image, as can be seen in Tab. 2). Experimentally, we find that it is particularly useful during the _initial stages_ of the denoising process. Through resampling we progressively generate \(R\) attention maps with different noise patterns. We propose to leverage these intermediate maps to further boost performance through in- and cross-step attention map manipulations.

In-step map update.We propose a novel attention pooling function \(f\), to update the attention maps within the denoising step. Specifically, a self-attention map is refined based on previous maps \(\{M_{t,k}\}_{k=1}^{r-1}\) created at the same timestep. Since resampling is a sequential process, we perform a progressive aggregation scheme: \(_{t,r}=f(M_{t,r},\{_{t,k}\}_{k=1}^{r-1})\). We found the element-wise min-pooling operator \(f(a,b)=min(a,b)\) to perform best in our experiments. We discuss other options for \(f\) in the appendix.

Cross-step map averaging.Resampling tends to favour "conservative" generations, often gradually neglecting fine image details like buttons and eyes as denoising progresses. This phenomenon it not resolved by the in-step update. To mitigate this issue, we propose to pass the self-attention map at time \(t\), \(_{t}\) to the next step in the denoising process. We implement this cross-step aggregation via EMA: \(_{t-1}= M_{t}+(1-)_{t-1}\), for \(\). This method effectively balances past priors with current data to enhance the denoising results. In practice we apply both methods in tandem, as detailed in Sec. 8.5. An example of our attention filtering is presented in Fig. 5. Note that our filtering mechanism is applied to the pre-softmax attention maps.

Figure 4: From SGD to Diffusion Models: An illustration of our conceptual analogy.

Figure 5: **Attention map filtering in action.** We compare the attention scores of zero123-XL (top) and Zero-to-Hero (bottom) wrt the region marked with a purple circle at different denoising steps. Both methods are initialized with the same seed. We observe that the strong correlation values in the upper right corner lead to exaggerated content creation (note the unrealistically elongated neck). Conversely, through filtering, Zero-to-Hero mitigates these artifacts, leading to robust view synthesis.

### Early-Stage Shape Guidance via Mutual Self-Attention

Of the two challenges describe in the introduction, attention map filtering handles well generating realistic outputs. Yet, we observe it is sometimes insufficient to enforce consistency with the input appearance and structure. We propose to utilize mutual self-attention as a complementary technique, to propagate information from the input to the target. Similar to prior works [49; 5; 1; 57; 52], we generate the input and target views in parallel. At each timestep \(t\), we obtain queries, keys and values of the self-attention layers for the input and the target branches. Similar to the mutual self-attention introduced in , we modify the self-attention operation of the target by replacing the target keys and values with those of the input branch: \(Attn(Q^{tgt},K^{in},V^{in})\) (Fig. 2 (bottom-right)).

Previous studies initiate mutual self-attention (MSA) at a later denoising step (small \(t\)). We find that at that stage, the structure has already been determined. Instead, to guide the structure of the target to be more consistent with the input, we find it more effective when applied from the beginning of the denoising process. While MSA is effective at transferring the appearance and structure of the input to the target, it may overfit. We find that _terminating the MSA process once the structure has been stabilized becomes crucial_ in mitigating overfitting. In practice we find that applying MSA during the first third of the denoising process is a good rule of thumb for optimal results. We refer the reader to Sec. 8.5 in the appendix for further details.

We view early-stage MSA as a complementary filtering scheme. Building on the property that generating the input view (which the model is conditioned on) is a much easier task for the model compared to generating novel views, keys and values predicted in the input branch are "cleaner". Mapping them to the target view thus refines the predicted scores, facilitating a more stable and reliable output.

### Hourglass Sampling Scheduler

As resampling is a time-consuming operation, significantly increasing the number of function evaluation (NFE), we seek an efficient scheduling scheme that will enable robust and high quality generation, while preserving the fast generation times of Zero-1-to-3. We use the common DDIM sampling , and propose an efficient scheduling scheme to select the sampled timesteps. In the experiments section, we demonstrate that increasing the number of denoising steps does not necessarily improve the performance, and therefore we aim to use an overall small number of sampled timesteps. Specifically, we find that denser sampling during the beginning of the denoising process is crucial to promote realism. We also find that denser sampling at the final steps enhances fine details. Therefore, we suggest a double heavy-tailed scheduling scheme we call _Hourglass_, according to which we divide the generation process into 3 stages, as detailed in 8.5. Within each stage, we sample steps uniformly via DDIM. However, in the first and last stages we sample steps more densely (at a higher rate) than in the middle stage, by a factor of \(_{den}\).

## 5 Experiments

**Datasets.** We evaluate our method on two datasets, following previous works. Firstly, _Google Scanned Objects (GSO)_ Dataset , which includes 1030 scanned household objects. However, the dataset's imbalance (e.g., 254 objects are categorized as "shoe") and the high proportion of symmetrical shapes limit its reliability for evaluation. To address this, inspired by , we select a challenging subset of 50 objects from GSO, avoiding symmetrical and repetitive shapes. For each object, we render 8 random views (details in Sec. 8.9 of the appendix) and synthesize the remaining views from each source view, generating each target view with 3 different seeds to calculate the average score per measure. Secondly, _RTMV_ Dataset , which consists of 3D scenes. Each scene contains 20 random objects. For evaluation, we randomly select 50 scenes, and 8 random views per scene. The evaluation process is the same as described for GSO.

**Metrics.** We report the image quality metrics PSNR, SSIM  and LPIPS . As these metrics are sensitive to slight color variations, we segment the generated targets and their corresponding real images, via thresholding, and report the Intersection Over Union (IoU) score.

### Evaluations

**Quantitative evaluation.** We evaluate Zero-to-Hero against zero-1-to-3 and on zero123-XL. We report all metrics for the original models using 25, 50 and 100 DDIM steps, and for our method applied to both models. In Tab. 1 and in Tab. 3, we provide the results for GSO and RTMV datasets,respectively. We include the number of sampled timesteps T and the total number of network evaluation NFE (accounting for resampling). Zero-to-Hero consistently improves performance across all metrics, taking a significant step towards bridging the performance gap to GT attention maps. All implementation details, including analysis of the inference cost of our modules, are provided in Sec. 8.5 of the appendix.

**Qualitative evaluation.** In Fig. 1, Fig. 9 and Fig. 10, we visually demonstrate how Zero-to-Hero is able to mitigate various artifacts generated by Zero-1-to-3, from implausible results to incorrect poses. In Fig. 1 and in Fig. 9, we present 3 examples per target view, generated with 3 random seeds, to emphasize the consistency and robustness our method offers.

### Ablation Study

To assess the contribution of Zero-to-Hero different components to the final performance, we evaluate our pipeline on our challenging GSO subset, starting from the baseline Zero123-XL and gradually adding each module. The results are summarized in Tab. 2. When reporting the results of the vanilla Zero123-XL, we use its _best_ score achieved by running the model with 25, 50, and 100 steps. A similar comparison against zero-1-to-3 is included in Sec. 8.6 of the appendix, demonstrating consistent behavior. We also analyze the affect of resampling and our attention filtering on the _generation diversity_ in Sec. 8.7 of the appendix. Additionally, we provide qualitative results, demonstrating the common contributions of AMF and MSA in Fig. 10 in the appendix.

### Attention Map Filtering Beyond Novel View Synthesis

Although our work addresses the core limitations of single view synthesis models, the condition enforcing effect of our Attention Map Filtering (AMF) is more general. We have conducted several preliminary experiments which demonstrate promising results. Further details are provided in Sec. 8.8.

**Conditional image generation**. A brief study of ControlNet models  demonstrated that they suffer from similar limitations as Zero-1-to-3 and its follow ups. Namely, lack of condition enforcement and frequent appearance of visual artifacts. We implemented our proposed AMF module for two pre-trained ControlNet models. We provide qualitative results for pose- and segmentation-conditioned ControlNet models in Fig. 6 and Fig. 13, respectively.

   Name & T & NFE & PSNR \(\) & SSIM \(\) & LPIPS \(\) & IOU \(\) \\  Zero-1-to-3 & 25 & 25 & 17.27 & 0.851 & 0.173 & 73.5\(\%\) \\ Zero-1-to-3 & 50 & 50 & 17.24 & 0.850 & 0.173 & 73.5\(\%\) \\ Zero-1-to-3 & 100 & 100 & 17.21 & 0.850 & 0.173 & 73.4\(\%\) \\ Ours (Zero-1-to-3) & 26 & 66 & **17.67** & **0.859** & **0.163** & **75.3\(\%\)** \\  Zero123-XL & 25 & 25 & 17.72 & 0.854 & 0.163 & 76.4\(\%\) \\ Zero123-XL & 50 & 50 & 17.71 & 0.854 & 0.163 & 76.4\(\%\) \\ Zero123-XL & 100 & 100 & 17.68 & 0.854 & 0.163 & 76.4\(\%\) \\ Ours (Zero123-XL) & 26 & 66 & **18.35** & **0.864** & **0.153** & **78.3\(\%\)** \\  Zero-1-to-3 + GT maps & 50 & 50 & 21.52 & 0.888 & 0.122 & 88.8\(\%\) \\ Zero123-XL + GT maps & 50 & 50 & 21.79 & 0.890 & 0.117 & 88.6\(\%\) \\   

Table 1: **Quantitative evaluation on a challenging GSO subset.** Zero-to-Hero consistently improves performance upon baselines, taking a significant step towards oracle map performance (bottom rows).

   Hourglass & Resample & AMF & MSA & PSNR \(\) & SSIM \(\) & LPIPS \(\) & IOU \(\) \\  ✗ & ✗ & ✗ & ✗ & ✗ & 17.72 & 0.854 & 0.163 & 76.4\(\%\) \\ ✓ & ✗ & ✗ & ✗ & ✗ & 17.74 & 0.855 & 0.162 & 76.6\(\%\) \\ ✓ & ✗ & ✗ & ✗ & 17.85 & 0.857 & 0.160 & 77.1\(\%\) \\ ✓ & ✓ & ✓ & ✗ & 17.92 & 0.859 & 0.157 & 77.8\(\%\) \\ ✓ & ✓ & ✗ & ✓ & 18.25 & 0.862 & 0.155 & 77.6\(\%\) \\ ✓ & ✓ & ✓ & ✓ & **18.35** & **0.864** & **0.153** & **78.3\(\%\)** \\   

Table 2: **Ablation Study.** We demonstrate the importance of each of Zero-to-Hero modules, applied to the base method Zero123-XL: Sample scheduling (Hourglass), Resampling (Resample), Attention map filtering (AMF), and Early-Stage Mutual Self-Attention (MSA). Consistent conclusions are reached with the base model Zero-1-to-3 and are shown in Sec. 8.6 of the appendix.

**Multi-view synthesis**. We integrate AMF into MVDream , a text-to-multiview model, and find that it helps to mitigate the same issues as in the single view case. In Fig. 14, we provide qualitative results.

## 6 Conclusions and Future Work

In this paper, we introduced Zero-to-Hero, a training-free method to boost the robustness of novel view synthesis. We enhanced the performance of a pre-trained Zero-1-to-3 diffusion model using two key innovations: a test-time attention map filtering mechanism that enhances output realism, and an effective use of source view information to improve input consistency. Our approach also features a novel timestep scheduler for maintaining computational efficiency. In future work, we aim to refine our method by developing trainable filtering mechanisms, enhancing pose authenticity via cross-attention manipulation, and extending our approach to other diffusion-based generative tasks.

**Limitations.** Our method, operating at test-time, is limited by the generative capabilities of the pre-trained model. If Zero-1-to-3 cannot correctly generate the target pose, our method may not enhance the output, as demonstrated in the inset figure.

**Broader impact.** Through enhancing view synthesis, our method offers significant benefits to applications in virtual reality, augmented reality, and robotics. As it requires no further training, it is readily accessible. However, this accessibility also simplifies the creation of realistic novel views, which could be exploited for malicious purposes such as deepfakes.

## 7 Acknowledgments

We sincerely thank Matan Atzmon for impactful discussions and James Lucas for his invaluable feedback. Or Litany is a Taub fellow and is supported by the Azrieli Foundation Early Career Faculty Fellowship. This research was supported in part by an academic gift from Meta. The authors gratefully acknowledge this support.

Figure 6: **Qualitative results for pose-conditioned ControlNet.** Qualitative results for pre-trained pose-conditioned ControlNet, without and with AMF. Both methods are initialized with the same seed. AMF leads to results that are more plausible and better align with the conditions.