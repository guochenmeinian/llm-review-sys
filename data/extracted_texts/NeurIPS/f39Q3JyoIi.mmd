# Collaborative Alignment of NLP models

Fereshte Khani

Microsoft

fkhani@microsoft.com &Marco Tulio Ribeiro

Google DeepMind

marcotcr@gmail.com

Work done while at Microsoft

###### Abstract

Despite substantial advancements, Natural Language Processing (NLP) models often require post-training adjustments to enforce business rules, rectify undesired behavior, and align with user values. These adjustments involve operationalizing "concepts"--dictating desired model responses to certain inputs. However, it's difficult for a single entity to enumerate and define all possible concepts, indicating a need for a multi-user, collaborative model alignment framework. Moreover, the exhaustive delineation of a concept is challenging, and an improper approach can create shortcuts or interfere with original data or other concepts.

To address these challenges, we introduce CoAlign, a framework that enables multi-user interaction with the model, thereby mitigating individual limitations. CoAlign aids users in operationalizing their concepts using Large Language Models, and relying on the principle that NLP models exhibit simpler behaviors in local regions. Our main insight is learning a _local_ model for each concept, and a _global_ model to integrate the original data with all concepts. We then steer a large language model to generate instances within concept boundaries where local and global disagree. Our experiments show CoAlign is effective at helping multiple users operationalize concepts and avoid interference for a variety of scenarios, tasks, and models.

## 1 Introduction

NLP models have showcased remarkable capabilities, yet they are not exempt from imperfections. Unacceptable values embedded in their training data, persistent errors, or violations of business rules highlight the need to teach certain _concepts_ to these models. A concept relates a set of inputs to desired behaviors, e.g. in sentiment analysis, a concept may dictate that "religion does not connote sentiment" (e.g., "I'm Muslim" is Neutral). Similarly, in natural language inference (NLI), the broader concept of "downward monotonicity" specifies entailment relations when certain parts of expressions are made more specific (e.g., "All cats like tuna" entails "All small cats like tuna").

The standard way of teaching concepts to models is adding new training data that exemplifies the concept, e.g. adding neutral sentences containing religious words , or adding entailment pairs that illustrate downward monotonicity . However, it is hard to ensure that the data provided does not lead to _shortcuts_, i.e. spurious correlations or heuristics that allow models to make predictions without capturing the true underlying concept, such as "all sentences with religious terms are neutral", or "going from general to specific leads to entailment". Further, the model may _overfit_ - fail to generalize from the instances provided to the actual concept, e.g. only recognizing pairs in the form ("all X...", "all ADJECTIVE X..."), and not pairs like ("all animals...", "all cats..."). Finally, both shortcuts and overfitting can lead to _interference_ with the original data or other concepts, e.g. leading to failures on "I love Islam" or pairs like ("Some cats like tuna", "Some small cats like tuna"). In sum, operationalizing concepts is challenging, since users typically cannot think of all concept boundaries and interactions in advance.

One possible solution is to ask domain experts to create data that covers the concept as comprehensively and precisely as possible, e.g. the GLUE diagnostics dataset  or the FraCaS test suite . However, these datasets are often expensive to create, small (and thus not suitable for training), and not exhaustive, as even experts still fail to think about all aspects and nuances of a concept . Another solution is to have users provide data incrementally while they receive feedback from the model, e.g. adversarial training  or adaptive testing . These do not require users to think about everything in advance, and can expose and correct model weaknesses. However, adversarial training does not include the notion of concepts explicitly, and adaptive testing does not address the interaction between different concepts or between a concept and the original data. As a result, users may not explore concept _boundaries_ efficiently, and thus do not know when they have sufficiently covered a concept or whether they have introduced interference with other concepts.

In this paper, we introduce Collaborative Alignment of NLP Models (CoAlign). CoAlign leverages the collective knowledge of multiple users to cover many concepts instead of relying on a single user. CoAlign allows users to collaborate with AI systems _and_ other users in specifying concepts and teaching them to models. Our key insight is making concepts explicit by learning a _local_ model for each concept, in addition to a _global_ model that integrates the original data and all additional concepts. When operationalizing a new concept, we rely on a large language model (LLM) to generate new instances where the local and global model disagree, and ask users to label such examples (Figure 1). Intuitively, these examples are either cases where the local model is not yet fully specified, or where the global model still makes errors on the concept (due to overfitting or shortcut reliance). As users label these examples, both models are updated until convergence, i.e. until the concept has been learned in a way that does not conflict with prior data or prior concepts. In a sense, each local model is ever-improving cheap expert in its respective concept. The speed of prediction of local models and diversity of examples generated by the LLM enable users to explore the boundaries between concepts and existing data, an exploration that could be challenging for users to do without aid.

Our experimental results demonstrate the effectiveness of CoAlign in operationalizing concepts and handling interference. We first show CoAlign outperforms AdaTest , a SOTA tool for debugging NLP models that also uses GPT-3 by revealing more bugs and fixing them without interference. We then demonstrate that CoAlign operatinalize concepts even when user starts with biased data, outperforming a model that relies solely on data collection. We compare the data selection mechanism of CoAlign to random selection and uncertainty sampling by running simplified version of CoAlign where instead of using GPT-3 we iteratively select examples from an unlabeled pool. We show CoAlign outperforms both baselines in teaching an NLI model about downward- and upward-monotone concepts , as well as teaching a sentiment analysis model about Amazon products reviews. Finally, in a pilot study, we demonstrated that CoAlign helped users clarify their concepts.

## 2 Setup

Let \(x\) be a text string, and \(y\) a categorical label, e.g. sentiment (positive, negative, neutral). We assume there is a true function \(f(x)=y\) that we aim to approximate with a model \((x)\). Assume

Figure 1: CoAlign loop for operationalizing a single concept. (a) The user starts by providing some seed data from the concept and their labels, (b) they are used to learn a local concept model. (c) GPT-3 is then prompted to generate new examples, prioritizing examples where the local model disagrees with the global model. (d) Actual disagreements are shown to the user for labeling, and (e) each label improves either the local or the global model. The loop c-d-e is repeated until convergence, i.e. until the user has operationalized the concept and the global model has learned it.

we have access to a "base" dataset \(D_{0}=\{(x_{1},y_{1}),,(x_{n},y_{n})\}\), e.g. of movie reviews, from base distribution \(P_{0}\). We refer to the model trained on \(D_{0}\) as the base model \(_{0}\).

A concept \(C_{i}\) is associated with a distribution \(P_{i}\) over the input space, e.g. a concept might place probability mass exclusively on sentences containing religious words. We say \(x C_{i}\) if \(P_{i}(x)>0\). Since it's hard for users to be exhaustive, we do not assume users can generate from \(P_{i}\), but that they can label any \(x\) with \(f(x)\), and as to whether it is in \(C_{i}\) or not. We further assume users can provide a very small number samples in the support of \(P_{i}\).

Without loss of generality, we assume that we have \(k\) users developing a model collaboratively, each with their own concept. Our goal is to train a model \(\) that does well on both the base distribution and all concepts, i.e., minimizing \(_{i=0}^{k}_{x P_{i}}[(x) f(x)]\).

## 3 Collaborative Alignment of NLP models

In this section we describe how a single user operationalizes their concept by producing a dataset \(D_{i}\) in the context of an existing global model \(\) trained on the base dataset \(D_{0}\) and previous concepts \(D_{1:i-1}\). If \(_{0}\) is already "aligned" within the concept, i.e. \(_{0}(x)=f(x)\) for all \(x C_{i}\), we would be done and there would be no need for \(D_{i}\). Thus, what we really want is for \(D_{i}\) to specify the boundary around "failures", cases not currently handled by \(_{0}\). We abstract away the choice model and learning procedure, assuming that the model can learn the concept given \(D_{i}\) (in practice, we use neural networks that are expressive enough).

### Sampling from the concept

Since by assumption we cannot sample from the concept distribution \(P_{i}\), it is a challenge to find regions of \(P_{i}\) where \(_{0}\) fails. To address this, we use GPT-3  as a generator \(\) to simulate a random walk within the concept. To do so, we construct a prompt with \(m\) in-concept examples, and use this prompt as input to \(\) to generate more samples. Then, we ask the user in the loop to accept or reject each generated sample \(x^{}\) (\(x^{}\) is accepted if \(x^{} C_{i}\)), and also to label the accepted \(x^{}\) with \(f(x^{})\). The value of \(m\) controls the tradeoff between precision and recall, with high \(m\) generating more in-concept examples and low \(m\) exploring the space more broadly.

Under some conditions it can be shown that \(\) simulates a Markov chain with stationary distribution \(P_{i}\) (Appendix A), but the weaker condition of connectivity suffices for finding the concept failures, i.e. there must be a path between any \(x^{},x^{} C_{i}\) with nonzero transition probabilities according to \(\) and the prompt. That is, if the concept is connected, with enough time we should be able to find regions of \(P_{i}\) that are not already learned by \(_{0}\).

While sampling from \(\)_eventually_ leads to the yet-unlearned concept regions, it is an inefficient use of human effort, as it does not use the user labels to guide generation (i.e. the user has to label many examples that the current model already handles correctly). A better approach would be ask the user to label in a way that maximizes the expected information gain for the concept, to which we now turn.

### Local Concept Models

Complex functions can be approximated by simpler functions in a local neighborhood, as evidenced by theoretical results (e.g. Taylor expansion) and empirical applications [8; 9]. Since a concept is a natural local neighborhood, we use this insight and learn a _local_ model \(_{i}\) to approximate \(f(x)\) in \(C_{i}\).

We present a toy example for intuition in Figure 2, where we show toy distributions \(P_{0}\) (Figure 1(a)) and \(P_{0}\) with an additional concept \(P_{1}\) (Figure 1(b)). In 1(b), \(_{0}\) learned on samples from \(P_{0}\) (dashed line) predicts most of \(P_{1}\) correctly, except for a small region in the bottom left quadrant. However, we would need _many_ random samples from \(P_{1}\) in order to learn that region, and reach the best model \(\) (solid line in Figure 1(b)). In contrast, we can learn a good local model \(_{1}\) for \(P_{1}\) with a trivial number of samples (Figure 1(c)). This local model can be used to produce a _disagreement region_ between \(_{1}\) and \(_{0}\) (Figure 1(d)), and sampling from that region would lead to failure discovery much faster.

More generally, we define a score function as the disagreement between the local and global function. This score function is used to steer generation such as to maximize the score of generated samples \(x^{}\), by adding instances to the prompt for \(\) with probability proportional to their score (similar to Ribeiro and Lundberg Ribeiro and Lundberg (2018), who use a different score function). We note that models may present false agreement on some samples, i.e. \(_{i}(x^{})=_{0}(x^{}) f(x^{})\). To account for this, we also sample from the agreement region sometimes, with a probability that decays over time as we gain more confidence in the local model.

### Operationalizing a concept: from disagreement to convergence

The local and global models disagree on regions where the concept has not yet been learned (global is wrong) or the local model does not fit the user's concept correctly (local is wrong). Thus, every label from the disagreement region results in an improvement in whichever model is incorrect. As labeling progresses, we update both the concept model \(_{i}\) and the global model \(\) until we reach convergence (i.e. \(\) has learned the concept \(C_{i}\)). Note that while \(_{i}\) is trained on data from \(P_{i}\), \(\) is trained to do well on the base distribution \(P_{0}\), as well as on all concepts \(P_{1:k}\).

We present pseudo-code for operationalizing a concept in Algorithm 1, and illustrate it in Figure 3. The local and global models are initialized in lines 1-2 (Figure 2(a) and 2(b)). In line 5 (Figure 2(c)), \(\) is prompted such as to generate in-concept samples that are likely to be in the disagreement region. Then, in line 6 (Figure 2(d)), we select a batch of \(b\) examples for labeling according to the disagreement score of the generated instances. This generation-labeling loop is repeated for \(L\) iterations, after which both the local and global models get updated in lines 7-8 (Figure 2(e)). For the sake of interactivity, in practice we do not train from scratch when updating the models, and instead finetune them from a previous checkpoint. After a few iterations, \(\) and \(_{i}\) converge (i.e. the user has operationalized the concept and the global model has learned it). If the number of generated samples \(b*L\) is large

Figure 3: This figure illustrates the main steps in CoAlign: (a) The user starts by providing a small number of data points and their labels within their domain (represented by the black circle), (b) we fit a simple model (shown as a linear model) to represent the user’s concept, (c) we use the generator (\(\)) to generate data points towards the region where the local model disagrees with the global model, (d) a diverse set of data points are selected for the user to label, (e) based on the user’s feedback, the local and global models are updated until convergence.

Figure 2: (a) A model is trained on two elliptical Gaussian (b) The spherical Gaussian are showing a user concept, who wants to find bugs in the model and teach the model about the new concept. However, since the model (dashed line) have high accuracy on the user concept it is hard to find bugs and teaching the model requires data from the low probability region. (c) We fit a classifier to the Spherical Gaussian which can be done with only a few data points (i.e., local model), (d) we then focus on disagreements between these two models to find bugs in the user concept.

enough, we can assume that no disagreement on generated sample between \(\) and \(_{i}\) means they have converged, and thus we stop and output the generated data \(D_{i}\) (line 9).

``` input :Base dataset \(D_{0}\), Concept datasets \(D_{1:i-1}\). A small set of samples from concept \(D_{i}\)
1Init local and global:Train \(_{i}\) on \(D_{i}\), and train \(\) on \(D_{0:i-1}\);
2do
3for\(L\) iterationsdo
4Generation:Prompt \(\) with subset from \(D_{i}\) chosen with probability \(|(x)-_{i}(x)|\) ;
5 Labeling:Select \(b\) samples with prob. \(|(x^{})-_{i}(x^{})|\). Users reject \(x^{}\) if out of concept, or add to \(D_{i}\) with label \(f(x^{})\);
6 Update local and global:Train \(_{i}\) on \(D_{i}\), and train \(\) on \(D_{0:i}\);
7while\(D_{i}\) was updated this round; output :A dataset \(D_{i}\) ```

**Algorithm 1**Operationalizing a new concept \(i\)

### Handling interference between concepts

When aligning a model to multiple users (or training a model on different distributions), two types of conflicts can happen: (1) Literal disagreements: where two users disagree in labeling the same input. (2) Interference: where over-generalization in one concept interfere with another concept (e.g., if a user adds a lot of neutral examples for the "religion does not connote sentiment" concept, then training the model on these new examples can cause the model to predict neutral for sentences like "I love Islam" which interfere with a concept that says "love X" is positive). As stated in Section 2, we assume there is a single true function \(f\) (thus no literal disagreement) and we only focus on handling interference. 2 Handling interference is crucial since any local change in ML models can interfere with other parts of the model [10; 11; 12].

Having local functions (cheap experts) enable us to check interference efficiently. Every time that a user operationalizes a concept according to Algorithm 1, we check the resulting global model \(\) against the local models for all previous concepts. To do so, we re-run Algorithm 1, only asking the user for labels if there is a newfound disagreement region. In practice, this means that a user adding a new concept needs to make sure it does not break any concepts from other users, a process similar to regression testing in software engineering.

While we want to handle interference, having multiplicity of concepts can be beneficial in refining a concept. In particular, we want the global model to not overfit to a concept (i.e., only memorize the training data) and generalize well; however generalizing well is dependent on other concepts and the previous data. Take, for instance, a world where only bananas are yellow. A user might guide an ML model to recognize bananas solely based on their yellow color. Now if a new user introduces another yellow object, like corn, the model must discern other distinguishing features. Merely combining training data for two such concepts doesn't suffice (as shown in ); the boundaries must be distinguished. Furthermore, interference can be beneficial by exposing false agreement regions between the global model and any individual concept model. In other words, while both \(\) and \(_{i}\) may be relying on the same shortcut to solve some regions of \(P_{i}\), it is unlikely that this shortcut does not cause interference between \(\) and all local models \(_{j}\) for \(j i\). In this case, the interference caused by adding \(C_{j}\) is actually beneficial, as it further refines concept \(C_{i}\).

In practice the original dataset (\(D_{0}\)) is often very large and we cannot fine-tune the model on \(D_{0:k}\). Instead of choosing the whole \(D_{0}\), every time we sample data points with highest disagreement between \(_{0}\) and \(\) from \(D_{0}\). In other word, we treat \(_{0}\) as a user with concept distribution \(P_{0}\), this enable us to deal with interference with original model as well (as an example in Figure 2 we might only choose the data points from elliptical Gaussian in disagreement region).

Linear regression analysis

We now study CoAlign in a noiseless overparametrized linear regression scenario. This particular setup is used in recent literature to gain some insights into the behaviors of deep networks [10; 11; 13; 14]. For more details and proofs see Appendix B.

**Setup.** Let \(x^{d}\) where only a subset the data points are valid. We assume \(^{}^{d}\) such that \(y={^{}}^{}x\). Let \(S_{i}\) denote the smallest subspace containing all valid data points of \(C_{i}\). Given \(k\) examples in \(C_{i}\), let \(S_{i}^{obv}\) denote the subspace observed by the training data, \(S_{i}^{uno}\) denote the unobserved subspace (thus \(S_{i}=S_{i}^{obv}+S_{i}^{uno}\)), and \(S_{i}^{inv}\) is the subspace that concept \(i\) does not have any variation in it. We consider overparametrized noiseless linear regression, where the number of features (\(d\)) exceeds the number of training data, enabling us to always interpolate all training data. We assume local and global models infer the min L\({}_{2}\) norm interpolant.

As a running example, let \(x^{3}\) and consider a concept where data points belonging to that concept satisfies \(x_{1}=x_{2}\). Let's assume we observed \(x=\) with label \(y=2\). In this case we have: \(S_{1}^{obv}=\{\}\), \(S_{1}^{uno}=\{\}\) and \(S_{1}^{inv}=\{[1,-1,0]\}\). The min-norm solution interpolating the concept is \(_{i}=\).

Operationalizing a concept.The min-norm solution can also be constructed by introducing constraints that projection of \(_{i}\) on \(S_{i}^{uno}\) and \(S_{i}^{inv}\) has to be zero. In the above example, the min-norm solution is the unique answer of the following linear equations (\(=0,[1,-1,0]=0,=2\)). When we teach the global model about a concept, the naive combination of data points could violate these constraints, leading to disagreements between local and global models. For our running example, suppose the original data point is \(x=,y=2\). Combining the concept data point with the original data point for inferring global model results in \(_{}=[,,]\), which leads to a disagreement between local and global model for data points varying in the \(\) direction.

In case of disagreement, two scenarios could arise: (1) the direction is non-zero and the local model needs more specification (happens a lot at the beginning), or (2) the direction is zero, but an explicit constraint is needed to prevent the global model from assuming other values. In our running example, the generator can help us to find a data point that the two model disagree 3. Let's assume the data point is \(x=\) where local model predicts \(0\) but global model predicts \(\). We show this data point to the user and either the user confirms \(\) as non-zero or makes the zero explicit, resulting in a global prediction of \(_{}=\).

Once we learn the local concept (i.e., all unobserved directions are indeed zero), how many of \(S_{i}^{uno}\) directions need to be added as explicit constraints? Intuitively, we do not need to query user for any direction in \(S_{i}^{uno}\) that has already been observed in \(S_{0}^{obv}\) or are orthogonal \(S_{0}^{obv}\) since there is no interference. The following proposition indicates the maximum number of disagreements for teaching global model about the local concept.

**Proposition 1**.: _If \(_{S_{i}^{uno}}(^{})=0\), then the maximum number of disagreement between local and global models is \((_{S_{0}^{obv}}(S_{i}^{uno}(S_{i}^{uno} S_{0}^ {obv})^{}))\)._

Handling interference between concepts.During the operationalization of concept \(i\), we maintain \(S_{0}^{obv}\) unchanged. However, in handling interference (Section 3.4), we add data to other concepts and the original data, potentially leading to new conflicts with concept \(i\). Therefore, in addition to considering the projection of \(S_{i}^{uno}\) on observed subspaces we need to consider the unobserved subspaces as well. Similar to above, if an unobserved direction in one concept has been observed in another concept we do not need to query user. With notation of \(S_{0:k}^{obv}\) denoting sum of all the \(S_{i}^{obv}\), and \(S_{-i}\) denotes sum of all subspaces except \(i\), the following proposition bounds number of times users need to add data to their concepts due to interference.

**Proposition 2**.: _If for all \(i\), \(_{S_{i}^{uno}}(^{})=0\) then the maximum number of times that we need to handle interference is \(_{i=1}^{k}(_{S_{-i}}(S_{i}^{uno}(S_{ i}^{uno} S_{0:k}^{obv})^{}))\)._

## 5 Experiments

We first show CoAlign outperforms AdaTest , a leading NLP debugging tool that also uses GPT-3, by revealing more bugs and resolving them without interference. We then show CoAlign can operationalize a concept even with biased seed data. We then show the selection mechanism of CoAlign outperforms baselines such as random sampling and uncertainty sampling.We conclude with a small pilot study of using CoAlign.4

CoAlign outperforms AdaTest.We consider a scenario where a user finds multiple bugs and wants to fix them. Following Ribeiro and Lundberg , we use Quora Question Pairs (QQP) dataset where the goal is to predict if two questions are duplicate or not. We finetune RoBERTa-Large model on the QQP dataset, despite high accuracy of the model (92.2%), Ribeiro et al.  identified multiple concepts where the model has low performance. We use the 6 concepts with highest failure (see Table 1 for two of the concepts example).

For each of the \(6\) concepts, AdaTest iteratively adds data with GPT-3 and adaptively find failures "until finding failures becomes qualitatively difficult". For each concept, we initialize CoAlign with the AdaTest generated data as \(D_{i}\). Even though the model has been 'debugged' with AdaTest, CoAlign quickly reveals \( 5\) semantically meaningful sub-categories of bugs for each concept (with many failures within each sub-category). We show a few examples from different sub-categories in Table 1, which illustrate that AdaTest had not yet operationalized various regions of concepts where the model was still failing.

We now compare CoAlign with AdaTest in terms of handling interference. To do so, we pick pairs of concepts \(C_{orig}\), \(C_{new}\) that might cause interference with one another, but that were debugged and reported as "fixed" by Ribeiro and Lundberg . We then run CoAlign on these pairs, noting that the output of both AdaTest and CoAlign are small concept datasets \(D_{orig}\) and \(D_{new}\).

We then train two models for each method, one finetuned on \(D_{orig}\) and one finetuned on the union of \(D_{orig}\) and \(D_{new}\). Finally, we generate new data from \(P_{orig}\) and manually label \(50\) instances where there is disagreement between the models, to check if adding \(D_{new}\) caused interference on \(P_{orig}\). We present the proportion of sentences that were broken (right to wrong) or fixed (wrong to right) when the new concept is added in Table 2, disregarding instances that are invalid, out of domain, or for which the label is unclear. The top pair seems more liable to interference between concepts, but we note that AdaTest data results in much more interference than CoAlign data. In the bottom pair,

  
**Concept** & **Examples** & **Examples of bugs found by CoAlign** \\  X person = not antonym (X) person & How can’t become a positive person? & predicts duplicate & How can’t become a positive person? \\  & How can’t become a positive person who not antonymate? & How can’t become a negative person? & How can’t become a negative person? \\  & & & & \\  & & & & \\  & & & & \\  Modifiers changes question intent & h. Mask Wright a photographic? & predicts no-reference & h.k an article? \\  & h. Mask Wright as a modified photographic? & where where we label & h.k an article among other papers? \\  & & & & \\  & & & \\  & & & \\  & & & \\        
    & \(C_{orig}\)+ non datanym (X’: \(C_{new}\): “Modifiers change position intent”) & \(C_{orig}\)+ suspension (X’: \(C_{new}\): “Tone X = inner ANOVA(X’) \\  broken by new concept & CoAlign & AdaTest & CoAlign & AdaTest \\  & 7/50 & 24/50 & 9/50 & 18/50 \\ fixed by new concept & 5/50 & 2/50 & 20/50 & 18/50 \\   

Table 1: Examples of bugs found by CoAlign in the concepts introduced by CheckList, which were subsequently “debugged” using AdaTest, demonstrating that AdaTest had not yet fully operationalized these concepts.

    & \(C_{orig}\)+ non datanym (X’: \(C_{new}\): “Modifiers change position intent”) & \(C_{orig}\)+ suspension (X’: \(C_{new}\): “Tone X = inner ANOVA(X’) \\   & CoAlign & AdaTest & CoAlign & AdaTest \\ broken by new concept & 7/50 & 24/50 & 9/50 & 18/50 \\ fixed by new concept & 5/50 & 2/50 & 20/50 & 18/50 \\   

Table 2: Comparison of CoAlign and AdaTest in terms of handling interference. For both methods, we labeled 50 sentences in the disagreement region of a model that only learned the original concept and a model that learned both original and the new concepts. CoAlign outperforms AdaTest when the new concept conflicts (top) or is similar (bottom) to the original concept.

adding a concept with CoAlign actually improves the original concept more often than interferes with it, while AdaTest data has a neutral effect.

CoAlign works even with biased seed data.To start working with CoAlign, a user should first provide some seed data (the first data points to train the local model). One natural question is that how much the model is dependent on the this seed data? This question is especially important when data is very biased and naive data collection might lead to very skewed data. Our goal is to understand if CoAlign can cover a concept even when the initial data points are very biased. For evaluating this question, we simulate a concept consists of reviews containing the phrases "battery life" or "my skin" (from now on, we refer to this concept as SB). We then simulate a very extreme scenario where instances with "battery life" are always positive, and those with "my skin" are always negative, from now on we refer to this distribution biased-SB. We train a weak base model by finetuning BERT-base-uncased on reviews that contain the word "install". Our goal is to see if CoAlign can lead to good accuracy in SB while only having data points from biased-SB.

We initialize CoAlign with \(10\) instances from biased-SB (\(5\) positive and \(5\) negative sentence). We then run CoAlign for \(5\) rounds adding \(10\) data points in each round. To avoid the need of user labels, we train a high accuracy model as an oracle to simulate user labels. As an oracle, we train RoBERTa-Large on the whole Amazon Review dataset where we only keep reviews with rating \(5\) (positive sentiment) and rating \(1\) (negative sentiment). The accuracy of oracle on validation dataset is 98.6%. As shown in Table 3, despite starting with biased samples, CoAlign achieves high accuracy across all of SB. Conversely, the same number of random instances from biased-SB (simulating naive data collection) only increases accuracy for biased-SB while decreasing accuracy on the whole concept (SB). Qualitatively, GPT-3 starts generating in-concept instances without the bias, as that is where the local concept model disagrees with the base model. This controlled experiment illustrates how a generator focused on disagreements can be helpful in exploring the boundaries of concepts, even when we start from a biased sample.

CoAlign outperforms random and uncertainty sampling.We adapt CoAlign so that it selects instances from a pool of unlabeled data based on the disagreement score rather than using GPT-3 as a generator (lines 5-6 in Algorithm 1). We compare CoAlign with two data selection baselines: random selection and uncertainty sampling [16; 17]. Each method selects a batch of \(50\) examples per iteration (CoAlign selects examples randomly in the first batch), after which models are retrained.

We use RoBERTa-Large  finetuned on MNLI (binary)  as a base model, and use the downward-monotone concept from an NLI monotonicity dataset  as a pool of unlabeled data. The base model starts with high accuracy on the base validation dataset (93.5%), and low accuracy on the concept (23.5%). We present accuracy on the concept over iterations in Figure 4. While accuracy on the base

    &  &  \\   & Overall & Worst & Overall & Worst \\  & average & case & average & case \\  Random & \(89.6\) & \(87.7\) & \(87.6\) & \(85.3\) \\ Uncertainty & \(91.6\) & \(90.6\) & \(87.9\) & \(85.9\) \\ CoAlign & \(91.9\) & \(90.7\) & \(88.2\) & \(86.0\) \\   

Table 4: CoAlign outperforms data selection baselines when simultaneously learning two concepts for MNLI (upward and downward monotone) and 4 concepts (product categories) for sentiment analysis. Results shown are an average of 10 runs.

Figure 4: CoAlign outperforms other data selection baselines when learning downward-monotone concept in MNLI task.

    & biased-SB & SB \\  Base & \(86.7 2.5\) & \(82.6 1.7\) \\ Data collection & \(98.6 0.9\) & \(80.7 1.6\) \\ CoAlign & \(94.9 1.7\) & \(94.5 1.1\) \\   

Table 3: When we have access to a biased dataset (biased-SB) with consistently negative skin reviews and positive battery reviews. CoAlign outperforms naive data collection across the entire skin and battery reviews (SB) dataset by efficiently conceptualizing concepts and avoiding shortcuts.

data remains constant throughout iterations, CoAlign's disagreement-based sampling is more efficient than uncertainty sampling or random selection.

In order to evaluate interference between concepts, we try to learn various concepts simultaneously. We use the same NLI model and concept as the previous paragraph, and add upward monotone  as an additional concept. We also use RoBERTa-LARGE finetuned on the "digital ebook" category of Amazon Reviews as a base model, and use 4 additional product categories as concepts to be learned (pet-product, home, home-improvement, and wireless). We run 10 iterations of each method, with random sampling and uncertainty sampling selecting an equal number of instances from each concept per round. Table 4 shows the per-concept average and worst case accuracy of each method, where we note that CoAlign produces better models on average.

**Pilot Study (qualitative).** We conducted a very small pilot study to show humans need assistance to operationalize their concept (i.e., we show that users might not even know the exact boundaries of their concept beforehand). Four users (one computer scientist and three with no prior knowledge of AI) used CoAlign to align a model within their chosen concept in either sentiment analysis or toxicity task. Each participant interacted with CoAlign for 5-7 rounds, and reported an improved alignment in their concept and an increase in sentence complexity over time. For example, for the Sentiment & Islam, the user-provided seed data matched the global model perfectly, indicating no initial bugs. However, in the first round of using CoAlign, some disagreement were found between the fitted local model and the global model. The user identified some of these as bugs in the global model (e.g. "Alice practices Islam daily" where the global model predicted negative) and some as bugs in the local model (e.g. "Alice is a radical Islamist" where the local model predicted neutral). As the user made repeated adjustments to both models, the disagreements gradually centered around the concept boundaries, reaching a point where users could no longer determine the correct behavior. This pattern of initial misalignment correction followed by uncertainty in label determination was consistent across all users, suggesting successful concept operationalization in clear label regions. Table 5 provides examples of user interactions with CoAlign5.

## 6 Related Work

Our work relates to three areas of research: debugging, alignment, and interference management. We briefly review the relevant literature and highlight the differences and contributions of our work.

**Debugging.** Numerous efforts aim to find and fix models failures. Checklist  uses templates to test and improve models in different areas; however, these templates have low coverage for finding bugs. Dynabench  iteratively discovers bugs in a model by using human-generated adversarial examples; however, this approach requires human creativity.The closest work to us is AdaTest , which uses an LLM (and a few prompts) as a proxy for the user's behavior. Thus, unlike CoAlign AdaTest proxies do not adapt to the user's feedback and is susceptible to the biases and limitations of the LLM, resulting in lower performance than CoAlign. Moreover, AdaTest do not consider the interference among different concepts.

  
**Task \& Topic** & **Seed data** & **Initial rounds** & **Final rounds** \\   & Lewis is a terrorist \(\) negative & Alice practices radical Islam & Alice pays for ISIS \\  & Alice practices Islam \(\) neutral & Alice is a fundamentalist Muslim & Lewis is Muslim extremist \\  & Marco goes to the mosque \(\) neutral & Alice wants to convert everyone to Islam & Lara, president of ISIS, said... \\   & \(\) m a man \(\) non-toxic & \(\) m feminine \\  & men are superior to women \(\) toxic & women like to use emotion to manipulate & Men have caused most of the wars \\   & ISIS is violent \(\) non-toxic & Dealing with Muslim countries can be tricky \\  & Muslims are violent \(\) toxic & Taliban is violent & Quron is a political manifesto \\   

Table 5: Some examples of a pilot study that qualitatively examines the effectiveness of the CoAlign in assisting users in conceptualizing their ideas and debugging the model. Users typically begin with simple sentences in their concept as seed data along with their labels. As users continue to use CoAlign, the CoAlign suggestions (disagreements between locally fitted model and the global model) become increasingly complex showing the diverse coverage of GPT-3.

**Alignment.** The objective is to align a model with human intentions, a complex task since humans often cannot articulate their intentions. A prevalent method to tackle this is Reinforcement Learning with Human in the Loop (RLHF) [22; 23; 24; 25], where the model learns a reward function based on human feedback on various outputs. The key distinction between our work and RLHF lies in our use of a local function for each concept, rather than a universal function for all concepts, and our generation of inputs in the model's disagreement region to assist users in better operationalizing their concept. Constitutional AI  works with multiple "expert" models in different domains, but they aggregate all the data generated by these models without addressing potential interference between them.

**Interference management.** Interference is a common problem in ML since it is hard to change a model's behavior locally without affecting other areas. The trade-off between accuracy and robust-accuracy where robust-training led to decrease in accuracy shown to be mitigated with self-training with the original model and using unlabeled data [11; 27]. Unlike their work, we do not have access to a reliable model for self-training and we need to improve the models while handling interference. Another type of interference is catastrophic forgetting [28; 29], in which learning on a new task may degrade the performance on the previously learned tasks. Some possible mitigation is multi-task learning [30; 31], or weight averaging between the original and the fine-tuned model . Unlike these works, we are interested in exploiting the interference between models, as they can help the user operationalize their concept in the context of the model better. Lastly, this work primarily addresses interference arising from shortcut learning, there may be literal disagreements among users[20; 21]. Although a crucial issue, it falls outside the scope of this paper. However, we like to note that our method can surface such disagreements. This could potentially facilitate resolution through discussions, voting, or modifying the model to reflect a range of perspectives, especially in situations where there isn't a consensus among users.

## 7 Conclusion

Specifying model behavior on specific concepts is crucial in the development of NLP models, as it allows users to encode business rules, fix undesirable behavior, and force alignment with user values. Operationalizing concepts in a way that avoids shortcuts, overfitting, and interference with prior data is challenging. In this paper we presented CoAlign, a framework that leverages local concept models and large language models to help users operationalize concepts effectively while avoiding interference. We showed that CoAlign is more effective than prior work at exploring problematic concept regions, even prior work that uses the same language model and relies on interactive user feedback. We envision a future where NLP models are developed in a collaborative fashion, similar to open source software or Wikipedia, and speculate that harnessing the perspectives and expertise of a large and diverse set of users would lead to better models, both in terms of overall quality and in various fairness dimensions. For this scenario to materialize, we need ways to help users express their knowledge, and verify the impact of their proposed changes to models (the equivalent of "diffs" or "regression tests"). We believe CoAlign is a step in this direction.

## 8 Broader Impact and Limitations

CoAlign aids in operationalizing concepts without filtering the values a user wishes the model to align with, which might inadvertently allow a malicious user to encode harmful behavior into the NLP model, a risk for which we currently have no safeguards. Next, we only handled interference that arises from machine learning shortcomings and can be addressed by adding more data. However, there might be literal disagreements between users (i.e., two users prefer different labels for the same sentence). Although our method can surface such disagreements, we lack a definitive solution to resolve disagreements between users.

Furthermore, our efforts were primarily on classification tasks, leaving out generative tasks (e.g., next word prediction). A possible workaround is to use a local classifier where a user only indicates whether an output aligns with their concept or not, then train the global model accordingly (similar to Ouyang et al.  but with multiple reward models instead of just one). Lastly, our theoretical framework is limited but our goal was to gain some initial insights into why interference occurs and estimates the number of instances required to address it. Handling malicious users, resolving literal disagreements, studying CoAlign for generative tasks and more general theoretical analysis of alignment are compelling further research directions.

Acknowledgment

We gratefully acknowledge the contribution of Scott Lundberg, whose insightful discussions and assistance in utilizing the AdaTest repository greatly enhanced our research. Further, we thank Brent Hecht and Zexue He for providing invaluable early feedback on this work.