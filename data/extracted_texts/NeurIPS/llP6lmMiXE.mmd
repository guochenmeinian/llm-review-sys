# A General Framework for Robust \(G\)-Invariance

in \(G\)-Equivariant Networks

 Sophia Sanborn

sanborn@ucsb.edu

&Nina Miolane

ninamiolane@ucsb.edu

Department of Electrical and Computer Engineering

UC Santa Barbara

###### Abstract

We introduce a general method for achieving robust group-invariance in group-equivariant convolutional neural networks (\(G\)-CNNs), which we call the \(G\)-triple-correlation (\(G\)-TC) layer. The approach leverages the theory of the triple-correlation on groups, which is the unique, lowest-degree polynomial invariant map that is also _complete_. Many commonly used invariant maps--such as the max--are incomplete: they remove both group and signal structure. A complete invariant, by contrast, removes only the variation due to the actions of the group, while preserving all information about the structure of the signal. The completeness of the triple correlation endows the \(G\)-TC layer with strong robustness, which can be observed in its resistance to invariance-based adversarial attacks. In addition, we observe that it yields measurable improvements in classification accuracy over standard Max \(G\)-Pooling in \(G\)-CNN architectures. We provide a general and efficient implementation of the method for any discretized group, which requires only a table defining the group's product structure. We demonstrate the benefits of this method for \(G\)-CNNs defined on both commutative and non-commutative groups--\(SO(2)\), \(O(2)\), \(SO(3)\), and \(O(3)\) (discretized as the cyclic \(C8\), dihedral \(D16\), chiral octahedral \(O\) and full octahedral \(O_{h}\) groups)--acting on \(^{2}\) and \(^{3}\) on both \(G\)-MNIST and \(G\)-ModelNet10 datasets.

## 1 Introduction

The _pooling_ operation is central to the convolutional neural network (CNN). It was originally introduced in the first CNN architecture--Fukushima's 1980 _Neocognitron_--and remained a fixture of the model since. The Neocognitron was directly inspired by the canonical model of the visual cortex as a process of hierarchical feature extraction and local pooling . In both the neuroscience and CNN model, pooling is intended to serve two purposes. First, it facilitates the local-to-global _coarse-graining_ of structure in the input. Second, it facilitates _invariance_ to local changes--resulting in network activations that remain similar under small perturbations of the input. In this way, CNNs construct hierarchical, multi-scale features that have increasingly large extent and increasing invariance.

The pooling operation in traditional CNNs, typically a local max or average, has remained largely unchanged over the last forty years. The variations that have been proposed in the literature  mostly tackle its _coarse-graining_ purpose, improve computational efficiency, or reduce overfitting, but do not seek to enhance its properties with respect to _invariance_. Both max and avg operations are reasonable choices to fulfill the goal of coarse-graining within CNNs and \(G\)-CNNs. However, they are excessively imprecise and lossy with respect to the goal of constructing robust representations of objects that are invariant only to irrelevant visual changes. Indeed, the max and avg operations are invariant to many natural image transformations such as translations and rotations, but alsoto unnatural transformations, including pixel permutations, that may destroy the image structure. This excessive invariance has been implicated in failure modes such as vulnerability to adversarial perturbations [20; 26], and a bias towards textures rather than objects . To overcome these challenges and enable robust and selective invariant representation learning, there is a need for novel computational primitives that selectively parameterize invariant maps for natural transformations.

Many of the transformations that occur in visual scenes are due to the actions of _groups_. The appreciation of this fact has led to the rise of group-equivariant convolutional networks (\(G\)-CNNs)  and the larger program of Geometric Deep Learning . While this field has leveraged the mathematics of group theory to attain precise generalized group-equivariance in convolutional network layers, the pooling operation has yet to meet its group theoretic grounding. Standardly, invariance to a group \(G\) is achieved with a simple generalization of max pooling: Max \(G\)-Pooling  --see Fig. 1 (top-right). However, this approach inevitably suffers from the lossiness of the max operation.

Here, we unburden the pooling operation of the dual duty of invariance and coarse-graining, by uncoupling these operations into two steps that can be performed with precision. We retain the standard max and avg pooling for coarse-graining, but introduce a new method for robust \(G\)-invariance via the group-invariant triple correlation --see Fig. 1 (bottom-right). The group-invariant triple correlation is the lowest-order complete operator that can achieve exact invariance . As such, we propose a general framework for robust \(G\)-Invariance in \(G\)-Equivariant Networks. We show the advantage of this approach over standard max \(G\)-pooling in several \(G\)-CNN architectures. Our extensive experiments demonstrate improved scores in classification accuracy in traditional benchmark datasets as well as improved adversarial robustness.

## 2 Background

We first cover the fundamentals of group-equivariant neural networks--also known as \(G\)-CNNs, or \(G\)-Equivariant Networks--before introducing the framework for \(G\)-Invariant Pooling.

### Mathematical Prerequisites

The construction of \(G\)-CNNs requires mathematical prerequisites of group theory, which we recall here. The interested reader can find details in .

**Groups.** A _group_\((G,)\) is a set \(G\) with a binary operation \(\), which we can generically call the _product_. The notation \(g_{1} g_{2}\) denotes the product of two elements in the set; however, it is standard to omit

Figure 1: **Achieving Robust \(G\)-Invariance in \(G\)-CNNs with the \(G\)-Triple-Correlation**. The output of a \(G\)-Convolutional layer is equivariant to the actions of \(G\) on the domain of the signal. To identify signals that are equivalent up to group action, the layer can be followed by a \(G\)-Invariant map that eliminates this equivariance. In \(G\)-CNNs, Max \(G\)-Pooling is a commonly used for this purpose. Taking the maximum of the \(G\)-Convolutional equivariant output is indeed invariant to the actions of the group. However, it is also lossy: many non-equivalent output vectors have the same maximum. Our methodâ€”the \(G\)_-Triple-Correlation_ is the lowest-order polynomial invariant map that is _complete_. As a complete invariant, it preserves all information about the signal structure, removing only the action of the group. Our approach thus provides a new foundation for achieving robust \(G\)-Invariance in \(G\)-CNNs.

the operator and write simply \(g_{1}g_{2}\)--a convention we adopt here. Concretely, a group \(G\) may define a class of transformations. For example, we can consider the group of two-dimensional rotations in the plane--the special orthogonal group \(SO(2)\)--or the group of two-dimensional rotations and translations in the plane--the special euclidean group \(SE(2)\). Each element of the group \(g G\) defines a _particular_ transformation, such as one _rotation by \(30^{}\)_ or one _rotation by \(90^{}\)_. The binary operation \(\) provides a means for combining two particular transformations--for example, first rotating by \(30^{}\) and then rotating by \(90^{}\). In mathematics, for a set of transformations \(G\) to be a group under the operation \(\), the four axioms of closure, associativity, identity and inverse must hold. These axioms are recalled in Appendix A.

**Group Actions on Spaces.** We detail how a transformation \(g\) can transform elements of a space, for example how a rotation of \(30^{}\) indeed rotates a vector in the plane by \(30^{}\). We say that the transformations \(g\)'s act on (the elements of) a given space. Specifically, consider \(X\) a space, such as the plane. A _group action_ is a function \(L:G X X\) that maps \((g,x)\) pairs to elements of \(X\). We say a group \(G\)_acts_ on a space \(X\) if the following properties of the action \(L\) hold:

1. _Identity_: The identity \(e\) of the group \(G\) "does nothing", i.e., it maps any element \(x X\) to itself. This can be written as: \(L(e,x)=x\).
2. _Compatibility_: Two elements \(g_{1},g_{2} G\) can be combined before or after the map \(L\) to yield the same result, i.e., \(L(g_{1},L(g_{2},x))=L(g_{1}g_{2},x)\). For example, rotating a 2D vector by \(30^{}\) and then \(40^{}\) yields the same result as rotating that vector by \(70^{}\) in one time.

For simplicity, we will use the shortened notation \(L_{g}(x)\) to denote \(L(g,x)\) the action of the transformation \(g\) on the element \(x\).

Some group actions \(L\) have additional properties and turn the spaces \(X\) on which they operate into _homogeneous spaces_. Homogeneous spaces play an important role in the definition of the \(G\)-convolution in \(G\)-CNNs, so that we recall their definition here. We say that \(X\) is a _homogeneous space_ for a group \(G\) if \(G\) acts transitively on \(X\)--that is, if for every pair \(x_{1},x_{2} X\) there exists an element of \(g G\) such that \(L_{g}(x_{1})=x_{2}\). The concept can be clearly illustrated by considering the surface of a sphere, the space \(S^{2}\). The sphere \(S^{2}\) is a homogeneous space for \(SO(3)\), the group of orthogonal \(3 3\) matrices with determinant one that define 3-dimensional rotations. Indeed, for every pair of points on the sphere, one can define a 3D rotation matrix that takes one to the other.

**Group Actions on Signal Spaces.** We have introduced essential concepts from group theory, where a group \(G\) can act on any abstract space \(X\). Moving towards building \(G\)-CNNs, we introduce how groups can act on spaces of signals, such as images. Formally, a _signal_ is a map \(f:^{c}\), where \(\) is called the domain of the signal and \(c\) denotes the number of channels. The _space of signals_ itself is denoted \(L_{2}(,^{c})\). For example, \(=^{2}\) or \(^{3}\) for 2D and 3D images. Gray-scale images have one channel (\(c=1\)) and color images have the 3 red-green-blue channels (\(c=3\)).

Any action of a group of transformations \(G\) on a domain \(\) yields an action of that same group on the spaces of signals defined on that domain, i.e., on \(L_{2}(,^{c})\). For example, knowing that the group of 2D rotations \(SO(2)\) acts on the plane \(=^{2}\) allows us to define how \(SO(2)\) rotates 2D gray-scale images in \(L_{2}(^{2},^{c})\). Concretely, the action \(L\) of a group \(G\) on the domain \(\) yields the following action of \(G\) on \(L_{2}(,^{c})\):

\[L_{g}[f](u)=f(L_{g^{-1}}(u)),.\] (1)

We use the same notation \(L_{g}\) to refer to the action of the transformation \(g\) on either an element \(u\) of the domain or on a signal \(f\) defined on that domain, distinguishing them using \([]\) for the signal case. We note that the domain of a signal can be the group itself: \(=G\). In what follows, we will also consider actions on real signals defined on a group, i.e., on signals such as \(:G\).

**Invariance and Equivariance**. The concepts of group-invariance and equivariance are at the core of what makes the \(G\)-CNNs desirable for computer vision applications. We recall their definitions here. A function \(:X Y\) is _\(G\)-invariant_ if \((x)=(L_{g}(x))\), for all \(g G\) and \(x X\). This means that group actions on the input space have no effect on the output. Applied to the group of rotations acting on the space of 2D images \(X=L_{2}(,^{c})\) with \(=^{2}\), this means that a \(G\)-invariant function \(\) produces an input that will stay the same for any rotated version of a given signal. For example, whether the image contains the color red is invariant with respect to any rotation of that image. A function \(:X Y\) is _\(G\)-equivariant_ if \((L_{g}(x))=L_{g}^{}((x))\) for all \(g G\) and \(x X\), where \(L\) and \(L^{}\) are two different actions of the group \(G\), on the spaces \(X\) and \(Y\) respectively. This means that a group action on the input space results in a corresponding group action of the same group element \(g\) on the output space. For example, consider \(\) that represents a neural network performing a foreground-background segmentation of an image. It is desirable for \(\) to be equivariant to the group of 2D rotations. This equivariance ensures that, if the input image \(f\) is rotated by \(30^{}\), then the output segmentation \((f)\) rotates by \(30^{}\) as well.

### \(G\)-Equivariant Networks

\(G\)-CNNs are built from the following fundamental building blocks: \(G\)-convolution, spatial pooling, and \(G\)-pooling. The \(G\)-convolution is equivariant to the action of the group \(G\), while the \(G\)-pooling achieves \(G\)-invariance. Spatial pooling achieves coarse-graining. We review the group-specific operations here. The interested reader can find additional details in [8; 10], which include the definitions of these operations using the group-theoretic framework of principal bundles and associated vector bundles.

#### 2.2.1 \(G\)-Convolution

In plain language, a standard translation-equivariant convolutional neural network layer sweeps filters across a signal (typically, an image), translating the filter and then taking an inner product with the signal to determine the similarity between a local region and the filter. \(G\)-CNNs  generalize this idea, replacing translation with the action of other groups that define symmetries in a machine learning task--for example, rotating a filter, to determine the presence of a feature in various orientations.

Consider a signal \(f\) defined on a domain \(\) on which a group \(G\) acts. A neural network filter is a map \(:^{c}\) defined with the same domain \(\) and codomain \(^{c}\) as the signal. A \(G\)-convolutional layer is defined by a set of filters \(\{_{1},...,_{K}\}\). For a given filter \(k\), the layer performs a \(G\)_-convolution_ with the input signal \(f\):

\[_{k}(g)=(_{k}*f)(g)=_{u}_{k}(L_{g^{-1}}(u))f(u)du,  g G,\] (2)

by taking the dot product in \(^{c}\) of the signal with a transformed version of the filter. In practice, the domain \(\) of the signal is discretized, such that the \(G\)-convolutional layer becomes:

\[_{k}(g)=_{u}_{k}(L_{g^{-1}}(u))f(u), g G.\] (3)

The output of one filter \(k\) is therefore a map \(_{k}:G\), while the output of the whole layer with \(K\) filters is \(:G^{K}\) defined as \((g)=[_{1}(g),,_{K}(g)]\) for all \(g G\). The \(G\)-convolution therefore outputs a signal \(\) whose domain has necessarily become the group \(=G\) and whose number of channels is the number of convolutional filters \(K\).

The \(G\)-convolution is _equivariant_ to the action of the group on the domain of the signal \(f\). That is, the action of \(g\) on the domain of \(f\) results in a corresponding action on the output of the layer. Specifically, consider a filter \(_{k}\), we have:

\[_{k}*L_{g}[f]=L_{g}^{}[_{k}*f], g G,\] (4)

where \(L_{g}\) and \(L_{g}^{}\) represent the actions of the same group element \(g\) on the functions \(f\) and \(_{k}*f\) respectively. This property applies for the \(G\)-convolutions of the first layer and of the next layers .

#### 2.2.2 \(G\)-Pooling

_Invariance_ to the action of the group is achieved by pooling over the group (\(G\)-Pooling) . The pooling operation is typically performed after the \(G\)-convolution, so that we restrict its definition to signals \(\) defined over a group \(G\). In \(G\)-pooling, a max typically is taken over the group elements:

\[_{k}=_{g G}_{k}(g).\] (5)

\(G\)-pooling extracts a single real scalar value \(_{k}\) from the full feature vector \(_{k}\), which has \(|G|\) values, with \(|G|\) the size of the (discretized) group \(G\) as shown in Fig. 1. When the group \(G\) is a grid discretizing \(^{n}\), max \(G\)-Pooling is equivalent to the standard spatial max pooling used in translation-equivariant CNNs, and it can be used to achieve coarse-graining. More generally, \(G\)-Pooling is \(G\)-invariant, as shown in . However, we argue here that it is excessively \(G\)-invariant. Although it achieves the objective of invariance to the group action, it also loses substantial information. As illustrated in Fig. 1, many different signals \(\) may yield same result \(\) through the \(G\)-pooling operation, even if these signals do not share semantic information. This excessive invariance creates an opportunity for adversarial susceptibility. Indeed, inputs \(f\) can be designed with the explicit purpose of generating a \(_{k}\) that will fool a neural network and yield an unreasonable classification result. For this reason, we introduce our general framework for robust, selective \(G\)-invariance.

## 3 The \(G\)-Triple-Correlation Layer for Robust \(G\)-Invariance

We propose a \(G\)-Invariant layer designed for \(G\)-CNNs that is _complete_--that is, it preserves all information about the input signal except for the group action. Our approach leverages the theory of the triple correlation on groups  and applies it to the design of robust neural network architectures. Its theoretical foundations in signal processing and invariant theory allows us to generally define the unique \(G\)-invariant maps of lowest polynomial order that are complete, hence providing a general framework for selective, robust \(G\)-invariance in \(G\)-CNNs .

### The \(G\)-Triple-Correlation Layer

The \(G\)_-Triple-Correlation_ (\(G\)-TC) on a real signal \(:G\) is the integral of the signal multiplied by two independently transformed copies of it :

\[_{}(g_{1},g_{2})=_{g G}(g)(gg_{1}) (gg_{2})dg.\] (6)

This definition holds for any locally compact group \(G\) on which we can define the Haar measure \(dg\) used for integration purposes . This definition above is applicable to the \(G\)-CNNs where \(\) is a collection of scalar signals over the group. We show in Appendix B that we can extend the definition to steerable \(G\)-CNNs where \(\) can be an arbitrary field .

In the equation above, the \(G\)-TC is computed for a pair of group elements \(g_{1},g_{2}\). In practice, we sweep over all pairs in the group. Appendix C illustrates the triple correlation on three concrete groups. Importantly, the \(G\)-triple-correlation is invariant to the action of the group \(G\) on the signal \(\), as shown below.

**Proposition 1**.: _Consider a signal \(:G^{c}\). The \(G\)-Triple-Correlation \(\) is \(G\)-invariant:_

\[_{L_{g}[]}=_{},\] (7)

_where \(L_{g}\) denotes an action of a transformation \(g\) on the signal \(\)._

The proof is recalled in Appendix D. We propose to achieve \(G\)-invariance in a \(G\)-CNN by applying the \(G\)-Triple-Correlation (\(G\)-TC) to the output \(\) of a \(G\)-convolutional layer. Specifically, we apply the \(G\)-TC to each real scalar valued signal \(_{k}\) that comes from the \(G\)-convolution of filter \(_{k}\), for \(k\{1,...,K\}\). We only omit the subscript \(k\) for clarity of notations. In practice, we will use the triple correlation on discretized groups, where the integral is replaced with a summation:

\[T_{}(g_{1},g_{2})=_{g G}(g)(gg_{1})(gg_{2}),\] (8)

for \(\) a scalar valued function defined over \(G\). While it seems that the layer computes \(T_{}(g_{1},g_{2})\) for all pairs of group elements \((g_{1},g_{2})\), we note that the real scalars \((gg_{1})\) and \((gg_{2})\) commute so that only half of the pairs are required. We will see that we can reduce the number of computations further when the group \(G\) possesses additional properties such as commutativity.

We note that the triple correlation is the spatial dual of the _bispectrum_, which has demonstrated robustness properties in the context of deep learning with bispectral neural networks . The goal of bispectral neural networks is to learn an unknown group \(G\) from data. The bispectral layer proposed in  assumes an MLP architecture. Our work is the first to generalize the use of bispectral invariants to convolutional networks. Here, we assume that the group \(G\) is known in advance, and exploit the theoretical properties of the triple correlation to achieve robust invariance. One path for future extension may be to combine our approach with the learning approach of , to parameterize and learn the group \(G\) that defines a \(G\)-Equivariant and \(G\)-Invariant layer.

### Selective Invariance through Completeness

We show here that the proposed \(G\)-triple-correlation is guaranteed to preserve all information aside from any equivariant component due to the group action on the input domain. This crucial property distinguishes our proposed layer from standard \(G\)-Pooling methods, which collapse signals and lose crucial information about the input (Figure 1). In contrast with standard, excessively invariant \(G\)-pooling methods, we show here that our \(G\)-TC layer is instead _selectively_\(G\)-invariant thanks to its _completeness_ property [54; 29; 31], defined here:

**Proposition 2**.: _Every integrable function with compact support \(G\) is completely identified--up to group action--by its \(G\)-triple-correlation. We say that the \(G\)-triple-correlation is complete._

Mathematically, an operator \(\) is complete for a group action \(L\) if the following holds: for every pair of signals \(_{1}\) and \(_{2}\), if \((_{1})=(_{2})\) then the signals are equal up to the group action, that is: there exists a group element \(h\) such that \(_{2}=L_{h}[_{1}]\).

The proof of the completeness of the \(G\)-triple-correlation is only valid under a precise set of assumptions  (Theorem 2). As we seek to integrate the \(G\)-triple-correlation to enhance robustness in neural networks, we investigate here the scope of these assumptions. First, the assumptions are not restrictive on the type of groups \(G\) that can be used. Indeed, the proof only requires the groups to be Tatsuuma duality groups and the groups of interest in this paper meet this condition. This includes all locally compact commutative groups, all compact groups including the groups of rotations, the special orthogonal groups \(SO(n)\), and groups of translations and rotations, the special euclidean groups \(SE(n)\). Second, the assumptions are not restrictive on the types of signals. Indeed, the signal only needs to be such that any of its Fourier transform coefficients are invertible. For example, when the Fourier transform coefficients are scalar values, this means that we require these scalars to be non-zero. In practical applications on real image data with noise, there is a probability 0 that the Fourier transform coefficients of the input signal will be exactly 0 (scalar case) or non-invertible (matrix case). This is because the group of invertible matrices is dense in the space of matrices. Therefore, this condition is also verified in the applications of interest and more generally we expect the property of completeness of our \(G\)-TC layer to hold in practical neural network applications.

### Uniqueness

The above two subsections prove that our \(G\)-Triple Correlation layer is selectively \(G\)-invariant. Here, we note that our proposed layer is the lowest-degree polynomial layer that can achieve this goal. In invariant theory, it is observed that the \(G\)-Triple Correlation is the _only_ third-order polynomial invariant (up to change of basis) . Moreover, it is the lowest-degree polynomial invariant that is also complete. It thus provides a unique and minimal-complexity solution to the problem of robust invariance within this function class.

### Computational Complexity

The \(G\)-Triple Correlation enjoys some symmetries that we can leverage to avoid computing it for each pair of group elements (which would represent \(|G|^{2}\) computations), hence making the feedforward pass more efficient. We summarize these symmetries here.

**Proposition 3**.: _Consider two transformations \(g_{1},g_{2} G\). The \(G\)-Triple Correlation of a real signal \(\) has the following symmetry:_

\[T_{}(g_{1},g_{2})=T_{}(g_{2},g_{1}).\]

_If \(G\) is commutative, the \(G\)-Triple Correlation of a real signal has the following additional symmetries:_

\[T_{}(g_{1},g_{2})=T_{}(g_{1}^{-1},g_{2}g_{1}^{-1})=T_{}(g_{ 2}g_{1}^{-1},g_{1}^{-1})=T_{}(g_{2}^{-1},g_{1}g_{2}^{-1})=T_{}(g_{ 1}g_{2}^{-1},g_{2}^{-1}).\]

The proofs are given in  for the group of translations. We extend them to any locally compact group \(G\) in Appendix E. In practice, these symmetries mean that even if there are theoretically \(|G|^{2}\) computations, this number immediately reduces to \(\) and further reduces if the group \(G\) of interest is commutative. In addition, more subtle symmetries can be exploited to reduce the computational cost to linear \(|G|+1\) for the case of one-dimensional cyclic groups  by considering the spectral dual of the \(G\)-TC: the bispectrum. We provide a computational approach to extend this reduction to more general, non-commutative groups in Appendix F. The theory supporting our approach has yet to be extended to this general case. Thus, there is an opportunity for new theoretical work that further increases the computational efficiency of the \(G\)-Triple-Correlation.

## 4 Related Work

**The Triple Correlation.** The triple correlation has a long history in signal processing [48; 5; 39]. It originally emerged from the study of the higher-order statistics of non-Gaussian random processes, but its invariance properties with respect to translation have been leveraged in texture statistics  and data analysis in neuroscience , as well as early multi-layer perceptron architectures in the 1990's [12; 33]. The triple correlation was extended to groups beyond translations in , and its completeness with respect to general compact groups was established in . To the best of our knowledge, the triple correlation has not previously been introduced as a method for achieving invariance in convolutional networks for either translation or more general groups.

**Pooling in CNNs.** Pooling in CNNs typically has the dual objective of coarse graining and achieving local invariance. While invariance is one desiderata for the pooling mechanism, the machinery of group theory is rarely employed in the computation of the invariant map itself. As noted in the introduction, max and average pooling are by far the most common methods employed in CNNs and \(G\)-CNNs. However, some approaches beyond strict max and average pooling have been explored. Soft-pooling addresses the lack of smoothness of the max function and uses instead a smooth approximation of it, with methods including polynomial pooling  and learned-norm , among many others [15; 14; 43; 44; 3; 45; 11; 35]. Stochastic pooling  reduces overfitting in CNNs by introducing randomness in the pooling, yielding mixed-pooling , max pooling dropout , among others [47; 58; 21]

**Geometrically-Aware Pooling.** Some approaches have been adopted to encode spatial or structural information about the feature maps, including spatial pyramid pooling , part-based pooling , geometric \(L_{p}\) pooling  or pooling regions defined as concentric circles . In all of these cases, the pooling computation is still defined by a max. These geometric pooling approaches are reminiscent of the Max \(G\)-Pooling for \(G\)-CNNs introduced by  and defined in Section 2.2.2, without the explicit use of group theory.

**Higher-Order Pooling.** Average pooling computes first-order statistics (the mean) by pooling from each channel separately and does not account for the interaction between different feature maps coming from different channels. Thus, second-order pooling mechanisms have been proposed to consider correlations between features across channels [38; 19], but higher-orders are not investigated. Our approach computes a third-order polynomial invariant; however, it looks for higher-order correlations within the group rather than across channels and thus treats channels separately. In principle, these approaches could be combined.

## 5 Experiments & Results

### Implementation

We implement the \(G\)-TC Layer for arbitrary discretized groups with an efficient implementation built on top of the ESCNN library [7; 50], which provides a general implementation of \(E(n)\)-Equivariant Steerable Convolutional Layers. The method is flexibly defined, requiring the user only to provide a (Cayley) table that defines the group's product structure. The code is publicly available at https://github.com/sophiaas/gtc-invariance. Here, we demonstrate the approach on the groups \(SO(2)\), and \(O(2)\), \(SO(3)\), and \(O(3)\), discretized as the groups \(C_{n}\) (cyclic), \(D_{n}\) (dihedral), \(O\) (chiral octahedral), and \(O_{h}\) (full octahedral), respectively. ESCNN provides implementations for \(G\)-Conv layers on all of these \(E(n)\) subgroups.

### Experimental Design

We examine the performance of the \(G\)-TC over Max \(G\)-Pooling in \(G\)-Equivariant Networks defined on these groups and trained on \(G\)-Invariant classification tasks. For the groups \(SO(2)\) and \(O(2)\) acting on \(^{2}\), we use the MNIST dataset of handwritten characters , and for the groups \(SO(3)\) and \(O(3)\) acting on \(^{3}\), we use the voxelized ModelNet10 database of 3D objects . We generate \(G\)-MNIST and \(G\)-ModelNet10 datasets by transforming the domain of each signal in the dataset by a randomly sampled group element \(g G\) (Figure 2).

In these experiments, we train pairs of models in parameter-matched architectures, in which only the \(G\)-Pooling method differs. Note that the purpose of these experiments is to compare _differences in performance_ between models using Max \(G\)-Pooling vs. the \(G\)-TC--not to achieve SOTA accuracy. Thus, we do not optimize the models for overall performance. Rather, we fix a simple architecture and set of hyperparameters and examine the change in performance that arises from replacing Max \(G\)-Pooling with the \(G\)-TC Layer (Figure 3).

To isolate the effects of the \(G\)-Pooling method, all models are comprised of a single \(G\)-Conv block followed by \(G\)-Pooling (Max or TC) and an MLP Classifier. Notably, while many \(G\)-Conv models in the literature use the semi-direct product of \(G\) with \(^{n}\)--i.e. incorporating the actions of the group \(G\) into a standard translational convolutional model--here, we perform only _pure_\(G\)-Conv, without translation. Thus, we use filters the same size as the input in all models. The \(G\)-Conv block is comprised of a \(G\)-Conv layer, a batch norm layer, and an optional nonlinearity. For the

Figure 3: **Models.** We compare two simple architectures comprised of a single G-Conv block followed by either a Max \(G\)-Pool layer or a \(G\)-TC Layer and an MLP Classifier.

Figure 2: **Datasets.** The \(O(2)\)-MNIST (top) and \(O(3)\)-ModelNet10 (bottom) datasets are generated by applying a random (rotation, reflection) pair to each element of the original datasets. Although we visualize the continuous group here, in practice, we discretize the group \(O(3)\) as the full octahedral group \(O_{h}\) to reduce computational complexity. \(SO(2)\) and \(SO(3)\) datasets are generated similarly, by applying a random rotation to each datapoint.

Max \(G\)-Pool model, ReLU is used as the nonlinearity. Given the third-order nonlinearity of the TC, we omit the nonlinearity in the \(G\)-Conv block in the TC Model. The \(G\)-TC layer increases the dimensionality of the output of the \(G\)-Conv block; consequently the input dimension of the first layer of the MLP is larger and the weight matrix contains more parameters than for the Max \(G\)-Pool model. To compensate for this, we increase the dimension of the output of the first MLP layer in the Max Model, to match the overall number of parameters.

#### Evaluation Methods

We evaluate the models in two ways. First, we examine differences in the raw _classification accuracy_ obtained by replacing Max \(G\)-Pooling with the \(G\)-TC Layer. Second, we assess the _completeness_ of the model by optimizing "metameric" stimuli for the trained models--inputs that yield the same pre-classifier representation as a target input, but are perceptually distinct. The completeness evaluation is inspired by a recent paper that incorporates the bispectrum--the spectral dual of the triple correlation--into a neural network architecture trained to yield \(G\)-invariant representations for \(G\)-transformed data . In this work, two inputs are considered "perceptually distinct" if they are not in the same group orbit. They find that all inputs optimized to yield the same representation in the bispectral model are identical up to the group action. By contrast, many metameric stimuli can be found for \(E(2)\)-CNN , a \(G\)-Equivariant CNN that uses Max \(G\)-Pooling. Given the duality of the bispectrum and the triple correlation, we expect to observe similar "completeness" for \(G\)-CNNs using the \(G\)-TC Layer.

### Classification Performance

We train \(G\)-TC and Max \(G\)-Pooling models on the \(SO(2)\) and \(O(2)\)-MNIST and chiral (\(O\)) and full (\(O_{h}\)) octahedral voxelized ModelNet10 training datasets and examine their classification performance on the test set. Full training details including hyperparameters are provided in Appendix G. Table 1 shows the test classification accuracy obtained by the Max-\(G\) and \(G\)-TC architectures on each dataset. Accuracy is averaged over four random seeds, with confidence intervals showing standard deviation. We find that the model equipped with \(G\)-TC obtains a significant improvement in overall classification performance--an increase of \(1.3\), \(0.89\), \(1.84\) and \(3.49\) percentage points on \(SO(2)\)-MNIST, \(O(2)\)-MNIST, \(O\)-ModelNet10 and \(O_{h}\)-ModelNet10 respectively.

### Completeness

Following the analysis of , we next evaluate the completeness of the models trained on the \(G\)-MNIST Dataset. Figure 4 shows inputs optimized to yield the same pre-classifier representation as a set of target images. In line with similar findings from , we find that all inputs yielding an identical representations and classifications in the \(G\)-TC Model are within the same group orbit. Notably, the optimized images are identical to the targets, _up to the group action_. This reflects exactly the completness of the \(G\)-TC: the \(G\)-TC preserves all signal structure up to the group action.

    & \(C8\)**-CNN on \(SO(2)\)-MNIST** & \(D16\)**-CNN on \(O(2)\)-MNIST** \\  Method & Accuracy & Parameters & Accuracy & Parameters \\  Max \(G\)-Pool & 95.23 \(\) 0.15 & 32,915 & 92.17\% \(\) 0.23 & 224,470 \\ \(G\)**-TC** & **96.53 \(\) 0.16** & 35,218 & **93.06 \% \(\) 0.09** & 221,074 \\    & \(O\)**-CNN on \(O\)-ModelNet10** & \(O_{h}\)**-CNN on \(O_{h}\)**-ModelNet10** \\  Method & Accuracy & Parameters & Accuracy & Parameters \\  Max \(G\)-Pool & 72.17\% \(\) 0.95 & 500,198 & 71.73\% \(\) 0.23 & 1,826,978 \\ \(G\)**-TC** & **74.01\% \(\) 0.48** & **472,066** & **75.22\% \(\) 0.62** & **1,817,602** \\   

Table 1: **Classification Accuracy & Parameter Counts for Models Trained on \(G\)-MNIST and \(G\)-ModelNet10**. Confidence intervals reflect standard deviation over four random seeds per model. The model equipped with \(G\)-TC rather than Max \(G\)-Pooling obtains significantly improved classification performance on all datasets.

Thus, any rotated version of the a target will yield the same \(G\)-TC Layer output. By contrast, many "metameric" misclassified stimuli can be found for the Max \(G\)-Pool Model, a consequence of the lossiness of this pooling operation.

## 6 Discussion

In this work, we introduced a new method for achieving robust group-invariance in group-equivariant convolutional neural networks. Our approach, the \(G\)_-TC Layer_, is built on the _triple correlation_ on groups, the lowest-degree polynomial that is a complete group-invariant map . Our method inherits its completeness, which provides measurable gains in robustness and classification performance as compared to the ubiquitous Max \(G\)-Pooling.

This improved robustness comes at a cost: the \(G\)-TC Layer increases the dimension of the output of a \(G\)-Convolutional layer from \(G\) to \(\). While the dimension of the discretized groups used in \(G\)-CNNs is typically small, this increase in computational cost may nonetheless deter practitioners from its use. However, there is a path to further reduction in computational complexity provided that we consider its spectral dual: the bispectrum. In , an algorithm is provided that exploits more subtle symmetries of the bispectrum to demonstrate that only \(|G|+1\) terms are needed to provide a complete signature of signal structure, for the one-dimensional cyclic group. In Appendix F, we extend the computational approach from  to more general groups and provided a path for substantial reduction in the complexity of the \(G\)-TC Layer, thus expanding its practical utility. Novel mathematical work that grounds our proposed computations in group theory is required to quantify the exact complexity reduction that we provide.

As geometric deep learning is applied to increasingly complex data from the natural sciences , we expect robustness to play a critical role in its success. Our work is the first to introduce the general group-invariant triple correlation as a new computational primitive for geometric deep learning. We expect the mathematical foundations and experimental successes that we present here to provide a basis for rethinking the problems of invariance and robustness in deep learning architectures.

Figure 4: **Optimized Model Metamers. For each model, 100 targets from the MNIST dataset were randomly selected. 100 inputs were randomly initalized and optimized to yield identical pre-classifier model presentations. All inputs optimized for the \(G\)-TC Model converge to the orbit of the target. By contrast, metamers that bear no semantic relationship to the targets are found for every target in the Max \(G\)-Pooling model.**