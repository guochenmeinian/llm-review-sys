# Regret-Optimal Model-Free Reinforcement Learning for Discounted MDPs with Short Burn-In Time

Xiang Ji

Princeton

Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ 08544, USA.

Gen Li

CUHK

Department of Statistics, The Chinese University of Hong Kong, Hong Kong, China.

###### Abstract

A crucial problem in reinforcement learning is learning the optimal policy. We study this in tabular infinite-horizon discounted Markov decision processes under the online setting. The existing algorithms either fail to achieve regret optimality or have to incur a high memory and computational cost. In addition, existing optimal algorithms all require a long burn-in time in order to achieve optimal sample efficiency, i.e., their optimality is not guaranteed unless sample size surpasses a high threshold. We address both open problems by introducing a model-free algorithm that employs variance reduction and a novel technique that switches the execution policy in a slow-yet-adaptive manner. This is the first regret-optimal model-free algorithm in the discounted setting, with the additional benefit of a low burn-in time.

## 1 Introduction

In reinforcement learning (RL), a crucial task is to find the optimal policy that maximizes its expected cumulative reward in any given environment with unknown dynamics. An immense body of literature is dedicated to finding algorithms that solve this task with as few samples as possible, which is the prime goal under this task. Ideally, one hopes to find an algorithm with a theoretical guarantee of optimal sample efficiency. At the same time, this task might be accompanied with additional requirements such as low space complexity and computational cost, as it is common that the state and action spaces exhibit high dimensions in modern applications. The combination of these various goals and requirements presents an important yet challenging problem in algorithm design.

The task of searching for optimal policy has been well-studied by existing work in the generative setting [33; 34; 21; 1]. This fundamental setting allows the freedom of querying samples at any state-action pair. In contrast, it is more realistic but difficult to consider the same task in the online setting, in which samples can only be collected along trajectories generated from executing a policy in the unknown Markov decision process (MDP). Solving this task with optimal sample efficiency requires a careful balance between exploration and exploitation, especially when coupled with other goals such as memory and computational efficiency.

MDPs can be divided into two types: the episodic finite-horizon MDPs and the infinite-horizon MDPs. Although these two types of MDPs can be approached in similar ways under the generative setting, there is a clear dichotomy between them in the online setting. In an episodic MDP, sample trajectories are only defined in fixed-length episodes, so samples are collected in episodes, and a reset to an arbitrary initial state occurs at the end of every online episode. Its transition kernel is usually assumed to be non-stationary over time. In contrast, the transition kernel of an infinite-horizon MDP stays stationary over time, and the online sample collection process amounts to drawing a single infinitely long sample trajectory with no reset. These differences render most optimal algorithms for episodicMDPs suboptimal when applied to infinite-horizon MDPs. Without reset and non-stationarity, the high dependency between consecutive trajectory steps in the infinite-horizon setting presents a new challenge over the episodic setting. In this work, we consider the infinite-horizon discounted MDPs, which is widely used in practice but still has some fundamental questions unanswered in theory.

### Sample Efficiency in Infinite-Horizon MDPs

To evaluate the sample efficiency of online RL algorithms, a natural and widely-accepted metric is the _cumulative regret_. It captures the performance difference between the optimal policy and the learned policy of an algorithm over its online interactions with a given MDP. The notion of cumulative regret was first introduced in the bandit literature and later adopted in the RL literature [2; 15]. It is profusely used in the online episodic RL literature. Such works aim to prove regret guarantees for algorithms and provide analyses that characterize such regret guarantees in terms of all problem parameters such as state space, action space and sample size in a non-asymptotic fashion. A cumulative regret guarantee can also suggest the sample complexity needed to reach a certain level of average regret.

In the online infinite-horizon setting, many works study a different metric called the sample complexity of exploration, first introduced in . In essence, given a target accuracy level \(\), this metric characterizes the total number of \(\)-suboptimal steps committed by an algorithm over an infinitely-long trajectory in the MDP. While this is indicative of the sample efficiency of an algorithm, the focus of this metric is very different from that of cumulative regret, as it only reflects the total number of failures but does not distinguish their sizes. As [24; 12] point out, even an optimal guarantee on the sample complexity of exploration can only be converted to a very suboptimal guarantee on the cumulative regret. To obtain a more quantitative characterization of the total volume of failures in the regime of finite samples, some works have turned to studying cumulative regret guarantees for algorithms.

It was not until recently that some works [24; 51; 12; 18] begin to research into the problem of cumulative regret minimization in infinite-horizon discounted MDPs. Among them,  focus on linear MDPs while others study tabular MDPs. In this work, we study the regret minimization problem in the tabular case. Hereafter and throughout, we denote the size of the state space, the size of the action space and the discount factor of the problem MDP with \(S\), \(A\) and \(\), respectively, and let \(T\) denote the sample size.

### Model-Based and Model-Free Methods

Since modern RL applications are often large-scale, algorithms with low space complexity and computational complexity are much desired. This renders the distinction between model-based algorithms and model-free algorithms particularly important. The procedure of a model-based method includes a model estimation stage that involves estimating the transition kernel and a subsequent planning stage that searches the optimal policy in the learned model. Thus, \(O(S^{2}A)\) space is required to store the estimated model. This is unfavorable when the state space is large and a memory constraint is present. Additionally, updating the transition kernel estimate brings a large computational burden. In comparison, model-free methods do not learn the entire model and thus can run with \(o(S^{2}A)\) space. Notably, most value-based methods such as Q-learning only require storage of an estimated Q-function, which can take as little as \(O(SA)\) memory. In the infinite-horizon discounted setting, although UCBVI-\(\) in  can achieve optimal regret, its model-based nature excats a \(O(S^{2}A)\) memory and computational cost; conversely, the algorithms in  and  are model-free but have suboptimal regret guarantee.

### Burn-in Cost in Regret-Optimal RL

Naturally, one aims to develop algorithms that find the optimal policy with the fewest number of samples. In regards to regret, this motivates numerous works to work towards algorithms with minimax-optimal cumulative regret. However, the job is not done once such an algorithm is found. As can be seen in the episodic RL literature, algorithms that achieve optimal regret as sample size \(T\) tends towards infinity can still have different performance in the regime when \(T\) is limited. Specifically, for every existing algorithm, there exists a certain sample size threshold such that regret is suboptimal before \(T\) exceeds it. Such threshold is commonly referred to as the initial _burn-in time_ of the algorithm. Therefore, it is of great interest to find an algorithm with low burn-in time so that it can still attain optimal regret in the sample-starved regime. Such effort has been made by [21; 1] in the generative setting and by [23; 26] in the online episodic setting. Yet, this important issue has not been addressed in the infinite-horizon setting, as optimal algorithms all suffer long burn-in times.

Specifically, while UCBVI-\(\) in  achieves a state-of-the-art regret guarantee of \(}}\), which they prove minimax-optimal, their theory does not guarantee optimality unless the samples size \(T\) becomes as large as

\[TA^{2}}{(1-)^{4}}.\]

  & Sample complexity & Cumulative & Range of \(T\) with & Space \\ Algorithm & of exploration & Regret & optimal regret & complexity \\  Delayed Q-learning  & \(^{4}}\) & \(}A^{}T^{}}{(1-)^{}}\) & never & \(SA\) \\  R-Max & \(A}{(1-)^{6}^{3}}\) & \(}A^{}T^{}}{(1-)^{}}\) & never & \(S^{2}A\) \\  UCB-Q & \(^{2}}\) & \(}A^{}T^{}}{(1-)^{}}\) & never & \(SA\) \\  MoRmax & \(^{2}}\) & \(}A^{}T^{}}{(1-)^{}}\) & never & \(S^{2}A\) \\  UCRL  & \(A}{(1-)^{3}^{2}}\) & \(}A^{}T^{}}{(1-)^{}}\) & never & \(S^{2}A\) \\  UCB-multistage  & \(}^{2}}\) & \(}A^{}T^{}}{(1-)^{}}\) & never & \(SA\) \\  UCB-multistage-adv1  & \(^{2}}\) & \(}A^{}T^{}}{(1-)^{}}\) & never & \(SA\) \\  MAIN2  & N/A & \(+S^{2}A^{2})T}{(1-)^{8}}}\) & never & \(SA\) \\  Double Q-learning  & N/A & \(}}\) & never & \(SA\) \\  UCBVI-\(^{3}\)  & N/A & \(}}\) & \([A^{2}}{(1-)^{4}},)\) & \(S^{2}A\) \\  Q-SlowSwitch-Adv (**This work**) & N/A & \(}}\) & \([},)\) & \(SA\) \\  Lower bound ;  & \(^{2}}\) & \(}}\) & N/A & N/A \\ 

Table 1: A comparison between our results and existing work in the online infinite-horizon discounted setting. Logarithmic factors are omitted for clearer presentation. The second column shows the sample complexity when the target accuracy \(\) is sufficiently small. The third column shows the regret when sample size \(T\) is sufficiently large (beyond the burn-in period). The algorithms in the first seven rows only have sample complexity results in their original works; their regret bounds are derived from their respective sample complexity bounds and presented in this table for completeness. Details about the conversions can be found in . The fourth column lists the sample size range in which regret optimality can be attained, which shows the burn-in time. We would like to point out that the results in [12; 24] are under slightly different regret definitions from the regret definition in  and this work. In fact, their regret metric can be more lenient, and our algorithm can also achieve \((}})\) optimal regret under it. This is further discussed in Remark 1 and Appendix B.

This threshold can be prohibitively large when \(S\) and \(A\) are huge, which is true in most applications. Thus, this makes reducing these factors in the burn-in cost particularly important. For instance, a 5-by-5 tic-tac-toe has a state space of size \(3^{25}\). While this is a manageable number in modern machine learning, any higher power of it may cause computational difficulties; in contrast, the horizon of this game is much smaller--no more than \(25\). Since no lower bound precludes regret optimality for \(T}\), one might hope to design an algorithm with smaller \(S\) and \(A\) factors in the burn-in cost so that it can achieve optimality even in the sample-starved regime.

### Summary of Contributions

While it is encouraging to see recent works have shown that in the discounted setting, model-free methods can provide nearly optimal guarantees on sample complexity of exploration and that model-based methods can provide nearly optimal finite-sample regret guarantees, there still lacks a _model-free_ approach that can attain _regret optimality_. In the orthogonal direction, there is still a vacancy for algorithms that can attain optimal regret for a broader sample size range, i.e., with fewer samples than \(A^{2}}{(1-)}\).

In fact, we can summarize these two lingering theoretical questions as follows:

_Is there an algorithm that can achieve minimax regret optimality with low space complexity and computational complexity in the infinite-horizon discounted setting, even when sample size is limited?_

We answer this question affirmatively with a new algorithm \(\)-\(\)-\(\), which uses variance reduction and a novel adaptive switching technique. It is the first model-free algorithm that achieves optimal regret in the infinite-horizon discounted setting. This result can be summarized as follows:

**Theorem** (informal).: _For any sample size \(T(1-)}\), \(\)-\(\)-\(\) is guaranteed to achieve near-optimal cumulative regret \((}})\) with space complexity \(O(SA)\) and computational complexity \(O(T)\)._

A formal theorem is presented in Section 4; its proof can be found in the full version . We also provide a complete summary of related prior results in Table 1. A discussion about the additional related work is deferred to Appendix A.

## 2 Problem Formulation

Let us specify the problem we aim to study in this section. Throughout this paper, we let \(()\) denote the probability simplex over any set \(\). We also introduce the notation \([m]:=\{1,2,,m\}\) for a positive integer \(m\).

### Infinite-Horizon Discounted Markov Decision Process

We consider an infinite-horizon discounted Markov decision process (MDP) represented with \((,,,P,r)\). Notably, we consider a tabular one, in which \(:=\{1,2,,S\}\) denotes the state space with size \(\) and \(:=\{1,2,,A\}\) denotes the action space with size \(A\). \(P:()\) denotes the probability transition kernel in that \(P(|s,a)()\) is the transition probability vector from state \(s\) when action \(a\) is taken. \(r:\) denotes the reward function, which is assumed to be deterministic in this work. Specifically, \(r(s,a)\) is the immediate reward for taking action \(a\) at state \(s\). Lastly, \(\) denotes the discount factor for the reward, which makes \(\) the effective horizon.

A (stationary) policy \(:()\) specifies a rule for action selection in that \((|s)()\) is the action selection probability vector at state \(s\). We overload this notation by letting \((s)\) denote the action policy \(\) takes at state \(s\). Given a policy \(\), the Q-function of \(\) is defined as

\[Q^{}(s,a):=[_{t=0}^{}^{t}r(s_{t},a_{t})\ \ s_{0}=s,a_{0}=a],\]in which \(s_{t+1} P(|s_{t},a_{t})\) for \(t 0\) and \(a_{t}(|s_{t})\) for \(t 1\). Moreover, the value function of \(\) is defined as

\[V^{}(s) :=[_{t=0}^{}^{t}r(s_{t},a_{t}) \;\;s_{0}=s],\]

in which \(s_{t+1} P(|s_{t},a_{t})\) and \(a_{t}(|s_{t})\) for \(t 0\). The Q-function and value function satisfy an equation, called the Bellman equation :

\[Q^{}(s,a) =r(s,a)+_{s^{} P(|s,a)}[V^{ }(s^{})].\] (1)

A policy \(^{}\) is called an optimal policy if it maximizes the value function for all states simultaneously. The optimal value function and optimal Q-function can be defined as

\[V^{}(s) :=_{}V^{}(s)=V^{^{}}(s)\] \[Q^{}(s,a) :=_{}Q^{}(s,a)=Q^{^{}}(s,a),\]

which satisfy

\[V^{}(s) =V^{^{}}(s) Q^{}(s,a)=Q^{^{ }}(s,a)\]

for any optimal policy \(^{}\). The optimal policy always exists and satisfies the Bellman optimality equation :

\[Q^{^{}}(s,a) =r(s,a)+_{s^{} P(|s,a)}[_ {a^{}}Q^{^{}}(s^{},a^{})]\] \[=r(s,a)+_{s^{} P(|s,a)}[V^{ }(s^{})].\] (2)

### Online Learning in an Infinite-Horizon MDP

We consider the online (single-trajectory) setting, in which the agent is permitted to execute a total of \(T\) steps sequentially in the MDP. More specifically, the agent starts from an arbitrary (and possibly adversarial) initial state \(s_{1}\). At each step \(t[T]\), the agent at state \(s_{t}\) computes policy \(_{t}\), takes action \(a_{t}\) based on \(_{t}(|s_{t})\), receives reward \(r(s_{t},a_{t})\), and transitions to state \(s_{t+1}\) in the following step. At the end of execution, the agent generates a trajectory \((s_{1},a_{1},r_{1},s_{2},a_{2},r_{2},,s_{T},a_{T},r_{T})\), which amounts to \(T\) samples.

### Problem: Regret Minimization

As a standard metric to evaluate the performance of the aforementioned agent over a finite number of \(T\) steps, the cumulative regret with respect to the sequence of stationary policies \(\{_{t}\}_{t=1}^{T}\) learned by the algorithm is defined as follows:

\[(T):=_{t=1}^{T}V^{}(s_{t})-V^{_{t}}(s_{t}) .\] (3)

Verbally, the regret measures the cumulative suboptimality between the optimal policy and the execution policy \(_{t}\) at each step throughout the \(T\)-step online interaction process. Naturally, one aims to minimize this regret by finding an algorithm whose regret scales optimally in \(T\). This would require a strategic balance between exploration and exploitation, which can be difficult when sample size \(T\) is small.

_Remark 1_.: In the infinite-horizon setting, many prior works [12; 24] consider slightly different regret definitions with respect to non-stationary policies. Specifically, at each \(s_{t}\) along the trajectory, this different regret metric compares the optimal value function \(V^{}(s_{t})\) against the expected cumulative reward of running the non-stationary policy \(\{_{k}\}_{k=t}^{}\) starting from \(s_{t}\). By doing this, it is effectively evaluating the cumulative reward difference between the stationary optimal policy and a non-stationary algorithm. While there exists no formal conversion between the regret defined in this way and the one in (3), it is expected to be smaller and thus more easily controlled than (3), because the execution policy \(_{t}\) improves over time. In addition, we can show our algorithm also achieves the same level of regret under this different definition with an analysis specifically tailored to our algorithm, which is deferred to Appendix B. Furthermore, since the transition kernel in the infinite-horizon setting is invariant over time and the optimal policy itself is also stationary, it is more natural to compare the optimal policy to a stationary policy, e.g., the policy \(_{t}\) deployed by the algorithm at each step, as in (3). Before this work, this has also been recently studied in [49; 18].

Notation.Given any vector \(x^{SA}\) that represents a function \(x:\), we use \(x(s,a)\) to denote the entry corresponding to the state-action pair \((s,a)\). We also denote the probability transition vector at \((s,a)\) with

\[P_{s,a}=P( s,a)^{1 S},\] (4)

that is, given any \(V^{S}\), \(P_{s,a}V=_{s^{} P( s,a)}[V(s^{})]\). For two vectors \(x,y^{SA}\), we override the notation \(x y\) to mean that \(x(s,a) y(s,a)\) in every dimension \((s,a)\).

## 3 Algorithm

In this section, we present our algorithm \(\)-SlowSwitch-Adv and some relevant discussion.

### Review: Q-Learning with UCB and reference advantage

First, we make a brief review of the Q-learning with UCB method proposed in , referred to as UCB-Q hereafter, and its variance-reduced variant UCB-Q-Advantage, later introduced in . The Q-function updates in Q-Slow Switch-Adv are inspired by these two methods.

Q-learning with UCBThe original Q-learning [41; 40] is a fixed-point iteration based on a stochastic approximation of the Bellman optimality equation (2). It uses a greedy policy with respect to its estimate of \(Q^{}\), whose update rule can be summarized as:

\[Q(s,a)(1-)Q(s,a)+(r(s,a)+_{s,a}V).\] (5)

Above, \(Q\) (resp. \(V\)) is the running estimate of \(Q^{}\) (resp. \(V^{}\)); \((0,1]\) is the (possibly varying) learning rate; \(_{s,a}V\) is a stochastic approximation of \(P_{s,a}V\) (cf. (4)). Commonly, \(V(s^{})\) is used for \(_{s,a}V\) in (5) as an unbiased estimate of \(P_{s,a}V\), when a sample of state transition from \((s,a)\), namely \((s,a,s^{})\), is available.

However, as  point out, using (5) naively suffers from great regret suboptimality, for it rules out the state-action pairs with high value but few observations. To promote the exploration of such state-action pairs, UCB-Q appends (5) with an exploration bonus. Its update rule can be written as:

\[Q^{}(s,a)(1-)Q^{}(s,a)+r(s,a )+_{s,a}V+b.\] (6)

To encourage exploration, the bonus \(b 0\) is designed to maintain an upper confidence bound (UCB) on \((_{s,a}-P_{s,a})V\), which in turn keeps \(Q^{}(s,a)\) as an "optimistic" overestimate of \(Q^{}(s,a)\).

Q-learning with UCB and reference advantageThe regret guarantee for UCB-Q is still shy of being optimal. In order to attain optimality, one can turn to the celebrated idea of variance reduction [16; 23; 34; 37], which decomposes the stochastic approximation target into two parts: a low-variance reference estimated with batches of samples and a low-magnitude advantage estimated with every new sample. In this spirit,  introduce UCB-Q-Advantage based on UCB-Q and a reference-advantage decomposition. Specifically, given a reference \(V^{}\) that is maintained as an approximation for \(V^{}\), the update rule of UCB-Q-Advantage reads:

\[Q^{}(s,a)(1-)Q^{}(s,a)+r(s,a)+ _{s,a}(V-V^{})+}}(s,a)+b^{}.\] (7)

This idea of (7) is used in subroutine **update-q-reference**() of our algorithm. Let us discuss it in more details:

* Given a transition sample \((s,a,s^{})\), we can take \(V(s^{})-V^{}(s^{})\) as an unbiased estimate of the advantage \(P_{s,a}(V-V^{})\). The magnitude of \(V-V^{}\) is small when \(V\) and \(V^{}\) are close. This engenders smaller stochastic volatility, compared to \(_{s,a}V\) in (6) from UCB-Q.
* The reference estimate \(}}\) is a stochastic approximation of \(PV^{}\). In our algorithm, the auxiliary estimate \(^{}\) (cf. Line 13 of Algorithm 2) is used as the estimate for \(PV^{}\). Specifically, \(^{}(s,a)\) is a running mean of \(P_{s,a}V^{}\) based on the samples from all past visitations of \((s,a)\). In contrast to the advantage, which is computed every time a new sample arrives, the reference is computed with a batch of samples and thus more stable. In sacrifice, the reference is only updated intermittently and not as up-to-date as the advantage.

The exploration bonus \(b^{}\) is computed from the auxiliary estimates in Line 8 and 9 to serve as an upper confidence bound on the aggregation of the aforementioned reference and advantage. Specifically, for each \((s,a)\), \(^{}(s,a)\) and \(^{}(s,a)\) are the running mean and 2nd moment of the reference \([PV^{}](s,a)\) respectively; \(^{}(s,a)\) and \(^{}(s,a)\) are the running mean and 2nd moment of the advantage \([P(V-V^{})](s,a)\) respectively; \(B^{}(s,a)\) combines the empirical standard deviations of the reference and the advantage; \(^{}(s,a)\) is the temporal difference between \(B^{}(s,a)\) and its previous value. \(b^{}(s,a)\) can be computed from these estimates as a temporally-weighted average of \(B^{}(s,a)\). Thanks to the low variability of the reference \(PV^{}\), we can obtain a more accurate, milder overestimation in the upper confidence bound for faster overall convergence.

### Review: Early settlement of reference value

Given the optimistic overestimates \(Q^{}\) and \(Q^{}\), it is natural to design an update rule of our Q-function estimate as the minimum of the estimate itself and these two overestimates (Line 9 of Algorithm 1). This makes our Q-function estimate monotonically decrease without violating the optimistic principle \(Q Q^{}\), which effectively enables us to lessen the overestimation in \(Q\) over time until it converges to \(Q^{}\). In fact, this is precisely the update rule in UCB-Q-Advantage . Nevertheless, we need to equip our algorithm with additional features to strive for regret optimality.

 introduced a way to update the reference with higher sample efficiency in the finite-horizon setting. As they noted, it is critical to update the reference \(V^{}\) in a smart fashion so as to balance the tradeoff between its synchronization with \(V\) and the volatility that results from too many stochastic updates. Concretely, the reference \(V^{}\) needs to be updated in a timely manner from \(V\) so that the magnitude of \(_{s,a}(V-V^{})\) can be kept low as desired, but the updates cannot be too frequent either, because the stochasticity or variance in \(}}(s,a)\) could be as high as that in \(_{s,a}V\) of (6) and thus lead to suboptimality if it is not carefully controlled. To resolve this dilemma, we can update \(V^{}\) until it becomes sufficiently close to \(V^{}\) and fix its value thereafter.

To this end, we maintain a "pessimistic" underestimate \(Q^{}\) (resp. \(V^{}\)) of \(Q^{}\) (resp. \(V^{}\)) in the algorithm, which are computed from the lower confidence bound for \(Q^{}\) (resp. \(V^{}\)). This can provide us with an upper bound on \(V^{}-V^{}\), which will be used to determine when the update of the reference \(V^{}\) should be stopped.

In particular, the if-else block in Line 23 to 26 is designed to keep the reference \(V^{}\) synchronized with \(V\) for each state \(s\) respectively and terminate the update once

\[V(s) V^{}+3 V^{}+3.\] (8)This can guarantee \(|V-V^{}| 6\) throughout the execution of the algorithm. As a result, the standard deviation of \(_{s,a}(V-V^{})\) is guaranteed to be \(O(1)\), which can be \(O()\) times smaller than the standard deviation of \(_{s,a}V\) in (2). This can lead to smaller \(\) factor in the final regret guarantee.

### Adaptive low-switching greedy policy

Although these aforementioned designs from the finite-horizon literature help increase the accuracy of our estimate \(Q\), they are still insufficient to attain regret optimality in the infinite-horizon setting. Since data collection takes place over a single trajectory with no reset, drastic changes in the execution policy can inflict a long-lasting volatility on the future trajectory and slow down the convergence. This is precisely the difficulty of the infinite-horizon setting over the finite-horizon one. The need to control the trajectory variability motivates us to design a novel adaptive switching technique.

Recall in UCB-Q and UCB-Q-Advantage, the execution policy is greedy with respect to the estimate \(Q\), i.e., \(_{t}(s_{t})=_{a}Q(s_{t},a)\). Every time \(Q\) gets updated, what the algorithm effectively does is to make an estimate of \(Q^{_{t}}\) with the samples generated by \(_{t}\). Such \(Q^{_{t}}\) is only estimated and updated once before the execution policy is switched to \(_{t+1}\). This seems insufficient from a stochastic fixed-point iteration perspective, so we seek to update it more and learn each \(Q^{_{t}}\) better before switching to a new policy.

To tackle this issue, we make the execution policy \(_{t}\) greedy to \(Q^{}\), which is updated lazily get adaptively in Q-SlowSwitch-Adv. Specifically, for every \((s,a)\), we use \((s,a)\) (cf. Line 12 in Algorithm 2) to keep track of the cumulative difference between the current \(Q(s,a)\) and \(Q_{M}(s,a)\), the latter of which is defined to be the value of \(Q(s,a)\) last time \(Q^{}\) is updated immediately after visiting \((s,a)\). Whenever \((s,a)\) exceeds \(\), indicating \(Q^{}(s,a)\) and the execution policy has become outdated with respect to the current \(Q(s,a)\), we reset \((s,a)\) and set \(u^{}\) to True, which will direct the algorithm to update the entire function \(Q^{}\) in the following step. **update-q-lazy**() updates \(Q^{}\) with the samples from \(\), which is a dictionary that serves as a buffer to store all the new sample transitions and their latest estimates since the last update of \(Q^{}\).

In contrast, conventional low-switching algorithms update the execution policy on a predetermined, exponentially phased schedule [5; 48]. While trajectory stability is attained with these algorithms, as time goes on, it takes them exponentially longer to update policy, making them oblivious to recent large updates in the estimated Q-function. This would lead to suboptimal regret in the infinite-horizon setting, as continual choices of suboptimal actions will keep a lasting effect on future trajectory in the absence of trajectory reset. This issue is overcome in our algorithm by ignoring minor changes in function \(Q\) yet still being adaptive to substantial changes in any state-action pair.

## 4 Main Results

Our model-free algorithm Q-SlowSwitch-Adv can achieve optimal regret with short burn-in time. Its theoretical guarantee can be summarized in the following theorem.

**Theorem 1**.: _Choose any \((0,1)\). Suppose \(=\) and \(c_{b}\) is chosen to be a sufficiently large universal constant in Algorithm 1. Then with probability at least \(1-\), Algorithm 1 achieves_

\[(T) C_{0}(}{(1-)^{3}}}+ }}{(1-)^{8}})\] (9)

_for an absolute constant \(C_{0}>0\)._

To prove this theorem, we need to use a recursive error decomposition scheme different from the existing work. The stationary nature of the infinite-horizon setting gives rise to several error terms unique to the infinite-horizon setting, and our novel switching technique is crucial at controlling them optimally. The proof is provided in the full version . Now let us highlight a few key properties of our algorithm.

Optimal regret with low burn-in.Q-SlowSwitch-Adv achieves optimal regret modulo some logarithmic factor as soon as the sample size \(T\) exceeds

\[T(1-)}.\] (10)

This burn-in threshold is significantly lower than the \(A^{2}}{(1-)}\) threshold in  when \(SA\). In other words, in the regime of (10), the regret of Q-SlowSwitch-Adv is guaranteed to satisfy

\[(T)(}} ),\] (11)

which matches the lower bound in Table 1.

Sample complexity.As a corollary of Theorem 1, it can be seen that Q-SlowSwitch-Adv attains \(\)-average regret (i.e. \((T)\) for any fixed \(T\)) with sample complexity

\[(^{2}}).\] (12)

This is lower than the sample complexity of the model-free algorithm in , which is \(^{2}}\).

Moreover, (12) holds true for any desired accuracy \(0,}{SA}\). This is a broader range than the ones in [12; 49], which involve higher order of \(S\) and \(A\) and only allow their algorithms to attain their respective optimal sample complexity in the high-precision regime.

Space complexity.Q-SlowSwitch-Adv is a model-free algorithm that keeps a few estimates of the Q-function during execution, so its memory cost is as low as \(O(SA)\). This is not improvable in the tabular setting, since it requires \(O(SA)\) units of space to store the optimal policy. In contrast, the model-based UCBVI-\(\) in  stores an estimate of the probability transition kernel and thus incurs a higher memory cost of \(O(S^{2}A)\).

Computational complexity.The computational cost of Q-SlowSwitch-Adv is only \(O(T)\). This is on the same order as reading samples along the \(T\)-length executed trajectory and is thus unimprovable. In comparison, our algorithm has a considerably lower computational cost than the one in , which requires \(O(S^{2}AT)\) operations overall.

## 5 Discussion

This work has introduced a model-free algorithm that achieves optimal regret in infinite-horizon discounted MDPs, which reduces the space and computational complexity requirement for regret optimality in the existing work. It also achieves optimal sample efficiency with a short burn-in time compared to other algorithms, including [12; 49]. Moreover, our algorithm has demonstrated the importance of switching policies slowly in infinite-horizon MDPs and introduced a novel technique might be of additional interest to future work. While our burn-in threshold is considerably reduced with respect to the order of \(S\) and \(A\), it still has nontrivial suboptimality in the effective horizon \(\), which is a price we pay for using the reference-advantage technique. It is open for future work to investigate how to further improve the effective horizon factors in the burn-in cost.