# Debiasing Scores and Prompts of 2D Diffusion for View-consistent Text-to-3D Generation

Susung Hong

Equal contribution.

**Donghoon Ahn1**

**Seungryong Kim2**

Korea University, Seoul, Korea

Korea University, Seoul, Korea

###### Abstract

Existing score-distilling text-to-3D generation techniques, despite their considerable promise, often encounter the view inconsistency problem. One of the most notable issues is the Janus problem, where the most canonical view of an object (_e.g._, face or head) appears in other views. In this work, we explore existing frameworks for score-distilling text-to-3D generation and identify the main causes of the view inconsistency problem--the embedded bias of 2D diffusion models. Based on these findings, we propose two approaches to debias the score-distillation frameworks for view-consistent text-to-3D generation. Our first approach, called score debiasing, involves cutting off the score estimated by 2D diffusion models and gradually increasing the truncation value throughout the optimization process. Our second approach, called prompt debiasing, identifies conflicting words between user prompts and view prompts using a language model, and adjusts the discrepancy between view prompts and the viewing direction of an object. Our experimental results show that our methods improve the realism of the generated 3D objects by significantly reducing artifacts and achieve a good trade-off between faithfulness to the 2D diffusion models and 3D consistency with little overhead. Our project page is available at https://susunghong.github.io/Debiased-Score-Distillation-Sampling/.

Figure 1: **Comparison between the baseline (SJC ) and ours (Debiased Score Distillation Sampling; D-SDS). Our debiasing methods qualitatively reduce view inconsistencies in zero-shot text-to-3D generation and the so-called _Janus problem_.**Introduction

Recently, significant advancements have been made in the field of zero-shot text-to-3D generation , particularly with the integration of score-distillation techniques [10; 27; 18; 13] and diffusion models [9; 25; 24; 5; 21; 20; 23; 2] to optimize neural radiance fields . These methods provide a solution for generating a wide range of 3D objects from a textual input, without requiring 3D supervision. Despite their considerable promise, these approaches often encounter the view inconsistency problem. One of the most notable problems is the multi-face issue, also referred to as _the Janus problem_, which is illustrated in the "Baseline" of Fig. 1. This problem constrains the applicability of the methods [10; 27; 18; 13], but the Janus problem is rarely formulated or carefully analyzed in previous literature.

To address the problem of view inconsistency, we delve into the formulation of score-distilling text-to-3D generation presented in [18; 27; 13; 10]. We generalize and expand upon the assumptions about the gradients concerning the parameters of a 3D scene in previous works such as DreamFusion  and Score Jacobian Chaining (SJC) , and identify the main causes of the problem within the estimated score. The score can be further divided into the unconditional score and pose-prompt gradient, both of which interrupt the estimation of unbiased gradients concerning the 3D scene. Additionally, since a naive text prompt describes a canonical view of an image such as front view, prior text-to-3D generation works [22; 18; 27; 13; 10] append a view prompt (_e.g._, "front view", "side view", "back view", "overhead view", _etc._) to the user's input, depending on the sampled camera angle, to better reflect its appearance from a different view. We present an analysis of the score with user prompts and view prompts and their effect on 3D, arguing that refining them is necessary for generating more realistic and view-consistent 3D objects.

Building on this concept and drawing inspiration from gradient clipping  and dynamic thresholding , we propose a score debiasing method that performs dynamic score clipping. Specifically, our method cuts off the score estimated by 2D diffusion models to mitigate the impact of erroneous bias (Fig. 2 and Fig. 3). With this debiasing approach, we reduce artifacts in the generated 3D objects and alleviate the view inconsistency problem by striking a balance between faithfulness to 2D models and 3D consistency. Furthermore, by gradually increasing the truncation value, which aligns with the coarse-to-fine nature of generating 3D objects [3; 15], we achieve a better trade-off for 3D consistency without significantly compromising faithfulness.

While the first attempt to address the bias issue in the scores, we further present a prompt debiasing method. In contrast to prior works [18; 27; 10] that simply concatenate a view prompt and user prompt, our method reduces inherent contradiction between them by leveraging a language model trained with a masked language modeling (MLM) objective , computing the point-wise mutual information. Additionally, we decrease the discrepancy between the assignment of the view prompt and camera pose by adjusting the range of view prompts. These enable text-to-image models [21; 20; 16] to predict accurate 2D scores, resulting in 3D objects that possess more realistic and consistent structures.

## 2 Background

Diffusion models.Denoising diffusion models [5; 23] generate images through progressive denoising process. During training, denoising diffusion probabilistic models (DDPM)  optimizes the following simplified objective:

\[L_{}:=_{(0,), _{0},t}[\|-_{}(_{ t},t)\|^{2}],\] (1)

where \(_{}\) is a network of the diffusion model parameterized by \(\), \(t\{T,T-1,...,1\}\) is a timestep, \(_{0}\) is an original image, and \(_{t}\) denotes a perturbed image according to the timestep \(t\). During inference, starting from \(_{t}\), DDPM samples a previous sample \(_{t-1}\) from a normal distribution with probability density of \(p_{}(_{t-1}|_{t})\).

Some works fit DDPM into the generalized frameworks, _e.g._, non-Markovian , score-based [25; 9], _etc._ Notably, denoising diffusion models have a tight relationship with score-based models [25; 9] in the continuous form. Furthermore, it has been shown that denoising diffusion models can be refactored into the canonical form of denoising score matching using the same network parameterization .

This formulation further facilitates the direct computation of 2D scores [24; 9] with the following equation:

\[_{} p(;)=(;)- }{^{2}},\] (2)

where \(D_{}\) is an optimal denoiser network trained for every \(\). With some preconditioning, a diffusion model \(_{}\)[5; 23; 17; 20] turns into a denoiser \(D_{}\).

Recent advancements in diffusion models have sparked increased interest in text-to-image generation [19; 20; 16; 16]. Diffusion guidance techniques [2; 6; 16; 7] have been developed to enable the control of the generation process based on various conditions such as class labels [6; 2], text captions , or internal information . In particular, our work conditions text prompts with classifier-free guidance , which is formulated as follows given a conditional diffusion model \(_{}(_{t},t,)\):

\[}=_{}(_{t},t,)+s( _{}(_{t},t,)-_{}(_{t},t)),\] (3)

where \(}\) is the guided output, \(\) is the user-given text prompt (_user prompt_ for brevity), and \(s\) is the guidance scale .

Score distillation for text-to-3D generation.Diffusion models have shown remarkable performance in text-to-image modeling [16; 19; 21; 7; 20]. On top of this, DreamFusion  proposes the score-distillation sampling (SDS) method that uses text-to-image diffusion models to optimize neural fields , achieving encouraging results. The score-distillation sampling utilizes the gradient computed by the following equation:

\[_{}L_{}_{(0,),t}[w(t)(_{}(_{t},t,)- )_{}}{}],\] (4)

where \(_{t}\) denotes the \(t\)-step noised version of \(_{}\) which is a rendered image from a NeRF network with parameters \(\), and \(w(t)\) is a scaling function only dependent on \(t\). This gradient omits the Jacobian of the diffusion backbone, leading to tractable optimization in differentiable parameterizations .

On the other hand, in light of the interpretation of diffusion models as denoisers, SJC  presents a new approach directly using the score estimation, called perturb-and-average scoring (PAAS). The work shows that the U-Net Jacobian emerging in DreamFusion is not even necessary, as well as forming a strong baseline using publicly open Stable Diffusion . The perturb-and-average score approximates to a score with an inflated noise level:

\[_{_{}} p_{}(_{}) _{n(0,)}[( _{}+ n;)-_{}}{^{2}}],\] (5)

where the expectation is practically estimated by Monte Carlo sampling. This score estimate is then directly plugged into the 2D-to-3D chain rule and produces:

\[_{}L_{}_{_{}} [_{_{}} p_{}(_{} )_{}}{}].\] (6)

Figure 2: **Illustration of our framework.** We propose prompt and score debiasing techniques to estimate robust and unbiased gradients of the 3D parameters w.r.t. the viewpoints.

Although the derivation is different from SDS in DreamFusion , it is straightforward to show that the estimation \(_{}L_{}\) is the same as \(_{}L_{}\) with a different weighting rule and sampler .

In general, frameworks distilling the score of text-to-image diffusion models [18; 10; 27; 13; 11; 22] achieve a certain level of view consistency by concatenating view prompts (_e.g._, "back view of") with user prompts [18; 10; 27; 13; 11; 22]. Although this is an important part in score distillation, it is rarely discussed. In the following section, we elucidate this altogether, uncovering the underlying causes of the Janus problem.

## 3 Score Distillation and the Janus Problem

SJC  defines the probability density function of parameters \(\) of 3D volume (_e.g._, NeRF ) as an expectation of the likelihood of 2D rendered images \(_{}\) from uniformly sampled object-space viewpoints (Eq. 6 in ). Unlike this definition, our approach defines the density function of the parameters \(\) as a product of conditional likelihoods given a set of uniformly sampled viewpoints \(\) and user prompt \(\). This can be expressed as:

\[_{}()=_{}p_{}(_{}|,),\] (7)

where \(p_{}\) and \(_{}\) denote the probability density of 2D image distribution and unnormalized density of 3D parametrizations, respectively. By using this formulation, we avoid using Jensen's inequality, in contrast to . Applying the logarithm to each side of the equation yields:

\[_{}()=_{} p_{}( _{}|,).\] (8)

By taking the gradient of \(_{}()\), we can directly obtain \(_{} p_{}()\), since the normalizing constant of \(_{}\) is irrelevant to \(\). Using the chain rule, we obtain:

\[_{} p_{}()=_{ }_{} p_{}(_{}|,) =Z_{}[_{} p_{ }(_{}|,)]\] (9) \[=Z_{}[_{_{ }} p_{}(_{}|,)_{}}{}],\]

where \(Z=||\) is a constant, and \(_{_{}} p_{}(_{}|,)\) is practically estimated by diffusion models . Note that this definition generalizes SJC  and even \(_{}_{}\) in DreamFusion, which can be easily seen as the estimation of Eq. 9 with a different weighting rule and sampler. This is further expanded by applying Bayes' rule as follows:

\[_{} p_{}()=Z_{ }(_{}} p_{}(_{})}_{}+_{}} p_{}(, |_{})}_{})_{}}{}.\] (10)

The first gradient term, reflecting the unconditional score modeled by 2D diffusion models [5; 25], contains a bias that affects images viewed from typical viewpoints during early optimization of 3D volume when \(_{}\) is noisy. This contributes to the Janus problem, as facial views are more prevalent in the 2D data distribution for some objects.

On the other hand, the pose-prompt gradient in Eq. 10 is guidance [25; 6; 2; 7] that drives the rendered image to better represent a specific camera pose and user prompt. The term is further expanded:

\[_{_{}} p_{}(,|_{ })=_{_{}} p_{}(|_{ })+_{_{}} p_{}(|_{ })+_{_{}} C,\] (11)

where \(C\) is defined as \(}(,|_{})}{p_{}( |_{})p_{}(|_{})}=}(|,_{})Methodology

### Score debiasing

Motivation and overview.If the unconditional score, \(_{_{}} p_{}(_{})\), is biased towards some viewing directions, which is likely in 2D data as mentioned in Sec. 3, it can negatively affect the 3D consistency and realism of generated objects through the chain rule (Eq. 9). Moreover, large magnitudes in the user prompt gradient, \(_{_{}} p_{}(|_{})\), can also cause issues by introducing text-related artifacts that are not present in the image rendered from a 3D field (see Fig. 1 and Fig. 3). Such artifacts include extra faces, beaks, and horns, which are unrealistic or inconsistent with the 3D object's structure.

High magnitude in those two terms is typically observed when the perturbed-and-denoised image by diffusion models significantly deviates from the rendered image in the corresponding pixels (Fig. 3). Hence, adjusting this gradient is necessary to reduce the artifacts and improve the realism of the generated 3D objects. However, the 2D bias that flows into the 3D field has hardly been formulated or adjusted for better optimization and 3D consistency.

The intuition behind the scale of the distilled score \(_{_{}} p_{}}(_{ })\) can be mathematically elucidated by examining its relationship with the expectation term. Concretely, the distilled score serves as an approximation of the expected value of the difference between the distorted image \(D(_{}+ n;)\) and the original rendered image \(_{}\), normalized by the square of the noise scale. This expectation is evaluated with respect to the normal distribution \((0,)\) from which the noise term \(n\) is sampled. Note that the noise term not only facilitates the use of diffusion models but can also be interpreted as a random perturbation applied to the rendered image \(_{}\).

In this context, the expectation term provides a measure of the sensitivity of the denoising process to variations in the noise. In other words, the magnitude of the estimated score can be interpreted as the (scaled) deviation of the original rendered image \(_{}\) from the 3D field. Notably, NeRF-W  also provides a mechanism for handling uncertainty by explicitly rendering the variance. On the contrary, we propose a novel and efficient method to directly clamp the estimated score, effectively suppressing significant deviations that ignore either geometry or appearance, thereby addressing the intrinsic bias inherent in score-based models.

Dynamic clipping of 2D-to-3D scores.In light of the need to control the flow of 2D scores to 3D volume (Sec. 4.1) and inspired by the clipping methods [14; 21], we propose an effective method that truncates the scores to mitigate the effects of bias and artifacts in the predicted 2D scores:

\[_{_{}} p_{}^{}= (_{_{}} p_{}(_{}| ,),_{}),\] (12)

where \((x,c)=((x,c),-c)\). This score clipping prevents artifacts such as extra faces, horns, eyes, and ears from appearing on the 3D objects.

However, the application of naive score clipping creates a large threshold-dependent tradeoff between 3D consistency and 2D fidelity: the lower the threshold, the more artifacts are removed, but at the expense of 2D fidelity. To circumvent this, we introduce an effective coarse-to-fine strategy [15; 3]:

\[_{}&:=(1-)_{ }+_{},\\ _{_{}} p_{}^{} &=(_{_{}} p_{}( _{}|,),_{}),\] (13)

w

Figure 3: **Visualization of the magnitude of the estimated \(_{_{}} p_{}(_{}|,)\) during the optimization. This visualization demonstrates that erroneous 2D scores result in critical artifacts, _e.g._, additional legs, beaks, and horns in this figure.**

where \(=)}{()}\). In the early stages of optimization, we focus on the overall structure and shape, which do not require the large magnitudes of the 2D scores, while in later stages, we focus more on the details that require higher magnitudes, so we increase the threshold as the optimization progresses. We provide an illustration in Appendix A.5 to show what the rendered image at each step looks like as the scene undergoes optimization.

### Prompt debiasing

Motivation and overview.Text-to-3D generation methods that distill diffusion models [18; 27] achieve a certain level of view consistency by concatenating view prompts (_e.g._, "back view of") with user prompts [18; 10; 27; 13; 11; 22]. This simple and effective method leverages the knowledge of large-scale text-to-image models.

However, we argue that the current strategy of creating a view-dependent prompt by simply concatenating a view prompt with a user prompt is intrinsically problematic, as it can result in a contradiction between them. This contradiction is one of the causes that make diffusion models not follow the view prompt.

Therefore, in the following subsection, we propose identifying the contradiction between the view prompt and user prompt using off-the-shelf language models trained with masked language modeling (MLM) .

Additionally, instead of naively assigning regular regions for view prompt augmentations, in the next subsection, we reduce the discrepancy between the view prompt and object-space pose by adjusting the regions.

Identifying contradiction.The prompt gradient term \(_{_{}} p_{}(|_{})\) may cancel out the pose gradient term \(_{_{}} p_{}(|_{})\) needed for the view consistency of generated 3D objects, as we can derive from Eq. 11. For example, if the view prompt is "back view of" and the user prompt is "a smiling dog", it results in a contradiction since an observer cannot see the dog's smile viewing from the back. This causes diffusion models not to follow a view prompt, but instead to follow a word like "smiling" in a user prompt, as shown in Fig. 4.

In this regard, we propose a method for identifying contradictions using language models trained with masked language modeling (MLM). Specifically, let \(V\) represent a set of possible view prompts, and let \(U\) be a set of size 2, which contains the presence and absence of a word in the user prompt for brevity. We then compute the following:

\[= U}P(v|u^{})P(u^{ })},\] (14)

where we technically model \(P(v|u)\) with masked language modeling by alternating the view prompts and normalizing them, and \(P(u)\) is a user-defined faithfulness. Note that Eq. 14 corresponds to the pointwise mutual information (PMI), as \((v,u)=\), and removing a contradiction involves eliminating a word with a low PMI value concerning the view prompts. In practice, a word from a user prompt is omitted if the value falls below a certain threshold.

Reducing discrepancy between view prompts and camera poses.Existing methods [18; 27; 10; 13] utilize view prompt augmentations by dividing the camera space into some regular sections (_e.g.,_ front, back, side, and overhead in DreamFusion ). However, this approach does not match the real distribution of object-centric poses in image-text pairs; _e.g.,_ the front view may cover a narrower

Figure 4: **Samples from Stable Diffusion  given a text prompt with contradiction. Despite “Back view of” is given in the prompts, the word “smiling” in the prompt makes diffusion models biased towards the front view of objects.**

region. Therefore, we make practical adjustments to the range of view prompts, such as reducing the azimuth range of the "front view" by half, and also search for precise view prompts [18; 27] that yield improved results.

## 5 Experiments

### Implementation details

We build our debiasing methods on the high-performing public repository of SJC . For all the results, including SJC and ours, we run 10,000 steps to optimize the 3D fields, which takes about 20 minutes using a single NVIDIA 3090 RTX GPU and adds almost no overhead compared to the baseline. We set the hyperparameters of SJC to specific constants  and do not change them throughout the experiments.

### Evaluation Metrics

Quantitatively evaluating a zero-shot text-to-3D framework is challenging due to the absence of ground truth 3D scenes that correspond to the text prompts. Existing works employ CLIP R-Precision [8; 18]. However, it measures retrieval accuracy through projected 2D images and text input, making it unsuitable for quantifying the view consistency of a scene.

Therefore, to measure the view consistency of generated 3D objects quantitatively, we compute the average LPIPS  between adjacent images, which we refer to as A-LPIPS. We sample 100 uniformly spaced camera poses from an upper hemisphere of a fixed radius, all directed towards the sphere's center at an identical elevation, and render 100 images from a 3D scene. Then, we average the LPIPS values evaluated for all adjacent pairs of images in the 3D scene, finally aggregating those averages across the scenes. The intuition behind this is that if there exist artifacts or view inconsistencies in a generated 3D scene, the perceptual loss will be large near those points.

In addition, to assess the faithfulness to the view-augmented prompt, we present a graph that illustrates the average CLIP similarities of rendered images for each azimuth, as determined by view-augmented prompts. This metric is designed to be high when the score-distillation pipeline effectively generates an accurate view of an object.

   Method & A-LPIPS\({}_{}\) & A-LPIPS\({}_{}\) \\  Baseline  & 0.2054 & 0.1526 \\ Debiased (Preserved) & 0.1963 & 0.1450 \\ Debiased (Ours) & **0.1940** & **0.1445** \\   

Table 1: **Quantitative evaluation. The best values are in bold, and the second best are underlined. _Preserved_ means user prompts are preserved, i.e., \(P(u)=1\) for all \(u\).

Figure 5: **Average CLIP similarities of rendered images for each azimuth, calculated using view-augmented prompts. The shaded areas, starting from the left, represent the \(90^{}\) regions for the front view and back view, respectively.**

### Comparison with the baseline

Quantitative results.We present quantitative results from 70 user prompts for the baseline , our combined method, and the method without the removal of contradicting words from a user prompt. Our method produces more consistent 3D objects than the baseline, as demonstrated in Table 1. Note that removing contradictions in prompts indeed leads to better results with respect to A-LPIPS, meaning that the generated objects overall in each azimuth are consistent with our debiasing methods.

We also present adherence to the view-augmented prompts in Fig. 5. The diagram illustrates shaded sections depicting the 90-degree front and back view zones, beginning from the left side. When comparing our unbiased outcomes to the baseline, we observe a clear and preferable pattern in CLIP

   Method & View consistency & Faithfulness & Overall quality \\  Baseline  & 9.58\% & 16.07\% & 9.67\% \\ Debiased (Ours) & **90.42\%** & **83.93\%** & **90.33\%** \\   

Table 2: **User study.**

Figure 6: **Comparison between Stable-DreamFusion , SJC , and ours. The baseline is original SJC . Our debiasing methods qualitatively reduce view inconsistencies in zero-shot text-to-3D and the so-called Janus problem.**similarities associated with the view-augmented prompts. In this pattern, the similarity with the view-augmented prompts reaches its highest point in the desired region. In contrast, the standard method exhibits minor fluctuations in CLIP similarities as we examine different angles in relation to the view prompts, implying less faithfulness to the viewing direction.

In addition to the user study outlined in Table 2, which evaluates view consistency, faithfulness to user prompts, and overall quality, we also report the success rate of the generation in Table 3. The success rate applies to 41 out of 70 prompts featuring countable faces. We marked as successful only those objects that do not exhibit the Janus problem, _i.e._, those with an accurate number of faces. Our method significantly outperforms the baseline in terms of success rate.

Overall, the experiments corroborate that our debiasing methods improve the realism and alleviate the Janus problem of generated 3D objects, without requiring any 3D guide  or introducing significant overhead or additional optimization steps to the zero-shot text-to-3D setting.

Qualitative results.We present qualitative results in Fig. 6. In addition to the results of SJC , which serves as the baseline for our experiments, we include those of Stable-DreamFusion , an unofficial re-implementation of DreamFusion  that utilizes Stable Diffusion . The results demonstrate that our methods significantly reduce the Janus, or view inconsistency problem. For example, given a user prompt "a majestic giraffe with a long neck," the whole body is consistently generated using our debiasing method, compared to the baseline with the Janus problem. Additionally, as a notable example, when considering "a mug with a big handle," our method successfully generates a mug with a single handle, while the counterparts generate multiple handles.

Additionally, to show that our method is not only applicable to SJC  with Stable Diffusion , but also to any text-to-3D frameworks that leverage score distillation, we present results on DreamFusion  with DeepFloyd-IF, and on concurrent frameworks such as Magic3D  and Prolific-Dreamer  in Appendix A.4, showcasing various outcomes.

### Ablation study

Ablation on debiasing methods.We present ablation results in Fig. 7, where we sequentially added prompt debiasing and score debiasing on top of the baseline. This demonstrates that they gradually improve the view consistency and reduce artifacts as intended.

Using prompt debiasing alone can resolve the multi-face problem to some extent. In the case of the prompt "a smiling cat", prompt debiasing eliminates the word "smiling" from the prompt. As can be seen in column 1 and column 3, the cat has a more realistic appearance compared with the baseline. However, the cat retains an additional ear. Sometimes, such as in the instance with the panda, it can even generate a new ear. Therefore, using prompt debiasing alone does not solve the problem of creating additional artifacts like ears. Applying score debiasing removes these extra ears in both cases, leading to more view-consistent text-to-3D generation in combination with prompt debiasing.

Ablation on dynamic clipping.To show some examples of the effect of dynamic clipping, we compare the results with those of static clipping and no clipping in Fig. 8. It demonstrates that naive static clipping can struggle to find a good compromise between 3D consistency and 2D

Figure 7: **Improvement of view consistency through prompt and score debiasing. We start from the baseline (SJC ) and apply score debiasing and prompt debiasing sequentially for each prompt, ”a smiling cat” and ”a cute and chubby panda munching on bamboo”, respectively.**

faithfulness, which means lowering the threshold can eliminate more artifacts like extra ears or eyes to achieve better realism, but it also returns fairly pixelated and collapsed appearance, as can be seen in (c). Conversely, employing dynamic clipping produces visually appealing outcomes with artifacts eliminated, closely resembling the consistency of static clipping at a low threshold. Moreover, it preserves intricate shapes and details without any pixelation or degradation of the object's visual presentation.

## 6 Conclusion

In conclusion, we have addressed the critical issue of view inconsistency in zero-shot text-to-3D generation, particularly focusing on the Janus problem. By dissecting the formulation of score-distilling text-to-3D generation and pinpointing the primary causes of the problem, we have proposed a dynamic score debiasing method that mitigates the impact of erroneous bias in the estimated score. This method significantly reduces artifacts and improves the 3D consistency of generated objects. Additionally, our prompt debiasing approach refines the use of user and view prompts to create more realistic and view-consistent 3D objects. Our work, D-SDS, presents a major step forward in the development of more robust and reliable zero-shot text-to-3D generation techniques, paving the way for further advancements in the field.