# The Prevalence of Neural Collapse

in Neural Multivariate Regression

 George Andriopoulos\({}^{1}\) Zixuan Dong\({}^{2,4}\) Li Guo\({}^{3}\) Zifan Zhao\({}^{3}\) Keith Ross\({}^{1}\)

\({}^{1}\) New York University Abu Dhabi \({}^{2}\) SFSC of AI and DL, NYU Shanghai

\({}^{3}\) New York University Shanghai \({}^{4}\) New York University

Equal contribution.Corresponding author: keithwross@nyu.edu

###### Abstract

Recently it has been observed that neural networks exhibit Neural Collapse (NC) during the final stage of training for the classification problem. We empirically show that multivariate regression, as employed in imitation learning and other applications, exhibits Neural Regression Collapse (NRC), a new form of neural collapse: (NRC1) The last-layer feature vectors collapse to the subspace spanned by the \(n\) principal components of the feature vectors, where \(n\) is the dimension of the targets (for univariate regression, \(n=1\)); (NRC2) The last-layer feature vectors also collapse to the subspace spanned by the last-layer weight vectors; (NRC3) The Gram matrix for the weight vectors converges to a specific functional form that depends on the covariance matrix of the targets. After empirically establishing the prevalence of (NRC1)-(NRC3) for a variety of datasets and network architectures, we provide an explanation of these phenomena by modeling the regression task in the context of the Unconstrained Feature Model (UFM), in which the last layer feature vectors are treated as free variables when minimizing the loss function. We show that when the regularization parameters in the UFM model are strictly positive, then (NRC1)-(NRC3) also emerge as solutions in the UFM optimization problem. We also show that if the regularization parameters are equal to zero, then there is no collapse. To our knowledge, this is the first empirical and theoretical study of neural collapse in the context of regression. This extension is significant not only because it broadens the applicability of neural collapse to a new category of problems but also because it suggests that the phenomena of neural collapse could be a universal behavior in deep learning.

## 1 Introduction

Recently, an insightful phenomenon known as neural collapse (NC)  has been empirically observed during the terminal phases of training in classification tasks with balanced data. NC has three principal components: (NC1) The features of samples within each class converge closely around their class mean. (NC2) The averages of the features within each class converge to form the vertices of a simplex equiangular tight frame. This geometric arrangement implies that class means are equidistant and symmetrically distributed. (NC3) The weight vectors of the classifiers in the final layer align with the class means of their respective features. These phenomena not only enhance our understanding of neural network behaviors but also suggest potential simplifications in the architecture and the training of neural networks.

The initial empirical observations of NC have led to the development of theoretical frameworks such as the layered-peeled model (Fang et al., 2021) and the unconstrained feature model (UFM) (Mixon et al., 2020). These models help explain why NC occurs in classification tasks theoretically. By allowing the optimization to freely adjust last-layer features along with classifier weights, these models provide important insights into the prevalence of neural collapse, showing that maximal class separability is a natural outcome for a variety of loss functions when the data is balanced (Han et al., 2021; Poggio and Liao, 2020; Zhou et al., 2022a,b).

Regression in deep learning is arguably equally important as classification, as it serves for numerous applications across diverse domains. In imitation learning for autonomous driving, regression is employed to predict continuous control actions (such as speed and steering angles) based on observed human driver behavior. Similarly, regression is used in robotics, where the regression model is trained to imitate expert demonstrations. In the financial sector, regression models are extensively used for predictive analytics, such as forecasting stock prices, estimating risk, and predicting market trends. Meteorology also heavily relies on regression models to forecast weather conditions. These models take high-dimensional inputs from various sensors and satellites to predict multiple continuous variables such as temperature, humidity, and wind speed. Moreover, many reinforcement learning algorithms include critical regression components, where regression is employed to predict value functions with the targets being Monte Carlo or bootstrapped returns.

While NC has been extensively studied in classification, to our knowledge, its prevalence and implications in regression remain unexplored. This paper investigates a new form of neural collapse within the context of neural multivariate regression. Analogous to the classification problem, we introduce Neural Regression Collapse (NRC): (NRC1) During training, the last-layer feature vectors collapse to the subspace spanned by the \(n\) principal components of the feature vectors, where \(n\) is the dimension of the targets (for univariate regression, \(n=1\)); (NRC2) The last-layer feature vectors also collapse to the subspace spanned by the weight vectors; (NRC3) The Gram matrix for the weight vectors converges to a specific functional form that depends on the square-root of the covariance matrix of the targets. A visualization of NRC is shown in Figure 1.

Employing six different datasets - including three robotic locomotion datasets, two versions of an autonomous driving dataset, and an age-prediction dataset - and Multi-Layer Perceptron (MLP) and ResNet architectures, we establish the prevalence of NRC1-NRC3. This discovery suggests a universal geometric behavior extending beyond classification into regression models, simplifying our understanding of deep learning more generally.

To help explain these phenomena, we then apply the UFM model to neural multivariate regression with an L2 loss function. In this regression version of the problem, the optimization problem aims to minimize the regularized mean squared error over continuous-valued targets. We show that when the regularization parameters in the UFM model are strictly positive, then (NRC1)-(NRC3) also emerge

Figure 1: Visualization of the neural regression collapse. The red dots represent the sample features, the blue arrows represent the row vectors of the last layer weight matrix, and the yellow plane represents the plane spanned by the principal components of the sample features. Here the target dimension is \(n=2\). The feature vectors and weight vectors collapse to the same subspace. The angle between the weight vectors takes specific forms governed by the covariance matrix of the targets.

as solutions in the UFM optimization problem, thereby providing a mathematical explanation of our empirical observations. Among many observations, we discover empirically and theoretically that when the regression parameters are zero or very small, there is no collapse; and if we increase the parameters a small amount above zero, the (NRC1)-(NRC3) geometric structure emerges.

To the best of our knowledge, this is the first empirical and theoretical study of neural collapse in the context of regression. By demonstrating the prevalence of neural collapse in regression tasks, we reveal that deep learning systems might inherently simplify their internal representations, irrespective of the specific nature of the task, whether it be classification or regression.

## 2 Related work

Neural collapse (NC) was first identified by Papyan et al. (2020) as a symmetric geometric structure observed in both the last layer features and classification vectors during the terminal phase of training of deep neural networks for classification tasks, particularly evident in balanced datasets. Since then, there has been a surge of research into both theoretical and empirical aspects of NC.

Several studies have investigated NC under different loss functions. For instance, (Han et al., 2021, Poggio and Liao, 2020, Zhou et al., 2022a) have observed and studied neural collapse under the Mean Squared Error (MSE) loss, while papers such as (Zhou et al., 2022b, Guo et al., 2024) have demonstrated that label smoothing loss and focal loss also lead to neural collapse. In addition to the last layer, some papers (He and Su, 2023, Rangamani et al., 2023) have also examined the occurrence of the NC properties within intermediate layers. Furthermore, beyond the balanced case, researchers have investigated the neural collapse phenomena in imbalanced scenarios. (Fang et al., 2021) identified a phenomenon called minority collapse for training on imbalanced data, while (Hong and Ling, 2023, Thrampoulidis et al., 2022, Dang et al., 2023) offer more precise characterizations of the geometric structure under imbalanced conditions.

To facilitate the theoretical exploration of the neural collapse phenomena, (Fang et al., 2021, Mixon et al., 2020) considered the unconstrained feature model (UFM). The UFM simplifies a deep neural network into an optimization problem by treating the last layer features as free variables to optimize over. This simplification is motivated by the rationale of the universal approximation theorem (Hornik et al., 1989), asserting that sufficiently over-parameterized neural networks can be highly expressive and can accurately approximate arbitrary smooth functions. Leveraging the UFM, studies such as (Zhu et al., 2021, Zhou et al., 2022a, Thrampoulidis et al., 2022, Tirer and Bruna, 2022, Tirer et al., 2023, Ergen and Pilanci, 2021, Wojtowytsch et al., 2020) have investigated models with different loss functions and regularization techniques. These studies have revealed that the global minima of the empirical risk function under UFMs align with the characterization of neural collapse observed by (Papyan et al., 2020). Beyond the UFM, some work (Tirer and Bruna, 2022, Sukenik et al., 2024) has extended the model to explore deep constrained feature models with multiple layers, aiming to investigate neural collapse properties beyond the last layer.

In addition to its theoretical implications, NC serves as a valuable tool for gaining deeper insights into DNN models and various regularization techniques (Guo et al., 2024, Fisher et al., 2024). It provides crucial insights into the generalization and transfer learning capabilities of neural networks (Hui et al., 2022, Kothapalli, 2022, Galanti et al., 2021), inspiring the design of enhanced model architectures for diverse applications. These include scenarios with imbalanced data (Yang et al., 2022, Kim and Kim) and contexts involving online continuous learning (Seo et al., 2024).

Despite extensive research on the neural collapse phenomena and its implications in classification, to the best of our knowledge, there has been no investigation into similar issues regarding neural regression models. Perhaps the paper closest to the current work is (Zhou et al., 2022a), which applies the UFM model to the balanced classification problem with MSE loss. Although focused on classification, (Zhou et al., 2022a) derive some important results which apply to regression as well as to classification. Our UFM analysis leverages this related paper, particularly their Lemma B.1.

## 3 Prevalence of neural regression collapse

We consider the multivariate regression problem with \(M\) training examples \(\{(_{i},_{i}),i=1,,M\}\), where each input \(_{i}\) belongs to \(^{D}\) and each target vector \(_{i}\) belongs to \(^{n}\). For the regression task, the deep neural network (DNN) takes as input an example \(^{D}\) and produces an output \(=f()^{n}\). For most DNNs, including those used in this paper, this mapping takes the form \(f_{,,}()=_{}( )+\), where \(_{}():^{D}^{d}\) is the non-linear feature extractor consisting of several nonlinear layers, \(\) is a \(n d\) matrix representing the final linear layer in the model, and \(^{n}\) is the bias vector. For most neural regression tasks, \(n<<d\), that is the dimension of the target space is much smaller than the dimension of the feature space. For univariate regression, \(n=1\). The parameters \(\), \(\), and \(\) are all trainable.

We train the DNN using gradient descent to minimize the regularized L2 loss:

\[_{,,}_{i=1}^{M}||f_{, ,}(_{i})-_{i}||_{2}^{2}+}{2}||||_{2}^{2}+}}{2}|||| _{F}^{2},\]

where \(||||_{2}\) and \(||||_{F}\) denote the \(L_{2}\)-norm and the Frobenius norm, respectively. As commonly done in practice, in our experiments we set all the regularization parameters to the same value, which we refer to as the weight-decay parameter \(_{WD}\), that is, we set \(_{}=_{}=_{WD}\).

### Definition of neural regression collapse

In order to define Neural Regression Collapse (NRC), let \(\) denote the \(n n\) covariance matrix corresponding to the targets \(\{_{i},i=1,,M\}\): \(=M^{-1}(-})(-})^{T}\), where \(=[_{1}_{M}]\), \(}=[}}]\), and \(}=M^{-1}_{i=1}^{M}_{i}\). Throughout this paper, we make the natural assumption that \(\) and \(\) have full rank. Thus \(\) is positive definite. Let \(_{}>0\) denote the minimum eigenvalue of \(\).

Denote \(:=[_{1}_{M}]\), where \(_{i}\) is the feature vector associated with input \(_{i}\), that is, \(_{i}:=_{}(_{i})\). Further denote the normalized feature vector \(}_{i}:=_{i}||_{i}||^{-1}\). Of course, \(\), \(\), and \(\) are changing throughout the course of training. For any \(p q\) matrix \(\) and any \(p\)-dimensional vector \(\), let \(proj(|)\) denote the projection of \(\) onto the subspace spanned by the columns of \(\). Let \(_{PCA_{n}}\) be the \(d n\) matrix with the columns consisting of the \(n\) principal components of \(\).

We say that _Neural Regression Collapse (NRC)_ emerges during training if the following three phenomena occur:

* NRC1 \(=_{i=1}^{M}||}_{i}- proj(}_{i}|_{PCA_{n}})||_{2}^{2} 0\).
* NRC2 \(=_{i=1}^{M}||}_{i}- proj(}_{i}|^{T})||_{2}^{2} 0\).
* There exists a constant \((0,_{})\) such that: \[=||^{T}}{|| ^{T}||_{F}}-^{1/2}-^{1/2}_{n}}{||^{1/2}-^{1/2}_{n}||_{F}}||_{F}^{2} 0.\]

NRC1 \( 0\) indicates that there is _feature-vector collapse_, that is, the \(d\)-dimensional feature vectors \(_{i}\), \(i=1,,M\), collapse to a much lower \(n\)-dimensional subspace spanned by their \(n\) principal components. In many applications, \(n=1\), in which case the feature vectors are collapsing to a line in the original \(d\)-dimensional space. NRC2 \( 0\) indicates that there is a form of _self duality_, that is, the feature vectors also collapse to the \(n\)-dimensional space spanned by the rows of \(\). NRC3 \( 0\) indicates that the last-layer weights have a _specific structure_ within the collapsed subspace. In particular, it gives detailed information about the norms of the row vectors in \(\) and the angles between those row vectors. NRC3 \( 0\) indicates that angles between the rows in \(\) are influenced by \(^{1/2}\). If the targets are uncorrelated so that \(\) and \(^{1/2}\) are diagonal, then NRC3 \( 0\) implies that the rows in \(\) will be orthogonal. NRC3 \( 0\) also implies a specific structure for the feature vectors, as discussed in Section 4.

### Experimental validation of neural regression collapse

In this section, we validate the emergence of NRC1-NRC3 during training across various datasets and deep neural network (DNN) architectures.

**Datasets**. The empirical experiments in this section are based on the following datasets:

* The **Swimmer, Reacher**, and **Hopper datasets** are based on MoJoCo (Todorov et al., 2012; Brockman et al., 2016; Towers et al., 2023), a physics engine that simulates diverse continuous multi-joint robot controls and has been a canonical benchmark for deep reinforcement learning research. In our experiments, we use publicly available expert datasets (see appendix A.1). Each dataset comprises raw robotic states as inputs (\(_{i}\)'s) and robotic actions as targets (\(_{i}\)'s). In order to put these expert datasets in an imitation learning context, we reduced the size of the dataset by keeping only a small portion of the episodes.
* The **CARLA dataset** originates from the CARLA Simulator, an open-source project designed to support the development of autonomous driving systems. We utilize a dataset Codevilla et al. (2018) sourced from expert-driven offline simulations. During these simulations, images (\(_{i}\)'s) from cameras mounted on the virtual vehicle and corresponding expert driver actions as targets (\(_{i}\)'s) are recorded as human drives in the simulated environment. We consider two dataset versions: a 2D version with speed and steering angle, and a 1D version with only the speed.
* The **UTKFace dataset**(Zhang et al., 2017) is widely used in computer vision to study age estimation from facial images of humans. This dataset consists of about 25,000 facial images spanning a wide target range of ages, races, and genders.

Table 1 summarizes the six datasets, with the dimensions of the target vectors \(\) ranging from one to three. The table also includes the minimum eigenvalue of the associated covariance matrix \(\) and the Pearson correlation values between the \(i\)-th and \(j\)-th target components for \(i j\). When \(n=1\), there is no correlation value; when \(n=2\), there is one correlation value between the two target components; and when \(n=3\), there are three correlation values among the three target components. From the table, we observe that the target components in CARLA 2D and Reacher are nearly uncorrelated, whereas those in Hopper and Swimmer exhibit stronger correlations.

**Experiment Settings**. For the Swimmer, Reacher, and Hopper datasets, we employed a four-layer MLP (with the last layer being the linear layer) as the policy network for the prediction task. Each layer consisted of 256 nodes, aligning with the conventional model architecture in most reinforcement learning research (Tarasov et al., 2022). For the CARLA and UTKFace datasets, we employed ResNet18 and ResNet34 He et al. (2016), respectively. To focus on behaviors associated with neural collapse and minimize the influence of other factors, we applied standard preprocessing without data augmentation.

All experimental results are averaged over at least 2 random seeds and variance is displayed by a shaded area. The choices of weight decay employed during training varied depending on the dataset. Also, the number of epochs required for training depends on both the dataset and the degree of weight decay. In particular, we used a large number of epochs when using very small weight decay values. appendix A provides the full experimental setup.

**Empirical Results**. Figure 2 presents the experimental results for the six datasets mentioned above. The results show that the training and testing errors decrease as training progresses, as expected. The converging coefficient of determination (\(R^{2}\)) also indicates that model performance becomes stable. Most importantly, the figure confirms the presence of NRC1-NRC3 across all six datasets. This indicates that neural collapse is not only prevalent in classification but also often occurs in multivariate regression.

  
**Dataset** & **Data Size** & **Input Type** & **Target Dimension \(n\)** & **Target Correlation** & \(_{}\) \\  Swimmer & 1,000 & raw state & 2 & -0.244 & 0.276 \\ Reacher & 1,000 & raw state & 2 & -0.00933 & 0.0097 \\ Hopper & 10,000 & raw state & 3 & [-0.215, -0.090, 0.059] & 0.215 \\  Carla 1D & 600,000 & RGB image & 1 & NA & 208.63 \\  Carla 2D & 600,000 & RGB image & 2 & -0.0055 & 0.156 \\  UTKFace & 25,000 & RGB image & 1 & NA & 1428 \\   

Table 1: Overview of datasets employed in our neural regression collapse analysis.

We also experimentally analyze the explained variance ratio (EVR) of principal components to further verify the collapse to the subspace spanned by the first \(n\) components. In Figure 3, we investigate the EVR of the first 5 principal components of \(\) during the training process. For all datasets, there is significant variance for all of the first \(n\) components after a short period of training; for other components, there is very low or even no variance. This also supports that a perfect collapse occurs in the subspace spanned by the first \(n\) principal components.

Our definition of NRC3 involves finding a scaling factor \(\) for which the property holds. Figure 4 illustrates the values of NRC3 as a function of \(\) for \(\) obtained after training. We observe that each

Figure 3: Explained Variance Ratio (EVR) for the first 5 principal components (PC).

Figure 2: Prevalence of NRC1-NRC3 in the six datasets. Train/Test MSE and the coefficient of determination (\(R^{2}\)) are also shown.

dataset exhibits a unique minimum value of \(\). More details about computing NRC3 can be found in appendix A.3.

Figure 5 investigates neural regression collapse for small values of the weight-decay parameter \(_{WD}\). (appendix A.4 contains results on all 6 datasets.) We see that when \(_{WD}=0\), there is no neural regression collapse. However, if we increase \(_{WD}\) by a small amount, collapse emerges for all three metrics. Thus we can conclude that the geometric structure NRC1-3 that emerges during training is due to regularization, albeit the regularization can be very small. In the next section, we will introduce a mathematical model that helps explain why there is no collapse when \(_{WD}=0\) and why it quickly emerges as \(_{WD}\) is increased above zero.

## 4 Unconstrained feature model

As discussed in the related work section, the UFM model has been extensively used to help explain the prevalence of neural collapse in the classification problem. In this section, we explore whether the UFM model can also help explain neural collapse in neural multivariate regression.

Specifically, we consider minimizing \((,,)\), where

\[(,,)=||+_{M}^{T}-||_{F}^{2}+}}{2M}|| ||_{F}^{2}+}}{2}||||_{F}^{2},\] (1)

where \(_{M}^{T}:=[1 1]\) and \(_{}\), \(_{}\) are non-negative regularization constants.

The optimization problem studied here bears some resemblance to the standard linear multivariate regression problem. If we view the features \(_{i}\), \(i=1,,M\), as the inputs to linear regression, then \(}_{i}:=_{i}+\) is the predicted output, and \(||_{i}-}_{i}||_{2}^{2}\) is the squared error. In standard linear regression, the \(_{i}\)'s are fixed inputs. In the UFM model, however, not only are we optimizing over the weights \(\) and biases \(\) but also over all the "inputs" \(\).

Figure 4: The optimal value of \(\) for NRC3.

Figure 5: Phase change in neural collapse for small weight-decay values

For the case of classification, regularization is needed in the UFM model to prevent the norms of \(\) and/or \(\) from going to infinity in the optimal solutions. In contrast, in the UFM regression model, the norms in the optimal solutions will be finite even without regularization. However, as regularization is typically used in neural regression problems to prevent overfitting, it is useful to include regularization in the UFM regression model as well.

### Regularized loss function

Throughout this subsection, we assume that both \(_{}\) and \(_{}\) are strictly positive. We shall consider the \(_{}=_{}=0\) case subsequently. We also make a number of assumptions in order to not get distracted by less important sub-cases. Throughout we assume \(n d\), that is, the dimension of the targets is not greater than the dimension of the feature space. As stated in a previous subsection, for problems of practical interest, we have \(n<<d\). Recall that \(\) is the covariance matrix of the target data. Since \(\) is a covariance matrix and is assumed to have full rank, it is also positive definite. It therefore has a positive definite square root, which we denote by \(^{1/2}\). Let \(_{}:=_{1}_{2}_{n}:=_ {}>0\) denote the \(n\) eigenvalues of \(\). We further define the \(n n\) matrix

\[:=^{1/2}-_{n},\] (2)

where \(c:=_{}_{}\). Also for any \(p q\) matrix \(\) with columns \(_{1},_{2},,_{q}\), we denote \([]_{j}\) to be the \(p q\) matrix whose first \(j\) columns are identical to those in \(\) and whose last \(q-j\) columns are all zero vectors, i.e., \([]_{j}=[_{1}\ _{2}_{j}\ ]\). All proofs are provided in the appendix.

**Theorem 4.1**.: _Any global minimum \((,,)\) for (1) takes the following form: If \(0<c<_{}\), then for any semi-orthogonal matrix \(\),_

\[=(}}{_{}})^{1/ 4}[^{1/2}]_{j*}, 14.226378pt=}}{_{}}}^{T}[^{1/2}]^{-1}(-}), 14.226378pt= },\] (3)

_where \(j*:=\{j:_{j} c\}\). If \(c>_{}\), then \((,,)=(,,})\). Furthermore, if \((,,)\) is a critical point but not a global minimum, then it is a strict saddle point._

Theorem 4.1 has numerous implications, which we elaborate on below.

### One-dimensional univariate case

In this subsection, we highlight the important special case \(n=1\), which often arises in practice (such as with Carla 1D and the UTKface datasets). When \(n=1\), \(\) is simply the scalar \(^{2}\), which is the variance of the one-dimensional targets over the \(M\) samples. Also, \(\) is a row vector, which we denote by \(\). Theorem 4.1, for \(n=1\) provides the following insights:

1. Depending on whether \(0<c<^{2}\) or not, the global minimum takes on strikingly different forms. In the case, \(c>^{2}\), corresponding to very large regularization parameters, the optimization problem ignores the MSE and entirely focuses on minimizing the norms \(||||_{F}^{2}\) and \(||||_{2}^{2}\), giving \(||||_{F}^{2}=0\), \(||||_{2}^{2}=0\).
2. When \(0<c<^{2}\), the optimal solution takes a more natural and interesting form: For any unit vector \(^{d}\), the solution \((,,b)\) given by \[^{T}=}(}-1) }, 28.452756pt=}}}{ }}}^{T}(-}),  28.452756ptb=,\] (4) is a global minimum. Thus, all vectors \(\) on the sphere given by \(||||_{2}^{2}=_{}(}-1)\) are optimal solutions. Furthermore, \(_{i}\), \(i=1,,M\), are all in the one-dimensional subspace spanned by \(\). Thus the optimal solution of the UFM model provides a theoretical explanation for NRC1-NRC2. (NRC3 is not meaningful for the one-dimensional case.) Note that the \(_{i}\)'s have a global zero mean and the norm of \(_{i}\) is proportional to \(|y_{i}-|\).

### General \(n\)-dimensional multivariate case

In most cases of practical interest, we will have \(c<_{}\), so that \([^{1/2}]_{j*}=^{1/2}\) in Theorem 4.1.

**Corollary 4.2**.: _Suppose \(0<c<_{}\). Then the global minima given by (3) have the following properties:_

_(i) All of the \(d\)-dimensional feature vectors \(_{i}\), \(i=1,,M\), lie in the \(n\)-dimensional subspace spanned by the \(n\) rows of \(\). (ii) \(^{T}=}}{_{ }}}[^{1/2}-_{n}]\), (iii) \(_{}||||_{F}^{2}=M_{}|||| _{F}^{2}\), (iv) \((,,)=nc/2+||^{1/2} ||_{F}^{2}\), (v) \(+_{M}^{T}-=-[ ^{1/2}]^{-1}(-})\)._

From Theorem 4.1 and Corollary 4.2, we make the following observations:

1. Most importantly, the global minima in the UFM solution match the empirical properties (NRC1)-(NRC3) observed in Section 3. In particular, the theory precisely predicts NRC3, with \(=c\). This confirms that the UFM model is an appropriate model for neural regression.
2. Unlike the one-dimensional case, the feature vectors are no longer colinear with any of the rows of \(\). Moreover, after rotation and projection (determined by the semi-orthogonal matrix \(\)), the angles between the target vectors in \(-}\) do not in general align with the angles between the feature vectors in \(\). However, if the target components are uncorrelated, so that \(\) is diagonal, then \(\) is also diagonal and there is alignment between \(\) and \(-}\).

Theorem 4.1 also provides insight into the "strong regularization" case of \(c>_{}\). In this case, the rows of \(\) and the feature vectors \(\) in the global minima belong to a subspace that has dimension even smaller than \(n\), specifically, to dimension \(j*<n\). To gain some insight, assume that the target components are uncorrelated so that \(\) is diagonal and \(_{j}=_{j}^{2}\), i.e., \(_{j}^{2}\) is the variance of the \(j\)-th target component. Then for a target component for which \(c>_{j}^{2}\), the corresponding row in \(\) will be zero and the component prediction will be \(}_{i}^{(j)}=}^{(j)}\) for all examples \(i=1,,M\). For more details, we refer the reader to Section D.1 in the appendix.

### Removing regularization

In the previous theorem and corollary, we assumed the presence or L2 regularization for \(\) and \(\), that is, we assumed \(_{}>0\) and \(_{}>0\). Now we explore the structure of the solutions to the UFM when \(_{}=_{}=0\). In this case, the UFM model is modeling the real problem with \(_{WD}\) equal to or close to zero. The loss function becomes:

\[L(,)=||-||_{F}^{ 2}.\] (5)

For this case, we do not need bias since we can obtain zero loss without it.

**Theorem 4.3**.: _The solution \((,)\) is a global minimum if and only if \(\) is any \(n d\) full rank matrix and_

\[=^{+}+(_{d}-^{+}) ,\] (6)

_where \(^{+}\) is the pseudo-inverse of \(\) and \(\) is any \(d M\) matrix. Consequently, when there is no regularization, for each full-rank \(\) there is an infinite number of global minima \((,)\) that do not collapse to any subspace of \(^{d}\)._

From Theorem 4.3, when there is no regularization, the feature vectors do not collapse. Moreover, any full rank \(\) provides an optimal solution. For example, for \(n=2\), the two rows of \(\) can have any angle between them except angle 0 and angle 180. This is very different from the results we have for \(_{},_{}>0\), in which case \(\) depends on the covariance matrix \(\). Note that if we set \(_{}=_{}\) and let \(_{} 0\), then the limit of \(\) still depends on \(\). Thus there is a major discontinuity in the solution when \(_{},_{}\) goes to zero. We also observed this phase shift in the experiments (see Figure 5). We can therefore conclude that neural regression collapse is not an intrinsic property of neural regression alone. The geometric structure of neural regression collapse is due to the inclusion of regularization in the loss function.

### Empirical results with UFM assumptions

We also provide empirical results for the case when we train with the same form of regularization as assumed by the UFM model. Specifically, we turn off weight decay and add an L2 penalty on the last-layer features \(_{i}\), \(i=1,,M\), and on the layer linear weights \(\). Additionally, we omit theReLU activation function in the penultimate layer, allowing the feature representation produced by the feature extractor to take any value, thus reflecting the UFM model. For these empirical results, when evaluating NRC3, rather than searching for \(\) as in the definition of NRC3, we use the exact value of \(\) given by Theorem 4.1, that is, \(=_{}_{}=c\).

Figure 6 illustrates training MSE and NRC metrics for varying values of \(c\). (For simplicity, we only considered the case where \(_{}=_{}\). Appendix B contains results on remaining datasets.) As we are considering a different model and loss function for these empirical experiments, convergence occurs more quickly and so we train for a smaller number of epochs. We can conclude that the UFM theory not only accurately predicts the behavior of the standard L2 regularization approach with weight-decay for all parameters (Figure 5), but also accurately predicts the behavior when regularization follows the UFM assumptions (Figure 6).

## 5 Conclusion

We provided strong evidence, both empirically and theoretically, of the existence of neural collapse for multivariate regression. This extension is significant not only because it broadens the applicability of neural collapse to a new category of problems but also because it suggests that the phenomena of neural collapse could be a universal behavior in deep learning. However, it is worth acknowledging that while we have gained a better understanding of the model behavior of deep regression models in the terminal phase of training, we have not addressed the connection between neural regression collapse and model generalization. This crucial aspect remains an important topic for future research.

Figure 6: Empirical results with UFM assumption where L2 regularization on \(\) and \(\) are used instead of weight decay.