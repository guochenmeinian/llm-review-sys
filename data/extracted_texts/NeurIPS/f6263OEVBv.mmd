# miniCTX: Neural Theorem Proving with

(Long-)Contexts

 Jiewen Hu  Thomas Zhu  Sean Welleck

Carnegie Mellon University

###### Abstract

Real-world formal theorem proving often depends on a wealth of context, including definitions, lemmas, comments, file structure, and other information. We introduce miniCTX, which tests a model's ability to prove formal mathematical theorems that depend on new context not encountered in training. miniCTX contains theorems sourced from real Lean projects and textbooks, each associated with a context that can span tens of thousands of tokens. Models are tasked with proving a theorem given access to code from the theorem's repository, which contains context that is needed for the proof. As a baseline for miniCTX, we tested fine-tuning and prompting methods that condition theorem proving on preceding context. Both approaches substantially outperform traditional methods that rely solely on state information. We found that this ability to use context is not captured by previous benchmarks such as miniF2F. Alongside miniCTX, we offer ntp-toolkit for automatically extracting and annotating theorem proving data, making it easy to add new projects into miniCTX to ensure that contexts are not seen during training. miniCTX offers a challenging and realistic evaluation of neural theorem provers.

## 1 Introduction

Formal theorem proving in interactive theorem provers (ITPs) provides a testbed for evaluating the reasoning capabilities of large language models (LLMs). Theorem proving capabilities can then directly translate to automation for mathematicians, such as via tools that complete or formalize proofs . However, despite their promise, we see a gap between the evaluation of current language model-based provers and the complexity of real-world theorem proving.

Our motivating observation is that theorems and proofs depend on various forms of _context_, such as newly-defined definitions and lemmas. For instance, to prove results about a square, one might first formalize a definition of a rectangle, prove some results about rectangles, then specialize them to a newly-defined square  (Figure 1). However, existing methods for training and evaluating LLM-based theorem provers often fail to incorporate the full range of contextual information available in real-world projects. For example, benchmarks often focus on proving standalone competition problems (e.g., miniF2F) or theorems from a library that the model has trained on (e.g., Mathlib ), and state-of-the-art LLM-based provers are trained to accept only a proof state as input, making them unaware of new theorems and definitions . While some existing work, including premise selection techniques  and datasets like CoqGym , have explored theorem proving based on information beyond the current state, they often focus on a subset of the available information. They primarily focus on providing relevant premises, such as lemmas, to assist in proof construction.

Building on these foundations, we propose miniCTX: a benchmark that seeks to expand the scope of context used in theorem proving. We extend beyond traditional premise selection explored in prior benchmarks (e.g., ) by incorporating a more comprehensive set of contextual elements. This includes premises, but also prior proofs, comments, notation, and structural components like imports and declarations. By doing so, miniCTX aims to drive the development of methods that understandand work with context that occurs in complex, real-world theorem proving tasks. Additionally, considering the common use of pre-trained language models we mitigate potential data contamination by continually and automatically updating miniCTX with new Lean projects, so that evaluated theorems are not seen during training. Our key contributions are:

miniCTX**Benchmark:** We introduce miniCTX, the first benchmark designed specifically to evaluate theorem proving in real-world settings where proofs depend on in-file definitions, lemmas, and context from formal projects. miniCTX presents a unique challenge by requiring models to reason over long contexts and handle dependencies that arise in real-world theorem proving tasks.

**ntp-toolkit:** To facilitate the automatic updating of miniCTX, we developed the ntp-toolkit, which automatically extracts relevant theorems and contexts from Lean projects. Additionally, we provide a Lean REPL wrapper that enables simpler evaluation on miniCTX.

**Baseline Evaluations:** We evaluate miniCTX on several existing baseline models, including different fine-tuning and prompting strategies, as well as premise selection. We also propose file-tuning, a strong baseline method for training models using full file contexts, where both the theorem statements and their surrounding context are provided during training. This approach establishes a robust baseline for future work on context-dependent theorem proving.

## 2 Theorem proving with context

Formal theorem proving involves two stages: defining mathematical objects and facts relevant to the desired result, then stating and proving the result itself. Many current language model-based provers focus on the proving process and are trained on static datasets that only use a fixed set of definitions. As a result, they lack the ability to recognize new definitions or lemmas at test time (Figure 1).

  
**Benchmark** & **Language** & **Premise** & **Full Context** & **Multi-source** & **Temporal Split** \\  miniF2F  & Multiple & \(\) & \(\) & \(\) & \(\) \\ ProofNet  & Lean & \(\) & ✓ & ✓ & \(\) \\ LeanDojo  & Lean & ✓ & \(\) & \(\) & \(\) \\ LeanStep  & Lean & ✓ & \(\) & ✓ & \(\) \\ CoqGym  & Coq & ✓ & \(\) & ✓ & \(\) \\ PISA  & Isabelle & \(\) & \(\) & ✓ & \(\) \\  miniCTX (Ours) & Lean & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison of theorem proving benchmarks across several key features.

Figure 1: Many state of the art provers are trained on a static dataset of theorems and proofs, then evaluated on standalone problems such as competition problems (left). We argue that neural provers must also operate in the realistic _context-dependent_ setting, in which results depend on working with new mathematical objects and their facts, notations, and the structural elements of the project (imports, variables, etc.) (right).

**Context-dependent proving.** We study _context-dependent theorem proving_, where the goal is for a model to generate proofs \(y\) for new theorems \(x\), based on a context \(c\) that includes background information such as definitions, lemmas, or natural language comments. Formally, the problem is

\[_{M}_{(x,c) p}_{y M(|x,c)}v(x,c,y),\] (1)

where \((x,c) p\) denotes a (theorem, context) pair from a context distribution \(p\), \(M\) is a model that produces a proof \(y\), and \(v\) returns 1 if the proof is correct and 0 otherwise.

We choose Lean  as the verifier \(v\), because of the large body of recent theorems in Lean that can be used as evaluation data, and the abundance of proving methods in Lean that we use as baselines. We treat a Lean repository as the distribution \(p\). Each context \(c\) is a subset of the repository, including new definitions, lemmas, notations, imports, and comments that are relevant to the theorem.

## 3 miniCTX: a benchmark for theorem proving with context

We develop miniCTX, a Lean 4 theorem proving benchmark of theorems that depend on newly-defined lemmas, definitions, and proofs from within a project. miniCTX is currently based on 376 theorems from four projects: (1) Prime Number Theorem (**Prime**) , (2) Polynomial Freman-Ruzsa Conjecture (**PFR**) , (3) an introductory text on theorem proving (**HTPI**) , (4) recent results from the standard mathematical library (**Mathlib**)  (motivation and details in SSD), (5) high energy physics formalization in HepLean (**HEP**) , and (6) scientific computing formalizations (**SciLean**) .

Each theorem in miniCTX consists of the theorem statement, preceding file contents up to the theorem statement, and metadata, in JSON (see SSE.1).

1. Theorem statement,
2. Preceding file contents up to the theorem statement,
3. Metadata, including file name, commit and time which the theorem was added, position and length of the theorem and proof, and the number and types of premises used in the human-written proof.

Using our benchmark, users can easily reconstruct the complete context for each theorem, including both in-file and cross-file context. The in-file context is provided directly by preceding file contents, while the cross-file context can be reconstructed using the metadata, which includes information on imported modules. We open-source the dataset and evaluation code.

### Key features and challenges

miniCTX introduces several key features that distinguish it from other theorem proving benchmarks, addressing challenges that have not been tackled by previous benchmarks:

**Real-world theorem proving.** Unlike popular benchmarks (e.g., miniF2F , ProofNet , FIMO ) that focus on isolated competition problems, real-world research-level theorem proving is heavily dependent on rich mathematical contexts. Therefore, miniCTX includes real-world, complex theorems from a variety of ongoing Lean projects, such as Prime Number Theorem (Prime) and Polynomial Freiman-Ruzsa Conjecture (PFR). They rigorously test a model's ability in real-world formalization projects. This diversity contrasts with the LeanDojo benchmark , which focuses solely on Mathlib, enabling miniCTX to better test a model's generalization in different settings.

**Contextual evaluation.** Proving a theorem often depends on new definitions, lemmas, or other contextual information, which a model may not have seen during training. miniCTX includes theorems along with this new context. During evaluation, the model is expected to leverage the provided new context to help prove the theorem.

Beyond previous datasets like LeanDojo  and CoqGym , which include relevant definitions and theorems, miniCTX includes additional useful contextual information that may make some theorems _easier_ to prove compared to standalone theorems. For instance, Lean source code can have natural language comments that may help constrain the space of possible proofs. Moreover, some proofs within a file often have analogous patterns or structure, which may make subsequent theorems easier to prove (see SSE.2). These additional forms of context occur in the real-world process of formalization, yet their use in neural theorem proving is underexplored.

## 4 Experiments

### Baselines

We evaluate several baselines on miniCTX, demonstrating the importance of context in real-world theorem proving. Our investigation reveals several open challenges that we discuss in SSA. See SSB for a detailed description of the motivation, baselines, data extraction, and evaluation setup, and SSH for full results and more detailed analysis. The baselines are as follows:

**Prompting LLMs.** We first test the ability of a state of the art API-based model, GPT-4o, to generate the complete proof in one pass (pass@1) given the theorem statement, with several few-shot examples provided for guidance. We additionally test whether adding context in the form of preceding file contents improves the proof rate of GPT-4o.

**State-tactic prompting.** Another common approach to theorem proving using language models is to let the model generate a tactic given the current proof state [7; 8; 9; 25]. Therefore, we test the _state-tactic prompting_ setting, which prompts a model specialized for mathematical tasks, Llemma-7b , to output a tactic given a proof state. At test time, the model generates one tactic at a time, and we use a best-first search to construct full proofs [7; 8; 9; 1].

**State-tactic tuning.** We follow this _state-tactic_ framework and fine-tune a _state-tactic tuned_ model from DeepSeek-Coder-1.3b  to input proof states and output tactics, trained on human-written tactics in Mathlib, the main mathematical library in Lean, extracted by ntp-tolkit.

**File-tuning.** We then test whether supplying context, in the form of preceding file contents, to the model improves performance. Similar to state-tactic tuning, we fine-tune a 1.3b model to generate a tactic based on (proof state, context) pairs, resulting in the _file-tuned_ model.

**Premise selection.** To better simulate a complete context and evaluate on project-level generalization, we apply premise selection to extract relevant premises from imported files within the same repository. We use the premise retriever provided by LeanDojo  to identify the top 20 most relevant definitions or lemmas from imported modules and append them to the in-file context.

### Results

**Context-dependent methods improve theorem proving.** Table 2 shows baseline performances on miniCTX. We see a dramatic improvement for the file-tuned model (trained on full file context) over the state-tactic model (trained only on proof states) (35.94% vs. 19.53%). Similarly, providing the preceding file context, which includes definitions and lemmas, to GPT-4o results in dramatic improvement compared to using just the proof state (27.08% vs. 11.72%). Figure 4 shows the performance of state-tactic tuned model and file-tuned model on problems with in-file dependencies compared to those without. These findings highlight the importance of providing models with rich

Figure 2: miniCTX is automatically updated with Lean projects to stay ahead of LLM training cutoff dates, making it a suitable benchmark for real-world theorem proving for pre-trained models. Figure 3: State-tactic vs. file tuning.

contextual information beyond the immediate proof state, also demonstrating that miniCTX is able to measure this ability of context-dependent proving.

**Premise selection improves performance on high cross-file dependency splits.** The results in Table 2 indicate that premise selection has a mixed impact on model performance. For the GPT-4o, premise selection improves performance on high cross-file dependency splits, such as PFR, PFR\({}_{}\), and SciLean. This suggests that premise selection helps capture the cross-file context, enabling GPT-4o to make better use of cross-file information. However, for the file-tuned model, premise selection does not consistently improve results, and even performs worse on the PFR\({}_{}\) split, which was designed to evaluate the effective use of cross-file premises. Also shown in Figure 4, GPT-4o benefits significantly from premise selection on problems with high cross-file dependencies, but degrades in other cases. This suggests that the retrieved premises differ significantly from the in-file context. Therefore, developing methods that effectively support the integration of cross-file context (e.g., premise selection) alongside in-file context remains an interesting open research direction for improving performance on the miniCTX benchmark.

**Evaluation on miniF2F.** We evaluate baselines on miniF2F, a standard benchmark based on competition problems that do not require context. The file-tuned model improves very little beyond the state-tactic model (33.61% vs. 32.79%), showing that the dramatic difference in context-dependent proving abilities seen on miniCTX cannot be captured by miniF2F.

**Additional analysis.** Further analysis shows that file-tuning delivers greater gains on problems with stronger dependencies on new lemmas. Both definitions and theorems are crucial in the context, and models show ability to learn proof structure from previous lemmas. See SS3.2 for more details.

## 5 Conclusion

We studied the realistic setting of proving theorems that depend on new information and project constraints, and formulated an evaluation framework for testing generalization using real Lean projects. We built miniCTX, and found that the predominant method for training neural theorem provers fails to enable context dependent proving. Our file tuning method provides a strong starting point for the new challenges opened by our investigation into theorem proving with context.

    & miniF2F &  \\ 
**Method** & **Test** & **Prime** & **PFR** & **PFR\({}_{}\)** & **Mathlib** & **HTPI** & **HEP** & **SciLean** & **Avg.** \\  GPT-4o (full proof) & — & 7.06 & 1.85 & 6.98 & 14.00 & 13.33 & 31.15 & 6.52 & 11.72 \\ + context & — & 31.76 & 5.56 & 34.88 & 26.00 & **17.78** & 49.18 & 17.39 & 27.08 \\ + context + premise & — & 29.41 & 7.41 & 39.53 & — & 15.56 & 44.26 & 21.74 & 26.82 \\ State-tactic promoting & 28.28 & 20.00 & 5.56 & 0.00 & 16.00 & 0.00 & 31.15 & 19.57 & 14.58 \\ State-tactic tuning & 32.79 & 17.65 & 5.56 & 0.00 & 22.00 & 11.11 & 52.46 & 19.57 & 19.53 \\ File tuning & **33.61** & 40.00 & 5.56 & **44.19** & **34.00** & 15.56 & **60.66** & **45.65** & **35.94** \\ + premise & — & **42.35** & **11.11** & 16.28 & — & 8.89 & 50.82 & 32.61 & 30.21 \\   

Table 2: Performance comparison (%) of different models on miniF2F and miniCTX.

Figure 4: Model performance by dependency on premises. For each theorem in miniCTX, we record as metadata whether its human-written proof depends on other definitions or theorems in the same file (“in-file”) or in other files (“cross-file”), and test the performance of baselines on each type.