# An \(\varepsilon\)-Best-Arm Identification Algorithm

# An \(\)-Best-Arm Identification Algorithm

for Fixed-Confidence and Beyond

 Marc Jourdan

marc.jourdan@inria.fr

&Remy Degenne

remy.degenne@inria.fr

Emilie Kaufmann

emilie.kaufmann@univ-lille.fr

Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189-CRIStAL, F-59000 Lille, France

###### Abstract

We propose EB-TC\({}_{}\), a novel sampling rule for \(\)-best arm identification in stochastic bandits. It is the first instance of Top Two algorithm analyzed for approximate best arm identification. EB-TC\({}_{}\) is an _anytime_ sampling rule that can therefore be employed without modification for fixed confidence or fixed budget identification (without prior knowledge of the budget). We provide three types of theoretical guarantees for EB-TC\({}_{}\). First, we prove bounds on its expected sample complexity in the fixed confidence setting, notably showing its asymptotic optimality in combination with an adaptive tuning of its exploration parameter. We complement these findings with upper bounds on its probability of error at any time and for any error parameter, which further yield upper bounds on its simple regret at any time. Finally, we show through numerical simulations that EB-TC\({}_{}\) performs favorably compared to existing algorithms, in different settings.

## 1 Introduction

In pure exploration problems, the goal is to answer a question about a set of unknown distributions (modelling for example the efficacy of a treatment) from which we can collect samples (measure its effect), and to provide guarantees on the candidate answer. Practitioners might have different pre-defined constraints, e.g. the maximal budget might be fixed in advance or the error made should be smaller than a fixed admissible error. However, in many cases, fixing such constraints in advance can be challenging since a "good" choice typically depends on unknown quantities. Moreover, while the budget is limited in clinical trials, it is often not fixed beforehand. The physicians can decide to stop earlier or might obtain additional fundings for their experiments. In light of those real-world constraints, regardless of its primal objective any strategy for choosing the next treatment should ideally come with guarantees on its current candidate answer that hold at any time.

We formalize our investigations in the well-studied stochastic bandit model , in which a learner interacts sequentially with an environment composed of \(K\) arms, which are unknown distributions \((_{i})_{i[K]}\) with finite means \((_{i})_{i[K]}\). At each stage \(n\), the learner chooses an arm \(I_{n}[K]\) based on the samples previously observed and receives a sample \(X_{n,I_{n}}\), random variable with conditional distribution \(_{I_{n}}\) given \(I_{n}\). It then proceeds to the next stage. An algorithm for the learner in this interaction is specified by a _sampling rule_, a procedure that determines \(I_{n}\) based on previously observed samples. Formally, the sampling rule defines for all \(n\) a function from \(([K])^{n-1}\) to the probability distribution on \([K]\), which is measurable with respect to the \(\)-algebra \(_{n}:=(\{I_{t},X_{t,I_{t}}\}_{t[n-1]})\). We call that \(\)-algebra _history_ before \(n\).

Identification tasksWe focus on best arm identification (BAI). In that task, the goal of the algorithm is to find which of the arms has the largest mean, and to do so with a small probability of error, asquickly as possible. If several arms have means very close to the maximum, finding the one with the highest mean might be difficult. However in practice we are often satisfied by any good enough arm, in the sense that its mean is greater than \(_{}-\), where \(_{}=_{i[K]}_{i}\). This is the \(\)-BAI task. Our results can also be adapted to the multiplicative \(\)-BAI task, in which all means are non-negative and we want to find an arm with mean \(_{i}(1-)_{}\) (see Appendix 1 for details).

Now that we have a (for now informal) goal, we need to complement the sampling rule with a recommendation rule that specifies which arm is the candidate returned by the algorithm for the best arm. We follow  and define that rule for all stages: for all \(n\), we denote by \(_{n}\) this \(_{n}\)-measurable function from \(([K])^{n-1}\) to \([K]\). We call _algorithm_ the combination of a sampling and a recommendation rule.

Performance criteriaThere are several ways to evaluate the performance of an algorithm for \(\)-BAI. Let \(_{}()=\{i[K]_{i}_{}-\}\) be the set of \(\)-good arms. The _probability of \(\)-error_ or the recommendation at \(n\) is defined as \(_{}(_{n}_{}())\). Introduced in , the expected _simple regret_ is defined as \(_{}[_{}-_{i_{n}}]\), and is independent of any parameter \(\). Based on those notions, several setting are studied in the bandit identification literature.

* Fixed confidence: we augment the algorithm with a _stopping rule_, a stopping time \(_{,}\) with respect to the history of samples and we impose that the algorithm should be \((,)\)-PAC. That is, its probability of \(\)-error at \(_{,}\) must satisfy \(_{}(_{,}<+,_{_{,}}_{}())\). The parameter \(\) is known to the algorithm. An algorithm is judged based on its expected sample complexity \([_{,}]\), the expected number of samples it needs to collect before it can stop and return a good arm with the required confidence.
* Fixed budget: we run the algorithm until a predefined time \(T\) and we evaluate it based on the probability of error at \(T\). This setting has been mostly studied for \(=0\), but  present the first bounds for \(>0\) for an algorithm that is actually agnostic to this value.
* Simple regret minimization: we evaluate the expected simple regret at \(T\).

Simple regret is typically studied in an anytime setting:  contains upper bounds on the simple regret at time \(n\) for any \(n^{*}\). Similarly,  propose the _anytime exploration_ setting, in which they control the error probability \((i_{n} i_{})\) for exact best arm identification. Interestingly, the authors build on an algorithm for the fixed-confidence setting, LUCB , whose sampling rule depends on the risk parameter \(\), which they replace by a sequence \(_{n}\). The algorithm that we study in this paper, motivated by the fixed-confidence \(\)-BAI problem, will already be _anytime_, which means that it does not depend on a given final time \(T\) or a confidence level \(\). We shall analyze its sample complexity in the fixed confidence setting but thanks to the anytime property we will also be able to prove guarantees on its probability of \(\)-error for every \( 0\) and its simple regret at any time.

Additional notation and assumptionWe denote by \(\) a set to which the distributions of the arms are known to belong. We suppose that all distributions in \(\) are 1-sub-Gaussian. A distribution \(_{0}\) is 1-sub-Gaussian if it satisfies \(_{X_{0}}[e^{(X-_{X_{0}}[X])}] e^{ ^{2}/2}\) for all \(\). For example, all distributions bounded in \([-1,1]\) are 1-sub-Gaussian. Let us denote by \(i^{*}():=*{arg\,max}_{i[K]}_{i}\) the set of arms with largest mean (i.e. \(i^{*}()=_{0}()\)). Let \(_{i}:=_{}-_{i}\) denote the sub-optimality gap of arm \(i\). We denote by \(_{K}^{K}\) the simplex of dimension \(K-1\).

Fixed-confidence \(\)-best-arm identificationLet \( 0\) and \((0,1)\) be fixed error and confidence parameters. In the _fixed-confidence \(\)_-BAI setting , the probability of error of an algorithm is required to be less than \(\) for all instances \(^{K}\). That requirement leads to an asymptotic lower bound on the expected sample complexity on any instance.

**Lemma 1** ().: _For all \((,)\)-PAC algorithms and all instances \(_{i}=(_{i},1)\) with \(^{K}\), \(_{ 0}_{}[_{,}]}{ (1/)} T_{}()\) where \(T_{}()=_{i_{}()}_{(0,1)}T_{,}(,i)\) with_

\[T_{,}(,i)^{-1}=_{w_{K},w_{i}=}_{j  i}-_{j}+)^{2}}{1/+1/w_{j}}\,.\] (1)

We say that an algorithm is asymptotically (resp. \(\)-)optimal if its sample complexity matches that lower bound, that is if \(_{ 0}_{}[_{,}]}{(1/ )} T_{}()\) (resp. \(T_{,}()=_{i_{}()}T_{ ,}(,i)\)). Notethat the expected sample complexity of an asymptotically \(1/2\)-optimal algorithm is at worst twice higher than that of any asymptotically optimal algorithm since \(T_{,1/2}() 2T_{}()\).

The asymptotic characteristic time \(T_{}()\) is of order \(_{i=1}^{K}\{^{-2},_{i}^{-2}\}\). It is computed as a minimum over all \(\)-good arms \(i_{}()\) of an arm-specific characteristic time, which can be interpreted as the time required to verify that \(i\) is \(\)-good. Each of the times \(_{(0,1)}T_{,}(,i)\) correspond to the complexity of a BAI instance (i.e. \(\)-BAI with \(=0\)) in which the mean of arm \(i\) is increased by \(\) (Lemma 9). Let \(w_{,}(,i)\) be the maximizer of (1). In , they show that \(T_{}()=T_{,^{*}(i^{*})}(,i^{*})\) and \(T_{,}()=T_{,}(,i^{*})\), where \(i^{*} i^{*}()\) and \(^{*}(i^{*})=_{(0,1)}T_{,}(,i^{*})\). For \(=0\), a similar lower bound to Lemma 1 holds for all \(\). Lower bounds of order \(_{i=1}^{K}_{i}^{-2}_{i}^{-2}\) (independent of \(\), but with a stronger dependence in the gaps) were also shown [17; 6; 38; 7]. Note that the characteristic time for \(\)-sub-Gaussian distributions (which does not have a form as "explicit" as (1)) is always smaller than the ones for Gaussian having the same means and variance \(^{2}\).

A good algorithm should have an expected sample complexity as close as possible to these lower bounds. Several algorithms for (\(\)-)BAI are based on modifications of the UCB algorithm [24; 17; 14]. Others compute approximate solutions to the lower bound maximization problem and sample arms in order to approach the solution [15; 11; 39]. Our method belongs to the family of Top Two algorithms [34; 36; 21], which select at each time two arms called leader and challenger, and sample among them. It is the first Top Two algorithm for the \(\)-BAI problem (for \(>0\)).

Any time and uniform \(\)-error boundIn addition to the fixed-confidence guarantees, we will prove a bound on the probability of error for any time \(n\) and any error \(\), similarly to the results of . That is, we bound \(_{}(i_{n}_{}())\) for all \(n\) and all \(\). This gives a bound on the probability of error in \(\)-BAI, and a bound on the simple regret of the sampling rule by integrating: \(_{}[_{}-_{i_{n}}]=_{}(i_{n} _{}())\,\).

The literature mostly focuses on the fixed budget setting, where the time \(T\) at which we evaluate the error probability is known and can be used as a parameter of the algorithm. Notable algorithms are successive rejects (SR, ) and sequential halving (SH, ). These algorithms can be extended to not depend on \(T\) by using a doubling trick [23; 41]. That trick considers a sequence of algorithms that are run with budgets \((T_{k})_{k}\), with \(T_{k+1}=2T_{k}\) and \(T_{1}=2K_{2}K\). Past observations are dropped when reaching \(T_{k}\), and the obtained recommendation is used until the budget \(T_{k+1}\) is reached.

### Contributions

We propose the EB-TC\({}_{_{0}}\) algorithm for identification in bandits, with a slack parameter \(_{0}>0\), originally motivated by \(_{0}\)-BAI. We study its combination with a stopping rule for fixed confidence \(\)-BAI (possibly with \(_{0}\)) and also its probability of error and simple regret at any time.

* EB-TC\({}_{_{0}}\) performs well empirically compared to existing methods, both for the expected sample complexity criterion in fixed confidence \(\)-BAI and for the anytime simple regret criterion. It is in addition easy to implement and computationally inexpensive in our regime.
* We prove upper bounds on the sample complexity of EB-TC\({}_{_{0}}\) in fixed confidence \(\)-BAI with sub-Gaussian distributions, both asymptotically (Theorem 1) as \( 0\) and for any \(\) (Theorem 2). In particular, EB-TC\({}_{}\) with \(>0\) is asymptotically optimal for \(\)-BAI with Gaussian distributions.
* We prove a uniform \(\)-error bound valid for any time for EB-TC\({}_{_{0}}\). This gives in particular a fixed budget error bound and a control of the expected simple regret of the algorithm (Theorem 3 and Corollary 1).

## 2 Anytime Top Two sampling rule

We propose an anytime Top Two algorithm, named EB-TC\({}_{_{0}}\) and summarized in Figure 1.

Recommendation ruleLet \(N_{n,i}:=_{t[n-1]}(I_{t}=i)\) be the number of pulls of arm \(i\) before time \(n\), and \(_{n,i}:=}_{t[n-1]}X_{t,I_{t}}(I_{t}=i)\) be its empirical mean. At time \(n>K\), we recommend the Empirical Best (EB) arm \(i_{n}_{i[K]}_{n,i}\) (where ties are broken arbitrarily).

### Anytime Top Two sampling rule

We start by sampling each arm once. At time \(n>K\), a Top Two sampling rule defines a leader \(B_{n}[K]\) and a challenger \(C_{n} B_{n}\). It then chooses the arm to pull among them. For the leader/challenger pair, we consider the Empirical Best (EB) leader \(B_{n}^{}=_{n}\) and, given a slack \(_{0}>0\), the Transportation Cost (TC\({}_{_{0}}\)) challenger

\[C_{n}^{_{0}}*{arg\,min}_{i B_{n}^{ }}^{}}-_{n,i}+_{0}}{,B_{n}^{}}+1/N_{n,i}}\,.\] (2)

The algorithm then needs to choose between \(B_{n}\) and \(C_{n}\). In order to do so, we use a so-called _tracking_ procedure . We define one tracking procedure per pair of leader/challenger \((i,j)[K]^{2}\) such that \(i j\), hence we have \(K(K-1)\) independent tracking procedures. For each pair \((i,j)\) of leader and challenger, the associated tracking procedure will ensure that the proportion of times the algorithm pulled the leader \(i\) remains close to a target average proportion \(_{n}(i,j)(0,1)\). At each round \(n\), only one tracking rule is considered, i.e. the one of the pair \((i,j)=(B_{n},C_{n})\).

We define two variants of the algorithm that differ in the way they set the proportions \(_{n}(i,j)\). _Fixed_ proportions set \(_{n}(i,j)=\) for all \((n,i,j)[K]^{2}\), where \((0,1)\) is fixed beforehand. Information-Directed Selection (_IDS_)  defines \(_{n}(i,j)=N_{n,j}/(N_{n,i}+N_{n,j})\) and sets \(_{n}(i,j):=T_{n}(i,j)^{-1}_{t[n-1]}((B_{t},C_ {t})=(i,j))_{t}(i,j)\) where \(T_{n}(i,j):=_{t[n-1]}((B_{t},C_{t})=(i,j))\) is the selection count of arms \((i,j)\) as leader/challenger.

Let \(N_{n,j}^{i}:=_{t[n-1]}((B_{t},C_{t})=(i,j),\ I_{t}=j)\) be the number of pulls of arm \(j\) at rounds in which \(i\) was the leader. We set \(I_{n}=C_{n}\) if \(N_{n,C_{n}}^{B_{n}}(1-_{n+1}(B_{n},C_{n}))T_{n+1}(B_{n},C_{n})\) and \(I_{n}=B_{n}\) otherwise. Using Theorem 6 in  for each tracking procedure (i.e. each pair \((i,j)\)) yields Lemma 2 (proved in Appendix H).

**Lemma 2**.: _For all \(n>K\), \(i[K]\), \(j i\), we have \(-1/2 N_{n,j}^{i}-(1-_{n}(i,j))T_{n}(i,j) 1\)._

The TC\({}_{_{0}}\) challenger seeks to minimize an empirical version of a quantity that appears in the lower bound for \(_{0}\)-BAI (Lemma 1). As such, it is a natural extension of the TC challenger used in the T3C algorithm  for \(_{0}=0\). In earlier works on Top Two methods , the choice between leader and challenger is randomized: given a fixed proportion \((0,1)\), set \(I_{n}=B_{n}\) with probability \(\), otherwise \(I_{n}=C_{n}\).  replaced randomization by tracking, and  proposed IDS to define adaptive proportions \(_{n}(B_{n},C_{n})(0,1)\). In this work we study both fixed proportions with \(=1/2\) and adaptive proportions with IDS. Empirically, we observe slightly better performances when using IDS (e.g. Figure 7 in Appendix J.2.1). While  tracked the leader with \(K\) procedures, we consider \(K(K-1)\) independent tracking procedures depending on the current leader/challenger pair.

Choosing \(_{0}\) shows that EB-TC (i.e. EB-TC\({}_{_{0}}\) with slack \(_{0}=0\)) suffers from poor empirical performance for moderate \(\) in BAI (see Appendix D.3 in  for a detailed discussion). Therefore, the choice of the slack \(_{0}>0\) is critical since it acts as a regularizer which naturally induces sufficient exploration. By setting \(_{0}\) too small, the EB-TC\({}_{_{0}}\) algorithm will become as greedy as EB-TC and perform poorly. Having \(_{0}\) too large will flatten differences between sub-optimal arms, hence it will behave more uniformly. We observe from the theoretical guarantees and from our experiments that it is best to take \(_{0}=\) for \(\)-BAI, but the empirical performance is only degrading slowly for \(_{0}>\). Taking \(_{0}<\) leads to very poor performance. We discuss this trade-off in more details in our experiments (e.g. Figures 5, 6 and 7 in Appendix J.2.1). When tackling BAI, the limitation of EB-TC can be solved by adding an implicit exploration mechanism in the choice of the leader/challenger

Figure 1: EB-TC\({}_{_{0}}\) algorithm with **fixed** or **IDS** proportions.

pair. For the choice of leader, we can use randomization (TS leader [34; 32; 36]) or optimism (UCB leader ). For the choice of the challenger, we can use randomization (RS challenger ) or penalization (TCI challenger , KKT challenger  or EI challenger ).

Anytime sampling ruleEB-TC\({}_{_{0}}\) is independent of a budget of samples \(T\) or a confidence parameter \(\). This anytime sampling rule can be regarded as a stream of empirical means/counts \((_{n},N_{n})_{n>K}\) (which could trigger stopping) and a stream of recommendations \(_{n}=i^{}(_{n})\). These streams can be used by agents with different kinds of objectives. The fixed-confidence setting couples it with a stopping rule to be \((,)\)-PAC. It can also be used to get an \(\)-good recommendation with large probability at any given time \(n\).

### Stopping rule for fixed-confidence \(\)-best-arm identification

In addition to the sampling and recommendation rules, the fixed-confidence setting requires a stopping rule. Given an error/confidence pair, the GLR\({}_{}\) stopping rule  prescribes to stop at the time

\[_{,}=\{n>K\ |\ _{i_{n}}_{n}}-_{n,i}+}{_{n} }+1/N_{n,i}}}\}_{n}=i^{}(_{n})\,\] (3)

where \(c:(0,1)_{+}\) is a threshold function. Lemma 3 gives a threshold ensuring that the GLR\({}_{}\) stopping rule is \((,)\)-PAC for all \( 0\) and \((0,1)\), independently of the sampling rule.

**Lemma 3** ().: _Let \( 0\) and \((0,1)\). Given any sampling rule, using the threshold_

\[c(n,)=2_{G}(((K-1)/)/2)+4(4+(n/2))\] (4)

_with the stopping rule (3) with error/confidence pair \((,)\) yields a \((,)\)-PAC algorithm for sub-Gaussian distributions. The function \(_{G}\) is defined in (23). It satisfies \(_{G}(x) x+(x)\)._

## 3 Fixed-confidence theoretical guarantees

To study \(\)-BAI in the fixed-confidence setting, we couple EB-TC\({}_{_{0}}\) with the GLR\({}_{}\) stopping rule (3) using error \( 0\), confidence \((0,1)\) and threshold (4). The resulting algorithm is \((,)\)-PAC by Lemma 3. We derive upper bounds on the expected sample complexity \(_{}[_{,}]\) both in the asymptotic regime of \( 0\) (Theorem 1) and for finite confidence when \(=_{0}\) (Theorem 2).

**Theorem 1**.: _Let \( 0\), \(_{0}>0\) and \((,)(0,1)^{2}\). Combined with GLR\({}_{}\) stopping (3), the EB-TC\({}_{_{0}}\) algorithm is \((,)\)-PAC and it satisfies that, for all \(^{K}\) with mean \(\) such that \(|i^{}()|=1\),_

\[_{ 0}_{}[_{ ,}]}{(1/)} T_{_{0}}()D_{ ,_{0}}() _{ 0}_{}[_{,}]}{(1/ )} T_{_{0},}()D_{,_{0}}(),\]

_where \(D_{,_{0}}()=(1+_{i i^{}}(_{0 }-)/(_{}-_{i}+))^{2}\)._

While Theorem 1 holds for all sub-Gaussian distributions, it is particularly interesting for Gaussian ones, in light of Lemma 1. When choosing \(=_{0}\) (i.e. \(D_{_{0},_{0}}()=1\)), Theorem 1 shows that EB-TC\({}_{_{0}}\) is asymptotically optimal for Gaussian bandits when using IDS proportions and asymptotically \(\)-optimal when using fixed proportions \(\). We also note that Theorem 1 is not conflicting with the lower bound of Lemma 1, as shown in Lemma 11 in Appendix C. Empirically we observe that the empirical stopping time can be drastically worse when taking \(_{0}<\), and close to the optimal one when \(_{0}>\) (Figures 5, 6 and 7 in Appendix J.2.1).

Until recently , proving asymptotic optimality of Top Two algorithms with adaptive choice \(\) was an open problem in BAI. In this work, we prove that their choice of IDS proportions also yield asymptotically optimal algorithms for \(\)-BAI. While the proof of Theorem 1 assumes the existence of a unique best arm, it holds for instances having sub-optimal arms with the same mean. This is an improvement compared to existing asymptotic guarantees on Top Two algorithms which rely on the assumption that the means of all arms are different [32; 36; 21]. The improvement is possible thanks to the regularization induced by the slack \(_{0}>0\).

While asymptotic optimality in the \(\)-BAI setting was already achieved for various algorithms (e.g. \(\)-Track-and-Stop (TaS) , Sticky TaS  or L\(\)BAI ), none of them obtained non-asymptotic guarantees. Despite their theoretical interest, asymptotic results provide no guarantee on the performance for moderate \(\). Furthermore, asymptotic results on Top Two algorithms require a unique best arm regardless of the considered error \(\): reaching asymptotic (\(\)-)optimality implicitly means that the algorithm eventually allocates samples in an optimal way that depends on the identity of the unique best arm, and that requires the unique best arm to be identified. As our focus is \(\)-BAI, our guarantees should only require that one of the \(\)-good arms is identified and should hold for instances having multiple best arms. The upper bound should scale with \(_{0}^{-2}\) instead of \(_{}^{-2}\) when \(_{}\) is small. Theorem 2 satisfies these requirements.

**Theorem 2**.: _Let \((0,1)\) and \(_{0}>0\). Combined with GLR\({}_{_{0}}\) stopping (3), the EB-TC\({}_{_{0}}\) algorithm with fixed \(=1/2\) is \((_{0},)\)-PAC and satisfies that, for all \(^{K}\) with mean \(\),_

\[_{}[_{_{0},}]_{ [0,_{0}]}\{T_{,_{0}}(,)+1,\;S_{,_{0}}()\}+2K^{2 }\;,\]

_where \(T_{,_{0}}(,)\) and \(S_{,_{0}}()\) are defined by_

\[T_{,_{0}}(,)=\{n n -1 2(1+)^{2}_{i_{}()}T_{ _{0},1/2}(,i)(+)^{2}\}\;,\] \[S_{,_{0}}()=h_{1}()}{a_{,_{0}}()}H_{, _{0}}(),\;)K^{2}}{a_{, _{0}}()}+1)\;,\] \[a_{,_{0}}()=_{}()}T_{_{0},1/2}(,i)}{_{i _{}()}T_{_{0},1/2}(,i)}\; _{i_{}(),j i}w_{_{0},1/ 2}(,i)_{j}\;,\]

_where \((0,1/2]\) is an analysis parameter and \(h_{1}(y,z) z+y(z+y(y))\) as in Lemma 51. \(T_{_{0},1/2}(,i)\) and \(w_{_{0},1/2}(,i)\) are defined in (1) and_

\[H_{,_{0}}():=()|}{ _{}()^{2}}+(|_{}()  i^{}()|)C_{,_{0}}()^{2}+ _{i_{}()}\{C_{,_{0} }(),_{i}^{-1}\}^{2}\;,\] (5)

_with \(_{}()=_{k_{}( )}_{k}\) and \(C_{,_{0}}()=\{2_{}()^{-1}-_{0}^{-1},_{0}^{-1}\}\)._

The upper bound on \(_{}[_{_{0},}]\) involves a \(\)-dependent term \(T_{,_{0}}(,)\) and a \(\)-independent term \(S_{,_{0}}()\). The choice of \(\) influences the compromise between those, and the infimum over \(\) means that our algorithm benefits from the best possible trade-off. In the asymptotic regime, we take \(=0\) and \( 0\) and we obtain \(_{ 0}_{}[_{_{0},}]/(1/ ) 2|i^{}()|T_{_{0},1/2}()\). When \(|i^{}()|=1\), we recover the asymptotic result of Theorem 1 up to a multiplicative factor \(2\). For multiple best arms, the asymptotic sample complexity is at most a factor \(2|i^{}()|\) from the \(\)-optimal one.

Given a finite confidence, the dominant term will be \(S_{,_{0}}()\). For \(=0\), we show that \(H_{,_{0}}(0)=(K\{_{},_{0} \}^{-2})\), hence we should consider \(>0\) to avoid the dependency in \(_{}\). For \(=_{0}\), there exists instances such that \(_{i_{_{0}}()}T_{_{0},1/2}(,i)\) is arbitrarily large, hence \(S_{,_{0}}(_{0})\) will be very large as well. The best trade-off is attained in the interior of the interval \((0,_{0})\). For \(=_{0}/2\), Lemma 10 shows that \(T_{_{0},1/2}(,i)=(K/_{0}^{2})\) for all \(i_{_{0}/2}()\) and \(H_{,_{0}}(_{0}/2)=(K/_{0}^{2})\). Therefore, we obtain an upper bound \((|_{_{0}/2}()|K_{0}^{-2} _{0}^{-1})\).

Likewise, Lemma 10 shows that \(_{j i}w_{_{0},1/2}(,i)_{j}(16(K-2)+2)^{-1}\) for all \(i_{_{0}/2}()\). While the dependency in \(a_{,_{0}}(_{0}/2)\) is milder in \(\)-BAI than in BAI (as it is bounded away from \(0\)), we can improve it by using a refined analysis (see Appendix E). Introduced in , this method allows to clip \(_{j i}w_{_{0},1/2}(,i)_{j}\) by a fixed value \(x[0,(K-1)^{-1}]\) for all \(i_{}()\).

Comparison with existing upper boundsThe LUCB algorithm  has a structure similar to a Top Two algorithm, with the differences that LUCB samples both the leader and the challenger and that it stops when the gap between the UCB and LCB indices is smaller than \(_{0}\). As LUCB satisfies \(_{}[_{_{0},}] 292H_{_{0}}()(H_{ _{0}}()/)+16\) where \(H_{_{0}}()=_{i}(\{_{i},_{0}/2\})^{-2}\), it enjoys better scaling than \(_{_{0}}\) for finite confidence. However, since the empirical allocation of LUCB is not converging towards \(w_{_{0},1/2}()\), it is not asymptotically \(1/2\)-optimal. While LUCB has better moderate confidence guarantees, there is no hope to prove anytime performance bounds since LUCB indices depends on \(\). In contrast, EB-TC\({}_{_{0}}\) enjoys such guarantees (see Section 4).

Key technical tool for the non-asymptotic analysisWe want to ensure that EB-TC\({}_{_{0}}\) eventually selects only \(\)-good arms as leader, for any error \( 0\). Our proof strategy is to show that if the leader is not an \(\)-good arm and empirical means do not deviate too much from the true means, then either the current leader or the current challenger was selected as leader or challenger less than a given quantity. We obtain a bound on the total number of times that can happen.

**Lemma 4**.: _Let \((0,1]\) and \(n>K\). Let \(T_{n}(i):=_{j i}(T_{n}(i,j)+T_{n}(j,i))\) be the number of times arm \(i\) was selected in the leader/challenger pair. Assume there exists a sequence of events \((A_{t}(n,))_{n U>K}\) and positive reals \((D_{i}(n,))_{i[K]}\) such that, for all \(t\{K+1,,n\}\), under the event \(A_{t}(n,)\),_

\[ i_{t}[K], T_{t}(i_{t}) D_{i_{t}}(n,)  T_{t+1}(i_{t})=T_{t}(i_{t})+1\;.\] (6)

_Then, we have \(_{t=K+1}^{n}(A_{t}(n,))_{i[K]}D_{i} (n,)\)._

To control the deviation of the empirical means and empirical gaps to their true value, we use a sequence of concentration events \((_{n,})_{n>T}\) defined in Lemma4 (AppendixG.2) such that \(_{}(_{n,}^{}) K^{2} n^{-s}\) where \(s 0\) and \((0,1]\). For the EB-TC\({}_{_{0}}\) algorithm with fixed \(=1/2\), we prove that, under \(_{n,}\), \(\{B_{t}^{}_{}()\}\) is a "bad" event satisfying the assumption of Lemma4. This yields Lemma5, which essentially says that the leader is an \(\)-good arm except for a logarithmic number of rounds.

**Lemma 5**.: _Let \((0,1]\), \(n>K\) and \( 0\). Under the event \(_{n,}\), we have_

\[_{i_{}()}_{j}T_{n}(i,j) n-1-8H_{, _{0}}()f_{2}(n,)-3K^{2}\;,\]

_where \(f_{2}(n,)=(1/)+(2+s) n\) and \(H_{,_{0}}()\) is defined in (5)._

Noticeably, Lemma5 holds for any \( 0\) even when there are multiple best arms. As expected the number of times the leader is not among the \(_{0}\)-good arms depends on \(H_{,_{0}}(_{0})=(K/_{0}^{2})\). The number of times the leader is not among the best arms depends on \(H_{,_{0}}(0)=(K(\{_{},_{0}\} )^{-2})\).

Time-varying slackTheorem1 shows the asymptotic optimality of the EB-TC\({}_{_{0}}\) algorithm with IDS for \(_{0}\)-BAI (where \(_{0}>0\)). To obtain optimality for BAI, we consider time-varying slacks \((_{n})_{n}\), where \((_{n})_{n}\) is decreasing, \(_{n}>0\) and \(_{n}_{+}0\). A direct adaptation of our asymptotic analysis on \(_{}[_{0,}]\) (see AppendixD), regardless of the choice of \((_{n})_{n}\), one can show that using GLR\({}_{0}\) stopping, the EB-TC\({}_{(_{n})_{n}}\) algorithm with IDS is \((0,)\)-PAC and is asymptotically optimal in BAI. Its empirical performance is however very sensitive to the choice of \((_{n})_{n}\) (AppendixJ.2.3).

## 4 Beyond fixed-confidence guarantees

Could an algorithm analyzed in the fixed-confidence setting be used for the fixed-budget or even anytime setting? This question is especially natural for EB-TC\({}_{_{0}}\), which does not depend on the confidence parameter \(\). Yet its answer is not obvious, as it is known that algorithms that have _optimal_ asymptotic guarantees in the fixed-confidence setting can be sub-optimal in terms of error probability. Indeed  prove in their AppendixC that for any asymptotically optimal (exact) BAI algorithm, there exists instances in which the error probability cannot decay exponentially with the horizon, which makes them worse than the (minimax optimal) uniform sampling strategy .

Their argument also applies to \(\)-optimal algorithms, hence to EB-TC\({}_{0}\) with \(=1/2\). However, whenever \(_{0}\) is positive, Theorem3 reveals that the error probability of EB-TC\({}_{0}\) always decays exponentially, which redeems the use of optimal fixed-confidence algorithms for a relaxed BAI problem in the anytime setting. Going further, this result provides an anytime bound on the probability to recommend an arm that is not \(\)-optimal, for any error \( 0\). This bound involves instance-dependent complexities depending solely on the gaps in \(\). To state it, we define \(C_{}:=[\{_{i} i[K]\}]\) as the number of distinct arm means in \(\) and let \(_{}(i):=\{k[K]_{}-_{k}=_{i}\}\) be the set of arms having mean gap \(_{i}\) where the gaps are sorted by increasing order \(0=_{1}<_{2}<<_{C_{}}\). For all \( 0\), let \(i_{}()=i\) if \([_{i},_{i+1})\) (with the convention \(_{C_{}+1}=+\)). Theorem3 shows that the exponential decrease of \(_{}(_{i}_{}())\) is linear.

**Theorem 3**.: _(see Theorem6 in AppendixF) Let \(_{0}>0\). The EB-TC\({}_{_{0}}\) algorithm with fixed proportions \(=1/2\) satisfies that, for all \(^{K}\) with mean \(\), for all \( 0\), for all \(n>5K^{2}/2\),_

\[_{}(i_{n}_{}()) K^{2 }e^{2}(2+ n)^{2}(-p(/2}{8H_{i_{}( )}(,_{0})}))\;.\]

_where \(p(x)=x- x\) and \((H_{i}(,_{0}))_{i[C_{}-1]}\) are such that \(H_{1}(,_{0})=K(2_{}^{-1}+3_{0}^{-1})^{2}\) and \(K/_{i+1}^{-2} H_{i}(,_{0}) K_{j[i]}\{2 _{j+1}^{-1},\;2/_{0}+1}{_{i+1}-_{ j}}+3_{0}^{-1}\}^{2}\) for all \(i>1\)._This bound can be compared with the following uniform \(\)-error bound of the strategy using uniform sampling and recommending the empirical best arm:

\[(_{n}^{}_{}() )_{i_{}()}(-^{2} n/K}{4}) K(-()+1}^{-2}})\]

Recalling that the quantity \(H_{i}(,_{0})\) in Theorem3 is always bounded from below by \(2K_{i+1}^{-1}\), we get that our upper bound is larger than the probability of error of the uniform strategy, but the two should be quite close. For example for \(=0\), we have

\[_{}(_{n} i^{}())(- ^{-1}+_{0}^{-1})^{2}} )\,,\ _{}(_{n}^{} i^{}()) (-^{-2}}).\]

Even if they provide a nice sanity-check for the use of a sampling rule with optimal fixed-confidence guarantees for \(_{0}\)-BAI in the anytime regime, we acknowledge that these guarantees are far from optimal. Indeed, the work of  provides tighter anytime uniform \(\)-error probability bounds for two algorithms: an anytime version of Sequential Halving  using a doubling trick (called DSH), and an algorithm called Bracketting Sequential Halving, that is designed to tackle a very large number of arms. Their upper bounds are of the form \(_{}(_{n}_{}()) (-(n/H()))\) with \(H()=_{i g()+1}^{2}}\) where \(g()=|\{i[K]_{i}^{}-\}|\). Therefore, they can be much smaller than \(K_{i_{}()+1}^{-2}\).

The BUCB algorithm of  is also analyzed for any level of error \(\), but in a different fashion. The authors provide bounds on its \((,)\)-_invertifiable sample complexity_, defined as the expectation of the smallest stopping time \(\) satisfying \(( t,_{n}_{}( )) 1-\). This notion is different from the sample complexity we use in this paper, which is sometimes called _verifiable_ since it is the time at which the algorithm can guarantee that its error probability is less than \(\). Interestingly, to prove Theorem3 we first prove a bound on the unverifiable sample complexity of \(_{_{0}}\), which is valid for all \((,)\), neither of which are parameters of the algorithm. More precisely, we prove that \(_{}( n>U_{i_{}(),}(, _{0}),\,_{n}_{}()) 1-\) for \(U_{i,}(,_{0})= n_{i}(,_{0})(1/ )+((1/))\). As this statement is valid for all \((0,1)\), applying it for each \(n\) to \(_{n}\) such that \(U_{i_{}(),_{n}}(,_{0})=n\), we obtain Theorem3. We remark that such a trick cannot be applied to BUCB to get uniform \(\)-error bounds for any time, as the algorithm does depend on \(\).

Simple regretAs already noted by , uniform \(\)-error bounds easily yield simple regret bounds. We state in Corollary1 the one obtained for \(_{_{0}}\). As a motivation to derive simple regret bounds, we observe that they readily translate to bounds on the cumulative regret for an agent observing the stream of recommendations \((_{n})\) and playing arm \(_{n}\). An exponentially decaying simple regret leads to a constant cumulative regret in this decoupled exploration/exploitation setting .

**Corollary 1**.: _Let \(_{0}>0\). Let \(p(x)\) and \((H_{i}(,_{0}))_{i[C_{}-1]}\) be defined as in Theorem3. The \(_{_{0}}\) algorithm with fixed \(=1/2\) satisfies that, for all \(^{K}\) with mean \(\), for all \(n>5K^{2}/2\),_

\[_{}[_{}-_{i_{n}}] K^{2}e^{2}(2+ n)^{2}_{i [C_{}-1]}(_{i+1}-_{i})(-p(/2}{8H _{i}(,_{0})}))\.\]

Following the discussion above, this bound is not expected to be state-of-the-art, it rather justifies that \(_{_{0}}\) with \(_{0}>0\) is not too much worse than the uniform sampling strategy. Yet, as we shall see in our experiments, the practical story is different. In Section5, we compare the simple regret of \(_{_{0}}\) to that of DSH in synthetic experiments with a moderate number of arms, revealing the superior performance of \(_{_{0}}\).

## 5 Experiments

We assess the performance of the \(_{_{0}}\) algorithm on Gaussian instances both in terms of its empirical stopping time and its empirical simple regret, and we show that it perform favorably compared to existing algorithms in both settings. For the sake of space, we only show the results for large sets of arms and for a specific instance with \(|i^{}()|=2\).

Empirical stopping timeWe study the impact of large sets of arms (up to \(K=1000\)) in \(\)-BAI for \((,)=(0.1,0.01)\) on the "\(=0.3\)" scenario of  which sets \(_{i}=1-((i-1)/(K-1))^{}\) for all \(i[K]\). EB-TC\({}_{_{0}}\) with IDS and slack \(_{0}=\) is compared to existing \(\)-BAI algorithms having low computational cost. This precludes algorithms such as \(\)-Track-and-Stop (TaS) , Sticky TaS  or \(\)-BAI adaptation of FWS  and DKM . In Appendix J.2.2, we compare EB-TC\({}_{}\) to those algorithms on benchmarks with smaller number of arms. We show that EB-TC\({}_{}\) performs on par with \(\)-TaS and \(\)-FWS, but outperforms \(\)-DKM. As Top Two benchmarks with fixed \(=1/2\), we consider T3C , EB-TCI  and TTUCB . To provide a fair comparison, we adapt them to tackle \(\)-BAI by using the stopping rule (3) and by adapting their sampling rule to use the TC\(\) challenger from (2) (with a penalization \( N_{n,i}\) for EB-TCI). We use the heuristic threshold \(c(n,)=((1+ n)/)\). While it is too small to ensure the \((,)\)-PAC property, it still yields an empirical error which is several orders of magnitude lower than \(\). Finally, we compare with LUCB  and uniform sampling. For a fair comparison, LUCB uses \(}\) as bonus, which is also too small to yield valid confidence intervals. Our results are averaged over \(100\) runs, and the standard deviations are displayed. In Figure 2(a), we see that EB-TC\({}_{}\) performs on par with the \(\)-T3C heuristic, and significantly outperforms the other algorithms. While the scaling in \(K\) of \(\)-EB-TCI and LUCB appears to be close to the one of EB-TC\({}_{}\), \(\)-TTUCB and uniform sampling obtain a worse one. Figure 2(a) also reveals that the regularization ensured by the TC\(\) challenger is sufficient to ensure enough exploration, hence other exploration mechanisms are superfluous (TS/UCB leader or TCI challenger).

Anytime empirical simple regretThe EB-TC\({}_{_{0}}\) algorithm with fixed \(=1/2\) and \(_{0}=0.1\) is compared to existing algorithms on the instance \(=(0.6,0.6,0.55,0.45,0.3,0.2)\) from , which has two best arms. As benchmark, we consider Doubling Successive Reject (DSR) and Doubling Sequential Halving (DSH), which are adaptations of the elimination based algorithms SR  and SH . SR eliminates one arm with the worst empirical mean at the end of each phase, and SH eliminated half of them but drops past observations between each phase. These doubling-based algorithms have empirical error decreasing by steps: they change their recommendation only before they restart. In Figure 2(b), we plot the average of the simple regret over \(10000\) runs and the standard deviation of that average (which is too small to see clearly). We observe that EB-TC\({}_{_{0}}\) outperforms uniform sampling, as well as DSR and DSH, which both perform worse due to the dropped observations. The favorable performance of EB-TC\({}_{_{0}}\) is confirmed on other instances from , and for "two-groups" instances with varying \(|i^{}()|\) (see Figures 10 and 12).

Supplementary experimentsExtensive experiments and implementation details are available in Appendix J. In Appendix J.2.1, we compare the performance of EB-TC\({}_{_{0}}\) with different slacks \(_{0}\) for IDS and fixed \(=1/2\). In Appendix J.2.2, we demonstrate the good empirical performance of EB-TC\({}_{_{0}}\) compared to state-of-the art methods in the fixed-confidence \(\)-BAI setting, compared to DSR and DSH for the empirical simple regret, and compared to SR and SH for the probability of \(0\)-error in the fixed-budget setting (Figure 13). We consider a wide range of instances: random ones, benchmarks from the literature  and "two-groups" instances with varying \(|i^{}()|\).

Figure 2: (a) Empirical stopping time on “\(=0.3\)” instances for varying \(K\) and stopping rule (3) using \((,)=(0.1,0.01)\). The BAI algorithms T3C, EB-TCI and TTUCB are modified to be \(\)-BAI ones. (b) Empirical simple regret on instance \(=(0.6,0.6,0.55,0.45,0.3,0.2)\), in which EB-TC\({}_{_{0}}\) with slack \(_{0}=0.1\) and fixed \(=1/2\) is used.

Perspectives

We have proposed the EB-TC\({}_{_{0}}\) algorithm, which is easy to understand and implement. EB-TC\({}_{_{0}}\) is the first algorithm to be simultaneously asymptotically optimal in the fixed-confidence \(_{0}\)-BAI setting (Theorem 1), have finite-confidence guarantees (Theorem 2), and have also anytime guarantees on the probability of error at any level \(\) (Theorem 3), hence on the simple regret (Corollary 1). Furthermore, we have demonstrated that the EB-TC\({}_{_{0}}\) algorithm achieves superior performance compared to other algorithms, in benchmarks where the number of arms is moderate to large. In future work, we will investigate its possible adaptation to the data-poor regime of  in which the number of arms is so large that any algorithm sampling each arm once is doomed to failure.

While our results hold for general sub-Gaussian distributions, the EB-TC\({}_{_{0}}\) algorithm with IDS and slack \(_{0}>0\) only achieves asymptotic optimality for \(_{0}\)-BAI with Gaussian bandits. Even though IDS has been introduced by  for general single-parameter exponential families, it is still an open problem to show asymptotic optimality for distributions other than Gaussians. While our non-asymptotic guarantees on \(_{}[_{_{0},}]\) and \(_{}[_{}-_{i_{n}}]\) were obtained for the EB-TC\({}_{_{0}}\) algorithm with fixed \(=1/2\), we observed empirically better performance when using IDS. Deriving similar (or better) non-asymptotic guarantees for IDS is an interesting avenue for future work.

Finally, the EB-TC\({}_{_{0}}\) algorithm is a promising method to tackle structured bandits. Despite the existence of heuristics for settings such as Top-k identification , it is still an open problem to efficiently adapt Top Two approaches to cope for other structures such as \(\)-BAI in linear bandits.