# Neural Conditional Probability for Uncertainty Quantification

Vladimir R. Kostic\({}^{1,2}\)   Karim Lounici\({}^{3}\)   Gregoire Parceau\({}^{3}\)

Giacomo Turri\({}^{1}\)   Pietro Novelli\({}^{1}\)   Massimiliano Pontil\({}^{1,4}\)

\({}^{1}\)CSML, Istituto Italiano di Tecnologia  \({}^{2}\)University of Novi Sad

\({}^{3}\)CMAP-Ecole Polytechnique  \({}^{4}\)AI Centre, University College London

###### Abstract

We introduce Neural Conditional Probability (NCP), an operator-theoretic approach to learning conditional distributions with a focus on statistical inference tasks. NCP can be used to build conditional confidence regions and extract key statistics such as conditional quantiles, mean, and covariance. It offers streamlined learning via a single unconditional training phase, allowing efficient inference without the need for retraining even when conditioning changes. By leveraging the approximation capabilities of neural networks, NCP efficiently handles a wide variety of complex probability distributions. We provide theoretical guarantees that ensure both optimization consistency and statistical accuracy. In experiments, we show that NCP with a 2-hidden-layer network matches or outperforms leading methods. This demonstrates that a a minimalistic architecture with a theoretically grounded loss can achieve competitive results, even in the face of more complex architectures.

## 1 Introduction

This paper studies the problem of estimating the conditional distribution associated with a pair of random variables, given a finite sample from their joint distribution. This problem is fundamental in machine learning, and instrumental for various purposes such as building prediction intervals, performing downstream analysis, visualizing data, and interpreting outcomes. This entails predicting the probability of an event given certain conditions or variables, which is a crucial task across various domains, ranging from finance (Markowitz, 1958) to medicine (Ray et al., 2017), to climate modeling (Harrington, 2017) and beyond. For instance, in finance, it is essential for risk assessment to estimate the probability of default given economic indicators. Similarly, in healthcare, predicting the likelihood of a disease, given patient symptoms, aids in diagnosis. In climate modeling, estimating the conditional probability of extreme weather events such as hurricanes or droughts, given specific climate indicators, helps in disaster preparedness and mitigation efforts.

According to Gao and Hastie (2022), there exist four main strategies to learn the conditional distribution. The first one relies on the Bayes formula for densities and proposes to apply non-parametric statistics to learn the joint and marginal densities separately. However, most of non-parametric techniques face a significant challenge known as the curse of dimensionality (Scott, 1991; Nagler and Czado, 2016). The second strategy, also known as Localization method, involves training a model unconditionally on reweighted samples, where weights are determined by their proximity to the desired conditioning point (Hall et al., 1999; Yu and Jones, 1998). These methods require retraining the model whenever the conditioning changes and may also suffer from the curse of dimensionality if the weighting strategy treats all covariates equally. The third strategy, known as Direct Learning of the conditional distribution involves finding the best linear approximation of the conditional density on a dictionary of base functions or a kernel space (Sugiyama et al., 2010; Li et al., 2007). Theperformance of these methods relies crucially on the selection of bases and kernels. Again for high-dimensional settings, approaches that assign equal importance to all covariates may be less effective. Finally, the fourth strategy, known as Conditional Training, involves training models to estimate a target variable conditioned on certain covariates. This is typically based on partitioning the covariates space \(\) into sets, followed by training models unconditionally within each partition (see Gao and Hastie, 2022; Winkler et al., 2020; Lu and Huang, 2020; Dhariwal and Nichol, 2021, and references therein). However, this strategy requires a large dataset to provide enough samples for each conditioning and is expensive as it requires training separate models for each conditioning input set, even though they stem from the same underlying joint distribution.

ContributionsThe principal contribution of this work is a different conditional probability approach that does not fall into any of the four aforementioned strategies. Rather than learning the conditional density directly, our method, called Neural Conditional Probability (NCP), aims to learn the _conditional expectation operator_\(_{Y|X}\) associated to the random variables \(X\) and \(Y\) based on data from their joint distribution. The operator is defined, for every measurable function \(f:\), as

\[[_{Y|X}f](x):=[f(Y)\,|\,X=x].\]

NCP is based on a principled loss, leveraging the connection between conditional expectation operators and deepCCA (Andrew et al., 2013) established in (Kostic et al., 2024), and can be used interchangeably to:

* retrieve the conditional density \(p_{Y|X}\) with respect to marginal distributions of \(X\) and \(Y\);
* compute conditional statistics \([f(Y)\,|\,X]\) for arbitrary functions \(f:\), including conditional mean, variance, moments, and the conditional cumulative distribution function, thereby providing access to all conditional quantiles simultaneously;
* estimate the conditional probabilities \([Y B\,|\,X A]\) for arbitrary sets \(B\) and \(A\) with theoretical non-asymptotic guarantees on accuracy, allowing us to easily construct conditional confidence regions.

Notably, our approach extracts statistics directly from the trained operator without retraining or resampling, and it is supported by both optimization consistency and statistical guarantees. In addition our experiments show that our approach matches or exceeds the performance of leading methods, even when using a basic a 2-hidden-layer network. This demonstrates the effectiveness of a minimalistic architecture combined with a theoretically grounded loss function.

Paper organizationIn Section 2 we review related work. Section 3 introduces the operator theoretic approach to model conditional expectation, while Section 4 discusses its training pipeline. In Section 5, we derive learning guarantees for NCP. Finally, Section 6 presents numerical experiments.

## 2 Related works

Non-parametric estimators are valuable for density and conditional density estimation as they don't rely on specific assumptions about the density being estimated. Kernel estimators, pioneered by Parzen (1962) and Rosenblatt (1956), are a widely used non-parametric density estimation method. Much effort has been dedicated to enhancing kernel estimation, focusing on aspects like bandwidth selection (Goldenshluger and Lepski, 2011), non-linear aggregation (Rigollet and Tsybakov, 2007), and computational efficiency (Langrene and Warin, 2020), as well as extending it to conditional densities (Bertin et al., 2014). A comprehensive review of kernel estimators and their variants is provided in (Silverman, 2017). See also (Tsybakov, 2009) for a statistical analysis of their performance. However, most of non-parametric techniques face a significant challenge known as the curse of dimensionality (Scott, 1991; Nagler and Czado, 2016), meaning that the required sample size for accurate estimation grows exponentially with the dimensionality of the data (Silverman, 2017). Additionally, the computational complexity also increases exponentially with dimensionality (Langrene and Warin, 2020).

Examples of localization methods include the work by Hall et al. (1999) for conditional CDF estimation using local logistic regression and locally adjusted Nadaraya-Watson estimation, as well as conditional quantiles estimation via local pinball loss minimization in (Yu and Jones, 1998). Examples of direct learning of the conditional distribution include (Sugiyama et al., 2010) via decomposition on a dictionary of base functions. Similarly, Li et al. (2007) explores quantile regression in reproducing Hilbert kernel spaces.

Conditional training is a popular approach which was adopted in numerous works, as in the recent work by Gao and Hastie (2022) where a parametric exponential model for the conditional density \(p_{}(y|x)\) is trained using the Lindsey method within each bin of a partition of the space \(\). This strategy has also been implemented in several prominent classes of generative models, including Normalizing Flow (NF) and Diffusion Models (DM) (Tabak and Vanden-Eijnden, 2010; Dinh et al., 2014; Rezende and Mohamed, 2015; Sohl-Dickstein et al., 2015). These models work by mapping a simple probability distribution into a more complex one. Conditional training approaches for NF and DM have been developed in many works including (e.g. Winkler et al., 2020; Lu and Huang, 2020; Dhariwal and Nichol, 2021). In efforts to lower the computational burden of conditional diffusion models, an alternative approach used heuristic approximations applied directly to unconditional diffusion models on computer vision related tasks (see e.g. Song et al., 2023; Zhang et al., 2023). However, the effectiveness of these heuristics in accurately mimicking the true conditional distributions remains uncertain. Another crucial aspect of these classes of generative models is that while the probability distribution is modelled explicitly, the computation of any relevant statistic, say \([Y|X]\) is left as an implicit problem usually solved by sampling from \(p_{}(y|x)\) and then approximating \([Y|X]\) via simple Monte-Carlo integration. As expected, this approach quickly becomes problematic as the dimension of the output space \(\) becomes large.

Conformal Prediction (CP) is a popular model-agnostic framework for uncertainty quantification (Vovk et al., 1999). Conditional Conformal Prediction (CCP) was later developed to handle conditional dependencies between variables, allowing in principle for more accurate and reliable predictions (see Lei and Wasserman, 2014; Romano et al., 2019; Chernozhukov et al., 2021; Gibbs et al., 2023, and the references cited therein). However, (CP) and (CCP) are not without limitations. The construction of these guaranteed prediction regions need to be recomputed from scratch for each value of the confidence level parameter and of the conditioning for (CCP). In addition, the produced confidence regions tend to be conservative.

## 3 Operator approach to probability modeling

Consider a pair of random variables \(X\) and \(Y\) taking values in probability spaces \((,_{},)\) and \((,_{},)\), respectively, where \(\) and \(\) are state spaces, \(_{}\) and \(_{}\) are sigma algebras, and \(\) and \(\) are probability measures. Let \(\) be the joint probability measure of \((X,Y)\) from the product space \(\). We assume that \(\) is absolutely continuous w.r.t. to the product measure of its marginals, that is \(\), and denote the corresponding density by \(p=d/d()\), also called point-wise dependency in Tsai et al. (2020), so that \((dx,dy)=p(x,y)(dx)(dy)\).

The principal goal of this paper is, given a dataset \(_{n}:=(x_{i},y_{i})_{i[n]}\) of observations of \((X,Y)\), to estimate the conditional probability measure

\[p(B\,|\,x):=[Y B\,|\,X=x], x,\,B_{ }.\] (1)

Our approach is based on the simple fact that \(p(B\,|\,x)=[}_{B}(Y)\,|\,X=x]\), where \(}_{B}\) denotes the characteristic function of set \(B\). More broadly we address the above problem by studying the conditional expectation operator \(_{Y|X} L^{2}_{}() L^{2}_{}()\), which is defined, for every \(f L^{2}_{}()\) and \(x\), as

\[[_{Y|X}f](x):=[f(Y)\,|\,X=x]=_{}f(y)p(dy\, |\,x)=_{}f(y)p(x,y)(dy),\]

where \(L^{2}_{}()\) and \(L^{2}_{}()\) denotes the Hilbert spaces of functions that are square integrable w.r.t. to \(\) and \(\), respectively. One readily verifies that \(\|_{Y|X}\|=1\) and \(_{Y|X}}_{}=}_{}\).

A prominent feature of the above operator is that its rank can reveal the independence of the random variables. That is, \(X\) and \(Y\) are independent random variables if and only if \(_{Y|X}\) is a rank one operator, in which case we have that \(_{Y|X}=}_{}}_{}\). It is thus useful to consider the deflated operator \(_{Y|X}=_{Y|X}-}_{} }_{}\,L^{2}_{}()  L^{2}_{}()\), for which we have that

\[[_{Y|X}f](x)=[f(Y)]+[_{Y|X}f](x), f L^{ 2}_{}().\] (2)

For dependent random variables, the deflated operator is nonzero. In many important situations, such as when the conditional probability distribution is a.e. absolutely continuous w.r.t. to the target measure, that is \(p(\,|\,x)\) for \(\)-a.e. \(x\), the operator \(_{Y|X}\) is compact, and, hence, we can write the SVD of \(_{Y|X}\) and \(_{Y|X}\) respectively as

\[_{Y|X}=_{i=0}^{}_{i}^{}\,u_{i}^{} v _{i}^{},_{Y|X}=_{i=1}^{}_{i}^{}\,u_{i}^{ } v_{i}^{},\] (3)

where the left \((u_{i}^{})_{i}\) and right \((v_{i}^{})_{i}\) singular functions form complete orthonormal systems of \(L^{2}_{}()\) and \(L^{2}_{}()\), respectively. Notice that the only difference in the SVD of \(_{Y|X}\) and \(_{Y|X}\) is the extra leading singular triplet \((_{0}^{},u_{0}^{},v_{0}^{})=(1,_{},_{})\) of \(_{Y|X}\). In terms of densities, the SVD of \(_{Y|X}\) leads to the characterization

\[p(x,y)=_{i=0}^{}_{i}^{}\,u_{i}^{}(x)\,v_{i}^{}( y)=1+_{i=1}^{}_{i}^{}\,u_{i}^{}(x)\,v_{i}^{}(y).\]

The mild assumption that \(_{Y|X}\) is a compact operator allows one to approximate it arbitrarily well with a (large enough) finite rank (empirical) operator. Choosing the operator norm as the measure of approximation error and appealing to the Eckart-Young-Mirsky Theorem (see Theorem 3 in Appendix B.1) one concludes that the best approximation is given by the truncated SVD, that is for every \(d\),

\[_{Y|X}_{Y|X}_{d}:=_{i=1}^{ d}_{i}^{}\,u_{i}^{} v_{i}^{},_{Y|X}_{d}_{(A ) d}\|_{Y|X}-A\|,\] (4)

where the minimum is given by \(_{d}^{}\), and the minimizer is unique whenever \(_{d+1}^{}<_{d}^{}\). This leads to the approximation of the joint density w.r.t. marginals \(p(x,y) 1+_{i=1}^{d}_{i}^{}\,u_{i}^{}(x)\,v_{i}^{ }(y)\), so that

\[[f(Y)\,|\,X=x][f(Y)]+_{i=1}^{d}_{i}^{ }\,u_{i}^{}(x)[f(Y)\,v_{i}^{}(Y)],\] (5)

which in particular, choosing \(f=_{B}\), gives

\[[Y B\,|\,X=x][Y B]+_{i=1}^{d}_{i}^ {}\,u_{i}^{}(x)\,[v_{i}^{}(Y)\,_{B}(Y)].\]

Moreover, we have that

\[[Y B\,|\,X A]=_{A},_{Y|X} _{B}}{[X A]}[Y B]+_{i=1 }^{d}\,_{i}^{}\,[u_{i}^{}(X)_{A}(X) ]}{[X A]}\,[v_{i}^{}(Y)\,_{B}(Y)],\]

for which the approximation error is bounded in the following lemma.

**Lemma 1** (Approximation bound).: _For any \(A_{}\) such that \([X A]>0\) and any \(B_{}\),_

\[|[Y B\,|\,X A]-[Y B]-_{A},_{Y|X}_{d}_{B}}{[X A]}|_{d+1}^{}\,[Y B]}{ [X A]}}.\] (6)

Neural network modelInspired by the above observations, to build the NCP model, we will parameterize the truncated SVD of the conditional expectation operator and then learn it. Specifically, we introduce two parameterized embeddings \(u^{}^{d}\) and \(v^{}^{d}\), and the singular values parameterized by \(w^{}^{d}\), respectively given by

\[u^{}(x){:=}[u_{1}^{}(x)\,\,u_{d}^{}(x)]^{},\ v^{ }(y){:=}[v_{1}^{}(y)\,\,v_{d}^{}(y)]^{},^{}{:=}[e^{-(w_{1}^{})^{2}},,e^{-(w_{d}^{})^{2}}]^{ },\]

where the parameter \(\) takes values in a prescribed set \(\).

We then aim to learn the joint density function \(p(x,y)\) in the (separable) form

\[p_{}(x,y):=1+_{i[d]}_{i}^{}u_{i}^{}(x)\,v_{i}^{ }(y)=1+^{} u^{}(x),v^{}(y),\]

where \(\) denotes element-wise product. One of the prominent losses considered for the task of learning \(p^{2}_{}\) is _the least squares density ratio_ loss \(_{}(p-p_{})^{2}-_{}p=_{ }p_{}^{2}-2_{}p_{}\), c.f. Tsai et al. (2020), also considered by HaoChen et al. (2022) in the specific context of augmentation graph in self-supervised deep learning, linked to kernel embeddings (Wang et al., 2022), and rediscovered and tested on DeepCCA tasks by Wells et al. (2024). Here, following the operator perspective, we use the characterization (4) of the optimal finite rank model to propose a new loss that: (1) excludes the known feature from the learning process, and (2) introduces a penalty term to enforce orthonormality of the basis functions. More precisely, our loss \(_{}():=()+()\) is composed of two terms. The first one

\[():=_{}p_{}^{2}-2_{}p _{}+[_{}p_{}]^{2}-_{}[_{}p_{}]^{2}-_{}[_{}p_{}]^{2}+2 _{}p_{}\] (7)is equivalent to solving (4) with \(A=_{i=1}^{d}_{i}^{}[u_{i}^{}-_{}u_{i}^{ }][v_{i}^{}-_{}v_{i}^{}]\) and can be written in terms of correlations between features. Namely, denoting the covariance and variance matrices by

\[[z,z^{}]:=[(z-[z])(z^{}- [z^{}])^{}][z]:=[(z-[z])(z-[z])^{}],\] (8)

and abbreviating \(u^{}:=u^{}(X)\) and \(v^{}:=v^{}(Y)\) for simplicity, we can write

\[():=([} u ^{}]\,[} v^{}]-2\,[} u^{},} v^{ }]).\] (9)

If \(p{=}p_{}\) for some \(\), then the optimal loss is the \(^{2}\)-divergence \((){=}D_{^{2}}(\,|\,){=}{-}_{i 1 }{_{i}^{*}}^{2}\) and, as we show below, \(()\) measures how well \(p_{}(x,y)-1\) approximates \(_{i[d]}_{i}^{*}\,u_{i}^{*}(x)\,v_{i}^{*}(y)\). However, in order to obtain a useful probability model, it is of paramount importance to _align_ the metric in the latent spaces with the metrics in the data-spaces \(L_{}^{2}()\) and \(L_{}^{2}()\). For different reasons, a similar phenomenon has been observed in Kostic et al. (2024) where dynamical systems are learned via transfer operators. In our setting, this leads to the second term of the loss that measures how well features \(u^{}\) and \(v^{}\) span relevant subspaces in \(L_{}^{2}()\) and \(L_{}^{2}()\), respectively. Namely, aiming \([u_{i}^{}(X)u_{j}^{}(X)]=[v_{i}^{}(Y)v_{j }^{}(Y)]=_{\{i=j\}}\), \(i,j\{0,1,,d\}\) leads to

\[(){:=}\|[u^{}(X)u^{}(X)^{}]{-}I\|_ {F}^{2}{+}\|[v^{}(Y)v^{}(Y)^{}]{-}I\|_{F}^{2}{+}\| [u^{}(X)]\|^{2}{+}2\|[v^{}(Y)\|^{2}.\] (10)

We now state our main result on the properties of the loss \(_{}\), which extends the result in Wells et al. (2024) to infinite-dimensional operators and guarantees the uniqueness of the optimum due to \(\).

**Theorem 1**.: _Let \(_{Y|X} L_{}^{2}() L_{}^{2}()\) be a compact operator and \(_{Y|X}=_{i=1}^{}_{i}^{}u_{i}^{} v_ {i}^{}\) be the SVD of its deflated version. If \(u_{i}^{} L_{}^{2}()\) and \(v_{i}^{} L_{}^{2}()\), for all \(\) and \(i[d]\), then for every \(\), \(_{}()-_{i[d]}_{i}^{*2}\). Moreover, if \(>0\) and \(_{d}^{}>_{d+1}^{}\), then the equality holds if and only if \((_{i}^{},u_{i}^{},v_{i}^{})\) equals \((_{i}^{},u_{i}^{},v_{i}^{})\)\(\)-a.e., up to unitary transform of singular spaces._

We provide the proof in Appendix B.3. In the following section, we show how to learn these canonical features from data and construct approximations of the conditional probability measure.

Comparison to previous methodsNCP does not fall into any of the four categories defined by Gao and Hastie (2022), as it does not aim to learn conditional density of \(Y|X\) directly. Instead, NCP focuses on learning the operator mapping \(L_{}^{2}() L_{}^{2}()\), from which all relevant task-specific statistics can be derived without requiring retraining. This approach effectively integrates with deep representation learning to create a latent space adapted to \(p(y|x)\). As a result, NCP efficiently captures the intrinsic dimension of the data, which is supported by our theoretical guarantees that depend solely on the latent space dimension (Theorem 2). In contrast, strategies designed for learning density often encounter significant limitations, such as the curse of dimensionality, potential substantial misrepresentation errors when the pre-specified function dictionary misaligns with the true distribution \(p(y|x)\), and high computational complexity due to the need for retraining. Experiments confirm NCP's capability to learn representations tailored to a wide range of data types--including manifolds, graphs, and high-dimensional distributions--without relying on predefined dictionaries. This flexibility allows NCP to outperform popular aforementioned methods.

## 4 Training the NCP inference method

In this section, we discuss how to train the model. Given a training dataset \(_{n}=(x_{i},y_{i})_{i[n]}\) and networks \((u^{},v^{},^{})\), we consider the empirical loss \(}_{}():=}()+ }()\), where we replaced (9) and (10) by their empirical versions. In order to guarantee the unbiased estimation, as we show within the proof of Theorem 1, two terms of our loss can be written using two independent samples \((X,Y)\) and \((X^{},Y^{})\) from \(\) as \((){=}[L(u^{}(X)-u^{}(X),u^{ }(X^{})-u^{}(X^{}),v^{}(Y)-v^{ }(Y),v^{}(Y^{})-v^{}(Y^{}),^{ })]\) and \((){=}[R(u^{}(X),u^{}(X^{}),v^{ }(Y),v^{}(Y^{}))]\), where the loss functionals \(L\) and \(R\) are defined for \(u,u^{},v,v^{}{}^{d}\) and \(s{}^{d}\) as

\[L(u,u^{},v,v^{},s){:=}(u^{}\, (s)v^{})^{2}{+}(v^{}\,(s)u^{ })^{2}{-}u^{}\,(s)v^{}{-}v^{}\,(s)u^{ },\] (11) \[R(u,u^{},v,v^{}){:=}(u^{}u^{})^{2}{-}(u{-}u^{ })^{}\,(u{-}u^{}){+}(v^{}v^{})^{2}{-}(v{-}v^{ })^{}(v{-}v^{}){+}2d.\] (12)

Therefore, at every epoch we take two independent batches \(_{n}^{1}\) and \(_{n}^{2}\) of equal size from \(_{n}\), leading to Algorithm 1. See Appendix A.1 for the full discussion, and Appendix A.2, where we also provide in Figure 4 an example of learning dynamics.

``` training data (\(X_{}\),\(Y_{}\))  train \(u^{}\), \(^{}\) and \(v^{}\) using the NCP loss  Center and scale \(X_{}\) and \(Y_{}\) for each epoch do  From (\(X_{}\),\(Y_{}\)) pick two random batches (\(X_{}\),\(Y_{}\)) and (\(X^{}_{}\),\(Y^{}_{}\))  Evaluate: \(U u^{}(X_{})\), \(U^{} u^{}(X^{}_{})\), \(V v^{}(Y_{})\), \(V^{} v^{}(Y^{}_{})\)  Compute \(}()\) as an unbiased estimate using (9) or (11)  Compute \(}()\) as an unbiased estimate using (10) or (12)  Compute NCP loss \(}_{}():=}()+ }()\) and back-propagate endfor ```

**Algorithm 1** Separable density learning procedure

Practical guidelines for trainingIn the following, we briefly report a few aspects to be kept in mind when using the NCP in practice, referring the reader to Appendix A for further details. The computational complexity of loss estimation presents three distinct methodological approaches. The first method utilizes unbiased estimation via covariance calculations in (9) and (10), achieving a computational complexity of \((nd^{2})\) for a batch size \(n\). An alternative approach employing U-statistics with (11) and (12) requires \((n^{2}d)\) operations per iteration, offering the estimation of the same precision. A third method involves batch averaging of (11) and (12), reducing computational complexity to \((nd)\), which enables seamless integration with contemporary deep learning frameworks, albeit potentially compromising training robustness through less accurate 4th-order moment estimations. Method selection remains contingent upon the specific problem's computational and statistical constraints. Further, the size of latent dimension \(d\), as indicated by Theorem 1 relates to the problem's "difficulty" in the sense of smoothness of joint density w.r.t. its marginals. Lastly, after the training, an additional post-processing may be applied to ensure the orthogonality of features \(u^{}\) and \(v^{}\) and improve statistical accuracy of the learned model.

Performing inference with the trained NCP modelWe now explain how to extract important statistical objects from the trained model \((^{},^{},^{})\). To this end, define the empirical operator

\[}^{}_{Y|X}:L^{2}_{}()\!\!\!\!L^{2}_{ }()[}^{}_{Y|X}f](x)\!\!:=\!\! _{i[d]}^{}_{i}^{}_{i}(x)\,}_{y}[^{}_{i}\,f], f L^{2}_{}(),\,x ,\] (13)

where \(}_{y}[^{}_{i}\,f]:=_{j[ n]}^{}_{i}(y_{j})f(y_{j})\). Then, _without any retraining nor simulation_, we can compute the following statistics:

\(\) Conditional Expectation: \([}^{}_{Y|X}f](x)\!:=\!}_{y}f\!+ \![}^{}_{Y|X}f](x),\,f L^{2}_{}(),x \).

\(\) Conditional moments of order \( 1\): apply previous formula to \(f(u)=u^{}\).

\(\) Conditional covariance: \(}^{}(Y|X):=}^{}_{Y|X}[ YY^{}]-}^{}_{Y|X}[Y]}^{}_{Y|X}[Y^{ }]\).

\(\) Conditional probabilities: apply the above conditional expectation formula with \(f(y)=_{B}(y)\), that is, \(_{y}(B)=}_{y}[_{B}]\) and \(_{}(B\,|\,x)=_{y}(B)+_{i[d]}^{}_ {i}^{}_{i}(x)\,}_{y}[^{} _{i}_{B}],\,B\!\!_{},x\!\!\). Then, integrating over an arbitrary set \(A_{}\) we get

\[_{}(B\,|\,A):=_{y}(B)+_{i[d]}^{}_ {i}\,}_{x}[^{}_{i}_{A}]}{ _{x}[_{A}]}\,}_{y}[^{} _{i}_{B}].\] (14)

\(\) Conditional quantiles: for scalar output \(Y\), the conditional CDF \(_{Y|X A}(t)\) is obtained by taking \(B=(-,t]\), and in Algorithm 3 in Appendix C we show how to extract quantiles from it.

## 5 Statistical guarantees

We introduce some standard assumptions needed to state our theoretical learning guarantees. To that end, for any \(A_{}\) and \(B_{}\) we define important constants, followed by the main assumption,

\[_{X}(A)\!:=\!1[X A]}{[X  A]}}_{Y}(B)\!:=\!1[Y B]}{[Y B]}}.\]

**Assumption 1**.: _There exists finite absolute constants \(c_{u},c_{v}>1\) such that for any \(\)_

\[}\|u^{}(x)\|_{l_{}} c_{u}, }\|v^{}(y)\|_{l_{}} c_{v}.\]

Next, we set \(_{}^{2}(X){:=}(\|u^{}(X)-[u^{}(X) ]\|_{l_{2}})\), \(_{}^{2}(Y){:=}(\|v^{}(Y)-[v^{}(Y )]\|_{l_{2}})\) and

\[_{n}(){:=}C(c_{u} c_{v}) \,(e^{-1})}{n}{+}(_{}(X)_{}(Y)))}{n}},\ _{n}(){:=}2}{n}},\] (15)

for some large enough absolute constant \(C>0\).

**Remark 1**.: _It follows easily from Assumption 1 that \(_{}^{2}(X){}c_{u}^{2}d\) and \(_{}^{2}(Y){}c_{v}^{2}d\). Consequently, assuming that \(n(c_{u} c_{v})d\), then \(_{n}()(c_{u} c_{v})[) /n}((e^{-1})/n)]\)._

Finally, for a given parameter \(\) and \((0,1)\), let us denote

\[_{}{:=}\{\|[_{Y|X}]_{d}{-}U_{} S_{}V_{}^{*}\|,\|U_{}^{*}U_{}{-}I\|,\|U_{}^{*}{ \,}_{X}\|,\|V_{}^{*}V_{}{-}I\|,\|V_{}^{*}{\, }_{}\|\},\ \] (16) \[_{n}():=_{d+1}^{*}+_{}+2_{}}(_{}+_{n}())+[ _{n}()]^{2}.\] (17)

In the following result, we prove that NCP model approximates well the conditional probability distribution w.h.p. whenever the empirical loss \(}_{}()\) is well minimized.

**Theorem 2**.: _Let Assumption 1 be satisfied, and in addition assume that_

\[(X{}A)(Y{}B){}_{n}( /3) n{}(c_{u} c_{v})^{2}d 8(6 ^{-1})[_{X}(A)_{Y}(B)].\] (18)

_Then for every \(A_{}\{\}\) and \(B_{}\{\}\)_

\[|[Y{}B\,|\,X{}A]}{[Y{}B]}-_{}(B\,|\,A)}{_{y}(B)}|(/3 )+[1{+}_{n}(/3)][2_{X}(A){+}4_{Y}(B)] _{n}(/3)}{[X{}A][Y{}B]}},\] (19)

_and_

\[|[Y{}B\,|\,X{}A]{-}_{}(B\,|\,A)} {[Y{}B]}|{}_{Y}(B)_{n}(/3){ +}_{n}(/3))_{X}(A)_{n}(/3){ +}_{n}(/3)}{[X{}A][Y{}B]}}\] (20)

_hold with probability at least \(1-\) w.r.t. iid draw of the dataset \(_{n}=(x_{j},y_{j})_{j[n]}\) from \(\)._

**Remark 2**.: _In Appendix B.5, we prove a similar result under a less restrictive sub-Gaussian assumption on the singular functions \(u^{}(X)\) and \(v^{}(Y)\)._

DiscussionThe rate \(_{n}()\) in (17) is pivotal for the efficacy of our method. If we appropriately choose the latent space dimension \(d\) to ensure accurate approximation (\(_{d+1}^{*} 1\)), achieve successful training (\(_{}_{d+1}^{*}\)), and secure a large enough sample size (\(_{n}() 1\)), Theorem 2 provides assurance of accurate prediction of conditional probabilities. Indeed, (20) guarantees (up to a logarithmic factor)

\[[Y{}B\,|\,X{}A]{-}_{}(B\,|\,A){=}O_{}(}{+}[Y{}B]}{[X{}A]}} (_{d+1}^{*}{+}_{}{+}_{X}(A)/ )),\]

Note the inclusion of the term \([X A]}\) in the denominator of the last term on the right-hand side, along with \(_{X}(A)\). This indicates a decrease in the accuracy of conditional probability estimates for rarely encountered event \(A\), aligning with intuition and with a known finite-sample impossibility result Lei and Wasserman (2014, Lemma 1) for conditional confidence regions when \(A\) is reduced to any nonatomic point of the distribution (i.e. \(A=\{x\}\) with \([X=x]=0\)). For rare events, a larger sample size \(n\) and a higher-dimensional latent space characterized by \(d\) are necessary for accurate estimation of conditional probabilities.

We propose next a non-asymptotic estimation guarantee for the conditional CDF of \(Y|X\) when \(Y\) is a scalar output. This result ensures in particular that accurate estimation of the true quantiles is possible with our method. Fix \(t\) and consider the set \(B_{t}=(-,t]\) meaning that \([Y{}B_{t}|X{}A]{=}F_{Y|X A}(t)\) and \([Y{}B_{t}]{=}F_{Y}(t)\). We define similarly for the NCP estimator of the conditional CDF \(_{Y|X A}(t){=}_{}(B_{t}\,|\,A)\). The result follows from applying (20) to the set \(B_{t}\).

**Corollary 1**.: _Let the Assumptions of Theorem 2 be satisfied. Then for any \(t\) and \((0,1)\), it holds with probability at least \(1-\) that_

\[|_{Y|X A}(t)-F_{Y|X A}(t)|(t)(1-F_{ Y}(t))_{n}}(/3)\\ +(t)}{[X A]}}(_{d+1}^{ }+2_{}+(2+1)_{n}(/3)+4 _{X}(A)_{n}(/3)).\] (21)

An important application of Corollary 1 lies in uncertainty quantification when output \(Y\) is a scalar. Indeed, for any \((0,1/2)\), we can scan the empirical conditional CDF \(_{Y|X A}\) for values \(t_{}<t_{}^{}\) such that \(_{Y|X A}(t_{}^{})-_{Y|X A}(t_{} )=1-\) and \(t_{}^{}-t_{}\) is minimal. That way we define a non-asymptotic conditional confidence interval \(_{}:=(t_{},t_{}^{}]\) with approximate coverage \(1-\). More precisely we deduce from Corollary 1 that

\[|[Y_{}\,|\,X A]-(1-)| _{n}(/6)\\ +[X A]}}(_{d+1}^{}+2 _{}+(2+1)_{n}(/6)+4_{X} (A)_{n}(/6)).\] (22)

In App B.6, we derive statistical guarantees for the conditional expectation and covariance of \(Y\).

## 6 Experiments

**Conditional density estimation** We applied our NCP method to a benchmark of several conditional density models including those of Rothfuss et al. (2019); Gao and Hastie (2022). See Appendix C.1 for the complete description of the data models and the complete list of compared methods in Tab. 2 with references. We also plotted several conditional CDF along with our NCP estimators in Fig. 6. To assess the performance of each method, we use Kolmogorov-Smirnov (KS) distance between the estimated and the true conditional CDFs. We test each method on nineteen different conditional values uniformly sampled between the 5%- and 95%-percentile of \(p(x)\) and computed the averaged performance over all the used conditioning values. In Tab. 1, we report mean performance (KS distance \(\) std) computed over 10 repetitions, each with a different seed. NCP with whitening (NCP-W) outperforms all other methods on 4 datasets, ties with FlexCode (FC) on 1 dataset, and ranks a close second on another one behind NF. These experiments underscore NCP's consistent performance. We also refer to Tab. 3 in App C.1 for an ablation study on post-treatments for NCP.

**Confidence regions** Our goal is to estimate conditional confidence intervals for two different data models (Laplace and Cauchy). We investigate the performance of our method in (22) and compare it to the popular conditional conformal prediction approach. We refer to App C.2 for a quick description of the principle underlying CCP. We trained an NCP model combined with an MLP architecture followed by whitening post-processing. See App C.2 for the full description. We obtained that

   Model & LinearGaussian & EconDensity & ArmaJump & SkewNormal & GaussianMixture & LGGMD \\  NCP - W & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ DDPM & \(\) & \(0.236 0.217\) & \(0.338 0.317\) & \(0.250 0.224\) & \(0.404 0.242\) & \(0.405 0.218\) \\ NF & \(\) & \(\) & \(0.143 0.010\) & \(0.032 0.002\) & \(0.107 0.003\) & \(0.254 0.004\) \\ KMN & \(0.601 0.004\) & \(0.362 0.017\) & \(0.487 0.004\) & \(0.381 0.009\) & \(0.309 0.001\) & \(0.224 0.005\) \\ MDN & \(0.225 0.013\) & \(0.048 0.001\) & \(0.163 0.018\) & \(0.087 0.001\) & \(0.129 0.007\) & \(0.176 0.013\) \\ LSCDE & \(0.420 0.001\) & \(0.118 0.002\) & \(0.247 0.001\) & \(0.107 0.001\) & \(0.202 0.001\) & \(0.268 0.024\) \\ CKDE & \(0.120 0.000\) & \(0.010 0.001\) & \(0.072 0.001\) & \(\) & \(0.048 0.001\) & \(0.230 0.014\) \\ NNKCDE & \(0.047 0.003\) & \(0.036 0.003\) & \(\) & \(0.030 0.002\) & \(0.035 0.002\) & \(0.183 0.006\) \\ RFCDE & \(0.128 0.007\) & \(0.141 0.009\) & \(0.133 0.015\) & \(0.142 0.012\) & \(0.130 0.012\) & \(0.121 0.006\) \\ FC & \(0.095 0.005\) & \(0.011 0.001\) & \(0.033 0.002\) & \(0.035 0.007\) & \(\) & \(\) \\ LCDE & \(0.108 0.001\) & \(0.026 0.001\) & \(0.113 0.002\) & \(0.075 0.006\) & \(0.035 0.001\) & \(0.124 0.002\) \\   

Table 1: Mean and standard deviation of Kolmogorov-Smirnov distance of estimated CDF from the truth averaged over 10 repetitions with \(n=10^{5}\) (best method in red, second best in bold black).

[MISSING_PAGE_FAIL:9]

to \(d=1000\), the computation time increases by only 20%, while maintaining strong statistical performance throughout.

High-dimensional experiment in molecular dynamicsWe investigate protein folding dynamics and predict conditional transition probabilities between metastable states. Figure 2 shows how, by integrating our NCP approach with a graph neural network (GNN), we achieve accurate state forecasting and strong uncertainty quantification, enabling efficient tracking of transitions. For further context and a full model description, see App C.3.

## 7 Conclusion

We introduced NCP, a novel neural operator approach to learn the conditional probability distribution from complex and highly nonlinear data. NCP offers a number of benefits. Notably, it streamlines the training process by requiring just one unconditional training phase to learn the joint distribution \(p(x,y)\). Subsequently, it allows us to efficiently derive conditional probabilities and other relevant statistics from the trained model analytically, without any additional conditional training steps or Monte Carlo sampling. Additionally, our method is backed by theoretical non-asymptotic guarantees ensuring the soundness of our training method and the accuracy of the obtained conditional statistics. Our experiments on learning conditional densities and confidence regions demonstrate our approach's superiority or equivalence to leading methods, even using a simple Multi-Layer Perceptron (MLP) with two hidden layers and GELU activations. This highlights the effectiveness of a minimalistic architecture coupled with a theoretically grounded loss function. While complex architectures often dominate advanced machine learning, our results show that simplicity can achieve competitive results without compromising performance. Our numerical experiments suggest that, while our approach works well across different datasets and models, the price we pay for this generality appears to be the need for a relatively large sample size (\(n 10^{4}\)) to start outperforming other methods. Hence, a future direction is to study how to incorporate prior knowledge into our method to make it more data-efficient. Future works will also investigate the performance of NCP for multi-dimensional time series, causality and more general sensitivity analysis in uncertainty quantification.

Figure 2: **Protein folding dynamics. Pairwise Euclidean distances between Chignolin atoms exhibit increased variance during folded metastable states (between 87-88\(\)s and around 89.5\(\)s). Ground truth is depicted in blue, predicted mean in orange, and the grey lines indicate the estimated 10% lower and upper quantiles.**