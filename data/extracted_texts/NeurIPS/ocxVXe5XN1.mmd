# Generalization Bounds via Conditional \(f\)-Information

Ziqiao Wang

School of Computer Science and Technology

Tongji University

Shanghai, China

ziqiaowang@tongji.edu.cn

&Yongyi Mao

School of Electrical Engineering and Computer Science

University of Ottawa

Ottawa, Canada

ymao@uottawa.ca

###### Abstract

In this work, we introduce novel information-theoretic generalization bounds using the conditional \(f\)-information framework, an extension of the traditional conditional mutual information (MI) framework. We provide a generic approach to derive generalization bounds via \(f\)-information in the supersample setting, applicable to both bounded and unbounded loss functions. Unlike previous MI-based bounds, our proof strategy does not rely on upper bounding the cumulant-generating function (CGF) in the variational formula of MI. Instead, we set the CGF or its upper bound to zero by carefully selecting the measurable function invoked in the variational formula. Although some of our techniques are partially inspired by recent advances in the coin-betting framework (e.g., Jang et al. (2023)), our results are independent of any previous findings from regret guarantees of online gambling algorithms. Additionally, our newly derived MI-based bound recovers many previous results and improves our understanding of their potential limitations. Finally, we empirically compare various \(f\)-information measures for generalization, demonstrating the improvement of our new bounds over the previous bounds.

## 1 Introduction

Understanding generalization is a longstanding topic of machine learning research. Recently, information-theoretic generalization measures, e.g., mutual information (MI) between the input (i.e. data) and output (i.e. hypothesis) of a learning algorithm, have been proposed by . While original MI-based information-theoretic generalization bounds have made significant progress in analyzing the generalization of stochastic gradient-based algorithms in nonconvex learning problems (e.g., deep learning) , they are found to have several limitations. One of the most severe limitations is their unboundedness, where the MI measure can be infinite while the true generalization error is small . To mitigate this issue, various bound-tightening techniques have been proposed . Among them,  introduces a conditional mutual information (CMI) framework, which allows for the derivation of a CMI-based bound that is strictly upper-bounded by a constant. In the CMI framework, an additional "ghost sample" is introduced alongside the original sample drawn from the unknown data distribution. A Bernoulli random variable sequence is then used to determine the membership of training data from these two samples. Ultimately, the CMI between these Bernoulli random variables and the output hypothesis of the learning algorithm, conditioned on these two samples, characterizes generalization. This setup, known as the "supersample" setting, ensures theboundedness of the CMI bound due to the constant entropy of a Bernoulli random variable. Intuitively, this CMI quantity leaks less information between data and the hypothesis, resulting in tighter bounds compared to the original MI quantity, as theoretically justified in . Furthermore, several studies have aimed to further tighten CMI-based bounds. For example, previous tightening techniques in  are also applicable to CMI bounds . Notably, another way to tighten the CMI bound is by utilizing variants of the original CMI quantity, such as functional CMI (\(f\)-CMI) , evaluated CMI (e-CMI) , and loss-difference CMI (ld-CMI) . As the output hypothesis is replaced by the predictions, loss pairs, and loss differences of the two samples, these CMI quantities are tighter than the original hypothesis-based CMI due to the data-processing inequality. While most information-theoretic bounds in the supersample setting focus on (conditional) mutual information measures, it is natural to ask whether there is a general way to extend this CMI framework to other statistical distance/divergence measures, such as other \(f\)-divergences.

Using alternative dependence measures to replace MI in information-theoretic generalization bounds has been studied in several existing works , with some of them considering the supersample setting. In particular,  provides some Wasserstein distance-based and total variation-based bounds in the supersample setting. Furthermore,  proposes a general convex analysis-based framework that replaces the input-output MI quantity in  with any strongly convex function. While  itself does not discuss the supersample setting, the further extension in  indeed considers invoking the supersample setting to strengthen the bounds.

In this work, we present a generic approach to derive generalization bounds based on conditional \(f\)-information, a natural extension from mutual information to other \(f\)-divergence-based dependence measures. Our proof strategy is significantly different from the original CMI framework  and the convex analysis (or regret analysis) based framework . Specifically, our development starts from the variational representation of \(f\)-divergence, which involves the convex conjugate of a convex function used to define the \(f\)-divergence. While the previous CMI framework focuses on upper-bounding the cumulant generating function (CGF) of a selected measurable function (which relates to the generalization error), we make a particular choice of such a measurable function, namely the inverse of that convex conjugate function. This choice makes CGF equal to (or upper bounded by) zero, hence eliminated from the variational representation. The remaining task is to lower bound this inverse of the convex conjugate function under a joint distribution. In the case of MI (or KL), the expectation of this inverse function is close to the _log wealth_ quantity used in the coin-betting framework for obtaining concentration inequalities . Indeed, the lower bounding techniques here are inspired by some inequalities used in the coin-betting framework , and we extend them to the cases of other \(f\)-divergence in the supersample setting. Notably, unlike , our conditional \(f\)-information generalization bounds do not rely on any existing regret guarantees of online learning algorithms. We discuss the connection with the coin-betting framework in Appendix E.

Given that \(f\)-divergences all obey the data-processing inequality , we focus on using loss difference (ld)-based \(f\)-information, an extension of ld-CMI in , in this work. Specifically, our main contributions are summarized as follows: 1) In the case where the loss difference between a data pair is bounded, we first provide a novel variational formula (cf. Lemma 3.1) for \(f\)-information, enabling us to derive conditional \(f\)-information-based generalization bounds; 2) For the KL case, we first provide an "oracle" CMI bound (cf. Theorem 3.1). This bound recovers many previous CMI bounds, including the square-root bound and fast-rate bounds in low empirical risk settings. It also helps us understand the potential looseness of previous square-root CMI bounds, where some quantities that can vanish as the sample size \(n\) increases are simply ignored. Additionally, some novel fast-rate bounds are implied (cf. Corollary 3.1-3.2). Particularly, Corollary 3.2 contains a term in the form of the product of total variation and KL divergence, a similar quantity also appearing in a recent PAC-Bayesian bound ; 3) We present several other \(f\)-information-based bounds, including the looser measure, \(^{2}\)-information (cf. Theorem B.2) and tighter measures, squared Hellinger (SH)-information (cf. Theorem 3.2) and Jensen-Shannon (JS)-information (cf. Theorem 3.3). Due to the similarity in expressions, most arguments for our CMI bounds are also applicable to these bounds based on \(f\)-information measures; 4) We extend our framework to the unbounded loss difference case, where we provide a refined variational formula for \(f\)-information (cf. Lemma 4.1), enabling us to provide a novel \(f\)-information-based bound (cf. Theorem 4.1 for the case of KL). The obtained bound is no worse than a previous information-theoretic generalization bound in  (adapted to the same supersample setting); 5) Empirical results show that the new bounds in our framework, particularly the squared Hellinger-information bound, outperform previous results.

Preliminaries

NotationThroughout this paper, we adopt a convention where capitalized letters denote random variables, and their corresponding lowercase letters represent specific realizations. Let \(P_{X}\) denote the distribution of a random variable \(X\), and \(P_{X|Y}\) represent the conditional distribution of \(X\) given \(Y\). When conditioning on a specific realization, we use \(P_{X|Y=y}\) or simply \(P_{X|y}\). Additionally, we use \(_{X}\) and \(_{P}\) interchangeably to denote the expectation over \(X P\), whenever it is clear from context. Moreover, \(_{X|Y=y}\) (or \(_{X|y}\)) denotes the expectation over \(X P_{X|Y=y}\).

Generalization ErrorLet \(\) be the domain of the instances, and let \(\) be the domain of the hypotheses. We denote the unknown distribution of the instances by \(\), and let \(S=\{Z_{i}\}_{i=1}^{n}\) be the training sample. A learning algorithm \(\), characterized by a conditional distribution \(P_{W|S}\), takes the training sample \(S\) as the input and outputs a hypothesis \(W\), i.e. \(:^{n}\). To evaluate the quality of the output hypothesis \(W\), we use a loss function \(:_{0}^{+}\). For any hypothesis \(w\), the population risk is defined as \(L_{}(w)_{Z^{}}[(w,Z^{})]\), where \(Z^{}\) is an independent testing instance. In practice, since \(\) is unknown, we use the empirical risk, which is defined as \(L_{S}(w)_{i=1}^{n}(w,Z_{i})\), as a proxy for the population risk of \(w\). The expected generalization error is then defined as \(_{}()_{W}[L_{}(W)] -_{W,S}[L_{S}(W)]\).

Supersample SettingWe follow the traditional supersample setting introduced by . Let an \(n 2\) matrix \(^{n 2}\) be the supersample, where each entry is drawn i.i.d. from the data distribution \(\). We index the columns of \(\) by \(\{0,1\}\), and denote the \(i\)th row of \(\) as \(_{i}\) with entries \((_{i,0},_{i,1})\). We use \(U=\{U_{i}\}_{i=1}^{n}(\{0,1\}^{n}),\) independent of \(\), to determine the training sample membership from \(\). Specifically, when \(U_{i}=0\), \(_{i,0}\) in \(\) is included in the training set \(S\), and \(_{i,1}\) is used for testing; \(U_{i}=1\) dictates the opposite case. Let \(_{i}=1-U_{i}\), the training sample \(S\) is then equivalent to \(_{U}=\{_{i,U_{i}}\}_{i=1}^{n}\), and the testing sample is \(_{}=\{_{i,_{i}}\}_{i=1}^{n}\). Additionally, we let \(L_{i,0}(W,_{i,0})\) and define \(L_{i,1}\) similarly. Let \( L_{i}=L_{i,1}-L_{i,0}\) be the loss difference in the \(i\)th row of \(\). To avoid complicated subscripts, we also use superscripts \(+\) and \(-\) to replace the subscripts \(0\) and \(1\), respectively, namely, \(_{i}^{+}=_{i,0}\), \(_{i}^{-}=_{i,1}\), \(L_{i}^{+}=L_{i,0}\) and \(L_{i}^{-}=L_{i,1}\). Furthermore, we define \(G_{i}(-1)^{U_{i}} L_{i}\), i.e. the testing loss minus the training loss at position \(i\). Clearly, \(_{i=1}^{n}G_{i}\) is an unbiased estimator of \(_{}()\). In other words, \(_{}()=_{i=1}^{n}_{ L _{i},U_{i}}[G_{i}]\).

\(f\)-Divergence and \(f\)-InformationThe family of \(f\)-divergence is defined as follows.

**Definition 2.1** (\(f\)-divergence ).: _Let \(P\) and \(Q\) be two distributions on \(\), and let \(:_{+}\) be a convex function with \((1)=0\). If \(P Q\), then the \(f\)-divergence between \(P\) and \(Q\) is defined as_

\[_{}(P||Q)_{Q}[( )]\!,\]

_where \(\) is a Radon-Nikodym derivative._

A variational representation for \(f\)-divergence, as provided below, has been independently investigated in previous works . Recently, this variational representation has also been applied to PAC-Bayesian generalization theory  and domain adaptation theory .

**Lemma 2.1** ([37, Corollary 3.5]).: _Let \(^{*}\) be the convex conjugate1 of \(\), and \(=\{g:(^{*})\}\). The following, known as variational formula of \(f\)-divergence, holds:_

\[_{}(P||Q)=_{g}_{ P}[g ()]-_{}\{_{ Q} [^{*}(g()+)]-\}.\]

Following [33, Section 7.8], let \(I_{}(X;Y)_{}(P_{X,Y}||P_{X}P_{Y})\) be the \(f\)-information, which extends the standard mutual information (MI) defined in terms of KL divergence.

We now set \(=( L_{i},U_{i})\) and \(g( L_{i},U_{i})=tG_{i}\) in Lemma 2.1, where \(t\), \(P=P_{ L_{i},U_{i}}\) is the joint distribution of \(( L_{i},U_{i})\) and \(Q=P_{ L_{i}}P_{U_{i}}\) is the product of the marginal distributions. Consequently, Lemma 2.1 directly implies that \[_{P}[G_{i}] _{t}(I_{}( L_{i};U_{ i})+_{}\{_{Q}[^{*}(t(-1)^{U_{i}^{ }} L_{i}+)]-\})\] (1) \[_{t}(I_{}( L_{i};U_ {i})+_{Q}[^{*}(t(-1)^{U_{i}^{}} L_{i})] ),\] (2)

where we let \(=0\) in Eq. (2). Notice that if \((x)=x x+c(x-1)\) for any constant \(c\), then \(I_{}(X;Y)\) becomes MI. In this case, the optimal \(^{*}=-_{Q}[e^{g()}]\) in Lemma 2.1, and Lemma 2.1 recovers the Donsker and Varadhan's variational formula (cf. Lemma A.1). Here, the second term in Eq. (1) represents the cumulant generating function of \(( L_{i},U_{i}^{})\). By analyzing the tail behavior of \(( L_{i},U_{i}^{})\) and applying Eq. (1), one can derive the final MI-based generalization bound. For example, if the loss is bounded, Eq. (1) recovers [24, Theorem 3.2] by using Hoeffding's Lemma.

On one hand, while Eq. (1) is tighter than Eq. (2), the parameter \(\) may not have an analytic form for divergences beyond KL and \(^{2}\). In this sense, Eq. (2) has its own merits. On the other hand, previous MI-based generalization bounds use concentration results to further upper bound the second term in Eq. (1). While this strategy has been successful for the MI case, for instance by using Hoeffding's Lemma, it may be challenging to obtain similar concentration results for other \(f\)-information. In this work, we introduce a novel approach to prove generalization bounds for \(f\)-information.

## 3 Conditional \(f\)-Information Bounds: Bounded Loss Difference Case

We begin by presenting a general lemma that will serve as a main recipe for obtaining generalization bounds in this section. This lemma introduces a new variational formula for \(f\)-divergence, which may be of independent interest beyond the context of generalization.

**Lemma 3.1**.: _Given random variables \(X\), \(Y\), and a measurable function \(f\) for \((X,Y)\). For every \(t[b_{1},b_{2}]\), assume that \(^{*}\) is invertible within the range of \(t f\), which we denote by \(_{t}\). Let \(^{*-1}\) be the inverse of \(^{*}\) on \(_{t[b_{1},b_{2}]}_{t}\), and let \(Y^{}\) be an independent copy of \(Y\). If \(_{X,Y^{}}[f(X,Y^{})]=0\), then_

\[_{t[b_{1},b_{2}]}_{X,Y}[^{*-1}(tf(X,Y))] I _{}(X;Y).\]

Although Lemma 3.1 is a simple and straightforward result derived from Lemma 2.1 by setting \(g=^{*-1}(tf)\), it is quite powerful for deriving \(f\)-information-based generalization bounds. Specifically, in the supersample setting, due to the symmetric properties of two columns of \(\), we set \(X= L_{i}\), \(Y=U_{i}\) and \(f( L_{i},U_{i}^{})=(-1)^{U_{i}^{}} L_{i}\) (where \(U_{i}^{}\) is an independent copy of \(U_{i}\)), the condition in Lemma 3.1, namely \(_{X,Y^{}}[f(X,Y^{})]=0\), is clearly met. Then we have

\[_{t}_{ L_{i},U_{i}}[^{*-1}(tG_{i}) ] I_{}( L_{i};U_{i}).\]

After carefully choosing \(b_{1},b_{2}\) such that \(^{*-1}\) is well defined, the main focus is to find a lower bound for the function \(^{*-1}\) over a certain interval. We will start with the standard MI case.

### Mutual Information (KL-based) Generalization Bound

Consider \((x)=x x+x-1\). Its convex conjugate function is \(^{*}(y)=e^{y}-1\) with the inverse \(^{*-1}(z)=(1+z)\). Building upon Lemma 3.1, we obtain the following bound:

**Theorem 3.1**.: _Assume the loss difference \((w,z_{1})-(w,z_{2})\) is bounded in \([-1,1]\) for any \(w\) and \(z_{1},z_{2}\), we have_

\[|_{}()|_{i=1}^{n}[ L_{i}^{2}]+|[G_{i}]|)I ( L_{i};U_{i})}.\]

We remark that a bounded loss difference is a less restrictive assumption than a strictly bounded loss. For example, if \(\) is \(L\)-Lipschitz in the second argument, then \((w,z_{1})-(w,z_{2}) L||z_{1}-z_{2}||\)Thus, as long as \(||z_{1}-z_{2}||\) is bounded, the loss difference is bounded even when \(\) itself is unbounded. Additionally, we refer to Theorem 3.1 as the "_oracle_" CMI bound because the upper bound itself includes the expected generalization error at position \(i\), \([G_{i}]\), which is precisely the quantity we aim to bound.

From Theorem 3.1, first, if we simply upper bound the loss difference \( L_{i}\) (and \(G_{i}\)) by one, Theorem 3.1 recovers [24, Theorem 3.2] up to a constant. More importantly, Theorem 3.1 indicates that in the case of bounded loss difference or bounded loss, solely using \(I( L_{i};U_{i})\) as the generalization measure for algorithm \(\) is insufficient to accurately characterize its generalization error. Specifically, when \(2([ L_{i}^{2}]+|[G_{i} ]|)\) vanishes as \(n\) increases, relying on the MI-based measure alone, i.e., \(;U_{i})}\), will always result in a slow convergence rate for \(|_{}()|\). While similar observations have been made in recent studies , where it is found that the sub-Gaussian variance proxy in previous MI bounds may also vanish in some examples, our Theorem 3.1 provides a more straightforward understanding of the potential looseness of \((;U_{i})})\). Additionally, further discussion comparing our oracle CMI bound with the MI-based results in  is provided in Appendix B.4. To highlight the differences in our proof techniques compared to previous information-theoretic generalization bounds, we provide a proof sketch below.

Proof Sketch of Theorem 3.1.: Lemma 3.1 gives us \(I( L_{i};U_{i})_{t}[(1+t(-1)^{U_{i}}  L_{i})]\). Let \(f(x)=(1+x)-x+ax^{2}\) and set \(a=[G_{i}]|}{2[G_{i}^{2 }]}+\). By demonstrating that \(f(x) 0\) holds when \(a\) and \(|x| 1-\), we have \(_{t>-1}[(1+tG_{i})]_{t[ -1,1-]}[tG_{i}-at^{2}G_{i}^{2}]\). The supremum is attained when \(t^{*}=[G_{i}]}{2a[G_{i}^{2}] }=[G_{i}]}{|[G_{i}]|+[G_{i}^{2}]}\), which is achievable. Therefore, \(I( L_{i};U_{i})_{t(-1,+)}_{ L_{i},U_{i }}[(1+t(-1)^{U_{i}} L_{i})]^{2}[G_{i}]}{4a[G_{i}^{2}]}\), which simplifies to

\[^{2}[G_{i}] 2(|[G_{i}]|+ [G_{i}^{2}])I( L_{i};U_{i}).\] (3)

The remaining steps are straightforward. See Appendix B.1 for the complete proof. 

It is also worth noting that by letting \(g=^{*-1}\) and \(_{Q}[g]=0\) in the case of KL divergence, we obtain \(^{*}=0\) in Lemma 2.1. As a result, Eq. (2) becomes equivalent to Eq. (1). In this sense, starting from Eq. (2) does not compromise the tightness of Eq. (1). Moreover, the proof provides deeper insights into the tightness of Theorem 3.1, which we will now discuss.

Small Empirical Risk Case or Realizable SettingPreviously, fast-rate information-theoretic generalization bounds in the realizable setting--where the square-root function is removed--have been derived by demonstrating that the CGF can be negative , or by invoking the channel capacity result of the binary channel .

Based on Theorem 3.1, the square-root function can be directly removed in the realizable setting. Specifically, if the learning algorithm \(\) is an interpolating algorithm, i.e., the training loss is always minimized to zero, then \(G_{i}\) always holds. In this case, \(|[G_{i}]|=[G_{i}]\) and \([G_{i}^{2}][G_{i}]\), allowing us to obtain \([G_{i}] 4I( L_{i};U_{i})\) from Eq. (3). Consequently, Theorem 3.1 simplifies to \(_{}()_{i=1}^{n}I( L_{i};U_{i})\). Note that for this bound to hold, the condition of zero training loss can be relaxed to the training loss being always no larger than the testing loss.

Furthermore, to achieve a better constant in the bound, observe that the proof of our Theorem 3.1 utilizes the inequality \((1+x) x-ax^{2}\) for \(|x| 1-\). If \(x\) always holds, one can use the inequality \((1+x) x 2\) instead, where equality holds when the loss is zero-one loss (See Figure 1(Right) for a visualized illustration).

Figure 1: Comparison of different \(^{*-1}\) (Left) and examples of \(x-ax^{2}\) for lower-bounding \((1+x)\) (Right).

As a result, we obtain \(_{}()_{i=1}^{n};U_{i})}{n 2}\), which is tighter than the previous bound and will never be vacuous (i.e. the bound is always smaller than \(1\)) since \(I( L_{i};U_{i}) H(U_{i})= 2\). In fact, this bound can exactly characterize the generalization error (i.e., equality holds) if the loss is the zero-one loss, as demonstrated in [24, Theorem 3.3].

New Fast-Rate BoundsTheorem 3.1 not only recovers the previous fast-rate ld-CMI bound in the realizable setting but also introduces new fast-rate bounds.

By solving Eq. (3) for \(|[G_{i}]|\), we obtain the following bound.

**Corollary 3.1**.: _Under the conditions of Theorem 3.1, we have_

\[|_{}()|_{i=1}^{n} (2I( L_{i};U_{i})+[ L_{i}^{2}]I(  L_{i};U_{i})}).\]

This fast-rate bound has not been proved in , yet it resembles [24, Eq. (2)] in structure. Specifically, the single-loss MI term, \(2I(L_{i}^{+};U_{i})\) in [24, Eq. (2)], is now substituted with \(I( L_{i};U_{i})\), and the empirical risk term, \([L_{S}(W)]\), is replaced by \([ L_{i}^{2}]\). Here, \( L_{i}=L_{i}^{-}-L_{i}^{+}\), and notice that both \(L_{i}^{+}\) and \(L_{i}^{-}\) follow the same marginal distribution due to the symmetric construction of the supersample, so we can obtain that \([ L_{i}^{2}]=[(L_{i}^{-}-L_{i}^{+}) ^{2}] 4[(L_{i}^{+}-[L_{i}^{+}])^{2 }]=4(L_{i}^{+})\). Consequently, Corollary 3.1 further implies the following bound:

\[|_{}()|_{i=1}^{n}( 2I( L_{i};U_{i})+2(L_{i}^{+})I( L_{ i};U_{i})}).\] (4)

Notably, Eq. (4) hints that if the variance of the single loss in the supersample is sufficiently small, the bound will decay at the same rate as in the realizable setting, namely \((I( L_{i};U_{i}))\), as the second term vanishes.

Instead of solving Eq. (3), if we directly upper-bound \(|[G_{i}]|\) in Theorem 3.1, we can also obtain the following bound from Theorem 3.1.

**Corollary 3.2**.: _Under the conditions of Theorem 3.1, we have_

\[|_{}()|_{i=1}^{n}( [ L_{i}^{2}]I( L_{i};U_{i})}+_{U_{i}}[_{}(P_{ L_{i}|U_{i}}, P_{ L_{i}})]I( L_{i};U_{i})}).\]

Note that Corollary 3.2 is tighter than Corollary 3.1 by Jensen's inequality and Pinsker's inequality. While we focus on the MI-based or KL-based generalization bound in this section, the inclusion of total variation (TV) in the second term of Corollary 3.2 expands the scope of the bound. Notably, in a recent study by , a component of the form \(_{}_{}}\) is also present in their generalization bounds. Specifically, they derive PAC-Bayesian generalization bounds using the Zhang-Cutkosky-Paschalidis (ZCP) divergence, initially explored in . Interestingly, the ZCP divergence can be further upper bounded by \((_{}_{}}+_{})\). Although it remains uncertain whether the first term in Corollary 3.2 is tighter than \(_{}\) or not, we demonstrate that the component \(_{}_{}}\) can emerge directly in the derivation of KL-based bounds, without using the ZCP divergence. Additionally, note that the term \([ L_{i}^{2}]\) in Corollary 3.2 can likewise be replaced by \(4(L_{i}^{+})\) term.

### Other \(f\)-Information-based Generalization Bound

Based on Lemma 3.1, it is straightforward to apply similar techniques to many well-known \(f\)-divergences or \(f\)-information measures to obtain generalization bounds. In this section, we discuss several other \(f\)-information-based bounds. Before we study the specific \(f\)-divergence, we remark that, in Section 3.1, our focus was on the unconditional mutual information \(I( L_{i};U_{i})\). As illustrated in , using the data-processing inequality (DPI) and the chain rule, we have \(I( L_{i};U_{i}) I(L_{i}^{+},L_{i}^{-};U_{i}) I(W;U_{i}| {Z}_{i})\). This makes it easy to derive other CMI variant-based bounds. While other \(f\)-divergences also satisfy DPI, they may not adhere to the chain rule. Accordingly, for these cases, we use the disintegrated conditional \(f\)-information, defined as \(I_{}^{z}(X;Y)_{}(P_{XY|z}||P_{X|z}P_{Y|z})\) (noting that \(I(X;Y|Z)=_{Z}[I_{}^{Z}(X;Y))]\)), instead of the unconditional quantity. In this context, at least \(I_{}( L_{i};U_{i}|_{i}) I_{}(W;U_{i}| {Z}_{i})\) still holds, ensuring that the hypothesis-based conditional \(f\)-information bound can be directly derived.

\(^{2}\)-based Generalization BoundConsider \((x)=(x-1)^{2}\). Its convex conjugate is \(^{*}(y)=}{4}+y\). For \(y-2\), the inverse of conjugate function \(^{*-1}(z)=2(-1)\) exists. Since \(2(-1)(1+z)\), any lower bound that holds for \((1+z)\) will also hold for \(2(-1)\). See Figure 1(Left) for a visualization. In other words, all the bounds in Section 3.1 will also hold when using \(^{2}\)-divergence. This can also be immediately noticed by the inequality \(_{}(P||Q)^{2}(P||Q)\). We defer the formal disintegrated \(^{2}\)-information bound in Appendix B.7.

Squared Hellinger (SH) Distance Generalization BoundConsider \((x)=(-1)^{2}\), and its convex conjugate is \(^{*}(y)=\). For \(y(-,1)\), the inverse function \(^{*-1}(z)=\) exists for \(z(-1,+)\). We then have the following oracle conditional squared Hellinger (SH)-information bound.

**Theorem 3.2**.: _Under the same conditions in Theorem 3.1, we have_

\[|_{}()|_{i=1}^{n}_{ _{i}}[ L_{i}^{2}| _{i}]+2|[G_{i}|_{i}]| )I_{^{2}}^{_{i}}( L_{i};U_{i})},\]

_where \(I_{^{2}}^{_{i}}( L_{i};U_{i})=_{ ^{2}}(P_{ L_{i},U_{i}|_{i}}\|P_{ L_{i }|_{i}}P_{U_{i}})\) is the (disintegrated) SH-information._

Given that \(_{^{2}}(P||Q)_{}( P,Q)\), the TV-based bound can be derived as a corollary from above.

Jensen-Shannon (JS) Divergence Generalization BoundConsider \((x)=x+\), with the convex conjugate \(^{*}(y)=-(2-e^{y})\). For \(y< 2\), the inverse function \(^{*-1}(z)=(2-e^{-z})\) exists for \(z>- 2\). This leads to the following oracle JS-information bound.

**Theorem 3.3**.: _Under the same conditions in Theorem 3.1, we have_

\[|_{}()|_{i=1}^{n}_{ _{i}}[ L_{i}^{2}| _{i}]+|[G_{i}|_{i}]| )I_{}^{_{i}}( L_{i};U_{i})},\]

_where \(I_{}^{_{i}}( L_{i};U_{i})=_{}(P_{ L_{i},U_{i}|_{i}}||P_{ L_{i}| _{i}}P_{U_{i}})\) is the (disintegrated) JS-information._

Jeffrey's divergence , which is defined as \(_{}(P||Q)_{}(P|| Q)+_{}(Q||P)\), is an \(f\)-divergence with the convex function \((x)=(x-1) x\). Since \(_{}(P||Q)_{}(P||Q)\), a Jeffrey's divergence-based bound can be derived as a corollary from Theorem 3.3 (and clearly, also from Theorem 3.1 due to the non-negativity of KL divergence).

Notably, Theorems 3.2-3.3 share similar expressions with Theorem 3.1, and deriving analogous corollaries to Corollaries 3.1-3.2 is feasible (see Appendix B.7).

In the proofs of Theorems 3.2 and Theorem 3.3, we continue to use \(x-ax^{2}\) to lower bound the inverse of conjugate functions \(\) and \((2-e^{-x})\) for SH-information and JS-information, respectively. However, due to the complexity of handling \((2-e^{-x})\) in the context of JS-information, we opted for a much rougher selection of the parameter \(a\) than in the proof of Theorem 3.1 for simplicity. Consequently, the constants in Theorem 3.3 are not optimal and can be refined with a more fine-grained analysis.

## 4 Extension to Unbounded Loss Difference

Previous bounds are typically applicable when dealing with bounded loss differences (albeit not necessarily within the range of \([-1,1]\)). This limitation arises from the fact that the function \(^{*-1}\) may fail to exist in Lemma 3.1 when \(f\) is unbounded, regardless of any non-trivial adjustments made to the range of \(t\). To overcome this limitation, we now extend our analysis to the case of unbounded loss differences by refining Lemma 3.1. One crucial modification to Lemma 3.1 involves defining the measurable function \(g=t^{*-1}_{|f| C}\) for some \(C 0\), where \(\) is the indicator function.

We present the following lemma tailored for unbounded functions.

**Lemma 4.1**.: _Let \(X\) be an arbitrary random variable, \(\) be a Rademacher variable, and \(t(-b,b)\). Assume that there exists a constant \(C 0\) such that \(^{*}\) is invertible in \((-bC,bC)\). If \(^{*}(0)=0\), then_

\[_{t(-b,b)}_{X,}[^{*-1}(t X) _{|X| C}] I_{}(X;).\]Note that \(^{*}(0)=0\) is satisfied by all divergences discussed in Section 3. Armed with Lemma 4.1, our proof strategy hinges on separately bounding the truncated terms \([G_{i}_{|X| C}]\) and \([G_{i}_{|X|>C}]\). To achieve this, we utilize Lemma 4.1 along with similar lower-bounding techniques as detailed in Section 3 to bound the first term. For the second term, we invoke Holder's inequality, drawing inspiration from prior works such as .

Before presenting the refined bound for unbounded loss difference, we introduce some additional notations. Define the \(L_{p}\) norm of a random variable \(X P\) as \(||X||_{p}\{(_{P}|X|^{p})^{}&p[1,),\\ *{ess\,sup}|X|p=,.\) where \(*{ess\,sup}|X|\{M:P(|X|>M)=0\}\) is the essential supremum. Furthermore, let the \(f\)-divergence generated by the convex function \(_{}(x)=|x-1|^{}\) be \(*{D}_{_{}}(P||Q)_{Q}[( -1)^{}]\). This divergence, which takes \(^{2}\)-divergence and total variation as special cases (with \(=2\) and \(=1\), respectively), and it has been utilized in previous information-theoretic generalization bounds . We denote the corresponding \(f\)-information of \(*{D}_{_{}}(P||Q)\) as \(I_{_{}}\).

Now, we are ready to present a MI-based bound, with straightforward extensions to other \(f\)-information measures.

**Theorem 4.1**.: _For constants \(C 0\), \(q 1\), and \(,[1,+]\) such that \(+=1\), denote \(_{1}=[ L_{i}^{2}_{| L _{i}| C}]+C|[G_{i}_{| L_{i}| C }]|)}\) and \(_{2}=(P(| L_{i}|>C))^{}||  L_{i}||_{q}\), we have_

\[|_{}()|_{C,q,,}_{i =1}^{n}(_{1};U_{i})}+_{2}[[ ]{I_{_{}}( L_{i};U_{i})})).\]

Since \(U_{i}\) is a Bernoulli random variable, according to [25, Lemma 1], we know that \(I_{_{}}( L_{i};U_{i})<1+2^{-1}\) for \(\) and remains bounded for other values of \(\) as well. Thus, Theorem 4.1 generally preserves the boundedness property of the original CMI bound unless \( L_{i}\) has an infinite \(L_{q}\)-norm. Additionally, due to truncation, \(_{1}C\) for any given \(C\), while \(_{2}\) heavily depends on the tail behavior of \( L_{i} P_{ L_{i}}\) (or \(G_{i} P_{U_{i}^{}}P_{ L_{i}}\) equivalently). We now discuss several common cases.

Bounded LossNotably, Theorem 4.1 also covers the bounded loss cases. For instance, if \(\) is bounded in \(\) a.s., setting \(C=1\) in Theorem 4.1 leads to \(_{2}=0\), directly recovering Theorem 3.1 for bounded loss. In fact, the choice of \(C=1\) might not necessarily be the optimal for \(\). Particularly, for certain \(C(0,1)\), we let \(=1\) and \(=\), then \(_{2}=|| L_{i}||_{}\) and \((I_{_{}}( L_{i};U_{i}))^{1/}=I_{}( L_{i };U_{i})\). Since \(*{D}_{}_{}}\) by Pinsker's inequality, this alternative choice holds potential for even tighter bounds than Theorem 3.1.

"Almost Bounded" LossWhen the loss function exhibits sub-Gaussian or sub-Gamma tail behaviors, the probability \(P(|G_{i}|>C)\) decays rapidly. By carefully selecting the threshold \(C\), the dominance of the first term in the bound can be expected. Assume the loss difference \( L_{i}\) is sub-Gaussian with certain variance proxy, say, \(\). Setting \(C=\) and \(q=2\), we have \(P(| L_{i}|>C) 2e^{-1}\) and \(|| L_{i}||_{q}\). Consequently, each term in the summation of Theorem 4.1 simplifies to \((;U_{i})}+(2e)^{}}[]{I_{_{}}(  L_{i};U_{i})})\). However, the relationship between \(I( L_{i};U_{i})\) and \(I_{_{}}( L_{i};U_{i})\) is not clear beyond the cases of \(=1\) and \(=2\) (corresponding to total variation and \(^{2}\)-information, respectively). Studying the overall behavior of the bound represents an intriguing direction for future research. Furthermore, consider the alternative case with \(C=0\) and \(q=1\), where the first term in Theorem 4.1 becomes zero. The second term, using \(|| L_{i}||_{}\) for sub-gaussian random variables, becomes \((}[]{I_{_{}}(  L_{i};U_{i})})\). Compared to existing MI bounds, e.g., \((;U_{i})})\), we know from \(*{D}_{}(P,Q)_{}(P||Q)}\) and \(*{D}_{}(P||Q)^{2}(P||Q)\) that \(I_{_{1}}( L_{i};U_{i});U_{i})} }( L_{i};U_{i})}\), suggesting that some \((1,2)\) may yield a tighter bound than MI.

Heavy-tailed LossHeavy-tailed losses, although lacking universally agreed definitions, typically refer to the cases where the moment-generating function (MGF) does not exists (away from \(0\)) .

Remarkably, our Theorem 4.1 remains meaningful for such losses since it does not rely on the existence of the MGF of the loss function. Specifically, without any additional knowledge about the tail behavior of \( L_{i}\), we have the following result from Theorem 4.1.

**Corollary 4.1**.: _Under the conditions in Theorem 4.1, let \(=\), we have_

\[|_{}()|_{i=1}^{n}(^{ }+^{})(;U_{i})})^{}(|| L_{i}||_{ 1}^{}|| L_{i}||_{q}[]{I_{_{}}( L_ {i};U_{i})})^{}.\]

It's worth noting that as \( 0\) (i.e. \(q 1\)), we can see that \((^{}+^{}) 1\). Consequently, in Corollary 4.1, the bound simplifies to \(_{i=1}^{n}(|| L_{i}||_{}[]{I_{_ {}}( L_{i};U_{i})})\). This is an extension of [25, Theorem 3], incorporating individual techniques  and loss-difference methods  in the supersample setting. Notably, our Corollary 4.1 improves upon [25, Theorem 3], as \( 0\) may not necessarily be the optimal choice. Furthermore, it is also intriguing to investigate whether our Corollary 4.1 can recover [29, Corollary 5].

## 5 Numerical Results

In this section, we conduct an empirical comparison between our novel conditional \(f\)-information generalization bounds and several existing information-theoretic generalization bounds. Our experimental setup closely aligns with the settings in . In particular, we undertake two distinct prediction tasks as follows: 1) _Linear Classifier on Synthetic Gaussian Dataset_, where we train a simple linear classifier using a synthetic Gaussian dataset; 2) _CNN and ResNet-50 on Real-World Datasets_. The second task follows the same deep learning training protocol as in , involves training a \(4\)-layer CNN on a binary MNIST dataset ("\(4\) vs \(9\)")  and fine-tuning a ResNet-50 model , pretrained on ImageNet , on CIFAR10 . In both tasks, we assess prediction error as our performance metric. That is, we utilize the zero-one loss function to compute generalization error. Note that during training, we use the cross-entropy loss as a surrogate to enable optimization with gradient-based methods.

The comparison of generalization bounds mainly focuses on those presented in Section 3. Specifically, we consider the oracle bounds, given in Theorem 3.1, Theorem 3.2, and Theorem 3.3, which we denote as _CMI(Oracle)_, _CSHI(Oracle)_, and _CJSI(Oracle)_, respectively. Given that the

Figure 2: Comparison of bounds on synthetic dataset, MNIST (“\(4\) vs \(9\)”), and CIFAR10. (a-b) Linear classification for two-class and ten-class data. (c-d) Dynamics of generalization bounds as dataset size changes. (e-f) Dynamics of generalization bounds during SGLD training.

squared Hellinger-information is a tighter measure than MI, and the constants in Theorem 3.2 exhibit less pessimism compared to those in the JS-information bound, we introduce two additional squared Hellinger-information bounds for comparison. The first one, akin to Corollary 3.2, is \(_{}()_{i=1}^{n}[ L_{i}^{2}]+2[_{}(P_ { L_{i}|U_{i},_{i}},P_{ L_{i}|_{i}})] )I_{^{2}}^{}( L_{i};U_{i})}\) (cf. Corollary B.1 in Appendix B.7), denoted as _CSHI(Var)_. The second one adopts a more pessimistic choice by replacing \([ L_{i}^{2}]\) with the constant \(1\), labeled as _CSHI(Worst)_. Additionally, we include three previous information-theoretic generalization bounds as baselines: the original ld-CMI bound in (24, Theorem3.1) (_ld-CMI_), the sharpness-based single-loss MI bound in (24, Theorem 4.5) (_Sharpness_), and the binary KL-based CMI bound in (23, Theorem 5) (_Binary kl_). Further details on experimental settings are available in the Appendix D.

The final results are shown in Figure 2. First, we observe that for the linear classifier on the two-class data (Figure 1(a)), most bounds are close to the exact generalization error, with _Sharpness_ being the tightest. Note that the sharpness-based MI bound in (24, Theorem 4.5) is applicable solely for zero-one loss. As the prediction task becomes more complex (Figures 1(b)-1(f)), our squared Hellinger-information-based bounds are the tightest. Specifically, the squared Hellinger-information-based bound is always tighter than the CMI bound in the oracle type. Notably, _CSHI(Var)_ also outperforms all other \(f\)-information-based bounds in these cases, and even the pessimistic _CSHI(Worst)_ bound outperforms them, except for _Sharpness_, in Figures 1(c)-1(d). Additionally, by comparing _CMI(Oracle)_ and _ld-CMI_, we see significant potential for improving the original ld-CMI bound. Finally, although the JS-information bound contains relatively pessimistic constants, it still outperforms some MI-based bounds in many cases (e.g., Figures 1(c)-1(e)). These observations suggest that considering alternative \(f\)-information rather than MI is indeed necessary for studying generalization.

## 6 Other Related Works and Limitations

In addition to the studies mentioned previously, there have been recent developments in information-theoretic generalization bounds in the supersample setting. For example, (54, 44) propose a "leave-one-out" (LOO) construction of supersamples, reducing the size of the supersample from \(2n\) to \(n+1\). Additionally, (55) constructs a "neighboring-hypothesis" matrix, enabling the derivation of a hypothesis-conditioned CMI bound. The key component in this bound is the product of the new CMI term and an algorithmic stability parameter. Furthermore, significant progress has been made in understanding MI or CMI as generalization measures in stochastic convex optimization (SCO) problems. Specifically, (56, 57, 58) all point out that MI or CMI bounds can be non-vanishing in some SCO examples, while the exact generalization error is vanishing. Notably, (58) discovers a "CMI-accuracy tradeoff" phenomenon in certain scenarios: For the generalization error to be upper bounded by a vanishing parameter, the original hypothesis-based CMI must be lower bounded by the reciprocal of this parameter (or even the reciprocal of its square), indicating that the CMI bound cannot characterize the learnability of such problems. Although this limitation is currently only observed for hypothesis-based CMI, it is anticipated to hold in other variants of CMI, such as ld-CMI.

Our work also contributes to understanding the potential looseness of previous CMI generalization bounds. However, the current state of our bounds cannot explain or resolve the limitation presented in (58) in a non-trivial way. Nonetheless, our framework provides some intuition that, as long as a gap exists between \(^{*-1}\) and \(x-ax^{2}\) (see Figure 1(Right)), there will always be room for constructing counterexamples that challenge information-theoretic measures, although such examples may be rare. Another limitation of this work is the lack of a high-probability generalization guarantee. There are two potential directions to address this: either by applying similar techniques to the framework in (59), which is the PAC-Bayesian counterpart of the CMI bound, or by combining it with recent information-bottleneck generalization bounds (60, 61).