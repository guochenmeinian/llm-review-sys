# Globally solving the Gromov-Wasserstein problem for point clouds in low dimensional Euclidean spaces

Globally solving the Gromov-Wasserstein problem for point clouds in low dimensional Euclidean spaces

Martin Ryner

Vironova AB, Stockholm, Sweden

Division of Numerical Analysis, Optimization and Systems Theory,

Department of Mathematics,

KTH Royal Institute of Technology, Stockholm, Sweden

martin.ryner@vironova.com, martinrr@kth.se

Jan Kronqvist

Division of Numerical Analysis, Optimization and Systems Theory,

Department of Mathematics,

KTH Royal Institute of Technology, Stockholm, Sweden

jankr@kth.se

Johan Karlsson

Division of Numerical Analysis, Optimization and Systems Theory,

Department of Mathematics,

KTH Royal Institute of Technology, Stockholm, Sweden

johan.karlsson@math.kth.se

###### Abstract

This paper presents a framework for computing the Gromov-Wasserstein problem between two sets of points in low dimensional spaces, where the discrepancy is the squared Euclidean norm. The Gromov-Wasserstein problem is a generalization of the optimal transport problem that finds the assignment between two sets preserving pairwise distances as much as possible. This can be used to quantify the similarity between two formations or shapes, a common problem in AI and machine learning. The problem can be formulated as a Quadratic Assignment Problem (QAP), which is in general computationally intractable even for small problems. Our framework addresses this challenge by reformulating the QAP as an optimization problem with a low-dimensional domain, leveraging the fact that the problem can be expressed as a concave quadratic optimization problem with low rank. The method scales well with the number of points, and it can be used to find the global solution for large-scale problems with thousands of points. We compare the computational complexity of our approach with state-of-the-art methods on synthetic problems and apply it to a near-symmetrical problem which is of particular interest in computational biology.

## 1 Introduction

Many important applications in machine learning deal with comparing sequences, images, and higher dimensional data, where the data is unstructured and not directly comparable. In physics, chemistry, biology, music, and linguistics, objects with greatly different properties often appear in symmetrical variations characterized by concepts such as isomerisms, chirality, harmonies, and alternations. Understanding, and being able to analyze, these types of variations can be truly critical as some variations in chemicals and biologicals may be toxic or even lethal. The Gromov-Wasserstein framework [15; 16; 17], has shown to be a powerful approach for comparing and matching such data, as it is invariant to translations and rotation. The Gromov-Wasserstein framework has, for example, been successfully applied to domain adaptation , graph matching , metric alignment , single-cell alignment , and word embedding .

The task of evaluating the Gromov-Wasserstein problem is in general considered to be intractable. Typically, the computational burden grows exponentially with the number of points describing the compared objects. In fact, a Gromov-Wasserstein problem can be formulated as quadratic assignment problem (QAP) [14; 5; 4], which is known to be NP-Hard. Naturally, there has been plenty of research on local and approximate methods for solving Gromov-Wasserstein and QAP problems [20; 23; 22; 27; 2; 24; 25]. However, objects containing symmetries or repeated patterns are particularly challenging for local optimization methods and may lead to significant errors in the estimated discrepancy as matching such objects with local optimization methods may accidentally find the sub-optimal reflections and rotations. The inability to detect such phenomena can have a great impact on the discovery of isomerisms and subsequently attributes of crucial importance.

In this paper, we develop a rigorous method for globally optimizing Gromov-Wasserstein problems by calculating a sequence of iteratively improving upper- and lower bounds. We consider a general class of Gromov-Wasserstein discrepancy problems where the points, representing the objects, belong to a Euclidean space. We show that such Gromov-Wasserstein problems can be formulated exactly as low-rank QAPs. We build upon this low-rank QAP representation to develop an algorithm that scales well with the number of points. The proposed algorithm can be characterized as a so-called cutting plane method [12; 10] where we solve a sequence of relaxed problems that are iteratively strengthened by generating and accumulating valid linear inequality constraints, _i.e.,_ cutting planes. The optimum of the relaxed problem provides a valid lower bound for the optimum of the Gromov-Wasserstein problem in each iteration. By solving a computationally cheap optimal transportation problem [18; 26; 6], we obtain both an upper-bound and a new cutting plane to strengthen the relaxation. We prove convergence for the proposed algorithm, and present a computational study that clearly shows the algorithm's efficiency and that the performance scales well with the number of points.

The main contribution of the paper can be summarized as:

* We identify a general class of Gromov-Wasserstein problems, for point clouds embedded in low dimensional Euclidean spaces, that can be exactly represented as a concave low-rank QAP. In particular, mappings of images fits well within our framework.
* We develop a method for solving this class of Gromov-Wasserstein problems by solving a sequence of alternating sub-problems, which are either low-dimensional or linear.
* We prove that the proposed algorithm converges to a global optimal solution. The algorithm produces an optimality certificate in each iteration, in the form of upper- and lower bounds, which informs us of the potential suboptimality if the algorithm is terminated early.
* We present a numerical study, showing the efficiency of the proposed algorithm by comparing to other global optimization methods. We also illustrate the importance of globally solving Gromov-Wasserstein problems on a problem in computational biology.

In Section 2 we introduce the Gromov-Wasserstein problems and how it can be written as a QAP. In Section 3 we identify a class of Gromov-Wasserstein discrepancy problems that can be written as a concave relaxed QAPs problem, and in Section 4 we present the main methodology and an algorithm for solving this class of problems. Finally, in Section 5 we present numerical results and an application in computational biology.

## 2 The Gromov-Wasserstein discrepancy problem

Let \(x_{1},x_{n}\) and \(y_{1},y_{n}\) be two sets of points and consider the problem of finding an assignment \(\) between the point sets such that the pairwise distances \(d_{}(x_{i},x_{i^{}})\) and \(d_{}(y_{(i)},y_{(i^{})})\) are as close as possible for \(i,i^{}=1,,n\), where \(d_{}\) and \(d_{}\) represents a notion of distance on the sets \(\) and \(\), respectively. This can be formulated as the discrete Gromov-Wasserstein discrepancy problem

\[_{ P}\ _{i,i^{},j,j^{}=1}^{n}(d_{ }(x_{i},x_{i^{}})-d_{}(y_{j},y_{j^{}}))^{2} _{i,j}_{i^{},j^{}},\] (1)

and where the assignment \(\) is represented by a permutation matrix \(\) and \(P\) is the set of all \(n n\) permutation matrices. The corresponding relaxed problem, where instead \(\) is in the set of doubly stochastic matrices, denoted by \(\), is often referred to as the Gromov-Wasserstein problem . In these formulations we note that

\[_{i,i^{},j,j^{}=1}^{n}(d_{}(x_{i},x_{i ^{}})-d_{}(y_{j},y_{j^{}}))^{2}_{i,j}_{i^{ },j^{}}\] \[=_{i,i^{},j,j^{}=1}^{n}(d_{}(x_{i},x_{ i^{}})^{2}-2d_{}(x_{i},x_{i^{}})d_{}(y_{j},y_{j^ {}})+d_{}(y_{j},y_{j^{}})^{2})_{i,j}_{i^{ },j^{}}\] \[= C_{x},C_{x}-2 C_{x}, C_{y} + C_{y},C_{y}\]

where \(C_{x}=[d_{}(x_{i},x_{i^{}})]_{i,i^{}=1}^{n}\), \(C_{y}=[d_{}(y_{j},y_{j^{}})]_{j,j^{}=1}^{n}\), and \(,\) denotes the standard (Frobenius) inner product. Since the first and third sums are independent of \(\), solving the discrete Gromov-Wasserstein problem (1) is the same as solving a quadratic assignment problem (QAP) on a simplified Koopmans-Beckmann form , namely as

\[_{ P}- C_{x}, C_{y}+ ( C_{x},C_{x}+ C_{y},C_{y}).\] (2)

This problem is in general NP-hard, and the number of variables scales with the number of data points, making (2) computationally intractable for problems of relevant size. Here, we focus on instances where the matrices \(C_{x},C_{y}\) are positive definite and low rank. By utilizing this structure, we develop an algorithm that is guaranteed to find a globally optimal solution and scales well with the number of points.

## 3 The Gromov-Wasserstein problem and low rank QAP

An important special case of the Gromow-Wasserstein problem, considered in [20; 22], is when the point clouds belong to the Euclidean space and the squared Euclidean distance is used as discrepancy. That is, when the set of points are \(x_{1},x_{n}^{_{x}}\) and \(y_{1},y_{n}^{_{y}}\), which we represent by the matrices

\[X=(x_{1},x_{2},,x_{n})^{_{x} n},  Y=(y_{1},y_{2},,y_{n})^{_{y} n}.\]

In this case it can be noted that the distance matrices \(C_{x}\) and \(C_{y}\) has a rank bounded by \(_{x}+2\) and \(_{y}+2\), respectively, which can be seen from the identities

\[C_{x}=(\|x_{i}-x_{j}\|_{2}^{2})_{i,j=1}^{n}=m_{x}^{T} -2X^{T}X+m_{}^{T},\] (3a) \[C_{y}=(\|y_{i}-y_{j}\|_{2}^{2})_{i,j=1}^{n}=m_{y}^{T} -2Y^{T}Y+m_{}^{T},\] (3b)

where \(m_{x}=(\|x_{1}\|^{2},\|x_{2}\|^{2},,\|x_{n}\|^{2})^{T}\), \(m_{y}=(\|y_{1}\|^{2},\|y_{2}\|^{2},,\|y_{n}\|^{2})^{T}\), and \(^{n 1}\) is a column vector of ones. This observation was also used in  for formulating the Gromov-Wasserstein problem as a quadratic problem of rank \((_{x}+2)(_{y}+2)\) and developing fast algorithms for the problem. However, the rank can be even further reduced and the corresponding Gromov-Wasserstein problem can be formulated as a QAP problem of rank \(_{x}_{y}\)[25, Lemma 4.2.3] (cf. [20; Proposition 1]).

**Proposition 1**.: _[_25_, Lemma 4.2.3]_ _Let \(\) be a doubly stochastic matrix and the matrices \(C_{x}\) and \(C_{y}\) given by (3), then it holds that_

\[ C_{x}, C_{y}= 2X Y^{T},2X Y ^{T}+ L,+2^{T}m_{y}^{T}m_{x},\]

_where \(L=2nm_{x}m_{y}^{T}-4m_{x}^{T}Y^{T}Y-4X^{T}Xm_{y}^{T}\)._

Proof.: The proposition follows by straightforward computations, where the expressions are simplified using \(==^{T}\). See the appendix for the proof.

Hence, the low rank QAP formulation of the discrete Gromov-Wasserstein problem can be stated as

\[_{ P}- 2X Y^{T},2X Y^{T}-  L,+c_{0}\] (4)

where \(L=2nm_{x}m_{y}^{T}-4m_{x}^{T}Y^{T}Y-4X^{T}Xm_{y}^{T}\), and \(c_{0}=( C_{x},C_{x}+ C_{y},C_{y}-41^{T}m_{y} ^{T}m_{x})/2\). The Gromov-Wasserstein problem can also be rewritten similarly and formulated as

\[_{ P}- 2X Y^{T},2X Y^{T}-  L,+c_{0},\] (5)

and since the objective function is concave, any optimal solution of (4) is also an optimal solution of the relaxed problem.

**Proposition 2**.: _Any optimal solution of the discrete Gromov-Wasserstein problem (4), is also an optimal solution to the Gromov-Wasserstein problem (5). Conversely, problem (5) always has an optimal solution in one extreme point,1 and any optimal extreme point to (5) is also an optimal solution to (4)._

Proof.: Since (5) is the minimization of a concave objective function over a convex sets \(\), it attains the optimal value in an extreme point of the feasible set. Since the permutation matrices are the extreme points to the doubly stochastic matrices, i.e., \(P=()\), (5) attains its minimum on \(P\). Further, the set of points in \(P\) for which (5) attains its minimum are the optimal solutions of (4). To show the converse statement, note that a minimum exists since \(\) is compact and the objective function is continuous. Further, since the objective function is concave, an optimum must be at an extreme point. Finally, since the extreme points of \(\) is the permutation matrices \(P\), any optimal extreme point of (5) is also feasible and optimal to (4). 

In the next section we will propose a methodology and an algorithm for solving this problem.

## 4 A cutting plane algorithm utilizing the low rank structure

By Proposition 2, we know that an optimal solution to the discrete Gromov-Wasserstein problem (4) can be obtained by solving the relaxed problem (5). However, the relaxed problem (5) is still a high-dimensional non-convex QP, which is NP-hard . The high dimensionality can, in particular, be a limiting factor in solving the problem. For example, it is known that the performance of spatial branch-and-bound, one of the main approaches for globally optimizing nonconvex problems , can scale poorly with the number of variables. Thus, directly optimizing either (1) or (5) by spatial branch-and-bound is not computationally tractable for larger instances. Our idea is to use the low-rank formulation of the Gromov-Wasserstein problem and perform the optimization in a projected subspace of dimension \(_{x}_{y}+1\) by solving a sequence of relaxed problems.

First, we note that problem (5) can be written as

\[_{W^{_{x}_{y}},w, } -\|W\|_{F}^{2}-w+c_{0}\] (6a) subject to \[W=2X Y^{T},\ w= L,.\] (6b)

Equivalence of problems (5) and (6) is shown by simply inserting the expressions for \(w\) and \(W\) into the objective function. Next, we project out the \(\) variables, and we define the feasible set in the \((w,W)\)-space as \(=_{W,w}(W^{_{x}_{y}},w ,\ |\ W=2X Y^{T},\ w= L, )\).

Constructing an H-representation of the polytope \(\), i.e., representing it by linear constraints of the form \( Z_{r},W+_{r}w_{r}\), is not trivial and the number of constraints can grow exponentially with the number of data points. Therefore, we propose an algorithm based on a cutting plane scheme to optimize over \(\).

Instead of directly optimizing the objective in (6a) over the feasible set \(\), which we don't have a tractable representation for, we relax the problem as

\[_{W^{d_{x} t_{y}},w} -\|W\|_{F}^{2}-w+c_{0}\] (7a) subject to \[ Z_{r},W+_{r}w_{r},\;r=1,,N.\] (7b)

The linear constraints (7b) are supporting hyperplanes of the feasible set \(\), which we will generate iteratively. The goal is to force the minimizer of problem (7) into the feasible set \(\) by using relatively few linear constraints. Keep in mind, we don't need a full representation of set \(\), we only need to capture the shape of \(\) in some areas of interest, e.g., the constraints defining the faces of \(\) at the optimal solution of problem (6) would suffice. The main advantage of the relaxation in problem (7) is that it contains far fewer variables than both problems (5) and (6), and the dimensionality is independent of the number of data points. Problem (7) can, therefore, be solved much more efficiently, especially in early iterations when the number of constraints is low. We will show that the constraints can be determined, as needed, by solving optimal transport problems. Based on this, we will develop an iterative approach that sequentially solves problem (7) and adds a constraint until the the solutions is the same as (6).

To initialize the search we determine a bounding box of \(\) and use this to define a set of constraints (7b). The bounding box is determined by the (elementwise) minimum and maximum of the variables \(w\) and \(W\) given by

\[_{}\;2(X Y^{T})_{i,j}  W_{i,j}_{}\;2(X Y^{T})_{i,j} i=1,,_{x};\;j=1,,_{y}\] (8a) \[_{} L,  w\;_{} L,,\] (8b)

which can each be computed efficiently by solving a standard optimal transport problem. Initializing the set of constraints by the bounding box ensures that (7) is well-defined and bounded.

If the minimizer of problem (7) is within \(\), then we can stop as the solution is optimal for (6).2 Otherwise, we improve the outer approximation of \(\) by adding new a constraint defined by \(Z_{N+1}\), \(_{N+1}\) and \(_{N+1}\). Let \((w_{N},W_{N})\) be the current optimal solution of (7), and assume that \((w_{N},W_{N})\), then we form a new constraint, a so-called cutting plane, that excludes \((w_{N},W_{N})\) from the feasible set of (7).

We form a new constraint based on the gradient of the objective function (7a), which is given by

\[_{(w,(W)T)}(-\|W\|_{F}^{2}-w,)=(-1,-2 (W)^{T}).\]

By letting \(_{N+1}=1\) and \(Z_{N+1}=2W_{N}\), the hyperplane defining the new constraint will have the (negative) gradient in the optimum \((w_{N},W_{N})\) as normal vector. Then we select \(_{N+1}\) such that the new constraint forms a supporting hyperplane of \(\) (7b). This can be found by solving the following optimal transport problem

\[_{N+1}:=_{W^{d_{x} t_{y}},w,}}  Z_{N+1},W+_{N+1}w=_{ }\; 4X^{T}W_{N}Y+L,.\] (9) subject to \[W=2X Y^{T},\;w= L,\]

When solving this problem, we also obtain a solution \(_{N}\) which is a doubly stochastic matrix (generically also a permutation matrix), which gives an upper bound for (6) and a candidate for the optimal solution. In the following subsection, we prove that the that algorithm, described in Algorithm 1, converges to a globally optimal solution.

A geometrical illustration of the algorithm is given in Figure (1). For illustrative purposes, we have used one-dimensional data resulting in a two-dimensional problem in the \((W,w)\)-space. The data sets consist of \(6\) points each where one of the data sets has a reflective symmetry. This results in two global optima and \(6!\) projected permutations in \(P\). The first solution of (7) is located at one of the corners of the bounding box and marked with a "1" (the subsequent solutions are marked "2" "5"). The infeasible point "1" is excluded from the search space by a cutting plane (red line, marked with an "A"). Following the same procedure we obtain point "2", and cutting plane "B". Adding further cutting planes excludes "3" and subsequently "4", resulting in the feasible and optimal point "5". Note that the cutting planes from iterations 3 and 4 almost overlap the cutting planes "A" and "B", since the gradients in the points 1 and 3 are very similar (the same for points 2 and 4).

### Proof of convergence of Algorithm 1

The main result considering convergence is presented in the following theorem.

**Theorem 1**.: _The gap between the upper bound and lower bound in Algorithm 1 converges to \(0\) (if the tolerance is \(=0\))._

Proof.: Consider the \(N\)th iteration in Algorithm (1), let \((w_{N},W_{N})\) be an optimal solution to (7), and \(_{N}\) is an optimal solution to (9) with corresponding points \((,)=( L,_{N},2X_{N}Y^{T})\), in the \((w,W)\)-space. Assume that the gap in the objective function between those two points is

\[_{N}=\|W_{N}\|_{F}^{2}+w_{N}-\|\|_{F}^{2}-.\] (10)

The new constraint is then defined by\( Z_{N+1},W+w_{N+1}\) where \(Z_{N+1}=2W_{N}\) and \(_{N+1}=2 W_{N},+\), and thus for any point \((w,W)\) that satisfy the constraint it must hold that

\[0 2 W_{N},-W+-w.\]

Figure 1: Left image: An illustrative example of the method on one-dimensional data. Right image: The area around the two global optima highlighting the sequence of optimal extreme points in the approximate cover and generation of cutting planes.

By substituting \(\) from (10), we obtain

\[_{N}  2 W_{N},-W-w+\|W_{N}\|_{F}^{2}+w_{N}-\| \|_{F}^{2}\] \[=w_{N}-w+2 W_{N},W_{N}-W-\|-W_{N}\|_{F}^{2}\] \[ w_{N}-w+2 W_{N},W_{N}-W\] \[(|w_{N}-w|^{2}+\|W_{N}-W\|_{F}^{2})^{1/2}(1+4\|W_{N}\|_{F}^{2} )^{1/2}\]

where we in the last step have used the Cauchy-Schwarz inequality.

For any iteration number \(N+k\) with \(k>0\), we have that \((w_{N+k},W_{N+k})\) is feasible for \( Z_{N+1},W+w_{N+1}\), and thus the Euclidean distance between \((w_{N},W_{N})\) and \((w_{N+k},W_{N+k})\) is at least \(_{N}/(1+4\|W_{N}\|_{F}^{2})\). If the gap in the algorithm does not converge to \(0\), then there is an \(>0\) for which \(_{N}\) for all \(N\) and thus the distance between any two points in the sequence \(\{(w_{N},W_{N})\}_{N}\) is bounded from below by \(/(1+4\{\|W\|_{F}^{2} W(8)\})\). However, since the infinite sequence of points \(\{(w_{N},W_{N})\}_{N}\) belong to a bounded set defined by (8), there must be a convergent subsequence, which contradicts that there is a positive lower bound on the distance between any two points. 

From Theorem 1 the gap between the upper and lower bound converges to zero, and thus Algorithm 1 converges to a globally optimal solution.

### Considerations when solving the relaxed problem

Problem (7) minimizes a concave function over a convex set. Thus, the solution is located in the extreme points of the convex set, i.e., the outer approximation of \(\). The standard approach to solve such problems is by branch and bound methods. However, the low dimension and sequential generation of constraints make it viable to search among the extreme points for an optimal solution.

To simplify notation, we define \(x^{T}=w&(W)^{T}^{r}\) where \(r:=_{x}_{y}+1\). Then we can write (7b) on the form \(A_{k}x b_{k}\). Note that, by construction, none of the constraints are strongly redundant as every constraint is satisfied with equality for a permutation. As the constraints are added sequentially, it is actually easy to compute the new extreme points by keeping track of previous extreme points as described in the following proposition.

**Proposition 3**.: _Assume that the extreme points \(\{x_{k}\}_{k}\) of the convex set described by \(Ax b\) are known. When adding a constraint \(A_{N}^{T}x b_{N}\), the additional extreme points are linear combinations of pairs of existing extreme points \(x_{k_{1}}\) and \(x_{k_{2}}\) both satisfying the same \(r-1\) constraints with equality and \(A_{N}^{T}x_{k_{1}} b_{n}\) and \(A_{N}^{T}x_{k_{2}}>b_{n}\) so that the combination satisfies \(A_{N}^{T}( x_{k_{1}}+(1-)x_{k_{2}})=b_{N}\)._

Proof.: This is done by counting the number of constraints satisfied with equality. See the appendix for the proof. 

Especially, in lower dimensions, e.g., with two or three dimensional data, this approach of keeping track of all extreme points and calculating new extreme points after adding a constraint can be very efficient for solving problem (7). More details of this method is provided in the appendix. In the numerical results, we present results where problem (7) is solved both by this extreme point search and the spatial branch and bound method in Gurobi.

## 5 Numerical results

### Computational efficiency

In this section we compare the time to solve the problem up to an accuracy measured in relative error with different methods: Algorithm 1 when (7) is solved with the extreme point method as described in section 4.2, Algorithm 1 when (7) solved using Branch & bound using Gurobi 10.0 , MILP1 formulation in  implemented in Gurobi and finally when (6) is directly solved using Gurobi. The MILP1 formulation can handle a larger class of problems, but is reported to handle very few dimensions. All computations were performed using Matlab on an Intel i5 2.9 GHz PC. The linear optimal mass problem (9) was solved using the package  which is based on the network simplex . The model problems tested are evenly distributed points in a unit disc or ball which we denote \(\), and normally distributed points \((0,)\). We denote \(_{1}:=(0,I)\), \(_{2}:=(0,(1,1,))\) and \(_{3}:=(0,(1,,))\). See Table 1 for numerical results. Some notes on the results

1. On 2-dimensional data (\(_{x}=_{y}=2\)), the extreme point method is particularly efficient.
2. For problems that need many extreme points (\(>10^{6}\)), which depends on the data itself, the handling of extreme points becomes the driver of computational cost.
3. Problems mainly containing reflections (e.g. \(_{3}\)) are easier to solve than those with room for rotations.
4. Directly solving (6) with Gurobi was not feasible for problems with \(n 500\).

### Comparison with local search method

We compare the results using the proposed method to a local search method provided in the Git-hub repository for . The entropy regularization parameter is set to \(0\), and the method is run with random initializations (including the first lower bound  used with success in ) until the relative error to the global optimum is less than a specific tolerance \(\). The problems are the same as in the previous section. Note here that in the local search methods we need an oracle in order to determine when we have reached a given performance level (which of course is not available in practice), whereas we in the proposed method computes upper and lower bounds.

Results for two and three dimensional data (\(_{x}=_{y}=2,3\)) are presented in Table 2. The results show that the proposed method performs better than multi-starting the local method when \(_{x}=_{y}=2\). For the matching of 2-dimensional data to 3-dimensional data, the local search method is surprisingly fast suggesting that the problems is of a completely different nature than when 2-d data is matched to 2-d data or 3-d data is matched to 3-d data.

   Type & \(n\) & \(_{x},_{y}\) & Rel. & Algorithm 1 [s] & MILP1 [s] & (6) B\&B [s] \\  & & & error & Extreme point / B\&B & & \\  \(\) & 10 & 2,2 & \(10^{-8}\) & 0.14 (0.07-0.3) / 21 (6-47) & 39 (11-58) & 0.15 (0.14-0.16) \\ \(\) & 100 & 2,2 & \(10^{-8}\) & 0.48 (0.3-0.7) / 86 (52-107) & - & 25 (19-39) \\ \(\) & 500 & 2,2 & \(10^{-8}\) & 11 (9-16) / 408 (269-511) & - & - \\ \(\) & 1000 & 2,2 & \(10^{-8}\) & 69 (54-85) / 576 (389-1059) & - & - \\ \(\) & 2000 & 2,2 & \(10^{-8}\) & 460 (313-653) / - & - & - \\  \(\) & 10 & 2,3 & \(10^{-8}\) & 1.8 (1.2-2.4) / 133 (45-296) & 105 (49-147) & 2.4(1.8-3.4) \\ \(\) & 100 & 2,3 & \(10^{-8}\) & 278 (99-813) / - & - & 172 (133-221) \\ \(\) & 500 & 2,3 & \(10^{-8}\) & 9568 / - & - & - \\  \(_{1}\) & 10 & 2,3 & \(10^{-8}\) & 0.51 (0.39-0.65) / 708 (233-1184) & 146 (66-227) & 3 (2.6-4.0) \\ \(_{1}\) & 100 & 2,3 & \(10^{-8}\) & 86 (20-275) / - & - & 95 (73-116) \\ \(_{1}\) & 500 & 2,3 & \(10^{-5}\) & 5310! / - & - & - \\  \(_{2}\) & 10 & 3,3 & \(10^{-2}\) & 1.8 (0.7-3.2) / 142 (73-210) & 117 (71-163) & 0.2(0.1-0.3) \\ \(_{2}\) & 100 & 3,3 & \(10^{-2}\) & 36 (22-55)/ - & - & 45(36-65) \\ \(_{2}\) & 500 & 3,3 & \(10^{-2}\) & 436 (228-862) / - & - & - \\  \(_{3}\) & 10 & 3,3 & \(10^{-2}\) & 1.2 (0.5-2.3) / 22 (11-43) & 72 (43-94) & 0.2(0.1-0.3) \\ \(_{3}\) & 100 & 3,3 & \(10^{-2}\) & 7 (5-8)/ 91 (76-111) & - & 10 (9-12) \\ \(_{3}\) & 500 & 3,3 & \(10^{-2}\) & 11 (9-16) / 161 (104-226) & - & - \\ \(_{3}\) & 1000 & 3,3 & \(10^{-2}\) & 25 (22-29) / 176 (149-224) & - & - \\ \(_{3}\) & 2000 & 3,3 & \(10^{-2}\) & 93 (91-100) / 578 (429-691) & - & - \\   

Table 1: Computational efficiency. Computational time on the format [mean (low - high)] from 5 repeats for various problem geometries, dimensions and sizes, for the proposed extreme point method, the same method using branch and bound (B&B), the problem formulation MILP1  and finally (6) implemented in Gurobi via Matlab interface. An ”-” indicates that the problem timed out, in such a way being incomparable with the proposed method. An ”!” indicates that the problem reached \(10^{4}\) iterations and stopped to the accuracy indicated.

Note that we have chosen to compare with the method from  rather than . This is since we have not optimized the optimal transport computations using the low rank structures in the problem. If we optimized the computations in this way, i.e., as in , we expect to get similar improvement as in  compared with .

### Application to symmetrical data for morphological analysis

In this example we investigate the impact of correctly evaluating the Gromov-Wasserstein discrepancy compared to estimating it by local search. As a test case, we examine the ability to classify Adeno Associated Viral (AAV) particles based on the Gromov-Wasserstein discrepancy on image data originating from transmission electron microscopy, hence \(_{x}=_{y}=2\). AAV particles are nearly round viral particles with multiple near-rotational symmetries as illustrated in Figure 2. By sampling \(n=500\) positions on each AAV particle proportional to the protein density, the point sets from pairs of particles \(X_{i}\) and \(X_{j}\) can subsequently be compared using the Gromov-Wasserstein discrepancy.

Computing the Gromov-Wasserstein discrepancy between all objects in a large set \(=\{X_{i}\}_{i}^{N}\) is tedious. Therefore, one may consider calculating the discrepancy to a subset of the objects that are well distributed under the Gromov-Wasserstein discrepancy. To find such a subset without actually calculating all pairs of discrepancies, we use a greedy approach by defining the index subset \(S_{k}=\{s_{i}\}_{i=1}^{k}\) by selecting the first object arbitrarily and then let the set grow by

\[S_{k+1}:=\{S_{k},*{argmax}_{i S_{k}}_{j S_{k}}d_{ GW}(X_{i},X_{j})\}.\] (11)

In this way all objects are closer than a tolerance to an object in the subset, and the way the subset is produced generates a monotonically decreasing tolerance. By using this procedure, every object obtains a feature vector of distances to the objects indexed by \(S_{k}\). Next the feature vectors are used as input to a K-means clustering and classification quality in terms of purity, adjusted rand index and normalized mutual information compared to an expert evaluation is presented in Figure 3.

The example shows that if the data contains symmetries, local search methods may get stuck on permutations that are locally optimal but that are far from globally optimal, see Figure 2. Thus the discrepancy obtained from local search is inappropriate to use as a discriminating feature, as shown in the lower performance in the classification illustrated in Figure 3. In the left subfigure of Figure 2, self-similarities are visited in the proposed method, and these local optima are in fact also almost optimal when used as initiation point using local search methods e.g., .

This example shows that when the Gromov-Wasserstein problem is calculated accurately it provides valuable information and biological meaning as it differentiates viral particles with different cargo and

    & & & Rel. & Algorithm 1 & & & Local search & \\ Type & \(n\) & \(_{x},_{y}\) & error \(\) & Exec. time [s] & Exec. time [s] & Initializations & Successful runs \\  \(\) & 100 & 2,2 & \(10^{-6}\) & 0.5 (0.4-0.6) & 4 (0.3-12.7) & 64 (4-205) & 5 \\ \(\) & 200 & 2,2 & \(10^{-6}\) & 1.6 (1.0-1.9) & 27 (13-42) & 129 (59-197) & 5 \\ \(\) & 300 & 2,2 & \(10^{-6}\) & 3.2 (2.7-3.9) & 174 (33-458) & 366 (69-959) & 5 \\ \(\) & 400 & 2,2 & \(10^{-6}\) & 6 (5-8) & 322 (97-685) & 321 (97-781) & 3 \\  \(\) & 100 & 2,3 & \(10^{-6}\) & 352 (99-669) & 23 (8-60) & 341 (110-853) & 5 \\ \(\) & 200 & 2,3 & \(10^{-6}\) & 1238 (643-2612) & 92 (11-168) & 421(49-778) & 3 \\ \(\) & 300 & 2,3 & \(10^{-6}\) & 3006 (601-4908) & 131 (4-344) & 270 (10-710) & 5 \\ \(\) & 400 & 2,3 & \(10^{-6}\) & 4729 (4279-4868) & 367 (5-343) & 365 (1-859) & 5 \\  \(_{1}\) & 100 & 2,2 & \(10^{-6}\) & 0.5 (0.3-0.7) & 0.6 (0.2-1.0) & 10 (3-16) & 5 \\ \(_{1}\) & 200 & 2,2 & \(10^{-6}\) & 0.9 (0.7-1.1) & 13.4 (1-37) & 61 (6-168) & 5 \\ \(_{1}\) & 300 & 2,2 & \(10^{-6}\) & 2.4 (2.0-2.9) & 37.5 (0.5-90) & 75 (1-182) & 5 \\ \(_{1}\) & 400 & 2,2 & \(10^{-6}\) & 4.4 (3.6-5.3) & 149 (22-378) & 142 (21-361) & 4 \\   

Table 2: Computational efficiency compared with local search . Computational time on the format [mean (low - high)] on two specific problems which contain near symmetries and the number of random initializations needed to achieve the required accuracy on the format [mean (low - high)]. The number of initializations were limited to 1000.

variations in capsid structure, and, at the same time, finds the optimal orientation positively revealing possible chiralities or isomerisms. It also shows that when the distance is calculated accurately, it provides better decision support than using local search methods.

## 6 Discussion

When using distances as input for statistical analyses, the accuracy of the measurement set a bound for the information resolution. If the measurement system introduces error of a certain structure, this can produce artefacts in the result and affect decisions taken on the result. When using distances for such purposes, it is necessary to either know the measurement error, the artefacts being produced, or using an accurate measurement system. In this paper we have provided a method which computes the Gromov-Wasserstein problem accurately, which reduces the uncertainty of such considerations.