# Dynamic Non-monotone Submodular Maximization

Kiarash Banihashem

kiarash@umd.edu

University of Maryland

&Leyla Biabani

l.biabani@tue.nl

TU Eindhoven

&Samira Goudarzi

samirag@umd.edu

Department of Mathematics and Computer Science, Eindhoven University of Technology, the Netherlands.

&MohammadTaghi Hajiaghayi

hajiagha@cs.umd.edu

University of Maryland

&Peyman Jabbarzade

peymanj@umd.edu

University of Maryland

&Morteza Monemizadeh

m.monemizadeh@tue.nl

TU Eindhoven

###### Abstract

Maximizing submodular functions has been increasingly used in many applications of machine learning, such as data summarization, recommendation systems, and feature selection. Moreover, there has been a growing interest in both submodular maximization and dynamic algorithms. In 2020, Monemizadeh  and Lattanzi, Mitrovic, Norouzi-Fard, Tarnawski, and Zadimoghaddam  initiated developing dynamic algorithms for the monotone submodular maximization problem under the cardinality constraint \(k\). In 2022, Chen and Peng  studied the complexity of this problem and raised an important open question: "_Can we extend [fully dynamic] results (algorithm or hardness) to non-monotone submodular maximization?_". We affirmatively answer their question by demonstrating a reduction from maximizing a non-monotone submodular function under the cardinality constraint \(k\) to maximizing a monotone submodular function under the same constraint. Through this reduction, we obtain the first dynamic algorithms solving the non-monotone submodular maximization problem under the cardinality constraint \(k\). We've derived two algorithms, both maintaining an \((8+)\)-approximate of the solution. The first algorithm requires \((^{-3}k^{3}^{3}(n)(k))\) oracle queries per update, while the second one requires \((^{-1}k^{2}^{3}(k))\). Furthermore, we showcase the benefits of our dynamic algorithm for video summarization and max-cut problems on several real-world data sets.

## 1 Introduction

Submodular functions are powerful tools for solving real-world problems as they provide a theoretical framework for modeling the famous "_diminishing returns_"  phenomenon arising in a variety of practical settings. Many theoretical problems such as those involving graph cuts, entropy-based clustering, coverage functions, and mutual information can be cast in the submodular maximization framework. As a result, submodular functions have been increasingly used in many applications of machine learning such as data summarization , feature selection , and recommendation systems . These applications include both the monotone and non-monotone versions of the maximization of submodular functions.

Applications of non-monotone submodular maximization.The general problem of non-monotone submodular maximization has been studied extensively in . Thisproblem has numerous applications in video summarization, movie recommendation , and revenue maximization in viral marketing 4. An important application of this problem appears in maximizing the difference between a monotone submodular function and a linear function that penalizes the addition of more elements to the set (e.g., the coverage and diversity trade-off). An illustrative example of this application is the maximum facility location in which we want to open a subset of facilities and maximize the total profit from served clients plus the cost of facilities we did not open . Another important application occurs when expressing learning problems such as feature selection using weakly submodular functions .

Our contribution.In this paper, we consider the non-monotone submodular maximization problem under cardinality constraint \(k\) in the _fully dynamic setting_. In this model, we have a _universal ground set_\(V\). At any time \(t\), ground set \(V_{t} V\) is the set of elements that are inserted but not deleted after their last insertion till time \(t\). More formally, we assume that there is a sequence of "updates" such that each update either inserts an element to \(V_{t-1}\) or deletes an element from \(V_{t-1}\) to form \(V_{t}\).

We assume that there is a (non-monotone) submodular function \(f\) that is defined over the universal ground set \(V\). Our goal is to maintain, at each point in time, a set of size at most \(k\) whose submodular value is maximum among any subset of \(V_{t}\) of size at most \(k\).

Since calculating such a set is known to be NP-hard  even in the offline setting (where you get all the items at the same time), we focus on providing algorithms with provable approximation guarantees, while maintaining fast update time. This is challenging as elements may be inserted or deleted, possibly in an adversarial order. While several dynamic algorithms exist for monotone submodular maximization, non-monotone submodular maximization is a considerably more challenging problem as adding elements to a set may decrease its value.

In STOC 2022, Chen and Peng  raised the following open question:

**Open problem:** "Can we extend [fully dynamic] results (algorithm or hardness) to non-monotone submodular maximization?"

In this paper, we answer their question affirmatively by providing the first dynamic algorithms for non-monotone submodular maximization.

To emphasize the significance of our result, it should be considered that although monotone submodular maximization under cardinality constraint has a tight \(\) approximation algorithm in the offline mode and nearly tight (\(2+e\)) approximation algorithms for both streaming and dynamic settings, there is a hardness result for the non-monotone version stating that it is impossible to obtain a \(2.04\) (i.e., \(0.491\)) approximation algorithm for this problem even in the offline setting, and to the best of our knowledge, the current state of the art algorithms for this problem have \(2.6\) and \(3.6\) (i.e., \(0.385\) and \(0.2779\)) approximation guarantees for offline  and streaming settings , respectively.

We obtain our result, by proposing a general reduction from the problem of dynamically maintaining a non-monotone submodular function under cardinality constraint \(k\) to developing a dynamic thresholding algorithm for maximizing monotone submodular functions under the same constraint. We first define \(\)-thresholding dynamic algorithms that we use in our reduction.

**Definition 1.1** (\(\)-Thresholding Dynamic Algorithm).: Let \(>0\) be a parameter. We say a dynamic algorithm is \(\)-thresholding if at any time \(t\) of sequence \(\), it reports a set \(S_{t} V_{t}\) of size at most \(k\) such that

* **Property 1:** either **a)**\(S_{t}\) has \(k\) elements and \(f(S_{t}) k\), or **b)**\(S_{t}\) has less than \(k\) elements and for any \(v V_{t} S_{t}\), the marginal gain \((v|S_{t})<\).
* **Property 2:** The number of elements changed in any update, i.e, \(|S_{t+1} S_{t}|+|S_{t} S_{t+1}|\), is not more than the number of queries made by the algorithm during the update.

In the above definition, the first property reflects the main intuition of threshold-based algorithms, while the last property is a technical condition required in our analysis. It's worth noting that the thresholding technique has been used widely for optimizing submodular functions . We next state our main result, which is a general reduction.

**Theorem 1.2** (Reduction Metatheorem).: _Suppose that \(f:2^{V}_{ 0}\) is a (possibly non-monotone) submodular function defined on subsets of a ground set \(V\) and let \(k\) be a parameter._

_Assume that for any given value of \(\), there exists a \(\)-thresholding dynamic algorithm with an expected (amortized) \(O(g(n,k))\) oracle queries per update. Then, there exist the following dynamic algorithms:_

* _A dynamic algorithm with an approximation guarantee of_ \((8+)\) _using an expected (amortized)_ \(O(k+(k,g(n,k)) g(n,k)^{-1}(k))\) _oracle queries per update._
* _A dynamic algorithm maintaining a_ \((10+)\)_-approximate solution of the optimal value of_ \(f\) _using an expected (amortized)_ \(O((k,g(n,k)) g(n,k)^{-1}(k))\) _oracle calls per update._

In , Monemizadeh developed a dynamic algorithm for monotone submodular maximization under cardinality constraint \(k\), which requires an amortized \(O(^{-2}k^{2}^{3}(n))\) number of oracle queries per update. Interestingly, in the appendix, we show that this algorithm is indeed \(\)-thresholding (for any given \(\)). Now, if we use this \(\)-thresholding dynamic algorithm inside our reduction Metatheorem 1.2, we obtain a dynamic algorithm that maintains a \((8+)\)-approximate solution using an expected amortized \(O(^{-3}k^{3}^{3}(n)(k))\) oracle queries per update.

The recent paper  of Banihashem, Biabani, Goudarzi, Hajiaghayi, Jabbarzade, and Monemizadeh develops a new dynamic algorithm for monotone submodular maximization under cardinality constraint \(k\), which uses an expected \(O(^{-1}k^{2}(k))\) number of oracle queries per update. A similar proof shows that this new algorithm is \(\)-thresholding as well. We have provided its pseudocode and a detailed explanation on why this algorithm is indeed \(\)-thresholding in the appendix. By exploiting this algorithm in our Reduction Metatheorem 1.2, we can reduce the number of oracle queries mentioned to an expected number of \(O(^{-2}k^{2}^{3}(k))\) per update.

The second result in Theorem 1.2 is also of interest as it can be used to devise a dynamic algorithm for non-monotone submodular maximization with polylogarithmic query complexity if one can provide a \(\)-thresholding dynamic algorithm for maximizing monotone submodular functions (under the cardinality constraint \(k\)) with polylogarithmic query complexity.

### Preliminaries

Submodular maximization.Let \(V\) be a ground set of elements. We say a function \(f:2^{V}_{ 0}\) is a _submodular_ function if for any \(A,B V\), \(f(A)+f(B) f(A B)+f(A B)\). Equivalently, \(f\) is a submodular function if for any subsets \(A B V\) and for any element \(e V B\), it holds that \(f(A\{e\})-f(A) f(B\{e\})-f(B)\). We define \((e|A):=f(A\{e\})-f(A)\) the _marginal gain_ of adding the element \(e\) to set \(A\). Similarly, we define \((B|A):=f(A B)-f(A)\) for any sets \(A,B V\). Function \(f\) is _monotone_ if \(f(A) f(B)\) holds for any \(A B V\), and it is _non-monotone_ if it is not necessarily the case. In the submodular maximization problem under cardinality constraint \(k\), we seek to compute a set \(S^{*}\) such that \(|S^{*}| k\) and \(f(S^{*})=_{|S| k}f(S)\), where \(f\) is a submodular function and \(k\) is a given parameter.

Query access model.Similar to recent dynamic works [40; 15], we assume the access to a submodular function \(f\) is given by an _oracle_. The oracle allows _set queries_ such that for every subset \(A V\), one can query the value \(f(A)\). In this query access model, the marginal gain \(_{f}(e|A) f(A\{e\})-f(A)\) for every subset \(A V\) and an element \(e V A\), can be computed using two set queries. To do so, we first query \(f(A\{e\})\) and then \(f(A)\).

Dynamic model.Let \(\) be a sequence of inserts and deletes of an underlying universe \(V\). We assume that \(f:2^{V}_{ 0}\) is a (possibly non-monotone) submodular function defined on subsets of the universe \(V\). We define time \(t\) to be the \(t^{}\) update (i.e., insertion or deletion) of sequence \(\). We let \(_{t}\) be the sub-sequence of updates from the beginning of sequence \(\) till time \(t\) and denote by \(V_{t} V\) the set of elements that are inserted but not deleted from the beginning of the sequence \(\) till any time \(t\). That is, \(V_{t}\) is the current ground set of elements. We let \(_{t}=_{S V_{t}:|S| k}f(S)\).

Query complexity.The _query complexity_ of a dynamic \(\)-approximate algorithm is the number of oracle queries that the algorithm must make to compute a solution \(S_{t}\) with respect to ground setwhose submodular value is an \(\)-approximation of \(OPT_{t}\). That is, \(|S_{t}| k\) and \(f(S_{t}) OPT_{t}\). Observe that the dynamic algorithm remembers every query it has made so far. Thus results of queries made in previous times may help find \(S_{t}\) in current time \(t\).

Oblivious adversarial model.The dynamic algorithms that we develop in this paper are in the _oblivious adversarial model_ as is common for analysis of randomized data structures such as universal hashing . The model allows the adversary, who is aware of the submodular function \(f\) and the algorithm that is going to be used, to determine all the arrivals and departures of the elements in the ground set \(V\). However, the adversary is unaware of the random bits used in the algorithm and so cannot choose updates adaptively in response to the randomly guided choices of the algorithm. Equivalently, we can suppose that the adversary prepares the full input (insertions and deletions) before the algorithm runs.

### Related Work

Offline algorithms.The offline version of non-monotone submodular maximization was first studied by Feige, Mirrokni, and Vondrak in . They studied _unconstrained non-monotone submodular maximization_ and developed constant-factor approximation algorithms for this problem. In the offline query access model, they showed that a subset \(S\) chosen uniformly at random has a submodular value which is a \(4\)-approximation of the optimal value for this problem. In addition, they also described two local search algorithms. The first uses \(f\) as the objective function, and provides \(3\)-approximation and the second uses a noisy version of \(f\) as the objective function and achieves an improved approximation guarantee \(2.5\) for maximizing unconstrained non-monotone non-negative submodular functions. Interestingly, they showed \((2-)\)-approximation for symmetric submodular functions would require an exponential number of queries for any fixed \(>0\).

Oveis Gharan and Vondrak  showed that an extension of the \(2.5\)-approximation algorithm can be seen as _simulated annealing_ method which provides an improved approximation of roughly \(2.4\). Later, Buchbinder, Feldman, Naor, and Schwartz  at FOCS'12, presented a randomized linear time algorithm achieving a tight approximation guarantee of \(2\) that matches the known hardness result of . Bateni, Hajiaghayi, and Zadimoghaddam  and Gupta, Roth, Schoenebeck, and Talwar  independently studied non-monotone submodular maximization subject to cardinality constraint \(k\) in the offline and secretary settings. In particular, Gupta _et al._ obtained an offline \(6.5\)-approximation for this problem.

All of the aforementioned approximation algorithms are offline, where the whole input is given in the beginning, whereas the need for real-time analysis of rapidly changing data streams motivates the study of this problem in settings such as the dynamic model that we study in this paper.

Streaming algorithms.The dynamic model that we study in this paper is closely related to the streaming model . However, the difference between these two models is that in the streaming model, we maintain a data structure using which we compute a solution at the end of the stream and so, the time to extract the solution is not important as we do it once. However, in the dynamic model, we need to maintain a solution after every update, thus, the update time of a dynamic algorithm should be as fast as possible.

The known streaming algorithms  work in the insertion-only streaming model and they do not support deletions as well as insertions. Indeed, there are streaming algorithms  for the monotone submodular maximization problem that support deletions, but the space and the update time of these algorithms depend on the number of deletions which could be \((n)\), where \(n=|V|\) is the size of ground set \(V\).

For monotone submodular maximization, Badanidiyuru, Mirzasoleiman, Karbasi, and Krause  proposed an insertion-only streaming algorithm with a \((2+)\)-approximation guarantee under a cardinality constraint \(k\). Chekuri, Gupta, and Quanrud  presented (insertion-only) streaming algorithms for maximizing monotone and non-monotone submodular functions subject to \(p\)-matchoid constraint5. Later, Mirzasoleiman, Jegelka, and Krause  and Feldman, Karbasi, and Kazemi  developed streaming algorithms with better approximation guarantees for maximizing a non-monotone function under a \(p\)-matchoid constraint. Currently, the best streaming algorithm for maximizing a non-monotone submodular function subject to a cardinality constraint is due to Alaluf, Ene, Feldman, Nguyen, and Suh  whose approximation guarantee is \(3.6+\), improving the \(5.8\)-approximation guarantee that was proposed by Feldman _et al._.

Dynamic algorithms.At NeurIPS 2020, Lattanzi, Mitrovic, Norouzi-Fard, Tarnawski, and Zadimoghaddam  and Monemizadeh  initiated the study of submodular maximization in the dynamic model. They presented dynamic algorithms that maintain \((2+)\)-approximate solutions for maximizing a monotone submodular function subject to cardinality constraint \(k\). Later, at STOC 2022, Chen and Peng  studied the complexity of this problem and they proved that developing a \(c\)-approximation dynamic algorithm for \(c<2\) is not possible unless we use a number of oracle queries polynomial in the size of ground set \(V\). In 2023, Banihashem, Biabani, Goudarzi, Hajiaghayi, Jabbarzade, and Monemizadeh developed an algorithm for monotone submodular maximization problem under cardinality constraint \(k\) using a polylogarithmic amortized update time. Concurrent works of Banihashem, Biabani, Goudarzi, Hajiaghayi, Jabbarzade, and Monemizadeh and Duetting, Fusco, Lattanzi, Norouzi-Fard, and Zadimoghaddam developed the first dynamic algorithms for monotone submodular maximization under a matroid constraint. Authors of  also improve the algorithm of  for monotone submodular maximization subject to cardinality constraint \(k\). There are also studies on the dynamic model of influence maximization, which shares similarities with submodular maximization .

In this paper, for the first time, we study the generalized version of their problem by presenting an algorithm for maximizing the non-monotone submodular functions in the dynamic setting.

## 2 Dynamic algorithm

In this section, we explain the algorithm that we use in the reduction that we stated in Metatheorm 1.2. The pseudocode of our algorithm is given in Algorithm 1, Algorithm 2, and Algorithm 3.

Such reductions were previously proposed in the offline model by , and later works extended this idea to the streaming model . We develop a reduction in the dynamic model inspired by these works, though in our proof, we require a tighter analysis to obtain the approximation guarantee in our setting.

We consider an arbitrary time \(t\) of sequence \(\) where \(V_{t}\) is the set of elements inserted before time \(t\), but not deleted after their last insertion. Let \(OPT^{*}_{t}=_{S V_{t}:|S| k}f(S)\). For simplicity, we drop \(t\) from \(V_{t}\) and \(OPT^{*}_{t}\), when it is clear from the context.

In the following, we assume that the value of \(OPT\) is known. Although the exact value of \(OPT^{*}\) is unknown, we can maintain parallel runs of our dynamic algorithm for different guesses of the optimal value. By using \((1+^{})^{i}\), where \(i\) as our guesses for the optimal value, one of our guesses \((1+^{})\)-approximates the value of \(OPT^{*}\). We show that the output of our algorithm satisfies the approximation guarantee in the run whose \(OPT\)\((1+^{})\)-approximates the value of \(OPT^{*}\). Later, in the appendix, we show that it is enough to consider each element \(e\) only in runs \(i\) for which we have \(}{k}(1+^{})^{i} f(e)(1 +^{})^{i}\). This method increases the query complexity of our dynamic algorithm by only a factor of \(O(^{-1} k)\).

Our approach for solving the non-monotone submodular maximization is to first run the thresholding algorithm with input set \(V\) to find a set \(S_{1}\) of at most \(k\) elements. Since \(f\) is non-monotone, subsets of \(S_{1}\) might have a higher submodular value than \(f(S_{1})\). Then, we use an \(\)-approximation algorithm (for \(0< 1\)) to choose a set \(S^{}_{1} S_{1}\) with guarantee \([f(S^{}_{1})]_{C S_{1}}f(C)\). Next, we run the thresholding algorithm with the input set \(V S_{1}\) and compute a set \(S_{2}\). At the end, we return set \(S=_{C\{S_{1},S_{1},S_{2}\}}f(C)\). Intuitively, for an optimal solution \(S^{*}\), if \(f(S_{1} S^{*})\) is a good approximation of \(OPT\), then \(f(S^{}_{1})\) is a good approximation of \(OPT\). On the other hand, if both \(f(S_{1})\) and \(f(S_{1} S^{*})\) are small with respect to \(OPT\), then we can ignore the elements of \(S_{1}\) and show that we can find a set \(S_{2} V S_{1}\) of size at most \(k\) whose submodular value is a good approximation of \(OPT\). The following lemma proves that the submodular value of \(S\) is a reliable approximation of the optimal solution. The formal proof of this lemma can be found in Section 2.

**Lemma 2.1** (Approximation Guarantee).: _Assuming that \(OPT^{*}[},OPT]\), the expected submodular value of set \(S\) is \([f(S)](1-O(^{}))}{6+}\)._

Next, we explain the steps of our reduction in detail.

Let us first fix the threshold \(=\). Then, we fix a \(\)-thresholding dynamic algorithm (for example,  or ) and suppose we denote it by DynamicThresholding. Before sequence \(\) of updates starts, we create two independent instances \(_{1}\) and \(_{2}\) of DynamicThresholding. The first instance will maintain set \(S_{1}\) and the second instance will maintain set \(S_{2}\). For instance \(_{i}\) where \(i\{1,2\}\), we consider the following subroutines:

* \(_{_{i}}(v)\): This subroutine inserts an element \(v\) to instance \(_{i}\).
* \(_{_{i}}(v)\): Invoking this subroutine will delete the element \(v\) from instance \(_{i}\).
* \(_{_{i}}\): This subroutine returns the maintained set (of size at most \(k\)) of \(_{i}\).

Extracting \(S_{1}\).After the update at time \(t\), first, we would like to set \(Z=S_{1}^{-}\{v\}\) or \(Z=S_{1}^{-}\{v\}\), if the update is the insertion of an element \(v\) or the deletion of an element \(v\), respectively, where \(S_{1}^{-}\) is the set \(S_{1}\) that instance \(_{1}\) maintains just before this update. To find set \(S_{1}^{-}\), we just need to invoke subroutine Extract\({}_{_{1}}\). If the update is an insertion, we insert it into instance \(_{1}\) using Insert\({}_{_{1}}(v,)\), and if the update is a deletion, we delete \(v\) from both \(_{1}\) and \(_{2}\) using Delete\({}_{_{1}}(v)\) and Delete\({}_{_{2}}(v)\). We then invoke Extract\({}_{_{1}}\) once again to return set \(S_{1}\).

Extracting \(S_{1}^{}\).Buchbinder _et al._ developed a method to extract a subset \(S_{1}^{} S_{1}\) whose submodular value is a good approximation of \(_{C S_{1}}f(C)\). In this algorithm, we start with two solutions \(\) and \(S_{1}\). The algorithm considers the elements (in arbitrary order) one at a time. For each element, it determines whether it should be added to the first solution or removed from the second solution. Thus, after a single pass over set \(S_{1}\), both solutions completely coincide, which is the solution that the algorithm outputs. They show that a (deterministic) greedy choice in each step obtains \(3\)-approximation of the best solution in \(S_{1}\). However, if we combine this greedy choice with randomization, we can obtain a \(2\)-approximate solution. Since we do a single pass over set \(S_{1}\), the number of oracle queries is \(O(|S_{1}|)\).

The second algorithm that we can use to extract \(S_{1}^{}\) is a random sampling algorithm proposed by Feige _et al._, which choose every element in \(S_{1}\) with probability \(1/2\). They show that this random sampling returns a set \(S_{1}^{}\) whose approximation factor is \(1/4\) of \(_{C S_{1}}f(C)\), and its number of oracle calls is \(O(1)\). We denote either of these two methods by SubsetSelection.

Extracting \(S_{2}\).Next, we would like to update the set \(S_{2}\) that is maintained by instance \(_{2}\). To do this, for every element \(u Z S_{1}\), we add it to \(_{2}\) using Insert\({}_{_{2}}(u,)\), and for every element \(u S_{1} Z\), we delete it from \(_{2}\) using Delete\({}_{_{2}}(u,)\). Finally, when \(_{2}\) exactly includes all the current elements other than the ones in \(S_{1}\), we call subroutine Extract\({}_{_{2}}\) to return set \(S_{2}\).

**Corollary 2.2**.: _We obtain the \((8+)\) approximation guaranty stated in the Metatheorem 1.2 by using the local search method for our SubsetSelection, and we get the \((10+)\) approximation guaranty by utilizing the random sampling method for our SubsetSelection subroutine._

Proof.: These are immediate results of Lemma 2.1, and \(\) being \(\) and \(\) in the local search method and random sampling method, respectively. 

```
1:\(\), where \(\) is \(\) or \(\) based on the selection of algorithm for SubsetSelection.
2:Instantiate two independent instances \(_{1}\) and \(_{2}\) of DynamicThresholding for monotone submodular maximization under cardinality constraint \(k\) using \(\)
```

**Algorithm 1** Initialization\((k,OPT)\)```
1:\(Z_{_{1}}\)
2:if\((v)\) is an insertion then
3: Invoke \(_{_{1}}(v),\ Z Z\{v\}\)
4:else
5: Invoke \(_{_{1}}(v),\ _{_{2}}(v),\ Z  Z\{v\}\)
6:\(S_{1}_{_{1}}\)
7:\(S^{}_{1}(S_{1})\)
8:for\(u S_{1} Z\)do
9:\(_{_{2}}(u)\)
10:for\(u Z S_{1}\)do
11:\(_{_{2}}(u)\)
12:\(S_{2}_{_{2}}\)
13: Return \(_{C\{S_{1},S^{}_{1},S_{2}\}}f(C)\)
```

**Algorithm 2**Update\((v)\)

```
1:functionUniformSubset(\(S\))
2:\(T\)
3:for\(s S\)do
4:if\(Coin()\)then\(\) With probability \(\)
5:\(T T\{s\}\)
6:return\(T\)
7:functionLocalSearchSubset(\(S\))
8:\(X_{0},\ Y_{0} S\).
9:for\(i=1\) to \(|S|\)do
10:\(a_{i} f(X_{i-1}\{s_{i}\})-f(X_{i-1}),\ b_{i} f(Y_{i-1 }\{s_{i}\})-f(Y_{i-1})\)
11:\(a^{}_{i}(a_{i},0),\ b^{}_{i}(b_{i},0)\)
12:if\(a^{}_{i}=b^{}_{i}=0\)then\(a^{}_{i}/(a^{}_{i}+b^{}_{i})=0\)
13:with probability \(a^{}_{i}/(a^{}_{i}+b^{}_{i})\)do:
14:\(X_{i} X_{i-1}\{s_{i}\},\ Y_{i} Y_{i-1}\)
15:elsedo:\(X_{i} X_{i-1},\ Y_{i} Y_{i-1}\{s_{i}\}\)
16:return\(X_{|S|}\) (or equivalently \(Y_{|S|}\))
```

**Algorithm 3**SubsetSelection\((S)\)

Analysis.In this section, we prove the correctness of our algorithms and analyze the number of oracle queries of our algorithms, which finishes the proof of Theorems 1.2.

Consider an arbitrary time \(t\). Let \(S_{t}\) be the reported set of DynamicThresholding at time \(t\). Recall that \(V_{t}\) is the ground set at time \(t\), and we drop the \(t\) for simplicity, so we use \(V\) and \(S\) to denote \(V_{t}\) and \(S_{t}\). We first present Lemma 2.3 whose proof is given in the appendix. Then we proceed to prove Lemma 2.1 and Theorem 1.2

**Lemma 2.3**.: _Suppose that set \(S\) satisfies Property 1.b of Definition 1.1. It means that \(S\) has less than \(k\) elements and for any \(v V S\), the marginal gain \((v|S)<\). Then, for any arbitrary subset \(C V\), we have \(f(S) f(S C)-|C|\)._

Proof of Lemma 2.1: Assume that at a fixed time \(t\), \(OPT^{*}\) and \(S^{*}\) are the optimal value and an optimal solution for the submodular maximization of function \(f\) under cardinality constraint \(k\). This means that \(|S^{*}| k\) and \(f(S^{*})=OPT^{*}\). Recall that \(=+)}\), where \(OPT\) is our guess for the optimal value. Also, by assumption we have \(OPT^{*}[},OPT]\), or equivalently \(OPT[OPT^{*},(1+^{})OPT^{*}]\).

To prove the lemma, we claim that \(([f(S_{1})],[f(S^{}_{1}

To prove the claim, we consider two cases. The first case is when \(f(S_{1} S^{*})\) and the second case is if \(f(S_{1} S^{*})<\).

Suppose the first case is true. Then, the subset selection algorithm (either random sampling method or local search) returns \(S_{1}^{}\) for which \([f(S_{1}^{})]_{S S_{1}}f(S)\). Since \(S_{1} S^{*} S_{1}\), we have

For the latter case, we show that \([f(S_{1})]+[f(S_{2})](1-O(^{}))}{3+1/2}\), inferring \(([f(S_{1})],[f(S_{2})])(1-O( ^{}))}{6+1/}\). Indeed, since \(S_{1}\) and \(S_{2}\) are reported by an \(\)-thresholding algorithm, if \(|S_{1}|=k\) or \(|S_{2}|=k\), then \(([f(S_{1})],[f(S_{2})])\) is at least \( k=\) by the first property of \(\)-thresholding algorithms.

Now suppose that \(|S_{1}|,|S_{2}|<k\), which means that Property 1.b of Definition 1.1 holds for \(S_{1}\) and \(S_{2}\). Therefore, we have \(f(S_{1}) f(S_{1} S^{*})-|S^{*}|\) and \(f(S_{2}) f(S_{2}(S^{*} S_{1}))-|S^{*} S_{1}|\) by Lemma 2.3. Besides, we have \(f(S_{1} S^{*})-<0\). Therefore,

\[f(S_{1})+f(S_{2}) f(S_{1} S^{*})-|S^{*}|+f(S_{2}(S^{*} S _{1}))-|S^{*} S_{1}|+f(S_{1} S^{*})-.\]

Since \(|S^{*} S_{1}||S^{*}| k\) we have

\[f(S_{1})+f(S_{2}) f(S_{1} S^{*})+f(S_{2}(S^{*} S_{1}))+f( S_{1} S^{*})-(2+1/(2)) k.\]

Since \(S_{1} S_{2}=\) and \(f\) is submodular, we have \(f(S_{1} S^{*})+f(S_{2}(S^{*} S_{1})) f(S_{1} S_{2}  S^{*})+f(S^{*} S_{1})\). Additionally, by the submodularity and non-negativity of \(f\), we have \(f(S_{1} S^{*}) f(S^{*})-f(S^{*} S_{1})\), because \(f(S^{*} S_{1})+f(S_{1} S^{*}) f(S^{*})+f()\). By adding the last two inequalities and using the non-negativity of \(f\) once again, we get \(f(S_{1} S^{*})+f(S_{2}(S^{*} S_{1}))+f(S_{1} S^{*}) f (S_{1} S_{2} S^{*})+f(S^{*}) f(S^{*})=OPT^{*}\). By putting everything together we have,

\[f(S_{1})+f(S_{2}) OPT^{*}-(2+1/(2)) k=OPT^{*}-()().\]

By using the assumption that \(OPT(1+^{})OPT^{*}\), we have,

\[f(S_{1})+f(S_{2}) OPT^{*}(1-()}{6 +1})) OPT^{*}((4+1)}{6+1 })=(1-O(^{}))[_{t=1}^{T}Q_{t}] O(T(k g(n,k),g(n,k)^{2}))\]

Proof.: Consider the case where the expected number of oracle calls made by the thresholding algorithm DynamicThreshold per each update is \(O(g(n,k))\). Per each update, our algorithm makes an update in instance \(_{1}\) causing \(O(g(n,k))\) oracle queries. Next, we make either \(O(k)\) or \(0\) oracle queries for the SubsetSelection subroutine, depending on the used method. We also make a series of updates in instance \(_{2}\), each causing \(O(g(n,k))\) oracle queries. The number of such updates is bounded by the number of changes in the output of instance \(_{1}\), which is bounded by both \(k\) and \(O(g(n,k))\) (according to the second property of Definition 1.1). These comprise all the oracle queries made by our algorithm at time \(t\). Therefore, the given bounds for this case hold. A detailed proof for the remaining bounds is provided in the appendix. 

## 3 Empirical results

In this section, we empirically study our \((8+)\)-approximation dynamic algorithm. We implement our codes in C++ and run them on a MacBook laptop with \(8\) GB RAM and \(M1\) processor. We empirically study the performance of our algorithm for video summarization and the Max-Cut problem.

Video summarization.Here, we use the Determinantal Point Process (DPP) which is introduced by , and combine it with our algorithm to capture a video summarization. We run our experiments on YouTube and Open Video Project (OVP) datasets from .

For each video, we use the linear method of  to extract a subset of frames and find a positive semi-definite kernel \(L\) with size \(n n\) where \(n\) is the number of extracted frames. Then, we try to find a subset \(S\) of frames such that it maximizes \()}{det(L+L)}\) where \(L_{S}\) is the sub-matrix of \(L\) restricted to indices corresponding to frames \(S\). Since \(L\) is a positive semi-definite matrix, we have \(det(L_{S}) 0\). Interestingly,  showed that \((det(L_{S}))\) is a non-monotone function. We use these properties and set \(f(S):=(det(L_{S})+1)\) to make \(f\) a non-monotone non-negative submodular function. Then we run our \((8+)\)-approximate dynamic algorithm to find the best \(S\) to maximize \(f(S)\) such that \(|S| k\) for \(k\).

First, we insert all frames to observe the quality of our algorithm. Figure 1 and 2 are the selected frames by our algorithm for Video 106 from YouTube and Video 36 from OVP, respectively, when we limit the number of selected frames to \(4\). Then, we create a sequence \(\) of updates of frames of each video. Similar to , we define the sequence as a sliding window model. That is, given a window of size \(W\) for a parameter \(W\), a frame is inserted at a time \(t\) and will be alive for a window of size \(W\) and then we delete that frame.

Figure 1: Video summarization of Susan Boyle’s performance on Britain’s Got Talent show (video 106) from YouTube.

Figure 2: Video summarization for “Senses And Sensitivity, Introduct. to Lecture 1 presenter” (video 36) from OVP.

To evaluate the performance of our algorithm, we benchmark (See Figure 3) the total number of query calls and the submodular value of set \(S\) of our algorithm and the streaming algorithm proposed for non-monotone submodular maximization so-called Sample-Streaming proposed in . This algorithm works as follows: Upon arrival of an element \(u\), with probability \((1-q)\), for a parameter \(0<q<1\), we ignore \(u\), otherwise (i.e., with probability \(q\)), we do the following. If the size of set \(S\) that we maintain is less than \(k\), i.e, \(|S|<k\) and \((u|S)>0\), we add \(u\) to \(S\). However, if \(|S|=k\), we select an element \(v S\) for which \((v:S)\) is minimum possible, where \((u:S)\) equals to \((u|S_{u})\) where \(S_{u}\) are elements that arrived before \(u\) in sequence \(\). If \((u|S)(1+c)(v:S)\) for a constant \(c\), we replace \(v\) by \(u\); otherwise, we do nothing. Now we convert this streaming algorithm into a dynamic algorithm. To accomplish this, we restart Sample-Streaming after every deletion that deletes an element of solution set \(S\) that is reported by Sample-Streaming's outputs. That is, if a deletion does not touch any element in set \(S\), we do nothing; otherwise we restart the streaming algorithm.

We run our algorithm for \(=k/2\) and compare the total oracle calls and average output of our algorithm and Sample-Streaming in Figure 3. To prove the approximation guarantee of our dynamic algorithm, we assumed \( 1\). However, in practice, it is possible to increase \(\) up to a certain level without affecting the output of the algorithm significantly. On the other hand, increasing \(\) reduces the total oracle calls and makes the algorithm faster. As you can see in Figure 3 plots (b) and (d), the submodular value of our algorithm is not worse than the Sample-Streaming algorithm whose approximation factor is \(3+2 5.828\) which is better than our approximation factor. Thus, our algorithm has an outcome better than our expectation, while its total oracle calls are better than Sample-Streaming algorithm (look at Figure 3 plots (a) and (c)).

We also empirically study the celebrated Max-Cut problem which is a non-monotone submodular maximization function (See ). These experiments are given in the appendix.

## 4 Conclusion

In this paper, we studied non-monotone submodular maximization subject to cardinality constraint \(k\) in the dynamic setting by providing a reduction from this problem to maximizing monotone submodular functions under the cardinality constraint \(k\) with a certain kind of algorithms(\(\)-thresholding algorithms). Moreover, we used our reduction to develop the first dynamic algorithms for this problem. In particular, both our algorithms maintain a solution set whose submodular value is a \((8+)\)-approximation of the optimal value and require \(O(^{-3}k^{3}^{3}(n)(k))\) and \(O(^{-1}k^{2}^{3}(k))\) oracle queries per update, respectively.

## 5 Acknowledgements

This work is partially supported by DARPA QuICC NSF AF:Small #2218678, and NSF AF:Small #2114269.

Figure 3: We plot the total number of query calls and the average output of our dynamic algorithm and Sample-Streaming on video 106 from YouTube and video 36 from OVP. In this figure, from left to right, Sub-figures (a) and (b) are the total oracle calls for video 106 and 36, respectively. Similarly, Sub-figures (c) and (d) are average submodular value for video 106 and 36, respectively.