# Time-MMD: Multi-Domain Multimodal Dataset

for Time Series Analysis

 Haoxin Liu\({}^{}\), Shangqing Xu\({}^{}\), Zhiyuan Zhao\({}^{}\), Lingkai Kong\({}^{}\),

**Harshavardhan Kamarthi\({}^{}\), Aditya B. Sasanur\({}^{}\), Megha Sharma\({}^{}\),**

**Jiaming Cui\({}^{}\), Qingsong Wen\({}^{@sectionsign}\), Chao Zhang\({}^{}\), B. Aditya Prakash\({}^{}\)\({}^{}\)**

Correspondence to: Haoxin Liu <hliu763@gatech.edu>, B. Aditya Prakash <badityap@cc.gatech.edu>

\({}^{}\)Georgia Institute of Technology \({}^{@sectionsign}\)Squirrel AI

###### Abstract

Time series data are ubiquitous across a wide range of real-world domains. While real-world time series analysis (TSA) requires human experts to integrate numerical series data with multimodal domain-specific knowledge, most existing TSA models rely solely on numerical data, overlooking the significance of information beyond numerical series. This oversight is due to the untapped potential of textual series data and the absence of a comprehensive, high-quality multimodal dataset. To overcome this obstacle, we introduce Time-MMD, the first multi-domain, multimodal time series dataset covering 9 primary data domains. Time-MMD ensures fine-grained modality alignment, eliminates data contamination, and provides high usability. Additionally, we develop MM-TSFlib, the first-cut multimodal time-series forecasting (TSF) library, seamlessly pipelining multimodal TSF evaluations based on Time-MMD for in-depth analyses. Extensive experiments conducted on Time-MMD through MM-TSFlib demonstrate significant performance enhancements by extending unimodal TSF to multimodality, evidenced by over 15% mean squared error reduction in general, and up to 40% in domains with rich textual data. More importantly, our datasets and library revolutionize broader applications, impacts, research topics to advance TSA. The dataset is available at https://github.com/AdityLab/Time-MMD.

## 1 Introduction

Time series (TS) data are ubiquitous across a wide range of domains, including economics, urban computing, and epidemiology [52; 56; 29]. Analytical tasks on such datasets hence find broad applications in various real-world scenarios such as energy forecasting, traffic planning, and epidemic policy formulation. Human experts typically complete such Time-Series Analysis (TSA) tasks by integrating multiple modalities of time-series data. For instance, epidemiologists combine numerical data on influenza infections with textual domain knowledge, policies, and reports to predict future epidemiological trends. However, most existing TSA models [59; 42; 78; 44; 81; 32; 80; 48] are unimodal, solely using numerical series.

Recently, with the development of Large Language Models (LLMs), the field of TSA is also undergoing an exciting transformative moment with the integration of natural language [73; 34]. Existing LLM-based TSA methods incorporate endogenous text derived from numerical series, such as linguistic descriptions of statistical information, which has demonstrated promising benefits [22; 25; 6; 40]. However, the potential of exogenous or auxiliary textual signals--such as information on concurrent events and policies that provide additional context to time series--remains untapped. This observation prompts a crucial question for multimodal TSA: **Can multimodal TSA models utilize theseexogenous textual signals effectively, thereby enhancing current TSA tasks and enabling new applications?**

The primary obstacle in addressing this question lies in the absence of a comprehensive, high-quality multimodal TS dataset, as evidenced by three significant gaps: (1) **Narrow data domains.** Data characteristics and patterns vary between different domains, such as the periodicity of numerical data and the sparsity of textual data. However, current multimodal TS datasets [(15; 70; 11; 49; 6)] focus solely on stock prediction tasks in the financial domain, which are unable to represent the diverse data domains. (2) **Coarse-grained modality alignment.** Existing multimodal TS datasets only ensure that the text and numerical data come from the same domain, such as general stock news and the prices of one specific stock. Clearly, an abundance of irrelevant text diminishes the effectiveness of multimodal TSA. (3) **Inherent data contamination.** Existing multimodal TS datasets overlook two main reasons of data contamination: (1) Textual data often contains predictions. For example, influenza outlook is a regular section in influenza reports. (2) Outdated test set, particularly the textual data, may have been exposed to LLMs, which are pretrained on vast corpuses. For example, the knowledge cutoff for Llama3-70B is December 2023, which is later than the cutoff dates for most existing multimodal TS datasets. These reasons lead to biased evaluations of general or LLM-based TSA models.

To address the identified gaps, this work aims to introduce a comprehensive, high-quality multimodal TS dataset that spans diverse domains and can be validated through its effectiveness and benefits for TSA. The main contributions of our work are:

* **Pioneering Multi-Domain Multimodal Time-Series Dataset.** We introduce Time-MMD, the first multi-domain multimodal time-series dataset that addresses the aforementioned gaps: (1) encompasses 9 primary data domains. (2) ensures fine-grained modality alignment through meticulously selected data sources and rigorous filtering steps. (3) disentangles facts and predictions from text; ensures all cutoff dates are up to May 2024. To the best of our knowledge, Time-MMD stands as the inaugural high-quality and comprehensive multimodal time-series dataset. We envision Time-MMD offering exciting opportunities to significantly advance time series analysis through multimodal extensions.
* **Pilot Multimodal Time-Series Forecasting Study.** We develop the first-cut multimodal time-series forecasting (TSF) library, MM-TSFlib, piloting multimodal TSA research based on Time-MMD. Our library MM-TSFlib features an end-to-end pipeline with a seamless interface that allows the integration of any open-source language models with arbitrary TSF models, thereby enabling multimodal TSF tasks. MM-TSFlib facilitates easy exploration of Time-MMD and supports future advancements in multimodal TSA.
* **Extensive Evaluations with Significant Improvement.** We conducted over 1,000 experiments of multimodal TSF on Time-MMD using MM-TSFlib. The multimodal versions outperformed corresponding unimodal versions in 95% of cases, reducing the mean squared error by an average of over 20% and up to 40% in some domains with rich textual data. This significant and consistent improvement demonstrates the high quality of Time-MMD, the effectiveness of MM-TSFlib, and the superiority of multimodal extensions for TSF.

We include additional related works in Appendix A and limitations in Appendix B.

## 2 Multi-Domain Multimodal Time-Series Dataset: Time-MMD

We first introduce the key challenges in constructing Time-MMD, followed by the construction pipeline. We then detail each component of the pipeline with corresponding data quality verification. Finally, we discuss considerations for fairness and data release.

**Challenges.** Creating a high-quality, multi-domain numerical-text series dataset presents significant challenges, encompassing the effective gathering, filtering, and alignment of useful textual data. First, textual sources are sparse. Unlike numerical data, typically provided by a "packaged" source, textual data are collected from a variety of dispersed sources, such as reports and news articles, necessitating extensive individual collection efforts. Second, textual information is noisy. Raw textual data often contains large portions of irrelevant information and potential data contamination, such as expert predictions in reports, requiring rigorous filtering processes to ensure data quality. Third, textual data requires precise alignment. It is essential to achieve temporal alignment between textual and numerical data by synchronizing reported times with numerical time steps (e.g., the time step where text is posted) and ensuring that the effective duration of textual information matches the relevant time frames at various granularities (e.g., a seasonal report should correspond to 12 time steps in a weekly time series). Additionally, the dataset faces challenges regarding ease of use, maintenance, and regular updates to remain relevant and useful for ongoing research and applications.

**Pipeline Overview.** We propose a comprehensive pipeline for constructing a text-numeric series dataset utilizing modern LLMs. As illustrated in Figure 1, the construction process is divided into three key steps: (1) _Numerical Series Data Construction._ We gather numerical data from reputable sources to ensure reliability and accuracy. (2) _Textual Series Data Construction._ Textual data is collected for fine-grained matching with the numerical data. The quality of this matching is ensured through human selection of data sources and raw text filtering by LLMs. Additionally, LLMs are employed to disentangle facts and predictions and generate summaries. (3) _Numerical-Textual Alignment._ We use binary timestamps to mark the start and end dates as a universal temporal alignment method between numerical and textual series, supporting the requirements of various downstream TSA tasks.

### Numerical Series Data Construction

**Data Source Selection.** We select data sources that are (1) reliable, containing verified knowledge; (2) actively released, allowing for updates with new data; and (3) multi-domain, covering various TSA patterns. Appendix C provides considerations for domain selection. Based on these principles, we choose nine data sources from different domains, as shown in Appendix D. Most sources are from government agencies, with the lowest update frequency being six months.

**Target Variable Selection.** For each domain, we select target variables with significant real-world implications, indicating easier text matching, as shown in Table 1. These variables span three distinct frequencies: daily, weekly, and monthly.

**Collection & Preprocessing.** We collect raw data for all available times, either from batch-released files or through individual scraping. We preprocess the data by discarding early years with a high proportion of missing values. We maintain the original frequency for most domains, adjusting it for security and climate domains due to irregular releases and difficult text matching, respectively. Figure 2 illustrates the diverse patterns present in each domain, such as periodicity and trends.

_Data Quality & Property._ As shown in Table 1 and Figure 2, the constructed numerical data provides comprehensive temporal coverage, ranging from the earliest in 1950 to the present, and exhibits distinct patterns, such as periodicity and trends.

### Textual Series Data Construction

**Data Source Selection: Selected Reports and Web Search Results.** The choice of data sources should take into account both extensive coverage and initial strong relevance to the numerical data. Consequently, we combine two appropriate data source types as follows: (1) Selected Reports: For each target variable, we manually select 1-2 highly relevant report series with guaranteed updates. For

Figure 1: Overview of the Time-MMD construction. We first construct numerical data, then construct textual data from search and report sources with LLM preprocessing targeted at the numerical data, and finally annotate the data with binary timestamps to support various downstream tasks.

instance, the weekly influenza report2 published by the Centers for Disease Control and Prevention of the United States is chosen as one of the report sources for the weekly influenza patients proportion of the United States. (2) Web Search Results: For each target variable, we design 2-3 highly relevant keywords used for web searching.

These two data sources complement each other: report data ensures higher relevance but cannot guarantee all-time coverage, while search results cover all times but are highly redundant; search results aggregate multiple data sources, while report data, usually in PDF or TXT format, cannot be extracted by searching. For frequency alignment, the textual data covers multiple frequencies, with search texts enabling daily precision and reports ranging from weekly to monthly and yearly intervals. Appendix D provides a complete list of keywords and reports source.

**Data Collection: Searching and Crawling.** For keyword web searching, we use the official Google API3 as the entry point. For each keyword, we collect the timestamp, source, title, and content from the top 10 results located each week from 1980 to present. For report data, we parse all available reports from each data source and preserve only plain-text paragraphs.

**Data Preprocessing: Filtering, Disentangling, and Summarizing.** To curate the collected raw text data, we introduce three key preprocessing steps: (1) Filtering to improve relevance; (2) Disentangling facts with predictions to mitigate data contamination; (3) Summarizing for better usability. Given the impracticality of performing these steps manually, we leverage the state-of-the-art LLM, Llama3-70B, to accomplish these tasks.

The prompt used for LLMs is detailed in Appendix F. We incorporate three specific strategies to alleviate the hallucination issue in LLMs and enhance preprocessing quality: (1) A concise introduction of the text. (2) Mandating the LLM to reference the data source, aiding constraint and

   Domain & Target & Dimension & Frequency & Number of Samples & Timespan \\  Agriculture & Retail Bruiler Composite & 1 & Monthly & \(496\) & 1983 - Present \\ Climate & Drought Level & 5 & Monthly & \(496\) & 1983 - Present \\ Economy & International Trade Balance & 3 & Monthly & \(423\) & 1989 - Present \\ Energy & Gasoline Prices & 9 & Weekly & \(1479\) & 1996 - Present \\ Environment & Air Quality Index & 4 & Daily & \(11102\) & 1982 - 2023 \\ Health & Influenza Patients Proportion & 11 & Weekly & 1389 & 1997 - Present \\ Security & Disaster and Emergency Grants & 1 & Monthly & \(297\) & 1999 - Present \\ Social Good & Unemployment Rate & 1 & Monthly & \(900\) & 1950 - Present \\ Traffic & Travel Volume & 1 & Monthly & \(531\) & 1980 - Present \\   

Table 1: Overview of numerical data in Time-MMD, covering key variables across nine domains with daily, weekly, or monthly frequencies, sourced from reputable government departments. Eight domains are updated to May 2024; the environment domain update is scheduled for June 2024. We focus on univariate time series forecasting for the target variable, as Time-MMD requires constructing aligned textual data. Covariates are provided for some datasets to inspire future research which are revealed in the dimension column.

Figure 2: Visualization of Time-MMD, highlighting distinct characteristics across different domains.

verification. (3) Permitting the LLM to indicate 'not available' when relevance is uncertain, to avoid fabrication. Appendix G provides a showcase of the text before and after processing.

_Data Quality & Property._ Overall, Figure 3 visualizes the extracted fact count per month over time by domain. Note that the Agriculture report data is of high volume around 2020 and therefore produces a peak. We make the following observations: (a) The search data count exhibits a gradual increasing trend, benefiting from the development of the Internet; the report data count has stabilized in recent years, indicating that release schedule has become stable. (b) The sparsity of textual data varies across different domains, with high-profile fields often accompanied by richer textual data. These validate the extensibility and updatability of Time-MMD and highlight the importance of its coverage across 9 diverse domains.

We further validate the effectiveness of key steps in textual data construction:

(a) Data sources selection. We use _relevance_ and _coverage_ ratio to describe the percentage of relevant texts and the proportion of numerical series data being covered by at least one fact, respectively. As demonstrated by Table 2, report data exhibits higher relevance but lower coverage; search data display the opposite pattern. Thus, our combined usage serves as a comprehensive solution.

(b) Data preprocessing. Figure 4 provides word cloud visualizations of constructed text data in the health domain, respectively for extracted facts, extracted predictions, and discarded text. Recall that the target variable here is the influenza patients proportion. Highly relevant words such as "pandemic", "vaccine", and "flu" appear more frequently in the extracted facts; research paper-related words such as "edu", "mdpi", and "university" are more common in the discarded text. Besides, the prediction text primarily contains words describing future, such as "will" and "next". These validate the effectiveness of LLM filtering and disentangling. Furthermore, Table 2 presents a comparison of the token count before and after preprocessing. The substantial decrease validates that LLM summarizing improves usability. Appendix H provides the manual verification results on a subset of the data to further validate the effectiveness of preprocessing using LLMs.

### Binary Time Stamps for Diverse TSA Tasks

To enable the Time-MMD for versatile and flexible use, we maintain binary timestamps for all numerical and text data, storing the manually verified start dates and end dates. Such binary stamps can be easily referred to while serving different tasks. For report text data, we manually verify the

    & Raw &  &  \\   & Tokens & Relevance(\%) & Coverage (\%) & Tokens & Relevance(\%) & Coverage (\%) & Tokens \\  Report & \(17.4\)k & \(84.3\)\(\)\(2.7\) & \(34.1\)\(\)\(26.8\) & \(37.6\)\(\)\(11.9\) & \(82.3\)\(\)\(28.5\) & \(33.8\)\(\)\(27.3\) & \(74.6\)\(\)\(16.1\) \\ Search & \(54.4\) & \(16.8\)\(\)\(3.3\) & \(90.7\)\(\)\(13.5\) & \(38.4\)\(\)\(4.0\) & \(16.0\)\(\)\(4.3\) & \(81.9\)\(\)\(17.2\) & \(62.8\)\(\)\(3.3\) \\   

Table 2: Statistics for text data. Relevance indicates the percentage of text data with relevant content. Coverage describes the proportion of numerical series data being covered by at least one fact. Details are provided in Appendix E. The statistics highlight the need for both reports and search data.

Figure 3: Visualization of relevant report (a, left) and search (b, right) counts in Time-MMD over time. Text counts from both reports and searches increase over time. Domains receiving more attention, such as the economy, contain more available relevant text data..

timestamps based on the release notes or report contents. For search data, we integrate adjacent search results within each week and mark timestamps correspondingly.

### Considerations for Fairness and Data Release

To consider fairness, we gather data from both the United States and African regions in the Health domain. As depicted in Figure 2, the numerical data of African region exhibits weaker periodicity. Figure 3 shows that the African region has considerably fewer reports compared to the United States. We encourage researchers to consider underrepresented groups when conducting multimodal TSA tasks.

To support various existing and potential novel TSA tasks, we include the following metadata when releasing Time-MMD: (1) Numerical Data: start & end time, target variable, other variables; (2) Text Data: start & end time, fact text (content & data source), prediction text (content & data source).

## 3 Multimodal Time-Series Forecasting Library: MM-TSFlib

In this section, we aim to illustrate the potential benefits of our Time-MMD for multimodal TSA by focusing on time-series forecasting (TSF), a fundamental TSA task. TSF involves predicting future events or trends based on historical time-series data. While most existing TSF methods primarily depend on numerical series, we aim to extend these unimodal TSF methods to multimodality. To achieve this, we contribute both formulating the multimodal TSF problem as well as introducing MM-TSFlib, the first-cut reasonable approach for multimodal TSF.

### Problem Formulation

Conventional unimodal TSF models take a numerical series as input and output future values of some or all of its features. Let the input variable of the numerical series be denoted as \(^{l d_{}}\), where \(l\) is the length of the _lookback window_ decided by domain experts and \(d_{}\) is the feature dimension at each time step. The output variable of the forecasts generated of _horizon window_ length \(h\) is denoted as \(^{h d_{}}\), where \(d_{}\) is the dimension of targets at each time step. For the sample at time step \(t\), denoted as \((_{t},_{t}),_{t}=[_{t-l+1}, _{t-l+2},,_{t}]\) and \(_{t}=[_{t+1},_{t+2},,_ {t+h}]\). Thus, the unimodal TSF model parameterized by \(\) is denoted as \(f_{}:\).

For multimodal TSF, the input variable of the textual series is also considered, which can be denoted as \(^{k d_{}}\), where \(k\) is the lookback window length of the text series, independent of \(l\), and \(d_{}\) is the feature dimension of the text. Although the text variable may have inconsistent feature dimensions, we slightly abuse the notation \(d_{}\) here for brevity. Thus, the multimodal TSF model parameterized by \(\) is denoted as \(g_{}:\).

### Pioneering Solution for Multimodal TSF

**Multimodal Integration Framework.** We propose a pioneering multimodal integration framework to extend existing unimodal TSF models to their multimodal versions. As illustrated in Figure 5, our framework features an end-to-end pipeline that integrates open-source language models with

Figure 4: Word cloud visualization for influenza patient proportion from the health domain. The discarded texts are identified by the LLM as irrelevant to the target variable. The results validate the effectiveness of LLM preprocessing.

various TSF models. Numerical and textual series are independently modeled using unimodal TSF models and LLMs with projection layers. These outputs are then combined using a learnable linear weighting mechanism to produce the final prediction. To reduce computational costs, we freeze the LLM parameters and train only the additional projection layers. We employ pooling layers to address the inconsistent dimensions of textual variables. This framework features an end-to-end training manner, with minimal overhead in trainable parameters.

**Multimodal TSF Library**. Building upon the multimodal dataset Time-MMD and integration framework, we present the first multimodal TSF library, named **MM-TSFilb**. MM-TSFilb supports multimodal extensions of over 20 unimodal TSF algorithms through 7 open-source (large) language models, including BERT (14), GPT-2 (50) (Small, Medium, Large, Extra-Large), Llama-2-7B (58), and Llama-3-8B4. We detail the implementations and language models in Appendix J.

MM-TSFilb is designed for ease of use with Time-MMD in multimodal TSA. Additionally, MM-TSFilb serves as a pilot toolkit for evaluating the multimodal extensibility of existing TSF models.

## 4 Experiments for Multimodal TSF

Based on the constructed MM-TSFilb, we further conduct comprehensive experiments to demonstrate the superiority of multimodal TSF and the high quality of Time-MMD."We further investigate the impact of data domains, horizon window size and the text modeling method.

### Experimental Setup

We adhere to the general setups following existing TSF literatures [67; 66; 48]. Regarding the horizon window length, we consider a wider range **from short- to long-term TSF** tasks, with four different lengths for each dataset according to frequency. We conduct TSF tasks on **all 9 domains of Time-MMD**. We employ the widely-adopted mean squared error (MSE) as the evaluation metric. A higher MSE indicates a better performance.

We comprehensively consider **12 advanced unimodal TSF methods across 4 types** including: (1) Transformer-based: Transformer , Reformer , Informer , Autoformer , Crossformer , Non-stationary Transformer ,FEDformer , iTransformer . (2) MLP-based: DLinear . (3) Agnostic: FiLM . (4) LLM-based: Time-LLM . Unless otherwise specified, we use GPT-2-Small as the LLM backbone in MM-TSFilb. We deployed two sets of experiments upon each TSF model, i.e., both unimodal and multimodal versions. More details about experimental setup are provided in Appendix K.

### Experimental Results

Our experiments aim to investigate the following five aspects. Appendix O provides detailed results.

**Effectiveness of multimodal TSF.** Figure 6 shows average MSE results for corresponding unimodal and multimodal versions of each TSF backbone. The multimodal versions consistently outperform corresponding unimodal versions. As detailed in Appendix O, the multimodal versions outperformed their unimodal versions in **95%** of over 1,000 experiments, reducing the mean squared error by over **15%** in average and up to **40%** in domains with rich textual data. Such consistent improvements fully validate the superiority of multimodal TSF and the effectiveness of our proposed multimodal framework in Section 3.

Furthermore, we observe that different TSF backbones benefit from multimodal extension to varying degrees. For example, the originally inferior Informer exhibits strong multimodal performance, which we attribute to its intrinsic design for modeling long-range dependencies that may benefit more from textual cues.

Additional experiments on text integration approach approaches are provided in Appendix L. We hope these results inspire more advanced multimodal TSF solutions.

**Quality of Time-MMD dataset.** Figure 6 shows that SOTA unimodal TSF models, such as iTransformer and PatchTST, maintain leading unimodal performance, validating the quality of Time-MMD's numerical data. Moreover, multimodal extension consistently and significantly improves performance by incorporating textual data, confirming the quality of Time-MMD's textual data.

**Influence of data domains.** Figure 6(a) shows the relationship between the relevant fact count and the reduced MSE via multimodal extension for each domain. The scatter plot generally illustrates a positive linear correlation, aligning with the innovation of integrating textual information. Besides, domain characteristics also influence multimodal performance, even with a similar fact count. For example, the security domain, focusing on disasters and emergency grants, exhibits higher unpredictability in the future thus benefits less from the historical textual information. This observation highlights the importance of Time-MMD's coverage of 9 diverse domains.

**Influence of the horizon window size.** Figure 6(c) shows the relationship between horizon window size and the average MSE reduction for each domain. Overall, the MSE reduction is stable and promising across different horizon window size, from short term to long term. These results demonstrate that the effectiveness of multimodal TSF is robust to different forecast horizon requirements.

**Influence of the text modeling method.** Firstly, we varied the LLM backbone in MM-TSFlib and evaluated corresponding multimodal performance on health domain. As shown in Figure 6(b), the choice of LLM backbone does not exhibit a significant correlation with multimodal TSF performance. For the GPT2 series, the scaling law is unclear for multimodal TSF, indicating no clear positive correlation between the parameter scale and TSF performance. Across different LLMs, multimodal TSF performance is relatively similar, even between the advanced Llama-3-8B and the earlier BERT. There might be three possible reasons: (1) Our proposed multimodal framework, although effective, still does not fully utilize the power of LLMs, particularly by only fine-tuning through projection layers. (2) Existing LLMs, pre-trained for natural language tasks, may not be directly suitable for multimodal TSF. (3) The embedding dimension of BERT is 768, much lower than the 4096 dimension

Figure 7: Results of exploratory experiments. (a) Influence of data domains: inherent characteristics and text richness influence performance. (b) Influence of LLM backbones: unclear correlation between multimodal TSF performance and LLM natural language capabilities. (c) Influence of horizon window size: improvements via multimodality are robust to horizon window size.

size of Llama-3-8B, thereby makes it easier to fine-tune an effective projection layer with limited training data.

Although LLMs are currently the mainstream approach for text modeling, they may not be suitable for certain scenarios, such as unfamiliar domains or unsupported languages. Therefore, we introduce Doc2Vec [(33)] a text embedding model trained from scratch, as an alternative for text modeling. The experimental results on three datasets across four prediction horizons are provided in Table 6. The results show that Doc2Vec is also effective but generally performs worse than BERT. Doc2Vec is further included in our MM-TSFlib, which will greatly enhance applicability to underrepresented languages and domains.

We provide additional experiments and discussions on multimodal modeling approaches in Appendix N, including the introduction of attention mechanisms [(59)] and the use of closed-source LLMs, such as GPT-3.5.

In summary, all these observations suggest that there is significant room for improvement in the methodological research for multimodal TSF.

## 5 Potential Future Works

Beyond its efficacy in enhancing time-series forecasting accuracy through multimodality extension (Section 4), Time-MMD holds significant potential in advancing time series analysis across a wide spectrum. In the following section, we discuss how Time-MMD might transform conventional approaches, facilitate novel methodologies, and may broadly impact the time series analysis domain in future works.

**Multimodal Time-Series Imputation.** Missing values in time series data, caused by sensor failures, system instability, or privacy concerns, pose a significant challenge in analysis. Conventional time-series imputation (TSI) methods [(8; 19; 3; 4)] often overlook valuable information captured in textual formats alongside the numerical data. For instance, incident reports, weather conditions, and special events can provide crucial context for imputing missing data points in traffic time series, but current methods fail to effectively incorporate this information. By constructing missing values using existing toolkits [(16)], Time-MMD can directly serve as multimodal imputation datasets. Time-MMD enables the integration of textual contextual information with numerical time series data, opening new avenues for multimodal time series analysis and enhancing imputation accuracy.

**Multimodal Time-Series Anomaly Detection.** Detecting anomalies in time series data is crucial for identifying unusual patterns that may indicate faults, fraud, or other significant events [(10; 5; 75)]. However, conventional anomaly detection methods [(47; 7; 69; 53)] are limited to expected pattern deviations in numerical data, overlooking valuable information in textual formats. For instance, news articles, social media posts, and market reports can provide critical context that influences financial market behavior and helps identify anomalies not evident from numerical data alone. Time-MMD enables a feasible way to support multimodal time series anomaly detection. For example, in influenza (health) dataset, if "flu outbreak" is mentioned in the text data, we can initially label the aligned numerical data as anomalous.

**Multimodal Foundation Time-Series Models.** The introduction of Time-MMD, a comprehensive text-numeric TS dataset, is expected to significantly advance multimodal-based TS methods, including the development of multimodal foundation TS models beyond existing unimodal models [(30)]. Time-MMD will facilitate further exploration with more complex and informative prompts, enhancing the performance and capabilities of fine-tuning methods. Additionally, it might spur research and development of multimodal models specifically tailored for TSA, an area with limited exploration compared to other domains like vision and video generation [(54; 62; 68; 17)].

## 6 Ethics Statement

While collecting data from government and news websites, we rigorously adhered to ethical standards to ensure compliance with website policies and avoid potential conflicts of interest. Mindful of copyright and regional policies, we restricted our collection to content freely available without premium access or subscription requirements. In collecting data from web searches, we used Google's official API to ensure that the data strictly complied with ethical standards.

## 7 Conclusion

In this work, we propose Time-MMD, the first multi-domain multimodal time series dataset, and develop MM-TSFlib, the first multimodal time series forecasting library, which facilitates a pilot study for multimodal time series analysis on Time-MMD. We conduct extensive experiments to demonstrate the high quality of Time-MMD, the effectiveness of MM-TSFlib, and the superiority of integrating textual information for time series analysis. We envision that this work catalyzes the transformation of time series analysis from unimodal to multimodal by integrating natural language.

**Acknowledgements:** This paper was supported in part by the NSF (Expeditions CCF-1918770, CAREER IIS-2028586, CAREER IIS-2144338, Medium IIS-1955883, Medium IIS-2106961, Medium IIS-2403240, PIPP CCF-2200269), CDC MInD program, Meta faculty gifts, and funds/computing resources from Georgia Tech and GTRI. This work used the Delta GPU Supercomputer at NCSA of UIUC through allocation CIS240288 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program.