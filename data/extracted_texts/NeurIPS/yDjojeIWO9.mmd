# Transferable Adversarial Attacks on SAM and Its Downstream Models

Song Xia\({}^{1}\), Wenhan Yang\({}^{2}\), Yi Yu\({}^{1}\), Xun Lin\({}^{3}\), Henghui Ding\({}^{4}\),

**Lingyu Duan\({}^{2,5}\), Xudong Jiang\({}^{1}\)**

\({}^{1}\)Nanyang Technological University, \({}^{2}\)Pengcheng Laboratory,

\({}^{3}\)Beihang University, \({}^{4}\)Fudan University, \({}^{5}\)Peking University

{xias0002,yuyi0010,exdjiang}@ntu.edu.sg, yangwh@pcl.ac.cn,

linxun@buaa.edu.cn, hhding@fudan.edu.cn, lingyupku@edu.cn

Corresponding author (exdjiang@ntu.edu.sg)

###### Abstract

The utilization of large foundational models has a dilemma: while fine-tuning downstream tasks from them holds promise for making use of the well-generalized knowledge in practical applications, their open accessibility also poses threats of adverse usage. This paper, for the first time, explores the feasibility of adversarial attacking various downstream models fine-tuned from the segment anything model (SAM), by solely utilizing the information from the open-sourced SAM. In contrast to prevailing transfer-based adversarial attacks, we demonstrate the existence of adversarial dangers even without accessing the downstream task and dataset to train a similar surrogate model. To enhance the effectiveness of the adversarial attack towards models fine-tuned on unknown datasets, we propose a universal meta-initialization (UMI) algorithm to extract the intrinsic vulnerability inherent in the foundation model, which is then utilized as the prior knowledge to guide the generation of adversarial perturbations. Moreover, by formulating the gradient difference in the attacking process between the open-sourced SAM and its fine-tuned downstream models, we theoretically demonstrate that a deviation occurs in the adversarial update direction by directly maximizing the distance of encoded feature embeddings in the open-sourced SAM. Consequently, we propose a gradient robust loss that simulates the associated uncertainty with gradient-based noise augmentation to enhance the robustness of generated adversarial examples (AEs) towards this deviation, thus improving the transferability. Extensive experiments demonstrate the effectiveness of the proposed universal meta-initialized and gradient robust adversarial attack (UMI-GRAT) toward SAMs and their downstream models. Code is available at https://github.com/xiasong0501/GRAT.

## 1 Introduction

Large foundation models that are trained on a broad scale of data have gained massive success in various applications , such as vision-language chatbot , text-image generation , image-grounded text generation , and anything segmentation . The segment anything model (SAM) , trained on vast amounts of data from the SA-1B dataset, is capable of handling diverse and complex visual tasks. The open accessibility of SAM makes it a promising foundation model, serving as the starting point for fine-tuning analytics models in certain domains and downstream applications, _e.g.,_ medical segmentation , 3D object segmentation , camouflaged object segmentation , overhead image segmentation , and high-quality segmentation . However, many studies  have highlighted the secure issues of deep learning models towards adversarial attacks. By corrupting the clean input with a finely crafted andEarly imperceptible adversarial perturbation, the attacker can mislead the well-trained model at a high success rate, with limited information available (_e.g.,_ the surrogate model or limited queries). Consequently, significant concerns arise regarding fine-tuning open-sourced models, as it inevitably leaks critical information on downstream models, increasing their vulnerability to adversarial attacks.

Existing adversarial attacks can be roughly categorized into white-box attacks [3; 18] and black-box attacks [56; 23], based on whether the attacker can fully access the victim model. The pre-requisite of fully accessing the victim model complicates the practical deployment of white-box attacks. Conversely, transfer-based black-box attacks that require less information pose a substantial threat to real-world applications. The prevalent transfer-based black-box adversarial approaches [8; 12; 17; 22; 32; 34; 36; 15; 60] typically suppose strong prior knowledge of the victim model's task and training data, such as a 1000-class classification task in ImageNet, thereby facilitating the training of a similar surrogate model to generate potent adversarial examples (AEs). However, few studies [64; 63] consider a more practical and challenging scenario wherein the attacker is unaware of the victim model's tasks and the associated training data, due to stringent privacy and security protection policies (_e.g.,_ datasets containing medical or human facial information). Moreover, the increasing size of large foundation models significantly amplifies the costs of training effective task-specific surrogate models. Thus, a more general and practical security concern is to explore the capability of attackers to mount adversarial attacks on any victim model even without the need of accessing to its downstream tasks-specific datasets to train a closely aligned surrogate model.

Given the practical security concerns of utilizing large foundation models, this paper investigates the potential risks associated with fine-tuning the open-sourced SAM on a private and encrypted dataset. We introduce a strong transfer-based adversarial attack called universal meta-initialized and gradient robust adversarial attack (UMI-GRAT), which effectively mislead SAMs and their various fine-tuned models without accessing the downstream task and training dataset, as illustrated in Figure 1.

The contributions of our paper are summarized as follows:

* We begin an investigation into a more practical while challenging adversarial attack problem: attacking various SAMs' downstream models by solely utilizing the information from the open-sourced SAMs. We provide the theoretical insights and build the experimental setting and benchmark, aiming to serve as a preliminary exploration for future research.
* We propose an offline universal meta-initialization (UMI) algorithm to extract the intrinsic vulnerability inherent in the foundation model, which is utilized as prior knowledge to enhance the effectiveness of adversarial attacks through meta-initialization.
* We theoretically formulate that, when using the open-sourced SAM as the surrogate model, a deviation occurs inevitably from the optimal direction of updating the adversary. Correspondingly, we propose a gradient robust loss to mitigate this deviation.
* Extensive experiments demonstrate the effectiveness of the proposed UMI-GRAT toward SAMs and its downstream models. Moreover, UMI-GRAT can serve as a plug-and-play strategy to significantly improve current state-of-the-art transfer-based adversarial attacks.

Figure 1: An illustration of UMI-GRAT towards SAM and its downstream tasks. The UMI-GRAT can mislead various downstream models by solely utilizing information from the open-sourced SAM.

Related Work

The adversarial attacks aim to mislead the target (or victim) model by adding a small adversarial perturbation in the clean input. Existing black-box attacks can be broadly categorized into query-based and transfer-based adversarial attacks.

### Query-based black box attacks

The query-based attacks consider the scenarios where the attacker does not have enough information to train a satisfying surrogate model, thus generating adversarial examples by interacting and analyzing the outputs from the victim model. This kind of attack can be divided into score-based query attacks (SQAs) [12; 24; 47] that update the AEs by observing the change of the model's prediction (_e.g.,_ the logits or softmax probability) and decision-based query attacks (DQAs) [7; 10] that only rely on the model's top-1 prediction to update the AEs. However, this black-box search is naturally the NP-hard problem, and solving the optimal update strategy is non-differentiable. This makes query-based attack requires thousands of interactions with the victim model, making query-based attacks characterized by low throughput, high latency, and marked conspicuousness to attack real-world deployed systems.

### Transfer-based black box attacks

The transfer-based adversarial attacks generate the AEs to mislead the victim model based on a similar surrogate model. Existing work mainly focuses on improving the transferability of AEs, which can be categorized into four groups: input-augmentation-based attacks [8; 56; 52] that enhances the effectiveness of generated AEs by augmenting the clean input (_e.g.,_ using crop or rotation), optimization-based attacks [13; 35; 51; 62] that utilizes a better optimization strategy to guide the update of AEs, model modification-based attacks [53; 4] or ensemble-based attacks [19; 43; 37; 33] that enhances the AEs by utilizing a more powerful surrogate model, and feature-based attacks [32; 34] that attack the extracted feature in the intermediate layer. However, most of the work makes a strong assumption that the surrogate and victim models are optimized for the same task with identical data distribution, for example, both surrogate and victim models are optimized on the ImageNet dataset to complete the classification task. In real-world deployed systems, due to privacy and security concerns, attackers typically cannot access the training data (_e.g.,_ datasets containing private information) or obtain the optimization objectives of the victim model, making training a similar surrogate model exceedingly difficult and unfeasible.

To address the challenge of deploying transfer-based black-box attacks without knowing the victim model's task and training dataset, this paper investigates the feasibility of attacking any victim model optimized for unknown tasks and distributions that significantly differ from the open-sourced surrogate model. We provide both theoretical and analytical evidence demonstrating that our proposed method can enhance the transferability and effectiveness of the generated AEs. Moreover, the proposed MUI-GRAT can serve as a plug-and-play adversarial generation strategy to enhance most existing transfer-based adversarial attacks for this challenging task.

## 3 Preliminaries

### Adversarial attacks

Let \(f\) be any deep learning model and \(\) be the loss function (_e.g.,_, the cross-entropy loss) that evaluates the quality of the model's prediction. Let \(_{}()=\{^{}:\|^{}- \|_{p}\}\) be an \(_{p}\)-norm ball centered at the input \(\), where \(\) is a pre-defined perturbation bound. For each input \(\), the untargeted adversarial attacks aim to find an adversarial perturbation \(\) by solving:

\[_{+_{}()}(f (),f(+)).\] (1)

An effective solution to Equation 1 is iteratively updating the adversarial perturbation \(\) based on the gradient of the loss function, for example, the iterative fast sign gradient method (I-FGSM) , which iteratively updates \(\) by:

\[_{t+1}=clip_{_{}}\{_{t}+  sign(_{_{t}}(f(), f(+_{t})))\},\] (2)where \(\) calculates the gradient and \(sign\) returns the sign (_i.e.,_-1 or +1). \(\) is a pre-defined step size to update the adversarial perturbation. \(clip\) constrains the magnitude of the perturbation by projecting \(\) into the boundary of the \(_{p}\)-norm ball \(_{}\).

### Segment anything model

The SAM consists of three parts: an image encoder \(f_{_{im}}\), a lightweight prompt encoder \(f_{_{pr}}\), and a lightweight mask decoder \(f_{_{mk}}\). SAM gives the mask prediction based on the image input \(\) and prompt input \(\), which is expressed as:

\[=(,)=f_{_{mk}}(f_{_{im}} (),f_{_{pt}}()),\] (3)

where \(f_{_{im}}\) is the image encoder that provides the fundamental understanding by converting natural images into feature embeddings and \(f_{_{pt}}\) extracts prompt embeddings. \(f_{_{mk}}\) is the mask decoder that gives the mask prediction by fusing the information from both feature and prompt embeddings.

## 4 Methodology

### Problem formulation

Let \(f_{_{s}}\) denote the foundation model trained on a general dataset \(D\), and \(f_{_{r}}\) denote the victim model fine-tuned on any downstream dataset \(D_{}\), the parameters of those two models typically satisfy that:

\[_{s}=_{_{s}}}_{(,) D}[(f_{_{s}}(), )];_{}=_{_{}}}_{(_{},_{}) D_{}}[ _{}(f_{_{}}(_{}),_ {})],\ _{}^{}^{}^{}^{}^{ }}}{}_{s}\] (4)

**Definition 1** (Transferable adversarial attack via open-sourced SAM).: _For any SAM's downstream model \(f_{_{s}}\) and the clean input \(x_{}\), without any further information on the downstream task and dataset, the attacker aims to find the adversarial perturbation \(_{s}\) such that:_

\[_{_{s}_{}}_{}(f_{ {}_{}}(_{}),f_{_{}}(_{ }+_{s}))\{_{s}=(f_{_{s}},_{})\},Private(D_{})\},\] (5)

_where \(\) is the adversarial attack strategy and \(f_{_{s}}\) is the open-sourced SAM. A solution to that is fine-tuning an optimal surrogate model \(f_{_{s}^{}}\) that closely aligns with the victim model. However, this approach becomes extremely challenging, when the downstream dataset is inaccessible to the attacker. Alternatively, an effective solution is to design an optimal attack strategy \(^{*}\) such that:_

\[^{*}=_{^{*}}}_{(_ {},_{}) D_{}}[_{}(f_{ _{}}(_{}),f_{_{}}(_{ }+^{*}(f_{_{s}},_{}))].\] (6)

Notably, \(f_{_{s}}\) and \(f_{_{}}\) are optimized on two distinctive distributions \(D\) and \(D_{}\) with losses \(\) and \(_{}\), leading to a significant input-output mapping gap and gradient disparity, such as \(Cosine\_similarity( f_{_{s}}(_{}), f_{_{s}}( _{})) 1\). This misalignment critically undermines the effectiveness of current gradient-based adversarial attack strategies.

**Further analysis on attacking SAMs.** The standard operation to deploy SAM on downstream tasks \(\) involves fine-tuning the image encoder \(f_{_{im}}\) to inherit some well-generalized knowledge. Concurrently, the lightweight prompt encoder \(f_{_{pt}}\) and the mask decoder \(f_{_{mk}}\) are trained from scratch to better accommodate the task. Considering the pivotal importance of feature embeddings and the substantial variation caused by full retraining, an intuitive approach to generate effective adversarial perturbation \(_{s}\) is utilizing the common information in \(f_{_{im}}\) to achieve:

\[_{_{s}_{}}(f_{_{ im}^{*}}(_{}),f_{_{im}^{*}}(_{}+_{s}))\ _{s}=^{*}(f_{_{im}},_{}),\] (7)

where \(_{im}^{*}\) denotes the updated parameters for the downstream model's image encoder after fine-tuning. Unless otherwise specified, we denote \(_{im}\) as \(\) in our subsequent content and take the general SAM image encoder \(f_{}\) as the surrogate model \(f_{_{s}}\). We aim to generate the transfer-based AEs to attack any fine-tuned image encoder \(f_{_{}}\) on task \(\), thereby misleading the entire prediction.

### Extract the intrinsic vulnerability via universal meta initialization

Considering the great variation brought by fine-tuning the model on a new task \(\), we aim to extract the intrinsic vulnerability of the foundational model that remains invariant after fine-tuning. Subsequently, this extracted vulnerability is leveraged as prior knowledge to initialize and enhance the adversarialattack \(^{*}\). Inspired by the universal adversarial perturbation  that maintains effectiveness across various inputs, we propose the universal meta initialization (UMI) algorithm, which optimizes the initialization of adversarial perturbation to ensure both effectiveness and fast adaptability by meta-learning . We define the universal and meta-initialized perturbation \(\) as follows.

**Definition 2** (**Universal and meta-initialized perturbation \(\)**).: _Given the foundation model \(f_{}\) and its fine-tuned models \(f_{_{}}\) on downstream tasks \(\), the universal and meta-initialized perturbation \(\) that extracts the intrinsic vulnerability ensures both effectiveness and fast adaptability, which are:_

1. _[leftmargin=*]_
2. _Effectiveness (universal adversarial perturbation):_ \(\) _extracts the intrinsic vulnerability in the foundation model, which can mislead the_ \(f_{}\) _successfully on most natural inputs_ \(\)_, which is:_ \[_{_{}} D}{ }[\{(f_{}(),f_{ }(+))>\}],\] (8) _where_ \(\) _is a pre-defined threshold for one successful attack, and_ \(\{\}\) _is the indicator function that returns 1 if the inside condition is satisfied, else 0._
3. _Fast adaptability (meta-initialization):_ _for any downstream task_ \(\) _with the corresponding private downstream dataset_ \(D_{}\) _and model_ \(f_{_{}}\)_, the attackers can maximize the loss_ \(\) _on downstream model_ \(f_{_{}}\) _by updating the initialization_ \(\) _via the surrogate model_ \(f_{}\) _in_ \(t\) _steps, which is:_ \[_{_{}}_{} D_{ }}{}[_{}(f_{_{}}( _{}),f_{_{}}(_{}+U^{t}()))],\] (9) \(U^{t}\) _is the operation to update_ \(\) _for_ \(t\) _steps based on input_ \(_{}\)_, which is defined as:_ \[U^{t}()=clip_{B_{}}[+ _{j=1}^{t}_{j}],\] (10) _where_ \(_{j+1}=_{} sign(( f_{}(_{}),f_{}(_{}+U^{j} ())))\) _if we attack the surrogate model_ \(f_{}\) _and update the adversarial perturbation based on the first-order gradient._

Generally, Equation 8 aims to extract the intrinsic vulnerability inherent in the model, which remains effective towards the input variation. Equation 9 guarantees that utilizing the perturbation \(\) as the initialization can rapidly threaten strong adversarial attacks for any downstream model. However, in Equation 9, \(D_{}\) and \(f_{_{}}\) are unknown if the attacker is precluded from the downstream dataset. An approximated solution to that involves using the dataset \(D\) that covers the distribution of most natural inputs and a general model \(f_{}\) that can approximately represent the expectation of \(f_{_{}}\), which is:

\[_{_{}} D}{ }[(f_{}(),f_{}( +U^{t}()))].\] (11)

To optimize the above two objectives simultaneously, our learning aims to move towards the direction that maximizes the inner product of the gradients computed on both objectives. We utilize a first-order meta-learning algorithm called Reptile , which defines the noise \(\) update in each round as:

\[=+_{i=1}^{n}( {}_{_{i}}-),\] (12)

where \(\) is the update step size, and \(}_{_{i}}=U_{_{i}}^{t}()\) is the updated perturbation on objective \(_{i}\) after optimizing \(t\) iterations. Here we set \(n=2\), corresponding to the two objectives in Equation 8 and 11. For \(_{1}\) that aims to optimize Equation 11, we set \(t=5\) and \(U_{_{1}}^{t}()\) the same as \(U^{t}\) defined in Equation 10. For \(_{2}\) that aims to optimize Equation 8, we set \(U_{_{2}}^{t}()\) as:

\[U_{_{2}}^{t}()_{+ }\|\|_{},\ (f_{}(),f_{} (x++))>,\ =_{j=1}^{t}_{j}.\] (13)

Equation 13 aims to find a minimal update \(\) nearby \(\) to mislead the model \(f_{}\). This can be achieved by using enough iterations \(t\) and a small but gradually increased norm-ball boundary \(\). While finding an effective UMI requires a substantial number of inputs and iterations, this process can be conducted fully offline, thus not hindering real-time adversarial attacks.

### Enhance the transferability via gradient robust loss

Besides the utilization of intrinsic weakness inherent in the foundation model to enhance the adversarial attack \(^{*}\), another method involves generating the adversarial perturbation that sustains robustness against the deviation arising from updates through a surrogate model that exhibits significant gradient disparity compared to the fine-tuned downstream model.

Let us first assume that the surrogate model \(f_{}\) consists of \(m\) sequentially connected modules, denoted as \(\{f_{^{1}}^{1},,f_{^{m}}^{m}\}\). The outputs of those modules are denoted as \(\{^{1},,^{m}\}\), with \(^{i}=f_{}^{i}(^{i-1})\). For the victim model \(f_{_{}}\) with updated parameter \(_{}\), the modules are denoted as \(\{f_{^{1}+^{1}}^{m},,f_{^{m}+^{m}}^{m}\}\). The output \(_{}\) of each module with the input \(_{}\) is denoted as:

\[_{}^{i}=f_{^{i}+^{i}_{}}^{i}( _{}^{i-1})=f_{^{i}}^{i}(_{}^{i-1})+h_{ ^{i}_{}}^{i}(_{}^{i-1}),\] (14)

where \(_{}^{0}=_{}\) and \(h_{^{i}_{}}^{i}\) is a hypothetical function that characterizes the update brought by \(^{i}_{}\).

**Proposition 1** (**Deviation in updating adversarial perturbation**).: _Let \(f_{_{}}\) be the victim model fine-tuned on any unknown task \(\), the deviation in the direction of updating the adversarial perturbation by maximizing a predefined loss \(\) in the surrogate model \(f_{}\) can be formulated as:_

\[_{}-_{s}=(_{} ^{m})(_{i=1}^{m}( f_{^{i}}^{i}( _{}^{i-1})+ h_{^{i}_{}}^{i}(_{ }^{i-1}))-_{i=1}^{m} f_{^{i}_{}}^{i} (_{}^{i-1})).\] (15)

In Equation 15, \(_{}(_{}^{m}) _{i=1}^{m}( f_{^{i}}^{i}(_{}^{i-1} )+ h_{^{i}_{}}^{i}(_{}^{i-1} ))\) is the update of adversarial perturbation if white-box attack the victim model and \(_{s}(_{}^{m}) _{i=1}^{m} f_{^{i}}^{i}(_{}^{i-1})\) is the update of adversarial perturbation by maximizing the pre-defined \(\) on feature embeddings of the surrogate model. Proposition 1 establishes by simultaneously considering the white-box scenarios for both surrogate and victim models to derive the gradient using the chain rule. It claims that \(h_{^{i}_{}}^{i}\) leads to a great deviation in updating the adversarial perturbation \(_{s}\) towards the optimal solution in attacking victim model if directly maximizing the feature embedding distance of the surrogate model, thus degrading the effectiveness of the generated AEs.

**Mitigate the deviation caused by gradient disparity.** To enhance effectiveness of the generated adversarial perturbation \(_{s}\) under the hypothetical update \( h_{_{}}\), we propose a gradient robust loss \(_{GR}\), that aims to mitigate the deviation in Equation 15 by gradient-based noise augmentation. Denote \((;,^{2})\) as the isotropic Gaussian noise with mean \(\) and variance \(^{2}\), which has the same dimension as \( h_{_{}}\). The robust update of adversarial perturbation \(_{s}^{*}\) on the surrogate model based on the noised augmented gradient is:

\[_{s}^{*}(_{}^{m}) _{i=1}^{m}( f_{^{i}}^{i}(_{}^{i-1} )+_{i} f_{^{i}}^{i}(_{ }^{i-1})).\] (16)

By ignoring higher-order uncertain terms in Equation 16, we can simplify it as:

\[_{s}^{*}(_{}^{m}) (_{i=1}^{m} f_{^{i}}^{i}(_{}^{i-1} )+_{i=1}^{m}_{i}_{j=1}^{i-1} f_{^{j}}^{j}(_{}^{j-1})).\] (17)

Following the adversarial perturbation update guidance in Equation 17, the corresponding gradient robust loss \(_{GR}\) is defined as :

\[_{GR}=\|f_{^{m}}^{m}(_{}^{m_{adv}}) -f_{^{m}}^{m}(_{}^{m})+_{i=1}^{ m-1}_{i}(f_{^{i}}^{i}(_{}^{i_{adv}} )-f_{^{i}}^{i}(_{}^{i}))\|_{p},\] (18)

Figure 2: The data flow of our UMI-GRAT, consisting of an offline learning process of UMI and a real-time gradient robust adversarial attack.

where \(^{i_{adv}}\) is the extracted adversarial feature by layer \(i\) and \(_{p}\) is a predefined norm-based measure that is decided by the \(\) (_e.g.,_\(p=1\) for L1 loss).

**Discussion with the intermediate-level attacks**. The intermediate-level attacks (ILAs) [34; 22; 32] also aim to maximize the dissimilarity of feature embeddings between the clean and adversarial inputs. However, the main concern in ILAs is how to find a directional vector \(\) to guide the update direction of \(f_{}()-f_{}(^{adv})\), thus assuring that this feature-vised dissimilarity can maximally mislead the final prediction. Different from that, our \(_{GR}\) considers the problem one step further: given an optimal directional vector \(\), how to generate the adversarial perturbation that is roust towards the potential gradient variation in the victim model, thus maximally misleading the victim model along the direction of \(\). Our core idea is hence in parallel with ILAs and can be well combined with them to enhance the attack's effectiveness. We experimentally analyze and visualize the cosine similarity of the generated perturbations on CT-scan images by white-box attacking open-sourced SAM and medical SAM using MI-FGSM , ILPD , and our gradient robust attack in Figure 3, illustrating that the proposed \(_{GR}\) effectively reduce the deviation caused by gradient variation and achieves a much transferability.

## 5 Implementation of the proposed MUI-GRAT

The detailed implementation of our proposed MUI-GRAT is illustrated in Algorithm 1 and Figure 2. Our UMI-GRAT method consists of two stages. The initial stage involves the offline learning of a universal meta-initialization (UMI), which aims to find the intrinsic vulnerability inherent in the foundation model. In the subsequent stage, we utilize the learned UMI as the prior knowledge to enhance the gradient-variation robust adversarial attack.

We utilize the image encoder from Vit-H-based SAM as the general foundation model \(f_{}\) for generating the UMI. The natural image dataset \(D\) consists of a total of 20,000 images, with 10,000 from ImageNet and 10,000 from the SA-1B dataset. We set the meta iterations \(T_{m}\) as 7 and the universal step size \(\) as 1. The function Uni_Meta_In returns the learned UMI \(\) that can be used to enhance the generation of subsequent input-specific adversarial perturbation.

In GR_Attack, we first adapt the calculated UMI \(\) with the task-specific image \(_{}\) by one-step update using FGSM  with the step size \(_{adp}=4\). Assume that \(}\) represents the mean of the feature embedding of natural images calculated by \(f_{}\), our \(_{adp}\) is defined as:

\[_{adp}= mean(f_{}(_{}))-}_{p}.\] (19)

Equation 19 aims to minimize the domain difference between \(vx_{}\) and the natural images by a specific generated perturbation. The UMI \(\) is then added by \(_{adp}\) and utilized to initialize \(_{s}^{*}\). We set \(T_{a}\) to 10, and update the adversarial perturbation \(_{s}^{*}\) by maximizing the gradient robust loss \(_{GR}\).

Figure 3: The cosine similarity of white-box generated perturbations on surrogate and victim models.

## 6 Experimental Results

### Experiment setup

**Evaluation details:** we conduct experiments on SAMs' downstream models including, medical image segmentation SAM , shadow segmentation SAM , and camouflaged object segmentation SAM . The datasets include: the synapse multi-organ segmentation dataset  that contains 3779 abdominal CT scans with 13 types of organs annotated, the ISTD dataset  that contains 1870 image triplets of shadow images, the COD10K dataset  that contains 5066 camouflaged object images, the CHAMELEON dataset that contains 76 camouflaged images, and the CAMO dataset , that contains 1500 camouflaged object images. We report the mean dice similarity score (mDSC) and mean Hausdorff distance (mHD) for evaluating medical segmentation, mean absolute error (MAE) and structural similarity (\(S_{}\)) for camouflaged object segmentation, and the bit error rate (BER) for shadow segmentation. In medical SAM, the image encoder is based on SAM-Vit-B and fine-tuned with LoRA . In shadow segmentation and camouflaged object segmentation SAM, the image encoders are based on SAM-Vit-H and fine-tuned with the adapter . The decoders in those models are all fully retrained.

**Compared methods:** We mainly compare and evaluate our method with current transfer-based adversarial attacks including gradient-based attacks called MI-FGSM  and PGN , input-augmentation based attacks called DMI-FGSM  and BSR , and intermediate-level feature based attack called ILPD .

**Implementation details:** we use the MI-FGSM  as our basic attack method. For all methods reported, we set the attack update iterations \(T_{a}\) as 10, with the \(l_{}\) bound \(=10\) and the step size \(=2\). For our UMI, we set the meta iterations \(T_{m}=7\), universal step size \(=1\). For PGN and BSR, we set the number of examples as 8 for efficiency.

### Main results

We report our main results in Table 1. The first row of the data presents the model performance with clean inputs. The second part of the data shows the model performance under different adversarial attacks, where the data with the strongest attack is bold. The results demonstrate that the adversarial examples generated by our proposed MUI-GRAT are more effective and generalizable than others, consistently posing significant adversarial threats across various downstream models. In medical segmentation and shadow segmentation tasks that share a great difference with the natural segmentation tasks, our proposed MUI-GRAT greatly surpasses others (_e.g.,_ MUI-GRAT reduces the mDSC from 81.88 to 5.22 while the previous best is 25.73.). This demonstrates the exceptional effectiveness of our proposed MUI-GRAT when the attacker lacks information about the victim model, thereby generating AEs using a surrogate model distinct from the victim model. In the camouflaged object segmentation task where the data closely resembles natural images, all methods exhibit strong transferability. Our MUI-GRAT achieves the best performance on the COD10K and CHAME datasets and performs comparably to the SOTA method on the CAMO dataset.

   Model & **Medical SAM ** & **Shadow-SAM ** &  \\  Dataset & CT-Scans & ISTD & COD10K & CAMO & CHAME \\  Metrics & mDSC\(\) & mHD \(\) & BER \(\) & \(S_{a}\) & MAE \(\) & \(S_{a}\) & MAE \(\) & \(S_{a}\) & MAE \(\) \\  Without attacks & 81.88 & 20.64 & 1.43 & 0.883 & 0.025 & 0.847 & 0.070 & 0.896 & 0.033 \\  MI-FGSM  & 40.83 & 64.42 & 4.31 & 0.372 & 0.214 & 0.331 & 0.286 & 0.352 & 0.250 \\ DMI-FGSM  & 34.51 & 74.20 & 4.39 & 0.455 & 0.134 & 0.395 & 0.210 & 0.416 & 0.164 \\ PGN  & 43.15 & 58.03 & 5.16 & 0.368 & 0.230 & 0.336 & **0.318** & 0.340 & 0.275 \\ BSR  & 25.7 & 94.48 & 5.20 & 0.414 & 0.146 & 0.372 & 0.226 & 0.402 & 0.178 \\ ILPD  & 33.65 & 65.98 & 4.40 & 0.366 & 0.245 & **0.310** & 0.316 & 0.327 & 0.287 \\ MUI-GRAT & **5.22** & **111.87** & **12.46** & **0.360** & **0.248** & 0.329 & 0.308 & **0.332** & **0.293** \\  MUI-GRAT+DMI-FGSM & 5.28 & 114.68 & 5.48 & 0.409 & 0.198 & 0.417 & 0.267 & 0.406 & 0.228 \\ MUI-GRAT+PGN & 9.62 & 115.87 & **33.98** & 0.358 & 0.262 & 0.353 & 0.306 & 0.332 & 0.296 \\ MUI-GRAT+BSR & 3.61 & 105.31 & 7.00 & 0.385 & 0.219 & 0.398 & 0.277 & 0.387 & 0.245 \\ MUI-GRAT+ILPD & **3.52** & **121.89** & 15.56 & **0.349** & **0.263** & **0.321** & **0.311** & **0.315** & **0.317** \\   

Table 1: Comparison results of transfer-based adversarial attacks on different models. The surrogate model is the open-sourced SAM.

In the last part of Table 1, we analyze the performance of our proposed MUI-GRAT when combined with other SOTA transfer-based attacks. Notably, all methods achieve an overall performance gain after combining with ours. Though a slight performance drop occurs when attacking the CAMO dataset, combining the MUI-GRAT brings great performance gain on attacking other tasks. This demonstrates the versatility of our MUI-GRAT, which can be seamlessly applied in a plug-and-play manner to bolster existing transfer-based attacks. We report the experiment results for attacking open-sourced SAMs in Appendix A.2 and analyze the real-time attack efficiency for each method in Appendix A.3.

### Analysis of the transferability

The transferability property across diverse models ensures the effectiveness of the adversarial example to mislead an unknown victim model. As discussed in Section 4.1, an intuitive objective for attacking SAM's downstream models is to maximize the dissimilarity of feature embedding extracted from the clean input \(x\) and adversarial input \(x^{adv}\). Based on this, to numerically evaluate the improvement of transferability brought by MUI-GRAT, we present the \(l_{2}\) distance of clean and adversarial feature embeddings attacked by MI-FGSM and ours and then analyze the distance gap between the surrogate and victim models. We propose that a viable transferable attack methodology should induce a substantial feature distance \( f\) in the victim model, while simultaneously ensuring minimal performance degradation \(\) during the transition from the surrogate to the victim model.

We show this comparison result in Figure 4, where we randomly pick a subset of inputs and show the distance of feature embedding for each clean-adversarial input pair. The average distance gap between the surrogate and victim models indicates the overall transferability of the attack method. In the medical and shadow segmentation SAM, where the data and task are distinct from the original SAM, we find a great performance drop for MI-FGSM when transferred from the surrogate model to the victim model. Though the adversarial examples generated by MI-FGSM induce a large feature distance in the surrogate model, their effect on the victim model is relatively minor. Conversely, the adversaries generated by MUI-GRAT maintain much better transferability, suffering from a small performance drop when transferred from the surrogate model to the victim model. In the camouflage object segmentation task, where the data are natural images and the segmentation objective is similar to the original SAM, both attack algorithms show good transferability (nearly no performance drop when transferred from the surrogate model to the victim models). Our MUI-GRAT shows better transferability with nearly no performance drop.

### Ablation study

In this section, we explore the contribution of the proposed MUI and GRAT by integrating them with MI-FGSM and PGN attacks. The ablation results are shown in Table 2. In scenarios where the task and dataset distributions of surrogate and victim models differ markedly (e.g., medical image segmentation and natural image segmentation), we observe that the GR loss significantly enhances effectiveness. Meanwhile, across all scenarios, the proposed MUI consistently contributes to enhancing the adversarial attacks. Particularly in camouflaged object segmentation when the surrogate and victim models exhibit close similarities, the MUI yields substantial benefits.

Figure 4: The \(l_{2}\) distance of feature embedding from clean inputs and adversarial examples. The small distance gap between the surrogate and victim models indicates better transferability.

This observation aligns with our analysis in Section 4. By assuming a hypothetical update \(h_{}\), in the victim model, the proposed gradient robust loss greatly enhances the effectiveness of the generated AEs towards the gradient variation, thus benefiting more for medical and shadow segmentation tasks. Moreover, the MUI aims to find the intrinsic vulnerability inherent in the basic foundation model through a broad general dataset, which is then provided as the prior knowledge for generating a more effective adversary. Therefore, in scenarios where the victim model inherits substantial information from the surrogate model, this prior knowledge becomes increasingly reliable and effective.

## 7 Conclusion

The security of utilizing large foundation models is a critical issue for deploying them in real-world applications. This paper, for the first time, considers a more challenging and practical attack scenario where the attacker executes a potent adversarial attack on SAM-based downstream models without prior knowledge of the task and data distribution. To achieve that, we propose a universal meta-initialization (UMI) algorithm to uncover the intrinsic vulnerabilities inherent in the foundation model. Moreover, by theoretically formulating the adversarial update deviation during the attacking process between the open-sourced SAM and its fine-tuned downstream models, we propose a gradient robust loss that simulates the corresponding uncertainty with gradient-based noise augmentation and analytically demonstrates that the proposed method effectively enhances the transferability. Extensive experiments validate the effectiveness of the proposed UMI-GRAT toward SAM and its downstream tasks, highlighting the vulnerabilities and potential security risks of the direct utilization and fine-tuning of open-sourced large foundation models.