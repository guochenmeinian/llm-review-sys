# ReactZyme: A Benchmark for

Enzyme-Reaction Prediction

 Chenqing Hua\({}^{1,3}\)1 Bozitao Zhong\({}^{2}\)1 Sitao Luan\({}^{1,3}\)

 Liang Hong \({}^{2}\) Guy Wolf \({}^{3,4}\) Doina Precup \({}^{1,3,5}\) Shuangjia Zheng\({}^{2}\)2

\({}^{1}\)McGill; \({}^{2}\)SJTU; \({}^{3}\)Mila; \({}^{4}\)UdeM; \({}^{5}\)DeepMind

###### Abstract

Enzymes, with their specific catalyzed reactions, are necessary for all aspects of life, enabling diverse biological processes and adaptations. Predicting enzyme functions is essential for understanding biological pathways, guiding drug development, enhancing bioproduct yields, and facilitating evolutionary studies. Addressing the inherent complexities, we introduce a new approach to annotating enzymes based on their catalyzed reactions. This method provides detailed insights into specific reactions and is adaptable to newly discovered reactions, diverging from traditional classifications by protein family or expert-derived reaction classes. We employ machine learning algorithms to analyze enzyme reaction datasets, delivering a much more refined view on the functionality of enzymes. Our evaluation leverages the largest enzyme-reaction dataset to date, derived from the SwissProt and Rhea databases with entries up to January 8, 2024. We frame the enzyme-reaction prediction as a retrieval problem, aiming to rank enzymes by their catalytic ability for specific reactions. With our model, we can _recruit proteins for novel reactions_ and _predict reactions in novel proteins_, facilitating enzyme discovery and function annotation (https://github.com/WillHua127/ReactZyme).

## 1 Introduction

Enzymes, as catalysts of biological systems, are the workhorses of various biological functions  (Fig. 1a). They accelerate and regulate nearly all chemical processes and metabolic pathways in organisms, from simple bacteria to complex mammals . The ability to understand and manipulate enzyme functions is fundamental to numerous scientific and industrial fields, including biosynthesis, where enzymes help to produce complex organic molecules , and synthetic biology, where they are engineered to create novel biological pathways . Furthermore, they can break down pollutants, thus playing a significant role in bio-remediation efforts . In the realm of protein evolution, examining enzyme functions across the tree of life enhances our understanding of the evolutionary processes that sculpt metabolic networks and enable organisms to adapt to their environments . As such, gaining insights into enzyme function is not merely an academic pursuit in life sciences but a necessity for practical applications in medicine, agriculture, and environmental management.

The current methodologies for enzyme annotation primarily rely on established databases and classifications such as KEGG Orthology (KO), Enzyme Commission (EC) numbers, and Gene Ontology (GO) annotations, each with its specific focus and methodology  (Fig. 1b). For instance, the EC system categorizes enzymes based on the chemical reactions they catalyze, providing a hierarchical numerical classification . KO links gene products to their functional orthologs across different species , whereas GO offers a broader ontology for describing the roles of genes and proteins in any organism .

Despite their widespread use, these systems have notable limitations. The EC classification, while widely used, sometimes groups vastly different enzymes under the same category or subdivides similar ones excessively, based on the substrates they interact with--leading to ambiguities in enzyme function characterization. GO annotations, although comprehensive, frequently lack specificity in defining enzyme functions and suffer from an underdeveloped database structure. Similarly, KO tends to categorize based on gene or protein families rather than specific functions, potentially assigning different identifiers to proteins with identical functions [15; 50].

Given these challenges, we propose a novel benchmark and a new enzyme-reaction dataset to learn enzymes more accurately by focusing on their catalyzed reactions directly rather than solely on gene family or human-assigned function types. The ReactZyme codes and dataset can be found on https://github.com/WillHua127/ReactZyme & https://zenodo.org/records/13635807. Our approach also leverages machine learning techniques--graph representation learning and protein language models--to analyze enzyme reaction data, providing a more nuanced understanding of enzyme functionality. This method aims to overcome the limitations of current annotation systems by offering a clearer, more consistent categorization of enzymes based on their biochemical roles, which could significantly enhance both academic research and industrial applications in enzyme technology. To this end, we summarize our ReactZyme enzyme-reaction dataset in Section 3 and the approach in Section 4 with a method visualization in Fig. 2, and introduce and the retrieval challenge and experiments in Section 5.

## 2 Related Work

**Protein Function Annotation**. Protein function annotation is a foundational task in bioinformatics, typically utilizing databases like Gene Ontology (GO), Enzyme Commission (EC) numbers, and KEGG Orthology (KO) annotations [12; 4; 48]. Traditional methods such as BLAST, PSI-BLAST, and eggNOG rely on sequence alignments and similarities to infer function [3; 2; 29]. Recently, deep learning has introduced innovative approaches for protein function prediction [56; 39; 8]. There are 2 types of protein function prediction model, one uses only protein sequence as their input, while the other also uses experimentally-determined or predicted protein structure as input. Generally, these methods typically predict EC or GO information to approximate protein functions, distinct from describing the exact catalysed reaction.

**Protein-Ligand Interaction Prediction**. Protein-ligand interaction prediction is another related area, with numerous models designed to identify potential bindings between proteins and ligands [10; 25; 73]. Most existing models, such as those for drug-target interaction (DTI), focus on stable bindings critical for therapeutic efficacy [72; 14], which differs from substrate-enzyme interactions where binding does not necessarily result in catalysis. Some models have also tackled the specific challenge of enzyme-substrate prediction, including the ESP model [37; 38]. This area differs from drug-target interactions, underscoring the unique dynamics of enzyme-substrate relationships where the interaction may not always lead to stable binding.

**Protein-Ligand Structure Prediction**. The protein-ligand structure prediction task, also referred to as ligand docking, has evolved with new methodologies emerging [14; 80; 1; 26]. Traditional docking methods like Vina , Gold , and Glide  have been complemented by deep learning approaches such as EquiBind , TankBind , E3Bind , UniMol , and DiffDock . Moreover, recent advances in protein-ligand structure prediction, such as AlphaFold 3 , RFAA

Figure 1: Overview of the enzyme-reaction prediction task. (a) Illustration of the enzymatic reaction process: substrate binds to the enzyme; formation of the enzyme-substrate complex; release of the product, leaving the enzyme for another catalytic cycle. (b) Current methods for enzyme reaction prediction: Search for annotated enzymes (e.g. sequence-based BLAST , structure-based FoldSeek ); prediction of EC/GO annotation (e.g. CLEAN ); enzyme-reaction prediction (ReactZyme).

, and Umol , provide detailed structural models of protein-ligand complexes, but they do not specifically address the functional interactions between enzymes and substrates. These methods are crucial for structure-based models but offer limited insight into the functional dynamics essential for understanding enzyme activity.

**Graph Representation Learning for Bioinformatics**. Graph representation learning emerges as a potent strategy for representing and learning about proteins and molecules, focusing on structured, non-Euclidean data [58; 47; 45; 46; 28; 44]. In this context, proteins and molecules can be effectively modeled as 2D graphs or 3D point clouds, where nodes correspond to individual atoms or residues, and edges represent interactions between them [21; 82; 27; 78]. Indeed, representing proteins and molecules as graphs or point clouds offers a valuable approach for gaining insights into and learning the fundamental geometric and chemical mechanisms governing protein-ligand interactions. This representation allows for a more comprehensive exploration of the intricate relationships and structural features within protein-ligand structures [64; 30; 79].

## 3 ReactZyme Dataset

### Dataset

**Overview**. Our study utilizes a comprehensive dataset compiled from the SwissProt and Rhea databases [7; 5]. SwissProt, a curated subset of the UniProt database, has been selected for its high-quality, human-derived functional annotations of protein sequences. This section of UniProt is particularly valuable for its expert-reviewed entries, which ensure reliable and accurate functional data, making it ideal for our analysis. Rhea is employed for its precise mapping from enzymes to specific catalyzed functions, offering detailed descriptions of biochemical reactions. The ReactZyme dataset can be downloaded via https://zenodo.org/records/11494913.

**Data Collection**. The SwissProt and Rhea dataset are downloaded on January 8, 2024, and includes data entries up to this date, providing the most recent and comprehensive data available for our study. We selectively exclude water molecules and unspecific functional groups that could mask the true molecular structures. Conversely, we keep metal ions, gas molecules, and other small molecules because of their potential to bind to proteins, a characteristic that presents a valuable learning feature for our model. To this end, the total dataset comprises \(178,463\) positive enzyme-reaction pairs, including \(178,327\) unique enzymes and \(7,726\) unique reactions.

**Compare to Other Datasets**. There are two datasets related to the enzyme-reaction prediction task. The first one is from ESP , which used GO annotation database for UniProt dataset, lay emphasis on the substrate binding to the enzyme. The ESP dataset contains \(18,351\) enzyme-substrate pairs with experimental evidence for substrate binding, contains \(12,156\) unique enzymes and \(1,379\) unique molecules. The other dataset is from EnzymeMap , which used as training set in CLIPZyme . EnzymeMap is a high-quality dataset of atom mapped and balanced enzymatic reaction, with enzyme information from BRENDA . This dataset contains \(46,356\) enzyme-driven reactions, including \(16,776\) distinct reactions and \(12,749\) enzymes. A comparison is illustrated in Table 1.

**ReactZyme Limitation**. While ReactZyme has the advantage of containing significantly more data than both ESP and EnzymeMap, it has some limitations. Notably, it lacks atom-mapping data, and the number of reactions is smaller than in EnzymeMap. This reduction in reaction count is because some reactions in ReactZyme are represented using functional groups rather than the full substrate. Futhermore, ReactZyme may not include sufficient coverage of the entirety of space of proteins and reactions in practical use. ReactZyme can be developed further for more practical interest in enzyme and substrate design.

### Data Split

We provide three dataset splits based on time, enzyme similarity, and reaction similarity. For each data split, \(10\%\) of the training data are randomly sampled for validation.

**Time Split**. The first data-split method is based on a specific date. We split the training and test samples by selecting enzyme-reaction pairs before \(2010\)-\(12\)-\(31\), for training and pairs after this date

  Dataset & \#Pair & \#Enzyme & \#Molecule/Reaction & SubstrateInfo & ProductInfo & ReactionInfo & Atom-Mapping \\  ESP & \(18,351\) & \(12,156\) & \(1,379\) & ✓ & ✗ & ✗ & ✗ \\  EnzymeMap & \(46,356\) & \(12,749\) & \(16,776\) & ✓ & ✓ & ✓ & ✓ \\  ReactZyme & \(178,463\) & \(178,327\) & \(7,726\) & ✓ & ✓ & ✓ & ✗ \\  

Table 1: Comparison of ESP, EnzymeMap, and ReactZymefor testing. This results in \(166,175\) training pairs and \(12,287\) test pairs, approximately a \(93\%/7\%\) training/test ratio. The training samples include \(166,084\) unique enzymes and \(7,726\) unique reactions, while the test samples include \(12,277\) unique enzymes and \(2,634\) unique reactions.

**Enzyme Similarity**. The second data-split method is based on enzyme similarity. We ensure that enzymes in the training set do not appear in the test set, using the Levenshtein distance  for sequence-based protein sequence comparison, ensuring at least \(60\%\) sequence difference between training and test set enzymes. This results in \(169,724\) training pairs and \(8,739\) test pairs, approximately a \(95\%/5\%\) training/test ratio. The training samples include \(169,596\) unique enzymes and \(7,726\) unique reactions, while the test samples include \(8,734\) unique unseen enzymes and \(1,573\) unique reactions.

**Reaction Similarity**. The third data-split method is based on reaction similarity, calculated by the Needleman-Wunsch algorithm on SMILES. We ensure that reactions in the training set do not appear in the test set. This results in \(163,771\) training pairs and \(14,692\) test pairs, approximately a \(91\%/9\%\) training/test ratio. The training samples include \(163,651\) unique enzymes and \(7,340\) unique reactions, while the test samples include \(14,688\) unique enzymes and \(386\) unique unseen reactions.

**Negative Sample**. A common method involves designating all enzymes within a training set that are not annotated for catalyzing a specific reaction as negative samples . Nevertheless, given the extensive size of our dataset, we opt for a strategy centered on enzyme and reaction similarity to construct negative samples. Specifically, for each verified positive enzyme-reaction pair, we identify the top-k enzymes that closely resemble the positive enzyme but do not have annotations for catalyzing the reaction, using them as negative samples. Similarly, we select the top-k reactions that are similar to the positive reaction but are not catalyzed by the positive enzyme, to serve as additional negative samples (k=1000). This method effectively narrows down the size of negative samples while retaining those of significance for both training and testing purposes. Despite our approach, the construction of negative samples still presents an unresolved challenge, remaining as an open question for future development.

## 4 ReactZyme Approach

We conceptualize the prediction of enzyme-substrate/product as a retrieval task, where it seeks to rank a given list of enzyme proteins according to their catalytic efficacy for a specified chemical reaction . The overarching goal is to understand the intricate interactions between enzymes and chemical reactions. To this end, we formulate strategies for the representation of the reactions and proteins to enhance the generalization capabilities of machine learning models in the retrieval task. More specifically, we highlight the development of representation methods that capture structural and functional subtleties of enzymes and reactions, which play a central role in predicting enzyme-substrate compatibility and catalytic potential. Our approach is visualized in Fig. 2.

Figure 2: Our methodology begins with the computation of conformations for structural insights from given reactions. Similarly, for enzymes, we employ AlphaFold to obtain their structures. Then, molecule encoders are used to transcribe 2D molecular graphs alongside their 3D geometry. For the initialization of enzyme features, protein language models are employed. The substrates and products are refined through cross-attention and the merged to form a single reaction representation. Enzyme features are further refined using an equivariant-GNN. These enzyme embeddings, along with reaction embeddings, are processed through an encoder-decoder to establish pair-wise relationships. And, a probability matrix between enzymes and reactions is computed to facilitate retrieval.

### Multi-View Reaction Representation

In representing the substrate and product of catalytic reactions, we employ both string and graph representations to capture the transition from substrates to products. Diverging from the previous enzyme datasets, such as CLEAN  and CLIPZyme , our dataset uniquely offers a combination of graph and geometric data representations. This allows the structural and functional information that is inherent in reactions to be captured in a more fine-grained manner, hence portraying a rich and informative description of the catalytic processes.

**SMILES**. Following CLEAN  and CLIPZyme , we continue to use SMILES  for representing substrates and products. This method is highly useful for its simplicity and ease of interpretation. Such representation concisely shows the substrate-to-product conversion process and uses some linear notation, which is particularly adept at conveying structural changes in a straightforward manner.

**Graph and Conformation**. Graph representation for substrates and products can capture the structural and functional information that is not typically included in string representations [33; 40; 74]. In these graphs, atoms are represented as nodes, while bonds are viewed as edges. Formally, consider a molecular graph denoted as \(=(,)\), \(^{N d_{v}}\) represents atom (node) features with each \(_{i}\) denotes one-hot encoded atom type, and \(^{N N d_{e}}\) represents edge (bond) features with each \(_{ij}\) denotes one-hot encoded bond type and connectivity. In addition to the graph representations for reactions, we use molecular conformations to incorporate geometric information. Formally, consider a molecular conformation denoted as \(=(,,)\), \(^{N 3}\) denotes additional geometric features, specifically atom positions. These conformations are computed through molecular force field optimization .

Once obtaining the graph representations \(_{s}=(_{s},_{s},_{s}), _{p}=(_{p},_{p},_{p})\) for substrates and products, respectively, we proceed to compute reaction embeddings. Consider a graph neural network denoted as \(\), we first use it to separately encode the graph representations as

\[}_{s},}_{s} =(_{s},_{s},_{s}),\;}_{s}^{N_{s} d_{v}^{T}},}_{s} ^{N_{s} N_{s} d_{v}^{T}},\] (1) \[}_{p},}_{p} =(_{p},_{p},_{p}),\;}_{p}^{N_{p} d_{v}^{T}},}_{p} ^{N_{p} N_{p} d_{v}^{T}},\] (2)

where \(},}\) denotes the updated node and edge representations, respectively. It then becomes challenging to formulate 'transitions' between substrates and products. One method to address this challenge is by constructing a pseudo-transition state graph denoted \(_{t}=(_{t},_{t})\), by adding the bond features for edges connecting the same pairs of nodes in the reactants and the products. Then the graph neural network \(\) can be used to update the transition graphs, and final reaction embedding can be computed by taking the aggregated node features, as \(=(}_{t})^{d_{r}}\). The concept of creating a pseudo-transition state graph is adopted in CLIPZyme .

However, we take a more direct approach by computing cross-attention between substrates and products to formulate the 'transitions', as follows:

\[}_{s}=(}_{s},W_{ }^{2}(}_{p},W_{}^{T})^{T}}{} })(}_{p}W_{})^{N_{s} d _{v}},\;}_{p}=(}_{p }W_{}^{2}(}_{p},W_{}^{T})^{T}}{}})(}_{s}W_{}^{p})^{N_{p}  d_{r}}.\] (3)

In here, the 'transitions' are learned through an attention mechanism that considers the pairwise relationships between atoms in substrates and atoms in products, and the edge features \(}_{s},}_{p}\) can be additionally used as attention biases in transformers . And the final reaction embedding is computed by taking the average of node features, as \(=([}_{s},}_{p}]) ^{d_{r}}\). In practice, for the choice of graph neural networks to process the structural information of substrate and product graphs \(=(,)\), we choose to use Molecule Attention Transformer-2D (MAT-2D)  and UniMol-2D ; and with additional geometric features \(=(,,)\), we choose to use MAT-3D and UniMol-3D.

### Enzyme Representation

When representing enzymes involved in catalytic reactions, we draw upon advancements in both protein structures and protein language models. This approach shares similarities with CLIPZyme , where we utilize a equivariant graph neural network to leverage information of protein structures. However, we are different in the additional use of a structure-based protein language model, where the protein embeddings are computed based on structure-aware sequence tokens.

**Protein Language Model Initialization**. Each protein is represented as a residue-level point cloud in Euclidean space, denoted as \(_{e}=(_{e},_{e},_{e})\), where \(_{e}\) represents the protein sequence and \(_{e}^{N_{e} d_{e}}\) represents residue features. Each residue \(_{i}_{e}\) can be initialized either with a one-hot encoded residue type or using embeddings from a protein language model (PLM). The protein structure is denoted as \(_{e}^{N_{e} 3}\), which can be initialized using AlphaFold  or by searching against the AlphaFold database . In practice, we use two protein language models, one using vanilla residue sequences and another using structure-aware residue sequences. The first PLM is the ESM model , which results in node features for each protein as \(_{e}^{}^{N_{e} 1280}\). To enhance our understanding of protein behavior, we employ a second structure-based protein language model called SaProt, which differs from ESM by taking structure-aware sequence tokens rather than vanilla sequence tokens. It is achieved this by first aligning the protein structures using FoldSeek. The updated protein sequence after FoldSeek alignment is denoted as \(}_{e}\), representing the structure-aware protein sequence. And SaProt computes structure-aware residue features, resulting in node features for each protein as \(_{e}^{}^{N_{e} 1280}\).

The final protein embedding is computed by taking the average of node features as, \(_{}=(_{e}^{}) ^{1280}\) and \(_{}=(_{e}^{}) ^{1280}\).

**GNN Encoding**. In addition to these embeddings, we utilize an equivariant graph neural network to encode the protein graphs \(_{e}^{}=(_{e}^{},_{e },_{e})\) and \(_{e}^{}=(_{e}^{},_{e },_{e})\). We employ the Frame Averaging Neural Network (FANN), denoted as \(\), to learn SE(3)-invariant node features . This approach possesses the effectiveness and efficiency advantage when dealing with large graphs. The frame averaging operation is achieved by first projecting the protein structure \(}_{e}\) onto a set of eight frames \(_{e}(_{e})\). These frames are constructed using Principal Component Analysis (PCA). Suppose \(_{1},_{2},_{3}\) denote the three principal components of a covariance matrix \(_{e}=(_{e}-_{e})^{T}(}_{e}-_{e})\), where \(_{e}\) denotes the Center-of-Mass of \(_{e}\). The frame set \((_{e})\) is defined as \((_{e})=\{_{1},_{2},_{3}\}\). Then the frame averaging operation computes SE(3)-invariant node features \(}_{e}\), as follows:

\[}_{e}=(_{e})|}_{_{e}(_{e})}(_{e},(_{e}-_{e}) _{e})^{N_{e} 1280}.\] (4)

And the final GNN-encoded protein embedding is computed by taking the average of node features as, \(_{}^{}=(}_{e}^{ })^{1280}\) and \(_{}^{}=(}_{e}^{ })^{1280}\).

### Enzyme-Reaction Prediction

Once we have the reaction and enzyme embeddings \(,\), designing models to learn the interactions between enzymes and reactions becomes quite flexible. While approaches like Transformer and attention mechanisms can be used to learn pairwise relationships from positive and negative enzyme-reaction pairs [69; 49], or Bidirectional Recurrent Neural Network (Bi-RNN) can capture enzyme-reaction interactions sequentially [76; 22], we take a more direct approach by employing an MLP network. Consider the input reaction embedding of dimension \(d_{r}\), the reaction encoder is a 4-layer Multi-Layer Perceptron (MLP) as:

\[_{r}=()=W_{4}(_{3}( {LN}_{3}(W_{3}(_{2}(_{2}(W_{2}(_{1}( _{1}(W_{1}+B_{1}))))+B_{2})))+B_{3})))+B_{4}^{256},\] (5)

where \(W_{1}^{d_{r} 512},B_{1}^{512},W_{2}^{512  256},B_{2}^{256},W_{3},W_{4}^{256 256},B_{3},B_{4} ^{256}\). The enzyme encoder, denoted as EnzymeEnc, has a similar architecture, with only modification in the first-layer MLP as \(W_{1}^{1280 512},B_{1}^{512}\). And the encoded reaction and enzyme representations have the dimension of \(256\), as \(_{r},_{e}^{256}\).

The decoder network is a 4-layer MLP that takes the encoded enzyme-reaction pair and computes the prediction score:

\[=(_{r},_{e})=W_{4}(W_{3}((W_{2}((W_{1}([_{r},_{e}])+B_{1}))+B_{2}))+B_{3})) ,\] (6)

where \(W_{1}^{512 256},B_{1}^{256},W_{2}^{256  128},B_{2}^{128},W_{3}^{128 64},B_{3} ^{64},W_{4}^{64 1}\). In Appendix C, we further compare the simple MLP-decoder network with Transformer- and Bi-RNN-decoder networks (in Tables 9, 10, and 11), showing their retrieval performance.

Benchmarking on ReactZyme Dataset

### Primary Empirical Evaluation

**Baseline Overview**. We summarize the baseline models used for the enzyme-reaction retrieval task. For reaction representation, we employ Molecule Attention Transformer-2D (MAT-2D) , and UniMol-2D  for 2D molecular graphs, as well as MAT-3D and UniMol-3D for 3D molecular conformations. For enzyme representation, we employ ESM  and a structure-aware protein language model, SaProt. Additionally, we use an equivariant graph neural network (FANN) to enhance residue-level representations.

**Metrics**. In the evaluation of the enzyme-reaction retrieval task, we use several metrics: Top-k Accuracy, Top-k Accuracy-N, Mean Rank, and Mean Reciprocal Rank (MRR). (1) Top-k Accuracy quantifies the proportion of instances where the correct enzyme (or reaction) is ranked within the model's top-k predictions, irrespective of its exact position. (2) Top-k Accuracy-N refines this by assessing the frequency at which the correct enzyme (or reaction) is not only within the top-k predictions but also occupies the precise rank specified by N within this subset. For instance, with k=1, the correct enzyme must be the model's foremost prediction. (3) Mean Rank calculates the average position of the correct enzyme in the retrieval list, with lower values indicating better performance. (4) MRR evaluates how quickly the correct enzyme is retrieved by averaging the reciprocal ranks of the first correct enzyme across all reactions, ranging from \(0\) to \(1\), with higher values indicating better performance. More details and implementations can be found in Appendix A.

**Results**. We present the average results of baseline models for time-based, enzyme similarity-based, and reaction similarity-based splits in Tables 2, 3, and 4, respectively. The top-performing results are highlighted in green, orange, and purple for each split type. In Table 2(a), ranking reactions for each enzyme, the vanilla ESM with 2D molecular graphs (MAT-2D + ESM) achieves \(32.46\%\) top-1 accuracy, \(40.47\) mean rank and \(0.455\) MRR. These results improve with molecular conformations and enzyme structure augmentation (UniMol-3D + ESM + GNN Encoding). For enzyme ranking per reaction (Table 2(b)), MAT-2D + ESM, MAT-2D + ESM) achieves \(21.75\%\) top-1 accuracy, \(165.31\) mean rank, and \(0.179\) MRR, with slight improvements using molecular conformations (MAT-3D + ESM). Similar improvements are seen in the enzyme similarity-based split. In Table 3(a), MAT-2D + SaProt achieves \(66.91\%\) top-1 accuracy, \(5.44\) mean rank and \(0.773\) MRR, which further improves with molecular conformations (UniMol-3D + ESM). In Table 3(b), MAT-2D + SaProt achieves \(39.99\%\) top-1 accuracy, \(23.59\) mean rank, and \(0.288\) MRR. With molecular conformations (UniMol-3D + ESM), accuracy and MRR improve slightly, though the mean rank drops. Reaction similarity-based splits pose significant challenges, especially for unseen reactions. In Table 4(a), MAT-2D + ESM achieves \(9.41\%\) top-1 accuracy, \(39.91\) mean rank and \(0.200\) MRR. Adding molecular conformations and enzyme structure augmentation (UniMol-3D + ESM + GNN Encoding) yields minimal improvement. Conversely, in Table 4(b), MAT-2D + ESM alone is sufficient.

Table 2: Average results of baseline models of _time-based split_. Top results are highlighted in green, orange, and purple, respectively.

**Summary**. It is evident that the tasks associated with the time-based and enzyme similarity-based splits are less challenging than the reaction similarity-based split. This is reflected by higher top-k accuracy, improved mean rank, and a greater Mean Reciprocal Rank (MRR), indicating increased confidence. The likely reason is that the training set for the time-based and enzyme similarity-based splits includes all reactions, whereas the test set for the reaction similarity-based split contains numerous unseen reactions. This makes the task significantly more demanding, yet it provides an excellent opportunity to evaluate the generalization capabilities of prediction models. Deep learning models employing 2D and 3D graph representations for reactions and enzymes prove effective in learning enzyme-reaction interactions, which are crucial for accurate enzyme-reaction prediction. Vanilla models such as ESM, when reactions augmented with MAT-2D and UniMol-2D, have shown promising results. These outcomes can be further enhanced by incorporating molecular conformation data (MAT-3D and UniMol-3D). Additionally, the use of an equivariant model (GNN Encoding) to represent enzyme structures has led to further improvements in prediction accuracy. This suggests that structural information plays a significant role in enzyme-reaction prediction tasks, a finding that was not observed in previous EC classification tasks. These methods prioritize enzyme functionality over mere gene family classification or human-assigned reaction categories.

Table 4: Average results of baseline models of _reaction-similarity-based split_. Top results are highlighted in green, orange, and purple, respectively.

Table 3: Average results of baseline models of _enzyme-similarity-based split_. Top results are highlighted in green, orange, and purple, respectively.

### Classic Annotation Method - BLAST

**Method**. To predict the reaction of an enzyme using BLAST, we employ BLASTp with default parameters. The training set sequences are used as the target database, while the test set sequences serve as the query. We use the following commands:

 Bash Command \(\) bash makeblastdb -in train.fasta -dbtype prot parse_seqids -out train_db blastp -query test.fasta -db train_db -outfmt "6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitstcore" -out results.tsv \\ 

If BLASTp finds a match between the test set and training set sequences, we set the corresponding value to \(1\), indicating that the sequences likely share the same reaction. If there is no match found, the value is set to \(0\), indicating no predicted reaction match.

For reaction-based sequence searches, where the reaction is known in the training set, we use the training set sequences as the query to search against the test set, applying the same criteria for setting the values.

**Results**. We compare the average neural network and BLAST results for time-, enzyme similarity-, and reaction similarity-based splits in Tables 5, 6, and 7, respectively. We highlight best performing models and use different colors distinguish between Top-k Accuracy, Mean Rank, and MRR.

**Analysis**. In the time-based split, we observe that the performance of Neural Networks and BLAST are quite similar in terms of Top-k Accuracy, Mean Rank, and MRR. The comparable performance of BLAST may be attributed to the presence of some enzyme and reaction sequences in the training

    &  & 
   &  &  &  &  \\ 
**Model** & **GENN Encoding** & Acc & ROC & Acc & ROC & Acc & ROC \\   & \(\) & 0.9904 & 0.8635 & 0.9897 & 0.8793 & 0.9715 & 0.5914 \\  & 0.9747 + Sartori & \(\) & 0.9734 & 0.8327 & 0.9880 & 0.8533 & 0.9719 & 0.5780 \\  & 0.9837 & 0.8937 & 0.8958 & 0.9837 & 0.8727 & 0.9683 & 0.8599 \\  & 0.9636 & 0.8268 & 0.9784 & 0.8498 & 0.9727 & 0.6019 \\  & 0.9708 & 0.8460 & 0.9846 & 0.8787 & 0.9723 & 0.5691 \\  & 0.9765 & 0.8546 & 0.9850 & 0.8617 & 0.9751 & 0.5823 \\   & \(\) & 0.9871 & 0.8630 & 0.9836 & 0.8617 & 0.9743 & 0.6041 \\  & 0.9864 & 0.8271 & 0.9707 & 0.8520 & 0.9718 & 0.5884 \\  & 0.9802 & 0.8552 & 0.9991 & 0.8800 & 0.9729 & 0.6091 \\  & 0.9751 & 0.8490 & 0.9737 & 0.8538 & 0.9732 & 0.5907 \\  & 0.9903 & 0.8747 & 0.9879 & 0.8010 & 0.9821 & 0.6285 \\  & 0.9843 & 0.8558 & 0.9828 & 0.8622 & 0.9786 & 0.5970 \\  

Table 8: Average accuracy and AUROC of baseline models for enzyme-reaction prediction. Top results are highlighted in green, orange, and purple, respectively.