# Active preference learning for ordering items

in- and out-of-sample

 Herman Bergstrom

Chalmers University of Technology

and University of Gothenburg

hermanb@chalmers.se

&Emil Carlsson

Sleep Cycle AB

Chalmers University of Technology

and University of Gothenburg

&Devdatt Dubhashi

Chalmers University of Technology

and University of Gothenburg

&Fredrik D. Johansson

Chalmers University of Technology

and University of Gothenburg

Equal contribution. Work was mainly performed while the author was a PhD student at Chalmers University of Technology.

###### Abstract

Learning an ordering of items based on pairwise comparisons is useful when items are difficult to rate consistently on an absolute scale, for example, when annotators have to make subjective assessments. When exhaustive comparison is infeasible, actively sampling item pairs can reduce the number of annotations necessary for learning an accurate ordering. However, many algorithms ignore shared structure between items, limiting their sample efficiency and precluding generalization to new items. It is also common to disregard how noise in comparisons varies between item pairs, despite it being informative of item similarity. In this work, we study active preference learning for ordering items with contextual attributes, both in- and out-of-sample. We give an upper bound on the expected ordering error of a logistic preference model as a function of which items have been compared. Next, we propose an active learning strategy that samples items to minimize this bound by accounting for aleatoric and epistemic uncertainty in comparisons. We evaluate the resulting algorithm, and a variant aimed at reducing model misspecification, in multiple realistic ordering tasks with comparisons made by human annotators. Our results demonstrate superior sample efficiency and generalization compared to non-contextual ranking approaches and active preference learning baselines.

## 1 Introduction

The success of supervised learning is built on annotating items at great volumes with small error. For subjective assessments, however, assigning a value from an arbitrary rating scale can be difficult and prone to inconsistencies, causing many to favor _preference feedback_ from pairwise comparisons (Yannakakis and Martinez, 2015; Christiano et al., 2017; Ouyang et al., 2022; Zhu et al., 2023). Preference feedback is sufficient to learn an _ordering_ of items (Furnkranz and Hullermeier, 2003), but for \(n\) items, there are \((n^{2})\) possible pairs of items to compare. A common solution is to use crowd-sourcing (Chen et al., 2013; Yang et al., 2021; Larkin et al., 2022), but many tasks require domain _expertise_, making annotations _expensive_ to collect. This is the case in the field of medical imaging, where annotations require trained radiologists (Phelps et al., 2015; Jang et al., 2022; Liden et al., 2024; Tarnasen and Bergstrom, 2023). So, how can we learn the best ordering possible from a limited number of comparisons?Classically, this problem is solved by active learning, sampling comparisons based on preference feedback and estimated item scores (Herbrich et al., 2006; Maystre and Grossglauser, 2017; Heckel et al., 2018). However, consider a radiologist who wants to quantify the expression of a disease in a collection of X-ray images. Purely preference-based algorithms utilize only the outcomes of comparisons but ignore the contents of the X-rays, which can reveal similarities between items and inform an ordering strategy. Moreover, the set we want to order is often larger than the set of items observed during training--we may want to rank new X-rays in relation to previous ones. This cannot be solved by learning per-item scores alone. As an alternative, active learning for classification can be used to fit a map from pairs of item contexts \(x_{i},x_{j}\) (e.g., the contents of images) to the comparison \(i_{?}j\), that can be applied to old and new items alike (Houlsby et al., 2011; Qian et al., 2015). However, as we show in Section 4, learning this map to recover a _complete ordering_ is distinct from the tasks preference learning is commonly used for, and existing algorithms lack theoretical justification for this application. Moreover, formal results for related problems, such as contextual bandits or reinforcement learning (Das et al., 2024; Filippi et al., 2010; Zhu et al., 2023; Benges et al., 2022), do not translate directly to effective active sampling criteria for ordering. There is a small body of work on learning a contextual model to recover the complete ordering (Jamieson and Nowak, 2011; Ailon, 2011) but these either assume noiseless preference feedback or that the noise is unrelated to the similarity of items, which is unrealistic for subjective assessments.

Contributions.We propose using a contextual logistic preference model to support efficient in-sample ordering and generalization to new items. Our analysis yields the first bound on the expected ordering error achievable given a collected set of comparisons (Section 4). This result justifies an active sampling principle that accounts for both epistemic and aleatoric uncertainty which we implement in a greedy deterministic algorithm called GURO (Section 5). We further propose a hybrid variant of the contextual preference model, compatible with GURO as well as existing sampling strategies, that overcomes model misspecification by adding per-item parameters (Section 5.1). We evaluate GURO and baseline algorithms in four diverse ordering tasks, three of which utilize comparisons performed by human annotators (Section 6). Our sampling strategy compares favorably to active preference learning baselines, and our hybrid model benefits both GURO and other sampling criteria, achieving the low variance of contextual models and the low bias of fitting per-item parameters. This results in faster convergence in-sample, better generalization to new items, and efficient continual learning when new items are added.

## 2 Ordering items with active preference learning

Our goal is to learn an ordering of items \(\) according to an unobserved score \(y_{i}\), defined for each item \(i\). The ground-truth ordering of \(\) is determined by a comparison function \(_{ij}:=[y_{i}>y_{j}]\), where \(_{ij}=1\) indicates that \(i\) ranks higher than \(j\). We assume there are no ties.

We define the _ordering error_\(R_{}(h)\) of a learned comparison function \(h:\{0,1\}\) as the frequency of pairwise inversions under a uniform distribution of item pairs,

\[R_{}(h)=_{i j}[h(i,j)_{ij}]\;,\] (1)

where \(n=||\). This error is equivalent to the normalized Kendall's Tau distance (Kendall, 1948).

Hypotheses \(h\) are learned from _preference feedback_--noisy pairwise comparisons \(C_{ij}\{0,1\}\) for items \((i,j)\) related to their score, for example, provided by human annotators. \(C_{ij}=1\) indicates that an annotator perceived that item \(i\) has a higher score than \(j\), i.e., that they prefer \(i\) over \(j\). _Our goal is to minimize the ordering error \(R_{}(h)\) for a fixed budget \(T 1\) of adaptively chosen comparisons_.

We are interested in contextual problems, where each item \(i\) is endowed with item-specific attributes \(x_{i}^{d}\). As we will see, this permits more sample-efficient ordering and learning algorithms that can order items out-of-sample, trained on comparisons of a subset of items \(_{D}\) and generalizing to \(_{D}\). Ordering algorithms based _only_ on preference feedback cannot solve this problem since observed comparisons are uninformative of new items.

Our _active preference learning_ scenario proceeds as follows: 1) A learner is given an annotation budget \(T\), a pool of items \(_{D}\) and item attributes \(x_{i}\) for \(i_{D}\). 2) Over rounds \(t=1,...,T\), the learner requests a comparison of two items \(i_{t},j_{t}_{D}\) according to a sampling criterion and receives noisy binary preference feedback \(c_{t} p(C_{ij})\), independently of previous comparisons. 3) After \(T\) rounds, the learner returns a comparison function \(h:\{0,1\}\). We denote the history of accumulated observations until and including time \(t\) by \(D_{t}=((i_{1},j_{1},c_{1}),...,(i_{t},j_{t},c_{t}))\).

We assume that comparisons \(C_{ij}\) follow a logistic model applied to the difference between item scores, \(p(C_{ij}=1)=(y_{i}-y_{j}),\) the so-called Bradley-Terry model (Bradley and Terry, 1952), which assumes linear stochastic transitivity (Oliveira et al., 2018). Throughout, \((z)=1/(1+e^{-z})\) and \((z)\) its derivative at \(z\). Specifically, we study the case where \(y_{i}\) is a linear function of item attributes, \(y_{i}=_{*}^{}x_{i}\,\) with \(_{*}^{d}\) the ground-truth coefficients. Thus, comparisons are determined by a logistic regression model applied to the attribute difference vector \(z_{ij} x_{i}-x_{j}\),

\[p(C_{ij}=1)=(_{*}^{}z_{ij})\.\] (2)

We face two kinds of uncertainty when actively learning the model in (2): _epistemic_ and _aleatoric_. Epistemic uncertainty, or model uncertainty, is the uncertainty about the true parameter \(_{*}\), while aleatoric uncertainty is the irreducible uncertainty about labels due to noisy annotation.

## 3 Related work

Active Preference Learning:_Preference learning_(Furnkranz and Hullermeier, 2003; Chu and Ghahramani, 2005) is related to the problem of _learning to rank_(Burges et al., 2005; Busse et al., 2012). When using adaptively chosen comparisons it may be posed as an _active learning_ or _bandit_ problem (Brinker, 2004; Long et al., 2010; Silva et al., 2014; Ling et al., 2020). Non-contextual active learners, such as TrueSkill (Herbrich et al., 2006; Minka et al., 2018), Hamming-LUCB (Heckel et al., 2018), and Probe-Rank (Lou et al., 2022) produce in-sample preference orderings, but must be updated if new items are to be ranked. Contextual algorithms, such as BALD (Houlsby et al., 2011), mitigate this by exploiting item structure and Kirsch and Gal (2022) show that many recently proposed contextual active learning strategies may be unified in a framework based on Fisher information. Similarly, methods have been proposed to recover a linear preference model by adaptively sampling paired comparisons (Qian et al., 2015; Massimino and Davenport, 2021; Canal et al., 2019). Still, this setting differs from ours in that we emphasize recovering the full ordering, not perfectly estimating the parameters. While it is true that knowing the parameters is sufficient to order the list, reducing uncertainty for all parameters equally will likely be wasteful (see Section 4). Ailon (2011) offer guarantees for ordering using contextual features in the noiseless setting, while Jamieson and Nowak (2011) analyze the setting where noise is unrelated to item similarity.

Bandits:Bandit algorithms with _relative_ or _dueling_ feedback (Yue and Joachims, 2009; Bengs et al., 2021; Yan et al., 2022) also learn from pairwise comparisons, and have been proposed both in contextual (Dudik et al., 2015) and non-contextual settings (Yue et al., 2012) to minimize regret or identify top-\(k\) items. Bengs et al. (2022) proposed CoLSTM, a contextual dueling bandit for regret minimization under linear stochastic transitivity, matching (2), and Di et al. (2023) gave variance-aware regret bounds for this setting. However, algorithms that find the top-\(k\) items, such as pure exploration bandits (Fang, 2022; Jun et al., 2021), can be arbitrarily bad at learning a full ordering (see Appendix D). Related are also George and Dimitrakakis (2023) who learn Kemeny rankings in non-contextual dueling bandits, and Wu et al. (2023) who minimize Borda regret. Zhu et al. (2023) studies the problem of estimating a preference model from offline data. Our analysis uses techniques from logistic bandits (Filippi et al., 2010; Li et al., 2017; Faury et al., 2020; Kveton et al., 2020).

Rlhf:Preference learning is commonly used when training large language models through reinforcement learning with human feedback (RLHF) (Christiano et al., 2017; Bai et al., 2022; Ouyang et al., 2022; Wu et al., 2023). In this line of work, Zhu et al. (2023) provide guarantees on the sample complexity of learning a preference model from offline data. They leverage similar tools from statistical learning and bandits as we do. In contrast to their work, we provide sampling strategies for the online setting. Mehta et al. (2023) consider active learning for RLHF in a dueling bandit framework where the goal is to optimize a contextual version of the Borda regret. Concurrent work by Mukherjee et al. (2024) and Das et al. (2024) studies a similar problem, as we do here, in the RLHF setting but with the objective to identify an optimal policy in a contextual bandit with dueling feedback. In contrast to their objective, we are interested in recovering the ordering of items. Das et al. (2024) use similar bandit techniques as we do, and their selection criterion, when adapted for ordering, corresponds to our NormMin baseline (see Section 6).

Which comparisons result in a good ordering?

We give an upper bound on the ordering error \(R_{}(h)\) for a hypothesis \(h\) fit using the feedback from a given set of \(T\) comparison queries \(_{T}=((i_{1},j_{1}),...,(i_{t},j_{T}))\). In other words, the bound attempts to answer the question "if we make queries \(_{T}\), how good can we expect our resulting model to be at ordering the items in \(\)?". That is, we condition on the queries themselves and reason about the uncertainty due to the stochastic feedback \(c_{t}\). In Section 5, we use insights from the result to design an active learning algorithm.

We restrict our analysis to the logistic model in (2) and denote by \(R() R_{}(h_{})\) the risk of the hypothesis defined by \(h_{}(i,j)=[^{}z_{ij}>0]\). Recall that \(z_{ij}=x_{i}-x_{j}\) for \(i,j\), and define \(z_{t} z_{i_{t}j_{t}}\) as the difference between attributes for the pair of items selected at round \(t=1,...,T\). Let \(_{t}\) be the maximum-likelihood estimate (MLE) fit to \(t\) rounds of feedback, \(D_{t}\)

\[_{t}=*{arg\,max}_{}_{s=1}^{t}(c_{s} (^{}z_{s})+(1-c_{s})(1-(^{}z_{s})))\;.\] (3)

Let \(_{ij}>0\) lower bound the margin of comparison, \(|(z_{ij}^{}_{s})-1/2|>_{ij}\) for all \(i,j\) and define \(_{*}=_{i j}_{ij}/|i-j|\). Next, let \(_{t}():=_{s=1}^{t}(z_{s}^{})z_{s}z_{ s}^{}\) be the Hessian of the negative log-likelihood of observations at time \(t\) under (2), given the parameter \(\), also known as _observed Fisher information_. We define \(}_{t}():=_{t}()\). For a square matrix \(V\), we define \(\|x\|_{V}=Vx}\). We make the following assumptions for our analysis:

**Assumption 1**.: \(_{*}\) _satisfies \(\|_{*}\|_{2} S\) for some \(S>0\)._

**Assumption 2**.: \( i\)_, we have \(\|x_{i}\|_{2} Q\) for \(Q>0\)._

**Assumption 3**.: \(_{T}(_{T})\) _and \(_{T}(_{*})\) have full rank and minimum eigenvalues larger than \(_{0}>0\)._

Assumption 1 implies that \(_{*}\) lies in some ball with radius \(S\) and cannot have unbounded coefficients. Assumption 2 states that there exists an upper bound on the norm of the feature vectors. This assumption is trivially satisfied whenever we have a finite set of data points. Both assumptions 1 and 2 are standard in the bandit literature and only required for our analysis. Assumption 3 is naturally satisfied for sufficiently large \(T\) by any sampling strategy with support on \(d\) linearly independent vectors, or can be ensured by allowing for a burn-in phase of \(d\) samples. Assumption 3 ensures the uniqueness of \(_{t}\).

We start by stating the following concentration result for the deviation of \((z_{ij}^{}_{T})\) from the true probability \((z_{ij}^{}_{*})\). Recall that, while queries \(_{T}\) (and therefore \(\{z_{t}\}_{t=1}^{T}\)) are fixed, the stochasticity in the feedback \(c_{t}\) implies that \(_{T}\) and consequently \(}_{T}^{-1}(_{T})\) are random. The proof of Lemma 1 is found in Appendix C and builds on results for optimistic algorithms in logistic multi-armed bandits (Filippi et al., 2010; Faury et al., 2020).

**Lemma 1** (Concentration Lemma).: _Define, for all pairs of items \(i,j\), and any \(>0\),_

\[_{ij}():=T}{8dC_{1}((z_{ ij}^{}_{T})\|z_{ij}\|_{}_{T}^{-1}(_{T})})^{2}} ,\;\;\;_{ij}():=\|z_{ij}\| _{}_{T}^{-1}(_{T})}^{2}}\;.\]

_Then, if \(:=_{ij}(),:=_{ij}()\) and \(,\),_

\[P(|(z_{ij}^{}_{*})-(z_{ij}^{}_{T})|> ) 2dT(+).\]

\(C_{1}\) _depends on \(S,_{0},Q\) from Assumptions 1-3 (see Appendix C for definition and proof)._

The concentration result in Lemma 1 is _verifiable_ (given by observables) since the upper bound depends only on the maximum likelihood estimate \(_{T}\) at time \(T\), not on \(_{*}\). We present a sharper, _unverifiable_ bound in Appendix C which instead depends on \(_{*}\) but does not suffer from the explicit scaling with \(d\) in the definitions of \(\) and \(\). The bound in Lemma 1 can also be expressed in terms of \(_{T}^{-1}(_{T})\) by using the equality \(||z_{ij}||_{}_{T}^{-1}(_{T})}^{2}=\|z_{ij}\|_{ }_{T}^{-1}(_{T})}^{2}\). As long as our sampling strategy ensures that the minimum eigenvalue of \(}_{T}(_{T})\) does not tend to zero, i.e., the strategy is _strongly consistent_(Chen et al., 1999), we have \(_{ij}(_{ij})[-_{ij}^{2}T/((z_{ij}^{} _{T})^{2}\|z_{ij}\|_{}_{T}^{-1}(_{T})}^{2})]\) and \(_{ij}(_{ij})[-_{ij}T/||z_{ij}||_{_{T}^{-1}(_{ T})}^{2}]\). Since \(_{ij}^{2}<_{ij}<1/2\) by definition, we can view \(\) as the _first-order_ term and \(\) as the _second-order_ term of our bound.

Lemma 1 formally captures the intuition that it should be easier to sort when annotations contain little noise, i.e., \((z_{ij}^{}_{T})\) is small. Especially, we observe \((z_{ij}^{}_{T}) 0\) for pairs where \(_{ij}\) is sufficiently large, causing the first-order term to vanish, leaving us with the faster decaying second-order term \(\). Lemma 1 also tells us that the hardest pairs to guarantee a correct ordering for are the ones with both high _aleatoric_ uncertainty under the MLE model, e.g., where annotators disagree or labels are noisy, captured by \((z_{ij}^{}_{T})\), as well as high _epistemic_ uncertainty captured by \(||z_{ij}||_{_{T}^{-1}(_{T})}\).

A direct consequence of Lemma 1 is the following bound on the ordering error of \(h_{_{T}}\) over \(\),

\[[R(_{T})]_{i j}(_{ij})+_{ij}(_{ij})),1\}}{n(n-1)}.\]

The right-hand side in the above inequality can be bounded further by utilizing that \(_{ij}|i-j|_{*}\). Together with Markov's inequality, this yields the following bound on \(P(R(_{T}))\).

**Theorem 1** (Upper bound on the ordering error).: _Let \(_{*}:=_{i j}_{ij}(_{*})\) and \(_{*}:=_{i j}_{ij}(_{*})\), with \(,\) from Lemma 1. Then, for \(_{*},_{*}\) and any \((0,1)\), the ordering error \(R(_{T})\) satisfies_

\[P(R(_{T}))((_{*}^ {-1}-1)^{-1}+(_{*}^{-1}-1)^{-1}) { n}(_{*}+_{*})\;,\]

_where \(_{*}\) and \(_{*}\) decay exponentially with \(T\)._

Theorem 1 suggests that the probability of \(R(_{T})\) decays exponentially with a rate that depends on the quantities \(_{i,j}(z_{ij}^{}_{T})\|z_{ij}\|_{_{T}^{-1 }(_{T})}\) and \(_{i,j}\|z_{ij}\|_{_{T}^{-1}(_{T})}^{2}\). Both quantities are random variables that depend on the particular sampling strategy that yields \(_{T}\). Focusing on the leading term, \(_{i,j}(z_{ij}^{}_{T})\|z_{ij}\|_{_{T}^{-1 }(_{T})}\), Theorem 1 suggests that an active learner should gather data to minimize this quantity and obtain the smallest possible bound. The factor \(\|z_{ij}\|_{_{T}^{-1}(_{T})}^{2}\) is the weighted norm of \(z_{ij}\) w.r.t. the inverse of the observed Fisher information (cf. Kiersch and Gal (2022)). It controls the shape of the confidence ellipsoid around \(_{T}\) and the width of the confidence interval around \(_{T}^{}z_{ij}\). The leading term in Theorem 1 re-scales this quantity with aleatoric noise under the MLE estimate \(_{T}\). This suggests that higher epistemic (model) certainty is needed in directions with high aleatoric uncertainty--where item similarity increases noise in comparisons.

In Appendix C.3, we comment on i) generalizations to regularized preference models, ii) applications to generalized linear models with other link functions, iii) lower bounds on the ordering error, and iv) an algorithm-specific upper bound.

## 5 Greedy uncertainty reduction for ordering (GURO)

We present an active preference learning algorithm based on greedy minimization of the bound in Theorem 1, called GURO. We begin with fully contextual preference models of the form \((^{}z_{ij})\) and return in Section 5.1 to parameterization variants to reduce the effects of model misspecification.

The main component of the bound in Theorem 1 to be controlled by an active learner is the term

\[_{i,j}\ (z_{ij}^{}_{T})\|z_{ij}\|_{ _{T}^{-1}(_{T})}\;,\] (4)

which represents the highest uncertainty in the comparison of any items \(i,j\) under the model \(_{T}\). A smaller value of (4) yields a smaller bound and a stronger guarantee. Recall that, for any \(t=1,...,T\), \(_{t}\) is the MLE estimate of the ground-truth parameter \(_{*}\) with respect to the observed history \(D_{t}\). Both factors in (4) are determined by the sampling strategy that yielded the item pairs \((i_{t},j_{t})\) in \(D_{T}\) and, therefore, \(_{T}\) and \(_{T}\) (the results of comparisons \(c_{ij}\) are outside the control of the algorithm, but \(z_{ij}\) are known).

Direct minimization of (4), for a subset \(_{D}\), is not feasible without access to comparisons \(c_{ij}\) and their likelihood under \(_{T}\). Instead, we adopt a greedy, alternating approach: In each round, a) a single pair is sampled for comparison by maximizing (4) under the current model estimate, and b) \(_{t}\) is recomputed based on \(D_{t}\). Specifically, at \(t=1,...,T\), we sample,

\[i_{t},j_{t}=*{arg\,max}_{i,j_{D},i j}(z_{ij}^{}_{t-1})\|z_{ij}\|_{_{t-1}^{-1}(_{t-1})}\;.\] (5)

We refer to this sampling criterion as Greedy Uncertainty Reduction for Ordering (GURO), since it reduces the uncertainty of \(_{t}\) in the direction of \(z_{ij}\). To see this, consider the change of \(_{t}(_{t})\) after a single play of \(i_{t},j_{t}\). The Sherman-Morrison formula (Sherman and Morrison, 1950) yields,

\[_{t}^{-1}(_{t-1})=_{t-1}^{-1}(_{t-1})-(z_{t}^{}_{t-1})_{t-1}^{-1}(_{t-1})z_{ t}z_{t}^{}_{t-1}^{-1}(_{t-1})}{1+(z_{t}^{} _{t-1})\|z_{t}\|_{_{t-1}^{-1}(_{t-1})}^{2}}\;,\] (6)

where \(z_{t} z_{i_{t}j_{t}}\). With \(\) as the second term in (6), it holds for all \(i<j\), with \(_{t-1}=_{t-1}(_{t-1})\), that \(||z_{ij}||_{_{t}^{-1}(_{t-1})}^{2}=||z_{ij}||_{_{t- 1}^{-1}}^{2}-||z_{ij}||_{}^{2}||z_{ij}||_{_{t-1}^{-1}}^{2}\). The inequality is strict for the pair \(i_{t},j_{t}\) in (5). As \(_{t}\) converges to \(_{*}\), this pair becomes representative of the maximizer of (4) provided there is no major systematic discrepancy between \(_{D}\) and \(\).

Surprisingly, GURO can also be justified from a Bayesian analysis. Consider a Bayesian model of the parameter \(\) with \(p()\) the prior belief and \(p( D_{t})\) the posterior after observing the preference feedback in \(D_{t}\). A natural active learning strategy is to sample items \(i_{t},j_{t}\) for which the model preference is highly uncertain under the posterior distribution,

\[i_{t},j_{t}=*{arg\,max}_{i,j_{D},i<j}}_{|D_{t-1}}[(^{}z_{ij})]\;,\] (7)

where \(}_{|D_{t-1}}[(^{}z_{ij})]\) is a finite-sample estimate of the variance in predictions, computed by sampling from the posterior. In Appendix B.3, we show that the first-order Taylor expansion of the true variance is equal to the GURO criterion. Hence, we refer to sampling according to (7) as BayesGURO. Unlike GURO, BayesGURO can incorporate prior knowledge through \(p()\) and benefits from controlled stochasticity through the empirical estimate \(}\), which makes it appropriate for batched algorithms--a deterministic criterion would construct batches of a single item pair. Both GURO and BayesGURO are presented in Algorithm 1.

Computational Complexity:Running the algorithms requires \(O(n^{2})\) operations each iteration to evaluate the sampling criteria (Equation 5 or 7) on all possible pairs, a problem shared by many active preference learning algorithms (Qian et al., 2015; Canal et al., 2019; Houlsby et al., 2011). A way of mitigating this computational complexity is to, at each time step, sample a fixed number of comparisons and only evaluate on these, similar to the approach taken in Canal et al. (2019). When only looking at a sample of \(m n^{2}\) pairs, the complexity is reduced to \(O(m)\). While making \(m\) too small can hurt the sample complexity, we describe in Appendix E how we implemented this sub-sampling strategy to speed up computations in one of our experiments and observed no noticeable change in performance. Lastly, we want to highlight that in many realistic scenarios, the computational burden males in comparison to the time it takes to query an annotator.

### Preference models for in- and out-of-sample ordering

Our default preference model \(h(i,j)=[f(i,j)>0]\) is based on a _fully contextual_ scoring function

\[f_{}(x_{i},x_{j})=^{}(x_{i}-x_{j})\;,\] (8)fit with a logistic likelihood \((f(i,j)) p(C_{ij}=1)\). The model's strength is that the variance in its estimates grows with \(d\), but not with \(n=||\), often resulting in quicker convergence than non-contextual methods for moderate dimension \(d\) (see, e.g., Figure 2c). The fully contextual model also generalizes to unseen items as long as the attributes for \(_{D}\) span attributes observed for \(\).

The limitations of a fully contextual model are model misspecification (error due to the functional form), and noise (error due to \(C\) not being fully determined by \(X\)). The former can be mitigated by applying the linear model to a representation function \(:^{d^{}}\), \(f_{}(x_{i},x_{j})=^{}((x_{i})-(x_{j}))\). A good representation \(\), e.g., from a foundation model, can mitigate misspecification and admit different input modalities. As demonstrated in Figure 5 in the Appendix, even a representation pre-trained for a different task can perform much better than a random initialization.2

Noise due to insufficiencies in \(X\) cannot be mitigated by a representation \((x)\); If annotators consistently compare items based on features \(U\) not included in \(X\), no function \(h(X_{i},X_{j})\) can perfectly order the items. However, for in-sample ordering of \(_{D}\), adding per-item parameters \(_{i}\) to the scoring function, one for each item \(i_{D}\), can mitigate both misspecification and noise,

\[f_{,}(x_{i},x_{j})=^{}((x_{i})-(x_{ j}))+(_{i}-_{j})\;.\] (9)

We call this a _hybrid_ model and apply it in "GURO Hybrid" and baselines in experiments. The term \(_{i}-_{j}\) can correct the residual of the fully contextual model, which is small if a) the context captures the most relevant information about the ordering, and b) the functional form \(^{}(x_{i})\) is nearly well-specified. Using \(_{i}-_{j}\) alone is sufficient in-sample, but has high variance (the dimension is \(n\) instead of \(d\)) and poor generalization (\(_{i}\) are unknown for items \(i_{D}\)). In practice, we use L2 regularization to prevent the model from learning an arbitrary \(\) by using the full expressivity of \(_{i}\) (see Appendix E for details). Empirically, our hybrid models exhibit the best of both worlds: When \(\) is poor, the model recovers and competes with non-contextual models (Figure 5); when \(\) is good, convergence matches fully contextual models (Figure 2).

## 6 Experiments

We evaluate GURO (Algorithm 1) and GURO Hybrid (see Section 5.1) in four image ordering tasks, one with logistic (synthetic) preference feedback, and three tasks based on real-world feedback from human annotators3.We provide a synthetic experiment in Appendix E.2 that includes empirical estimates of the bound in Theorem 1. The experiments include five diverse baseline algorithms, described next. BALD (Houlsby et al., 2011) is _a priori_ the strongest baseline since it is a contextual active learning algorithm, unlike the others. Its selection criterion greedily maximizes the decrease in posterior entropy, which amounts to reducing the epistemic uncertainty and includes a term to downplay the influence of aleatoric uncertainty. This is not always beneficial, as suggested by our analysis in Section 4, since learners may require several comparisons of high-uncertainty pairs to get the order right. CoLSTM (Bengs et al., 2022) is a contextual bandit algorithm, developed for regret minimization and is not expected to perform well here. It is included to illustrate the mismatch between regret minimization and our setting.

TrueSkill (Herbrich et al., 2006; Graepel, 2012) is a non-contextual skill-rating system that models the score of each item as a Gaussian distribution, disregarding item attributes, and has been adopted in various works to score items based on subjective pairwise comparisons (Larkin et al., 2022; Naik et al., 2014; Sartori et al., 2015). We use the sampling rule from Hees et al. (2016), designed for ordering. Finally, we include Uniform sampling, and to illustrate the importance of accounting for aleatoric uncertainty, we use a version of GURO called NormMin that ignores the \((z_{ij}^{}_{t})\) term and plays the pair maximizing \(\|z_{ij}\|_{_{t}^{-1}(_{t})}\), i.e., it minimizes the _second-order_ term in Lemma 1. NormMin corresponds to the selection criterion in the concurrent work Das et al. (2024), adapted to our problem of finding the correct ordering. We refer the reader to Appendix E.2 for a detailed comparison where NormMin performs significantly worse than Uniform on certain problem instances, and Appendix E for details regarding the implementation and the choice of hyperparameters for GURO, BayesGURO, and baselines.

### Ordering X-ray images under the logistic model

Our first task (X-RayAge) is to order X-ray images based on perceived age (Ieki et al., 2022) where the preference feedback follows a (well-specified) logistic model. We base this experiment on the data from the Kaggle competition "X-ray Age Prediction Challenge" (Felipe Kitamura, 2023) which contains more than \(10\ 000\) de-identified chest X-rays, along with the person's true age. Features were extracted using the 121-layer DenseNet in the TorchXrayVision package (Cohen et al., 2022) followed by PCA projection, resulting in \(35\) features. A ridge regression model, \(_{*}\), was fit to the true age (\(R^{2} 0.67\)). During active learning, feedback is drawn from \(p(C_{ij}=1)=(_{*}^{}z_{i,j})\), where \(\) (set to 0.1) controls the noise level. We only include the fully contextual models here since they are well-specified by design, meaning \(\) can be ordered using only contextual features.

In the first setting, we sub-sample 200 X-ray images uniformly at random from the full set. A ground-truth ordering of these elements is derived using the learned linear model. Figure 0(a) shows the ordering error over 2 000 iterations. GURO and BayesGURO perform similarly, both better than the baselines. BALD starts off converging about as fast as GURO, but plateaus, most likely as a result of actively avoiding comparisons with high aleatoric uncertainty--pairs where annotators disagree in their preferences. The poor performance of CoLSTM highlights the discrepancy between regret minimization and recovering a complete ordering.

In the second setting, we evaluate how well the algorithms generalize to new items. First, we sample 300 X-ray images from the full dataset. Next, we split these into two sets, with one (\(I_{D}\)) containing the youngest \(50\%\) and the other (\(I_{E}\)) the oldest \(50\%\). The algorithms were then trained to order the list containing the younger subjects, but were simultaneously evaluated on how well they could sort the list containing the older subjects. The continuously measured difference in ordering error evaluated on \(I_{E}\) and \(I_{D}\) are presented in Figure 0(b). While all algorithms are worse at ordering items in \(I_{E}\), GURO and BayesGURO achieve the lowest average difference. Together with Figure 0(a), this means that our proposed algorithms achieved the best in-sample and out-of-sample orderings. For completeness, the in-sample performance of algorithms in the generalization experiment in Figure 0(b) are included in Appendix E.2.

### Ordering items with human preference data

Next, we evaluate our algorithm on three publicly available datasets to study the algorithms' performance when preference feedback comes from human annotators (see Table 1 for an overview, detailed information of datasets in Appendix E.1). The datasets are IMDB-WIKI-SbS (Pavlichenko and Ustalov, 2021), where annotators have stated which of two people appear older, ImageClarity (Zhang et al., 2016), where modified versions of the same image have been compared according to the level of distortion, as well as the extended WiscAds dataset (Carlson and Montgomery, 2017), where labels correspond to which political advertisement is perceived as more negative toward an

Figure 1: **X-RayAge. Performance of active sampling strategies when comparisons are simulated using a logistic model according to (2). In-sample Kendallâ€™s Tau distance \(R_{I_{D}}\) on 200 images (left) and generalization error \(R_{I_{E}}-R_{I_{D}}\) for models trained on 150 images and evaluated on 150 images from a different distribution (right). All results are averaged over \(100\) different random seeds.**opponent. In all datasets, pairs of items were sampled uniformly for annotation. For each experiment, we construct a feature vector \((x_{i})^{d}\) for all \(n\) items using a pre-trained embedding model followed by PCA, applied to reduce computational complexity. We restrict algorithms to only query pairs for which an annotation exists and remove the annotation from the pool once queried. In cases where multiple annotations exist for the same pair, the feedback is chosen randomly among these.

The images in the ImageClarity dataset have been constructed to have an objective ground truth ordering but this is not the case for WiscAds or IMDB-WIKI-SbS. As the ground-truth ordering is generally unknown also in real-world applications, we evaluate methods by the error on a held-out set of comparisons \(D^{}\), \(_{D^{}}(h)=|}_{(i,j,c) D^{}} [h(i,j) c]\). This serves as an empirical analog of Kendall's Tau distance and a minimizer of \(_{D^{}}(h)\) will minimize \(R_{}(h)\) for sufficiently large \(D^{}\), but will not converge toward \(0\) since there is inherent noise in annotations. This metric makes no assumptions on the ground truth ordering unlike the alternative approach of fitting an ordering to all available comparisons, see e.g., Maystre and Grossglauser (2017). In Appendix E.2,

   Dataset & \(n\) & \(d\) & \#comparisons & Data type & Embedding Model \\ 
**ImageClarity** & \(100\) & \(63\) & \(27\,730\) & Image & ResNet34 (Imagenet) \\
**WiscAds** & \(935\) & \(162\) & \(9\,528\) & Text & all-mpnet-base-v2 \\
**IMDB-WIKI-SbS** & \(6072\) & \(75\) & \(110\,349\) & Image & FaceNet (CASIA-Webface) \\   

Table 1: Datasets with preference feedback from annotators. Pretrained models are ResNet34 (He et al., 2016), all-mpnet-base-v2 (Reimers and Gurevych, 2019), and FaceNet (Schroff et al., 2015).

Figure 2: The empirical error \(_{D^{}}(h)\) on a holdout comparison set \(D^{}\) when comparisons are made by human annotators. The plots are averaged over \(100\) (a,b) or 10 (c,d) seeds, and the shaded area represents one standard deviation above and below the mean. For every seed, \(10\%\) of comparisons were used for the holdout set. In (d) we initially order a list \(_{D}\) of \(3\ 000\) images. After \(10\ 000\) comparisons the remaining \(3\ 072\) images, \(_{D}\), are added.

we show results for the latter that highlight the limitations of estimating a "ground-truth" ordering, as well as the similar results when measuring the distance to the objective ground-truth ordering of the ImageClarity dataset. The longest trajectory (single seed) for any algorithm took less than 35hrs to complete on one core of an Intel Xeon Gold 6130 CPU and required at most 10 GB of memory.

In all experiments, we compare fully contextual (8) and hybrid (9) versions of GURO, BALD, and Uniform, as well as TrueSkill. The results of each experiment can be seen in Figure 2. Figure 1(a) shows that the ImageClarity dataset is the easiest to order using contextual (non-hybrid) features. This is expected, as features relevant to the level of distortion are low-level. In this case, the choice of adaptive strategy has a modest impact on the ordering error. Figures 1(b) and 1(c) highlight the differences between modeling strategies. The fully contextual algorithms initially improve rapidly, achieving a rough ordering of the items, before plateauing and not making any real improvements. This indicates that the features are informative enough to roughly order the list, but insufficient for retrieving a more granular ordering. The non-contextual TrueSkill converges at a much slower pace but keeps improving steadily throughout. Perhaps most interesting are the hybrid algorithms, which seemingly reap the benefits of both methods, improving as quickly as the contextual methods, but avoiding the plateau. In fact, in Figure 5 in the Appendix we show that the hybrid models perform comparably to TrueSkill even when features are completely uninformative.

The limitations of BALD are most noticeable in the fully contextual case, where it plateaus at a higher error compared to GURO and Uniform. This is however not as prominent when we use BALD in conjunction with our hybrid model, likely a result of the increased dimensionality of the model causing BALD Hybrid to attribute more of the observed errors to model uncertainty. While this initially causes the algorithm to avoid fewer comparisons that are subject to aleatoric uncertainty, the final iterations in Figure 1(c) suggest that BALD Hybrid can still run into this issue given enough samples. In all experiments, GURO and GURO Hybrid perform better than or similar to our baselines, never worse. Additionally, Figures 1(b) and 1(c) showcase how our hybrid model can increase performance when used with existing sampling strategies, such as BALD or Uniform.

The final experiment, visible in Figure 1(d), is a few-shot scenario where after some time, additional images are added to the pool of items. IMDB-WIKI-SbS was used as it contained the highest number of both images and comparisons. The initial pool consists of \(3\ 000\) images sampled from the dataset. After \(10\ 000\) steps, the remaining \(3\ 072\) images were added to the pool. The results again emphasize the differences between our three types of models; the increase in error of the fully contextual model is very slight, likely a result of added samples being drawn from the same distribution. For TrueSkill, the error increases drastically as a result of the algorithm not having seen these items before and having no way of generalizing the results of previous comparisons to them. Lastly, the hybrid algorithms seem to be moderately affected. The error increases as the model has not yet tuned any of the added per-item parameters, but the extent is much smaller than for TrueSkill as the model can provide a rough ranking of the out-of-sample elements using the contextual features.

## 7 Conclusion

We have demonstrated the benefits of utilizing contextual features in active preference learning to efficiently order a list of items. Empirically, this leads to quicker convergence, compared to non-contextual methods, and allows algorithms to generalize out-of-sample. We derived an upper bound on the ordering error and used it to design an active sampling strategy that outperforms or matches baselines on realistic image and text ordering tasks. Both theoretical and empirical results highlight the benefit of accounting for noise in comparisons when learning from human annotators.

The optimality of our sampling strategy remains an open question. A future direction is to derive a lower bound on the ordering error, and prove an--ideally matching--algorithm-specific upper bound. However, constructing upper bounds for related fixed-budget tasks is an open problem (Qin, 2022). Moreover, motivated by the annotation setting, our focus has been on reducing sample complexity and we leave it to future work to explore potential linear approximations of the sampling criteria and other trade-offs between sample complexity and computational complexity. Further, our approach can potentially be improved by performing representation learning throughout the learning process. Finally, our experiments are constrained to a limited amount of already-collected (offline) human preference data, causing different algorithms to select disproportionately similar comparisons. Future work should evaluate the strategies in an online setting.