# First Order Stochastic Optimization

with Oblivious Noise

 Ilias Diakonikolas

Department of Computer Sciences

University of Wisconsin-Madison

ilias@cs.wisc.edu

&Sushrut Karmalkar

Department of Computer Sciences

University of Wisconsin-Madison

skarmalkar@wisc.edu

&Jongho Park

KRAFTON

jongho.park@krafton.com

&Christos Tzamos

Department of Informatics

University of Athens

tzamos@wisc.edu

###### Abstract

We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup. In our setting, in addition to random observation noise, the stochastic gradient may be subject to independent _oblivious noise_, which may not have bounded moments and is not necessarily centered. Specifically, we assume access to a noisy oracle for the stochastic gradient of \(f\) at \(x\), which returns a vector \( f(,x)+\), where \(\) is the bounded variance observation noise and \(\) is the oblivious noise that is independent of \(\) and \(x\). The only assumption we make on the oblivious noise \(\) is that \([=0]\) for some \((0,1)\). In this setting, it is not information-theoretically possible to recover a single solution close to the target when the fraction of inliers \(\) is less than \(1/2\). Our main result is an efficient _list-decodable_ learner that recovers a small list of candidates, at least one of which is close to the true solution. On the other hand, if \(=1-\), where \(0<<1/2\) is sufficiently small constant, the algorithm recovers a single solution. Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, which may be of independent interest.

## 1 Introduction

A major challenge in modern machine learning systems is to perform inference in the presence of outliers. Such problems appear in various contexts, such as analysis of real datasets with natural outliers, e.g., in biology (Rosenberg et al., 2002; Paschou et al., 2010; Li et al., 2008), or neural network training, where heavy-tailed behavior arises from stochastic gradient descent when the batch size is small and when the step size is large (Hodgkin and Mahoney, 2021; Gurbuzbalaban et al., 2021). In particular, when optimizing various neural networks, there exists strong empirical evidence indicating that gradient noise frequently displays heavy-tailed behavior, often showing characteristics of unbounded variance. This phenomenon has been observed for fully connected and convolutional neural networks (Simsekli et al., 2019; Gurbuzbalaban and Hu, 2021) as well as attention models (Zhang et al., 2020). Hence, it is imperative to develop robust optimization methods for machine learning in terms of both performance and security.

In this paper, we study robust first-order stochastic optimization under heavy-tailed noise, which may not have any bounded moments. Specifically, given access to a noisy gradient oracle for \(f(x)\)(which we describe later), the goal is to find a _stationary point_ of the objective function \(f:^{d}\), \(f(x):=_{}[f(,x)]\), where \(\) is drawn with respect to an arbitrary distribution on \(^{d}\).

Previous work on \(p\)-heavy-tailed noise often considers the case where one has access to \( f(,x)\) such that \(_{}[ f(,x)]= f(x)\) and \(_{}[\| f(,x)- f(x)\|^{p}]^{p}\) for \(p=2\)(Nazin et al., 2019; Gorbunov et al., 2020), and recently for \(p(1,2]\)(Cutkosky and Mehta, 2021; Sadiev et al., 2023). In contrast, Diakonikolas et al. (2019) investigates a more challenging noise model in which a small proportion of the sampled functions \(f(,x)\) are arbitrary adversarial outliers. In their scenario, they have full access to the specific functions being sampled and can eventually eliminate all but the least harmful outlier functions. Their algorithm employs robust mean estimation techniques to estimate the gradient and filter out the outlier functions. Note that their adversarial noise model requires that at least half of the samples are inliers.

This work aims to relax the distributional assumption of heavy-tailed noise such that the fraction of outliers can approach one, and do not have any bounded moment constraints while keeping optimization still computationally tractable. Thus, we ask the following natural question:

_What is the weakest noise model for optimization in which efficient learning is possible, while allowing for strong corruption of almost all samples?_

In turn, we move beyond prior heavy-tailed noise models with bounded moments and define a new noise model for first-order optimization, inspired by the oblivious noise model studied in the context of regression (Bhatia et al., 2015; d'Orsi et al., 2021b). We consider the setting in which the gradient oracle returns noisy gradients whose mean may not be \( f(x)\), and in fact, may not even exist. The only condition on our additive noise distribution of \(\) is that with probability at least \(\), oblivious noise takes the value zero. Without this, the optimization problem would become intractable. Notably, oblivious noise can capture any tail behavior that arises from independent \(p\)-heavy-tailed noise.

We now formally define the oblivious noise oracle below.

**Definition 1.1** (Oblivious Noise Oracle).: _We say that an oracle \(_{,,f}(x)\) is an oblivious-noise oracle, if, when queried on \(x\), the oracle returns \( f(,x)+\) where \(\) is drawn from some arbitrary distribution \(Q\) and \(\) is drawn from \(D_{}\) independent of \(x\) and \(\), satisfying \(_{ D_{}}[=0]\). The distribution \(Q\) satisfies \(_{ Q}[ f(,x)]= f(x)\) and \(_{ Q}[\| f(,x)- f(x)\|^{2}]^{2}\)._

Unfortunately, providing an approximate stationary point \(\) such that \(\| f()\|\), for some small \(>0\), is information-theoretically impossible when \( 1/2\). To see this, consider the case where \(_{}[=0]=1\) and the function \(f(0,x)=x^{2}\). If \(\) follows a uniform distribution over the set \(\{-2,0,2\}\), when we query the oracle for a gradient at \(x\), we will observe a uniform distribution over \(\{2x-2,2x,2x+2\}\). This scenario cannot be distinguished from a similar situation where the objective is to minimize \(f(0,x)=(x-1)^{2}\), and \(\) follows a uniform distribution over \(\{0,2,4\}\).

To address this challenge, we allow our algorithm to work in the _list-decodable_ setting, implying that it may return a small list of candidate solutions such that one of its elements \(\) satisfies \(\| f()\|\). We now define our main problem of interest.

**Definition 1.2** (List-Decodable Stochastic Optimization).: _The problem of List-Decodable Stochastic Optimization with oblivious noise is defined as follows: For an \(f\) that is \(L\)-smooth and has a global minimum, given access to \(_{,,f}()\) as defined in Definition 1.1, the goal is to output a list \(\) of size \(s\) such that \(_{}\| f()\|\)._

### Our Contributions

As our main result, we demonstrate the equivalence between list-decodable stochastic optimization with oblivious noise and list-decodable mean estimation (LDME), which we define below.

**Definition 1.3** (List-Decodable Mean Estimation).: _Algorithm \(\) is an \((,,s)\)-LDME algorithm for \(\) (a set of candidate inlier distributions) if with probability \(1-_{}\), it returns a list \(\) of size \(s\) such that \(_{}\|-_{x D}[x]\|\) for \(D\) when given \(m_{}\) samples \(\{z_{i}+_{i}\}_{i=1}^{m_{}}\) for \(z_{i} D\) and \(_{i} D_{}\) where \(_{ D_{}}[=0]\). If \(1-\) is a sufficiently small constant less than \(1/2\), then \(s=1\)._We define \(_{}\) to be the following set of distributions over \(^{d}\): \(_{}:=\{D_{D}[\|x-_{D}[x]\|^{2}] ^{2}\}\). We also use \(()\) to hide all log factors in \(d,1/,1/,1/\), where \(\) denotes the failure probability and \(\) is a multiplicative parameter we use in our algorithm.

Our first theorem shows that if there exists an efficient algorithm for the problem of list-decodable mean estimation and an inexact learner (an optimization method using inexact gradients), there is an efficient algorithm for list-decodable stochastic optimization.

**Theorem 1.4** (List-Decodable Stochastic Optimization \(\) List-Decodable Mean Estimation).: _Suppose that for any \(f\) having a global minimum and being \(L\)-smooth, the algorithm \(_{G}\), given access to \(g_{x}\) satisfying \(\|g_{x}- f(x)\| O()\), recovers \(\) satisfying \(\| f()\| O()+\) in time \(T_{G}\). Let \(_{ME}\) be an \((,O(),s)\)-LDME algorithm for \(_{}\). Then, there exists an algorithm (Algorithm 2) which uses \(_{ME}\) and \(_{G}\), makes \(m=m_{_{ME}}+(T_{G}(O(1)/)^{2/}/^{5})\) queries to \(_{,,f}\), runs in time \(T_{G}(d,1/,(O(1)/)^{1/},(1/ ))\), and returns a list \(\) of \(s\) candidates satisfying \(_{x}\| f(x)\| O()+\) with probability \(1-_{_{ME}}\)._

To achieve this reduction, we develop an algorithm capable of determining the translation between two instances of a distribution by utilizing samples that are subject to additive noise. It is important to note that the additive noise can differ for each set of samples. As far as we know, this is the first guarantee of its kind for estimating the location of a distribution, considering the presence of noise, sample access, and the absence of density information.

**Theorem 1.5** (Noisy Location Estimation).: _Let \((0,1)\) and let \(D_{},D_{z},D_{z^{}}\) be distributions such that \(_{ D_{}}[=0]\) and \(D_{z},D_{z^{}}\) are possibly distinct mean zero distributions with variance bounded by \(^{2}\). Then there is an algorithm (Algorithm 5) which, for unknown \(t^{d}\), takes \(m=((1/^{5})(O(1)/)^{2/})\) samples \(\{_{i}+z_{i}+t\}_{i=1}^{m}\) and \(\{_{i}+z_{i}^{}\}_{i=1}^{m}\), where \(_{i}\) and \(_{i}\) are drawn independently from \(D_{}\) and \(z_{i}\) and \(z_{i}^{}\) are drawn from \(D_{z}\) and \(D_{z^{}}\) respectively, runs in time \((d,1/,(O(1)/)^{1/},(1/))\), and with probability \(1-\) recovers \(t^{}\) such that \(|t-t^{}| O()\)._

The fact that there exist algorithms for list-decodable mean estimation with the inliers coming from \(_{}\) allows us to get concrete results for list-decodable stochastic optimization.

Conversely, we show that if we have an algorithm for list-decodable stochastic optimization, then we also get an algorithm for list-decodable mean-estimation. This in turn implies, via Fact 2.1, that an exponential dependence on \(1/\) is necessary in the list-size if we want to estimate the correct gradient up to an error of \(O()\) when the inliers are drawn from some distribution in \(_{}\).

**Theorem 1.6** (List-Decodable Mean Estimation \(\) List-Decodable Stochastic Optimization).: _Assume there is an algorithm for List-Decodable Stochastic Optimization with oblivious noise that runs in time \(T\) and makes \(m\) queries to \(_{,,f}\), and returns a list \(\) of size \(s\) containing \(\) satisfying \(\|f()\|\). Then, there is an \((,,s)\)-LDME algorithm for \(\) that runs in time \(T\), queries \(m\) samples, and returns a list of size \(s\)._

If \(=1-\), where \(0<<1/2\) is at most a sufficiently small constant, the above theorems hold for the same problems, but with the constraint that the list is singleton.

### Related Work

Given the extensive robust optimization and estimation literature, we focus on the most relevant work.

Optimization with Heavy-tailed NoiseThere is a wealth of literature on both the theoretical and empirical convergence behavior of stochastic gradient descent (SGD) for both convex and non-convex problems, under various assumptions on the stochastic gradient (see, e.g., Hardt et al. (2016), Wang et al. (2021) and references within). However, the noisy gradients have shown to be problematic when training ML models (Shen and Sanghavi, 2019; Zhang et al., 2020), hence necessitating robust optimization algorithms.

From a theoretical point of view, several noise models have been proposed to account for inexact gradients. A line of work (D'Aspremont, 2008; So, 2013; Devolder et al., 2014; Cohen et al., 2018) studies the effects of inexact gradients to optimization methods in terms of error accumulation. For instance, Devolder et al. (2014) demonstrates that, given an oracle that outputs an arbitrary perturbation of the true gradient, the noise can be determined adversarially to encode non-smooth problems. Alternatively, many recent works (Lan, 2012; Gorbunov et al., 2020; Cutkosky and Mehta, 2021; Mai and Johansson, 2021; Sadiev et al., 2023) have studied \(p\)-heavy-tailed noise, an additive stochastic noise to the true gradient where one has access to \( f(,x)\) such that \(_{}[ f(,x)]= f(x)\) and \(_{}[\| f(,x)- f(x)\|^{p}]^{p}\). For instance, Sadiev et al. (2023) propose and analyze a variant of clipped-SGD to provide convergence guarantees for when \(p(1,2]\). However, these noisy oracles, whether deterministic or stochastic, assume bounded norm or moments on the noise, an assumption that is not present in the oblivious noise oracle.

Robust EstimationOur oblivious noise oracle for optimization is motivated by the recent work on regression under oblivious outliers. In the case of linear regression, the oblivious noise model can be seen as the weakest possible noise model that allows almost all points to be arbitrarily corrupted, while still allowing for recovery of the true function with vanishing error (Bhatia et al., 2015; Suggala et al., 2019). This also captures heavy-tailed noise that may not have any moments. The setting has been studied for various problems, including online regression (Pesme and Flammarion, 2020), PCA (D'Orsi et al., 2021), and sparse signal recovery (D'Orsi et al., 2022).

On the other hand, there has been a flurry of work on robust estimation in regards to worst-case adversarial outliers (see, e.g., Diakonikolas et al. (2016); Lai et al. (2016); Charikar et al. (2017); Diakonikolas et al. (2019)). Robust mean estimation aims to develop an efficient mean estimator when \((1-)\)-fraction of the samples is arbitrary. In contrast, list-decodable mean estimation generates a small list of candidates such that one of these candidates is a good estimator. While robust mean estimation becomes information-theoretically impossible when \( 1/2\), the relaxation to output a list allows the problem to become tractable for any \((0,1]\)(Charikar et al., 2017; Diakonikolas et al., 2022). See Diakonikolas and Kane (2019, 2023) for in-depth treatments of the subject.

In the past, robust mean estimators have been used to perform robust gradient estimation (see, e.g., Charikar et al. (2017); Diakonikolas et al. (2019)), which is similar to what we do in our paper. However, these results assume access to the entire function set, which allows them to discard outlier functions. In contrast, in our setting, we only have access to a noisy gradient oracle, so at each step, we get a fresh sample set and, hence, a different set of outliers. This introduces further difficulties, which we resolve via location estimation.

We note a subtle difference in the standard list-decodable mean estimation setting and the setting considered in this work. The standard list-decodable mean estimation setting draws an inlier with probability \(\) and an outlier with the remaining probability. In contrast, our model gets samples of the kind \(+z\) where \(z\) is drawn from the inlier distribution, and \([=0]>\), and is arbitrary otherwise. The algorithms for the mixture setting continue to work in our setting as well. Another difference is that the standard setting requires that the distribution have bounded variance _in every direction_. On the other hand, in the optimization literature, the assumption is that the stochastic gradient has bounded expected squared _norm_ from the expectation.

Location estimationLocation estimation has been extensively studied since the 1960s. Traditional approaches to location estimation have focused on achieving optimal estimators in the asymptotic regime. The asymptotic theory of location estimation is discussed in detail in (Van der Vaart, 2000).

Recent research has attempted to develop the finite-sample theory of location estimation. These efforts aim to estimate the location of a Gaussian-smoothed distribution with a sample complexity that matches the optimal sample complexity up a constant (see (Gupta et al., 2022) and (Gupta et al., 2023)). However, these results assume prior knowledge of the likelihood function of the distribution up to translation.

Another closely related work initiates the study of robust location estimation for the case where the underlying high-dimensional distribution is symmetric and an \(0< 1/2\) fraction of the samples are adversarially corrupted Novikov et al. (2023). This follows the line of work on robust mean estimation discussed above.

We present a finite-sample guarantee for the setting with noisy access to samples drawn from a distribution and its translation. Our assumption on the distribution is that it places an \(\) mass at some point, where \((0,1)\) and, for instance, could be as small as \(1/d^{c}\) for some constant \(c\). The noise is constrained to have mean zero and bounded variance, but crucially the noise added to the samples coming from the distribution and its translation might be drawn from _different distributions_. To the best of our knowledge, this is the first result that has noisy access, does not have prior knowledge of the probability density of the distribution and achieves a finite-sample guarantee.

### Technical Overview

For ease of exposition, we will make the simplifying assumption that the observation noise is bounded between \([-,]\). Let \(y\) and \(y^{}\) be two distinct mean-zero noise distributions, both bounded within the range \([-,]\). Define \(\) to be the oblivious noise drawn from \(D_{}\), satisfying \([=0]\). Assume we have access to the distributions (i.e., we have infinite samples).

Stochastic Optimization reduces to Mean Estimation.In Theorem1.4, we show how we can leverage a list-decodable mean estimator to address the challenge of list-decodable stochastic optimization with oblivious noise (see Algorithm2). The key idea is to recognize that we can a generate list of gradient estimates, and update this list such that one of the elements always closely approximates the true gradient in \(_{2}\) norm at the desired point.

The algorithmic idea is as follows: First, run a list-decodable mean estimation algorithm on the noisy gradients at \(x_{0}=0\) to retrieve a list \(\) consisting of \(s\) potential gradient candidates. This set of candidates contains at least one element which closely approximates \( f(0)\).

One natural approach at this stage would be to perform a gradient descent step for each of the \(s\) gradients, and run the list-decodable mean estimation algorithm again to explore all potential paths that arise from these gradients. However, this naive approach would accumulate an exponentially large number of candidate solutions. We use a location-estimation algorithm to tackle this issue.

We can express \(f(,x)\) as \(f(x)+e(,x)\) where \(_{}[e(,x)]=0\) and \(_{}[\|e(,x)\|^{2}]<^{2}\). When we query \(_{,,f}()\) at \(x\), we obtain samples of the form \( f(x)++ e(,x)\), i.e., samples from a translated copy of the oblivious noise distribution convolved with the distribution of \( e(,x)\). We treat the distribution of \( e(,x)\) as observation noise. The translation between the distributions of our queries to \(_{,,f}()\) at \(x\) and \(0\) correspond to \( f(x)- f(0)\). To update the gradient, it is sufficient to recover this translation accurately. By doing so, we can adjust \(\) by translating each element while maintaining the accuracy of the estimate.

Finally, we run a first-order learner for stochastic optimization using these gradient estimates. We select and explore \(s\) distinct paths, with each path corresponding to an element of \(\) obtained earlier. Since one of the paths always has approximately correct gradients, this path will converge to a stationary point in the time that it takes for the first-order learner to converge.

Noisy Location Estimation in 1-D.The objective of noisy location estimation is to retrieve the translation between two instances of a distribution by utilizing samples that are subject to additive mean-zero and bounded-variance noise.

In Lemma3.1, we establish that if Algorithm1 is provided with a parameter \((0,1)\) and samples from the distributions of \(+y\) and \(+y^{}+t\), it can recover the value of \(t\) within an error of \(O()\). Here \(\) and \(\) are independent draws from the same distribution.

Observe that in the absence of additive noise \(y\) and \(y^{}\), \(t\) can be estimated exactly by simply taking the median of a sufficiently large number of pairwise differences between independent samples from \(\) and \(+t\). This is because the distribution of \(-\) is symmetric at zero and \([-=0]=^{2}\). However, with unknown \(y\) and \(y^{}\), this estimator is no longer consistent since the median of zero-mean and \(\)-variance random variables can be as large as \(O()\) in magnitude. In fact, Claim3.2 demonstrates that for the setting we consider the median of \(+y\) can as far as \(O(/)\) far from the mean of \(\). However, it does recover a rough estimate \(t^{}_{r}\) such that \(|t-t^{}_{r}| O(/)\).

Consequently, our approach starts by using \(t^{}_{r}\) as the estimate for \(t\), and then iteratively improving the estimate by mitigating the heavy-tail influence of \(\). Note that if \(\) did not have heavy tails, then we could try to directly compute \(t=[+y^{}+t]-[+y]\). Unfortunately, due to \(\)'s completely unconstrained tail, \(\) may not even possess a well-defined mean. Nonetheless, if we are able to condition \(\) to a bounded interval, we can essentially perform the same calculation described above. In what follows, we describe one step of an iterative process to improve our estimate of \(t\).

Rejection Sampling.Suppose we have an initial rough estimate of \(t\), given by \(t^{}_{r}\) such that \(|t^{}_{r}-t|<A\). We can then re-center the distributions to get \(+z\) and \(+z^{}\), where \(z\) and \(z^{}\) have means of magnitude at most \(A\), and have variance that is \(O()\). Claim 3.4 then shows that we can refine our estimate of \(t\). It does so by identifying an interval \(I=[-k,k]\) around \(0\) such that the addition of either \(z\) or \(z^{}\) to \(\) does not significantly alter the distribution's mass within or outside of \(I\). We show that since \(z,z^{}\), and \(\) are independent, the \(I\) that we choose satisfies \([+z^{}+z^{} I]-[+z+z  I]=[+z^{} I]-[+z+z  I] A=[z^{}]-[z] A\) for some \(<1\).

To see that such an interval \(I\) exists, we will show that for some \(i\), there are pairs of intervals \((A i,(i+1)A]\) and \([-(i+1)A,-iA)\) which contain a negligible mass with respect to \(\). Since \(z\) and \(z^{}\) can move mass by at most \((A+1)\) with high probability, it is sufficient to condition on the interval around \(0\) which is contained in this pair of intervals. Since we do not have access to \(\), we instead search for pairs of intervals of length \(O(A)\) which have negligible mass with respect to both \(+z\) and \(+z^{}\), which suffices.

To do this, we will demonstrate an upper bound \((i)\) on the mass crossing the \(i^{th}\) pair of intervals described above. This will satisfy \(_{i}(i)=C^{}\) for some constant \(C^{}\).

To show that this implies there is an interval of negligible mass, we aim to demonstrate the existence of a \(k\) such that \(k(k)\). We will do this through a contradiction. Suppose this is not the case for all \(i[0,k]\), then \(_{i=0}^{k}(i)_{i=0}^{k}(1/i)\). If \(k(10C^{}/)\), we arrive at a contradiction because the right-hand side is at least \(_{i=0}^{k}(1/i)((10C/))>10C^{}\), while the left-hand side is bounded above by \(C\).

Claim C.4 demonstrates, via a more involved and finer analysis, that for the bounded-variance setting where intervals very far away can contribute to mass crossing \(A i\), there exists an \(k\) such that \(k(k)[||<A k]\), and that taking the conditional mean restricted to the interval \([-kA,kA]\) allows us to improve our estimate of \(t\). Here \((k)\) denotes an upper bound on the total probability mass that crosses intervals described above.

Extension to Higher Dimensions.In order to extend the algorithm to higher dimensions, we apply the one-dimensional algorithm coordinate-wise, but in a randomly chosen coordinate-basis. Lemma 3.5 uses the fact that representing the distributions in a randomly rotated basis ensures that, with high probability, the inlier distribution will project down to each coordinate with a variance of \(O(/)\) to extend Lemma 3.1 to higher dimensions.

Mean Estimation reduces to Stochastic Optimization.In Theorem 1.6 we show that the problem of list-decodable mean estimation can be solved by using list-decodable stochastic optimization for the oblivious noise setting. This establishes the opposite direction of the reduction to show the equivalence of list-decodable mean estimation and list-decodable stochastic optimization.

The reduction uses samples from the list-decodable mean estimation problem to simulate responses from the oblivious oracle to queries at \(x\). Let the mean of the inlier distribution be \(\). If the first-order stochastic learner queries \(x\), we return \(x+s\) where \(s\) is a sample drawn from the list-decodable mean-estimation problem. These correspond to possible responses to the queries when \(f(x)=(1/2)\|x+\|^{2}\), where \(\) is the true mean. The first-order stochastic learner learns a \(\) from a list \(\) such that \(\| f()\|=\|+\|\); then the final guarantee of the list-decodable mean estimator follows by returning \(-\) which contains \(-\).

## 2 Preliminaries

Basic NotationFor a random variable \(X\), we use \([X]\) for its expectation and \([X E]\) for the probability of the random variable belonging to the set \(E\). We use \((,^{2})\) to denote the Gaussian distribution with mean \(\) and variance matrix \(^{2}\). When \(D\) is a distribution, we use \(X D\) to denote that the random variable \(X\) is distributed according to \(D\). When \(S\) is a set, we let \(_{X S}[]\) denote the expectation under the uniform distribution over \(S\). When clear from context, we denote the empirical expectation and probability by \(}\) and \(}\). We denote \(\|\|\) as the \(_{2}\)-norm and assume \(f:^{d}\) is differentiable and \(L\)-smooth, i.e., \(\| f(x)- f(x^{})\| L\|x-x^{}\|\) for all \(x,x^{}^{d}\).

\(\) will always denote the oblivious noise drawn from a distribution \(Q\) satisfying \(_{ Q}[=0]\). \(y,y^{}\) will be used to denote mean-zero and variance at-most \(^{2}\) random variables. Also define \(e(,x):=f(,x)-f(x)\) and the interval \((a,b]:=( a, b]\).

FactsWe use these algorithmic facts in the following sections. We use a list-decodable robust mean estimation subroutine in a black-box manner. The proof for list-decodable mean estimation can be found in Appendix D, while such algorithms can be found in prior work, see, e.g., Charikar et al. (2017), Diakonikolas et al. (2020). We also define a \((,)\)-inexact-learner for \(f\).

**Fact 2.1** (List-decoding algorithm).: _There is an \((,,((1/)^{1/^{2}}))\)-LDME algorithm for the an inlier distribution belonging to \(_{}\) which runs in time \((d(1/)^{1/^{2}})\) and succeeds with probability \(1-\). Conversely, any algorithm which returns a list, one of which makes an error of at most \(O()\) in \(_{2}\) norm to the true mean, must have a list whose size grows exponentially in \(1/\)._

_If \(1-\) is a sufficiently small constant less than half, then the list size is \(1\) to get an error of \(O(\ )\)._

We now define the notion of a robust-learner, which is an algorithm which, given access to approximate gradients is able to recover a point at which the gradient norm is small.

**Definition 2.2** (Robust Inexact Learner).: _Let \(f\) be a \(L_{s}\)-smooth function with a global minimum and \(_{,f}^{}()\) be an oracle which when queried on \(x\) returns a vector \(g_{x}\) satisfying \(\|g_{x}- f(x)\|\). We say that an algorithm \(_{G}\) is an \((,)\)-inexact-learner for \(f\) if, given access to \(_{,f}^{}\), the algorithm \(_{G}\) recovers \(\) satisfying \(\| f()\|+\). We will assume \(_{G}\) runs in time \(T_{G}\)._

Several algorithms for convex optimization amount to there being an existence of a robust learner in the convex setting. More recently, for smooth nonconvex optimization, the convergence result for SGD under inexact gradients due to Ajalloeian and Stich (2020) doubles as a \((,)\)-inexact-learner running in \(T_{G}=O(LF/(+)^{2})\) iterations, where \(F=f(x_{0})-_{x}f(x)\) and \(x_{0}\) is the initial point. This follows by an application of Theorem 4 from Ajalloeian and Stich (2020) and by taking the point in the set of iterates that minimizes the gradient norm.

## 3 Location Estimation

To estimate how the gradient changes between iterations, we will need to estimate the shift between a distribution and its translation. In this section, we give an algorithm which, given access to samples from a distribution and its noisy translation returns an estimate of \(t\) accurate up to an error of \(O()\) in \(_{2}\)-norm. We show a proof sketch here, and a more detailed proof of all the claims involved in Appendix C.

**Lemma 3.1** (One-dimensional location-estimation).: _There is an algorithm (Algorithm 1) which, for \(m=(1/^{5})(O(1)/)^{2/}(1/)\), given samples \(\{_{i}+y_{i}+t\}_{i=1}^{m}\) and \(\{_{i}+y^{}_{i}\}_{i=1}^{m}\) where \(_{i}\) and \(_{i}\) are both drawn from \(D_{}\), \(y_{i}\) and \(y^{}_{i}\) are drawn from distinct distributions with mean-zero with variance bounded above by \(\) and \(t\) is an unknown translation, runs in time \(}(1/,(O(1)/)^{1/},(1/))\) and recovers \(t^{}\) such that \(|t-t^{}|\ O()\)._

Proof.: We first identify \(t\) up to an error of \(O(/)\) with the following claim.

**Claim 3.2** (Rough Estimate).: _There is an algorithm which, for \(m=O((1/^{4})\ (1/))\) given samples \(\{_{i}+y_{i}+t\}_{i=1}^{m}\) and \(\{_{i}+y^{}_{i}\}_{i=1}^{m}\) where \(_{i}\) and \(_{i}\) are both drawn from \(D_{}\), \(y_{i}\) and \(y^{}_{i}\) are drawn from distinct distributions with mean-zero with variance bounded above by \(\) and \(t\) is an unknown translation, returns \(|t^{}_{r}-t| O(^{-1/2})\)._

We use \(t^{}_{r}\) to center the two distributions up to an error of \(^{-1/2}\). Let the centered distributions be given by \(+z\) and \(+z^{}\) where \(z,z^{}\) are independent, have means that are at most \(^{-1/2}\) in magnitude and have variance that is bounded by \(4^{2}\).

Let \(A B\) denote the symmetric difference of the two events \(A,B\). To improve our estimate further, we restrict our attention to an interval \(I=(-i,i)\) such that \(O(}[ I])\) and \(}[(+z^{} I)\ \ ( I)]<O( }[ I])\), i.e., neither \(z\) nor \(z^{}\) moves more than a negligible mass of \(\) either into, or out of \(I\).

To detect such an interval (if it exists), we control the total mass contained in, and moved across the intervals \((-i-1,-i)\) and \((i,i+1)\) when \(z\) or \(z^{}\) are added to \(\). We denote this mass by \(P^{}(i)\). Claim C.4 shows that there is a function \(()\) which we can compute using our samples from the distributions of \(+z\) and \(+z^{}\), which serves as a good upper bound on \(P^{}(i)\).

**Claim 3.3**.: _There exists a function \(:^{+}\) which satisfies the following: 1. For all \(i\), \((i) P(i,z)+P(i,z^{})\) which can be computed using samples from \(+z\) and \(+z^{}\). 2. There is a \(k[(1/),(C/+1/()^{})^{1/}]\) such that \(k(k)<_{j=0}^{k}(k)\). 3. \(_{j=0}^{k}(k)=O(}[|+z| A k ]+}[|+z^{}| A k])\). 4. With probability \(1-\), for all \(i<(O(1)/)^{1/}/\), \((i)\) can be estimated to an accuracy of \(O(/i)\) by using \((O(1)/)^{2/}(1/)/^{5}\) samples from the distributions of \(+z\) and \(+z^{}\)._

Claim 3.4 shows that if such a \(()\) exists, then the difference of the conditional expectations suffices to get a good estimate. This is because conditioning on \(|+z|<10k\) satisfying conditions (2) and (3) above is almost the same as conditioning on \(||<10k\).

**Claim 3.4**.: _Suppose \(i>()^{-1}\) and \(()\) satisfies the conclusions of Claim C.4. If \(z,z^{}\) have \(A\) bounded means and \(4^{2}\) bounded variances for some \(A\), then \([+z|+z| A i]-[+z^{}| +z^{}| A i]=[z]-[z^{}] O(A)\)_

The conclusion of the one-dimensional location estimation theorem then follows by putting these together, and iterating the above steps after re-centering the means. The sample complexity is governed by the following considerations:(1) We require the \([+z|+z|^{-1/2}k]-[+z^{}| +z^{}|^{-1/2}k]\) to concentrate around its mean for some \(i(C/+1/()^{})^{1/}\). (2) We require \([+z(i,i+1)]\) to concentrate for all integer \(i\) of magnitude at most \((C/+1/()^{})^{1/}\).

An application of Hoeffding's inequality (Lemma A.1) and a union bound over all the events prove that it suffices to ensure sample complexity to be \(m(1/,(O(1)/)^{1/},(1/))\).

The following Lemma 3.5 uses the algorithm for one dimension to derive a higher-dimensional guarantee. The idea is to perform a random rotation, which, with high probability, ensures that the variance of the distribution is \(O(/)\) along each basis and then apply the one-dimensional algorithm coordinate-wise according to the random bases. We defer the proof to Appendix C. The algorithm referenced in this lemma is named "ShiftHighD", which we use later.

**Lemma 3.5** (Location Estimation).: _Let \(D_{y_{i}}\) for \(i\{1,2\}\) be the distributions of \(+z_{i}\) for \(i\{1,2\}\) where \(\) is drawn from \(D_{}\) and \(_{ D_{}}[=0]\) and \(z_{i} D_{i}\) are distributions over \(^{d}\) satisfying \(_{D_{i}}[x]=0\) and \(_{D_{i}}[\|x\|^{2}]^{2}\). Let \(v^{d}\) be an unknown shift, and \(D_{yz,v}\) denote the distribution of \(y_{2}\) shifted by \(v\). There is an algorithm (Algorithm 5), which draws \((1/,(O(1)/)^{1/},(1/))\) samples each from \(D_{y_{1}}\) and \(D_{yz,v}\) runs in time \((d,1/,(O(1)/)^{1/},(1/))\) and returns \(v^{}\) satisfying \(\|v^{}-v\| O()\) with probability \(1-\)._

## 4 Algorithmic Results for Stochastic Optimization

Our first main result demonstrates that if we have access to an algorithm that recovers a list of size \(s\) such that one of the means is \(O()\)-close to the true mean in \(_{2}\) norm, then there is an algorithm which is able to recover \(\) such that \(\| f()\| O()+\) given access to gradients that are close to the true gradient up to an \(_{2}\) error of \(O()\).

### Proof of Theorem 1.4: List-decodable Stochastic Optimization \(\) LDME

**Input:**\(\), \(\), \(\), \((,O(),s)\)-LDME algorithm \(_{ME}\) for \(_{}\), \(_{,,f}()\), \((O(),)\)-learner \(_{G}\).

1. Let \(m^{}=m_{_{ME}}\)

2. Query \(_{,,f}(0)\) to get samples \(S^{*}\{ f(_{i},0)+_{i} i[m^{}]\}\).

3. Let \(_{0}:=\{g_{1},,g_{s}\}_{ME}(S)\).

4. Initialize starting points \(x_{i}^{}:=g_{i}\) for \(i[s]\).

5. For each \(i[s]\), run \(_{G}\) with \(x_{i}^{0}\) as the initial point and \((x;_{,,f}(),O(), _{0})_{i}\) (the \(i\)-th element of the list) as gradient access to output \(x_{i}^{final}\).

6. Return \(\{x_{i}^{final} i[s]\}\).

**Algorithm 2** Noisy Gradient Optimization: NoisyGradDec\((,,,,_{G},_{ME})\)

Proof.: The key to our proof is to recognize that at every step, we effectively have an oracle for an inexact gradient of \(f\) in the sense of \(_{O(),f}^{}\) as defined in Definition 2.2.

**input:**\(x\), oracle \(_{,,f}\), error \(\), a list \(_{0}\) of candidates such that \(_{g}\|g- f(0)\| O()\).

1. Let \(:=/\) and \(m^{}=((1/)^{3/})\)

2. Query \(_{,,f}(0)\) to get samples \(S^{*}\{ f(_{i},0)+_{i} i[m^{}]\}\).

3. Query \(_{,,f}(x)\) to get samples \(S\{ f(_{i},x)+_{i} i[m^{}]\}\).

4. \(v:=(S,S^{*},,)\).

5. Return \(_{0}+v\).

**Claim 4.1**.: _Given an initial list \(_{0}:=\{g_{1},,g_{s}\}\) such that there is some fixed \(i[s]\) for which \(\|g_{i}- f(0)\| O()\), a point \(x\), access to \(_{,,f}()\), Algorithm 3 returns a list \(:=\{g^{}_{1},,g^{}_{s}\}\) such that \(\|g^{}_{i}- f(x)\| O()\) for the same \(i\)._

Proof.: \(_{,,f}(y)\) returns samples of the kind \(\{_{i}+e(,x)_{i}+ f(x)\}_{i=1}^{m}\) and \(\{_{i}+e(,0)_{i}+ f(0)\}_{j=1}^{m}\) for \(y=x\) and \(y=0\), with \(e(,y)_{i}\) being drawn from a distribution with \(0\) and variance bounded by \(^{2}\) and \(_{i}\) and \(_{i}\) being drawn from \(D_{}\) where \(_{ D_{}}[=0]\). Hence, one can interpret the samples drawn as being in the setting of Lemma 3.5 with the shift \(v= f(0)- f(x)\).

Let \(S_{1}\) and \(S_{2}\) be drawn from \(_{,,f}(y)\) with \(y=0\) and \(y=x\). Running Algorithm 5 on \(S_{1},S_{2}\), we recover \(v\) satisfying \(\|v-( f(x)- f(0))\| O()\). A triangle inequality now tells us that if we set \(g^{}_{i}:=g_{i}+v\), we get \(\|g^{}_{i}- f(x)\|=\|g^{}_{i}-g_{i}+g_{i}- f(0)+ f (0)- f(x)\| O()+O()=O()\). 

\(g^{}_{i}\) can be interpreted as the output of \(O^{}_{O(),f}(x)\), since \(\|g^{}_{i}- f(x)\| O()\). The result then follows from the guarantees of \(_{G}\), since for at least one of the sequences of gradients, Algorithm 2 replicates every step of \(_{G}\). 

Substituting the guarentees of creftype 2.1 for the list-decoding algorithm in the above theorem then gives us the following corollary, the proof of which we defer to Appendix E.

**Corollary 4.2**.: _Given access to oblivious noise oracle \(_{,,f}\), a \((O(),)\)-inexact-learner \(_{G}\) running in time \(T_{G}\), there exists an algorithm which takes \(((O(1)/)^{1/^{2}},(T_{G}/))\) samples, runs in time \(T_{G}(d,(O(1)/)^{1/^{2}},(1/ ))\), and with probability \(1-\) returns a list \(\) of size \(((1/)^{1/^{3}})\) such that \(_{x}\| f(x)\| O()+\). Additionally, the exponential dependence on \(1/\) in the size of the list is necessary._

### Proof of Theorem 1.6: LDME \(\) List-Decodable Stochastic Optimization

In this subsection, we show the converse of the results from the previous subsection, i.e. that list-decodable stochastic optimization can be used to perform list-decodable mean estimation.

Proof.: Assume there exists an algorithm \(\) that can recover a \(s\)-sized list containing \(\) such that \(\| f()\|\) when given access to \(^{}_{,,f}()\). From the list-decodable mean estimation setting, denote \(D\) to be the distribution of \(+z\) where \([z]=\) and \([\|z-\|^{2}]<^{2}\), and \([=0]\).

The goal of LDME is to recover \(\) from samples from \(D\). We will show that we can recover an \(s\)-sized list that contains a \(\) satisfying \(\|-\|\).

We do this by simulating the oracle \(_{,,(1/2)\|x+\|^{2}}()\) for \(\). To do this, whenever \(\) asks for a query at \(x\), we return \(x+p\) where we sample \(p D\) of the list-decodable setting. This effectively simulates the oracle for the objective function \(f(x)=(1/2)\|x+\|^{2}\), where the oblivious noise \(\) is the same, and the observation noise is \((z-)\). Hence \(\) will return a list \(\) of size \(s\) containing \(\) satisfying \(\|+\|\). Finally, return \(-\). This is the solution to list-decodable mean estimation because \(-\) contains \(:=-\), which satisfies \(\|-\|\). 

## 5 Conclusion

In this paper, we have initiated the study of stochastic optimization in the presence of oblivious noise, which extends the traditional heavy-tailed noise framework. In our setting, the stochastic gradient is additionally affected by independent oblivious noise that lacks bounded moments and may not be centered. We have also designed an algorithm for finite-sample noisy location estimation based on taking conditional expectations, which we believe is of independent interest. We note that while the exponential dependence on \(1/\) for the size of the list is unavoidable, but it is an open problem to show that this is the case for the problem of noisy location estimation.