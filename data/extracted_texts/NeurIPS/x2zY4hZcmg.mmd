# Dynamic Model Predictive Shielding for

Provably Safe Reinforcement Learning

Arko Banerjee

The University of Texas at Austin

arko.banerjee@utexas.edu

&Kia Rahmani

The University of Texas at Austin

kia@durable.ai

&Joydeep Biswas

The University of Texas at Austin

joydeepb@utexas.edu

&Isil Dillig

The University of Texas at Austin

isil@utexas.edu

###### Abstract

Among approaches for provably safe reinforcement learning, Model Predictive Shielding (MPS) has proven effective at complex tasks in continuous, high-dimensional state spaces, by leveraging a _backup policy_ to ensure safety when the learned policy attempts to take unsafe actions. However, while MPS can ensure safety both during and after training, it often hinders task progress due to the conservative and task-oblivious nature of backup policies. This paper introduces Dynamic Model Predictive Shielding (DMPS), which optimizes reinforcement learning objectives while maintaining provable safety. DMPS employs a local planner to dynamically select safe recovery actions that maximize both short-term progress as well as long-term rewards. Crucially, the planner and the neural policy play a synergistic role in DMPS. When planning recovery actions for ensuring safety, the planner utilizes the neural policy to estimate long-term rewards, allowing it to _observe_ beyond its short-term planning horizon. Conversely, the neural policy under training learns from the recovery plans proposed by the planner, converging to policies that are both _high-performing_ and _safe_ in practice. This approach guarantees safety during and after training, with bounded recovery regret that decreases exponentially with planning horizon depth. Experimental results demonstrate that DMPS converges to policies that rarely require shield interventions after training and achieve higher rewards compared to several state-of-the-art baselines.

## 1 Introduction

Safe Reinforcement Learning (SRL)  aims to learn policies that adhere to important safety requirements and is essential for applying reinforcement learning to safety-critical applications, such as autonomous driving. Some SRL approaches give _statistical_ guarantees and reduce safety violations by directly incorporating safety constraints into the learning objective . In contrast, _Provably Safe_ RL (PSRL) methods aim to learn policies that _never_ violate safety and are essential in high-stakes domains where even a single safety violation can be catastrophic .

A common approach to PSRL is _shielding_, where actions proposed by the policy are monitored for potential safety violations. If necessary, these actions are overridden by a safe action that is guaranteed to retain the system's safety. Model Predictive Shielding (MPS) is an emerging PSRL method that has proven effective in high-dimensional, continuous state spaces with complex dynamics, surpassing previous shielding approaches in applicability and performance . At a high level, MPS methods leverage the concept of _recoverable states_ that lead to a safe equilibrium in \(N\) time-steps by followinga so-called _backup policy_. MPS approaches dynamically check (via simulation) if a proposed action results in a recoverable state and follow the backup policy if it does not.

However, an important issue with existing MPS approaches is that the backup policies are not necessarily aligned with the primary task's objectives and often propose actions that, while safe, impede progress towards task completion--even when alternative safe actions are available that do not obstruct progress in the task. Intuitively, this occurs because the backup policy is designed with the sole objective of driving the agent to the closest equilibrium point rather than making task-specific progress. For instance, in an autonomous navigation task, if the trained policy suggests an action that could result in a collision, MPS methods revert to a backup policy that simply instructs halting, rather than trying to find a more optimal recovery approach, such as finding a route that steers around the obstacle. As a result, the recovery phase of MPS frequently inhibits the learning process and incurs high _recovery regret_, meaning that there is a large discrepancy between the return from executed recovery actions and the maximum possible returns from the same states.

Motivated by this problem, we propose a novel PSRL approach called _Dynamic Model Predictive Shielding_ (DMPS), which aims to optimize RL objectives while ensuring provable safety under complex dynamics. The key idea behind DMPS is to employ a local planner [11; 12] to dynamically identify a safe recovery action that optimizes finite-horizon progress towards the goal. Although the computational overhead of planning grows exponentially with the depth of the horizon, in practice, a reasonably small horizon is sufficient for allowing the agent to recover from unsafe regions.

In DMPS, the planner and the neural policy under training play a synergistic role. First, when using the planner to recover from potentially unsafe regions, our optimization objective not only uses the finite-horizon reward but also uses the Q-function learned by the neural policy. The integration of the Q-function into the optimization objective allows the planner to take into account _long-term reward_ beyond the short planning horizon needed for recovering safety. As a result, the planner benefits from the neural policy in finding recovery actions that optimize for long-term reward. Conversely, the integration of the planner into the training loop allows the neural policy to learn from actions suggested by the planner: Because the planner dynamically figures out how to avoid unsafe regions while making task-specific progress, the policy under training also learns how to avoid unsafe regions in a _smart_ way, rather than taking overly conservative actions. From a theoretical perspective, DMPS can guarantee provable safety both during and after training. We also provide a theoretical guarantee that the regret from recovery trajectories identified by DMPS is bounded and that it decays at an exponential rate with respect to the depth of the planning horizon. These properties allow DMPS to learn policies that are both high-performing and safe.

We have implemented the DMPS algorithm in an open-source library and evaluated it on a suite of 13 representative benchmarks. We experimentally compare our approach against Constrained Policy Optimization (CPO) , PPO-Lagrangian (PPO-Lag) [13; 14], Twin Delayed Deep Deterministic Policy Gradient (TD3)  and state-of-the-art PSRL approaches, MPS  and REVEL . Our results indicate that policies learned by DMPS outperform all baselines in terms of total episodic return and achieve safety with minimal shield invocations. Specifically, DMPS invokes the shield \(76\%\) less frequently compared to the next best baseline, MPS, and achieves \(29\%\) higher returns after convergence.

To summarize, the contributions of this paper are as follows: First, we introduce the DMPS algorithm, a novel integration of RL and planning, that aims to address the limitations of model predictive shielding. Second, we provide a theoretical analysis of our algorithm and prove that recovery regret decreases exponentially with planning horizon depth. Lastly, we present a detailed empirical evaluation of our approach on a suite of PSRL benchmarks, demonstrating superior performance compared to several state-of-the-art baselines.

## 2 Related Work

PsrlThere is a growing body of work addressing safety issues in RL [1; 2; 17; 18; 19; 20; 21]. Our approach falls into the category of provably safe RL (PSRL) techniques [7; 18], treating safety as a hard constraint that must never be violated. This is in contrast to _statistically safe_ RL techniques, which provide only statistical bounds on the system's safety by constraining the training objectives [3; 22; 23; 24; 25]. These soft guarantees, however, are insufficient for domains like autonomous driving, where each failure can be catastrophic. Existing PSRL methods can be categorized based on whether they guarantee safety _throughout the training phase_ or only _post-training_ upon deployment . Our approach falls into the former category and employs a shielding mechanism that ensures safety both during training and deployment.

_Safety Shielding._ Many PSRL works use _safety shields_. These methods typically synthesize a domain-specific policy ahead of time and use it to detect and substitute unsafe actions at runtime. However, traditional shielding methods tend to be computationally intractable, leading to limited usability. For instance,  presents one of the early works in shielding, where safety constraints are specified in Linear Temporal Logic (LTL), and a verified reactive controller is synthesized to act as the safety shield at runtime. However, this approach is restricted to discrete state and action spaces, due to the complexity of shield synthesis. Another example is , which enhances PSRL performance by substituting shield-triggering actions with verified actions and reducing the frequency of shield invocations. In Model Predictive Shielding (MPS) , a backup policy dynamically assesses the states from which safety can be reinstated and, if necessary, proposes substitute actions. Unlike pre-computed shields, MPS can handle high-dimensional, non-linear systems with continuous states and actions, without incurring an exponential runtime overhead . MPS has also been successfully applied to stochastic dynamics systems  and multi-agent environments . However, a significant limitation of current MPS methods is the separation of safety considerations from the RL objectives, which hinders learning when employing the recovery policy.

_RL and Planning._ Planning has traditionally been considered a viable complement to reinforcement learning, combining real-time operations in both the actual world and the learned environment model . With recent developments in deep model-based RL  and the success of planning algorithms in discrete  and continuous spaces , the prospect of combining these methods holds great promise for solving challenging real-world problems. Integrating planning within RL has also been applied to safety measures . For instance, Safe Model-Based Policy Optimization  minimizes safety violations by detecting non-recoverable states through forward planning using an accurate dynamics model of the system. However, it only employs planning to identify unsafe states, not to find optimal recovery paths from such situations. To the best of our knowledge, \(\) is the first method that leverages dynamic planning for optimal recovery, within the framework of model predictive shielding.

_Classical Control._ There is a long line of classical control approaches to the problem of safe navigation . One common approach, for instance, is the use of _control barrier functions_ (CBFs) . CBFs have been used across many domains in robotics to achieve safety with high confidence . While useful, these approaches tend to make assumptions about the environment (e.g. differentiability, closed-form access to dynamics) that cannot easily be reconciled with the highly general RL framing of this work.

## 3 Preliminaries

_Mdp._ We formulate our problem using a standard Markov Decision Process (MDP) \(=,_{U},_{0},, ,,\), where \(^{n}\) is a set of states, \(_{U}\) is a set of unsafe states, \(_{0}\) are the initial states, \(^{m}\) is a continuous action space, \(:\) is a deterministic transition function, \(:\) is a deterministic reward function, and \(\) is the discount factor. We define \(\) to be the set of all agent policies, where each policy \(\) is a function from environment states to actions, _i.e.,_\(:\). For any set \(S\), the set of _reachable states_ from \(S\) in \(i\) steps under policy \(\), is denoted by \(_{i}(,S)\), and is defined recursively as follows: \(_{1}(,S)\{(,(s)) s S\}\) and \(_{i+1}(,S)_{1}(,_{i}( ,S))\). The set of _all_ reachable states under a policy \(\) is \(()_{1 i}_{i}(,_{0})\).

_Psrl._ The standard objective in RL is to find a policy \(\) that maximizes a performance measure, \(J()\), typically defined as the expected infinite-horizon discounted total return. In provably safe reinforcement learning (PSRL), the aim is to identify a _safe_ policy that maximizes the above measure. The set of safe policies is denoted by \(_{}\) and consists of policies that never reach an unsafe state, _i.e.,_\(_{}()_{U}=\). Therefore, the objective of PSRL is to find a policy \(_{}^{*}_{_{}}J()\).

## 4 Model Predictive Shielding

Under the Model Predictive Shielding (MPS) framework , a safe policy, \(_{}\), is constructed by integrating two sub-policies: a _learned policy_, \(_{}\), and a _backup policy_, \(_{}\). Depending on the current state of the system, control of the agent's actions is delegated to one of these two policies. The learned policy is typically implemented as a neural network that is trained using standard deep RL techniques to optimize \(J()\). However, this policy may violate safety during training or deployment, _i.e.,_\(_{}_{}\). On the other hand, the backup policy, \(_{}\), is specifically designed for safety and is invoked to substitute potentially unsafe actions proposed by the learned policy.

Due to domain-specific constraints on system transitions, the backup policy is effective only from a certain subset of states, called _recoverable_ states. For instance, given the deceleration limits of an agent, a backup policy that instructs the agent to halt can only avoid collisions from states where there is sufficient distance between the agent and the obstacle. At a high level, the recoverability of a given state \(s\) in MPS is determined by a function \(:\). This function performs an \(N\)-step forward simulation of \(_{}\) from \(s\) and checks whether a safety equilibrium can be established.

Figure 1 gives an overview of the control delegation mechanism in \(_{}\).1 Given state \(s\), the agent first forecasts the next state \(\) that would be reached by following the learned policy (double-bordered, yellow box). If \((,_{})\) is true, then \(_{}\) simply returns the same action as \(_{}\), as marked by. Otherwise, if \((,_{})\) is false, \(_{}\) delegates control to the backup policy \(_{}\), as indicated by. The selected action, \(a^{}\), is then executed in the environment (single-bordered, green box), resulting in a new state \(s^{}\). From this state, \(_{}\) is executed again, and the process repeats.

The safety of \(_{}\) relies on the fact that all recoverable states are safe and that the backup policy is _closed_ under the set of recoverable states. Thus, we can inductively show that the agent remains in safe and recoverable states; thus \(_{}_{}\).

#### Example.

Consider an agent on a 2D plane aiming to reach a goal while avoiding static obstacles. Figure 2 (a) presents an unsafe trajectory proposed by the learned policy. Figure 2 (b) presents the trajectory under MPS. As discussed above, \(_{}\) delegates control to the backup policy in such potentially unsafe situations, which corresponds here to applying maximum deceleration away from the obstacle. This causes the agent to come to a halt, which is suboptimal. Figure 2 (c) depicts a DMPS trajectory. Figure 2 (d) depicts DMPS planning in an environment containing a static obstacle and a low-reward puddle region. The latter two are explained in section 5.

### Recovery Regret

While MPS guarantees worst-case safety, shield interventions can hinder the training of \(_{}\) and compromise overall efficacy. To formalize this limitation, we introduce the concept of _Recovery_

Figure 2: (a) Unsafe trajectory leading to a collision. (b) Safe but sub-optimal trajectory. (c) Optimal and safe trajectory. (d) An instance of the planning phase.

_Regret_, which measures the expected performance gap between \(_{}\) and the optimal policy. To this end, we first introduce a helper function \(_{}:\{0,1\}\), which, given a state \(s\), indicates whether control in \(s\) is delegated to the backup policy, _i.e.,_\(_{}(s)=1\) if \(((s,_{}(s),_{}))\) and \(_{}(s)=0\) otherwise. In any state \(s\) where control is delegated to the backup policy, the _optimal value_\(V^{*}(s)\) represents the maximum expected reward that can be achieved from \(s\), and the _optimal action-value_\(Q^{*}(s,_{}(s))\) represents the value of executing \(_{}\) in \(s\) and thereafter following an optimal policy. Thus, we can define Recovery Regret (\(RR\)) as the expected discrepancy between these two values across states where \(_{}\) is invoked, _i.e.,_

\[RR(_{},_{})_{s^{_{ }}}_{}(s)V^{*}(s)-Q^{*}(s,_{}(s))\]

In the above formula, \(^{_{}}\) is the _discounted state visitation distribution_ associated with \(_{}\), _i.e.,_\(^{_{}}(s)(1-)_{t=0}^{}^{t}Pr(s_{t} \!=\!s s_{0}_{0},\ _{i 0}.\ s_{i+1}\!=\! (s_{i},_{}(s_{i})))\). This term assists in quantifying how frequently each state is visited when measuring the regret of a backup policy.

In existing MPS approaches, the misalignment between backup policies and the optimal policy can result in substantial recovery regrets. For example, Figure 2 (c) illustrates the optimal sequence of actions in the previously discussed scenario, where the agent avoids a collision by maneuvering around the obstacle. However, in MPS, the training process lacks mechanisms to teach such optimal behavior to \(_{}\) and only teaches overly cautious and poorly rewarded actions depicted in Figure 2 (b).

## 5 Dynamic Model Predictive Shielding

In this section, we introduce the Dynamic Model Predictive Shielding (DMPS) framework that builds on top of MPS but aims to address its shortcomings. Similar to the control delegation mechanism in MPS, the policy \(_{}\) initially attempts to select its next action using a learned policy \(_{}\). If the action proposed by \(_{}\) leads to a state that is not recoverable using \(_{}\), an alternative safe action is executed instead, as in standard MPS. However, rather than using the task-oblivious policy \(_{}\), the backup action is selected using a _dynamic backup policy_\(_{}^{*}\). More formally,

\[_{}(s)=_{}(s)&\ ((s,_{}(s)),_{}) \\ _{}^{*}(s)&\] (1)

The core innovation behind \(\) is the dynamic backup policy \(_{}^{*}\), which is realized using a _local planner_[45; 77], denoted as a function \(()\). At a high level, \(\) performs a finite-horizon lookahead search over a predetermined number of steps (\(n\)) and identifies a sequence of backup actions optimizing the expected returns during recovery. Specifically, the function \(\) takes an initial state \(s_{0}\) and a state-action value function \(Q:\), and returns a sequence of task-optimal actions \(a_{0:n}^{*}^{n+1}\) defined as follows:

\[(s_{0},Q)*{arg \,max}_{a_{0:n}^{n+1}}(_{i=0}^{n-1}^{i} (s_{i},a_{i}))+^{n} Q(s_{n},a_{n}),\\ \ _{0 i<n}[s_{i+1}=(s_{i},a_{i} )]\ \ _{0 i n}(s_{i},_{}).\] (2)

Crucially, the recovery plan is required to only lead to _recoverable_ states within the finite planning horizon (_i.e.,_\((s_{i},_{})\)). Note that the backup policy \(_{}\) is used by the planner in deciding the recoverability of a state. Beyond satisfying the hard safety constraint, the planner is also required to optimize the objective function shown in Equation 2. Importantly, this objective accounts for both the immediate rewards within the planning horizon, \((s_{i},a_{i})\), _as well as_ the estimated long-term value from the terminal state \(s_{n}\) beyond the planning horizon, as defined by \(Q\). As a result, the planner benefits from the long-term reward estimates learned by the neural policy \(_{}\).

Figure 2 (d) illustrates an agent planning a recovery path around an obstacle. The actions considered by \(\) are represented by gray edges. Two optimal paths on this tree, depicted in green and red, yield similar rewards due to the symmetric nature of the reward function relative to the goal position. The planner selects the green path on the left as its final choice due to a water puddle on the right side of the goal, which, if traversed, would result in lower returns. Since the puddle lies outside of the planning horizon, the decision to opt for the green path is informed by access to the

Q-function. By executing the first action on the green path and repeating dynamic recovery, the agent can demonstrate the desired behavior shown in Figure 2 (c).

Given the plan returned by planRec, the dynamic backup policy \(^{*}_{}\) returns the first action in the plan \(a^{*}_{0}\) as its output. However, planRec could, in theory, fail to find a safe and optimal plan, even though one exists. In such cases, planRec returns a special symbol \(\), and \(^{*}_{}\) reverts to the task-oblivious backup policy \(_{}\). Thus, we have:

\[^{*}_{}(s)=a^{*}_{0}&(s,Q_{ _{}})=a^{*}_{0:n}\\ _{}(s)&(s,Q_{_{}})= \] (3)

An outline of \(_{}\) is shown in Figure 1 where the control delegation mechanism is represented by the red dashed arrows (marked as \(\) and \(\)) instead of the blue solid arrow (marked as \(\)).

### Planning Optimal Recovery

The specific choice of the planning algorithm to solve Equation 2 depends on the application domain. However, DMPS requires two main properties to be satisfied by the planner: _probabilistic completeness_ and _asymptotic optimality_. The former property states that the planner will eventually find a solution if one exists, while the latter states that the found plan converges to the optimal solution as the allocated resources increase. There exist several state-of-the-art planners that satisfy both of these requirements, including sampling-based planners such as RRT*  and MCTS , which have been shown to be particularly effective at finding high-quality solutions in high-dimensional continuous search spaces [53; 54; 55]. These methods construct probabilistic roadmaps or search trees and deal with the exponential growth in the search space by incrementally exploring and expanding only the most promising nodes based on heuristics or random sampling. Given an implementation of planRec that satisfies the aforementioned requirements, a significant outcome in DMPS is that, as the depth of the planning horizon (\(n\)) increases, the expected return from \(^{*}_{}\) approaches the globally optimal value. The following theorem states the optimality of recovery in \(_{}\), in terms of exponentially diminishing recovery regret of \(^{*}_{}\) as \(n\) increases.

**Theorem 5.1**.: 2 _(Simplified) Suppose the use of a probabilistically complete and asymptotically optimal planner with planning horizon \(n\) and sampling limit \(m\). Under mild assumptions of the MDP, the recovery regret of policy \(^{*}_{}\) used in \(_{}\) is almost surely bounded by order \(^{n}\) as \(m\) goes to infinity. In other words, with probability 1,_

\[_{m}RR(^{*}_{},_{})=( ^{n}).\]
While our proposed recovery can be used during deployment irrespective of how the neural policy is trained, our method integrates the planner into the training loop, allowing the neural policy to learn to "imitate" the safe actions of the planner while making task-specific progress. Hence, the training loop converges to a neural policy that is both high-performant _and_ safe. This is very desirable because DMPS can avoid expensive shield interventions that require on-line planning during deployment.

Algorithm 1 presents a generic deep RL algorithm for training a policy \(_{}\). Lines 3-4 of the algorithm perform initialization of the neural policy \(_{}\) as well as the _replay buffer_\(\), which stores a set of tuples \( s,a,s^{},r\) that capture transitions and their corresponding reward \(r\). Each training episode begins with the agent observing the initial state \(s\) (line 6) and terminates when the goal state is reached or after a predetermined number of steps are taken (line 7). The agent first attempts to choose its next action \(a_{}\) using the learned policy \(_{}\) (Line 8). If the execution of \(a_{}\) leads to a non-recoverable state according to \(_{}\) (line 9), the algorithm first adds a record to the replay buffer where the current state and action are associated with a high negative reward \(r^{-}\) (line 10). It then performs dynamic model predictive shielding to ensure safety (lines 11-12) as discussed earlier: If the planner yields a valid policy, the the next action is chosen as the first action in the plan (line 11); otherwise, the backup policy \(_{}\) is used to determine the next action. Then, \(a_{}\) is executed at line 13 to obtain a new state \(s^{}\) and its corresponding reward \(r\). This new transition and its corresponding reward are again added to the replay buffer (line 14), which is then used to update the neural policy at line 16, after the termination of the current training episode.

## 6 Experiments

In this section, we present an empirical study of DMPS on 13 benchmarks and compare it against 4 baselines. The details of our implementation and experimental setup are presented in Appendix A.3.

Benchmarks.We evaluate our approach on five _static benchmarks_ (ST), where the agent's environment is static, and eight _dynamic benchmarks_, where the agent's environment includes moving objects. Static benchmarks (obstacle, obstacle2, mount-car, road, and road2d) are drawn from prior work [16; 27] and include classic control problems. The more challenging dynamic benchmarks (dynamic-obst, single-gate, double-gates, and double-gates+) require the agent to adapt its policy to accommodate complex obstacle movements. Specifically, dynamic-obs features moving obstacles along the agent's path. In single-gate, a rotating circular wall with a small opening surrounds the goal position. double-gate is similar but includes _two_ concentric circular walls around the goal, and double-gate+ is the most challenging, featuring increased wall thickness to force more efficient navigation through the openings. For each dynamic benchmark, we consider two different agent dynamics: _differential drive dynamics_ (DD), featuring an agent with two independently driven wheels, and _double integrator dynamics_ (DI), where the agent's acceleration in any direction can be adjusted by the policy. A detailed description of benchmarks can be found in Appendix A.3.

Baselines.We compare DMPS (with a planning horizon of \(n=5\)) with five baselines. Our first baseline is MPS, the standard model predictive shielding approach. The second baseline is REVEL, a recent PSRL approach that learns verified neurosymbolic policies through iterative mirror descent. REVEL requires a white-box model of the environment's worst-case dynamics, which is challenging to develop for dynamic benchmarks; hence, we apply REVEL only to static benchmarks. We also compare DMPS against three established RL methods: CPO, PPO-Lag (PPO Lagrangian) [13; 14], and TD3. All aim to reduce safety violations during training by incorporating safety constraints into the objective. In TD3, a negative penalty is applied to unsafe steps. In CPO, a fixed number of violations are tolerated. Finally, in PPO-Lag, a Lagrangian multiplier is used to balance safety cost with reward.

### Safety Results

Table 1 presents our experimental results regarding safety. All results are averaged over 5 random seeds. For PSRL methods (DMPS, MPS, and REVEL), which guarantee worst-case safety, we report the average number of shield invocations per episode. Generally, less frequent shield invocation indicates higher performance of the approach. For SRL methods (TD3, PPO-Lag and CPO), we report the average number of safety violations per episode.

Due to the relative simplicity of the static benchmarks, the PSRL baselines achieve safety with very infrequent shield invocations. Even the SRL baselines are mostly able to avoid safety violations in these benchmarks. On the other hand, in dynamic benchmarks, PSRL approaches heavily rely on shielding to achieve safety, and SRL approaches violate safety more frequently. Notably, the number of DMPS shield invocations is significantly lower than in other baselines. Across all dynamic benchmarks, DMPS invokes the shield an average of \(24.7\) times per episode, whereas MPS triggers the shield \(124.1\) times. The results also indicate that the standard deviation values for shield invocations in DMPS are consistently lower than those of MPS, indicating more stable and predictable performance.

Figure 3 shows the number of shield triggers plotted against episodes for the double-gate and double-gate+ dynamic benchmarks. The error regions are 1-sigma over random seeds. Under both agent dynamics, DMPS achieves significantly fewer shield invocations compared to MPS. Moreover, the number of shield invocations in DMPS consistently decreases as training progresses, whereas this trend is not present in MPS. In fact, in many scenarios, the number of shield invocations in MPS increases with more training because the learned policy reduces its exploratory actions and adheres more strictly to a direct path to the goal. However, this frequently leads to being blocked by obstacles and results in repeated shield invocations.

### Performance Results

Table 2 presents the per-episode return over the last 10 test episodes of a run. The reported mean and standard deviations are computed over 5 random seeds. The results indicate that DMPS and MPS exhibit comparable performance across most static benchmarks, with the exception of the more challenging obstacle and obstacle2 benchmarks for which DMPS significantly outperforms MPS. In dynamic benchmarks, DMPS outperforms MPS in all benchmarks except for single-gate (DI), where both methods achieve equivalent results. Both DMPS and MPS consistently outperform REVEL. The SRL approaches (CPO, PPO-Lag, and TD3) perform reasonably well in most static benchmarks but their performance significantly deteriorates in dynamic benchmarks, failing to achieve positive returns in any instance. This is because the policies in CPO, PPO-Lag, and TD3 rapidly learn to avoid both the obstacles and the goal due to harsh penalties for safety violations, thus accruing the negative rewards for each timestep spent away from the goal.

   &  &  \\   &  &  &  &  &  &  &  \\  & & mean & sd & mean & sd & mean & sd & mean & sd & mean & sd & mean & sd \\   & obstacle & **0.0** & 0.0 & **0.0** & 0.0 & 4.0 & 8.0 & **0.0** & 0.0 & **0.0** & 0.0 & **0.0** & **0.0** & **0.0** \\  & obstacle2 & **0.0** & 0.0 & **0.0** & 0.0 & 33.8 & 30.3 & 0.9 & 0.2 & 6.42 & 0.19 & **0.0** & 0.0 \\  & mount-car & **0.0** & 0.0 & **0.0** & 0.0 & 4.0 & 4.0 & 2.0 & 2.1 & 22.2 & 28.9 & **0.0** & 0.0 \\  & road & **0.0** & 0.0 & **0.0** & 0.0 & 0.8 & 0.74 & **0.0** & 0.0 & **0.0** & 0.0 & **0.0** & 0.0 \\  & road2d & **0.0** & 0.0 & 0.0 & **0.0** & **0.0** & 0.0 & 0.0 & **0.0** & 0.0 & 0.0 & 0.0 & **0.0** & 0.0 \\   & dynamic-obst & **9.3** & 1.8 & 144.1 & 26.2 & - & - & 1.7 & 0.8 & 3.6 & 3.9 & **0.8** & 1.2 \\  & single-gate & **0.1** & 0.0 & 0.2 & 0.1 & - & - & 2.0 & 1.2 & 10.0 & 5.5 & **0.0** & 0.0 \\  & double-gates & **4.5** & 1.2 & 28.3 & 6.9 & - & - & 2.1 & 1.7 & 11.8 & 6.3 & **0.3** & 0.5 \\  & double-gates+ & **24.2** & 6.4 & 239.8 & 16.3 & - & - & 2.9 & 1.0 & 6.5 & 4.5 & **0.0** & 0.0 \\   & dynamic-obst & **105.2** & 39.9 & 144.9 & 39.9 & - & - & 1.7 & 1.9 & 2.9 & 2.2 & **0.8** & 0.16 \\  & single-gate & **3.1** & 1.6 & 7.4 & 6.9 & - & - & 5.2 & 3.5 & 6.7 & 7.2 & **0.1** & 0.2 \\   & double-gates & **5.5** & 1.9 & 52.5 & 17.6 & - & - & 3.9 & 1.7 & 7.9 & 9.2 & **3.0** & 5.5 \\   & double-gates+ & **18.4** & 13.0 & 106.2 & 18.5 & - & - & 12.1 & 2.9 & 6.9 & 6.1 & **0.2** & 0.4 \\  

Table 1: Safety Results

Figure 3: Shield Invocations in double-gate and double-gate+

Figure 4 presents the total return plotted against the episode number for the double-gate and double-gate+ dynamic benchmarks. The error regions are 1-sigma over random seeds. The CPO and PPO-Lag baselines are omitted due to their significantly poor performance, which distorts the scale of the graphs. In all benchmarks, DMPS demonstrates superior performance compared to the other methods. While MPS performs adequately in three of the benchmarks, it exhibits poor performance in the double-gate+ (DI) benchmark.

### Analysis

We analyze the performance of DMPS and MPS on the double-gate+ environment under double-integrator dynamics. Figure 4(a) shows trajectories from the first half of training when the agent's policy and \(Q\) function are still under-trained. When the agents approach the first gate and attempt to cross it, the shield is triggered. In the case of MPS, this shield simply pushes the agent back outside the gate (see blue trajectory), and the agent is unable to make any progress. In contrast, DMPS plans actions that maximize an \(n\)-step return target, allowing the agent to initially make progress. However, we can observe from the green trajectory that the agent's \(Q\) function and policy are under-trained: after having made it through the obstacles, the agent is not sufficiently goal-aware to reliably make it to the center. This makes the DMPS planner objective inaccurate with respect to long-term return, causing suboptimal actions to be selected by the shield. In the red trajectory, for instance, the agent spends a large portion of the trajectory trying to cross the second gate, only to retreat and get stuck in that position until eventually getting through.

As training progresses, the MPS backup policy hinders the agent's exploration of the environment, impeding the learning of the neural policy. By the end, most runs of the MPS agent are unable to progress past the first gate. Figure 4(b) shows one of the few MPS runs that successfully make it through the first gate towards the end of training; however, the MPS agent still fails to make it through

   &  &  \\   & &  &  &  &  &  &  \\   & & mean & sd & mean & sd & mean & sd & mean & sd & mean & sd & mean & sd \\   & obstacle & 32.7 & 0.3 & 8.6 & 47.9 & \(-41.6\) & \(52.7\) & \(32.8\) & \(0.0\) & \(32.8\) & \(0.4\) & \(\) & \(0.0\) \\  & obstacle2 & 20.2 & 15.2 & \(-18.3\) & 3.2 & 9.3 & 21.1 & 33.0 & \(0.2\) & \(\) & \(0.1\) & \(-1.2\) & \(3.0\) \\  & mount-car & 81.2 & 0.3 & \(\) & 1.8 & 11.4 & 34.1 & 9.6 & \(35.3\) & \(-21.3\) & \(2.9\) & \(-30.0\) & \(35.5\) \\  & road & \(\) & 0.0 & \(\) & 0.0 & 9.7 & 16.4 & \(\) & 0.0 & \(\) & 0.05 & \(\) & 0.0 \\  & road2d & \(\) & 0.2 & \(\) & 0.2 & 11.2 & 16.5 & \(\) & \(0.0\) & \(\) & \(0.1\) & \(\) & \(0.1\) \\   & dynamic-obs & \(\) & \(0.0\) & \(-1.3\) & \(1.9\) & - & \(-\) & \(-21.4\) & \(21.1\) & \(-4.2\) & \(24.9\) & \(-5.0\) & \(0.3\) \\  & single-gate & \(\) & 0.0 & \(\) & 0.0 & - & - & \(-19.6\) & \(17.0\) & \(-2.0\) & \(1.1\) & \(-2.1\) & \(0.2\) \\  & double-gates & \(\) & 1.0 & 11.5 & 1.1 & - & - & \(-6.7\) & \(5.4\) & \(-3.9\) & \(11.1\) & \(-3.6\) & \(1.0\) \\  & double-gates+ & \(\) & 0.6 & \(-0.9\) & 0.1 & - & - & \(-17.4\) & \(12.8\) & \(-2.8\) & \(0.4\) & \(-4.1\) & \(1.1\) \\   & dynamic-obst & \(\) & 3.7 & 6.3 & 2.1 & - & - & \(-4.5\) & \(0.5\) & \(-3.6\) & \(0.8\) & \(-5.3\) & \(0.5\) \\  & single-gate & \(\) & 0.1 & 11.4 & 0.1 & - & - & \(-2.5\) & \(0.3\) & \(-2.4\) & \(0.2\) & \(-3.1\) & \(0.5\) \\  & double-gates+ & \(\) & 0.5 & 10.9 & 1.8 & - & - & \(-2.4\) & \(0.6\) & \(-1.9\) & \(0.9\) & \(-3.4\) & \(0.5\) \\   & double-gates+ & \(\) & 0.6 & 8.5 & 2.3 & - & - & \(-2.5\) & \(0.3\) & \(-20.7\) & \(22.8\) & \(-3.6\) & \(0.3\) \\  

Table 2: Performance Results

Figure 4: Episodic Returns in double-gate and double-gate+

Figure 5: Example trajectories in double-gate+.

the second gate. In contrast, the DMPS agent can learn from the actions suggested by the planner, improving its policy and \(Q\) function. This improvement makes the DMPS planner's objective a more accurate estimation of long-term return, further strengthening the planner. Consequently, the DMPS agent demonstrates significantly better behaviors, as shown by the red trajectory in Figure 4(b).

## 7 Limitations

### Determinism

First, our approach requires a perfect-information deterministic model, which could limit its usability in real-world deployment. Much prior work on provably safe locomotion makes the same determinism assumptions that we do [10; 31; 80], with some such algorithms even having been deployed on real robots . However, extending our DMPS approach to stochastic environments is a promising direction for future work. In particular, since prior MPS work has been extended to work in stochastic settings , we believe our DMPS approach can be similarly extended to the stochastic setting.

### Computational Overhead

Another potential limitation of our approach is the computational overhead of the planner. We use MCTS in an anytime planning setting, where we devote a fixed number of node expansions to searching for a plan. The clock time needed is linear in the allocated compute budget. However, the worst-case computational complexity to densely explore the search space, as in general planning problems, would be \(O((H))\) since the planning search space grows exponentially. Our implementation used MCTS with 100 node expansions, a plan horizon of 5, and a branch factor of 10, which amounts to exploring 1000 states in total. On average, we found that when the shield is triggered, planning takes 0.4 seconds per timestep. The code we used is unoptimized, written in Python, and single-threaded. Since MCTS is a CPU-intensive search process, switching to a language C++ would likely yield significant speed improvements, and distributing computation over multiple cores would further slash the compute time by the number of assigned cores. We perform some additional experiments on the double-gates+ environment, outlined in subsection A.4, to see how necessary compute budget scales with planner depth, confirming a rough exponential relationship.

### Sufficiency of Planning Horizon

Finally, it can be asked whether small planning horizons (in our case, we used \(H=5\)) are sufficient to solve tricky planning problems. Our planner objective is designed to ensure that the planner accounts for both short-term and long-term objectives, preventing overly myopic behavior even with short horizons. However, the length of the horizon still affects how close to the globally optimal solution the result is, with a tradeoff of computational cost. To see this empirically, we conducted an experiment on the double-gates+ environment, re-evaluating it under different planner depths and observing performance differences. Despite better initial performance, the low horizon agent converged to the same performance once the policy had fully stabilized. As part of our analysis, we also found that trivial planning (\(H=1\)) does not work, reaffirming the necessity of a planner. More analysis and experimental results can be found in subsection A.5.

## 8 Conclusions

In this paper, we proposed Dynamic Model Predictive Shielding (DMPS), which is a variant of Model Predictive Shielding (MPS) that performs dynamic recovery by leveraging a local planner. The proposed approach takes less conservative actions compared to MPS while still guaranteeing safety, and it learns neural policies that are both effective and safe in practice. Our evaluation on several challenging tasks shows that DMPS improves upon the state-of-the-art methods in Safe Reinforcement Learning.

Impact Statement.Over the past decade, reinforcement learning has experienced significant advancements and is increasingly finding applications in critical safety environments, such as autonomous driving. The stakes in such settings are high, with potential failures risking considerable property damage or loss of life. This study seeks to take a meaningful step for mitigating these risks by developing reinforcement learning agents that are rigorously aligned with real-world safety requirements.

Acknowledgements.This work is partially supported by the National Science Foundation (CCF-2319471 and CCF-1901376). Any opinions, findings, and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.