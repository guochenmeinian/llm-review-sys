# Improving Generalization and Convergence by Enhancing Implicit Regularization

Mingze Wang\({}^{1,3,}\) Jinbo Wang\({}^{1,3}\) Haotian He\({}^{1,3}\) Zilin Wang\({}^{1}\) Guanhua Huang\({}^{5,6}\) Feiyu Xiong\({}^{3}\) Zhiyu Li\({}^{3}\) Weinan E\({}^{1,2,3,4}\) Lei Wu\({}^{1,2,}\)

\({}^{1}\)School of Mathematical Sciences, Peking University

\({}^{2}\)Center for Machine Learning Research, Peking University

\({}^{3}\)Institute for Advanced Algorithms Research (Shanghai) \({}^{4}\)AI for Science Institute

\({}^{5}\)School of Data Science, University of Science and Technology of China \({}^{6}\)ByteDance Research

{mingzewang, wangjinbo, haotianhe, wangzilin}@stu.pku.edu.cn

guanhuahuang@mail.ustc.edu.cn {xiongfy, lizy}@iaar.ac.cn

{weinan, leiwu}@math.pku.edu.cn

Correspondence to: Mingze Wang and Lei Wu.

###### Abstract

In this work, we propose an Implicit Regularization Enhancement (IRE) framework to accelerate the discovery of flat solutions in deep learning, thereby improving generalization and convergence. Specifically, IRE decouples the dynamics of flat and sharp directions, which boosts the sharpness reduction along flat directions while maintaining the training stability in sharp directions. We show that IRE can be practically incorporated with _generic base optimizers_ without introducing significant computational overload. Experiments show that IRE consistently improves the generalization performance for image classification tasks across a variety of benchmark datasets (CIFAR-10/100, ImageNet) and models (ResNets and ViTs). Surprisingly, IRE also achieves a \(2\)_speed-up_ compared to AdamW in the pre-training of Llama models (of sizes ranging from 60M to 229M) on datasets including Wikitext-103, Minipile, and Openwebtext. Moreover, we provide theoretical guarantees, showing that IRE can substantially accelerate the convergence towards flat minima in sharpness-aware minimization (SAM).

## 1 Introduction

Deep learning has achieved remarkable success across a variety of fields, including computer vision, scientific computing, and artificial intelligence. The core challenge in deep learning lies in how to train deep neural networks (DNNs) efficiently to achieve superior performance. Understanding and improving the generalization and convergence of commonly-used optimizers, such stochastic gradient descent (SGD) (Robbins and Monro, 1951; Rumelhart et al., 1986), in deep learning is crucial for both theoretical research and practical applications.

Notably, optimizers often exhibit a preference for certain solutions in training DNNs. For instance, SGD and its variants consistently converge to solutions that generalize well, even when DNNs are highly over-parameterized and there are many solutions that generalize poorly. This phenomenon is referred to as _implicit regularization_ in the literature (Neyshabur et al., 2014; Zhang et al., 2017).

The most popular explanation for implicit regularization is that SGD and its variants tend to converge to flat minima (Keskar et al., 2016; Wu et al., 2017), and flat minima generalize better (Hochreiter andSchmidhuber, 1997; Jiang et al., 2019). However, the process of this _implicit sharpness regularization_ occurs at a very slow pace, as demonstrated in works such as Blanc et al. (2020), Li et al. (2022), and Ma et al. (2022). Consequently, practitioners often use a large learning rate (LR) and extend the training time even when the loss no longer decreases, ensuring the convergence to flatter minima (He et al., 2016; Goyal et al., 2017; Hoffer et al., 2017). Nevertheless, the largest allowable LR is constrained by the need to maintain training stability. In addition, Foret et al. (2021) proposed SAM, which aims to explicitly regularize sharpness during training and has achieved superior performance across a variety of tasks.

**Our contributions** can be summarized as follows:

* We propose an Implicit Regularization Enhancement (IRE) framework to speed up the convergence towards flatter minima. As suggested by works like Blanc et al. (2020), Li et al. (2022) and Ma et al. (2022), the implicit sharpness reduction often occurs at a very slow pace, along flat directions. Inspired by this picture, IRE particularly accelerates the dynamics along flat directions, while keeping sharp directions' dynamics unchanged. As such, IRE can boost the implicit sharpness reduction substantially without hurting training stability. For a detailed illustration of this mechanism, we refer to Section 2.
* We then provide a practical IRE framework, which can be efficiently incorporated with generic base optimizers. We evaluate the performance of this practical IRE in both vision and language tasks. For vision tasks, IRE consistently improves the generalization performance of popular optimizers like SGD, Adam, and SAM in classifying the CIFAR-10/100 and ImageNet datasets with ResNets (He et al., 2016) and vision transformers (ViTs) (Dosovitskiy et al., 2020). For language modelling, we consider the pre-training of Llama models (Touvron et al., 2023) of various sizes, finding that IRE surprisingly can accelerate the pre-training convergence. Specifically, we observe a remarkable \(2.0\) speedup compared to AdamW in the scenarios we examined, despite IRE being primarily motivated to speed up the convergence to flat solutions.
* Lastly, we provide theoretical guarantees showing that IRE can achieves a \((1/)\)-time acceleration over the base SAM algorithm in minimizing the trace of Hessian, where \((0,1)\) is a small hyperparameter in SAM.

### Related works

**Implicit sharpness regularization.** There have been extensive attempts to explain the mystery of implicit regularization in deep learning (see the survey by Vardi (2023) and references therein). Here, we focus on works related to implicit sharpness regularization. Wu et al. (2018, 2022) and Ma and Ying (2021) provided an explanation of implicit sharpness regularization from a dynamical stability perspective. Moreover, in-depth analysis of SGD dynamics near global minima shows that the SGD noise (Blanc et al., 2020; Li et al., 2022; Ma et al., 2022; Damian et al., 2021) and the edge of stability (EoS)-driven (Wu et al., 2018; Cohen et al., 2021) oscillations (Even et al., 2024) can drive SGD/GD towards flatter minima. Additional studies explored how training components, including learning rate and batch size (Jastrzebski et al., 2017), normalization (Lyu et al., 2022), cyclic LR (Wang and Wu, 2023), influence this sharpness regularization. Furthermore, some works have provided theoretical evidence explaining the superior generalization of flat minima for neural networks (Ma and Ying, 2021; Mulayoff et al., 2021; Wu and Su, 2023; Gatmiry et al., 2023; Wen et al., 2023). Our work is inspired by this line of research, aiming to boost implicit sharpness regularization by decoupling the dynamics along flat and sharp directions.

**Sharpness-aware minimization.** IRE shares the same motivation as SAM in enhancing sharpness regularization, although their specific approaches differ significantly. It is worth noting that the per-step computational cost of SAM is twice that of base optimizers. Consequently, there have been numerous attempts to reduce the computational cost of SAM (Kwon et al., 2021; Liu et al., 2022; Du et al., 2021; Mi et al., 2022; Mueller et al., 2024). In contrast, the per-step computational cost of IRE is only approximately 1.1 times that of base optimizers (see Table 5). Moreover, we provide both theoretical and experimental evidence demonstrating that the mechanism of IRE in boosting sharpness regularization is nearly orthogonal to that of SAM.

**Optimizers for large language model (LLM) pre-training.** (Momentum) SGD (Sutskever et al., 2013; Nesterov, 1983) and its adaptive variants like Adagrad (Duchi et al., 2011), RMSProp (Tieleman, 2012), and Adam (Kingma and Ba, 2014) have been widely used in DNN training. Despite the effortsin designing better adaptive gradient methods (Liu et al., 2019; Luo et al., 2019; Heo et al., 2020; Zhuang et al., 2020; Xie et al., 2022b;a), AdamW(Adam+decoupled weight decay) (Loshchilov and Hutter, 2017) has become the default optimizer in LLM pre-training. Recently, Chen et al. (2024) discovered Lion by searching the space of adaptive first-order optimizers; Liu et al. (2024) introduced Sophia, a scalable second-order optimizer. In this paper, we instead empirically demonstrate that IRE can accelerate the convergence of AdamW in the pre-training of Llama models.

### Notations

Throughout this paper, let \(:^{p}_{ 0}\) be the function of total loss, where \(p\) denotes the number of model parameters. For a \(^{2}\)-submanifold \(\) in \(^{p}\), we denote the tangent space of \(\) at \(\) as \(_{}\), which is a linear subspace in \(^{p}\). For \(f^{1}()\) and \(\), let \(_{}f():=_{_{} } f()\) denote the Riemannian gradient, where \(_{_{}}:^{p} ^{p}\) denotes the orthogonal projection to \(_{}\). For a symmetric matrix \(A^{p p}\), its eigen pairs are denoted as \(\{(_{i},_{i})\}_{i[p]}\) with the order \(_{1}_{p}\). We use \(P_{i;j}(A)=_{k=i}^{j}_{k}_{k}^{}\) to denote the projection operator onto \(\{_{i},,_{j}\}\). Denote \((,)\) as the Gaussian distribution with mean \(\) and covariance matrix \(\), and \(()\) as the uniform distribution over a set \(\). Given a vector \(=(h_{1},,h_{p})\), let \(||=(|h_{1}|,,|h_{p}|)\). We denote by \(\) the all-ones vector. We will use standard big-O notations like \(()\), \(()\), and \(()\) to hide constants.

## 2 An Illustrative Example Motivating IRE

In this section, we provide an illustration of how the dynamics along flat directions can _reduce the sharpness_ (curvatures along sharp directions) and how IRE can accelerate this sharpness reduction. To this end, we consider the following phenomenological problem:

\[_{^{p}}():=^{}H( )/2,\] (1)

where \(^{m}\), \(^{p-m}\), and \(=(,)^{p}\). We assume \(H()^{2}(^{p-m})\) and \(_{}_{}(H())>0\). Then, the minimizers of \(()\) form a \(m\)-dim manifold \(=\{(,):=\}\) and the Hessian at any \(\) is given by \(^{2}()=&\\ &H()\). For clarity, we shall call \(\) and \(\) the flat and sharp directions, respectively.

**Example 2.1**.: _The loss landscape of fitting zero labels with two-layer neural networks (2LNNs) exhibits exactly the form (1). Let \(f(;)=^{}(;W)\) be a 2LNN with \(=(W,)\). Then \(()=_{(,y)}[(f(;)-y)^{2 }]/2=^{}_{}[(;W)(;W)^{}]/2=:^{}H(W)/2\)._

For brevity, we further assume \(H()=(())\) with \(()=(_{1}(),,_{m}())\). In this case, the GD dynamics can be naturally decomposed into the flat and sharp directions as follows

\[_{t+1} =_{t}-_{i=1}^{m}v_{t,i}^{2}_ {i}(_{t}),\] (2) \[_{t+1} =(-(_{t}))_{t},\]

where \(\) denotes the element-wise multiplication of two vectors.

**The implicit sharpness regularization.** From Eq. (2), we can see that 1) the flat direction \(_{t}\)'s dynamics monotonically reduces the sharpness \(()\) as long as \(_{t}\) is nonzero; 2) the sharp direction \(_{t}\)'s dynamics determines the speed of sharpness reduction. The larger \(|_{t}|\) is, the faster the curvature \(()\) decreases. Particularly, when near convergence, we have \(|_{t}|=o(1)\) and thus the implicit sharpness reduction is _very slow_ during the late phase of GD. Figure 0(a) provides a visualization of this slow implicit sharpness reduction.

**Accelerating the sharpness reduction.** Inspired by the above analysis, we can accelerate the sharpness reduction by speeding up the flat directions' dynamics. To this end, there are two approaches:

* **Naively increasing the global learning rate \(\) (**fail**). Increasing \(\) accelerates the dynamics of \(_{t}\), but the largest allowed \(\) is constrained by curvatures of sharpest directions. In GD (2), to maintain training stability, \(\) must be smaller than \(2/_{i}_{i}(_{t})\). Otherwise, \(_{t}\)'s dynamics will blow up. As illustrated in Figure 0(b), setting \(=2\) leads to divergence, whereas \(=1\) ensures convergence.
* **Increasing only the flat directions' learning rate (our approach, IRE)**. Specifically, for GD (2), the GD-IRE dynamics is given by \[_{t+1} =_{t}-(1+)_{i=1}^{m}v_{t,i}^{2} _{i}(_{t}),\] (3) \[_{t+1} =(-(_{t}))_{t},\] where \(>0\) controls the enhancement strength. In GD-IRE (3), \(_{t}\)'s dynamics is \((1+)\) faster than that of GD (2). Notably, the sharp directions' dynamics (\(_{t}\)) are unchanged. The choice of \(\) only needs to maintain the stability of flat directions' dynamics, for which, we can always take a significantly large \(\) to enhance the sharpness regularization. As demonstrated in Figure 0(c), IRE with larger \(\) always find flatter minima.

**Remark 2.2** (The generality).: It is worth noting that similar implicit sharpness regularization also holds for SGD (Ma et al., 2022; Li et al., 2022) and SAM (Wen et al., 2023). In this section, we focus on the above toy model and GD mainly for illustration. In Appendix B, we provide an analogous illustrative analysis of how IRE accelerates the sharpness reduction of SGD. In Section 5, we further provide theoretical evidence to show that IRE can boost the implicit sharpness regularization of SAM.

## 3 A Practical Framework of Implementing IRE

Although the preceding illustration of IRE is for GD, in practice, we can incorporate IRE with _any base optimizers_. Specifically, for a generic update: \(_{t+1}=_{t}-_{t}\), the corresponding IRE modification is given by

\[_{t+1}=_{t}-(_{t}+_{t} _{t}),\] (4)

where \(\) denotes the enhancement strength and \(_{t}:^{p}^{p}\) projects \(_{t}\) into the _flat directions_ of the landscape. The flat directions and corresponding projection operator \(_{t}\) can be estimated using the Hessian information.

However, estimating the full Hessian matrix \(^{2}(_{t})^{p p}\) is computationally infeasible. Consequently, we propose to use only the _diagonal Hessian_\((^{2}(_{t}))^{p}\) to estimate \(_{t}\). Let \(_{t}^{p}\) be an estimate of the diagonal Hessian. Then, we perform the projection as follows

\[_{t}_{t}=_{t}_{t},(_{t})_{i}= 1&(|_{t}|)_{i}_{}(|_{t}|, )\\ 0&,\] (5)

where \((0,1)\) and \(_{}(|_{t}|,)\) returns the \(|p|\)-th smallest value in \(|_{t}|\). Note that \(_{t}^{p}\) denotes a mask vector and the above approximate projection essentially masks the top-\((1-)\) sharp coordinates out. As such, the projection (5) will retain the _top-\(\) flat coordinates_. Noticing that in DNNs, there are much more flat directions than sharp directions (Yao et al., 2020), we thus often use \(>0.5\) in practice.

Figure 1: A \(2\)-d example of (1): \((u,v)=(1+u^{2})v^{2}/2\). The gray arrows denote to the minima manifold \(=\{(u,v):v=0\}\), where the smaller the \(|u|\), the flatter the minimizer. The red marker highlights the flattest minimizer \((0,0)\). (a) The dynamics of GD (\(=1\)), which moves _slowly_ towards flatter minima as it converges. (b) The dynamics of GD (\(=2\)), which diverges due to the excessively large \(\). (c) The behavior of our IRE approach with varying \(\)’s v.s. GD (\(=1\)). Is is shown that IRE can significantly accelerate the \(u_{t}\)’s dynamics, almost reaching the flattest minimum \((0,0)\) when taking a very large \(\).

A light-weight estimator of the diagonal Hessian.Let \((,)\) be the cross-entropy loss. Given an input data \(^{d_{x}}\) and label \(^{d_{y}}\), let the model's prediction be \(f(;)^{d_{y}}\). The Fisher (Gauss-Newton) matrix \(F()\) is widely acknowledged to be a good approximation of the Hessian, particularly near minima. Thus, we can estimate the diagonal Hessian by \(_{t}=(F(_{t}))\), which has been widely used in deep learning optimization (Martens and Grosse, 2015; Grosse and Martens, 2016; George et al., 2018; Mi et al., 2022; Liu et al., 2024). Given an input batch \(\{(_{b},_{b})\}_{b=1}^{B}\), the empirical diagonal Fisher is given by \((())=_{b=1}^{B}( f(_{b};);}_{b})(f(_{b};);}_{b})\), where \(}_{b}(f(;_{b}))\). However, as noted by Liu et al. (2024), implementing this estimator is computationally expensive due to the need to calculate \(B\) single-batch gradients. Liu et al. (2024) proposed a more convenient estimator \((_{}())\), only requires computing the mini-batch gradient \(}_{B}()=_{b=1}^{B}( f(_{b};);}_{b})\) with \(}_{b}(f(_{b};))\):

\[_{t}=(_{}())=B }_{B}()}_{B}( {}).\] (6)

According to Liu et al. (2024), this estimator is an unbiased estimate of the empirical diagonal Fisher, i.e., \(_{}}[(_{}())]=_{}}[(( ))]\). For more discussions on the efficiency of this estimator, please refer to (Liu et al., 2024, Section 2). Additionally, for squared loss, one can simply use Fisher as the estimator (Liu et al., 2024).

The practical IRE and computational efficiency.The practical IRE is summarized in Algorithm 1, which is notably lightweight. The estimation of \(_{t}\) using (6) requires computational resources roughly equivalent to one back-propagation. Consequently, by setting \(K=10\) in Algorithm 1 (estimating the projection every 10 steps), the average per-step computational load of IRE is _only \(1.1\) times_ that of the base optimizer. This claim can be empirically validated as shown in Table 5.

``` Input:\(_{0}\), \(T\), \(K\), learning rate \(\{_{t}\}_{t}\), warm-up time \(T_{}\), IRE hyperparams: \( 0\) and \((0.5,1)\); for\(t=T_{},,T-1\)do  Compute the original update direction \(_{t}\) according to the base optimizer; if\((t-T_{}) K=0\)then  Estimate the diagonal Hessian \(_{t}^{p}\) using Eq. (6);  Update the mask \(_{t}^{p}\) using Eq. (5); else \(_{t}=_{t-1}\) \(_{t+1}=_{t}-_{t}_{t}+_{t}_{t}}\); Output:\(_{T}\). ```

**Algorithm 1**Practical IRE (A practical framework of implementing IRE)

## 4 Experiments

In this section, we evaluate how IRE performs when incorporating with various base optimizers. Specifically, we examine the incorporation of IRE with SGD (SGD-IRE), SAM (SAM-IRE), and AdamW (AdmIRE) across vision and language tasks.

### Image classification

#### 4.1.1 Validating our motivation

To show that IRE can accelerate the sharpness reduction, we train WideResNet-16-8 (Zagoruyko and Komodakis, 2016) on CIFAR-10 dataset (Krizhevsky and Hinton, 2009) by SAM-IRE (with \(K=10\), varying \(\) and \(\)). Here, we incorporate IRE into SAM starting from the \(30\)-th epochs. We vary \(\{0.8,0.9,0.95\}\) and \(\{0,2,5,10\}\). Regarding the learning rate (LR), both constant LR and decayed LR are considered. The sharpness is measured by \((^{2}())\). Further experimental details can be found in Appendix C.

As depicted in Fig. 2(a), SAM-IRE (with constant LR) consistently finds flatter solutions compared to SAM and higher \(\) always leads to flatter minima. Additionally, SAM-IRE also shows robustness to 

[MISSING_PAGE_FAIL:6]

### Large language model pre-training

We now evaluate IRE in the pre-training of decoder-only large language models (LLMs). Following the training protocol of Llama models, we employ the AdamW optimizer with hyperparameters \(_{1}=0.9,_{2}=0.95\) and weight decay \(=0.1\)(Touvron et al., 2023). The learning rate strategy includes a warm-up phase followed by a cosine decay scheduler, capped at lr_max. In each experiment, we tune lr_max only for AdamW and use it also for AdmIRE, for which the IRE is activated at the end of warm-up phase.

#### 4.2.1 Computational efficiency and hyperparameter robustness

The first experiment is conducted to verify both the computational efficiency and the robustness of hyperparameters \((,)\) in IRE for pre-training tasks. Specifically, we train a 2-layer decoder-only Transformer (8M) on the Wikitext-2 dataset (4.3M) (Merity et al., 2016) by AdamW and AdmIRE (with \(K=10\) and varying \(,\)). The total training duration is 100k steps, including a 3k-step warm-up phase.

First, we tune lr_max in AdamW, identifying the optimal lr_max=6e-4. Subsequently, we train both AdamW and AdmIRE using this lr_max.

Computational efficiency.As shown in Table 5, AdmIRE with \(K=10\) (estimating the projection mask every 10 steps) is computationally efficient: the average time per step of AdmIRE is only \(1.12\) times that of AdamW, corresponding to the theoretical estimation (\(1.1\) times).

Robustness to hyperparameters.Figure 3 shows that AdmIRE, with varying \(\) and \(\), consistently speeds up the pre-training. Remarkably, with the best configuration, AdmIRE can achieves **5.4\(\) speedup** than well-tuned AdamW.

More experimental details and results are deferred to Appendix C.

#### 4.2.2 Experiments on Llama models

Llama (Touvron et al., 2023), a popular open LLM, exhibits remarkable capabilities across general domains. In this section, we examine the performance of AdmIRE in training Llama models of various sizes across various datasets:

* **Llama (60M) on wikitext-103 (0.5G).** Wikitext-103 (Merity et al., 2016) serves as a standard language modeling benchmark for pre-training, which contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article.
* **Llama (119M) on minipile (6G).** Minipile (Kaddour, 2023), a 6GB subset of the deduplicated Pile (825GB) (Gao et al., 2020) presents a highly diverse text corpus. Given its diversity, training on minipile poses more challenges and potential instabilities for optimizers compared to Wikitext-103.
* **Llama (229M) on openwebtext (38G).** Openwebtext (Gokaslan and Cohen, 2019), an open-source recreation of the WebText corpus, is extensively utilized for LLM pre-training such as RoBERTa (Liu et al., 2019) and GPT-2 (Radford et al., 2019).

Additionally, gradient clipping is adopted to maintain the training stability (Pascanu et al., 2012). First, we tune lr_max in AdamW for each of the three experiments, separately. The optimal lr_max

    & Top-1 & Top-5 \\  AdamW & 78.7 & 94.0 \\ AdmIRE (\(=2,=0.6\)) & **79.0 (+0.3)** & **94.3 (+0.3)** \\ AdmIRE (\(=2,=0.8\)) & **79.1 (+0.4)** & **94.2 (+0.2)** \\   

Table 4: ViT-S on ImageNet.

Figure 3: Transformer on wikitext-2.

identified for these three experiments is all 6e-4. Then, both AdamW and AdmIRE are trained using this optimal lr_max. For more details, please refer to Appendix C.

**AdmIRE is \(2\) faster than AdamW**. The results are reported in Figure 4. We can see that AdmIRE consistently achieves a \(2.1\) speedup compared with well-tuned AdamW for all three cases.

Notice that the primary motivation behind IRE is to speed up the sharpness reduction, which only requires to increase learning rate along completely flat (zero-curvature) directions. However, practical implementation may also increase the learning rate along directions with small but non-zero curvatures, which can further speed up loss convergence. A thorough explanation for the significant acceleration provided by this approach is left for future research.

We further assess the sharpness reduction capability of IRE for LLM pre-training. Specifically, we compare the sharpness of solutions, \((^{2}())\), found by AdamW/AdmIRE during pre-training of Llama (60M) on wiki-103 dataset (corresponding to Figure 4 (left). The results shown in Table 6 demonstrate that AdamIRE not only achieves the same loss in _only half the iterations_ required by AdamW, but also the solutions found by AdmIRE are _significantly flatter_ than that found by AdamW.

Recently, Liu et al. (2023) revealed a strong correlation between the sharpness and downstream task performance, suggesting that for models with the same pre-training loss, flatter solutions yield better performance on downstream tasks. Based on this, we hypothesize that the solutions found by IRE may also have better performance in downstream tasks, which we leave to future work.

## 5 Theoretical Guarantees for IRE on SAMs

Both empirical (Foret et al., 2021) and theoretical (Wen et al., 2023) studies have validated that SAM algorithms exhibit superior sharpness regularization compared to (S)GD. In this section, we provide a theoretical analysis demonstrating that IRE can further enhance the sharpness regularization of SAM algorithms substantially.

### Theoretical setups

Recall that \(():=_{i=1}^{n}_{i}( )\) denote the total loss, where \(_{i}()\) is the loss on the \(i\)-th data. Without loss of generality, we assume \(_{}()=0\). We further make the following assumption:

**Assumption 5.1** (Manifold of minimizers).: Assume that \(^{4}(^{p})\), \(:=*{arg\,min}_{}( )\) is a \((p-m)\)-dim \(^{2}\)-submanifold in \(^{p}\) for some \(m[p]\), and \((^{2}())=m\) for any \(\).

The above connectivity assumption on the manifold of minimizers \(\) has been empirically verified in works such as Draxler et al. (2018) and Garipov et al. (2018), and theoretically supported in Cooper (2018). This assumption is also widely used in the theoretical analysis of implicit regularization (Fehrman et al., 2020; Li et al., 2022; Arora et al., 2022; Wen et al., 2023).

Besides, we introduce the following definitions to characterize the dynamics of gradient flow (GF) near the minima manifold \(\), which is also used in the related works above.

    & AdamW & AdmIRE \\  training steps & 100k & 50k \\  final \(()\) & 2.47 & 2.47 \\  final \((^{2}())\) & 120.41 & 88.86 \\   

Table 6: Comparison of the sharpness of the solutions found by AdamW/AdmIRE.

Figure 4: AdmIRE outperforms AdamW in the pre-training of Llama models.

**Definition 5.2** (Limiting map of GF).: Consider the GF: \((t)}{t}=-((t))\) starting from \((0)=\). Denote by \(():=_{t}(t)\) the limiting map of this GF.

**Definition 5.3** (Attraction set of \(\)).: Let \(U\) be the attraction set of \(\) under GF, i.e., GF starting in \(U\) converges to some point in \(\). Formally, \(U:=\{^{p}:()\}\).

As proven in (Arora et al., 2022, Lemma B.15), Assumption 5.1 ensures that \(U\) (in Definition 5.3) is open and \(()\) (in Definition 5.2) is \(^{2}\) on \(U\).

### Theoretical results

The stochastic SAM (Foret et al., 2021) is given by

\[_{t+1}=_{t}- _{i_{t}}(_{t}+_{ i_{t}}(_{t})}{\|_{i_{t}}(_{t})\|} ),i_{t}([n]).\] (7)

The generalization capability of standard SAM can be bounded by the average sharpness, \(^{}():=_{( ,I)}(+/\|\|)\)(Foret et al., 2021). This leads researchers to also explore average SAM (Wen et al., 2023; Zhu et al., 2023; Ujyarty et al., 2022), which minimizes \(^{}\):

\[_{t+1}=_{t}- (_{t}+_{t}/\|_{t}\|),_{t}(,I).\] (8)

**Two-phase algorithms.** Our theoretical focus is on how IRE accelerates the sharpness reduction of SAM (7) and (8) _near the minima manifold \(\)_. Thus, we analyze the two-phase algorithms. Specifically, let the initialization \(_{0} U\). In **Phase I** (\(t T_{}\)), we employ GF \(_{t}}{t}=-(_{t})\) to ensure that the loss decreases sufficiently; then in **Phase II** (\(T_{}<t T_{}+T_{}:=T\)), we incorporate IRE into the standard / average SAM.

**Effective dynamics: sharpness regularization.** The implicit regularization of SAMs can be modeled using effective dynamics. In Phase II, \(_{t}\) are close the manifold of minimizers \(\) and let \(_{t}:=(_{t})\). Then, the effective dynamics is given by \(\{_{t}\}_{t=T_{}+1}^{T}\), revealing how SAMs explore the manifold of minimizers \(\). Particularly, Wen et al. (2023a) showed that the effective dynamics of standard/average SAM are both

\[[_{t+1}]=_{t}-_{ }_{}[^{2}(_{t})/2 ]+o(_{}),\] (9)

which minimizes the trace of Hessian on \(\). The difference between the standard SAM (7) and average SAM (8) lies in the effective learning rate (LR) \(_{}\)'s. A visual illustration of some quantities in (9) is provided in the figure above.

**Summary of our theoretical results.** In this section, we show that incorporating IRE into SAMs can significantly increase the effective LR \(_{}\) in (9) while maintaining the same training stability as SAMs. In Table 7, we present the effective LR for SAMs and the SAM-IREs. We see clearly that IRE can accelerate the sharpness reduction by a non-trivial factor for both standard and average SAM.

**Remark 5.4** (The mechanism of IRE's success).: The success of SAM-IRE follows the same mechanism illustrated in Section 2. The key fact that IRE only increases the LR along flat directions has two implications: 1) It does not change the trend of implicit regularization in Eq. (9) but accelerates SAMs' effective dynamics by a factor of \((1+)\); 2) Since the LR is only increased along flat directions, \(\) can be set substantially large without hurting the training stability, because the dynamics in sharp directions remain unchanged. Specifically, we theoretically justify in SAM-IRE, \(\) can be selected as large as \(1/\).

   Algorithm & Effective LR: \(_{}\) \\  average SAM (8) & \(^{2}/p\) (Thm 5.5) \\  IRE + average SAM (8) & \(^{1.5}/p\) (Thm 5.5) \\  standard SAM (7) & \(^{2}\) (Thm 5.6; Wen et al. (2023a)) \\  IRE + standard SAM (7) & \(\) (Thm 5.6) \\   

Table 7: Comparison of the implicit regularization strength of SAMs w/o IRE.

#### 5.2.1 IRE on average SAM: An \((1/^{0.5})\) acceleration

We first consider IRE on average SAM. Let \(T_{}\) be the hitting time: \(T_{}:=\{t 0:\|_{t}-(_{t})\|= ()\}\). When running GF in Phase I, Definition 5.3 guarantees \(T_{}<\). Thus, at the starting of Phase II, \(\|_{t}-(_{t})\|=()\). Furthermore, the following result holds for Phase II.

**Theorem 5.5** (IRE on average SAM).: _Suppose Assumption 5.1 holds. If \(=(1)\) and \(=()\) in SAM (8), \(/}\), and \(_{t}=P_{m+1:p}(^{2}(_{t}))\) in IRE (4), then with high probability at least \(1-T_{}^{2}(-(1/(+p^{-1})))\), \(\|_{t}-(_{t})\|=()\) holds for all \(T_{} t T\). Furthermore, the effective dynamics of \(_{t}:=(_{t})\) satisfies:_

\[_{_{t}}[_{t+1}]=_{t}-}{p}_{}[^{2}( _{t})/2]+(^{3/2}^{2}).\]

Note that \(=()\) and \(\) can be as large as \(1/\). Consequently, the effective LR of minimizing the trace of Hessian can be selected as large as \(_{}=(+1)^{2}/p=(^{1.5}/p)\). In contrast, that of average SAM is at most \((^{2}/p)\). The proof of Theorem 5.5 can be found in Appendix D.

#### 5.2.2 IRE on standard SAM: An \((1/)\) acceleration

This subsection delves into IRE on standard SAM (7), which is more widely used and often yields better performance than average SAM (8). However, since standard SAM (7) requires stochastic gradients \(_{i}()\) (\(i[n]\)), we need an additional assumption regarding the features on the manifold (see Setting E.1), which is commonly used in the literature (Du et al., 2018, 2019; Li et al., 2022; Arora et al., 2022; Wen et al., 2023a). We defer it to Appendix E due to space constraints. Under this Setting, Assumption 5.1 holds naturally with \(m=n\).

During Phase I of GF, Definition 5.3 ensures that there exists \(t<\) such that \(\|_{t}-(_{t})\|=(^{1-})\) for any \([0,1)\). We define \(T_{}\) as the hitting time: \(T_{}:=\{t 0:\|_{t}-(_{t})\|= (^{1-})\}\). Then the following result holds for Phase II, whose proof can be founded in Appendix E.

**Theorem 5.6** (IRE on standard SAM).: _Under Setting E.1, if \(,=(1)\) in SAM (7), \(/}\), and \(_{t}=P_{n+1:p}(^{2}(_{t}))\) in IRE (4), then with high probability at least \(1-T_{}^{2}(-(1/^{}))\), \(\|_{t}-(_{t})\|=(^{1-})\) holds for all \(T_{} t T\). Moreover, the effective dynamics \(_{t}=(_{t})\) satisfies:_

\[_{i_{t}}[_{t+1}]=_{t}-(1+)^{2}_{ }[^{2}(_{t})/2] +((+1)^{2}(+^{1-})).\]

Taking \(=0\) and \(=0\) recovers the result established in Wen et al. (2023a). However, \(\) can be as large as \(1/\), where IRE provides a \((1/)\)-time acceleration over the standard SAM.

## 6 Conclusion

In this work, we propose a novel IRE framework to enhance the implicit sharpness regularization of base optimizers. Experiments demonstrate that IRE not only consistently improves generalization but also accelerates loss convergence in the pre-training of Llama models of various sizes. The code is available at https://github.com/wmz9/IRE-algorithm-framework.

For future work, there are two urgent directions: 1) understanding why IRE can accelerate convergence, which may require studying the interplay between IRE and the Edge of Stability (EoS) (Wu et al., 2018; Jastrzebski et al., 2017; Cohen et al., 2021); and 2) conducting a larger-scale investigation into the acceleration of AdmIRE compared to AdamW in LLM pre-training, as well as the downstream performance of the LLMs pre-trained by AdmIRE.