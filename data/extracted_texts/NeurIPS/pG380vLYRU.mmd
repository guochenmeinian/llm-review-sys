# Faster Accelerated First-order Methods for Convex Optimization with Strongly Convex Function Constraints

Faster Accelerated First-order Methods for Convex Optimization with Strongly Convex Function Constraints

 Zhenwei Lin

Shanghai University of Finance and Economics

zhenweilin@163.sufe.edu.cn

&Qi Deng

Antai College of Economics and Management

Shanghai Jiao Tong University

qdeng24@sjtu.edu.cn

Corresponding author

###### Abstract

In this paper, we introduce faster accelerated primal-dual algorithms for minimizing a convex function subject to strongly convex function constraints. Prior to our work, the best complexity bound was \((1/)\), regardless of the strong convexity of the constraint function. It is unclear whether the strong convexity assumption can enable even better convergence results. To address this issue, we have developed novel techniques to progressively estimate the strong convexity of the Lagrangian function. Our approach, for the first time, effectively leverages the constraint strong convexity, obtaining an improved complexity of \((1/)\). This rate matches the complexity lower bound for strongly-convex-concave saddle point optimization and is therefore order-optimal. We show the superior performance of our methods in sparsity-inducing constrained optimization, notably Google's personalized PageRank problem. Furthermore, we show that a restarted version of the proposed methods can effectively identify the optimal solution's sparsity pattern within a finite number of steps, a result that appears to have independent significance.

## 1 Introduction

In this paper, we are interested in the following convex function-constrained problem:

\[_{^{n}} f() g _{i}() 0,\ 1 i m,\] (1)

where \(f:^{n}\) is a convex continuous function and bounded from below and \(g_{i}:^{n}\), \(i=1,2,,m\), are strongly convex continuous functions. An important application of this problem, commonly encountered in statistics and engineering, involves the objective \(f()\) as a proximal-friendly regularizer and \(g_{i}()\) as a data-driven loss function used to gauge model fidelity.

To apply first-order methods for the above function-constrained problems, a common strategy involves a double-loop procedure that repeatedly employs fast first-order methods, such as Nesterov's accelerated method, to solve specific strongly convex proximal subproblems. Popular methods among this category include Augmented Lagrangian methods [18; 33], level-set methods , penalty methods . When both \(f()\) and \(g_{i}()\) are convex and smooth (or composite), it has been found that these double-loop algorithms can attain an iteration complexity of \((1/)\) to achieve an \(\)-errorin both the optimality gap and constraint violation. When the objective is strongly convex, the complexity can be further improved to \((1/)\) ().

In contrast to these double-loop algorithms, single-loop algorithms remain popular due to their simplicity in implementation. Along this research line,  developed a first-order algorithm based on linearizing the augmented Lagrangian function, which obtains an iteration complexity of \((1/)\).  extended the augmented Lagrangian method to stochastic function-constrained problems where both objective and constraint exhibit an expectation form. Viewing (1) as a special case of the min-max problem:

\[_{^{n}}_{^{m}}\ (,):=f()+_{i=1}^{m}y_{i}g_{i}( ),\ y_{i} 0,\ i=1,2,,m,\] (2)

 proposed to solve (1) and (2) by an accelerated primal-dual method (APD), which generalizes the primal-dual hybrid gradient method  initially developed for saddle point optimization with bilinear coupling term. Under mild conditions, APD achieves the best iteration complexity of \((1/)\) for general convex constrained problem and a further improved complexity of \((1/)\) when \(f()\) is strongly convex.  proposed a unified constrained extrapolation method that can be applied to both deterministic and stochastic constrained optimization problems.

Despite these recent progresses, to the best of our knowledge, all available algorithms are suboptimal in the presence of strongly convex function constraints (1). Specifically, direct applications of previously discussed algorithms yield an \((1/)\) complexity, which is inferior to the \((1/)\) optimal bound for the strongly-convex-concave saddle point problem . It is somewhat unsatisfactory that the strong convexity of \(g()\) has not been found helpful in further algorithmic acceleration. The core underlying issue arises from the dynamics of saddle point optimization: it is the strong convexity of \((,)\) that offers more potential acceleration advantages, yet the strong convexity of \((,)\) is substantially harder to estimate than that of \(g()\). This difficulty is compounded by the interplay between \(g()\) and the varying dual sequence \(\{_{k}\}\). The challenge naturally leads us to question: _Is it possible to further improve the convergence rate of first-order methods for solving the strongly convex constrained problem (1)?_

Key intuitionsWe make an assumption that the minimizer of \(f()\) is infeasible for the function constraint \(g_{i}() 0\), \(1 i m\). If this assumption were not made, we would be dealing with an unconstrained optimization problem that would not depend on \(g()\). This assumption also implies that the optimal dual variables are non-zero, and as a result, the Lagrangian function is strongly convex with respect to \(\). By leveraging the strong convexity, we can use more aggressive step sizes and achieve faster convergence rates compared to other state-of-the-art algorithms.

Applications in sparsity-constrained optimizationWe consider the constrained Lasso-type problem, which minimizes a sparsity-inducing regularizer while explicitly ensuring data-driven error remains controlled:

\[_{^{n}}\ \|\|_{1}\ g( ) 0,\] (3)

where \(g()\) is a convex smooth loss term. A motivating application is the approximate personalized PageRank problem , where \(g()=,Q-,\) is strongly convex quadratic and \(Q\) integrates the graph Laplacian with an identity matrix. Compared to the standard Lasso problem , \(_{^{n}}g()+\|\|_{1}\), the constrained problem (3) offers enhanced control over the data fitting error. This advantage, however, is counterbalanced by the challenge of dealing with a nonlinear constraint. Besides concerns about the efficiency in solving (3), it is often desired to show the active set (or sparsity) identification, namely, the nonzero patterns of the optimal solution \(^{*}\) can be identified by the solution sequence \(\{_{k}\}\) in a finite number of iterations. Identifying the embedded solution structure within a broader context is referred to as the manifold identification problem . Exploiting the sparsity pattern is particularly desirable in large-scale PageRank problems, as it could result in significant runtime savings. For the regularized Lasso-type problem, it has been known that proximal gradient methods (e.g. ) possess the finite active-set identification property. Specifically,  introduced "active set complexity", which is defined as the number of iterations required before an algorithm is guaranteed to have reached the optimal manifold, and they proved the proximal gradient method with constant stepsize can identify the optimal manifold in a finite number of iterations. However, for the problem (3), it remains unclear whether first-order methods can identify the sparsity pattern in finite time.

ContributionsWe address the theoretical questions about strongly convex constrained optimization and the application of sparse optimization. Our contributions are summarized as follows.

First, we present a new accelerated primal-dual algorithm with progressive strong convexity estimation (APDPro) for solving problem (1). APDPro employs a novel strategy to estimate the lower bound of the dual variables, which leads to a gradually refined estimated strong convexity modulus of \((,)\). With additional cut constraints on the dual update, APDPro is able to separate the dual search space from the origin point, which is critical for maintaining the desired strong convexity over the entire solution path. With these two important ingredients, APDPro exhibits an \((\|_{0}-^{*}\|+D_{Y})/ \) complexity bound to obtain an \(\)-error on the function value gap and constraint violation, where \(D_{Y}\) is a known upper-bound of \(\|y_{0}-^{*}\|\). Moreover, we show that for the last iterate to have an \(\) error (i.e., \(\|_{K}-^{*}\|^{2}\)), APDPro requires a total iteration of \((\|_{0}-^{*}\|+\|_{0}-^{*}\|)/\). Both complexity results appear new in the literature for strongly convex-constrained optimization.

Second, we present a new restart algorithm (rAPDPro) which calls APDPro repeatedly with the input parameters properly changing over time. Different from APDPro, rAPDPro dynamically adjusts the iteration number of APDPro in each epoch based on the progressive strong convexity estimation. We show that rAPDPro exhibits a complexity of \((D_{X}/)+D_{Y}/\) to ensure \(\)-error in the last iterate convergence where \(D_{X}\) is the estimated diameter of the primal feasible domain. While it is difficult to improve the overall \((1/)\) bound, rAPDPro appears to be more advantageous when \(D_{X}\) and \(D_{Y}\) are the same order of \(\|_{0}-^{*}\|\) and \(\|_{0}-^{*}\|\), respectively, and \(D_{X} D_{Y}\). In addition, we show that a similar restart strategy can further accelerate the standard APD. The multistage-accelerated primal dual method (msAPD) obtains a comparable \((1/)\) complexity of APDPro without introducing additional cut constraint.

Third, we apply our proposed methods to the sparse learning problem (3). In view of the theoretical analysis, all our methods converge at an \((1/)\) rate, which is substantially better than the rates of state-of-the-art first-order algorithms. Moreover, we conduct a new analysis to show that the restart algorithm rAPDPro has the favorable feature of identifying the optimal sparsity pattern. Note that such active-set/manifold identification is substantially more challenging to prove due to the coupling of dual variables and constraint functions. To establish the desired property, we develop asymptotic convergence of the dual sequence to the optimal solution, which can be of independent interest.

OutlineSection 2 sets notations and assumptions for the later analysis. Section 3 presents the APDPro algorithm and develops its stepsize rule and complexity rate. Section 4 presents the restart APDPro (rAPDPro) algorithm. Section 5 applies our proposed methods for sparsity-inducing optimization and shows the sparsity identification result for rAPDPro. Section 6 empirically examine the convergence performance and sparsity identification of our proposed algorithms. Finally, we draw the conclusion in Section 7. All the missing proofs are provided in the appendix sections.

## 2 Preliminaries

We use bold letters like \(\) to represent vectors. Suppose \(^{n}\), \(q 1\), we use \(\|\|_{q}=(_{i=1}^{n}|_{(i)}|^{q})^{1/q}\) to represent the \(l_{q}\)-norm, where \(_{(i)}\) is the \(i\)-th element of \(\). For brevity, \(\|\|\) stands for \(l_{2}\)-norm. For a matrix \(A\), we denote the matrix norm induced by 2-norm as \(\|A\|=_{\|\| 1}\|A\|\). The normal cone of \(\) at \(\) is denoted as \(_{}():=\{, - 0,\}\). Let \((,r)\) be the closed ball centered at \(\) with radius \(r>0\), i.e., \((,r)=\{\|-\| r\}\). We denote the set of feasible solutions by \(_{G}:=\{ g_{i}() 0, i[m]\}\) and write the constraint function as \(G():=[g_{1}(),,g_{m}()]^{}\). We assume each \(g_{i}()\) is a \(_{i}\) strongly convex function, and denote \(:=[_{1},,_{m}]^{}\). Let \([m]:=\{1,,m\}\) for integer \(m\). We denote minimum and maximum strongly convexity \(:=_{j[m]}\{_{j}\}\), and \(:=_{j[m]}\{_{j}\}\) and the vector of elements \(0\) by \(\). The Lagrangian function of problem (1) is given by \((,):=f()+,G()\) where \(_{+}^{m}\).

**Definition 1** (KKT condition).: _We say that \(^{*}\) satisfies the KKT condition of (1) if there exists a Lagrangian multiplier vector \(^{*}_{+}^{m}\) such that \(_{x}(^{*},^{*})\) and \(^{*},G(^{*})=0\)._

The KKT condition is necessary for optimality when a constraint qualification (CQ) holds at \(^{*}\). We assume Slater's CQ (Assumption 1) holds, which guarantees that an optimal solution is also a KKT point .

**Assumption 1**.: _There exists a strictly feasible point \(}^{n}\) such that \(G(})<\)._We use \(}\) to denote a strictly feasible point throughout the paper. Moreover, we require Assumption 2 to circumvent any trivial solution.

**Assumption 2**.: _For any \(_{0}^{*}*{argmin}_{^{n}}f( )\), there exists an \(i[m]\) such that \(g_{i}(_{0}^{*})>0\)._

**Remark 1**.: _Assumption 2 is essential for our analysis. While verifying Assumption 2 can be indeed challenging, it is achievable for the sparsity-inducing problem considered in our paper. In this example, the solution \(_{0}^{*}=\) is the single minimizer of the sparsity penalty._

Next, we give several useful properties about the optimal solutions of problem (1). Please refer to Appendix D.1 for the proof of Proposition 1 and Appendix D.2 for the proof of Proposition 2.

**Proposition 1**.: _Suppose Assumption 1 holds. Then, for any optimal solution \(^{*}\) of problem (1), there exists \(^{*}^{m}\) such that KKT condition holds. Moreover, \(^{*}\) falls into set \(:=\{\ |\ \|\|_{1}\}\), where \(:=})-_{^{n}}f( )}{_{i[m]}\{-g_{i}(})\}}\)._

**Proposition 2**.: _Under Assumption 2, \(^{*}\) is the unique solution of (1). Furthermore, set \(^{*}=*{argmax}_{_{+}^{m}} (^{*},)\) is convex and bounded._

In view of Assumption 2, Proposition 2, and closedness of the subdifferential set of proper convex functions [2, Theorem 3.9], [27, Chapter 23], we know that \(( f(^{*}),)>0,\) where \(( f(^{*}),):=_{ f( ^{*})}\|\|.\) Furthermore, we make the following assumption:

**Assumption 3**.: _Throughout the paper, suppose that a constant \(r\) satisfying_

\[( f(^{*}),) r>0,\] (4)

_is known._

We give some important examples for which the lower bound \(r\) can be estimated. Suppose \(f()\) is a Lasso regularizer, i.e., \(f()=\|\|_{1}\), then \(r=1\) satisfies (4). More general, consider the group Lasso regularizer, i.e., \(f()=_{i=1}^{B}p_{i}\|_{(i)}\|\), where \(_{(i)}^{b_{i}}\) and \(_{i=1}^{B}b_{i}=n\), \(B\) is the number of blocks, then \(r=_{i[B]}\{p_{i}\}\) when \(^{*}\). Another example is \(f()=^{}\), then we have \(r=\|\|\).

**Remark 2**.: _Condition (4) is similar to the bounded gradient assumption that has been used for accelerating the convergence of the Frank-Wolfe algorithm. See Appendix B for more discussions._

When considering the Lipschitz continuity of function in \(^{n}\), even quadratic functions are not Lipschitz continuous. However, the Lipschitz continuity of \(g_{i}(x)\) is crucial for algorithm convergence. Therefore, we define the bounded feasible region in the following proposition, with its proof provided in Appendix D.3.

**Proposition 3**.: _Let \(:=},_{i[m]}2(_{i}^{*})}{_{i}}}\), where \(_{i}^{*}=*{argmin}_{^{n}}g_{i}( )\). Then under Assumptions 1 and 2, we have \(^{*}\ \)._

**Assumption 4**.: _There exist \(L_{X},L_{G}>0\) such that_

\[\| G()- G(})\|  L_{X}\|-}\|,\ \ ,},\] (5) \[\|G()-G(})\|  L_{G}\|-}\|,\ \ ,},\] (6)

_where \( G():=[ g_{1}(),, g_{m}()]^{n m}\) and \(\) is defined in Proposition 3._

The Lipschitz smoothness of the Lagrangian function with respect to the primal variable \(\) is crucial for the convergence of algorithms. Given that the dual variable \(\) is bounded from above, and considering the smoothness of the constraint functions, we can derive the smoothness of the Lagrangian function. Combining (5) and the fact \(\|\|\|\|_{1},\), we obtain that

\[\| G()- G(})\| L _{XY}\|-}\|\ \ ,},\ ,\] (7)

where \(L_{XY}=L_{X}\). For set \(\), \(\), we use \(D_{X}\) and \(D_{Y}\) to denote their diameters, respectively, i.e., \(D_{X}:=_{_{1},_{2}}\|_{1}- _{2}\|\) and \(D_{Y}:=_{_{1},_{2}}\|_{1}- _{2}\|\).

## 3 APD with progressive strong convexity estimation

We present the Accelerated Primal-Dual Algorithm with Progressive Strong Convexity Estimation (APDPro) to solve problem (1). For problem (1), APDPro achieves the improved convergence rate \((1/)\) without relying on the uniform strong convexity assumption [11; 22]. For the rest of this paper, we denote \(_{f,}(-,):=_{}}\,f(})+, }+\|}-\|^{2}\) as the proximal mapping.

We describe APDPro in Algorithm 1. The main component of APDPro contains a dual ascent step to update \(_{k}\) based on the extrapolated gradient, followed by a primal proximal step to update \(_{k}\). Compared with standard APD , APDPro has two more steps. First, line 4 of Algorithm 1 applies a novel cut constraint to separate the dual sequence \(\{_{k}\}\) from the origin, which allows us to leverage the strong convexity of the Lagrangian function and hence obtain a faster rate of convergence than APD. Second, to use the strong convexity more effectively, in line 9, we perform a progressive estimation of the strong convexity by using the latest iterates \(_{k}\) and \(}_{k}\). Throughout the algorithm process, we use a routine Improve to construct a non-decreasing sequence \(\{_{k}\}\), which provides increasingly refined lower bounds of the strong convexity of the Lagrangian function.

```
0:\(_{0}>0,_{0}>0\), \(_{0},_{0},_{0} 0,N>0\)
1:Initialize:\((_{-1},_{-1})(_{0}, _{0}),}_{0}_{0},_{-1 }_{0},T_{0}=0\)
2:Set \(_{XY}=}D_{X}^{2}+}D_{Y}^{2}\)
3:for\(k=0,1,,N\)do
4:\(_{k}\{_{+}^{m}\,\,\|\,\| \|_{1}_{k}\}\),
5:\(_{k}(1+_{k-1}/_{k})G(_{k-1}/ _{k})G(_{k-1})\)
6:\(_{k+1}_{_{k}}\,\| -(_{k}+_{k}_{k})\|^{2}\)
7:\(_{k+1}_{f,}(_{k}-_{ k} G(_{k})_{k+1},_{k})\)
8: Compute \(t_{k}\), \(}_{k+1}(T_{k}}_{k}+t_{k}_{k+ 1})/(T_{k}+t_{k})\), \(T_{k+1} T_{k}+t_{k}\)
9: Update \(_{k+1}(_{k},}_{k},}_{k},_{k}-_{XY}}{_{k-1}},}{T_{k}},_{k})\)
10: Update \(_{k+1}\) and \(_{k+1}\) depending on \(_{k+1}\)
11:endfor
12:Output:\(_{N+1},_{N+1}\)
13:procedureImprove(\(\), \(}\), \(\), \(\), \(_{}\))
14: Compute \(=\{r\| G()\|+L_{X} ^{-1},}{r}}+ ^{2}}{2 r^{2}}+ })\|}{r}}^{-2}\}\)
15: Set \(_{}=\{_{},\}\)
16:return\(_{}\)
17:endprocedure ```

**Algorithm 1** Accelerated Primal-Dual Algorithm with Progressive Strong Convexity Estimation (APDPro)

**The Improve step** In order to estimate the strong convexity of the Lagrangian function, we rely on the subdifferential separation (eq. (4)) to bound the dual variables. From the first-order optimality condition in minimizing \((,^{*})\) and the fact that \(^{*}\,\) (Proposition 3), we have \( f(^{*})+ G(^{*})^{*} +_{}(^{*})= f(^{*})+ G (^{*})^{*}\). It follows from (4) that

\[r\| G(^{*})^{*}\|\| G(^{*}) \|\|^{*}\|\|^{*}\|_{1}\| G(^{*})\|,\] (8)

where the last inequality use the fact that \(\|\|\|\|_{1}\). Note that the bound \(\|^{*}\|_{1} r/\| G(^{*})\|\) can not be readily used in the algorithm implementation because \(^{*}\) is generally unknown. To resolve this issue, we develop more concrete dual lower bounds by using the generated solution \(}\) in the proximity of \(^{*}\). As we will show in the analysis, APDPro keeps track of two primal sequences \(\{_{k}\}\) and \(\{}_{k}\}\), for which we can establish bounds on \(\|_{k}-^{*}\|^{2}\) and \((^{*})^{}\|}-^{*}\|^ {2}/2\), respectively. This drives us to develop the following lower bound property, with the proof provided in Appendix E.1.

**Proposition 4**.: _Suppose Assumption 4 holds. Let \(^{*}^{*}\) be a dual optimal solution._

1. _Suppose that_ \(\|}-^{*}\|^{2} 2\)_, then we have_ \[\|^{*}\|_{1} h_{1}(},):=r\| G( })\|+L_{X}^{-1}.\] (9)2. _Suppose_ \((^{*})^{}\|}-^{*}\|^{2}  2\)_, then we have_ \[\|^{*}\|_{1} h_{2}(},):=[}{r }}+^{2}}{2 r^{2}}+})\|}{r}}]^{-2}.\] (10)

Our next goal is to conduct the convergence analysis for APDPro in Theorem 1 and Corollary 1. Complete proof details are provided in Appendix E.2 and E.3.

**Theorem 1**.: _Suppose for any \(^{*}^{*}\), \((^{*})^{}_{0}\) holds, and let the sequence \(\{_{k},_{k},t_{k},_{k+1}\}\) generated by Algorithm 1 satisfy:_

\[t_{k+1}(_{k+1}^{-1}-_{k+1}) t_{k}_{k}^{-1}, t_{k+1} _{k+1}^{-1} t_{k}_{k}^{-1}, L_{XY}+L_{G}^{2}_{k}_ {k}^{-1}.\] (11)

_Then, the set \(_{k}\) is nonempty and \(^{*}_{k}\). Let \((,):=}\|-_{ 0}\|^{2}+}\|-_{0}\|^{2},}_{K}=T_{K}^{-1}_{s=0}^{K-1}t_{s}_{s}\). The sequence \(\{}_{k},_{k},}_{k}\}\) generated by APDPro satisfies_

\[_{K-1}^{-1}}{2T_{K}}\|^{*}-_{K}\|^{2}+ (}_{K},^{*})-(^{*}, }_{K})}(^{*},^{*}).\] (12)

Next, we develop more concrete complexity results in Corollary 1.

**Corollary 1**.: _Suppose that \(_{k},_{k},t_{k}\) satisfy:_

\[_{0}^{-1} L_{XY}+L_{G}^{2}_{0},\ t_{k}=_{k}/ _{0},\] (13) \[_{k+1}=_{k}/_{k}},\ _{k+1}= _{k}_{k}/_{k+1}\]

_Then we have_

\[f(}_{K})-f(^{*}) _{K}(K+1)K}}\|_{0}-^{*}\|^{2}+^{2}}{2_{0}} ,\] \[\|[G(}_{K})]_{+}\| (6+_{0}_{K}(K+1)K)} {1}{2_{0}}\|_{0}-^{*}\|^{2}+^{2}}{2_ {0}},\] (14) \[\|_{K}-^{*}\|^{2} }{_{K}^{2}_{0}^{2}K^{2}+9( _{0}/_{0})}(^{*},^{*}).\]

_where \(c^{*}:=f(^{*})-_{}f()/_{i [m]}\{-g_{i}(})\}>0\), \(_{k}=2_{s=0}^{k}_{s}s/k(k+1)\) and \(_{k}\) satisfy the following condition, \(_{k+1}:=_{k}^{2}k^{2}+(3_{k+1}_{k} )}k/(k+1),_{1}=3/_{0}}\)._

**Remark 3**.: _In view of Corollary 1, APDPro obtains an iteration complexity of \((1/_{K}})\), which is substantially better than the \((1/)\) bound of APD  and ConEx  when the strong convexity parameter \(_{K}\) is relatively large compared with \(\)._

**Remark 4**.: _Additionally, we argue that even when \(_{K}=O()\), APDPro can obtain the matching \((1/)\) bound of the state-of-the-art algorithms. Specifically, using the definition of \(_{k},_{k}\), we can easily derive the monotonicity of \(\{_{k}\}\). It follows from \(_{k+1}=_{k}_{k}/_{k+1}=_{k}_{k}/(_{k}/_{k}})_{k},\) that \(T_{k}=_{s=0}^{k-1}t_{k}=_{0}^{-1}_{s=0}^{k-1}_{k} k\). Using a similar argument to that of Corollary 1, we obtain the bound \(f(}_{K})-f(^{*})(1/K)\) and \(\|[G(}_{K})]_{+}\|(1/K)\)._

**Remark 5**.: _The implementation of APDPro requires knowing an upper bound on \(\|^{*}\|\). When the bound is unavailable,  developed an adaptive APD which still ensures the boundedness of dual sequence via line search. Since our main goal of this paper is to exploit the lower-bound rather than the upper bound of \(\|^{*}\|\), we leave the extension for the future work._

## 4 APDPro with a restart scheme

Note that in the worst case, APDPro exhibits an iteration complexity of \((D_{X}+D_{Y})/\), which has a linear dependence on the diameter. While the \((1/)\) is optimal , it is possible to improve the complexity with respect to the primal part from \(D_{X}/\) to \(D_{X}/\). To achieve this goal, we propose a restart scheme (rAPDPro) that calls APDPro repeatedly and present the details in Algorithm 2. Inspired by , we set the iteration number as a function of the estimated strong convexity, detailed in the TerminateIter procedure. For convenience in describing a double-loop algorithm, we use superscripts for the number of epochs and subscripts for the number of sub-iterations in parameters \(,,,\), e.g., \(_{1}^{S}\) meaning the \(\) output of first iterations at \(S\)-th epoch. To avoid redundancy in the Algorithm 2, we call the APDPro iteration directly. Note that the notation system here is identical to that of APDPro, with the only difference being the use of superscripts to distinguish the number of epochs.

```
0:\(_{N_{-1}}^{-1} 0,>0\), \(_{0}(0,1)\), \((0,1)\), \(_{N_{-1}}^{-1},_{N_{-1}}^{-1},S\)
1: Compute \(=(1-_{0})L_{XY}+L_{G}^{2}/^{-1}\)
2:for\(s=0,1,,S\)do
3:\(_{0}^{s}=,_{0}^{s}=,(_{s-1}^{s}, _{N_{-1}}^{s})(_{N_{s-1}}^{s-1},_{N_{ -1}}^{s-1}),(_{0}^{s},_{0}^{s})(_{N_{s -1}}^{s-1},_{N_{s-1}}^{s-1}),_{0}^{s}=_{N_{s-1}}^{s-1}\)
4: Set \(_{XY}=^{s}}D_{X}^{2}+^{s}}D_{Y}^{2},_{-1}^{s}_{0}^{s},T_{0}^{s}=0,k=0,_{0}^{s}=1, N_{s}=\)
5:while\(k<N_{s}\)do
6: Run line 4-10 of APDPro with index set \((s,k)\)
7: Update \(N_{s},_{k+1}^{s}\)TerminateIter\((_{k}^{s},_{k+1}^{s},s,k)\), \(k k+1\)
8:endwhile
9:endfor
10:Output:\(_{N_{S}}^{S},_{N_{S}}^{S}\)
11:procedureTerminateIter(\(_{},,s,k\))
12: Compute \(_{}=_{}^{2}k^{2}+3_{}k}&k>1\\ 3}&k=1\)
13: Compute \(N=\{6(_{}_{0}^{s})^{-1},^{s} 3 D_{Y}/_{}D_{X}^{s}_{0}^ {s}}\}\)
14:return\(N,_{}\)
15:endprocedure ```

**Algorithm 2** Restarted APDPro (rAPDPro)

In Theorem 2, we show the overall convergence complexity of rAPDPro with the proof provided in Appendix F.1.

**Theorem 2**.: _Let \(\{_{0}^{s}\}_{s 0}\) be the sequence generated by rAPDPro, then we have_

\[\|_{0}^{s}-^{}\|^{2}_{s} D_{X}^{2}  2^{-s}, s 0.\] (15)

_As a consequence, rAPDPro will find a solution \(_{0}^{S}\) such that \(\|_{0}^{S}-^{}\|^{2}\) for any \((0,D_{X}^{2})\) in at most \(S:=_{2}(D_{X}^{2}/)\) epochs. Moreover, The iteration number of rAPDPro to find \(_{0}^{S}\) such that \(\|_{0}^{S}-^{}\|^{2}\) is bounded by_

\[T_{}:=_{0}^{s}}+2\, _{2}}{}+1++2) }{_{2}^{s}_{0}^{s}}} }{},\] (16)

_where \(_{1}\) and \(_{2}\) satisfy \(_{s=0}^{S}(_{N_{s}}^{s})^{-1}=(_{1})^{-1}(S+1)\) and \(_{s=0}^{S}^{s}/_{N_{s}}^{s}=(_{2})^{-1}_{s=0}^ {S}^{s}\), respectively._

**Remark 6**.: _The bound \(T_{}\) depends on \(\), \(_{1}\) and \(_{2}\). If \(_{1}=O(-_{2})^{-1}\) or \(_{2}=O()\), then we have \(T_{}=\), which implies that we can not guarantee \(\|_{0}^{s}-^{}\|\) at finite iterations. \(T_{}=\) implies that there exists an epoch with infinite sub-iterations. Hence, rAPDPro is reduced to APDPro if we only consider that epoch._

**Remark 7**.: _Comparison of rAPDPro and_ APDPro _involves a number of factors. In particular,_ rAPDPro _compares favorably against_ APDPro _if \(\|_{0}-^{}\|=( D _{X})\). Moreover, the complexity (16) can be slightly improved if \(D_{X}\) is replaced by any tighter upper bound of \(\|_{0}^{s}-^{}\|\). However, it is still unknown whether we can directly replace \(D_{X}\) with \(\|_{0}^{s}-^{}\|\) in (16)._

**Dual Convergence** For dual variables, we establish asymptotic convergence to the optimal solution, a key condition for developing the active-set identification in the later section. For ease in notation, it is more convenient to label the generated solution as a whole sequence using a single subscript index: \(_{1},_{2},,_{N};_{1},_{2}, ,_{N}\). Hence, we use the index system \(j\) and \((s,k)\) interchangeably. Note that \(\{_{0}^{s+1},_{0}^{s+1}\}\) and \(\{_{N_{s}+1}^{s},_{N_{s}+1}^{s}\}\) correspond to the same pair of points. We present the dual asymptotic result in the following theorem, with the proof provided in Appendix F.2.

**Theorem 3**.: _Assume \(^{-1}>\) and choose \(_{0}>0\) such that \(1>_{j 0}\{_{j-1}/_{j}\}+_{0}\). We have \((^{},^{})\) satisfy the KKT condition, where \(^{}\) is any limit point of \(\{_{j}\}\) generated by_ rAPDPro_._

**Remark 8**.: _To establish the asymptotic convergence of the dual variable, we introduce an additional constant \((0,1)\), which implies that the initial step size must meet a stricter requirement than the convergence condition specified in Corollary 1. Since \(_{k}^{s}/_{k-1}^{s}=^{s}_{k}^{s}}\), \(\{_{k}^{s}\}\) is bounded due to the boundedness of the dual variable, \(\{_{k}^{s}\}\) is monotonically decreasing, then \(_{0 k N_{s}}\{_{k-1}^{s}/_{k}^{s}\}(1+)^{-1/2}\). Hence, inequality, \(1>_{j 0}\{_{j-1}/_{j}\}+_{0}\), is always satisfiable if we choose proper \(,_{0}\) such that \((1+)^{-1/2}+_{0}\). Furthermore, Assumption \(()^{-1}>\) is mild. Since we always choose \(\) large enough in \(\), \(\) can be sufficiently small._

**Remark 9**.: _Both algorithms proposed previously require solving quadratic optimization with linear constraints when updating dual variables, which may introduce implementation overheads when the constraint number is high. Inspired by the multi-stage algorithm, we additionally propose an algorithm (Multi-Stage APD, msAPD) that uses different step sizes in different stages and dynamically adjusts the number of iterations in each stage by leveraging strong convexity, as detailed in Appendix H._

## 5 Active-set identification in sparsity-inducing optimization

In this section, we apply our proposed algorithms to the aforementioned sparse learning problem:

\[ f(),\;\;g() 0,\;= _{(1)}_{(B)},\;_{(i)}^{n_{ i}},1 i B,\] (17)

where \(f()=_{i=1}^{B}p_{i}\|_{(i)}\|\) is the group Lasso regularizer and \(g()\) is a strongly convex function. We use \(_{(i)}\) to express the \(i\)-th block coordinates of \(\). The goal of this section is to show that \(\) can identify the sparsity pattern of the optimal solution of (17) in a finite number of iterations.

In general, suppose that \(f()\) has a separable structure \(f()=_{i=1}^{B}f_{i}(_{(i)})\), we define the active set \(()\) for \(f()\) by \(():=\{i: f_{i}(_{(i)})\}\). For \(f()=_{i=1}^{B}p_{i}\|_{(i)}\|\), it is easy to see that \(()\) is the index set of the zero blocks: \((^{*})=i:_{(i)}^{*}=}\). Next, we describe one property for the optimal solution of (17) in Proposition 5 with the proof provided in Appendix G.1.

**Proposition 5**.: _Under Assumptions 1 and 2, the KKT point for (17) is unique._

To identify the sparsity pattern (active set) of the optimal solution, it is common to assume the existence of a non-degenerate optimal solution, which is stronger than the standard optimality condition [24; 29]. We say that \(^{*}\) is non-degenerate if \(\,(^{*},^{*})= ( f(^{*})+ g(^{*})^{*})\) for the Lagrangian multiplier \(^{*}\), where \(\) stands for the relative interior. More specifically, \((^{*},^{*})\) satisfies the block-wise optimality condition

\[-[ g(^{*})^{*}]_{(i)}= f_{i}( _{(i)}^{*}),&\,i(^{*}),\\ -[ g(^{*})^{*}]_{(i)}( f _{i}(_{(i)}^{*})),&\,i(^{*}).\]

Inspired by , we use the radius \(:=_{i(^{*})}p_{i}-\|[ g(^{*})^{*}]_{(i)}\|}\), which describes the certain distance between the gradient and "subdifferential boundary" of the active set. We demonstrate in the following theorem that the optimal sparsity pattern is identified when the iterates fall in a neighborhood dependent on \(\), with the proof provided in Appendix G.2.

**Theorem 4**.: _Set \(:=},_{i[m]}2(_{i}^{*})}{_{i}}}+\) with \(>0\) and \(3L_{XY}(+(2L_{XY})^{-1})>\) in \(\), then we have there exists a epoch \(_{0}\) such that \(_{(i)}^{*}=_{k(i)}^{s},s_{0},\; k[N_{ s}],\; i(^{*})\)._

**Remark 10**.: _The active-set identification result is achieved using the optimality condition at the next iterate \(_{i}^{k+1}\). To ensure \(_{i}^{k+1}\;\), we define an expanded region, which prevents cases where the normal cone differs from \(\{\}\)._

## 6 Numerical study

In this section, we examine the empirical performance of our proposed algorithms for solving the sparse Personalized PageRank [8; 9; 23]. The constrained form of Personalized PageRank can bewritten as follows: \(_{^{n}}\ \ \|D^{1/2}\|_{1}\ \ \  ,Q-,D^{-1/2}  b\), where \(Q,D\) and \(\) are generated by graph. We implement both rAPDPro and msAPD. We skip APDPro as we observe that the restart strategy consistently improves the algorithm performance. For comparison, we consider the state-of-the-art accelerated primal-dual (APD) method , APD with restart mechanism at fixed iterations (APD+restart) and Mirror-Prox . 6 small to medium-scale datasets from various domains in the Network Datasets  are selected in our experiments. All experiments are implemented on Mac mini M2 Pro, 32GB. Due to the page limit, we only report results on three datasets and leave more details in the last Appendix I.

We plot the relative function value gap \(|f()-f(^{*})|/|f(^{*})|\) and the feasibility violation \(\{G(),0\}\) over the iteration number in Figure 1, respectively. Firstly, in terms of both optimality gap and constraint violation, the performance of rAPDPro and msAPD is significantly better than that of APD, APD+restart and Mirror-Prox. Additionally, rAPDPro and msAPD often converge to high-precision solutions. Secondly, based on the experimental results, it is indeed observed that msAPD exhibits a periodic variation in convergence performance, which aligns with our algorithm theory.

Next, we examine the algorithm's effectiveness in identifying sparsity patterns. We computed a nearly optimal solution \(^{*}\) from MOSEK. Note that \(^{*}\) is a dense vector. For numerical consideration, we truncate the coordinate values of \(^{*}\) to zero if the absolute value is below \(10^{-8}\) and perform the same truncation to all the generated solutions of the compared algorithms. Then we use

Figure 2: The experimental results on active-set identification. Datasets (Left-Right order) correspond to bio-CE-HT, bio-CE-LC and econ-beaflw. The \(x\)-axis reports the iteration number and the \(y\)-axis reports accuracy in active-set identification.

\((^{*})|+[^{c}()^{c}( ^{*})])/n\) to measure the accuracy of identifying the active set, where \(||\) denotes the set cardinality. For rAPDPro, we consider the last iterate \(_{k}\) while for APD, msAPD and Mirror-Prox, we plot the result on \(}_{k}\), as these are the solutions where the convergence rates are established. Figure 2 plots the experiment result, from which we observe that rAPDPro and msAPD are highly effective in identifying the active set. Often, they are able to recognize the structure of the active set within a small number of iterations. Overall, the experimental results show the great potential of our proposed algorithms in identifying the sparsity structure and are consistent with our theoretical analysis.

## 7 Conclusion

The key contribution of this paper is that we develop several new first-order primal-dual algorithms for convex optimization with strongly convex constraints. Using some novel strategies to exploit the strong convexity of the Lagrangian function, we substantially improve the best convergence rate from \((1/)\) to \((1/)\). In the application of constrained sparse learning problems, the experimental study confirms the advantage of our proposed algorithms against state-of-the-art first-order methods for constrained optimization. Moreover, we show that one of our proposed algorithms rAPDPro has the favorable feature of identifying the sparsity pattern in the optimal solution. For future work, one direction is to apply the adaptive strategy, such as line search, to our framework to deal with cases when the dual bound is unavailable. Another interesting direction is to further exploit the active set identification property in a general setting. For example, it would be interesting to incorporate our algorithm with active constraint identification, which could be highly desirable when there are a large number of constraints. It would also be interesting to consider a more general convex objective when the proximal operator is not easy to compute.