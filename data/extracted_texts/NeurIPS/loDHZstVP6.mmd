# FinBen: A Holistic Financial Benchmark for Large Language Models

Qianqian Xie\({}^{b,a}\), Weiguang Han\({}^{b}\), Zhengyu Chen\({}^{b}\), Ruoyu Xiang\({}^{a}\), Xiao Zhang\({}^{a}\), Yueru He\({}^{a}\), Mengxi Xiao\({}^{b}\), Dong Li\({}^{b}\), Yongfu Dai\({}^{g}\), Duanyu Feng\({}^{g}\), Yijing Xu\({}^{a}\), Haoqiang Kang\({}^{c}\), Ziyan Kuang\({}^{l}\), Chenhan Yuan\({}^{c}\), Kaihai Yang\({}^{c}\), Zheheng Luo\({}^{c}\), Tianlin Zhang\({}^{c}\), Zhiwei Liu\({}^{c}\), Guojun Xiong\({}^{j}\), Zhiyang Deng\({}^{i}\), Yuchen Jiang\({}^{i}\), Zhiyuan Yao\({}^{i}\), Haohang Li\({}^{i}\), Yangyang Yu\({}^{i,*}\), Gang Hu\({}^{h}\), Jiajia Huang\({}^{k}\), Xiao-Yang Liu\({}^{c,*}\), Alejandro Lopez-Lira\({}^{d,*}\), Benyou Wang\({}^{f}\), Yanzhao Lai\({}^{m}\), Hao Wang\({}^{g}\), Min Peng\({}^{b,*}\), Sophia Ananiadou\({}^{c,*}\), Jimin Huang\({}^{a,*}\)

The Fin AI, \({}^{b}\)Wuhan University, \({}^{c}\)The University of Manchester, \({}^{d}\)University of Florida,

\({}^{e}\)Columbia University, \({}^{f}\)The Chinese University of Hong Kong, Shenzhen,

\({}^{g}\)Sichuan University, \({}^{h}\)Yunnan University, \({}^{i}\)Stevens Institute of Technology

\({}^{j}\)Stony Brook University, \({}^{k}\)Nanjing Audit University,

\({}^{l}\)Jiangxi Normal University, \({}^{m}\)Southwest Jiaotong University

###### Abstract

LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of comprehensive benchmarks, the rapid development of LLMs, and the complexity of financial tasks. In this paper, we introduce FinBen, the first extensive open-source evaluation benchmark, including \(42\) datasets spanning \(24\) financial tasks, covering eight critical aspects: information extraction (IE), textual analysis, question answering (QA), text generation, risk management, forecasting, decision-making, and bilingual (English and Spanish). FinBen offers several key innovations: a broader range of tasks and datasets, the first evaluation of stock trading, novel agent and Retrieval-Augmented Generation (RAG) evaluation, and two novel datasets for regulations and stock trading. Our evaluation of \(21\) representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals several key findings: While LLMs excel in IE and textual analysis, they struggle with advanced reasoning and complex tasks like text generation and forecasting. GPT-4 excels in IE and stock trading, while Gemini is better at text generation and forecasting. Instruction-tuned LLMs improve textual analysis but offer limited benefits for complex tasks such as QA. FinBen has been used to host the first financial LLMs shared task at the FinNLP-AgentScen workshop during IJCAI-2024, attracting \(12\) teams. Their novel solutions outperformed GPT-4, showcasing FinBen's potential to drive innovations in financial LLMs. All datasets and code are publicly available for the research community2, with results shared and updated regularly on the Open Financial LLM Leaderboard3.

3

[FIGURE

## 1 Introduction

Recently, Large Language Models (LLMs) (Brown et al., 2020) such as ChatGPT4 and GPT-4 (OpenAI, 2023), have reshaped the field of natural language processing (NLP) and exhibited remarkable capabilities in specialized domains across mathematics, coding, medicine, law, and finance (Bubeck et al., 2023). Within the financial domain, recent several studies (Xie et al., 2023; Lopez-Lira and Tang, 2023; Li et al., 2023; Xie et al., 2023; Liu et al., 2023; Yang et al., 2023; Xie et al., 2024) have shown the great potential of LLMs such as GPT-4 on financial text analysis and prediction tasks. While their potential is evident, a comprehensive understanding of their capabilities and limitations for finance remains largely unexplored. This is due to a lack of extensive evaluation studies and benchmarks, and the inherent complexities associated with the professional nature of financial tasks.

Existing financial domain evaluation benchmarks, including PIXIU (Xie et al., 2023b), FinanceBench (Islam et al., 2023) and BizBench (Koncel-Kedziorski et al., 2023), have **limited evaluation tasks** and primarily **focus on financial NLP tasks**, as shown in Table 1. Most existing benchmarks cover only a small number of evaluation tasks and are centered on NLP capabilities, such as information extraction (IE) and question answering (QA) (Huang et al., 2024; Liu et al., 2024; Hu et al., 2024; Yang et al., 2024; Zhao et al., 2024, 2024, 2024). While PIXIU stands out by covering the highest number of tasks, it includes only one evaluation task in most categories. This narrow focus limits their ability to comprehensively evaluate LLMs across the diverse and complex landscape of financial applications, such as forecasting, risk management, and decision-making. It is insufficient for a thorough evaluation of LLM capabilities, especially in the financial area.

To bridge this gap, we propose FinBen, a novel comprehensive open-source evaluation benchmark developed through the collaborative efforts of experts in both computer science and finance. As shown in Figure 1, FinBen comprises \(42\) datasets spanning \(24\) financial tasks, meticulously organized to assess LLMs across eight critical aspects: information extraction (IE), textual analysis (TA), question answering (QA), text generation (TG), risk management (RM), forecasting (FO), decision-making (DM), and bilingual (English and Spanish). Each category targets specific skills of financial data processing and analysis, ensuring a thorough evaluation of LLMs and showcasing their proficiency in managing complex financial scenarios.

FinBen introduces several innovations over existing benchmarks: 1) **New tasks**: FinBen introduces a significantly larger number of tasks and datasets, making it the most holistic benchmark for financial LLMs with the highest number of tasks and datasets. This extensive range provides a more robust evaluation of LLM capabilities in diverse financial contexts. 2) **Broader coverage**: Covering eight aspects of the financial sector, FinBen is the first benchmark to include the evaluation of stock trading, which is the fundamental task in the financial sector, involving complex decision-making processes that impact market dynamics and investment strategies. 3) **New evaluation strategy**: FinBen is the first benchmark to include agent-based evaluation and retrieval-augmented generation (RAG)

  
**Benchmark** & **Language** & **Dataset Task IE** & **TA** & **QA** & **TG** & **RM** & **FO** & **DM** & **SP** \\  CFBenchmark & Chinese & 8 & 7 & 1 & 3 & 3 & **X** & **X** & **X** \\ Fin-Eva & Chinese & 1 & 1 & **X** & **X** & 1 & **X** & **X** & **X** \\ PIXIU & English & 15 & 8 & 1 & 3 & 1 & 1 & 1 & **X** \\ FinanceBench & English & 1 & 1 & **X** & 1 & **X** & **X** & **X** & **X** \\ BizBench & English & 8 & 5 & 2 & **X** & 2 & 1 & **X** & **X** & **X** \\ FinBen & English, Spanish & 42 & 24 & 6 & 8 & 3 & 1 & 4 & 1 & 4 \\   

Table 1: Comparison of different financial benchmarks based on the number of tasks and datasets and the task counts across aspects: information extraction (IE), textual analysis (TA), question answering (QA), text generation (TG), risk management (RM), forecasting (FO), decision-making (DM), and spanish (SP).

Figure 1: FinBen’s evaluation datasets with sizes ranging from \(100\) to \(4,000\).

based evaluation. These innovative strategies provide a more dynamic and realistic assessment of LLMs, reflecting their ability to interact with and retrieve relevant information from vast datasets. \(4\)) **Novel datasets**: FinBen proposes two novel open-source datasets of QA and stock trading tasks for the research community, pushing the boundaries of what LLMs can achieve and setting a new standard for dataset comprehensiveness. 5) **Empowering financial LLMs research**: Leveraging financial tasks in FinBen, we hosted the first shared task (see Appendix G for details) focused on financial LLMs at the FinNLP-AgentScen workshop during IJCAI-2024 5. This event attracted 12 teams, leveraging our benchmark to develop novel LLMs-based solutions within the financial domain. Remarkably, the proposed methods achieved superior performance compared to GPT-4, demonstrating the benchmark's potential to foster innovations and advance the state-of-the-art (SOTA) in financial LLMs.

Based on FinBen, we assess \(21\) representative general LLMs such as GPT-4, ChatGPT, and the latest Gemini, and financial LLMs, and have the following findings: 1) **Superior Capabilities with Limitations**: While LLMs exhibit exceptional prowess in IE and textual analysis tasks, they underperform in areas necessitating advanced reasoning and complex IE, such as text generation and forecasting. 2) **Potential in Stock Trading**: SOTA LLMs have demonstrated considerable promise in stock trading applications. However, there remains significant room for improvement due to their limitations in reasoning and comprehensive forecasting abilities. 3) **Closed-Source Superiority**: Closed-source commercial LLMs continue to lead in performance within the financial domain. Specifically, GPT-4 excels in IE, text analysis, QA, and intricate stock trading tasks, while Gemini shows superior capabilities in text generation and forecasting. 4) **Open-Source Improvements and Limitations**: While open-source, instruction-tuned financial LLMs have shown notable enhancements in textual analysis and IE tasks, the advantages of instruction-tuning are less pronounced when it comes to complex tasks such as QA, text generation, and forecasting.

In summary, the main contributions of this paper are: 1) we present FinBen, the first comprehensive open-sourced evaluation benchmark for LLMs in the financial domain, 2) we utilize a novel taxonomy covering eight aspects for organizing financial evaluation tasks, 3) we develop two novel evaluation datasets for the research community, and 4) we conduct systematic evaluation of \(21\) LLMs using FinBen, showcasing their advantages and limitations and highlighting directions for future work.

## 2 FinBen

In this section, we delve into the specifics of FinBen, detailing the evaluation taxonomy, data sources, and evaluation tasks.

### The Taxonomy of Financial Evaluation Tasks

In the dynamic landscape of financial technology, evaluating the capabilities of LLMs necessitates a comprehensive and structured approach. We propose a novel taxonomy for financial evaluation tasks, categorizing and assessing LLMs across eight financial domains inspired by established taxonomies in financial tasks (Cao, 2022; Li et al., 2023b; Zhao et al., 2024b): **Information Extraction (IE)**, **Textual Analysis (TA)**, **Question Answering (QA)**, **Text Generation (TG)**, **Risk Management (RM)**, **Forecasting (FO)**, **Decision-Making (DM)**, and **Spanish (SP)**. **Information Extraction** focuses on identifying key entities and relationships within financial documents, transforming unstructured data into structured insights (Costantino and Coletti, 2008). **Textual Analysis** delves into content and sentiment analysis of financial texts, aiding in market trend understanding (Loughran and McDonald, 2020). **Question Answering** evaluates the model's ability to comprehend and respond to financial queries (Maia et al., 2018). **Text Generation** assesses the production of coherent financial text (La Quatra and Cagliero, 2020). **Risk Management** involves evaluating creditworthiness, detecting fraud, and ensuring regulatory compliance (Aziz and Dowling, 2019). **Forecasting** predicts future financial trends, enabling strategic responses to market dynamics (Abu-Mostafa and Atiya, 1996). **Decision-Making** assesses the model's proficiency in making informed financial decisions, such as developing trading strategies and optimizing investment portfolios (Paiva et al., 2019). Finally, **Spanish** evaluates the model's capabilities in other languages except for English, particularly in low-resource languages.

### Data Sources

FinBen's evaluation tasks are drawn from three primary data sources: 1) open-sourced datasets from existing studies originally released for non-LLM evaluation settings. Domain experts have designed diverse prompts and reformulated these datasets into instruction-response pairs, making them suitable for evaluating the zero-shot performance of LLMs. 2) datasets from existing evaluation benchmarks such as PIXIU. These datasets have already been transformed into the instruction tuning format, allowing for seamless integration and direct use in FinBen. 3) novel datasets introduced in this paper. These datasets are designed to address gaps in existing benchmarks and provide unique challenges for financial LLMs evaluation. Novel datasets include (As shown in Table 2):

**FinTrade**. The FinTrade dataset is developed specifically for stock trading tasks, integrating historical stock prices, filings data, and news data for 10 stocks over a one-year period. It provides a robust foundation for evaluating LLMs in agent-based financial trading scenarios. The dataset is composed of three main components6: (1) **Stock Price Data**: Historical price data for 497 trading days, obtained via the yfinance API from Yahoo Finance, includes OHLCV (open, high, low, close, adjusted close price, and volume) metrics. Adjusted close prices are used to maintain consistency in the return series, minimizing the impact of corporate actions like dividends and stock splits. (2) **Filings Data**: Summary sections from Form 10-Q (quarterly reports) and Form 10-K (annual reports) are retrieved from the EDGAR database of the U.S. Securities and Exchange Commission (SEC). Over one year, each stock is linked to three quarterly reports and one annual report, providing crucial quarterly insights. (3) **News Data**: Daily news data, compiled from multiple publicly accessible datasets, provides short-term market perspectives, enabling the agent to account for market sentiment. The table below summarizes the data statistics.

**Regulations**. The Regulations dataset focuses on long-form question answering related to Over-the-Counter (OTC) derivatives and financial regulations within the European Union. Derived from the European Securities and Markets Authority's (ESMA) comprehensive document on Regulation (EU) No 648/2012 (EMIR), it maps QA pairs to relevant articles from EMIR and other directives. EMIR, implemented to enhance transparency and reduce risks in derivatives trading, governs OTC derivatives, central counterparties, and trade repositories. The dataset includes 254 QA pairs, meticulously curated with domain experts to ensure relevance and accuracy, addressing key regulatory issues such as reporting requirements, clearing thresholds, and obligations for financial and non-financial counterparties. The QAs are updated to reflect ongoing regulatory changes, providing a dynamic resource for testing LLMs' understanding of complex regulatory frameworks. This dataset serves as a critical tool for both regulatory compliance and academic research.

### Tasks

Table 2 and Figure 1 shows all tasks, datasets, data statistics, and evaluation metrics covered by FinBen7.

Information extraction:It spans seven datasets across six information extraction tasks. _1) Named entity recognition_ extracts entities like LOCATION, ORGANIZATION, and PERSON from financial agreements and SEC filings, using the NER (Alvarado et al., 2015) and FINER-ORD (Shah et al., 2023b) datasets. _2) Relation extraction_ identifies relationships such as "product/material produced" and "manufacturer" in financial news and earnings transcripts with the FINRED dataset (Sharma et al., 2022). _3) Causal classification_ discerns whether sentences from financial news and SEC filings convey causality using the SC dataset (Mariko et al., 2020). _4) Causal detection_ identifies cause and effect spans in financial texts with the CD dataset (Mariko et al., 2020). _5) Numeric labeling_ tags numeric spans in financial documents using the FNXL dataset (Sharma et al., 2023), focusing on automating the assignment of labels from a large taxonomy to numeral spans in sentences. _6) Textual analogy parsing_ involves identifying common attributes and comparative elements in textual analogies by extracting analogy frames, utilizing the FSRL dataset (Lamm et al., 2018), which maps analogous facts to semantic role representations and identifies the analogical relations between them. The evaluation of these tasks is focused on the F1 score (Goutte and Gaussier, 2005), Entity F1 score (Derczynski, 2016), and the Exact Match Accuracy (EM Accuracy) metric (Kim et al., 2023).

Textual analysis:This encompasses eight classification tasks for evaluating LLMs. _1) Sentiment analysis_ focuses on extracting sentiment information (positive, negative, or neutral) from financial texts, using three datasets: the Financial Phrase Bank (FPB) (Malo et al., 2014), FiQA-SA (Maia et al., 2018), and TSA (Cortis et al., 2017). _2) News headline classification_ analyzes additional information, like price movements in financial texts, using the Headlines dataset (Sinha and Khandait, 2021). _3) Hawkish-Dovish classification_ aims to classify sentences from monetary policy texts as 'hawkish' or 'dovish' focusing on the nuanced language and economic implications of financial texts, using the FOMC (Shah et al., 2023a) dataset. _4) Argument unit classification_ categorizes sentences as claims or premises using the FinArg AUC dataset (Sy et al., 2023). _5) Argument relation detection_ identifies relationships (attack, support, or irrelevant) between social media posts using the FinArg ARC dataset (Sy et al., 2023). _6) Multi-class classification_ targets categorizing a variety of financial texts, including analyst reports, news articles, and investor comments, utilizing the MultiFin dataset (Jorgensen et al., 2023). _7) Denolpleteness classification_ predicts if mergers and acquisitions events are "completed" or remain "rumors" based on news and tweets, employing the MA dataset (Yang et al., 2020a). _8) ESG issue identification_ focuses on detecting Environmental, Social, and Governance (ESG) concerns in financial documents using the MLESG dataset (Chen et al., 2023a). For all datasets, evaluation utilizes the accuracy and F1 Score.

Question answering.It includes 4 datasets from three QA tasks, challenging LLMs to respond to financial queries. _1) Numerical QA_ focuses on solving questions through multi-step numerical reasoning with financial reports and tables, utilizing the FinQA (Chen et al., 2021) and TATQA (Zhu et al., 2021) dataset. _2) Multi-turn QA_ is an extension of QA with multi-turn questions and answers based on financial earnings reports and tables, using the ConvFinQA dataset (Chen et al., 2022b). F1

    &  &  &  &  \\  NER (Alvarado et al., 2015) & named entity recognition & 980 & Entity F1 & CC BY-SA 3.0 \\ FFNER-ORD (Shah et al., 2023b) & named entity recognition & 1,890 & Entity F1 & CC BY-NC 4.0 \\ FinRD (Sharma et al., 2022) & relation extraction & 1,068 & F1, Entity F1 & Public \\ SC (Marke et al., 2020) & causal classification & 8,630 & F1, Entity F1 & CC BY 4.0 \\ CD (Marke et al., 2020) & causal detection & 226 & F1, Entity F1 & CC BY 4.0 \\ FNAL, Sharma et al., 2023) & numeric labeling & 318 & F1, EM Accuracy & Public \\ FSRL (Lamn et al., 2018) & textual analogy parsing & 97 & F1, EM Accuracy & MIT License \\  FPB (Malo et al., 2014) & sentiment analysis & 970 & F1, Accuracy & CC BY-SA 3.0 \\ FiQA-SA (Mais et al., 2018) & sentiment analysis & 235 & F1 & Public \\ TSA (Cortis et al., 2017) & sentiment analysis & 561 & F1, Accuracy & CC BY-NC-SA 4.0 \\ Headlines (Sohn and Khandait, 2021) & news headline classification & 2,238 & Avg F1 & CC BY 9X.0 \\ FOMC (Shah et al., 2023a) & hash-both classification & 496 & F1, Accuracy & CC BY-NC-SA 4.0 \\ FinArg-ACC (Sy et al., 2023) & argument unit classification & 969 & F1, Accuracy & CC BY-NC-SA 4.0 \\ FinArg-ACC (Sy et al., 2023) & argument relations classification & 496 & F1, Accuracy & CC BY-NC-SA 4.0 \\ MultiFin (Jorgensen et al., 2023) & multi-class classification & 690 & F1, Accuracy & Public \\ MA (Yang et al., 2020a) & dual components classification & 500 & F1, Accuracy & Public \\ MLBSG (Chen et al., 2023a) & ESG Issue identification & 300 & F1, Accuracy & CC BY-NC-ND \\  FinQA (Chen et al., 2021) & question answering & 1,147 & EM Accuracy & MIT License \\ TATQA (Zhu et al., 2021) & question answering & 1,668 & F1, EM Accuracy & MIT License \\ “Regulations & long-term question answering & 254 & RUOLBERTScore & Public \\ ComFinQA (Chen et al., 2022b) & multi-turn question answering & 1,490 & EM Accuracy & MIT License \\  ECTSM (Mahieber et al., 2022) & text summarization & 495 & RUOLBERTscore, BATRScore & Public \\ EDTSum (Xie et al., 2023b) & text summarization & 2,000 & RUOLGE, BETRScore, BATRScore & Public \\  BigData22 (Soun et al., 2022) & stock movement prediction & 1,470 & Accuracy, MCC & Public \\ ACL18 (Van and Cohen, 2018) & stock movement prediction & 3,720 & Accuracy, MCC & MIT License \\ CLNIKIscore (Derczynski, 2016) and the Exact Match Accuracy (EM Accuracy) metric (Kim et al., 2023) are used to evaluate these tasks. _3) Long-form QA_ involves presenting models with complex, detailed questions that require extensive and nuanced answers, often incorporating legal interpretations and practical applications. In our evaluation, we utilize our newly proposed Regulations dataset, which focuses on intricate questions and answers related to financial regulations like EMIR. We assess the model responses using ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2019).

**Text generation.** This task assesses the models' ability to produce coherent and informative text. Our focus is on _text summarization_, utilizing the ECTSUM (Mukherjee et al., 2022) dataset for summarizing earnings call transcripts. We also include EDTSUM, specifically designed for condensing financial news articles into concise summaries, constructed from original data in (Zhou et al., 2021). Evaluation employs ROUGE (Lin, 2004), BERTScore (Zhang et al., 2019), and BART Score (Yuan et al., 2021) to measure alignment, factual consistency, and information retention between machine-generated and expert summaries.

**Forecasting.** The forecasting task challenges models to predict future market and investor behaviors from emerging patterns. We focus on the _stock movement prediction_ task, forecasting stock directions as either positive or negative, based on historical prices and tweets. Three datasets are included: BigData22 (Soun et al., 2022), ACL18 (Xu and Cohen, 2018) and CIKM18 (Wu et al., 2018).

**Risk management**. It challenges LLMs to accurately identify, extract, and analyze relevant risk-related information, interpret numerical data, and understand complex relationships. We include 4 tasks: _1) Credit scoring_ classifies individuals as "good" or "bad" credit risks using historical customer data, employing datasets including: German (Hofmann, 1994), Australia (Quinlan, [n. d.]) and LendingClub (Feng et al., 2023). _2) Fraud detection_ involve categorizes transactions as "fraudulent" or "non-fraudulent", using two datasets: ccf (Feng et al., 2023) and ccFraud (Feng et al., 2023). _3) Financial distress identification_ aims to predict a company's bankruptcy risk, using the polish (Feng et al., 2023) and taiwan dataset (Feng et al., 2023). Note that the dataset name describes only the region of the company, and the content within the datasets is in English. _4) Claim analysis_ anonymizes client data for privacy, labeling a "target" to indicate claim status, using two datasets: PortoSeguro (Feng et al., 2023) and travelinsurance (Feng et al., 2023). It is noticed that the dataset name such as German and taiwan, only indicates customer sources and all content is in English. F1 score and Matthews correlation coefficient (MCC) (Chicco and Jurman, 2020) are used for evaluating these tasks.

**Decision-making.** Strategic decision-making (Punt, 2017) evaluates the model's proficiency in synthesizing diverse information to formulate and implement trading strategies, a challenge even for experts. We innovatively introduce the SOTA financial LLM agent FinMem (Yu et al., 2023, 2024) to evaluate LLMs on the _stock trading_ task. We construct the novel FinTrade dataset, containing 10 stocks, simulating real-world trading through historical prices, news, and sentiment analysis. Performance is measured by Cumulative Return (CR) (Ariel, 1987), Sharpe Ratio (SR) (Sharpe, 1998), Daily (DV) and Annualized volatility (AV) (Zhou et al., 2023), and Maximum Drawdown (MD) (Magdon-Ismail and Atiya, 2004), offering a comprehensive assessment of profitability, risk management, and decision-making prowess.

**Spanish.** Spanish financial datasets (Zhang et al., 2024) evaluate model performance in low-resource language settings. We include six datasets in our analysis: TSA-ES (Zhang et al., 2024) and FinanceES (Zhang et al., 2024), both designed for sentiment analysis in the Spanish financial domain, where model performance is measured using F1 score. For multi-class classification, we utilize the Spanish subset of the MultiFin dataset (Jorgensen et al., 2023), with F1 score as the primary metric. The EFP (Zhang et al., 2024) and EFPA (Zhang et al., 2024) datasets, focused on Spanish financial question-answering, are evaluated using F1 score to assess the accuracy of predicted answers. Finally, for summarization tasks, the FNS-2023 (Zhang et al., 2024) dataset, which consists of Spanish company reports, is evaluated using ROUGE scores to measure the quality of generated summaries.

## 3 Evaluation

We evaluate the zero-shot (from our evaluation) and few-shots (results from previous papers) performance of \(21\) representative general LLMs and financial LLMs on the FinBen benchmark, including: 1) ChatGPT: A LLM developed by OpenAI. 2) GPT-4 (OpenAI, 2023): The SOTA commercialized LLMs proposed by OpenAI. 3) Gemini Pro (Team et al., 2023): A multimodal LLM with 50Tparameters, released by Google. 4) LLaMA2-7/70B-chat (Touvron et al., 2023b): An open-sourced instruction-following LLM with 7B and 70B parameters developed by MetaAI. 5) LLaMA3-8B8: An open-sourced LLMs developed by MetaAI, using more training data than LLaMA2. 6) ChatGLM3-6B (Du et al., 2022): A conversational LLM with 6B parameters, jointly released by Zhipu AI and Tsinghua KEG. 7) Baiduhan2-6B (Baiduhan, 2023): An open-source LLM with 6B parameters, launched by Baiduhan Intelligent Technology. 8) InternLM-7B (Team, 2023): An open-sourced 7B parameter base model tailored for practical scenarios, proposed by SenseTime. 9) Falcon-7B (Almazrouei et al., 2023): A 7B parameter causal decoder-only LLM model trained on 1500B tokens of RefinedWeb enhanced with curated corpora. 10) Mixtral 8\(\)7B (Jiang et al., 2024): A LLM with the Sparse Mixture of Experts (SMoE) architecture. 11) Code LLaMA-7B (Roziere et al., 2023): An open-source LLM model for generating programming code, launched by Meta AI with 7B parameters. 12) FinGPT (Yang et al., 2023a): A 7B instruction finetuned financial LLM based on LLaMA 7B (Touvron et al., 2023a) with sentiment analysis tasks. 13) FinMA-7B (Xie et al., 2023b): A 7B instruction finetuned financial LLM based on LLaMA 7B with multiple NLP and forecasting tasks. 14) DISC-FinLLM (Chen et al., 2023b): An open-sourced financial LLM, fine-tuned from Baiduhan-13B-Chat (Baiduhan, 2023). 15) CFGPT (Li et al., 2023a): An open-source LLM, specifically designed for the financial sector and trained on Chinese financial datasets, which comprises 7B parameters. 16) Qwen2-7B/72B (qwe, 2024): Instruction-tuned LLMs developed by Alibaba Cloud with 7B and 72B parameters, optimized for financial and general NLP tasks. 17) Xuanyuan-6B/70B (Zhang et al., 2023c): Instruction-tuned LLMs designed for financial NLP tasks with 6B and 70B parameters. 18) LLaMA3.1-8B/70B (Dubey et al., 2024): LLaMA3 series models with 8B and 70B parameters, fine-tuned with enhanced data for a wide range of NLP tasks.

**Experimental Settings** We set the maximum generation tokens for LLMs to 1024 and the batch size to 20,000 for all experiments. These experiments are exclusively conducted on 16 NVIDIA A100 80G GPUs, taking approximately 600 hours to complete. Including the GPT-4 API costs, the total expenditure amounts to approximately $51,000.

## 4 Results

Table 3 and Table 4 shows the performance of 14 representative LLMs on all datasets in the FinBen. We also report results of non-LLM methods (traditional methods) in Appendix H.

### Information Extraction and Textual Analysis Results

As shown in Table 3, for IE tasks, GPT-4 demonstrates superior performance in named entity recognition tasks, including NER, FINER-ORD, and FinRED. InternLM 7B achieves the best results in causal classification (SC). However, for more complex information extraction tasks, such as causal detection (CD) and numerical understanding (FNXL and FSRL), even GPT-4's performance is limited, with Gemini showing only slightly better results, still falling short of expectations. Additionally, while financial domain-specific LLMs developed by instruction tuning such as FinMA 7B exhibit improvements over general domain LLMs such as LLaMA2 7B-chat, they continue to struggle with both named entity recognition and complex extraction tasks. These findings highlight significant opportunities for advancement in financial causal detection and numerical understanding for LLMs.

Regarding TA tasks, instruction fine-tuned models like FinMA 7B exhibit the best performance in sentiment analysis tasks, including FPB, FiQA-SA, and Headlines. However, the generalization ability of FinMA 7B is limited due to the diversity of TA tasks in the financial domain. It performs even worse than general domain LLMs such as LLaMA2-7B-chat on other TA tasks, where GPT-4, Gemini, and LLaMA2 70B show superior results. This underscores the limitations of instruction fine-tuned models, which may be constrained by the parameter size and ability of their base models.

Models tailored for the Chinese language, such as CFGPT sft-7B-Full, which is fine-tuned on Chinese financial data, exhibit limited improvement on some datasets and even a decline in performance on others like MultiFin compared to its base model InternLM 7B. This trend suggests a language-based discrepancy, indicating that fine-tuning with Chinese data may adversely affect performance on English tasks. These findings underscore the complexities of cross-lingual adaptation in model training, highlighting the challenges in achieving consistent performance across different languages.

[MISSING_PAGE_FAIL:8]

sentences. Among open-source LLMs, LLaMA2 70B stands out in text summarization. Conversely, CFGPT sft-7B-Full consistently shows a decrease in performance compared to its foundational model, InternLM 7B.

### Forecasting and Risk Management Results

For forecasting, it is crucial to acknowledge that all LLMs fail to meet expected outcomes and lag behind traditional methodologies. This consistent observation with existing studies Xie et al. (2023b) underlines a notable deficiency in LLMs' capacity to tackle forecasting as effectively as traditional methods. Even the best-performing models, such as GPT-4 and Gemini, only perform slightly better than random guessing. This reveals significant potential for enhancement in LLMs, including industry leaders like GPT-4 and Gemini, particularly in forecasting tasks that demand complex reasoning abilities.

In RM tasks, such as credit scoring, fraud detection, and identifying financial distress, data often exhibit significant imbalances. Instances representing individuals with low credit scores, those prone to fraud, and companies at risk of financial distress constitute only a small percentage of the overall dataset. In such scenarios, LLMs with low instruction-following abilities (such as LLaMA2-7B-chat and LLaMA2-70B) tend to classify all cases into a single class, resulting in an MCC score of 0. These tasks, with tabular inputs and highly imbalanced distribution, pose a significant challenge for LLMs in the financial domain.

### Decision Making Results

The comparative analysis of various LLMs on the complex task of stock trading, is presented in Table 49. This task requires models to understand, summarize, and reason with multimodal financial data (texts and time series), leading to sophisticated trading decisions that necessitate a range of skills, from fundamental comprehension and summarization to reasoning and decision-making.

Among the evaluated LLMs, GPT-4 distinguishes itself by achieving the highest Sharpe Ratio (SR) over 1, indicating superior investment performance through optimal risk-return balance. It also records the minimal Maximum Drawdown (MDD), suggesting effective limitation of potential losses, thereby offering a more secure investment avenue compared to other models, including those using reinforcement learning methods like DQN, PPO, and A2C, which show significantly lower SR and higher MDD.

Tables 4 and 10 reinforce these findings, highlighting GPT-4's exceptional performance in this challenging domain. Additional results and analyses from these models in Table 5 contrast their performances with the traditional _Buy & Hold_ strategy, which considerably lags behind.

In contrast, ChatGPT exhibits significantly lower performance metrics, indicating limitations in its financial decision-making capabilities. Gemini, on the other hand, secures the position of second-best performer, showcasing lower risk and volatility compared to GPT-4, yet maintaining commendable returns. When considering open-source models, LLaMA-70B, despite its lower volatility, yields the least profit among the LLMs, highlighting a trade-off between risk management and profitability.

  
**Model** & **CR (\%)\(\)** & **SR\(\)** & **DV (\%)\(\)** & **AV (\%)\(\)** & **MD (\%)\(\)** \\  Buy \& Hold & -4.00 \(\) 22.39 & 0.02 \(\) 0.87 & 3.59 \(\) 1.34 & 56.43 \(\) 21.00 & 30.67 \(\) 17.48 \\ GPT-4 & **28.19 \(\) 25.27** & **1.51 \(\) 1.08** & 2.52 \(\) 1.30 & 39.88 \(\) 20.66 & **18.34 \(\) 9.77** \\ GPT-4o & -5.54 \(\) 19.12 & -0.19 \(\) 0.84 & 2.73 \(\) 1.30 & 43.62 \(\) 20.67 & 29.96 \(\) 18.89 \\ GPT3-Turbo & 4.48 \(\) 22.23 & 0.15 \(\) 0.82 & 2.84 \(\) 1.47 & 45.39 \(\) 23.35 & 28.83 \(\) 15.40 \\ Ilam2-70B & 4.02 \(\) 24.65 & 0.52 \(\) 1.48 & 2.18 \(\) 1.28 & 34.86 \(\) 20.38 & 25.55 \(\) 16.83 \\ Ilam3-70B & -2.57 \(\) 22.63 & -0.04 \(\) 1.19 & 2.71 \(\) 1.54 & 43.42 \(\) 24.65 & 29.31 \(\) 15.57 \\ gemini & 14.95 \(\) 28.04 & 1.03 \(\) 1.24 & **2.17 \(\) 1.39** & **34.67 \(\) 22.23** & 20.13 \(\) 11.36 \\   

Table 4: The average trading performance (95% Confidence Interval) comparison for different LLMs across 10 stocks. The results include large LLMs only (\( 70B\)), as models with smaller contexts have difficulty understanding the instructions and producing a static strategy of holding.

For smaller models with parameters less than 70 billion, a marked inability to adhere to trading instructions consistently across transactions is noted. This is attributed to their limited comprehension, extraction capabilities, and constrained context windows. This limitation underscores the critical challenges smaller LLMs face in tasks requiring intricate financial reasoning and decision-making, thereby spotlighting the necessity for more advanced models to tackle decision making tasks effectively.

### Spanish Results

Table 3 presents the performance of various models on six Spanish financial datasets, highlighting significant language disparities. ChatGPT, GPT-4 and Gemini show limited performance compared with English datasets. Miktral 7B performs competitively, showing that the multilingual ability can improve language-specific tasks. Smaller models, particularly from the LLaMA family, struggle with domain complexities, reinforcing the importance of robust multilingual pretraining. While top models excel in sentiment analysis, all models underperform in summarization tasks on FNS, stressing the need for enhanced adaptation to specialized Spanish financial language.

## 5 Conclusion

In this work, we present FinBen, a comprehensive benchmark specifically designed to evaluate LLMs in the financial domain. FinBen includes \(42\) diverse datasets spanning 24 tasks, meticulously organized to assess LLMs across eight critical aspects: information extraction, textual analysis, question answering, text generation, risk management, forecasting, decision-making, and Spanish. This breadth of coverage sets FinBen apart from existing financial benchmarks, enabling a more robust and nuanced evaluation of LLM capabilities. Our evaluation of \(21\) LLMs, including GPT-4, ChatGPT, and Gemini, reveals their key advantages and limitations, highlighting directions for future work. Looking ahead, FinBen continuously evolves into an open FinLLM leaderboard (Lin et al., 2024). We will incorporate additional languages and multimodal financial tasks (Yanglet and Deng, 2024) and expand the range of financial tasks to further enhance its applicability and impact.

**Openness**: Our FinBen project follows the model openness framework (White et al., 2024) by providing a comprehensive set of financial datasets and evalution codes under OSI-approved licenses.

**Limitations**: We acknowledge several limitations that could impact FinBen's effectiveness and applicability. The restricted size of available datasets may affect the models' financial understanding and generalization across various contexts. Computational constraints limited our evaluation to the LLaMA 70B model, potentially overlooking the capabilities of larger models. Additionally, the tasks are based on American market data and English texts, which may limit the benchmark's applicability to global financial markets. Responsible usage and safeguards are essential to prevent potential misuse, such as financial misinformation or unethical market influence10.

**Ethical Statement**: The authors take full responsibility for any potential legal issues arising from FinBen's development and dissemination. All data used are publicly available, non-personal, and shared under the MIT license, adhering to privacy and ethical guidelines. This manuscript and associated materials are for academic and educational use only and do not provide financial, legal, or investment advice. The authors disclaim any liability for losses or damages from using the material, and users agree to seek professional consultation and indemnify the authors against any claims arising from its use11.

   Model & Cumulative Return & Sharpe Ratio & Standard Deviation & Annualized Volatility & Max Drawdown \\  A2C & -4.2232 & -0.2586 & 2.7522 & 43.6898 & 30.5819 \\ PPO & -0.5586 & 0.0085 & 2.7531 & 43.7048 & 28.9496 \\ DQN & -2.9924 & -0.1656 & 2.7486 & 43.6319 & 31.78 \\   

Table 5: Traditional model performances on stock trading.