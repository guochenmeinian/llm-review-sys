# Data-Centric Learning from Unlabeled Graphs

with Diffusion Model

 Gang Liu

University of Notre Dame

gliu7@nd.edu

&Eric Inae

University of Notre Dame

einae@nd.edu

&Tong Zhao

Snap Inc.

tzhao@snap.com

&Jiaxin Xu

University of Notre Dame

jxu24@nd.edu

&Tengfei Luo

University of Notre Dame

tluo@nd.edu

&Meng Jiang

University of Notre Dame

mjiang2@nd.edu

###### Abstract

Graph property prediction tasks are important and numerous. While each task offers a small size of labeled examples, unlabeled graphs have been collected from various sources and at a large scale. A conventional approach is training a model with the unlabeled graphs on self-supervised tasks and then fine-tuning the model on the prediction tasks. However, the self-supervised task knowledge could not be aligned or sometimes conflicted with what the predictions needed. In this paper, we propose to extract the knowledge underlying the large set of unlabeled graphs as a specific set of useful data points to augment each property prediction model. We use a diffusion model to fully utilize the unlabeled graphs and design two new objectives to guide the model's denoising process with each task's labeled data to generate task-specific graph examples and their labels. Experiments demonstrate that our data-centric approach performs significantly better than fifteen existing various methods on fifteen tasks. The performance improvement brought by unlabeled data is _visible_ as the generated labeled examples unlike the self-supervised learning.

## 1 Introduction

Graph data such as molecules and polymers are found to have attractive properties in drug and material discovery (Bohm et al., 2004; Huang et al., 2021), but annotating them requires specialized knowledge, as well as lengthy and costly experiments in wet labs (Cormack and Elorza, 2004). So, it is important for graph property predictors to learn _useful knowledge_ from unlabeled graphs.

Self-supervised learning (Hu et al., 2019; Rong et al., 2020; You et al., 2021; Kim et al., 2022) utilizes unlabeled graphs to learn through _predictive tasks_ or _contrastive tasks_ to represent and transfer the knowledge as _model parameters_. Despite the empirical success in language and vision (Brown et al., 2020; He et al., 2022), their performance on graph data applications remains unsatisfactory because of the significant gap between the graph self-supervised task and the graph label prediction task. Models trained on node attribute prediction (Hu et al., 2019) as a simple _predictive_ self-supervised task extract too limited knowledge from the graph structure, which has been observed after too fast convergence (Sun et al., 2022). More complex tasks like graph context prediction (Hu et al., 2019; Zhang et al., 2021) may transfer knowledge that conflicts with downstream tasks. Aromatic rings, for instance, are a prevalent structure in molecules (Maziarka et al., 2020) and are considered valuable in context prediction tasks (Zhang et al., 2021). However, graph properties such as oxygen permeability can be more related to non-aromatic rings in some cases (Liu et al., 2022), which is overlooked if not using tailored predictive tasks specifically for downstream tasks. As predictive tasks strive foruniversality, the transferred knowledge may force models to focus more on aromatic rings, leading to poor prediction.

On the other line, _contrastive_ tasks (You et al., 2021; Kim et al., 2022) aim to learn the similarity between original and perturbed graphs. However, the learned similarity can hardly generalize across tasks (Kim et al., 2022). First, perturbations without domain knowledge, _e.g.,_ bioisosteres, do not preserve broad biological properties (Sun et al., 2021). Second, it is difficult, if not impossible, to find universal perturbations that generalize to diverse property prediction tasks. For example, bioisosteric (subgraph) replacements produce similar biological properties for molecules. And they may reduce toxicity (Brown, 2014). So, contrastive tasks with bioisosteric replacement enforce the similarity between toxic and non-toxic molecules. However, models pre-trained on such contrastive tasks hurt the performance on downstream tasks, _e.g.,_ toxicity prediction.

Our _data-centric_ idea avoids the use of self-supervised tasks that are not appropriate. We use a diffusion probabilistic model (known as _diffusion model_) to capture the data distribution of _unlabeled graphs_, leveraging its capability of distribution coverage, stationarity, and scalability (Dhariwal and Nichol, 2021). At the stage of performing a particular property prediction task, the reverse process, guided by novel task-related optimization objectives, generates new task-specific labeled examples. Minimal sufficient knowledge from the unlabeled data is transferred into these examples, instead of uninterpretable model parameters, and then to enhance the training of prediction models.

To implement our idea, we propose a _Data-Centric Transfer_ framework (DCT) based on a diffusion model for graph data, as shown in Figure 0(b). It aims to transfer minimal sufficient knowledge from unlabeled graphs to property predictors by data augmentation. The diffusion model gradually adds Gaussian noise to a graph from which a score function (_i.e.,_ the gradient of the log probability density) is then learned to estimate the noise step by step to reverse the process. DCT trains the diffusion model on the unlabeled graphs to get ready to augment any labeled dataset. Given a labeled graph from a particular task (_i.e.,_ type of property), the diffusion model adds noise to perturb it by a few steps and then generates a new graph through the score function. The new graph could be close to the distribution of the unlabeled graphs for diversity, however, it would lose the relatedness to the target task. So, we add two task-related objectives into the score function to guide the reverse process. When a predictor model \(f\) has been trained on the task, given an original labeled graph \(G\), the first objective is to optimize the new graph \(G^{}\) to _sufficiently_ preserve the label of \(G\) with \(f\). The second objective is to optimize \(G^{}\) to be very different from (_i.e., minimally_ similar to) \(G\). These two objectives ensure that \(G^{}\) carries minimal sufficient knowledge from the unlabeled graphs to be an augmentation of \(G\). DCT iteratively generates new examples to augment the labeled dataset and progressively trains the prediction model with it.

We test DCT on _fifteen_ graph property prediction datasets from three fields: chemistry (molecules), material science (polymers), and biology (protein-protein interaction graphs). DCT achieves the best performance over all these tasks. We find that the state-of-the-art self-supervised methods often struggle to transfer knowledge to regression tasks, etc. DCT reduces the mean absolute error relatively by 13.4% and 10.2% compared to the best baseline on the molecule and polymer graph regression tasks, respectively.

Figure 1: Comparing the diagrams of the existing approach and the proposed approach to learning from unlabeled graphs for a variety of graph property prediction tasks.

## 2 Problem Definition

Given \(K\) property prediction tasks, there are \(N^{[k]}\) labeled graph examples for the \(k\)-th task. They are \(\{(G_{i},y_{i}) G_{i}^{[k]},y_{i}^{[k]}\}_{i=1}^{ N^{[k]}}\), where \(^{[k]}\) is the graph space and \(^{[k]}\) is the label space of the task. The prediction model with parameters \(\) is defined as \(f_{}^{[k]}:^{[k]}^{[k]}\). \(f_{}^{[k]}\) consists of a GNN and a multi-layer perceptron (MLP). Without the loss of generality, we consider Graph Isomorphism Networks (GIN) (Xu et al., 2019) to encode graph structures. Given a graph \(G=(,)^{[k]}\) in the task \(k\), GIN updates the representation vector of node \(v\) at \(l\)-layer:

\[_{v}^{l}=^{l}((1+)_{v}^{l-1 }+_{u(v)}_{u}^{l-1})\,, \]

where \(\) is a learnable scalar and \(u(v)\) is one of node \(v\)'s neighbor nodes. After stacking \(L\) layers, the readout function (_e.g.,_ summation) gets the graph representation across all the nodes. The predicted label is:

\[=((\{_{v}^{L} v G \})). \]

\(f_{}^{[k]}\) is hard to be well-trained because it is hard to collect graph labels at a large scale (\(N^{[k]}\) is small).

Fortunately, regardless of the tasks, a large number of **unlabeled graphs** are usually available from the same or similar domains. Self-supervised learning methods (Hu et al., 2019) rely on hand-crafted tasks to extract _knowledge_ from the unlabeled examples \(\{G_{j}^{[U]},j=1,,N\}\) as _pre-trained model parameters \(\)_. The uninterpretable parameters are transferred to warm up the prediction models \(\{f_{}^{[k]}\}_{k=1}^{K}\) on the \(K\) downstream graph property prediction tasks. However, the gap and even conflict between the self-supervised tasks and the property prediction tasks lead to suboptimal performance of the prediction models. In the next section, we present the DCT framework that transfers knowledge from the unlabeled graphs with a data-centric approach.

## 3 The Data-Centric Transfer Framework

### Overview of Proposed Framework

The goal of data-centric approaches is to augment training datasets by generating useful labeled data examples. Under that, the goal of our data-centric transfer (DCT) framework is to _transfer_ the knowledge from unlabeled data into the data augmentation. Specifically, for each graph-label pair \((G^{[k]}^{[k]},\,y^{[k]}^{[k]})\) in the task \(k\), the framework is expected to output a new example \(G^{[k]}\) with the label \(y^{[k]}\) such that (1) \(y^{[k]}=y^{[k]}\) and (2) \(G^{[k]}\) and \(G^{[k]}\) are from the same graph space \(^{[k]}\). However, if the graph structures of \(G^{[k]}\) and \(G^{[k]}\) were too similar, the augmentation would duplicate the original data examples, become useless, and even cause over-fitting. So, the optimal graph data augmentation should _enrich the training data with good diversity as well as preserve the labels of the original graphs_. To achieve this, DCT utilizes a diffusion probabilistic model to first _learn the data distribution from unlabeled graphs_ (Section 3.2). Then DCT adapts the reverse process in the diffusion model to _generate task-specific labeled graphs for data augmentation_ (Section 3.3). Thus, the augmented graphs will be derived from the distribution of a huge collection of unlabeled data for _diversity_. To _preserve the labels_, DCT controls the reverse process with two task-related optimization objectives to transfer _minimal sufficient knowledge_ from the unlabeled data. The first objective minimizes an upper bound of mutual information between the augmented and the original graphs in the graph space. The second objective maximizes the probability of the predicted label of augmented graphs being the same as the label of original graphs. The first is for minimal knowledge transfer, and the second is for sufficient knowledge transfer. DCT integrates the two objectives into the reverse process of the diffusion model to guide the generation of new labeled graphs. DCT iteratively trains the graph property predictor (used in the second objective) and creates the augmented training data. To simplify notations, we remove the task superscript \([k]\) in the following sections.

### Learning Data Distribution from Unlabeled Graphs

The diffusion process for graphs in Figure 2 applies to both graph structure and node features. The diffusion model slowly corrupts unlabeled graphs to a standard normal distribution with noise. Forgraph generation, the model samples noise from the normal distribution and learns a score function to reverse the perturbed noise. Given an unlabeled graph \(G\), we use continuous time \(t[0,T]\) to index multiple diffusion steps \(\{G^{(t)}\}_{t=1}^{T}\) on the graph, such that \(G^{(0)}\) follows the original data distribution and \(G^{(T)}\) follows a prior distribution like the normal distribution. The forward diffusion is a stochastic differential equation (SDE) from the graph to the noise:

\[G^{(t)}=(G^{(t)},t)t+g(t)\; , \]

where \(\) is the standard Wiener process, \((,t):\) is the drift coefficient and \(g(t):\) is the diffusion coefficient. \((G^{(t)},t)\) and \(g(t)\) relate to the amount of noise added to the graph at each infinitesimal step \(t\). The reverse-time SDE uses gradient fields or scores of the perturbed graphs \(_{G^{(t)}} p_{t}(G^{(t)})\) for denoising and graph generation from \(T\) to \(0\)(Song et al., 2021):

\[G^{(t)}=[(G^{(t)},t)-g(t)^{2}_{G^{(t)}} p_ {t}(G^{(t)})]t+g(t)}, \]

where \(p_{t}(G^{(t)})\) is the marginal distribution at time \(t\) in forward diffusion. \(}\) is a reverse time standard Wiener process. \(t\) here is an infinitesimal negative time step. The score \(_{G^{(t)}} p_{t}(G^{(t)})\) is unknown in practice and it is approximated by the score function \((G^{(t)},t)\) with score matching techniques (Song et al., 2021). On graphs, Jo et al. (2022) used two GNNs to develop the score function \((G^{(t)},t)\) to de-noise both node features and graph structures and details are in appendix B.3.

### Generating Task-specific Labeled Graphs

_Self-training_ approaches would propose to either (1) select unlabeled graphs by a graph property predictor or (2) generate graphs directly from the standard diffusion model, and then use the predictor to assign them labels so that the training data could be enriched. However, we have observed that neither of them can guarantee positive impact on the prediction performance. In fact, as shown in Figure 3, they make very little or even negative impact. That is because _the selected or directly-generated graphs are too different from the labeled graph space of the target tasks_. Task details of ten datasets are in appendix C.

Figure 3: Relative improvement (increased AUC or reduced MAE) from three data-centric methods (over ten runs), compared to the basic GIN: Blue is for self-training with selected real unlabeled graphs. Green is for self-training with graphs directly generated by a standard diffusion model. Red is for DCT that generates task-specific labeled graphs. The first two often make little or negative impact. Our DCT has consistent and significant improvement shown as the percentages in red.

Figure 2: Diffusion model in DCT: It performs task-specific data augmentation using objectives \(_{1}\) and \(_{2}\) in the reverse process. The model was trained on unlabeled graphs to learn the general data distribution. Then it generates \((G^{},y^{}=y)\) based on \((G,y)\) in the reverse process. It perturbs \(G\) with \(D\) steps and optimizes \(G^{}\) to be minimally similar to \(G\) (Objective \(_{1}\)) and sufficiently preserve the label of \(G\) (Objective \(_{2}\)).

Given a labeled graph (\(G,y\)) from the original dataset of a specific task, the new labeled graph (\(G^{},y^{}\)) is expected to provide _useful knowledge to augment_ the training set. We name it _the augmented graph_ throughout this section. The augmented graph is desired to have the following two properties, as in Section 3.1: **Task relatedness**: As an effective training data point, \(G^{}\) and \(y^{}\) are from the graph/label spaces of the specific task where \((G,y)\) come from and thus transfer sufficient task knowledge into the training set; **Diversity**: If \(G^{}\) was too similar to \(G\), the new data point would cause severe over-fitting on the property prediction model. The augmentation aims to learn from unlabeled graph to create diverse data points, which should contain minimal task knowledge about \(G\).

The selected unlabeled graphs used in _self-training_ have little task relatedness because the unlabeled data distribution might be too far from the one of the specific task. Existing graph _data augmentation_ methods could not create diverse graph examples because they manipulated labeled graphs and did not learn from the unlabeled graphs. Our novel data-centric approach DCT works towards both desired properties by transferring _minimally sufficient knowledge_ from the unlabeled graphs: **Sufficiency** is achieved by maximizing the possibility for label preservation (i.e., \(y^{}=y\)). It ensures that the knowledge from unlabeled graphs is task-related; **Minimality** refers to the minimization of graph similarity between \(G^{}\) and \(G\) to ensure that the augmentation introduces diversity. Both optimizations can be formulated using mutual information \((\ ;)\) to generate task-specific labeled data (\(G^{},y^{}\)):

**Definition 3.1** (Sufficiency for Data Augmentation).: The augmented graph \(G^{}\) sufficiently preserves the label of the original graph \(G\) if and only if \((G^{};y)=(G;y)\).

**Definition 3.2** (Minimal Sufficiency for Data Augmentation).: The Sufficiency is minimal for data augmentation if and only if \((G^{};G)(;G)\), \(\) represents any augmented graph that sufficiently preserves the original graph's label.

Self-supervised tasks applied a similar philosophy in pre-training (Soatto and Chiuso, 2016), however, they did not use labeled data from any specific tasks. So the optimizations were unable to extract useful knowledge and transfer it to the downstream (Tian et al., 2020). In our DCT that performs task-specific data augmentation, the augmented graphs can be optimized toward the objectives using any labeled graph \(G\) and its label \(y\):

\[_{_{1}}_{_{2}}\ \ _{G}[ _{1}(G^{};G)+_{2}(G^{};y )]. \]

For the first objective, we use the leave-one-out variant of InfoNCE (Poole et al., 2019; Oord et al., 2018) as the upper bound estimation. For the \(i\)-th labeled graph \((G_{i},y_{i})\),

\[_{1}_{}(G^{}_{i};G_{i})= {p(G^{}_{i}|G_{i})}{_{j=1,j i}^{M}p(G^{}_{i}|G_{j})}, \]

where \(G^{}_{i}\) is the augmented graph. When \(G^{}_{i}\) is optimized, \(G_{i}\) makes a positive pair; \(\{G_{j}\}\) (\(j i\)) are \(M-1\) negative samples of labels that do not equal \(y_{i}\). (\(M\) is a hyperparameter.) We use cosine similarity and a softmax function to calculate \(p(G^{}_{i}|G_{j})=(G^{}_{i},G_{j}))}{_{j =1}^{M}((G^{}_{i},G_{j}))}\). In practice, we extract statistical features of graphs to calculate their similarity. Details are in appendix B.2.

For the second objective, we denote the predicted label of the augmented graph \(G^{}\) by \(f_{}(G^{})\). We maximize the log likelihood \( p(y|f_{}(G^{}))\) to maximize \(_{2}(G^{};y)\). Specifically, after the predictor \(f_{}\) is trained for several epochs on the labeled data, we freeze its parameters and use it to optimize the augmented graphs so they are task-related:

\[(G^{})=_{}(G^{};G)-  p(y|f_{}(G^{})). \]

**Framework details:** As shown in Figure 2, after the diffusion model learns the data distribution from unlabeled graphs, given a labeled graph \(G\) from a specific task, DCT perturbs it for \(D\) (\(D T\)) steps. The perturbed noisy graph, denoted by \(^{(D)}\), stays inside the task-specific graph and label space, rather than the noise distribution (at step \(T\)). To reverse the noise in it and generate a task-specific augmented example \(G^{}\), DCT integrates the loss function in Eq. (7) into the score function \((,t)\) for minimal sufficient knowledge transfer:

\[^{(t)}=[(^{(t)},t)-g(t)^{2}( (^{(t)},t)-_{^{(t)}}( {G}^{(t)}))]t+g(t)}, \]

where \(\) is a scalar for score alignment between \(\) and \(\) to avoid the dominance of any of them: \(=(^{(t)},t)\|_{2}}{\|_{^{(t)}} (^{(t)})\|_{2}}\). Because \(^{(t)}\) is an intermediate state in the reverse process, the noise in it may fail the optimizations. So, we design a new sampling method named _double-loop sampling_ for accurate loss calculation. It has an inner-loop sampling using Eq. (4) to sample \(_{(t)}\), as the denoised version of \(^{(t)}\) at the reverse time \(t\). Then \(_{}(_{(t)})\) is calculated as an alternative for \(_{^{(t)}}(^{(t)})\). Finally, an outer-loop sampling takes one step to guide denoising using Eq. (8).

DCT iteratively creates the augmented graphs \((G^{},y^{})\), updates the training dataset \(\{(G_{i},y_{i})\}\), and trains the graph property predictor \(f_{}\). In each iteration, for task \(k\), \(n N^{[k]}\) labeled graphs of the lowest property prediction loss are selected to create the augmented graphs.The predictor is better fitted to these graphs for more accurate sufficiency estimation of the augmentation.

## 4 Experiments

In this section, we present and analyze experimental results to demonstrate the outstanding performance of DCT, the usefulness of new optimization objectives, the effect of hyperparameters and iterative process, and the interpretability of "visible" knowledge transfer from unlabeled graphs.

### Experimental Setup

Tasks and metrics:Experiments are conducted on 15 graph property prediction tasks in chemistry, material science, and biology, including seven molecule classification, three molecule regression tasks from open graph benchmarks (Hu et al., 2020), four polymer regression tasks, and protein function prediction (PPI) (Hu et al., 2019). Dataset statistics is presented in Table 1. We use the area under the ROC curve (AUC) to evaluate classifiers and mean absolute error (MAE) for regressors.

Baselines and implementation:Besides GIN, there are three lines of baseline methods: (1) _self-supervised learning methods_ including EdgePred, AttrMask, ContextPred in (Hu et al., 2019), InfoMax (Velickovic et al., 2019), JOAO (You et al., 2021), GraphLoG (Xu et al., 2021), MGSSL Zhang et al. (2021) and D-SLA (Kim et al., 2022), (2) _semi-supervised learning methods_ including self-training with selected unlabeled graphs (ST-Real) and generated graphs (ST-Gen) and InfoGraph (Sun et al., 2020), and (3) _graph data augmentation (GDA) methods_ including FLAG (Kong et al., 2022), GREA (Liu et al., 2022a), and G-MixUp (Han et al., 2022). For self-supervised pre-training, we follow their own settings and directly use their pre-trained models if available. For semi-supervised learning methods and DCT, we use 113K QM9 (Ramakrishnan et al., 2014) and 306K PPI graphs (Hu et al., 2019) as unlabeled data sources for the tasks on molecules/polymers and proteins, respectively. For DCT, we tune three major hyper-parameters: the number of perturbation steps \(D\), the number of negative samples \(M\), and top-\(n\) % labeled graphs of lowest property prediction loss selected for data augmentation.

### Outstanding Property Prediction Performance

We report the model performance using mean and standard deviation over 10 runs Table 2. DCT is the best on all 15 tasks compared to the state-of-the-art baselines. Our observations are:

   Data Type & Dataset & \# Graphs & Prediction Task & \# Task & Avg./Max \# Nodes & Avg./Max \# Edges \\   & ogbg-HIV & 41,127 & Classification & 1 & 25.5 / 222 & 54.9 / 502 \\  & ogbg-ToxCast & 8,576 & Classification & 617 & 18.8 / 124 & 38.5 / 268 \\  & ogbg-Tox21 & 7,831 & Classification & 12 & 18.6 / 132 & 38.6 / 290 \\  & ogbg-BBBP & 2,039 & Classification & 1 & 24.1 / 132 & 51.9 / 290 \\  & ogbg-BACE & 1,513 & Classification & 1 & 34.1 / 97 & 73.7 / 202 \\  & ogbg-ClinFox & 1,477 & Classification & 2 & 26.2 / 136 & 55.8 / 286 \\  & ogbg-SIDER & 1,427 & Classification & 27 & 33.6 / 492 & 70.7 / 1010 \\  & ogbg-Lipo & 4200 & Regression & 1 & 27 / 115 & 59 / 236 \\  & ogbg-ESOL & 1128 & Regression & 1 & 13.3 / 25 & 27.4 / 124 \\  & ogbg-FreeSolv & 642 & Regression & 1 & 8.7 / 24 & 16.8 / 50 \\   & GlassTemp & 7,174 & Regression & 1 & 36.7 / 166 & 79.3 / 362 \\  & MeltingTemp & 3,651 & Regression & 1 & 26.9 / 102 & 55.4 / 212 \\  & ThermCond & 759 & Regression & 1 & 21.3 / 71 & 42.3 / 162 \\  & O\({}_{2}\)Perm & 595 & Regression & 1 & 37.3 / 103 & 82.1 / 234 \\  Proteins & PPI & 88000 & Classification & 40 & 49.4 / 111 & 890.8 / 11556 \\   

Table 1: Statistics of datasets for graph property prediction in different domains.

**(1) GIN is the most competitive baseline and outperforms self-supervised learning methods.** On 7 of 15 tasks, GIN outperforms all the 7 self-supervised learning methods. Because self-supervised pre-training imposes constraints on the model architecture, it undermines the true power of GNNs and under-performs the GNNs that are properly used.

**(2) Self-training and GDA methods perform better than GIN but cannot effectively learn from unlabeled data.** Self-training (ST-Real and ST-Gen) is often the best baseline in regression tasks. GDA (GREA and G-MixUp) methods outperform self-training in most classification tasks except ogbg-SIDER, because they are often designed to exploit categorical labeled data and remain under-explored for regression. Although self-training benefits from selecting unlabeled examples in some graph regression tasks, they are _negatively_ affected by the unlabeled graphs in the classification tasks such as ogbg-ToxCast and ogbg-ClinTox. As indicated in Figure 3, it is inappropriate to pseudo-label unlabeled graphs in self-training due to the huge gap between the unlabeled data and target task.

**(3) DCT transfers useful knowledge from unlabeled data by data augmentation.** DCT outperforms the best baseline relatively by +3.9%, +13.4%, and +10.2% when there are only 1,210, 513, and 4,303 training graphs on ogbg-BACE, ogbg-FreeSolv, and GlassTemp, respectively. Compared to the self-supervised baselines, the improvement from DCT is more significant, so the knowledge transfer is more effective. For example, on ogbg-FreeSolv and O\({}_{2}\)Perm, DCT performs better than the best self-supervised baselines relatively by +45.8% and +8.0%, respectively. On regression tasks that involve knowledge transfer across domains (_e.g.,_ from molecules to polymers), DCT reduces MAE relatively by 1.9% \(\) 10.2% compared to the best baseline. All these results demonstrate the outstanding performance of task-specific data augmentation in DCT.

    & & &  \\   & & & ogbg-TuxCast & ogbg-ToxCast & ogbg-Tox21 & ogbg-BBBP & ogbg-BACE & ogbg-ClinTox & ogbg-SIDER \\  & \# Training Graphs & 32,901 & 6,860 & 6,264 & 1,631 & 1,210 & 1,181 & 1,141 \\   & GIN & 77.4(1.2) & 66.9(2.2) & 76.0(0.6) & 67.5(2.7) & 77.5(2.8) & 88.8(3.5) & 58.1(0.9) \\   & EdgePred & 78.1(1.3) & 63.9(0.4) & 75.5(0.4) & 69.9(0.5) & 79.5(1.0) & 62.9(2.3) & 59.7(0.8) \\  & AttrMask & 77.1(1.7) & 64.2(0.5) & 76.6(0.4) & 63.9(1.2) & 79.3(0.7) & 70.4(1.1) & 60.7(0.4) \\  & ContextPred & 78.4(1.0) & 63.7(0.3) & 75.0(0.1) & 68.8(1.6) & 75.7(1.0) & 63.2(0.5) & 60.7(0.5) \\  & InproMax & 75.4(1.8) & 61.7(0.9) & 75.5(0.4) & 69.2(0.5) & 76.8(0.2) & 73.0(0.2) & 58.6(0.5) \\  & JOAO & 76.2(0.2) & 64.8(0.3) & 74.8(0.5) & 69.3(2.5) & 75.9(3.9) & 69.4(4.5) & 60.8(0.6) \\  & GraphLog & 74.8(1.1) & 63.2(0.8) & 75.4(0.8) & 67.5(2.3) & 80.4(3.6) & 69.0(0.6) & 57.0(0.8) \\  & MossSL & 77.1(1.1) & 65.7(0.4) & 77.2(0.3) & 66.9(0.9) & 81.3(2.4) & 69.8(5.9) & 63.6(1.0) \\  & D-SLA & 76.9(0.9) & 60.8(1.2) & 76.1(0.1) & 62.6(1.0) & 80.3(0.6) & 78.3(2.4) & 55.1(1.0) \\   & InfoGraph & 73.3(0.7) & 61.5(1.1) & 67.6(0.9) & 61.6(4.0) & 75.9(1.8) & 62.2(8.5) & 56.3(2.3) \\  & ST-Real & 78.3(0.6) & 64.5(1.0) & 76.2(0.3) & 66.7(1.9) & 77.4(1.8) & 82.2(2.4) & 60.8(1.2) \\  & ST-Gen & 77.9(1.6) & 65.1(0.1) & 75.8(0.9) & 66.3(1.5) & 78.4(0.0) & 87.3(1.9) & 59.3(1.9) \\   & FLAG & 74.6(1.7) & 59.9(1.8) & 76.9(0.7) & 66.6(1.0) & 79.1(1.2) & 85.1(3.4) & 57.6(2.3) \\  & GREA & 79.3(0.9) & 67.5(0.7) & 77.2(1.2) & 69.7(1.3) & 82.4(2.4) & 87.9(3.7) & 60.1(2.0) \\  & G-MixUp & 77.1(1.1) & 55.6(1.1) & 64.6(0.4) & 70.2(1.0) & 77.8(3.3) & 60.2(7.5) & 56.8(3.5) \\   & DCT (Ours) & **79.5(1.0)** & **68.1(0.2)** & **78.2(0.2)** & **70.8(0.5)** & **85.6(0.6)** & **92.1(0.8)** & **63.9(0.3)** \\   &  &  &  \\   & & ogbg-Lip & ogbg-ESOI & ogbg-FreeSolv & GlassTemp & MellingTemp & ThermCond & QPerm & PPI \\ \# Training Graphs & 3,360 & 902 & 513 & 4,303 & 2,189 & 455 & 356 & 60,715 \\   & GIN & 0.545(0.019) & 0.766(0.165) & 1.639(0.146) & 26.4(0.2) & 40.9(2.2) & 3.25(0.19) & 201.3(45.0) & 69.1(0.0) \\  & EdgePred & 0.585(0.009) & 1.062(0.086) & 2.249(0.150) & 27.6(1.4) & 47.4(0.3) & 3.69(0.50) & 207.3(41.7) & 63.7(1.1) \\  & AttMask & 0.57(0.3) & 1.014(0.014) & 1.952(0.088) & 27.7(0.3) & 45.8(2.6) & 3.17(1.2) & 19.9(0.0) & 64.1(1.0) \\  & ContextPred & 0.592(0.007) & 0.971(0.291) & 1.92(0.151) & 27.6(0.3) & 46.7(0.1) & 15.0(2.4) & 171.2(0.5) & 60.1(2.0) \\  & InfoMax & 0.581(0.009) & 0.935(0.018) & 2.197(0.129) & 27.5(0.3) & 46.5(2.9) & 3.31(0.25) & 231.0(0.5) & 63.3(3.2) \\  & JOAO & 0.596(0.109) & 1.098(0.070) & 2.465(0.085) & 27.5(0.3) & 46.0(0.2) & 3.55(0.29) & 207.7(4.7) & 61.5(1.5) \\  & GraphLog & 0.577(0.104) & 1.099(0.079) & 2.37(0.263) & 29.5(1.3) & 50.3(0.3) & 30.1(0.17) & 229.7(46.8) & 210.6(0.6) \\  & MossSL & 0.56(0.007) & 0.998(0.031) & 1.956(0.077) & 26.9(0.4) & 24.7(0.2) & 3.10(0.1) & 201.1(3.9) & N.A. \\  & D-SLA & 0.563(0.000) & 1.064(0.000) & 2.190(0.149) & 27.5(1.9) & 51.7(2.5) & 2.71(0.00) & 257.0(0.2) & 650.1(0.2) \\   & InfoGraph & 0.793(0.004) & 1.285(0.000) & 3.710(0.145) & 30.8(1.2) & 51.2(5.1) & 2.75(0.15) & 207.2(0.18) & 67.7(0.0) \\  & ST-Real & 0.52(0.000) & 0.788(0.000) & 1.770(0.251) & 26.6(0.3) & 42.3(1.2) & 2.64(0.07) & 256.0(0.175) & 68.9(0.1) \\   & ST-Gen & 0.531(0.0104) & 0.724(0.02) & 1.54(0.02) & 26.8(0.3) & 42.0(0.8) & 2.70(0.00) & 262.1(0.1) & 68.6(0.0) \\   & FLAG & 0.528(0.012) & 0.755(0.009) & 1.565(0.008) & 26.6(1.0) & 44.2(0.2) & 3.05(0.10) & 177.2(0.00) & 69.2(0.2) \\  & G GREA & 0.586(0.004) & 0.805(0.135) & 1.829(0.368) & 26.7(0.10) & 41.8(0.8) & 3.23(0.1

### Ablation Studies and Performance Analysis

**Comprehensive ablation studies:** In Table 3, we investigate how the task-related objectives in Eq. (5) impact the performance of DCT. First, DCT outperforms the top baseline even if the two task-related optimization objectives are disabled. This is because DCT generates new training examples based on original labeled graphs: the data augmentation has already improved the diversity of the training dataset a little bit. Second, adding the objective \(_{1}\) further improves the performance by encouraging the generation of diverse examples, because it minimizes the similarity between the original graph and augmented graph in the graph space. Third, we receive the best performance of DCT when it combines \(_{1}\) and \(_{2}\) objectives to generate task-related and diverse augmented graphs. When we change the unlabeled data source from QM9 to the ZINC dataset from (Jo et al., 2022), similar observations confirm the necessity of the task-related objectives.

**Effect of hyper-parameters:** The impacts of three hyper-parameters of DCT are studied: the number of perturbation steps \(D\), the number of negative samples \(M\) in Eq. (6), and the number of augmented graphs in each iteration (_i.e.,_ top-\(n\) % selected graph for augmentation). Results from Figure 4 show that DCT is robust to a wide range of \(D\) and \(M\) valued from 0 to 10. They suggest that \(D\) and \(M\) can be set as 5 in most cases. As for the number of the augmented graphs in each iteration, results show that noisy graphs are often created when \(n\) is higher than 30%, because the predictor cannot effectively guide the data augmentation for those labeled graphs whose labels are hard to predict. So, 10% is suggested as the default of top-\(n\)%.

**Iterative process:** Figure 5 investigates the relationship between the quality of augmented graphs and the accuracy of property prediction models. We save a predictor checkpoint every 20 epochs

Figure 4: Effect of hyper-parameters, including the number of perturbation steps \(D\), the number of negative graphs \(M\), and top-\(n\) % labeled graphs whose labels are predicted the most accurately and that are selected for data augmentation, where \(n\).

    &  &  &  \\   & \(_{1}(G^{},G)\) & \(_{2}(G^{},g)\) & BACE SIDER & FreeSolv & O\({}_{2}\)Perm \\   & 82.4(2.4) & 60.8(1.2) & 1.547(0.082) & 177.7(0.67) \\   &  & ✗ & ✗ & 84.4(2.6) & 63.7(0.3) & 1.473(0.192) & 177.4(27.3) \\  & & ✓ & ✗ & 85.2(1.3) & 63.7(0.2) & 1.415(0.145) & 171.4(14.0) \\  & & ✓ & ✗ & 84.7(1.8) & 63.8(0.5) & 1.344(0.096) & 172.6(02.9) \\  & & ✓ & ✓ & **85.6(0.6)** & **63.9(0.3)** & **1.339(0.075)** & **165.6(**24.3)** \\   &  & ✗ & ✗ & 82.8(1.8) & 63.5(0.7) & 1.524(0.219) & 175.5(11.9) \\  & & ✓ & ✗ & 83.3(2.2) & 63.5(0.7) & 1.455(0.207) & 172.4(0.08) \\   & & ✗ & ✓ & 84.3(0.6) & 63.5(0.6) & 1.514(0.214) & 171.5(26.0) \\   & & ✓ & ✓ & 84.9(0.4) & 63.7(0.7) & 1.408(0.092) & 169.3(15.3) \\   

Table 3: Comprehensive ablation studies for DCT on tasks ogbg-BACE, ogbg-SIDER, ogbg-FreeSolv, and O\({}_{2}\)Perm. Objectives include minimizing \(_{1}(G^{},G)\) and/or maximizing \(_{2}(G^{},y)\).

to guide the generation of the augmented examples. We evaluate the quality of augmented graphs by using them to train GIN and report AUC/MAE. The data augmentation gradually decreases the training loss of property prediction. On the other hand, the increased GIN performance indicates that the quality of augmented examples is also improved over epochs. The data augmentation and predictor training mutual enhance each other.

### Interpretability of Visible Knowledge Transfer

Knowledge transfer by data augmentation gives visible examples, allowing us to study what is learned. We visualize a few augmented graphs in DCT using ogbg-BACE and O\({}_{2}\)Perm. We adapt top-k pooling (Knyazev et al., 2019) to select the subgraphs that GIN used for prediction. The selected subgraphs are highlighted in green in Figure 6. The three examples show that _the augmented graphs can identify and preserve the core structures_ that GIN uses to predict property values. These augmented graphs are chemically valid, showing that _concepts such as some chemical rules from the unlabeled graphs are successfully transferred to downstream tasks_. More results are in appendix D.2. Regarding task-specific knowledge, it is known that the fluorine atom and the methyl group are usually negatively and positively correlated to the permeability, respectively (Park et al., 2003; Corrado and Guo, 2020). The augmented examples show that DCT _captures this domain knowledge with the task-related objectives_. In example (b), DCT replaces most of the fluorine atoms with the methyl groups. It encourages GIN to learn the positive relationship between the methyl group and the permeability so that GIN predicts a high label value. In example (c), DCT replaces the methyl groups with fluorine atoms. It encourages GIN to learn the negative relationship between the fluorine atom and the permeability so that GIN predicts a low label value.

## 5 Related Work

### Graph Property Prediction

Graph neural networks (GNNs) (Kipf and Welling, 2017; Xu et al., 2019) are commonly used for graph property prediction in chemistry and polymer informatics tasks (Otsuka et al., 2011; Hu et al., 2020; Zhou et al., 2022). However, it is hard to annotate enough labels in these domains. Recent work used _self-supervised tasks_ such as node attribute prediction and graph structure prediction (Hu et al., 2019; You et al., 2021; Kim et al., 2022) to pre-train architecture-fixed GNNs. Sun et al. (2022) observed that the existing methods might fail to transfer knowledge from unlabeled graph data. Flexible GNN architectures for downstream tasks would be desirable.

Figure 5: Data augmentation and model training mutually enhance each other over epochs. The predictor is saved every 20 epochs to guide the generation of augmented graphs. The performance of GIN trained on these augmented graphs reflects the quality of the augmented data.

Figure 6: Case studies of augmented graphs. The green highlighted subgraphs are from GIN with top-k pooling. Examples show that the augmented graphs from DCT preserve the core structures of original graphs. Key concepts in the unlabeled graphs like chemical validity are transferred to downstream tasks. Domain knowledge such as the relationship between the permeability and the fluorine atom/methyl group is captured to guide task-specific generation.

_Graph data augmentation_ (GDA) methods do not restrict GNN architecture choices to improve prediction accuracy (Trivedi et al., 2022; Zhao et al., 2022, 2023; Ding et al., 2022). They learn to create new examples that preserve the properties of original graphs (Liu et al., 2022b, a; Kong et al., 2022; Han et al., 2022; Luo et al., 2022). However, they purely manipulate labeled examples and thus _cannot utilize the knowledge in unlabeled graphs_. Our DCT combines the knowledge from the unlabeled dataset and the labeled task dataset. It creates label-preserved graph examples with the knowledge transferred from the unlabeled data. It allows the GNN models to have flexible architectures.

### Learning from Unlabeled Data

_Pre-training on self-supervised tasks_ such as masked image modeling and autoregressive text generation is effective for large language and vision models (Brown et al., 2020; He et al., 2022). However, the hand-crafted self-supervised tasks could hardly help models learn useful knowledge from unlabeled graphs _due to the gap between these label-agnostic tasks and the downstream prediction tasks_ towards drug discovery and material discovery (Sun et al., 2021; Kim et al., 2022; Inae et al., 2023). A universal self-supervised task to learn from the unlabeled graphs remains under-explored (Sun et al., 2022; Trivedi et al., 2022).

_Semi-supervised learning_ assumes that unlabeled and labeled data are from the same source (Liu et al., 2023). The learning objective in the latent space is usually mutual information maximization that encourages similarity between the representations of unlabeled and labeled graphs (Sun et al., 2020). However, _the distributions of the unlabeled and labeled data could be very different_ due to the different types of sources (Hu et al., 2019), leading to negative impacts on the property prediction on the labeled graphs. _Self-training_, as a specific type of semi-supervised learning method, selects the unlabeled graphs of confidently predictable labels and assigns pseudo-labels for them (Lee et al., 2013; Iscen et al., 2019). Many studies have explored improving uncertainty estimation (Gal and Ghahramani, 2016; Tagasovska and Lopez-Paz, 2019; Amini et al., 2020) to help the model filter out noise for reliable pseudo-labels. Recently, pseudo-labels have been applied in imbalanced learning (Liu et al., 2023) and representation learning (Ghiasi et al., 2021). However, self-training is restricted to confidently predictable labels and may ignore the huge number of any other unlabeled graphs (Huang et al., 2022). Therefore, it cannot fully utilize the knowledge in the unlabeled graphs.

In contrast, our DCT employs a diffusion model to extract knowledge (as the diffusion and reverse processes) from _all the unlabeled graphs_. DCT represents the knowledge as task-specific labeled examples to augment the target dataset, instead of uninterpretable pre-trained model parameters. We note that self- or semi-supervised learning does not conflict with DCT, and we leave their combinations for future work.

### Diffusion Models on Graphs

Recent works have improved the diffusion models on graphs (Niu et al., 2020; Jo et al., 2022; Vignac et al., 2022; Kong et al., 2023; Chen et al., 2023). EDP-GNN (Niu et al., 2020) employed score matching for permutation-invariant graph data distribution. GDSS (Jo et al., 2022) extended the continuous-time framework (Chen et al., 2023) to model node-edge joint distribution. DiGress (Vignac et al., 2022) used the transition matrix to preserve the discrete natures of the graph structure. GraphARM (Kong et al., 2023) introduced a node-absorbing autoregressive diffusion process. EDGE (Chen et al., 2023) focused on efficiently generating larger graphs. Instead of improving the generation performance of the diffusion model, our model builds on score-based diffusion models (Jo et al., 2022; Song et al., 2021) for predictive tasks, _i.e.,_, graph classification and graph regression.

## 6 Conclusion

In this work, we made the first attempt to transfer minimal sufficient knowledge from unlabeled graphs by data augmentation. We proposed a data-centric framework to use the diffusion model trained on the unlabeled graphs and use two task-related objectives to generate task-specific augmented graphs. Experiments demonstrated the performance of the proposed framework through visible augmented examples. It is better than self-supervised learning, self-training, and graph data augmentation methods on as many as 15 tasks.