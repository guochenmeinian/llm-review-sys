# Matrix Compression via Randomized Low Rank

and Low Precision Factorization

 Rajarshi Saha, Varun Srivastava, Mert Pilanci

Department of Electrical Engineering

Stanford University

Stanford, CA 94305, USA

{rajsaha,vsriva,pilanci}@stanford.edu

###### Abstract

Matrices are exceptionally useful in various fields of study as they provide a convenient framework to organize and manipulate data in a structured manner. However, modern matrices can involve billions of elements, making their storage and processing quite demanding in terms of computational resources and memory usage. Although prohibitively large, such matrices are often approximately low rank. We propose an algorithm that exploits this structure to obtain a low rank decomposition of any matrix \(\) as \(\), where \(\) and \(\) are the low rank factors. The total number of elements in \(\) and \(\) can be significantly less than that in \(\). Furthermore, the entries of \(\) and \(\) are quantized to low precision formats - compressing \(\) by giving us a low rank and low precision factorization. Our algorithm first computes an approximate basis of the range space of \(\) by randomly sketching its columns, followed by a quantization of the vectors constituting this basis. It then computes approximate projections of the columns of \(\) onto this quantized basis. We derive upper bounds on the approximation error of our algorithm, and analyze the impact of target rank and quantization bit-budget. The tradeoff between compression ratio and approximation accuracy allows for flexibility in choosing these parameters based on specific application requirements. We empirically demonstrate the efficacy of our algorithm in image compression, nearest neighbor classification of image and text embeddings, and compressing the layers of LlaMa-7b. Our results illustrate that we can achieve compression ratios as aggressive as one bit per matrix coordinate, all while surpassing or maintaining the performance of traditional compression techniques.

## 1 Introduction

Low-rank structures for matrices have proven to be incredibly valuable and ubiquitous across numerous fields of study. Several real-world matrices approximately exhibit low-rank structure due to inherent redundancy or patterns, allowing them to be approximated using low-rank factors. Udell and Townsend  provide a potential justification by considering a generative model with latent variables for real-world matrices. Applications where low-rank structures in matrices are exploited include, but are not limited to, imaging (Lingala et al. ), fine-tuning large language models (Aghajanyan et al. , Hu et al. , Karimi Mahabadi et al. , Valipour et al. , Wang et al. ), compressing neural networks (Ben Noach and Goldberg , Idelbayev and Carreira-Perpinan , Mao et al. , Phan et al. , Swaminathan et al. , Tahaei et al. , Wang et al. , Winata et al. , Yu et al. ), obtaining efficient NN architectures (Jaderberg et al. , Tai et al. ), etc.

Given a matrix \(^{n d}\), a low-rank approximation is given by \(\), where \(^{n m}\), \(^{m d}\) denote the _left_ and _right_ low-rank factors with \(m\{n,d\}\). Since \(m(n+d) nd\)the total number of entries in \(\) can be significantly smaller than \(\), enabling matrix compression. The need for compression is driven by the overwhelming storage demands and computational complexity linked to large matrices. In a comparable but distinct line of work, low-precision (LP) representations have also been studied extensively as a means to reduce the memory footprint. Quantization of a continuous valued variable using a small number of bits reduces storage requirement while trading off accuracy. In addition, they also facilitate low latency for real-time inference and low energy consumption (see Gholami et al. ). Works of Alimisis et al. , Safaryan et al.  have studied various matrix compression operators for distributed optimization.

In this work, we introduce **LPLR**: _Low-Precision Low-Rank_ factorization - a general matrix compression algorithm that simultaneously exploits the low-rank structure of a matrix and quantizes it, to obtain a low-rank factorization, such that the elements of the factors are represented using a small number of bits. Although LPLR can generically refer to a class of algorithms whose main goal is to obtain low-precision representations and exploiting low-rank structures in matrices, in the rest of the paper, we reserve this acronym for our proposed algorithm in Alg. 1. Fig. 1 shows the effectiveness of LPLR in preserving the semantic features in image compression. Other algorithms designed towards achieving the same objective have been named and described appropriately throughout the paper.

### Related Works

**Low-rank approximation**: The optimal rank-\(k\) approximation of a matrix \(^{n d}\), denoted as \(_{k}\), can be obtained by performing the singular value decomposition (SVD) of \(\), which gives \(=^{}\). To obtain \(_{k}\), we keep only the top \(k\) singular values along with their corresponding left and right singular vectors, setting the remaining singular values to zero. In other words, \(_{k}=_{k}_{k}_{k}^{}=_{i =1}^{k}_{i}_{i}_{i}^{}\), where \(_{i}\), \(_{i}\), and \(_{i}\) represent the \(i\)-th singular value, left singular vector, and right singular vector, respectively. The matrix \(_{k}\) minimizes the Frobenius norm \(\|-}\|_{}\) under the constraint that the rank of \(}\) is at most \(k\). This result is known as the Eckart-Young-Mirsky theorem , which states that \(\|-_{k}\|_{}^{2}=_{i>k}_{i}^{2}\). However, computing the SVD has a high computational complexity of \((nd^{2})\) (\(n d\) without loss of generality), which can be impractical for large matrices. Therefore, alternative methods such as in Jin et al. , Ye and Du , Zhang et al.  have been proposed which solve variants of the minimization problem. Chi et al.  provide a survey of optimization-based approaches for obtaining low-rank approximation.

Along a parallel line of work, randomized low-rank factorization algorithms have demonstrated their effectiveness in handling massive datasets efficiently while maintaining competitive accuracy levels. Several works, including those by Derezinski et al. , Drineas and Mahoney , Drineas et al. , Halko et al. , Ma and Solomonik , Mahoney , Martinsson et al. , Tropp et al. , Witten and Candes  have focused on reducing the complexity of computing SVD to approximately \((nm^{2})\), where \(m d\). These algorithms aim to approximate the range space of matrix \(\) by utilizing a sketched version \(\), where \(^{d m}\) is a random matrix with \(m d\). The columns of \(\) are then projected onto this approximate range space. By choosing \(m=k+p\), where \(p 2\) is a small integer, the resulting low-rank approximation \(\) provided by these algorithms can be proven to be within a small multiplicative factor of the optimal rank-\(k\) approximation, stated as \(\|-\|_{}^{2}(1+ )\|_{k}-\|_{}^{2}\). The sketching matrix \(\) is typically selected from the

Figure 1: Compression of Shepp-Logan phantom (a standard test image for medical image reconstruction). Naive quant. was done with \(2\)-bits per pixel of this \(10^{3} 10^{3}\) image. Quantizing the SVD factors “directly” (i.e., DSVD) and (our) LPLR/LSVD algorithms, factorize the image into a product of tall & wide matrices which reduces the total number of elements, allowing each entry to be represented using upto \(8\)-bits of precision per pixel. Despite the increase in precision per pixel, the total number of bits remains the same at \(2 10^{6}\).

widely used class of Johnson-Lindenstrauss (JL) embeddings. A popular choice when \(\) is dense is to sample a Gaussian matrix where each entry \(S_{ij}(0,)\).

**Randomized quantization**: JL embeddings also have practical uses in vector quantization. In this context, when quantizing a vector \(^{d}\), instead of quantizing \(\) directly, the encoder quantizes \(\), and then the decoder obtains an approximation of \(\) through \(^{}()\). This randomized transformation equalizes the coordinate values of \(\), allowing for a smaller range of values and higher precision for the quantizer Q under a fixed bit-budget. As a result, it leads to a smaller \(_{2}\) error compared to independently quantizing each coordinate in a naive manner. Works such as Chen et al. , Lyubarskii and Vershynin , Mayekar and Tyagi , Safaryan et al. , Saha et al. [56; 57; 58], Studer et al. , Suresh et al. [61; 62], Vargaftik et al. [71; 72], Young et al.  explore different variations of this concept for different applications.

## 2 Proposed Algorithm

Prior to detailing our algorithm, we first review the characteristics of the _uniformly dithered quantizer_ that we employ within our approach. Uniformly dithered quantization has been utilized in various prior works Alistarh et al. , Bernardo et al. , Gray and Stockham , Mayekar and Tyagi , Suresh et al. [61; 62], and is succinctly described in SS(2.1) below.

### Uniformly dithered quantizer

Let us consider quantizing a scalar \(x\) with \(|x|\). Given a _bit-budget_ of \(\) bits, the scalar quantizer with _dynamic range_\(\) is described by first specifying the \(M=2^{}\) quantization points as:

\[q_{1}=-,q_{2}=-+,q_{3}=-+2,,q_ {M}=-+(M-1).\]

Here, the resolution is given by \(=}{M-1}\), and the quantizer operation is defined as:

\[_{,}(x)=q_{k+1}&\ \ r,\\ q_{k}&\ \ 1-r,\] (1)

where \(k=_{j}\{q_{j} x\}\), i.e., \(x[q_{k},q_{k+1})\), and \(r=}{}\). Such a quantizer satisfies

\[[_{,}(x)]=x\ \ \ \ \ \ (_{,}(x)-x)^{2}}{4}=^{2}}{(2^{}-1)^{2}},\] (2)

i.e., it is unbiased and the quantization error variance is dictated by \(\) and \(\). Here, the \(()\) is over the randomness from dithering in (1) (ref. App. C). If the input \(x\) to the quantizer falls outside this range, i.e., \(x>\) or \(x<-\), the quantizer is said to be _saturated_. Finally, to quantize any matrix \(\), we obtain \(_{,}()\) by quantizing each entry independently, i.e., \([_{,}()]_{ij} _{,}(X_{ij})\).

### Proposed algorithm: LPLR

In order to comprehend low precision and low rank representations, we first introduce a simple strategy, which we name as _direct-SVD quant_. This method involves two main steps: It first computes the optimal rank-\(k\) decomposition \(_{k}=_{k}_{k}_{k}^{}\), and then, it quantizes the low-rank factors independently, namely, \(=(_{k}_{k})\) and \(=^{}(_{k}^{})\). Here, Q and \(^{}\) are uniform scalar quantizers with respective bit-budgets \(\) and \(^{}\). A detailed analysis of this direct-SVD quantization approach can be found in Appendix H, along with pseudocode provided in Algorithm 2. Despite its simplicity, this is not the optimal strategy due to two reasons. Firstly, it necessitates computing the SVD of \(\), which is computationally expensive at \((nd^{2})\). Secondly, let us consider improving the approximation to \(\) when the first factor is fixed to \((_{k})\) by solving the optimization problem:

\[^{*}=*{arg\,min}_{^{k d}} \|(_{k})-\|_{}^{2 }=(_{k})^{}.\] (3)

This computes the projection of the columns of \(\) onto the range space of \((_{k})\). Suppose the resulting projection coefficients are further quantized to get \(^{}((_{k})^{})\). Since \(^{*}\) is the solution of (3), for a sufficiently large value of \(^{}\), it is evident that \(\|(_{k})^{}((_{k} )^{})-\|_{}^{2}\|(_{k}_{k})^{}(_{k}^{} )-\|_{}^{2}\), and hence, better than direct-SVD quant. However, this approach of projecting onto the range space of \((_{k})\) (which we refer to as LPLR-SVD and analyze in App. I), still requires the computation of \(_{k}\), so we can replace \(_{k}\) by \(\), i.e., an approximation of the range space obtained through random linear combinations of the columns of \(\) (also known as a randomized rangefinder ). This leads us to our **LPLR** algorithm, described in Alg. 1, which finds \(^{*}=_{^{n d}}\|( )-\|_{}^{2}\) and forms the low-rank low-precision approximation \(()^{}(^{*})\) where \(,^{}\) are quantization operators. While the solution of this problem is available in closed form as \(^{*}=()^{}\), one can also use an approximation of \(^{*}\) by solving this least-squares minimization using an iterative method such as conjugate gradient descent.

In addition to the above argument supporting the superiority of LPLR compared to other baselines (ref. to Tabs. 1 and 2), there exists another essential reason why LPLR outperforms them. This reason directly relates to the selection of \(\) as a Gaussian matrix, which is an integral component of LPLR. Random Gaussian matrices are JL embeddings and possess an equalization property that enhances the precision of uniform quantizers. In particular, let us consider an arbitrary vector \(^{d}\) and obtain an estimate as \(}=^{}()\) using a uniform quantizer \(\). It can be shown that the vector quantization error remains constant and does not grow with the dimension \(d\), expressed as \(\|}-\|_{2}^{2}=(1)\). This represents a substantial improvement compared to the naive strategy of independently quantizing each coordinate of \(\), which leads to a quantization error growth rate of \((d)\). We provide a detailed explanation of this phenomenon in App. D. Furthermore, even when the quantization is \(1\)-bit per coordinate, e.g., \(()=()\), this embedding provides strong near-isometric embedding properties due to the properties of random hyperplane tessellations .

While the strong equalization property of Gaussian embeddings is a known result, certain works such as Saha et al. [57; 58], Suresh et al. [61; 62] opt for using randomized Hadamard embeddings instead of Gaussian ones. The reason behind this choice is twofold: (i) Gaussian matrices are dense, requiring \((d^{2})\) multiplications when computing \(\), and (ii) the entries of \(\) are floating point numbers that must be stored in full precision, contradicting the objective of quantizing \(\) using fewer bits. However, these concerns are not problematic for **LPLR** because the effects of \(\) in the first low-rank factor are neutralized by the second low-rank factor, and \(\) does not need to be stored. In fact, we exploit both the equalization property and the subspace approximation property of Gaussian matrices to derive a superior upper bound for the approximation error, as discussed next in SS3.

## 3 Approximation Error Analysis

We are now in a position to state the approximation error guarantee of **LPLR** Let us denote the \(i^{}\) row of our input matrix \(\) as \(^{(i)}\). For convenience of analysis, we make the following assumption.

**Assumption 3.1**.: Rows of matrix \(\) have bounded norm, i.e., \(\|^{(i)}\|\) for some known \(>0\).

The following result gives an informal upper bound on the expected Frobenius norm error of the factorization returned by Alg. 1.

**Theorem 3.2**.: **LPLR approximation error (Informal)** _Suppose our input matrix \(^{n d}\) with \(\|^{(i)}\|=(1)\) has singular values \(_{1},,_{r}\) with \(r=()\) and target rank as \(k\). Let \(()=_{1}/_{r}\) and \((_{k})=_{1}/_{k}\) respectively be the condition numbers of \(\) and the best rank-\(k\) approximation of \(\), and let us denote \(=\{(),(_{k})(1-c_{4} _{k+1}/_{k})^{-1}\}\). Furthermore, for a sufficiently small constant \(>0\), suppose the dynamic range of \(\) is set to be \(c_{1}\)_and that of \(^{}\) is set to \(2\). Then, the_ **LPLR** _factorization returned by Alg. 1 satisfies_

\[\|-\|_{}^{2} (1+)\|_{k}-\|_{ }^{2}+,\]

_while utilizing a total budget of \(_{2}((_{k})}{}}}{})})\) bits for \(n d\). Here, \(c_{1}\), \(c_{2}\), \(c_{3}\), and \(c_{4}\) are constants that depend on \(\)._

We provide a less formal version of our main result here, suitable for interpretation. The formal statement of this result, including specific constant values, can be found in Thm. G.2 of App. G. It does not necessitate the assumption \(n d\) and provides distinct thresholds for \(\) and \(^{}\). Thm. G.2 asserts that, for a target rank-\(k\), as long as \(m k+2\), one can ensure an arbitrarily small approximation error of \(\) by selecting the number of bits to be at least above a certain threshold budget. The threshold depends on the error tolerance \(\), dimensions \(n\) and \(d\), the sketch size \(m\), and the spectrum of \(\). The value of \(\) is determined by taking the smaller of two quantities. In the case of matrices with a sharp decline in singular values (e.g., matrices of exact rank-\(k\)), where the ratio \(_{k}/_{k+1}\) approaches zero, \((_{k})\). For matrices with a smoother spectrum (e.g., all singular values are equal), \(=()\), the condition number of the input matrix \(\).

**Remark 1**.: We consider two distinct scenarios based on Asm. 3.1. The first case assumes that the row norms are bounded by a constant, represented by \(=(1)\). This assumption is reasonable when the rows of \(\) correspond to different normalized features of a data point. The second case assumes that the individual entries of \(\) are bounded by a constant, i.e., \(A_{ij}=(1)\). This implies that \(=()\), which is a reasonable assumption for scenarios like images, where it is known that each pixel value is bounded. In Tab. 1, we compare the performance of the algorithms when \(=(1)\), while in Tab. 2, we assume \(=()\). Thm. 3.2 assumes that \(=(1)\). The expressions in Tab. 2 can be obtained in a similar manner from the formal version in Thm G.2.

### Analysis outline

The derivation of the upper bound on the approximation error of LPLR is presented in App. G. In this section we outline a brief proof sketch that highlights the main challenges of the proof. As mentioned already in SS2.2, the analysis of LPLR utilizes the subspace embedding and equalization properties of random Gaussian matrices. A key component in Alg. 1 is the choice of dynamic range for quantizers \(\) and \(^{}\). In our analysis, we assume that when either \(\) or \(^{}\) gets saturated in lines \(3\) and \(5\) of Alg. 1, a trivial factorization of \(=\) is returned. We choose the dynamic ranges \(_{}\) and \(_{^{}}\) to be sufficiently high enough so that this happens with a very low probability. Formally, for quantizer \(\), Lemma E.1 states the following:

\[\|\|_{}^{2}n^{2}m}{})}{m}}1-^{2}}.\]

Here, \(\|\|_{}\) is max-norm of the matrix \(\), i.e., the coordinate with maximum magnitude. This concentration result is a consequence of the equalization property of Gaussian matrix \(\).

On the other hand, the input to the second quantizer is \(()^{}\) and Lemma E.2 provides a concentration result for the max-norm of this matrix. Although in a general worst-case scenario, the coordinate values of the pseudo-inverse of a matrix with small entries can be large (Alon and Vu ), because we compute the pseudo-inverse of the matrix \(\), which is rectangular (\(m n\)) and with random Gaussian entries, \(\|()^{}\|_{2}\) does not shoot up arbitrarily as shown by Rudelson and Vershynin . We then show that \(\|()^{}\|_{2}\) is not too far from \(\|()^{}\|_{2}\), allowing us to derive an expression for \(_{^{}}\). We get for \(=\) and \(t=^{2}}{})}{m}}\),

\[\|()^{}\|_{}-1-t}1-^{2}}.\]

The second part of the analysis deals with upper bounding the approximation error when the second low-rank factor is unquantized, i.e., \(\|()()^{} -\|_{}^{2}\) conditioned on the event that \(\) and \(^{}\) are unsaturated. We reduce this problem to analyzing the solution of the following:

\[}=*{arg\,min}_{}\| ^{}_{k}-()\|_{ }^{2}.\] (4)We refer to (4) as the _sketched least squares problem with quantized response_ as it is a variant of the generalized least squares problem, \(^{*}=*{arg\,}_{}\|^{} _{k}-\|_{}^{2}\). This is potentially a problem of independent interest, and we analyze the solution of (4) in detail in App. F. Exploiting the subspace embedding property of \(\) we show that

\[\|^{*}_{k}-\|_{}^{2} \|}^{}_{k}- \|_{}^{2}\|^{*} _{k}-\|_{}^{2}+.\]

This leads us to the proof of Lemma G.1 which gives the approximation error of LPLR when \(\) and \(^{}\) are unsaturated. Finally, taking into account the low-probability saturation events, for which the error is \(\|\|_{}^{2}\), we derive our main result in Thm. G.2. Subsequently, we discuss the approximation made in App. G.2 and arrive at the informal result of Thm. 3.2.

### Comparison with baselines

We are now in a position to compare the performance with baselines in Tabs. 1 and 2.

**Naive quantization**: The most straightforward baseline for matrix quantization is naive quantization where each coordinate of the matrix is quantized independently, agnostic to any low-rank structure in the matrix \(\). In this, we allocate \(\) bits to each coordinate of \(\) and since there are \(nd\) entries in the matrix, from (1), the Frobenius norm error is upper bounded by \(^{2}nd}{(2^{B}-1)^{2}}\). Note that this holds true irrespective of whether \(=(1)\) or \(()\) because \(\|^{(i)}\|\) implies \(A_{ij}\). To ensure that the error is within a certain tolerance \(\), we then require \(_{2}()\) bits. In this, and also other expressions for bit-budget requirements of algorithms in Tabs. 1 and 2, we have ignored the multiplicative constant factors inside the \(_{2}()\) for simplicity of exposition. The exact expressions can be found in the corresponding appendices where we derive them.

One of the primary reasons why both direct-SVD and LPLR are expected to perform better than naive is that the former strategies exploit the low-rank structure of the matrices to reduce the total number of parameters being quantized, i.e., \(k(n+d)\) for direct-SVD and \(m(n+d)\) for LPLR, vs. \(nd\) for naive. Given a total bit-budget for the entire matrix \(\), since we now quantize fewer parameters than before, we can allocate a higher number of bits to each parameter, enabling higher precision. The price we pay for exploiting the low-rank structure of matrices is the additional \(\|_{k}-\|_{}^{2}\) dependent term, which is usually very small for matrices that can be well approximated by a low-rank structure. For matrices that are exactly rank \(k\), this term is \(0\). As we see in our numerical simulations in SS4, several real-world matrices can be well-approximated by a low-rank structure.

**Direct-SVD quant.**: From Tab. 1, we see that to achieve an \(\)-quantization error, direct-SVD requires \(_{2}(k)\) bits per entry, which is greater than \(_{2}(})\) required by LPLR (ignoring the

 Algorithms & Approximation error & Bit-budget (per entry) & Computation \\  Naive uniform & \(\) & \(_{2}()\) & \((nd)\) \\ Direct-SVD & \(\|_{k}-\|_{}^{2}+\) & \(_{2}()\) & \((nd^{2})\) \\
**LPLR** (_ours_) & \((1+)\|_{k}-\|_{}^{2}+\) & \(_{2}(_{k})}{}}d^{2}}{})})\) & \((ndm)\) \\   

Table 1: Comparison with baselines (row-norm bound is constant, i.e., \(\|^{(i)}\|=(1)\)). \(k,m\{d,n\}\). \(n\): no. of rows, \(d\): no. of columns, \(m\): sketch size, \(\): error tolerance, \(=k/(m-k-1)\). The expressions for bit-budget (per entry) ignores constant multiplicative factors inside the \(_{2}()\). We assume \(n d\).

 Algorithms & Approximation error & Bit-budget (per entry) & Computation \\  Naive uniform & \(\) & \(_{2}()\) & \((nd)\) \\ Direct-SVD & \(\|_{k}-\|_{}^{2}+\) & \(_{2}()\) & \((nd^{2})\) \\
**LPLR** (_ours_) & \((1+)\|_{k}-\|_{}^{2}+\) & \(_{2}(_{k})}{}nmd^{2}}{})})\) & \((ndm)\) \\   

Table 2: Comparison with baselines (individual entries of \(\) are bounded by a constant, i.e., \(A_{ij}=(1)\)). Dimension dependent terms are color highlighted for ease of comparison with Tab. 1.

logarithmic terms). Evidently, LPLR demands fewer bits than direct-SVD because \(k,m\{n,d\}\) for inherently low-rank matrices. For the regime presented in Tab. 2, the bit requirement for direct-SVD remains unchanged. However, LPLR now requires \(_{2}(nm)\), slightly more than direct-SVD, due to the additional \(\) factor. Thus, it makes sense to expect that direct-SVD can perform better in this regime. This is supported by our numerical experiments in Tabs. 4 to 7, where direct-SVD indeed outperforms LPLR in certain scenarios. Nevertheless, it is crucial to emphasize that direct-SVD necessitates computing the SVD, which can be prohibitive for very large matrices due to the current memory limitations of available GPUs, making LPLR the only viable option.

**Computational complexity**: Unsurprisingly, naive quant. requires the least computation, i.e., \((nd)\), as it just does a single pass over all the elements of \(\). The \((nd^{2})\) complexity of direct-SVD quant. stems from the requirement of computing SVD (assuming \(n d\)). LPLR is the best of both worlds - for the same bit-budget, LPLR has a smaller approximation error than both direct-SVD and naive, and a complexity of \((ndm)\), arising from the requirement to compute \(\), i.e., a product of two dense matrices of dimensions \(n d\) and \(d m\), which is better than direct-SVD, since \(m d\).

## 4 Numerical Simulations

### Overview

We evaluate the robustness of LPLR on multiple tasks, namely, image compression, binary, and multi-class classification across disparate domains including vision, text and raw images, and neural network weight matrices. We consider a range of input configurations to showcase the performance and non linear effects of joint quantization and low rank approximation on a given dataset, especially at lower bit budgets. LPLR provides competitive results at bit budgets as low as a single bit, providing extreme model compression while maintaining non trivial performance for the task at hand.

**Baselines.** We employ naive quantization, which quantizes the input matrix by rounding to the nearest scalar in the underlying data type's quantization grid, as our primary benchmark. Naive quant. and its variants - Dettmers et al. , Yao et al. , are the most popular method in use across domains, as their memory and computational run time requirements scale extremely well with model and dataset sizes. In addition, we also evaluate the performances of direct-SVD quantization and LPLR-SVD to disambiguate between the entangled effects of quantization and exploiting low rank structure.

**Metrics.** We evaluate LPLR performance using task specific goodness of fit metrics, as well as relative Frobenius norm error between the original and matrix reconstructed from its low-rank factors, i.e., \(-_{}^{2}\). In addition to this, we enforce parity between the number of bits used by all quantization schemes, so that the total space required (in bits) for storing the approximated matrix is _identical_ across LPLR, LPLR-SVD, direct-SVD quant., and naive quant.

**Notation.** In all our experiments, we denote the bit budget for the left low rank factor \(\) as \(\), for the right low rank factor \(\) as \(^{}\), and the corresponding bit budget for naive quantization as \(_{}\). For simplicity, we maintain equal bit budgets \(=^{}\) for both quantizers \(\) and \(^{}\). Wherever necessary, we also abbreviate direct-SVD quant. as DSVD, LPLR-SVD as LSVD, and naive quant. as NQ.

The main algorithm is implemented in Pytorch (Paszke et al. ), and utilizes Hugging Face  implementations of all datasets and large language models. All experiments were performed on a single GPU NVIDIA TITAN RTX. Further simulations and experimental details can be found in App. J. Our code is available at https://github.com/pilancilab/matrix-compressor.

### Image Compression

Image compression is a prototypical application of low rank matrix compression, as images are known to be significantly rank deficient in many practical scenarios (Zhang et al. , Zhou et al. ). In this task, we apply LPLR on \(1000 1000\) dimensional Shepp Logan phantom images from Gach et al. . These are a set of synthetic 2D images designed to simulate the typical characteristics and structures found in computed tomography (CT) scans. They consist of geometric shapes, including circles and ellipses, representing different tissues or organs within the scanned object.

The main results are summarized in Tab. 3. To ensure a fair comparison, we adjust the sketch size/target rank so that bit budgets are identical between naive quant. and LPLR. This allowsus to preserve the original datatype of the image (consequently a large dynamic range), while substantially reducing the pixels used for representing the image to as low as \(1\) bit per pixel (on average). Specifically, in Figure 1, we can observe the least visual distortion in the case of LPLR, which preserves _critical_ semantic features of the images, such as the small ellipses. It is clear that LPLR outperforms both techniques at lower naive quantization bit budgets. We attribute the better visual and quantitative performance to the higher dynamic range available to LPLR as well as structure preserved in the low rank decomposition.

### Embeddings extracted from pre-trained models

The efficacy of pre-trained embeddings is well established in vision (Li et al. , Parisi et al. ), text (Qi et al. , Rezaeinia et al. ) for rapid feature computation as an input to a variety of downstream tasks. Embeddings also play a crucial role in a number of software applications, including but not limited to, open source vector search libraries (Liu , Marqo ), semantic search engines (Amazon AWS ), vector databases (Pinecone ). Since most applications rely on proximity in "embedded space", it is essential that common operations on embeddings be computationally efficient. Specifically, one would like to optimize nearest neighbor (NN) searches which solve the optimization problem \(*{arg\,min}_{i}\|_{i}-\|_{2}^{2}\) (reducible to \(*{arg\,max}_{i}\) where \(\) is the matrix with training vectors \(\{_{i}\}\) as its rows, and \(\) is the query vector). The time complexity of NN search scales linearly with dimensions of \(^{n d}\) and number of neighbors (\(k\)). By embedding the data matrix in a dimension \(m d\), we directly speedup the run-time and reduce storage costs. Moreover, as datasets grow exponentially in size (especially document databases) and transfer learning becomes the dominant modality of training new models, embedding compression becomes a necessity for storing data without a corresponding exponential increase in hardware requirements.

#### 4.3.1 Embedding Classification

In this experiment, we evaluate several embeddings of standard datasets, namely CIFAR-10, CIFAR-100, IMDB and Emotion datasets. CIFAR-10 consists of 60,000 color images divided into 10 classes, with each class containing 6,000 images. The dataset is split into 50,000 training images and 10,000 test images, with a resolution of 32x32 pixels. CIFAR-100 increases the number of classes to 100 categories for an identical training and test size as CIFAR-10. The IMDB (mte ) dataset consists of 25,000 train and test sentences containing annotated binary sentiment labels for movie reviews, plot summaries and other rating information. The Emotion (mte ) dataset is a sentiment analysis dataset, containing 16,000 train and 2000 test sentences, each exemplifying a singular emotion, which represents the sentiment label for that sentence.

For CIFAR-10 and CIFAR-100, we embed the entire dataset using MobileNet v3 (Howard et al. ) pretrained on ImageNet (Deng et al. ) producing an embedding matrix of dimension \(60000 1024\), which we compress using LPLR and compare with the baselines in SS4.1. To evaluate the goodness of embeddings, we build a \(3\)-NN, a KNN Classifier using \(K=3\) nearest neighbors under Euclidean distance). We report the performance of the model using standard classification metrics - classification accuracy and weight averaged F1 score. We utilize a uniform bit budget \(=^{}=8\) bits for the quantizers \(,^{}\) across all cases. Tabs. 4 and 5 present our results under this setup. For each case we benchmark absolute performance using a \(3\)-NN classifier on the training set.

   &  & _{}\)} &  &  &  &  \\  &  & & & & & \\ 
32 & 15 & 1 & 0.610 & 0.553 & **0.506** & 0.532 \\ 
28 & 17 & 1 & 0.557 & 0.546 & **0.490** & 0.532 \\ 
24 & 20 & 1 & 0.540 & 0.537 & **0.454** & 0.532 \\ 
20 & 25 & 1 & 0.485 & 0.529 & **0.426** & 0.532 \\ 
16 & 31 & 1 & 0.447 & 0.523 & **0.391** & 0.532 \\ 
12 & 41 & 1 & 0.402 & 0.518 & **0.360** & 0.532 \\ 
8 & 62 & 1 & 0.340 & 0.508 & **0.326** & 0.532 \\  

Table 3: Comparison of **LPLR** and **LPLR-SVD** (**LSVD**) Frobenius norm errors with baselines, for different input LPLR bit budgets. Each triplet \(,B_{})}\) of configurations has an **identical compression ratio**. Here, \(=^{}\). The second column specifies the sketch size \(m\) for LPLR, and target rank \(k\) for DSVD or LSVD. We provide results for input bit budgets at a finer granularity to identify regimes where naïve quant. is outperformed.

Similarly, we embed text sentences from IMDB and Emotion databases using BeRT (Devlin et al. ) into \(512\) dimensional vectors, and construct a \(3\)-NN classifier using Euclidean distance to perform binary and multi-class classification on the respective embeddings, and report classification metrics in Tabs. 6 and 7. We see that LPLR outperforms direct-SVD quant. and naive quant. at lower bit budgets, and has performance parity as we increase \(_{_{0}}\) to \(4\) bits. We find that we match (and even exceed) the unquantized benchmark at single bit precision, which we attribute to the dominating low rank factorization, and its regularizing effect on data under extreme rank constraints. It is important to note that performance parity with direct-SVD quant. is also a successful outcome, since LPLR provides runtime improvements over taking an SVD to compress the data.

#### 4.3.2 Compressing Weight Matrices of a Large Language Model

In this section, we present results on a major application of matrix compression - compressing the weight matrices of deep neural networks. We choose LlaMa by Touvron et al. , a popular foundation Large Language Model (LLM) as the network of our choice. LLMs are a natural candidate for matrix compression, due to their massive stacked transformer layers, rendering them difficult to deploy on several GPUs, let alone a single GPU. Many methods have emerged to quantize and compress these models in order to make them amenable to single GPU deployment and inference, including naive quantization with outlier exclusion (Dettmers et al. ), second order methods (Frantar et al. ), low rank parameter reduction (Hu et al. ), amongst others.

We apply LPLR to the \(2\)-dimensional weight tensors, i.e., matrices in LlaMa, leaving any other tensor, which does not lend itself to a low rank decomposition, unquantized. Figs. 1(a) and 1(b) (better

    & Accuracy (\%) &  \\  \(_{q}\) & **LPLR** & **LSVD** & DSVD & NQ & **LPLR** & **LSVD** & DSVD & NQ & **LPLR** & **LSVD** & DSVD & NQ \\
1 & 0.313 & **0.241** & 0.229 & 6.63 & 73 & 74 & **75** & 50 & 74 & 74 & **75** & 33 \\
2 & 0.235 & 0.178 & **0.161** & 1.016 & **74** & **74** & **74** & 50 & **74** & **74** & 50 \\
4 & 0.148 & 0.122 & **0.098** & 0.417 & **75** & 74 & **75** & 73 & 74 & 74 & **75** & 73 \\   

Table 6: **IMDB embeddings generated by BERT with an unquantized accuracy and F1 score \(75\%\) and \(74\%\) respectively**: Results on LPLR and LPLR-SVD with \(=^{}=8\) bits

    & Accuracy (\%) &  \\  \(_{q}\) & **LPLR** & **LSVD** & DSVD & NQ & **LPLR** & **LSVD** & DSVD & NQ & **LPLR** & **LSVD** & DSVD & NQ \\
1 & **1.05** & 1.08 & 1.09 & 7.17 & **92** & **92** & **92** & 11 & **92** & **92** & **92** & 4 \\
2 & **1.08** & 1.1 & 1.1 & 2.29 & **92** & **92** & 91 & 30 & **92** & **92** & 91 & 23 \\
4 & **1.1** & 1.11 & 1.11 & 1.15 & 91 & **92** & 91 & 91 & 91 & **92** & 91 & 91 \\   

Table 4: **CIFAR10 embeddings generated by MobileNetV3 with an unquantized accuracy and F1 score \(91\%\):Results on LPLR and LPLR-SVD with \(=^{}=8\) bits 1**

    & Accuracy (\%) &  \\  \(_{q}\) & **LPLR** & **LSVD** & DSVD & NQ & **LPLR** & **LSVD** & DSVD & NQ & **LPLR** & **LSVD** & DSVD & NQ \\
1 & **1.05** & 1.08 & 1.09 & 7.17 & **92** & **92** & **92** & 11 & **92** & **92** & **92** & 4 \\
2 & **1.08** & 1.1 & 1.1 & 2.29 & **92** & **92** & 91 & 30 & **92** & **92** & 91 & 23 \\
4 & **1.1** & 1.11 & 1.11 & 1.15 & 91 & **92** & 91 & 91 & 91 & **92** & 91 & 91 \\   

Table 5: **CIFAR100 embeddings generated by MobileNetV3 with an unquantized accuracy and F1 score \(76\%\):Results on LPLR and LPLR-SVD with \(=^{}=8\) bits**resolution in App. J) showcase our results on applying LPLR and LPLR-SVD with bit budgets of \(8\) bits and \(4\) bits respectively, using relative Frobenius norm error as the metric. While it is clear that LPLR and LPLR-SVD perform significantly better across all layers (on average), there are outliers where naive quant. is the better choice. We can a observe periodic structure in the error profile of naive quant., implying that the low rank structure is a function of the index of attention layer in transformer blocks. It is important to note that a low Frobenius norm error is not a direct indicator of performance for other task specific metrics. It is possible to construct a holistic compression strategy using error profiles similar to Figs. 3 and 4 to adopt a per-layer quantization strategy, minimizing both task specific metrics as well as relative Frobenius norm error. We discuss this further in Appendix K.

## 5 Conclusions

In this work, we have considered the problem of obtaining a low-precision and low-rank factorization of a matrix. Such a factorization of a matrix into a product of tall and wide matrices has several advantages, including compression of the original matrix. We propose a fast randomized algorithm to obtain this factorization which requires \((nmd)\) computations - considerably faster than alternative methods. Our algorithm employs a Gaussian sketch to estimate the range space of matrices that are approximately low-rank. By utilizing the properties of subspace approximation and equalization in Gaussian embeddings, we establish an upper bound on the approximation error attained by our algorithm, and show that it can be significantly smaller than its counterparts. Finally, we empirically evaluate our method on several vision and text datasets, where we show significant task performance at highly compressed bit budgets as low as a _single_ bit. This provides a novel pragmatic approach to work with large datasets and models in real world settings, making them more accessible to researchers and deployment on regular consumer hardware.