# LLM-AutoDA:

Large Language Model-Driven Automatic Data Augmentation for Long-tailed Problems

 Pengkun Wang\({}^{1,2}\)1, Zhe Zhao\({}^{1,3}\)1 Haibin Wen\({}^{5}\),

**Fanfu Wang\({}^{6}\)**, **Binwu Wang\({}^{1}\)**, **Qingfu Zhang\({}^{3}\)2, Yang Wang\({}^{1,2}\)3**

\({}^{1}\)University of Science and Technology of China (USTC), Hefei, China

\({}^{2}\)Suzhou Institute for Advanced Research, USTC, Suzhou, China

\({}^{3}\)City University of Hong Kong, Hong Kong, China

\({}^{5}\)MorongAI, Suzhou, China

\({}^{6}\)Lanzhou University, Lanzhou, China

{pengkun@ustc.edu.cn, zz4543@mail.ustc.edu.cn, haibin65535@gmail.com, wangff21@lzu.edu.cn, wbw2024@ustc.edu.cn, qingfu.zhang@cityu.edu.hk, angyan@ustc.edu.cn

###### Abstract

The long-tailed distribution is the underlying nature of real-world data, and it presents unprecedented challenges for training deep learning models. Existing long-tailed learning paradigms based on re-balancing or data augmentation have partially alleviated the long-tailed problem. However, they still have limitations, such as relying on manually designed augmentation strategies, having a limited search space, and using fixed augmentation strategies. To address these limitations, this paper proposes a novel LLM-based long-tailed data augmentation framework called LLM-AutoDA, which leverages large-scale pretrained models to automatically search for the optimal augmentation strategies suitable for long-tailed data distributions. In addition, it applies this strategy to the original imbalanced data to create an augmented dataset and fine-tune the underlying long-tailed learning model. The performance improvement on the validation set serves as a reward signal to update the generation model, enabling the generation of more effective augmentation strategies in the next iteration. We conducted extensive experiments on multiple mainstream long-tailed learning benchmarks. The results show that LLM-AutoDA outperforms state-of-the-art data augmentation methods and other re-balancing methods significantly. The code is available in https://github.com/DataLab-atom/LLM-LT-AUG.

## 1 Introduction

As a revolutionary technology, deep learning has shown a broad and significant impact on various tasks, including image classification , object detection , natural language processing , and many interdisciplinary research problems . The success of these endeavors relies heavily on the support of large-scale manually curated datasets, e.g., ImageNet . However, for the convenience of training and evaluating models, most artificially constructed datasets typically follow the assumption of uniform distribution, which contradicts the real-world data distribution, i.e., long-tailed distribution. This deviation between the ideal and real distributions has resulted in many deepmodels trained on balanced datasets failing to produce satisfactory results in real-world applications, e.g., they only perform well on a few classes and ignore many vulnerable classes .

To address the ubiquitous long-tailed problem, researchers have been continuously proposing various carefully designed research paradigms. One popular approach is to rebalance the training data by oversampling the tail classes or undersampling the head classes . However, this approach cannot fundamentally address the problem of insufficient global information, even though the data for the tail classes increases significantly. Another line of work focuses on designing specialized loss functions or reweighting strategies to alleviate the impact of class imbalance . However, these methods either introduce additional computational overhead or require careful manual design.

Recently, using data augmentation (DA) to improve long-tailed learning has attracted significant attention from researchers and is considered as a viable research paradigm . For example, FASA  enhances the tail classes by generating class-level features based on Gaussian priors. Remix  achieves this goal through a rebalanced mixup approach. However, these DA-based methods either manipulate high-dimensional information in the feature space or directly apply traditional transformations (e.g., flipping, cropping, and rotation) to expand the training set and generate diverse data, without considering the underlying relationships between data augmentation and class classes. To avoid ineffective augmentation, some studies suggest applying different augmentations to different classes . Typically, CUDA  improves the overall performance of models by dynamically adjusting the augmentation intensity for each class during training. Considering the issue of pseudo-boosting in augmentation, DODA  allows each class to choose its own suitable augmentation method, thereby avoiding weak classes being sacrificed. Unfortunately, they still have significant limitations: (i) _these strategies are often based on manually designed human knowledge and experience, which may be suboptimal for specific data and tasks._ (ii) _the search space of these strategies is often limited._ (iii) _these fixed strategies lack flexibility to adapt to changes in the data distribution during the training process._

To address the above limitations, we leverage the recently popular large language models (LLMs)  to provide augmentation suggestions for long-tailed learning. We first designed a feasible and straightforward framework called SimpleLLM, which guides the LLM to generate augmentation strategies and apply them to long-tailed learning by providing specific prompts. Analysis revealed that the augmentation strategies generated by SimpleLLM are comparable to the effectiveness of CUDA  and DODA . Figure 1 illustrates the differences between this framework and previous methods.

Figure 1: Different long-tailed data augmentation paradigms. (a) The traditional augmentation paradigm randomly samples augmentations from the fixed strategy. (b) The strategy fixed augmentation paradigm samples augmentations from the fixed strategy according to the data distribution. (c) The LLM-driven augmentation paradigm combines LLMs with long-tailed learning to learn the optimal augmentation strategy.

Furthermore, inspired by AutoML , particularly automated data augmentation , we propose **LLM-AutoDA**, a novel LLM-based long-tailed data augmentation framework. LLM-AutoDA leverages large-scale pretrained models to automatically search for the optimal augmentation strategies suitable for long-tailed data distributions. Specifically, we first define a broad search space that includes augmentation operations and their parameters. Then, we train an augmentation strategy generation model that generates augmentation strategies based on the class-wise statistics of the long-tailed data. This strategy is applied to the original imbalanced data to create an augmented dataset, which is used to fine-tune the underlying long-tailed learning model. Importantly, the performance improvement on the validation set serves as a reward signal to update the generation model, enabling the generation of more effective augmentation strategies in the next iteration. This process is repeated until the performance converges or the computational budget is exhausted.

Compared to previous long-tailed data augmentation methods, LLM-AutoDA offers several advantages: (i) it leverages LLMs to automatically learn augmentation strategies tailored to the characteristics of long-tailed data, without relying on human expertise. (ii) it has a more extensive search space, allowing it to discover more novel strategies. (iii) it can dynamically adjust the augmentation strategies based on performance feedback during the training process, providing flexibility and robustness. Extensive experiments on multiple mainstream long-tailed learning benchmarks demonstrate that LLM-AutoDA outperforms state-of-the-art data augmentation methods and other rebalancing techniques significantly.

The main contributions of this work are summarized as follows:

* _New augmentation paradigm_: We combine LLMs with long-tail data augmentation for the first time, providing a novel perspective for efficient long-tail learning.
* _New automated framework_: We propose a novel AutoML framework called LLM-AutoDA, which automates the search for effective data augmentation strategies for long-tailed learning, significantly reducing the cost of manually designing augmentation strategies.
* _Compelling empirical results_: We conduct extensive experiments on multiple mainstream long-tailde benchmarks, demonstrating the superiority of LLM-AutoDA compared to state-of-the-art methods.
* _In-depth analysis and insights_: We provide detailed analysis and insights into the discovered augmentation strategies, guiding future research in long-tailed learning.

## 2 Related Work

### Long-tailed Learning (LTL)

Various approaches have been proposed to address the long-tailed learning problem, including rebalancing the training data through over-sampling the tail classes [6; 13] or under-sampling the head classes [12; 14], modifying loss functions or adjusting class weights during training [22; 11], and decoupling representation and classifier learning . Among these methods, data augmentation has emerged as a promising solution for long-tailed learning [8; 9; 20; 25; 46]. The key idea is to generate additional samples for the tail classes to alleviate the data imbalance issue. Recent research has attempted to design sophisticated strategies by observing performance changes during the training process to adjust augmentation operator  or intensity . However, these methods still rely on hand-crafted augmentation strategies that may not be optimal for the specific long-tailed data and have limited search space for discovering novel and effective strategies. In contrast, LLM-AutoDA automatically learns data augmentation strategies tailored to the long-tailed data distribution without manual design.

### Large Language Models (LLMs)

Large language models (e.g., BERT , GPT [29; 30; 3], and T5 ) have achieved remarkable success in various natural language processing tasks. They exhibit strong generalization abilities and can be easily fine-tuned for downstream tasks with limited labeled data [16; 28]. Researchers have explored the potential of LLMs in automating algorithm design and implementation, such as generating source code , optimizing hyperparameters , and designing neural architectures . However, the interaction between large and small language models and its impact on improving the design of small models have been less explored, particularly in the context of data augmentation for long-tailed learning. LLM-AutoDA aims to bridge this gap by harnessing the knowledge and generative capabilities of large language models to discover effective data augmentation strategies tailored to long-tailed distributions automatically. By defining a rich search space of augmentation operations and training an augmentation strategy model conditioned on the class-wise statistics, LLM-AutoDA can generate adaptive and optimized augmentations specifically designed for the given long-tailed data. This novel approach opens up new possibilities for leveraging the interaction between large and small language models to improve the design of machine learning algorithms in various imbalanced learning scenarios.

## 3 LLM \(\) LTL: Can LLMs Provide DA Strategies for Long-tailed Learning?

In this section, we attempt to analyze whether LLMs can be applied to long-tailed learning and how to implement this learning paradigm.

**SimpleLLM.** As shown in Figure 1(b), strategy fixed DA is the current mainstream paradigm for long-tailed data augmentation. It utilizes carefully designed augmentation strategies to dynamically adjust augmentation operators or intensities during the training process, allowing different classes to choose advantageous augmentation methods. In this paradigm, the key step is to design a high-quality augmentation strategy. When dealing with balanced data distributions, we often employ class-independent augmentation strategies, which apply the same data augmentation to all classes. However, as mentioned in DODA , when dealing with imbalanced data distributions, this class-independent augmentation strategy can potentially sacrifice certain classes, thus requiring the design of class-dependent augmentation strategies. However, the manual design process for such strategies is highly complex and costly.

Recent research has shown that LLMs can replace many manually engineered tasks . Inspired by this, we first designed a simple yet efficient paradigm for generating augmentation strategies called SimpleLLM. As shown in Figure 2, we initially constructed a data augmentation-themed prompt from the perspective of prompt engineering, including task description, algorithm input, algorithm output, parameter interpretation, etc. We then input this prompt into pre-trained LLMs to generate a functional function that conforms to the prompt, i.e., an algorithm implementation containing augmentation strategies. Finally, this augmentation strategy is applied to the conventional training process of long-tailed learning. It is worth noting that this paradigm allows us to generate multiple augmentation strategies suitable for long-tailed learning at a low cost.

**Comparative Analysis.** To further validate the effectiveness of the augmentation strategies generated by this paradigm, we conducted experiments on CIFAR-100-LT (IR=100) dataset. We selected several mainstream long-tailed learning baselines and integrated SimpleLLM with them. In addition, we compared the latest state-of-the-art long-tailed DA methods, CUDA and DODA, under the same settings. The experimental results, as shown in Figure 4, reveal that SimpleLLM achieves acceptable average performance, comparable to CUDA and DODA, indicating that LLMs can be used as generators of augmentation strategies to enhance the performance of long-tailed learning.

Under this paradigm, we believe that with appropriate prompts, augmentation strategies similar to CUDA and DODA can also be generated by LLMs. In other words, within a search space, we can obtain multiple similar locally optimal strategies.

Figure 2: Strategy generation paradigm of SimpleLLM.

## 4 LLM-AutoDA: A Resourceful Adviser for Long-tailed Learning

### Framework

The overall framework of LLM-AutoDA is illustrated in Figure 3. The framework consists of two interactive modules: the LLM-based data augmentation strategy generation module and the long-tailed learning training and evaluation module.

**LLM-based Data Augmentation Strategy Generation.** As shown in Figure 3 (left, pink), to design the DA strategies automatically, LLM-AutoDA incorporates a pre-trained LLM \(\) as a search operator. Using prompt engineering techniques, a series of prompt templates are designed to incorporate prior knowledge about data augmentation into the generation process of LLM. LLM generates diverse data augmentation strategies based on these prompts, including both natural language descriptions and Python code implementations. Furthermore, the generated strategies are stored in a strategy pool and interact with the long-tail learning model to search for the optimal data augmentation strategy.

**Long-tailed Learning Training and Evaluation.** As shown in Figure 3 (right, pink), LLM-AutoDA utilizes a pretrained long-tailed learning model \(\) for fine-tuning on a given long-tailed distributed dataset \(\). At the beginning of each training epoch, the algorithm adaptively determines the DA operator \(_{c}\) and DA intensity \(_{c}\) for each class based on information such as the accuracy of each class in the previous epoch and historical accuracy.

The key aspect of LLM-AutoDA lies in the synergy between the DA strategies generated by LLMs and the long-tailed learning model. This close interaction allows the discovered DA strategies to dynamically align with the model training process, effectively enhancing the performance of long-tailed learning in a targeted manner.

### Strategy Evaluation

To evaluate the performance of candidate data augmentation strategies, we insert them into the model training process, conduct a small amount of additional training on the training set, and then test the accuracy on the validation set, using the accuracy as the fitness score for the algorithm. Assuming the data augmentation function generated by LLM is denoted as \(f_{aug}\) and the training-testing function is denoted as \(\), the evaluation process can be represented as follows:

\[Fitness(f_{aug})=(f_{aug},e_{ckp},N_{ft})\] (1)

Figure 3: Overview of LLM-AutoDA. LLM-AutoDA leverages large-scale pretrained models to automatically search for the optimal augmentation strategies suitable for long-tailed data distributions.

where \(e_{ckp}\) is the starting checkpoint epoch number and \(N_{ft}\) is the epoch number of fine-tune. The function \(\) injects \(f_{aug}\) into the training flow:

\[(f_{aug},e_{ckp},N_{ft})=Acc_{val}(Fine-tune(f_{aug},_{ train},e_{ckp},N_{ft}))\] (2)

The \(Finetune\) function starts training from the \(e_{ckp}\) checkpoint and performs \(N_{ft}\) epochs of training on the training set \(_{train}\). At the beginning of each epoch, it dynamically selects data augmentation methods for each class using \(faug\).

\[_{c}^{(t)},_{c}^{(t)}=f_{aug}(_{c}^{(t-1)}, _{c}^{(t-1)},_{c}^{(t-1)},_{c}^{(t-1)},_{c}^{(t-1)},t)\] (3)

Here, \(_{c}^{(t)}\) represents the DA selection matrix for class \(c\) at time \(t\), \(_{c}^{(t)}\) represents the corresponding augmentation intensity, \(_{c}^{(t-1)}\) represents the weights of each augmentation method on class \(c\) from the previous time step, \(_{c}^{(t-1)}\) represents the accuracy of class \(c\) at time \(t-1\), and \(_{c}^{(t-1)}\) represents the historical accuracy of class \(c\) when using different augmentation methods in the previous step.

After training, the model is evaluated using the no augmented validation set \(_{val}\), and the overall accuracy \(Acc_{val}\) is obtained as the fitness score for \(f_{aug}\).

\[Fitness(f_{aug})=Acc_{val}(Finetune(f_{aug},_{train},e_{ckp},N_{ft }),_{val})\] (4)

A higher fitness score indicates better performance of the algorithm on long-tailed distributions. By injecting candidate algorithms into the real training process and evaluating them on the validation set, we can accurately and efficiently measure their actual ability to address the long-tailed problem.

### LLM-based Search Operator

LLM-AutoDA leverages Pretrained Language Models (PLMs) to automatically generate data augmentation algorithms. To guide the PLM in generating algorithms that meet specific requirements, we employ prompt engineering techniques and carefully design a series of prompts. By incorporating task descriptions, input-output formats, novelty requirements, and other prior knowledge through prompts, we can constrain the generation process of the PLM within the desired search space. We design the following three types of search operators, corresponding to different prompt templates:

* **Initialization operator**\(I\): Based on the task description prompt \(P_{task}\) and the knowledge base of data augmentation \(\), a set of randomly initialized population algorithms \({A_{i}^{(0)}}_{i=1}^{N}\) is generated.
* **Crossover operator**\(E\): Building upon \(P_{task}\), \(N_{p}\) parent algorithms \({A_{i}^{(t)}}_{i}=1^{N_{p}}\) from the current population are used as references, along with the incorporation of knowledge base \(\). The PLM is required to generate \(N_{e}\) new algorithms \({A_{j}^{(t)}}_{j}=1^{N_{e}}\) that are different in both form and logic from the existing algorithms, thereby expanding the search space.
* **Mutation operator**\(M\): Based on \(P_{task}\), \(N_{m}\) individuals \({A_{i}^{(t)}}_{i=1}^{N_{m}}\) are selected from the current population, and local improvement directions are provided. The PLM is tasked with generating a mutated algorithm \(_{i}^{(t)}\) for each \(A_{i}^{(t)}\) within its neighborhood for further exploration.

Taking the crossover operator \(E\) as an example, its prompt \(P_{E}\) can be represented as follows:

\[P_{E}(P_{task},\{A_{i}^{(t)}\}_{i=1}^{N_{p}},,D_{func})=P_{task}+P_ {ref}(\{A_{i}^{(t)}\}_{i=1}^{N_{p}})+P_{know}()+P_{diff}+P_{format}(D _{func})\] (5)

where \(P_{task}\) represents the task description, \(A_{i}i=1^{N}\) represents \(N\) parent algorithms, and \(Dfunc\) represents the domain of the objective function. \(P_{ref}\) formats the parent algorithms into reference code, \(P_{diff}\) requires the generation of new algorithms that are completely different from the existing ones, and \(P_{format}\) specifies the input and output of the objective function.

Once we have obtained the prompt \(P_{E}\), we input it into the pretrained language model \(\), and as a result, we obtain \(N_{e}\) new crossover algorithms.

\[\{A_{j}^{(t)}\}_{j=1}^{N_{e}}=(P_{E}(P_{task},\{A_{i}^{(t)}\}_{i=1} ^{N_{p}},,D_{func}))\] (6)

Each \(A_{j}^{(t)}\) typically consists of a natural language description of the algorithm and its corresponding Python code implementation. Similarly, the prompts \(P_{I}\) and \(P_{M}\) for the initialization operator \(I\) and the mutation operator \(M\) can be constructed in a similar manner, with the main difference lying in the introduced prior information.

Experiments

### Experimental Settings

**Datasets and Metrics.** Like most long-tailed learning methods, we conducted experiments on several mainstream long-tailed learning datasets, including CIFAR-100-LT , ImageNet-LT , and iNaturalist 2018 . Among them, CIFAR-100-LT is the long-tailed version of CIFAR-100, with various imbalance ratios. To validate the effectiveness of LLM-AutoDA in addressing the long-tailed problem, we selected three testing environments: 50, 100, 200. Compared to CIFAR-100-LT, both ImageNet-LT and iNaturalist 2018 have more classes and larger data sizes. It is worth noting that, similar to CIFAR-100-LT, ImageNet-LT is a long-tailed version artificially constructed from the well-known ImageNet  dataset. On the other hand, iNaturalist 2018 is a naturally occurring long-tailed dataset collected from the real world. We used the official complete versions of these datasets, and detailed information about the datasets is provided in Appendix B. We use Top-1 accuracy as the evaluation metric and provide the performance of subsets based on the class divisions provided by the official datasets.

**Baselines.**

Following the settings of CUDA , we considered various research theories when selecting the baselines. In addition to the classic cross-entropy loss (CE) , we also validated different data augmentation methods on other baselines, such as loss-based re-balancing methods: CE-DRW , LDAM-DRW , Balanced Softmax (BS) , and model-based re-balancing methods: RIDE , BCL . In terms of data augmentation methods, we compared LLM-AutoDA with the latest SOTA methods: CUDA  and DODA . We observed their advantages and disadvantages by combining these DA methods with the long-tailed baselines. The relevant descriptions of the baselines are also provided in Appendix A.

**Implementation Details.** All our models are implemented based on PyTorch . We trained and evaluated the models on 2 NVIDIA Tesla A100 GPUs and reported the experimental results. We utilized the powerful gpt-3.5-turbo for strategy generation and employed AEL  for strategy optimization. In the experimental process, we first trained the models for 50 epochs without using augmentation strategies, then continued training with augmentation strategies for an additional 20 epochs, employing a novel evaluation mechanism. Additionally, during the final evaluation stage of long-tailed learning, we adopted the same settings as DODA  for all baseline methods and our approach.

### Comparison with the State-of-the-art

**Results on CIFAR-100-LT.** We first evaluated LLM-AutoDA and other long-tailed data augmentation (DA) methods on CIFAR-100-LT dataset (IR = 50, 100). The experimental results are shown in Table 1. From the results, it can be observed that both SimpleLLM and the improved version LLM-AutoDA significantly improve the global accuracy of the model compared to the original long-tailed learning baseline, achieving robust improvements. In the horizontal comparison with long-tailed DA methods CUDA  and DODA , as analyzed earlier, SimpleLLM achieves comparable performance to the former through a non-optimized way. This indicates that within our framework, a locally optimal strategy can replace a carefully designed complex strategy. In addition, LLM-AutoDA brings more significant and stable gains, reflecting that existing long-tailed DA methods may be suboptimal strategies within our augmentation strategy space, while LLM-AutoDA can provide the

Figure 4: Average accuracy (%) on CIFAR-100-LT dataset (Imbalance ratio=100) with CUDA and DODA. SimpleLLM is comparable to the effectiveness of CUDA and DODA when combined with long-tailed learning baselines.

optimal augmentation strategy through continuous optimization. To evaluate the effectiveness of LLM-AutoDA in highly imbalanced scenarios, we adjusted the imbalance ratio to 200 and conducted comparative experiments in Appendix D.

Results on ImageNet-LT and iNaturalist 2018.We also conducted comparative experiments on large-scale benchmark datasets, ImageNet-LT and iNaturalist 2018. As expected, different long-tailed learning methods showed significant performance improvement when integrated with LLM-AutoDA. Similar to the highly imbalanced setting mentioned earlier, both of these large-scale datasets are inherently highly imbalanced. Therefore, the augmentation strategies provided by LLM-AutoDA can consistently demonstrate superiority in various imbalanced environments. Importantly, LLM-AutoDA does not rely on meticulous manual design, which reduces the optimization cost on large-scale datasets.

### More Analysis and Discussion

**Do different LLMs produce differentiated effects?** In the aforementioned experiments, we used GPT-3.5  as the LLM to respond to the designed prompts. To analyze whether LLM-AutoDA is dependent on specific LLM models (e.g., GPT-3.5), we replaced the LLM model in LLM-AutoDA with other popular methods such as GPT-4  and Claude-3-Opus, and conducted experiments. The experimental results, shown in Figure 5, demonstrate that all three LLMs exhibit consistent performance trends when selecting augmentation strategies in different score ranges. For instance, they all show high performance near the augmentation strategies with scores around 12, while augmentation strategies with excessively high scores lead to performance degradation across the three LLMs due to insufficient diversity.

**Do different population numbers have an impact on performance?** In LLM-AutoDA, we employed two different types of prompts: crossover prompts and mutation prompts. Crossover prompts involve transforming multiple parent populations into a single population, while mutation prompts replace the current augmentation strategy with an equivalent one.

Throughout the iterative process of framework evolution, when performing crossover and mutation operations, we need to specify the number of populations generated each time. Figures 6 and 7 illustrate the scores of strategies generated by two different mutation prompts (m1, m2) and two different crossover prompts (e1, e2), respectively. It can be observed that different prompts exhibit consistent trends in score variations. Particularly, compared to e1, e2 demonstrates a higher score variance, which indirectly reflects the bias in its prompt content.

**Why are fixed strategy methods often local optima?** Strategy fixed data augmentation methods, such as CUDA and DODA, aim to adapt to long-tailed distributions by dynamically adjusting the augmentation operators or intensities. However, their focus is limited. We visualized the loss variations when searching for optimal augmentation strategies using different paradigms. From Figure 8, it can be observed that CUDA (i.e., green plane) and DODA (i.e., red

    &  &  \\   & Head & Medium & Tail & All & Head & Medium & Tail & All \\  CE  & 64.0 & 33.8 & 5.8 & 41.6 (+0.0) & 73.9 & 63.5 & 55.5 & 61.0 (+0.0) \\ CE + CUDA & 67.1 & 47.1 & 13.4 & 47.2 (+5.6) & 74.6 & 65.0 & 57.2 & 62.5 (+1.5) \\ CE + DODA & 67.4 & 47.5 & 13.9 & 48.1 (+6.5) & 74.9 & 66.0 & 58.4 & 63.6 (+2.6) \\ CE + LLM-AutoDA & 68.2 & 47.1 & 14.3 & 50.4 (+8.8) & 75.1 & 66.3 & 58.9 & 64.0 (+3.0) \\  CE-DRW  & 61.7 & 47.3 & 28.8 & 50.1 (+0.0) & 68.2 & 67.3 & 66.4 & 67.0 (+0.0) \\ CE-DRW + CUDA & 61.7 & 48.4 & 30.5 & 51.1 (+1.0) & 68.8 & 67.9 & 66.5 & 67.4 (+0.4) \\ CE-DRW + DODA & 62.4 & 48.5 & 31.3 & 52.2 (+2.1) & 69.0 & 68.2 & 67.8 & 68.2 (+1.2) \\ CE-DRW + LLM-AutoDA & 62.8 & 48.3 & 31.7 & 51.6 (+1.5) & 68.8 & 68.8 & 68.1 & 68.7 (+1.7) \\  LDAM-DRW  & 60.4 & 46.9 & 30.7 & 49.8 (+0.0) & - & - & - & 66.1 (+0.0) \\ LDAM-DRW + CUDA & 63.2 & 48.2 & 31.2 & 51.5 (+1.7) & 68.0 & 67.5 & 66.8 & 67.3 (+1.2) \\ LDAM-DRW + DODA & 63.7 & 48.6 & 31.9 & 52.4 (+2.6) & 68.6 & 68.1 & 67.9 & 68.7 (+2.6) \\ LDAM-DRW + LLM-AutoDA & 63.3 & 49.4 & 32.4 & 52.5 (+2.7) & 68.0 & 69.4 & 68.6 & 69.5 (+3.4) \\  BS  & 60.9 & 48.8 & 32.1 & 51.0 (+0.0) & 65.7 & 67.4 & 67.5 & 67.3 (+0.0) \\ BS + CUDA & 61.8 & 49.1 & 31.8 & 51.5 (+0.5) & 67.6 & 68.2 & 68.3 & 68.2 (+0.9) \\ BS + DODA & 61.9 & 49.5 & 32.4 & 52.0 (+1.0) & 68.1 & 68.9 & 69.5 & 69.4 (+2.1) \\ BS + LLM-AutoDA & 62.5 & 50.0 & 32.8 & 52.5 (+1.5) & 68.0 & 69.1 & 69.9 & 69.8 (+2.5) \\  RIDE  & 64.9 & 50.4 & 34.4 & 53.6 (+0.0) & 70.4 & 71.8 & 71.8 & 71.6 (+0.0) \\ RIDE + CUDA & 66.0 & 51.7 & 34.7 & 54.7 (+1.1) & 70.6 & 72.6 & 72.7 & 72.4 (+1.4) \\ RIDE + DODA & 66.6 & 51.9 & 35.9 & 55.8 (+2.2) & 70.9 & 72.4 & 73.9 & 73.7 (+2.8) \\ RIDE + LLM-AutoDA & 67.1 & 52.3 & 37.3 & 56.5 (+2.9) & 70.9 & 72.8 & 73.8 & 73.9 (+3.0) \\  BCL  & 65.3 & 53.5 & 36.3 & 55.6 (+0.0) & 69.4 & 72.4 & 71.8 & 71.8 (+0.0) \\ BCL + CUDA & 66.8 & 53.9 & 36.6 & 56.3 (+0.7) & 70.8 & 72.7 & 72.0 & 72.2 (+0.4) \\ BCL + DODA & 66.9 & 54.1 & 37.4 & 56.9 (+1.3) & 71.2 & 73.2 & 73.4 & 73.7 (+1.9) \\ BCL + LLM-AutoDA & 67.2 & 55.1 & 38.3 & 57.5 (+1.9) & 70.9 & 73.6 & 74.7 & 74.2 (+2.4) \\   

Table 2: Accuracy (%) on ImageNet-LT and iNaturalist 2018 datasets with SOTA DA methods. **Blod** indicates the best performance while underline indicates the second best. (+) and (-) indicate the relative gain.

Figure 8: Visualization of the process of finding the optimal solution for different augmentation paradigms.

plane) can only search for local optimal strategies on a single plane, while LLM-AutoDA is capable of flexibly searching for the points with the lowest loss across the entire curved surface to obtain a global optimal solution.

## 6 Conclusion

Existing paradigms for long-tailed learning have partially alleviated the long-tailed problem but still have limitations. To address this, this paper presents an LLM-driven long-tailed data augmentation framework called LLM-AutoDA, which utilizes large-scale pre-trained language models to automatically search for data augmentation strategies optimized for long-tailed data distributions. Experiments on multiple mainstream benchmark datasets demonstrate that LLM-AutoDA outperforms state-of-the-art methods.