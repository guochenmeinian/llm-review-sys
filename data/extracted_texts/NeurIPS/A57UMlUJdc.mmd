# Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration

Zhihan Liu\({}^{1}\)   Miao Lu\({}^{2}\)   Wei Xiong\({}^{3}\)   Han Zhong\({}^{4}\)   Hao Hu\({}^{5}\)

**Shenao Zhang\({}^{1}\)   Sirui Zheng\({}^{1}\)   Zhuoran Yang\({}^{6}\)   Zhaoran Wang\({}^{1}\)**

\({}^{1}\)Northwestern University  \({}^{2}\)Stanford University  \({}^{3}\)University of Illinois Urbana-Champaign

\({}^{4}\)Peking University  \({}^{5}\)Tsinghua University  \({}^{6}\)Yale University

{zhihanliu2027,shenaozhang2028,siruizheng2025}@u.northwestern.edu

miaolu@stanford.edu, wx13@illinois.edu, hanzhong@stu.pku.edu.cn

huh22@mails.tsinghua.edu.cn, zhuoran.yang@yale.edu, zhaoranwang@gmail.com

Equal contribution.

###### Abstract

In reinforcement learning (RL), balancing exploration and exploitation is crucial for achieving an optimal policy in a sample-efficient way. To this end, existing sample-efficient algorithms typically consist of three components: estimation, planning, and exploration. However, to cope with general function approximators, most of them involve impractical algorithmic components to incentivize exploration, such as data-dependent level-set constraints or complicated sampling procedures. To address this challenge, we propose an easy-to-implement RL framework called _Maximize to Explore_ (MEX), which only needs to optimize _unconstrainedly_ a single objective that integrates the estimation and planning components while balancing exploration and exploitation automatically. Theoretically, we prove that the MEX achieves a sublinear regret with general function approximators and is extendable to the zero-sum Markov game setting. Meanwhile, we adapt deep RL baselines to design practical versions of MEX in both the model-based and model-free settings, which outperform baselines in various MuJoCo environments with sparse reward by a stable margin. Compared with existing sample-efficient algorithms with general function approximators, MEX achieves similar sample efficiency while also enjoying a lower computational cost and is more compatible with modern deep RL methods. Our codes are available at https://github.com/agentification/MEX.

## 1 Introduction

The crux of online reinforcement learning (online RL) lies in maintaining a balance between exploiting the current knowledge of the agent about the environment and exploring unfamiliar areas . To fulfill this, agents in existing sample-efficient RL algorithms predominantly undertake three tasks: i) _estimate_ a hypothesis using historical data to encapsulate their understanding of the environment; ii) perform _planning_ based on the estimated hypothesis to exploit their current knowledge; iii) further _explore_ the unknown environment via carefully designed exploration strategies.

There exists a long line of research on integrating the aforementioned three components harmoniously, to find optimal policies in a sample-efficient manner. From theoretical perspectives, existing theories aim to minimize the notion of _online external regret_ which measures the cumulative suboptimality gap of the policies learned during online learning. It is well studied that one can design both statistically and computationally efficient algorithms (e.g., upper confidence bound (UCB), ) with sublinear online regret for tabular and linear Markov decision processes (MDPs). But when itcomes to MDPs with general function approximations, most of them involve impractical algorithmic components to incentivize exploration. Usually, to cope with general function approximations, agents need to solve constrained optimization problems within data-dependent level-sets , or sample from complicated posterior distributions over the space of hypotheses , both of which pose considerable challenges for implementation. From a practical perspective, a prevalent approach in deep RL for balancing exploration and exploitation is to use an ensemble of neural networks , which serves as an empirical approximation of the UCB method. However, such an ensemble method suffers from high computational cost and lacks theoretical guarantee when the underly MDP is neither linear nor tabular. As for other deep RL algorithms for exploration , such as curiosity-driven method , it also remains unknown in theory whether they are provably sample-efficient in the context of general function approximations.

Hence, in this paper, we are aimed at tackling these issues and answering the following question:

_Under general function approximation, can we design a sample-efficient and easy-to-implement RL framework to trade off between exploration and exploitation?_

To this end, we propose an easy-to-implement RL framework, _Maximize to Explore_ (MEX), as an affirmative answer to the question. To strike a balance between exploration and exploitation, MEX propose to maximize a weighted sum of two objectives: (i) the optimal expected total return associated with a given hypothesis and (ii) the negative estimation error of that hypothesis. Consequently, MEX naturally combines planning and estimation components in just a single objective. By choosing the hypothesis that maximizes the weighted sum and executing the optimal policy with respect to the chosen hypothesis, MEX automatically balances between exploration and exploitation.

We highlight that the objective of MEX is _not_ obtained by taking the Lagrange dual of the constrained optimization objective within data-dependent level-sets .This is because the coefficient of the weighted sum, which remains fixed, is data-independent and predetermined for all episodes. Contrary to Lagrangian methods, MEX does not necessitate an inner loop of optimization for dual variables, thereby circumventing the complications associated with minimax optimization. As a maximization-only framework, MEX is friendly to implementations with neural networks and does not rely on sampling or ensemble.

In the theory part, we prove that MEX achieves a sublinear regret \(}((H) d_{}^{1/2}(1/)  K^{1/2})\) under mild assumptions and is thus sample-efficient, where \(K\) is the number of episodes and \(H\) is the horizon length. Here \(d_{}()\) is the Generalized Eluder Coefficient (GEC)  that characterizes the complexity of learning the underlying MDP under general function approximations. Because the class of low-GEC MDPs includes almost all known theoretically tractable MDP instances, our proved result can be tailored to a multitude of specific settings with either a model-free or a model-based hypothesis, such as MDPs with low Bellman eluder dimension , MDPs of bilinear class , and MDPs with low witness rank . Besides, thanks to the flexibility of the MEX framework, we further extend it to online RL in two-player zero-sum Markov games (MGs), for which we further extend the definition of GEC to two-player zero-sum MGs and establish the sample efficiency with general function approximations. Moving beyond theory and into practice, we adapt famous RL baselines TD3 and MBP0 to design practical versions of MEX in model-free and model-based fashion, respectively. On various MuJoCo environments  with sparse rewards, experimental results show that MEX outperforms baselines steadily and significantly. Compared with other deep RL algorithms, MEX has low computational overhead and straightforward implementation while maintaining a theoretical guarantee.

**Contributions.** We conclude our contributions from three perspectives.

1. We propose an easy-to-implement RL algorithm framework MEX that _unconstrainedly_ maximizes a single objective to fuse estimation and planning, automatically trading off between exploration and exploitation. Under mild structural assumptions, we prove that MEX achieves a sublinear regret \(}((H) d_{}^{1/2}(1/) K^{1/2})\) with general function approximators, and thus is sample-efficient. Here \(d_{}()\) is the generalized Eluder Coefficient (GEC) of the underlying MDP.
2. We instantiate the generic MEX framework to several model-based and model-free examples and establish corresponding theoretical results. Further, we extend the MEX framework to two-player zero-sum MGs and also prove the sample efficiency with an extended definition of GEC.
3. We design practical implementations of MEX for MDPs in both model-based and model-free styles. Experiments on various MuJoCo environments with sparse reward demonstrate the effectiveness of our proposed MEX framework.

### Related work

**Sample-efficient RL with function approximation.** The success of DRL methods has motivated a line of work focused on function approximation scenarios. This work originated in the linear case [74; 81; 12; 39; 84; 5; 82; 57; 91; 90] and is later extended to general function approximation. Wang et al.  first study the general function approximation using the notion of eluder dimension , which takes the linear MDP  as a special case but with inferior results. Zanette et al.  consider a different type of framework based on Bellman completeness, which assumes that the class used for approximating the optimal Q-functions is closed in terms of the Bellman operator and improves the results for linear MDP. After this, Jin et al.  consider the eluder dimension of the class of Bellman residual associated with the RL problems, which captures more solvable problems. Another line of works focuses on the low-rank structures of the problems, where Jiang et al.  propose the Bellman rank for model-free RL and Sun et al.  propose the witness rank for model-based RL. Following these two works, Du et al.  propose the bilinear class, which contains more MDP models with low-rank structures [6; 67; 39; 57; 12; 91] by allowing a flexible choice of discrepancy function class. However, it is known that neither BE nor bilinear class captures each other. Dann et al.  first consider eluder-coefficient-type complexity measure on the Q-type model-free RL. It was later extended by Zhong et al.  to cover all the above-known solvable problems in both model-free and model-based manners. Foster et al. [25; 23] study another notion of complexity measure, the decision-estimation coefficient (DEC), which also unifies the Bellman eluder dimension and bilinear class. The DEC framework is appealing due to the matching lower bound in some decision-making problems, where all other complexity measures do not have. However, due to the presence of a minimax subroutine in its definition, they require a much more complicated minimax optimization oracle and cannot apply to the classical optimism-based or sampling-based methods. Chen et al. , Foster et al.  extend the vanilla DEC (to the model-free case) by incorporating an optimistic modification, which was originally referred to as the feel-good modification in Zhang . Chen et al.  study Admissible Bellman Characterization (ABC) class to generalize BE. They also extend the GOLF algorithm and Bellman completeness in model-free RL to the model-based case by considering more general (vector-form) discrepancy loss functions to construct sharper in-sample error estimators and obtain sharper bounds compared to Sun et al. . Xie et al.  connect the online RL with the coverage condition in the offline RL, and also study the GOLF algorithm proposed in Jin et al. .

**Algorithmic design in sample-efficient RL with function approximation.** The most prominent approach in this area is based on the principle of "Optimism in the Face of Uncertainty" (OFU), which dates back to Auer et al. . For instance, for linear function approximation, Jin et al.  propose an optimistic variant of Least-Squares Value Iteration (LSVI), which achieves optimism by adding a bonus at each step. For the general case, Jiang et al.  first propose an elimination-based algorithm with optimism in model-free RL and is extended to model-based RL by . After these, Du et al. , Jin et al.  propose two OFU-based algorithms, which are more similar to the lin-UCB algorithm  studied in the linear contextual bandit literature. The model-based counterpart (Optimistic Maximum Likelihood Estimation) is studied in Liu et al. , Chen et al. . Specifically, these algorithms explicitly maintain a confidence set that contains the ground truth with high probability and conducts a constraint optimization step at each iteration to select the most optimistic hypothesis in the confidence set. The other line of work studies another powerful algorithmic framework based on posterior sampling. For instance, Zanette et al.  study randomized least-squares value iteration (RLSVI), which can be interpreted as a sampling-based algorithm and achieves an order-optimal result for linear MDP. For general function approximation, the works mainly follow the idea of the "feel-good" modification proposed in Zhang . These algorithms start from some prior distribution over the hypothesis space and update the posterior distribution according to the collected samples but with certain optimistic modifications in either the prior or the loglikelihood function. Then the hypothesis for each iteration is sampled from the posterior and guides data collection. In particular, Dann et al.  studies the model-free Q-type problem, and Agarwal and Zhang  studies the model-based problems, but under different notions of complexity measures. Zhong et al.  further utilize the idea in Zhang  and extend the posterior sampling algorithm in Dann et al.  to be a unified sampling-based framework to solve both model-free and model-based RL problems, which is also shown to apply to the more challenging partially observable setting. In addition to the OFU-based algorithm and the sampling-based framework, Foster et al.  propose the Estimation-to-Decisions (E2D) algorithm, which can solve problems with low DEC but requires solving a complicated minimax subroutine to fit in the framework of DEC.

Due to the limitation of the page, we defer the remaining discussions of **Relationship with reward-biased maximum likelihood estimation**, **Exploration of DRL**, and **Two-player Zero-Sum Markov Game** to Appendix B.

## 2 Preliminaries

### Episodic Markov Decision Processes and Online Reinforcement Learning

We consider an episodic MDP defined by a tuple \((,,H,,r)\), where \(\) and \(\) are the state and action spaces, \(H\) is a finite horizon, \(=\{_{h}\}_{h=1}^{H}\) with \(_{h}:()\) the transition kernel at the \(h\)-th timestep, and \(r=\{r_{h}\}_{h=1}^{H}\) with \(r_{h}:\) the reward function at the \(h\)-th timestep. Without loss of generality, we assume that the reward function \(r_{h}\) is both deterministic and known.

In the episodic MDP, the agent interacts with the environment by the following _online_ protocol. At the beginning of the \(k\)-th episode, the agent selects a policy \(^{k}=\{_{h}^{k}:()\}_{h=1}^{H}\). It takes an action \(a_{h}^{k}_{h}^{k}( x_{h}^{k})\) at timestep \(h\) and state \(x_{h}^{k}\). After receiving the reward \(r_{h}^{k}=r_{h}(x_{h}^{k},a_{h}^{k})\) from the environment, it transits to the next state \(x_{h+1}^{k}_{h}( x_{h}^{k},a_{h}^{k})\). When the agent reaches the state \(x_{H+1}^{k}\), it ends the \(k\)-th episode. Without loss of generality, we assume that the initial state \(x_{1}=\) is fixed. Our analysis can be generalized to the setting where \(x_{1}\) is sampled from a distribution on \(\).

**Policy and value functions**. For a policy \(=\{_{h}:()\}_{h=1}^{H}\), we denote by \(V_{h}^{}:\) and \(Q_{h}^{}:\) its value function and state-action value function at the \(h\)-th timestep, which characterizes the expected total rewards received by the agent under policy \(\) afterward, starting from some \(x_{h}=x\) (or \(x_{h}=x,a_{h}=a\), resp.), till the end of the episode. Specifically,

\[V_{h}^{}(x) :=_{}[_{h^{}=h}^{H}r_{h^{}}(x_{ h^{}},a_{h^{}})|x_{h}=x], x,\] (2.1) \[Q_{h}^{}(x,a) :=_{}[_{h^{}=h}^{H}r_{h^{}}(x_{ h^{}},a_{h^{}})|x_{h}=x,a_{h}=a],(x,a) .\] (2.2)

We know there exists an optimal policy \(^{*}\) which has the optimal value function for all initial states , that is, \(V_{h}^{^{*}}(s)=_{}V_{h}^{}(x)\) for all \(h[H]\) and \(x\). For simplicity, we abbreviate \(V^{^{*}}\) as \(V^{*}\) and the optimal state-action value function \(Q^{^{*}}\) as \(Q^{*}\). Moreover, the optimal value functions \(Q^{*}\) and \(V^{*}\) satisfy the following Bellman optimality equation , given by

\[V_{h}^{*}(x) =_{a}Q_{h}^{*}(x,a), V_{H+1}^{*}(x)=0,\] (2.3) \[Q_{h}^{*}(x,a) =(_{h}Q_{h+1}^{*})(x,a):=r_{h}(x,a)+_{x^{}_{h}( x,a)}[V_{h+1}^{*}(x^{} )],\] (2.4)

for all \((x,a,h)[H]\). We call \(_{h}\) the Bellman optimality operator at timestep \(h\). Also, for any two functions \(Q_{h}\) and \(Q_{h+1}\) on \(\), we define

\[_{h}(Q_{h},Q_{h+1};x,a):=Q_{h}(x,a)-_{h}Q_{h+1}(x,a), (x,a),\] (2.5)

as the Bellman residual at timestep \(h\) of \((Q_{h},Q_{h+1})\).

**Performance metric.** We measure the online performance of an agent after \(K\) episodes by _regret_. We assume that the learner predicts the optimal policy \(^{*}\) via \(^{k}\) in the \(k\)-th episode for each \(k[K]\). Then the regret of \(K\) episodes is defined as the cumulative suboptimality gap of \(\{^{k}\}_{k[K]}\)2,

\[(K)=_{k=1}^{K}V_{1}^{*}(x_{1})-V_{1}^{^{k}}(x_{1}).\] (2.6)

The target of sample-efficient online RL is to achieve sublinear regret (2.6) with respect to \(K\).

### Function Approximation: Model-Free and Model-Based Hypothesis

To handle MDPs with large or even infinite state space, we introduce a family of function approximators. In specific, we consider a hypothesis class \(=_{1}_{H}\), which can be specified to model-based and model-free settings respectively. Also, we denote by \(=_{1}_{H}\) as the space of Markovian policies. We now specify \(\) for model-free and model-based settings.

**Example 2.1** (Model-free hypothesis).: _For model-free hypothesis class, \(\) contains state-action value functions of the MDP, i.e., \(_{h}\{f_{h}:\}\). Specifically, for any \(f=(f_{1},,f_{H})\), we denote \(Q_{f}=\{Q_{h,f}\}_{h[H]}\) with \(Q_{h,f}=f_{h}\). Also, we denote the corresponding optimal state-value function \(V_{f}=\{V_{h,f}\}_{h[H]}\) with \(V_{h,f}()=_{a}Q_{h,f}(,a)\) and denote the corresponding optimal policy by \(_{f}=\{_{h,f}\}_{h[H]}\) with \(_{h,f}()=_{a}Q_{h,f}(,a)\). Finally, we denote the optimal state-action value function under the true model, i.e., \(Q^{*}\), by \(f^{*}\)._

**Example 2.2** (Model-based hypothesis).: _For model-based hypothesis class, \(\) contains models of the MDP, i.e., the transition kernel. Specifically, we denote \(f=_{f}=(_{1,f},,_{H,f})\). For any \((f,)\), we define \(V_{f}^{}=\{V_{h,f}^{n}\}_{h[H]}\) as the state-value function induced by model \(_{f}\) and policy \(\). We use \(V_{f}=\{V_{h,f}\}_{h[H]}\) to denote the corresponding optimal state-value function, i.e., \(V_{h,f}=_{}V_{,f}^{n}\). The corresponding optimal policy is denoted by \(_{f}=\{_{h,f}\}_{h[H]}\), where \(_{h,f}=_{}V_{h,f}^{n}\). Finally, we denote the true model \(\) of the MDP as \(f^{*}\)._

We remark that the main difference between the model-based hypothesis (Example 2.2) and the model-free hypothesis (Example 2.1) is that model-based RL directly learns the transition kernel of the underlying MDP, while model-free RL learns the optimal state-action value function. Since we do not add any specific structural form to the hypothesis class, e.g., linear function or kernel function, we are in the context of _general function approximations_[67; 37; 20; 89; 15].

## 3 Algorithm: Maximize to Explore for Online RL

In this section, we propose **Maximize to Explore** (MEX, Algorithm 1) for online RL in MDPs with general function approximations. With a novel single objective, **MEX** automatically balances the goal of exploration and exploitation. We first give a generic algorithm framework and then instantiate it to model-free (Example 2.1) and model-based (Example 2.2) hypotheses respectively.

**Generic algorithm design.** In each episode \(k[K]\), the agent first estimates a hypothesis \(f^{k}\) using historical data \(\{^{s}\}_{s=1}^{k-1}\) by maximizing a composite objective (3.1). Specifically, in order to achieve exploiting history knowledge while encouraging exploration, the agent considers a single objective that sums: **(a)** the negative loss \(-L_{h}^{k-1}(f)\) induced by the hypothesis \(f\), which represents the exploitation of the agent's current knowledge; **(b)** the expected total return of the optimal policy associated with this hypothesis, i.e., \(V_{1,f}\), which represents exploration for a higher return. With a tuning parameter \(>0\), the agent balances the weight put on the tasks of exploitation and exploration. The agent then predicts \(^{*}\) via the optimal policy with respect to the hypothesis that maximizes the composite exploration-exploitation objective function, i.e., \(_{f^{k}}\). Also, the agent executes certain exploration policy \(_{}(f^{k})\) to collect data \(^{k}=\{(x_{h}^{k},a_{h}^{k},r_{h}^{k},x_{h+1}^{k})\}_{h=1}^{H}\) and updates the loss function \(L_{h}^{k}(f)\). The choice of \(_{}(f^{k})\) depends on the specific MDP structure, and we refer to examples in Section 5 for detailed discussions.

We highlight that **MEX** is **not** a Lagrangian duality of constrained optimization objectives within data-dependent level-sets [37; 20; 15]. In fact, **MEX** only needs to fix the parameter \(\) across each episode. Thus \(\) is independent of data and predetermined, which contrasts Lagrangian methods that involve an inner loop of optimization for the dual variables. We also remark that we can rewrite (3.1) as a joint optimization \((f,)=*{argsup}_{f,}V_{1,f}^{}(x_{1}) -_{h=1}^{H}L_{h}^{k-1}(f).\) When \(\) tends to infinity, **MEX** can be reduced to vanilla Actor-Critic framework , where critic \(f\) minimizes estimation error and actor \(\) conducts greedy policy following the critic \(f\). In the following two parts,we instantiate Algorithm 1 to model-based and mode-free hypotheses by specifying the loss function \(L^{k}_{h}(f)\).

**Model-free algorithm.** For model-free hypothesis (Example 2.1), (3.1) becomes

\[f^{k}=*{argsup}_{f}\{_{a_{1} }Q_{1,f}(x_{1},a_{1})-_{h=1}^{H}L^{k-1}_{h}(f)\}.\] (3.2)

Regarding the choice of the loss function, for seek of theoretical analysis, to deal with MDPs with low Bellman eluder dimension  and MDPs of bilinear class , we assume the existence of certain function \(l\) which generalizes the notion of Bellman residual.

**Assumption 3.1**.: _Suppose the function \(l:_{h}_{h+1}( )\) satisfies: **i)** (**Generalized Bellman completeness) ** there exists an operator \(_{h}:_{h+1}_{h}\) such that for any \((f^{},f_{h},f_{h+1})_{h}_{h+1}\) and \(_{h}=(x_{h},a_{h},r_{h},x_{h+1}) \), it holds \(l_{f^{}}(f_{h},f_{h+1}),_{h}-l_{f^{}} (_{h}f_{h+1},f_{h+1}),_{h}=_{x_{ h+1}_{h}(|x_{h},a_{h})}l_{f^{}}(f_{h},f_{h+1}), _{h},\)_

_where we require that \(_{h}f^{*}_{h+1}=f^{*}_{h}\) and that \(_{h}f_{h+1}_{h}\) for any \(f_{h+1}_{h+1}\) and step \(h[H]\); **ii)** (**Boundedness)** it holds that \(_{f^{}}\|l_{f^{}}((f_{h},f_{h+1}),_{ h})\|_{} B_{l}\) for some constant \(B_{l}>0\)._

We then set the loss function \(L^{k}_{h}\) as an empirical estimation of the generalized squared Bellman error \(|_{x_{h+1}_{h}(|x_{h},a_{h})}[l_{f^{*}}((f_{h},f_ {h+1}),^{*}_{h})]|^{2}\), given by

\[L^{k}_{h}(f)=_{s=1}^{k}l_{f^{s}}(f_{h},f_{h+1}),^{*}_{h} ^{2}-_{f^{}_{h}_{h}}_{s=1}^{k}l_{f^{s}} (f^{}_{h},f_{h+1}),^{*}_{h}^{2}.\] (3.3)

We remark that the subtracted infimum term in (3.3) is to handle the variance terms in the estimation to achieve a fast theoretical rate. Similar essential ideas are also adopted by .

**Model-based algorithm.** For model-based hypothesis (Example 2.2), (3.1) becomes

\[f^{k}=*{argsup}_{f}\{_{}V^{}_ {1,_{f}}(x_{1})-_{h=1}^{H}L^{k-1}_{h}(f)\},\] (3.4)

which is a joint optimization over the model \(_{f}\) and the policy \(\). In the model-based algorithm, we choose the loss function \(L^{k}_{h}\) as the negative log-likelihood loss, defined as

\[L^{k}_{h}(f)=-_{s=1}^{k}_{h,f}(x^{s}_{h+1}|x^{s}_{h},a^{s}_{h }).\] (3.5)

## 4 Regret Analysis for MEX Framework

In this section, we establish a regret analysis for the MEX framework (Algorithm 1). We give a generic theoretical guarantee which holds for both model-free and model-based settings. We first present three key assumptions needed for sample-efficient learning with MEX. In Section 5, we specify the generic theory to specific examples of MDPs and hypothesis classes satisfying these assumptions.

Firstly, we assume that the hypothesis class \(\) is well-specified, containing the true hypothesis \(f^{*}\).

**Assumption 4.1** (Realizablity).: _We assume that the true hypothesis \(f^{*}\)._

Then we need to make a structural assumption on the MDP to ensure sample-efficient online learning. Inspired by Zhong et al. , we require the MDP to have low _Generalized Eluder Coefficient_ (GEC). A low GEC indicates that the agent can effectively mitigate out-of-sample prediction errors by minimizing in-sample errors derived from historical data. To introduce, we define a discrepancy function \(_{f^{}}(f;_{h}):( )\) which characterizes the error of a hypothesis \(f\) on data \(_{h}=(x_{h},a_{h},r_{h},x_{h+1})\). Specific choices of \(\) are given in Section 5 for concrete model-free and model-based examples.

**Assumption 4.2** (Low Generalized Eluder Coefficient ).: _We assume that given an \(>0\), there exists \(d()_{+}\), such that for any sequence of \(\{f^{k}\}_{k[K]}\), \(\{_{}(f^{k})\}_{k[K]}\),_

\[_{k=1}^{K}V_{1,f^{k}}-V^{_{f^{k}}}_{1}_{>0}\{_{h=1}^{H}_{k=1}^{K}_{s=1}^{k-1}_{_{h}_{}(f ^{*})}[_{f^{s}}(f^{k};_{h})]+++ HK\}.\]

_We denote the smallest number \(d()_{+}\) satisfying this condition as \(d_{}()\)._As is shown by Zhong et al. , the low-GEC MDP class covers almost all known theoretically tractable MDP instances, such as linear MDP [81; 39], linear mixture MDP [5; 57; 12], MDPs of low witness rank , MDPs of low Bellman eluder dimension , and MDPs of bilinear class .

Finally, we make a concentration-style assumption which characterizes how the loss function \(L_{h}^{k}\) is related to the expectation of the discrepancy function \([]\) appearing in the definition of GEC. For ease of presentation, we assume that \(\) is finite, i.e., \(||<\), but our result can be directly extended to an infinite \(\) using covering number arguments [72; 37; 49; 38].

**Assumption 4.3** (Generalization).: _We assume that \(\) is finite, i.e., \(||<+\), and that with probability at least \(1-\), for any episode \(k[K]\) and hypothesis \(f\), it holds that_

\[_{h=1}^{H}L_{h}^{k-1}(f^{*})-L_{h}^{k-1}(f)-_{h=1}^{H}_{s =1}^{k-1}_{_{h}_{}(f^{s})}[_{f^{s}}(f;_ {h})]+BH(HK/)+(||),\]

_where \(B=B_{l}^{2}\) for model-free hypothesis (Assumption 3.1) and \(B=1\) for model-based hypothesis._

Such a concentration style inequality is well known in the literature of online RL with general function approximation and similar analysis is also adopted by [37; 15]. With Assumptions 4.1, 4.2, and 4.3, we can present our main result (see Appendix D.1 for a proof).

**Theorem 4.4** (Online regret of **Mex** (Algorithm 1)).: _Under Assumptions 4.1, 4.2, and 4.3, by setting_

\[=}(1/)}{(H(HK/)+(| |)) B K}},\]

_then the regret of Algorithm 1 after \(K\) episodes is upper bounded by_

\[(K)}(1/)(H(HK/ )+(||)) B K},\]

_with probability at least \(1-\). Here \(d_{}()\) is defined in Assumption 4.2._

Theorem 4.4 shows that the regret of Algorithm 1 scales with the square root of the number of episodes \(K\) and the polynomials of horizon \(H\), \(\)\(d_{}(1/)\), and log covering number \((,1/K)\). When the number of episodes \(K\) tends to infinity, the average regret vanishes, meaning that the output policy of Algorithm 1 achieves global optimality. Since the regret of Algorithm 1 is sublinear with respect to the number of episodes \(K\), Algorithm 1 is proved to be sample-efficient. In Appendix C, we extend the algorithm framework and the analysis to the two-player zero-sum Markov game (MG) setting, for which we also extend the definition of GEC to two-player zero-sum MGs.

Besides, as we can see from Theorem 4.4 and its specifications in Section 5, \(\) matches existing theoretical results in the literature of online RL with general function approximations [89; 67; 20; 37; 19; 2]. But in the meanwhile, \(\) does not require explicitly solving a constrained optimization problem within data-dependent level-sets or performing a complex sampling procedure. This advantage makes \(\) a principled approach with easier practical implementation. We conduct deep RL experiments for \(\) in Section 6 to demonstrate its power in complicated online problems.

## 5 Examples of **Mex** Framework

In this section, we specify Algorithm 1 to model-based and model-free hypothesis classes for various examples of MDPs of low GEC (Assumption 4.2), including MDPs with low witness rank , MDPs with low Bellman eluder dimension , and MDPs of bilinear class . For ease of presentation, we assume that \(||<\), but our result can be directly extended to infinite \(\) using covering number arguments [72; 37; 49]. All the proofs of the propositions in this section are in Appendix E.

We note that another important step in specifying Theorem 4.4 to concrete hypothesis classes is to check Assumption 4.3 (supervised learning guarantee). It is worth highlighting that, in our analysis, for both model-free and model-based hypotheses, we provide supervised learning guarantees in a neat and unified manner, independent of specific MDP structures.

### Model-free online RL in Markov Decision Processes

In this subsection, we specify Algorithm 1 for model-free hypothesis class \(\) (Example 2.1). For a model-free hypothesis class, we choose the discrepancy function \(\) as, given \(_{h}=(x_{h},a_{h},r_{h},x_{h+1})\),

\[_{f^{}}(f;_{h})=(_{x_{h+1}_{ h}(|x_{h},a_{h})}[l_{f^{}}((f_{h},f_{h+1}),_{h})] )^{2}.\] (5.1)where the function \(l:_{h}_{h+1}( )\) satisfies Assumption 3.1. We specify the choice of \(l\) in concrete examples of MDPs later. In the following, we check and specify Assumptions 4.2 and 4.3 in Section 4 for model-free hypothesis classes.

**Proposition 5.1** (Generalization: model-free RL).: _We assume that \(\) is finite, i.e., \(||<+\). Then under Assumption 3.1, with probability at least \(1-\), for any \(k[K]\) and \(f\), it holds that_

\[_{h=1}^{H}L_{h}^{k-1}(f^{*})-L_{h}^{k-1}(f)-_{h=1}^{H}_{s= 1}^{k-1}_{_{h}_{}(f^{*})}[_{f^{s}}(f;_ {h})]+B_{l}^{2}H(HK/)+(||),\]

_where \(L\) and \(\) are defined in (3.3) and (5.1) respectively. Here \(B_{l}\) is specified in Assumption 3.1._

Proposition 5.1 specifies Assumption 4.3 for model-free hypothesis classes. For Assumption 4.2, we need structural assumptions on the MDP. Given an MDP with generalized eluder dimension \(d_{}\), we have the following corollary of our main theoretical result (Theorem 4.4).

**Corollary 5.2** (Online regret of MEX: model-free hypothesis).: _Given an MDP with generalized eluder coefficient \(d_{}()\) and a finite model-free hypothesis class \(\) with \(f^{*}\), under Assumption 3.1, setting_

\[=}(1/)}{(H(HK/)+(| |)) B_{l}^{2} K}},\] (5.2)

_then the regret of Algorithm 1 after \(K\) episodes is upper bounded by_

\[(T) B_{l}}(1/) (H(HK/)+(||)) K},\] (5.3)

_with probability at least \(1-\). Here \(B_{l}\) is specified in Assumption 3.1._

Corollary 5.2 can be directly specified to MDPs with low GEC, including MDPs with low Bellman eluder dimension  and MDPs of bilinear class . See Appendix E.1 for a detailed discussion.

### Model-based online RL in Markov Decision Processes

In this subsection, we specify Algorithm 1 for model-based hypothesis class \(\) (Example 2.2). For model-based hypothesis class, we choose the discrepancy function \(\) in Assumption 4.2 and 4.3 as the hellinger distance. Given data \(_{h}=(x_{h},a_{h},r_{h},x_{h+1})\), we let

\[_{f^{}}(f;_{h})=D_{}(_{h,f}(|x_ {h},a_{h})\|_{h,f^{*}}(|x_{h},a_{h})),\] (5.4)

where \(D_{}(\|)\) denotes the Hellinger distance. We note that by (5.4), the discrepancy function \(\) does not depend on the input \(f^{}\). In the following, we check and specify Assumption 4.2 and 4.3.

**Proposition 5.3** (Generalization: model-based RL).: _We assume that \(\) is finite, i.e., \(||<+\). Then with probability at least \(1-\), for any \(k[K]\), \(f\), it holds that_

\[_{h=1}^{H}L_{h}^{k-1}(f^{*})-L_{h}^{k-1}(f)-_{h=1}^{H}_{s =1}^{k-1}_{_{h}_{}(f^{*})}[_{f^{*}}(f;_ {h})]+H(H/)+(||),\]

_where \(L\) and \(\) are defined in (3.5) and (5.4) respectively._

Proposition 5.3 specifies Assumption 4.3 for model-based hypothesis classes. For Assumption 4.2, we also need structural assumptions on the MDP. Given an MDP with generalized eluder dimension \(d_{}\), we have the following corollary of our main theoretical result (Theorem 4.4).

**Corollary 5.4** (Online regret of MEX: model-based hypothesis).: _Given an MDP with generalized eluder coefficient \(d_{}()\) and a finite model-based hypothesis class \(\) with \(f^{*}\), by setting_

\[=}(1/)}{(H(H/)+(| |)) K}},\]

_then the regret of Algorithm 1 after \(K\) episodes is upper bounded by, with probability at least \(1-\),_

\[(K)}(1/)(H(H/ )+(||)) K},\] (5.5)

Corollary 5.4 can be directly specified to MDPs with low GEC, including MDPs with low witness rank . We refer to Appendix E.2 for a detailed discussion.

Experiments

In this section, we aim to answer the following two questions: **(a)** What are the practical approaches to implementing \(\) in both model-based (\(\)-\(\)) and model-free (\(\)-\(\)) settings? **(b)** Can \(\) handle challenging exploration tasks, especially in sparse reward scenarios?

**Experimental setups.** We evaluate the effectiveness of \(\) by assessing its performance in both standard \(\) locomotion tasks and sparse reward locomotion and navigation tasks within the MuJoCo  environment. For sparse reward tasks, we select \(\), \(\), \(\), \(\), and \(\) adapted from Yu et al. , where the agent receives a reward only when it successfully attains the desired velocity or goal. To adapt to deep RL settings, we consider infinite-horizon \(\)-discounted MDPs and \(\) variants. We report the results averaged over five random seeds. The full experimental settings are in Appendix H.

**Implementation details.** For the model-based variant \(\)-\(\), we use the following objective:

\[_{}_{}\ _{(x,a,r,x^{})}[ _{}(x^{},r\,|\,x,a)]+^{}_{ x}V^{}_{}(x),\] (6.1)

where we denote by \(\) the initial state distribution, \(\) the replay buffer, and \(^{}\) corresponds to \(1/\) in the previous theory sections. We leverage the _score function_ to obtain the model value gradient \(_{}V^{}_{_{}}\) in a similar way to likelihood ratio policy gradient , with the gradient of action log-likelihood replaced by the gradient of state and reward log-likelihood in the model. Specifically,

\[_{}\,_{x}V^{}_{_{}}(x) =_{^{}_{}}r+ V^{}_{_{}}(x^{})-Q^{}_{_{}}(x,a)_{ }_{}(x^{},r\,|\,x,a),\] (6.2)

where \(^{}_{}\) is the trajectory under policy \(\) and transition \(_{}\), starting from \(\). We refer the readers to previous works [62; 76] for a derivation of (6.2). The model \(\) and policy \(\) in (6.1) are updated iteratively in a Dyna  style, where model-free policy updates are performed on model-generated data. Particularly, we adopt \(\) to update the policy \(\) and estimate the value \(Q^{}_{_{}}\) by performing temporal difference on the model data generated by \(_{}\). We also follow  to update the model using mini-batches from \(\) and normalize the advantage \(r_{h}+ V^{}_{_{}}-Q^{}_{_{}}\) within each mini-batch.

For the model-free variant \(\)-\(\), we observe from (3.2) that adding a maximization bias term to the standard TD error is sufficient for exploration. However, this may lead to instabilities as the bias term only involves the state-action value function of the current policy, and thus the policy may be ever-changing. To address this issue, we adopt a similar treatment as in \(\) by subtracting a baseline state-action value from random policy \(=()\) and obtain the following objective:

\[_{}_{}\ _{}[r+ Q_{}(x^{ },a^{})-Q_{}(x,a)^{2}]+^{} _{}_{a}Q_{}(x,a)-_{a }Q_{}(x,a).\] (6.3)

We update \(\) and \(\) in (6.3) iteratively in an actor-critic fashion. Due to space limits, we refer the readers to Appendix H for more implementation details of \(\)-\(\).

**Results.** We report the performance of \(\)-\(\) and \(\)-\(\) in Figures 1 and 2, respectively. We compare \(\)-\(\) with \(\), where our method differs from \(\)_only_ in the inclusion of the value gradient in (6.2) during model updates. We find that \(\)-\(\) offers an easy implementation with minimal computational overhead and yet remains highly effective across sparse and standard MuJoCo tasks. Notably, in the sparse reward settings, \(\)-\(\) excels at achieving the goal velocity and outperforms \(\) by a stable margin. In standard \(\) tasks, \(\)-\(\) showcases greater sample efficiency in challenging high-dimensional tasks with higher asymptotic returns.

We then compare \(\)-\(\) with the model-free baseline \(\). We observe that \(\) fails in many sparse reward tasks, while \(\)-\(\) significantly boosts the performance. In standard MuJoCo gym tasks, \(\)-\(\) also steadily outperforms \(\) with faster convergence and higher final returns.

## 7 Conclusions

In this paper, we introduce a novel RL algorithm framework--_Maximize to Explore_ (\(\))--aimed at striking a balance between exploration and exploitation in online learning scenarios. \(\) is provably sample-efficient under general function approximations and is easy to implement. Theoretically, we prove that under mild structural assumptions (low generalized eluder coefficient (GEC)), \(\) achieves\(}()\)-online regret for MDPs. We further extend the definition of GEC and MEX framework to two-player zero-sum Markov games (see Appendix C) and also prove the \(}()\)-online regret. In practice, we adapt MEX to deep RL methods in both model-based and model-free styles and apply them to sparse-reward MuJoCo environments, outperforming baselines significantly. We hope our work can shed light on future research of designing both statistically efficient and practically effective RL algorithms with powerful function approximation.