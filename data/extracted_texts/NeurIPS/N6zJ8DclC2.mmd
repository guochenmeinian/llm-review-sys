# Natural Counterfactuals With Necessary Backtracking

Guang-Yuan Hao\({}^{1,5}\), Jiji Zhang\({}^{1}\), Biwei Huang\({}^{2}\), Hao Wang\({}^{3}\), Kun Zhang\({}^{4,5}\)

\({}^{1}\)Chinese University of Hong Kong, \({}^{2}\)University of California San Diego, \({}^{3}\)Rutgers University

\({}^{4}\)Carnegie Mellon University, \({}^{5}\)Mohamed bin Zayed University of Artificial Intelligence

guangyuanhao@outlook.com, jijizhang@cuhk.edu.hk

bih007@ucsd.edu, hw488@cs.rutgers.edu, kunz1@cmu.edu

The first two authors contributed equally to this work.

###### Abstract

Counterfactual reasoning is pivotal in human cognition and especially important for providing explanations and making decisions. While Judea Pearl's influential approach is theoretically elegant, its generation of a counterfactual scenario often requires too much deviation from the observed scenarios to be feasible, as we show using simple examples. To mitigate this difficulty, we propose a framework of _natural counterfactuals_ and a method for generating counterfactuals that are more feasible with respect to the actual data distribution. Our methodology incorporates a certain amount of backtracking when needed, allowing changes in causally preceding variables to minimize deviations from realistic scenarios. Specifically, we introduce a novel optimization framework that permits but also controls the extent of backtracking with a "naturalness" criterion. Empirical experiments demonstrate the effectiveness of our method. The code is available at https://github.com/GuangyuanHao/natural_counterfactuals.

## 1 Introduction

Counterfactual reasoning, which aims to answer what a feature of the world would have been if some other features had been different, is often used in human cognition, to perform self-reflection, provide explanations, and inform decisions [29; 6]. For AI systems to achieve human-like abilities of reflection and decision-making, incorporating counterfactual reasoning is crucial. Judea Pearl's structural approach to counterfactual modeling and reasoning  has been especially influential in recent decades. Within this framework, counterfactuals are conceptualized as being generated by surgical interventions on the variables to be changed, while leaving its causally upstream variables intact and all downstream causal mechanisms invariant. These counterfactuals are thoroughly non-backtracking in that the desired change is supposed to happen without tracing back to changes in causally preceding variables. Reasoning about these counterfactuals can yield valuable insights into the consequences of hypothetical actions. Consider a scenario: a sudden brake of a high-speed bus caused Tom to fall and injure Jerry. Non-backtracking counterfactuals would tell us that if Tom had stood still (despite the sudden braking), then Jerry would not have been injured. Pearl's approach supplies a principled machinery to reason about conditionals of this sort, which are usually useful for explanation, planning, and responsibility allocation.

However, such surgical interventions are sometimes so removed from what is or can be observed that it is difficult or even impossible to learn from data the consequences of such interventions. In the previous example, preventing Tom's fall in a sudden braking scenario requires defying mechanisms that are difficult or even physically impossible to disrupt. As a result, there are likely to be no data points in the reservoir of observed scenarios that are consistent with a person standing still during asudden braking. If so, it can be very challenging to learn to generate such a counterfactual from the available data, as we will demonstrate in our experiments.2

In this paper, we introduce a notion of "natural counterfactuals" to address the above issue with non-backtracking counterfactuals. Our notion will allow a certain amount of backtracking, to keep the counterfactual scenario "natural" with respect to the available observations. For example, rather than the unrealistic scenario where Tom does not fall at a sudden bus stop, a more natural counterfactual scenario to realize the change to not-falling would involve changing at the same time some causally preceding events, such as changing the sudden braking to gradually slowing down. On the other hand, our notion also constrains the extent of backtracking; in addition to a naturalness criterion, we formulate an optimization scheme to encourage minimizing backtracking while meeting the naturalness criterion.

As we will show empirically, this new notion of counterfactual is especially useful from a machine learning perspective. When interventions lead to unrealistic scenarios relative to the training data, predicting counterfactual outcomes in such scenarios can be highly uncertain and inaccurate . This issue becomes particularly pronounced when non-parametric models are employed, as they often struggle to generalize to unseen, out-of-distribution data . The risk of relying on such counterfactuals is thus substantial, especially in high-stake applications like healthcare and autonomous driving. In contrast, our approach amounts to searching for feasible changes that keep generated counterfactuals within their original distribution, employing backtracking when needed. This strategy effectively reduces the risk of inaccurate predictions and ensures more reliable results.

In short, our approach aims to achieve the goal of ensuring that counterfactual scenarios remain sufficiently realistic with respect to the actual data distribution by permitting minimal yet necessary backtracking. It is designed with two major elements. First, we need criteria to determine the feasibility of interventions, ensuring they are realistic with respect to the actual data distribution. Second, we appeal to backtracking when and only when it is necessary to avoid infeasible interventions, and need to develop an optimization framework to realize this strategy. To be clear, our aim is not to propose uniquely correct semantics for counterfactuals or to show that our notion of counterfactuals is superior to others for all purposes. Rather, our goal is to develop a notion of counterfactual that is theoretically well-motivated and practically useful in some contexts and for certain purposes. There may in the end be a general semantics to unify fruitfully our notion and other notions of counterfactual in the literature, but we will not attempt that in this paper. Our working assumption is that there can be different but equally coherent notions of counterfactuals, and we aim to show that the notion developed in this paper is particularly useful for generating counterfactual instances that are feasible with respect to data.

The key contributions of this paper include:

* Developing a more flexible and realistic notion of natural counterfactuals, addressing the limitations of non-backtracking reasoning while keeping its merits as far as possible.
* Introducing an innovative and feasible optimization framework to generate natural counterfactuals.
* Detailing a machine learning approach to produce counterfactuals within this framework, with empirical results from simulated and real data showcasing the superiority of our method compared to non-backtracking counterfactuals.

## 2 Related Work

**Non-backtracking Counterfactual Generation.** As will become clear, our theory is presented in the form of counterfactual sampling or generation. [28; 18; 8; 30] use the deep generative models to learn a causal model from data given a causal graph, and then use the model to generate non-backtracking counterfactuals. Our experiments will examine some of these models and demonstrate their difficulties in dealing with interventions that are unrealistic relative to training data, due to the fact that the causal model learned from data is not reliable in handling inputs that are out-of-distribution.

**Backtracking Counterfactuals.** Backtracking in counterfactual reasoning has drawn plenty of attention in philosophy , psychology , and cognitive science .  proposes a theory that is in a spirit similar to ours, in which backtracking is allowed but limited by some requirement of matching as much causal upstream as possible.  shows that people use both backtracking and non-backtracking counterfactuals in practice and tend to use backtracking counterfactuals when explicitly required to explain causes for the supposed change in a counterfactual.  is the first work, as far as we know, to formally introduce backtracking counterfactuals in the framework of structural causal models. The main differences between this work and ours are that  requires "full" backtracking given a causal model, all the way back to exogenous noise terms, and measures closeness in terms of these noise terms, whereas we limit backtracking to what is needed to render the counterfactual "natural", and the measure of closeness in our framework can be defined directly on endogenous, observed variables, which is arguably desirable because changes to the unobserved variables are by definition outside of our control and not actionable. More detailed comparisons can be found in Sec. F of the Appendix.

**Algorithmic Recourse and Counterfactual Explanations.** Algorithmic recourse (AR)  and counterfactual explanations (CE) [36; 10; 23; 3; 24; 34; 32; 37] represent two leading strategies within explainable AI that heavily exploit counterfactual reasoning or reasoning about intervention effects. Various studies [16; 1; 27; 15; 37] explore concepts of feasibility akin to that of "naturalness" in this work, though apply them in quite different contexts. The objectives of CE and AR are to pinpoint minimal alterations to an input (in CE) or minimal interventions (in AR) that would either induce a desirable output from a predictive model (in CE) or lead to a desirable outcome (in AR). In contrast, our work here focuses primarily on generating counterfactuals. While this paper does not directly address CE or AR, our notion of natural counterfactuals is likely to be very relevant to these tasks.

## 3 Preliminaries

In this section, we begin by outlining various basic concepts in causal inference, followed by an introduction to non-backtracking counterfactuals.

**Structural Causal Models (SCM).** We assume there is an underlying recursive SCM  of the following sort to represent the data generating process. An SCM \(:=,,,p()\) consists of exogenous (noise) variables \(=\{_{1},...,_{N}\}\), endogenous (observed) variables \(=\{_{1},...,_{N}\}\), functions \(=\{f_{1},...,f_{N}\}\), and a joint distribution \(p()\) of noise variables, which are assumed to be jointly independent. Each function, \(f_{i}\), specifies how an endogenous variable \(_{i}\) is determined by its parents \(_{i}\):

\[_{i}:=f_{i}(_{i},_{i}), i=1,...,N\] (1)

Such an SCM entails a (causal) Bayesian network over the observed variables, consisting of the directed acyclic graph over \(\) in which there is an arrow from each member of \(_{i}\) to \(_{i}\), and the joint distribution of \(\) induced by \(\) and \(p()\). **In our setting, we assume this causal graph is known and samples from this joint distribution are available, but f and \(p()\) are not given**, though some assumptions on \(\) and \(p()\) will be needed for identifiability.

**Local Mechanisms.** Functions in \(\) are usually regarded as representing local mechanisms. In this paper, however, we will use the term "local mechanism" to refer to the conditional distribution of an endogenous variable given its parent variables, i.e., \(p(_{i}|_{i})\) for \(i=1,...,N\), which can be estimated from the available information. Note that a local mechanism in this sense implicitly encodes the properties of the corresponding noise variable; given a fixed value of \(_{i}\), the noise \(_{i}\) determines the probability distribution of \(_{i}\). Hence, the term "local mechanism" will also be used sometimes to refer to the distribution of the noise variable \(p(_{i})\).

**Intervention**. Given an SCM, an intervention on a set of endogenous variables \(\) is represented by replacing the functions for members of \(\) with constant functions \(X=x^{*}\), where \(X\) and \(x^{*}\) is the target value of \(X\), and leaving the functions for other variables intact .3

**Non-Backtracking Counterfactuals.** Let \(\), \(\), and \(\) be sets of endogenous variables. A general counterfactual question takes the following form: given evidence \(=\), what would the value of \(\) have been if \(\) had taken the value setting a"? In this paper, we focus on a special case of thisquestion in which \(=\), i.e., the evidence is a complete data point covering all observed variables. That is, given an actual data point, we consider what the data point would have been if a variable had taken a different value than its actual value. The Pearlian, non-backtracking reading of this question takes the counterfactual supposition of \(=^{*}\) to be realized by an intervention on \(\). This means that in the envisaged counterfactual scenario, all variables in the causal upstream of \(\) keep their actual values while \(\) takes a different value. As mentioned, a potential problem is that such a scenario is outside of the support of available data, and so it is often unreliable to make inferences about the downstream variables in the scenario based on the available data.

## 4 A Framework for Natural Counterfactuals

\(Do()\) and \(Change()\) Operators.Using Pearl's influential do-operator, the non-backtracking mode appeals to \(do(=^{*})\), an intervention to set the value \(\) to \(^{*}\), to generate a counterfactual instance. However, in our framework, the counterfactual supposition is not necessarily realized by an intervention on \(\), while keeping all its causal upstream intact. Instead, when \(do(=^{*})\) results in a counterfactual setting that violates a naturalness criterion, some backtracking will be invoked. To differentiate from the intervention \(do(=^{*})\), we will often use \(change()\) and write \(change(=^{*})\) to denote a desired modification in \(\). Different semantics for counterfactuals correspond to different interpretations of the change operator. In this paper, we explore an interpretation that connects the change operator to the do operator in a relatively straightforward manner.

The basic idea is that \(change(=^{*})\) will correspond to \(do(=^{*})\) for some set \(\) that includes \(\) and possibly some of \(\)'s causal ancestors. When \(=\{\}\), this is equivalent to a non-backtracking interpretation. In general, however, some variables in \(\)'s causal upstream need to change together with \(\) in order to keep the counterfactual scenario within the relevant support. A central component of our approach is to design a way to determine \(\) and \(^{*}\), given a request of \(change(=^{*})\). We will call the resulting \(do(=^{*})\) the **least-backtracking feasible (LBF) intervention** for \(change(=^{*})\). Once the LBF intervention is determined, inferences can be made in the same fashion as in Pearl's approach .

To determine the LBF intervention for \(change(=^{*})\), we formulate it as an optimization problem to search for a minimal change of \(\)'s causal ancestors that, together with changing \(\) to \(^{*}\), satisfy a "naturalness" criterion. Let \(()\) denote the set of \(\)'s ancestors in the given causal graph together with \(\) itself. We define the optimization framework, **F**easible **I**ntervention **O**ptimization (FIO), as follows:

\[)^{*}}{} D(an(),an()^{*})\] (2) s.t. \[=^{*},\] \[g_{n}(an()^{*})>.\]

where \(an()\) and \(an()^{*}\) represent the actual value setting and the counterfactual value setting of \(()\) respectively (note that \(()\)). \(g_{n}()\) measures the naturalness of the counterfactual value setting of \(()\) and \(\) is a small constant. So the optimization has a naturalness criterion as a constraint. On the other hand, \(D()\) is a distance metric designed to encourage the counterfactual value setting to invoke the least amount of backtracking. Below we develop these two components in Sec. 4.1 and Sec. 4.2, respectively. Once we obtain the counterfactual value \(an()^{*}\) by FIO, the variable set for the LBF intervention includes \(\) and other variables corresponding to the difference between \(an()^{*}\) and \(an()\), i.e., \(\) is always included in \(\) even when \(an()^{*}=an()\).

### Naturalness Constraints

As indicated previously, the intended purpose of the naturalness constraint is to confine the counterfactual instance sufficiently within the support of the data distribution. Roughly and intuitively, the more frequently a value occurs, the more it is considered to be "natural". Moreover, we would like to use a measure of naturalness that takes into account local mechanisms according to the given causal graph rather than just the joint distribution. Therefore, we propose to assess naturalness by examining the distribution characteristics, such as density, of each variable's value \(_{j}=_{j}^{*}\) given the variable's local mechanism \(p(_{j}|pa_{j}^{*})\), where \(_{j}\)\(()\) and \(pa_{j}^{*}\) denotes its parental value setting.4

#### 4.1.1 Local Naturalness Criteria

We start by proposing some measures of the naturalness of one variable's value, \(_{j}^{*}\), within the counterfactual data point \(an()^{*}\) in this section, followed by defining a measure of the overall naturalness of \(an()^{*}\) in the next.

Informally, a value satisfies the criterion of _local \(\)-natural generation_ if it is a natural outcome of its local mechanism. The proposed measures of naturalness will depend on the specific value \(_{j}=_{j}^{*}\), alongside its parent value \(_{j}=pa_{j}^{*}\), noise value \(_{j}=_{j}^{*}\), and the corresponding local mechanism, expressed by \(p(_{j}|_{j}=pa_{j}^{*})\) or \(p(_{j})\). The cumulative distribution function (CDF) for noise variable \(_{j}\) at \(_{j}=_{j}^{*}\) is \(F(_{j}^{*})=_{-}^{_{j}^{*}}p(_{j})d _{j}\), and for the conditional distribution \(p(_{j}|_{j}=pa_{j}^{*})\) at \(_{j}=_{j}^{*}\) is \(F(_{j}|pa_{j}^{*})=_{-}^{_{j}^{*}}p(_ {j}=_{j}^{*}|pa_{j}^{*})d_{j}\).

We propose the following potential criteria based on entropy-normalized density, CDF of exogenous variables, and CDF of conditional distributions, respectively. The entropy-normalized naturalness measure evaluates the naturalness of \(_{j}^{*}\) in relation to its local mechanism \(p(_{j}|_{j}=pa_{j}^{*})\). The CDF-based measures, namely the latter two criteria, consider data points in the tails to be less natural. Each of these criteria has its own intuitive appeal, and their relative merits will be discussed subsequently. Below, we use \(g_{l}(_{j}^{*})\) to represent a (local) naturalness measure of \(_{j}^{*}\). We consider the following three possible measures:

1. Entropy-Normalized Measure:\(g_{l}(_{j}^{*})=p(_{j}^{*}|pa_{j}^{*})e^{H(_{j}|pa_{j}^ {*})}\), where \(H(_{j}|pa_{j}^{*})=[- p(_{j}|pa_{j}^{*})]\);
2. Exogenous CDF Measure: \(g_{l}(_{j}^{*})=(F(_{j}^{*}),1-F(_{j}^{*}))\);
3. Conditional CDF Measure: \(g_{l}(_{j}^{*})=(F(_{j}^{*}|pa_{j}^{*}),1-F( _{j}^{*}|pa_{j}^{*}))\);

where the function \(()\) returns the minimum of the given values. When \(g_{l}(_{j}^{*})>\), we say the \(_{j}^{*}\) given its causal parents' values satisfies the criterion of local \(\)-natural generation.

Some comments on these choices are in order:

**Choice (1): Entropy-Normalized Measure.** Specifically, Choice (1), \(p(_{j}^{*}|pa_{j}^{*})e^{H(_{j}|pa_{j}^{*})}\), can be rewritten as \(e^{ p(_{j}^{*}|pa_{j}^{*})+[- p(_{j}|pa_ {j}^{*})]}\), where \(- p(_{j}^{*}|pa_{j}^{*})\) can be seen as the measure of surprise of \(_{j}^{*}\) given \(pa_{j}^{*}\) and \([- p(_{j}|pa_{j}^{*})]\) can be considered as the expectation of surprise of the local mechanism \(p(_{j}|pa_{j}^{*})\). Hence, the measure quantifies the relative naturalness (i.e., negative surprise) of \(_{j}\). Implementing this measure is usually straightforward when employing a parametric SCM where the conditional distributions can be explicitly represented.

**Choice (2): Exogenous CDF Measure.** If using a parametric SCM, we might directly measure differences on exogenous variables. However, in a non-parametric SCM, exogenous variables are in general not identifiable, and different noise variables may have different distributions. Still, we may consider using the CDF of exogenous variables to align the naturalness of different distributions, based on a common assumption for non-parametric SCMs in the machine learning system. The assumption is that the support of the local mechanism \(p(_{j}|_{j}=pa_{j}^{*})\) does not contain disjoint sets, the function \(f_{j}\) in the SCM is monotonically increasing with respect to the noise variable \(_{j}\), which is assumed to follow a standard Gaussian distribution . Data points from the tails of a standard Gaussian can be thought of as improbable events. Hence, \(_{j}=_{j}^{*}\) satisfies local \(\)-natural generation when its exogenous CDF \(F(_{j}^{*})\) falls within the range \((,1-)\). In practice, for a single variable, \(_{j}\) is a one-dimensional variable, and it is easier to enforce the measure than Choice (1), which involves conditional distributions.

**Choice (3): Conditional CDF Measure.** The measure treats a particular value in the tails of local mechanism \(p(_{j}|pa_{j}^{*})\) as unnatural. Hence, \(_{j}=_{j}^{*}\) meets local \(\)-natural generation when \(F(_{j}=_{j}^{*}|pa_{j}^{*})\) falls within the range \((,1-)\) instead of tails. This measure can be used in parametric models where the conditional distribution can be explicitly represented. It can also be easily used in non-parametric models and the measure is equivalent to Choice (2) when those modelssatisfy the assumption mentioned in Choice (2), since the CDF \(F(_{j}|pa_{j}^{*})\) has a one-to-one mapping with the CDF \(F(_{j})\), i.e., \(F(_{j}^{*}|pa_{j}^{*})=F(_{j}^{*})\), when \(_{j}^{*}=f(pa_{j}^{*},_{j}^{*})\).

#### 4.1.2 Global Naturalness Criteria

Given a local naturalness measure \(g_{l}\), we simply define a global naturalness measure for \(an()^{*}\) as

\[g_{n}(an()^{*})=_{_{j}^{*} an()^{*}}(g_{l }(_{j}^{*})).\] (3)

That is, \(g_{n}(an()^{*})\) returns the smallest local naturalness value among members of \(()\). Finally, we can define a criterion of \(\)-natural generation to assess whether the counterfactual value \(an()^{*}\) is sufficiently natural.

**Definition 1** (\(\)-Natural Generation).: _Given an SCM containing a set \(\), let \(()\) contain all ancestors of \(\) and \(\) itself. A value setting \(()=an()^{*}\) satisfies \(\)-natural generation, if and only if, \(g_{n}(an()^{*})>\) and \(\) is a small constant._

Obviously, a larger value of \(\) implies a higher standard for the naturalness of the counterfactual value setting \(an()^{*}\). To consider only feasible interventions, we require \(an()^{*}\) to meet \(\)-natural generation, which is a constraint used in FIO.

### Distance Measure to Limit Backtracking

We now turn to the distance measure in Eqn. 2 of the FIO framework. We considered two distinct distance measures in this work. The first prioritizes minimizing changes in the observed causal ancestors of the target variable of the desired change. The second focuses on reducing alterations in local mechanisms, regarding them as inherent costs of an intervention. Due to space limitations, we will introduce here only the simpler measure in terms of minimal changes in the observable causal ancestors. A discussion of the other measure can be found in Sec. G.

For our purpose, the \(L^{1}\) norm is a good choice, as it encourages sparse changes and thus sparse backtracking:

\[D(an(),an()^{*})=\|an()^{*}-an() \|_{1}\] (4)

where \(an()\) and \(an()^{*}\) represent the actual value and counterfactual value of \(\)'s ancestors \(()\) respectively, where \(()\). Because endogenous variables may vary in scale, e.g., a normal distribution with a range of \((-,)\) versus a uniform distribution over the interval \(\), we standardize each endogenous variable before computing the distance. This normalization ensures a consistent and fair evaluation of changes.

Implicitly, this distance metric favors changes in variables that are proximal to \(\), since altering a more remote variable typically results in changes to more downstream variables. In the extreme case, when the value \(an()^{*}\) corresponding to \(do(=^{*})\), i.e., the one corresponding to the non-backtracking counterfactual, already meets the \(\)-natural generation criterion, the distance metric \(D(an(),an()^{*})\) will achieve the minimal value \(|-^{*}|\). In such a case, no backtracking is needed and the non-backtracking counterfactual will be generated. However, If \(do(=^{*})\) does not meet the \(\)-natural generation criterion, it becomes necessary to backtrack.

### Identifiability of Natural Counterfactuals

As said, we assume we do not have prior knowledge of the functions of the SCM and so noise variables are in general not identifiable from the observed variables. However, if we assume the SCM satisfies the conditions of the following theorem, then the counterfactual instance resulting from an LBF intervention is identifiable.

**Theorem 4.1** (Identifiable Natural Counterfactuals).: _Given the causal graph and the joint distribution over \(\), suppose \(_{i}\) satisfies the following structural causal model: \(_{i}:=f_{i}(_{i},_{i})\) for any \(_{i}\), assume every \(f_{i}\), though unknown, is smooth and strictly monotonic w.r.t. \(_{i}\) for fixed values of \(_{i}\). Then, given an actual data point \(=\), with an LBF intervention \(do(=^{*})\) (satisfying the criterion of \(\)-natural generation), the counterfactual instance \(=^{*}\) is identifiable: \(=^{*}|do(=^{*}),=\)._This theorem confirms the identifiability of our natural counterfactuals from the causal graph and the joint distribution over the observed variables.5 Specifically, since \(do(=^{*})\) satisfies the criterion of \(\)-natural generation, it guarantees that the resulting counterfactual instance falls within the support of the observed joint distribution. Then, building on Theorem 1 from , we can demonstrate that using the actual data distribution allows for the inference of natural counterfactuals without knowing the functions or the noise distributions of the SCM.

## 5 A Practical Method for Generating Natural Counterfactuals

In this section, we provide a practical method for solving (approximately) the FIO problem described in the last section. We assume that we are given data sampled from the joint distribution of the endogenous variables and a causal graph, and that the underlying SCM satisfies the assumptions in Theorem 4.1. We learn a generative model for the endogenous variables from data, serving as an estimated SCM to generate natural counterfactuals: \(_{i}:=_{i}(_{i},_{i})\) for \(i=1,...,N\), where \(_{i}\) is assumed to be reversible given \(_{i}\). Note that, unlike the functions in the true SCM, these learned functions in general do not generalize well to out-of-distribution data, as demonstrated in the experiments, which, recall, is a main motivation for employing natural counterfactuals instead. For simplicity, we assume the noise distribution is standard Gaussian, though the identifiability of natural counterfactuals only requires that noise variables are continuous and do not depend on the specific form of the noise distribution. The specific FIO problem we target plugs the distance measure (Eqn. 4) and the naturalness measure from Choice (3) in Sec. 4.1.1 (or Choice (2), which is equivalent to Choice (3) given the assumptions) into Eqn. 2:

\[)^{*}}{}\|an()^{*}-an()\|_{1}\] (5) \[s.t.=^{*},\] \[<F(_{j}=_{j}^{*}|pa_{j}^{*})<1- ,_{j}().\]

Again, in theory, there may be no solution for \(an(A)^{*}\) if the naturalness criterion is demanding. In the extreme case, for example, even when \(^{*}=\), if \(\) is set so high that the actual instance does not satisfy the condition of \(\)-natural generation, then no solution exists.

We propose to solve this optimization problem using the following approximate method. Since the estimated functions \(_{j}\) are assumed to be reversible, we can reformulate the problem of searching for optimal values of the endogenous variables as one of searching for optimal values of the exogenous noise variables. A feasible approach is to use the Lagrangian method  to minimize the following objective loss:

\[(_{()}^{*})=_{ _{j}^{*}_{()}^{*}}|_{j}(pa _{j}^{*},_{j}^{*})-_{j}|+w_{}_{_{j}^ {*}_{()}^{*}}[(-F(_{ j}^{*}),0)+(+F(_{j}^{*})-1,0)]\] (6) \[s.t._{}^{*}=_{}^{-1}(pa _{}^{*},^{*})\]

where the optimization parameters are the counterfactual values of the noise variables corresponding to \(\)'s ancestors, \(_{()}^{*}\), and the function \(()\) returns the maximum between two values. The first term is the distance measure in the FIO problem, while the second term implements the constraint of \(\)-natural generation. The hyperparameter \(w_{}\) serves to modulate the penalty imposed on non-natural values. Notice that, in order to ensure the hard constraint \(=^{*}\), \(\)'s noise value \(_{}^{*}\) is not optimized explicitly, since the value \(_{}^{*}\) is fully determined by \(^{*}\) and other noise values. Hence, only noise values other than \(_{}^{*}\) are optimized. Further details are provided in Sec. D.

For simplicity, we have focused on the case of "full evidence" in presenting our framework and method. But it is straightforward to extend the approach to address cases of "partial evidence", i.e., \(\). In such cases, we can simply sample from \(p(|)\), treating it as full evidence, and obtain a natural counterfactual from the resulting sample.

## 6 Experiments

In this section, we evaluate the effectiveness of our method through empirical experiments on four synthetic datasets and two real-world datasets.

We propose using the deviations between generated and ground-truth outcomes as a measure of performance. We expect our natural counterfactuals to significantly reduce errors compared to non-backtracking counterfactuals. This advantage can be attributed to the effectiveness of our method in performing necessary backtracking that identifies feasible interventions, keeping counterfactual values within the data distribution, so that the learned functions are applicable. On the other hand, non-backtracking counterfactuals often produce out-of-distribution values [13; 31], posing challenges for generalization using the learned functions.

### Simulation Experiments

We start with four simulation datasets, which we use designed SCMs to generate. Please refer to the Appendix for more details about these datasets. Let's first look at _Toy 1_, which contains three variables \((n_{1},n_{2},n_{3})\). \(n_{1}\) is the confounder of \(n_{2}\) and \(n_{3}\), and \(n_{1}\) and \(n_{2}\) cause \(n_{3}\).

**Experimental Settings.** Again, we assume data and a causal graph are known, but not the ground-truth SCM. We employ normalizing flows to learn a generative model of variables \((n_{1},n_{2},n_{3})\) compatible with the causal order. Given the pretrained causal model and a data point from the test set as evidence, we set \(n_{1}\) or \(n_{2}\) as \(\) and randomly sample values from test dataset as counterfactual values of the target variable \(n_{1}\) or \(n_{2}\). In our natural counterfactuals, we use Eqn. 6 to determine LBF interventions, with \(=10^{-4}\) and \(w_{}=10^{4}\), while in non-backtracking counterfactuals, \(n_{1}\) or \(n_{2}\) is directly intervened on. We report the Mean Absolute Error (MAE) between our learned counterfactual outcomes and ground-truth outcomes on \(n_{2}\) or/and \(n_{3}\) with multiple random seeds. Notice there may be no feasible interventions for some changes, as we mentioned in Sec. 5, and thus we only report outcomes with feasible interventions, which are within the scope of our natural counterfactuals.

**Visualization of Counterfactuals on a Single Sample.** We assess the counterfactual outcomes for a sample \((n_{1},n_{2},n_{3})=(-0.59,0.71,-0.37)\), given the desired alteration \(change(n_{2}=0.19)\). For natural counterfactuals, it is necessary to backtrack to \(n_{1}\) to realize the change. This step ensures

   Dataset &  & _Toy 2_ &  &  \\  \(do\) or \(change\) & \))} & \))} & \))} & \))} & \))} & \))} & \))} & \))} & \))} \\  Outcome & \(n_{2}\) & \(n_{3}\) & \(n_{3}\) & \(n_{2}\) & \(n_{2}\) & \(n_{3}\) & \(n_{4}\) & \(n_{3}\) & \(n_{4}\) & \(n_{4}\) & \(n_{2}\) & \(n_{3}\) & \(n_{3}\) \\  Non-backtracking & 0.477 & 0.382 & 0.297 & 0.315 & 0.488 & 0.472 & 0.436 & 0.488 & 0.230 & 0.179 & 0.166 & 0.446 & 0.429 \\ Ours & 0.434 & 0.354 & 0.114 & 0.303 & 0.443 & 0.451 & 0.423 & 0.127 & 0.136 & 0.137 & 0.158 & 0.443 & 0.327 \\   

Table 1: MAE Results on _Toy 1_ to _Toy 4_ (Lower MAE is better). To save room, we also write “\(do\)” for “\(change\)” for natural counterfactuals.

Figure 1: The Visualization Results on _Toy 1_ (View the enlarged figure in Fig. 3 in the Appendix).

that the pair \((n_{1},n_{2})\) remains within a high-density area of the data distribution. In essence, our intervention targets the composite variable \(C=(n_{1},n_{2})\). On the other hand, for non-backtracking counterfactual, the intervention simply modifies \(n_{2}\) to 0.19 without adjusting \(n_{1}\), making \((n_{1},n_{2})\) out of data support. In Fig. 1 (a), we depict the original data point (yellow), the non-backtracking counterfactual (purple), and the natural counterfactual (green) for \((n_{1},n_{2})\). The ground-truth support for these variables is shown as a blue scatter plot.

_(1) Feasible Intervention VS Hard Intervention._ Non-backtracking counterfactuals apply a hard intervention on \(n_{2}\) (\(d{o}(n_{2}=0.19)\)), shifting the evidence (yellow) to the post-intervention point (purple), which lies outside the support of \((n_{1},n_{2})\). This shows that direct interventions can result in unnatural values. Conversely, our natural counterfactual (green) remains within the support of \((n_{1},n_{2})\) due to backtracking and the LBP intervention on \((n_{1},n_{2})\).

_(2) Outcome Error._ We calculate the absolute error between \(n_{3}\)'s model prediction and ground-truth value using either the green or purple point as input for the model \(p(n_{3}|n_{1},n_{2})\). The error for the green point is significantly lower at \(0.03\), compared to \(2.31\) for the purple point. This lower error with the green point is because it stays within the data distribution after an LBP intervention, allowing for better model generalization than the out-of-distribution purple point.

**Counterfactuals on Whole Test Set.** In Fig. 1 (b), we illustrate the superior performance of our counterfactual method on the test set, notably outperforming non-backtracking counterfactuals. This is evident as many outcomes from non-backtracking counterfactuals for \(n_{3}\) significantly diverge from the \(y=x\) line, showing a mismatch between predicted and ground-truth values. In contrast, our method's outcomes largely align with this line, barring a few exceptions possibly due to learned model's imperfections. This alignment is attributed to our method's consistent and feasible interventions, enhancing prediction accuracy, while non-backtracking counterfactuals often lead to infeasible results. Table 1 supports these findings, demonstrating that our approach exhibits an MAE reduction of \(61.6\%\) when applied to \(n_{2}\), compared with the non-backtracking method. _Furthermore, our method excels even when intervening in the case of \(n_{1}\), a root cause, by excluding points that do not meet the \(\)-natural generation criteria, further demonstrating its effectiveness._

**Additional Causal Graph Structures.** Our method also shows superior performance on three other simulated datasets with varied causal graph structures (_Toy 2_ to _Toy 4_), as demonstrated in Table 1.

### MorphoMNIST

As depicted in Fig. 4 (a) of Appendix, \(t\) (digit stroke thickness) causes both \(i\) (stroke intensity) and \(x\) (images), with \(i\) being the direct cause of \(x\) in MorphoMNIST. In this experiment, mirroring those in Section 6.1, we incorporate two key changes. First, we utilize two advanced deep learning models, V-SCM  and H-SCM . Although both models are referred to as "SCM," they only learn the conditional distributions of endogenous variables and, in theory, do not capture the functional relationships for out-of-distribution inputs. Second, due to the absence of ground-truth SCM for assessing outcome error, we adopt the **counterfactual effectiveness** metric from . This involves training a predictor on the dataset to estimate parent values \((,)\) from a counterfactual image \(x\) generated by model \(p(x|t,i)\) with the input \((t,i)\), and then computing the absolute error \(|t-|\) or \(|i-|\).

**Ablation Study on Naturalness Threshold \(\).** Table 2 demonstrates that our error decreases with increasing \(\), regardless of whether V-SCM or H-SCM is used. This trend suggests that a larger \(\) sets a stricter standard for naturalness in counterfactuals, enhancing the feasibility of interventions and consequently lowering prediction errors. This improvement is due to that deep-learning models are more adept at generalizing to high-frequency data .

    &  &  &  &  \\   & & & \(t\) & \(i\) & \(t\) & \(i\) \\   & - & NB & 0.336 & 4.532 & 0.283 & 6.556 \\   & \(10^{-4}\) & & 0.314 & 4.506 & 0.171 & 4.424 \\  & \(10^{-3}\) & Ours & 0.298 & 4.486 & 0.161 & 4.121 \\  & \(10^{-2}\) & & 0.139 & 4.367 & 0.145 & 3.959 \\   & - & NB & 0.280 & 2.562 & 0.202 & 3.345 \\   & \(10^{-4}\) & & 0.260 & 2.495 & 0.105 & 2.211 \\   & \(10^{-3}\) & Ours & 0.245 & 2.442 & 0.096 & 2.091 \\   & \(10^{-2}\) & & 0.093 & 2.338 & 0.083 & 2.063 \\   

Table 2: Ablation Study on \(\) (Lower MAE is better)

### 3DIdentBOX

In this study, we employ two practical public datasets from 3DIdent-BOX , namely Weak-3DIdent and Strong-3DIdent, where each image contains a teapot. Both datasets share the same causal graph, as depicted in Fig. 7 (b) of the Appendix, which includes an image variable \(x\) and its seven parent variables, with a single variable \(b\) and three pairs of variables: \((h,d)\), \((v,)\), and \((,)\), where one is the direct cause of the other in each pair. The primary distinction between Weak-3DIdent and Strong-3DIdent lies in the strength of the causal relationships between each variable pair, with Weak-3DIdent exhibiting weaker connections (Fig. 7 (c)) compared to Strong-3DIdent (Fig. 7 (d)). Our approach mirrors the MorphoMNIST experiments, using H-SCM as the pretrained causal model with \(=10^{-3}\).

**Influence of Causal Strength.** As Table 3 reveals, our method outperforms non-backtracking on both datasets, with a notably larger margin in Strong-3DIdent. This increased superiority is due to a higher incidence of infeasible hard interventions in non-backtracking counterfactuals within the Strong-3DIdent dataset.

**See Appendix for More Details.** Please refer to the Appendix for information on datasets, generated samples on MorphoMNIST and 3DIdentBox, standard deviation of results, settings of model training and FIO, differences between our natural counterfactuals and related works, and more.

## 7 Conclusion

Given a non-parametric SCM learned from data and a causal graph, non-backtracking counterfactual inference or generation may be highly unreliable because the corresponding non-backtracking intervention can result in a scenario that is far removed from the data based on which the SCM is learned. To address this issue, we have proposed a notion of natural counterfactuals, which incorporates a naturalness constraint and aims to keep the counterfactual supposition within the support of the training data distribution with minimal backtracking. We also developed a practical method for the generation or inference of natural counterfactuals, the effectiveness of which was demonstrated by empirical results.

In case it is not already transparent, we have not shown, nor did we intend to argue, that natural counterfactuals are superior to non-backtracking counterfactuals for all purposes. Non-backtracking counterfactuals, when correctly inferred, have clear advantages in revealing causal relations between events and the effects of specific actions. However, as we showed in this paper, in the context of data-driven counterfactual reasoning, inference or generation of non-backtracking counterfactuals often go astray due to the challenges of out-of-distribution generalization. Although correct information about non-backtracking counterfactuals has considerable action-guiding values, using unreliable information for that purpose is epistemologically and ethically dubious. We intend our framework of natural counterfactuals to mitigate this kind of risk, though it may appear less elegant theoretically.

Our current method is based on the assumption that the learned functions are invertible. One purpose of using the assumption is to ensure the identifiability of natural counterfactuals when one only has access to endogenous variables. If the assumption does not hold, identifiability is not guaranteed. For example, suppose \(Y=XU_{1}+U_{2}\) where \(Y\) and \(X\) are endogenous variables and \(U_{1}\) and \(U_{2}\) are exogenous noises, then the counterfactual outcome will not be identifiable. However, if we also assume a known distribution of exogenous variables, then our method can be generalized without assuming invertible functions or independent exogenous variables.

Finally, we hasten to reiterate that we do not claim that the particular distance measures and naturalness measures used in this paper are the only choices or among the best. It will be interesting to study and compare alternative implementations in future work.

   Dataset & - & \(d\) & \(h\) & \(v\) & \(\) & \(\) & \(\) & \(b\) \\   & Non & 0.025 & 0.019 & 0.035 & 0.364 & 0.27 & 0.077 & 0.0042 \\  & Ours & 0.024 & 0.018 & 0.034 & 0.349 & 0.221 & 0.036 & 0.0041 \\   & Non & 0.100 & 0.083 & 0.075 & 0.387 & 0.495 & 0.338 & 0.0048 \\  & Ours & 0.058 & 0.047 & 0.050 & 0.298 & 0.316 & 0.139 & 0.0047 \\   

Table 3: MAE Results on Weak-3DIdent and Strong-3DIdent (abbreviated as “Weak” “Strong” for simplicity). Lower MAE is better. For clarity, we use “Non” to denote the non-backtracking.