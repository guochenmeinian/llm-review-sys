# DiffPano: Scalable and Consistent Text to Panorama Generation with Spherical Epipolar-Aware Diffusion

DiffPano: Scalable and Consistent Text to Panorama Generation with Spherical Epipolar-Aware Diffusion

 Weicai Ye\({}^{1,3,}\)1 Chenhao Ji\({}^{2,}\)2 Zheng Chen\({}^{4}\) Junyao Gao\({}^{2}\) Xiaoshui Huang\({}^{3}\)

Song-Hai Zhang\({}^{4}\) Wanli Ouyang\({}^{3}\) Tong He\({}^{3,58}\) Cai rong Zhao\({}^{2,28}\) Guofeng Zhang\({}^{1,58}\)

\({}^{1}\)State Key Lab of CAD&CG, Zhejiang University \({}^{2}\)Tongji University \({}^{3}\)Shanghai AI Laboratory \({}^{4}\)Tsinghua University

maikeyeweicai@gmail.com jichenhao@tongji.edu.cn zhangguofeng@zju.edu.cn

###### Abstract

Diffusion-based methods have achieved remarkable achievements in 2D image or 3D object generation, however, the generation of 3D scenes and even \(360^{}\) images remains constrained, due to the limited number of scene datasets, the complexity of 3D scenes themselves, and the difficulty of generating consistent multi-view images. To address these issues, we first establish a large-scale panoramic video-text dataset containing millions of consecutive panoramic keyframes with corresponding panoramic depths, camera poses, and text descriptions. Then, we propose a novel text-driven panoramic generation framework, termed DiffPano, to achieve scalable, consistent, and diverse panoramic scene generation. Specifically, benefiting from the powerful generative capabilities of stable diffusion, we fine-tune a single-view text-to- panorama diffusion model with LoRA on the established panoramic video-text dataset. We further design a spherical epipolar-aware multi-view diffusion model to ensure the multi-view consistency of the generated panoramic images.

Figure 1: **DiffPano allows scalable and consistent panorama generation (i.e. room switching) with given unseen text descriptions and camera poses. Each column represents the generated multi-view panoramas, switching from one room to another.**

Extensive experiments demonstrate that DiffPano can generate scalable, consistent, and diverse panoramic images with given unseen text descriptions and camera poses. Code and dataset are available at https://zju3dv.github.io/DiffPano.

## 1 Introduction

Generating scenarios from the text descriptions that meet one's expectations is an imaginative and marvelous journey, which has many potential applications, such as VR roaming  for metaverse [67; 68; 69; 72], physical world simulation [2; 6; 14], embodied agents in scene navigation and manipulation , etc. With the advent of the AIGC era, several works [8; 14; 20; 29; 42; 43; 48; 51; 52] on object generation even scene generation has emerged. However, they generally only generate a series of perspective images, making it impossible to comprehensively simulate the entire environment for scene understanding [27; 19; 66] and reconstruction [3; 5; 65; 64; 30; 35; 34; 26; 21; 7; 12; 47; 49; 58]. Given this, some methods [70; 9] try to generate panoramas to solve these problems by taking advantage of the inherent characteristics of panoramic images, which can capture the surrounding environment with a single shot.

These methods can be roughly divided into four categories: 1) Directly single-view equirectangular projection (ERP) panorama generation . However, its camera is immovable, making it incapable of scene exploration; 2) Multiple perspective views generation methods [20; 43; 48; 40] without considering multi-view panorama generation. 3) The inpainting solutions are based on the infinite expansion of a single perspective view, which lacks 3D awareness. Single scene optimization methods  with inpainting have no generalization ability and cost too much time for optimization. 4) Directly extending the multiple perspective views generation methods  to the ERP panorama, which is difficult to converge and results in poor multi-view consistency (see Fig. 5).

This paper aims to generate scalable and multi-view consistent panoramic images from text descriptions and camera poses (see Fig. 1) with many potential applications such as immersive VR roaming with unlimited scaps and preview for interior home design. However, achieving this goal is not trivial. To the best of our knowledge, there is currently a lack of rich and diverse panoramic datasets to meet the task of text-to-multi-view ERP panorama generation. To this end, we propose a novel panoramic video-text dataset and a generation framework suitable for the text-to-multi-view panorama generation task, advancing the development of this field. Specifically, we first establish a large-scale panoramic video-text dataset using Habitat Simulator  (see Sec. 3), which contains millions of panoramic keyframes and corresponding panoramic depths, camera poses, and text descriptions. Next, built upon the proposed dataset, we propose a generation framework for consistent multi-view ERP panorama generation (see Sec. 4), termed DiffPano. The DiffPano framework consists of a single-view text-to-panorama diffusion model (see Sec. 4.1) and a spherical epipolar-aware multi-view diffusion model (see Sec. 4.2). The single-view text-to-panorama diffusion model is obtained by fine-tuning the stable diffusion model  of perspective images using LoRA . Considering that the single-view pano-based diffusion model cannot guarantee the consistency of generated multi-view panoramas with different camera poses, we derive a spherical epipolar constraint applicable to panoramic images, inspired by the perspective epipolar constraint. We then incorporated it as a spherical epipolar-aware attention module (see Sec. 4.2) into the multi-view panoramic diffusion model to ensure the multi-view consistency of the generated ERP panoramic images.

Since there are no related methods for comparison, we try to extend the MVDream method  to generate multi-view ERP panoramas. Trained and tested on the proposed panoramic video-text dataset, extensive experiments demonstrated that compared to the modified MVDream, our proposed multi-view panorama generation based on spherical epipolar-aware attention can generate more scalable and consistent panoramic images. Our method also demonstrates the generalization ability of the original diffusion model to generate satisfactory multi-view ERP panoramas with given unseen text descriptions and camera poses.

Our contributions can be summarized as follows: 1) To the best of our knowledge, we are the first to propose a scalable and consistent multi-view panorama generation task from text descriptions and camera poses. 2) We established a large-scale diverse and rich panoramic video-text dataset, which fosters the research of text-to-panoramic video generation. 3) We propose a novel text-driven panoramic generation framework with a spherical epipolar attention module, allowing scalable and consistent panorama generation with unseen text descriptions and camera poses.

Related work

### Single-View Panorama Generation

Recently, latent diffusion model (LDM) methods have attracted widespread attention, and many single-view panorama generation works [70; 48; 74; 24; 60; 56; 33; 54; 62] have emerged, achieving remarkably impressive results. Among them, MVDiffusion  simultaneously generates eight fixed-viewpoint perspective images through a multi-view Correspondence-Aware diffusion model and stitches them together to produce a panorama. However, it cannot support the generation of top and bottom views, and the generated panorama resembles wide-angle images with an extensive field of view rather than true \(360^{}\) images. Some methods [55; 13] solve this problem using equirectangular projection (ERP) and try to facilitate the interaction between the left and right sides during the panorama generation process to enhance the left-right continuity property inherent in ERP images. To address the domain gap between panorama and perspective images, PanFusion  proposed a novel dual-branch diffusion model that mitigates the distortion of perspective images projected on panoramas while providing global layout guidance. However, its more complex model architecture incurs longer inference times for panorama generation. In addition, PanFusion cannot be expanded as an effective pre-trained model to the multi-view panorama generation task due to its excessive network parameters. To strike a balance between computational complexity and ensuring left-right continuity of panoramas, our proposed single-view panorama-based stable diffusion model only requires fine-tuning with LoRA  to learn panoramic styles and achieve good edge continuity while maintaining higher generation speed and simpler architecture.

The existing single-view panorama generation methods cannot achieve scalable panorama generation. The core of our paper lies in the generation of multi-view consistent panoramic images, which we will introduce in Section 2.2. More importantly, the single-view panoramic image generated by previous methods mainly supports 3DoF roaming, while our method can generate multi-view panoramic images for 6DoF roaming, which can serve as the inputs for \(360^{}\) Gaussian Splatting  or \(360^{}\) NeRF [10; 11]. Our method also has a great potential value in \(360^{}\) relightable novel view synthesis with the combination of \(360^{}\) multi-view inverse rendering method .

### Multi-View Image Generation

To the best of our knowledge, there is no work focusing on multi-view panorama generation. We review the existing works about multi-view generation for perspective images in this part.

Zero123  laid the foundation for 3D object generation based on multi-view generation, while the pose-guided diffusion model [51; 40] explored consistent view synthesis of scenes. However, iteratively applying the diffusion model to generate individual views in the multi-view generation task may lead to poor multi-view consistency of generated images due to accumulated errors. To generate high-quality multi-view images simultaneously, some methods [31; 43; 57; 32] modify the UNets in the diffusion model into a multi-branch form and achieve the effect of generating consistent multi-view images through the interaction between different branch.

Currently, most multi-view generation tasks focus on generating multi-view perspective images of single objects [43; 61; 63; 28; 46; 45] or scenes [51; 15], while minimal research has been conducted on multi-view panorama generation. Narrow FoV (field-of-view) drawbacks of perspective images lead to the fact that the existing generation methods can only generate a very local region of the scene at a time. Our work focuses on the task of exploring the generation of \(360^{}\) images from multiple different viewpoints. Due to the camera projection difference between panoramic and perspective images, achieving consistency in multi-view panoramas is challenging. It is impossible to directly apply the existing epipolar attention module [51; 20] to multi-view panoramas. We strive to derive the spherical epipolar line formula for panoramic images and propose a spherical epipolar attention module to ensure the multi-view consistency of the generated panoramas.

### Panoramic Dataset

Great progress in text to single-view panorama generation has been witnessed. However, text-to-multi-view panorama generation is still a blank slate. One of the main limitations of this task is the lack of suitable datasets. The common panoramic datasets used in single-view panorama generation consist of indoor HDR dataset , outdoor HDR dataset , HDR360-UHD dataset , Structured3D ,Standford 2D-3D-S , and Matterport3D dataset , etc. Most of these datasets are relatively small in scale and only have single-view panoramas, which cannot support multi-view panorama generation, except Matterport3D . In addition, the sky box images in Matterport3D  contain only sparse views. Although HM3D  provides the textured mesh of 1000 scenes, it lacks the corresponding text description for each view. To generate multi-view panoramas, we render cube maps at each viewpoint in the 3D meshes of HM3D, using the Habitat Simulator , and stitch them into panoramas. We generate complete text descriptions corresponding to the panoramas by using Blip2  to create text descriptions for each face of the cube map separately, and then summarizing them using Llama2 . In this way, we obtain a panoramic video-text dataset that includes camera poses, corresponding panoramas, and text descriptions of the panoramas, which facilitates subsequent multi-view panorama generation tasks.

## 3 Panoramic Video-Text Dataset

Due to the lack of high-quality panorama-text datasets, most text-to-panorama generation tasks require researchers to construct their own datasets. The dataset constructed in PanFusion  suffers from blurrness at the top and bottom of the panoramic images, and the corresponding text descriptions are not precise enough. To address these issues, we utilize the Habitat Simulator  to randomly select positions within the scenes of the Habitat Matterport 3D (HM3D)  dataset and render the six-face cube maps. These cube maps are then interpolated and stitched together to form panoramas so we can obtain panoramas with clear tops and bottoms. To generate more precise text descriptions for the panoramas, we first use BLIP2  to generate corresponding text descriptions for each obtained cube map, and then employ Llama2  to summarize and receive accurate and complete text descriptions. Furthermore, the Habitat Simulator allows us to render images based on camera trajectories within the HM3D scenes, enabling the creation of a dataset that simultaneously includes camera poses, panoramas, and corresponding text descriptions. This dataset will be utilized in the multi-view panorama generation (see Sec. 4.2).

## 4 Proposed Method: DiffPano

DiffPano is capable of generating multi-view consistent panoramas conditioned on camera viewpoints and textual descriptions, as illustrated in Fig. 1. In this section, we first introduce our single-view panorama stable diffusion in Sec. 4.1. We then elaborate on how to extend single-view panorama generation to multi-view consistent panorama generation by leveraging the spherical epipolar attention module in Sec. 4.2.

Figure 2: **Panoramic Video Construction and Caption Pipeline.**

### Single-View Panorama-Based Stable Diffusion

A straightforward way to generate a single-view panorama from text is to train a text-to-panorama diffusion model from scratch with a large number of text-panorama pairs, which is both time-consuming and computationally expensive. However, stable diffusion  leverages a vast amount of perspective images and their corresponding textual descriptions as training data, endowing it with excellent prior knowledge of perspective images and strong text understanding capabilities. An economical and effective way for panorama generation is to fine-tune the trained perspective diffusion model with a few text-panorama pairs. To this end, panorama generation from text can be regarded as a style transfer of images generated by stable diffusion, converting them from perspective style to panoramic style, and requiring them to satisfy the left-right continuity property of panoramas.

LoRA-based fine-tuningDiffusion models for text-to-image generations possess excellent prior knowledge of 2D images and strong text comprehension capabilities. We aim to preserve these abilities of the model while fine-tuning it to generate images in the style of panoramas. We employ the Low-Rank Adaptation (LoRA)  fine-tuning method, which has been previously used in large language models. Compared to full fine-tuning, LoRA is faster and requires fewer computational resources. In our approach, we freeze all the parameters of the original Stable Diffusion model and add trainable layers to the UNet component using the LoRA fine-tuning method. We then train the model using our custom-created panorama-text dataset. To improve the left-right continuity of the generated images, we perform data augmentation on the panorama training dataset by randomly concatenating a portion of the right side of the panorama to the left side. Experiments demonstrate that the panoramas generated using this method exhibit satisfactory left-right continuity.

### Spherical Epipolar-Aware Multi-View Diffusion

Built upon our proposed single-view panorama stable diffusion in Sec. 4.1, we extend the single-view diffusion model to a multi-view diffusion model with a spherical epipolar-aware attention module to generate multi-view scalable and consistent panoramas.

Spherical Epipolar-Aware Attention ModuleEpipolar attention was proposed in [20; 51] to ensure consistency between generated multi-view perspective images. However, due to the differences in imaging methods between perspective and panoramic views, existing epipolar attention cannot be directly used for panoramic views. To overcome this challenge, we derived the epipolar line for panoramic images in the equirectangular projection (ERP), and the specific proof process is provided in Appendix A. Equation 14 shows the mathematical form of the spherical epipolar line in ERP images. The spherical epipolar line is visualized in the spherical epipolar-aware attention module of Fig. 3. We extend the principle of epipolar attention to panoramic images to implement the spherical epipolar-aware attention module. Given a pixel \(\) in the target view, we calculate its corresponding spherical coordinates \(_{sphere}\) based on the spherical projection process:

Figure 3: **DiffPano Framework. The DiffPano framework consists of a single-view text-to-panorama diffusion model and a spherical epipolar-aware multi-view diffusion model. It can support text to single-view panorama or multi-view panorama generation.**\[ =(0.5-}{W}) 2\] (1) \[ =(0.5-}{H}),\]

where \(x_{pix}\) and \(y_{pix}\) are the pixel coordinates of \(\), \(\) and \(\) are its corresponding spherical coordinates and \(W\) and \(H\) are the resolutions of panorama. We then transform the spherical coordinate system to the Cartesian coordinate system to obtain the camera coordinates \(p_{camera}\) corresponding to \(\) :

\[x_{cam} =()()\] (2) \[y_{cam} =()\] \[z_{cam} =()().\]

The camera coordinates of \(_{cam}\) are converted to world coordinates \(_{world}\) through the camera's pose matrix. This allows us to compute the ray from the camera center to \(_{world}\) in the world coordinate system and construct the spherical epipolar attention module.

Given \(N\) feature maps \(F=F_{i}|1 i N\) corresponding to \(N\) panoramic images and their respective camera pose matrices, we implement cross-attention between different views through the spherical epipolar-aware module. During the generation process, each feature map in \(F\) can be considered as the target view, and the \(K\) nearest views are selected from the remaining features as reference views.

For each feature point in the target view feature map, we uniformly sample \(S\) points on the ray between the feature point and the camera. All sampled points are reprojected onto the feature maps of the \(K\) reference views, and the corresponding feature values are obtained through interpolation. We denote the features in the target view as \(q\), and the features of all sampled points in the reference views as \(k\) and \(v\). The cross-attention is then constructed using these features.

Let \(p_{i}\) be a feature point in the target view feature map \(F_{t}\), and \(p_{i,j}|1 j S\) be the \(S\) uniformly sampled points on the ray between \(p_{i}\) and the camera center. We reproject these points onto the \(K\) reference view feature maps \(F_{r_{k}}|1 k K\) to obtain the corresponding feature values \(f_{i,j,k}|1 j S,1 k K\). The query \(q_{i}\), key \(k_{i}\), and value \(v_{i}\) for the cross-attention mechanism are defined as follows:

\[q_{i}=F_{t}(p_{i}), k_{i}=f_{i,j,k}|1 j S,1 k K, v_ {i}=f_{i,j,k}|1 j S,1 k K.\] (3)

The cross-attention output \(o_{i}\) for the feature point \(p_{i}\) is computed as:

\[o_{i}=Attention(q_{i},k_{i},v_{i})=softmax(k_{i}^{T}}{})v_{ i},\] (4)

where \(d\) is the dimension of the query and key vectors.

Positional EncodingTo enhance the model's understanding of 3D spatial relationships between different views, we follow EpiDiff  to employ the positional encoding method from Light Field Networks (LFN) . In the world coordinate system, let \(p_{i}\) be a pixel in the target view, and \(o_{i}\) and \(d_{i}\) be the origin and direction of the ray between \(p_{i}\) and the camera center, respectively. The Plucker coordinates \(r_{i}\) of the ray are computed as:

\[r_{i}=(o_{i} d_{i},d_{i}).\] (5)

For each sampled point \(p_{i,j}\) on the ray, its corresponding spherical depth \(z_{i,j}\) is transformed using a harmonic transformation to get \(_{z}(z_{i,j})\). Similarly, the Plucker coordinates \(r_{i}\) are transformed as \(_{r}(r_{i})\).The positionally encoded features \(_{r}(r_{i})\) and \(_{z}(z_{i,j})\) are then concatenated to obtain the combined positional encoding \((r_{i},z_{i,j})\):

\[(r_{i},z_{i,j})=[_{r}(r_{i}),_{z}(z_{i,j})].\] (6)

The combined positional encoding \((r_{i},z_{i,j})\) is then concatenated with the feature maps \(F_{t}\) and \(F_{r_{k}}|1 k K\) to obtain the enhanced feature representations \(t\) and \(r_{k}|1 k K\):

\[t(p_{i})=[F_{t}(p_{i}),(r_{i},z_{i,j})],r_{k}(p_{i,j })=[F_{r_{k}}(p_{i,j}),(r_{i},z_{i,j})],\] (7)where \([,]\) denotes concatenation. These enhanced feature representations are then used in the cross-attention mechanism to improve the model's understanding of 3D spatial relationships between different views.

Two-Stage TrainingThe main difference between panoramic images and perspective images is that panoramic images contain \(360^{}\) content of the surroundings, while perspective images only contain content from a given viewpoint. Therefore, when the camera only rotates or translates by a small amount, the corresponding content in the panoramic image hardly changes. To make the generated multi-view panoramic images better match each corresponding text, we divide the training into two stages. In the first stage, we use the selected dataset with almost no change in image content (small camera movement) for training, which enhances the effect of the spherical epipolar-aware attention module. In the second stage, we increase the camera movement distance between each viewpoint and train with images that generate new content, improving the model's ability to understand text based on changes in perceived spatial location while ensuring multi-view consistency and enhancing scalability.

## 5 Experiment

DatasetWe leverage the Habitat Simulator  to render a panoramic video dataset based on the Habitat Matterport 3D (HM3D) dataset . The pipeline of dataset rendering and captioning is shown in Sec. 3. After post-processing such as dataset filtering, we constructed 8,508 panorama-text pairs as training sets for single-view panorama generation. For multi-view panorama generation, we constructed 19,739 multi-view panorama-text pairs with nearly identical image content and 18,704 multi-view panorama-text pairs with different image contents as training sets. For specific details regarding the dataset, please refer to Appendix B.

Implementation DetailsIn the multi-view panorama generation, we simultaneously generate \(N=4\) panoramas from different viewpoints. Within the spherical epipolar-aware attention module, we consider the two nearest views to the target view as reference views, i.e., \(K=3\), and sample \(S=10\) points along each ray. We conducted separate training for 100 epochs on datasets with almost identical image content and datasets with varying image content. Please refer to Appendix C for further implementation details.

Evaluation MetricsTo evaluate the performance of our proposed single-view panorama-based stable diffusion model, we employ three commonly used metrics: Frechet Inception Distance (FID) , Inception Score (IS) , and CLIP Score (CS) . FID measures the similarity between the distribution of generated panoramas and the distribution of real images. IS assesses the quality and diversity of the generated panoramas. CS is utilized to evaluate the consistency between the input text and the generated panoramas. To further evaluate the consistency of multi-view panorama generation, we leverage the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM)  metrics. PSNR and SSIM quantify the pixel-level differences and structural similarity between the generated views respectively.

### Single-View Panorama Generation

BaselinesWe evaluate the performance of our proposed method by comparing it with the following baseline approaches for text-to-panorama generation:

* _Text\(2Light\)_is a two-stage approach that first generates a low-resolution panorama based on the input text, and then expands it to ultra-high resolution.
* _PanFusion_is a dual-branch text-to-panorama model that aims to mitigate the distortion caused by projecting perspective images onto a panoramic canvas while providing global layout guidance.

Since the panoramic images generated by MVDiffusion  do not include top and bottom viewpoints, they are not complete panoramas. Therefore, we do not compare our method with MVDiffusion.

Quantitative ResultsTable 1 presents the quantitative results. The FID, IS, and CS values of Text2Light are from the PanFusion . We trained our proposed Pano-SD using the same dataset as PanFusion  and re-evaluated the performance of PanFusion. From the results, it can be observed that our method slightly outperforms others in terms of CS value and achieves a significantly lower FID value compared to Text2Light, which is close to PanFusion . Moreover, due to the simplicity and efficiency of our model architecture, our method has a substantial advantage in inference time, which can significantly improve the efficiency of subsequent multi-view generation tasks.

Qualitative ResultsThe qualitative comparison results are shown in Figure 4. We compare our method with the two models trained on their respective datasets. The panoramas generated by Text2Light exhibit poor left-right consistency. On the other hand, the panoramas generated by PanFusion suffer from blurriness at the bottom and top regions, which affects the overall integrity of the panoramas. In contrast, our model is capable of generating panoramas with clear bottom and top regions and better left-right continuity. However, due to the quality of the dataset, the image quality may be slightly inferior.

### Multi-View Panorama Generation

To the best of our knowledge, there is no method for multi-view panorama generation, the existing SOTA method MVDream for perspective images cannot be directly applied to multi-view panorama generation. To verify the validity of our proposed spherical epipolar-aware attention module, we adapted MVDream to the panorama generation task as a comparative baseline.

Specifically, we remove the spherical epipolar-aware attention module from our method and load the pre-trained LoRA layers from Pano-SD. We then convert the 3D self-attention in the UNet to the form used in MVDream . Additionally, we transform the camera pose matrix into camera

   Method & FID\(\) & IS\(\) & CS\(\) & Inference time(s)\(\) \\  Text2Light & 76.50 & 3.60 & 27.48 & 50.4 \\ PanFusion & **47.62** & **4.49** & 28.73 & 27.6 \\ Pano-SD(Ours) & 48.52 & 3.30 & **28.98** & **5.1** \\   

Table 1: Quantitative Panorama Comparisons with Baseline Methods

Figure 4: **Text to Panorama Comparison between TextLight, PanFusion, and Ours.**

[MISSING_PAGE_FAIL:9]

Number of Reference Views and Sample PointsTo explore the influence of varying numbers of reference frames and sampling points on model performance, we generate 5-frame multi-view panoramas simultaneously, and compute the FID, IS, and CS metrics for the first frame of each group to assess the quality of the generated panorama under different quantities of reference frames and sampling points. Furthermore, we set the same camera pose for the first and last frames of each generated panorama group, and calculate the PSNR and SSIM values between these two frames to evaluate the model's multi-view consistency. As shown in Table. 3, increasing the number of reference frames and sampling points can improve the quality of generated panoramas to a certain extent, but the changes in image diversity and consistency with text remain marginal. With an increasing number of reference frames and sampling points, the model's consistency exhibits improvement, however, when the number of sampling points becomes excessive (\(S\)=12), the multi-view consistency of the model diminishes.

One-Stage vs Two-StageWe conduct ablation experiments on one-stage and two-stage training. Table. 4 shows that the two-stage method will obtain the higher FID values. The IS of the two-stage training method is lower, and the diversity is reduced to a certain extent, which is slightly worse than the one-stage training. However, it should be noted that the images after one-stage training will have ghosting, but the two-stage will not.

## 6 Conclusion

We have proposed the panoramic video-text dataset and panorama generation framework with spherical epipolar-aware attention for text-to-single-view or multi-view panorama generation. Extensive experiments demonstrate that our method can achieve scalable, consistent, and diverse multi-view panoramas.

Limitation and Future WorkAlthough our method demonstrates the ability to generate consistent multi-view panoramas under the same setting as the training phase, it is important to note that as the number of frames increases during inference, the model tends to hallucinate content.

Exploring the use of video diffusion models to improve the consistency of generated multi-view panoramas is a promising direction. Longer panoramic videos are expected to be realized based on the generated panoramas as conditions.

## 7 Acknowledgements

This work was partially supported by the National Key Research and Development Program of China (No. 2023YFF0905104) and National Natural Science Foundation of China (Nos. 61932003, 62076184 and 62473286).