# Marrying Causal Representation Learning with Dynamical Systems for Science

Dingling Yao, Caroline Muller, and Francesco Locatello

Institute of Science and Technology Austria

###### Abstract

Causal representation learning promises to extend causal models to hidden causal variables from raw entangled measurements. However, most progress has focused on proving identifiability results in different settings, and we are not aware of any successful real-world application. At the same time, the field of dynamical systems benefited from deep learning and scaled to countless applications but does not allow parameter identification. In this paper, we draw a clear connection between the two and their key assumptions, allowing us to apply identifiable methods developed in causal representation learning to dynamical systems. At the same time, we can leverage scalable differentiable solvers developed for differential equations to build models that are both identifiable and practical. Overall, we learn explicitly controllable models that isolate the trajectory-specific parameters for further downstream tasks such as out-of-distribution classification or treatment effect estimation. We experiment with a wind simulator with partially known factors of variation. We also apply the resulting model to real-world climate data and successfully answer downstream causal questions in line with existing literature on climate change. Code is available at https://github.com/CausalLearningAI/crl-dynamical-systems.

## 1 Introduction

Causal representation learning (CRL)  focuses on _provably_ retrieving high-level latent variables from low-level data. Recently, there have been many casual representation learning works compiling, in various settings, different theoretical identifiability results for these latent variables . The main open challenge that remains for this line of work is the broad applicability to real-world data. Following earlier works in disentangled representations (see  for a summary of data sets), existing approaches have largely focused on visual data. This is challenging for various reasons. Most notably, it is unclear what the causal variables should be in computer vision problems and what would be interesting or relevant causal questions. The current standard is to test algorithms on synthetic data sets with "made-up" latent causal graphs, e.g., with the object class of a rendered 3d shape causing its position, hue, and rotation .

In parallel, the field of machine learning for science  shows promising results on various real-world time series data collected from some underlying dynamical systems. Some of these works primarily focus on time-series forecasting, i.e., building a neural emulator that mimics the behavior of the given times series data ; while others try to additionally learn an explicit ordinary differential equation simultaneously . However, to the best of our knowledge, none of these methods provide explicit identifiability analysis indicating whether the discovered equation recovers the ground truth underlying governing process given time series observations; or even whether the learned representation relates to the underlying steering parameters. At the same time, many scientific questions are inherently causal, in the sense that physical laws govern the measurements of all the natural data we can record, e.g., across different environments and experimental settings. Identifying such an underlying physical process can boost scientific understanding and reasoning in numerous fields; for example, in climate science, one could conductsensitivity analysis of _layer thickness_ parameter on atmosphere motion more efficiently, given a neural emulator that identifies the _layer thickness_ in its latent space. However, whether mechanistic models can be practically identified from data is so far unclear (Sutton et al., 2016, Table 1).

This paper aims to identify the underlying _time-invariant_ physical parameters from real-world time series, such as the previously mentioned _layer thickness_ parameter, while still preserving the ability to forecast efficiently. Thus, we connect the two seemingly faraway communities, causal representation learning and machine learning for dynamical systems, by phrasing parameter estimation problems in dynamical systems as a latent variable identification problem in CRL. The benefits are two folds: (1) we can import all identifiability theories for free from causal representation learning works, extending discovery methods with additional identifiability analysis and, e.g., multiview training constructs; (2) we showcase that the scalable mechanistic neural networks (Sutton et al., 2016) recently developed for dynamical systems can be directly employed with causal representation learning, thus providing a scalable implementation for both identifying and forecasting real-world dynamical systems.

Starting by comparing the common assumptions in the field of parameter estimation in dynamical systems and causal representation learning, we carefully justify our proposal to translate any parameter estimation problem into a latent variable identification problem; we differentiate three types of identifiability: _full identifiability_, _partial identifiability_ and _non-identifiability_. We describe concrete scenarios in dynamical systems where each kind of identifiability can be theoretically guaranteed and restate _exemplary_ identifiability theorems from the causal representation learning literature with slight adaptation towards the dynamical system setup. We provide a step-by-step recipe for reformulating a parameter estimation problem into a causal representation learning problem and discuss the challenges and pitfalls in practice. Lastly, we successfully evaluate our parameter identification framework on various _simulated_ and _real-world_ climate data. We highlight the following contributions:

* We establish the connection between causal representation learning and parameter estimation for differential equations by pinpointing the alignment of common assumptions between two communities and providing hands-on guidance on how to rephrase the parameter estimation problem as a latent variable identification problem in causal representation learning.
* We equip discovery methods with provably identifiable parameter estimation approaches from the causal representation learning literature and their specific training constructs. This enables us to maintain both the theoretical results from the latter and the scalability of the former.
* We successfully apply causal representation learning approaches to simulated and real-world climate data, demonstrating identifiability via domain-specific downstream causal tasks (OOD classification and treatment-effect estimation), pushing one step further on the applicability of causal representation for real-world problems.

**Remark on the novelty of the paper:** Our main contribution is establishing a connection between the dynamical systems and causal representation learning fields. As such, we do not introduce a new method per se. Meanwhile, this connection allows us to introduce CRL training constructs in methods that otherwise would not have any identification guarantees. Further, it provides the first avenue for causal representation learning applications on real-world data. These are both major challenges in the respective communities, and we hope this paper will serve as a building block for cross-pollination.

## 2 Parameter Estimation in Dynamical Systems

We consider dynamical systems in the form of

\[}(t)=_{}((t)) (0)=_{0},\  p_{},\ t[0,t_{}]\] (1)

where \((t)^{d}\) denotes the state of a system at time \(t\), \(f_{}^{1}(,)\) is some smooth differentiable vector field representing the constraints that define the system's evolution, characterized by a set of physical parameters \(=_{1} _{N}\), where \(^{N}\) is an open, simply connected real space associated with the probability density \(p_{}\). Formally, \(f_{}\) can be considered as a functional mapped from \(\) through \(M:^{1}(,)\). In our setup, we consider _time-invariant_, _trajectory-specific_ parameters \(\) that remain constant for the whole time span \([0,t_{}]\), but variable for different trajectories. For instance, consider a robot arm interacting with multiple objects of different mass; a parameter \(\) could be the object's masses \(m_{+}\) in Newton's second law \((t)=(t)}}{{m}}\), with \((t)\) denote the force applied at time \(t\). Depending on the object the robot arm interacts with, \(m\) can take different values, following the prior distribution \(p_{}\). \((0)=_{0}\)denotes the initial value of the system. Note that higher-order ordinary differential equations can always be rephrased as a first-order ODE. For example, a \(\)-th order ODE in the following form:

\[x^{()}(t)=f=(x(t),x^{(1)}(t),,x^{(-1)}(t),),\]

can be written as \(}(t)=f_{}((t))\), where \((t)=(x(t),x^{(1)}(t),,x^{(-1)}(t))^{ d}\) denotes state vector constructed by concatenating the derivatives. Formally, the solution of such a dynamical system can be obtained by integrating the vector field over time: \((t)=_{0}^{t}f((),)d\).

**What do we mean by "parameters"?** The parameters \(\) that we consider can be both explicit and implicit. When the functional form of the ODE is given, like Newton's second law, the set of parameters is defined explicitly and uniquely. For real-world physical processes where the functional form of the state evolution is unknown, such as the sea-surface temperature change, we can consider _latitude-related_ features as parameters. Overall, we use _parameters_ to generally refer to any _time-invariant_, _trajectory-specific_ components of the underlying dynamical system.

**Assumption 2.1** (Existence and uniqueness).: For every \(_{0}\), \(\), there exists a unique continuous solution \(_{}:[0,t_{}]\) satisfying the ODE (eq. (1)) for all \(t[0,t_{}]\).

**Assumption 2.2** (Structural identifiability).: An ODE (eq. (1)) is _structurally_ identifiable in the sense that for any \(_{1},_{2}\), \(_{_{1}}(t)=_{_{2}}(t)\, t [0,t_{}]\) holds if and only if \(_{1}=_{2}\).

_Remark 2.1_.: Asm. 2.2 implies that it is _in principle_ possible to identify the parameter \(\) from a trajectory \(_{}\). Since this work focuses on providing concrete algorithms that guarantee parameter identifiability _given infinite number of samples_, the structural identifiability assumption is essential as a theoretical ground for further algorithmic analysis. It is noteworthy that a non-structurally identifiable system can become identifiable by reparamatization. For example, linear ODE \(}(t)=ab(t)\) with parameters \(a,b^{2}\) is structurally non-identifiable as \(a,b\) are commutative. But if we define \(c:=ab\) as the overall growth rate of the linear system, then \(c\) is structurally identifiable.

**Problem setting.** Given an observed trajectory \(:=(_{}(t_{0}),,_{}(t_ {T}))^{T}\) over the discretized time grid \(:=(t_{0},,t_{T})\), our goal is to investigate the identifiability of structurally identifiable parameters by formulating concrete conditions under which the parameter \(\) is (i) fully identifiable, (ii) partially identifiable, or (iii) non-identifiable _from the observational data_. We establish the identifiability theory for dynamical systems by converting classical parameter estimation problems  into a latent variable identification problem in causal representation learning . For both (i) and (ii), we empirically showcase that existing CRL algorithms with slight adaptation can successfully (_partially_) identify the underlying physical parameters.

## 3 Identifiability of Dynamical Systems

This section provides different types of theoretical statements on the identifiability of the underlying _time-invariant_, _trajectory-specific_ physical parameters \(\), depending on whether the functional form of \(f_{}\) is known or not. We show that the parameters from an ODE with a known functional form can be _fully identified_ while parameters from unknown ODEs are in general _non-identifiable_. However, by incorporating some weak form of supervision, such as multiple similar trajectories generated from certain overlapping parameters , parameters from an unknown ODE can also be _partially identified_. Detailed proofs of the theoretical statements are provided in App. B.

### Identifiability of Dynamical Systems with Known Functional Form

We begin with the identifiability analysis of the physical parameters of an ODE with **known** functional form. Many real-world data we record are governed by known physical laws. For example, the bacteria growth in microbiology could be modeled with a simple logistic equation under certain conditions, where the parameter of interest in this case would be the _growth rate_\(r_{+}\) and _maximum capacity_\(K_{+}\). Identifying such parameters would be helpful for downstream analysis. To this end, we introduce the definition of _full identifiability_ of a physical parameter vector \(\).

**Definition 3.1** (Full identifiability).: A parameter vector \(\) is fully identified if the estimator \(}\) converges to the ground truth parameter \(\) almost surely.

**Definition 3.2** (ODE solver).: An ODE solver \(F:^{T}\) computes the solution \(\) of the ODE \(f_{}=M()\) (eq. (1)) over a discrete time grid \(T=(t_{1},,t_{T})\).

**Corollary 3.1** (Full identifiability with known functional form).: _Consider a trajectory \(^{T}\) generated from a ODE \(f_{}((t))\) satisfying Asms. 2.1 and 2.2, let \(}\) be an estimator minimizing the following objective:_

\[(})=\|F(})-\| _{2}^{2}\] (2)

_then the parameter \(\) is **fully-identified** (Defn. 3.1) by the estimator \(}\)._

_Remark 3.1_.: The estimator \(\) of eq.2 is considered as some learnable parameters that can be directly optimized. If we have multiple trajectories \(\) generated from different realizations of \( p_{}\), we can also amortize the prediction \(}\) using a smooth encoder \(g:^{T}\). In this case, the loss above can be rewritten as: \((g)=_{,t}[\|F(g())-(t)\|_ {2}^{2}]\), then the optimal encoder \(g^{*}*{argmin}(g)\) can generalize to unseen trajectories \(\) that follow the same class of physical law \(f\) and fully identify their trajectory-specific parameters \(\).

**Discussion.** Many works on machine learning for dynamical system identification follow the principle presented in Cor. 3.1, and most of them solely differ concerning the architecture they choose for the ODE solver. For example, SINDy-like ODE discovery methods [9; 10; 23; 24; 47] approximate the ground truth vector field \(f\) using a linear weighted sum over a set of library functions and learn the linear coefficients by sparse regression. For any ODE \(f\) that is linear in \(\), i.e., the ground truth vector field is in the form of \(f_{}(,t)=_{i=1}^{m}_{i}_{i}()\) for a set of known base functions \(\{_{i}\}_{i[m]}\), SINDy-like approaches can fully identify the parameters by imposing some sparsity constraint. Another line of work, gradient matching , estimates the parameters probabilistically by modeling the vector field \(f_{}\) using a Gaussian Process (GP). The modeled solution \((t)\) is thus also a GP since GP is closed under integrals (a linear operator). Given the functional form of \(f_{}\), the model aims to match the estimated gradient \(}\) and the evaluated vector field \(f_{}((t))\) by maximizing the likelihood, which is equivalent to minimizing the least-squares loss (eq.2) under Gaussianity assumptions. Hence, the gradient matching approaches can _theoretically_ identify the underlying parameters under Cor. 3.1. Formal statements and proofs for both SINDy-like and gradient matching approaches are provided in App. B. _Note that most ODE discovery approaches [9; 10; 23; 24; 47; 68] refrain from making identifiability statements and explicitly states it is unknown which settings yield identifiability._

### Identifiability of Dynamical Systems without Known Functional Form

In traditional dynamical systems, identifiability analysis usually assumes the functional form of the ODE is known ; however, for most real-world time series data, the functional form of underlying physical laws remains uncovered. Machine learning-based approaches for dynamical systems work in a black-box manner and can clone the behavior of an unknown system [12; 13; 46], but understanding and identifiability guarantees of the learned parameters are so far missing. Since most of the physical processes are inherently steered by a few underlying _time-invariant_ parameters, identifying these parameters can be helpful in answering downstream scientific questions. For example, identifying climate zone-related parameters from sea surface temperature data could improve understanding of climate change because the impact of climate change significantly differs in polar and tropical regions. Hence, we aim to provide identifiability analysis for the underlying parameters of an unknown dynamical system by converting the classical parameter estimation problem of dynamical systems into a latent variable identification problem in causal representation learning. We start by listing the common assumptions in CRL and comparing the ground assumptions between these two fields.

**Assumption 3.1** (Determinism).: The data generation process is deterministic in the sense that observation \(\) is generated from some latent vector \(\) using a deterministic solver \(F\) (Defn. 3.2).

   &  &  \\ _ref_ & _assumption_ & _assumption_ & _ref_ & \\ 
2.1 & _existence \& uniqueness_ & _determ. gen._ & 3.1 & Both 2.1 and 3.1 implies deterministic generative process. \\  & & & & & \\  & & \(supp()=\) & 3.3 & 2.1 implies 3.3 as \(_{}\) uniquely exists for all \(\). \\
2.2 & _structural identifiability_ & _injectivity_ & 3.2 & 2.2 implies 3.2 of the solution \(_{}\). \\  

Table 1: **Comparing typical assumptions** of parameter estimation for dynamical systems and latent variable identification in causal representation learning. We justify that the common assumptions in both fields are aligned, providing theoretical ground for applying identifiable CRL methods to learning-based parameter estimation approaches in dynamical systems.

**Assumption 3.2** (Injectivity).: For each observation \(\), there is only one corresponding latent vector \(\), i.e., the ODE solve function \(F\) (Defn. 3.2) is injective in \(\).

**Assumption 3.3** (Continuity and full support).: \(p_{}\) is smooth and continuous on \(\) with \(p_{}>0\) a.e.

**Assumption justification.** Tab. 1 summarizes common assumptions in traditional parameter estimation in dynamical systems and causal representation learning literature. We observe strong alignment between the ground assumptions in these two fields that justifies our idea of employing causal representation learning methods in parameter estimation problems for dynamical systems: (1) Asm. 2.1 implies that given a fixed initial value \(_{0}\), there exists a unique solution \((t),\,t[0,t_{}]\) for any \(f_{}\) with \(\). In other words, parameter domain \(\) is fully supported (Asm. 3.3), and these ODE solving processes from \(F()\) (Defn. 3.2) are deterministic, which aligns with the standard Asm. 3.1 in CRL. Since the ODE solution \(F()\) (SS 2) is continuous by definition, the continuity assumption from CRL (Asm. 3.3) is also fulfilled. (2) Asm. 2.2 emphasizes that each trajectory \(\) can only be uniquely generated from one parameter vector \(\), which means the generating process \(F\) (Defn. 3.2) is injective in \(\) (Asm. 3.2).

Next, we reformulate the parameter estimation problem in the language of causal representation learning. We first cast the generative process of the dynamical system \(f_{}((t))\) as a latent variable model by considering the underlying physical parameters \( p_{}\) as a set of _latent variables_. Given a trajectory \(\) generated by a set of underlying factors \(\) based on the vector field \(f_{}((t))\), we consider the observed trajectory as some _unknown nonlinear_ mixing of the underlying \(\), with the mixing process specified by individual vector field \(f_{}((t))\). This interpretation of observations aligns with the standard setup of causal representation learning; for instance, high-dimensional images are usually generated from some lower-dimensional latent generating factors through an unknown nonlinear process. Thus, estimating the parameters of unknown dynamical systems becomes equivalent to inferring the underlying generating factors in causal representation learning.

After transforming the parameter estimation into a latent variable identification problem in CRL, we can directly invoke the identifiability theory from the literature. Based on Locatello et al. [38, Theorem 1.], we conclude that the underlying parameters from an unknown system are in general _non-identifiable_. Nevertheless, several works proposed different weakly supervised learning strategies that can _partially identify_ the latent variables . To this end, we define partial identifiability in the context of dynamical systems by slightly adapting the definition of block-identifiability proposed by Von Kugelgen et al. :

**Definition 3.3** (Partial identifiability).: A partition \(_{S}:=(_{i})_{i S}\) with \(S[N]\) of parameter \(\) is partially identified by an encoder \(g:^{T}\) if the estimator \(}_{S}:=g()_{S}\) contains all and only information about the ground truth partition \(_{S}\), i.e. \(}_{S}=h(_{S})\) for some invertible mapping \(h:_{S}_{S}\) where \(_{S}:=_{i S}_{i}\).

Note that the inferred partition \(}_{S}\) can be a set of _entangled_ latent variables rather than a single one. In the multivariate case, one can consider the \(}_{S}\) as a bijective mixture of the ground truth parameter \(_{S}\).

**Corollary 3.2** (Identifiability without known functional form).: _Assume a dynamical system \(f\) satisfying Asms. 2.1 and 2.2, a pair of trajectories \(,}\) generated from the same system \(f\) but specified by different parameters \(,}\), respectively. Assume a partition of parameters \(_{S}\) with \(S[N]\) is shared across the pair of parameters \(,}\). Let \(g:^{T}\) be some smooth encoder and \(:^{T}\) be some left-invertible smooth solver that minimizes the following objective:_

\[(g,)=_{,}})_{S}-g(})_{S}\|_{2}^{2}}_{Alg_{ment} }+(g())-\|_{2}^{2}+\| (g(}))-}\|_{2}^{2}}_{Sfficiency},\] (3)

_then the shared partition \(_{S}\) is partially identified (Defn. 3.3) by \(g\) in the statistical setting._

**Discussion.** We remark that an implicit ODE solver \(\) is introduced in eq. (3) because the functional form \(f_{}\) is unknown. Intuitively, Cor. 3.2 provides partial identifiability results for the shared partition of parameters between two trajectories. We can consider the trajectories to be different simulation experiments but with certain sharing conditions, such as two wind simulations that share the same _layer thickness_ parameter. This partial identifiability statement is mainly concluded from the theory in the multiview CRL literature . Note that this corollary is _one exemplary__demonstration_ of achieving partial identifiability in dynamical systems. Many identifiability results from the causal representation works can be reformulated similarly by replacing their decoder with a differentiable ODE solver \(\). The high-level idea of multiview CRL is to identify the shared part between different views by enforcing alignment on the shared coordinates while preserving a sufficient information representation. _Alignment_ can be obtained by either minimizing the \(L_{2}\) loss between the encoding from different views on the shared coordinates  or maximizing the correlation on the shared dimensions correspondingly ; _Sufficiency_ of the learned representation is often prompted by maximizing the entropy  or minimizing the reconstruction error . Other types of causal representation learning works will be further discussed in SS 5.

## 4 CRL-construct of Identifiable Neural Emulators for Dynamical Systems

This section provides a step-by-step construct of a neural emulator that can (1) identify the _time-invariant, trajectory-specific_ physical parameters from some unknown dynamical systems if the identifiability conditions are met and (2) efficiently forecast future time steps. Identifiability can be guaranteed by employing causal representation learning approaches (SS 3) while forecasting ability can be obtained by using an efficient mechanistic solver  as a decoder. For the sake of simplicity, we term these identifiable neural emulators as _identifiers_. We remark that the general architecture remains consistent for most CRL approaches, while the learning object differs slightly in _latent regularization_, which is specified by individual identifiability algorithms. Intuitively, the _latent regularization_ can be interpreted as an additional constraint put on the learned encodings imposed by the setting-specific assumptions, such as the _alignment_ term in multiview CRL (Cor. 3.2). In the following, we demonstrate building an _identifier_ in the multiview setting from scratch and showcase how it can be easily generalized to other CRL approaches with slight adaptation.

**Architecture.** Since the parameters of interest are _time-invariant_ and _trajectory-specific_ (SS 2), we input the whole trajectory \(=((t_{1}),,(t_{T}))\) to a smooth encoder \(g:^{T}\), as shown in Fig. 1. Then, we decode the trajectory \(}\) from estimated parameter vector \(}:=g()\) using a mechanistic solver . The high-level idea of mechanistic neural networks is to approximate the underlying dynamical system using a set of explicit ODEs \(_{}}:C(,})=0\) with learnable coefficients \(^{d_{}}\). The explicit ODE family \(_{}}\) can then be interpreted as a constrained optimization problem and can thus be solved using a _neural relaxed linear programming solver_[47, Sec 3.1].

In more detail, the original design of MNN predicts the coefficients from the input trajectory \(\) using an MNN encoder \(g_{}\); however, as we enforce the estimated parameter \(\) to preserve _sufficient_ information of the entire trajectory \(\), we instead predict the coefficients \(\) from the estimated parameter \(}\) with the encoder \(g_{}:^{d_{}}\). Formally, the coefficients \(\) are computed as \(=g_{}(})\) where \(}=g()\). The resulting ODE family \(_{}}\) provides a broad variability of ODE

Figure 1: **Model overview with sea surface temperature inputs: Our _mechanistic identifier_ extracts the underlying time-invariant latitude-related parameters \(\), providing a versatile neural emulator for downstream causal analysis.**

parametrizations. A detailed formulation of \(_{}}\) at \(t\)[47, eq. (3)] is given by

\[^{l}c_{i}(t;})u^{(i)}}_{}+^{r}_{k}(t;})g_{k}(t,\{u^{(j)} \})}_{}=b(t;}),\] (4)

where \(u^{(i)}\) is \(i\)-th order approximations of the ground truth state \(\). Like in any ODE solving in practice, solving eq. (4) requires discretization of the continuous coefficients in time (e.g., \(c_{i}(t;})\)). Discretizing the ODE representation \(_{}}\) gives rise to:

\[_{i=0}^{l}c_{i,t}u_{t}^{(i)}+_{j=0}^{r}_{k,t}g_{k}(\{u_{t}^{(j)}\} )=b_{t} s.t.(u_{t_{1}},u_{t_{1}}^{},)=,\] (5)

where \(\) denotes the initial state vector of the ODE representation \(_{}}\). To this end, we present the explicit definition of the learnable coefficients \(:=(c_{i,t},_{k,t},b_{t},s_{t},)\) with \(t,i[l],k[r]\), which is a concatenation of linear coefficients \(c_{i,t}\), nonlinear coefficients \(_{i,k}\), adaptive step sizes \(s_{t}\) and initial values \(\). Note that we dropped the \(}\) in the notation for simplicity, but all of these coefficients \(\) are predicted from \(}\), as described previously. At last, MNN converts ODE solving into a constrained optimization problem by representing the \(_{}}\) using a set of constraints, including ODE equation constraints, initial value constraints, and smoothness constraints [47, Sec 3.1.1]. This optimization problem is then solved by _neural relaxed linear programming_ solver [47, Sec 3.1] in a time-parallel fashion, thus making the overall mechanistic solver scalable and GPU-friendly.

**Learning objective and latent regularizers.** Depending on whether the functional form of the underlying dynamical system is known or not, the proposed neural emulator can be trained using the losses given in Cor. 3.1 or Cor. 3.2, respectively. When the functional form is unknown, we employ CRL approaches to _partially_ identify the physical parameters. We remark that the causal representation learning schemes mainly differ in the latent regularizers, specified by the assumptions and settings. Therefore, we provide a more extensive summary of different causal representation learning approaches and their corresponding latent regularizer in Tab. 6.

## 5 Related Work

**Multi-environment CRL.** Another important line of work in causal representation learning focuses on the multi-environment setup, where the data are collected from multiple different environments and thus _non-identically distributed_. Causal variable identifiability are shown under _single node intervention per node_ with parametric assumptions on the mixing functions  or on the latent causal model . These parametric assumptions can be lifted by additionally assuming _paired intervention per node_, as demonstrated by . Overall, given the fruitful literature in multi-environment causal representation learning, we believe applying multi-environments methods to build identifiable neural emulators (SS 4) would be an exciting future avenue.

**CRL and dynamical systems.** Recent CRL works have been tackling the parameter identification problem in dynamical systems in a parametric setting. For instance, Rajendran et al.  considers a Gaussian linear time invariant system with control input and Balsells-Rodas et al.  assumes a switching dynamical system. By contrast, we show identifiability in a more general setting without specific parameter assumptions on the dynamical systems and prove different granularity of parameter identification under different system prior system knowledge (full identifiability if parametric form is known (Cor. 3.1) and partial identifiability when the system is unknown (Cor. 3.2)). Another closely related line of works, temporal causal representation learning, typically assume an _"intervenable"_ time series in the _latent space_, splitting the latent variables into two partitions, with one following the default dynamics and the other following the intervened dynamics. The goal of temporal CRL is to provably retrieve these _time-varying_ latent causal variables, such as inferring the position of a ball from raw images over time . Unlike these temporal CRL works, our approach models dynamics directly in the observational space, focusing on the _time-invariant, trajectory-specific_ physical parameters such as gravity or mass. Overall, our framework addresses a different hierarchy of problems. We believe both problems are orthogonal yet equally important, encouraging cross-pollination in future work.

**ODE discovery.** The ultimate goal of ODE discovery is to learn a human-interpretable equation for an unknown system, given discretized observations generated from this system. Recently, many machine learning frameworks have been used for ODE discovery, such as sparse linear regression , symbolic regression , simulation-based inference .

Becker et al. , d'Ascoli et al.  exploit transformer-based approaches to dynamical symbolic regression for univariate ODEs, which is extended by d'Ascoli et al.  to multivariate case. Schroder and Macke  employs _simulation-based variational inference_ to jointly learn the operators (like addition or multiplication) and the coefficients. However, this approach typically runs simulations inside the training loop, which could introduce a tremendous computational bottleneck when the simulator is inefficient. On the contrary, our approach works offline with pre-collected data, avoiding simulating on the fly. Although ODE discovery methods can provide symbolic equations for data from an unknown trajectory, the inferred equation does not have to align with the ground truth. In other words, theoretical identifiability guarantees for these methods are still missing.

**Identifiability of dynamical systems.** Identifiability of dynamical systems has been studied on a _case-by-case_ basis in traditional system identification literature [4; 64; 43]. Liang and Wu  studied ODE identifiability under measurement error. Scholl et al.  investigated the identifiability of ODE discovery with non-parametric assumption, but only for univariate cases. More recently, several works have advanced in identifiability analysis of _linear_ ODEs from a _single_ trajectory [17; 48; 59]. Overall, current theoretical results cannot conclude whether an unknown nonlinear ODE can be identified from observational data. Hence, in our work, we do not aim to identify the whole equation of the dynamical systems but instead focus on identifying the time-invariant parameters.

## 6 Experiments

This section provides experiments and results on both simulated and real-world climate data. We first validate full parameter identifiability (Cor. 3.1) under a wide range of known dynamical systems, as demonstrated in SS 6.1. Next, we consider in SSSS 6.2 and 6.3 time series data governed by an unknown physical process, so we employ the multiview CRL approach together with mechanistic neural networks to build our identifiable neural emulator (termed as _mechanistic identifier_), following the steps in SS 4. We compare _mechanistic identifier_ with three baselines: (1) _Ada-GVAE_, a traditional multiview model that uses a vanilla decoder instead of a mechanistic solver. (2) _Time-invariant MNN_, proposed by . We choose this variant of MNN as our baseline for a fair comparison. (3) _Contrastive identifier_, a contrastive loss-based CRL approach without a decoder [16; 65; 71]. We train _mechanistic identifier_ using eq. (3) and other baselines following the steps given in the original papers. After training, we evaluate these methods on their identifiability and long-term forecasting capability.

### Theory Validation: ODEBench

We demonstrate point-wise parameter identification results (presented as RMSE, mean \(\) std) over ODE system with known functional forms, including 63 dynamical systems from ODEBench  and the Cart-Pole system inspired by . Results are summarized in Tab. 7. For each system, we sample 100 tuples of the parameters \(\) within a valid range to e.g., preserve the chaotic properties. For each tuple, we solve the problem as outlined in Cor. 3.1, by either regressing the estimated observation \(F(})\) onto the true observation \(\), or equivalently, regressing the estimated vector field \(f_{}()\) onto the derivatives \(}\), given the same initial conditions. Note the data derivatives \(}\) can be obtained from numerical approximation in case it is not given a priori. The resulting root-mean-square deviation (RMSE) is calculated and averaged across the parameter dimensions. We report the mean and standard deviation of these averaged RMSEs over the 100 independent runs. From Tab. 7, we observe highly accurate point estimates for all stationary system parameters \(\), thereby validating Cor. 3.1 across various experimental settings.

### Wind Simulation

**Experimental setup.** Our experiment considers longitudinal and latitudinal wind velocities (also termed \(u,v\) wind components) from the global wind simulation data generated by various _layer-thickness_ parameters. Fig. 2 depicts the wind simulation output at a certain time point. To train the multiview approaches, we generate a tuple of three views: After sampling the first view \(^{1}\) randomly throughout the whole training set, we sample another trajectory \(^{2}\) from a different location which shares the same simulation condition as the first one, compared to the first view, the third view \(^{3}\) is then sampled from another simulation but at the same location. Overall, \(^{1},^{2}\) share the global simulation conditions like the _layer thickness_ parameter

Figure 2: **Wind simulation**: \(u,v\) components [m/s] of simulated air motion over the globe.

while \(^{1},^{3}\) only share the local features. All three views share global atmosphere-related features that are not specified as simulation conditions. More details about the data generation process and training pipeline are provided in App. C.2.

**Parameter identification.** In this experiment, we use the learned representation to classify the ground-truth labels generated by discretizing the generating factor _layer thickness_, and report the accuracy in Fig. 3. In more detail, we use latent dim=12 for all models and split the learned encodings into three partitions \(S_{1},S_{2},S_{3}\), with four dimensions each. Then, we individually predict the ground truth _layer thickness_ labels from each partition. According to the previously mentioned view-generating process, the _layer thickness_ parameter should be encoded in \(S_{1}\) for both _contrastive_ and _mechanistic identifiers_. This hypothesis is verified by Fig. 3 since both _contrastive_ and _mechanistic identifiers_ show a high accuracy of acc\(\)1 in the first partition \(S_{1}\) and low accuracy in other partitions. On the contrary, _Ada-GVAE_ and _TI-MNN_ performed significantly worse with an average acc. of 60% everywhere. Overall, Fig. 3 shows both the necessity of explicit time modeling using MNN solver (compared to _Ada-GVAE_) and identifiability power of multiview CRL (compared to _TI-MNN_).

### Real-world Sea Surface Temperature

**Experimental setup.** We evaluate the models on sea surface temperature dataset _SST-V2_. For the multiview training, we generate a pair trajectories from a small neighbor region (\( 5^{}\)) along the _same latitude_. We believe these pairs share certain climate properties as the locations from the same latitude share _roughly_ the amount of direct sunlight which will directly affect the sea surface temperature. Further information about the dataset and training procedure is provided in App. C.2.

**Time series forecasting.** We chunk the time series into slices of 4 years in training while keeping last four years as out-of-distribution forecasting task. To predict the last chunk, we input data from 2015 to 2018 to get the learned representation \(}\). Since we assume \(}\) to be _time-invariant_, we decode \(}\) together with 10 initial steps of 2019 to predict the last chunk. Note that _contrastive identifier_ is excluded from this task as it does not have a decoder. As shown in Tab. 2, the forecasting performance of _mechanistic Identifier_ surpasses _Ada-GVAE_ by a great margin, showcasing the superiority of integrating scalable mechanistic solvers in real-world time series datasets. At the same time, _TI-MNN_ performed worse and unstably despite the MNN component, verifying the need of the additional information bottleneck (parameter encoder \(g\)) and the multiview learning scheme.

**Climate-zone classification.** Since there is no ground truth latitude-related parameters available, we design a downstream classification task that verifies our learned representation encodes the latitude-related information. The goal of the task is to predict the climate zone _(tropical, temperate, polar)_ from the learned _shared_ representation because the latitude uniquely defines climate zones. We evaluated the methods in both _in-distribution_ (ID) and _out-of-distribution_ (OOD) setup for all baselines. In the OOD setting, we input data from longitude \(10^{}\) to longitude \(360^{}\) when training the classifier while keeping the first \(10\) degree as our out-of-distribution test data. Tab. 2 show that both _contrastive_ and _mechanistic identifiers_ perform decently, supporting the applicability of identifiable multiview CRL algorithms in dynamical systems. Overall, the performance of multiview CRL-based approaches (_contrastive and mechanistic identifiers_) far exceeds _Ada-GVAE_ and _TI-MNN_, again showcasing the superiority of the combination of causal representation learning and mechanistic solvers.

    &  \\  & **Acc.(ID)(\(\))** & **Acc.(OOD)(\(\))** & **Forecast. error(\(\))** \\  Ada-GVAE & \(0.468 0.001\) & \(0.467 0.000\) & \(0.043 0.044\) \\ TI-MNN & \(0.697 0.049\) & \(0.668 0.074\) & \(0.024 0.016\) \\ Contr. Identifier & \(\) & \(\) & ✗ \\
**Mech. Identifier** & \(\) & \(0.824 0.016\) & \(\) \\   

Table 2: **Performance evaluation on the SST-V2 data on various types of tasks. Results averaged over three random seeds with standard deviation, provided as (m \(\) std).**

Figure 3: **Prediction accuracy on _layer thickness_ parameter on wind simulation data, evaluated on individual encoding partitions \(S_{1},S_{2},S_{3}\). Results averaged from three random runs.**

**Average treatment effect estimation.** We further investigate the effect of climate zone on average temperature along one specific latitude through _average treatment effect_ (ATE) estimation. Formally, we consider the latitudinal average temperature as outcome \(Y\), two climate zones (_tropical_\((T=0)\), _polar_\((T=1)\)) as binary treatments, and the predicted latitude-specific features as unobserved mediators. Formally, ATE is defined as: \(:=[Y|do(T=1)]-[Y|do(T=0)]\). Since ATE cannot be computed directly , we estimate it using the popular _AIPW_ estimator . Fig. 4 illustrates that the estimated ATE from the non-identified representation lacks a discernible pattern  whereas the identified representation exhibits a noisy yet clear increasing trend, indicating the global warming effect. This is because the non-identified representation failed to isolate the covariates \(\), leading to biased treatment effect estimates. To estimate treatment effects, the covariates (i.e., the latitude-related parameters we identify) must not be influenced by the treatment (i.e., the climate zones). Otherwise, they become confounders, leading to incorrect estimates .

## 7 Limitations and Conclusion

In this paper, we build a bridge between causal representation learning and dynamical system identification. By virtue of this connection, we successfully equipped existing mechanistic models (focusing on  in practice for scalability reasons) with identification guarantees. Our analysis covers a large number of papers, including [10; 23; 24; 47; 68] explicitly refraining from making identifiability statements. At the same time, our work demonstrated that causal representation learning training constructs are ready to be applied in the real world, and the connection with dynamical systems offers untapped potential due to its relevance in the sciences. This was an overwhelmingly acknowledged limitation of the causal representation learning field [3; 11; 16; 39; 58; 62; 65; 71]. Having clearly demonstrated the mutual benefit of this connection, we hope that future work will scale up identifiable mechanistic models and apply them to even more complex dynamical systems and real scientific questions. Nevertheless, this paper has several technical limitations that could be addressed in future work. First of all, the proposed theory explicitly requires _determinism_ as one of the key assumptions (Asm. 3.1), which directly excludes another important type of differential equation: Stochastic Differential Equations. Second, we assume we directly observe the state \(\) without considering measurement noise. Although the empirical results were promising on real-world noisy data (SS 6.3), we believe explicitly modeling measurement noise would elevate the theory. Finally, our identifiability analysis focuses on the infinite data regime, which is unrealistic in real-world scenarios.