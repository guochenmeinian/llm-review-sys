# Hypothesis Testing the Circuit Hypothesis in LLMs

Claudia Shi\({}^{*}\)\({}^{1}\)

Nicolas Beltran-Velez\({}^{*}\)\({}^{1}\)

Achille Nazaret\({}^{*}\)\({}^{1}\)

Carolina Zheng\({}^{*}\)\({}^{1}\)

Adria Garriga-Alonso\({}^{3}\)

Andrew Jesson\({}^{1}\)

Maggie Makar\({}^{2}\)

David M. Blei\({}^{1}\)

\({}^{1}\)Department of Computer Science, Columbia University, New York, USA

\({}^{2}\)Computer Science and Engineering, University of Michigan, Ann Arbor, USA

\({}^{3}\)FAR AI, USA

Equal contribution.

###### Abstract

Large language models (LLMs) demonstrate surprising capabilities, but we do not understand how they are implemented. One hypothesis suggests that these capabilities are primarily executed by small subnetworks within the LLM, known as circuits. But how can we evaluate this hypothesis? In this paper, we formalize a set of criteria that a circuit is hypothesized to meet and develop a suite of hypothesis tests to evaluate how well circuits satisfy them. The criteria focus on the extent to which the LLM's behavior is preserved, the degree of localization of this behavior, and whether the circuit is minimal. We apply these tests to six circuits described in the research literature. We find that synthetic circuits - circuits that are hard-coded in the model - align with the idealized properties. Circuits discovered in Transformer models satisfy the criteria to varying degrees. To facilitate future empirical studies of circuits, we created the _circuitry_ package, a wrapper around the _TransformerLens_ library, which abstracts away lower-level manipulations of hooks and activations. The software is available at https://github.com/blei-lab/circuitry.

## 1 Introduction

The field of mechanistic interpretability aims to explain the inner workings of large language models (LLMs) through reverse engineering. One promising direction is to identify "circuits" that correspond to different tasks. Examples include circuits that perform context repetition (Olsson et al., 2022), identify indirect objects (Wang et al., 2023), and complete docstrings (Heimersheim and Janiak, 2023). Such research is motivated by the circuit hypothesis, which posits that LLMs implement their capabilities via small subnetworks within the model. If the circuit hypothesis holds, it would be scientifically interesting and practically useful. For example, it could lead to valuable insights about the emergence of properties such as in-context learning (Olsson et al., 2022) and grokking during training (Stander et al., 2023; Nanda et al., 2023). Moreover, identifying these circuits could aid in explaining model performance and controlling model output, such as improving truthfulness.

In this work, we empirically study the circuit hypothesis to assess its validity in practice. We begin by defining the ideal properties of circuits, which we posit to be: 1. _Mechanism Preservation:_ The performance of an idealized circuit should match that of the original model. 2. _Mechanism_Localization:_ Removing the circuit should eliminate the model's ability to perform the associated task. 3. _Minimality:_ A circuit should not contain any redundant edges.

We translate these properties into testable hypotheses. Some of these hypotheses depend on the strict validity of the idealized circuit hypothesis, while others are more flexible, allowing us to quantify the extent to which discovered circuits align with the ideal properties.

We apply these tests to six circuits described in the literature that each correspond to a different task: two synthetic, hard-coded circuits and four discovered in Transformer models. These circuits have also been used to benchmark automatic circuit discovery algorithms (Conmy et al., 2023; Syed et al., 2023).

We find that the synthetic circuits align well with the idealized properties and our hypotheses while the discovered circuits do not strictly adhere to the idealized properties. Nevertheless, these circuits are far from being random subnetworks within the model. Among the discovered circuits, the induction circuit (Olsson et al., 2022) passes two of the three idealized tests. The Docstring circuit (Heimersheim and Janiak, 2023) passes the minimality test. Furthermore, the empirical results indicate that these circuits can be significantly improved, bringing them closer to idealized circuits. For example, for two of them, removing \(20\%\) of the edges had little impact on their ability to approximate the model.

The contributions of this paper are: 1. A suite of formal and testable hypotheses derived from the circuit hypothesis. 2. A set of statistical procedures and software to perform each test. 3. An empirical study of existing circuits and their alignment to the circuit hypothesis.

### Related work

This research fits in the broader field of mechanistic interpretability. We provide a brief overview of related work here and a more comprehensive discussion in SS A.

Olah et al. (2020) introduced the concept of a circuit. Subsequently, various circuits have been proposed, particularly in vision models (Mu and Andreas, 2020; Cammarata et al., 2021; Schubert et al., 2021) and language models (Olsson et al., 2022; Wang et al., 2023; Hanna et al., 2023; Lieberum et al., 2023). The literature has been especially effective in explaining small Transformers that perform algorithmic tasks (Nanda et al., 2023; Heimersheim and Janiak, 2023; Zhong et al., 2023; Quirke et al., 2023; Stander et al., 2023).

This work builds on the growing effort around evaluating the quality of interpretability results (Doshi-Velez and Kim, 2017; Casper et al., 2023; Mills et al., 2023; Hase et al., 2024; Jacovi and Goldberg, 2020; Geiger et al., 2021; Chan et al., 2022; Wang et al., 2023; Schwettmann et al., 2023; Lindner et al., 2024; Friedman et al., 2024; Variengien and Winsor, 2023). It is closely related to the works of Wang et al. (2023) and Conny et al. (2023). Wang et al. (2023) introduce three criteria - faithfulness, minimality, and completeness - to evaluate the Indirect Object Identification circuit. Faithfulness serves as a metric, while minimality and completeness involve searching the space of circuits. Our idealized criteria are similar in spirit to Wang et al. (2023), but the specific tests differ. A key distinction is our adoption of a hypothesis testing framework, where none of our tests require searching the space of circuits. Conny et al. (2023) develops an automatic circuit discovery algorithm and assess the quality of circuits by measuring edge classification quality against a set of benchmark circuits.

## 2 Mechanistic Interpretability and LLMs

In this section, we define the necessary ingredients for mechanistic interpretability in LLMs.1

### LLMs as computation graphs

A Transformer-based LLM is a neural network that takes in a sequence of input tokens and produces a sequence of logits over possible output tokens. We define it as a function \(M:\), where \(=\{(x^{1},,x^{L})|\;x^{} V,\;L_{ 1}\}\) is the space of sequences of tokens, \(V\) is the space of possible tokens, called the vocabulary, and \(=\{(o^{1},,o^{L})|\;o^{}^{|V|},\;L _{ 1}\}\) is the space of sequences of logits over the vocabulary.

The function \(M\) is computed by a sequence of smaller operations that compose to form a **computational graph**. A computational graph is a directed acyclic graph \(=(,)\), where \(\) is the set of nodes and \(\) is the set of edges. Each node \(v\) represents an operation with one or more inputs and a single output. Each edge \((u,v)\) denotes that the output of node \(u\) is used as the input to node \(v\). We recursively define the output of node \(v\) as \(a_{v}=v(a_{v}^{})\), where \(a_{v}^{}=\{a_{u} u,(u,v)\}\) are the inputs to \(v\). We denote the number of inputs to \(v\) as \(d_{v}\).

We can use different levels of granularity to define the nodes of a computational graph, each leading to different types of interpretability. Following Elhage et al. (2021), we define the nodes of the computational graph of an LLM to be attention heads and MLP layers. The edges correspond to the residual connections between them. We also include input nodes corresponding to the embeddings of the input tokens and output nodes corresponding to the logits.

### Tasks: measuring the performance of a model

To measure whether a particular model performs a specific function, we define a _task_, \(\), as a tuple \(=(,s)\) of a dataset \(=\{(x_{i},y_{i})\}_{i=1}^{n}\) and a score \(s:\). The dataset \(\) contains pairs of inputs \(x_{i}\) and output \(y_{i}\). The score maps a sequence of logits, such as the output of the model \(M(x_{i})\), and the ground truth information \(y_{i}\) to a real number indicating the performance of the model's output on that particular example: a higher score indicates better performance.

**Example 1** (Greater-Than).: _An example task is the greater-than operation (Hanna et al., 2023), where we evaluate whether the model can perform this task as it would appear in natural language. The dataset \(\) contains inputs from \(x_{i}=\)"The noun lasted from the year XXYY to the year XX" where noun is an event, e.g "war", XX is a century, e.g. 16, and YY is a specific year in the century. The score function is the difference in assigned probabilities between the years smaller than \(y_{i}=YY\) and the years greater than or equal to \(y_{i}\). The implied task is to predict the next token YY's as any year greater than YY so as to respect chronological order._

A **circuit** is a subgraph \(C=(_{C},_{C})\) of the computational graph \(\). It includes the input and output nodes and a subset of edges, \(_{C}\), that connect the input to the output. We let \(\) denote the space of all circuits. Fig. 1 depicts one such circuit in a simplified computational graph of a two-layer attention-only Transformer. Given a circuit, we define its **complement**\(\) to be the subgraph of \(\) that includes all edges not in \(C\) and their corresponding nodes.

### Circuits of an LLM

A circuit specifies a valid subgraph, but it is not sufficient to specify a runnable model. Recall that a node \(v\) in the circuit is a function with a collection of inputs \(a_{u}\) corresponding to each \((u,v)\) present in \(\). If the edge \((u,v)\) is removed, then what input \(a_{u}\) should be provided to node \(v\)?

One solution, called **activation patching**, is to replace all inputs \(a_{u}\) with an alternative value \(a_{u}^{*}\), one for each edge \((u,v)\) that is absent from the circuit.

There are various ways to choose a value for \(a_{u}^{*}\). Two common approaches are zero ablation, which sets \(a_{u}^{*}\) to 0 (Olsson et al., 2022), and Symmetric Token Replacement (STR) patching (Chan et al., 2022; Geiger et al., 2024; Zhang and Nanda, 2024). STR sets \(a_{u}^{*}\) differently for each input \(x_{i}\) and proceeds as: First, create a corrupted input \(x_{i}^{c}\), which should be like \(x_{i}\) but with key tokens changed to semantically similar ones. For example, in the greater-than task with input \(x_{i}=\) "The war lasted from the year 1973 to the year 19", we might replace it with \(x_{i}^{c}=\) "The war lasted from the year 1901 to the year 19". The meaning is preserved but the \( 73\) constraint is removed. Then, run the model on \(x_{i}^{c}\) and cache all the activations \(a_{u}^{*}\). Finally, run the circuit on \(x_{i}\), replacing the input \(a_{u}\) of \(v\) with the cached \(a_{u}^{*}\) for all edges \((u,v)_{C}\) iteratively until reaching at the output node.

We use the notation \(C(x)\) to denote the output of the circuit \(C\) on the input \(x\), where the ablation scheme is implicit. When we compute the output of the complement of the circuit, namely \((x)\), we say that we _knock out_ the circuit \(C\) from the model \(M\).

### Evaluation metric: faithfulness

Given a circuit \(C(x)\) and a task \(=(,s)\), we can use the score function to evaluate how well the circuit performs the task. However, in mechanistic interpretability, the goal is often to evaluate whether the circuit replicates the behavior of the model, which is known as faithfulness.

We define a faithfulness metric, \(F:\), that maps two circuits and an example in \(\) to a real number measuring the similarity in behavior of these two circuits with respect to this particular example. We then define the **faithfulness** of circuit \(C\) to model \(M\) on task \(\) as

\[F_{}(M,C)_{(X,Y)}[F(M,C,X,Y)].\] (1)

We call \(F_{}(M,C)\) the _faithfulness score_ of circuit \(C\). For example, a faithfulness metric could be the \(l^{k}\) norm between the score of the model and the score of the circuit,

\[F(M,C,x,y)=|s(M(x),y)-s(C(x),y)|^{k},\]

with \(k\{1,2\}\). However, \(F\) can be more general and non-symmetric, such as the KL divergence between the logits of \(M\) and \(C\)(Conmy et al., 2023). Following convention, a lower value for \(F_{}(M,C)\) means the circuit is more faithful to the model.

## 3 Hypothesis Testing on Circuits

In this section, we develop three tests that formalize the following idealized criteria for a circuit: 1. _Mechanism Preservation_: The circuit should approximate the original model's performance on the task. 2. _Mechanism Localization_: The circuit should include all information critical to the task's execution. 3. _Minimality_: The circuit should be as small as possible.

In SS 3.1, we discuss three idealized but stringent hypotheses implied by these criteria and develop tests for each. Then, in SS 3.2, we develop two flexible tests for _Mechanism Localization_ and _Mechanism Preservation_, allowing users to design the null hypotheses and determine to what extent a circuit aligns with the idealized properties.

Standard hypothesis testing has 5 components:

1. A variable of interest, \(Z^{*}\), e.g., the faithfulness of the candidate circuit.
2. A reference distribution \(_{Z}\) over other \(Z\) that we wish to compare \(Z^{*}\) to, along with \(n\) samples \((z_{i})_{i=1}^{n}\) from it, e.g., the faithfulness of \(n\) randomly sampled circuits.
3. A null hypothesis \(H_{0}\), which relates \(Z^{*}\) and \(_{Z}\) and which we assume holds true. E.g., \(H_{0}\): "the candidate circuit is less faithful than \(90\%\) of random circuits from \(_{Z}\)."

Figure 1: Simplified computational graph of a two-layer LLM with two attention heads (without MLPs). Nodes in each layer connect to all nodes in the next layer via residual connections. A highlighted arbitrary circuit is shown in blue. In a detailed graph, each incoming edge to an attention head splits into three: query, key, and value.

4. A real-valued statistic \(t((z_{i})_{i=1}^{n})\) computed from the \(n\) samples, with known distribution when \(H_{0}\) holds, e.g, the number of times the candidate circuit is less faithful than a random circuit is a binomial variable with success probability \(90\%\) if \(H_{0}\) holds.
5. A confidence level \(1-\) and a rejection region \(R_{}\) such that if \(H_{0}\) is true, then the test statistic falls in \(R_{}\) with probability less than \(\). If we observe that \(t\) does fall in \(R_{}\), we conclude that \(H_{0}\) is false and we reject \(H_{0}\). We will be correct \(1-\) of the time when \(H_{0}\) is true.

Finally, defining a rejection region for each \(\), the \(p\)-value is the smallest \(\) such that \(t((z_{i})_{i=1}^{n}) R_{}\)(Young and Smith, 2005). The smaller the \(p\)-value, the stronger the evidence against \(H_{0}\).

To perform a hypothesis test, we specify the five components above, obtain the samples \(z_{1},,z_{n}\), compute the test statistic \(t(z_{1},,z_{n})\) with the associated \(p\)-value, and reject the null hypothesis with confidence \(1-\) if \(t(z_{1}, z_{n}) R_{}\).

### Idealized tests

We develop three tests, _Equivalence_, _Independence_, and _Minimality_, which are direct implications of the idealized criteria. These tests are designed to be stringent: if a circuit passes them, it provides strong evidence that the circuit aligns with the idealized criteria.

We assume we have a model \(M\), a task \(=(,s)\) with a score function \(s\), and a faithfulness metric \(F\). We are then given a candidate circuit \(C^{*}\) to evaluate.

**Equivalence.** Intuitively, if \(C^{*}\) is a good approximation of the original model \(M\), then \(C^{*}\) should perform as well as \(M\) on any random task input. Hence, the difference in task performance between \(M\) and \(C^{*}\) should be indistinguishable from chance. We formalize this intuition with an equivalence test: _the circuit and the original model should have the same chance of outperforming each other._

We write the difference in the task performance between the candidate circuit and the original model on one task datapoint \((x,y)\) as \((x,y)=s(C^{*}(x);y)-s(M(x);y)\), and let the null hypothesis be

\[H_{0}:|_{(X,Y)}((X,Y)>0 )-|<,\] (2)

where \(>0\) specifies a tolerance level for the difference in performance.

To test this hypothesis, we use a nonparametric test designed specifically for null hypotheses like \(H_{0}\). The test statistic is the number of times \(C^{*}\) and \(M\) outperform each other. We provide a detailed description of the test in SS B.1.

Since \(H_{0}\) is in the idealized direction, if we reject the null, we claim with confidence \(1-\),

**Non-Equivalence: \(C^{*}\)** _and \(M\) are unlikely to be equivalent on random task data._

**Independence.** If a circuit is solely responsible for the operations relevant to a task, then knocking it out would render the complement circuit unable to perform the task. An implication is that the performance of the complement circuit is independent of the original model on the task.

To formalize this claim, we define the null hypothesis as

\[H_{0}:s(}(X);Y)\!\!\! s(M(X);Y),\] (3)

where the randomness is over \(X\) and \(Y\).

To test this hypothesis, we use a permutation test. Specifically, we measure the independence between the performance of the complement circuit and the performance of the original model by using the Hilbert Schmidt Independence Criterion (HSIC) (Gretton et al., 2007), a nonparametric measure of independence. We provide a formal definition of HSIC and describe the test in SS B.3.

If the null is rejected, it implies that the complement circuit and the original model's performances are not independent. We claim with confidence \(1-\),

**Non-Independence:** _Knocking out the candidate circuit does not remove all the information relevant to the task that is present in the original model._Minimality.For minimality, we ask whether the circuit contains unnecessary edges, which are defined to be edges which when removed do not significantly change the circuit's performance.

Formally, we define the change induced by removing an edge \(e_{C}\) from a circuit \(C\) as

\[(e,C)=_{(x,y)}|s(C(x),y)-s(C_{-e}(x),y) |,\] (4)

where \(C_{-e}=(,_{C}\{e\})\).

We are interested in knowing whether for some specific edge \(e^{*}_{C}\) the value \((e^{*},C^{*})\) is significant. The problem now becomes how to define the reference distribution against which to compare \((e^{*},C^{*})\). Ideally, we would like to form the distribution \((e,C^{*})\) induced by unnecessary edges \(e\). But we do not know which \(e\) in \(C^{*}\) are unnecessary (finding them is precisely our goal).

To address this problem, we augment \(C^{*}\) to create "inflated" circuits. An inflated circuit \(C^{I}\) of \(C^{*}\) is obtained by adding a random path to \(C^{*}\) that introduces at least one new edge. Our assumption is that the randomly added path is unnecessary to the circuit performance, and so removing one of the added edges and studying the change in performance will provide our reference distribution.

We define \((C^{I},e^{I})^{I}\) such that \(C^{I}\) is a random inflated circuit obtained with the above procedure, and \(e^{I}\) is an edge sampled uniformly at random over the novel edges \(_{C^{I}}_{C^{*}}\). We then compare \((e^{*},C^{*})\), the change induced by removing edge \(e^{*}\) from \(C^{*}\), against the distribution of the random variable \((e^{I},C^{I})\), the change induced by removing what is assumed to be an irrelevant edge from an inflated version of \(C^{*}\). A graphical illustration of this procedure is in Fig. 4.

We define the null hypothesis as

\[H_{0}:_{C^{I},e^{I}^{I}}((e^{*},C^{*})>( e^{I},C^{I}))>q^{*},\] (5)

where \(q^{*}\) is a prespecified quantile.

The null hypothesis states that removing the edge \(e^{*}_{C}\) induces a significant change in the circuit score compared to removing the random edge in the inflated circuit. If we reject \(H_{0}\), we have found an unnecessary edge. We use a tail test in Algorithm 1 to compute the \(p\)-value.

If we perform the test on multiple different edges in the circuit, we need to correct for multiple hypothesis testing. To do so we use the Bonferroni correction, which is a conservative correction that controls the family-wise error rate . If we test \(m\) edges in the circuit, the corrected significance level is \(/m\).

If we test against multiple edges and after Bonferroni correction there is at least one edge for which the null hypothesis is rejected, then we claim with confidence \(1-\),

**Non-Minimality:**_The circuit has unnecessary edges._

### Flexible tests

SS 3.1 presents stringent tests that align with the idealized versions of circuits. Passing any of these tests is a notable achievement for any circuit. Here, we consider two flexible ways of testing mechanism preservation (_sufficiency_) and mechanism location (_partial necessity_).

Instead of comparing the candidate circuit to the original model, we compare \(C^{*}\) against random circuits drawn from a reference distribution. Different definitions of the reference distribution modulate the difficulty of the tests. We demonstrate that by varying the definition of the reference distribution, we can determine the extent to which the circuit aligns with the idealized criteria.

**Sufficiency.** For the sufficiency test, we ask whether the candidate circuit is particularly faithful to the original model compared to a random circuit from a reference distribution.

The variable of interest is the faithfulness of \(C^{*}\) to \(M\), \(Z^{*}=F_{}(M,C^{*})\). We define the reference distribution \(_{Z}\) as the distribution of \(Z=F_{}(M,C^{r})\) induced by sampling random circuits \(C^{r}\) from a chosen distribution \(\). The null hypothesis is

\[H_{0}:_{C^{r}}(F_{}(M,C^{*})<F_{}(M,C^{r}))  q^{*},\] (6)

where \(q^{*}\) is a prespecified quantile.

The advantage of a null hypothesis like Eq. 6 is that we can change the reference distribution \(\) and quantile \(q^{*}\) to capture to what degree we test the circuit hypothesis. For example, an easier (but important) version of the test is to have the reference distribution be over all circuits of the same size as \(C^{*}\). This test will verify that the candidate circuit is not simply a draw from the distribution of random circuits, ensuring that it is better than at least a fraction \(q^{*}\) of random circuits.

Moreover, we can modulate the difficulty and the implied conclusions of the test by changing the size of the random circuits relative to \(C^{*}\) and/or the target quantile \(q^{*}\). If our distribution of random circuits produces a fraction \(\) of circuits that are supersets of \(C^{*}\) (which we expect to be comparable to \(C^{*}\)), we can set \(q^{*}=1-\), an upper bound for the test's stringency.

The test statistic for Eq. 6 is the proportion of times \(C^{*}\) is more faithful than \(C^{r}_{i}\) for \(n\) circuits \(C^{r}_{i}\) sampled from \(\), \(t(C^{r}_{1},,C^{r}_{n})=_{i=1}^{n}(M,C^{r}) <F_{}(M,C^{r}_{i})\}}{n}\). Under \(H_{0}\), the test statistic follows a binomial distribution, and we compute the associated \(p\)-value using a one-sided binomial test. This procedure is described in more detail in Algorithm 1 of SS B.

If the \(p\)-value is less than the significance level \(\) for a quantile \(q^{*}\), we claim with confidence \(1-\),

**Sufficiency:**_The probability that \(C^{*}\) is more faithful to \(M\) than \(C^{r}\) is at least \(q^{*}\)._

**Partial necessity.** If the candidate circuit is responsible for solving a task in the model, then removing it will impair the model's ability to perform the task. However, this impairment may not be so severe as to make the model entirely independent of the complement circuit's output as tested in the independence test.

Instead, we define _partial necessity_: compared to removing a random reference circuit, removing the candidate circuit significantly reduces the model's faithfulness. The null hypothesis is

\[H_{0}:_{C^{r} R}(C^{*}C^{r}) q^{*},\] (7)

where \(q^{*}(0,1)\) is a user-chosen parameter and where "\(C^{*}\) is worse to knock out than \(C^{r}\)" is shorthand for \(F_{}(M,})>F_{}(M,})\).

Similar to the sufficiency test, this hypothesis test is highly flexible in its design. An easier version involves using a reference distribution over circuits from the complement \(}\) distribution. This allows us to determine whether the edges in the candidate circuit are particularly important for task performance compared to a random circuit. Another approach is to define the reference distribution by sampling from the original model \(M\), enabling us to assess whether the significance of a knockdown effect could have occurred by chance.

The test statistic is the proportion of times that \(}\) is less faithful than \(}\). Similar to the sufficiency test, we apply a binomial test to get the \(p\)-value. If \(H_{0}\) is rejected, we claim with confidence \(1-\),

**Partial necessity:**_The probability that knocking out \(C^{*}\) damages the faithfulness to \(M\) more than knocking out a random reference circuit is at least \(q^{*}\)._

## 4 Empirical Studies

We apply hypothesis tests to six benchmark circuits from the literature: two synthetic and four manually discovered. The synthetic circuits align with the idealized properties, validating our criteria.

  
**Test** & **Tracr-P** & **Tracr-R** & **Induction** & **IOI** & **G-T** & **DS** \\  Equivalence & ✓ & ✓ & \(\) & \(\) & \(\) & \(\) \\ Independence & ✓ & ✓ & ✓ & \(\) & \(\) & \(\) \\ Minimality & ✓ & ✓ & ✓ & \(\) & \(\) & ✓ \\   

Table 1: Hypothesis testing results for six circuits using the three idealized tests. A (✓) indicates the null hypothesis is rejected, while a (\(\)) indicates it is retained. The gray shaded boxes denote synthetic circuits, which align with our hypothesized behavior. For the equivalence test, \(=0.1\).

While the discovered circuits align with the hypotheses to varying degrees, the tests help assess their quality and analyze how well each circuit aligns with the idealized criteria.

### Experimental setup

We use the experiment configuration from ACDC (Conmy et al., 2023) for all tasks and circuits and perform the ablations using TransformerLens (Nanda and Bloom, 2022). Below, we briefly describe each task, with detailed explanations in SS D. We omit the greater-than (G-T) task as it was detailed in SS 2. Both IOI and greater-than use GPT-2 small, while the other tasks use various small Transformers.

**Indirect Object Identification (IOI, Wang et al. 2023)**: The goal is to predict the indirect object in a sentence containing two entities. For example, given the sequence "When Mary and John went for a walk, John gave an apple to", the task is to predict the token " Mary". The score function is logit(" Mary") - logit(" John").

**Induction (Olsson et al. 2022)**: The objective is to predict B after a sequence of the form AB...A. For example, given the sequence "Vernon Dursley and Petunia Durs", the goal is to predict the token "ley" (Ethage et al., 2021). The score function for this task is the log probability assigned to the correct token.

**Docstring (DS, Heimersheim and Janiak 2023)**: The objective is to predict the next variable name in a Python docstring. The score function is the logit difference between the correct answer and the most positive logit over the set of alternative arguments.

**Tracr (Lindner et al. 2024)**: For tracer-r, the goal is to reverse an input sequence. For tracer-p, the goal is to compute the proportion of x tokens in the input. The score function is the \(^{2}\) distance between the correct and predicted output. Both of these tasks have "ground truth" circuits, as the Transformers are compiled RASP programs Weiss et al. (2021), hence we call them synthetic circuits.

**Experiment details.** To construct the reference distributions \(\) of random circuits for the different tests, we sample paths in \(M\) (or \(}\)) from the input nodes (embeddings) to the output node (logits) using a random walk. For the sufficiency and partial necessity tests, we start from an empty circuit and augment it with the sampled paths until it has at least \(k\) edges, where \(k\) is a number we vary in our experiments. For the minimality test, we inflate the circuit by adding one randomly sampled path, and we then randomly choose an edge in the added path to knock out. We draw \(100\) random circuits to form the reference distribution for the sufficiency and partial necessity tests. For minimality, we draw \(10,000\) random edges for G-T and IOI and \(1000\) random edges for the other circuits. In all experiments, we use Eq. 1 with \(^{2}\) norm as the faithfulness metric. We set \(q^{*}\) to be \(0.9\) and \(\) to be \(0.05\).

### Results

Below we report and analyze key findings across tests. Additional results are reported in SS E.

**Idealized tests.** Table 1 presents the results for the six circuits across the three idealized hypothesis tests. The synthetic circuits (highlighted in grey) align well with our hypotheses, supporting the validity of these tests. Among the discovered circuits, alignment with the idealized hypotheses varies:

Figure 2: Left: The relative faithfulness of the candidate circuit compared to a random circuit from the reference distribution of varying sizes (x-axis). Dotted vertical lines indicate the actual size of the circuits. Right: The probability that a random circuit contains the canonical circuit.

the Induction circuit passes both the minimality and independence tests, while the Docstring circuit pass the minimality test. However, none of the other discovered circuits satisfy the idealized tests. Notably, both the Induction and Docstring circuits were identified in toy Transformer models and are relatively small in size. SS E.

**Sufficiency test.** We apply the sufficiency test to study the extent to which existing circuits align with the circuit hypothesis. As noted in SS 3.2, we can adjust the reference distribution to vary the test's stringency. Fig. 2 (left) illustrates the relative faithfulness of the candidate circuits compared to different reference circuit distributions.

The IOI and G-T circuits are significantly more faithful than random circuits at \(90\%\) of the original model's size, while the DS and Induction circuits outperform random circuits at \(30\%\) size. These results suggest their faithfulness is not due to random chance.

If the circuit hypothesis holds, we can expect the probability a randomly sampled circuit is as faithful as the candidate circuit to be equal to the probability the random circuit contains the candidate circuit. In Fig. 2 (right), we illustrate this probability under our sampling algorithm. We observe that the curve on the left is similar to the inverse of the right. Notably, while the Induction and DS circuits appear similar in Fig. 2 (left), they differ in Fig. 2 (right). The difference suggests that the Induction circuit is more closely aligned with the idealized properties compared to the DS circuit.

**Partial necessity test.** We now analyze the knockdown effect of the candidate circuit. Similar to the sufficiency test, we can define different reference distributions that reflect different underlying hypotheses. Table 2 reports the results of the hypothesis tests under two reference distributions.

We observe that when the reference circuit is drawn from the complement \(}\), the knockdown effect for the candidate circuits is significant across tasks. This suggests that edges in the candidate circuit play a more significant role in task performance than edges in the complement circuit. However, when compared against reference circuits drawn from the model \(M\), we find that knocking down the candidate circuit does not always have a more significant effect than knocking down a random circuit of the same size.

We increase the size of the random circuits to further investigate when knocking down a random circuit has a more significant effect than knocking down the candidate circuit. Surprisingly, we observed that for G-T and IOI, knocking down the complete model has less impact than knocking down the candidate circuit. This pattern appears with the STR ablation scheme but is absent with zero-ablation. Specifically, with zero-ablation, we observe a monotonic decrease in the relative significance of the candidate circuit as the size of the random circuits increase. In contrast, with STR ablation, its significance remains constant.

These findings suggest that the knockdown metric alone is insufficient to assess circuit quality, particularly when using STR ablation, as it is sensitive to artifacts from the validation dataset.

  
**Reference circuit** & **Induction** & **IOI** & **G-T** & **DS** \\  \(C^{r}}\) & ✓ & ✓ & ✓ & ✓ \\ \(C^{r} M\) & ✓ & ✓ & ✓ & \(\) \\   

Table 2: A (\(\)) indicates that knocking down \(C^{*}\) is significantly worse than knocking down \(C^{r}\), while (\(\)) means the converse. \(C_{r}\) is the same size as \(C^{*}\) but draws from different reference distribution.

Figure 3: The faithfulness of the circuit as we gradually knock down more edges from the canonical circuit. Edges are removed in order of their minimality score, starting with the least minimal. The dotted line shows the canonical circuit’s faithfulness, and the solid line shows an empty circuit’s faithfulness. Removing a few minimal edges does not significantly affect faithfulness.

One explanation is that all edges in the circuit are essential, so knocking down any edge impairs the model's task performance. If a random circuit includes the candidate circuit's edges, the effect is similar. To investigate this, we build on the minimality result.

**Minimality.** Recall from Table 1 that the G-T and IOI canonical circuits are not minimal. In Fig. 3, we gradually knock out more edges from the canonical circuit and report the faithfulness of the modified circuit. For G-T, we can remove around 50% of the edges while retaining the same faithfulness. For IOI, we can remove about 20% of the edges. However, we notice that the faithfulness of IOI does not vary monotonically as more edges are knocked out, revealing the complex mechanisms of circuits (e.g., negative mover heads). Although Docstring is minimal, we can still remove a small subset of edges without impacting faithfulness. This is because the reference edges are all approximately zero and the Docstring circuit was discovered in a toy Transformer model.

## 5 Discussion & Limitations

Do existing circuits align with the circuit hypothesis? We develop a suite of idealized and flexible tests to empirically study this question. The results suggest that while existing circuits do not strictly adhere to the idealized hypotheses, they are far from being random subnetworks.

Our tests successfully differentiate circuits by their alignment with the idealized properties, identifying the Induction circuit (Conmy et al., 2023) as the most aligned. We also demonstrate the limitations of existing evaluation criteria, showing that the knockdown effect alone is insufficient to determine circuit quality and that some benchmark circuits are not minimal.

Our tests and empirical studies have several limitations. The idealized tests are stringent, while the flexible tests are sensitive to circuit size measurements and require careful null hypothesis design. For the non-equivalence and non-independence tests, the desired direction is to retain the null, but we did not empirically study the Type II error associated with these tests. Furthermore, the empirical study uses the original experimental setup, whereas existing work and our ablation studies show that circuits are not robust to changes in the experimental setup.

Despite these limitations, we believe the study provides an overview of the extent to which existing circuits align with the idealized properties. We also believe that the tests will aid in developing new circuits, improving existing circuits, and scientifically studying the circuit hypothesis.

## 6 Acknowledgements

C.S, N.B, C.Z., and D.B. were funded by NSF IIS-2127869, NSF DMS-2311108, NSF/DoD PHY-2229929, ONR N00014-17-1-2131, ONR N00014-15-1-2209, the Simons Foundation, and Open Philanthropy. M.M. was funded by NSF 2153083 and NSF 2337529. A.N. was supported by funding from the Eric and Wendy Schmidt Center at the Broad Institute of MIT and Harvard, and the Afric Family Fund.

The authors thank Sebastian Salazar and Eli N. Weinstein for comments on the manuscript and helpful discussion. They also thank the contributors to the Automatic Circuit Discovery codebase (Conmy et al., 2023), which underlies a significant part of this paper's code.