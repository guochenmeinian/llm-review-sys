# ImageNet3D: Towards General-Purpose

Object-Level 3D Understanding

 Wufei Ma\({}^{1}\), Guofeng Zhang\({}^{1}\), Qihao Liu\({}^{1}\), Guanning Zeng\({}^{2}\),

Adam Kortylewski\({}^{4,5}\), Yaoyao Liu\({}^{6}\), Alan Yuille\({}^{1}\)

\({}^{1}\)Johns Hopkins University \({}^{2}\)Tsinghua University \({}^{4}\)University of Freiburg

\({}^{5}\)Max Planck Institute for Informatics \({}^{6}\)University of Illinois Urbana-Champaign

###### Abstract

A vision model with general-purpose object-level 3D understanding should be capable of inferring both 2D (_e.g._, class name and bounding box) and 3D information (_e.g._, 3D location and 3D viewpoint) for arbitrary rigid objects in natural images. This is a challenging task, as it involves inferring 3D information from 2D signals and most importantly, generalizing to rigid objects from unseen categories. However, existing datasets with object-level 3D annotations are often limited by the number of categories or the quality of annotations. Models developed on these datasets become specialists for certain categories or domains, and fail to generalize. In this work, we present ImageNet3D, a large dataset for general-purpose object-level 3D understanding. ImageNet3D augments \(200\) categories from the ImageNet dataset with 2D bounding box, 3D pose, 3D location annotations, and image captions interleaved with 3D information. With the new annotations available in ImageNet3D, we could (i) analyze the object-level 3D awareness of visual foundation models, and (ii) study and develop general-purpose models that infer both 2D and 3D information for arbitrary rigid objects in natural images, and (iii) integrate unified 3D models with large language models for 3D-related reasoning. We consider two new tasks, probing of object-level 3D awareness and open vocabulary pose estimation, besides standard classification and pose estimation. Experimental results on ImageNet3D demonstrate the potential of our dataset in building vision models with stronger general-purpose object-level 3D understanding. Our dataset and project page are available here: https://imagenet3d.github.io.

## 1 Introduction

General-purpose object-level 3D understanding requires models to infer both 2D (_e.g._, class name and bounding box) and 3D information (_e.g._, 3D location and 3D viewpoint) for arbitrary rigid objects in natural images. Correctly predicting these 2D and 3D information is crucial to a wide range of applications in robotics  and general-purpose artificial intelligence . Despite the success of previous learning-based approaches , embodied or multi-modal LLM agents with stronger 3D awareness will not only reason and interact better with the 3D world , but also alleviate certain key limitations, such as shortcut learning  or hallucination .

Despite the importance of object-level 3D understanding, previous datasets in this area were limited to a very small number of categories  or specific domains, such as autonomous driving  or indoor furniture . Subsequent works then focused on developing specialized models that excel at 3D tasks for the categories and domains considered in these datasets. While these specialized models are found useful for certain downstream applications, they fail easily when generalizing to novel categories. It is largely understudied of how to develop unified 3D models that are capable of inferring 2D and 3D information for all common rigid objects in natural images.

In the following, we consider two types of unified 3D models. **(i) Pretrained vision encoders with object-level 3D awareness.** Vision encoders from DINO , CLIP , Stable Diffusion , etc. are pretrained with self-supervised or weakly-supervised objectives. By learning a 3D discriminative representation, these vision encoders can be integrated into vision systems and benefit downstream recognition and reasoning. _While these encoders are found useful for 3D-related dense prediction tasks , their object-level 3D awareness remains unclear._ **(ii) Supervised 3D models.** By training on a large number of diverse data with 3D annotations, these models may achieve a stronger robustness and generalization ability. **However, there has been a lack of large-scale 3D datasets with a wide range of rigid categories, which constrains us from developing large unified 3D models for rigid objects or study the generalization and emerging properties of these models.**

In this work, we present ImageNet3D, a large dataset for general-purpose object-level 3D understanding. We extend \(200\) categories from ImageNet21k  with 2D bounding box and 6D pose annotations for more than \(86,000\) objects. To facilitate research on the two problems introduced above, ImageNet3D incorporates three key designs (see Figure 1). **(i) A large number of categories and instances.** ImageNet3D presents 2D and 3D annotations for a large number of object instances from a wide range of common rigid object categories found in natural images, as opposed to previous datasets focusing on specific categories and domains (see Table 1). This allows us to train and evaluate large unified 3D models capable of inferring both 2D and 3D information for arbitrary rigid objects. **(ii) Cross-category 3D alignment.** We align the canonical poses of all \(200\) categories based on semantic parts, shapes, and common knowledge, as shown in Figure 3. This is crucial for models to benefit from joint learning from multiple categories and to generalize to unseen categories, while omitted in previous datasets . **(iii) Natural captions with 3D information.** We adopt a GPT-assisted approach  and produce image captions interleaved with 3D information. These captions will be valuable assets to integrate unified 3D models with large language models [25; 26] and perform 3D-related reasoning from natural images and language.

With the three key designs and new 3D annotations collected, ImageNet3D distinguishes itself from all previous 3D datasets and facilitates the evaluation and research of general-purpose object-level 3D understanding. Besides standard classification and pose estimation as studied in previous works [13; 24], we further consider two new tasks, probing of object-level 3D awareness and open-vocabulary pose estimation. Experimental results show that with ImageNet3D, we can develop general-purpose models capable of inferring 3D information for a wide range of rigid categories. Moreover, baseline results on ImageNet3D reveal the limitations of current 3D approaches and present new problems and challenges for future studies.

## 2 Related Works

**Datasets with 3D annotations.** Previous datasets with 3D annotations have led to significant advancements of 3D models for 3D object detection [29; 30] and pose estimation [31; 32; 14]. However, most existing datasets are limited to a small number of categories [13; 14; 15] or specific domains, such as autonomous driving [16; 17] or indoor furniture . ObjectNet3D  extends the

Figure 1: **Overview of ImageNet3D data and annotations.** ImageNet3D provides **3D location and viewpoint (_i.e.,_ 6D pose)** for more than 86,000 objects. We also annotate **cross-category 3D alignment** for the 200 rigid categories in ImageNet3D. Lastly we generate **image captions interleaved with 3D information** to integrate unified 3D models with large language models.

number of categories but the quality of the annotations constrains us from developing large unified 3D models. Our ImageNet3D largely extends the number of categories and instances, improves the annotation quality, and presents other crucial annotations such as cross-category 3D alignment and natural captions interleaved with 3D information. ImageNet3D allows us to develop unified 3D models for general-purpose 3D understanding and facilitates studies on new research problems, such as probing of object-level 3D awareness and open-vocabulary pose estimation.

**Category-level pose estimation.** Our work is closely related to category-level pose estimation, where a model predicts 3D or 6D poses for arbitrary instances from certain rigid categories. Previous works have explored keypoint-based methods  and 3D compositional models [32; 33]. However, these approaches limited their scopes to a small number of categories, and as far as we know, there were no attempts to develop large unified models for all common rigid categories. We further consider open-vocabulary pose estimation where models generalize to similar but novel categories. This topic has also been discussed in recent parallel works [34; 35] but  was limited to synthetic data rendered with photorealistic CAD models.

**3D awareness of visual foundation Models.** Recent work demonstrates the significant capabilities of large-scale pretrained vision models in 2D tasks [20; 36; 37; 38; 21], suggesting robust 2D representations. Beyond benchmarking the semantic and localization capabilities of visual backbones [39; 40; 41; 42; 43; 44], Banani et al.  studied the 3D awareness of these 2D models using trainable probes and zero-shot inference methods. However, their exploration was limited to only two basic aspects of 3D understanding - single-view surface reconstruction and multi-view consistency - due to absence of large datasets with 3D annotations. We further analyze the 3D awareness of visual models and provide a more comprehensive understanding of their progress in learning about the 3D structure of the world, demonstrating the significance of our proposed ImageNet3D.

## 3 ImageNet3D Dataset

ImageNet3D dataset aims to facilitate the evaluation and research of general-purpose object-level 3D understanding models. Besides 6D pose annotations for more than \(86,000\) objects from \(200\) categories, we annotate meta-classes, cross-category 3D alignment, and natural captions interleaved with 3D information as demonstrated in Figure 1. We start by presenting our dataset construction in Section 3.1. Then in Section 3.2 we introduce the necessity of cross-category 3D alignment and how it is achieved in our dataset. Lastly, we provide details on our image caption generation in Section 3.3.

### Dataset Construction

**Overview.** We choose the ImageNet21k dataset  as our data source, as it provides a large and diverse set of images with class labels. We start by annotating 2D bounding boxes for the object instances in the images. We adopt a machine-assisted approach for 2D bounding box annotations, where a Grounding DINO model  is used to produce 2D bounding boxes prompted with the category label. The bounding box annotations are then filtered and improved by human evaluators. Next, we collect 3D CAD models as representative shapes for each object category from Objavverse .

   Dataset & Images & \# categories & \# instances & Annotations \\  PASCAL3D+ (2014)  & Real & \(12\) & \(12,000\) & 6D pose \\ ObjectNet3D (2016)  & Real & \(100\) & \(57,000\) & 6D pose \\ CAMERA25 (2019)  & Synthetic & \(6\) & \(1,000\) & 3D bbox \\ REAL275 (2019)  & Real & \(6\) & \(24\) & 3D bbox \\ Objectron (2021)  & Real & \(9\) & \(18,000\) & 3D bbox \\ Wild6D (2022)  & Real & \(5\) & \(2,000\) & 3D bbox \\  ImageNet3D (ours) & Real & \(200\) & \(86,000\) & 6D pose, captions, object \\  & & & & visual quality, cross-category \\  & & & & 3D alignment \\   

Table 1: **Comparison between ImageNet3D and previous datasets with 3D annotations. Previous datasets are limited by the number of rigid categories [13; 28; 14] or the quality of the annotations , constraining the development of large unified 3D models for general-purpose 3D understanding.**The CAD models are carefully aligned based on their semantic parts and provide canonical poses for 6D pose annotations. For 3D annotations, we recruit a total of 30 annotators to annotate 6D poses for the objects, as well as the scene density and object visual quality. Lastly we generate natural captions interleaved with 6D poses with GPT-4o. An overview of our data generation pipeline is visualized in Figure 2.

**Object categories.** Our goal is to provide 3D annotations for all common rigid categories in real world. To achieve this, we carefully examine previous 2D and 3D datasets for image classification , object detection [47; 29; 28], and pose estimation [24; 27]. We choose the categories that are rigid, have well-defined shapes with certain variance, and have enough number of images available, which leads to the 200 categories in ImageNet3D. For detailed discussions on the choice of categories, please refer to Section A.1. Moreover, to leverage existing research in the field, we adopt the 100 categories and raw images from ObjectNet3D  and largely extend the number of categories and instances. As one of our goals is to improve the quality of 3D annotations, we only take unannotated images from ObjectNet3D, and all 3D annotations on these images are our original work. In Section D, we perform human evaluation on the annotation qualities in ObjectNet3D  and our ImageNet3D.

**Annotator recruitment.** We recruit 30 annotators for data annotation. To improve the quality of the collected data, each annotator must complete an onboarding stage before starting. The onboarding stage includes training sessions where we present detailed instructions of various annotations and proper ways to handle boundary cases. Additionally, each annotator must annotate sample questions and meet the accuracy threshold to qualify for subsequent work. Please refer to Section A and Section C regarding our annotator guidelines, training sessions, and ethics statement.

**Data collection.** We develop a web-based tool for data annotation so annotators can easily access the platform without local installation. A screenshot of our annotation tool is shown in Figure 5. For each object in ImageNet3D, the annotator needs to annotate the following. **(i) 3D location and 3D viewpoint (i.e., 6D pose):** For more intuitive annotating, the 3D location is parameterized as a combination of 2D location and distance of the object. The 3D viewpoint is defined as the rotation of the object with respect to the canonical pose of the category, and represented by three rotation parameters, azimuth, elevation, and in-plane rotation (see Figure 6). **(ii) Density of the scene:** A binary label indicating if the scene is dense with many objects from the same category close to each other. **(iii) Visual quality of the object:** A categorical label with one of the four options: good, partially visible, barely visible, not visible. We refer the readers to Section C where we provide links to our annotation guidelines and instructions.

### Cross-Category 3D Alignment

As explained in Section 3.1, the 3D viewpoint of an object is defined as the rotation of the object with respect to the canonical pose of this category. However, in previous datasets such as ObjectNet3D , canonical poses from different categories are not necessarily "aligned". From the canonical poses depicted in Figure 4, the parts where the pencils "write" or the paintbrushes "paint" are pointing to different directions, and the spouts of faucets and kettles are also mis-aligned.

**As we scale up the number of categories in 3D-annotated datasets, having cross-category 3D alignment is a crucial design for the study of general-purpose object-level 3D understanding.**

Figure 2: An overview of our ImageNet3D dataset creation pipeline.

While objects from different categories have their unique characteristics, certain semantic parts are often shared between multiple categories, such as the wheels of "ambulances" and "forklifts" or push handles of "shopping carts" and "hand mowers". Correctly aligning the canonical poses will (i) allow models to utilize the semantic similarities between parts of different categories and exploit the benefits of joint learning from multiple categories, and (ii) generalize to novel categories by inferring 3D viewpoints from semantic parts that the model has seen from other categories during training.

Therefore, we manually align the canonical poses of all 200 categories in ImageNet3D. Specifically, we consider the following three rules. **(i) Semantic parts:** categories sharing similar semantic parts, such as wheels, push handles, or spouts, should be aligned. **(ii) Similar shapes:** categories sharing similar shapes, such as fans, Ferris wheels, and life buoys, should be aligned. **(iii) Common knowledge:** certain categories are pre-defined with a "front" direction from common knowledge, such as "refrigerator", "treadmill", or "violin".

### Natural Captions with 3D Information

An important application of general-purpose object-level 3D understanding models is to integrate them with large language models (LLMs) and benefit downstream multi-modal reasoning. This would largely improve the 3D-awareness of multi-modal large language models (MLLMs) and improve 3D-related reasoning capabilities, such as poses  and distances . Previous approaches integrated segmentation or human pose modules with MLLMs [25; 26] and demonstrate strong multi-modal reasoning abilities.

To integrate general-purpose 3D understanding with existing MLLMs, we present image captions interleaved with 3D information. As shown in Figure 1, our captions provide a detailed description of the image, object appearances and locations, as well as mutual relations. Moreover, for objects with 3D annotations, we add a special <pose6d> token as a reference to our 2D and 3D annotations for this object. To generate these image captions with 3D information, we adopt a GPT-assisted approach  and feed 2D and 3D annotations to the model via the textual prompts. Then GPT-4v is used to integrate these information and produce a coherent image caption interleaved with 3D information. Please refer to Section A.3 for details on caption generation as well as our GPT-4v prompts.

## 4 Tasks

With the new data and annotations available in ImageNet3D, we hope to push forward the evaluation and research of general-purpose object-level 3D understanding. We consider two new tasks, probing of 3D object-level awareness 4.1 and open-vocabulary pose estimation 4.2, besides joint image classification and category-level pose estimation 4.3. We further other standard computer vision tasks, such as image classification and object detection, and report the full performance in our dataset page.

### Linear Probing of Object-Level 3D Awareness

Recent developments of large-scale pretraining have yielded visual foundation models with strong capabilities. Self-supervised approaches such as MAE  and DINO  provide strong and generalizable feature representations that benefit downstream recognition and localization. When jointly trained with language supervision, CLIP features  demonstrate transferability to a wide range of multi-modal tasks. Moreover, foundation models for specific tasks, _e.g._, MiDaS  for depth estimation, also show impressive capabilities when applied to arbitrary images.

Are these visual foundation models object-level 3D aware? Can these feature representations distinguish objects from different 3D viewpoints or retrieve objects from similar 3D viewpoints? A parallel work  found that certain foundation models have better 3D awareness despite trained without 3D supervision. However, they focused on low-level tasks such as depth estimation and part correspondence. It remains unclear if these visual foundation models are object-level 3D aware and produce 3D discriminative object representations.

In this task, we aim to evaluate the object-level 3D awareness of visual foundation models by linear probing the frozen feature representations on 3D viewpoint classification task. This is because models with superior object-level 3D awareness would produce 3D discriminative features that help to classify the viewpoints correctly. Compared to low-level tasks such as depth estimation and part correspondence, object-level 3D awareness is directly associated with high-level scene understanding that is crucial to downstream recognition and reasoning in robotics and visual question answering.

**Task formulation.** We evaluate object-level 3D awareness by linear probing the frozen feature representations on 3D viewpoint classification task. Specifically, three linear classifiers are trained with respect to each of the three parameters encoding 3D viewpoint. To ensure that the neural features encode rich information about the target object with 3D annotations, we adopt a data pre-processing step where we crop and resize the image based on the 2D and 3D annotations (see Figure 7). Following the linear probing setting on ImageNet1k , we apply grid search to a range of hyperparameters, such as learning rate, pooling strategy, and training epochs, and select the best performance achievable with the frozen backbone features.

**Evaluation.** To jointly evaluate the classification results on three viewpoint parameters, we adopt the **pose error** given by the angle between the predicted rotation matrix and the groundtruth rotation matrix 

\[(R_{},R_{})=(R_{}^{} R_{})\|_{}}{}\] (1)

Based on the pose errors, we compute **pose estimation accuracy**, which is the percentage of samples with pose errors smaller than a pre-defined threshold.

### Open-Vocabulary Pose Estimation

Existing 3D models for pose estimation [31; 32; 14] or object detection [30; 29] focused on scenarios where object images and 3D annotations from the target categories are available at training time. These models fail easily when generalizing to novel categories that posses similar semantic parts with categories that the models are trained on. A recent study  investigated the open-vocabulary pose estimation problem from synthetic data rendered with photorealistic CAD models. However, the synthetic dataset demonstrates limited variations in both object appearances and image backgrounds, while our ImageNet3D provide 3D annotations on real images from a wide range of rigid categories to study this problem.

How can 3D models generalize to novel categories? Intuitively models may utilize semantic parts that are shared between novel categories and categories that are seen during training. As demonstrated in Figure 8, models may generalize 3D knowledge learned from cars (_i.e._, sedans and SUVs) to fire trucks based on the wheels and body of vehicles, or from hand barrows to shopping cars based on the push handles. Additionally, open-vocabulary pose estimation models may utilize large-scale 2D pre-training data or vision-language supervision and learn useful semantic information. For instance, after seeing 2D images of people riding a bicycle and a tricycle, models would learn to align the semantic parts and generalize from bicycles to tricycles. Lastly we provide detailed descriptions of object shape, part structure, and how humans interact with these objects for all categories in ImageNet3D (see Section A.1). Models may utilize such information and learn transferable features that generalize to novel rigid categories.

**Task formulation.** We split the 200 categories in ImageNet to 63 common categories for training and 137 categories for open-vocabulary pose estimation. Models may utilize additional 2D data for pretraining but may be only trained on 3D annotations from the 63 common categories. During testing time, models have access to our annotated category-level captions besides the testing images. For complete lists of categories used for training and open-vocabulary pose estimation, please refer to Section C.

**Evaluation.** Following standard pose estimation tasks [13; 24], we report _pose estimation accuracy_ and median _pose error_ (Eq. 1) on testing data from novel categories that are unseen during training.

### Joint Image Classification and Category-Level Pose Estimation

For joint image classification and category-level pose estimation, a model first classifies the object and then predicts the 3D viewpoint of the object. A prediction is only considered correct if both the predicted class label is correct and the pose error is within a given threshold.

While this task has been studied in previous datasets [13; 24], ImageNet3D brings new challenges to existing models. Previous studies often focused on 12 or 20 categories [31; 32; 33] - how can we scale up these category-level 3D models to 200 categories while retaining a comparable performance? Moreover, with the meta classes and more categories available, we can better assess the limitations of current category-level pose estimation models.

**Task formulation.** For each of the 200 categories, we split the samples into training and validation splits, each accounting for about 50% of the data. Based on the number of samples used for training, we can further evaluate models under zero-shot, few-shot, and fully supervised settings.

**Evaluation.** Following , we adopt the **3D-aware classification accuracy**, where a prediction is correct only if the predicted class label is correct and the predicted pose error is lower than a given threshold.

## 5 Experimental Results

In this section we report the baseline performance of linear probing of object-level 3D awareness in Section 5.1, open-vocabulary pose estimation in Section 5.2, and joint image classification and category-level pose estimation in Section 5.3. For implementation details of various baseline models, including hyperparameters and hardware setup, please refer to Section B in the appendix. All experimental results in this section are based on the first version of ImageNet3D with 189 categories. Please refer to our dataset page for new releases of ImageNet3D and updated baseline results.

### Linear Probing of Object-Level 3D Awareness

**Baselines.** We measure the object-level 3D awareness for a range of general-purpose vision models designed for representation learning [52; 49; 19; 51], multi-modal learning , and depth estimation . These models adopt standard transformer architectures and we train a linear probe on frozen class embedding features. We focus on model sizes comparable to ViT-base and report the training supervisions and datasets in Table 2.

**Results.** We report the pose estimation accuracies with threshold \(/6\) for various baseline methods in Table 2. Results show that visual foundation models trained without 3D supervision demonstrates a reasonable level of object-level 3D awareness. Specifically, we find that DINO v2 largely outperforms other approaches in terms of object-level 3D awareness, followed by MAE, DINO, and MiDaS. However, the gap between these models are much smaller than the findings in . Our ImageNet3D provides valuable assets to assess these visual foundation models from the perspective of object-level 3D awareness. In Section E.1 we present results on different metrics and study the scaling properties of self-supervised approaches on object-level 3D awareness.

### Open-Vocabulary Pose Estimation

**Baselines.** Open-vocabulary pose estimation is a rather new topic, and there are no existing baselines designed specifically for this task. Oryon  operates on RGBD data and requires an image of the same object from a different viewpoint as a reference. OV9D  studies the problem by generating photorealistic synthetic data but the code is not available for reproduction. Hence for baseline results, we consider models that learn category-agnostic features that generalize to novel categories and

Figure 8: **Illustration of open vocabulary pose estimation. Open-vocabulary models may utilize large-scale 2D data, vision-language supervision, or our category descriptions to learn transferable features and generalize to novel rigid categories.**instances. Two types of approaches are considered: **(i) Classification-based methods** that formulate pose estimation as a classification problem. A pose classification head is trained on top of the backbone features. We consider two types of backbones, ResNet50 and SwinTransformer-Tiny, as our baselines. **(ii) 3D compositional models** learn neural mesh models with contrastive features and perform analysis-by-synthesis during inference. We develop _NMM-Sphere_, which is a 3D compositional model with a general sphere mesh for all categories and trained with class and part contrastive features .

**Results.** We report the pose estimation accuracy with threshold \(/6\) in Table 3 and present the full results in Section E.2. Results show that by annotating cross-category 3D alignment, models trained with category-agnostic features can generalize to novel categories with a reasonable performance. However, generalization abilities of current 3D models are still quite limited when compared to models trained on annotations from novel categories. Open-vocabulary pose estimation is a rather new topic but is crucial to the development of general-purpose 3D understanding. We call for future studies on this challenging but important problem.

### Image Classification and Category-Level Pose Estimation

Baselines.We consider two types of baseline methods: **(i) Classification-based methods** that formulate pose estimation as a classification problem and train a shared pose classification head. Following previous works [31; 33], we extend ResNet50 and SwinTransformer-Tiny for pose estimation, denoted by "ResNet50-General" and "SwinTrans-T-General". **(ii) 3D compositional models** learn neural mesh models with contrastive features and perform analysis-by-synthesis during inference. NOVUM  adopts category-level meshes and more robust rendering techniques. We develop _NMM-Sphere_, which is a 3D compositional model with a general sphere mesh for all categories and is trained with class and part contrastive features .

**Results.** We report the 3D-aware classification accuracy with threshold \(/6\) in Table 4. Results show that with ImageNet3D, we can develop general-purpose models capable of inferring 3D information

    &  &  &  &  \\   & & & & Avg. & Elec. & Fur. & Hou. & Mus. & Spo. & Veh. & Work \\  Deff **III** & ViT-B/16 & classification & ImageNet21k & 36.6 & 47.9 & 48.2 & 36.8 & 21.5 & 16.6 & 35.0 & 25.3 \\ MAE  & ViT-B/16 & SSL & ImageNet1k & 46.6 & 57.6 & 67.8 & 40.2 & 29.0 & 20.2 & 58.4 & 25.6 \\ DINO  & ViT-B/16 & SSL & ImageNet1k & 42.0 & 53.1 & 57.0 & 39.8 & 28.0 & 19.3 & 45.3 & 27.0 \\ DINO v2  & ViT-B/14 & SSL & LVD-142M & **56.3** & **64.0** & **75.3** & **47.9** & **32.9** & **23.5** & **74.7** & **38.1** \\ CLIP  & ViT-B/16 & VLM & _private_ & 39.7 & 50.3 & 52.8 & 39.7 & 23.1 & 19.3 & 39.8 & 26.4 \\ MiDaS  & ViT-L/16 & depth & MIX-6 & 40.5 & 50.9 & 56.7 & 40.2 & 26.7 & 18.9 & 39.2 & 28.1 \\   

Table 2: **Quantitative results on probing of object-level 3D awareness. We report the \(/6\)_pose estimation accuracy_ for the average performance on all categories, as well as the performance for each meta class (from left to right): _electronics_, _furniture_, _household items_, _music instrument_, _sports equipment_, _vehicles & transportation_, and _work equipment_. Among the tested visual foundation models, DINO v2 demonstrated the best object-level 3D awareness.**

    &  \\   & Avg. & Electronics & Furniture & Household & Music & Sports & Vehicles & Work \\  ResNet50-General & 53.6 & 49.2 & 52.4 & 45.8 & 26.0 & 65.2 & 56.5 & 58.5 \\  _(trained on novel categories)_ & & & & & & & & \\  ResNet50-General & **37.1** & 30.1 & **35.6** & **28.1** & 11.8 & **51.7** & **36.7** & **40.9** \\ SwinTrans-T-General & 35.8 & 30.9 & 34.3 & 26.1 & 12.2 & 46.2 & 34.4 & 39.2 \\ NMM-Sphere & 29.5 & **31.7** & 25.4 & 21.7 & **25.6** & 19.8 & 33.4 & 19.3 \\   

Table 3: **Quantitative results on open-vocabulary pose estimation. We report the _pose estimation accuracy_ with threshold \(/6\) on testing data from novel categories unseen during training. We report the average performance on all novel categories, as well as performance for novel categories in each meta class. Results show that models with category-agnostic features can generalize to novel categories, but by a limited amount.**for a wide range of common rigid categories. However, we also note that there is a clear performance degradation as the number of categories scale up, as compared to results found in previous works . We present the full results with other metrics and backbones and study the scaling properties of pose estimation models in Section E.3.

## 6 Conclusion

In this paper we present ImageNet3D, a large dataset for general-purpose object-level 3D understanding. ImageNet3D largely extends the number of rigid categories and object instances, as compared to previous datasets with 3D annotations. Moreover, ImageNet3D improves the quality of 3D annotations by annotating cross-category 3D alignment, and provides new types of annotations, such as object visual qualities and image captions interleaved with 3D information that enable new research problems. We provide baseline results on standard 3D tasks, as well as novel tasks such as probing of object-level 3D awareness and open-vocabulary pose estimation. Experimental results show that with ImageNet3D, we can develop general-purpose models capable of inferring 3D information for a wide range of rigid categories. We also identify limitations of existing 3D models from our baseline experiments and discuss new problems and challenges for future studies.

Limitations.As our image data are collected from ImageNet21k, most images are object-centric with only one or two instances. Thus our dataset may not be suitable for 3D object detection or tasks that require object co-occurrences. As far as we know, there are no existing 3D object detection datasets with 3D annotations for more than 20 categories. One reason is that annotating 6D poses for multiple categories is very time consuming, and category co-occurrences follow a long-tail distribution. On the other hand, previous studies [32; 33] have found that compositional models trained on object-centric data have the ability to generalize to real images with multiple objects or partial occlusion, which makes our ImageNet3D a competitive option when developing models for general-purpose object-level 3D understanding.