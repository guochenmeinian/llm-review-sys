# Local to Global: Learning Dynamics and Effect of Initialization for Transformers

Ashok Vardhan Makkuva

EPFL

&Marco Bondaschi

EPFL

&Chanakya Ebote

EPFL

&Adway Girish

EPFL

&Alliot Nagle

UT Austin

&Hyeji Kim

UT Austin

&Michael Gastpar

EPFL

###### Abstract

In recent years, transformer-based models have revolutionized deep learning, particularly in sequence modeling. To better understand this phenomenon, there is a growing interest in using Markov input processes to study transformers. However, our current understanding in this regard remains limited with many fundamental questions about how transformers learn Markov chains still unanswered. In this paper, we address this by focusing on first-order Markov chains and single-layer transformers, providing a comprehensive characterization of the learning dynamics in this context. Specifically, we prove that transformer parameters trained on next-token prediction loss can either converge to global or local minima, contingent on the initialization and the Markovian data properties, and we characterize the precise conditions under which this occurs. To the best of our knowledge, this is the first result of its kind highlighting the role of initialization. We further demonstrate that our theoretical findings are corroborated by empirical evidence. Based on these insights, we provide guidelines for the initialization of single-layer transformers and demonstrate their effectiveness. Finally, we outline several open problems in this arena. Code is available at: https://github.com/Bond1995/Markov.

## 1 Introduction

Transformers have been at the forefront of recent successes across various fields including natural language processing . To obtain insights into their impressive sequential modeling capabilities, a notable emerging theme among several recent works is to model the input data as a Markov process.

Using this Markovian perspective, works such as , among others, study the in-context learning capabilities of transformer.  analyzes the loss-landscape for the next-token prediction task, while  shows an equivalence between the attention mechanism and Markov models. Although these works reveal interesting insights about transformers and their capabilities, many fundamental questions about their learning dynamics remain unanswered. In particular, a comprehensive characterization of their training dynamics vis-a-vis the data distributional properties and the role of initialization is still missing.

To address this gap, in this paper, we focus on the canonical setting of first-order Markov chains and single-layer transformers and analyze the learning dynamics in this context. Specifically, we prove (Thms. 2, 3, and 8) that the input data properties and the parameter initialization play a significant role in the convergence of the transformer parameters to either local or global minima on the loss surface. Further, we precisely characterize (Figs. 1 and Fig. 2) the specific data characteristics and the region of initialization under which this convergence occurs. Based on these insights, we provide guidelinesfor the initialization of transformer parameters and empirically corroborate our theoretical findings. On the theoretical front, our analysis provides a novel gradient flow analysis of the transformer parameters, capitalizing on their low-rank structure during training. Our main contributions can be summarized as follows:

* _Theoretical analysis:_ We precisely characterize the loss landscape and gradient flow dynamics for single-layer transformers with first-order Markov chains (Secs. 3 and 4). We demonstrate that transformer parameters trained on next-token prediction loss can converge to global or local minima, depending on the initialization and the Markovian data properties, and determine the exact conditions under which this occurs (Thms. 2, 3, and 8). To the best of our knowledge, this is the first result of its kind.
* _Insights into initialization:_ Our theoretical analysis underscores the crucial role of initialization in transformer parameter training. Specifically, we demonstrate how the standard Gaussian initialization scheme can lead the convergence to local or global minima depending on the Markovian data properties (Thms. 2 and 8, Figs. 1 and 2).
* _Guidelines:_ Based on these insights, we provide practical guidelines for parameter initialization, corroborated by empirical evidence demonstrating their effectiveness (Sec. 5.2).

**Notation.** We denote scalars by italic lower case letters like \(x,y\) and Euclidean vectors and matrices in bold: \(,,\), etc. \(\|\|\) denotes the \(_{2}\)-norm for Euclidean vectors and Frobenius norm for matrices. \([k]\{1,,k\}\), and for a sequence \((x_{n})_{n 1}\), define \(x_{k}^{m}(x_{k},,x_{m})\) if \(k 1\) and \((x_{1},,x_{m})\) otherwise. For \(z\), the sigmoid \((z) 1/(1+e^{-z})\), \((z)(0,z)\) and the convex logistic loss \(_{}(z)(1+(-z))(0,)\). For events \(A\) and \(B\), \((A)\) denotes the probability of \(A\) whereas \((A B)\) the conditional probability. Let \((x,y)\) be a pair of discrete random variables on \([k][k]\) with the probability mass function (pmf) of \(x\) being \(_{x}=(p_{1},,p_{k})^{k}\). Then its Shannon entropy is defined as \(H(x)=H(_{x})-_{i[k]}p_{i} p_{i}\). The conditional entropy is defined to be \(H(y|x) H(x,y)-H(x)\). The entropy rate of a stochastic process \((x_{n})_{n 1}\) is defined as \(_{n}H(x_{n}^{n})/n\). We simply write \(x=y\) to mean \((x=y)=1\). We also use the shorthand \((y=j x)\) for \((y=j x=x)\) as a function of the random variable \(x\). For \(p(0,1)\), the binary entropy function \(h()\) is defined as \(h(p)-p p-(1-p)(1-p)\).

## 2 Problem Setting

We formally define the problem setting for analysis of single-layer transformers with Markovian data, following .

**Input data.** We assume that the input word sequence \(\{x_{n}\}_{n=1}^{N}\{0,1\}^{N}\) is a first-order time-homogenous Markov chain with a fixed kernel \(=(_{ij})\). That is, the transition probability

Figure 1: Gradient flow dynamics and initialization effect for single-layer transformers. \((p,q)\) are Markov switching probabilities, and \((e,w)\) are the embedding and weight parameters (Sec. 2). (a), (c): The flow is aligned along energy contour lines, converging to local or global optima. (b), (d): \(_{*}\) is the basin of convergence for global minima, \(_{}\) for the local minima, and yellow asymptotes for the saddle point. Notice the contrasting behavior for Gaussian initialization around origin for \(p+q 1\).

\(_{ij}(x_{n+1}=j x_{n}=i)=(x _{n+1}=j x_{n}=i,\,x_{n}^{n-1})\), for any \(x_{n}^{n-1},i,j\{0,1\}\). In particular, we consider \(=[1-p,p;\,q,1-q]\) where \(p=_{01}=(x_{n+1}=1 x_{n}=0)\) and \(q=_{10}=(x_{n+1}=0 x_{n}=1)\) denote the switching probabilities from the states \(0\) and \(1\) respectively. We call \(p+q\), the _switching factor_. We assume that the process is already mixed, i.e. \(x_{n}\) for all \(n\), where \((_{0},_{1})=(q,p)/(p+q)\) is the stationary distribution satisfying \(=\). Succinctly, \((x_{n})_{n 1}(,)\). For this process, the entropy rate, \(H(x_{n+1}|x_{n})=(q\,h(p)+p\,h(q))\), and the entropy of the marginal, \(H(x_{n})=H()\), are both constant in \(n\).

**Transformer architecture.** We consider a single-layer transformer with a single-head attention and ReLU non-linearity. Given an input sequence \(\{x_{n}\}_{n=1}^{N}\), it performs the following mathematical operations at each \(n[N]\) to predict the next-token probability \(f_{}(x_{1}^{n})\):

\[x_{n}\{0,1\}}_{n}}_{n}}_{n}}_{n} }f_{}(x_{1}^{n}),\]

where

\[_{n} =x_{n}\,+_{n}^{d},\] (Embedding) \[_{n} =_{n}+_{i[n]}*{}_{n,i} _{V}\,_{i}^{d},\] (Attention) \[_{n} =_{n}+_{2}(_{1}\,_{n}) ^{d},\] (Feed-forward) \[_{n} =,_{n}+b ^{}\] (Linear) \[f_{}(x_{1}^{n}) _{}(x_{n+1}=1 x_{1}^{n} )=(_{n}).\] (Prediction)

Here \((,\{_{n}\}_{n=1}^{N},,_{1}, _{2},b,)^{D}\) denotes the full list of the transformer parameters from the embedding layer till the linear layer (SS A details them). While the underlying data \(\{x_{n}\}_{n=1}^{N}\) is Markovian, i.e. \((x_{n+1}=1 x_{1}^{n})=(x_{n+1}=1 x _{n})\), the transformer is agnostic to this fact and it can potentially utilize the full past \(x_{1}^{n}\) in the Attention layer, via the attention weights \(_{n,i}\), to predict the next-symbol probability \(f_{}(x_{1}^{n})=_{}(x_{n+1}=1 x_{1}^{ n})\). Note that it suffices to estimate the symbol \(1\) probability as the vocabulary is binary. We also refer to the above architecture as "full model".

**Loss and training.** The transformer parameters \(\) are usually initialized according to standard Gaussian distribution \((0,^{2})\) with a small variance \(^{2}\) and are trained using gradient-based methods to minimize the cross-entropy loss on the next-token prediction, i.e.

\[_{}L(), L()- _{n[N]}_{x_{1}^{n+1}}[x_{n+1} f_{}(x_{1}^{n })\,+(1-x_{n+1})(1-f_{}(x_{1}^{n}))].\] (1)

When the input sequence \(\{x_{n}\}_{n=1}^{N}(,)\), the minimal loss equals its entropy-rate, i.e. \(L_{}_{}L()=H(x_{n+1}|x_{n})\).

**Loss landscape.** A key surprising observation in  is that the loss function \(L()\) admits both the global and local minima depending on the switching factor \(p+q\) of the Markovian data, and the weight-tying of the embedding and linear weights (\(=\)) of the transformer. In particular, they show that

1. for all \((p,q)(0,1)^{2}\), there exists a global minimum \(_{}\) for the loss \(L\) such that its prediction matches the Markov kernel, i.e. \(_{_{}}(x_{n+1}=1 x_{1}^{n})= (x_{n+1}=1 x_{n})\).
2. if \(p+q>1\) and the weights are tied (\(=\)), there exists a bad local minimum \(_{}\) for \(L\) whose prediction equals the marginal, i.e. \(_{_{}}(x_{n+1}=1 x_{1}^{n})= (x_{n+1}=1)\).

In view of these results, we focus on the weight-tying scenario and hence let \(=\) to be a single parameter in \(^{d}\). Thus, \(=(=,\{_{n}\}_{n=1}^{N},,_{1},_{2},b)\). We interchangeably refer to \(\) as both the transformer and the set of parameters.

**Our objective.** While the aforementioned results detail the static landscape of the loss, they do not characterize the learning dynamics on the loss surface and the effect of initialization, which plays a central role in training machine learning models . In view of these shortcomings, the main objective of this paper is to address the following question:

1. _Can we explain how the initialization and learning dynamics affect the convergence of the transformer parameters_ \(\) _to the local or global optima?_

## 3 Canonical Low-rank Parameterization

**Motivation.** Given the complexity of the transformer architecture and the non-convex loss function, it is challenging to analyze the learning dynamics directly [24; 14]. To tackle this, we capitalize on the following empirical observation  which is the motivating idea behind our approach: when trained by gradient-based methods, the weight matrices \((_{V},,_{1},_{2})\) at the optima \(_{}\) and \(_{}\) exhibit _rank-one_ structure, whose eigenvector is the same direction in which the both the token embedding \(\) and the positional embeddings \(_{n}\) are all aligned in. Interestingly, such low-rank solutions can also be shown to be theoretically optimal (SS B details these structures). While these observations illustrate the implicit bias towards low-rank solutions at the final convergence, a natural question arises: _if we initialize with low-rank parameters, will they remain low-rank during training?_ In Sec. 5.1, we affirmatively address this based on a thorough empirical evaluation for single-layer transformers and inspired by these empirical phenomena, without loss of generality, we restrict our attention to these low-rank manifolds to characterize the learning dynamics. This is similar in spirit to , where they assume special attention matrix structure for learning induction heads.

**Parameterization.** More specifically, we consider a special low-rank parameterization that is empirically observed and capitalize on it to address _(Q.1)_. Interestingly, along this low-rank manifold, it suffices to consider a reduced set of parameters \(^{2}\) or \(^{3}\) given by:

\[=(e,w)^{2},=(e,w,a) ^{3}.\] (Reparameterization)

Here \(e\) denotes the _embedding_ scalar, \(w\) the _weight_, and \(a\) the _attention_ parameter respectively. Now we describe the parameterization of the transformer vis-a-vis these scalars and refer to SS C for a more detailed descripton. Let the input \(\{x_{n}\}_{n=1}^{N}\) be a first-order Markov chain as in Sec. 2 and let \(n[N]\) be fixed. Then we have

\[:=e, _{n}=(-) _{n}=e(x_{n}-), e,\{ 1\}^{d}/,\] \[:_{V}=\, ^{}_{n}=e(x_{n}- )+,}_{ a 0}(_{i[n]}_{n,i}  e(x_{i}-)),^{d}.\]

The scalar \(a\) is the product of \(,\) and the scaling in the attention weights \(_{n,i}\) (Eq. (39)), which is empirically close to zero for first-order Markov chains. Hence for the ease of exposition, we first let \(a=0\) and analyze the general case when \(a\) in Sec. 4.1. We continue:

\[:_{1}=} \,\,^{},_{2}=}\,\,^{}_{n}=e(x_{n}-)(1+4w|w|x_{n}),w .\]

\(\) is the all-one vector in \(^{r}\) with \(r=4d\) typically in practice. Substituting this \(_{n}\) in the linear layer with \(=\) and bias \(b\), the logits and the probabilities simplify to:

\[:_{n}(e,w,b) =e^{2}(1+2w|w|)\,x_{n}+b-}{2},\] (2) \[:f_{(,b)}(x_{1}^{n}) =(_{n})(0,1), (e,w).\] (3)

Finally, using the equivalence between the cross-entropy loss and the logistic loss \(_{}()\), the loss function in Eq. (1) can be compactly written as (Lemma 6):

\[L(,b)=_{n[N]}[ _{}((2x_{n+1}-1)_{n}())],^{2},b.\] (4)

Due to convexity of \(_{}()\), it follows that \(L(,b)\) is convex in the bias \(b\) for any fixed \(\), whose minimizer, \(b_{}()=_{b}L( ,b)\), has a closed form expression (Lemma 5). Hence, without loss of generality, we consider the loss with this optimal bias \(b_{}\):

\[L() L(,b_{})= _{n[N]}[_{}((2x_{n+1}-1)(e^{2}(1+2w|w| )\,x_{n}+b_{}-}{2}))].\] (5)Empirically, this roughly translates to running the gradient-based algorithm for the bias for more steps at each \(\). In practice, one additional step is usually sufficient (see Sec. 5). Eq. (5) resembles the standard logistic regression loss  whose binary labels are \(2x_{n+1}-1\{ 1\}\) and the logits given by \(e^{2}(1+2w|w)\,x_{n}+b_{}-e^{2}/2\), for each \(n[N]\). The key difference here is that the logits are a non-linear function of the parameters \((e,w)\) unlike in the standard setting.

We briefly summarize our assumptions below.

**Assumption 1** (Canonical parameterization).: For our theoretical analysis, we assume that the effective transformer parameters are canonically parameterized as \(=(e,w,a)^{3}\). First we study the scenario when \(a=0\) with \(=(e,w)\) and build upon these observations to study the general setting of \(=(e,w,a)\) in Sec. 4.1.

### Loss Landscape with Canonical Parameterization

With the new set of parameters \(=(e,w)^{2}\), we are now ready to analyze the loss \(L()\) in Eq. (5). First we recall the definition of a critical point . A point \(_{}^{2}\) is a critical or a stationary point for \(L\) if \( L(_{})=0\). A critical point \(_{}\) is a _local minimum_ if there exists a neighborhood \(U\) around \(_{}\) such that \(L(_{}) L()\) for all \( U\), and a _local maximum_ if \(L(_{}) L()\). If the neighborhood \(U\) is whole of \(^{2}\), it is a _global minimum/maximum_. On the other hand, a critical point is a saddle point if for all neighborhoods \(U\) around \(_{}\), there are \(_{1},_{2} U\) such that \(L(_{1}) L(_{}) L(_{2})\).

Thm. 1 below provides a complete characterization of the loss landscape in terms of the aforementioned critical points.

**Theorem 1** (All critical points).: _Let the input sequence be \(\{x_{n}\}_{n=1}^{N}(,)\), the transformer parameters \(=(e,w)^{2}\), and the next-token prediction loss \(L()\) be as in Eq. (5). Then for any \((p,q)(0,1)^{2}\) with \(p+q 1\) and \(N\),_

1. _the set of all global minima is given by_ \[_{}(p,q)\{(e,w)^{2}:e^{2}(1+2w|w |)=\},\] (6)
2. _the set of all local minima is given by_ \[_{}(p,q)\{(e,w)^{2}:e=0,\,(p+q-1) (1+2w|w|)>0\},\] (7)
3. _the set of all local maxima is given by_ \[_{}(p,q)\{(e,w)^{2}:e=0,\,(p+q-1) (1+2w|w|)<0\},\] (8)
4. _and the set of all saddle points is_ \[_{}(p,q)\{(0,-1/)\}.\] (9)

_Thus the set of all critical points is_

\[\{^{2}: L()=0\}=_{}_{}_{}_{ }.\] (10)

_In addition, for any \(_{}_{},_{}_{ }\), \(_{}_{}\), and \(_{}_{}\), the loss values satisfy_

\[H(x_{n+1} x_{n})=L(_{})<L(_{})=L(_ {})=L(_{})=H(x_{n+1}).\]

Proof.: We refer to SS E. 

Fig. 1 illustrates the loci of these critical points for \(p+q<1\) and \(p+q>1\). Motivated by empirical observations, while  characterizes local minima for \(p+q>1\), it is interesting to note that our Thm. 1 shows that local minima also exist for \(p+q<1\) (Eq. (7) and Fig. 2(a)). So why did they find the minima only when \(p+q>1\)? The answer to this, and more broadly to question _(Q.1)_ lies in the learning dynamics and initialization for \(\), which we study in the next section.

Learning Dynamics

Capitalizing on the loss landscape in terms of the critical points in Thm. 1, we now focus on the convergence of gradient-based algorithms to these points. In this regard, we analyze the dynamics of the gradient-flow (GF), which can be viewed as a continuous-time analogue of gradient-descent . The gradient-flow of the parameters, \((_{t})_{t 0}\), on \(L\) is governed by

\[_{t}}{t}=- L(_{t}), _{t}=(e_{t},w_{t})^{2},\,t 0,\] (GF)

where \(_{t}(t)\) is a \(C^{1}\) (continuously differentiable) curve in \(^{2}\) starting with a randomly initalized \(_{0}\). To characterize these trajectories, we define an _energy function_\((,)\), which plays a crucial in the GF dynamics. It is defined as

\[(e,w) e^{2}-(w^{2}+(w)|w|), (e,w)^{2},\] (11)

where \(\{(e,w=0)\}\). Note that \(\) is well-defined and finite for all the points in its domain. On the other hand, \(_{w 0^{-}}(e,w)=-\) whereas \(_{w 0^{+}}(e,w)=\) for any fixed \(e\). Thus the \(\) corresponding to \(w=0\) serves as an energy barrier for the flow. Figs. 2(a) and 2(b) illustrate this by visualizing the energy contour lines. The utility of the energy function is captured in the following lemma.

**Lemma 1** (Constant energy along the flow).: _For any \((p,q)(0,1)^{2}\) and initialization \(_{0}=(e_{0},w_{0})^{2}\), let \((_{t})_{t 0}\) be the corresponding GF trajectory starting from \(_{0}\). If \(_{0}^{2}\), the energy stays constant along the trajectory, i.e._

\[(_{t})=e_{t}^{2}-(w_{t}^{2}+(w_{t}) |w_{t}|)=(_{0}), t 0.\] (12)

_On the other hand, if \(_{0}\), we have that \(_{t}\) for all \(t 0\) with \(w_{t}=w_{0}=0\), i.e. if we initialize on the \(\), the trajectory always stays there._

We are now ready to present the main results of our paper. Specifically, Thm. 2 and Thm. 8 highlight the role of the switching factor of the Markovian data, \(p+q\), and the parameter initialization, \(_{0}\), in deciding whether the GF converges to local optima or global optima. First we define the energy value \(_{}(e=0,w=-1/)=-(1+ 2 )/2\).

**Theorem 2** (GF dynamics for \(p+q>1\)).: _Let \((p,q)(0,1)^{2}\) with \(p+q>1\), the input sequence be \(\{x_{n}\}_{n=1}^{N}(,)\), and \((_{t})_{t 0}\) be the corresponding GF trajectory starting from \(_{0}\). Then for all initializations \(_{0}^{2}\), the gradient flow converges to a critical point of the loss \(L\). That is, there exists a \(_{}^{2}\) such that \(_{t}_{t}=_{}\) and \( L(_{})=0\). In particular, \(_{}\) is a_

1. _a local minimum if_ \[_{0}_{}(e,w):w(-1/,0),\,e(-g(w),g(w)),\,g(w)=-(-w)+ _{}}}\] \[\{(e,w):w 0\}\,,\]
2. _a saddle point if_ \(_{0}_{}(e,w):w[-1/ ,0),\,e=-(-w)+_{}}}\)_,_
3. _a local maximum if_ \(_{0}_{}(e,w):e=0,\,w<- 1/}\)_,_
4. _and a global minimum if_ \(_{0}_{}^{2}( _{}_{}_{ })\)_._

_Consequently, when \(p+q>1\), if we use the standard initialization \(_{0}(0,^{2}_{2})\) with \(^{2} 1/\), \(_{}\) will be a local minimum with high probability. If \(p+q<1\), under the same initialization scheme, \(_{}\) will be a global minimum with high probability._

Proof sketch.: The main idea behind the proof is to show that if we do not initialize on the \(\), the flows stays on the constant energy contour (Lemma 1) and hence converges to a critical point of the loss \(L\), which is at the intersection of the contour line and the set of critical points (Lemmas. 10 and 11). By determining where these intersections occur, the corresponding basins of convergence \(_{},,_{}\) are obtained by showing that an initialization in a specific set leads to the said critical point (Thm. 1). The proof for \(_{0}\) is similar.

Figs. (b)b and (d)d illustrate these initialization sets corresponding to the convergence basins for \(p=q=0.9\) and \(p=q=0.1\) respectively. An analogous result about GF dynamics for \(p+q<1\) is presented in Thm. 8 (SS F.2). Here a key difference is that small Gaussian initialization around origin leads to a global minimum \(_{}\) with high probability (Fig. (d)d).

**Key insights.** Together, Thm. 2 and Thm. 8 address our motivating question _(Q.1)_ by fully characterizing the GF dynamics in terms of initialization and input data properties. Specifically, our results explain the phenomenon in  wherein they observe local minima for \(p+q>1\) more often than for \(p+q<1\), owing to standard Gaussian initialization around origin (Figs. (b)b and (d)d). However, in practice, we often do not know the input switching factor, raising a natural questions: _is there a data-agnostic initialization that always converges to global minima?_ Indeed, as can be seen from Figs. (b)b and (d)d, there is a common region of initialization in the negative half-plane above the saddle-asymptotes (in yellow) that leads to the global minima convergence irrespective of the switching \(p+q\). Mathematically, this region is given by \(_{}\{(e,w):w<0,|e|>-(-w)+ _{}}\}\). We empirically corroborate this fact in Sec. 5.2.

### Gradient Flow with Attention

In this section, the consider the attention scalar \(a\) (Sec. 3) and study the gradient flow dynamics with the parameters \(=(e,w,a)^{3}\). The parameter \(a\) captures the overall scaling from the value, key, and query components in the attention layer. Recall that the soft-max attention weights are given by \(_{n,i}(_{n},_{i}/)\), where \(_{n}=_{Q}_{n}\) and \(_{i}=_{K}_{i}\) are the query and key embeddings for any position \(i[n]\). Using the low-rank structure of the query and key matrices, satisfying \(_{Q}^{}_{K}=(q^{2}d)\,^{}\) and the value matrix \(_{V}=^{}\) for some \(q\) and \(^{d}\) (SS G), and assuming linear attention \(_{n,i}_{n},_{i}/\), we define a single scalar \(a, q^{2}d^{5/2}/4\) that captures the essence of the attention layer (Eq. (39)). We note that linear attention weights are a standard assumption in the transformer analysis literature . Using this parameterization, similar to the steps in Sec. 3, we obtain the final loss function to be

\[L()=[_{}((2Y-1)(e^{2}[( X-)(1+ae^{2})(1+2w|w|)+w|w(1+ae^{2})]]+b_{ }))],\]

where \(=(e,w,a)\) and \(b_{}\) is the corresponding optimal bias. \(L\) recovers the loss in Eq. (5) when \(a=0\). In Thm. 10, we determine the set of all critical points of \(L\) in terms of global minima and local optima in closed-form expressions, analogous to Thm. 1. Capitalizing on this characterization, we now shift our focus to the analysis of the gradient flow in \(^{3}\). To this end, let \((_{t})_{t 0}\) be a \(C^{1}\) curve in \(^{3}\) governed by

\[_{t}}{t}=- L(_{t}), _{t}=(e_{t},w_{t},a_{t})^{3},\,t 0,\] (GF-attn)

starting with a randomly initalized \(_{0}\). We define the _energy function_\((,,)\) as

\[(e,w,a) e^{2}-(w^{2}+(w)|w|)-2a^{2 },(e,w,a)^{3},\] (13)

Figure 2: Gradient flow dynamics for the canonical parameters \(=(e,w,a)^{3}\) with the attention scalar \(a\). Notice the contrasting behavior for Gaussian initialization around origin for \(p+q\) smaller and greater than one. For an enhanced view of the flow near the origin, please refer to Fig. 5.

where \(\{(e,w=0,a)\}\). It is similar to its counterpart in Eq. (11), except for the \(2a^{2}\) term. Fig. 2 visualizes this energy surface and the set of critical points, which reveal close resemblance to that of Fig. 1 in \(^{2}\). Capitalizing on the energy function, we now present our main result with the attention.

**Theorem 3** (GF dynamics with attention).: _For any \((p,q)(0,1)^{2}\) and initialization \(_{0}^{3}\), let \((_{t})_{t 0}\) be the corresponding GF-attn trajectory starting from it. Then for all \(_{0}^{3}\), the gradient flow converges to a critical point of the loss \(L\). That is, there exists a \(_{}^{3}\) such that \(_{t}_{t}=_{}\) and \( L(_{})=0\). Further,_

1. _[label=_()_]_
2. _if_ \(_{0}^{3}\)_, we have_ \((_{})=(_{t})= (_{0})\) _for all_ \(t 0\)_. Hence_ \(_{}\) _is at the intersection of the energy contour line_ \(=_{0}\) _with that of the set of critical points._
3. _if_ \(_{0}\)_, we have_ \(_{t}\) _for all_ \(t 0\) _and hence_ \(_{}\)_._

Proof.: We refer to SS G and SS N.4. 

Thm. 3 shows that the learning dynamics with attention closely resemble those without it (Thms. 2 and 8). While the set of all critical points of \(L\), and thus the limit points of the flow, has a closed-form expression (Thm. 10), deriving the same for the initialization sets \(_{}\) and \(_{}\) to determine the basin of convergence is technically challenging (see discussion in SS G). Nonetheless, empirical observations with the standard Gaussian initialization around origin reveal a similar picture as in the two-dimensional setting for both the \(p+q<1\) and \(p+q>1\) cases (Fig. 2). We believe it's an interesting direction of future research to theoretically characterize this, analogous to Thms. 2 and 8. We refer to SS G for additional details and proofs.

## 5 Empirical Results

We empirically validate our canonical parameterization \(^{3}\) (Sec. 3) by demonstrating full model convergence to low-rank parameters through both qualitative and quantitative evidence. Qualitatively, we visualize weight matrices across iterations; quantitatively, we plot the percentage of energy captured by the top-rank components across iterations. We then demonstrate the generalization of our theoretical findings on local optima and initialization with canonical parameters to the full model \(^{D}\). We conclude with a discussion on higher-order and multi-state Markov chains.

### Low-rank Parameters

**Full model converges to low-rank.** We let the input Markov sequence to be \(\{x_{n}\}_{n=1}^{N}((p,q),(p,q))\) for \(p=0.2,q=0.3,N=1024\) and consider the full model as defined in Sec. 2 with embedding dimension \(d=8\). First, we initialize the parameters \(=(=,\}}_{n=1}^{N},,_{1},_{2 },b)\) using the standard Gaussian initialization with standard deviation \(0.001\) and train them using SGD on a batch size \(B=16\) and for \(t=800\) iterations. In Fig. 6, we track the value matrix \(_{V}^{d d}\) and the weight matrix \(_{1}^{d4 d}\) across iterations. We observe that at convergence both \(_{V}\) and \(_{1}\) are approximately rank-one with one of their components being same as the embedding vector (the row in \(_{V}\) and column in \(_{1}\)). Further, the embedding vector has all entries in \(\{ 1\}\) up to a scaling. We observe the same conclusion for other weight matrices \(_{K,Q},_{2}\) and for all values of \((p,q)(0,1)^{2}\). Fig. 3 also quantitatively demonstrates this.

**Full model initialized at low-rank remains low-rank during training.** Inspired by the low-rank structure obtained above, we randomly initialize the weight parameters as rank-one matrices and the embeddings on the hypercube \(\{ 1\}^{d}\). After the initialization, we train them without any low-rank restrictions, and track them during the course of training. Interestingly, here we observe that the parameters still stay low-rank as illustrated in Fig. 7 and Fig. 3. A similar conclusion holds for the remaining weight matrices. Together these results provide the empirical basis for our canonical parameterization analysis in Sec. 3.

### Effect of Initialization: Broader Implications

Now we investigate the findings of Sec. 3 and Sec. 4, derived for the canonical low-rank model, more broadly in the context of full model in Sec. 2. In particular, as shown in Thm. 2 and Fig. 1dfor \(p+q>1\), any small initialization around zero would lead a local minima convergence. To test this hypothesis, we compare the standard initialization where all the transformer parameters \(=(=,\{_{n}\}_{n=1}^{N},,_{1},_{2},b)\) are randomly chosen around zero with small variance \(=0.02\), with a new initialization based on our results, where we initialize the embedding vector \(\) such that all ordinates are equal to \(e=0.5\), \(_{1}\) to be constant with the scalar \(w_{1}=1\) and \(_{2}\) constant with \(w_{2}=-1\) (corresponding to \(_{}\) in Fig. 1d). We indeed observe that the final test loss matches the unigram loss for the standard initialization, while it converges to the optimal bigram loss for our initialization (see Fig. 4). Together these results indicate that though our analysis used canonical parameterization, the corresponding insights are more general and apply more broadly to the general full model. In a similar spirit, analysis of initialization effects for deeper architectures is an interesting avenue of future research.

### Higher-Order and Multi-State Markov Chains

While the primary focus of this paper has been on binary first-order Markov chains, we believe it's possible to extend our analysis to both multi-state and higher-order settings. On the multi-state front, akin to the binary case,  already demonstrates the effect of switching probability and weight-tying on the final model convergence. Here, first characterizing the loss landscape and then the associated learning dynamics in line with our approach is an interesting direction. On the other hand, a recent work  establishes a surprising result that any \(k^{}\)-order Markov chain can be represented by a

Figure 4: Comparison between the average loss curve for the standard gaussian initialization around \(0\) and our initialization, for \(p=0.5\) and \(q=0.8\). Starting from the standard initialization, the model converges to a local minimum corresponding to the unigram model. With our initialization, it converges to the global minimum corresponding to the bigram model.

Figure 3: Convergence to rank one parameters: percentage of energy contained in the first rank component of the weight matrices \(_{1}\) and \(_{V}\) across iterations. The percentage is computed as \(^{2}}{_{i}_{i}^{2}}\), where the \(_{i}\)â€™s are the singular values of the matrices in descending order.

three layer transformer with just one head per layer, relying on induction head mechanism. Analyzing gradient flow dynamics using appropriate canonical parameterization (cf. ) in this scenario is also a fruitful direction of research.

## 6 Related Works

The recent success of transformer models in deep learning has sparked significant interest and active research in understanding them [38; 25; 16; 27; 15; 37; 40; 32]. In relation to our paper, they can be broadly classified into two topics: (i) **In-context learning (ICL):** ICL refers to the ability of transformers learn and reason from information present in their context [10; 13; 4; 35; 39; 7; 21; 17]. Along this thread, the works most relevant to ours are [8; 14; 24], which use Markovian input data to understand the ICL mechanism. [8; 14] heuristically show how gradient-based updates can learn an induction-head mechanism using a simplified transformer architecture with frozen encodings, query matrices and linear activations. On the other hand, we consider the canonical parameterization, capitalizing on inherent low-rank parameters, to provide a full characterization of the learning dynamics.  demonstrates how two-layer transformers with GD learn induction head mechanism when the input has a causal tree dependency, such as in Markov chains. In this work, we focus on the GF dynamics for single-layer transformers and show how they can also converge to local optima, further highlighting the role of initialization. (ii) **Training dynamics:** On the other hand, numerous works have investigated the training dynamics of transformers. For instance,  examines the gradient flow in a simplified single-layer transformer, while  studies the process by which self-attention integrates input tokens, assuming the decoder learns faster than the attention layer. Unlike these settings, our focus is on understanding the training dynamics of the full transformer model end-to-end. Other related works include , which analyzes gradient dynamics in LSTM Seq2seq models, , which shows how Vision Transformers learn spatial structures, and , which demonstrates that a single-layer transformer can learn a constrained topic model. A closely related work is , which shows that self-attention has a Markovian structure, but our focus is on self-attention's capability in modeling Markov chains and the associated training dynamics.

## 7 Conclusion

In this work, we present a novel characterization of gradient flow dynamics for (weight-tied) single-layer transformers with first-order Markov chains. Specifically, we highlight the significant role of the parameter initialization and inherent properties of the Markovian data in determining the parameter convergence to either global minima or local optima. Drawing upon these insights, we offer practical guidelines for parameter initialization, corroborated by empirical results demonstrating their effectiveness. While our current analysis is limited to single-layer models, uncovering similar results with gradient flow analysis for deeper architectures and higher order Markov chains is open and an interesting avenue for future research.