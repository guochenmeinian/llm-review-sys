# Decentralized Randomly Distributed Multi-agent Multi-armed Bandit with Heterogeneous Rewards

Mengfan Xu\({}^{1}\) Diego Klabjan\({}^{1}\)

\({}^{1}\)Department of Industrial Engineering and Management Sciences, Northwestern University

MengfanXu2023@u.northwestern.edu, d-klabjan@northwestern.edu

###### Abstract

We study a decentralized multi-agent multi-armed bandit problem in which multiple clients are connected by time dependent random graphs provided by an environment. The reward distributions of each arm vary across clients and rewards are generated independently over time by an environment based on distributions that include both sub-exponential and sub-Gaussian distributions. Each client pulls an arm and communicates with neighbors based on the graph provided by the environment. The goal is to minimize the overall regret of the entire system through collaborations. To this end, we introduce a novel algorithmic framework, which first provides robust simulation methods for generating random graphs using rapidly mixing Markov chains or the random graph model, and then combines an averaging-based consensus approach with a newly proposed weighting technique and the upper confidence bound to deliver a UCB-type solution. Our algorithms account for the randomness in the graphs, removing the conventional doubly stochasticity assumption, and only require the knowledge of the number of clients at initialization. We derive optimal instance-dependent regret upper bounds of order \( T\) in both sub-Gaussian and sub-exponential environments, and a nearly optimal mean-gap independent regret upper bound of order \( T\) up to a \( T\) factor. Importantly, our regret bounds hold with high probability and capture graph randomness, whereas prior works consider expected regret under assumptions and require more stringent reward distributions.

## 1 Introduction

Multi-armed Bandit (MAB)  is an online sequential decision-making process that balances exploration and exploitation while given partial information. In this process, a single player (agent, client) aims to maximize a cumulative reward or, equivalently, minimize the cumulative loss, known as regret, by pulling an arm and observing the reward of that arm at each time step. The two variants of MAB are adversarial and stochastic MAB, depending on whether rewards are chosen arbitrarily or follow a time-invariant distribution, respectively. Recently, motivated by the development of federated learning , multi-agent stochastic multi-armed bandit has been drawing increasing attention (commonly referred to as multi-agent MAB). In this variant, multiple clients collaboratively work with multiple stochastic MABs to maximize the overall performance of the entire system. Likewise, regret is an important performance measure, which is the difference between the cumulative reward of always pulling the global optimal arm by all clients and the actual cumulative reward gained by the clients at the end of the game, where global optimality is defined with respect to the average expected reward values of arms across clients. Thereafter, the question for each client to answer is essentially how to guarantee an optimal regret with limited observations of arms and insufficient information of other clients. Assuming the existence of a central server, also known as the controller, , allow a controller-client framework where the controller integrates and distributes the inputs from and to clients, adequately addressing the challengeposed by the lack of information of other clients. However, this centralization implicitly requires all clients to communicate with one another through the central server and may fail to include common networks with graph structures where clients perform only pair-wise communications within the neighborhoods on the given graphs. Non-complete graphs capture the reality of failed communication links. Removing the centralization assumption leads to a decentralized multi-agent MAB problem, which is a challenging but attracting direction as it connects the bandit problem and graph theory, and precludes traditional centralized processing.

In the field of decentralized multi-agent MAB, it is commonly assumed that the mean reward value of an arm for different clients is the same, or equivalently, homogeneous. This assumption is encountered in (Landgren et al., 2016, 2021; Zhu et al., 2020; Martinez-Rubio et al., 2019; Agarwal et al., 2022; Wang et al., 2022; Li and Song, 2022; Sankararaman et al., 2019; Chawla et al., 2020). However, this assumption may not always hold in practical scenarios. In recent years, there has been an increasing emphasis on heterogeneous reward settings, where clients can retain different mean values for the rewards of the same arm. The transition to heterogeneous reward settings presents additional technical challenges. Clients are unable to infer the global optimal arms without sequential communications regarding the rewards of the same arm at other clients. Such communications, however, are limited by the partially observed rewards, as other clients may not pull the same arm, and constrained by the underlying graph structure. We study the heterogeneous setting with time varying graphs.

Traditionally, rewards are assumed to be sub-Gaussian distributed. However, there has been a recent focus on MAB with heavy-tailed reward distributions. This presents a non-trivial challenge as it is harder to concentrate reward observations in sublinear time compared to the light-tailed counterpart (Tao et al., 2022). In the work of (Jia et al., 2021), sub-exponential rewards are considered and analyzed in the single-agent MAB setting with newly proposed upper confidence bounds. Meanwhile, for multi-agent MAB, heavy-tailed distributions are examined in a homogeneous setting in (Dubey et al., 2020). However, the heterogeneous setting studied herein has not yet been formulated or analyzed, posing more challenges compared to the homogeneous setting, as discussed earlier.

Besides rewards, the underlying graph assumptions are essential to the decentralized multi-agent MAB problem, as increased communication among clients leads to better identification of global optimal arms and smaller regret. The existing works (Sankararaman et al., 2019; Chawla et al., 2020) relate regret with graph structures and characterize the dependency of regret on the graph complexity with respect to conductance. When considering two special cases, (Chawla et al., 2020) demonstrates the theoretical improvement achieved by circular ring graphs compared to complete graphs, and (Li and Song, 2022) numerically shows that the circular ring graph presents the most challenging scenario with the largest regret, while the complete graph is the simplest. There are two types of graphs from a time perspective: time-invariant graphs, which remain constant over time, and time-varying graphs, which depend on time steps and are more challenging but more general. Assumptions on time-invariant graphs include complete graphs (Wang et al., 2021) where all clients can communicate, regular graphs (Jiang and Cheng, 2023) where each client has the same number of neighbors, and connected graphs under the doubly stochasticity assumption (Zhu et al., 2020, 2021, 2021, 2021). Independently from our work, recent work (Zhu and Liu, 2023) has focused on time-varying \(B\)-connected graphs, where the composition of every \(l\) consecutive graphs is a strongly connected graph. However, their doubly stochasticity assumption, where all elements of edge probability also called weight matrices are uniformly bounded by a positive constant, can be violated in several cases. Additionally, their graphs may be strongly correlated to meet the connectivity condition when \(l>1\), which may not always hold in practice. No research has been conducted on time-varying graphs with only connectivity constraints or without constraints on connectivity. Additionally, current time-varying graphs do not provide insight into how the graphs change over time. As the graphs are generated by the environment, similar to the generation of rewards, it remained unexplored considering random graphs in an i.i.d manner, such as random edge failures or random realizations as pointed out for future research in (Martinez-Rubio et al., 2019). We also address this situation.

Traditionally, random graphs have often been formulated using the Erdos-Renyi (E-R) model, which has been widely adopted in various research domains. The model, described by \(G(M,c)\), consists of \(M\) vertices with each pair of vertices being connected with probability \(c\). Notably, the E-R model is 1) not necessarily connected and 2) stochastic that allows for random edge failures, and has found applications in mean-field game (Delarue, 2017) and majority vote settings (Lima et al., 2008). Though it has only been used in numerical experiments for the decentralized multi-agent MABsetting with homogeneous rewards (Dubey et al., 2020), the theoretical study of this model in this context remained unexplored until this work, let alone with heterogeneous rewards and time-varying graphs. Alternatively, one can consider all connected graphs (there are exponentially many of them), and the environment can randomly sample a connected graph and produce i.i.d. samples of such random connected graphs. This approach mimics the behavior of stochastic rewards and allows the environment to exhaust the sample space of connected graphs independently, without the doubly stochasticity assumption, which, however, has not yet been studied and it is also addressed herein.

For the multi-agent MAB framework, methods in MAB are a natural extension. (Zhu et al., 2021) adapt the UCB algorithm to the multi-agent setting. This algorithm uses weighted averages to achieve consensus among clients and is shown to have a regret of order \( T\) for time-invariant graphs. A follow-up study in (Zhu and Liu, 2023) re-analyzes this algorithm for time-varying \(B\)-connected graphs under the aforementioned assumptions under the doubly stochasticity assumption by adding an additional term compared to UCB. An effective UCB-based method for random graphs without doubly stochasticity assumption and for sub-exponential distributed rewards remained unexplored.

This paper presents a novel contribution to the decentralized multi-agent MAB problem by studying both heterogeneous rewards and time-varying random graphs, where the distributions of rewards and graphs are independent of time. To the best of our knowledge, this is the first work to consider this problem and to investigate it with heavy-tailed reward distributions. Specifically, the paper investigates 1) heterogeneous sub-exponential and sub-Gaussian distributed rewards and 2) random graphs including the possibly disconnected E-R model and random connected graphs, and applies them to the decentralized multi-agent MAB framework. This work bridges the gap between large-deviation theories for sub-exponential distributions and multi-agent MAB with heterogeneous rewards, and the gap between random graphs and decentralized multi-agent MAB.

To this end, we propose a brand new algorithmic framework consisting of three main components: graph generation, DrFed-UCB: burn-in period, and DrFed-UCB: learning period. For the learning period, we modify the algorithm by (Zhu et al., 2021) by introducing new UCB quantities that are consistent with the conventional UCB algorithm and generalize to sub-exponential settings. We also introduce a newly proposed stopping time and a new weight matrix without the doubly stochasticity assumption to leverage more information in random graphs. A burn-in period is crucial in estimating the graph distribution and initializing the weight matrix. We embed and analyze techniques from random graphs since the number of connected graphs is exponentially large in the number of vertices, and directly sampling such a graph is an NP-hard problem. In particular, we use the Metropolis-Hastings method with rapidly mixing Markov chains, as proposed in (Gray et al., 2019), to approximately generate random connected graphs in polynomial time. We additionally demonstrate its theoretical convergence rate, making it feasible to consider random connected graphs in the era of large-scale inference.

We present comprehensive analyses of the regret of the proposed algorithm, using the same regret definition as in existing literature. Firstly, we show that algorithm DrFed-UCB achieves optimal instance-dependent regret upper bounds of order \( T\) with high probability, in both sub-Gaussian and sub-exponential settings, consistent with prior works. We add that although both (Zhu et al., 2020) and our analyses use the UCB framework, the important algorithmic steps are different and thus also the analyses. Secondly, we demonstrate that with high probability, the regret is universally upper bounded by \(O( T)\) in sub-exponential settings, including sub-Gaussian settings. This upper bound matches the upper and lower bounds in single-agent settings up to a \( T\) factor, establishing its tightness.

The paper is organized as follows. We first introduce the notations used throughout the paper, present the problem formulation, and propose algorithms for solving the problem. Following that, we provide theoretical results on the regret of the proposed algorithm in various settings.

## 2 Problem Formulation and Methodologies

### Problem Formulation

Throughout, we consider a decentralized system with \(M\) clients that are labeled as nodes \(1,2,,M\) on a time-varying network, which is described by an undirected graph \(G_{t}\) for \(1 t T\) where parameter \(T\) denotes the time horizon of the problem. Formally, at time step \(t\), \(G_{t}=(V,E_{t})\) where \(V=\{1,2,,M\}\) and \(E_{t}\) denotes the edge set consisting of pair-wise nodes and representing the neighborhood information in \(G_{t}\). The neighbor set \(_{m}(t)\) include all neighbors of client \(m\) based on \(G_{t}\). Equivalently, the graph \(G_{t}\) can be represented by the adjacency matrix \((X^{t}_{i,j})_{1 i,j M}\) where the element \(X^{t}_{i,j}=1\) if there is an edge between clients \(i\) and \(j\) and \(X^{t}_{i,j}=0\) otherwise. We let \(X_{i,i}=1\) for any \(1 i M\). With this notation at hand, we define the empirical graph (adjacency matrix) \(P_{t}\) as \(P_{t}=^{t}X^{t}_{i,j})_{1 i,j M}}{t}\). It is worth emphasizing that 1) the matrix \(P_{t}\) is not necessarily doubly stochastic, 2) the matrix captures more information about \(G_{t}\) than the prior works based on \(|_{m}(t)|\), and 3) each client \(m\) only knows the \(m\)-th row of \(P_{t}\) without knowledge of \(G_{t}\), i.e. \(\{P_{t}(m,j)\}_{j}\) are known to client \(m\), while \(\{P_{t}(k,j)\}_{j}\) for \(k m\) are always unknown. Let us denote the set of all possible connected graphs on \(M\) nodes as \(_{}\).

We next consider the bandit problems faced by the clients. In the MAB setting, the environment generates rewards. Likewise, we again use the term, the environment, to represent the source of graphs \(G_{t}\) and rewards \(r^{m}_{i}(t)\) in the decentralized multi-agent MAB setting. Formally, there are \(K\) arms faced by each client. At each time step \(t\), for each client \(1 m M\), let the reward of arm \(1 i K\) be \(r^{m}_{i}(t)\), which is i.i.d. distributed across time with the mean value \(^{m}_{i}\), and is drawn independently across the clients. Here we consider a heterogeneous setting where \(^{m}_{i}\) is not necessarily the same as \(^{j}_{i}\) for \(m j\). At each time step \(t\), client \(m\) pulls an arm \(a^{t}_{m}\), only observes the reward of that arm \(r^{m}_{a^{t}_{m}}(t)\) from the environment, and exchanges information with neighbors in \(G_{t}\) given by the environment. In other words, two clients communicate only when there is an edge between them.

By taking the average over clients as in the existing literature, we define the global reward of arm \(i\) at each time step \(t\) as \(r_{i}(t)=_{m=1}^{M}r^{m}_{i}(t)\) and the subsequent expected value of the global reward as \(_{i}=_{m=1}^{M}^{m}_{i}\). We define the global optimal arm as \(i^{*}=*{arg\,max}_{i}_{i}\) and arm \(i i^{*}\) is called global sub-optimal. Let \(_{i}=_{i^{*}}-_{i}\) be the sub-optimality gap. This enables us to quantify the regret of the action sequence (policy) \(\{a^{t}_{m}\}_{1 m M}^{1 t T}\) as follows. Ideally, clients would like to pull arm \(i^{*}\) if knowledge of \(\{_{i}\}_{i}\) were available. Given the partially observed rewards due to bandits (dimension \(i\)) and limited accesses to information from other clients (dimension \(m\)), the regret is defined as \(R_{T}=T_{i^{*}}-_{t=1}^{T}_{m=1}^{M}^{m}_{a^{t}_{m}}\) which measures the difference of the cumulative expected reward between the global optimal arm and the action sequence. The main objective of this paper is to develop theoretically robust solutions to minimize \(R_{T}\) for clients operating on time-varying random graphs that are vulnerable to random communication failures, which only require knowledge of \(M\).

### Algorithms

In this section, we introduce a new algorithmic framework that incorporates two graph generation algorithms, one for the E-R model and the other for uniformly distributed connected graphs. More importantly, the framework includes a UCB-variant algorithm that runs a learning period after a burn-in period, which is commonly referred to as a warm-up phase in statistical procedures.

#### 2.2.1 Graph Generation

We investigate two types of graph dynamics as follows, for which we propose simulation methods that enable us to generate and analyze the resulting random graphs.

E-R random graphsAt each time step \(t\), the adjacency matrix of graph \(G_{t}\) is generated by the environment by element-wise sampling \(X^{t}_{i,j}\) according to a Bernoulli distribution. Specifically, \(X^{t}_{i,j}\) follows a Bernoulli distribution with parameter \(c\).

Uniformly distributed connected graphsAt each time step \(t\), the random graph \(G_{t}\) is generated by the environment by uniformly sampling a graph from the sample space of all connected graphs \(_{M}\), which yields the adjacency matrix \((X^{t}_{i,j})_{1 i j M}\) corresponding to \(G_{t}\). Generating uniformly distributed connected graphs is presented in Algorithm 3 as in Appendix. It is computationally infeasible to exhaust the sample space \(_{M}\) since the number of connected graphs is exponentially large. To this end, we import the Metropolis-Hastings method in (Gray et al., 2019) and leverage rapidly mixing Markov chains. Remarkably, by adapting the algorithm into our setting which yields a new algorithm, we construct a Markov chain that converges to the target distribution after a finite number of burn-in steps. This essentially traverses graphs in \(_{M}\) through step-wise transitions, with a complexity of \(O(M^{2})\) from previous states. More precisely, at time step \(s\) with connected graph \(G_{s}\), we randomly choose a pair of nodes and check whether it exists in the edge set. If this is the case, we remove the edge from \(G_{s}\) and check whether the remaining graph is connected, and only accept the graph as \(G_{s+1}\) in the connected case. If the edge does not exist in the edge set, we add it to \(G_{s}\) and get \(G_{s+1}\). In this setting, let \(c=c(M)\) be the number of times an edge is present among all connected graphs divided by the total number of connected graphs. It is known that \(c=2\)\((1)\) [Trevisan]. The distribution of \(G_{s}\) eventually converges to the uniform distribution in \(_{M}\). The formal statements are in Appendix.

#### 2.2.2 Main Algorithm

In the following, we present the proposed algorithm, DrFed-UCB, which comprises of a burn-in period and a learning period described in Algorithm 1 and Algorithm 2, respectively.

We start by introducing the variables used in the algorithm with respect to client \(m\). We use \(_{i}^{m}(t),n_{m,i}(t)\) to denote reward estimators and counters based on client \(m\)'s own pulls of arm \(i\), respectively, and use \(_{i}^{m},N_{m,i}(t)\) to denote reward estimators and counters based on the network-wide pulls of arm \(i\), respectively. By network-wide, we refer to the clients in \(_{m}(t)\). Denote the stopping time for the filtration \(\{G_{s}\}_{s=1}^{t}\) as \(h_{m,j}^{t}=max_{s t}\{(m,j) E_{s}\}\); it represents the most recent communication between clients \(m\) and \(j\). The weights for the network-wide and local estimators are \(P_{t}^{}(m,j)\) and \(d_{m,t}\) defined later, respectively.

There are two stages in Algorithm 1 where \(t L\) as follows. For the first \(_{1}\) steps, the environment generates graphs based on one of the aforementioned graph generation algorithms to arrive at the steady state, while client \(m\) pulls arms randomly and updates local estimators \(\{_{i}^{m},n_{i}^{m}\}_{i}\). Afterwards, the environment generates the graph \(G_{t}\) that follows the distribution of interest, while client \(m\) updates \(\{_{i}^{m},n_{i}^{m}\}_{i}\) and row \(m\) of \(P_{t}\) by exchanging information with its neighbors in the graph \(G_{t}\). Note that client \(m\) does not have any global information about \(G_{t}\). At the end of the burn-in period, client \(m\) computes the network-wide estimator \(_{i}^{m}\) by taking the weighted average of local estimators of other clients (including itself), where the weights are given by the \(m\)-th row of weight matrix \(P^{}\) and \(d\), which depend on \(P\) and knowledge of \(M\) and satisfy \(_{j=1}^{M}P_{t}^{}(m,j)+d_{m,t}M=1\).

Subsequently, we describe Algorithm 2 where \(t L+1\). There are four phases in one iteration enumerated below in the order indicated. A flowchart of this algorithm is provided in Appendix.

UcBGiven the estimators \(_{i}^{m}(t),n_{m,i}(t),N_{m,i}(t),_{i}^{m}(t)\), client \(m\) either randomly samples an arm or pulls the arm that maximizes the upper confidence bound using \(_{i}^{m}(t),n_{m,i}(t)\), depending on whether \(n_{m,i}(t) N_{m,i}(t)-K\) holds for some arm \(i\). This additional condition ensures consensus among clients regarding which arm to pull. The upper confidence bound \(F(m,i,t)\) is specified as \(F(m,i,t)= t}{n_{m,i}(t)}}\) and \(F(m,i,t)= T}{n_{m,i}(t)}}+ T}{n_{m,i}(t)}\) in settings with sub-Gaussian and sub-exponential rewards, respectively. Constants \(C_{1}\) and \(C_{2}\) are determined in the analyses and they depend on \(\) which is an upper bound of standard deviations of the reward values (it is formally defined later).

Environment and client interactionAfter client \(m\) pulls arm \(a_{m}^{t}\), the environment sends the reward \(r_{m,a_{m}^{t}}^{t}\) and the neighbor set \(_{m}(t)\) in \(G_{t}\) to client \(m\). Client \(m\) does not know the whole \(G_{t}\) and obtains only the neighbor set \(_{m}(t)\).

TransmissionClient \(m\) sends the maintained local and network-wide estimators \(\{_{i}^{m}(t),_{i}^{m}(t),n_{m,i}(t),N_{m,i}(t)\}_{i}\) to all clients in \(_{m}(t)\) and receives the ones from them.

Update estimatorsAt the end of an iteration, client \(m\) first updates the \(m\)-th row of matrix \(P_{t}\). Subsequently, client \(m\) updates the quantities \(\{_{i}^{m}(t),_{i}^{m}(t),n_{m,i}(t),N_{m,i}(t)\}_{1 i  K}\) adhereing to: \(t_{m,j}=max_{s_{1}}\{(m,j) E_{s}\}\) and \(0\) if such an \(s\) does not exist: \(n_{m,i}(t+1)=n_{m,i}(t)+_{a_{m}^{t}=i},N_{m,i}(t+1)=\{n_{m,i}(t+1 ),_{i,j}^{m}(t),j_{m}(t)\}\)\((2)\)\(_{i}^{m}(t+1)=_{i}^{m}(t) n_{m,i}(t)+n_{m,i}(t) _{a_{m}^{t}=i}}{n_{m,i}(t+1)},P_{t}^{}(m,j)=}\) if \(P_{t}(m,j)>0\) and \(0\) otherwise \(_{i}^{m}(t+1)=_{j=1}^{M}P_{t}^{}(m,j)}_{i, j}^{m}(t_{m,j})+d_{m,t}_{j_{m}(t)}}_{i,j}^{m}(t )+d_{m,t}_{j_{m}(t)}}_{i,j}^{m}(t_{m,j})\) with \(d_{m,t}=(1-_{j=1}^{M}P_{t}^{}(m,j))/M\)

Similar to , the algorithm balances between exploration and exploitation by the upper confidence bound and a criterion on \(n_{m,i}(t)\) and \(N_{m,i}(t)\) that ensures that all clients explore arms at similar rates and thereby "staying on the same page." After interactingwith the environment, clients move to the transmission stage, where they share information with the neighbors on \(G_{t}\), as a preparation for the update stage.

Different from the upper confidence bound approach in (Zhu and Liu, 2023), which has an extra term of \(\) in the UCB criterion, our proposal is aligned with the conventional UCB algorithm. Meanwhile, our update rule differs from that in (Zhu et al., 2021b; Zhu and Liu, 2023) in three key aspects: (1) maintaining a stopping time \(t_{m,j}\) that tracks the most recent communication to client \(j\), and (2) updating \(_{i}^{m}\) based on both \(_{i}^{j}\) and \(_{i}^{j}\) for \(j_{m}(t)\), and (3) using a weight matrix based on \(P_{t}^{}\) and \(P_{t}\) computed from the trajectory \(\{G_{s}\}_{s t}\) in the previous steps. The first point ensures that the latest information from other clients is leveraged, in case there is no communication at the current time step. The second point ensures proper integration of both network-wide and local information, smoothing out biases from local estimators and reducing variances through averaging. The third point distills the information carried by the time-varying graphs and determines the weights of the available local and network-wide estimators, removing the need for the doubly stochasticity assumption. The algorithm assumes that the clients know \(M\) and \(^{2}\).

We note that \(t_{m,j}\) is the stopping time by definition and that \(_{i}^{m}\) is an unbiased estimator for \(_{i}^{m}\) with a decaying variance proxy. Meanwhile, the matrices \(P_{t}^{}\) and \(P_{t}\) are not doubly stochastic and keep track of the history of the random graphs. By introducing \(t_{m,j}\) and \(P_{t}\), we can show that the global estimator \(_{i}^{m}(t)\) behaves similarly to a sub-Gaussian/sub-exponential random variable with an expectation of \(_{i}\) and a time-decaying variance proxy proportional to \(n_{j,i}(t)}\). This ensures that the concentration inequality holds for \(_{i}^{m}(t)\) with respect to \(_{i}\) and that client \(m\) tends to identify the global optimal arms with high probability, which plays an important role in minimizing regret. The formal statements are presented in the next section.

## 3 Regret Analyses

In this section, we show the theoretical guarantees of the proposed algorithm, assuming mild conditions on the environment. Specifically, we consider various settings with different model assumptions. We prove that the regret of Algorithm 2 has different instance-dependent upper bounds of order \( T\) for settings with sub-Gaussian and sub-exponential distributed rewards, and a mean-gap independent upper bound of order \( T\) across settings. Many researchers call such a bound instance independent but we believe such a terminology is misleading and thus we prefer to call it man-gap independent, given that it still has dependency on parameters pertaining to the problem instance. The results are consistent with the regret bounds in prior works.

### Model Assumptions

By definition, the environment is determined by how the graphs (E-R or uniform) and rewards are generated. For reward we consider two cases.

Sub-gAt time step \(t\), the reward of arm \(i\) at client \(m\) has bounded support \(\), and is drawn from a sub-Gaussian distribution with mean \(0_{i}^{m} 1\) and variance proxy \(0(_{i}^{m})^{2}^{2}\).

Sub-eAt time step \(t\), the reward of arm \(i\) at client \(m\) has bounded support \(\), and follows a sub-exponential distribution with mean \(0_{i}^{m} 1\) and parameters \(0(_{i}^{m})^{2}^{2},0_{i}^{m}\).

With these considerations, we investigate four different environments (settings) based on the two graph assumptions and the two reward assumptions: Setting 1.1 corresponds to E-R and Sub-g, Setting 1.2 to Uniform and Sub-g, Setting 2.1 to E-R and Sub-e, and Setting 2.2 to Uniform and Sub-e. For each setting, we derive upper bounds on the regret in the next section.

### Regret Analyses

In this section, we establish the regret bounds formally when clients adhere to Algorithm 2 in various settings. We denote Setting 1.1, Setting 1.2 with \(M<11\), and Setting 1.2 with \(M 11\) as \(s_{1},s_{2}\) and \(s_{3}\), respectively. Likewise, we denote Setting 2.1, Setting 2.2 with \(M<11\), and Setting 2.2 with \(M 11\) as \(S_{1},S_{2}\) and \(S_{3}\), respectively. See Table 1 in Appendix for a tabular view of the various settings.

Note that the randomness of \(R_{T}\) arises from both the reward and graph observations. Considering \(S_{1},S_{2},S_{3}\) differ in the reward assumptions compared to \(s_{1},s_{2},s_{3}\), we define an event \(A\) that preserves the properties of the variables with respect to the random graphs. Given the length of the burn-in period \(L_{i}\) for \(i\{s_{1},s_{2},s_{3}\}\) and the fact that \(L_{s_{i}}=L_{S_{i}}\) since it only relies on the graph assumptions, we use \(L\) to denote \(_{i}L_{s_{i}}\). Parameters \(0<,<1\) are any constants, and the parameter \(c=c(M)\) represents the mean value of the Bernoulli distribution in \(s_{1},S_{1}\) and the probability of an edge in \(s_{2},S_{2},s_{3}\), and \(S_{3}\) among all connected graphs (see (1)). We define events \(A_{1}=\{ t L,||P_{t}-cE||_{}\},A_{2}=\{ t_{0}, t L, j, m,t+1-_{j}t_{m,j} t_{0} c_{0} _{i}m_{l,i}(t+1)\}\), and \(A_{3}=\{ t L,G_{t}\) is connected\(\}\). Here \(E\) is the matrix with all values of \(1\). Constant \(c_{0}=c_{0}(K,_{i i^{*}}_{i},M,,)\) is defined later. Since \(c=c(M)\) this implies that \(G_{t}\) depends on \(M\). We define \(A=A_{,}=A_{1} A_{2} A_{3}\), which yields \(A\) with \(\) being the sub-\(\)-algebra formed by \(\{,,A,A^{c}\}\). This implies \(E[|A_{,}]\) and \(P[|A_{,}]\) are well-defined, since \(A\) only relies on the graphs and removes the differences among \(s_{1},s_{2},s_{3}\) (\(S_{1},S_{2},S_{3}\)), enabling universal regret upper bounds.

Next, we demonstrate that event \(A\) holds with high probability.

**Theorem 1**.: _For event \(A_{,}\) and any \(1>,>0\), we have \(P(A_{,}) 1-7\)._

Proof Sketch.: The complete proof is deferred to Appendix; we discuss the main logic here. The proof relies on bounding the probabilities of \(A_{1},A_{2},A_{3}\) separately. For \(A_{1}\), its upper bound holds by the analysis of the mixing time of the Markov chain underlying \(G_{t}\) and on the matrix-form Hoeffding inequality. We obtain an upper bound on \(P(A_{2})\) by analyzing the stopping time \(t_{m,j}\) and the counter \(n_{m,i}(t)\). For the last term \(P(A_{3})\), we show that the minimum degree of \(G_{t}\) has a high probability lower bound that is sufficient for claiming the connectivity of \(G_{t}\). To put all together, we use the Bonferroni's inequality and reach the lower bound of \(P(A_{,})\). 

Subsequently, we have the following general upper bound on the regret \(R_{T}\) of Algorithm 2 in the high probability sense, which holds on \(A\) in any of the settings \(s_{1},s_{2},s_{3}\) with sub-Gaussian rewards.

**Theorem 2**.: _Let \(f\) be a function specific to a setting and detailed later. For every \(0<<1\) and \(0<<f(,M,T)\), in setting \(s_{1}\) with \(c+)^{}}\), \(s_{2}\) and \(s_{3}\), with the time horizon \(T\) satisfying \(T L\), the regret of Algorithm 2 with \(F(m,i,t)= t}{n_{m,i}(t)}}\) satisfies that \(E[R_{T}|A_{,}] L+_{i i^{*}}(\{[ T }{_{i}^{2}}],2(K^{2}+MK)\}+}{3P(A_{,})}+K^{ 2}+(2M-1)K)=O( T)\) where the length of the burn-in period is explicitly \(L=\{}{}{^{2}}, T}{c_{0}}},}{ p^{*}}+25}{2^{2}},T}{c_{0}},}{ p^{*}}+25}{1-} }{2^{2}},)}{(- {1-M})}}{c_{0}}\}\) with \(\) being the spectral gap of the Markov chain in \(s_{2},s_{3}\) that satisfies \(1-}{ 2p^{*}} 4+1}\), \(p^{*}=p^{*}(M)<1\) and \(c_{0}=c_{0}(K,_{i i^{*}}_{i},M,,)\), and the instance-dependent constant \(C_{1}=8^{2}\{12}\}\)._

Proof Sketch.: The proof is carried out in Appendix; here we describe the main ideas as follows. We note that the regret is proportional to the total number of pulling global sub-optimal arms by the end of round \(T\). We fix client \(m\) for illustration without loss of generality. We tackle all the possible cases when clients pull such a sub-optimal arm - (i) the condition \(n_{m,i}(t) N_{m,i}(t)-K\) is met, (ii) the upper confidence bounds of global sub-optimal arms deviate from the true means, (iii) the upper confidence bounds of global optimal arms deviate from the true means, and (iv) the mean values of global sub-optimal arms are greater than the mean values of global optimal arms. The technical novelty of our proof is in that 1) we deduce that the total number of events (ii) and (iii) occurring canbe bounded by some constants using newly derived conditional concentration inequalities that hold by our upper bounds on the conditional moment generating functions and by the unbiasedness of the network-wide estimators and 2) we control (i) by analyzing the scenarios where the criteria are met, which do not occur frequently. 

**Remark** (**Specification of the parameters**).: _Note that the choice of \(f\) depends on the problem settings. Specifically, in setting \(s_{1}\), we set \(f(,M,T)=+)^{}}\). By the definition of \(c\), we have \(f(,M,T)<c\). In setting \(s_{2}\) with \(M<11\), we specify \(f(,M,T)=\) which meets \(f<c\) due to (1). Lastly, in setting \(s_{3}\) with \(M 11\), we choose \(f(,M,T)=\) and again we have \(f<c\) due to (1). We observe that the regret bound is dependent on the transition kernel \(\) and the spectral gap \(\) of the underlying Markov chain associated with \(\). This indicates the significance of graph complexities and distributions within the framework of the random graph model when deriving the regret bounds, in a similar manner as the role of graph conductance in the regret bounds established in (Sankararaman et al., 2019; Chawla et al., 2020) for time-invariant graphs._

To proceed, we show a high probability upper bound on the regret \(E[R_{T}|A_{,}]\) of Algorithm 2 for settings \(S_{1},S_{2},S_{3}\) with sub-exponential rewards.

**Theorem 3**.: _Let \(f\) be a function specific to a setting and defined in the above remark. For every \(0<<1\) and \(0<<f(,M,T)\), in settings \(S_{1}\) with \(c+)^{}}S_{2 },S_{3}\) with the time horizon \(T\) satisfying \(T L\), the regret of Algorithm 2 with \(F(m,i,t)= T}{n_{m,i}(t)}}+ T}{n_{m,i}(t)}\) satisfies \(E[R_{T}|A_{,}] L+_{i i^{*}}(_{i}+1)(([  T}{_{i}^{2}}],[ T}{_{i}}],2(K^{ 2}+MK))+)^{T^{3}}}+K^{2}+(2M-1)K)=O( T)\) where \(L,C_{1}\) are specified as in Theorem 2 and \(}{C_{1}}\)._

Proof Sketch.: The proof is detailed in Appendix. The proof logic is similar to that of Theorem 2. However, the main differences lie in the upper confidence bounds, which require proving new concentration inequalities and moment generating functions for the network-wide estimators. 

In addition to the instance-dependent regret bounds of order \(O(})\) that depend on the sub-optimality gap \(_{i}\) which may be arbitrarily small and thereby leading to large regret, we also establish a universal, mean-gap independent regret bound that applies to settings with sub-exponential and sub-Gaussian rewards. A formal proof is deferred to Appendix.

**Theorem 4**.: _Assume the same conditions as in Theorems 2 and 3. The regret of Algorithm 2 satisfies that \(E[R_{T}|A_{,}] L_{1}+)^{T^{3} }}+(,C_{2}) T}+1}))^{T^{3} }}+K(C_{2}( T)^{2}+C_{2} T+ T})=O(  T)\). where \(L_{1}=(L,K(2(K^{2}+MK)))\), \(L,C_{1}\) is specified as in Theorem 2, and \(}{C_{1}}>\). The involved constants depend on \(^{2}\) but not on \(_{i}\)._

### Other Performance Measures

Communication costAssuming a constant cost of establishing a communication link, as defined in (Wang et al., 2020; Li and Song, 2022), denoted as \(c_{1}\), the communication cost \(C_{T}\) can be calculated as \(C_{T}=c_{1}_{t=1}^{T}|E_{t}|\), which is proportional to \(_{t=1}^{T}|E_{t}|\). Alternatively, following the framework proposed in (Wang et al., 2022; Li and Wang, 2022; Sankararaman et al., 2019; Chawla et al., 2020), the communication cost can be defined as the total number of communications among clients, which can be represented as \(C_{T}=_{t=1}^{T}|E_{t}|\), similar to the previous definition. Regarding the quantity \(C_{T}=_{t=1}^{T}|E_{t}|\), the number of edges \(E_{t}\) could be \(O(M)\) for sparse graphs, and at most \(O(M^{2})\). In the random graph model, the expected number of edges is \(c\), which implies \(O(M^{2})\) in the worst case scenario and the total communication cost, in a worst-case scenario, is of order \(O(TM^{2})\). This analysis holds also for the random connected graph case where \(c\) represents the probability of having an edge. This cost aligns with the existing work of research on decentralized distributed multi-agent MAB problems without a focus on the communication cost, where edge-wise communication is a standard practice. Optimizing communication costs to achieve sublinearity, as discussed in (Sankararaman et al., 2019; Chawla et al., 2020), is a subject for future research.

Complexity and privacy guaranteeAt each time step, the time complexity of the graph generation algorithm is \(O(M^{2}+M+|E_{t}|) O(M^{2})\), where the first term accounts for edge selection, and the second and third terms are for graph connectivity verification using Algorithm 1 (benefitting from the use of Markov chains for graph generation). The main algorithm comprises various stages involvingmultiple agents. The overall time complexity is calculated as \(O(MK+M^{2}+MK+M^{2})=O(M^{2}+MK)\), consistent with that of (Zhu et al., 2021b; Dubey et al., 2020). It is noteworthy that most procedures can operate in parallel (synchronously), such as arm pulling, broadcasting, estimator updating, and E-R model generation, with the exception being random connected graph generation due to the Markovian property. Privacy guarantees are also important. Here, we note that clients only communicate aggregated values of raw rewards. Differential privacy is not the focus of this work but may be considered in future research.

## 4 Numerical Results

In this section, we present a numerical study of the proposed algorithm. Specifically, we first demonstrate the regret performance of Algorithms 2 and 3, in comparison with existing benchmark methods from the literature, in a setting with time-invariant graphs. Moreover, we conduct a numerical experiment with respect to time-varying graphs, comparing the proposed algorithm with the most recent work (Zhu and Liu, 2023). Furthermore, the theoretical regret bounds of the proposed algorithm, as discussed in the previous section, exhibit different dependencies on the parameters that determine the underlying problem settings. Therefore, we examine these dependencies through simulations to gain insights into the exact regret incurred by the algorithm in practice. The benchmark algorithms include GoSInE (Chawla et al., 2020), Gossip_UCB (Zhu et al., 2021b), and Dist_UCB (Zhu and Liu, 2023). Notably, GoSInE and Gossip_UCB are designed for time-invariant graphs, while Dist_UCB and the proposed algorithm (referred to as DrFed-UCB) are tailored for time-varying graphs. Details about numerical experiments are refered to Appendix.

Benchmark comparison resultsThe visualizations for both time-invariant and time-varying graphs are in Appendix. We evaluated regret by averaging over 50 runs, along with the corresponding 95% confidence intervals. In time-invariant graphs, DrFed-UCB consistently demonstrates the lowest average regret, showcasing significant improvements. Dist_UCB and DrFed-UCB exhibit larger variances (Dist_UCB having the largest variances), which might have resulted from the communication mechanisms designed for time-varying graphs. In time-varying graphs, our regret is substantially lower compared to that of Dist_UCB. In terms of time complexity, DrFed-UCB and GoSInE are approximately six times faster than Dist_UCB.

Regret dependency resultsAdditionally, we illustrate how the regret of DrFed-UCB depends on several factors, including the number of clients \(M\), the number of arms \(K\), the Bernoulli parameter \(c\) for the E-R model, and heterogeneity measured by \(_{i,j,k}|_{i}^{k}-_{j}^{k}|\). The visualizations are available in Appendix. We observe that regret monotonically increases with the level of heterogeneity and the number of arms, while it monotonically decreases with connectivity, which is equivalent to an increase in graph complexity. However, this monotonic dependency does not hold with respect to \(M\) due to the accumulation of the information gain.

## 5 Conclusions

In this paper, we consider a decentralized multi-agent multi-armed bandit problem in a fully stochastic environment that generates time-varying random graphs and heterogeneous rewards following sub-gaussian and sub-exponential distributions, which has not yet been studied in existing works. To the best of our knowledge, this is the first theoretical work on random graphs including the E-R model and random connected graphs, and the first work on heterogeneous rewards with heavy tailed rewards. To tackle this problem, we develop a series of new algorithms, which first simulate graphs of interest, then run a warm-up phase to handle graph dynamics and initialization, and proceed to the learning period using a combination of upper confidence bounds (UCB) with a consensus mechanism that relies on newly proposed weight matrices and updates, and using a stopping time to handle randomly delayed feedback. Our technical novelties in the results and the analyses are as follows. We prove high probability instance-dependent regret bounds of the order of \( T\) in both sub-gaussian and sub-exponential cases, consistent with the regret bound in the existing works that only consider the expected regret. Moreover, we establish a nearly tight instance-free regret bound of order \( T\) for both sub-exponential and sub-gaussian distributions, up to a \( T\) factor. We leverage probabilistic graphical methods on random graphs and draw on theories related to rapidly mixing Markov chains, which allows us to eliminate the doubly stochasticity assumption through new weight matrices and a stopping time. We construct new network-wide estimators and invent new concentration inequalities for them, and subsequently incorporate the seminal UCB algorithm into this distributed setting. A discussion on future work is refered to Appendix.