# MambaTalk: Efficient Holistic Gesture Synthesis

with Selective State Space Models

 Zunnan Xu, Yukang Lin, Haonan Han, Sicheng Yang, Ronghui Li, Yachao Zhang, Xiu Li

Shenzhen International Graduate School, Tsinghua University

University Town of Shenzhen, Nanshan District, Shenzhen, Guangdong, P.R. China

Work done as intern at Tecent.Equal Contribution.Corresponding author.

###### Abstract

Gesture synthesis is a vital realm of human-computer interaction, with wide-ranging applications across various fields like film, robotics, and virtual reality. Recent advancements have utilized the diffusion model to improve gesture synthesis. However, the high computational complexity of these techniques limits the application in reality. In this study, we explore the potential of state space models (SSMs). Direct application of SSMs in gesture synthesis encounters difficulties, which stem primarily from the diverse movement dynamics of various body parts. The generated gestures may also exhibit unnatural jittering issues. To address these, we implement a two-stage modeling strategy with discrete motion priors to enhance the quality of gestures. Built upon the selective scan mechanism, we introduce MambaTalk, which integrates hybrid fusion modules, local and global scans to refine latent space representations. Subjective and objective experiments demonstrate that our method surpasses the performance of state-of-the-art models. Our project is publicly available at https://kkakkkka.github.io/MambaTalk/.

## 1 Introduction

Gesture synthesis is a critical area of research in human-computer interaction (HCI), which has very broad application prospects, such as film, robotics, virtual reality, and digital human development . The task is challenging due to the variable correlation between speech and gestures, as the same spoken content can elicit markedly different gestures among speakers. Meanwhile, the generated gestures should synchronize with the speaker's rhythm, emotional cues, and intentions .

Recent works in co-speech gesture generation have shown great progress . By introducing new datasets  and more modalities , previous work achieved end-to-end gesture generation based on RNN-based models . With the success of transformer in nature language processing  and video sequence modeling , recent works  leverage the power of attention mechanism to generate more expressive gestures that better synchronize with speech. By further combining emotional and style related features, EMoG  achieve better quality gesture generation. With the development in human recognition model , EMAGE  proposes a masked audio-gesture modeling strategy to enhance unified holistic gesture synthesis. Recently, with the development of diffusion model in generative tasks , the latest works  have applied the diffusion model to gesture synthesis, significantly improving the diversity of generated gesture. DiffuseStyleGesture  presents a diffusion model-based approach for generating diverse co-speech gestures by incorporating cross-local attention and self-attention mechanisms, and utilizing classifier-free guidance for style control. DiffuseStyleGesture+  further considers the text modality as an additional input and utilizes channel concatenation to merge the text featurewith the audio feature. Deichler _et al._ also incorporates the text modality as an additional input and employs contrastive learning to enhance the features. However, the exploration of generation for co-speech gesture sequences with low latency remains relatively uncharted, constraining its application in dynamic, interactive environments. RNN-based models often struggle with the long-term forgetting issue [54; 29], which impairs their ability to generate long sequences of gestures effectively. Additionally, these models may produce gestures that lack variability, tending towards an average representation . Transformer-based models depend heavily on subtle positional encoding to capture the order of input elements [44; 50; 73]. Meanwhile, their computational complexity, which grows quadratically with the length of the input sequence, poses a challenge for generating long sequences of gestures. For the diffusion-based model, the intricate sampling strategy and iterative process lead to high computational expenses , which hinder their broad adoption in gesture generation scenarios.

State space models (SSMs) have recently shown significant potential in addressing challenges related to modeling sequences with low latency . Inspired by continuous state space models from control systems and enhanced by HiPPO initialization , SSMs  show promise in addressing long-term forgetting issue. These advancements have been integrated into large-scale representation models [38; 41]. Some pioneering works have applied SSMs for tasks like language understanding [38; 41], content-based reasoning , and visual recognition [35; 57]. In our work, we further explore the potential of SSMs in co-speech gesture synthesis. We observe that directly applying the selective scan mechanism from Mamba  to gesture generation as a sequence modeling model would result in jittery outputs. To refine the generated gestures, we propose a two-stage modeling strategy. In the first training stage, we enhance the discrete motion priors derived from VQVAEs  by integrating velocity and acceleration losses. In the second stage, by utilizing motion priors from VQVAEs, we introduce individual learnable queries for different body parts, thereby alleviating the jittering issue. Meanwhile, considering that the direct application of Mamba encounters the challenge of limb movements across different body parts tending to average out, we propose a hybrid scanning approach in the second stage to enhance the motion representation in the latent space. Specifically, we refine the design of spatial and temporal modeling within latent spaces by introducing a global-to-local modeling strategy and integrating attention mechanisms along with a selective scanning approach into the framework's design. Considering the significant differences in deformation and movement patterns among different body parts , we propose local and global scan modules for refining the latent space representations of the movements across various body parts. These approaches enable dynamic interaction and iterative refinement of different body parts while maintaining low latency, leading to more diverse and rhythmic gestures. Our contributions can be summarized as below:

* We are the first to explore the potential of the selective scan mechanism for co-speech gesture synthesis, achieving a diverse and realistic range of facial and gesture animations.
* We introduce MambaTalk, an innovative framework that integrates hybrid scanning modules (e.g., local and global scan). The integration enhances the latent space representations for gesture synthesis, thereby refining the distinct movement patterns across various body parts.
* Extensive experiments and analyses demonstrate the effectiveness of our proposed method.

Figure 1: Our two-stage method for co-speech gesture generation with selective state space models. In the first stage, we construct discrete motion spaces to learn specific motion codes. In the second stage, we develop a speech-driven model of the latent space using selective scanning mechanisms.

Related Work

### Co-speech Gesture Generation

Co-speech gesture generation aims to automatically generate gestures based on speech input. Existing approaches can be broadly categorized into three groups: (i) Rule-based methods: These methods rely on pre-defined rules and gesture libraries to generate gestures based on speech features [23; 56]. While offering interpretable results, they require significant manual effort in creating gesture datasets and defining rules. (ii) Statistical models: These approaches leverage data-driven techniques to learn mapping rules between speech and gestures, often employing pre-defined gesture units [22; 25]. While overcoming the limitations of manual rule creation, these methods still rely on handcrafted features. (iii) Deep learning methods: Recent advancements in deep learning have enabled neural networks to capture the complex relationship between speech and gestures directly from raw multimodal data [70; 33; 68; 32]. This progress has established deep learning approaches, particularly recurrent neural networks (RNNs) [70; 33; 61], transformers [4; 45], and diffusion models [2; 74; 51; 72; 21; 6], as the prevailing paradigm for co-speech gesture generation. However, each of these models suffers from certain limitations that hinder their performance. RNNs inherently process sequences in a serial manner, where each timestep's computation depends on the output of the previous timestep. This limits their ability to efficiently handle long sequences and introduces cumulative latency. Meanwhile, RNNs lack inherent parallelism, further restricting their potential for high-speed computation. Transformers consider all positions within a sequence at every timestep, resulting in high computational complexity, especially for long sequences. While diffusion models significantly enhance the diversity of generated outputs, the sampling process is computationally expensive. To overcome these limitations, our method investigates the capacity of selective state space models in the field of gesture synthesis. To the best of our knowledge, we are the first to apply selective state space models to the task of gesture generation.

### Selective State Space Models

State Space Models (SSMs) are a novel class of models recently integrated into deep learning for state space transformation [15; 10]. As foundational models evolve, various subquadratic-time architectures have emerged, including linear attention, gated convolution, recurrent models, and structured state space models (SSMs), aimed at mitigating the computational inefficiencies of Transformers when dealing with lengthy sequences. However, these advancements have yet to match the performance of attention mechanisms in critical modalities like language processing.

SSMs draw inspiration from continuous state space models in control systems and, when combined with HiPPO initialization , as seen in LSSL , show promise in tackling long-range dependency issues. However, the computational and memory demands of the state representation render LSSL impractical for real-world use. To address this, S4  suggests normalizing the parameters into a diagonal structure. This has led to the emergence of various structured SSMs with diverse configurations, such as complex-diagonal structures [17; 14], multiple-input multiple-output (MIMO) support , diagonal-plus-low-rank decomposition , and selection mechanisms . These models have been incorporated into large-scale representation models [38; 41].

These models primarily focus on the application of SSMs to long-range and sequential data like language and speech, for tasks such as language understanding [38; 41], content-based reasoning , and pixel-level 1-D image classification . Recently, some pioneering work [35; 57; 59] have explored their application in visual recognition. We further demonstrate that by incorporating the selective scan mechanism from mamba  and the discrete motion priors from VQVAEs , our proposed MambaTalk is capable of matching the performance of existing popular holistic gesture synthesis models, highlighting the potential of MambaTalk as a powerful gesture synthesis model.

## 3 Method

We aim to synthesize sequential 3D co-speech gestures from speech signals (e.g., audio and text) using selective state space models. However, simply applying such a model to gesture synthesis leads to severe gesture jittering issues. We also found that maintaining performance is challenging due to the significant variations in movement patterns exhibited by different body parts. To overcome these challenges, we suggest modeling the gesture space using the acquired discrete motion patterns.

Subsequently, we propose to develop speech-conditioned selective state space models within this framework. This approach is designed to enhance the model's robustness against uncertainties that arise from cross-modal discrepancies. As shown in Figure 2, our framework consists of two stages: (i) modeling the discrete gestures and facial motion spaces (SS3.2) and (ii) learning speech-conditioned selective state space models (SS3.3) to generate 3D co-speech gestures.

### Preliminaries

**Selective State Spaces Model.** In our approach, we adopt the Selective State Spaces model (Mamba ) that incorporates a selection mechanism and a scan module (S6). This model is designed to make sequence modeling, as it dynamically selects salient input segments for prediction, thereby enhancing its focus on pertinent information and improving overall performance. Unlike the traditional S4 model, which uses time-invariant matrices \(A\), \(B\), \(C\), and scalar \(\), Mamba introduces selection mechanism that allows for the learning of these parameters from the input data using fully-connected layers. This adaptability enables model to better generalize and perform complex modeling tasks. Mamba operates by defining the state space with structured matrices that introduce specific constraints on the parameters, facilitating efficient computation and data storage. For each batch and each dimension, the model processes the input \(x_{t}\), hidden state \(h_{t}\), and output \(y_{t}\) at each time step \(t\). We have \(h_{0}=_{0}x_{0}\) when \(t=0\). When t > 0, the model's formulation is as follows:

\[ h_{t}=_{t}h_{t-1}+_{t}x_{t},\\ y_{t}=C_{t}h_{t},\] (1)

where \(_{t},_{t}\), and \(C_{t}\) are matrices and vectors that are updated at each time step, allowing the model to adapt to the temporal dynamics of the input sequence. With discretization, let \(\) denote the sampling interval, \(( A)\) denote the matrix exponential, the transformation of the system's state over one time step can be represented as follows:

\[=( A),\\ =( A)^{-1}(( A)-I) B,\\ h_{t}=h_{t-1}+x_{t},\] (2)

Figure 2: We propose a two-stage method for co-speech gesture generation. We first train multiple VQ-VAEs for face and different parts of body reconstruction. This step learns discrete motion priors through multiple codebooks. In the second stage, we train a speech-driven gesture generation model in the latent motion space with local and global scan modules.

where \(( A)^{-1}\) denotes the inverse of matrix \( A\), \(I\) denotes the identity matrix. The scan module within Mamba is designed to capture temporal patterns and dependencies across multiple time steps by applying a set of trainable parameters or operations to each segment of the input sequence. In our framework, Mamba serves as a sequence modeling tool for decoding gesture actions across different parts of the body. By modifying the decoder's input and the range of features, we utilize Mamba to separately model the global motion features and local motion features of different body parts. These operations are learned during training and assist the model in processing sequential data.

### Discrete Gestures and Facial Motion Spaces

To ensure visual realism in motion animations from speech signals, we learn extra motion priors to depict accurate movements and natural expressions. Building on this concept, we propose a method to represent the gesture motion space using multiple discrete codebooks.

**Motion Quantization.** Considering the substantial variations in deformation magnitude and periodicity among various body parts, our approach involves learning multiple codebooks tailored for the reconstruction of distinct body parts. For illustrative purposes, we detail the formulation of a single codebook. Denotes \(C\) as the dimensionality of each latent vector, \(N\) as the number of vectors in the codebook, for the codebook \(=\{_{k}^{C}\}_{k=1}^{N}\), we employ a set of allocated items \(\{_{k}\}_{k}\) to represent the holistic gesture motion \(_{t}\). Here, \(\) represents the chosen index sets. The element-wise quantization function \(Q()\) maps each item \(}_{t}\) in \(}\) to its closest match \(_{k}\) in the codebook \(\):

\[_{}=Q(}):=*{arg\,min}_{ _{k}}\|}_{t}-_{k}\| _{2},\] (3)

where the codebook entries act as the foundational motion elements within the discrete motion space. To establish this, we follow [64; 32] to pre-train a CNN-based Vector Quantized-Variational Autoencoder (VQ-VAE), which comprises an encoder \(E\), a decoder \(D\), and a context-rich codebook \(\). This is done through the self-reconstruction of gesture motions.

The sequence of motions \(_{1:T}\) is initially transformed into a temporal feature representation \(=E(_{1:T}) R^{T^{} H C}\), where \(H\) represents the count of gesture components, \(T^{}\) indicates the quantity of temporal units encoded (with \(P=}\) frames per unit). Subsequently, we derive the quantized motion sequence \(Z_{q} R^{T^{} H C}\) by quantization function \(Q()\). This function \(Q\) maps each element in \(\) to its closest corresponding entry within the codebook \(\):

\[_{}=Q(E(_{1:T})),}_{1:T}=D(_{}).\] (4)

**Training objectives.** For the training of the quantized autoencoder, we employ motion-level losses to mitigate the jittering issue of generated gestures, along with two intermediate losses at the code level:

\[_{}=&_{rec}(,})+_{vel}(^{}, }^{})+_{acc}(^{},}^{})\\ &+\|(})-_{} \|_{2}^{2}+\|}-(_{ })\|_{2}^{2},\] (5)

where \(^{}\) and \(^{}\) means the velocity and acceleration of motion, \(sg()\) denotes a stop-gradient operation, \(_{}\) are Geodesic  loss and the last two terms are designed to refine the codebook entries. For facial motions, we utilize MSE loss for both velocity (\(_{}\)) and acceleration (\(_{}\)) loss. For body motions, we use L1 loss as \(_{}\) and \(_{}\). Additionally, for the foot contact loss, we employ MSE loss as the loss function. These terms work by minimizing the distance between the codebook \(Z\) and the embedded features \(\). Given that the quantization function (Equation 3) is non-differentiable, we utilize the straight-through gradient estimator  to propagate the gradients.

### Speech-Driven Selective State Spaces Gesture Synthesis Model

**Overall Framework.** Utilizing the acquired discrete motion prior, we establish a cross-modal mapping from speech inputs to target motion codes, enabling the generation of realistic co-speech gesture motions. In our approach to speech-driven gesture synthesis, we utilize audio sequences \(A=\{a_{1},,a_{N}\}\) and text sequences \(T=\{t_{1},,t_{N}\}\) as inputs to guide the generation of co-speech gestures \(G=\{g_{1},,g_{N}\}\). Here, \(N\) signifies the total frame count, and \(g_{i} R^{55 6+100+4+3}\)denotes 55 pose joints in Rot6D, \(R^{100}\) FLAME parameters, \(R^{4}\) foot contact labels, \(R^{3}\) global translations for the \(i\)-th frame. The gesture synthesis model, comprising audio encoders \(E_{}\) and text encoders \(E_{}\) and multiple selective state space models \(D_{}\) for different parts of the body, is trained on the discrete motion space, conditioned on the speech, as shown in Figure 2.

**Speech Feature Extraction.** For audio feature extraction, two CNN-based audio feature extraction networks are employed to respectively extract features from amplitude, raw audio and decoded tokens from Wav2vec2CTC . Considering that the movements of body parts are not closely linked to raw audio, the audio encoder for body parts does not utilize raw audio as input. Specifically, we integrate these features along the channel dimension to obtain audio features \(f_{A}=\{fa_{1},...,fa_{N}\}\). For processing speech input words, we employ pre-trained FastText  to obtain word embeddings, which are then refined by linear projections to produce text features \(f_{T}=\{ft_{1},...,ft_{N}\}\). We further fuses features from the input modalities (e.g., audio and text features). The speaker ID embeddings \(s_{id}\) are first combined with audio and text features through additive operation. By concatenating feature vectors along the channel dimension, we further apply linear transformations to determine the weight factors, and then integrating the features through an element-wise summation. The process can be formalized as:

\[w_{T} =(W_{T}[f_{A}+s_{id},f_{T}+s_{id} ]),\] (6) \[w_{A} =(W_{A}[f_{A}+s_{id},f_{T}+s_{id} ]),\] \[_{T} =w_{T} f_{A}+(1-w_{T}) f_{T},\] \[_{A} =w_{A} f_{A}+(1-w_{A}) f_{T},\]

where \(\) denotes the softmax operation, \(\) denotes the Hadamard product, and \(W_{T}\) and \(W_{A}\) represent linear mapping matrices used to adjust the dimensions of merged features. \(_{T}\) and \(_{A}\) denote the fused features.

**Global and Local Scans.** Recognizing the diverse deformations and motion patterns in various body parts, we propose using global scan module and multiple local scan modules to model the movements of different body parts (e.g., face, hand, upper and lower body) with fused multi-modal features from previous modules. By acquiring the speech features from audio and text encoders, we first improve the perception of motion patterns among them using a global scan module by combining the speech features along the sequence dimension. Subsequently, by utilizing self-attention mechanism(\(_{}\)), we model the global information across different sequence tokens. Following previous work , we establish a set of learnable parameters (\(Q_{global}\)) and employ masked motion to facilitate the acquisition of global information. Then, we employ self-attention mechanisms to enhance the global information and obtain the refined features. These refined features are fed into Mamba to extract temporal perceptual information. Considering the differences in representation between the body and face, we employ two independent MAMBA models to model the temporal features of the face and body, respectively. We then merge these features using a linear layer to obtain global features. The process can be fomulized as below:

\[_{global} =_{}(Q_{global}),\] (7) \[f_{speech} =([_{T},_{A}]),\] \[_{global} =(_{global}),\] \[f_{global} =([f_{speech},_{global}]),\]

where \([]\) denotes the concatenation operation of features in the dimension of the sequence. To enhance the generalization of the model, we incorporate the learnable queries to foster the queries' ability to learn motion patterns. As shown in Figure 2, the queries from global scan are integrated with input speech features through a multihead cross-attention mechanism (\(_{}\)). This allows queries to learn the most relevant information from the speech input. The process can be formally defined as follows:

\[f_{}=_{}(_{global},[_{T}, _{A}]),\] (8)

where \(f_{}\) denotes the feature of refined learnable queries. Utilizing the extracted perceptual features from various body parts, we proceed to employ Mamba to extract temporal features from the sequence, which can be formalized as below:

\[F_{face}=(f_{refine}),\] (9)

where \(F_{}\) corresponds to the temporal features of facial motion. The same approach is utilized to generate temporal features for the hand, upper body, and lower body by inputting their respective perceptual features \(f_{hand}\), \(f_{upper}\), and \(f_{lower}\) into the corresponding Mamba modules. One distinction is that we incorporate an additional self-attention layer before the Mamba layer to enhance the perception of body movements. The local latent features are then fed into their respective VQ-Decoders to produce the final motion predictions.

**Training Objectives.** The model's training objectives are composed of a composite loss function that harmonizes reconstruction and cross-entropy losses. This design aims to augment the accuracy of motion generation, encompassing the face, hands, upper, and lower body. The loss of latent reconstruction, represented by \(L_{reclatent}\), is quantified using the Mean Squared Loss (MSELoss). Here, \(z_{i}\) corresponds to the true latent vectors, while \(_{i}\) are the vectors reconstructed by the model. The latent reconstruction loss is expressed as:

\[L_{reclatent}=_{i=1}^{N}\|z_{i}-_{i}\|^{2},\] (10)

where \(N\) denotes the number of frames. Concurrently, to encourage diversity in the generated motions, we optimize the cross-entropy loss for latent code class classification \(L_{cls}\). Specifically, we employ Negative Log-Likelihood Loss (NLLLoss), where \(y_{i}\) represents the true class labels for each sample, and \(_{i}\) denotes the model's predicted class labels. This loss is calculated as the negative sum of the logarithm of the predicted probabilities for the correct classes:

\[L_{cls}=-_{i=1}^{N}_{c=1}^{C}y_{ic}(_{ic}),\] (11)

where \(N\) signifies the total number of frames, \(C\) is the total number of classes, and \(y_{ic}\) is a binary indicator of whether class \(c\) is the correct label for sample \(i\). The total loss \(L\) is a weighted sum of the categorical and latent reconstruction losses, with \(\) and \(\) serving as balance hyper-parameters:

\[L= L_{cls}+ L_{reclatent},\] (12)

where \(=1\) and \(=3\) for hands, upper and lower body motion. For facial motion, we set \(=0\) and \(=3\). By optimizing the total loss, the model is trained to generate diverse gesture results.

## 4 Experiments

### Experiments Setting

We train and evaluate on the BEAT2 dataset proposed by . BEAT2 contains 60 hours of data with high finger quality for 25 speakers (12 female and 13 male). The dataset comprises 1762 sequences, each with an average duration of 65.66 seconds. Each sequence includes a response to a daily inquiry. We split datasets into 85%/7.5%/7.5% for the train/val/test set. We follow previous work  to select data from Speaker 2 for training and validation to ensure fair comparison.

### Implementation Details

We utilize the Adam optimizer with a learning rate of \(2.5 10^{-4}\). To maintain stability, we apply gradient norm clipping at a value of 0.99. In the construction of the VQVAEs, we employ a uniform initialization for the codebook, setting the codebook entries to feature lengths of 512 and establishing the codebook size at 256. The numerical distribution range for the codebook initialization is defined as \([-1/,1/)\). The codebook is solely updated during the first stage, and in the second stage of training for the speech-to-gesture mapping, the codebook remains frozen. The VQVAEs are trained for \(200\) epochs, with a learning rate of \(2.5 10^{-4}\) for the first 195 epochs, which is then reduced to \(2.5 10^{-5}\) for the final 5 epochs. During the second stage, the model is trained for 100 epochs. All experiments are conducted using one NVIDIA A100 GPU.

### Metrics

To evaluate the realism of body gestures, we employ Frechet Gesture Distance (FGD)  to measure the proximity of the distribution between the ground truth and generated gestures. Subsequently, Diversity  is quantified by computing the average L1 distance across multiple gesture clips. The synchronization between speech and motion is achieved using Beat Constancy (BC) . For facial motions, we assess positional accuracy by calculating the vertex Mean Squared Error (MSE) . Additionally, the difference between the ground truth and the generated facial vertices is measured using the vertex L1 difference (LVD) . _More details about metrics and efficiency analysis are provided in the supplementary materials._

### Quantitative Results

As shown in Table 1, our method attains the lowest FGD and highest BC when compared to the previously top-performing method. This highlights the superior ability of MambaTalk in discerning and correlating the audio-motion beats. The lowest FGD also emphasizes the high quality and naturalness of our generated movements, showing the ability of MambaTalk to capture real motion dynamics. This also demonstrates the authenticity of our generated motions, affirming the successful capture of inherent motion characteristics. Some results are marked as "-" because these methods can not generate facial movements. Moreover, our method outperforms previous methods in terms of MSE and LVD, with substantial improvements of 18.11% and 8.72%, respectively. These two enhancements highlight the superior accuracy and fidelity of our method in capturing fine-grained details, affirming its efficacy in synthesizing realistic and authentic facial motions.

### Qualitative Analysis

**User Study.** We conducte a user study to assess the visual quality of the generated co-speech 3D gestures. For each method under comparison, we produce 10 gesture samples, which were then converted into video clips for evaluation by 39 participants. In each evaluation session, participants are presented with 20 seconds video clips generated by various models. They are instructed to assess the clips across the following dimensions: (i) naturalness, (ii) appropriateness, (iii) synchrony and (iv) smoothness. For naturalness, they evaluate the similarity of the generated gestures to those made by humans, paying attention to the authenticity and smoothness of the movements. In terms of appropriateness, they consider the alignment of the gestures with the spoken content, taking into account both the explicit meaning and the underlying semantics. For synchrony assessment, they examine the timing of the gestures in relation to the speech rhythm, audio, and facial expressions to ensure a harmonious and integrated performance. For smoothness, they assess the gestures for any abrupt stops or unnatural jerks that might indicate a lack of fluidity in motion. We mainly compare two state-of-art methods with our proposed method (with and without VQVAE): CaMN , EMAGE , and the ground truth. As presented in Table 2, our method's average scores are higher than previous methods.

**Visualization.** As depicted in Figure 3, our approach yields gestures that exhibit enhanced rhythmic alignment and a more natural appearance. For instance, when conveying "we were", our method

   Methods & Venue & FGD \(\) & BC \(\) & Diversity \(\) & MSE \(\) & LVD \(\) \\   \\  S2G  & ICRA 2019 & 28.15 & 4.683 & 5.971 & - & - \\ Trimodal  & TOG 2020 & 12.41 & 5.933 & 7.724 & - & - \\ HA2G  & CVPR 2022 & 12.32 & 6.779 & 8.626 & - & - \\ DisCo  & ACMMM 2022 & 9.417 & 6.439 & 9.912 & - & - \\ CaMN  & ECCV 2022 & 6.644 & 6.769 & 10.86 & - & - \\ DiffStyleGesture  & IJCAI 2023 & 8.811 & 7.241 & 11.49 & - & - \\   \\  Habible _et al._ & IVA 2021 & 9.040 & 7.716 & 8.21 & 8.614 & 8.043 \\ TalkShow  & CVPR 2023 & 6.209 & 6.947 & **13.47** & 7.791 & 7.771 \\ EMAGE  & CVPR 2024 & \(5.512\) & 7.724 & \(13.06\) & \(7.680\) & \(7.556\) \\ MambaTalk (Ours) & - & **5.366** & **7.812** & 13.05 & **6.289** & **6.897** \\   

Table 1: Quantitative results on BEAT2. FGD (Frechet Gesture Distance) multiplied by \(10^{-1}\), BC (Beat Constancy) multiplied by \(10^{-1}\), Diversity, MSE (Mean Squared Error) multiplied by \(10^{-7}\), and LVD (Learned Vector Distance) multiplied by \(10^{-5}\). The best results are in bold.

instructs the subject to hold both hands in front of the chest, a nuanced detail absent in both CaMN and EMAGE's outcomes, where either one or both arms hang down. Additionally, when representing "no place to", our method aligns with the ground truth by extending both arms upwards, whereas CaMN and EMAGE have their arms tucked in next to the body. In the case of "up", our generated result raises the right arm in alignment with the semantics of movement. In the context of "moving around" where our left and right arm swings may differ from the ground truth, the overall movement remains consistent.

Interestingly, for "sound of gunfire", a difficult semantic for the model to learn, our method still generates the result of the character's right hand clenched in a fist and the arm bent to indicate a tense situation. For the emotion of fear expressed by "is horrible", the result of our method is similar to the ground truth, with the character's hands hanging down and face facing downward, which is a visual representation of the psychological state of panic and fear. In addition, as illustrated in Figure 3, our generated motions exhibit not only diverse characteristics, such as the range of motion and which hands to use, but also a high degree of consistency with the ground truth.

### Ablation Study

**Effect of VQVAEs.** We confirm the significant role of the VQVAEs. As shown in Table 2 and Table 3, the integration of VQVAEs is essential for the functionality of our approach, contributing to the generation of gestures that exhibit smoother transitions and a more human-like quality. As

   Methods & Naturalness\(\) & Appropriateness\(\) & Synchrony\(\) & Smoothness\(\) & Avg. \\  CaMN  & 3.08 & 3.34 & 3.25 & 3.50 & 3.29 \\ EMAGE  & 3.85 & 4.04 & 3.89 & 4.21 & 3.99 \\ \(_{}\) & 1.24 & 1.24 & 1.18 & 1.29 & 1.24 \\ **Ours** & 4.04 & 4.00 & 3.91 & 4.35 & 4.08 \\ Ground Truth & 4.57 & 4.54 & 4.23 & 4.63 & 4.50 \\   

Table 2: User study results on naturalness (human likeness), appropriateness (the degree of consistency with the speech content), synchrony (the level of synchronization with the speech rhythm) and smoothness (the fluency of actions). The rating score range is 1-5, with 5 being the best. “Avg.” denotes the average scores. \(\) indicates the higher the better.

Figure 3: Visualization of the gestures generated by CaMN, EMAGE and our method. Unreasonable results are indicated by red boxes and reasonable ones by green boxes.

demonstrated in Table 3, the removal of the VQVAEs ("\(-\)VQVAEs") from the model is also associated with performance decline, manifesting reduction in FGD, BC, Diversity, MSE and LVD.

**Effect of Local Scan.** We validate the effectiveness of the local scan. The ablation study is divided into two segments: (i) multi head cross attention and (ii) Mamba models for different part of bodys. As shown in Table 3, incorporating multi head cross attention enhances our method's capability to generate gestures with higher beat constancy. The incorporation of the Mamba from local scan generate gestures characterized by greater diversity. Concurrently, there is an observed improvement in the FGD of the generated gestures.

**Effect of Global Scan.** We validate the effectiveness of the global scan, as listed in Table 3, the incorporation of global scan improves the overall performance of our method. For the multi-head self-attention module in global scan, the incorporation of multi-head self-attention acquires improvement of Diversity and a degradation for FGD. Additionally, the ablation results demonstrate that incorporating Mamba enhances the global scan's capability to generate gestures with higher diversity. The FGD of generated gestures is better at the same time.

**Effect of Different Audio Encoder.** To validate the effectiveness of the audio encoder, we replace the CNN-based audio encoder with a pre-trained Whisper  and Vav2Vec2 , as listed in Table 4. Unlike CNN-based audio encoders that are randomly initialized and trained from scratch, when using Whisper and Wav2Vec2, we initialize the encoder using pre-trained weights and fix the parameters of the feature extractor. We observe a notable enhancement in facial generation when utilizing Whisper, however, the body generation results were subpar. In contrast, while Wav2Vec2 demonstrates some improvement in body generation, it results in a substantial decline in facial generation quality.

## 5 Conclusion

In this study, we propose a framework to employ the state space models in gesture synthesis. To alleviate the problem of jitter in gesture synthesis, we have implemented discrete motion priors, which enhance the effectiveness of the selective scan mechanism and lead to smoother results. We further incorporate the selective state space models with attention mechanisms to enhance the refinement of motion features in latent space. These modules capture the subtle movements and deformations of various body parts, thereby enhancing the overall quality of the generated gestures. By utilizing a linear time series modeling strategy with selective state space, our method achieves high-quality full body gesture generation with low latency.

   Method & FGD \(\) & BC \(\) & Diversity \(\) & MSE \(\) & LVD \(\) \\  Ours & 5.366 & 7.812 & 13.048 & 0.629 & 6.897 \\ Whisper  & 6.791 & 7.515 & 12.617 & 0.537 & 6.445 \\ Wav2vec2  & 5.343 & 7.956 & 13.164 & 0.973 & 8.452 \\   

Table 4: Ablation study on different audio encoders. \(\) denotes the lower the better, and \(\) denotes the higher the better. FGD multiplied by \(10^{-1}\), BC multiplied by \(10^{-1}\), Diversity, MSE multiplied by \(10^{-7}\), and LVD multiplied by \(10^{-5}\).

   Method & FGD \(\) & BC \(\) & Diversity \(\) & MSE \(\) & LVD \(\) \\  Ours & 5.366 & 7.812 & 13.048 & 0.629 & 6.897 \\ \(-\) VQVAEs & 12.051 & 7.447 & 8.462 & 1.316 & 9.235 \\ \(-\) Local Scan (\(_{}\)) & 7.189 & 6.701 & 13.216 & 0.638 & 6.938 \\ \(-\) Local Scan (Mamba) & 7.277 & 7.742 & 12.844 & 0.627 & 6.941 \\ \(-\) Global Scan (\(_{}\)) & 6.308 & 7.882 & 11.875 & 0.644 & 6.972 \\ \(-\) Global Scan (Mamba) & 6.149 & 7.840 & 12.605 & 0.592 & 6.752 \\   

Table 3: Ablation study on different components of our proposed method. \(\) denotes the lower the better, and \(\) denotes the higher the better. FGD multiplied by \(10^{-1}\), BC multiplied by \(10^{-1}\), Diversity, MSE multiplied by \(10^{-7}\), and LVD multiplied by \(10^{-5}\).