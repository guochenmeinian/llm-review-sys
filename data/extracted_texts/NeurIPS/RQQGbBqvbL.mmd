# Collaborative Refining for Learning from Inaccurate Labels

Bin Han, Yi-Xuan Sun, Ya-Lin Zhang, Libang Zhang, Haoran Hu

Longfei Li, Jun Zhou, Guo Ye, Huimei He

Ant Group

{binlin.hb, xuan.syx, lyn.zyl, libang.zlb, hhr327996,

longyao.llf, jun.zhoujun, yeguo.yg, huimei.hhm}@antgroup.com

Corresponding author.

###### Abstract

This paper considers the problem of learning from multiple sets of inaccurate labels, which can be easily obtained from low-cost annotators, such as rule-based annotators. Previous works typically concentrate on aggregating information from all the annotators, overlooking the significance of data _refinement_. This paper presents a collaborative refining approach for learning from inaccurate labels. To refine the data, we introduce the _annotator agreement_ as an instrument, which refers to whether multiple annotators agree or disagree on the labels for a given sample. For samples where some annotators _disagree_, a comparative strategy is proposed to filter noise. Through theoretical analysis, the correlations among multiple sets of labels, the respective models trained on them, and the true labels are uncovered, so that relatively reliable labels can be identified. For samples where all annotators _agree_, an aggregating strategy is designed to mitigate potential noise. Guided by theoretical bounds on loss values, a sample selection criterion is introduced and improved to be more robust against potentially problematic values. Through these two modules, all the samples are refined during training, and these refined samples are used to train a lightweight model simultaneously. Extensive experiments are conducted on benchmark and real-world datasets, which demonstrate the superiority of the proposed framework.

## 1 Introduction

Deep learning has recently demonstrated remarkable achievements across a wide array of applications . The cornerstone of its success rests on the availability of high-quality datasets. However, in industrial environments, obtaining accurate labels can often be costly and time-consuming. For example, in financial scenarios, it is challenging for human annotators to accurately assign labels to predict whether a user will default. Currently, many banks and companies utilize rules as low-cost autonomous annotators, especially in the early stages of some financial products. In healthcare scenarios, many rule-based algorithms are also employed to identify patients at risk of developing certain conditions, allowing for early intervention and potentially preventing more severe complications. Although these annotators can provide labels in a time- and cost-efficient manner , a single low-cost annotator usually produces biased or inaccurate labels in practice .

On the other hand, multiple annotators can provide diverse perspectives with different insights and knowledge to mitigate errors and biases of individual annotators . Learning from multiple sets of inaccurate labels provided by multiple annotators has garnered widespread attention andrelative works can be divided into two main categories. One stream of research focuses on advanced aggregation algorithms that infer the true labels before or during the training stage. The simplest aggregation algorithm is Majority Voting , which treats the labels equally by voting, while the approach known as Weighted Majority Voting  uses a weight vector to model annotators' expertise. Enhanced Bayesian Classifier Combination (EBCC)  tends to infer truth by modeling the correlation between annotators.

Others focus on training models under the supervision of all annotations. By viewing the ground-truth labels as latent variables, some methods that consider the relationships among multiple sets of labels have been proposed to infer true labels based on the Expectation-Maximization (EM) algorithm [1; 2; 34; 43]. Despite the effectiveness of these EM algorithms, they suffer from computational complexity during the training phase. In recent studies, an increasing number of studies focus on end-to-end learning. These end-to-end algorithms [3; 7; 10; 21; 27; 45; 30] can directly learn from multiple sets of noisy labels and map these noisy labels to part of the model (e.g., transition matrix), encouraging the model to learn knowledge from all noisy labels collectively .

Previous works typically concentrate on aggregating information from all the annotators through label aggregation techniques (e.g., voting) or using labels from all annotators in an end-to-end manner (e.g., mapping multiple sets of noisy labels to part of the model). Most related works often neglect the significance of data _refinement_. Although some methods have recognized the need for data refinement [21; 31; 42], they merely adopt the small-loss criterion  to filter noisy samples, and refinement is not their core contribution or their key point. On the contrary, we contend that refining a relatively clean dataset during training is the central point, which can alleviate the demands placed on model design and enhance the model's performance. To refine the dataset, an essential step is to assess the reliability of labels from multiple sets. We consider _annotator agreement_ as an instrument, which refers to whether multiple annotators agree or disagree on the labels for a given sample. In this paper, we leverage the annotator agreement information and propose a novel framework named **C**ollaborative **R**efining for **L**earning from inaccurate labels (CRL). For samples where some annotators disagree, labels are ambiguous, and learning from all sets of labels can degrade the model's performance, a comparative strategy is proposed to mitigate noise. For samples where all annotators agree, an aggregating strategy is designed to filter out potential noise. The main contributions of this work can be described as follows:

* For samples where some annotators disagree, we conduct theoretical analysis to uncover the correlations among multiple sets of labels, the respective models trained on them, and the true labels. Guided by theoretical insights, a method called **L**abel **R**efining for samples with **D**isagreements (LRD) is proposed to identify the most reliable label by comparing loss values.
* For samples where all annotators agree, we analyze the theoretical bounds on loss values. Based on these bounds, a method called **R**obust **U**nion **S**election (RUS) is proposed, in which we propose a loss-based selection criterion to select trustworthy samples and improve it to be more robust against potentially problematic values.
* Comprehensive experiments are conducted on benchmark and real-world datasets, which demonstrate the effectiveness of our framework. Moreover, our framework is designed to be independent of any specific model architecture, making it compatible with most existing methods, which is confirmed by further experiments.

## 2 Preliminaries

This paper concentrates on binary classification problems with multiple sets of inaccurate labels. Let \(D=\{_{i},}_{i}\}_{i=1}^{N}\) be a dataset consisting of \(N\) instances labeled by \(R\) annotators. \(_{i}^{d}\) is the feature vector of the \(i\)-th instance and \(}_{i}=\{_{i}^{r}\}_{i=1}^{R}\) is a \(R\)-dimensional vector representing the labels provided by \(R\) annotators, \(_{i}^{r}\{0,1\}\). Denote \(y_{i}^{*}\) as the unobserved ground-truth label for the \(i\)-th instance. \(y_{i}^{*}\) is considered as a latent variable decided by a latent function \(f^{*},i.e.,f^{*}(_{i})=y_{i}^{*}\). \(f_{}\) denotes a classifier parameterized by \(\), and \(f_{}(_{i})=(_{i})\) denotes the predicted probability after the activation function, i.e., sigmoid. The binary cross-entropy loss function of the pair \((_{i},_{i}^{r})\) and model \(f_{}\) is

\[(f_{}(_{i}),_{i}^{r})=_{i}^{r}(f_{ }(_{i}))+(1-_{i}^{r})(1-f_{}(_{i})).\] (1)

The goal of learning from multiple annotators is to get the optimal classifier \(f_{}\). that satisfies \(f_{^{*}}(_{i})=f^{*}(_{i})\).

For the model architecture, several shared embedding layers are used to jointly extract information, followed by \(R+1\) submodels \(\{f_{_{r}}\}_{r=1}^{R+1}\): \(R\) submodels \(\{f_{_{r}}\}_{r=1}^{R}\) are responsible for making predictions based on \(R\) sets of labels, and the \((R+1)\)-th submodel \(f_{_{R+1}}\) generates the final prediction. These \(R+1\) submodels are simultaneously trained, with \(\{f_{_{r}}\}_{r=1}^{R}\) learning from \(R\) sets of labels while \(f_{_{R+1}}\) concurrently training from the refined data.

## 3 Method

In this paper, we present a novel framework called collaborative refining for learning from inaccurate labels (CRL). Within the framework, _annotator agreement_ is utilized to partition the samples into two categories, i.e., samples where annotators disagree or agree. This concept facilitates the development of targeted strategies for each category.

### Collaborative Refining Framework

Based on the _annotator agreement_, the whole dataset can be partitioned into two parts: \(D_{d}\) which contains samples where some annotators disagree, i.e., \( r_{0},r_{1}\{1,...,R\}\), \(_{r_{0}}^{r_{0}}_{i}^{r_{1}}\); \(D_{a}\) which contains samples where all annotators agree, i.e., \( r_{0},r_{1}\{1,...,R\}\), \(_{i}^{r_{0}}=_{i}^{r_{1}}\). As mentioned in the introduction, for \(D_{d}\), the labels are ambiguous, and learning from all sets of labels may degrade the model's performance. To this end, we propose Label Refining for samples with Disagreements (LRD) to select relatively reliable labels. For \(D_{a}\), where labels are more reliable but not immune to errors. To address this, we present Robust Union Selection (RUS) to select trustworthy samples.

In the framework, through LRD and RUS, \(R\) submodels \(\{f_{_{r}}\}_{r=1}^{R}\) collaboratively refine unreliable dataset \(D_{d}\) and \(D_{a}\) into relatively reliable dataset \(D_{d}^{*}\) and \(D_{a}^{*}\). \(D_{d}^{*}\) and \(D_{a}^{*}\) are utilized to train the final submodel \(f_{_{R+1}}\) with binary cross-entropy loss function as shown in Eq. (1). For prediction, \(f_{_{R+1}}\) is utilized. Note that RUS and LRD operate concurrently and share an identical set of \(R+1\) submodels. Our framework improves model performance exclusively by refining information, which is independent of any particular backbone, allowing for seamless substitution and flexibility in the model structure. The complete CRL framework is shown in Algorithm 1.

### Label Refining for Samples with Disagreements

When some annotators disagree, learning from all sets of labels may degrade the model's performance. A rational idea is to figure out which label is more reliable. Through theoretical analysis, we uncover the relationships among multiple sets of labels, the ground-truth label, and the submodels' predictions for a given sample, which in turn guide the design of the algorithm.

We follow the widely used class-conditional noise assumption [10; 26], i.e., \(p(^{r}|y^{*},)=p(^{r}|y^{*}), r\{1,,R\}\). Under this assumption, the noise transition matrix for the \(r\)-th annotator can be formulated as \(T^{r}^{2 2}\), where \(T^{r}_{ij}=p(^{r}=j|y^{*}=i)\) denotes the probability of an \(i\)-th class sample flipped into the \(j\)-th class for the \(r\)-th annotator.

Inspired by Gui et al. , we give the following theorem about the relationships among multiple sets of labels, the ground-truth label, and the submodels' predictions, proof can be found in Appendix A.1. Assume neural networks \(f_{_{0}},f_{_{1}}\) are used to minimize the expected loss using labels from two annotators respectively.

**Theorem 1**.: _Let \((,y^{*},^{0},^{1})\) be any sample with ground-truth label \(y^{*}\) and two conflicting labels \(^{0}\), \(^{1}\) from two annotators, i.e., \(^{0}^{1}\). Assume \(T^{0}\) and \(T^{1}\) satisfy \(T^{0}_{ii}>0.5\) and \(T^{1}_{ii}>0.5\), \( i\{0,1\}\), \((f_{_{0}^{*}}(),^{0})<(f_{_{1}^{*}}(), ^{1})\) if and only if \(y^{*}=^{0}\)._

**Remark.** Theorem 1 indicates that when the diagonal elements of two noise transition matrices are greater than 0.5, if two models are trained with these two sets of labels, for a sample \(\) on which these two annotators disagree, the more reliable label can be selected by comparing the loss values in their respective models.

Then, the above theorem from the scenario of two annotators can be expanded into multiple annotators, proof can be found in Appendix A.2. Assume a series of neural networks \(\{f_{_{r}}\}_{r=1}^{R}\) are used to minimize the expected loss using labels from \(R\) annotators respectively.

**Corollary 1**.: _Let \((,y^{*},\{^{r}\}_{r=1}^{R})\) be any sample with ground-truth label \(y^{*}\) and \(R\) conflicting labels \(\{^{r}\}_{r=1}^{R}\) from \(R\) annotators, i.e., \(_{0},r_{1}\{1,...,R\}\), \(^{r_{0}}^{r_{1}}\). Assume \(T^{r}_{i}>0.5\), \( i\{0,1\}\) and \(r\{1,...,R\}\), if \((f_{^{*}_{i}}(),^{k})=(\{(f_{^{*}_{r}}( ),^{r})\}_{r=1}^{R})\), \(y^{*}=^{k}\)._

**Remark**.: Corollary 1 indicates that if \(R\) models are trained with \(R\) sets of labels, when the observed labels of sample \(\) are not the same, we can get the most reliable label for this sample by choosing the label which has the smallest loss value.

Corollary 1 encourages us to infer the most reliable labels of the samples where some annotators disagree. Based on Theorem 1 and Corollary 1, we propose our LRD method to deal with \(D_{d}\). As mentioned in the preliminaries, \(R\) single-label submodels are trained. The training loss for these \(R\) submodels can be written as:

\[_{BCE}=_{i=0}^{N}_{r=1}^{R}w^{r}^{r}_ {i}(f_{_{r}}(_{i}))+(1-^{r}_{i})(1-f_{_{r} }(_{i})),\] (2)

where \(w^{r}\) is a weighting factor introduced to address the predictive bias for positive and negative samples which may arise due to sample imbalance. Denote the number of positive labels in \(r\)-th set of labels as \(n^{r}_{1}\) and that of negative labels as \(n^{r}_{0}\), a common strategy for setting \(w^{r}\) is \(w^{r}=_{0}}{n^{r}_{1}}\).

Through these submodels, we identify the most reliable labels by comparing their respective loss values over the same sample. Based on Corollary 1, for sample \((_{i},\{^{r}_{i}\}_{r=1}^{R}) D_{d}\), we get the refined label \(^{*}_{i}\) by

\[^{*}_{i}=^{k}_{i},\] (3)

where the most reliable index \(k\) for sample \(_{i}\) is acquired through:

\[k=*{argmin}_{r}\{(f_{_{r}}(_{i}),^{r}_ {i})\}_{r=1}^{R}.\] (4)

In this way, for any instance \((_{i},\{^{r}_{i}\}_{r=1}^{R})\), we can refine it into \((_{i},^{*}_{i})\). Then we can construct the refined dataset \(D^{*}_{d}\) and utilize it to train the submodel \(f_{_{R+1}}\) with the binary cross-entropy loss. This submodel can be deployed and utilized for final prediction. In practice, these refined labels are held constant after several training epochs to mitigate the over-fitting issue. The procedure of LRD is included in Algorithm 1. Note that LRD and RUS operate concurrently, with LRD focusing on \(D_{d}\) and RUS on \(D_{a}\).

### Robust Union Selection

For samples where all annotators agree, the labels are generally presumed to be reliable, however, they are not immune to errors. For instance, if the annotators have similar defects, although the annotators agree, the labels may still be inaccurate. To this end, guided by theoretical bounds on loss values, we introduce a loss-based selection criterion and modify it to be more robust against potentially problematic values.

Inspired by the small-loss selection criterion mentioned in many works under single noise label scenario [11; 13; 23], small-loss data tends to be more clean. However, relying solely on the prediction provided by one of the submodels is unstable. Once the selection is wrong, the inferiority of accumulated errors will arise . Since we have \(R\) predictions for each sample with \(R\) sets of labels from our model, naturally, average loss values can be utilized, which can be denoted as:

\[_{i}=_{r=1}^{R}(f_{_{r}}(_{i}), ^{r}_{i}),\] (5)

which is more stable and serves as an estimation of the mean \(\). Thanks to the \(R\) sets of labels provided by \(R\) annotators, these submodels can be diverse, which can ease the inferiority of accumulated errors .

However, simply taking the average can also lose some correctly labeled samples. This issue originates from two aspects: _annotator defects_ and _hard samples_. Firstly, different annotators may differ in quality or be suitable for different kinds of samples, which leads to the situation where some submodels may struggle with prediction and incur a larger loss for some correctly labeled samples.

Secondly, some correctly labeled samples may be difficult to learn, as a result, submodels may give unstable predictions to them. Note that this issue is avoided in LRD since we use the minimum loss value of the same sample across submodels there. To address this issue, a robust criterion for sample selection is defined.

We begin by defining a non-decreasing smooth function \(\) for loss values, which can be written as:

\[(z)=(1+z+}{2}),\] (6)

where \(z\) is a positive variable, this function is inspired by the Taylor expansion of the exponential function. Loss values can be easily substituted into this equation to serve as an _soft estimation_ of the underlying mean \(\). Eq. (6) can reduce the side effect of extremum, which can mitigate _annotator defects_ and _hard samples_ issues. Based on Eq. (6), the robust average loss \(_{i}^{}\) can be written as:

\[_{i}^{}=_{r=1}^{R}((f_{_{r}}( _{i}),_{i}^{r})).\] (7)

To enhance the robustness of \(_{i}^{}\), the predictions from historical models are introduced. Denote the submodel's parameters \(_{r}\) in the \(t\)-th epoch as \(_{r}^{t}\), the chosen set of epochs as \(T\), then Eq. (7) can be rewritten as follows:

\[_{i}^{}=_{r=1}^{R}_{t T}((f_{ _{r}^{t}}(_{i}),_{i}^{r})),\] (8)

where \(|T|\) represents the total number of the selected epochs, for instance, we can choose the latest five epochs or some fixed epochs as \(T\) during the training process.

Based on the soft estimation \(_{i}^{}\) of the underlying mean \(\), we further introduce the lower bound of the underlying mean \(\) to give correctly labeled samples which are discarded due to _annotator defects_ or because they are _hard samples_ a chance to be selected. Inspired by Xia et al. , we give the following theorem about the lower bound. The proof is similar to Theorem 1 in .

**Theorem 2**.: _Let \(\{z_{j}\}_{j=1}^{n}\) be an observation set with mean \(\) and variance \(^{2}\). By utilize a non-decreasing function \((z)=(1+z+}{2})\), we have_

\[_{j=1}^{n}(z_{j})-(n+(2n)}{n^{2}})}{n-^{2}},\] (9)

_with probability at least \(1-\)._

**Remark.** This theorem defines the lower bound of the mean of an observation set. The first term in the lower bound is a robust average, which can reduce the side effects of extremum. The second term is related to the number of observed values and the variance.

Based on Theorem 2, our selection criterion \(c_{i}\) for \(_{i}\) is defined as:

\[c_{i}=_{i}^{}-_{i}^{2}(n+_{i}^{2}(2n)}{n^{2}})}{n-_{i}^{2}}\] (10)

where \(n\) is the number of observed loss values for \(_{i}\), i.e., \(n=R|T|\) as shown in Eq. (8), \(_{i}^{2}\) is the variance of observed loss values for \(_{i}\). Since we don't know the distribution of the loss values for \(_{i}\) in submodels, \(_{i}^{2}\) serves as an approximation of the latent true variance.

Generally speaking, the second term in Eq. (10) gives samples with relatively large variance a chance to be selected for training. If we use the normal average loss, as shown in Eq. (5), these samples may be discarded due to _annotator defects_ or because they are _hard samples_. Thanks to Eq. (10), they can be included in the selection; if these samples consistently yield large loss values throughout the training process and across the majority of submodels, they will be discarded in subsequent epochs.

By using the selection criterion in Eq. (10), we can sort the samples in \(D_{a}\) and select the smallest \(p\) proportion to form \(D_{a}^{*}\), which can be utilized to train \(f_{_{R+1}}\). In practice, positive and negative samples are selected separately. The process of RUS is included in Algorithm 1.

Note that RUS and LRD operate concurrently, and they share an identical set of \(R+1\) submodels which are trained simultaneously. In RUS, the \(R\) submodels \(\{f_{_{r}}\}_{r=1}^{R}\) collaboratively refine \(D_{a}\), while in LRD, these submodels are used to refine \(D_{d}\). \(f_{_{R+1}}\) learns from the refined samples.

Experiment

### Dataset

**Benchmark datasets.** All the methods are evaluated on \(13\) benchmark datasets with two kinds of noise, including five NLP datasets named Agnews, 20News, IMDb, Yelp and Amazon, four tabular datasets named Diabetes, Backdoor, Campaign and Waveform, and four image datasets named Celeba, SVHN, Fashion-MNIST (denote as F-MNIST) and CIFAR-10. Note that these datasets only have ground-truth labels, we simulate three annotators per dataset with label quality \(k=0.3\) through the following two methods:

* **Instance-dependent noise.** Inspired by Zhao et al. , \(k\) proportion of each dataset is used to train three sets of diverse tree-based models to act as rule-based annotators, i.e., Decision Tree, RandomForest, LightGBM. Their predictions are used as multiple sets of labels.
* **Class-dependent noise.** The labels of the positive samples are randomly preserved at proportions of \(k\), \(k+0.1\), and \(k+0.2\) respectively, while interchanging the labels for the remaining positive samples with those of negative samples, thus establishing three sets of labels. Note that these three sets of labels are generated independently.

**Real-world datasets.** Experiments are also conducted on two real-world datasets: CIFAR-10N and Sentiment. Both datasets were published on Amazon Mechanical Turk for annotation.

Details of these datasets and labels can be found in Appendix B.

### Compared Methods and Implementation Details

We compare our framework with various methods: (1) Single, in which a three-layer MLP is trained directly with one set of labels; (2) NN-Mjv , in which a three-layer MLP is trained using the labels aggregated through majority voting; (3) HE_A and HE_M , which train several individual models for each set of inaccurate labels and aggregate their outputs for predictions by averaging and maximizing; (4) CL , which trains an end-to-end model with parametric source-specific transition matrices; (5) DN , which exploits information from multiple sets of labels with different softmax output layers; (6) Label aggregation methods called Enhanced Bayesian Classifier Combination (EBCC) and Independent Bayesian Classifier Combination (IBCC) , three-layer MLPs are trained using the labels inferred by EBCC or IBCC; (7) Weakly supervised end-to-end learner WeSEL ; (8) CoNAL , which assumes that the annotation noise is attributed to two sources (common noise and individual noise), and combines these two noise by a Bernoulli random variable; (9) SLF , which proposes to make the weight vectors and the confusion matrices data-dependent, and comes up with two regularization methods for the confusion matrix to guide the training process; (10) ADMoE , which leverages the Mixture of Experts (MoE) architecture to encourage specialized learning from multiple noisy sources. Note that for a fair comparison, we do not use the noisy-label aware gating in ADMoE which inputs labels during both training and testing.

Most of the compared methods are adopted from their respective codebases. The hyperparameters are set according to the recommendations in their papers. The only modification we made to ensure a fair comparison is to use a three-layer MLP with a hidden dimension of 128 as the backbone.

For our method, as mentioned in the preliminaries, we use a three-layer MLP of hidden dimension 128 to act as shared layers, and \(R+1\) three-layer MLPs of hidden dimension 128 to serve as \(R+1\) submodels. The \((R+1)\)-th submodel \(f_{_{R+1}}\) is utilized for final prediction. There is no hyperparameter in LRD. For RUS, we set the proportion of selected samples \(p=0.8\), and take the 5th epoch and the latest epoch during training as the selected epochs in Eq.( 8). In practice, LRD-generated labels are held constant after 5 training epochs to mitigate the over-fitting issue.

For all of the methods, experiments are conducted with 0.001 learning rate, 100 training epochs, and 256 batch size on MLP with hidden dimension 128 for a fair comparison. For benchmark datasets, 70% of each dataset is utilized for training, 5% for validation, and 25% for testing. For real-world datasets, we use the original training and testing datasets. We report the average AUC of the last 10 epochs as results.

[MISSING_PAGE_FAIL:7]

Through these experiments, we demonstrate the effectiveness of LRD and RUS. These results also indicate that adding LRD to the baseline improves the performance more noticeably than when adding RUS. It is reasonable because LRD is designed to correct labels in \(D_{d}\), while RUS is designed to select samples in \(D_{a}\). As is illustrated before, the samples in \(D_{d}\) are more ambiguous and the samples in \(D_{a}\) are more likely to be correctly labeled, thus greater improvement can be achieved by effectively dealing with \(D_{d}\).

**Quality analysis of the refined data.** Further experiments are conducted to explore how LRD and RUS work. Since the LRD method is designed to aggregate multiple sets of labels into a reliable one, we conduct experiments to show the label quality produced by LRD. We train our model for 5 epochs and collect the labels aggregated by the LRD algorithm on \(D_{d}\). Then we compare the quality of these labels with the three sets of original labels and the labels obtained through voting. The experiments are conducted on thirteen datasets mentioned above with class-dependent noise. The results are shown in Figure 2, the average AUC of these labels is taken as the metric. LRD can consistently provide higher-quality labels. Naturally, our model trained with these labels yields better results than with the original labels.

Since RUS is designed to select samples, experiments are conducted to show the quality of the selected samples. The model is trained for 5 epochs then the selected samples are fetched. RUS is compared with the following two methods: (1) selecting the same proportion of samples randomly; (2) replacing the criterion in RUS with the mean of the loss values in Eq. (5) to select the same proportion of samples. The proportion of selection \(p\) is set as \(0.8\). Since the total number of ground-truth positive samples stays the same, we compare the number of true positive samples selected by the three methods mentioned above. These experiments are repeated for 100 times and the average improvements over random selection are presented in Figure 3. As shown in Figure 3, on the four datasets including image, NLP, and tabular data, our proposed method has a better performance in selecting samples. Naturally, our model trained with these high-quality samples can get better results.

**Further verification of LRD-generated labels.** The quality of LRD-generated labels is shown in the foregoing discussion in Figure 2, whereby we demonstrate the effectiveness of LRD after training models for five epochs. We further examine LRD's performance in the early training stages. We train

    &  \\   & Single & NN-Miy & HE\_A & HE\_M & CL & DN & NN-EBCC & NN-IBCC & WeaSEL & SLF & CoNAL & ADMoE & Ours \\  Sentiment & 0.712 & 0.727 & 0.744 & 0.730 & 0.724 & 0.732 & 0.728 & 0.736 & 0.730 & 0.686 & 0.741 & 0.722 & **0.753** \\ CIFAR-10N & 0.791 & 0.853 & 0.788 & 0.786 & 0.788 & 0.807 & 0.850 & 0.849 & 0.851 & 0.821 & 0.816 & 0.761 & **0.866** \\   

Table 2: Real-world results with AUC as the evaluation metric. The best results are in bold.

Figure 1: AUC comparison under different label quality \(k\).

our model on datasets with class-dependent noise for 100 / 500 steps and examine the qualities of LRD-generated labels on \(D_{d}\). As shown in Table 4, in the early stages of LRD learning (e.g., 100 steps), the LRD-generated labels already outperform the original labels. With ongoing training, label quality rises (e.g., 500 steps). When LRD-generated labels are frozen at step 100, the final average AUC on these four datasets is 0.835, which is quite close to the average AUC of 0.845 obtained by freezing LRD at step 500. These results indicate that we have considerable flexibility in choosing when to fix the refined labels. In the main results, the LRD-generated labels are frozen after training for five epochs.

**Cooperating with other algorithms.** As mentioned before, our framework tends to collaboratively refine the dataset into a higher-quality one, which is independent of the model architecture. We simply utilize a MLP to learn from the refined dataset, which can be replaced by other algorithms. We train our model for 5 epochs, extract the refined dataset, and complement it with the original three sets of labels to form a new dataset. Then this new dataset is utilized to train SLF, CoNAL, and ADMoE in the same manner as in the main experiments. As shown in Table 5, when cooperating with our framework, ADMoE, SLF, and CoNAL achieve improvement over their original versions, which demonstrates that our framework can cooperate well with the existing methods.

## 5 Conclusion and Future Work

In this paper, we present a framework called Collaborative Refining for Learning from inaccurate labels (CRL), which focuses on learning a model from multiple sets of inaccurate labels. In our framework, we utilize the _annotator agreement_ to assess the reliability of labels from multiple sets with disparities and split the dataset into two parts: where some annotators disagree and where all annotators agree. For samples where some annotators disagree, Label Refining for samples with Disagreements (LRD) is proposed to select relatively reliable labels. Based on a theoretical analysis of the relationships among multiple label sets, ground-truth labels, and model predictions, LRD identifies the most reliable label by comparing the loss values. For samples where all annotators agree, Robust Union Selection (RUS) is proposed to select trustworthy samples to form a higher-quality dataset. Guided by theoretical bounds on loss values, RUS introduces a loss-based selection criterion and improves it to be more robust against potentially problematic values. Meanwhile, the refined datasets are used to train a lightweight model that can be deployed and utilized for final prediction. Extensive experiments are conducted to demonstrate the effectiveness of the framework. The framework is designed to be independent of any specific model architecture, making it compatible with most existing methods, which is confirmed by further experiments.

This work currently focuses on the binary classification task with the existence of inaccurate labels, which is fundamental and commonly encountered in practice. This work also lays the foundation for subsequent research of multi-class classification tasks with inaccurate labels. However, in the multi-class scenario, the proposed method needs to be further adapted. For instance, an issue arises with samples where annotators disagree, particularly when all the given labels for a particular sample are incorrect in the multi-class setting. LRD tends to select one of these incorrect labels as the inferred label, negatively impacting the model's performance. In binary classification problems, this situation is naturally avoided because when annotators disagree, one of the labels must be correct. A possible solution could be to develop a more refined measure based on annotator agreement, which could be utilized to decide whether to discard these samples, using LRD or RUS.

In the current approach, the samples are segmented based on the presence or absence of label disagreement, and further processes are performed on the correlated segments. We believe that the sample segmentation strategy can be further improved. To achieve this, we can introduce a new metric: the consistency rate, defined as the proportion of agreement among annotators for the same sample. By establishing a threshold for this consistency rate, we can effectively partition the dataset. Specifically, for samples with a consistency rate exceeding the threshold, RUS is applied, whereas for samples with a consistency rate below the threshold, LRD is utilized. The determination of this threshold is related to the label qualities of the dataset and the number of label sets available, which is suitable for practical applications. Since these ideas are not yet mature enough, we did not include these parts in the current manuscript. We leave these issues for future research.