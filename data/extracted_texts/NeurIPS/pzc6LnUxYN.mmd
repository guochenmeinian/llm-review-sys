# StateMask: Explaining Deep Reinforcement Learning through State Mask

Zelei Cheng

Northwestern University

Evanston, IL 60208

zelei.cheng@northwestern.edu

&Xian Wu

Northwestern University

Evanston, IL 60208

xianwu2024@u.northwestern.edu

&Jiahao Yu

Northwestern University

Evanston, IL 60208

jiahao.yu@northwestern.edu

&Wenhai Sun

Purdue University

West Lafayette, IN 47907

sun841@purdue.edu

&Wenbo Guo

Purdue University

West Lafayette, IN 47907

henrygvb@purdue.edu

&Xinyu Xing

Northwestern University

Evanston, IL 60208

xinyu.xing@northwestern.edu

Equal Contribution.

###### Abstract

Despite the promising performance of deep reinforcement learning (DRL) agents in many challenging scenarios, the black-box nature of these agents greatly limits their applications in critical domains. Prior research has proposed several explanation techniques to understand the deep learning-based policies in RL. Most existing methods explain why an agent takes individual actions rather than pinpointing the critical steps to its final reward. To fill this gap, we propose StateMask, a novel method to identify the states most critical to the agent's final reward. The high-level idea of StateMask is to learn a mask net that blinds a target agent and forces it to take random actions at some steps without compromising the agent's performance. Through careful design, we can theoretically ensure that the masked agent performs similarly to the original agent. We evaluate StateMask in various popular RL environments and show its superiority over existing explainers in explanation fidelity. We also show that StateMask has better utilities, such as launching adversarial attacks and patching policy errors.

## 1 Introduction

Deep reinforcement learning has shown promising performance in a variety of challenging scenarios, such as allowing a robot to learn a control policy directly from the camera or sensor inputs  or training an agent to master Go games . However, DRL policies typically lack explainability, which is intrinsically difficult for humans to comprehend and establish trust. To broaden the adoption of DRL-based applications in critical fields, there is a pressing need to increase the transparency of DRL agents through explanation.

Prior research has proposed various methods to derive an explanation for DRL. For example, researchers utilized the saliency map (_e.g.,_) and model approximation methods(_e.g.,_[14; 15; 16; 17]) to identify which regions in an agent's observation are most critical to the agent's current action. Considering the nature of DRL, researchers also explored the relationship between the agent's states and its final reward. Take the work  as an example. Researchers demonstrated that the state-reward relationship could also be used as an explanation for DRL agent developers. Besides, they also showed that pinpointing the critical time steps towards the final reward could help agent developers better understand agent behaviors and even troubleshoot policy defects.

In this work, our research focuses on the DRL explanation methods that explore the state-reward relationships. We evaluate the existing DRL explanation approaches in this category on a range of tasks from simple simulated environments to sophisticated real-world applications, and from normal-form games to extensive-form games. We discovered that existing state-reward-based DRL explanation methods provide relatively low fidelity in state-reward relationship identification for various reasons. For example, the method proposed in  suffers from low explanation fidelity because of the limited capacity of their proposed approximation model and the error introduced in the final reward approximation. The methods introduced by  and  experience low fidelity because an agent's value function does not always reflect the state importance to its final reward.

To this end, we propose StateMask2, a novel method to pinpoint the time steps or, in other words, states most critical to a DRL agent's expected total reward. At a high level, StateMask works as follows. For a target agent, we train a mask net that blinds the agent to take random actions at certain time steps. If a time step is critical, the mask net does not blind the target agent. The agent takes action as its policy suggests. In contrast, if the time step is less critical, the mask net blinds the agent. The agent takes random actions. The rationale behind this design is that a DRL agent's final reward is not equally important for each time step. Randomly taking some actions at non-critical time steps has no impact on the agent's final reward. Technically, to learn the mask net, we carefully design an objective function that minimizes the performance difference between the original agent and the masked agent. We demonstrate, both theoretically and empirically, that performance differences could monotonically decrease during training.

While other explanation methods could track down the time steps most or least critical to an agent's final reward, this is the first work that could identify an agent's critical time steps with high fidelity across various types of RL environments, especially sophisticated normal-form games and extensive-form games. Evaluating StateMask in various game environments, we show that the explanation derived from our method demonstrates significantly higher fidelity than those produced by existing explanation techniques. Compared with the existing methods, StateMask allows humans to understand the agent's behaviors better and calibrate appropriate trust. Agent users can easily pinpoint an agent's policy defects and patch the identified defeat under the guidance of explanations. Last but not least, security professionals can launch adversarial attacks using the explanation from our method.

In summary, this paper makes the following contributions.

* StateMask and provide a theoretical analysis. We show that an agent armed with explainability is equivalent to the original agent in terms of agent performance.
* We evaluate StateMask on 10 different tasks ranging from simple normal-form games (_e.g.,_ Pong) to sophisticated normal-form games (_e.g.,_ StarCraft II) and extensive-form games (_e.g.,_ Connect 4). We show that StateMask can significantly outperform existing explainers.
* We demonstrate that our explanation can be highly helpful for understanding agent behavior, launching adversarial attacks, and even remediating the agent's errors.

## 2 Related Work

**DRL Explanation.** Existing DRL explanation techniques can be mainly categorized into two classes based on their different explanation objectives: (1) the observation-action-based method pinpointing which regions in an agent's observation are most important to its action at an individual time step; 2) the state-reward-based method tracking down which time steps have the most critical influence on the agent's final reward.

Recall that a DRL agent's policy network takes as input the agent's current observation and outputs the corresponding action. In this setup, pinpointing regions in an observation most critical to the corresponding action is the same as identifying the most important regions in a deep neural network's (DNN) input. As such, existing works along this direction mainly utilize or extend the explanation methods developed for DNN classifiers to derive such explanations. Technically, they mainly leverage three lines of methods: post-hoc white-box explanation methods [21; 22; 23; 24], post-hoc black-box explanation methods [15; 25; 26; 27], and self-explainable methods [28; 29; 30; 31]. In addition, another line of work identifies important regions by decomposing the agent's instant reward to regions in its current observation and selecting the regions with high rewards [32; 33; 34; 35].

Regarding the second type of explanation methods, most of the techniques identify the critical steps based on the value function [19; 20; 36]. Given that the value function measures the goodness of the current state towards the agent's expected total reward, it is natural to use it for critical time step selection. Recently, Guo _et al._ proposed EDGE that establishes state-reward relationship by collecting a set of trajectories and then approximating an explanation model offline. However, as discussed in , EDGE has two limitations. First, the approximation method is based on a statistical model not effective in deriving explanations for agents that usually generate long trajectories. Second, for each new trajectory, EDGE needs to build an independent approximation model, which significantly limits its efficiency and scalability.

Our proposed technique falls into the second type of explanation, which follows a different design path to overcome the limitations above. As we will discuss later, our method is superior to existing techniques in multiple aspects: higher explanation fidelity (Section 4), better utilities (Section 5), as well as better generalizability (to extensive-form games) and practicability (in the cases where the agent's value function/network is not available).

**Perturbation-based Explanation.** Perturbation techniques have been widely utilized within the field of computer vision to gain a deeper understanding of black-box image classifier [37; 38; 39; 40]. These approaches involve introducing minor modifications to the target image and observing how the classifier's outputs change, which enables us to identify the regions of greatest importance within the original image. Our explanation method shares a similar idea with these perturbation methods. Rather than explaining supervised learning (_e.g.,_ image classifier), our focus is on interpreting a DRL agent by introducing perturbations in non-critical time steps. This temporal perspective provides unique insights into the DRL agent, which has not been explored in prior research.

## 3 Key Technique

In the following, we describe the rationales behind our design and then discuss the key technical challenges, followed by the technical details of tackling the challenges.

### Overview

**Assumptions.** To ensure the practicability of our method, we do not assume access to the agent's value function or policy network. Instead, we only assume having access to the agent's observations and the ability to query the agent's policy and thereby obtain corresponding actions. Note that our explanation method does not change the target agent's policy. For a multi-agent environment, we explain each agent individually.

**Design Rationale.** To tackle the limitations confronted by existing state-reward-based explanation methods, our idea is to parameterize the importance of the agent's current time step as a neural network model (denoted as state mask). The state mask net takes as input the current state and outputs its importance to the agent's final reward. The _key challenge_ of this solution is to propose a proper criterion or metric to decide the importance of the current time step and thus design an objective function to train this state mask. In our work, we draw insights from existing counterfactual-based explanation methods designed for supervised classifiers [37; 41; 42; 43]. At a high level, these methods identify important features in an input sample by nullifying or perturbing the features and observing the corresponding changes in the classification output. They deem the features triggering the largest prediction changes as the most important ones. Similarly, in our problem, we can decide whether a time step is important by randomizing the agent's action at a certain time step and observing changes in its final reward. A small variation in the final reward indicates the reward is not sensitive to the action change, further implying the low importance of the state. In contrast, a large reward change infers the high importance of the corresponding state.

To elaborate on this idea, we use a pong game as an example. As shown in Figure 1(a), after the DRL agent launches the ball and when the ball is moving towards its opponent, the agent has a few time steps of freedom to choose random actions. This is because the agent's actions at these steps will not change the movement of the ball and will be less likely to influence the opponent's actions as well (given the opponent's focus is on the ball). As a result, forcing the DRL agent to take random actions at these steps will likely have no influence on the game's final result and thus trigger only minor changes to the agent's final reward. However, if we randomly perturb the DRL agent's action at the time steps when the agent is about to touch the ball (Figure 1(b)), it will likely cause the agent to fail and thus trigger a large variation in the agent's final reward.

**State Mask Modeling.** Guided by this idea, we design the state mask that takes the current state as input and outputs the probability of whether to randomize the agent's action at the current state. The objective of this state mask is to blind a target agent and thus vary its actions at certain states in a trajectory without changing the agent's final reward. This probability can be taken as the importance of the current state (_i.e.,_ the higher the probability is, the lower the importance is). This design introduces _a challenge_. That is how to decide whether varying the action at the current state will influence the agent's future reward.

To address this challenge, we model it as an RL problem, where we fix the target agent's policy and treat it as part of the environment. Then, we introduce another agent to decide whether to randomize the target agent's action at each step or not. Its policy can be modeled as the state mask above. This agent's goal is to preserve the target agent's total reward while randomizing actions at more states.

Following this idea, we propose a novel explanation framework, StateMask. Figure 2 illustrates this explanation system. As mentioned above, we add a state mask (agent) to the environment. We fit the current state \(s_{t}\) into both the state mask \(\) and the target agent \(\). For a given state \(s_{t}\), the state mask outputs a binary action \(a_{t}^{e}\) of either "zero" or "one", and the target agent will sample the action \(a_{t}\) from its policy. The final action is determined by the following equation

\[a_{t} a_{t}^{e}=a_{t},&a_{t}^{e}=0\,,\\ a_{}&a_{t}^{e}=1\,,\] (1)

where \(a_{t}^{e}=0\) represents keeping the target agent's current action. On the contrary, \(a_{t}^{e}=1\) refers to replacing the target agent's current action with a random action \(a_{}\) uniformly sampled from its action space. Note that \(a_{}\) is different from \(a_{t}\).

The goal of our state mask is to find the non-important time steps and randomize their actions without changing the expected total reward of the target agent. Formally, it can be expressed as 3

\[J()=|()-()|\,,\] (2)

where \(\) denotes the policy of the target agent and \(\) denotes the policy of the perturbed agent (_i.e.,_ integration of the state mask \(\) and target agent \(\)). \(()\) is the expected total reward of an agent by following a certain policy. By fixing \(\), \(()\) is also fixed and \(()\) depends only on the parameters

Figure 1: Illustration of the Pong game. The DRL agent controls the yellow (right) paddle and a non-RL rule-based opponent controls the white (left) paddle.

Figure 2: The StateMask framework.

of the state mask. As a result, by solving Eqn. (2), we can obtain a state mask. By applying this resolved mask to each state, we will be able to assess the state importance at any time step.

**Key Optimization Challenges and Solutions.** Obtaining a non-trivial solution of Eqn. (2) presents the following challenge. To ensure the optimization function is differentiable, we need to decompose and remove the absolute value operation. A straightforward solution is to compare the value of \(()\) and \(()\) during each training iteration. If \(()\) is larger than \(()\), we update the state mask by minimizing \(()\). Otherwise, we maximize \(()\). With simple reformulation, we can then leverage the state-of-art policy gradient method - Proximal Policy Optimization (PPO)  to update the state mask in each iteration. However, the key limitation of this solution is the lack of convergence guarantee. Suppose the policy of the perturbed agent \(\) is near a local optimal and \(()\) is less than \(()\). Then, updating the perturbed policy by maximizing \(()\) may lead to an overly large \(()\), which, in turn, requires minimizing \(()\). When this situation occurs, the perturbed policy \(\) may oscillate around the local optimal without actually converging. To address this problem, we borrow the idea of TRPO technique , carefully designing a surrogate objective function for Eqn. (2). As we will elaborate below, through the surrogate objective function, we can guarantee the difference between \(()\) and \(()\) (_i.e.,_\(|()-()|\)) monotonically decreases during the training process, and our optimization process will at least converge to a local optimal rather than oscillating around it.

### Technical Details

In this section, we formally introduce our proposed StateMask framework, including modeling the state mask and designing the surrogate objective function to ensure monotonicity.

**Defining the State Mask.** An infinite-horizon Markov Decision Process (MDP) is defined as \(\{,,P,R,\}\), where \(\) and \(\) are the state and action space. \(P:()\) is the state transition function. \(R:\) is the reward function and \((0,1)\) is the discount factor. A policy \(\) is a mapping from \(\) to a probability distribution over \(\). The expected total reward of a policy \(\) is defined as \(()=_{s_{0},a_{0},...}[_{t=0}^{}^{t}R(s_{ t},a_{t})]\).

Based on this definition, we define our explanation as an Explanation Markov Decision Process problem (E-MDP), in which we formalize state mask as an explanation policy \(:(^{e})\), where \(^{e}=\{0,1\}\). With this definition, the integration of the explanation policy \(\) and the target policy \(\) (that we aim to explain) then becomes \((|s)=(a^{e}=0|s)(|s)+(a^{e}=1|s) a_{}\), where \(a_{}(|s)\).

As mentioned above, we fix \(\). In this way, we can transform the E-MDP into a single-player environment, where the state-transition, reward function, and \(()\) depend only on the policy of the state mask \(\). As such, we can redefine \(()\) as \(()\). The goal here is to train an optimal policy \(\) to minimize the difference between \(()\) and \(()\).

**Solving the Explanation Policy.** With the definitions above, the objective function in Eqn. (2) could be reformulated as \(J()=|(_{})-()|\), where \(\) refers to the parameters of the explanation policy. As is discussed in Section 3.1, to ensure the monotonic property, we subtly design a surrogate objective function to solve \(\). To introduce our designed objective function, we first present the following lemma.

**Lemma 1**.: _() The difference between \(L_{_{_{old}}}(_{})\) and \((_{})\) is bounded by_

\[|(_{})-L_{_{_{old}}}(_{} )| C^{}(_{_{old}}( s)\| _{}( s))\,.\] (3)

\(_{}\) denotes the new policy, updated from \(_{_{old}}\). \(L_{_{_{old}}}(_{})=(_{_ {old}})+_{s}_{_{_{old}}}(s)_{a^{e}} _{}(a^{e} s)A_{_{_{old}}}(s,a^{e})\), where \(_{_{_{old}}}(s)=_{t}^{t}P(s_{t}=s| _{_{old}})\) and \(A_{_{_{old}}}\) is the advantage function of \(_{_{old}}\). \(^{}(_{_{old}}( s)\|_ {}( s))=_{s}(_{_{old}}(  s)\|_{}( s))\) and \(C\) is a constant.

Based on Lemma 1, we can derive the following theorem (The derivation is in Supplement S1).

**Theorem 1**.: _The following inequalities are the sufficient conditions for satisfying \(|(_{})-()||(_{old})-( )|\)._

\[& L_{_{old}}(_{})( _{_{old}})+C^{}(_{_{old}}(  s)\|_{}( s))\,,\\ & L_{_{_{old}}}(_{})+C ^{}(_{_{old}}( s)\|_{ }( s)) 2()-(_{_{old}})\,. \] (4)Based on Theorem 1, we propose the following objective function to update \(_{}\) from \(_{_{old}}\).

\[_{} L_{_{_{old}}}(_{})\] (5) s.t. \[L_{_{_{old}}}(_{}) 2( )-(_{_{old}})-,\] \[_{a^{c}_{}}[a^{c}] c,_{s_{_{_{old}}}}[(_{_{old}}( s)\|_{}( s))]\, \,.\]

where \(_{a^{c}_{}}[a^{c}] c\) constrains the lower bound sparsity for the output of \(_{}\) across all states, with \(c\) as a constant hyper-parameter. It rules out the trivial solution where the state mask outputs \(0\) at any state. As detailed in Supplement S2, a policy \(_{}\) solved from Eqn. (5) satisfies the conditions in Eqn. (4) and thus enables the desired monotonicity.

Eqn. (5) can be further transformed into the following function based on the Lagrangian duality.

\[_{ 0}_{} L_{_{_{old}}}(_{})-(L_{ _{_{old}}}(_{})-2()+( _{_{old}})+)\,,\] (6) s.t. \[_{a^{c}_{}}[a^{c}] c \,,_{s_{_{_{old}}}}[( {}_{_{old}}\|_{})]\,.\]

Below we further discuss our techniques for solving Eqn. (6). In particular, we first follow the PPO algorithm  and transform the Eqn. (6) into the following objective function for \(\) (See Supplement S3 for more details)

\[_{}[(1-)(_{}(a_{t}^{c} s_{t})}{_{_{old}} (a_{t}^{c} s_{t})}A_{_{_{old}}}, (_{}(a_{t}^{c} s_{t})}{_ {_{old}}(a_{t}^{c} s_{t})},1-,1+)A_ {_{_{old}}})+wL_{t}^{}].\] (7)

Here, \(A_{_{_{old}}}=A_{_{_{old}}}(s_{t},a_{t}^{c})\) and \(L_{t}^{}=_{a_{t}^{c}_{}}[a_{t} ^{c}]\,.\) sgn is the sign function which returns \(1\) or \(-1\) based on whether the input is positive or negative. \(w\) is a hyperparameter controlling the weight of the corresponding term during optimization. The outer expectation is taken over a finite batch of state and action, sampled by running the current policy \(_{_{old}}\).

Similarly, we can also transform Eqn. (6) into the following objective function with respect to \(\)

\[_{ 0}(2()-(_{_{old}})- -L_{_{_{old}}}(_{}))\,.\] (8)

We follow the primal-dual method [46; 47] and iteratively update \(\) and \(\) by solving Eqn. (7) and Eqn. (8) using the gradient-based optimization method. Algorithm 1 briefly presents our learning process. Supplement S4 shows a more detailed algorithm including how to estimate \(()\) and \(A_{_{_{old}}}\).

``` Input: Target agent's policy \(\) and the estimation of its expected total reward \(()\) Output: Mask net \(_{}\) Initialization: Initialize the weights \(\) for the mask net \(_{}\) for iteration=1, 2,... do  Run the current policy \(_{_{old}}\) in the environment for \(T\) steps and get estimation for \(A_{_{_{old}}}\) Solve the objective function with respect to \(\) in Eqn. (7) and update \(_{old}\)  Solve the objective function with respect to \(\) in Eqn. (8) and update \(_{old}\) endfor ```

**Algorithm 1** The learning algorithm for training the mask net.

As mentioned above, as an explanation method, we fix the policy of the target agent in the environment and only train our state mask. For a multi-agent environment, we can learn an individual state mask for each target agent. This ensures the generalizability of our method to multi-agent setups. As specified in Supplement S4, with minor implementation modifications, we can also generalize our method from normal-form games, where the agents observe the same state and take action simultaneously, to extensive-form agents, where the agents make decisions in sequential order.

## 4 Evaluation

In the following, we start with our experiment setup and design, followed by experiment results and analysis. To ensure the reproducibility of our results, we specify the evaluation details in Supplement S5.

### Experiment Setup

**Environment Selection.** We select _10_ representative environments to demonstrate the effectiveness of StateMask across four types of environments: simple normal-form game (CartPole, Pendulum, and Pong ), sophisticated normal-form game (You-Shall-Not-Pass , Kick-And-Defend , and StarCraft II ), perfect-information (simple) extensive-form game (Connect 4, Tic-Tac-Toe and Breakthrough ), and imperfect-information (sophisticated) extensive-form game (DouDizhu ).

**Baseline Selection.** Sharing the same explanation goal, as discussed in Section 2, two existing methods explore the state-reward relationship - the value-function-based method and EDGE . For the value-function-based method, we implement two methods - Value-max method [19; 20] and LazyMDP . The value-max method associates the state importance with its value function \(V(s)\). LazyMDP pinpoints critical steps according to the lazy gap, which is defined as \(_{a}Q(s,a)-_{a}[Q(s,a)]\). Here, \(Q(s,a)\) is the action-value function. For EDGE, we implement it based on their code . We take an absolute value for the important scores drawn by these methods to align their meaning with our methods.

### Experiment Design

**Experiment I: Expected Reward Preservation.** Recall the state mask blinds the agent and forces it to take random actions at some least important time steps. If the blinding is accurate, the agent's performance should not be affected, and the agent's final reward should not vary significantly. As a result, one of our evaluations is to examine whether the masked agent could preserve its expected total reward. To do so, for a target agent from a game, we compute its discounted total reward across 500 rounds. Then, we treat the averaged computation result as the estimation of the agent's expected total reward \(()\). Following the computation of the expected reward, we further train our StateMask in the corresponding game.

As mentioned above, the combination of the state mask and the target agent's policy is the perturbed policy. Therefore, after having the state mask in hand, we further compute the discounted total reward of the perturbed policy across 500 rounds. We treat the averaged computation result as the approximation of \((_{})\), and then use it to compute the relative value variation \()-(_{})|}{|()|}\). As is shown from this equation, the relative value variation represents the change of the reward. If this measure is low, it indicates that the state mask has a minimal impact upon the target agent's performance. The state mask accurately pinpoints the time step least important to the final reward.

**Experiment II: Explanation Fidelity.** Apart from evaluating our explanation method based on the measure of expected reward preservation, we also employ a fidelity test proposed in  to assess its effectiveness. The fidelity test involves using the corresponding explanation method to identify and rank the most crucial time steps in a given trajectory. Subsequently, a continuous sequence of time steps is selected based on the rank of the time steps. The fidelity test evaluation algorithm proposed in  ensures that the selected sequence includes the maximum number of critical steps, indicating the most crucial continuous time steps leading to the final reward of the agent. Additionally, the length of the sequence can be specified as an input to the fidelity test evaluation algorithm. For instance, one can limit the sequence to only the top \(K\) time steps. For more information about the fidelity test, readers can refer to Supplement S5.

Following the rest of the fidelity test introduced in , for a given trajectory and the most critical sequence in it, we let the agent fast-forward to the beginning of the critical sequence. Then, we force the target agent to take random actions until the end of the critical sequence. Later onwards, we use the target agent's policy to complete the game. By doing so, we simulate a situation where we force the agent not to follow its policy at the most critical time steps. If the time steps are truly critical to the final reward, we expect the action replacement at critical time steps could introduce the highest reward variation compared with replacing any other continuous sequence containing \(K\) time steps. In , Guo _et al._ introduce a fidelity score to measure the change of the reward after the replacement of the actions at a continuous sequence of time steps. The higher the measure is, the higher the explanation fidelity is.

In this work, we use StateMask (the state masks trained above) and baseline methods to explain 500 trajectories of each target agent and obtain the time step importance for each trajectory. Then, for each trajectory, we select their top \(K\) important steps based on the importance ranking given by eachmethod and compute the fidelity score. We compute the average score across 500 trajectories as the fidelity score for that explanation method. We set \(K=10\%,20\%,30\%,40\%\) and report the fidelity of the selected methods under each setup. We repeat each experiment 3 times with different random seeds and report the mean and standard deviation.

### Experiment Result

Due to the space limit, we show the results of five games: Pong, You-Shall-Not-Pass, Connect 4, DouDizhu, and StarCraft II. Supplement S5 shows the other games' results, which are aligned with those discussed in this section.

**Relative Value Errors.** The maximum/minimum/mean relative value errors across all 10 games are 14.98%/0.00%/5.17% (The exact values are in Supplement S5). These results verify the effectiveness of our design objective function (Eqn (6)) in preserving the target agent's performance. In addition, we also show the trends and variations in \(|()-(_{})|\) during the training process for each game. The figures in Supplement S5 demonstrate that our design could indeed achieve the monotonical property, and further improve the stability of the training process.

**Fidelity Scores.** Figure 3 shows the fidelity scores of StateMask against the three baseline approaches in five games while the results of the remaining five games could be found in Supplement S5. First, we can observe that our method has the highest fidelity scores across all games in all settings, indicating that StateMask provides more faithful explanations for the target agents than the baseline approaches. Second, we discover that in two complex normal-form and extensive-form games (DouDizhu and StarCraft II), StateMask has a significant advantage over EDGE. This result validates our argument in Section 2 that EDGE's approximation fails to handle long trajectories. Besides, we also discover that StateMask outperforms value-based methods in those two games. We believe that the reason is that the value function is trained to facilitate policy learning and typically tends to output similar values for a sequence of continuous states , causing the time step importance to be less distinguishable. While StateMask specifically quantifies the importance of each time step via action randomization, giving more specific and distinguishable important scores.

In addition to the experiments above, we also demonstrate our method's efficiency and its insensitivity against the variations in the key hyper-parameters. We also verify the effectiveness of our method on not well-trained policies with sub-optimal performance. Due to the space limit, we detail these experiments in the supplementary materials. We further conduct an additional comparison to verify the necessity of minimizing the difference in the agent's expected total reward before and after applying the state mask. We consider an alternative design that directly maximizes the agent's expected total reward after applying the state mask (_i.e.,_\(J()=_{}(_{})\)) and compares it with our method. Our results show that StateMask produces higher fidelity explanations on not well-trained policies than this alternative method (See Supplement S10 for more details).

Figure 4: Visualization of the identified critical time steps in Pong game. The DRL agent controls the yellow paddle and a non-RL rule-based opponent controls the white paddle.

Figure 3: Fidelity scores of explanations generated by selected methods in 5 games. The x-axis represents the choice of \(K\). “VALUE” stands for Value-max method, and “LAZY” stands for LazyMDP. The black line refers to the standard deviation of 3 runs. The bar refers to the mean and the line on the bar represents the standard deviation.

## 5 Utility of Explanation

In this section, we demonstrate that with a higher explanation fidelity, StateMask could also achieve better utilities in three aspects: understanding agent behaviors, launching adversarial attacks, and patching agent errors.

Understanding Agent Behaviors.We visualize the identified critical steps in all 10 games to demonstrate how StateMask helps humans understand a DRL agent's behavior. Due to the space limit, we show the results of two games: Pong (normal-form game), Connect 4 (perfect-information extensive-form game), and leave the rest in Supplement S6.

Regarding the Pong game as Figure 4 shows, our explanation method highlights the frames when the ball is approaching the yellow paddle as the most important time steps whereas the states when the ball flies to the white paddle as the least important time steps. It aligns with humans' intuition since the steps when the ball is flying towards the yellow paddle are apparently critical to the final rewards. Without proper actions at these critical time steps, the DRL agent will miss the ball and lose the game.

In Connect 4, two players take turns dropping colored stones onto a grid to connect four stones in a line. Figure 5 visualizes one game where the black player wins. StateMask pinpoints the first time step (where the target agent makes the first move) as an important step. Allis _et al._ shows that placing a stone in the center column as the first action typically brings an advantage. StateMask also highlights the last two time steps, which drive the target agent to win the game. 4

Additionally, we conduct a user study 5 to compare our explanation model with baseline methods (Value-max, EDGE, and LazyMDP) and see which one could help humans gain a better understanding of a DRL agent's behavior. Our study is determined to be exempt from an IRB review by the IRB panel. In our user-study survey, the participants are first given some background information about this project and asked for their consent to proceed. The hypothesis to be tested is that our method can offer a better explanation of the target agent's behavior in comparison with baseline methods. We use the convenience sampling method  to recruit 41 participants for our study. The participants come from a variety of backgrounds in explainable reinforcement learning (XRL), with 20% having never heard of XRL, 51% having some knowledge of it, and 29% being an expert or having published papers in this field. Next, the participants are presented with two groups of videos of the Kick-And-Defend game. The videos record the agent's play and highlight the critical time steps based on different explanation methods. Additionally, these two groups include trajectories played by near-optimal and sub-optimal agents. The participants are instructed to watch these two groups of videos and choose which explanation best matched their intuition.

  Applications & Games & EDGE & Value-max & LazyMDP & Ours \\   & Pong & -63.04 (3.44) & -0.40 (0.40) & -36.17 (1.15) & **-84.90 (1.40)** \\   & You-Shall-Not-Pass & -2.64 (0.60) & -14.80 (3.20) & -28.87 (1.21) & **-35.93 (1.81)** \\   & Connect 4 & -33.33 (1.56) & -26.30 (1.44) & -22.57 (1.40) & **-35.37 (3.00)** \\   & DoulDizhu & -0.25 (1.71) & -18.00 (0.31) & -25.20 (0.76) & **-29.94 (0.64)** \\   & StarCraft II & -7.00 (4.148) & -77.33 (5.69) & -72.00 (13.53) & **-83.00 (7.21)** \\   & Pong & +1.13 (0.12) & +0.53 (0.12) & +1.47 (0.12) & **+1.87 (0.12)** \\   & You-Shall-Not-Pass & +0.07 (0.12) & +1.07 (0.12) & +0.12 (0.12) & **+2.42 (0.18)** \\   & Connect 4 & +2.00 (0.00) & +1.47 (0.12) & +1.33 (0.12) & **+2.40 (0.20)** \\   & DoulDizhu & +0.00 (0.00) & +0.13 (0.12) & +2.00 (0.00) & **+0.47 (0.12)** \\  

Table 1: The target agent’s performance in different applications. Numbers before the brackets are means and those in the brackets are standard deviations.

Figure 5: Visualization of the identified critical steps in the Connect 4 game. The black player is our explanation target player. The states where the blue player takes actions are marked with a gray bar.

According to the survey results in Figure 6, 71% of the participants prefer our explanation method when the agent is near-optimal and 59% of them favor our explanation method when the agent is sub-optimal. Since the majority of participants choose our method, our hypothesis is confirmed. Our method outperforms the baseline methods in helping the user understand a DRL agent's policy.

**Launching Adversarial Attacks.** We follow the method developed in EDGE  and launch attacks against the target agent based on explanations drawn by the selected methods. Specifically, we fix the percent of time steps under attack \(T\) as 10% and launch this attack against 500 episodes. We then compute the average reward that the agent gathers before and after the attack. Table 1 shows the results in five games (same as the games in Section 4.3). Supplement S7 further shows additional results when varying the attack ratio from 10% to 30% in all ten games. As shown in the figure, the adversarial attack under the guidance of all explanation methods could dramatically drop the victim agent's performance, indicating the effectiveness of the interpretation and the exploitability of the attack. However, the performance losses observed on the adversarial attacks under the guidance of the three baseline methods are significantly lower than what we observed on the attack enabled by our method. This again implies that the attack guided by our method is more effective, and thus our interpretation has higher fidelity than the three baseline methods.

**Patching Agent Errors.** Rather than directly applying the rule-based patching method in EDGE , we propose a novel learning-based method (Supplement S8 shows the advantages of our learning-based method over the rule-based one proposed in EDGE). Given a set of trajectories, we first leverage selected explanation methods to identify a sequence of continuous time steps that are most critical to the final reward in each trajectory. Then, we replay these trajectories to the starting point of the critical steps and fine-tune the agent's policy till the game end. We validate our patching method on seven games (except CartPole, Breakthrough, and StarCraft II whose agent has a nearly optimal performance) and report the results in Table 1 and Supplement S8. As shown in the table, our method-guided agent achieves the best performance after retraining.

## 6 Conclusion and Discussion

In this work, we propose StateMask, a novel explanation method for DRL agents. The use of StateMask as an explainer can reveal the state-reward relationship with exceptional accuracy. Compared to other types of explainers, StateMask offers the highest level of fidelity in explanation. By providing such precise explanations, we can assist humans in comprehending agent behaviors, launching adversarial attacks, and even identifying and correcting policy errors.

Our work suggests several promising directions for future research. First, in safety-critical environments, masking the agent's state and forcing it to take random actions may not be feasible due to high exploration costs. However, to train DRL agents in such environments, the model developer typically needs to build a simulator for the environment, which can be used to generate explanations. As part of future work, we will explore building simulators for such an environment if unavailable. Second, although StateMask achieves high fidelity, it may not identify cases where positively and negatively important states in the same trajectory offset each other's influence on the final reward. In the future, we will investigate additional steps to handle such cases. Third, while our explanation methods can identify important states, presenting the states to humans may not be ideal. Other future efforts will also include converting critical states to human-understandable strategies. Finally, our study in Section 5 shows the feasibility of explanation-guided policy fine-tuning. We plan to improve the fine-tuning efficiency and the generalizability of the retrained DRL agents in the future.

Figure 6: Results of our user study.