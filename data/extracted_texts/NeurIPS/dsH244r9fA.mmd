# Counterfactual-Augmented Importance Sampling

for Semi-Offline Policy Evaluation

Shengpu Tang, Jenna Wiens

Computer Science & Engineering

University of Michigan, Ann Arbor, MI, USA

{tangsp,wiensj}@umich.edu

Reviewed on OpenReview: https://openreview.net/forum?id=dsH244t9fA

###### Abstract

In applying reinforcement learning (RL) to high-stakes domains, quantitative and qualitative evaluation using observational data can help practitioners understand the generalization performance of new policies. However, this type of off-policy evaluation (OPE) is inherently limited since offline data may not reflect the distribution shifts resulting from the application of new policies. On the other hand, online evaluation by collecting rollouts according to the new policy is often infeasible, as deploying new policies in these domains can be unsafe. In this work, we propose a semi-offline evaluation framework as an intermediate step between offline and online evaluation, where human users provide annotations of unobserved counterfactual trajectories. While tempting to simply augment existing data with such annotations, we show that this naive approach can lead to biased results. Instead, we design a new family of OPE estimators based on importance sampling (IS) and a novel weighting scheme that incorporate counterfactual annotations without introducing additional bias. We analyze the theoretical properties of our approach, showing its potential to reduce both bias and variance compared to standard IS estimators. Our analyses reveal important practical considerations for handling biased, noisy, or missing annotations. In a series of proof-of-concept experiments involving bandits and a healthcare-inspired simulator, we demonstrate that our approach outperforms purely offline IS estimators and is robust to imperfect annotations. Our framework, combined with principled human-centered design of annotation solicitation, can enable the application of RL in high-stakes domains.

Figure 1: Left - The state transition diagram of a tree MDP with 3 steps and 2 actions. States are denoted by \(\), actions are denoted by arrows \(\{,\}\), rewards are denoted in red and given only at terminal transitions. Center - The behavior policy takes a specific sequence of actions and leads to the factual trajectory, leaving the rest of the state-action space with poor support. Right - The counterfactual annotations provided by human annotators (indicated by \(\)) capture information (in this example, the terminal reward under any policy that takes \((,)\) for the second and third steps) about support-deficient regions of the state-action space not visited by the behavior policy.

Introduction

Reinforcement learning (RL) has gained popularity in recent years for its ability to solve sequential decision-making problems in various domains [1; 2; 3; 4; 5; 6; 7]. Despite these successes, it remains challenging to deploy and use RL in highly consequential or safety-critical domains, such as healthcare, education, and public policy [8; 9; 10; 11; 12]. One of the major roadblocks that distinguishes RL-based systems from their supervised learning counterparts is evaluation.

Evaluation of supervised learning models often involves calculating prediction accuracy against a labeled test set . In contrast, evaluation of RL policies is less straightforward and often involves interacting with the environment [2; 3; 14; 15; 16; 17; 18; 19]. For domains that lack accurate simulators, this means deploying new policies in the real environment. For instance, in healthcare, online evaluation would require clinicians to follow RL recommendations in selecting treatments for real patients. While mathematically sound, this presents clear safety issues and potential disruptions to workflows. Therefore, most work in these areas has relied exclusively on retrospective evaluations using observational data [20; 21; 22], focusing on both quantitative and qualitative aspects. Quantitative evaluations make use of statistical off-policy evaluation (OPE) methods to account for the distribution shift resulting from the application of new policies [23; 24; 25]. Despite their wide use, OPE is fundamentally limited by the available offline data. In particular, past work has noted that unexpected bias and large variance  among other reasons make these approaches unreliable [22; 26]. On the other hand, qualitative evaluations typically aim to verify with domain experts whether the RL recommendations are reasonable, but are difficult to standardize and may be susceptible to confirmation bias .

In this work, we consider an intermediate step before prospective deployment that improves upon offline evaluation of RL policies. Specifically, we assume human domain experts can provide annotations of unobserved counterfactual trajectories that are small deviations of the observed trajectory (Figure 1), where each annotation is some summary of the expected outcomes of counterfactual trajectories. For example, in healthcare domains, such annotations may be obtained by asking clinicians what they think would happen to the patient if a different treatment were to be used. Intuitively, these counterfactual annotations can make up for regions of the state-action space with poor support in the offline dataset. However, as we demonstrate, simply adding the annotations as new trajectories to the offline dataset will change the state distribution and lead to biased results. Thus, we design a new OPE estimator based on importance sampling (IS) that incorporates both the offline factual data and counterfactual annotations without introducing additional bias. We analyze the theoretical properties of our proposed estimator, noting its advantages over standard IS. Specifically, our estimator requires a weaker condition on support to achieve unbiasedness and has the potential to reduce variance. Through a series of proof-of-concept experiments using toy problems and a healthcare-inspired simulator, we show the benefits of our approach in making use of counterfactual annotations to enable better evaluations of RL policies, even when annotations are biased, noisy, or missing. Our semi-offline evaluation framework represents an important step that complements offline evaluations by providing additional confidence in RL policies.

## 2 Problem Setup

We consider Markov decision processes (MDPs) defined by a tuple \(=(,,P,R,d_{1},,T)\), where \(\) and \(\) are the state and action spaces, \(P:()\) and \(R:()\) are the transition and reward functions, \(d_{1}()\) is the initial state distribution, \(\) is the discount factor, \(T^{+}\) is the fixed horizon. \(p(s^{}|s,a)\) denotes the probability density function of \(P\), and \((s,a)\) denotes the expected reward. A policy \(:()\) specifies a mapping from each state to a probability distribution over actions. A \(T\)-step trajectory following policy \(\) is denoted by \(=[(s_{t},a_{t},r_{t})]_{t=1}^{T}\) where \(s_{1} d_{1},a_{t}(s_{t}),r_{t} R(s_{t},a_{t}),s_{t+1} p(s_{ t},a_{t})\). Here, \(a(s)\) is short for \(a(|s)\) and \(s^{} p(s,a)\) for \(s^{} p(|s,a)\). Let \(J=_{t=1}^{T}^{t-1}r_{t}\) denote the return of the trajectory, which is the discounted sum of rewards. The value of a policy \(\) is the expected return, defined as \(v()=_{}[J]\). The value function of policy \(\), denoted by \(V^{}:\), maps each state to the expected return starting from that state following policy \(\). Similarly, the action-value function (i.e., the Q-function), \(Q^{}:\), is defined by further restricting the action taken from the starting state. Formally, \(V^{}(s)=_{}[J|s_{1}=s]\), and \(Q^{}(s,a)=_{}[J|s_{1}=s,a_{1}=a]\). We also consider value functions at specific horizons: \(V^{}_{t:T}(s)=_{}[_{t^{}=t}^{T}^{t^{}-1}r _{t^{}}|s_{t}=s]\), and \(Q^{}_{t:T}(s,a)=_{}[_{t^{}=t}^{T}^{t^{}-1 }r_{t^{}}|s_{t}=s,a_{t}=a]\). Throughout the paper we also consider the non-sequential, bandit setting with horizon \(T=1\). In this case, a "trajectory" (or, a sample) is denoted by \(=(s,a,r)\) where we omit the time step subscript.

Our goal is to estimate \(v(_{e})\), the value of an evaluation policy \(_{e}\), given data that were previously collected by some behavior policy \(_{b}\) in the same environment defined by \(\). Let \(=\{^{(i)}\}_{i=1}^{N}\) denote the dataset containing \(N\) independent trajectories drawn according to \(_{b}\) and \(\).

**OPE.** The typical approach to this problem relies on off-policy evaluation (OPE). Importance sampling (IS) is a common OPE approach that reweights samples based on how likely they are to occur under \(_{e}\) relative to \(_{b}\). Given a trajectory \(\), the 1-step and cumulative IS ratios are defined as \(_{t}=(a_{t}|s_{t})}{_{b}(a_{t}|s_{t})}\) and \(_{1:t}=_{t^{}=1}^{t}_{t^{}}\). The per-decision IS estimator, \(^{}=_{t=1}^{T}_{1:t}^{t-1}r_{t}\), is an unbiased estimator of \(v(_{e})\)[27; 28]. We also consider its recursive definition: \(^{}=v_{T}\) where \(v_{0}=0\), \(v_{T-t+1}=_{t}(r_{t}+ v_{T-t})\). In this paper, we discuss the properties of IS-based estimators over a single trajectory; our results naturally generalize to dataset \(\) containing \(N\) trajectories where the final estimator is the average over trajectories. For the bandit setting, we refer to PDIS simply as the IS estimator, \(^{}= r=(a|s)}{_{b}(a|s)}r\).

**Counterfactual Annotations.** In addition to the offline dataset \(\), our semi-offline framework assumes access to accompanying counterfactual annotations. To introduce the notation, we start with the non-sequential, bandit setting where \(T=1\), dropping the time step subscripts. Given a factual sample \(=(s,a,r)\), let \(c^{}\{0,1\}\) be a binary indicator for whether the counterfactual action \(\{a\}\) is associated with an annotation, and let the annotation be \(g^{}\). We use \(G:()\) to denote the annotation function such that \(g^{} G(s,)\). A counterfactual-augmented sample \(^{}=(,)\) consists of the factual sample \(\) and counterfactual annotations \(=\{g^{}:c^{}=1\}\), where each \(g^{} G(s,)\). Intuitively, a "good" annotation should reflect the scenario where the counterfactual action \(\) is taken and the reward \(g^{} R(s,)\) is observed.

**Assumption 1** (Perfect annotation, bandit).: \(_{g G(s,a)}[g]=(s,a), s,a\).

For the sequential setting, we define the corresponding notation with time step subscripts: for \((s_{t},a_{t},r_{t})\) occurring at step \(t\) of trajectory \(\), we define counterfactual indicators \(c^{}_{t}\) for \(\{a_{t}\}\) and annotations \(_{t}=\{g^{}_{t}:c^{}_{t}=1\}\). Figure 2 provides an example trajectory with counterfactual annotations. Here, each \(g^{}_{t} G_{t}(s_{t},)\) is drawn from the horizon-\(t\) annotation function \(G_{t}\). While the general notion of counterfactual annotations could be used to capture different information (e.g., the instantaneous reward of the counterfactual action, \(R(s,)\)), in this work, we study a specific version that allows us to extend the theory of the bandit setting. Specifically, the annotation for counterfactual action \(\) summarizes the annotator's belief of the expected future return (sum of rewards) in the remaining \(T-t+1\) steps after taking the counterfactual action \(\) from state \(s_{t}\), and then following the evaluation policy \(_{e}\). In other words, the annotation plays the same role as the Q-function. This leads to a more refined assumption on the horizon-specific annotation function \(G_{t}\).

**Assumption 2** (Perfect annotation, MDP).: \(_{g G_{t}(s,a)}[g]=Q^{_{e}}_{t:T}(s,a), s,a\).

Under Assumption 2, if we obtained _infinitely many_ annotations for _all_ initial states and _all_ actions, then evaluation becomes trivial (we essentially recover the Q-function of all initial states). However, we consider the non-asymptotic regime where not every annotation is available, as certain annotations might be difficult to obtain. For example, annotating initial states requires reasoning about the full horizon \(T\). Furthermore, since this is a rather strong assumption (we need different annotations for each \(_{e}\)), later we explore a relaxation where the annotations reflect the behavior policy \(_{b}\) instead.

## 3 Methods

To motivate our approach, we begin with a didactic bandit example to illustrate how the naive incorporation of counterfactual annotations can yield biased estimates. In order to address this issue, we propose a modification of IS estimators that reweights the factual data and counterfactual annotations. We formally describe how this idea applies to IS (in the bandit setting) and PDIS (in the sequential RL setting), giving rise to a family of semi-offline counterfactual-augmented IS estimators. We study the impact of different assumptions regarding the annotations on the performance of our proposed estimators both theoretically (Section 4) and empirically (Section 5).

Figure 2: A trajectory augmented with counterfactual annotations, where the action space is \(=\{,,\}\). The factual trajectory \(\) is shown in black. Solid blue arrows indicate the counterfactual annotations were queried and obtained; dashed gray arrows indicate the annotations are not available. Each transition arrow is labeled with (action, value), where value is either an observed immediate reward or a counterfactual annotation.

### Intuition

Consider a one-step bandit (Figure 2(a)) with two states \(\{s_{1},s_{2}\}\) (drawn with equal probability) and two actions, up (\(\)) and down (\(\)). The reward from \(s_{1}\) is \(+1\) and from \(s_{2}\) is \(0\) (i.e., rewards do not depend on the action), meaning all policies have an expected reward of \(0.5\). Suppose the behavior policy always selects \(\), generating a dataset with poor support for policies that assign nonzero probabilities to \(\) (Figure 2(b)). Now suppose we also have access to human-provided annotations of counterfactual actions, but not all counterfactual annotations are available (either because they were never queried or the users declined to provide annotations). In our example (Figure 2(c)), one annotation is collected for the counterfactual action \(\) at state \(s_{1}\), indicating that the human annotator believes the reward for taking action \(\) from state \(s_{1}\) is \(+1\) (which is the true reward). To make use of this information, one might be tempted to add the counterfactual annotation as a new sample. The augmented dataset (Figure 2(d)) would allow us to evaluate policies (e.g., using IS) that assign non-zero probabilities to \(\) in state \(s_{1}\). While seemingly plausible, this naive approach inadvertently changes the state distribution and results in a dataset inconsistent with the original problem (it looks like state \(s_{1}\) is seen more often than reality). A quick calculation reveals that applying IS to this unweighted augmented dataset gives a biased estimate of a new policy as \(2/3\) instead of \(0.5\) (see Appendix B). To address this issue, in Section 3.2 we propose a new reweighting procedure that maintains the state distribution of the original dataset while incorporating counterfactual annotations.

### Augmenting IS Estimators with Counterfactual Annotations

To avoid the bias issue described in Section 3.1, informally, we want to split the contribution of each sample between the factual data and counterfactual annotations. Given a factual sample \((s,a,r)\) and the associated counterfactual annotations \(\), let \(=\{w^{a}\}\{w^{}:\{a\}\}\) be a set of user-defined non-negative weights that satisfy \(w^{a}+_{\{a\}}w^{}=1\). These weights specify how much we want the estimator to "listen" to the counterfactual annotations (\(w^{}\)) relative to the factual data (\(w^{a}\)). We restrict \(w^{}=0\) when \(c^{}=0\), i.e., non-zero weight is only allowed when the annotation is available. In general, one may assign different weights for each occurrence of \((s,a)\) (e.g., the counterfactual annotation is obtained for one instance but missing for another); let \((|s,a)=[w^{}]\) denote the average weight assigned to \(\) when the factual data is \((s,a)\) (see example in Appendix B.1). After reweighting, the state distribution is maintained (since the weights associated with each sample sum to \(1\)) but the state-conditional action distributions have changed; this "weighted" augmented dataset can be seen as if it was generated using a different behavior policy.

**Definition 1** (Augmented behavior policy).: \[_{b^{}}(a|s)=(a|s,a)_{b}(a|s)+_{ \{a\}}(a|s,)_{b}(|s).\]

Here, \(_{b^{}}(a|s)\) represents the probability that information about action \(a\) is observed for state \(s\) (similar to how an "average policy" may be defined for multiple behavior policies ), either as a factual action in the dataset, or as an annotated counterfactual action when some other action \(\) is the factual action. Next, we define our proposed estimators (for bandits) based on IS.

**Definition 2** (Counterfactual-augmented IS).: Given a counterfactual-augmented sample \(^{}=(,)\) and weights \(=\{w^{}:\}\), where \(=(s,a,r)\), \(=\{g^{}:c^{}=1\}\), the C-IS estimator is \(^{}=w^{a}^{a}r+_{\{a \}}w^{}^{}g^{}\), where \(^{}=(|s)}{_{b^{}}(a|s)}\) for each \(\).

The C-IS estimator is a weighted convex combination of the factual IS estimate \(^{a}r\) and the counterfactual IS estimates \(^{}g^{}\) for all counterfactual actions \(\{a\}\). We also study a special case where all annotations are available and the weights are split equally among actions, such that \(w^{a}=w^{}=1/||\). Then, \(_{b^{}}\) becomes the uniformly random policy, and after substituting into Definition 2, we obtain the following estimator.

Figure 3: (a) The state diagram of a bandit problem with two states and two actions. (b) A factual dataset containing two samples. (c) The factual samples augmented with counterfactual annotations. (d) The (unweighted) augmented dataset constructed from factual samples and counterfactual annotations. Compared to the original factual dataset, the relative frequency of \(s_{1}\) vs \(s_{2}\) has changed from \(1:1\) to \(2:1\).

**Definition 3** (C-IS with equal weights).: Given a counterfactual-augmented sample \(^{}=(,)\), the C*-IS estimator is \(^{}=_{e}(a|s)r+_{\{a _{t}\}}_{e}(|s)g^{}\).

_Remark_.: Definition 3 provides an alternative interpretation of the estimator when using equal weights: if Assumption 1 holds (i.e., the annotation function \(G\) is the true reward function \(R\)), we effectively observe both the factual and counterfactual rewards from \(R\). Then, we can directly use the definition of the value function to calculate the expected reward under \(_{e}\) using the action probabilities \(_{e}(|s)\).

For the sequential setting, given a trajectory with \(T\) steps, we define the collection of weights over all time steps, \(=\{w_{t}^{a_{t}}:t=1...T\}\{w_{t}^{}:\{a_{t}\},t=1...T\}\). The augmented behavior policy \(_{b^{}}\) is similarly defined (see Definition 1). By extending the recursive definition of PDIS, we obtain the following two estimators (assuming either arbitrary weights or equal weights).

**Definition 4** (Counterfactual-augmented PDIS).: Given a counterfactual-augmented trajectory \(^{}=(,)\) and weights \(\) as defined above, where \(=[(s_{t},a_{t},r_{t})]_{t=1}^{T}\), \(=\{g_{t}^{}:c_{t}^{}=1\}\), the C-PDIS estimator is \(^{}=v_{T}\), with \(v_{T}\) defined recursively as \(v_{0}=0\), \(v_{T-t+1}=w_{t}^{a_{t}}_{t}^{a_{t}}(r_{t}+ v_{T-t})+_{ \{a_{t}\}}w_{t}^{}_{t}^{}g^{ }\) for \(t=T...1\), where \(_{t}^{}=(|s_{t})}{_{b^{}}( |s_{t})}\) for each \(\).

**Definition 5** (C-PDIS with equal weights).: Given a counterfactual-augmented trajectory \(^{}=(,)\), the C*-PDIS estimator is \(^{}=v_{T}\), with \(v_{T}\) defined recursively as \(v_{0}=0\), \(v_{T-t+1}=_{e}(a_{t}|s_{t})(r_{t}+ v_{T-t})+_{ \{a_{t}\}}_{e}(|s_{t})g_{t}^{}\) for \(t=T...1\).

Next, we study the theoretical properties of our proposed estimators, relating their OPE performance (in terms of bias and variance) to assumptions on counterfactual annotations and the offline dataset.

## 4 Theoretical Analyses

We first present results for the bandit setting, where we study and compare the properties of the C-IS estimator with standard IS in terms of bias, variance, and the assumptions required, highlighting scenarios where bias and variance reduction is guaranteed. We then show how these results generalize to C-PDIS in the sequential RL setting. Finally, we discuss practical implications of the theoretical results. Full derivations are in Appendix C.

To begin, we review existing results for IS. Recall the following assumption of common support.

**Assumption 3** (Common support).: \(_{e}(a|s)>0_{b}(a|s)>0, s,a\).

If Assumption 3 holds, IS is unbiased (i.e., \(_{}[^{}]=v(_{e})\)), and its variance is :

\[[^{}]=_{s d_{1}}[V^{_{e}}(s)]+ _{s d_{1}}[_{a_{e}(s)}[(a|s)(s,a)]]+ _{s d_{1}}[_{a_{e}(s)}[(a|s)^{2}\,_{ R}(s,a)^{2}]]\] (1)

where \(_{R}(s,a)^{2}=_{r R(s,a)}[r]\) is the variance associated with the reward function \(R(s,a)\). The first term reflects the inherent randomness from the state distribution not related to importance sampling. The second term reflects the randomness in the behavior policy, whereas the third term reflects the randomness in rewards; these two terms are affected by the distribution of importance ratios \((a|s)\). When Assumption 3 is not satisfied, the IS estimator is biased , where the bias is related to actions with no support: \([^{}]=[^{}]-v(_{ e})=_{s d_{1}}[-_{a(s,_{b})}_{e}(a|s)(s,a)]\), with \((s,_{b})=\{a:_{b}(a|s)=0\}\) denoting the set of unsupported actions.

Intuitively, when Assumption 3 does not hold, the C-IS estimator can make use of information from the counterfactual annotations for unsupported actions, thereby reducing bias compared to IS (Section 4.1). For cases when IS is already unbiased, counterfactual annotations play the role of additional data and should help further reduce variance (Section 4.2).

### Bias Analyses for C-IS

To formalize the effect of counterfactual annotations on support, we state the following assumption.

**Assumption 4** (Common support with annotations).: \(_{e}(a|s)>0_{b^{}}(a|s)>0, s,a \).

Assumption 4 is a weaker version of Assumption 3, because \(_{b^{}}(a|s)>0\) requires either \((a|s,a)_{b}(a|s)>0\) (same as Assumption 3, assuming \((a|s,a) 0\)) or \((a|s,)_{b}(|s)>0\) for at least some \(\{a\}\). In other words, information about action \(a\) can be from either a factual sample or counterfactual annotations (recall Definition 1). Next, we state the main results for the bias of C-IS (unless specified otherwise, expectations are taken with respect to \(_{^{},}\)). These results hold for any nonzero \(\) and directly generalize to the special case of C*-IS where the weights are \(1/||\)

**Theorem 1** (Unbiasedness of C-IS).: _In the bandit setting, when both Assumptions 1 and 4 hold, the C-IS estimator is unbiased, \([^{}]=v(_{e})\)._

**Proposition 2** (Bias of C-IS due to support).: _When Assumption 1 holds but Assumption 4 is violated, \([^{}]=_{s d_{1}}- _{a(s,_{b^{}})}_{e}(a|s)(s,a)\) where \((s,_{b^{}})=\{a:_{b^{}}(a|s)=0\}\) are unsupported actions in the counterfactual-augmented dataset._

**Proposition 3** (Bias of C-IS due to imperfect annotations).: _When Assumption 4 holds but Assumption 1 is violated, \([^{}]=_{s d_{1}}_ {a_{e}(s)}_{W}(s,a)\;_{G}(s,a)\), where we measure violation of Assumption 1 as \(_{G}(s,a)=_{g G(s,a)}[g]-(s,a)\), and \(_{W}(s,a)=1-(a|s)}{_{b}(|s|s)}\)._

**Proposition 4** (A sufficient condition for bias reduction).: _If Assumption 1 holds (but Assumption 4 is violated), \((s,a) 0\) for all \(s,a\), and there exists \((s,a)\) such that \(_{b}(a|s)=0\), \(_{b^{}}(a|s)>0\), \(_{e}(a|s)>0\), \((s,a)>0\), then \(|[^{}]|<|[^{ }]|\)._

There are two sources of bias for C-IS: missing annotations contribute to the bias as the rewards of unsupported actions (Proposition 2), whereas imperfect annotations contribute to the bias as the annotation error over supported actions (Proposition 3). If both assumptions are violated, the resulting bias is the combination of the two (see C). If both assumptions hold, C-IS is unbiased (Theorem 1). Even when not all counterfactual annotations are collected (Assumption 4 is violated), C-IS can evaluate more policies without bias (assuming perfect annotations), because there is a larger space of policies "supported" by the counterfactual-augmented dataset. In particular, if there is at least one counterfactual annotation for an action with no support in the factual data, C-IS has less bias than IS (under mild conditions, Proposition 4). Lastly, we note the a useful corollary of Theorem 1.

**Corollary 5** (Expectation of augmented importance ratios).: _Let \(_{W}^{}=w^{a}^{a}+_{\{a\}}w^ {}^{}\) given \(\) and \(\). Under Assumption 4, \([_{W}^{}]=1\)._

_Remark_.: Corollary 5 suggests that for each sample, \(_{W}^{}\) plays a similar role as the standard importance ratio \(\) in IS, which may be used for calculating the effective sample size (ESS) . Naturally, we can also create a weighted version of our proposed estimators (e.g., C-WIS), with the normalization factor defined using \(_{W}^{}\).

### Variance Analyses for C-IS

Compared to the bias analyses above, the variance of C-IS has a more involved dependence on weights \(\) as well as the variance of the annotation function, \(_{G}(s,a)^{2}=_{g G(s,a)}[g]\). For clarity, we defer the full derivations to Appendix C.3; here, we present results for C*-IS where the weights are all set to \(1/||\) and the annotation function has the same variance as the reward function.

**Theorem 6** (Variance of C*-IS).: _Assuming \(_{G}(s,a)^{2}=_{R}(s,a)^{2}\), under Assumptions 1 and 4,_

\[[^{}]=_{s d_{1}}[V^{_{e}}(s)]+ _{s d_{1}}_{a_{b}(s)}_{b}(a|s)\,( a|s)^{2}\,_{R}(s,a)^{2}\] (2)

_where \((a|s)=(a|s)}{_{b}(a|s)}\) is the importance ratio under the original behavior policy._

**Proposition 7** (Variance Reduction of C*-IS).: _Under the premise of Theorem 6, \([^{}][^{}]\)._

Comparing Eqn. (2) with the three terms of the variance decomposition of IS in Eqn. (1), we note that the first term \(_{s d_{1}}[V^{_{e}}(s)]\) is identical, the dropped second term (of Eqn. (1)) is a non-negative variance term, and the third term is scaled by a factor \(_{b}(a|s) 1\) (for each instantiation of the expression inside the expectation), leading to a guaranteed variance reduction (Proposition 7). We derive the full variance decomposition for C-IS in Theorem 13. Unlike C*-IS, variance reduction is not guaranteed for C-IS. This is due to additional non-negative terms that depend on the variance/covariance of weights \(\) and terms that depend on the difference in variance between annotations and rewards \(_{G}(s,a)^{2}-_{R}(s,a)^{2}\) (could be positive or negative); these terms all vanish to zero in the case of C*-IS where weights are constant (\(1/||\)) and \(_{G}(s,a)^{2}=_{R}(s,a)^{2}\).

### Extensions to C-PDIS

We note that the corresponding results in the bandit setting can be derived for the MDP setting using an induction-style proof, with similar interpretations of the factors that contribute to reduced bias and reduced variance when compared to standard PDIS. Below, we briefly demonstrate how the unbiasedness result (Theorem 1) extends to the MDP setting. We further explore the sequential RL setting in the empirical experiments.

**Theorem 8** (Unbiasedness of C-PDIS).: _In the MDP setting, when both Assumptions 2 and 4 hold, the C-PDIS estimator is unbiased, \([^{}]=v(_{e})\)._

Proof Sketch.: Here, we explain the intuition behind the proof for C*-PDIS, which proceeds via a backward induction (Figure 4). In the recursive definition (Definition 4), we aim to show that every \(v_{T-t+1}\) is an unbiased estimator of the horizon-\(t\) value function. At each horizon \(t\), we can view the problem as a one-step bandit problem (reducing the estimator to C*-IS), where the factual action leads to a factual trajectory and the counterfactual action(s) leads to the counterfactual annotation(s), both of which are used to construct unbiased estimates of horizon-\(t\) Q-values (for \(a_{t}\) and \(\), respectively). In the end, the estimates from the two branches are combined according to \(_{e}\), resulting in the correct expectation of the horizon-\(t\) value of state \(s\). See full proof for C-PDIS in Appendix C.4. 

### Practical Implications

So far, we have been focusing on the theoretical framework for incorporating counterfactual annotations into OPE. However, the actual implementation of this approach poses several practical challenges. We believe this underscores the fact that this is a rich area for research with many potential directions. In this section, we address several real-world scenarios that do not adhere to the theoretical assumptions - specifically, when annotations are biased, noisy, or missing.

**Correcting annotation bias.** Comparing Assumptions 1 and 2, we note an important distinction between the bandit setting and the sequential RL setting. For bandits, we simply want \(G\) to mimic the reward model \(R\). In contrast, for the RL setting, \(G_{t}\) should ideally mimic the Q-function of the evaluation policy \(Q^{_{e}}_{t:T}\). Such annotations can be difficult if not impossible to obtain in practice (e.g., for healthcare, it would require asking clinicians to reason about a _sequence_ of counterfactual actions and predict the outcome). Thus, we additionally consider a relaxation of Assumption 2 where instead \(_{g G_{t}(s,a)}[g]=Q^{_{e}}_{t:T}\) reflecting expected return under the behavior policy (which is more likely in practice). This annotation bias \(_{G}=Q^{_{b}}_{t:T}-Q^{_{e}}_{t:T}\) results in a biased estimator (Proposition 3). To correct this bias, we suggest first estimating the annotation bias function \(_{G}(s,a)\) using an approximate MDP built from offline data, and then mapping each annotation as \(^{}=g^{}-_{G}(s,)\) (see Appendix D.1 for details). In Section 5.2, we empirically measure the impact of this alternative assumption and the bias-correction procedure (which we denote as \(Q^{_{b}}^{_{e}}\)) on OPE performance.

**Reweighting noisy annotations.** So long as Assumption 1 or 2 is satisfied, the noise (i.e., variance) of the annotations does not affect unbiasedness; however, as shown in Theorem 13, the variance of our proposed estimators directly depends on how noisy the annotations are. Intuitively, if the annotation variance is smaller than the reward variance, we want the final estimate to "listen" more to the annotations and less to the factual data (and vice versa). We empirically explore the impact of annotation noise in both Section 5 and Appendix E, noting that while using equal weights (as in C*-IS) outperforms standard IS, adjusting weights based on the relative magnitudes of \(^{2}_{R}\) and \(^{2}_{G}\) can further improve OPE performance.

**Inputing missing annotations.** For many real-world domains, it is unlikely that we can obtain an annotation for every counterfactual action at every time step (the total number of annotations needed is \((||-1)NT\)). While desirable to use equal weights as in C*-IS (and C*-PDIS) due to its variance reduction guarantee, this is not possible if some annotations are missing (in which case the factual data must have \(w^{a}=1\)), and this can actually lead to higher variance (see Appendix D.2 for an example). To alleviate this variance increase, we suggest estimating an annotation model \(\) from available annotations and using it to impute the missing annotations (see Appendix D.2). Although \(\) may be a biased estimator of \(G\) (due to annotation noise) and introduce additional bias to the final estimate, we empirically observe a favorable bias-variance trade-off (Section 5.2).

Figure 4: Illustration of proof idea. (i) The factual estimate \(v_{T-t}\) provides an unbiased estimate of the horizon-\((t+1)\) value of state \(s_{t+1}\). (ii) When combined with the factual reward \(r_{t}\), we obtain an unbiased estimate of the horizon-\(t\) Q-value of \((s_{t},a_{t})\). (iii) By assumption, the counterfactual annotations provide unbiased estimates of the horizon-\(t\) Q-value of \((s_{t},)\). (iv) The factual and counterfactual estimates are combined using \(_{e}\). For clarity, we omit details about state distributions here (see appendix).

In general, one would expect more counterfactual annotations to reduce both bias and variance; at the same time, these annotations may be imperfect (biased or noisy) and directly increase bias or variance. Our analyses in Section 4 and Appendix C show that both bias and variance depend on weights \(\), suggesting the weighting scheme as a key mechanism to achieve optimal bias and variance. In Appendix D.3 we explore an analytical approach for solving the variance-minimizing weighting scheme and note the solution is highly non-trivial. We empirically explore the impact of different weights in Appendix E.1 and note that using equal weights (as in C*-IS) is a promising heuristic, since it achieves good performance in most settings. We believe that optimizing the weights can further improve OPE performance and is an interesting direction for future work.

## 5 Experiments

First, through a suite of simple bandit problems, we verify the theoretical properties of C-IS. Then, we apply our approach to a healthcare-inspired RL simulation domain, where we compare the performance of our proposed approach, C-PDIS, to several baselines in terms of their OPE accuracy and ability to rank policies, and explore robustness to bias, noise, and missingness in the annotations.

### Synthetic Domains - Bandits

We consider a class of bandit problems with two states \(\{s_{1},s_{2}\}\) (drawn with equal probability), two actions \(=\{,\}\) (recall Figure 3), and corresponding reward distributions \(R(s_{i},a)(_{(s_{i},a)},^{2})\). Without loss of generality, we assume \(\) is always taken from \(s_{2}\) by both \(_{b}\) and \(_{e}\). For \(s_{1}\), we consider deterministic policies in which one action is always taken, and a stochastic policy that takes the two actions with equal probability (see column/row header in Table 1). Given \((_{b},_{e})\), we draw \(1,000\) samples following \(_{b}\) and then evaluate \(_{e}\) using various estimators, including standard IS, the naive baseline of adding counterfactual annotations as new samples (Section 3.1), and C*-IS. We assume that counterfactual annotations are only available for \(s_{1}\), and all annotations are drawn from the true reward function. We measure the bias, standard deviation (std, the square root of variance), and root mean-squared error (RMSE) of the estimators with respect to \(v(_{e})\).

**Naive baseline fails due to bias, whereas C*-IS can reduce bias and/or variance compared to IS.** In Table 1, we display the results for \(_{(s_{1},)}=1\), \(_{(s_{1},)}=2\), \(_{(s_{2},)}=1\), \(=0.5\) (other settings in Appendix E.1). The naive baseline fails to improve upon IS (and often underperforms IS in terms of RMSE) and can have a nonzero bias even when IS is unbiased (Table 1, row 2, column 2). C*-IS achieves a lower RMSE than IS across all settings considered. The benefits of C*-IS align with our theoretical analyses. (i) _Bias Reduction for Support-Deficient Data._ When \(_{b}\) is deterministic (first row), the unselected action has poor support and IS has a nonzero bias whereas C*-IS is unbiased. Though C*-IS sometimes has a larger variance than IS, the bias reduction outweighs the variance increase and leads to overall lower RMSE. (ii) _Variance Reduction for Well-Supported Data._ In the second row, data generated by \(_{b}\) has full support. Both IS and C*-IS are unbiased, but C*-IS leverages counterfactual annotations to achieve lower variance and lower RMSE.

We vary the assumptions (e.g., weights in C-IS, noisy/missing annotations) and present further experiments in Appendix E.1. These results suggest that (i) **Equal weights (in C*-IS) is a good heuristic though not always "optimal"** and (ii) **Imputing missing annotations can reduce variance**.

### Healthcare Domain - Sepsis Simulator

Next, we apply our approach to evaluate policies in a simulated RL domain modeled after the physiology of sepsis patients . Following prior work , we collected \(50\) offline datasets from the sepsis simulator (using different random seeds) each with \(1000\) episodes by following an \(\)-greedy behavior policy with respect to the optimal policy where \(=0.1\). We considered a set of deterministic policies (including the optimal policy) as evaluation policies, which have different performance and

   \(_{b}\)\(_{e}\) & \([\ 1\,\ 0\ ]\) & \([\ 0\,\ 1\ ]\) & \([0.5,0.5]\) \\   &  & \(-1\ 0.6\ 1.2\) & \(-0.5\ 0.5\ 0.7\) \\  & & & \(0.2\ 18.8\) & \(0.1\ 0.7\ 0.7\) \\  & & & \(0.7\ 0.7\) & \(0\ 0.5\ 0.5\) \\   & & \(0\ 0.9\ 0.9\) & \(0\ 1.6\ 1.6\) &  \\  & \(0\ 1\ 1\ 1\ 0.2\ 1.8\) & \(1.8\) & & \\   & \(0\ 0.5\ 0.5\) & \(0\ 0.7\ 0.7\) & & \\   

Table 1: Summary of performance on the bandit problem for various \(_{b}\) (rows) and \(_{e}\) (columns), where each policy is denoted by its probabilities assigned to the two actions from \(s_{1}\). Each cell of the table corresponds to a \((_{b},_{e})\) combination, for which we report (bias, std, RMSE) for three estimators: IS in the top row, naive in the middle row, and C*-IS in the bottom row. Settings with \(_{b}(s_{1})=\) are omitted due to symmetry.

varying degrees of similarity vs behavior. We compared our proposed estimator (with different annotation functions) with a set of baselines, including standard PDIS (without annotations) and two naive baselines (with perfect annotations): "naive unweighted" simply adds counterfactual annotations as new trajectories and has the same issue discussed in Section 3.1, whereas "naive weighted" reweights the annotations at the trajectory level instead of per-decision. See Appendix E.2 for detailed experimental setup. As the main OPE metric, we report RMSE of value estimates vs true values as well as the effective sample size (ESS). Additionally, we report metrics for two downstream uses of OPE for model selection. (i) When used to rank policies. We report the Spearman's rank correlation between \((_{e})\) and \(v(_{e})\) (computed over all \(_{e}\)'s) . (ii) When used to determine whether \(_{e}\) is better or worse than \(_{b}\). We formulate a binary classification problem of \(v(_{e}) v(_{b})\) vs \(v(_{e})<v(_{b})\), and report the accuracy, false positive rate (FPR) and false negative rate (FNR).

**C*-PDIS outperforms all baselines across all metrics in the ideal setting.** As shown in Table 2, when all counterfactuals are available and annotated with the evaluation policy's Q-function (\(G=Q^{_{e}}\)), C*-PDIS outperforms baseline PDIS (without annotations) in all metrics, demonstrating that it provides more accurate OPE estimates. In contrast, the two naive approaches fail to provide accurate estimates and often underperform standard PDIS.

**C*-PDIS is robust to biased annotations.** Under the more realistic scenario where \(G=Q^{_{b}}\), i.e., annotations summarize the future returns under \(_{b}\) rather than \(_{e}\), we observe a degradation in all metrics compared to the ideal case, though C*-PDIS is still superior to PDIS (Table 2). Applying the bias correction procedure (\(G=Q^{_{b}}^{_{e}}\), see Appendix D.1) helps in recovering performance closer to the ideal case, especially when \(_{e}\) is far from \(_{b}\) (Figure 5-left).

**Variance reduction of C*-PDIS outweighs the effect of noisy annotations.** When annotations are perturbed with increasing amounts of noise (Figure 5-center), performance degradation is minimal even at the highest level of noise tested (with a std of 1, which is large relative to the reward range \([-1,1]\), and larger than \(0.31\) the std of initial state values). Our estimator remains competitive relative to baselines, suggesting that the benefit of variance reduction from additional data (through counterfactual annotations) outweighs the variance increase from annotation noise, even when annotations are much noisier than factual data. See Appendix E.2 for variations of this experiment.

    &  & \(\)**RMSE** & \(\)**ESS** & \(\)**Spearman** & \(\)**\% Accuracy** & \(\)**\% FPR** & \(\)**\% FNR** \\   & PDIS (\(w\) on annot.) & 0.113 \(\)0.083 & 76.8 \(\)4.04 & 0.596 \(\)0.110 & 76.5 \(\)3.5 & 33.7 \(\)8.7 & 15.9 \(\)6.6 \\  & Naive unweighted (\(G=Q^{_{b}}\)) & 0.128 \(\)0.066 & 207.2 \(\)2.95 & 0.089 \(\)0.099 & 50.0 \(\)6.0 & 11.6 \(\)6.3 & 78.1 \(\)1.36 \\  & Naive weighted (\(G=Q^{_{e}}\)) & 0.097 \(\)0.006 & 300.8 \(\)117.6 & 0.420 \(\)0.097 & 64.3 \(\)4.7 & 24.0 \(\)1.27 & 44.3 \(\)11.4 \\   & C*-PDIS (\(G=Q^{_{e}}\)) & **0.013**\(\)0.005 & **994.0**\(\)0.1 & **9.995**\(\)0.003 & **95.7**\(\)3.1 & 4.5 \(\)6.9 & **4.2 \(\)5.3** \\  & C*-PDIS (\(G=Q^{_{b}}\)) & 0.070 \(\)0.003 & **994.0**\(\)0.1 & 0.961 \(\)0.011 & 86.8 \(\)8.2 & 22.0 \(\)2.01 & 8.2 \(\)11.3 \\   & C*-PDIS (\(G=Q^{_{b}}\)) & 0.028 \(\)0.007 & **994.0**\(\)0.1 & 0.979 \(\)0.010 & 90.1 \(\)5.4 & **4.2**\(\)6.6 & **14.1**\(\)9.7 \\   

Table 2: Comparison of baseline and proposed estimators in terms of OPE performance (RMSE, ESS), ranking performance (Spearman’s rank correlation) and binary classification performance (accuracy, FPR, FNR) on the sepsis simulator, reported as mean \(\) std from 50 repeated runs. **Bolded** results are the best for each metric, whereas highlighted results outperform all baselines.

Figure 5: (Left) RMSE of C*-PDIS vs. distance to \(_{b}\) (in terms of KL divergence) for each \(_{e}\), plotted with linear trend lines. OPE error increases as \(_{e}\) becomes more different from behavior. (Center&Right) Performance of our proposed approach under noisy and missing annotations. Trend lines show average of 50 runs \(\) one std. C*-PDIS is generally robust to noise, and imputing the missing annotations can help maintain competitive performance (relative to the ideal setting) even in the presence of high degrees of missingness.

**Collecting more annotations and imputing missing annotations improves performance.** As the amount of available annotations increases (Figure 5-right), our approach interpolates between baseline PDIS and the ideal case of C*-PDIS with a monotonic improvement in performance. Furthermore, imputing annotations achieves better performance, suggesting it is a promising strategy when not all annotations can be collected in practice. See Appendix E.2 for variations of this experiment where the imputed annotations have varying degrees of bias due to annotation noise.

## 6 Related Work

There is a rich literature on statistical methods for offline policy evaluation (OPE), including direct methods (DM), importance sampling (IS), and doubly-robust (DR) approaches . DM directly uses offline data to learn the value function of the evaluation policy (e.g., using model-based or model-free approaches) . We do not consider DM in our work since it often involves function approximators whose bias and variance may be difficult to analyze . On the other hand, IS uses a weighted average of trajectory returns to correct the distributional mismatch between the evaluation and behaviour policies . Our proposed estimator directly builds on IS and uses counterfactual annotations to reduce its variance (and bias), while carefully addressing the nuances involved with reweighting the counterfactual annotations to maintain the unbiasedness property. Finally, DR approaches combine IS and DM and reduce the variance of IS by using the estimates from DM as a control variate . Our approach is a complementary source of variance reduction and may be combined with DR approaches by modifying our current definitions to include a control variate. We note that other approaches exist for managing the variance of IS in long-horizon settings by considering the stationary or marginalized state distributions . Incorporating counterfactual annotations into these estimators is an interesting direction of future research.

Our work focuses on OPE rather than policy learning, but the broader theme of human input for RL has recently gained renewed attention, particularly in natural language processing tasks . In many of these problems, human input is in the form of preferences (or rankings) over actions, states, or (sub-)trajectories . Other related annotation approaches also exist in non-RL areas, where past work has proposed to ask annotators to alter text to match a counterfactual target label  and incorporating annotations of potential unmeasured confounders . In contrast, our work investigates the role of a specific form of human input, counterfactual annotations in offline RL, in improving OPE performance. We focus on offline _evaluation_ due to its practical importance in high-stakes RL domains such as healthcare, though our insights could also potentially benefit offline _learning_ in these domains. While we did not discuss how counterfactual annotations are obtained, our theoretical analysis establishes a thorough understanding of their desirable characteristics. This can help motivate methods for converting different forms of human feedback into useful counterfactual annotations (e.g., learning reward/annotation models from human preferences ). Conversely, progress in the field of preference learning and learning-to-rank may benefit our approach by providing mechanisms to solicit high-quality annotations .

## 7 Discussion & Conclusion

In this paper, we propose a novel semi-offline policy evaluation framework that incorporates counterfactual annotations into traditional IS estimators. We emphasize that the naive approach of viewing annotations as additional data can lead to bias and propose a simple reweighting scheme to address this issue. We formally study the theoretical properties of our approach, identifying scenarios where bias and variance reduction is guaranteed. Driven by a deep understanding of these theoretical properties, we further propose practically motivated strategies to handle biased, noisy, or missing annotations. Through proof-of-concept experiments on bandits and a healthcare-inspired RL simulator, we demonstrate that our approach outperforms standard IS and is robust to imperfect annotations. Our semi-offline framework serves as an intermediate step between offline and online evaluations and has the potential to enable practical applications of RL in high-stakes domains. Though motivated by current limitations of offline evaluation, we caution that our approach is not meant to replace existing OPE methods, but rather to complement them. Collecting annotations from domain experts comes at a cost (of real human time and labor), and thus, our approach should only be applied after a policy has passed all checks on retrospective data. While not explored in this paper, future work should focus on assessing the quality of counterfactual annotations in the domains of interest through human experiments. See Appendix A for more detailed discussions on limitations, societal impacts, and future directions. Overall, we believe our contributions will inspire further investigations into the practical obstacles that emerge in semi-offline evaluation (e.g., devising human-centered strategies for soliciting counterfactual annotations that align with theoretical assumptions) and will bring RL closer to reality in healthcare and other high-stakes decision-making domains.

## Data and Code Availability

The code for all experiments is available at https://github.com/MLD3/CounterfactualAnnot-SemiOPE.