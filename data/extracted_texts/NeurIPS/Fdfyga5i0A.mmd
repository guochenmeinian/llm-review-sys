# Mnemosyne: Learning to Train Transformers with Transformers

Deepali Jain

Google DeepMind,

jaindeepali@google.com

&Krzysztof Choromanski

Google DeepMind and Columbia University,

kchloro@google.com

&Avinava Dubey

Google Research,

avinavadubey@google.com

&Sumeet Singh

Google DeepMind,

ssumeet@google.com

&Vikas Sindhwani

Google DeepMind,

sindhwani@google.com

&Tingnan Zhang

Google DeepMind,

tingnan@google.com

&Jie Tan

Google DeepMind,

jietan@google.com

equal contribution

###### Abstract

In this work, we propose a new class of learnable optimizers, called _Mnemosyne_. It is based on the novel spatio-temporal low-rank implicit attention Transformers that can learn to train entire neural network architectures, including other Transformers, without any task-specific optimizer tuning. We show that Mnemosyne: (a) outperforms popular LSTM optimizers (also with new feature engineering to mitigate catastrophic forgetting of LSTMs), (b) can successfully train Transformers while using simple meta-training strategies that require minimal computational resources, (c) matches accuracy-wise SOTA hand-designed optimizers with carefully tuned hyper-parameters (often producing top performing models). Furthermore, Mnemosyne provides space complexity comparable to that of its hand-designed first-order counterparts, which allows it to scale to training larger sets of parameters. We conduct an extensive empirical evaluation of Mnemosyne on: (a) fine-tuning a wide range of Vision Transformers (ViTs) from medium-size architectures to massive ViT-Hs (36 layers, 16 heads), (b) pre-training BERT models and (c) soft prompt-tuning large 11B+ T5XXL models. We complement our results with a comprehensive theoretical analysis of the compact associative memory used by Mnemosyne which we believe was never done before.

## 1 Introduction

Learning-to-learn (L2L) systems  used to train machine learning (ML) optimizers can be thought of as a natural lifting (to the optimizer-space) of the idea that has revolutionized ML: replacing hand-engineered features with the learned ones.

The idea to train ML optimizers is tempting, yet the lift to the optimizer-space comes at a price: the instances used to train such systems are the optimization problems themselves. Generalization in such a setting means: the ability to transfer knowledge to "similar" optimization tasks not seen in training. Rigorous mathematical analysis of the properties of L2L systems, that involves defining distributions over optimization problems, becomes challenging and is a subject on its own. Indeed, the literature on _meta-learning_ is voluminous  and of critical importance in many disciplines such as Robotics, where transfer knowledge from simulator to hardware is a notoriously difficult problem .

A standard approach to learning optimizers is to cast it as a _sequential decision problem_, where a function \(f\) called an _optimizee_ is optimized via another function \(g_{}\) (an optimizer) with learnable parameters \(\). Function \(f\) can take as input \(\) the parameters of a neural network (NN) and output its corresponding test loss on a given task. The optimizer \(g_{}\) updates \(\) as:

\[_{0}\ \\ _{t+1}=g_{}(f,_{0},...,_{t})&t>0 \] (1)

Function \(g_{}\) is either trained by minimizing the meta-loss objective \(_{}\), which is usually a sum of \(f\)-losses over certain time horizons, with supervised/reinforcement learning  or in a completely supervised way, where it learns to imitate expert-optimizers . It usually does not act directly on the sequence \((_{0},...,_{t})\), but its processed-version, e.g. the sequence of the corresponding gradients \(( f(_{0}),..., f(_{t}))\) if \(f\) is differentiable (which does not need to be the case ). In practice, the processed-versions often have much richer structure : _"...various rolling statistics including...loss data structures, the rolling momentum / rms terms, as well as the gradient clipping state..."_.

In this paper, we abstract from the low-level design of the sequential inputs to \(g_{}\) (which is a subject on its own) and different training strategies of \(g_{}\). Our interest is in the core design of \(g_{}\) since it acts as a memory-based system. Indeed, most models of \(g_{}\) leverage recurrent NN cells, such as LSTMs , that keep the history of the optimization-rollout in the form of a compact learnable latent state. In addition, due to the fact that inputs \(\) to \(f\) are usually high-dimensional (e.g. neural networks' parameter-vectors), \(g_{}\) is often factorized to process independently different dimensions of \(\).

The Transformer-revolution in ML set the stage for a very different way to model sequential data and in general: to model memory - the _attention mechanism_. It is natural to ask whether attention architectures can be used to replace LSTM memory cells in L2L systems. Applying Transformers here is compelling - modeling long-range relationships over time by avoiding catastrophic forgetting (which is what LSTMs struggle with, but Transformers are particularly good at) is especially relevant for optimizing highly non-convex objectives in deep learning. Yet these benefits come at the cost of quadratic (in the sequence-length) space and time complexity.

**Contributions.** We propose a new class of learnable optimizers, called _Mnemosyne_. It is based on the novel spatio-temporal low-rank implicit attention Transformers that can learn to train entire neural network architectures, including other Transformers, without any task-specific optimizer tuning. We show that Mnemosyne: (a) outperforms popular LSTM optimizers (also with new feature engineering to mitigate catastrophic forgetting of LSTMs), (b) can successfully train Transformers while using simple meta-training strategies leveraging minimal computational resources, (c) matches accuracy-wise SOTA hand-designed optimizers with carefully tuned hyper-parameters (often producing top performing models). As we show in Sec. 5, SOTA hand-designed optimizers are very sensitive to the hyperparameter choice: a hyperparameter selection that is optimal for one dataset might perform very poorly on another one. Thus the ability of the optimizer to automatically "implicitly learn" optimal hyperparameters is of critical practical importance. Furthermore, Mnemosyne provides space complexity comparable to that of its standard hand-designed first-order counterparts, which allows it to scale to training larger sets of parameters. We conduct an extensive empirical evaluation of Mnemosyne on: (a) fine-tuning a wide range of Vision Transformers (ViTs) from medium-size architectures to massive ViT-Hs (36 layers, 16 heads), (b) pre-training BERT models and (c) soft prompt-tuning large 11B+ T5XXL models. We also conduct a thorough theoretical analysis of the compact associative memory used by Mnemosyne, to the best of our knowledge never done before.

Mnemosyne leverages several algorithmic techniques: (a) efficient Transformers, called _Performers_, applying implicit low-rank attention and guaranteeing linear (in the history length for causal and parameter-tensor size for the spatial attention) time and space complexity, (b) bi-directional and uni-directional attention combined in the unified spatio-temporal system, (c) hierarchical spatial attention mechanism to further reduce memory footprint of the spatial attention encoders that can be thought of as a novel attention pooling mechanism (see: 3.2.1). Obtained L2L mechanism effectively acts as a _compact associative memory_ (CAM) (see: Sec. 4) fed with latent representations summarizing groups of parameters induced by the natural structure of the target model to be trained (the so-called _topological encodings_, see: Sec 3.2). It thus provides the best of both worlds: efficiency due to the compact fixed-size hidden state (as in LSTMs) and expressiveness since this hidden state approximates regular Transformer's attention via the CAM-mechanism. We also believe that this paper lays the groundwork for the research on general-purpose attention-based learnable optimizers and foundational models for L2L systems.

Related work

The research on L2L systems involves a plethora of techniques: curriculum learning (e.g. incrementally increasing optimizer's unroll length ), randomly scaled optimizees in training with relative scaling of input gradients , hierarchical RNN-architectures with lower memory and compute overhead that are capable of capturing inter-parameter dependencies  and more [9; 51; 15; 68].

Regular Transformers are recently considered in this setting, in particular to tune hyperparameters , to learn BFGS-type optimization for motion reconstruction  or as memory-systems in class-incremental learning . Scalable Transformers [65; 64] were designed to address computational limitations of their regular counterparts. Several efficient attention mechanisms were proposed, based on hashing , clustering , dimensionality reduction  or sparsity .

In this paper, we apply in particular methods approximating attention via low-rank decomposition of the attention matrix [18; 44; 46; 17; 43; 20], due to their intrinsic connection with associative memory and energy-models  (while used in the causal attention setting). Regular Transformers can be thought of as differentiable dictionaries applying powerful _associative memory mechanisms_[37; 56], i.e. modern Hopfield networks  with exponential memory. Linear low-rank attention mechanisms are their compact variants  with intriguing theoretical properties and as such are perfect candidates for scalable memory-systems with respect to the temporal axis (Mnemosyne addresses also the problem of the efficient spatial encodings of the trainable parameters, see Sec. 3.2). That interpretation has profound practical consequences as giving guidance on the optimal version of the low-rank attention mechanism. Mnemosyne uses the so-called _hyperbolic cosine random features_ (see: Sec. 3.3.1, A.1) providing particularly low variance of the softmax-kernel estimation, a key ingredient of those modern associative memory models.

## 3 Learning to learn with spatio-temporal attention

In this section, we give the description of Mnemosyne. Since the system consists of several components, we start by providing a high-level overview. We then discuss individual components in more depth: topological (spatial) encoder in Sec. 3.2 and temporal (causal) encoder in Sec. 3.3.

### Preliminaries: tree-like optimization domains

Consider an optimizee \(f:\). In the simplest case, we can take: \(^{d}\) for \(d_{+}\). However it will be convenient to impose a tree-like structure on the elements \(\). We will assume that there exists a tree \(_{f}\) such that for every \(=(x_{1},...,x_{d})^{}\), the set \(\{x_{1},...,x_{d}\}\) is partitioned into non-empty subsets \(S_{1},...,S_{l}\) corresponding to different leaves of \(_{f}\), where \(l\) stands for their number. Not only does \(_{f}\) define partitioning of \(\{x_{1},...,x_{d}\}\) through the subsets residing in different leaves, but it also imposes a natural hierarchy. The construction might seem artificial at first glance, but we have a good reason to introduce it early on - it is a predominant description of the NN structure (our main interest in this paper is to optimize NNs with learnable optimizers thus we identify elements of \(\) with different instantiations of the particular NN model). In this context, different leaves correspond to individual tensors of parameters (e.g. weight-matrices or bias-vectors; here subsets \(_{i}\) are just their flattened representations) and tree-induced hierarchy describes how the NN layers/modules emerge from those individual tensors 2. This more general framework covers also the setting \(^{d}\), where \(_{f}\) is a star-tree with different leaves corresponding to different variables \(x_{i}\).

### Topological encoder

Standard L2L systems  act independently on the individual parameters to be optimized. We refer to this strategy as a _coordinate-wise_ approach. It has the advantage of enabling lots of data for meta-training (since the optimization trajectory of each scalar parameter is a viable training point) and corresponds to \(_{f}\) being a star-tree (see: Sec. 3.1). However it comes at a price of the memory footprint since space complexity becomes linear in the total number of trainable parameters. Mnemosyne can be successfully applied in the coordinate-wise framework (see: Sec. 5.2), but supports an arbitrary \(_{f}\), in particular the natural variant, where leaves correspond to the individual tensors of the NN to be optimized and that we refer to as the _tensor-wise_ approach.

Mnemosyne acts independently on each tensor \(\) from each leaf of a given input \(_{f}\). Tensor \(\) is first flattened/vectorized and a representation vector \(\) is associated with each parameter of \(\). Sincein this paper we minimize feature engineering for the L2L systems, our default choice for \(\) is the \(2\)-dimensional vector of the absolute value of the gradient dimension and its sign (that was used on a regular basis in several papers on the subject), but we emphasize that Mnemosyne is agnostic to the particular representation choice. The resulting sequence of representations \(_{}\) is then transformed by the bi-directional attention of the Performer model . A latent encoding of a fixed token from that sequence is then output as a _topological encoding_ of \(\).

#### 3.2.1 Compactifying topological encoder with the hierarchical pooling

Using bi-directional linear attention from Performers is justified by the fact that sequence \(_{}\) can be in practice very long. In fact it can easily surpass \(1M\) tokens (for instance if \(\) is a square weight-tensor corresponding to two consecutive layers of size \(>1K\) each). In that setting, even linear attention might not suffice.To address it, we introduce additional hierarchical pooling mechanism, leading to the _hierarchical pooling encoder_ (HPE). Sequence \(_{}\) is first split into chunks of a fixed length \(L_{+}\): \(_{}^{1},_{}^{2},...\) (the last chunk might be shorter). Topological encoding is applied in parallel to each chunk. The procedure is repeated for the resulting sequence of the topological encodings, which is already shorter by the multiplicative factor of \(L\) (potentially with a different \(L\) even though here we assume hat \(L\) is the same). The total number of repetitions is in practice a small constant \(h_{}\) and leads to the final sequence of length \(l=(_{})}{L^{h_{}}}\), where \((_{})\) is the original length and \(l\) as a small constant. If \((_{})\) is small enough, hierarchical pooling is not applied. The resulting \(l\)-length sequence of latent \(d\)-dimensional encodings is output as a topological encoding and fed to the temporal encoder defined below. We refer to different tokens of that sequence as _meta-tokens_, \(\).

### Temporal encoder: Compact Associative Memory (CAM) model

**Preliminaries.** The temporal module consists of one or more temporal encoders stacked together that process the meta-tokens, \(\). It treats the length \(l\) of the sequence \(\) as a batch size \(b\). For a given meta-token, denote by \(\{^{}\}_{=1}^{M}^{d}\) its corresponding latent encodings obtained over time. We will refer to them here as _memory-vectors_ (patterns). We obtain their latent embeddings: _queries_, _keys_ and _values_ via learnable linear transformations \(_{Q},_{K}^{N d}\), \(_{V}^{d d}\) as follows:

\[^{}=_{Q}^{},\ \ ^{}=_{K}^{ },\ \ ^{}=_{V}^{}\] (2)

Take a kernel \(:^{N}^{N}\), and its linearization: \((,)=[()^{}( )]\) for some (randomized) \(:^{N}^{r}\). We refer to \((),()\) as _random feature_ (RF) vectors and define a _hidden state_ encapsulating memory of the system of first \(t\) patterns as:

\[_{t}=_{=1}^{t}_{t}()(^{ })(^{})^{}^{r d},\\ _{t}=_{=1}^{t}_{t}()(^{})^ {r}\] (3)

A _discount-function_\(_{t}:\) is applied to depriotize older patterns.

#### 3.3.1 Updating hidden states and the outputs of the temporal module

Note that temporal encoder's hidden state \(_{}(t)=(_{t},_{t})\) is of size **independent** from the number of its implicitly stored patterns \(t\). When patterns are added, \(_{}(t)\) needs to be efficiently updated on-the-fly. It is easy to see that this can be done for the _exponential discount_ strategy: \(_{t}()=(-(t-))\) with \( 0\) (\(=0\) turns off discounting). We have the following:

\[_{t+1}&=(-) _{t}+(^{t+1})(^{t+1})^{},\\ &_{t+1}=(-)_{t}+( ^{t+1})\] (4)

With the definition of the temporal encoder's hidden state, we can now explain how it acts on the input vectors \(^{d}\). New vector \(^{}=+\) is obtained as follows, where \(=_{Q}\):

\[=_{t}^{}()}{()^{ }_{t}}=_{=1}^{t}()()^{}( ^{})}{_{=1}^{t}_{t}(i)()^{ }(^{i})}^{}\] (5)

Vector \(^{}\) can be computed in time \(O_{M}(1)\) and is given as a convex combination of value vectors \(^{}\), with coefficients proportional to approximated kernel values, but modulated by the discount-function.

For \(_{Q}=_{K}=_{V}=_{d}\), \(_{t} 1\) and with exact kernel values \((,^{i})\) in Eq. 5 (rather than their approximated versions \(()^{}(^{i})\)), dynamical systems defined by Eq. 5 become effectively Hopfield networks and, as energy-based models with energies given as \(E(;\{^{}\}_{=1}^{t})=-_{=1}^{t}(,^{})\), retrieve memory-vectors upon energy-minimization-driven convergence . The retrieval quality depends on the kernel, with the softmax-kernel \((,)}}{{=}} (^{})\) providing particularly strong theoretical results. For arbitrary \(_{Q},_{K},_{V}\), but still with \(_{t} 1\) and exact kernel values, Eq. 5 turns into regular Transformers' attention. Upon replacement of the exact kernel values with the approximate ones, Performer model is recovered.

We think about Eq. 5 as a _generalized_ (since it uses \(_{t}\)) _compact_ (since it provides efficient hidden state and input update-rules via linearized kernels from Performers) _associative memory_ model (CAM) (as opposed to the regular associate memory model from Hopfield networks).

In Mnemosyne, we choose softmax-kernel as \(\), since it is a default choice for Transformers. We observed that the FAVOR++ mechanism defining \(\) from , and denoted by us as \(_{F++}\), provides the most robust performance, but since it is harder to implement in the temporal encoder setting (see: discussion in Sec. A.1), in practice we apply the so-called _hyperbolic cosine random features_ from , performing very similarly, as \(_{F++}\). More details including in particular exact definitions and ablation studies over different random feature mechanisms can be found in Sec. A.1, B.3.

### Putting it all together

Details of the hierachical pooling encoder (HPE) and the compact associative memory (CAM) modules are depicted in Fig. 1. With all the components of Mnemosyne described, we present the complete design of Mnemosyne for two application modes: **coordinate-wise** and **tensor-wise**, shown in Fig. 2. In the coordinate-wise application, the enriched gradient input \(_{i}\) (see: Sec. 3.2) of every parameter \(x_{i}\) is processed separately in parallel by a CAM module followed by an MLP layer to produce the update \( x_{i}\). This makes the optimizer design simple and allows it to learn optimization from small-scale problems since each parameter input serves as a separate training example. In this setting, CAM stores a fix-sized memory state for each parameter thereby making the optimizer memory state scale linearly with the number of parameters. In the tensor-wise application, every tensor \(_{}\) in the parameter-tree is processed as a whole by an HPE to produce meta-tokens \(\) for CAM. Now the CAM memory state becomes very compact because it stores a state for each of the (small number of) meta-tokens rather than every token in the original input sequence. CAM output \(\) has the same shape as \(\). It is transformed into a fix-sized encoding \(\) by a spatial attention encoder (SPE). This encoding is broadcast to all the input tokens of the tensor via concatenations with vectors \(\). Each resulting vector \(^{}=\) is processed by a single MLP-layer to get the update tensor \(\).

Figure 1: Pictorial description of the hierarchical pooling encoding (HPE) and compact associative memory (CAM) in Mnemosyne on the example of modifying a single weight-tensor of a given NN. Consider training a single \(3 4\) weight tensor of a toy feedforward fully connected NN ((a)). Three snapshots of this tensor ((b)) represent its consecutive instantiations in the optimization process. HPE ((c)) acts as follows. Each tensor is vectorized and chunked into sub-sequences that are spatially encoded by the bi-directional Performers. Presented pooling mechanism consists of two layers. This results in the input tensors \(^{0},^{1},^{2}\) to CAM. Here the first dimension (the final number of meta-tokens) serves as a batch one (\(b=2\)) and \(N=4\) (see: notation from Sec. 3.3). Those tensors are first linearly mapped via \(\) matrices to keys (shown above) and queries and then non-linearly transformed via the \(\)-mapping. The transformed variants are leveraged by the associative memory.

## 4 The theory of Mnemosyne's compact associative memory

In this section, we analyze Mnemosyne's compact associative memory (CAM) from the theoretical point of view. We show that, as its regular non-compact counterpart, it is capable of storing patterns, but (as opposed to the former) in the implicit manner. We start by providing a concise introduction to associative memory models as "analytical" (rather than combinatorial) energy-based nearest-neighbor search systems. We then present our main result (Theorem 4.3) stating that "on average" CAMs can restore exponentially many (in space dimensionality) number of patterns, provided that they are spread "well enough". To the best of our knowledge, this is the first result of this type.

### Regular exponential associative memory

As in several other papers providing a theoretical analysis of the associative memory, we consider feature vectors taken from the set \(\{-1,+1\}^{N}\). We denote by \(\{^{}\}_{=1}^{M}\) the set of all the memory-vectors to be stored. For the given input \(\), in the regular exponential associative memory model, the energy of the system is defined as:

\[E_{}(;^{1},...^{M})=-_{=1}^{M}(^{}^{ }).\] (6)

The dynamical system defining the interactions with the associative memory and whose goal is to retrieve the relevant memory for the given input vector \(\{-1,+1\}^{N}\) (its nearest neighbor in \(\{\}_{=1}^{M}\)) has the following form: \( T_{i_{1}}() T_{i_{2}}(T_{i_{1}}())...\), for the initial point \(\), where \(i_{1},i_{2},...\) are chosen independently at random and \(T_{j}:\{-1,+1\}^{N}\{-1,+1\}^{N}\) only updates the jth entry of its input as follows (for \([j;x]\) denoting vector \(\), but with its jth dimension equal to \(x\)):

\[T_{j}()[j]=[E_{}([j;-1];^{1},...^{M})-E_{ }([j;1];^{1},...^{M})]\] (7)

Thus at every step, a random dimension of the input vector is chosen and its value is flipped if that operation decreases the energy of the system.

**Definition 4.1** (the capacity of the associative models).: We say that the model described above stores memories \(\{^{}\}_{=1}^{M}\) if there exists \((0,)\) such that \(T_{j}(^{})[j]=^{}_{j}\) for any \(j\) and \(^{}\) taken from the Hamming ball \((^{}, N)\) centered in \(^{}\) and of Hamming radius \( N\).

It was proven in  that if the memories are chosen uniformly and independently at random from \(\{-1,+1\}^{N}\), then with probability approaching \(1\) as \(N\) the model stores all the memories as long as the memory-set is not too large. Most importantly, the upper bound on the memories-set size is exponential in \(N\).

_Remark 4.2_.: Despite the exponential capacity of the model, all its memories need to be explicitly stored (to compute the energy-function) and compute time is proportional to their number. Thus for large number of memories, the space and time complexity makes the retrieval infeasible in practice.

### Mnemosyne's Compact Associative Memory (CAM)

We denote by \(_{1},_{2},..._{r}\) samples chosen independently from the multivariate Gaussian distribution \((0,_{N})\) and define the energy of the system as follows (see: Sec. 3.3.1, A.1):

\[E_{}(;^{1},...^{M})=_{F+}()^{}(^ {1},...,^{M}),\] (8)

where \(\) is given as: \((^{1},...,^{M})=-_{=1}^{M}_{F++}(^{}).\) We refer to the number of random projection vectors \(\) as the number of _random features_ (RFs).

Equipped with this new energy function, we define the corresponding dynamical system in the same way as for the regular associative memory model. Calculating energy \(E_{}\) can be now

Figure 2: Two modes of Mnemosyne application: (a) **coordinate-wise** and (b) **tensor-wise**.

done efficiently in time \(O(r)\), once vector \(\) is computed (thus independently from the number of implicitly stored memories). In the online/streaming setting, when the memories come one by one, updating vector \(\) can be done in time \(O(Nr)\) per query.

### The capacity of Mnemosyne's memory

We are ready to present our main theoretical result. We assume the setting from Sec. 4.2. The extended version of this result, providing in addition concentration results, is given in Sec. A.2.

**Theorem 4.3** (storage of compact associative memories).: _Denote by \(^{1},...,^{M}\{-1,+1\}^{N}\) the memory-vectors. Assume that the Hamming distance between any two memory-vectors is at least \( N\) for some \(>0\). Take some \(0<<\). Then the following is true for any memory-vector \(^{l}\) for \(l=1,...,\) and any input \(^{l}(^{l}, N)\) as long as \(M(2N(-2))}{2e^{2}}\): the expected change of the energy of the compact associative memory system \((E_{})\) associated with flipping the value of the dimension of \(^{l}\) is positive if that operation increases the distance from its close neighbor \(^{l}\) and is negative otherwise._

_Remark 4.4_.: Theorem 4.3 says that the expected value of the change of the energy of the system has the correct sign even if the number of stored patterns is exponential in their dimensionality, provided that the patterns are well separated. By the analogous argument as the one from the proof of Theorem 4.3 in Sec. A.2, a similar statement for the method applying other RF-mechanism for softmax-kernel estimation can be derived. However \(_{F++}\) provides **the smallest** variance among all competitors as the most accurate currently known mechanism for the unbiased softmax-kernel approximation.

## 5 Experiments

This section is organized as follows. Sec. 5.1 is a warm-up, where we provide initial comparison of Mnemosyne with several hand-designed optimizers and an LSTM-based learned optimizer baseline, on the tasks of training smaller NN architectures. Our results show that Mnemosyne consistently outperforms other variants and that popular \(\) optimizer  is the second best. Note also that \(\) is an optimizer of choice for training large Transformer models. Therefore in the following sections, we focus on the detailed comparison of Mnemosyne with \(\) for larger architectures.

In Sec. 5.2, we test the **coordinate-wise Mnemosyne** for ViT fine-tuning and soft prompt-tuning 11B+ T5XXL models. Optimizer's memory scales linearly with the NN size for the coordinate-wise variant and \(\). However, depending on the size and number of temporal attention layers in the coordinate-wise Mnemosyne, the linear multiplicative constant can be prohibitively large. This restriction is alleviated by the tensor-wise variant. In Sec. 5.3, we present the results for the **tensor-wise Mnemosyne** on BERT MLM pre-training and ViT fine-tuning. The tensor-wise Mnemosyne's memory state scales with the number of tensors instead of the number of model parameters. However, this variant requires meta-training with larger tensors for best generalization results since now each tensor serves as a single training example instead of each parameter which was the case for the coordinate-wise variant. To take advantage of the efficiency of the tensor-wise and the efficacy of the coordinate-wise variant, we present **Super-Mnemosyne**, in Sec. 5.4, that merges them.

All Mnemosyne variants are meta-trained on small scale MLP and VIT training tasks for a short horizon of \(100\) steps. More detailed description of the meta-training set-up is given in the Appendix (Sec: B) along with the details of the experiments in this section. Additional results including: (a) comparison of the CAM mechanism with regular attention (Sec. B.2), (b) studies over different RF-mechanisms (Sec. B.3), ablations over: (c) different discount factors and different number of random features for the CAM mechanism (Sec. B.5) and (d) different depths of the temporal modules of Mnemosyne (Sec. B.6) are also given in the Appendix.

We have finalized the design choices for Mnemosyne optimizer used throughout this section based on the extensive theoretical analysis in previous sections and various ablation studies detailed in the Appendix. In particular:

1. The choice of the random feature (RF) mechanism is motivated by strong theoretical guarantees for the capacity of the corresponding associative memory. Other RF mechanisms that do not admit our theoretical analysis clearly underperform in the ablation results.
2. Our default hyperbolic cosine RF mechanism is motivated by the provably reduced estimation variance as explained previously.

3. We choose hyperbolic cosine RFs as they are shown more robust than regular positive RFs in ablation.
4. Our use of exponential discount strategy (EDS) to smoothly discount the past is theoretically motivated previously. We show empirical verification of EDS in the appendix and choose the discount factor based on ablation results.

### Warm-up: Mnemosyne vs other optimizers for smaller NN architectures

We start by comparing coordinate-wise Mnemosyne with standard optimizers: \(\), \(\), \(\) as well as popular learnable optimizers using LSTMs . All optimizers were tested on the task of training Vision Transformer (ViT)  architectures not seen during meta-training. Considered ViT has \(3\) attention and MLP layers of size \(16\) and \(2\) heads. Loss curves for image classification with ViTs on different datasets (MNIST, CIFAR10, CIFAR100) are shown as first three plots in Fig. 3. Minimal feature engineering techniques were applied. In the last plot of Fig. 3, we inserted Mnemosyne into \(\) optimizer  which originally used LSTMs. This learnable optimizer applies sophisticated feature engineering to mitigate the problem of catastrophic forgetting of the LSTM-cells. Note that we did not replicate their large-scale meta-training set-up, only the optimizer architecture and features. Mnemosyne outperforms its counterparts in all these scenarios. In particular, it successfully trains attention-based architectures on unseen tasks.

Now we proceed with the comparison of Mnemosyne and \(\) with different learning rates on larger Transformers. We emphasize that we do not conduct any hyperparameter tuning for Mnemosyne.

### Coordinate-wise Mnemosyne results

**ViT last-layer fine-tuning:** We tested a coordinate-wise Mnemosyne with \(2\) temporal encoders on ViT fine-tuning . Due to memory constraints, we only trained the last layer of ViT by freezing other parameters of the model. The results on three different datasets: \(2012\), \(365\) and \(\)-\(\)-\(2011\) for ViT-H(32) (\(36\) layers, \(16\) heads, \(32 32\) patch shape) are presented in Fig. 4. Additional results, including ablations with varying architecture sizes and other datasets, are shown in Appendix Sec: B.8. We conclude that Mnemosyne (without any hyperparameter tuning) matches the top-performing \(\) variant. Note that \(\) is very sensitive to the choice of the learning rate \(\).

In Table 1 we compare coordinate-wise Mnemosyne with two other classes of learnable optimizers: a) Those using S4 memory units  based on state space machines as an alternative to LSTMs and Transformers b) The Lion family of learnable optimizers  using a different approach of symbolic programming. In the case of ViT-L architecture, we skip S4 due to poor performance on the previous tasks. Mnemosyne is clearly the best among all three variants. In contrast to Lion, Mnemosyne does not require any hyperparameter tuning.

Figure 4: Fine-tuning the last layer of ViT-H(32) on different datasets with coordinate-wise Mnemosyne.

Figure 3: **First three plots:** Test loss curves for training ViTs with Mnemosyne on: MNIST, CIFAR10 and CIFAR100. Minimal feature engineering is applied. **Last plot**: Test loss curves for training an MLP on MNIST. Both Mnemosyne and LSTM are used within \(\) architecture using sophisticated feature engineering.

**ViT multi-layer fine-tuning:** The _light Mnemosyne_ variant with a single temporal encoder can scale up to fine-tune \(2\) Transformer layers along with the last layer. We show the result for fine-tuning ViT-B(16) on \(\) in Fig. 5. The non-Mnemosyne layers are trained using an untuned \(\) and the Adam variants for comparison fine-tune the full model. We observe that full-model finetuning is a hard task where all but one \(\) variant fail to reach the optimal accuracy. Here, even by training a part of the network with Mnemosyne, we achieve the best accuracy. Also note that the best \(\) learning rate in this experiment (\(1e^{-4}\)) was performing poorly for the previous one. We see this in subsequent results as well.

**Soft prompt-tuning T5XXL:** We tested coordinate-wise Mnemosyne for soft prompt-tuning 11B+ T5XXL Transformers  on the SuperGLUE task . This method was introduced as a scalable alternative to fine-tuning pre-trained models for several downstream tasks, with learnable prompts injected into Transformer layers and modulating its behaviour. The trainable soft-prompt contains \(12288\) parameters. Mnemosyne outperforms all \(\) variants (see: Fig. 6).

### Tensor-wise Mnemosyne results

In this section, we evaluate the memory-efficient tensor-wise Mnemosyne.

**BERT NLP Transformer pre-training:** We showcase the ability of tensor-wise Mnemosyne to pre-train a BERT-base text Transformer  on the standard masked language modeling (MLM) task.

Coordinate-wise Mnemosyne was not applicable here due to the memory footprint and thus it became a practical test for the memory efficient tensor-wise variant. With that variant, we were able to train **86M parameters** of Bert-base model thereby showcasing the scalability of tensor-wise Mnemosyne. We compare Mnemosyne with different variants of \(\) in Fig 7. We see that Mnemosyne matches the performance of the best \(\) variant _even though it was never exposed to the MLM task during meta-training_. Several \(\) variants get stuck in a local optima, worse than that found using Mnemosyne.

**ViT multi-layer fine-tuning:** Now we show the performance of tensor-wise Mnemosyne on fine-tuning ViTs. Although tensor-wise Mnemosyne can scale up to the full ViT model, here we freeze large tensors in the Transformer layers and fine-tune the rest. Training large tensors with tensor-wise Mnemosyne requires data and compute intensive large scale meta-training (to ensure that spatial encoders learn how to encode long sequences well). This will be the focus of the future work. For this experiment, we consider the ViT-H(32) model and three datasets:

   Method & ViT-H & ViT-H & ViT-L \\  & (ps=14) & (ps=8) & (ps=32) \\  Mnemosyne & 72.3 & 81.4 & 74.0 \\ S4 & 7.5 & 35.0 & - \\ Lion lr=1e-1 & 67.2 & 76.4 & 66.8 \\ Lion lr=1e-2 & 68.3 & 80.0 & 70.0 \\ Lion lr=2e-3 & 70.0 & 80.0 & 71.4 \\ Lion lr=1e-3 & 72.3 & 81.2 & 71.4 \\ Lion lr=1e-4 & 70.0 & 81.0 & 71.4 \\   

Table 1: Comparison of coordinate-wise Mnemosyne with Lion and S4-based learned optimizer. Accuracy after fine-tuning the last layer of different ViT architectures and different patch sizes (ps) on Imagenet2012 dataset for 500 iterations.

Figure 5: Fine-tuning ViT-B(16).

Figure 6: Soft prompt-tuning T5XXL.

Figure 7: MLM task with BERT.

Cifar\(100\), \(365\), \(\) in Fig. 8. Mnemosyne shows equivalent or better performance as compared to the optimal \(\).

### Super-Mnemosyne: combining coordinate- and tensor-wise strategies

In this last set of ViT-experiments, we combine tensor-wise and coordinate-wise light Mnemosyne. Since applying the former on large tensors requires more intense meta-training which is out of the scope of this paper, we decided to optimize the largest tensors with the coordinate-wise and others with the tensor-wise Mnemosyne (to keep meta-training simple). This combination turned out to minimize total memory usage. As we see in Fig. 9, Mnemosyne outperforms optimal \(\) variants.

## 6 Broader impacts & limitations

We believe that Mnemosyne opens a research on attention-based optimizers for general-purpose optimization. Our system should be used responsibly due to the potential for misuse, significant societal impact, and carbon footprint of Transformers . In the future we want to analyze in more depth the impact on more complex meta-training strategies on the quality of learned optimizers.

## 7 Conclusion

We proposed a new class of learnable optimizers applying efficient spatio-temporal attention, called Mnemosyne. We show that they outperform their LSTM-based counterparts and can be successfully used to fine/soft prompt-tune and pre-train large Transformer models, matching optimal hard-coded variants without any hyper-parameter tuning, often producing top performing models.

Figure 8: Fine-tuning multi-layer ViT-H(32) on different datasets with tensor-wise Mnemosyne.

Figure 9: Fine-tuning **full** ViT-B(16) on different datasets with Super-Mnemosyne.

[MISSING_PAGE_FAIL:11]

*  Tianlong Chen, Weiyi Zhang, Jingyang Zhou, Shiyu Chang, Sijia Liu, Lisa Amini, and Zhangyang Wang. Training stronger baselines for learning to optimize. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
*  Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. _arXiv preprint arXiv:2302.06675_, 2023.
*  Yutian Chen, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Timothy P. Lillicrap, and Nando de Freitas. Learning to learn for global optimization of black box functions. _CoRR_, abs/1611.03824, 2016.
*  Yutian Chen, Xingyou Song, Chansoo Lee, Zi Wang, Qiuyi Zhang, David Dohan, Kazuya Kawakami, Greg Kochanski, Arnaud Doucet, Marc'Aurelio Ranzato, Sagi Perel, and Nando de Freitas. Towards learning universal hyperparameter optimizers with transformers. _CoRR_, abs/2205.13320, 2022.
*  Krzysztof Choromanski, Han Lin, Haoxian Chen, Tianyi Zhang, Arijit Sehanobish, Valerii Likhosherstov, Jack Parker-Holder, Tamas Sarlos, Adrian Weller, and Thomas Weingarten. From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked transformers. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 3962-3983. PMLR, 2022.
*  Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
*  Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. _CoRR_, abs/2204.02311, 2022.
*  Sankalan Pal Chowdhury, Adamos Solomou, Avinava Dubey, and Mrinmaya Sachan. On learning the transformer kernel. _Transactions of Machine Learning Research_, 2022.
*  Mete Demircigil, Judith Heusel, Matthias Lowe, Sven Upgang, and Franck Vermet. On a model of associative memory with huge storage capacity. _Journal of Statistical Physics_, 168(2):288-299, may 2017.
*  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, pages 4171-4186. Association for Computational Linguistics, 2019.

*  Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
*  Alireza Fallah, Aryan Mokhtari, and Asuman E. Ozdaglar. Generalization of model-agnostic meta-learning algorithms: Recurring and unseen tasks. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 5469-5480, 2021.
*  Roy Frostig, Matthew Johnson, and Chris Leary. Compiling machine learning programs via high-level tracing. 2018.
*  Erik Gartner, Luke Metz, Mykhaylo Andriluka, C Daniel Freeman, and Cristian Sminchisescu. Transformer-based learned optimization. _arXiv preprint arXiv:2212.01055_, 2022.
*  Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. _arXiv preprint arXiv:2111.00396_, 2021.
* ICANN 2001, International Conference Vienna, Austria, August 21-25, 2001 Proceedings_, volume 2130 of _Lecture Notes in Computer Science_, pages 87-94. Springer, 2001.
*  John J. Hopfield. Hopfield network. _Scholarpedia_, 2(5):1977, 2007.
*  Jeffrey Ichnowski, Yahav Avigal, Vishal Satish, and Ken Goldberg. Deep learning can accelerate grasp-optimized motion planning. _Science Robotics_, 5(48):eabd7710, 2020.
*  Ahmet Iscen, Thomas Bird, Mathilde Caron, Alireza Fathi, and Cordelia Schmid. A memory transformer network for incremental learning. _CoRR_, abs/2210.04485, 2022.
*  Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalashnikov, Alex Irpan, Julian Ibarz, Sergey Levine, Raia Hadsell, and Konstantinos Bousmalis. Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019_, pages 12627-12637. Computer Vision Foundation / IEEE, 2019.
*  Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M. Kakade, and Michael I. Jordan. Stochastic gradient descent escapes saddle points efficiently. _CoRR_, abs/1902.04811, 2019.
*  Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015.
*  Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.
*  Dmitry Krotov and John J. Hopfield. Dense associative memory for pattern recognition. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain_, pages 1172-1180, 2016.
*  Dmitry Krotov and John J. Hopfield. Large associative memory problem in neurobiology and machine learning. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.

*  Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021_, pages 3045-3059. Association for Computational Linguistics, 2021.
*  Jeffrey Li, Mikhail Khodak, Sebastian Caldas, and Ameet Talwalkar. Differentially private meta-learning. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.
*  Ke Li and Jitendra Malik. Learning to optimize. In _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings_. OpenReview.net, 2017.
*  Weiwei Li and Emanuel Todorov. Iterative linear quadratic regulator design for nonlinear biological movement systems. In _ICINCO (1)_, pages 222-229. Citeseer, 2004.
*  Jacky Liang, Saumya Saxena, and Oliver Kroemer. Learning active task-oriented exploration policies for bridging the sim-to-real gap. In Marc Toussaint, Antonio Bicchi, and Tucker Hermans, editors, _Robotics: Science and Systems XVI, Virtual Event / Corvalis, Oregon, USA, July 12-16, 2020_, 2020.
*  Valerii Likhosherstov, Krzysztof Choromanski, Jared Davis, Xingyou Song, and Adrian Weller. Sub-linear memory: How to make performers slim. _CoRR_, abs/2012.11346, 2020.
*  Valerii Likhosherstov, Krzysztof Choromanski, Avinava Dubey, Frederick Liu, Tamas Sarlos, and Adrian Weller. Chefs' random tables: Non-trigonometric random features. _to appear at AAAI 2023_, abs/2205.15317, 2022.
*  Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the difficulty of training transformers. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020_, pages 5747-5763. Association for Computational Linguistics, 2020.
*  Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, and Tie-Yan Liu. Stable, fast and accurate: Kernelized attention with relative positional encoding. _CoRR_, abs/2106.12566, 2021.
*  Kaifeng Lv, Shunhua Jiang, and Jian Li. Learning gradient descent: Better generalization and longer horizons. In Doina Precup and Yee Whye Teh, editors, _Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017_, volume 70 of _Proceedings of Machine Learning Research_, pages 2247-2255. PMLR, 2017.
*  Luke Metz, James Harrison, C. Daniel Freeman, Amil Merchant, Lucas Beyer, James Bradbury, Naman Agrawal, Ben Poole, Igor Mordatch, Adam Roberts, and Jascha Sohl-Dickstein. Velo: Training versatile learned optimizers by scaling up. _CoRR_, abs/2211.09760, 2022.
*  Luke Metz, Niru Maheswaranathan, C. Daniel Freeman, Ben Poole, and Jascha Sohl-Dickstein. Tasks, stability, architecture, and compute: Training more effective learned optimizers, and using them to train themselves. _CoRR_, abs/2009.11243, 2020.
*  Luke Metz, Niru Maheswaranathan, Jeremy Nixon, C. Daniel Freeman, and Jascha Sohl-Dickstein. Understanding and correcting pathologies in the training of learned optimizers. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pages 4556-4565. PMLR, 2019.
*  Luke Metz, Niru Maheswaranathan, Jonathon Shlens, Jascha Sohl-Dickstein, and Ekin D. Cubuk. Using learned optimizers to make models robust to input noise. _CoRR_, abs/1906.03367, 2019.

*  Dattika K. Naik and Richard J. Mammone. Meta-neural networks that learn by learning. _[Proceedings 1992] IJCNN International Joint Conference on Neural Networks_, 1:437-442 vol.1, 1992.
*  Vitchyr H. Pong, Ashvin V. Nair, Laura M. Smith, Catherine Huang, and Sergey Levine. Offline meta-reinforcement learning with online self-supervision. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 17811-17829. PMLR, 2022.
*  Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.
*  Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.
*  Hubert Ramsauer, Bernhard Schafl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Milena Pavlovic, Geir Kjetil Sandve, Victor Greiff, David P. Kreil, Michael Kopp, Gunter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. Hopfield networks is all you need. _CoRR_, abs/2008.02217, 2020.
*  Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings_. OpenReview.net, 2017.
*  Anirban Roy and Sinisa Todorovic. Learning to learn second-order back-propagation for cnns using lstms. In _24th International Conference on Pattern Recognition, ICPR 2018, Beijing, China, August 20-24, 2018_, pages 97-102. IEEE Computer Society, 2018.
*  Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. _Trans. Assoc. Comput. Linguistics_, 9:53-68, 2021.
*  Adam Santoro, Sergey Bartunov, Matthew M. Botvinick, Daan Wierstra, and Timothy P. Lillicrap. Meta-learning with memory-augmented neural networks. In Maria-Florina Balcan and Kilian Q. Weinberger, editors, _Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016_, volume 48 of _JMLR Workshop and Conference Proceedings_, pages 1842-1850. JMLR.org, 2016.
*  Imanol Schlag, Kazuki Irie, and Jurgen Schmidhuber. Linear transformers are secretly fast weight programmers. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 9355-9366. PMLR, 2021.
*  Sumeet Singh, Jean-Jacques Slotine, and Vikas Sindhwani. Optimizing trajectories with closed-loop dynamic sqp. _arXiv preprint arXiv:2109.07081_, 2021.
*  Richard S. Sutton. Adapting bias by gradient descent: An incremental version of delta-bar-delta. In William R. Swartout, editor, _Proceedings of the 10th National Conference on Artificial Intelligence, San Jose, CA, USA, July 12-16, 1992_, pages 171-176. AAAI Press / The MIT Press, 1992.
*  Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
*  Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. _CoRR_, abs/2009.06732, 2020.
*  Sebastian Thrun and Lorien Y. Pratt, editors. _Learning to Learn_. Springer, 1998.

* Vaswani et al.  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 5998-6008, 2017.
* Vicol et al.  Paul Vicol, Luke Metz, and Jascha Sohl-Dickstein. Unbiased gradient estimation in unrolled computation graphs with persistent evolution strategies. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 10553-10563. PMLR, 2021.
* Wang et al.  Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. _CoRR_, abs/2006.04768, 2020.
* Weidinger et al.  Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of harm from language models. _arXiv preprint arXiv:2112.04359_, 2021.
* Wichrowska et al.  Olga Wichrowska, Niru Maheswaranathan, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Nando de Freitas, and Jascha Sohl-Dickstein. Learned optimizers that scale and generalize. In Doina Precup and Yee Whye Teh, editors, _Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017_, volume 70 of _Proceedings of Machine Learning Research_, pages 3751-3760. PMLR, 2017.
* Xiao et al.  Xuesu Xiao, Tingnan Zhang, Krzysztof Marcin Choromanski, Tsang-Wei Edward Lee, Anthony G. Francis, Jake Varley, Stephen Tu, Sumeet Singh, Peng Xu, Fei Xia, Sven Mikael Persson, Dmitry Kalashnikov, Leila Takayama, Roy Frostig, Jie Tan, Carolina Parada, and Vikas Sindhwani. Learning model predictive controllers with real-time attention for real-world navigation. In Karen Liu, Dana Kulic, and Jeffrey Ichnowski, editors, _Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand_, volume 205 of _Proceedings of Machine Learning Research_, pages 1708-1721. PMLR, 2022.
* Yao et al.  Huaxiu Yao, Linjun Zhang, and Chelsea Finn. Meta-learning with fewer tasks through task interpolation. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* Younger et al.  Arthur Steven Younger, Sepp Hochreiter, and Peter R. Conwell. Meta-learning with backpropagation. _IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222)_, 3:2001-2006 vol.3, 2001.
* Zaheer et al.  Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. _Advances in Neural Information Processing Systems_, 33:17283-17297, 2020.
* Zhao et al.  Zihao Zhao, Anusha Nagabandi, Kate Rakelly, Chelsea Finn, and Sergey Levine. MELD: meta-reinforcement learning from images via latent state models. In Jens Kober, Fabio Ramos, and Claire J. Tomlin, editors, _4th Conference on Robot Learning, CoRL 2020, 16-18 November 2020, Virtual Event / Cambridge, MA, USA_, volume 155 of _Proceedings of Machine Learning Research_, pages 1246-1261. PMLR, 2020.
* Zhu et al.  Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In _IEEE international conference on computer vision_, pages 19-27, 2015.
* Zou et al.  Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for convergences of adam and rmsprop. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019_, pages 11127-11135. Computer Vision Foundation / IEEE, 2019.

Proofs

### Kernels & their linearizations for temporal encoders in Mnemosyne

We tested different transformations \(\) and discovered that those leading to most accurate approximation of the softmax-kernel lead to most effective memory mechanisms for Mnemosyne's temporal encoders (see: Sec. 3.3). Our starting variant is the so-called _FAVOR+_ mechanism from , given as follows for \((,r)}}{{=}}}(-\|^{2}}{2})\) and \(_{1},...,_{r}(0,_{N})\):

\[_{F+}()=(,r)((_{1}^{} ),...,(_{r}^{}))^{}\] (9)

Random vectors \(_{1},...,_{r}\) form a block-orthogonal ensemble (see: ). We applied also its improvement relying on the so-called _hyperbolic cosine random features_, where \(\) is the concatenation operator:

\[_{HF+}()=(,r)_{i=1}^{}(( _{i}^{}),(-_{i}^{}))^{}\] (10)

Both randomized transformations provide **unbiased** estimation of the softmax-kernel, yet the latter one (that can be cast as modified \(_{F+}\) via the antithetic Monte Carlo trick) has provably lower approximation variance.

#### a.1.1 The curious case of linearization with bounded features

The last variant for the efficient estimation of the softmax-kernel we applied, is a very recent mechanism _FAVOR++_ from , given as:

\[_{F++}()=}_{i=1}^{r}(-\| _{i}\|_{2}^{2}+B_{i}^{}+C\|\|^{2})^{},\]

where we have: \(=-A\), \(B=}\), \(C=-\), \(D=(1+4)^{}\), \(A=1-\) and \((0,1)\) is a free parameter. As opposed to the previous variants, mechanism \(_{F++}()\) provides an estimation via **bounded** random variables (since \(>0\)), leading to stronger concentration results (beyond second moment) and still unbiased approximation.

The optimal choice of \(\) depends on the kernel inputs. The formula for \(\) optimizing the variance of the kernel matrix estimation \(=[(^{i},^{j})]_{i,j=1,...,M}\) induced by the softmax-kernel \(\) (in the bi-directional case) is not tractable. However choosing \(\) by optimizing certain derivative of the variance-objective was showed to work well in several applications :

\[^{*}=+8N}-2-N}{4}\] (11)

for \(=}_{i=1}^{M}_{j=1}^{M}\|^{i}+^ {j}\|^{2}\). Since \(\) can be rewritten as: \(=}(_{i=1}^{M}\|^{i}\|_{2}^{2}+_{j=1}^{M} \|^{j}\|_{2}^{2}+2^{})\) for \(=_{i=1}^{M}^{i}\) and \(=_{j=1}^{M}^{j}\), computing \(^{*}\) in the bi-directional setting can be clearly done in time linear in \(M\) as a **one-time** procedure. Then the computation of \(_{}(M)\) follows. Compute-time per memory-vector remains \(O_{M}(1)\).

However Mnemosyne's temporal encoder applied uni-directional attention. In the uni-directional case, we have: \(_{t}=_{j=1}^{t}\|^{t}+^{j}\|^{2}= (\|^{t}\|_{2}^{2}+_{j=1}^{t}\|^{j}\|_{2}^{2 }+2(^{t})^{}(t))\), where \((t)=_{j=1}^{t}^{j}\) for \(t=1,...,M\). Instead of one \(\), we now have \(M\) values \(_{t}\) since not all memories are known at once. We can still achieve \(O_{1}(M)\) compute-time per \(_{t}\), repeating the trick from the bi-directional case, but that would need to be followed by the re-computation of \((^{})\) (with new \(\)-parameter) for \(=1,...,t\) which of course is not possible since vectors \(\{\}_{=1}^{t}\) are not explicitly stored (and for a good reason - computational benefits), see: Eq. 3.

Thickening Mnemosyne's memory:To obtain efficient uni-directional Mnemosyne's memory cell also for the \(_{F++}\)-mechanism, we propose to "thicken" in that setting the hidden state from Eq. 3, replacing \(_{}(t)=(_{t},_{t})\) with \(_{}(t)=(\{_{t}^{}\}_{},\{_ {t}^{}\}_{},_{t},_{t})\), where we have: \(_{t}=_{j=1}^{t}^{j}\), \(_{t}=_{j=1}^{t}\|^{j}\|_{2}^{2}\) and furthermore: \(_{t}^{}\), \(_{t}^{}\) correspond to versions of \(_{t}\) and \(_{t}\)respectively, using parameter \(\) to define mapping \(\). The set \(\) is obtained by discretizing interval \((0,1)\) into a fixed number of chunks \(c\) (and effectively quantizes \((0,1)\)). The strategy is now clear: when the new pattern comes, we first update the entire thickened state, and then compute \(^{*}\). We finalize by finding \(\) closest to \(^{*}\) to transform an input and using for that the "slice" of the hidden state corresponding to \(\). We see that all these operations can be made efficiently with only \(c\)-multiplicative term (**independent** from the number of patterns \(M\)) in space and time complexity.

FAVOR++ mechanism, as FAVOR+, can also be adapted to its hyperbolic cosine variant. In practice FAVOR+ mechanism worked similarly to FAVOR++, yet the proper adaptation of the latter one was important, since (see: Sec. 4), this variant provides strongest theoretical guarantees for the capacity of the entire compact associative memory model.

### The proof of the extended version of Theorem 4.3

We start by providing an extended version of Theorem 4.3, enriched with the exact formula of the variance of \((E_{})\). We prove it below. We borrow the notation from Sec. A.1.

**Theorem A.1** (storage of compact associative memories).: _Denote by \(^{1},...,^{M}\{-1,+1\}^{N}\) the memory-vectors. Assume that the Hamming distance between any two memory-vectors is at least \( N\) for some \(>0\). Take some \(0<<\). Then the following is true for any memory-vector \(^{l}\) for \(l=1,...,\) and any input \(^{l}(^{l}, N)\) as long as \(M(2N(-2))}{2e^{2}}\): the expected change of the energy of the compact associative memory system \((E_{})\) associated with flipping the value of the dimension of \(^{l}\) is positive if that operation increases the distance from its close neighbor \(^{l}\) and is negative otherwise. Furthermore, the variance of \((E_{})\) is of the form:_

\[((E_{}))=(V_{1}+V_{2}-2V_{3}-V_{4}-V _{5}+2V_{6})\] (12)

_where:_

\[V_{1}=_{_{1},_{2}\{1,...,M\}}(^{_{1}}+^{_{2}}+2 ^{l}),\ \ V_{2}=_{_{1},_{2}\{1,...,M\}}(^{_{1}}+^{_{2}}+ 2^{l})\]

\[V_{3}=_{_{1},_{2}\{1,...,M\}}(^{_{1}}+^{_{2}}+ ^{l}+^{l})\ \ V_{4}=_{_{1},_{2}\{1,...,M\}}((^{_{1}})^{} ^{l})((^{_{2}})^{}^{l})\]

\[V_{5}=_{_{1},_{2}\{1,...,M\}}((^{_{1}})^{}^{l})((^{_{2}})^{}^{l})\ \ V_{6}=_{_{1},_{2}\{1,...,M\}}((^{_{1}})^{} ^{l})((^{_{2}})^{}^{l})\] (13)

_for \(^{l}\) denoting \(^{l}\) with one of its dimensions flipped and:_

\[()}}{{=}}D^{4}(-2N)(1+ 8)^{-}(}{2(1-8)}\|\|^{2})\] (14)

Proof.: Take a memory \(^{l}\{-1,+1\}^{N}\) and an input \(^{l}(^{l}, N)\). Denote by \((^{l},i)\) a vector obtained from \(^{l}\) by replacing \(^{l}(i)\) with \(-^{l}(i)\). Let us study the change of the energy of the system as we flip the value of the ith dimension of the input \(^{l}\) since the sign of this change solely determines the update that will be made. We have the following:

\[(E_{})=E((^{l},i);^{1},...,^{ M})-E(^{l};^{1},...,^{M})=E_{}+E_{},\] (15)

where:

\[E_{}=_{k=1}^{r}(W_{k}^{l}-Z_{k}^{l}),\] (16)

\[E_{}=_{k=1}^{r}_{\{1,...,M\}\{ l\}}(W_{k}^{}-Z_{k}^{}),\] (17)and furthermore: \(W_{k}^{i}=a_{k}^{i}b_{k}\), \(Z_{k}^{i}=a_{k}^{i}c_{k}\) for:

\[ a_{k}^{i}&=D(-)(B _{k}^{}^{i}-\|_{k}\|_{2}^{2}),\\ b_{k}&=D(-)(B_{k}^{} ^{l}-\|_{k}\|_{2}^{2}),\\ c_{k}&=D(-)(B_{k}^{} (^{l},i)-\|_{k}\|_{2}^{2}). \] (18)

If \(_{1},...,_{r}(0,_{N})\) then, from the fact that \(E_{}\) is the unbiased estimation of \(E_{}\), we get:

\[[X_{k}]&=((^{l})^{ }^{l}),\\ [Y_{k}]&=((^{l})^{} (^{l},i)),\\ [W_{k}^{}]&=((^{})^{} ^{l}),\\ [Z_{k}^{}]&=((^{})^{} (^{l},i)),\] (19)

This is a direct consequence of the OPRF-mechanism introduced in . Variables: \(X_{k}\), \(Y_{k}\), \(W_{k}^{}\) and \(Z_{k}^{}\) for \(=1,...,M\) are simply unbiased estimators of the softmax-kernel values obtained via applying OPRF-mechanism. Let us now compute the expected change of the energy of the system:

\[[(E_{})]=[E_{}]+ [E_{}],\] (20)

where:

\[[E_{}]=_{k=1}^{r}([X_{k}]- [Y_{k}])=_{k=1}^{r}(((^{l})^{} ^{l})-((^{l})^{}(^{l},i)))\] (21)

and

\[[E_{}]&=_{k=1}^{r}_{\{1,...,M\}\{l\}}([W_{k} ^{}]-[Z_{k}^{}])=\\ _{k=1}^{r}_{\{1,...,M\}\{l\}} (((^{})^{}^{l})-((^{})^{} (^{l},i)))\] (22)

We will first upper bound \(|[E_{}]|\). We have:

\[|[E_{}]|& _{k=1}^{r}_{\{1,...,M\}\{l\}}((( ^{})^{}^{l})+((^{})^{}( ^{l},i)))\\ _{k=1}^{r}_{\{1,...,M\}\{l\}}((N( 1-2(-)))+(N(1-2(-)+)))\\ & 2M(N(1-2(-)+))\] (23)

We will now consider two cases:

**Case 1: \(^{l}(i)=^{l}(i)\):**

In this setting, flipping the value of the ith dimension of the input vector increases its distance from the close neighbor. Therefore in this case we would like the energy change of the system to be positive (so that the flip does not occur). From the Equation 21, we obtain:

\[[E_{}]& _{k=1}^{r}((N(1-2))-(N(1-2)-2)))=\\ &(N(1-2))(1-e^{-2})\] (24)Thus we obtain:

\[[(E_{ rand})](N(1-2))(1-e^{-2})-2M(N(1-2(- )+))\] (25)

Therefore, if the following holds:

\[M(2N(-2))}{2e^{2}},\] (26)

then \([(E_{ rand})]>0\).

**Case 2: \(^{l}(i)=-^{l}(i)\):**

In this setting, flipping the value of the ith dimension of the input vector decreases its distance from the close neighbor. Therefore in this case we would like the energy change of the system to be negative (so that the flip does not occur). From the Equation 21, we obtain:

\[[E_{ signal}]_{k=1} ^{r}((N(1-2))-(N(1-2)+2)))=\\ (N(1-2))(1-e^{2})\] (27)

Thus we obtain:

\[[(E_{ rand})](N(1-2))(1-e^{2})+2M(N(1-2( -)+))\] (28)

Therefore, if the following holds:

\[M(2N(-2))-1}{2e^{2}},\] (29)

then \([(E_{ rand})]<0\). Note that the bound from Inequality 26 is stronger than the one from Inequality 29. That completes the proof of the first part of the theorem.

Now we will compute the variance of \((E_{ rand})\). Denote:

\[Z_{k}=_{\{1,...,M\}}(W_{k}^{}-Z_{k}^{})\] (30)

Note that if \(_{1},...,_{r}\) are chosen independently then \(Z_{k}\) for \(k=1,...,r\) are independent. The following is true:

\[&((E_{ rand}))=(E_{  signal}+E_{ noise})=(_{k=1}^{r}_{ \{1,...,M\}}(W_{k}^{}-Z_{k}^{}))\\ &=(_{k=1}^{r}Z_{k})=}_{k=1}^{r}(Z_{k})=}_{k=1}^{r} (_{\{1,...,M\}}(W_{k}^{}-Z_{k}^{}))\\ &=}_{k=1}^{r}([(_{ \{1,...,M\}}(W_{k}^{}-Z_{k}^{}))^{2}]-([_{\{1,...,M\}}(W_{k}^{}-Z_{k}^{})])^{2} )\] (31)Therefore we have:

\[((E_{}))=}_{k=1}^{r}(_{_{1},_{2}\{1,,M\}}[W_ {k}^{_{1}}W_{k}^{_{2}}]+_{_{1},_{2}\{1,,M\}}[ Z_{k}^{_{1}}Z_{k}^{_{2}}])\\ -}_{k=1}^{r}_{_{1},_{2}\{1,,M \}}[W_{k}^{_{1}}Z_{k}^{_{2}}]\\ -}_{k=1}^{r}(_{_{1},_{2}\{1, ,M\}}[W_{k}^{_{1}}][W_{k}^{_{2}}]+_{_{1},_{2}\{1,,M\}}[Z_{k}^{_{1}}][Z_{k}^{_{2}} ])\\ -}_{k=1}^{r}_{_{1},_{2}\{1,,M \}}[W_{k}^{_{1}}][Z_{k}^{_{2}}]\] (32)

Note that from the fact that our random feature map based estimators are unbiased, we get (as we already noted before in Equation 19 and put here again for Reader's convenience):

\[[W_{k}^{}]=((^{})^{} ^{}),\\ [Z_{k}^{}]=((^{})^{}( ^{},i)),\] (33)

Let us now define:

\[()=D^{4}(-2N)(B^{}-4\| \|_{2}^{2}).\] (34)

Note that the following is true:

\[[W_{k}^{_{1}}W_{k}^{_{2}}]=( ^{_{1}}+^{_{2}}+2^{})\\ [Z_{k}^{_{1}}Z_{k}^{_{2}}]=(^{_{1}}+ ^{_{2}}+2(^{},i))\\ [W_{k}^{_{1}}Z_{k}^{_{2}}]=(^{_{1}}+ ^{_{2}}+^{}+(^{},i)) \] (35)

Thus it remains to find closed-form formula for \(()\) for any given \(^{N}\).

From the proof of Theorem 3.1 in , we get for \(A<0\):

\[[(A\|\|^{2}+B^{})]=(1-2A)^{-}(}{2(1-2A)}\|\|^{2})\] (36)

Thus we obtain:

\[()=D^{4}(-2N)(1+8)^{-}( {B^{2}}{2(1-8)}\|\|^{2})\] (37)

Plugging to Equation 32 formulae from Equation 33 and Equation 35 and utilizing Equation 37 for \(\), we obtain the formula for the variance from the statement of the Theorem. 

## Appendix B Experiment details

### Warm-up for Mnemosyne and other optimizers: additional results

**Preliminaries:** At each timestep \(t\), gradient \( f(_{t})\) is input to the optimizer. The gradient is pre-processed as proposed in . Coordinate-wise Mnemosyne's using two temporal encoders is applied. The Mnemosyne's memory cell interfaces with the rest of the system similarly to any RNN-cell. Each cell uses exponential discount factor \(=0.1\), \(r=16\) random projections, \(16\) hidden dimensions and \(1\) attention head. The memory cell output is fed to a fully connected layer, returning the update to be applied to the NN parameters of the optimizee.

**Meta-training:** We refer to training optimizer's parameters \(\) as _meta-training_ to distinguish from the optimizee NN training. Mnemosyne's optimizer is meta-trained on MNIST classification task with small MLP and \(3\) small ViT models. The optimizee MLPs are sampled from this hyperparameter distribution: \(l\) hidden layers of size in range \(\) and \(\) or \(\) activation function. The optimizee ViTs have \(l\) layers, \(h\) heads, with hidden dimension in range \(\), mlp dimension in range \(\) and head dimension in range \(\). The optimizee task is to train the model for \(100\) steps on batches of \(64\) image-class examples.

Hybrid loss function to improve generalization:To promote generalization, we use the random-scaling trick proposed by . Mnemosyne's optimizer is meta-trained by gradient descent using Adam optimizer with learning rate \(=3e^{-4}\) to minimize a combination of two loss functions. The first is the task loss given by the sum of optimizee losses in a truncated roll-out of \(5\) MNIST training steps. The other one is an imitation loss given by the mean squared error between Mnemosyne's updates and expert-optimizer (Adam) updates for same inputs. Importantly, this imitation loss is different from the one proposed in  which uses off-policy expert roll-outs for imitation. In our case, we provide expert supervision for the on-policy updates. This mitigates the problem of divergence from expert's trajectory, often observed in behaviour cloning. Our imitation loss acts as a regularizer which prevents Mnemosyne's optimizer from over-fitting on the optimizee task that it is trained on. We emphasize that expert's learning rate \(_{}=3e^{-2}\)**was not obtained via any tuning process**.

Our optimizer model has minimal input feature engineering and our meta-training setup is significantly simpler than those considered in the literature . Even so, we can successfully apply Mnemosyne's optimizer to a variety of tasks due to its efficient memory mechanism. Furthermore, Mnemosyne's memory cells can be easily combined with any of the existing L2L methods that use LSTMs for memory-encoding.

**Results:** After meta-training, Mnemosyne's optimizer was tested on NN training tasks with different NN architectures and datasets. Recall that Mnemosyne only saw one ML task of MNIST classifier training for \(100\) steps during meta-training. Fig. 10 shows that Mnemosyne can optimize MLPs with different NN architectures and activation functions on MNIST image classifier training. Note that, Mnemosyne converges significantly faster than popular analytical optimizers, RMSprop and Adam while retaining similar asymptotic performance. Mnemosyne can train NNs for long horizons of thousands of steps while baseline LSTM optimizer  struggles to minimize classification loss beyond a few hundred steps.

**Transformers:** The results were already presented in the main body of the paper (see: Sec. 5.1). We want to add that, as for experiments from Fig. 10, here Mnemosyne's optimizer is faster than standard analytical optimizers and much more stable than LSTM optimizer. Fig. 11 shows the benefit of using expert imitation-loss for long-horizon stability of the Mnemosyne's optimizer.

Our results on training Transformers with Mnemosyne naturally lead to the question of the role that Transformer-based optimizers can play in training Transformers architectures. It is well known that Transformer training requires nontrivial optimization techniques , e.g. learning rate schedulers (for that reason SGD was replaced with Adam in Transformer-training). Furthermore, for larger architectures training is slow, often prohibitively (unless the model is trimmed down, for instance by replacing long-range attention modeling with local attention of the controllable attention radius). Attention-based optimizers can potentially address this problem, since they improve convergence (and thus effectively reduce training time) even if meta-trained on much simpler tasks as we show in Fig. 3.

Figure 10: Validation loss curves when training MLP with Mnemosyne compared to other methods for MNIST image classification. Optimization curves for \(4\) different MLP architectures in this order: (1 layer, \(20\) hidden dim, \(\) activation), (2 layers, \(20\) hidden dim, \(\) activation), (1 layer, \(40\) hidden dim, \(\) activation), (1 layer, \(20\) hidden dim, \(\) activation) are shown.

### Mnemosyne's CAM mechanism vs regular attention

We have tried to use regular Transformer blocks to encode associative memory for Mnemosyne's temporal module. For applying regular attention to online optimizer autoregressively, a limited-length cache of historical gradients has to be maintained. A self-attention map over the history sequence is generated and used to encode memory. Fig. 12 (left) shows the meta-training curves for regular attention optimizers with different history cache lengths. As we increase the cache length, the performance improves and the memory requirement scales quadratically. Due to this limitation, we could not implement a regular attention based optimizer with cache length more than \(100\). On the other hand, Performer's memory cell defining CAM can attend to theoretically unbounded history and out-performs regular attention variants with fixed memory requirement.

### Different RF-mechanisms: detailed look

Fig. 12 (middle) compares the performance of Mnemosyne's optimizer applying FAVOR+ and FAVOR++ mechanisms in CAM. FAVOR++ mechanism provides strongest theoretical guarantees for the capacity of the associative memory model. It also leads initially to faster convergence, but asymptotically performs similarly as the FAVOR+ variant. Due to the simpler implementation of FAVOR+, we use it for all experiments with Mnemosyne's optimizer.

Optimizers with both regular positive and hyperbolic random features kernel learn similarly, but the latter has much lower variance (see: Fig. 12 (right)) and thus it became our default choice.

Figure 11: Impact of training the optimizer with combined meta loss and imitation loss can be seen in generalization to a long horizon rollout. All variants were trained only on length \(100\) rollouts.

Figure 12: Ablation Studies. **Left:** Comparison of the Mnemosyne’s linear CAM with regular attention memory blocks with different history cache lengths (\(h\)). **Middle:** Meta-training curves of Mnemosyne optimizer with FAVOR+ and FAVOR++ mechanism for CAM. **Right:** Meta-training curves of Mnemosyne optimizer with different kernel transformation functions for CAM.

### Ablations over different kernel functions in CAM mechanism

In Fig. 13, we compare different kernel functions for linear attention in CAM. Our ablation study shows that FAVOR+ method outperforms other functions. This informs the choice of the kernel in all our experiments.

### Ablations over different discount factors and number of RFs in CAM mechanism

In Fig. 14, we present detailed ablation studies over discount factors \(\) as well as the number of random features applied by our default CAM mechanism leveraging hyperbolic cosine random features.

### Benchmarking different depths of the temporal module

Finally, we run ablations over different number of temporal encoders in Mnemosyne's temporal block. We noticed that modest increase of the number of encoders improves loss in meta-training and meta-training very deep variants is particularly challenging (as requiring much more data). Since in this paper we decided to use simple meta-training strategies and furthermore increasing the number of temporal encoders did not lead to substantial gains, we decided to choose shallow temporal encoders' architectures. The results are presented in Fig. 15.

### Compute Resources Used

All Mnemosyne optimizer variants were trained and tested on a TPU pod containing \(4\) TPU v3 chips with JAX. Hundreds of rounds of training and inference were needed to compare different variations, tasks and meta-losses.

Figure 14: **Left:** The comparison of Mnemosyne applying different discount factors with \(\) optimizer in meta-training (MLP optimization). **Right**: The comparison of Mnemosyne applying different number of random features in the hyperbolic cosine random feature mechanism used in CAM.

Figure 13: Comparison of the coordinate-wise Mnemosyne with ReLU kernel, square kernel and FAVOR+ approximation of the softmax kernel. We present the meta-training loss of the three variants. FAVOR+ mechanism outperforms other variants.

### Coordinate-wise Mnemosyne versus hard-coded optimizers for larger ViTs

#### b.8.1 ViT last-layer fine-tuning

In this study, we benchmarked Mnemosyne on different sizes of ViT architectures: ViT-Base, ViT-Large and ViT-Huge (ViT-B(x), ViT-L(x) and ViT-H(x) respectively, where \(x\) defines the patch size), see: Tab: 2. We used the coordinate-wise variant of the Mnemosyne. We run tests on the following datases: \(\), \(\) and \(\)-\(2011\). We were optimizing the last layer of the ViT-architecture and used \(\) expert with learning rate \(=3e^{-2}\) as a regularizer (see: our discussion above on meta-training). The learning rate was not tuned in any way. In fact (as we show below) \(\) optimizer applying this learning rate is characterized by the sub-optimal performance. We tried two versions of Mnemosyne: (a) a variant that solely optimizes the last layer of ViT (reported in the main body) and (b) the _hybrid_ variant, where Mnemosyne is used to optimize the weight-matrix of the last layer and \(\) with learning rate \(=e^{-3}\), to optimize the bias vector. That learning rate was also not tuned in any particular way and, as before, if applied purely within \(\), produces sub-optimal results. The purpose of that last experiment was to assess how efficient the strategy of optimizing jointly with Mnemosyne and a hand-designed optimizer is. The results are presented in Fig. 16 and Fig. 17. We see that: (a) Mnemosyne without any hyperparameter tuning matches or outperforms optimal \(\) variants, (b) it also substantially outperforms \(\) variant used as an expert in meta-training. This is valid for both: regular Mnemosyne as well as the hybrid version.

#### b.8.2 ViT multi-layer fine-tuning

Here, we used a _light_ version of coordinate-wise Mnemosyne using a single temporal encoder layer with hidden dimension \(8\). This reduced the memory requirement of the Mnemosyne optimizer state. We fine-tune ViT-B model on CIFAR-100 dataset with batch size \(128\). We were able to fine-tune last \(2\) transformer layers along with the embedding, cls and head layers with Mnemosyne. Rest of the model was fine-tuned with \(\) (learning rate = \(1e^{-3}\)). For comparison, the same baseline \(\) variant is to fine-tune the complete model.

Figure 16: Coordinate-wise Mnemosyne across different ViT architectures and datasets, as described in Sec. B.8.1. Mnemosyne matches or outperforms optimal \(\) variants without any hyperparameter tuning.

Figure 15: Comparison of the meta-training loss for Mnemosyne variants applying different number of temporal encoders \(k\). Since several variants on the left figure performs similarly, on the right figure we highlight top two. The most, remaining is conducted on the MT D optimization racke and MNIST data.

### Tensor-wise Mnemosyne versus hard-coded optimizers for ViT-H

We finetuned the embedding and cls layer of ViT-H (see Tab: 2 for hyperparameter) using tensorwise (\( 1M\) params), while the head was trained using Adam. The rest of the transformer parameters are fixed to the pre-trained value for all methods. The batch size was set at 128 for all methods.

### Super-Mnemosyne: combining coordinate- and tensor-wise strategies for ViTs

We finetuned the top-8 layers of the ViT-Base model (see Tab: 2) along with the head, cls and embedding layer before we ran out of memory ie \( 50M\) parameters with a batch size of 256. Large tensor such as: a) the MLP block withing each layer, b) the head layer was finetuned using lite version of coordinate-wise. Rest of the tensors were finetuned using tensorwise. The bottom 4 layers of the model were kept fixed for Mnemosyne. For Adam baselines we finetuned all layers.

### BERT-pretraining NLP Transformers with Mnemosyne

We trained the Bert base model, whose Hyperparameters are shown in Tab: 3. The details of the training dataset used is shown in Tab: 4. We trained all parameters from scratch for all methods, with a batch size of 512. For the Mnemosyne results shown in Fig: 7, we trained all parameters except the token embedding using Tensorwise Mnemosyne (\( 86M\) parameters). The token embedding was trained using Adam with learning rate \(1e-4\). For Adam baseline we trained all parameters.

### Soft prompt-tuning massive T5XXL Transformers with Mnemosyne

We use coordinate-wise Mnemosyne to prompt-tune  T5XXL model  (see Table 5 for hyper-parameters) on SuperGLUE benchmark. Batch size \(32\) was used. The length of the soft-prompt sequence was \(30\) and each soft-prompt vector was of size \(4096\), making the total number of trainable parameters \(122880\).

   Model & Heads & Layers & Hidden Dim. & MLP Dim. & Params & Patch Size \\  ViT-Base & 12 & 12 & 768 & 3072 & 86M & 16 \\ ViT-Large (16) & 24 & 16 & 1024 & 4096 & 307M & 16 \\ ViT-Large (32) & 24 & 16 & 1024 & 4096 & 307M & 32 \\ ViT-Huge & 32 & 16 & 1280 & 5120 & 632M & 32 \\   

Table 2: Hyperparameters for the different ViT models used in this paper

Figure 17: The results from Fig. 16, but narrowed down to the comparison between two Mnemosyne variants and \(\) optimizer applying learning used in meta-training of these variants of Mnemosyne. The expert substantially underperforms in all the cases (we explicitly put the gains coming from the two variants of Mnemosyne as compared to the expert variant). This shows that Mnemosyne does not learn to imitate the expert.

   Model & Heads & Layers & Hidden Dim. & MLP Dim. & Params & Compute & Loss \\  Bert-Base & 12 & 12 & 768 & 3072 & 110M & 4x2 TPUv3 & MLM \\   

Table 3: Hyperparameters for the Bert base model

## Appendix C Mnemosyne for training initial optimization conditions

Mnemosyne can be also applied to learn initial conditions for the optimization.

In this section, our considered model of using \(g_{}\) is more general than the one presented in Eq. 1 and is of the form given below for \(g_{}=(g_{_{1}}^{},g_{_{2}}^{})\), \(=[_{1};_{2}]\) and a _context-vector_\(\):

\[_{0}=g_{_{1}}^{}(),\\ _{t+1}=g_{_{2}}^{}(f,_{0},...,_{t})t>0\] (38)

The optimizer \(g\) is now explicitly split into two-parts: (1) \(g_{_{1}}^{}\) that learns initial optimization point from the context-vector, e.g. the image encoding the scene (as it is the case in learnable-MPC setting ), and (2) \(g_{_{2}}^{}\) that processes the history of the optimization steps, as we described before. Depending on the application, either one or the other optimizer (or both) are turned on. Critically, both are encoded as light scalable Transformers. Optimizer \(g_{_{1}}^{}\) applies spatial bi-directional attention while \(g_{_{2}}^{}\) uses the spatio-temporal variant, described in the main body of the paper.

Below we show how this paradigm can be used in Robotics to learn initial points for the MPC optimization.

### Spatial Mnemosyne for initializing MPC optimizers

**Preliminaries:** We applied Mnemosyne with bidirectional spatial attention for learning trajectory optimizers to be used for wheeled robot navigation in complex, photo-realistic simulated environments. The robot uses vision sensors for observing an occupancy grid of the environment and navigates using linear and angular velocity control. Navigation in challenging environments requires efficient high-speed robot control. Model Predictive Control (MPC) presents an efficient approach to the navigation problem, provided that the motion-planning with the environment model can be carried out within computational real-time limits . Motion planning with efficient trajectory optimizers such as iterative Linear Quadratic Regulator (iLQR)  is one way to implement MPC. However, in challenging layouts with narrow corridors and doors and with conservative safety and collision avoidance constraints, iLQR struggles to converge to the optimal trajectory. Sequential Quadratic Programming (SQP)  is often a more robust optimizer that can handle difficult non-linear constraints. However, SQP is significantly, sometimes \( 10\) times, slower than iLQR and hence cannot be deployed in real-time settings. Deep Learning models have been shown to accelerate SQP by warm starting the optimization process . We learn Mnemosyne's optimizer to imitate SQP

   Model & Encoder & Decoder & Heads & Head & Embedding & MLP & Params & Compute \\  & Layers & Layers & & Dim. & Dim. & Dim. & & & \\  T5XXL & 24 & 24 & 64 & 64 & 4096 & 10240 & 11B & 2x2x4 \\  & & & & & & & & TPUv3 \\   

Table 4: Dataset used for pre training.

Figure 18: Navigation in a photo-realistic environment with MPC policy using Mnemosyne optimizer for initializing SQP solver.

behaviour and initialize it with an approximately optimal trajectory as a starting point from which SQP refines to an optimized and feasible trajectory.

**Training details:** Mnemosyne's optimizer, \(g_{_{t}}^{init}\), receives current robot pose \(p_{r}\), a goal pose \(p_{g}\) and a visual occupancy grid as the context, \(\). The occupancy grid is processed by an image encoder module, a ViT where the attention mechanism is approximated by bidirectional Mnemosyne memory. As in a ViT, the occupancy grid is first pre-processed by a convolution layer and then flattened to a sequence. Each element (token) of the sequence corresponds to a different \(5 5\) patch of the original frame which is then enriched with positional encoding. The pre-processed input is then fed to \(3\) Mnemosyne attention and MLP layers of hidden dimension \(64\). The final embedding of one of the tokens is chosen as a latent representation of the occupancy grid, \(l_{oc}\). \(p_{r}\), \(p_{g}\) and \(l_{oc}\) are concatenated and processed by an MLP which outputs the predicted action trajectory, \(_{0}\).

An offline dataset of SQP optimization examples is collected by running MPC navigation agent in \(2787\) different environments. Each navigation run has \(180\) steps on average. For each MPC step, one instance of trajectory optimization with SQP was run and a pair of input context \(\) and the final optimal trajectory \(_{T}\) was recorded. A total of \( 500,000\) training examples were collected. Mnemosyne's optimizer was trained with supervised learning on the SQP dataset by minimizing mean squared error between the SQP optimal trajectories and the predicted trajectories.

**Results:** After training, the predicted trajectory from Mnemosyne was used to initialize SQP optimization. Without Mnemosyne initialization, SQP optimization was capped at maximum \(10\) iterations. It took on average \(4.78\) iterations and \(0.12\)sec for the SQP solution to complete. With Mnemosyne initialization, SQP is only run for \(1\) iteration to reach the optimal trajectory. SQP generates a trajectory that satisfies kinematic, dynamic and safety constraints for the robot which transformer alone e.g.  cannot natively guarantee. This reduces the optimization time by more than half to \(0.048\)sec on average which is under real-time constraint. It includes \(0.011\)sec for Mnemosyne inference and the rest for SQP iteration. A sequence of snapshots during navigation with Mnemosyne-SQP optimizer in a sample environment is shown in Fig. 18.