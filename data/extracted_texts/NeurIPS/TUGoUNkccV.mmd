# Correlative Information Maximization: A Biologically Plausible Approach to Supervised Deep Neural Networks without Weight Symmetry

Correlative Information Maximization: A Biologically Plausible Approach to Supervised Deep Neural Networks without Weight Symmetry

 Bariscan Bozkurt\({}^{1,2,3}\)  Cengiz Pehlevan\({}^{4,5}\)  Alper T. Erdogan\({}^{2,3}\)

\({}^{1}\) Gatsby Computational Neuroscience Unit, UCL, United Kingdom

\({}^{2}\)KUIS AI Center, Koc University, Turkey \({}^{3}\)EEE Department, Koc University, Turkey

\({}^{4}\)John A. Paulson School of Engineering & Applied Sciences and Center for Brain Science, Harvard University, Cambridge, 02138 MA, USA

\({}^{5}\)Kempner Institute for the Study of Natural and Artificial Intelligence

{bbozkurt15, alperdogan}@ku.edu.tr cpehlevan@seas.harvard.edu

###### Abstract

The backpropagation algorithm has experienced remarkable success in training large-scale artificial neural networks; however, its biological plausibility has been strongly criticized, and it remains an open question whether the brain employs supervised learning mechanisms akin to it. Here, we propose correlative information maximization between layer activations as an alternative normative approach to describe the signal propagation in biological neural networks in both forward and backward directions. This new framework addresses many concerns about the biological-plausibility of conventional artificial neural networks and the backpropagation algorithm. The coordinate descent-based optimization of the corresponding objective, combined with the mean square error loss function for fitting labeled supervision data, gives rise to a neural network structure that emulates a more biologically realistic network of multi-compartment pyramidal neurons with dendritic processing and lateral inhibitory neurons. Furthermore, our approach provides a natural resolution to the weight symmetry problem between forward and backward signal propagation paths, a significant critique against the plausibility of the conventional backpropagation algorithm. This is achieved by leveraging two alternative, yet equivalent forms of the correlative mutual information objective. These alternatives intrinsically lead to forward and backward prediction networks without weight symmetry issues, providing a compelling solution to this long-standing challenge.

## 1 Introduction

How biological neural networks learn in a supervised manner has long been an open problem. The backpropagation algorithm , with its remarkable success in training large-scale artificial neural networks and intuitive structure, has inspired proposals for how biologically plausible neural networks can perform the necessary efficient credit-assignment for supervised learning in deep neural architectures . Nonetheless, certain aspects of the backpropagation algorithm, combined with the oversimplified nature of artificial neurons, have been viewed as impediments to proposals rooted in this inspiration .

One of the primary critiques regarding the biological plausibility of the backpropagation algorithm is the existence of a parallel backward path for backpropagating error from the output towards the input, which uses the same synaptic weights as the forward path . Although such weight transport, or weight symmetry, is deemed highly unlikely based on experimental evidence ,some biologically plausible frameworks still exhibit this feature, which is justified by the symmetric structure of the Hebbian updates employed in these frameworks [2; 5; 6].

The concerns about the simplicity of artificial neurons have been addressed by models which incorporate multi-compartment neuron models into networked architectures and ascribe important functions to dendritic processing in credit assignment [7; 8; 9; 10]. This new perspective has enabled the development of neural networks with improved biological plausibility.

In this article, we propose the use of correlative information maximization (CorInfoMax) among consecutive layers of a neural network as a new supervised objective for biologically plausible models, which offers

* a principled solution to the weight symmetry problem: our proposed information theoretic criterion aims to maximize the linear dependence between the signals in two neighboring layers, naturally leading to the use of linear or affine transformations in between them. A key property of this approach is that employing two alternative expressions for the correlative mutual information (CMI) results in potentially _asymmetric forward and backward prediction networks_, offering a natural solution to the weight transport problem. Consequently, predictive coding in both directions emerges as the inherent solution to the correlative information maximization principle, fostering signal transmission in both forward and top-down directions through asymmetrical connections. While the CorInfoMax principle enhances information flow in both directions, the introduction of set membership constraints on the layer activations, such as non-negativity, through activation nonlinearities and lateral inhibitions, encourages compression of information and sparse representations .
* a normative approach for deriving networks with multi-compartment neurons: the gradient-based optimization of the CorInfoMax objective naturally leads to network models that employ multi-compartment pyramidal neuron models accompanied by interneurons as illustrated in Figure 1.

As derived and explained in detail in Section 2, the resulting networks incorporate lateral connections and auto-synapses (autapses) to increase the entropy of a layer, promoting utilization of all dimensions within the representation space of that layer. Meanwhile, asymmetric feedforward and feedback connections act as forward and backward predictors of layer activation signals, respectively, to reduce the conditional entropies between layers, targeting the elimination of redundancy.

### Related work

#### 1.1.1 Multi-compartmental neuron model based biologically plausible approaches

Experimentally grounded studies, such as [7; 12], have been influential for considering a role for dendritic-processing in multi-compartmental neurons for learning and credit assignment . Subsequent research has explored biologically plausible models with supervised learning functionality, such as the two-compartment neuron model by Urbanczik and Senn  and the three-compartment pyramidal neuron model by Sacramento et al. . Both models integrate non-Hebbian learning and spike-time dependent plasticity, while the latter includes SST interneurons . Similar frameworks have been proposed by  and , with the latter introducing a normative framework based on multi-compartment neuron structure, top-down feedback, lateral and feedforward connections, and Hebbian and non-Hebbian learning rules, emerging from the optimization of a prediction error objective with a whitening constraint on co-layer neurons.

In a similar vein to , we propose an alternative normative framework based on information maximization principle. In this framework, the three-compartment structure and associated forward, top-down and lateral synaptic connections stem from the maximization of CMI between adjacent layers, without the imposition of any whitening constraint.

#### 1.1.2 Weight symmetry problem

A central concern regarding the biological plausibility of the backpropagation algorithm pertains to the weight symmetry issue: synaptic weights in the feedback path for error backpropagation are transposes of those used in the forward inference path [2; 3; 16]. The requirement of tied weights in backpropagation is questionable for physically distinct feedforward and feedback paths in biological systems, leading many researchers to focus on addressing the weight symmetry issue.

Various strategies have been devised to address the weight symmetry issue. For example, the feedback alignment approach, which fixes randomly initialized feedback weights and adapts feedforward weights, was offered as a plausible solution . Later Akrout et.al.  proposed its extension by updating feedback weights towards to the transpose of the feedforward weights. Along the similar lines, Amit introduced antisymmetry through separate random initializations . Liao et al.  showed that the sign of the feedback weights (rather than their magnitude) affects the learning performance, and proposed the sign-symmetry algorithm.

Intriguingly, this symmetric weight structure is also observed in biologically plausible frameworks such as predictive coding (PC) [21; 22; 23], equilibrium propagation (EP) [24; 25; 26], and similarity matching . This phenomenon can be rationalized by the transpose symmetry of the Hebbian update with respect to inputs and outputs. The EP framework in  unties forward and backward connections inspired by [28; 29], and only yields small performance degradation. A more recent approach by Golkar et al.  addresses this challenge by integrating two alternative forward prediction error loss function terms associated with the same network layer and leveraging presumed whitening constraints to eliminate shared feedback coefficients.

In existing predictive coding-based schemes such as [21; 22; 23], the loss function contains only forward prediction error terms. The feedback connection with symmetric weights, which backpropagates forward prediction error, emerges due to the gradient-based optimization of the PC loss. In contrast, our framework's crucial contribution is the adoption of two alternative expressions for the correlative mutual information between consecutive network layers as the central normative approach. Utilizing these two alternatives naturally leads to both forward and backward prediction paths with asymmetric weights, promoting information flow in both feedforward and top-down directions. Unlike the work of , our method circumvents the need for layer whitening constraints and additional forward prediction terms to achieve asymmetric weights.

#### 1.1.3 Correlative information maximization

Information maximization has been proposed as a governing or guiding principle in several machine learning and neuroscience frameworks for different tasks: (i) The propagation of information within a self-organized network as pioneered by Linsker . (ii) Extracting hidden features or factors associated with observations by maximizing information between the input and its internal representation such as independent component analysis (ICA-InfoMax) approach by . In the neurosciencedomain, the motivation has been to provide normative explanations to the behaviour of cortical activities evidenced by experimental work, such as orientation and visual stimuli length selectivity of primary visual cortex neurons [32; 33]. The same idea has been recently extended in the machine learning field by the Deep Infomax approach where the goal is to transfer maximum information from the input of a deep network to its final layer, while satisfying prior distribution constraints on the output representations . (iii) Matching representations corresponding to two alternative augmentations or modalities of the same input in the context of self-supervised learning .

Correlative mutual information maximization has been recently proposed as an alternative for Shannon Mutual Information (SMI), due to its desirable properties : (i) maximization of CMI is equivalent to maximizing linear dependence, which may be more relevant than establishing arbitrary nonlinear dependence in certain applications , (ii) it is based only on the second order statistics, making it relatively easier to optimize. We additionally note that criteria based on correlation are intrinsically linked to local learning rules, leading to biologically plausible implementations, [38; 39]. Erdogan  proposed the use of CorInfoMax for solving blind source separation (BSS) problem to retrieve potentially correlated components from their mixtures. Ozsoy et al.  proposed maximizing the CMI between the representations of two different augmentations of the same input as a self-supervised learning approach. More recently, Bozkurt et al.  introduced an unsupervised framework to generate biologically plausible neural networks for the BSS problem with infinitely many domain selections using the CMI objective.

In this article, we suggest employing the CorInfoMax principle for biologically plausible supervised learning. The key difference compared to the unsupervised framework presented in  is the utilization of two alternative forms of mutual information. This leads to a bidirectional information flow that enables error backpropagation without encountering the weight symmetry issue.

## 2 Deep correlative information maximization

### Network data model

We assume a dataset with \(L\) input data points \([t]^{m}\), \(t=1,,L\), and let \(_{T}[t]^{n}\) be the corresponding labels. We consider a network with \(P-1\) hidden layers whose activities are denoted by \(^{(k)}^{N_{k}}\), \(k=1,,P-1\). For notational simplicity, we also denote input and output of the network by \(^{(0)}\) and \(^{(P)}\), i.e., \(^{(0)}[t]=[t]\) and \(^{(P)}[t]=}[t]\). We consider polytopic constraints for the hidden and output layer activities, i.e., \(^{(k)}^{(k)}\), where \(^{(k)}\) is the presumed polytopic domain for the \(k\)-th layer [11; 40]. We note that the polytopic assumptions are plausible as the activations of neurons in practice are bounded. In particular, we will make the specific assumption that \(^{(k)}=_{,+}=\{: \}\), i.e., (normalized) activations lie in a nonnegative unit-hypercube. Such nonnegativity constraints have been connected to disentangling behavior [41; 42; 43], however, we consider extensions in the form of alternative polytopic sets corresponding to different feature priors  (see Appendix C). More broadly, the corresponding label \(_{T}\) can be, one-hot encoded label vectors for a classification problem, or discrete or continuous valued vectors for a regression problem.

### Correlative information maximization based signal propagation

Our proposed CorInfoMax framework represents a principled approach where both the structure of the network and its internal dynamics as well as the learning rules governing adaptation of its parameters are not predetermined. Instead, these elements emerge naturally from an explicit optimization process. As the optimization objective, we propose the maximization of _correlative mutual information_ (see Appendix A between two consecutive network layers. As derived in future sections, the proposed objective facilitates information flow--input-to-output and vice versa, while the presumed domains for the hidden and output layers inherently induce information compression and feature shaping.

In Sections 2.2.1 and 2.2.2, we outline the correlative mutual information-based objective and its implementation based on samples, respectively. Section 2.3 demonstrates that the optimization of this objective through gradient ascent naturally results in recurrent neural networks with multi-compartment neurons. Finally, Section 2.4 explains how the optimization of the same criterion leads to biologically plausible learning dynamics for the resulting network structure.

#### 2.2.1 Stochastic CorInfoMax based supervised criterion

We propose the total correlative mutual information among consecutive layers, augmented with the mean-square-error (MSE) training loss, as the stochastic objective to be maximized:

\[J(^{(1)},,^{(P)})=_{k=0}^{P-1}I^{( _{k})}(^{(k)},^{(k+1)})-E(\|_{T}-^{(P)}\|_{2}^{2}),\] (1)

where, as defined in  and in Appendix A,

\[I^{}_{(_{k})}(^{(k)},^{(k+1)})=(_{^{(k+1)}}+_{k})- (_{(k+1)}{}_{*}^{(k+ 1)}}+_{k}),\] (2)

is the correlative mutual information between layers \(^{(k)}\) an \(^{(k+1)}\), \(_{^{(k+1)}}=E(^{(k+1)}^{(k+1)})\) is the autocorrelation matrix corresponding to the layer \(^{(k+1)}\) activations, and \(_{(k+1)}{}_{*}^{(k+1)}}\) corresponds to the error autocorrelation matrix for the best linear regularized minimum MSE predictor of \(^{(k+1)}\) from \(^{(k)}\). Therefore, the mutual information objective in (2) makes a referral to the _regularized_ **forward**_prediction problem_ represented by the optimization

\[_{ff}^{(k)}}{}E(\|}{{}}^{(k+1)}\|_{2}^{2})+_{k}\|_{ff}^{(k)} \|_{F}^{2}}{{}}^{(k+1)}= ^{(k+1)}-_{ff}^{(k)}^{(k)},\] (3)

and \(_{*}^{(k+1)}\) is the forward prediction error corresponding to the optimal forward predictor \(_{ff,*}^{(k)}\).

If we interpret the maximization of CMI in (2): the first term on the right side of (2) encourages the spread of \(^{(k+1)}\) in its presumed domain \(^{(k+1)}\), while the second term incites the minimization of redundancy in \(^{(k+1)}\) beyond its component predictable from \(^{(k)}\).

An equal and alternative expression for the CMI can be written as (Appendix A)

\[I^{}_{(_{k})}(^{(k)},^{(k+1)})=(_{^{(k)}}+_{k})- (_{}{{}}_{*}^{( k)}}+_{k}),\] (4)

where \(_{}{{}}_{*}^{(k)}}\) corresponds to the error autocorrelation matrix for the best linear regularized minimum MSE predictor of \(^{(k)}\) from \(^{(k+1)}\). The corresponding _regularized_ **backward**_prediction problem_ is defined by the optimization

\[_{fb}^{(k)}}{}E(\|}{{}}^{(k)}\|_{2}^{2})+_{k}\|_{fb}^{(k)} \|_{F}^{2}}{{}}^{(k)}= ^{(k)}-_{fb}^{(k)}^{(k+1)}.\] (5)

We observe that the two alternative yet equivalent representations of the correlative mutual information between layers \(^{(k)}\) and \(^{(k+1)}\) in (2) and (4) are intrinsically linked to the forward and backward prediction problems between these layers, which are represented by the optimizations in (3) and (5), respectively. As we will demonstrate later, the existence of these two alternative forms for the CMI plays a crucial role in deriving a neural network architecture that overcomes the weight symmetry issue.

#### 2.2.2 Sample-based supervised CorInfoMax criterion

Our aim is to construct a biologically plausible neural network that optimizes the total CMI, equation (1), in an adaptive manner. Here, we obtain a sample-based version of (1) as a step towards that goal. We first define the exponentially-weighted sample auto and cross-correlation matrices as follows:

\[}_{^{(k)}}[t]= }}{1-_{}^{t}}_{i=1}^{t}_{}^{t-i}^{(k)}[i]^{(k)}[i]^{T},}_{^{(k+1)}}[t]= }}{1-_{}^{t}}_{i=1}^{t} _{}^{t-i}^{(k)}[i]^{(k+1)}[i]^{T},\] (6)

for \(k=0,,P\), respectively, where \(0_{}<1\) is the forgetting factor. Next, we define two equivalent forms of the sample-based CMI, \(^{()}(^{(k)},^{(k+1)})[t]\):

\[^{}_{(_{k})}(^{(k)},^{(k+1)})[t] =(}_{^{(k+1)}}[t]+ _{k})-(}_{}{{}}_{*}^{(k+1)}}[t]+_{k}),\] (7) \[^{}_{(_{k})}(^{(k)},^{(k+1)})[t] =(}_{^{(k)}}[t]+ _{k})-(}_{}{{}}_{*}^{(k)}}[t]+_{k}),\] (8)where \(}_{}_{*}^{(k+1)}}[t]\) is the exponentially-weighted sample autocorrelation matrix for the forward prediction error at level-\((k+1)\), \(}^{(k+1)*}[t]\), corresponding to the best linear exponentially-weighted regularized least squares predictor of \(}^{(k+1)}[t]\) from the lower level activations \(}^{(k)}[t]\). Similarly, \(}_{}^{(k)}}[t]\) is the exponentially-weighted autocorrelation matrix for the backward prediction error at level-\((k)\), \(}^{(k)}[t]\), corresponding to the best linear exponentially-weighted regularized least squares predictor of \(}^{(k)}[t]\) from the higher level activations \(}^{(k+1)}[t]\).

The sample-based CorInfoMax optimization can be written as:

\[}^{(k)}[t],k=0,,P}{} _{k=0}^{P-1}^{(_{k})}(}^{(k)},}^{(k+1)})[t]-\|}_{T}[t]- }^{(P)}[t]\|_{2}^{2}=(}^{(1)},,}^{(P )})[t]\] (9a) \[ }^{(k)}[t]^{(k)},k=1,,P,\] (9b) \[}^{(0)}[t]=}[t],\] (9c)

As outlined in Appendix B, we can employ Taylor series linearization to approximate the \(\) terms associated with forward and backward prediction errors in (2) and (4) in the form

\[}_{}^{(k+1)}}[t]+ _{k}\] \[}_{i=1}^{t}_{ }}^{t-i}\|}^{(k+1)}[i]-}_{ff,*}^{(k)}[t]}^{(k)}[i]\|_{2}^{2}+_{k}\|}_{ff,*}^{(k)}[t]\|_ {F}^{2}+N_{k+1}(_{k})\] (11) \[}_{}^{(k)}}[t]+ _{k}\] \[}_{i=1}^{t}_{ }}^{t-i}\|}^{(k)}[i]-}_{fb,*}^{(k)}[t]}^{(k+1)}[i]\|_{2}^{2}+_{k}\|}_{fb,*}^{(k)}[t]\|_{F}^{2}+N_{k} (_{k}),\]

where \(}_{ff,*}^{(k)}[t]\) is the optimal linear regularized weighted least squares forward predictor coefficients in predicting \(}^{(k+1)}[i]\) from \(}^{(k)}[i]\) for \(i=1,,t\), and \(}_{fb,*}^{(k)}[t]\) is the optimal linear regularized weighted least squares backward predictor coefficients in predicting \(}^{(k)}[i]\) from \(}^{(k+1)}[i]\) for \(i=1,,t\). Consequently, the optimal choices of forward and backward predictor coefficients are coupled with the optimal choices of layer activations.

In the online optimization process, we initially relax the requirement on the optimality of predictors and start with random predictor coefficient selections. During the learning process, we apply a coordinate ascent-based procedure on activation signals and predictor coefficients. Specifically, at time step-\(t\), we consider two phases:

1. First, we optimize with respect to the activations \(\{}^{(k)}[t],k=1,,P\}\), where we assume predictor coefficients to be fixed. This phase yields network structure and output dynamics,
2. Next, we update the forward and backward predictor coefficients \(_{ff}^{(k)}\) and \(_{fb}^{(k)}\), for \(k=1,,P\), to reduce the corresponding forward and backward prediction errors, respectively. This phase provides update expressions to be utilized in learning dynamics.

As the algorithm iterations progress, the predictor coefficients converge to the vicinity of their optimal values.

For the first phase of the online optimization, we employ a projected gradient ascent-based approach for activations: for \(k=1,,P-1\), the layer activation vector \(}^{(k)}[t]\) is included in the objective function terms \(^{()}(}^{(k-1)},}^{(k)})[t]\) and \(^{()}(}^{(k)},}^{(k+1)})[t]\). Therefore, to calculate the gradient with respect to \(}^{(k)}[t]\), we can use expressions in (7) and (8). More specifically, we choose \(_{k}(}^{(k)})[t]=^{(_{k-1})}( }^{(k-1)},}^{(k)})[t]+^{(_{k})}(}^{ (k)},}^{(k+1)})[t]\) for \(k=1,,P-1\), to represent the components of the objective function in (9a) involving \(}^{(k)}[t]\). As described in Appendix G.1, this choice is instrumental in avoiding weight transport problem. Similarly, we can write the component of the objective function in (9a) that is dependent on the final layer activations as \(_{P}(^{(P)})[t]=^{()}(^{(P-1)}, ^{(P)})[t]-\|^{(P)}[t]-_{T}[t]\|_{ 2}^{2}\).

Based on the derivations presented in Appendix D, which directly incorporate the approximations from (10) and (11), we can express the gradient of the objective function in (9a) with respect to \(^{(k)}\), for \(k=1,,P-1\) as:

\[_{^{(k)}}(^{(1)},,^{(P)})[t] =2_{^{(k)}}[t]^{(k)}[t]-}}^{(k)}[t]-}}^{(k)}[t],\] (12)

where \(=}}{_{}}\),

\[}^{(k)}[t]=^{(k)}[t]-_{ff}^ {(k-1)}[t]^{(k-1)}[t],}^{(k)}[ t]=^{(k)}[t]-_{fb}^{(k)}[t]^{(k+1)}[t],\] (13)

and \(_{^{(k)}}[t]=(}_{^{(k)}}[t]+ _{k-1})^{-1}(}_{^{(k)}}[t]+_{k} )^{-1}\). Similarly, for \(k=P\), we have

\[_{^{(P)}}(^{(1)},,^{(P)})[t] =_{^{(P)}}[t]^{(P)}[t]-}}^{(P)}[t]-(^{ (P)}[t]-_{T}[t]).\] (14)

### Neural network formulation based on information maximization

In this section, we develop a biologically plausible neural network grounded on the correlative information maximization-based network propagation model outlined in Section 2.2. To achieve this, we employ projected gradient ascent optimization for determining layer activations \(^{(1)}[t],^{(2)}[t],,^{(P)}[t]\), which shape the network structure and dynamics, as well as updating the corresponding synapses that govern the learning dynamics.

#### 2.3.1 Network structure and neural dynamics

In this section, we show that the projected gradient ascent solution to the optimization in (9) defines a multilayer recurrent neural network. To this end, we introduce the intermediate variable \(^{(k)}\) as the updated layer-\(k\) activations prior to the projection onto the domain set \(^{(k)}\). Utilizing the gradient expressions in (12)-(13), we can express the network dynamics for layers \(k=1,,P-1\) as follows (see Appendix E for details):

\[_{}^{(k)}[t;s]}{ds} =-g_{lk}^{(k)}[t;s]+}^{(k)}[ t]^{(k)}[t;s]-}}^{(k)}_{u}[t;s]-}}^{(k)}_{u}[t;s],\] (15) \[}^{(k)}_{u}[t;s] =^{(k)}[t;s]-_{ff}^{(k-1)}[t]^{(k-1)}[ t;s],}^{(k)}_{u}[t;s]=^{(k)}[t;s]-_{ fb}^{(k)}[t]^{(k+1)}[t;s],\] (16) \[^{(k)}[t;s] =_{+}(^{(k)}[t;s]),\] (17)

where \(t\) is the discrete data index, \(s\) is the continuous time index corresponding to network dynamics, \(_{}\) is the update time constant, \(^{(k)}[t]=_{k}(2_{^{(k)}}[t]+g_{lk})\), and \(_{+}\) represents the elementwise clipped-ReLU function corresponding to the projection onto the nonnegative unit-hypercube \(_{,+}\), defined as \(_{+}(u)=(1,(u,0))\).

To reinterpret the dynamics in (15) to (17) as a multi-compartmental neural network, for \(k=1,,P-1\), we define the signals:

\[_{A}^{(k)}[t;s]=^{(k)}[t]^{(k)}[t;s]+_{fb}^{(k)}[t ]^{(k+1)}[t;s],_{B}^{(k)}[t;s]=_{ff}^{(k-1)}[t ]^{(k-1)}[t;s],\] (18)

which allow us to rewrite the network activation dynamics (15) to (17) as:

\[_{}^{(k)}[t;s]}{ds}=-g_{lk}^{(k)}[t;s]+g_{A,k}(_{A}^{(k)}[t;s]-^{(k)}[t;s])+g_{B,k}( _{B}^{(k)}[t;s]-^{(k)}[t;s]),\] (19) \[^{(k)}[t;s]=_{+}(^{(k)}[t;s]),\] (20)

where \(g_{A,k}=}\) and \(g_{B,k}=}\). Similarly, for the output layer, we employ the same expressions as (19) and (20) with \(k=P\), except that in this case we have:

\[_{A}^{(P)}[t;s]=^{(P)}[t]^{(k)}[t;s]-(^{(P) }[t;s]-_{T}[t]),_{B}^{(P)}[t;s]=_{ff}^{(P-1)}[t] ^{(P-1)}[t;s],\] (21)where \(g_{B,P}=}\), \(g_{A,P}=\) and \(^{(P)}[t]=^{-1}(_{^{(P)}}[t]+g_{lk})\).

Remarkably, the equations (18) to (21) reveal a biologically plausible neural network that incorporates three-compartment pyramidal neuron models, as presented in [9; 10]. This intricate architecture, of which two-layer segment is demonstrated in Figure 1, naturally emerges from the proposed correlative information maximization framework. In this network structure:

* \(^{(k)}\) embodies the membrane potentials for neuronal somatic compartments of the neurons at layer-\(k\), where \(_{}\) is the membrane leak time constant of soma.
* \(^{(k)}_{B}\) corresponds to membrane potentials for basal dendrite compartments, receiving feedforward input originating from the previous layer.
* \(^{(k)}_{A}\) denotes the membrane potentials for distal apical dendrite compartments, which gather top-down input from the subsequent layer and lateral inputs represented by \(^{(k)}[t]^{(k)}\) in (18) and (21). Decomposing \(^{(k)}\) into \(^{(k)}-^{(k)}\), we find that \(^{(k)}\) mirrors autapses, and the off-diagonal component \(^{(k)}\) corresponds to lateral inhibition synapses. We use \(^{(k)}=-^{(k)}^{(k)}\) to represent the activations of SST interneurons  that generate lateral inhibitions to the apical dendrites.
* Forward (backward) prediction errors manifest in the membrane voltage differences between soma and basal (distal) compartments of the pyramidal neurons.
* Forward (backward) prediction coefficients \(^{(k)}_{ff}\) (\(^{(k)}_{fb}\)) are associated with feedforward (top-down) synapses connecting layers \((k)\) and \((k+1)\).
* The inverse of the regularization coefficient \(_{k}\) is related to the conductance between soma and dendritic compartments. This is compliant with the interpretation of the \(^{-1}\) in Appendix A.2 as the sensitivity parameter that determines the contribution of the prediction errors to the CMI. Conversely, at the output layer, the augmentation constant \(\) corresponds to the conductance between soma and distal compartments. This relationship can be motivated by modifying the objective in (9a) as \[_{k=0}^{P-1}^{(_{k})}(^{(k)},^{(k+1)}) [t]+^{)}}(^{(P)},_{T})[t],\] (22) where, through the first-order approximation, the \(^{(P)}[t]\) dependent portion of \(^{)}}(^{(P)},_{T}) [t]\) can be expressed as \(-\|^{(P)}[t]-^{(P)}_{fb}_{T}[t]\|_{2}^{2}\). For accuracy, we enforce \(^{(P)}_{fb}=\).

### Learning dynamics

Network parameters consists of feedforward \(^{(k)}_{ff}\), feedback \(^{(k)}_{fb}\) and lateral \(^{(k)}\) coefficients.The learning dynamics of these coefficients are elaborated below:

* _Feedforward Coefficients_ are connected to the forward prediction problem defined by the optimization in (3). We can define the corresponding online optimization objective function as \(C_{ff}(^{(k)}_{ff})=_{k}\|^{(k)}_{ff}\|_{F}^{2}+\| }^{(k+1)}[t]\|_{2}^{2}\) for which the the partial derivative is given by \[(^{(k)}_{ff}[t])}{^{(k)}_{ff}}=2 _{k}^{(k)}_{ff}[t]-2}^{(k+1)}[ t]^{(k)}[t]^{T}.\] (23) In Appendix H, we provide a discussion on rewriting (23) in terms of the membrane voltage difference between the distal apical and soma compartments of the neuron, based on the equilibrium condition for the neuronal dynamics: \[-}^{(k+1)}[t]^{(k)}[t]^{T}=g_{B,k}^ {-1}(g_{A,k}^{(k)}_{A}[t]-(g_{lk}+g_{A_{k}})^{(k)}_{*}[t]+ _{*}[t])^{(k)}[t]^{T},\] (24) where \(_{*}[t]\) is nonzero only for neurons that are silent or firing at the maximum rate.
* Similarly, _Feedback Coefficients_ are connected to the backward prediction problem defined by the optimization in (5), and the corresponding online optimization objective function as \(C_{fb}(^{(k)}_{fb})=_{k}\|^{(k)}_{ff}\|_{F}^{2}+\| }^{(k)}[t]\|_{2}^{2}\) for which the partial derivative is given by \[(^{(k)}_{fb}[t])}{^{(k)}_{fb}}=2 _{k}^{(k)}_{fb}[t]-2}^{(k)}[t] ^{(k+1)}[t]^{T}.\] (25)To compute the updates of both feedforward and feedback coefficients, we use the EP approach , where the update terms are obtained based on the contrastive expressions of partial derivatives in (23) and (25) for the nudge phase, i.e., \(=^{}>0\), and the free phase, i.e., \(=0\), : \[_{ff}^{(k)}[t] }((}^{(k+1)}[t]^{(k)}[t]^{T})_{=^{}}-( }^{(k+1)}[t]^{(k)}[t]^{T})_{ =0}),\] (26) \[_{fb}^{(k)}[t] }(}^{(k)}[t]^{(k+1)}[t]^{T})_{=^{}}-( }^{(k)}[t]^{(k+1)}[t]^{T})_{ =0}\,.\] (27)
* _Lateral Coefficients_, \(^{(k)}\) are the inverses of the \(\) perturbed correlation matrices. We can use the update rule in  for their learning dynamics after the nudge phase: \[^{(k)}[t+1]=_{}^{-1}(^{(k)}[t]- ^{(k)}[t]^{(k)}[t]^{T}),^{(k)}=^{(k)}[t] ^{(k)}[t]_{=^{}}.\] (28) As we derived in Appendix F, we can rewrite the update rule of the lateral weights in terms of the updates of autapses and lateral inhibition synapses as follows: \[_{ii}^{(k)}[t+1]=_{}^{-1}_{ii }^{(k)}[t]-_{}^{-1}_{k}2^{2}(_{i}^{( k)}[t])^{2}+_{k}gl_{lk}(1-_{}^{-1}), i\{1, ,N_{k}\}\] (29) \[_{ij}^{(k)}[t+1]=_{}^{-1}^{( k)}[t]_{ij}+_{}^{-1}_{k}2^{2}_{i}^{(k)}[t] _{j}^{(k)}[t],\;\; i,j\{1,,N_{k}\},i j\] (30)

## 3 Discussion of results

* In (A.14), we devise an update for layer activation \(^{(k)}\) by employing two distinct forms of the CMI associated with \(^{(k)}\): \(^{(_{k-1})}(^{(k-1)},^{(k)})[t]\), the CMI with the preceding layer, encompassing the forward prediction error for estimating \(^{(k)}\), and \(^{(_{k})}(^{(k)},^{(k+1)})[t]\), the CMI with the subsequent layer, incorporating the backward prediction error for estimating \(^{(k)}\). Employing these alternative expressions is crucial in circumventing the weight transport problem and offering a more biologically plausible framework. For further discussion, please refer to Appendix G.
* In the context of the proposed correlative information maximization framework, forward and backward predictive coding naturally emerges as a crucial mechanism. By incorporating both alternative expressions of CMI, the framework focuses on minimizing both forward and backward prediction errors between adjacent layers via feedforward and feedback connections. These connections foster bidirectional information flow, thereby enhancing the overall learning process.
* Figure 1 depicts the interplay between the CorInfoMax objective and the corresponding network architecture. The emergence of lateral connections and autapses can be attributed to the maximization of the unconditional layer entropy component of the CMI, which allows for efficient utilization of the available representation dimensions and avoids dimensional degeneracy. Simultaneously, the minimization of conditional entropies between adjacent layers gives rise to feedforward and feedback connections, effectively reducing redundancy within representations.
* We employ time-contrastive learning, as in GenRec , EP  and CSM , by implementing separate phases with Hebbian and anti-Hebbian updates, governed by an assumed teaching signal. It has been conjectured that the teaching signal in biological networks can be modeled by the oscillations in the brain [2; 46; 47]. Although the oscillatory rhythms and their synchronization in the brain are elusive, they are believed to play an important role in adaptive processes such as learning and predicting upcoming events [48; 49].

## 4 Numerical experiments

In this section, we evaluate the performance of our CorInfoMax framework with two layer fully connected networks on image classification tasks using three popular datasets: MNIST , Fashion-MNIST , and CIFAR10 . We used layer sizes of \(784,500,10\) for both MNIST and Fashion-MNIST datasets while we used layer sizes of \(3072,1000,10\) for CIFAR10 dataset, and the final layer size \(10\) corresponds to one-hot encoded ouput vectors. Further details including full set of hyperparameters can be found in Appendix J. We compare the effectiveness of our approach against other contrastive methods, such as EP  and CSM , as well as explicit methods, including PC  and PC-Nudge , when training multilayer perceptron (MLP) architectures.

We examine two distinct constraints on the activations of CorInfoMax Networks: (i) \(_{,+}\), representing the nonnegative part of the unit hypercube, and (ii) \(_{1,+}=\{: 0,\|\|_{1} 1\}\), denoting the nonnegative part of the unit \(_{1}\)-norm ball . Table 1 presents the test accuracy results for each algorithm, averaged over 10 realizations along with the corresponding standard deviations. These findings demonstrate that CorInfoMax networks can achieve comparable or superior performance in relation to the state-of-the-art methods for the selected tasks. Additional information regarding these experiments, as well as further experiments, can be found in the Appendix. Our code is available online1.

## 5 Discussion and Conclusion

In this article, we have presented the correlative information maximization (CorInfoMax) framework as a biologically plausible approach to constructing supervised neural network models. Our proposed method addresses the long-standing weight symmetry issue by providing a principled solution, which results in asymmetric forward and backward prediction networks. The experimental analyses demonstrates that CorInfoMax networks provide better or on-par performance in image classification tasks compared to other biologically plausible networks while alleviating the weight symmetry problem. Furthermore, the CorInfoMax framework offers a normative approach for developing network models that incorporate multi-compartment pyramidal neuron models, aligning more closely with the experimental findings about the biological neural networks. The proposed framework is useful in obtaining potential insights such as the role of lateral connections in embedding space expansion and avoiding degeneracy, feedback and feedforward connections for prediction to reduce redundancy, and activation functions/interneurons to shape feature space and compress. Despite the emphasis on supervised deep neural networks in our work, it's crucial to highlight that our approach--replacing the backpropagation algorithm, which suffers from the weight transportation problem, with a normative method devoid of such issues--is potentially extendable to unsupervised and self-supervised learning contexts.

One potential limitation of our framework, shared by other supervised approaches, is the necessity for model parameter search to improve accuracy. We discuss this issue in detail in Appendix K. Another limitation stems from the intrinsic nature of our approach, which involves the determination of neural activities through recursive dynamics (see Appendix J). While this aspect is fundamental to our methodology, it does result in slower computation times compared to conventional neural networks in digital hardware implementation. However, it is worth noting that our proposed network, characterized by local learning rules, holds the potential for efficient and low-power implementations on future neuromorphic hardware chips. Furthermore, our method employs the time contrastive learning technique known as Equilibrium Propagation, which necessitates two distinct phases for learning.

    & MNIST & FashionMNIST & CIFAR10 \\ 
**CorInfoMax-\(_{,+}\)** (Appendix J.4) & \(97.62 0.1\) & \(88.14 0.3\) & \(51.86 0.3\) \\
**CorInfoMax-\(_{1,+}\)** (Appendix J.6) & \(97.71 0.1\) & \(88.09 0.1\) & \(51.19 0.4\) \\ EP & \(97.61 0.1\) & \(88.06 0.7\) & \(49.28 0.5\) \\ CSM & \(98.08 0.1\) & \(88.73 0.2\) & \(40.79^{*}\) \\ PC & \(98.17 0.2\) & \(89.31 0.4\) & - \\ PC-Nudge & \(97.71 0.1\) & \(88.49 0.3\) & \(48.58 0.7\) \\  Feedback Alignment (with MSE Loss) & \(97.99 0.03\) & \(88.72 0.5\) & \(50.75 0.4\) \\ Feedback Alignment (with CrossEntropy Loss) & \(97.95 0.08\) & \(88.38 0.9\) & \(52.37 0.4\) \\ BP (with MSE Loss) & \(97.58 0.01\) & \(88.39 0.1\) & \(52.75 0.1\) \\ BP (with CrossEntropy Loss) & \(98.27 0.03\) & \(89.41 0.2\) & \(53.96 0.3\) \\   

Table 1: Test accuracy results (mean \(\) standard deviation from \(n=10\) runs) for CorInfoMax networks are compared with other biologically-plausible algorithms. The performance of CSM on the CIFAR10 dataset is taken from , while the remaining results stem from our own simulations.

Acknowledgments and Disclosure of Funding

This research was supported by KUIS AI Center Research Award. B. Bozkurt acknowledges the support by Gatsby PhD programme, which is supported by the Gatsby Charitable Foundation (GAT3850). C. Pehlevan is supported by NSF Award DMS-2134157, NSF CAREER Award IIS-2239780, and a Sloan Research Fellowship. This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence.