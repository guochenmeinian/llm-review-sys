# Learning Bregman Divergences with Application to Robustness

Mohamed-Hicham Leghettas

Department of Computer Science

ETH Zurich, Switzerland

mleghettas@inf.ethz.ch &Markus Puschel

Department of Computer Science

ETH Zurich, Switzerland

pueschel@inf.ethz.ch

###### Abstract

We propose a novel and general method to learn Bregman divergences from raw high-dimensional data that measure similarity between images in pixel space. As a prototypical application, we learn divergences that consider real-world corruptions of images (e.g., blur) as close to the original and noisy perturbations as far, even if in \(L^{p}\)-distance the opposite holds. We also show that the learned Bregman divergence excels on datasets of human perceptual similarity judgment, suggesting its utility in a range of applications. We then define adversarial attacks by replacing the projected gradient descent (PGD) with the mirror descent associated with the learned Bregman divergence, and use them to improve the state-of-the-art in robustness through adversarial training for common image corruptions. In particular, for the contrast corruption that was found problematic in prior work we achieve an accuracy that exceeds the \(L^{p}\)- and the LPIPS-based adversarially trained neural networks by a margin of 27.16% on the CIFAR-10-C corruption data set.

## 1 Introduction

The need to measure the semantic distance between images is a recurring requirement in various computer vision tasks, including image retrieval [55; 53; 2], near-duplicate detection , face recognition , and zero-shot learning . This has led to a significant body of research in the field of metric learning [75; 7], which focuses on developing automated methods for learning such distances. The most successful approaches to assessing similarity between images involve encoding them into a compact latent space and computing the \(L^{p}\)-distance between the resulting latent features. Image encoders are typically residual neural networks or vision transformers that are pre-trained in a supervised , weakly-supervised , or self-supervised  fashion. The latent space is usually assumed Euclidean and hence the \(L^{2}\)-norm is the common choice, although some non-Euclidean geometries have been considered .

Image similarity measures are also crucial in the field of robust machine learning. Since models are known to be sensitive to small input perturbations [9; 68; 56], a robustness study requires a measure for the difference between clean and perturbed inputs. A common choice is the \(L^{p}\)-norm computed in the pixel space. It lacks semantic meaning but adversarial training (AT) for robustness using these norms (via adversarial training  and its many follow-up variants, e.g., [69; 80; 12; 72; 14; 61; 37]) has been found to also improve the robustness to distribution shifts associated with common, realistic image corruptions like blur or contrast changes [20; 33]. Conversely, corruption robustness evaluation is shown more reliable than adversarial robustness evaluations when distinguishing successful adversarial defense methods from ones that merely cause vanishing gradients .

Both metric learning and corruption robustness approaches obtain similarity measures by calculating standard norms in latent spaces. In this work, we take a different route by learning Bregman divergences directly in the pixel space. This way, we benefit from a strong mathematical underpinningincluding the associated _mirror descent_, an optimization framework to natively solve constrained problems that we then put to use for AT.

**Bregman divergence and mirror descent.** The Bregman divergence  (referred to as BD in the remaining paper) is a generalization of the Kullback-Leibler (KL) divergence , and is widely used in statistics and information theory to define distances in spaces where the Euclidean geometry is not appropriate such as probability distributions, covariance descriptors, random processes and others [16; 18; 6; 67; 27; 29]. It is defined via an underlying base function (e.g., the Shannon entropy for the KL divergence) that has to be strongly convex and with invertible gradient. Nemirovski and Yudin introduced the mirror descent framework  as a method for minimizing a function by utilizing a Bregman divergence to incorporate the geometric structure of the underlying space.

**Contributions.** In this paper we offer progress in the quest for similarity measures through a theoretically principled approach to learn BDs for images in pixel space and exploit the associated mirror descent for achieving robustness through AT. Our main contributions are as follows:

* We provide a novel self-supervised algorithm to derive BDs for images in pixel space. The key idea is to learn eligible base functions using a suitable network architecture. These divergences are semantic in the sense that they assess similar images as close and randomly perturbed ones as far from the clean image, even if in Euclidean distance the converse holds.
* We then learn first BDs that are corruption-specific, where similar images are derived by applying image corruptions from CIFAR-10-C dataset . Then we learn BDs that are corruption-oblivious where similar images are obtained from Berkeley-Adobe Perceptual Patch Similarity (BAPPS) dataset .
* We show that the learned BDs are consistent and successfully distinguish between corrupted and noisy images. We also show that a BD learned to mimic human judgment on the BAPPS dataset performs well on the two alternative forced choice (2AFC) test.
* We then propose a mirror-descent-inspired algorithm to perform semantic adversarial attacks using the learned BDs instead of the \(L^{p}\)-norm and adopt this attack for AT. Doing so we improve the state-of-the-art in AT-based corruption robustness on CIFAR-10-C. In particular for the contrast and fog corruptions that are known to be problematic (e.g.,  and ), the improvements are a substantial 27% and 13% increase in accuracy.

## 2 Background

We first recall standard adversarial training (AT) with projected gradient descent (PGD). Then we provide background on the BD  and the associated mirror descent framework, which generalizes PGD .

**Adversarial training.** Let \(l(,y;)\) be a loss of a classifier parameterized by \(\) where the input image \(^{n}\) and the label \(y\) are sampled from the data distribution \(\). As formalized by , training an adversarially robust model amounts to solving the following min-max optimization problem:

\[_{}_{(,y)}[_{^{ }()}l(^{},y;)]\] (1)

where \(()\) is the set of images that are considered similar to \(\). Under the common \(L^{p}\) threat model, \(()\) is defined as an \(L^{p}\) ball centered on \(\) of chosen radius \(\): \(()=(,)\). 1 In this case, the inner maximization problem is solved by PGD, which consists of iterating over two steps: a gradient-based update followed by a projection into \((,)\).

**Bregman divergence (BD).** For a strongly convex \(h:\) (called base function) on a given space \(\) (called the primal space) with thus strictly monotonous gradient \( h:\) (\(\) is called the dual space), the associated BD \(D_{h}:[0,)\) from \(\) to \(^{}\) is defined as

\[D_{h}(^{})=h(^{})-h()-  h(),^{}-.\] (2)

The BD is similar to a metric or distance (non-negative, zero iff \(=^{}\)), except that in general it is not symmetric in its arguments and only satisfies a weaker version of the triangle inequality (whoseexact form is not relevant here). \(D_{h}\) is convex in its first argument but not necessarily in the second . The projection of an \(\) on a closed and convex set \(\) w.r.t. to \(D_{h}\) exists and is unique:

\[_{}()=*{arg\,min}_{^{}}D_{h}(^{}).\] (3)

The generic concepts are shown in the first column in Tab. 1; the other columns are examples. The squared Euclidean distance is a BD for \(h\) chosen as the squared \(L^{2}\)-norm. More interestingly, if \(h\) is the negative Shannon entropy, the associated BD is the Kullback-Leibler (KL) divergence. Various other divergences have been defined [6; 67; 29].

The _Bregman ball_ centered on \(\) with radius \(\) is then given by

\[_{h}(,)=^{} D_{ h}(^{})}.\] (4)

The ball \(_{h}\) is bounded, compact if \(\) is closed, and convex .

**Mirror descent.** Mirror descent  is a framework for optimizing functions \(f:\) possibly constrained to a feasible convex set \(\): \(_{}f()\), given a suitable base function \(h\) that defines a BD. Mirror descent requires the gradient \( h\) (called the _mirror map_) and the existence of \(( h)^{-1}\) (called the _the inverse map_). The algorithm is iterative as shown in the first column in Tab. 1. After initializing \(^{0}\) at any point in \(\), each iteration \(t\) consists of four steps: _(i)_ mapping the current point \(^{t}\) to a point in the dual space \(^{t}= h(^{t})\) through the mirror map, _(ii)_ taking a gradient step of size \(\): \(^{t+1}=^{t}- f(^{t})\), _(iii)_ mapping \(^{t+1}\) back to the primal space using the inverse map: \(^{*}=( h)^{-1}^{t+1}\), _(iv)_ projecting \(^{*}\) into the feasible set \(\) w.r.t. \(D_{h}\): \(^{t+1}=_{}(^{*})\) with (3).

As shown in Tab. 1, for the Euclidean divergence, mirror descent is exactly PGD. For the KL divergence it becomes the so-called hedge algorithm . In this paper, as sketched in the fourth column, we will learn base functions \(h\) that we call \(\) and associated divergences \(D_{}\) for common image corruptions and use them for AT.

## 3 Learning a BD

As first main contribution we exploit the theory of BD to derive new similarity measures for images. Namely, we learn a base function \(h=\) that satisfies the properties to make \(D_{}\) a divergence. Mathematically, this \(\) will play the same role as the Shannon entropy for KL divergence. Formally, the challenge is to learn a \(\) with the following properties:

 Generic & Euclidean norm & KL divergence & Ours \\  Some space \(\) & Euclidean space & Discrete distributions & Images \\  Base function \(h:\) _(strongly convex)_ & \(h()=||||_{2}^{2}\) & \(h()=_{i}_{i}(_{i})\) _(Shannon entropy)_ & \(h=\ \) _(an input convex NN)_ \\  Mirror map \( h:\) _(strictly monotone)_ & \( h()=\) & \( h()_{i}=(_{i})\) & \( h\) \\  Inverse map \(( h)^{-1}:\) & \(( h)^{-1}()=\) & \(( h)^{-1}()_{i}=e^{_{i}}\) & Fenchel conjugate \(\) \\ 
**Bregman Divergence** & \(D_{h}(^{})\) & \(||^{}-||_{2}^{2}\) & \(_{i}_{i}_{i}}{_{i}}\) & \(D_{}\) (learned divergence) \\ 
**Mirror descent** & PGD & Hedge algorithm & Ours \\ \(^{t}= h(^{t})\) & & & \(^{t}=(^{t})\) \\ \(^{t+1}=^{t}- f(^{t})\) & \(^{*}=^{t}- f(^{t})\) & \(^{*}_{i}=^{t}_{i}e^{- t_{i}}\) & \(^{t+1}=^{t}+ l(^{t})\) \\ \(^{*}=( h)^{-1}(^{t+1})\) & \(^{t+1}=_{}(^{*})\) & \(^{t+1}=_{}(^{*})\) & \(^{*}=(^{t+1})\) \\ \(^{t+1}=_{}(^{*})\) & & & \(^{t+1}=_{}(^{*})\) \\   

Table 1: Notation and context of our approach. First column: generic concepts associated with the BD and mirror descent. Second and third column: known instantiations. Last column: our learned BDs with a novel approach to robustness as application.

1. \(\) is strongly convex and differentiable, and thus \(D_{}\) a divergence;
2. \(()\) and \(()^{-1}()\) are (approximately) computable to execute mirror descent.

### Strongly convex architecture

We propose to model \(\) as a deep neural network with a particular architecture: the _input convex neural network (ICNN)_ for which we propose a self-supervised learning algorithm. The architecture is an \(L\)-layered deep neural network with activations \(^{l}\) defined as:

\[^{1}=q^{0}[^{0}]\\ ^{1}=g^{0}[^{1}^{1}+^{0}+^{0}] \\ ^{l}=q^{l-1}[^{l-1}]\\ ^{l}=g^{l-1}[^{l}^{l}+^{l-1}^{l-1}+^{ l-1}]\ 2 l L.\] (5)

And finally the output is defined as \(()=^{L}+||||_{2}^{2}\) with \(>0\). The weights \(^{l}\), \(^{l}\), and \(^{l}\) with the biases \(^{l}\) are learnable parameters while \(q^{l}\) and \(g^{l}\) are non-linear activation functions.

The function \(\) is convex provided that all \(^{1},..,^{L-1}\) and \(^{1},..,^{L-1}\) are non-negative and all the activation functions \(q^{l}\) and \(g^{l}\) are convex and non-decreasing [1, Proposition 1]. Furthermore, adding the term \(||||_{2}^{2}\) to the final layer ensures that \(\) is \(\)-strongly convex.

We can choose the activations \(q^{l}\) to be the Hadamard square and the weights \(^{1},..,^{L-1}\) to be the identity matrix. As we intend to compute the derivative of this network with respect to the input (to obtain \(\)), the derivative of the Hadamard square will be linear feedthroughs. This activation function has proven to be the effective in practical settings. Further, we set all the activation functions \(g^{l}\) to be the continuously differentiable exponential linear unit (CELU)  and the linear layers as convolutions. Once we have \(\), we numerically approximate the evaluation of the mirror map \(()()\) using automatic differentiation  to obtain the associated divergence as

\[D_{}(^{})=(^{})-()- (),^{}-.\] (6)

### Training divergences for corruptions

A real-world corruption of an image \(()\) (like blurred or with changed contrast) typically lies at a large \(L^{2}\) distance \(\) (say 10) of the clean image \(\) and thus an \(L^{2}\)-based attack with this \(\) would not find it but instead an extremely noisy one \(}\) at similar distance which would likely not be recognizable by a human. As an additional problem, the \(L^{p}\)-based AT also does not converge for large \(\) and typically very small \(\) around \(0.1\) are used .

Our second main contribution is to train \(\) such that the induced \(D_{}\) considers a corrupted image \(()\) close to the clean \(\) while considering noisy images \(\{}^{i}\}_{i=1}^{m}\) far away even when the Euclidean

Figure 1: Learning a BD in two dimensions. (a) The original point is \(=(0,0)\), the noisy perturbations are in blue, the corrupted points \(()\) (in red) have angles between \(\) and \(\). (b) Heat map of the \(L^{2}\)-distance to the origin, which is unable to distinguish corrupted from noisy points. (c) Heat map of our learned BD trained on the samples in (a), which considers corrupted points very close compared to noisy points.

distance suggests the opposite. This means each of the divergences \(D_{}(}^{i})\), \(i=1,...,m\), should be larger than \(D_{}(())\) or equivalently \(-D_{}(())>-D_{}(}^{i} )\). We propose the following _Bregman loss_\(l_{B}(;,)\) to jointly enforce these \(m\) inequalities:

\[l_{B}(;,)=-(())}} {e^{-D_{}(())}+_{i}e^{-D_{}(} ^{i})}}.\]

The loss \(l_{B}(;,)\) can be interpreted as a cross entropy where the logits vector is the negative of the BDs \([-D_{}(()),-D_{}(}^{1} ),...,-D_{}(}^{m})]\) and the ground truth class always corresponds the first entry. Then, we learn \(\) by minimizing:

\[_{,}_{}[l_{B}(;, )].\] (7)

After successful training the Bregman ball \(_{}(,D_{}(()))\) contains the transformed image \(()\) by definition but does not contain any of the noisy images \(\{}^{i}\}_{i=1}^{m}\). We execute this approach on an example in two dimensions as illustrated in Fig. 1.

**Sampling noisy images.** To train for (7) we need a way to sample random images \(\{}^{i}\}_{i=1}^{m}\) at a distance proportional to that of the corrupted image \(||()-||_{2}\). This distance is controlled by the proportion coefficient \(d(0,1]\). In other words, we sample \(\{}^{i}\}_{i=1}^{m}\) from some distribution \(}\) such that:

\[_{i}||}^{i}-||_{2}=d\ ||()-||_{ 2}.\]

We chose this distribution to be the isotropic Gaussian:

\[}=+(1/)d\ ||()-||_{2},(0,_{n}).\] (8)

This way the expectation \([||}-||_{2}]\) is asymptotically equivalent to \(d||()-||_{2}\) (proof in Appendix A).

## 4 Mirror descent adversarial training

As the third main contribution, we use our learned BDs \(D_{}\) to achieve corruption robustness through AT. First, as part of the threat model we define the neighborhood of a clean image \(\) as a Bregman ball:

\[()=_{}(,).\] (9)

Then, we perform the attack by instantiating mirror descent (Tab. 1) to solve the inner maximization problem in (1). As explained in Sec. 2, doing so requires the inverse map \(()^{-1}\) and a projection w.r.t. \(D_{}\) that we discuss next.

**Inverse map.** Since \(\) is a gradient of a neural network, its inverse \(^{-1}\) is not readily available. To obtain an approximation, we leverage the Fenchel conjugate \(:\) of \(\), which exists for convex \(\), is again convex, and defined as:

\[()=_{}\ ,-().\] (10)

If \(\) is of so-called _Legendre type_ (i.e., proper closed, essentially smooth and essentially strictly convex ), then  states that \(()^{-1}=\). In general, checking that a function is Legendre type is difficult , in particular in this case where the function is a neural network. So instead of deriving a closed-form solution using this result, we use it to motivate an approximation: first defining the conjugate \(\) again as an ICNN with the exact same architecture as \(\) in (5); then training by minimizing: 2

\[_{,}_{}[ ||(())-||_{2}].\] (11)

Now \(()()\) is again computed using automatic differentiation and approximates \(()^{-1}()\) as desired.

**Projection.** The projection w.r.t. a BD into a general convex set is difficult to compute . Numerical solutions only exist for special sets such as hyperplanes or affine spaces that are not applicable to our set of interest \(()\). So to approximate the projection of \(^{*}\) into \(()\) (see last row last column of Tab. 1), we perform a binary search over the segment having \(\) and \(^{*}\) as endpoint until we find a point \(^{t+1}()\). This heuristic is not guaranteed to produce optimal results, as there may exist points \(^{}()\), closer to \(^{*}\) than \(^{t+1}\), that are out of the considered line segment. However, as we will show, it is fast enough to be incorporated in training and it yields good results (see Sec. 6).

## 5 Related work

**Corruption robustness via data augmentation.** Much of the prior literature on corruption robustness aims to improve out-of-distribution generalization by using simulated and augmented images for training. Many such data augmentation techniques are based on creating synthetic training examples through mixing pairs of training images and their labels. This is achieved for example by linear weighted blending of images  or by cutting and pasting parts of an image onto another . Researchers also fused images based on masks computed through frequency spectrum analysis , based on adaptive masks  or based on model-generated features . Other works considered a hybrid version of these mixing methods , a stochastic version of them , an ensemble of them  or a concurrent combination of them .

**Adversarial attacks without \(}\)-norms.** Another line of work focuses on adversarial image perturbations not constrained by \(L^{p}\)-norms.  introduces semantic adversarial attacks that target image transformation parameters instead of image pixels. Similarly,  targets spatial transformations.  manipulates the hue and saturation components in the hue saturation value (HSV) color space to create adversarial examples. In addition to colorization,  also tweaked the texture of objects within images.  modified colors within the invisible range. Some works altered the semantic features of images through conditional generative models  or conditional image editing .

**Robustness via learned similarity metric.** The closest related work adopts the so-called learned perceptual image patch similarity (LPIPS) to study robustness. LPIPS is a weighted sum of the \(L^{2}\) of the feature maps taken from the activation layers of a trained convolutional network:

\[(,^{})=_{l}w_{l}||_{l}()-_ {l}(^{})||_{2},\] (12)

where \(_{l}\) is the feature map up to the \(l\)-layer and \(w_{l}\) weighs the contribution of the layer \(l\).  and  propose an attack similar to  by adding the LPIPS along with the \(L^{p}\)-norm. Differently,  and  used LPIPS as a function to define the set of similar images (refer to Sec. 2 for notation context): \(()=\{^{}^{n}|\ (,^{ })\}\). Since the projection into this LPIPS-based set does not admit a closed-from expression, solving the inner maximization problem of (1) (i.e., performing the adversarial attack) requires approximation  or relaxation . The resulting attacks and their associated AT have been proven effective to train robust models against common image corruptions. We compare against LPIPS in our experiments.

**Learning BDs.** BDs have been widely used in machine learning but are typically hand-engineered and not learned .  learn BDs relying on piecewise linear functions and linear lower bound approximation to ensure the convexity of the learned base functions. These methods are limited to low-dimensional inputs, either tabular data or extracted features.  uses Gorbbi solvers  for lower bounds approximation as part of clustering/ranking algorithms. Similarly,  use functional BD and apply it to clustering while  learn the architecture proposed by  through a contrast learning algorithm. Recently,  proposed to learn a BD for clustering where its input are features extracted from a CNN. In contrast, we are the first to learn an end-to-end BD on images from raw data where the inputs to the divergence are pixels in a way that yields a convex base functions by construction without bound approximation. This allows us to instantiate the Bregman ball (to define robustness) and to run the mirror descent framework (to train for robustness), which is not possible with prior methods.

## 6 Corruption-specific Bregman divergences

We first show that we can successfully learn a BD that assesses corrupted images (for a given type of corruption) as close and randomly perturbed ones as far from the clean image, even if in Euclideandistance the converse holds (see Sec. 3). We perform experiments on CIFAR-10  and consider the 14 noise-free corruptions from CIFAR-10-C  that can be applied with severities from 1 to 5. One focus are the corruptions of contrast and fog, which have been found the most challenging in AT [25; 41]. We first analyze the learned BDs and then show robustness results when used with AT.

### Learning the BD

Learning a BD amounts to learning the base function. For both the base function \(\) and its conjugate \(\) we use the same architecture: an ICNN with 12 convolutional layers followed by 4 fully connected layers. The strong-convexity parameter is chosen as \(=10^{-3}\). The mirror map and the inverse map are numerically approximated using autograd.grad from PyTorch's automatic differentiation engine . As an initialization phase, we first train \(\) and \(\) such that \(\) and \(\) approximate the identity function (so initially \(=^{-1}\) holds) on uniformly drawn samples from the usual range of pixels \(^{n}\):

\[_{,}_{(^{n})}[\| ()-\|_{2}].\] (13)

This identity training is performed for 7,000 steps using the Adam optimizer  with a batch size of 64, a learning rate of \(3 10^{-4}\) and no weight decay. For a given corruption \(\), we then train \(\) with (7) while randomly sampling its severity (1 to 5) for each image at each epoch. The hyperparameter \(d\) for sampling noisy images in (8) is uniformly sampled from \([10^{-7},0.99]\). The training batch contains 32 clean images, one corrupted image for each clean image, and \(m=63\) samples of noisy images per clean image (2,080 images in total). The training is performed for 10 epochs using the AdamW optimizer  with an initial learning rate of \(10^{-4}\) and a weight decay of \(10^{-9}\). After each update of \(\) according to (7), we also update \(\) according to (11). Finally, we freeze the parameters of \(\) and continue training \(\) for an additional 10 epochs. The training converged for 10 out of the 14 considered corruptions.

It is conceivable to train a BD to be symmetric, however, it is not a good idea since a perfectly symmetric BD is just a quadratic function. However, we found that our learned divergence is qualitatively symmetric in the sense that it performs equally well with flipped arguments (see App. D for details).

**Performance on corruption vs. noise.** We first show in Fig. 3 that the learned divergence \(D_{}\) on images (dimension \(n=3072\)) agrees with the 2D example in Fig. 1. To do so we consider, for the test set of 10,000 clean images \(\), contrast-corrupted images \(()\) with severity \(s=5\) (red, one per clean image) and a set of noisy images \(}\) (blue, one per clean image). The noisy images are sampled from (8) with \(d=1.0\). With this choice, the distribution of the \(L^{2}\)-distances to the clean image is equal for the noisy and the corrupted images (Fig. 3(a).) Fig. 3(b) shows the distribution of learned divergences to the clean image. Here, all corrupted images are very close (mean 3.8, std 6.0) but the noisy ones far (mean 8385, std 4939), which shows that the learned BD works as intended. Visual results on images from ImageNet are provided in App. D.

Next, we generate multiple corrupted images with different severities from \(s=1\) to \(s=5\) and report their divergences from the clean images in Fig. 3. The divergence considers more severely corrupted images further from the clean images as expected. All these results are qualitatively the same for all 10 corruptions with learned BDs.

**Comparison against other similarity measures.** We evaluate how well different similarity measures distinguish between noisy and corrupted images. For each clean image and the corresponding corrupted version \(()\) with severity 5, we sample 9 noisy images \(}\). We repeat the sampling for different noise coefficients \(d\) as shown in Fig. 4. We consider 5 similarity measures \(\) to distinguish between noisy and corrupted images: \(=L^{2}\) over the pixels, our trained BD \(=D_{}\), and the three state-of-the-art metric learning methods Dino , Unicom , and Moco (v2) .

First we measure the similarities: \(((),),(}^{1},),,( }^{9},)\). An accurate similarity measure yields \(((),)\) smaller than the rest. We test this accuracy over the test set for multiple values of noise coefficients \(d\) in Fig. 3(a). Our learned divergence performs best by far, and considers noisy images further compared to corrupted images for all \(d\), whereas other state-of-the-art metric learning measures only do so for high noise \(d\).

Next, we inspect the ratio \(r=(},)/((),)\) and report the aggregated results over the test set in Fig. 3(b). For \(=L^{2}\) this ratio is \(d\) by construction. We observe that all learned \(\) offer better ratios (distinguish corrupted from noisy images better) than \(L^{2}\) and that this distinction improves with \(d\) as expected. Consistent with the accuracy results, our trained BD outperforms the other measures by yielding ratios \(r>1\) for all noise levels \(d\),

### AT with mirror descent

As an application of our learned BD, we perform AT by instantiating the associated mirror descent (see Sec. 4) and compare against the relaxed LPIPS AT (RLAT) . We show that the proposed method improves the state of the art in adversarial training-based robustness on several common image corruptions. For the classification model \(f\), we use the PreAct ResNet-18 architecture , which was also used by . For a fair comparison, we set the number of iterations for our attack to \(T=1\) to match the one-step attack used in RLAT. We also compare against the \(L^{2}\) PGD AT. AT details are reported in App E. The mirror descent is executed following Alg. 2. Samples from the generated adversarial images are provided in App. E.

**AT for contrast corruption.** Both PGD and RLAT fail to improve robustness against contrast as found by  and  and replicated in Tab. 2. Our mirror descent AT using the learned BD for contrast improve this robustness considerably across all severities (on average from 63.92% to 87.4%). Surprisingly, mirror descent AT for the zoom blur corruption yields further improvement to 90.03% on average. We discuss the reason in the next expanded experiment.

**Comparing AT for different corruptions.** We expand the previous experiment by mirror descent AT for fog, and brightness corruptions and report the average accuracy across severities in Tab. 6 for different corruptions. In all considered cases, our mirror descent AT maintains high accuracy on clean images. We notice that AT with the zoom blur divergence performs best for contrast, brightness, and very well for fog, but, surprisingly, not for zoom blur, for which LPIPS AT is best.

[MISSING_PAGE_FAIL:9]

CIFAR-10-C. We then re-execute adversarial training on CIFAR-10 with this divergence. The results on CIFAR-10-C in Tab. 5 show again that our method outperforms RLAT and PGD especially for the fog and the contrast corruptions where PGD and RLAT are known to fail  and . Zoom blur improves over all prior results in Tab. 6. Again our AT does not improve the other categories.

## 8 Discussion

**Limitations.** When used with AT, our method introduces an overhead in first training for a valid divergence and then in performing the mirror descent with the heuristic projection (see App. C for a detailed cost analysis). Our method does provide adversarial examples within the trained Bregman ball using the suggested line search projection, however a better heuristic for projection is one important avenue for improvements.

The training of BDs did not succeed for some CIFAR-10-C corruptions nor for all its corruptions simultaneously, but worked on BAPPS. Further, AT with our mirror descent-inspired AT is unable to improve robustness on prior work for several corruptions. We attribute these issues to the small scale of the used convex architecture \(\). Scaling up and training on larger data sets with larger image sizes should be easily straightforward with more GPUs, instead of the one V100 GPU we had access to. However, despite the relatively small scale, the results in Sec. 7 demonstrate that our method can successfully learn a corruption-oblivious BD that exhibits robust generalization across various types of corruptions, when given a suitable training set.

**Broader impact.** One of the contributions of this paper is to increase the corruption robustness of machine vision models specifically by using the AT machinery. Corruption robustness enhances the reliability and safety of models deployed in various applications such as autonomous driving.

## 9 Conclusion

We see our main contribution in showing how to learn a BD from raw high-dimensional data with an approach that should generalize to settings other than the image corruptions considered here. The benefit is in importing the associated theoretical underpinning of the BD such as the compactness of Bregman balls and the well-established mirror descent. The latter motivated us to consider AT for corruption robustness as prototypical application. We considered the two very different data sets CIFAR-10-C and BAPPS to obtain both corruption-specific and corruption-oblivious BDs, and demonstrated that they are consistent in various ways: the former approximately symmetric and monotonous in the corruption severity, the latter outperforming LPIPS in mimicking human judgment. The associated ATs were particularly successful on contrast and fog that troubled prior work.

Our contribution is only a first step and opens various avenues for further improvements including the use of more complex architectures for learning the base functions and thus the BDs, better heuristics for the mirror descent projections, and applications and data sets beyond images.

    & Traditional & CNN-based & Super Resolution & Video deblur & Colorization & Frame Interpolation \\  LPIPS & 51.41 & 72.10 & 60.46 & 54.25 & 55.18 & **55.55** \\ Bregman (ours) & **63.65** & **79.57** & **61.04** & **56.95** & **61.63** & 53.73 \\   

Table 4: Accuracy of the trained Bregman divergence compared to LPIPS evaluated on different categories of the 2AFC task from the BAPPS dataset.

    & Clean & Contrast & Fog & Zoom blur \\  PGD & **93.65** & 63.19 & 77.18 & 86.08 \\ RLAT & 93.28 & 62.87 & 77.01 & 85.89 \\ Bregman (ours) & 93.61 & **77.70** & **88.00** & **87.12** \\   

Table 5: Corruption robustness of the learned corruption-oblivious Bregman divergence compared to PGD and RLAT.