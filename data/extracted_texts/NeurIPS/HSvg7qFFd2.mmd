# Unsupervised Representation Learning of Brain Activity via Bridging Voxel Activity and Functional Connectivity

Unsupervised Representation Learning of Brain Activity via Bridging Voxel Activity and Functional Connectivity

 Ali Behrouz

Cornell University

ab2947@cornell.edu

&Parsa Delavari

University of British Columbia

parsadlr@student.ubc.ca

&Farnoosh Hashemi

Cornell University

sh2574@cornell.edu

Equal Contribution.

###### Abstract

Effective brain representation learning is a key step toward the understanding of cognitive processes and unlocking detecting and potential therapeutic interventions for neurological diseases/disorders. Existing studies have focused on either (1) voxel-level activity, where only a single weight relating the voxel activity to the task (i.e., aggregation of voxel activity over a time window) is considered, missing their temporal dynamics, or (2) functional connectivity of the brain in the level of region of interests, missing voxel-level activities. In this paper, we bridge this gap and design BrainMixer, an unsupervised learning framework that effectively utilizes both functional connectivity and associated time series of voxels to learn voxel-level representation in an unsupervised manner. BrainMixer employs two simple yet effective MLP-based encoders to simultaneously learn the dynamics of voxel-level signals and their functional correlations. To encode voxel activity, BrainMixerfuses information across both time and voxel dimensions via a dynamic self-attention mechanism. To learn the structure of the functional connectivity graph, BrainMixer presents a temporal graph patching and encodes each patch by combining its nodes' features via a new adaptive temporal pooling. Our experiments show that BrainMixer attains outstanding performance and outperforms 14 baselines in different downstream tasks and experimental setups.

## 1 Introduction

The recent advancement of neuroimaging provide rich information to analyze the human brain. The provided data, however, is high-dimensional and complex , which makes it hard to take advantage of powerful machine learning models in analyzing them. To overcome this challenge, representation learning serves as the backbone of machine learning methods on neuroimage data and provides a low-dimensional representation of brain components at different levels of granularity, enabling the understanding of behaviors , brain functions  and/or detecting neurological diseases .

In the brain imaging literature, studies have mainly focused on two spatial scales--voxel-level and network-level--as well as two analysis approaches--multivariate pattern analysis (MVPA) and functional connectivity . The MVPA approach is often employed at the voxel-level scale and in task-based studies to associate neural activities at a very fine-grained and local level with particular cognitive functions, behaviors, or stimuli. This method has found applications in various areas, including the detection of neurological conditions , neurofeedback interventions , decoding neural responses to visual stimuli , deciphering memory contents , and classifying cognitive states . The functional connectivity analysis, on the other hand, focuses on the temporal correlations or statistical dependencies between the activity of different brain regions at larger scales to assess how these areas communicate and collaborate. This method has been utilized to study various topics such as task-related network dynamics [32; 43] and the effects of neurological disorders on brain connectivity [33; 29].

**Limitation of Previous Methods.** Despite the advances in the representation learning of brain signals, existing studies suffer from a subset of five limitations: 1 Study the human brain at a single scale: Most existing studies study the brain at either voxel-level or functional connectivity, while these two scales can provide complementary information to each other; e.g., although voxel-level activity provides detailed and more accurate information about brain activity, it misses the information about how different areas communicate with each other at a high level. Recently, this limitation has motivated researchers to search for new methods of integrating these two levels of analyses [66; 62]. 2 Supervised setting: Learning brain activity in a supervised setting relies on a large number of clinical labels while obtaining accurate and reliable clinical labels is challenging due to its high cost .3 Missing information by averaging: Most existing studies on voxel activities aggregate measured voxel activity (e.g., its blood-oxygen level dependence) over each time window to obtain a single beta weight [73; 88; 72]. However, this approach misses the voxel activity dynamic over each task. Moreover, most studies on brain functional connectivity also aggregate closed voxels to obtain brain activity in the Region of Interest (ROI) level, missing individual voxel activities. 4 Missing the dynamics of the interactions: Some existing studies neglect the fact that the functional connectivity of the human brain dynamically changes over time, even in resting-state neuroimaging data . In task-dependent neuroimage data, subjects are asked to perform different tasks in different time windows, and the dynamics of the brain activity play an important role in understanding neurological disease/disorder . 5 Designed for a particular task or neuroimaging modality: Due to the different and complex clinical patterns of brain signals , some existing methods are designed for a particular type of brain signal data [52; 13], and there is a lack of a unified framework.

**Application to Understanding Object Representation in the Brain.** Understanding object representation in the brain is a key step toward revealing the basic building blocks of human visual processing . Due to the hierarchical nature of human visual processing, it requires analyzing brain activity at different scales, i.e., both functional connectivity and voxel activity. However, there is a small number of studies in this area, possibly due to the lack of proper large-scale datasets. Recently, Hebart et al.  provided a large-scale fMRI and MEG datasets, THINGS, to fill this gap. However, the preprocessed data by Hebart et al.  not only does not provide functional connectivity, but it also has aggregated voxel activity over each time window, and provides a single beta weight for each voxel, missing dynamics of voxel activity. To address this limitation, we present two newly preprocessed versions of this dataset that provide both functional connectivity and voxel activity timeseries of fMRI and MEG modalities. See Appendix B for more details.

**Contributions.** To overcome the above limitations, we leverage both voxel-level activity and functional connectivity of the brain. We present BrainMixer, an unsupervised MLP-based brain representation learning approach that jointly learns representations of the voxel activity and functional connectivity. BrainMixer uses a novel multivariate timeseries encoder that binds information across both time and voxel dimensions. It uses a simple MLP with functional patching to fuse information across different timestamps and learns dynamic self-attention weights to fuse information across voxels based on their functionality. On the other hand, BrainMixer uses a novel temporal graph

Figure 1: **Schematic of the BrainMixer. BrainMixer consists of two main modules: (1) Voxel Activity Encoder (top), and (2) Functional Connectivity Encoder (bottom).**

learning method to encode the brain functional connectivity. The graph encoder first extracts temporal patches using temporal random walks and then fuses information within each patch using the designed dynamic self-attention mechanism. We further propose an adaptive permutation invariant pooling to obtain patch encodings. Since voxel activity and functional connectivity encodings are different views of the same context, we propose an unsupervised pre-training approach to jointly learn voxel activity and functional connectivity by maximizing their mutual information. In the experimental evaluations, we provide two new large-scale graph and timeseries datasets based on THINGS . Extensive experiments on six datasets show the superior performance of BrainMixer and the significance of each of its components in a variety of downstream tasks.

For the sake of consistency, we explain BrainMixer for fMRI modality; however, as it is shown in SS4, it can simply be used for any other neuroimaging modalities that provide a timeseries for each part of the brain (e.g., MEG and EEG). When dealing with MEG or EEG, we can replace the term "voxel" with "channel". Supplementary materials can be found in this link.

## 2 Related Work

Timeseries Learning.Attention mechanisms are powerful models to capture long-range dependencies and so recently, Transformer-based models have attracted much attention in time series forecasting [103; 55]. Due to the quadratic time complexity of attention mechanisms, several studies aim to reduce the time and memory usage of these methods . Another type of work uses (hyper)graph learning frameworks to learn (higher-order) patterns in timeseries [67; 75]. Inspired by the recent success of MLP-Mixer, Li et al.  and Chen et al.  presented two variants of MLP-Mixer for timeseries forecasting. All these methods are different from BrainMixer, as 1 they use static attention mechanisms, 2 do not take advantage of the functionality of voxels in patching, and 3 are designed for timeseries forecasting and cannot simply be extended to various downstream tasks on the brain.

MLP-based Graphs Learning.Learning on graphs has been an active research area in recent years [44; 90; 16]. While most studies use message-passing frameworks to learn the local and global structure of the graph, recently, due to the success of MLP-based methods , MLP-based graph learning methods have attracted much attention [42; 10]. For example, Cong et al.  and He et al.  presented two extensions of MLP-Mixer to graph-structured data. However, all these methods are different from BrainMixer and specifically FC Encoder, as either 1 use time-consuming graph clustering algorithms for patching, 2 are static methods and cannot capture temporal properties, or 3 are attention-free and cannot capture the importance of nodes.

Graph Learning and Timeseries for Neuroscience.In recent years, several studies have analyzed functional connectivity to differentiate human brains with a neurological disease/disorder [45; 18; 93]. With the success of graph neural networks in graph data analysis, deep learning models have been developed to predict brain diseases by studying brain network structures [7; 108; 26]. Moreover, several studies focus on brain signals [24; 79] to detect neurological diseases. For example, Cai et al.  designed a self-supervised learning framework to detect seizures from EEG and SEEG data. However, these methods are different from BrainMixer as they are designed for a particular task (e.g., classification), a particular neuroimaging modality (e.g., fMRI or EEG), and/or supervised settings.

## 3 Method: BrainMixer

Detailed discussion about background concepts can be found in Appendix A.

Notation.We represent the neuroimaging of a human brain as \(=\{^{(t)}\}_{t=1}^{T}\) where \(^{(t)}=(,_{F}^{(t)},^{(t)},)\) represents the neural data in time window \(1 t T\). Here, \(\) is the set of voxels, \(_{F}^{(t)}=(,^{(t)},^{(t)})\) is the functional connectivity graph, \(^{(t)}\) is the set connections between voxels, \(^{(t)}\) is the correlation matrix (weighted adjacency matrix of \(_{F}^{(t)}\)), \(^{(t)}^{||(t)}\) is a multivariate timeseries of voxels activities, \((t)\) is the length of the timeseries, and \(\) is the set of functional systems in the brain  in time window \(t\). In task-dependent data, each time window \(t\) corresponds to a task, and in resting state data, we have \(T=1\). We let \(t_{}=_{t=1,,T}(t)\)representing the maximum length of timeseries. BrainMixer consists of two main modules 1_Voxel Activity_ (VA) Encoder and 2 Functional Connectivity (FC) Encoder:

### Voxel Activity Encoder

The main goal of this module is to learn the time series of the voxel-level activity. However, the activities of voxels are not disjoint; for example, an increase in fusiform face area (FFA) activity might be associated with a rise in V1 activity. Accordingly, effectively learning their dynamics patterns requires both capturing cross-voxel and within-voxel time series information. The vanilla MLP-Mixer can be used to bind information across both of these dimensions, but the human brain has unique traits that make directly applying vanilla MLP-Mixer insufficient/impractical. First, there does not exist in general a canonical grid of the brain to encode voxel activities, which makes patch extraction challenging. Second, contrary to images that can be divided into patches of the same size, the partitioning of voxels might not be all the same size due to the complex brain topology. Third, vanilla MLP-Mixer employs a fixed static mixing matrix for binding patches, while in the brain the functionality of each patch is important and a different set of patchs should be mixed differently based on their connections and functionality. To address these challenges, the _VA Encoder_ employs two submodules, _time-mixer_ and _voxel-mixer_ with dynamic mixing matrix, to fuse information across both time and voxel dimensions, respectively.

The human brain is comprised of functional systems (FS) , which are groups of voxels that perform similar functions . We take advantage of this hierarchical structure and patch voxels based on their functionality. However, the main challenge is that the sizes of the patches (set of voxels with similar functionality) are different. To this end, inspired by the inference of ViT models , we linearly interpolate patches with smaller sizes.

Functional Patching.Let \(||\) be the number of voxels and \(^{||(T t_{})}\) represents the time series of voxels activities over all time windows. We split \(\) to spatio-temporal patches \(_{i}\) with size \(|f_{i}| t_{}\), where \(f_{i}\) is a functional system . To address the challenge of different patch sizes, we use Interpolate\((.)\) to linearly interpolate patches to the same size \(N_{p}\): i.e., \(}_{i}=(_{i})\), where \(}_{i}^{N_{p} t_{}}\). We let \(}^{|| t_{}}\) be the matrix of \(}_{i}\).

Voxel-Mixer.Since the effect of each task (e.g., in task-based fMRI) on brain activity as well as the time it lasts varies , for different tasks, we might need to emphasize more on a subset of voxels. To this end, to bind information across voxels, we use a dynamic attention mechanism that uses a learnable dynamic mixing matrix \(_{i}\), learning to mix a set of input voxels based on their functionality. While using different learnable matrices for mixing voxels activity provides a more powerful architecture, its main challenge is a large number of parameters. To mitigate this challenge, we first reduce the dimensions of \(}\), split it into a set of segments, denoted as \(S\), and then combine the transformed matrices. Given a segment \(s S\) we have:

\[}^{(t)^{(s)}}=}^{(t)}\;^{(s)}_{}\;^{|| d},\] ( _Dimension Reduction_ ) \[^{(s)}_{i}=(( }^{(t)^{(s)}})^{(s)^{(i)}}_{}) \;^{1||},\] ( _Learning Dynamic Mixer_ ) \[^{(t)}_{}=[_{s S}^ {(s)}}^{(t)^{(s)}}]_{}\;\; ^{|| t_{}},\] ( _Dynamic Positional Encoding_ ) \[^{(t)}_{}=(}^{(t)})+(^{(t)}_{}\,^{(t)^{}}_{}}{})^{(t)}_{ },\] ( _Dynamic Self-Attention_ )

where \(^{(s)}_{}^{t_{} d}\), \(^{(s)^{(i)}}_{}^{d||| |}\), \(_{}^{t_{} t_{}}\) are learnable parameters, \(\|\) is concatenation, and \((.)\) is row-wise sigmoid normalization. Note that for different segments we use different dimensionality reduction matrices to reinforce the power of the Voxel Mixing.

Time Mixer.We then fuse information in the time dimension by using the Time Mixer submodule. To this end, the Time Mixer employs a 2-layer MLP with layer-normalization :

\[^{(t)}_{}=^{(t)}_{}+( ((^{(t)}_{})^ {(1)}_{})^{(2)}_{})^{| | t_{}},\] (1)where \(_{}^{(1)}\) and \(_{}^{(1)}\) are learnable matrices, \((.)\) is an activation function (we use GeLU ), and LayerNorm is layer normalization .

### Functional Connectivity Encoder

To encode the functional connectivity graph, we design an MLP-based architecture that learns both the structural and temporal properties of the graph. Inspired by the recent success of all-MLP architecture in graphs , we extend MLP-Mixer to temporal graphs. We first define patches in temporal graphs. While patches in images, videos, and multivariate timeseries can simply be non-overlapping regular grids, patches in graphs are overlapping non-grid structures, which makes the patching extraction challenging. He et al.  suggest using graph partitioning algorithms to extract graph patches; however, these partitioning algorithms 1 only consider structural properties, missing the temporal dependencies, and 2 can be time-consuming, limiting the scalability to dense graphs like brain functional connectome. To this end, we propose a temporal-patch extraction algorithm such that nodes (voxels) in each patch share similar temporal and structural properties.

**Temporal Patching.** To extract temporal patches from the graph, we use a biased temporal random walk that walks over both nodes (voxels) and timestamps. Given a functional connectivity graph \(_{F}=\{_{F}^{(t)}\}_{t=1}^{T}\), we sample \(M\) walks with length \(m+1\) started from node (voxel) \(v_{0}\) like: \(:(v_{0},t_{0})(v_{1},t_{1}) (v_{m},t_{m})\), such that \((v_{i-1},v_{i})^{(t_{i})}\), and \(t_{0} t_{1} t_{2} t_{m}\). Note that, contrary to some previous temporal random walks [92; 10], we allow the walker to walk in the same timestamp at each step. While backtracking over time, we aim to capture temporal information and extract the dynamics of voxels' activity over _related_ timestamps. Previous studies show that doing a task can affect brain activity even after 2 minutes . To this end, since more recent connections can be more informative, we use a biased sampling procedure. Let \(v_{}\) be the previously sampled node, we use hyperparameters \(,_{0} 0\) to sample a node \(v\) with probability proportional to \(((t-t_{}+_{0}))\), where \(t\) and \(t_{}\) are the timestamps that \((v_{},v)^{(t)}\) and the timestamp of the previous sample, respectively. In this sampling procedure, smaller (resp. larger) \(\) means less (resp. more) emphasis on recent timestamps. Each walk started from \(v\) can be seen as a temporal subgraph, and so we let \(_{v}\) be the union of all these subgraphs (walks started from \(v\)). We treat each of \(_{v}\) as a temporal patch.

**Temporal Pooling Mixer.** Given the temporal graph patches that we extracted above, we need to encode each patch to obtain patch encodings (we later use these patch encodings as their corresponding voxel's encodings). While simple poolings (e.g., \((.)\)) are shown to miss information , more complicated pooling functions consider a static pooling rule. However, as discussed above, the effect of performing a task on the neuroimaging data might last for a period of time and the pooling rule might change over time. To this end, we design a temporal pooling, \((.)\), that dynamically pools a set of voxels in a patch based on their timestamps.

Given a patch \(_{v_{0}}=\{v_{0},v_{1},,v_{k}\}\), for each voxel we consider the correlation of its activity with other voxels' as its preliminary feature vector. That is, for each voxel \(v\), we consider its feature vector in the time window \(t\) as \(_{v}^{(t)}\), the \(v\)'s corresponding row in \(^{(t)}\). We abuse the notation and use \(_{_{v}}^{(t)}\) to refer to the set of \(^{(t)}\)'s rows corresponding to \(_{v}\). Since patch sizes are different, we zero pad \(_{_{v}}^{(t)}\) matrices to a fixed size. Note that this zero padding is important to capture the size of each voxel neighborhood. The voxel with more zero-padded dimensions in its patch has less correlation with others. To capture both cross-feature and cross-voxel dependencies, we can use the same architecture as the Time Mixer and Voxel-Mixer. However, the main drawback of this approach is that a pooling function is expected to be permutation invariant while the Voxel Mixer phase is permutation variant. To address this challenge, we fuse information across features in a non-parametric manner as follows:

\[_{}^{(t)}=_{_{v}}^{(t)}+(((_{_{v}}^{(t)})^{ }))^{}^{|_{v}| d^{}},\] (2)

where \((.)\) is an activation function, \((.)\) is used to normalize across features to bind and fuse feature-wise information in a non-parametric manner, avoiding permutation variant operations, and \(d^{}\) is the feature vector size. To dynamically fuse information across voxels, we use the same idea as dynamic self-attention in SS3.1 and learn dynamic matrices \(_{_{i}}\); let \(d_{}\) be the patch size:

\[_{_{i}}=(( _{}^{(t)})_{}^{(i)}) ^{1 d^{}}\] (3) \[_{_{v}}=((_{ }^{(t)})+_{}^{(t)}(_{}^{(t)}\,_{}^{(t)}}{}}}))^{1 d^{}},\] (4)

where \(_{}^{(t)}=_{}^{(t)}_{}\) is the transformation of \(_{}^{(t)}\) by dynamic matrix \(_{}\).

**Theorem 1**.: TPMixer _is permutation invariant and a universal approximator of multisets._

**Time Encoding.** To distinguish different timestamps in the functional connectivity graph, we use a non-learnable time encoding module proposed by Cong et al. . This encoding approach helps reduce the number of parameters, and also it has been shown to be more stable and generalizable . Given hyperparameters \(,\), and \(d\), we use feature vector \(=\{^{-i/}\}_{i=0}^{d-1}\) to encode each timestamp \(t\) using \((t)\) function. Therefore, we obtain the time encoding as \(_{t}=(t)\).

**Voxel-, Edge-, and Graph-level Encodings.** Depending on the downstream task, we might obtain voxel-, edge-, or graph-level encodings. For each voxel \(v\), we let \(^{(t)}[_{v}]\) be the set of connections in the patch of \(v\). To obtain the voxel-level encoding of each voxel \(v\), \(_{v}\), we use patch encoding and concatenate it with all the weighted mean of timestamp encodings; i.e., \(_{v}^{t}=([_{_{v}}\|_{v}])\), where \(_{v}=^{t}^{(t)}[_{v}]_{t}}{ _{t=1}^{t}^{(t)}[_{v}]}\). For a connection \(e=(u,v)^{(t)}\), we obtain its encoding by concatenating its endpoints and its timestamp encodings; i.e., \(_{(u,v)}^{(t)}=([_{u}^{t},_{v}^{t},_{t}])\). Finally, to obtain the graph level encoding, we use vanilla MLP-Mixer on patch encodings; let \(^{(t)}\) be the matrix whose rows are \(_{v}^{(t)}\):

\[_{}^{(t)}=^{(t)}+_{ {patch}}^{(2)}(_{}^{(1)} (^{(t)})),\] (5) \[(_{F}^{(t)})=(_{ }^{(t)}+((_{}^{(t )})_{}^{(1)})_{}^{( 2)}).\] (6)

Similar to the above, to obtain the brain-level encoding, \(_{}^{(t)}\), based on voxel acitivity timeseries, we use MLP-Mixer on \(_{}^{(t)}\).

### Self-supervised Pre-training

In SS3.1 and SS3.2 we obtained the encodings of the same contexts, from different perspectives. In this section, inspired by [40; 5], we use the mutual information of these two perspectives from the same context, to learn voxel- and brain-level encodings in a self-supervised manner. To this end, let \(\) be the voxel-level encodings obtained from functional connectome, \(_{}^{(t)}=(_{F}^{(t)})\) be the global encoding (brain-level) of the functional connectome, \(_{}^{(t)}\) be the voxel activity encodings from the brain activity timeseries, and \(_{}^{(t)}\) be the global encoding (brain-level) of the voxel activity timeseries, we aim to maximize \(I(_{}^{(t)},_{v,i}^{(t)})+I(_{}^ {(t)},(_{}^{(t)})_{v,j})\) for all \(v\) and possible \(i,j\). Following previous studies , we use Noise-Contrastive Estimation (NCE)  and minimize the following loss function:

\[_{(_{}^{(t)},_{v,i}^{(t)} )}[_{}[_{}(_{}^ {(t)},_{v,i}^{(t)},)]]+_{(_{}^{(t)},(_{}^{(t)})_{v,j})}[ _{}[_{}(_{}^{(t)},(_{}^{(t)})_{v,j},)]],\] (7)

where \(\) is the set of negative samples, \((_{}^{(t)},_{v,i}^{(t)})\) and \((_{}^{(t)},(_{}^{(t)})_{v,j})\) are the positive sample pairs, and \(_{}\) is a standard Log-Softmax.

## 4 Experiments

**Dataset.** We use six real-world datasets: 1 We present Bvfc, a task-based fMRI dataset that includes voxel activity timeseries and functional connectivity of 3 subjects when looking at the 8460images from 720 categories. This data is based on THINGS dataset . 2Bvfc-MEG is the MEG counterpart of Bvfc. 3 ADHD  contains data for 250 subjects in the ADHD group and 450 subjects in the typically developed (TD) control group. 4 The Seizure detection TUH-EEG dataset  consists of EEG data (31 channels) of 642 subjects. 5 ASD  contains data for 45 subjects in the ASD group and 45 subjects in the TD group. 6 HCP  contains data from 7440 neuroimaging samples each of which is associated with one of the seven ground-truth mental states.

**Evaluation Tasks.** In our experiments we focus on 4 downstream tasks: 1 Edge-Anomaly Detection (AD), 2 Voxel AD, 3 Brain AD, and 4 Brain Classification. For the edge and voxel AD tasks, we follow previous studies , and inject 1% and 5% anomalous edges into the functional connectivity in the control group. For brain AD all datasets has ground-truth anomalies (see Appendix E.2). The ground truth anomalies in Bvfc are the brain responses to not recognizable images, generated by BigGAN , and for other datasets are brain activity of people living with ADHD, seizure, and ASD. For brain classification, we focus on the prediction of 1 categories of images seen by the subjects (in Bvfc, and Bvfc-MEG), and 2 age prediction and mental state decoding (in HCP-Age, and HCP-Mental). The details of the setup are in Appendix E.

**Baselines.** We compare BrainMixer with state-of-the-art time series, graph, and brain anomaly detection and learning models: 1 Graph-based methods: GOutlier , NetWalk , HyperSAGCN , Graph MLP-Mixer (GMM) , GraphMixer. 2 brain-network-based methods: BrainGnn , FbNetGen , BrainNetCnn , ADMire , and BNTransformer , PTGB . 3 Time-series-based methods: Usad , Time Series Transformer (TST) , and Mvts . We may exclude some baselines in some tasks as they cannot be applied in that setting. We use the same training procedure as BrainMixer. The details are in Appendix E.1.

**Brain Classification.** Table 1 reports the performance of BrainMixer and baselines on multi-class brain classification tasks. BrainMixer achieves the best accuracy on all datasets with 14.3% average improvement (\(20.3\%\) best improvement) over the best baseline. There are three main reasons for BrainMixer's superior performance: 1 While the time series-based model only uses voxel activity timeseries, and graph-based methods only use functional connectivity graph, BrainMixer takes advantage of both and learns the brain representation at different levels of granularity, which can provide complementary information. 2 Static methods (e.g., PTGB, BrainGnn, etc.), miss the dynamics of brain activity, while BrainMixer employs a time encoding module to learn temporal properties. 3 Compared to graph learning methods (e.g., GMM, GraphMixer, etc.), BrainMixer is specifically designed for the brain, taking advantage of its special properties.

**Anomaly Detection.** Table 2 reports the performance of BrainMixer and baselines on anomaly detection tasks at different scales: i.e., edge-, voxel-, and brain-level. BrainMixer achieves the best AUC-PR on all datasets with 6.2%, 5.7%, 4.81% average improvement over the best baseline in edge AD, voxel AD, and brain AD, respectively. The main reasons for this superior performance are as above. Note that brain-level anomaly detection can also be seen as a brain classification task. However, here, based on the nature of the data, we separate these two tasks.

**How Does BrainMixer Detect GAN Generated Images?** The visual cortex, responsible for processing visual information, is hierarchically organized with multiple layers building upon simpler features at lower stages . Initially, neurons detect edges and colors, but on deeper levels, they specialize in recognizing more complex patterns and objects. Figure 2 (Left) (resp. (Right)) reports the distribution of detected voxel activity by BrainMixer when the subject looking at non-recognizable images (resp. natural images). Interestingly, while the distributions share similar patterns in lower

  Methods & Bvfc & Bvfc-MEG & HCP-Mental & HCP-Age \\   Usad & \(48.52_{1.94}\) & \(50.02_{11.13}\) & \(73.49_{15.36}\) & \(39.17_{14.16}\) \\ HypensAGCN & \(51.92_{1.47}\) & \(11.59_{18.93}\) & \(90.74_{11.61}\) & \(47.38_{1.19}\) \\ GMM & \(53.11_{1.14}\) & \(53.04_{12.79}\) & \(90.92_{13.45}\) & \(47.45_{11.20}\) \\ GraphMixer & \(53.71_{12.52}\) & \(53.12_{11.93}\) & \(91.31_{13.14}\) & \(48.32_{11.11}\) \\ BrainNetCN & \(49.10_{13.05}\) & \(50.12_{11.92}\) & \(53.85_{11.48}\) & \(42.26_{12.62}\) \\ BransGNN & \(50.63_{1.25}\) & \(50.63_{1.08}\) & \(50.52_{12.97}\) & \(41.08_{1.54}\) \\ PenFrien & \(51.80_{1.09}\) & \(50.94_{12.91}\) & \(54.74_{12.88}\) & \(42.82_{12.9}\) \\ ADMire & \(54.36_{1.99}\) & \(58.45_{11.92}\) & \(98.74_{11.93}\) & \(47.82_{11.72}\) \\ PTGB & \(55.95_{11.15}\) & \(55.11_{12.02}\) & \(92.51_{11.31}\) & \(48.41_{11.47}\) \\ BNTransformer & \(55.03_{1.38}\) & \(55.11_{11.74}\) & \(91.71_{11.14}\) & \(47.94_{11.11}\) \\ BrainMixer & \(67.24_{1.47}\) & \(62.58_{1.08}\) & \(96.32_{1.09}\) & \(57.83_{1.09}\) \\  

Table 1: Performance on multi-class brain classification: Mean ACC (%) \(\) standard deviation.

[MISSING_PAGE_FAIL:8]