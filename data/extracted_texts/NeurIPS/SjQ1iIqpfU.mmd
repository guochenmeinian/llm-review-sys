# CoBo: Collaborative Learning via Bilevel Optimization

Diba Hashemi

EPFL

diba.hashemi@epfl.ch &Lie He 1

Tencent Inc.

liam.he15@gmail.com &Martin Jaggi

EPFL

martin.jaggi@epfl.ch

###### Abstract

Collaborative learning is an important tool to train multiple clients more effectively by enabling communication among clients. Identifying helpful clients, however, presents challenging and often introduces significant overhead. In this paper, we model _client-selection_ and _model-training_ as two interconnected optimization problems, proposing a novel bilevel optimization problem for collaborative learning. We introduce CoBo, a _scalable_ and _elastic_, SGD-type alternating optimization algorithm that efficiently addresses these problem with theoretical convergence guarantees. Empirically, CoBo achieves superior performance, surpassing popular personalization algorithms by 9.3% in accuracy on a task with high heterogeneity, involving datasets distributed among 80 clients.2

## 1 Introduction

In a classic collaborative learning scenario, \(n\) clients, each with a distinct machine learning task, seek solutions that potentially outperform their individual solvers through a collective effort. Common collaborative learning frameworks generally alternate between training local models on individual datasets and synchronizing updates among collaborators. More concretely, during the computation step, client \(i[n]\) trains a \(d\)-dimensional model \(_{i}^{d}\) to minimize its loss function, \(f_{i}:^{d}\). In the subsequent communication step, client \(i\) exchanges updates with clients, potentially benefiting from collaboration.

Despite the plethora of collaborative learning frameworks, the ideal approach to collaborate remains under-exploited. The FedAvg algorithm learns a single global model over pooled datasets from all clients, i.e., \(_{^{d}}_{i=1}^{n}f_{i}()\). However, due to heterogeneous data distributions among clients, a global model may significantly underperform compared to personal models trained on local datasets for certain clients, which can discourage their participation in collaborative training . Ditto addresses this issue by training personal models with a regularization term that penalizes deviations from a global model . Although Ditto enables personal models to leverage the global model, it offers only a coarse-grained level of collaboration. In instances where clients' data exhibit significant differences, the Ditto algorithm is constrained to facilitating collaboration at a global level, thereby neglecting the inherent heterogeneity among clients.

Clustering-based federated learning algorithms have been developed to accommodate scenarios in which clients' data originate from multiple clusters . Nevertheless, these algorithms typically inherit the limitations associated with clustering techniques, including the need to predetermine the number of clusters, initialize cluster centers, and other such prerequisites, which can diminish their practical utility.

In this paper, we propose a bilevel optimization framework to enhance collaborative learning by discovering better structural relationships among clients. The inner problem focuses on optimizing a binary collaborator selection variable \(w_{ij}\{0,1\}\), determined based on a gradient alignment measure for each pair of clients. In the outer problem, we train personalized models \(^{n d}\), incorporating a penalization term that accounts for the distances between clients, as dictated by the collaboration weights established in the inner problem.

The contributions of this paper can be summarized as follows:

* We model collaborative learning through a novel bilevel optimization formulation that yields more generalizable solutions by fully exploiting the inherent structure of collaboration.
* We propose CoBo, an SGD-type alternating optimization algorithm that efficiently solves the bilevel problem. CoBo scales with the number of clients \(n\) and is elastic to the number of clients.
* We prove that CoBo enjoys theoretical convergence guarantees for collaborative learning with cluster structures.
* Empirically, CoBo surpasses popular personalized federated learning baselines in experiments involving highly heterogeneous federated learning settings and Large Language Models (LLMs).

## 2 Problem formulation

In this paper, we model collaborative learning as a bilevel optimization problem, where personalized models \(^{d n}\) are trained in the outer problem, and collaborative weights \(^{n n}\) are determined by the inner problem. More concretely,

\[_{[_{1},,_{n}]^{d n}} _{i=1}^{n}f_{i}(_{i})+_{1 i<j n}w^{*}_{ij} \|_{i}-_{j}\|_{2}^{2}\] (Model-Training) \[w^{*}_{ij}\] (Client-Selection)

where \(>0\) is a hyperparameter for penalization. We break down the formulation as follows.

Outer problem: training personalized models.In the outer problem (Model-Training), client \(i\) trains its model \(_{i}\) by minimizing its loss function \(f_{i}\), along with penalizing its distances to neighboring models, e.g. \(_{j}\), as weighted by \(w^{*}_{ij}>0\).

Our formulation is similar to DITO , but with two key differences: First, DITO uses uniform and fixed collaboration weights and penalizes the distance between \(_{i}\) and a global model, whereas we penalize the distances between pairs of clients and adjust the collaboration weights during training. Consequently, when client tasks are heterogeneous--such as clients drawn from clusters--the performance of a global model deteriorates, and DITO's local models cannot benefit from fine-grained collaboration. In contrast, our method is able to exploit such structure and achieve better performance in diverse settings.

Inner Problem: Finding Collaborators.In the inner problem, we decompose the task of optimizing \(^{n n}\) into independent sub-problems, one for each entry of \(\). We relax the binary collaborator selection variable \(w_{ij}\{0,1\}\) to a continuous weight \(w_{ij}\). As the objective function is linear with respect to \(w_{ij}\), and the domain is convex, optimization algorithms such as Frank-Wolfe [10; 17] or projected gradient descent can efficiently find the maximizers, which occur at \(0\) or \(1\).

It is important to note that \(w^{*}_{ij}\) does not imply a permanent connection between clients \(i\) and \(j\), but rather a temporary assessment based on the current states of \(_{i}\) and \(_{j}\).

A simple inner problem with two clients is illustrated in Figure 1. The \(f_{1}\), \(f_{2}\) are their loss functions, and suppose \(_{1}\), \(_{2}\), and \(\) are the minimizers of \(f_{1}\), \(f_{2}\), and \((f_{1}+f_{2})\). Suppose \(_{1}\), \(_{2}\), and \(\) are the minimizers of \(f_{1}\), \(f_{2}\), and \((f_{1}+f_{2})\) respectively. The model weights at points \(A,B,C\) demonstrate three scenarios for updating \(\).

* **Point A:** The model \(^{A}\) is far away from \(\), i.e., \(\|^{A}-\|>>_{i}\|_{i}-\|\). The descent directions of the clients have a positive inner product; therefore \(w_{12}=1\). Collaboration at this stage speeds up training.
* **Point B:** The model \(^{B}\) is closer to \(\), i.e., \(\|^{B}-\|\|_{i}-\|\). In this case, moving closer to the minimizer \(\) of \((f_{1}+f_{2})\) no longer helps both clients get closer to the minimizers of their own losses \(_{i}\). The inner problem yields \(w_{12}=0\) and the clients disconnect.
* **Point C:** The models \(_{1}^{C}\) and \(_{2}^{C}\) are already disconnected. The gradients computed at their midpoint suggest they should remain disconnected; thus \(w_{12}=0\).

Because collaboration weights in Client-Selection are determined in a pairwise fashion, our formulation, unlike clustering-based methods , does not require knowledge of cluster sizes, allowing clients to join and leave during collaborative training. This elasticity enables our method to be applicable in a wider range of scenarios.

**Remark 1** (Extensions).: _While Client-Selection is defined over a box constraint \(^{n n}\), it can be easily extended to other convex domains. For example, in all-for-one type collaborative training, the weights are optimized over a simplex. The experiment on language models is deferred to Section 4.3._

### Algorithm

We propose a novel stochastic gradient descent (SGD)-type alternating optimization algorithm, termed CoBo, to solve the bilevel optimization problem defined by (Model-Training) and (Client-Selection). The algorithm alternates between updating the model variables \(\) and the collaboration weights \(\).

In each iteration \(t\), we first fix the model variables \(\{_{i}^{t}\}_{i=1}^{n}\) and update the collaboration weights by applying projected gradient ascent with step size \(>0\) to the inner problem (Client-Selection):

\[w_{ij}^{t+1}=_{}(w_{ij}^{t}+ f_{i }(_{i}^{t}+_{j}^{t}}{2}), f_{j}( {_{i}^{t}+_{j}^{t}}{2})) i,j [n].\] (1)

Next, with the updated collaboration weights \(\{w_{ij}^{t+1}\}\) are fixed, we optimize the model variables \(\{_{i}\}_{i=1}^{n}\) using the following update rule derived from the outer problem (Model-Training):

\[_{i}^{t+1}=_{i}^{t}-( f_{i}(_{i}^{t})+ _{k=1}^{n}w_{ik}^{t+1}(_{i}^{t}-_{k}^{t}))  i[n],\] (2)

where \(>0\) is the step size for updating the model variables. This alternating process is repeated until convergence.

The detailed implementation of the algorithm is provided in Algorithm 1. In this implementation, the full gradients \(\{ f_{i}\}_{i[n]}\) in (1) and (2) are replaced by their stochastic estimates. Additionally, collaborative weights are updated with a probability of \(()\), leading to an expected computation of

Figure 1: Diagram of the inner problem (Client-Selection) represented through a contour of \((f_{1}+f_{2})\). The blue arrows \(\) are gradients computed at middle point \((_{1}+_{2})\) to determine connectivity. The red arrows \(\) represent gradients computed at local models to update model weights.

\((n)\) gradients. This results in an overhead comparable to that of standard decentralized learning methods [25; 19], thereby enabling client selection with minimal additional cost.

Compared to federated clustering algorithms, which require global synchronization before applying clustering oracles, the inner problem (Client-Selection) in CoBo is solved in a pairwise fashion. This pairwise approach makes the algorithm non-blocking and robust to stragglers, providing greater flexibility and efficiency. Not all pairwise weights have to be computed in each iteration. In Table 1 we compare the performance of multiple edge-sampling strategies.

## 3 Theoretical results

In this section, we define assumptions in collaborative learning settings and show that CoBo converges to stationary points. The following assumptions regarding the local optimization objective \(f_{i}\) are commonly adopted in the literature [1; 19]:

**(A1) \(L\)-smooth.**_For all \(\) and \(\) in \(^{d}\) and \(i[n]\), the loss function \(f_{i}\) has \(L\)-Lipschitz gradients, i.e._

\[\| f_{i}()- f_{i}()\| L\|- \|\,.\]

**(A2) Noise bound.**_For all \(^{d}\) and \(i[n]\), there exists \(^{2}>0\) such that the stochastic gradient has bounded noise_

\[_{}[\| f_{i}(;)-_{}[  f_{i}(;)]\|^{2}]^{2}\,.\]

**(A3) Global minimum.**_For all \(i[n]\), the loss function \(f_{i}\) has a global lower bound \(f_{i}^{*}\)._

The next assumption characterizes the possible relationships between clients. In the first case, when reaching the stationary point \(\) of their joint objective \(f_{i}+f_{j}\), then by (5) implies that \( f_{i}()= f_{j}()=\) client \(i\) and \(j\) reach their own stationary points. In the second case, when client \(i\) reaches its stationary point, the gradient of \(j\) is lower bounded by a positive constant, meaning they don't share stationary points. This leads to eventual 

**(A4) Collaborativeness.**_If clients \(i\) and \(j\) are collaborative, then there exists \(M_{ij}>0\) such that_

\[\| f_{i}()- f_{j}()\|_{2}^{2} M_{ij}^{2} \| f_{i}()+ f_{j}()\|_{2}^{2}\; ^{d}.\] (5)

_Otherwise, there exists \(_{ij}^{2}>0\) such that_

\[\| f_{i}()\|_{2}^{2}+\| f_{j}()\| _{2}^{2}_{ij}^{2}\;^{d}.\] (6)

This assumption is similar to [39, Assumptions 4,5], but we define relations for pairs of clients instead of clusters. In the next example, we use quadratics to demonstrate (A4)

**Example 2**.: _Assume that there are \(K\) clusters with \([n]=_{k[K]}\;_{k}\) and \(_{k}_{k^{}}=\) for all \(k k^{}[K]\). Consider the \(k\)-th cluster with center \(_{k}\) and client \(i_{k}\), the loss function is \(f_{i}()=}{2}\|-_{k}\|_{2}^{2}\) where \(a_{i}>0\). Then for clients \(i,j\) in the same cluster, i.e. \(i,j_{k}\)_

\[\| f_{i}()- f_{j}()\|_{2}^{2}=(a_{i}-a_{j} )^{2}\|-_{k}\|_{2}^{2}=-a_{j})^{2}}{(a_{i}+a_{j})^{2 }}\| f_{i}()+ f_{j}()\|_{2}^{2}.\]

_The \(M_{ij}=-a_{j}|}{a_{i}+a_{j}}\) in this case. On the other hand, for \(i_{k}\) and \(j_{k^{}}\) and \(_{k}_{k^{}}\),_

\[\| f_{i}()\|_{2}^{2}+\| f_{j}()\| _{2}^{2}=a_{i}^{2}\|-_{k}\|_{2}^{2}+a_{j}^{2}\|-_{ k^{}}\|_{2}^{2}=^{2}a_{j}^{2}}{(a_{i}^{2}+a_{j}^{2})^{2}}\|_{k}-_{k^{}}\|_{2}^{2}\]

_where the lower bound \(_{ij}^{2}=^{2}a_{j}^{2}}{(a_{i}^{2}+a_{j}^{2})^{2}}\| _{k}-_{k^{}}\|_{2}^{2}>0\)._

Finally, we derive a convergence theorem with the assumption that clients are drawn from clusters, as e.g. in [33, Assumption 2].

**(A5) Cluster.**_All clients are drawn from clusters where within each cluster clients share stationary points._

**Theorem I**.: _Suppose Assumption 1,2,3,4,5 hold true. Suppose that CoBo solves (4) with mini-batch size \(b\). Consider clients \(i\) and \(j\) in the same cluster \(\) of size \(c\). Suppose that \(M_{ij}^{2}(0,)\), \(b}2L(c-2)^{2}\) and \(_{ik}^{2}\| f_{i}()+ f_{k}()\|_{2}^{2}\) for all \(\) and \(k\). Let \(L}{c}\) and step size_

\[\{}}_{i,j }(_{ij}(_{ij}^{0})-_{ij}^{ *})},L}\}.\]

_The consensus distance also converges to 0, i.e._

\[T}_{t=0}^{T-1}_{i,j}[\| _{i}^{t+1}-_{j}^{t+1}\|_{2}^{2}] ^{2}}{^{2}c^{2}}}{c^{2}T} _{i,j}(_{ij}(_{ij}^{0})-_{ij}^{*})}.\]

_Moreover, the gradient norm is upper bounded._

\[T}_{t=0}^{T-1}_{i,j}[\| _{ij}(_{ij}^{t})\|_{2}^{2}] 3}{c^{2}T}_{i,j} (_{ij}(_{ij}^{0})-_{ij}^{*})}.\]

This theorem suggests that clients inside the same cluster gradually reach consensus. This cluster-level consensus model reaches stationary point of their losses by (5). Note that a larger penalization parameter \(\) and smaller values of \(M_{ij}^{2}\) lead to faster convergence, which aligns with our expectations. Note that \(M_{ij}\) in (A4) measures how well i,j collaborate. A smaller \(M_{ij}\) leads to better consensus distance in Theorem I, with \(M_{ij}=0\) leading to identical data distribution. The following corollary states the convergence of norm of client gradient of model \(_{i}\).

**Corollary II**.: _Under same conditions as Theorem I, \(\| f_{i}(_{i}^{t})\|_{2}^{2}\) converges at a similar rate_

\[T}_{t=0}^{T-1}_{i,j}\| f_{i} (_{i}^{t})\|_{2}^{2} 4}{c^{2}T}_{i,j}( _{ij}(_{ij}^{0})-_{ij}^{*})}.\]Experiments

In this section, we present three experiments to demonstrate the practical effectiveness of CoBo. In the first two experiments, we benchmark CoBo in both a cross-silo federated learning setup involving 8 clients and a cross-device setup with 80 clients, using the CIFAR-100 dataset for multi-task learning . In the third experiment, we train language models on subsets of the Wiki-40B dataset while learning domain weights within a simplex . Compared to state-of-the-art personalized federated learning baselines, CoBo obtains personalized models of higher quality and correctly identifies cluster structures. Details of the experiments, including descriptions of the architectures and the system setup, are deferred to Appendix B.

Throughout the experiments, we use popular federated learning baselines such as FedAvg, Federated Clustering (abbreviated as FC) , Ditto, IFCA , and an oracle algorithm. The definition of the oracle baseline varies in each setup and will be discussed case by case. Note that we additionally provide clustering-based algorithms, i.e., FC and IFCA, with the actual number of clusters. Their experimental statistics reported in this section, such as accuracy and perplexity, include this advantage. In addition to previous baselines, we also compare CoBo with a collaborative fine-tuning approach for large language models that leverages performance on validation data to determine the collaboration weights  (referred to as Validation Based in Table 2).

### Cross-silo federated learning experiment with 8 clients

In this experiment, we evaluate the performance of CoBo by comparing the average accuracies of local models against those of established collaborative learning baselines. Our objective is to assess how effectively CoBo discerns and leverages the structure of data clusters relative to other collaborative learning algorithms.

We simulate a cross-silo multi-task environment where training a single model across all clients yields poor performance, thus highlighting the necessity for client selection. Our experimental configuration consists of 4 clusters, each containing 2 clients utilizing the ResNet-9 model . To encourage collaboration within clusters, we randomly allocate half of the dataset to each client in a cluster. To differentiate between clusters, we introduce label diversity by flipping the image labels in each cluster using distinct random seeds. This process ensures that each class maintains unique labels across all clusters, effectively creating a scenario where a universally trained model would not be optimal, thereby necessitating personalized models that can cater to the specific label distribution of each cluster.

In this context, collaboration among clients within the same cluster is advantageous, as their datasets are complementary. There are two primary reasons why collaboration between different clusters may not be beneficial: (1) the dataset available to clients within each cluster is identical, negating the incentive to collaborate with clients from other clusters; and (2) the label flipping across clusters could mean that inter-cluster collaboration might actually degrade local model performance.

Given these considerations, we designate an oracle algorithm for our scenario: FedAvg implemented separately within each cluster. This ensures that collaboration is confined to where it is most beneficial. Additionally, the oracle collaboration matrix is defined to be a block-diagonal matrix, with entries of 1 for pairs of clients within the same cluster (indicating collaboration) and entries of 0 for pairs from different clusters (indicating no collaboration). This matrix serves as a benchmark for the ideal collaboration structure in our simulated environment.

To enable the practical application of CoBo, we sample pairs of clients in each iteration to update their collaboration weights. We begin by examining the impact of various sampling strategies on the performance of CoBo. The primary approach involves sampling with a constant probability of \((1/n)\). Additionally, we observe that CoBo identifies an appropriate collaboration matrix early in the training process, motivating the use of a time-step-dependent sampling rate, \((1/t)\). We also implement a mixed strategy: employing the constant sampling rate, \((1/n)\), for the initial 0.2% of iterations, followed by a switch to the time-dependent sampling rate, \((1/t)\), for the remainder of the training. A comparison of these strategies with the non-sampling oracle, where all pairs are updated in every iteration, is presented in Table 1. While CoBo demonstrates consistent performance across all sampling strategies, achieving results close to those of the non-sampling oracle, the mixed strategy shows a slight performance advantage.

To further assess performance, we trained CoBo and other baseline algorithms for a total of 40,000 iterations. Figure 1(b) presents the accuracy diagram. We observe that CoBo almost reaches the performance bound established by the Oracle. Moreover, CoBo achieves a fixed accuracy of 60% in 4,500 iterations, which is 30% faster than Ditto. For better comparison, the values of accuracy and loss are reported in Table 2. Additionally, the evolution of the collaboration matrix for clustering algorithms and CoBo is illustrated in Figure 3. CoBo starts to identify clients with similar label permutations as early as 300 iterations and stabilizes in less than 5,000 iterations (12.5% of the training phase). IFCA always degenerates to one fully connected cluster, while FC periodically suffers from clustering mistakes even at the end of training.

Figure 1(a) presents the results of the cross-silo experiment under various configurations to further assess the robustness of CoBo. First, we modify the fraction of the dataset allocated to each client. Intuitively, the total amount of data available to a cluster directly impacts the performance of CoBo. Next, we experiment with different numbers of clusters, each containing two clients, and observe that

    & Acc.(\%) & Loss \\  Constant (\((1/n)\)) & 73.05 & 1.104 \\ Time-dependent (\((1/t)\)) & 73.18 & 1.226 \\ Mixed & 74.77 & 1.081 \\ No Sampling (Oracle) & 74.93 & 1.278 \\   

Table 1: Comparison of the average performance of CoBo across different sampling strategies for updating the weights of client pairs in the collaboration matrix. All strategies demonstrate performance close to that of the non-sampling oracle. However, the mixed strategy, which combines a constant sampling rate at the start with a time-dependent rate during later training phases, shows superior performance.

Figure 2: (1(a)) Average accuracy in cross-silo experiments with varying factors, including the fraction of the dataset available to clients, the number of clusters, and the number of clients per cluster. (1(b)) Average accuracy of personalized models for cross-silo federated learning with 8 clients. The ”Oracle” denotes applying FedAvg to the clients with the same label permutation.

the number of clusters does not significantly affect CoBo's accuracy. Additionally, we investigate the effect of varying the number of clients per cluster while maintaining a fixed total of four clusters. In this setup, the dataset is partitioned among clients within each cluster, resulting in less data per client as the cluster size increases. Despite this, CoBo leverages collaboration to maintain robust performance even with larger cluster sizes.

### Cross-device experiment experiment with 80 clients

In this experiment, we demonstrate the performance of CoBo in a challenging cross-device federated learning setting characterized by significant data heterogeneity. We create 10 clusters of varying sizes: 2 clusters consist of 6 clients each, another 2 comprise 7 clients each, and so on. Each cluster is allocated data from 10 distinct classes out of the total 100 classes available in the CIFAR-100 dataset, ensuring that the data across clusters are disjoint. Within each cluster, the data are distributed uniformly at random among the clients. We proceed to train individual ResNet-9 models  on each client's data for a total of 20,000 iterations. This setup allows us to observe the behavior of CoBo and its ability to handle both the quantity and diversity of data across different client groups and cluster sizes.

We define the oracle algorithm and the corresponding collaboration matrix in the same manner as in Section 4.1. Note that while we manually create the clusters, inter-cluster collaboration may still be helpful in practice, and it is impossible to know the actual ground truth in this case. Consequently, we recognize that the oracle may not correspond to the optimal performance. Nevertheless, this oracle still exhibits superior performance compared to other baselines that lack prior knowledge of the data distribution among clients, as evidenced by the results presented in Table 2. The collaboration matrix and accuracy plots are deferred to Figure 5 and Figure 6 in Appendix B, respectively.

In this challenging experiment, CoBo surpasses all other baselines by at least 5.7% in accuracy. This supports the conclusion that CoBo scales well with the size of collaborative learning and effectively exploits collaboration weights among clients at a fine-grained level.

Figure 3: Collaboration matrices learned by Federated Clustering (FC), IFCA, and CoBo at different stages of training for cross-silo experiment with 8 clients. The diagonals are masked out. The oracle matrix is a block diagonal matrix with blocks of size 2. The collaboration matrix of CoBo already starts to look similar to oracle matrix within as low as 300 iterations (0.75% of the total iterations), and converges to it within 5000 iterations (12.5% of the total iterations). On the other hand, IFCA yields a fully-connected matrix while FC occasionally diverges from the achieved cluster structures (e.g., iterations 300, 5000, and 40000), even at the end of training.

### Collaborative fine-tuning on language models

Recently, Large Language Models (LLMs) have gained significant popularity due to their ability to effectively solve challenging tasks. Their downstream performance can be further enhanced by fine-tuning; however, the scarcity of data often leads to inferior performance and necessitates collaboration. Therefore, we conduct an experiment with four clients, each having a pre-trained GPT-2 base model3 with 124 million parameters in total , and a subset of articles from the Wiki-40B dataset  in one of the following four languages: Catalan, Spanish, German, or Dutch. We use LoRA for the Self-Attention and MLP layers for fine-tuning, which accounts for 0.47% of the full parameters .

For data-hungry tasks, such as those involving LLMs, contributions from all domains are valuable. Clustering methods fall short in this aspect due to their binary, discrete outputs, which do not capture the nuanced degrees of collaboration needed. CoBo addresses this limitation by allowing for a continuous range of collaboration intensities, achieved by a simple yet effective modification to the projection domain in (1). Specifically, we employ a probability simplex, denoted as \(_{i}=\{w_{ij} 0,_{j}w_{ij}=1\}\), as the domain of the inner problem.

In Table 2, we compare the perplexity of CoBo with baselines after 500 iterations, when FedAvg converges. There are no oracle domain weights in this experiment due to the complex coherence among languages; therefore, we omit the oracle algorithm in the table. CoBo achieves the best perplexity among all algorithms. In Figure 4, we demonstrate the domain weights learned for the Catalan language. Overall, Catalan assigns the highest collaboration weight to Spanish, which is reasonable considering the similarity between the two languages.

    &  &  &  \\   & Acc.(\%) & Loss & Imp.(\%) & Acc.(\%) & Loss & Imp.(\%) & Perplexity & Imp.(\%) \\  Local & \(64.9 0.1\) & 1.67 & - & \(54.9 0.1\) & 1.40 & - & \(41.26 0.38\) & - \\ FedAvg & \(18.8 0.1\) & 2.66 & 0 & \(53.9 0.1\) & 1.79 & 29 & \(64.84 0.00\) & 0 \\ Fine-tuning FedAvg & \(70.2 0.2\) & 1.77 & 0 & \(58.9 0.1\) & 1.88 & 94 & \(46.70 0.07\) & 0 \\ Ditto & \(73.5 0.3\) & 1.55 & **100** & \(70.3 0.1\) & 1.21 & **100** & \(40.05 0.01\) & **100** \\ IFCA & \(18.6 0.1\) & 2.75 & 0 & \(45.6 0.8\) & 2.15 & 4 & - & - \\ FC & \(55.1 0.4\) & 1.79 & 0 & - & - & - & - & - \\ Validation Based & - & - & - & - & - & - & \(42.90 1.68\) & 75 \\ CoBo & \(\) & **1.08** & **100** & \(\) & **0.97** & **100** & \(\) & **100** \\  Oracle & \(75.4 0.2\) & 1.07 & 100 & \(83.6 0.3\) & 0.70 & 100 & - & - \\   

Table 2: Comparisons of model quality and fairness measure of personalized models for cross-silo experiment with 8 clients, and cross-device experiment with 80 clients, and the language modelling experiment with 4 clients having different languages. Federated clustering (FC) is not scalable with number of clients due to its \((n^{2})\) complexity, and therefore ignored in the cross-device fl experiment. The clustering algorithms IFCA and FC are not applicable to LLMs and there ignored. Note that Oracle is not defined in the LLMs experiment. The column “Imp.(%)” demonstrates the percentage of clients with improved performance compared to local training.

Figure 4: Domain weights found by CoBo for Catalan language. There are 4 domains in total: Catalan, Spanish, German, and Dutch. The curves are smoothed by exponential moving average.

Related Work

Personalized federated learning.Personalized federated learning has received significant attention in recent years due to its potential to tailor models to individual user data while benefit from collaboration [33; 35; 36; 9; 22; 3]. There are various flavors of personalized federated learning. Ditto trains personalized models by incorporating a regularization term that penalizes the divergence from a global model . Many personalization works assume that clients are drawn from clusters. For example, Martoj et al.  use K-nearest neighbors (KNN) to determine collaborators. Mansour et al. , Ghosh et al. , Werner et al.  develop \(K\) personalized models and assign clients to clusters based on criteria such as minimum function values or gradient similarities. Additionally, Even et al.  provided theoretical insights by establishing lower bounds, which demonstrate that the optimal gradient filtering strategy involves clustering clients with identical optima.

Federated learning with client selectionIn federated learning, client selection is often performed by simultaneously minimizing task losses and collaborative weights in a single-level objective function. Zantedeschi et al.  minimize task losses augmented with a penalization term \(w_{ij}\|_{i}-_{j}\|_{2}^{2}\), similar to our outer problem. However, optimizing \(w_{ij}\) directly can lead to a degenerate solution (\(w_{ij}=0\)), which necessitates an additional penalization for small \(w_{ij}\) values. Smith et al.  approach multi-task learning by minimizing task losses with a more sophisticated penalization term that accounts for the relationships between tasks. This formulation requires the client-selection function to be consistent with client selection, which can negatively impact performance. Apart from multi-task federated learning, a similar bilevel optimization formulation has been used by Le Bars et al.  to find a sparse mixing matrix while training a consensus model in the outer problem.

Bilevel optimization and alternating optimization.Bilevel optimization is a powerful tool which models a broad range of problems, such as reinforcement learning [6; 30; 16; 15; 37; 31], and linearly-solvable Markov decision process , meta-learning [8; 20], etc. A typical bilevel optimization problem, as the name indicates, consists of an outer and an inner optimization problem whose variables are inter-dependent. Typical bilevel optimization solvers requires hessian information which is usually expansive to acquire . On the other hand, alternating optimization tools has been used be used to solve bilevel optimization problem [2; 4]. While in general there is no universal convergence guarantees for alternative optimizations, the special structure of our inner problem ensures the convergence of CoBo to the stationary point.

## 6 Conclusions

Existing collaborative learning algorithms only allow coarse-grained collaboration, which leads to inferior performance in practice. To address this issue, we model collaborative learning as a special bilevel optimization problem where client selection is based on the optimization of a linear function of gradient alignment measure for each pair of clients. In addition, we propose an efficient SGD-type alternating optimization algorithm CoBo which is scalable, elastic, and enjoy theoretical guarantees. Besides, CoBo empirically outperforms popular personalized federated learning algorithms in realistic collaborative learning problems.

Limitations.In this work, we do not take privacy into consideration. The existing algorithm requires exchanging gradients between collaborators when updating weight \(\) which may raise privacy concerns. We defer the discussion of privacy-preserving collaborative learning framework to future work.

Acknowledgement.We acknowledge funding from Swiss National Science Foundation (SNSF) grant number 200020_200342, from Huawei Cloud Intelligent Cloud Technologies Initiative, and from Google Research Collaborations.