# Individual Arbitrariness and Group Fairness

Carol Xuan Long\({}^{}\), Hsiang Hsu\({}^{*}\)\({}^{}\), Wael Alghamdi\({}^{*}\)\({}^{}\), Flavio P. Calmon\({}^{}\)

equal contributions. Prepared prior to employment at JPMorgan Chase & Co.. Email: hsiang.hsu@jpmchase.com.John A. Paulson School of Engineering and Applied Sciences, Harvard University, Boston, MA 02134. Emails: carol_long@g.harvard.edu, alghamdi@g.harvard.edu, flavio@seas.harvard.edu.

###### Abstract

Machine learning tasks may admit multiple competing models that achieve similar performance yet produce arbitrary outputs for individual samples--a phenomenon known as predictive multiplicity. We demonstrate that fairness interventions in machine learning optimized solely for group fairness and accuracy can exacerbate predictive multiplicity. Consequently, state-of-the-art fairness interventions can mask high predictive multiplicity behind favorable group fairness and accuracy metrics. We argue that a third axis of "arbitrariness" should be considered when deploying models to aid decision-making in applications of individual-level impact. To address this challenge, we propose an ensemble algorithm applicable to any fairness intervention that provably ensures more consistent predictions.

## 1 Introduction

Non-arbitrariness is an important facet of non-discriminatory decision-making. Substantial arbitrariness exists in the training and selection of machine learning (ML) models. By simply varying hyperparameters of the training process (e.g., random seeds in model training), we can produce models with arbitrary outputs on individual input samples [9; 13; 26; 35]. The phenomenon where distinct models exhibit similar accuracy but arbitrary individual predictions is called _predictive multiplicity_1. The arbitrary variation of outputs due to unjustified choices made during training can disparately impact individual samples, i.e., predictive multiplicity is not equally distributed across inputs of a model. When deployed in high-stakes domains (e.g., medicine, education, resume screening), the arbitrariness in the ML pipeline may target and cause systemic harm to specific individuals by excluding them from favorable outcomes [7; 15; 40].

Popular fairness metrics in the ML literature do not explicitly capture non-arbitrariness. A widely recognized notion of non-discrimination is _group fairness_. Group fairness is quantified in terms of, for example, statistical parity , equal opportunity, equalized odds , and variations such as multi-accuracy  and multi-calibration . Broadly speaking, methods that control for group fairness aim to guarantee comparable performance of a model across population groups in the data. The pursuit of group fairness has led to hundreds of fairness interventions that seek to control for performance disparities while preserving accuracy .

The central question we tackle in this paper is: Do models corrected for group fairness exhibit less arbitrariness in their outputs? We answer this question in the _negative_. We demonstrate that state-of-the-art fairness interventions may improve group fairness metrics at the expense of exacerbating arbitrariness. The harm is silent: the increase in arbitrariness is masked by favorable group fairness and accuracy metrics. Our results show that arbitrariness lies beyond the fairness-accuracy frontier:predictive multiplicity should be accounted for _in addition_ to usual group-fairness and accuracy metrics during model development.

Figure 1 illustrates how fairness interventions can increase predictive multiplicity. Here, state-of-the-art fairness interventions are applied4 to a baseline random forest classifier to ensure group fairness (mean equalized odds , see Definition 6) in a student performance binary prediction task. We produce multiple baseline classifiers by varying the random seed used to initialize the training algorithm. Each baseline classifier achieves comparable accuracy and fairness violation. They also mostly agree in their predictions: for each input sample, the standard deviation of output scores across classifiers is small (see Definition 3). After applying a fairness intervention to each randomly initialized baseline classifier, we consistently reduce group fairness violations at a small accuracy cost, as expected. However, predictive multiplicity changes significantly post-intervention: for roughly half of the students, predictions are consistent across seeds, whereas for 20% of the students, predictions are comparable to a coin flip. For the latter group, the classifier output depends on the choice of a random seed instead of any specific input feature. The increase in predictive multiplicity is masked by the fairness-accuracy curve, does not impact all samples equally, and is consistent across datasets and learning tasks.

At first, the increase in predictive multiplicity may seem counter-intuitive: adding fairness constraints to a learning task should reduce the solution space, leading to less disagreement across similarly-performing classifiers relative to an unconstrained baseline. We demonstrate that, in general, this is not the case. For a given hypothesis class, the non-convex nature of group fairness constraints can in fact _increase_ the number of feasible classifiers at a given fairness and accuracy level. We show that this phenomenon occurs even in the simple case where the hypothesis space is comprised of threshold classifiers over one-dimensional input features, and the optimal baseline classifier is unique. To address this challenge, we demonstrate - both theoretically and through experiments - that ensembling classifiers is an effective strategy to counteract this multiplicity increase.

The main contributions of this work include5:

1. We demonstrate that the usual "fairness-accuracy" curves can systematically mask an increase of predictive multiplicity. Notably, applying state-of-the-art fairness interventions can incur higher arbitrariness in the ML pipeline.
2. We show that multiplicity can be arbitrarily high even if group fairness and accuracy are controlled, when we do not have perfect classifiers. Hence, fairness interventions optimized solely for fairness and accuracy cannot, in general, control predictive multiplicity. We also provide examples of why fairness constraints may exacerbate arbitrariness.

Figure 1: Accuracy-fairness frontier does not reveal arbitrariness in competing models. **Left**: Fairness-Accuracy frontier of baseline and fair models corrected by 5 fairness interventions; point clouds generated by different random seed choices. **Middle**: The cumulative distribution functions (CDF) of per-sample score std. across classifiers at different intervention levels (see Definition 3). For each sample, std. is measured across competing scores produced by classifiers initialized with different random seeds. A _wider_ CDF indicates _more_ disparity of the impact of arbitrariness on different individuals. **Right**: The distribution of score std. relative to the thresholded baseline model. Removing samples that receive very low score std. both from thresholded baseline and fair classifiers, the largest group (blue area) in this violin plot are those individuals for which std. increases from 0 to a large positive value (median around 0.15). Hence, significant arbitrariness is introduced by the fairness intervention, in addition to and separate from the effects of thresholding the baseline.

3. We propose an ensemble algorithm that reduces multiplicity while maintaining fairness and accuracy. We derive convergence rate results to show that the probability of models disagreeing drops exponentially as more models are added to the ensemble.
4. We demonstrate the multiplicity phenomena and benchmark our ensemble method through comprehensive experiments using state-of-the-art fairness interventions across real-world datasets.

### Related Works

Multiplicity, its implications, and promises.Recent works have investigated various factors that give rise to multiplicity. D'Amour et al.  studied how under-specification presents challenges to the credibility of modern machine learning algorithms. More precisely, under-specified optimization problems in machine learning admit a plethora of models that all attain similar performance, and which model to deploy in practice may ultimately depend on arbitrary choices of the randomization made during training procedure . The arbitrariness of the model could potentially harm the reproducibility of model predictions , and hence the credibility of the conclusion made thereof.

Creel and Hellman  thoroughly explored the notion of arbitrariness in machine learning and discuss how high-multiplicity predictions can lead to systematized discrimination in society through "algorithmic leviathans." Multiplicity in prediction and classification can also have beneficial effects. Black et al. , Semenova et al. , and Fisher et al.  view multiplicity of equally-performing models as an opportunity to optimize for additional criteria such as generalizability, interpretability, and fairness. Coston et al.  develop a framework to search over the models in the Rashomon set for a better operation point on the accuracy-fairness frontier. However, they do not discuss the potential predictive multiplicity cost of existing fairness interventions nor propose algorithms to reduce this cost.

The work most similar to ours is . Cooper et al.  consider the problem of predictive multiplicity as a result of using different splits of the training data. Therein, they quantify predictive multiplicity by prediction variance, and they propose a bagging strategy  to combine models. Our work considers a different problem where predictive multiplicity is exacerbated by group-fairness interventions. Our work is also different from Cooper et al.  as we fix the dataset when training models and consider multiplicity due to randomness used during training. In this sense, our ensemble algorithm is actually a voting ensemble  (see Section 4); see also ensembling and reconciliation strategies proposed by Black et al.  and Roth et al.  that aim to create more consistent predictions among competing models. To the best of the authors' knowledge, we are the first to measure and report the arbitrariness cost of fairness interventions.

Hidden costs of randomized algorithms.Recent works  examine the potential detrimental consequences of randomization in the ML pipeline. In their empirical study, Ganesh et al. observe that group fairness metrics exhibit high variance across models at different training epochs of Stochastic Gradient Descent (SGD). The authors point out that random data reshuffling in SGD makes empirical evaluation of fairness (on a test set) unreliable, and they attribute this phenomenon to the volatility of predictions in minority groups. Importantly, they do not incorporate fairness interventions in their experiments. In contrast, we apply fairness interventions to baseline models. Specifically, we examine the variance in predictions among models with similar fairness and accuracy performances. In addition to the observations made by Ganesh et al., our theoretically-grounded study reveals the different paths that lead to group-fairness, i.e., that arbitrariness can be an unwanted byproduct of imposing fairness constraints. Kroo et al.  empirically study if fairness interventions reduce bias equally across groups, and examine whether affected groups overlap across different fairness interventions. In contrast, our work examines the _multiplicity cost_ of group fairness and its tension with individual-level prediction consistency, rather than the _fairness cost_ of randomness in the ML pipeline. Another work on the hidden cost of randomized algorithms is given by Kulynych et al. , who report that well-known differentially-private training mechanisms can exacerbate predictive multiplicity.

In an early work , Lipton et al. indirectly points to the potential arbitrary decision on individuals as a result of imposing group fairness constraints. They give an illustrative example using synthetic hiring data to show that a fair model resorts to using irrelevant attribute (hair length) to make hiring decision in order to achieve near-equal hiring rate for men and women.

Problem Formulation and Related Works

We explain the setup and relevant definitions in this section.

Prediction tasks.We consider a binary classification setting with training examples being triplets \((,S,Y)\) with joint distribution \(P_{,S,Y}\). Here, \(\) is an \(^{d}\)-valued feature vector, \(S\) is a discrete random variable supported on \([K]\{1,,K\}\) representing \(K\) (potentially overlapping) group memberships, and \(Y\) is a binary (i.e., \(\{0,1\}\)-valued) random variable denoting class membership.6 We consider probabilistic classifiers in a hypothesis space \(\), where each \(h\) is a mapping \(h:^{d}\). Each value of a classifier \(h()\) aims to approximate \(P_{Y|=}(1)\). The predicted labels \(\) can be obtained by thresholding the scores, e.g., \(=\{h() 0.5\}\), where \(\{\,\,\}\) is the indicator function. Finally, we denote by \(_{c}\) the probability simplex over \(c\) dimensions.

Randomized training procedures and the Rashomon set.We assume access to the following:

1. a training dataset of \(n\) i.i.d samples \(\{(_{i},s_{i},y_{i})\}_{i=1}^{n}\) drawn from \(P_{,S,Y}\);
2. a randomized training procedure \(\); and
3. an induced distribution \(()\) on the hypothesis class of predictors \(\).

We denote a sampled classifier by \(h()\), which can be sampled, for example, using different random seeds at the beginning of the execution of procedure \(\) on \(\). For concreteness, the above data may for example correspond to the following practical setting.

**Example 1**.: The dataset \(\) can be comprised of resumes of individuals applying for a job, and the training procedure \(\) is an algorithm to predict whether to extend an interview opportunity for an applicant. For example, \(\) can be a neural network with unspecified hyperparameters (e.g., random seed that needs to be chosen at the outset); alternatively, \(\) can be the same pre-trained neural network composed with a fairness intervention method. The classifiers considered will be the last layer of the neural network (or the classifier after fairness enhancement), which will belong to a hypothesis class \(\) determined by the chosen neural network architecture. By varying the random seed, say, \(m\) times, we would obtain _independent_ classifiers, denoted by \(h_{1},,h_{m}}{{}}( )\). \(\)

We are interested in detecting whether competing classifiers (i.e., deployed for, and performing similarly in the same prediction task) have conflicting predictions non-uniformly across individuals. Next, we define the set of competing models obtained from the randomized training procedure \(\).

For a loss function \(:\{0,1\}^{+}\), finite dataset \(^{d}[K]\{0,1\}\), and classifier \(h:^{d}\), we let the empirical loss incurred by \(h\) on \(\) be denoted by \((h;)||^{-1}_{(,s,y)}(h(),y)\). The (_empirical_) _Rashomon set_ (\(\)-level set) of competing models is defined as the set of models with loss lower than \(\), i.e., \((,,)\{h\ :\ (h; )\}\). We extend the definition of the Rashomon set to take into consideration the effect of the randomized algorithm \(\), as follows.

**Definition 1** (Empirical Rashomon Set of Randomized Training Procedure).: Fix a finite dataset \(\), a hypothesis class \(\), and a randomized training procedure \(\) inducing the distribution \(()\) on \(\). Given a loss function \(:\{0,1\}^{+}\) and a parameter \(>0\), we define the _empirical Rashomon set with \(m\) models induced by \(\)_ as the collection of \(m\) classifiers independently sampled from \(()\) and having empirical loss less than \(\):

\[}_{m}(,,)\{h_{ 1},,h_{m}\ :\ h_{1},,h_{m}}{{}}( )(h_{j};)\  j[m]\}.\] (1)

Here, \(\) is an approximation parameter that determines the size of the set. The set \(}_{m}(,,)\) can be viewed as an approximation of the _Rashomon set_ of "good" models , and indeed we have the inclusion \(}_{m}(,,)( ,,)\) where \(=(())\). Note that the set \(}_{m}(,,)\) is itself _random_ even for a fixed dataset \(\), where the source of randomness is coming from the distribution \(()\). In the sequel, we omit the arguments of \(}_{m}(,,)\) when they are clearly implied from context.

There are various metrics to quantify predictive multiplicity across models in \(}_{m}\) by either considering their output scores  or thresholded predictions . e focus on two metrics: 1) _ambiguity_ for evaluating the predictive multiplicity of thresholded predictions, and 2) _cumulative distribution function (CDF) of standard deviation (std.) of output scores_ when model outputs are in the interval \(\) (interpreted as the probability of the positive class). Those two metrics are defined as follows.

**Definition 2** (Ambiguity ).: Fix a dataset \(=\{(_{i},s_{i},y_{i})\}_{i[n]}^{d}[K ]\{0,1\}\) and a finite set of models \(\). Let \(f(r)\{r 0.5\}\) be the thresholding function. The _ambiguity_ of a dataset over the set of models \(\) is the proportion of points in the dataset that can be assigned a conflicting prediction by a competing classifier within \(\):

\[(,)|} _{i[n]}\;_{h,h^{}}\{f(h( _{i})) f(h^{}(_{i}))\}.\] (2)

To define the CDF of std. of scores, we first delineate what we mean by empirical std. of scores.

**Definition 3** (Std. of Scores).: Fix a finite set of models \(=\{h_{j}\}_{j[m]}\). The empirical standard deviation (std.) of scores for a sample \(^{d}\) relative to \(\) is defined by

\[s(,)_{j[m]}(h_{j}( )-_{})^{2}},\] (3)

where \(_{}_{j[m]}h_{j}()\) denotes the empirical mean (over \(\)) of the scores.

Further, to understand the std. of scores of the population, we consider the empirical cumulative distribution function of the std. of the scores, defined as follows.

**Definition 4** (Quantiles of std. of Scores).: Fix a dataset \(=\{(_{i},s_{i},y_{i})\}_{i[n]}^{d}[K ]\{0,1\}\) and a finite set of models \(\). We define the empirical cumulative distribution function of the std. of the scores by (where \(s(,)\) is the empirical std. as in Definition 3)

\[_{,}(t)|} _{i[n]}\{s(_{i},) t\}.\] (4)

**Example 2**.: Consider a resume screening task where the algorithm decides whether to extend an interview opportunity. If \(_{,}(0.5)=90\%\), then for 10% of the individuals in the dataset, the predictions produced by the competing models are arbitrary and conflicting: regardless of the mean scores, with an std. of at least 0.5, there would exist models with scores falling above and below the one-half threshold, so the thresholded output can be both 0 (no interview) and 1 (offer interview). \(\)

A note on related metrics.An alternative measurement of score variation is _Viable Prediction Range_ as defined in , which measures the difference in max and min scores among competing models on each individual. For thresholded scores, the original definition of _Ambiguity_ considers the proportion of a flip in prediction with respect to a baseline model (from empirical risk minimization with fixed hyperparameters and randomness). Since we consider randomized training procedures with no clear baseline model, the definition for ambiguity above is a variation of the original.

Group fairness.We consider three group fairness definitions for classification tasks--statistical parity (SP), equalized odds (EO), and overall accuracy equality (OAE) . OAE and Mean Equalized Odds (MEO) are defined below as they are used in the next sections, and we refer the reader to Appendix D for the remaining definitions.

**Definition 5** (Overall Accuracy Equality, OAE).: Let \(\) be the predicted label obtained, e.g., from thresholding the scores of a classifier \(h:^{d}\). The predictor \(\) satisfies overall accuracy equality (OAE) if its accuracy is independent of the group attribute: for all groups \(s,s^{}[K]\),

\[(=Y S=s)=(=Y S=s^{}).\] (5)

For binary classification, SP boils down to requiring the average predictions to be equal across groups, while EO requires true positive rates (TPR) and false positive rates (FPR) to be calibrated. In this paper, we consider mean equalized odds (MEO): the average of absolute difference in FPR and TPR for binary groups \(S\{0,1\}\). We consider binary group since this is the setup for most fairness intervention methods.

**Definition 6** (Mean Equalized Odds, MEO [4; 23]).: Let \(\) be the predicted label, \(S\{0,1\}\) denotes binary group membership. Mean Equalized Odds is the average odds difference for binary groups:

\[(|_{S=0}-_{S=1}|+| _{S=0}-_{S=1}|),\] (6)

where \(_{S=s}(=1 Y=1,S=s)\) and \(_{S=s}(=1 Y=0,S=s)\).

To examine whether current fairness intervention methods lead to an exacerbation of multiplicity, we survey state-of-the-art intervention methods, including Reductions , Fair Projection , Reject Options , and EqQdds . We offer a brief discussion of their mechanism in Appendix D.

## 3 Orthogonality of Fairness and Arbitrariness

We discuss next why arbitrariness is a third axis not captured by fairness and accuracy. Models with similar fairness and accuracy metrics can differ significantly in predictions. Moreover, a set of fair and approximately accurate models can attain maximal predictive multiplicity. We also explore through an example one fundamental reason why adding fairness constraints can lead to more arbitrariness.

**Example 3** (Ambiguity \(\) OAE).: Overall Accuracy Equality (OAE, Definition 5) does not capture the ambiguity of model outputs (Definition 2). Consider two hypothetical models that are fair/unfair and exhibit high/low predictive multiplicity in Figure 2. Here, in each panel, the rectangle represents the input feature space and the shaded regions represent the error region of each model.

In the top left panel, both Model 1 and 2 have equal accuracy for both groups, since the proportion of the error regions (red stripes and pink shade) for both groups are the same. Hence, both models are considered group-fair in terms of OAE. However, the error regions of the two models are disjoint. Since ambiguity is measured by the percentage of the samples that receive conflicting predictions from either models, samples from the union of the two error regions contribute to ambiguity. Hence, Model 1 and 2 bear high predictive multiplicity despite being group-fair.

In the lower right panel, Model 1 and 2 attain low fairness and low predictive multiplicity. Both models have higher accuracy for Group 2 than Group 1, so they are both unfair. The error regions completely overlap, which means that the two models are making the same error--ambiguity is \(0\). \(\)

The schematic diagram in Figure 2 shows that predictive multiplicity is not captured by OAE. Indeed, ambiguity of a collection of models is a _global_ property (i.e., verified at the collection level), whereas OAE is a _local_ property (i.e., verified at the classifier level). Hence, one should not _a priori_ expect that a set of competing models each satisfying OAE would necessarily comprise a Rashomon set with favorable ambiguity.

We prove the orthogonality of OAE and Statistical Parity (SP) from ambiguity formally in the proposition below, where we show that it is possible to construct classifiers with very stringent accuracy and perfect fairness constraint, albeit with maximal ambiguity. We determine the Rashomon set using the 0-1 loss:

\[_{0 1}(h;)=|}_{(_{i},s_{i}, y_{i})}\{_{i} y_{i}\},\]

where \(_{i}\{0,1\}\) is the class membership of \(_{i}\) predicted by \(h\). We prove the following orthogonality in Appendix A.

**Proposition 3.1** (Orthogonality of OAE/SP and Ambiguity).: _Fix any empirical loss value \(0<\) and any number of models \(m>\). Then, for some finite dataset \(^{d}[K]\{0,1\}\), there is a realization of the empirical Rashomon set \(}_{m}=\{h_{j}\}_{j[m]}\) satisfying the following simultaneously:_

Figure 2: Illustration on two models being fair/unfair and exhibit high/low predictive multiplicity through the models’ error regions in each of the 4 cases. The metrics for fairness and predictive multiplicity are Overall Accuracy Equality (Definition 5) and ambiguity (Definition 2), respectively.

1. _Each_ \(h_{j}\) _has 0-1 loss upper bounded by_ \(_{}(h_{j};)\)_;_
2. _Each_ \(h_{j}\) _satisfies OAE perfectly, or each_ \(h_{j}\) _satisfies SP perfectly;_
3. _The collection_ \(}_{m}\) _has the worst ambiguity, i.e.,_ \((,}_{m})=100\%\)_._

**Remark 1**.: For OAE, such Rashomon set \(}_{m}\) exists for _any_ dataset \(\) satisfying the two conditions:

1. with \(n_{k}\) denoted the number of samples in \(\) belonging to group \(k[K]\), the greatest common divisor of the \(n_{k}\) is at least \((m-1)/(m-1)\).
2. if \((,s,y),(,s^{},y^{})\) share the same feature vector \(\), then \(y=y^{}\) too.

The requirement that the \(n_{k}\) share a large enough common divisor is used in the proof to guarantee _perfect_ OAE. One could relax this requirement at the cost of nonzero OAE violation.

Proposition 3.1 implies that there exists a dataset and competing models for which all samples receive conflicting predictions. Specifically, we can construct a large enough set of competing models (\(m>\)) such that 100% of the samples in the dataset can receive conflicting predictions from this set of perfectly fair models with respect to OAE.

In the next example, we demonstrate that, counter-intuitively, adding a fairness constraint can enlarge the set of optimal models, thereby increasing predictive multiplicity. This points to a fundamental reason why adding fairness constraints can lead to more arbitrariness in model decisions.

**Example 4** (Arbitrariness of Threshold Classifiers with Fairness Constraint).: Given a data distribution of a population with two groups (Figure 3**Left**), consider the task of selecting a threshold classifier that predicts the true label. Without fairness considerations, the optimal threshold is 0 - i.e., assigning positive predictions to samples with \(>0\) and negative predictions to \( 0\) minimizes the probability of error (Figure 3**Right**). This optimal model is unique. Adding a fairness constraint that requires Mean EO \( 0.1\), the previously optimal classifier at 0 (with Mean EO \(=0.15\), **Right**) does not meet the fairness criteria. Searching over the threshold classifiers that minimize the probability of error while satisfying Mean EO constraint yields two equally optimal models (red and blue dots **Right**) with distinct decision regions (red and blue arrows **Left**). Even in this simple hypothesis class, the addition of fairness constraints yields multiple models with indistinguishable fairness and accuracy but with distinct decision regions. The arbitrary selection between these points can lead to arbitrary outputs to points near the boundary. \(\)

## 4 Ensemble Algorithm for Arbitrariness Reduction

To tackle the potential arbitrariness cost of fairness intervention algorithms, we present a disparity-reduction mechanism through ensembling. We provide theoretical guarantees and numerical benchmarks to demonstrate that this method significantly reduces the predictive multiplicity of fair and accurate models.

In a nutshell, given competing models \(h_{1},,h_{m}\), we argue that the disparity in their score assignment can be reduced by considering a convex combination of them, defined as follows.

Figure 3: Data distribution of a population with two groups used in Example 2 (**Left**). In **Right**, without the Mean EO constraint (6) (green line), there is a unique optimal classifier (with threshold 0) that attains the smallest probability of error (blue line). Adding the Mean EO constraint enlarges the set of optimal threshold classifiers to two classifiers (red and blue dots) with indistinguishable accuracy and fairness levels (**Right**) but different decision regions. We illustrate the decision regions of each classifier as red and blue arrows on the **Left**.

**Definition 7** (Ensemble Classifier).: Given \(m\) classifiers \(\{h_{1},,h_{m}:^{d}\}\) and a vector \(_{m}\), we define the _\(\)-ensembling_ of the \(h_{j}\) to be the convex combination \(^{,}_{j[m]}_{j}h_{j}\).

### Concentration of Ensembled Scores

We prove in the following result that _any_ two different ensembling methods agree for fixed individuals with high probability. Recall that we fix a dataset \(\) and a set of competing models \(()\) coming from a stochastic training algorithm \(\) (see Section 2). All proofs are provided in Appendix B.

**Theorem 4.1** (Concentration of Ensembles' Scores).: _Let \(h_{1},,h_{m};_{1},,_{m}}{{}}()\) be \(2m\) models drawn from \(()\), and \(^{,},}^{,}\) be the ensembled models (constructed with \(\{h_{1},,h_{m}\}\) and \(\{_{1},,_{m}\}\) respectively) for \(,_{m}\) (see Definition 7) satisfying \(\|\|_{2}^{2},\|\|_{2}^{2} c/m\) for an absolute constant \(c\). For every \(^{d}\) and \( 0\), we have the exponentially-decaying (in \(m\)) bound_

\[(|^{,}()-}^{,}()|) 4e^{-^{2}m/ (2c)}.\] (7)

_In particular, for any validation set \(_{}^{d}\) of size \(|_{}|=n\), we have the uniform bound_

\[(|^{,}()-}^{,}()|<\,_{})>1-4ne^{-^{2}m/(2c)}.\] (8)

### Concentration of Predictions Under Ensembling

The above theorem implies that we can have a dataset of size that is exponential in the number of accessible competing models and still obtain similar scoring for _any_ two ensembled models (uniformly across the dataset).

In practice, one cares more about the agreement of the final prediction of the classifiers. The following result extends Theorem 4.1 to the concentration of thresholded classifiers. For this, we need to define the notion of _confident classifiers_.

**Definition 8** (Confident Classifier).: Fix a probability measure \(P_{}\) over \(^{d}\) and constants \(\), \(\). We say that a classifier \(h:^{d}\) is _\((P_{},,)\)-confident_ if \((|h()-|<)<\).

In other words, \(h\) is a confident classifier if it is "more sure" of its predictions. We observe in experiments that models corrected by fairness interventions have scores concentrated around 0 and 1.

Using confident classifiers, we are able to extend Theorem 4.1 to thresholded ensembles, as follows.

**Theorem 4.2**.: _Let \(^{,},}^{,}\) be as in Theorem 4.1, and assume that both ensembled classifiers are \((P_{},,)\)-confident in the sense of Definition 8. Let \(f(t)\{t 0.5\}\) be the thresholding function. For any set \(_{}^{d}\) of size \(|_{}|=n\), we may guarantee the probability of agreement in the predictions for all samples under the two ensembles to be at least_

\[(f(^{,}())=f(}^{,}())\,_{}) 1-(4e^{-2 ^{2}m/c}+2)n.\] (9)

We note that in the fairness-intervention setting, the set \(_{}\) in the above theorem would be chosen as the subset of samples having the same group attribute. Thus, the size \(n_{0}\) of \(_{}\) would be significantly smaller than the total size of the dataset, and the parameter \(\) then can be required to be moderately small.

**Remark 2**.: In Appendix C, we discuss how to optimize the ensembling parameters \(\). In the next section, we will stick to the uniform ensembling: \(^{,}=_{j[m]}h_{j}\), i.e., \(=\). This simple uniform ensemble suffices to illustrate the main goal of this paper: that arbitrariness can be a by-product of fairness intervention methods, and ensembling can mitigate this unwanted effect.

## 5 Experimental Results

We present empirical results to show that arbitrariness is masked by favorable group-fairness and accuracy metrics for multiple fairness intervention methods, baseline models, and datasets 7. We also demonstrate the effectiveness of the ensemble in reducing the predictive multiplicity of fair models.

Setup and Metrics.We consider three baseline classifiers (Base): random forest (RF), gradient boosting (GBM), and logistic regression (LR), implemented by Scikit-learn . By varying the random seed, we obtain 10 baseline models with comparable performance. Then, we apply various state-of-the-art fairness methods (details in Appendix D) on the baseline models to get competing fair models.

On the test set, we compute mean accuracy, Mean EO (Definition 6), and predictive multiplicity levels on competing models before and after fairness interventions. We use ambiguity (Definition 2) and score standard deviations (Definition 3) as metrics for predictive multiplicity.

Datasets.We report predictive multiplicity and benchmark the ensemble method on three datasets - two datasets in the education domain: the high-school longitudinal study (HSLS) dataset  and the ENEM dataset  (see Alghamdi et al.  Appendix B.1), and the UCI Adult dataset which is based on the US census income data. The ENEM dataset contains Brazilian college entrance exam scores along with student demographic information and socio-economic questionnaire answers (e.g. if they own a computer). After pre-processing, the dataset contains 1.4 million samples with 139 features. Race is used as the group attribute \(S\), and Humanities exam score is used as the label \(Y\). Scores are quantized into two classes for binary classification. The race feature \(S\) is binarized into White and Asian (\(S=1\)) and others (\(S=0\)). The experiments are run with a smaller version of the dataset with 50k samples. Complete experimental results can be found in Appendix E.

Results that Reveal Arbitrariness.We juxtapose the fairness-accuracy frontier and metrics for predictive ambiguity to reveal arbitrariness masked by favorable group-fairness and accuracy metrics in Figure 1 and 4. Starting with 10 baseline classifiers by varying the random seed used to initialize the training algorithm, we apply the fair interventions Reduction , Rejection, Leveraging  to obtain point clouds of models with comparable fairness and accuracy metrics. In Figure 4, we take models that achieve very favorable accuracy and MEO metrics (in blue rectangle in **Left**) and plot the std. of scores to illustrate predictive multiplicity **Right**. Group fairness violations are greatly reduced (from 0.28 in baseline to 0.04 in fair models) at a small accuracy cost (from 67% in baseline to 66% in fair models). However, there is higher arbitrariness.

Compared to baseline (red curve), fair models corrected by Reduction and ROC produce lower score arbitrariness for the bottom 50% but much higher arbitrariness for the top 50% of samples; importantly, the induced arbitrariness becomes _highly nonuniform across different individuals_ after applying the two fairness intervention. We observe that Leveraging produce models that agree on 90% of the samples, thereby not inducing concerns of arbitrariness.

Remarkably, arbitrariness does not vary significantly among models with different fairness levels. We consider two sets of models trained with high and low fairness constraints using Reduction in Figure 1.

Results on the Effectiveness of Ensembling.We pair our proofs in Section 4 with experiments that demonstrate the concentration of scores of ensembled models. In Figure 5 **Left**, taking the competing

Figure 4: Quantile plot on high-fairness bin for various fairness interventions v.s. baseline on ENEM. **Left**: Fairness-Accuracy frontier. **Right**: Fair models produce larger score std. at top percentiles compared to the baseline model (horizontal axis computed via (6)). (Rejection and Leveraging output thresholded scores directly.)

models in the high-fairness bins corrected with Reduction that achieve an Mean EO violation of \(0.04\) but very high score std. for half of the samples (blue rectangle in Figure 4), we ensemble the models with increasing number of models per ensemble (\(m\)) ranging from 1 to 30. For each \(m\), we measure std. of scores in 10 such ensembles. The top percentile std. of the ensembled fair models drops to baseline with 30 models. Similar convergence occur on the HLS dataset. Importantly, the ensembled models are still fair, the Mean EO violations of the ensembled models remain low.

## 6 Final Remarks

We demonstrate in this paper that arbitrariness is a facet of responsible machine learning that is orthogonal to existing fairness-accuracy analyses. Specifically, fairness-vs-accuracy frontiers are insufficient for detecting arbitrariness in the predictions of group-fair models: two models can have the same fairness-accuracy curve while at the same time giving widely different predictions for subsets of individuals. We demonstrate this undesirable phenomenon both theoretically and experimentally on state-of-the-art fairness intervention methods. Furthermore, towards mitigating this arbitrariness issue, we propose an ensemble algorithm, where a convex combination of several competing models is used for decision-making instead of any of the constituent models. We prove that the scores of the ensemble classifier concentrate, and that the ensuing predictions can be made to concentrate under mild assumptions. Importantly, we exhibit via real-world experiments that our proposed ensemble algorithm can reduce arbitrariness while maintaining fairness and accuracy.

Limitations.The proposed framework for estimating the predictive multiplicity of fairness interventions requires re-training multiple times, limiting its applicability to large models. We consider model variation due to randomness used during training. In practice, competing models may exist due to inherent uncertainty (i.e., a non-zero confidence interval) when evaluating model performance on a finite test set. In this regard, models with comparable average performance can be produced by searching over this Rashomon set even if training is deterministic (e.g., equivalent to solving a convex optimization problem).

Future directions.An interesting future direction is to explore the multiplicity cost of fairness interventions in such deterministic settings. Furthermore, our ensembling strategy may not guarantee that the ensemble classifier ensures fairness constraints due to the non-convex nature of such constraints. Though we empirically observe that fairness constraints are indeed satisfied by the ensemble model, proving such guarantees theoretically would be valuable.

Societal impacts.While fairness intervention algorithms can effectively reduce the disparate impact among population groups, they can induce predictive multiplicity in individual samples. The increase in predictive multiplicity does not impact all individuals equally. If predictive multiplicity caused by fairness interventions is not accounted for, some individuals will bear the brunt of arbitrary decision-making--their predictions could be arbitrary upon re-training the classifier using different random initializations, leading to another level of disparate treatment to certain population groups.

Figure 5: Standard deviation of ensembled models trained on ENEM and HLS with baseline random forest classifiers. We fix the high-fairness bin and vary the number of models \(m\) in each ensemble. As we increase the number of ensembles, score std. (on 10 ensembles) drops and meets the score std. of 10 baseline RFC when \(m=30\) on ENEM and \(m=17\) on HLS. (Mean EO is computed using (6).

Acknowledgements.The authors would like to thank Jamelle Watson-Daniels, Arpita Biswas and Bogdan Kulynych for hepful discussions on initial ideas. This material is based upon work supported by the National Science Foundation under grants CAREER 1845852, CIF 1900750, CIF 2312667 and FAI 2040880, and by Meta Ph.D. fellowship.

Disclaimer.This paper was prepared by Hsiang Hsu prior to his employment at JPMorgan Chase & Co.. Therefore, this paper is not a product of the Research Department of JPMorgan Chase & Co. or its affiliates. Neither JPMorgan Chase & Co. nor any of its affiliates makes any explicit or implied representation or warranty and none of them accept any liability in connection with this paper, including, without limitation, with respect to the completeness, accuracy, or reliability of the information contained herein and the potential legal, compliance, tax, or accounting effects thereof. This document is not intended as investment research or investment advice, or as a recommendation, offer, or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction.