# Semi-supervised Multi-label Learning with Balanced Binary Angular Margin Loss

Ximing Li\({}^{1,2}\)   Silong Liang\({}^{1,2}\)   Changchun Li\({}^{1,2,}\)1   Pengfei Wang\({}^{3,4}\)   Fangming Gu\({}^{1,2}\)

\({}^{1}\)College of Computer Science and Technology, Jilin University, China

\({}^{2}\)Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, China

\({}^{3}\)Computer Network Information Center, Chinese Academy of Sciences, China

\({}^{4}\)University of Chinese Academy of Sciences, Chinese Academy of Sciences, China

liximing86@gmail.com, changchunli93@gmail.com, liangsl23@mails.jlu.edu.cn, pfwang@cnic.cn, gufm@jlu.edu.cn

###### Abstract

Semi-supervised multi-label learning (SSMLL) refers to inducing classifiers using a small number of samples with multiple labels and many unlabeled samples. The prevalent solution of SSMLL involves forming pseudo-labels for unlabeled samples and inducing classifiers using both labeled and pseudo-labeled samples in a self-training manner. Unfortunately, with the commonly used binary type of loss and negative sampling, we have empirically found that learning with labeled and pseudo-labeled samples can result in the variance bias problem between the feature distributions of positive and negative samples for each label. To alleviate this problem, we aim to balance the variance bias between positive and negative samples from the perspective of the feature angle distribution for each label. Specifically, we extend the traditional binary angular margin loss to a balanced extension with feature angle distribution transformations under the Gaussian assumption, where the distributions are iteratively updated during classifier training. We also suggest an efficient prototype-based negative sampling method to maintain high-quality negative samples for each label. With this insight, we propose a novel SSMLL method, namely **S**emi-**S**upervised **M**ulti-**L**abel **L**earning with **B**alanced **B**inary **A**ngular **M**argin loss (**S\({}^{2}\)ML\({}^{2}\)-b****b****a**m). To evaluate the effectiveness of S\({}^{2}\)ML\({}^{2}\)-b****b****a**m, we compare it with existing competitors on benchmark datasets. The experimental results validate that S\({}^{2}\)ML\({}^{2}\)-b****a**m can achieve very competitive performance.

## 1 Introduction

Multi-label learning (MLL) refers to the classification problem where each training sample can be associated with multiple labels . For example, in text categorization, a text can involve a certain number of topics simultaneously [2; 3]; and in image annotation, an image can contain multiple objects of interest in one scene [4; 5]. Compared with single-label learning, MLL is a more prevalent paradigm in real-world scenarios, and it has been widely used in many applications such as information retrieval [6; 7] and recommendation systems [8; 9].

Despite the successful application of MLL, the competitive performance of most MLL methods heavily depends on the large volume of training samples with precise supervision . Unfortunately, it is expensive to manually annotate each sample, so it is naturally time-consuming to collect loads of labeled training samples. Accordingly, the community has turned to alternative candidates to MLL, and raised the question of whether one can induce robust MLL classifiers with a small number of labeled samples and a large number of unlabeled samples, which are cheaper to collect. This concept gives birth to the emerging research topic of semi-supervised **m**ulti-**l**abel **I**earning (**SSMLL**), and many attempts have been recently proposed .

Generally, the topic of SSMLL, as its name suggests, is in parallel inherited from semi-supervised learning (SSL) and MLL. The current prevalent ideas are estimating pseudo-labels of unlabeled samples with SSL techniques and inducing MLL classifiers with both labeled and pseudo-labeled samples in a self-training manner. Following the prior arts , the binary kind of losses, _e.g._ binary cross-entropy loss and asymmetric loss , are commonly used to optimize MLL classifiers, where those are equivalent to optimizing the binary loss between the positive and negative samples for each label. To alleviate the imbalanced issue between positive and negative samples, especially for the scenarios with massive labels, the negative sampling tricks are often employed . Unfortunately, in our preliminary experiments, we found such training paradigms suffer from the **variance bias** problem by using the labeled and pseudo-labeled samples in the context of SSMLL, since it is difficult to guarantee estimating accurate pseudo-labels. To be specific, the problem implies that for each label, in SSMLL the variance difference between feature distributions of positive and negative samples is often larger than the ones in fully supervised learning, as illustrated in Fig.1. In this situation, each trained binary boundary tends to keep away from the Bayesian optimal one, resulting in performance degradation.

To tackle this problem, we propose a novel SSMLL method, namely **S**emi-**S**upervised **M**ulti-**L**abel **L**earning with **B**alanced **B**inary **A**ngular **M**argin loss (**S\({}^{2}\)ML\({}^{2}\)-****b****b****b****b****am**). The basic insight of S\({}^{2}\)ML\({}^{2}\)-b****b****b****b****b****am is to balance the variance bias between positive and negative samples from the perspective of the feature angle distribution for each label. To be specific, we extend the binary angular margin (BAM) loss, which measures the prediction loss by using the angle between the feature and binary boundary for each label. We suppose that for each label these feature angles of positive and negative samples are drawn from label-specific "positive" and "negative" Gaussian distributions, which are estimated by employing both labeled and pseudo-labeled samples during classifier training. Therefore, we can apply some linear Gaussian transformations over these feature angle distributions, so as to balance the variance bias between positive and negative samples for each label. Upon this idea, we design a new balanced binary angular margin (BBAM) loss and construct a novel S\({}^{2}\)ML\({}^{2}\)-b****b****b****am method based on the designed BBAM loss and self-training manner. We also suggest an efficient prototype-based negative sampling method to maintain high-quality negative samples for each label. We evaluate the proposed S\({}^{2}\)ML\({}^{2}\)-b****b****b****am by comparing the most recent competitors on benchmark datasets. Experimental results indicate the superior performance of S\({}^{2}\)ML\({}^{2}\)-b****b****b****am.

In summary, the main contributions of this paper are listed as follows:

* We develop a novel SSMLL method, namely S\({}^{2}\)ML\({}^{2}\)-b****b****b****am, by balancing the variance bias between positive and negative samples from the perspective of the feature angle distribution for each label.

Figure 1: The variance difference between feature distributions (VDFD) of positive and negative samples computed in semi-supervised and supervised manners across labels \(\{6,7,14,17\}\) of _VOC2012_.

* We design a new BBAM loss by extending the traditional binary angular margin loss with feature angle distribution transformations under the Gaussian assumption, and suggest an efficient prototype-based negative sampling method to maintain high-quality negative samples for each label.
* We construct extensive experiments to evaluate S\({}^{2}\)ml\({}^{2}\)-bbam, and experimental results demonstrate the effectiveness of S\({}^{2}\)ml\({}^{2}\)-bbam.

## 2 Formulation and Analysis

### Problem Formulation

By convention, we use \(\) to denote the sample feature vector and \(\{0,1\}^{K}\) the label indicator vector of \(K\) pre-defined classes, where \(0/1\) implies a sample is irrelevant/relevant to the category. In the task of SSMLL, we are formally given a collection of training samples \(=\{_{l},_{u}\}\), where \(_{l}=\{(_{i}^{l},_{i}^{l})\}_{i=1}^{i=N_{l}}\) and \(_{u}=\{_{j}^{u}\}_{j=1}^{j=N_{u}}\) are the collections of \(N_{l}\) labeled and \(N_{u}\) unlabeled samples, respectively. The goal of SSMLL is to induce a classifier \(f_{}()\), parameterized by \(\), from \(\) and use the classifier \(f_{}()\) to predict the label indicator vectors for future samples.

Broadly speaking, the classifier \(f_{}()\) typically consists of a backbone encoder and a classification layer, parameterized by \(^{e}\) and \(^{c}\), respectively (_i.e._\(=\{^{e},^{c}\}\)). Specifically, the backbone encoder transforms any original feature vector \(\) into a more discriminative latent feature \(=f_{^{e}}()\); the classification layer applies \(\) to generate its corresponding predictive logits \(=f_{^{e}}()\). Given an SSMLL training dataset \(\), the classifier \(f_{}()\) is commonly optimized by minimizing the following generic self-training objective concerning \(\) on \(B_{l}\)-sized labeled and \(B_{u}\)-sized unlabeled batches:

\[()=K}_{i=1}^{B_{l}}_{k=1}^{K}(p_ {ik}^{l},y_{ik}^{l})+K}_{i=1}^{B_{u}}_{k=1}^{K} (p_{ik}^{u},y_{ik}^{u}), \]

where \((,)\) is a binary loss function; \(\) is the coefficient parameter; \(_{i}^{l}=f_{}(_{i}^{l})\) and \(_{i}^{u}=f_{}(_{i}^{u})\) are the predictive logits of labeled and unlabeled samples, respectively; \(_{i}^{u}\) is the pseudo-label of unlabeled samples induced from its current classifier prediction \(_{i}^{u}\).

### How Variance Bias Affects the Performance

As shown in Fig.1, we have observed that the generic self-training objective of SSMLL may suffer from the variance bias problem. Here, we discuss how it will affect the classification performance. We treat SSMLL as \(K\) independent semi-supervised binary classification (SSBC) tasks. For each SSBC task, let \(\{(_{i},y_{i}^{*})\}\{_{i}\}\) be the training data, where \(^{d}\) and \(y^{*}\{-1,+1\}\) is the ground-truth label. Besides, let \(\{-1,+1\}\) be the pseudo-label. For clarity and conciseness, we study the SSBC training data drawn from a mixture Gaussian distribution \(^{*}\), which can be defined by the following distribution over \((,y)^{d}\{ 1\}\):

\[y=+1,\ p=,\\ -1,\ p=1-,( ,_{+}^{2})&y=+1;\\ (-,_{-}^{2})&y=-1, \]

where \(\) is the prior probability of class "+1", \(=\{_{1},,_{d}\}^{}\), \(_{+}=(\{_{+}^{(1)},,_{+}^{(d)}\})\), \(_{-}=(\{_{-}^{(1)},,_{-}^{(d)}\})\), \(_{i},_{-}^{(i)},_{+}^{(i)}>0\  i[d]\), and \(_{i=1}^{d}(_{+}^{(i)})^{2}:_{i=1}^{d}(_{-}^{(i)})^{2}=1:M^{2}\) with \(M>0,M 1\). We concentrate on analyzing the effect of the variance proportion \(M\) of the distribution \(^{*}\) on the performance of the linear model \(f_{ssl}()=(,+b)\), where the parameters \(^{d},b\), and \((t)\) evaluates to \(+1\) if scalar \(t 0\) and to \(-1\) otherwise. For simplicity, we denote

\[(f,+1)=_{(,y)^{*}}[(f( )=-1)|y=+1],\ (f,-1)=_{(,y)^{*}}[ (f()=+1)|y=-1],\]

where \((t)\) is the indicator function that takes 1 where t is true and 0 otherwise. We have the following theorems, whose proof can be found in the Appendix A.

**Theorem 2.1**.: _Given an SSBC dataset with pseudo-labels \(=\{(_{i},y_{i})\}=\{(_{i},y_{i}^{*})\}\{( _{i},_{i})\}\), the optimal linear classifier \(f_{ssl}\) minimizing the average standard classification error is given by:_\[f_{ssl}=*{arg\,min}_{f}_{(,y)}[ (f() y)]. \]

_When \(M>1\), it has the intra-class standard classification errors for the two classes :_

\[(f_{ssl},+1) =A-M+q(M,,_{-},_{+}) },\] \[(f_{ssl},-1) =-M A++q(M,,_{-}, _{+})},\]

_and when \(M<1\), they are given by:_

\[(f_{ssl},+1) =A+M+q(M,,_{-},_{+}) },\] \[(f_{ssl},-1) =-M A-+q(M,,_{-}, _{+})},\]

_where \(()\) is the cumulative distribution function (c.d.f.) of standard Gaussian distribution \((0,1)\), \(A=-1)^{}}, q(M,,_{-},_{+} )=-1}\), \(C=-2_{+})}{(1-)(2-2 _{-}-_{+})}\), \(=_{i=1}^{i=d}_{i}\), \(=^{i=d}(_{+}^{(i)})^{2}}\), and \(\{_{-},_{+}\}\) are the proportions of negative instances being treated as positive ones and positive instances being treated as negative ones within pseudo-labels, respectively. If \(_{i=1}^{d}(_{+}^{(i)})^{2}=_{i=1}^{d}(_{-}^{(i)})^{2}\), i.e. \(M=1\), the intra-class standard classification errors for the two classes can be expressed as follows:_

\[(f_{ssl},+1)=-C^{2}}{2} ,(f_{ssl},-1)=+C^{2}}{2 }.\]

Following [24; 25; 26], We employ _variance of class-wise accuracy_ (VCA) to quantitatively measure the model fairness and present the definition of VCA below.

**Definition 2.2**.: (VCA) Given a classifier \(f:\) where \(=\{1,2,3,,K\}\), the variance of class-wise accuracy of \(f\) is defined as \(VCA(f)=_{i=1}^{K}(p(i)-)\), where \(p(i)=[f()=i|y=i]=1-[f() i|y=i]\) and \(=_{i=1}^{K}p(i)\).

**Theorem 2.3**.: _Given an trained linear SSBC model \(f_{ssl}\) in Eq.(3), the variance of class-wise accuracy \(VCA(f_{ssl})\) is increasing when \(M\) for \(M>1\) and \(M 0\) for \(M<1\). Suppose \(-2_{+})}{(1-)(2-2 _{-}-_{+})}=0\), then when \(M=1\), \((f_{ssl},+1)=(f_{ssl},-1)\) and \(VCA(f_{ssl})=0\)._

_Remark 2.4_.: According to Theorem 2.3, the bigger or smaller value of \(M\) will result in the increase of the variance of class-wise accuracy \(VCA(f_{ssl})\), which implies that the SSBC classifier \(f_{ssl}\) induced by Eq.(3) is unfair. Note that \(M\) is the variance proportion of feature distributions of positive and negative samples as defined in (2). Therefore, to improve the fairness of the induced classifier, we propose to balance the variance bias of positive and negative samples for each label from the feature angle distribution perspective, leading to our S\({}^{2}\)ml\({}^{2}\)-bbam.

## 3 Proposed S\({}^{2}\)ml\({}^{2}\)-bbam Method

In this section, we introduce the proposed SSMLL method named **S\({}^{2}\)ml\({}^{2}\)-bbam**.

### Overview

Generally, our S\({}^{2}\)ml\({}^{2}\)-bbam is built on the generic self-training objective of SSMLL formulated by Eq.1. Specifically, we propose a novel **B**alanced **B**inary **A**ngular **M**argin (**BBAM**) loss \(_{}(,)\), aiming to balance the variance bias of positive and negative samples for each label from the feature angle distribution perspective with the Gaussian assumption. By applying our proposed BBAM loss to the generic SSMLL self-training objective in Eq.1, the objective of S\({}^{2}\)ml\({}^{2}\)-bbam can be formulated as follows:

\[()=K}_{i=1}^{B_{l}}_{k=1}^{K} _{ik}_{}(p_{ik}^{l},y_{ik}^{l})+K}_ {i=1}^{B_{u}}_{k=1}^{K}_{ik}_{}(p_{ik}^{u},y_{ik}^ {u}), \]

where

\[_{ik}=1&(_{i},_{i})_{k}; \\ 1&y_{ik}=1;\\ 0&, k[K],\  i[N_{l}]\ [N_{u}],\]and \(\{_{k}\}_{k=1}^{k=K}\) denotes high-quality negative sample sets constructed by negative sampling.

Here, pseudo-labels of unlabeled data \(\{_{i}^{u}\}_{j=1}^{i=N_{u}}\) are produced by employing the Class-Aware Pseudo-labeling (CAP) trick , which drives their label distribution towards the prior one that is estimated with the labeled samples. Specifically, given the current classifier predictions \(\{_{i}^{u}\}_{i=1}^{i=N_{u}}\) of unlabeled samples, \(\{_{i}^{u}\}_{i=1}^{i=N_{u}}\) are given by:

\[y_{ik}^{u}=1&p_{ik}^{u}>=_{k};\\ 0&p_{ik}^{u}<=_{k};\\ -1&, k[K],\  i[N_{u}], \]

where the class-aware thresholds \(\{_{k}\}_{k=1}^{k=K}\) and \(\{_{k}\}_{k=1}^{k=K}\) are calculated by solving the equations:

\[^{N_{u}}(p_{ik}^{u}>=_{k})}{N_{u }}=^{N_{l}}(p_{ik}^{l}=1)}{N_{l}},\\ ^{N_{u}}(p_{ik}^{u}<=_{k})}{N_{u}}=^{N_{l}}(p_{ik}^{l}=0)}{N_{l}}, k[K],\  i[N_{u}],\]

and \(y_{ik}^{u}=-1\) means that it will not be used for the classifier training.

### BSAM loss

In this section, we introduce the proposed BBAM loss. As its name suggests, our BBAM loss is extended from the **B**inary **A**ngular **M**argin (BAM) loss, which measures the label-specific prediction risk by using the angle between the latent feature and boundary. Formally, for a training sample \((_{i},_{i})\), the BAM loss can be formulated as:

\[_{}(p_{ik},y_{ik})=-(-m)} })&y_{ik}=1;\\ -(1--m)}})&y_{ik}=0, \]

where \(p_{ik}=(_{ik})=_{i}^{u}_{k}^{c}}{\|_{i}\|_{2}\|_{k}^{c}\|_{2}}\), \(\|\!\!\|_{2}\) is the \(_{2}\)-norm of vectors; \(_{i}\) and \(_{k}^{c}\) denote the latent feature of sample \(i\) and the weight vector of the classification layer for category \(k\), respectively; \(_{ik}\) is the angle between \(_{i}\) and \(_{k}^{c}\), \(s\) and \(m\) are the parameters used to control the rescaled norm and magnitude of cosine margin, respectively.

Reviewing the BAM loss in Eq.6, one can observe that it calculates the loss by employing the label angles of samples for each category. We consider that its trained binary boundary tends to deviate from the Bayesian optimal one for each category in SSMLL, where for most categories, the differences between feature distribution variances of corresponding positive and negative samples are much larger than ones in fully supervised learning. To address this issue, for each category \(k\), we suppose that label angles of its positive samples and ones of its negative samples are drawn from a label-specific "positive" Gaussian distribution \((_{k}^{(p)},(_{k}^{2})^{(p)})\) and a label-specific "negative" one \((_{k}^{(n)},(_{k}^{2})^{(n)})\), respectively. According to the properties of Gaussian distribution, we can easily transfer them into ones \((_{k}^{(p)},_{k}^{2})\) and \((_{k}^{(n)},_{k}^{2})\) with balanced variance \(_{k}^{2}=^{2})^{(p)}+(_{k}^{2})^{(n)}} {2}\), by performing the following Gaussian linear transformations on those label angles:

\[_{k}^{(p)}(_{ik})=a_{k}^{(p)}_{ik}+b_{k}^{(p)}, _{k}^{(n)}(_{ik})=a_{k}^{(n)}_{ik}+b_{k}^{(n)},\] \[a_{k}^{(p)}=_{k}}{_{k}^{(p)}}, b _{k}^{(p)}=(1-a_{k}^{(p)})_{k}^{(p)}, a_{k}^{(n)}=_{k}}{_{k}^{(n)}}, b_{k}^{(n)}=(1-a_{k}^{(n)})_{k}^{(n)},  k[K]. \]

With these linear transformation pairs \(\{(_{k}^{(p)}(),_{k}^{(n)}())\}\), for each category, label angles of both positive and negative samples can be refined into ones drawn from balanced angular distributions with one same variance, _e.g._

\[_{k}^{(p)}(_{ik})(_{k}^{(p)},_{k}^ {2})y_{ik}=1;_{k}^{(n)}(_{ik})(_{k}^{(n)}, _{k}^{2})y_{ik}=0.\]Accordingly, the BAM loss in Eq.6 can be rewritten as the following BBAM loss:

\[_{}(p_{ik},y_{ik})=-(^{(p)}(_{ik}))-m)}})&y_{ik}=1;\\ \\ -(1-^{(n)}(_{ik}))-m)}})&y_{ ik}=0. \]

**Estimating label angle variances.** As mentioned above, we concentrate on estimating label-specific "positive" and "negative" angular distributions, _i.e._\(\{(_{k}^{(p)},(_{k}^{2})^{(p)})\}_{k=1}^{k=K}\) and \(\{(_{k}^{(n)},(_{k}^{2})^{(n)})\}_{k=1}^{k=K}\), for each category whose draws are the angles between its label prototype \(_{k}\) and latent features of its corresponding positive and negative samples, respectively. Here, we approximate \(\{(_{k}^{(p)},(_{k}^{2})^{(p)})\}_{k=1}^{k=K}\), \(\{(_{k}^{(n)},(_{k}^{2})^{(n)})\}_{k=1}^{k=K}\), and \(\{_{k}\}_{k=1}^{k=K}\) with labeled and pseudo-labeled samples per-epoch.

For convenience, we denote \(=\{(_{i},_{i})\}_{i=1}^{i=N_{i}+N_{u}}=\{( _{i}^{l},_{i}^{l})\}_{i=1}^{i=N_{l}}\{(_{i}^ {u},_{i}^{u})\}_{i=1}^{i=N_{u}}\) as the couple set of latent features and labels or pseudo-labels of training samples \(\) in the current epoch. We calculate label prototypes \(\{_{k}\}_{k=1}^{k=K}\) by averaging latent features of positive samples in \(\) as:

\[_{k}=^{N_{i}+N_{u}}(y_{ik}=1)_{i} }{_{i=1}^{N_{i}+N_{u}}(y_{ik}=1)},\; k[K]. \]

Consequently, the label angles between label prototypes and latent features of samples are given by:

\[_{ik}=(_{i}^{}_{k}}{\|_{i} \|_{2}\|_{k}\|_{2}}),\; k[K],\; i[N_{l}+N_{u}],\]

Accordingly, the estimations of \(\{(_{k}^{(p)},(_{k}^{2})^{(p)})\}_{k=1}^{k=K}\) and \(\{(_{k}^{(n)},(_{k}^{2})^{(n)})\}_{k=1}^{k=K}\) based on the current negative sample sets \(\{_{k}\}_{k=1}^{k=K}\) can be formulated as:

\[_{k}^{(p)}=^{N_{i}+N_{u}}(y_{ik}=1) _{ik}}{_{i=1}^{N_{i}+N_{u}}(y_{ik}=1)},(_{k}^{2 })^{(p)}=^{N_{i}+N_{u}}(y_{ik}=1)(_{ik}-_{k}^ {(p)})^{2}}{_{i=1}^{N_{i}+N_{u}}(y_{ik}=1)-1},\] \[_{k}^{(n)}=^{N_{i}+N_{u}}_{ik}(y _{ik}=0)_{ik}}{_{i=1}^{N_{i}+N_{u}}_{ik}(y_{ik}=0)}, (_{k}^{2})^{(n)}=^{N_{i}+N_{u}}_{ik}(y_{ik}=0)(_{ik}-_{k}^{(n)})^{2}}{_{i=1}^{N_{i}+N_{u}}_{ik} (y_{ik}=0)-1}. \]

Besides, to avoid the misleading effect of false positive or negative samples, we also employ moving average with a learning rate \(\) over \(\{(_{k}^{(p)},(_{k}^{2})^{(p)})\}_{k=1}^{k=K}\), \(\{(_{k}^{(n)},(_{k}^{2})^{(n)})\}_{k=1}^{k=K}\), and \(\{_{k}\}_{k=1}^{k=K}\).

### Negative Sampling

For efficiency, we suggest a prototype-based negative sampling method. Specifically, for each label, we tend to select those negative samples that are more similar to its positive samples, because they are more difficult to discriminate and would be more informative for the classifier training [21; 22]. To achieve this, for each category, we measure similarity scores of negative samples based on label prototypes \(\{_{k}\}_{k=1}^{k=K}\), and construct the nearest neighbor negative sample sets \(\{_{k}\}_{k=1}^{k=K}\) as:

\[_{k}=\{(_{i},_{i})|d(_{i}, _{k})(\{d(_{i},_{k})\}_{(_{i},_{i})_{k}}),(_{i},_{i}) _{k}\} k[K],\]

where \(d()\) is the vector distance (_e.g._ cosine distance), \(()\) outputs a set of samples with the top-\(M\) minimum distance values; and \(\{_{k}\}_{k=1}^{k=K}\) is the negative sample set of category \(k\) defined as:

\[_{k}=\{(_{i}^{l},_{i}^{l})|(_{i} ^{l},_{i}^{l})_{l},y_{ik}^{l}=0\}\{(_{i} ^{u},_{i}^{u})|_{i}^{u}_{u},y_{ik}^{u}=0\}.\]

Accordingly, the final negative sample sets \(\{_{k}\}_{k=1}^{k=K}\) are generated by:

\[_{k}=\{(_{i},_{i})|(_{i},_{i}) (_{k})\} k[K], \]

with size \(\{|_{k}|= N_{k}\}_{k=1}^{k=K}\), where \(N_{k}=_{i=1}^{N_{l}}(y_{ik}^{l}=1)+_{i=1}^{N_{u}}(y_ {ik}^{u}=1)\), \(\) controls the proportion of positive and negative samples of each category. And we update those negative sample sets \(\{_{k}\}_{k=1}^{k=K}\) per-epoch for efficiency.

### Model Training Summary

We describe the full training process of \(^{2}^{2}\)-bham. To avoid inaccurate pseudo-labels in the early training stage, following , we warm up the classifier \(f_{}()\) with the BAM loss of Eq.6 over labeled samples \(_{l}\) by \(T_{0}\) epochs. Given the initialized \(f_{}()\), we continue to train it with the BBAM loss of Eq.8 over labeled samples \(_{l}\) and unlabeled samples \(_{u}\) by \(T_{t}\) epochs. At each epoch, we update pseudo labels \(\{y^{}_{i}\}_{i=1}^{i=N_{u}}\) by using Eq.5, label prototypes \(\{_{k}\}_{k=1}^{k=K}\), \(\{(^{(p)}_{k},(^{2}_{k})^{(p)})\}_{k=1}^{k=K}\) and \(\{(^{(n)}_{k},(^{2}_{k})^{(n)})\}_{k=1}^{k=K}\) by using Eqs.9 and 10, and perform the negative sampling by using Eq.11. For clarity, the full training process is outlined in Appendix B.

## 4 Experiments

### Experimental Settings

**Datasets.** We employ 5 widely used MLL datasets, including image datasets Pascal VOC-2012 (VOC) , MS-COCO2014 (COCO)  and Animals with Attributes2 (AWA) , text datasets Ohsumed  and AAPD . For clarity, the detailed characteristics of these datasets are displayed in Table 1. Following , we transform these datasets into SSL versions. For each dataset, we randomly select \(\) training samples as labeled ones, and the remaining as unlabeled ones. We set \(\{5\%,10\%,15\%,20\%\}\), to explore the performance of our method under different data proportions. The image size is resized to 224 for all datasets.

**Baselines.** We employ 5 baseline methods for comparisons, including SoftMatch , FlatMatch , MIME , DRML , and CAP . DRML and CAP are SSMLL methods; SoftMatch and FlatMatch are SSL methods; MIME is a single-positive multi-label learning (SPMLL) method. For SSL and SPMLL methods, we follow CAP to apply them to SSMLL tasks.

**Evaluation metrics.** We employ 5 evaluation metrics, including Micro-F1, Macro-F1, mean average precision (mAP), Hamming Loss and One Loss , and compute them with the Scikit-Learn tool.2

**Implementation details.** We use the pre-trained ResNet-50  as the backbone for image datasets and BERT-base-uncased model  for text datasets. We set the decay of EMA as 0.9997. The batch size is 32 for VOC, 128 for AWA and 64 for COCO, Ohsumed and AAPD. The warm-up epoch \(T_{0}\) is 12. The \(s\) and \(m\) are 20 and 0.4 in VOC, 20 and 0.3 in COCO, 10 and 0.2 in AWA, Ohsumed and AAPD. The parameters for negative sampling \(\) are set to 5.

### Results

The experimental results are presented in Table 2 and Table 3. Overall, our method achieves good performance on all metrics. Our model ranks _1st_ on average on five datasets and has a significant advantage over baselines. The detailed analyses are presented as follows.

**Comparing with SSMLL methods:** We can observe that \(^{2}^{2}\)-bham has advantages over recent SSMLL methods. Especially in the Micro-F1 and Macro-F1, our method has significant improvement. On both VOC and COCO, our F1 and mAP values increase by an average of 0.1 and 0.01. Furthermore, on Ohsumed and AAPD, we surprised to discover from the results that our method also has good results. In all data proportions, the average improvement on the mAP is 0.11, 0.14 on Macro-F1 and 0.19 on Micro-F1. This result is foreseeable because our method balanced angle variance using

   Dataset & \#Training & \#Testing & \#Classes & \#Avg. Positive Classes \\  VOC & 5,717 & 5,823 & 20 & 1.46 \\ COCO & 82,081 & 40,137 & 80 & 2.94 \\ AWA & 30,337 & 6,985 & 85 & 30.78 \\ Ohsumed & 22,054 & 10,300 & 23 & 1.65 \\ AAPD & 53,840 & 1,000 & 54 & 2.41 \\   

Table 1: Summary of the dataset statistics

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

domain adaptation strategies. At the same time, how to effectively align pseudo labels with real labels is also an important issue. CAP  developed a class-distribution-aware thresholding strategy to control the assignment of positive and negative pseudo-labels. However, the current SSMLL methods have not paid attention to the **variance bias** problem, which affects the performance of the methods.

**MLL methods.** MLL has multiple research directions. Some methods focus on the model structure. For instance,  proposed a graph convolutional networks model to improve the performance of multi-label image recognition.  proposes a unified framework that combines CNNs and RNNs. Some others focus on exploiting label correlations to improve performance. LSF-CI calculates instance correlation in the feature space and label correlation in the label space through a probabilistic neighborhood graph model and cosine similarity. Due to the complete label information of the training samples, the MLL method can theoretically achieve Bayesian optimal classifier boundaries. However, in semi supervised learning, incorrect pseudo labels may provide incorrect guidance for classification boundaries.

**SSL methods.** Pseudo Label  is one of the earliest semi-supervised learning methods for neural networks. It generates pseudo labels for unlabeled data and continuously improves the accuracy of pseudo labels as the model is optimized. As data augmentation technology has advanced, an increasing number of SSL methods are incorporating this technology . Further research has been conducted on the threshold issue of pseudo labels in . By developing dynamic threshold strategies, they have been able to obtain more accurate pseudo labels, effectively enhancing the performance of the SSL methods. In order to utilize pseudo labels with low confidence but correct classification,  proposes an effective method that fits the confidence distribution of truncated Gaussian functions. Moreover,  discovered that the generalization ability of SSL models is impacted by disconnection between labeled data and unlabeled data, and proposed the FlatMatch method to address this issue. However, it's important to note that these SSL methods are designed to handle multi-class single-label tasks  and cannot be directly applied to multi-label learning scenarios.

## 6 Conclusion

In this paper, we proposed a novel SSMLL method, namely S\({}^{2}\)ML\({}^{2}\)-bbam. Our S\({}^{2}\)ML\({}^{2}\)-bbam balances the variance bias between positive and negative samples from the perspective of the feature angle distribution for each label. To achieve this, we design a novel balanced binary angular margin loss by extending the traditional binary angular margin loss with feature angle distribution transformations under the Gaussian assumption, where the distributions are iteratively updated during classifier training. We also suggest an efficient prototype-based negative sampling method to maintain high-quality negative samples for each label. Empirical results demonstrate that our S\({}^{2}\)ML\({}^{2}\)-bbam outperforms current SSMLL baseline methods.

## Limitations

From the empirical results, we found that S\({}^{2}\)ml\({}^{2}\)-bbam suffers from slightly lower mAP scores on the benchmarks VOC and COOC when increasing the proportion of labeled training samples. This may restrict the range of applications and scenarios in which S\({}^{2}\)ml\({}^{2}\)-bbam can be effectively used. And we will further exploit it in our future works.

## Broader Impacts

The paper focuses solely on the technical aspects of SSMLL algorithms. Therefore, this work can benefit a wide range of machine learning researchers. Also, we do not expect our efforts to have any negative consequences.