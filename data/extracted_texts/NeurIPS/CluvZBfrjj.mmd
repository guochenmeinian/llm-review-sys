# From Instance Training to Instruction Learning:

Task Adapters Generation from Instructions

 Huanxuan Liao\({}^{1,2}\), Shizhu He\({}^{1,2}\)1, Yao Xu\({}^{1,2}\), Yuanzhe Zhang\({}^{1,2}\),

**Yanchao Hao\({}^{3}\), Shengping Liu\({}^{4}\), Kang Liu\({}^{1,2}\), Jun Zhao\({}^{1,2}\) \({}^{1}\)** The Key Laboratory of Cognition and Decision Intelligence for Complex Systems,

Institute of Automation, Chinese Academy of Sciences, Beijing, China

\({}^{2}\) School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China

\({}^{3}\) Platform and Content Group, Tencent, Beijing, China \({}^{4}\) Unisound, Beijing, China

liaohuanxuan2023@ia.ac.cn {shizhu.he, yao.xu, kliu, jzhao}@nlpr.ia.ac.cn

Corresponding author

###### Abstract

Large language models (LLMs) have acquired the ability to solve general tasks by utilizing instruction finetuning (IFT). However, IFT still relies heavily on instance training of extensive task data, which greatly limits the adaptability of LLMs to real-world scenarios where labeled task instances are scarce and broader task generalization becomes paramount. Contrary to LLMs, humans acquire skills and complete tasks not merely through repeated practice but also by understanding and following instructional guidelines. This paper is dedicated to simulating human learning to address the shortcomings of instance training, focusing on instruction learning to enhance cross-task generalization. Within this context, we introduce **T**ask **A**dapters **G**eneration from **I**nstructions (**TAGI**), which automatically constructs the task-specific model in a parameter generation manner based on the given task instructions without retraining for unseen tasks. Specifically, we utilize knowledge distillation to enhance the consistency between TAGI developed through _Learning with Instruction_ and task-specific models developed through _Training with Instance_, by aligning the labels, output logits, and adapter parameters between them. TAGI is endowed with cross-task generalization capabilities through a two-stage training process that includes hypernetwork pretraining and finetuning. We evaluate TAGI on the Super-Natural Instructions and P3 datasets. The experimental results demonstrate that TAGI can match or even outperform traditional meta-trained models and other hypernetwork models, while significantly reducing computational requirements. Our code will be available at [https://github.com/Xnhyacinth/TAGI](https://github.com/Xnhyacinth/TAGI).

## 1 Introduction

Large language models (LLMs) have acquired the ability to solve general tasks by utilizing instruction finetuning (IFT), which describes different tasks in the same natural language format [3; 6; 23]. However, IFT still relies heavily on instance training of extensive task data {(_Description_, _[Demonstrations]_, _Source_, _Target_)} [37; 39], which faces significant limitations in adapting LLMs to real-world scenarios where labeled task instances are scarce and broader task generalization becomes paramount.

Therefore, for better cross-task generalization, the "zero-shot" learning ability of LLMs is crucial for real-world applications: models learned with instructions can achieve non-trivial performance on unseen tasks with just a single instruction that provides a comprehensive description of the task (e.g.,"_You will be given sentences in which your task is to recognize the name of a person."_). Traditionally, achieving this capability involves meta-training the model by associating each input with specific task instructions [21; 37]. For example, GPT-3  has demonstrated strong "zero-shot" capabilities through meta-training. However, these methods heavily depend on the foundation model's abilities and are inefficient for various unseen tasks [22; 44], as they require reprocessing extensive task instructions and some supplementary task data (e.g., examples from few-shot instances) for each input (see the top of Figure 1).

In recent years, researchers have begun to explore meta-learning to enhance the cross-task generalization capabilities of LLMs, aiming to construct flexible, reusable and robust task-specific models [1; 34]. For example, task-specific models such as Adapter , LoRA , and Prefix  have been constructed by a hypernetwork . This approach significantly enhances task generalization by processing instructions efficiently, reducing redundant computations . However, these methods heavily depend on a substantial corpus of training instances, which can hinder their capacity to efficiently learn and construct task-specific models based on provided instructions .

In fact, contrary to LLMs, humans acquire skills and complete tasks not only through repeated practice but also by understanding and following instructional guidelines . For example, a tourist with basic knowledge of riding vehicles can easily learn to use new ones abroad for the first time with the help of travel guides. This paper aims to mimic the way humans learn skills by understanding instructions. This shift represents a modest evolution in task model construction, transitioning from traditional instance training models to a contemporary approach focused on instruction learning. By providing task instructions, the novel paradigm offers an automated solution for generating task-specific adapters and seamlessly integrating them into the base model. This approach aims to streamline the development of task-specific models while enhancing their ability to generalize across diverse tasks with instructions.

Guided by this goal, we introduce **T**ask **A**dapters **G**eneration from **I**nstructions (TAGI), which converts instructions to task-specific adapters using a hypernetwork. Under the knowledge distillation framework [10; 36], we enable models to the "_Learning with Instruction_" paradigm in a manner analogous to the "_Training with Instance_" paradigm. TAGI will enhance the alignment between the task-specific model \(_{k}\) (acting as the teacher) and the vanilla LLM \(_{0}\) combined with the generated task adapters \(_{k}\) (acting as the student) (see the bottom of Figure 1). This alignment is achieved not only through instance training but also by incorporating parameter learning for task-specific models based on instructions. Specifically, we align the student under two distinct paradigms, encompassing not just the targets and logits, but also the adapters' parameters by an L2 regularization within instruction, which represents the enhancement of the understanding of instructions and the ability to generate more efficient task-specific adapters. Moreover, TAGI endows the model with task generalization capabilities through a two-stage training process: hypernetwork pretraining on standard text pretraining data (e.g., C4 ), followed by finetuning on meta-training tasks. This allows it to generalize effectively across unseen tasks without sacrificing performance.

We evaluate TAGI on the Super-Natural Instructions (SNI)  and P3  datasets. Experimental results demonstrate its ability to effectively generate adapters for unseen tasks, surpassing meta-trained models by 2% in SNI and 5% in P3, while significantly reducing computational demands by 60%, and outperforming other hypernetwork models by 7%. Notably, our method does not require additional parameter updating or gradient back-propagation, and it avoids the inefficiency of repeatedly encoding instructions during inference. We summarize our contributions as follows:

* We propose a novel model construction paradigm by imitating human learning abilities, **Learning with Instruction**, for the cross-task generalization of the LLMs. To the best of our knowledge, it is the first time that a task-specific model has been generated based on instruction learning, and its capabilities and parameters are distilled from a teacher model trained on instance learning.

Figure 1: Comparison of the typical **Training with Instance** and the proposed **Learning with Instruction**: The former involves training the model at the instance level with parameter updates, while the latter generates a task-specific adapter at the task level with parameter generation.

* We used a knowledge distillation framework to develop task-specific models within the instruction learning paradigm. By aligning model parameters comprehensively, the TAGI method improves the model's ability to understand instructions and solve unseen tasks more accurately and efficiently.
* Comprehensive quantitative and qualitative assessments have highlighted the effectiveness of TAGI on two publicly available large-scale instruction datasets, with lower inference costs.

## 2 Related Work

TAGI draws inspiration from previous research on instruction following, hypernetworks and knowledge distillation. In this section, we will delve into the pioneering work in these areas.

**Instruction Following** is often used to evaluate the cross-task generalization of LLMs, and it is dedicated to handling any task described in natural language. Recent findings suggest that additional finetuning of LLMs with instructions substantially improves their zero-shot capabilities [6; 38; 39]. Moreover, large-scale multi-task meta-training has been shown to equip models with the ability to address new tasks in zero- or few-shot scenarios, facilitated by standard task formats and prompts [30; 44] alongside providing concise task instructions and select examples [24; 37]. However, the instructions and examples can significantly escalate the computational burden compared to task-specific models. Existing works attempt to mitigate this issue involved creating adapters to separately process instructions and examples [13; 42] with reduced performance. To overcome these limitations, we introduce a new paradigm that draws on instruction-based learning, simulating instance training to enhance the perception and processing capabilities of LLMs for handling unseen tasks.

**Hypernetworks**[8; 31] are neural networks that generate weights for other neural networks , which are designed to use fewer parameters to dynamically build task-specific models [9; 33]. Notable works such as HyperTuning , HINT , and Hypter  have all adopted hypernetworks to convert task instructions and demonstrations into adapters for LLMs. And MEND  utilizes hypernetworks to compress demonstrations for distilled vectors. Although they all avoided processing lengthy instructions repeatedly and utilized adapters to make training and testing more cost-effective , they still have a performance loss compared to meta-training . The proposed method TAGI incorporates the utilization of hypernetworks, which are instrumental in generating task-specific adapters that are seamlessly integrated into LLMs. Compared to existing models based on hypernetworks, TAGI not only trains at the instance level but also incorporates knowledge distillation to supervise the adapters generated by hypernetworks, thereby achieving both efficiency and effectiveness.

**Knowledge Distillation** is a technique where a smaller model (student) learns to mimic the predictions of a larger model (teacher), aiming to retain performance while reducing computational resources . Indeed, the application of knowledge distillation is the essential difference between the proposed method in this paper and other hypernetwork-based methods such as HINT  and Hypter . Recently, some works  utilize knowledge distillation to finetune small language models such as T5 , enabling them to act as LLMs with pre-prompting without any given prompts. Compared with the typical knowledge distillation methods of LLMs, the proposed method TAGI in this paper further utilizes model parameter alignment and aims to mimic another learning paradigm of human skill learning. We not only calculate the Kullback-Leibler (KL) divergence  between teacher and student models , but also compute the L2 regularization between the generated adapter by instruction learning and task-specific models by instance training.

## 3 Methods

### Problem Setting

**Cross-task Generalization:** Given a set of tasks \(=\{_{1},...,_{||}\}\), where each task \(_{i}\) contains a set of (_source_, _target_) samples \(_{i}=\{(s_{1},t_{1}),...,(s_{n},t_{n})\}\). We categorize these tasks into three distinct non-overlapping groups for validating out-of-distribution generalization: meta-train (\(_{train}\)), meta-valid (\(_{valid}\)), and meta-test (\(_{test}\)), assuming all tasks adhere to a text-to-text format. For example, \(_{train}\) comprises tasks like translation and question answering, the \(_{valid}\) and \(_{test}\) encompass tasks such as paraphrasing and natural language inference respectively. Within the \(_{train}\), the goal is to utilize the data for training and transfer knowledge to facilitate learning to resolve the test tasks. For all methods discussed, aside from the original unsupervised pretraining of the language model backbone on separate corpora, the model learning primarily takes place through multi-task training on the \(_{train}\).

### Task Adapters Generation from Instructions (TAGI)

In this section, we will introduce the detailed method of TAGI. For each (unseen) task, TAGI consists of two core components: a **hypernetwork**SS 3.2.1 which receives task instructions and generates parameter-efficient adapters, and a task-specific model which combines the **vanilla LLM** and the generated adapters from hypernetwork.

Unlike traditional meta-training methods, we transition from _training with instance_ to _learning with instruction_, which not only addresses efficiency issues at the instance level but also incorporates parameter alignment for the task-specific model parameters at the instruction level. Specifically, the complete process is shown in Figure 2, we initially train the LoRA modules SS 3.2.2 on various upstream tasks (seen tasks) with task datasets of meta-train (\(_{train}\)). Specifically, for \(N\) distinct upstream tasks, we independently train \(N\) LoRA modules, with each module denoted as \(_{i}\) for task \(_{i}\), presumed to represent the optimal model for its respective task. Subsequently, TAGI is committed to building proprietary models for downstream tasks (unseen tasks). Its training process is bifurcated into two primary phases: hypernetwork pretraining SS 3.2.3 and hypernetwork finetuning SS 3.2.4 which encompasses distillation and alignment.

#### 3.2.1 Hypernetwork for Converting Instructions into LoRA

A pivotal element of our model is the hypernetwork that converts task instructions (descriptions and demonstrations) into a parameter-efficient module. Our hypernetwork comprises two crucial components: the **encoder**, derived from the vanilla LLM2, is designed to minimize encoding biases by converting task instructions into a continuous contextual representation. This representation is then fused with LLM input and concated with encoded input for the decoder. Additionally, the **adapter generator**, utilizing a basic MLP design , is both lightweight and efficient, effectively converting encoded instructions into parameter-efficient modules.

Figure 2: Overview of TAGI. The hypernetwork takes instruction as input and generates adapters subsequently integrated into the vanilla LLM, and constructed the task-specific model as student. After training the task models through instances on multiple basic tasks as a teacher, TAGI constructs task-specific models by aligning the labels, output logits, and adapter parameters between teacher and student models. To improve compliance with task instructions and the efficacy of weight generation, TAGI undergoes a two-stage hypernetwork training process: hypernetwork pretraining and finetuning. a-c are random divisions of the sampled sentences from pretraining datasets.

**Encoder:** Prior studies simply concatenated encoded instructions with inputs, overlooking the interactions between them. To address this, we integrated a hierarchical cross-attention layer into the encoder of the LLM to refine the input representation with embedded instruction details. Specifically, for an input \(x\) and its corresponding task instruction \(i_{x}\), we initially employ the encoder within the hypernetwork to encode the instruction into representations \(_{x}^{s d}\). Then, we feed the \(x\) into the model and obtain the output representation \(_{l}\) from the self-attention sublayer in the \(l\)-th layer. Ultimately, \(_{l}\) is processed through the \(l\)-th cross-attention layer, resulting in a text representation that is enriched with instruction information:

\[_{l}=_{l}(_{l},_{x}) \]

where \(_{l}\) conducts multi-head attention on the query, key, and value matrices, followed by residual connection and layer normalization. The final input to the decoder is the concatenation of the encoded instruction and the encoded fusion input, i.e., \((_{x};_{l})\).

**Adapter Generator:** Considering the efficiency and effectiveness, we utilize a two-layer multi-layer perceptron (MLP) to generate parameter-efficient modules (e.g., LoRA) for the encoded instruction. To differentiate between the query \(\) and value \(\) matrices as well as the layers, we introduce layer ids \(_{l}^{\{,\}}\{0,,2\#\}\) as positional information. We use a unique network for each layer and share it between \(\) and \(\) (i.e., one network is used for a certain layer LoRA generation).

\[_{l}^{\{,\}}=_{l}(_{x_{k }};_{l}^{\{,\}}_{l}^{}=2l,_{l}^{}=2l+1) \]

where \(_{l}^{}\) and \(_{l}^{}\) are the \(l\)-th LoRA of \(\) and \(\), respectively.

#### 3.2.2 LoRA Tuning for Task-specific Models

LoRA  efficiently reduces the number of trainable parameters by decomposing the update of the LLM's attention weight matrix (denoted as \(_{0}^{d k}\)) into low-rank matrices. Specifically, LoRA updates the weight matrix as \(_{0}+=_{0}+\), with \(^{d r}\) and \(^{r k}\) being trainable low-rank matrices of rank \(r\), significantly smaller in dimensions than \(d\) and \(k\). We finetune a robust baseline to derive the LoRA parameters \(_{i}\) for task-specific models for \(i\)-th task, facilitating LLM instruction learning and parameter alignment. SNI is categorized into 60 types based on task types, while P3 encompasses 36 categories, corresponding to 60 and 36 parameter modules, respectively.

#### 3.2.3 Hypernetwork Pretraining for Preliminary Generalization

Previous research [5; 27] has demonstrated that pretraining hypernetworks can substantially improve the model's cross-task generalization capabilities. Adhering to the HINT , we pretrain the hypernetwork on C4  before finetuning it on a diverse multi-task prompt dataset. As illustrated in the right segment of Figure 2, given an input sequence, we partition it into randomly sized segments \(a\), \(b\), and \(c\), where \(a\) is fed into the hypernetwork, \(b\) into the LLM, and \(c\) is the segment to predict. During this stage, training is conducted by minimizing the cross-entropy loss \(_{}\), aiming to ensure that the hypernetwork learns to recognize instructions to enhance generalization ability.

\[_{}=P_{((a))}(c b) \]

#### 3.2.4 Hypernetwork Finetuning for Instruction Learning

At this stage, TAGI is finetuned on a multi-task prompt dataset, enabling it to learn the generation of optimal parameters from task instructions, thereby ensuring effective generalization to future unseen tasks. Similar to the pretraining phase, task instructions (alongside some few-shot samples) replace \(a\), the main input replaces \(b\), and the target replaces \(c\). In each iteration, the hypernetwork generates LoRA parameters and encodes the instructions. LoRA is a parameter-efficient module (i.e., inserting into the model), and the encoded instructions are integrated with the encoder's embeddings for information fusion and concatenated with the fused encoding input during decoding. Beyond the standard \(_{}\), we employ knowledge distillation for instruction learning: a strong baseline combining complete task instructions and input, serves as the teacher, while the model incorporating generated LoRA parameters with the input, acts as the student. The KL divergence \(_{}\) measures the discrepancy in word probability distributions between the two models as an implicit learning outcome, and the MSE loss \(_{}\) calculates the difference between the generated parameters andthose of task-specific parameter-efficient modules as an explicit learning intermediate result. The formulation of finetuning is as follows:

\[_{}=(_{i},(a)) \]

\[_{}=(P_{(+_{i})}(x(a;b))  P_{(+(a))}(x b)) \]

\[_{}=_{}+_{1}_{}+_{2}_{} \]

where \(a_{i}\), \(_{i}\) is the optimal LoRA modules of the \(i\)-th task, \(_{1}\) and \(_{2}\) are the hyper-parameter to control the importance of distillation in finetuning.

## 4 Experiments

We first present the datasets (SS 4.1) and baselines (SS 4.2) used in our evaluation and then discuss three research questions (RQs):

RQ1: Can the proposed instruction learning paradigm effectively learn the ability of instance training? Can it support cross-task generalization of LLMs? (SS 4.4)

RQ2: How many foundation tasks does TAGI need to learn to achieve better results? (SS 4.5)

RQ3: What is the impact of different modules and learning stages on TAGI? (SS 4.7)

### Datasets

To demonstrate the generality of our method, we evaluate our approach on two popular multi-task instruction datasets3: **Super-Natural Instructions (SNI)** and **T0 split of P3 (P3)**.

**SNI** comprising over 1,600 task datasets, each dataset includes a task definition and a set of fixed positive and negative demonstrations. We follow the previous research [13; 27] and examine two settings: only using the task definition as the input to the hypernetwork (**'Def'**), and using the definition along with two few-shot positive examples (**'Def + 2 Pos'**). We only use the English tasks in the dataset and the model's generation is evaluated on a set of 119 unseen tasks using ROUGE-L.

**P3** composed of 62 task datasets, the T0 model is trained with these tasks divided into meta-training and meta-test sets. The format of the prompts takes into consideration 0-shot reasoning and typically includes instructions or possible answer options. We follow the precedent work  by using the T0 training subset 36 tasks to train our model. The evaluation is conducted based on the accuracy scores of multiple-choice questions for unseen 11 tasks in the meta-test set (MTest11).

### Baselines

We compare the characteristics of **TAGI** against eight primary groups of baselines (as shown in Table 1): _1)_**No FT**: models without finetuning. _2)_**HyperTuning**: models that use hypernetwork to convert demonstrations into adapters without instruction fusion. _3)_**Hyper**: models based on hypernetwork do not use pretraining. _4)_**HINT**: models pretrain hypernetwork and concat instruction. _5)_**T0** and **Tk-Instruct**: strong baselines fully finetuned on P3 and SNI respectively with instruction concatenated. _6)_**Full FT**: models finetuned on target tasks. _7)_**Decoder-only model**: decoder-only models fully finetuned like GPT-2  and OPT . _8)_**FID-ICL**: ICL method use encoder intermediate fusion.

   Method & Pre- & Instr. & Low Infer. & Instr. & Unseen \\  & Train & Fus. & Cost & Learning & Task \\  Simple FT & ✗ & ✔ & ✗ & ✗ & ✗ \\ T0  / **Tk-Instruct** & ✗ & ✔ & ✗ & ✗ & ✔ \\ Hyper  & ✗ & ✗ & ✔ & ✗ & ✔ \\ HyperTuning  & ✗ & ✗ & ✔ & ✗ & ✔ \\ HINT  & ✔ & ✗ & ✔ & ✗ & ✔ \\
**TAGI (Ours)** & ✔ & ✔ & ✔ & ✔ & ✔ \\   

Table 1: Compare the characteristics of all comparison methods and the proposed TAGI. More comparisons can be seen in C.1.

### Implementations

We limit our scope to encoder-decoder models for our experiments4. We use T5-LM-Adapt5 and T0  as initializations in our experiments. The two model groups have the same architectural framework but differ in weight; T0 uses T5-LM-Adapt for initialization and undergoes multi-task training on the P3 meta-training set. For **SNI**, only T5-LM-Adapt is considered, and three different sizes are tested: Base (250M), XL (3B), and XXL (11B), with the teacher model being TK-Instruct . For **P3**, we experimented with two sets of models of three different sizes: Base (250M), Large (800M), and XL (3B) with the only template as input, while the teacher model being FiD-ICL  with 16-shot examples. The A.4 contains more implementation details and experimental settings.

### Main Results

**Super-Natural Instructions.** We report the performance and inference costs of TAGI models and baselines in Table 2. Our analysis and findings yield several key insights:

\(\) Firstly, **methods lacking finetuning exhibit subpar performance**. As shown in the first row of the table, the performance of No FT is significantly lower than other baseline methods by approximately 30 points (except for Hypler), which underscores the critical role of inductive bias, introduced during meta-training, in enhancing the model's instructional adherence and cross-task generalization.

\(\) Secondly, **TAGI demonstrates notable improvements over other hypernetwork-based baselines, with only a marginal increase in inference overhead** (see Table 2 last column). We find that TAGI still outperforms the advanced method HINT (\( 2\) points) while achieving similar computational savings. This highlights the efficacy of instruction learning with knowledge distillation. The underperformance of HINT and Hypertuning may stem from their sole reliance on cross-entropy with the target during meta-training, lacking explicit supervision of intermediate task-specific module parameters and implicit supervision of the teacher outcome. This deficiency impedes their ability to fully leverage instruction tasks for generating superior adapter parameters during meta-test.

\(\) Thirdly, **TAGI consistently matches or even surpasses robust baselines in both zero- and few-shot settings**. Comparing TAGI with multi-task finetuning approaches such as Full FT and TK-Instruct, we observe that TAGI achieves comparable performance (\(0-2.3\) points) except for 11B while utilizing approximately 2.5 \(\) fewer FLOPs. TAGI's performance on the 11B model is somewhat lacking, potentially attributable to either insufficient training due to resource limitations or a decrement in performance stemming from the omission of parameter alignment constraints due to time constraints6. In alignment with prior research, TAGI significantly surpasses GPT-2 and OPT-13B in comparative analyses with decoder-only models (\( 10\) points in GPT2 and \( 7\) points

    &  &  & Avg. Rel. \\ 
**Method** & Base (250M) & XL (3B) & XXL (11B) & Base (250M) & XL (3B) & XXL (11B) & FLOPs \\  No FT & 8.8 & 14.3 & 26.2 & 9.4 & 13.6 & 30.5 & \(\)1.0 \\ Tk-Instruct\({}^{}\) & **35.3** & 48.0 & **53.6** & 42.1 & 54.0 & **62.0** & \(\)1.0 \\  _\# Decoder-only model_ & & & & & & & \\ GPT-2 XL (1.5B)\({}^{*}\) & - & 38.2 & - & - & 45.3 & - & \(\)**0.33** \\ OPT (13B)\({}^{*}\) & - & - & 44.8 & - & - & 51.5 & \(\)0.36 \\  _\# Hyperneuron-based model_ & & & & & & & \\ Hyper\({}^{*}\) & 12.1 & 16.8 & 15.5 & 10.6 & 14.2 & 13.4 & \(\)0.35 \\ HyperTuning\({}^{}\) & - & 38.9 & - & - & 48.6 & - & \(\)0.34 \\ HINT\({}^{*}\) & 33.3 & 47.2 & 51.1 & 41.8 & 53.2 & 56.4 & \(\)0.37 \\
**TAGI (Ours)** & **35.3** & **48.4** & 52.3 \({}^{}\) & **42.5** & **56.3** & 58.4 \({}^{}\) & \(\)0.39 \\   

Table 2: RougeL results on Super-Natural Instructions. The best results are in bold, while the second-best are underlined. \(*\), \({}\) means that those results are from HINT  and  respectively, "-" means not reported. \({}\) indicates that there is no parameter alignment loss in the hypernetwork finetuning because the model is too large, leading to a significant amount of time required for LoRA tuning for each task. The Average Relative FLOPs cost is calculated relative to Tk-Instruct. We use the number of FLOPs required by each model to process one task (containing 100 examples).

in OPT-13B), affirming the superiority of encoder-decoder models within similar meta-learning frameworks. Overall, TAGI fulfills its objective by enhancing cross-task generalization capabilities through instruction learning and striking an optimal balance between performance and efficiency.

**P3.** We report results on the T0 evaluation set in Table 3, with full results in C.2.

\(\) Firstly, examining the ICL-based methods presented in the middle section, it is evident that all three ICL aggregation strategies achieve superior performance. This underscores the utility of instructions and demonstrations in aiding LLMs. However, these methods require concatenating extensive demonstrations during both training and inference, which significantly increases computational demands and reduces efficiency (\( 2\) - \( 13.2\) inference time). In contrast, **TAGI by leveraging solely task instructions one time, attains comparable or superior accuracy levels while significantly curtailing computational burdens (\(\)0.88)**. TAGI demonstrates a slight disadvantage (merely \(1.2\) points) to FiD-ICL  on T5-LM, yet it outperforms other methods (\( 1\) point). For T0, it is only 1.5 points lower than Full FT and exceeds all ICL-based methods. Notably, TAGI does not require the 16 examples like the ICL-based method, nor does it necessitate repeated processing of instructions like the baselines, significantly reducing inference overhead.

\(\) A comparison of the first three lines of results indicates that for large or XL models, **initializing with T5-LM outperforms T0**. We hypothesize that the process of training T5-LM to transition into T0 might result in the dilution of world knowledge or the diminishment of certain specific capabilities, thereby attenuating the benefits derived from meta-training. Conversely, for models of base size, T0 serves as a more effective initialization point.

\(\) Furthermore, **TAGI outperforms competing hypernetwork models7**. By comparing the last two columns, it is evident that the performance in MTest11 surpasses HINT and Hypertuning by \(0.5\) and \(4.6\) points respectively. Additionally, in the HyperT5 evaluation, the performance exceeds Hypertuning by \(1\) point. This aligns with prior findings, suggesting that instruction learning augments the hypernetwork's task comprehension and its capacity to generate task-specific adapters.

### Varying Number of Meta-Training Tasks

A fundamental component of our methodology is incorporating parameter alignment in instruction learning. Consequently, it is imperative to examine the effect of varying the number of tasks on which

    &  &  & Avg. Rel. \\ 
**Method** & Base (250M) & Large (800M) & XL (3B) & Base (250M) & Large (800M) & XL (3B) & Infer. Time \\  _\# MTest11 Avg._ & & & & & & & \\ Zero-shot & 43.9 & 41.5 & 42.6 & 49.1 & 52.4 & 57.6 & \( 1.0\) \\ Full FT & 44.6 & 45.5 & 47.2 & **51.9** & **56.6** & **61.4** & \( 1.0\) \\ Metranian \(\) & 44.1 & 52.4 & 53.1 & 50.1 & 52.4 & 56.8 & \( 1.0\) \\  _\# ICL-based method_ & & & & & & & \\ Concat-IC\({}^{}\) & 44.2 & 47.6 & - & 48.6 & 53.2 & - & \( 4.1\) \\ FiD-ICL\({}^{}\) & **47.0** & **58.2** & **60.0** & 51.0 & 53.4 & 58.2 & \( 1.9\) \\ Ensemble-ICL\({}^{}\) & 44.6 & 54.5 & 52.6 & 49.9 & 53.7 & 57.7 & \( 13.2\) \\  _\# Hypernetwork-based model_ & & & & & & & \\ Hyper\({}^{*}\) & - & - & - & - & - & 56.2 & - \\ HINT\({}^{*}\) & - & - & - & - & - & 60.3 & - \\
**TAGI (Ours)** & 45.6 & 54.7 & 58.9 & 50.8 & 53.8 & 58.8 & \(\)0.88 \\  _\# HyperT5 Avg. (Without SIClce dataset)_ & & & & & & & \\ FiD-ICL\({}^{}\) & **46.9** & **55.8** & 60.6 & **51.7** & 53.9 & 58.5 & \( 1.9\) \\ HyperTuning\({}^{}\) & - & 54.6 & 59.6 & - & - & - & - \\
**TAGI (Ours)** & 46.7 & 56.0 & 59.8 & **51.7** & **54.6** & **59.2** & \(\)**0.88** \\   

Table 3: Average accuracy results over T0 evaluation tasks after training on the T0 P3 train set. \(\) means results are from . \(\) trained by us followed the Tk-Instruct (meta-training) . Our method uses only template inputs without demonstrations yet achieves competitive performance with ICL-based methods using 16 shots, with much-reduced inference overhead. The Average Relative Inference Time is calculated relative to the Metatrain. We use the inference time required by each model to process all 11 test tasks with batch_size of 1.

parameter alignment is applied on outcomes and its influence on the generalization capabilities of LLMs. To this end, we conduct a comprehensive experimental analysis to compare the efficacy of instruction learning with parameter alignment across a spectrum of task quantities against instruction learning devoid of parameter alignment. Tasks are organized in descending order based on the number of datasets encompassed within each. Subsequently, a predetermined number of tasks are sequentially selected for meta-training purposes. This approach allows us to systematically evaluate the impact of parameter alignment on learning and generalization as the number of tasks varied.

From Figure 3, we find that, firstly, **an increase in the number of tasks correlates with improved performance across all methods**, suggesting that meta-training across a broader array of tasks enhances the model's instruction-following capabilities. However, the practical limitations of sourcing a sufficient quantity of tasks for meta-training must be acknowledged. Secondly, it was observed that the TAGI model exhibits lower overall performance in the absence of parameter alignment for instruction learning, yet it demonstrates a smaller relative standard deviation and less variability in performance in response to the number of tasks. This pattern aligns with the expected outcomes of instruction learning, highlighting the **efficacy of our approach in bolstering the model's ability to adhere to task instructions and generate task-specific adapters**.

### Parameter Size against Performance

We analyzed the proportion of generated parameter sizes relative to the total parameter size during the generation of various ranks, and compared this to the performance of the full meta-training fine-tuning method, as demonstrated in Figure 4 and Table 7. We can find that **TAGI requires only about 10% of the parameters to outperform full meta-training fine-tuning which indicates that the limited parameters generated by the Hypernetwork serve as an optimal solution for task completion**. The ability to adaptively construct models tailored to specific tasks removes the necessity for additional fine-tuning, underscoring TAGI's effectiveness and efficiency.

Figure 4: The percentage of generated parameters (%) against performance (RougeL). The backbone model is T5-LM-Base, all trained for 20,000 steps.

Figure 3: The performance of different numbers of meta-training tasks. The backbone model is T5-LM-Base, all trained for 20,000 steps.

### Ablation Study

To evaluate the significance of each component within the TAGI model, we conducted a series of experiments across two meta-task datasets utilizing the T5-LM-XL (3B) model. The results as depicted in the Table 4, highlight that the **instructions fusion plays a pivotal role in enhancing model performance**. This process facilitates dynamic interaction between the input and the instructions, enriching the model's input with additional contextual information, reminiscent of the substantial benefits observed with ICL. Moreover, **pretraining emerges as a critical phase**, markedly improving the capabilities of models that have not undergone pretraining, thereby significantly enhancing their proficiency in interpreting and executing task instructions. Furthermore, the systematic removal of **various components during the finetuning phase indicates a consistent decline in performance**, underscoring the integral contribution of each component to the model's overall efficacy.

Compared to meta-learning methods such as LoRA fine-tuning (rank=32) "Tk-Instruct-LoRA", prefix fine-tuning (num_virtual_tokens=32) "Tk-Instruct-prefix", and full fine-tuning "Tk-Instruct", our TAGI method enhances task comprehension and utilization which achieved through a hypernetwork that dynamically generates adapter LoRA insertions into the LLM based on input, leads to better cross-task generalization capabilities. Notably, prefix fine-tuning excels in the Def + 2Pos scenario, likely due to its effective integration of information from positive examples. Conversely, the Def scenario performs less satisfactorily, indicating that instructions alone are insufficient for optimal results. Comparative analysis with other hypernetwork models reveals that TAGI's ablation performance remains robust, affirming the effectiveness of each step in bolstering TAGI's operational efficiency.

## 5 Conclusions

In this paper, we introduce an innovative method of instruction learning designed to emulate instance training. This approach enables the model to achieve specified tasks and learn from instructions on how to address a category of problems. The proposed TAGI seamlessly integrates instruction into the input and processes the instruction simultaneously, thereby ensuring minimal inference overhead. Concurrently, we employ a knowledge distillation framework to facilitate instruction learning for distilling skills and aligning task-specific models. This allows the hypernetwork to transform task instructions into an efficient module inserted into the LLMs, thereby boosting generalization performance. Remarkably, TAGI consistently equals or surpasses the efficacy of conventional meta-training approaches while requiring fewer FLOPs and obviating the need for additional model parameters updating or gradient back-propagation. Future work will investigate more potent hypernetwork pretraining techniques and develop superior instruction fusion methods to augment the hypernetwork's expressive capability, thereby enhancing the model's ability to generalize to unseen tasks. Moreover, future work will investigate various task type classifications and the generalization effects of cross-modal tasks in instruction learning.

## 6 Acknowledgements

This work was supported by National Key R&D Program of China (No. 2022YFF0711900) and the National Natural Science Foundation of China (No.62376270, No.62276264). This work was supported by the Youth Innovation Promotion Association CAS.