# Generalization Bound and Learning Methods for Data-Driven Projections in Linear Programming

Shinsaku Sakaue

The University of Tokyo and RIKEN AIP

Tokyo, Japan

sakaue@mist.i.u-tokyo.ac.jp

&Taihei Oki

Hokkaido University

Hokkaido, Japan

oki@icredd.hokudai.ac.jp

###### Abstract

How to solve high-dimensional linear programs (LPs) efficiently is a fundamental question. Recently, there has been a surge of interest in reducing LP sizes using _random projections_, which can accelerate solving LPs independently of improving LP solvers. This paper explores a new direction of _data-driven projections_, which use projection matrices learned from data instead of random projection matrices. Given training data of \(n\)-dimensional LPs, we learn an \(n k\) projection matrix with \(n>k\). When addressing a future LP instance, we reduce its dimensionality from \(n\) to \(k\) via the learned projection matrix, solve the resulting LP to obtain a \(k\)-dimensional solution, and apply the learned matrix to it to recover an \(n\)-dimensional solution. On the theoretical side, a natural question is: how much data is sufficient to ensure the quality of recovered solutions? We address this question based on the framework of _data-driven algorithm design_, which connects the amount of data sufficient for establishing generalization bounds to the _pseudo-dimension_ of performance metrics. We obtain an \(}(nk^{2})\) upper bound on the pseudo-dimension, where \(}\) compresses logarithmic factors. We also provide an \((nk)\) lower bound, implying our result is tight up to an \(}(k)\) factor. On the practical side, we explore two simple methods for learning projection matrices: PCA- and gradient-based methods. While the former is relatively efficient, the latter can sometimes achieve better solution quality. Experiments demonstrate that learning projection matrices from data is indeed beneficial: it leads to significantly higher solution quality than the existing random projection while greatly reducing the time for solving LPs.

## 1 Introduction

Linear programming (LP) has been one of the most fundamental tools used in various industrial domains , and how to address high-dimensional LPs efficiently has been a major research subject in operations research. To date, researchers have developed various fast LP solvers, most of which stem from the simplex or interior-point method. Recent advances include a parallelized simplex method  and a randomized interior-point method . Besides the improvements in LP solvers, there has been a growing interest in reducing LP sizes via _random projections_, motivated by the success of random sketching in numerical linear algebra . Such a projection-based approach is _solver-agnostic_ in that it can work with any solvers, including the aforementioned recent solvers, for solving reduced-size LPs. This solver-agnostic nature is beneficial, especially considering that LP solvers have evolved in distinct directions of simplex and interior-point methods.

In the context of numerical linear algebra, there has been a notable shift towards learning sketching matrices from data, instead of using random matrices . This data-driven approach is effective when we frequently address similar instances. The line of previous research has demonstrated that learned sketching matrices can greatly improve the performance of sketching-based methods.

### Our contribution

Drawing inspiration from this background, we study a _data-driven projection_ approach for accelerating repetitive solving of similar LP instances, which often arise in practice  (see also Remark 3.2). Our approach inherits the solver-agnostic nature of random projections for LPs, and it can improve solution quality by learning projection matrices from past LP instances. Our contribution is a cohesive study of this data-driven approach to LPs from both theoretical and practical perspectives, as follows.

**Generalization bound.** We first formulate the task of learning projection matrices as a statistical learning problem and study the generalization bound. Specifically, we analyze the number of LP-instance samples sufficient for bounding the gap between the empirical and expected objective values attained by the data-driven projection approach. Such a generalization bound is known to depend on the _pseudo-dimension_ of the class of performance metrics. We prove an \(}(nk^{2})\) upper bound on the pseudo-dimension (Theorem 4.4), where \(n\) and \(k\) are the original and reduced dimensionalities, respectively, and \(}\) compresses logarithmic factors. A main technical non-triviality lies in Lemma 4.3, which elucidates a piecewise polynomial structure of the optimal value of LPs as a function of input parameters. Besides playing a key role in proving Theorem 4.4, Lemma 4.3 offers general insight into the optimal value of LPs, which could have broader implications. We also give an \((nk)\) lower bound on the pseudo-dimension (Theorem 4.5). As experiments demonstrate later, we can get high-quality solutions with \(k\) much smaller than \(n\), suggesting our result, with only an \(}(k)\) gap, is nearly tight.

**Learning methods.** We then explore how to learn projection matrices in Section 5. We consider two simple learning methods based on principal component analysis (PCA) and gradient updates. The former efficiently constructs a projection matrix by extracting the top-\(k\) subspace around which optimal solutions of future instances are expected to appear. The latter, although more costly, directly improves the optimal value of LPs via gradient ascent. In Section 6, experiments on various datasets confirm that projection matrices learned by the PCA- and gradient-based methods can lead to much higher solution quality than random projection , while greatly reducing the time for solving LPs.

### Related work

**Random projections for LPs.** Vu et al.  introduced a random-projection method to reduce the number of equality constraints, and Poirion et al.  extended it to inequality constraints. As discussed therein, reducing the number of inequality constraints of LPs corresponds to reducing the dimensionality (the number of variables) of dual LPs. Recently, Akchen and Misic  developed a column-randomized method for reducing the dimensionality of LPs. While these studies provide high probability guarantees, we focus on data-driven projections and discuss generalization bounds.

**Data-driven algorithm design.**_Data-driven algorithm design_, initiated by Gupta and Roughgarden , has served as a foundational framework for analyzing generalization bounds of various data-driven algorithms [8; 10; 11; 12; 39; 9]. Our statistical learning formulation in Section 3 and a general proof idea in Section 4 are based on it. Among the line of studies, the analysis technique for data-driven integer-programming (IP) methods [10; 11] is close to ours. The difference is that while their technique is intended for analyzing IP methods (particularly, branch-and-cut methods), we focus on LPs and discuss a general property of the optimal value viewed as a function of input parameters. Thus, our analysis is independent of solution methods, unlike the previous studies. This aspect is crucial for analyzing our solver-agnostic approach. Some studies have also combined LP/IP methods with machine learning [14; 21; 41], while learning of projection matrices has yet to be studied.

**Learning through optimization.** Our work is also related to the broad stream of research on learning through optimization procedures [4; 47; 1; 13; 42; 36; 3; 20; 45; 19], which we discuss in Appendix A.

**Notation.** For a positive integer \(n\), let \(_{n}\) and \(_{n}\) be the \(n n\) identity matrix and the \(n\)-dimensional all-zero vector, respectively, where we omit the subscript when it is clear from the context. For two matrices \(\) and \(\) with the same number of rows (columns), \([,]\) (\([;]\)) denotes the matrix obtained by horizontally (vertically) concatenating \(\) and \(\).

## 2 Reducing dimensionality of LPs via projection

We overview the projection-based approach for reducing the dimensionality of LPs [37; 2]. For ease of dealing with feasibility issues, we focus on the following inequality-form LP with input parameters\(^{n}\), \(^{m n}\), and \(^{m}\):

\[_{^{n}}^{}. \]

When \(n\) is large, restricting variables to a low-dimensional subspace can be helpful for computing an approximate solution to (1) quickly. Specifically, given a _projection matrix_\(^{n k}\) with \(n>k\), we consider solving the following _projected LP_, instead of (1):

\[_{^{k}}^{} . \]

Once we get an optimal solution \(^{*}\) to the projected LP (2), we can recover an \(n\)-dimensional solution, \(}=^{*}\), to the original LP (1). Note that the recovered solution is always feasible for (1), although not always optimal. We measure the solution quality with the objective value \(^{}}=^{}^{*}\). Ideally, if \(\)'s columns span a linear subspace that contains an optimal solution to (1), the recovered solution \(}=^{*}\) is optimal to (1) due to the optimality of \(^{*}\) to (2). Therefore, if we find such a good \(\) close to being ideal with small \(k\), we can efficiently obtain a high-quality solution \(}=^{*}\) to (1) by solving the smaller projected LP (2).

**Remark 2.1** (Solver-specific aspects).: As mentioned in Section 1, this projection-based approach is solver-agnostic in that we can apply any LP solver to projected LPs (2). To preserve this nature, we focus on designing projection matrices and do not delve into solver-specific discussions. Experiments in Section 6 will use Gurobi as a fixed LP solver, which is a standard choice. Strictly speaking, projections alter the sparsity and numerical stability of projected LPs, which can affect the performance of solvers. This point can be important, especially when original LPs are sparse and solvers exploit the sparsity. Investigating how to take such solver-specific aspects into account is left for future work.

## 3 Data-driven projection

While the previous studies [44; 37; 2] have reduced LP sizes via random projections, we may be able to improve solution quality by learning projection matrices from data. We formalize this idea as a statistical learning problem. Let \(\) denote the set of all possible LP instances and \(\) an unknown distribution on \(\). Given LP instances sampled from \(\), our goal is to learn \(\) that maximizes the expected optimal value of projected LPs over \(\). Below, we assume the following three conditions.

**Assumption 3.1**.: _(i) Every \(\) takes the inequality form (1), (ii) \(=_{n}\) is feasible for all \(\), and (iii) optimal values of all instances in \(\) are upper bounded by a finite constant \(H>0\)._

Although Assumption 3.1 narrows the class of LPs we can handle, it is not as restrictive as it seems. Suppose for example that LP instances in \(\) have identical equality constraints. While such LPs in their current form do not satisfy (i), we can convert them into the inequality form (1) by considering the null space of the equality constraints (see Appendix C for details), hence satisfying (i). This conversion is useful for dealing with LPs of maximum-flow and minimum-cost-flow problems on a fixed graph topology, where we can remove the flow-conservation equality constraints by considering the null space of the incidence matrix of the graph. Regarding condition (ii), we may instead assume that there exists an arbitrary common feasible solution \(_{0}\) without loss of generality. This is because we can translate the feasible region so that \(_{0}\) coincides with the origin \(_{n}\). Condition (ii) also implies that for any \(^{n k}\), projected LPs are _feasible_ (i.e., their feasible regions are non-empty) since \(=_{k}\) is always feasible for any projected LPs. Condition (iii) is satisfied simply by focusing on _bounded_ LPs (i.e., LPs with finite optimal values) and setting \(H\) to the largest possible optimal value. Conditions (ii) and (iii) also ensure that the optimal value of projected LPs always lies in \([0,H]\), which is used to derive a generalization bound in Section 4. In Section 6, we see that many problems, including packing and network flow problems, can be written as LPs satisfying Assumption 3.1.

Due to condition (i), we can identify each LP instance \(\) with its input parameters \((,,)\) in (1), i.e., \(=(,,)\). For an LP instance \(\) and a projection matrix \(^{n k}\), we define

\[u(,)=\{^{}\,:\,\,\} \]

as the optimal value of the projected LP. Our goal is to learn \(^{n k}\) from LP instances sampled from \(\) to maximize the expected optimal value on future instances, i.e., \(_{}[u(,)]\).

**Remark 3.2** (Validity of the setting).: The above statistical learning setting regarding LP instances is not an artifact. As Fan et al.  discussed, LPs often serve as descriptive models, and each instance can be viewed as a realization of input parameters following some distribution. Such a scenario arises in, for example, daily production planning and flight scheduling. Note that the statistical learning setting is also widely used as a foundational framework in data-driven algorithm design [25; 8; 12; 9].

Generalization bound

This section studies the generalization bound, namely, how many samples from \(\) are sufficient for guaranteeing that the expected optimal value, \(_{}[u(,)]\), of learned \(\) is close to the empirical optimal value on sampled instances. First, let us overview the basics of learning theory. Let \(^{}\) be a class of functions, where each \(u\) takes some input \(\) and returns a real value. We use the following _pseudo-dimension_ to measure the complexity of a class of real-valued functions.

**Definition 4.1**.: Let \(N\) be a positive integer. We say \(^{}\)_shatters_ an input set, \(\{_{1},,_{N}\}\), if there exist threshold values, \(t_{1},,t_{N}\), such that each of all the \(2^{N}\) outcomes of \(\{\,u(_{i}) t_{i}\,:\,i=1,,N\,\}\) is realized by some \(u\). The _pseudo-dimension_ of \(\), denoted by \(()\), is the maximum size of an input set that \(\) can shatter.

In our case, the set \(\) consists of functions \(u(,):\), defined in (3), for all possible projection matrices \(^{n k}\). Each \(u(,)\) takes an LP instance \(=(,,)\) as input and returns the optimal value of the projected LP. Assumption 3.1 ensures that the range of \(u(,)\) is bounded by \([0,H]\) for all \(^{n k}\). Thus, the well-known uniform convergence result (see, e.g., Anthony and Bartlett [5, Theorem 19.2] and Balcan [7, Theorem 29.2]) implies that for any distribution \(\) on \(\), \(>0\), and \((0,1)\), if \(N=((H/)^{2}(()+(1/)))\) instances drawn i.i.d. from \(\) are given, with probability at least \(1-\), for all \(^{n k}\), it holds that

\[_{i=1}^{N}u(,_{i})-_{}[u(,)]. \]

That is, if a projection matrix \(\) produces high-quality solutions on \(N(H/)^{2}()\) instances sampled i.i.d. from \(\), it likely yields high-quality solutions on future instances from \(\) as well. Thus, analyzing \(()\) of \(=\{\,u(,):\,:\,^{n k}\,\}\) reveals the sufficient sample size.

**Remark 4.2** (Importance of uniform convergence).: While the above generalization bound is not the sole focus of learning theory, it is particularly valuable in data-driven algorithm design, as is also discussed in the literature . Note that (4) holds uniformly for all \(^{n k}\), offering performance guarantees _regardless of how \(\) is learned_. Thus, we may select learning methods based on their empirical performance. This is helpful since there are no gold-standard methods for learning parameters of algorithms; we discuss learning methods for our case in Section 5. This situation differs from the standard supervised learning setting, where we minimize common losses, e.g., squared and logistic. Additionally, the uniform bound ensures that learned \(\) does not overfit sampled instances.

### Upper bound on \(()\)

Building upon the above learning theory background, a crucial factor for establishing the generalization bound is \(()\). To upper bound this, we give a structural observation of the optimal value of LPs (Lemma 4.3) and combine it with a general proof idea in data-driven algorithm design .

We first overview the general proof idea. Suppose that we have an upper bound on the number of outcomes of \(\{\,u(,_{i}) t_{i}\,:\,i=1,,N\,\}\) that grows more slowly than \(2^{N}\). Since shattering \(N\) instances requires \(2^{N}\) outcomes, the largest \(N\), such that the upper bound is at least \(2^{N}\), serves as an upper bound on \(()\) (intuitively, \(()_{2}(\)"upper bound on the number of outcomes")). Below, we discuss bounding the number of outcomes, which is the most technically important step.

To examine the number of possible outcomes, we consider a fundamental question related to sensitivity analysis of LPs: _how does the optimal value of an LP behave when input parameters change?1_ In our case, a projected LP has input parameters \((^{},,)^{k}^{m k }^{m}\), where \(^{}\) and \(\) change with \(^{n k}\). Thus, addressing this question offers insight into the number of outcomes. Lemma 4.3 provides an answer for a more general setting, which might find other applications beyond our case since learning through LPs is not limited to the projection-based approach .

**Lemma 4.3**.: _Let \(t\) be a threshold value. Consider an LP \(=(},},})^{k} ^{m k}^{m}\) such that each entry of \(}\), \(}\), and \(}\) is a polynomial of degree at most \(d\) in \(\) real variables, \(^{}\).__Assume that \(\) is bounded and feasible for every \(^{}\). Then, there are up to \((m+2k+2)\) polynomials of degree at most \((2k+1)d\) in \(\) whose sign patterns (\(<0,=0\), or \(>0\)) partition \(^{}\) into some regions, and whether \(\{}^{}\,:\,}}\,\} t\) or not is identical within each region._

Proof.: First, we rewrite the LP \(=(},},})\) as an equivalent \(2k\)-dimensional LP with non-negativity constraints: \(\{\,}^{}(^{+}-^{-})\,:\,}(^ {+}-^{-})},\,[^{+};^{-}]\,\}\). The resulting constraint matrix, \(^{}[},-};-_{2k}]\), has full column rank, which simplifies the subsequent discussion. Note that the maximum degree of input parameters remains at most \(d\), while the sizes, \(m\) and \(k\), increase to \(m^{} m+2k\) and \(k^{} 2k\), respectively. Below, we focus on the reformulated LP \((^{},^{},^{})^{k^{}} ^{m^{} k^{}}^{m^{}}\), where \(^{}[};-}]\), \(^{}[};_{2k}]\), and \(^{}\) has full column rank.

We consider determining \(\{\,^{}\,:\,^{}^{}\,\} t\) or not by checking all vertices of the feasible region. For any size-\(k^{}\) subset, \(I\{1,,m^{}\}\), of row indices of \(^{}^{m^{} k^{}}\), let \(^{}{}_{I}\) denote the \(k^{} k^{}\) submatrix of \(^{}\) with rows restricted to \(I\) and \(^{}{}_{I}^{k^{}}\) the corresponding subvector of \(^{}\). For every subset \(I\) with \(^{}{}_{I} 0\), let \(_{I}^{}{}_{I}^{-1}^{}{}_{I}\). Since the LP is bounded and feasible, and \(^{}\) has full column rank, there is a vertex optimal solution written as \(_{I}=^{}{}_{I}^{-1}^{}{}_{I}\) for some \(I\) (see the proof of Korte and Vygen [31, Proposition 3.1]). Thus, the optimal value is at least \(t\) if and only if there exists at least one size-\(k^{}\) subset \(I\) with \(^{}{}_{I} 0\), \(^{}{}_{I}^{}\), and \(^{}_{I} t\).

Based on the above observation, we identify polynomials whose sign patterns determine \(\{\,^{}\,:\,^{}^{} \,\} t\) or not. For any subset \(I\), if \(^{}{}_{I} 0\), Cramer's rule implies that \(_{I}=^{}{}_{I}^{-1}^{}{}_{I}\) is written as \(_{I}()/^{}{}_{I}\), where \(_{I}()\) is some \(k^{}\)-valued polynomial vector of \(\) with degrees at most \(k^{}d\). Thus, we can check \(^{}_{I}^{}\) and \(^{}_{I} t\) by examining sign patterns of \(m^{}+1\) polynomials, \(^{}{}_{I}()-(^{}{}_{I})^{}\) and \(^{}_{I}()-t^{}{}_{I}\), whose degrees are at most \((k^{}+1)d\). Considering all the \(}\) choices of \(I\), there are \(}{k^{}}(m^{}+2)\) polynomials of the form \(^{}{}_{I}\), \(^{}{}_{I}()-(^{}{}_{I})^{}\), and \(^{}_{I}()-t^{}{}_{I}\) with degrees at most \((k^{}+1)d\) such that their sign patterns partition \(^{}\) into some regions, and \(\{\,^{}\,:\,^{}^{} \,\} t\) or not is identical within each region. Substituting \(m+2k\) and \(2k\) into \(m^{}\) and \(k^{}\), respectively, completes the proof. 

Lemma 4.3 states that the outcome of whether \(u(,)=\{\,^{}\,:\,\,\}\) exceeds \(t\) or not is determined by sign patterns of polynomials of \(\), and an upper bound on the sign patterns of polynomials is known as _Warren's theorem_, as detailed shortly. Combining them with the aforementioned general idea yields the following upper bound on \(()\).

**Theorem 4.4**.: \(()=(nk^{2} mk)\)_._

Proof.: Let \((,t)\) be a pair of an LP instance and a threshold value. Setting \(=\) and \(d=1\) in Lemma 4.3, we have up to \((m+2k+2)\) polynomials of degree at most \(2k+1\) whose sign patterns determine whether \(u(,) t\) or not. Thus, given \(N\) pairs of input instances and threshold values, \((_{i},t_{i})_{i=1}^{N}\), we have up to \(N(m+2k+2)\) polynomials whose sign patterns determine \(u(,_{i}) t_{i}\) or not for all \(i=1,,N\), i.e., outcomes of \(N\) instances.

Warren's theorem states that given \(\) polynomials of \(\) variables with degrees at most \(\), the number of all possible sign patterns is at most \((8e/)^{}\) (see also Goldberg and Jerrum [24, Corollary 2.1]). In our case, the number of polynomials is \(=N(m+2k+2)\), and each of them has \(=nk\) variables (\(\)'s entries) and degrees at most \(=2k+1\). Thus, the number of all possible outcomes is at most \((8eN)^{nk}( )^{nk}(m,k)^{nk^{2}}\). To shatter the set of \(N\) instances, the right-hand side must be at least \(2^{N}\). Taking the base-2 logarithm, it must hold that \(N nk_{2}+(nk^{2} mk)N +(nk^{2} mk)\), where we used \(x_{2}\) for \(x>0\). Therefore, \(\) can shatter \((nk^{2} mk)\) instances, obtaining the desired bound on \(()\). 

### Lower bound on \(()\)

We then provide an \((nk)\) lower bound on \(()\) to complement the above \(}(nk^{2})\) upper bound, implying the tightness up to an \(}(k)\) factor. See Appendix B for the proof.

**Theorem 4.5**.: \(()=(nk)\)Our proof indeed gives the same lower bound on the \(\)_-fat shattering dimension_ for \(<1/2\), which implies a lower bound of \((nk/)\) on \(N\), the sample size needed to guarantee (4) [5, Theorem 19.5]. Thus, in terms of the sample complexity, our result is tight up to an \(}(k/)\) factor. The \(1/\) gap is inevitable in general [5, Section 19.5], while closing the \(}(k)\) gap is an interesting open problem.

## 5 Learning methods

We then discuss how to learn projection matrices from training datasets. From the bound (4), given \(N\) training LP instances, the expected solution quality on future instances likely remains within the range of \(\) from the empirical one, where \( H()/N} Hk\) due to Theorem 4.4, _regardless of how we learn a projection matrix \(\)_. Therefore, in practice, we only need to find an empirically good projection matrix \(\), which motivates us to explore various ideas for learning \(\). Below, we discuss two natural ideas: PCA- and gradient-based methods.

**Remark 5.1** (Training time).: We emphasize that learning methods are used only before addressing future LP instances and not once a projection matrix \(\) is learned. Hence, they can take much longer than the time for solving new LP instances. Similarly, we suppose that optimal solutions to training instances are available, as we can compute them a priori. Note that similar premises are common in most data-driven algorithm research [28; 14; 12; 9; 21; 41]. Considering this, our learning methods are primarily intended for conceptual simplicity, not for efficiency. For completeness, we present the theoretical time complexity and the training time taken in the experiments in Appendix E.

### PCA-based method

As described in Section 2, a projection matrix \(\) should preferably have columns that span a low-dimensional subspace around which future optimal solutions will appear. Hence, a natural idea is to use PCA to extract such a subspace, regarding optimal solutions to training instances as data points.

Formally, let \(^{N n}\) be a matrix whose \(i\)th row is an optimal solution to the \(i\)th training instance. We apply PCA to this \(\). Specifically, we subtract the mean, \(}=^{}_{N}\), from each row of \(\) and apply the singular value decomposition (SVD) to \(-_{N}}^{}\), obtaining a decomposition of the form \(^{}=-_{N}}^{}\). Let \(_{k-1}^{n(k-1)}\) be the submatrix of \(\) whose columns are the top-\((k-1)\) right-singular vectors of \(-_{N}}^{}\). We use \(=[},_{k-1}]^{n k}\) as a projection matrix. Here, \(}\) is concatenated due to the following consideration: since \(_{k-1}\) is designed to satisfy \(_{k-1}^{}^{}-}_{N}^{}\) for some \(^{}^{(k-1) N}\), we expect \([},_{k-1}]^{}\) to hold for some \(^{k N}\), hence \(=[},_{k-1}]\). This method is not so costly when optimal solutions to training LP instances are given, as it only requires finding the top-\((k-1)\) right-singular vectors of \(-_{N}}^{}\).

### Gradient-based method

While the PCA-based method aims to extract the subspace into which future optimal solutions are likely to fall, it only uses optimal solutions and discards input parameters of LPs. As a complementary approach, we provide a gradient-based method that directly improves the optimal value of LPs.

As a warm-up, consider maximizing \(u(,)=\{\,^{}\,:\,\,\}\) of a single LP instance \(=(,,)\) via gradient ascent. Assume that the projected LP satisfies a _regularity condition_, which requires the existence of an optimal solution \(^{*}\) at which active constraints are linearly independent. Then, \(u(,)\) is differentiable in \(\) and the gradient is expressed as follows [42, Theorem 1] (see Appendix D for details of the derivation):

\[ u(,)=^{*}-^{}^{*} ^{*}, \]

where \(^{*}^{m}\) is a dual optimal solution. Thus, we can use the gradient ascent method to maximize \(u(,)\) under the regularity condition. However, this condition is sometimes prone to be violated, particularly when _Slater's condition_ does not hold (i.e., there is no strictly feasible solution). For example, if the original LP has a constraint \(_{n}\) and every column of \(\) has opposite-sign entries, it is likely that only \(=_{k}\) satisfies \(_{n}\) by equality, which is the unique optimal solution but not strictly feasible. In this case, the regularity condition is violated since all rows of \(^{n k}\) are active at \(_{k}\) and linearly dependent due to \(n>k\). To alleviate this issue, we apply the following projection for \(j=1,,k\) before computing the gradient in (5):

\[_{:,j}_{^{n}}\{\,\|-_{:,j} \|_{2}\,:\,\,\}, \]

where \(_{:,j}\) denotes the \(j\)th column of \(\). This minimally changes each column \(_{:,j}\) to satisfy the original constraints. Consequently, any convex combination of \(\)'s columns is feasible for the original LP, increasing the chance that there exists a strictly feasible solution in \(\{\,^{k}\,:\,\,\}\), although it is not guaranteed. This improves the likelihood that the regularity condition is satisfied.

Given \(N\) training instances, \(_{1},,_{N}\), we repeatedly update \(\) as with SGD: for each \(_{i}\), we iterate to compute the gradient (5) and to update \(\) with it. The projection (6) onto the feasible region of \(_{i}\) comes before computing the gradient for \(_{i}\). We call this method SGA (stochastic gradient ascent).

### Final projection for feasibility

The previous discussion suggests that making each column of \(\) feasible for training LP instances can increase the likelihood that future LP instances projected by \(\) will have strictly feasible solutions. Considering this, after obtaining a projection matrix \(\) with either the PCA- or gradient-based method, we project each column of \(\) onto the intersection of the feasible regions of training LP instances, which we call the _final projection_. This can be done similarly to (6) replacing the constraints with \([_{1};;_{N}][_{1};;_{N}]\). If \(_{1},,_{N}\) are identical, we can do it more efficiently by replacing the constraints with \(_{1}\{_{1},,_{N}\}\), where the minimum is taken element-wise. Note that although the final projection can be costly for large \(N\), we need to do it only once at the end of learning \(\). This final projection never fails since \(_{n}\) is always feasible as in Assumption 3.1.

## 6 Experiments

We experimentally evaluate the data-driven projection approach.2 We used MacBook Air with Apple M2 chip, 24 GB of memory, and macOS Sonoma 14.1. We implemented algorithms in Python 3.9.7 with NumPy 1.23.2. We used Gurobi 10.0.1  for solving LPs and computing projection in (6). We used the following three synthetic and five realistic datasets, each of which consists of \(300\) LP instances (\(200\) for training and \(100\) for testing). Table 1 summarizes LP sizes of the eight datasets.3

**Synthetic datasets.** We consider three types of LPs representing packing, maximum flow, and minimum-cost flow problems, denoted by Packing, MaxFlow, and MinCostFlow, respectively. A packing problem is an LP with non-negative parameters \(\), \(\), and \(\). We created a base instance by drawing their entries from the uniform distribution on \(\) and multiplying \(\) by \(n\). We then obtained 300 random instances by multiplying all input parameters by \(1+\), where \(\) was drawn from the uniform distribution on \([0,0.1]\). To generate MaxFlow and MinCostFlow LPs, we first randomly created a directed graph with \(500\) arcs and fixed source and sink vertices, denoted by \(s\) and \(t\), respectively. We confirmed there was an arc from \(s\) to \(t\) to ensure feasibility. We set base arc capacities to \(1\), which we perturbed by multiplying \(1+\) with \(\) drawn from the uniform distribution on \([0,0.1]\), thus obtaining 300 MaxFlow instances. For MinCostFlow, we set supply at \(s\) and demand at \(t\) to \(1\). We set base arc costs to \(1\) for all arcs but \((s,t)\), whose cost was fixed to be large enough, and perturbed them similarly using \(1+\) to obtain 300 MinCostFlow instances. We transformed MaxFlow and MinCostFlow instances into equivalent inequality-form LPs with a method given in Appendix C, which requires a (trivially) feasible solution \(_{0}\). For MaxFlow, we used \(_{0}=\) (i.e., no flow) as a trivially feasible solution. For MinCostFlow, we let \(_{0}\) be all zeros but a single \(1\) at the entry corresponding to \((s,t)\), which is a trivially feasible (but costly) solution.

    & Packing & MaxFlow & MinCostFlow & GROW7 & ISRAEL & SC205 & SCAGR25 & STAIR \\  \(m\) & 50 & 1000 & 1000 & 581 & 316 & 317 & 671 & 696 \\ \(n\) & 500 & 500 & 500 & 301 & 142 & 203 & 500 & 467 \\   

Table 1: Sizes of inequality-form LPs, where \(m\) (\(n\)) represents the number of constraints (variables).

**Realistic datasets.** We used five LPs in Netlib , GROW7, ISRAEL, SC205, SCAGR25, and STAIR. For each, we generated datasets of 300 random instances. To create realistic datasets, we made them contain 2% of outliers as follows. For normal 98% data points, we perturbed coefficients of objective functions by multiplying \(1+0.1\), where \(\) was drawn from the normal distribution; for 2% outliers, we perturbed them by multiplying \(1+\), i.e., 10 times larger noises. Except for ISRAEL, the LPs have equality constraints. We transformed them into inequality-form LPs as described in Appendix C, using \(_{0}\) found by the initialization procedure of Gurobi's interior-point method.4

**Methods.** We compared four methods, named Full, ColRand, PCA, and SGA. The first two are baseline methods, while the latter two are our data-driven projection methods. Note that all four methods solved LPs with Gurobi, the state-of-the-art commercial solver. The only difference among them lies in how to reduce the dimensionality of LPs, as detailed below.

**Full:** a baseline method that returns original \(n\)-dimensional LPs without reducing the dimensionality.

**ColRand:** a column-randomized method based on the work by Akchen and Misic , which reduces the dimensionality by selecting \(k\) out of \(n\) variables randomly and fixing the others to zeros.

**PCA:** the PCA-based method that reduces the dimensionality with a projection matrix \(\) learned as in Section 5.1, followed by the final projection described in Section 5.3.

**SGA:** the gradient-based method that learns \(\) as described in Section 5.2, followed by the final projection as with PCA. We initialized \(\) with that obtained by PCA and conducted a single epoch of training, setting the learning rate to \(0.01\).5

For ColRand, PCA, and SGA, we used increasing values of the reduced dimensionality, \(k=,2,\), until it reached the maximum value no more than \(\), i.e., up to 10% of the original dimensionality. PCA and SGA learned projection matrices \(\) from \(N=200\) training instances, which were then used to reduce the dimensionality of 100 test instances. For ColRand, we tried \(10\) independent choices of \(k\) variables and recorded the average and standard deviation.

**Results.** Figure 1 shows how the solution quality and running time of Gurobi differ among the four methods, where "objective ratio" means the objective value divided by the optimal value computed by Full. For all datasets except STAIR, PCA and/or SGA with the largest \(k\) achieved about 95% to 99% objective ratios, while being about 4 to 70 times faster than Full. Regarding STAIR, PCA and SGA attained 13.1% and 51.2% objective ratios, respectively. By stark contrast, ColRand resulted in objective ratios close to zero in most cases except for Packing and ISRAEL. The results suggest that given informative training datasets, data-driven projection methods can lead to significantly better solutions than the random projection method. Regarding running times, there were differences between PCA/SGA and ColRand, which were probably caused by the numerical property of Gurobi. Nonetheless, all of the three were substantially faster than Full. In summary, the data-driven projection methods achieve high solution quality while greatly reducing the time for solving LPs.

Comparing PCA and SGA, SGA achieved better objectives than PCA in Packing, MaxFlow, MinCostFlow, and STAIR, while performing similarly in GROW7 and SC205. In ISRAEL and SCAGR25, SGA was worse than PCA, but this is not surprising since optimizing \(u(,)\) is a non-convex problem. The results suggest that no method could be universally best. Fortunately, the generalization bound (4) justifies selecting a learning method based on empirical performance. Specifically, if we adopt a learning method that produces \(\) with the best empirical performance on \(N\) instances at hand, its expected performance on future instances is likely to stay within the range of \(\) of the empirical one, where \( H()/N} Hk\) since \(()=}(nk^{2})\) due to Theorem 4.4. If \(N\) is sufficiently large, the high empirical performance is expected to be maintained on future instances.

Additionally, we examined the effect of the noise strength on the performance of PCA and SGA using the synthetic datasets. The details of the experiment and the results are shown in Appendix G. Therein, we found that PCA and SGA were robust against noise on capacities and costs in MaxFlow and MinCostFlow datasets. This is probably because they can exploit fixed topologies of underlying graphs, even if the capacities and costs are largely perturbed. Fixed graph topologies are common in real-world LPs, such as those appear in transportation planning. Our data-driven projection methods can be effective in such scenarios, particularly if sufficiently large datasets of such LP instances are available.

## 7 Conclusion

We have studied the data-driven projection approach to LPs. We have established a generalization bound by proving an \(}(nk^{2})\) upper bound on the pseudo-dimension and complemented it by an \((nk)\) lower bound. We have also proposed PCA- and gradient-based learning methods and experimentally evaluated them. Our theoretical and empirical findings lay the groundwork for the further development of the data-driven approach to LPs and contribute to the broader trend of AI/ML for optimization .

## 8 Limitations and discussions

Our work is limited to the statistical learning setting with assumptions on LP instances (see Section 3 and Assumption 3.1). In particular, the current approach cannot deal with equality constraints varying across instances since they usually make LP instances have no common feasible solution. Despite the narrowed applicability, we believe our setting is a reasonable starting point for developing the data-driven projection approach to LPs, as is also discussed in Remark 3.2 and the paragraph following Assumption 3.1. Overcoming these limitations will require more involved methods, such as training neural networks to extract meaningful low-dimensional subspaces from non-i.i.d. messy LP instances.

Our learning methods are not efficient, and applying them to huge LPs in practice might be challenging. Similar challenges are common in most data-driven algorithm research, as discussed in Remark 5.1, and we believe our conceptually simple learning methods are helpful for future research. Regarding the data-driven approach to low-rank approximation, Indyk et al.  found that a few-shot learning method is useful for efficiently learning sketching matrices. A key ingredient in their method is a

Figure 1: Plots of objective ratios (upper) and Gurobi’s running times (lower, semi-log) for Full, ColRand, PCA, and SGA averaged over 100 test instances. The error band of ColRand indicates the standard deviation over \(10\) independent trials. The results of Full are shown for every \(k\) for reference, although it always solves \(n\)-dimensional LPs and hence is independent of \(k\).

surrogate loss, which enjoys a consistency guarantee and whose gradient can be computed efficiently. Empirically, they found that minimizing this loss through only a few iterations of SGD yields a good sketching matrix. We expect that similar ideas will be effective for learning projection matrices for LPs efficiently, while how to design surrogate losses in our setting is left for future work. For the same reason, our experiments are limited to moderate-size LPs, as mentioned in Footnote 3. Nevertheless, the results sufficiently serve as a proof of concept of the data-driven projection approach.

There also exist general limitations of the projection-based approach . First, it does not consider solver-specific aspects, including numerical stability and sparsity, as discussed in Remark 2.1. Second, the projection-based approach has a limited impact on the theoretical time complexity. The theoretical time complexity of the projection-based approach is dominated by two factors: multiplying \(\) to reduce the dimensionality and solving the projected LP. Recent theoretical studies have revealed that solving an LP takes asymptotically the same computation time as matrix multiplication , suggesting projections may not contribute to improving the total theoretical time complexity. Nevertheless, the projection-based approach leads to dramatic speedups in practice, as in Figure 1. Moreover, it can be even faster beyond the theoretical implications when GPUs are available. It is noteworthy that the projection-based approach largely benefits from GPUs, as matrix multiplication can be highly parallelized. An exciting future direction is to combine recent GPU-implemented LP solvers  with projections, which will have vast potential for solving huge LPs efficiently. Exploring data-driven projections for reducing the number of constraints will also be interesting, while this involves addressing the feasibility issue. Poirion et al.  used random projections for reducing the number of inequality constraints, which would provide useful insights into this direction.