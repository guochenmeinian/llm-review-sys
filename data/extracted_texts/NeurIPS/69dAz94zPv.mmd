# Logarithmic-Regret Quantum Learning Algorithms

for Zero-Sum Games

 Minbo Gao

State Key Laboratory of Computer Science,

Institute of Software, Chinese Academy of Sciences,

Beijing, China

and University of Chinese Academy of Sciences,

Beijing, China

gmbl17@tsinghua.org.cn

&Zhengfeng Ji

Department of Computer Science

and Technology,

Tsinghua University,

Beijing, China

jizhengfeng@tsinghua.edu.cn

&Tongyang Li

Center on Frontiers of Computing Studies,

and School of Computer Science,

Peking University,

Beijing, China

tongyangli@pku.edu.cn

&Qisheng Wang

Graduate School of Mathematics,

Nagoya University,

Nagoya, Japan

QishengWang1994@gmail.com

###### Abstract

We propose the first online quantum algorithm for solving zero-sum games with \((1)\) regret under the game setting.1 Moreover, our quantum algorithm computes an \(\)-approximate Nash equilibrium of an \(m n\) matrix zero-sum game in quantum time \((/^{2.5})\). Our algorithm uses standard quantum inputs and generates classical outputs with succinct descriptions, facilitating end-to-end applications. Technically, our online quantum algorithm "quantizes" classical algorithms based on the _optimistic_ multiplicative weight update method. At the heart of our algorithm is a fast quantum multi-sampling procedure for the Gibbs sampling problem, which may be of independent interest.

## 1 Introduction

Nash equilibrium is one of the most important concepts in game theory. It characterizes and predicts rational agents' behaviors in non-cooperative games, finding a vast host of applications ranging from analyzing wars  and designing auctions , to optimizing networks .

It was shown in Daskalakis et al. , Chen and Deng  that finding a Nash equilibrium is \(\)-hard for general games. Nevertheless, computing the Nash equilibrium for specific types of games, such as zero-sum games, is particularly interesting. A zero-sum game requires that the utility of one player is the opposite of the other's, a condition that often appears in, for example, chess games. Von Neumann's minimax theorem  promises that every finite two-player zero-sum game has optimal mixed strategies.

Zero-Sum Games.For a two-player zero-sum game represented by an \(m n\) matrix \(\), the Nash equilibrium is the solution pair \((x,y)\) to the following min-max problem:

\[_{x_{m}}_{y_{n}}x^{}y,\]

where \(x_{m}\) and \(y_{n}\) are \(m\)- and \(n\)-dimensional probability distributions, respectively. Usually, we are satisfied with an approximate Nash equilibrium rather than an exact one. An \(\)-approximate Nash equilibrium is a solution pair \((x,y)\) such that:

\[_{y^{}_{n}}x^{}y^{}-_{x^{ }_{m}}x^{}y.\]

Online Learning.Since the matrix \(\) of the zero-sum game usually has a large dimension in practice, it is common that we trade accuracy for space and time efficiency. Thus, online learning becomes increasingly significant in these scenarios. Online learning studies the situation when data is only available in sequential order and aims at making good decisions in this setup. In evaluating online learning algorithms, regret is an important criterion that measures how good an algorithm is compared with the optimal static loss (see more details in Section 2.3).

The idea of the online learning algorithms for zero-sum games stems from repeated play in game theory, e.g., fictitious play . Specifically, we simulate the actions of two players for multiple rounds. In each round, players make decisions using a no-regret learning algorithm, considering the opponent's previous actions. For example, a famous algorithm of this type was proposed in Grigoriadis and Khachiyan  inspired by the exponential Hedge algorithm. The algorithm has regret \(()\) and \(T\) rounds, establishing the convergence rate of \((1/)\).

It takes about two decades before the next improvement in Daskalakis et al.  to happen, where the authors proposed a strongly-uncoupled algorithm, achieving \((1)\) total regret if both players use the algorithm. They used the technique of minimizing non-smooth functions using smoothed approximations proposed in Nesterov , and this technique was later developed in Nesterov , Nemirovski  for broader classes of problems. Later, it was found in Syrgkanis et al.  that the optimistic multiplicative weight algorithm also leads to \((1)\) total regret with regret bounded by variation in utilities; this algorithm was recently extended to correlated equilibria in multi-player general-sum games in Anagnostides et al. . It was proved in Hsieh et al.  that optimistic mirror descent with a time-varying learning rate can also achieve \((1)\) total regret for multi-players. Our quantum algorithm follows the overall idea of the optimistic multiplicative weight update and the regret bounded by variation methods .

Quantum Computing and Learning.Quantum computing has been rapidly advancing in recent years. Specifically, many machine learning problems are known to have significant quantum speedups, e.g., support vector machines , principal component analysis , classification , etc. The combination of quantum computing and online learning has recently become a popular topic. For instance, online learning tools have been applied to solving semidefinite programs (SDPs) with quantum speedup in the problem dimension and the number of constraints . In addition, as an important quantum information task, the online version of quantum state learning has been systematically developed with good theoretical and empirical guarantees .

For finding the Nash equilibrium of zero-sum games, a quantum algorithm was proposed in van Apeldoorn and Gilyen  by "quantizing" the classical algorithm in Grigoriadis and Khachiyan , achieving a quadratic speedup in the dimension parameters \(m\) and \(n\). At the same time, quantum algorithms for training linear and kernel-based classifiers were proposed in Li et al. , which have similar problem formulations to zero-sum games. Recently, an improved quantum algorithm for zero-sum games was proposed in Bouland et al.  using dynamic Gibbs sampling. All of the above quantum algorithms are based on the multiplicative weight update method, and as a consequence, they all share the \(O()\) regret bound.

### Main Result

Our result in this paper establishes a positive answer to the following open question: _Does there exist a learning algorithm with \((1)\) regret allowing quantum speedups?_

Inspired by the optimistic follow-the-regularized-leader algorithm proposed in Syrgkanis et al. , we propose a sample-based quantum online learning algorithm for zero-sum games with \(O((mn))\) total regret, which is near-optimal. If we run this algorithm for \(T\) rounds, it will compute an \((1/T)\)-approximate Nash equilibrium with high probability, achieving a quadratic speedup in dimension parameters \(m\) and \(n\). Formally, we have the following quantum online learning algorithm:

**Theorem 1.1** (Online learning for zero-sum games).: _Suppose \(T(m+n)\). There is a quantum online algorithm for zero-sum game \(^{m n}\) with \(\|\| 1\) such that it achieves a total regret of \(O((mn))\) with high probability after \(T\) rounds, while each round takes quantum time \((T^{1.5})\)._

Our algorithm does not need to read all the entries of the input matrix \(\) at once. Instead, we assume that our algorithm can query its entries when necessary. The input model is described as follows:

* Classically, given any \(i[m],j[n]\), the entry \(A_{i,j}\) can be accessed in \((1)\) time.
* Quantumly, we assume that the entry \(A_{i,j}\) can be accessed in \((1)\) time _coherently_.

This is the _standard quantum input model_ for zero-sum games adopted in previous literature . See more details in Section 2.4.

In addition, same as prior works , our algorithm outputs _purely classical vectors_ with succinct descriptions because they are sparse (with at most \(T^{2}\) nonzero entries). Overall, using standard quantum inputs and generating classical outputs significantly facilitate end-to-end applications of our algorithm in the near term.

As a direct corollary, we can find an \(\)-_approximate Nash equilibrium_ by taking \(T=(1/)\), resulting in a quantum speedup stated as follows. A detailed comparison to previous literature is presented in Table 1.

**Corollary 1.2** (Computing Nash equilibrium).: _There is a quantum online algorithm for zero-sum game \(^{m n}\) with \(\|A\| 1\) that, with high probability, computes an \(\)-approximate Nash equilibrium in quantum time \((/^{2.5})\).4_

Quantum Lower Bounds.In the full version of , they showed a lower bound \(()\) for the quantum query complexity of computing an \(\)-approximate Nash equilibrium of zero-sum games for constant \(\). Therefore, our algorithm is tight in terms of \(m\) and \(n\).

   Approach & Type & Regret & Update Cost Per Round & Classical/Quantum Time Complexity \\ 
 & Classical & \(()\) & \((m+n)\) & \(((m+n)/^{2})\) \\
 & Quantum & \(()\) & \((/)\) & \((/^{3})\) \\
 & Quantum & \(()\) & \((/^{0.5})\) & \((/^{2.5})\) 2  \\ 
 & Classical & \((1)\) & \((mn)\) & \((mn/)\) \\
 & Classical & \((1)\) & \(()\) & \((mn+/)\) \\ Our result 3  & Quantum & \((1)\) & \((/^{1.5})\) & \((/^{2.5})\) \\   

Table 1: Online Algorithms for \(\)-Approximate Nash Equilibria of Zero-Sum Games.

### Our Techniques

Our quantum online algorithm is a stochastic modification of the optimistic multiplicative weight update proposed by Syrgkanis et al. . We choose to "quantize" the optimistic online algorithm because it has a better convergence rate for zero-sum games than general multiplicative weight update algorithms. During the update of classical online algorithms, the update term (gradient vector) is computed in linear time by arithmetic operations. However, we observe that it is not necessary to know the exact gradient vector. This motivates us to apply stochastic gradient methods for updates so that certain fast quantum samplers can be utilized here and bring quantum speedups.

Specifically, in our quantum online algorithm, we need to establish an upper bound on the expectation of the total regret of our stochastic update rule, and also deal with errors that may appear from noisy samplers. To reduce the variance of the gradient estimator, we need multiple samples (from Gibbs distributions) at a time. To this end, we develop a fast quantum multi-Gibbs sampler that produces multiple samples by preparing and measuring quantum states.

Sample-Based Optimistic Multiplicative Weight Update.Optimistic online learning adds a "prediction loss" term to the cumulative loss for regularized minimization, giving a faster convergence rate than the non-optimistic versions for zero-sum games. Arora et al.  surveyed the use of the multiplicative weight update method in various domains, but little was known for the optimistic learning method at that time. Daskalakis et al.  proposed an extragradient method that largely resembles the optimistic multiplicative weight. Syrgkanis et al.  gave a characterization of this update rule--RVU (Regret bounded by Variation in Utilities) property, which is very useful in proving regret bounds. Subsequently, the optimistic learning method is applied to other areas, including training GANs  and multi-agent learning .

However, when implementing the optimistic learning methods, we face a fundamental difficulty--we cannot directly access data from quantum states without measurement. To resolve this issue, we get samples from the desired distribution and use them to estimate the actual gradient. This idea is commonly seen in previous literature on quantum SDP solvers [7; 50; 8; 49]. Then we prove the regret bound (see Theorem 3.2) of our algorithm by showing that it has a property similar to the RVU property . Moreover, we need multiple samples to obtain a small "variance" of the stochastic gradient (by taking the average of the samples), to ensure that the expected regret is bounded. Our fast quantum multi-Gibbs sampler produces the required samples and ensures further quantum speedups. In a nutshell, we give an algorithm (Algorithm 1) which modifies the optimistic multiplicative weight algorithm in Syrgkanis et al.  to fit the quantum implementation. This is _the first quantum algorithm that implements optimistic online learning_ to the best of our knowledge.

Fast Quantum Multi-Gibbs Sampling.The key to our sample-based approach is to obtain multiple samples from the Gibbs distribution after a common preprocessing step. For a vector \(p^{n}\) with \(_{i[n]}|p_{i}|\), a sample from the Gibbs distribution with respect to \(p\) is a random variable \(j[n]\) such that \([j=l]=)}{_{i=1}^{n}(p_{i})}\). Gibbs sampling on a quantum computer was first studied in Poulin and Wocjan , and was later used as a subroutine in quantum SDP solvers [7; 50; 8; 49]. However, the aforementioned quantum Gibbs samplers produce one sample from an \(n\)-dimensional Gibbs distribution in quantum time \(()\); thus, we can produce \(k\) samples in quantum time \(( k)\). Inspired by the recent work Hamoudi  about preparing multiple samples of quantum states, we develop a fast quantum Gibbs sampler (Theorem 4.2) which _produces \(k\) samples from a Gibbs distribution in quantum time \(()\)_. Our quantum multi-Gibbs sampling may have potential applications in sample-based approaches for optimization tasks that require multiple samples.

Technically, the main idea is based on quantum rejection sampling [24; 39], where the target quantum state \(|u\) is obtained by post-selection from a quantum state \(|u_{}\) that is easy to prepare (see Section 4). The algorithm has the following steps (Here we assume \(=O(1)\) for simplicity): To bring \(|u_{}\) closer to the target \(|u\), we find the \(k\) (approximately) most dominant amplitudes of \(|u\) by quantum \(k\)-maximum finding  in quantum time \(()\). In quantum \(k\)-maximum finding, we need to estimate the amplitudes of \(|u\) and compare them coherently, which requires the estimation should be consistent. To address this issue, we develop consistent quantum amplitude estimation (see Appendix C) for our purpose based on consistent phase estimation [2; 47], which is of independent interest. Then, we correct the tail amplitudes of \(|u_{}\) by quantum singularvalue transformation , resulting in an (approximate) target state with amplitude \(()\) (see Appendix D for details). Finally, we post-select the target state by quantum amplitude amplification  in quantum time \(()\). This follows that \(k\) samples of the target quantum state can be obtained in \(k()=()\) time.

We believe that our technique can be extended to a wide range of distributions whose mass function is monotonic and satisfies certain Lipschitz conditions.

## 2 Preliminaries

### General Mathematical Notations

For convenience, we use \([n]\) to denote the set \(\{1,2,,n\}\). We use \(e_{i}\) to denote a vector whose \(i\)-th coordinate is \(1\) and other coordinates are \(0\). For a vector \(v^{n}\), \(v_{i}\) is the \(i\)-th coordinate of \(v\). For a function \(f\), we write \(f(v)\) to denote the result of \(f\) applied to its coordinates, i.e., \(f(v)=(f(v_{1}),f(v_{2}),,f(v_{n}))\). We use \(_{n}\) to represent the set of \(n\)-dimensional probability distributions, i.e., \(_{n}:=\{v^{n}:_{i=1}^{n}v_{i}=1, i[n],v_{i}  0\}\). Here the \(i\)-th coordinate represents the probability of event \(i\) takes place. We use \(\|\|\) for vector norms. The \(l_{1}\) norm \(\|\|_{1}\) for a vector \(v^{n}\) is defined as \(\|v\|_{1}:=_{i=1}^{n}|v_{i}|\). For two \(n\)-dimensional probability distributions \(p,q_{n}\), their total variance distance is defined as: \(d_{}(p,q)=\|p-q\|_{1}=_{i=1}^{ n}|p_{i}-q_{i}|\). We will use \(^{m n}\) to denote a matrix with \(m\) rows and \(n\) columns. \(A_{i,j}\) is the entry of \(\) in the \(i\)-th row and \(j\)-th column. \(\|\|\) denotes the operator norm of matrix \(\). For a vector \(v^{n}\), we use \((v)\) to denote the diagonal matrix in \(^{n n}\) whose diagonal entries are coordinates of \(v\) with the same order.

### Game Theory

Let us consider two players, Alice and Bob, playing a zero-sum game represented by a matrix \(^{m n}\) with entries \(A_{i,j}\), where \([m]\) is the labeled action set of Alice and \([n]\) of Bob. Usually, Alice is called the row player and Bob is called the column player. \(A_{i,j}\) is the payoff to Bob and \(-A_{i,j}\) is the payoff to Alice when Alice plays action \(i\) and Bob plays action \(j\). Both players want to maximize their payoff when they consider their opponent's strategy.

Consider the situation that both players' actions are the best responses to each other. In this case, we call the actions form a _Nash equilibrium_. A famous minimax theorem by von Neumann  states that we can exchange the order of the min and max operation and thus the value of the Nash equilibrium of the game can be properly defined. To be more exact, we have: \(_{x_{m}}_{y_{m}}x^{}y=_{y _{m}}_{x_{m}}x^{}y\). Here the value of the minimax optimization problem is called the _value of the game_. Our goal is to find an approximate Nash equilibrium for this problem. More formally, we need two probabilistic vectors \(x_{m},y_{n}\) such that the following holds: \(_{y^{}_{m}}x^{}y^{}-_{x^{ }_{m}}x^{}y\). We will call such a pair \((x,y)\) an \(\)-_approximate Nash Equilibrium_ for the two-player zero-sum game \(\). Computing an \(\)-approximate Nash for a zero-sum game is not as hard as for a general game. For a general game, approximately computing its Nash equilibrium is \(\)-hard [16; 12].

Here, we emphasize an important observation that will be used throughout our paper: we can add a constant or multiply by a positive constant on all \(\)'s entries without changing the solution pair of the Nash equilibrium. This is because, in the definition of the Nash equilibrium, the best response is always in comparison with other possible actions, so only the order of the utility matters. Thus without loss of generality, we can always rescale \(\) to \((+c)/2\) where \(\) is an \(m n\) matrix with all entries being \(1\) with \(c\) being the largest absolute value of \(\)'s entries, and let \(^{}=/\|\|\) to guarantee that \(\) has non-negative entries and has operator norm no more than \(1\).

### Online Learning

#### 2.3.1 Notions of online learning

In general, online learning focuses on making decisions in an uncertain situation, where a decider is not aware of the current loss and is required to make decisions based on observations of previous losses. To be more exact, we fix the number of rounds \(T\) and judge the performance of the algorithm (in the following of this subsection we use "decider" with the same meaning as the "algorithm") in these \(T\) rounds. Assume that the decider is allowed to choose actions in the domain \(\), usually a convex subset of a finite-dimensional Euclidean space. Let \(t[T]\) denote the current number of rounds. At round \(t\), the decider chooses an action \(x_{t}\). (The action may depend on the decider's previous observation of the loss functions \(l_{i}\) for all \(i[t-1]\).) Then the decider will get the loss \(l_{t}(x_{t})\) for the current round. We assume that the decider can also observe the full information of \(l_{t}\), i.e., the formula of the function. We judge the performance of the algorithm by its _regret_: \((T)=_{i=1}^{T}l_{t}(x_{t})-_{x}_{i=1}^{ T}l_{t}(x)\). Intuitively, the regret shows how far the total loss in \(T\) rounds caused by the algorithm is from the optimal static loss.

#### 2.3.2 Online learning in zero-sum games

To demonstrate how to compute the approximate Nash equilibrium using online learning algorithms, we present a useful proposition here. It states that from any sublinear regret learning algorithm \(\) with regret \((T)\), we can find an \(((T)/T)\)-approximate Nash equilibrium of the zero-sum game in \(T\) rounds.

To be more precise, let us consider the following procedure. Let \(\) be the matrix for the two-player zero-sum game. The algorithm starts with some initial strategies \(u_{0}_{m},v_{0}_{n}\) for the two players. Then at each round \(t\), for each player, it makes decisions with previous observations of the opponent's strategy. In particular, the row player is required to choose his/her action \(u_{t}_{m}\) after considering the previous loss functions \(g_{i}(x)=v_{i}^{}^{}x\) for \(i[t-1]\). Similarly, the column player chooses his/her action \(v_{t}_{n}\) with respect to the previous loss functions \(h_{i}(y)=-u_{i}^{}y\) for \(i[t-1]\). After both players choose their actions at this round \(t\), they will receive their loss functions \(g_{t}(x):=v_{t}^{}^{}x\) and \(h_{t}(y)=-u_{t}^{}y\), respectively.

Suppose after \(T\) rounds, the regret of the row player with respect to the loss functions \(g_{t}(x)\) is \((T)\), and the regret of the column player with loss functions \(h_{t}(y)\) is \(^{}(T)\). We can write the total regret \((T)+^{}(T)\) explicitly: \((T)+^{}(T)=T(_{v_{n}} v, ^{}u-_{u_{m}} u, )\), where the average strategy is defined as \(=_{i=1}^{T}u_{i}/T\), \(=_{i=1}^{T}v_{i}/T\). This pair is a good approximation of the Nash equilibrium for the game \(\) if the regret is \((T)\).

### Quantum Computing

In quantum mechanics, a \(d\)-dimensional quantum state is described by a unit vector \(v=(v_{0},v_{1},,v_{d-1})^{}\), usually denoted as \(|v\) with the Dirac symbol \(|\), in a complex Hilbert space \(^{d}\). The computational basis of \(^{d}\) is defined as \(\{|i\}_{i=0}^{d-1}\), where \(|i=(0,,0,1,0,,0)^{}\) with the \(i\)-th (\(0\)-indexed) entry being \(1\) and other entries being \(0\). The inner product of quantum states \(|v\) and \(|w\) is defined by \( v|w=_{i=0}^{d-1}v_{i}^{*}w_{i}\), where \(z^{*}\) denotes the conjugate of complex number \(z\). The norm of \(|v\) is defined by \(\||v\|=\). The tensor product of quantum states \(|v^{d_{1}}\) and \(|w^{d_{2}}\) is defined by \(|v|w=(v_{0}w_{0},v_{0}w_{1},,v_{d_{1}-1}w_{d_{2}-1})^{}^{d_{1}d_{2}}\), denoted as \(|v|w\) for short.

A quantum bit (qubit for short) is a quantum state \(|\) in \(^{2}\), which can be written as \(|=|0+|1\) with \(||^{2}+||^{2}=1\). An \(n\)-qubit quantum state is in the tensor product space of \(n\) Hilbert spaces \(^{2}\), i.e., \((^{2})^{ n}=^{2^{n}}\) with the computational basis \(\{|0,|1,,|2^{n}-1\}\). To obtain classical information from an \(n\)-qubit quantum state \(|v\), we measure \(|v\) on the computational basis and obtain outcome \(i\) with probability \(p(i)=| i|v|^{2}\) for every \(0 i<2^{n}\). The evolution of a quantum state \(|v\) is described by a unitary transformation \(U|v U|v\) such that \(UU^{}=U^{}U=I\), where \(U^{}\) is the Hermitian conjugate of \(U\), and \(I\) is the identity operator. A quantum gate is a unitary transformation that acts on \(1\) or \(2\) qubits, and a quantum circuit is a sequence of quantum gates.

Throughout this paper, we assume the quantum oracle \(_{}\) for a matrix \(^{m n}\), which is a unitary operator such that for every row index \(i[m]\) and column index \(j[n]\), \(_{}\!|i\!|j\!|0 =|i\!|j\!|A_{i,j}\). Intuitively, the oracle \(_{}\) reads the entry \(A_{i,j}\) and stores it in the third register; this is potentially stronger than the classical counterpart when the query is a linear combination of basis vectors, e.g., \(_{k}_{k}|i_{k}|j_{k}\) with \(_{k}|_{k}|^{2}=1\). This is known as the _superposition_ principle in quantum computing.

Note that this input model for matrices is commonly used in quantum algorithms, e.g., linear system solvers  and semidefinite programming solvers .

A quantum (query) algorithm \(\) is a quantum circuit that consists a sequence of unitary operators \(G_{1},G_{2},,G_{T}\), each of which is either a quantum gate or a quantum oracle. The quantum time complexity of \(\) is measured by the number \(T\) of quantum gates and quantum oracles in \(\). The execution of \(\) on \(n\) qubits starts with quantum state \(|0^{ n}\), then it performs unitary operators \(G_{1},G_{2},,G_{T}\) on the quantum state in this order, resulting in the quantum state \(|=G_{T} G_{2}G_{1}|0^{ n}\). Finally, we measure \(|\) on the computational basis \(|i\) for \(0 i<2^{n}\), giving a classical output \(i\) with probability \(| i||^{2}\).

 Quantum Algorithm for Online Zero-Sum Games by Sample-Based Optimistic Multiplicative Weight Update

Now, we present our quantum algorithm for finding an approximate Nash equilibrium for zero-sum games (Algorithm 1). This algorithm is a modification of the optimistic multiplicative weight algorithm , in which we use stochastic gradients to estimate true gradients. This modification utilizes the quantum advantage of Gibbs sampling (as will be shown in Section 4). It is the source of quantum speedups in the algorithm and also the reason that we call the algorithm _sample-based_. To this end, we first give the definition of Gibbs sampling oracles.

**Definition 3.1** (Approximate Gibbs sampling oracle).: Let \(p^{n}\) be an \(n\)-dimensional vector, \(>0\) be the approximate error. We let \(_{p}^{}(k,)\) denote the oracle which produces \(k\) independent samples from a distribution that is \(\)-close to the Gibbs distribution with parameter \(p\) in total variation distance. Here, for a random variable \(j\) taking value in \([n]\) following the Gibbs distribution with parameter \(p\), we have \([j=l]=(p_{l})/_{i=1}^{n}(p_{i})\).

```
0:\(^{m n}\), additive approximation \(\), approximate Gibbs sampling oracle \(^{}\) with error \(_{}\), total episode \(T\), learning rate \((0,/6)\).
0:\((,)\) as the approximate Nash equilibrium of the matrix game \(\).
1: Initialize \(_{m}\), \(_{n}\), \(x_{1}_{m}\), \(y_{1}_{n}\).
2: Set \(g_{1} x_{1}\), \(h_{1} y_{1}\).
3:for\(t=1,,T\)do
4: Get \(T\) independent samples \(i_{1}^{t},i_{2}^{t},,i_{T}^{t}\) from the Gibbs sampling oracle \(_{-h_{t}}^{}(T,_{})\).
5: Choose the action \(_{t}=_{N=1}^{T}_{i_{N}^{t}}^{t}/T\).
6: Update \(x_{t+1} x_{t}+_{t}\), \(g_{t+1} x_{t+1}+_{t}\). \(+_{t}\).
7: Get \(T\) independent samples \(j_{1}^{t},j_{2}^{t},,j_{T}^{t}\) from the Gibbs sampling oracle \(_{^{}g_{t}}^{}(T,_{ })\).
8: Choose the action \(_{t}=_{N=1}^{T}_{j_{N}^{t}}^{t}/T\).
9: Update \(y_{t+1} y_{t}+_{t}\), \(h_{t+1} y_{t+1}+_{t}\), \(+_{t}\).
10:endfor
11:return\((,)\). ```

**Algorithm 1** Sample-Based Optimistic Multiplicative Weight Update for Matrix Games

Suppose the zero-sum game is represented by matrix \(^{m n}\) with \(\|\| 1\). Our sample-based optimistic multiplicative weight update algorithm is given in Algorithm 1. Algorithm 1 is inspired by the classical optimistic follow-the-regularized-leader algorithm (see Appendix A for more information). In that classical algorithm, the update terms are essentially \([_{t}]\) and \([_{t}]\), which are computed deterministically by matrix arithmetic operations during the update. In contrast, we do this probabilistically by sampling from the Gibbs distributions and \(_{t}\) and \(_{t}\) in Line 5 and Line 10 are the corresponding averages of the samples. For this to work, we need to bound the expectation of the total regret (see Appendix B) based on the RVU property (Definition A.2). Technically, the \(l_{1}\) variances of \(_{t}\) and \(_{t}\) turn out to be significant in the analysis. To reduce the variances, we need multiple independent samples identically distributed from Gibbs distributions (see Line 4 and Line 7 in Algorithm 1). Because of the randomness from Gibbs sampling oracles, the total regret \((T)+^{}(T)\) of Algorithm 1 is a random variable. Nevertheless, we can bound the total regret by \(O((mn))\) with high probability as follows.

**Theorem 3.2**.: _Let \(_{}=1/T\). After \(T\) rounds of playing the zero-sum game \(\) by Algorithm 1, the total regret will be bounded by_

\[(T)+^{}(T) 144++12,\]

_with probability at least \(2/3\). Then, for any constant \((0,/6)\), the total regret is \(O((mn))\). Moreover, if we choose \(T=(1/)\), then Algorithm 1 will return an \(\)-approximate Nash equilibrium of the game \(\)._

The proof of Theorem 3.2 is deferred to Appendix B.2. Combining Theorem 3.2 and our fast quantum multi-Gibbs sampler in Theorem 4.2 (which will be developed in the next section), we obtain a quantum algorithm for finding an \(\)-approximate Nash equilibrium of zero-sum games.

**Corollary 3.3**.: _If we choose \(T=(1/)\) and use quantum multi-Gibbs sampling (Algorithm 2) with \(_{}=1/T\), then Algorithm 1 will return an \(\)-approximate Nash equilibrium in quantum time \((/^{2.5})\)._

## 4 Fast Quantum Multi-Gibbs Sampling

In Algorithm 1, the vectors \(h_{t}\) and \(g_{t}\) updated in each round are used to generate independent samples from Gibbs distributions \(_{-h_{t}}^{}\) and \(_{^{}g_{t}}^{}\). Here, \(h_{t}\) and \(g_{t}\) are supposed to be stored in classical registers. To allow quantum speedups, we store \(h_{t}\) and \(g_{t}\) in quantum-read classical-write random access memory (QRAM) , which is commonly used in prior work [49; 6]. Specifically, a QRAM can store/modify an array \(a_{1},a_{2},,a_{}\) of classical data and provide quantum (read-only) access to them, i.e., a unitary operator \(U_{}\) is given such that \(U_{}|i\,|0|i\,|a_{i}\). Without loss of generality (see Remark 4.3), suppose we have quantum oracle \(_{}\) for \(^{n n}\) with \(A_{i,j}\), and QRAM access to a vector \(z^{n}\) with \(z_{i} 0\). We also need the polynomial approximation of the exponential function for applying the QSVT technique :

**Lemma 4.1** (Polynomial approximation, Lemma 7 of ).: _Let \( 1\) and \(_{P}(0,1/2)\). There is a classically efficiently computable polynomial \(P_{}[x]\) of degree \(O((_{P}^{-1}))\) such that \(P_{}(x) 1\) for \(x[-1,1]\), and \(_{x[-1,0]}\,P_{}(x)-( x) _{P}\)._

Then, we can produce multiple samples from \(_{^{}}^{}\) efficiently by Algorithm 2 on a quantum computer. Algorithm 2 is inspired by Hamoudi  about preparing multiple samples of a quantum state, with quantum access to its amplitudes. However, we do not have access to the exact values of the amplitudes, which are \((z)_{i}\) in our case. To resolve this issue, we develop consistent quantum amplitude estimation (see Appendix C) to estimate \((z)_{i}\) with a unique answer (Line 1). After having prepared an initial quantum state \(|u_{}\), we use quantum singular value decomposition  to correct the tail amplitudes (Line 6), and finally obtain the desired quantum state \(|_{}\) by quantum amplitude amplification  (Line 7). We have the following (see Appendix D for its proof):

**Theorem 4.2** (Fast quantum multi-Gibbs sampling).: _For \(k[n]\), if we set \(_{P}=(ke_{}^{2}/n)\), then Algorithm 2 will produce \(k\) independent and identical distributed samples from a distribution that is \(_{}\)-close to \(_{^{}}^{}\) in total variation distance, in quantum time \(()\)._

_Remark 4.3_.: If \(^{m n}\) is not a square matrix, then by adding 0's we can always enlarge \(\) to an \((m+n)\)-dimensional square matrix. For \(_{-h_{t}}^{}\) as required in Algorithm 1, we note that \(_{(-)h_{t}}^{}\) indicates the same distribution as \(_{-h_{t}}^{}\), where \(\) has the same size as \(\) with all entries being \(1\). From the above discussion, we can always convert \(\) to another matrix satisfying our assumption, i.e., with entries in the range \(\).

_Remark 4.4_.: The description of the unitary operator \(U^{}\) defined by the polynomial \(P_{2}\) can be classically computed to precision \(_{P}\) in time \((^{3})\) by Haah , which is \((1/^{3})\) in our case as \( T=(1/)\) is required in Corollary 3.3. This extra cost can be neglected because \((/^{2.5})\) dominates the complexity whenever \(=((m+n)^{-1})\).

_Remark 4.5_.: Our multi-Gibbs sampler is based on maximum finding and consistent amplitude estimation, with a guaranteed worst-case performance in each round. In comparison, the dynamic Gibbs sampler in  maintains a hint vector, resulting in an amortized time complexity per round.

```
0: Quantum oracle \(_{}\) for \(^{n n}\), QRAM access to \(z^{n}\) with \(\|z\|_{1}\), polynomial \(P_{2}\) with parameters \(_{P}\) by Lemma 4.1, number \(k\) of samples. We write \(u=\).
0:\(k\) independent samples \(i_{1},i_{2},,i_{k}\).
1: Obtain \(_{}|i\!|0|i\! |_{i}\) by consistent quantum amplitude estimation such that \(u_{i}_{i} u_{i}+1\).
2: Find the set \(S[n]\) of indexes of the \(k\) largest \(_{i}\) by quantum \(k\)-maximum finding, with access to \(_{}\).
3: Obtain \(_{i}\) for all \(i S\) from \(_{}\), then compute \(^{*}=_{i S}_{i}\) and \(W=_{i S}(_{i})+(n-k)(^{*})\).
4:for\(=1,,k\)do
5: Prepare the quantum state \(|u_{}=_{i S}_{i})}{W}}|i +_{i S}^{*})}{W}}|i\).
6: Obtain unitary \(U^{}\) such that \( 0|^{ a}U^{}|0^{ a}=P_{2 }(u-\{,^{*}\})/4\) by QSVT.
7: Post-select \(|_{} 0|^{ a}U^{}|u_{ }|0^{ a}\) by quantum amplitude amplification.
8: Let \(i_{}\) be the measurement outcome of \(|_{}\) in the computational basis.
9:endfor
10:return\(i_{1},i_{2},,i_{k}\). ```

**Algorithm 2** Quantum Multi-Gibbs Sampling \(_{z}^{}(k,_{})\)

## 5 Discussion

In our paper, we propose the first quantum online algorithm for zero-sum games with near-optimal regret. This is achieved by developing a sample-based stochastic version of the optimistic multiplicative weight update method . Our core technical contribution is a fast multi-Gibbs sampling, which may have potential applications in other quantum computing scenarios.

Our result naturally gives rise to some further open questions. For instance: Can we improve the dependence on \(\) for the time complexity? And can we further explore the combination of optimistic learning and quantum computing into broader applications? Now that many heuristic quantum approaches for different machine learning problems have been realized, e.g.in Havlicek et al. , Saggio et al. , Harrigan et al. , can fast quantum algorithms for zero-sum games be realized in the near future?