# A generalized neural tangent kernel

for surrogate gradient learning

 Luke Eilers

Department of Physiology, University of Bern, Switzerland

Institute for Applied Mathematics, University of Bonn, Germany

luke.eilers@unibe.ch

Corresponding author

&Raoul-Martin Memmesheimer

Institute of Genetics, University of Bonn, Germany

rm.memmesheimer@uni-bonn.de

&Sven Goedeke

Bernstein Center Freiburg, University of Freiburg, Germany

Institute of Genetics, University of Bonn, Germany

sven.goedeke@bcf.uni-freiburg.de

###### Abstract

State-of-the-art neural network training methods depend on the gradient of the network function. Therefore, they cannot be applied to networks whose activation functions do not have useful derivatives, such as binary and discrete-time spiking neural networks. To overcome this problem, the activation function's derivative is commonly substituted with a surrogate derivative, giving rise to surrogate gradient learning (SGL). This method works well in practice but lacks theoretical foundation. The neural tangent kernel (NTK) has proven successful in the analysis of gradient descent. Here, we provide a generalization of the NTK, which we call the surrogate gradient NTK, that enables the analysis of SGL. First, we study a naive extension of the NTK to activation functions with jumps, demonstrating that gradient descent for such activation functions is also ill-posed in the infinite-width limit. To address this problem, we generalize the NTK to gradient descent with surrogate derivatives, i.e., SGL. We carefully define this generalization and expand the existing key theorems on the NTK with mathematical rigor. Further, we illustrate our findings with numerical experiments. Finally, we numerically compare SGL in networks with sign activation function and finite width to kernel regression with the surrogate gradient NTK; the results confirm that the surrogate gradient NTK provides a good characterization of SGL.

## 1 Introduction

Artificial neural networks (ANNs) originate from the biologically inspired perceptron . While the perceptron has a binary output that is faithful to the all-or-none behavior of spiking neurons in the nervous system, most activation functions considered nowadays for ANNs are smooth (like the logistic function) or at least semi-differentiable (like the ReLU function). Differentiable network functions enable the learning of network weights with methods that leverage the gradient of the network function with respect to the network weights like backpropagation .

1986]. These methods are very successful [LeCun et al., 2015], but cannot be used without well-defined gradients.

This is a problem when considering more biologically plausible ANNs, which are typically used in computational neuroscience to understand how the networks of spiking neurons in our nervous system work. These include spiking neural networks (SNNs). Motivated by the energy efficiency of our brain, SNNs and similar networks, such as binary neural networks (BNNs), are considered in the context of neuromorphic computing [Merolla et al., 2014]. Both discrete-time SNNs and BNNs have in common that their activation functions do not have useful derivatives, which renders standard gradient-descent training impossible [Nerfici et al., 2019, Taherkhani et al., 2020, Tavanaei et al., 2019, Roy et al., 2019, Pfeiffer and Pfeil, 2018]. For the scope of this paper, these activation functions can be thought of as step-like functions like the sign function, in which cases the derivative vanishes almost everywhere and is thus no longer informative about the shape of the activation function.

Surrogate gradient learning resolves this issue by providing the missing information about the activation function in the form of a surrogate derivative [Hinton, 2012, Bengio et al., 2013, Zenke and Ganguli, 2018]. As a result, the gradient-based methods for classical ANNs can be leveraged with great success [Zenke and Vogels, 2021]. However, a theoretical basis underpinning the intuition is missing and it is often unclear which surrogate derivative should be chosen for a particular network. For a review focusing on surrogate gradient learning methods, which we are most interested in, see Neftci et al. . For a comprehensive review of other learning methods for SNNs, we refer to Taherkhani et al. , Tavanaei et al. , and Eshraghian et al. .

The neural tangent kernel (NTK) introduced by Jacot et al.  allows to formulate gradient descent as a kernel method. Just as ANNs with randomly initialized weights converge under certain conditions to Gaussian processes (GPs) in the infinite-width limit [Matthews et al., 2018, Lee et al., 2018], the NTK converges at initialization and during training to a deterministic kernel in the same limit. Moreover, the NTK then describes how the network function changes under gradient descent in the infinite-width regime. This led to both a better theoretical understanding of gradient descent and practical applications to neural network training; see Section 1.2 for more details.

### Contribution

We study the NTK for networks with sign function as activation function. As the NTK theory is not directly applicable due to an ill-defined derivative, we consider the NTK for a sequence of activation functions that approximates the sign function and then derive a principled way of generalizing the NTK to gain more theoretical insight into surrogate gradient learning. Our contributions are as follows:

* We provide a clear definition of the infinite-width limit in Section 2.2, capturing the different notions used in the literature due to the different choices of rates at which the layer widths increase. This definition is used consistently in all mathematical statements and the respective proofs.
* In Section 2.3, we demonstrate that the direct extension of the NTK for the sign activation function using an approximating sequence is not well-defined due to the divergence of the kernel's diagonal. This illustrates, from the NTK perspective, that gradient descent is ill-defined for activation functions with jumps and how this problem will be mitigated by surrogate gradient learning. Moreover, we connect this divergence to results by Radhakrishnan et al.  in Theorem 2.3 and show that the direct extension of the NTK can be seen as a well-defined kernel for classification.
* We define a generalized version of the NTK in Section 2.4 using so-called quasi-Jacobian matrices and prove the convergence to a deterministic, in general asymmetric, kernel in the infinite width limit at initialization in Theorem 2.4. Using the generalized NTK, we formulate surrogate gradient learning in terms of the generalized NTK for networks with differentiable activation functions. This novel NTK is named the SG-NTK and we prove its convergence to a deterministic kernel during training in Theorem 2.5.
* For both the diverging direct extension of the NTK and the SG-NTK with sign activation function and arbitrary surrogate derivative, we derive exact analytical expressions in sections D.1, D.2 and E.2. In particular, we identify the terms that emerge from SGL and that prevent divergence.
* In Section 3, we illustrate our findings, in particular Theorem 2.4 and Theorem 2.5, using simulations. Numerical experiments show that the distribution of networks trained with SGL shows agreement with the distribution given by the SG-NTK.

Mathematically precise versions of all statements can be found in the appendix, which is self-contained to ensure rigor and a consistent notation throughout all theorems and proofs.

### Related work

The neural tangent kernel.The convergence of randomly initialized ANNs in the infinite-width limit to Gaussian processes under appropriate scaling of the weights has first been described by Neal (1996) for a single hidden layer and has been extended to multiple hidden layers and other network architectures like CNNs (Matthews et al., 2018; Garriga-Alonso et al., 2018; Yang, 2019; Lee et al., 2018). The NTK was introduced by Jacot et al. (2018) with first results on its convergence at network initialization and during training. Theoretical results on the implication of the convergence of NTKs on the behaviour of trained wide ANNs were given by Lee et al. (2019); Arora et al. (2019); Allen-Zhu et al. (2019). In Section C.2, we review central theorems on the NTK to enable a clear comparison to our theoretical results.

The NTK has been generalized to all kinds of ANN standard architectures Yang (2020) such as CNNs Arora et al. (2019) and attention layers Hron et al. (2020). In particular, it has been generalized to RNNs Alemohammad et al. (2020), which are particularly interesting in light of SNNs due to their shared temporal dimension. Bordelon and Pehlevan (2022) derive a generalization of the NTK called effective NTK for different learning rules using an approach similar to ours. Note that all of these extensions require well-defined gradients.

The ability of overparameterized neural networks to converge during training and to generalize can be explained using the NTK (Allen-Zhu et al., 2019; Bordelon et al., 2020; Bietti and Mairal, 2019; Du et al., 2019). The NTK has also been used in more applied areas such as neural architecture search (Chen et al., 2021) and dataset distillation (Nguyen et al., 2021).

Surrogate gradient learning.In the context of computational neuroscience, the idea of replacing the derivative of the output of a spiking neuron with a surrogate derivative was introduced by Bohte (2011). To deal with the temporal component in SNNs or more generally RNNs, the resulting gradient is usually combined with backpropagation through time (BPTT). Prominent examples of surrogate gradient approaches include SuperSpike by Zenke and Ganguli (2018) and a number of works with different surrogate derivatives (Wu et al., 2018, 2019; Shrestha and Orchard, 2018; Bellec et al., 2018; Esser et al., 2016; Wozniak et al., 2020). In the general ANN literature, the method is better known as straight-through estimation (STE) and was introduced by Hinton (2012) and by Bengio et al. (2013) in more detail. It was successfully applied by Hubara et al. (2016) and Cai et al. (2017).

Surrogate gradient learning or STE is only heuristically motivated and it is hence desirable to derive a theoretical basis. The influence of different surrogate derivatives on the method has been analyzed through systematic numerical simulations Zenke and Vogels (2021), revealing that the particular shape has a minor impact compared to the scale. In a more theoretical work, Yin et al. (2019) analyzed the convergence of STE for a Heaviside activation function with three different surrogate derivatives and found that the descent directions of the respective surrogate gradients reduce the population loss when the clipped ReLU function is used as surrogate derivative. Gygax and Zenke (2024) examine how SGL is connected to smoothed probabilistic models (Bengio et al., 2013; Neftci et al., 2019; Jang et al., 2019) and to stochastic automatic differentiation (Arya et al., 2022). In particular, they consider SGL for differentiable activation functions, as we do in our derivation of the SG-NTK.

## 2 Theoretical results

### Notation and NTK parametrization

We consider multilayer perceptrons with so-called neural tangent kernel parametrization. For a network with depth \(L\), layer width \(n_{l}\) for \(0 l L\), activation function \(\), weight matrices \(W^{(l)}^{n_{l} n_{l-1}}\), and biases \(b^{(l)}^{n_{l}}\), the preactivations with NTK parametrization are given by

\[h^{(1)}(x) =}{}}W^{(1)}x+_{b}\,b^{(1)},\] \[h^{(l+1)}(x) =}{}}W^{(l+1)}(h^{(l)} (x))+_{b}\,b^{(l+1)}l=1,,L-1,\]where \(_{w}>0\), \(_{b} 0\), and we initialize \(W^{(l)}_{ij},b^{(l)}_{i}}}{{}}(0,1)\) for all \(i,j\). The NTK parametrization differs from the standard parametrization by a rescaling factor of \(1/}\) in layer \(l+1\). The network function is then given by \(f(\,\,)=h^{(L)}(\,\,)^{n_{0}}^{n_{L}}\) and notably the last layer is a preactivation layer. We denote the total number of weights by \(P\). More details can be found in Section C.1.

Notation (see Remark C.1 and C.2)By default, we will interpret any vector as a column vector, i.e., we identify \(^{n}\) with \(^{n 1}\). This is the case even when writing \(x=(x_{1},,x_{n})^{n}\) for handier notation. Row vectors will be indicated within calculations using the transpose operator, \(x^{}\). For a function \(f^{n_{1}}^{n_{2}}\) and \(=(x_{1},,x_{d})^{d n_{1}}\), we define the vector \(f()(f(x_{1}),,f(x_{d}))^{d n_{2}}\). For \(n_{2}=1\), we denote the gradient of \(f\) by \( f(x)^{n_{1}}\) for all \(x^{n_{1}}\). If \(n_{2}>1\), we denote the Jacobian matrix of \(f\) by \(Jf^{n_{1}}^{n_{2} n_{1}}\). A subscript of the form \(J_{0}f(x)\) denotes Jacobian matrices with respect to a subset of variables. We write \(f(n)]{-0.0pt}{$$}}g(n)\) to denote that \(f(n)\) and \(g(n)\) are asymptotically proportional, i.e., \(f(n) Kg(n)\) for some constant \(K 0\).

### The infinite-width limit

We will consider neural networks in the limit of infinitely many hidden layer neurons, i.e., \(n_{l}\) for all \(1 l L-1\). We will see later, when paraphrasing existing results from the literature, that different ways of taking the number of hidden neurons to infinity can be found. To formalize these notions, we use the definition of a width function from Matthews et al. (2018) with slight modifications:

**Definition 2.1** (Width function).: _For every layer \(l=0,,L\) and any \(m\), the number of neurons in that layer is given by \(r_{l}(m)\), and we call \(r_{l}\) the width function of layer \(l\). We say that a width function \(r_{l}\) is strictly increasing if \(r_{l}(m)<r_{l}(m+1)\) for all \(m 1\). We set_

\[_{L}\{(r_{l})_{l=1}^{L-1} r_{l}0<l<L\},\]

_the set of collections of strictly increasing width functions for network depth \(L\)._

Every element of \(_{L}\) provides a way to take the widths of the hidden layers to infinity by setting \(n_{l}=r_{l}(m)\) for any \(1 l<L\) and considering \(m\). The notions of infinite-width limits used in the literature will now correspond to classes \(R_{L}\) for which the respective limiting statements hold. This is captured in the following definition.

**Definition 2.2** (Types of infinite-width limits).: _Consider a statement \(\) of the form "Let an ANN have depth \(L\) and network layer widths defined by \(n_{0}\), \(n_{L}\), and \(n_{l} r_{l}(m)\) for \(1 l<L\) and some \((r_{l})_{l=1}^{L-1}_{L}\). Then, for fixed \(n_{0}\) and any \(n_{L}\), the statement \(\) holds as \(m\)." We also write the statement \(\) as \((r)\)._

1. _[label=()]_
2. _We say that such a statement_ \(\) _holds strongly, if_ \((r)\) _holds for any_ \(r_{L}\)_. This can be interpreted as requiring that the statement holds as_ \(_{1 l<L}(n_{l})\)_. We will also write "_\(\) _holds as_ \(n_{1},,n_{L-1}\) _strongly"._
3. _We say that such a statement_ \(\) _holds for_ \((n_{l})_{1 l L-1}]{-0.0pt}{$$}}n\)_, if_ \(\) _holds for all_ \(r_{L}\) _with_ \(r_{l}(m)]{-0.0pt}{$$}}m\) _for all_ \(1 l<L\)_. This means that_ \((r)\) _holds for all_ \(r_{L}\) _such that_ \(r_{p}(m)/r_{q}(m)_{p,q}(0,)\) _as_ \(m\)_. We will also write "_\(\) _holds as_ \((n_{l})_{1 l<L}]{-0.0pt}{$$}}n\)_"._
4. _We say that such a statement_ \(\) _holds weakly, if_ \(\) _holds for at least one_ \(r_{L}\)_. We will also write "_\(\) _holds as_ \(n_{1},,n_{L-1}\) _weakly". This type of infinite-width limit is tightly connected to the sequential infinite-width limit._

**Remark 2.1** (Connection between weak and sequential infinite-width limits).: _In the sequential infinite-width limit, meaning that \(n_{1},,n_{L-1}\) sequentially, the layer widths are not strictly finite. This is opposed to applications, where layer widths may be large but finite. Hence, the infinite-width limits using width functions as explained above are more meaningful in practice. For a sequential limit \(_{n_{1}}_{n_{2}}f(n_{1},n_{2})=f^{*}\), we can find functions \(n_{1}(m)\) and \(n_{2}(m)\) such that \(_{m}f(n_{1}(m),n_{2}(m))=f^{*}\). However, the rate at which \(n_{1}(m),n_{2}(m)\) diverge as \(m\) cannot generally be controlled. Since the weak infinite-width limit allows for arbitrary rates, any sequential infinite-width limit can hence be turned into a weak infinite-width limit as defined in Definition 2.2 (iii)._We use Definition 2.2 to paraphrase the convergence of ANNs to GPs in the infinite-width limit:

**Theorem 2.1** (Theorem 4 from Matthews et al. (2018)).: _Any network function \(f\) of depth \(L\) defined as in Section 2.1 with continuous activation function \(\) that satisfies the linear envelope property, i.e., there exist \(c,m 0\) with \(|(u)| c+m|u|\) for all \(u\), converges in distribution as \(n_{1},,n_{L-1}\) strongly to a multidimensional Gaussian process \((X_{j})_{j=1}^{n_{L}}\) for any fixed countable input set \((x_{i})_{i=1}^{}\). It holds \(X_{j}}{}(0,^{(L)})\) where the covariance function \(^{(L)}\) is recursively given by_

\[^{(1)}(x,x^{})=^{2}}{n_{0}} x,x^{} +_{b}^{2},^{(L)}(x,x^{})=_{w}^{2}\,_{g(0,^{(L-1)})}[(g(x))\,(g(x^{}))]+ _{b}^{2}.\] (1)

We also write \(^{(L)}=_{}\). The theorem states that the distribution of the network function, which is given by its randomly initialized weights, approaches the distribution of independent GPs as the hidden layer widths increase. Hence, a large-width network will approximately be a realization of the respective GPs. Note that this result can be generalized to non-continuous activation functions without well-defined derivatives that fulfil the linear envelope property, like \((z)=(z)\). We provide a rigorous proof of this kind of generalization for the case \(n_{1},,n_{L-1}\) weakly in Theorem E.3. \(_{}\) remains well-defined in this case since the expectation in Equation (1) does. When approximating the sign function with scaled error function, i.e., \((z)=_{m}(z)(m z)\), it holds that \(_{m}_{_{m}}=_{}\) due to the dominated convergence theorem.

### Direct extension of the neural tangent kernel in the infinite-width limit

We consider a dataset \(=(,)\) with \(=(x_{1},,x_{d})^{d n_{0}}\) and \(=(y_{1},,y_{d})^{d n_{L}}\). To solve the regression problem, i.e., to find weights \(^{P}\) such that \(f(x_{i};)=y_{i}\) for all \(i=1,,d\), we apply gradient descent in continuous time, also know as gradient flow, with learning rate \(\) and loss function \(^{d n_{L}}^{d n_{L}} \). This means that, using the chain rule, the learning rule can then be written as

\[}{t}_{t}=-\,_{}(f( ;_{t});)=-\,J_{}f(;_{t })^{}\,_{f(;_{t})}(f(; _{t});).\] (2)

To derive the NTK, we observe that the network function then evolves according to

\[}{t}f(x;_{t}) =J_{}f(x;_{t})\,}{t}_ {t})}{=}-\,J_{}f(x;_{t})J_{}f( ;_{t})^{}\,_{f(;_{t})} (f(;_{t});)\] (3) \[:-\,_{t}(x,)_{f( ;_{t})}\,(f(;_{t});),\]

where we implicitly defined the empirical neural tangent kernel as \((x,x^{}) J_{}f(x;)J_{}f(x^{ };)^{}\), which depends on the current weights \(=_{t}\). This means that the learning dynamics of the network functions during training are given by a kernel whose entries are the scalar products between the gradients of the network's output neuron activity, \(_{i\,j}(x,x^{})=_{}f_{i}(x;), _{}f_{j}(x^{};)\). The key result on the NTK is that the empirical NTK converges in the infinite-width limit to a constant kernel, the analytic NTK, at initialization, \(=_{0}\), and during training, \(=_{t}\):

**Theorem 2.2** (Theorem 1 from Jacot et al. (2018) for general \(_{w}>0\)).: _For any network function of depth \(L\) defined as in Section 2.1 with Lipschitz continuous activation function \(\), \(^{(L)}\) converges in probability to a constant kernel \(^{(L)}_{n_{L}}\) as \(n_{1},,n_{L-1}\) weakly. This means that for all \(x,x^{}^{n_{0}}\) and \(1 i,j n_{L}\), it holds \(^{(L)}_{i\,j}(x,x^{})_{ij}\,^{(L)}(x,x^{ })\) in probability, where \(_{ij}\) denotes the Kronecker delta. We call \(^{(L)}\) the analytic neural tangent kernel of the network, which is recursively given by_

\[^{(1)}(x,x^{})=^{(1)}(x,x^{}),^{(L)}(x,x^{ })=^{(L)}(x,x^{})+^{(L-1)}(x,x^{})^{(L)}(x,x^{}),\]

_where \(^{(l)}\) are defined as in Theorem 2.1 and we define_

\[^{(L)}(x,x^{})=_{w}^{2}\,_{g( 0,^{(L-1)})}[(g(x))\,(g(y))].\] (4)

We also write \(^{(L)}=_{}\). If \(_{t}\) are the weights during gradient flow learning at time \(t 0\) as before, Theorem 2.2 shows that \(^{(L)}_{t}^{(L)}_{n_{L}}\) for \(t=0\) in the infinite-width limit. This revealsthat the gradients of the output neurons, \(_{}f_{i}(x;)\), are mutually orthogonal. Under additional assumptions this convergence also holds for the whole training duration, \(t>0\), see Theorem 2 of Jacot et al. (2018) for the case \(n_{1},,n_{L-1}\) weakly, and Chapter G of Lee et al. (2019) for \((n_{l})_{1 l<L} n\). Hence, the kernel that describes the learning dynamics stays constant in the infinite-width limit. This implies that the distribution of the network function during training also converges to GPs (Lee et al., 2019, Theorem 2.2). Then, any output neuron after training has mean \(m(x)=^{(L)}(x,)^{(L)}(,)^{-1} \) under the assumption of a mean squared error (MSE) loss, see Section C.2.1. The expression for the mean is equivalent to kernel regression with the NTK.

The above results show that gradient flow for networks with randomly initialized weights is characterized by the analytic NTK \(^{(L)}\) in the infinite-width limit. Since we are interested in gradient flow for networks with activation functions inadequate for gradient descent training, we want to know whether the analytic NTK can be extended to such activation functions. We see that the derivative of the activation function does not have to be defined point-wise for Equation (4). In particular, activation functions with distributional derivatives, e.g., \(}{z}(z)=2\,(z)\) can be taken into consideration, where \(\) denotes the Dirac delta distribution. If we again approximate the sign function with scaled error functions \(_{m}\), \(m\), it holds

\[_{g(0,_{_{m}})}[}{}_{m}(g(x))}{}_{m}(g( y))]_{g(0,_{})}[2\,(g(x)) 2\,(g(y))],\] (5)

in case the right-hand side exists. A rigorous analysis of this limit can be found in Section D.1. Heuristically, a simple observation suffices: if \(x=y\), we have a one-dimensional integral over two delta distributions, which yields infinity. On the other hand, if \(x y\) and \(_{}\) is non-degenerate, a two-dimensional integral over two delta distributions yields a finite value. We derive analytic expressions for \(_{m}_{_{m}}_{}\) in Lemma D.3. We call this kernel singular, because \(_{}(x,x)=\) and \(_{}(x,y)\) if \(x y\). By the same reasoning, this divergence occurs for any activation function with jumps. Note that for the mean given by kernel regression, this implies \(m(x_{i})=y_{i}\) and \(m(x)=0\) if \(x\) is not a training point. Intuitively, this is because the network is initialized with zero mean and different input points are uncorrelated under the singular kernel, so only the training points are learned. A limit kernel with this property also arises if the activation function is fixed but the depth of the network goes to infinity as was shown by Radhakrishnan et al. (2023). We adopt the ideas of their proof to show that the sign of \(m\) is still useful for classification:

**Theorem 2.3** (Inspired by Lemma 5 of Radhakrishnan et al. (2023); see Theorem D.4).: _Let \(_{b}^{2}>0\) or let all \(x_{i}^{n_{0}}\) be pairwise non-parallel. Let \(L 2\) and \(x_{i}_{R}^{n_{0}-1}\) for all \(i=1,,d\), where \(_{R}^{n_{0}-1}^{n_{0}}\) is the sphere of radius \(R\). Assuming that \(_{}^{(L)}(x,) 0\) for almost all \(x_{R}^{n_{0}-1}\), it holds_

\[_{m}(_{m}^{(L)}(x,)_{m}^{ (L)}(,)^{-1})=(_{ }^{(L)}(x,))_{R}^{n_{0}-1}.\]

The estimator \(_{}^{(L)}(x,)=_{i=1}^{n}_{ }^{(L)}(x,x_{i})\,y_{i}\) has the form of a so-called Nadaraya-Watson estimator and is well-defined for singular kernels such as \(^{(L)}\). If we assume a classification problem in the sense that \(\{-1,1\}^{n}\), it holds \(_{}^{(L)}(x_{i},)= _{}^{(L)}(x_{i},)\) at training point \(x_{i}\).

### Generalization of the neural tangent kernel and application to surrogate gradient learning

The above singularity of the limit kernel can be avoided by considering surrogate gradient learning instead of gradient descent. First, we introduce a generalization of the NTK that later leads to the surrogate gradient NTK.

By definition and originally due to the chain rule, the empirical NTK consists of two Jacobian matrices of the network function with respect to the weights. The Jacobian matrix can be thought of as a recursive formula, \(J_{}f(x;)=G(x,;,)\), which is given by the input \(x\) and the architecture of the network, including the activation function \(\) and its derivative \(\). This formula can be modified to define a quasi-Jacobian matrix, \(J^{_{1},_{1}}(x;) G(x,;_{1},_{1})\), where \(_{1}\) does not have to be the derivative of \(_{1}\). Analogous to the definition of the empirical NTK we define the empirical generalized NTK to be \((x,x^{}) J^{_{1},_{1}}(x;)\,J^{ _{2},_{2}}(x^{};)^{}\), where \(_{1},_{1},_{2},_{2}\) are specified in the following theorem.

**Theorem 2.4** (Generalized version of Theorem 1 by Jacot et al. ; see Theorem E.4).: _For activation functions \(_{1}\), \(_{2}\) and so-called surrogate derivatives \(_{1}\), \(_{2}\) such that \(_{1},_{2},_{1}\), and \(_{2}\) satisfy the linear envelope property and are continuous except for finitely many jump points, denote the empirical generalized neural tangent kernel_

\[^{(L)}(x,x^{})=J^{(L),_{1},_{1}}(x;)\,J ^{(L),_{2},_{2}}(x^{};)^{}x,x^{}^{n_{0}},\]

_as before. Then, for any \(x,x^{}^{n_{0}}\) and \(1 i,j n_{L}\), it holds \(^{(L)}_{ij}(x,x^{})}_{ij}I^{(L)}(x,x^{}),\) as \(n_{1},,n_{L-1}\) weakly. We call \(I^{(L)}\) the analytic generalized neural tangent kernel, which is recursively given by_

\[I^{(1)}(x,x^{})=^{(1)}_{1,2}(x,x^{}),\;I^{(L)} (x,x^{})=^{(L)}_{1,2}(x,x^{})+I^{(L-1)}(x,x^{}) ^{(L)}_{1,2}(x,x^{})L 2,\] \[^{(L)}_{1,2}(x,x^{})=_{w}^{2}\,[ _{1}(Z_{1})\,_{2}(Z_{2})]+_{b}^{2}\;L 2\;_{1,2}(x,x^{})=^{2}}{n_{0}}  x,x^{}+_{b}^{2},\] \[^{(L)}_{1,2}(x,x^{})=_{w}^{2}\,[_{1}(Z_{1})\,_{2}(Z_{2})]\;\;(Z_{1},Z_{2})(0,(^{(L-1)}_{1,2}(x,x )&^{(L-1)}_{1,2}(x,x^{})\\ ^{(L-1)}_{1,2}(x,x^{})&^{(L-1)}_{2,2}(x^{},x^{}) )).\]

\(_{1}\) and \(_{2}\) denote the covariance functions of the Gaussian processes that arise from network functions \(f_{1},f_{2}\) with activation functions \(_{1},_{2}\), respectively, in the infinite-width limit. The covariance between these two Gaussian processes is denoted by \(_{1,2}\). This covariance function is asymmetric in the sense that \([f_{1}(x_{1}),f_{2}(x_{2})][f_{1}(x_{ 2}),f_{2}(x_{1})]\) in general.

We show in Theorem 2.4 that the generalized empirical NTK tends to a generalized analytic NTK at initialization in an infinite-width limit. Now, the SGL rule can be written using the generalized NTK, similar to Equation (3):

\[}{t}f(x;_{t}) =J_{}f(x;_{t})\,}{t}_{t}= -\,J_{}f(x;_{t})J^{,}(x;_{t})^{ }\,_{f(;_{t})}(f(; _{t});)\] (6) \[=:-\,_{t}(x,)\,_{f(; _{t})}(f(;_{t});)\] (7)

Here, we chose \(_{1}=_{2}=\), \(_{1}=\) and \(_{2}=\) in the previous definition of the generalized NTK. This we call the surrogate gradient NTK (SG-NTK) with activation function \(\) and surrogate derivative \(\). Compared to the classical NTK, one of the true Jacobian matrices is replaced by the quasi-Jacobian matrix, since the learning rule is SGL instead of gradient flow. Note that we assume that the derivative of the activation function, \(\), exists. Theorem 2.4 shows convergence at time \(t=0\). We extend this convergence to \(t>0\) in the following theorem:

**Theorem 2.5** (Based on Theorem G.2 from Lee et al. ; see Theorem E.5).: _Let \(,,\) as before, all Lipschitz continuous, and \(,\) bounded. Let \(f_{t}\) be a network function with depth \(L\) initialized as in Section 2.1 and trained with MSE loss and SGL with surrogate derivative \(\). Assume that the generalized NTK converges in probability to the analytic generalized NTK of Theorem 2.4, \(^{(L)} I^{(L)}_{n_{L}},\) as \((n_{t})_{t=1}^{L-1} n\). Furthermore, assume that the smallest and largest eigenvalue of the symmetrization of \(I^{(L)}(,)\), \(S^{(L)}I^{(L)}(,)+I^{(L)}(, )^{}/2,\) are given by \(0<_{}_{}<\) and that the learning rate is given by \(>0\). Then, for any \(>0\) there exist \(R>0,N\) and \(K>1\) such that for every \(n N\), the following holds with probability at least \(1-\) over random initialization:_

\[_{t[0,)}\|^{(L)}_{t}(,)-I^{(L)}( ,)\|_{F}R}{_{}}n^{- },\;\|\|_{F}\]

We also write \(I^{(L)}=I_{,}\) or simply \(I_{}\). The explicit analytic expression of the SG-NTK is derived in Section E.2 for activation function \(=_{m}\), \(m\) and surrogate derivative \(=\) as well as general surrogate derivatives. \(I_{_{m},\,}\) converges as \(m\), because compared to Equation (5) we obtain \([_{m}(g(x))\,(g(y))][2(g(x ))\,(g(y))]\) and the delta distribution yields a finite value. In Section E.2, we show this rigorously and derive the analytic expressions. Hence, we define \(I_{,}_{m}I_{_{m}, }\).

For any \(m\) we can consider the SGL dynamics given by Equation (7) in the infinite-width limit to obtain

\[}{t}f(x;_{t})=-\,I_{_{m}}(x, )\,_{f(;_{t})}(f(;_ {t});).\]With MSE error, this equation is solved by a GP with mean \(m(x)=I_{_{m}}(x,)I_{_{m}}(, )^{-1}\) for \(t\) and it is natural to assume that the networks trained with SGL converge in distribution to this GP in the infinite-width limit, analogous to the standard NTK case (Lee et al., 2019, Theorem 2.2). However, the empirical counterpart to \(I_{}\) does not exist as we cannot formulate an empirical SG-NTK for the sign activation function due to the missing Jacobian matrix, compare Equation (6). We suggest that even in this case the network trained with SGL will converge in distribution to the GP given by \(I_{}\), since SGL with activation function \(_{m}\) will approach SGL with activation function sign as \(m\) and the GP given by \(I_{_{m}}\) will approach the GP given by \(I_{}\) as \(m\). Numerical experiments in Section 3 indicate that this is indeed the case. We conclude that, remarkably, the SG-NTK can be defined for network functions without well-defined gradients and is informative about their learning dynamics under SGL.

## 3 Numerical experiments

We numerically illustrate the divergence of the analytic NTK, \(_{_{m}}\), shown in Section 2.3 and the convergence of the SG-NTK in the infinite-width limit, \(^{(L)}^{(L)}\), at initialization and during training shown in Section 2.4. Simultaneously, we visualize the convergence of the analytic SG-NTK, \(I_{_{m}} I_{}\). We consider a regression problem on the unit sphere \(^{}=\{x^{2}:\|x\|=1\}\) with \(||=15\) training points, which is shown in Figure B.1, and train 10 fully connected feedforward networks with two hidden layers, and activation function \(_{m}\) for \(t=10000\) time steps and with MSE loss. The NTK only depends on the dot product (Radhakrishnan et al., 2023) and thus the angle between its two arguments, \(=(x,x^{})\). Hence, we plot the NTKs as functions of this angle, where \(=0\) corresponds to \(x=x^{}\).

In Figure 1, the empirical and analytic NTKs for the networks described above and trained with gradient descent are plotted for \(m\{2,5,20\}\) and hidden layer widths \(n\{10,100,500,1000\}\). In addition, the analytic NTK for \(m\) is plotted. Note that the steep slope of \(_{m}\) for \(m=20\) results in \(_{m}\) being very close to the sign function. For any \(m\), we observe that the empirical NTKs converge to the analytic NTK both at initialization and after training as NTK theory states. Figure B.3 illustrates this further by displaying the mean squared errors between the empirical NTKs and the respective analytic NTK. The convergence slows down for larger \(m\). Further, the plots confirm that the analytic NTKs diverge as \(m\) if and only if \(=0\). To show this more clearly, we scaled the y-axis with the inverse hyperbolic sine (asinh), which is approximately linear for small absolute values and logarithmic for large absolute values.

For Figure 2, we use the same setup as before, but train the networks using SGL with surrogate derivative \(=\), and compare the empirical and analytic SG-NTKs instead of NTKs. We observe that the empirical SG-NTKs converge to the analytic SG-NTK as \(n\) both at initialization and after training in accordance with Theorem 2.4 and Theorem 2.5. Figure B.4 illustrates this further by displaying the mean squared errors between empirical SG-NTKs and resp

Figure 1: We plot empirical and analytic NTKs of 10 networks for different hidden layer widths \(n\) and activation functions \(_{m}\). The kernels are plotted at initialization and after gradient descent training with \(t=1e4\) time steps, learning rate \(=0.1\), and MSE error. The y-axis is asinh-scaled.

Moreover, we observe that the analytic SG-NTKs indeed converge to a finite limit as \(m\), as shown in Section 2.4.

Finally, we consider SGL for networks with the same architecture and training objective as before, but with sign activation function, which can be seen as the case \(m\) of the setups above. We examine whether the distribution of network functions trained with SGL agrees with the distribution of the GP given by the limit kernel \(I_{}\). Specifically, we compare 500 networks trained with SGL for \(t=30000\) time steps, which represent the distribution of the network function after training, to the mean and confidence band of the GP. The mean of the GP is given by kernel regression using the SG-NTK, \(m(x)=I_{}(x,)I_{}(,)^{- 1}\), and the confidence band is given by \(m(x) 2_{}(x)\), where \(_{}(x)\) is the standard deviation at \(x\). We observe in Figure 2(a) that the mean of the trained networks is close to the GP's mean for network width \(n=500\) and that most networks lie within the confidence band. The mean of the networks differs from the kernel regression using the kernel \(_{}\). Figure 2(b) shows that this agreement between SGL and the SG-NTK already exists for a network width of \(n=100\), demonstrating that the SG-NTK predicts the SGL dynamics of networks with moderate width. Note that the variance in the networks' output and the confidence band can be reduced (see Arora et al. (2019) and Section A).

Figure 3: Comparison of SGL learning in networks with different hidden layer widths with SG-NTK predictions. **(a)** 500 networks (blue) with sign activation function and hidden layer widths \(n=500\) trained with SGL using the surrogate derivative \(=}\) for \(t=3e4\) time steps plotted together with their mean (cyan), the SG-NTK-GP’s mean (black) and confidence band (grey), and the \(_{}\) kernel regression (dashed). Training points are indicated with crosses. **(b)** The mean of ensembles of 500 networks is plotted as in (a) for different hidden layer widths.

Figure 2: We plot empirical and analytic SG-NTKs of ten networks for different hidden layer widths \(n\) and activation functions \(_{m}\). The kernels are plotted at initialization and after surrogate gradient learning with \(t=1e4\) time steps, learning rate \(=0.1\), MSE error, and surrogate derivative \(=}\).

Conclusion

Gradient descent training is not applicable to networks with sign activation function. In the present study, we have first shown that this is even true for the infinite-width limit in the sense that the NTK diverges to a singular kernel. We found that this singular kernel still has interesting properties and allows for classification, but it is unusable for regression.

We then studied SGL, which is applicable to networks with sign activation function. We defined a generalized version of the NTK that can be applied to SGL and derived a novel SG-NTK. We proved that the convergence of the NTK in the infinite-width limit extends to the SG-NTK, both at initialization and during training. Strikingly, we were able to derive an SG-NTK for the sign activation function, \(I_{}\), by approximating the sign function with error functions. We suggest that this SG-NTK predicts the learning dynamics of SGL, and support this claim with heuristic arguments and numerical simulations.

A limitation of our work is that due to the considered NTK framework, our results are naturally only applicable to sufficiently wide networks with random initialization. Further, we only prove the convergence of the SG-NTK during training for activation functions with well-defined derivatives. More rigorous analysis should be carried out on how the connection between SGL and the SG-NTK carries over to activation functions with jumps, as shown by our simulations.

Our derivation of the SG-NTK opens a novel path towards addressing the many unanswered questions regarding the training of binary networks, in particular regarding the class of functions that SGL learns for wide networks and how that class differs for different activation functions and surrogate derivatives.