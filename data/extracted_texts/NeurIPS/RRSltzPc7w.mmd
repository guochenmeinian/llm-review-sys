# IMPRESS: Evaluating the Resilience of Imperceptible Perturbations Against Unauthorized Data Usage in Diffusion-Based Generative AI

IMPRESS: Evaluating the Resilience of Imperceptible Perturbations Against Unauthorized Data Usage in Diffusion-Based Generative AI

 Bochuan Cao

The Pennsylvania State University

bccao@psu.edu

&Changjiang Li

Stony Brook University

meet.cjli@gmail.com

&Ting Wang

Stony Brook University

inbox.ting@gmail.com

&Jinyuan Jia

The Pennsylvania State University

jinyuan@psu.edu

&Bo Li

University of Illinois, Urbana Champaign

lbo@illinois.edu

&Jinghui Chen

The Pennsylvania State University

jzc5917@psu.edu

###### Abstract

Diffusion-based image generation models, such as Stable Diffusion or DALL-E 2, are able to learn from given images and generate high-quality samples following the guidance from prompts. For instance, they can be used to create artistic images that mimic the style of an artist based on his/her original artworks or to maliciously edit the original images for fake content. However, such ability also brings serious ethical issues without proper authorization from the owner of the original images. In response, several attempts have been made to protect the original images from such unauthorized data usage by adding imperceptible perturbations, which are designed to mislead the diffusion model and make it unable to properly generate new samples. In this work, we introduce a perturbation purification platform, named IMPRESS, to evaluate the effectiveness of imperceptible perturbations as a protective measure. IMPRESS is based on the key observation that imperceptible perturbations could lead to a perceptible inconsistency between the original image and the diffusion-reconstructed image, which can be used to devise a new optimization strategy for purifying the image, which may weaken the protection of the original image from unauthorized data usage (e.g., style mimicking, malicious editing). The proposed IMPRESS platform offers a comprehensive evaluation of several contemporary protection methods, and can be used as an evaluation platform for future protection methods.

## 1 Introduction

Diffusion-based image generation models, e.g., Stable Diffusion (Rombach et al., 2022) and DALL-E 2 (Ramesh et al., 2022), have gained increasing attention due to their exceptional performance in synthesizing high-quality samples by leveraging given images. Despite their superior performance, existing studies (Shan et al., 2023) showed that they could be abused to generate new images without proper authorization from the original data owner. For instance, they could be used to learn from the original artworks of a specific artist and generate artistic images that mimic his/her style without authorization, which may lead to violation of intellectual property or copyrights (Dixit, 2023; Joseph Saveri Law Firm LLP, 2023; Edwards, 2022). They can also be used to maliciously edit the imagesof celebrities downloaded from the Internet to create disinformation (Pranav Dixit, 2023; Li et al., 2022a). Those _unauthorized data usage_ without the consent from the original data owner creates severe ethical concerns and causes profound societal harm (Shan et al., 2023).

To prevent unauthorized data usage, some recent studies (Shan et al., 2023; Salman et al., 2023) proposed to add imperceptible perturbations to images such that the latent diffusion model (LDM) (Rombach et al., 2022b), a state-of-the-art diffusion-based image generation model, is not able to leverage perturbed images to generate high-quality samples. For instance, PhotoGuard (Salman et al., 2023) proposed to add imperceptible perturbations to the images such that LDMs cannot generate realistic images when attempting to edit upon the perturbed image. GLAZE (Shan et al., 2023) proposed to add imperceptible perturbations to the artworks of an artist such that the LDMs cannot learn from perturbed images to generate samples that mimic the style of the artist. These works share a common trait: they all aim to add imperceptible perturbations to the original images to protect them from being "correctly" processed by the LDMs without proper authorization.

Although those protection methods (Shan et al., 2023; Salman et al., 2023) show some promising demos on preventing unauthorized data usage, we still lack a systematic understanding of their robustness and practical performances. For instance, since the perturbations added by those methods are imperceptible, we could potentially purify a perturbed image to disable its perturbation.

In this work, we aim to bridge the gap by conducting a systematic examination of those protection methods. We argue that this examination is essential from two aspects. First, the effectiveness of those protection methods under strong, adaptive perturbation purification is unknown. As a result, the examination could help users obtain a better understanding on the effectiveness of those protection methods in different scenarios. Second, the examination could also shed light on the design of new protection methods. The developed system can also be used as an evaluation platform for testing the effectiveness of future protection methods.

Our key observation is that the current "successful" imperceptible perturbations usually lead to a perceptible inconsistency between the original image and the diffusion-reconstructed image (i.e., the image-to-image output of the LDM without given any prompt), which can be used to devise a new optimization strategy for purifying the image and disabling the added perturbation. Specifically, we derive two conditions that the purified image should satisfy: _similarity condition_ and _consistency condition_. The similarity condition suggests that the purified image should be visually close to the perturbed image, and thus visually close to the original image (since the perturbation is imperceptible). The consistency condition means that the reconstructed image by a LDM for the purified image should be similar to itself. This condition stems from the observations that the clean image can reconstruct the original image and satisfy such consistency condition while the perturbed image cannot. We defer the detailed derivation of those two conditions to Section 4. Given those two conditions, we respectively design two losses to quantify them, enabling us to formulate the purification of a

Figure 1: Examples of imperceptible perturbations that are less effective in preventing unauthorized data usage after being purified by IMPRESS. **Left:** Stable diffusion generated images mimicking the style of Claude Monet using clean images, GLAZE (Shan et al., 2023) protected images, and IMPRESS purified images for model fine-tuning. **Right:** images edited by Stable diffusion using the prompt “A person in an airplane” with clean images, PhotoGuard (Salman et al., 2023) protected images, and IMPRESS purified images as model input.

perturbed image as an optimization problem. Finally, we solve the optimization problem and obtain the purified image. In summary, our contributions are as follows:

* We have explored and analyzed existing protection methods for unauthorized data usage in diffusion models and pointed out the potential risks. We found that existing methods heavily rely on adding an imperceptible perturbation. Yet such an imperceptible perturbation usually leads to a perceptible inconsistency between the original image and the diffusion-reconstructed image.
* Based on the empirical findings, we propose **IMPRESS**, i.e., **IM**perceptible **P**erturbation **RE**moval **S**y**Stem, a new platform for evaluating the effectiveness of imperceptible perturbations as a protective measure by purifying a perturbed image with consistency-based losses.
* We also test the effectiveness of adaptive protection designs which also feature our consistency-based loss upon those existing protection methods. We find that it is difficult to generate effective imperceptible perturbations through such a simple adaptive design, highlighting the need for more advanced methods to prevent unauthorized data usage in LDMs.

## 2 Related Works

**Diffusion-based Image Generation Systems.** Recently, Diffusion Probabilistic Models (DPMs) (Ho et al., 2020) have achieved impressive results in the field of image generation (Croitoru et al., 2023), including applications such as unconditional image generation(Ho et al., 2020; Nichol and Dhariwal, 2021; Song et al., 2020), text-to-image synthesis (Shi et al., 2022; Rombach et al., 2022; Saharia et al., 2022), image-to-image translation (Wolleb et al., 2022; Li et al., 2022; Wang et al., 2022; Zhao et al., 2022; Saharia et al., 2022), image editing (Avraham et al., 2022; Meng et al., 2021), and image inpainting (Lugmayr et al., 2022; Bugeau and Bertalmio, 2009). DPMs typically employ a U-Net (Ronneberger et al., 2015) architecture as the underlying neural backbone, which naturally adapts to image-like data (Dhariwal and Nichol, 2021; Ho et al., 2020; Song et al., 2020; Ronneberger et al., 2015).

**Latent Diffusion Models.** In image generation tasks, training and evaluating DPMs in the original image feature space can lead to low inference speed and high training costs. Recent works have been trying to address this issue by using advanced sampling strategies (Song et al., 2020; Kong and Ping, 2021; San-Roman et al., 2021), hierarchical approaches (Ho et al., 2022; Vahdat et al., 2021), and feature compression strategy (Rombach et al., 2022). Among them, latent diffusion model (Rombach et al., 2022) employs a pre-trained image encoder and decoder, enabling the DPM to work in a compressed, low-dimensional latent space, thus reducing the computational cost of training and accelerating inference speed while maintaining almost the same quality of synthesized images.

**Unauthorized Data Usage in Diffusion Models.** The powerful image generation and editing capabilities of AI have also raised ethical concerns, including malicious image editing (Goodfellow et al., 2014; Mirza and Osindero, 2014; Salimans et al., 2016; Isola et al., 2017; Zhu et al., 2017; Zhang et al., 2017; Karras et al., 2018; Brock et al., 2019; Karras et al., 2019; Rombach et al., 2022; Ramesh et al., 2022) or training image generation models using unauthorized images from artists and mimicking their styles (Shan et al., 2023). Some works have attempted to address this issue, such as removing certain knowledge from models (Gandikota et al., 2023) or making images unlearnable or uneditable (Huang et al., 2021; Fu et al., 2021; Shan et al., 2023; Salman et al., 2023). However, other studies argue that such imperceptible perturbations are fragile (Radiya-Dixit et al., 2021).

## 3 Preliminaries

We briefly introduce the preliminaries on Latent Diffusion Models (Rombach et al., 2022) and existing protection methods (Salman et al., 2023; Shan et al., 2023).

**Latent Diffusion Models (LDMs).** Latent diffusion model (Rombach et al., 2022) aims to train the diffusion model on a lower-dimensional latent space to generate high-quality samples while saving computation costs. Moreover, it can utilize additional information, such as text, to guide the generation process, enabling it to generate or edit images with guidance from a prompt (e.g., a text). Given an image \(\), LDM first uses an image encoder \(\) to embed it into a latent representation, i.e., \(=()\) and then perform \(T\) forward diffusion steps by progressively adding Gaussian noise \((,)\), to the latent space. For simplicity, we use \(_{1},_{2},,_{T}\) to denote latent representations at each forward diffusion step. Suppose we have a prompt \(y\) (e.g., a certain text phrase), a feature extractor \(_{}\)and we denote the embedding of \(y\) as \(_{}(y)\). The goal of the LDM is to train a conditional denoising network \(_{}\), e.g., a UNet (Ronneberger et al., 2015), to model the conditional distribution \(p(|y)\) by gradually recovering \(\) from \(_{T}\) based on \(_{}(y)\). Suppose \(_{}(_{t},t,_{}(y))\) is the Gaussian noise estimated in the \(t\)-th step. We can train \(_{}\) based on the following loss:

\[L_{}:=_{(),y, (,),t}\|-_{ }(_{t},t,_{}(y))\|_{2}^{2}.\] (3.1)

Suppose \(}\) is the latent representation obtained after \(T\) denoising steps. We can use an image decoder \(\) to obtain the generated image \(}\) based on \(}\), i.e., \(}=(})\). For simplicity, we denote the end-to-end framework to obtain \(}\) based on \(\) and \(y\) as \(}=f_{}(;y)\). Given a prompt, LDMs can be used to generate a sample or edit an image. Note that when there is no prompt \(y\), the LDM reconstructs its input image \(\), i.e., \(\) would be very similar to \(f_{}()\).

**PhotoGuard.**PhotoGuard (Salman et al., 2023) proposes to add carefully crafted perturbations to an image before making it publicly available (e.g., uploading it to the Internet) to raise the cost of malicious image editing. When using LDMs to edit the perturbed images, the output is usually less realistic. Specifically, PhotoGuard proposes two approaches: _encoder attack_ and _diffusion attack_.

The goal of _the encoder attack_ is to add a perturbation \(_{}\) to an image \(\) such that the image encoder \(\) produces similar outputs for \(+_{}\) and a target image \(_{}\) (e.g., a pure gray image or random noise). Formally, \(_{}\) can be obtained by solving the following optimization problem:

\[_{}=*{argmin}_{\|\|_{} }\|(+)-( _{})\|_{2}^{2},\] (3.2)

where \(\) is the \(L_{}\) norm perturbation budget. The noise added to the image could be imperceptible to human eyes when \(\) is small. Thus when LDMs are used to edit \(+_{}\), the resulting output would be similar to the target image instead. _The diffusion attack_, on the other hand, is more direct and aims to craft a perturbation \(_{}\) such that the output of the LDM is similar to \(_{}\):

\[_{}=*{argmin}_{\|\|_{} }\|f_{}(+)-_{}\|_{2}^{2}.\] (3.3)

Compared to the encoder attack which only targets the image encoder, the diffusion attack considers the whole LDM model with prompts, achieving better empirical performance but is less efficient.

**GLAZE.**GLAZE (Shan et al., 2023) aims to add perturbations to the artworks of an artist such that LDMs cannot learn the correct style of the artist from perturbed artworks. Given an image \(\), GLAZE first chooses a target style \(T\) that is sufficiently different from the style of \(\). Then, GLAZE transfers \(\) to the target style \(T\) using a pre-trained style transfer model \(\). For simplicity, we use \((,T)\) to denote the style-transferred image. Given \((,T)\), GLAZE crafts the perturbation \(_{}\) by solving the following optimization problem:

\[_{}=_{}\|((,T)),(+)\|_{2}^{2}+( {LPIPS}(,+)-_{L},0),\] (3.4)

where LPIPS (Learned Perceptual Image Patch Similarity) (Zhang et al., 2018) measures user-perceived image distortion, \(_{L}\) is the perturbation budget, and \(\) is a hyper-parameter adjusting the strength of the LPIPS regularization term. Roughly speaking, GLAZE aims to perturb images (e.g., artworks of an artist) such that a LDM generates samples with the target style instead of the original style when learning from the perturbed images.

## 4 Proposed Method

In this section, we formally study the potential vulnerabilities in existing protection methods and discuss our proposed method to disable perturbations added by those methods.

### Analyzing Potential Vulnerabilities of Existing Methods

We notice that existing protection methods (e.g., PhotoGuard and GLAZE) all aim to add imperceptible perturbations such that LDMs cannot "correctly" learn or process the perturbed image for style mimicking or malicious editing. While such perturbations certainly raise the cost of unauthorized data usage, it is not flawless. Specifically, one special use case of LDM is to let it _reconstruct_ the image, i.e., let an image \(\) run through the entire LDM (encoder, forward diffusion, backward diffusion,decoder) without any prompt. In such a case, the output of the LDM (denoted by \(f_{}()\)) should be close to the original input \(\). However, since the imperceptible perturbations are able to deceive the LDMs (to believe that the image is from another style or that the image's latent embedding is close to a target image), its reconstruction result \(f_{}(_{})\) may no longer be close to the perturbed input \(_{}\). Rather, it could become similar to a target image or target style instead.

Figure 2 shows an example painting by Claude Monet and its protected version by GLAZE (Shan et al., 2023). When using LDMs to reconstruct the two images respectively, we can observe that the protected image leads to noticeable inconsistency after reconstruction while the clean image stays roughly the same. In other words, the reconstructed version of the perturbed image is no longer close to itself, rather, it is more close to the targeted style (e.g., Cubism by Picasso). By contrast, a clean image and its reconstructed version do not have such inconsistency. This inspires us to leverage the inconsistency to purify the perturbed image to disable such imperceptible perturbations.

### Our Proposed Method

Based on our previous empirical findings, given a perturbed image \(_{}\) created by existing protection methods via imperceptible perturbations, we aim to purify the perturbed image to disable its perturbation. We first derive the two conditions that the purified image \(_{}\) should satisfy, then formulate **IMPRESS**, i.e., **IM**perceptible **P**erturbation **RE**moval **S**y**Stem, as an optimization problem based on those two conditions and provide our solution to solve it, and finally show our complete algorithm.

**Two conditions for purified image.** The purified image \(_{}\) should satisfy two conditions: _similarity condition_ and _consistency condition_. Next, we respectively derive them.

1. **Similarity condition.** Our ultimate goal is to make sure the purified image \(_{}\) stays visually close to the original image \(\), i.e., \(_{}\). Recall that existing methods (Salman et al., 2023; Shan et al., 2023) aim to add an imperceptible perturbation to a given image \(\) to craft a perturbed image \(_{}\) (see Section 3 for details). Therefore, the perturbed image \(_{}\) is already close to the original image \(\). Based on this observation, since we only have access to the perturbed image \(_{}\), requiring the purified image \(_{}\) to stay visually close to the perturbed image \(_{}\) would be sufficient to guarantee \(_{}\). This leads to _similarity condition_.
2. **Consistency condition.** As our goal is to find a purified image \(_{}\) close to the original image \(\), \(_{}\) should share some similar traits as \(\). Our observation in Section 4.1 suggests that a clean image can stay roughly the same after diffusion-reconstruction while the perturbed image cannot. Therefore, to make \(_{}\) close to \(\), we also require the purified image \(_{}\) to be able to reconstruct itself using LDMs, i.e., \(_{} f_{}(_{})\). This leads to _consistency condition_.

**Formulating IMPRESS as an optimization problem.** Our key idea is to define two losses that respectively quantify the similarity condition and consistency condition.

1. **Quantifying similarity condition.** Our similarity condition means that \(_{}\) would be visually close to \(_{}\). Following previous studies (Shan et al., 2023), we use LPIPS (Learned Perceptual Image Patch Similarity) (Zhang et al., 2018) to quantify the visual difference between \(_{}\) and \(_{}\). Formally, we have the loss \(((_{},_{})-_{L},0)\), where \(_{L}\) is the perceptual perturbation budget. We note that LPIPS is widely adopted in various fields (Cherepanova et al.,

Figure 2: Examples of inconsistencies observed when using LDMs to reconstruct the protected images. **Left**: the diffusion-reconstructed clean image looks similar to the original image; **Right**: images protected by GLAZE (Shan et al., 2023) leads to noticeable inconsistency after reconstruction.

2021; Laidlaw et al., 2020; Rony et al., 2021) as it provides a measure of human-perceived image distortion. Compared to \(_{p}\)-norm constraint, LPIPS offers more flexible constraint ranges. If the modified image still appears similar to the original image from a human perspective, it allows for larger changes in certain parts of the image, which aligns with our goal of preserving as much semantic information from \(_{}\) as possible.
2. **Quantifying consistency condition.** Our consistency condition implies that \(_{} f_{}(_{})\). Intuitively, we can define the consistency loss as \(||_{}-f_{}(_{})|| _{2}^{2}\). However, it is very challenging to directly optimize this loss as it involves the complicated diffusion process (i.e., gradually adding and removing Gaussian noise). To address the challenge, we simplify the loss by removing the diffusion process in it. In particular, we define the loss \(||_{}-((_{}))|| _{2}^{2}\) to quantify the consistency condition, where \(\) and \(\) are the image encoder and decoder in the LDM. Note that \(||_{}-((_{}))|| _{2}^{2}\) can approximate \(||_{}-f_{}(_{})|| _{2}^{2}\) as the goal of the diffusion process (without any prompt) is to reconstruct latent representation. Moreover, such approximation enables us to solve the optimization problem efficiently.

Combining the above two losses together, our final optimization problem is as follows:

\[_{_{}}_{}-((_{}))||_{2}^{2}}_{}+(_{},_{ })-_{L},0)}_{},\] (4.1)

where \(\) is a hyper-parameter to balance the two losses. Given a perturbed image \(_{}\), the solution to the optimization problem in Eq. (4.1) is the purified image \(_{}\). We use projected gradient descent to solve the optimization problem efficiently. In practice, we initialize \(_{}\) by \(_{}\) plus an extra small Gaussian perturbation similar as in Madry et al. (2018). A summary of our proposed method is shown in Algorithm 1 in the Appendix.

## 5 Experiments

In this section, we conduct comprehensive experiments to demonstrate the effectiveness of the proposed method and verify its capability to disable imperceptible perturbations introduced by existing protection methods. Specifically, we evaluate against two protection methods on the specific tasks for which these methods were proposed: _style mimicking_ and _malicious editing of images_.

### Experimental Settings

Style MimickingWe conduct all the _style mimicking_ experiments on the WikiArt dataset (Saleh and Elgammal, 2015). For GLAZE1(Shan et al., 2023), we essentially followed the settings of the

Figure 3: A sketch of our method consists of two losses: the _similarity loss_ and the _consistency loss_. The _similarity loss_ requires the purified image \(_{}\) to stay close to \(_{}\) while the _consistency loss_ requires the encoder/decoder reconstructed image to stay close \(_{}\) itself.

original paper and reproduced the code: given a victim artist \(V\), we randomly select 124 of their works. We use ViT+GPT2 model to generate a title for each artwork and attach the victim artist \(V\)'s name to the generated title as our prompt \(y\). Next, we randomly draw 24 works from these 124 art pieces as the style imitation training set, and the remaining 100 as the test set. We fine-tune a pre-trained LDM on the training set, and then generate new artworks mimicking the style of the victim artist by the prompt \(y\).

Malicious Editing of ImagesFor _malicious editing of images_, we utilize the code and settings made public by Photoguard (Salman et al., 2023). The purpose of _malicious editing_ is to modify the background of a photo following the given malicious prompt. We conduct our experiments on the Helen (Le et al., 2012) dataset for this task. We selected the 80 images with the smallest face-to-image ratio from the Helen dataset and their corresponding face masks. Subsequently, we use Photoguard to generate protective noise for these images. Then, we attempt to modify the images using a latent diffusion model. We generate noise using the most powerful _Diffusion attack-L2_ method in Photoguard.

BaselinesTo show the effectiveness of IMPRESS is obtained through our consistency-based loss design, we also consider three other simple post-processing baselines: JPEG compression of the image, adding Gaussian noise to the image, and resizing the image. For JPEG compression, we set the quality factor as 15%. For adding Gaussian noise, we set the mean as 0 and the variance as 0.15. For the image resizing method, we first resize the \(512 512\) image to \(256 256\), then resize it back to \(512 512\).

### Experimental Results on Style Mimicking

In Table 1, we present the experimental results for the task of _Style Mimicking_. To conduct the experiment, we fine-tune the pre-trained LDMs using clean data, GLAZE (Shan et al., 2023) protected data, and the post-processed data by our method as well as other baselines. We then use those fine-tuned models to generate new images mimicking the style of a certain artist and evaluate whether the generated image can successfully mimic the style of the artist. We consider two metrics for evaluating the generated image styles by constructing two image style classifier:

1. **the CLIP classifier accuracy.** Following the GLAZE paper (Shan et al., 2023), we use the pre-trained CLIP model (Radford et al., 2021) to build a style classier to measure whether the generated images belong to the genre of victim artist \(V\). We treat the genre classification provided in the WikiArt dataset as the ground truth label, and use the intersection of the 27 historical genres and 13 digital genres from WikiArt, as candidate labels for the CLIP model.
2. **the Diffusion classifier accuracy.** We consider another classifier that builds directly upon diffusion models. The Diffusion Classifier (Li et al., 2023) is a Zero-Shot image classification method, which uses conditional density estimates of images in pre-trained Diffusion models to construct Zero-Shot image classifiers. In our experiments, we used the same set of candidate labels (39 art genres) to build a diffusion classifier for style classification.

For each generated image, we use both the CLIP classifier and Diffusion classifier to evaluate whether the style mimicking succeeds (whether the victim artist \(V\)'s style falls into the top-3 classification results). Typically, all protection methods aim to lower the classifier accuracy as much as possible to prevent style mimicking from happening if the data usage is unauthorized. As can be seen from Table 1, the images generated using clean data samples achieve an average accuracy of 90.8% on the CLIP classifier and 95.4% on the Diffusion classifier. While the generated images using the protected data

    &  &  &  \\   & & & JPG & Noise & Resize & Ours \\  CLIP classifier Acc & 90.8 \(\) 3.2 & 42.5 \(\) 8.3 & 64.9 \(\) 7.6 & 47.9 \(\) 7.0 & 66.6 \(\) 3.4 & **87.0 \(\) 6.7** \\ Diffusion classifier Acc & 95.4 \(\) 3.1 & 42.9 \(\) 9.5 & 67.6 \(\) 9.0 & 40.9 \(\) 9.8 & 68.0 \(\) 9.5 & **82.3 \(\) 8.3** \\   

Table 1: Comparison of style mimicking performances when using fine-tuning data from clean images, GLAZE-protected images and post-processed protected images by our method and baselines. A higher accuracy suggests that the generated image style is close to the artist to be mimicked.

samples certainly lower the accuracy to \( 42\%\), after our purified process, the generated images can again achieve an average accuracy of 87.0% and 82.3% on the CLIP classifier and Diffusion classifier respectively, close to the case of clean data samples. By contrast, all the other post-processing baselines only achieved limited improved accuracy, clearly outperformed by our IMPRESS method. We also present some sample results in Figure 4. From Figure 4 (left), we can clearly observe that although the GLAZE method can successfully mislead the LDM when directly used, after using IMPRESS to purify the protected image, we can still exploit the remaining information.

### Experimental Results on Malicious Editing

For the _Malicious Editing of Images_ task, we use a pre-trained LDM to edit images following the guidance of certain prompts. Again we consider input images from clean images, images protected by Photoguard (Salman et al., 2023), and the post-processed images by our method as well as other baselines. We followed the same settings as in the Photoguard paper (Salman et al., 2023), using SSIM (Wang et al., 2004), PSNR, and VIF-p (Sheikh and Bovik, 2006) as image similarity algorithms to measure the fidelity of the generated images. We calculate similarity scores between the edited

    &  &  \\   & & JPG & Noise & Resize & Ours \\  SSIM & 0.41 \(\) 0.06 & **0.52 \(\) 0.07** & 0.18 \(\) 0.05 & 0.49 \(\) 0.08 & 0.49 \(\) 0.08 \\ PSNR & 14.24 \(\) 1.57 & 15.51 \(\) 1.69 & 11.06 \(\) 0.94 & 14.94 \(\) 1.59 & **15.61 \(\) 2.03** \\ VIF-p & 0.11 \(\) 0.03 & 0.15 \(\) 0.04 & 0.06 \(\) 0.02 & 0.14 \(\) 0.05 & **0.16 \(\) 0.04** \\   

Table 2: Comparison of malicious editing performances with input data from clean images, Photoguard-protected images and post-processed protected images by our method and baselines. The higher score indicates better alignment with the clean images.

Figure 4: The experimental results of IMPRESS. **Left:** 4 groups of samples in Style Mimicking, each group of samples is generated by Stable diffusion when using clean images, GLAZE (Shan et al., 2023) protected images, and our IMPRESS purified images for model fine-tuning. **Right:** 4 groups of samples in Malicious Editing of Images, each group of samples is obtained by editing the clean image, PhotoGuard (Salman et al., 2023) protected image, and our IMPRESS purified image.

clean images and the edited protected images, as well as the post-processed images. Higher scores indicate results that are closer to the edited clean images.

According to Table 2, our proposed IMPRESS method achieved the best performance on the PSNR and VIF-p metrics, scoring 15.61 and 0.16, respectively, suggesting the edited images are still close to true images. We also obtained a score of 0.49 on the SSIM metric, which is the second-highest score. Note that in malicious editing, even simple image transformation methods can achieve relatively good results on those comparison-based metrics, which has also been pointed out in Salman et al. (2023) and Sandoval-Segura et al. (2023). This phenomenon confirms our point of view: existing image protection technologies are fragile. In Figure 4 (right), we show some real examples that while directing applying PhotoGuard can certainly cause the edited images less realistic, after being purified by our IMPRESS, the malicious editing would still go smoothly with no obvious artifact affecting the image fidelity.

In Figure 5, we also show some comparison with other post-processing baselines on both style mimicking and image editing. We can observe that IMPRESS largely outperforms other baselines for style mimicking while on image editing, all baselines can achieve decent performances in removing the perturbations. This is also consistent with our previous quantitative results, suggesting the imperceptible perturbations on the image editing task is significantly more fragile.

### Adaptive Image Protection with Consistency-based Losses

In this section, we explore whether it is possible to leverage our consistency-based losses for building even better protection methods that could survive our IMPRESS method and successfully prevent unauthorized data usage in diffusion models. Specifically, since we know that keeping \(\) as consistent as possible with \((())\) is important, we add this regularization term to the optimization function of the existing protection method design. Take GLAZE as an example and we have the following optimization problem:

\[_{}\|((,T)),(+)\|_{2}^{2}+( (,+)-_ {L},0)+||(+)-( (+))||_{2}^{2},\]

The coefficient of the added consistency regularization term is denoted as \(\). A meticulous tuning of the weight of the adaptive loss term has been carried out, as demonstrated in Table 3. We have modulated the weight of the adaptive loss term extensively, allowing it to comprise approximately 10% to 95% of the total loss. Our observations reveal that a minuscule proportion of the adaptive loss term in the entire loss leads the experimental results to mirror those of conventional image protection techniques, whereas an excessive proportion adversely impacts the performance of image protection.

The most pronounced effect of the adaptive protection technology is observable when \(=40\), and we show the experimental results in Table 4 and 5 based on that. However, even under this condition, IMPRESS manages to attain around 80% accuracy by successfully mitigating the adaptive protective noise. Our experiments indicate that the adaptive design does not genuinely offer robust protection, since IMPRESS can still adeptly eliminate the protective noise, allowing the successful execution of style mimicking tasks, it is conjectured that attaining an optimal equilibrium among the three objective terms may pose a significant challenge.

To enhance the balance of competing objectives, we have incorporated the Orthogonal Projected Gradient Descent (O-PGD) approach by Gowal et al. (2020) into our methodology, as delineated

Figure 5: Comparison of different post-processing methods on style mimicking and image editing tasks.

in Table 6 in the pdf. However, it's discernible that the utilization of O-PGD doesn't substantially bolster the efficacy of the adaptive method.

Based on our experimental observations, we hypothesize that the subpar performance of the adaptive method stems primarily from two intertwined complexities:1) The introduced supplementary loss term complicates the optimization of the loss function. For GLAZE method, the loss function of the adaptive method has three components. Optimizing three different loss components is typically challenging, and striking a balance among these components remains an intricate task. 2) The loss term employed for the adaptive method might clash with the inherent loss term of image protection techniques. The purpose of image protection technologies is to induce semantical differences between generated images and original ones. Our image purification approach aims to make the image maintains its semantic properties (consistency after reconstruction), potentially leading to underlying optimization conflicts. These results suggest that designing adaptive image protection techniques specifically for IMPRESS might inherently be challenging.

## 6 Conclusion

In this work, proposed IMPRESS, a unified platform to evaluate the effectiveness of imperceptible perturbations as a protective measure. We demonstrated that it is difficult for the current imperceptible perturbations-based protection methods to prevent certain images from being correctly learned or processed by diffusion models as those imperceptible perturbations could be potentially removed with consistency-based losses. The proposed IMPRESS platform offers a comprehensive evaluation on several contemporary protection methods and can be used as an evaluation platform for future protection methods.

    &  &  &  \\   & & Protected & Purified & Protected & Purified \\  CLIP classifier Acc & 90.8 & 42.5 & 87.0 & 48.0 (40.8) & 83.9 (81.9) \\ Diffusion classifier Acc & 95.4 & 42.9 & 82.3 & 47.8 (44.1) & 84.(84.0) \\   

Table 6: Experimental results of adaptive defense after using O-pgd. In parentheses are the results of the adaptive defense experiments reported in the paper (without the use of O-pgd)

    &  &  &  \\   & & Protected & Purified & Protected & Purified \\  CLIP classifier Acc & 90.8 \(\) 3.2 & 42.5 \(\) 8.3 & 87.0 \(\) 6.7 & 40.8 \(\) 9.1 & 81.9 \(\) 5.5 \\ Diffusion classifier Acc & 95.4 \(\) 3.1 & 42.9 \(\) 9.5 & 82.3 \(\) 8.3 & 44.1 \(\) 9.3 & 84.0 \(\) 6.6 \\   

Table 4: Experimental Results of Adaptive Protection on Style Mimicking Tasks

    &  &  &  \\   & & Protected & Purified & Protected & Purified \\  SSIM & 0.41 \(\) 0.06 & 0.49 \(\) 0.08 & 0.42 \(\) 0.04 & 0.52 \(\) 0.05 \\ PSNR & 14.24 \(\) 1.57 & 15.61 \(\) 2.03 & 14.54 \(\) 1.74 & 15.90 \(\) 1.23 \\ VIF-p & 0.11 \(\) 0.03 & 0.16 \(\) 0.04 & 0.14 \(\) 0.04 & 0.16 \(\) 0.03 \\   

Table 5: Experimental Results of Adaptive Protection on Malicious Editing Tasks