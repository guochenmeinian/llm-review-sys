# SyncVIS: Synchronized Video Instance Segmentation

Rongkun Zheng\({}^{1}\) Lu Qi\({}^{2}\) Xi Chen\({}^{1}\) Yi Wang\({}^{3}\)

**Kun Wang\({}^{4}\) Yu Qiao\({}^{3}\) Hengshuang Zhao\({}^{1}\)\({}^{*}\)**

\({}^{1}\)The University of Hong Kong \({}^{2}\)University of California, Merced

\({}^{3}\)Shanghai Artificial Intelligence Laboratory \({}^{4}\)SenseTime Research

{zrk22@connect, hszhao@cs}.hku.hk

Corresponding author

###### Abstract

Recent DETR-based methods have advanced the development of Video Instance Segmentation (VIS) through transformers' efficiency and capability in modeling spatial and temporal information. Despite harvesting remarkable progress, existing works follow asynchronous designs, which model video sequences via either video-level queries only or adopting query-sensitive cascade structures, resulting in difficulties when handling complex and challenging video scenarios. In this work, we analyze the cause of this phenomenon and the limitations of the current solutions, and propose to conduct synchronized modeling via a new framework named SyncVIS. Specifically, SyncVIS explicitly introduces video-level query embeddings and designs two key modules to synchronize video-level query with frame-level query embeddings: a synchronized video-frame modeling paradigm and a synchronized embedding optimization strategy. The former attempts to promote the mutual learning of frame- and video-level embeddings with each other and the latter divides large video sequences into small clips for easier optimization. Extensive experimental evaluations are conducted on the challenging YouTube-VIS 2019 & 2021 & 2022, and OVIS benchmarks, and SyncVIS achieves state-of-the-art results, which demonstrates the effectiveness and generality of the proposed approach. The code is available at https://github.com/rkzheng99/SyncVIS.

## 1 Introduction

Video Instance Segmentation (VIS) is a fundamental while challenging vision task that aims to detect, segment, and track object instances inside videos based on a set of predefined object categories at the same time. With the prosperous video media, VIS has attracted various attention due to its numerous vital applications in areas such as video understanding, video editing, autonomous driving, etc.

Benefiting from favorable long-range modeling among frames, query-based offline VIS methods  like Mask2Former-VIS , and SeqFormer  begin to dominate the VIS. Inspired by the object detection method DETR , they learn a group of queries that can track and segment potential instances simultaneously across the multiple frames of a video. On the other hand, online VIS approaches like IDOL  also exploit the temporal consistency of query embeddings and associate instances via linking the corresponding query embeddings frame by frame. Albeit the success gained by those methods, we find they barely capitalize multi-frame inputs. In practice, the Mask2Former-VIS  would significantly perform worse if more input frames are given during training (evidenced in Fig. 3). This is paradoxical to our common sense that more frames could facilitate deep learning models obtaining more motion information of instances.

For this problem, many researchers  point out that the video-level queries are vitally hard to track the instances well if receiving many frames in training. That is because the trajectorycomplexity will increase in polynomials along with the number of frames. Therefore, state-of-the-art methods like SeqFormer  and VITA  usually decouple the trajectory into spatial and temporal dimensions, which are modeled by frame-level and video-level queries, respectively. Specifically, they utilize the frame-level queries to segment each frame independently and then associate these frame-level queries with video-level queries, which are responsible for the final video-level prediction. The well-trained frame-level queries guarantee the quality in the spatial dimension and thus decrease the burden of video queries. However, we argue that two issues remain in these asynchronous designs (as illustrated at the left of Fig. 1). First, with the asynchronous structure, the wellness of video-level queries heavily relies on the learning of former frame-level queries, inside which some motion information may be lost because it is an image encoding stage (rather than video encoding), which leads to the sensitivity of queries to the learning quality of former stages. Second, previous works have not solved the bipartite matching among more frames (rather than single frame), and thus the optimization complexity of trajectories remains exorbitant. Both two issues block the further development of query-based methods for video instance segmentation.

To this end, we propose to model video and frame queries synchronously with a new framework named SyncVIS to address the above-mentioned issues. Built upon DETR-style structures , our SyncVIS has two key components: the synchronized video-frame modeling paradigm and the synchronized embedding optimization strategy. Both designs put effort into unifying the frame- and video-level predictions in synchronization. The synchronized video-frame modeling paradigm makes frame- and video-level embeddings interact with each other in a query-robust parallel manner, rather than a query-sensitive cascade structure. Then the synchronized embedding optimization strategy adds a video-level buffer state to generate more tractable intermediate bipartite matching optimization compared with only frame-level losses. Fig. 1 demonstrates the schematic difference between the asynchronous state-of-the-art method and our synchronous approach. Our model is schematically simple but practically more effective, with exquisite designs as follows.

In the synchronized video-frame modeling paradigm, we employ frame and video-level embeddings in the transformer decoder to model object segmentation and tracking synchronously. Specifically, frame-level embeddings are assigned to each sampled frame, and responsible for modeling the appearance of instances, and video-level embeddings are a set of shared instance queries for all sampled frames, which are used to characterize the general motion (In the DETR-style architecture, when video queries are associated with features across time via the decoder, they can effectively model instance-level motion through the cascade structure. In Mask2Former-VIS, the use of video queries alone enables the capture of instance motion). Frame-level embeddings are kept on each frame to attend to instances locally. In each decoder layer, the video-level embeddings are aggregated to refine frame-level embeddings on the corresponding frame. The refined frame-level embeddings, in turn, are aggregated into video-level embeddings. By repeating this synchronization in decoder layers, SyncVIS incorporates the semantics and movement of instances in each frame. In the synchronized embedding optimization strategy, we focus more on video-level bipartite matching. Concretely, we decouple the input video into several clips to synchronize video and frame, and the total number of clips is related to the combinatorial number. Then, we calculate each clip loss independently by video-level bipartite matching, so that video embeddings can maintain their association ability.

We evaluate our SyncVIS on four popular VIS benchmarks, including YouTube-VIS 2019 & 2021 & 2022 , and OVIS-2021 . The experiments show the effectiveness of our method with signifi

Figure 1: Comparison of video instance segmentation paradigms. Previous methods (left part) like VITA  adopt **asynchronous** query-sensitive structures to model instance appearances and trajectories. Our model (right part) employs frame and video embeddings in a query-robust **synchronous** manner, and they synchronize with each other through the transformer decoder to generate the refined video-level query embeddings for the prediction. Also, we employ a synchronized embedding optimization strategy ’Sync. Optim.’ instead of the classic optimization approach.

cant improvement over the current state-of-the-art methods VITA , DVIS , and CTVIS . Our contributions are as follows:

* We analyze the limitations of existing video instance segmentation methods and propose a framework named SyncVIS with synchronized video-frame modeling. It can well characterize instances' trajectories under complex and challenging video scenarios.
* We develop two critical modules: a synchronized video-frame modeling paradigm and a synchronized embedding optimization strategy. The former adopts a synchronized paradigm to alleviate error accumulation in cascade structures. The latter divides large video sequences into small clips for easier optimization.
* We conduct extensive experimental evaluations on challenging VIS benchmarks, including YouTube-VIS 2019 & 2021 &2022, and OVIS 2021, and the achieved state-of-the-art results demonstrate the effectiveness and generality of the proposed approach.

## 2 Related Works

**Online video instance segmentation.** Most online VIS methods adopt the tracking-by-detection paradigm, integrating a tracking branch into image instance segmentation models. These methods predict detection and segmentation within a local range using a few frames and associate these outputs using matching algorithms. MaskTrack R-CNN  incorporates a tracking branch to Mask R-CNN . Many subsequent approaches [4; 35; 21], follow this pipeline, measuring the similarities between frame-level predictions and associating them with different matching modules. CrossVIS  uses the instance feature in the current frame to pixel-wisely localize the same instance in another frame. MinVIS  implements a query-based image instance segmentation model  on individual frames and associate query embeddings via bipartite matching.

Contrarily, some previous works [9; 19; 10; 14], draw inspiration from Video Object Segmentation , Multi-Object Tracking [8; 24; 41; 2; 26; 39], and Multi-Object Tracking and Segmentation . GenVIS  adopts a novel target label assignment strategy and builds instance prototype memory in query-based sequential learning. IDOL , based on Deformable-DETR , introduces a contrastive learning head that acquires discriminative instance embeddings for association . CTVIS  improved upon IDOL by constructing a consistent paradigm for both training and inference. However, online VIS methods usually adopt frame-level query and ignore the video-level associations across non-adjacent frames, which is problematic when handling complex long videos.

**Offline video instance segmentation.** Offline methods predict instance masks and trajectories through the whole video in one step using the whole video as input. STEm-Seg  proposes a single-stage model which learns and clusters the spatio-temporal embeddings. MaskProp  and Propose-Reduce  improve association and mask quality by mask propagation. Efficient-VIS  uses a tracklet query paired with a tracklet proposal to represent object instances. VisTR  successfully adapts DETR  to VIS, using instance queries to model the whole video. IFC  proposes inter-frame communication transformers, using memory tokens to model associations across frames. By adapting Mask2Former  to 3D spatio-temporal features, Mask2Former-VIS  becomes the state-of-the-art by exploiting its mask-oriented representation. TeViT  introduces a new approach based on transformers instead of the CNN backbone and associates temporal information efficiently. SeqFormer  decomposes the shared instance queries into frame-level box ones and utilizes video-level instance queries to relate different frames. Recently, VITA  uses object tokens to represent the whole video and employs video queries to decode semantics from object tokens. TMT-VIS  manages to jointly train multiple datasets to improve performance via different taxonomy information. However, these methods typically implement only video query or utilize asynchronous structures, and the final query-sensitive approaches have difficulties dealing with complex scenarios.

## 3 Method

Video instance segmentation can be formulated into a set prediction problem, which can be addressed by a DETR  style framework like Mask2Former . We first revisit the Mask2Former-VIS , one of the baselines that our method is built on. Then we propose a synchronized transformer named SyncVIS to address challenging video scenarios, with its two key designs.

[MISSING_PAGE_FAIL:4]

level \(_{}^{}^{1 N C}\). \(_{}^{}\) focuses on every frame separately, while \(_{}^{}\) mainly interacts with the whole video features.

Based on the design of the transformer decoder, we concurrently introduce frame- and video-level embeddings to each layer. Here, the frame- and video-level embeddings are replicated for \(T\) and \(1\) times by learnable frame- and video-level embeddings at first when given a video with \(T\) frames. Thus both the frame- and video-level embeddings pass the transformer decoder layer and two kinds of interaction operations for synchronous exchange and refinement. For each step, these two embeddings are updated as follows:

\[_{}^{}=^{}(h_{}^{}(h_{}^{}(_{}^{},))),\] (1)

where t \(\) {f, v} indicates the frame- or video-level embeddings and \(\) means the pyramid features extracted from the backbone. \(^{}\) is the embeddings processed by the \(l^{}\) transformer decoder layer. The \(h_{}^{}(q,r)\) indicates the cross-attention with video-level query embedding \(q\) and frame-level reference embedding \(r\). In our design, frame-level embeddings are assigned to each sampled frame, and responsible for modeling the appearance of instances, and video-level embeddings are a set of shared instance queries for all sampled frames, which are used to characterize the general motion (because they encode the position information of instances across frames, and thereby naturally contain the motion information).

Then, we feed the frame- and video-level embeddings into the proposed synchronous structure for mutual information exchange and refinement as follows:

\[_{}^{}= h_{}^{}( _{}^{},^{}(_{}^{}))+(1-)_{}^{},\] (2)

\[_{}^{}= h_{}^{}( _{}^{},^{}(_{}^{}))+(1-)_{}^{},\] (3)

where 'v-s' and 'f-s' mean that we only select top \(N_{k}\) embeddings in key and value to interact with the query, while 'fv' and 'vf' indicate the refinement direction of the feedforward network, from frame to video and video to frame. \(\) (set to 0.05) is the update momentum of video-level embeddings, because we presume that the aggregation of frame-level features should not change the general video-level embeddings significantly, and vice versa.

The motivation behind this approach is similar to that of the masked attention mechanism used in Mask2Former. The key difference lies in the dimension where the masking happens. In Mask2Former, the strategy is to mask out the background regions within the spatial dimension. On the other hand, our method works differently by masking out background embeddings within the key and value dimensions. This is done by selecting the top \(N_{k}\) embeddings based on the confidence scores provided by the prediction head. Therefore, while both methods aim to reduce the influence of irrelevant background information, they do so in different ways: Mask2Former masks spatially, while our method targets key and value embeddings.

### Synchronized Embedding Optimization

Video-level bipartite matching is a challenging memory-costly problem that remains asynchronous: The matching approaches from previous VIS methods are adapted directly from DETR, so the complexity of matching increases with the number of frames in a video, as the instances are not restrained to a single frame, but could be in any frame in the video. Even though larger input frames can bring more trajectory information of instances for prediction, this presents a challenge due to the resulting trajectory complexity, which scales polynomically with the input. Conversely, when the input is insufficient, the system may lack the necessary information to function optimally. Such asynchrony is the motivation of our new optimization strategy.

Regarding this, we present the synchronized embedding optimization strategy using the divide-and-conquer: if we want to associate frame \(t_{i}\) to frame \(t_{j}\) and yet the time interval may be large, an effective approach is to find \(k\), \(s.t i<k<j\), and associate \(t_{i}\) to \(t_{k}\) as well as \(t_{k}\) to \(t_{j}\). When the model achieves better segmentation results on sub-clips, combining these local optimums and we can achieve a better matching. Therefore, when generating the output predictions, we would divide the predictions into several sub-clips, and optimize each sub-clips independently. This sub-clip is like a video-level buffer to help synchronizing video-level and frame-level embeddings. By optimizing the local sub-sequence of the video, rather than the entire video sequence, if the target instance becomes occluded in certain frames, our optimizing strategy can adjust the features within the sub-sequence to adapt to this change, without being affected by the unoccluded frames. The size of sub-clips, \(T_{s}\), is variable across all VIS datasets: as for VIS datasets with fewer instances per video, such as Youtube-VIS 2019, \(T_{s}\) is set to 3, while for OVIS, \(T_{s}\) works best at 2 (discussed in Sec. 4.4). In order to further reduce the complexity for better optimization, dividing into smaller sub-clips can accelerate the optimization. Also, keeping the size of two is able to maintain the temporal information. In this way, our video-level objective \(_{}\) could be divided into several clips as follows:

\[_{}=_{0 i j T}_{( ,)},\] (4)

where \(T\) indicates the number of input frames. And the overall training loss \(\) for our model can be formulated as:

\[=_{\{,\}}_{ }^{}(_{}^{},_{ }^{})+_{\{,\}} _{}^{}(_{}^{}, _{}^{})+\]

\[_{\{,\}}_{}^{}(_{}^{},_{}^{})+ _{},\] (5)

where \(_{}^{}\) and \(_{}^{}\) denote the cross-entropy loss for frame- and video-level classification. Similarly, \(_{}^{}\), \(_{}^{}\), \(_{}^{}\), and \(_{}^{}\) denote the binary cross-entropy and dice loss for frame- and video-level mask prediction, respectively. Here \(\) is the prediction, and \(\) is the ground truth, and \(\) refers to classification while \(\) refers to mask. \(_{}\) represents the contrastive loss, which is applied in online settings (but not in offline) as IDOL  does, where the previous frame is set as a reference frame and the current frame is set as key frame.

### Implementation Details

Our method is built on detectron2 . Hyper-parameters regarding the pixel and transformer decoder are the same as these of Mask2Former-VIS . In the synchronized video-frame modeling, we set the number of frame-level and video-level embeddings \(N\) to 100. To extract the key information, we set the \(N_{k}\) to 10. Following the design of Mask2Former-VIS , we first trained our model on COCO  before training on VIS datasets. We use the AdamW  optimizer with a base learning rate of 5e-4 on Swin-Large backbone in YoutubeVIS 2019 (we use different training iterations and learning rates for different datasets). During inference, each frame's shorter side is resized to 360 pixels for ResNet  and 448 pixels for Swin . Most of our experiments are conducted on 4 A100 GPUs (80G), and on a cuda 11.1, PyTorch 3.9 environment. The training time is approximately 1.5 days when training with the Swin-L backbone.

## 4 Experiments

**Datasets and metrics.** YouTube-VIS dataset is a large-scale video database for video instance segmentation. The dataset has seen three iterations, in 2019, 2021, and 2022, with each adding more challenges to the dataset . The first iteration, YouTube-VIS 2019, contains 2.9k videos with an average duration of 4.61 seconds. The validation set has an average length of 27.4 frames per video and covers 40 predefined categories. The dataset was updated to YouTube-VIS 2021 with longer videos with more complex trajectories. As a result, the validation videos' average length increased to 39.7 frames. The most recent update, YouTube-VIS 2022, adds an additional 71 long videos to the validation set and 89 extra long videos to the test set.

OVIS dataset is another resource for video instance segmentation, particularly focusing on scenarios with severe occlusions between objects . It consists of 25 object categories and 607 training videos. Despite a smaller number of training videos compared to the YouTube-VIS datasets, the OVIS videos are much longer, averaging 12.77 seconds each. OVIS emphasizes the complexity of the scenes and the severity of occlusions between objects.

### Main Results

We compare SyncVIS with state-of-the-art approaches which are with ResNet-50 and Swin-L backbones on the YouTube-VIS 2019 & 2021 & 2022  & OVIS 2021  benchmarks. The results are reported in Tables 1, 2 and 3.

**YouTube-VIS 2019.** Table 1 shows the comparison on YouTube-VIS 2019. When applying our design to CTVIS, we discover that the forward passing of CTVIS is still asynchronous. While a single frame produces the frame embedding, there is no explicit video-level embedding to interact with the frame-level instance embedding. In our design, we add a set of video-level embeddings that gradually update with the frame-level embeddings. Our SyncVIS sets new state-of-the-art results under all of the settings. Among the online approaches, SyncVIS gets the highest performance of 57.9% AP and 67.1% AP with ResNet-50 and Swin-L backbones, which outperforms the previous best solution CTVIS  by 2.8 and 1.5 points, exceeds the top-ranking method DVIS  by 6.7 and 3.2 points, respectively. We list the model parameters and FPS of SeqFormer (220M/27.7), VITA (229M/22.8), and our SyncVIS (245M/22.1). Our model performs notably better with similar model parameters and inference speed. The two designs in SyncVIS can also boost the performance of both offline and online VIS solutions and can set new records in both settings, demonstrating the effectiveness and importance of synchronous modeling.

**YouTube-VIS 2021 & 2022.** Table 1 also compares the results on YouTube-VIS 2021. Our method hits the new records on the two backbone settings. SyncVIS achieves 51.9% AP and 62.4% AP with ResNet-50 and Swin-L backbones, respectively, outperforming the previous SOTA by 1.8 and 1.2 points, which further demonstrates the effectiveness of our approach. In Table 2, SyncVIS exceeds the previous SOTA by 1.1 points, proving its potency in handling complex long video scenarios.

**OVIS.** Table 3 illustrates the competitiveness of SyncVIS on the challenging OVIS dataset. SyncVIS also shows superior performance over other high-performance algorithms with 36.3% AP and 50.8% AP on ResNet-50 and Swin-L backbones, outperforming the current strongest architecture DVIS  by 2.2 and 0.9 points, respectively. SyncVIS harvests the highest performance on all four datasets, further evidencing its effectiveness and generality.

### Ablation Studies

We ablate our proposed components, which are conducted with ResNet-50 on YouTube-VIS 2019.

   } &  &  &  \\  & & & AP & AP\({}_{10}\) & AP\({}_{75}\) & AR\({}_{1}\) & AR\({}_{10}\) & AP & AP\({}_{50}\) & AP\({}_{75}\) & AR\({}_{1}\) & AR\({}_{10}\) \\   **VITA** \\  } & CrossVIS  & ResNet-50 & 36.3 & 56.8 & 38.9 & 35.6 & 40.7 & 34.2 & 54.4 & 37.9 & 30.4 & 38.2 \\  & MaskTrack R-CNN  & ResNet-50 & 38.6 & 56.3 & 43.7 & 35.7 & 42.5 & 36.9 & 54.7 & 40.2 & 30.6 & 40.9 \\  & MinVIS  & ResNet-50 & 47.4 & 69.0 & 52.1 & 45.7 & 55.7 & 44.2 & 66.0 & 48.1 & 39.2 & 51.7 \\  & TCOVIS  & ResNet-50 & 52.3 & 73.5 & 57.6 & 49.8 & 60.2 & 49.5 & 71.2 & 53.8 & 41.3 & 55.9 \\  & IDOL  & ResNet-50 & 49.5 & 74.0 & 52.9 & 47.7 & 58.7 & 43.9 & 68.0 & 49.6 & 38.0 & 50.9 \\  & DVIS  & ResNet-50 & 51.2 & 73.8 & 57.1 & 47.2 & 59.3 & 46.4 & 68.4 & 49.6 & 39.7 & 53.5 \\  & CTVIS  & ResNet-50 & 55.1 & 78.2 & 59.1 & 51.9 & 63.2 & 50.1 & 73.7 & 54.7 & 41.8 & 59.5 \\  & SyncVis & ResNet-50 & **57.9** & **81.3** & **60.8** & **53.1** & **64.4** & **51.9** & **74.3** & **56.3** & **43.0** & **60.4** \\   **VITA** \\  } & MinVIS  & Swin-L & 61.6 & 83.3 & 68.6 & 54.8 & 66.6 & 55.3 & 76.6 & 62.0 & 45.9 & 60.8 \\  & DVIS  & Swin-L & 63.9 & 87.2 & 70.4 & 56.2 & 69.0 & 58.7 & 80.4 & 66.6 & 47.5 & 64.6 \\  & TCOVIS  & Swin-L & 64.1 & 86.6 & 69.5 & 55.8 & 69.0 & 61.3 & 82.9 & 68.0 & 48.6 & 65.1 \\  & IDOL  & Swin-L & 64.3 & 87.5 & 71.0 & 55.6 & 69.1 & 56.1 & 80.8 & 63.5 & 45.0 & 60.1 \\  & CTVIS  & Swin-L & 65.6 & 87.7 & 72.2 & 56.5 & 70.4 & 61.2 & 84.0 & 68.8 & 48.0 & 65.8 \\  & SyncVIS & Swin-L & **67.1** & **88.9** & **73.0** & **57.5** & **71.2** & **62.4** & **84.5** & **69.6** & **49.1** & **66.5** \\   **VITA** \\  } & EfficientVIS  & ResNet-50 & 37.9 & 59.7 & 43.0 & 40.3 & 46.6 & 34.0 & 57.5 & 37.3 & 33.8 & 42.5 \\  & IFC  & ResNet-50 & 41.2 & 65.1 & 44.6 & 42.3 & 49.6 & 35.2 & 55.9 & 37.7 & 32.6 & 42.9 \\  & MaskFormer-VIS  & ResNet-50 & 46.4 & 68.0 & 50.0 & - & - & 40.6 & 60.9 & 41.8 & - & - \\  & TeVT  & MSgShifT & 46.6 & 71.3 & 51.6 & 44.9 & 54.3 & 37.9 & 61.2 & 42.1 & 35.1 & 44.6 \\  & SeqFormer  & ResNet-50 & 47.4 & 69.8 & 51.8 & 45.5 & 54.8 & 40.5 & 62.4 & 43.7 & 36.1 & 48.1 \\  & VITA  & ResNet-50 & 49.8 & 72.6 & 54.5 & 49.4 & 61.0 & 45.7 & 67.4 & 49.5 & **40.9** & 53.6 \\  & DVIS  & ResNet-50 & 52.6 & 74.5 & **58.2** & **51.2** & **61.7** & **48.9** & **71.4** & **52.8** & 40.4 & **57.9** \\   & **SyncVIS** & ResNet-50 & **54.2** & **75.1** & **58.2** & **51.2** & **61.7** & **64.4** & 51.8 & 74.6 & 58.2 & 42.8 & 58.1 \\   & SeqFormer  & Swin-L & 59.3 & 82.1 & 66.4 & 51.7 & 64.4 & 51.8 & 74.6 & 58.2 & 42.8 & 58.1 \\  & MaskFormer-VIS  & Swin-L & 60.4 & 84.4 & 67.0 & - & - & 52.6 & 76.4 & 57.2 & - & - \\  & VITA  & Swin-L & 63.0 & 86.9 & 67.9 & 56.3 & 68.1 & 57.5 & 80.6 & 61.0 & 47.7 & 62.6 \\  & DVIS  & Swin-L & 64.9 & 87.0 & **72.7** & 56.5 & 69.3 & 60.1 & **82.0** & 67.4 & 47.7 & **65.7** \\  & SyncVIS & Swin-L & **65.7** & **87.3** & 72.5 & **56.7** & **69.8** & **60.3** & 81.8 & **67.5** & **48.6** & 65.4 \\   

Table 1: Results comparison on the YouTube-VIS 2019 and 2021 validation sets. We group the results by online or offline methods, and then with ResNet-50 or Swin-L backbone structures. SyncVIS is the model to which we add our two designs based on CTVIS and VITA. Typically, since our design is orthogonally designed for decoder and optimization, our module could seamlessly integrate with both online & offline approaches without bells and whistles. Our algorithm gets the best AP performance under all of the settings.

[MISSING_PAGE_FAIL:8]

level embeddings to update the frame-level ones, and feed the frame-level embeddings to prediction heads to generate the masks and instance classes independently (denoted as 'Video\(\)Frame').

In Table 5, we find that without embedding enhancement, the decrease in performance is conspicuous as up to 6.4 points. With either unidirectional asynchronous embedding enhancement strategy, the result gets improved but is still not paired with the bidirectional synchronized video-frame modeling. This signifies several points: first, introducing frame-level embeddings to refine video-level embeddings can increment the performance by adding more frame-level instance details, thus strengthening the representative ability of video-level embeddings. Second, video-level embeddings contain more spatial-temporal information, and utilizing video-level embeddings to predict segmentation results for the video can receive better results. Third, adopting synchronized video-frame modeling is better than unidirectional modeling. Even though adding frame-specific information to video-level embeddings can contribute to representing more instance details, building the mutual association and aggregation leads to a stronger representation ability to characterize the semantics and motions.

**Modeling structure.** We suppose the superiority of using a synchronous structure over a cascade one is that the former avoids motion information loss and error accumulation. In Table 6, we evaluate these two structures. For the cascade structure, we use frame-level embeddings to extract information and associate image-level embeddings with video-level ones. The synchronized video-frame modeling and synchronized embedding optimization remain the same in cascade structure experiments. The synchronous structure gets 1.6 points higher AP performance than the cascade one, demonstrating the superior design of the proposed synchronous structure over the classical cascade structure.

**Query selection.** As shown in Table 6, utilizing only video-level queries performs better than only adopting frame-level ones. Frame-level queries segment each frame independently and focus less on the association across frames, which leads to lower performance. Our synchronous model, on the other hand, adopts both queries and achieves the best performance, validating the effectiveness of our synchronized video-frame modeling paradigm.

**Aggregation strategy.** Table 7 shows the results of different aggregation strategies in the synchronized video-frame modeling. In the 'Query Similarity', we select the most similar embeddings by computing the cosine similarity between video-level and frame-level embeddings. Note we compute similarities frame-by-frame and concatenate the top \(N_{k}\) embeddings together as input to the aggregation module. In the 'Mask Similarity', we get similarities of corresponding mask embeddings to determine the most similar ones. We use class scores (i.e., 'Class Prediction') to select key embeddings that work the best. Since some objects only appear in a few frames, the most similar embeddings may represent the background in extreme cases, disturbing the useful information for discrimination. Both aggregation methods have such problems, and using mask similarity is even worse since masks are insufficient to encode motion fully, leading to ineffective similarity calculation.

**Aggregation embedding size.** Table 8 shows the performance of SyncVIS with varying numbers of embedding in the aggregation stage of the synchronized video-frame modeling paradigm. When selecting top \(N_{k}=10\) embeddings to aggregate, the model performance reaches its best. When \(N_{k}\) decreases, the aggregated key information contained in embeddings is not sufficient, the selected one may not encode the semantic information of all instances in the video, and therefore cause the drop in performance. Alternately, when \(N_{k}\) gets larger than optimum, the redundant query features dilute the original information, which also leads to performance degradation.

### Synchronized Embedding Optimization

**Sub-clips size.** Table 8 shows the results of SyncVIS with a varying \(T_{s}\) of sub-clips. The larger the sizes of sub-clips are, the more complicated the optimization will be, and embeddings are less likely to capture the proper semantics and trajectories. When we set the size of sub-clips to 3, the model achieves its best performance. When \(T_{s}\) decreases to the lowest, the problem of optimizing the whole video descends to optimizing each frame, weakening the model's ability to associate

   Method & AP & \(_{50}\) & \(_{75}\) & \(_{1}\) & \(_{10}\) \\  Query Similarity & 49.7 & 72.8 & 53.2 & 48.7 & 60.3 \\ Mask Similarity & 48.2 & 71.6 & 52.8 & 47.8 & 59.1 \\  Class Prediction & 51.5 & 73.2 & 55.9 & 49.5 & 60.4 \\   

Table 7: Ablation study on aggregation strategies.

frames temporally. When \(T_{s}\) increases, though there is a gain in the performance when compared to undivided circumstances, the optimization is still more complex, making the training process hard to reach optimum. Learned embeddings are insufficient to capture all semantics for sub-clip, and therefore the performance is weaker than the optimal \(T_{s}\) value. However, \(T_{s}=3\) is the optimum for Youtube-VIS 2019 & 2021. For Youtube-VIS 2022 and OVIS, SyncVIS performs best when \(T_{s}\) is 2, which is the smallest size to maintain temporal associations. We suppose, that for more complex scenarios, dividing into smaller sub-clips is beneficial for query embeddings to associate across frames and accelerate the optimization. In optimization strategy, our main goal is to reduce the increasing optimization complexity as the input frame number grows. To realize this target, our strategy is to divide the video into several sub-clips that could make optimization easier while retaining the temporal motion information. Longer Sub-clips could provide the model with more temporal information, but their optimization complexity also rises polynomially. By optimizing sub-clips, models can better adapt to changes in the target instance within the video, particularly in cases of occlusion of many similar instances (In OVIS, most cases are videos with many similar instances, most of which are occluded in certain frames). By optimizing the local sub-sequence of the video, rather than the entire video sequence, if the target instance becomes occluded in certain frames, our optimizing strategy can adjust the features within the sub-sequence to adapt to this change, without being affected by the unoccluded frames.

**Generality.** The proposed optimization strategy is effective and general that can be adapted into various DETR-based approaches. In these frameworks, the optimization problem for long video sequences still exists. As in Table 9, when adding our optimization strategy to Mask2Former-VIS, we harvest notable performance gains on all three benchmarks. This demonstrates that the proposed optimization can be treated as a robust design suitable for different video scenarios.

## 5 Conclusion

We have proposed SyncVIS for synchronized Video Instance Segmentation. Unlike the current VIS approaches that use asynchronous structures, SyncVIS utilizes a synchronized video-frame modeling paradigm to encourage the synchronization between frame embeddings and video embeddings in a synchronous manner, which incorporate both semantics and movement of instances more effectively. Moreover, SyncVIS develops a plug-and-use synchronized embedding optimization strategy during training, which reduces the complexity of bipartite matching in a divide-and-conquer approach. Based on these two designs, our SyncVIS outperforms current methods and achieves SOTA on four challenging benchmarks. We hope that our method can provide valuable insights and motivate the future VIS research.

**Broader impacts and limitations.** SyncVIS is designed to propose a new synchronized structure for VIS with promising performance. We hope this work can contribute to further applications in video-related tasks and real-life applications. However, even though our model achieves promising results, it has a problem segmenting very crowded or heavily occluded scenarios, which is discussed in the supplementary.

**Acknowledgement.** This work is partially supported by the National Natural Science Foundation of China (No. 62201484), National Key R&D Program of China (No. 2022ZD0160100), HKU Startup Fund, and HKU Seed Fund for Basic Research.

   \(N_{k}\) & AP & AP\({}_{10}\) & AP\({}_{50}\) & AP\({}_{15}\) & AR\({}_{1}\) & AR\({}_{10}\) & \(T_{s}\) & AP\({}_{15}\) & AR\({}_{10}\) \\ 
5 & 51.1 & 73.0 & 55.4 & 49.1 & 59.3 & 1 & 50.9 & 73.7 & 54.9 & 49.0 & 60.1 \\
10 & 51.5 & 73.2 & 55.9 & 49.5 & 60.4 & 2 & 51.3 & 73.3 & 53.5 & 49.6 & 49.2 & 60.2 \\
25 & 50.9 & 73.5 & 55.1 & 48.4 & 59.6 & 3 & 51.5 & 73.2 & 55.9 & 49.5 & 60.4 \\
90 & 49.8 & 72.8 & 52.3 & 47.4 & 56.7 & 4 & 50.7 & 73.8 & 54.1 & 47.9 & 58.9 \\
100 & 47.5 & 70.4 & 51.4 & 46.8 & 56.1 & 5 & 50.4 & 73.5 & 54.2 & 47.2 & 58.1 \\   

Table 8: Ablation study on the aggregation embedding size \(N_{k}\) of synchronized video-frame modeling paradigm and the sub-clip size \(T_{s}\) of synchronized embedding optimization strategy.

   Datasets & Method & AP & AP\({}_{50}\) & AP\({}_{75}\) \\   & Mask2Former-VIS & 45.1 & 65.7 & 49.0 \\  & \(\) Optimization & 46.7 & 68.6 & 50.7 \\   & Mask2Former-VIS & 39.8 & 59.8 & 41.5 \\  & \(\) Optimization & 41.3 & 62.1 & 42.5 \\   & Mask2Former-VIS & 10.6 & 25.4 & 7.2 \\  & \(\) Optimization & 12.3 & 27.1 & 9.2 \\   

Table 9: Ablation study on synchronized embedding optimization strategy with ResNet-50 backbone.