# Unleashing the Power of Graph Data Augmentation

on Covariate Distribution Shift

Yongduo Sui\({}^{1}\)

This work was done during author's internship at Ant Group.

Qitian Wu\({}^{2}\)

Jiancan Wu\({}^{1}\)

Qing Cui\({}^{3}\)

Longfei Li\({}^{3}\)

Jun Zhou\({}^{3*}\)

Xiang Wang\({}^{1*}\)

Xiangnan He\({}^{1*}\)

\({}^{1}\)University of Science and Technology of China,

\({}^{2}\)Shanghai Jiao Tong University, \({}^{3}\)Ant Group

syd2019@mail.ustc.edu.cn,echo7408jtu.edu.cn,

{wujcan,xiangwang1223,xiangnanhe}@gmail.com,

{cuiqing.cq,longyao.llf,jun.zhoujun}@antgroup.com

###### Abstract

The issue of distribution shifts is emerging as a critical concern in graph representation learning. From the perspective of invariant learning and stable learning, a recently well-established paradigm for out-of-distribution generalization, stable features of the graph are assumed to causally determine labels, while environmental features tend to be unstable and can lead to the two primary types of distribution shifts. The correlation shift is often caused by the spurious correlation between environmental features and labels that differs between the training and test data; the covariate shift often stems from the presence of new environmental features in test data. However, most strategies, such as invariant learning or graph augmentation, typically struggle with limited training environments or perturbed stable features, thus exposing limitations in handling the problem of covariate shift. To address this challenge, we propose a simple-yet-effective data augmentation strategy, Adversarial Invariant Augmentation (AIA), to handle the covariate shift on graphs. Specifically, given the training data, AIA aims to extrapolate and generate new environments, while concurrently preserving the original stable features during the augmentation process. Such a design equips the graph classification model with an enhanced capability to identify stable features in new environments, thereby effectively tackling the covariate shift in data. Extensive experiments with in-depth empirical analysis demonstrate the superiority of our approach. The implementation codes are publicly available at https://github.com/yongduosui/AIA.

## 1 Introduction

While recent advances have made solid progress in learning effective representations for graph-structured data, most of the existing approaches operate under the assumption that training and test graphs are independently drawn from an identical distribution . However, this assumption often falls short in real-world scenarios due to the out-of-distribution (OOD) data that potentially exists during the testing phase , which results in distribution shifts between training and test graphs. As a result, there is increasing research interest in OOD generalization on graphs or learning with distribution shifts on graphs . Some of the typical recent works attempt to build effective methods for handling general distribution shifts on graphs, from (causal) invariant learning , model architecture designs , and data augmentation .

With more specific views, other attempts focus on designing generalizable models for tackling distribution shifts in particular domains with certain data formats, _e.g.,_ molecular graphs , recommender systems [9; 10], and anomaly detection .

However, the majority of existing studies primarily focus on the correlation shift, one type of distribution shift concerning OOD generalization [12; 13], leaving another equally important type of distribution shift, _i.e.,_ the covariate shift, largely under-explored in graph representation learning. From the perspective of invariant learning and stable learning [14; 15; 16], covariate shift is in stark contrast to correlation shift _w.r.t._ stable and environmental features of graph data. Specifically, according to the commonly-used graph generation hypothesis in prior studies [17; 18; 5; 8], there often exist stable features, which are informative features of the entire graphs and can reflect the predictive patterns in data. Based on this, the relationship between stable features and labels is assumed to be invariant across environments. The remaining features could be unstable and varying across different environments, which mainly causes the following two distribution shifts: (1) correlation shift indicates that environments and labels establish inconsistent statistical correlations in training and test data, under the assumption that test environments are covered in the training data; whereas, (2) covariate shift characterizes that the environmental features in test data are unseen in training data [2; 12].

Considering a toy example in Figure 1, the environmental features _ladder_ and _tree_ are different in training and test data, which forms the covariate shift. Taking molecular property predictions as another example, functional groups (_e.g.,_ nitrogen dioxide (NO\({}_{2}\))) are stable to determine the predictive property of molecules [5; 8]. Whereas, scaffolds (_e.g.,_ carbon rings) are usually patterns irrelevant to the molecule properties, which can be seen as environmental features [2; 19]. In practice, we often need to use molecules collected in the past to train models, expecting that the models can predict the properties of molecules with new scaffolds in the future [20; 2; 19].

Considering the differences between correlation and covariate shifts, we take a close look into the existing efforts on graph generalization. They mainly fall into the following research lines, each of which has inherent limitations to solving the covariate shift. _i) Invariant Graph Learning_[17; 18; 5; 21] recently becomes a prevalent paradigm for OOD generalization. The basic idea is to capture stable features by minimizing the empirical risks in different environments. Unfortunately, it implicitly makes a prior assumption that test environments are available during training. This assumption is unrealistic owing to the obstacle of training data covering all possible test environments. Learning in limited environments can alleviate the spurious correlations that are hidden in the training data, but fail to extrapolate the test data with unseen environments. _ii) Graph Data Augmentation_[22; 23] perturbs graph features to enrich the distribution seen during training for better generalization. It can be roughly divided into node-level , edge-level , and graph-level [26; 7] with random  or adversarial strategies . However, blindly augmenting the graphs can presumably destroys the stable features, and makes the augmented distributions out of control. For example, in Figure 1, the random strategy of DropEdge  will inevitably perturb the stable features (highlighted by red circles). As such, it may not sufficiently address the covariate shift and could potentially affect the generalization ability. Hence, we naturally ask a question: "_Compared to the training data, can we generate new data that satisfy two conditions: 1) having new environments; 2) keeping the original stable features unchanged?_"

Towards this end, we introduce two intuitive principles for graph augmentation: environmental feature discrepancy and stable feature consistency. The discrepancy principle promotes the exploration of new environments beyond the scope of training data, while the consistency principle seeks to maintain the integrity of stable features during augmentation. In order to achieve these principles, we devise a simple yet effective graph augmentation strategy: Adversarial Invariant Augmentation (AIA). Specifically, we employ an adversarial augmenter, a network that augments graphs by adversarially generating masks on them, thereby facilitating OOD exploration to enhance environmental discrep

Figure 1: \(P_{}\) and \(P_{}\) denote the training and test distributions. \(P_{}\) and \(P_{}\) represent the distributions of augmented data via DropEdge and AIA.

ancy. To foster stable feature consistency, we use another network, _i.e.,_ stable feature generator, which constructs masks that encapsulate stable features. We then delicately merge these masks and apply them to the graph data. As depicted in Figure 1, AIA primarily augments environmental features while leaving the stable elements unchanged. Our approach equips the graph classifier model with an enhanced ability to identify stable features in new environments and effectively mitigate the covariate shift issue. We also conduct extensive experiments and in-depth analyses. The experimental results highlight the limitations of several previous methods and underscore the superiority of our method in addressing the covariate shift issue, providing empirical support for our claims.

## 2 Preliminaries

We define the uppercase letters (_e.g.,_\(G\)) as random variables. The lower-case letters (_e.g.,_\(g\)) are samples of variables, and the blackboard bold typefaces (_e.g.,_\(\)) denote the sample spaces. Let \(g=(,)\) denote a graph, where \(\) and \(\) are its adjacency matrix and node features, respectively. It is assigned with a label \(y\) with a fixed labeling rule \(\). Let \(=\{(g_{i},y_{i})\}\) denote a dataset that is divided into a training set \(_{}=\{(g_{i}^{e},y_{i}^{e})\}_{e_{}}\) and a test set \(_{}=\{(g_{i}^{e},y_{i}^{e})\}_{e_{}}\). \(_{}\) and \(_{}\) are the index sets of training and test environments, respectively.

### Definitions and Problem Formations

In this work, we focus on the graph classification scenario, which aims to train models with \(_{}\) and infer the labels in \(_{}\), such as molecular property prediction. From the viewpoints of invariant learning and stable learning, the inner mechanism of the labeling rule \(\) is usually assumed to depend on the stable features [17; 18; 5; 8; 21], which are informative substructures of the entire graph. The relationship between the stable features and labels is assumed to be invariant across different environments, which makes OOD generalization possible . Environmental features in graph data are assumed to have no causal-effect on labels. For instance, the chemical properties of molecules are mainly determined by specific functional groups, which can be regarded as stable features [17; 5; 19; 8]. Conversely, their scaffold structures, often irrelevant to their properties, can be seen as environmental features [20; 8].

Due to the instability of environmental features and the limitations in the data collection process, the training and test distributions are often inconsistent in real-world scenarios, _i.e.,_\(P_{}(G,Y) P_{}(G,Y)\), which leads to two main types of distribution shifts [2; 12]: (1) Correlation shift (_aka._ spurious correlation or concept shift ) refers to \(P_{}(G|Y)+P_{}(G|Y),P_{}(G)=P_{}(G)\). It indicates that there exist spurious statistical correlations in the training data, while these correlations might not hold in the test data. (2) Covariate shift denotes \(P_{}(G|Y)=P_{}(G|Y),P_{}(G) P_{}(G)\), which means that there exist new features, _e.g.,_ environmental features, in the test data. It may be ascribed to either insufficient quantity or diversity of data in the training set, as well as the unknown characteristics of the test environments. We provide formal definitions and examples of these two distribution shifts in Appendix A. Here, inspired by the prior study , we offer a formal definition to measure the graph covariate shift.

**Definition 2.1** (**Graph Covariate Shift**): _Let \(P_{}\) and \(P_{}\) denote the probability functions of the training and test distributions. We measure the covariate shift between distributions \(P_{}\) and \(P_{}\) as_

\[(P_{},P_{})=_{}|P _{}(g)-P_{}(g)|dg,\] (1)

_where \(=\{g P_{}(g) P_{}(g)=0\}\), which covers the features (e.g., environmental features) that do not overlap between the two distributions._

\((,)\) is always bounded in \(\). The issue of graph covariate shift is very common in practice. For example, we often need to train models on past molecular graphs, and hope that the model can predict the chemical properties of future molecules with new features, _e.g.,_ new scaffolds . In addressing the graph OOD issue, a majority of the strategies [17; 5; 18; 29; 21] grounded in invariant graph learning aim to pinpoint stable or invariant features. This is mainly accomplished by minimizing empirical risks across an array of training environments. Nonetheless, these methodologies frequently operate under the assumption of a shared input space across training and test data. This assumption, however, often fails on covariate shift. It presents substantial challenges for these models when it comes to accurately identifying stable features within new testing environments. Consequently, while these methods typically exhibit satisfactory performance in managing correlation shifts, they often underperform in the face of covariate shifts. In this work, we focus on the covariate shift issue in graph classification, and we also give a formal definition of this problem as follows.

**Problem 2.2** (Graph Classification under Covariate Shift): _Given the training and test sets with environment sets \(_{}\) and \(_{}\), they follow distributions \(P_{}\) and \(P_{}\), and satisfy: \((P_{},P_{})>\), where \((0,1)\) represents the degree of covariate shift. We aim to use the data collected from training environments \(_{}\), and learn a powerful graph classifier \(f^{*}:\) that performs well in all possible test environments \(_{}\):_

\[f^{*}=*{arg\,min}_{f}\;_{e_{}} ^{e}[(f(g),y)],\] (2)

_where \(^{e}[(f(g),y)]\) is the empirical risk on the environment \(e\), and \((,)\) is the loss function._

## 3 Methodology

To solve Problem 2.2, our idea is to generate new graphs through data augmentation. In this section, we first propose two principles for graph augmentation. Guided by these principles, we design a novel graph augmentation method, AIA, which can effectively address the covariate shift issue.

### Two Principles for Graph Augmentation

We can observe that covariate shift is mainly caused by the scarcity of training environments. Hence, we first propose the discrepancy principle for graph augmentation.

**Principle 3.1** (Environmental Feature Discrepancy): _Given a graph set \(\{g\}\) with distribution function \(P\), let \(T()\) denote an augmentation function that augments graphs \(\{T(g)\}\) to distribution \(\). Then \(T()\) should meet \((P,) 1\)._

From the perspective of data distribution, it requires that \(\) should keep away from the original distribution \(P\). From the perspective of data instances, it emphasizes the discrepancy in the environments between the generated graphs and the original graphs. Since it does not give constraints on stable features, we here propose the second principle for graph augmentation.

**Principle 3.2** (Stable Feature Consistency): _Given a set of graphs \(\{g\}\) with a corresponding stable feature set \(\{g_{}=(_{},_{})\}\). Let \(T()\) denote an augmentation function that augments graphs \(\{T(g)\}\) with a corresponding stable feature set \(\{_{}=(}_{}, }_{})\}\). Then \(T()\) should meet \([\,|_{}-}_{} _{F}^{2}] 0\) and \([\,|_{}-}_{} \|_{F}^{2}] 0\), where \(\|\|_{F}\) is the Frobenius norm._

It necessitates that the stable features of the generated graphs should maintain consistency with those of the original graphs. This principle ensures the preservation of these stable features within the original training data, thereby safeguarding sufficient information pertaining to the labels. Consequently, this principle enhances the potential for generalization.

### Out-of-distribution Exploration

Given a GNN model \(f()\) with parameters \(\), we decompose \(f= h\), where \(h():^{d}\) is a graph encoder to yield \(d\)-dimensional representations, and \(():^{d}\) is a classifier. To comply with Principle 3.1, we need to do OOD exploration. Inspired by distributionally robust optimization [30; 31], we consider the following optimization objective:

\[_{}\{\;_{}\{_{}[( f(g),y)]:D(,P)\}\},\] (3)

where \(P\) and \(\) are the original and explored data distributions, respectively. \(D(,)\) is a distance metric between two probability distributions. The solution to Equation (3) guarantees the generalization within a robust radius \(\) of the distribution \(P\). To better measure the distance between distributions,as suggested by , we adopt the Wasserstein distance [33; 34] as the distance metric. The distance metric function can be defined as \(D(,P)=_{(,P)}_{}[c( ,g)]\), where \((,P)\) is the set of all couplings of \(\) and \(P\); \(c(,)\) is the cost function. Studies [35; 34] also suggest that the distances in representation space typically correspond to semantic distances. Hence, we define the cost function in the representation space and give the transportation cost as \(c(,g)=\|h()-h(g)\|_{2}^{2}\). It denotes the "cost" of augmenting the graph \(g\) to \(\). We can observe that it is difficult to set a proper \(\). Instead, we consider the Lagrangian relaxation for a fixed penalty coefficient \(\). Inspired by , we can reformulate Equation (3) as follows:

\[_{}_{}\{_{}[(f( g),y)]- D(,P)\}=_{P}[(f(g),y)]}\,,\] (4)

where \((f(g),y):=_{}\{(f(),y)-  c(,g)\}\). And we define \((f(g),y)\) as the robust surrogate loss. If we conduct gradient descent on the robust surrogate loss, we will have:

\[_{}(f(g),y)=_{}(f(^{*}),y),\ \ \ \ ^{*}=}{}\{(f( ),y)- c(,g)\}.\] (5)

\(^{*}\) is an augmented view of the original data \(g\). Hence, to achieve OOD exploration, we need to perform graph data augmentation via Equation (5) on the original data \(g\).

### Implementations of AIA

Equation (5) endows the ability of OOD exploration to data augmentation, which makes the augmented data meet the discrepancy principle. To achieve the consistency principle, we also need to capture stable features. Hence, we design a graph augmentation strategy: Adversarial Invariant Augmentation (AIA). The overview of the proposed framework is depicted in Figure 2, which mainly consists of two components: adversarial augmenter and stable feature generator. Adversarial augmenter achieves OOD exploration through adversarial data augmentation; meanwhile, the stable feature generator keeps stable feature consistency by identifying stable features from data. Below we elaborate on the implementation details.

**Adversarial Augmenter & Stable Feature Generator.** We design two networks, adversarial augmenter \(T_{_{1}}()\) and stable feature generator \(T_{_{2}}()\), which generate masks for nodes and edges of graphs. They have the same structure and are parameterized by \(_{1}\) and \(_{2}\), respectively. Given an input graph \(g=(,)\) with \(n\) nodes, mask generation network first obtains the node representations via a GNN encoder \(()\). To judge the importance of nodes and edges, it adopts two MLP networks \(_{1}()\) and \(_{2}()\) to generate the soft node mask matrix \(^{x}^{n 1}\) and edge mask matrix \(^{u}^{n n}\) for graph data, respectively. In summary, the mask generation network can be decomposed as:

\[=(g),^{x}_{i}=(_{1}( _{i})),^{a}_{ij}=(_{2}([ _{i},_{j}])),\] (6)

where \(^{n d}\) is node representation matrix, whose \(i\)-th row \(_{i}=[i,:]\) denotes the representation of node \(i\), and \(()\) is the sigmoid function that maps the mask values \(^{x}_{i}\) and \(^{a}_{ij}\) to \(\).

Figure 2: The overview of Adversarial Invariant Augmentation (AIA) Framework.

**Adversarial Invariant Augmentation.** To estimate \(^{*}\) in Equation (5), we define the following adversarial learning objective:

\[_{_{1}}\{_{}=_{P_{}} [(f(T_{_{1}}(g)),y)- c(T_{_{1}}(g),g)]\}.\] (7)

Then we can augments the graph by \(T_{_{1}}(g)=(_{}^{a}, _{}^{x})\), where \(\) is the broadcasted element-wise product. Although adversarially augmented graphs guarantee environmental discrepancy, it might destroys the stable parts. Therefore, we utilize the stable feature generator \(T_{_{2}}()\) to capture stable features and combine them with different environmental features. Following the sufficiency and invariance conditions [17; 3; 18; 5; 8; 29], we define the stable feature learning objective as:

\[_{,_{2}}\{_{}=_{P_{ }}[(f(T_{_{2}}(g)),y)+(f(),y)]\},\] (8)

where \(=(}^{a}, }^{x})\) is the augmented graph. It adopts the mask combination strategy: \(}^{a}=(^{a}-_{}^{a}) _{}^{a}+_{}^{a}\) and \(}^{x}=(^{x}-_{}^{x}) _{}^{x}+_{}^{x}\), where \(_{}^{a}\) and \(_{}^{x}\) are generated by \(T_{_{2}}()\), \(^{a}\) and \(^{x}\) are all-one matrices, and if there is no edge between node \(i\) and node \(j\), then we set \(_{ij}^{a}\) to 0. Now we explain this combination strategy. Taking \(}^{x}\) as an example, since \(_{}^{x}\) denotes the captured stable regions via \(T_{_{2}}()\), \(^{x}-_{}^{x}\) represents the complementary parts, which are environmental regions. \(_{}^{x}\) represents the adversarial perturbation, so \((^{x}-_{}^{x})_{}^ {x}\) is equivalent to applying the adversarial perturbation on environmental features, meanwhile, sheltering the stable features. Finally, \(+_{}\) signifies that the augmented data should preserve the original stable features. Consequently, it satisfies both principles. Upon analysis of Equation (8), the first term implies that the stable features are sufficient for making right predictions. The second term promotes right and invariant predictions under generated environments utilizing stable features.

**Regularization.** For Equation (7), the adversarial optimization tends to remove more nodes and edges, so we should also constrain the perturbations. Although Equation (8) satisfies the sufficiency and invariance conditions, it is necessary to impose constraints on the ratio of the stable features to prevent trivial solutions. Hence, we first define the regularization function \(r(,k,)=(_{ij}_{ij}/k-)+(_{ij}[_{ij}>0]/k-)\), where \(k\) is the total number of elements to be constrained, \(\{0,1\}\) is an indicator function. The first term penalizes the average ratio close to \(\), while the second term encourages an uneven distribution. Given a graph with \(n\) nodes and \(m\) edges, we define the regularization term for adversarial augmentation and stable feature learning as:

\[_{}}=_{P_{}}[r(_{ }^{x},n,_{a})+r(_{}^{a},m,_{ a})],\] (9)

\[_{}}=_{P_{}}[r(_{ }^{x},n,_{s})+r(_{}^{a},m,_{ s})],\] (10)

where \(_{s}(0,1)\) is the ratio of stable features, we usually set \(_{a}=1\) for adversarial learning, which can alleviate excessive perturbations. The algorithm is provided in Appendix D.1.

## 4 Theoretical Discussions

In this section, we engage in theoretical discussions to elucidate our learning objective and its connections with the covariate shift. We first explore the relationship between our optimization objective and the discrepancy principle. Recalling the optimization objective of Equation 3, we aspire to identify a distribution \(\) that can manifest within a Wasserstein ball , which is centered on distribution \(P\), with distance \(\) serving as the radius. Under appropriate conditions, we find that our learning optimization objective can establish a close connection with OOD exploration.

**Proposition 4.1**: _Consider a probability distribution \(P\) defined over a measurable space \((,)\), where \(\) denotes the sample space and \(\) is a \(\)-algebra on \(\). We construct two Wasserstein balls with \(P\) at their center and radii \(_{1}\) and \(_{2}\) respectively. Utilizing Equation 3, we generate two distinct distributions, \(_{1}\) and \(_{2}\), within the space \((,)\). If (i) \(P\) is an isotropic distribution; (ii) \(_{1},_{2}\) such that \(P()=_{1}(-_{1})=_{2}( -_{2})\); (iii) \(_{1}<_{2}\), then we have \((P,_{1})(P,_{2})\)._

This suggests that by appropriately increasing the robustness radius in AIA, we can effectively amplify the covariate shift between the training and generated distributions. This in turn underscores the reliability of our discrepancy principle, to a certain degree. Comprehensive proofs and detailed discussions supporting these conclusions can be found in Appendix B.

## 5 Experiments

In this section, we conduct extensive experiments to answer the following **R**esearch **Q**uestions:

* **RQ1:** Compared to existing efforts, how does AIA perform under covariate shift?
* **RQ2:** Can the proposed AIA achieve the principles of environmental feature discrepancy and stable feature consistency, thereby alleviating the graph covariate shift?
* **RQ3:** How do the different components and hyperparameters of AIA affect the performance?

### Experimental Settings

**Datasets.** We use graph OOD datasets  and OGB datasets , which include Motif, CMNIST, Molbbbp, and Molhiv. Following , we adopt the base, color, size, and scaffold data splitting to create various covariate shifts. The details of the datasets, metrics, implementations, and other settings are provided in Appendix D.2 and D.4. More experiments are provided in Appendix E.

**Baselines.** We adopt 16 baselines, which can be divided into the following three specific categories:

* **General Generalization Algorithms:** ERM, IRM , GroupDRO , VREx .
* **Graph Generalization Algorithms:** DIR , CAL , GSAT , OOD-GNN , StableGNN , CIGA , DisC .
* **Graph Augmentation:** DropEdge , GREA , FLAG , M-Mixup , \(\)-Mixup .

### Main Results (RQ1)

We first make comparisons with various baselines in Table 1, and have the following observations:

Most generalization and augmentation methods fail under covariate shift. VREx achieves a 2.81% improvement on Motif (base). For two shifts of Molhiv, data augmentation methods GREA and DropEdge obtain 1.20% and 0.77% improvements. The invariant learning methods, _i.e.,_ DIR and CAL also obtain 4.60% and 1.53% improvements on CMNIST and Molbbbp (size). Unfortunately, none of the methods consistently outperform ERM. For example, GREA and DropEdge perform poorly on Motif (base), \(\)11.92% and \(\)23.58%. DIR and CAL also fail on Molhiv. These show that both invariant learning and data augmentation methods have their own weaknesses, which lead to unstable performance when facing complex and diverse covariate shifts from different datasets.

AIA consistently outperforms most baseline methods. Compared with ERM, AIA can obtain significant improvements. For two types of covariate shifts on Motif, AIA surpasses ERM by 4.98% and 4.11%, respectively. In contrast to the large performance variances on different datasets achieved by

    &  &  &  &  &  \\   & & base & size & color & scaffold & size & scaffold & size \\   & ERM & 68.66\({}_{4.25}\) & 51.74\({}_{2.88}\) & 28.60\({}_{1.87}\) & 68.10\({}_{1.68}\) & 78.29\({}_{3.76}\) & 69.58\({}_{2.51}\) & 59.94\({}_{2.37}\) \\  & IRM & 70.65\({}_{4.17}\) & 51.41\({}_{1.78}\) & 78.23\({}_{2.13}\) & 67.22\({}_{2.11}\) & 77.56\({}_{2.48}\) & 67.97\({}_{1.84}\) & 59.00\({}_{2.92}\) \\ Generalization & GroupDRO & 68.24\({}_{8.92}\) & 51.95\({}_{3.68}\) & 29.07\({}_{3.14}\) & 66.67\({}_{2.39}\) & 79.27\({}_{2.13}\) & 70.64\({}_{2.27}\) & 58.98\({}_{2.16}\) \\  & VREx & 71.47\({}_{4.69}\) & 52.67\({}_{5.54}\) & 28.48\({}_{4.27}\) & 68.74\({}_{4.10}\) & 78.62\({}_{2.37}\) & 70.77\({}_{2.24}\) & 58.53\({}_{2.38}\) \\   & DIR & 62.07\({}_{4.85}\) & 52.27\({}_{4.56}\) & 33.20\({}_{6.17}\) & 66.86\({}_{2.25}\) & 76.40\({}_{4.43}\) & 68.07\({}_{2.29}\) & 58.08\({}_{2.31}\) \\  & CAL & 65.63\({}_{4.29}\) & 51.85\({}_{2.50}\) & 27.93\({}_{2.34}\) & 68.06\({}_{2.60}\) & 79.50\({}_{4.81}\) & 67.37\({}_{3.51}\) & 57.95\({}_{2.24}\) \\  & GSAT & 62.80\({}_{4.11}\) & 53.20\({}_{3.53}\) & 28.71\({}_{4.16}\) & 67.84\({}_{7.14}\) & 75.63\({}_{3.53}\) & 66.63\({}_{1.39}\) & 58.06\({}_{4.19}\) \\  & OOD-GNN & 61.10\({}_{1.97}\) & 52.61\({}_{4.67}\) & 26.49\({}_{2.94}\) & 66.72\({}_{2.13}\) & 79.48\({}_{4.19}\) & 70.46\({}_{1.97}\) & 60.60\({}_{3.07}\) \\  & StableGNN & 57.07\({}_{4.10}\) & 46.93\({}_{8.85}\) & 28.38\({}_{4.19}\) & 66.74\({}_{1.30}\) & 77.47\({}_{4.89}\) & 68.44\({}_{4.13}\) & 56.71\({}_{2.79}\) \\  & CIGA & 66.43\({}_{1.13}\) & 49.14\({}_{8.34}\) & 32.22\({}_{2.27}\) & 64.92\({}_{2.09}\) & 65.98\({}_{3.81}\) & 69.40\({}_{2.39}\) & 59.55\({}_{2.56}\) \\  & DisC & 51.08\({}_{3.38}\) & 50.39\({}_{1.15}\) & 24.99\({}_{1.78}\) & 67.12\({}_{2.11}\) & 56.59\({}_{1.09}\) & 68.07\({}_{4.15}\) & 58.76\({}_{6.91}\) \\   & DropEdge & 45.08\({}_{4.46}\) & 45.63\({}_{4.61}\) & 24.65\({}_{2.50}\) & 66.49\({}_{1.55}\) & 78.32\({}_{2.44}\) & 70.78\({}_{1.38}\) & 58.53\({}_{1.12}\) \\  & GREA & 56.74\({}_{6.23}\) & 54.31\({}_{4.02}\) & 29.02\({}_{2.36}\) & 67.24\({}_{6.79}\) & 74.34\({}_{3.52}\) & 67.79\({}_{2.56}\) & 67.41\({}_{2.29}\) \\   & FLAG & 61.12\({}_{5.39}\) & 51.66\({}_{4.14}\) & 32.30\({}_{2.69}\) & 67.69\({}_{2.36}\) & 79.26\({}_{2.26}\) & 68.45\({}_{2.30}\) & 60.59\({}_{2.95}\) \\   & M-Mixup & 70.08\({}_{3.82}\) & 51.48\({}_{4.19}\) & 26.47\({}_{4.45}\) & 68.75\({}_{3.43}\) & 78.92\({}_{2.43}\) & 68.88\({}_{2.43}\) & 59.03\({}_{3.31}\) \\   & \(\)-Mixup & 59.66\({}_{6.73}\) & 52.81\({}_{6.73}\) & 31.85\({}_{5.82}\) & 67.44\({}_{1.62}\) & 78.55\({}_{1.16}\) & 70.01\({}_{2.52}\) & 59.34\({}_{2.43}\) \\   & AIA (ours) & **73.64\({}_{8.15}\)** & **55.85\({}_{7.98}\)** & **36.37\({}_{4.44}\)** & **70.79\({}_{1.53}\)** & **51.03\({}_{4.15}\)** & **61.64\({}_{4.37}\)** \\  , AIA consistently obtains the leading performance across the board. For CMNIST, AIA achieves a performance improvement of 3.17% compared to the best baseline DIR. For Motif, the performance is improved by 2.17% and 1.72% compared to VREx and GREA. These results illustrate that AIA can overcome the shortcomings of invariant learning and data augmentation. Armed with the principles of environmental feature diversity and stable feature invariance, AIA achieves stable and consistent improvements on different datasets with various covariate shifts. In addition, although we focus on covariate shift in this work, we also carefully check the performance of AIA under correlation shift, and the results are presented in Appendix E.

### In-depth Analyses (RQ2)

In this section, we conduct qualitative and quantitative experiments to support our two principles. Firstly, we utilize \((,)\) as the measurement to quantify the degree of covariate shift. The detailed estimation procedure is provided in Appendix C. We select four different domains, _i.e.,_ base, size, color and scaffold, to create covariate shifts. The experimental results are shown in Table 2. We calculated covariate shifts between the augmentation distribution \(P_{}\) with the training \(P_{}\) or test distribution \(P_{}\). "Aug-Train" and "Aug-Test" represent \((P_{},P_{})\) and \((P_{},P_{})\), respectively. From the results in Table 2, we make these observations.

**Discrepancy Principle.** The term "Original" denotes the training distribution prior to augmentation. It's observed that substantial covariate shifts exist between the training and test distributions, ranging from 0.419 to 0.557. The DropEdge technique notably expands _Aug-Train_, with a range of 0.627 to 0.851, while concurrently increasing _Aug-Test_, as evidenced by CMNIST (0.490 to 0.539) and Molbbbb (0.419 to 0.737). However, a distribution that deviates excessively from the test distribution may not effectively address the issue of covariate shift. FLAG, which perturbs only the node features, yields minor values in both _Aug-Train_ and _Aug-Test_. \(\)-Mixup notably augments _Aug-Train_ by generating OOD samples, but doesn't necessarily limit _Aug-Test_. Finally, AIA extends the disparity with the training distribution by augmenting environmental features, signifying that AIA can adeptly implement the principle of environmental feature discrepancy. Simultaneously, the imposed consistency constraint on stable features restricts the generated distribution from straying too far from the test distribution, as observed in Motif-base (0.557 to 0.462), Motif-size (0.522 to 0.098), and CMNIST (0.490 to 0.307).

**Augmentation Diversity.** We further delve into the diversity of data augmentation. The concept of augmentation diversity stems from the intuition that augmentations with greater degrees of freedom yield better performance . Accordingly, we propose conditional entropy to

    &  &  &  &  \\   & Aug-Train & Aug-Test & Aug-Train & Aug-Test & Aug-Train & Aug-Test & Aug-Train & Aug-Test \\  Original & 0 & 0.557\(\)0.141 & 0 & 0.522\(\)0.421 & 0 & 0.490\(\)0.226 & 0 & 0.419\(\)0.079 \\  DropEdge & 0.772\(\)0.213 & 0.515\(\)0.033 & 0.851\(\)0.018 & 0.161\(\)0.271 & 0.627\(\)0.186 & 0.539\(\)0.260 & 0.758\(\)0.192 & 0.737\(\)0.218 \\ FLAG & 0.001\(\)0.001 & 0.533\(\)0.016 & 0.002\(\)0.018 & 0.507\(\)0.121 & 0.003\(\)0.002 & 0.442\(\)0.002 & 0.001\(\)0.001 & 0.413\(\)0.008 \\ \(\)-Mixup & 0.690\(\)0.186 & 0.472\(\)0.038 & 0.816\(\)0.154 & 0.299\(\)0.343 & 0.408\(\)0.228 & 0.351\(\)0.138 & 0.551\(\)0.258 & 0.545\(\)0.231 \\ AIA (ours) & 0.369\(\)0.169 & 0.462\(\)0.063 & 0.649\(\)0.143 & 0.098\(\)0.070 & 0.516\(\)0.106 & 0.307\(\)0.106 & 0.422\(\)0.049 & 0.393\(\)0.028 \\   

Table 2: Covariate shift comparisons with different augmentation strategies.

   Method & Full Graph & Env. Feature & Sta. Feature \\  DropEdge & 0.999\(\)0.065 & 0.933\(\)0.029 & 0.971\(\)0.067 \\ AIA (ours) & 0.561\(\)0.223 & 0.508\(\)0.136 & 0.259\(\)0.106 \\   

Table 3: Augmentation Diversity.

Figure 3: Visualizations of the augmented graphs via AIA.

measure the diversity of generated data: \(H(|G)=-_{G}[_{}p(|G)p (|G)]\), where \(\) and \(G\) represent the generated graph and original graph, respectively. To substantiate our ability to manage perturbed environmental features while preserving stable features, we examine diversity at the feature level, _i.e.,_ stable and environmental features. We employ the Motif dataset for validation due to its inclusion of ground-truth labels for stable and environmental features. The results in Table 3 reveal that our approach can guarantee the diversity of environmental features while constraining the variation of stable features.

To substantiate two principles of AIA, we present a selection of augmented graphs in Figure 3. These graphs are randomly sampled during the training phase. In the original Motif, the stable feature, is represented by the green portion and its type determines the label, while the yellow portion signifies the base-graph, or the environmental feature. Figure 3 (_Right_) exhibits the augmented samples generated during training. Nodes depicted in darker colors and edges with broader lines indicate higher soft-mask values. These results lead us to several noteworthy observations.

**Visualization Analyses.** AIA primarily perturbs the environmental features, while leaving the stable components undisturbed. In the Motif dataset, the base-graph represents a _ladder_ and the motif-graph signifies a _house_. Following augmentation, the nodes and edges of the _ladder_ graph undergo perturbations. However, the _house_ component remains consistently stable throughout the training process. It shows that AIA successfully adheres to the proposed two principles, thus providing empirical support for our claims. Furthermore, under covariate shift, we also depict the stable features identified by AIA in comparison to other baseline methods (refer to Appendix E.4). It further underscores the limitations of alternative methods and highlights the superior performance of AIA.

### Ablation Study (RQ3)

**Adversarial Augmentation & Stable Feature Learning.** As illustrated in Figure 4 (_Left_), "w/o Adv" and "w/o Sta" denote AIA without adversarial augmentation and without stable feature learning, respectively. RDIA is a variant that replaces adversarial augmentation in AIA with random augmentation (_i.e.,_ random masks). The performance degrades when either component is used independently, compared to their combined application in AIA. The removal of adversarial perturbations results in a loss of the invariance condition inherent in stable feature learning [17; 29], leading to suboptimal outcomes. Conversely, the sole use of adversarial augmentation disrupts the stable features, thereby diminishing the performance. RDIA surpasses ERM, yet falls short of AIA, indicating that although randomness can foster discrepancy, it is less effective than the adversarial strategy.

**Sensitivity Analysis.** We conduct experiments to explore the sensitivities of ratio \(_{s}\) and penalty coefficient \(\). The results are displayed in Figure 4 (_Middle_) and (_Right_). \(_{s}\) with 0.3-0.8 performs well on Motif and Molbbbp, while Molhiv is better in 0.1-0.3. It indicates that \(_{s}\) is a dataset-sensitive hyper-parameter that needs careful tuning. For \(\), the appropriate values range from 0.1-1.5.

## 6 Related Work

**Graph Data Augmentation**[22; 23; 43] enlarges the training distribution by perturbing features in graphs. Recent studies [44; 13] observe that it often outperforms other generalization efforts [14; 31]. DropEdge  randomly removes edges, while FLAG  augments node features with an adversarial strategy. M-Mixup  interpolates graphs in semantic space. However, studies [14; 45]

Figure 4: (_Left_): Different components in AIA. (_Middle_): Different ratios \(_{s}\) of stable features. (_Right_): Different penalties \(\). Dashed lines denote the ERM.

point out that stable features are the key to OOD generalization. These augmentation efforts are prone to perturb the stable features, which easily lose control of the augmented data distribution.

**Invariant Graph Learning** has been widely adopted by recent works as a paradigm for handling distribution shifts on graphs. The pioneering works [3; 17] leverage the causal invariance principle to model the invariant predictive patterns in data for the OOD generalization purpose. With the similar spirit, GREA  and CAL  aim to learn stable features by considering different environments. Some other works also utilize invariant learning to develop generalizable models and algorithms for improving the generalization _w.r.t._ molecular graphs  and recommender systems .

**Out-of-Distribution Learning on Graphs** has aroused wide research interest in the graph learning community. One line of research is centered around the goal of improving the OOD generalization capabilities of models when encountered with test data from new unseen distributions [46; 3; 47; 39; 40; 17; 38; 48; 5]. Another line of research, differently, aims to identify the OOD data in the testing set and improve the reliablity of models against OOD data for which the model should reject for prediction [49; 50; 51]. The latter task is called Out-of-Distribution Detection in the literature and serves as another under-explored area that has different technical aspect from the present work. Due to space constraints, we put more discussions of other related studies in Appendix G.

## 7 Conclusion

In this study, we address the pervasive yet largely unexplored issue of covariate shift in graph learning. We introduce a novel graph augmentation method, AIA, grounded in two principles: environmental feature discrepancy and stable feature consistency. The discrepancy principle enables the model to explore new environments, thereby facilitating better generalization to potentially unseen environments. Meanwhile, the consistency principle maintains the integrity of stable features. We conduct extensive comparisons with various baseline models and perform thorough analyses.

## 8 Limitations and Broader Impacts

This paper presents a graph augmentation method, AIA, designed to bolster the academic community's application of data augmentation methodologies. We do not foresee any immediate, direct, or adverse societal implications resulting from our study's findings. We also present additional discussions regarding AIA's limitations and potential future work in Appendix H.