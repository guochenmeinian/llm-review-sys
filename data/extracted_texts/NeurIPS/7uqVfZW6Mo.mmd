# Not All Diffusion Model Activations

Have Been Evaluated as Discriminative Features

Benyuan Meng\({}^{1,2}\) Qianqian Xu\({}^{3,4}\)+ Zitai Wang\({}^{3}\)

\({}^{1}\)Institute of Information Engineering, CAS

\({}^{2}\)School of Cyber Security, University of Chinese Academy of Sciences

\({}^{3}\)Key Lab. of Intelligent Information Processing, Institute of Computing Technology, CAS

\({}^{4}\)Peng Cheng Laboratory

\({}^{5}\)School of Cyber Science and Tech., Shenzhen Campus of Sun Yat-sen University

\({}^{6}\)School of Computer Science and Tech., University of Chinese Academy of Sciences

\({}^{7}\)Key Laboratory of Big Data Mining and Knowledge Management, CAS

mengbenyuan@iie.ac.cn {xuqianqian,wangzitai}@ict.ac.cn

caoxiaochun@mail.sysu.edu.cn qmhuang@ucas.ac.cn

Corresponding authors.

###### Abstract

Diffusion models are initially designed for image generation. Recent research shows that the internal signals within their backbones, named activations, can also serve as dense features for various discriminative tasks such as semantic segmentation. Given numerous activations, selecting a small yet effective subset poses a fundamental problem. To this end, the early study of this field performs a large-scale quantitative comparison of the discriminative ability of the activations. However, we find that many potential activations have not been evaluated, such as the queries and keys used to compute attention scores. Moreover, recent advancements in diffusion architectures bring many new activations, such as those within embedded ViT modules. Both combined, activation selection remains unresolved but overlooked. To tackle this issue, this paper takes a further step with a much broader range of activations evaluated. Considering the significant increase in activations, a full-scale quantitative comparison is no longer operational. Instead, we seek to understand the properties of these activations, such that the activations that are clearly inferior can be filtered out in advance via simple qualitative evaluation. After careful analysis, we discover three properties universal among diffusion models, enabling this study to go beyond specific models. On top of this, we present effective feature selection solutions for several popular diffusion models. Finally, the experiments across multiple discriminative tasks validate the superiority of our method over the SOTA competitors. Our code is available at this url.

## 1 Introduction

Diffusion models [21; 10; 37; 36] are powerful generative models that progressively reconstruct images from Gaussian noises through a series of denoising steps. Typically, a U-Net  is trained as the noise predictor backbone to perform denoising. Recently, the impressive generative capability inspires the application to discriminative tasks such as semantic segmentation [52; 58] or semantic correspondence [29; 56]. In this direction, _diffusion feature_ is one simple yet effective approach, where the intermediate signals, named activations, are extracted from the pre-trained diffusion U-Net as dense features [2; 58; 56; 29; 53; 13; 14].

The complex architecture of the diffusion U-Net provides many activations that can serve as features. However, these activations inherently vary in quality, inducing significant performance gaps on discrimination. Hence, selecting a small yet effective subset from these activations has become a fundamental problem. In the early stage, Baranchuk _et. al_ perform a large-scale quantitative comparison among activations within Guided Diffusion . Later, the activations they select are followed by most studies in this field, pursuing other improvements .

However, we find that this fundamental issue is far from solved. On one hand, Baranchuk _et. al_ only consider activations between neighboring modules that comprise the main residuals. This means that many potential activations are excluded from the candidate pool, such as the queries and keys in the self-attention blocks. Moreover, recent developments of diffusion architecture, such as cross-attention  or embedded deep vision transformers (ViTs) , have introduced additional types of activations. Hence, as shown in upper Figure 1, only a small fraction of potential activations in modern diffusion models have been evaluated for their discriminative ability, which might hinder future work in this direction. For example, lower Figure 1 shows that Stable Diffusion XL (SDXL) , which is more advanced than Stable Diffusion v1.5 (SDv1.5) , fails to achieve better performance with the feature selection solution proposed in .

In this paper, we revisit the fundamental problem of feature selection, considering a more comprehensive candidate pool of activations. Due to its large volume, a full-scale quantitative comparison is no longer operational, urging us to modify the previous research methodology in . As illustrated in Figure 1, instead of a direct quantitative comparison, we first explore the properties of diffusion U-Nets. These properties allow us to qualitatively and efficiently filter out many activations that are highly likely to be sub-optimal, shrinking the candidate pool for the quantitative comparison. More importantly, we find these properties are universal among diffusion models, making it possible to generalize our findings to more models beyond those covered in this paper.

Specifically, the properties we find, which are distinct to existing knowledge of model properties , exactly correspond to three top-to-bottom levels of the diffusion U-Net: (i) **Diffusion noises** at the macro level: the diffusion process induces a new type of noise on both low- and high-frequency signals. (ii) **In-resolution granularity changes** at the in-resolution level: the changes in information granularity are not only across **but** also within resolutions. (iii) **Locality without positional embeddings** at the sub-module level: the embedded ViTs in diffusion U-Nets present a new type of local information different from that induced by the conventional positional embeddings . Based on these insights, we develop effective feature selection solutions for several popular diffusion models. Finally, the experiments on three discriminative tasks, including semantic correspondence, semantic segmentation, and label-scarce segmentation, validate the superiority of our solutions over the SOTA methods.

Figure 1: Prior arts only consider a small fraction of potential activations in diffusion models. As a result, more advanced diffusion architecture fails to achieve better performance (SDXL _v.s._ SDv1.5). In contrast, we consider a broader range of candidate activations. To facilitate the quantitative comparison, we first make a comprehensive and generalizable analysis to qualitatively filter out many candidates in advance. On top of this, our method achieves superior performance (75.2 PCK@0.1).

In summary, the contributions of this work are three-fold:

* **Revisit of a fundamental problem**: To the best of our knowledge, we are the first to point out that the fundamental issue of feature selection remains unresolved in the realm of diffusion feature.
* **Generic insights**: The properties we find are universal among different diffusion U-Nets, which can provide valuable insights for future work.
* **Extensive validation**: Extensive experiments on three discriminative tasks validate the effectiveness of the feature selection solutions induced by our insights.

## 2 Related Work

**Diffusion models for discrimination.** We discuss three mainstream ways to use diffusion models for discrimination. (i) Diffusion classifier [26; 7] utilizes Bayes' theorem to transform a pre-trained diffusion model into an image classifier. This method enjoys a theoretical guarantee and does not need additional training. However, it is limited to image-level tasks. (ii) The second way is to model discriminative tasks as image-to-image generation tasks with diffusion models [22; 23]. This method is suitable for various dense vision tasks but requires heavy training. (iii) Diffusion feature [2; 58; 56; 41; 29], the focus of this paper, follows the traditional practice of feature extraction to pursue the balance between wider applicability and less training. This makes it adaptable for different tasks and alleviates the training needs for the diffusion model. Only small downstream models may require training.

The diffusion feature approach has seen various improvements. Some techniques toggle the input settings of diffusion models, such as seeking better timesteps [60; 29]. The others add trainable parameters outside the diffusion model to refine the outputs or provide an efficient fine-tuning alternative [58; 27; 46]. Additionally, some studies explore completely training-free methods that utilize spatial attention information [51; 43], and some attempts focus on text-free diffusion models [32; 33].

Despite the progress, previous methods only consider activations between neighboring modules, leaving many potential candidates unevaluated. Our work shows that such an overlook can hinder model performance. By introducing a more comprehensive feature selection solution, our method could generically enhance both existing and future diffusion feature approaches.

**Analysis on model properties.** The inner properties of neural networks have consistently received much attention [5; 34]. For transformers, Geva _et al._ show that the feed-forward layers act as key-value memories and are interpretable for humans. Amir _et al._ extend this insight to vision transformers and put it into practical applications such as image classification, while Vilas _et, al._ try to make more detailed interpretation of ViT activations.

The methodology of these studies provides valuable guidance for our research. However, since diffusion models are trained for generation, our study relies more on qualitative analysis and feature visualization compared to previous work.

## 3 Preliminaries: Architecture of Diffusion U-Nets

Diffusion models typically involve a forward pass and a reverse pass . During the forward pass, noise is gradually added to a clean image \(_{0}^{3 w h}\) until the image resembles Gaussian noise, where \(w\) and \(h\) denote the width and height, respectively. This process can be denoted by \(_{t} q(_{t}|_{0})\), where \(q\) represents the noise posterior and \(t\) is the timestep. In the reverse pass, an end-to-end neural network \(_{}\), as parameterized by \(\), learns to predict the noises and thus reconstruct the image. One such denoising step can be denoted as \(=_{}(_{t},t,)\), where \(\) is the predicted noise, and \(\) is the condition that describes the expected image content. Although there are alternative formulations for diffusion models [39; 28; 40], they all rely on this neural network backbone \(_{}\). This study focuses on this backbone, typically implemented as U-Net , rather than other components of diffusion models. Next, we will detail the architecture of the diffusion U-Net and standardize the _terminology_ referring to different parts of the model, as shown in Figure 2.

We start with an overview. Following the initial U-Net design , the U-Net has three main _stages_: down-stage, mid-stage, and up-stage. The down-stage reduces the resolution of activations, while the up-stage increases it. Both stages contain multiple _resolutions_, in each of which the activations share the same resolution. Furthermore, each resolution includes several _modules_, including _ResModule_ (convolutional ResNet  structures), _ViT Module_, and _Downsampler/Upsampler_ (simple convolutional layers). Previous diffusion feature approaches only consider activations between these adjacent modules, _i.e._, inter-module activations.

We next dive into the details below the module level. Among these modules, ResModule and ViTs adopt _residual connection_, where an increment activation is added element-wise to the residual activation to refine it. Specifically, ResModule uses simple convolutional layers to produce increments, whereas ViTs use complex attention mechanisms, which will be further explained next. Typically, ViT operates as a standalone model followed by a decoder that produces the output predictions for visual tasks [12; 25; 9; 18]. However, in the diffusion U-Net, multiple ViTs are embedded into the network, and their outputs serve as increment activations.

Furthermore, each ViT module consists of several stacked _basic blocks_. A basic block typically has a _self-attention layer_ to perform attention on the image itself and a _feed-forward layer_, essentially a two-layer MLP . Modern diffusion U-Net introduces an additional _cross-attention layer_ between the two layers, enabling the fusion of the image and additional textual prompts. Besides, each layer includes a residual connection, meaning that the increment activation added to the outer residual is also the internal residual.

In a nutshell, the architecture described above provides abundant activations that can serve as dense features. Given the massive activations, it is no longer operational to perform a full-scale quantitative comparison. Hence, we next make a comprehensive analysis of model properties to better understand these activations, which can help the qualitative filtering.

## 4 Distinct Properties of Diffusion U-Nets

The diffusion U-Net has many interesting properties, but now we only focus on those distinct from the knowledge of traditional U-Net  or ViTs . As shown in Figure 3, this section highlights three noticeable properties, each of which corresponds to a different level of the diffusion U-Net architecture described in Section 3. Notably, **these properties can be universally observed in different samples and diffusion models, though all visualization in the main content is conducted on the same image and SDXL model for consistency**. Additional visualization is provided in Appendix A. Besides, the omitted properties are available in Appendix E.

Figure 2: U-Net architecture (upper) and the ViT module (lower), taking SDXL as an example.

### Asymmetric Diffusion Noises

The first property is at the macro level and closely related to the overall diffusion process. It is common that high-frequency signals are typically noisy [5; 34]. This phenomenon can also be observed in diffusion U-Nets, especially within the increment activations of residual connection. However, this does not mean that low-frequency signal activations in diffusion U-Nets are free from noises. As shown in Figure 3(a), the diffusion process introduces a new type of noise that also impacts low-frequency signals. This is not surprising since the diffusion process requires the backbone to process noisy inputs and predict noises as outputs. As a result, activations near the inputs or outputs, regardless of their frequency, also suffer from such noises. Considering the special cause, we name such noises _diffusion noises_.

How does the influence of diffusion noises spread across diffusion U-Nets? As shown in Figure 3(a), diffusion noises exist throughout the entire down-stage, with a decreasing magnitude. Remarkably, during the early half of the up-stage, activations are rather clean, with no perceivable diffusion noises. Only in the later half do diffusion noises start to resurface. The existence of this asymmetric behavior can also be indirectly supported by the ablation curves in many fellow studies such as [2; 32; 33]. Furthermore, as shown in Appendix A, such an asymmetric pattern is consistent across activations in different diffusion models.

Similar to common high-frequency noises, diffusion noises can degenerate feature quality. Hence, this property can serve as a criterion for identifying and filtering out sub-optimal activations, which we will delve into in Section 5.

### In-Resolution Granularity Changes

The second property is at the in-resolution level and closely related to recent advances in diffusion architectures. Specifically, the design of U-Net follows the idea of resolution hierarchy . Consequently, the overall architecture displays a fine-coarse-fine granularity trend, looking like the alphabet "U". Traditional U-Nets implement this architecture with a relatively large number of resolutions, while each resolution is typically small, equipped with two or three simple convolutional layers.

Figure 3: We highlight three properties of diffusion U-Nets that **are distinct from existing knowledge about other models**: (a) Asymmetric diffusion noises. (b) In-resolution granularity changes. (c) Locality without positional embeddings: pixels within the orange circle resemble nearby background pixels more than distant pixels on the horse’s neck that are semantically closer.

Hence, the understanding of traditional U-Net focuses on the granularity changes across resolutions, implicitly assuming that the change within a single resolution is negligible.

However, diffusion U-Nets become much "fatter". In other words, modern diffusion U-Nets typically have much fewer resolutions, but each resolution is significantly enlarged. For example, SDv1.5 only has four resolutions , and SDXL further decreases this number to three , as shown in Figure 2. Meanwhile, each resolution can produce much more activations, primarily thanks to the embedded ViT modules. This architectural evolution makes the granularity change within a single resolution more significant, as depicted in Figure 3(b).

Different granularity carries varied information and quality, resulting in different discriminative performance on downstream tasks . Hence, this discovery of _in-resolution granularity changes_ highlights the necessity to evaluate more activations, especially those within the embedded ViTs, as they are of different granularity.

### Locality without Positional Embeddings

For the third distinct property, we delve into the sub-module level, _i.e._, the blocks in embedded ViT modules. Positional embeddings, which are widely used in language transformers  and ViTs , aim to provide spatial information for each input token. Consequently, the activations of traditional ViTs display strong positional information, where the latent pixel resembles nearby pixels more than those that are semantically similar but far away . This phenomenon is significant for the layers close to the inputs. When the layer goes deeper, the tokens are refined with semantic information, making the activations display less positional information. However, only in the last few layers, such positional information becomes negligible.

In contrast, ViT modules in diffusion U-Nets do not use positional embeddings , perhaps because the other U-Net components have provided sufficient spatial cues. This change results in distinct properties of the activations. On one hand, positional information is negligible for most activations despite how near they are to the inputs. For example, even in the first basic block, cross-attention query activations contain no perceivable positional information. On the other hand, the queries and keys of self-attention still display non-negligible positional information, marked with orange circles in Figure 3(c). Specifically, the latent pixel on the horse's neck is a light blue color, similar to the pixels to its left that actually represent the background. In contrast, the pixels above the circle are in purple color, though they also represent the horse's neck. Such comparison shows that a latent pixel is more similar to other pixels that are spatially near it than those semantically closer to it.

We name this phenomenon _locality_ since it has a different mechanism from that induced by positional embeddings. As pointed out in , self-attention allows ViT to integrate global and local information even in the shallow layers, and the attention scope enlarg _w.r.t._ layer depth. Even without positional embeddings, self-attention activations are generally consistent with this insight, leading to the existence of locality. Nevertheless, the magnitude has indeed greatly reduced, compared to the visualization of conventional ViT activations in . As shown in Figure 3(c), in shallow activations, locality exists but is inherently weaker, as much semantic information is still reserved. In addition, locality quickly diminishes as the layer goes deeper.

Since positional information can degrade the quality of activations , its absence has the potential to enhance the activations in ViT modules. Moreover, locality can play a special role in activation filtering, as presented in Section 5.

### Universality of Three Properties

Although the visualization in Figure 3 is conducted only on SDXL, the scope of these properties is not limited to the specific architecture. Further supporting evidence is available in Appendix A.

1. Diffusion noises directly arise from the diffusion process. Hence, it is promising to extend this property to other diffusion backbones.
2. In-resolution granularity changes come from the "fatness" of U-Nets, making it potentially applicable to more traditional U-Nets.
3. Locality originates from the self-attention mechanism in ViT architectures, so it is broadly applicable to standalone or embedded ViTs where positional embeddings are absent.

## 5 Enhanced Feature Selection from Diffusion U-Nets

So far, we have had a more comprehensive understating of the properties of diffusion U-Nets. All these properties, especially in-resolution granularity changes, encourage us to reconsider the feature selection solution, with a special emphasis on the activations in ViT modules. With these properties, we are also able to filter out many low-quality activations qualitatively, followed by a thus simplified quantitative comparison.

### Qualitative Filtering

**Avoiding Diffusion Noises.** As shown in Figure 4(a), diffusion noises tend to degrade the quality of activations. Hence, it is natural to filter out the activations severely affected by diffusion noises from the candidate pool. Specifically, according to the asymmetric trend of diffusion noises, **we only consider activations in the early half of the up-stage**, which are rather clean. This approach will significantly reduce the number of candidate activations and simplify the quantitative comparison.

**Avoiding Self-Attention Locality.** The locality in self-attention modules is another important factor that can degrade the activations. The empirical evidence in Figure 4(b) demonstrates that these activations are generally inferior to the others, such as those from cross-attention layers or the outputs of ViT basic blocks. Consequently, it is rather safe to **filter out most activations in self-attention modules** from our candidate pool.

**Using Locality to Suppress Diffusion Noises.** So far, the activations in the candidate pool are clean and free from locality. However, all these activations are low-resolution ones since high-resolution activations are generally noisy and thus filtered out. This is unfavorable since some detailed information might only exist in high-resolution activations. To address this issue, we exploit a side effect of self-attention locality. Specifically, as indicated in Figure 4(c), locality can help suppress diffusion noises via its focus on spatial structures. Although locality is sub-optimal, it is still superior to severe diffusion noises. In view of this, the candidate pool **reserves self-attention activations extracted from the later half of the up-stage**.

We have filtered out many candidate activations based on the distinct properties of diffusion U-Nets. Additionally, all increment activations in residual connection can be further filtered out since they introduce high-frequency noises [5; 34]. After such qualitative filtering, a small but high-quality candidate pool is available. Taking SDXL as an example, the number of candidates decreases from 279 to 63, _i.e._, a 78% reduction. Next, we explain how to conduct this quantitative comparison briefly.

Figure 4: (a) Diffusion noises result in a significant performance degeneration (Resolution#1). (b) Locality degrades the quality of self-attention activations (Block#0 and Block#5). (c) Locality in self-attention activations can suppress diffusion noises, leading to better quality than noisy activations (41.41 _v.s._ 34.58). All PCK@0.1\({}_{}()\) results are evaluated on the semantic correspondence task.

### Quantitative Comparison

The quantitative comparison follows the protocol of . Specifically, given an input image \(_{0}\), we first perform the forward pass with a pre-defined timestep \(t\) to generate the noisy image \(_{t}\). Then, the U-Net backbone conducts one denoising step. Instead of collecting the model output \(\), we gather U-Net activations and consolidate them to the candidate pool, as described in Section 5.1. Afterward, each activation is individually fed to a downstream model to evaluate its discriminative potential, with the details of the model described in Appendix C. Such comparison is made fair by using the same dataset for all activations. Finally, we can obtain the capability ranking of activations from each resolution, according to which features can be selected wisely. Notably, we conduct such comparisons on multiple datasets to guarantee generalizability, since the best activations may differ among different scenes . Thanks to qualitative filtering, **the time cost for each (model, dataset) pair has been reduced by more than one week**, equipped with Nvidia(R) RTX 3090 GPUs.

Given the capability ranking, feature selection is in fact flexible, as it is possible to combine multiple activations and specifically tune the choice for a task. For practicality, we provide off-the-shelf feature combinations for SDv1.5 and SDXL that are likely to generically perform well in Appendix B, according to the results detailed in Appendix D. **Each of them consists of four activations mainly from ViT modules rather than inter-module positions**. Taking SDv1.5 as an example, one of the four selected features is from one self-attention layer in the highest resolution, which utilizes our observation in Section 4.

## 6 Experimental Validation on Multiple Discriminative Tasks

To validate the effectiveness of our feature selection solution, the experiments are conducted on three popular discriminative tasks: semantic correspondence, semantic segmentation, and label-scarce segmentation. The SOTA methods for each task are selected as competitors. Besides, we also compare with two baselines that select the conventional inter-module activations as features , named _Legacy-v1.5_ and _Legacy-XL_. For our method, we provide the following implementations:

* _Ours-v1.5 & Outs-XL_: Features extracted from SDv1.5 and SDXL, respectively.
* _Ours-XL-\(t\)_: For fairness, we further enhance SDXL features with some additional techniques that are also adopted by SOTAs. The techniques we select, _i.e._, attention maps  and feature amalgamation , are relatively simple and lightweight.

More experimental details are in Appendix C, such as task information, evaluation metrics, and implementation details. Besides, Appendix D presents additional results not covered here.

### Empirical Results on Semantic Correspondence

We present the experimental results for semantic correspondence in Table 1. From the results, we have the following observations:

   Category & Method & PCK@0.1\({}_{}\) & PCK@0.1\({}_{}\) \\   & DINO & 51.68 & 41.04 \\  & DHPF & 55.28 & 42.63 \\  & DIFT & - & 52.90 \\  & DHF & 72.56 & 64.61 \\   & Legacy-v1.5 & 75.14 & 66.73 \\  & Legacy-XL & 66.00 & 59.16 \\   & Ours-v1.5 & 77.78 & 69.83 \\  & Ours-XL & 81.72 & 75.18 \\   & Ours-XL-t & 83.90 & 76.86 \\   

Table 1: Experimental results on the semantic correspondence task. The best results are in bold font and the runner-up is underlined.

1. _Ours-v1.5_ outperforms _Legacy-v1.5_ (77.78 _v.s._ 75.14 on PCK@0.1img). The main reason is that _Legacy-v1.5_ fails to effectively handle the diffusion noises in high-resolution activations. Previous approaches either reserve the noisy activations and thus suffer from performance degradation , or simply discard high-resolution activations and thus suffer from information loss . In contrast, our approach uses self-attention locality to suppress diffusion noises and harvest better high-resolution activations.
2. _Legacy-XL_ is inferior to _Legacy-v1.5_ (66.00 _v.s._ 75.14 on PCK@0.1img). At first glance, this result is counter-intuitive since SDXL is more advanced than SDv1.5. However, the analysis in Section 4 can unravel the mystery. Specifically, since SDXL has more ViT modules, the more valuable activations shift from inter-module positions to these embedded ViTs. Since the baseline does not consider ViT modules, _Legacy-XL_ fails to achieve better performance. In contrast, _Ours-XL_ shows improvement over _Ours-v1.5_ (81.72 _v.s._ 77.78 on PCK@0.1img). This is consistent with the advance in model architecture and again validates our analysis.
3. _Ours-XL-t_ significantly outperforms the SOTA method with a similar amalgamation technique, _i.e._, DHF  (83.90 _v.s._ 72.56 on PCK@0.1img). This performance gain again validates the effectiveness of our method.

### Empirical Results on Semantic Segmentation

As shown in the left part of Table 2, _Ours-XL-t_ and _Ours-XL_ achieve state-of-the-art performance on the semantic segmentation task (45.71 and 43.45 _v.s._ 40.89 on ADE20K), demonstrating its effectiveness and generalizability. Furthermore, the most competitive SOTA, Meta Prompts , introduces a large number of trainable parameters and uses the diffusion U-Net recurrently, which is rather time-consuming. In contrast, our method delivers superior results with efficiency maintained.

Unlike the results in semantic correspondence, _Legacy-XL_ outperforms _Legacy-v1.5_ on CityScapes (71.67 _v.s._ 64.01), and the performance gap between _Legacy-v1.5_ and _Ours-v1.5_ is narrow (64.01 _v.s._ 64.10). This is because this task utilizes a relatively large-scale downstream model, which can significantly refine the input features and thus reduce the gap in feature quality. Nevertheless, _Ours-XL_ still achieves a significant improvement over _Legacy-XL_ (74.47 _v.s._ 71.67).

### Empirical Results on Label-Scarce Segmentation

One advantage of diffusion features is the applicability to label-scarce scenarios . For validation under such conditions, we experiment on the label-scarce segmentation task, with results presented in the right part of Table 2. The observations are generally similar to those on the semantic correspondence task. For example, _Ours-v1.5_ outperforms _Legacy-v1.5_ (60.2 _v.s._ 59.4), _Legacy-XL_ is inferior to _Legacy-v1.5_ (53.0 _v.s._ 59.4), and _Ours-XL_ is better than _Ours-v1.5_ (62.7 _v.s._ 60.2).

Next, we focus on the comparison with the SOTA method, DDPM . Although DDPM is a relatively early study, it outperforms the other competitors and our most implementations. This result has two reasons. On one hand, DDPM performs the diffusion process directly in the image space

    &  &  &  &  \\  & & ADE20K & CityScapes & & Horse-21 \\   & MaskCLIP & 23.70 & - & SwAVw2 & 54.0 \(\) 0.9 \\  & ODISE & 29.90 & - & MAE & 63.4 \(\) 1.4 \\  & VPD & 37.63 & 55.06 & DatasetDDPM & 60.8 \(\) 1.0 \\  & Meta Prompts & 40.89 & 71.94 & DDPM & 65.0 \(\) 0.8 \\   & Legacy-v1.5 & 40.26 & 64.01 & Legacy-v1.5 & 59.4 \(\) 1.3 \\  & Legacy-XL & 27.78 & 71.67 & Legacy-XL & 53.0 \(\) 0.9 \\   & Ours-v1.5 & 41.07 & 64.10 & Ours-v1.5 & 60.2 \(\) 0.9 \\  & Ours-XL & 43.45 & 74.47 & Ours-XL & 62.7 \(\) 0.7 \\   & Ours-XL-t & 45.71 & 75.89 & Ours-XL-t & 66.3 \(\) 0.9 \\   

Table 2: Experimental results on semantic segmentation and its altered version with scarce labeled data, evaluated using mIoU\(\) metric. The best results are in bold font and the runner-up is underlined.

rather than the currently common practice of compressed latent space. Although inefficient, such an implementation yields better discriminative features. On the other hand, DDPM utilizes diffusion models specifically trained on each dataset. In comparison, we use pre-trained general-purposed SDv1.5 or SDXL to be more efficient and thus consistent with the motivation of diffusion feature. Hence, it is rather challenging to surpass this SOTA method. Fortunately, our best implementation, _Ours-XL-t_, achieves this goal with the help of additional lightweight techniques (66.3 _v.s._ 65.0).

## 7 Conclusion and Future Work

In this study, we revisit the fundamental problem of feature selection from diffusion U-Nets. We point out that prior arts only consider a limited range of potential activations. In contrast, we consider a much wider range of activations as candidates, especially those extracted from the embedded ViT modules. Given the large volume of the candidate pool, we first analyze the properties of diffusion U-Nets. The properties we find are universal such that our observations are not limited to the specific diffusion architecture. Based on these properties, we qualitatively filter out many activations with low quality, facilitating the following quantitative comparison. On top of this, concrete feature selection solutions are proposed for two popular diffusion models, _i.e._, SDv1.5 and SDXL. Finally, extensive experiments on three discriminative tasks validate the effectiveness of our method.

However, we are not sure whether our observations can generalize well to recently-developed DiT models  since they have a markedly different architecture from U-Net-based diffusion models. Thus, analyzing DiT models is a promising topic for future research.