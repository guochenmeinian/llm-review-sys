# Optimal Transport-based Labor-free Text Prompt Modeling for Sketch Re-identification

Rui Li, Tingting Ren, Jie Wen, Jinxing Li

School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen

lrhit@stu.hit.edu.cn, {rentt0410,lijinxing158}@gmail.com, jiewen_pr@126.com

Equal Contribution.Corresponding Author.

###### Abstract

Sketch Re-identification (Sketch Re-ID), which aims to retrieve target person from an image gallery based on a sketch query, is crucial for criminal investigation, law enforcement, and missing person searches. Existing methods aim to alleviate the modality gap by employing semantic metrics constraints or auxiliary modal guidance. However, they incur expensive labor costs and inevitably omit fine-grained modality-consistent information due to the abstraction of sketches. To address this issue, this paper proposes a novel _Optimal Transport-based Labor-free Text Prompt Modeling_ (OLTM) network, which hierarchically extracts coarse- and fine-grained similarity representations guided by textual semantic information without any additional annotations. Specifically, multiple target attributes are flexibly obtained by a pre-trained visual question answering (VQA) model. Subsequently, a text prompt reasoning module employs learnable prompt strategy and optimal transport algorithm to extract discriminative global and local text representations, which serve as a bridge for hierarchical and multi-granularity modal alignment between sketch and image modalities. Additionally, instead of measuring the similarity of two samples by only computing their distance, a novel triplet assignment loss is further proposed, in which the whole data distribution also contributes to optimizing the inter/intra-class distances. Extensive experiments conducted on two public benchmarks consistently demonstrate the robustness and superiority of our OLTM over state-of-the-art methods.

## 1 Introduction

With the growing need for urban public safety, traditional person re-identification (Re-ID) methods  are gradually becoming inadequate for criminal investigations and missing person tracking, as the individuals of interest may not have been captured by surveillance cameras. To bolster social security management and combat criminal activities, sketch person re-identification (Sketch Re-ID), which utilizes eyewitness clues to draw professional sketches as queries and match target images in a photo gallery database, has received widespread attention from researchers and scholars , as shown in Fig. 1. Nonetheless, due to the considerable disparity in modal heterogeneity resulting from the varied sketch styles of different artists and the

Figure 1: The illustration of sketch Re-ID. Different artists create sketches based on clues provided by witness to assist the police in identifying targets.

diverse postures of real pedestrians in monitoring, sketch Re-ID remains highly challenging and necessitates further exploration and investigation.

Giving the high generalization and inherent abstraction of person characteristics in sketches, the pedestrian features depicted in a single sketch (such as clothing and gender) may match multiple similar real images, as illustrated in Fig. 1. A viable solution is to leverage the inter-modality interaction within the feature space to achieve hard alignment. Such methods typically employ loss constraints to directly map different modalities into a generic latent space [4; 7; 8; 9]. However, this hard alignment manner may not fully capture the complex dependencies and correlations that exist within and across modalities. To compensate for the lack of details in the above manner, another branch is to introduce an intermediate modality to bridge two source modalities. For instance, [10; 11] generate simulated sketches through adversarial learning, but the generated sketches are inevitably corrupted with noise due to limited generation performance. Additionally, [7; 12] construct benchmarks that contain textual information to alleviate modal gap;  improves inference efficiency by introducing text only during the training process. Despite the fact that these additional texts do contribute to mitigating the modal gap, they are all manually labeled, requiring significant human labor in real-world applications. Moreover, existing text-guided methods [6; 7] only focus on global text embeddings as masks, neglecting finer and richer local features. Therefore, this paper aims to address two key challenges: **i) developing sufficient textual information as a transition mechanism without incurring additional costs**, and **ii) further exploring fine-grained discriminative information for multi-granularity interaction.**

To address the above issues, we propose the Optimal Transport-based Labor-free Text Prompt Modeling (OLTM) framework, which implicitly incorporates text semantic information during training, facilitating hierarchical and multi-granularity modal alignment. In particular, OLTM is composed of three main components: i) text prompt reasoning (TPR); ii) text-injected coarse-grained alignment module (TCA); iii) consensus-guided fine-grained interaction module (CFI). On the one hand, to introduce text sequences without additional manual annotation, we dynamically transfer pre-trained language-visual knowledge into the downstream task. Specifically, TPR first generates multi-dimensional person attributes based on real images with a pre-trained visual question answering (VQA) model. Then, these attributes are inserted as fixed parts into learnable prompts to obtain the textual embedding representations. TCA integrates global parts of the embeddings to achieve the coarse-grained alignment across modalities. On the other hand, to explore fine-grained information, we employ optimal transport theory to enhance deep-level interaction. Concretely, TPR formulates the mapping from local parts of the textual embeddings to more discriminative feature representations, i.e., consensus, as an optimal transport problem. Subsequently, guided by consensus, CFI selectively focuses on key details, extracting fine-grained conceptual representations for sketch-ID. In addition, due to the significant heterogeneity gap between modalities, using Euclidean distance as a sole metric to measure feature similarity is inadequate. Thus, we propose a triplet assignment loss to optimize feature distance measurement and improve model performance. Extensive experiments are conducted on two challenging benchmarks, which demonstrate the favorable comparison of OLTM with other state-of-the-art methods.

The core contributions of this work are summarized as follows: (1) This paper proposes a novel optimal transport-based labor-free text prompt modeling framework for sketch Re-ID. To our best knowledge, this is _the first attempt_ to apply VQA-generated text responses as a means to achieve modal alignment in sketch Re-ID _without any additional annotations_. (2) A novel text prompt reasoning module is deployed to dynamically extract global textual embeddings and discriminative fine-grained consensus, which guide the hierarchical multi-granularity alignment module in injecting semantic knowledge into the modeling process. (3) A new triplet assignment loss is proposed, which optimizes inter-/intra-class distance by considering overall data distribution information.

## 2 Related Work

**Sketch Re-identification** As an important part of public safety guarantee, sketch Re-ID is a novel and challenging task that aims to match a person image with given professional sketches. Existing sketch Re-ID methods could be roughly classified into two groups according to their interaction modes, i.e., hard alignment methods [4; 8; 9] and soft alignment methods [12; 10; 6]. The former try to learn modality embeddings in a common latent space by employing some modality interaction operations or semantic metrics. Pang _et al_.  pioneered a sketch-photo benchmark and introducedcross-domain adversarial learning to narrow the feature gap. Zhang _et al_.  proposed an advanced cross-modal learning mechanism for handling non-corresponding information between modalities. However, due to significant differences between modalities, this direct alignment paradigm inevitably loses fine-grained modality-specific cues . Hence, some of the latter methods investigate gentler alignment techniques through transitional modality. For example, Chen _et al_.  designed a dynamic updatable auxiliary sketch modality to increases the diversity of training samples; Zhai _et al_.  introduced a multi-modal Re-ID task by combining text and sketch as query for retrieval, exploiting their complementary advantages. Obviously, auxiliary modalities lacking detailed information may introduce noise, while data annotation is a labor-intensive task. In this paper, we _first attempt_ to use text attributes generated by a reasonable VQA model as guidance for achieving multi-granularity alignment across modalities in sketch Re-ID.

**Optimal Transport** For optimizing the moving cost between distributions, Optimal Transport (OT) was first proposed by Kantorovich , which has shown significant potential in machine learning and computer vision, e.g., domain adaptation [15; 16; 17], learning with noisy labels [18; 19], and feature matching [20; 21]. Zhang _et al_.  incorporated OT into the re-ranking phase of image retrieval, significantly improving accuracy and efficiency. Similarly, Sergio _et al_.  first applied OT in visual place recognition and introduced a novel local feature aggregation method. In semi-supervised person Re-ID, OT often achieves the mapping between pseudo labels and classes as a classifier [24; 25]. In addition, Ling _et al_.  designed a assignment strategy for alleviating the intra-identity variations; Wasserstein distance was used to rectify the original global distance between samples and provides aligned distance estimation for local features . Considering the enormous potential of OT in feature aggregation and distribution mapping, our study adopts OT to assist fine-grained alignment between modalities and guide the model in extracting the overall sample distribution pattern.

**Prompt Learning** Prompt learning initially garnered widespread attention and extensive research in natural language processing [28; 29; 30], which has gradually demonstrated significant potential in vision-language (V-L) models [31; 32; 33] and pure vision models [34; 35; 36]. Prompt learning provides a flexible way to adapt pre-trained models to downstream tasks by training only additional parameters. This enables prompts to capture task-specific information while guiding the fixed model's performance [37; 38]. In sketch-based image retrieval,  innovatively learns a unified prompt for different branches in CLIP's  visual encoding layer, fully exploiting CLIP's zero-shot learning potential. In text-to-image person Re-ID,  introduces a multi-prompt strategy to integrate text prompts from various sources for fine-grained interaction. Furthermore, Li et al.  first attempt to conduct in-depth research on zero-shot multi-modal ReID through a large foundational model. In this paper, we delve into the significant application of prompt learning in sketch Re-ID, innovatively generating global text representations by integrating fixed and learnable prompts, and utilizing OT to reason consensus for effectively guiding detailed interactions across modalities.

## 3 Preliminaries

### Problem Statement

To ensure clarity, we represent the gallery containing \(m\) images \(I\) as \(=\{I_{i},y_{i}\}_{i=1}^{m}\) and the query set containing \(n\) sketches \(S\) as \(=\{S_{j},y_{j}\}_{j=1}^{n}\), where \(y\{1,,C\}\) are the identity labels for \(C\) distinct pedestrian entities. Notably, each entity may include multiple images and sketches. The goal of Sketch Re-ID is to retrieve pedestrian images from the gallery \(\) that match one or multiple given sketch. Like , there exist two types of query methods: single sketch query and multiple sketches query. This section will use single sketch query as an example, and the same applies to multiple sketches query. Formally, we define a matching function \(:^{n m}\) that assigns a similarity score to each pair \((I_{i},S_{j})\). The objective is to learn a function \(\) such that for any sketch \(S_{j}\) and image \(I_{i}\),

\[(I_{i},S_{j})>(I_{k},S_{j}) if y_{i}=y_{j}\ and\ y_{k} y_{j},\] (1)

where \(I_{i}\) and \(I_{k}\) are images from the gallery set, and \(y_{i}\) and \(y_{k}\) are their respective identity labels.

### Optimal Transport

Optimal Transport is a mathematical theory that focuses on finding an efficient solution between two probability distributions, minimizing the cost of transporting one distribution into another. We briefly review the theoretical derivation of optimal transport. Let \(_{r}:=\{_{+}^{r}|^{}_{r}=1\}\) representsthe probability simplex, where \(_{r}\) is the \(r\)-dimensional vector of ones. Given two probability simplex vectors \(_{m}\) and \(_{n}\) and a cost matrix \(^{m n}\), the objective of OT is to seek the optimal transport plan \(^{*}\) mapping \(\) to \(\) at the minimum cost:

\[ d_{}(, )=_{(,)},,\\ (,)=\{ _{+}^{m n}_{n}= ,^{}_{m}= \},\] (2)

where \((,)\) denotes the transport polytope of \(\) and \(\), i.e., the solution space of \(\). The above problem is to find optimal solution \(^{*}\) in a set of all possible joint probabilities of \((X,Y)\), where \(X\) and \(Y\) represent random variables with marginal distribution \(\) and \(\).

Eq. 8 indicates that OT is a linear programming problem which is theoretically solvable in polynomial time, but its complexity becomes prohibitively high as the feature dimension increases . To this end, Sinkhorn algorithm  adopts an iterative strategy to obtain the optimal solution \(^{*}=Diag()Diag()\) with near-square complexity . \(\) and \(\) can be solved through alternately iterating the following two equations: \(^{(z)}=/(^{(z-1)})\) and \(^{(z)}=/(^{}^ {(z)})\), where \(=exp(/)\), \(\) is the regularization coefficient and \(z\) is the iterations (cf. **Appendix**). Since this method integrates the importance of all features when solving the optimal solution, it can analyze the overall data distribution.

## 4 The Proposed Method

### Overall Architecture

Fig. 2 provides an overview of the OLTM architecture. The image and text encoders discussed in this paper are based on CLIP , and any language-visual model utilizing a Transformer architecture may also be employed. Notably, the image encoders utilized for both images and sketches employ shared weights to ensure the mapping of features into a unified semantic space. For an input RGB image, we obtain embeddings \(=\{_{},_{1},, _{p}\}^{(p+1) d}\) through the image encoder, where \(p\) is the number of non-overlapping patches, \(_{}\) and \(_{}=\{_{i}\}_{i=1}^{p}\) represent \(d\)-dimensional global and local features, respectively. Similarly, the embeddings of a sketch can be represented as \(=\{_{cls},_{local}\}=\{_{},_{1},,_{p}\}^{(p+ 1) d}\). Firstly, to provide reasonable text semantic

Figure 2: Overview of our proposed OLTM network. Our model includes four main parts, i.e., text prompt reasoning (TPR), text-injected coarse-grained alignment Module (TCA), consensus-guided fine-grained interaction module (CFI) and triplet assignment loss (TAL). Specifically, TPR flexibly generates target characteristics through VQA, and combines prompt learning and optimal transport to reason text global embedding and local consensus. TCA and CFI extract modality-specific representations from image and sketch modalities to achieve hierarchical and multi-granularity alignment. Finally, TAL is designed to optimize distance measurement between samples and improve the model’s capacity to capture local relationships.

guidance, Text Prompt Reasoning (TPR) generates attribute descriptions about pedestrians based on the RGB image, and obtains the textual embeddings \(=\{_{cos},_{local}\}\) through prompt learning. Then, TPR extracts the fine-grained consensus \(\) from \(_{local}\) through clustering. Subsequently, Text-injected Coarse-grained Alignment (TCA) module embeds global contextual information \(_{cos}\) into visual features \(_{}\) and \(_{}\). Meanwhile, Consensus-guided Fine-grained Interaction (CFI) module utilizes \(\) to address fine-grained semantic misalignment between \(_{local}\) and \(_{local}\). Additionally, due to the significant differences between sketches and RGB images, Euclidean distance between independent samples may ignore the influence of overall sample distribution. Thus, we introduce a more comprehensive distance measurement method and propose triplet assignment loss \(_{tal}\). During training, all these modules will be jointly optimized through identity loss \(_{id}\) and \(_{tal}\): \(_{OLTM}=_{id}+_{tal}\), where \(\) is a scaling factor. During inference, only \(_{}\) and \(_{}\) are used to match queries for practical application requirements.

### Text Prompt Reasoning

Significant image differences and inherent abstract nature, cause semantic misalignment during knowledge acquisition, severely impacting model's reasoning and generalization capabilities. To address this issue, TPR introduces intermediate modality to guide alignment between modalities without additional costs. Moreover, TPR employs a dynamic consensus acquisition strategy to enhance the discriminative power of local text features.

**Text Attribute Generation**  Sketches, unlike conventional Re-ID tasks, are vulnerable to subjective emotions and drawing skills of artists, leading to a lack of detailed information crucial for model learning. The text's objectivity and flexibility prompt the model to focus more on semantic contextual information during knowledge acquisition. However, directly generating a comprehensive textual description of pedestrian images inevitably introduces irrelevant noise, thereby reducing model performance. Therefore, we retain the advanced modeling capabilities of large-scale language-visual models for images as possible. Specifically, for a given RGB image, TPR utilizes a pre-trained visual question answering model to address \(k\) specific details (cf. **Appendix**) and acquire corresponding descriptions for the target: \(att=\{att_{1},att_{2},,att_{k}\}\). Importantly, this process introduces textual detail guidance during model training but excludes text-related components during inference.

**Learnable Prompt Strategy**  Inspired by , we combine the learnable prompt with original text attributes, without incurring additional expert knowledge compared to the handcrafted prompt. Concretely, TPR initially transform these attributes into tokens through CLIP tokenizer, i.e., \(=Tokenizer(att)\). Then, \(\) learnable prompts \(\{p_{1},p_{2},,p_{l}\}\) are embedded into these fixed attributes tokens, forming the textual description: \(=\{p_{1},a_{1},p_{2},a_{2},,p_{l},a_{k}\}\). This integration introduces a dynamic knowledge learning mechanism that reduces noise introduction compared to handcrafted prompts, while enhancing the flexibility of modal interaction and transferability of text embeddings. Subsequently, the whole token \(\) is fed into a frozen text encoder to generate text embeddings \(=\{_{},_{1},_{2},,_{n},_{ }\}^{(n+2) d}\), where \(_{}\) and \(_{}\) denote the [SOS] and [EOS] token, \(n\) is the number of \(d\)-dimensional word tokens. Based on widely-used token selection, \(_{}\) serves as the global feature, while \(_{}=\{_{j}\}_{j=1}^{n}\) represents a sequence of basic local tokens.

**Dynamic Consensus Acquisition**  To more effectively address fine-grained semantic variations (e.g., hats, shoes) across modalities, we explore methods to filter out non-informative features for enhancing the representational capacity of text embedding for detailed information. Therefore, based on local textual feature \(_{local}\), we employ metric learning to draft a dynamic consensus acquisition strategy for capturing the discriminative prototypical representations \(\).

To begin, in order to adaptively learn related-task knowledge, local text representations \(_{local}^{n d}\) are fed into consensus multi-layer perceptron (ConMLP) blocks to achieve feature enhancement. Then, a prototypical descriptor is formed by assigning all enhanced features to a set of atoms. The cost matrix \(_{+}^{n m}\) can be calculated for assignment, where the \((i,j)\)-th element \(_{i,j}\) represents the cost of assigning a feature to an atom. In other words, \(\) evaluates the affinity between the enhanced features and the prototypical atoms. Concretely, to introduce certain priors as guidance , the enhanced features are used to learn the cost matrix through two fully connected layers initialized randomly. In addition, some irrelevant information in textual features, such as those representing association, may disrupt the model's ability to learn the target details. Inspired by the solutions used in graph matching and key-point matching [20; 23], we set a learnable "bin" in \(\). Due to the differences between prior distributions, non-informative features can be assigned to it. Specifically,we extend the cost matrix from \(\) to \(}=[,}]_{+}^{n(m+1)}\), and \(}=w_{n}\), where \(w\) is a learnable parameter and \(_{n}=[1,,1]^{}^{n}\) represents \(n\)-dimensional vector of ones. Following Sec. 3.2, we consider that assignment where the enhanced features' mass, \(=_{n}\), should be assigned to the atoms or the "bin", \(=[_{m}^{},n-m]^{}\), is an optimal transport problem:

\[ d_{}}(,)=_{(,)}},,\\ (,)=\{_{+}^{n (m+1)}|_{m+1}=,^{}_{n}=\}. \] (3)

The optimal assignment plan can be solved by Sinkhorn algorithm (cf. **Appendix**) through iterative strategy. For better descriptors quality, the extra "bin" is discarded to obtain the optimal assignment \(_{+}^{n m}\). Finally, the augmented comprehensive representation, i.e., consensus \(^{m d}\), is obtained by aggregating enhanced textual features and optimal transport plan: \(=^{}_{local}\).

### Hierarchical and Multi-granularity Modal Alignment

TPR develops a robust transitional means to facilitate reasonable and efficient modal alignment. In order to leverage effectively the multi-granularity textual information, TCA utilizes global embedding to achieve coarse-grained modal interaction, while CFI optimizes fine-grained alignment based on enhanced comprehensive representation.

#### 4.3.1 Text-injected Coarse-grained Alignment Module

TCA utilizes transformer architecture, known for its efficiency in modeling long-distance dependencies, to capture global information, as shown in Fig. 2. Furthermore, we implement a cross-attention mechanism using global text embedding to emphasize contextual information injection at the beginning. For an input pair \((,)\), cross-attention mechanism computes the \(query\) using text global representation \(_{eos}\) and derives the \(key\) and \(value\) from modality-specific global features \(_{cls}\) and \(_{cls}\) :

\[_{R/S}=_{eos}^{Q},_{R/ S}=(/)_{cls}^{K},_{R/S}=(/)_{cls} ^{V},\\ CA(/,_{eos})=Attention(_{R/S},_{R/S}, _{R/S}),\] (4)

where \(/\) signifies identical operations across both modalities; \(^{Q}\), \(^{K}\) and \(^{V}\) denote shared learnable parameters, while \(_{R/S}\), \(_{R/S}\) and \(_{R/S}\) represent \(query\), \(key\) and \(value\) for either the RGB or sketch modality, respectively. After obtaining the fusing features that contain textual knowledge, TCA introduces the standard transformer blocks to refine modality-specific features. Finally, we can acquire the final global concept representations \(_{cls}^{{}^{}}\) and \(_{cls}^{{}^{}}\) for sketch Re-ID.

#### 4.3.2 Consensus-guided Fine-grained Interaction Module

Due to the inherent complexity of sketches and RGB images and the potential for semantic misalignment during learning, capturing detail variations in modalities is crucial. Fortunately, the fine-grained consensus \(\) provided by TPR, which contains rich detail information, offers a key solution to this problem. CFI adopts a transformer structure based on multi-head cross attention, and it converts the original local feature \(_{local}\) and \(_{local}\) to more discriminative representations through for robust Re-ID:

\[}_{R/S}=^{},}_{R/S}=(/)_{local}^{},}_{R/S} =(/)_{local}^{},\\ Head_{h}^{R/S}=Attention(}_{R/S},}_{R/S}, }_{R/S}),\\ MH(/,)=Concat(Head_{1}^{R/S},,Head_{H}^{R/S}),\] (5)

where \(^{}\), \(^{}\) and \(^{}\) denote the parameters of project layers of \(h\)-th head for both modalities. As a result, the text detail information about pedestrian characteristics can assist our model to address the problem caused by the diversity and uncertainty of sketches and RGB images. Eventually, the local concept representations \(_{local}^{{}^{}}\) and \(_{local}^{{}^{}}\) can be obtained.

### Triplet Assignment Loss

The triple loss, a commonly-used matching loss in cross-modal learning, performs well in performance through adjusting the distance of hardest negatives in scenarios like image-text matching [12; 48] and video-text retrieval . However, this strategy independently trains each semantic part with equal contribution, disregarding the overall data distribution impact when multiple samples from different modalities exhibit slight differences. This oversight may lead to inaccurate estimation of sample distances and potentially result in sub-optimal local minima . To this end, we propose a new triplet assignment loss (TAL) to establish a more rational measure for evaluating the proximity of local features.

For an input positive pair \((_{i},_{i})\) in a mini-batch \(x\), the feature representations obtained through model inference are \(^{{}^{}}_{i}\) and \(^{{}^{}}_{i}\). If we treat the feature sets of all samples for each of two modalities in \(x\) as two discrete distributions, their alignment can be considered an optimal transport problem. The cost matrix \(}\) is derived from pairwise feature similarities: \(}_{i,j}=[(^{{}^{}}_{i})^{}^{{}^{}}_{j}]_ {+}\). We aim to acquire the optimal transport matrix \(^{*}\) with the least amount of cost, where \(^{*}_{i,j}\) represents the assignment weight of \((_{i},_{j})\) obtained after balancing the overall distribution. TAL can be represented based on triplet loss as the weighted sum of the original distance and the optimal assignment distance, dynamically updated at a certain rate \(\):

\[_{tal}(_{i},_{i})& =[m-D(_{i},_{i})+D(_{i},}_{h})]_{+}+[m-D(_{ i},_{i})+D(}_{h},_{i})]_{+},\\ D(_{i},_{i})&= E(_{i}, {S}_{i})+(1-)(1-^{*}_{i,i})E(_{i},_{i})\] (6)

where \([x]_{+}=(x,0)\), \(}_{h}=argmax_{R_{j} R_{i}}D(_{j},_{i})\) and \(}_{h}=argmax_{S_{j} S_{i}}D(_{i},_{j})\) are the most similar negatives in \(x\) for \((_{i},_{i})\), and \(E(_{i},_{i})=\|^{{}^{}}_{i}-^{{}^{}}_{i}\| _{2}\) denotes the Euclidean distance between feature representations.

## 5 Experiment

### Experiment Setup

**Datasets** Two publicly available benchmark datasets, namely PKU-Sketch  and Market-Sketch-1K , are utilized for performance evaluation. Both of them are sketched and annotated by professional artists. **PKU-Sketch** is the first publicly Sketch Re-ID dataset, containing data for 200 pedestrians, with each individual being represented through one sketch and two photos. In accordance with the setting of , we randomly select 150 identities for training and 50 for testing, and final results are derived from the average of 10 experimental runs. **Market-Sketch-1K** is a large-scale dataset derived from the Market-1501 , which is created by six artists based on descriptions, featuring multiple perspectives and artistic styles. The training set consists of 2,332 sketches and 12,936 photos, while the testing set comprises 2,375 sketches and 19,732 photos. Following the experimental setup in , our method will be evaluated in three settings: single-query, multi-query, and cross-style retrieval.

   & **Query** & **Backbone** & **Reference** & **mAP** & **Rank@1** & **Rank@5** & **Rank@10** \\  DDAG  & S & ResNet50 & ECCV’2020 & 12.13 & 11.22 & 25.40 & 35.02 \\ CM-NAS  & S & ResNet50 & ICCV’2021 & 0.82 & 0.70 & 2.00 & 3.90 \\ CAJ  & S & ResNet50 & ICCV’2021 & 2.38 & 1.48 & 3.97 & 7.34 \\ MMN  & S & ResNet50 & MM’2021 & 10.41 & 9.32 & 21.98 & 29.58 \\ DART  & S & ResNet50 & CVPR’2022 & 7.77 & 6.58 & 16.75 & 23.42 \\ DCLNet  & S & ResNet50 & MM’2022 & 13.45 & 12.24 & 29.20 & 39.5 \\ DSCNet  & S & ResNet50 & TIFS’2022 & 14.73 & 13.84 & 30.55 & 40.34 \\ DEEN  & S & ResNet50 & CVPR’2023 & 12.62 & 12.11 & 25.44 & 30.94 \\  BDG  & S &  &  &  & 19.61 & 18.10 & 38.95 & 50.75 \\  & M & & & 24.45 & 24.70 & 50.40 & 63.45 \\  UNIReID  & S &  &  & 34.97 & 31.52 & 57.17 & 70.46 \\  & M & & & 55.18 & 56.63 & 82.33 & 91.97 \\   & S &  &  & **38.35** & **36.75** & **63.88** & **74.05** \\  & M & & & **62.55** & **69.48** & **90.36** & **95.18** \\  

Table 1: Comparison with state-of-the-art methods on Market-Sketch-1K dataset. Both training and testing set uses all sketches. ‘S’ and ‘M’ represent single-query and multi-query, respectively. ‘Backbone’ refers to network structure used by each method, mainly including: ResNet50  and CLIP . **Bold** values represent the optimal results.

Evaluation MetricsIn line with [4; 9; 6; 59], we use Rank-\(k\) metrics (\(k=1,5,10\)) and mean Average Precision (mAP) as evaluation metrics. The higher values of the above three metrics, the better performance.

**Implementation Details** OLTM uses a pre-trained CLIP-ViT-B/16  as image encoder, and extracts text features with a pre-trained CLIP Text Transformer. Fine-grained text attributes are derived from a ViLT-based  VQA model. Importantly, our VQA model is replaceable. To avoid the cost overhead from multiple calls to VQA model during training and inference, text attribute generation is performed during the data processing stage. For multi-query scenarios, we input a weighted sum of multiple sketches. Input images are resized to \(288 144\), and augmented with random horizontal flipping and style augmentation . More experimental configuration details are available in the supplementary materials.

### Comparison with State-of-the-Art Methods

**Performance on Market-Sketch-1K.** The experimental results for Market-Sketch-1K are shown in Tab. 1. OLTM significantly outperforms all state-of-the-art methods in both single-query and multi-query settings. In the single-query scenario, OLTM achieves an mAP of 38.35% and a Rank-1 of 36.75%, surpassing state-of-the-art method by 3.38% and 5.23%, respectively. In the multi-query scenario, OLTM achieves an mAP of 62.55% and a Rank-1 of 69.48%, exceeding state-of-the-art methods by 7.37% and 12.85%, respectively. This result is primarily due to the introduction of textual semantic information in the training process of OLTM, which implicitly guides images and sketches to focus on modality-invariant features. Through hierarchical and multi-granularity alignment, the model is able to uncover discriminative fine-grained information, leading to more

 
**Methods** & **Backbone** & **Reference** & **mAP** & **Rank@1** & **Rank@5** & **Rank@10** \\  TripleSN  & - & CVPR’2016 & - & 9.0 & 26.8 & 42.2 \\ GNSiamese  & GoogleNet & TOG’2016 & - & 28.9 & 54.0 & 62.4 \\ AFLNet  & GoogleNet & MM’2018 & - & 34.0 & 56.3 & 72.5 \\ LMDI  & VGG-16 & Neuro’2020 & - & 49.0 & 70.4 & 80.2 \\ SKetchTrans  & ViT & MM’2022 & - & 84.6 & 94.8 & 98.2 \\ CCSC  & ViT & MM’2022 & 83.7 & 86.0 & 98.0 & **100.0** \\ SKetchTrans+  & ViT & PAMI’2023 & - & 85.8 & 96.0 & 99.0 \\ UNIReID\({}^{}\) & CLIP & CVPR’2023 & 88.7 & 92.4 & 98.0 & 99.6 \\ DALNet  & ResNet50 & AAAI’2024 & 86.2 & 90.0 & 98.6 & **100.0** \\  OLTM (Ours) & CLIP & - & **91.4** & **94.0** & **99.4** & **100.0** \\  

Table 2: Comparison with state-of-the-art methods on PKU-Sketch dataset. ‘Backbone’ includes GoogleNet , VGG-16 , ResNet50, ViT , and CLIP. ‘-’ denotes the unavailable results. ‘\({}^{}\)’ indicates that we reproduce UNIReID results following our training configuration.

Figure 3: The Rank-5 retrieval results on two datasets. For the Market-Sketch-1K dataset, both single-query and multi-query scenarios are presented. **Green** border indicates correctly retrieved target pedestrians, while yellow border indicates incorrectly matched pedestrians.

accurate queries. The visualization results on Market-Sketch-1K are shown in Fig. 3. When sketch possesses clearly distinguishable features, query results are satisfactory. In contrast, when sketch closely resembles multiple images, making its features challenging to distinguish with the naked eye, the model encounters errors that are justifiable. In addition, to test the generalization capability of OLTM on unknown styles, cross-style retrieval evaluation is performed on Market-Sketch-1K, and detailed results are in supplementary material.

**Performance on PKU-Sketch.** Tab. 2 presents the model's performance on PKU-SKetch dataset. The results indicate that our OLTM outperforms all competitors by a significant margin. For example, mAP and Rank-1 of OLTM are remarkably high at 91.4 and 94.0, surpassing state-of-the-art method by 5.2% and 4.0%, respectively. Because the sketches on PKU-Sketch contain more detailed information, they can assist multi-granularity interaction in acquiring more fine-grained knowledge. Fig. 3 illustrates the top-5 visualization results of OLTM on PKU-Sketch. Our method can accurately identify the target pedestrians despite challenges such as variations in posture, viewpoint, occlusion, sketch abstraction, and different painting styles.

### Ablation Study

In this section, ablation experiments are conducted on Market-Sketch-1K to evaluate the effectiveness of each component within the OLTM framework.

**The Effectiveness of Text Prompt Reasoning.** To evaluate the contribution of different prompt setting, we train the model with different combination of each setting. As shown in Tab. 3, the combination of VQA and Prompt brings significant contribution. Compared to the strategy of directly aligning different modalities, leveraging TPR to implicitly guide the alignment results in improvements of 7.08% in mAP and 9.44% in rank-1. This significant improvement is primarily attributed to introducing textual information during the training phase, which enables the model to effectively capture semantic correlations between images and sketches during inference. Furthermore, compared to manually annotated fixed attributes, those generated by VQA enhance the model's performance by adaptively adjusting the level of detail on which it focuses. Moreover, the introduction of learnable prompts improved the mAP and rank-1 by 0.79% and 4.02%, respectively, compared to fixed templates. Prompt learning can enhance the network's learning and reasoning capabilities, allowing it to more flexibly adapt to diverse modalities and enhance its sensitivity to subtle distinctions.

**The Effectiveness of Our Designed Modules.** To validate the effectiveness of the TCA and CFI module, we progressively integrate them into the baseline and evaluate performance. The results in Tab. 3 indicate that both modules significantly enhance the alignment and interaction capabilities of model across modalities. Specifically, TCA improves mAP and rank-1 by 3.36% and 4.82% compared to baseline, respectively. The introduction of textual information in TCA effectively provides semantic guidance for coarse-grained alignment between modalities, enabling the model

    &  &  &  \\  Handcrafted & VQA & Template & Prompt & Baseline & TCA & CFI & \(_{}\) & \(_{}\) & \(_{}\) & mAP & Rank@1 \\  - & - & - & - & & & & & & & 55.47 & 60.04 \\ ✓ & - & ✓ & - & ✓ & ✓ & ✓ & ✓ & - & ✓ & 61.46 & 68.07 \\ ✓ & - & - & ✓ & & & & & & & 61.81 & 67.47 \\ - & ✓ & ✓ & - & & & & & & & 61.76 & 65.46 \\  - & ✓ & - & ✓ & ✓ & - & - & & ✓ & 57.74 & 60.84 \\  & & & & ✓ & ✓ & - & - & - & 61.10 & 65.66 \\  - & ✓ & - & ✓ & ✓ & ✓ & ✓ & - & - & 54.93 & 57.83 \\  & & & & & & ✓ & ✓ & ✓ & ✓ & - & 61.63 & 66.06 \\  - & ✓ & - & ✓ & ✓ & ✓ & ✓ & ✓ & - & ✓ & **62.55** & **69.48** \\   

Table 3: Ablation studies on Market-Sketch-1K dataset. Training and testing are under the multi-query setting. “Handcrafted” and “VQA” denote manually annotated and VQA generated text attributes, respectively. “Template” represents the sentence template defined by experts. “Prompt” denotes the learnable text prompts. The ‘Baseline’ uses an image encoder to process both modalities and employs simple cross-attention to integrate the global features. ‘\(_{}\)’  represents the hard triplet loss. **Bold** values represent the optimal results.

to focus more on similar semantic relationships. Furthermore, the integration of CFI has increased mAP and rank-1 by 1.45% and 3.82%, respectively. CFI selectively focuses on key regions in visual concept representations through semantic consensus. This process optimizes feature interaction and ensures capturing more detailed information at a fine-grained level.

**The Effectiveness of Triplet Assignment Loss.** The experimental results in Tab. 3 demonstrate that the combination of proposed TAL \(_{}\) and identity loss \(_{}\) achieves optimal performance. The identity loss ensures that the model correctly identifies different individual identities, while TAL further optimizes feature space by pulling positive samples closer and pushing negative samples apart. Additionally, to verify the generalization of TAL, as shown in Tab. 4, we achieve superior performance across various network frameworks by substituting TAL for the weighted regularization triplet loss (WRT)  and hard triplet loss (HTL). Balancing between Euclidean distance and optimal transport distance can significantly enhance model performance. Please refer to the supplementary materials for more verification experiments on the key role of overall data distribution in enhancing sample feature distance.

## 6 Conclusion

In this paper, we present a optimal transport-based labor-free text prompt modeling (OLTM) framework for sketch re-identification. OLTM embeds text prompt reasoning module and distance measurement into transformer for achieving hierarchical multi-granularity alignment through the guidance of text semantics, leveraging the advantages of prompt learning and optimal transport. In addition, to address the limitations of Euclidean distance in measuring sample similarity, we propose a triplet assignment loss that guarantees a more effective standard based on the overall data distribution. Extensive experiments conducted on two public datasets indicate outstanding performance compared to other state-of-the-art methods for sketch Re-ID.

**Discussion:** In this work, we employ text injection and sample distance optimization to direct the model's attention toward key details, thereby minimizing performance losses due to modal gap and sample abstraction. However, our experiments revealed that when confronted with extremely vague sketch samples (i.e., those that human cannot discern features or match), the model's identification process deteriorates into a random selection among multiple potential outcomes. Therefore, sketch Re-ID heavily depend on the quality of sketches. Enhancing the discriminability of sketches without incurring additional labor costs is a topic worthy of further exploration in future research.