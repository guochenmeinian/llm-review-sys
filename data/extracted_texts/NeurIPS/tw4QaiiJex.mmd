# The Bayesian Stability Zoo

Shay Moran

Department of Mathematics

& Department of Computer Science

Technion - Israel Institute of Technology;

smoran@technion.ac.il

Hilla Schefler

Department of Mathematics

Technion - Israel Institute of Technology

hillas@campus.technion.ac.il

&Jonathan Shafer

Computer Science Division

UC Berkeley

shaferjo@berkeley.edu

###### Abstract

We show that many definitions of stability found in the learning theory literature are equivalent to one another. We distinguish between two families of definitions of stability: _distribution-dependent_ and _distribution-independent Bayesian stability_. Within each family, we establish equivalences between various definitions, encompassing approximate differential privacy, pure differential privacy, replicability, global stability, perfect generalization, TV stability, mutual information stability, KL-divergence stability, and Renyi-divergence stability. Along the way, we prove boosting results that enable the amplification of the stability of a learning rule. This work is a step towards a more systematic taxonomy of stability notions in learning theory, which can promote clarity and an improved understanding of an array of stability concepts that have emerged in recent years.

## 1 Introduction

Algorithmic stability is a major theme in learning theory, where seminal results have firmly established its close relationship with generalization. Recent research has further highlighted the intricate interplay between stability and additional properties of interest beyond statistical generalization. These properties encompass privacy , fairness , replicability , adaptive data analysis , and mistake bounds in online learning .

This progress has come with a proliferation of formal definitions of stability, including pure and approximate Differential Privacy , Perfect Generalization , Global Stability , KL-Stability , TV-Stability , \(f\)-Divergence Stability , Renyi Divergence Stability , and Mutual Information Stability , as well as related combinatorial quantities such as the Littlestone dimension  and the clique dimension .

It is natural to wonder to what extent these various and sundry notions of stability actually differ from one another. The type of equivalence we consider between definitions of stability is as follows.

_Definition A and Definition B are **weakly equivalent** if for every hypothesis class \(\) the following holds:_

\[\]

 _has a PAC learning rule that is_ _stable according to Definition A_ \[\] _has a PAC learning rule that is_ _stable according to Definition B_

This type of equivalence is weak because it does _not_ imply that a learning rule satisfying one definition also satisfies the other.

Recent results show that many stability notions appearing in the literature are in fact weakly equivalent. The work of  has shown sample efficient reductions between approximate differential privacy, replicability, and perfect generalization. Combined with the work of , a rich web of equivalences is being uncovered between approximate differential privacy and other definitions of algorithmic stability (see Fig. 1).

In this paper we extend the study of equivalences between notions of stability, and make it more systematic. Our starting point is the following observation: many of the definitions mentioned above belong to a broad family of definitions of stability, which we informally call _Bayesian definitions of stability_. Definitions in this family roughly take the following form: a learning rule \(A\) is considered stable if the quantity

\[dA(S),\]

is small enough, where:

* \(d\) is a measure of dissimilarity between distributions.
* \(\) is a specific _prior distribution_ over hypotheses;
* \(A(S)\) is the _posterior distribution_, i.e., the distribution of hypotheses generated by the learning rule \(A\) when applied to the input sample \(S\).

Namely, a Bayesian definition of stability is parameterized by a choice of \(d\), a choice of \(\), and a specification of how small the dissimilarity is required to be.1

**Remark 1.1**.: _To understand our choice of the name Bayesian stability, recall that the terms prior and posterior come from Bayesian statistics. In Bayesian statistics the analyst has some prior distribution over possible hypothesis before conducting the analysis, and chooses a posterior distribution over hypotheses when the analysis is complete. Bayesian stability is defined in terms of the dissimilarity between these two distributions._

A central insight of this paper is that there exists a meaningful distinction between two types of Bayesian definitions, based on whether the choice of the prior \(\) depends on the population distribution \(\):

* Distribution-_independent_ (DI) stability. These are Bayesian definitions of stability in which \(\) is some fixed prior that depends only on the class \(\) and the learning rule \(A\), and does not depend on the population distribution \(\). Namely, they take the form: \[\ \  m:\ d(A(S),),\] where \(S^{m}\).
* Distribution-_dependent_ (DD) stability. Here, the prior may depend also on \(\), so each population distribution \(\) might have a different prior. Namely: \[\ _{}\  m:\ d(A(S),_{}).\]

A substantial body of literature has investigated the interconnections among distribution-dependent definitions. In Theorem 1.4, we provide a comprehensive summary of the established equivalences. A natural question arises as to whether a similar web of equivalences exists for distribution-independent definitions. Our principal contribution is to affirm that, indeed, such a network exists. Identifying such equivalences is a step towards creating a comprehensive taxonomy of stability definitions.

### Our Contribution

Our first main contribution is an equivalence between distribution-independent definitions of stability.

**Theorem** (**Informal Version of Theorem 2.1)**.: _The following definitions of stability are weakly equivalent:_

1. _Pure Differential Privacy;_ 1. _Pure Differential Privacy;_ 2. _Distribution-Independent_ \(\)_-Stability;_ 1. _Distribution-Independent One-Way Pure Perfect Generalization;_ 2. _Distribution-Independent_ \(_{}\)_-Stability for_ \((1,)\)_._ 2. _Distribution-Independent_ \(_{}\)_-Stability for_ \((1,)\)_._ 3. _Where_ \(_{}\) _is the Renyi divergence of order_ \(\)_. Furthermore, a hypothesis class_ \(\) _has a PAC learning rule that is stable according to one of these definitions if and only if_ \(\) _has finite fractional clique dimension (See Appendix_ B.1_)._

**Remark 1.2**.: _Observe that DI \(\)-stability is equivalent to DI \(_{1}\)-stability, and DI one-way pure perfect generalization is equivalent to DI \(_{}\)-stability. Therefore, The above theorem can be viewed as stating a weak equivalence between pure differential privacy and \(_{}\)-stability for \([1,]\)._

**Remark 1.3**.: _In this paper we focus purely on the information-theoretic aspects of learning under stability constraints, and therefore we consider learning rules that are mathematical functions, and disregard considerations of computability and computational complexity._

Table 1 summarizes the distribution-independent definitions discussed in Theorem 2.1. All the definitions in each row are weakly equivalent.

One example for how the equivalence results can help build bridges between different stability notions in the literature is the connection between pure differential privacy and the PAC-Bayes theorem. Both of these are fundamental ideas that have been extensively studied. Theorem 2.1 states that a hypothesis class admits a pure differentially private PAC learner if and only if it admits a distribution independent \(\)-stable PAC learner. This is an interesting and non-trivial connection between two well studied notions. As a concrete example of this connection, recall that thresholds over the real line cannot be learned by a differentially private learner . Hence, by Theorem 2.1, there does not exist a PAC learner for thresholds that is \(\)-stable. Another example is half-spaces with margins in \(^{d}\). Half-spaces with margins are differentially private learnable , therefore there exists a PAC learner for half-spaces with margins that is \(\)-stable.

Our second main contribution is a boosting result for weak learners that have bounded \(\)-divergence with respect to a distribution-independent prior. Our result demonstrates that distribution-independent \(\)-stability is boostable. It is interesting to see that one can simultaneously boost both the stability and the learning parameters of an algorithm.

**Theorem** (**Informal Version of Theorem 2.2)**.: _Let \(\) be a hypothesis class. If there exists a weak learner \(A\) for \(\), and there exists a prior distribution \(\) such that the expectation of \((A(S))\) is bounded, then there exists a \(\)-stable PAC learner that admits a logarithmic divergence bound._

The proof of Theorem 2.2 relies on connections between boosting of PAC learners and online learning with expert advice.

   Name & Dissimilarity & Definition \\  \(\)-Stability & \(_{S}[(A(S)) o(m)] 1-o(1)\) & 3.6 \\ \(_{}\)-Stability & \(_{S}[_{}(A(S)) o(m)] 1-o(1)\) & 3.6 \\ Pure Perfect Generalization & \(_{S}:\;A(S)() e^{o(m)} () 1-o(1)\) & 3.7 \\   

Table 1: Distribution-independent Bayesian definitions of stability.

[MISSING_PAGE_EMPTY:4]

* \(\) _has a PAC learning rule that is stable according to one of the definitions 1 to 6 (and the cardinality of the domain is as described above);_
* \(\) _has finite Littlestone dimension; (Definition C.3)_
* \(\) _has finite clique dimension. (Definition C.5)_

We emphasize that Theorem 1.4 is a summary of existing results, and is not a new result. We believe that our compilation serves as a valuable resource, and that stating these results here in a unified framework helps to convey the conceptual message of this paper. Namely, the fact that a large number of disparate results can neatly be organized based on our notions of distribution-dependent and distribution-independent definitions of stability is a valuable observation that can help researchers make sense of the stability landscape.

### Related Works

The literature on stability is vast. Stability has been studied in the context of optimization, statistical estimation, regularization (e.g.,  and ), the bias-variance tradeoff, algorithmic stability (e.g., ; see bibliography in Section 13.6 of ), bagging , online learning and optimization and bandit algorithms (e.g., ; see bibliography in Section 28.6 of ), and other topics.

There are numerous definitions of stability, including pure and approximate Differential Privacy , Perfect Generalization , Global Stability , KL-Stability , TV-Stability , \(f\)-Divergence Stability , Renyi Divergence Stability , and Mutual Information Stability .

Our work is most directly related to the recent publication by Bun et al. . They established connections and separations between replicability, approximate differential privacy, max-information and perfect generalization for a broad class of statistical tasks. The reductions they present are sample-efficient, and nearly all are computationally efficient and apply to a general outcome space. Their results are central to the understanding of equivalences between notions of stability as laid out in the current paper.

A concurrent work by Kalavasis et al.  showed that TV-stability, replicability and approximate differential privacy are equivalent; this holds for general statistical tasks on countable domains, and for PAC learning on any domain. They also provide a statistical amplification and TV-stability boosting algorithm for PAC learning on countable domains.

Additionally, recent works  have shown an equivalence between differential privacy and robustness for estimation tasks.

Theorem 2.2 is a boosting result. Boosting has been a central topic of study in computational learning theory since its inception in the 1990s by Schapire  and Freund . The best-known boosting algorithm is AdaBoost , which has been extensively studied. Boosting also has rich connections with other topics such as game theory, online learning, and convex optimization (see , Chapter 10 in , and Chapter 7 in ).

## 2 Technical Overview

This section presents the complete versions of Theorems 1.4 and 2.2. We provide a concise overview of the key ideas and techniques employed in the proofs. All proofs appear in the appendices.

   Name & Dissimilarity & Definition & References \\  \(\)-Stability & \(_{S}[(A(S)_{}) o(m)]  1-o(1)\) & 3.6 &  \\ TV-Stability & \(_{S}[(A(S),_{})] o(1)\) & 3.13 &  \\ \(\)-Stability & \(_{S}[(A(S)_{})] o(m)\) & 3.12 &  \\ Perfect Generalization & \(_{S}[ O:\,A(S)() e^{}_{ }()+] 1-o(1)\) & 3.7 &  \\ Global Stability & \(_{S,h_{}}[A(S)=h]\) & 3.11 &  \\ Replicability & \(_{r}_{S,h_{,r}}[A(S;r)=h_{r}]\) & 3.10 &  \\   

Table 2: Distribution-dependent Bayesian definitions of stability.

Please refer to Section 3 for a complete overview of preliminaries, including all technical terms and definitions.

### Equivalences between DI Bayesian Notions of Stability

The following theorem, which is one of the main results of this paper, shows the equivalence between different distribution-independent definitions. The content of Theorem 2.1 is summarized in Table 1.

**Theorem 2.1** (Distribution-Independent Equivalences).: _Let \(\) be a hypothesis class. The following is equivalent._

1. _There exists a learning rule that PAC learns_ \(\) _and satisfied pure differential privacy (Definition_ 3.5_)._
2. \(\) _has finite fractional clique dimension._
3. _For every_ \([1,]\)_, there exists a learning rule that PAC learns_ \(\) _and satisfied distribution-independent_ \(_{}\)_-stability (Definition_ 3.6_)._
4. _For every_ \([1,]\)_, there exists a distribution-independent_ \(_{}\)_-stable PAC learner_ \(A\) _for_ \(\)_, that satisfies the following:_ 1. \(A\) _is interpolating almost surely. Namely, for every_ \(\)_-realizable distribution_ \(\)_,_ \(_{S^{m}}[_{S}(A(S))=0]=1\)_._ 2. \(A\) _admits a divergence bound of_ \(f(m)=O( m)\)_, with confidence_ \((m) 0\)_. I.e., for every_ \(\)_-realizable distribution_ \(\)_,_ \(_{}(A(S)\,\|\,) O( m)\) _with probability_ \(1\)_, where_ \(S^{m}\) _and_ \(\) _is a prior distribution independent of_ \(\)_._ 3. _For every_ \(\)_-realizable distribution_ \(\)_, the expected population loss of_ \(A\) _with respect to_ \(\) _satisfies_ \(_{S^{m}}[_{}(A(S))] O(  m})\)_._

_In particular, plugging \(=1\) in Item (ii) implies \(\)-stability with divergence bound of \(f(m)=O( m)\) and confidence \((m) 0\). Plugging \(=\) implies distribution-independent one-way \(\)-pure perfect generalization, with \((m) O( m)\) and confidence \((m) 0\)._

#### 2.1.1 Proof Idea for Theorem 2.1

We prove the following chain of implications:

\[)}}{{ }}_{})}}{{ }}_{}\;[1,] )}}{{}}.\]

Pure DP\(_{}\)-Stability.The first step towards proving implication (1) is to define a suitable prior distribution \(\) over hypotheses. The key tool we used in order to define \(\) is the characterization of pure DP via the fractional clique dimension . In a nutshell,  proved that (i) a class \(\) is pure DP learnable if and only if the fractional clique dimension of \(\) is finite; (ii) the fractional clique dimension is finite if and only if there exists a polynomial \(q(m)\) and a distribution over hypothesis \(_{m}\), such that for every realizable sample \(S\) of size \(m\), we have

\[_{h_{m}}[_{S}(h)=0].\] (1)

(For more details please refer to Appendix B.1.) Now, the desired prior distribution \(\) is defined to be a mixture of all the \(_{m}\)'s.

The next step in the proof is to define a learning rule \(A\): (i) sample hypotheses from the prior \(\); (ii) return the first hypothesis \(h\) that is consistent with the input sample \(S\) (i.e. \(_{S}(h)=0\)). \(A\) is well-defined since with high probability it will stop and return a hypothesis after \( q(m)\) re-samples from \(\). Since the posterior \(A(S)\) is supported on \(\{h:_{S}(h)=0\}\), a simple calculation which follows from Equation (1) shows that for every realizable distribution \(\), \(_{}(A(S)\,\|\,)(q(m))\) almost surly where \(S^{m}\).

Finally, since for \([1,]\) the Renyi divergence \(_{}(_{1}\,\|\,_{2})\) is non-decreasing in \(\) (see Lemma A.1), we conclude that \((A(S)\,\|\,) O( m)\), hence by PAC-Bayes theorem \(A\) generalizes.

\(_{}\)-Stability\(_{}\)-Stability\([1,]\).This implication is immediate since the Renyi divergence \(_{}(_{1}\,\|\,_{2})\) is non-decreasing in \(\).

\(_{}\)-**Stability**\([1,]\)**Pure DP.** In fact, it suffices to assume \(\)-stability. We prove that the promised prior \(\) satisfies that for every realizable sample \(S\) of size \(m\), we have \(_{h}[_{S}(h)=0](m)}\), and conclude that \(\) is pure DP learnable. Given a realizable sample \(S\) of size \(m\), we uniformly sample \( m m\) examples from \(S\) and feed the new sample \(S^{}\) to the promised \(\)-stable learner \(A\). By noting that if \((A(S^{}))\) is small, one can lower bound the probability of an event according to \(\) by its probability according to \(A(S^{})\). The proof then follows by applying a standard concentration argument.

### Stability Boosting

We prove a boosting result for weak learners with bounded \(\) with respect to a distribution-independent prior. We show that every learner with bounded \(\) that slightly beats random guessing can be amplified to a learner with logarithmic \(\) and expected loss of \(O( m})\).

**Theorem 2.2** (Boosting Weak Learners with Bounded \(\)).: _Let \(\) be a set, let \(\{0,1\}^{}\) be a hypothesis class, and let \(A\) be a learning rule. Assume there exists \(k\) and \(>0\) such that_

\[():\ _{S ^{k}}[_{}(A(S))]-,\] (2)

_and there exists \(\{0,1\}^{}\) and \(b 0\) such that_

\[():\ _{S ^{k}}[(A(S))] b.\] (3)

_Then, there exists an interpolating learning rule \(A^{}\) that PAC learns \(\) with logarithmic \(\)-stability. More explicitly, there exists a prior distribution \(^{}\{0,1\}^{}\) and function \(b^{}\) and \(^{}\) that depend on \(\) and \(b\) such that_

\[()\  m:\] \[_{S^{m}}[(A^{}(S) ^{}) b^{}(m)=O((m))]=1,\] (4) _and_ \[_{S^{m}}[_{}(A^{ }(S))]^{}(m)=O}.\] (5)

#### 2.2.1 Proof Idea for Theorem 2.2

The strong learning rule \(A^{}\) is obtained by simulating the weak learner \(A\) on \(O( m/^{2})\) samples of constant size \(k\) (which are carefully sampled from the original input sample \(S\)). Then, \(A^{}\) returns an aggregated hypothesis - the majority vote of the outputs of \(A\). As it turns out, \(A^{}\) satisfies logarithmic \(\)-stability with respect to the prior \(^{}\) that is a mixture of majority votes of the original prior \(\). The analysis involves a reduction to regret analysis of online learning using expert advice, and also uses properties of the \(\)-divergence.

## 3 Preliminaries

### Divergences

The Renyi \(\)-divergence is a measure of dissimilarity between distributions that generalizes many common dissimilarity measures, including the Bhattacharyya coefficient (\(=1/2\)), the Kullback-Leibler divergence (\(=1\)), the log of the expected ratio (\(=2\)), and the log of the maximum ratio (\(=\)).

**Definition 3.1** (Renyi divergence; ).: _Let \((1,)\). The Renyi divergence of order \(\) of the distribution \(\) from the distribution \(\) is_

\[_{}()= (_{x}(x)}{ (x)}^{-1}).\]_For \(=1\) and \(=\) the Renyi divergence is extended by taking a limit. In particular, the limit \( 1\) gives the Kullback-Leibler divergence,_

\[_{1}() =_{x}(x)}{ (x)}=(),\] _and_ \[_{}() =(*{ess\,sup}_{}(x)}{(x)}),\]

_with the conventions that \(0/0=0\) and \(x/0=\) for \(x>0\)._

### Learning Theory

We use standard notation from statistical learning (e.g., ). Given a hypothesis \(h:\{0,1\}\), the _empirical loss_ of \(h\) with respect to a sample \(S=\{(x_{1},y_{1}),,(x_{m},y_{m})\}\) is defined as \(_{S}(h)=_{i=1}^{m}[h(x_{i}) y_{i}]\). A learning rule \(A\) is _interpolating_ if for every input sample \(S\), \(_{h A(S)}[_{S}(h)=0]=1\). The _population loss_ of \(h\) with respect to a population distribution \(\) over \(\{0,1\}\) is defined as \(_{}(h)=_{(x,y)}[h(x) y]\). A population \(\) over labeled examples is _realizable_ with respect to a class \(\) if \(_{h}_{}(h)=0\). We denote the set of all realizable population distributions of a class \(\) by \(*{Realizable}()\). Given a learning rule \(A\) and an input sample \(S\) of size \(m\), the _population loss_ of \(A(S)\) with respect to a population \(\) is defined as \(_{h A(S)}[_{}(h)]\).

A hypothesis class \(\) is _Probably Approximately Correct (PAC) learnable_ if there exists a learning rule \(A\) such that for all \(*{Realizable}()\) and for all \(m\), we have \(_{S^{m}}[_{}(A(S))](m)\), where \(_{m}(m)=0\).

**Theorem 3.2** (PAC-Bayes Bound; ; Theorem 31.1 in ).: _Let \(\) be a set, let \(\{0,1\}^{}\), and let \((\{0,1\})\). For any \((0,1)\) and for any \(()\),_

\[*{}_{S^{m}} ():\ _{}()_{S}()+()+(m/)}{2(m-1)}}  1-.\]

### Definitions of Stability

Throughout the following section, let \(\) be a set called the _domain_, let \(\{0,1\}^{}\) be a hypothesis class, and let \(m\) be a sample size. A _randomized learning rule_, or a _learning rule_ for short, is a function \(A:\ (\{0,1\})^{*}\{0,1\}^{ }\) that takes a training sample and outputs a distribution over hypotheses. A _population distribution_ is a distribution \((\{0,1\})\) over labeled domain elements, and a _prior distribution_ is a distribution \(\{0,1\}^{}\) over hypotheses.

#### 3.3.1 Differential Privacy

Differential privacy is a property of an algorithm that guarantees that the output will not reveal any meaningful amount of information about individual people that contributed data to the input (training data) used by the algorithm. See  for an introduction.

**Definition 3.3**.: _Let \(,_{ 0}\), and let \(\) and \(\) be two probability measures over a measurable space \((,)\). We say that \(\) and \(\) are \((,)\)-indistinguishable and write \(_{,}\), if for every event \(\), \(() e^{}()+\) and \(() e^{}( )+}\)._

**Definition 3.4** (Differential Privacy; ).: _Let \(,_{ 0}\). A learning rule \(A\) is \((,)\)-differentially private if for every pair of training samples \(S,S^{}(\{0,1\})^{m}\) that differ on a single example, \(A(S)\) and \(A(S^{})\) are \((,)\)-indistinguishable._

Typically, \(\) is chosen to be a small constant (e.g., \( 0.1\)) and \(\) is negligible (i.e., \((m) m^{-(1)}\)). When \(=0\) we say that \(A\) satisfies _pure_ differentially privacy.

**Definition 3.5** (Private PAC Learning).: \(\) _is privately learnable or DP learnable if it is PAC learnable by a learning rule \(A\) which is \(((m),)\)-differentially-private, where \((m) 1\) and \((m)=m^{-(1)}\). \(A\) is pure DP learnable if the same holds with \((m)=0\)._

#### 3.3.2 \(_{}\)-Stability and \(\)-Stability

**Definition 3.6** (\(_{}\)-Stability).: _Let \([1,]\). Let \(A\) be a learning rule, and let \(f:\) and \(:\) satisfy \(f(m)=o(m)\) and \((m)=o(1)\)._1. _A is distribution-independent_ \(_{}\)_-stable if_ \[\;\; m:\;_{S^{m}}[_{ }(A(S)\,\|\,) f(m)] 1-(m).\]
2. _A is distribution-dependent_ \(_{}\)_-stable if_ \[\;_{}\; m:\;_{S ^{m}}[_{}(A(S)\,\|\,_{}) f (m)] 1-(m).\]

_The function \(f\) is called the divergence bound and \(\) is called the confidence. The special case of \(=1\) is referred to as \(\)-stability ._

#### 3.3.3 Perfect Generalization

**Definition 3.7** (One-Way Perfect Generalization).: _Let \(A\) be a learning rule, and let \(:\) satisfy \((m)=o(1)\)._

1. _Let_ \(:\) _satisfy_ \((m)=o(m)\)_._ _A is_ \(\)_-pure perfectly generalizing with confidence_ \(\) _if_ \[\;\; m:\;_{S^{m}} :\;A(S)() e^{(m)}() 1-(m).\]
2. _([_1_]__[_16_]__:) Let_ \(,_{ 0}\)_. A is_ \(\)_-approximately perfectly generalizing with confidence_ \(\) _if_ \[\;_{}\; m:\;_{S ^{m}}[:\;A(S)() e^{} _{}()+] 1-(m).\]

#### 3.3.4 Replicability

**Definition 3.8** (Replicability; ).: _Let \(_{>0}\) and let \(\) be a distribution over random strings. A learning rule \(A\) is \(\)-replicable if_

\[, m:\;_{S_{1},S_{2}^{m}\\ r}[A(S_{1};r)=A(S_{2};r)],\]

_where \(r\) represents the random coins of \(A\)._

**Remark 3.9**.: _Note that both in  and in  the definition of \(\)-replicability is slightly different. In their definition, they treat the parameter \(\) as the failure probability, i.e., \(A\) is a \(\)-replicable learning rule by their definition if the probability that \(A(S_{1};r)=A(S_{2};r)\) is at least \(1-\)._

There exists an alternative 2-parameter definition of replicability introduced in .

**Definition 3.10** (\((,)\)-Replicability; ).: _Let \(,_{>0}\) and let \(\) be a distribution over random strings. Coin tosses \(r\) are \(\)-good for a learning rule \(A\) with respect to a population distribution \(\) if there exists a canonical output \(h_{r}\) such that for every \(m\), \(_{S^{m}}[A(S;r)=h_{r}]\). A learning rule \(A\) is \(\)-replicable if_

\[:\;_{r}[r ].\]

#### 3.3.5 Global Stability

**Definition 3.11** (Global Stability; ).: _Let \(>0\) be a global stability parameter. A learning rule \(A\) is \((m,)\)-globally stable with respect to a population distribution \(\) if there exists a canonical output \(h\) such that \([A(S)=h]\), where the probability is over \(S^{m}\) as well as the internal randomness of \(A\)._

#### 3.3.6 \(\)-Stability

**Definition 3.12** (Mutual Information Stability; ).: _A learning rule \(A\) is \(\)-stable if there exists \(f:\) with \(f=o(m)\) such that_

\[\; m:I(A(S),S) f(m),\]

_where \(S^{m}\)._

#### 3.3.7 \(\)-Stability

**Definition 3.13** (\(\)-Stability; Appendix A.3.1 in ).: _Let \(A\) be a learning rule, and let \(f:\) satisfy \(f(m)=o(1)\)._

1. \(A\) _is distribution-independent_ \(\)_-stable if_ \[\;\; m :\;_{S^{m}}[(A(S),)] f(m).\]
2. \(A\) _is distribution-dependent_ \(\)_-stable if_ \[\;_{}\; m :\;_{S^{m}}[(A(S),_{})] f(m).\]

#### 3.3.8 Max Information

**Definition 3.14**.: _Let \(A\) be a learning rule, and let \(,_{ 0}\). A has \((,)\)-max-information with respect to product distributions if for every event \(\) we have_

\[[(A(S),S)] e^{}[(A(S),S^{ })]+\]

_where are \(S,S^{}\) are independent samples drawn i.i.d from a population distribution \(\)._