# Universal Rates of Empirical Risk Minimization

Steve Hanneke

Department of Computer Science

Purdue University

steve.hanneke@gmail.com &Mingyue Xu

Department of Computer Science

Purdue University

xu1864@purdue.edu

###### Abstract

The well-known _empirical risk minimization_ (ERM) principle is the basis of many widely used machine learning algorithms, and plays an essential role in the classical PAC theory. A common description of a learning algorithm's performance is its so-called "learning curve", that is, the decay of the expected error as a function of the input sample size. As the PAC model fails to explain the behavior of learning curves, recent research has explored an alternative universal learning model and has ultimately revealed a distinction between optimal universal and uniform learning rates (Bousquet et al., 2021). However, a basic understanding of such differences with a particular focus on the ERM principle has yet to be developed.

In this paper, we consider the problem of universal learning by ERM in the realizable case and study the possible universal rates. Our main result is a fundamental _tetrachotomy_: there are only four possible universal learning rates by ERM, namely, the learning curves of any concept class learnable by ERM decay either at \(e^{-n},1/n,\)\(/n\), or arbitrarily slow rates. Moreover, we provide a complete characterization of which concept classes fall into each of these categories, via new complexity structures. We also develop new combinatorial dimensions which supply sharp asymptotically-valid constant factors for these rates, whenever possible.

## 1 Introduction

The classical statistical learning theory mainly focuses on the celebrated PAC (Probably Approximately Correct) model (Vapnik and Chervonenkis, 1974; Valiant, 1984) with emphasis on supervised learning. A particular setting therein, called the _realizable_ case, has been extensively studied. Complemented by the "no-free-lunch" theorem (Antos and Lugosi, 1996), the PAC framework, which adopts a minimax perspective, can only explain the best _worst-case_ learning rate by a learning algorithm over all realizable distributions. Such learning rates are thus also called the _uniform_ rates. However, the uniform rates can only capture the upper envelope of all learning curves, and are too coarse to explain practical machine learning performance. This is because real-world data is rarely worst-case, and the data source is typically fixed in a given learning scenario. Indeed, Cohn and Tesauro (1990, 1992) observed from experiments that practical learning rates can be much faster than is predicted by PAC theory. Moreover, many theoretical works (Schuurmans, 1997; Koltchinskii and Beznosova, 2005; Audibert and Tsybakov, 2007, etc.) were able to prove faster-than-uniform rates for certain learning problems, though requiring additional modelling assumptions. To distinguish from the uniform rates, these rates are named the _universal_ rates and was formalized recently by Bousquet et al. (2021) via a distribution-dependent framework. Unlike the simple _dichotomy_ of the optimal uniform rates: every concept class \(\) has a uniform rate being either linear \(()/n\) or "bounded away from zero", the optimal universal rates are captured by a _richotomy_: every concept class \(\) has a universal rate being either exponential, linear or arbitrarily slow (see Thm.1.6 Bousquet et al., 2021).

In supervised learning, a family of successful learners called the _empirical risk minimization_ (ERM) consist of all learning algorithms that output a sample-consistent classifier. In other words, an ERMalgorithm is any learning rule, which outputs a concept in \(\) that minimizes the empirical error (see Appendix A for a formal definition). For notation simplicity, we first introduce

**Definition 1** (**Version space, Mitchell, 1977**).: _Let \(\) be a concept class and \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\) be a dataset, the version space induced by \(S_{n}\), denoted by \(V_{S_{n}}()\) (or \(V_{n}()\) for short), is defined as \(V_{S_{n}}():=\{:h(x_{i})=y_{i}, i[n]\}\)._

Now given labeled samples \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\), an ERM algorithm is any learning algorithm that outputs a concept in the sample-induced _version space_, that is, a sequence of universally measurable functions \(_{n}:S_{n}_{n} V_{S_{n}}(),n \). Throughout this paper, we will simply denote an ERM algorithm by its output predictors \(\{_{n}\}_{n}\).

It is well-known that the ERM principle plays an important role in understanding general uniform learnability: a concept class is uniformly learnable if and only if it can be learned by ERM. However, while the optimal \(()/n\) rate is achievable by some improper learner (Hanneke, 2016a), ERM algorithms can at best achieve a uniform rate of \((()/n)())}\). Moreover, such a gap has been shown to be unavoidable in general (Auer and Ortner, 2007), which leaves a challenging question to study: what are the sufficient and necessary conditions on \(\) for the entire family of ERM algorithms to achieve the optimal error? Indeed, many subsequent works have devoted to improving the logarithmic factor in specific scenarios. The work of Gine and Koltchinskii (2006) refined the bound by replacing \(())}\) with \(()/n))}\), where \(()\) is called the _disagreement coefficient_. Based on this, Hanneke and Yang (2015) proposed a new data-dependent bound with \(_{1:n}/())}\), where \(_{1:n}\) is a quantity related to the _version space compression set size_ (a.k.a. the _empirical teaching dimension_). As a milestone, the work of Hanneke (2016b) proved an upper bound \((()/n)_{}/( ))}\) and a lower bound \((()+_{})})/n\), where \(_{}\) is called the _star number_ of \(\) (see Definition 4 in Section 2). Though not quite matching, these two bounds together yield an optimal linear rate when \(_{}<\). Thereafter, the uniform rates by ERM can be described as a _trichotomy_, namely, every concept class \(\) has a uniform rate by ERM being exactly one of the following: \(1/n\), \(\) and "bounded away from zero".

From a practical perspective, many ERM-based algorithms are designed and are widely applied in different areas of machine learning, such as the logistic regression and SVM, the CAL algorithm in active learning, the gradient descent (GD) algorithm in deep learning. Since the worst-case nature of the PAC model is too pessimistic to reflect the practice of machine learning, understanding the distribution-dependent performance of ERM algorithms is of great significance. However, unlike that a distinction between the optimal uniform and universal rates has been fully understood, how fast universal learning can outperform uniform learning in particular by ERM remains unclear. Furthermore, we are lacking a complete theory to the characterization of the universal rates by ERM, though certain specific scenarios that admit faster rates by ERM have been discovered (Schuurmans, 1997; van Handel, 2013). In this paper, we aim to answer the following fundamental question:

**Question 1**.: _Given a concept class \(\), what are the possible rates at which \(\) can be universally learned by ERM?_

We start with some basic preliminaries of this paper. We consider an _instance space_\(\) and a _concept class_\(\{0,1\}^{}\). Given a probability distribution \(P\) on \(\{0,1\}\), the _error rate_ of a classifier \(h:\{0,1\}\) is defined as \(_{P}(h):=P((x,y)\{0,1\}:h(x) y)\). A distribution \(P\) is called _realizable_ with respect to \(\), denoted by \(P()\), if \(_{h}_{P}(h)=0\). Note that in this definition, \(h^{*}\) satisfying \(_{P}(h^{*})=_{h}_{P}(h)\) is called the _target concept_ of the learning problem, and is not necessary in \(\). We may also say that \(P\) is a realizable distribution centered at \(h^{*}\). Given an integer \(n\), we denote by \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n} P^{n}\) a i.i.d. \(P\)-distributed dataset. In the universal learning framework, the performance of a learning algorithm is commonly measured by its _learning curve_(Bousquet et al., 2021; Hanneke et al., 2022; Bousquet et al., 2023), that is, the decay of the _expected error rate_\([_{P}(_{n})]\) as a function of sample size \(n\). With these settings settled, we are now able to formalize the problem of universal learning by ERM.

**Definition 2** (**Universal learning by ERM**).: _Let \(\) be a concept class, and \(R(n) 0\) be a rate function. We say_

* \(\) _is universally learnable at rate_ \(R\) _by ERM, if for every distribution_ \(P()\)_, there exist parameters_ \(C,c>0\) _such that for every ERM algorithm,_ \([_{P}(_{n})] CR(cn)\)_, for all_ \(n\)_._

[MISSING_PAGE_FAIL:3]

learnable but necessarily with arbitrarily slow rates, or is not universally learnable at all. Throughout this paper, we only consider the case where the given concept class is universally learnable by ERM. We leave it an open question whether there exists a nice characterization that determines the universal learnability by ERM.

### Main results

In this section, we summarize the main results of this paper. The examples in Section 1.1 reveal that there are at least four possible universal rates by ERM. Interestingly, we find that these are also the only possibilities. The following two theorems consist of the main results of this work. In particular, Theorem 1 gives out a complete answer to Question 1. It expresses a fundamental _tetrachotomy_: there are exactly four possibilities for the universal learning rates by ERM: being either exponential, or linear, or \((n)/n\), or arbitrarily slow rates. Moreover, Theorem 2 specifies the answer by pointing out for what realizable distributions (targets), those universal rates are sharp.

**Theorem 1** (**Universal rates for ERM)**.: _For every class \(\) with \(|| 3\), the following hold:_

* \(\) _is universally learnable by ERM with exact rate_ \(e^{-n}\) _if and only if_ \(||<\)_._
* \(\) _is universally learnable by ERM with exact rate_ \(1/n\) _if and only if_ \(||=\) _and_ \(\) _does not have an infinite star-eluder sequence._
* \(\) _is universally learnable by ERM with exact rate_ \((n)/n\) _if and only if_ \(\) _has an infinite star-eluder sequence and_ \(()<\)_._
* \(\) _requires at least arbitrarily slow rates to be learned by ERM if and only if_ \(()=\)_._

**Remark 3**.: _The formal definition of the star-eluder sequence can be found in Section 2. Unlike the separation between exact \(e^{-n}\) and \(1/n\) rates is determined by the cardinality of the class, and the separation between exact \((n)/n\) and arbitrarily slow rates is determined by the VC dimension of the class, whether there exists a simple combinatorial quantity that determines the separation between exact \(1/n\) and \((n)/n\) rates is unclear and might be an interesting direction for future work. We thought that it is likely the star number \(_{}\) (Definition 4) is the correct characterization here, but it turns out not unfortunately (see details in Section 4 and Appendix B.3)._

Based on Theorem 1, a distinction between the performance of ERM algorithms and the optimal universal learning algorithms can be revealed, which we present in the following table (the required definitions in "Case" are deferred to Section 2, and examples can be found in Appendix B.2).

  Optimal rate & Exact rate by ERM & Case & Example \\  \(e^{-n}\) & \(1/n\) & infinite eluder sequence but no infinite Littestone tree & Example 12 \\  \(e^{-n}\) & \((n)/n\) & infinite star-eluder sequence but no infinite Littestone tree & Example 13 \\  \(e^{-n}\) & arbitrarily slow & infinite VC-eluder sequence but no infinite Littestone tree & Example 15 \\  \(1/n\) & \((n)/n\) & infinite star-eluder sequence but no infinite VCL tree & Example 14 \\  \(1/n\) & arbitrarily slow & infinite VC-eluder sequence but no infinite VCL tree & Example 16 \\   Furthermore, the distinction between the universal rates and the uniform rates by ERM can also be fully captured, and are depicted schematically in Figure 1 as an analogy to the Fig.4 of Bousquet et al. (2021). Besides the examples in Section 1.1, we also need the following additional example concerning the Littlestone dimension to appear in the diagram.

**Example 6** (\((n)/n\) learning rate and unbounded Littlestone dimension).: _We consider here the class of two-dimensional halfspaces, that is, \(:=^{2}\) and \(_{,}:=\{(+b  0):^{2},b\}\). It is a classical fact that for any integer \(d\), the class of halfspaces on \(^{d}\) has a finite VC dimension \(d\), but has an infinite Littlestone tree, and thus having unbounded Littlestone dimension (Shalev-Shwartz and Ben-David, 2014). Finally, to show that this class is universally learnable by ERM at exact \((n)/n\) rate, we simply consider the subspace \(^{1}\), this is indeed an infinite star set of \(_{,}\) centered at \(h_{}\) and thus an infinite star-eluder sequence._

As a complement to Theorem 1, the following Theorem 2 gives out target-specified universal rates. We say a target concept \(h^{*}\) is universally learnable by ERM with exact rate \(R\) if all realizable distribution \(P\) considered in Definition 2 are centered at \(h^{*}\). In other words, \(\) is universally learnable with exact rate \(R\) is equivalent to say all realizable target concepts are universally learnable with exact rate \(R\). Concretely, for each of the four possible rates stated in Theorem 1, Theorem 2 specifies the target concepts that can be learned at such exact rate by ERM.

**Theorem 2** (**Target specified universal rates**).: _For every class \(\) with \(|| 3\), and a target concept \(h^{*}\), the following hold:_

* \(h^{*}\) _is universally learnable by ERM with exact rate_ \(e^{-n}\) _if and only if_ \(\) _does not have an infinite eluder sequence centered at_ \(h^{*}\)_._
* \(h^{*}\) _is universally learnable by ERM with exact rate_ \(1/n\) _if and only if_ \(\) _has an infinite eluder sequence centered at_ \(h^{*}\)_, but does not have an infinite star-eluder sequence centered at_ \(h^{*}\)_._
* \(h^{*}\) _is universally learnable by ERM with exact rate_ \(/n\) _if and only if_ \(\) _has an infinite star-eluder sequence centered at_ \(h^{*}\)_, but does not have an infinite VC-eluder sequence centered at_ \(h^{*}\)_._
* \(h^{*}\) _requires at least arbitrarily slow rates to be universally learned by ERM if and only if_ \(\) _has an infinite VC-eluder sequence centered at_ \(h^{*}\)_._

All detailed proofs appear in Appendix D. We also provide a brief overview of the main idea of each proof as well as some related concepts in Section 2.

An additional part of this work presents a _fine-grained analysis_(Bousquet et al., 2023) of the universal rates by ERM, which complements the _coarse rates_ used in Theorem 1. Concretely, we provide a characterization of sharp distribution-free constant factors of the ERM universal rates, whenever possible. The characterization is based on two newly-developed combinatorial dimensions, called the _star-eluder dimension_ (or SE dimension) and the _VC-eluder dimension_ (or VCE dimension) (Definition 9). We say "whenever possible" because distribution-free constants are unavailable for certain cases (Remark 16). Such a characterization can also be considered as a refinement to the classical PAC theory, in a sense that it is sometimes better but only asymptotically-valid. Due to space limitation, we defer the definition of fine-grained rates and related results to Appendix C.

### Related works

**PAC learning by ERM.** The performance of consistent learning rules (including the ERM algorithm) in the PAC (distribution-free) framework has been extensively studied. For VC classes, Blumer et al. (1989) gave out a \(/n\) upper bound of the uniform learning rate. Despite the well-known equivalence between uniform learnability and uniform learnability by the ERM principle (Vapnik and Chervonenkis, 1971), the best upper bounds for general ERM algorithms differ from the optimal sample complexity by an unavoidable logarithmic factor (Auer and Ortner, 2007). By analyzing the _disagreement coefficient_ of the version space, the work of Gine and Koltchinskii (2006); Hanneke (2009) refined the logarithmic factor in certain scenarios. Furthermore, not only being a relevant

Figure 1: A Venn diagram depicting the tetrachotomy of the universal rates by ERM and its relation with the uniform rates characterized by the VC dimension and the star number.

measure in the context of active learning (Cohn et al., 1994; El-Yaniv and Wiener, 2012; Hanneke, 2011, 2014), the _region of disagreement_ of the version space was found out to have an interpretation of _sample compression scheme_ with its size known as the _version space compression set size_(Wiener et al., 2015; Hanneke and Yang, 2015). Based on this, the label complexity of the CAL algorithm can be converted into a bound on the error rates of all consistent PAC learners (Hanneke, 2016b). Finally, Hanneke and Yang (2015); Hanneke (2016b) introduced a simple combinatorial quantity named the _star number_, and guaranteed that a concept class with finite star number can be uniformly learned at linear rate.

**Universal Learning.** Observed from empirical experiments, the actual learning rates on real-world data can be much faster than the one described by the PAC theory (Cohn and Tesauro, 1990, 1992). The work of Benedek and Itai (1988) considered a setting lies in between the PAC setting and the universal setting called _nonuniform learning_, in which the learning rate may depend on the target concept but still uniform over the marginal distributions. After that, Schuurmans (1997) studied classes of _concept chains_ and revealed a distinction between exponential and linear rates along with a theoretical guarantee. Later, more improved learning rates have been obtained for various practical learning algorithms such as stochastic gradient decent and kernel methods (Koltchinskii and Beznosova, 2005; Audibert and Tsybakov, 2007; Pillaud-Vivien et al., 2018, etc.). Additionally, van Handel (2013) studied the uniform convergence property from a universal perspective, and proposed the _universal Glivenko-Cantelli property_. Until very recently, the universal (distribution-dependent) learning framework was formalized by Bousquet et al. (2021), in which a complete theory of the (optimal) universal learnability was obtained as well. After that, Bousquet et al. (2023) carried out a fine-grained analysis on the "distribution-free tail" of the universal _learning curves_ by characterizing the optimal constant factor. As generalizations, Kalavasis et al. (2022); Hanneke et al. (2023) studied the universal rates for multiclass classification, and Hanneke et al. (2022) studied the universal learning rates under an interactive learning setting.

## 2 Technical overview

In this section, we discuss some technical aspects in the derivation of our main results in Section 1.2. Our analysis to the universal learning rates by ERM is based on three new types of complexity structures named the _eluder sequence_, the _star-eluder sequence_ and the _VC-eluder sequence_. More details can be found in Sections 3-4 and all technical proofs are deferred to Appendix D.

**Definition 3** (**Realizable data)**.: _Let \(\) be a concept class on an instance space \(\), we say that a (finite or infinite) data sequence \(\{(x_{1},y_{1}),(x_{2},y_{2}),\}(\{0,1\})^{}\) is realizable (with respect to \(\)), if for every \(n\), there exists \(h_{n}\) such that \(h_{n}(x_{i})=y_{i}\), for all \(i[n]\)._

**Definition 4** (**Star number)**.: _Let \(\) be an instance space and \(\) be a concept class. We define the region of disagreement of \(\) as DIS\(():=\{x: h,gh(x) g(x)\}\). Let \(h\) be a classifier, the star number of \(h\), denoted by \(_{h}()\) or \(_{h}\) for short, is defined to be the largest integer \(s\) such that there exist distinct points \(x_{1},,x_{s}\) and concepts \(h_{1},,h_{s}\) satisfying DIS\((\{h,h_{i}\})\{x_{1},,x_{s}\}=\{x_{i}\}\), for every \(1 i s\). (We say \(\{x_{1},,x_{s}\}\) is a star set of \(\) centered at \(h\).) If no such largest integer \(s\) exists, we define \(_{h}=\). The star number of \(\), denoted by \(()\) or \(_{}\), is defined to be the maximum possible cardinality of a star set of \(\), or \(_{}=\) if no such maximum exists._

**Remark 4**.: _From this definition, it is clear that the star number \(_{}\) of \(\) satisfies \(_{}()\). Indeed, any set \(\{x_{1},,x_{d}\}\) that gathered by \(\) is also a star set of \(\) based on the following reasoning: Since \(\{x_{1},,x_{d}\}\) is shattered by \(\), there exists \(h\) such that \(h(x_{1})==h(x_{d})=0\). Moreover, for any \(i[d]\), there exists \(h_{i}\) satisfying \(h_{i}(x_{i})=1\) and \(h_{i}(x_{j})=0\) for all \(j i\). An immediate implication is that a VC-eluder sequence is always a star-eluder sequence (see Definition 6 and Definition 7 below)._

With these basic definitions in hand, we next define the three aforementioned sequences:

**Definition 5** (**Eluder sequence)**.: _Let \(\) be a concept class, we say that \(\) has an eluder sequence \(\{(x_{1},y_{1}),,(x_{d},y_{d})\}\), if it is realizable and for every integer \(k[d]\), there exists \(h_{k}\) such that \(h_{k}(x_{i})=y_{i}\) for all \(i<k\) and \(h_{k}(x_{k}) y_{k}\). The eluder dimension of \(\), denoted by \(E()\), is defined to be the largest integer \(d 1\) such that \(\) has an eluder sequence \(\{(x_{1},y_{1}),(x_{2},y_{2}),,(x_{d},y_{d})\}\). If no such largest \(d\) exists, we say \(\) has an infinite eluder sequence and define \(E()=\). We say an infinite eluder sequence \(\{(x_{1},y_{1}),(x_{2},y_{2}),\}\) is centered at \(h\), if \(h(x_{i})=y_{i}\) for all \(i\)._

**Remark 5**.: _It has been proved that \(\{_{},())}\} () 4^{\{_{},2^{( )}\}}\)[Li et al., 2022, Thm.8], where \(()\) is the Littestone dimension of \(\). Moreover, the very recent work of Hanneke (2024) proved that \(()|| 2^{_{} ()}\), which implies that any concept class with finite star number and finite Littlestone dimension must be a finite class._

Before proceeding to the next two definitions, we define a sequence of integers \(\{n_{k}\}_{k}\) as \(n_{1}=0\), \(n_{k}:=\) for all \(k>1\), which satisfies \(n_{k+1}-n_{k}=k\) for all \(k\).

**Definition 6** (Star-eluder sequence).: _Let \(\) be a concept class and \(h\) be a classifier. We say that \(\) has an infinite star-eluder sequence \(\{(x_{1},y_{1}),(x_{2},y_{2}),\}\) centered at \(h\), if it is realizable and for every integer \(k 1\), \(\{x_{n_{k}+1},,x_{n_{k}+k}\}\) is a star set of \(V_{n_{k}}()\) centered at \(h\)._

**Definition 7** (VC-eluder sequence).: _Let \(\) be a concept class and \(h\) be a classifier. We say that \(\) has an infinite VC-eluder sequence \(\{(x_{1},y_{1}),(x_{2},y_{2}),\}\) centered at \(h\), if it is realizable and labelled by \(h\), and for every integer \(k 1\), \(\{x_{n_{k}+1},,x_{n_{k}+k}\}\) is a shattered set of \(V_{n_{k}}()\)._

**Remark 6**.: _In the definitions of eluder sequence and VC-eluder sequence, "\(\{(x_{1},y_{1}),(x_{2},y_{2}),\}\) centered at \(h\)" simply means the sequence is labelled by \(h\). However, the words "centered at" in the definition of star-eluder sequence is more meaningful. In this paper, we give them a uniform name in order to make Theorem 2 look consistent._

**Remark 7**.: _An infinite star-eluder (VC-eluder) sequence requires the version space to keep on having star (shattered) sets with infinitely increasing sizes. If the size cannot grow infinitely, the largest possible size of the star (shattered) set is called the star-eluder (VC-eluder) dimension (Definition 9), which plays an important role in our fine-grained analysis (Appendix C). To distinguish the notion of star-eluder (VC-eluder) sequence here from the \(d\)-star-eluder (\(d\)-VC-eluder) sequence defined in Appendix C, we may call the construction in Definition 6 an infinite strong star-eluder sequence, and the construction in Definition 7 an infinite strong VC-eluder sequence._

Proof Sketch of Theorem 1 and 2.: The proof of Theorem 1 is devided into two parts (Section 3 and Section 4). Roughly speaking, for each equivalence therein, we first characterize the exact universal rates by ERM via the three aforementioned sequences (see Theorems 3-6 in Section 3). We have to prove a lower bound together with an upper bound for the sufficiency since we are showing the "exact" universal rates. The lower bound is established by constructing a realizable distribution on the existent infinite sequence, and the derivation of upper bound is strongly related to the classical PAC theory. To prove the necessity, we will use the method of contradiction. Then in Section 4, we establish equivalent characterizations via those well-known complexity measures, whenever possible. Theorem 2 is an associated target-dependent version, and is directly proved by those corresponding lemmas in Section 3. The complete proof structure for Theorem 1 can be summarized as follow:

**For the first bullet**, we start by proving that \(\) is universally learnable with exact rate \(e^{-n}\) if and only if \(\) does not have an infinite eluder sequence (Theorem 3), and then we extend the equivalence by showing that \(\) does not have an infinite eluder sequence if and only if \(\) is a finite class (Lemma 8). **For the second bullet**, we prove that \(\) is universally learnable with exact rate \(1/n\) if and only if \(\) has an infinite eluder sequence but does not have an infinite star-eluder sequence (Theorem 4). Then the desired equivalence follows immediately from the first bullet. **For the third bullet**, we prove that \(\) is universally learnable with exact rate \(/n\) if and only if \(\) has an infinite star-eluder sequence but does not have an infinite VC-eluder sequence (Theorem 5). The desired equivalence comes in conjunction with the claim that \(\) has an infinite VC-eluder sequence if and only if \(\) has infinite VC dimension (Lemma 9). Finally, **for the last bullet**, it suffices to prove that \(\) requires at least arbitrarily slow rates to be universally learned by ERM if and only if \(\) has an infinite VC-eluder sequence (Theorem 6). 

## 3 Exact universal rates

Sections 3 and 4 of this paper are devoted to the proof ideas of Theorems 1 and 2 with further details. In this section, we give a complete characterization of the four possible exact universal rates by ERM (\(e^{-n},1/n,/n\) and arbitrarily slow rates) via the existence/nonexistence of the three combinatorial sequences defined in Section 2. For each of the following "if and only if" results (Theorems 3-6), we are required to prove both the sufficiency and the necessity. The sufficiency consists of both an upper bound and a lower bound since we are proving the exact universal rates. Thenecessity also follows simply by the method of contradiction, given the rates are exact. All technical proofs are deferred to Appendix D.1.

### Exponential rates

**Theorem 3**.: \(\) _is universally learnable by ERM with exact rate \(e^{-n}\) if and only if \(\) does not have an infinite eluder sequence._

The lower bound for sufficiency is straightforward and was established by Schuurmans (1997).

**Lemma 1** (\(e^{-n}\) **lower bound**).: _Given a concept class \(\), for any learning algorithm \(_{n}\), there exists a realizable distribution \(P\) with respect to \(\) such that \([er_{P}(_{n})] 2^{-(n+2)}\) for infinitely many \(n\), which implies that \(\) is not universally learnable at rate faster than exponential rate \(e^{-n}\)._

**Remark 8**.: _Note that this lower bound is actually stronger than desired in a sense that it holds for any learning algorithm (not necessarily for ERM algorithms)._

**Lemma 2** (\(e^{-n}\) **upper bound**).: _If \(\) does not have an infinite eluder sequence (centered at \(h^{*}\)), then \(\) (\(h^{*}\)) is universally learnable by ERM at rate \(e^{-n}\)._

Proof of Theorem 3.: The sufficiency follows directly from the lower bound in Lemma 1 together with the upper bound in Lemma 2. Furthermore, Lemma 3 in Section 3.2 proves that the existence of an infinite eluder sequence leads to a linear lower bound of the ERM universal rates. Therefore, the necessity follows by using the method of contradiction. 

### Linear rates

**Theorem 4**.: \(\) _is universally learnable by ERM with exact rate \(1/n\) if and only if \(\) has an infinite eluder sequence but does not have an infinite star-eluder sequence._

**Lemma 3** (\(1/n\) **lower bound**).: _If \(\) has an infinite eluder sequence centered at \(h^{*}\), then \(h^{*}\) is not universally learnable by ERM at rate faster than \(1/n\)._

**Lemma 4** (\(1/n\) **upper bound**).: _If \(\) does not have an infinite star-eluder sequence (centered at \(h^{*}\)), then \(\) (\(h^{*}\)) is universally learnable by ERM at rate \(1/n\)._

Proof of Theorem 4.: To prove the sufficiency, on one hand, the existence of an infinite eluder sequence implies a linear lower bound based on Lemma 3. On the other hand, if \(\) does not have an infinite star-eluder sequence, Lemma 4 yields a linear upper bound. The necessity can be proved by the method of contradiction. Concretely, if either of the two conditions fail, the universal rates will be either \(e^{-n}\) or at least \(/n\), based on Lemma 2 in Section 3.1 and Lemma 5 in Section 3.3. 

### \(/n\) rates

**Theorem 5**.: \(\) _is universally learnable by ERM with exact rate \(/n\) if and only if \(\) has an infinite star-eluder sequence but does not have an infinite VC-eluder sequence._

**Lemma 5** (\(/n\) **lower bound**).: _If \(\) has an infinite star-eluder sequence centered at \(h^{*}\), then \(h^{*}\) is not universally learnable by ERM at rate faster than \(/n\)._

**Remark 9**.: _Note that the conclusion in Remark 5 explains why the intersection of "infinite Littlestone classes" and "classes with finite star number" is empty in Figure 1. However, we mention in Remark 3 that infinite star number does not guarantee an infinite star-eluder sequence (see Appendix B.3 for details). Hence, Remark 5 cannot explain why the intersection of "infinite Littlestone classes" and "classes that are learnable at linear rate by ERM" is also empty. To address this problem, we give out the following additional result:_

**Proposition 1**.: _Any infinite concept class \(\) has either an infinite star-eluder sequence or infinite Littlestone dimension._

**Lemma 6** (\(/n\) **upper bound**).: _If \(\) does not have an infinite VC-eluder sequence (centered at \(h^{*}\)), then \(\) (\(h^{*}\)) is universally learnable by ERM at \(/n\) rate._

Proof of Theorem 5.: To prove the sufficiency, on one hand, if \(\) has an infinite star-eluder sequence, the universal rates have a \(/n\) lower bound based on Lemma 5. On the other hand, if \(\) does not have an infinite VC-eluder sequence, then Lemma 6 yields a \(/n\) upper bound. The necessity can be proved using the method of contradiction based on Lemma 4 in Section 3.2 and Lemma 7 in Section 3.4 below. 

### Arbitrarily slow rates

**Theorem 6**.: \(\) _requires at least arbitrarily slow rates to be learned by ERM if and only if \(\) has an infinite VC-eluder sequence._

Proof of Theorem 6.: Given the necessity proved by Lemma 6 in Section 3.3, it suffices to prove the sufficiency, which is completed by the following Lemma 7. 

**Lemma 7** (**Arbitrarily slow rates**).: _If \(\) has an infinite VC-eluder sequence centered at \(h^{*}\), then \(h^{*}\) requires at least arbitrarily slow rates to be universally learned by ERM._

## 4 Equivalent characterizations

In Section 3, it has been shown that the eluder sequence, the star-eluder sequence and the VC-eluder sequence are the correct characterizations of the exact universal learning rates by ERM. However, the definitions to them are somewhat non-intuitive. Therefore, in this section, we aim to build connections between these combinatorial sequences and some well-understood complexity measures, which will then give rise to our Theorem 1. Concretely, we have the following two equivalences (see Appendix D.2 for their complete proofs).

**Lemma 8**.: \(\) _has an infinite eluder sequence if and only if \(||=\)._

**Lemma 9**.: \(\) _has an infinite VC-eluder sequence if and only if \(()=\)._

Maybe surprisingly, unlike the above two equivalences, \(_{}=\) is inequivalent to the existence of an infinite star-eluder sequence. Indeed, it is straightforward from definition that if \(\) has an infinite star-eluder sequence, then it must have \(_{}=\). However, the converse is not true.

**Proposition 2**.: \(_{}=\) _if \(\) has an infinite star-eluder sequence. Moreover, there exist concept classes \(\) with \(_{}=\) but does not have any infinite star-eluder sequence._

**Remark 10**.: _Based on the results in Section 3, the proposition essentially states that the gap between \(1/n\) and \(/n\) exact universal rates by ERM is not characterized by the star number \(_{}\). We wonder whether there is some other simple combinatorial quantity that is determinant to this gap, which would be an valuable direction for future work._

Why is the case of star-eluder sequence different from the other two structures? We suspect that such a distinction may arise from the following: unlike the eluder sequence and the VC-eluder sequence, the centered concept of a star-eluder sequence is much more meaningful (see Remark 6). Concretely, within its definition, the set of the following \(k\) points is not only required to be a star set of the version space \(V_{n_{k}}()\), but is required to be centered at the same labelling target. This intuitively implies that there might exists a class such that for arbitrarily large integer \(k\), it can witness a star set of size \(k\), but with a \(k\)-specified center (for different \(k\)). Such a class (e.g. Examples 19, 20 in Appendix B.3) does have infinite star number but will not have an infinite star-eluder sequence. Indeed, the relations between those star-related notions (star number, star-eluder dimension, star set and star eluder sequence) turn out to be more complicated than expected, and we leave it to Appendix B.3.

## 5 Appendix Summary

Due to page limitation, we leave some interesting results as well as all the proofs to Appendices, which are briefly summarized below. Given extra required notations and definitions in Appendix A and related technical lemmas in Appendix E, the main body of supplements consists of three parts, namely, Appendices B, C and D.

Specifically, Appendix B contains three sub-parts. In Appendix B.1, we provide direct mathematical analysis (without using our characterization in Section 1.2) for those basic examples in Section 1.1. In Appendix B.2, we provide details of examples that appeared in Section 1.2. These examples are carefully constructed, providing evidence that ERM algorithms cannot guarantee the optimal universal rates (Bousquet et al., 2021). In Appendix B.3, we construct nuanced examples to distinguish between the following notions: star number \(_{}\) (Definition 4), the star-eluder dimension \(()\) (Definition 9), star set (Definition 4) and star eluder sequence (Definition 6). These examples will convince the readers why our characterization in Theorem 1 uses the star eluder sequence rather than the star number (see our discussions in Remarks 3 and 10).

Appendix C presents a fine-grained analysis of the asymptotic rate of decay of the universal learning curves by ERM, whenever possible. This will be an analogy to the optimal fine-grained universal learning curves studied in Bousquet et al. (2023). Concretely, we provide a characterization of sharp distribution-free constant factors of the ERM universal rates. Our characterization of these constant factors is based on two newly-developed combinatorial dimensions, namely, the _star-eluder dimension_ (or SE dimension) and the _VC-eluder dimension_ (or VCE dimension) (Definition 9). We say "whenever possible" because distribution-free constants are unavailable for certain cases (see our discussion in Remark 16). Such a characterization can be considered as a refinement to the classical PAC theory, in a sense that it is sometimes better but only asymptotically-valid.

Finally, Appendix D includes all the missing proofs for the theorems and lemmas that have shown up in previous sections.

## 6 Conclusion and Future Directions

In this paper, we reveal a fundamental tetrachotomy of the universal learning rates by the ERM principle and provide a complete characterization of the exact universal rates via certain appropriate complexity structures. Additionally, by introducing new combinatorial dimensions, we are able to characterize sharp asymptotically-valid constant factors for these rates, whenever possible. While only the realizable case is considered in this paper, we believe analogous results can be extend to different learning scenarios such as the agnostic case. Generalizing the results from binary classification to multiclass classification would be another valuable future direction. Moreover, since this paper considers the "worst-case" ERM in its nature, studying the universal rates of the "best-case" ERM is also an interesting problem which we leave for future work.