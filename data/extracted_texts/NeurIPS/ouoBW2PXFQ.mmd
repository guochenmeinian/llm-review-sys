# CALANet: Cheap All-Layer Aggregation for Human Activity Recognition

Jaegyun Park\({}^{1}\), Dae-Won Kim\({}^{1,*}\), Jaesung Lee\({}^{2,}\)

\({}^{1}\)School of Computer Science and Engineering, Chung-Ang University, Republic of Korea

\({}^{2}\)Department of Artificial Intelligence, Chung-Ang University, Republic of Korea

jgp0566.cau@gmail.com, {dwkim, curseor}@cau.ac.kr

Corresponding authors.

###### Abstract

With the steady growth of sensing technology and wearable devices, sensor-based human activity recognition has become essential in widespread applications, such as healthcare monitoring and fitness tracking, where accurate and real-time systems are required. To achieve real-time response, recent studies have focused on lightweight neural network models. Specifically, they designed the network architectures by restricting the number of layers shallowly or connections of each layer. However, these approaches suffer from limited accuracy because the classifier only uses the features at the last layer. In this study, we propose a cheap all-layer aggregation network, CALANet, for accuracy improvement while maintaining the efficiency of existing real-time HAR models. Specifically, CALANet allows the classifier to aggregate the features for all layers, resulting in a performance gain. In addition, this work proves that the theoretical computation cost of CALANet is equivalent to that of conventional networks. Evaluated on seven publicly available datasets, CALANet outperformed existing methods, achieving state-of-the-art performance. The source codes of the CALANet are publicly available at https://github.com/jgpark92/CALANet.

## 1 Introduction

Human activity recognition (HAR) is a fundamental technique in healthcare , fitness tracking , and surveillance . Wearable sensor-based HAR has drawn attention in pervasive computing applications due to the popularity of smart wearable devices in recent years . Specifically, it aims to identify motion details of users or activity tracks from sensor signal patterns . To this end, neural networks (NNs) have been widely used to achieve a superior learning performance without handcrafted feature engineering . Besides, with advances in microelectronics and inertial sensor-based wearable devices, recent researchers have focused on achieving real-time systems . Especially, a recent trend across most studies has become increasingly to train NNs on a resource-rich computing device and then deploy them to resource-limited wearable devices, where inference is executed .

Recent real-time HAR studies have focused on one-dimensional (1D) convolutional neural networks (CNNs) compatible with various hardware accelerators and deployment frameworks . CNNs include convolution and pooling operations, which allow them to extract more abstracted high-level features as input signals pass from early to later layers. Specifically, the pooling operation abstracts signals by reducing the feature size (temporal resolution), which can be regarded as a sampling of signal. As a result, the final classifier predicts an activity class based on the abstracted or sampled features at the last layer. The sampled features are more semantic and globalthan those of prior layers, but it has not been proven that the last layer is the optimal representation . Although the high-level features sampled by the pooling operation can avoid over-fitting of the classifier , for the HAR dataset, the loss of some detailed information makes it challenging to classify activities that share similar semantics, such as "Sit" and "Talk-Sit."

Figure 1 illustrates intermediate features in the forward pass of conventional CNNs. Conventional CNNs classify "Jump" and "Sit" well but tend to misclassify "Talk-Sit" as another activity, as shown in Figure 1. These experimental observations have also been reported in existing studies . Specifically, the features at the eighth layer (with more compact and short waveforms) make it easy for the classifier to discriminate "Jump" and "Sit," compared with the ones at the third layer. On the contrary, the features at the third layer (with more detailed information) can be more suitable than the ones at the eighth layer when classifying "Sit" and "Talk-Sit" that have similar vibrations in signal waveforms. Although which layer has the best features depends on the activity, conventional CNNs classify multiple activities only using the features at the last layer.

The objective of real-time HAR is to maximize accuracy under real-time constraints. To achieve real-time response, prior HAR studies designed the network architectures of CNNs by restricting the number of layers shallowly  or reducing connections of each layer . However, these approaches suffer from limited accuracy because their classifier only uses the features at the last layer. A straightforward approach to address this issue is to allow the classifier to use the features for all layers , but this leads to a substantial increase in computational cost, particularly as the number of layers deepens. Therefore, our goal is to design a novel network architecture that allows the network to aggregate the features for all layers _while maintaining the computational cost of the conventional CNNs regardless of network depth_, as shown in Figure 1.

Figure 1: Analysis of representations in our experiments on KU-HAR dataset . In a conventional CNN, the classifier predicts activities only using the feature representations at the last layer. Features at the early layer include the detailed information of original signals that may confound the classifier. In comparison, features at the later layer are more semantic, but the features (with more compact and short waveforms) make it challenging to classify activities that share similar semantics. Our goal is to design a CALANet that allows the classifier to use features for all layers while maintaining the inference time of conventional CNNs.

In this paper, we propose a novel network, CALANet, with a cheap _all-layer_ aggregation (CALA) structure. To achieve our goal, CALANet includes (1) learnable channel-wise transformation matrices and (2) scalable layer aggregation pool. First, we introduce new learnable channel-wise transformation matrices (LCTMs) to minimize an increase in computational cost due to all-layer aggregation. Given intermediate features at a specific layer (with temporal resolution \(T\) and the number of channels \(M\)), \(M\) LCTMs generate a vector with \(N T\) elements based on linear transformation and combination without increasing the theoretical computation cost. Second, we improve the effectiveness of all-layer aggregation by introducing a scalable layer aggregation pool (SLAP) that allows CALANet to stack layers without significantly increasing computational costs. As a result, the main contributions of this paper are as follows:

* We proposed CALANet with a CALA structure that allows the network to aggregate the features for all layers while maintaining the efficiency of CNNs regardless of network depth based on (1) LCTMs and (2) SLAP.
* We theoretically proved that the computational cost of CALANet is equivalent to that of conventional CNNs, including even shallow networks.
* We empirically demonstrated the effectiveness of CALANet in achieving superior performance compared to 11 state-of-the-art methods on seven public benchmark datasets.

## 2 Related Work

Many comprehensive surveys in HAR literature have highlighted the importance of NN-based models and real-time applications [7; 10; 34; 42; 43; 44; 52; 61]. For instance, early real-time HAR studies adopted two-dimensional (2D) convolutional NNs (CNNs) with shallow architectures [9; 25; 33; 45]. Specifically, they transformed the sensor signal patterns to 2D spectral images as an input of 2D CNNs. However, these approaches require complex preprocessing, such as discrete or short-time Fourier transform, which increases the overhead during continuous processing for real-time HAR. Meanwhile, Ignatov  proposed a one-dimensional (1D) CNN architecture using basic statistical features to encode global temporal information. Although this approach demonstrated the potential of 1D CNN, it still requires several data preprocessing like calculating the histogram of input signals.

Recent real-time HAR studies focus on 1D CNNs without any complex data preprocessing. For example, Zebin et al.  proposed a CNN architecture comprising four convolution layers. Furthermore, they showed the efficiency of parameter quantization as post-processing for further optimization. In another study, Wan et al.  adopted a CNN architecture including three convolution layers. They demonstrated the superiority of CNNs compared with recurrent NN variants on HAR datasets with basic activities. To enhance shallow CNNs, Huang et al.  introduced a cross-channel communication that exchanges information among channels within the same layer. These models achieved real-time HAR by shallowly restricting the number of layers, resulting in limited accuracy.

To alleviate the issue, some studies have considered efficient variants of convolution at each layer instead of reducing the number of layers. For example, Gao et al.  proposed a selective kernel module that divides the convolution into split, fuse, and select steps to adjust receptive field size adaptively. Similarly, Tang et al.  designed a hierarchical-split block to enhance multiscale temporal features by composing channel groups hierarchically. In another study, Teng et al.  proposed RepHAR, which re-parameterizes a pretrained multibranch CNN to a plain CNN before deploying it into resource-limited devices. However, these approaches still are insufficient to classify activities that have similar vibrations in signal waveforms because the classifier only uses the features at the last layer. Meanwhile, Park et al.  introduced a grouped temporal shift network that can flexibly re-design a network architecture to support various hardware specifications. Although this network can derive layer-specific structures suitable for a given computational budget, its performance is limited according to an initial network architecture. Therefore, its performance can be improved by using our CALANet as the initial network, as will be described in Section 4.2.

Besides, the classifier requires both local and global temporal representations to achieve high HAR accuracy . Specifically, the locality of CNNs improves accuracy due to their translational invariance concerning the precise location of activity within a segment of time-series data . On the other hand, recurrent layers or attention mechanisms have an advantage for global feature extraction because they can model long-term dependencies. In this regard, many studies have attempted to integrate recurrent layers [8; 24; 35; 50; 56] or attention mechanisms [40; 63] into CNNs, which has increased both accuracy and inference times. The increase in inference time is primarily because of the lack of device-level optimizations compared with CNNs [15; 29; 60]. These accuracy-oriented networks will be compared with our CALANet in Section 4.2.

## 3 Cheap All-Layer Aggregation Network

In this section, our goal is to design a CNN architecture that can aggregate features for all layers into the final classifier without increasing the computational cost of CNNs. Furthermore, we prove that the theoretical computation cost of the proposed CALANet is equivalent to that of conventional CNNs, including even shallow networks.

### Computational cost of convolutional neural networks

Our goal is to improve the HAR accuracy while maintaining the efficiency of CNNs. Therefore, before deriving a novel network structure and proving its theoretical efficiency, we formalize the computational cost of the conventional CNNs in a generalized form. Note that we only consider the computational cost in the feed-forward step, not the training step, which is unrelated to inference time. Let \(^{(l)}\) be the \(l\)-th layer of a network, where \(^{(l)}\) and \(^{(l+1)}\) are calculated sequentially and independently. Therefore, the theoretical computation cost of the CNN can be defined as a summation for each computation of layers, as described in Definition 1.

**Definition 1**.: _Let \(()\) be the computational cost of calculating the output of each layer. Given a network architecture \(\) with \(L\) layers, the input \(X^{(l)}\) is fed into the \(l\)-th layer with trainable parameters \(^{(l)}\) to calculate the output \(X^{(l+1)}\). Due to this layer composition, its computational cost is formalized as_

\[C_{n}()=_{l=1}^{L}(^{(l)}(X^{(l)};^{(l)} )).\] (1)

To formalize the computation cost of CNNs, we borrow the concept of time complexity as the upper bound of the computational cost. Similar to Proposition 3 of , Eq. (1) is simplified by Proposition 1.

**Proposition 1**.: _The time complexity of CNNs is formalized as:_

\[(L-1)( _{k}^{2}L).\] (2)

_where \(\) and \(\) are the number of channels and temporal resolution for input data, and \(\) and \(_{k}\) are average numbers for output channels and kernel sizes across a network, respectively._

The proof is given in Appendix A. Because we do not restrict the number of layers shallowly, we will assume that the condition of Eq. (2) is always true in this paper.

### Learnable channel-wise transformation matrix

In this section, we introduce a learnable channel-wise transformation matrix (LCTM) that allows our CALANet to aggregate features for all layers without increasing the theoretical computation cost. Figure 2 shows the network architecture of CALANet. Given input signals, the convolution and pooling layers extract the high-level sampled features by calculating temporal correlations and reducing the feature resolution. After that, the features for all layers are connected with the classifier via the cheap all-layer aggregation module based on the LCTMs.

Let \(_{}\) be a feature vector of \(m\)-th channel with temporal resolution \(T=|_{}|\) at any layer. Because \(T\) varies with the layer, we define a mapping function \(f:_{}^{N}\), where a constant value \(N T\). After that, we calculate \(f\) via a transformation matrix \(^{N,T}\). As the mapping function is calculated for each channel of features, \(m\)-th feature vector is transformed to \(_{}^{N}\) as follows:

\[_{}=_{}_{}\] (3)

where this transformation can be interpreted as a compression of global temporal information. After that, a linear combination is conducted to calculate relations between channels as follows:

\[}=_{m}a_{m}_{}\] (4)where \(}^{N}\) is a vector and \(a_{m}\) is \(m\)-th coefficient of the linear combination.

We replace \(a_{m}}\) with \(}^{N,T}\). As a result, Eq. (4) is simplified as follows:

\[}=_{m}}}\] (5)

where elements of \(}\) can directly be optimized by stochastic gradient descent because these matrix multiplications can be implemented by dense layers. Therefore, we define \(B_{m}\) as LCTM. Herein, the time complexity of calculating \(M\) LCTM operations is \((TMN)\). Finally, \(}\) for all layers are concatenated and then fed into the classifier.

To investigate whether our CALANet can maintain the efficiency of conventional CNNs, we analyze the time complexity of CALANet. Based on Proposition 1, the time complexity of CALANet is formalized in Lemma 1.

**Lemma 1**.: _The time complexity of CALANet is equivalent to:_

\[(_{k}^{2}L).\] (6)

The proof is given in Appendix B. According to Proposition 1 and Lemma 2, our CALANet can aggregate features for all layers while maintaining the efficiency of conventional CNNs.

### Scalable layer aggregation pool

The effectiveness of all-layer aggregation depends on a layer aggregation pool, i.e., the number of layers, as shown in Table 3. To improve the accuracy of CALANet further, we also introduce a scalable layer aggregation pool (SLAP) that allows CALANet to stack layers without significantly increasing computational cost. To this end, we, in this section, aim to omit \(L\) in Eq. (2). Inspired by ShuffleNet , we first use the grouped convolution and channel shuffle to reduce the time complexity of the standard convolutions. Precisely, the M input channels are evenly divided into \(G\) channel groups. After that, the standard convolution generates \( N/G\) output channels for each channel group. Subsequently, the channel shuffle operation is executed.

Figure 2: Network architecture of CALANet. Convolution and pooling layers extract the sampled features by reducing the temporal resolution. CALANet aggregates the features for all layers based on the linear transformation and combination.

The entire channels are fully related by the channel shuffle operations if and only if (the number of layers) \(\) (the number of channels within each channel group) \(\) (the number of channel groups) . Therefore, \(G\) is set into a value satisfying \(L N G^{2}\). As \(N G\) and \(G\) are inversely proportional to the computational cost, we set \(G\) into \(L\) without any loss of information for channel correlations. Consequently, the time complexity of the stack of the standard convolutions is reduced in Lemma 2.

**Lemma 2**.: _The time complexity of calculating the standard convolutions is reduced to:_

\[(_{k}^{2}).\] (7)

The proof is given in Appendix C. To reduce the time complexity of all-layer aggregation, we focus on the norm of vectors extracted from LCTMs, i.e., \(|}|\) from Eq. (5). The LCTMs for all layers generate a vector with \(L\) elements fed into the Softmax layer to classify the activities. The large number of units in the Softmax layer may incur overfitting . Therefore, we fix the number of features fed into the Softmax layer to \(\) by dividing the number of rows of LCTM in Eq. (5) by \(L\), resulting in \(_{}^{N/L,M}\). Consequently, the time complexity of all-layer aggregation is reduced in Lemma 3.

**Lemma 3**.: _The time complexity of calculating all-layer aggregation is formalized as:_

\[(^{2}).\] (8)

The proof is given in Appendix D. Finally, we introduce CALANet with the SLAP by omitting the factor \(L\) from its time complexity. Consistent with Lemma 2 and Lemma 3, the time complexity of CALANet is reduced in Theorem 1.

**Theorem 1**.: _The time complexity of CALANet is reduced to:_

\[(_{k}^{2}).\] (9)

The proof is given in Appendix E. From Theorem 1, we crosscheck the efficiency of our CALANet by making comparisons with the time complexity of shallow CNNs in Corollary 1 and Corollary 2.

**Corollary 1**.: _The time complexity of CALANet is equivalent to the shallow CNNs with \(L 2\)._

Proof.: The time complexity of shallow CNNs with \(L=2\) is \((_{k})+(_{k}^{2})=(_{k}^{2})\). It is equivalent to Eq. (9). 

**Corollary 2**.: _The time complexity of CALANet is equivalent to the shallow CNNs with \(L=1\) if \(\)._

Proof.: The time complexity of shallow CNNs with \(L=1\) is \((_{k})\). If \(\), then it is equivalent to Eq. (9). 

In conclusion, our CALANet has a computation cost comparable to shallow CNNs. Especially from Corollary 2, the time complexity of CALANet becomes equivalent to the shallow CNNs even with \(L=1\) as the number of sensors increases.

## 4 Experiments

In this section, we evaluate the superiority of CALANet. In Section 4.1, we describe the experimental setup. Section 4.2 presents the compared results of CALANet and other networks on seven HAR datasets. Section 4.3 provides an in-depth analysis via an ablation study. Lastly, Section 4.4 measures the actual inference time of CALANet.

### Experimental Settings

**Dataset.** We used seven public benchmark datasets, including various sampling frequencies, the number of activities, and sensors. They include **UCI-HAR**, **UniMiB-SHAR**, **DSADS**, **OPPORTUNITY**, **KU-HAR**, **PAMAP2**, and **REALDISP**. The details for each dataset are described in Appendix F.

**Baseline.** We compared CALANet with 11 baseline networks. To evaluate the efficiency of CALANet, we used Shallow ConvNet , RepHAR , and Res-GTSNet  as state-of-the-art models in real-time HAR. Meanwhile, we adopted four CNNs with recurrent layers or attention mechanisms, including DeepConvLSTM , Bi-GRU-I , RevAttNet , and IF-ConvTransformer , to verify the effectiveness of our all-layer aggregation. In addition, we used four networks, T-ResNet [54; 12], T-FCN [54; 12], MILLET , and DSN  that achieved substantial success in the time-series classification (TSC), which is more general-purpose than HAR. The details for the models and hyperparameters are described in Appendix G.

To evaluate the performance of CALANet, we used two metrics: F1-score and floating-point operations (FLOPs). Because the HAR datasets inherently involve a class imbalance, the F1-score has been commonly used as an alternative for accuracy. In particular, FLOPs have been widely used to describe how many operations a given model requires to run a single pattern. In addition, we investigate the change in performance according to \(L\), as will be described in Section 4.3. Meanwhile, Res-GTSNet  can derive layer-specific structures suitable for a given computational budget, and the original paper adopted T-ResNet as an initial network. To improve the efficiency of CALANet further, we also designed the CALA-GTSNet by replacing T-ResNet with our CALANet.

### Comparison results

Table 1 shows the results of comparing CALANet and the baseline networks. The experiments ran ten times, and the average values were recorded on all the datasets. In addition, we performed a paired

    &  &  &  &  \\  Model & F1 & FLOPs & F1 & FLOPs & F1 & FLOPs & F1 & FLOPs \\  CALANet (**Ours**) & 96.1 & **7.6M** & 78.3 & **8.8M** & 90.0 & **8.5M** & 81.6 & **19.3M** \\ CALA-GTSNet (**Ours**) & 94.7\(\) & 3.3M & 74.1\(\) & 4.8M & 87.2\(\) & 5.4M & 78.4\(\) & 15.0M \\ Shallow ConvNet  & 92.5\(\) & 17.9M & 72.2\(\) & 18.2M & 85.6\(\) & 48.5M & 79.5\(\) & 74.3M \\ RepHAR  & 95.1\(\) & 31.8M & 71.6\(\) & 37.3M & 85.5\(\) & 32.9M & 80.0\(\) & 26.0M \\ Res-GTSNet  & 94.5\(\) & 6.4M & 77.2\(\) & 7.51M & 84.4\(\) & 7.3M & 76.0\(\) & 6.4M \\  DeepConvLSTM  & 91.4\(\) & 67.2M & 71.6\(\) & 80.4M & 85.5\(\) & 68.3M & 62.0\(\) & 50.4M \\ Bi-GRU-I  & 94.6\(\) & 46.1M & 75.2\(\) & 54.0M & 85.6\(\) & 48.7M & 77.2\(\) & 39.8M \\ RevAttNet  & 95.1\(\) & 143.1M & 76.7\(\) & 168.7\(\) & 87.6\(\) & 140.2M & 78.6\(\) & 101.5M \\ IF-ConvTransformer  & 95.4 & 209.8M & 77.0\(\) & 183.5M & 87.5\(\) & 628.4M & 82.2 & 986.2M \\  T-ResNet [54; 12] & 95.3 & 123.2M & 76.5\(\) & 145.5M & 87.3\(\) & 125.8M & 80.9 & 96.9M \\ T-FCN [54; 12] & 95.8 & 68.9M & 76.9\(\) & 80.6M & 86.7\(\) & 76.1M & 76.2\(\) & 65.8M \\ MILLET  & 94.7\(\) & 111.6M & 81.4\(\) & 129.9M & 84.3\(\) & 132.8M & 82.3 & 125.0M \\ DSN  & 95.4 & 270.8M & 79.8 & 320.0M & 86.4\(\) & 265.7M & 71.8\(\) & 192.1M \\   

Table 1: Comparison results on seven datasets. \(\)/\(\) indicates that the corresponding model is significantly worse/better than CALANet according to a paired \(t\)-test at a 95% significance level.

\(t\)-test at the 95% significance level on each dataset. In Table 1, \(/\) indicates that the compared network was significantly worse/better than CALALet regarding the F1-score.

**Comparison with real-time CNNs.** In Table 1, the F1-scores of CALANet were statistically superior to real-time HAR models on all datasets. In particular, CALANet has the lowest FLOPs compared to other real-time HAR models with standard convolution layers on seven datasets. Meanwhile, Res-GTSNet, with an efficient variant of the convolution, exhibited significantly low FLOPs. This variant can be easily integrated with our CALANet to reduce its FLOPs further. As shown in Table 1, CALA-GTSNet outperformed Res-GTSNet on 86% of the datasets. Also, CALA-GTSNet has lower FLOPs than Res-GTSNet on 71% of the datasets. As a result, CALANet and GTSNet can complement each other to improve the accuracy or reduce computations. These results demonstrated that our cheap all-layer aggregation can maintain a low computational cost.

**Comparison with accuracy-oriented networks.** We noted that real-time or efficient HAR models using wearable sensors process the input signals with short segmentation lengths for rapid response. If CNNs are sufficient to extract meaningful information from the short-term signals, unnecessary increases in inference time due to integration with recurrent layers or attention mechanisms can be avoided. In Table 1, CALANet outperformed two CNNs with recurrent layers, i.e., DeepConvLSTM and Bi-GRU-I, on all datasets. Compared with RevAttNet and IF-ConvTransformer, CALANet exhibited a comparable F1-score despite its significantly low FLOPs. These results indicate that CNNs are sufficient to model the temporal information for the real-time HAR dataset. Compared with TSC models, CALANet showed comparable performance despite its significantly low FLOPs. These results demonstrated that our cheap all-layer aggregation can significantly improve HAR accuracy while maintaining low FLOPs.

### Ablation study

**The breakdown effect of CALANet.** We conducted an ablation study to investigate the effectiveness and efficiency of our CALANet. The key components of CALANet are LCTMs and SLAP. Therefore, we compared the performance of our CALANet with that of its two variants, which were obtained by removing each component. The first variant removes the SLAP described in Section 3.3. The second variant replaces the LCTMs with fully-connected layers that have the same number of units as the input size. Table 2 shows that our LCTMs substantially reduced the FLOPs for calculating all-layer aggregation without losing the F1-score. In addition, the SLAP enhanced the effectiveness of all-layer

    & &  &  \\  Networks & \(L\) & F1 & FLOPs & F1 & FLOPs \\  CALANet with LCTMs + SLAP & 9 & 97.5 & 29.7M & 79.4 & 74.9M \\ CALANet with LCTMs only & 4 & 93.8\(\) & 60.0M & 73.1\(\) & 113.3M \\ CALANet with ALA only & 4 & 95.0\(\) & 577.9M & 72.8\(\) & 1.7G \\   

Table 2: Ablation study of CALANet on two datasets; LCTMs: Learnable channel-wise transformation matrices, SLAP: Scalable layer aggregation pool, ALA: All-layer aggregation

Figure 3: Tradeoff between the FLOPs and F1-score.

aggregation even while reducing FLOPs. Especially, Figure 3 shows the tradeoff between the FLOPs and F1-score with varying numbers of layers in CALANet with/without LCTMs and SLAP. The tradeoff curves closer to the top-left are more efficient, with a higher F1-score per FLOPs. As shown in Figure 3, CALANet with LCTMs and SLAP achieved a higher F1-score in similar computational cost than one without LCTMs and SLAP.

**Effect of scalable layer aggregation pool.** We investigated the layer aggregation pool at which the best F1-score of CALANet is achieved on seven datasets. Table 3 shows the change in F1-score of CALANet as the layer aggregation pool \(L\) increased; herein, the best F1-score is indicated by the bold font on each dataset. As shown in Table 3, the layer aggregation pool and F1-score tend to be proportional. In Figure 4, CALANet with SLAP (red line) exhibited a negligible increase in FLOPs compared with CALANet only with LCTMs. As a result, SLAP allows CALANet to stack layers without significantly increasing FLOPs.

**Performance analysis on similar activities.** To verify the performance of CALANet, we investigated the confusion matrices (see Appendix H). Prior works [47; 24] suffered from activities that have similar vibrations in signal waveforms, such as "Sit" and "Talk-Sit," as described in Section 1. Compared with these works, our CALANet significantly improved the accuracy of those activities on the KU-HAR dataset. For other examples, these activities include ("rope jumping" and "waking") [20; 14; 48; 49] and ("knees bending crouching" and "reach heels backwards") . On the other hand, our CALANet correctly classified "rope jumping" as "waking" compared to RepHAR that misclassified "rope jumping" as "walking" 20 times  on the PAMAP dataset. Compared to MG-WHAR  misclassified "knees bending crouching" by approximately 20% as "reach heels backwards", our CALANet misclassified "knees bending crouching" as "reach heels backwards" only two times on the REALDISP dataset.

**Applicability of CALA structure.** Our CALA structure can effectively be applied to existing CNNs if the following constraints are satisfied: (1) the layers of a network architecture should be calculated sequentially and independently; (2) the output of each layer should be able to be expressed as a (temporal length \(\) channel size) matrix. To the best of our knowledge, most wearable sensor-based human activity recognition models can satisfy the above constraints. In Table 4, we applied our LCTMs and SLAP to SqueezeNet . Specifically, the output of a squeeze convolution layer in each fire module is fed into LCTMs and connected to the last layer. As a result, our modification significantly improved the F1-score of SqueezeNet on 71% of all datasets while maintaining its FLOPs. In addition, we applied CALANet to the ECG heartbeat classification problem using the MIT-BIH arrhythmia dataset . CALANet exhibited comparable performance with other networks

Figure 4: Comparison among two networks with regard to the impact of \(L\) on the FLOPs.

    &  \\  Datasets & 2 & 3 & 5 & 9 & 17 \\  UCI-HAR & 93.2 & 93.9 & 94.8 & **96.1** & 95.7 \\ UniMiB-SHAR & 72.8 & 76.0 & 78.0 & **78.3** & 77.5 \\ DSADS & 87.2 & 84.5 & 86.0 & **90.0** & 89.4 \\ OPPORTUNITY & 78.9 & 80.2 & 79.0 & **81.6** & 80.3 \\ KU-HAR & 90.0 & 94.7 & 92.7 & 97.5 & **97.7** \\ PAMAP2 & 72.5 & 73.4 & 76.5 & **79.4** & 79.0 \\ REALDISP & 92.9 & 96.7 & 96.9 & **98.2** & 97.7 \\   

Table 3: F1-score of CALANet on different layer aggregation pool, i.e., network depth \(L\)[13; 39] designed to process ECG signals (see Appendix I). This result shows that CALANet has promising applicability to other ML applications.

### Real-Time Activity Prediction

To estimate the actual response time of our CALANet, we used the AMD Ryzen 7 5800X 8-Core Processor without the support of graphics processing units. Particularly, we compared the inference time of CALANet with Shallow ConvNet. Similar to the conventional real-time HAR studies, the measurements were repeated 1,000 times, and the minimum, maximum, and mean values were recorded. Table 5 shows the inference time of the two networks, where the window length was set to 300 (3 \(s\)) to slide one instance at a time. CALANet exhibited a response time similar to Shallow ConvNet, even though its depth is nine times deeper than that of Shallow ConvNet. Consequently, these measurements show that our model is sufficient to meet the real-time requirements.

## 5 Conclusion

In this article, we proposed an effective neural network called CALANet for real-time HAR from wearable sensors. In particular, our CALANet has an all-layer aggregation structure that can aggregate features for all layers based on the learnable channel-wise transformation matrix and scalable layer aggregation pool. As a result, CALANet improved HAR accuracy while maintaining the efficiency of existing real-time HAR models. In addition, we proved that the computational cost of CALANet is equivalent to that of shallow CNNs. Our experiments demonstrated that CALANet could achieve state-of-the-art performance on the HAR datasets under low latency.

Future studies can be conducted to overcome the limitations of the proposed method. CALANet does not consider the various computational budgets that can be changed according to the specific devices and the runtime optimizations of actual devices, such as memory access costs and parallel computations. For example, future studies may further improve CALANet by introducing a new operator designed to match the target device.

**Acknowledgement.** This research was supported in part by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korean Government (MSIT) (2021-0-01341, Artificial Intelligence Graduate School Program (Chung-Ang University)), in part by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korean Government (MSIT) (2021-0-00766, Development of Integrated Development Framework that supports Automatic Neural Network Generation and Deployment optimized for Runtime Environment), and in part by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2023R1A2C1006745).

    &  &  &  &  \\  Model & F1 & FLOPs & F1 & FLOPs & F1 & FLOPs & F1 & FLOPs \\   SqueezeNet + CALA (**Ours**) \\ SqueezeNet \\  & 92.4 & 8.2M & 75.8 & 9.5M & 87.3 & 11.4M & 68.4 & 12.3M \\  SqueezeNet \\  & 92.1 & 10.4M & 74.9 & 12.4M & 84.7**V** & 13.7M & 59.7**V** & 13.2M \\    &  &  &  \\   & F1 & FLOPs & F1 & FLOPs & F1 & FLOPs & \\   SqueezeNet + CALA (**Ours**) \\ SqueezeNet \\  & 96.7 & 19.5M & 70.4 & 41.5M & 93.7 & 34.8M & \\ 
 SqueezeNet \\  & 94.5**V** & 25.5M & 68.0**V** & 52.9M & 85.6**V** & 39.7M & \\   

Table 4: Comparison results of SqueezeNet with/without CALA structure on seven datasets.

    &  \\  Model & Min & Mean & Max \\  CALANet & 1.59ms & 2.25ms & 3.40ms \\ Shallow ConvNet & 1.57ms & 2.15ms & 3.48ms \\   

Table 5: Actual inference time of CALANet