# AMAG: Additive, Multiplicative and Adaptive Graph Neural Network For Forecasting Neural Activity

Jingyuan Li\({}^{1}\), Leo Scholl\({}^{1}\), Trung Le\({}^{1}\), Pavithra Rajeswaran\({}^{2}\), Amy Orsborn\({}^{1,2}\), Eli Shlizerman\({}^{1,3}\)

1. Department of Electrical & Computer Engineering,

2. Department of Bioengineering,

3. Department of Applied Mathematics

University of Washington, Seattle, WA

{jingyli6,lscholl,tle45,pavir,aorsborn,shlizee}@uw.edu

###### Abstract

Latent Variable Models (LVMs) propose to model the dynamics of neural populations by capturing low-dimensional structures that represent features involved in neural activity. Recent LVMs are based on deep learning methodology where a deep neural network is trained to reconstruct the same neural activity given as input and as a result to build the latent representation. Without taking past or future activity into account such a task is non-causal. In contrast, the task of forecasting neural activity based on given input extends the reconstruction task. LVMs that are trained on such a task could potentially capture temporal causality constraints within its latent representation. Forecasting has received less attention than reconstruction due to recording challenges such as limited neural measurements and trials. In this work, we address modeling neural population dynamics via the forecasting task and improve forecasting performance by including a prior, which consists of pairwise neural unit interaction as a multivariate dynamic system. Our proposed model--Additive, Multiplicative, and Adaptive Graph Neural Network (AMAG)--leverages additive and multiplicative message-passing operations analogous to the interactions in neuronal systems and adaptively learns the interaction among neural units to forecast their future activity. We demonstrate the advantage of AMAG compared to non-GNN based methods on synthetic data and multiple modalities of neural recordings (field potentials from penetrating electrodes or surface-level micro-electrocorticography) from four rhesus macaques. Our results show the ability of AMAG to recover ground truth spatial interactions and yield estimation for future dynamics of the neural population.

## 1 Introduction

The ability to encode, decode, and interpret neural activity could facilitate novel applications targeted toward neural activity, such as Brain-Computer Interfaces (BCIs) . A promising methodology for neural activity encoding and decoding is the use of artificial Deep Neural Networks (DNN) . These approaches encode neural activity into latent neural representations, facilitating decoding and interpretation of neural signals and correlating the activity with other modalities, e.g., brain-to-text translation .

Most existing DNN architectures, such as Latent Factor Analysis via Dynamical Systems (LFADS) and its variants, infer latent dynamics from single-trial population activities by encoding the neural time series into a summarized representation from which the original time series could be regenerated, a procedure known as reconstruction task . While effective in capturing low-dimensional latent neural manifolds, the task does not consider causal aspects of neural dynamics. In contrast,the forecasting task, which aims to predict future activity based on past data, adhering to temporal causality constraints, could provide further insight into neural representations that are closer to representations employed by the brain [82; 55; 56; 18]. In addition, learning to forecast offers practical advantages since the signals that are predicted can help to reduce latency in real-time neural decoding applications (e.g., BCIs) or serve as a reference to detect real-time deviations of current recordings compared to earlier ones, indicating dynamic changes in the neuronal system or equipment malfunction .

Neural activity forecasting is inherently more challenging than reconstruction due to the requirement of a larger number of recorded neural signals, which are typically limited, and sensitivity to noise present in the signals. Incorporating priors into model structure design can be beneficial to ensure that instead of learning noisy information or overfitting the training data, the model learns features for forecasting future activity that are generalizable. A well-known prior is the spatial interaction among neurons, involving both addition [27; 9] and multiplication processes [19; 25] which could be modeled with Hodgkin-Huxley or Leaky Integrate-and-Fire [27; 1]. Previous works have utilized probabilistic graphical models to learn interactions [14; 42; 69; 36; 10] with a focus on capturing the neuron interaction itself rather than addressing downstream forecasting tasks. Models that learn the downstream task by considering spatial interaction have been proposed, such as Spatial Temporal Neural Data Transformer (STNDT) and Embedded Interaction Transformer (EIT), where the interaction between neurons is modeled using an attention mechanism [37; 44; 68]. Graph Neural Networks Networks (GNNs), designed to handle graph-structured data, are an alternative to the attention mechanism to model neuron interactions .

In this study, we propose a GNN with additive and multiplicative message-passing operations, called AMAG, for neuron signal forecasting. As shown in Fig. 1 (A), AMAG includes three modules: temporal encoding (TE), spatial interaction (SI), and temporal readout (TR). TE and TR model temporal dynamics for each recording channel independently. SI models the interaction between recording channels by leveraging the additive and multiplicative message-passing operations. We test the model on synthetic datasets and show the ability of AMAG to recover the ground truth interaction between channels. In addition, we demonstrate the effectiveness of the proposed model by testing it on two neural local field potential recording types (each with datasets from two animals). The first type is a publicly available dataset recorded by Utah array (penetrating probes) and the second type is a novel dataset of micro-electrocorticography (\(\)ECoG) array recordings from the cortical surface. These four recordings are from the motor cortices of rhesus macaque monkeys performing reaching. We compare our model with other approaches, including GNN or non-GNN based. These computational evaluations demonstrate that AMAG outperforms other models regarding forecasting accuracy on neural recordings. We summarize the main contributions of our work below:

* _Modeling spatial-temporal relationship._ We present a graph neural network (GNN), called AMAG, that can forecast neuron activity by using sample-dependent additive and multiplicative message-passing operations with a learnable adjacency matrix.
* _Learning underlying spatial interaction._ We demonstrate the ability of AMAG to learn the underlying spatial interactions on synthetic datasets and show the importance of learning to forecast in discovering such interactions.

Figure 1: AMAG Overview. (A) The forecasting task and AMAG architecture with temporal Encoding (TE), Spatial Interaction (SI), and Temporal Readout (TR) modules to address the task. (B) Components of SI module.

* _Generating future neural signals with AMAG._ We apply AMAG to four neural recording datasets and demonstrate its reliability in generating future neural activity while recovering the channel spatial proximity in the learned adjacency matrix and aligning neural trajectories in the latent space.

## 2 Related work

**Modeling Dynamics of Neural Signals.** Studying the dynamics of neural signals provides insights into the computation performed in the brain . Neural dynamics has been modeled by Gaussian Process [73; 79; 81] and linear dynamical systems [45; 46; 23; 41; 2]. More recently, a series of nonlinear models have been proposed, including Multi-Layer Perceptron (MLP) [43; 83; 7], RNNs [52; 34; 57; 32; 60], Neural ODEs , and Transformers [77; 37; 44; 47]. Latent representations of population dynamics are often extracted while these models optimize for a reconstruction objective, upon which interpretable features emerge naturally [54; 58; 4]. A recent model that could forecast future neural activities is Preferential Subspace IDentification algorithm (PSID)[56; 55]. PSID proposes to forecast future neural activity along with behavior variables in two stages. In the first stage, the model learns to predict behavior and the behaviorally-relevant portion of neural signals. In the second stage, PSID learns to forecast both behaviorally-relevant and behaviorally-irrelevant neural signals [56; 55].

**Time Series Forecasting.** Time series forecasting, i.e., the prediction of next values in a time series, is a well-studied topic that encompasses various applications, such as traffic flow forecasting, motion prediction, and electric load forecasting. Examples of proposed methods include methods that are CNN based [31; 8], RNN based [24; 13; 26], Multi-layer perceptron  and so on. Transformers with additional components for capturing long-term dependencies have also been proposed for time series forecasting [68; 16; 74]. Also, models such as Graph Neural Networks (GNNs) have been proposed for time series forecasting focusing on datasets with inherent structural organization, e.g., traffic flow [30; 77; 38]. For example, these methods have been applied to functional Magnetic Resonance Imaging (fMRI) forecasting  leveraging Diffusion Convolutional Recurrent Neural Network (DCRNN) and Graph WaveNet (GWNet) [76; 40]. In these applications, GNN has been embedded in GRU gating mechanism to learn the spatial-temporal relationship (DCRNN), or CNN and GNN have been proposed to be intertwined to perform spatial-temporal modeling (GWNet). In contrast, GraphS4mer, which combines graph neural networks and structured state space models, performs temporal embedding followed by spatial message-passing in a sequential manner . These methods perform only the additive message-passing, which may not be sufficient for a complex system such as the brain.

**GNNs for Neural Signals.** In recent years, GNNs have been proposed as plausible models for analyzing brain activity across different modalities [62; 33; 12; 29]. In non-invasive neural recordings, GNNs have been applied to EEG and fMRI data to capture the relationships between different brain regions or features. For example, LGGNet learns local-global-graph representations of EEG for various cognitive classification tasks , and BrainGNN has been proposed to identify neurological biomarkers with GNN, leveraging the topological and functional information of fMRI . Recently, the method has been extended to analyze ECoG recordings as a graph diffusion process . A few applications of GNNs to invasive neural measurements have been introduced, although these are less ubiquitous than GNNs for other recording types. The Two-Stream GNN model, which has been designed to classify anesthetized states based on ECoG recording by building two fixed graphs, is a notable example of such a model . The first graph in this model is the functional connectivity matrix computed from the phase lag index, and the second is the dual version of the first one. The Two-Stream GNN is not suitable for forecasting since it combines temporal signals into a unified vector as the model input.

In light of the related work, the proposed AMAG is a GNN based approach which explicitly models interactions between neural signals to learn the dynamic features and interactions that can generate future neural signals. This is different from earlier models of learning neural dynamics with the primary focus on reconstruction without explicit modeling neural signal interactions. AMAG incorporates novel additive and multiplicative message-passing operations, which distinguish it from methods focusing on time series forecasting to associate GNNs with neural dynamics.

Methods

Our objective is to develop a model that estimates future neural dynamics given past activities. We define the relationship between future and past as \(}_{t+1:t+}=f_{}(_{0:t})\), where \(^{T C D}\) represents neural signals recorded from \(C\) electrode channels over a time window of size \(T\), with each channel having \(D\) dimensional features. \(_{0:t}\) denotes neural signals from \(0\) to \(t\), and \(}_{t+1:t+}\) corresponds to predicted future neural signals for time window \(t+1\) to \(t+\).

To estimate \(f_{}()\), we formulate an optimization problem minimizing the loss \(\) between predicted future signals \(}_{t+1:t+}\) and ground truth future signals \(_{t+1:t+}\) as follows

\[_{f_{}}_{}(f_{}_{0:t}),_{t+1:t+}. \]

Specifically, \(f_{}()\) is constructed from three sub-modules as shown in Fig. 1: Temporal Encoding (**TE**) Module, Spatial Interaction (**SI**) Module, and Temporal Readout (**TR**) Module. The TE module captures temporal features for each channel individually. Then SI facilitates information exchange between channels, followed by TR to generate future neural activities. We describe the three modules in detail in the sections below.

### SI with _Add_ and _Modulator_

SI is constructed as a GNN. We represent a Graph \(=(,,)\), where \(\) denotes a set of nodes \(||=C\), \(\) is a set of edges, and weighted adjacency matrix \(^{C C}\) contains the weights of edges. The neighborhood of a node \(v\) is written as \((v)\), where \((v)=\{u|u V,(u,v)\}\). We denote neural signal from channel \(v\) at time \(t\) as \(X_{t}^{(v)}\). \(FC\), \(MLP\) and \(()\) in the following sections represent a one-layer neural network, multi-layer perceptron, and sigmoid function, respectively.

Considering two types of interaction between neurons, additive  and multiplicative interactions [19; 25], we design the SI module to incorporate two sub-graphs for additive and multiplicative operations, namely Add and Modulator modules, respectively. In addition, we introduce a sample-dependent matrix, called Adaptor, to adjust the Add for each input sample, accounting for the complexity and dynamic nature of the brain as a system.

**Add Module**. The Add Module performs the additive interaction between channels with the message-passing function \(_{a}()\), depending on the adjacency matrix \(_{a}^{C C}\). Assuming we have \(d\) dimensional channel features \(_{t}^{(v)}\) for node \(v\) at timestep \(t\) (\(_{t}^{(v)}^{d}\) ), \(_{a}()\) updates \(_{t}^{(v)}\) with the additive message from neighbor node \(u\) as \(_{a}(_{t}^{(u)},_{t}^{(v)},_{a})=A_{a}^{(u,v)}_{t}^{(u)}\), where \(u_{a}(v)\). Such that the updated channel feature of Add Module is the weighted feature of neighborhood,

\[_{t}^{(v)}=_{u_{a}(v)}A_{a}^{(u,v)}_{t}^{(u)}. \]

The element \((u,v)\) in the adjacency matrix \(_{a}\) indicates the influence of channel \(u\) on \(v\). Since \(_{a}\) is shared for all the input sequences, while channel interaction can change across inputs, we introduce a sample-dependent _Adaptor_ Module. We treat \(_{a}\) as a fundamental adjacency matrix, and then we further learn a sample-dependent Adaptor as a matrix \(^{C C}\) to uniquely modulate \(_{a}\) depending on each sample. Then, the shared message-passing function Eq. 3 becomes a sample-dependent message-passing function,

\[_{t}^{(v)}=_{u_{a}(v)}S^{(u,v)}A_{a}^{(u,v)}_{t}^{(u )}. \]

\(S^{(u,v)}\) represents interaction strength between channel \(u\) and \(v\) relying on temporal embeddings for each channel, e.g., \(^{(v)}=[_{1}^{(v)},,_{t}^{(v)}]\) where \(t\) is the context window for future neural signal generation. Specifically, we compute \(S^{(u,v)}\) as \((MLP([^{(u)},^{(v)}]))\), such that \(S^{(u,v)}\) ranges from 0 to 1, similar to matrix construction for learning interpretable GNNs .

Since the fundamental adjacency matrix for the Add Module is unknown, \(_{a}\) needs to be learned during the optimization of the model parameters. We observe that direct learning of \(_{a}\) from random initialization could make the training process unstable and thus instead, we initialize the \(_{a}\) with the precomputed correlation matrix from the neural recordings to stabilize the learning process.

**Modulator Module**. The Modulator Module is the multiplicative message-passing operator, with function \(_{m}()\). We incorporate the adjacency matrix \(_{m}^{C C}\) to encode the neighborhood information that performs a multiplicative operation. \(_{m}()\) indicates how the feature of the neighborhood modulates the target node feature, which can be expressed as \(_{m}(_{t}^{(u)},_{t}^{(v)},_{m})=_{m}^{(u,v )}_{t}^{(u)}_{t}^{(v)}\), where \(\) represents Hadamard product. The Modulator Module includes all the multiplicative information from the neighbors to update feature for target channel \(u\) at time \(t\) as \(_{t}^{(u)}\)

\[_{t}^{(v)}=_{u_{m}(v)}_{m}(_{t}^{(u)},_{t}^{(v)},_{m}) \]

Similarly to the adjacency matrix for the Add Module, the adjacency matrix in the Modulator Module is also trainable and is initialized from the correlation matrix.

In summary, at a specific timestep \(t\), the output of SI is

\[_{t}^{(v)}=_{1}_{t}^{(v)}+_{2}FC(_{t}^{(v)})+_ {3}FC(_{t}^{(v)}), \]

with \(_{1}\), \(_{2}\), and \(_{3}\) controlling the contributions of the self-connection, Add Module, and Modulator Module respectively.

### Temporal Processing with TE and TR Modules

TE and TR perform temporal encoding and decoding. In particular, TE embeds input neural signals \(^{(v)}^{T D}\) from channel \(v\) into the embedding space \(^{(v)}^{T d}\). For multi-step forecasting, we mask neural signals after the context window \(t\) with constant, i.e., \(_{t:T}^{(v)}=constant\). Then SI updates representation of channel \(v\) as \(^{(v)}^{T d}\), followed by TR generating future neuron activity \(}_{t:T}^{(v)}\) taking \(^{(v)}\) as inputs. TE and TR can be Transformer or GRU. For further details on TE and TR, see Appendix A.2.

**Parameter Learning**. In summary, AMAG estimates future neural signals with \(f_{}\) implementing TE, SI, and TR sequentially to the past neural signals. The optimal parameters \(\) are obtained through backpropagation minimizing the Mean Squared Error (MSE) loss,

\[=_{}}_{t:T}-_{t:T} _{2}. \]

## 4 Experiments

### Learning The Adjacency Matrix of a Synthetic Dataset

**Synthetic Dataset.** To explore the ability of AMAG to capture channel interactions, we generate synthetic datasets using linear non-Gaussian dynamic systems similar to previous works [61; 5; 6]. Specifically, with adjacency matrix \(\) describing the spatial interactions among variables (channels), datasets \(\) are generated recursively at each time step as \(X_{t}=X_{t-1}+(X_{t-1},)+\), where \(\) is uniformly distributed noise term. We consider two scenarios where the connection ratio of the adjacency matrix is 0.2 and 0.5, respectively. The resulting datasets consist of multichannel time series, where the value of each channel at a given time step depends on its past and its neighboring channels, as defined by the adjacency matrix.

**Adjacency Matrix Recovery with AMAG.** We evaluate the ability of AMAG to recover the ground truth adjacency matrix and its dependency on tasks. We do so by additionally incorporating reconstruction and masked input reconstruction tasks as supplementary training objectives together with one-step and multi-step and forecasting tasks. The learned adjacency matrices, along with the ground truth matrix for 20 channels, are visualized in Fig.2 (Additional results with different numbers of channels in Appendix B.1). To assess the accuracy of recovering the underlying ground truth adjacency matrix, we employ the F1 Score as a quantitative measure. Our results show that the one-step forecasting task achieves the highest recovery of ground truth edges (\(F1=\) at 0.5 connection ratio), followed by the multi-step forecasting task (\(F1=\)). This is compared to the reconstruction task which yields a lower F1 score of \(\). The lower performance of reconstruction tasks could be due to the model's access to future signals, which potentially alleviates the need to learn the interactions with neighbors that future signals rely on. While multi-step forecasting generatesmultiple future signals, some adjacency matrices may generate more accurate signals than others at a given moment, but considering all future moments collectively, different adjacency matrices may achieve similar performance, resulting in learned adjacency matrices being less accurate than one-step forecasting.

### Evaluation on Forecasting with Neural Signals

**Dataset and Preprocessing** In this study, we collect neural signals from monkey Affogato (A), and Beignet (B) with \(\)ECoG implanted in motor cortices, covering five different subregions (Primary Motor Cortex (M1), Premotor Cortex, Frontal Eye Field (FEF), Supplementary Motor Area (SMA), and Dorsolateral Prefrontal Cortex (DLPFC)) with 239 effective \(\)ECoG electrodes arrays which have been used in . During data collection, the subjects performed a reaching task towards one of eight different directions. All procedures were approved by the University of Washington Institutional Animal Care and Use Committee. In the following experiments, we include all electrodes from Monkey A and electrodes in M1 from Monkey B (More in the Appendix B.2). We additionally apply AMAG on two public datasets with field potential recorded with Utah arrays from the M1 of Monkey Mihili (M) and Chewie (C) where subjects performed similar hand-reaching task [22; 21]. M and C datasets contain the computed Local Motor Potential (LMP) and power band signal from eight frequency bands: 0.5-4 Hz, 4-8 Hz, 8-12 Hz, 12-25 Hz, 25-50 Hz, 50-100 Hz, 100-200 Hz, and 200-400 Hz. The resulting signal is downsampled to 30ms. Full broadband data from A and B were collected at 25 kHz. We then applied a preprocessing procedure to extract LMP and powerband signals from A and B datasets as used in M and C datasets [20; 64]. The powerband signals are included as additional feature dimensions. In experiments, we treat a target reaching trial as a sample for all four datasets. On M and C datasets, trials are temporally aligned to detect movement onset and include 300 ms before to 450 ms after movement onset. For the A and B datasets, trials are temporally aligned to the appearance of the peripheral target and include 600ms after the peripheral target onset.

**Benchmark Methods,** We compare AMAG with existing approaches, including variants of RNNs, Transformers, and GNNs. Specifically, we compare with the following methods: _LRNN_, RNN model without a non-linear activation function and gating mechanism ; _RNNf_, RNN model with non-linear activation and gating mechanism [32; 47]; _LFADS_, RNN based model with pre-computed initial states . _NDT_, a decoder based transformer in temporal domain to estimate neural firing rate ; _STNDT_, a model that is similar to NDT with additional attention based spatial interaction between neurons ; _TERN_, a model that includes the GRU for encoding and temporal Transformer for decoding [32; 47]. _RNN PSID_, RNN based model with 2-stage training for learning behavior-related neural space and behavior-unrelated neural space combined to ensure accurate

Figure 2: Colormap visualization of AMAG learned adjacency matrices arranged by F1 score (right better), with yellow indicating Ground Truth recovery while gree-blue indicating lower accuracy, as reconstruction task (first column), reconstruction of masked input task (second column), multi-step forecasting (third column) and one-step forecasting task (fourth column), alongside ground truth adjacency matrices generating synthetic data (last column), demonstrating AMAGâ€™s ability to recover the underlying spatial interactions doing forecasting tasks.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

**Investigating Adjacency Matrix Learned by AMAG.** We hypothesize that the learned adjacency matrix in AMAG indicates interaction strength between channels and channels with high interaction strength are vital in forecasting. To verify the assumption, we group the channels into High (H), Mid-high (M-H), Mid-low (M-L), and Low (L) according to connection strength in \(Aa\). We estimate their importance as the degree of performance drop when channels in each group are masked in AMAG. As the testing of the generality of the channel importance, we performed the same procedure in NDT. The forecasting performance (\(R^{2}\)) of NDT and AMAG with masked channels as input are shown in Fig. 4 (A). Generally, masking of channels consistently leads to performance drop, particularly with more significant degradation of performance in both AMAG and NDT when masking happens in the H group. This confirms our assumption that the weights learned in adjacency matrices provide informative insights into channel importance. Additionally, we investigate the relationship between learned weights and channel proximity. Figure 4 (B) illustrates the projection of the interaction strength of a target channel (T) onto other channels in the space organized by the \(\)ECoG arrangement. We observe that target channels exhibit stronger interactions with surrounding channels, which aligns with the properties of \(\)ECoG arrays. These experiments collectively demonstrate that the learned adjacency matrix values are meaningful in terms of identifying important channels, and the learned strength of connections aligns with the spatial proximity among ECoG arrays.

**Neuron Trajectory Alignment.** Apart from learning meaningful adjacency matrices, it is also advantageous to study neural trajectories AMAG has learned during the forecasting task. For that purpose, we project the original neural signals and their hidden embeddings from AMAG into the 2D space with PCA in Fig. 4(C). We find that, while neural trajectories, projected from the original neural signals, are tangled with random start and endpoints, with the projection from the hidden embeddings of AMAG, the neural trajectories are disentangled for the corresponding time-step and exhibit a circular structure similar to . This illustration indicates that AMAG learns informative features that can be visualized as coherent and interpretable neural trajectories in 2D space.

**Computation Complexity.** We additionally compare in Table 3 the number of parameters \(\#\)P(M), training time per epoch T(S), and maximum memory cost during training M(GB). We observe that GNN based methods use much fewer parameters than non-GNN methods, with AMAG consistently using a minimal or comparable (DCRNN for one-step forecasting) number of parameters when compared to graph model baselines (DCRNN, GWNet, GS4-G). This demonstrates that the source of the contribution to AMAG performance is its architecture rather than the number of parameters. The graph architecture of AMAG provides the topological constraint across channels. The absence of the topological constraint in non-GNN based methods could result in overfitting with limited training samples. Adding regularization terms, e.g., weight decay could help, but will also limit models' capacity. We demonstrate it in Fig. 3 (B). Adding weight decay to NDT improves performance (orange triangle vs purple circle), however, as the attention dimension increases, the effect of regularization diminishes, and for dim>1024, weight decay does not contribute to further improvement.

### Ablation Study

Here we examine how each component in AMAG contributes to forecasting. Specifically, we consider the following ablated versions of AMAG: (1) removing self-connection in AMAG (amAG), (2) removing both the "Add" and "Modulator" module (-AG), (3) Only sample dependent "Adaptor" is kept in "Add" Module (\(\)MAG), (4) Adjacency matrix is not learnable (AM-G), (5) The "Add" is ablated from the graph operation (-MAG), (6) "Modulator" is removed (A-AG), (7) The "Adaptor" in excluded from the "Add" (aMAG). The results are shown in Table.4.

    & **AMAG** & LRNN & LFADS & STNDT & NDT & RNNf & TERN & DCRNN & GWNet & GS4-G \\  \(\#\)P(M) & **0.27** & 4.59 & 5.53 & 2.38 & 1.06 & 3.55 & 9.93 & 0.60 & 1.12 & 1.52 \\ T(S) & **9.74** & **2.18** & **3.44** & **4.70** & **5.96** & **7.22** & **8.48** & 11.01 & 12.27 & 13.53 \\ M(GB) & **5.74** & **0.12** & **0.15** & **0.19** & **0.04** & **0.11** & **0.34** & **1.69** & **1.53** & **4.48** \\   \\ \(\#\)P(M) & **0.22** & 1.25 & 5.84 & 8.31 & 3.70 & 3.55 & 9.93 & **0.15** & 1.12 & N/A \\ T(S) & **9.90** & **1.97** & **3.29** & **4.61** & **5.94** & **7.26** & **8.58** & 11.22 & 12.54 & N/A \\ M(GB) & **5.36** & **0.04** & **0.15** & **0.43** & **0.10** & **0.11** & **0.34** & **1.05** & **1.53** & N/A \\   

Table 3: _**_Computation complexity estimation for multi-step and one-step forecasting task. Bold marks AMAG estimates and those methods whose estimates are better than AMAG._**We find that removing _Add_ or _Modulator_ modules can cause the most significant forecasting accuracy drop (-AG) as well as the self-connection part in the graph (amAG). When keeping sample-dependent Adaptor term only (\(\)MAG) in the Add module, the performance is very close to ablating the Add module (-MAG). The sample-dependent Adaptor term appears to be less important compared to other components but still reduces the accuracy if being ablated from AMAG (aMAG).

In addition, we investigate the effect of initialization (random versus correlation) when the adjacency matrices are not adaptable (AM-G), specifically, AM-G (Rand Init 1), AM-G (Rand Init 2), AM-G (Corr Init). The forecasting accuracy of AM-G compared to the other two variants demonstrates the importance of correlation initialization. In particular, random and correlation initialization achieve similar performance when the adjacency matrices are adaptable, while random initialization leads to a less stable training process, see Appendix B.4. In Fig. 5, we visualized the learned adjacency matrix with correlation initialization (top) and random initialization (bottom) for one-step (left) and multi-step forecasting (right). Globally, the learned adjacency matrices show similar connection patterns, while locally, the connection can be different with different initialization matrices. This indicates that regardless of the initialization method, AMAG can converge to graph patterns with globally similar connections while multiple local connections are available to achieve similar forecasting performance.

## 5 Discussion

In conclusion, we propose AMAG for modeling spatial-temporal interaction between neural signals via forecasting future neuron activity. Experiments with both synthetic and neural signals show potential for discovering underlying neural signal interactions. The learned neural features by AMAG appear to disentangle in the PCA space. While adjacency matrix recovery is promising as training AMAG with one-step forecasting, more animal experiments could be done together with AMAG to understand the biological meaning of the learned adjacency matrix. It is worth noting that the memory requirement for GNN training increases as the number of neural recording channels increases. For future development, improvements to adapt the model for multi-region, multi-channel recordings could be considered.

    & \(R^{2}\) & \(CORR\) & \(MSE\) \\ -AG & 0.424\(\)1e-3 & 0.650\(\)1e-3 & 0.0456\(\)1e-4 \\ amAG & 0.427\(\)6e-3 & 0.652\(\)4e-3 & 0.0453\(\)5e-4 \\ -MAG & 0.647\(\)2e-3 & 0.805\(\)6e-4 & 0.0274\(\)2e-4 \\ \(\)MAG & 0.648\(\)1e-3 & 0.806\(\)4e-4 & 0.0273\(\)6e-5 \\ -A-AG & 0.652\(\)9e-4 & 0.807\(\)3e-4 & 0.0268\(\)1e-4 \\ aMAG & 0.655\(\)2e-3 & 0.810\(\)1e-3 & 0.0269\(\)2e-4 \\  AM-G Rand Init) & 0.575\(\)2e-2 & 0.767\(\)7e-3 & 0.0329\(\)1e-3 \\ AM-G (Corr Init) & 0.617\(\)2e-4 & 0.786\(\)7e-4 & 0.0296\(\)2e-5 \\  AMAG (Rand Init) & 0.652\(\)1e-3 & 0.807\(\)8e-4 & 0.0270\(\)1e-4 \\ AMAG (Corr Init) & **0.657\(\)2e-3** & **0.811\(\)2e-3** & **0.0266\(\)2e-4** \\   

Table 4: _Comparison between the Ablated versions of AMAG._

Figure 5: Visualization of the adjacency matrix of Add module (\(A_{a}\)) and Modulator module (\(A_{m}\)) learned by AMAG performing one-step forecasting (left) and multi-step forecasting (right) with correlation initialization (top) and random initialization (bottom).

Acknowledgements

Authors acknowledge the support in part by A3D3 National Science Foundation grant OAC-2117997 (ES,JL,TL,ALO), the Simons Foundation (LS,PR,ALO), the Washington Research Foundation Fund (ES), Departments of Electrical Computer Engineering (ES,JL,TL), Applied Mathematics (ES). Authors are thankful to the Center of Computational Neuroscience, and the eScience Center at the University of Washington. Authors would like to thank Rahul Biswas for discussions of synthetic dataset generation, Pan Li for discussions regarding GNNs, and the anonymous reviewers for insightful feedback.

## 7 Supplementary Information and Data

Supplementary information is available in the Appendix. Additional information and data will be made available through the project page [https://github.com/shlizee/AMAG](https://github.com/shlizee/AMAG).