# Algorithmic Collective Action in Recommender Systems: Promoting Songs by Reordering Playlists

Joachim Baumann

University of Zurich

baumann@ifi.uzh.ch

&Celestine Mendler-Dunner

ELLIS Institute, Tubingen

MPI for Intelligent Systems, Tubingen

Tubingen AI Center

celestine@tue.ellis.eu

Work completed while at the Max-Planck Institute for Intelligent Systems, Tubingen.

###### Abstract

We investigate algorithmic collective action in transformer-based recommender systems. Our use case is a collective of fans aiming to promote the visibility of an underrepresented artist by strategically placing one of their songs in the existing playlists they control. We introduce two easily implementable strategies to select the position at which to insert the song and boost recommendations at test time. The strategies exploit statistical properties of the learner to leverage discontinuities in the recommendations, and the long-tail nature of song distributions. We evaluate the efficacy of our strategies using a publicly available recommender system model released by a major music streaming platform. Our findings reveal that even small collectives (controlling less than 0.01% of the training data) can achieve up to \(40\) more test time recommendations than songs with similar training set occurrences, on average. Focusing on the externalities of the strategy, we find that the recommendations of other songs are largely preserved, and the newly gained recommendations are distributed across various artists. Together, our findings demonstrate how carefully designed collective action strategies can be effective while not necessarily being adversarial.

## 1 Introduction

In the ever-evolving landscape of music discovery, the challenge of accessing and sifting through the overwhelming number of tracks released daily has become increasingly difficult. This has resulted in a strong dependence on platforms like Spotify, Deezer, and Apple Music, which distribute and promote music through algorithmic song recommendations. These systems rely on historical data to learn user preferences and predict future content consumption .

It has been widely documented that music recommendation systems suffer from popularity bias as they tend to concentrate recommendation exposure on a limited fraction of artists, often overlooking new and emerging talent . As the success and visibility of artists are deeply influenced by the algorithms of these platforms, this can lead to a considerable imbalance in the music industry  and reinforce existing inequalities . Thus, artists have started to fight for more transparency and fairer payments from online streaming services. The "Justice at Spotify" campaign, launched by the Union of Musicians and Allied Workers , has been signed by more than 28,000 artists. At the same time, the International Society for Music Information Retrieval has been arguing for promoting the discovery of less popular artists by recommending 'long-tail' items , as have other researchers .

In this work, we explore algorithmic collective action as an alternative means for emerging artists to gain exposure in machine learning-powered recommender systems by mobilizing their fan base. Algorithmic collective action  refers to the coordinated effort of a group of platform participants who strategically report the part of the training data they control to influence prediction outcomes. Our work is situated in an emerging literature that recognizes data as a powerful lever for users to promote their interests on digital platforms [57; 24].

### Our work

We study algorithmic collective action in transformer-based recommender systems. As a case study, we consider the task of automatic playlist continuation (APC), which is at the heart of many music streaming services. APC models take a seed playlist (an ordered list of unique songs) as input and recommend songs to follow. They are trained on the universe of playlists stored on the platform. The collective consists of platform users who can modify the subset of playlists they own. The goal of the collective is to promote a less popular artist by increasing the recommendations of their songs at test time. To this end, we consider collective action strategies where participants of the collective agree on a target song \(s^{*}\) to strategically place in their playlists.

We motivate and discuss two strategies to choose the position of \(s^{*}\) within any given playlist. Both strategies are derived from a statistical optimality assumption on the recommender and do not require knowledge of the specifics of the model architecture or the model weights. Instead, they use that the model is trained to fit sequential patterns in existing data and build on aggregate song statistics that are feasible to gather from public information. We empirically test our strategies using an industry-scale APC model that has been deployed to provide recommendations for millions of users on Deezer--one of the biggest streaming platforms in the world. To train the model, we use the Spotify Million Playlist Dataset, treating each playlist as a user and randomly sampling a fraction to compose the collective.

We find that by strategically choosing the position of the target song, collectives can achieve significant over-representation at test time, see Figure 1 for a teaser. We experiment with collectives composed of a random sample of users owning between \(0.001\%\) and \(2\%\) of the training data instances. Interestingly, even tiny user collectives, controlling as few as \(60\) playlists, can achieve an amplification of up to \(25\), referring to the song's recommendation frequency relative to the training frequency. This is \(40\) more than an average song occurring at the same frequency in the training data. In contrast, placing the song in a fixed position in every playlist is largely ineffective.

Our strategy satisfies a strict authenticity constraint and thus preserves user experience at training time by design. Interestingly, we find that also at test time recommendations are largely preserved; not only on aggregate but also for members of the collective. As a consequence, the strategies come with small externalities for users, and at the same time, they also have a relatively small effect on model performance. For large collectives controlling \(>3\%\) of the playlists, the effect cor

Figure 1: By strategically choosing the position at which to insert the target song in a playlist, collectives can achieve a disproportionally high recommendation frequency relative to training set occurrences, compared to naturally occurring songs. Amplification of one corresponds to matching frequencies in train and test.

song recommendation replacing an otherwise relevant song in less than \(15\%\) of the cases, leaving other recommendations unaltered. Thus, in the hypothetical case where the promoted song is indeed relevant, this could lead to an overall gain in more than \(85\%\) of the cases, even though the total number of test-time recommendations is fixed. Lastly, we show that the newly gained recommendations are taken from artists of diverse popularity without any indication that a specific artist suffers disproportionally.

Taken together, our work demonstrates a first example of collective action in sequential recommender systems. We show how collective action goals can be achieved while largely preserving service quality and user experience. The feasibility of such strategies raises many interesting questions, challenges, and opportunities for future work.

### Related work

The fairness of recommendation systems on online platforms remains a pressing issue for both content consumers and producers [10; 32; 19; 26]--see Zehlike et al.  for a detailed overview. Several recent works study individual strategic users attempting to influence their own recommendations [5; 25; 12; 13]. Other works consider adding antidote data to fight polarization and unfairness [42; 18].

Beyond recommender systems, a related line of work centers the users in the study of machine learning systems. Vincent and Hecht  call for _conscious data contribution_, Vincent et al.  discuss data strikes, and Vincent et al.  emphasize the potential of data levers as a means to gain back power over platforms. Hardt et al.  introduce the framework of _algorithmic collective action_ for formally studying coordinated strategies of users against algorithmic systems. They empirically demonstrate the effectiveness of collective action in correlating a signal function with a target label. Sigg et al.  inspect collective action at inference time in combinatorial systems. Complementing these findings, we demonstrate that collective action can be effective even without control over samples at inference time. We highlight a so far understudied dimension of algorithmic collective action by discussing and illuminating the externalities of algorithmic collective action strategies.

At a technical level, our findings most closely relate to _shilling attacks_, or more broadly, _data poisoning attacks_ [c.f., 49]. Shilling attacks are usually realized by injecting fake user profiles and ratings in order to push the predictions of some targeted items [45; 47]. Due to the fraudulent nature of these attacks, there are little design restrictions on the profiles, and they often come with considerable negative effects for the firm [38; 20]. Data poisoning attacks in recommender systems predominantly focus on collaborative filtering-based models, with a few exceptions; Zhang et al.  propose a reinforcement learning-based framework to promote a target item, Yue et al.  provide a solution to extract a black-box model's weights through API queries to then generate fake users for promoting an item, and Yue et al.  propose injecting fake items into seemingly real item sequences (at inference time and without retraining) with a gradient-guided algorithm, requiring full access to the model weights. Taking the perspective of collective action, we focus on easy-to-implement strategies that require minimal knowledge of the model and operate under an authenticity constraint to preserve the utility of altered playlists while seamlessly integrating into natural interaction with the platform.

Further, our work pertains to a broader scholarly literature interested in improving labor conditions for gig workers on digital platforms [e.g., 29; 52], optimizing long-term social welfare in online systems , and understanding dynamics in digital marketplaces . The type of strategic data modification we consider falls under the umbrella of _adversarial feature feedback loops_. Taking advantage of collective strategies to change model outcomes more broadly has been studied in tabular data , computer vision , and recently in generative AI .

## 2 Preliminaries on automatic playlist continuation

We use automatic playlist continuation (APC) as a running example of a sequential recommendation task. APC forms the backbone of major streaming platforms, such as Spotify and Deezer. To formally define the recommendation task, let \(=\{s_{1},...,s_{n}\}\) denote the universe of songs, where \(n 1\) denotes the number of unique songs. A playlist \(p\) is composed of an ordered list of songs selected from \(\) without replacement. Given a seed playlist \(p\), the firm's goal is to predict follow-up songs that the user likely listens to. We consider a top-\(K\) recommender system that outputs a personalized ordered list of \(K 1\) songs. We write \(_{K}(p)\) for the set of \(K\) songs recommended for a seed playlist \(p\).

### Transformer-based recommender

Over the past years, most large platforms have shifted from relying on collaborative filtering-based models for APC to building deep learning-based recommenders that account for sequential and session-based information [21; 51; 34]. In this work, we focus on transformer-based recommender systems that posit the following structure: Each song \(s\) is mapped to a song embedding vector \(h_{s}=(s)\), where \(\) denotes the embedding function. Each playlist \(p=[s_{1},s_{2},...,s_{L}]\) is mapped to an embedding vector \(h_{p}\) by aggregating the embeddings of the songs contained in the playlist as \(h_{p}=g(h_{s_{1}},h_{s_{2}},...,h_{s_{L}})\), where \(g\) is a sequence-aware attention function. We assume all playlists have length \(L\) smaller than the attention window for the purpose of exposition. At inference time, the recommendation of the next \(K\) songs for a given playlist \(p\) is determined by evaluating the similarity between the playlist seed \(p\) and all potential follow up songs \(s p\) as

\[(s,p):= h_{s},h_{p}.\] (1)

Then, the \(K\) songs with the largest similarity value are recommended in descending order of similarity. We denote the set of recommended songs as

\[_{K}(p)=*{arg\,max}_{S^{} [p;^{}]=K}\;_{s S^{}}(s,p).\] (2)

The embeddings \(\) and the attention function \(g\) are parameterized by neural networks. They are trained from existing user-generated playlists in a self-supervised manner by repeatedly splitting playlists into a seed context and a target sequence and employing a contrastive loss function for training.

Statistical abstraction.We do not assume the collective has knowledge of the parameters of either \(\) or \(g\). Instead, the design of the strategy builds on the assumption that sequential, transformer-based models are trained such that \((s,p)\) is large for songs \(s\) that frequently follow context \(p\) in the training data, and small otherwise. This approximately is robust to nuances in hyperparameter choices or architecture design and applies to any sufficiently expressive and well trained model.

### Typical imbalances in recommendations

On today's music streaming platforms, a small number of artists receive the vast majority of recommendations, while the majority receive few or none. This imbalance is illustrated by the Lorenz curve in Figure 2, which is based on recommendations derived from the Deezer model on the Spotify MPD dataset (see Section 4). The Gini coefficient measuring inequality corresponds to \(0.87\). Streaming and radio statistics reveal an even more severe imbalance: the top 1% of newly released songs receive 99.99% of radio plays and 90% of streams go to just 1% of artists .

Considering Figure 1 we can also see that songs with high prevalence in the training data are recommended disproportionately often at test time compared to their training set frequency (referring to the slope of \(\)\(1.8\) of the blue point cloud). This gain in exposure through the recommender comes at the expense of many low-frequency songs that receive no reco

Figure 2: **Imbalance in recommendation distribution**. (left) The Lorenz curve shows that 80% of all recommendations are concentrated among just 10% of artists. (right) The Spotify track frequency distribution shows the long tail of song frequencies in user-generated playlists: close to 50% of tracks in playlists occur only once.

further amplifying existing imbalances. Considering Spotify's substantial power to influence song consumption among platform users , withholding initial exposure for these songs limits their potential to reach a broader audience, significantly impacting an artist's career. In this work, we focus on collective efforts to boost recommendations for one of these underrepresented songs.

## 3 Algorithmic collective action for promoting songs

We build on the framework of Hardt et al.  and consider collectives that are composed of a fraction \((0,1]\) of randomly sampled users on the platform. We assume each user controls a single playlist. Members of the collective can strategically manipulate their playlists. We use \(()\) to describe the strategy of mapping an original playlist to a modified playlist.

Success and amplification.Let \(_{0}\) denote the distribution over playlists. The goal of the collective is to increase the exposure of a target song \(s^{*}\) for a randomly sampled playlist from \(_{0}\) at test time. We measure the success of collective action as

\[S():=_{p_{0}}\;1[s^{*}_{K}( p)].\] (3)

The recommender system \(_{K}\) is trained on a partially manipulated training dataset \(\), composed of \(N\) samples from \(_{0}\), among which \( N\) have been transformed under \(\).

We are particularly interested in measuring the effectiveness of a strategy relative to the effort of the collective. Therefore, we define _amplification_ (Amp) as the fraction of newly gained target recommendations at test time divided by the fraction of manipulated playlists in the training set:

\[()=(S()-S(0))\] (4)

An amplification of \(0\) means that the strategy is ineffective, an amplification of \(1\) means that the song frequency in the training set is proportionally represented in the model's predictions, and an amplification larger than \(1\) means that collective action achieves a disproportionate influence on the recommender. In the following, we choose a song \(s^{*}\) that does not currently appear in the training data, hence \(S(0)=0\).

### Authenticity constraint

Participants of the collective are users of the platform. We design collective action strategies under the following authenticity constraint, not to compromise user experience:

**Definition 1** (Authenticity constraint).: We say a strategy \(:p p^{}\) is authentic iff the Levenshtein distance between \(p\) and \(p^{}=(p)\) satisfies \((p,p^{}) 1\) for any \(p\).

The Levhenstein distance , also known as edit distance in information theory, counts the number of operations needed to transform one sequence into another. The song insertion strategy we propose in this work is one concrete instantiation of \(\) that satisfies this constraint. More specifically, our strategy consists of inserting an agreed-upon target song \(s^{*}\) at a specific position in every playlist \(p\). In contrast, existing adversarial strategies typically perform larger modifications to playlists and would not satisfy this constraint [62; 58].

### Algorithmic lever

The algorithmic lever of the collective is to strategically choose, for each playlist, the position \(i^{*}\) at which to insert the target song. Under our probabilistic assumption about the learner, strategically placing \(s^{*}\) after \(p\) means that the similarity between \(p\) and \(s^{*}\) is increased. Thus, by choosing the index \(i^{*}\), the collective targets context \(p^{-}_{i^{*}}\) referring to the sequence of songs \(s_{j}\) in \(p\) up to index \(j=i\). Recall that the collective aims to be among the top \(K\) songs with high frequency over a randomly sampled context \(p\) at test time. Our strategies exploit two different algorithmic levers towards this goal: Indirectly targeting Clusters of similar contexts (InClust) or Directly targeting Low-Frequency contexts (DirLoF). Pseudocode for the two strategies can be found in Figure 3.

Concentrating effort.Inclusion in the set \(_{K}(p)\) leads to a song's recommendation for context \(p\) at test time. In turn, being ranked in position \(K+1\) does not yield any recommendations. Instead, the probability mass in the tails is reallocated to the top \(K\) songs at test time. This discontinuity can be exploited by the members of the collective to target specific contexts in a coordinated fashion to increase the likelihood of inclusion. Compared to random song placement, the collective can increase the mass on a particular context by a factor of \(L\). The InClust strategy implements a way for selecting contexts to target, projecting this intuition from the non-parametric setting to the embedding space of the recommender. Namely, it systematically places \(s^{*}\) directly _before_ each occurrence of a popular song \(s_{0}\). In that way, it targets the region in the context embedding space around \(h_{s_{0}}\) in a coordinated fashion. To implement this strategy, the collective repeatedly determines the most frequent song in their playlists, places \(s^{*}\) before every occurrence of this song, and then repeats this with the remaining playlists until all of them are used.

Strategically exploiting overrepresentation.An alternative lever the collective has available is to strategically target contexts that are overrepresented among the playlists the collective controls. Meaning that the frequency of the context among the playlists owned by the collective is larger than the overall frequency in the training data due to finite sample artifacts. The DirLoF strategy aims to identify such contexts by targeting infrequently occurring songs and exploiting the long-tail nature of the overall song frequency distribution (see Figure 2). The core intuition is that if they manage to target low-frequency contexts, a single song placement might be sufficient to overpower existing signals. To identify low-frequency contexts, the collective uses the frequency of the last song as a proxy. For each playlist, it selects the anchor songs \(s_{0}\) with the smallest overall song frequency and places \(s^{*}\) right after \(s_{0}\).

### Obtaining song statistics

The InClust strategy targets high-frequency contexts, whereas the DirLoF strategy targets low-frequency contexts. However, there is an important difference when implementing the strategies. InClust can be implemented from only statistics obtained from the songs in playlists the collective owns; all it requires is participants to set up infrastructure for pooling this information, either through an app, an online service, or other means. In contrast, to effectively implement the DirLoF strategy, the collective needs statistics about the full training data to identify the songs that are least popular overall. However, they typically do not have direct access to this information. Instead, as a proxy, they can leverage publicly available user-generated playlists, which are often accessible through official APIs (e.g., Spotify). Additionally, scraping external data sources can provide supplementary information. We evaluate the use of scraped song streams in Section 4.2.

Figure 3: Song insertion strategies, pseudocode and illustration.

## 4 Empirical evaluation

We evaluate our collective action strategies against a public version of Deezer's transformers-based APC solution that "has been deployed to all users" [7, p. 472]. To train the model, we use the Spotify Million Playlist Dataset (MPD), which is currently the largest public dataset for APC . It contains one million playlists generated by US Spotify users between 2010 and 2017, with an average length of 66.35 tracks from nearly 300,000 unique artists.

Model training and evaluation.We use the standard methodologies used in APC for model training and testing.2 We start by randomly selecting 20,000 playlists to build a test and validation set of equal sizes. The remaining 980,000 playlists are used for training the model. The collective intervenes by strategically modifying an \(\) fraction of the playlists composing the training and validation set. We consider collectives of size \([0.00001,0.02]\) which corresponds to \(10\) to \(20000\) playlists. For evaluation on the test set, every playlist \(p\) is split into a seed context and a masked end. The length of the seed context is chosen randomly in \(\) for each playlist and models are evaluated by comparing the model's recommendations based on the seed playlist to the masked ground truth. We employ five-fold cross-validation, using different random seeds for sampling the playlists designated for training, validation, and testing, as well as for selecting the subset controlled by the collective. We use bootstrapped 95% confidence intervals (CI) over folds when reporting results.

Baselines.We consider four baseline strategies to compare with our collective strategies, each performing the same number of target song insertions. The Random strategy inserts \(s^{*}\) at a Random position in the playlist, Insert@\(i\) inserts the song always at position \(i\) in every playlist, the AtTheEnd strategy places \(s^{*}\) as the last song of the playlist, and Random@\(i\)-\(j\) inserts \(s^{*}\) at a random position between indices \(i\) and \(j\). Unlike our collective strategies, these baselines do not require coordination among participants beyond the shared goal of promoting \(s^{*}\).

### Success of collective action

We start by evaluating the success of the proposed strategies, assuming full information about song frequencies in the training set to illustrate the potential. In Figure 4, we plot the amplification for different \(\). In particular, we observe that strategic song placement allows very small collectives (\( 0.1\%\)) to be successful, whereas Random or fixed placement of \(s^{*}\) is ineffective.

For \(=0.025\%\), the DirLoF strategy achieves amplification of up to \(25\). In contrast to an average song that naturally occurs in \(0.025\%\) of the playlists, the number of recommendations is \(40\) larger, as these low-frequency songs are typically ignored by the recommender. This suggests that collective action could make a tremendous difference for these artists: suppose an artist's song is streamed

Figure 4: **Success of our collective action strategies. For tiny collectives DirLoF achieves an amplification of up to \(25\) while uncoordinated strategies (Random, AtTheEnd) are mostly ineffective. For larger collectives, InClust outperforms DirLoF. Amplification significantly exceeds \(1\), implying a disproportional test-time effect due to targeted song placement.**10,000 times, yielding a revenue of \(\$40\) at a royalty rate of \(\$0.004\) per stream ; an amplification of \(25\) would hypothetically increase this revenue to \(\$1,000\). While this example is purely illustrative (as actual royalties depend on the platform and payment model used), it emphasizes the link between recommendations and potential revenue.

For collective sizes of \( 0.1\%\) the InClust starts being effective, as it has enough mass to effectively compete with existing signals associated with a cluster of similar context embeddings. As the strategy can target several such clusters at the same time, amplification increases with \(\) though with diminishing returns, achieving \(=10\) for \( 2\%\). From Figure 1, we can see that in the regime of \(2\%\) training data frequency, a typical song enjoys an amplification of \(1.8\).

We also observe that the success of the random strategy increases with the collective size. This implies that even minimal coordination, in which members agree to all insert the same song \(s^{*}\), _independent_ of the playlist they own, can already lead to significant amplification. Amplification values for the other baselines inserting \(s^{*}\) at a fixed position are all close to 0 (see Table 1 in Appendix C.4).

Robustness to hyperparameters.Our strategies are designed based on a statistical intuition of sequential generation and should not be sensitive to specifics of the model architecture. We demonstrate the robustness with additional experiments where we vary the hyperparameters of the model (see Table 2 in Appendix C.5). However, the design of our strategies relies on the assumption that the model approximates the conditional probabilities in the training data sufficiently well. Accordingly, the effectiveness of the strategy decreases if model training is stopped early (see Table 3).

Hybrid strategy.Building on these observations, we construct a hybrid strategy that interleaves the two approaches by first using InClust to target indirect anchors that appear at least \(\) times in the collective and then deviates to DirLoF for playlists where no such anchor is present (we use \(=10\)). This corresponds to the dashed line in Figure 4. We come back to this strategy in Section 4.3.

### DirLoF strategy with approximate song statistics

The DirLoF strategy critically relies on training data song frequency estimates to determine the low-frequency anchor songs. We investigate the strategy's success with partially available song information in Figure 5. We find that if a collective of size \(=1\%\) has access to \(1\%\) of the remaining training data they do not control, they can already achieve \( 30\%\) of the amplification in the full information setting, with \(10\%\) of the data, it is \(>50\%\) of the achievable amplification.

By default, user-generated playlists on streaming platforms are often publicly accessible, enabling researchers to gather song frequency data through API calls. However, the amount of training data that can be aggregated is limited by the platform's API rate limits. Alternatively, proxy statistics can be used to increase the fraction of songs for which estimates are available. To illustrate the feasibility of this approach, we implemented a scraper to obtain current stream counts from Spotify. Although these counts are visible in the Spotify browser version, they are not accessible through the Spotify API. The scraped data reflects song popularity as of 2024, which is not ideal given our experiment relies on the much older Spotify MPD dataset (collected between 2010 and 2017). Nonetheless, these counts serve as effective proxies, as Figure 5 impressively shows.

Figure 5: **Information bottleneck. The empirical amplification of the DirLoF strategy decreases with worse song statistics but scraped song streaming counts can serve as a practical solution.**

Despite the temporal gap, a collective of size \(=1\%\) can achieve over \(85\%\) of the amplification achievable in a full-information setting simply by using 2024 stream data to approximate past popularity levels. Even a smaller collective of \(=0.1\%\) can reach about \(50\%\) of the amplification seen in the full-information scenario. In practice, scraped stream counts are likely to be more accurate proxies, as models are typically trained on more recent data. However, within the scope of our study, it remains impossible to access historical stream counts that would reflect popularity as of the time the playlists were originally generated. Thus our proof of concept should be seen as a lower bound.

### Internalities and externalities of algorithmic collective action

We now inspect the effect of our strategies on other participants in the system, including the firm, other artists, and the members of the collective. For this investigation, we focus on the hybrid strategy.

First, we gauge the impact of collective action on the firm. This helps us understand the overall quality degradation of the service and the incentives of the firm to protect against collective action. We compare the performance under a recommender trained on the clean data and a recommender trained on the manipulated data. Figure 6 shows the corresponding loss in performance due to collective action for three different evaluation metrics. We find that our strategy (solid lines) only affects the recommender's performance marginally. We also show a conservative adversarial baseline (dashed lines), which simulates a scenario where successful collective action results in the first relevant item in playlist recommendations being replaced by the target song while leaving other recommendations unaltered. The considerably larger performance loss of this baseline indicates that our strategy only rarely affects relevant songs. Finally, as a thought experiment, consider \(s^{*}\) as a relevant recommendation (dotted lines). Then, collective action even enhances the system's performance. This reference is meant to illustrate an optimistic scenario where collective action helps the recommender detect underrepresented but emerging and popular artists.

Second, we inspect the effect of collective action on other artists. To this end, Figure 7 depicts the change in recommendations for individual songs of different popularity. Songs are binned by frequency and the bars indicate variation across songs. The star shows the target song \(s^{*}\), and the corresponding increase in recommendations. We see that recommendations replaced by the target song seem to span songs of all popularity levels. In particular, our strategy does not harm specific songs or artists disproportionally and, as intended, has by far the largest effect on the targeted song \(s^{*}\).

Finally, we focus on the experience for participants who listen to the playlists. At training time our strategies are designed to only ask for minimal modifications with the goal to preserve user experience for members of the collective. We envision this to be an important factor for incentivizing participation in practice. Non-participating individuals are not affected at this stage. At test time, we find that user recommendations are largely preserved for both participating and non-participating individuals. More precisely, participating in collective action does not deteriorate the fraction of relevant songs participants get recommended, i.e., performance remains equivalent across all three recommendation quality metrics (see Figure 13 in Appendix C.6).

Figure 6: Effect of algorithmic collective action on recommendation performance. Performance loss relative to training on clean data for the hybrid/random strategies (solid lines), a conservative adversarial baseline (dashed lines), and an optimistic scenario where \(s^{*}\) is treated as relevant (dotted lines).

## 5 Conclusion

This work studies how collective action strategies can empower participants to exert a targeted influence on platform-deployed algorithms. By experimenting with an industry-scale transformer-based APC model, we demonstrate how strategically inserting a single song within randomly sampled playlists in the training data, can effectively increase recommendations of that song. Intriguingly, we find that the strategy only minimally interferes with service quality, and the recommendations for other users on the platform are largely preserved.

The proposed concept of participating in collective action to steer recommender system outcomes is grounded in the idea that users on online platforms should leave their digital traces more consciously. Thereby, their consumption behavior functions as a lever to reclaim some control over the data that platforms use to predict and recommend future content. Our emphasis on authenticity stands in clear contrast to adversarial machine learning techniques, which are often artificially designed and sometimes malicious in intent.

While altering a single playlist alone has little impact, the true power of algorithmic collective action lies in mobilizing a sufficiently large number of participants around a shared objective. This allows underrepresented artists to gain visibility through coordination. In our case, coordination corresponds to agreeing on a target song and an insertion procedure. The actual implementation of the strategy is possible with very limited technical skills and knowledge of the algorithm. We demonstrate how information for setting the parameters of the strategy can effectively be gathered using web scraping techniques. What we leave for future work is the actual implementation of an app to orchestrate collective action and share all the relevant information with the participants.

Our work suggests a widely unexplored design space for effective collective action strategies that differ from typical adversarial data poisoning attacks [c.f. 49; 62; 58; 59]. They can offer a powerful data lever to counter existing power imbalances [56; 57], and a community-centric approach to participatory AI . Thus, understanding the role of economic power [23; 22], formalizing incentives , as well as quantifying long-term payoffs, dynamics, and equilibria, under collective action promises to be a fruitful direction for future work.

## 6 Limitations and potential for misuse

Grounding algorithmic collective action means identifying both its opportunities and challenges. The power that arises from gaining control over the learning algorithm through collective action can also be abused by individuals controlling a substantial number of playlists. Instead of collective goals, these individuals could leverage similar methods to pursue individualistic goals, creating a different incentive structure and potentially posing a risk to the system. Similarly, popular artists could use our strategy to gain additional exposure and reinforce inequalities among artists. Thus, incentive structures will crucially determine the desirability of the resulting market outcome. Designing larger-scale collective action strategies that promote fairness and equity on online platforms as well as mechanisms that disincentivize malicious use remains a crucial open question.

Figure 7: Impact of collective action on other songs. We use \(=1\%\) to obtain an upper bound on the effect. \( R\) denotes the change in the number of recommendations for a song due to collective action. Songs are sorted by their training set frequency and aggregated into 50 evenly spaced bins, whose means are represented by the blue dots with 95% CI.