# On the Overlooked Pitfalls of Weight Decay and How to Mitigate Them: A Gradient-Norm Perspective

Zeke Xie

Zhiqiang Xu

MBZUAI

Jingzhao Zhang

Tsinghua University

Issei Sato

The University of Tokyo

Masashi Sugiyama

###### Abstract

Weight decay is a simple yet powerful regularization technique that has been very widely used in training of deep neural networks (DNNs). While weight decay has attracted much attention, previous studies fail to discover some overlooked pitfalls on large gradient norms resulted by weight decay. In this paper, we discover that, weight decay can unfortunately lead to large gradient norms at the final phase (or the terminated solution) of training, which often indicates bad convergence and poor generalization. To mitigate the gradient-norm-centered pitfalls, we present the first practical scheduler for weight decay, called the _Scheduled Weight Decay_ (SWD) method that can dynamically adjust the weight decay strength according to the gradient norm and significantly penalize large gradient norms during training. Our experiments also support that SWD indeed mitigates large gradient norms and often significantly outperforms the conventional constant weight decay strategy for _Adaptive Moment Estimation_ (Adam).

## 1 Introduction

Deep learning  has achieved great success in numerous fields over the last decade. Weight decay is the most popular regularization technique for training deep neural networks that generalize well . In deep learning, there exist two types of "weight decay": \(L_{2}\) regularization and weight decay.

People commonly use the first type of \(L_{2}\) regularization for training of deep neural networks as \(_{t}=_{t-1}-_{t}(L(_{t-1})+_{L_{2}}\| _{t-1}\|^{2})\), where \(\) is the gradient operator, \(\|\|\) is the \(L_{2}\)-norm, \(t\) is the index of iterations, \(\) denotes the model weights, \(_{t}\) is the learning rate, \(L()\) is the training loss, and \(_{L_{2}}\) is the \(L_{2}\) regularization hyperparameter. In principle, the \(L_{2}\) regularization strength is chosen to be constant during training of DNNs and people do not need to schedule \(L_{2}\) regularization like learning rate decay. The second type of weight decay was first proposed by Hanson and Pratt  to directly regularize the weight norm:

\[_{t}=(1-^{})_{t-1}-_{t})}{},\] (1)

where \(^{}\) is the (vanilla) weight decay hyperparameter.

While the two types of "weight decay" seem similar to Stochastic Gradient Descent (SGD), Loshchilov and Hutter  reported that they can make huge differences in adaptive gradient methods, such as Adam . Adam often generalizes worse and finds sharper minima than SGD  for popular convolutional neural networks (CNNs), where the flat minima have been argued to be closely related with good generalization . Understanding and bridging the generalizationgap between SGD and Adam have been a hot topic recently. Loshchilov and Hutter  argued that directly regularizing the weight norm is more helpful for boosting Adam than \(L_{2}\) regularization, and proposed Decoupled Weight Decay as

\[_{t}=(1-_{t}_{W})_{t-1}-_{t})}{},\] (2)

where \(_{W}\) is the decoupled weight decay hyperparameter. We display Adam with \(L_{2}\) regularization and Adam with Decoupled Weight Decay (AdamW) in Algorithm 1. To avoid abusing notations, we strictly distinguish \(L_{2}\) regularization and weight decay in the following discussion.

While fine analysis of weight decay has attracted much attention recently , some pitfalls of weight decay have still been largely overlooked. This paper analyzes the currently overlooked pitfalls of (the second-type) weight decay and studies how to mitigate them by a gradient-norm-aware scheduler. The main contributions can be summarized as follows.

First, supported by theoretical and empirical evidence, we discover that, weight decay can lead to large gradient norms especially at the final phase of training. The large gradient norm problem is significant in the presence of scheduled or adaptive learning rates, such as Adam. From the perspective of optimization, a large gradient norm at the final phase of training often leads to poor convergence. From the perspective of generalization, penalizing the gradient norm is regarded as beneficial regularization and may improve generalization . However, previous studies fail to discover that the pitfalls of weight decay may sometimes significantly hurt convergence and generalization.

Second, to the best of our knowledge, we are the first to demonstrate the effectiveness of scheduled weight decay for mitigating large gradient norms and boosting performance. Inspired by the gradient-norm generalization measure and the stability analysis of stationary points, we show that weight decay should be scheduled with the gradient norm for adaptive gradient methods. We design a gradient-norm-aware scheduler for weight decay, called Scheduled Weight Decay (SWD), which significantly boosts the performance of weight decay and bridges the generalization gap between Adam and SGD. Adam with SWD (AdamS) is displayed in Algorithm 2. The empirical results in Table 1 using various DNNs support that SWD often significantly surpasses unscheduled weight decay for Adam.

``` \(g_{t}= L(_{t-1})+_{t-1}\); \(m_{t}=_{1}m_{t-1}+(1-_{1})g_{t}\); \(v_{t}=_{2}v_{t-1}+(1-_{2})g_{t}^{2}\); \(_{t}=}{1-_{1}^{2}}\); \(_{t}=}{1-_{1}^{2}}\); \(_{t}=_{t-1}-_{t}+}}_{t}- _{t-1}\); ```

**Algorithm 1**Adam/AdamW

**Structure of this paper.** In Section 2, we analyze the overlooked pitfalls of weight decay, namely large gradient norms. In Section 3, we discuss why large gradient norms are undesired. In Section 4, we propose SWD to mitigate the large gradient norm and boosting performance of Adam. In Section 5, we empirically demonstrate the effectiveness of SWD. In Section 6, we conclude our work.

   \(g_{t}= L(_{t-1})+_{t-1}\); \\ \(m_{t}=_{1}m_{t-1}+(1-_{1})g_{t}\); \\ \(v_{t}=_{2}v_{t-1}+(1-_{2})g_{t}^{2}\); \\ \(_{t}=}{1-_{1}^{2}}\); \\ \(_{t}=}{1-_{1}^{2}}\); \\ \(_{t}=_{t-1}-_{t}+}}_{t}- _{t-1}\); \\   

Table 1: Test performance comparison of \(L_{2}\) regularization, Decoupled Weight Decay, and SWD for Adam, which corresponds to Adam, AdamW, and the proposed AdamS, respectively. We report the mean and the standard deviations (as the subscripts) of the optimal test errors. We make the lowest two test errors bold for each model. The SWD method even enables Adam to generalize as well as SGD and even outperform complex Adam variants for the five popular models.

Overlooked Pitfalls of Weight Decay

In this section, we report the overlooked pitfalls of weight decay, on convergence, stability, and gradient norms.

**Weight decay should be coupled with the learning rate scheduler.** The vanilla weight decay described by Hanson and Pratt  is given by Equation (1). A more popular implementation for vanilla SGD in modern deep learning libraries, such as PyTorch and TensorFlow, is given by Equation (2), where weight decay is coupled with the learning rate scheduler. While existing studies did not formally study why weight decay should be coupled with the learning rate scheduler, this trick has been adopted by most deep learning libraries.

We first take vanilla SGD as a studied example, where no momentum is involved. It is easy to see that vanilla SGD with \(L_{2}\) regularization \(\|\|^{2}\) is also given by Equation (2). Suppose the learning rate is fixed in the whole training procedure. Then Equation (2) will be identical to Equation (1) if we simply choose \(^{}=\). However, learning rate decay is quite important for training of deep neural networks. So Equation (2) is not identical to Equation (1) in practice.

**Which implementation is better?** In Figure 1, we empirically verified that the popular Equation-(2)-based weight decay indeed outperforms the vanilla implementation in Equation (1). We argue that Equation-(2)-based weight decay is theoretically better than Equation-(1)-based weight decay in terms of the stability of the stationary points. While we use the notations of _"stable"/"unstable" stationary points_ for simplicity, we formally term "stable"/"unstable" stationary points as _strongly/weakly stationary points_ in Definition 1. This clarifies that there can exist a gap between true updating and pure gradient-based updating for stationary points.

**Definition 1** (Strongly/Weakly Stationary Points).: _Suppose \(:\) is a function that updates model parameters during training where \(\) is the parameter space. We define \(^{}\) a strongly stationary point for \(\) if \(\|^{}-(^{})\|=0\) holds during the whole training process. We define \(^{}\) a weakly stationary point for \(\) if \(\|^{}-(^{})\|=0\) holds only for some iterations._

The training objective at step-\(t\) of Equation (1) is \(L()+}{_{t}}\|\|^{2}\), while that of Equation (2) is \(L()+\|\|^{2}\). Thus, stationary points of the regularized loss given by Equation (2) are stable during training, while that given by Equation (1) is unstable due to the dynamic training objective.

**Weight decay increases gradient norms at the final phase of deterministic optimization and stochastic optimization.** Following the standard convergence analysis of Gradient Descent (GD)  with the extra modification of weight decay, we prove Theorem 1 and show that even GD with vanilla weight decay may not converge to any non-zero stationary point due to missing stability of the stationary points.

**Theorem 1** (Non-convergence due to vanilla weight decay and scheduled learning rates).: _Suppose learning dynamics is governed by GD with vanilla weight decay (Equation (1)) and the learning rate \(_{t}(0,+)\) holds. If \(\) such that satisfies \(0<|_{t}-_{t+1}|\) for any \(t>0\), then the learning dynamics cannot converge to any non-zero stationary point satisfying the condition_

\[(\| f_{t}(_{t})\|^{2},\| f_{t}(_{t+1})\|^{2}) ^{2}\|^{}\|^{2}}{_{t} _{t+1}}>0,\]

_where \(f_{t}()=L()+}{_{t}}\|\|^{2}\) is the regularized loss._

We leave the proof in Appendix A.1. Theorem 1 means that even if the gradient norm is zero at the \(t\)-th step, the gradient norm at the \(t+1\)-th step will not be zero. Thus, the solution does not

Figure 1: We compared Equation-(1)-based weight decay and Equation-(2)-based weight decay by training ResNet18 on CIFAR-10 via vanilla SGD. In the presence of a popular learning rate scheduler, Equation-(2)-based weight decay shows better test performance. It supports that the form \(-_{t}\) is a better weight decay implementation than \(-^{}\).

converge to any non-zero stationary point. GD with unstable minima has no theoretical convergence guarantee like GD . More importantly, due to unstable stationary points, the gradient norms (of the regularized/original loss) naturally also do not converge to zero. This may explain why weight decay should be coupled with the learning rate to improve stability of stationary points during training. This non-convergence problem is especially significant when \(\) is relatively large in the presence of scheduled or adaptive learning rates, such as Adam.

Existing studies [48; 12] proved that the gradient norm bound of SGD in convergence analysis depends on the properties of the training objective function and gradient noise. We further prove Theorem 2 and reveal how the gradient norm bound in convergence analysis depends on the weight decay strength \(\) when weight decay is involved. We leave the proof in Appendix A.2.

**Theorem 2** (Convergence analysis on weight decay).: _Assume that \(L()\) is an \(\)-smooth function1, \(L\) is lower-bounded as \(L() L^{*}\), \(L(,X)\) is the loss over one minibatch \(X\), \([ L(,X)- L()]=0\), \([\| L(,X)- L()\|^{2}]^{2}\), and \(\| L()\| G\) for any \(\). Let SGD optimize \(f()=L()+\|\|^{2}\) for \(t+1\) iterations. If \(}\), we have_

\[_{k=0,,t}[\| f(_{k})\|^{2}] }[C_{1}+C_{2}],\] (3)

_where_

\[C_{1} =)+\|_{0}\|^{2}-L^{ }}{C},\] (4) \[C_{2} =C(+)((G+(\|\|))^{2}+^{ 2}),\] (5)

_and \((\|\|)=_{k=0,,t}\|_{k}\|\) is the maximum \(L_{2}\) norm of \(\) over the iterations._

Theorem 1 shows the non-convergence problem with improper weight decay. Theorem 2 further shows that, even with proper convergence guarantees and stable minima, the gradient norm upper bound at convergence still monotonically increases with the weight decay strength \(\). Obviously, Theorem 2 theoretically supports that weight decay can result in large gradient norms. The phenomenon can also be widely observed in the following empirically analysis.

Figure 2: Weight decay increases the gradient norms and the squared gradient norms ResNet18 for various optimizers. Top Row: the gradient norms. Bottom Row: the squared gradient norms.

**Empirical evidence of large gradient norms resulted by weight decay.** Theorem 1 suggests that both unstability and strength of weight decay may increase the gradient norm even in full-batch training. While weight decay is helpful for regularizing the model weights' norm, we clearly observed in Figure 2 that weight decay significantly increases the gradient norms in training of DNNs as we expect. Note that the displayed weight decay of AdamW is rescaled by the factor \(0.001\). The observation generally holds for various optimizers, such as AdamW, Adam and SGD. Figure 3 further shows that the observation holds for both scale-invariant models (VGG16 with BatchNorm) and scale-variant models (VGG16 without BatchNorm). Moreover, the observation still holds for gradient norms with or without regularization included, because the numerical difference due to regularization is smaller than their gradient norms by multiple orders of magnitude in the experiments. Zhang et al.  argued that weight decay increases the "effective learning rate" due to smaller weight norms resulted by weight decay. However, the "effective learning rate" interpretation may not explain large gradient norms, because a small weight norm does not necessarily indicate a large gradient norm. Interestingly, Figure 3 shows that the gradient curves of VGG16 without BatchNorm and weight decay sometimes exhibit strange spikes, which can be wiped out by weight decay.

## 3 Large Gradient Norms Are Undesired

In this section, we demonstrate that the large gradient norm is a highly undesirable property in deep learning because large gradient norms correspond to multiple pitfalls.

First, small gradient norms are required for good convergence, which is desired in optimization theory . Popular optimizers also expect clear convergence guarantees . The optimizers with no convergence guarantees may cause undesirable training behaviors in practice.

Figure 3: Weight decay increases the gradient norms and the squared gradient norms of VGG16 with BatchNorm (scale-invariant) and VGG16 without BatchNorm (scale-variant) on CIFAR-10. Top Row: the gradient norms. Bottom Row: the squared gradient norms.

Second, regularizing the gradient norm has been recognized as an important beneficial regularization effect in recent papers [28; 3; 11; 55; 54]. Li et al.  and Geiping et al.  expressed implicit regularization of stochastic gradients as

\[R_{}=|}_{X}\| L( ,X)\|^{2}=(),\] (6)

where \(L(,X)\) indicate the loss over one data minibatch \(X\), \(||\) is the number of minibatches per epoch, and \(\) is the expected squared gradient norms over each minibatch. Recent studies, Zhao et al.  and Zhang et al. , also successfully improved generalization by explicitly adding the gradient-norm penalty into stochastic optimization. Obviously, penalizing the gradient norm is considered as an important and beneficial regularizer in deep learning.

Third, large gradient norms may theoretically indicate poor generalization according to the gradient-norm generalization measures [27; 2]. For example, Theorem 11 of Li et al.  derived a gradient-norm-based generalization error upper bound that uses the squared gradient norms, written as

\[\,=(}{n}[^{2}}{_{t}^{2}}(_{t})]} ),\] (7)

where \(n\) is the training data size, \(_{t}^{2}\) is the gradient noise variance at the \(t\)-th iteration, and \((_{t})=|}_{X}\| L (,X)\|^{2}\) is the squared gradient norm with the batch size as 1. An et al.  presented more empirical evidences for supporting the gradient-norm-based generalization bound (7). Moreover, large gradient norms are also believed to be closely related to minima sharpness [20; 57; 43; 47; 42; 46; 7], while sharp minima often indicate poor generalization [16; 17; 14; 51; 21]. As our analysis focuses on the large gradient norm at the final phase (or the terminated solution) of training, our conclusion is actually not contradicted to the viewpoint [1; 29; 10] that the gradient-noise regularization effect of large initial learning rates at the early phase of training is beneficial.

We also report that the gradient norm and the squared gradient norm show very similar tendencies in our experiments. For simplicity, we mainly use \(\) as the default gradient norm measure in this paper, unless we specify it otherwise. As we discussed above, the squared gradient norm \(\) is more closely related to implicit gradient regularization, generalization bounds, and SWD than other gradient norms. Overall, the theoretical results above supported that the small gradient norm is a desirable property in deep learning. However, our theoretical analysis suggests that weight decay can lead to large gradient norms and hence hurt generalization. We suggest that gradient-norm-centered poor convergence and generalization can lead to the serious but overlooked performance degradation of weight decay.

## 4 Gradient-Norm-Aware Scheduled Weight Decay

In this section, we design the first practical scheduler to boost weight decay by mitigating large gradient norm.

We focus on scheduled weight decay for Adam in this paper for three reasons. First, Adam is the most popular optimization for accelerating training of DNNs. Second, as the adaptivity of Adam updates the learning rate every iteration, Adam suffers more than SGD from the pitfalls of weight decay due to adaptive learning rates. Third, SGD usually employs \(L_{2}\) regularization rather than weight decay.

Adam with decoupled weight decay is often called AdamW , which can be written as

\[_{t}=(1-)_{t-1}- v_{t}^{-}m_{t},\] (8)

where \(v_{t}\) and \(m_{t}\) are the exponential moving average of the squared gradients and gradients in Algorithm 1, respectively, and the power notation of a vector means the element-wise power of the vector. We interpret \( v_{t}^{-}\) as the effective learning rate for multiplying the gradients. However, we clearly see that decoupled weight decay couples weight decay with the vanilla learning rate rather than the effective learning rate. Due to the differences of weight decay and \(L_{2}\) regularization, AdamW and Adam optimize different training objectives. The minimum \(^{}\) of the regularized loss function optimized by AdamW at the \(t\)-th step is not constant during training. Thus, the regularized loss function optimized by AdamW has unstable minima, which may result in large gradient normssuggested by Theorem 1. This may be an internal fault of all optimizers that use adaptive learning rates and weight decay at the same time.

To mitigate the pitfall of large gradient norms, we propose a gradient-norm-aware scheduler for weight decay as

\[_{t}=(-_{t}^{-})_{t-1}- v _{t}^{-}m_{t},\] (9)

where \(_{t}\) is the mean of all elements of the vector \(v_{t}\). Moreover, as \(v_{t}\) is the exponential moving average of squared gradient norms, the factor \(_{t}\) is expected to be equivalent to the squared gradient norms divided by model dimensionality. While the proposed scheduler cannot give perfectly stable minima for Adam, it can penalize large gradient norms by dynamically adjusting weight decay. We call weight decay with this gradient-norm-aware scheduler Scheduled Weight Decay (SWD). The pseudocode is displayed in Algorithm 2. The code is publicly available at GitHub. In Section 5, we empirically demonstrate the advantage of the gradient-norm-aware scheduler. Note that AdamS is slightly slower than AdamW but slightly faster than Adam, while the cost difference is nearly ignorable (usually less than \(5\%\)).

We note that \(\) is not expected to be zero at minima due to stochastic gradient noise, because the variance of the stochastic gradient is directly observed to be much larger than the expectation of the stochastic gradient at/near minima and depends on the Hessian . In some cases, such as full-batch training, it is fine to add a small value (e.g., \(10^{-8}\)) to \(\) to avoid being zero as a divisor.

Lewkowycz and Gur-Ari  recently proposed a simple weight decay scheduling method which has a limited practical value because it only works well with a constant learning rate, as the original paper claimed. Bjorck et al.  empirically studied early effects of weight decay but did not touch mitigating the pitfalls of weight decay. In contrast, our experiments suggest that SWD indeed effectively penalizes the large gradient norm and hence improve convergence and generalization.

Figure 4 verifies multiple advantages of SWD. First, SWD dynamically adjusts the weight decay strength inverse to \(\) as we expect. Second, obviously, SWD can effectively penalize large gradient norms compared with constant weight decay during not only the final training phase but also the nearly whole training procedure. Third, SWD can often lead to lower training losses than constant weight decay. Figure 5 clearly reveals that, to choose proper weight decay strength, SWD dynamically

Figure 4: The curves of the weight decay scheduler, \(\), and training losses of SWD and constant weight decay. Top Row: ResNet18 on CIFAR-10. Bottom Row: ResNet34 on CIFAR-100. SWD mitigates large gradient norms.

employs a stronger scheduler when the initially selected weight decay hyperparameter \(\) is smaller and a weaker scheduler when \(\) is larger compared to the optimal weight decay choice, respectively.

## 5 Empirical Analysis

In this section, we first empirically verify that weight decay indeed affects gradient norms and further conducted comprehensive experiments to demonstrate the effectiveness of the SWD scheduler in mitigating large gradient norms and boosting performance.

**Models and Datasets.** We train various neural networks, including ResNet18/34/50 , VGG16 , DenseNet121 , GoogLeNet , and Long Short-Term Memory (LSTM) , on CIFAR-10/CIFAR-100 , ImageNet, and Penn TreeBank . The optimizers include popular Adam variants, including AMSGrad , Yogi , AdaBound , Padam , and RAdam . See Appendix B and C for more details and results.

**Generalization.** Figure 7 shows the learning curves of AdamS, AdamW, and Adam on several benchmarks. In our experiments, AdamS always leads to lower test errors. Figure 19 in Appendix shows that, even if with similar or higher training losses, AdamS still generalizes significantly better than AdamW, Adam, and recent Adam variants. Figure 6 displays the learning curves of all adaptive gradient methods. The test performance of other models can be found in Table 1. The results on ImageNet (See Figure 18 in Appendix) shows that, AdamS can also make improvements over AdamW. Simply scheduling weight decay for Adam by SWD even outperforms complex Adam variants. We report that most Adam variants surprisingly generalize worse than SGD (See Appendix C).

**Minima sharpness.** Previous studies  reported that the minima sharpness measured by the Hessian positively correlate to the gradient norm. Note that a number of popular minima sharpness depends on the Hessian at minima . In Figures 9 and 9, we visualize the top Hessian eigenvalues to measure the minima sharpness. Figure 9 shows that stronger weight decay leads to sharper minima. Figure 9 suggests that AdamS learns significantly flatter minima than AdamW. Both large gradient norms and sharp minima correspond to high-complexity solutions and often hurts generalization . This is a surprising finding for weight decay given the conventional belief that weight decay as a regularization technique should encourage low-complexity solutions.

**The Robustness of SWD.** Figure 11 shows that AdamS is more robust to the learning rate and weight decay than AdamW and Adam. AdamS has a much deeper and wider basin than AdamW and Adam. Figure 10 further demonstrates that AdamS consistently outperforms Adam and AdamW under various weight decay hyperparameters. According to Figure 10, we also notice that the optimal decoupled weight decay hyperparameter in AdamW can be very different from \(L_{2}\) regularization and SWD. Thus, AdamW requires re-tuning the weight decay hyperparameter in practice, which is time-consuming. The robustness of selecting hyperparameters can be supported by Figure 5, where the proposed scheduler also depends on the initial weight decay strength. It means that the proposed gradient-norm-aware scheduler exhibit the behavior of negative feedback to stabilize the strength of weight decay during training.

**Cosine learning rate schedulers and warm restarts.** We conducted comparative experiments on AdamS, AdamW, and Adam in the presence of cosine annealing schedulers and warm restarts proposed by Loshchilov and Hutter . We set the learning rate scheduler with a recommendedsetting of Loshchilov and Hutter . Our experimental results in Figure 12 suggest that AdamS consistently outperforms AdamW and Adam in terms of both training losses and test errors. It demonstrates that, with complex learning rate schedulers, the advantage of SWD still holds.

**Limitations.** While the proposed scheduler boosts weight decay in most experiments, the improvement of SWD may be limited sometimes. SWD does not work well particularly when \(L_{2}\) regularization even yields better test results than weight decay (See the experiments on Language Modeling Figure 13 in Appendix C). Our paper does not touch the pitfall of \(L_{2}\) regularization and how to schedule its strength. We leave scheduling \(L_{2}\) regularization as future work.

## 6 Conclusion

While weight decay has attracted much attention, previous studies failed to recognize the problems of unstable stationary points and large gradient norm caused by weight decay. In this paper, we theoretically and empirically studied the overlooked pitfalls of weight decay from the gradient-norm perspective. To mitigate the pitfalls, we propose a simple but effective gradient-norm-aware scheduler for weight decay. Our empirical results demonstrate that SWD can effectively penalize large gradient norms and improve convergence. Moreover, SWD often significantly improve generalization over constant schedulers. The generalization gap between SGD and Adam can be almost closed by such a simple weight decay scheduler for CNNs.

We emphasize that the proposed method is the first scheduled weight decay method rather than a novel optimizer. Although our analysis mainly focused on Adam, SWD can be easily combined with other optimizers, including SGD and Adam variants. To the best of our knowledge, this is the first formal touch on the art of scheduling weight decay to boost weight decay. We believe that scheduled weight decay as a new line of research may inspire more theories and algorithms that will help further understanding and boosting weight decay in future. Our work just made the first step along this line.

Figure 11: The test errors of ResNet18 on CIFAR-10. AdamS has a much deeper and wider basin near dark points (\( 4.9\%\)). The optimal test errors of AdamS, AdamW, and Adam are \(4.52\%\), \(4.90\%\), and \(5.49\%\), respectively. The displayed weight decay of AdamW has been rescaled by the factor \(=0.001\).

Figure 12: The test curves and training curves of ResNet18 and VGG16 on CIFAR-10 with cosine annealing and warm restart schedulers. The weight decay hyperparameter: \(_{L_{2}}=_{S}=0.0005\) and \(_{W}=0.5\). Left Two Figures: Test curves. Right Two Figures: Training curves. AdamS yields significantly lower test errors and training losses than AdamW and Adam.