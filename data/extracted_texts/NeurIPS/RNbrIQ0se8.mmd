# Ada-MSHyper: Adaptive Multi-Scale Hypergraph Transformer for Time Series Forecasting

Zongjiang Shang, Ling Chen, Binqing Wu, Dongliang Cui

State Key Laboratory of Blockchain and Data Security

College of Computer Science and Technology

Zhejiang University

{zongjiangshang, lingchen, binqingwu, runnercd}@cs.zju.edu.cn

Corresponding author: Ling Chen.

###### Abstract

Although transformer-based methods have achieved great success in multi-scale temporal pattern interaction modeling, two key challenges limit their further development: (1) Individual time points contain less semantic information, and leveraging attention to model pair-wise interactions may cause the information utilization bottleneck. (2) Multiple inherent temporal variations (e.g., rising, falling, and fluctuating) entangled in temporal patterns. To this end, we propose **A**d**aptive **M**ulti-**S**cale **H**ypergraph Transformer (Ada-MSHyper) for time series forecasting. Specifically, an adaptive hypergraph learning module is designed to provide foundations for modeling group-wise interactions, then a multi-scale interaction module is introduced to promote more comprehensive pattern interactions at different scales. In addition, a node and hyperedge constraint mechanism is introduced to cluster nodes with similar semantic information and differentiate the temporal variations within each scales. Extensive experiments on 11 real-world datasets demonstrate that Ada-MSHyper achieves state-of-the-art performance, reducing prediction errors by an average of 4.56%, 10.38%, and 4.97% in MSE for long-range, short-range, and ultra-long-range time series forecasting, respectively. Code is available at https://github.com/shangzongjiang/Ada-MSHyper.

## 1 Introduction

Time series forecasting has demonstrated its wide applications across many fields , e.g., energy consumption planning, traffic and economics prediction, and disease propagation forecasting. In these real-world applications, the observed time series often demonstrate complex and diverse temporal patterns at different scales. For example, due to periodic human activities, traffic occupation and electricity consumption show clear daily patterns (e.g., afternoon or evening), weekly patterns (e.g., weekday or weekend), and even monthly patterns (e.g., summer or winter).

Recently, deep models have achieved great success in time series forecasting. To tackle intricate temporal patterns and their interactions at different scales, numerous foundational backbones have emerged, including recurrent neural networks (RNNs) , convolutional neural networks (CNNs) , graph neural networks (GNNs) , and transformers . Particularly, due to the capabilities of depicting pair-wise interactions and extracting multi-scale representations in sequences, transformers are widely used in time series forecasting. However, some recent studies show that even simple multi-scale MLP  or naive series decomposition methods  can outperform transformer-based methods on various benchmarks. We argue the challenges that limit the effectiveness of transformers in time series forecasting are as follows.

The first one is _semantic information sparsity_. Different from natural language processing (NLP) and computer vision (CV), individual time point in time series contains less semantic information [5; 29]. Compared to pair-wise interactions, group-wise interactions among time points with similar semantic information (e.g., neighboring time points or distant but strongly correlated time points) are more emphasized in time series forecasting. To address the problem of semantic information sparsity, some recent works employ patch-based approaches [12; 23] and hypergraph structures  to enhance locality and capture group-wise interactions. However, simple partitioning of patches and predefined hypergraph structures may introduce a large amount of noise and be hard to discover implicit interactions.

The second one is _temporal variations entanglement_. Due to the complexity and non-stationary of real-world time series, the temporal patterns of observed time series often contain a large number of inherent variations (e.g., rising, falling, and fluctuating), which may mix and overlap with each other. Especially when there are distinct temporal patterns at different scales, multiple temporal variations are deeply entangled, bringing extreme challenges for time series forecasting. To tackle the problem of temporal variations entanglement, recent studies employ series decomposition [30; 39] and multi-periodicity analysis [27; 29] to differentiate temporal variations at different scales. However, existing methods lack the ability to differentiate temporal variations within each scale, making temporal variations within each scale overlap and become entangled with redundant information.

Motivated by the above, we propose Ada-MSHper, an **A**d**aptive **M**ulti-**S**cale **H**yper**graph Transformer for time series forecasting. Specifically, Ada-MSHper map the input sequence into multi-scale feature representations, then by treating the multi-scale feature representations as nodes, an adaptive multi-scale hypergraph structure is introduced to discover the abundant and implicit group-wise node interactions at different scales. To the best of our knowledge, Ada-MSHper is the first work that incorporates adaptive hypergraph modeling into time series forecasting. The main contributions are summarized as follows:

* We design an adaptive hypergraph learning (AHL) module to model the abundant and implicit group-wise node interactions at different scales and a multi-scale interaction module to perform hypergraph convolution attention, which empower transformers with the ability to model group-wise pattern interactions at different scales.
* We introduce a node and hyperedge constraint (NHC) mechanism during hypergraph learning phase, which utilizes semantic similarity to cluster nodes with similar semantic information and leverages distance similarity to differentiate the temporal variations within each scales.
* We conduct extensive experiments on 11 real-world datasets. The experimental results demonstrate that Ada-MSHper achieves state-of-the-art (SOTA) performance, reducing error by an average of 4.56%, 10.38%, and 4.97% in MSE for long-range, short-range, and ultra-long-range time series forecasting, respectively, compared to the best baseline.

## 2 Related Work

**Deep Models for Time Series Forecasting.** Deep models have shown promising results in time series forecasting. To model temporal patterns at different scales and their interactions, a large number of specially designed backbones have emerged. TAMS-RNNs  captures periodic temporal dependencies through multi-scale recurrent structures with different update frequencies. TimesNet  extends the 1D time series into the 2D space, and models multi-scale temporal pattern interactions through 2D convolution inception blocks. Benefiting from the attention mechanism, transformers have gone beyond contemporaneous RNN- and CNN-based methods and achieved promising results in time series forecasting. FEDformer  combines mixture of expert and frequency attention to capture multi-scale temporal dependencies. Pyraformer  extends the input sequence into multi-scale representations and models the interactions between nodes at different scales through pyramidal attention. Nevertheless, with the rapid emergence of linear forecasters [22; 35; 11], the effectiveness of transformers in this direction is being questioned.

Recently, some methods have attempted to fully utilize transformers and paid attention to the inherent properties of time series. Some of these methods are dedicated to addressing the problem of semantic information sparsity in time series forecasting. PatchTST  segments the input sequence into subseries-level patches to enhance locality and capture group-wise interactions. MSHyper models group-wise interactions through multi-scale hypergraph structures and introduces \(k\)-hop connections to aggregate information from different range of neighbors. However, constrained by the fixed windows and predefined rules, these methods cannot discover implicit interactions. Others emphasize on addressing the problem of temporal variations entanglement in time series forecasting. FilM  differentiates temporal variations at different scales by decomposing the input series into different period lengths. iTransformer  combines inverted structures with transformer to learn entangled global temporal variations. However, these methods cannot differentiate temporal variations within each scale, making temporal variations within each scale overlap and become entangled with redundant information.

**Hypergraph Neural Networks.** As a generalized form of GNNs, hypergraph neural networks (HGNNs) have been applied in different fields, e.g., video object segmentation , stock selection , multi-agent trajectory prediction , and time series forecasting . HyperGCN  is the first work that incorporates convolution operation into hypergraphs, which demonstrates the superiority of HGNNs over ordinary GNNs in capture group-wise interactions. Recent studies [2; 33] show that HGNNs are promising to model group-wise pattern interactions. LBSN2Vec++  uses hypergraphs for location-based social networks, which leverages heterogeneous hypergraph embeddings to capture mobility and social relationship pattern interactions. GroupNet  utilizes multi-scale hypergraph for trajectory prediction, which combines relational reasoning with hypergraph structures to capture group-wise pattern interactions among multiple agents.

Considering the capability of HGNNs in modeling group-wise interactions, in this work, an adaptive multi-scale hypergraph transformer framework is proposed to model the group-wise pattern interactions at different scales. Specifically, an AHL model is designed to model the abundant and implicit group-wise node interactions. In addition, a NHC mechanism is introduced to cluster nodes with similar semantic information and differentiate temporal variations within each scale, respectively.

## 3 Preliminaries

**Hypergraph.** A hypergraph is defined as \(=\{,\}\), where \(=\{e_{1},,e_{m},,e_{M}\}\) is the hyperedge set and \(=\{v_{1},,v_{n},,v_{N}\}\) is the node set. Each hyperedge represents group-wise interactions by connecting a set of nodes \(\{v_{1},v_{2},,v_{n}\}\). The topology of hypergraph can be represented as an incidence matrix \(^{N M}\), with entries \(_{nm}\) defined as follows:

\[_{nm}=\{1,& v_{n} e_{m}\\ 0,& v_{n} e_{m}.\] (1)

The degree of the \(n\)th node is defined as \(d(v_{n})=_{m=1}^{M}_{nm}\) and the degree of the \(m\)th hyperedge is defined as \(d(v_{m})=_{n=1}^{N}_{nm}\). Further, the node degrees and hyperedge degrees are sorted in diagonal matrices \(_{}^{N N}\) and \(_{}^{M M}\), respectively.

**Problem Formulation.** Given the input sequence \(_{1:T}^{}=\{_{t}_{t} ^{D},t[1,T]\}\), where \(_{t}\) represents the values at time step \(t\), \(T\) is the input length, and \(D\) is the feature dimension. The task of time series forecasting is to predict the future \(H\) steps, which can be formulated as follows:

\[}_{T+1:T+H}^{}=(_{1:T}^{ };)^{H D},\] (2)

where \(}_{T+1:T+H}^{}\) denotes the forecasting results, \(\) denotes the mapping function, and \(\) denotes the learnable parameters of \(\). The description of the key notations are given in Appendix A.

## 4 Ada-MSHyper

As previously mentioned, the core of Ada-MSHyper is to promote more comprehensive pattern interactions at different scales. To accomplish this goal, we first map the input sequence into sub-sequences at different scales through the multi-scale feature extraction (MFE) module. Then, by treating multi-scale feature representations as nodes, the AHL module is introduced to model the abundant and implicit group-wise node interactions at different scales. Finally, the multi-scale interaction module is introduced to model group-wise pattern interactions at different scales. Notably, during the hypergraph learning phase, an NHC mechanism is introduced to cluster nodes with similar semantic information and differentiate temporal variations within each scale. The overall framework of Ada-MSHyper is shown in Figure 1.

### Multi-Scale Feature Extraction (MFE) Module

The MFE module is designed to get the feature representations at different scales. As shown in Figure 1(a), suppose \(^{s}=\{_{t}^{s}|_{t}^{s}^{D},t[1,N^{s}]\}\) denotes the sub-sequence at scale \(s\), where \(s=1,...,S\) denotes the scale index and \(S\) is the total number of scales. \(N^{s}=}{l^{s-1}}\) is the number of nodes at scale \(s\) and \(l^{s-1}\) denotes the size of the aggregation window at scale \(s-1\). \(^{1}=^{1}_{1:T}\) is the raw input sequence and the aggregation process can be formulated as follows:

\[^{s}=Agg(^{s-1};^{s-1})^{N^{s} D },s 2,\] (3)

where \(Agg\) is the aggregation function, e.g., 1D convolution or average pooling, and \(^{s-1}\) denotes the learnable parameters of the aggregation function at scale \(s-1\).

### Adaptive Hypergraph Learning (AHL) Module

The AHL module automatically generates incidence matrices to model implicit group-wise node interactions at different scales. As shown in Figure 1(b), we first initialize two kinds of parameters, i.e., node embeddings \(_{}^{s}^{N^{s} D}\) and hyperedge embeddings \(_{}^{s}^{M^{s} D}\) at scale \(s\), where \(M^{s}\) is hyperparameters, representing the number of hyperedges at scale \(s\). Then, we can obtain the scale-specific incidence matrix \(^{s}\) by similarity calculation, which can be formulated as follows:

\[^{s}=SoftMax(ReLU(_{}^{s}(_{}^{s})^{T})),\] (4)

where the \(ReLU\) activation function is used to eliminate weak connections and the \(SoftMax\) function is applied to normalize the value of \(^{s}\). In order to reduce subsequent computational costs and noise interference, the following strategy is designed to sparsify the incidence matrix:

\[_{nm}^{s}=_{nm}^{s},&_{nm}^{s}  TopK(_{n^{s}}^{s},)\\ 0,&_{nm}^{s} TopK(_{n^{s}}^{s},)\] (5)

where \(\) is the threshold of \(TopK\) function and denotes the max number of neighboring hyperedges connected to a node. The final values of \(_{nm}^{s}\) can be obtained as follows:

\[_{nm}^{s}=1,&_{nm}^{s}>\\ 0,&_{nm}^{s}<\] (6)

where \(\) denotes the threshold, and the final scale-specific incidence matrices can be represented as \(\{^{1},,^{s},,^{S}\}\). Compared to previous methods, our adaptive hypergraph learning is novel from two aspects. Firstly, our methods can capture group-wise interactions at different scales, while most previous methods [5; 21] can only model pair-wise interactions at a single scale. Secondly, our methods can model abundant and implicit interactions, while many previous methods [23; 26] depend on fixed windows and predefined rules.

Figure 1: The framework of Ada-MSHyper.

### Node and Hyperedge Constraint (NHC) Mechanism

Although the AHL module can help discover implicit group-wise node interactions at different scales, we argue that the pure data-driven approach faces two limitations, i.e., unable to efficiently cluster nodes with similar semantic information and differentiate temporal variations within each scale. To tackle the above dilemmas, we introduce the NHC mechanism during hypergraph learning phase.

Given the multi-scale feature representations \(\{^{1},,^{s},,^{S}\}\) generated from the MFE module, and the scale-specific incidence matrices \(\{^{1},,^{s},,^{S}\}\) generated from the AHL module, we first get the initialized node feature representations \(}^{s}=f(^{s})\) at scale \(s\), where \(f\) can be implemented by the multi-layer perceptron (MLP). As shown in Figure 2(a), the initialized hyper-edge feature representations can be obtained by the aggregation operation based on \(^{s}\). Specifically, for the \(i\)th hyperedge \(e_{i}^{s}\) at scale \(s\), its feature representations \(_{i}^{s}\) can be formulated as follows:

\[_{i}^{s}=avg(_{v_{j}^{s}(e_{i}^{s})} _{j}^{s})^{D},\] (7)

where \(avg\) is the average operation, \((e_{i})\) represents the neighboring nodes connected by \(e_{i}^{s}\) at scale \(s\), and \(_{j}^{s}}^{s}\) is the \(j\)th node feature representations at scale \(s\). The initialized hyperedge feature representations at different scales can be represented as \(\{}^{1},,}^{s},, }^{S}\}\). Then, based on semantic similarity and distance similarity, we introduce node constraint to cluster nodes with similar semantic information and leverage hyperedge constraint to differentiate the temporal variations of temporal patterns.

**Node Constraint.** In the data-driven hypergraph, we observe that some nodes connected by the same hyperedge contain distinct semantic information. To cluster nodes with similar semantic information and reduce irrelevant noise interference, we introduce node constraint based on the semantic similarity between nodes and their corresponding hyperedges. As shown in Figure 2(b), for the \(j\)th node at scale \(s\), we first obtain its semantic similarity difference \(_{j}^{s}}\) with its corresponding hyperedges:

\[_{j}^{s}}=\{abs(_{j}^{s}-_{i}^{s})|v_{j}^{s}(e_{i}^{s})\},\] (8)

where \(abs\) refers to the operation of calculating the absolute value. The node loss \(L_{node}^{s}\) at scale \(s\) based on node constraint can be formulated as follows:

\[L_{node}^{s}=}_{i=1}^{N^{s}}_{j}^{ s}},\] (9)

where \(N^{s}\) is the number of nodes at scale \(s\). Empowered by the node constraint, our method can enjoy more advantageous group-wise semantic information than pure data-driven hypergraph. Further experimental results and visualization analysis in Section 5.3 and Appendix H demonstrate the effectiveness of the node constraint in clustering nodes with similar semantic information.

**Hyperedge Constraint.** Since time series is a collection of data points arranged in chronological order, some recent works [23; 26] show that connecting multiple nodes sequentially through patches or hyperedges can represent specific temporal variations. Therefore, to deal with the problem of temporal variations entanglement, we introduce hyperedge constraint based on distance similarity. As shown in Figure 2(c), we first compute the cosine similarity to reflect the correlation of any two hyperedge representations at scale \(s\), which can be formulated as follows:

\[_{i,j}=_{i}^{s}(_{j}^{s})^{T}}{\| _{i}^{s}\|_{2}\|_{j}^{s}\|_{2}},\] (10)

Figure 2: The node and hyperedge constraint mechanism.

where \(_{i,j}\) represents the correlation weight. \(_{i}^{s}\) and \(_{j}^{s}\) are the \(i\)th and \(j\)th hyperedge representation at scale \(s\), respectively. Then, we use Euclidean distance \(D_{i,j}\) to measure the differentiation magnitude between any two hyperedge representations, which can be formulated as follows:

\[D_{i,j}=\|_{i}^{s}-_{j}^{s}\|_{2}=^{D}((_{i}^{s})^{d}-(_{j}^{s})^{d})^{2}},\] (11)

The hyperedge loss \(L_{hyper}^{s}\) at scale \(s\) based on the correlation weight and Euclidean distance can be formulated as follows:

\[L_{hyper}^{s}=)^{2}}_{i=1}^{M^{s}} _{j=1}^{M^{s}}(_{i,j}D_{i,j}+(1-_{i,j})max( -D_{i,j},0)),\] (12)

where \(>0\) denotes the threshold. Notably, when \(_{i,j}=1\), indicating that \(e_{i}^{s}\) and \(e_{k}^{s}\) are deemed similar, the hyperedge loss turns to \(L_{hyper}=)^{2}}_{i=1}^{M^{s}}_{j=1 }^{M^{s}}_{i,j}D_{i,j}\), where the loss will increase if \(D_{i,j}\) becomes large. Conversely, when \(_{i,j}=0\), meaning \(e_{i}\) and \(e_{k}\) are regarded as dissimilar, the hyperedge loss turns to \(L_{hyper}=)^{2}}_{i=1}^{M^{s}}_{j=1 }^{M^{s}}(1-_{i,j})max(s-D_{i,j},0)\), where the loss will increase if \(D_{i,j}\) falls below the threshold and turns smaller. Other cases lie between the above circumstances. We further provide the visualization results in Section 5.3 and appendix H to verify that our constraint loss can differentiate temporary variations of temporary patterns within each scale and promote forecasting performance. The final constraint loss \(L_{const}\) based on node constraint and hyperedge constraint can be formulated as follows:

\[L_{const}=_{s=1}^{S}L_{node}^{s}+(1-) _{s=1}^{S}L_{hyper}^{s},\] (13)

where \(\) denotes the hyperparameter controlling the balance between node loss and hyperedge loss.

### Multi-Scale Interaction Module

To promote more comprehensive pattern interactions at different scales, a direct way is to mix multi-scale node feature representations at different scales. However, we argue that intra-scale interactions and inter-scale interactions reflect different aspects of pattern interactions, where intra-scale interactions mainly depict detailed interactions between nodes with similar semantic information and inter-scale interactions highlight macroscopic variations interactions[9; 27]. Therefore, instead of directly mixing multi-scale pattern information as a whole, we introduce the multi-scale interaction module to perform inter-scale interactions and intra-scale interactions.

**Intra-Scale Interaction Module.** Due to the semantic information sparsity of time series, traditional pair-wise attention may may cause the information utilization bottleneck . In contrast, some recent studies [23; 26] show that group-wise interactions can provide more informative insights in time series forecasting. To capture group-wise interactions among nodes with similar semantic information within each scale, we introduce hypergraph convolution attention within the intra-scale interaction module. Specifically, given \(^{s}\), we first use attention mechanism to capture the interaction strength of each node \(v_{i}^{s}}^{s}\) and its related hyperedges at scale \(s\), which can be formulated as follows:

\[}_{ij}^{s}=[_{i}^{s}, _{j}^{s}]))}{_{e_{k}^{s}(v_{i}^{s})}exp( (f_{t}[_{i}^{s},_{k}^{s}])},\] (14)

where \([.,.]\) denotes the concatenation operation of the \(i\)th node and its related hyperedges. \(f_{t}\) is a trainable MLP, and \((v_{i}^{s})\) is the neighboring hyperedges connected to \(v_{i}^{s}\), which can be accessed using \(^{s}\). Then, considering the symmetric normalized hypergraph Laplacian convolution \(=_{v}^{-1/2}_{e}^{-1}^ {T}_{v}^{-1/2}\) used in HGNN , the multi-head hypergraph convolution attention can be formulated as follows:

\[}}^{s}=_{j=1}^{}( (_{v^{s}}^{-1/2}}_{j}^{s}_{e^{s}}^{- 1}(}_{j}^{s})^{}_{v^{s}}^{-1/2}}^{s}_{j}^{s}))^{N^{s} D},\] (15)

where \(}}^{s}\) is the updated node feature representations at scale \(s\), \(\) is the aggregation function used for combing the outputs of multi-head, e.g., concatenation or average pooling. \(\) is the activation function, e.g., LeakyReLU and ELU. \(}_{s}^{s}\) and \(_{s}^{s}\) are the enriched incidence matrix and the learnable weight matrix of the \(\)th head at scale \(s\), respectively. \(\) is the number of heads.

**Inter-Scale Interaction Module.** The inter-scale interaction module is introduced to capture pattern interactions at different scales. To achieve this goal, a direct way is to model group-wise node interactions across all scales. However, detailed group-wise node interactions across all scales can introduce redundant information and increase computation complexity. Therefore, we adopt a hyperedge attention within the inter-scale interaction module to capture macroscopic variations interactions at different scales. Technically, based on the hyperedge representations \(}=\{}^{1},,}^{s},, {}^{S}\}\), we first adopt linear projections to get queries, keys, and values \(\), \(\), \(^{M D}\). Then the hyperedge attention can be formulated as follows:

\[}=softmax(^{}}{}}),\] (16)

where \(}\) is the updated hyperedge feature representations.

### Prediction Module & Loss Function

After obtaining the updated node and hyperedge feature representations, we concatenate them and feed them into a linear layer for prediction. We choose Mean Squared Error (MSE) as our forecasting loss, which can be formulated as follows:

\[L_{mse}=\|}_{T+1:T+H}^{}-_ {T+1:T+H}^{}\|_{2}^{2},\] (17)

where \(_{T+1:T+H}^{}\) and \(}_{T+1:T+H}^{}\) are ground truth and forecasting results, respectively. Notably, during training phase, \(L_{mse}\) is used to regulate the overall learning process, while \(L_{const}\) is only used to constrain hypergraph learning process.

### Complexity Analysis

For the MFE module, the time complexity is \((Nl)\), where \(N\) is the number of nodes at the finest scale and \(N\) is equal to the input length \(T\). \(l\) is the aggregation window size at the finest scale. For the AHL module, the time complexity is \((MN+M^{2})\), where \(M\) is the number of hypergraphs at the finest scale. For the intra-scale interaction module, since \(_{v}\) and \(_{e}\) are diagonal matrices, the time complexity is \((MN)\). For the inter-scale interaction module, the time complexity is \(M^{2}\). In practical operation, \(M\) and \(l\) is the hyperparameter and is much smaller than \(N\). As a result, the total time complexity of Ada-MSHyper is bounded by \((N)\).

## 5 Experiment

### Experimental Setup

**Datasets.** For long-range time series forecasting, we conduct experiments on 7 commonly used benchmarks, including ETT (ETTh1, ETTh2, ETTm1, and ETTm2), Traffic, Electricity, and Weather datasets following [30; 21; 26]. For short-range time series forecasting, we adopt 4 benchmarks from PEMS (PEMS03, PEMS04, PEMS07, and PEMS08) following [21; 27]. For ultra-long-range time series forecasting, we adopt ETT datasets following . Table 1 gives the dataset statistics. In addition, the forecastability is derived from one minus the entropy of the Fourier decomposition of a time series[27; 14]. Higher values mean greater forecastability.

**Baselines.** We compare Ada-MSHyper with 15 competitive baselines, i.e., iTransformer , MSHyper , PatchTST , TimeMixer , MSGNet , CrossGNN , TimesNet , WITRAN , SCINet , Crossformer , FiLM , DLinear , FEDformer , Pyraformer , and Autoformer .

   Dataset &  &  &  &  &  \\  ETT (s) atten & 7 & 0.66 & 102, 36, 70, 101 & (15 units, flowby) & (0.38-0.55) & Telegrams \\  Weight & 21 & 0.66 & 128, 36, 70, 101 & (10 units) & 0.75 & Weight \\  Electricity & 311 & 0.62 & 0.82 & 36, 70, 101 & Handy & 0.77 & Intensity \\  Traffic & 862 & 0.66 & 128, 36, 70, 101 & Handy & 0.68 & Transmission \\ 
**Transics (s) atten & (70880) & (122, 24, 40) & 5 units & (0.40-0.85) & Traffic network \\   

Table 1: Dataset statistics.

[MISSING_PAGE_FAIL:8]

almost all benchmarks, with an average error reduction of 4.97% and 2.21% compared to the best baseline in MSE and MAE, respectively. (2) Compared with other baselines, PatchTST and MSHper achieve competitive results. The reason may be that group-wise interactions can help mitigate the issue of semantic information sparsity. (3) Compared to PatchTST and MSHper, Ada-MSHper achieves superior performance, the reason may be that the inter-scale interaction module can help capture macroscopic variations interactions, especially for the ultra-long-rang time series.

### Ablation Studies

**AHL Module.** To investigate the effectiveness of the AHL model, we conduct ablation studies by designing the following three variations: (1) Replacing the AHL module with adaptive graph learning module (-AGL). (2) Replacing the AHL model with one incidence matrix to capture group-wise node interactions at different scales (-one). (3) Replacing the AHL module with predefined multi-scale hypergraphs (-PH), i.e., each hyperedge connected a fixed number of nodes (4 in our experiment) in chronological order. The experimental results on ETH1 dataset are shown in Table 5. We can observe that -AGL gets the worst forecasting results, indicating the importance of modeling group-wise interactions. In addition, -PH and -one perform worse than Ada-MSHper, showing the effectiveness of adaptive hypergraph and multi-scale hypergraph, respectively.

**NHC Mechanism.** To investigate the effectiveness of the NHC mechanism, we conduct ablation studies by designing the following three variations: (1) Removing the node constraint (-w/o NC). (2) Removing the hyperedge constraint (-w/o HC). (3) Removing the NHC mechanism (-w/o NHC). The experimental results on ETH1 dataset are shown in Table 5. We can observe that Ada-MSHper performs better than -w/o NC and -w/o HC, showing the effectiveness of node constraint and hyperedge constraint, respectively. In addition, -w/o NHC gets the worst forecasting results, which demonstrates the superiority of the NHC mechanism in adaptive hypergraph learning. More results about ablation studies are shown in Appendix F.

To further demonstrate the effectiveness of the node constraint in clustering nodes with similar semantic information, we present case visualization with -w/o NHC and -w/o HC on Electricity dataset. We randomly select one sample and plot the node values at the finest scale. We categorize the nodes into four groups based on the node values. Nodes with the same color indicate that they may have similar semantic information. As shown in Figure 2(a), for the target node, nodes of other colors may be considered as noise. We drew the nodes related to the target node in black color based on incidence matrix \(^{1}\). As shown in Figure 2(b), due to the lack of node constraint, -w/o NHC can only capture the interactions among the target node and neighboring nodes and cannot distinguish nuanced noise information. In Figure 2(c), with the node constraint, -w/o HC can cluster neighboring and distant but still strongly correlated nodes. In Figure 2(d), with the NHC mechanism, Ada-MSHper cannot only cluster nodes with similar semantic information but can differentiate temporal variations. The full visualization results are shown in Appendix H.

### Parameter Studies

    & Ada-MSHyper & Transference & MSHper & Transference & MSHper & Transference & WITRAN & PathTST & DLinear & Cross-layer & PIDE/learner & Predformer & Auto-learner \\  & (Ours) & (2024) & (3204) & (3204) & (3202) & (3202) & (3203) & (2023) & (3022) & (3022) & (3022) & (3022) \\  Metric & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE & MSE MAE \\  ETH1 & **0.4565 0.457** & 0.766 0.611 & 0.745 0.610 & 0.804 0.631 & 0.734 0.833 & 0.699 0.588 & 0.666 0.648 & 0.9121 & 0.706 0.637 & 1.083 0.89 0.675 \\  ETH2 & **0.4480.00** & 0.541 0.518 & 0.515 0.495 & 0.540 0.532 & 0.540 0.537 & 0.510 0.498 & 1.218 0.787 & 2.530 1.233 & 0.625 0.574 & 3.263 1.599 & 0.656 0.648 \\  ETH1 & **0.484** (0.46) & 0.854 0.495 & 0.540 0.480 & 0.532 0.483 & 0.532 0.476 & 0.501 **0.440** & 0.5400 0.498 & 3.555 1.483 & 0.522 0.591 & 1.099 0.811 & 0.631 0.550 \\  ETH2 & **0.425** **0.434** & 0.468 0.449 & 0.464 0.427 & 0.465 0.449 & 0.446 **0.424** & 0.462 0.448 & 0.655 0.574 & 3.555 1.483 & 0.487 0.475 & 4.566 1.745 & 0.516 0.491 \\   

Table 4: Results of ultra-long-range time series forecasting under multivariate settings. Results are averaged from all prediction lengths. Full results are listed in Appendix E.

   Variation &  &  &  &  &  &  & Ada-MSHper \\  Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ 
96 & 0.542 & 0.560 & 0.422 & 0.437 & 0.386 & 0.403 & 0.390 & 0.403 & 0.384 & 0.416 & 0.393 & 0.422 & **0.372** & **0.393** \\
336 & – & – & 0.559 & 0.502 & 0.448 & 0.452 & 0.423 & 0.437 & 0.430 & 0.435 & 0.425 & 0.441 & **0.422** & **0.433** \\
729 & – & – & 0.563 & 0.617 & 0.456 & 0.458 & 0.448 & **0.457** & 0.451 & 0.460 & 0.449 & 0.466 & **0.445** & 0.459 \\   

Table 5: Results of different adaptive hypergraph learning methods and constraint mechanisms.

We perform parameter studies to measure the impact of the number of scales (#scales) and the max number of hyperedges connected to a node (#hyperedges). The experimental results on ETTh1 dataset are shown in Figure 4, we can see that: (1) the best performance can be obtained when #scales is 3. The reason is that smaller #scales cannot provide sufficient pattern information and larger #scales may introduce excessive parameters and result in overfitting problems. (2) The optimal #hyperedges is 5. The reason is that smaller #hyperedges cannot capture group-wise interactions sufficiently and larger #hyperedges may introduce noise. More results about parameter studies are shown in Appendix G.

### Computational Cost

We compare Ada-MSHper with the two latest transformer-based methods, i.e., iTransformer and PatchTST, on traffic datasets with the output length of 96. The experimental results are shown in Table 6. Although we have a larger number of parameters, we achieve lower training time and lower GPU occupation due to the matrix sparsity operation in the model and the optimization of hypergraph computation provided by _torch_geometry_. Considering the forecasting performance and the computation cost, Ada-MSHper demonstrates its superiority over existing methods.

## 6 Conclusions and Future Work

In this paper, we propose Ada-MSHper with an adaptive multi-scale hypergraph for time series forecasting. Empowered by the AHL module and multi-scale interaction module, Ada-MSHper can promote more comprehensive multi-scale group-wise pattern interactions, addressing the problem of semantic information sparsity. Experimentally, Ada-MSHper achieves the SOTA performance, reducing prediction errors by an average of 4.56%, 10.38%, and 4.97% in MSE for long-range, short-range, and ultra-long-range time series forecasting, respectively. In addition, the visualization analysis and the ablation studies demonstrate the effectiveness of NHC mechanism in clustering nodes with similar semantic information and in addressing the issue of temporal variations entanglement.

In the future, this work can be extended in the following two aspects. First, since 2D spectrogram data may offer a better representation for time series forecasting, we will adapt our framework to the 2D spectrogram data in time-frequency domain. Second, since the features extracted by the MFE module may contain redundant information, we will design a disentangled multi-scale feature extraction module to extract more independent and representative features.

   Methods & Training Time & \# Parameters & GPU Occupation & MSE results \\  Ada-MSHper & **6.499\%** & 8,965,392 & **6.542MB** & **0.384** \\ iTransformer & 7.863\% & 6,731,984 & 6,738MB & 0.395 \\ PatchTST & 17.603\% & **548.704** & 9,798MB & 0.526 \\   

Table 6: Computation cost.

Figure 4: The impact of hyperparameters.

Figure 3: Visualization the node constraint effect on Electricity dataset.

Acknowledgement

This work was supported by the Science Foundation of Donghai Laboratory (Grant No. DH-2022ZY0013).