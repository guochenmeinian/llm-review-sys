# Incremental Uncertainty-aware Performance Monitoring with Labeling Intervention

Alexander Koebler\({}^{1,2}\) Thomas Decker\({}^{1,3}\)* Ingo Thon\({}^{1}\)

Volker Tresp\({}^{3,4}\) Florian Buettner\({}^{2,5,6}\)

\({}^{1}\)Siemens AG \({}^{2}\)Goethe University Frankfurt \({}^{3}\)LMU Munich

\({}^{4}\)Munich Center for Machine Learning (MCML)

\({}^{5}\)German Cancer Research Center (DKFZ) \({}^{6}\)German Cancer Consortium (DKTK)

{alexander.koebler, thomas.decker, ingo.thon}@siemens.com

volker.tresp@lmu.de, florian.buettner@dkfz.de

Equal contribution

###### Abstract

We study the problem of monitoring machine learning models under temporal distribution shifts, where circumstances change gradually over time, often leading to unnoticed yet significant declines in accuracy. We propose Incremental Uncertainty-aware Performance Monitoring (IUPM), a novel label-free method that estimates model performance by modeling time-dependent shifts using optimal transport. IUPM also quantifies uncertainty in performance estimates and introduces an active labeling strategy to reduce this uncertainty. We further showcase the benefits of IUPM on different datasets and simulated temporal shifts over existing baselines.

## 1 Introduction

Deployed machine learning models often face the critical challenge of distribution shifts, where the data encountered in production deviates from the data used during training. Many relevant shift scenarios involve changes over time, which are often gradual and continuous . These shifts are characterized by the fact that the statistical properties of the data or the environment change progressively rather than abruptly. This gradual nature can make time-dependent shifts more insidious, as they may not be immediately apparent but can still lead to substantial degradation in prediction quality over time . Therefore, anticipating and understanding temporal performance changes is essential for ensuring the reliability and effectiveness of a machine learning model in dynamic environments . However, directly monitoring the performance during the deployment is challenging as labeled data is often unavailable in production. Moreover, obtaining labels can be cumbersome, time-consuming, and costly, leading to delays in the assessment. Therefore, an increasing number of label-free estimation methods have been proposed that aim to anticipate the model performance purely based on unlabeled data available at runtime.

Figure 1: Illustration of Incremental Uncertainty-aware Performance Monitoring (IUPM) with label intervention.

While this task is provably impossible in general , it can be approached by leveraging prior knowledge about the structure of the shift . Existing techniques leverage strategies based on feature statistics , importance weighting [23; 3], model confidence [14; 13], disagreement between models [16; 1], or model differences after retraining . However, only a few works explicitly try to incorporate the additional structure arising from the temporal nature of shifts into the estimation process [2; 28]. Furthermore, existing techniques are unable to quantify any uncertainty related to the performance estimate, cannot anticipate model degradation for arbitrary loss functions, and provide no information on how to best aid estimation with limited labeled data. To address these challenges we make the following contributions:

* We propose **Incremental Uncertainty-aware Performance Monitoring (IUPM)**, as a novel label-free performance estimation method tailored to temporal distribution shifts (see Figure 1).
* We quantify the uncertainty of the estimated performance to ensure reliable detection of model degradation during deployment.
* We introduce an active intervention step to reduce uncertainty in the performance estimate by labeling examples contributing the highest uncertainty to the performance estimate.

## 2 Incremental Uncertainty-aware Performance Monitoring (IUPM)

Optimal TransportOptimal Transport (OT) aims at finding the cost-minimizing way to transform one probability measure into another . Consider having \(n_{0}\) samples from a domain \(_{0}=\{x_{0}^{i}\}_{i=1}^{n_{0}}\) and \(n_{1}\) samples from another domain \(_{1}=\{x_{1}^{i}\}_{i=1}^{n_{1}}\) with corresponding empirical distributions

\[_{0}=_{x_{0}_{0}}}_{x_{0}} _{1}=_{x_{1}_{1}}}_{x_{1}}\]

where \(_{x}\) denotes the Dirac measure. For a cost function \(c:_{0}_{1}^{+}\), the transformation of \(_{0}\) into \(_{1}\) can be formalized by a coupling \(\) which represents a valid distributions over \((_{0}_{1})\) with marginals corresponding to \(_{0}\) and \(_{1}\). Identifying the cost-optimal coupling reads:

\[=_{}_{x_{0}_{0}}_{x_{1} _{1}}c(x_{0},x_{1})(x_{0},x_{1})=\{ ^{n_{0} n_{1}}_{n_{1}}=_{0 },^{T}_{n_{0}}=_{1}\}\]

which can be solved using different algorithmic approaches . In the discrete sample case the obtained \(^{n_{0} n_{1}}\) simply is a matrix with entries \((x_{0},x_{1})\). Moreover, the conditional coupling

\[(X_{0}=x_{0}|X_{1}=x_{1})=,x_{1})}{_{x_{0} _{0}}(x_{0},x_{1})}\]

is a left-stochastic matrix whose entries can be interpreted as transition probabilities when moving from samples of \(X_{1}\) to samples of \(X_{0}\) following the most cost-efficient path.

Incremental Performance Estimation using OTConsider a machine learning model \(f\) that has been trained with labeled data \(\{(X_{0},Y_{0})\}\) from the distribution \(P_{0}(X_{0},Y_{0})\). Suppose \(f\) is deployed in order to make predictions over time \(t>0\) with respect to data \(\{(X_{t},Y_{t})\}_{t=1}^{t}\) each distributed with \(P_{t}(X_{t},Y_{t})\). During runtime \((t>0)\) we only have access to unlabeled data from \(X_{t}\) and our goal is to estimate only based on this information how well the model performs over time. To do so we proceed as follows: Let \(_{t}(X_{t-1}|X_{t})\) be the conditional coupling linking data from \(X_{t}\) to data from \(X_{t-1}\) in a cost-efficient way. Further, we define \(_{t}(X_{0}|X_{t})=_{i=1}^{t}_{i}(X_{i-1}|X_{i})\) describing the transition matrix obtained from composing all incremental transition matrices \(_{i}(X_{i-1}|X_{i})\) via matrix multiplication. It expresses the overall transition probabilities of going back to the labeled data available at \(t=0\) by connecting samples of two subsequent time points incrementally using an individual optimal transport coupling. Based on this we propose the following strategy to estimate missing labels for performance evaluation over time.

\[(Y_{t}|X_{t})=_{_{t}(X_{0}|X_{t})}[P(Y_{0}|X_{0})]\]

This means that our label estimate \((Y_{t}|X_{t})\) arises as mixture distribution  combining labeled data in \(X_{0}\) according to the accumulated incremental coupling results. This strategy is explicitly reasonable for temporal shifts as we leverage their gradual nature by modeling subsequent distribution shifts incrementally using Optimal Transport. It implicitly assumes that the true decision boundary around data points from \(X_{t}\) is similar to the ones of data points from \(X_{t-1}\) that are linked via the cost-efficient coupling, which again suits in particular gradual shifts over time. Given an arbitrary loss function \(\) to measure model performance and a set of samples \(_{t}\) from \(X_{t}\), the resulting performance estimate for IUPM at time \(t\), denoted by \(}_{t}^{}\), is given by:

\[}_{t}^{}=_{P(X_{t})}_{ (Y_{t}|X_{t})}[(f(X_{t}),Y_{t})]=}_{x_{ t}_{t}}_{(Y_{t}|X_{t}=x_{t})}[(f(x_{t}),Y_{ t})]\]

Note that \((Y_{t}|X_{t})\) is an actual predictive distribution that also internalized uncertainty for cases where linked samples have contradicting labels. Thus, we can use it to quantify the uncertainty of the anticipated performance using the expected standard deviation (SD) of the sample-wise loss estimates:

\[(}_{t}^{})=_{P(X_{t})} _{(Y_{t}|X_{t})}[(f(X_{t}),Y_{t})]\]

Labeling InterventionIn addition to providing a means for users to consolidate their trust in the performance estimate, the quantified uncertainty \((}_{t}^{})\) can also be used to automatically trigger efficient relabeling when the uncertainty exceeds an acceptable level. To make the most efficient use of a limited labeling budget allowing to label only \(m\) samples, we propose a Uncertainty Intervention (UI) strategy only querying ground truth labels for critical samples \(x_{t}\) that contribute the largest uncertainty to the overall performance estimate:

\[m_{x_{t}_{t}}_{(Y_{t}|X_{t}=x_{t})} [(f(x_{t}),Y_{t})]\]

where \(m\) denotes the operator selecting the top \(m\) elements maximizing the objective. The new labels are used to update \(_{t}(X_{0}|X_{t})\) such that \((Y_{t}|X_{t}=x_{t})\) assigns a fixed label removing the accumulated uncertainty for sample \(x_{t}\).

## 3 Experiments

We evaluate our proposed IUPM approach on two different settings and data modalities. First, to assess the general functionality of IUPM, we present results based on synthetic toy examples with continuous shifts in two-dimensional space. Second, to emphasize the connection to real-world scenarios, we perform experiments based on MNIST  and continuous image perturbations, e.g., resembling a gradual camera degradation. Throughout the experiments, we compare our approach to four existing performance estimation methods described in . Average Confidence (AC) simply estimates the prediction accuracy as the expectation of the confidence for the predicted class across the data set. The more sophisticated Difference Of Confidence (DOC)  uses the discrepancy between the model confidence on the source and target data sets as an estimate of performance degradation. Average Threshold Confidence (ATC)  learns a threshold for model confidence on the initialization data set \(_{0}\) and estimates accuracy on the current set as the fraction of examples where model confidence exceeds this threshold. Lastly, we consider Importance re-weighting as proposed by . Additionally, we evaluate the use of the direct mapping \((X_{0}|X_{t})\) for label transport and performance estimation inspired by . We call this approach Non-Incremental Performance Estimation (NIPM). Across all experiments, we consider the model accuracy as the loss criteria \(\) to evaluate performance. For more details, we also refer to Appendix A.1 and A.2.

Translation and Rotation in Input SpaceWe use three two-dimensional toy data sets (Fig. 2) provided by . For all three datasets, we train a Random Forest (RF), XGBoost (XGB), and a Multilayer Perceptron (MLP) classifier in the initial source distribution. After training at \(t=0\), all data sets are shifted for \(t=1,,100\) steps to simulate gradual changes over time. For the "Clusters" and "Moons" data sets, this shift results from rotating both classes by 2\({}^{}\) per step. The

Figure 2: Synthetic two-dimensional toy datasets and corresponding shifts.

"Circles" dataset experiences a translation shift by \(0.02\) in the x-direction only on the inner circle class. The results in Table 1 show that IUPM consistently outperforms all baselines across all datasets and models. The error in the accuracy estimate can be further reduced by intervening on the labels. In this case, we relabel the top 50% of \(_{t}\) according to our introduced UI strategy once \((}_{t}^{})>0.1\).

Comparison between Targeted and Random Label InterventionTo validate the utility of our proposed uncertainty indicator, we provide an additional experiment that quantifies the benefit of actively selecting the samples \(x_{k}\) to be relabeled during label intervention. More Specifically, we compare relabeling \(m\) samples based on our proposed instance-wise performance uncertainty \(m_{x_{t}_{t}}_{(Y_{t}|X_{t}=x_{t})} [(f(x_{t}),Y_{t})]\) with randomly selected \(m\) samples from \(_{t}\). The results in Table 2 show a considerable benefit of targeted sampling for the majority of scenarios, demonstrating that it can improve the efficiency of the labeling intervention under a limited labeling budget. This supports the hypothesis that correcting the labels of examples with the highest instance-wise uncertainty also has an increased benefit to the overall performance estimation.

Monitoring Performance Degradation due to Image PerturbationsTo assess more complex shifts, we monitor a model classifying handwritten digits  that experience a shift caused by common image perturbations  such as rotation, used in a related context in , translation, and scaling of the digits. For this experiment, we trained a LeNet . Further, we perform the matching

    &  &  &  \\   & **RF** & **XGB** & **MLP** & **RF** & **XGB** & **MLP** & **RF** & **XGB** & **MLP** \\ 
**ATC** & 0.4482 & 0.4701 & 0.4378 & 0.3821 & 0.3596 & 0.3696 & 0.3513 & 0.3455 & 0.3545 \\ 
**AC** & 0.4370 & 0.4908 & 0.4436 & 0.3264 & 0.3753 & 0.3532 & 0.2793 & 0.3342 & 0.3268 \\ 
**DOC** & 0.4417 & 0.4564 & 0.4522 & 0.3657 & 0.3501 & 0.3708 & 0.3594 & 0.3436 & 0.3495 \\ 
**IM** & 0.4580 & 0.4700 & 0.4746 & 0.3980 & 0.3609 & 0.3807 & 0.3572 & 0.3448 & 0.3508 \\ 
**NIPM** & 0.4303 & 0.4260 & 0.4731 & 0.2514 & 0.2256 & 0.2325 & 0.0822 & 0.0823 & 0.0793 \\ 
**IUPM** & 0.2852 & 0.2837 & 0.2996 & 0.0897 & 0.0837 & 0.1065 & 0.0330 & 0.0368 & 0.0335 \\ 
**IUPM\({}_{UI}\)** & **0.0270** & **0.0272** & **0.0265** & **0.0244** & **0.0242** & **0.0222** & **0.0160** & **0.0157** & **0.0158** \\   

Table 1: Mean Average Error (MAE) between ground truth and estimated accuracy using baseline methods and IUPM across three synthetic data sets and three different models.

    &  &  &  \\   & **RF** & **XGB** & **MLP** & **RF** & **XGB** & **MLP** & **RF** & **XGB** & **MLP** \\ 
**IUPM** & 0.2852 & 0.2837 & 0.2996 & 0.0897 & 0.0837 & 0.1065 & 0.0330 & 0.0368 & 0.0335 \\ 
**IUPM\({}_{UI}\)** & **0.0270** & **0.0272** & **0.0265** & **0.0244** & 0.0242 & 0.0222 & **0.0160** & **0.0157** & **0.0158** \\ 
**IUPM\({}_{RI}\)** & 0.0336 & 0.0327 & 0.0296 & 0.0256 & **0.0234** & **0.0198** & **0.0160** & 0.0177 & 0.0176 \\   

Table 2: Mean Average Error (MAE) between ground truth and estimated accuracy given random sample selection during intervention (RI) and our proposed Uncertainty Intervention (UI) across three synthetic data sets and three different models.

based on representations of the second classification layer of this LeNet, reducing the dimensional of the matched samples from \(784\) to \(84\) and demonstrating a generalizing approach to deal with higher dimensional data also used in . All shifts are evaluated for \(20\) steps, where Figure 3 illustrates the performance estimation for a 180\({}^{}\) rotation. Even without intervention, IUPM can best estimate the performance. By allowing label intervention with the same settings as in the previous experiment, the performance estimation error can be reduced and the associated uncertainty can be limited to the predefined value \((}_{t}^{})<0.1\). The observations are further supported by the results on two additional shifts in Table 3.

## 4 Conclusion

We have introduced a novel method tailored to monitor a deployed machine learning model facing gradual distribution shifts over time. Our IUPM approach takes a step towards achieving a more reliable assessment of the quality of a model's output at run time by explicitly taking the uncertainty of the performance estimate into account, which current methods lack. This allows to simultaneous increase the user's confidence in the estimate and to perform a targeted label intervention to efficiently restore a sufficient trustworthiness of the system only when needed. We have shown that over two different data modalities and five different shifts, IUPM outperforms a number of existing performance estimation approaches. Incorporating additional insights from related fields such as gradual domain adaption  or active testing  as well as scaling to more complex data sets with real-world shifts, are interesting avenues for future research.