# Song Lai\({}^{3}\), Qingfu Zhang\({}^{3}\), Yang Wang\({}^{1,2}\)

Breaking Long-Tailed Learning Bottlenecks: A Controllable Paradigm with Hypernetwork-Generated Diverse Experts

 Zhe Zhao\({}^{1,3}\), Haibin Wen\({}^{5}\), Zikang Wang\({}^{1}\), Pengkun Wang\({}^{1,2}\), Fanfu Wang\({}^{6}\),\({}^{1}\)University of Science and Technology of China (USTC), Hefei, China

\({}^{2}\)Suzhou Institute for Advanced Research, USTC, Suzhou, China

\({}^{3}\)City University of Hong Kong, Hong Kong, China

\({}^{4}\)Harbin Institute of Technology, Harbin, China

\({}^{5}\)MorongAI, Suzhou, China

\({}^{6}\)Lanzhou University, Lanzhou, China

{zz4543@mail.ustc.edu.cn, haibin65535@gmail.com, 2021211940@stu.hit.edu.cn, wangff21@lzu.edu.cn, songlai2-c@my.cityu.edu.hk, qingfu.zhang@cityu.edu.hk, pengkun.wang@ustc.edu.cn, angyan@ustc.edu.cn

Pengkun Wang and Yang Wang are corresponding authors.

###### Abstract

Traditional long-tailed learning methods often perform poorly when dealing with inconsistencies between training and test data distributions, and they cannot flexibly adapt to different user preferences for trade-offs between head and tail classes. To address this issue, we propose a novel long-tailed learning paradigm that aims to tackle distribution shift in real-world scenarios and accommodate different user preferences for the trade-off between head and tail classes. We generate a set of diverse expert models via hypernetworks to cover all possible distribution scenarios, and optimize the model ensemble to adapt to any test distribution. Crucially, in any distribution scenario, we can flexibly output a dedicated model solution that matches the user's preference. Extensive experiments demonstrate that our method not only achieves higher performance ceilings but also effectively overcomes distribution shift while allowing controllable adjustments according to user preferences. We provide new insights and a paradigm for the long-tailed learning problem, greatly expanding its applicability in practical scenarios. The code can be found here: https://github.com/DataLab-atom/PRL.

## 1 Introduction

In many real-world tasks such as object detection and image classification, we face the challenge of long-tailed distributions. Since the samples of the head classes account for the vast majority in the datasets while the tail class samples are extremely scarce , this extreme imbalance in the data makes the model prone to overfitting towards the head classes during training, resulting in poor performance on the tail classes .

To address the long-tailed distribution problem, existing research has proposed a series of methods such as re-sampling  and modifying the loss function , with the common idea of focusing on improving the performance of the tail classes. However, these methods typically assume that the distributions of the training and test data remain invariant, and thus cannot well handle the common situations of distribution shift between training and testing in real-world scenarios. Some more recent works such as RIDE  and LADE  propose using multiple expert models to obtain stronger distribution adaptability. Building on this, SADE  further adaptively combines the outputs of these experts during testing to adapt to the current test distribution. These approaches alleviate the problem of distribution mismatch between training and testing to some extent .

However, in addressing distribution shifts across different test scenarios, the goal of these multi-expert model-based methods is still to maximize the overall performance, i.e., pursuing the optimal overall performance metric across all classes, and obtaining a fixed trade-off for this purpose. But in different application scenarios, _users may have different preferences and needs for the relative trade-off between head and tail classes. Simply pursuing the overall optimal solution may not meet this flexibility requirement_. For example, in classifying lung CT images, when screening for difficult cases, we care more about whether all possible disease types (i.e., tail classes) can be covered to avoid missed diagnoses, compared to routine physical examinations. For some serious diseases such as lung cancer, we may also be willing to moderately increase the false positive rate in exchange for higher coverage of the tail classes, to ensure that no patients are missed. Another example is wildlife detection. Within nature reserves, we want the model to accurately detect common species (i.e., head classes) to understand their population sizes. But when looking for rare species (i.e., tail classes), we care more about covering all species, even at the cost of some false detections.

As can be seen, in different application scenarios, there are significant differences in user preferences for the weighting of head and tail categories, which current long-tailed learning methods often fail to fully satisfy. Therefore, developing an interpretable and controllable method for handling long-tail distributions that adapts to specific user preferences for head and tail categories becomes a new research direction in the field of long-tailed learning.

In light of this, we propose an interpretable and controllable long-tail learning method (**PRL**). This method aims not only to overcome potential distribution shifts from a single training distribution to any testing distribution but more importantly, to flexibly adjust the weights of head and tail categories according to actual user demands. To address these challenges, we introduce a new long-tailed learning paradigm based on a diverse set of experts and hypernetworks, which can adapt to a wide range of distribution scenarios and meet personalized user preferences.

To tackle the aforementioned challenges, we propose a new long-tailed learning paradigm based on diverse experts and hypernetworks, as illustrated in Figure. For the first challenge, existing multi-expert model-based methods train fixed expert models for specific distributions, requiring strong distribution assumptions and struggling to handle more complex and variable distributions. Therefore, instead of maximizing the performance of each expert individually, we pursue modeling and optimizing the hypervolume over the entire Pareto front curve, learning a set of solutions that cover all possible distribution scenarios. This requires us to sample with the goal of covering the entire Pareto front during optimization. For the second challenge, unlike LADE and SADE which output a fixed trade-off solution under distribution shift, we can flexibly output a dedicated model solution that matches the user's preference in any test distribution scenario. In this way, our method can not only adapt to changes in the test distribution, but also allow controllable adjustment of the head-tail trade-off according to the user's actual needs.

Our contributions can be summarized as follows:

* _New scenario and insight_: we make the first attempt at a controllable trade-off based on user preferences in the context of long-tailed learning and test distribution shift scenarios, greatly expanding the applicability in real-world scenarios.
* _New learning paradigm_: we propose a new interpretable and controllable long-tailed learning method that can acquire the ability to overcome test distribution shift from a single distribution dataset and satisfy user preferences in any shifted distribution scenario.
* _Compelling empirical results_: extensive experiments demonstrate that our method achieves higher performance ceilings, effectively overcomes test distribution shift, and can be controlled by user preferences.

## 2 Related Work

Long-tailed distributions are prevalent in real-world data, leading to imbalanced datasets that pose challenges for machine learning models [30; 20]. To address this issue, researchers have proposed various methods, including re-sampling, loss function modification, and multi-expert models.

Re-sampling methods balance class distributions by oversampling tail classes [24; 3] or undersampling head classes . Loss function modification approaches assign higher weights to tail class losses [27; 26] or use meta-learning to alleviate undersampling issues [14; 32]. Multi-expert models train multiple experts on different class distributions and combine their outputs, adapting to various test distributions [37; 38; 31]. Most existing methods assume specific distributions during training or testing, limiting real-world applicability with distribution shifts, and cannot accommodate varying user needs for head and tail class trade-offs. We propose an approach to overcoming distribution assumptions and achieve interpretable, controllable trade-offs in long-tailed learning.

## 3 Theory

In this section, we analyze the distribution shift problem from a theoretical perspective and provide the definition and properties of the environment's total variation distance, laying the theoretical foundation for the methods section.

**Definition 1** (Distribution Discrepancy across Environments).: _Given \(M\) training environments \(_{1},,_{M}\), with class prior probability vectors \(^{1},,^{M}\), respectively, where \(^{m}=(_{1}^{m},,_{K}^{m})\), \(_{K}^{m}\) denotes the probability of the \(k\)-th class appearing in environment \(_{m}\), and \(_{k=1}^{K}_{k}^{m}=1\). If there exist \(i,j,k,l\) such that \(^{i}}{_{i}^{i}}^{j}}{_{i}^{i}}\) holds, then these \(M\) environments are said to have distribution discrepancy._

Traditional empirical risk minimization (ERM) methods on a single training distribution struggle to handle distribution discrepancy, which can affect generalization. This limitation can be characterized by the following theorem:

Figure 1: Illustration of our method: (a) Existing methods train for a specific long-tailed distribution but may fail on arbitrarily skewed test distributions. (b) Multi-expert learns different experts for different distributions from one training set but lacks flexibility for arbitrary distributions/preferences. (c) Our method samples preference vectors during training to simulate distributions, and can flexibly adjust the preference vector during testing for flexible long-tailed classification.

**Theorem 1** (Limitation of ERM).: _Let \(f(;)\) be a classifier learned via ERM on \(_{m}\), then its risk on the test environment \(_{}\) is:_

\[R_{}(f)=R_{m}(f)+_{i=1}^{K}(_{i}^{}-_{i}^{m}) _{ P_{}(|y=z)}[(f(;),i)]\] (1)

_where \(R_{m}(f)\) and \(R_{}(f)\) are the risks of \(f\) on \(_{m}\) and \(_{}\), respectively, and \(^{}\) is the class prior of the test environment._

To measure the distribution discrepancy across environments, we introduce the environment's total variation distance (ETVD):

**Definition 2** (Environment Total Variation Distance).: _The total variation distance between environments \(_{i}\) and \(_{j}\) is defined as: \((_{i},_{j})=_{k=1}^{K}|_{k}^{i }-_{k}^{j}|\), and the ETVD of \(M\) environments is defined as: \((_{1},,_{M})=_{i,j\{1,,M \}}(_{i},_{j})\)_

Using ETVD, we can further bound the risk of the ERM-learned classifier on the test environment:

**Corollary 1**.: _Under the assumptions of Theorem 1, let \(M=_{i,}(f(;),i)\), then_

\[R_{}(f) R_{m}(f)+2M((_{m},_{ })+(_{1},,_{M}))\] (2)

This corollary shows that the test risk of the ERM-learned classifier is affected not only by the distribution discrepancy between the training environment and the test environment but also by the distribution discrepancy among training environments (i.e., ETVD). To overcome the diversity shift, we propose minimizing the empirical risks across multiple training environments to capture the distributional characteristics of different environments, thereby learning a set of diversity experts.

Next, we provide a theoretical analysis of the domain adaptation algorithm based on diversity experts proposed in this paper. To characterize the generalization performance of this algorithm, we first introduce the following notations:

Let \(\{f_{1},,f_{N}\}\) be the \(N\) experts learned via ERM on the \(N\) training environments \(\{_{1},,_{N}\}\), respectively, and \(\) be the final classifier obtained by ensembling these \(N\) experts. Define the empirical risk of the ensemble classifier \(\) on environment \(_{m}\) as:

\[_{m}()=_{i=1}^{N}R_{m}(f_{i})\] (3)

We can obtain the following theorem regarding the generalization performance of the ensemble classifier:

**Theorem 2**.: _Under the above notations and definitions, the risk of the ensemble classifier \(\) on the test environment \(_{}\) satisfies:_

\[R_{}()_{m=1}^{N}R_{m}(f_{m})+2M( _{m=1}^{N}(_{m},_{})+ (_{1},,_{N}))\] (4)

_where \(M=_{i,}((),i)\)._

Theorem 2 shows that the test risk of the ensemble classifier consists of three parts: the average empirical risk of all experts, the average total variation distance between the training environments and the test environment, and the weighted average of ETVD among the training environments. Compared to single-environment ERM, the diversity experts method learns a set of experts to capture the distributional characteristics of different environments, which can reduce the distribution discrepancy between the training environments and the test environment, thereby achieving better generalization performance.

## 4 Methodology

### Problem Formulation

Consider a \(K\)-class classification problem with a training set \(=\{(_{i},y_{i})\}_{i=1}^{N}\), where each class \(k\) has \(N_{k}\) samples. Let \(_{}\) denote the empirical distribution over \(\). The goal is to learn a classifier\(f:^{K}\) that generalizes well across various test distributions \(_{}\). Traditional empirical risk minimization (ERM) methods optimize the loss under \(_{}\), but may fail to adapt to changes in \(_{}\), especially in long-tailed scenarios.

To improve the robustness of \(f\), we optimize the losses under multiple importance-weighted distributions. Define an \(M\)-dimensional simplex:

\[_{M}:=\{_{+}^{M}_{i=1}^{M} _{i}=1\}\] (5)

Each \(_{M}\) corresponds to an importance-weighted distribution \(_{}\):

\[_{}(,y):=_{k=1}^{K}_{k} _{k}( y)_{k}(y)\] (6)

where \(_{k}( y)\) and \(_{k}(y)=}{N}\) are the conditional distribution and prior for class \(k\), respectively.

The objective is to learn a set of classifiers \(:=\{f^{(i)}\}_{i=1}^{M}\) that achieve low risk simultaneously across all \(_{}\), forming the Pareto optimal solution:

\[_{}\,(_{_{_{1}}}( ),\,,\,_{_{_{M}}}( ))\] (7)

where \(_{_{}}():=_{( ,y)_{}}[_ {i=1}^{M}(f^{(i)}(),y)]\). Pursuing an approximate Pareto solution across all distributions leads to models with stronger generalization capabilities.

### Diverse Experts

Let \(\) and \(\) denote the input and output spaces, respectively. We introduce \(T=3\) classifiers \(\{f_{i}\}_{i=1}^{T}\) as diverse experts. These experts share a feature extractor \(_{}:^{d}\), but use different classifier heads \(\{g_{w_{i}}\}_{i=1}^{T}\):

\[f_{i}()=g_{w_{i}}(_{}()), i=1,,T\] (8)

To generate diverse experts, we introduce a hypernetwork \(h_{}\) that takes random noise \(^{k}\) as input and outputs the classifier head parameters \(_{i}^{d C}\):

\[_{i}=h_{}(_{i}),_{i}( ), i=1,,T\] (9)

where \(()\) is the Dirichlet distribution with parameter \(_{+}^{k}\). The hypernetwork \(h_{}\) consists of three linear layers with ReLU activations.

During training, we sample \(\{_{i}\}_{i=1}^{T}\) from \(()\) and use \(h_{}\) to generate \(\{_{i}\}_{i=1}^{T}\). The loss function for a training batch is:

\[=_{i=1}^{T}_{i}(f_{i})\] (10)

where \(_{i}\) is the classification loss for the \(i\)-th expert \(f_{i}\), defined the same as in SADE: \(_{1}\) is the standard cross-entropy loss; \(_{2}\) is the balanced softmax loss, where the logits are adjusted by adding the log of the prior probabilities of each class; \(_{3}\) is the inverse softmax loss, where the logits are adjusted by adding the log of the prior probabilities and subtracting the scaled log of the inverse prior probabilities.

### Stochastic Convex Ensemble

Let \(_{i}(,)\) denote the loss function of the \(i\)-th expert \(f_{i}\) on dataset \(\), where \(=\{,\}\) represents all trainable parameters. The objective is to jointly optimize the losses of all \(T\) experts:

\[_{}_{i=1}^{T}_{i}(, )\] (11)To promote diversity among experts, we introduce the Stochastic Convex Ensemble (SCE) strategy, which aims to minimize the worst-case loss of the convex combination of experts:

\[_{}_{_{T}}_{i=1}^{T}p_{i}_ {i}(,)\] (12)

where \(}=(p_{1},,p_{T})^{}_{T}\) is the weight vector, and \(_{T}:=\{}_{+}^{T}|_{i=1}^{T}p_{i}=1\}\) is the \(T\)-dimensional simplex.

Inspired by the max-min inequality, we relax the SCE objective to:

\[_{}(_{i=1}^{T}_{i}(, )+_{i=1}^{T}(_{i}( ,)))\] (13)

where \(>0\) is a hyperparameter. As \( 0\), the relaxed objective approaches the original SCE objective. The term \(_{i=1}^{T}(_{i}( {},))\) promotes diversity among experts.

### Preference-Controlled Trade-off

During testing, we can control the trade-off between head and tail classes using a preference vector \(^{*}=(_{1}^{*},_{2}^{*},_{3}^{*})^{} ^{3}\), where \(^{3}\) is the 3-dimensional simplex.

Given a trained preference vector \(=(r_{1},r_{2},r_{3})^{}^{3}\), we compute the test-time preference vector \(}^{3}\) as:

\[}=^{*}}{^{}^{*}}\] (14)

where \(\) denotes the Hadamard product. The test-time preference vector \(}\) is then input to the hypernetwork \(h_{}\) to generate the classifier head parameters for each expert:

\[}_{i}=h_{}(}), i=1,,T\] (15)

where \(}_{i}^{d C}\) is the weight matrix for the \(i\)-th expert's classifier head. For a test sample with feature vector \(^{d}\), the output of the \(i\)-th expert is:

\[}_{i}=^{}}{\|\|_{2}}}_{i}}{\|}_{i}\|_{}},&\\ ^{}}_{i}+}_{i}^{},&\] (16)

where \(\|\|_{}\) is the Frobenius norm and \(}_{i}^{C}\) is the bias vector for the \(i\)-th expert. By adjusting \(^{*}\), we can control the model's focus on head or tail classes, enabling flexible trade-offs to suit different application needs.

**An observation on our method.** To better understand this part, we use Figure 2 to demonstrate the effectiveness of preference control in overcoming distribution shifts, as well as the flexibility of our method. For preferences, the coordinate system is a three-dimensional orthogonal coordinate; for accuracy, the coordinate system represents the performance on the farward50, uni., and backward50 splits of the CIFAR100-LT dataset. The dark plane represents the plane formed by different preference vectors, and the outer surface represents the corresponding performance on the three distributions for these preference vectors. The yellow dots are the results of running SADE, whose preferences are uncontrollable, so the results of each run are random dots, lying below our purple plane, indicating that their performance is lower than our method (i.e., being dominated in the Pareto optimal set). This figure illustrates that our method can cover unknown distributions without additional training, and unlike previous methods, it can trade off performance by adjusting the preference vector. We will analyze this in more depth in the experimental section.

Figure 2: Mapping from preference to model properties.

## 5 Experiments

In this section, we first evaluate the superiority of PRL in terms of both standard and test-agnostic long-tailed recognition to demonstrate that our method has a higher performance ceiling under the traditional setup. Then, we analyze the effectiveness of our method in changing the trade-off for long-tailed classes through input preferences. Furthermore, we conduct necessary ablation studies.

### Experimental Setups

**Datasets.** We evaluate our method on four benchmark datasets: ImageNet-LT , CIFAR100-LT , Places-LT , and iNaturalist 2018 . These datasets have varying imbalance ratios, ranging from 10 to 256. CIFAR100-LT has three versions with different imbalance ratios. Detailed statistics are in Appendix D.

**Baselines.** We compare PRL with various state-of-the-art long-tailed recognition methods, including two-stage methods (MiSLAS ), logit-adjusted training (Balanced Softmax , LADE ), ensemble learning (RIDE , SADE ), causal inference (Causal ), representation learning (LSC ), and balanced posterior averaging (BalPoE ). These methods address the long-tail problem from different perspectives. Further details are provided in the appendixA.

**Evaluation protocols and implementation details.** We evaluate the models on multiple test datasets with different class distributions using micro accuracy. We report the accuracy of many-shot, medium-shot, and few-shot classes. We use the same setup for all methods, including ResNeXt-50 for ImageNet-LT, ResNet-32 for CIFAR100-LT, ResNet-152 for Places-LT, and ResNet-50 for iNaturalist 2018 as backbones. We employ hypernets (MLPs) to output trainable parameters of experts and adopt the cosine classifier for prediction. Unless specified, we use \(=1.2\) for the Dirichlet distribution, \(=0.3\) for stochastic annealing, SGD with momentum 0.9, train for 200 epochs, and set the initial learning rate to 0.1 with linear decay. During test-time training, we train aggregation weights for 5 epochs with a batch size of 128, using the same optimizer and learning rate as in training. Other details please refer to Appendix G.

### Comparative Evaluation on Standard and Test-Agnostic Long-Tailed Recognition

We conduct extensive experiments on four widely-used long-tailed datasets, including CIFAR100-LT, Places-LT, iNaturalist 2018, and ImageNet-LT, to evaluate the performance of our proposed PRL method in comparison with state-of-the-art approaches.

**Results on standard long-tailed recognition.** Table 1 demonstrates the effectiveness of our proposed method, PRL, on four benchmark datasets under the standard long-tailed recognition setting, where the test class distribution is uniform. PRL consistently achieves the highest top-1 accuracy across all datasets, outperforming the previous state-of-the-art methods, LSC  and BalPoE . On CIFAR100-LT, PRL improves the accuracy by 0.6% to 0.8% compared to LSC and BalPoE, showcasing its robustness to different imbalance ratios (IR=10, 50, and 100). Similarly, on Places

Figure 3: Analysis of the preference control for the trade-off between head and tail class performance. We present the results on three distributions. The vertical axis represents accuracy. The horizontal axis shows the results after clustering by frequency, from head classes to tail classes (left to right).

LT, iNaturalist 2018, and ImageNet-LT, PRL obtains the best performance, surpassing the existing methods by a clear margin. The superior performance of PRL in the standard long-tailed recognition setting validates the efficacy of our approach in mitigating the bias towards head classes and improving the recognition accuracy of tail classes.

**Results on distribution-shift long-tailed recognition.** We evaluate the performance of PRL and other methods in the distribution-shift long-tailed recognition setting, where the test class distribution is unknown and different from the training distribution. Tables 2 and 3 show the Top-1 accuracy results on various test class distributions (including forward LT, uniform, and backward LT) for CIFAR100-LT (IR=100) and ImageNet-LT, respectively.

On different test distributions of both datasets, PRL consistently outperforms all compared methods. On CIFAR100-LT (IR100), PRL achieves the highest accuracy across all settings, surpassing LSC , BalPoE , and SADE . Even under the most challenging backward LT distribution, PRL can maintain its outstanding performance. On ImageNet-LT, PRL obtains the best results across all test distributions, significantly outperforming LSC, BalPoE, and SADE.

The consistent improvements achieved by PRL highlight its higher performance ceiling, indicating the effectiveness of our method design in overcoming distribution shifts. We further conduct distribution-shift experiments on Places-LT and iNaturalist 2018, where PRL also achieves impressive results. Please refer to the AppendixF for detailed results.

    &  &  &  &  \\    & IR=10 & IR=50 & & & & & \\  Softmax & 59.1 & 45.6 & 41.4 & 31.4 & 64.7 & 48.0 \\ Causal  & 59.4 & 48.8 & 45.0 & 32.2 & 64.4 & 50.3 \\ Balanced Softmax  & 61.0 & 50.9 & 46.1 & 39.4 & 70.6 & 52.3 \\ MiSLAS  & 62.5 & 51.5 & 46.8 & 38.3 & 70.7 & 51.4 \\ LADE  & 61.6 & 50.1 & 45.6 & 39.2 & 69.3 & 52.3 \\ RIDE  & 61.8 & 51.7 & 48.0 & 40.3 & 71.8 & 56.3 \\ SADE  & 63.6 & 53.8 & 48.8 & 40.9 & 72.7 & 58.8 \\ LSC  & 65.0 & 56.5 & 51.8 & 41.3 & 73.9 & 60.2 \\ BalPoE  & 64.8 & 56.3 & 52.0 & 40.8 & 75.0 & 59.3 \\  PRL(ours) & **65.6** & **57.3** & **52.8** & **41.6** & **75.1** & **60.8** \\   

Table 1: Top-1 accuracy on CIFAR100-LT, Places-LT, iNaturalist 2018, and ImageNet-LT, where the test class distribution is uniform.

    &  &  &  &  \\   & & 50 & 25 & 10 & 5 & 2 & 1 & 2 & 5 & 10 & 25 & 50 \\  Softmax & ✗ & 63.3 & 62.0 & 56.2 & 52.5 & 46.4 & 41.4 & 36.5 & 30.5 & 25.8 & 21.7 & 17.5 \\ BS & ✗ & 57.8 & 55.5 & 54.2 & 52.0 & 48.7 & 46.1 & 43.6 & 40.8 & 38.4 & 36.3 & 33.7 \\ MiSLAS & ✗ & 58.8 & 57.2 & 55.2 & 53.0 & 49.6 & 46.8 & 43.6 & 40.1 & 37.7 & 33.9 & 32.1 \\ LADE & ✗ & 56.0 & 55.5 & 52.8 & 51.0 & 48.0 & 45.6 & 43.2 & 40.0 & 38.3 & 35.5 & 34.0 \\ LADE & ✓ & 62.6 & 60.2 & 55.6 & 52.7 & 48.2 & 45.6 & 43.8 & 41.1 & 41.5 & 40.7 & 41.6 \\ RIDE & ✗ & 63.0 & 59.9 & 57.0 & 53.6 & 49.4 & 48.0 & 42.5 & 38.1 & 35.4 & 31.6 & 29.2 \\ SADE & ✗ & 65.2 & 62.5 & 58.8 & 55.4 & 51.2 & 48.8 & 43.0 & 43.9 & 42.4 & 42.2 & 42.0 \\ LSC & ✗ & 67.8 & 64.2 & 60.2 & 58.1 & 53.2 & 51.6 & 44.7 & 45.7 & 44.2 & 44.7 & 48.0 \\ BalPoE & ✗ & 69.0 & 65.2 & 61.2 & 59.0 & 54.2 & 51.7 & 45.7 & 46.6 & 45.2 & 45.2 & 45.8 \\  PRL (ours) & ✗ & **69.5** & **65.7** & **61.7** & **59.5** & **54.7** & **52.2** & **46.2** & **47.1** & **45.7** & **45.7** & **48.5** \\   

Table 2: Top-1 accuracy on CIFAR100-LT (IR100) with various unknown test class distributions.

Figure 4: A more comprehensive example of how preference influences performance.

**User preference control.** We evaluated the model's performance on many-shot, medium-shot, and few-shot classes on CIFAR100-LT under different preference settings (R=(1.0, 2.7), R=(0.5, 2.5), R=(1.9, 1.1)). Table 4 shows that by adjusting the preference value R, we can effectively control the trade-off between many-shot and few-shot classes. When R=(1.0, 2.7), the model performs best on many-shot classes; when R=(1.9, 1.1), the model performs better on few-shot classes at the cost of a slight drop in performance on many-shot classes. R=(0.5, 2.5) achieves the best performance on medium-shot classes, indicating that our method can balance performance across different classes with an appropriate setting. As shown in Figure 3, we analyze the performance trade-offs between head and tail classes across three different distributions, demonstrating how our preference control mechanism allows flexible adjustment of model behavior. The results clearly show how adjusting preferences affects accuracy across different class frequency groups.

Figure 4 provides a more comprehensive visualization of the performance on the head classes under the forward50 distribution. The plane represents the performance of the head classes without inputting any preference, while the red dots indicate the preference positions in polar coordinates that can improve the performance of the head classes, and the green dots represent the preference positions that may degrade the performance. These experimental results demonstrate the effectiveness of our method in controlling the trade-off for long-tailed classes based on user preferences. By adjusting the preference without the need for retraining the model, we can flexibly adapt to different application scenarios and requirements, achieving a desired trade-off in long-tailed recognition tasks that aligns with practical needs.

    &  &  &  \\   & Many & Middle & Few & Many & Middle & Few & Many & Middle & Few \\  Forward & 50 & **61.4** & 50.4 & 36.5 & 61.0 & 52.6 & 31.5 & 61.1 & 48.9 & 40.3 \\  & 25 & **61.6** & 48.3 & 28.4 & 60.6 & 49.6 & 31.5 & 59.7 & 49.4 & 33.1 \\  Uni & 1 & 61.6 & 51.4 & 33.2 & 61.6 & 51.5 & 33.2 & 61.6 & 51.4 & 33.2 \\  Backward & 25 & **63.8** & 49.4 & 31.1 & 60.2 & 48.2 & 32.1 & 63.2 & 48.2 & 32.2 \\  & 50 & **66.6** & 47.1 & 30.6 & 66.1 & 48.9 & 30.9 & 64.6 & 47.8 & 31.7 \\   

Table 4: Control of trade-off preference for long-tailed classes with different preferences, **bold text**, underlined text, and dashed underline respectively indicate the highest performance of the head, middle, and tail classes in this line.

Figure 5: Ablation analysis, including the ablation of the hypernetwork and Chebyshev polynomials.

    &  &  &  &  \\   & & 50 & 25 & 10 & 5 & 2 & 1 & 2 & 5 & 10 & 25 & 50 \\  Softmax & ✗ & 66.1 & 63.8 & 60.3 & 56.6 & 52.0 & 48.0 & 43.9 & 38.6 & 34.9 & 30.9 & 27.6 \\ BS & ✗ & 63.2 & 61.9 & 59.5 & 57.2 & 54.4 & 52.3 & 50.0 & 47.0 & 45.0 & 42.3 & 40.8 \\ MiSLAS & ✗ & 61.6 & 60.4 & 58.0 & 56.3 & 53.7 & 51.4 & 49.2 & 46.1 & 44.0 & 41.5 & 39.5 \\ LADE & ✗ & 63.4 & 62.1 & 59.9 & 57.4 & 54.6 & 52.3 & 49.9 & 46.8 & 44.9 & 42.7 & 40.7 \\ LADE & ✓ & 65.8 & 63.8 & 60.6 & 57.5 & 54.5 & 52.3 & 50.4 & 48.8 & 48.6 & 49.0 & 49.2 \\ RIDE & ✗ & 67.6 & 66.3 & 64.0 & 61.7 & 58.9 & 56.3 & 54.0 & 51.0 & 48.7 & 46.2 & 44.0 \\ SADE & ✗ & 69.7 & 67.5 & 65.4 & 62.3 & 60.3 & 58.3 & 56.7 & 54.9 & 54.3 & 53.1 & 52.6 \\ LSC & ✗ & 72.0 & 69.7 & 67.5 & 65.3 & 62.7 & 60.2 & 59.2 & 58.5 & 57.9 & 57.5 & 57.0 \\ BalPoE & ✗ & 72.2 & 69.7 & 67.2 & 64.3 & 62.2 & 59.5 & 58.5 & 57.7 & 56.9 & 56.7 & 56.6 \\  PRL (ours) & ✗ & **72.7** & **70.2** & **68.0** & **65.8** & **63.2** & **60.7** & **59.7** & **59.0** & **58.4** & **58.0** & **57.5** \\   

Table 3: Top-1 accuracy on ImageNet-LT with various unknown test class distributions.

**Ablation study.** We conduct ablation studies on CIFAR100-LT to evaluate the impact of removing the hypernetwork (w.o. hnet) and removing the Chebyshev polynomial (w.o. stch) on the model's performance under different unknown test class distributions (as shown in Figure 5). The complete model (ours) performs best across all distributions. Removing either the hypernetwork or the Chebyshev polynomial leads to performance degradation, highlighting their importance in dynamically adjusting the model behavior to adapt to distribution shifts and learning preference-aware representations. This ablation study verifies the effectiveness of different components in our method, which work together to better handle unknown test distributions and data imbalance issues in long-tailed recognition.

## 6 Conclusion

This study introduces a novel long-tailed learning paradigm to address distribution shifts between training and testing datasets. Our hypernetwork-based approach generates adaptable classifiers, achieving Pareto optimality for real-time adaptation. During inference, the model adjusts based on user-defined trade-offs between head and tail classes, enhancing flexibility. Empirical results show improved accuracy and adaptability to class imbalances and distribution shifts. Our work establishes an interpretable, generalizable, and controllable framework for long-tailed learning, meeting diverse user needs.