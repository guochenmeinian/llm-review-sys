# Selectivity Drives Productivity: Efficient Dataset

Pruning for Enhanced Transfer Learning

Yihua Zhang\({}^{1,*}\)   Yimeng Zhang\({}^{1,*}\)   Aochuan Chen\({}^{1,*}\)   Jinghan Jia\({}^{1}\)   Jiancheng Liu\({}^{1}\)

**Gaowen Liu\({}^{2}\)   Mingyi Hong\({}^{3}\)   Shiyu Chang\({}^{4}\)   Sijia Liu\({}^{1}\)**

\({}^{1}\) Michigan State University, \({}^{2}\) Cisco Research,

\({}^{3}\) University of Minnesota, Twin City, \({}^{4}\) UC Santa Barbara

\({}^{*}\) Equal Contribution

###### Abstract

Massive data is often considered essential for deep learning applications, but it also incurs significant computational and infrastructural costs. Therefore, dataset pruning (DP) has emerged as an effective way to improve data efficiency by identifying and removing redundant training samples without sacrificing performance. In this work, we aim to address the problem of DP for transfer learning, _i.e._, _how to prune a source dataset for improved pretraining efficiency and lossless finetuning accuracy on downstream target tasks_. To our best knowledge, the problem of DP for transfer learning remains open, as previous studies have primarily addressed DP and transfer learning as separate problems. By contrast, we establish a unified viewpoint to integrate DP with transfer learning and find that existing DP methods are not suitable for the transfer learning paradigm. We then propose two new DP methods, label mapping and feature mapping, for supervised and self-supervised pretraining settings respectively, by revisiting the DP problem through the lens of source-target domain mapping. Furthermore, we demonstrate the effectiveness of our approach on numerous transfer learning tasks. We show that source data classes can be pruned by up to \(40\% 80\%\) without sacrificing downstream performance, resulting in a significant \(2 5\) speed-up during the pretraining stage. Besides, our proposal exhibits broad applicability and can improve other computationally intensive transfer learning techniques, such as adversarial pretraining. Codes are available at [https://github.com/OPTML-Group/DP4TL](https://github.com/OPTML-Group/DP4TL).

## 1 Introduction

The abundance of the training data has long been regarded as the key driver of the contemporary machine learning (ML) algorithms . However, the untrimmed, ever-growing training dataset could not only introduce training biases that compromise the model performance , but also poses an almost insurmountable obstacle to high training efficiency . Therefore, understanding the impact of the training data and selecting the most critical samples has emerged as an important goal, collectively referred to as the problem of _dataset pruning_ (**DP**).

Although the feasibility and the promise of DP have been unveiled in numerous applications, such as noisy data cleansing , continue learning  and active learning , greater emphasis is placed on the _in-domain_ training setting, _i.e._, the pruned training set share the similar distribution as the evaluation set. Examples of methods to condense the training dataset with lossless generalization (on the in-distribution testing dataset) include data influence functions , model training dynamics , and coreset selection . In contrast, our work investigates theproblem of DP in the **transfer learning** paradigm , which has emerged as a popular way to leverage the knowledge of a _foundation model_ learned on a _source_ dataset (referring to '_pretraining_' stage) to further enhance the performance on a cross-domain _target_ dataset (referring to '_finetuning_' stage). Recent evidence  has shown that some source data points could make a _harmful influence_ in the downstream performance. In particular, the previous study  showed that removing specific source data classes can improve the transfer learning accuracy of a pretrained model.

Yet, _efficiently_ identifying those _harmful_ source data classes for improved transfer learning is a highly non-trivial task. Firstly, the unique pretrain-finetune paradigm complicates the analysis of the influence of source data on downstream performance, making it an indirect and very challenging process. Consequently, existing gradient and data influence-based in-domain DP methods  cannot be naively adapted to the transfer setting. Secondly, transfer learning encompasses a wide range of source training methods other than supervised learning, such as self-supervised learning (**SSL**) . Therefore, a generic DP framework is desired given various pretraining scenarios. Thirdly, from an efficiency standpoint, the design of efficient DP algorithms is non-trivial. Even in the non-transfer learning paradigm, a majority of current in-domain DP methods  introduce substantial computational overheads. For instance, influence function-based DP methods  necessitate the calculation of the inverse Hessian matrix of model parameters, which is a highly demanding and computationally intensive process. Moreover, training dynamics-based methods , such as GraNd-Score  and Forgetting-Score , necessitate model training on the entire dataset multiple times. As a result, the development of an efficient and effective DP framework tailored for transfer learning remains a significant challenge in the field.

The most relevant work to ours is , which proposed a brute-force method to evaluate the influence of every source class in the downstream task following a leave-one-out principle. While this method effectively selects the most influential source classes, it is prohibitively slow, as the data selection process demands significantly higher computational resources than performing transfer learning itself. This brings us to the central question addressed in this paper:

_(Q) How can we extend DP to transfer learning with minimal computation overhead, broad applicability, and improved target performance?_

To address **(Q)**, we formally define the task of DP (dataset pruning) for transfer learning. We start by uncovering the limitations of conventional in-domain DP methods when applied to transfer learning tasks, and establish an intrinsic connection between source dataset pruning for transfer learning and source-target domain mapping. In the supervised pretraining paradigm, we propose an effective and scalable label mapping (**LM**)-based framework capable of pinpointing the most beneficial source

Figure 1: **Left: An illustration of the proposed dataset pruning methods (LM and FM) and their performance overview. Large scale source dataset is pruned by LM and FM through a small surrogate model (ResNet-18). Large foundation models can achieve up to \(5\) speed-up on pretraining without no downstream performance drop. Right: Downstream performance overview at different pruning ratios when ResNet-101  is used as the source model (trained on the pruned source dataset) with ImageNet  as the source dataset transferring to OxfordPets  and StanfordCars . The feature extractor is fixed during finetuning.**

data labels for a cross-domain target dataset. Furthermore, we extend the idea of LM to feature mapping (**FM**) for the SSL pretraining protocol, where data labels are unavailable. To achieve a greater practical impact, we demonstrate that our proposed methods (LM and FM) can facilitate effective DP over a small, simple surrogate source model, and further translate the positive impact of the pruned source dataset to other larger, more complex source models. We provide a schematic overview and result highlights in **Fig. 1**. Our contributions can be summarized as follows:

We connect the concept of DP (dataset pruning) to transfer learning for the first time and formalize the problem of DP for transfer learning.

We develop a highly efficient and effective framework for DP in transfer learning, leveraging the source-target domain mapping with customized LM (label mapping) and FM (feature mapping) for both supervised and SSL (self-supervised learning) settings. Our approach is principled and achieves significant speedups compared to the state-of-the-art methods.

We empirically show the effectiveness of our proposals (LP and FP) on 8 downstream tasks. We find that the source dataset (ImageNet) can be pruned up to \(40\% 80\%\) without sacrificing the downstream performance, together with a \(2 5\) speed-up in the pretraining stage. Our proposal also unlocks a door to prune a dataset using a simple surrogate source model (_e.g._, ResNet-18) and then reuse the pruned dataset to improve transfer learning on a larger source model (_e.g._, ResNet-101).

Lastly, we show that our proposed DP framework can benefit other computationally-intensive transfer learning techniques. For example, DP for adversarial pretraining  leads to a \(1\% 2\%\) improvement in downstream performance with time consumption similar to standard pretraining.

## 2 Related Work

Dataset pruning.DP (dataset pruning) is an emerging technique to improve the data efficiency of model training by selecting the most representative training samples or removing the less influential ones . Thus, the problem of coreset selection  can also be considered a form of DP. Prior studies on DP range from clustering-based methods  to the more recent score-based pruning methods . In the latter approach, an importance score is assigned to each training data point to quantify its influence on a particular permanence metric during model learning. Specifically, score-based DP methods can be broadly categorized into two main groups: influence function-based approaches  and training dynamics-based approaches . The first category measures data influence by examining the effect of data removal on the learning algorithm used during training  and the model's prediction . However, influence function-based approaches typically require high computational costs due to the need for high-order derivatives and complex optimization methods, such as bi-level optimization, as used in . Although an approximate influence score can be obtained efficiently, it may result in a large estimation error . In the second category, training dynamics-based approaches find statistical indicators for pruned data from the training trajectory. Examples of DP metrics include data loss/error , prediction confidence , model gradient norm , forgetting event , and compactness . However, these methods typically require repeated training to ensure the representativeness of the collected statistics . In addition to improving data efficiency, DP has been applied in a variety of contexts, such as noisy label cleansing , continue learning , active learning , and reducing annotation cost of a finetuner .

Data valuation & attribution.Data valuation  and attribution  are research streams related to dataset pruning that aim to quantify the influence of training data points on model's performance. Unlike dataset pruning, these approaches do not focus primarily on training efficiency but instead aim to enhance interpretability , adversarial robustness , generative model design , and data acquisition quality . Representative methods include Shapley value-based methods , datamodels , sampling-based methods , and proxy-based methods . Recently, Kim et al.  proposed to select data samples from a large public dataset (Open-Set) for self-supervised learning given a specific target dataset through distribution mismatch. However, it fails to make a general framework in both supervised and self-supervised scenarios, leaving the former under-explored. The most relevant work to ours is , which leverages a leave-one-out analysis to quantify the influence of source data on downstream tasks in the context of transfer learning. With this inspiration, we propose to connect DP to transfer learning in this work.

Recent advancements in transfer learning.Transfer learning has been a prominent area over the past decade . Significant strides have been made in understanding, analyzing, and improving various aspects of this technique . Recent studies  have rigorously analyzed the relationship between source model performance and downstream task effectiveness, arguing that a narrow focus on minimizing source training loss may not lead to improved transfer learning results. To improve transfer learning, adversarial training has been shown to benefit the transferability of source pretraining on target downstream tasks . There also exist studies to identify and understand the failure cases in transfer learning . Other research  has examined model transferability from the perspective of model sharpness and argues that a good pretrained model should be situated in a flat basin in the downstream loss landscape. Last but not the least, transfer learning has progressed in several other directions, such as self-supervised learning , model weight pruning , and visual prompting .

## 3 Problem Formulation

In this section, we introduce some essential preliminaries on transfer learning and DP (dataset pruning), and elucidate the design challenge of DP for transfer learning.

Preliminaries on transfer learning and connection to DP.Let \(g f\) denote a deep neural network that consists of a feature extractor \(f\) and a classification head \(g\), where \(\) denotes the function composition. Given source and target datasets \(_{}\) and \(_{}\), we study transfer learning in the "_pretrain-finetune_" paradigm. The primary goal of _pretraining_ is to obtain a high-quality feature extractor \(f_{}:\), which draws a mapping from the input space (\(\)) to the deep representation space (\(\)) in a data-rich source domain (\(_{}\)). Popular pertaining recipes include supervised learning (**SL**)  and self-supervised learning (**SSL**)  depending on whether the source labels (\(y_{}\)) are available or not in \(_{}\). In the _finetuning_ stage, the pretrained model is further trained on a specific downstream task under the target dataset \(_{}\). Transfer learning expects improved downstream performance over training on \(_{}\) from scratch. In this work, we consider two finetuning protocols, linear probe (**LP**) and full-finetune (**FF**), with \(_{}\) being a labeled dataset. LP finetunes the linear classification head \(g\) with a fixed feature extractor \(f_{}\), acquired from pretraining. In contrast, FF finetunes the entire model \(g f\) from the initialization \(f_{}\). FF typically yields a better transfer learning accuracy than LP, but the former takes a higher computation cost.

Our motivation for connecting transfer learning with DP comes from a recent data-based perspective on transfer learning . The study shows that removing certain _source data classes_ from \(_{}\) can potentially improve the accuracy of a finetuned model on \(_{}\). However, the task of evaluating the transfer effects of source data and removing their influence from a pre-trained source model has not been addressed efficiently. The approach developed in  involves a leave-one-out analysis to estimate the influence of a source class \(c\) on a target example \(t\), which is computed as the prediction discrepancy of the finetuned source model at \(t\) when the class \(c\) is either included or excluded from \(_{}\). During this process, one must train multiple source models (over \(7000\) models on ImageNet in ) from scratch over different subsets of \(_{}\) for a given target task. This approach becomes computationally unaffordable when dealing with large source datasets like ImageNet given limited computing resources. To address this challenge, we propose a DP perspective on transfer learning.

Problem of interest: DP for transfer learning.Next, we introduce the background of DP and the problem we focus on. Let \(=\{_{1},_{2},,_{N}\}\) denote a dataset consisting of \(n\) samples, where each sample \(z_{i}\) is represented as a pair \((_{i},y_{i})\), with \(_{i}\) denoting the input feature vector and \(y_{i}\) denoting the corresponding label. DP aims to generate a pruned dataset \(}=\{}_{1},}_{2},,}_{M}\}\) with \(M<N\), which can reduce the training cost without a significant decrease in model generalization performance when trained on \(}\). In the context of , instead of individual source data sample \(\{}_{i}\}\), the entire source classes are evaluated and selected in terms of transfer influences. Based on the above, we define the problem of our interest below.

**(DP for transfer learning)** How to _prune_ source data classes to obtain \(}_{}\) (a subset of \(_{}\)), with lossless or improved transfer learning accuracy of the source model (\(f_{}\)) on a target task \(_{}\)?

DP for transfer learning has two key distinctions (\(}\)-\(}\)) from the vanilla DP setup. First (\(}\)), DP must be performed in the source domain (\(_{}\)), while its effectiveness is evaluated based on the target domain (\(}\)). This 'cross-domain' challenge makes the design of efficient and effective DP highly non-trivial. For example, prior work  utilizes a computationally-intensive leave-one-out analysis. Classical influence function-based methods [10; 19; 20; 21; 22; 23; 101], which trace the eventual model's prediction through the learning algorithm and back to its training data, are also computationally infeasible due to the complex bi-level optimizer and the calculation of high-order derivatives. Second (), the pre-trained source model (\(f_{}\)) in today's transfer learning regime is typically of a large scale. This motivates us to develop a DP method that can be independent of the source model, while the pruned source dataset \(}_{}\) remains effective in the original transfer learning setup. Given this challenge, we will design DP methods for transfer learning using a _simple surrogate source model_ to avoid the computation on the large source model \(f_{}\) (see **Fig. 1** for an illustration).

**Conventional DP methods lack effectiveness on transfer learning.** Given the challenges () posed by DP for transfer learning, we further conduct a preliminary study to investigate the effectiveness of existing 8 DP methods, including SP , SSP , GraNd , EL2N , Moderate, Forget, InfMax, and InfSum. Our results show that these methods are _unable_ to yield significant improvements over _random_ source data pruning. **Fig. 2** shows the transfer learning performance of the ResNet-101  model trained on different pruned versions of the ImageNet dataset when LP-based finetuning is conducted on the downstream Flowers102  and OxfordPets  datasets. As we can see, in transfer learning, random pruning is a solid baseline for various state-of-the-art DP methods, which have demonstrated superior performance to the former in the non-transfer learning regime. Therefore, it is crucial to develop an efficient and effective DP method specifically tailored for transfer learning.

## 4 Label/Feature Mapping-based DP for Transfer Learning

In this section, we first introduce a simple yet powerful DP method called label mapping (**LM**) by leveraging the class-discriminative capability of a supervised source model. We then extend the concept of LM to feature mapping (**FM**), suitable for self-supervised pretraining.

**LM-based DP for supervised pretraining.** Following the notations used in Sec. 3, we represent the model obtained through supervised learning on the source dataset (\(}\)) as \(g_{} f_{}\). This model predicts a source label given an input example (\(\)). In DP for transfer learning, the source data classes serve as the variables to be pruned. Thus, we express \(}\) as \(}=\{_{}^{(1)},,_{}^{(N)}\}\) for \(N\) source classes, \(_{}^{(i)}\) denotes the set of data points belonging to the source class \(i\). Additionally, the pruner has access to the target dataset \(}=\{_{1},,_{n}\}\), which consists of \(n\) target data points. Our objective is to utilize the information provided by \(g_{} f_{}\), \(}\), and \(}\) to devise a computationally efficient DP criterion. Importantly, our criterion should not involve model training, distinguishing it from the non-scalable approach in . An important observation by  in transfer learning is that transferred knowledge improves transferability. This insight is supported by the loss landscape analysis in transfer learning: The finetuned weights may remain within the flat basin of the pretrained weights for enhanced transfer learning performance.

We next propose to extract the 'transferred knowledge' by leveraging the class-discriminative capability of the source model \(g_{} f_{}\) on the target data samples in \(}\). Specifically, we focus on monitoring the responsiveness of the source label predictions made by \(g_{} f_{}\) when using target samples \(\{_{i}}\}_{i=1}^{n}\) as input data. Here we resize these target samples to ensure their resolution alignment with source data. For the \(i\)th source data class \(_{}^{(i)}\), we then define its pruning score below:

\[_{}(_{}^{(i)})=_{j=1}^{n} (g_{} f_{}(_{j})=i),\ \ \ i=1,2,,N,\] (LM)

Figure 2: Transfer learning accuracy of existing DP methods on ImageNet at different pruning ratios, where ResNet-101 is the source model, and linear probing (LP) is used for downstream finetuning on the target datasets Flowers102 (**Left**) and OxfordPets (**Right**).

where \(1()\) represents an indicator function that evaluates to \(1\) when the condition \(\) is satisfied, and \(0\) otherwise. We refer to the aforementioned formula as LM (Label Mapping) since the condition \(g_{} f_{}(_{j})=i\) establishes a mapping between the predicted labels of the target samples \(\{_{j}\}_{j=1}^{n}\) by the source model and the corresponding source labels \(i\). The larger \(_{}\) in (LM) signifies that a specific source class is more frequently utilized to interpret the target data. Consequently, this indicates a more tightly connected relationship between the source class and the transferred knowledge. Therefore, we prune (or select) source data classes with low (or high) \(_{}\) values.

Although LM is simple, it offers several advantages. _Firstly_, it is highly efficient in computation. Given a pre-trained source model \(g_{} f_{}\), the calculation of LM only requires forward passes through the model. In cases where obtaining the pretrained model in the source domain is challenging, our proposal supports an alternative approach: training a _smaller and simpler surrogate model_ to carry out LM. This surrogate model can effectively replace the complex pretrained model \(g_{} f_{}\) and facilitate the efficient execution of the pruning process. As we show in **Fig. 2(a)**, employing ResNet-18  is sufficient to successfully prune the source dataset (ImageNet ). The resulting DP scheme remains effective to improve transfer learning utilizing other larger source models, such as ResNet-101 . _In addition_, **Fig. 2(b)** shows that the computational overhead incurred by training the small surrogate model (ResNet-18) for DP is insignificant, compared to the time saved during the pretraining phase of a larger source model (ResNet-101) on the pruned dataset for transfer learning. _Lastly_, pretraining on the subset found by LM can guide the source model towards a flatter region in the downstream loss landscape, (see results in **Fig. A4)**. The source model trained on the LM-pruned dataset achieves a higher flatness score than baselines, which aligns with the understanding of transfer learning in .

**FM-based DP framework for self-supervised pretraining.** The requirement for _labeled_ source data in LM may pose limitations on the application of DP methods, particularly in the context of self-supervised learning (**SSL**)-based pertaining. To address this limitation, we introduce a new method called FM (Feature Mapping). Unlike LM, FM determines the DP scheme using only the feature extractor network \(f_{}\), the unlabeled source dataset \(_{}\), and the target dataset \(_{}\). This allows us to overcome the dependence on labeled source data, making FM applicable in SSL scenarios. The inspiration for FM is derived from the deep clustering technique  operating in the representation space, which can generate _pseudo source labels_ using _cluster indices_ provided by _e.g._, \(K\)-means. With the assistance of deep clustering, we can represent the unlabeled source dataset \(_{}\) as \(_{}=\{_{}^{(1)},_{ }^{(2)},,_{}^{(K)}\}\), where \(_{}^{(k)}\) denotes the set of source data samples within cluster \(k\). Building upon the similar spirit as LM, we propose ranking the importance of pseudo source classes for DP by evaluating the source feature extractor's responsiveness to target data samples. To achieve this objective, we quantify the responsiveness of \(f_{}\) for a target sample \(\) as follows:

\[r()=*{arg\,min}_{k[K]}\ \|f_{}( )-_{_{}^{(k)}}[f_{ }()]\|_{2}, \]

where \(_{_{}^{(k)}}[f_{}( )]\) is the centroid of source data within the cluster \(k\), and \(r()\) is the nearest pseudo label as the responsiveness of \(f_{}\) against \(\). The FM score then integrates (1) with (LM):

\[_{}(_{}^{(i)})=_{j=1}^{n} (r(_{j})=i),\ \ \ i=1,2,,K,\] (FM)

where different from (LM), \(K\) represents the number of pseudo source classes produced by deep clustering, and \(r(_{j})\) corresponds to the source feature extractor's prediction on \(_{j}\). It is important to note that the value of \(K\) is a free parameter of the deep clustering process. Our empirical study in Fig. A2 shows that FM is quite robust to the choice of \(K\) without sacrificing the benefit of DP for

Figure 3: Preliminary studies on the usage of surrogate models for LM. **(a)** The downstream performance (on Flowers102) of using the source model ResNet-101 trained on the pruned ImageNet delivered by LM at different pruning ratios. Here LM is conducted using either RN-101 or a smaller surrogate model ResNet-18. **(b)** Computation time decomposition analysis for obtaining the pretrained model using ResNet-18 as surrogate model with different pruning ratios.

transfer learning compared to using the unpruned source dataset. Lastly, it is worth mentioning that FM can also be applied in the context of supervised pretraining by specifying data labels as clusters.

## 5 Experiments

In this section, we provide a comprehensive set of experiments and analyses to showcase the effectiveness of our proposed methods (LM and FM) in diverse transfer learning scenarios.

### Experiment setup

**Datasets and models.** In line with existing transfer learning benchmarks [41; 106], we utilize ImageNet-1K  for pretraining and **8** datasets as downstream tasks. These datasets include DTD , Flowers102 , UCF101 , Food101 , SUN397 , OxfordPets , StanfordCars , and CIFAR10 . Please refer to Tab. A1 for more details about the datasets. As discussed in Sec. 4, we utilize ResNet-18 (RN-18)  as the surrogate source model for pruning source classes. This method significantly reduces the computational cost associated with DP, making the process more efficient. Subsequently, a range of larger models, _e.g.,_ ResNet-101 (RN-101) and ViT-B/16 , are trained on the (pruned) ImageNet and then finetuned on downstream tasks.

**Baselines, training, and evaluation.** By examining the performance of the existing **8** DP baselines as shown in Fig. 2, our experiments focus on two of the most effective methods: 1@ GraNd and 2@ Moderate, together with 3@ Random (the random pruning strategy). In Fig. A1, we show more results of the rest DP baselines. Unfortunately, we are unable to include the existing data attribution method  as our baseline, as it does not release its pruning results, and we are unable to repeat its experiments due to the need for intensive computations. Unless specified otherwise, we focus on the supervised pretraining setting in the experiments. For self-supervised pretraining, we follow the implementation of MoCov2 . The finetuning strategies employed include LP (linear probing), which finetunes the classification head with fixed feature extractor, and FF (full finetuning), which finetunes the entire source model. For FM-based DP method, we utilize K-means clustering to group the ImageNet training data points into \(K=2000\) clusters for the computation of (1).

In accordance with the terminology convention in model pruning [87; 113; 114; 115], we refer the term '**winning subsets'** to the obtained source subsets that do _not_ compromise downstream performance. Among these winning subsets, we identify the one with the highest pruning ratio as the '**best winning subset'**. We then evaluate the performance of DP methods from the following aspects: the downstream performance of the model pretrained on the pruned source dataset obtained by various DP methods, and the pruning ratio of the best winning subset achieved by DP methods, accompanied by the corresponding time saved in the pretraining phase.

### Experiment results

**LM/FM improves transfer learning accuracy by identifying 'winning subsets'.** We first showcase the significant improvements achieved by our proposed DP methods (LM and FM) compared to baselines. Our methods successfully identify winning subsets of the ImageNet, yielding transfer accuracy on par or even better than scenarios without pruning.

**Fig. 4** presents the downstream accuracy of transfer learning vs. different pruning ratios. Here DP is performed using the surrogate model (RN-18) on ImageNet for \(8\) downstream tasks. The source model (RN-101) is then trained on the pruned ImageNet and the transfer learning accuracy is assessed through LP (linear probing) and FF (full finetuning). We also present the downstream performance without pruning the source dataset (No Prune) as a reference for winning subsets. As we can see, both LM and FM significantly outperform the baselines by a substantial margin. Notably, LM and FM consistently identify winning subsets with significantly larger pruning ratios in all settings. This highlights the effectiveness of our proposed methods in achieving substantial dataset pruning without hurting downstream performance. Furthermore, we observe that the downstream performance of LM and FM initially improves and then declines as the pruning ratio increases. This is not surprising, since the initial increase in performance corresponds to the scenario where harmful source classes are removed, consistent with . When the source dataset continues to shrink, the performance inevitably decreases, as a natural consequence of reducing the size of the source dataset. Moreover, in some datasets of small sizes (_e.g.,_ OxfordPets), LP achieves performance comparable to FF. Thisis attributed to the fact that the performance gap between LP and FF using the large-scale RN-101 model on small-scale datasets tends to diminish.

   Dataset &  &  &  &  \\ Panning Ratio & 0\% & 50\% & 60\% & 70\% & 80\% & 0\% & 0\% & 50\% & 60\% & 70\% & 80\% & 0\% & 50\% & 60\% & 70\% & 80\% \\  Random & & 63.32 & 61.27 & 59.09 & 53.75 & & 45.63 & 45.08 & 45.54 & 39.81 & & 82.23 & 82.60 & 81.03 & 80.02 \\ Mogenic+ & 69.26 & 63.37 & 62.45 & 63.31 & 57.42 & 47.24 & 45.73 & 45.54 & 44.23 & 40.82 & 85.17 & 82.45 & 81.45 & 81.69 & 81.32 \\ GAND & 69.26 & 64.23 & 63.41 & 54.62 & 74.36 & 45.72 & 45.85 & 45.24 & 41.72 & 85.17 & 82.85 & 82.44 & 82.14 & 81.73 \\ FM (ours) & **69.92** & **60.99** & **70.29** & **70.21** & **48.46** & **48.58** & **47.90** & **46.00** & & **85.22** & **85.42** & **84.37** & **84.61** \\   

Table 2: The downstream performance with different source data pruning ratios in the SSL pretraining setting. A randomly initialized RN-101 is self-supervised pretrained using MoCo v2 on each full/pruned source dataset and finetuned on the downstream task through LP. The best result in each pruning ratio is marked in **bold** and the performance surpassing the unpruned setting (pruning ratio \(0\%\)) is highlighted in \(\).

Figure 4: Source dataset pruning trajectory given by downstream testing accuracy (%) vs. source dataset pruning ratio (%) in the supervised pretraining setting. Here the source model RN-101 is trained on each full/pruned source dataset (ImageNet) and finetuned on different downstream tasks through LP and FF. The downstream performance without pruning (No Prune) is marked with the black dashed line. Results are averaged over three independent trials (see exact numbers and variances in Tab. A2).

   Dataset &  &  &  &  \\ Panning Ratio & 0\% & 50\% & 60\% & 70\% & 80\% & 0\% & 50\% & 60\% & 70\% & 80\% & 0\% & 50\% & 60\% & 70\% & 80\% \\  Random & & 63.32 & 61.27 & 59.09 & 53.75 & & 45.63 & 45.08 & 43.54 & 39.81 & & 82.23 & 82.60 & 81.03

**Tab. 2** highlights the effectiveness of FM-based DP in the self-supervised pretraining setup for three representative downstream tasks. As we can see, the transfer learning accuracy achieved by using FM consistently outperforms baselines in the self-supervised pretraining paradigm. FM can identify winning subsets for transfer learning even in the challenging regime of large pruning ratios, ranging from \(50\%\) to \(80\%\). For instance, in the case of SUN397, FM-based winning subsets achieves a pruning ratio up to \(70\%\), and for Flowers102 the maximum pruning ratio is \(60\%\). These pruning merits align with the findings for FM in the supervised pretraining setup, as illustrated in Tab. 1.

**DP enhances the efficiency of source pretraining. Tab. 3** displays the computation time required to obtain the pretrained source model using LM at different pruning ratios. The reported time consumption includes the entire pipeline, encompassing surrogate model training (RN-18), DP process, and source model training (RN-101) on the pruned ImageNet dataset. The runtime cost of the conventional transfer learning on the full ImageNet dataset for RN-101 is also listed as a reference. As we can see, DP enjoys high efficiency merit of source training. Taking the \(5.4\) hours required for source training on the full ImageNet dataset as a reference, LM-enabled \(20\%\) pruned ImageNet achieves a \(15\%\) reduction in training time. Moreover, the efficiency advantage increases to \(76\%\) when the pruning ratio reaches \(80\%\) and these computational benefits do not sacrifice transfer learning accuracy at all.

Next, we compare the efficiency of our methods with the state-of-the-art dataset pruning baselines in Tab. 4, including GraNd, Moderate, and the brute-force method proposed in . We showcase the efficiency comparison with a pruning ratio of \(60\%\). As we can see, our method (even using a surrogate model) achieves a substantial computation efficiency improvement over other methods. In particular,  involves training thousands of source models. Thus, its computational cost is typically unaffortable in experiments.

**DP enables efficient adversarial pretraining. Tab. 5** showcases the improvement in transfer learning accuracy and efficiency achieved by our proposed LP-based DP method when incorporating _adversarial training_ (**AT**)  on either the full or pruned ImageNet dataset. This transfer learning setup is motivated by the findings in , showing that enhancing the robustness of the source model against adversarial attacks through AT can improve transfer learning accuracy in both LP and FF-based finetuning scenarios. We then report downstream accuracies using LP and FF on two specific downstream datasets chosen due to the large room for improvement in transfer learning accuracy, as shown in Fig. 4. We also determine the pruning ratios for LM by selecting those that led to the best winning subsets. Our experimental results demonstrate that employing AT on both the unpruned and pruned source datasets can improve transfer learning accuracy. Specifically, we refer to AT on the original unpruned ImageNet dataset as Dense-AT, and AT on the LM-pruned ImageNet dataset as LM-AT. One notable advantage of integrating LM into AT is the significant improvement in computation efficiency. A key highlight of this approach is that LM-AT achieves a similar computation time as the standard source training on ImageNet (Dense), while exhibiting almost no accuracy drop compared to Dense-AT. This observation demonstrates the potential to accelerate AT through DP.

**Robustness against the surrogate model size.** To explore the sensitivity of our proposed method to the surrogate model's size and accuracy, we show the transfer learning performance on the task Oxford-Pets against different surrogate model sizes in **Fig. 5**. It is evident that even though the performance of the surrogate model on the source dataset (ImageNet) decreases, the downstream performance of RN-101 pretrained on the LM-based pruned source subsets remains relatively stable. Further, **Tab. A6** compares the class indices selected by the corresponding surrogate models. Interestingly, the most relevant source class selections exhibit a high level of agreement across surrogate models of differing sizes.

  Method & Moderate & GraNd & Brutie-Force & ours \\  Time &  &  &  &  \\ Consumption (h) & & & & & \\  

Table 4: Time consumption comparison with a pruning ratio of \(60\%\) of different dataset pruning methods. Other settings follow Fig. 4.

   Pruning Ratio & 0\% & 20\% & 40\% & 60\% & 80\% \\  Time &  &  &  &  &  \\ Consumption (h) & & & & & \\   &  &  &  &  &  \\ Consumption (h) & & & & & \\   

Table 3: Time consumption of LM/FM in Fig.4 to obtain the pretrained model. The report time consumption covers surrogate model (RN-18) training, LM/FM dataset pruning, and source model pretraining (RN-101).

   Method &  &  &  &  &  \\   &  Ratio \\  &  \(}{}\) \\  &  \(}{}\) \\  &  \(}{}\) \\  &  \(}{}\) \\  &  \(}{}\) \\  &  \(}{}\) \\  &  \(}{}\) \\  &  \(}{}\) \\  &  \(}{}\) \\  \\  Dense &  &  &  &  &  &  &  &  \\ Loss & & & & & & & \\   Loss \\ LM \\  } &  &  &  &  &  &  &  &  &  \\ Loss & & & & & & & \\   Loss \\ LM \\  } &  &  &  &  &  &  &  &  &  &  \\ Loss & & & & & & & \\   Loss \\ LM \\  } &  &  &  &  &  &  &  &  &  \\ Loss & & & & & & & \\   Loss \\ LM \\  } &  &  &  &  &  &  &  &  &  \\ Loss & & & & & & & \\   

Table 5: Downstream performance of models pretrained on full/pruned source dataset (Dense/LM) w/wo adversarial pretraining (Adv). For DenseAdv and LM-Adv, \(3\)-step adversarial training  is used for pretraining. Pruning ratio, downstream test accuracy (Acc.), and time consumptions for obtaining pretrained models are reported.

[MISSING_PAGE_FAIL:10]