# Is Your LiDAR Placement Optimized for

3D Scene Understanding?

 Ye Li\({}^{1}\) Lingdong Kong\({}^{2}\) Hanjiang Hu\({}^{3}\) Xiaohao Xu\({}^{1}\) Xiaonan Huang\({}^{1}\)

\({}^{1}\)University of Michigan, Ann Arbor \({}^{2}\)National University of Singapore

\({}^{3}\)Carnegie Mellon University

https://github.com/ywyeli/Place3D

###### Abstract

The reliability of driving perception systems under unprecedented conditions is crucial for practical usage. Latest advancements have prompted increasing interest in multi-LiDAR perception. However, prevailing driving datasets predominantly utilize single-LiDAR systems and collect data devoid of adverse conditions, failing to capture the complexities of real-world environments accurately. Addressing these gaps, we proposed Place3D, a full-cycle pipeline that encompasses LiDAR placement optimization, data generation, and downstream evaluations. Our framework makes three appealing contributions. 1) To identify the most effective configurations for multi-LiDAR systems, we introduce the Surrogate Metric of the Semantic Occupancy Grids (M-SOG) to evaluate LiDAR placement quality. 2) Leveraging the M-SOG metric, we propose a novel optimization strategy to refine multi-LiDAR placements. 3) Centered around the theme of multi-condition multi-LiDAR perception, we collect a 280,000-frame dataset from both clean and adverse conditions. Extensive experiments demonstrate that LiDAR placements optimized using our approach outperform various baselines. We showcase exceptional results in both LiDAR semantic segmentation and 3D object detection tasks, under diverse weather and sensor failure conditions.

## 1 Introduction

Accurate 3D perception plays a crucial role in autonomous driving, involving detecting the objects around the vehicle and segmenting the scene into meaningful semantic categories. LiDARs are becoming crucial for driving perception due to their capability to capture detailed geometric information about the surroundings . While the latest models achieved promising accuracy on standard datasets, _e.g._, nuScenes  and SemanticKITTI , improving the resilience of perception under corruptions and sensor failures is still a critical yet under-explored task .

Recent studies have primarily focused on refining the sensing systems by designing new algorithms with novel model architectures  or 3D representations . The selection of LiDAR configurations, however, often relies on industry experience and design aesthetics. Therefore, existing literature has potentially overlooked optimal LiDAR placements for maximum sensing efficacy.

An intuitive approach for optimizing LiDAR configurations involves a comprehensive cycle of data collection, model training, and validation across various LiDAR setups to enhance the autonomous driving system's perception accuracy . However, this approach faces significant challenges due to the substantial computational resources and extensive time required for data collection and processing . Although recent works  made preliminary attempts to explore the impactof LiDAR placements on perception accuracy, they have neither proposed an optimization method nor evaluated the performance in adverse conditions.

In this work, we delve into the optimization of sensor configurations for autonomous vehicles by tackling two critical sub-problems: 1) the performance evaluation of sensor-specific configurations, and 2) the optimization of these configurations for enhanced 3D perception tasks, encompassing 3D object detection and LiDAR semantic segmentation. To achieve this goal, we propose a systematic LiDAR placements evaluation and optimization framework, dubbed Place3D. The overall pipeline is endowed with the capability to synthesize point cloud data with customizable configurations and diverse conditions, including common corruptions, external disturbances, and sensor failures.

We first introduce an easy-to-compute Surrogate Metric of Semantic Occupancy Grids (M-SOG) to evaluate the equality of LiDAR placements. Next, we propose a novel optimization approach utilizing our surrogate metric based on Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to find the near-optimal LiDAR placements. To verify the correlation between our surrogate metric and assess the effectiveness of our optimization approach on both clean and adverse conditions, we design an automated multi-condition multi-LiDAR data simulation platform and establish a comprehensive benchmark consisting of a total of seven LiDAR placement baselines inspired by existing self-driving configuration from the autonomous vehicle companies.

Our benchmark, along with a large-scale multi-condition multi-LiDAR perception dataset, encompasses state-of-the-art learning-based perception models for 3D object detection [51; 107; 63; 109] and LiDAR semantic segmentation [18; 111; 86; 117; 50], as well as six distinct adverse conditions coped with weather and sensor failures [46; 95]. Utilizing the proposed framework, we explored how various perturbations and downstream 3D perception tasks affect optimization outcomes.

To summarize, this work makes the following key contributions:

* To the best of our knowledge, Place3D serves as the first attempt at investigating the impact of multi-LiDAR placements for 3D semantic scene understanding in diverse conditions.
* We introduce M-SOG, an innovative surrogate metric to effectively evaluate the quality of LiDAR placements for both detection (sparse 3D) and segmentation (dense 3D) tasks.
* We propose a novel optimization approach utilizing our surrogate metric to refine LiDAR placements, which exhibit excellent LiDAR semantic segmentation and 3D object detection accuracy and robustness, outperforming baselines for relatively 9% in metrics.
* We contribute a 280,000-frame multi-condition multi-LiDAR point cloud dataset and establish a comprehensive benchmark for LiDAR-based 3D scene understanding evaluations. We hope this work can lay a solid foundation for future research in this relevant field.

## 2 Related Work

**LiDAR Sensing.** LiDAR sensing is critical in autonomous vehicles, providing essential structural information for their operation [59; 3; 73]. Utilizing 3D data, LiDAR supports tasks such as semantic scene understanding [7; 26; 29; 6; 85; 48; 10], generation [75; 120; 74; 97], decision making [25; 8; 19; 87], and simulation [23; 68; 67; 93]. This work specifically targets LiDAR-based perception and simulation, which are at the forefront of current research in autonomous vehicle technology.

**LiDAR-Based 3D Scene Understanding.** LiDAR segmentation and 3D object detection are key tasks for 3D scene understanding. Segmentation models are categorized into point-based [88; 35; 36; 110; 77], range view [70; 90; 21; 98; 113; 16; 47; 2; 45; 99], bird's eye view [111; 115; 12], voxel-based [18; 86; 117; 33], and multi-view fusion [58; 38; 78; 60; 62; 14; 13; 61; 100] methods. While they show promising results on benchmarks, their performance across different LiDAR configurations is not well explored. For 3D object detection, models typically use point-based [82; 106; 105; 116] or voxel-based [114; 51; 69; 92; 83; 55; 66; 104] representations, with recent works favoring fusion for improved detection [64; 80; 81; 63; 56]. Our study complements these approaches by optimizing LiDAR placements to enhance performance under various real-world conditions.

**LiDAR Perception Robustness.** The reliability of LiDAR-based 3D scene understanding models in real-world scenarios is crucial [84; 39]. Recent studies have explored the robustness of 3D perception models against adversarial attacks [112; 96], common corruptions [46; 49; 94; 103; 43; 4], adverse weather [31; 30; 79; 37], sensor failures [108; 15; 28], and combined motion and sensor perturbations[102; 95]. To our knowledge, no prior work has focused on optimizing LiDAR placement to improve semantic 3D scene understanding robustness. Our Place3D addresses this gap by studying various placement strategies to enhance robustness in both in-domain and out-of-domain scenarios.

**Sensor Placement Optimization.** Optimizing sensor placements can enhance performance and address the challenges of heuristic design [42; 101]. In autonomous driving, optimizing LiDAR placements is relatively new . Hu _et al._ studied multi-LiDAR placements for 3D object detection, while Li _et al._ examined LiDAR-camera configurations for multi-modal detection. Other works [41; 44; 9; 40] explored roadside LiDAR placements for V2X applications. Distinguishing from these efforts, our work is the first to investigate LiDAR placements for robust 3D scene understanding in challenging conditions. We establish benchmarks for both LiDAR semantic segmentation and 3D object detection, providing an in-depth analysis of optimal placements for robust perception.

## 3 Place3D: A Full-Cycle LiDAR Placement Pipeline

In this section, we introduce the Surrogate Metric of Semantic Occupancy Grids (M-SOG) to assess the 3D perception performance of sensor configurations (Section 3.2). Leveraging M-SOG, we propose a novel optimization strategy for multi-LiDAR placement (Section 3.3). Our approach is theoretically grounded; we provide an optimality certification to ensure that optimized placements are close to the global optimum (Section 3.4). Figure 1 depicts an overview of our Place3D framework.

### Preliminary Formulation of Surrogate Metric

**Region of Interest.** Following the literature of sensor placement [34; 57], we define the Region of Interest (ROI) for perception as a finite 3D cuboid space \([l,w,h]\) with ego-vehicle in the center, and divide the ROI space \(S\) into voxels with resolution \([_{l},_{w},_{h}]\) as \(S=\{v_{1},v_{2},...,v_{N}\}\), where \(N=}}}\) denotes the total number of voxels.

**Probabilistic Occupancy Grids (POG).** We define POG as the joint probability of all non-zero voxels in the ROI as \(p_{POG}=p(v_{1},v_{2},...,v_{N})=_{i=1,\ p(v_{i}) 0}^{N}p(v_{i})\), where each voxel is assumed to be independently and identically distributed. The voxel \(v_{i}\) is occupied if it is within any 3D bounding box \(b_{c}\) of object class \(c\) at frame \(t\), which is denoted as \(v_{i}=b_{c}^{(t)}\). Therefore, the probability of being occupied for voxel \(v_{i}\) among all \(T\) frames can be estimated as \(p(v_{i})=_{t=1}^{T}(v_{i}=b_{c}^{(t)})/T\).

**Probabilistic Surrogate Metric.** Now given some LiDAR placement \(L\), the conditional POG can be found as the conditional joint distribution \(p_{POG|L}=p(v_{1},v_{2},...,v_{N}|L)\) in the literature .

Figure 1: Place3D **pipeline for multi-LiDAR placement optimization.** We first utilize the semantic point cloud synthesized in CARLA (a) to generate Probabilistic SOG (b) and obtain voxels covered by LiDAR rays to compute M-SOG (c). We propose a CMA-ES-based optimization strategy to maximize M-SOG, finding optimal LiDAR placement (d). To verify the effectiveness of our LiDAR placement optimization strategy, we contribute a multi-condition multi-LiDAR dataset (c) and evaluate the performance of baselines and optimized placements on both clean and corruption data (f).

To show how much uncertainty the placement \(L\) can reduce in the ROI, the naive surrogate metric _S-MIG_ is introduced as the reduction of the entropy as \(-H_{POG|L}=-_{i=1}^{N}H(v_{i}|L)\) with constant \(H_{POG}=_{i=1}^{N}H(v_{i})\), where \(H(v_{i})\), \(H(v_{i}|L)\) can be found through the entropy of Bernoulli distribution \(p(v_{i})\), \(p(v_{i}|L)\), respectively.

### M-SOG: Surrogate Metric of Semantic Occupancy Grids

The naive surrogate metric _S-MIG_ employs 3D bounding boxes as priors to understand 3D scene distribution. However, this approach exhibits substantial deviations from the actual physical boundaries of the objects and overlooks the occlusion relationships between objects and the environment in LiDAR applications. Furthermore, it is limited to detection tasks only. To overcome these limitations, we propose the Surrogate Metric of Semantic Occupancy Grids to assess and optimize LiDAR placement for 3D scene understanding tasks. Leveraging the Semantic Occupancy Grids derived from diverse environments, our approach can be expanded to tackling adverse conditions.

**Semantic Occupancy Grids (SOG).** Given a 3D driving scene with a set of semantic classes \(Y=\{y_{1},y_{2},...,y_{M}\}\), where \(M\) represents the total number of semantic classes in the scene and assuming that each voxel can only be occupied by one semantic tag for each frame, we denote \(y^{(t)}(v_{i})\) as the semantic class of voxel \(v_{i}\{v_{1},v_{2},...,v_{N}\}\) at time frame \(t\{1,2,...,T\}\). Notably, empty voxels are also considered a semantic class. Accordingly, the set of voxels occupied by semantic class \(y_{c}\) at frame \(t\) is defined as \(S_{y_{c}}^{(t)}=\{v_{i}|y^{(t)}(v_{i})=y_{c}\},\ c=1,2,...,M,\ t=1,2,...,T\). Then, we introduce SOG to describe the total semantic voxel distribution:

\[S_{SOG}^{(t)}=\{S_{y_{1}}^{(t)},S_{y_{2}}^{(t)},...,S_{y_{M}}^{(t)}\},\ t=1,2,...,T\.\] (1)

**Probabilistic Semantic Occupancy Grids.** In contrast to the Bernoulli distribution _POG_, we propose a more accurate Multinomial distribution: Probabilistic Semantic Occupancy Grids (_P-SOG_), denoted as \(p_{SOG}\), to represent the probability distribution of voxels belonging to certain semantic classes. Before estimating \(p_{SOG}\), we first traverse all frames from given scenes to obtain the probability \(\) for each voxel \(v_{i}\) occupied by each semantic class \(y_{c}\):

\[(v_{i}=y_{c})=|v_{i} S_{y_{c}}^{(t)}\}|} {T},\ c=1,2,,M\.\] (2)

Notably, we denote voxel \(v_{i}\) being occupied by semantic class \(y_{c}\) as \(v_{i}=y_{c}\), and \(\) indicates estimated distributions from observed samples, whereas \(p\) refers to the statistical parameters to be estimated, which are unknown and non-random. We compute the joint probability of all non-zero voxels in the ROI to estimate the \(p_{SOG}\). Following the literature , the joint distribution over the voxel set \(S\) is:

\[_{SOG}=(v_{1},v_{2},...,v_{N})=_{i=1,\ (v_{i}) 0}^{N} (v_{i})\.\] (3)

The uncertainty of Probabilistic Semantic Occupancy Grids reflects the 3D scene understanding capability of sensors within a given scene. To quantify this uncertainty, we calculate the entropy for the probability distribution of each voxel \(v_{i}\) in \(_{SOG}\) as: \((v_{i})=-_{c=1}^{M}(v_{i}=y_{c})(v_{i}=y_{c})\).

Based on the independent and identically distributed assumption of _SOG_, the entropy of _P-SOG_ over the voxel set \(S\) is given by: \(_{SOG}=_{v_{i} p_{S}}_{i=1}^{N}(v_{i})\), where \(p_{S}\) denotes the \(p_{SOG}\) over the voxel set \(S\). From the perspective of density estimation, the true _P-SOG_ and its corresponding entropy can be estimated as \(p_{SOG}=_{SOG}\) and \(H_{SOG}=_{SOG}\), respectively.

**Surrogate Metric of Semantic Occupancy Grids.** To evaluate LiDAR placements, we further analyze the joint probability distribution of voxels covered by LiDAR rays with varied LiDAR placements. Leveraging the Amanatides and Woo's Fast Voxel Traversal Algorithm , we obtain the voxels covered by LiDAR rays as \(S|L_{j}=(S,L_{j})=\{v_{1}^{L_{j}},v_{2}^{L_{j}},...,v_{N_{j}}^{L_{j}}\}\) given LiDAR configuration \(L=L_{j}\), \(j=1,2,...,J\), where \(j\) indexes the LiDAR placements, \(J\) is the number of total configurations, and \(N_{j}\) is the number of voxels covered by rays of LiDAR configuration \(L=L_{j}\). Then, the semantic entropy distribution of _P-SOG_ over the voxel set \(S|L_{j}\) can be estimated as:

\[H_{SOG}^{L_{j}}=_{v_{i}^{L_{j}} p_{S|L_{j}}}_{i=1}^{N_{j}} (v_{i}^{L_{j}})\.\] (4)We further define the information gain of 3D scene understanding as \( H=H_{SOG}-H_{SOG}^{L_{j}}\) to describe the perception capability of LiDAR configuration \(L_{j}\). Since \(H_{SOG}\) is a constant given certain 3D semantic scenes with the fixed _P-SOG_ distribution, we propose the normalized Surrogate Metric of Semantic Occupancy Grids (M-SOG) as follows to evaluate the perception capability:

\[M_{SOG}(L_{j}) =-}H_{SOG}^{L_{j}}=-}_{v_{i }^{L_{j}} p_{S|L_{j}}}_{i=1}^{N_{j}}H(v_{i}^{L_{j}})\] \[=}_{i=1}^{N_{j}}_{c=1}^{M}(v_{i}^{L_{ j}}=y_{c})(v_{i}^{L_{j}}=y_{c})\;.\] (5)

### Sensor Configuration Optimization

We adopt the heuristic optimization based on the Covariance Matrix Adaptation Evolution Strategy (CMA-ES)  to find an optimized LiDAR configuration.

**Objective Function.** We define the objective as \(F(_{j})=M_{SOG}(L_{j})\), where \(_{j}^{n}\) represents the LiDAR configuration \(L_{j}\) and is subject to the physical constraint \(P(_{j})=0\), and \(P()>0\) if it is violated, _e.g._, mutual distance between LiDARs and distance from a 2D plane. In our optimization, \(_{j}\) includes the 3D coordinates and rolling angles of LiDARs, \(\) is the finite cuboid space above the vehicle roof. Without ambiguity, we omit subscript \(j\) in the following text. We then transform the constrained optimization into the unconstrained Lagrangian form as \(G()=-F()+ P()\), where \(\) is the Lagrange multiplier. We optimize \(G()\) through an iterative process that adapts the distribution of candidate solutions as Algorithm 1.

**Optimization Approach.** We first define a multivariate normal distribution \((^{(k)},(^{(k)})^{2}^{(k)})\), where \(^{(k)}\), \(^{(k)}\), and \(^{(k)}\) are the mean vector, step size, and covariance matrix of the distribution of iteration \(k\), respectively. We discretize the configuration space \(\) with density \(\) and sample

Figure 2: **Visualized LiDAR Placements.** We compare the LiDAR placements optimized from our proposed M-SOG metric (for LiDAR semantic segmentation and 3D object detection) and heuristic LiDAR placements utilized by major autonomous vehicle companies (see Appendix Section B.1).

candidates \(u_{i}^{(k)}(^{(k)},(^{(k)})^{2}^{(k)})\) in each iteration \(k\), where \(i\) indexes the candidates. We update mean vector \(^{k+1}\) for the next iteration \(k+1\) as the updated center of the search distribution for LiDAR configuration. The overall process can be depicted as follows:

\[^{(k+1)}=_{i=1}^{M_{k}}w_{i}}_{i}^{(k)},\ G(}_{1}^{(k)}) G(}_{2}^{(k)}) G(}_{M_{k}}^{(k)})\,\] (6)

where \(M_{k}\) is the number of best solutions we adopt to generate \(^{(k+1)}\), and \(w_{i}\) are the weights based on solution fitness. We obtain the evolution path \(_{}^{(k+1)}\) that accumulates information about the direction of successful steps as follows:

\[_{}^{(k+1)}=(1-c_{})_{ }^{(k)}+})^{2}}^{M_{k}} w_{i}^{2}}}^{(k+1)}-^{(k)}}{^{(k)}}\,\] (7)

where \(c_{}\) is the learning rate for the covariance matrix update. The covariance matrix \(\) controls the shape and orientation of the search distribution for LiDAR configurations. It can be updated at each iteration \(k\) in the following format:

\[^{(k+1)}=(1-c_{})^{(k)}+c_{}_{}^{(k+1)}_{}^{(k+1)^{T}}\.\] (8)

Similarly, we update \(_{}\) as the evolution path for step size adaptation. Then, the global step size \(\) can be found below for the scale of search to balance exploration and exploitation:

\[_{}^{(k+1)}=(1-c_{})_{}^{(k)}+)^{2}}^{M_{k}}w_{i}^{2}}}^{(k+1)}-^{(k)}}{^{(k)}}\,\] (9)

\[^{(k+1)}=^{(k)}(}{d_{}}(_{}^{(k+1)}\|}{E\|(,)\|}-1 ))\,\] (10)

where \(c_{}\) is the learning rate for updating the evolution path \(_{}\). \(d_{}\) is a normalization factor to calibrate the pace at which the global step size is adjusted.

### Theoretical Analysis

Once the evolution optimization empirically converges, it holds that the optimized solution is the local optima of the \(\)-density Grids space of LiDAR configuration space. In this section, we provide a stronger optimality certification to theoretically ensure that the optimized placement is close to the global optimum under the assumption of bounded and smooth objective function. Due to space limits, the full proof has been attached to the Appendix Section E.

**Theorem 1** (Optimality Certification).: _Given the continuous objective function \(G:^{n}\) with Lipschitz constant \(k_{G}\) w.r.t. input \(^{n}\) under \(_{2}\) norm, suppose over a \(\)-density Grids subset \(S\), the distance between the maximal and minimal of function \(G\) over \(S\) is upper-bounded by \(C_{M}\), and the local optima is \(_{S}^{*}=_{ S}G(x)\), the following optimality certification regarding \(x\) holds that:_

\[\|G(^{*})-G(_{S}^{*})\|_{2} C_{M}+k_{G}\,\] (11)

_where \(^{*}\) is the global optima over \(\)._

The global optimality certification Theorem 1 is applicable in practice because the Lipschitz constant \(k_{G}\) and the distance between the maximal and minimal of objective function \(G\) over \(S\) can be approximated easily through calculating \(G(_{i}^{(k)})\) of Algorithm 1 for each sampled \(_{i}^{(k)}\) over the \(\)-density Grids subset. Besides, we have a more general corollary below to further relax the assumption of bounded objective function.

**Corollary 1**.: _When \(\) is a hyper-rectangle with the bounded \(_{2}\) norm of domain \(U_{i}\) at each dimension \(i\), with \(i=1,2,,n\), Theorem 1 can hold in a more general way by only assuming that the Lipschitz constant \(k_{G}\) of the objective function is given, where Equation (11) becomes:_

\[\|G(^{*})-G(_{S}^{*})\|_{2} k_{G}_{i=1}^{n} U_{i}+k_{G}\.\] (12)

## 4 Experiments

### Benchmark Setups

**Data Generation.** We generate LiDAR point clouds and ground truth using CARLA . We use the maps of Towns 1, 3, 4, and 6 and set 6 ego-vehicle routes for each map. We incorporate 23 semantic classes for LiDAR semantic segmentation and 3 instance classes for 3D object detection. Data collection is performed for 10 LiDAR placements, resulting in a total of 280,000 frames: 1) For each placement, we gather 340 clean scenes and 360 corrupted scenes, with each scene consisting of 40 frames. 2) The clean set comprises 13,600 frames, including 11,200 samples (280 scenes) for training and 2,400 samples (60 scenes) for validation, following the split ratio used in nuScenes . 3) The corruption set is used for robustness evaluation and contains 6 different conditions, each with 2,400 samples (60 scenes). More details are in the Appendix Section A.

**LiDAR Configurations.** We adopt five commonly employed heuristic LiDAR placements, which have been adopted by leading autonomous driving companies, as our baseline. These placements are represented in Figure 1(a)-e. Following KITTI , we configured the LiDAR sensor with a vertical field of view of [-24.8, 2.0] degrees. To achieve the _Line-roll_ and _Pyramid-roll_ configurations, we adjusted LiDAR roll angles for the _Line_ and _Pyramid_ setups, as depicted in Figure 2. We present detailed LiDAR configurations in the Appendix Section B.1.

**Corruption Types.** To replicate adverse conditions, we synthesized six types of corrupted point clouds on the validation set of each sub-set, following the settings of the Robo3D benchmark . These corruptions can be categorized into 1) severe weather conditions, including "snow", "fog", and "wet ground", 2) external disturbances, including "motion blur", and 3) internal sensor failures, including "crosstalk" and "incomplete echo". We include the features and implementation details of corruptions in the Appendix Section B.2.

**P-SOG Synthesis.** We follow the steps as depicted in Figure 3 to generate semantic occupancy grids for each LiDAR scene. We first collect point clouds and semantic labels using high-resolution LiDARs and sequentially divide all samples into multiple subsets. Through the transformation of world coordinates of the ego-vehicle, frames of point clouds of each subset are aggregated into one frame of dense point cloud. Then, we utilize the voting strategy to determine the semantic label for each voxel in ROI and generate the SOG. This process is executed across all subsets to produce P-SOG. Notably, the P-SOG is only generated on the scenes of the training set.

**Surrogate Metric of SOG.** We compute the scores of M-SOG separately for 3D detection and segmentation, as shown in Table 1. To evaluate M-SOG for segmentation, we utilize all semantic classes to generate semantic occupancy grids. For detection, we specifically focus on the _car_ semantic type, while merging the remaining semantic classes into a single category for M-SOG analysis.

  
**Metrics** [\(M_{SOG}\)] & Center & Line & Pyramid & Square & Trapezoid & L-roll & P-roll & **Ours** \\  
3D Detection (\( 10^{-6}\)) & \(-1.26\) & \(-1.65\) & \(-1.34\) & \(-1.54\) & \(-1.52\) & \(-1.41\) & \(-1.35\) & \(-1.18\) \\
3D Segmentation (\(\)\(10^{-6}\)) & \(-1.58\) & \(-2.62\) & \(-2.56\) & \(-2.35\) & \(-2.89\) & \(-3.13\) & \(-1.63\) & \(-1.29\) \\   

Table 1: **Optimized M-SOG Metrics.** We report the \(M_{SOG}\) scores for both detection and segmentation tasks in our pipeline. The scores are calculated based on the _car_ class for detection and all semantic classes for segmentation. _Line-roll_ and _Pyramid-roll_ are abbreviated as _L-roll_ and _P-roll_.

Figure 3: **Pipeline of Probabilistic SOG generation.** We first merge multiple frames of raw point clouds (a) into dense point clouds (b). Then, we voxelize dense point clouds into SOG, _i.e._, semantic occupancy grids (c), and traverse all frames of dense point clouds to synthesize probabilistic SOG (d).

**Detector and Segmentor.** For the benchmark, we conduct experiments using four LiDAR semantic segmentation models, _i.e._, MinkUNet , SPVCNN , PolarNet , and Cylinder3D , and four 3D object detection models, _i.e._, PointPillars , CenterPoint , BEVFusion-L , and FSTR . The detailed training configurations are included in the Appendix Section B.3.

### Comparative Study

We conduct benchmark studies to evaluate the performance of varied LiDAR placements in both clean and adverse conditions. We extensively examine the correlation between the proposed surrogate metric, known as M-SOG, and the final performance results. Through our analysis, we are able to demonstrate the effectiveness and robustness of the entire Place3D framework.

**Effectiveness of M-SOG Surrogate Metric in Place3D.** In Table 1, we report the scores of the M-SOG surrogate metric for various LiDAR placements. In Tables 2 and 3, we benchmark the 3D object detection and LiDAR semantic segmentation performance of varied LiDAR placements with state-of-the-art algorithms. Figure 4 illustrates the correlation between the proposed M-SOG

    &  &  &  &  \\  &  &  &  &  &  &  &  &  &  &  &  &  \\   Clean \(\) & \(65.7\) & \(67.1\) & \(72.7\) & \(59.7\) & \(59.3\) & \(68.9\) & \(62.7\) & \(67.6\) & \(68.4\) & \(60.7\) & \(63.4\) & \(69.9\) \\  Fog \(\) & \(55.9\) & \(39.3\) & \(55.6\) & \(51.7\) & \(42.8\) & \(55.5\) & \(52.9\) & \(48.6\) & \(51.0\) & \(55.6\) & \(40.7\) & \(52.0\) \\ Wet Ground \(\) & \(63.8\) & \(66.6\) & \(64.4\) & \(60.2\) & \(57.9\) & \(66.4\) & \(60.3\) & \(66.6\) & \(52.2\) & \(61.9\) & \(64.3\) & \(55.6\) \\ Show \(\) & \(25.1\) & \(35.6\) & \(16.7\) & \(35.5\) & \(31.3\) & \(4.7\) & \(25.2\) & \(30.2\) & \(5.0\) & \(33.5\) & \(38.3\) & \(2.7\) \\ Motion Blur \(\) & \(35.8\) & \(35.6\) & \(37.6\) & \(52.0\) & \(46.1\) & \(39.4\) & \(50.7\) & \(55.1\) & \(42.5\) & \(51.5\) & \(53.9\) & \(44.2\) \\ Crosstalk \(\) & \(24.7\) & \(19.5\) & \(36.9\) & \(27.1\) & \(13.6\) & \(34.3\) & \(17.3\) & \(14.8\) & \(26.6\) & \(26.5\) & \(18.6\) & \(37.1\) \\ Incomplete Echo \(\) & \(64.5\) & \(68.7\) & \(71.5\) & \(59.2\) & \(57.1\) & \(68.3\) & \(60.2\) & \(65.9\) & \(60.9\) & \(61.2\) & \(63.7\) & \(68.7\) \\ 
**Average \(\)** & \(45.0\) & \(43.9\) & \(47.4\) & \(47.6\) & \(41.5\) & \(44.8\) & \(44.4\) & \(46.9\) & \(39.7\) & \(48.4\) & \(46.6\) & \(43.4\) \\   &  &  &  &  \\  &  &  &  &  &  &  &  &  &  &  &  &  \\   Clean \(\) & \(59.0\) & \(61.0\) & \(68.5\) & \(58.5\) & \(60.6\) & \(69.8\) & \(62.2\) & \(67.9\) & \(69.3\) & \(66.5\) & \(68.3\) & \(73.0\) \\  Fog \(\) & \(49.7\) & \(40.9\) & \(52.1\) & \(48.6\) & \(42.2\) & \(49.7\) & \(52.2\) & \(47.2\) & \(50.7\) & \(59.5\) & \(59.1\) & \(57.6\) \\ Wet Ground \(\) & \(60.4\) & \(61.3\) & \(64.6\) & \(59.2\) & \(62.0\) & \(65.4\) & \(60.9\) & \(67.1\) & \(67.9\) & \(66.6\) & \(66.7\) & \(67.2\) \\ Snow \(\) & \(27.6\) & \(36.3\) & \(3.1\) & \(26.9\) & \(27.0\) & \(2.6\) & \(26.6\) & \(31.6\) & \(2.1\) & \(17.6\) & \(24.0\) & \(5.9\) \\ Motion Blur \(\) & \(51.7\) & \(49.1\) & \(36.7\) & \(50.4\) & \(49.9\) & \(37.4\) & \(52.5\) & \(56.5\) & \(44.1\) & \(56.7\) & \(56.0\) & \(48.7\) \\ Crosstalk \(\) & \(18.4\) & \(16.9\) & \(30.0\) & \(21.2\) & \(16.5\) & \(27.3\) & \(19.3\) & \(13.7\) & \(31.9\) & \(24.5\) & \(18.7\) & \(41.0\) \\ Incomplete Echo \(\) & \(59.3\) & \(60.7\) & \(65.6\) & \(58.0\) & \(61.0\) & \(67.8\) & \(60.8\) & \(66.7\) & \(70.0\) & \(66.9\) & \(66.9\) & \(63.3\) \\   & \(44.5\) & \(43.8\) & \(42.0\) & \(44.1\) & \(43.1\) & \(41.7\) & \(45.4\) & \(47.1\) & \(44.5\) & \(48.6\) & \(48.6\) & \(47.3\) \\   

Table 2: **Performance evaluations of LiDAR semantic segmentation under clean and adverse conditions. For each LiDAR placement strategy, we report the mIoU scores (\(\)), represented in percentage (\(\%\)). The average scores only include adverse scenarios.**

    &  &  &  &  \\  &  &  & 

surrogate metric and perception capacity. The results demonstrate a clear correlation, where the performance generally improves as the M-SOG increases. While there might be fluctuations in some placements with specific algorithms, the overall relationship follows a linear correlation, highlighting the effectiveness of our M-SOG for computationally efficient sensor optimization purposes. We show more plots in the Appendix Sections C.1 and C.2.

**Comparisons to Existing Sensor Placement Methods.** The effectiveness of LiDAR sensor placement metrics lies in the linear correlation between metrics and actual performance. We set the same ROI size and conduct a quantitative comparison with _S-MIG_ on 3D detection (as _S-MIG_ only works for 3D detection task). As shown in Figure 5, our metric exhibits better linear correlation. Since we introduce semantic occupancy information as a prior for the evaluation metric, our method more accurately characterizes the boundaries in the 3D semantic scene and addresses the issue of objects and environment occlusions when using 3D bounding boxes as priors. Moreover, our LiDAR placement method makes the first attempt to include both detection and segmentation tasks. We present additional comparisons in the Appendix Section C.3.

**Superiority of Optimization via Place3D.** As shown in Tables 2 and 3, our optimized configurations achieved the best performance in both segmentation and detection tasks under both clean and adverse conditions among all models. We report per-class quantitative results in the Appendix Sections C.1 and C.2. The improvement from optimization remains significant even when comparing our optimized configurations against the best-performing baseline in each task. For segmentation, optimized placements outperform the best-performing baseline by up to 0.8% on clean datasets and by as much as 1.5% on corruption. For detection, optimized configurations exceed the best-performing baseline by up to 0.7% on clean datasets and by up to 0.3% on corruption.

**Robustness of Optimization via Place3D.** The M-SOG utilizes semantic occupancy as prior knowledge, which is invariant to changing conditions, thereby enabling robustness of optimized placement in adverse weather. Under the corrupted setting, although the correlation between M-SOG and perception performance is not as obvious as that in the clean setting and shows some fluctuation, our optimized LiDAR configuration consistently maintained its performance in adverse conditions, mirroring its effectiveness in the clean condition. While the top-performing baseline LiDAR configurations in clean datasets might be notably worse compared to others when faced with corruption, the optimized configuration via Place3D consistently shows the best performance under adverse conditions. We showcase several qualitative results on this aspect in the Appendix Section D.

**Intuitive Interpretation.** We observe several intuitive reasons why refined sensor placement is beneficial in our experiments. 1) LiDAR heights. Increasing the average height of the 4 LiDARs improves performance, likely due to an expanded field of view. 2) Variation in heights. A greater

Figure 4: The correlation between M-SOG and LiDAR semantic segmentation  models performance in the _clean_ condition.

Figure 5: Comparisons of M-SOG with S-MIG  using BEVFusion-L  and PointPillars .

difference in LiDAR heights enhances perception, as varied height distributions capture richer features from the sides of objects. 3) Uniform distribution. The pyramid placement performs better in segmentation, as a more spread-out and uniform distribution of 4 LiDARs captures a detailed surface.

### Ablation Study

In this section, we further analyze the interplay between our proposed optimization strategy and perception performance to address two questions: 1) _How does our optimization strategy inform LiDAR placements to improve robustness against various forms of corruption?_ 2) _How can our optimization strategy enhance perception performance in scenarios with limited placement options?_

**Optimizing Against Corruptions.** In the previous subsection, we deploy the placement optimized on the clean point clouds directly on corrupted ones to assess the robustness. To further showcase the capability of our method against corruptions, we compute the M-SOG scores utilizing the P-SOG derived from the corrupted semantic point clouds under adverse conditions and perform the optimization process, as shown in Figure 5(a). Quantitatively compared with _Ours_ in Table 2, the configurations derived from optimization on adverse data outperform the configurations generated solely on clean data. These results indicate that customizable optimization tailored for adverse settings is an effective approach to enable robust 3D perception.

**Optimizing 2D Placements.** Placing LiDARs at different heights on the roof might affect automotive aesthetics and aerodynamics. Therefore, we investigate the effect of our optimization algorithm in the presence of constraints, by limiting the LiDARs to the same height and considering only 2D placements. We fix the height of all LiDARs to find the optimal placement on the horizontal plane of the vehicle roof. We use the _Line_, _Square_, and _Trapezoid_ placements in Figure 2 for comparison. Experimental results in Figures 5(b) and 5(c) show that our optimized LiDAR placements outperform baseline placements at the same LiDAR height on both clean and corruption settings.

**Influence of LiDAR Roll Angles.** Fine-tuning the orientation angles of the LiDARs on autonomous vehicles is an effective strategy to minimize blind spots and broaden the perception range. For 3D segmentation, _Pyramid-roll_ slightly outperforms _Pyramid_, whereas _Line-roll_ falls short of _Line_, which is highly aligned with scores of M-SOG in Table 1. For 3D detection tasks, the situation is reversed, but still consistent with the M-SOG results. This suggests that adjusting the LiDAR's angle can have varying impacts on the performance of 3D object detection and LiDAR semantic segmentation.

## 5 Conclusion

In this work, we presented Place3D, a comprehensive and systematic LiDAR placement evaluation and optimization framework. We introduced the Surrogate Metric of Semantic Occupancy Grids (M-SOG) as a novel metric to assess the impact of various LiDAR placements. Building on this metric, we culminated an optimization approach for LiDAR configuration that significantly enhances LiDAR semantic segmentation and 3D object detection performance. We validate the effectiveness of our optimization approach through a multi-condition multi-LiDAR point cloud dataset and establish a comprehensive benchmark for evaluating both baseline and our optimized LiDAR placements on detection and segmentation in diverse conditions. Our optimized placements demonstrate superior robustness and perception capabilities, outperforming all baseline configurations. By shedding light on refining the robustness of LiDAR placements for both tasks under diverse driving conditions, we anticipate that our work will pave the way for further advancements in this field of research.

Figure 6: Ablation results (mIoU) of placement strategies on segmentation models .