# Is One GPU Enough? Pushing Image Generation at Higher-Resolutions with Foundation Models.

Athanasios Tragakis

University of Glasgow

Glasgow, United Kingdom

a.tragakis.1@research.gla.ac.uk

&Marco Aversa

Dotphoton

Zug, Switzerland

marco.aversa@dotphoton.com

Eduinstanya Kaul

University of Glasgow

Glasgow, United Kingdom

chaitanya.kaul@glasgow.ac.uk

&Roderick Murray-Smith

University of Glasgow

Glasgow, United Kingdom

Roderick.Murray-Smith@glasgow.ac.uk

Daniele Faccio

University of Glasgow

Glasgow, United Kingdom

Daniele.Faccio@glasgow.ac.uk

Equal Contribution.

The code for our work is available at https://thanos-db.github.io/Pixelsmith/.

###### Abstract

In this work, we introduce Pixelsmith, a zero-shot text-to-image generative framework to sample images at higher-resolutions with a single GPU. We are the first to show that it is possible to scale the output of a pre-trained diffusion model by a factor of 1000, opening the road for gigapixel image generation at no additional cost. Our cascading method uses the image generated at the lowest resolution as a baseline to sample at higher-resolutions. For the guidance, we introduce the Slider, a tunable mechanism that fuses the overall structure contained in the first-generated image with enhanced fine details. At each inference step, we denoise patches rather than the entire latent space, minimizing memory demands such that a single GPU can handle the process, regardless of the image's resolution. Our experimental results show that Pixelsmith not only achieves higher quality and diversity compared to existing techniques, but also reduces sampling time and artifacts.1

## 1 Introduction

Recent advances in diffusion models (DMs) have revolutionized the field of high-fidelity image generation. Foundational works like Ho et al. (2020), Sohl-Dickstein et al. (2015), Song and Ermon (2019) established the groundwork, leading to significant breakthroughs demonstrated by Dhariwal and Nichol (2021). These models have evolved rapidly, with innovations such as new sampling techniques Lu et al. (2022), Song et al. (2020) and capabilities for inpainting Lugmayr et al. (2022) and image editing Brooks et al. (2023), Mokady et al. (2023), Nichol et al. (2021). However, even though DMs have shown remarkable results on high-fidelity image generation, scaling them to high-resolutions is still an open challenge. The introduction of the latent diffusion model (LDM) Rombach et al. (2022) has made high-resolution image synthesis more accessible. However, most pre-trained DMs based on the LDM are limited to generating images with a maximum resolution of\(1024^{2}\) pixels due to constraints in computational resources and memory efficiency. Following this approach, aiming to ultra-high-resolution generation would incur additional training and data costs, and the model would be unable to run on a single GPU.

Recently, several works have focused on scaling pre-trained models to higher-resolutions, highlighting new possibilities and challenges in the field Aversa et al. (2024); Du et al. (2023); Gu et al. (2023); He et al. (2023). This allows already available models to be used with no extra costs and no additional carbon footprint. However, most methods addressing this problem either require expensive GPUs, as memory demands increase with resolution or prolonged generation times. Moreover, scaling the native resolution of a generative foundation model to higher-resolutions introduces artifacts due to the direct mapping into a prompt-image embedding space. For a specific prompt, we would expect to sample an image matching the description and of the same size as the training data images. Consequently, even scaling up by a factor of 2 would lead to the duplication of the image produced by the prompt across the higher-resolution image. However, since the images in the training set contain information at different resolutions, we can leverage this prior knowledge implicitly learned by the diffusion model to guide the generation at higher-resolutions and enhance fine details. Addressing these challenges is crucial for applications that demand ultra-high-resolution images, such as gigapixel photography, medical imaging, satellite imagery and high-definition digital art.

To overcome the challenges in ultra-high-resolution image generation, we introduce Pixelsmith, an adaptable framework that utilizes pre-trained generative models for scalable gigapixel synthesis.

Our contributions are outlined as follows: 1. We introduce Pixelsmith, the first framework capable of generating gigapixel-resolution images using a single GPU. 2. We develop the Slider, a dynamic tool that allows users to adjust the balance between overall image structure and fine-detail enhancements in the generation process. 3. We enhance and adapt the random patch denoising strategy to text-to-image pre-trained diffusion models, leading to minimized memory usage. 4. We provide a masking method that, combined with the Slider, reduces the number of artifacts at higher-resolutions.

## 2 Related Work

Pre-trained DMs are trending toward increasing native resolutions. Notable examples include Stable Diffusion (SD) Rombach et al. (2022), which started at \(512^{2}\), then \(768^{2}\) in SD 2, and \(1024^{2}\) in SDXL Podell et al. (2023), SD Cascade Pernias et al. (2024), and SD 3 Esser et al. (2024). Similarly, DALL-E Ramesh et al. (2021) continues to increase resolution with each version OpenAI (Nov 6 2023). This trend shows a growing demand for higher-resolution generation.

Currently, high-resolution image generation often involves a super-resolution model applied after the initial text-to-image generation Saharia et al. (2022). This additional model increases costs due to training and domain-specific fine-tuning requirements.

### Trained Models

Trained models are those that are specifically designed for multi-resolution generation. Recent models like Matryoshka Gu et al. (2023) can generate various resolutions up to \(1024^{2}\) through a progressive training schedule. However, scaling beyond this is limited. CogView3 Zheng et al. (2024), based on Relay Teng et al. (2023), upsamples from a base resolution of \(512^{2}\), though higher-resolutions like \(4096^{2}\) remain a future goal. Fine-tuning methods such as DiffFit Xie et al. (2023) are costly, requiring 51 V100 GPU days. ASD Zheng et al. (2024) introduces a memory-efficient sampling method capable of _theoretically_ generating images up to \(18432^{2}\) resolution. Patch-DM Ding et al. (2023), by training on \(64^{2}\) patches, can generate resolutions like 1024x512. LEGO Zheng et al. (2023) and Inf-DiT Yang et al. (2024) also rely on training.

### Adapted Models

Adapted models, on the other hand, modify pre-trained models to generate higher-resolutions without additional training. MultiDiffusion Bar-Tal et al. (2023) offers controllable generation through multiple processes but is slow and prone to structural errors Du et al. (2023); He et al. (2023); Zheng et al. (2024). ScaleCrafter He et al. (2023) improves resolution by altering the convolution kernel dilation but faces memory limitations. DemoFusion Du et al. (2023) adapts MultiDiffusionfor constant memory use, though it is time-consuming. ElasticDiffusion Haji-Ali et al. (2023) and SyncDiffusion Lee et al. (2023) aim for specific high-resolution goals, but object repetition remains a problem at large scales Jin et al. (2024). Recently, HiDiffusion Zhang et al. (2023) introduced modifications to the UNet architecture to prevent object duplication; however, it is limited to a maximum resolution of \(4096^{2}\) due to high GPU memory requirements. AccDiffusion Lin et al. (2024) employs patch-content-aware prompts and adjusts the attention masks within the UNet to suppress artifacts, but these changes result in blurry images. Similarly, Fouriscale Huang et al. (2024) is another model that alters the UNet but exhibits issues at higher-resolutions, particularly at \(4096^{2}\). Current approaches excel in some areas but fall short in others. To enable high-resolution generation on a single consumer GPU, models must be adapted to avoid retraining costs. Efficient memory usage is crucial to prevent the need for expensive, high-memory GPUs, and the process must be fast and artifact-free. We propose a flexible framework that addresses these challenges based on text conditions.

Figure 1: Examples of generated images using Pixelsmith. The proposed framework generates images on higher-resolutions than the pre-trained model without any fine-tuning. Images at different resolutions are shown with cut-out areas for both Pixelsmith and the base model. The higher-resolution images are in scale with the images generated by the base model. Only the lower resolution version of the gigapixel image has been resized for a better visualisation. Some cut-outs of the gigapixel generation have resolution close to the base model which is \(1024^{2}\) and it can be seen that the images are comparable in aesthetics showing that our framework is capable of true gigapixel generations (**zoom in** to see in better detail).

## 3 Foundations

### Diffusion models

DMs Ho et al. (2020); Sohl-Dickstein et al. (2015) are probabilistic generative models that first add noise to a distribution during diffusion and then learn to remove this noise during denoising. This way, during training, a Gaussian probability distribution is learnt and during inference sampling from the Gaussian leads to the data probability distribution. Executing this process in the latent space Rombach et al. (2022) is more resource efficient allowing for faster training and inference times. In a Latent Diffusion Model, let \(z_{0}_{}(x)\) represent the clean training data \(x q(x)\) mapped to the latent space by the VQ-VAE encoder \(_{}\). Beginning from \(z_{0}\), at each timestep \(t\{1,,T\}\), noisy latents \(z_{1},,z_{T} q(z_{t} z_{t-1}):=(z_{t};}z_{t-1},_{t})\) are sampled according to a noise schedule function based on the timestep \(_{t}\). During training, the model learns to denoise these latent variables using a conditional prompt \(c\). At inference time, starting from a random latent \(z_{T}(0,)\), the model generates by iteratively sampling \(z_{0} p_{}(z_{0} z_{T},c)=_{i=1}^{T}p_{}(z_{t-1} z _{t},c)\).

### Patch sampling

The default LDM denoising process samples the entire latent space at each timestep, which becomes resource-intensive as the resolution increases. Methods like Li et al. (2024) distribute this load across multiple GPUs, but this requires expensive hardware. To address this, we adapted and refined the DiffInfinite sampling method Aversa et al. (2024) for text-to-image DMs, enabling ultra-high-resolution generation efficiently on a single GPU.

At each timestep, random patches are selected for denoising, and this process is repeated until the entire latent space is denoised. The randomness is controlled for efficiency. We track which pixels have been denoised at each timestep, and select areas where pixels have not been denoised yet. This sampling process avoids excessive inference times, especially at very high-resolutions like \(32768^{2}\). In our experiments, we sample images at higher-resolution using fixed \(128^{2}\) patches in the latent space. To handle overlapping patches, pixel values are temporarily reverted when already denoised pixels are encountered, ensuring correct processing. After the current patch is denoised, the pixels that were reverted are restored to their denoised values. Only the first denoising of each pixel is retained for each timestep, preventing redundant denoising (see Fig. 2).

Despite its advantages, the DiffInfinite sampling method still relies on segmentation masks to condition each patch, providing rich spatial information. However, in text-to-image DMs, where a global text prompt conditions the entire latent space, this method is insufficient on its own. Applying the same text prompt to each patch results in repetitive content and poor-quality generations. To overcome this, we introduce a guiding mechanism that incorporates structural information between patches alongside the text prompt, improving the quality and diversity of the generated images.

Figure 2: Overview of the patch denoising process proposed by DiffInfinite: The top row represents the latent space, while the bottom row tracks the timesteps for each pixel. Each pixel should be denoised only once per timestep, so when overlapping occurs, already denoised pixels revert to their previous values from the prior timestep. After denoising, these reverted pixels are restored to their original denoised state from the current timestep.

## 4 Method

### Problem statement

Currently, pre-trained text-to-image LDMs generate a latent variable \(z_{0} p_{}(z_{0}|z_{T},c)\) with a fixed size using a diffusion process, which is then decoded into pixel space using the decoder \(_{}\) of a VAE. In this section, we show how to leverage foundational generative models to sample arbitrarily large images based on a given prompt. Without requiring additional training or fine-tuning, the LDM, trained on images with size \(H W 3\), is adapted to generate images with size \(mH nW 3\), where \(m,n 1\) are the scaling factors for achieving higher-resolution.

We present Pixelsmith, a framework with a flexible number of cascaded steps (Section 4.3) designed to generate images at ultra-high-resolution on a single GPU. The implementation of the Slider (Section 4.4) provides control over the generation process using patches (Section 4.2), making it resource-efficient regardless of the resolution.

Figure 3: Proposed framework overview. 1. _Text-to-image Generation_: A pre-trained text-to-image diffusion model generates an initial image based on the input text prompt. 2. _Upsampling process_: The generated image is upscaled (in this use case by a factor x4) and encoded into the latent space to guide the creation of a higher-resolution image. 3. _Image guidance preparation_: The encoded image is degraded through the diffusive forward model, creating the guidance latents. 4. _Image generation_: the Slider (indicated by a blue line) adjusts the extent of guidance. _Left of Slider_ (Guided Generation): guidance latents control the image generation. The framework fuses guidance latents (green patches) with high-resolution latents (purple patches) using the Fast Fourier Transformation (FFT). The phases are averaged and combined with the amplitude, then transformed back via the inverse FFT (iFFT). A chess-like mask integrates information from the successive guidance step (orange), resulting in fully processed patches (cyan). _Right of Slider_ (Pure Generation): the generation relies only on the prompt. _Higher-Resolution Comparison_: while the base model upscales the best with disfigured hands, the proposed method enhances details, corrects distortions, and prevents new artifacts.

### Framework overview

In this section, we describe the workflow of Pixelsmith, detailing how the framework adapts pretrained text-to-image LDMs to generate images with higher-resolutions on a single GPU (see Fig. 3). In order to generate ultra-high-resolution images without artifacts, we introduce these key components: the Slider (see Sec. 4.4), patch averaging (see Sec. 4.5) and masking (see 4.6).

Text-to-image generationFirst, given a conditional prompt \(c\), we use SDXL to sample the latent variable \(_{0}\) and decode it into pixel space as \(=_{}(_{0})\), generating a \(1024^{2}\) resolution image.

Upsampling processAfter the image generation, we apply an upsampling algorithm to increase the image resolution (see Sec. 4.3 for details). In our case, we used Lanczos interpolation Lanczos (1950) to scale up to the desired resolution. However, upsampling leads to a blurred output and lack of additional content. The upsampled image, \(x^{guid}\), will serve as guidance for our generative process.

Image guidance preparationOnce the guidance image is encoded in the VAE's latent space \(z_{0}^{guid}=_{}(x^{guid})\), we can easily sample each latent variable of the diffusion process through the forward diffusion process \(z_{t}^{guid} q(z_{t}^{guid}|z_{0}^{guid})\).

Image generationThe generative process starts from \(z_{T}(0,I)\), which has the same dimensions as \(z_{T}^{guid}\). At each step, a random patch is cropped as described in Section 3.2:

\[_{t}^{(i,j)}=^{i,j}(z_{t}),_{t}^{guid,(i,j)}=^{i,j}(z_{t}^{guid})\] (1)

where \(^{i,j}:^{4,h,w}^{4,p_{h},p_{w}}\) is a cropping function that extracts patches of size \((p_{h},p_{w})\) from the latent variables \(z_{t}\) and \(z_{t}^{guid}\) of size \((h,w)\) at the coordinates \((i,j)\). For simplicity, we will refer to the cropped latent patch \(_{t}^{(i,j)}\) as \(_{t}\) and \(_{t}^{guid,(i,j)}\) as \(_{t}^{guid}\) throughout this discussion.

The Slider's position (see Sec. 4.4 for details), indicated by a blue line in Fig. 3, determines whether the guidance mechanism (see Sec. 4.4.1 for details) or unguided patch denoising will be applied. In the unguided mode, each patch is based solely on the previous one and the text condition, similar to a conventional patch denoising process. The Slider allows control over whether a generated image will be slightly or significantly altered compared to the previous resolution.

After the denoising process has ended, the latents \(z_{0}\) are decoded and the higher-resolution image is generated. Using a cascade upsampling approach, the generated image can be upsampled again, repeating the process to achieve an even higher-resolution image.

### Higher generation in one step

Most of existing higher-resolution generative methods rely on cascade sampling approaches Denton et al. (2015); Ho et al. (2022); Menick and Kalchbrenner (2018). Our approach, however, differs from previous works like Du et al. (2023); Guo et al. (2024) in two significant ways.

First, we perform upsampling directly in the pixel space rather than in the latent space. Manipulating the latent space distribution with transformations like upsampling can introduce distortions that the scheduler has not accounted for Chang et al. (2024), leading to degraded image quality Hwang et al. (2024). Second, our flexible method enables higher-resolution image generation in two generative steps: first by producing the base image, then by enhancing it to the desired resolution. For instance, Du et al. (2023) requires passing through intermediate resolutions (\(1024^{2}\), \(2048^{2}\), \(3072^{2}\)) to achieve a \(4096^{2}\) resolution, with even higher-resolutions necessitating additional steps. This results in prolonged waiting times and multi-scale duplications, as each step may produce duplicates at different resolutions (Appendix D).

In contrast, our framework can generate any resolution directly from the base one, making it feasible to scale up to a \(32768^{2}\) image directly from the base resolution of \(1024^{2}\). However, intermediate steps can sometimes yield better small-sized details (Appendix E). Our flexible approach supports both two-step generation and cascade generation, allowing users to select their preferred sampling method depending on the use case.

### Slider

Currently, higher-resolution generative models often encounter issues such as duplications and atypical anatomical features or structures He et al. (2023). To mitigate these artifacts, we introduce the _Slider_ (indicated by a blue line in Figure 3). The Slider is a parameter that determines at which denoising step there is the transition between the guidance mechanism introduced in Sec. 4.4.1 and traditional unguided generation. The Slider can either constrain the denoising process to eliminate undesirable outputs or allow for less constrained generation to add details.

When set to zero, the generation is entirely unconstrained. This results in an image that might resemble the \(1024\)2 base image but will likely contain multiple duplicates. At \(t=T\), the generation is highly guided, leading to an image that is almost identical to the previous resolution. The optimal value for the Slider varies depending on the base image. Adjusting the Slider manually is necessary to achieve the best output. This manual adjustment allows for fine-tuning the balance between constrained and unconstrained generation, ensuring that the resulting image maintains the desired quality and level of detail. The ideal Slider setting is influenced by factors such as image complexity, the presence of fine details, and the desired resolution. Experimentation and iterative adjustments are often required to find the most effective value for each specific image and resolution.

An example demonstrating the impact of the Slider's position can be found in Appendix G.

#### 4.4.1 Guidance mechanism

The guidance mechanism combines the random patches \(_{t}^{}\), \(_{t}\), and \(_{t-1}^{}\) to generate the updated patch \(_{t-1}\). First, the patches \(_{t}^{}\) and \(_{t}\) are transformed into the Fourier space using the Fast Fourier Transform (\(\)), where their phase components \(_{t}\) and \(_{t}^{}\) are averaged:

\[(_{t})=_{t}e^{i_{t }}\\ (_{t}^{})=_{t}^{}e^{i_{t}^{}}_{t}=(e^{i_{t}}+e^{i_{t}^{}})\] (2)

Here, \(_{t}\) and \(_{t}^{}\) represent the amplitudes, while \(_{t}\) and \(_{t}^{}\) are the phases.2 The averaged phase \(_{t}\) is then combined with the amplitude \(_{t}\) from \(_{t}\) to form a new Fourier representation \(_{t}^{}=_{t}e^{i_{t}}\). While the amplitude \(_{t}\) preserves the intensity and contrast information, the phase averaging process ensures that the resulting phase contains structural characteristics from both

Figure 4: Masking effects on higher-resolution generation (\( 16\) the original resolution). (left) Image generated using SDXL. (center) Image generated with Pixelsmith with masking. (right) Image generated with Pixelsmith without masking. We highlighted the artifacts introduced by generating at higher scales. These artifacts demonstrate the challenges of maintaining coherence and accuracy when scaling up the resolution without additional guidance.

the generated and guiding images. This modified Fourier representation is transformed back to the spatial domain using the inverse Fast Fourier Transform \(_{t}^{}=i(_{t}^{})\). The output \(_{t}^{}\) is then used as the condition for the reverse diffusion process to generate the next patch \(_{t-1}^{} p_{}(_{t-1}^{} _{t}^{},c)\). This solution suppresses local artifacts and helps maintain low-frequency structural consistency between the reverse diffusion steps, reducing the likelihood of significantly altering the global properties of the generated image. However, the guidance mechanism alone still generates some long-range discrepancies across the image due to prompt sharing between the patches. To mitigate these artifacts and prevent prompt duplications across the latent space, we introduce a masking method(see Section 4.6).

### Patch averaging

Overlapping patches can sometimes cause visible differences at their borders. To address this, we introduce a transition zone where the values of overlapping patches at timestep \(t\) are averaged, producing a smooth and seamless denoised output. In Appendix C, we illustrate the artifacts caused by patch overlap and show how averaging effectively removes them.

### Masking

One of the main reasons patch-based image generation in diffusion models Bar-Tal et al. (2023) suffers from artifacts at higher-resolutions is the use of the overall image text prompt for each patch during the denoising process. This leads to duplicate structures forming in the latent space. We partially addressed the spatial coherence with the guidance mechanism in Section 4.4.1. To further improve results, we combine the sampled \(_{t}^{iFFT}\) with the image guidance \(_{t}^{guid}\) using a chess-like mask \(\) (Equation 3).

\[_{t}^{masked}=_{t}^{iFFT}+(1-)_{t}^{guid} =_{i,j}=0&i+j\\ 1&i+j\] (3)

In Fig. 4, we show the comparison between a higher-resolution image generated with and without masking with the image guidance. The image on the right, which is not constrained by pixels from \(_{t}^{guid}\), exhibits more freedom in generation and, as a result, introduces notable artifacts. For example, clouds transform into waves, and the blue sky morphs into the sea. Additionally, a small, seemingly artificial lake or river appears in the middle of the village, situated unnaturally close to the beach.

## 5 Experiments

Pixelsmith is tested on a single RTX 3090 GPU, with all tested resolutions requiring 8.4 GB of memory. Performance is evaluated on the LAION-5B dataset Schuhmann et al. (2022) by randomly sampling 1,000 image and text prompt pairs. The metrics used for evaluation are Frechet Inception Distance (FID) Heusel et al. (2017), Kernel Inception Distance (KID) Binkowski et al. (2018), Inception Score (IS) Salimans et al. (2016), and CLIP Score Radford et al. (2021), with the FID metric computed using the clean-FID approach Parmar et al. (2022) (for further comparisons, see Appendix B). Additionally, we compare our results with a super-resolution model in Appendix F.

### Ablation study

We conducted a quantitative examination of our framework at a resolution of \(2048 2048\) pixels, focusing on key factors that influence its performance: the Slider position, the role of amplitude and phase in the latent space, the importance of masking during guidance, and the impact of averaging overlapping patches. Our results are shown in Table 1.

Slider positionOur findings highlight the critical role of the Slider position in the quality of the generated images. Setting the Slider to position 30 (_proposed_ method) provides an optimal trade-off between guidance and generation, outperforming other positions like 1 (_SP1_, no guidance), 24 (_SP24_, mid-point), and 49 (_SP49_, full guidance). A Slider position of 1 introduces numerous artifacts due to insufficient guidance, while a position of 49 lacks fine detail because it relies too heavily on lower-resolution. Position 24 has been chosen as mid-point in the diffusion process, it is closer to the optimal value but does not yield the best results across our dataset.

Amplitude and phaseIn exploring the role of amplitude and phase within the latent space, we experimented with different configurations. The _proposed_ method averages the phase of the guidance latents and the current latents while using the amplitude of the current latents, as detailed in the Methods section (see Figure 3). We compared it with other setups: one that averages the amplitude of both latent spaces while using the phase of the current latents (denoted as \(\) in Table 1), and another that averages both amplitude and phase from both latent spaces (denoted as \(\&\)). The proposed method demonstrated superior performance in maintaining image quality and preserving fine details. Averaging the two phases provides guidance through structural information, while averaging the amplitudes only conveys the intensity of the frequencies. Averaging both offers information on both structure and intensity which overly influences the final image, leading to a lack of fine details expected in a higher-resolution output.

MaskingMasking emerges as a significant factor in enhancing image quality. The quantitative metrics indicate that the chess-like mask leads to slightly better scores compared to using no mask (as seen in the _No Mask_ column of Table 1 compared to the _proposed_ one). Visual inspections reveals that incorporating the mask is crucial for removing artifacts (see Figure 4).

Overlapping patchesLastly, we assessed the impact of averaging overlapping patches. Implementing patch averaging improved the performance compared to not using it, as evidenced by the comparison between the _proposed_ method and the _No Averaging_ column in Table 1. Although, despite the small difference in the metrics, averaging overlapping patches effectively eliminates artifacts that occur at patch borders due to overlapping regions, resulting in smoother and more coherent images (see Appendix C). This discrepancy underscores the limitations of metrics like FID in detecting high-resolution artifacts, highlighting the necessity of qualitative assessments.

 
**Metric** & **SP1** & **SP24** & **SP49** & \(\) & \(\&\) & **No Mask** & **No Aver.** & **Proposed** \\  FID\(\) & 74.890 & 63.168 & 64.195 & 63.476 & 63.173 & 64.473 & 63.493 & **63.116** \\ KID\(\) & 0.008 & **0.002** & 0.003 & **0.002** & **0.002** & 0.003 & 0.003 & **0.002** \\ IS\(\) & 19.928 & 19.941 & 19.290 & 19.933 & 19.897 & 19.892 & 19.626 & **24.242** \\ CLIP\(\) & 32.198 & 32.451 & 32.425 & 32.730 & 32.432 & 32.355 & 32.416 & **33.429** \\  

Table 1: A quantitative examination of our framework through ablations.

Figure 5: Qualitative comparisons: This figure highlights how other models suffer from duplications (red arrows) and introduce artifacts in areas with complex, high-frequency patterns (purple arrows). In contrast, Pixelsmith effectively eliminates these issues. (**zoom in** to see in better detail).

### Comparison

We compare Pixelsmith to state-of-the-art frameworks in the higher-resolution image generation task, as well as to SDXL, which serves as the base model for our adaptation. To ensure a fair comparison, we include only models with native resolutions comparable to ours, excluding those trained at resolutions smaller than \(1024^{2}\). While Pixelsmith outperforms other methods in most metrics (see Table 2), its real advantage lies in its ability to adapt to the unique characteristics of each individual image generation. This flexibility enables precise control to emphasize details and reduce artifacts. In Figure 5, we qualitatively show how Pixelsmith maintain the image structure by adding fine-details while other models introduce duplications or artifacts. There, we highlighted with red arrows the duplications and with purple arrows spurious artifacts. Additional comparisons can be found in Appendix B.

## 6 Discussion and Considerations

Pixelsmith can generate images at arbitrarily high-resolutions, but as the resolution increases, adding more generative details without introducing artifacts becomes difficult due to the smaller denoising patch size relative to the latent space. For example, a \(32768^{2}\) image maps to a \(4096^{2}\) latent space with a \(128^{2}\) denoising patch. The higher the resolution to generate in a single step, the higher the Slider value is required to suppress artifacts. However, increasing the Slider value reduces the finer detail generation and return an image which closely resembles the previous resolution, limiting the true quality of the higher-resolution image. Generating an image through a cascade generation approach allows for using a lower Slider value at each stage, providing more details and fewer artifacts compared to single-step generation, as the lower Slider settings can be more effective in this incremental process. Additionally, appropriate metrics for evaluating high-resolution images are lacking, and further research is necessary to improve quantitative evaluation methods, as highlighted in previous studies Jin et al. (2024) as well.

## 7 Conclusions

In this work, we introduced Pixelsmith, a text-to-image generative framework that leverages a pre-trained foundation model to generate higher-resolution images. Our flexible method uses a base image generated at the lowest resolution, combined with the Slider mechanism, to ensure both structural coherence and fine detail enhancement in the resulting images. By employing patch-based denoising, we have significantly reduced memory demands, making the generation of ultra-high-resolution images feasible on consumer-grade GPUs. Experimental results demonstrate that our model outperforms current state-of-the-art methods in both image quality and generation efficiency.

 
**Resolution** & **Model** & **FID\(\)** & **KID\(\)** & **IS\(\)** & **CLIP\(\)** & **Time (sec)\(\)** \\  \)} & SDXL Podell et al. (2023) & 111.452 & 0.020 & 12.046 & 29.804 & 71 \\  & ScaleCrafter He et al. (2023) & 77.543 & 0.006 & 17.112 & 30.904 & 80 \\  & DemoFusion Du et al. (2023) & 64.422 & **0.002** & 19.307 & 32.648 & 219 \\  & HiDiffusion Zhang et al. (2023) & 74.773 & 0.0051 & 17.572 & 31.250 & **50** \\  & FouriScale Huang et al. (2024) & 75.378 & 0.007 & 17.987 & 31.028 & 162 \\  & AccDiffusion Lin et al. (2024) & 65.728 & 0.004 & 19.776 & 31.789 & 231 \\  & Pixelsmith (**Ours**) & **63.116** & **0.002** & **24.242** & **33.429** & 130 \\  \)} & SDXL Podell et al. (2023) & 195.117 & 0.069 & 7.709 & 24.565 & 515 \\  & ScaleCrafter He et al. (2023) & 105.132 & 0.018 & 13.542 & 27.767 & 1257 \\   & DemoFusion Du et al. (2023) & 66.186 & 0.003 & 18.940 & 32.319 & 1632 \\   & HiDiffusion Zhang et al. (2023) & 97.614 & 0.015 & 13.681 & 27.708 & **255** \\   & FouriScale Huang et al. (2024) & 125.390 & 0.028 & 11,837 & 26.802 & * \\   & AccDiffusion Lin et al. (2024) & 67.084 & 0.003 & 19.323 & 32.010 & 1710 \\   & Pixelsmith (**Ours**) & **63.686** & **0.002** & **19.741** & **32.369** & 549 \\  

* Inference time not available for FouriScale at \(4096^{2}\) resolution due to out of memory on an RTX 3090.

Table 2: Quantitative comparisons with existing works.

## 8 Acknowledgements

D.F., A.T. acknowledge support from Royal Academy of Engineering through the Chairs in Emerging Technologies scheme. D.F., R.M-S., C.K. acknowledge funding from _QuantIC_ Project funded by EPSRC Quantum Technology Programme (grant EP/MO1326X/1, EP/T00097X/1), and _Google_. R.M-S, C.K. acknowledge funding from EP/R018634/1, EP/T021020/1, and EP/Y029178/1. For the purpose of open access, the author(s) has applied a Creative Commons Attribution (CC BY) license to any Accepted Manuscript version arising.