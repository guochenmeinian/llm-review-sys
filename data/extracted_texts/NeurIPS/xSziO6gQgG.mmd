# Implicit Optimization Bias of

Next-token Prediction in Linear Models

 Christos Thrampoulidis

Department of Electrical and Computer Engineering

University of British Columbia

Vancouver, Canada

cthrampo@ece.ubc.ca

###### Abstract

We initiate an investigation into the optimization properties of next-token prediction (NTP), the dominant training paradigm for modern language models. Specifically, we study the structural properties of the solutions selected by gradient-based optimizers among the many possible minimizers of the NTP objective. By framing NTP as cross-entropy minimization across _distinct_ contexts, each tied with a _sparse_ conditional probability distribution across a finite vocabulary of tokens, we introduce "NTP-separability conditions" that enable reaching the data-entropy lower bound. With this setup, and focusing on linear models with fixed context embeddings, we characterize the optimization bias of gradient descent (GD): Within the data subspace defined by the sparsity patterns of distinct contexts, GD selects parameters that equate the logits' differences of in-support tokens to their log-odds. In the orthogonal subspace, the GD parameters diverge in norm and select the direction that maximizes a margin specific to NTP. These findings extend previous research on implicit bias in one-hot classification to the NTP setting, highlighting key differences and prompting further research into the optimization and generalization properties of NTP, irrespective of the specific architecture used to generate the context embeddings.

## 1 Introduction

Next-token prediction (NTP) has emerged as the go-to paradigm in training modern language models, revolutionizing various applications such as machine translation, text-summarization, and language generation . In NTP, models are trained to predict the most probable token given a sequence of preceding tokens, commonly referred to as the _context_. Concretely, the objective is to learn a mapping from the input context to the probability distribution over the (finite) vocabulary of possible tokens, enabling the model to generate a token that is contextually appropriate [9; 8]. Recently, the NTP paradigm has witnessed remarkable empirical success through its utilization on large-scale deep-learning architectures trained on vast corpora of data [66; 67; 86], leading to unprecedented advances in the field, and the swift integration of these advanced language models into society . Concurrently, researchers have raised critical concerns about robustness, interpretability, and fairness-bias issues arising from our limited understanding of the fundamental operational principles of these models [10; 6]. Despite progress, a comprehensive theory that elucidates the fundamentals of modern language models--including key components like the NTP paradigm and transformer architecture, particularly in terms of optimization and generalization principles--is still lacking.

We initiate an investigation when implicit optimization biases in training language models under the NTP paradigm, particularly in overparameterized regimes where the empirical-loss reaches its lower bound and there is many possible minimizers. To formalize the NTP paradigm, considerautoregressive model \(q_{}\) parameterized by \(\) trained to predict the next-token on sequences of length \(T\) using the cross-entropy (CE) loss:

\[_{}\,}_{_{n}}_{t[T]}- (q_{}(z_{t}\,|\,z_{1},,z_{t-1})).\] (1)

Here, sequences \(=(z_{1},,z_{T})\) consist of tokens \(z_{t}\) from a finite vocabulary \(=\{1,,V\}\) and \(}\) is expectation over training set \(_{n}\) of \(n\) such sequences sampled from some underlying true distribution over sequences. Typically, the model \(q_{}\) outputs probability of the next token computed via softmax applied on output logits, which are computed by projecting \(d\)-dimensional embeddings \(h_{^{}}\) to the \(V\)-dimensional space with a trainable linear decoder \(^{ d}\). Formally, 1

\[q_{}(z_{t}\,|\,z_{1},,z_{t-1})=_{z_{t}} h_{^{}}(z_{1},,z_{t-1})\,=j^{} v\\ z^{} z_{t}}(_{z^{}}-_{z_{ t}})^{}h_{^{}}(z_{1},,z_{t-1})}\,.\]

The CE loss is then minimized over \(=(,^{})\) using gradient-based methods, e.g. (S)GD, Adam.

We pose the question: _Given training set \(_{n}\), what are the structural properties of the weights \(\) found by minimizing the NTP objective with gradient-based optimizers?_ As in prior research in one-hot supervised classification 2 (e.g. [97; 7; 76; 34]), we specifically target this question in an _overparameterized_ setting, where the NTP objective (1) may have an infinite number of solutions, representing an infinite number of models \(\) that minimize the training loss. The central challenge is to discern the particular solution the optimizer is inherently biased towards. Since this 'bias' is not explicitly introduced through regularization but is instead in the training objective and algorithmic structure, it is termed 'implicit bias' . The exploration of implicit bias has a long history in the traditional supervised one-hot classification (see _Related Work in Sec. 6_). In this traditional scenario, the training set comprises feature-label pairs \((,y)\), where \(^{p}\) is a continuous feature, and \(y\) represents its unique label. The optimization process minimizes the following training objective (over \(,^{}\)): \(}_{(,y)}[-(_{y}(h_{^{}}()))].\)

At first glance, excluding the sequential format of Eq. (1), the NTP training scenario might seem identical to traditional one-hot prediction: both aim to minimize the same CE loss across models that parameterize probabilities using the softmax of logits. Consider predicting the next token over fixed-length sequences, say sequences of length \(t-1\), via optimizing: \(}_{}[-(_{z_{t}}(h_{}(z_{1},,z_{t-1})))].\) The context here acts as the feature, and the next token as the label. Recent works [49; 52] draw on such apparent similarities to the traditional one-hot classification paradigm to extrapolate known results from the latter to the NTP setting. However, this comparison overlooks a fundamental, yet critical difference in the nature of the training data that distinguishes these two paradigms (even when the sequential format of Eq. (1) is disregarded): In the traditional setting, each feature (e.g., image) is assigned a single label (e.g., image category). In contrast, in the NTP setting, contexts \(z_{1},,z_{t-1}\) of finite length sampled from finite vocabularies are naturally repeated in a (vast) training set, potentially multiple times, each time followed by _different_ tokens \(z_{t}\). Consequently, the NTP paradigm involves training over \(m n\)_distinct_ (non-repetitive) contexts, each followed by a multitude of possible next tokens, appearing at varying frequencies. For instance, the context "She is excellent at her role as a" may be followed by next tokens such as "doctor," "lawyer," "reviewer," or "mother," each with different frequencies. Importantly, certain vocabulary tokens may _not_ appear after a given context; e.g., in the above example, tokens like "run," "and," etc., will not follow.

**Model.** We study NTP training over a finite vocabulary employing the following model. Given a large training set of \(n\) total sequences, we identify \(m n\)_distinct_ contexts. Each distinct context \(j[m]\) is linked to a \(V\)-dimensional empirical probability vector \(}_{j}\), which encodes the frequency with which each vocabulary token follows the context throughout its occurrences in the training set. Crucially, the probability vectors \(}_{j}\) are _sparse_, i.e., the support set \(_{j}\) of \(}_{j}\) satisfies \(|_{j}|||=V\). In an extreme where \(|_{j}|=1, j[m]\), the probability vector \(}_{j}\) becomes one-hot, leading to a scenario reminiscent of the traditional classification setting described earlier. However, such an extreme is essentially improbable in practical language modeling . With this framing, the NTP paradigm is also related to supervised vision classification with _soft labels_, which advocates for training models on datasets where each example is associated with a vector of soft labels (rather than a one-hot vector), such as by averaging multiple annotators' hard labels , knowledge distillation  or label smoothing . With this connection, our analysis can also be interpreted (more broadly) as investigating the implicit bias of _sparse_ soft-label classification.

### Contributions and Organization

**Formulation.** Recognizing the differences between NTP and one-hot classification, we study the question of implicit optimization bias within the NTP setting. To facilitate this, we utilize the model outlined in the previous paragraph and detailed in Sec. 2. For concreteness, our analysis adopts a 'top-down' approach, training only the decoding (also referred to as word-embedding) matrix \(^{V d}\) while keeping context-embeddings fixed. This approach mirrors foundational studies on implicit optimization bias in one-hot classification [76; 34], which first focused on linear models. It allows exploring the complexities of the NTP training objective, distinct from the embedding architecture3, and while it renders the logits linear and the objective convex, it still poses a technical challenge in terms of determining parameter convergence [76; 34; 37; 60; 38].

**Conditions for reaching entropy.** In Sec. 3, we identify the necessary and sufficient conditions for the logits of the trained model to enable the CE loss to approach its lower bound, the empirical conditional entropy. We introduce two conditions: \(_{}\)-compatibility and NTP-separability, which impose constraints on mutually orthogonal subspaces that are determined by the _sparsity patterns_ of _distinct_ contexts within the dataset. These conditions determine the necessary and sufficient overparameterization a model needs to achieve the empirical entropy lower bound during training.

**Margin in NTP setting.** Motivated by the NTP-separability condition, we introduce a margin concept for NTP in Sec. 4, which extends the classical definition of margin used in one-hot supervised classification . We further establish the relevance of this new margin notion for optimization by demonstrating that a decoder maximizing the NTP-margin, denoted as \(^{}\), guides the directional convergence of the ridge-regularized CE minimizer, \(}_{}\), as the regularization parameter \( 0\).

**Implicit bias of GD.** We establish that \(^{}\) also determines the implicit bias of gradient descent (GD) iterates in Sec. 5. Specifically, in the limit of iterations \(k\), the GD iterates grow undoubtedly in norm and converge to a finite \(^{}\) within a data subspace \(\), while simultaneously aligning with \(^{}\) in the complementary subspace \(^{}\). The finite component \(^{}\) solves a system of linear equations associated with the \(_{}\)-compatibility condition.

Finally, we numerically verify these findings and discuss related and future work in Secs. 6 and 7. Additional experiments, further related work and detailed proofs are in the appendix.

## 2 Setup

Let vocabulary \(=[V]:=\{1,,V\}\) represent a set of \(V\) tokens (e.g. words) and \(_{1:t}=(z_{1},,z_{t})\) denote sequence of \(t\) tokens \(z_{t}\). To simplify presentation, we focus on predicting the \(T\)-th token \(z_{T}\) given contexts \(_{<T}:=_{1:T-1}\) of fixed length, and we further let \(=_{<t}\) denote the context and \(z\) denote the last token. See App. C for straightforward extension to the sequential format of Eq. (1).

We assume access to a training set consisting of \(n\) sequences \(_{n}:=\{(_{i},z_{i})\}_{i[n]}\), with \(_{i}:=^{T-1}\) and \(z_{i}\). Let \(h:^{d}\) an embedding map that maps contexts (i.e., sequences of \(T-1\) tokens) to \(d\)-dimensional embeddings. The map \(h\) can be parameterized (e.g. by a transformer  or an LSTM ), but this paper assumes that it is fixed. The next-token is predicted via a linear model \(f_{}:^{V}\) parameterized by decoding matrix \(^{V d}\), such that \(f_{}()=h()\). When the model output passes through a softmax, it defines the model's probability mass function for the next-token prediction, given as \(_{}(|)=(f_{}())\), where \(():^{V}^{V-1}\) is the softmax and \(^{V-1}\) is the \(V\)-dimensional simplex. The decoder is trained by minimizing the empirical CE loss \(():=_{i[n]}-(_{} z_{i}|_{i})).\)

**Distinct sequences and next-token distributions.** Given dataset \(_{n}\) we denote \(}_{1},,}_{m}\) the \(m n\)_distinct_ contexts among the (large number of) total \(n\) contexts \(_{1},,_{n}\) within \(_{n}\). Let \(_{j}\)be the empirical probability of distinct context \(}_{j}\). That is, \(1 n_{j} n\) is the number of contexts \(_{i}\) that equal \(}_{j}\). Furthermore, for each distinct context \(}_{j},j[m]\) let \(}_{j}^{V-1}\) denote the probability vector of conditional next-token distribution, i.e., \(_{j,z}:=}(z|}_{j}),z,j [m]\). In other words, \(n_{j}_{j,z}\) is the number of occurrences of token \(z\) as a follow-up to context \(}_{j}\). Finally, we denote the support set and size of the _support set_ of these conditional distributions as \(_{j}:=\{z|_{j,z}>0\}\) and \(S_{j}:=|_{j}|\). Tokens \(z_{j}\) and \(v_{j}\) are referred to as 'in-support' and 'out-of-support' respectively. Onwards, we implicitly assume that "_not all tokens are likely after every context_,' i.e. \( j[m]\) such that \(S_{j}<V\). This mild assumption is naturally satisfied in language modeling under rich enough vocabulary. With this notation, 4 we can express the NTP training loss as

\[()=-_{j[m]}_{j}_{z} _{j,z}(_{z}(h(}_{j})))=-_ {j[m]}_{j}_{z_{j}}_{j,z}( _{z}(}_{j})),\] (2)

where, in the last line we defined the shorthand \(}_{j}=h(}_{j})\). Similarly, we let \(_{i}=h(_{i}),i[n]\). With some abuse of notation, we then obtain the following equivalent descriptions of the training set

\[\{(_{i},z_{i})\}_{i[n]}=:_{n}_{m}:=\{( }_{j},_{j},_{j,z})\}_{j[m]}\]

that emphasizes _distinct_ contexts and their respective sparse next-token probability distributions.

**Entropy.** The _empirical \(T\)-gram entropy_ (referred to hereafter as entropy for simplicity) of the dataset is : \(_{T}:=:=}_{(,z)_{n}} [-((z|))]=-_{j[m]}_{z _{j}}_{j}_{j,z}(_{j,z})\). It lower bounds the \(\) loss since \(()=+(} \|}_{})\) and the KL divergence is nonnegative.

## 3 When can the NTP-loss reach the entropy lower-bound?

The first question we ask is: Under what conditions on the training data can the \(\) loss reach its entropy lower-bound? By the entropy lower-bound, \(()=( }\|}_{})=0\) iff for all \(j[m]\) and all \(z\): \(_{}(z|}_{j})=_{j,z}\). Equivalently, for all \(j[m]\):

\[_{z}(}_{j}) =_{j,z}, z_{j}\,,\] (3a) \[_{v}(}_{j}) =0, v_{j}\,.\] (3b)

Beginning with (3a), this requires5 the training data to satisfy the \(_{}\)-compatibility condition defined below.

**Definition 1** (\(_{}\)-compatible).: _Let \(_{v}\) denote the \(v\)-th standard basis vector in \(^{V}\). We say that training data \(_{m}\) are NTP-entropy-compatible if there exists \(V d\) matrix \(^{}\) satisfying:_

\[ j[m],z z^{}_{j}\,:\,\,\,\,(_{z}-_{z^{}})^{}^{}}_{j}=(_{j,z}/_{j,z^{}})\,.\] (4)

We comment on the independence of the constraints: Fix any \(j[m]\). Then, the set of constraints (as expressed in Eq. (4)) for all \(z z^{}_{j}\) (yielding \(}{2}\) constraints in total) is equivalent to the set of the same constraints for any anchor \(z_{j}_{j}\) and \(z^{} z_{j}_{j}\), i.e., an effective total of \(S_{j}-1\) linearly independent constraints for each \(j[m]\). Additionally, note that the system of equations in Eq. (4) constrains \(^{}\) with respect to a specific subspace of \(V d\) matrices:

\[=(\{(_{z}-_{z^{}}) }_{j}^{}\,:\,z z^{}_{j},j[m]\} ),\] (5)

that is defined in terms of context embeddings and their respective support sets. Assuming Eqs. (4) have a solution, we denote the _unique_ solution _within the subspace_\(\) as \(^{*}\) for later reference 6.

Next, we examine Eq. (3b), which requires softmax outputs be zero for tokens that never occur following a fixed context throughout the dataset. Due to the strict positivity of softmax, the constraint is never satisfied for _finite_\(\). Thus, for all finite \(\), there exists a gap between the cross-entropy loss and its lower bound, i.e., \(()>\). Yet, it is possible to approach entropy as the norm of the weights \(\) grows, provided that weights move in the appropriate direction formalized below.

**Definition 2** (NTP-separable).: _We say that training data \(_{m}\) are NTP-separable if there exists \(V d\) matrix \(^{}\) satisfying the following:_

\[ j[m],z z^{}_{j} : (_{z}-_{z^{}})^{}^{ }}_{j}=0\] (6a) \[ j[m],z_{j},v_{j} : (_{z}-_{v})^{}^{} }_{j} 1\,.\] (6b)

As before, it is easy to see that the constraints in (6) can be equivalently expressed by enforcing (6a) and (6b) for an anchor \(z_{j}_{j}\) and all \(z^{}_{j}\{z_{j}\}\) and \(v_{j}\), respectively. Consequently, there exist effectively \(V-1\) linearly independent constraints per context \(j[m]\).

We now discuss the interpretation of these constraints. The subspace constraints in Eq. (6a) project \(^{}\) onto the subspace \(^{}\), which is the orthogonal complement of the subspace \(\) defined in (5). This leaves the softmax probabilities of possible next tokens (in set \(_{j}\)) intact, and fully determined by \(^{}\) as per the NTP\({}_{}\)-compatibility condition. Formally, \(^{}+^{}\) continues satisfying (4). Moving on the halfspace constraints in (6b), we can interpret these using Kesler's construction as enforcing linear separability in the space \(^{V d}\): Each \(d\)-dimensional context embedding \(}_{j}\) is mapped to \(S_{j}(V-S_{j})\) higher-dimensional points \((_{z}-_{v})}_{j}^{},z_{j},v _{j}\). These points collectively for all \(j[m]\) must lie within the interior of the same halfspace induced by the hyperplane \((^{},)=0\). Refer to Fig. 1(Left) and its caption for an alternative interpretation of the rows of \(^{}\) as word-embeddings in \(^{d}\) (illustration in \(d=2\)).

The impact of NTP-separability on the softmax probabilities can be understood algebraically by considering \(_{}:=^{}\) and \(v_{j}\). We have:

\[_{v}(^{}}_{j}) =_{z_{j}}e^{(_{z}-_{v}) ^{}^{}}_{j}}+_{v^{} _{j}}e^{(_{v^{}}-_{v})^{}^{ }}_{j}}^{-1}\] \[_{z_{j}}e^{(_{z}-_ {v})^{}^{}}_{j}}^{-1}\] \[ e^{-},\] (7)

where the first inequality removes non-negative exponential terms and the second one follows from (6b). The upper bound above approaches \(0\) as \(\), thus (3b) holds asymptotically in \(\).

Taking into account the observations made above, the satisfaction of both conditions guarantees convergence of the cross-entropy loss \(\) to \(\). This is formalized in the proposition below.

**Proposition 1**.: _Assume training data \(_{m}\) is NTP\({}_{}\)-compatible and NTP-separable, with the respective matrices \(^{}\) and \(^{}\) satisfying conditions (4) and (6). While all finite \(\) satisfy \(()>\), it holds for \(^{}=^{}+^{}\) that \((^{})\)._

Hence, \(\) approaches its lower-bound in the limit of a _direction_\(^{}}:=^{}/\|^{}\|\) and _offset_\(^{}\) satisfying the constraints of NTP-separability and NTP-compatibility, respectively. In other words, parameter weights \(\) that minimize the \(\) loss consist of two components: a finite projection \(_{}:=_{}()=^{}\) onto the data subspace \(\) and an infinite-norm component onto the orthogonal complement \(^{}\) in the direction of \(^{}\).

Finally, we note that while Defns. 1 and 2 are stated for linear models, they naturally extend to a more general formulation for _nonlinear_ models. Specifically, consider NTP-separability (similar for NTP-compatibility): the general conditions require that both the decoder weights \(\) and model weights \(\), which parameterize the embeddings \(}_{j}=h_{}(}_{j})\), must satisfy Eq. (6) simultaneously.

### The role of overparameterization

We show that overparameterization provides a sufficient condition for the solvability of Eqs. (4) and (6). Start with the halfspace constraints in Eq. (4) for NTP\({}_{}\)-compatibility. These can be compactly expressed as \(_{j,z}^{}}_{j}=_{j,z}\), where \(_{j,z_{j}}^{(S_{j}-1) V}\) has rows \(_{z_{j}}-_{z^{}}\) and \(_{j,z_{j}}^{(S_{j}-1)}\) has entries \(_{j,z_{j}}/_{j,z^{}}\) for some anchor \(z_{j}_{j}\). Now, since the rows of \(_{j,z_{j}}\) are linearly independent, the question becomes equivalently that of determining when \(^{}}_{1},,}_{m}= _{1,z_{1}}^{}_{1,z_{1}},,_{m,z_{m}}^{} _{m,z_{m}}\) has a solution. This is always the case when \(d>m\) and the \(d m\)embedding matrix \(}=[}_{1},,}_{m}]\) is full rank (\(m\)). Then, there exists \(^{}\) such that condition (4) holds. In fact, \(}^{}\) has a nullspace, implying the existence of an infinite number of solutions to (4). These solutions take the form \(^{}=^{}+^{}_{}\), where \(^{}\) is the unique solution onto the subspace, and \(^{}_{}^{}\).

In contrast to (4), the constraints in (6) involve linear inequalities. However, a sufficient proxy for feasibility in this case is that the corresponding system of equations (instead of inequalities) has a solution. By following the exact same argument as before, we arrive at the same sufficient conditions for the existence of a solution \(^{}\). We summarize these findings.

**Lemma 1** (Overparameterization implies NTP-separability).: _Assume overparameterization \(d>m\) and full-rank embedding matrix \(}^{d m}\). Then, there exists an infinite number of solutions \(^{}\) and \(^{}\) that satisfy conditions (4) and (6), respectively._

Thus, \(d>m\), 7 which also generically favors full-rankness of the embedding matrix , implies both NTP\({}_{}\)-compatibility and NTP-separability. Combined with Prop. 1, it also implies that there are infinitely many possible directions \(^{}\) along which the NTP loss approaches \(\), motivating the implicit-bias question: For a specific iterative algorithm aimed at minimizing the NTP loss, which direction does it prefer? We will address this question in the remainder of the paper.

**Remark 1**.: _In the trivial case where \(S_{j}=1, j[m]\) (one-hot classification), the entropy lower bound is zero and is attained iff the data is linearly separable. Indeed, \(\) reduces to the empty set, and NTP-separability simplifies to traditional multiclass separability. For binary classification,  showed that \(d/m>1/2\) is sufficient and necessary for data in general position to be linearly separable. More recently, several works have extended this analysis to structured (random) data, including [71; 57; 54; 12]. The exact threshold in corresponding mutliclass settings is more intricate, but [19; 81; 11] have made progress in this direction. An interesting question is determining exact thresholds for NTP-separability, which would improve upon the sufficient condition of Lemma 1._

## 4 Regularization path

This section investigates the implicit bias of NTP by examining the minimization of CE loss through iterates defined as follows for an increasing sequence of positive regularization parameters \(B\):

\[}_{B}:=_{|\| B}()\,.\] (8)

This involves minimizing a strictly convex function in a bounded domain; thus, \(}_{B}\) is unique. This section's main result characterizes the limit of \(}_{B}\) as \(B\) under NTP-separability/compatibility. Before that, we first define the next-token prediction support-vector machines (SVM) problem.

**Definition 3** (Ntp-Svm).: _Given NTP-separable training set \(_{m}\), NTP-SVM solves the following:_

\[^{}:=_{}\ \|\| ^{V d}$ satisfying  and . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . This is a strongly convex quadratic program with \(mV-_{j[m]}S_{j}\) linear inequality and \(_{j[m]}S_{j}-m\) linear equality constraints. Its solution can be also defined as the classifier that maximizes margin between in and out-of -support tokens while being constrained on the orthogonal compelemnt \(^{}\):

\[}^{}=_{|\|=1, ^{}}_{j[m],z_{j},v_{j}}( _{z}-_{v})^{}}_{j}\,.\]

It turns out this direction determines the preferred limiting direction of the regularization path.

**Theorem 1** (Implicit bias of the regularization-path).: _Assume training data \(_{m}\) is NTP\({}_{}\)-compatible and NTP-separable. Let \(}_{B}\) be defined as in (8). Then, it holds that \(_{B}\{}_{B}}{|}_{B}|}, ^{}}{|}^{}|}\}=1\,.\)_

The proof sketch below illustrates how the NTP-separability/compatibility assumptions influence the outcome and why the regularization path induces an optimization bias toward the NTP-SVM direction. Complementing Thm. 1, we also show (see Lemma 4 in the appendix) that \(_{B}_{}_{B}=^{}\,.\) These together provide a complete characterization of the implicit optimization bias of (8).

Proof sketch (App. E.2 for details).: We first show \(}_{B}\) is on the boundary: \(\|}_{B}\|=B\). If not, then \((}_{B}),^{}=0\). But, few algebraic manipulations show \(-(}_{B}),^{}\) equals

\[_{j[m]}_{j}_{z_{j}}_{j,z}_ {z^{}_{j},z^{} z}s_{j,z^{}}\,(_{z}- _{z^{}})^{}^{}}_{j}+_{v _{j}}s_{j,v}\,(_{z}-_{v})^{}^{}}_{j},\]

where we denote \(s_{j,v}:=_{v}(}_{B}}_{j})>0,v,j[m]\). The first term in the parenthesis is zero by (6a), while the second term is strictly positive by (6b), leading to contradiction.

Now, consider a 'genie' point \(_{B}^{}=^{}+R(B)^{}\), where \(^{}\) satisfies (4), and \(R=R(B)\) is chosen such that \(\|_{B}^{}\|=B\). We will show that \(_{B}^{}\) attains a small CE loss as \(B\) (hence, \(R\)) grows. To do this, denote for convenience the logits

\[_{j,v}^{}:=_{v}^{}^{}}_{j} _{j,v}^{}:=_{v}^{}^{}}_{j}\]

for all for \(v,j[m]\), and note that \(_{v}^{}_{B}^{}}_{j}=_{j,v}^{}+R\, _{j,v}^{}\). By using (4) and (6a):

\[_{z^{}_{j}}e^{-(_{j,z}^{}+R_{j,z}^{ }-_{j,z^{}}^{}-R_{j,z^{}}^{})} =_{z^{}_{j}}e^{-(_{j,z}^{}-_{j,z^{} }^{})}=_{z^{}_{j}}_{j,z^{}}}{ _{j,z}}=_{j,z}}.\]

Moreover, using (6b) and defining \(C:=Ve^{\|^{}\|}\) for \(M:=_{j[m]}|}_{j}|\), gives:

\[_{v_{j}}e^{-(_{j,z}^{}+R_{j,z}^{ }-_{j,v}^{}-R_{j,v}^{})} e^{-R}\,_{v _{j}}e^{-(_{j,z}^{}-_{j,v}^{})} C\,e^{-R}.\]

Combining the above within Eq. (2), using \((1+x) x,x>0\) and the fact that \(_{j},_{j,z}\) are probabilities, yields:

\[(_{B}^{})_{j[m]}_{j}_{z _{j}}_{j,z}_{j,z}}+C\,e^{-R} +C\,e^{-R}\,.\] (9)

Next, towards contradiction, we will show that if \(}_{B}\) is _not_ in the direction of \(^{}\), then it incurs a loss that is larger than \((_{B}^{})\). The trick here is to bound the KL divergence term:

\[(}_{B})-=_{j[m]}_{j }_{z_{j}}_{j,z}_{j,z}_{z ^{}_{j}}e^{_{j,z^{}}-_{j,z}}+_{v _{j}}e^{_{j,v}-_{j,z}},\] (10)

where we denote logits \(_{j,v}:=_{v}^{}}_{B}}_{j}\). Assume there exists \(>0\) and arbitrarily large \(B\) satisfying:

\[\|^{}\|/B\,}_{B}-^ {}>.\] (11)

Define \(}=(}_{B}-^{})/R^{}(B),\) where \(R^{}=R^{}(B)>0\) can be chosen so that \(\|}\|=\|^{}\|\). Further choose \(B\) large enough so that Eq. (11) guarantees \(\|}-^{}\|^{}\), for some \(^{}>0\). Since \(^{}\) is the unique minimizer of (NTP-SVM) and \(\|}\|=\|^{}\|\), there exists \((0,1)\) and \(j[m]\) such that at least one of the following is true: _(i)_\( z\) and \(z^{} z_{j}\) such that \(|(_{z}-_{z^{}})^{}}}_{j}|\)_(ii)_\( z_{j},v_{j}\) such that \((_{z}-_{v})^{}}}_{j} 1-\).

_Case (i):_ Without loss of generality \((_{z}-_{z^{}})^{}}}_{j}-\) (otherwise, flip \(z,z^{}\)). Thus, ignoring all but the \((j,z,z^{})\)-term in (10) and using \(_{j,z^{}}-_{j,z} R^{}+_{j,z ^{}}}{_{j,z}}\) gives

\[(}_{B})-_{j}_{j,z} _{j,z}e^{(_{j,z^{}}-_{j,z})}}}{n}\,.\]

Comparing this to (9) for large enough \(B\) gives that \((}_{B})>(_{B}^{})\), a contradiction.

_Case (ii):_ We can assume \(}^{}\), since otherwise we are in Case (i). Now, again ignoring all but the \((j,z)\) term in the CE loss for which the assumption holds for some \(v_{j}\), we find

\[(}_{B})-_{j}_{j,z} _{j,z}_{z^{}_{j}}e^{(_{j,z ^{}}-_{j,z})}+e^{(_{j,v}-_{j,z})}.\]Using \(_{}(}_{B})=^{}\) and (4) yields \(_{z^{}_{j}}e^{(_{z^{}}-_{z,z})}=}}\). Moreover, by assumption of Case (ii): \(e^{_{z,v}-_{z,z}} e^{-R^{}(1-)}\,e^{_{z,v}^{*}-_ {z,z}^{*}} e^{}e^{-R^{}(1-)}\), for \(c^{} e^{-|^{}||M}\). Putting together yields:

\[(}_{B})-_{j}_{j,z}1+_{j,z}c^{}e^{-R^{}(1-)} c ^{}e^{-R^{}(1-)}2n^{2}\,,\]

where the second inequality uses \((1+x),x>0\). Compare this with (9): For large enough \(B\), since \(R,R^{}\) grow at the same rate, it holds \(}{2n^{2}}e^{-R^{}(1-)}>Ce^{-R}\). Thus, \((}_{B})>(_{B}^{*})\), a contradiction. In either case, we arrive at a contradiction, which completes the proof. 

## 5 Gradient Descent

This section studies the implicit bias of GD. Denote the GD iterates at time \(k\) by \(_{k}=_{k-1}-(_{k-1})\) for arbitrary initial point \(_{0}\) and constant step-size \(>0\) small enough to guarantee descent. The first observation is that the norm of the GD iterates increases with iterations.

**Lemma 2** (Norm growth).: _If training data are \(_{}\)-compatible and NTP-separable, then \(_{k}(_{k})=\) and \(_{k}\|_{k}\|=\)._

This is intuitive because the CE loss is convex in \(\) (thus, GD approaches the objective's infimum \(\)), and, in view of Proposition 1, the CE loss at all finite \(\) is bounded away from \(\). The relevant question then becomes that of determining the limit of the direction of the GD iterates.

**Theorem 2** (Implicit bias of GD).: _Assume \(_{}\)-compatible and NTP-separable training data \(_{m}\). Then, it holds that \(_{k}\{_{k}}{|_{k}|},^{ }}{|^{}|}\}=1\,.\) Moreover, \(_{k}_{}(_{k})=^{*}\)._

The theorem establishes 8 that in the limit of iterations: \(_{k}^{*}+\|_{}(_{k})\|^{}},\) which is analogous to the result we obtained previously for the regularization path. Although its proof is more involved compared to the proof of Thm. 1, the proof of its main ingredient (Lem. 5 in the appendix) is conceptually similar: It involves comparing the loss \((_{k})\) for large iterations \(k\) to the loss evaluated at a "genie" point that is chosen so that: (i) On the subspace \(\), it agrees with \(_{k}\). This is because it is easy to show that \(_{}(_{k})\) converges to \(^{*}\) by standard gradient descent analysis for convex functions; (ii) On the orthogonal subspace \(^{}\), it follows the optimal (with respect to accelerating loss decrease) max-margin direction \(^{}}^{}\). To establish the loss comparison, the ideas is to compare the values of the adjusted loss \(_{}()()- (_{}())\).

We validate our analysis with experiments on synthetic data in App. A. For illustration, Fig. 1 shows a 2D setting with \(m=3\) distinct contexts, each followed by \(S_{j}=3\) tokens/words out of total \(V=5\) words in the vocabulary. The left subfigure illustrates: (i) In black markers, the context-embedding geometry along with the associated support sets for each context A, B, and C. (ii) In colored markers, the geometry of word-embeddings, that is the max-NTP-margin vectors \((^{})^{}e_{v},v\), to which GD directionally converges. See caption for interpretation and Fig. 2 in the App. for vis. of the finite component of word-embeddings on the subspace \(\). The right subfigure shows results of GD training with respect to training loss, norm growth, alignment with \(^{}\), and convergence to \(^{*}\) on \(\). See App. A for further implementation details and additional experiments.

## 6 Related work

We build on the literature on implicit optimization bias of CE loss in one-hot supervised classification.  show that for linear models and linearly-separable data, GD converges in direction to the max-margin classifier. This result strengthens  that showed the regularization path of CE minimization converges to the same limit. Closer to us, [34; 37] extend the analysis to encompass general binary data as follows: the data are linearly separable only on a certain subspace, and they show that GD converges, in direction, towards the max-margin classifier confined within that subspace. On the orthogonal subspace, it converges to a finite point. While operationally similar, Thms. 1, 2 cannot be directly derived from theirs since our setting is neither binary nor one-hot. Nevertheless, our proofs extend the foundational work of [68; 34; 37], akin to numerous other studies that explore extensions to nonlinear architectures[50; 35; 28; 29; 83; 89], and to stochastic and adaptive algorithms [60; 64; 21; 47; 77; 3; 14; 2]. The implicit bias viewpoint has also created opportunities to study generalization in overparameterized settings. [31; 4; 57; 22] build a two-stage approach initially leveraging implicit bias to simplify the complexities of optimization before addressing generalization. This narrows the generalization question to the properties of the corresponding max-margin classifier [58; 13; 43; 78; 23; 100; 72; 94]. The same strategy has also been adopted to study model robustness to adversarial perturbations [33; 80; 16], out-of-distribution data , and imbalances [69; 15; 42]. Our results motivate such extensions in the richer NTP setting.

Recent work  also studies forms of implicit bias for language models trained to reach the risk lower bound. However, they assume training with population loss and analyze implicit bias through Hessian-trace minimization without providing explicit parameter characterizations as in Thm. 2. Crucially, their results do _not_ apply to CE loss9 or to sparse support-sets. Another interesting work  studies learning abilities of autoregressive training and inference. However, their findings do _not_ apply to NTP as they inherently assume each context is followed by a unique next token.

Finally, although stemming from different perspectives, the form of our convergence results echoes a recent conjecture by  regarding implicit optimization bias in transformers. Unlike their conjecture, which focuses on binary classification, our results are rigorously proven and apply to the NTP setting. Further detailed discussion on related follow-up work on implicit optimization bias in self-attention architectures, as initiated by , is deferred to Appendix B. In contrast to this line of work, we here focus on the optimization biases of the NTP training-paradigm itself, which is orthogonal to the intricacies of the specific architecture generating the context embeddings.

Figure 1: Vis. of NTP implicit optimization bias in a setting with \(m=3\)_distinct_ contexts, embedding dimension \(d=2\), vocabulary of \(||=5\) words and support sets of length \(|_{j}|=3,j\). _Left:_ Vis. of _context embeddings_\(}_{j}\) in circle black markers (marked as A,B,C) and of their associated support sets \(_{j}\) (colored text below each marker). Colored vectors (star markers) represent max-NTP-margin vectors \(_{}^{}:=_{v}^{}^{},v\) found by GD. Interpreting decoder vectors as _word embeddings_ leads to intuitive findings on their geometry learned by NTP training. E.g., word embedding \(_{3}\) (almost) aligns with context-embedding \(A\) and the normal hyperplane it defines separates \(A\) from \(B\) and \(C\), since word \(3\) only appears after context \(A\). The rest of the words follow two contexts each and their word-representation naturally belongs to the cone defined by the embeddings of those respective contexts. The wider the cone, the larger the magnitude of the word embedding to compensate for the large angle between context-representations that share the same next-word. Note that geometry of depicted word embeddings only depends on support sets, but the conditional probabilities define another set of word representations on an orthogonal (matrix) subspace; see text for details and vis. _Right:_ Upper/lower graphs confirm the predictions of Lemma 2 and of Theorem 2, respectively.

Conclusion, limitations and future work

Towards characterizing implicit regularization effects, we highlight two key aspects of NTP training: _(i)_ Formulating it as CE optimization over _distinct_ contexts; this is long recognized in language modeling (e.g., ) since Shannon's initial work, yet seemingly overlooked in recent studies, such as . _(ii)_ Accounting for _sparsity_ in the matrix of next-token conditional probabilities. While traditional language modeling techniques often mitigate sparsity using smoothing heuristics that assign non-zero probabilities to unobserved next tokens , we recognize sparsity as a critical factor in NTP optimization that influences parameter divergence10.

As the first study of implicit biases in NTP training, our results are based on several assumptions essential for establishing an initial foundational understanding. The framework allows for various exciting promising research directions, some of which we outline below.

Even within the assumed linear setting and GD, interesting directions involve:

\(\)**NTP-separability thresholds:** Identifying exact thresholds for NTP-separability under distributional assumptions, akin to previous work on one-hot separability (Remark 1). However, relaxing the overparameterization requirement that the embedding dimension \(d\) be proportional to the number of distinct contexts \(m\) would necessitate exploring non-convex architectures (see 'Memory capacity' below).

\(\)**Generalization:** Studying generalization in NTP settings by examining statistical properties of the NTP-SVM solution. Past research has successfully undertaken similar investigations for one-hot classification (see Sec. 6). While we acknowledge the importance of addressing specific challenges inherent to NTP --such as determining an appropriate measure of generalization, or establishing suitable statistical models for context-embeddings that respect the discrete nature of the underlying token subsequences--we believe this direction holds promise for further exploration.

In addition to these, essential extensions include relaxing the linearity assumption.

\(\)**Architecture-specific embeddings:** A bottom-up approach considering architecture-specific embeddings could begin by modeling the embeddings produced by, for instance, a shallow transformer and analyzing the effects of optimization biases on the training of both the transformer and the decoder weights. This complements the works of , who investigate one-layer self-attention with a fixed decoder. A challenge in this approach is balancing the restriction to shallow transformers (for analytical tractability) with ensuring that the NTP loss reaches the entropy lower bound. This may require constraining the training data distribution, for example, to a Markov chain .

\(\)**Memory capacity in NTP settings:** Without imposing further restrictions on the data beyond the discrete nature of tokens from a finite vocabulary, there is a strong case for investigating the memory capacity of sequence-to-sequence architectures, such as transformers, in the context of NTP. Recent studies on transformer memory capacity  do _not_ apply here.

\(\)**Unconstrained features:** Extending the top-down approach, one could consider freely optimizing context embeddings together with decoder vectors (also known as word embeddings). The resulting log-bilinear model, reminiscent of wor2vec models , extends the unconstrained features model, which has recently been employed to investigate neural collapse geometry in one-hot classification settings . This idea offers a promising avenue for uncovering structures in the geometries of context and word embeddings when learned jointly, potentially revealing new insights into the capabilities of sufficiently expressive language models (see Fig. 1 for cases involving only the latter).

\(\)**Other optimizers:** Exploring the NTP implicit bias of adaptive algorithms, such as Adam, potentially building on recent works in this area focused on one-hot classification .

We hope this work inspires further research in the discussed directions, contributing to a deeper understanding of the intricacies involved and potentially yielding improvements in NTP training.