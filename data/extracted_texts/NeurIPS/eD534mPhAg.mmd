# Evaluating Post-hoc Explanations for Graph Neural Networks via Robustness Analysis

Junfeng Fang1, Wei Liu1\({}^{}\), Yuan Gao1, Zemin Liu2, An Zhang2,

Xiang Wang1\({}^{}\), Xiangnan He1\({}^{}\)

\({}^{1}\)University of Science and Technology of China,

2National University of Singapore

{fjf,lw0118,yuangao}@mail.ustc.edu.cn,

{zeminliu,an_zhang}@nus.edu.sg,

{xiangwang1223,xiangnanhe}@gmail.com

Liu Wei is equal contribution to this paper.

###### Abstract

This work studies the evaluation of explaining graph neural networks (GNNs), which is crucial to the credibility of post-hoc explainability in practical usage. Conventional evaluation metrics, and even explanation methods -- which mainly follow the paradigm of feeding the explanatory subgraph to the model and measuring output difference -- mostly suffer from the notorious out-of-distribution (OOD) issue. Hence, in this work, we endeavor to confront this issue by introducing a novel evaluation metric, termed OOD-resistant Adversarial Robustness (OAR). Specifically, we draw inspiration from adversarial robustness and evaluate post-hoc explanation subgraphs by calculating their robustness under attack. On top of that, an elaborate OOD reweighting block is inserted into the pipeline to confine the evaluation process to the original data distribution. For applications involving large datasets, we further devise a Simplified version of OAR (SimOAR), which achieves a significant improvement in computational efficiency at the cost of a small amount of performance. Extensive empirical studies validate the effectiveness of our OAR and SimOAR. Code is available at https://github.com/MangoKiller/SimOAR_OAR.

## 1 Introduction

Post-hoc explainability has manifested its extraordinary power to explain graph neural networks (GNNs) . Given a GNN-generated prediction for a graph, it aims to identify an explanatory subgraph, which is expected to best support the prediction and make the decision-making process more credible, fair, and understandable . However, the reliable evaluation of explanation quality remains a key challenge. As a primary solution, _Human supervision_ seeks to justify whether the explanations align with human knowledge , but it is often too subjective, thus hardly providing quantifiable assessments. Another straightforward solution is quantitatively measuring the agreement between the generated and ground-truth explanations, such as _Precision_ and _Recall_. Unfortunately, access to the ground truth is usually unavailable and labor-extensive, thereby limiting the scope of evaluations based on this method.

Recently, a compromised paradigm -- _Feature Removal_ -- has been prevailing to quantitatively evaluate the explanation's predictive power as compared to the full graph, without exploiting the human supervision and ground truth. The basic idea is to first remove the unimportant features and feed the remaining part (_i.e._, explanatory subgraph) into the GNN, and then observe how the prediction changes. The prediction discrepancy instantiates _Accuracy_ and _Fidelity_ of theexplanation, reflecting "how accurate and faithful the explanation is to recover the prediction of the input graph". Despite the prevalence, these removal-based metrics usually come with the caveat of the out-of-distribution (OOD) issue [14; 13]. Specifically, as the after-removal subgraphs are likely to lie off the distribution of full graphs [15; 16], the GNN is forced to handle these off-manifold inputs and easily gets erroneous predictions [17; 18]. Take Figure 1 (a) as an example. For the full molecular graph, the GNN classifies it as "mutagenic", which is reasonable due to the presence of mutagenic \(-_{2}\) group; whereas, when taking the subgraph, _i.e.,_ non-mutagenic C-Cl group solely as the input, the GNN surprisingly maintains its output "mutagenic". Clearly, the prediction on the explanatory subgraph might be skeptical, which easily deteriorates the faithfulness of the removal-based evaluations.

In sight of this, recent efforts [18; 17] are beginning to mitigate the OOD issue via the _Generation-based_ metrics. Instead of directly feeding the to-be-evaluated subgraph into the target GNN, they use a generative model [19; 20] to imagine and generate a new full graph conditioned on the subgraph. These methods believe that the generation process could infill the subgraph and pull it closer to the original graph distribution. As Figure 1 (b) shows, comparing the predictions on this new graph and the original graph could be the surrogate evaluation. While intuitively appealing, the generative models easily inherit the data bias and inject it into the infilling part. Considering Figure 1 (b) again, since molecules in the Mutagenicity dataset comprising non-mutagenic chloride (\(-\)) always carry amino (\(-_{2}\)), a generative model is prone to capture this occurrence bias and tend to infill \(-_{2}\) with the \(-\)-involved subgraphs. This bias not only exerts on the generations but also makes the evaluation inconsistent with the GNN behavior: in the generative model, \(-\) is assigned with more "mutagenic" scores as it usually accompanies the mutagenic partner \(-_{2}\); in contrast, the GNN finds no "mutagenic" clues in \(-\). In a nutshell, the generation-based metrics show respect to the data distribution somehow but could be inconsistent with GNNs' behavior and lose control of the infilling part.

Scrutinizing these removal- and generation-based metrics (as summarized in Figure 1), we naturally raise a question: "_Can we design a metric that respects the data distribution and GNN behavior simultaneously?_" To this end, we draw inspiration from adversarial robustness [21; 22] and propose a new evaluation framework, OAR (OOD-resistant Adversarial Robustness), to help reliably assess the explanations. As shown in Figure 1 (c), OAR encapsulates two components: constrained attack and OOD reweighting, which respect the GNN behavior and data distribution, respectively. Specifically:

* Intuitively, perturbations on label-irrelevant features should be ineffective to the GNN prediction, while those on label-relevant features are supposed to be impactful and destructive to the prediction [22; 21]. Hence, for the original input graph, the attack model performs perturbations constrained on the complementary part of its explanation. This perturbing game aims to naturally take control of the "infilling" process, making the explanatory subgraph less influenced by its infilling.
* Having obtained a set of perturbed graphs, the reweighting component estimates the OOD score of each perturbed graph, which reflects the degree of distribution shift from the original data

Figure 1: Pipelines and flaws of different evaluation methods. In the “Input” graph, \(-_{2}\) is considered as the ground truth explanation for its mutagenicity. Best viewed in color.

distribution. Then, we feed these graphs into GNN and reweight their predictions with OOD scores.

The sum of weighted predictions can quantitatively evaluate the importance of the target subgraph.

We validate the effectiveness of OAR in evaluation tasks across various start-of-the-art explanation methods, datasets, and backbone GNN models. OAR has manifested surprising consistency with the metrics like _Precision_, _Recall_ and _Human Supervision_. Furthermore, to better generalize to the large datasets, we also provide a Simplified version of OAR (SimOAR) achieving significant improvements in computational efficiency at the expense of a small amount of performance degradation. Our main contributions can be summarized as follows:

* We propose a novel metric, OAR for evaluating GNNs' explainability, which tries to resolve the limitation of current removal- and generative-based evaluations by taking both data distribution and GNN behavior into account (Section 2.2).
* We provide a simplified version of OAR, SimOAR for better generalization to the evaluation tasks involving large datasets, which greatly shortens the execution time while only sacrificing a small amount of performance (Section 2.3).
* Experimental results demonstrate that our OAR/SimOAR outperforms the current evaluation metrics by a large margin, and further validate the high efficiency of SimOAR (Section 3).

## 2 Methodology

In this section, we propose an evaluation method for the explainability of GNNs from the perspective of adversarial robustness. We start with the notation of GNNs and its explainability in Section 2.1. After that, we detail our evaluation metric, OAR via three progressive steps (Section 2.2). In Section 2.3, we provide a simplified version of OAR called SimOAR for applications demanding more efficient execution.

### Problem Formulation

**Graph neural networks (GNNs).** GNNs have achieved remarkable success due to their powerful representation ability. Without loss of generality, we focus on the graph classification task in this work: a well-trained GNN model \(f\) takes a graph \(\) as the input and outputs its probabilities \(\) over classes \(\{1,...,C\}\), _i.e._, \(=f()^{C}\). Typically, \(\) is an undirected graph involving the node set \(\) and the edge set \(\). We first introduce the feature of node \(v_{i}\) as a \(d\)-dimensional vector and collect the features of all nodes into \(^{|| d}\). Then we define an adjacency matrix \(^{||||}\) to describe graph topology, where \(A_{uv}=1\) if the edge connecting nodes \(u\) and \(v\) exists, _i.e._, \((u,v)\), otherwise \(A_{uv}=0\). Based on these, \(\) can be alternatively represented as \(=(,)\).

**Explainability for GNNs.** Upon the GNN model \(f\), explanation techniques of GNNs generally study the underlying relationships between their outputs \(\) and inputs \(\). They focus on explainability _w.r.t._ input features, aiming to answer "_Which parts of the input graph contribute most to the model prediction?_". Towards this end, explainers typically assign an importance score to each input feature (_i.e._, node \(v_{i}\) or edge \((v_{i},v_{j})\)) to trace their contributions. Then they select the salient part (_e.g._, a subset of nodes or edges with top contributions) as the explanatory subgraph \(_{s}\) and delete the complementary part \(_{}=_{s}\). We formulate the explanation method as \(h\) and yield the above process as \(_{s}=h(,f)\).

### OOD-Resistant Adversarial Robustness

Retrospecting the removal- and generation-based evaluations, we emphasize that both these classes come with inherent limitations. Specifically, Removal-based metrics pay less heed to the data distribution thus forcing GNNs to handle off-manifold instances, while generation-based metrics are inconsistent with GNN behavior and lose control of the infilling part. Fortunately, in this section, we claim that it is possible to get the best of and avoid the pitfalls of both worlds -- removal-based and generation-based metrics -- by taking both GNN behavior and data distribution into account.

To meet these challenges, we elaborate our evaluation metric, OOD-resistant adversarial robustness (OAR) via three progressive steps: **in the first step**, we formulate the adversarial robustness tailored for GNNs explanations, which naturally conforms to the GNN behavior; **in the second step**, we introduce a tractable and easy-to-implement objective of above adversarial robustness; **in the third step**, we introduce an elaborate OOD reweighting block which confines the overall evaluation process to the original data distribution.

**STEP 1: Formulation of Adversarial Robustness.** We prioritize the introduction of _adversarial robustness_ in machine learning [21; 22; 23] that motivates our method. Concretely, given a machine learning model, an input \(x\) and a subset of the input \(x_{s} x\), the _adversarial robustness_ of \(x_{s}\) denotes the minimum perturbation leading to the wrong prediction, on the condition that perturbation is only imposed on \(x_{s}\). Inspired by this idea, we define the adversarial robustness of GNN explanation \(_{s}\), and formulate it as the minimum adversarial perturbation \(\) on the structure of complementary subgraph \(_{s}\). More formally,

**Definition 1**.: _Given a GNN model \(f\), an input graph \(=(,)\) with prediction \(\) and explanation \(_{s}\), suppose that \(^{}=(^{},^{})\) is the graph generated by adding and deleting edges in \(\), the adversarial robustness \(\) of explanation \(_{s}\) is defined as:_

\[_{_{s}}=_{^{}}_{u}_{v u}|A_{uv}-A^{}_{uv}|\] (1) _s.t._ \[*{arg\,max}_{i}f(^{})_{i} *{arg\,max}_{i}_{i},_{u_{s}}_{v _{s} u}|A_{uv}-A^{}_{uv}|=0,\]

_where \(\) and \(_{s}\) are the node sets of \(\) and \(_{s}\), respectively._

_Definition 1_ identifies the quality of explanation \(_{s}\) as the difficulty of reversing the prediction by perturbing features not belonging to \(_{s}\) solely. That is, when \(_{s}\) is fixed, the more difficult it is to fool the model by perturbing its complementary, the more important \(_{s}\) is. The key intuition behind this inference is: if an explanation comprises most of the label-relevant features, it is conceivably hard to change the prediction by manipulating the remaining features that are label-irrelevant. Thus, according to _Definition 1_, we can find that: a good explanation would yield high adversarial robustness \(\) and vice versa, which naturally conforms to the GNN behavior.

It seems that adversarial robustness is the feasible metric to evaluate the GNNs' explanations. However, there are still two matters standing in the way of its adoption: 1) Is its objective (_i.e._, Equation (1)) tractable and easy to implement? 2) Does it respect the data distribution?

**STEP 2: Finding a Tractable Objective.** To answer the first question, we argue that Equation (1) is hard to realize and sometimes even intractable owing to two possible reasons:

* The primary reason is that adversarial attacks may fail to find any adversarial example, since a solution satisfying two conditions in Equation (1) simultaneously may not exist. In other words, if the explanation \(_{s}\) is precise enough, it is almost impossible to reverse the prediction via manipulating features in the complementary part \(_{s}\) which are mainly label-irrelevant.
* It is notoriously hard to search for the minimum adversarial perturbation \(\) in most cases. Current attack methods  typically turn to find an alternative sub-optimal solution. Thus, leveraging these methods could introduce additional bias and threaten the fairness of evaluation.

To address these issues and make the evaluation objective tractable and easy to implement, we first formulate the inference derived from _Definition 1_ :

**Proposition 1**.: _When the high quality explanation \(_{s}\) is anchored (fixed), perturbations restricted to the complementary part \(_{s}\) have a weak influence on the model prediction._

While _Definition 1_ evaluates the explanations via the adversarial robustness \(\), _Proposition 1_ indicates a more straightforward way to the tractable evaluation objective from its dual perspective. To be more specific, _Definition 1_ quantifies perturbation on \(_{s}\) causing change of prediction; conversely, _Proposition 1_ quantifies change of prediction caused by perturbation on \(_{s}\). More formally,

Figure 2: The pipeline of OAR, which takes both model behavior and data distribution into account.

**Definition 2**.: _Given a GNN model \(f\), an input graph \(=(,)\) with prediction \(\) and explanation \(_{s}\), suppose that \(^{}=(^{},^{})\) is the graph generated by adding and deleting edges in \(\), the approximate adversarial robustness \(^{*}\) of \(_{s}\) is defined as:_

\[^{*}_{_{s}}=_{^{}} (f(^{})_{c}-_{c})\] (2) _s.t._ \[c=_{i}_{i},_{u_{s}}_{v _{s} u}|A_{uv}-A^{}_{uv}|=0,\]

_where \(_{s}\) refers to the node set of \(_{s}\); \(c\) denotes the predicted class of \(\) by model \(f\); \(f(^{})_{c}\) represents the probability value of \(f(^{})\) for the given class \(c\)._

Stemmed from _Definition 2_, the evaluation method to quantify the adversarial robustness of GNN explanations is more explicit and computationally convenient. As shown in Figure 2, for the to-be-evaluated subgraph \(_{s}\) (_i.e._, C-CI in red dotted box), we anchor it and randomly perturb the remain part \(_{s}\) to get graphs \(^{}\) (_i.e._, molecules in green dotted box). Then we compare the expectation of the prediction of \(^{}\) with the prediction of the original graph \(\). If they are close, most features in \(_{s}\) must be label-irrelevant. Hence, we can assign high quality for the explanation \(_{s}\).

So far, there is only one question left: how to ensure that the aforementioned evaluation process respects the data distribution?

**STEP 3: OOD Reweighting Block Tailored for GNNs.** Before elaborating our OOD block, we first retrospect that in most scenarios, a tiny perturbation would not induce large distribution shifts along the input space, thanks to the approximate continuity of input features (_e.g._, image pixels in _computer vision_). Unfortunately, the structural features of GNNs' inputs -- adjacency matrix comprising of \(0\)s and \(1\)s -- are discrete thus only one perturbation (_e.g._, adding or deleting an edge) could induce large distribution shifts, and further violate the underlying properties, such as node degree distribution , graph size distribution  and domain-specific constraints .

Thus, it is pivotal to construct an OOD reweighting block for assessing whether the generated graph \(^{}\) deviates from the data manifold. This block is expected to assign an "OOD score" -- the degree of distribution shift between \(^{}\) and original graph \(\) -- to each \(^{}\). However, it is non-trivial to quantify the degree of OOD . Inspired by the great success of _graph anomaly detection_, we treat the OOD instance as the "anomaly" since it is isolated from the original data distribution, and naturally employ the common usage module of anomaly detection -- _variational graph auto-encoder_ (VGAE)  containing an encoder and a decoder -- to instantiate our OOD reweighting block. Additionally, the great success of diffusion generative models has been recognized recently , and thus this domain is deferred for future investigation.

Concretely, as shown in Figure 2, the preparation for evaluations is training the VGAE model on the dataset \(\) where input graph \(\) is sampled. After that, we can leverage the reconstruction loss to approximate the degree of OOD for any generated instance \(^{}\). To be more specific:

* Given \(^{}=(^{},^{})\), the **encoder** first learns a latent matrix \(\) according to \(^{}\) and \(^{}\), where row \(_{i}\) corresponds to the node \(v^{}_{i}\) in \(^{}\). Note that \(_{i}\) is assumed to follow the independent normal distributions with expectation \(_{i}\) and variance \(^{2}_{i}\). Formally: \[q(|^{},^{})=_{i=1}^{|^{} |}q(_{i}|^{},^{})=_{i=1}^{|^{ }|}(_{i}_{i},(^{2}_ {i})),\] (3) where \(\) and \(\) are parameterized by two two-layer GCNs  called \(_{}\) and \(_{}\).
* Then, the **decoder** recovers the adjacency matrix \(^{}\) based on \(\): \[p(^{}|)=_{i=1}^{|^{}|} _{j=1}^{|^{}|}p(A^{}_{ij}_{i},_{j}),\] (4) \[p(A^{}_{ij}=1_{i},_{j})=( _{i}^{}_{j}),\] where \(()\) is the logistic sigmoid function.
* The **OOD score** of \(^{}\) is given by the normalized reciprocal of the reconstruction loss \(_{recon}(^{})\), \[_{recon}(^{})=- p(^{}), ==_{}(^{},^{}).\] (5)Since VGAE is trained on the dataset \(\), \(^{}\) straying far from the data distribution of \(\) would get the high reconstruction loss \(_{recon}\). Thus, as the reciprocal of \(_{recon}\), the OOD score of \(^{}\) must be low. Conversely, if \(^{}\) is in distribution, it would gain a high OOD score because it is easy to be reconstructed. Based on this, our OOD block can mitigate the impact of OOD instances. Specifically, the OOD score is utilized as the weight of each prediction when calculating the expectation of the generated graph's prediction. This allows for the marginalization of instances with low OOD scores, as shown in the gray dotted box of Figure 2.

**Overall evaluation process.** As the last piece of the OAR puzzle, _i.e.,_ OOD reweighting block has been instantiated, let's revisit Figure 2 and summarize the overall process of OAR:

1. [leftmargin=*]
2. Before we evaluate the explanatory subgraph \(_{s}\), the OOD reweighting block (_i.e.,_ VGAE) is trained on the dataset \(\) where input graph \(\) is sampled.
3. Then, we fix the \(_{s}\) and randomly perturb the structure of the complementary part \(_{s}\) to get \(^{}\).
4. Each \(^{}\) is fed into GNN \(f\) and VGAE simultaneously to audit prediction and OOD score. Both GNN's behavior and data distribution are taken into consideration in this step.
5. At last, according to the predictions and their weights (_i.e.,_ OOD scores), we calculate the weighted average of the generated graphs' predictions. The closer this average is to the original prediction of \(\), the higher the quality of the explanation \(_{s}\) is.

The pseudocode and the tricks to expedite computations are detailed in Appendix A.

### A simplified version of OAR

To better generalize to large datasets and reduce the computational complexity, we provide a simplified version of OAR called SimOAR in this section. Compared with OAR, SimOAR achieves a significant improvement in computational efficiency at the expense of a small amount of performance degradation. Concretely, SimOAR is mainly motivated by three empirical inferences after executing OAR:

* The most time-consuming part of OAR is its preparatory work, _i.e.,_ training OOD reweighting block, especially for large datasets. For example, on the dataset _MNIST superpixels_ containing 70,000 graphs, the converged process of VGAE occupies \(93.7\%\) of OAR's execution time.
* In the course of generating \(^{}\), the number of perturbation operations is roughly proportional to the degree of distribution shift given by the OOD block. For example, the graph \(^{}_{1}\) created via deleting one edge typically gets a higher OOD score than the graph \(^{}_{2}\) created via deleting five edges.
* For two generated graphs generated via the same perturbation times, they generally get similar reconstruction losses and are assigned similar OOD scores.

Based on these, to expedite computations and simplify the OAR, we deactivate the OOD reweighting block (_i.e._, deleting all the sketches in gray dotted boxes in Figure 2) in OAR. As compensation for data distribution, we restrict the ratio of the number of perturbations to the number of edges in the original graph \(\) to a pre-defined minor value \(R\). Since the generated graphs typically share similar reconstruction losses and OOD scores while \(R\) is fixed, we directly calculate their average prediction to approximate their excepted prediction. The pseudocode and more implementation details of SimOAR are provided in Appendix A.

It is worth noting that despite the potential existence of a few generated graphs \(^{}\) in SimOAR that fall outside the distribution, the performance of SimOAR still significantly surpasses that of current evaluation methods _w.r.t_ consistency with both metrics based on ground truth and human supervision. Hence, in light of the efficiency of the SimOAR, we strongly advocate for its adoption as a predominant alternative to prevalent removal-based evaluation metrics. At the heart of SimOAR - and a **central thesis** of this paper - is the perspective that, during evaluation, rather than deleting all non-explanatory nodes and then gauging the resultant output variations, it is more insightful to **randomly delete a portion of the non-explanatory nodes multiple times and then gauge the average output variations.**

## 3 Experiments

We present empirical results to demonstrate the effectiveness of our proposed methods OAR and SimOAR. The experiments aim to investigate the following research questions:

* **RQ1:** How is the evaluation quality of OAR and SimOAR compared to that of existing metrics?
* **RQ2:** How is the generalization of OAR and SimOAR compared to that of existing metrics?
* **RQ3:** What is the impact of the designs (_e.g._, the OOD reweighting block) on the evaluations?

### Experimental settings

To evaluate the effectiveness of our method, we utilize four benchmark datasets: BA3 , TR3 , Mutagenicity [40; 41], and MNIST-sp , which are publicly accessible and vary in terms of domain and size. Moreover, to generate the explanations of the graphs in the datasets mentioned above, we adopt several state-of-the-art post-hoc explanation methods, _i.e.,_ SA , GradCAM , GNNExplainer , PGExplainer , CXPlain  and ReFine . The prevailing metrics -- removal-based evaluation (RM for short) and generation-based evaluation, _i.e.,_ DSE  -- are selected as the baselines. Detailed experimental details can be found in Appendix B.

### Measurement metric

We elaborate on the measurement metric of existing evaluation methods in this part since how to fairly define the quality of an evaluation method is critical to our research.

**Ground-truth explanations.** We first follow the prior studies [11; 45; 43] and treat the subgraphs coherent to data generation procedure or human knowledge as ground truth. Although ground truth might not conform to the decision-making process exactly, it contains sufficient discriminative information to help justify the quality of explanations. Moreover, it's worth emphasizing again that our method does not depend on ground truth, and gathering ground truth is only for fair comparison.

**Consistency with metric based on ground-truth explanations.** Specifically, given a to-be-evaluated subgraph \(_{s}\) and its corresponding ground-truth explanation \(_{s}^{GT}\), we use **Recall** as the gold evaluation metric defined as \((_{s})=|_{s}_{s} ^{GT}|/|_{s}^{GT}|\), where \(_{s}\) and \(_{s}^{GT}\) are the edge set of \(_{s}\) and \(_{s}^{GT}\); \(||\) denotes the cardinal function of set. Hence, for any evaluation method, we can calculate its consistency with Recall to quantify its performance via Kendall correlation \(\) defined as:

\[(\{r^{i}\}_{i=1}^{n},\{s^{i}\}_{i=1}^{n})= _{i<j}I((r^{i}-r^{j})=(s^{i}-s^{j})),\] (6)

where \(\{r^{i}\}_{i=1}^{n}\) and \(\{s^{i}\}_{i=1}^{n}\) are a pair of Recall values and evaluation scores; \(()\) is the sign function and \(I()\) is the indicator function. The bigger \(\) is, the higher the evaluation scores are consistent with Recall values, and thus the better the evaluation method is.

**Consistency with human intuition.** The consistency between evaluation results and human intuition is also an important reference. In view of the high subjectivity of human intuition, we organized a large-scale user study engaging 100 volunteers. Results are shown in Appendix C.

    & & SA & GradCAM & GNNExplainer & PGExplainer & CXPlain & ReFine & \(\) \\   & Recall & 88.12\(\)0.00\({}^{(1)}\) & 84.53\(\)0.00\({}^{(2)}\) & 76.83\(\)4.64\({}^{(3)}\) & 65.32\(\)5.21\({}^{(3)}\) & 54.77\(\)4.4\({}^{(6)}\) & 72.90\(\)13.72\({}^{(4)}\) & - \\  & RM & 35.39\(\)0.00\({}^{(3)}\) & 37.67\(\)0.00\({}^{(4)}\) & 43.32\(\)1.97\({}^{(1)}\) & 29.25\(\)2.3\({}^{(3)}\) & 27.78\(\)0.96\({}^{(6)}\) & 41.24\(\)1.55\({}^{(2)}\) & 0.73 \\  & DSE & 43.08\(\)2.31\({}^{(1)}\) & 41.38\(\)1.75\({}^{(2)}\) & 22.25\(\)2.14\({}^{(1)}\) & 37.92\(\)2.52\({}^{(3)}\) & 24.60\(\)3.75\({}^{(3)}\) & 29.31\(\)2.96\({}^{(6)}\) & 0.73 \\  & OAR & 93.12\(\)4.60\({}^{(1)}\) & 86.20\(\)3.72\({}^{(2)}\) & 80.19\(\)1.68\({}^{(3)}\) & 65.48\(\)3.7\({}^{(3)}\) & 59.69\(\)2.96\({}^{(6)}\) & 71.02\(\)4.54\({}^{(4)}\) & **1.00*** \\  & SimOAR & 84.39\(\)3.50\({}^{(1)}\) & 83.44\(\)4.11\({}^{(2)}\) & 62.25\({}^{(2)}\) & 50.22\(\)2.6\({}^{(2)}\) & 55.49\(\)2.92\({}^{(6)}\) & 60.42\(\)3.73\({}^{(4)}\) & **0.93** \\   & Recall & 82.08\(\)0.00\({}^{(1)}\) & 77.00\(\)0.00\({}^{(2)}\) & 60.09\(\)4.97\({}^{(4)}\) & 55.85\(\)4.78\({}^{(4)}\) & 44.39\(\)5.73\({}^{(6)}\) & 74.19\(\)3.30\({}^{(3)}\) & - \\  & RM & 55.08\(\)0.00\({}^{(3)}\) & 51.15\(\)0.00\({}^{(4)}\) & 79.08\(\)4.31\({}^{(1)}\) & 50.45\(\)2.01\({}^{(5)}\) & 47.72\(\)3.32\({}^{(6)}\) & 64.60\(\)2.72\({}^{(2)}\) & 0.67 \\  & DSE & 48.51\(\)1.00\({}^{(1)}\) & 37.32\(\)2.54\({}^{(2)}\) & 44.82\(\)2.90\({}^{(2)}\) & 33.71\(\)3.70\({}^{(2)}\) & 35.65\(\)1.92\({}^{(6)}\) & 39.49\(\)3.51\({}^{(3)}\) & 0.73 \\  & OAR & 95.23\(\)3.75\({}^{(1)}\) & 87.51\(\)6.60\({}^{(2)}\) & 72.63\(\)5.89\({}^{(3)}\) & 59.06\(\)4.87\({}^{(5)}\) & 51.61\(\)3.16\({}^{(3)}\) & 63.82\(\)5.43\({}^{(4)}\) & **0.93*** \\  & SimOAR & 88.45\(\)0.00\({}^{(4)}\) & 76.37\(\)4.52\({}^{(3)}\) & 53.54\(\)4.6\({}^{(6)}\) & 65.88\(\)3.89\({}^{(6)}\) & 62.98\(\)4.05\({}^{(7)}\) & 75.59\(\)2.73\({}^{(7)}\) & **0.86** \\   & Recall & 43.98\(\)0.00\({}^{(4)}\) & 44.39\(\)0.00\({}^{(4)}\) & 54.63\(\)0.09\({}^{(1)}\) & 30.13\(\)1.0\({}^{(6)}\) & 38.96\(\)2.67\({}^{(4)}\) & 47.88\(\)1.60\({}^{(2)}\) & - \\  & RM & 21.34\(\)0.00\({}^{(4)}\) & 19.10\(\)0.00\({}^{(5)}\) & 22.23\(\)1.02\({}^{(3)}\) & 25.04\(\)0.35\({}^{(2)}\) & 27.15\(\)0.69\({}^{(1)}\) & 17.58\(\)0.90\({}^{(6)}\) & 0.33 \\  & DSE & 30.37\(\)4.00\({}^{(2)}\) & 29.19\(\)2.30\({}^{(3)}\) & 14.03\(\)1.77\({}^{(6)}\) & 28.45\(\)2.60\({}^{(1)}\) & 22.95\(\)2.44\({}^{(4)}\) & 21.32\(\)1.29\({}^{(5)}\) & 0.20 \ by space, we further exhibit and discuss more detailed implementations and results in Appendix B, including but not limited to the generalization of OAR involving the evaluation of the post-hoc explanations for node classification task and the inherent explanations.

### Study of explanation evaluation (RQ1)

As a preparation for the experiments, we first collect the explanatory subgraphs \(\{^{i}_{s}\}\) for a set of graphs \(\{^{i}_{s}\}_{i=1}^{N}\) and the corresponding well-trained GNN model \(f\). We denote the evaluation score on \(^{i}_{s}\) based on RM, DSE and our OAR and SimOAR by \(s^{i}_{}\), \(s^{i}_{}\), \(s^{i}_{}\) and \(s^{i}_{}\), respectively. For more faithful comparison, we present both the explainer-level correlation defined as \(_{*}=(\{_{i=1}^{N}(^{i,h}_{s})\}_{h},\{_{i=1}^{N}s^{i,h}_{* }\}_{h})\) and the instance-level correlation defined as \(_{*}=(\{(^{i}_{s}) \}_{i=1}^{N},\{s^{i}_{*}\}_{i=1}^{N})\), where \(*\) can be RM, DSE, OAR, and SimOAR; \(^{i,h}_{s}\) means the subgraph \(^{i}_{s}\) is extracted by explainer \(h\), \(\) is the set of explainers. The explainer-level results, under all evaluation methods, on all datasets, are presented in Table 1. Moreover, considering the instance-level results share a similar tendency, we presented the representative results on BA3 and MNIST-sp in Figure 3 (a). According to Table 1 and Figure 3 (a) we can find that:

* **Observation 1: OAR outperforms other methods in all cases.** Substantially, Kendall rank correlation greatly improves after leveraging the paradigm of adversarial robustness. The most notable case is the explainers' rank on BA3 and MUTAG, where \(^{*}\) = 1.00 achieves a tremendous increase from the RM and the DSE. It demonstrates the effectiveness and universality of OAR/SimOAR and verifies that OAR/SimOAR can be leveraged to boost the quality of evaluations.
* **Observation 2: SimOAR performs a little worse than OAR, but still significantly improves over the strongest baselines.** To be more specific, the average score of SimOAR is 7.83% less than that of OAR, but still, 42.62% higher than RM and 37.45% higher than DSE. It demonstrates that SimOAR is adequate for the task of explanation evaluations in most cases.

Further analysis of the results presented in Table 1 reveals that:

* **Observation 3: OAR/SimOAR presents a more fair and faithful comparison among explainers.** The rankings provided by the OAR and SimOAR are highly consistent (_i.e._, \(^{*}=0.928\) on average) with the references, while the removal- and generation-based rankings unsurprisingly pale by comparison. These empirical results give us the courage to leverage OAR and SimOAR to evaluate emerging explanation methods in the future.

### Study of generalization (RQ2)

Although the experiment provided in 3.3 is detailed and fair, we contend that the generalization of OAR and SimOAR is still unexamined. Specifically, a specific explainer always has a preference

Figure 3: The performance of various evaluation metrics. (a) Correlation between metrics and Recall across various backbone explainers, where the vertical axis represents the normalized Kendall rank correlation. (b) Consistency between Recall and the scores provided by metrics. The more monotonously increasing the curve is, the better the evaluation metric is. Best viewed in color.

for certain patterns, and thus the explanatory subgraphs extracted by it often have similar structures. Therefore, the experimental results based on limited explainers may not generalize well to other existing or future explainers, especially those based on different lines of thought.

As it is impractical for us to cover all these explainers, we resort to directly generalizing the to-be-evaluated subgraphs. That is, we make use of fake explanatory subgraphs which are randomly sampled from the full graph. The detailed sampling algorithm and settings can be found in Appendix C. In these settings, the best case is that the evaluation score is monotonically increasing _w.r.t._ the Recall level, which indeed reaches the best consistency. Average normalized scores under all evaluation methods are shown in Figure 3 (b), which indicate that:

* **Observation 4: OAR/SimOAR greatly improves the consistency between evaluation scores and Recalls, which indicates that our method has tremendous potential to perform well on other explainers.** Conversely, removal- and generation-based methods are negatively correlated with the importance involving the set of to-be-evaluated subgraphs which get high Recalls.

### Study of designs (RQ3)

**Effectiveness of OOD block.** We first focus on the effectiveness of the OOD block. The most immediate impact of OOD block on OAR can be estimated by comparing the performance of OAR and SimOAR, which can be also deemed as the ablation experiments. Hence, we turn to qualitatively analyze the OOD block via some case studies shown in Figure 4, where all the OOD scores belonging to the same datasets are normalized to the range of 0 to 1. Based on the information conveyed in Figure 4, the following observation can be made:

* **Observation 5: OOD block can assign the lower weights to the subgraph which violent the underlying properties of the full graph.** For example, graph properties of chemical molecules, such as the valency rules, impose some constraints on syntactically valid molecules. Hence, the invalid molecular subgraphs, which destroy the integrity of the carbon ring by simply removing some bonds (edges) or atoms (nodes) are assigned low scores by the OOD block.

**Time complexity.** To further explore the efficiency of our evaluation method and the designed module in it, we count the running time of the evaluation process on every single graph and average the time over the entire test set to obtain per-graph time consumption. The comparison is provided in Table 2. According to Table 2 we can find that:

* **Observation 6: SimOAR greatly reduces the execution time.** Concretely, the execution speed has nearly doubled after leveraging the metric of SimOAR. This significant improvement in efficiency corresponds to the original intention of SimOAR and verifies the success of SimOAR's design.

## 4 Related Work & Further Discussion

**OAR & Contemporary Evaluation Metrics.** Apart from the qualitative evaluation methods based on human intuition, recent literature categorizes quantitative metrics into four primary categories: accuracy, faithfulness, stability, and fairness [47; 48; 49; 50; 51; 52]. Notably, Precision and Recall metrics align with the accuracy category, while our proposed OAR and SimOAR fall under the faithfulness category. Among these methods, the most recently proposed faithfulness metric is GEF , which however omits quantification of the distribution shift in subgraphs. Nevertheless, we

   & BA3 & TR3 & MNIST-sp & MUTAG \\  RM & 0.11\(\)0.02 & 0.10\(\)0.01 & 0.15\(\)0.03 & 0.13\(\)0.01 \\  DSE & 1.73\(\)0.27 & 1.60\(\)0.14 & 2.12\(\)0.39 & 1.88\(\)0.25 \\  OAR & 1.16\(\)0.05 & 1.32\(\)0.06 & 1.77\(\)0.10 & 1.78\(\)0.03 \\  SimOAR & 0.84\(\)0.07 & 0.65\(\)0.08 & 1.03\(\)0.07 & 0.96\(\)0.06 \\  

Table 2: Per-graph time consumption. (ms)

Figure 4: Case studies for OOD reweighting block with the graphs randomly selected from datasets MNIST-sp, MUTAG, and BA3 arranged from top to bottom. Best viewed in color.

have exhibited the experimental comparison between GEF and our metrics _w.r.t_ the latest dataset SHAPEGGEN  in Appendix B.

**OOD & GNNs Explainability** The OOD issue is one of the most critical challenges in the post-hoc explainability domain currently [16; 53]. To sidestep this challenge, many studies have pivoted towards the development of inherently explainable GNNs [15; 54]. Notwithstanding the complexity of the task, efforts such as FIDO  are making significant successes in addressing the OOD problem within post-hoc explanations. Concurrently, CGE  leverages the lottery ticket hypothesis [57; 58; 59; 60; 61] to craft the cooperative explanation for both GNNs and graphs, wherein the OOD challenge is potentially mitigated through the EM algorithm. GIBE  delves into the intersection of the OOD issue and regularization, viewing it through the lens of information theory. Furthermore, MixupExplainer  and CoGE  navigate the OOD problem from the generation and recognition stances, respectively.

Simultaneously, the evaluation of inherent explanations encounters the same hurdles as post-hoc explanations: it's challenging to quantify in the absence of the ground truth. Fortunately, by introducing an additionally well-trained GNN, OAR can be employed to **evaluate inherent explanations** in a similar way to evaluate post-hoc explanations. The experimental results are shown in Appendix B.

**Limitations & Concerns.** While we acknowledge the effectiveness of our methods, we also recognize their limitations. Concretely, despite utilizing SimOAR to expedite the evaluation process, our paradigm remains more time-intensive compared to the conventional removal-based metric. To overcome this constraint, a probable solution is summarizing the optimal number of perturbations and implementing a self-adaptive extraction module to select the perturbed features.

Furthermore, we recognize potential apprehensions regarding the migration of trust issues from the black-box GNN to the equally non-transparent VGAE. Nevertheless, we posit that the repercussions of this are substantially mitigated in our streamlined method. Specifically, SimOAR bypasses VGAE in favor of employing transparent heuristics for perturbation generation, effectively addressing the aforementioned trust concerns. It's noteworthy that, while SimOAR's performance may be marginally below or comparable to OAR's, it consistently exceeds other benchmarks. This emphasizes VGAE's restrained impact and reaffirms our recommendation of SimOAR over OAR.

## 5 Conclusion

In this paper, we explored the evaluation process of GNN explanations and proposed a novel evaluation metric, OOD-resistant adversarial robustness (OAR). OAR gets inspiration from the notion of adversarial robustness and evaluates the quality of explanations by calculating their robustness under attack. It addresses the inherent limitations of current removal- and generation-based evaluation metrics by taking both data distribution and GNN behavior into account. For applications involving large datasets, we introduce a simplified version of OAR (SimOAR), which achieves a significant increase in computational efficiency at the cost of a small amount of performance degradation. This work represents an initial attempt to exploit evaluation metrics for post-hoc GNN explainability from the perspective of adversarial robustness and resistance to OOD.

## 6 Ethics Statement

This work is primarily foundational in GNN explainability, focusing on the development of a more reliable evaluation algorithm. Its primary aim is to contribute to the academic community by enhancing the understanding and implementation of the evaluation process. We do not foresee any direct, immediate, or negative societal impacts stemming from the outcomes of our research.