# Hyper-opinion Evidential Deep Learning for

Out-of-Distribution Detection

 Jingen Qu

School of Computer Science and Technology

Tongji University, Shanghai, China

newcity@tongji.edu.cn

&Yufei Chen

School of Computer Science and Technology

Tongji University, Shanghai, China

yufeichen@tongji.edu.cn

&Xiaodong Yue

Artificial Intelligence Institute

Shanghai University, Shanghai, China.

yswantfly@shu.edu.cn

&Wei Fu

School of Computer Science and Technology

Tongji University, Shanghai, China

cs_fuwei@outlook.com

&Qiguang Huang

School of Computer Science and Technology

Tongji University, Shanghai, China

1753543@tongji.edu.cn

corresponding author

###### Abstract

Evidential Deep Learning (EDL), grounded in Evidence Theory and Subjective Logic (SL), provides a robust framework to estimate uncertainty for out-of-distribution (OOD) detection alongside traditional classification probabilities. However, the EDL framework is constrained by its focus on evidence that supports only single categories, neglecting the other collective evidences that could corroborate multiple in-distribution categories. This limitation leads to a diminished estimation of uncertainty and a subsequent decline in OOD detection performance. Additionally, EDL encounters the vanishing gradient problem within its fully-connected layers, further degrading classification accuracy. To address these issues, we introduce hyper-domain and propose Hyper-opinion Evidential Deep Learning (HEDL). HEDL extends the evidence modeling paradigm by explicitly integrating sharp evidence, which supports a singular category, with vague evidence that accommodates multiple potential categories. Additionally, we propose a novel opinion projection mechanism that translates hyper-opinion into multinomial-opinion, which is then optimized within the EDL framework to ensure precise classification and refined uncertainty estimation. HEDL integrates evidences across various categories to yield a holistic evidentiary foundation for achieving superior OOD detection. Furthermore, our proposed opinion projection method effectively mitigates the vanishing gradient issue, ensuring classification accuracy without additional model complexity. Extensive experiments over many datasets demonstrate our proposed method outperforms existing OOD detection methods.

## 1 Introduction

Deep Learning (DL) models have been widely adopted in many real-world applications[25; 57; 64; 15]. However, these models are trained under the implicit assumption that the training and test data aredrawn from the same distribution, leading to overconfident predictions. Thus when a DL model encounters an input that differs from its training data, it may be overconfident with wrong prediction, bringing rise to the out-of-distribution (OOD) problem. The resolution of the OOD problem is of utmost importance, and researchers have devoted significant attention to studying the intricacies of OOD detection.

To address OOD problem, a variety of methods have been developed in DL. Some researchers apply post-processors to the base classifier to generate an uncertainty score for OOD detection. These post-hoc methods only take effect at inference phase and are easy to use, but rely on the performance of the pretrained model. Others propose training methods that involve training-time regularization, which require more computational resources. To train an uncertainty-aware model without additional computation, a recent search leverages Evidence Theory and Subjective Logic (SL) with DNNs, called Evidential Deep Learning (EDL). EDL offers uncertainty estimation in neural networks which represents the degree of 'unknown' in opinion. It modifies the existing DL structure slightly and allows neural network to quantify the uncertainty for OOD detection with a well-defined theory framework. Evidential models have been extended to many areas such as open set recognition, classification, multi-view learning. The EDL models face several challenges, with one primary issue arising from the theoretical framework. The evidence in multinomial-opinion in EDL exclusively supports singleton sets, which contains only one category. In other words, EDL only captures the evidence which supports single category and rejects others. As a result, EDL is unable to effectively leverage vague evidence, such as features supporting a composite set containing multiple categories. As Figure 1 shows, EDL suffers from performance degradation in the face of ambiguous samples.

In addition, the parameters of fully-connected layer in EDL models are facing vanishing gradient problem when number of category in datasets rises. Vanishing gradient in EDL leads to failure in classification of several categories. To mitigate this problem, Pandey et al. introduce regularization techniques. However, these efforts yield unsatisfactory results in real world OOD detection tasks.

To train an evidential model maintaining classification accuracy and providing reliable uncertainty estimation for OOD detection, we incorporate EDL with hyper-opinion and propose Hyper-opinion Evidential Deep Learning (HEDL). While EDL is built upon multinomial-opinion in a basic domain, hyper-opinion represents the opinion in the hyper-domain, which includes the basic domain and the composite sets. Through the concepts of composite set, HEDL is able to learn from vague evidence ignored by EDL. HEDL provides an effective mechanism for quantifying evidence that supports composite sets, thereby enhancing the differentiation of OOD data and classification accuracy. Our major contributions can be summarized as follows:

* We introduce an evidential representation within the hyper-domain, which integrates sharp evidence that supports a singular category, with vague evidence that accommodates multiple potential categories, to establish a more comprehensive and accurate evidentiary foundation.
* We develop a hyper-opinion framework within the hyper-domain and propose a novel opinion projection. This method transfers hyper-opinion to multinomial-opinion, allocating evidence to each category precisely and mitigating the vanishing gradient problem, while preserving computational efficiency.

Figure 1: Belief and uncertainty masses across varying levels of In-distribution sample vagueness. As sample gets vaguer, EDL tends to extract a minimal quantity of sharp evidence, results in elevated uncertainty estimation. HEDL demonstrates the capability to extract vague evidence as sample vagueness increases, thereby maintaining lower uncertainty levels.

* Our proposed Hyper-opinion Evidential Deep Learning (HEDL) procures more exhaustive evidence, which refines the precision of uncertainty estimation, and consequently enhances the performance of OOD detection while maintaing ID classification accuracy.
* We carry out experiments over multiple challenging datasets to validate the OOD detection in HEDL outperforms existing OOD detection methods.

## 2 Related Work

### Uncertainty based OOD Detection

Accurately quantifying predictive uncertainty in DL models is crucial for recognizing out-of-distribution (OOD) samples. Traditional softmax-based models provide confidence estimation through class posteriors, which are inversely correlated with predictive uncertainty. Several methods applicable to pre-trained classifiers that output class posteriors using softmax have been proposed[3; 14; 53; 37; 60; 18], including Out-of-Distribution Detector for Neural Networks and Mahalanobis Distance. Besides, deep ensemble is a technique developed for uncertainty quantification, which constructs an ensemble of neural networks and measures uncertainty based on the agreement/disagreement across the ensemble components. However, this approach significantly increases the scale of model parameters, leading to high computational and storage complexity. Alternatively, neural networks based on Bayesian statistics called Bayesian neural networks[12; 4; 42] is raised to to quantify different uncertainties in Bayesian formalism. Bayesian methods normally apply approximation to address the intractability issue in marginalization of latent variables. And as such methods require sampling for uncertainty quantification, leading to expensive computations. A recent research effort has summarized OOD detection methods and established an OOD benchmark .

### Evidential Deep Learning

EDL introduces a conjugate higher-order evidential prior for the likelihood distribution that enables the model to capture the evidence vacuity as predictive uncertainty. The training of an EDL model can be regarded as an evidence-collecting process. Researches on multiple applications with EDL have been done, e.g., Dirichlet prior is introduced over the multinomial likelihood for evidential classification[2; 73; 11], evidential models for regression[1; 48], adversarial robustness and calibration. Most existing methods built upon EDL are trained on evidential losses conjunct with regularization of the evidence to guide the evidence vacuity, _i.e._, uncertainty, behavior[47; 56]. Some EDL models combine with the idea of outlier exposure that provides access of OOD data to guide the evidence learning process of EDL models[40; 41].

In this work, we focus on evidential models for classification and OOD detection, and consider settings where no extra regularization and OOD data are used during model training to make the proposed approach more broadly applicable to practical real-world situations.

Figure 2: Framework of HEDL. HEDL framework is composed of three integral components. The first part transfers the extracted features to evidence and models them with in hyper-opinion framework. Subsequently, the second component projects the hyper-opinion to multinomial-opinion. Ultimately, the framework optimizes the opinion to attain precise classification and to furnish robust uncertainty estimations for OOD detection.

Proposed Method

Our method's framework is depicted in Figure 2, which operates under the assumption of no prior information.

### Hyper-opinion Belief

Subjective Logic (SL) is a theory of uncertain reasoning based on probability theory and belief theory in a domain \(\), which represents the set of exclusive possible states of a variable situation. It introduces the concepts of belief mass and uncertainty mass to describe the degree of belief and uncertainty about an event.

Traditional EDL is built upon multinomial-opinion within domain \(\) in SL and domain \(\) is a limited portion of hyper-domain \(()\), where \(()\) is the powerset of \(\).

\[()=()/\{\{\},\{\}\}.\] (1)

Let us consider a domain \(\) with cardinality of \(K\), SL provides a belief mass \(b_{k}\) representing the belief degree and a base rate \(a_{k}\) representing the prior information for each singleton \(k=1,...,K\) and an overall uncertainty mass of \(u\). The three compose a multinomial-opinion \(=(,u,)\), belief mass and uncertainty mass sum up to one, eg.,

\[u+_{k=1}^{K}b_{k}=1, u 0 and b_{k} 0 for k =1,...,K.\] (2)

Our method models the evidence in hyper-domain \(()\) with hyper-opinion, which provides a belief mass \(b_{x}^{H},x()\), representing the belief degree of set \(x\). Along with \(^{H}\) and \(u\), the three compose a hyper-opinion \(^{H}=(^{H},u,^{H})\) and the hypernomial belief mass distribution also follows the additivity requirement:

\[b^{H}:()\] \[u+_{x()}b_{x}^{H}=1.\] (3)

Hyper opinion allows belief mass to be divided into two types called sharp belief mass and vague belief mass. Belief mass that only supports a specific singleton is called sharp belief mass, eg., \(k\), it discriminates between this and other singletons. EDL built upon the multinomial-opinion only offers sharp belief mass estimation. Considers a domain \(\) of K mutually exclusive singletons, for each singleton \(k=1,...,K\), sharp belief mass is

\[b_{k}^{}=b_{k}^{H}\, k.\] (4)

Belief mass assigned to a composite set \(x()\), where \(()=()/\), represents vague belief mass because it expresses cognitive vagueness. It supports the truth of multiple singletons in \(\) simultaneously. Vague belief mass can be allocated to a singleton \(k\) as

\[b_{k}^{}=_{x()}a(k|x)b_{x}^{H}, a( k|x)=}{_{i x}a_{i}}, k, x (),\] (5)

where \(a(k|x)\) is relative base rate. When no prior information is available, \(a(k|x)\) can be simplified to

\[a(k|x)=, k, x( ),\] (6)

where \(|x|\) is the cardinality of \(x\). Then in hyper-opinion, a belief mass \(b_{x}^{H}\) for a set \(x\) is computed using the evidence for the set. Let \(e_{x}^{H} 0\) be the evidence derived for the set \(x\), then the belief \(b_{x}^{H}\) and the uncertainty \(u\) are computed as

\[b_{x}^{H}=^{H}}{S} u=}{S},  S=_{x()}e_{x}^{H}+KW_{prior}.\] (7)

By introducing hyper-opinion, vague beliefs that assigned to composite sets can be take into consideration, which better measure comprehensive evidence and estimate uncertainty more accurately.

In practice, we activate the features extracted by the neural network as evidence in hyper-domain, and build them within hyper-opinion to distinguish sharp belief and vague belief. This allows the model to maintain its vagueness among similar in-distribution categories, thereby ensuring that the uncertainty remains low.

### Opinion Projection

A projection from hyper-opinion to multinomial-opinion is needed to realize the projected probability of each singleton. Therefore we introduce a novel opinion projection implementation that projects belief mass from hyper-opinion into multinomial-opinion, with \(b_{k}^{}\) and \(b_{k}^{}\) that can be calculated by Eq. 4 and Eq. 5, following

\[b_{k}=b_{k}^{}+b_{k}^{}, k.\] (8)

We activate the features extracted by neural network for ascertaining non-negative evidence within the hyper-domain. After associate evidence with belief in hyper-opinion, we determine the set each belief mass supports as mentioned in section 3.1, and project the belief mass from hyper-opinion to multinomial-opinion.

Specifically, we apply a unit step activation function to the parameters of the fully connected layer, eg., Heaviside function

\[H(x)=1,x>0\\ 0,else.\] (9)

It offers an access to a matrix \(W^{S}=H(W)\), where \(W\) corresponds to the weight matrix of the fully connected layer. \(W^{S}\) represents the information of set each belief mass supports.

Assume there are \(K\) singletons and \(N\) belief masses supporting different sets, it offers a matrix \(W^{S}_{N,K}\). For a belief mass \(b_{x}^{H}\) supporting set \(x\), \(W^{S}_{x}\) is a vector that contains information about which singletons belong to the set \(x\).

Once the set each belief mass supports has been identified, projecting hyper-opinion to multinomial-opinion is straightforward. For each belief mass within the hyper-opinion, we can compute its relative base rate to each singleton, and allocate belief mass accordingly. For a singleton \(k\), its total projected multinomial-opinion belief mass is

\[b_{k}=_{x(}(b_{x}^{H}W_{x,k}^{p}),\] (10)

\[W_{x,k}^{p}=H(W_{x,k})}{_{i=1}^{K}(a_{i}H(W_{x,i}))}=W_{x,k}^{S}}{_{i=1}^{K}(a_{i}W_{x,i}^{S})}, k,x (),\] (11)

where \(\) is the base rate. Without any prior information, Eq. 11 can be simplied to

\[W_{x,k}^{p}=^{S}}{_{i=1}^{K}W_{x,i}^{S}}, k,x().\] (12)

To date, we have successfully delineated the process of projecting belief mass from a hyper-opinion to a multinomial-opinion within a neural network framework. In practical terms, this projection is executed by applying a linear transformation to the output of the fully connected layer. This transformation facilitates the allocation of belief mass to the respective singletons in the multinomial-opinion. Consequently, the incremental computational complexity associated with our method is constant as O(1).

\[= G(W,^{H}), G(W,^{H})=^{ H}}{W^{H}},\] (13)

where \(\) is the output of fully-connected layer and \(W,W^{p},^{H}\) are all detached variables, making \(G(W,^{H})\) a constant during one training epoch.

The output after opinion projection represents the projected multinomial-opinion in EDL, which has the equivalent meaning in EDL and can be optimized with the same techniques. We used an example to show why the uncertainty estimation of HEDL outperforms EDL in Appendix A.

### Multinomial-opinion Optimization

By building evidence within hyper-domain and projecting hyper-opinion belief mass into multinomial-opinion belief mass, we construct a flow that can be optimized in multinomial-opinion framework to obtain the comprehensive evidence and accurate uncertainty estimation for OOD detection, which is similar to traditional EDL.

As the sum of evidence \(_{x()}e_{x}^{H}\) and uncertainty \(u\) remain the same during the projection, we can pass the belief mass in the form of evidence to simplify the calculation. Therefore the projected probability distribution derived from the projected multinomial-opinion can correspond to an expected probability distribution derived from a Dirichlet distribution parameterized by \(\)

\[&=(, u,) Dir(|.),\\ _{k}&=e_{k}+a_{k}W_{prior}=b_{k}S+a_{k}W_{ prior}.\] (14)

The Dirichlet distribution is a probability density function (pdf) for possible values of the probability mass function (pmf) \(P\) and is given by:

\[Dir(P|.)=)}_{i}^{K}p_{i}^{_{i}-1}.\] (15)

In projected multinomial-opinion, the expected probability for the \(k^{th}\) singleton calculation is

\[_{k}=}{S},\] (16)

which allows to be optimized by the loss function defined in EDL

\[_{i}()=[_{j=1}^{K}-y_{ij}(p_{ij})] )}_{j=1}^{K}p_{ij}^{_{ij}-1}d_{i}= _{j=1}^{K}y_{ij}(S_{i})-(_{ij}),\] (17)

where \(()\) is the digamma function, \(y_{i}\) is a one-hot vector encoding the ground-truth class of observation \(x_{i}\) with \(y_{ij}=1\) and \(y_{ik}=0\) for all \(k j\), and \(_{i}\) be the parameters of the Dirichlet density on the predictors.

At this point, we have established the complete framework of HEDL, spanning all stages ranging from input processing to classification and uncertainty estimation. Our method objective has the following proposition in the Appendix B.

By establishing the framework of HEDL, we comprehensively extract the sharp and vague evidence each sample contains and allocate preciously, thereby enabling accurate classification. Moreover, comprehensive evidence contributes to improved uncertainty estimation and subsequently enhances the performance of OOD detection.

## 4 Experiment

In this section, we describe our experimental setup and demonstrate the effectiveness of our method on a wide range of OOD evaluation benchmarks and the most widely used metric AUROC is adopted[52; 21; 10; 37]. We also conduct an ablation analysis that leads to an improved understanding of our approach.

### Setup

**In-distribution Datasets.** We use the CIFAR-10, CIFAR-100, Flower-102 and CUB-200-2011 as ID data.

**Out-of-distribution Datasets.** For the OOD test datasets, we use three common benchmarks: SVHN, Textures, Places365, that are used in Openood-benchmark. There is no overlapping between ID datasets and OOD datasets.

**Evaluation Metrics.** We measure the following metrics: 1) FPR95 measures the false positive rate (FPR) when the true positive rate (TPR) is equal to 95%. Lower scores indicate better performance. 2) AUROC measures the area under the Receiver Operating Characteristic (ROC) curve, which displays the relationship between TPR and FPR. The area under the ROC curve can be interpreted as the probability that a positive ID example will have a higher detection score than a negative OOD example. 3) AUPR measures the area under the Precision-Recall (PR) curve. The PR curve is created by plotting precision versus recall. AUROC is the most common metric[52; 21; 10; 37] and we use AUROC as the main metric for OOD detection performance while accuracy measures performance of detecting ID samples. Our goal is to detect more OOD samples while maintaining ID classification performance.

[MISSING_PAGE_FAIL:7]

### OOD Detection Results

The comparative results on CIFAR-10 and CIFAR-100 are detailed in Table 1, and the results on Flower-102 and CUB-200-2011 are shown in Appendix C. For each model, we utilize three OOD datasets, thereby aiming to achieve more realistic and generalized outcomes. We reveal a common challenge: when confronted with more complex data scenarios, training methods struggle to maintain both accuracy and OOD detection capabilities simultaneously. However, HEDL consistently achieves better OOD detection performance than existing state-of-the-art OOD detection methods while preserving the accuracy of ID classification, even under complex data scenarios. Notably, HEDL accomplishes this enhancement without additional regularization strategies or hyperparameters, indicating strong generalization ability on different datasets, it also avoids incurring higher computational costs. The experimental training time analysis of HEDL can be found in Appendix D.

### Gradient Analysis

The gradient norms of fully-connected layer parameters over EDL and HEDL during training is shown in Figure 3, alongside the final accuracy for each category. The sum of these gradient norms has been normalized for comparative analysis. It is observed that the gradient norms for several parameters within the fully-connected layer of the EDL model remain zero throughout the training process, which correlates with a significantly lower final accuracy for these categories. This outcome is indicative of the vanishing gradient problem. Conversely, HEDL does not experience this issue, demonstrating that our proposed method effectively circumvents the challenge of vanishing gradients within the fully-connected layer.

### Ablation Study

EDL suffers from a notable decline in both ID accuracy and OOD detection when facing a proportional rise in the volume of vague evidence. In contrast, HEDL demonstrates the capability to consistently extract comprehensive evidence and maintain its performance regardless of the dataset scale.

We investigate the performance of our method with ablation experiments on two challenging fine-grained datasets. The fine-grained datasets contain more vagueness among categories and can better prove the effectiveness of our methods. We conduct ablation experiments on the effects of hyper-opinion and opinion projection, respectively. Note that opinion projection can only be built upon hyper-opinion.

Figure 4 illustrates the uncertainty distribution of ID and OOD samples across different datasets for EDL, HEDL without opinion projection, and HEDL itself. Notably, on the latter three more complex datasets, the approaches based on hyper-opinion exhibits a distinct performance advantage. It is also

    & &  &  \\   & &  & ID data &  & ID data \\  Multinomial-opinion & Hyper-opinion & Opinion-projection & FPR95\(\) & ALP8\(\) & AUROC\(\) & Acc.\(\) & FPR95\(\) & ALP8\(\) & AUROC\(\) & Acc.\(\) \\  - & - & - & 14.86 & 95.94 & 97.42 & 83.75 & 30.29 & 91.18 & 94.35 & **75.82** \\ - & - & - & 100.00 & 66.95 & 67.23 & 66.84 & 98.03 & 71.80 & 75.27 & 59.87 \\ - & - & - & 11.90 & 95.83 & 97.61 & 81.40 & 9.32 & 91.57 & 97.82 & 52.30 \\  - & - & **3.98** & **88.73** & **99.07** & **84.13** & **3.82** & **97.80** & **98.91** & 74.62 \\   

Table 2: Ablation experiment results on Flower-102 and CUB-200-2011. Results show that EDL fails to extract evidence fully. HEDL without projection can extract comprehensive evidence to distinguish ID and OOD samples but fails to classify ID categories. HEDL can further assign evidence correctly and obtain accurate classification.

Figure 3: The sum of gradient norms within the fully-connected layer for each category in CIFAR-100 throughout the training process.

worth observing that, in these datasets, instances of ID data with maximum uncertainty are present in the EDL model. This phenomenon can be attributed to the failure of extracting evidence of those categories due to the vanishing gradient problem.

Table 2 shows that evidence built on hyper-opinion can be considered comprehensively, leading to accurate uncertainty estimation and above baseline OOD detection performance. But without the correct projection from hyper-opinion to multinomial-opinion, vague evidence can not be assigned precisely, leading to inaccurate classification.

## 5 Conclusion

In this paper, we propose Hyper-opinion Evidential Deep Learning (HEDL), a novel approach designed to generate precise uncertainty estimation for Out-of-Distribution (OOD) detection. Our method encapsulates a comprehensive representation of evidence within hyper-opinion, which allows model to preserve its vagueness among In-Distribution categories to reject OOD data.

Additionally, by projecting hyper-opinion to multinomial-opinion, HEDL circumvents the vanishing gradient problem encountered in the fully-connected layers of traditional EDL. This projection is optimized within an established framework, yielding accurate and reliable evidence. Notably, our method

Figure 4: The normalized density distribution of normalized uncertainty for ID and OOD samples across differing datasets.

accomplishes superior OOD detection performance while simultaneously upholding classification accuracy without incurring additional computational complexity. Extensive experimental results across numerous datasets substantiate the efficacy of the proposed Hyper-opinion Evidential Deep Learning.

**Limitations and societal impact.** Our proposed HEDL method achieves best performance by transfering learning on pre-trained models. In future work, it is necessary to reduce the dependence on pre-trained models and explore alternative approaches. This work aims to improve the safety of deep learning models, which tends to benefit a wide range of applications of AI in social life.