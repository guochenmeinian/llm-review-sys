# Continual Counting with Gradual Privacy Expiration

Joel Daniel Andersson

Basic Algorithms Research Copenhagen

University of Copenhagen

jda@di.ku.dk

&Monika Henzinger

Institute of Science and Technology Austria

Klosterneuburg, Austria

monika.henzinger@ist.ac.at

Rasmus Pagh

Basic Algorithms Research Copenhagen

University of Copenhagen

pagh@di.ku.dk

&Teresa Anna Steiner

University of Southern Denmark

steiner@imada.sdu.dk

&Jalaj Upadhyay

Rutgers University

jalaj.upadhyay@rutgers.edu

###### Abstract

Differential privacy with gradual expiration models the setting where data items arrive in a stream and at a given time \(t\) the privacy loss guaranteed for a data item seen at time \((t-d)\) is \( g(d)\), where \(g\) is a monotonically non-decreasing function. We study the fundamental _continual (binary) counting_ problem where each data item consists of a bit, and the algorithm needs to output at each time step the sum of all the bits streamed so far. For a stream of length \(T\) and privacy _without_ expiration continual counting is possible with maximum (over all time steps) additive error \(O(^{2}(T)/)\) and the best known lower bound is \(((T)/)\); closing this gap is a challenging open problem.

We show that the situation is very different for privacy with gradual expiration by giving upper and lower bounds for a large set of expiration functions \(g\). Specifically, our algorithm achieves an additive error of \(O((T)/)\) for a large set of privacy expiration functions. We also give a lower bound that shows that if \(C\) is the additive error of any \(\)-DP algorithm for this problem, then the product of \(C\) and the privacy expiration function after \(2C\) steps must be \(((T)/)\). Our algorithm matches this lower bound as its additive error is \(O((T)/)\), even when \(g(2C)=O(1)\).

Our empirical evaluation shows that we achieve a slowly growing privacy loss with significantly smaller empirical privacy loss for large values of \(d\) than a natural baseline algorithm.

## 1 Introduction

_Differential privacy under continual observation_ has seen a renewed interest recently  due to its application in private learning  and statistics . In this model, the curator gets the database in the form of a stream and is required to output a given statistic continually. Chan et al.  and Dwork et al.  introduced the _binary (tree) mechanism_ which allows us to estimate the running count of a binary stream of length \(T\) with additive error \(O(^{2}(T)/)\) under \(\)-differential privacy.

The traditional definition of continual observation considers every single entry in the stream equally important for analysis and has equal confidentiality. However, in many applications of continual observation, the data becomes less sensitive with time. For example, consider the case where thestream tracks visits to a certain location or website: it being visited a minute ago may constitute more sensitive information than if it was visited a week ago. To capture such scenarios, Bolot et al.  defined _privacy with expiration_, where the privacy of a streamed data item decreases as a function of the time elapsed since it was streamed. However, known algorithms for privacy with expiration work only in the setting when we expect no privacy after a certain time has elapsed [4, Section 6].

This lack of algorithms for privacy with expiration influences some real-world design choices . In particular, real-world deployments either allocate every user a "privacy budget" that is diminished every time their data is used, such that their data should not be used once the privacy budget reaches zero, or they account for privacy loss over the time. However, since the data can still be useful, another common approach in these deployments use the heuristic of "refreshing the privacy budget", i.e., the privacy budget is reset to a positive default value after a prescribed time period, irrespective of how much privacy budget has been used so far. This, for example, was pointed out by Tang et al.  in Apple's first large-scale deployment. However, refreshing the privacy budget is very problematic as the privacy loss is, in the worst case, multiplied by the number of refreshes, for example, if the old data is reused (i.e., the privacy expiration is _linear_).

In this paper, we study continual counting with gradual privacy expiration, generalizing the result in Bolot et al. . Our main contributions are algorithms with the following assets:

* _Improve accuracy._ We achieve an additive error of \(O((T)/)\) for a large class of privacy expiration functions and show that this is optimal in a particular sense. This is in contrast to continual counting without expiration, where there is a gap of a \( T\) factor . Our work generalizes the \(((T)/)\) lower bound for continual counting to a wide class of privacy expiration functions and shows that for any additive error \(C\), the product of \(C\) and the privacy expiration function after \(2C\) steps must be \(( T)\). We match this lower bound as our additive error is \(O((T)/)\), even when the expiration function after \(2C\) steps is a constant.
* _Scale well._ Our algorithms work for unbounded streams, run in amortized \(O(1)\) time per update, \((T)\) space, and offer different trade-offs than conventional continual counting algorithms. In allowing for a growing privacy loss, we show that polylogarithmic privacy expiration is sufficient for optimal additive error, and parameterize the algorithm by the speed of the privacy expiration; as expected, faster privacy expiration yields a smaller error.

We supplement these theoretical guarantees with empirical evaluations.

**Related Works.** Before presenting our contributions in detail, we give a brief overview of the most relevant related work. Since Chan et al.  and Dwork et al. , several algorithms have been proposed for privately estimating _prefix-sum under continual observation_, i.e., given a stream of inputs \(x_{1},x_{2},\) from some domain \(\), output \(y_{t}=_{i t}x_{i}\) for all \(t 1\). _Continual binary counting_ is a special case of prefix sum when \(=\{0,1\}\) and \(x_{t}\) is provided at time \(t\).

When the input is given as a stream, earlier works improved on the basic binary mechanism under (i) distributional assumptions on data , (ii) structural assumptions on data , and (iii) that the importance of data (both with respect to utility and sensitivity) decreases with elapsed time , or (iv) by enforcing certain conditions on the behavior of the output . In recent work, Fichtenberger et al.  gave algorithms to improve the worst-case non-asymptotic guarantees under continual observation using the _matrix mechanism_ and Denisov et al.  used similar approach to provide empirical results that minimize the mean-squared error. Subsequently, Henzinger et al.  showed that the algorithm in Fichtenberger et al.  achieves almost optimal mean-squared error.

These earlier works are in the traditional definition of privacy under continual observation, i.e., they consider data privacy to be constant throughout the stream. The only exception is the work of Bolot et al. , which defined differentially private continual release with privacy expiration parameterized by a monotonically non-decreasing function \(g\) and gave an algorithm for the special case that the data loses all its confidentially after a prescribed time. Our work is in this privacy model. There is another line of work motivated by applications in private learning that studies privacy-preserving prefix sum without restricting access to the data points (such as allowing multiple passes)  and providing privacy-preserving estimates under various privacy models like shuffling . Since we focus on continual observation, we do not compare our results with this line of work.

The work whose techniques are the most related to ours is the algorithm in Dwork et al.  for continual counting satisfying pan-privacy . Roughly speaking, an algorithm is _pan-private_ if it is resilient against intruders who can observe snapshots of the internal states of the algorithms.

### Our Contributions

We start by first formally stating the problem. As mentioned above, the focus of this work is privacy with expiration given as Definition 3 in Bolot et al.:

**Definition 1.1**.: _Let \(g:_{ 0}\) be a non-decreasing function1. Let \(\) be a randomized online algorithm that takes as an input a stream \(x_{1},x_{2},\) and at every time step \(t\) outputs \((x_{1},,x_{t})\). \(\) satisfies \(\)-differential privacy with expiration (function) \(g\) if for all \( 1\), for all measurable \(S()^{*}\)2, all possible inputs \(x_{1},,x_{}\), all \(j\) and all \(x_{j}^{}\) with \(|x_{j}-x_{j}^{}|\)_

\[[((x_{1},,x_{j},,x_{t}))_{t=1}^{} S]  e^{g(-j)}[((x_{1},,x_{j}^{},,x_{t}))_{t=1}^{} S],\]

_where the probability is over the coin throws of \(\). We refer to \(g(-j)\) as the privacy loss._

Letting \(T\) be the length of the input stream, the best known bound on the \(_{}\)-error for continual counting under \(\)-differential privacy is \(O(^{2}(T)/)\), achieved by the algorithms in . Alternatively, the analysis of  can be used to show that running this algorithm with \(^{}=(T)\) achieves \(\)-differential privacy with expiration function \(g(d)=(T)\) for all \(d=1,,T\), and error \(O((T)/)\). Our main contribution is to show that better trade-offs are possible: In particular, we can achieve the same error with a _strictly smaller function_\(g\), i.e. we can get an \(O((T)/)\) bound on the \(_{}\)-error with an expiration function of \(g(d) d\). More generally, our algorithm provides a trade-off between privacy loss and both \(_{}\)-error and expected \(_{2}^{2}\)-error for all expiration functions \(f(d)\) that satisfy (roughly) \(f(d) 1+^{}(d)\) for any \(>0\). The exact expiration function \(g\) is stated below in Theorem 1.2. It also includes a parameter \(B\) that allows the privacy loss to be "shifted" by \(B\) time steps, i.e., there is no privacy loss in the first \(B\) time steps. If the length \(T\) of the stream is unknown, then \(B\) is a constant. If \(T\) is given to the algorithm, then \(B\) can be a function of \(T\).

By Definition 1.1, any algorithm satisfying differential privacy with expiration \(g\) also fulfills differential privacy with any expiration function that is pointwise at least as large as \(g\). Specifically, for two functions \(f\) and \(g\) defined on the same domain \(\), we say \(f g\) if \(f(x) g(x)\) for all \(x\). We are now ready to state our main theorem:

**Theorem 1.2**.: _Let \(_{>0}\{\}\) be a constant, and let parameters \(_{>0}\) and \(B\) be given. There exists an algorithm \(\) that approximates prefix sums of a (potentially unbounded) input sequence \(x_{1},x_{2},\) with \(x_{i}\) satisfying \(\)-differential privacy with any expiration function \(f\) such that \(f g\), where_

\[g(d)=0&d<B\\ O(1+^{}(d-B+1))&d B\]

_Considering all releases up to and including input \(t\), the algorithm \(\) uses \(O(B+ t)\) space and \(O(1)\) amortized time per input/output pair and has the following error guarantees at each individual time step \(t\) for \(>0\),_

* \(_{}[((x)-_{i=1}^{t}x_{i})^{2}]= O(B^{2}+^{3-2}(t)/^{2})\)_,_
* \(|(x)-_{i=1}^{t}x_{i}|=O(B+^{q}(t)/ )\) _with probability_ \(1-\) _where_ \(q=(1/2,3/2-)\)_._

The case when \(\{0,3/2\}\) is covered in Appendix C.3. Note that choosing \(>3/2\) implies a constant expected squared error at each time step if \(B=O(1)\). Parameter \(\) controls the trade-off between the _asymptotic_ growth of the expiration function and the error, while \(\) controls the trade-off between _initial_ privacy (after \(B\) time steps which is \( g(B)\) ) and the error, which is inversely proportional to \(\). Also, for releasing \(T\) outputs we have the following corollary.

**Corollary 1.3**.: _The algorithm \(\) with \(B=O((T)/)\) and \( 1\) incurs a maximum (over all time steps) additive \(_{}\)-error of \(O((T)/)\) when releasing \(T\) outputs with probability \(1-1/T^{c}\), for constant \(c>0\), and achieves privacy with expiration function \(g\) as in Theorem 1.2._

In Section 6, we provide empirical evidence to show that we achieve a significantly smaller empirical privacy loss than a natural baseline algorithm. Finally, we complement our upper bound with the following lower bound shown in Appendix C.4.

**Theorem 1.4**.: _Let \(\) be an algorithm for binary counting for streams of length \(T\) which satisfies \(\)-differential privacy with expiration \(h\). Let \(C\) be an integer such that \(\) incurs a maximum additive error of at most \(C<T/2\) over \(T\) time steps with a probability of at least \(2/3\). Then_

\[2C h(2C-1).\]

Note that Theorem 1.4 gives a lower bound for \(h(j)\) for a specific \(j\), namely \(j=2C-1\), and as \(h\) is non-decreasing by Definition 1.1, the lower bound also holds for all \(h(j^{})\) with \(j^{} j\).

Note that Theorem 1.4 shows that our algorithm in Corollary 1.3 achieves a tight error bound for the expiration functions \( 1\) and \(B=O((T)/)\). Assume \(^{}\) is an algorithm that approximates prefix sums in the continual setting and which satisfies differential privacy with expiration function \(h\) and maximum error \(C B/2+1\) at all time steps with probability at least 2/3 for an even \(B\). When run on a binary input sequence, \(^{}\) solves the binary counting problem. Thus, by Theorem 1.4, we have that \(2C h(B+1) 2C h(2C-1) =()\).

Now consider the algorithm \(\) given in Corollary 1.3 and note that, by definition of the expiration function \(g\), \(g(B+1)=O(1)\) and that Corollary 1.3 shows that \(C=O((T)/)\). This is tight as Theorem 1.4 shows that for such an expiration function \(C=((T)/)\).

### Technical Overview

Central to our work is the event-level pan-private algorithm for continual counting by Dwork et al. . Similarly to the binary tree algorithm of Dwork et al. , a noise variable \(z_{I}\) is assigned to every _dyadic interval_\(I\) (see Section 3.2 for a formal definition) contained in \([0,T-1]\). Let this set of dyadic intervals be called \(\). In the version of the binary tree algorithm of Chan et al. , the noise added to the sum of the values so far (i.e. the non-private output) at any time step \(1 t T\) is equal to \(_{I D_{[0,t-1]}}z_{I}\), where \(D_{[0,t-1]}\) is the dyadic interval decomposition of the interval \([0,t-1]\). The pan-private algorithm adds different noise to the output: it adds at time \(t\) the _noises for all intervals containing_\(t-1\), i.e., \(_{I\{I:\,t-1 I\}}z_{I}\). This pan-private way of adding noise helps us bound the privacy loss under expiration. For two neighboring streams differing at time step \(j\), we can get the same output at \( j\) by shifting the values of the noises of a set of disjoint intervals covering \([j,]\) each by at most 1. Using that \(|D_{[j,]}|=O((-j+1))\), we show that the algorithm satisfies a logarithmic privacy expiration.

In our algorithm, we make four changes to the above construction (i.e., the pan-private construction in Dwork et al. ): **(i)** We do not initialize the counter with noise separately from that introduced by the intervals. **(ii)** We split the privacy budget unevenly across the levels of the dyadic interval set instead of uniformly allocating it. This allows us to control the asymptotic growth of the expiration function, and the error. This change, however, requires a more careful privacy analysis. **(iii)** At time \(t\) we add noise identified by intervals containing \(t\), not \(t-1\). While this is a subtle difference, it allows us to exclude intervals starting at \(0\) from \(\), leading to our algorithm running on _unbounded_ streams with utility that depends on the current time step \(t\). Said differently, our algorithm does not need to know the stream length in advance. This is in contrast to Dwork et al.  where the construction requires an upper bound \(T\) on the length of the stream so that the utility guarantee at each step is fixed and a function of \(T\). **(iv)** We allow for a delay of \(B\), meaning we output \(0\) for the initial \(B\) steps. This gives perfect privacy for the first \(B\) steps, and, since each element of the stream is in \(\), the delayed start leads only to an additive error of \(O(B)\).

## 2 Preliminaries

Let \(_{>0}\) denote the set \(\{1,2,\}\) and \(_{ 0}\) the set of non-negative real numbers. We use the symbol \(g\) to denote the function that defines the privacy expiration, i.e., \(g:_{ 0}\). We fix the symbol\((x)\) to denote the randomized streaming algorithm that, given an input \(x=x_{1},x_{2},\) as a stream, provides \(\)-differential privacy with expiration \(g\). All algorithms _lazily draw noise_, meaning that a "noise" random variable is only drawn when first used and is re-used and _not_ re-drawn when referenced again. For a random variable, \(Z\), we use \((Z)\) to denote its support. For a sequence of random variables \(Z=Z_{1},Z_{2},\), we use \((Z)\) to denote \((Z_{1})(Z_{2})\).

**Helpful lemmas.** We now collect some helpful lemmas shown formally in Appendix C. To show privacy with expiration in the following sections, we repeatedly use the following observation: a similar lemma has been used to show the standard definition of differential privacy, e.g., in the proof of Theorem 2.1 of Dwork et al. . Informally, it says that if there exists a map \(q\) between random choices made by algorithm \(\) such that for any input \(x\) and fixed sequence \(z\) of random choices, the map returns a sequence \(q(z)\) such that (1) the output \((x,z)\) equals the output \((x^{},q(z))\) and (2) the probability of picking \(z\) is similar to the probability of picking \(q(z)\), then \(\) is private. The notion of "similar probability" is adapted to the definition of differential privacy with expiration and depends on the function \(g\). All the results in this section are shown formally in Appendix C.1:

**Fact 2.1**.: _Consider an algorithm \(\) that uses a sequence of random variables \(Z=Z_{1},Z_{2},\) as the only source of randomness. We can model \(:(Z)\) as a (deterministic) function of its actual input from the universe \(\) and the sequence of its random variables \(Z\). Suppose that for all \(_{>0}\), \(j\) and all neighboring pairs of input streams \(x=x_{1},,x_{j},,x_{}\) and \(x^{}=x_{1},,x^{}_{j},,x_{}\), there exists a function \(q:(Z)(Z)\) such that \((x;z)=(x^{};q(z))\) and_

\[_{z Z}[z] e^{ g(-j)}_{z^{}  Z}[z^{} q()] (Z).\]

_Then \(\) satisfies \(\)-differential privacy with expiration \(g\)._

**Lemma 2.2**.: _Let \(Z=Z_{1},Z_{2},,Z_{k}\) be a sequence of independent Laplace random variables, such that \(Z_{i}(b_{i})\) for \(b_{i}>0\), for all \(i[k]\). Let \(q\) be a bijection \(q:(Z)(Z)\) of the following form: For all \(:=(_{1}_{2}_{k})^{k}\), and for all \(z(Z)\), we have \(q(z)=z+(Z)\). Then for all \((Z)\) we have_

\[_{z Z}[z] e^{s}_{z Z}[z q()],  s=_{i=1}^{k}|}{b_{i}}.\]

## 3 Warmup

As a warm-up, we give two simple algorithms for two obvious choices of the expiration function: the linear expiration function \(g(d)=d\) and the logarithmic expiration function \(g(d)=2(d+1)+2\).

### A Simple Algorithm with Linear Privacy Expiration

First, we consider a simple algorithm which gives \(\)-differential privacy with expiration \(g:_{ 0}\), where \(g(d)=d\). The maximum error of this algorithm over \(T\) time steps is bounded by \(O(^{-1}(T/))\), with probability at least \(1-\). The algorithm \(_{}\) is given in Algorithm 1. It adds fresh Laplace noise to any output sum. Note that this is the same algorithm as the Simple Counting Mechanism I from Chan et al. . However, we show that for the weaker notion of differential privacy with linear expiration, Laplace noise with _constant_ scale suffices, even though the sensitivity of \(_{}\) running on a stream of length \(T\) is \(T\). To prove this, we show that for two neighbouring streams differing at time step \(j\), we obtain the same output by "shifting" the values of the Laplace noises for all outputs after step \(j\) by at most 1. We defer the proof of the following lemma to Appendix C.2.

**Lemma 3.1**.: _The algorithm \(_{}\), given in Algorithm 1, is \(\)-differentially private with expiration \(g\), where \(g:\) is the identity function \(g(d)=d\) for all \(d\). It incurs a maximum additive error of \(O(^{-1}(T/))\) over all \(T\) time steps simultaneously with probability at least \(1-\)._

### A Binary-Tree-Based Algorithm with Logarithmic Privacy Expiration

Next, we show how an algorithm similar to the binary tree algorithm  gives \(\)-differential privacy with expiration \(g:_{ 0}\), where \(g(d)=2(d+1)+2\). This result can also be derived from Theorem 1.2 by setting \(=1\) and \(B=0\). As in the case when \(g(d)=d\), the maximum error of this algorithm over \(T\) time steps is again bounded by \(O(^{-1}(T/))\), with probability at least \(1-\). Similarly to the binary tree algorithm, we define a noise variable for every node in the tree, but we do this in the terminology of _dyadic intervals_. We consider the _dyadic interval set_\(\) on \([1,)\) (formally defined shortly), associate a noise variable \(z_{I}\) with each interval \(I\), and at time step \(t\) add noise \(z_{I}\) for each \(I\) that contains \(t\). This is similar to the construction in Dwork et al. , with the exception that they instead consider the dyadic interval set on \([0,T-1]\), add noise \(z_{I}\) at time \(t\) if \(t-1 I\), and initialize their counter with noise from the same distribution. Our choice of \(\) allows the algorithm to run on unbounded streams, and leads to adding up \(1+(t)\) noise terms at step \(t\) rather than \(1+(T)\). For privacy, we will argue that if two streams differ at time \(j\), then we get the same outputs up to time \( j\) by considering a subset of disjoint intervals in \(\) covering \([j,]\), and shifting the associated Laplace random variables appropriately. In the following, we describe this idea in detail. We start by describing the dyadic interval decomposition of an interval.

```
1:Input: A stream \(x_{1},x_{2},\), privacy parameter \(\)
2: Lazily3draw \(Z_{t-1}()\)
3: At time \(t=1\), output \(0\)
4:for\(t=2\)to\(\)do
5: At time \(t\), output \(_{i=1}^{t-1}x_{i}+Z_{t-1}\)
6:endfor ```

**Algorithm 1**\(_{}\): Continual counting under linear gradual privacy expiration

**Dyadic interval decomposition.** For every non-negative integer \(\), we divide \([1,)\) into disjoint intervals of length \(2^{}\): \(^{}=\{[k 2^{},(k+1) 2^{}-1],k_{>0}\}\). We call \(=_{=0}^{}^{}\) the _dyadic interval set_ on \([1,)\), and \(^{}\) the \(\)-th _level_ of the dyadic interval set. We show the following two facts in Appendix C.

**Fact 3.2**.: _Let \(\) be the dyadic interval set on \([1,)\). For any interval \([a,b]\), \(1 a b\), there exists a set of intervals \(D_{[a,b]}\), referred to as the dyadic interval decomposition of \([a,b]\), such that (i) the sets in \(D_{[a,b]}\) are disjoint; (ii) \(_{I D_{[a,b]}}I=[a,b]\); and (iii) \(D_{[a,b]}\) contains at most 2 intervals per level, and the highest level \(\) of an interval satisfies \((b-a+1)\)_

Figure 1: An example of the noise structure for Algorithm 2 and Algorithm 3 for \(B=0\) on two neighbouring streams \(x\) and \(y\) differing in position 3. The nodes correspond to the dyadic intervals. The filled nodes mark the intervals \(I\) for which the noise \(Z_{I}\) is shifted by one between \(x\) and \(y\) to get the same outputs for \(=6\). The fat nodes mark the intervals \(I\) corresponding to the \(Z_{I}\) which are used in the computation of the fourth prefix sum \(s_{4}\).

**Fact 3.3**.: _Let \(\) be the dyadic interval set on \([1,)\), and for \(t_{>0}\) define \(_{t}=\{I:t I\}\) as the intersection of \(t\) with \(\). Then \(|_{t}|= t+1\)._

**Lemma 3.4**.: _The algorithm \(_{}\) given in Algorithm 2 satisfies \(\)-differential privacy with expiration \(g\), where \(g:\) is defined as \(g(x)=2(x+1)+2\). It incurs a maximum additive error of \(O(^{-1}(T/))\) over all \(T\) time steps simultaneously with probability at least \(1-\)._

**Privacy.** We use Fact 2.1 and Lemma 2.2 to argue privacy of \(_{}\): Let \(x\) and \(x^{}\) differ at time \(j\). Note that the prefix sums fulfill the following properties: (i) \(_{i=1}^{t}x_{i}=_{i=1}^{t}x_{i}^{}\) for all \(t<j\) and (ii) \(_{i=1}^{t}x_{i}^{}=_{i=1}^{t}x_{i}+y\) for all \(t j\), where \(y=x_{j}^{}-x_{j}^{}[-1,1]\).

In the following, we refer to the output of the algorithm run on input \(x\) and with values of the random variables \(z\) as \(_{}(x;z)\). Let \( j\) be given, and consider \(S(_{})^{*}\). Let \(Z=(Z_{I})_{I}\) be the sequence of Laplace random variables used by the algorithm. For any fixed output sequence \(s S\), let \(z=(z_{I})_{I}\) be a sequence of values that the Laplace random variables need to assume to get output sequence \(s\) for input \(x\). That is \(_{i=1}^{t}x_{i}+_{I_{t}}z_{I}=s_{t}\) for \(t 1\). Let \(D_{[j,]}\) be the decomposition of \([j,]\) as defined in Fact 3.2. We define a bijection \(q\) satisfying the properties of Fact 2.1 as follows: \(q(z)=z^{}=(z_{I}^{})_{I}\) such that

\[z_{I}^{}=z_{I}^{}=z_{I}& I D_{[j,]} \\ z_{I}^{}=z_{I}+y& I D_{[j,]}.\]

We show the two properties needed to apply Fact 2.1:

(1) Note that \((_{}(x;z))_{t}=_{i=1}^{t}x_{i}+_{I_{t}}z_{I}\). For \(t<j\), we have \(t[j,]\) and therefore \(t I\) for any \(I D_{[j,]}\). Therefore, we have

\[_{i=1}^{t}x_{i}+_{I_{t}}z_{I}=_{i=1}^{t}x_{i}^{ }+_{I_{t}}z_{I}=_{i=1}^{t}x_{i}^{}+_{I _{t}}z_{I}^{}.\]

For \(j t\), we have that \(t\) is contained in exactly one \(I D_{[j,]}\). Thus, \(z_{I}^{}=z_{I}+y\) for exactly one \(I_{t}\), and \(z_{I^{}}^{}=z_{I^{}}\) for all \(I^{}_{t}\{I\}\). Further, since \(t j\), we have that \(_{i=1}^{t}x_{i}=_{i=1}^{t}x_{i}^{}-y\). Together, this shows the first property of Fact 2.1 as

\[_{i=1}^{t}x_{i}+_{I_{t}}z_{I}=_{i=1}^{t}x_{i}^{ }-y+_{I_{t}}z_{I}^{}+y=_{i=1}^{t}x_{i}^{ }+_{I_{t}}z_{I}^{},\]

(2) By Fact 3.2, \(|D_{[j,]}| 2((-j+1)+1)\). Thus, by Lemma 2.2 for any \((Z)\),

\[_{z Z}[z] e^{_{I D_{[j,]}}|y|} _{z Z}[z q()] e^{2((-j+1)+1)}_{z  Z}[z q()],\]

so the second property of Fact 2.1 is fulfilled with \(g(x)=2(x+1)+2\). By Fact 2.1, we have differential privacy with privacy expiration \(g(x)=2(x+1)+2\).

**Accuracy.** To show accuracy at step \(t\), let \(Y_{t}=_{I_{t}}Z_{I}\), i.e. the noise added at time step \(t\). By Fact 3.3 we add \(k=|_{t}|= t+1\) Laplace noises with scale \(\). Let \(M_{t,}=\{,\}\). By Corollary B.4, we have that \([|Y_{t}|>M_{t,}] ,\) for any \(<1\). Setting \(=^{}/T\), it follows that with probability at least \(1-^{}\), \(|Y_{t}|=O(^{-1}(T/^{}))\) for all time steps \(t T\) simultaneously.

## 4 Proof of Theorem 1.2 and Corollary 1.3

Section 3.2 shows that we can obtain an error smaller than the binary mechanism by using differential privacy with a logarithmic expiration function. Here we show a general trade-off between the expiration function's growth and the error's growth. Two techniques are needed to showBudgeting across levels.The privacy budget is split unevenly across levels of the dyadic interval set in order to control the asymptotic growth of the expiration function. Specifically, the budget at level \(\) is chosen to be proportional to \((+1)^{-1}\). The case \(=1\) corresponds to the even distribution used in the construction of Section 3.2.

Our algorithm is shown as Algorithm 3. In the following, we refer to the output of the algorithm run on input \(x\) and with values of the random variables \(z\) as \((x;z)\).

```
1:Input: A stream \(x_{1},x_{2}\), privacy parameter \(\), parameters \(B\), \(_{>0}\{3/2\}\)
2: Let \(\) be the dyadic interval set on \([1,)\) and \(_{t}=\{I:t-B I\}\)
3:\(\), \(I^{}\), lazily draw i.i.d. \(Z_{I}(}{e})\)
4:for\(t=1\)to\(B\)do
5: At time \(t\), output \(0\)
6:endfor
7:for\(t=B+1\)to\(\)do
8: At time \(t\), output \(s_{t-B}=_{i=1}^{t-B}x_{i}+_{I_{t}}Z_{I}\)
9:endfor ```

**Algorithm 3** Continual counting, gradual privacy expiration

**Privacy.** We now show that \(\) satisfies Definition 1.1. For \(x\) and \(x^{}\) that differ (only) at time \(j\), the prefix sums fulfill the following properties:

* \(_{i=1}^{t}x_{i}=_{i=1}^{t}x^{}_{i}\) for all \(t<j\) and
* \(_{i=1}^{t}x^{}_{i}=_{i=1}^{t}x_{i}+y\) for all \(j t\), where \(y=x^{}_{j}-x_{j}[-1,1]\).

Let \(j\) and \(\) be defined as in Definition 1.1. Due to the delay, if \(^{}=-B<j\) (corresponding to \(d<B\)) the privacy claim is immediate since the output distributions of \(\) up to step \(\) are identical on the two inputs. Otherwise, for \(^{}=-B j\), i.e., for \(d B\), we wish to use Fact 2.1 and Lemma 2.2. Let \(Z=(Z_{I})_{I}\) be the sequence of Laplace random variables used by \(\). For input \(x\) consider a fixed length-\(\) output sequence consisting of \(B\) zeros followed by \(s_{1},,s_{^{}}\). Let \(z=(z_{I})_{I}\) be a sequence of values for the Laplace random variables in order to produce this output sequence with input \(x\). That is, \(s_{t}=_{i=1}^{t}x_{i}+_{I_{t}}z_{I}\) for \(t 1\). Let \(D_{[j,^{}]}\) be the decomposition of \([j,^{}]\) as defined in Fact 3.2. We define a bijection \(q\) satisfying the properties of Fact 2.1 as follows: \(q(z)=z^{}=(z^{}_{I})_{I}\) such that

\[z^{}_{I}=z^{}_{I}=z_{I}& I D_{[j,^{ }]}\\ z^{}_{I}=z_{I}+y& I D_{[j,^{}]}.\]

We show the two properties needed to apply Fact 2.1:

**Lemma 4.1**.: _The function \(q(z)\) satisfies \((x;z)=(x^{};q(z))\) and for \(g(d)=O(1+^{}(d-B+1))\) we have \(_{z Z}[z] e^{cg(-j)}[z^{}]\) for all \((Z)\)._

**Space and time.** Algorithm 3 can update the sums \(_{i=1}^{t-B}x_{i}\) and \(_{I_{t}}Z_{I}\) in each time step \(t\) using the following idea: The \(B\) most recent inputs are kept in a buffer to allow calculation of prefix sums with delay and also the \((t)+1\) random variables of those values that were added to the most recent output. At a given step, each random value that is no longer used is subtracted from the most recent output, and each new random value is added. An amortization argument as in the analysis of the number of bit flips in a binary counter yields the \(O(1)\) amortized bound.

**Accuracy.** To show the accuracy guarantee, we need to account for the error due to delay as well as the noise required for privacy. It is easy to see that the delay causes an error of at most \(B\), since the sum of any \(B\) inputs is bounded by \(B\). Thus, for both error bounds it remains to account for the error due to noise. At every time step \(t\) after \(B\), the output is the delayed prefix sum plus a sum of Laplace distributed noise terms as indicated by \(_{t-B}\) with parameters \(b_{}=(+1)^{1-}/\), where \(=0,,(t-B)\). To bound the variance of the noise, \(2_{}b_{}^{2}\) we compute:

\[_{=0}^{(t-B)}b_{}^{2} =}_{=0}^{(t-B)}( +1)^{2(1-)}}(1+_{1}^{ (t+2}x^{2(1-)}x)\] \[}(1+[x^ {3-2}]_{x=1}^{x=(t)+2})=O(}{^{2}}).\]

This calculation assumes \( 3/2\), proving the statement on the squared error in Theorem 1.2. For the high probability bound we invoke Lemma B.3 with \(b_{M}=_{}(b_{})=(1,(2t)^{1-})/)=O((1+ (t)^{1-})/)\). For \(=b_{}^{2}}+b_{M}=O(b_{M})\), applying Lemma B.3 says that the error from the noise is \(O()=O((t)^{(0.5,1.5- )})\) with probability \(1-\), proving Theorem 1.2.

Proof of Corollary 1.3.For releasing \(T\) outputs, choosing \(=1/T^{c+1}\), \(c>0\) being a constant, and using a union bound over all outputs gives a bound on the maximum noise equal to \(O((T)^{(1,2-)}/)\) with probability \(1-1/T^{c}\), proving Corollary 1.3.

## 5 Lower Bound on the Privacy Decay

The lower bound follows from a careful packing argument. The proof is deferred to Appendix C.4.

**Theorem 5.1**.: _Let \(\) be an algorithm for binary counting for streams of length \(T\) which satisfies \(\)-differential privacy with expiration \(g\). Let \(C\) be an integer and assume that the additive error of \(\) is bounded by \(C<T/2\) at all time steps with a probability of at least 2/3. Then_

\[_{j=0}^{2C-1}g(j)(T/6C)/\]

The lower bound extends to mechanisms running on unbounded streams. By Definition 1.1, \(g(j)\) is non-decreasing in \(j\). This immediately gives Theorem 1.4.

## 6 Empirical Evaluation

We empirically evaluated (i) how the privacy loss increases as the elapsed time increases for Algorithm 3, (ii) how tightly the corresponding theoretical expiration function \(g\) of Theorem 1.2 bounds this privacy loss, and (iii) how this privacy loss compares to the privacy loss of a realistic baseline. As different algorithms have different parameters that can affect privacy loss, we use the following approach to perform a fair comparison: In the design of \(\)-differentially private algorithms the error of different algorithms is frequently measured with the same value of the privacy loss parameter \(\). Here, we turn this approach around: _We compare the privacy loss (as a function of elapsed time) of different algorithms whose privacy parameter \(\) is chosen to achieve the same error._

We empirically compute the privacy loss for Algorithm 3 by considering the exact dyadic decompositions used for the privacy argument (see Appendix A for details). As a baseline to compare against, we break the input stream into intervals of length \(W\), run the'standard' binary mechanism \(_{B}\) of Chan et al.  with a privacy parameter \(_{cur}\) on the current interval, and compute the sum of all prior intervals with a different privacy guarantee \(_{past}\). As for both algorithms that we evaluate it is straightforward to compute the _mean-squared error_ (MSE) for all outputs on a stream of length \(T\), while the corresponding maximum absolute error can only be observed empirically, we fix the MSE for \(T=d_{max}+1\), where \(d_{max}\) is the greatest \(d\) (on the \(x\)-axis) shown in each plot. We normalize each plot to achieve the same MSE over the first \(T\) outputs, across all algorithms and parameter choices. For all runs of Algorithm 3 we used \(B=0\), as for larger values of \(B\), the primary effect would be to shift the privacy loss curve to the right. We picked the MSE to be \(1000\) for all plots as it leads to small values of the empirical privacy loss. As for both algorithms, the privacy loss _does not_ depend on the input data; we used an all-zero input stream, a standard approach in the industry (see, for example, Thakurta's  plenary talk at USENIX, 2017).

Figure 2(a) shows that \(g\) from Theorem 1.2 is a good approximation of the empirical privacy loss, and that both exhibit the same polylogarithmic growth. Figure 2(b) shows that for large enough \(d\) our algorithm has lower privacy loss than the baseline algorithm. See more details in the appendix.

## 7 Conclusion

In this work, we give the first algorithm for the continual counting problem for privacy with expiration for a wide range of expiration functions and characterize for which expiration functions it is possible to get an \(_{}\)-error of \(O((T)/)\). We also give a general lower bound for any such algorithms and show that ours is tight for certain expiration functions. It would be interesting to study this model further, e.g., with slower-growing expiration functions, and also algorithms for other problems in continual observation, such as maintaining histograms and frequency-based statics over changing data. Specifically, it would be an interesting direction for future research to study problems in this model, where a polynomial error gap between the batch model and the continual release model is known to exist (for example max sum  and counting distinct elements ), and to see if this model allows for new trade-offs. Further, one of the main applications of continual counting algorithms is in privacy-preserving federated learning algorithms, specifically in stochastic gradient descent (see e.g. ). It would be interesting to explore how our algorithm can be deployed in this setting.

Though the concept of privacy expiration has not been defined for approximate differential privacy, it is natural to wonder if there exist analogous results in this setting, which, in general, allows better privacy-utility trade-offs. We note that for \(\)-zero-concentrated differential privacy  there is a natural analog of Definition 1.1 for which it seems possible to prove results analogous to those shown here for pure differential privacy. In that context it would also be interesting to see whether the matrix mechanism can be used to improve the constants in the error, similar to .

## 8 Acknowledgements

Monika Henzinger: This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (Grant agreement No. 101019564) and the Austrian Science Fund (FWF) grant DOI 10.55776/Z422, grant DOI 10.55776/I5982, and grant DOI 10.55776/P33775 with additional funding from the netidee SCIENCE Stiftung, 2020-2024.

Joel Daniel Andersson and Rasmus Pagh are affiliated with Basic Algorithms Research Copenhagen (BARC), supported by the VILLUM Foundation grant 16582, and are also supported by Providentia, a Data Science Distinguished Investigator grant from Novo Nordisk Fonden.

Teresa Anna Steiner is supported by a research grant (VIL51463) from VILLUM FONDEN. This work was done while Teresa Anna Steiner was a Postdoc at the Technical University of Denmark.

Jalaj Upadhyay's research was funded by the Rutgers Decanal Grant no. 302918 and an unrestricted gift from Google.

Figure 2: Plots on the privacy loss for our Algorithm 3 and a baseline algorithm.