# Holistic Transfer: Towards Non-Disruptive

Fine-Tuning with Partial Target Data

 Cheng-Hao Tu\({}^{*1}\), Hong-You Chen\({}^{*1}\), Zheda Mai\({}^{1}\), Jike Zhong\({}^{1}\), Vardaan Pahuja\({}^{1}\),

**Tanya Berger-Wolf\({}^{1}\), Song Gao\({}^{2}\), Charles Stewart\({}^{3}\), Yu Su\({}^{1}\), Wei-Lun Chao\({}^{1}\)**

\({}^{1}\)The Ohio State University, \({}^{2}\)University of Wisconsin-Madison, \({}^{3}\)Rensselaer Polytechnic Institute

Equal contributions

###### Abstract

We propose a learning problem involving adapting a pre-trained source model to the target domain for classifying all classes that appeared in the source data, using target data that covers only a partial label space. This problem is practical, as it is unrealistic for the target end-users to collect data for all classes prior to adaptation. However, it has received limited attention in the literature. To shed light on this issue, we construct benchmark datasets and conduct extensive experiments to uncover the inherent challenges. We found a dilemma -- on the one hand, adapting to the new target domain is important to claim better performance; on the other hand, we observe that preserving the classification accuracy of classes missing in the target adaptation data is highly challenging, let alone improving them. To tackle this, we identify two key directions: 1) disentangling domain gradients from classification gradients, and 2) preserving class relationships. We present several effective solutions that maintain the accuracy of the missing classes and enhance the overall performance, establishing solid baselines for holistic transfer of pre-trained models with partial target data.

## 1 Introduction

We are entering an era in which we can easily access pre-trained machine-learning models capable of classifying many objects. In practice, when end-users deploy these models in diverse domains, adaptation is often necessary to achieve high accuracy. To address this issue, considerable efforts have been devoted to domain adaptation (DA), aiming to transfer domain knowledge from the source to the target. This involves updating the model on an adaptation set collected from the target environment. Although various DA settings have been proposed to tackle different scenarios , we argue that previous setups rely on a strong assumption that hinders their applicability in the real world: the adaptation set contains samples from all possible labels (e.g., classes) in the target environments. Taking classification as an example, to adapt a pre-trained source model's recognition ability over \(C\) classes to a novel domain, DA necessitates access to a target dataset encompassing all \(C\) classes.

In reality, the adaptation set may only have samples of much fewer classes \(C^{}<C\) due to limited sample size, collections bias, etc. Many pertinent examples can be found in realistic AI applications. Considering wildlife monitoring  as an instance, where data collection often occurs passively, through smart camera traps, awaiting animal appearances. Consequently, when a smart camera trap is deployed to a new location and requires adaptation, assembling a comprehensive target dataset containing all the animal species becomes a daunting task, especially if a specific rare species does not appear within the data collection period due to seasonal variations or animal migrations.

To underscore the significance of the problem we address in this paper, it is worth noting that the literature has made significant progress towards training a general recognition model that can predict almost any classes in the vocabulary space such as CLIP  and ALIGN . However, we argue that the current DA approaches applied to these pre-trained models face a practical learning dilemma in adapting the versatile classification capability. On the one hand, adaptation to the target domain style is preferred. On the other hand, the models adapted on _target training data_ whose labels cover a subset of the _target test data_ can lead to risky outcomes for the end users. Through experiments, we observed that fine-tuning the model with empirical risk minimization on such partial target data, which fails to represent the overall target distribution, hampers the performance of classes that were _already pre-trained but unavailable during target adaptation_.

This is frustrating for both parties as a lose-lose dilemma: on the pre-training side, it negates the extensive effort invested in preparing for such a source model as the foundation for adaptation. On the side of downstream users, despite the collection of a target dataset for adaptation purposes, the difficulty of covering all target classes results in an adapted model performing _even worse_ than the source model in the actual target environment. In response to the dilemma, we ask: _can we transfer more holistically, i.e., improving the overall performance without hurting missing class performance?_ We propose a novel and realistic transfer learning paradigm that aims to transfer the source model's discriminative capabilities to the target domain -- specifically, the task of classifying all \(C\) objects in the source domain -- while working with a target training dataset comprising only a limited subset of these classes, i.e., \(C^{}<C\). We refer to the problem as **Holistic Transfer (HT)**.

**Contributions.** We formulate a new learning paradigm called Holistic Transfer (HT).

1. We define a new, practical transfer learning paradigm that complements existing ones (section 2).
2. We construct extensive benchmarks in subsection 2.2 and section 4 to support HT studies.
3. We systematically explore methods to approach HT in section 3 and provide strong baselines.
4. Our extensive experiments shed insights towards solving HT (section 4 and section 5).

## 2 Holistic Transfer Learning

### Problem Definition

We propose a novel and realistic transfer learning paradigm called **Holistic Transfer (HT)**.

**Setup.** We focus on the task of multi-class classification. Let us consider a source model parametrized by \(_{}\). It is pre-trained on a source dataset1\(=\{(_{i}^{},y_{i}^{})\}_{i=1}^{||}\), where \(_{i}^{}\) is the input (e.g., an image) and \(y_{i}^{}\) is the corresponding label (e.g., a class of the \(C\) classes in the label space).

  
**Dataset** & **Scenario** & **Source** & **Target Training** & **Target Test** \\  & & Real (65 classes) & Clipart (30 classes) & Clipart (65 classes) \\ Office-Home  & Domain shift & & Many writers: 0-9, a-z & Writer X: a, 5 & Writer X: b, 5, e, x \\ FEMNIST  & Personalization on & & & \\  & scarce data & & & \\  & & & Manyorations & Location N: before 2010 & Location N: after 2010 \\ iWildCam  & Camera traps & & & \\  & in the wild & & CLIP’s training data & Caltech101 (50 classes) & Caltech101 (101 classes) \\ VTAB  & FT zero-shot & & & \\  & & & & \\ iNatural & & & & \\ (Fungi)  & Confusion classes & & & \\   

Table 1: **Motivating examples of Holistic Transfer (HT). We propose Holistic Transfer, a novel and realistic transfer learning problem. Unlike traditional paradigms, HT focuses on scenarios where only a subset of classes from the target test environment is present in the target training set. Naive fine-tuning (FT) here can be disruptive.**Let the joint distribution of the source be \(P_{}\) and the true distribution of the target environment be \(P_{^{}}\). The goal of HT, just like standard domain adaptation, is to leverage the source model to learn a target model \(_{}\) that performs well on the true target data, measured by a loss \(\).

**Remarks:****Holistic Transfer.** Typical transfer learning setups assume the target training data \(\) are sampled IID from \(P_{^{}}\). However, this may not hold in practical scenarios due to sampling bias or insufficient sample size, making the target training data \(\) biased; i.e., \(P_{} P_{^{}}\). The target training data \(=\{(_{i}^{},y_{i}^{})\}_{i=1}^{||}\) from the target domain are limited to a subset of classes \(C^{}<C\).

At first glance, our setting seems quite similar to partial DA , which also considers the situation of partial target classes (i.e., \(C^{}<C\)). However, its goal is to perform well only on those \(C^{}\) classes after adaptation. In contrast, our setting aims to adapt the source model's holistic capability of recognizing all \(C\) classes to the target domain. Below we first give the HT definition grounding on the literature formally. We will contrast the HT problem with other existing learning paradigms in subsection 2.3.

**Assumptions of \(P_{}\) vs. \(P_{^{}}\).** Following standard domain adaptation [75; 56; 55], we consider \(P_{}\) and \(P_{^{}}\) have covariate shifts; thus the target features need to be adapted from the source ones to tailor the target domain style, i.e., \(P_{}(|y) P_{^{}}(|y)\). Typically, the label space of the true target domain is already covered by the source counterpart, or \(_{^{}}_{}\).

**Assumptions of \(P_{}\) vs. \(P_{^{}}\).** We study the setting that \(\) is a (potentially biased) sampled dataset from \(P_{^{}}\), therefore, \(P_{}\) and \(P_{^{}}\) should share the same domain styles and semantics. That is, \(P_{}(|y) P_{^{}}(|y),P_{}(y|) P_{^{}}(y|)\). However, the label space of \(P_{}\) might be incomplete w.r.t. \(P_{^{}}\); \(P_{}(y) P_{^{}}(y)\). In other words, the test set \(^{}\) sampled from \(P_{}\). might contain data belonging to classes seen in \(\) but not appear in the target training set \(\). The ultimate goal of the target model is to perform well on \(P_{^{}}\).

**Objective.** The objective of HT is to learn a predictor \(f(;_{}):_{^{}} _{^{}}\) that minimizes the expected loss on the true target distribution \(P_{^{}}\) but _given only the partial observations of a target training dataset \(\) sampled from \(P_{}\) without accessing to \(P_{^{}}\)_:

\[(f;P_{^{}})=_{(,y) P_{^{}}}[(y,f())];}(f; )=_{\{(_{i},y_{i})\}_{i=1}^{||}}(y_{ i},f(_{i})).\] (1)

That is, we can only compute the empirical risk \(}(f;)\) on \(\) but not the true risk \((f;P_{^{}})\).

**Challenges in a realistic HT scenario.** We focus on a more realistic scenario of HT that comes with the following practical constraints and properties when solving \((f;P_{^{}})\) in Equation 1.

* **Missing classes in target training data.** The main challenge is that some classes might only appear in \(P_{^{}}\) in testing but not yet seen in the target training set \(\). Minimizing \(}(f;)\) naively might be risky since it is biased estimation of \((f;P_{^{}})\).
* **Disparate impact .** Training on the partial target data \(\) can result in disproportionate effects on the performance of unseen target classes in testing. The impact may vary based on the semantic affinity between classes, potentially causing serious false negatives. For instance, if a biologist fine-tunes an ImageNet classifier on some non-toxic plants and uses it to recognize plants in the wild, it will likely misclassify those visually similar toxic plants as non-toxic ones.
* **Covariate shifts from the source domain.** If we only have \(\), the missing classes are very difficult to be predicted as it is zero-shot classification. A more practical scenario is to leverage an external source classifier that has been pre-trained on \(\) with many classes. This relaxes the problem to be a special (supervised) domain adaptation task that leverages the source classifier's discriminative ability (as \(P_{}(y|) P_{^{}}(y|)\)) while adapting the input styles \(P_{}() P_{^{}}()\).
* **Source-data-free transfer.** Last but not least, the source model \(_{}\) is available but the source dataset \(\) is not. Due to privacy concerns or the dataset's large size, accessing the source dataset is often unrealistic, especially for foundational models pre-trained in large scales like CLIP .

Therefore, we believe HT is an important and challenging problem to approach for improving towards safer transfer learning in the wild, _given only a source classifier \(_{}\) and a partial target dataset \(\)_.

### Benchmarks Curation for Holistic Transfer: Motivating Examples

To support the study of the HT problem, we create a benchmark that covers extensive scenarios across both experimental and realistic public datasets. More details about dataset curation are in the supplementary. As introduced in Table 1, HT is applicable in various use cases, such as:

* Office-Home : domain adaptation while some classes are missing in the target training set.

* FEMNIST : personalized hand-written alphanumeric recognition with writers' styles. Training samples of a single writer are insufficient to cover all classes.
* iWildCam : species recognition across camera traps of many geo-locations. Adaptation is based on images collected so far, and is deployed for the future; i.e., samples are biased temporally.
* VTAB : fine-tuning zero-shot models for diverse vision tasks with partial classes each.
* iNaturalist (2021 version, Fungi) : classification of visually-similar poisonous fungi.

Each task pre-trains a source model on the source domain, transfers to the target domain with only partial classes, and tests on the full-class target test data. For the Office-Home dataset, the source model is trained on one domain different from the target domain. For the FEMNIST/iWildCam datasets, the source and target data are split by writers/locations. For the VTAB and iNaturalist datasets, there are no explicit domains. A general dataset can be used for pre-training the source model. Here we will use CLIP  just as an example.

### Comparison to Other Paradigms: To Adapt or Not to Adapt, and How?

The most common way of transfer learning is probably fine-tuning, i.e., initializing the target model with the source model parameters and optimizing it with empirical risk minimization (ERM) on the target data. In the HT setup, fine-tuning by minimizing \(}(f;)\) in Equation 1, on the one hand, improves on the performance of those classes seen in the target training set \(\). On the other hand, it can be disruptive in terms of the true risk \((f;P_{^{*}})\) due to catastrophic forgetting  of those classes _already seen in the source domain_ but _unseen during the target fine-tuning_. This creates a dilemma for the user about whether to adapt or not. As shown in the example in Figure 1 (details in section 4), naive fine-tuning does improve those seen classes over the source model while unseen classes can degrade drastically. On the contrary, our proposed treatment (see section 3) can maintain the unseen performance and improve the seen ones, compared to the source model (the \(0\) epoch).

We note the HT problem we proposed is a unique machine learning paradigm that lies between and _complementary_ to existing learning-with-distribution-shift problems . From a high-level and simplified view, many of them involve three stages: 1) source training on \(\) to obtain a source model, 2) target training on \(\), then 3) evaluation of the target goal distribution \(P_{}^{test}\).

We summarize the difference to some popular paradigms in Table 2, including general transfer learning and (supervised) domain adaptation (DA) [98; 75] that assume the target distributions in training and testing are matched. Continual learning (CL) [37; 51; 47] that typically disallows access to source data during target training, aiming to build a multi-tasking model that performs well on all the (possibly many) distributions that have been trained on so far. They all assume the training and test data are matched in distributions (\(P_{} P_{}^{test}\)). Some recent works about OOD domain generalization [41; 3; 79] study fine-tuning from a versatile pre-trained model and preserving its robustness in testing to different unseen input styles of the seen classes but not for unseen classes.

As illustrated in Figure 2 (d), HT requires a very different ability (the green arrow): "generalize" the style shifts learned on the target seen classes to the classes unseen during target training (the red dashed arrow). Compared to OOD generalization, our goal is much harder since it requires transferring the source model to the target styles, for classes both seen and unseen in the target training data. Given only seen classes, using fine-tuning here is notoriously biased to them and disrupts the unseen performance. HT is also addressing quite different challenges from source-free DA , unsupervised DA (with partial classes ), and test-time adaptation  which focus on recovering the true labels for the unlabeled target data. More related works are in the supplementary.

**Our goals.** To this end, we raise the following research questions:

1. Can we resolve the adaptation dilemma and transfer holistically from the source model, improving the test performance of classes seen in the target training set while not hurting the unseen ones?
2. Is it possible to generalize the style shifts to unseen classes? Or how far are we?
3. When should HT work and not work in realistic cases?

Figure 1: **Fine-tuning disrupts unseen target class performance.** An HT example of Ar \(\) Cl domains on Office-Home dataset.

## 3 Towards Non-Disruptive Fine-Tuning

### Solving the HT Objective

We now discuss approaching the HT objective of Equation 1. The key difficulty is the fact that the true risk \((f;P_{^{*}})\) and the empirical risk \(}(f;)\) are in different distributions (i.e., \(P_{^{*}}(y) P_{}(y)\)), such that methods relying on ERM fine-tuning can be seriously biased to seen classes, as discussed in subsection 2.3. Let \(P_{}\) has \(C\) labels \(_{}=\{y_{1}^{},...,y_{C}^{}\}\) and \(P_{^{*}}\) has some extra unseen labels \(_{^{*}}=_{}\{y_{C+1}^{},y_{C+2}^{},...,y_{C*}^{}\}\). Worth noting, we fairly assume the source data cover all target classes \(_{^{*}}_{}\); if a test class was not seen in \(\), it becomes a zero-shot problem which might not be solved by transfer learning.

**Oracle solutions.** An oracle solution is to directly match the distributions, making \(P_{^{*}}(y)=P_{}(y)\) by augmenting data to \(\). For instance, one can collect more data for those unseen classes in the target domain, or translate the source data sample of an unseen class \((^{},y_{C}^{})\) into \((^{},y_{C}^{})\) with a style transfer model from \(\) to \(\). They are obviously not available for HT-- the former requires the cost to collect complete labeled data in the target (essentially the upper bound in domain adaptation); the latter needs to access the source data which is unrealistic as discussed in subsection 2.1.

**Suboptimal solutions.** By leveraging techniques in other paradigms we discussed in subsection 2.3, it might be possible to improve over plain fine-tuning. However, none explicitly recovers the joint probability of the target goal distribution \(P_{^{*}}(,y)\), but only \(P_{}(,y)\) or \(P_{}(y)\).

Next, we propose two directions that together could approach \(P_{^{*}}(,y)\) under HT constraints.

### Disentangling Covariate Shifts from Disruptive Concept Shifts

Minimizing \(}(f;)\) on \(\) transfers the model from \(P_{}(,y;_{})\) to \(_{}(,y;_{})\) which optimizes towards the target styles \(P_{}(|y)\) (which should be the same as \(P_{^{*}}(|y)\)) but also collapses the label distribution to \(_{}(y;_{})\). The model will unlikely predict a \(y_{C}^{}\) for any inputs. If we can only adapt the covariates from the source classifier, then we have

\[_{^{*}}(,y;_{}) :=_{^{*}}(y|;_{}) {P}_{^{*}}(;_{}),\] (2) \[_{^{*}}(y|;_{ })P_{}(;_{}),_{^{*}}(;_{}) P_{ }(;_{})]\] (3) \[ P_{}(y|;_{})P_{ }(;_{}).[P_{^{*}}(y| ) P_{}(y|) P_{}(y|)]\] (4)

   Settings & Target Training & Target Goal \(P_{}^{test}\) & \(P_{}\) vs. \(P_{}^{test}\) & \(P_{}\) vs. \(P_{}^{test}\) \\  Standard transfer & \(_{}\) and/or \(,\) & \(P_{}\) & \(P_{} P_{}^{test}\) & \(P_{} P_{}^{test}\) \\ Domain adaptation & \(_{}\) and/or \(,\) & \(P_{}\) & \(P_{}(y|) P_{}^{test}(y|)\) & \(P_{} P_{}^{test}\) \\ Continual learning & \(_{};\) & \(P_{}+P_{}\) & \(P_{} P_{}^{test}\) & \(P_{}^{test} P_{}+P_{}\) \\ OOD generalization & \(_{}\), \(\) & \(P_{^{*}}\) & \(P_{} P_{}^{test}\) & \(P_{}(y) P_{}(|y)\) \\  Holistic transfer & \(_{}\), \(\) & \(P_{^{*}}\) & \(P_{}(y|) P_{}^{test}(y|)\) & \(P_{}(|y) P_{^{*}}(|y) \\ P_{}(y) P_{^{*}}()\) \\   

Table 2: **Comparison of different paradigms: Source Training\(\)Target Training\(\)Test on Target Goal.** Let \(P_{}\) and \(P_{}\) be the distributions of the source data \(\) and the target training set \(\), respectively. \(P_{}^{test}\) is the target test distribution as the final goal. \(P_{^{*}}\) is the distribution that may have certain discrepancy from \(P_{}\).

Figure 2: **Different settings of learning with distribution shift.** Consider a data distribution space of different styles (**x-axis**) and classes (**y-axis**), each algorithm leverages the source data (the **blue region**) to facilitate the knowledge transfer (the green arrow) to the target training data (the yellow region ). The target test distribution \(P_{}^{test}\) is highlighted with the **black box**. The red arrow represents the generalization for unseen test data.

Equation 3 will hold if \(P_{^{*}}(|y) P_{}(|y)\). However, how can we disentangle the covariate shifts from \(}(f;)\) and change the style to \(P_{}()\) only without concept shift ? We explore:

* **Updating batchnorm statistics only.** The statistics in batchnorm layers (in feed-forward networks) are believed to capture the domain information. Updating them is often considered as a strong baseline in DA . It infers on \(\) and re-calculates the feature means and variances, which is less likely to be impacted by the missing classes issue.
* **Training extra instance normalization layers.** We explore another way based on instance normalization (IN) , which normalizes within each input itself over its spatial dimension. IN is widely used in style transfer to capture the input styles without changing their semantics. We examine a way for HT by inserting extra IN layers into the source model (which may not have IN layers originally) and fine-tuning them with other DNN parameters fixed to prevent concept shifts.

Unfortunately, we found updating normalization layers is not yet satisfying -- receiving limited improvements and still suffering from catastrophic forgetting. We thus hypothesize the key is to consider the optimization besides architectures.

**Proposed Leave-Out Local SGD (LOLSGD).** In our HT problem, fine-tuning is affected by the covariate shift (from the source to the target) and the disruptive concept shift (from classifying all the classes to classifying the seen classes). We aim to disentangle them or, more precisely, reduce the disruptive concept shift by subsampling multiple datasets that each contain a subset of the seen classes.

We propose a novel approach based on local SGD  in distributed learning. The local SGD gradient is accumulated over several consecutive SGD steps, possibly many runs in parallel. For notations, let the local SGD gradient computed on the samples that their labels are inside a label space \(\) be \(()\). If sampling IID from \(\) and the local step is \(1\), it reduces to standard SGD. Instead of using \((_{})\), which will bias to the seen target classes \(_{}\), our idea is to mitigate the change towards the label and encourage the change in styles.

Inspired by recent theoretical analyses in meta-representation learning , we intentionally sample \(m\) non-IID subsets of labels \(_{}^{m}_{}\) by _leaving out_ some classes, compute \((_{}^{m})\) for each \(m\), and average their local gradients \((_{}^{m})\)

\[_{}(_{};M):=_{m}( _{}^{m}),\,(_{}^{m}):=-_{}_{\{i|||y_{i} _{}^{m}\}}(y_{i},f(_{i};)).\] (5)

During training, each update (with size \(\)) becomes \(-_{}\) instead of \(-}(f;)\). Intuitively, \((_{}^{m})\) is biased towards the respective classes \(_{}^{m}\) and drifts away from each other along more local steps, similar to the model shift problem in decentralized learning . As a blessing here (rather than a curse!), our key insight is averaging them could "cancel out" the gradients biased to certain classes. The style should be adapted for Equation 3 without conflicts since all local gradients are directed towards the same target style, as illustrated in Figure 3. When updating the model separately in parallel with these subsampled datasets, each updated model is affected by a different concept shift but a shared covariate shift. Averaging these models can potentially cancel out the disruptive concept shifts and strengthen the shared covariate shift. We verify our intuition in Table 19 in the supplementary. Compared to \(}(f;)\), \(_{}\) averages \(M\) (\(M=10\) in our experiments) accumulated local gradients thus more expensive to compute. As will be shown by experiments, LOLSGD can outperform standard SGD with similar computation budgets in fair comparisons.

LOLSGD is fundamentally different from meta-learning, whose goal is to learn a meta-modal that can be easily applied to a future task (e.g., a few-shot classification task). While meta-learning also subsamples its meta-training set, it is mainly to simulate multiple future tasks for learning the meta-model, not to cancel out unwanted gradients. LOLSGD is also an extension of local SGD. The goal of local SGD is mainly to reduce the communication overhead of large-scale distributed training. In contrast, in LOLSGD, the target training data set is not decentralized initially, but we strategically subsample it to simulate different concept shifts.

Figure 3: **LOLSGD illustration.**

### Preserving Class Relationships

The subsection 3.2 assumes we can change the styles while not changing the conditional probability of the source model \(P_{}(y|)\) in Equation 4. We observe in practice it can still be sensitive and lean towards \(\{y^{}\}\). We, therefore, hypothesize that it is necessary to have some regularization to preserve the nice class relationships encoded in the source model already during target training such that Equation 4 could hold. Let us decompose the predictor \(f(;)=g(;),=h(;)\) to be a feature extractor \(h\) and a linear classifier \(g\). We explore the following strategies:

* **Frozen linear classifier.** Even the source features is already perfect for \(P_{}(y|)=P_{^{*}}(y|)\), it can collapse quickly as the logits of \(y^{}\) will be suppressed by \(}(f;)\). We found it crucial to freeze the linear classifier \(\), which corresponds to the common practice in source-free DA .
* **Selective distillation.** Another way is to utilize distillation that can prevent forgetting in CL [47; 93; 22], forcing the soft labels of the target model and the source model to match. Given a sample \((,y^{})\) and let \(f^{}()\) be the logits of \(y^{}\) dimensions. Here, only the unseen class logits need to be distilled since seen class logits are reliable as the target model is trained on ground truths, unlike source seen class logits (which are suboptimal due to domain shift). We add a distillation loss \(_{}=((f^{}(;_{}))||(f^{}(;_{})))\) with the Kullback-Leibler (KL) divergence, where \(()\) is the softmax function.
* **Feature rank regularization.** A key observation is in the feature dynamic of fine-tuning on the partial target data -- instead of shifting to another full-rank space, we found the features tend to collapse to a low-dimensional space, as shown in Figure 4 -- implying the target model "shortcutting" the solution and hampering the generalization ability  to handle unseen target classes. We consider a regularizer to avoid too many singular values of the features becoming zeros. Given the feature matrix of a mini-batch \(^{N d}\) and its mean feature \(}^{d}\), we compute its covariance matrix \(=_{n=1}^{N}(_{n}-})(_{n}-})^{}\) and a regularizer \(_{}=\|(^{})\|_{2} ^{2}\), inspired by [36; 17; 66].

We note that, these techniques only preserve the source class relationships, but they do not improve the transfer to the target. Thus they should be considered together with ERM or LOLSGD for HT.

**Ensemble with the source model.** As pointed out by , a way to reclaim some ability of the source model is to average the parameters of the source and target models, assuming they lie in a linearly-connected space. We consider it complementary post-processing for the target training techniques we considered above. We relax the assumption and further examine the ensemble of the source and target models in their predictions (after the softmax function \(\) for logits).

## 4 Experiments

**General setup.** To answer the questions we raised in subsection 2.3, we experiment on extensive HT scenarios proposed in subsection 2.2. For each task, we pre-train a source model in standard practice, discard the source data, and adapt the source parameters \(_{}\) to obtain the target model. We split the target data for training and testing, based on different scenarios. The training and test sets share the same styles but cover different sets of classes, where some classes are unseen in training. We assume the source model covers all classes and the goal is to adapt to the target domain for both the seen and unseen classes. All methods use the cross-entropy loss for \(\) and SGD momentum optimizer. All experiments fine-tune the source model for \(20\) epochs (\(10\) for FEMNIST) by default. For the regularizers in section 3, we attach them as \(+_{}_{}\), where the weights \(\) are quite stable thus we did not search for it exhaustedly for every method, but use the same ones per dataset. For the proposed LOLSGD, we set \(M=10\) and randomly drop \(3\) classes when sampling \(_{}^{m}\) in Equation 5. Each subgradient in LOLSGD is by

   Methods / Acc. &  \\  Source & 63.90 & 65.65 \\ Naive Target (w/o SE) & 53.50 & 22.36 \\  Naive Target (WiSE) & 53.56 & 25.26 \\ Naive Target (SE) & 67.46 & 50.89 \\  SGD (WiSE) \(\) & 69.75 & 60.42 \\ SGD (SE) \(\) & 71.01 & 59.19 \\ SWA \(\) & 64.99 & 45.10 \\ SWAD \(\) & 65.05 & 45.30 \\ SGD \(+_{}\) & 69.78 & 62.94 \\ SGD \(+_{}\) & **71.05** & 65.48 \\  LOLSGD \(\) & 70.70 & 63.14 \\ LOLSGD \(+_{}\) & 68.71 & 64.56 \\ LOLSGD \(+_{}\) & 69.94 & **66.29** \\  
**Best**\(\) & 17.55 & 43.12 \\   

Table 4: Effects of Source Ensemble (Avg. as in Table 3).

Figure 4: **Feature rank collapse.** Top singular values of Office-Home adapted features.

local SGD (\(\) epoch) and we run the same total epochs, for a fair computation budget. **Please see the supplementary for the details of setup, hyperparameters, and more analyses.**

### Domain Adaptation on Partial Target Data

We first use the Office-Home dataset  with ResNet-50  to study the effectiveness of the methods in section 3. Office-Home is a popular domain adaptation benchmark consisting of 65 object categories from four domains (Art, Clipart, Real, and Product).

**Methods.** We consider baselines including directly predicting with the **Source** model and naively fine-tuning it for a **Target** model. As proposed in section 3, we explore methods that promote covariate shifts by learning the limited parameters in normalization layers with the backbone parameters frozen, like **batchnorms (BN)** and an extra **instance normalization (IN)** layer. We include another baseline **LP-FT** proposed for the OOD generalization of fine-tuning but not for missing classes in HT. To compare with our LOLSGD, we also include related techniques in domain generalization based on stochastic weight average (**SWA**)  and its extension **SWAD**.

**Comparison.** We highlight the following observations in Table 3:

* **Naive fine-tuning is disruptive:** the unseen performance drops drastically, ultimately degrading overall accuracy and confirming the challenges of HT for all pairs of domains.
* **Normalization layers may help:** maintaining unseen accuracy and improving the seen little.
* **Freezing the classifier during training is necessary:** we found that the frozen classifier is crucial when the feature extractor is adapted (Avg. \(+21\%\) Unseen vs. Target), motivating our approach of preserving class relationships subsection 3.3.
* \(_{}\) **and \(_{}\) regularize forgetting further:** largely mitigating unseen performance drops.
* **LOLSGD is effective:** without any regularizers, the proposed local SGD optimization can outperform the strong generalized SGD baselines SWA and SWAD.
* **LOLSGD with regularizers performs best overall:** as we suggested in section 3, LOLSGD needs to be carefully regularized while it adapts the domain styles.

**Effects of Source Ensemble (SE).** We further post-process the target models learned in Table 3 by ensembling with the source model, as introduced in subsection 3.3. We also include W1SE  that ensembles the model weights instead of predictions. Generally, we found SE may reclaim some unseen performance, but not necessarily improve overall, as summarized in Table 4.

_Overall, we show that HT is promising, improving the overall performance without sacrificing the unseen accuracy (highlighted in red in Table 3). Notably, our proposed solutions are based on optimization and agnostic to architectures. We will mainly focus on them later as they perform better.

   Domains source\(-\)target &  &  &  &  &  &  &  \\ Methods / Acc. & Overall & Unseen & Overall & Unseen & Overall & Unseen & Overall & Unseen & Overall & Unseen & Overall & Unseen & Overall & Unseen \\  Source & 47.07 & 50.29 & 67.45 & 72.00 & 72.73 & 74.68 & 65.73 & 68.19 & 51.13 & 49.26 & 79.28 & 79.47 & **63.90** & **65.65** \\ Naive Target & 49.46 & 9.06 & 52.39 & 23.75 & 59.48 & 32.91 & 54.93 & 28.03 & 46.92 & 63.64 & 62.31 & 34.08 & **38.50** & **22.36** \\   BN only & 45.94 & 14.33 & 54.30 & 30.25 & 63.22 & 47.91 & 61.47 & 42.32 & 49.10 & 13.42 & 71.57 & 51.96 & **57.60** & 73.26 \\ BN (statics) only & 47.44 & 50.44 & 65.63 & 65.03 & 70.88 & 71.76 & 74.12 & 61.67 & 73.03 & 59.56 & 55.75 & 80.09 & **52.64** & 62.41 & 62.71 \\ IN only & 48.96 & 49.42 & 68.19 & 71.13 & 76.23 & 74.26 & 66.40 & 68.46 & 54.89 & 51.48 & 79.35 & 79.05 & **48.17** & 65.81 \\ LP-FT & 43.91 & 67.38 & 40.56 & 16.38 & 55.81 & 25.60 & 50.67 & 19.14 & 45.01 & 3.54 & 56.21 & 22.21 & **49.95** & **15.60** \\  SGD (w/ frozen classifier \(@sectionsign\)) & 52.11 & 24.12 & 64.36 & 44.75 & 70.26 & 55.27 & 68.27 & 82.31 & 50.15 & 52.66 & 24.19 & 75.75 & 59.92 & 62.64 & **35.44** & **35.44** \\ SGD (\(+\)\(_{}\)\(@sectionsign\) & 56.54 & 39.18 & 72.81 & 61.63 & 75.73 & 67.51 & 70.13 & 64.15 & 61.96 & 51.45 & 60.70 & 50.53 & 69.63 & 57.41 \\ SGD \(+\)\(_{}\)\(@sectionsign\) & 59.17 & 39.47 & 70.68 & 57.57 & 74.31 & 64.28 & **70.40** & **64.42** & 61.65 & 39.09 & 79.57 & 68.86 & 69.30 & 55.64 \\ SWA \(@sectionsign\) & 53.25 & 26.32 & 65.25 & 46.00 & 71.16 & 56.82 & 68.40 & 55.26 & 56.24 & 26.55 & 75.39 & 59.64 & **64.99** & 45.10 \\ SWAD \(@sectionsign\) & 53.38 & 26.46 & 66.52 & 46.00 & 71.61 & 57.24 & 68.58 & 55.53 & 55.87 & 26.40 & 57.68 & 69.20 & 65.05 & 45.30 \\ LOLSGD\(@sectionsign\) & 54.77 & 35.09 & 70.83 & 56.88 & 74.91 & 64.84 & 70.53 & 62.53 & 58.77 & 35.25 & 80.02 & 69.27 & 68.58 & **53.98** \\  LOLSGD \(+\)\(_{}\)\(@sectionsign\) & 58.57 & 43.86 & 72.59 & 64.53 & 75.06 & 68.92 & 70.13 & 66.04 & 61.58 & 44.10 & 80.82 & 74.02 & 69.79 & 60.26 \\ LOLSGD \(+\)\(_{}\)\(@sectionsign\) & 57.44 & 46.35 & 75.24 & 67.38 & 76.33 & 70.75 & 69.07 & 65.77 & 60.68 & 45.58 & 81.56 & 75.28 & 70.05 & 61.85 \\ LOLSGD \(+\)\(_{}\)\(@sectionsign\) & **60.83** & **51.75** & **75.75** & **70.25** & **76.70** & **74.26** & 70.13 & 69.54 & **65.71** & **53.10** & **82.95** & **80.31** & **72.01** & **66.54** \\  
**Best**\(\)Acc\_Base\_Target & 15.87 & 42.69 & 23.36 & 46.50 & 17.22 & 41.35 & 15.47 & 36.39 & 18.79 & 46.76 & 20.64 & 46.23 & **18.51** & **44.18** \\  Oracle & 79.32 & 82.46 & 90.96 & 91.50 & 84.57 & 84.95 & 78.13 & 80.59 & 79.85 & 75.52 & 91.92 & 92.04 & **84.13** & **84.51** \\   

Table 3: Domain adaptation 65-way test accuracy on Office-Home with 30 seen and 35 unseen classes. Variances of random seeds are provided in the supplementary due to space limit. Blue: HT methods suggested by us in section 3. Red: methods that significantly improve overall accuracy and successfully maintain unseen accuracy on the source model. \(@sectionsign\): the linear classifier is frozen during training.

[MISSING_PAGE_FAIL:9]

### Studies: When should HT Work and Not Work?

**Semantic gaps of the source model.** As discussed in subsection 2.3, HT relies on the DA assumption that the source and target tasks share similar semantics, i.e., \(P_{}(y|) P_{}(y|)\). There should be little semantic shifts from the source to the target. In Table 7, we found the unseen class performance of some tasks is very low, and applying HT methods cannot help much. We hypothesize that the images are likely to be predicted as other meanings by the CLIP model rather than the goal of the target task. Looking at the Street View House Numbers (SVHN) dataset , we adversarially add class names like "number plate" to confuse with the digits (e.g., "9"). Interestingly, we found about \(60\%\) of the cases, CLIP fails to predict a number (Table 8). This validates that \(P_{}(y|)\) pre-trained by CLIP is quite different from the digit classification, therefore, it becomes very hard if not impossible for it to recognize unseen digits correctly. Similarly, DTD (texture) and EuroSAT (land use) could suffer from the same concept shift issue. Resolving it requires a "task-level generalization" (e.g., object recognition \(\) digits classification), which is obviously out of the scope of HT.

**Disparate impact.** If we fine-tune a task, should it benefit other similar tasks? We reveal that the answer might be _negative_ in HT settings. We fine-tune CLIP on a task following Table 7, evaluate on _others_ using the corresponding CLIP text classifiers, and summarize the accuracy difference to the original CLIP in Figure 5. Intriguingly, more similar tasks in fact drop more, likely because they are _disrupted_ by the fine-tuned task most. This again highlights the importance of HT; we found our method makes it more robust.

**Case study: fine-tuning can lead to serious false negatives.** So far, we mainly evaluate HT from the lens of unseen class accuracy. Here, we discuss confusion in predictions and a case that transfers without considering HT could pose even larger risks. Considering training a classifier for recognizing mushrooms on some non-toxic ones and encountering a visually similar but toxic one in testing, the classifier will likely label it as one of the non-toxic ones. Even worse, such bias is exaggerated by the adaptation, due to the partial training classes. We build an extreme case that selects 6 pairs of fungi (toxic/non-toxic each) from the iNaturalist dataset  and fine-tunes on only the non-toxic ones from CLIP. The results in Table 9 show such concern exists and should be considered in future algorithm designs.

## 5 Conclusions (Related Works & Limitations in Supplementary)

We introduce a novel and practical transfer learning problem that emphasizes generalization to unseen classes in the target domain but seen in the source domain. Despite its challenges, we establish strong baselines and demonstrate the potential for improving both seen and unseen target classes simultaneously, paving the way for holistic transfer. To facilitate further research, we construct a comprehensive benchmark with diverse scenarios and conduct insightful experiments. In future work, we envision exploring various directions, including improved disentanglement of domain styles and classes, integrating our approach with other paradigms like test-time adaptation, and extending beyond classification tasks.

Figure 5: **Disparate impact: will fine-tuning improve similar tasks? We fine-tune CLIP on the training set of a task and evaluate on others. Naively fine-tuned target models actually _degrade more_ on more similar tasks, while our HT method is much more robust. Tasks are sorted in the x-axis based on Affinity Score following .**

  Classes & Predictions \\ “0” to “9” & 39.4\% \\  “number plate” & 32.7\% \\ “house number” & 16.8\% \\ “wallpaper” & 11.1\% \\  

Table 8: Examining HT of zero-shot CLIP on SVHN dataset.