# State-wise Constrained Policy Optimization

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Reinforcement Learning (RL) algorithms have shown tremendous success in simulation environments, but their application to real-world problems faces significant challenges, with safety being a major concern. In particular, enforcing state-wise constraints is essential for many challenging tasks such as autonomous driving and robot manipulation. However, existing safe RL algorithms under the framework of Constrained Markov Decision Process (CMDP) do not consider state-wise constraints. To address this gap, we propose State-wise Constrained Policy Optimization (SCPO), the first general-purpose policy search algorithm for state-wise constrained reinforcement learning. SCPO provides guarantees for state-wise constraint satisfaction in expectation. In particular, we introduce the framework of Maximum Markov Decision Process, and prove that the worst-case safety violation is bounded under SCPO. We demonstrate the effectiveness of our approach on training neural network policies for extensive robot locomotion tasks, where the agent must satisfy a variety of state-wise safety constraints. Our results show that SCPO significantly outperforms existing methods and can handle state-wise constraints in high-dimensional robotics tasks.

## 1 Introduction

Reinforcement learning (RL) has achieved remarkable progress in games and control tasks (Mnih et al., 2015; Vinyals et al., 2019; Brown and Sandholm, 2018; He et al., 2022; Zhao et al., 2019). However, one major barrier that limits the application of RL algorithms to real-world problems is the lack of safety assurance. RL agents learn to make reward-maximizing decisions, which may violate safety constraints. For example, an RL agent controlling a self-driving car may receive high rewards by driving at high speeds but will be exposed to high chances of collision. Although the reward signals can be designed to penalize risky behaviors, there is no guarantee for safety. In other words, RL agents may sometimes prioritize maximizing the reward over ensuring safety, which can lead to unsafe or even catastrophic outcomes (Gu et al., 2022).

Emerging in the literature, safe RL aims to provide safety guarantees during or after training. Early attempts have been made under the framework of constrained Markov Decision Process, where the majority of works enforce cumulative constraints or chance constraints (Ray et al., 2019; Achiam et al., 2017; Liu et al., 2021). In real-world applications, however, many critical constraints are instantaneous. For instance, collision avoidance must be enforced at all times for autonomous cars (Zhao et al., 2023). Another example is that when a robot holds a glass, the robot can only release the glass when the glass is on a stable surface. The violation of those constraints will lead to irreversible failures of the task. In this work, we focus on state-wise (instantaneous) constraints.

The State-wise Constrained Markov Decision Process (SCMDP) is a novel formulation in reinforcement learning that requires policies to satisfy hard state-wise constraints. Unlike cumulative or probabilistic constraints, state-wise constraints demand full compliance at each time step as formalized by Zhao et al. (2023). Existing state-wise safe RL methods can be categorized based onwhether safety is ensured during training. There is a fundamental limitation that it is impossible to guarantee hard state-wise safety during training without prior knowledge of the dynamic model. The best we can achieve in a model free setting is to learn to satisfy the constraints using as few samples as possible, which is the focus of this paper. We aim to provide theoretical guarantees on state-wise safety violation and worst case reward degredation during training.

Our approach is underpinned by a key insight that constraining the maximum violation is equivalent to enforcing state-wise safety. This insight leads to a novel formulation of MDP called the _Maximum Markov Decision Process_ (MMDP). With MMDP, we establish a new theoretical result that provides a bound on the difference between the maximum cost of two policies for episodic tasks. This result expands upon the cumulative discounted reward and cost bounds for policy search using trust regions, as previously documented in literature . We leverage this result to design a policy improvement step that not only guarantees worst-case performance degradation but also ensures state-wise cost constraints. Our proposed algorithm, _State-wise Constrained Policy Optimization_ (SCPO), approximates the theoretically-justified update, which achieves a state-of-the-art trade-off between safety and performance. Through experiments, we demonstrate that SCPO effectively trains neural network policies with thousands of parameters on high-dimensional simulated robot locomotion tasks; and is able to optimize rewards while enforcing state-wise safety constraints. This work represents a significant step towards developing practical safe RL algorithms that can be applied to many real-world problems.

## 2 Related Work

### Cumulative Safety

Cumulative safety requires that the expected discounted return with respect to some cost function is upper-bounded over the entire trajectory. One representative approach is constrained policy optimization (CPO) , which builds on a theoretical bound on the difference between the costs of different policies and derives a policy improvement procedure to ensure constraints satisfaction. Another approach is interior-point policy optimization (IPO) , which augments the reward-maximizing objective with logarithmic barrier functions as penalty functions to accommodate the constraints. Other methods include Lagrangian methods  which use adaptive penalty coefficients to enforce constraints and projection-based constrained policy optimization (PCPO)  which projects trust-region policy updates onto the constraint set. Although our focus is on a different setting of constraints, existing methods are still valuable references for illustrating the advantages of our SCPO. By utilizing MMDP, SCPO breaks the conventional safety-reward trade-off, which results in stronger convergence of state-wise safety constraints and guaranteed performance degradation bounds.

### State-wise Safety

Hierarchical PolicyOne way to enforce state-wise safety constraints is to use hierarchical policies, with an RL policy generating reward-maximizing actions, and a safety monitor modifying the actions to satisfy state-wise safety constraints. Such an approach often requires a perfect safety critic to function well. For example, conservative safety critics (CSC)  propose a safe critic \(Q_{C}(s,a)\), providing a conservative estimate of the likelihood of being unsafe given a state-action pair. If the safety violation exceeds a predefined threshold, a new action is re-sampled from the policy until it passes the safety critic. However, this approach is time-consuming. On the other hand, optimization-based methods such as gradient descent or quadratic programming can be used to find a safe action that satisfies the constraint while staying close to the reference action. Unrolling safety layer (USL)  follows a similar hierarchical structure as CSC but performs gradient descent on the reference action iteratively until the constraint is satisfied based on learned safety critic \(Q_{C}(s,a)\). Finally, instead of using gradient descent, Lyapunov-based policy gradient (LPG)  and SafeLayer  directly solve quadratic programming (QP) to project actions to the safe action set induced by the linearized versions of some learned critic \(Q_{C}(s,a)\). All these approaches suffer from safety violations due to imperfect critic \(Q_{C}(s,a)\), while those solving QPs further suffer from errors due to the linear approximation of the critic. To avoid those issues, we propose SCPO as an end-to-end policy which does not explicitly maintain a safety monitor.

End-to-End PolicyEnd-to-end policies maximize task rewards while ensuring safety at the same time. Related work regarding state-wise safety after convergence has been explored recently. Some approaches (Liang et al., 2018; Tessler et al., 2018) solve a primal-dual optimization problem to satisfy the safety constraint in expectation. However, the associated optimization is hard in practice because the optimization problem changes at every learning step. Bohez et al. (2019) approaches the same setting by augmenting the reward with the sum of the constraint penalty weighted by the Lagrangian multiplier. Although claimed state-wise safety performance, the aforementioned methods do not provide theoretical guarantee and fail to achieve near-zero safety violation in practice. He et al. (2023) proposes AutoCost to automatically find an appropriate cost function using evolutionary search over the space of cost functions as parameterized by a simple neural network. It is empirically shown that the evolved cost functions achieve near-zero safety violation, however, no theoretical guarantee is provided, and extensive computation is required. FAC (Ma et al., 2021) does provide theoretically guaranteed state-wise safety via parameterized Lagrange functions. However, FAC replies on strong assumptions and performs poorly in practice. To resolve the above issues, we propose SCPO as an easy-to-implement and theoretically sound approach with no prior assumptions on the underlying safety functions.

## 3 Problem Formulation

### Preliminaries

In this paper, we are especially interested in guaranteeing safety for episodic tasks, which falls within in the scope of finite-horizon Markov Decision Process (MDP). An MDP is specified by a tuple \((,,,R,P,)\), where \(\) is the state space, and \(\) is the control space, \(R:\) is the reward function, \(0<1\) is the discount factor, \(:\) is the initial state distribution, and \(P:\) is the transition probability function. \(P(s^{}|s,a)\) is the probability of transitioning to state \(s^{}\) given that the previous state was \(s\) and the agent took action \(a\) at state \(s\). A stationary policy \(:()\) is a map from states to a probability distribution over actions, with \((a|s)\) denoting the probability of selecting action \(a\) in state \(s\). We denote the set of all stationary policies by \(\). Subsequently, we denote \(_{}\) as the policy that is parameterized by the parameter \(\).

The standard goal for MDP is to learn a policy \(\) that maximizes a performance measure \(_{0}()\) which is computed via the discounted sum of reward:

\[_{0}()=_{}[_{t=0}^{H}^{t}R( s_{t},a_{t},s_{t+1})],\] (1)

where \(H\) is the horizon, \(=[s_{0},a_{0},s_{1},]\), and \(\) is shorthand for that the distribution over trajectories depends on \(:s_{0},a_{t}(|s_{t}),s_{t+1} P(|s_{t},a_{t})\).

### State-wise Constrained Markov Decision Process

A constrained Markov Decision Process (CMDP) is an MDP augmented with constraints that restrict the set of allowable policies. Specifically, CMDP introduces a set of cost functions, \(C_{1},C_{2},,C_{m}\), where \(C_{i}:\) maps the state action transition tuple into a cost value. Analogous to (1), we denote

\[_{C_{i}}()=_{}[_{t=0}^{H}^{t }C_{i}(s_{t},a_{t},s_{t+1})]\] (2)

as the cost measure for policy \(\) with respect to cost function \(C_{i}\). Hence, the set of feasible stationary policies for CMDP is then defined as follows, where \(d_{i}\):

\[_{C}=\{\; i,_{C_{i}}() d_{i}\}.\] (3)

In CMDP, the objective is to select a feasible stationary policy \(_{}\) that maximizes the performance measure:

\[}\;_{0}(),\;\;\;_{ C}.\] (4)

[MISSING_PAGE_FAIL:4]

where \(ist\) is some distance measure, and \(>0\) is a step size. For actual implementation, we need to evaluate the constraints first in order to determine the feasible set. However, it is challenging to evaluate the constraints using samples during the learning process. In this work, we propose SCPO inspired by recent trust region optimization methods Schulman et al. (2015). SCPO approximates (10) using (i) KL divergence distance metric \(ist\) and (ii) surrogate functions for the objective and constraints, which can be easily estimated from samples on \(_{k}\). Mathematically, SCPO requires the policy update at each iteration is bounded within a trust region, and updates policy via solving following optimization:

\[_{k+1} =}{}}_{  d^{_{k}}\\ a}[A^{_{k}}(,a)]\] (11) \[ _{ d^{_{k}}}[_{KL}(\|_{ k})[]],\] \[_{D_{i}}(_{k})+}_{ {c}^{_{k}}\\ a}\!\![A_{D_{i}}^{_{k}}(,a)]+2(H+1) _{D_{i}}^{}} w_{i},i=1,,m.\]

where \(_{KL}(^{}\|)[]\) is KL divergence between two policy \((^{},)\) at state \(\), the set \(\{_{}:\ _{ d^{_{k}}}[_{KL}(\|_{ k})[]]\}\) is called _trust region_, \(d^{_{k}}(1-)_{t=0}^{H}^{t}P(_{t}=|_{ k})\), \(^{_{k}}_{t=0}^{H}P(_{t}=|_{k})\) and \(_{D_{i}}^{}_{}|_{a}[A_{D_ {i}}^{_{k}}(,a)]|\). We then show that SCPO guarantees (i) worst case maximum state-wise cost violation, and (ii) worst case performance degradation for policy update, by establishing new bounds on the difference in returns between two stochastic policies \(\) and \(^{}\) for MMDPs.

Theoretical Guarantees for SCPOWe start with the theoretical foundation for our approach, i.e. a new bound on the difference in state-wise maximum cost between two arbitrary policies. The following theorem connects the difference in maximum state-wise cost between two arbitrary policies to the total variation divergence between them. Here total variation divergence between discrete probability distributions \(p,q\) is defined as \(_{TV}(p\|q)=_{i}|p_{i}-q_{i}|\). This measure can be easily extended to continuous states and actions by replacing the sums with integrals. Thus, the total variation divergence between two policy \((^{},)\) at state \(\) is defined as: \(_{TV}(^{}\|)[]=_{TV}(^{}( |)\|(|))\).

**Theorem 1** (Trust Region Update State-wise Maximum Cost Bound).: _For any policies \(^{},\), with \(_{D}^{^{}}_{}|_{a^ {}}[A_{D}^{}(,a)]|\), and define \(^{}=_{t=0}^{H}P(_{t}=|)\) as the non-discounted augmented state distribution using \(\), then the following bound holds:_

\[_{D}(^{})-_{D}()}_{^{}\\ a^{}}\![A_{D}^{}(,a)+2(H+1) _{D}^{^{}}_{TV}(^{}\|)[]].\] (12)

The proof for Theorem 1 is summarized in Appendix A. Next, we note the following relationship between the total variation divergence and the KL divergence Boyd et al. (2003); Achiam et al. (2017): \(_{^{}}[_{TV}(p\|q)[]] _{^{}}[_{KL}(p\|q)[ ]]}\). The following bound then follows directly from Theorem 1:

\[_{D}(^{})_{D}()+}_{^{}\\ a^{}}\!\![A_{D}^{}(,a)+2(H+1) _{D}^{^{}}_{^{ }}[_{KL}(^{}\|)[]]}].\] (13)

By Equation (13), we have a guarantee for satisfaction of maximum state-wise constraints:

**Proposition 1** (SCPO Update Constraint Satisfaction).: _Suppose \(_{k},_{k+1}\) are related by (11), then \(D_{i}\)-return for \(_{k+1}\) satisfies_

\[ i,_{D_{i}}(_{k+1}) w_{i}.\]

Proposition 1 presents the first constraint satisfaction guarantee under MMDP. Unlike trust region methods such as CPO and TRPO, which assume a discounted sum characteristic, MMDP's non-discounted sum characteristic invalidates these theories. As the maximum state-wise cost is calculated through a summation of non-discounted increments, analysis must be performed on a finite horizon to upper bound the worst-case summation. In contrast, the theory behind CPO relies on infinite horizon analysis with discounted constraint assumptions, which is not applicable for MMDP settings.

Next, we provide the performance guarantee of SCPO. Previous analyses of performance guarantees have focused on infinite-horizon MDP. We generalize the analysis to finite-horizon MDP, inspired by previous work (Kakade and Langford, 2002; Schulman et al., 2015; Achiam et al., 2017a), and prove it in Appendix B. The infinite-horizon case can be viewed as a special case of the finite-horizon setting.

**Proposition 2** (SCPO Update Worst Performance Degradation).: _Suppose \(_{k},_{k+1}\) are related by (11), with \(^{_{k+1}}_{i}|_{a_{k+1}}[A^{_ {k}}(,a)]|\), then performance return for \(_{k+1}\) satisfies_

\[(_{k+1})-(_{k})- ^{_{k+1}}}{1-}.\]

## 5 Practical Implementation

In this section, we show how to (a) implement an efficient approximation to the update (11), (b) encourage learning even when (11) becomes infeasible, and (c) handle the difficulty of fitting augmented value \(V^{}_{D_{i}}\) which is unique to our novel MMDP formulation. The full SCPO pseudocode is given as algorithm 1 in appendix C.

Practical implementation with sample-based estimationWe first estimate the objective and constraints in (11) using samples. Note that we can replace the expected advantage on rewards using an importance sampling estimator with a sampling distribution \(_{k}\)(Achiam et al., 2017a) as

\[_{ d^{_{k}},\ a}[A^{_{k}}(,a)]= _{ d^{_{k}},\ a_{k}}[)}{ _{k}(a|)}A^{_{k}}(,a)].\] (14)

(14) allows us to replace \(A^{_{k}}\) with empirical estimates at each state-action pair \((,a)\) from rollouts by the previous policy \(_{k}\). The empirical estimate of reward advantage is given by \(R(,a,^{})+ V^{_{k}}(^{})-V^{_{k} }()\). \(V^{_{k}}()\) can be computed at each augmented state by taking the discounted future return. The same can be applied to the expected advantage with respect to cost increments, with the sample estimates given by \(D_{i}(,a,^{})+V^{_{k}}_{D_{i}}(^{})-V^{_ {k}}_{D_{i}}()\). \(V^{_{k}}_{D_{i}}()\) is computed by taking the non-discounted future \(D_{i}\)-return. To proceed, we convexify (11) by approximating the objective and cost constraint via first-order expansions, and the trust region constraint via second-order expansions. Then, (11) can be efficiently solved using duality (Achiam et al., 2017a).

Infeasible constraintsAn update to \(\) is computed every time (11) is solved. However, due to approximation errors, sometimes (11) can become infeasible. In that case, we follow (Achiam et al., 2017a) to propose an recovery update that only decreases the constraint value within the trust region. In addition, approximation errors can also cause the proposed policy update (either feasible or recovery) to violate the original constraints in (11). Hence, each policy update is followed by a backtracking line search to ensure constraint satisfaction. If all these fails, we relax the search condition by also accepting decreasing expected advantage with respect to the costs, when the cost constraints are already violated. Denoting \(c_{i}_{D_{i}}(_{k})+2(H+1)^{}_{D}-w_{i}\), the above criteria can be summarized as

\[_{ d^{_{k}}}[_{KL}(\|_{k})[]] \] (15)

\[_{ d^{_{k}},a}[A^{_{k}}_{D_{i}}(,a)]-_{ d^{_{k}},a_{k}}[A^{_{k}}_ {D_{i}}(,a)](-c_{i},0).\] (16)

Note that the previous expected advantage \(_{ d^{_{k}},a_{k}}[A^{_{k}}_{D_{i}}( ,a)]\) is also estimated from rollouts by \(_{k}\) and converges to zero asymptotically, which recovers the original cost constraints in (11).

Imbalanced cost value targetsA critical step in solving (11) is to fit the cost increment value functions \(V^{_{k}}_{D_{i}}(_{t})\). By definition, \(V^{_{k}}_{D_{i}}(_{t})\) is equal to the maximum cost increment in any future state over the maximal state-wise cost so far. In other words, the true \(V^{_{k}}_{D_{i}}\) will always be zero for all \(_{t:H}\) when the maximal state-wise cost has already occurred before time \(t\). In practice, this causes the distribution of cost increment value function to be highly zero-skewed and makes the fitting very hard. To mitigate the problem, we sub-sample the zero-valued targets to match the population of non-zero values. We provide more analysis on this trick in Q3 in section 6.2.

[MISSING_PAGE_FAIL:7]

Considering different robots, constraint types, and constraint difficulty levels, we design 14 test suites with 5 types of robots and 9 types of constraints, which are summarized in Table 1 in Appendix. We name these test suites as {Robot}-{Constraint Type}-{Constraint Number}.

Comparison GroupThe methods in the comparison group include: (i) unconstrained RL algorithm TRPO (Schulman et al., 2015) (ii) end-to-end constrained safe RL algorithms CPO (Achiam et al., 2017), TRPO-Lagrangian (Bohez et al., 2019), TRPO-FAC (Ma et al., 2021), TRPO-IPO (Liu et al., 2020), PCPO (Yang et al., 2020), and (iii) hierarchical safe RL algorithms TRPO-SL (TRPO-Safety Layer) (Dalal et al., 2018), TRPO-USL (TRPO-Unrolling Safety Layer) (Zhang et al., 2022). We select TRPO as our baseline method since it is state-of-the-art and already has safety-constrained derivatives that can be tested off-the-shelf. For hierarchical safe RL algorithms, we employ a warm-up phase (\(1/3\) of the whole epochs) which does unconstrained TRPO training, and the generated data will be used to pre-train the safety critic for future epochs. For all experiments, the policy \(\), the value \((V^{},V^{}_{D})\) are all encoded in feedforward neural networks using two hidden layers of size (64,64) with tanh activations. More details are provided in Appendix D.

Evaluation MetricsFor comparison, we evaluate algorithm performance based on (i) reward performance, (ii) average episode cost and (iii) cost rate. Comparison metric details are provided in Appendix D.3. We set the limit of cost to 0 for all the safe RL algorithms since we aim to avoid any violation of the constraints. For our comparison, we implement the baseline safe RL algorithms exactly following the policy update / action correction procedure from the original papers. We emphasize that in order for the comparison to be fair, we give baseline safe RL algorithms every advantage that is given to SCPO, including equivalent trust region policy updates.

### Evaluating SCPO and Comparison Analysis

Low Dimension SystemWe select four representative test suites on low dimensional system (Point, Swimmer, Drone) and summarize the comparison results on Figure 4, which demonstrate that SCPO is successful at approximately enforcing zero constraints violation safety performance in all environments after the policy converges. Specifically, compared with the baseline safe RL methods, SCPO is able to achieve (i) near zero average episode cost and (ii) significantly lower cost rate without sacrificing reward performance. The baseline end-to-end safe RL methods (TRPO-Lagrangian, TRPO-FAC, TRPO-IPO, CPO, PCPO) fail to achieve the near zero cost performance

Figure 4: Comparison of results from four representative test suites in low dimensional systems (Point, Swimmer, and Drone).

even when the cost limit is set to be 0. The baseline hierarchical safe RL methods (TRPO-SL, TRPO-USL) also fail to achieve near zero cost performance even with an explicit safety layer to correct the unsafe action at every time step. End-to-end safe RL algorithms fail since all methods rely on CMDP to minimize the discounted cumulative cost while SCPO directly work with MMDP to restrict the state-wise maximum cost by Proposition 1. We also observe that TRPO-SL fails to lower the violation during training, due to the fact that the linear approximation of cost function \(C(_{t},a,_{t+1})\)(Dalal et al., 2018) becomes inaccurate when the dynamics are highly nonlinear like the ones we used in MuJoCo (Todorov et al., 2012). More detailed metrics for comparison and experimental results on test suites with low dimension systems are summarized in Appendix D.3.

High Dimension SystemTo demonstrate the scalability and performance of SCPO in high-dimensional systems, we conducted additional tests on the Ant-Hazard-8 and Walker-Hazard-8 suites, with 8-dimensional and 10-dimensional control spaces, respectively. The comparison results for high-dimensional systems are summarized in Figure 1, which show that SCPO outperforms all other baselines in enforcing zero safety violation without compromising performance in terms of return. SCPO rapidly stabilizes the cost return around zero and significantly reduces the cost rate, while the other baselines fail to converge to a policy with near-zero cost. The comparison results of both low dimension and high dimension systems answer **Q1**.

Maximum State-wise CostAs pointed in Section 3.3, the underlying magic for enabling near-zero safety violation is to restrict the maximum state-wise cost to stay around zero. To have a better understanding of this process, we visualize the evolution of maximum state-wise cost for SCPO on the challenging high-dimensional Ant-Hazard-8 and Walker-Hazard-8 test suites in Figure 5, which answers **Q2**.

Ablation on Sub-sampling Imbalanced Cost Increment Value TargetsAs pointed in Section 5, fitting \(V_{D_{i}}^{_{k}}(_{t})\) is a critical step towards solving SCPO, which is challenging due to zero-skewed distribution of cost increment value function. To demonstrate the necessity of sub-sampling for solving this challenge, we compare the performance of SCPO with and without sub-sampling trick on the aerial robot test suite, summarized in Figure 6. It is evident that with sub-sampling, the agent achieves higher rewards and more importantly, converges to near-zero costs.

That is because sub-sampling effectively balances the cost increment value targets and improves the fitting of \(V_{D_{i}}^{_{k}}(_{t})\). We also attempted to solve the imbalance issue via over-sampling non-zero targets, but did not observe promising results. This ablation study provides insights into **Q3**.

## 7 Conclusion and Future Work

This paper proposed SCPO, the first general-purpose policy search algorithm for state-wise constrained RL. Our approach provides guarantees for state-wise constraint satisfaction at each iteration, allows training of high-dimensional neural network policies while ensuring policy behavior, and is based on a new theoretical result on Maximum Markov Decision Process. We demonstrate SCPO's effectiveness on robot locomotion tasks, showing its significant performance improvement compared to existing methods and ability to handle state-wise constraints.

Limitation and future workOne limitation of our work is that, although SCPO satisfies state-wise constraints, the theoretical results are valid only in expectation, meaning that constraint violations are still possible during deployment. To address that, we will study absolute state-wise constraint satisfaction, i.e. bounding the _maximal possible_ state-wise cost, which is even stronger than the current result (satisfaction in expectation).

Figure 5: Maximum state-wise cost

Figure 6: SCPO sub-sampling ablation study with Drone-3DHazard-8