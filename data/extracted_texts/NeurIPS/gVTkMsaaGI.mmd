# Amortizing intractable inference in diffusion models

for vision, language, and control

 Siddarth Venkatraman*

Mila, Universite de Montreal

Moksh Jain*

Mila, Universite de Montreal

Luca Scimeca*

Mila, Universite de Montreal

Mila, Universite de Montreal

Minsu Kim*

Mila, Universite de Montreal

KAIST

&Marcin Sendera*

Mila, Universite de Montreal

Jagiellonian University

&Luca Scimeca*

Mila, Universite de Montreal

Luke Rowe

Mila, Universite de Montreal

&Sarthak Mittal

Mila, Universite de Montreal

Pablo Lemos

Mila, Universite de Montreal

Ciela Institute

Dreamfold

Emmanuel Bengio

Recursion

&Alexandre Adam

Mila, Universite de Montreal

Ciela Institute

&Jarrid Rector-Brooks

Mila, Universite de Montreal

Dreamfold

Yoshua Bengio

Mila, Universite de Montreal

CIFAR

&Glen Berseth

Mila, Universite de Montreal

CIFAR

&Nikolay Malkin

Mila, Universite de Montreal

University of Edinburgh

&mila.quebec

###### Abstract

Diffusion models have emerged as effective distribution estimators in vision, language, and reinforcement learning, but their use as priors in downstream tasks poses an intractable posterior inference problem. This paper studies _amortized_ sampling of the posterior over data, \( p^{}() p()r()\), in a model that consists of a diffusion generative model prior \(p()\) and a black-box constraint or likelihood function \(r()\). We state and prove the asymptotic correctness of a data-free learning objective, _relative trajectory balance_, for training a diffusion model that samples from this posterior, a problem that existing methods solve only approximately or in restricted cases. Relative trajectory balance arises from the generative flow network perspective on diffusion models, which allows the use of deep reinforcement learning techniques to improve mode coverage. We illustrate the broad potential of unbiased inference of arbitrary posteriors under diffusion priors across a collection of experiments: in vision (classifier guidance), language (infilling under a discrete diffusion LLM), and multimodal data (text-to-image generation). Beyond generative modeling, we apply relative trajectory balance to the problem of continuous control with a score-based behavior prior, achieving state-of-the-art results on benchmarks in offline reinforcement learning. Code is available at this link.

## 1 Introduction

Diffusion models [68; 27; 72] are a powerful class of hierarchical generative models, used to model complex distributions over images [51; 12; 63], text [4; 13; 40; 24; 23; 43], and actions in reinforcement learning [30; 83; 32] to name a few. In each of these domains, downstream problems require sampling product distributions, where a pretrained diffusion model serves as a prior \(p()\) that is multiplied by an auxiliary constraint \(r()\). For example, if \(p()\) is a prior over images defined by a diffusion model, and \(r()=p(c)\) is the likelihood that an image \(\) belongs to class \(c\), then class-conditional image generation requires sampling from the Bayesian posterior \(p( c) p()p(c)\). In offline reinforcement learning, if \((a s)\) is a conditional diffusion model over actions serving as a behavior policy, KL-constrained policy improvement [55; 44] requires sampling from the normalized product of \((a s)\) with a Boltzmann distribution defined by a \(Q\)-function, \(^{*}(a s)(a s)( Q(s,a))\). In language modeling, various conditional generation problems [43; 23; 29] amount to posterior sampling under a discrete diffusion model prior. Table 1 summarizes four such problems that the proposed method improves upon prior work.

The hierarchical nature of the generative process in diffusion models, which generate samples from \(p()\) by a deep chain of stochastic transformations, makes exact sampling from posteriors \(p()r()\) under a black-box function \(r()\) intractable. Common solutions to this problem involve inference techniques based on linear approximations [73; 33; 31; 11] or stochastic optimization [22; 48]. Others estimate the 'guidance' term - the difference in drift functions between the diffusion models sampling the prior and posterior - by training a classifier on noised data , but when such data is not available, one must resort to approximations or Monte Carlo estimates [70; 14; 10], which are challenging to scale to high-dimensional problems. Reinforcement learning methods that have recently been proposed for this problem [8; 16] are biased and prone to mode collapse (Fig. 1).

Contributions.Inspired by recent techniques in training diffusion models to sample distributions defined by unnormalized densities [89; 62; 78; 65], we propose an asymptotically unbiased training objective, called relative trajectory balance (RTB), for training diffusion models that sample from posterior distributions under a diffusion model prior (SS2.2). RTB is derived from the perspective of diffusion models as continuous generative flow networks . This perspective also allows us to freely leverage off-policy training, when data with high density under the posterior is available (SS2.3). RTB can be applied to iterative generative processes beyond standard diffusion models: our methods generalize to discrete diffusion models and extend existing methods for autoregressive language models (SS2.4).

Our experiments demonstrate the versatility of our approach in a variety of domains:

* In **vision**, we show that RTB achieves competitive classifier-guided image generation for unconditional diffusion vision priors (SS3.1) and can be used to improve caption-conditioned generation under text-to-image foundation model priors (SS3.2).
* In **language modeling**, we report strong results for infilling tasks with discrete diffusion language models (SS3.3).
* Finally, we show that RTB achieves state-of-the-art results on **continuous control** benchmarks that leverage score-based behavior priors (SS3.4).

## 2 Learning posterior samplers with diffusion priors

We consider the problem of posterior inference under a prior given by a hierarchical generative model. In this section, we present the mathematical setting (SS2.1), our proposed RTB objective (SS2.2), and training methods for RTB (SS2.3). We will first discuss the case of a diffusion prior over \(^{d}\), and later discuss how the methods generalize to arbitrary hierarchical priors (SS2.4).

  Domain & Prior \(p()\) & Constraint \(r()\) & Posterior \\  Conditional image generation (§3.1) & Image diffusion model \(p()\) & Classifier likelihood \(p(c)\) & Class-conditional distribution \(p( c)\) \\ Text-to-image generation (§3.2) & Text-to-image foundation model & RLBF reward model & Aligned text-to-image model \\ Language infilling (§3.3) & Discrete diffusion model & Autoregressive completion likelihood & Infilling distribution \\ Offline RL policy extraction (§3.4) & Diffusion model as behavior policy & Boltzmann data of \(Q\)-function & Optimal KL-constrained policy \\  

Table 1: Sources of diffusion priors and constraints.

### Background and setting: Diffusion models as hierarchical generative models

A denoising diffusion model generates data \(_{1}\) by a Markovian generative process:

\[_{0}_{ t}_{2  t}_{1}=,\] (1)

where \( t=\) and \(T\) is the number of discretization steps.1 The initial distribution \(p(_{0})\) is fixed (typically to \((,)\)) and the transition from \(_{t-1}\) to \(_{t}\) is modeled as a Gaussian perturbation with time-dependent variance:

\[p(_{t+ t}_{t})=(_{t+  t}_{t}+u_{t}(_{t}) t,_{t}^{2}  t).\] (2)

The scaling of the mean and variance by \( t\) is insubstantial for fixed \(T\), but ensures that the diffusion process is well-defined in the limit \(T\) assuming regularity conditions on \(u_{t}\)[64; 53]. The process given by (1, 2) is then identical to Euler-Maruyama integration of the stochastic differential equation (SDE) \(d_{t}=u_{t}(_{t})\ dt+_{t}\ d_{t}\).

The likelihood of a denoising trajectory \(_{0}_{ t}_{1}\) factors as

\[p(_{0},_{ t},,_{1})=p(_{0 })_{i=1}^{T}p(_{i t}_{(i-1) t})\] (3)

and defines a marginal density over the data space:

\[p(_{1})=\,p(_{0},_{ t},,_{1})\ d_{0}\ d_{ t} d_{1- t}.\] (4)

A reverse-time process, \(_{1}_{1- t}_{0}\), with densities \(q\), can be defined analogously, and similarly defines a conditional density over trajectories:

\[q(_{0},_{ t},,_{1- t} _{1})=_{i=1}^{T}q(_{(i-1) t}_{i  t}).\] (5)

In the training of diffusion models, as discussed below, the process \(q\) is typically fixed to a simple distribution (usually a discretized Ornstein-Uhlenbeck process), and the result of training is that \(p\) and \(q\) are close as distributions over trajectories.

Diffusion model training as divergence minimization.Diffusion models parametrize the drift \(u_{t}(_{t})\) in (Equation 2) as a neural network \(u(_{t},t;)\) with parameters \(\) and taking \(_{t}\) and \(t\) as input. We denote the distributions over trajectories induced by (Equation 3, Equation 4) by \(p_{}\) to show their dependence on the parameter.

In the most common setting, diffusion models are trained to maximize the likelihood of a dataset. In the notation above, this corresponds to assuming \(q(_{1})\) is fixed to an empirical measure (with the

Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see §C.

points of a training dataset \(\) assumed to be i.i.d. samples from \(q(_{1})\)). Training minimizes with respect to \(\) the divergence between the processes \(q\) and \(p_{\,}\):

\[D_{}(q(_{0},_{M},,_{ 1}) p_{\,}(_{0},_{M},,_{1}))\] (6) \[=D_{}(q(_{1}) p_{\,}(_{1}))+_{_{1}-q(_{1})}D_{}(q(_{0},_{M},,_{1- t}_{1}) p_{\, }(_{0},_{M},,_{1- t} _{1}))\] \[ D_{}(q(_{1}) p_{\,}( _{1}))=_{_{1}-q(_{1})}[- p_{ \,}(_{1})]+.\]

where the inequality - an instance of the data processing inequality for the KL divergence - shows that minimizing the divergence between distributions over trajectories is equivalent to maximizing a lower bound on the data log-likelihood under the model \(p_{\,}\).

As shown in , minimization of the KL in (Equation 6) is essentially equivalent to the traditional approach to training diffusion models via denoising score matching [80; 68; 27]. Such training exploits that for typical choices of the noising process \(q\), the optimal \(u_{t}(_{t})\) can be expressed in terms of the Stein score of \(q(_{1})\) convolved with a Gaussian, allowing an efficient stochastic regression objective for \(u_{t}\). For full generality of our exposition for arbitrary iterative generative processes, we prefer to think of (Equation 6) as the primal objective and denoising score matching as an efficient means of minimizing it.

Trajectory balance and distribution-matching training.From (Equation 6) we also see that the bound is tight if the conditionals of \(p_{\,}\) and \(q\) on \(_{1}\) coincide, _i.e._, \(q\) is equal to the posterior distribution of \(p_{\,}\) conditioned on \(_{1}\). Indeed, the model \(p_{\,}\) minimizes (Equation 6) for a distribution with continuous density \(q(_{1})\) if and only if, for all denoising trajectories,

\[p_{\,}(_{0},_{ t},,_{1})=q( _{1})q(_{0},_{M},,_{1- t} _{1}).\] (7)

This was named the _trajectory balance (TB) constraint_ by  - by analogy with a constraint for discrete-space iterative sampling  - and is a time-discretized version of a constraint used for enforcing equality of continuous-_time_ path space measures in  (see  for asymptotic analysis).

In [61; 38], the constraint (7) was used for the training of diffusion models in a _data-free_ setting, where instead of i.i.d. samples from \(q(_{1})\) one has access to a (possibly unnormalized) density \(q(_{1})=e^{-(_{1})}/Z\) from which one wishes to sample. These objectives minimize the squared log-ratio between the two sides of (7), which allows the trajectories \(_{0}_{ t} _{1}\) used for training to be sampled from any training distribution, such as 'exploratory' modifications of \(p_{\,}\) or trajectories found by local search (MCMC) in the target space. The flexibility of off-policy exploration that this allows was studied by . Such objectives contrast with on-policy, simulation-based approaches that require differentiating through the sampling process [_e.g._, 89; 78; 6; 79].

### Intractable inference under diffusion priors

Consider a diffusion model \(p_{\,}\), defining a marginal density \(p_{\,}(_{1})\), and a positive constraint function \(r:^{d}_{>0}\). We are interested in training a diffusion model \(p_{\,}^{}\), with drift function \(u_{\,}^{}\), that would sample the product distribution \(p^{}(_{1}) p_{\,}(_{1})r(_{1})\). If \(r(_{1})=p(\,_{1})\) is a conditional distribution over another variable \(\), then \(p^{}\) is the Bayesian posterior \(p_{\,}(_{1})\).

Because samples from \(p^{}(_{1})\) are not assumed to be available, one cannot directly train \(p\) using the objective (6). Nor can one directly apply objectives for distribution-matching training, such as those that enforce (7), since the marginal \(p_{\,}(_{1})\) is not available. However, we make the following observation (proof in SSA).

**Proposition 1** (Relative TB constraint).: _If \(p_{\,}\), \(p_{\,}^{}\), and the scalar \(Z_{\,}\) jointly satisfy the relative trajectory balance (RTB) constraint_

\[Z_{\,} p_{\,}^{}(_{0},_{ t}, ,_{1})=r(_{1})p_{\,}(_{0},_ { t},,_{1})\] (8)

_for every denoising trajectory \(_{0}_{ t} _{1}\), then \(p_{\,}^{}(_{1}) p_{\,}(_{1})r( _{1})\), i.e., the diffusion model \(p_{\,}^{}\) samples the posterior distribution. Furthermore, if \(p_{\,}\) also satisfies the TB constraint (7) with respect to the noising process \(q\) and some target density \(q(_{1})\), then \(p_{\,}^{}\) satisfies the TB constraint with respect to the target density \(q^{}(_{1}) q(_{1})r(_{1})\), and \(Z= q(_{1})r(_{1})\,d_{1}\)._

Note that the two joints appearing in (8) are defined as products over transitions, via (3).

Relative trajectory balance as a loss.Analogously to the conversion of the TB constraint (7) into a trajectory-dependent training objective in [46; 38], we define the _relative trajectory balance loss_ as the discrepancy between the two sides of (8), seen as a function of the vector \(\) that parametrizes the posterior diffusion model and the scalar \(Z_{}\) (parametrized via \( Z_{}\) for numerical stability):

\[_{}(_{0}_{ t} _{1};):=( p_{}^{}( _{0},_{ t},,_{1})}{r(_{1}) p_{}(_{0},_{ t},,_{1})})^{2}.\] (9)

Optimizing this objective to \(0\) for all trajectories ensures that (8) is satisfied. While the RTB constraint (8) has a similar form to TB (7), RTB involves the ratio of two denoising processes, while TB involves the ratio of a forward and a backward process. However, the name'relative TB' is justified by interpreting the densities in a TB constraint relative to a measure defined by the prior model; see SS2.4.

If we assume \(p_{}(_{0})=p_{}^{}(_{0})\) are fixed (_e.g._, to a standard normal), then (9) reduces to

\[(}{r(_{1})}+_{i=1}^{T}^ {}(_{i t}_{(i-1) t})}{p_{ }(_{i t}_{(i-1) t})})^{2}.\] (10)

Notably, the gradient of this objective with respect to \(\) does not require differentiation (backpropagation) into the sampling process that produced a trajectory \(_{0}_{1}\). This offers two advantages over on-policy simulation-based methods: (1) the ability to optimize \(_{}\) as an off-policy objective, _i.e._, sampling trajectories for training from a distribution different from \(p_{}^{}\) itself, as discussed further in SS2.3; (2) backpropagating only to a subset of the summands in (10), when computing and storing gradients for all steps in the trajectory is prohibitive for large diffusion models (see SSH.1).

Comparison with classifier guidance.It is interesting to contrast the RTB training objective with the technique of _classifier guidance_ used for some problems of the same form. If \(r(_{1})=p(_{1})\) is a conditional likelihood, classifier guidance relies upon writing \(u_{t}(_{t})-u_{t}^{}(_{t})\) explicitly in terms of \(_{_{t}} p(_{t})\), by combining the expression of the optimal drift \(u_{t}\) in terms of the score of the target distribution convolved with a Gaussian (cf. SS2.1), with the 'Bayes' rule' for the Stein score: \(_{_{t}} p(_{t})=_{_{t}} p(_{t})+_{_{t}} p( _{t})\).

Classifier guidance gives the _exact_ solution for the posterior drift when a differentiable classifier on noisy data, \(p(_{t})= p(_{1})p( _{1}_{t})\,d_{1}\), is available. Unfortunately, such a classifier is not, in general, tractable to derive from the classifier on noiseless data, \(p(_{1})\), and cannot be learned without access to unbiased data samples. RTB is an asymptotically unbiased objective that recovers the difference in drifts (and thus the gradient of the log-convolved likelihood) in a data-free manner.

### Training, parametrization, and conditioning

Training and exploration.The choice of which trajectories we use to take gradient steps with the RTB loss can have a large impact on sample efficiency. In _on-policy_ training, we use the current policy \(p_{}^{}\) to generate trajectories \(=(_{0}_{1})\), evaluate the reward \( r(_{1})\) and the likelihood of \(\) under \(p_{}\), and a gradient updates on \(\) to minimize \(_{}(;)\).

However, on-policy training may be insufficient to discover the modes of the posterior distribution. In this case, we can perform _off-policy_ exploration to ensure mode coverage. For instance, given samples \(_{1}\) that have high density under the target distribution, we can sample _noising_ trajectories \(_{1}_{1- t} _{0}\) starting from these samples and use such trajectories for training. Another effective off-policy training technique uses replay buffers. We expect the flexibility of mixing on-policy training with off-policy exploration to be a strength of RTB over on-policy RL methods, as was shown for distribution-matching training of diffusion models in .

Conditional constraints and amortization.Above we derived and proved the correctness of the RTB objective for an arbitrary positive constraint \(r(_{1})\). If the constraints depend on other variables \(\) - for example, \(r(_{1};)=p(_{1})\) - then the posterior drift \(u_{}^{}\) can be conditioned on \(\) and the learned scalar \( Z_{}\) replaced by a model taking \(\) as input. Such conditioning achieves amortized inference and allows generalization to new \(\) not seen in training. Similarly, all of the preceding discussion easily generalizes to _priors_ that are conditioned on some context variable.

Efficient parametrization and Langevin inductive bias.Because the deep features learned by the prior model \(u_{}\) are expected to be useful in expressing the posterior drift \(u_{}^{}\), we can choose to initialize \(u_{}^{}\) as a copy of \(u_{}\) and to fine-tune it, possibly in a parameter-efficient way (as described in each section of SS3). This choice is inspired by the method of amortizing inference in large language models by fine-tuning a prior model to sample an intractable posterior .

Furthermore, if the constraint \(r(_{1})\) is differentiable, we can impose an inductive bias on the posterior drift similar to the one introduced for diffusion samplers of unnormalized target densities in  and shown to be useful for off-policy methods in . namely, we write

\[u_{}^{}(_{t},t)=_{1}(_{t},t;) +_{2}(_{t},t,)_{_{t}} r(_ {t}),\] (11)

where \(_{1}\) and \(_{2}\) are neural networks outputting a vector and a scalar, respectively. This parametrization allows the constraint to provide a signal to guide the sampler at intermediate steps.

Stabilizing the loss.We propose two simple design choices for stabilizing RTB training. First, the loss in (9) can be replaced by the empirical _variance_ over a minibatch of the quantity inside the square, which removes dependence on \( Z_{}\) and is especially useful in conditional settings, consistent with the findings of . This amounts to a relative variant of the VarGrad objective  (see (23) in SSG). Second, we employ loss clipping: to reduce sensitivity to an imperfectly fit prior model, we do not perform updates on trajectories where the loss is close to 0 (see SSE,SSF).

### Generative flow networks and extension to other hierarchical processes

RTB as TB under the prior measure.The theoretical foundations for continuous generative flow networks  establish the correctness of enforcing constraints such as trajectory balance (7) for training sequential samplers, such as diffusion models, to match unnormalized target densities. While we have considered Gaussian transitions and identified transition kernels with their densities with respect to the Lebesgue measure over \(^{d}\), these foundations generalize to more general _reference measures_. In SSB, we show how the RTB constraint can be recovered as a special case of the TB constraint for a certain choice of reference measure derived from the prior.

Extension to arbitrary sequential generation.While our discussion was focused on diffusion models for continuous spaces, the RTB objective can be applied to any Markovian sequential generative process, in particular, one that can be formulated as a generative flow network in the sense of [5; 38]. This includes, in particular, generative models that generate objects by a sequence of discrete steps, including autoregressive models and discrete diffusion models. In the case of discrete diffusion, where the intermediate latent variables \(_{t}\) lie not in \(^{d}\) but in the space of sequences, one simply replaces the Gaussian transition densities by transition probability _masses_ in the RTB constraint (8) and objective (9). In the case of autoregressive models, where only one sequence of steps can generate any given object, the backward process \(q\) becomes trivial, and the RTB constraint for a model \(p_{}^{}\) to sample a sequence \(\) from a distribution with density \(r()p_{}()\) is simply \(Z_{}p_{}^{}()=r()p_{}()\) for all sequences \(\). We note that a sub-trajectory generalization of this objective was used in  to amortize intractable inference in autoregressive language models.

## 3 Experiments

In this section, we present empirical results to validate the efficacy of relative trajectory balance. Our experiments are designed to demonstrate the wide applicability of RTB to sample from posteriors for diffusion priors with arbitrary rewards on vision, language, and continuous control tasks.

### Class-conditional posterior sampling from unconditional diffusion priors

We evaluate RTB in a classifier-guided visual task where we wish to learn a diffusion posterior \(p_{}^{}( c) p_{}()p(c )\) given a pretrained diffusion prior \(p_{}()\) and a classifier \(r()=p(c)\).

Setup.We consider two 10-class image datasets, MNIST and CIFAR-10, using off-the-shelf unconditional diffusion priors from  and standard classifiers \(p(c)\) for both datasets. We perform parameter-efficient fine-tuning of \(p_{}^{}\), initialized as a copy of the prior \(p_{}\), using the RTB objective (see SSE.1 for details). The RTB objective is optimized on trajectories sampled

[MISSING_PAGE_FAIL:7]

images. Following DPOK , we use ImageReward , which has been trained to match human preferences as well as prompt accuracy to attributes such as the number of objects, color, and compositionality, as the reward \( r(_{1},)\). As reference, we present comparisons against DPOK with the default KL regularization \(=0.01\) and DPOK with \(=0.0\), which is equivalent to DDPO . We measure the final average reward and the diversity of the generated image, as measured by the average pairwise cosine distance between CLIP embeddings  of a batch of generated images. Further details about the experimental setup and ablations are discussed in SSH.

Results.Fig. 3 plots the diversity versus log reward on a set of prompts from [16; 87]. In terms of average \( r(_{1},)\), RTB either matches or outperforms DPOK, while generally achieving lower reward than DDPO. The CLIP diversity score for RTB and DPOK are on average higher than DDPO, which is expected since it does not use KL regularization. For qualitative image assessments, refer to Fig. 4 and SSH.2. Through this experiment, we show that RTB scales well to high dimensional, multimodal data, matching state-of-the-art methods for fine-tuning text-to-image diffusion models.

### Text infilling with discrete diffusion language models

To evaluate our approach on discrete diffusion models, we consider the problem of text infilling , which involves filling in missing tokens given some context tokens. While discrete diffusion models - unlike their continuous counterparts - can be challenging to train [49; 4; 9; 74], _score entropy discrete diffusion_[SEDD; 43] matches the language modeling performance of autoregressive language models of similar scale. Non-autoregressive generation in diffusion language models can provide useful inductive biases for infilling, such as the ability to attend to context on both sides of a target token.

Setup.We use the ROCStories corpus , a dataset of short stories containing 5 sentences each. We adopt the task setup from , where the first 3 sentences of a story \(\) and the last sentence \(\) are given, and the goal is to generate the fourth sentence \(\) such that the overall story is coherent and consistent. The fourth sentence can involve a turning point in the story and is thus challenging to fill in. We aim to model the posterior \(p^{}(,) p( )p_{}(,)\) where \(p\) is a SEDD language model prior (a conditional model over \(\) given \(\)) and \(p_{}\) is an autoregressive language model fine-tuned with a maximum likelihood objective on a held-out subset of the dataset. As baselines, we consider simply prompting the diffusion language model with \(\) (Prompt \(()\)) and \(,\) (Prompt \((,)\)). Additionally, to contextualize the performance, we also consider autoregressive language model baselines from , which studied this problem under an autoregressive prior \(p()\). SFT is trained on \(50,000\) examples compared to \(1000\) for RTB, and serves as an upper bound on the performance in this task. See SSF for further details about the experimental setup.

Figure 4: Images generated from prior (top row), DPOK (middle row) and RTB (bottom row) for 4 different prompts. Images in the same column share the random DDIM seed. More images in §H.2.

Figure 3: Fine-tuning Stable Diffusion with ImageReward. We report mean \( r(_{1},)\) and diversity, measured as the mean cosine distance between CLIP embeddings for a batch of 100 generated images.2

Results.Following , we use three standard metrics to measure the similarity of the generated infills with the reference infills from the dataset: BERTScore  (with DEBRTa ), BLEU-4 , and GLEU-4 . Table 3 summarizes the results. We observe that the diffusion language model performs significantly better than the autoregressive language model without any fine-tuning. RTB further improves the performance over prompting, and even outperforms the strongest autoregressive baseline of GFlowNet fine-tuning. We provide some examples of generated text in SSF.

### KL-constrained policy search in offline reinforcement learning

The goal of RL algorithms is to learn a policy \((a s)\), _i.e._, a mapping from states \(s\) to actions \(a\) in an environment, that maximizes the expected cumulative discounted reward . In the offline RL setting , the agent has access to a dataset \(=\{(s_{t}^{i},a_{t}^{i},s_{t+1}^{i},r_{t}^{i})\}_{i=1}^{N}\) of transitions (where each sample \((s_{t},a_{t},s_{t+1},r_{t})\) indicates that an agent taking action \(a_{t}\) at state \(s_{t}\) transitioned to the next state \(s_{t+1}\) and received reward \(r_{t}\)). This dataset is assumed to be generated by a _behavior policy_\((a s)\), which may be a diffusion model trained on \(\). Offline RL algorithms must learn a new policy \(\) which achieves high return using only this dataset without interacting with the environment.

An important problem in offline RL is policy extraction from trained \(Q\)-functions [55; 25; 44]. For reliable extrapolation, one wants the policy to predict actions that have high \(Q\)-values, but also have high density under the behavior policy \(\), as naive maximization can result in choosing actions with low probability under \(\) and thus unreliable predictions from the \(Q\)-function. This is formulated as a KL-constrained policy search problem:

\[*{argmax}_{}_{s-d_{},a(a s)}[Q( s,a)],_{s-d_{}}[D_{}((a s) (a s))],\] (13)

where \(d_{}\) is the distribution over states induced by following the policy \(\). The optimal policy \(\) in (13) is the product distribution \(^{*}(a s)(a s)( Q(s,a))\) for some inverse temperature \(\). If \((a s)\) is a conditional diffusion model over continuous actions \(a\) conditioned on state \(s\), we use RTB to fine-tune a diffusion behavior policy to sample from \(^{*}\), using \(\) as the prior and \(( Q(s,a))\) as the target constraint. We use a \(Q\)-function trained using IQL .

Setup.We test on continuous control tasks in the D4RL suite , which consists of offline datasets collected using a mixture of SAC policies of varying performance. We evaluate on the halfcheetah, hopper and walker2d MuJoCo  locomotion tasks, each of which contains three datasets of transitions: "medium" (collected from an early-stopped policy), "medium-expert" (collected from both an expert and an early-stopped policy) and "medium-replay" (transitions stored in the replay buffer prior to early stopping). We compare against standard offline RL baselines (Behavior Cloning (BC), CQL , and IQL ) and diffusion-based offline RL methods which are currently state-of-the-art: Diffuser [D; 30], Decision Diffuser [DD; 2], D-QL , IDQL , and QGPO . For algorithm implementation details, hyperparameters, and a report of baselines, see SSG.

Results.Table 4 shows that RTB matches state-of-the-art results across the D4RL tasks. In particular, RTB performs strongly in the medium-replay tasks, which contain the most suboptimal data and consequently the poorest behavior prior. We highlight that our performance is similar to QGPO , which learns intermediate energy densities for diffusion posterior sampling.

## 4 Other related work

Composing iterative generative processes.Beyond the approximate posterior sampling algorithms and application-specific techniques discussed in SS1 and SS3, several recent works have explored the use of hierarchical models, such as diffusion models, as modular components in generative processes. Diffusion models can be used to sample product distributions to induce compositional structure in images [41; 15]. Amortized Bayesian inference [35; 60; 59; 20] is another domain of sampling from product distributions where diffusion models are now being used . Beyond product models, 

   Model & Algorithm 1 Metric \(\) & BLEU-4 & BLEU-4 & BLEU-4 & BERTScore \\   & Proempting & 0.010+0.06 & 0.022+0.001 & 0.005+0.001 \\  & Supervised fine-tuning & 0.012+0.02 & 0.023+0.001 & 0.013+0.001 \\  & GSI fine-tuning  & 0.019+0.001 & 0.034+0.002 & 0.022+0.001 \\   & Proempt (\(\)) & 0.011+0.002 & 0.023+0.002 & 0.014+0.003 \\  & Promote (\(\),\(\)) & 0.014+0.00 & 0.027+0.003 & 0.002+0.004 \\   & **RTB (ours)** & 0.025+0.002 & 0.045+0.002 & 0.156+0.003 \\   & SFT (upper bound) & 0.031\(\)0.002 & 0.057\(\)0.004 & 0.182+0.005 \\   

Table 3: Results on the story infilling task with autoregressive and discrete diffusion language models. Metrics are computed with respect to reference infills from the dataset. All metrics are mean\({}_{}\) over 5 samples for each of the 100 test examples. RTB with discrete diffusion prior performs better than best baseline with autoregressive prior.

studies ways to amortize other kinds of compositions of hierarchical processes, including diffusion models, while  proposes methods to sample the product of many iterative processes in application to federated learning. Finally, models without hierarchical structure, such as normalizing flows, have been used to amortize intractable inference in pretrained diffusion models [_e.g._, 17]. In contrast, our method performs posterior inference by _fine-tuning_ a prior model, developing a direction on flexible extraction of information from large pretrained models .

Diffusion samplers.Several prior works seek to amortize MCMC sampling from unnormalized densities by training diffusion models for efficient mode-mixing . Our work is most closely related to continuous GFlowNets , which offer an alternative perspective on training diffusion samplers using off-policy flow consistency objectives . Recently, Berner et al.  have shown connections among existing families of diffusion sampling algorithms and analyzed their continuous-time limits.

## 5 Conclusions and future work

Relative trajectory balance provides a new approach to training diffusion models to generate unbiased posterior samples given a diffusion prior and an arbitrary reward function. Through experiments on a variety of domains - vision, language, continuous control - we demonstrated the flexibility and general applicability of RTB. RTB can be optimized with off-policy trajectories, and future work can explore ways to leverage off-policy training, using techniques such as local search  to improve sample efficiency and mode coverage. Simulation-based objectives in the style of  are also applicable to the amortized sampling problems we consider and should be explored, as should simulation-free extensions, _e.g._, through objectives that are local in time . The ability to handle arbitrary black-box likelihoods also makes RTB a useful candidate for inverse problems in domains such as 3D object synthesis with likelihood computed via a renderer [_e.g._, 56, 82], imaging problems in astronomy [_e.g._, 1], medical imaging [_e.g._, 73], and molecular structure prediction [_e.g._, 84].

Moreover, RTB could facilitate a breakthrough in modeling molecular dynamics--a notoriously challenging task due to the need to sample rare-event trajectories in chemical simulations--by converting these problems into posterior inference over amplified distributions of rare-event samples. Notably, Seong et al.  have already explored a preliminary version of this concept by employing TB with a reward multiplied by the prior likelihood, which is effectively equivalent to RTB.

Limitations.RTB learns the posterior through simulation-based training, which can be slow and memory-intensive. Additionally, the RTB objective is computed on complete trajectories without any local credit-assignment signal, which can result in high variance in the gradients. Guarantees on the error incurred by imperfect fit of the prior model, amortization, and time discretization (analogous to 's analysis for diffusion samplers) have not been obtained and should be considered in future work.

Broader impact.While our contributions focus on an algorithmic approach for learning posterior samplers with diffusion priors, we acknowledge that like other advances in generative modelling, our approach can potentially be used by nefarious actors to train generative models to produce harmful content and misinformation. At the same time, our approach can be also be used to mitigate biases captured in pretrained models and applied to various scientific problems.

   Task \(\) Algorithm \(\) & BC & CQL & IQL & D & DD & D-QL & IDQL & QGPO & **RTB (ours)** \\  halfcheetah-medium-expert & 55.2 & 91.6 & 86.7 & 79.8 & 90.6\({}_{1.3}\) & 96.1\({}_{0.3}\) & 95.9 & 93.5\({}_{0.3}\) & 74.93\({}_{1.72}\) \\ hopper-medium-expert & 52.5 & 105.4 & 91.5 & 107.2 & 111.8\({}_{1.8}\) & 110.7\({}_{1.3}\) & 108.6 & 108.0\({}_{2.5}\) & 96.11\({}_{1.53}\) \\ walker2d-medium-expert & 107.5 & 108.8 & 109.6 & 108.4 & 108.8\({}_{1.7}\) & 109.7\({}_{0.3}\) & 112.7 & 110.7\({}_{0.6}\) & 109.52\({}_{0.11}\) \\  halfcheetah-medium & 42.6 & 44.0 & 47.4 & 44.2 & 49.1\({}_{1.1}\) & 50.6\({}_{0.5}\) & 51.0 & 54.1\({}_{1.0}\) & 53.70\({}_{0.38}\) \\ hopper-medium & 52.9 & 58.5 & 66.3 & 58.5 & 79.3\({}_{3.6}\) & 82.4\({}_{4.6}\) & 65.4 & 98.0\({}_{0.2}\) & 82.76\({}_{3.7}\) \\ walker2d-medium & 75.3 & 72.5 & 78.3 & 79.7 & 82.5\({}_{1.4}\) & 85.1\({}_{0.9}\) & 82.5 & 86.0\({}_{0.7}\) & 87.29\({}_{3.15}\) \\  halfcheetah-medium-replay & 36.6 & 45.5 & 44.2 & 42.2 & 39.3\({}_{4.1}\) & 47.5\({}_{0.3}\) & 45.8 & 47.6\({}_{1.4}\) & 48.11\({}_{0.56}\) \\ hopper-medium-replay & 18.1 & 95.0 & 94.7 & 96.8 & 100.0\({}_{0.7}\) & 100.7\({}_{0.6}\) & 92.1 & 96.92\({}_{0.2}\) & 100.40\({}_{0.21}\) \\ walker2d-medium-replay & 26.0 & 77.2 & 73.9 & 61.2 & 75.0\({}_{4.3}\) & 39.4\({}_{3.15}\) & 85.1 & 84.4\({}_{4.1}\) & 93.57\({}_{2.63}\) \\   

Table 4: Average rewards of trained policies on D4RL locomotion tasks (mean\({}_{}\) over 5 random seeds). Following past work, numbers within 5% of maximum in every row are highlighted.