# An Information Criterion for Controlled Disentanglement of Multimodal Data

Chenyu Wang\({}^{*}{}^{1,2}\), Sharut Gupta\({}^{*}{}^{1}\), Xinyi Zhang\({}^{1,2}\), Sana Tonekaboni\({}^{2}\),

&Stefanie Jegelka\({}^{1,3}\), Tommi Jaakkola\({}^{1}\), Caroline Uhler\({}^{1,2}\)

\({}^{1}\)MIT \({}^{2}\)Broad Institute of MIT and Harvard \({}^{3}\)TU Munich

Equal contribution

###### Abstract

Multimodal representation learning seeks to relate and decompose information available in multiple modalities. By disentangling modality-specific information from information that is shared across modalities, we can improve interpretability and robustness and enable tasks like counterfactual generation. However, separating these components is challenging due to their deep entanglement in real-world data. We propose **Disentangled**Sclf-**S**upervised **L**earning (DisentangledSSL), a novel self-supervised approach that effectively learns disentangled representations, even when the so-called _Minimum Necessary Information_ (MNI) point is not achievable. It outperforms baselines on multiple synthetic and real-world datasets, excelling in downstream tasks, including prediction tasks for vision-language data, and molecule-phenotype retrieval for biological data.

## 1 Introduction

Multimodal representation learning integrates information from different modalities to form holistic representations, with applications ranging from vision-language tasks  to biological data analysis . Models like CLIP  use self-supervised learning to capture shared information, assuming that mutual information across modalities is key for downstream tasks . However, the modality gap--stemming from inherent differences in representation and content--causes misalignment and limits their effectiveness in real-world scenarios . This highlights the need for a _disentangled representation space_ that captures both shared and modality-specific information effectively. Such a space, accounting for both _coverage_ and _disentanglement_ of the information, is crucial for interpretability and tasks where distinct modalities provide essential insights .

Disentangled representation learning in multimodal data began with works on Variational Autoencoders and Generative Adversarial Networks  aiming to isolate data variations. Recent self-supervised methods have advanced this by learning shared and modality-specific information either sequentially or jointly . Zhang et al.  emphasized the importance of learning disentangled representations for multimodal data in biological contexts. However, rigorous guarantees for disentanglement are lacking, especially when the _Minimum Necessary Information_ (MNI)  point is unattainable. In many real-world applications, shared and modality-specific information are deeply entangled, i.e. the shared and modality-specific components are intertwined and result in similar observations, leading to unattainable MNI. Figure 1 illustrates an example in the context of high-content drug screens. We provide additional examples of unattainable MNI in Appendix B.

In this work, we propose DisentangledSSL, a self-supervised approach for multimodal data that effectively separates shared and modality-specific information. Building on information theory, we devise a step-by-step optimization strategy to learn these representations and maximize the variationallower bound on our objectives. Unlike existing works, we tackle challenging cases where MNI is unattainable and provide a formal analysis of representation optimality. Empirical results show that DisentangledSSL outperforms baselines on various datasets, including multimodal prediction in MultiBench  and molecule-phenotype retrieval in high-content drug screening datasets[32; 3].

## 2 Method

In this section, we detail our proposed method, DisentangledSSL, for learning disentangled representations in multimodal data. We begin by outlining the graphical model that formalizes the problem in Section 2.1 and defining the key properties of "desirable" representations in Section 2.2. Subsequently, we describe the DisentangledSSL framework in Section 2.3. We provide theoretical proofs of the optimality in Appendix E, and specific training objectives in Appendix H.

### Multimodal Representation Learning with Disentangled Latent Space

DisentangledSSL learns disentangled representations in latent space, separating modality-specific information from shared factors across paired observations (\(X^{1}\), \(X^{2}\)). This generative process is modeled in Figure 2. Each observation is generated from two distinct latent representations: the modality-specific representations (\(Z_{s}^{1}\) and \(_{s}^{2}\)) that contain information exclusive to their respective modalities, and a shared representation (\(Z_{c}\)) that contains information common to both modalities. We refer to these as the true latents.

DisentangledSSL infers the shared representation from both modalities independently, i.e. \(_{c}^{1} p(|X^{1})\) and \(_{c}^{2} p(|X^{2})\). The modality-specific information for each modality is encoded by variables \(_{s}^{1}\) and \(_{s}^{2}\). Note that for the true latents, \(Z_{s}^{1}\) and \(Z_{c}\) are conditionally dependent on \(X^{1}\) due to the V-structure in the graphical model. To preserve such dependencies in the inferred latents, \(_{s}^{1}\) and \(_{s}^{2}\) are conditioned on both the respective observations and the inferred shared representations, with \(_{s}^{1} p(|X^{1},_{c}^{1})\) and \(_{s}^{2} p(|X^{2},_{c}^{2})\).

### Information Criteria for the Optimal Inferred Representations

We establish information-theoretic criteria to ensure the shared and modality-specific representations are informative and disentangled, capturing key features while minimizing redundancy.

Figure 1: Post-perturbation phenotype (\(X_{1}\)) (i.e., cellular images or gene expression after the application of a drug to cells) and molecular structure (\(X_{2}\)) of an underlying drug perturbation system where cancer cells are targeted and killed while healthy cells remain unaffected. The Venn diagram illustrates shared and specific information between modalities \(X_{1}\) and \(X_{2}\): shared content is shown in red, modality-specific content in green, and entangled content due to unattainable MNI in orange. For example, for the drug mechanism, the molecular structure conveys full information, while the phenotype provides partial information (i.e. mechanisms causing cell death). Similarly, for the states of healthy cells, the phenotype specifies their cell states, whereas the molecular structure only indicates that the cells are unaffected without detailing their specific states.

Figure 2: Graphical model.

#### 2.2.1 Information Bottleneck Principle and Minimum Necessary Information

The shared representations \(_{c}^{1}\) and \(_{c}^{2}\) should provide a compact yet expressive form of shared information. This trade-off between compression and expressivity is studied by the principles of the information bottleneck (IB) in both supervised and self-supervised settings . IB objective has been utilized to learn the optimal representations \(Z^{1}\) of \(X^{1}\) with respect to \(X^{2}\) under the Markov chain \(Z^{1} X^{1} X^{2}\). It balances the trade-off between preserving relevant information about \(X^{2}\), i.e. \(I(Z^{1};X^{2})\), and compressing the representation, i.e. \(I(Z^{1};X^{1})\) (see more details in Appendix C). The optimal representation should be both **sufficient**, i.e. \(I(Z^{1};X^{2})=I(X^{1};X^{2})\), and **minimal**. Based on these criteria, \(Z^{1}\) is said to capture **Minimum Necessary Information (MNI)** between \(X^{1}\) and \(X^{2}\) if \(I(X^{1};X^{2})=I(Z^{1};X^{2})=I(Z^{1};X^{1})\), indicating an ideal scenario of full disentanglement between \(X^{1}\) and \(X^{2}\) with no extraneous information, i.e. \(I(Z^{1};X^{1}|X^{2})=0\)2.. In general, it may be unattainable for an arbitrary distribution \(p(X^{1},X^{2})\) (see Appendix F). Despite its significance, the optimality of latent representations when MNI is unattainable is often overlooked in prior work.

#### 2.2.2 Optimal Shared Representations: MNI attainable or not

We propose a definition of the optimality of the shared representations that applies to both scenarios - when MNI is attainable or not, as defined in Equation 1:

\[_{c}^{1*}&=*{arg \,min}_{Z^{1}}I(Z^{1};X^{1}|X^{2}),\;\;I(X^{1};X^{2})-I(Z^{1};X^{2}) _{c}\\ _{c}^{2*}&=*{arg\,min}_{Z^{2}}I (Z^{2};X^{2}|X^{1}),\;\;I(X^{1};X^{2})-I(Z^{2};X^{1})_{c} \] (1)

Formally, minimizing conditional mutual information, \(I(Z^{1};X^{1}|X^{2})\), ensures the shared representation captures only the truly common information between \(X^{1}\) and \(X^{2}\), excluding modality-specific details unique to \(X^{1}\). Compared with \(I(Z^{1};X^{1})\) in IB, it provides a more precise measure of compression and a more robust objective.

The constraint \(I(X^{1};X^{2})-I(Z^{1};X^{2})_{c}\) ensures that \(_{c}^{1*}\) retains a substantial portion of the shared information between \(X^{1}\) and \(X^{2}\), controlling the difference within the limit \(_{c}\) and preventing significant information loss. We utilize the **IB curve**\(F()\)3, representing the maximum \(I(Z^{1};X^{2})\) for a given compression level \(I(X^{1};Z^{1})\), to illustrate the optimality in Figure 3. MNI is depicted as point A, and \(_{c}^{1*}\) corresponding to \(_{c}\) is shown as point C. When MNI is attainable, setting \(_{c}=0\) achieves MNI. In contrast, Achille and Soatto  formulated the optimization as \(Z^{1}=*{arg\,min}_{Z^{1}:Z^{1}X^{1}X^{2}}I(X^{1} ;Z^{1})\),_s.t._\(I(Z^{1};X^{2})=I(X^{1};X^{2})\), leading to MNI when attainable. This holds in supervised settings, assuming the data label \(X^{2}\) is a deterministic function of \(X^{1}\), as used by previous methods . However, in general multimodal self-supervised scenarios where MNI is not attainable, this results in point B in Figure 3, which includes information of \(X^{1}\) that has little relevance to \(X^{2}\) to satisfy the equality constraint, causing a gap between the objective and the ideal representation.

#### 2.2.3 Optimal Specific Representations: Ensuring Coverage and Disentanglement

The modality-specific representations \(Z_{s}^{1}\) and \(Z_{s}^{2}\) should capture information unique to each modality, being highly informative yet minimally redundant with the shared representations. The formal definition of these optimal representations is provided in Equation 2.

\[ Z_{s}^{1}&=*{arg\,max}_{Z ^{1}}I(Z^{1};X^{1}|X^{2}),\;\;I(Z^{1};_{c}^{1*})_{ s}\\ Z_{s}^{2*}&=*{arg\,max}_{Z^{2}}I(Z^{2} ;X^{2}|X^{1}),\;\;I(Z^{2};_{c}^{2*})_{s}\] (2)

Figure 3: IB CurveTo extract modality-specific information, \(_{c}^{1*}\) maximizes the conditional mutual information \(I(Z^{1};X^{1}|X^{2})\), capturing details unique to \(X^{1}\) without redundancy from \(X^{2}\). This same term is minimized for optimal shared representations in Equation 1. Integrating both representations together, the objectives ensure comprehensive coverage of all relevant information from each modality. Meanwhile, the constraint \(I(Z^{1};_{c}^{1*})_{s}\) limits redundancy between the modality-specific representation and the shared representation \(_{c}^{1*}\), ensuring disentanglement by separating unique aspects of \(X^{1}\) from shared components with \(X^{2}\). The parameter \(_{s}\) controls the trade-off between information coverage and disentanglement.

### DisentangledSSL: a Step-by-Step Optimization Algorithm

To achieve the optimal representations discussed in Section 2.2, we introduce a two-step training procedure. The first step focuses on optimizing for the shared latent representation to captures the minimum necessary information as close as possible. Then, the second step utilizes the learned shared representations in step 1 to facilitate the learning of modality-specific representations. This sequential approach is formalized in the optimization objectives given in Equations 3 and 4.

**Step 1:** Learn the shared latent representations by encouraging the shared representation encoded from one modality to be highly informative about the other modality, while minimizing redundancy.

\[_{c}^{1*}&=*{ arg\,max}_{Z^{1}}L_{c}^{1}=*{arg\,max}_{Z^{1}}I(Z^{1};X^{2})-  I(Z^{1};X^{1}|X^{2})\\ _{c}^{2*}&=*{arg\,max}_{Z^{2}}L_ {c}^{2}=*{arg\,max}_{Z^{2}}I(Z^{2};X^{1})- I(Z^{2};X^{2 }|X^{1})\] (3)

**Step 2:** Learn the modality-specific latent representations based on the learned shared representations.

\[_{s}^{1*}&=*{ arg\,max}_{Z^{1}}L_{s}^{1}=*{arg\,max}_{Z^{1}}I(Z^{1},_{c}^{2*} ;X^{1})- I(Z^{1};_{c}^{1*})\\ _{s}^{2*}&=*{arg\,max}_{Z^{2}}L_ {s}^{2}=*{arg\,max}_{Z^{2}}I(Z^{2},_{c}^{1*};X^{2})-  I(Z^{2};_{c}^{2*})\] (4)

The hyperparameters \(\) and \(\) balance relevance and redundancy in the shared and modality-specific representations respectively. We use the same values for both modalities since they operate on similar information scales. Our sequential training approach, instead of a joint one, stems from the self-sufficient nature of each optimization procedure, where a sub-optimal representation does not enhance the learning of the other. We provide theoretical proofs of the optimality of DisentangledSSL in Appendix E, and specific training objectives for each information term in Appendix H.

## 3 Experimental Results

We conduct experiments on a simulation study (results in Appendix I.1) and two real-world multimodal settings. We compare DisentangledSSL against a disentangled variational autoencoder baseline, DMVAE , as well as various multimodal contrastive learning methods, including CLIP , which aligns modalities to learn shared representations, and FOCAL  and FactorCL , which aim to capture both shared and modality-specific information. Additionally, we test a joint optimization variant, JointOpt , to demonstrate the benefits of our step-by-step approach.

### MultiBench

We utilize the real-world multimodal benchmark from MultiBench , which includes curated datasets across various modalities, such as text, images, and tabular data, along with a downstream label that we expect shared and specific information to have varying importance for. We evaluate the linear probing accuracy of representations learned by each pretrained model, as shown in Table 1. FactorCL-emb refers to the embeddings of FactorCL before projection heads, while FactorCL-proj uses the concatenation of all projection head outputs. All models use representations (or concatenation of them, if applicable) with the same dimensionality. We present results for the combined shared and specific representations for FOCAL and JointOpt in Table 1, with individual results in Appendix I.2. DisentangledSSL consistently outperforms baselines across all datasets, demonstrating its ability to capture valuable information for downstream tasks. Combining shared and specific representations of DisentangledSSL improves performance in most cases, showing both contribute to label prediction, while in MOSI and MUSTARD, only shared or specific information contributes significantly.

### High-content Drug Screening.

**Dataset description.** As characterized in Figure 1, we use two high-content drug screening datasets which provide phenotypic profiles after drug perturbation: RXRX19a  containing cell imaging profiles, and LINCS . We conduct train-validation-test splitting according to molecules. Models are pretrained to learn representations of molecular structures and corresponding phenotypes. We provide additional details on experimental settings in Appendix I.3.

**Molecule-phenotype retrieval using shared representations.** We evaluate the shared representations on the molecule-phenotype retrieval task, which identifies molecules likely to induce a specific phenotype. The shared information, which connects the molecular structure and phenotype, plays a key role in this task. We tune \(\) according to validation set performance and show results of top N accuracy (N=1,5,10) and mean reciprocal rank (MRR) on the test set in Table 2. DisentangledSSL significantly outperforms baselines on both datasets, demonstrating its effectiveness in capturing relevant shared features while excluding irrelevant details. Notably, compared to the variant without the information bottleneck constraint, i.e. DisentangledSSL (\(=0\)), the full DisentangledSSL model preserves critical shared features, achieving superior performance in this retrieval task, where shared information is essential.

**Disentanglement measurement.** To assess the effectiveness of learning modality-specific representations, we introduce the Reconstruction Gain (RG) metric, which quantifies the disentanglement between shared and modality-specific representations. We use 2-layer MLP decoders to reconstruct the original data from the shared and modality-specific representations, both individually and jointly, and compute the reconstruction \(R^{2}\) for each case. A higher gain in \(R^{2}\) when using combined representations indicates lower redundancy and better disentanglement.

As shown in Table 3, DisentangledSSL achieves the highest RG scores across both modalities and datasets, demonstrating superior disentanglement. In contrast, other methods show redundancy due to insufficient constraints for disentanglement during pretraining. We also test the representations from the pretrained model on the counterfactual generation task, with detailed results provided in Appendix I.3.

   Dataset & MIMIC & MOSEI & MOSI & UR-FUNNY & MUSTARD \\  CLIP & 64.97 (0.60) & 76.87 (0.45) & 64.24 (0.88) & 62.73 (0.92) & 56.04 (4.19) \\ FactorCL-emb & 65.25 (0.45) & 71.80 (0.64) & 62.97 (0.81) & 63.29 (2.07) & 56.76 (4.66) \\ FactorCL-proj & 59.43 (1.70) & 74.61 (1.65) & 56.02 (1.26) & 61.25 (0.47) & 55.80 (2.18) \\ FOCAL & 64.42 (0.34) & 76.77 (0.51) & 63.65 (1.09) & 62.98 (1.52) & 54.35 (0.00) \\ JointOpt & 66.11 (0.64) & 76.71 (0.14) & 64.24 (1.75) & 63.58 (1.45) & 56.52 (2.61) \\  DisentangledSSL (shared) & 63.16 (0.48) & 76.94 (0.22) & **65.16** (0.81) & 64.14 (1.53) & 54.11 (1.51) \\ DisentangledSSL (specific) & 65.73 (0.09) & 75.99 (0.60) & 51.70 (0.72) & 60.27 (1.28) & **61.60** (2.61) \\ DisentangledSSL (both) & **66.44** (0.31) & **77.45** (0.06) & 65.11 (0.80) & **64.24** (1.54) & 56.52 (2.18) \\   

Table 1: Prediction accuracy (%) of the representations learned by different methods on MultiBench datasets and standard deviations over 3 random seeds.

   Dataset &  &  \\ Top N Acc (\%) & N=1 & N=5 & N=10 & MRR & N=1 & N=5 & N=10 & MRR \\  Random & 0.30 & 1.50 & 3.00 & - & 0.15 & 0.75 & 1.51 & - \\ CLIP & 3.300(0.40) & 8.303(0.52) & 11.59(0.20) & 0.103(0.001) & 3.95(0.04) & 10.81(0.06) & 15.10(0.20) & 0.146(0.001) \\ DMVAE & 3.85(0.36) & 8.70(0.30) & 11.84(0.32) & 0.106(0.002) & 4.31(0.09) & 11.45(0.13) & 15.88(0.17) & 0.156(0.001) \\ JointOpt & 3.41(0.49) & 8.54(0.14) & 11.64(0.14) & 0.110(0.002) & **4.67**(0.09) & 11.60(0.11) & 16.02(0.15) & 0.161(0.001) \\ FOCAL & 3.61(0.51) & 8.71(0.69) & 1.914(0.74) & 0.108(0.003) & 4.34(0.17) & 11.24(0.19) & 15.74(0.26) & 0.157(0.002) \\ DisentangledSSL (\(=0\)) & 3.390(0.54) & **8.25**(0.33) & 11.53(0.20) & 0.109(0.003) & 4.36(0.13) & 11.27(0.39) & 15.81(0.47) & 0.158(0.001) \\ DisentangledSSL & **4.03**(0.39) & **9.62**(0.20) & **13.12**(0.23) & **0.111**(0.001) & 4.48(0.21) & **11.70**(0.40) & **16.39**(0.39) & **0.163**(0.002) \\   

Table 2: Retrieving accuracy and mean reciprocal rank (MRR) of molecule-phenotype retrieval.

   Dataset &  &  \\ Metric & RG-molecule & RG-phenotype & RG-molecule & RG-phenotype \\  FOCAL & 0.117(0.006) & 0.547(0.001) & 0.122(0.001) & 0.618(0.002) \\ DMVAE & 0.123(0.002) & 0.545(0.003) & 0.139(0.002) & 0.605(0.003) \\ JointOpt & 0.130(0.001) & 0.524(0.001) & 0.103(0.000) & 0.604(0.002) \\ DisentangledSSL & **0.153**(0.003) & **0.591**(0.001) & **0.143**(0.000) & **0.622**(0.002) \\   

Table 3: Reconstruction gain (RG) in \(R^{2}\) of representations for each modality (molecular structure/phenotype).