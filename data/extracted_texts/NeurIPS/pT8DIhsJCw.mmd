# Parameter-efficient Tuning of Large-scale Multimodal Foundation Model

Haixin Wang\({}^{1,}\), Xinlong Yang\({}^{1,}\)1, Jianlong Chang\({}^{2,}\)2, Dian Jin\({}^{3}\), Jinan Sun\({}^{1,}\),

Shikun Zhang\({}^{1}\), Xiao Luo\({}^{1}\), Qi Tian\({}^{2}\)

Equal contribution. \({}^{}\)Corresponding author.

###### Abstract

Driven by the progress of large-scale pre-training, parameter-efficient transfer learning has gained immense popularity across different subfields of Artificial Intelligence. The core is to adapt the model to downstream tasks with only a small set of parameters. Recently, researchers have leveraged such proven techniques in multimodal tasks and achieve promising results. However, two critical issues remain unresolved: _how to further reduce the complexity with lightweight design_ and _how to boost alignment between modalities under extremely low parameters_. In this paper, we propose **A** graceful **p**rompt framework for **c**ross-modal **t**ransfer (**A**u**rora**) to overcome these challenges. Considering the redundancy in existing architectures, we first utilize the mode approximation to generate 0.1M trainable parameters to implement the multimodal parameter-efficient tuning, which explores the low intrinsic dimension with only 0.04% parameters of the pre-trained model. Then, for better modality alignment, we propose the Informative Context Enhancement and Gated Query Transformation module under extremely few parameters scenes. A thorough evaluation on six cross-modal benchmarks shows that it not only outperforms the state-of-the-art but even outperforms the full fine-tuning approach. Our code is available at: https://github.com/WillDreamer/Aurora.

## 1 Introduction

The era of large models in deep learning has arrived, with increasingly more large-scale pre-trained models exhibiting remarkable generation and inference capabilities in the fields of text , vision , and multi-modality . Nowadays, the mainstream strategy in the community involves pre-training models on large-scale data, and then fine-tuning them for each downstream task. This approach has been demonstrated to be effective in various cross-modal tasks, as evidenced by .

However, this high-performing strategy is not always advantageous, as several practical factors limit its further development. The primary concern is excessive parameter dependency. For example, GPT-3  (175B params) obtains striking performances while coming at the cost of a massive parameter count. The enormous parameter size has two obvious drawbacks. Firstly, it incurs significant computations and physical storage costs, making pre-training and transfer very expensive. Besides, fine-tuning limits pre-trained knowledge's effectiveness in small-scale downstream tasks. Both of them hinder the further extension of large models from specific datasets to more general scenarios.

To relieve the high cost of large pre-trained models, a series of parameter-efficient transfer learning (PETL) methods  have been proposed. The common paradigm is to freeze the backbonenetwork of the large model and introduce a small number of additional parameters. Recently, some works have begun to focus on PETL in the multimodal field, such as UniAdapter , VL-Adapter , and MAPLE . However, their common idea is to combine existing architectures used in NLP for multimodal models, which simply inserts learnable parameters in the backbone networks of unimodal and multimodal branches to achieve good performances. Their simple designs cannot integrate the essence of efficient parameter transfer into multimodal models. There are still two main challenges that need to face: one is _how to transfer knowledge in an extremely lightweight manner_, and the other is _how to boost alignment between modalities under extremely low parameters_.

To overcome the challenges mentioned above, we propose \(}\) graceful prompt framework for cross-modal transfer, named **Aurora**, which surpasses the state of the art with a remarkably small parameter budget of merely \(\)100K, thereby aptly living up to its name due to lightness. **First**, inspired by the idea of CANDECOMP/PARAFAC (CP) decomposition , we utilize mode approximation to generate lightweight learnable parameters for the attention-based architectures. As attention is proven to inevitably fit the background noise , most other features are redundant. For example, the categories in CLIP  are essentially infinite, which results in only a few features responding to a single class. This can be easily proven by the fact that the logits output by CLIP are mostly between 0.2 to 0.3 with very little fluctuation, indicating that most features are consistent. In other words, although pre-trained models have large-scale high-dimensional parameters, the intrinsic dimension corresponding to each downstream task is not large . Thus, our mode approximation can fine-tune a very small number of parameters to achieve good performance on downstream tasks theoretically.

**Second**, in contrast to existing methods that directly insert a small network for cross-modal fusion, we mine deeper into the unimodal information and fuse them in a controllable manner with multimodal representations, thereby achieving desirable results with lower parameter dependence. In our proposed Informative Context Enhancement module, each feature is enriched by contextual information to boost the modality fusion. Furthermore, we adaptively control the ratio of textual information supplied for modality fusion in our proposed Gated Query Transformation module.

Finally, we evaluate Aurora on six cross-modal tasks and two zero-shot tasks, which obtains state-of-the-art performance compared to other PETL methods. Compared to the full fine-tuning, Aurora even obtains 1.8% and 0.5% performance improvement on MSRVTT and VQAv2 benchmarks averagely but only with about 0.05% trainable parameters of the pre-trained model.

## 2 Related Work

### Vision-Language Models

In vision-language pre-training, large-scale image-text pairs are used to pre-train the Vision-Language Models (VLMs) for downstream tasks [13; 56; 41; 72; 3; 76]. The architecture of VLMs typically consists of a visual encoder, a textual encoder, and a cross-modal fusion module. VLMs have been applied to various tasks, including image generation , image captioning [39; 26], visual question answering [12; 11; 20], and cross-modal retrieval [58; 28]. This article focuses on how to efficiently fine-tune VLMs on downstream tasks and utilizes BLIP-base  for vision-language tasks.

### Parameter-efficient Transfer Learning

As the size of recent models increases rapidly, updating the models in parameter-efficient ways becomes crucial. The huge computational burden brought by these pre-trained models will undoubtedly impose unnecessary burdens on transfer learning. PETL [17; 74] methods diverge from the conventional approach of fine-tuning the entire pre-trained model, instead only learning a few additional parameters for knowledge transfer. There are three common categories, prompt tuning [77; 30; 66; 46], adapter tuning [32; 10; 62] and parameter tuning [25; 31; 61]. We aim to relieve the computational burden by developing a graceful prompt framework specifically for cross-modal transfer, adapting frozen pre-trained multimodal models to downstream tasks across a broad distribution.

### Tensor Decomposition

Tensor decomposition [35; 4; 59; 51] is an important research area studied for decades, which aims to approximate a tensor through a set of low-rank factors with diverse applications. In general, the most widely used decompositions are Tucker decomposition  and CANDECOMP/PARAFAC (CP) decomposition , both of which can be seen as generalizations of the matrix singular value decomposition (SVD) . CP decomposition can also be seen as a special Tucker decomposition whose core tensor is diagonal, but is more _lightweight_ and _explainable_, meaning that a high-order tensor can be uniquely represented as the sum of a set of rank-one tensors theoretically. Here, we leverage the idea of CP decomposition to implement mode approximation for lightweight prompts.

## 3 Methodology

### Background

**Frozen Backbone.** BLIP  is a unified VLP framework which has multimodal mixture of encoder-decoder(MED) architecture with both understanding and generation capabilities. We utilize BLIP-base as the frozen backbone and the pre-trained weights can be downloaded from Salesforce. Its visual encoder is ViT-B  and the text encoder is the same as BERT  while the text decoder replaces the self-attention layers with causal self-attention layers. It uses cross-attention layers to gather information from encoded visual representations using the textual representations as query. It's flexible to choose different components in the BLIP architecture to perform different multimodality downstream tasks.

We start with the core attention-based Transformer architecture mostly utilized in existing large-scale multimodal models [56; 44; 73; 41] for representation. The input image/text is divided into several non-overlapping patches, and then these patches appended a [CLS] token are fed into an embedding layer followed by the Transformer blocks with multi-head attention as the core operation. It copies the input embedding into query, key, and value by three projection matrices \(W_{q}^{l}\), \(W_{k}^{l}\), and \(W_{v}^{l}^{d d}\) respectively, where \(l\) represents the \(l\)-th layer. The pre-trained model contains many dense layers which perform matrix multiplication as follows:

\[(X_{q,m}^{l},X_{k,m}^{l},X_{v,m}^{l})= (^{l}W_{q,m}^{l}(X_{k,m}^{l}W_{k,m}^{l})^{T}}{ })X_{v,m}^{l}W_{v,m}^{l}\]

where \(m\{I,T,C\}\) denotes the vision, text, and cross-modal modality. For the unimodal vision/text modality branch, \(m\) are all from the vision/text modality. For the cross-modal modality branch, \(X_{q,m}^{l}\) is from the text modality, while the other two are from the visual modality. Assume there are \(L\) layers in total, we can stack all of the attention-based weight matrices in the multimodal pre-trained model, and derive the tensor \(}_{0}=\{W_{q}^{l},W_{k}^{l},W_{v}^{l}\}_{l=1}^{L} ^{d d N}\), where \(d\) is the dimension of the embedding token, and \(N\) denotes the total number of the weight matrices.

**Parameter Update.** For downstream tasks, directly updating the weight tensor with full fine-tuning will consume a huge amount of computation and brings about a heavy storage burden. We aim to update the knowledge with a few additional trainable parameters following the idea of PETL methods. Due to the redundancies of \(}_{0}\), we hope to implement the mode approximation of the tensor \(}_{0}\) to get the new learnable weight tensor \(}\) for downstream knowledge transfer. The

Figure 1: Comparison of existing PETL methods for downstream cross-modal tasks. (a) **Adapter**, which involves inserting a learnable small network into a pre-trained model; (b) **LoRA**, which employs a down and up tensor as updated parameters for low-rank approximation (R \(\) d), added to the pre-trained model; and (c) our proposed **Aurora**, which utilizes mode approximation to further reduce the number of trainable parameters added to the pre-trained model. Notably, the red blocks represent trainable parameters, while the blue ones indicate the frozen backbone.

differences between our Aurora and existing PETL methods are demonstrated in Figure 1. Therefore, the backward propagation on the downstream training data \(\) can be expressed as:

\[_{}}=_{}}=(;}_{0}+ {})}{}}\] (1)

### Lightweight Design for PETL

To ensure a successful parameter-efficient tuning framework, it is important to prioritize a lightweight design that enables easy deployment and scalability. Recent research  has shown that when self-attention-based architecture is pre-trained on large-scale datasets, there is a significant amount of feature redundancy due to the limited responses of individual classes in a nearly infinite feature space. In fact, there exists a low-dimensional reparameterization that is as effective for fine-tuning as the full parameter space . This inherent dimensionality describes the minimum dimension required to solve the optimization problem it defines to a certain level of precision. Considering the pre-trained parameters as a tensor, we recognize that approximation can effectively preserve low-rank yet discriminative non-redundant features, and scale down the weight tensor of pre-trained large-scale models along certain directions, thus making them more suitable for downstream tasks. Therefore, we propose a novel mode approximation method named Aurora, which utilizes mode approximation to update the frozen \(}_{0}\). Specifically, we borrow the idea of CANDECOMP/PARAFAC (CP) decomposition to decompose the learnable parameters \(}\) into a series rank-one tensors to explore the inherent dimensionality embedded in the features. The detailed architecture is shown in Figure 2.

**CP Decomposition.** In the typical tensor decomposition method, a 3D tensor has three modes (further introduced in Appendix), each of which can be viewed as the reduced projection of the tensor in a specific dimension. Given the 3D updated weight tensor \(}^{d d N}\), CP decomposition factorizes this tensor into a sum of \(R\) rank-one components in total, and each component can be formalized as the outer product of three decomposed vectors in the formulation of:

\[}=_{r=1}^{R}_{r}(} }})\] (2)

where \(}^{d}\), \(}^{d}\), and \(}^{N}\) are decomposed vectors for the \(r\)-th component, while each vector belongs to the corresponding mode matrix, i.e., \(}\) is the column vector in

Figure 2: Demonstration of the overall framework. The frozen backbone network is shown in grey. The trainable parameters in color represent: blue for vision tasks, pink for text tasks, and the gradient color for fused modalities. Notably, globally shared parameters are represented in purple.

\([u_{1},,u_{r},,u_{R}]\). In addition, \(\) represents the outer product, \(_{r}\) is the coefficient scalar of each component, and \(R\) is the rank of CP decomposition. For better understanding, each component contributes to the value of the tensor in the sum of scalar products:

\[}_{ijk}=_{r=1}^{R}_{r}(u_{r,i}v_{r,j}p_{r,k}),  i,j\{1,2,,d\},k\{1,2,,N\}\] (3)

where \(i,j,k\) denote the indices of the three modes.

**Mode Approximation.** In multimodal tasks, learning modality-specific representations and modality fusion representations are both important. Therefore, we aim to implement mode approximation to update the frozen attention-based weight tensor \(}_{0}\), including the self-attention module in the vision/text encoders, and the cross-attention module in the multimodal encoders, which are based on the pre-trained multimodal foundation model like BLIP . We first approximate the attention-based weight in these modules by initializing three mode factors, i.e., \(U=[},,}]\), \(V=[},,}]\), and \(P=[},,}]\). \(U,P\) are randomly initialized with Gaussian distribution, and \(V\) is with zeros, so that \(}=0\) before training. It should be noted that \(U,V\) are shared as global factors utilized for mode approximation, which means that our Aurora considers the cross-modal interaction and share the knowledge between these weight matrices in each modality. Besides, to further capture the discriminative features of each modality, we randomly initialize the learnable coefficient vector \(^{m}^{R}\) for each weight matrix on the modality \(m\) respectively. With these three mode factors, we can implement the mode approximation in the forward propagation by the inverse progress of CP decomposition with input tensor \(}^{m}\) as follows:

\[}^{m}=}_{0}}^{m}+(_{r=1 }^{R}_{r}^{m}(}}}))}^{m}\] (4)

Analyzed from the perspective of prompt learning, our idea of approximating the pre-trained weight parameters \(}_{0}\) with additional trainable parameters \(}\) can be essentially understood as the soft prompts, which learns on downstream data based on CP decomposition. Such prompts not only provide better guidance for downstream tasks, but also are very lightweight in design, which greatly facilitates the application of pre-trained models to many cross-modal tasks in the unified mechanism.

### Modality Alignment Design

Unlike existing methods that directly insert learnable networks to explicitly achieve cross-modal alignment, we further propose two effective modules to align different modalities with few trainable parameters. Thus, with the addition of mode approximation above, we can achieve a graceful prompt framework for cross-modal transfer, which is both lightweight and high-performance.

**Informative Context Enhancement.** For better modality alignment, we aim to provide prompts that can activate the fusion features after the cross-attention module. Inspired by the development in In-Context Learning [52; 18], we realize that the demonstration template is important for the prompts. The most intuitive approach is to align image-text pairs to obtain more cross-modal contextual information. However, even with relevant image regions, there may still be more than one way to describe these regions. Some texts may accurately summarize the content of an image, whereas others may not. Without _a priori_ for matched textual information, we determine to introduce the context enhancement module to provide coverage of possible textual information.

We adopt the image-grounded text branch from BLIP and design a specific demonstration template for cross-modal prompt tuning. Given the fusion features of the image-grounded text branch \(F=\{},,|}}\}^{||  E}\) and the textual query features of the self-attention module \(T=\{},,|}}\}^{||  E}\), we utilize all of the query features with dimension \(E\) in a batch \(\) as the context for enhancement. Specifically, we calculate the attention score \(^{||||}\) between feature \(}\) and each textual query feature from \(}\) to \(|}}\) as:

\[_{ij}=}})}{_{b=1}^{||} (}})}\] (5)

In order to generate a more effective advanced fusion feature \(F^{}\), all the adaptively weighted query features \(T\) within a batch are collected together with a specific fusion feature \(}\) to form the demon stration template. This form can adaptively absorb context query information to derive a better enhanced fusion feature for the image-text matching loss \(_{}\) as \(_{i}}=}+_{j=1}^{||}_{ij}}\).

**Gated Query Transformation.** Another reason why modality alignment is difficult is that the multimodality fusion branch network is deep, which can cause textual information to be lost during training. To address this issue, we propose a novel approach inspired by gating mechanisms to explicitly model the relative contributions of textual information during modality alignment. Specifically, instead of directly concatenating the fusion representation \(\) (output of the Cross-Attention block) with the query representation \(\) (output of the Self-Attention block) as residuals in existing methods , we learn a gated query function to balance the contributions of both modalities. Our gated query transformation involves two steps. First, we implement the transformation as follows: \(}=()+\), where \(\) and \(\) are zero-initialized learnable transformation matrix and bias with an activation function \(\). Second, we calculate the query gate \(\) by computing the product of \(}\) and \(\) with Softmax. It should be noted that \(\) and \(\) are zero initialized, so that \(}\) is zero at the beginning of training. Hence, the query gate explicitly measures the contribution of the query representation in the formulation of \(+(-)}\) to update the fusion representation \(\).

## 4 Experiments

### Experimental Settings

**Datasets & Baselines.** We evaluate Aurora on six benchmarks spanning four cross-modal tasks: image-text retrieval, question answering (QA), and video-text retrieval and QA. We compare Aurora with two types of approaches: full fine-tuning methods including SOTA for each task, and frozen backbone methods including LoRA  and UniAdapter . See more details in Appendix.

**Implementation Details.** Our implementation is based on Salesforce's open-source codebase . Following , we also apply BLIP-base  as our vision-language backbone for all the multimodal downstream tasks. We use PyTorch to implement all experiments on 8\(\) NVIDIA V100 GPU (32G). We use AdamW  optimizer with a weight decay of 0.05 and a learning rate of 1e-4 obtained from grid search for all experiments. Note that during the fine-tuning process, the parameters of the backbone model are kept frozen. More training details can be seen in Appendix.

### Performance Comparisons on Cross-modal Tasks

**Image-Text Retrieval.** Table 1 shows performances for image-text retrieval tasks on MSCOCO  and FLICKR30K . It can be observed that our Aurora (R=64) achieves comparable results with the state-of-the-art frozen backbone method while using only 0.5% of its parameters. When we

    &  &  &  &  &  \\  & & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\   \\  UNITER & 330M & 65.7 & 88.6 & 93.8 & 52.9 & 79.9 & 88.0 & 87.3 & 98.0 & 99.2 & 75.6 & 94.1 & 96.8 \\ VILLA & 330M & - & - & - & - & - & 87.9 & 97.5 & 98.8 & 76.3 & 94.2 & 96.8 \\ OSCAR & 330M & 73.5 & 92.2 & 96.0 & 57.5 & 82.8 & 89.8 & - & - & - & - & - \\ ALIGN & 820M & 77.0 & 93.5 & 96.9 & 59.9 & 83.3 & 89.3 & 98.3 & 98.8 & **100.0** & 84.9 & 97.4 & 98.6 \\ ALBEF & 210M & 77.6 & 94.3 & 97.2 & 60.7 & 84.3 & 90.5 & 95.9 & 99.8 & **100.0** & 85.6 & 97.5 & **98.9** \\ BLIP & 223M & **81.9** & **95.4** & **97.8** & **64.3** & **85.7** & **91.5** & **97.3** & 99.9 & **100.0** & **87.3** & 97.6 & **98.9** \\   \\  LoRA (r = 32) & 10.6M & 80.0 & 94.1 & 97.2 & 62.1 & 84.4 & 90.6 & 96.2 & 99.7 & 99.8 & 85.8 & 97.1 & 98.4 \\ UniAdapter (r=128) & 4.6M & 79.8 & 94.2 & 97.5 & 62.3 & 84.5 & 90.8 & 97.1 & **100.0** & **100.0** & 86.5 & 97.4 & 98.8 \\ UniAdapter (r=512) & 18.8M & 80.1 & 94.6 & 97.4 & 62.6 & 84.6 & 90.9 & 97.1 & 99.9 & **100.0** & 86.4 & 97.4 & **98.9** \\  Aurora (ours, r=64) & **0.1M** & 80.2 & 95.1 & 97.7 & 62.4 & 84.5 & 91.0 & 96.8 & **100.0** & **100.0** & 86.7 & **97.8** & 98.7 \\ \(_{PETL}\) & **0.5\%** & +0.1 & +0.5 & +0.2 & -0.2 & -0.1 & +0.1 & -0.3 & +0.0 & +0.0 & +0.2 & +0.4 & -0.2 \\ Aurora (ours, r=128) & **0.2M** & 80.7 & 95.3 & **97.8** & 62.8 & 84.8 & 91.0 & 97.2 & **100.0** & **100.0** & 86.8 & 97.6 & **98.9** \\ \(_{PETL}\) & **1\%** & **+0.6** & **+0.7** & +0.3 & +0.2 & +0.2 & +0.1 & +0.1 & +0.0 & +0.0 & +0.3 & +0.2 & +0.0 \\   

Table 1: Results on image-text retrieval datasets MSCOCO and FLICKR30K. Using the text query, we simplify retrieving images as T\(\)I and vice versa. Recall@\(K\) represents the recall of top-\(K\) returned samples. # Tunable is the size of the learnable parameters in the backbone network. \(_{PETL}\) represents the performance gap between our Aurora and the best PETL method.

increase the rank to 128, Aurora can further boost the performance, surpassing all the frozen backbone methods, and even outperforming some full fine-tuning methods with fewer trainable parameters.

**Video-Text Retrieval.** To further verify the performance of our Aurora in the field of video-text retrieval, we conduct experiments on two video datasets, MSRVTT  and DiDemo , and the results are presented in Table 2. With only about 0.1M trainable parameters, our Aurora directly achieves better performances than all the frozen backbone methods, and we outperform most full fine-tuning methods. This indicates that our Aurora has an excellent understanding ability under video-text scenes, even with relatively few trainable parameters.

**Visual Question Answering.** In an effort to further explore the potential of our Aurora, we evaluate it on VQA/VideoQA tasks with VQAv2 /MSRVTT-QA  datasets and demonstrate the evaluation results in Table 3. Unlike retrieval tasks, VQA task needs to verify the model's multimodal generative ability. We share the trainable parameters of the multimodal encoder and multimodal decoder to further reduce the parameter amount. From the results, we find that our Aurora outperforms UniAdapter and all the full fine-tuning methods, indicating that Aurora can have powerful transfer ability for downstream generative tasks.

### Performance Comparisons on Zero-shot Setting

To evaluate the generalization ability of Aurora, we conduct experiments on cross-modal tasks with zero-shot setting, and make comparisons with the pretrained version, full fine-tuning method, LoRA, and UniAdapter.

    &  &  &  &  &  \\  & & & R@1 & R@5 & R@10 &  &  &  & R@5 & R@10 &  &  &  &  &  &  &  &  &  \\   \\  ClipBERT & 16\(\)448 & 5M & 135M & 22.0 & 46.8 & 59.9 & 6.0 & 20.4 & 48.0 & 60.8 & 6.0 \\ Frozen in Time & 32\(\)224 & 5M & 180M & 31.0 & 59.5 & 70.5 & 3.0 & 34.6 & 65.0 & 74.7 & 3.0 \\ ALPRO & 8\(\)224 & 5M & 245M & 33.9 & 60.7 & 73.2 & 3.0 & 35.9 & 67.5 & 78.8 & 3.0 \\ VIOLET & 5\(\)224 & 138M & 306M & 34.5 & 63.0 & 73.4 & - & 32.6 & 62.8 & 74.7 & - \\ All-in-one & 9\(\)224 & 138M & 110M & 37.9 & 68.1 & 77.1 & - & 32.7 & 61.4 & 73.5 & 3.0 \\ CLIP4Clip & 12\(\)224 & 400M & 124M & 43.1 & 70.4 & 80.8 & 2.0 & 42.8 & 68.5 & 79.2 & 2.0 \\ CLIP-Hilker & 120\(\)224 & 400M & 124M & 47.7 & **74.1** & **82.9** & - & - & - & - & - \\   \\  CLIP-Promot & 16\(\)224 & 400M & 64M & 36.7 & 64.6 & - & - & - & - & - & - \\ LoRA (r=32) & 8\(\)224 & 129M & 10.6M & 49.9 & 72.0 & 81.3 & 2.0 & 50.9 & 75.3 & 82.4 & 2.0 \\ UniAdapter (r=128) & 8\(\)224 & 129M & 4.6M & 49.7 & 71.9 & 81.5 & 2.0 & 49.0 & 75.5 & 83.3 & 2.0 \\ UniAdapter (r=512) & 8\(\)224 & 129M & 18.8M & 50.6 & 73.4 & 81.6 & **1.0** & 52.1 & 77.3 & 85.2 & **1.0** \\ Aurora (ours, r=64) & 8\(\)224 & 129M & **0.11** & **52.4** & 73.9 & 82.0 & **1.0** & **53.1** & **77.4** & **85.3** & **1.0** \\ \(_{PETL}\) & - & - & **0.5\(\%\)** & +1.8 & +0.5 & +0.4 & +0.0 & +1.0 & +0.1 & +0.0 \\   

Table 2: Results on two benchmark video-text retrieval datasets, MSR-VTT and DiDemo. Recall@\(K\) and MdR are utilized as the evaluation metric, where MdR measures the median rank of target items in the retrieved ranking list. Input means the sampling number and frame shape of each video.

    &  &  &  &  &  &  &  \\  & & test-dev & test-std & & & test acc \\   \\  VL-T5/BART & 165M & - & 71.30 & CLIPBERT & 135M & 37.4 \\ SOHO & 155M & 73.25 & 73.47 & ALPRO & 245M & 42.1 \\ OSCAR & 330M & 73.61 & 73.82 & Just-Aak & 200M & 41.5 \\ UNITER & 330M & 73.82 & 74.03 & VIOLET & 306M & 43.9 \\ ALBEF & 266M & 75.84 & 76.04 & MERLOT & 233M & 43.1 \\ BLIP & 337M & 77.44 & 77.48 & All-in-one & 110M & 44.3 \\   \\  UniAdapter (r=128) & 4.6M & 73.72 & 73.71 & UniAdapter (r=128) & 4.6M & 44.2 \\ UniAdapter (r=512) & 18.8M & 75.44 & 75.56 & UniAdapter (r=512) & 18.8M & 44.7 \\ Aurora (ours, r=64) & **0.1M** & **77.69** & **77.87** & Aurora (ours, r=64) & **0.1M** & **44.8** \\ \(_{PETL}\) & - & +2.25 & +2.31 & \(_{PETL}\) & - & +0.1 \\   

Table 3: Results on two visual question answering datasets, VQAv2 and MSRVTT-QA. # Tunable represents the number of learnable parameters. For VQAv2, we report the test-dev and test-std results, for MSRVTT-QA, accuracy is used as the evaluation metric.

Specifically, experiments are based on the pre-trained large-scale multimodal foundation models, i.e., BLIP, which is further tuned in each corresponding method on MSCOCO. Then, we utilize two video datasets as the zero-shot data for validation. Table 4 provides an overview of the performance of Aurora on various zero-shot multimodal tasks. It is obvious that Aurora achieves the highest zero-shot performance while requiring the least number of trainable parameters during vision-language pre-training, which represents more powerful general-purpose understanding ability.

### Analysis of Different Designs

We thoroughly evaluate how the core operations in Aurora affect results, i.e., the rank \(R\), parameter sharing, context enhancement, and the gated query transformation. We conduct experiments to analyze the impacts of different designs, which are implemented on several cross-modal tasks.

**How Rank of CP Decomposition Affects Aurora?**

Results on Flickr30K and DiDemo are shown in the left three columns in Figure 3. A fundamental conclusion is that as \(R\) increases, the dimensionality of the model representation also increases, leading to better performance on downstream tasks. However, when \(R\) reaches a certain range, the rate of increase slows down, which may be related to the redundancy of high-dimensional features.

In addition, we show in (d) of Figure 3 that as \(R\) increases, the growth rate of parameter size in Aurora is much slower than that of LoRA and UniAdapter. As another tensor decomposition method, our scalability and ease of deployment are much stronger than the baselines.

**How Does Aurora Benefit from Informative Context Enhancement?**

For an in-depth investigation, we introduce three ablated variants, i.e., **Aurora w/o C** removes the context enhancement, **Aurora w. R** replaces the context with random vectors, and **Aurora w. M** simply takes the average features of all queries. Ablation results are shown in Figure 4, where we separate the results on two \(y\)-axes respectively, which can better demonstrate the differences across different indicators more clearly. And the results of our Aurora are marked by the black upper lines. Surprisingly, using random vectors to supplement fused features still achieves better results than removing this module. Moreover, we observe that adaptive weight \(\) causes better performance than the uniform weight. These phenomena fully demonstrate that with few parameters,

    &  &  &  \\  & & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\  BLIP & 223M & 41.5 & 62.0 & 70.7 & 42.1 & 59.6 & 67.3 \\ BLIP + FFT & 223M & 42.5 & 62.8 & 71.6 & 43.0 & 60.5 & 68.3 \\ BLIP + LoRA & 10.6M & 42.7 & 62.8 & 71.4 & 43.4 & 60.3 & 68.2 \\ BLIP + UniAdapter & 18.8M & 42.2 & 62.6 & 71.1 & 43.1 & 60.2 & 67.9 \\ BLIP + Aurora & **0.1M** & **43.1** & **63.5** & **72.0** & **44.6** & **61.4** & **68.6** \\   

Table 4: Zero-shot performance analysis.

Figure 4: Analysis of the impact of the informative context enhancement module.

Figure 3: The answer to how rank \(R\) affects Aurora. **(a)**, **(b)**, and **(c)** show the performance increase accompanied with larger \(R\) on three different cross-modal tasks. Notably, our results are divided on two \(y\)-axes for clear demonstration, where Recall@1 is shown on the left axis and Recall@5/10 are on the right one. **(d)** compares the parameter scalability with other PETL methods.

supplementing more context information to fused features is crucial to promote modality alignment for downstream tasks, and our context enhancement is the optimal choice.

**How Does Aurora Benefit from Parameter Sharing?**

See Figure 5 for the effectiveness of parameter sharing. We observe that the parameter sharing strategy can achieve a better capability to locate the most semantic-related visual parts for specific words in the text. It should be noted that we are able to reduce 0.4M parameters through parameter sharing. Moreover, we also conduct quantitative experiments on FLICKR30K, in which parameter sharing results in better performance with an average improvement of 0.1. Therefore, we can draw the conclusion that parameter sharing during training not only decreases the parameter dependence but also boost the cross-modal interaction for better performances.

**How Does Aurora Benefit from Gated Query Transformation?**

To further validate the effectiveness of the Gated Query Transformation module, we conduct a comprehensive ablation study on MSCOCO and DiDemo datasets with two variants. The experimental results are shown in Figure 6, where **Aurora w/o Q** (blue bars) represents the removal of the Gated Query Transformation and **Aurora w. O** (red bars) initializes the transformation matrix and bias with ones. The gap between the blue bars and ours shows that this module helps promote the final performance to some extent, which also indicates that adaptively incorporating text information into the modality fusion branch has a positive effect on modality alignment under the deep network architecture. In addition, the difference between the red bars and ours shows that initializing from zeros is more conducive to adaptively controlling the proportion of text feature fusion, and can gradually achieve better modality alignment for better results.

### Visualization Analysis

**Parameter Distribution.** We visualize the parameter distributions of the pre-trained model, fully fine-tuned model, and our Aurora in Figure 7. Aurora involves gradually embedding the knowledge learned from downstream tasks into the parameters of a pre-trained model, while amplifying the original parameters in a certain direction. From the left part, we can see that our Aurora only adjusts the pre-trained model parameters in a small local range, but it can have a better effect on downstream

Figure 5: Visualization of cross-attention map comparisons on Flickr30K, which shows the capability to locate the most semantic-related visual parts for specific words in the text.

Figure 6: Analysis of the impact of the gated query transformation module.

Figure 7: The left three columns show the parameter distribution of the pre-trained large-scale multimodal foundation model (BLIP) _vs._ our Aurora, which is tuned on MSCOCO. And the right part is full fine-tuned model _vs._ Aurora. Notably, \(}_{q}\) is the stack of the query projection matrices in different modality branches.

tasks. Meanwhile, we can see from the right part that Aurora's parameter distribution is very close to that of the fully fine-tuned model, with only small changes in a small range. These small changes enable our method to achieve superior performance on many downstream tasks.

## 5 Conclusion

This paper proposes Aurora, a graceful prompt framework for cross-modal transfer. We first leverage mode approximation to implement multimodal prompt tuning, which explores the low intrinsic dimension with only 0.04% parameters of the pretrained model. Then, to better reduce the modality gap, we propose Informative Context Enhancement and Gated Query Transformation modules under extremely low parameter scenarios. Extensive evaluation of Aurora on six cross-modal benchmarks shows that it not only outperforms the state-of-the-art but even surpasses full fine-tuning approach.

**Limitations.** The representation ability of our Aurora to some extent depends on the setting of the rank \(R\). However, it also unavoidably increases the parameter size and thus the computational cost. Therefore, it is crucial for us to select a proper rank in the trade-off between parameter size and performance. Unfortunately, it is challenging for us to determine a suitable rank beforehand with the variations of downstream tasks and data sizes, which poses a challenge to the application of our model. We will further optimize this issue in future work.

**Broader Impact.** Multimodal pre-trained large models have wide applications in the real world, including image-text understanding, retrieval, question answering, and generation. As models scale up, they will achieve more amazing performances. Our Aurora enables lightweight transfer of large-scale models, which allows us to better utilize cloud-based large models and guide them to produce high-performance edge applications under constrained computational resources.