# Hierarchical and Density-based Causal Clustering

Kwangho Kim

Korea University

kwanghk@korea.ac.kr &Jisu Kim

Seoul National University

jkim82133@snu.ac.kr Larry A. Wasserman

Carnegie Mellon University

larry@stat.cmu.edu &Edward H. Kennedy

Carnegie Mellon University

edward@stat.cmu.edu

###### Abstract

Understanding treatment effect heterogeneity is vital for scientific and policy research. However, identifying and evaluating heterogeneous treatment effects pose significant challenges due to the typically unknown subgroup structure. Recently, a novel approach, causal k-means clustering, has emerged to assess heterogeneity of treatment effect by applying the k-means algorithm to unknown counterfactual regression functions. In this paper, we expand upon this framework by integrating hierarchical and density-based clustering algorithms. We propose plug-in estimators which are simple and readily implementable using off-the-shelf algorithms. Unlike k-means clustering, which requires the margin condition, our proposed estimators do not rely on strong structural assumptions on the outcome process. We go on to study their rate of convergence, and show that under the minimal regularity conditions, the additional cost of causal clustering is essentially the estimation error of the outcome regression functions. Our findings significantly extend the capabilities of the causal clustering framework, thereby contributing to the progression of methodologies for identifying homogeneous subgroups in treatment response, consequently facilitating more nuanced and targeted interventions. The proposed methods also open up new avenues for clustering with generic pseudo-outcomes. We explore finite sample properties via simulation, and illustrate the proposed methods in voting and employment projection datasets.

## 1 Introduction

### Heterogeneity of Treatment Effects

Causal effects are typically summarized using population-level measures, such as the average treatment effect (ATE). However, these summaries may be insufficient when treatment effects vary across subgroups. For example, finding the subgroups that experience the least or greatest benefit from a specific treatment is of particular importance in personalized medicine or policy evaluation, where the subgroup effects of interest may diverge significantly from the population effect. Even while experiencing the same treatment effects, some people may have been exposed to a significantly higher baseline risk. In the presence of effect heterogeneity, the typically unknown subgroup structure poses significant challenges in accurately identifying and evaluating subgroup effects compared to population-level effects.

To delve deeper than the information provided by the population summaries and to better understand treatment effect heterogeneity, investigators often estimate the conditional average treatment effect (CATE) defined by

\[(Y^{1}-Y^{0} X),\]where \(Y^{a}\) is the potential outcome that would have been observed, possibly contrary to fact, under treatment \(A=a\), and \(X\) is a vector of observed covariates. The estimation of the CATE has the potential to facilitate the personalization of treatment assignments, taking into account the characteristics of each individual. Admittedly, the CATE is the most commonly-used estimand to study treatment effect heterogeneity. Various methods have been proposed to obtain accurate estimates of and valid inferences for the CATE, with a special emphasis in recent years on leveraging the rapid development of machine learning methods (e.g., 3; 20; 21; 27; 35; 42; 46; 52; 53; 58; 62).

Subgroup analysis has been the most common analytic approach for examining heterogeneity of treatment effect. Selection of subgroups reflecting one's scientific interest plays a central role in the subgroup analysis. Statistical methods aimed at finding such subgroups from observed data have been termed _subgroup discovery_. The selection of such subgroups may be informed by mechanisms and plausibility (e.g., clinical judgment), taking into account prior knowledge of treatment effect modifiers. They could be chosen by directly subsetting the covariate space, often in a one-variable-at-time fashion (e.g., 49). Most existing studies on data-driven subgroup discovery identify subgroups where the CATE exceeds a prespecified threshold of clinical relevance, allowing researchers to prioritize subgroups with enhanced efficacy or favorable safety profiles (e.g., 6; 11; 44; 47; 51; 63). Some recent advances proposed heuristics for discovering rules based on a specific CATE estimator subject to a certain optimality criterion, yet without any theoretical exploration (e.g., 8; 15; 23; 48). Wang and Rudin (59) proposed an algorithm to automatically find a subgroup based on the causal rule: (CATE \(>\) ATE). Kallus (31) proposed a subgroup partition algorithm for determining a subgroup structure that minimizes the personalization risk.

### Causal Clustering

In contrast to earlier work predominantly focused on supervised learning approaches, there is a growing interest in analyzing heterogeneity in causal effects from an unsupervised learning perspective, particularly within the causal discovery literature. Based on the causal graph or structural causal model framework, there has been a series of recent attempts to learn _structural heterogeneity_ through clustering analysis (e.g., 25; 26; 41; 45). Conversely, the exploration of _treatment effect heterogeneity_ in the potential outcome/counterfactual framework using unsupervised learning methods has received significantly less attention. To our knowledge, only one paper has developed such methods; Kim et al. (39) have proposed _Causal k-Means Clustering_, a new framework for exploring heterogeneous treatment effects leveraging tools from cluster analysis, specifically k-means clustering. It allows one to understand the structure of effect heterogeneity by identifying underlying subgroups as clusters without imposing a priori assumptions about the subgroup structure.

To illustrate, we consider binary treatments and project a sample onto the two-dimensional Euclidean space \(([Y^{0} X],[Y^{1} X])\). It is immediate to see that closer units are more homogeneous in terms of the CATE, which provides vital motivation for uncovering subgroup structure via cluster analysis on this particular counterfactual space. (See (a) & (e) in Figure 1). This approach has the capability to uncover complex subgroup structures beyond those identified by CATE summary statistics or histograms. Moreover, it holds particular promise in outcome-wide studies featuring multiple treatment levels (56; 57), because instead of probing a high-dimensional CATE surface to assess the subgroup structure, one may attempt to uncover lower-dimensional clusters with similar responses to a given treatment set.

However, the method proposed by Kim et al. (39) only applies to k-means clustering. Despite is popularity, k-means has some drawbacks. It works best when clusters are at least roughly spherical. It also has trouble clustering data when the clusters are of varying sizes and density, or based on non-Euclidean distance. Furthermore, the cluster centers (centroids) might be dragged by outliers, or outliers might even get their own cluster. Other commonly-employed clustering algorithms, particularly hierarchical and density-based approaches, could mitigate some of these limitations (1; 9; 22; 29; 40). Density-based clustering is applicable for identifying clusters of arbitrary sizes and shapes, while concurrently exhibiting robustness to noise and outlier data points. Hierarchical clustering proves beneficial in scenarios where the data exhibit a nested structure or inherent hierarchy, irrespective of their shape, and can accommodate various distance metrics. It enables for the creation of a dendrogram, which provides insights into the interrelations among clusters across multiple levels of granularity. Figure 1 illustrates the three methods in the causal clustering framework with binary treatments, where hierarchical and density-based clustering methods produce more reasonable subgroup patterns.

In this work, we extend the work of Kim et al.  by integrating hierarchical and density-based clustering algorithms into the causal clustering framework. We present plug-in estimators which are simple and readily implementable using off-the-shelf algorithms. Unlike k-means clustering, which requires the margin condition , our proposed estimators do not rely on such strong structural assumptions on the outcome process. We study their rate of convergence, and show that under the minimal regularity conditions, the additional cost of causal clustering is essentially the estimation error of the outcome regression functions. Our findings significantly extend the capabilities of the causal clustering framework, thereby contributing to the progression of methodologies for identifying homogeneous subgroups in treatment response, consequently facilitating more nuanced and targeted interventions. In a broader sense, causal clustering may be construed as a nonparametric approach to clustering involving unknown functions, a domain that has received far less attention than conventional clustering techniques applied to fully observed data, notwithstanding its substantive importance. Therefore, the proposed methods also open up new avenues for clustering with generic pseudo-outcomes that have never been observed, or have been observed only partially.

## 2 Framework

Following Kim et al. , we consider a random sample \((Z_{1},...,Z_{n})\) of \(n\) tuples \(Z=(Y,A,X)\), where \(Y\) represents the outcome, \(A=\{1,...,q\}\) denotes an intervention with finite support, and \(X^{d}\) comprises observed covariates. For simplicity, we focus on univariate outcomes, although our methodology can be easily extended to multivariate outcomes. Throughout, we rely on the following widely-used identification assumptions:

**Assumption C1** (consistency).: \(Y=Y^{a}\) _if \(A=a\)._

**Assumption C2** (no unmeasured confounding).: \(A\!\!\! Y^{a} X\)_._

**Assumption C3** (positivity).: \((A=a X)\) _is bounded away from 0 a.s. \([]\)._

For \(a\), let the outcome regression function be denoted by

\[_{a}(X)=(Y^{a} X)=(Y X,A=a).\]

Then, the pairwise CATE can be consequently defined as \(_{aa^{}}(X)=_{a}(X)-_{a^{}}(X)\) for any pair \(a,a^{}\). The _conditional counterfactual mean vector_\(:^{q}\) projects a unit characteristic onto a \(q\)-dimensional Euclidean space spanned by the outcome regression functions \(\{_{a}\}\):

\[(X)=[_{1}(X),,_{q}(X)]^{}. \]

Figure 1: Two instances in which the three clustering techniques result in distinct subgroups for the projected sample. The grey dotted diagonal line indicates no treatment effects.

Adjacent units in the above counterfactual mean vector space would have similar responses to a given set of treatments by construction. If all coordinates of a point \((X)\) are identical for a given \(X\), it indicates the absence of treatment effects on the conditional mean scale. Hence, conducting cluster analysis on the transformed space by \(\) allows for the discovery of subgroups characterized by a high level of within-cluster homogeneity in terms of treatment effects. Crucially, standard clustering theory is not immediately applicable here since the variable to be clustered is \(\), a set of the unknown regression functions that must be estimated. We let \(\{_{a}\}\) be some estimators of \(\{_{a}\}\). In Sections 3 and 4, we analyze the nonparametric _plug-in_ estimators for hierarchical and density-based causal clustering, respectively, where we estimate each \(_{a}\) with flexible nonparametric methods and perform clustering based on \(=(_{1},,_{q})^{}\).

It is worth noting that \(\) can be easily customized for a specific need through reparametrization, without affecting our subsequent results. For example, it is possible that the difference in regression functions may be more structured and simple than the individual components (e.g., 13; 35). In this case, a parametrization such as \(=(_{2}-_{1},_{3}-_{1},)\) could render our clustering task easier by allowing us to harness this nontrivial structure (e.g., smoothness or sparsity) (see 39, Section 2).

**Notation.** We use the shorthand \(_{(i)}=(X_{i})\) and \(_{(i)}=(X_{i})=[_{1}(X_{i}),..., _{q}(X_{i})]^{}\). We let \(\|x\|_{p}\) denote \(L_{p}\) norm for any fixed vector \(x\). For a given function \(f\) and \(r\), we use the notation \(\|f\|_{,r}=[(|f|^{r})]^{1/r}=[|f(z)|^{r }d(z)]^{1/r}\) as the \(L_{r}()\)-norm of \(f\). We use the shorthand \(a_{n} b_{n}\) to denote \(a_{n}b_{n}\) for some universal constant \(>0\). Further, for \(x^{q}\) and any real number \(r>0\), we let \((x,r)\) denote an open ball centered at \(x\) with radius \(r\) with respect to \(L_{2}\) norm, i.e., \((x,r)=\{y^{q}:\|x-y\|_{2}<r\}\) and use the notation \(}(x,r)\) for the closed ball. Lastly, we use the symbol \(\) to denote equivalence relation between two notationally distinct quantities, especially when introducing a simplified notation.

## 3 Hierarchical Causal Clustering

Hierarchical clustering methods build a set of nested clusters at different resolutions, typically represented by a binary tree or dendrogram. Consequently, they do not necessitate a predetermined number of clusters and allow for the simultaneous exploration of data across multiple granularity levels based on the user's preferred similarity measure. Moreover, hierarchical clustering can be performed even when the data is only accessible via a pairwise similarity function. There are two types of hierarchical clustering: agglomerative and divisive. The agglomerative approach forms a dendrogram from the bottom up, finding similarities between data points and iteratively merging clusters until the entire dataset is unified into a single cluster. The divisive approach employs a top-down strategy, whereby clusters are recursively partitioned until individual data points are reached. Here we only consider the agglomerative approach which is more common in practice . We remark that the similar argument in this section may be applicable to the divisive approach as well.

Consider a distance or dissimilarity between points, i.e., \(d:^{q}^{q}\). As in previous studies (e.g., 14; 16; 30), we extend \(d\) so that we can compute the distance, or _linkage_, between sets of points \(S_{1}() S_{1}\) and \(S_{2}() S_{2}\) in the conditional counterfactual mean vector space as \(D(S_{1},S_{2})\). There are three common distances between sets of points used in hierarchical clustering: letting \(N_{1}\) be the number of points in \(S_{1}\) and similarly for \(N_{2}\), we define the _single_, _average_, and _complete_ linkages by \(_{s_{1} S_{1},s_{2} S_{2}}d(s_{1},s_{2})\), \(N_{2}}_{s_{1} S_{1},s_{2} S_{2}}d(s_{1},s_{2})\), and \(_{s_{1} S_{1},s_{2} S_{2}}d(s_{1},s_{2})\), respectively. Single linkage often produces thin clusters while complete linkage is better at spherical clusters. Average linkage is in between. Causal clustering entails estimating the nuisance regression functions \(\{_{a}\}\), which necessitates the following assumption.

**Assumption A1**.: _Assume that either (i) \(\{_{a}\}\) and \(\{_{a}\}\) are contained in a Donsker class, or (ii) \(\{_{a}\}\) is constructed from a separate independent sample of same size._

Assumption A1 is required essentially because in our estimation procedure, we use the sample twice, once for estimating the nuisance functions \(\{_{a}\}\) and again for determining the clusters. One may use the full sample if we restrict the flexibility and complexity of each \(_{a}\) through the empirical process conditions, as in (i), which may not be satisfied by many modern machine learning tools. In order to accommodate this added complexity from employing flexible machine learning, we can instead use sample splitting (e.g., 12; 64), as in (ii). We refer the readers to Kennedy [32; 33; 34] for more details.

In the following proposition, we give an error bound of computing the set distance with the conditional counterfactual mean vector estimates.

**Proposition 3.1**.: _Let \(D\) denote the single, average, or complete linkage between sets of points, induced by the distance function such that \(d(x,y)\|x-y\|_{1}\). Then under Assumption A1, for any two sets \(S_{1},S_{2}\) in \(\{_{(i)}\}\) and their estimates \(_{1},_{2}\) with \(\{_{(i)}\}\),_

\[|D(S_{1},S_{2})-D(_{1},_{2})|_{a }\|_{a}-_{a}\|_{}.\]

A proof of the above proposition and all subsequent proofs can be found in Appendix. Proposition 3.1 suggests that in the agglomerative clustering we shall obtain identical cluster sets beyond a certain level of the dendrogram, where the distance between the closest pair of branches exceeds the outcome regression error. The result applies to a wide range of distance functions in Euclidean space.

In some problems it might be expensive to compute similarities between all \(n\) items to be clustered (i.e., \(O(n^{2})\) complexity). Eriksson et al.  proposed the hierarchical clustering of \(n\) items only based on an adaptively selected small subset of pairwise similarities on the order of \(O(n n)\). By virtue of Proposition 3.1 their algorithm is also applicable to our framework as long as \(_{a}\) is a consistent estimator for \(_{a}\) [see 16, Theorem 4.1].

In contrast to k-means clustering, it is not straightforward to analyze the performance of hierarchical clustering with respect to the true target clustering, because we build a set of nested clusters across various resolutions (a hierarchy) such that the target clustering is close to some pruning of that hierarchy. Moreover, conventional linkage-based algorithms may have difficulties in the presence of noise. Balcan et al.  proposed a novel robust hierarchical clustering algorithm capable of managing these issues. Their algorithm produces a set of clusters that closely approximates the target clustering with a specified error rate even in the presence of noise, and it is adaptable to an inductive setting, where only a small subset of the entire sample is utilized. We shall adapt their algorithm for causal clustering, and analyze the performance of our proposed algorithm.

We consider an inductive setting where we only have access to a small subset of points from a much larger data set. This can be particularly important when running an algorithm over the entire dataset is computationally infeasible. Suppose that \(\{C_{1},...,C_{k}\}\) is the target clustering, and that there exist \(N\) samples in total. Assuming we are given a random subset \(^{n}\) of size \(n\), \(n N\), consider a clustering problem \((^{n},l)\) in the conditional counterfactual mean vector space where each point \(^{n}\) has a true cluster label \(l()\{C_{1},...,C_{k}\}\). Further we let \(C()\) denote a cluster corresponding to the label \(l()\), and \(n_{C()}\) denote the size of the cluster \(C()\). To proceed, we define the following _good-neighborhood_ property to quantify the level of noisiness in our population distribution.

**Definition 3.1** (\((,)\)-Good Neighborhood Property for Distribution).: _For a fixed \(^{}^{q}\), let \((^{})=\{:C()=C(^{})\}\), i.e., a set whose label is equal to \(C(^{})\), and \(r_{^{}}=_{r}\{r:[(^{},r)] [(^{})]\}\). The distribution \(_{,}\) satisfies \((,)\)-good neighborhood property if \(_{,}=(1-)_{}+_{}\) where \(_{}\) is a probability distribution that satisfies_

\[_{}\{(^{},r_{^{}}) (^{})\},\]

_and \(_{}\) is a valid probability distribution._

The good-neighborhood property in Definition 3.1 is a distributional generalization of both the \(\)-strict separation and the \(\)-good neighborhood property from Balcan et al. . \(,\) can be viewed as noise parameters indicating the proportion of data susceptible to erroneous behavior. Next, we assume the following mild boundedness conditions on the population distribution and outcome regression function.

**Assumption A2**.: \(\|\|_{2},\|\|_{2} B\) _for some finite constant \(B\) a.s. \([]\)._

**Assumption A3**.: \(_{,}\) _in Definition 3.1 has a bounded Lebesgue density._

In the next theorem, we analyze the inductive version of the robust hierarchical clustering [5, Algorithm 2] in the causal clustering framework. We prove that when the data satisfies the good neighborhood properties, the algorithm achieves small error on the entire data set, requiring only a small random sample whose size is independent of that of the entire data set.

**Theorem 3.2**.: _Suppose that \(^{N}\) consists of \(N\) i.i.d samples from \(_{,}\) that satisfies the \((,)\)-good neighborhood property in Definition 3.1. For \(n N\), consider a random subset \(^{n}=\{_{(1)},,_{(n)}\}^{N}\) and its estimates \(}^{n}=\{_{(1)},,_{(n)}\}\) in which clustering to be performed. Let \(=_{a}\|_{a}-_{a}\|_{}\), and for any \(_{N}(0,1)\), define_

\[^{}=+O(}}),^{}=+O(}}),\] \[=O(+(})+()}).\]

_Then under Assumptions A1,A2, and A3, as long as the smallest target cluster has size greater than \(12(^{}+^{}+)N\), the inductive robust hierarchical clustering  on \(}^{n}\) with \(n=(+,^{})} +,^{})})\) produces a hierarchy with a pruning that has error at most \(^{}+\) with respect to the true target clustering with probability at least \(1--2_{N}\)._

The main implication of Theorem 3.2 is that, in essence, the natural misclassification error \(\) from the \(\)-good neighborhood property has increased by \(O_{}(_{a}\|_{a}-_{a}\|_ {})\) due to the costs associated with causal clustering.

## 4 Density-based Causal Clustering

The idea of density-based clustering was initially proposed as an effective algorithm for clustering large-scale, noisy datasets . The density-based methods work by identifying areas of high point concentration as well as regions of relative sparsity or emptiness. It offers distinct advantages over other clustering techniques due to their adeptness in handling noise and capability to find clusters of arbitrary sizes and shapes. Further, it does not require a-priori specification of number of clusters. Here, we focus on the level-set approach [see 50, and the references therein].

With a slight abuse of notation, we let \(P\) be the probability distribution of \(\) to distinguish it from the observational distribution \(\), and \(p\) be the corresponding Lebesgue density. We also let \(K\) denote a valid _kernel_, i.e., a nonnegative function satisfying \( K(u)du=1\). We construct the oracle kernel density estimator \(_{h}\) with the bandwidth \(h>0\) as

\[_{h}(^{})=_{i=1}^{n}}K( -^{}}{h}),\]

for \(^{}^{q}\). Then we define an average oracle kernel density estimator by \((}) p_{h}\) and the corresponding upper level set by \(L_{h,t}=\{:p_{h}()>t\}\). Suppose that for each \(t\), \(L_{h,t}\) can be decomposed into finitely many disjoint sets: \(L_{h,t}=C_{1} C_{l_{t}}\). Then \(_{t}=\{C_{1},...,C_{l_{t}}\}\) is the _level set clusters_ of our interest at level \(t\).

With regard to the analysis of topological properties of the distribution \(P\), the upper level set of \(p_{h}\) plays a role akin to that of the upper level set of the true density \(p\), yet it presents various advantages, as indicated in previous studies [e.g., 19, 37, 50, 60]; \(p_{h}\) is well-defined even when \(p\) is not, \(p_{h}\) provides simplified topological information, and the convergence rate of the kernel density estimator with respect to \(p_{h}\) is faster than with \(p\). For such reasons, we typically target the level set \(L_{h,t}\) induced from \(p_{h}\) in lieu of that from \(p\) [see, e.g., 38, Section 2].

When each \(_{(i)}\) is known (or has it been observed), the level sets could be estimated by computing \(_{h,t}=\{:_{h}()>t\}\). Specifically, for each \(t\) we let \(}_{t}=\{:_{h}()>t\}\), and construct a graph \(G_{t}\) where each \(_{(i)}}_{t}\) is a vertex and there is an edge between \(_{(i)}\) and \(_{(j)}\) if and only if \(\|_{(i)}-_{(j)}\|_{2} h\). Then the clusters at level \(t\) are estimated by taking the connected components of the graph \(G_{t}\), which is referred to as a _Rips graph_. _Persistent homology_ measures how the topology of \(R_{t}\) varies by the value of \(t\). See, for example, Bobrowski et al. , Fasy et al. , Kent et al.  more information on the algorithm and its theoretical features.

However, in our causal clustering framework, the oracle kernel density estimator \(_{h}\) is not computable since we do not observe each \(_{(i)}\). Thus we construct a plug-in version of the kernel density estimator:

\[_{h}(^{})=_{i=1}^{n}}K( _{(i)}-^{}}{h}),\]

with estimates \(\{_{(i)}\}\). Then we target the corresponding level set \(_{h,t}=\{:_{h}()>t\}\). To account for the added complications in estimating \(_{h,t}\), we introduce the following regularity conditions on the kernel \(K\), along with the bounded-density condition from Assumption A3 on the distribution \(P\).

**Assumption A3**.: \(p\) _is bounded a.s. \([P]\)._

**Assumption A4**.: _The kernel function \(K\) has a support on \((0,1)}\). Moreover, it is Lipschitz continuous with constant \(M_{K}\), i.e., for all \(x,y^{q}\), \(|K(x)-K(y)| M_{K}\|x-y\|_{2}\)._

The Hausdorff distance is a common way of measuring difference between two sets that are embedded in the same space. In what follows, we define the Hausdorff distance for any two subsets in Euclidean space.

**Definition 4.1** (Hausdorff Distance).: _Consider sets \(S_{1},S_{2}^{q}\). We define the Hausdorff distance \(H(S_{1},S_{2})\) as_

\[H(S_{1},S_{2})=\{_{x S_{1}}_{y S_{2}}\|x-y\| _{2},_{y S_{2}}_{x S_{1}}\|x-y\|_{2}\}.\]

Note that the Hausdorff distance can be equivalently defined as

\[H(S_{1},S_{2})=\{ 0:\,S_{1} S_{2,}S_{2} S_{1,}\},\]

where for \(i=1,2\), \(S_{i,}:=\{y^{q}:x S_{i}\|x-y\|_{2}\}\).

To estimate the target level set \(L_{t,h}=\{p_{h}>t\}\) using the estimator \(_{t,h}=\{_{h}>t\}\), we normally assume that the function difference \(\|_{h}-p_{h}\|_{}\) is small. To apply this condition to the set difference \(H(L_{t,h},_{t,h})\), we have to ensure that the target level set \(L_{t,h}\) does not change drastically when the level \(t\) perturbs. We formalize this notion as follows.

**Definition 4.2** (Level Set Stability).: _We say that the level set \(L_{t,h}=\{w^{q}:\,p_{h}(w)>t\}\) is stable if there exists \(a>0\) and \(C>0\) such that, for all \(<a\),_

\[H(L_{t-,h},L_{t+,h}) C.\]

The next theorem shows provided that the target level set \(L_{h,t}\) is stable in the sense of Definition 4.2, our level set estimator \(_{h,t}\) is close to the target level set \(L_{h,t}\) in the Hausdorff distance.

**Theorem 4.1**.: _Suppose that \(L_{h,t}\) is stable and let \(H(,)\) be the Hausdorff distance between two sets. Let the bandwidth \(h\) vary with \(n\) such that \(\{h_{n}\}_{n}(0,h_{0})\) and_

\[_{n}))_{+}}{nh_{n}^{q}}<.\]

_Then, under Assumptions A1, A2, A3\({}^{}\), and A4, we have that with probability at least \(1-\),_

\[H(_{h,t},L_{h,t})))_{+}+(2/ )}{nh_{n}^{q}}}+^{q+1}}\{_{a}\| {}_{a}-_{a}\|_{1}+},\,h_{n}\}.\]

The above theorem ensures that the estimated level sets in the causal clustering framework do not significantly deviate from \(L_{h,t}\), provided that the error of \(_{a}\) remains small. As a consequence, we show that causal clustering may also be accomplished via level-set density-based clustering, albeit at the expense of estimating the nuisance regression functions for the outcome process. The bandwidth \(h\) may be selected either by minimizing the error bounds derived from Theorem 4.1 or by employing data-driven methodologies (e.g., 38, Remark 5.1).

## 5 Empirical Analyses

### Simulation Study

Here, we explore finite-sample properties of our proposed plug-in procedures via simulation. In particular, we investigate the effect of nuisance estimation on the performance of causal clustering to empirically validate our theoretical findings in Sections 3 and 4.

For hierarchical causal clustering, we use the simulation setup akin to that of Kim et al. . Letting \(n=2500\), we randomly pick \(10\) points in a bounded hypercube \(^{3}\): \(\{c_{1}^{*},...,c_{10}^{*}\}\), and assign roughly \(n/10\) points following truncated normal distribution to each Voronoi cell associated with \(c_{j}^{*}\); these are our \(\{_{(i)}\}\). Next, we let \(_{a}=_{a}+\) with \( N(0,n^{-})\), which ensures that \(\|_{a}-_{a}\|=O_{}(n^{-})\). Following Balcan et al. , by repeating simulations \(100\) times, we compute classification error as a proxy of the clustering performance using different values of parameter \(\) fixing the value of \(\) and \(\). The results are presented in Figure 2 (a) & (b) with standard deviation error bars. The simulation result supports our finding in Theorem 3.2, indicating that the price we pay for the proposed hierarchical causal clustering is inflated \(\) due to the nuisance estimation error.

For density-based causal clustering, we utilize the toy example from Fasy et al. , originally used to illustrate the cluster tree. We consider a mixture of three Gaussians in \(^{2}\). Then, roughly \(n/3\) points for each of the three clusters are generated, which are our \(\{_{(i)}\}\). Similarly as before, we let \(_{a}=_{a}+\) with \( N(0,n^{-})\). Next, letting \(h=0.01\), we compute \(_{h}\) and \(_{h}\), and the corresponding level sets \(L_{h,t}\) and \(_{h,t}\) for different values of \(t\). For each \(n\), we calculate the mean Hausdorff distance between \(_{h,t}\) and \(L_{h,t}\) through \(100\) repeated simulations, and present the results

Figure 3: (a) Histogram of the true CATE in the test set. In the original study , individuals with zero treatment effects are assigned to the label \(L=0\). (b) The result of density-based causal clustering. Units in Cluster C1 appear to have higher baseline risk (\(_{0}\)). (c) We observe that points in Clusters C1 and C2 are more concentrated around the right upper area (larger \(_{0},_{1}\)) and the lower left area (smaller \(_{0},_{1}\)), respectively.

Figure 2: (a), (b): The y-axis represents classification error from hierarchical (causal) clustering, where we fix \(=0.01,0.1\) and vary \(\). (c), (d): The y-axis represents the average of \(H(_{h,t},L_{h,t})\) from density-based (causal) clustering, where we fix \(t=0.05,0.1\) and vary \(n\).

in Figure 2 (c) & (d) with error bars. Again, the results corroborate the conclusion from Theorem 4.1 that the cost of causal clustering is associated with the nuisance estimation error.

### Case Study

In this section, we illustrate our method through two case studies. We use semi-synthetic data on the voting study and real-world data on employment projections.

**Voting study**. Nie and Wager  considered a dataset on the voting study originally used by Arceneaux et al. . They generated synthetic treatment effects to render discovery of heterogeneous treatment effects more challenging. We use the same setup as Nie and Wager [46, Chapter 2], where we have binary treatments, binary outcomes, and \(11\) pretreatment covariates. While Nie and Wager  specifically focused on accurate estimation of the CATE, here we aim to illustrate how the proposed method can be used to uncover an intriguing subgroup structure. We randomly chose a training set of size \(13000\) and a test set of size \(10000\) from the entire sample. Then we estimate \(\{_{(i)}\}\) using the cross-validation-based Super Learner ensemble  to combine regression splines, support vector machine regression, and random forests on the training sample, and perform the density-based causal clustering on the test sample using DeBaC1 function in TDA R package .

In Figure 3-(b), we see two clusters in our conditional counterfactual mean vector space that are clearly separable from each other, one with nearly zero subgroup effect (Cluster C2) and the other with negative effect (Cluster C1). They correspond to the two largest branches at the bottom of the tree (Figure 3-(c)). Roughly 4% of the points are classified as noise. Interestingly, units in Cluster C1 appear to have higher baseline risk \(_{0}\) than Cluster C2. This is indeed more clearly noticeable in

Figure 4: The estimated causal clusters on two principal-component hyperplanes with axes representing the first and second, second and third principal components in the conditional counterfactual mean vector space, respectively.

Figure 5: The density plots of the pairwise CATE of six other education levels relative to the doctoral degree across clusters. We observe a substantial degree of effect heterogeneity. The red dashed vertical lines denote the zero CATE.

Figure 3-(c); Clusters C1 and C2 have a higher concentration of units in the right upper area (larger \(_{0},_{1}\)) and the lower left area (smaller \(_{0},_{1}\)).

**Employment projection data**.

The dataset, obtained from the US Bureau of Labor Statistics (BLS), provides projected employment by occupation. Specifically, the dataset consists of projected 10-year employment changes (2018-2028) computed from the BLS macroeconomic model across various occupations. We have eight education levels (No formal education, High school, Bachelor's degree, etc.). Here, we aim to uncover subgroup structure in the effects of entry-level education on projected employment. Our data also include four covariates: baseline employment in 2018, median annual wage in 2019, work experience, and on-the-job training.

Again we randomly split the data into two independent sets and use the super learner ensemble to estimate the nuisance regression functions. We then find clusters using robust hierarchical causal clustering described in Section 3. Since we have multi-level treatments this time (\(q=8\)), for ease of visualization, in Figure 4 we present the resulting clusters in two-dimensional hyperplanes with axes representing the first and second, second and third principal components, respectively. We also present the density plots for some of the pairwise CATEs across clusters in Figure 5.

In Figure 4, we observe four distinct clusters which are quite clearly separable from each other on the principal component hyperplanes. It appears that some clusters show considerably different effects from the others (e.g., Cluster 3), as shown in Figure 5. Our findings indicate a substantial heterogeneity in the effects of entry-level education on projected employment.

## 6 Discussion

Causal clustering is a new approach for studying treatment effect heterogeneity that draws on cluster analysis tools. In this work, we expanded upon this framework by integrating widely-used hierarchical and density-based clustering algorithms, where we presented and analyzed the simple and readily implementable plug-in estimators. Importantly, as we do not impose any restrictions on the outcome process, the proposed methods offer novel opportunities for clustering with generic unknown pseudo outcomes.

There are some caveats and limitations which should be addressed. First, causal clustering plays a more descriptive and discovery-based than prescriptive role compared to other approaches. It enables efficient discovery of subgroup structures and intriguing subgroup features as illustrated in our case studies, yet will likely be less useful for informing specific treatment decisions. Understanding this trade-off is thus important, and we recommend using our methods in conjunction with other approaches. Nonetheless, the clustering outputs could be potentially utilized as an useful input for subsequent learning tasks, such as precision medicine or optimal policy. Next, our theoretical findings show that when the nuisance regression functions \(\{_{a}\}\) are modeled nonparametrically, the clustering performance essentially inherits from that of \(\{_{a}\}\). The convergence rate of \(_{a}\) can be arbitrarily slow as the dimension of the covariate space increases. Kim et al.  addressed this issue by developing an efficient semiparametric estimator that achieves the second-order bias, and so can attain fast rates even in high-dimensional covariate settings . In future work, we aim to develop more efficient semiparametric estimators for hierarchical and density-based causal clustering. Extension to other robust clustering methods, such as hierarchical density-based clustering , would be a promising direction for future research as well. Lastly, our proposed methods are currently limited to the standard identification strategy under the no-unmeasured-confounding assumption, which is typically vulnerable to criticism [e.g., 28, Chapter 12]. To widen the breadth of the causal clustering framework, we will also be exploring extensions to other identification strategies, such as instrumental variable, mediation, and proximal causal learning.

## 7 Broader Impact

The proposed method provides a general framework for causal clustering that is not specifically tailored to any particular application, thereby reducing the potential for unintended societal or ethical impacts. Nonetheless, it is important to note that the identified subgroup structure was constructed entirely based on treatment effect similarity, without accounting for fairness or bias.

Acknowledgements

This work was partially supported by a Korea University Grant (K2407471) and the National Research Foundation of Korea (NRF) grant funded by the Korea governement (MSIT)(No. NRF-2022MJ36A1063595). This work was also partially supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) [NO.RS-2021-II11343, Artificial Intelligence Graduate School Program (Seoul National University)] and the New Faculty Startup Fund from Seoul National University. Part of this work was completed while Kwangho Kim was a Ph.D. student at Carnegie Mellon University.