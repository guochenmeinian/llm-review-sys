# Recurrent neural networks: vanishing and exploding gradients are not the end of the story

Nicolas Zucchet

Department of Computer Science

ETH Zurich

nzucchet@ethz.ch &Antonio Orvieto

ELLIIS Institute Tubingen

MPI for Intelligent Systems

Tubingen AI Center

antonio@tue.ellis.eu

###### Abstract

Recurrent neural networks (RNNs) notoriously struggle to learn long-term memories, primarily due to vanishing and exploding gradients. The recent success of deep state-space models (SSMs), a subclass of RNNs, to overcome such difficulties challenges our theoretical understanding. In this paper, we delve into the optimization challenges of RNNs and discover that, as the memory of a network increases, changes in its parameters result in increasingly large output variations, making gradient-based learning highly sensitive, even without exploding gradients. Our analysis further reveals the importance of the element-wise recurrence design pattern combined with careful parametrizations in mitigating this effect. This feature is present in deep SSMs, as well as in other architectures, such as LSTMs. Overall, our insights provide a new explanation for some of the difficulties in gradient-based learning of RNNs and why some architectures perform better than others.

Recurrent neural networks [RNNs; 1; 2] have long been the canonical architecture for modeling temporal data [3; 4]. However, they are notoriously difficult to train on long sequences, as error signals flowing backward in time tend to either vanish or explode [5; 6; 7; 8]. Attention mechanisms , as featured in transformers , address these issues by enabling direct token-to-token communication, considerably simplifying signal propagation across long time intervals. Yet, their superior performance comes with increased computational and memory costs, due to their quadratic scaling in the sequence length. This limitation has motivated significant research aimed at making transformers more efficient [11; 12; 13; 14; 15].

A promising line of research in this direction involves a new type of linear recurrent networks known as deep state-space models [SSMs; 16; 17; 18; 19; 20; 21; 22]. These models trade expressivity for faster training speed, and they have been shown to be particularly effective at capturing long-range dependencies. In this paper, we wonder whether this effectiveness can be solely attributed to their ability to avoid vanishing and exploding gradients. The simplicity of such models presents an opportunity for in-depth theoretical analysis. We focus on signal propagation within these models.

After reviewing classical results on recurrent neural networks in Section 1, we demonstrate that they can suffer from an understudied problem: as the recurrent network encodes longer memories, the network's activity becomes increasingly sensitive to changes in its parameters, even when its dynamics remains stable. In Section 3, we then show that SSMs, as well as other architectures such as LSTMs, are well equipped to mitigate this issue. We then analyze a simple teacher-student task (Section 4). This task already reveals the remarkable complexity underlying the learning of linear recurrent networks and enables us to verify empirically our theory. Finally, we discuss how our findings extend to more realistic scenarios (Section 5), both in terms of architectures and data. Overall, our paper provides theoretical insights into the training of recurrent neural networks, an area where such analysis is rare. While vanishing and exploding gradients are well-known challenges, our results demonstrate that this is not the end of the story - there exists an additional layer of complexity beyond them.

Vanishing and exploding gradients

Let us first introduce the notations we will be using throughout the rest of the paper. We consider a recurrent neural network with hidden state \(h_{t}\), update function \(f_{}\) parametrized by \(\), and input sequence \((x_{t})\). The average performance of the network is measured by a loss \(L\). We have

\[h_{t+1}=f_{}(h_{t},x_{t+1})\ \ \ \ L=[_{t=1} ^{T}L_{t}(h_{t})].\] (1)

The gradient of the instantaneous loss \(L_{t}\) with respect to the parameters \(\) is then equal to

\[L_{t}}{}=}{ h_{t }}h_{t}}{}=}{ h_{t }}_{t^{} t}h_{t}}{h_{t^{}}} }{}(h_{t^{}-1},x_{t^{}})\] (2)

In the equation above, we used \(\) to denote partial derivatives and \(\) for total derivatives. Using this notation enables us to distinguish between \(_{h_{t}}L_{t}\), which corresponds to the error backpropagated from the current loss term to the hidden state through the readout function, and \(_{h_{t}}L\), which accumulates the errors that are backpropagated through the future hidden state values. In particular, \(_{h_{t}}L=_{h_{t}}L_{t}\) and \(_{h_{t}}L=_{h_{t}}L_{t}(h_{t})+_{t^{}>t}_{h_{t}}L_{t^{}}(h_{t^{}})\). When stacking several recurrent layers on top of each other, \(_{h_{t}}L\) corresponds to the current error being backpropagated to the hidden state \(h_{t}\) through the hierarchy of the network and \(_{h_{t}}L\) to future error signals backpropagated through the recurrence.

Early work  highlighted the difficulty for gradient descent to make recurrent neural networks remember past inputs that will later be useful to produce a desired behavior. This is due to the fact that error signals flowing backward in time tend to either explode or vanish. The key quantity is

\[h_{t}}{h_{t^{}}}=_{i=t^{}}^{t-1} }{ h_{i}}=_{i=t^{}}^{t-1}}{ h}(h_{i},x_{i+1}).\] (3)

One can remark that this quantity exponentially converges to 0 when the spectral radius of the Jacobian \(_{h}f_{}\) is upper bounded by a constant strictly smaller than 1, and can exponentially explode if there exists some component bigger than 1. The error signal at time \(t\) backpropagated to time \(t^{}\) behaves similarly, as \(_{h_{t^{}}}L_{t}=_{h_{t}}L_{t}\,_{h_{t}^{ }}h_{t}\). Gradient-based learning of long-term memories is thus difficult: the contribution of past hidden states to the current loss becomes either negligible or predominant as the time span considered increases.

Since then, the analysis has been refined [6; 7; 8] and the development of recurrent architectures has mostly been driven by the desire to solve this pathological issue. Most famously, the LSTM  unit, and later on the GRU , solve this problem by using memory neurons that facilitate direct information storage and retrieval, and thus facilitate error backpropagation. Other approaches to solving this problem, to name a few, involve gradient clipping [24; 8], activity normalization [25; 26; 27], careful weight initialization [28; 29] or enforcing architectural constraints such as hierarchical processing [30; 31], orthogonal weight matrices [32; 33; 34] and oscillations [35; 36; 37].

## 2 The curse of memory

According to common deep learning wisdom, it is often believed that solving the vanishing and exploding gradients problem enables recurrent neural networks to learn long-term dependencies. We challenge this view and question: is solving those issues really enough to ensure well-behaved loss landscapes? We answer negatively by showing that gradients can explode as the memories of the network are kept for longer, even when the dynamics of the network remains stable.

### Intuition

Recurrent neural networks have something special: the very same update function \(f_{}\) is applied over and over. Therefore, modifying the parameters \(\) will not only influence one update, as changing the weights of a given layer in a feedforward neural network would, but all of them. As the memory of the network gets longer, the hidden states keep a trace of the effects of more updates. Hidden states thus become increasingly sensitive to parameter changes. This is the _curse of memory_. We borrow the term from [38; 39], although we use it in a different context, and note that Martens and Sutskever  hypothesized that such a phenomenon could arise in RNNs and hinder their optimization.

Let us formalize our intuition by considering the sensitivity of the hidden state \(h_{t}\) on the parameters \(\):

\[h_{t}}{}=_{t^{} t}h_{t}}{h_{t^{}}}}{}(h_{t^{ }-1},x_{t^{}}).\] (4)

When information stays in the network's memory for longer, the number of non-negligible Jacobian \(_{h_{t^{}}}h_{t}\) terms increases. As a result, the magnitude of this sensitivity increases when the network encodes longer-term dependencies, and learning \(\) becomes trickier. It is crucial to observe that this phenomenon arises even when exploding gradients are removed from the picture by constraining the eigenvalues of the recurrent Jacobian to be smaller than one and ensuring that the network dynamics remains stable. The rest of this section will be dedicated to studying this behavior quantitatively.

### Signal propagation in linear diagonal recurrent neural networks

We study how hidden state and gradient magnitudes evolve as the network encodes longer-term dependencies. Ideally, these quantities do not vanish or explode, as it improves the conditioning of the loss landscape  and eases optimization [42; 43]. We operate under the following assumptions:

1. **Linear diagonal recurrent neural networks**. We restrict ourselves to update functions of the form \(f_{}(h_{t},x_{t+1})= h_{t}+x_{t+1}\) with \(\) a vector of the size of \(h_{t}\) and \(\) the element-wise product. For ease of exposition, we present results for real-valued \(\) here; see Appendix B.2 for the complex-valued setting. While this assumption is strong, it allows us to identify some crucial mechanisms and models like S4 , S5  and LRUs  satisfy it. We later show that our analysis can model some features of more sophisticated networks. Note that we do not consider the input and readout mappings usually featured in recurrent layers as they are feedforward layers and signal propagation within them is already well understood [e.g., 44; 45].
2. **Infinite time horizon**. We consider infinite sequences and initialize the network dynamics at \(t_{0}=-\). It simplifies our calculations while being a reasonable assumption when the sequences considered are longer than the characteristic timescales of the dependencies we want to learn.
3. **Wide-sense stationarity**. We assume the different quantities that the network receives, which include the inputs \(x_{t}\), to be wide-sense stationary (WSS). A random process \(X_{t}\) is said to be WSS if its auto-correlation function is independent of time, that is, for all \(t\) and \(\), \(_{X}[X_{t+}X_{t}]=:R_{X}()\), where \(_{X}\) denotes the expectation over the data. It corresponds to assuming that the statistics of the data are invariant to time shifts. This is a standard assumption when analyzing stochastic processes . It keeps our calculations concise and does not qualitatively affect our conclusions (cf. Section 5). We discuss how to relax it in Appendix B.2.4.

Figure 1: **Optimization of recurrent neural networks gets harder as their memory increases.****A.** Evolution of the second moment of \(_{}h_{t}\) as a function of the recurrent parameter \(\) and of the input \(x\) auto-correlation decay rate \(\), when \(h_{t+1}= h_{t}+x_{t}\). As the memory of the network increases (\( 1\)), \(h_{t}\) becomes more sensitive to changes in \(\), particularly as the elements in the input sequence are more correlated (\( 1\)). The explosion of \(_{}h_{t}\) is faster than the one of \(h_{t}\), as highlighted with the grey line obtained for \(=1\). See Section 2.2 for more detail. **B, C.** Illustration of the phenomenon on the toy one-dimensional teacher-student task of Section 4.1, in which the teacher is parametrized by a real number \(^{*}\) and the student by a complex number \(\). In B, \(\) varies on the real axis, and it varies on the circle of radius \(^{*}\) parametrized by \(\) in C. The loss becomes sharper as information is kept longer in memory, making gradient-based optimization nearly impossible.

We are now equipped to analyze signal propagation in one recurrent layer, both in the forward and backward passes. We show that both hidden states and backpropagated errors explode as \(|| 1\).

Forward pass.Here, we are interested in understanding how the hidden state second moment \([h_{t}^{2}]\) evolves as a function of \(\) and of the input auto-correlation function \(R_{x}\). After a calculation that we defer to Appendix B.2, we obtain

\[[h_{t}^{2}]=}(R_{x}(0)+2 _{ 1}^{}R_{x}()).\] (5)

Importantly, this quantity goes to infinity as longer-term dependencies are encoded within the network, that is \(|| 1\). Additionally, the divergence speed depends on the input data distribution: it increases as consecutive time steps in the input distribution become more correlated (i.e., less of the \(R_{x}()\) terms are negligible). This behavior already highlights potential difficulties of gradient-based learning of deep neural networks containing linear recurrent layers as the variance of neural activity can become arbitrarily large, hindering learning abilities of deeper layers.

Backward pass.Let us first derive the gradient of the loss with respect to \(\). Using the chain rule we have \(_{}L=_{t}_{h_{t}}L\,_{}h_{t}\). We thus seek to understand how \(_{}h_{t}\) behaves. We remark that \(_{}h_{t+1}=_{}h_{t}+h_{t}\) so that \(_{}h_{t}\) is a low pass filtered version of the hidden state, which is itself a low pass filter version of the inputs. It therefore comes as no surprise that the second moment of \(_{}h_{t}\) diverges faster than the one of \(h_{t}\) when \(|| 1\). More precisely, we get

\[[(h_{t}}{})^{2} ]=}{(1-^{2})^{3}}(R_{x}(0)+2_{  1}^{}R_{x}())+)^{2}} (_{ 1}^{}R_{x}()).\] (6)

We plot the exact behavior of this quantity when the auto-correlation of \(x\) satisfies \(R_{x}()=^{||}\) on Figure 1 and refer the interested reader to Appendix B.2 for a derivation of Equation 6. More generally, the hidden state of the network, and thus its final output, becomes increasingly sensitive to changes in recurrent parameters as the network reaches the edge of dynamical stability (\(|| 1\)).

The last quantity we need to consider is the error that is backpropagated to the inputs \(x\) of the recurrent layer. It can be observed that the backward pass is dual to the forward pass in the sense that it is a recurrent process that receives backpropagated errors \(_{h_{t}}L\) and it runs in reverse time:

\[L}{x_{t}}=L}{h_{t}} { h_{t}}{ x_{t}}=L}{h_{t+1}}}{ h_{t}}+}= L}{h_{t+1}}+},\] (7)

in which we made use of \(_{x_{t}}h_{t}=1\). It follows that the analysis we did for the forward pass also holds here. Crucially, this implies that the explosion behavior will be most significant for the recurrent parameters rather than for potential input or readout weights.

### Extending the analysis to the non diagonal case

We now generalize our results to fully connected linear recurrent neural networks of the form \(h_{t+1}=Ah_{t}+x_{t}\). For the sake of the analysis, we assume that \(A\) is complex diagonalizable, that is there exists a complex-valued matrix \(P\) and a complex-valued vector \(\) such that \(A=P()P^{-1}\). Note that this occurs with probability one under random initialization of \(A\). In this case,

\[h_{t}=Ph_{t}^{}\ h_{t+1}^{}=( )h_{t}^{}+P^{-1}x_{t+1}\] (8)

and

\[h_{t}}{A}=}{ P}+}{ h_{t}^{} }h_{t}^{}}{}+}{ h_{t}^{}} h_{t}^{}}{P^{-1}}}{ A}.\] (9)

From the analysis above, we know that the dominating term in the limit \(|| 1\) among \(_{P}h_{t}\), \(_{}h_{t}\) and \(_{P}^{-1}h_{t}\) is \(_{}h_{t}\), as \(P\) and \(P^{-1}\) act as readout and input weights. Given that all other terms do not directly depend on the magnitude of \(\), we have that \(_{A}h_{t}_{h_{t}^{}}h_{t}\,_{ }h_{t}^{}\,_{A}\); cf. Appendix B.2.3 for formal statements. This has two consequences: First, the sensitivity of \(h_{t}\) on \(A\) will explode as longer memories are encoded and this directly comes from the eigenvalues of \(A\). Second, as each entry of \(A\) typically impacts all eigenvalues of the matrix, the explosion behavior will be distributed across all entries, whereas it was concentrated on the eigenvalues for the diagonal case. We will later observe that this has significant practical consequences and partly explains why fully connected linear RNNs are difficult to train. As a side note, we remark that enforcing the matrix \(A\) to be orthogonal solves vanishing and exploding gradient issues but these weights may remain sensitive to learn because of the curse of memory.

## 3 Mitigating the curse of memory

We have discussed the sensitivity of recurrent networks to parameter updates. Given this problem, how can it be mitigated? We show that recurrent networks with diagonal connectivity are particularly well suited for this purpose. Besides enabling control over the Jacobian and avoiding exploding gradients, they facilitate the mitigation of the curse of memory. We additionally highlight that deep state-space models and gated RNNs inherently incorporate such mechanisms.

### A solution: normalization and reparametrization

Both forward and backward passes explode as the network encodes longer memories. When \(h_{t+1}= h_{t}+x_{t+1}\), we argue that it is relatively straightforward to mitigate this effect. We aim to keep \([h_{t}^{2}]\), \([(_{}h_{t})^{2}]\) and \([(_{x_{t}}h_{t})^{2}]\) independent of \(\), similar to initialization schemes that maintain the magnitude of neural activity constant in deep networks [44; 45], regardless of the layer width [42; 47; 43].

Input normalization.A simple way to enforce \([h_{t}^{2}]\) to stay constant is to introduce a scaling factor \(()\) applied to the inputs a neuron receives, that satisfies \(()^{2}[h_{t}^{2}]=(1)\). Given that the backward propagation of output errors to inputs is dual to the forward pass, the role of \(\) in the backward pass will be similar. The value \(\) needs to take therefore both depends on the input distribution to normalize the forward pass, as well as on the output error distribution to normalize the backward pass. Perfect normalization is likely unrealistic, but some normalization can help, as shown in Figure 2.A.

Eigenvalue reparametrization.We are now left with keeping the gradient of the loss with respect to \(\) under control. Input normalization partly reduces the memory-induced exploding effect, but not entirely as the variance of \(_{}h_{t}\) is much larger than the one of \(h_{t}\) (cf. Fig.1.A). Reparametrization can close that gap. Indeed, if \(\) is parametrized by \(\), we have that \(_{}h_{t}=_{}h_{t}_{}\). Choosing a parameterization that is more and more granular as \(\) goes to 1 thus helps in keeping the magnitude of \(_{}h_{t}\) constant. Assuming \(\) is independent of \(\) for simplicity, achieving \([(_{}h_{t})^{2}]=(1)\) requires solving the differential equation \(()^{2}^{}()^{2}[(_{ }h_{t})^{2}]=1\). While deriving a universal optimal parametrization is again unrealistic due to dependency on the input distribution, reparametrization definitely helps, as shown in Figure 2.B. Figure 6 illustrates how it affects the loss landscape.

The case of complex numbers.We have not yet discussed the case \(\), relevant for SSMs such as S4 . We extend our analysis to complex-valued \(\) in Appendix B.3.2. Briefly, it reveals that changes in the magnitude of \(\) have a similar impact as in the real case, but this similarity does not extend to the angle. To keep the sensitivity on the angle constant, its parametrization must depend on the magnitude of \(\). However, doing so hurts learning, particularly far from optimality, as we exemplify in Appendix B.3.2. A key implication of this analysis is that the sensitivity of the hidden state on the angle of \(\) explodes as its magnitude approaches 1.

Figure 2: **Illustration of the effects of normalization and reparametrization.** It can effectively control the magnitude of **A**. \([h_{t}^{2}]\) and **B**. \([(_{}h_{t})^{2}]\) over all \(\) values when the input auto-correlation satisfies \(R_{x}()=^{\|\|}\) with \(=0\), but does not manage do to so for other type of distributions (\( 0\)). Here, we use \(()=}\), decouple it from \(\) when differentiating, and take \(=(-())\), as in . The grey line indicates the value the two quantities take without any normalization and reparametrization, when \(=1\).

### Several RNN architectures implicitly alleviate the curse of memory

Deep state-space models, as well as gated RNNs, feature some form of normalization and reparametrization which help keeping signal propagation under control. We discuss how below.

Deep state-space models.Deep SSMs were originally motivated as discretizations of the differential equation \(=Ah+Bx\). Naive discretization of the differential equation yields \(h_{t+1}=(+tA)h_{t}+tBx_{t+1}\) which already provides some input normalization. More elaborate discretization schemes, such as the zero-order hold, effectively reparametrize the \(A\) matrix, e.g. with \((tA)\). Here, diagonalization arises from computational efficiency and simplicity reasons . While such models can approximate any smooth mappings [48; 49], their expressivity remains limited . The next generation of these models, including Mamboa , incorporates input-dependent gates which modulate \(t\) depending on the input \(x_{t}\). The theory we developed above does not strictly apply to this setting as \(t\) is no longer constant. However, since the recurrence Jacobian remains diagonal, we expect the qualitative behaviors we analyzed to remain.

Gated RNNs.While the original motivation behind gated RNNs such as LSTMs  or GRUs  largely differs from the one of SSMs, they share similar mechanisms. In these networks, the memories stored in hidden neurons can be erased through a forget gate, and incoming inputs can selectively be written in memory through an input gate. Mathematically, this corresponds to hidden state updates of the form \(h_{t+1}=f_{t+1} h_{t}+i_{t+1} x_{t+1}\), with the forget \(f_{t+1}\) and input \(i_{t+1}\) gates being independent non-linear functions of \(x_{t+1}\) and \(h_{t}\). The forget gate is akin to \(\) and usually involves a sigmoid non-linearity, which has a similar effect as reparametrizing \(\) in the backward pass. The input gate can act as an input normalization depending on the initialization of the network or if is coupled to the forget gate as in the GRU (\(f_{t}=1-i_{t}\)) . Importantly, the gates here depend on the hidden states and thus make the Jacobian \(_{h_{t}}h_{t+1}\) non diagonal. Yet, we argue that these architectures still have a bias towards diagonality. Indeed, the contributions of the hidden state through the forget and input gates are indirect, and they can be ignored when the weights connecting the hidden states to the gates are small. Empirically, we find that GRUs lie in this regime at initialization, cf. Section D.2, so that our theory accurately captures signal propagation in GRUs. We additionally confirm that signal propagation is well behaved in gated RNNs in Section 5. In regimes in which this approximation does not hold, studying signal propagation requires a much more sophisticated analysis than the one we have done here .

## 4 A linear teacher-student analysis

We start our empirical analysis with a teacher-student task using linear recurrent networks . This is arguably the simplest setting in which one can train recurrent networks. Yet, as we shall see, it is already remarkably complex and captures some of the differences between different architectures observed in more realistic settings . It additionally makes it possible to control the characteristic time constants of the data, which is only possible with synthetic data.

Our investigation starts with the one-dimensional setting to provide an intuitive illustration of the consequences of the curse of memory on the loss landscape. We then address the general setting and observe that linear networks indeed suffer from the curse of memory, and that the remedies we studied in the last section are effective. We additionally find that diagonality greatly modifies the structure of the loss landscape and helps optimizers with adaptive learning rates to compensate for eventual increased sensitivities.

### The one-dimensional case

We first consider a student and a teacher following the one-dimensional dynamics \(h_{t+1}= h_{t}+x_{t+1}\), with complex-valued parameter \(\) for the student and \(^{*}\) for the teacher. For simplicity, we independently draw each \(x_{t+1}\) from a unit normal distribution (its autocorrelation function is \(R_{x}()=_{=0}\)) and note that other input distributions do not qualitatively change the results. The performance of the student is measured by a loss \(L\) that averages the per time-step losses \(L_{t}:=|h_{t}-h_{t}^{*}|^{2}\) over the entire sequence.

This simple model already captures two key difficulties of gradient-based learning of recurrent neural networks. In Figure 1, we plot the resulting loss landscape for different \(^{*}\) values, when \(\) evolves on the positive part of the real axis (Fig. 1.B) and when it evolves on the circle of radius \(|^{*}|\) in the complex plane (Fig. 1.C). We restrict \(\)s to have absolute values smaller than one: exploding gradients are out of the picture. Still, two difficulties for gradient-based learning appear here. On one side, vanishing gradients lead to flat loss regions that are hard to escape. On the other side, the loss sharpens as the student encodes longer memories because of the curse of memory. As a consequence, gradient-based optimization is extremely tedious, already in this simple example.

### Diagonal connectivity simplifies optimization

We now move to the general case in which the teacher evolves according to

\[h_{t+1}=Ah_{t}+Bx_{t+1}\ \ \ \ y_{t}=Ch_{t}+Dx_{t}.\] (10)

with \(h_{t}^{n}\), \(x_{t}\) drawn i.i.d. from \((0,1)\), \(A^{n n}\), \(B^{n 1}\), \(C^{1 n}\) and \(D^{1 1}\). Here both inputs and outputs are scalars.

Given the intuition we have developed so far, we expect fully connected linear recurrent neural networks to struggle solving the task when the teacher encodes longer memories, not only because of exploding gradients but also due to the curse of memory. Conversely, diagonality facilitates the eigenvalue reparametrization needed to avoid exploding gradients and make them better behaved. We run the following experiment to verify this intuition. We draw random teachers with hidden dimension \(n=10\) and transform the complex eigenvalues of the recurrent matrix \(A\) to have magnitudes close to a value \(_{0}\) that we control1. The larger \(_{0}\) is, the longer the memories encoded by the teacher are. We train a linear RNN, as well as an LRU , with hidden dimension \(64\) on this task. The students are therefore largely overparametrized. We chose the LRU architecture to represent deep SSMs due to its simplicity. This architecture uses input normalization and an exponential reparametrization of the eigenvalues, similar to what we analyze in Section 3. Both networks are trained using the Adam optimizer  and cosine annealing schedule for \(10\)k steps, on batches of size \(128\). To ensure that we are in the infinite sequence length regime, we take the sequences to be of length \(300\), that is three times longer than the characteristic time scale of the teacher. Learning rates are tuned separately for each method and training distribution. The results, which we plot in Figure 3.A, confirm our intuition: LRUs significantly outperform linear RNNs when long memories have to be learned, despite having 10 times fewer parameters.

Next, we wonder which design choices behind the LRU architecture are crucial to this performance improvement. To this end, we interpolate between a linear RNN and an LRU in the following way: First, we restrict the weight matrix of the linear RNN to a block diagonal with blocks of size 2. Each block can represent a complex number, so the network can represent 32 complex numbers in total. We additionally double the number of hidden neurons. Second, we change those \(2 2\) blocks (and their input and output weights) to be complex numbers. Finally, we add the \(\) input normalization and the exponential parametrization to obtain the final LRU architecture. We report the results of this experiment in Figure 3.B. Surprisingly, we find the gap comes from making the weight matrix block diagonal (\(4 4\) blocks are here enough, cf. Figure 11 in the Appendix). Interestingly, this change reduces the number of parameters the model has and slightly reduces the model expressivity. An explanation of this behavior is therefore likely to be related to the optimization properties of those models. We confirm this hypothesis in the next section.

Figure 3: **LRUs are better at replicating a teacher’s behavior than linear RNNs.****A.** As the teacher encodes longer dependencies (\(_{0} 1\)), the linear RNN struggles to reproduce it, but not the LRU. **B.** An ablation study (\(_{0}=0.99\)) reveals that this gap mainly comes from having a close to diagonal recurrent connectivity matrix. See Section 4.2 for more detail.

### On the importance of adaptive learning rates

So far, our results highlight the importance of having a close to diagonal recurrent connectivity matrix. In this section, we show that this parametrization alone does not mitigate any exploding behavior but modifies the structure of the loss landscape, making it possible for optimizers with adaptive learning rates to compensate for these behaviors.

To demonstrate this, we consider the Hessian of the loss:

\[^{2}L}{^{2}}=_{t}_{x}[ {h_{t}}{}L_{t}}{ h_{t}^{2 }}h_{t}}{}^{}+}{  h_{t}}^{2}h_{t}}{^{2}}].\] (11)

If the network can perfectly fit the target data, which is the case in the experiments above, the second term vanishes at optimality. We plot the Hessian at optimality in Figure 4.A and B for a standard linear recurrent network and one with complex diagonal parametrization, both with \(4\) hidden neurons (\(_{0}=0.99\)). We observe that the eigenvalue spectra are similar for the two architectures, both exhibiting large terms that are characteristic of the curse of memory, which makes learning with stochastic gradient descent almost impossible2. However, their structures differ. For the fully connected linear RNN, the top eigenvectors are distributed over many coordinates, whereas they are concentrated on a few coordinates for the complex diagonal one. This feature aids adaptive optimization [e.g., 57]: adapting to large curvature is much easier for Adam when the pathological directions are aligned to the canonical basis. This is what we observe in practice.

In Figure 4.C and D, we compare the effective learning rate used by Adam, which we compute by providing a vector of ones to the optimizer. For the dense linear RNN, the adaptive learning rates cannot compensate for the intricate coupling between components, resulting in small learning rates overall. Conversely, the sensitive directions of complex diagonal RNNs are concentrated on few

Figure 4: **Differences in learning abilities between fully connected and complex diagonal linear RNNs are due to a better structure of the loss landscape.****A, B.** Hessian of the loss at optimality, its 10 eigenvectors with greatest eigenvalues and its eigenspectra for a fully connected RNN (A) and a complex diagonal one (B). The spectra are almost the same. However, the top eigenvectors are concentrated on few coordinates for the complex diagonal one but not for the fully connected one. **C, D.** This structure makes it possible for Adam to efficiently deal with the extra sensitivity, as shown with the effective learning rates that it uses at the end of learning. For the fully connected one (C), Adam uses small learning rates to compensate for the sensitivity, whereas it can use larger ones for the complex diagonal one without hindering training stability. The horizontal grey line shows the learning rate used, which is here \(10^{-3}\).

parameters, which adaptive learning rates can compensate for. This leads to more targeted and overall larger learning rates, significantly speeding up learning. As a side note, the complex eigenvalues of the teacher come in conjugate pairs. However, during training, the complex values of the complex RNN are not conjugates of each other, thereby increasing Hessian diagonality. Finally, performing this analysis for the LRU, we find that the Hessian spectrum is similar to the diagonal setting and that the exploding dimensions of the Hessian are almost exclusively due to the angle parameter, consistently with our theoretical analysis; see Figure 9.A and C. The loss landscape for S4 model  can be qualitatively similar to the complex diagonal RNN or to the LRU, depending on which regime it is in; see Figure 9.B and D.

Before concluding this section, we investigate whether certain eigenvalue distributions can break the diagonal structure of the Hessian, thereby complicating optimization and increasing the need for eigenvalue reparametrization. In Appendix C.2, we provide a theoretical quantification of the intuitive result that more concentrated eigenvalues lead to less diagonal Hessian. Consequently, the performance gap between complex-valued diagonal networks and LRUs widens, although the former still greatly outperform their fully-connected counterpart (see Figure 10). An important corollary is that increasing the number of hidden neurons breaks the diagonal structure of the loss landscape, thus reducing the effectiveness of optimizers with adaptive learning rates in mitigating the curse of memory. This observation may explain why Orvieto et al.  reported a more substantial performance improvement from eigenvalue reparametrization than what we observe in our study (cf. block diagonal vs. LRU in Figure 3.B).

## 5 Signal propagation in deep recurrent networks at initialization

The ultimate goal of our theoretical quest is to gain insights into the training of practical recurrent network architectures. Specifically, we aim to verify whether the trends established theoretically and in controlled experiments hold in practice, by studying signal propagation at initialization on a realistic next-token prediction natural language processing task.

To that matter, we provide sentences as input to deep recurrent networks that contain four blocks and use a next-token prediction loss to measure their performance. Each block consists of a recurrent layer followed by a feedforward gated linear unit . By default, there are no normalization layers in this architecture. More details can be found in Appendix D.1. We empirically study how \([h_{t}^{2}]\) and \([(_{}L)^{2}]\) evolve when the characteristic time scale of the recurrent layers, controlled through \(_{0}\), increases. We compare three different recurrent layers: a complex-valued diagonal RNN (cRNN), a LRU and a GRU initialized with the Chrono initialization .

The results are consistent with our theory. Complex-valued RNNs suffer from the curse of memory and recurrent parameters grow faster to infinity than the rest as \(_{0}\) goes to 1. Perhaps more surprisingly,

Figure 5: **Signal propagation in deep recurrent networks at initialization is consistent with our theory.****A.**\([h_{t}^{2}]\) after the first and the fourth layer, as a function of the exponential decay parameter \(_{0}\), for complex-valued diagonal RNN (cRNN), LRU, and GRU recurrent layers. The input normalization present in the LRU and in the GRU effectively keeps neural activity constant across \(_{0}\) values. **B.** Comparison of the evolution of the loss gradient \([(_{}L)^{2}]\) for the different recurrent layers and specific groups of parameters. For the complex diagonal RNN, the gradients of all parameters explode and in particular the ones of the recurrent parameters, whereas only the ones of the angle of \(\) explode for the LRU, consistently with the theory. Error signal propagation in GRUs is under control: the magnitude of the gradients is independent of \(_{0}\). The GRU-specific parameters exhibit smaller gradients than the feedforward (ff) ones. **C.** Layer normalization keeps the overall gradient magnitude under control in cRNNs. Batch normalization yields similar results.

a finer grain analysis reveals that the gradient magnitude is independent of the layer; see Figure 13. LRUs almost perfectly mitigate exploding behaviors in the forward pass (Figure 5.A) as well as in the backward pass (Figure 5.B), except for the angle parameter, consistently with our previous analysis. We also wonder whether layer normalization can replace the input normalization and reparametrization of the LRU. We find that it mitigates the memory-induced gradient explosion at the macroscopic level (Figure 5.C), but it likely kills any learning signal for the smallest eigenvalues . Finally, the GRU manages to keep the gradient magnitude constant over different characteristic time constants, consistently with the intuition we developed in Section 3.2. Preliminary experiments revealed that same trends also hold for LSTMs.

The results presented above for the GRU align qualitatively with the intuition developed throughout the paper. We now consider how well our theory can quantitatively explain this behavior. The primary difference between our simple model and GRUs is that the \(\) values, referred to as forget gates in the GRU terminology, depend on both inputs and hidden states, and are therefore not constant. Interestingly, we find that GRUs almost behave like the diagonal linear RNNs we have focused on in this paper, particularly for slowly decaying recurrent neurons with high \(_{0}\) values (see Appendix D.2). Consequently, applying our theory as if this context-dependency does not exist only introduces minor approximation errors, which we confirm empirically in Appendix D.3. Given the similarity of the Chrono initialization to those used in modern architectures like Mamboa  and Hawk , we expect our theory to also serve as a good proxy for studying signal propagation in these models at initialization.

## 6 Conclusion

Vanishing and exploding gradients complicate the learning of recurrent networks, but solving these problems is not enough. We uncovered yet another difficulty of training such networks, which is rooted in their iterative nature and arises at the edge of dynamical stability. Reparametrizations and adaptive learning rates can effectively mitigate this behavior in practice, and diagonalizing the recurrence simplifies both. Our analysis additionally reveals the complexity of learning the angle of complex eigenvalues, which may explain why complex numbers were not found to be useful in most recent state-space model architectures .

A side finding of our study is the symbiosis between independent modules, which are here neurons and can be more generally small heads, with adaptive learning rate optimizers in linear recurrent networks. Such a design pattern has promising properties: it facilitates online learning  and compositional generalization , allows for high level of parallelization , and matches, at a high level, the modular organization of the cortex in cortical columns . Understanding how to increase the expressivity of small linear modules while keeping their great optimization properties constitutes a promising avenue for future research.

## Limitations

The theory we introduced, with its focus on signal propagation, only addresses the training dynamics of recurrent neural networks. Consequently, it does not provide insights into other important questions such as generalization abilities or memory capacities of these networks. The main assumption underlying this analysis is that the recurrence is both diagonal and linear. While this approach offers valuable insights, it can only approximate signal propagation in more sophisticated architectures. Given the simplicity of the analytical tools employed, there is little hope that this framework can be extended to more general settings without significant modifications.