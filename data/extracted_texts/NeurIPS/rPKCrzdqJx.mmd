# Regret Minimization in Stackelberg

Games with Side Information

 Keegan Harris

School of Computer Science

Carnegie Mellon University

Pittsburgh, PA 15213

keeganh@cs.cmu.edu

&Zhiwei Steven Wu

School of Computer Science

Carnegie Mellon University

Pittsburgh, PA 15213

zhiweiw@cs.cmu.edu

&Maria-Florina Balcan

School of Computer Science

Carnegie Mellon University

Pittsburgh, PA 15213

ninamf@cs.cmu.edu

###### Abstract

Algorithms for playing in Stackelberg games have been deployed in real-world domains including airport security, anti-poaching efforts, and cyber-crime prevention. However, these algorithms often fail to take into consideration the additional information available to each player (e.g. traffic patterns, weather conditions, network congestion), which may significantly affect both players' optimal strategies. We formalize such settings as _Stackelberg games with side information_, in which both players observe an external _context_ before playing. The leader commits to a (context-dependent) strategy, and the follower best-responds to both the leader's strategy and the context. We focus on the online setting in which a sequence of followers arrive over time, and the context may change from round-to-round. In sharp contrast to the non-contextual version, we show that it is impossible for the leader to achieve no-regret in the full adversarial setting. Motivated by this result, we show that no-regret learning is possible in two natural relaxations: the setting in which the sequence of followers is chosen stochastically and the sequence of contexts is adversarial, and the setting in which contexts are stochastic and follower types are adversarial.

## 1 Introduction

A _Stackelberg game_ is a strategic interaction between two utility-maximizing players in which one player (the _leader_) is able to _commit_ to a (possibly mixed) strategy before the other player (the _follower_) takes an action. While Stackelberg's original formulation was used to model economic competition between firms, Stackelberg games have been used to study a wide range of topics in computing ranging from incentives in algorithmic decision-making  to radio spectrum utilization . Perhaps the most successful application of Stackelberg games to solve real-world problems is in the field of security, where the analysis of _Stackelberg security games_ has led to new methods in domains such as passenger screening at airports , wildlife protection efforts in conservation areas , the deployment of Federal Air Marshals on board commercial flights , and patrol boat schedules for the United States Coast Guard .1

However in many real-world settings which are typically modeled as Stackelberg games, the payoffs of the players often depend on additional _contextual information_ which is not captured by the Stackelberg game framework. For example, in airport security the severity of an attack (as well as the "benefit" of a successful attack to the attacker) depends on factors such as the arrival and departure city of a flight, the whether there are VIP passengers on board, and the amount of valuable cargo on the aircraft. Additionally, there may be information in the time leading up to the attack attempt which may help the security service determine the type of attack which is coming . For instance, in wildlife protection settings factors such as the weather and time of year may make certain species ofwildlife easier or harder to defend from poaching, and information such as the location of tire tracks may provide context about which animals are being targeted. As a result, the optimal strategy of both the leader and the follower may change significantly depending on the side information available.

**Overview of our results.** In order to capture the additional information that the leader and follower may have at their disposal, we formalize such settings as _Stackelberg games with side information_. Specifically, we consider a setting in which a leader interacts with a sequence of followers in an online fashion. At each time-step, the leader gets to see payoff-relevant information about the current round in the form of a _context_. After observing the context, the leader commits to a mixed strategy over a finite set of actions, and the follower best-responds to both (1) the leader's strategy and (2) the context in order to maximize their utility. We allow the follower in each round to be one of \(K\)_types_. Each follower type corresponds to a different mapping from contexts, leader strategies, and follower actions to utilities. While the leader may observe the context before committing to their mixed strategy, they do not get to observe the follower's type until after the round is over. Under this setting, the goal of the leader is to minimize their _regret_ with respect to the best _policy_ (i.e. the best mapping from contexts to mixed strategies) in hindsight, with respect to the realized sequence of followers and contexts.

We show that in the fully adversarial setting (i.e. the setting in which both the sequence of contexts and follower types is chosen by an adversary), no-regret learning is not possible, even when the policy class is highly structured. We show this via a reduction from the problem of online linear thresholding, for which it is known that no no-regret learning algorithm exists. Motivated by this impossibility result, we study two natural relaxations: (1) a setting in which the sequence of contexts is chosen by an adversary and the sequence of follower types is chosen stochastically, and (2) a setting in which the sequence of contexts is chosen stochastically and the sequence of follower types is chosen by an adversary.

In the stochastic follower setting we show that the greedy algorithm (Algorithm 1), which estimates the leader's expected utility for the given context and plays the mixed strategy which maximizes their estimate, achieves no-regret as long as the total variation distance between their estimate and the true distribution is decreasing with time. We then show how to instantiate the leader's estimation procedure so that the regret of Algorithm 1 is \(O(\{K,A_{f}\})\), where \(T\) is the time horizon, \(K\) is the number of follower types, and \(A_{f}\) is the number of follower actions. In the stochastic context setting, we show the leader can obtain \(O(+K)\) regret by playing Hedge over a finite set of policies (Algorithm 2). An important intermediate result in both settings is that it is (nearly) without loss of generality to consider leader policies which map to a finite set of mixed strategies \(_{}\), given context \(\).2

Next, we extend our algorithms for both types of adversary to the setting in which the leader does not get to observe the follower's type after each round, but instead only gets to observe their action. We refer to this type of feedback as _bandit feedback_. Both of our extensions to bandit feedback make use of the notion of a _barycentric spanner_, a special basis under which bounded loss estimators may be obtained for all leader mixed strategies. In the bandit stochastic follower setting, we use the fact that in addition to being bounded, a loss estimator constructed using a barycentric spanner has low variance, in order to show that a natural extension of our greedy algorithm obtains \((T^{2/3})\) regret.We also make use of barycentric spanners in the (bandit) stochastic context setting, albeit in a different way. Here, our algorithm proceeds by splitting the time horizon into blocks, and using a barycentric spanner to estimate the leader's utility from playing according to a set of special policies in each block. We then play Hedge over these policies to obtain \((T^{2/3})\) leader regret.3 See Table 1 for a summary of our results.

**Related work.** Letchford et al.  consider the problem of learning the leader's optimal mixed strategy in the repeated Stackelberg game setting against a perfectly rational follower with an unknown payoff matrix. Peng et al.  study the same setting as Letchford et al. . They provide improved rates and prove nearly-matching lower bounds. Learning algorithms to recover the leader's optimal mixed strategy have also been studied in Stackelberg security games .

Our work builds off of several results established for online learning in (non-contextual) Stackelberg games in Balcan et al. . In particular, our results in Section 4.2 and Appendix C.2 may be viewed as a generalization of their results to the setting in which the payoffs of both players depend on an external context. Roughly speaking, Balcan et al.  show that it without loss to play Hedge over a finite set of _mixed strategies_ in order to obtain no-regret against an adversarially-chosen sequence of follower types. In order to handle the additional side information available in our setting, we instead play Hedge over a finite set of _policies_, each of which map to a finite set of (context-dependent) mixed strategies. However, the discretization argument is more nuanced in our setting. In particular, it is not without loss of generality to consider a finite set of policies. As a result, we need to bound the additional regret incurred by the leader due to the discretization. More recent work on learning in Stackelberg games provides improved regret rates in the full feedback  and bandit feedback  settings, and considers the effects of non-myopic followers  and followers who respond to calibrated predictions .

Lauffer et al.  study a Stackelberg game setting in which there is an underlying (probabilistic) state space which affects the leader's rewards, and there is a single (unknown) follower type. In contrast, we study a setting in which the sequence of follower types and/or contexts may be chosen adversarially. Sessa et al.  study a repeated game setting in which the players receive additional information (i.e. a context) at each round, much like in our setting. However, their focus is on repeated normal-form games, which require different tools and techniques to analyze compared to the repeated Stackelberg game setting we consider. Other work has also considered repeated normal-form games which change over time in different ways. In particular, Zhang et al. , Anagnostides et al.  study learning dynamics in time-varying game settings, and Harris et al.  study a meta-learning setting in which the game being played changes after a fixed number of rounds.

Finally, our problem may be viewed is a special case of the contextual bandit setting with adversarially-chosen utilities [28; 29; 24], where the learner gets to observe "extra information" in the form of the follower's type (Section 4) or the follower's action (Section 5). However, there is much to gain from taking advantage of the additional information and structure that is present in our setting. Besides having generally worse regret rates, another reason not to use off-the-shelf adversarial contextual bandit algorithms in our setting is that they typically require either (1) the learner to know the set of contexts they will face beforehand (the _transductive_ setting; Syrgkanis et al. [28; 29], Rakhlin and Sridharan ) or (2) for there to exist a small set of contexts such that any two policies behave differently on at least one context in the set (the _small separator_ setting; Syrgkanis et al. ). We require no such assumptions.

## 2 Setting and background

**Notation.** We use \([N]:=\{1,,N\}\) to denote the set of natural numbers up to and including \(N\) and \(()\) to denote the closure of the set \(\). \([a]\) denotes the \(a\)-th component of vector \(\), and \(()\) denotes the probability simplex over the set \(\). \((,)=|p(x)-q(x)|dx\) is the total variation distance between distributions \(\) and \(\), and \(_{t}[x]=[x|_{t}|\) is shorthand for the expected value of the random variable \(x\), conditioned on the filtration up to (but not including) time \(t\). All proofs may be found in the Appendix. Finally, while we present our results for general Stackelberg games with side information, our results are readily applicable to the special case of Stackelberg _security_ games with side information.

**Our setting.** We consider a game between a leader and a sequence of followers. In each round \(t[T]\), Nature reveals a context \(_{t}^{d}\) to both players.4 The leader moves first by playing some mixed strategy \(_{t}^{A}\) over a set of (finite) leader actions \(\), i.e., \(_{t}()\). The size of \(\)

  & Full Feedback & Bandit Feedback \\  Fully Adversarial & \((T)\) & \((T)\) \\  & (Section 3) & (Section 3) \\  Stochastic Followers, & \(O(\{K,A_{f}\})\) & \(O(K^{2/3}A_{f}^{2/3}T^{2/3}^{1/3}T)\) \\ Adversarial Contexts & (Section 4.1) & (Appendix C.1) \\  Stochastic Contexts, & \(O(+K)\) & \(O(KA_{f}^{1/3}T^{2/3}^{1/3}T)\) \\ Adversarial Followers & (Section 4.2) & (Appendix C.2) \\ 

Table 1: Summary of our results. Under bandit feedback, we consider a relaxed setting in which only the leaderâ€™s utility depends on the side information.

is \(A:=||\). Having observed the leader's mixed strategy, the follower _best-responds_ to both \(_{t}\) and \(_{t}\) by playing some action \(a_{f}_{f}\), where \(_{f}\) is the (finite) set of follower actions and \(A_{f}:=|_{f}|\).

**Definition 2.1** (Follower Best-Response).: _Follower \(f\)'s best-response to context \(\) and mixed strategy \(\) is \(b_{f}(,)_{a_{f}_{f}}_{a_{i} }[a_{l}] u_{f}(,a_{l},a_{f})\), where \(u_{f}:_{f}\) is follower \(f\)'s utility function. In the case of ties, we assume that there is a fixed and known ordering over actions which determines how the follower best-responds, i.e. if \(a>a^{}\) for \(a,a^{}_{f}\) then the follower will break ties between \(a\) and \(a^{}\) in favor of \(a\).5_ 
We allow for the follower in round \(t\) (denoted by \(f_{t}\)) to be one of \(K 1\)_follower types_\(\{^{(1)},,^{(K)}\}\) (where \(K T\)). Follower type \(^{(i)}\) is characterized by utility function \(u_{^{(i)}}:_{f}[ 0,1]\), i.e. given a context \(\), leader action \(a_{l}\), and follower action \(a_{f}\), a follower of type \(^{(i)}\) would receive utility \(u_{^{(i)}}(,a_{l},a_{f})\). We assume that the set of all possible follower types and their utility functions are known to the leader, but that the follower's type at round \(t\) is not revealed to the leader until _after_ the round is over. We denote the leader's utility function by \(u:_{f}\) and assume it is known to the leader. We often use the shorthand \(u(,,a_{f})=_{a_{l}}[a_{l}]  u(,a_{l},a_{f})\) to denote the leader's expected utility of playing mixed strategy \(\) under context \(\) against follower action \(a_{f}\). Follower \(f_{t}\)'s expected utility \(u_{f_{t}}(,,a_{f})\) is defined analogously.

A leader _policy_\(:\) is a (possibly random) mapping from contexts to mixed strategies. If the leader using policy \(_{t}\) in round \(t\) and observes context \(_{t}\), their strategy \(_{t}\) is given by \(_{t}_{t}(_{t})\).

**Definition 2.2** (Optimal Policy).: _Given a sequence of followers \(f_{1},,f_{T}\) and contexts \(_{1},,_{T}\), the strategy given by the leader's optimal-in-hindsight policy for context \(\) is \(^{*}()_{}_{t=1}^{T}u( ,,b_{f_{t}}(,))\{ _{t}=\}\)._

We measure the leader's performance against the optimal policy via the notion of _contextual Stackelberg regret_ (_regret_ for short).

**Definition 2.3** (Contextual Stackelberg Regret).: _Given a sequence of followers \(f_{1},,f_{T}\) and contexts \(_{1},,_{T}\), the leader's contextual Stackelberg regret is \(R(T):=_{t=1}^{T}u(_{t},^{*}(_{t}),b_{f_{t}}( _{t},^{*}(_{t})))-u(_{t},_{t},b_{f _{t}}(_{t},_{t}))\), where \(_{1},,_{T}\) is the sequence of mixed strategies played by the leader._

If an algorithm achieves regret \(R(T)=o(T)\), we say that it is a _no-regret_ algorithm. We consider three ways in which Nature can select the sequence of contexts/followers:

1. If the sequence of contexts (resp. follower types) are drawn i.i.d. from some fixed distribution, we say that the sequence of contexts (resp. follower types) are chosen _stochastically_.
2. If Nature chooses the sequence of contexts (resp. follower types) before the first round in order to harm the leader (possibly using knowledge of the leader's algorithm), we say that the sequence of contexts (resp. follower types) are chosen by a _non-adaptive adversary_.
3. If Nature chooses context \(_{t}\) (resp. follower \(f_{t}\)) before round \(t\) in order to harm the leader (possibly using knowledge of the leader's algorithm and the outcomes of the prior \(t-1\) rounds), we say that the sequence of contexts (resp. follower types) are chosen by an _adaptive adversary_.

Our impossibility results in Section 3 hold when both the sequence of contexts _and_ the sequence of follower types are chosen by either type of adversary. Our positive results in Section 4 hold when either the sequence of contexts _or_ the sequence of follower types are chosen by either type of adversary (and the other sequence is chosen stochastically). Our extension to bandit feedback (Section 5, where the leader only gets to observe the follower's best-response instead of their type) holds whenever one sequence is chosen by a non-adaptive adversary and the other sequence is chosen stochastically.

## 3 On the impossibility of fully adversarial no-regret learning

We begin with a negative result: no-regret learning is not possible in the setting of Section 2 if the sequence of contexts and the sequence of followers is chosen by an adversary. While this is not necessarily surprising given that Definition 2.3 allows for the optimal policy \(^{*}\) to be arbitrarily complex, we show that this result holds _even when the policy class to which \(^{*}\) belongs is highly structured._ We show this via a reduction to the online linear thresholding problem, for which it is known that no-regret learning is impossible.

**Online linear thresholding.** The online linear thresholding problem is a repeated two-player game between a learner and an adversary. Before the first round, an adversary chooses a _cutoff_\(s\) which is _unknown_ to the learner. In each round, the adversary chooses a point \(_{t}\) and reveals it to the learner. \(_{t}\) is assigned label \(y_{t}=1\) if \(_{t}>s\) and label \(y_{t}=-1\) otherwise. Given \(_{t}\), the learner makes a _guess_\(g_{t}\) (the probability they place on \(y_{t}=1\)), and receives utility \(u_{}(_{t},g_{t})=g_{t}\{y_{t}=1\}+(1-g_{t}) \{y_{t}=-1\}\). The learner gets to observe \(y_{t}\) after round \(t\) is over. The learner's policy \(_{t}:\) is a mapping from points in \(\) to guesses in \(\). The optimal policy \(^{*}\) makes guess \(^{*}(_{t})=1\) if \(_{t}>s\) and \(^{*}(_{t})=0\) otherwise. The learner's regret after \(T\) rounds is given by \(R_{}(T)=T-_{t=1}^{T}u_{}(_{t},g_{t})\), since the optimal policy achieves utility \(1\) in every round. In order to prove a lower bound on contextual Stackelberg regret in our setting, we make use of the following well-known lower bound on regret in the online linear thresholding setting (see e.g. ).

**Lemma 3.1**.: _Any algorithm suffers regret \(R_{}(T)=(T)\) in the online linear thresholding problem when the sequence of points \(_{1},,_{T}\) is chosen by an adversary._

**Theorem 3.2**.: _If an adversary can choose both the sequence of contexts \(_{1},,_{T}\) and the sequence of followers \(f_{1},,f_{T}\), no algorithm can achieve better than \((T)\) contextual Stackelberg regret in expectation over the internal randomness of the algorithm, even when \(^{*}\) is restricted to come from the set of linear thresholding functions._

The reduction from online linear thresholding proceeds by creating an instance of our setting such that the sequence of contexts \(z_{1},,z_{T}\) correspond to the sequence of points \(_{1},,_{T}\) encountered by the learner, and the sequence of follower types \(f_{1},,f_{T}\) correspond to the sequence of labels \(y_{1},,y_{T}\). We then show that a no-regret algorithm in the online thresholding problem can be obtained by using an algorithm which minimizes contextual Stackelberg regret on the constructed game instance as a black box. However this is a contradiction, since by Lemma 3.1 the online thresholding problem is not online learnable by any algorithm. See Figure 1 for a visualization of our reduction.

Intuitively, this reduction works because the adversary can "hide" information about the follower's type \(f_{t}\) in the context \(z_{t}\). However, there exists a family of problem instances in which learning this relationship between contexts and follower types as hard as learning the threshold in the online linear thresholding problem, for which no no-regret learning algorithm exists by Lemma 3.1.

## 4 Limiting the power of the adversary

Motivated by the impossibility result of Section 3, we study two natural relaxations of the fully adversarial setting: one in which the sequence of followers is chosen stochastically but the contexts are chosen adversarially (Section 4.1) and one in which the sequence of contexts is chosen stochastically but followers are chosen adversarially (Section 4.2). In both settings we allow the adversary to be adaptive.

An important structural results for both Section 4.1 and Section 4.2 is that for any context \(\), the leader incurs only negligible regret by restricting themselves to policies which map to mixed

Figure 1: Summary of our reduction from the online linear thresholding problem. At time \(t[T]\), (1.) the learner observes a point \(_{t}\), (2.) the learner takes a guess \(g_{t}\), and (3.) the learner observes the true label \(y_{t}\). Given a regret minimizer for our setting, we show how to use it in a black-box way (by constructing functions \(h_{1}\), \(h_{2}\), \(h_{3}\)) to achieve no-regret in the online linear thresholding problem.

strategies in some finite (and computable) set \(_{}\). In order to state this result formally, we need to introduce the notion of a _contextual best-response region_, which is a generalization of the notion of a best-response region in (non-contextual) Stackelberg games (e.g. [21; 5]).

**Definition 4.1** (Contextual Follower Best-Response Region).: _For follower type \(^{(i)}\), follower action \(a_{f}_{f}\), and context \(\), let \(_{}(^{(i)},a_{f})\) denote the set of all leader mixed strategies such that a follower of type \(^{(i)}\) best-responds to all \(_{}(^{(i)},a_{f})\) by playing action \(a_{f}\) under context \(\), i.e., \(_{}(^{(i)},a_{f})=\{:b_{ ^{(i)}}(,)=a_{f}\}\)._

**Definition 4.2** (Contextual Best-Response Region).: _For a given function \(:\{^{(1)},,^{(K)}\}_{f}\), let \(_{}()\) denote the set of all leader mixed strategies such that under context \(\), a follower of type \(^{(i)}\) plays action \((^{(i)})\) for all \(i[K]\), i.e. \(_{}()=_{i[K]}_{}( ^{(i)},(^{(i)}))\)._

For a fixed contextual best-response region \(_{}()\), we refer to the corresponding \(\) as the _best-response function_ for region \(_{}()\), as it maps each follower type to its best-response for every leader strategy \(_{}()\). We sometimes use \(^{(,)}\) to refer to the best-response function associated with mixed strategy \(\) under context \(\), and we use \(_{}\) to refer to the set of all best-response functions under context \(\). Note that \(|_{}| A_{f}^{K}\) for any context \(\). This gives us an upper-bound on the number of best-response regions for a given context.

One useful property of all contextual best-response regions is that they are convex and bounded polytopes. To see this, observe that every contextual follower best-response region (and therefore every contextual best-response region) is (1) a subset of \(^{d}\) and (2) the intersection of finitely-many half-spaces. While every \(_{}()\) is convex and bounded, it is not necessarily closed. If every contextual best-response region were closed, it would be without loss of generality for the leader to restrict themselves to the set of policies which map every context to an extreme point of some contextual best-response region. In what follows, we show that the leader does not "lose too much" (as measured by regret) by restricting themselves to policies which map to some _approximate_ extreme point of a contextual best-response region.

**Definition 4.3** (\(\)-approximate extreme points).: _Fix a context \(\) and consider the set of all non-empty contextual best-response regions. For \(>0\), \(_{}()\) is the set of leader mixed strategies such that for all best-response functions \(\) and any \((_{l})\) that is an extreme point of \((_{}())\), \(_{}()\) if \(_{}()\). Otherwise there is some \(^{}_{}()\) such that \(^{}_{}()\) and \(\|^{}-\|_{1}\)._

Note that Definition 4.3 is constructive. We set \(=()\) so that the additional regret from only considering policies which map to points in \(_{}_{}()\) is negligible. As a result, we use the shorthand \(_{}:=_{}()\) throughout the sequel. The following lemma is a generalization of Lemma 4.3 in Balcan et al.  to our setting, and its proof uses similar techniques from convex analysis.

**Lemma 4.4**.: _For any sequence of followers \(f_{1}, f_{T}\) and any leader policy \(\), there exists a policy \(^{()}:_{} _{}\) that, when given context \(\), plays a mixed strategy in \(_{}\) and guarantees that \(_{t=1}^{T}u(_{t},(_{t}),b_{f_{t}}(_{t}, (_{t})))-u(_{t},^{()}(_{t}), b_{f_{t}}(_{t},^{()}(_{t}))) 1\). Moreover, the same result holds in expectation over any distribution over follower types \(\)._

Since we do not restrict the context space to be finite, the leader cannot pre-compute \(_{}\) for every \(\) before the game begins. Instead, they can compute \(_{_{t}}\) in round \(t\) before they commit to their mixed strategy. While \(_{_{t}}\) is computatable, it may be exponentially large in \(A_{f}\) and \(K\). However this is to be expected as Li et al.  show that in its general form, solving the non-contextual version of the online Stackelberg game problem is NP-Hard.

### Stochastic follower types and adversarial contexts

In this setting we allow the sequence of contexts to be chosen by an adversary, but we restrict the sequence of followers to be sampled i.i.d. from some (unknown) distribution over follower types \(\). When picking context \(_{t}\), we allow the adversary to have knowledge of \(\) and \(f_{1},,f_{t-1}\), but not \(f_{t}\). Under this relaxation, our measure of algorithm performance is _expected_ contextual Stackelberg regret, where the expectation is taken over the randomness in the distribution over follower types.

**Definition 4.5** (Expected Contextual Stackelberg Regret).: _Given a distribution over followers \(\) and a sequence of contexts \(_{1},,_{T}\), the leader's expected contextual Stackeleberg regret is \([R(T)]:=_{f_{1},,f_{T}}[_{t=1}^{T}u (_{t},^{*}(_{t}),b_{f_{t}}(_{t},^{*}( _{t})))-u(_{t},_{t},b_{f_{t}}(_{t}, _{t}))]\), where \(^{*}\) is the optimal policy given knowledge of \(_{1},,_{T}\) and \(\)._Under this setting, the utility for policy \(\) may be written as \(_{f_{1},,f_{T}}[_{t=1}^{T}u(_{t},(_{t}),b_{f_{t}}(_{t},(_{t})))]= _{t=1}^{T}_{f_{1},,f_{t-1}}[_{t}[u( _{t},(_{t}),b_{f_{t}}(_{t},(_{t} )))]]\). Our algorithm (Algorithm 1) proceeds by estimating the inner expectation \(_{t}[u(_{t},(_{t}),b_{f_{t}}(_{t}, (_{t})))]\) as

\[}_{t}[u(_{t},(_{t}),b_{f_{t}}( _{t},(_{t})))]:= u(_{t},(_{t }),a_{f})d}(b_{f_{t}}(_{t},(_{t}))=a_{f})\] (1)

and acting greedily with respect to our estimate. Here \(_{t}(b_{f_{t}}(_{t},(_{t}))=a_{f})\) is the (estimated) probability that the follower's best-response is \(a_{f}\), given context \(_{t}\) and leader mixed strategy \((_{t})\). As we will see, different instantiations of \(_{t}\) will lead to different regret rates for Algorithm 1. However, before instantiating Algorithm 1 with a specific estimation procedure, we provide a general result which bounds the regret of Algorithm 1 in terms of the total variation distance between the sequence \(\{_{t}\}_{t[T]}\) and the true distribution \(p\).

**Theorem 4.6**.: _Let \((,):=[p(b_{f_{t}}(,)=a_{f})] _{a_{f}_{f}}\) and \(}_{t}(,):=[_{t}(b_{f_{t}}( ,)=a_{f})]_{a_{f}_{f}}\). The expected contextual Stackelberg regret (Definition 4.5) of Algorithm 1 satisfies_

\[[R(T)] 1+2_{t=1}^{T}_{f_{1},,f_{t-1}}[ ((_{t},^{()}(_{t})), }_{t}(_{t},^{()}(_{t})) )+((_{t},_{t}(_{t})),}_{t}(_{t},_{t}(_{t})))].\]

Theorem 4.6 shows that the regret of Algorithm 1 is proportional to how well it estimates \((,)\) over time (as measured by total variation distance), on (1) the sequence of contexts chosen by the adversary and (2) the sequence of mixed strategies played by Algorithm 1 and the (near-)optimal policy \(^{()}\). While we instantiate Algorithm 1 in the setting where there are finitely-many follower types and follower actions, Theorem 4.6 opens the door to provide meaningful regret guarantees in settings in which there are infinitely-many follower types and/or follower actions.6 We now instantiate the estimation procedure in Algorithm 1 in two different ways to get end-to-end regret guarantees. First, the leader can get regret \(O(K)\) by estimating the distribution of follower types directly.

**Corollary 4.7**.: _If \(}_{t}=\{_{t}(f_{t}=^{(i)})\}_{i[K]}\), \(_{t+1}(f=^{(i)})=_{=1}^{t}\{f_ {}=^{(i)}\}\), and \(_{1}(f=^{(i)})=\) for \(i[K]\), then the regret of Algorithm 1 satisfies \([R(T)]=O(K)\)._

The leader can obtain a complementary regret bound of \(O(A_{f})\) if they instead estimate the probability that the follower best-responds with action \(a_{f}_{f}\), given a particular context \(\) and leader mixed strategy \(\).7 In what follows, we use \(_{(^{(,)}=a_{f})}\{0,1\}^{K}\) to refer to the indicator vector whose \(i\)-th component is \(_{(^{(,)}(^{(i)})=a_{f})}\), i.e. the indicator that a follower of type \(^{(i)}\) best-responds to context \(\) and mixed strategy \(\) by playing action \(a_{f}\).

**Corollary 4.8**.: _If \(}_{t}(,)=\{_{t}(_ {(^{(,)}=a_{f})})\}_{a_{f}_{f}}\), \(_{t+1}(_{(^{(,)}=a)})= {t}_{=1}^{t}\{b_{f_{}}(,)=a\}\), and \(_{1}(_{(^{(,)}=a)})= {A_{f}}\) for \(a_{f}_{f}\), then the regret of Algorithm 1 satisfies \([R(T)]=O(A_{f})\)._

### Stochastic contexts and adversarial follower types

We now consider the setting in which the sequence of contexts are drawn i.i.d. from some unknown distribution \(\) and the follower \(f_{t}\) is chosen by an adversary with knowledge of \(\) and \(_{1},,_{t-1}\)but not \(_{t}\). As was the case in Section 4.1, we consider a relaxed notion of regret which compares the performance of the leader to the best policy in expectation, although now the expectation is taken with respect to the distribution over contexts \(\).

**Definition 4.9** (Expected Contextual Stackelberg Regret, II).: _Given a distribution over contexts \(\) and a sequence of followers \(f_{1},,f_{T}\), the leader's expected contextual Stackebeberg regret is_

\[[R(T)]:=_{_{1},,_{T} }[_{t=1}^{T}u(_{t},^{*}(_{t}),b_{f_ {t}}(_{t},^{*}(_{t})))-u(_{t},_{t}, b_{f_{t}}(_{t},_{t}))],\]

_where \(^{*}\) is the optimal policy given knowledge of \(f_{1},,f_{T}\) and \(\)._

Our key insight is that when the sequence of contexts is generated stochastically, to obtain no-regret it suffices to (1) play a standard, off-the-shelf online learning algorithm (e.g. Hedge) over a finite (albeit exponentially-large) set of policies in order to find one which is approximately optimal and then (2) bound the resulting discretization error.

**Lemma 4.10**.: _When the sequence of contexts is determined stochastically, the expected utility of any fixed policy \(\) may be written as_

\[_{_{1},,_{T}}[_{t=1}^{T}u( _{t},(_{t}),b_{f_{t}}(_{t},(_{t })))]=_{i=1}^{K}_{}[u(,(),b_{^{(i)}}(,()))](_{t=1}^{T}_ {_{1},,_{t-1}}[\{f_{t}=^{(i)}\}] ).\]

Using Lemma 4.10, we now show that it suffices to play Hedge over a finite set of policies \(\) in order for the leader to obtain no-regret (Algorithm 2). The key step in our analysis is to show that the discretization error is small for our chosen policy class \(\).8 For a given weight vector \(^{K}\), let \(^{()}():=_{_{ }}_{i=1}^{K}u(,,b_{^{(i)}}(, ))[i]\). For a given set of weight vectors \(\), we set \(\) to be the induced policy class, i.e. \(:=\{^{()}\}_{}\).

**Theorem 4.11**.: _If \(=\{\ :\ ^{K},T [i], i[K]\}\) and \(=}\), then Algorithm 2 obtains expected contextual Stackelberg regret (Definition 4.9) \([R(T)]=O(+K)\)._

We conclude by briefly comparing our results with those of the non-contextual Stackelberg game setting of Balcan et al. . In particular, the setting of this subsection may be viewed as a generalization of the setting of Balcan et al.  in which the leader and follower utilities at time \(t\) also depend on a stochastically-generated context \(_{t}\). When \(||=1\), we recover their setting exactly. Under their non-contextual setting, there is only one set of approximate extreme points \(_{}\), and so we write the \(=_{}\). Here it suffices to consider the set of constant "policies" which always map to one of the (approximate) extreme points in \(\). Plugging this choice of \(\) into Algorithm 2, we recover their algorithm (and therefore also their regret rates) exactly.

However, it is also worth noting that more care must be taken to obtain regret guarantees against an adaptive adversary in our setting compared to the non-contextual setting of Balcan et al. .9 In particular, we need to bound the discretization error due to considering a finite set of policies, but it is without loss of generality to consider a finite set of mixed strategies in the non-contextual setting.

### Simulations

We empirically evaluate the performance of Algorithm 1 and Algorithm 2 on synthetically-generated data. We consider a setup in which \(K=5\), \(A=A_{f}=3\), and the context dimension \(d=3\). Utility functions are linear in both the context and player actions, and are sampled u.a.r. from \([-1,1]^{3 3 3}\).

We compare the cumulative reward of our algorithms to each other and the algorithm of Balcan et al.  (which does not leverage side information) as a baseline. We simulate non-stochastic context arrivals in Figure 1(a) by displaying the same context for \(T/4\) time-steps in a row. Follower types are chosen u.a.r. from each of the five follower types. In Figure 1(b), contexts are generated stochastically by sampling each component u.a.r. from \([-1,1]\). Followers are chosen non-stochastically by deterministically cycling over the five types. In Figure 1(c), both contexts and follower types are chosen stochastically. Specifically, contexts are generated as in Figure 1(b) and follower types are generated as in Figure 1(a).

We find that Algorithm 1 and Algorithm 2 perform similarly across instances, and both significantly out-perform the baseline of Balcan et al. . It would be interesting to find instances for which Algorithm 1 (resp. Algorithm 2) performs poorly whenever followers (resp. contexts) are chosen non-stochastically.

## 5 Extension to bandit feedback

We have so far assumed that the leader gets to observe the follower's type after each round. However this assumption may not always hold in real-world Stackelberg game settings. For example, in cyber security domains it may be hard to deduce the organization responsible for a failed cyber attack. In wildlife protection, a very successful poacher may never be seen by the park rangers. Instead, the leader may only be able to observe the action the follower takes at each round. Following previous work on learning in non-contextual Stackelberg games, we refer to this type of feedback as _bandit_ feedback. What can we say about the leader's ability to learn under bandit feedback when there is side information?

While our impossibility result of Section 3 immediately applies to this more challenging setting, our algorithms from Section 4 do not. This is because we can no longer compute quantities such as \(\{f_{t}=^{(i)}\}\) or \(b_{f_{t}}(_{t},)\) for an arbitrary mixed strategy \(\) from just follower \(f_{t}\)'s action alone. We still assume that the follower is one of \(K\) different types, although \(f_{t}\) is now _never_ revealed to the leader.

We allow ourselves two relaxations when designing learning algorithms which operate under bandit feedback. First, while the leader's utility function may still depend on the context \(_{t}\), we assume that the follower's utility is a function of the leader's mixed strategy \(_{t}\) alone, i.e. \(u_{f}(,,a_{f})=u_{f}(,a_{f})\) for all \(\). This allows us to drop the dependence on \(_{t}\) from both the follower's best response and the set of approximate extreme points, i.e. \(b_{f}(,)\) becomes \(b_{f}()\) and \(_{}\) becomes \(\). Furthermore, our definitions of contextual follower best-response region (Definition 4.1) and contextual best-response region (Definition 4.2) collapse to their non-contextual counterparts. Depending on the application domain, the assumption that only the leader's utility depends on the side

Figure 2: Cumulative average reward of Algorithm 1, Algorithm 2, and the algorithm of Balcan et al.  (which does not take side information into consideration) over five runs in a synthetic data setup. Shaded regions represent one standard deviation.

information may be reasonable. For instance, while an institution would prefer that a server with less traffic is hacked compared to one with more, a hacker might only care about the information hosted on the server (which may not be related to network traffic patterns). Second, we design algorithms with regret guarantees which only hold against a _non-adaptive_ adversary.10 Despite these relaxations, the problem of learning under bandit feedback still remains challenging because of the exponentially large size of \(\). While a natural first step is to estimate \(p(b_{f_{t}}()=a_{f})\) (i.e. the probability that follower at round \(t\) best-responds with action \(a_{f}\) when the leader plays mixed strategy \(\)) for all \(\) and \(a_{f}_{f}\), doing so naively would take exponentially-many rounds, due to the size of \(\).

Building off of results in the non-contextual setting of Balcan et al. , we leverage the fact that the leader's utility for different mixed strategies is not independent. Instead, they are _linearly_ related through the frequency of follower types which take a particular action, given a particular leader mixed strategy. Therefore, it suffices to estimate this linear function (which can be done using as few as \(K\) samples) to get an unbiased estimate of \(p(b_{f_{t}}()=a_{f})\) for any \(\) and \(a_{f}_{f}\). Borrowing from the literature on linear bandits, we use a _barycentric spanner_ to estimate \(\{\{p(b_{f_{t}}()=a_{f})\}_{a_{f}_{f}}\}_{ }\) in both partial adversarial settings we consider. A barycentric spanner for compact vector space \(\) is a special basis such that any vector in \(\) may be expressed as a linear combination of elements in the basis, _with each linear coefficient being in the range \([-1,1]\)_.

In Appendix C.1, we use the property that estimators constructed using barycentric spanners have _low variance_ to show that an explore-then-exploit algorithm achieves \(O(K^{2/3}A_{f}^{2/3}T^{2/3}^{1/3}T)\) expected contextual Stackelberg regret in the setting with stochastic follower types and adversarial contexts. Specifically, our algorithm (Algorithm 3) plays a special set of \(K\) mixed strategies \(N\) times each, then uses barycentric spanners to estimate \(\{p_{t}(_{\{^{(,)=a_{f})\}\}}\}_{a_{f} _{f}}\) for all \(\) and \(\), after which Algorithm 3 plays greedily like in Section 4.1.

In Appendix C.2, we use the property that estimators constructed using barycentric spanners are _bounded_ to design a reduction to our algorithm in Section 4.2 which achieves \(O(KA_{f}^{1/3}T^{2/3}^{1/3}T)\) expected contextual Stackelberg regret whenever the sequence of contexts is chosen stochastically and the sequence of follower types is chosen by an adversary. Finally, while it may be possible to obtain \(O()\) regret without using barycentric spanners, this would come at the cost of a linear dependence on \(||\) (and therefore an _exponential_ dependence on \(K\) and \(A_{f}\)) in regret.

## 6 Conclusion

We initiate the study of Stackelberg games with side information, which despite the presence of side information in many Stackelberg game settings, has not received attention from the community. We focus on the online setting in which the leader faces a sequence of contexts and follower types. We show that when both sequences are chosen adversarially, no-regret learning is not possible even for highly structured policy classes. When either sequence is chosen stochastically, we obtain algorithms with \(()\) regret. We also explore an extension to bandit feedback, in which we obtain \((T^{2/3})\) regret in both settings. There are several exciting avenues for future research; we highlight two below.

1. **Intermediate forms of adversary.** The two relaxations of the fully adversarial setting that we consider, while natural, rule out the leader learning about the follower's type from the context. Although we prove that learning is impossible in the fully adversarial setting, our lower bound does not rule out, e.g. settings where the mapping from contexts to follower types has finite Littlestone dimension. It would be interesting to further explore this direction to pin down when no-regret learning is possible.
2. \((T^{1/2})\)**regret under bandit feedback.**Bernasconi et al.  obtain \(O(T^{1/2})\) regret when learning in non-contextual Stackelberg games under bandit feedback against an adversarially-chosen sequence of follower types via a reduction to adversarial linear bandits. However, applying similar steps to Bernasconi et al. in our setting results in a reduction to a generalization of the (adversarial) contextual bandit problem for which we are not aware of any regret minimizing algorithm. Nevertheless, we view exploring whether \((T^{1/2})\) contextual Stackelberg regret is possible under bandit feedback as a natural and exciting future direction.