# Derandomizing Multi-Distribution Learning

Kasper Green Larsen

Department of Computer Science

Aarhus University

larsen@cs.au.dk

&Omar Montasser

Department of Statistics and Data Science

Yale University

omar.montasser@yale.edu

&Nikita Zhivotovskiy

Department of Statistics

University of California, Berkeley

zhivotovskiy@berkeley.edu

This work was primarily done while the author was a FODSI-Simons postdoc at UC Berkeley.

###### Abstract

Multi-distribution or collaborative learning involves learning a single predictor that works well across multiple data distributions, using samples from each during training. Recent research on multi-distribution learning, focusing on binary loss and finite VC dimension classes, has shown near-optimal sample complexity that is achieved with oracle efficient algorithms. That is, these algorithms are computationally efficient given an efficient ERM for the class. Unlike in classical PAC learning, where the optimal sample complexity is achieved with deterministic predictors, current multi-distribution learning algorithms output randomized predictors. This raises the question: can these algorithms be derandomized to produce a deterministic predictor for multiple distributions? Through a reduction to discrepancy minimization, we show that derandomizing multi-distribution learning is computationally hard, even when ERM is computationally efficient. On the positive side, we identify a structural condition enabling an efficient black-box reduction, converting existing randomized multi-distribution predictors into deterministic ones.

## 1 Introduction

We consider the problem of multi-distribution learning where there are \(k\) unknown data distributions \(=\{_{1},,_{k}\}\) over \(\{-1,1\}\), where \(\) is an input domain and \(\{-1,1\}\) are the possible labels. The goal is to learn a classifier \(f:\{-1,1\}\) that satisfies

\[_{}(f):=_{i}_{_{i }}(f)_{k}_{i}_{_{i}}(h)+ ,_{_{i}}(f)=_{(x,y) _{i}}[f(x) y].\] (1)

Here \(\{-1,1\}^{}\) is the benchmark hypothesis class of VC-dimension \(d\) that the learner competes against, and \(_{h}_{i}_{_{i}}(h)\) is the optimal worst-case error that can be achieved with classifiers from \(\). The framework of multi-distribution learning, introduced by Haghtalab et al. , is a natural generalization of agnostic PAC learning [22; 21; 5], and captures several important applications such as min-max fairness [12; 18; 15; 8; 20], and group distributionally robust optimization .

In the _realizable_ setting, where \(_{h}_{}(h)=0\), there is a learning algorithm using \(((d+k)/)\) samples to produce such a deterministic classifier \(f\), see e.g., the works [4; 7; 13]. Here, and throughout the paper, \(\) hides terms that are \((dk/())\).

In the more challenging _agnostic_ setting, where \(:=_{h}_{_{l}}(h)\) is greater than \(0\), recent works show that the sample complexity is \(((d+k)/^{2})\). We refer the reader to Table 1 in  for a detailed sample complexity comparison of prior algorithms. Importantly, the guarantee provided by all existing algorithms is slightly different from the objective (1) above. Concretely, all previous algorithms do _not_ produce a deterministic classifier \(f:\{-1,1\}\), but instead output a distribution \(F\) over \(\), such that

\[_{i}_{f F}[_{_{i}}(f)]_{h }_{i}_{_{i}}(h)+.\] (2)

Due to the fact that classical PAC bounds, which involve learning from a single distribution, are achieved using deterministic predictors it is somewhat unsatisfactory to always output a randomized predictor in the multi-distribution case. Observe that because, as in (2), we want optimal performance simultaneously for all distributions, even using a randomized algorithm is somewhat problematic. Indeed, assume that in practice we want to sample a single \(\) according to \(F\) and use it as our predictor. Now, if we seek a guarantee like (1) for \(\), then the best we can guarantee from (2) is to use Markov's inequality and a union bound over all \(k\) distributions to ensure that

\[_{_{i}}() 2k(_{h}_ {i}_{_{i}}(h)+),\]

with probability at least \(1/2\), which is, of course, too conservative. Let us also remark that there are examples of distributions \(F\) for which this is basically tight. Consider e.g. an input domain \(=x_{1},,x_{k}\) and \(k\) hypotheses \(h_{1},,h_{k}\) such that \(h_{i}(x_{i})=-1\) and \(h_{i}(x_{j})=1\) for \(j i\). Let \(_{i}\) be the distribution that returns \((x_{i},1)\) with probability \(1\). Then for the uniform distribution \(F=k^{-1}_{i}h_{i}\) over classifiers, we have \(_{i}_{f F}[_{_{i}}(f)]=1/k\), but for any single \(f\) in the support of \(F\), we have \(_{}(f)=_{i}_{_{i}}(f)=1\). The example also shows that for every fixed distribution \(_{i}\), if we sample an \(f\) from \(F\), then with probability \(1/k\), its error exceeds the expectation by a factor \(k\) for that distribution \(_{i}\). There may thus be a large gap between the guarantees of a deterministic and randomized classifier, i.e. the bounds in (1) and (2) are quite different.

The main focus of our work, is on replacing the random classifiers in previous works on agnostic multi-distribution learning by deterministic classifiers and understanding the inherent complexity of doing so. In particular, we are interested in understanding any inherent statistical or computational gaps in multi-distribution learning between deterministic classifiers and randomized classifiers.

#### Our contributions

Our first contribution is a strong negative result towards derandomizing previous classifiers. Recall that the complexity class BPP denotes bounded-error probabilistic polynomial time2. That is, problems that have polynomial time randomized algorithms that are correct with probability at least \(2/3\) on every input. It is conjectured that \(=\) and thus most likely \(\). Recall that a set of \(n\) points is shattered if each of the \(2^{n}\) possible labelings of the points can be realized by some \(h\). Our negative result is then the following.

**Theorem 1**.: _If \(\), then as \(n=\{d,k,1/\}\) tends to infinity, for every hypothesis class \(\) of VC-dimension \(d\) for which one can find \(n\) points shattered by \(\) in polynomial time, any multi-distribution learning algorithm for \(\) that on the set of \(k\) input distributions \(=\{_{1},,_{k}\}\) with probability at least \(2/3\) produces a deterministic classifier \(f:\{-1,1\}\) with \(_{}(f)_{h}_{ }(h)+\), must have either \(n^{(1)}\) (i.e. super-polynomial) training time, or \(f\) has \(n^{(1)}\) evaluation time._

We remark that this computational hardness result holds even when the class \(\) admits efficient Empirical Risk Minimization (ERM), and even when the distributions _are known_ to the learning algorithm. This highlights that the hardness stems not from the need to sample from the underlying distributions nor from the hardness of ERM, but from the computational problem of deciding which label to assign the points of the input domain.

Note that the assumption in Theorem 1 that one can find a set of \(n\) shattered points in polynomial time is not restrictive. Finding such points is trivial for many \(\), i.e., simply choose \(0,e_{1},,e_{d-1}^{d-1}=\) for linear classifiers with VC-dimension \(d\). More generally, the standard result on the class of classifiers induced by positive halfspaces in \(^{d}\) shows that this class has VC dimension \(d\), and for any set of points such that at most \(d\) of its points are contained on a single hyperplane, any subset of size \(d\) of this set is shattered. Similar properties are also known for the classes induced by balls in \(^{p}\) and positive sets in the plane defined by polynomials of degree at most \(p-1\). See  for a detailed exposition of these examples.

While this might have been the end of the story, our NP-hardness proof fortunately highlights a path to circumventing the lower bound. In particular, the proof carefully uses data distributions \(_{1},,_{k}\) for which \(_{i}(y x)\) varies between the distributions. Here \(_{i}(y x)\) denotes the conditional distribution of the label \(y\) of a sample \((x,y)\) given \(x\). We thus consider the following restricted version of collaborative learning in which \(_{i}(y x)=_{j}(y x)\) for all \(x,i,j\). That is, the \(k\) different distributions may vary arbitrarily over \(\), but the label \(y\) of any \(x\) follows the same distribution for all \(_{i}\). As a particular model of _label consistent learning_, one may think of a deterministic labeling setup where it is assumed that there is \(f^{}:\{1,-1\}\) such that across all distributions \(y=f^{}(x)\), while no assumption is made that \(f^{}\) belongs to \(\). Remarkably, in terms of sample complexity, in the case of a single distribution, the case of deterministic labeling is almost as hard as the general agnostic case as shown in . Thus, we believe our label-consistent multi-distribution learning setup is quite natural and interesting.

Furthermore, this restriction turns out to be sufficient for derandomizing multi-distribution learning algorithms. In particular, we give a new algorithm, Algorithm 1, that uses a randomized (i.e., an algorithm that outputs a randomized predictor given the training data) multi-distribution learning algorithm (like (2)) as a black-box, and produces from it a deterministic classifier, as in (1).

**Theorem 2**.: _For any finite domain \(\), if the data distributions \(_{1},,_{k}\) are label-consistent, then given a multi-distribution learning algorithm \(\) that uses \(m(k,d,,,)\) samples and \(t(k,d,,,)\) training time to produce, with probability \(1-\), a distribution \(F\) over classifiers from \(\) satisfying \(_{i}_{f F}[_{_{i}}(f)]+\), Algorithm 1 produces with probability \(1-\) a classifier \(f:\{-1,1\}\) with \(_{}(f)+\) with the sample complexity_

\[m(k,d,,/2,/2)+O(k^{2}(k/))/ ^{2}).\]

_Using the additional ideas in Section 3.1, the training time of Algorithm 1 is_

\[t(k,d,,/2,/2)+(k/^{2}+(| |/)).\]

_If the evaluation time of hypotheses in \(\) is bounded by \(s\), then the evaluation time of the classifier \(f\) is bounded by_

\[O(s|F|)+((k/)(||/)).\]

Note that several of the previous randomized multi-distribution learning algorithms are indeed computationally efficient as long as ERM is efficient over \(\). This includes the algorithm in  that has a near-optimal sample complexity of \(m(k,d,,,)=((d+k)/^{2})\) with \(t(k,d,,,)=(k,d,^{-1}, (1/))t_{ERM}\) and \(|F|=(k,d,^{-1},(1/))\), where \(t_{ERM}\) denotes the time complexity of ERM over \(\). Plugging this into Theorem 2 gives a polynomial time deterministic multi-distribution learning algorithm.

We view the restriction to finite domains \(\) in Theorem 2 as rather mild, as any realistic implementation of a learning algorithm requires an input representation that can be stored on a computer. Moreover, our running time dependency on \(||\) is only logarithmic. Even so, in Section 3.2 we give some initially promising directions for extending our algorithm to infinite \(\).

**Discussion of implications.** Prior work has shown that in agnostic multi-distribution learning, a sample complexity of \((dk/^{2})\), which is worse than the optimal \(((d+k)/^{2})\) sample complexity, is unavoidable with _proper_ learning algorithms, which are algorithms restricted to outputting a classifier in the class \(\)[23, Theorem 18]. In contrast, our negative result in Theorem 1 implies that there is _no_ sample-efficient and oracle-efficient multi-distribution learning algorithm that aggregates multiple ERM predictors in polynomial time. For example, our result rules out the simple majority-vote aggregation approach (which is feasible in the realizable setting when \(=0\)). Note, however, that this does _not_ rule out the existence of computationally _inefficient_ aggregation approaches to construct deterministic predictors. That is, putting computational efficiency aside, it is still an open question whether there exists a sample-efficient and oracle-efficient multi-distribution learning algorithm that outputs a deterministic predictor, and we know from the lower bound of Zhang et al. [23, Theorem 18] that this predictor must be _improper._Hardness of derandomization

In this section, we prove that it is NP-hard to derandomize multi-distribution learning in the most general setup of input distributions \(_{i}\) over \(\{-1,1\}\). In particular, the hardness proof carefully exploits that different data distributions may assign different labels to the same \(x\).

Our NP-hardness proof goes via a reduction from Discrepancy Minimization. In Discrepancy Minimization, we are given as input an \(n n\) matrix with \(0\)-\(1\) entries. The goal is to find a "coloring" \(z\{-1,1\}^{n}\) such that every entry of \(Az\) is as small as possible in absolute value. Formally, we seek to minimize \(\|Az\|_{}\). The seminal work by Charikar et al.  showed NP-hardness of computing the best coloring. In full details, their results are as follows.

**Theorem 3** ().: _There is a constant \(c>0\) such that it is NP-hard to distinguish whether an input matrix \(A\{0,1\}^{n n}\) has \(\|Az\|_{2} cn\) for all \(z\{-1,1\}^{n}\), or whether there exists \(z\{-1,1\}^{n}\) with \(Az=0\)._

Since \(\|Az\|_{}\|Az\|_{2}/\), this similarly implies that it is NP-hard to distinguish whether all \(z\) have \(\|Az\|_{} c\), or there is a \(z\) with \(Az=0\).

Let us now use Theorem 3 to prove our hardness result, Theorem 1. We remark that NP-hardness is formally defined in a uniform model of computation where a Turing Machine takes an encoded input on a tape and decides language membership. As we believe our reduction is clear without going into such formalities, we have deferred a discussion of how to formalize multi-distribution learning in a uniform model of computation to Appendix 4.

Proof.: Let \(n=\{d,k/2,c^{2}/(4^{2})\}\) and let \(\) be an arbitrary hypothesis set of VC-dimension \(d\) for which we can find a set of \(n\) points that are shattered by \(\) in \(n^{O(1)}\) time. This is possible due to our assumption.

Let \(\) denote an arbitrary deterministic multi-distribution algorithm. Given a matrix \(A\{0,1\}^{n n}\) such that either \(\|Az\|_{} c\) for all \(z\{-1,1\}^{n}\), or there exists a \(z\{-1,1\}^{n}\) with \(Az=0\), we will now use \(\) to correctly distinguish these two cases with probability at least \(2/3\), thus concluding that the running time of \(\) is super-polynomial unless \(=\).

Start by computing an arbitrary set \(x_{1},,x_{n}\) of \(n\) points that are shattered by \(\). Now define \(2n\) distributions \(_{1}^{+},_{1}^{-},,_{n}^{+},_{n}^{-}\). Distribution \(_{i}^{+}\) and \(_{i}^{-}\) are both defined from the \(i\)-th row of \(A\). If \(m_{i}\) denotes the number of ones in the \(i\)-th row of \(A\), we let \(_{i}^{+}\) return the sample \((x_{j},1)\) with probability \(1/m_{i}\) for each \(j\) with \(a_{i,j}=1\). The distribution \(_{i}^{-}\) similarly returns \((x_{j},-1)\) with probability \(1/m_{i}\) for each \(j\) with \(a_{i,j}=1\). Observe that these distributions can be described using \(n\) bits each.

Now consider running the multi-distribution learning algorithm \(\) on distributions \(_{1}^{+},_{1}^{-},,_{n}^{+},_{n}^{-}\) to obtain a deterministic classifier \(:\{-1,1\}\). Evaluate \(f\) on \(x_{1},,x_{n}\) and compute \(_{}(f)\). This can be done trivially in polynomial time using the definitions of the distributions. If \(_{}(f)<1/2+\), then output that there exists \(z\{-1,1\}^{n}\) such that \(Az=0\). Otherwise, output that no such \(z\) exists. Clearly this runs in polynomial time. It thus remains to argue correctness.

Consider first the case where there exists \(z\{-1,1\}^{n}\) with \(Az=0\). Since \(z\) has inner product \(0\) with every row of \(A\), it follows that it assigns \(1\) to precisely half of the non-zero entries of the \(i\)-th row and \(-1\) to the remaining half. The labeling \(z\) of \(x_{1},,x_{n}\) thus has \(_{_{1}^{+}}(z)=_{_{1}^{ -}}(z)=1/2\) and \(_{}(z)=1/2\). Furthermore, since \(x_{1},,x_{n}\) are shattered by \(\), it follows that \(_{h}_{}(h) 1/2\). By correctness of \(\), it must hold with probability at least \(2/3\) that we correctly output that there exists \(z\) with \(Az=0\).

Consider next the case that every \(z\{-1,1\}^{n}\) has \(\|Az\|_{} c\). It follows that there is a row \(a_{i}\) such that the vector \(v=(f(x_{1}),,f(x_{n}))\) has \(|v^{T}a_{i}| c\). Let \(=(v^{T}a_{i})\). Then

\[_{_{i}^{-}}(f) = }_{j:a_{i,j}=1}\{f(x_{j})=\}= }_{j:a_{i,j}=1}(1/2)(f(x_{j})+1)\] \[= 1/2+}_{j:a_{i,j}=1}f(x_{j})=1/2+a_{i}}{2m_{i}}\] \[= 1/2+a_{i}|}{2m_{i}} 1/2+c/(2).\]

Since we chose \(n=\{d,k/2,c^{2}/(4^{2})\}\), we have \(c/(2)\) and thus we return with probability \(1\) that all \(z\{-1,1\}^{n}\) have \(\|Az\|_{} c\). 

Let us end by observing that the distributions used in the above hardness result have \( 1/2\). The proof can be modified to prove lower bounds for smaller \(\) by adding a dummy point \(x_{0}\) and letting all distributions return \((x_{0},1)\) with probability \(1-2^{}\) and the points in the above distributions with probability \(2^{}/m_{i}\). This reduces the value of \(\) to around \(^{}\). However, we also need to reduce \(n\) to \(\{d,k/2,(c^{}/)^{2}\}\). This agrees well with the fact that for realizable multi-distribution learning, i.e., \(=0\), it is in fact possible to compute a deterministic classifier in polynomial time.

## 3 Deterministic multi-distribution learner

In this section, we give our algorithm for derandomizing multi-distribution learners for label-consistent distributions, i.e., we assume \(_{i}(y x)=_{j}(y x)\) for all \(i,j,x\).

We start by presenting the high level ideas of our algorithm. Recall that we defined \(=_{h}_{}(h)\), where \(=\{_{1},,_{k}\}\). First, consider running any of the previous randomized multi-distribution learners, producing a distribution \(F\) over hypotheses in \(\) satisfying \(_{i}_{f F}[_{_{i}}(f)] +/2\). Consider randomly rounding this distribution to a deterministic classifier \(\) as follows: For every \(x\) independently (recall that we focus on finite domains), sample an \(f F\) and let \((x)=f(x)\). For any distribution \(_{i}\), we clearly have \(_{f}[_{_{i}}()]=_{f F }[_{_{i}}(f)]+/2\). However, as also discussed in the introduction, it is not clear that we can union bound over all \(k\) distributions and argue that \(_{_{i}}()+\) for all of them simultaneously. Notice however that the independent choice of \((x)\) for each \(x\) gets us most of the way. Indeed, if we let \(Z_{x}\) be a random variable (determined by \((x)\)) giving \(_{y(y|x)}[(x) y]\), then \(_{_{i}}()=_{x} _{i}(x)Z_{x}\), where \(_{i}(x)\) denotes the probability of \(x\) under \(_{i}\) and \(_{i}(y x)\) gives the conditional distribution of the label \(y\) given \(x\). Now notice that \(_{i}(x)Z_{x}\) is a random variable taking values in \(\{(1/2-|_{x}|)_{i}(x),(1/2+|_{x}|)_{i}(x)\}\) where \(_{x}=_{y_{1}(y|x)}[y=1]-1/2\) denotes the _bias_ of the label of \(x\). Furthermore, these random variables are independent. We also have \([_{_{i}}()]=_{f F }[_{_{i}}(f)]\). Thus by Hoeffding's inequality

\[[|_{_{i}}(f)-_{f F}[ _{_{i}}(f)]|>/2]<2(-}{2_{x X}(2_{x}_{i}(x))^{2}} ).\]

Examining this expression closely, we observe that this probability is small if \(_{x}_{i}(x)\) is small for all \(x\).

Using this observation, our algorithm then starts by drawing \((^{-2})\) samples from each distribution \(_{i}\) and collecting all \(x\) for which the fraction of \(1\)'s and \(-1\)'s is so biased towards either \(1\) or \(-1\), that the majority label almost certainly equals \((_{x})\). We then let \((x)\) equal this majority label for all such \(x\), and put these \(x\) into a set \(T\).

What remains is all \(x T\). Here we show that these \(x\) have so little bias, i.e., \(_{x}_{i}(x)\) is so small, that the random rounding strategy above suffices. The full algorithm is shown as Algorithm 1.

Before giving the formal analysis of the algorithm, note that storing the classifier \(\) is quite expensive, as we need to remember the random choice of \((x)\) for every \(x T\). This is one place where we use the assumption that \(\) is finite. Note however that even for finite \(\), storing \(||\) random choices to represent the classifier might be infeasible. Furthermore, the sampling of \((x)\) for every \(x\) also adds \(||\) to the running time, which is again too expensive. We propose a method for reducing the storage and running time requirement later in this section. For now, we analyse Algorithm 1 without worrying about \(||\).

``` Input: Distributions \(=\{_{1},,_{k}\}\). Precision \(>0\), failure probability \(>0\), randomized multi-distribution learner \(\). Result: Classifier \(:\{-1,1\}\).
1 Let \(C>0\) be a large enough constant.
2 Let \(=Ck/()\).
3 Let \(T=\).
4 Run \(\) with \(=\{_{1},,_{k}\}\) and \(\) as input, precision \(/2\) and failure probability \(/2\) to obtain a distribution \(F\) over classifiers in \(\).
5for\(i=1,,k\)do
6 Draw \(m=C^{2}()/^{2}\) samples \(\{(x_{j},y_{j})\}_{j=1}^{m}\) from \(_{i}\).
7 For every \(x T\) such that \(n_{i,x}:=|\{j:x_{j}=x\}|>0\), let \(_{i,x}=(|\{j:x_{j}=x y_{j}=1\}|-|\{j:x_{j}=x y_{j}=-1\}|)/n_{ i,x}\).
8 For every \(x T\) such that \(n_{i,x}>0\), if \(|_{i,x}|>}\), add \(x\) to \(T\) and let \((x)=(_{i,x})\).
9 For every \(x T\), independently draw an \(f F\) and let \((x)=f(x)\). return\(\) ```

**Algorithm 1**DeterministicLearner(\(,,,\))

Analysis.In our analysis, we separately handle \(x T\) and \(x T\). The two technical results we need are stated next. First, define the _bias_\(_{x}\) of an \(x\) as \(_{y_{1}(y|x)}[y=1]-1/2\). We say that an \(x\) is _heavily biased_ if

\[_{x}^{2}_{i}(x)>}{8(4k/)}\]

for at least one \(i\), and _lightly biased_ otherwise. Intuitively, our algorithm ensures that \(T\) contains all heavily biased \(x\) and that all predictions made on \(x T\) are correct. This is stated in the following

**Lemma 4**.: _It holds with probability at least \(1-/4\) that every heavily biased \(x\) is in \(T\), and furthermore, for every \(x T\), we have \((x)=(_{x})\)._

Next, we also show that random rounding outside \(T\) suffices.

**Lemma 5**.: _Assume every heavily biased \(x\) is in \(T\) after the for-loop. Then with probability at least \(1-/4\) over the random choice of \((x)\) with \(x T\), it holds for all \(i\) that_

\[|_{f F}[_{(x,y)_{i}}[1\{x T  f(x) y\}]]-_{(x,y)_{i}}[1\{x T (x) y\}]|/2.\]

Before giving the proof of Lemma 4 and Lemma 5, let us use these two results to complete the proof of Theorem 2.

Proof of Theorem 2.: From a union bound and Lemma 4 and Lemma 5, we have with probability \(1-\), that all of the following hold

* The invocation of \(\) in step 1 of Algorithm 1 returns a distribution \(F\) with \(_{i}_{f F}[_{_{i}}(f)] +/2\).
* For every \(x T\), we have \((x)=(_{x})\).
* For every distribution \(_{i}\), \[|_{f F}[_{(x,y)_{i}}[1\{x T  f(x) y\}]]-_{(x,y)_{i}}[1\{x T (x) y\}]|/2.\]Assume now that all of the above hold. We rewrite \(_{}()\) by splitting the contributions to the error into \(x T\) and \(x T\),

\[_{}() = _{i}_{_{i}}()\] \[= _{i}(_{(x,y)_{i}}[1\{x T (x) y\}]+_{(x,y)_{i}}[1\{x T (x) y\}]).\]

Using that \((x)=(_{x})\) for \(x T\), we have \(_{(x,y)_{i}}[1\{x T(x) y\}]=_{ z\{-1,1\}^{T}}[_{_{i}}[1\{x T z(x) y\}]]\). Thus the above is bounded by

\[_{i}(_{z\{-1,1\}^{T}}[_{_{i}}[1\{x T  z(x) y\}]+_{f F}[_{_{i}}[1\{x  T f(x) y\}]])+/2.\]

Since every \(f\) in the support of \(F\) is a deterministic classifier, we have

\[_{z\{-1,1\}^{T}}[_{_{i}}[1\{x T z(x) y \}]_{f F}[_{_{i}}[1\{x T f(x)  y\}].\]

We therefore have

\[_{}() _{i}(_{f F}[_{_{i }}[1\{x T f(x) y\}]]+_{f F}[_{ _{i}}[1\{x T f(x) y\}]])+/2\] \[=_{i}_{f F}[_{_{i}}(f)]+ /2\] \[+.\]

This completes the proof of Theorem 2. 

Proof of Lemma 4.: We first define the two types of failures that may occur:

* For every \(i\) and every \(x\) with \(_{x}^{2}_{i}(x)>^{2}/(8(4k/))\), let \(E^{1}_{i,x}\) denote the event that \(n_{i,x}<(C/2)_{x}^{-2}\).
* For every \(i\), let \(E^{2}_{i}\) denote the event that there is an \(x\) with \(n_{i,x}>0\) and \(|_{x}-_{i,x}/2|>)}\).

Assume first that none of the events occur. Consider a heavily biased \(x\). Then there is an \(i\) for which \(_{x}^{2}_{i}(x)>^{2}/(8(4k/))\). Since \(E^{1}_{i,x}\) does not occur, we have \(n_{i,x}(C/2)_{x}^{-2}\). Since \(E^{2}_{i}\) does not occur, we also have \(|_{x}-_{i,x}/2|)}\). Hence \(|_{i,x}| 2|_{x}|-2)}\). But \(|_{x}|}\) and thus \(|_{i,x}|(-1/2)}\). For \(C\) large enough, this is at least \(}\), which puts \(x\) in \(T\) during step 8 of Algorithm 1. Thus every heavily biased \(x\) is in \(T\). Secondly, note that when an \(x\) is added to \(T\) in iteration \(i\) for the for-loop, we have \(|_{i,x}|>}\). Since \(E^{2}_{i}\) does not occur, we have \(|_{x}-_{i,x}/2|)}\). But this implies \(_{x}[_{i,x}/2-)},_{i,x}/2+)}]\). Since \(|_{i,x}|>}\), every number in this interval has the same sign as \(_{i,x}\), i.e. \((x)=(_{i,x})=(_{x})\). Thus what remains is to bound the probability of these events.

For \(E^{1}_{i,x}\), fix an \(i\) and \(x\) with \(_{x}^{2}_{i}(x)>^{2}/(8(4k/))\), we have

\[[n_{i,x}]=D_{i}(x)m>^{2}m/(8_{x}^{2}(4k/))> C_{x}^{-2}.\]

For \(C\) large enough, we get from a Chernoff bound that \([E^{1}_{i,x}]=[n_{i,x}<(C/2)_{x}^{-2}]<^{-2}\).

For \(E^{2}_{i}\), let us first condition on an outcome of the values \(n_{i,x}\) for all \(x\). Then for every \(x\), we have that \(p_{i,x}:=|\{j:x_{j}=x y_{j}=1\}|-|\{j:x_{j}=x y_{j}=-1\}|\) is distributed as the sum of \(n_{i,x}\) independent \(-1/1\) random variables taking the value \(1\) with probability \(_{x}+1/2\). Hence \([p_{i,x}]=2_{x}n_{i,x}\). Since \(_{i,x}=p_{i,x}/n_{i,x}\), it follows from Hoeffding's inequality that

\[[|_{x}-_{i,x}/2|>}] = [|2_{x}n_{i,x}-p_{i,x}|>2}]\] \[< 2(-}{4n_{i,x}})=2^{-2}.\]For any fixed values \(n_{i,x}\), there are at most \(m\) distinct \(x\) with a non-zero \(n_{i,x}\). A union bound over all of them implies \([E_{i}^{2} n_{i,x}] 2m^{-2}\). Since this upper bound holds for any outcome of the \(n_{i,x}\), we have also \([E_{i}^{2}] 2m^{-2}\).

We now observe that for every \(i\), there are at most \(^{-2}8(4k/)\) distinct \(x\) with \(_{x}^{2}_{i}(x)>^{2}/(8(4k/))\). Hence \([_{x}E_{i,x}^{1}]^{-2}8(4k/)^{-2}\). A union bound over all \(i\) finally implies

\[[(_{i}_{x}E_{i,x}^{1})(_{i}E_{i}^{2})] k^{-2} (^{-2}8(4k/)+2m)\]

Since \(=Ck/()\) and \(m=C^{2}()/^{2}\), we have for large enough \(C\) that this probability is bounded by \(/4\). 

Proof of Lemma 5.: Fix a distribution \(_{i}\). Observe that for any \(x T\), we have that the distribution of \((x)\) is the same as \(f(x)\) for \(f F\). Hence \(_{}[_{(x,y)_{i}}[1\{x T (x) y\}]]=_{f F}[_{(x,y)_{i} }[1\{x T f(x) y\}]]\). Denote this expectation by \(\). If we let \(Z_{x}\) be the random variable (as a function of \((x)\)) taking the value \(_{y_{i}(y|x)}[(x) y]\), then

\[_{(x,y)_{i}}[1\{x T(x) y\}]= _{x T}_{i}(x)Z_{x}.\]

Observe that \(Z_{x}\) is either \(1/2-|_{x}|\) or \(1/2+|_{x}|\), depending on whether \((x)=(_{x})\) or not. Hence \(_{i}(x)Z_{x}[_{i}(x)(1/2-|_{x}|),_{i }(x)(1/2+|_{x}|)]\) and the \(Z_{x}\) are independent. We thus get from Hoeffding's inequality and that \(x T\) are lightly biased that

\[_{}[_{x T}_{ i}(x)Z_{x}>+/4] < (}{_{x  T}(2|_{x}|_{i}(x))^{2}})\] \[ (}{_{x T }_{i}(x)^{2}/(4k/)})\] \[ (-(4k/))=/(4k).\]

A union bound over all \(_{i}\) completes the proof. 

### Reducing storage and time

The above description of Algorithm 1 requires the storage of an independent random choice of \((x)\) for every \(x\). This is infeasible for large \(\), both in terms of space usage and the time needed for making these random choices. Instead, we can reduce the storage requirements by using an \(r\)-wise independent hash function \(q:\) for a sufficiently large output domain \(\) to make the random rounding. Recall that an \(r\)-wise independent hash function hashes any set of up to \(r\) distinct keys \(x_{1},,x_{r}\) independently and uniformly at random into \(\). Such a hash function can be implemented in space \(O(r(||||))\) bits and evaluated in time \((r(||||))\) by e.g., interpreting an \(x\) as an index into \([||]=\{0,,||-1\}\) and letting \(q(x)=_{i=0}^{r-1}_{i}x^{i}(p)\) for a prime \(p=||>||\) and the \(_{i}\) independent and uniformly random in \([p]\). Using fast multiplication algorithms, \(q(x)\) can be evaluated in time \((r(||||))\), even when \((||||)\) bits does not fit in a machine word. The time to sample the hash function is only \(O(r||||)\) (we just need the random coefficients of the polynomial).

Instead of storing \((x)\) for every \(x T\) explicitly, the learning algorithm instead stores \(q\) and the distribution \(F\). Given this information, we evaluate \((x)\) by computing \(q(x)\) and letting \((x)=1\) if \(q(x)_{f F}[f(x)=1]||-1\) and \(-1\) otherwise. Since \(q(x)\) is uniform over \(\) for any \(x\), we have \([(x)=1]=_{f F}[f(x)=1]||/||\). This probability satisfies \(_{f F}[f(x)=1]-1/||_{f F}[f(x)=1]_{f F }[f(x)=1]\) and is thus almost the same rounding probability as in Algorithm 1. Since previous multi-distribution learning algorithms also store \(F\), this only adds \(O(r(||||))\) bits to the storage.

What remains is to determine an \(r\) and \(||\) for which this is sufficient for the guarantees of Algorithm 1. We will show that \(r=2(4k/)\) and \(||=(^{-3}(k/))\) suffices if we increase the sample complexity of Algorithm 1 by a logarithmic factor. Observe that the \(O(r(||(k/)/))\) extra bits is only proportional to storing \(O((k/))\) samples from \(\), provided that \((k/)/\) is no larger than a polynomial in \(||\). The space overhead is thus very minor.

We only give an outline of how to modify the proof in the previous section to work with \(r\)-wise independence as it follows the previous proof rather uneventfully. First, redefine the threshold for being heavily biased to \(_{x}^{2}_{i}(x)>^{2}/(C^{}^{2}(4k/))\) for large enough constant \(C^{}\).

For the proof of Lemma 4 to still go through, this requires us to increase \(m\) by a \(C^{}\) factor, i.e. to \(CC^{}^{3}()/^{2}\), and also increase \(\) by \(C^{}\) to \(CC^{}k/()\). Then the only change to the proof, is that we have an event \(E^{1}_{i,x}\) for every \(i\) and every \(x\) with \(_{x}^{2}_{i}(x)>^{2}/(C^{}^{2}(4k/))\). Otherwise, all conditions in the events \(E^{1}_{i,x}\) and \(E^{2}_{i}\) remain the same. Thus the proof still goes through if we can argue \([E^{1}_{i,x}]^{-2}\). So fix an \(i\) and \(x\) with \(_{x}^{2}_{i}(x)>^{2}/(C^{}^{2}(4k/))\). Then \([n_{i,x}]=_{i}(x)m>^{2}m/(C^{}_{x} ^{2}^{2}(4k/))>C_{x}^{-2}\). This is the same lower bound on \([n_{i,x}]\) as the previous proof and thus we can complete the steps. Finally, note that we finished the proof of Lemma 4 by a union bound. Here we needed \(k^{-2}(^{-2}8(4k/)+2m)</4\). This is still the case for our new \(m\) and \(\).

Now for the proof of Lemma 5, we used Hoeffding's inequality. This requires the random rounding to be independent for different \(x\). With our modified approach, the roundings are only \(r\)-wise independent and thus we need the following variant of Hoeffding's inequality for \(r\)-wise independent random variables

**Theorem 6** ().: _Let \(Z_{1},,Z_{n}\) be a sequence of \(r\)-wise independent random variables for \(r 2\) with \(|Z_{i}-[Z_{i}]| 1\) for all outcomes. Let \(Z=_{i}Z_{i}\) with \([Z]=\) and let \(^{2}(Z)=_{i}^{2}(Z_{i})\) denote the variance of \(Z\). Then the following holds for even \(r\) and any \(Q\{r,^{2}(Z)\}\):_

\[[|Z-| T](T^{2}})^{r/2}.\]

If we repeat the proof of Lemma 5, define \(Z_{x}\) as the random variable (as a function of the random choice of \(q\)) taking the value \(_{y_{i}(y|x)}[(x) y]\). Note that \(Z_{x}\{1/2-|_{x}|,1/2+|_{x}|\}\). This also implies that \(|Z_{x}-[Z_{x}]| 2|_{x}|\) for all outcomes of \(Z_{x}\). When all heavily biased \(x\) are in \(T\), we have \(_{x}^{2}_{i}(x)^{2}/(C^{}^{2}(4k/ ))\) for all \(x T\). This implies \(|_{x}|/((4k/)_{i}(x)})\). Now let \(=2/((4k/)})\). Then

\[_{(x,y)_{i}}[1\{x T(x) y\}]= _{x T}_{i}(x)Z_{x}=_{x  T}_{i}(x)Z_{x}}{}.\]

The random variable \(_{i}(x)Z_{x}/\) thus satisfies \(|_{i}(x)Z_{x}/-[_{i}(x)Z_{x}/]|  2_{i}(x)|_{x}|/_{i}(x)} 1\) for all outcomes. This also gives us \(^{2}(_{i}(x)Z_{x}/)_{i}(x)\) and thus

\[^{2}(_{x T}_{i}(x)Z_{x }}{})_{x T}_{i}(x) 1.\]

Now consider the expected value (with \(a b=[a-b,a+b]\))

\[^{} = [_{x T}_{i}(x)Z_{ x}]\] \[= _{x T}_{i}(x)_{q} _{y_{i}(y|x)}[(x) y]\] \[ _{x T}_{i}(x)(_{f F}[_{y_{i}(y|x)}[f(x) y]] 1/||)\] \[ _{f F}[_{(x,y)_{i}}[1\{x  T f(x) y\}]] 1/||.\]Letting \(=_{f F}[_{(x,y)_{i}}[1\{x T f( x) y\}]]\), we then have by Theorem 6 with \(Q=r\) that

\[[|_{x T}_{i}(x) Z_{x}-| T] [|_{x T}_ {i}(x)Z_{x}-^{}| T-1/||]\] \[=[|_{x T}_{i}(x)Z_{x}}{}-^{}/| T-/||]\] \[(}{e^{2/3}(T-/||)^{2}} )^{r/2}.\]

Inserting \(T=/(2)\) and using \(r=2(4k/)\), \(|| 4^{2}/\) gives \((T-/||)/(4)\) and thus finally implies

\[[|_{(x,y)_{i}}[1\{x T (x) y\}]-_{f F}[_{(x,y)_{i}}[1\{x T f(x) y\}]]|/2]\] \[(^{2}}{e^{2/3}^{2}})^ {r/2}=(}{C^{}e^{2/3}^{2}(4k/)})^{r/2}= (e^{2/3}})^{r/2}\] \[ e^{-r/2}=/(4k).\]

Here, the last inequality follows for \(C^{}\) large enough. Thus, if we increase the sample complexity to \(m(k,d,,/2,/2)+O(k^{3}(k/())/ ^{2})\), then we may sample and store a hash function using only \(O((n/)(||(k/)/))\) extra bits and \(O((n/)(||(k/)/))\) time.

### Infinite Input Domains

In the above presentation of our algorithm, we have assumed a finite input domain \(\). While we believe this is a very reasonable assumption, we here present some initial ideas for how this restrictions might be circumvented.

Assume that the black-box randomized multi-distribution learner \(\) always outputs a distribution \(F\) over a finite number of classifiers in \(\). Let \(m\) be an upper bound on the size of the support. Then since \(\) has VC-dimension \(d\), the _dual_ VC-dimension is at most \(2^{d}\). By Sauer-Shelah, this implies that the number of distinct ways \(x\) may be labeled by the support of \(F\) is bounded \(+1}\), i.e. finite. We believe that treating just the distinct ways \(x\) is labeled by the hypotheses in the support should be sufficient to recover our results for finite \(\).