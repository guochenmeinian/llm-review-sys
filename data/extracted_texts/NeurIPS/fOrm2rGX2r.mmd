# C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models

Yuzhen Huang\({}^{1}\)   Yuzhuo Bai\({}^{*2}\)   Zhihao Zhu\({}^{1}\)   Junlei Zhang\({}^{1}\)   Jinghan Zhang\({}^{1}\)

Tangjun Su\({}^{1}\)   Junteng Liu\({}^{1}\)   Chuancheng Lv\({}^{2}\)   Yikai Zhang\({}^{1}\)   Jiayi Lei\({}^{1}\)

Yao Fu\({}^{3}\)   Maosong Sun\({}^{2}\)   Junxian He\({}^{14}\)

\({}^{1}\)Shanghai Jiao Tong University  \({}^{2}\)Tsinghua University  \({}^{3}\)University of Edinburgh

\({}^{4}\)The Hong Kong University of Science and Technology

ceval.benchmark@gmail.com

[https://cevalbenchmark.com](https://cevalbenchmark.com)

Equal Contribution. Full list of individual contributions is detailed in Appendix A.Correspondence to Junxian He <junxianh@cse.ust.hk>. Work done while affiliated with SJTU.The C-Eval data and evaluation code are available at [https://github.com/hkust-nlp/ceval](https://github.com/hkust-nlp/ceval).

###### Abstract

New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models _in a Chinese context_. C-Eval comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-Eval is accompanied by C-Eval Hard, a subset of very challenging subjects in C-Eval that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-Eval, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over 60%, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-Eval will help analyze important strengths and shortcomings of foundation models, and foster their development and growth for Chinese users.1

## 1 Introduction

Evaluation benchmarks are at the core role for AI development. While traditional NLP benchmarks were mostly designed to measure specific and relatively simple abilities, large language models (LLMs), or foundation models, have demonstrated various new capabilities and shifted the evaluation focus to more general and intricate skills, such as broad world knowledge and complex reasoning. To align with the new era of LLMs, new benchmarks are proposed recently to probe a diverse set of LLM abilities. For example, MMLU (Hendrycks et al., 2021), BIG-bench (Srivastava et al., 2022), and HELM (Liang et al., 2022) benchmarks attempt to aggregate a wide range of NLP tasks for holistic evaluation. Some other benchmarks specifically focus on advanced LLM abilities that emerge with scale, such as reasoning (Cobbe et al., 2021), hard math problem-solving (Hendrycks et al., 2021), and coding (Chen et al., 2021). While traditional NLP benchmarks are becoming obsolete, these new ones are extensively used in recent research to drive development of the latest LLMs (Taylor et al., 2022; Chowdhery et al., 2022; Hoffmann et al., 2022; Touvron et al., 2023; OpenAI, 2023).

However, these modern benchmarks primarily target English language, resulting in limited understanding of LLMs' capabilities in other languages. In this work, we focus on evaluating the advanced abilities of foundation models in a Chinese context, one of the most widely spoken language in theworld. Although there has been a recent surge in powerful Chinese LLMs, such as GLM-130B (Zeng et al., 2023), Wenxin Yiyan (Baidu, 2023), and MOSS (OpenLMLab, 2023), the corresponding evaluation significantly lags behind, with the CLUE benchmark (Xu et al., 2020), the Chinese counterpart of GLUE (Wang et al., 2019), serving as the best available standard. We emphasize that simply translating English benchmarks as in OpenAI (2023), even with flawless translations, does not fulfill the goal - LLMs intended for use in a Chinese environment should be evaluated on their knowledge of Chinese users' primary interests, such as Chinese culture, history, and laws, as well as other competencies unique in Chinese society. In contrast, English benchmarks tend to exhibit geographical biases towards the domestic knowledge of the regions that produce them.

To narrow the gap between Chinese LLM development and evaluation, we present C-Eval, the first comprehensive Chinese evaluation suite to thoroughly assess LLMs' advanced knowledge and reasoning abilities in a Chinese context. C-Eval consists of 13948 multiple-choice exam questions spanning 52 diverse disciplines, ranging from humanities to science and engineering, as depicted in Figure 1. The questions are collected from four difficulty levels: middle school, high school, college, and professional tests. Along with C-Eval, we introduce C-Eval Hard as an accompanied benchmark, a subset of particularly challenging subjects in C-Eval that demands highly advanced reasoning abilities to solve, such as advanced mathematics and college physics. Notably, C-Eval Hard is among the few benchmarks for _advanced_ reasoning where GPT-4 still struggles, achieving an accuracy of 53.3%, making it the first Chinese benchmark at this level.

We conduct experiments to evaluate the most advanced LLMs on C-Eval in both answer-only and chain-of-thought settings. Results show that GPT-4 is the only model that surpasses 60% average accuracy. However, its 66.4% accuracy indicates that there is still large room for improvement in current LLMs. Despite not specially tailored for Chinese data, GPT-4, ChatGPT, and Claude emerge as the top three performers on C-Eval. Upon examining the results of LLMs focused on Chinese, we find that while some models managed to narrow the gap on Chinese knowledge test with ChatGPT, acquiring reasoning abilities seems more challenging. On C-Eval Hard, in particular, most models could only retain near-random accuracy. In addition to its use as a whole, we envision C-Eval as a suite of benchmarks, subsets of which could be separately utilized to assess certain model abilities of interest and analyze important strengths and limitations of foundation models. We hope C-Eval could guide the developers to understand the abilities of their models from multiple dimensions and facilitate the growth of foundation models for Chinese users.

## 2 The C-Eval Evaluation Suite

### Design Principle

Overview:The motivation of C-Eval is to help developers quickly understand the abilities of their models from multiple dimensions, so that they could target the shortcomings of the models and

Figure 1: Overview diagram of C-Eval. Different colors of the subjects indicate four difficulty levels: middle school, high school, college, and professional.

improve them accordingly. To this end, we focus on the relatively advanced abilities of LLMs such as world knowledge and reasoning, which are arguably the most critical skills for LLMs nowadays. While different LLMs may perform similarly in simple scenarios like casual conversations, complex tasks are often the key differentiators between LLMs (OpenAI, 2023). Therefore, we construct C-Eval from real-world, challenging human exams in China that are used to assess humans' abilities from multiple dimensions. We only select questions of a multi-choice format, similar to Hendrycks et al. (2021), because: (1) metrics are clearly defined (i.e. accuracy), and (2) multi-choice questions are a simple but good proxy to evaluate the _potential_ of advanced abilities of foundation models, which we consider could be easily exploited and reflected in various downstream applications through specialized instruction tuning (Chung et al., 2022; Wang et al., 2022). Each question has four choices and only one choice is the correct answer. LLMs are intended to be used to solve these questions through prompting. The questions in C-Eval span 52 diverse disciplines that we later cluster them into broader categories as STEM, humanities, social science, and other areas. Summarized statistics of C-Eval is shown in Table 1, and more detailed statistics per subject are in Appendix B.

Attempt to mitigate data contamination:Exam questions from national tests, such as China's national college entrance exams (commonly known as Gaokao) and national professional exams, are often widely distributed and accessible online. Consequently, these questions may inadvertently be crawled and incorporated into LLM pretraining, leading to potential data leakage issues. To mitigate this risk, we collect our data either from mock exams or from small-scale local exams, such as those available online from specific high schools. This deviates from previous work that built benchmarks using the exact questions from official national exams (Zhong et al., 2023). Moreover, most samples in C-Eval are sourced from PDF or Microsoft Word documents on the Internet, rather than directly from plain text or structured questions. These documents are subsequently parsed and carefully annotated by the authors to obtain the final structured format, a process that often involves complex LaTeX equation conversion for certain subjects. This further minimizes the risk of data contamination.

### Data Collection

Subject selection:C-Eval covers four difficulty levels: middle school, high school, college, and professional. We include the standard subjects for middle and high school levels in China, except for the English subject.2 For the college level, we select 25 representative subjects from all the 13 official categories of undergraduate majors listed by the Ministry of Education in China,3 at least one subject from each category is included in C-Eval to assure comprehensiveness. For the professional level, we refer to the official national vocational qualification directory in China4 and choose 12

  
**Category** & **\# Subjects** & **\# Questions** \\   \\ STEM & 20 & 4495 \\ Humanities & 11 & 2676 \\ Social Science & 10 & 2845 \\ Other & 11 & 3932 \\   \\ Middle School & 7 & 1409 \\ High School & 8 & 1594 \\ College & 25 & 6249 \\ Professional & 12 & 4696 \\   \\ Dev & 52 & 260 \\ Valid & 52 & 1346 \\ Test & 52 & 12342 \\  Total & 52 & 13948 \\   

Table 1: Statistics of C-Eval.

Figure 2: Example from college economics. English translations are shown for better readability.

representative ones, such as physician, legal professional, and civil servant qualification exams. We also cluster these subjects into four categories in terms of their topic: STEM (Science, Technology, Engineering, and Mathematics), Social Science, Humanities, and Other areas. All the 52 subjects and their assigned categories are illustrated in Figure 1.

Data sources:The primary source of our data is mock exams freely available on the internet. In addition, a portion of the college-level questions are past exam questions from top universities in China, publicly shared by the students. A minor fraction of college questions are mock questions for the national graduate entrance exam, sourced from the Weipu website5 - these questions are not freely available to the public, and we have obtained their authorization to include around 2000 such questions into C-Eval.

Data Processing:The collected data come in various formats, primarily as PDF or Microsoft Word documents and a minor fraction as web pages. PDF documents are initially processed into text. All questions are subsequently parsed - automatically when possible, and otherwise manually by the authors - into a structured format, as exemplified in Figure 2. For subjects with complex mathematical notations such as many subjects in the STEM category, we manually convert them into standard LaTeX formats, similar to Hendrycks et al. (2021); Taylor et al. (2022). All the questions in C-Eval are processed to include exactly four choices. Most of the original questions were accompanied by four choices already, and we eliminate questions with fewer than four options and randomly drop incorrect choices for questions with more than four options. All questions also go through the standard data preprocessing pipeline, such as deduplication and cleaning. Following this, the questions undergo several rounds of human validation by the authors, and all the LaTeX notations are ensured to be complied without syntax errors. We process at least around 200 questions for each subject, and randomly split the questions into a development set, a validation set, and a test set within each subject. The development split per subject consists of five exemplars to facilitate few-shot evaluation. These dev exemplars are also annotated with explanations to enable few-shot chain-of-thought settings (Wei et al., 2022), as we detail next. The validation and test set are created with a 1:9 ratio, where the validation set is intended to be used for hyperparameter tuning.

Explanation data generation:Chain-of-thought (COT) reasoning (Kojima et al., 2022; Wei et al., 2022) - that prompts LLMs to generate a text sequence of reasoning process along with the final answer - has shown great success on reasoning-heavy tasks. Compared to zero-shot COT, the few-shot version is more commonly used and achieves the state-of-the-art performance on various tasks (Gao et al., 2022; Wang et al., 2023; Zhou et al., 2023; Xie et al., 2023). To facilitate the potential usage of C-Eval in a few-shot COT setting, we combine automatic generation and human annotation to produce high-quality explanation data for the development split. Specifically, we first prompt GPT-4 to generate step-by-step explanation to explain the ground-truth answer, then we

Figure 3: An development example with explanations from high school chemistry. English translations are shown below the corresponding Chinese text for better readability.

manually revise the generated explanations to obtain the final explanations. Details on prompting GPT-4 are in Appendix C. A dev example with explanations is illustrated in Figure 3.

### C-Eval Hard

We select 8 challenging math, physics, and chemistry subjects from C-Eval to form a separate benchmark, C-Eval Hard, which includes advanced mathematics, discrete mathematics, probability and statistics, college chemistry, college physics, high school mathematics, high school chemistry, and high school physics. These subjects often involve with complex LaTeX equations and require non-trivial reasoning abilities to solve. An example from advanced mathematics is shown in Figure 4. C-Eval Hard aligns with recent efforts to create difficult benchmarks to assess advanced reasoning abilities (Hendrycks et al., 2021; Suzgun et al., 2022), which are the key differentiators among various LLMs and could reflect LLMs' potential in general and complex scenarios. We emphasize that C-Eval Hard is the first Chinese benchmark to provide highly complicated reasoning questions.

### Evaluation

We use accuracy as the metric. While ground-truth labels of the development and validation splits are released, we keep the labels of the test split private. This is to ensure the fair use of the C-Eval, as the C-Eval data may unconsciously be included in pretraining data due to web crawling. Instead, users are required to submit their model predictions to [https://cevalbenchmark.com](https://cevalbenchmark.com) to automatically obtain the test accuracy, where a public leaderboard is maintained. Users have the option to include their submission results in the live leaderboard, depending on their own preference.

## 3 Experiment

### Setup

We evaluate LLMs in both zero- and five-shot settings on C-Eval, where the five exemplars are from the development split. We adopt regular expressions to extract answer choices from the model responses, ensuring that we can successfully extract answers for nearly all cases. We report answer-only (AO) results on both zero- and five-shot settings and chain-of-thought (COT) results on the five-shot setting only, as we found that it was often difficult to extract the answer choices from zero-shot COT predictions where the generation does not follow specific patterns. Prompts of AO and COT are shown in Appendix D. We note that in the COT setting, the five-shot exemplars could exceed the maximum context length of some LLMs for certain subjects. In such cases, we dynamically reduce the number of exemplars to fit within the context window.

### Models

To give a comprehensive view of the status of LLM in a Chinese language context, we evaluate 11 accessible top-performing LLMs that are able to process Chinese input, covering diverse organizations and varying in size, as shown in Table 2. ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023) are the strongest GPT model variants from OpenAI. Claude (Anthropic, 2022), developed by Anthropic, is often considered comparable to ChatGPT. We evaluate both the Claude-v1.3 and Claude-instant-v1.0 variants, with Claude-instant being a lighter version. Bloomz-mt (Muennighoff et al., 2022) is based on the pretrained multilingual BLOOM model (Scao et al., 2022) with multitask prompted finetuning, thus is suitable for non-English languages. LLaMA (Touvron et al., 2023) is probably

Figure 4: Example from advanced mathematics, a subject in C-Eval Hard. English translations are shown below the corresponding Chinese text for better readability.

the best open-weight foundation model so far that achieves the highest accuracy on the English MMLU benchmark within open-weight models. The aforementioned models except Bloomz-mt are English-oriented during development, while they are able to process Chinese input because a minor fraction of Chinese text is present in the pretraining data.

We further include recent LLMs developed by Chinese institutions or individuals that is Chinese-oriented. GLM-130B (Zeng et al., 2023) and ChatGLM-6B (THUDM, 2023a) are based on the General Language Model architecture (GLM, Du et al. (2022)) trained on English and Chinese data. ChatGLM-6B is further adapted on conversational data. Chinese-LLaMA (Cui et al., 2023) is an adaptation of LLaMA, which is further pretrained on Chinese data. Chinese-Alpaca (Cui et al., 2023) performs instruction tuning based on Chinese-LLaMA. MOSS (OpenLMLab, 2023) is the first publicly available Chinese LLM, and it follows a training procedure similar to ChatGPT. We note that there are some other commercial Chinese-oriented LLMs whose weights and APIs are not directly open to the public at the time of writing this paper, such as Wenxin Yiyan (Baidu, 2023), Tongyi Qianwen (Alibaba, 2023), and Xunfei Xinghuo (iFLYTEK, 2023), these models may have strong performance, yet we are not authorized to evaluate and publicize their results. Therefore, we only report results from models with open APIs or weights in this work, while we anticipate the developers of other models to submit and optionally publicize their models' results in our website. A detailed description of the evaluated models can be found in Appendix E.

### Results

General comparison:Zero- and five-shot answer-only results are shown in Table 3 and Table 4 respectively. We report the average accuracy, while a detailed breakdown of accuracy per subject is provided in Appendix F. GPT-4 is the only model that exceeds 60% average accuracy, highlighting the challenge presented by C-Eval. GPT-4 significantly outperforms all other models, with the second-best model, ChatGPT, trailing over 14 percentage points behind in both zero- and five-shot settings. Claude-v1.3 achieves similar performance to ChatGPT, in terms of both the category-wise

  
**Model** & **Creator** & **\#Parameters** & **Access** \\  GPT-4 & OpenAI & _undisclosed_ & API \\ ChatGPT & OpenAI & _undisclosed_ & API \\ Claude-v1.3 & Anthropic & _undisclosed_ & API \\ Claude-instant-v1.0 & Anthropic & _undisclosed_ & API \\ Bloomz-mt & BigScience & 176B & Weights \\ LLaMA-65B & Meta & 65B & Weights \\ GLM-130B & Tsinghua & 130B & Weights \\ ChatGLM-6B & Tsinghua & 6B & Weights \\ Chinese-LLaMA-13B & Cui et al. & 13B & Weights \\ Chinese-Alpaca-13B & Cui et al. & 13B & Weights \\ MOSS & Fu et al. & 16B & Weights \\   

Table 2: Models evaluated in this paper.

  
**Model** & **STEM** & **Social Science** & **Humanities** & **Other** & **Average** \\  Random & 25.0 & 25.0 & 25.0 & 25.0 & 25.0 \\  GPT-4 & 65.2 & 74.7 & 62.5 & 64.7 & 66.4 \\ ChatGPT & 49.0 & 58.0 & 48.8 & 50.4 & 51.0 \\ Claude-v1.3 & 48.5 & 58.6 & 47.3 & 50.1 & 50.5 \\ Bloomz-mt & 39.1 & 53.0 & 47.7 & 42.7 & 44.3 \\ GLM-130B & 36.7 & 55.8 & 47.7 & 43.0 & 44.0 \\ Claude-instant-v1.0 & 38.6 & 47.6 & 39.5 & 39.0 & 40.6 \\ ChatGLM-6B & 33.3 & 48.3 & 41.3 & 38.0 & 38.9 \\ LLaMA-65B & 32.6 & 41.2 & 34.1 & 33.0 & 34.7 \\ MOSS & 31.6 & 37.0 & 33.4 & 32.1 & 33.1 \\ Chinese-Alpaca-13B & 27.4 & 39.2 & 32.5 & 28.0 & 30.9 \\ Chinese-LLaMA-13B & 28.8 & 32.9 & 29.7 & 28.0 & 29.6 \\   

Table 3: Zero-shot average accuracy (%) in answer-only setting. We report the average accuracy over the subjects within each category. “Average” column indicates the average accuracy over all the subjects.

average and the overall average. In addition to average accuracy, Table 9 in Appendix F reveals that GPT-4 surpasses ChatGPT in almost every subject, indicating a comprehensive advantage. Among Chinese-oriented models, GLM-130B exhibits the best performance, ranking the fifth in terms of both zero- and few-shot performance, 7.0 and 14.1 points behind ChatGPT in zero- and five-shot settings respectively. We observe that smaller models, despite undergoing instruction tuning, still struggle to achieve a 40% accuracy. This contradicts recent assertions that a 10B-scale instruction-tuned model can achieve comparable performance to ChatGPT (Taori et al., 2023; Chiang et al., 2023) - we argue that while these models may perform well on simpler tasks, their inherent advanced abilities significantly lag behind when faced with more complex scenarios.

**Does few-shot prompting help?** Comparing Table 4 to Table 3, we find that while few-shot prompting helps many models achieve better results, it hurts performance of GLM-130B, Bloomz-mt, ChatGLM-6B, MOSS, and Chinese-Alpaca-13B. All of these models have undergone instruction tuning,6 and we hypothesize that the accuracy drop is because that these models have not (appropriately) incorporated few-shot demonstrations into the instruction tuning stage, as emphasized in Chung et al. (2022), thus sacrificing few-shot in-context learning performance to obtain enhanced zero-shot instruction-following abilities.

**Does chain-of-thought prompting help?** The average accuracy in the COT setting is reported in Table 5, while Table 10 in Appendix F provides a detailed breakdown of the accuracy per subject. We exclude Bloomz-mt and Chinese-Alpaca-13B since these two models are unable to generate valid COT reasoning for a large portion of questions, failing to produce final answers. All models achieve comparable or lower average accuracy than in the answer-only setting. This suggests that

  
**Model** & **STEM** & **Social Science** & **Humanities** & **Other** & **Average** \\  Random & 25.0 & 25.0 & 25.0 & 25.0 & 25.0 \\  GPT-4 & 67.1 & 77.6 & 64.5 & 67.8 & 68.7 \\ ChatGPT & 52.9 & 61.8 & 50.9 & 53.6 & 54.4 \\ Claude-v1.3 & 51.9 & 61.7 & 52.1 & 53.7 & 54.2 \\ Claude-instant-v1.0 & 43.1 & 53.8 & 44.2 & 45.4 & 45.9 \\ GLM-130B & 34.8 & 48.7 & 43.3 & 39.8 & 40.3 \\ Bloomz-mt & 35.3 & 45.1 & 40.5 & 38.5 & 39.0 \\ LLaMA-65B & 37.8 & 45.6 & 36.1 & 37.1 & 38.8 \\ ChatGLM-6B & 30.4 & 39.6 & 37.4 & 34.5 & 34.5 \\ Chinese-LLaMA-13B & 31.6 & 37.2 & 33.6 & 32.8 & 33.3 \\ MOSS & 28.6 & 36.8 & 31.0 & 30.3 & 31.1 \\ Chinese-Alpaca-13B & 26.0 & 27.2 & 27.8 & 26.4 & 26.7 \\   

Table 4: Five-shot average accuracy (%) in answer-only setting. We report the average accuracy over the subjects within each category. “Average” column indicates the average accuracy over all the subjects.

  
**Model** & **STEM** & **Social Science** & **Humanities** & **Other** & **Average** \\  Random & 25.0 & 25.0 & 25.0 & 25.0 & 25.0 \\  GPT-4 & 67.3 & 76.5 & 64.4 & 66.6 & 68.3 \\ Claude-v1.3 & 51.9 & 63.2 & 50.9 & 53.6 & 54.2 \\ ChatGPT & 47.8 & 58.3 & 47.7 & 48.5 & 50.0 \\ Claude-instant-v1.0 & 43.3 & 52.7 & 41.3 & 42.4 & 44.5 \\ ChatGLM-6B & 29.9 & 40.0 & 37.9 & 34.5 & 34.5 \\ MOSS & 27.3 & 38.1 & 33.6 & 29.4 & 31.2 \\ LLaMA-65B & 28.0 & 36.3 & 29.3 & 30.0 & 30.3 \\ GLM-130B & 24.8 & 33.1 & 30.8 & 30.0 & 28.8 \\ Chinese-LLaMA-13B & 20.5 & 30.5 & 28.2 & 27.1 & 25.4 \\   

Table 5: Five-shot average accuracy (%) in chain-of-thought setting. We report the average accuracy over the subjects within each category. “Average” column indicates the average accuracy over all the subjects. Bloomz-mt and Chinese-Alpaca-13B are excluded as they could not generate valid reasoning and thus fail to answer for many questions.

COT prompting does not necessarily improve results for many subjects in C-Eval. The primary reasons for this are twofold: (1) many subjects in C-Eval are not reasoning-intensive, and additional reasoning steps may impair performance. This observation is supported by Chung et al. (2022), who noted performance degradation on MMLU with COT prompting. (2) Some models fail to leverage the benefits of COT prompting, particularly those that did not undergo COT-inclusive instruction tuning. Chung et al. (2022) reported this, noting an 8-point accuracy drop when using COT on MMLU with the 540B PaLM model. This finding partly elucidates the significant decrease in performance of the GLM-130B and LLaMA-65B models in the COT setting. Encouragingly, we still observe that COT prompting leads to considerable improvements for some models in certain subjects - for example, detailed results in Table 10 show that COT improves GPT-4's performance in college physics from 50.6% to 60.2%, in probability and statistics from 53.6% to 62.0%, ChatGLM's performance in middle school physics from 20.2% to 41.0%, and in high school geography from 29.2% to 38.2%.

Difference between English- and Chinese-oriented models:GLM-130B is the best-performing Chinese-oriented model in our assessment, thus we focus on comparing it to the represented English-oriented model, ChatGPT, in zero-shot answer-only settings. We do not analyze GPT-4 here since it is not at the same level as all other models, and comparing GLM-130B to it is not very helpful and informative. As illustrated in Table 3, while GLM-130B underperforms ChatGPT by 7.0 points on overall average, the gap significantly narrows on the social science and humanities category, lagging only 2.2 and 1.1 points behind respectively. This reflects that by leveraging more Chinese data, the model might achieve performance on par with or even superior to ChatGPT in areas pertaining to Chinese knowledge, such as history, politics, and law, highlighting situations where Chinese-oriented models may have the upper hand. However, concurrently, we note a significant difference of 12.3 points between GLM-130B and ChatGPT in the STEM category, which implies a substantial gap in more complex tasks that necessitate advanced reasoning skills.

Results on C-Eval Hard:Table 6 shows the average accuracy on C-Eval Hard. GPT-4 can only achieve 53.3%, 54.9%, 56.8% accuracy on zero-shot AO, five-shot AO, and five-shot COT settings respectively, implying the difficulty of C-Eval Hard. Interestingly, chain-of-thought prompting improves GPT-4 slightly on these extremely challenging subjects. Indeed, only GPT-4, ChatGPT, and Claude manage to make meaningful progress - improving by at least 10 points - over a random baseline. Our results further confirm that some critical distinction among LLMs comes out when the tasks become complex enough. We underscore the importance of evaluating LLMs in such challenging settings, as current LLM development goes beyond creating a casual chatbot - it involves the development of complex systems or agents capable of interacting with various data types, receiving feedback, reasoning and using tools, and even performing actions (Mialon et al., 2023).

Results on the validation split:Since we do not publicly release the labels for our test split, we provide the average accuracy on the validation split as a reference for developers. The validation split comprises a total of 1346 questions, with each subject contributing fewer than 30 validation questions on average. Therefore, tracking accuracy on a specific subject may not yield significant insights. Instead, we report the average answer-only accuracy across all subjects in Table 7. The average validation accuracy closely mirrors the average test accuracy as presented in Table 3 and Table 4. Additionally, the model ranking on the validation split broadly corresponds to that on the test split. These observations suggest that developers may use the average validation accuracy as a good indicator for expedited development processes.

  
**Model** & **Zero-shot AO** & **Five-shot AO** & **Five-shot COT** \\  GPT-4 & 53.3 & 54.9 & 56.8 \\ Claude-v1.3 & 37.6 & 39.0 & 39.2 \\ ChatGPT & 36.7 & 41.4 & 35.0 \\ Claude-instant-v1.0 & 32.1 & 35.5 & 33.4 \\ Blooming-mt & 30.8 & 30.4 & – \\ GLM-130B & 30.7 & 30.3 & 22.6 \\ LLaMA-65B & 29.8 & 31.7 & 21.4 \\ ChatGLM-6B & 29.2 & 23.1 & 26.1 \\ MOSS & 28.4 & 24.0 & 21.6 \\ Chinese-LLaMA-13B & 27.5 & 27.3 & 15.4 \\ Chinese-Alpaca-13B & 24.4 & 27.1 & – \\   

Table 6: Average accuracy on C-Eval Hard in both answer-only (AO) and chain-of-thought (COT) settings.

## 4 Related Work

English benchmarks:Traditional English benchmarks mainly focus on assessing certain abilities of models on a single task or a single type of tasks, such as natural language understanding (NLU, Wang et al. (2019)), reading comprehension (Rajpurkar et al., 2018), machine translation (Bojar et al., 2014), and summarization (Hermann et al., 2015; Narayan et al., 2018). As a representative example, the GLUE benchmark (Wang et al., 2019) combines a collection of NLU tasks, and has witnessed superhuman model performance due to the burst of pretraining models such as BERT (Kenton and Toutanova, 2019) and GPT (Radford et al., 2019). In order to assess the capabilities of LLMs more comprehensively, recent benchmarks have cast light on the broader knowledge and advanced abilities. The MMLU benchmark (Hendrycks et al., 2021) provides multi-domain and multi-task evaluation collected from real-world examinations and books. LLMs' performance on MMLU fluctuates around random-chance accuracy until they reach the scale of GPT-3. The BIG-bench benchmark (Srivastava et al., 2022) consists of 204 diverse tasks, some of which are considered to be beyond the capabilities of current LLMs. The HELM benchmark (Liang et al., 2022) aggregates 42 different tasks and evaluates LLMs with 7 metrics ranging from accuracy to robustness.

Chinese benchmarks:Despite the flourishing of English benchmark, language abilities in Chinese language environment remain under-developed. The CLUE benchmark (Xu et al., 2020) is the first large-scale Chinese NLU benchmark, and still serves as the most widely-used and best available Chinese benchmark. Recently, the AGIEval benchmark (Zhong et al., 2023) contains data from the Chinese College Entrance Exam, Chinese lawyer qualification test and Chinese civil service examination. The MMCU benchmark (Zeng, 2023) consists of tests from four major domains including medicine, law, psychology and education, which are also collected from Chinese College Entrance Exam, qualification test as well as university examinations. Compared to AGIEval and MMCU, C-Eval (1) has a broader coverage of domains (SS2.2), (2) features four different levels of difficulty - particularly, the C-Eval Hard benchmark is the first Chinese benchmark to provide sophisticated reasoning problems, and (3) makes an effort to mitigate data leakage - our questions mostly come from mock exams as PDF or Microsoft Word documents that are further processed by us, while AGIEval and MMCU collects the exact questions from past national exams in China.

## 5 Discussion

We believe that the evaluation of LLMs should transcend the scope of casual conversational bots, guiding developers in preparing LLMs to function in more complex scenarios. This was the primary motivation behind the creation of C-Eval, a challenging evaluation suite. We hope that C-Eval along with C-Eval Hard have made important progress on this direction particularly in a Chinese context. We also note that, C-Eval, along with all the English-language benchmarks, is far from perfect for LLM evaluation. There are many other abilities such as reasoning over and calling APIs, as well as multiple aspects that extend beyond accuracy, including safety, bias, and robustness. We leave further exploration on their evaluation for future work.

  
**Model** & **Zero-shot** & **Five-shot** \\  GPT-4 & 66.7 & 69.9 \\ Claude-v1.3 & 52.1 & 55.5 \\ ChatGPT & 50.8 & 53.5 \\ Bloomz-mt & 45.9 & 38.0 \\ GLM-130B & 44.2 & 40.8 \\ Claude-instant-v1.0 & 43.2 & 47.4 \\ ChatGLM-6B & 39.7 & 37.1 \\ LLaMA-65B & 38.6 & 39.8 \\ MOSS & 35.1 & 28.9 \\ Chinese-Alpaca-13B & 32.0 & 27.2 \\ Chinese-LLaMA-13B & 29.4 & 33.1 \\   

Table 7: Average accuracy on the validation split in the answer-only setting.