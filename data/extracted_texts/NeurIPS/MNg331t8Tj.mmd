# Advancing Fine-Grained Classification by Structure and Subject Preserving Augmentation

Eyal Michaeli

Department of Computer Science

Reichman University

eyal.michaeli@post.runi.ac.il &Ohad Fried

Department of Computer Science

Reichman University

ofried@runi.ac.il

###### Abstract

Fine-grained visual classification (FGVC) involves classifying closely related sub-classes. This task is difficult due to the subtle differences between classes and the high intra-class variance. Moreover, FGVC datasets are typically small and challenging to gather, thus highlighting a significant need for effective data augmentation. Recent advancements in text-to-image diffusion models offer new possibilities for augmenting classification datasets. While these models have been used to generate training data for classification tasks, their effectiveness in full-dataset training of FGVC models remains under-explored. Recent techniques that rely on Text2Image generation or Img2Img methods, often struggle to generate images that accurately represent the class while modifying them to a degree that significantly increases the dataset's diversity. To address these challenges, we present SaSPA: Structure and Subject Preserving Augmentation. Contrary to recent methods, our method does not use real images as guidance, thereby increasing generation flexibility and promoting greater diversity. To ensure accurate class representation, we employ conditioning mechanisms, specifically by conditioning on image edges and subject representation. We conduct extensive experiments and benchmark SaSPA against both traditional and recent generative data augmentation methods. SaSPA consistently outperforms all established baselines across multiple settings, including full dataset training, contextual bias, and few-shot classification. Additionally, our results reveal interesting patterns in using synthetic data for FGVC models; for instance, we find a relationship between the amount of real data used and the optimal _proportion_ of synthetic data. We release our source code.

Figure 1: Various generative augmentation methods applied on Aircraft . Text-to-image often compromises class fidelity, visible by the unrealistic aircraft design (i.e., tail at both ends). Img2Img trades off fidelity and diversity: lower strength (e.g., 0.5) introduces minimal semantic changes, resulting in higher fidelity but limited diversity, whereas higher strength (e.g., 0.75) introduces diversity but also inaccuracies such as the incorrectly added engine. In contrast, SaSPA achieves high fidelity and diversity, critical for Fine-Grained Visual Classification tasks. _D - Diversity. F - Fidelity_Introduction

Deep learning's remarkable success across various applications relies heavily on large-scale annotated datasets, such as ImageNet , which provide the foundational data necessary for training effective models. However, in fine-grained visual classification (FGVC), the datasets are typically smaller and less diverse, presenting unique challenges in training robust models. Data augmentation emerges as a natural solution to artificially enhance dataset size and variability. However, traditional data augmentation methods are limited in the amount of diversity they introduce .

Text-to-image diffusion models have opened new avenues for generative image augmentation. Within the realm of classification, diffusion models have shown promise on standard image recognition datasets such as ImageNet [2; 50; 3]. However, their application in FGVC remains under-explored.

Generating synthetic data for FGVC presents unique challenges, as preserving class fidelity is (1) more crucial than with common object datasets due to the similarity between classes and the reliance of the models on subtle details to differentiate between classes and (2) challenging to achieve because the training data for text-to-image models often lacks a substantial representation of these distinct objects . For instance, there might be enough data to accurately represent "An airplane", but not "A Boeing 767-200 airplane".

Recent generative methods evaluated for FGVC augmentation typically use real images as guidance in an Img2Img manner [15; 54; 21; 60]. While this helps maintain visual similarity to the target domain, it limits the degree of diversity that can be introduced, resulting in a trade-off between class fidelity and diversity  (see Figure 1). We aim to free the generative process from this constraint of adhering to specific source images. To this end, we propose SaSPA: Structure and Subject Preserving Augmentation, a method that conditions the generation on more abstract representations rather than direct image inputs. Specifically, we leverage structural conditioning in the diffusion model via edge maps extracted from source images. This allows the generated samples to respect the broad shape and composition of objects in the target domain. Crucially, the lack of specific image conditioning enables greater flexibility in rendering surface details. To further ensure the preservation of fine-grained class characteristics, we integrate subject representation conditioning. By combining edge-based structural conditioning with category-level conditioning, SaSPA can generate highly diverse, class-consistent synthetic images without being overly influenced by any specific real data sample.

Furthermore, to enrich the diversity and applicability of our generated images, we generate prompts with an LLM according to the dataset meta-class (a class encompassing all sub-classes). These prompts are designed to guide the diffusion model in producing variations that are not only diverse but also class-consistent and relevant to the target domain. Additionally, to maintain the quality and relevance of the generated images, we implement a robust filtering strategy that eliminates any samples that fail to meet predefined quality thresholds by utilizing a dataset-trained model and CLIP.

**We summarize our contributions as follows**: (1) We propose SaSPA, a generative augmentation pipeline for fine-grained visual classification that generates diverse, class-consistent synthetic images without relying on specific real images for conditioning. (2) We conduct extensive experiments and benchmark SaSPA against both traditional and recent generative data augmentation methods. SaSPA consistently outperforms all established baselines across multiple settings, including the challenging and less-explored full dataset training, as well as in scenarios of contextual bias and few-shot classification. (3) Our analysis provides insights on effectively leveraging synthetic data to improve the performance of fine-grained classification models. For instance, we find that as the amount of real data decreases, we should increase the _proportion_ of synthetic data used.

## 2 Related Work

**Data Augmentation with Generative Models.** Synthesizing training samples using generative models is an active and challenging area of research. Initial efforts in this field [70; 4; 49; 33] leveraged Generative Adversarial Networks (GANs) to create labeled training samples. Recently, the emergence of powerful text-to-image diffusion models such as Stable Diffusion  has created exciting opportunities for advancing generative image augmentation. These models have been employed across a range of applications, including semantic segmentation [19; 61; 62; 36], object detection [8; 7; 59], and classification [34; 2; 50; 3], demonstrating their versatility and effectiveness For image classification tasks, diffusion models have demonstrated promising results on standard image recognition datasets such as ImageNet [2; 50; 3]. However, their application in FGVC has typically been limited to particular settings such as few-shot learning [21; 54; 52; 26] where data scarcity significantly enhances the impact of data augmentation, contextual bias, and domain generalization , settings that are more straightforward to enhance as targeted augmentations can directly address and balance the skewed distributions. Our goal is to tackle the more challenging task of training on full FGVC datasets. Moreover, recent generative augmentation methods often use Img2Img techniques like SDEdit to maintain class fidelity, though this comes at the cost of reduced diversity. Some methods involve fine-tuning the network or its components, which can be expensive and may still struggle to balance class fidelity with the added diversity necessary for effective FGVC augmentation. Our goal is to avoid the decrease in diversity associated with using real images as guidance and to avoid the complexity and expense of fine-tuning the generation model.

**Text-to-Image Diffusion Models.** Diffusion models  have achieved unprecedented success in generating photo-realistic images . Models like Stable Diffusion , DALL-E 2 , and others [37; 48] exemplify this capability. These models have also driven advancements in other generative areas. For instance, SDEdit  integrates real images partway through the reverse diffusion process for image editing. Techniques like ControlNet  and T2I-Adapter  condition image generation on inputs beyond text such as edges and world normals, while methods such as Textual Inversion  and DreamBooth  can generate specific subjects from just a few example images. More recently, BLIP-diffusion , which is based on Stable Diffusion and BLIP-2 , has demonstrated impressive zero-shot subject-driven generation using only one example image. Our method benefits directly from these advancements, employing ControlNet and BLIP-diffusion.

**Traditional Data Augmentation.** Traditional data augmentation methods typically include operations such as random cropping, flipping, and color-space changes to generate new variations . Recent strategies, like mixup-based methods, aim to enhance diversity by blending patches from two input images  or using convex combinations . Weakly Supervised Data Augmentation Network (WS-DAN), used in recent FGVC works such as CAL , aims to improve FGVC by generating attention maps to highlight discriminative object parts and guiding augmentation with attention cropping and dropping. However, these methods introduce limited diversity , as they do not alter the semantic features present in the image.

## 3 Method

Our goal is to augment a labeled training dataset for FGVC to increase its diversity while faithfully representing the sub-classes. The key insight of our method is to minimize reliance on any particular source image during generation and instead condition the generation on more abstract representations, thereby increasing diversity while accurately representing the designated class (see Figure 1). To achieve this, we employ abstract conditions such as edges, which capture the object's structure,

Figure 2: **SaSPA Pipeline: For a given FGVC dataset, we generate prompts via GPT-4 based on the meta-class. Each real image undergoes edge detection to provide structural outlines. These edges are used \(M\) times, each time with a different prompt and a different subject reference image from the same sub-class, as inputs to a ControlNet with BLIP-Diffusion as the base model. The generated images are then filtered using a dataset-trained model and CLIP to ensure relevance and quality.**

[MISSING_PAGE_FAIL:4]

reference image is selected from the same sub-class but _differs_ from the real image used to extract the edge map (we experiment with BLIP-diffusion inputs in Table 4). Specifically, we generate \(M=2\) augmentations for each real image in the training set: we extract an edge map for each real image, and for each edge map, we randomly select \(M\) prompts and \(M\) subject reference images from the same sub-class. These inputs are then fed into the generation model of ControlNet with BLIP-diffusion as the base model to produce \(M\) augmentations of the real image. Example augmentations are visualized at Figure 3. DTD  has only one prompt per real image, because we utilize image captions as prompts for it, as explained in Appendix D.

### Filtering

We aim to remove low-quality augmentations, which appear in two forms: (1) _meta-class_ misrepresentation and (2) _sub-class_ misrepresentation.

**Semantic Filtering**. To alleviate _meta-class_ misrepresentation, we utilize semantic filtering as described in ALIA . Using CLIP , this process evaluates the relevance of generated images to the specific task at hand. For example, in a car dataset, each generated image is assessed against a variety of prompts such as "a photo of a car", "a photo of an object", "a photo of a scene", "a photo", and "a black photo". Images that CLIP does not recognize as "a photo of a car" are excluded to ensure that the augmented dataset closely aligns with the target domain.

**Predictive Confidence Filtering**. To ensure each augmentation faithfully represents its designated _sub-class_, we implement a predictive confidence filtering strategy inspired by recent work  strategy _CLIP Filtering_. This method employs CLIP  to filter out images that do not strongly correlate with the textual labels of their class among all classes in the dataset. However, the limitation of using CLIP in this context is its insufficient granularity in understanding fine-grained concepts. For our method, we discard any augmented images for which the true label does not rank within the top-k predictions of a baseline model trained on the original dataset. This approach helps to exclude images that likely misrepresent the source _sub-class_, thus maintaining a high-quality dataset for model training. In our implementation, we use \(k=10\). Further details about this method, the baseline model used, and other filtering techniques are discussed in Appendix E.2.

### Training Downstream Model

We train the downstream classification model using the filtered, generated samples. Let \(\) denote the augmentation ratio, representing the probability that a real training sample will be replaced with a generated synthetic sample during each epoch. This replacement process is repeated for every sample in each epoch, allowing each real sample to be either retained or replaced by an augmented version. We employ this replacement strategy instead of simply adding the augmented data to the original dataset, as doing so would unnecessarily increase the number of iterations per epoch. By that, we ensure fair comparisons across training sessions.

Figure 3: Example augmentations using our method (SaSPA). The {} placeholder represents the specific sub-class.

## 4 Experiments

Our objective is to explore the extent to which synthetic data, particularly through our approach, contributes to various FGVC tasks. We aim to understand the significance of each component of our method and identify optimal strategies for leveraging synthetic data in FGVC.

### Experimental Setup

For generation, we employ BLIP-diffusion for _SaSPA_ and Stable Diffusion v1.5  for all other diffusion-based augmentation methods.

For training, we follow the implementation strategy outlined in the CAL study , tailored for FGVC. We use ResNet50  as the primary architecture within the CAL framework unless specified otherwise. Each dataset is fine-tuned using pre-trained ImageNet weights. More data generation and training details can be found in Appendix D.

**Comparison Methods.** We benchmark our method, _SaSPA_, against established traditional and generative data augmentation techniques. In the _traditional_ category, our comparisons include: **CAL-Aug :** utilizes random flipping, cropping, and color-space variations. **RandAugment :** applies a series of random image transformations such as rotation, shearing, and color variations to training images. **CutMix :** generates mixed samples by randomly cutting and pasting patches between training images to encourage the model to learn more localized and discriminative features. **Combined Methods:** Tests the synergistic effects of CAL-Aug with CutMix and RandAug with CutMix. In the _generative_ category, we compare with: **Real-Guidance :** applies Img2Img with a low translation strength (\(s=0.15\)) to maintain high fidelity to the original images. **ALIA :** Uses real image captions and GPT-generated domain descriptions based on these captions as prompts for Img2Img translations. Detailed descriptions of these baseline methods are in Appendix D.3.

### Fine-grained Visual Classification

**Datasets.** We evaluate on five FGVC datasets, using the _full_ datasets for training. We use Aircraft , Stanford Cars , CUB , DTD , and CompCars . For datasets lacking a predefined validation split, we establish one. For CompCars, we utilize the exterior car parts split, focusing exclusively on classifying images of car components: head light, tail light, fog light, and front into the correct car type. Further details on the exact splits are provided in Appendix C.

**Results.** We present the test accuracy of various augmentation methods in Table 1. For each dataset, the most effective _traditional_ augmentation method (marked by an underline) is identified using its validation set and consistently combined with all _generative_ approaches to optimize performance for that dataset. This approach is grounded in findings that standalone _generative_ methods generally perform better when integrated with _traditional_ augmentations , a trend also evident in Table 7.

   Type & Augmentation Method & Aircraft & CompCars & Cars & CUB & DTD \\   & No Aug & 81.4 & 67.0 & 91.8 & 81.5 & 68.5 \\  & CAL-Aug & 84.9 & 70.5 & 92.4 & 82.5 & 69.7 \\  & RandAug & 83.7 & 72.5 & 92.6 & 81.5 & 69.3 \\  & CutMix & 81.8 & 66.9 & 91.7 & 81.8 & 69.2 \\  & CAL-Aug + CutMix & 84.5 & 70.2 & 92.7 & 82.4 & 69.7 \\  & RandAug + CutMix & 84.0 & 72.6 & 92.7 & 81.2 & 69.2 \\   & Real Guidance & 84.8 & 73.1 & 92.9 & 82.8 & 68.5 \\  & ALIA & 83.1 & 72.9 & 92.6 & 82.0 & 69.1 \\   & SaSPA w/o BLIP-diffusion & **87.4** & 74.8 & 93.7 & 83.0 & 69.8 \\  & SaSPA & 86.6 & **76.2** & **93.8** & **83.2** & **71.9** \\   

Table 1: **Results on full FGVC Datasets.** This table presents the test accuracy of various augmentation strategies across five FGVC datasets. The highest values for each dataset are shown in **bold**, while the highest validation accuracies achieved by traditional augmentation methods are underlined.

Our key findings: (1) _SaSPA_ achieves consistent improvements across all datasets, with or without BLIP-diffusion integration, and it consistently outperforms traditional and generative augmentation methods by a significant margin. (2) The benefits of BLIP-diffusion vary depending on dataset characteristics; while it improves performance in datasets where texture and style play a crucial role in differentiation, such as DTD, CUB, and CompCars, it is not optimal for the Aircraft dataset, and has no significant impact on the Cars dataset, where structural features are more important for classification. We attribute this to the fact that using BLIP-diffusion confines the augmentations to be similar to other subjects within the same _sub-class_, which can limit diversity. (3) Both generative baselines fail to achieve consistent improvements and sometimes even reduce performance.

### Few-shot Learning

**Experimental Setting**. This section investigates the efficacy of various augmentation strategies in few-shot fine-grained classification scenarios, focusing on how synthetic data affects performance with increasing numbers of training examples ("shots"). We conduct evaluations using three datasets: Aircraft , Cars , and DTD , assessing performance at 4, 8, 12, and 16 shots.

The training and data generation approaches remain consistent with those described in Appendix D, with two modifications: we use 100 epochs (down from 140), and we do not employ _predictive confidence filtering_ for shots 4 and 8. The latter adjustment is due to the reduced reliability of the model's predictions, resulting from the limited training data. Additionally, we increase the augmentation ratio to \(=0.6\), as identified to be better for scenarios with limited data in Table 5.

**Results**. The results in Figure 4 show that _SaSPA_ consistently outperforms all other augmentation methods across all datasets and various shot counts. As seen in other works [54; 21], the benefit of augmentation diminishes as the number of shots increases, a trend most noticeable in the Cars dataset. Contrary to prior work, the gains provided by _SaSPA_ remain substantial even at higher shot counts; notably, in the Cars dataset at 16 shots, _SaSPA_ achieves an accuracy of 91.0%, surpassing the second-best performance of 88.3% by RG . Interestingly, _SaSPA_ sometimes matches or exceeds the performance enhancement achieved by increasing the dataset size. For example, in the DTD dataset, utilizing _SaSPA_ with 8 shots results in an accuracy of 54.8%, slightly surpassing the 54.6% obtained when adding 50% more real data (a total of 12 shots) when relying solely on real data and the best traditional augmentation.

### Mitigating Contextual Bias (Airbus vs. Boeing)

**Experimental Setup.** To evaluate the effectiveness of our method in mitigating real-world contextual biases, we use the contextual bias split of the Aircraft  dataset constructed by Dunlap et al. . The split uses two visually similar classes: Boeing-767 and Airbus-322. Each image in this split is categorized as "sky", "grass", or "road" depending on its background, with ambiguous examples filtered out. The bias in the dataset is introduced by training on 400 samples where Airbus aircraft are exclusively associated with road backgrounds and Boeing aircraft with grass backgrounds, although both types may appear against sky backgrounds. The exact split breakdowns are in Table 19.

Figure 4: Figure 4: Few-shot test accuracy across three FGVC datasets: Aircraft, Cars, and DTD, using different augmentation methods. The number of few-shots tested includes 4, 8, 12, and 16. We can see that for all datasets and shots, SaSPA outperforms all other augmentation methods.

We follow the same training and generation implementation settings as for the FGVC setting (Appendix D), and we compare against the same _generative_ methods. We also compare against the optimal _traditional_ augmentation for the Aircraft dataset (CAL-Aug), as defined in Section 4.2.

**Results.** The results in Table 2 show that _SaSPA_ outperforms all other methods in overall and out-of-domain (OOD) accuracy, demonstrating its effectiveness in mitigating contextual bias. However, it falls short in in-domain (ID) accuracy. A distinct inverse relationship is observed between ID and OOD accuracy: methods that induce more significant changes from the original image--such as ALIA, which uses stronger translations than Real-Guidance (RG)--tend to achieve higher OOD accuracy but lower ID accuracy. This trend suggests that greater modifications can help reduce over-fitting to in-domain characteristics, enhancing the model's ability to generalize effectively to new, unseen conditions. As depicted in Figure 1, even a higher translation strength (\(s=0.5/0.75\) ) yields limited diversity compared to our method. Consequently, the alterations produced by RG and ALIA are insufficient to significantly mitigate the contextual bias present in the dataset, as effective background variation is crucial for addressing such biases.

### Comparing _SaSPA_ with Concurrent Work _diff-mix_

In this section, we compare our method with _diff-mix_, a generative augmentation approach proposed concurrently by Wang et al. . _diff-mix_ was also evaluated on full FGVC datasets and demonstrated impressive results. This method enriches datasets through image translations between classes, utilizing personalization techniques such as textual inversion  and DreamBooth  to fine-tune the generative model for each _sub-class_. This fine-tuning enhances the model's ability to capture and represent class-specific nuances. In contrast, our method does not involve fine-tuning, aiming to simplify the process and minimize computational costs.

**Experimental Setup.** In this analysis, we evaluate the performance of our _SaSPA_ augmentation method using the _diff-mix_ training setup, as detailed in their work. By using their open-source implementation, we further assess the robustness of our method with a different training setup. To ensure fairness, We use the same number of augmentations (M) as diff-mix did. More details regarding training setup are in Appendix B.3.

**Results.** Our results, detailed in Table 3, highlight where _SaSPA_ performs well and identify areas for potential improvement. The findings can be summarized as follows: (1) While _diff-mix_ employs computationally intensive fine-tuning techniques to enhance class representation, we prioritize simplicity and lower computational demands in our approach. Despite this, _SaSPA_ consistently outperforms _diff-mix_ on both the Aircraft and Cars datasets across all architectures, whether combined with CutMix or used alone, demonstrating its robustness across various augmentation contexts. This

   Augmentation Method & Acc. & ID Acc. & OOD Acc. \\  Best Trad Aug (CAL-Aug) & 71.0 & **88.2** & 10.2 \\ Real Guidance  & 71.7 & 86.9 & 17.7 \\ ALIA  & 71.8 & 84.9 & 25.1 \\ SaSPA w/o BLIP-diffusion & **73.0** & 81.9 & **41.5** \\   

Table 2: Classification performance on the contextually biased Aircraft dataset , detailing overall, in-domain (ID) and out-of-domain (OOD) accuracies for each augmentation method.

    & &  &  \\  Aug. Method & FT Strategy & Aircraft & Car & CUB & Aircraft & Car & CUB \\  CutMix \(\) & - & 89.44 & 94.73 & 87.23 & 83.50 & 94.83 & **90.52** \\  Diff-Mix \(\) & TI+DB & 90.25 & 95.12 & 87.16 & 84.33 & 95.09 & 90.05 \\ Diff-Mix + CutMix\(\) & TI+DB & 90.01 & 95.21 & **87.56** & 85.12 & 95.26 & 90.35 \\  SaSPA (Ours) & ✗ & 90.59 & 95.29 & 86.92 & 85.48 & 95.12 & 89.70 \\ SaSPA (Ours) + CutMix & ✗ & **90.79** & **95.34** & 87.14 & **85.72** & **95.37** & 89.92 \\   

Table 3: Comparison to concurrent work _diff-mix_. Test accuracy on 3 FGVC datasets. \(\) indicates values taken from the diff-mix paper. _TI - Textual Inversion, DB - DreamBooth, ✗- No fine-tuning_.

also shows that conditioning the generation on more abstract representations, as we do for correct class representation, can overcome the absence of extensive fine-tuning. (2) The CUB dataset posed unique challenges, with _diff-mix_ outperforming _SaSPA_ using ResNet50 and both _diff-mix_ and _SaSPA_ under-performing relative to CutMix using ViT-B/16. Notably, despite _SaSPA_ outperforming all methods, including CutMix in Table 1, it does not perform as well here. We hypothesize that the use of higher resolution emphasizes finer details in each class, which may be overwhelming for _SaSPA_ on some datasets but less so for _diff-mix_, likely due to the heavy fine-tuning process integrated into their method. These results indicate that some form of fine-tuning might be advantageous for complex datasets like CUB to achieve better performance. The full table, including comparisons to more augmentation methods, can be found in Appendix B.3.

### Effect of Different Generation Strategies on Performance

In Table 4, we conduct an extensive ablation study to evaluate the effectiveness of our proposed generation strategies. Specifically, we examine the integration of edge guidance, Img2Img as an alternative for edge guidance with strength = \(0.5\) and in combination with edge guidance with strength = \(0.85\), and subject representation. We also investigate the effect of using the same image for both edges extraction and the subject reference image ("Edges=Ref."). Additionally, we test the impact of appending half of the prompts with artistic styles (column 'Art.') as described in Appendix B.8.

**The results** demonstrate the importance of combining structural and subject-level conditioning while enabling diverse generations through separate input sources, yielding the best performance across most datasets. Key observations include: (1) Edge Guidance alone improves performance significantly compared to Text-to-Image or SDEdit  (Img2Img), highlighting its important role in providing structural guidance. (2) Subject representation alone does not enhance performance, indicating additional structural conditioning is necessary. (3) Using different source images for edges and subject reference images adds beneficial diversity. (4) Surprisingly, text-to-image generation (first row) outperforms SDEdit, likely due to its increased diversity and despite the lower fidelity, which our filtering mechanism can handle as it filters out low-fidelity images. (5) Incorporating artistic prompts has inconsistent effects, usually boosting performance with Edge Guidance but often degrading it when combined with subject representation. This inconsistency may stem from the fact that subject representation uses BLIP-diffusion , which is a different base model than Stable Diffusion , as Stable Diffusion is fine-tuned. Additionally, in CUB, artistic prompts offer no improvement even when using Edge Guidance without subject representation, likely due to the dataset's heavy reliance on color as a primary discriminator between bird types, potentially disrupted by artistic prompts.

   Method & Edge Guidance & Img2Img & Subj. & Inputs & Art. & Aircraft & Cars & CUB & DTD \\  Best trad aug & - & - & - & - & - & 84.3 & 92.7 & 81.4 & 67.9 \\  _Ours_ & & & & - & & 83.3 & 92.9 & 82.1 & 67.8 \\  & & ✓ & & - & & 83.0 & 92.8 & 80.7 & 66.0 \\  & & & ✓ & - & & 81.5 & 91.6 & 81.1 & 68.1 \\   & ✓ & & & - & & 85.7 & 93.4 & 81.8 & 68.4 \\  & ✓ & & & - & ✓ & **86.2** & 93.8 & 81.6 & 68.6 \\  & ✓ & ✓ & & - & ✓ & 84.9 & 93.0 & 81.3 & 67.8 \\   & ✓ & & ✓ & Edges=Subj. & & 85.2 & 93.1 & 81.3 & 68.7 \\  & ✓ & & ✓ & Edges\(\)Subj. & ✓ & 85.5 & 93.7 & 82.6 & 69.2 \\  & ✓ & & ✓ & Edges\(\)Subj. & & 85.4 & **93.9** & **83.0** & **69.9** \\   

Table 4: Ablation Study: Effects of different generation strategies on various FGVC Datasets. ‘Subj. means subject representation is used. ‘Edges=Subj.’ indicates that the real image used to extract the edges is the same as the subject reference image. ‘Art.’ indicates that half the prompts are _appended_ with artistic styles. For each dataset, **bold** indicates the highest validation accuracy, and underline indicates the second highest. Ticks under each column mean the component is used.

## 5 Limitations and Future Directions

**Limitations**. Although we demonstrated that SaSPA could generate images with high class fidelity through conditions such as edge maps and subject representation, it still remains dependent on the underlying generation models. For instance, we found that applying SaSPA to the CUB dataset at a higher resolution does not improve performance. Additionally, SaSPA relies on large language models (LLMs) to generate relevant and diverse prompts given the meta-class. While this is usually effective, it may not produce optimal prompts if the LLM lacks knowledge of the meta-class.

**Future Directions**. Several avenues exist to enhance the flexibility and performance of our method in future research. Firstly, we hope our work inspires the use of additional methods to condition the synthesis process beyond using real images, as we have shown to be effective. Another promising avenue is to apply SaSPA to additional tasks such as classification of common objects, object detection, and semantic segmentation. Additionally, maintaining temporal consistency in settings that use consecutive frames, such as autonomous driving, remains a significant challenge. Addressing this issue could expand the applicability of SaSPA to a broader range of use cases. Moreover, ongoing advancements in generative models are likely to bring further improvements to our pipeline. Finally, effectively generating and using synthetic data remains an active research area, and identifying optimal strategies for both the generation process and the training integration remains an important future direction.

## 6 Conclusion

We propose SaSPA, a generative augmentation method specifically designed for FGVC. Our method generates diverse, class-consistent synthetic images through conditioning on edge maps and subject representation. SaSPA consistently outperforms both traditional and recent generative data augmentation methods. It demonstrates superior performance across multiple settings, including multiple setups of the challenging and less-explored full dataset training, as well as in scenarios of contextual bias and few-shot classification. Limitations and future directions are discussed in Section 5.

## 7 Acknowledgments

This work was supported in part by the Israel Science Foundation (grant No. 1574/21).