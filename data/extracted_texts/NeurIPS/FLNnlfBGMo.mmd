# Efficient Prompt Optimization Through the Lens of Best Arm Identification

Chengshuai Shi

University of Virginia

cs7ync@virginia.edu

&Kun Yang

University of Virginia

ky9tc@virginia.edu

&Zihan Chen

University of Virginia

brf3rx@virginia.edu

&Jundong Li

University of Virginia

jundong@virginia.edu

&Jing Yang

The Pennsylvania State University

yangjing@psu.edu

&Cong Shen

University of Virginia

cong@virginia.edu

indicates equal contributions, random order.

###### Abstract

The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically finding good prompts, i.e., prompt optimization. Most existing works follow the scheme of selecting from a pre-generated pool of candidate prompts. However, these designs mainly focus on the generation strategy, while limited attention has been paid to the selection method. Especially, the cost incurred during the selection (e.g., accessing LLM and evaluating the responses) is rarely explicitly considered. To overcome this limitation, this work provides a principled framework, TRIPLE, to efficiently perform prompt selection under an explicit budget constraint. TRIPLE is built on a novel connection established between prompt optimization and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB); thus, it is capable of leveraging the rich toolbox from BAI-FB systematically and also incorporating unique characteristics of prompt optimization. Extensive experiments on multiple well-adopted tasks using various LLMs demonstrate the remarkable performance improvement of TRIPLE over baselines while satisfying the limited budget constraints. As an extension, variants of TRIPLE are proposed to efficiently select examples for few-shot prompts, also achieving superior empirical performance.

## 1 Introduction

Large language models (LLMs) have rapidly changed technology landscapes in our society . Researchers continuously find effective ways to unlock their potential on various downstream tasks. Among different research directions, the remarkable ability of LLMs to follow instructions has motivated the study of searching for suitable prompts to interact with them . This approach is particularly attractive as it does not require updating the inside parameters of an LLM, and is natural in the way of human conversations. Nevertheless, it has also been recognized that the performance of an LLM is sensitive to the selected prompts , and manually designing suitable prompts can be a labor-intensive process . Thus, there is a growing interest to perform automatic prompt optimization .

While these studies have proposed different prompt optimization designs, they commonly follow the approach of generating a pool of candidate prompts and then selecting from them. With a deeper look, it can be recognized that the focus in these existing works largely leans towards how to generate the candidate pool, while limited attention has been paid towards how to select from the candidates.

For example, many works  directly evaluate all the generated prompts on the entire development dataset. However, this less-emphasized selection process typically requires accesses to LLMs, which are often (1) _financially costly_ (e.g., each OpenAI API access incurs a cost); (2) _time-wise consuming_ (e.g., even a locally hosted LLM would typically require seconds to respond); (3) under _total usage limits_ (e.g., OpenAI has hard per-day and per-month limits on API accesses). Furthermore, it is often overlooked that evaluating the responses of an LLM for different candidate prompts can be costly as many tasks (e.g., writing improvement, mathematical reasoning, etc.) would require human (and sometimes domain expert) opinions. As a result, the prompt optimization process can incur an unaffordable cost without a proper selection method.

To make the learning process more accessible, this work proposes to study prompt optimization under an explicitly imposed budget constraint when interacting with the targeted LLM, in addition to the previously considered requirements (e.g., discrete, interpretable, and black-box). To the best of our knowledge, budget constraints are only briefly mentioned in Zhou et al. , Pryzant et al. , and there are no systematic or principled investigations of how to address the limited budget constraint in prompt optimization. The main contributions of this work are summarized as follows.

\(\) The constraint of a limited budget is explicitly introduced into prompt optimization, which has been largely ignored before. As most of the prompt optimization methods rely on selecting from a pre-generated candidate prompt pool, we focus our study on how to carefully allocate budgets to test each candidate prompt so that the optimal one can be learned efficiently and effectively.

\(\) We propose a general solution framework, termed TRIPLE (**b**es**T**a**R**m **I**dentification for **P**rompt **LE**arning), by establishing a novel connection between prompt optimization and multi-armed bandits (MAB) . In particular, we focus on harnessing the power of fixed-budget best arm identification (BAI-FB)  to address prompt optimization (especially, selection) with a limited budget constraint. Two representative designs TRIPLE-SH and TRIPLE-CR, inspired by celebrated BAI-FB algorithms, are presented. To improve scalability, two enhanced methods, TRIPLE-CLST and TRIPLE-GSE, are further proposed, where prompt embeddings are leveraged by exploiting the ideas of clustering and function approximation to accelerate the learning process.

\(\) Extensive experimental results are reported using well-adopted prompt tasks and varying LLMs to demonstrate the superiority of TRIPLE over previous baselines. In particular, on GPT3.5 and Llama2, compared with baseline methods also not using prompt embeddings, the basic TRIPLE-SH and TRIPLE-CR achieves performance improvements by (on average) \(3\%\) to \(16\%\). When leveraging prompt embeddings, the enhanced TRIPLE-CLST and TRIPLE-GSE also outperform corresponding baselines by (on average) \(10\%\) to \(56\%\) with fewer prompts than budget and (on average) \(16\%\) to \(45\%\) with more prompts than budget. The gains are further evidenced on other LLMs, i.e., Gemma and Mistral. Moreover, the proposed methods can be directly plugged into two popular prompt optimization pipelines, APE  and APO , with end-to-end performances significantly improved over their original implementations.

\(\) This work extends broadly to providing a new perspective of prompt optimization from MAB, and also a new application scenario of MAB in prompt optimization. This established connection may spark further innovations in both fields. As one concrete example, we extend the study to optimizing the selection of examples in few-shot prompts , which can be recognized as a BAI-FB problem in the setup of combinatorial bandits . Experimental results illustrate that the extensions of TRIPLE achieve superior performance, demonstrating its rich potential.

**Key Related Works.** We discuss a few works that explicitly or implicitly touch upon the selection efficiency in prompt optimization, and a complete literature review can be found in Appendix A. First, Zhou et al.  discusses a naive filtering strategy without theoretical or empirical justifications. Chen et al.  leverages Bayesian optimization (BO) with expected improvement (EI) as the acquisition function to select continuous soft prompts. BO can be viewed as similar to BAI while mostly focusing on infinite-arm cases . Moreover, Pryzant et al. , Lin et al.  use specific MAB methods targeting regret minimization to perform prompt selection, which, as further illustrated in Sec. 3.3, are not well-suited as they optimize the cumulative selection performance over a period instead of the final selection output. Thus, compared with this work, existing investigations either lack a comprehensive discussion of the connection between prompt optimization and MAB or choose unsuitable MAB techniques to tackle prompt optimization. Moreover, as illustrated in Sec. 5, the TRIPLE solution outperforms the previously adopted methods empirically.

Prompt Optimization under a Limited Budget

Following Zhou et al. , Chen et al. , we present a concrete formulation of the problem of prompt optimization. Consider that we are using an LLM \(f()\), which provides a mapping from any input \(X\) to a distribution \(_{}\) over the language space \(\). The answer \(\) given by the LLM is assumed to be sampled from \(f(X)\) as \( f(X)\). Note that instead of treating \(f()\) as a deterministic function providing a specific output answer, we generally consider the practical setting where the answers of LLM exhibit a certain level of randomness.

For prompt optimization, we aim to find a prompt \(p\) such that when concatenated with inputs \(X\) of a certain task (i.e, as \([p;X]\)), it provides good performance in expectation with respect to the input distribution \(_{X}\) and the inherent randomness of LLM \(f()\). The performance is measured as

\[(p):=_{X_{X}}_{ f([p;X])}[s(X, )],\]

where \(s(X,)\) denotes a score function that measures the quality of the output \(\) for the input \(X\).

Motivated by the common usage scenario of LLMs, recent studies have imposed several constraints on this learning problem , where the three key ones are **(I) black-box**: the method can be applied to black-box LLMs, i.e., only have access to an API \(f()\) and no access to the intermediate structure or parameters inside (including gradients, output likelihood, etc.); **(II) discrete**: the learned prompt must be discrete characters, instead of continuous values (i.e., soft prompts); and **(III) interpretable**: the learned prompt must be understandable by humans, instead of gibberish words.

Intuitively, the process of learning a good prompt requires interactions with the LLM (i.e., sample \( f([p;X])\) and evaluating its responses (i.e., obtain score \(s(X,)\)). However, as mentioned in Sec. 1, such interactions and evaluations are costly. Thus, besides the aforementioned constraints, we further explicitly take into account that the prompt optimization process should have **(IV) a limited budget**: the total number of trials with the LLM that happen during the learning is at most \(N\). Finally, the prompt optimization problem considered in this work can be formulated as:

\[$ with high performance $(p^{*})$ under constraints of}\\ \]

Directly tackling this prompt optimization problem has been widely recognized as challenging even without the constraint of a limited budget . As highlighted in Pryzant et al. , Chen et al. , it essentially requires performing a black-box discrete optimization. Instead, many proposed methods rely on the pipeline of first generating a pool of candidate prompts and then selecting from it . The prompt generation can either be performed manually or follow designed automatic protocols. For example, the famous APE design  selects from prompts generated by an LLM using demonstrations. From a unified perspective, we can simplify the problem into generating a pool of prompts \(\) and finding the optimal prompt in it:

\[p^{*}:=_{p}(p).\]

While many efforts have been devoted along this line, we recognize that they are largely focused on how to generate prompts, while limited attention has been paid to how to select from the already generated prompts (as mentioned in Sec. 1 and further discussed in Appendix A). Naive treatments, such as uniformly evaluating all prompts, are understandable since budget limitations are not considered previously, i.e., unlimited evaluations can be performed. With an explicit budget limitation, however, we need to carefully allocate the budgets to each prompt so that the optimal prompt (or at least a sufficiently good one) can be correctly learned, which is the main focus of this work. An overview of the considered prompt optimization pipeline and our focus is illustrated in Fig. 1.

Figure 1: The commonly adopted prompt optimization pipeline. Previous works mostly investigate the generation component and ignore costs during selection, where GrIPS and APE are proposed in Prasad et al. , Zhou et al. . This work, instead, focuses on the selection component under an explicit budget constraint.

Connecting Prompt Optimization with Best Arm Identification

We provide a new perspective of prompt optimization through the lens of tools in multi-armed bandits (MAB) . In particular, prompt optimization under a limited budget is shown to be intrinsically aligned with the problem of fixed-budget best-arm identification (BAI-FB) . In the following, a brief introduction to MAB is first provided. Then, the connection between prompt optimization (especially selection) and MAB (especially BAI-FB) is established. Based on this connection, we propose to fully leverage the rich toolbox from BAI-FB to perform efficient prompt optimization.

### Multi-armed Bandits

The research of multi-armed bandits (MAB) has a long and rich history; see representative surveys of Lattimore and Szepesvari , Bubeck et al. . The most basic form of MAB, i.e., the finite-armed stochastic bandits, considers a system with a set \(\) finite arms (i.e., actions) that provide stochastic rewards when pulled. When interacting with the system, the agent can select one arm \(k\) to pull at each time, and she receives a stochastic reward: \(r_{k}_{k}(_{k})\), where \(_{k}(_{k})\) denotes the action \(k\)'s reward distribution with an unknown expectation \(_{k}\).

The learning objective of the agent in MAB can be roughly divided into two categories: (1) _regret minimization,_ which maximizes the expected cumulative rewards collected by the agent ; (2) _best arm identification,_ which targets at outputting the best arm \(k^{*}=_{k}_{k}\). These two objectives often require different learning strategies. Regret minimization typically relies on a carefully designed balance between exploration (i.e., obtaining new information) and exploitation (i.e., collecting higher rewards based on the previous information). Best arm identification, on the other hand, is also called pure exploration as it only focuses on obtaining information to find the best arm. We here particularly note that although the designs targeting regret minimization often can converge to the optimal arm \(k^{*}\) given a sufficient period of time, they are known to be inefficient for the objective of best arm identification in the MAB studies.

### A Bandit View of Prompt Optimization

Based on the above introduction, it can be intuitively understood that the prompt optimization (especially, selection) problem can be mapped into an MAB setting:

* The pool of candidate prompts \(\) is equivalent to the set of arms \(\);
* Using a prompt \(p\) to interact with LLM can be viewed as selecting a bandit arm \(k\) to pull in MAB;
* The feedback of the score function, i.e., \(s(X,)\), provides the reward signal \(r_{k}\), where \(_{k}(_{k})\) characterizes the randomness of \(X_{X}\) and \( f([p;X])\). The expected performance \((p)\) is the counterpart of the expected reward \(_{k}\) in MAB.

It can be further recognized that the target of prompt optimization is more suitable to be captured as the _best arm identification_ (BAI) problem, instead of a regret minimization one, as it only cares about finding the optimal prompt \(p^{*}\) instead of the cumulative performance of interactions performed during the learning process.

With the relationship between prompt optimization and BAI established, we further consider the constraint of learning under a limited budget. We argue that this aligns with one of the main research directions in BAI called _fixed-budget best arm identification_ (BAI-FB) . BAI-FB particularly considers the problem of _maximizing the probability of correctly identifying the best arm \(k^{*}\) while not pulling arms more than \(T\) times._ It can be observed that this formulation matches the goal of prompt optimization under a limited budget; thus BAI-FB provides a perfect toolbox to enhance the commonly required prompt selection process. The connection between prompt optimization and MAB, in particular, BAI-FB, is further illustrated in Table 1. To avoid confusion, in the remainder of this paper, we will adopt the notation of prompt optimization as introduced in Sec. 2.

  
**Prompt Optimization** & **Multi-armed Bandits** \\  The pool of prompts \(\) & The arm set \(\) \\ Interact LLM via prompt \(p\) & Pull arm \(k\) \\ Score \(s(X,)\) & Reward \(r_{k}\) \\ Randomness in \(X\) and \(\) & Randomness in \(_{k}\) \\ Performance \((p)\) & Expected reward \(_{k}\) \\  Learn the optimal prompt & Fixed-budget best arm identification (BAI-FB) \\   

Table 1: Prompt Optimization and MAB.

### Harnessing the Power of BAI-FB

As mentioned, we recognize that prompt optimization under a limited budget is a matching application scenario for BAI-FB. In this paper, we propose a general framework called TRIPLE (bes**T** a**Rm**I**dentification for **P**rompt **LE**arning) to harness the power of BAI-FB in solving the prompt optimization problem. This is possible because BAI-FB has witnessed significant development over the years, with several efficient designs being proposed. As a first step, we choose two popular and successful BAI-FB schemes and implement them for prompt optimization, which are briefly described below. Their complete descriptions are provided in Algs. 2 and 3 of Appendix C.

**Sequential Halving (SH).** SH is one of the first provably efficient BAI-FB designs  and remains popular after a decade of its proposal. It follows a protocol that divides the total budget \(N\) into \(_{2}(||)\) equal-length phases. In each phase, SH uniformly tries all active prompts (initialized as \(\)) and eliminates half of them with the lower sample means for the next phase. The final active arm is output as the identified optimal prompt.

**Continuously Reject (CR).** CR is a recently proposed method , which can be viewed as an extension of the classical Successively Reject (SR) design . It uniformly explores active prompts (initialized as \(\)) and performs potential elimination of poorly-performed prompts after each pull. The elimination is based on carefully designed criteria using the Large Deviation Principle. It can be observed that, without the phased structure, CR is more adaptive than SH (and SR), which makes it appealing both theoretically and practically.

While MAB has found broad applications in recommender systems , healthcare , wireless communications , and beyond , a systematical connection between MAB and prompt optimization has not been established before to the best of our knowledge, which may spark new research activities (see discussions in Sec. 7). In addition, although SH and CR are selected as the representatives, the connection between prompt optimization and MAB is fundamental. Any existing or forthcoming BAI-FB designs can be flexibly incorporated into TRIPLE, e.g., the Bayesian perspective provided in Komiyama et al. , Atsidakou et al. 

_Remark 3.1_.: As mentioned in Sec. 1, Pryzant et al. , Lin et al.  leverage specific MAB designs to perform prompt selection without a comprehensive discussion as above on their connection. Moreover, Pryzant et al.  argues that UCB  is suitable, while Lin et al.  also uses a UCB-variant, NeuralUCB , as the core method. However, both of UCB and NeuralUCB are designed for regret minimization (i.e., optimizing the cumulative interaction performance during learning). As illustrated in Sec. 3.1, designs for regret minimization cannot achieve optimal performance for the goal of identifying the optimal arm (i.e., BAI), which thus are not well-suited for prompt optimization.

## 4 Handling Large Candidate Pools via Prompt Embeddings

The connection built in the last section provides us with the core idea of leveraging BAI-FB designs to tackle prompt optimization. As having been theoretically established , solving BAI-FB without additional structures, however, will unavoidably incur an identification error that is positively related to the number of candidate prompts \(||\). In other words, given a larger pool of prompts, it becomes harder to find the optimal prompt with the basic BAI-FB designs, which restricts their applicability to practical prompt optimization problems (where possibly the number of prompts exceeds the budget).

The key reason behind this is that each candidate prompt is treated _independently_ in the basic BAI-FB. Thus, budgets need to be assigned to all the prompts and no information can be shared among them, which is often not the case in prompt optimization. For a prompt optimization problem, the underlying task is often stable, e.g., rewriting emails, constructing TLDR, etc. The candidate prompts, regardless of their generation methods, should all reflect the purpose of the underlying task and thus share similarities. For example, the candidate prompts generated via demonstrating LLMs  often share similar structures and differ only in a few words or word orders.

With the above observation, we target sharing information among prompts during learning. To achieve this, we propose to leverage an embedding model, denoted as \(:^{d}\), to obtain the sentence embedding of the prompts: \(e(p):=(p)^{d},\;\;:=\{e(p):p\}\), where \(d\) refers to the embedding dimension. In the experiments, the OpenAI embedding API is adopted while, in general, any sufficiently expressive models can be incorporated. Also, due to this flexibility, using embedding models is fundamentally different from requiring a white-box LLM [13; 48]. With theobtained prompt embeddings, we propose two useful enhancements to further improve the learning effectiveness when the pool of candidate prompts is large.

### Leveraging Similarities via Clustering

Since the key challenge is a large pool of candidate prompts, an intuitive idea is to effectively decrease the size of the pool. We thus propose a two-phased BAI-FB scheme for prompt optimization. In Phase I, the entire pool of candidate prompts is clustered into several groups based on their embeddings, and BAI-FB is performed on the clusters with an initial target of finding the optimal cluster (or the few good clusters). Then, in Phase II, BAI-FB is performed on the prompts in the optimal cluster with the target of identifying one final prompt. For both phases, different BAI-FB designs can be incorporated, e.g., SH and CR. The entire procedure, referred to as TRIPLE-CLST, is described in Alg. 1.

The effectiveness of TRIPLE-CLST relies on the clustering results produced in Phase I. Ideally, prompts with similar performances should be clustered together. Then, Phase I can quickly eliminate the prompts with poor performances, leaving a small pool of good prompts for Phase II to process. In the experiments, this intuitive phenomenon is indeed observed. In particular, in Fig. 2, as expected, prompts in the same cluster share similar performances. In particular, it can be observed that the prompts in the same cluster (i.e., the same color and shape) share similar performance (i.e., similar sizes). Especially, the optimal prompt (marked by the red star) is clustered together with a few prompts with comparably near-optimal performances.

### Sharing Information via Function Approximation

Besides clustering, another idea to incorporate the prompt embeddings is to learn a common function (e.g., an MLP) to predict the prompt performances based on their embeddings. Similar ideas of function approximation have also been widely adopted in MAB literature to share information among large action spaces, with functions ranging from linear ones  to neural networks . In the setting considered in this work, we adopt a recently developed BAI-FB scheme as described in the following as TRIPLE-GSE, with details provided in Alg. 4 of Appendix C.

**GSE.** The general phased elimination flow of SH described in 3.3 is inherited. The major difference is that SH uses sample means to perform eliminations. GSE , on the other hand, leverages collected samples from previous phases to train a reward function \(g_{}():^{d}\) that maps prompt embeddings to the predicted performance, which is further used to eliminate prompts.

```
1:Input: the pool of candidate prompts \(\) and their embeddings \(\), overall budget \(N\), Phase I budget \(N_{1}\), number of clusters \(L\)
2: Cluster \(\) into clusters \(=\{^{1},,^{L}\}\) based on embeddings \(\) (e.g., via \(k\)-means)
3: Obtain \(}^{}\) BAI-FB\((,N_{1})\) {Phase I}
4: Obtain \(^{}\) BAI-FB\((}^{},N-N_{1})\) {Phase II}
5:Output: prompt \(^{}\)
```

**Algorithm 1**TRIPLE-CLST

## 5 Experiments

In this section, extensive experimental results are reported to evaluate the efficiency of TRIPLE across diverse prompting tasks from two standard datasets: Instruction-Induction  and BigBench . The results reported in this section are mainly collected from GPT-3.5, Llama2, Gemma, and Mistral (see the specific model numbers listed in Appendix E.1). Full experimental details can be found in Appendix E. The complete results of \(47\) tasks are reported in Appendix F, while here we particularly focus on \(12\) representative tasks, which are not too hard (i.e., all generated prompts achieve near-zero performances) or too easy (i.e., all generated prompts achieve near-one performances). The experimental codes can be found at [https://github.com/ShenGroup/TRIPLE](https://github.com/ShenGroup/TRIPLE).

Figure 2: Clusters for \(30\) prompts for “movie recommendation” (left)  and “rhymes” (right) . Prompts in the same cluster are labeled by the same color and shape. The performance of each prompt is represented by the size of its shape (the larger the better). The embeddings are projected using T-SNE .

### Evaluating \(\) with Fixed Prompt Pools

As \(\) main focuses on the prompt selection component, we perform initial evaluations in an isolated fashion of selecting from fixed pools of candidate prompts. For this experiment, candidate pools of prompts are generated following the well-established APE design  with a high LLM temperature to ensure randomness. Then, under a limited budget, the performances of \(\) algorithms are compared with the following four baselines, where the latter two (i.e., BO and NeuralUCB) leverage prompt embeddings:

* **Uniform.** Many previous designs choose to evaluate the entire candidate pool on all development data  which corresponds to uniformly dividing the total budget to test all prompts.
* **UCB.** The upper confidence bound (UCB) method is a famous design for regret minimization in MAB. We evaluate UCB using its vanilla version from Auer et al. , which is reported to have good performance in Przyant et al. .
* **BO.** Bayesian optimization (BO) with expected improvement (EI) acquisition function is adopted in Chen et al. , which assumes a Gaussian process prior specified by prompt embeddings to perform posterior updates and makes selection to maximize EI. To further examine the performance of BO, another acquisition function, i.e., probability of improvement (PI), is also adopted.
* **NeuralUCB.** Lin et al.  uses NeuralUCB  to perform prompt selection, which extends UCB by training a reward function to predict prompt performances based on embeddings.

**Performance with fewer prompts than budget.** We first test candidate pools with \(30\) prompts per task. Results reported in Fig. 3(a) reflect the selection performance with an overall budget of \(150\). It can be observed that \(\)-SH and \(\)-CR achieve better performance than Uniform (\(15\%\) and \(12\%\) improvements on average for GPT-3.5; \(15\%\) and \(16\%\) for Llama2) and UCB (\(5\%\) and \(3\%\) improvements on average for GPT-3.5; \(6\%\) and \(7\%\) for Llama2). Moreover, for methods using

Figure 3: Performance comparisons of various prompt selection methods on the selected tasks. The red dashed lines label the performances normalized over (i.e., \(1\) on the y-axis) and the red stars mark the best methods. The reported results are aggregated over 20 independent runs. The full results on 47 tasks are reported in Appendix F.

prompt embeddings, the enhanced TRIPLE-CLST and TRIPLE-GSE also demonstrate remarkable improvements over BO-EI (\(11\%\) and \(10\%\) on average for GPT-3.5; \(56\%\) and \(52\%\) for Llama2) and NeuralUCB (\(17\%\) and \(17\%\) improvements on average for GPT-3.5; \(26\%\) and \(27\%\) for Llama2). These results empirically evidence the superiority of TRIPLE with or without prompt embeddings.

Performance with more prompts than budget.In the above test, the budget is larger than the number of candidate prompts. We further perform experiments in a more difficult setting, i.e., there are more prompts than the budget. In particular, candidate pools with \(150\) prompts per task are generated, and the overall budget is set as \(100\). In this scenario, only the methods that can leverage embeddings (i.e., BO, NeuralUCB, TRIPLE-CLST, TRIPLE-GSE) can be used, as otherwise the total budget is not sufficient to provide even one evaluation to initiate the performance estimation of each candidate prompt. Results are reported in Fig. 3(b). In particular, it can be observed that TRIPLE-CLST and TRIPLE-GSE significantly improve over BO-EI (\(21\%\) and \(28\%\) on average for GPT-3.5; \(31\%\) and \(42\%\) for Llama2) and NeuralUCB (\(38\%\) and \(45\%\) on average for GPT-3.5; \(26\%\) and \(16\%\) for Llama2).

A summary of the averaged performance ranks of the baselines and TRIPLE is listed in Table 2, which contains results on four LLMs (i.e., GPT-3.5, Llama2, Mistral, Gemma). It can be observed that in varying setups and with different LLMs, the proposed TRIPLE methods consistently obtain better performances than the previous baselines, remarking its efficiency and broad applicability.

Impact of the total budget.For a more comprehensive understanding, using candidate pools with \(30\) prompts, we further examine the impact of budgets, starting with \(5\) evaluations per prompt on average (i.e., \(150\) overall as adopted in Fig. 3(a)), and then gradually increasing to \(30\) (i.e., \(900\) overall, which is the same as the experiments in Zhou et al. ). From the results shown in Fig. 4, we see that the improvements of TRIPLE over baselines are more pronounced with lower budgets. In particular, with a budget of \(10\) evaluations per prompt on average (i.e., \(300\) overall), TRIPLE-CR, TRIPLE-CLST and TRIPLE-GSE maintain notable \(9.7\%\), \(13.5\%\) and \(17.4\%\) improvement over Uniform, respectively; when the budget escalates to \(20\) evaluations per prompt on average, TRIPLE-CLST and TRIPLE-GSE still achieve an approximate \(8\%\) improvement. Once the budget reaches \(30\) evaluations per prompt on average (i.e., \(900\) overall), all methods provide approximately the same performance as they can all identify the optimal prompts under this generous budget.

Impact of the prompt pool size.Moreover, we investigate the prompt selection performance under prompt pools with different sizes. First, while Figs. 3(a), 3(b) and Table 2 has demonstrated the superiority of TRIPLE with \(30\) and \(150\) prompts, we further enlarge the size of prompt pool size to \(1000\) and consider an overall budget of \(500\). The results reported in Fig. 5 further illustrate that the improvement of TRIPLE over the baselines is consistent across the sizes of prompt pools. Also, to benefit empirical usage, we take a deep look into whether larger prompt pools are necessary to provide better candidates. From Fig. 6, it can be observed that actually the prompt performance distributions do not vary much with the pool size increased from \(100\) to \(1000\), indicating that generating a sufficiently large prompt pool (e.g., \(100\)) is enough to further perform the selection and find the final prompt candidate to use.

two end-to-end designs are considered, aiming to assess the performance of TRIPLE in more fluid and iterative settings, which are discussed in the following with our implementation details.

* **APE.** Proposed by Zhou et al. , the APE pipeline lets LLMs generate prompt candidates and then selects from them. In our experiments, for each task, following original templates, \(30\) prompts are generated, followed by different methods to perform selection with a budget of \(5\) evaluations per prompt on average (i.e., \(150\) LLM accesses overall). Zhou et al.  suggest a non-iterative version with uniform evaluations of prompts, which is taken as the baseline here.
* **APO.** The APO pipeline  is an iterative one, letting LLMs criticize the previous prompts. Here, following the original templates, three iterations are performed and \(10\) prompts are generated per iteration. Different selection methods are then tested with a budget of \(50\) per iteration so that an overall budget of \(150\) is used, aligning with that of APE. Pryzant et al.  have reported UCB as the most effective prompt selection method, which is adopted as the baseline here. We note that OPRO  shares a similar iterative scheme as APO while using a different component to improve prompts. Due to their similarity, the experiments are mainly focused on APO here, while TRIPLE can also be flexibly integrated with OPRO.

The end-to-end experiment results are reported in Table 3, which reveal the consistently better performance of TRIPLE over the originally adopted baseline methods. This observation highlights the applicability and flexibility of the TRIPLE framework, i.e., it can benefit any prompt optimization pipelines requiring a selection component.

## 6 Extension: Selections of Examples for Few-shot Prompts

Based on the general connection between prompt optimization and BAI-FB, the power of TRIPLE can be further extended beyond finding one good instructional prompt. In the following, we provide discussions on how to leverage TRIPLE to efficiently select examples for few-shot prompts.

As noticed in Brown et al. , LLMs can perform varying tasks when prompted with several related examples, i.e., few-shot prompting. It has been widely recognized that a good choice of examples in few-shot prompts is important to obtain good downstream performances . Using the terminology introduced in Sec. 1, we can formulate the problem of example selection as follows. From a set of examples \(\), we target at selecting \(M\) examples \((g_{1},,g_{M})\) to form a few-shot prompt, whose performance is measured as \((g_{1},,g_{M}):=_{X_{X}}_{  f([g_{1},,g_{M};X])}[s(X,)]\). The optimal selection of examples can be defined as \((g_{1}^{*},,g_{M}^{*}):=_{g_{1},,g_{M}}(g _{1},,g_{M})\).

From the MAB perspective, the learning target can be interpreted as finding the optimal combination of \(M\) arms from the overall arm set \(\), which is the focus of the study on combinatorial MAB (CMAB) . Then, TRIPLE can be further extended to incorporate BAI-FB designs from CMAB to perform the desired example selection. In particular, based on some heuristics on the performance \((g_{1},,g_{M})\), TRIPLE-SAR and TRIPLE-CSAR are proposed, extending Chen et al. , Gabillon et al. , which are further discussed in Appendix D. The performances of these extensions are presented in Table 4, with more details and results provided in Appendix E.6 and F.

## 7 Conclusions

Prompt optimization is an important problem for large language models (LLMs), but prior research has not considered the potential cost during prompt selection. We have explicitly incorporated a budget constraint to prompt optimization, and studied the problem of how to efficiently select prompts with the given budget. A systematical connection between multi-armed bandits (MAB) and prompt optimization was established. Through this lens, we proposed a general framework, termed TRIPLE, to fully harness the power of fixed-budget best arm identification (BAI-FB) to perform prompt optimization. Besides standard BAI-FB designs, two embedding-based enhancements were proposed to accelerate learning. Extensive experimental results demonstrated the superiority of TRIPLE over multiple representative tasks and various targeted LLMs. Furthermore, we showed that TRIPLE could be plugged into popular end-to-end prompt optimization pipelines, with better performance than previous implementations, demonstrating its effectiveness and flexibility.

In addition to the technical contributions, we believe that the connection between prompt optimization and MAB may be of broader interest. It not only provides a rich set of tools from MAB to advance prompt optimization but also introduces a new application scenario for MAB (especially BAI) research. In particular, the discussed extension to the selection of examples for few-shot prompts demonstrates the rich potential of TRIPLE. As future steps, the research on contextual bandits  may provide insights into selecting input-dependent prompts . Also, the application of prompt optimization may spark new research efforts in MAB, e.g., efficient BAI methods for correlated arms.

 
**Tasks** & **Random** & **Uniform** & **SAR** & **CSAR** & **Tasks** & **Random** & **Uniform** & **SAR** & **CSAR** \\  \#1 & 0.65\(\)0.07 & 0.63\(\)0.13 & 0.67\(\)0.07 & 0.66\(\)0.07 & \#7 & 0.98\(\)0.03 & 1.00\(\)0.00 & 1.00\(\)0.00 & 1.00\(\)0.00 \\  \#2 & 0.21\(\)0.06 & 0.26\(\)0.05 & 0.24\(\)0.07 & 0.27\(\)0.07 & \#8