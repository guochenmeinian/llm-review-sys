# Rating

[MISSING_PAGE_EMPTY:1]

Introduction

Since its release in November 2022, the language model _Chat Generative Pre-trained Transformer_ (ChatGPT) has rapidly become a widely known question-and-answer dialogue system. ChatGPT has been referenced in mainstream media across the globe  and across all major internet platforms . With similar reactions, the release of ChatGPT's successor, GPT-4, followed in March 2023 .

The performance of ChatGPT has been analyzed in a large number of exam-related use cases, with varying degrees of scientific rigor, ranging from detailed studies to anecdotal evidence. Use cases include passing the _United States Medical Licensing Examination_ (USMLE) , scoring highly on the _Psychology Today_ Verbal-Linguistic Intelligence IQ Test , and answering (and generating) Operations Management exam questions that were deemed to be within the scope of a typical MBA curriculum , all with a performance that elicited a positive sense of surprise from the authors. In turn, the performance of GPT-4 even surpasses that of ChatGPT on a large batch of academic and professional exams [7, Table 1]. Such strong task-related performance indicates that large language models (LLMs) could be frequently used as assistants in many domains.

In this paper, we introduce a new dataset called GHOSTS, which measures the advanced mathematical abilities of LLMs. Using this dataset, we perform a detailed analysis of the mathematical capabilities of ChatGPT on two of its versions, the 9-January-2023 and the 30-January-2023 version. Note that, according to the release notes, the 30-January-2023 version should possess "_improved factuality and mathematical capabilities_" . We further examine the performance of GPT-4 on a smaller dataset called miniGHOSTS, which exhibits statistics similar to the larger GHOSTS dataset. We further make available microGHOSTS, which itself is a subset of miniGHOSTS, and is designed to facilitate pre-screen of language models, by incurring minimal human evaluation costs. Our analysis includes but is not limited to testing how many of the skills necessary to do professional mathematics can be emulated by these models. Examples of such skills are the ability to answer computational questions, the ability to complete mathematical proofs that have gaps or missing steps, the ability to solve questions that are more focused on deep insights and original solutions, such as those of mathematical olympiads, and the ability to survey the literature and think across domains. None of the previous benchmarks (see Section 2) cover such a broad range of mathematical abilities.

To achieve the goals outlined above, GHOSTS consists of carefully composed prompts aimed at testing different aspects of LLMs related to mathematical comprehension; see Section 3. This includes both hand-crafted prompts as well as samples from existing datasets that were devised to test models specifically trained for mathematical comprehension .

For brevity, we use the expression "**(Chat)GPT**" to refer collectively to both the ChatGPT and GPT-4 language models. We refer to Appendix C for further details regarding different (Chat)GPT versions.

To evaluate the output of (Chat)GPT, we designed a thorough testing methodology, including warning and error codes that represent various possible failure modes of (Chat)GPT. We score (Chat)GPT's responses, report on the results using this methodology, and compare (Chat)GPT to a selection of state-of-the-art models trained for mathematical comprehension. In summary, the contributions of this article are threefold:

* **Benchmark for testing the mathematical capabilities of LLMs:** We introduce a new natural-language mathematics dataset, called GHOSTS2, to test the capabilities of LLMs across a range of aspects regarding advanced mathematical comprehension; see Section 3. It consists of two subdatasets derived from state-of-the-art datasets of mathematical queries for language models. Additionally, we devise four hand-crafted subdatasets covering further mathematical tasks. Parts of our dataset consist of problems that were selected to have a high probability of not being in the data on which (Chat)GPT was trained; see tags D1-D3 from Table 1. 
* **Insight for mathematical use of (Chat)GPT:** Based on our benchmark, we show for which types of questions and which domains of mathematics, (Chat)GPT may be useful and how it could be integrated into the workflow of a mathematician. On the other hand, we identify the failure modes, as well as the limits of its capabilities. This can aid future efforts to develop LLMs that perform better in mathematics. Our analysis is akin to a _mathematical model card_ in terms of intended use, metrics, evaluation data, and qualitative analyses, as mathematical strengths and weaknesses of (Chat)GPT are summarized; see Section 4.
* **Evaluation of improvements of (Chat)GPT:** We can further use our benchmark to track the mathematical capabilities of (Chat)GPT variants over time. As a first step, we analyze the impact of the upgrade from the 9-January-2023 to the 30-January-2023 version of ChatGPT, which promises a better mathematical performance according to the release notes. Then, we proceed to investigate what performance increases the successor GPT-4 brings; see Section 4.1.

## 2 Related Work

As a language model, (Chat)GPT can be universally employed to perform mathematical reasoning and, therefore, has to compete with technologies in this space that are sometimes decades old. Performing mathematical reasoning in an automated way has a long history and can be traced back to 1959 , the most focus being devoted to proving theorems . According to Harrison , there is a realization that classical approaches, using a symbolic encoding of mathematics, have reached a "plateau".

On the other hand, there is now a growing body of literature on learning mathematical relationships directly in a supervised-learning manner  or by using LLMs to perform mathematical reasoning directly on mathematics encoded in natural language . Sometimes, the distinction is blurred, because architectures of LLMs can also be used in a supervised-learning setting and have been employed successfully in learning mathematical relationships, such as between the syntactical form of a function and its integral .

Among the supervised approaches, we mention , where a Transformer architecture  was used to generate symbolic, closed-form solutions to integrals and first and second-order differential equations, which outperformed classical solvers3, such as Mathematica, MATLAB, and Maple by at least 14% on a test set of integration problems. On the task of solving differential equations, the Transformer-based approach still exceeds the classical approach, but by a smaller margin (at least 4% in the case of first-order differential equations and with more varied results for second-order equations).

Regarding LLMs, recent ones, for instance, PaLM  (released in 2022), are tested only on elementary-level mathematical reasoning datasets, such as the MathQA or GSM8K datasets . We hypothesize that this is due to a lack of advanced-level natural language mathematics datasets. Moreover, the results obtained indicate that the models at that time had difficulty with much simpler datasets than ours. For example, the version of PaLM with 540 billion parameters only correctly solves 58% of the problems of the GSM8K dataset, even with chain-of-thought prompting and access to an external calculator [24, Table 10]. This model nonetheless outperforms GPT-3 , which only achieves 54% on the same dataset. Variations of BERT  have been shown to only solve between 28% and 37% of the problems when fine-tuned and tested on the _Algebra Question Answering with Rationales_ (AQuA-RAT) dataset , which is the direct predecessor of MathQA. For some models, such as BLOOM  or the LaMDA model  (both released in 2022), an evaluation of the mathematical reasoning capability is entirely missing. An up-to-date survey on mathematical datasets and the performance of various LLMs can be found in .

Most similar to our dataset is the NaturalProofs dataset  and the NaturalProofsGen dataset . In this paragraph, we illustrate the similarities and differences between these datasets and ours. NaturalProofs and NaturalProofsGen are similar among themselves and cover graduate-level mathematics by focusing on data from ProofWiki4 (the latter dataset), as well as on the Stacks Project5 and two open-source textbooks (the former dataset). Using the LaTeX source code, which is available for all these resources, annotated theorems and their proof graphs are extracted. The annotations consist of reference graphs highlighting references to other theorems or definitions, the idea being that these references capture the "skeleton" of a proof. This task resembles the mathematical abilities that the _Named Theorem Proof Completion_ subdataset from the GHOSTS dataset evaluates (see Table 1), although (1) we only retrieve a single reference, and (2) (Chat)GPT, as far as known, does not use training objectives that make use of information from data annotation, in contrast to models evaluated in .

Our framework pertains to general language model evaluation, which may be presented in a black-box manner (as is the case for (Chat)GPT), and therefore does not allow to leverage any additional information, such as reference graphs. This is also reflected in the human evaluation schema introduced in [34, Table 24], which classifies common model mistakes. As reference graphs form the foundation of how the mathematical proofs are engineered, many elements of the evaluation schema are strongly tailored toward this representation of mathematical data. Our benchmark is not reference-centric and therefore allows evaluations of _any_ type of proof (including computations, as featured in the _Symbolic-Integration_ subdataset, which we consider to be a particular kind of proof). Therefore, our methodology includes further and more general failure modes to make for a more fine-grained evaluation that explains the nature of the errors. We refer to Appendix A for further related works.

## 3 The GHOSTS, miniGHOSTS, and microGHOSTS Dataset

We assess the mathematical reasoning capabilities of two ChatGPT versions, 9-January-2023 and 30-January-2023, and of GPT-4 by first creating a collection of 709 prompts from various sources, and subsequently evaluating the models on (subsets of) these data points. We rate the corresponding outputs provided by the models and collect statistics, such as error types, output lengths, or the stability of the answer under prompt engineering, see Sections 3.2 and 4 and Appendices B and D. This yields a total of 1636 ratings by human experts.

We divide our dataset, the entire collection of prompts, into six _subdatasets_, called

which, in turn, consists of multiple _files_, see Table 1. The boldface letters make up the **GHOSTS** acronym. Details on motivation, composition, collection process, and intended uses of the GHOSTS dataset are summarized in our datasheet in Appendix H, Sections H.1, H.2, H.3, and H.5, respectively.

GPT-4 was evaluated on a subset of 170 prompts, which we call the **miniGHOSTS** dataset. Specifically, after having created the GHOSTS dataset, we heuristically selected a subset of 10 prompts from each file of the subdatasets included in GHOSTS, having the same mean rating and the same standard deviation (of ChatGPT's output) as the original file; see also our datasheet in Appendix H for more information. In this sense, these subsets can be considered to have the most relevance by capturing the "essence" of the model performance in the respective file. The role of miniGHOSTS is thus to reduce the costly evaluation procedure of the full GHOSTS dataset on a new language model. The **microGHOSTS** dataset represents a further reduction of the miniGHOSTS dataset, where one question was extracted from each file making up the miniGHOSTS dataset, for a total of 14 questions. The role of microGHOSTS is to allow rapid pre-screening of a language model, where each of the 14 questions was chosen to be a question representative of the mathematical problems in that file, as well as being a problem that (Chat)GPT typically struggled with. Reference solutions, explanations, and known LLM failure modes are provided and discussed for the microGHOSTS dataset in order to aid raters who are not mathematically trained. For more information, see Appendix E.

### Subdatasets

The subdatasets that make up our GHOSTS dataset are summarized in Table 1. In the following, we describe each subdataset in more detail.

Grad-Text.This subdataset consists of a collection of books (R. Durrett's _Probability Theory_, J. R. Munkres _Topology_ and W. Rudin's _Functional Analysis_) that are widely used in universities to teach upper undergraduate or first-year graduate courses in a degree in mathematics. We have used most of the exercises from these books' first and second chapters, except for , where we only used exercises from the first chapter, which was longer than the other books' chapters.

Holes-in-Proofs.This subdataset consists of a number of proofs sourced from math.stackexchange.com, as well as some proofs sourced from books (S. Axler's _Linear Algebra Done Right_ and W. Rudin's _Principles of Mathematical Analysis_), and from the MATH dataset , where parts of the proofs were intentionally deleted and the LLM was prompted to fill in the gaps: This was done either by (1) using a MISSING token, (2) finishing the proof early and prompting the LLM to complete it, or (3) explicitly asking for certain conditions or results.

Olympiad-Problem-Solving.This subdataset consists of a selection of exercises from A. Engel's _Problem-Solving Strategies_ book, which is often used to prepare for mathematical competitions. We selected and graded the LLM outputs on one hundred exercises drawn from all chapters.

Symbolic-Integration.This subdataset consists of random samples of integrals from the test set of . There are three ways in which integrals are generated in : _Forward generation_ (FWD), _Backward generation_ (BWD), and _Backward generation with integration by parts_ (IBP). We sample 21 integrals from FWD test set, 20 integrals from the BWD test set, and 59 integrals from the IBP test set. As these integrals are given in Polish/prefix notation, a natural-language prompt conversion of them is unlikely to be witnessed in the training dataset of (Chat)GPT. The assessment was done by verifying the correctness of the output both by using Mathematica, as well as making use of the provided solutions

**Subdataset Name** & **Size** & **Compprised of the json file(s)** & **Tags** \\  _Grad-Text_ & 28 & W. Rudin, Functional Analysis (ch. 1) & M3 Q4 \\  & 15 & W. Rudin, Functional Analysis (ch. 2) & M3 Q4 \\  & 37 & J. Munkres, Topology (ch. 1) & M3 Q4 \\  & 29 & J. Munkres, Topology (ch. 2) & M3 Q4 \\  & 21 & R. Durrett, Probability Theory & M3 Q4 \\  _Holes-in-Proofs_ & 60 & Proofs Collection A & M3 Q1 Q2 Q5 \\  & 52 & Proofs Collection B Prealgebra & M1 Q5 \\  & 50 & Proofs Collection B Precalculus & M1 Q5 \\  _Olympiad-Problem-Solving_ & 101+24 & Olympiad Problem Solving & M4 Q4 D2 \\  _S Symbolic-Integration_ & 100 & Symbolic Integration & M2 Q3 D1 \\  _MATH_ & 50 & MATH Algebra & M1 M2 M3 Q3 Q4 \\  & 50 & MATH Counting and Probability & M1 M2 M3 Q3 Q4 \\  & 18 & MATH Prealgebra & M1 Q3 Q4 \\  & 20 & MATH Precalculus & M1 Q3 Q4 \\  _Search-Engine-Aspects_ & 30 & Definition Retrieval & M3 Q2 D3 \\  & 30 & Reverse Definition Retrieval & M3 Q1 Q2 D3 \\  & 18 & Named Theorem Proof Completion & M3 Q2 Q5 D3 \\  

Table 1: A summary of all the files from all the subdatasets comprising our GHOSTS dataset, together with their size, i.e., the number of prompts and their associated attribute tags. The tags M\(i\), Q\(i\), and D\(i\) relate to the level of Mathematical difficulty, the Question type, and the Out-of-Distribution type from Section 3.1, respectively. For the _Olympiad-Problem-Solving_ subdataset, 24 further prompts were created where prompt engineering was applied, see Appendix 4.2. These 24 prompts do not count towards the 709 total prompts, only towards the 1636 evaluations.

(in Polish notation), which  generated using SymPy. In particular, we notice that all integrals in this dataset have solutions that can be expressed using elementary functions.

MATH.This subdataset consists of a random sample of problems from the MATH dataset . The latter dataset attaches a level of difficulty to each problem. We focused on two domains, Algebra and Probability Theory, and sampled an equal number of problems at each level of difficulty.

Search-Engine-Aspects.This subdataset consists of problems that were not sampled from a particular source but generated by a human expert in the field. In the file _Named Theorem Proof Completion_, we focused on prompting the LLM to provide proof outlines of various theorems that are sufficiently well-known within Functional Analysis to have names. In the _Definition Retrieval_ file, we asked the LLM to correctly state various definitions centered around Functional Analysis and Topology. In contrast, in the _Reverse Definition Retrieval_ file, we verified whether the LLM was able to deduce the name of a mathematical object by describing its properties.

Because input to (Chat)GPT is purely textual (at the time of writing), certain types of questions that might be stated and solved in a non-text-based fashion (e.g., questions involving graphical diagrams, without text explaining the diagram6, as occasionally occur in ), have been excluded. Our subdatasets can be categorized along the following dimensions (see Appendix B.1 for more details):

* **Mathematical difficulty (ascending):** (M1) Elementary arithmetic problems, (M2) Symbolic problems, (M3) (Under)graduate-level exercises, (M4) Mathematical olymiad problems.
* **Question type:** (Q1) Stating mathematical facts, (Q2) Overview-type review questions, (Q3) Computational questions, (Q4) Theorem proofs or puzzle solutions, (Q5) Proof-completion questions.
* **Types of high out-of-distribution likelihood:** (D1) Nontrivial problem encoding, (D2) Succinct solution, (D3) Spoken dialogue.

The existing datasets of natural-language mathematics are far from covering all possible combinations across these dimensions. In our well-crafted GHOSTS datasets, we have striven to cover each of these aspects individually, as can be seen in Table 1. The next section specifies the format of our dataset and the methodology for analyzing (Chat)GPT's output.

### Format

The format of each of the subdatasets that make up our GHOSTS dataset follows the same convention. Each subdataset consists of JSON-formatted files, and our format is similar to, e.g., the AQuA-RAT dataset . A single data point7 in a file has the following form:

``` "prompt":"Let$XX$beatopologicalvectorspace.Allsetstmentionedbelowareunderstoodtobethesubsetof$X$.Proethefollowingstatement:If$AA$and$B$arecompact,sois$A+B$","output":"Thestatementiswrongingeneral.Considertheexample$A=[-1,1]{}times{}$and$B={}${}$times{}$.Then$AA$and$B$arecompactbut$A+B=[-1,1]{}times{}$isnotcompact."rating":"2","errorcodes":["e3","e5_2","e5_4"],"warningcodes":[],"comment":"Thegiven$A+B$actually*is*compact.","msc":["46A03"],"ref":"Rudin-FunctionalAnalysis-Second-Ed.Part1-ex3/d-page38","confidence": "high", "timestamp": "2023-01-31"

We require each data point to have the same JSON keys as in this example, some of which may be empty depending on the prompt. Among the listed keys, the rating key stands out as the most fundamental one. Its value serves as a condensed representation of the mathematical capability of the tested language model, compressed into a one-dimensional measure ranging from 1 (lowest) to 5 (highest). A more nuanced and fine-grained perspective on the mathematical capabilities is provided by the errorcodes and warningcodes keys. The msc key denotes the _mathematics subject classification_. We explain each JSON key in Appendix B.2. For end-users of (Chat)GPT, it is desirable to avoid having a long-winded dialogue to arrive at a solution. Therefore, we require that (Chat)GPT provides us with the correct solution given only the input prompt without any subsequent interaction.

### Human Input in Dataset Creation and Mathematical Evaluation

For all data points, the values of the keys rating, errorcodes, warningcodes, comment, and confidence were manually labeled, without any automation. The msc, ref, and timestamp keys were populated in a semi-automatic way, since their values change only slightly within the same subdataset.

Two of the subdatasets, the _MATH_ subdataset and the _Symbolic-Integration_ subdataset, use prompts taken from existing datasets, the MATH dataset by  and the dataset comprising integrals from , respectively. This was done to compare how (Chat)GPT performs against existing state-of-the-art models that use these datasets, see Section 4. Nonetheless, significant additional annotation effort was involved, since, in both cases, the authors rated the output. Furthermore, in the second case, the data are publicly presented in a Polish notation format, and conversion was necessary. The prompts of the other subdatasets were hand-crafted by the authors. We refer to Appendix B.6 for more information on different aspects of human effort in dataset creation.

## 4 Results

Will ChatGPT get you through a university math class? No, you would be better off copying from your average peer--unless it is undergraduate mathematics, for which GPT-4 can offer sufficient (but not perfect) performance.

If we take a rating of 3.5, the average between the lowest and highest rating, to be the threshold between success and failure, then Figure 1 shows that for a majority of subdatasets, both versions of ChatGPT will not pass. However, for GPT-4, the situation is different, and, on miniGHOSTS, it passes (sometimes barely) on all subdatasets files, except W. Rudin, Functional Analysis (ch. 2), which tests graduate-level mathematical knowledge and the Olympiad Problem Solving file, which tests mathematical problem-solving skills. We note that, unless otherwise stated, we do not use prompt-engineered questions in the results presented here (see Appendix 4.2).

We first focus on the results of the 9-January-2023 version of ChatGPT and note that the results for the 30-January-2023 are very similar, as can be inferred from the figures. On average, the 9-January-2023 version achieves a rating of 3.20 with a standard deviation8 of 1.23. It performs particularly poorly on proof-based questions in the style of graduate-level exercises or mathematical olympiads, as well as more complicated symbolic calculations. We note that prompt engineering only slightly improved the results for such complex questions; see Appendix 4.2. However, in tasks that only required filling in gaps or stating mathematical facts, ChatGPT was mostly able to achieve a score above 3.5. In particular, ChatGPT was strong at recognizing the context of the question, and the notation of the output almost always matched the one given in the prompt, see Figure 4 in the appendix. Generally, Figure 1 indicates that the ratings closely correspond to how mathematicians would rank the difficulty of the exercises. In this context, we note that the length of the prompt does not have a clear effect on the rating; see Figure 9 in the appendix. We present results for different mathematical fields in Figure 5 in the appendix. For a detailed qualitative analysis of the results on the different subdatasets, we refer to Appendix D.1. Finally, we note that (Chat)GPT almost never expressed any form of uncertainty, even if its output has been completely wrong; see Appendix D.2.

Comparing ChatGPT to the performance obtained by , who correctly solved nearly 100% of the integrals in a collection of 500 test equations [13, Table 3], the 9-January-2023 version of ChatGPT achieves an average rating of 2.51 (standard deviation: 0.87) on our random sample of their dataset (after conversion from Polish notation to LaTeX). Specifically, a rating of 2 is dominating 70% of the time, followed by a rating of 3 and 4 for 13% of the prompts each; see also Figure 7 in the appendix. GPT-4 achieves an average of 3.50 (standard deviation: 1.43), barely a passing grade, on the corresponding subset from miniGHOST. These scores trail far behind the performance achieved by the model in . The situation is similar when comparing ChatGPT to Minerva [21, Table 3]. Their best model achieved an accuracy of 50% on the MATH dataset . However, the 9-January-2023 version of ChatGPT achieves a perfect score only on 29% of our random samples from the MATH

Figure 1: Average rating for each file in each subdataset (bold) of GHOSTS on the 9-January-2023 and the 30-January-2023 versions of ChatGPT and for miniGHOSTS on GPT-4. Note that the maximal ranking is 5 and the minimal ranking, where the question was at least understood, is 2, see Appendix B.4; the lower rating of 1 indicates that the answer completely misses the question. Thus, a reasonable passing grade, i.e., 50% of points, corresponds to a score of 3.5, as indicated by the vertical dotted line. The error bars represent 95% confidence intervals.

[MISSING_PAGE_FAIL:9]

two JSON objects. These lists contain the original question that was not prompt-engineered, as well as the prompt-engineered question. The latter question is identified, as it contains the string <prompt engineered> as the value in the comment key. These lists containing prompt-engineered questions are in the same hierarchy in the JSON file as the other questions from the subdataset.

About 20% of the questions were prompt-engineered: In these cases, ChatGPT was instructed to proceed either step-by-step, by prefixing the sentence "Let's answer this question step by step." or by adding words that formulate the mathematical task in a more explicit way, i.e., by adding "Prove that..." or "Show that..." to the prompt5. Instructing ChatGPT to proceed step-by-step in this way was shown to increase the performance of GPT-3 on datasets that test mathematical reasoning (e.g., GSM8K); furthermore, this is a type of prompt engineering that is recommended by OpenAI in their _cookbook_ to improve reliability10.

As a result of prompt engineering, for the 9-January-2023 version of ChatGPT, the number of wrong statements and computations (i.e., error codes e2, e3, and e4) decreased, while the number of errors rooted in faulty logic (i.e., error code e5) actually increased. Overall, prompt engineering improves the average rating only slightly, see Figure 3 from the Appendix.

For the questions from _Olympiad-Problem-Solving_ that were selected for the miniGHOSTS dataset, we allow to sample from the entire _Olympiad-Problem-Solving_ subdataset, since the goal of miniGHOSTS is not to measure prompt-engineering effects. Therefore, some of the questions in the miniGHOSTS version of the _Olympiad-Problem-Solving_ subdataset contain prompt-engineered questions. The <prompt engineered> string was therefore removed from the comments in the miniGHOSTS dataset.

## 5 Conclusion

We have examined the behavior of (Chat)GPT across various tasks that test different aspects of mathematical skill. Contrary to the media sensation that (Chat)GPT has caused, (Chat)GPT is not yet ready to deliver high-quality proofs or calculations _consistently_. At the same time, the quality of the answers can be positively surprising. Moreover, our evaluation of GPT-4 on the miniGHOSTS dataset reveals promising improvements over ChatGPT's performance. In Appendix G, we collect the best and worst results for a number of selected subdatasets. The best responses can be seen to justify the media sensation. It thus seems fair to say that (Chat)GPT is _inconsistently bad_ at advanced mathematics: While its capabilities generally drop with the mathematical difficulty of a prompt, it does give insightful proofs in a few cases.

However, (Chat)GPT falls short of achieving the same performance as models specifically trained for single tasks. These models, in contrast, lack the flexibility of (Chat)GPT, which is a _universal_ tool suitable for any area of mathematics. In fact, (Chat)GPT's ability to search for mathematical objects, given information about them, is where it shines. For a user that is already sufficiently mathematically proficient to discern the correctness of (Chat)GPT's output, (Chat)GPT can be integrated as an assistant in the user's workflow. It can function as a search engine or knowledge base to speed up various lookup tasks, as they often occur at certain stages of mathematical research.

Due to the prohibitive annotation effort, the GHOSTS dataset is not yet large enough to significantly improve the mathematical capabilities of LLMs by fine-tuning them on GHOSTS; though we believe it is sufficiently comprehensive to allow an evaluation and comparison of LLMs (and more rapid evaluation using miniGHOSTS and microGHOSTS, respectively). We encourage other researchers to mine our dataset beyond the descriptive statistics that we computed to gain a deeper understanding of how LLMs behave on mathematical tasks. Finally, we hope that our work motivates other mathematicians to contribute to this growing field, by evaluating their LLMs on micro/mini/GHOSTS in order to establish a thorough benchmark for assessing the mathematical abilities of LLMs.