# Adaptive Quasi-Newton and Anderson Acceleration Framework with Explicit Global Convergence Rates

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Despite the impressive numerical performance of quasi-Newton and Anderson/nonlinear acceleration methods, their global convergence rates have remained elusive for over 50 years. This paper addresses this long-standing question by introducing a framework that derives novel and adaptive quasi-Newton or nonlinear/Anderson acceleration schemes. Under mild assumptions, the proposed iterative methods exhibit explicit, non-asymptotic convergence rates that blend those of gradient descent and Cubic Regularized Newton's method. Notably, these rates are achieved adaptively, as the method autonomously determines the optimal step size using a simple backtracking strategy. The proposed approach also includes an accelerated version that improves the convergence rate on convex functions. Numerical experiments demonstrate the efficiency of the proposed framework, even compared to a fine-tuned BFGS algorithm with line search.

## 1 Introduction

Consider the problem of finding the minimizer \(x^{}\) of the unconstrained minimization problem

\[f(x^{})=f^{}=_{x^{d}}f(x),\]

where \(d\) is the problem's dimension, and the function \(f\) has a Lipschitz continuous Hessian.

**Assumption 1**.: _The function \(f(x)\) has a Lipschitz continuous Hessian with a constant \(L\),_

\[\;\;y,\;z^{d},\|^{2}f(z)-^{2}f(y)\| L \|z-y\|.\] (1)

In this paper, \(\|.\|\) stands for the maximal singular value of a matrix and for the \(_{2}\) norm for a vector. Many twice-differentiable problems like logistic or least-squares regression satisfy Assumption 1.

The Lipschitz continuity of the Hessian is crucial when analyzing second-order algorithms, as it extends the concept of smoothness to the second order. The groundbreaking work by Nesterov et al.  has sparked a renewed interest in second-order methods, revealing the remarkable convergence rate improvement of Newton's method on problems satisfying Assumption 1 when augmented with cubic regularization. For instance, if the problem is also convex, accelerated gradient descent typically achieves \(O(})\), while accelerated second-order methods achieve \(O(})\). Recent advancements have further pushed the boundaries, achieving even faster convergence rates of up to \((})\) through the utilization of hybrid methods [42; 14] or direct acceleration of second-order methods [43; 27; 39].

Unfortunately, second-order methods may not always be feasible, particularly in high-dimensional problems common in machine learning. The limitation is that exact second-order methods require solving a linear system that involves the Hessian of the function \(f\). This main limitation motivated alternative approaches that balance the efficiency of second-order methods and the scalability of first-order methods, such as _inexact/subspace/stochastic techniques_, _nonlinear/Anderson acceleration_, and _quasi-Newton_ methods.

### Contributions

Despite the impressive numerical performance of quasi-Newton methods and nonlinear acceleration schemes, there is currently no knowledge about their global explicit convergence rates. In fact, global convergence cannot be guaranteed without using either exact or Wolfe-line search techniques. This raises the following long-standing question **that has remained unanswered for over 50 years**:

_What are the non-asymptotic global convergence rates of quasi-Newton and Anderson/nonlinear acceleration methods?_

This paper provides a partial answer by introducing generic updates (see algorithms 1 to 3) that can be viewed as cubic-regularized quasi-Newton methods or regularized nonlinear acceleration schemes.

Under mild assumptions, the iterative methods constructed within the proposed framework (see algorithms 3 and 6) exhibit _explicit, global and non-asymptotic_ convergence rates that interpolate the one of first order and second order methods (more details in appendix A):

* Convergence rate on non-convex problems (Theorem 4): \(_{i}\| f(x_{i})\| O(t^{-}+t^{-})\),
* Convergence rate on (star-)convex problems (Theorems 5 and 6): \(f(x_{t})-f^{} O(t^{-2}+t^{-1})\),
* Accelerated rate on convex problems (Theorem 8): \(f(x_{t})-f^{} O(t^{-3}+t^{-2})\).

### Related work

Inexact, subspace, and stochastic methods.Instead of explicitly computing the Hessian matrix and Newton's step, these methods compute an approximation using sampling , inexact Hessian computation [29; 19], or random subspaces [20; 31; 34]. By adopting a low-rank approximation for the Hessian, these approaches substantially reduce per-iteration costs without significantly compromising the convergence rate. The convergence speed in such cases often represents an interpolation between the rates observed in gradient descent methods and (cubic) Newton's method.

Nonlinear/Anderson acceleration.Nonlinear acceleration techniques, including Anderson acceleration , have a long standing history [3; 4; 28]. Driven by their promising empirical performance, they recently gained interest in their convergence analysis [61; 26; 60; 37; 66; 64; 69; 68; 53; 62; 63; 6; 63; 6; 67; 8; 54]. In essence, Anderson acceleration is an optimization technique that enhances convergence by extrapolating a sequence of iterates using a combination of previous gradients and corresponding iterates. Comprehensive reviews and analyses of these techniques can be found in notable sources such as [37; 7; 36; 35; 5; 17]. However, these methods do not generalize well outside quadratic minimization and their convergence rate can only be guaranteed asymptotically when using a line-search or regularization techniques [59; 65; 53].

Quasi-Newton methods.Quasi-Newton schemes are renowned for their exceptional efficiency in continuous optimization. These methods replace the exact Hessian matrix (or its inverse) in Newton's step with an approximation that is updated iteratively during the method's execution. The most widely used algorithms in this category include DFP [18; 25] and BFGS [58; 30; 24; 10; 9]. Most of the existing convergence results predominantly focus on the asymptotic super-linear rate of convergence [67; 32; 12; 11; 15; 22; 70; 71]. However, recent research on quasi-Newton updates has unveiled explicit and non-asymptotic rates of convergence [49; 51; 50; 40; 41]. Nonetheless, these analyses suffer from several significant drawbacks, such as assuming an infinite memory size and/or requiring access to the Hessian matrix. These limitations fundamentally undermine the essence of quasi-Newton methods, which are typically designed to be Hessian-free and maintain low per-iteration cost through their low-memory requirement and low-rank structure.

Recently, Kamzolov et al.  introduced an adaptive regularization technique combined with cubic regularization, with global, explicit (accelerated) convergence rates for any quasi-Newton method. The method incorporates a backtracking line search on the secant inexactness inequality that introduces a quadratic regularization. However, this algorithm relies on prior knowledge of the Lipschitz constant specified in Assumption 1. Unfortunately, the paper does not provide an adaptive method to find jointly the Lipschitz constant as well, as it is _a priory_ too costly to know which parameter to update. This aspect makes the method impractical in real-world scenarios.

Paper OrganizationSection 2 introduces the proposed novel generic updates and some essential theoretical results. Section 3 presents the convergence analysis of the iterative algorithm, which uses one of the proposed updates. Section 4 is dedicated to the accelerated version of the proposed framework. Section 5 presents examples of methods generated by the proposed framework.

## 2 Type-I and Type-II Step

This section first examines a remarkable property shared by quasi-Newton and Anderson acceleration: the sequence of iterates of these methods can be expressed as a combination of _directions_ formed by previous iterates and the current gradient. Building upon this observation, section 2.1 investigates how to obtain second-order information without directly computing the Hessian of the function \(f\) by _approximating_ the Hessian within the subspace formed by these directions. Subsequently, section 2.2 demonstrates how to utilize this approximation to establish an _upper bound_ for the function \(f\) and its gradient norm \(\| f(x)\|\). Minimizing these upper bounds, respectively, leads to a type-I and type-II method.

Motivation: what quasi-Newton and nonlinear acceleration schemes actually do?The BFGS update is a widely used quasi-Newton method for unconstrained optimization. It approximates the inverse Hessian matrix using updates based on previous gradients and iterates. The update reads

\[x_{t+1}=x_{t}-h_{t}H_{t} f(x_{t}),\ \ H_{t}=H_{t-1}(I-d_{t} ^{T}}{g_{t}^{T}d_{t}})+d_{t}(d_{t}^{T}^{T}d_{t}+g_{t}^{T }H_{t-1}d_{t}}{(g_{t}^{T}d_{t})^{2}}-^{T}H_{t-1}}{g_{t}^{T}d_{t}})\]

where \(H_{t}\) is the approximation of the inverse Hessian at iteration \(t\), \(h_{t}\) is the step size, \(d_{t}=x_{t}-x_{t-1}\) is the step direction, \(g_{t}= f(x_{t})- f(x_{t-1})\) is the gradient difference. After unfolding the equation, the BFGS update can be seen as a combination of the \(d_{i}\)'s and \( f(x_{t})\),

\[x_{t+1}-x_{t}=H_{0}P_{0} P_{t} f(x_{t})+_{i=1}^{t}_{i}d_ {i},\] (2)

where \(P_{i}\) are projection matrices in \(^{d d}\) and \(_{i}\) are coefficients. Similar reasoning can be applied to other quasi-Newton formulas (see appendix B for more details).

This observation aligns with the principles of Anderson acceleration methods. Considering the same vectors \(d_{t}\) and \(g_{t}\), Anderson acceleration updates \(x_{t+1}\) as:

\[^{}=_{}\| f(x_{t})+_{i=0}^{t-1}_{i}r_{i} \|, x_{t+1}-x_{t}=_{i=0}^{t}_{i}^{}(d_{i}-h_{t}g_{i} ),\]

where \(h_{t}\) is the relaxation parameter, which can be seen as the step size of the method. As all \(x_{i}\)'s belong to the span of previous gradients, the update is similar to (2), see appendix B for more details. This is not surprising, as it has been shown that Anderson acceleration can be viewed as a quasi-Newton method . Some studies have explored the relationship between these two classes of optimization techniques and established strong connections in terms of their algorithmic behavior .

Hence, quasi-Newton algorithms and nonlinear/Anderson acceleration methods utilize previous directions \(d_{i}\) and the current gradient \( f(x_{t})\) in subsequent iterations. However, their convergence is guaranteed only if a line search is used, and their convergence speed is heavily dependent on \(H_{0}\) (quasi-Newton) or \(h_{t}\) (Anderson acceleration) .

### Error Bounds on the Hessian-Vector Product Approximation by a Difference of Gradients

Consider the following \(d N\) matrices that represent the _algorithm's memory_,

\[Y=[y_{1},,y_{N}], Z=[z_{1},,z_{N}], D=Y-Z, G=[ , f(y_{i})- f(z_{i}),].\] (3)

For example, to mimic quasi-Newton techniques, the matrices \(Y\) and \(Z\) can be defined such that,

\[D=[,x_{t-i+1}-x_{t-i},], G=[, f(x_{t-i+1})-  f(x_{t-i}),],\ \ i=1 N.\]

Motivated by (2), this paper studies the following update, defined as a linear combination of the previous directions \(d_{i}\),

\[x_{+}-x=D^{N}.\] (4)

The objective is to determine the optimal coefficients \(\) based on the information contained in the matrices defined in (3). Notably, the absence of the gradient in the update (4) distinguishes this approach from (2), allowing for the development of an adaptive method that eliminates the need for an initial matrix \(H_{0}\) (quasi-Newton methods) or a mixing parameter \(h_{t}\) (Anderson acceleration).

Under assumption (1), the following bounds hold for all \(x,y,z,x_{+}^{d}\),

\[\| f(y)- f(z)-^{2}f(z)(y-z)\|\| y-z\|^{2},\] (5) \[f(x_{+})-f(x)- f(x)(x_{+}-x)-(x_{+}-x)^{ T}^{2}f(x)(x_{+}-x)\|x_{+}-x\|^{3}.\] (6)

The accuracy of the estimation of the matrix \(^{2}f(x)\), depends on the _error vector_\(\),

\[}}{{=}}[_{1},, _{N}],_{i}}}{{=}}\|d_{i}\|(\|d_{i}\|+2\|z_{i}-x\|).\] (7)

The following Theorem 1 explicitly bounds the error of approximating \(^{2}f(x)D\) by \(G\).

**Theorem 1**.: _Let the function \(f\) satisfy Assumption 1. Let \(x_{+}\) be defined as in (4) and the matrices \(D,\,G\) be defined as in (3) and vector \(\) as in (7). Then, for all \(w^{d}\) and \(^{N}\),_

\[-_{i=1}^{N}|_{i}|_{i}  w^{T}(^{2}f(x)D-G)_{i=1} ^{N}|_{i}|_{i},\] (8) \[\|w^{T}(^{2}f(x)D-G)\| \|\|.\] (9)

Proof sketch and interpretation.The theorem states that the Hessian-vector product \(^{2}f(x)(y-z)\) can be approximated by the difference of gradients \( f(y)- f(z)\), providing a cost-effective approach to estimate \(^{2}f\) without computing it. This property is the basis of quasi-Newton methods. The detailed proof can be found in appendix F. The main idea of the proof is as follows. From (5) with \(y=y_{i}\) and \(z=z_{i}\), writing \(d_{i}=y_{i}-z_{i}\), and Assumption 1,

\[\| f(y_{i})- f(z_{i})-^{2}f(x)(y_{i}-z_{i})\| \|d_{i}\|^{2}+\|^{2}f(x)-^{2}f(z)\|\|d_{i}\| _{i}.\]

The _first_ term in \(_{i}\) bounds the error of (5), while the _second_ comes from the distance between (5) and the current point \(x\) where the Hessian is estimated. Then, it suffices to combine the inequalities with coefficients \(\) to obtain Theorem 1.

### Type I and Type II Inequalities and Methods

In the literature, Type-I methods often refer to algorithms that aim to minimize the function value \(f(x)\), while type-II methods minimize the gradient norm \(\| f(x)\|\)[23; 73; 13]. Applying the bounds (6) and (5) to the update in (4) yields the following Type-I and Type-II upper bounds, respectively.

**Theorem 2**.: _Let the function \(f\) satisfy Assumption 1. Let \(x_{+}\) be defined as in (4), the matrices \(D,\,G\) be defined as in (3) and \(\) be defined as in (7). Then, for all \(^{N}\),_

\[f(x_{+}) f(x)+ f(x)^{T}D+H }{2}+}{6}, H}}{{= }}D+D^{T}G+\|D\|\|\|}{2}\] (10) \[\| f(x_{+})\|\| f(x)+G\|+ _{i=1}^{N}|_{i}|_{i}+\|D\|^{2},\] (11)

The proof can be found in appendix F. Minimizing eqs. (10) and (11) leads to algorithms 1 and 2, respectively, whose constant \(L\) is replaced by a parameter \(M\), found by backtracking line-search. A study of the (strong) link between these proposed algorithms and nonlinear/Anderson acceleration and quasi-Newton methods can be found in appendix B.

Solving the sub-problemsIn algorithms 1 and 2, the coefficients \(\) are computed by solving a minimization sub-problem in \(O(N^{3}+Nd)\) (see appendix C for more details). Usually, \(N\) is rather small (e.g. between \(5\) and \(100\)); hence solving the subproblem is negligible compared to computing a new gradient \( f(x)\). Here is the summary:

* **In algorithm 1**, the subproblem can be solved easily by a convex problem in two variables, which involves an eigenvalue decomposition of the matrix \(H^{N N}\).
* **In algorithm 2**, the subproblem can be cast into a linear-quadratic problem of \(O(N)\) variables and constraints that can be solved efficiently with SDP solvers (e.g., SDPT3).

```
0: First-order oracle for \(f\), matrices \(G,\,D\), vector \(\), iterate \(x\), initial smoothness \(M_{0}\).
1: Initialize \(M}{2}\)
2:do
3:\(M 2M\) and \(HD+D^{T}G}{2}+_{N}\)
4:\(^{}_{}f(x)+ f(x)^{T}D+ {2}^{T}H+}{6}\)
5:\(x_{+} x+D\)
6:while\(f(x_{+}) f(x)+ f(x)^{T}D^{}+[^{}]^{T}H ^{}+\|^{3}}{6}\)
7:return\(x_{+}\), \(M\) ```

**Algorithm 1** Type-I Subroutine with Backtracking Line-search

## 3 Iterative Type-I Method: Framework and Rates of Convergences

The rest of the paper analyzes the convergence rate of methods that use algorithm 1 as a subroutine; see algorithm 3. The analysis of methods that uses algorithm 2 is left for future work.

### Main Assumptions and Design Requirements

This section lists the important assumptions on the function \(f\). Some subsequent results require an upper bound on the radius of the sub-level set of \(f\) at \(f(x_{0})\).

**Assumption 2**.: _The radius of the sub-level set \(\{x:f(x) f(x_{0})\}\) is bounded by \(<\)._

To ensure the convergence toward \(f(x^{})\), some results require \(f\) to be star-convex or convex.

**Assumption 3**.: _The function \(f\) is star convex if, for all \(x^{d}\) and \(\),_

\[f((1-)x+ x^{})(1-)f(x)+ f(x^{}).\]

**Assumption 4**.: _The function \(f\) is convex if, for all \(y,\,z^{d}\), \(f(y) f(z)+ f(z)(y-z)\)._

The matrices \(Y,\,Z,\,D\) must meet some conditions listed below as "requirements" (see section 5 for details). All convergence results rely on _one_ of these conditions on the projector onto \((D)\),

\[P_{t}}}{{=}}D_{t}(D_{t}^{T}D_{t})^{-1}D_{t}^ {T}.\] (12)

**Requirement 1a**.: _For all \(t\), the projector \(P_{t}\) of the stochastic matrix \(D_{t}\) satisfies \([P_{t}]=\)._

**Requirement 1b**.: _For all \(t\), the projector \(P_{t}\) satisfies \(P_{t} f(x_{t})= f(x_{t})\)._

The first condition guarantees that, in expectation, the matrix \(D_{t}\) spans partially the gradient \( f(x_{t})\), since \([P_{t} f(x_{t})]= f(x_{t})\). The second condition simply requires the possibility to move towards the current gradient when taking the step \(x+D\). This condition resonates with the idea presented in (2), where the step \(x_{+}-x\) combines previous directions and the current gradient \( f(x_{t})\).

In addition, it is required that the norm of \(\|\|\) does not grow too quickly, hence the next assumption.

**Requirement 2**.: _For all \(t\), the relative error \(\|}{\|D_{t}\|}\) is bounded by \(\)._

The Requirement 2 is also non-restrictive, as it simply prevents taking secant equations at \(y_{i}-z_{i}\) and \(z_{i}-x_{i}\) too far apart. Most of the time, \(\) satisfies \( O(R)\).

Finally, the condition number of the matrix \(D\) also has to be bounded.

**Requirement 3**.: _For all \(t\), the matrix \(D_{t}\) is full-column rank, which implies that \(D_{t}^{T}D_{t}\) is invertible. In addition, its condition number \(_{D_{t}}}}{{=}}^{T}D_{t} \|\|(D_{t}^{T}D_{t})^{-1}\|}\) is bounded by \(\)._

The condition on the rank of \(D\) is not overly restrictive. In most practical scenarios, this condition is typically satisfied without issue. However, the second condition might be hard to meet, but section 5 studies strategies that prevent \(_{D}\) from exploding by taking orthogonal directions or pruning \(D\).

### Rates of Convergence

When \(f\) satisfies Assumption 1, algorithm 3 ensures a minimal function decrease at each step.

**Theorem 3**.: _Let \(f\) satisfy Assumption 1. Then, at each iteration \(t 0\), algorithm 3 achieves_

\[f(x_{t+1}) f(x_{t})-}{12}\|x_{t+1}-x_{t}\|^{3}, M_{t+1}< \{2L\ ;\ }{2^{}}\}.\] (13)

Under some mild assumptions, algorithm 3 converges to a critical point for non-convex functions.

**Theorem 4**.: _Let \(f\) satisfy Assumption 1, and assume that \(f\) is bounded below by \(f^{*}\). Let Requirements 1b to 3 hold, and \(M_{t} M_{}\). Then, algorithm 3 starting at \(x_{0}\) with \(M_{0}\) achieves_

\[_{i=1,\,,\,t}\| f(x_{i})\|\{} (12)-f^{}}{M_{}})^{2/3}\ ;\ (}{t^{1/3}})(12)-f^{ }}{M_{}})^{1/3}\},\]

_where \(C_{1}= L(}{2})+_{i[0,t]}\|(I- P_{i})^{2}f(x_{i})P_{i}\|\)._

Going further, algorithm 3 converges to an optimum when the function is star-convex.

**Theorem 5**.: _Assume \(f\) satisfy Assumptions 1 to 3. Let Requirements 1b to 3 hold. Then, algorithm 3 starting at \(x_{0}\) with \(M_{0}\) achieves, for \(t 1\),_

\[(f(x_{t})-f^{}) 6)-f^{}}{t(t+1)(t+2)}+}{2}+(3R)^{2}}{4},\]

_where \(C_{2}}{=} L^{}{2}}+_{i [0,t]}\|^{2}f(x_{i})-P_{i}^{2}f(x_{i})P_{i}\|\)._

Finally, the next theorem shows that when algorithm 3 uses a stochastic \(D\) that satisfies Requirement 1a, then \(f(x_{t})\) also converges in expectation to \(f(x^{})\) when \(f\) is convex.

**Theorem 6**.: _Assume \(f\) satisfy Assumptions 1, 2 and 4. Let Requirements 1a, 2 and 3 hold. Then, in expectation over the matrices \(D_{i}\), algorithm 3 starting at \(x_{0}\) with \(M_{0}\) achieves, for \(t 1\),_

\[_{D_{t}}[f(x_{t})-f^{}][t]^{3}}(f(x_{0})-f^{})+t]^{2}} }{2}+t]}(3R)^{2}}{2},\]

_where \(C_{3}}{=} L^{}{2}}+_{i[0,t]}\|^{2}f(x_{i})\|\)._

InterpretationThe rates presented in Theorems 4 to 6 combine the ones of cubic regularized Newton's method and gradient descent (or coordinate descent, as in Theorem 6) for functions with Lipschitz-continuous Hessian. As \(C_{1},C_{2}\), and \(C_{3}\) decrease, the rates approach those of cubic Newton.

The constants \(C_{1}\), \(C_{2}\), and \(C_{3}\) quantify the error of approximating \(D^{2}f(x)D\) by \(H\) in (10) into two terms. The first represents the error made by approximating \(^{2}f(x)D\) by \(G\), while the second describes the low-rank approximation of \(^{2}f(x)\) in the subspace spanned by the columns of \(D\). The approximation is more explicit in \(C_{3}\), where increasing \(N\) reduces the constant up to \(N=d\).

To retrieve the convergence rate of Newton's method with cubic regularization, the approximation needs to satisfy three properties: **1)** the points contained in \(Y_{t}\) and \(Z_{t}\) must be close to each other, and to \(x_{t}\) to reduce \(\) and \(\|\|\); **2)** the condition number of \(D\) should be close to 1 to reduce \(\); **3)**\(D\) should span a maximum dimension in \(^{d}\) to improve the approximation of \(^{2}f(x)\) by \(P^{2}f(x)P\).

For example, \(Z_{t}=x_{t}_{N}^{T}\), \(D_{t}=h_{N}\) with \(h\) small, and \(Y_{t}=Z_{t}+D_{t}\) achieve these conditions. This (naive) strategy estimates all directional second derivatives with a finite difference for all coordinates and is equivalent to performing a Newton's step in terms of complexity.

``` First-order oracle \(f\), matrices \(G,\,D\), vector \(\), iterate \(x\), smoothness \(M_{0}\), minimal norm \(\)  Initialize \(M}{2}\), \(+(1+_{D}^{2})\), \(\) whileExitFlag is False do  Update \(M\) and \(HD+D^{T}}{2}+_{N}\) \(^{*}_{}f(x)+ f(x)^{T}D+ ^{T}H+}{6}\) \(x_{+} x+D\) If\(- f(x_{+})^{T}D)\|^{3/2}}{}\) and \(\|D\|\)thenExitFlag\(\)LargeStep If\(-f(x_{+})^{T}D)\|^{2}}{M(+)}\)thenExitFlag\(\)SmallStep endwhile return\(x_{+}\), \(\), \(M\), \(\),ExitFlag ```

**Algorithm 4** Type-I subroutine with backtracking for the accelerated method

## 4 Accelerated Algorithm for Convex Functions

This section introduces algorithm 5, an accelerated variant of algorithm 3 for convex functions, designed using the estimate sequence technique from . It consists in iteratively building a function \(_{t}(x)\), a regularized lower bound on \(f\), that reads

\[_{t}(x)=^{ b_{i}}}(_{i= 0}^{t}b_{i}(f(x_{i})+ f(x_{i})(x-x_{i}))+_{t}^{(1)} \|^{2}}{2}+_{t}^{(2)}\|^{3}}{6}),\]

where \(_{t}^{(1,2)}\) are non-decreasing. The key aspects of acceleration are as follows (see section 4 for more details): **1)** The accelerated algorithm makes a step at a linear combination between \(v_{t}\), the optimum of \(_{t}\), and the previous iterate \(x_{t}\). **2)** It uses a modified version of algorithm 1, see algorithm 4. **3)** Under some conditions, the step size can be considered as "large", i.e., similar to a cubic-Newton step. The \(>0\) ensures the step is sufficiently large to ensure theoretical convergence - but setting \(=0\) does not seem to impact the numerical convergence. The presence of both small and large steps is crucial to obtain the theoretical rate of convergence.

``` First-order oracle \(f\), initial iterate and smoothness \(x_{0},\,M_{0}\), number of iterations \(T\).  Initialize \(G_{0},\,D_{0},\,_{0},\,_{0}^{(1)},\,_{0}^{(2)},\,\), \(x_{1},\,M_{1},(M_{0})_{1}\). for\(t=1,\,,\,T-1\)do  Update \(G_{t},\,D_{t},\,_{t}\).  do  Compute \(v_{t}_{t}\), set \(y_{t}=x_{t}+v_{t}\), and update \((M_{0})_{t}\) \(\{x_{t+1},\}(f,G_{t},D_{t}, _{t},y_{t},(M_{0})_{t},)\) if\(_{t+1}(v_{t+1}) f(x_{t+1})\)then%% Parameters adjustment if needed  ValidBound\(\)False ifExitFlag isSmallStepthen\(_{t}^{(1)} 2_{t}^{(1)}\), otherwise\(_{t}^{(2)} 2_{t}^{(2)}\) else  ValidBound\(\)True%% Successful iteration endif whileValidBound isFalse endfor return\(x_{T}\) ```

**Algorithm 5** Adaptive Accelerated Type-I Algorithm (Sketch, see appendix D for the full version)

**Theorem 7**.: _Assume \(f\) satisfy Assumptions 1, 2 and 4. Let Requirements 1b to 3 hold. Then, algorithm 5 starting at \(x_{0}\) with \(M_{0}\) achieves, for all \(>0\) and for \(t 1\),_

\[f(x_{t})-f^{})_{}^{2}}{L}()^{2 }+)_{}}{3}\{1\;;\;\} ()^{3}+^{(1)}R^{2}}{2}+ ^{(2)}R^{3}}{6}}{(t+1)^{3}}.\]

_where \(^{(1)}=0.5(L+M_{1}^{2})+\|  f(x_{0})-P_{0} f(x_{0})P_{0}\|,^{(2)}=M_{1}+L\),_

\[(M_{0})_{}=(2+(2^{2}+))+(2-1) _{0 i t}\|(I-P_{i})^{2}f(x_{i})P_{i}\|.\]

[MISSING_PAGE_FAIL:8]

Random Subspace:Inspired by , this technique randomly generates \(D_{t}\) at each iteration by either taking \(D_{t}\) to be \(N\) random (rescaled) canonical vectors or by using the \(Q\) matrix from the QR decomposition of a random \(N D\) matrix. This ensures that \(D_{t}\) satisfies Requirement 1a. For clarity, in the experiments, only the QR version is considered.

## 6 Numerical Experiments

This section compares the methods generated by this paper's framework to the fine-tuned \(\)-BFGS algorithm from minFunc. More experiments are conducted in appendix E. The tested methods are the Type-I iterative algorithms (algorithm 3 with the techniques from section 5). The step size of the forward estimation was set to \(h=10^{-9}\), and the condition number \(_{D_{t}}\) is maintained below \(=10^{9}\) with the iterates only and Greedy techniques. The accelerated algorithm 6 is used only with the _Forward Estimates Only_ technique. The compared methods are evaluated on a logistic regression problem with no regularization on the Madelon UCI dataset . The results are shown in fig. 1.

Regarding the number of iterations, the greedy orthogonalized version outperforms the others due to the orthogonality of directions (resulting in a condition number of one) and the meaningfulness of previous gradients/iterates. However, in terms of gradient oracle calls, the recommended method, _orthogonal forward iterates only_, achieves the best performance by striking a balance between the cost per iteration (only two gradients per iteration) and efficiency (small and orthogonal directions, reducing theoretical constants). Surprisingly, the accelerated method's performance is suboptimal, possibly because it tightens the theoretical analysis, diminishing its inherent adaptivity.

## 7 Conclusion, Limitation, and Future work

This paper introduces a generic framework for developing novel quasi-Newton and Anderson/Nonlinear acceleration schemes, offering a global convergence rate in various scenarios, including accelerated convergence on convex functions, with minimal assumptions and design requirements.

One limitation of the current approach is requiring an additional gradient step for the _forward estimate_, as discussed in Section 5. However, this forward estimate is crucial in enabling the algorithm's adaptivity, eliminating the need to initialize a matrix \(H_{0}\) (quasi-Newton) or employ a mixing parameter \(h_{0}\) (Anderson acceleration).

In future research, although unsuitable for large-scale problems, the method presented in this paper can achieve super-linear convergence rates, as with infinite memory, they would be as fast as cubic Newton methods. Utilizing the average-case analysis framework from existing literature, such as , could also improve the constants in Theorems 4 and 5 to match those in Theorem 6. Furthermore, exploring convergence rates for type-2 methods, which are believed to be effective for variational inequalities, is a worthwhile direction.

Ultimately, the results presented in this paper open new avenues for researchs. It may also provide a potential foundation for investigating additional properties of existing quasi-Newton methods and may even lead to the discovery of convergence rates for an adaptive, cubic-regularized BFGS variant.

Figure 1: Comparison between the type-1 methods proposed in this paper and the optimized implementation of \(\)-BFGS from minFunc with default parameters, except for the memory size. All methods use a memory size of \(N=25\).