# Efficient Online Clustering with Moving Costs

Dimitris Christou

UT Austin

christou@cs.utexas.edu

&Stratis Skoulakis

LIONS, EPFL

efstratios.skoulakis@epfl.ch

&Volkan Cevher

LIONS, EPFL

volkan.cevher@epfl.ch

First author contribution.

###### Abstract

In this work we consider an online learning problem, called _Online \(k\)-Clustering with Moving Costs_, at which a _learner_ maintains a set of \(k\) facilities over \(T\) rounds so as to minimize the connection cost of an adversarially selected sequence of clients. The _learner_ is informed on the positions of the clients at each round \(t\) only after its facility-selection and can use this information to update its decision in the next round. However, updating the facility positions comes with an additional moving cost based on the moving distance of the facilities. We present the first \(( n)\)-regret polynomial-time online learning algorithm guaranteeing that the overall cost (connection \(+\) moving) is at most \(( n)\) times the time-averaged connection cost of the _best fixed solution_. Our work improves on the recent result of Fotakis et al.  establishing \((k)\)-regret guarantees _only_ on the connection cost.

## 1 Introduction

Due to their various applications in diverse fields (e.g. machine learning, operational research, data science etc.), _clustering problems_ have been extensively studied. In the well-studied \(k\)-\(\) problem, given a set of clients, \(k\) facilities should be placed on a metric with the objective to minimize the sum of the distance of each client from its closest center .

In many modern applications (e.g., epidemiology, social media, conference, etc.) the positions of the clients are not _static_ but rather _evolve over time_. For example the geographic distribution of the clients of an online store or the distribution of Covid-19 cases may drastically change from year to year or respectively from day to day . In such settings it is desirable to update/change the positions of the facilities (e.g., compositions of warehouses or Covid test-units) so as to better serve the time-evolving trajectory of the clients.

The clients' positions may change in complex and unpredictable ways and thus an _a priori knowledge_ on their trajectory is not always available. Motivated by this, a recent line of research studies clustering problems under the _online learning framework_ by assuming that the sequence of clients' positions is _unknown_ and _adversarially selected_. More precisely, a _learner_ must place \(k\) facilities at each round \(t 1\) without knowing the positions of clients at round \(t\) which are revealed to the learner only after its facility-selection. The learner can use this information to update its decision in the next round; however, moving a facility comes with an additional moving cost that should be taken into account in the learner's updating decision, e.g. moving Covid-19 test-units comes with a cost .

Building on this line of works, we consider the following online learning problem:

**Problem 1** (_Online \(k\)-Clustering with Moving Costs_).: _Let \(G(V,E,w)\) be a weighted graph with \(|V|=n\) vertices and \(k\) facilities. At each round \(t=1,,T\):_

1. _The learner selects_ \(F_{t} V\)_, with_ \(|F_{t}|=k\)_, at which facilities are placed._2. _The adversary selects the clients' positions,_ \(R_{t} V\)_._
3. _The learner learns the clients' positions_ \(R_{t}\) _and suffers_ \[=_{j R_{t}}\ _{i F_{t}\\ j}}d_{G}(j,i)\] \[+(F_{t-1},F_{t})}_{}\]

_where \(d_{G}(j,i)\) is the distance between vertices \(i,j V\); \(M_{G}(F_{t-1},F_{t})\) is the minimum overall distance required to move \(k\) facilities from \(F_{t-1}\) to \(F_{t}\); and \( 0\) is the facility-weight._

An _online learning algorithm_ for Problem 1 tries to minimize the overall (connection \(+\) moving) cost by placing \(k\) facilities at each round \(t 1\) based only on the previous positions of clients \(R_{1},,R_{t-1}\). To the best of our knowledge, Problem 1 was first introduced in 2. If for any sequence of clients, the overall cost of the algorithm is at most \(\) times the overall connection cost of the _optimal fixed placement of facilities \(F^{*}\)_ then the algorithm is called \(\)-regret, while in the special case of \(=1\) the algorithm is additionally called _no-regret_.

Problem 1 comes as a special case of the well-studied _Metrical Task System_ by considering each of the possible \(\) facility placements as a different state. In their seminal work,  guarantee that the famous _Multiplicative Weights Update algorithm_ (\(\)) achieves \((1+)\)-regret in Problem 1 for any \(>0\). Unfortunately, running the \(\) algorithm for Problem 1 is not really an option since it requires \((n^{k})\) time and space complexity. As a result, the following question naturally arises:

**Q.**_Can we achieve \(\)-regret for Problem 1 with polynomial-time online learning algorithms?_

Answering the above question is a challenging task. Even in the very simple scenario of time-invariant clients, i.e. \(R_{t}=R\) for all \(t 1\), an \(\)-regret online learning algorithm must essentially compute an \(\)-_approximate solution_ of the \(k\)-\(\) problem. Unfortunately the \(k\)-\(\) problem cannot be approximated with ratio \(<1+2/e 1.71\) (unless \([n^{ n}]\)) which excludes the existence of an \((1+2/e)\)-regret polynomial-time online learning algorithm for Problem 1. Despite the fact that many \((1)\)-approximation algorithms have been proposed for the \(k\)-median problem (the best current ratio is \(1+\)), these algorithms crucially rely on the (offline) knowledge of the whole sequence of clients and most importantly are not designed to handle the moving cost of the facilities .

In their recent work, Fotakis et al.  propose an \((k)\)-regret polynomial-time online learning algorithm for Problem 1 _without_ moving costs (i.e. the special case of \(=0\)). Their approach is based on designing a _no-regret_ polynomial-time algorithm for a _fractional relaxation_ of Problem 1 and then using an _online client-oblivious_ rounding scheme in order to convert a fractional solution to an integral one. Their analysis is based on the fact that the connection cost of _any possible client_ is at most \((k)\) times its fractional connection cost. However in order to establish the latter guarantee their rounding scheme performs abrupt changes on the facilities leading to huge moving cost.

Our Contribution and Techniques.In this work, we provide a positive answer to question (**Q**), by designing the first polynomial-time online learning algorithm for Online \(k\)-Clustering with Moving Costs that achieves \(( n)\)-regret for any \( 0\). The cornerstone idea of our work was to realize that \((1)\)-regret can be established with a polynomial-time online learning algorithm in the special case of \(G\) being a Hierarchical Separation Tree (HST). Then, by using the standard metric embedding result of , we can easily convert such an algorithm to an \(( n)\)-regret algorithm for general graphs. Our approach for HSTs consists of two main technical steps:

1. We introduce a fractional relaxation of Problem 1 for HSTs. We then consider a specific regularizer on the fractional facility placements, called _Dilated Entropic Regularizer_, that takes into account the specific structure of the HST. Our first technical contribution is to establish that the famous _Follow the Leader algorithm_ with dilated entropic regularization admits \((1)\)-regret for any \( 0\).

2. Our second technical contribution is the design of a novel _online client-oblivious_ rounding scheme, called \(\&\), that converts a fractional solution for HSTs into an integral one. By exploiting the specific HST structure we establish that \(\&\), despite not knowing the clients' positions \(R_{t}\), simultaneously guarantees that \((i)\) the connection cost of each client \(j R_{t}\) is upper bounded by its fractional connection cost, and \((ii)\) the expected moving cost of the facilities is at most \((1)\) times the fractional moving cost.

Experimental Evaluation.In Section F of the Appendix we experimentally compare our algorithm with the algorithm of Fotakis et al. . Our experiments verify that our algorithm is robust to increases of the facility weight \(\) while the algorithm of  presents a significant cost increase. We additionally experimentally evaluate our algorithm in the \(\) and \(10\) datasets. Our experimental evaluations suggest that the \(( n)\)-regret bound is a pessimistic upper bound and that in practise our algorithm performs significantly better. Finally, we evaluate our algorithm both in the random arrival case (where the requested vertices are drawn uniformly at random from the graph) as well as in adversarial settings, where the request sequences are constructed through some arbitrary deterministic process.

Related Work.As already mentioned, our work most closely relates with the work of Fotakis et al.  that provides an \((k)\)-regret algorithm running in polynomial-time for \(=0\).  also consider Problem 1 for \(=0\) with the difference that the connection cost of clients is captured through the \(k\)-\(\) objective i.e. the sum of the squared distances. They provide an \((1+)\)-regret algorithm with \(((k^{2}/^{2})^{2k})\) time-complexity that is still exponential in \(k\). [18; 28] study the special case of Problem 1 in which \(G\) is the line graph and \(=1\) while assuming \(1\)_-lookahead_ on the request \(R_{t}\). For \(k=1\),  provide an \((1+)\)-competitive online algorithm meaning that its cost is at most \((1+)\) times the cost of the _optimal dynamic solution_ and directly implies \((1+)\)-regret.  extended the previous result by providing a \(63\)-competitive algorithm for \(k=2\) on line graphs. Our work also relates with the works of  and  that study offline approximation algorithms for clustering problems with _time-evolving metrics_. Finally our work is closely related with the research line of online learning in combinatorial domains and other settings of online clustering. Due to space limitations, we resume this discussion in Section A of the Appendix.

## 2 Preliminaries and Our Results

Let \(G(V,E,w)\) be a weighted undirected graph where \(V\) denotes the set of vertices and \(E\) the set of edges among them. The weight \(w_{e}\) of an edge \(e=(i,j) E\) denotes the cost of traversing \(e\). Without loss, we assume that \(w_{e}\) and \(w_{e} 1\) for all edges \(e E\). The _distance_ between vertices \(i,j V\) is denoted with \(d_{G}(i,j)\) and equals the cost of the minimum cost path from \(i V\) to \(j V\). We use \(n:=|V|\) to denote the cardinality of \(G\) and \(D_{G}:=_{i,j V}d_{G}(i,j)\) to denote its diameter.

Given a placement of facilities \(F V\), with \(|F|=k\), a client placed at vertex \(j V\) connects to the _closest open facility_\(i F\). This is formally captured in Definition 1.

**Definition 1**.: _The connection cost of a set of clients \(R V\) under the facility-placement \(F V\) with \(|F|=k\) equals_

\[C_{R}(F):=_{j R}_{i F}d_{G}(j,i)\]

Next, consider any pair of facility-placements \(F,F^{} V\) such that \(|F|=|F^{}|=k\). The moving distance between \(F\) and \(F^{}\) is the minimum overall distance needed to transfer the \(k\) facilities from \(F\) to \(F^{}\), formally defined in Definition 2.

**Definition 2**.: _Fix any facility-placements \(F,F^{} V\) where \(|F|=|F^{}|=k\). Let \(\) be the set of all possible matchings from \(F\) to \(F^{}\), i.e. each \(\) is a one-to-one mapping \(:F F^{}\) with \((i) F^{}\) denoting the mapping of facility \(i F\). The moving cost between \(F\) and \(F^{}\) equals_

\[M_{G}(F,F^{}):=_{}_{i F}d_{G}(i,(i))\]

At each round \(t 1\), an online learning algorithm \(\) for Problem 1 takes as input all the _previous_ positions of the clients \(R_{1},,R_{t-1} V\) and outputs a facility-placement \(F_{t}:=(R_{1},,R_{t-1})\)such that \(F_{t} V\) and \(|F_{t}|=k\). The performance of an online learning algorithm is measured by the notion of _regret_, which we formally introduce in Definition 3.

**Definition 3**.: _An online learning algorithm \(\) for Problem 1 is called \(\)-regret with additive regret \(\) if and only if for any sequence of clients \(R_{1},,R_{T} V\),_

\[[_{t=1}^{T}C_{R_{t}}(F_{t})+_{t=2}^{T}M_{G} (F_{t-1},F_{t})]_{|F^{*}|=k}_{t=1}^{T}C_{R_{t}}(F ^{*})+\]

_where \(F_{t}=(R_{1},,R_{t-1})\) and \(,\) are constants independent of \(T\)._

An online learning algorithm \(\) selects the positions of the \(k\) facilities at each round \(t 1\) solely based on the positions of the clients in the previous rounds, \(R_{1},,R_{t-1}\). If \(\) is \(\)-regret then Definition 3 implies that its time-averaged overall cost (connection \(+\) moving cost) is at most \(\) times the time-averaged cost of the _optimal static solution!_3 Furthermore, the dependency on \(\) is known to be optimal  and \(\) is typically only required to be polynomially bounded by the size of the input, as for \(T\) the corresponding term in the time-averaged cost vanishes.

As already mentioned, the seminal work of  implies the existence of an \((1+)\)-regret algorithm for Problem 1; however, this algorithm requires \((n^{k})\) time and space complexity. Prior to this work, the only polynomial-time4 online learning algorithm for Problem 1 was due to Fotakis et al. , for the special case of \(=0\). Specifically, in their work the authors design an online learning algorithm with the following guarantee:

**Theorem** (Fotakis et al. ).: _There exists a randomized online learning algorithm for Problem 1 that runs in polynomial time (w.r.t. \(T\), \(n\) and \( D_{G}\)) such that_

\[[_{t=1}^{T}C_{R_{t}}(F_{t})](k) _{|F^{*}|=k}_{t=1}^{T}C_{R_{t}}(F^{*})+(k n  D_{G})\]

Clearly, the algorithm of  has not been designed to account for charging the moving of facilities, as indicated by the absence of the moving cost in the above regret guarantee. The main contribution of this work is to obtain (for the first time) regret guarantees that also account for the moving cost.

**Theorem 1**.: _There exists a randomized online learning algorithm for Problem 1 (Algorithm 2) that runs in polynomial time (w.r.t. \(T\), \(n\) and \( D_{G}\)) and admits the following regret guarantee:_

\[[_{t=1}^{T}C_{R_{t}}(F_{t})+_{t=2}^{T}M_{G} (F_{t-1},F_{t})]( n)_{|F^{*}|=k}_{t=1}^ {T}C_{R_{t}}(F^{*})+\]

_for \(=(k n^{3/2} D_{G}(,1))\) and any \( 0\)._

**Remark 1**.: _We remark that while our additive regret \(\) is larger than the corresponding term in  by a factor of \(o()\), our results apply to any \( 0\) while the algorithm of  can generally suffer unbounded moving cost for \(\), as our experimental results verify._

### HSTs and Metric Embeddings

In this section we provide some preliminary introduction to Hierarchical Separation Trees (HSTs), as they consist a key technical tool towards proving Theorem 1. A _weighted tree_\((V,E,w)\) is a weighted graph with no cycles. Equivalently, for any pair of vertices \(i,j V\) there exists a unique path that connects them. In Definition 4, we establish some basic notation for tree graphs.

**Definition 4**.: _Fix any tree \((V,E,w)\). For every vertex \(u V\), \((u) V\) denotes the set children vertices of \(u\) and \(p(u)\) denotes its unique parent, i.e. \(u(p(u))\). The root \(r V\) of \(\) is the unique node with \(p(r)=\) and the set \(L():=\{u V\,:\,(u)=\}\) denotes the leaves of \(\). We use \((u)\) to denote the depth of a vertex \(u V\), i.e. the length of the (unique) path from the root \(r\) to \(u\), and \(h():=_{u L()}(u)\) to denote the height of \(\). We use \((u):=h()-(u)\) to denote the level of a vertex \(u V\). Finally, \(T(u) V\) denotes the set of vertices on the sub-tree rooted at \(u\), i.e. the set of vertices that are descendants of \(u\)._Next, we proceed to define a family of well-structured tree graphs that constitute one of the primary technical tools used in our analysis.

**Definition 5**.: _A Hierarchical Separation Tree (HST) is a weighted tree \((V,E,w)\) such that (i) for any node \(u\) and any of its children \(v cld(u)\), the edge \(e=(u,v)\) admits weight \(w_{e}=2^{(v)}\), and (ii) the tree is balanced, namely \(lev(u)=0\) for all leaves \(u L()\)._

In their seminal works,  and later  showed that HSTs can approximately preserve the distances of any graph \(G(V,E,w)\) within some logarithmic level of distortion.

**Theorem 2**.: _For any graph \(G(V,E,w)\) with \(|V|=n\) and diameter \(D\), there exists a polynomial-time randomized algorithm that given as input \(G\) produces an HST \(\) with height \(h() D\) s.t._

1. \(L()=V\)_, meaning that the leaves of_ \(\) _correspond to the vertices of_ \(G\)_._
2. _For any_ \(u,v V\)_,_ \(d_{G}(u,v) d_{}(u,v)\) _and_ \([d_{}(u,v)]( n) d_{G}(u,v)\)_._

Theorem 2 states that any weighted graph \(G(V,E,w)\) can be embedded into an HST \(\) with \(( n)\)-distortion. This means that the distance \(d_{G}(u,v)\) between any pair of vertices \(u,v V\) can be approximated by their respective distance \(d_{}(u,v)\) in \(\) within an (expected) factor of \(( n)\).

**Remark 2**.: _We note that traditionally HSTs are neither balanced nor are required to have weights that are specifically powers of \(2\). However, we can transform any general HST into our specific definition, and this has been accounted for in the statement of the above theorem. The details are deferred to Section B of the Appendix._

## 3 Overview of our approach

In this section we present the key steps of our approach towards designing the \(( n)\)-regret online learning algorithm for Problem 1. Our approach can be summarized in the following three pillars:

1. In Section 3.1 we introduce a _fractional relaxation_ of Problem 1 in the special case of HSTs (Problem 2). Problem 2 is an artificial problem at which the learner can place a _fractional amount of facility_ to the leaves of an HST so as to fractionally serve the arrived clients. Since the _optimal static solution_ of Problem 2 lower bounds the _optimal static solution_ of Problem 1 in the special case of HSTs, the first step of our approach is to design an \((1)\)-regret algorithm for Problem 2.
2. In Section 3.2 we present the formal guarantees of a novel randomized rounding scheme, called \(\), that is client-oblivious and converts any _fractional solution_ for Problem 2 into an actual placement of \(k\) facilities on the leaves of the HST with just an \((1)\)-overhead in the connection and the moving cost.
3. In Section 3.3 we present how the _fractional algorithm_ for Problem 2 together with the \(\) rounding naturally lead to an \((1)\)-regret online learning algorithm for Problem 1 in the special case of HSTs (Algorithm 1). Our main algorithm, presented in Algorithm 2, then consists of running Algorithm 1 into an \(( n)\) HST embedding of input graph.

### A Fractional Relaxation for HSTs

In this section we introduce a fractional relaxation for Problem 1, called _Fractional \(k\)-Clustering with Moving Costs on HSTs_ (Problem 2). Fix any HST \((V,E,w)\) (in this section, \(V\) denotes the nodes of the HST). We begin by presenting a _fractional extension_ of placing \(k\) facilities on the leaves of \(\).

**Definition 6**.: _The set of fractional facility placements \(()\) consists of all vectors \(y^{|V|}\) such that_

1. \(y_{v}\) _for all leaves_ \(v L()\)_._
2. \(y_{v}=_{u(v)}y_{u}\) _for all non-leaves_ \(v L()\)_._
3. \(_{v L()}y_{v}=k\)_, i.e. the total amount of facility on the leaves equals_ \(k\)For a leaf vertex \(v L()\), \(y_{v}\) simply denotes the fractional amount of facilities that are placed on it. For all non-leaf vertices \(v L()\), \(y_{v}\) denotes the total amount of facility placed in the leaves of the sub-tree \(T(v)\). Thus, any integral vector \(y()\) corresponds to a placement of \(k\) facilities on the leaves of \(\).

In Definitions 7 and 8 we extend the notion of connection and moving cost for fractional facility placements. In the special case of integral facility placements, Definitions 7 and 8 respectively collapse to Definitions 1 and 2 (a formal proof is given in Claims 1 and 2 of Section C of the Appendix).

**Definition 7**.: _The fractional connection cost of a set of clients \(R L()\) under \(y()\) is defined as_

\[f_{R}(y):=_{j R}_{v P(j,r)}2^{lev(v)+1}(0,1-y_{v})\]

_where \(P(j,r)\) denotes the set of vertices in the (unique) path from the leaf \(j L()\) to the root \(r\)._

**Definition 8**.: _The fractional moving cost between any \(y,y^{}()\) is defined as_

\[||y-y^{}||_{}:=_{v V()}2^{lev(v) }|y_{v}-y^{}_{v}|\]

We are now ready to present our fractional generalization of Problem 1 in the special case of HSTs.

**Problem 2** (_Fractional \(k\)-Clustering with Moving Costs on HSTs).: _Fix any HST \(\). At each round \(t=1,,T\):_

1. _The learner selects a vector_ \(y^{t}()\)_._
2. _The adversary selects a set of clients_ \(R_{t} L()\)_._
3. _The learner suffers cost_ \(f_{R_{t}}(y^{t})+||y^{t}-y^{t-1}||_{}\)_._

In Section 4, we develop and present an \((1)\)-regret algorithm for Problem 2 (see Algorithm 3). To this end, we present its formal regret guarantee established in Theorem 3.

**Theorem 3**.: _There exists a polynomial-time online learning algorithm for Problem 2 (Algorithm 3), such that for any sequence \(R_{1},,R_{T} L()\), its output \(y^{1},,y^{T}\) satisfies_

\[_{t=1}^{T}f_{R_{t}}(y^{t})+_{t=2}^{T}||y^{t}-y^{t-1}||_{} _{y^{*}()}_{t=1}^{T}f_{ R_{t}}(y^{*})+\]

_for \(=(k|L()|^{3/2} D_{} (,1))\)._

### From Fractional to Integral Placements in HSTs

As already mentioned, the basic idea of our approach is to convert at each round \(t 1\) the _fractional placement_\(y^{t}()\) produced by Algorithm 3 into an integral facility placement \(F_{t} L()\) with \(|F_{t}|=k\) on the leaves of the HST. In order to guarantee small regret, our rounding scheme should preserve both the connection and the moving cost of the fractional solution within constant factors for _any possible set of arriving clients_. In order to guarantee the latter, our rounding scheme \(\) (Algorithm 4) uses shared randomness across different rounds. \(\) is rather complicated and is presented in Section 5. To this end, we present its formal guarantee.

**Theorem 4**.: _There exists a linear-time deterministic algorithm, called \(\) (Algorithm 4), that takes as input an HST \(\), a fractional facility placement \(y()\) and a vector \(^{|V|}\) and outputs a placement of \(k\) facilities \(F(,y,)\) on the leaves of \(\) (\(F L()\) and \(|F|=k\)) such that_

1. \(_{(0,1)}[C_{R}(F)]=f_{R}(y)\) _for all client requests_ \(R L()\)_._
2. \(_{(0,1)}[ M_{}(F, F^{})] 4||y-y^{}||_{}\) _for all other fractional facility placements_ \(y^{}()\) _and_ \(F^{}(,y^{},)\)Item \(1\) of Theorem 4 establishes that although \(\&\) is _oblivious_ to the arrived set of clients \(R_{t} L()\), the expected connection cost of the output equals the _fractional connection cost_ under \(y^{t}()\). Item 2 of Theorem 4 states that once the same random seed \(\) is used into two consecutive time steps, then the expected moving cost between the facility-placements \(F_{t}\) and \(F_{t+1}\) is at most \((1)\)-times the fractional moving cost between \(y^{t}\) and \(y^{t+1}\). Both properties crucially rely on the structure of the HST and consist one of the main technical contributions of our work.

### Overall Online Learning Algorithm

We are now ready to formally introduce our main algorithm (Algorithm 2) and prove Theorem 1. First, we combine the algorithms from Theorems 3 and 4 to design an \((1)\)-regret algorithm for Problem 1 on HSTs (Algorithm 1). Up next we present how Algorithm 1 can be converted into an \(( n)\)-regret online learning algorithm for general graphs, using the metric embedding technique of Theorem 2, resulting to our final algorithm (Algorithm 2).

```
1:Input: A sequence \(R_{1},,R_{T} L()\).
2: The learner samples \(_{v}(0,1)\) for all \(v V()\).
3:for each round \(t=1\)to\(T\)do
4: The learner places the \(k\) facilities to the leaves of the HST \(\) based on the output \(F_{t}:=\&(,y^{t},)\).
5: The learner learns \(R_{t} L()\).
6: The learner updates \(y^{t+1}()\) by running Algorithm 3 for Problem 2 with input \(R_{1},,R_{t}\).
7:endfor ```

**Algorithm 1**\((1)\)-regret for HSTs.

**Theorem 5**.: _For any sequence of client requests \(R_{1},,R_{T} L()\), the sequence of facility-placements \(F_{1},,F_{T} L()\) produced by Algorithm 1 satisfies_

\[[_{t=1}^{T}C_{R_{t}}(F_{t})+_{t=2}^{T}M_{ }(F_{t},F_{t-1})] 6_{|F^{*}|=k}_{t=1}^{T}C_{R_{t}} (F^{*})+\]

_for \(=(k|L()|^{3/2} D_{} (,1))\)._

Theorem 5 establishes that Algorithm 1 achieves constant regret in the special case of HSTs and its proof easily follows by Theorems 3 and 4. Then, the proof of Theorem 1 easily follows by Theorem 2 and Theorem 5. All the proofs are deferred to Section C of the Appendix.

## 4 \((1)\)-Regret for Fractional HST Clustering

In this section we present the \((1)\)-regret algorithm for Problem 2, described in Algorithm 3, and exhibit the key ideas in establishing Theorem 3. Without loss of generality, we can assume that the facility-weight satisfies \( 1\)5.

Algorithm 3 is the well-known online learning algorithm _Follow the Regularized Leader_ (\(\)) with a specific regularizer \(R_{}()\) presented in Definition 9. Our results crucially rely on the properties of this regularizer since it takes into account the HST structure and permits us to bound the fractional moving cost of \(\).

**Definition 9**.: _Given an HST \(\), the dilated entropic regularizer \(R_{}(y)\) over \(y()\) is defined as_

\[R_{}(y):=_{v r}2^{(v)}(y_{v}+_{v}) (+_{v}}{y_{p(v)}+_{p(v)}})\]

_where \(_{v}:=(k/n)|L() T(v)|\) and \(n:=|L()|\)._

```
1:Input: An adversarial sequence \(R_{1},,R_{T} L()\).
2:for\(t=1\)to\(T\)do
3: The learner selects \(y^{t}()\).
4: The learner suffers cost \(f_{R_{t}}(y^{t})+||y^{t}-y^{t-1}||_{}\).
5: The learner updates \(y^{t+1}_{y()}[_{s=1}^{t}f _{R_{s}}(y)+() R_{}(y)]\).
6:endfor ```

**Algorithm 3**\(\) with dilated entropic regularization

Algorithm 3 selects at each step \(t\) the facility placement \(y^{t}()\) that minimizes a convex combination of the total fractional connection cost for the sub-sequence \(R_{1},,R_{t-1}\) and \(R_{}(y)\). The regularization term ensures the stability of the output, which will result in a bounded fractional moving cost.

Analysis of Algorithm 3.Due to space limitations, all proofs are moved to Section D of the Appendix. The primary reason for the specific selection of the regularizer at Definition 9 is that \(R_{}()\) is strongly convex with respect to the norm \(||||_{}\) of Definition 8, as established in Lemma 1 which is the main technical contribution of the section. We use \(D=D_{}\) for the diameter of \(\).

**Lemma 1**.: _For any vectors \(y,y^{}()\),_

\[R_{}(y^{}) R_{}(y)+ R_{}(y),y^{}-y+(8kD^{2})^{-1}||y-y^{}||_{ }^{2}\]

The strong convexity of \(R_{}(y)\) with respect to \(||||_{}\) is crucial since it permits us to bound the moving cost of Algorithm 3 by its fractional connection cost.

**Lemma 2**.: _For any sequence \(R_{1},,R_{T} L()\), the output of Algorithm 3 satisfies_

\[_{t=2}^{T}||y^{t}-y^{t-1}||_{}_{t=1}^{ T}f_{R_{t}}(y^{t})+( kD)\]

We remark that using another regularizer \(R()\) that is strongly convex with respect to another norm \(||||\) would still imply Lemma 1 with respect to \(||||\). The problem though is that the _fractional moving cost_\(_{t=1}^{T}||y_{t}-y_{t-1}||\) can no longer be associated with the actual moving cost \(_{t=1}^{T}M_{}(F_{t},F_{t-1})\). It is for this reason that using a regularizer that is strongly convex with respect to \(||||_{}\) is crucial.

Next, by adapting the standard analysis of \(\) to our specific setting, we derive Lemma 3 establishing that Algorithm 3 admits bounded connection cost.

**Lemma 3**.: _For any sequence \(R_{1},,R_{T} L()\), the output of Algorithm 3 satisfies_

\[_{t=1}^{T}f_{R_{t}}(y^{t})_{y^{*}}_{t=1}^{T}f_{ R_{t}}(y^{*})+(kn^{3/2}D)\]

The proof of Theorem 3 directly follows by Lemma 2 and 3. We conclude the section by presenting how Step \(5\) of Algorithm 3 can be efficiently implemented, namely

\[_{y()}_{t}(y):=_{s=1}^{t}f_{R_{s}}(y)+( ) R_{}(y).\]

Since \(_{t}(y)\) is strongly convex and the set \(()\) is a polytope, one could use standard optimization algorithms such as the _ellipsoid method_ or _projected gradient descent_ to approximately minimize\(_{t}(y)\) given access to a _sub-gradient oracle for \(_{t}()\)_. In Claim 11 of Section D of the Appendix, we establish that the sub-gradients of \(()\) can be computed in polynomial time and thus any of the previous methods can be used to approximately minimize \(()\). In Lemma 4 we establish the intuitive fact that approximately implementing Step \(5\) does not affect the guarantees of Theorem 3.

**Lemma 4**.: _Let \(y^{t}\) be the minimizer of \(_{t}()\) in \((T)\) and let \(z^{t}()\) be any point such that \(_{t}(z^{t})_{t}(y^{t})+\) for some \(=(T^{-1/2})\). Then,_

\[f_{R_{t}}(z^{t})+||z^{t}-z^{t-1}||_{} f_{R_{t}}(y^{t})+||y^{t}- y^{t-1}||_{}+(kn^{3/2}D) T^{-1/2}\]

**Remark 3**.: _In our implementation of the algorithm, we approximately solve Step 5 of Algorithm 3 via Mirror Descent based on the Bregman divergence of \(_{}()\). This admits the same convergence rates as projected gradient descent but the projection step can be computed in linear time with respect to the size of the HST \(\). We present the details of our implementation in Section C of the Appendix._

## 5 The Cut\(\&\)Round Rounding

In this section we present our novel rounding scheme (Algorithm \(\&\)) as well as the main steps that are required in order to establish Theorem 4. To ease notation, for any real number \(x 0\) we denote its decimal part as \((x)=x- x\). We comment that our rounding scheme simply maintains and updates a distribution over the vertices of the HST, and can be thus implemented in polynomial-time. Similar rounding schemes, like the one presented in , typically maintain a distribution over all possible facility-placements, which generally cannot be implemented in polynomial-time.

```
1:Input: An HST \(\), a fractional placement \(y()\) and thresholds \(_{v}\) for all \(v V()\).
2:\(Y_{r} k\)
3:for levels \(=h()\) to 1 do
4:for all nodes \(v\) with \(lev(v)=\)do
5:\(Y_{rem} Y_{v}\)
6:\(y_{rem} y_{v}\)
7:for all children \(u(v)\)do
8:\(Y_{u}(y_{u},Y_{rem},y_{rem},_{u})\)
9:\(Y_{rem} Y_{rem}-Y_{u}\)
10:\(y_{rem} y_{rem}-y_{u}\)
11:endfor
12:endfor
13:endfor
14:return\(F:=\{u L():Y_{u}=1\}\). ```

**Algorithm 4**\(\&\).

On principle, \(\&\) (Algorithm 4) assigns to each vertex \(v\) an integer number of facilities \(Y_{v}\) to be placed at the leaves of its sub-tree. Notice that due to sub-routine \(\) (Algorithm 5), \(Y_{v}\) either equals \( y_{v}\) or \( y_{v}+1\). \(\&\) initially assigns \(k\) facilities to the set of leaves that descend from the root \(r\), which is precisely \(L()\). Then, it moves in decreasing level order to decide \(Y_{v}\) for each node \(v\). Once \(Y_{v}\) is determined (Step 5), the \(Y_{v}\) facilities are allocated to the sub-trees of its children \(u(v)\) (Steps 7-10) via sub-routine \(\) using the thresholds \(_{u}\), in a manner that guarantees that \(Y_{v}=_{u(v)}Y_{u}\) (see Section E.1 of the Appendix). This implies the feasibility of \(\&\), as exactly \(k\) facilities are placed in the leaves of \(\) at the end of the process.

Assuming that the set of thresholds \(_{v}\) is randomly drawn from the uniform distribution in \(\), sub-routine \(\) (Algorithm 5) guarantees that \(Y_{v}\) either equals \( y_{v}\) or \( y_{v}+1\) while \(_{}[Y_{v}]=y_{v}\). This is formally captured in Lemma 5 and is crucial in the proof of Theorem 4.

**Lemma 5**.: _Consider Algorithm 4 given as input a vector \(y()\) and random thresholds \(_{v}(0,1)\). Then,_

\[Y_{v}=\{|y_{v}|&1-(y_{v})\\ |y_{v}|+1&(y_{v}).\]

By coupling Lemma 5 with the HST structure we are able to establish Theorem 4. The proof is technically involved and thus deferred to Section E of the Appendix.

## 6 Conclusion

In this work, we designed the first polynomial-time online learning algorithm for _Online \(k\)-Clustering with Moving Costs_ that achieves \(( n)\)-regret with respect to the cost of the optimal _static_ facility placement, extending the results of Fotakis et al.  for the special case of \(=0\). The cornerstone of our approach was to realize and establish that \((1)\)-regret is plausible for HST metrics. This was achieved through designing a dilated entropic regularizer to capture the structure of the HST and combine it with the FTRL algorithm, as well as designing a lossless (up to constant factors) rounding scheme that simultaneously works for both the connection and the moving cost. Both of these components where central towards acquiring constant regret on HSTs.

A interesting future direction is to investigate whether a polynomial-time online learning algorithm with \((1)\)-regret for the problem is theoretically possible or not. Since the \(( n)\)-factor is inherently lost when using HST embeddings, this would require a significantly different approach to the one presented in this work. Finally, we comment that our current optimality guarantees are with respect to the optimal _static_ facility placement. Going beyond the notion of regret, an intriguing future direction is establishing guarantees with respect to the _optimal dynamic facility-placement_ that moves facilities from round to round by suffering the corresponding moving cost.