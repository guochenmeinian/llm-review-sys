# One-to-Multiple: A Progressive Style Transfer Unsupervised Domain-Adaptive Framework for Kidney Tumor Segmentation

One-to-Multiple: A Progressive Style Transfer Unsupervised Domain-Adaptive Framework for Kidney Tumor Segmentation

 Kai Hu

Xiangtan University

kaihu@xtu.edu.cn

&Jinhao Li

Xiangtan University

jhli@smail.xtu.edu.cn

&Yuan Zhang

Xiangtan University

yuanz@xtu.edu.cn

&Xiongjun Ye

Cancer Hospital, Chinese Academy of Medical Sciences

yexiongjun@cicams.ac.cn

&Xieping Gao

Hunan Normal University

xpgao@hunnu.edu.cn

Corresponding authors.

###### Abstract

In multi-sequence Magnetic Resonance Imaging (MRI), the accurate segmentation of the kidney and tumor based on traditional supervised methods typically necessitates detailed annotation for each sequence, which is both time-consuming and labor-intensive. Unsupervised Domain Adaptation (UDA) methods can effectively mitigate inter-domain differences by aligning cross-modal features, thereby reducing the annotation burden. However, most existing UDA methods are limited to one-to-one domain adaptation, which tends to be inefficient and resource-intensive when faced with multi-target domain transfer tasks. To address this challenge, we propose a novel and efficient _One-to-Multiple_ Progressive **S**tyle **T**ransfer **U**nsupervised **D**omain-**A**daptive (PSTUDA) framework for kidney and tumor segmentation in multi-sequence MRI. Specifically, we develop a multi-level style dictionary to explicitly store the style information of each target domain at various stages, which alleviates the burden of a single generator in a multi-target transfer task and enables effective decoupling of content and style. Concurrently, we employ multiple cascading style fusion modules that utilize point-wise instance normalization to progressively recombine content and style features, which enhances cross-modal alignment and structural consistency. Experiments conducted on the private MSKT and public KiTS19 datasets demonstrate the superiority of the proposed PSTUDA over comparative methods in multi-sequence kidney and tumor segmentation. The average Dice Similarity Coefficients are increased by at least 1.8% and 3.9%, respectively. Impressively, our PSTUDA not only significantly reduces the floating-point computation by approximately 72% but also reduces the number of model parameters by about 50%, bringing higher efficiency and feasibility to practical clinical applications. 2

## 1 Introduction

Kidney tumor segmentation is a crucial task in medical image analysis, playing an essential role in the diagnosis, staging, and treatment of kidney cancer [1; 2; 3]. Previous studies have primarily focused on computed tomography (CT) images [4; 5; 6; 7; 8; 9], but in recent years, magnetic resonance imaging (MRI) has emerged as a safer alternative due to its non-radiative nature [10; 11; 12]. Compared to CT,MRI offers superior contrast for characterizing kidney tissue and tumors, enabling a clearer distinction between normal and pathological tissues. Furthermore, the multi-parameter imaging capability of MRI generates diverse sequences [13; 14; 15] that enhances the comprehensive description of pathological features. These advantages have established MRI as the preferred modality for clinicians in diagnosing kidney disease, particularly kidney tumors, and have facilitated the widespread use of multi-sequence MRI in clinical practice for kidney and tumor segmentation.

In existing studies, traditional supervised segmentation methods typically rely on a large amount of annotated data, and performing fine-grained annotations for a single sequence is already a time-consuming and labor-intensive task, let alone for every sequence. Unsupervised domain adaptation (UDA) methods [16; 17; 18; 19; 20] have been proposed as a promising solution to address these challenges. Current UDA methods mainly focus on one-to-one domain adaptation, involving a single source domain and a target domain. Although these methods perform well in certain scenarios, they lack scalability, meaning that for multi-target domain tasks, training paired generators for each source-target domain pair results in significant computational costs and resource consumption. Furthermore, one-to-one domain adaptation methods can only learn fixed mappings between two domains, thereby failing to capture the potential commonalities and connections among multiple sequences.

As an initial attempt to apply multi-domain adaptation technology to medical imaging, Xu et al.  proposed OMUDA for abdominal organ segmentation using multi-sequence MRI. However, the architecture of OMUDA is derived from StarGAN v2 , which was originally designed for style transfer in natural images and has not been specifically tailored and optimized for the needs of domain adaptation in medical imaging. Although it shows improvement in resolving domain confusion across multiple domains, this limitation leads to suboptimal generated images with regards to organ structural consistency and detail preservation. Consequently, the task of one-to-multiple domain adaptation in medical imaging remains a significant challenge, necessitating more refined and specialized approaches for further advancement.

In this paper, we propose a _One-to-Multiple_**P**rogressive **S**tyle **T**ransfer **U**nsupervised **D**omain-**A**daptive (PSTUDA) framework, which explicitly stores multi-level style features from different domains in designated multi-level style dictionaries, thus alleviating the burden on the generator and achieving the decoupling of content features from style features. Our PSTUDA employs multiple cascaded style fusion modules to recombine multi-level style features from different domains with content features layer by layer using **P**oint-wise **I**nstance **N**ormalization (PIN), thereby ensuring that the generated images across multiple target domains have high-quality style and structure.

The main contributions of our work are summarised as follows:

* We explore a novel and efficient _One-to-Multiple_**P**rogressive **S**tyle **T**ransfer **U**nsupervised **D**omain-**A**daptive (PSTUDA) framework, which is capable of simultaneously transferring a single annotated source domain to multiple unannotated target domains, significantly reducing the need for tedious domain adaptation work for each target domain.
* We introduce a multi-level style dictionary that stores style information for each domain at different stages of style transfer, alleviating the burden on the generator to perform multiple tasks and effectively decoupling content features from style features.
* We propose a progressive style transfer paradigm and a **P**oint-wise **I**nstance **N**ormalization (PIN) method. The former comprises multiple cascading style fusion modules, each recombining content features with corresponding style features through PIN, thereby achieving better cross-modal alignment and structural consistency.
* We construct a multi-sequence kidney tumor MRI dataset called MSKT to facilitate research on kidney tumor analysis. Quantitative and qualitative results on the MSKT and the public dataset KiTS19 show that our PSTUDA framework outperforms the state-of-the-art methods and significantly improves segmentation performance and training efficiency.

## 2 Related Work

### Kidney Tumor Segmentation

Computer-aided diagnostic methods for kidney tumor segmentation play a crucial role in clinical practice [23; 24; 25; 26; 27]. The significant variability in the size, shape, and location of kidneys and tumors presents a considerable challenge to accurate segmentation. Yu et al.  developed Crossbar-Net to capture global and local features of kidney tumors through crossbar patches and focused on difficult-to-segment regions through a cascade training strategy. Myronenko et al.  designed a dedicated boundary branch supervised by an edge-aware loss term to enhance the consideration of organ and tumor edge information. These approaches are centered on improving the ability of the model to recognize the complex morphology of kidney tumors. To further advance this effort, Pandey et al.  integrated active contouring with 3D-U-Net to achieve precise delineation of kidney tumor shapes, showcasing the potential of merging deep learning with traditional image processing techniques. However, these approaches typically rely on fully supervised learning with extensive pixel-level annotations. To address this limitation, researchers are exploring alternative solutions in the fields of semi-supervised, self-supervised, and unsupervised learning. Ma et al.  introduced an Affinity Network that learns from limited data using k-nearest neighbors attention pooling layers. Similarly, Ciga et al.  and Faust et al.  developed methods that enhance feature learning and guide tumor analysis through self-supervised and unsupervised techniques, respectively. By reducing the reliance on annotated data, these methods not only lower costs but also improve the model's generalization capabilities.

### Unsupervised Domain Adaptation

UDA is one of the important methods for addressing domain difference. It aims to transfer a model from an annotated source domain to an unannotated target domain. Existing works have mostly focused on one-to-one domain adaptation [34; 35; 36; 37; 38; 39], yielding impressive outcomes. For example, CycleGAN  used cycle consistency constraints to transform unpaired images from one domain to another. CyCADA  enforced cycle consistency by combining methods of image space alignment and latent representation space alignment. MUNIT  decomposed image representations into content and style codes, enabling multimodal image translation. For medical image domain adaptation, SIFA  achieved domain alignment from both image and feature perspectives, enabling the segmentation network to adapt to the unannotated target domain. Thereafter, DEPL  further improved the segmentation accuracy by employing multi-source domain extension and selective pseudo-labelling strategies.

However, these one-to-one domain adaptation methods lack scalability when handling multiple domain transfer tasks, as they can only learn the relationships between two different domains at a time. StarGAN  performed image-to-image translation across multiple domains using a single model, and then the improved version, StarGAN v2  further enhanced diversity in generated images by introducing style codes specific to each domain. Additionally, Sharma and Hamarneh  proposed a multi-modal generative adversarial network that leveraged redundant information from available sequences to synthesize missing MRI pulse sequences in patient scans. Gholami et al.  developed an information-theoretic approach that aimed to find shared latent spaces for domain adaptation across multiple target domains. It should be noted that these methods predominantly focus on the diversity of the generated images, while paying less attention to the structural consistency before and after image translation.

### Normalization of Image Translation

In UDA tasks, image normalization is a key step that can help models to learn and transfer features effectively [45; 46; 47; 48; 49; 50]. Instance Normalization (IN)  normalizes the features of each sample to improve the quality and realism of generated images. Adaptive Instance Normalization (AdaIN)  dynamically adjusts the relationship between style and content features of input images to enable a rapid transformation across arbitrary styles. Batch-Instance Normalization (BIN)  explicitly normalizes unnecessary style variations in images while preserving useful styles. AdaAttN  introduces adaptive attention normalization to optimize the effects of arbitrary neural style transfer. GramLIN  continuously measures the proximity of the current stylized output to the target style to achieve progressive stain transfer. In medical image domain adaptation tasks, it is crucial to maintain structural consistency during image translation, in addition to changing image styles. Therefore, we propose a novel normalization method called PIN, which progressively fuses content and style features at each pixel by considering image details and local style differences. The goal is to ensure well-structured images after transfer.

## 3 Method

### Overview of the proposed One-to-Multiple Framework

Formally, for a one-to-multiple domain adaptation task, we have one source domain \(S\) and multiple target domains \(T_{i}\). The source domain \(S\) is annotated, denoted as \(S=\{(x_{j},y_{j})\}_{j=1}^{N_{s}}\), while the target domains are unannotated, denoted as \(T_{i}=\{x_{j}^{T_{i}}\}_{j=1}^{N_{T_{i}}}\). Each domain is assigned a domain label, for example, \(L=[0,1,...,N]\) representing the source domain 0, target domain 1, _etc._, up to target domain \(N\). Figure 1(a) illustrates our PSTUDA framework, which mainly consists of a shared generator (including an encoder, a decoder, and multiple style fusion modules) and a multi-scale discriminator. \(X_{src}\), \(X_{trgs}\), and \(X_{recon}\) represent the input image from the source domain, the generated pseudo-target domain images corresponding to the input image, and the image reconstructed back to the source domain from the pseudo-target domain images, respectively. Our task involves two stages. The goal of the first stage is to train a single generator \(G\) such that given a source domain image \(X_{src} S\) and any target domain label \(l_{i}\), it can generate a pseudo-target domain image \(X_{trg\_i}\) corresponding to the image \(X_{src}\), _i.e._, \(G(X_{src},l_{i}) X_{trg\_i}\). The objective of the second stage is to utilize the generated pseudo-target domain images \(X_{trgs}\) and source domain annotation \(y\) to train a segmentation network to achieve accurate segmentation of the kidney and tumor.

### Multi-level Style Dictionary

Our design is inspired by the StarGAN v2 multi-domain image generation framework, which employs a Mapping Network and a Style Encoder to obtain style encodings. The Mapping Network transforms random Gaussian noise into diverse style encodings, while the Style Encoder extracts style encodings from reference images. These techniques are essential for transforming natural image styles and enhancing artistic variety. However, in medical image domain adaptation, they encounter challenges in extracting comprehensive style features. Style encodings from a single image may not capture the full range of styles within a domain, and deriving styles from Gaussian noise, while introducing

Figure 1: (a) The overall architecture of the proposed One-to-Multiple Progressive Style Transfer Unsupervised Domain-Adaptive framework, which includes a shared generator and a discriminator. The generator is composed of an encoder, a decoder, and multiple style fusion modules. (b) shows the progressive style transfer process, achieved through cascaded style fusion modules.

diversity, can result in instability. In medical image analysis, such instability could potentially compromise feature recognition and segmentation accuracy.

To address these challenges, we propose to define a set of learnable multi-level style dictionaries for each domain, where the style information at each level closely corresponds to the respective phase of style fusion. This multi-level structure allows for starting with basic style features and exploring more complex layers of style, thereby providing a progressively refined path for the style fusion process. During the generative adversarial process, the continuous updating and learning of the style dictionary can adapt to the ever-changing style demands. The early-stage stylized features will provide feedback for subsequent style feature learning, thus guiding them to update in a dynamic and targeted manner. By learning style information from the whole domain, the style dictionary becomes more representative, which effectively overcomes the limitation of extracting style encoding from a single image. Moreover, the iterative updates of style dictionaries enhance stability and reduce uncertainty in extracting style encodings from random noise. Storing style encoding information in a multi-level dictionary helps alleviate burden on the generator and achieves effective decoupling of content features and style features. This allows it to focus on capturing domain-invariant features such as structure and shape. In this way, the multi-levels style dictionary refines complex styles within target domains at each level, ensuring that each can timely reflect latest progress in style transfer process more precise and coordinated.

### Progressive Style Transfer Paradigm

In PSTUDA, multiple cascaded style fusion modules and decoders constitute the core components of progressive style transfer. Figure 1(b) illustrates the main processes of the first and last (decoding phase) style transfer stages, with similar style transfer processes in the intermediate stages. In the first style transfer stage, the style fusion module requires two inputs: the content feature \(X_{1}^{H W C}\) obtained by downsampling from the encoder, and the first-level style encoding \(V_{1}^{S S 1}\) from the target domain, matched with the current style fusion module. We utilize the domain label \(L\) multiplied by the first style dictionary to select the target style encoding \(V_{1}\). First, the content feature \(X_{1}\) passes through a convolutional layer to obtain the content feature \(_{1}\), and the target style encoding \(V_{1}\) undergoes two consecutive \(1 1\) convolutions for channel transformation to obtain the style feature \(_{1}^{H W C}\).

After obtaining the content feature \(_{1}\) and style feature \(_{1}\), we propose a novel style fusion normalization method, Point-wise Instance Normalization (PIN), to effectively combine the two for more subtle and accurate pixel-level style transfer. The PIN can be defined as:

\[PIN(_{l},_{l})=_{chw}(_{l}) Norm(_{l})+ _{chw}(_{l}),\] (1)

\[_{chw}(_{l}),_{chw}(_{l})=Chunk(h_{(2c)hw}),\] (2)

\[h_{(2c)hw}=ConvBlock(_{l}),\] (3)

where \(Norm()\) denotes the Instance Normalization of the content feature \(_{l}^{H W C}\) at layer \(l\). The parameters \(_{chw}\) and \(_{chw}\) are scaling and shifting parameters specific to the target domain. These parameters are derived by applying \(onvBlock\) and \(Chunk\) operations to the style feature \(_{l}^{H W C}\) and are adjusted to match the spatial dimensions of the content feature \(_{l}\), thus enabling independent style transfer at each pixel. By rescaling the feature map using these parameters, style information specific to the target domain is integrated into the feature map for style transfer. PIN provides unique style statistics for each spatial point of the content feature, allowing for customized style fusion based on different regions and features of the image content. This facilitates finer local style variations and richer style details, making it particularly suitable for applications requiring precise style adjustments, such as medical image segmentation.

In the final stage of style transfer, the decoder takes the stylized content feature \(X_{n}^{H^{} W^{} C^{}}\) from the previous layer and the last-level style encoding \(V_{n}^{S S 1}\) as input. To spatially match the content feature \(X_{n}\), the style encoding \(V_{n}\) first undergoes upsampling through a deconvolutional layer for scale transformation, followed by two consecutive \(1 1\) convolutions for channel transformation, resulting in the style feature \(_{n}^{H^{} W^{} C^{}}\). The other style fusion operations are similar to those in the first stage. There are two considerations for performing style transfer during the decoding phase. Firstly, the decoding phase is the process of image reconstruction, where integratingtly features can effectively incorporate high-level style information from the target domain into the image. This helps to better express the target style while preserving content information, especially in terms of details and textures. Secondly, as the decoder is responsible for upsampling, it has the opportunity to refine and restore image details while enlarging the feature maps. Introducing style transfer at this stage ensures that these details not only conform to the content structure but also match the characteristics of the target style, thus achieving effective fusion of content and style at different scales.

### Generator and Discriminator Architecture

GeneratorAs previously mentioned, the generated pseudo-target domain images will be used in conjunction with the corresponding segmentation annotations from the source domain to train the subsequent segmentation networks. Therefore, it is crucial to maintain structural consistency during the image translation process for the success of downstream applications. The generator in StarGAN v2, due to its multiple downsampling steps, unfortunately leads to a loss of spatial information, which poses challenges in maintaining structural consistency during image translation. In light of CycleGAN's outstanding performance in one-to-one medical image translation, we adopt the approach of OMUDA by integrating the generator architecture of CycleGAN into our encoder and decoder. To facilitate a single generator to handle one-to-multiple transfer tasks, we replace the IN layers in the decoder with PIN layers. The complete generator comprises an encoder, a decoder, and a series of cascaded style fusion modules. Among them, the encoder, equipped with two downsampling layers and multiple IN layers, is tasked with extracting domain-invariant features from the input image of the source domain. The cascaded style fusion modules are responsible for integrating the style features of the target domain with the domain-invariant features of the source domain. The decoder is composed of two upsampling layers embedded with PIN and is responsible for style transfer and image reconstruction during the decoding phase.

DiscriminatorInspired by Wang et al.'s work , we enhance the original discriminator architecture of StarGAN v2 with a multi-scale mechanism. As depicted in Fig. 2, the discriminator is composed of multiple independent discrimination branches, each containing four residual blocks , with each residual block having \(N\) output branches serving \(N\) specific target domains. The input to the discriminator includes not only the original image but also images processed through different scales of downsampling. The multi-branch output of the discriminator first multiplies with the target domain label \(L\) to select the discrimination output for the corresponding domain before proceeding to authenticity determination. By evaluating the generated images at various scales, the multi-scale discriminator can more comprehensively judge the realism of the images, thereby encouraging the generator to produce images with higher quality. More detailed information on the generator and discriminator architectures can be found in Appendix B.1.

### Loss Function

For convenience, we give the following symbol definition. \(x_{s} X\) denotes a source domain image, with its corresponding source domain label \(l_{s} L\). \(l_{t} L\) represents any target domain label. \(G\) and \(D\) stand for the generator and discriminator, respectively. The loss functions in the generator include adversarial loss \(_{adv}\), cycle consistency loss \(_{cyc}\), and identity loss \(_{idt}\). The loss in the discriminator includes adversarial loss \(_{adv}\). Among them, the adversarial loss is defined as

\[_{adv}=E_{x_{s},l_{t}}[log(1-D(G(x_{s},l_{t}),l_{t}))]+ E_{x_{s}}[logD(x_{s},l_{s})],\] (4)

Figure 2: Architecture of the Multi-Scale Discriminator, composed of multiple residual blocks.

which encourages the generator to generate images that are indistinguishable from the target domain images by the discriminator. The cycle consistency loss is computed as

\[_{cyc}=E_{x_{s},l_{s},l_{t}}[\|G(G(x_{s},l_{t}),l_{s})-x_{s }\|],\] (5)

which ensures that an image translated from the source domain to the target domain can be translated back to the original image, thus preserving the structural consistency of the image throughout the translation process. The identity loss can be defined as

\[_{idt}=E_{x_{s},l_{s}}[\|(G(x_{s},l_{s})-x_{s}\| ],\] (6)

which is used to preserve the content of the source domain images when the generator is applied to them. The complete training loss can be summarized as follows:

\[_{G}_{D}_{adv}+_{cyc}_{cyc}+_{idt }_{idt},\] (7)

where \(_{cyc}\) and \(_{idt}\) denote the weights for the corresponding loss terms and they are set to 10 and 1 in our experiments, respectively.

## 4 Experiments

### Dataset

In this study, we utilize two datasets for the performance evaluation of the method. The first one is a private dataset named MSKT, which comprises 104 cases, each including four sequences: T1c, FS T2W, T2W, and DWI. The second is the publicly available KiTS19  dataset. We conduct our first set of comparative experiments using the MSKT dataset. Specifically, the annotated T1c data is served as the source domain, while the unannotated FS T2W, T2W, and DWI data are considered as the target domain. For our second set of comparative experiments, we combine KiTS19 with the MSKT dataset. In this case, all training data from KiTS19 are used as the source domain, and all four sequences of the MSKT are considered as the target domain. We ensure that there is no overlap between the source domain, target domain, and test set to prevent any potential information leakage. More details about the dataset and evaluation metrics are provided in Appendix A.

### Comparative Study

To comprehensively evaluate the performance of the proposed method, we compare PSTUDA with five other state-of-the-art UDA methods on the MSKT and KiTS19 datasets: CycleGAN , MUNIT , SIFA , DEPL , and StarGAN v2 . Except for SIFA and DEPL, the others are limited to cross-domain image transfer. Therefore, we train a dedicated U-Net  from scratch based on the pseudo-target domain images generated by these methods for multiple sequences. For a fair comparison, all hyperparameters in the U-Net remain consistent. Due to the one-to-one image transfer characteristics of SIFA, DEPL, CycleGAN, and MUNIT, we train a separate model for each sequence using these methods to handle multi-sequence MR image transformations.

Due to the unpaired nature of multi-sequence MR images, it is challenging to quantitatively assess the style effects and structural consistency of the generated images. Experience suggests that if anatomical structures are distorted during the image transfer process, the pseudo-target domain images generated will not correspond with the annotations of the source domain, thereby impairing segmentation performance. Moreover, if there is a significant difference between the style of the generated images and the real target domain images, this discrepancy in data distribution may also detrimentally affect the segmentation results. Hence, the quality of the generated images is indirectly indicated by the segmentation results.

Results on MSKTTable 1 presents the Dice Similarity Coefficient (\(DSC\)) and 95% Hausdorff Distance (\(HD_{95}\)) results for all methods across different MR sequences. The average \(DSC\) indicate that our PSTUDA outperforms other methods in segmentation performance on all MRI sequences. We also observe that PSTUDA significantly surpasses MUNIT, SIFA, DEPL, and StarGAN v2 from both \(DSC\) and \(HD_{95}\). Although SIFA and DEPL are more efficient end-to-end domain adaptation segmentation methods, they do not perform well in our task. The images generated by these methods exhibit structural distortions and lack naturalness (4\({}^{th}\) and 5\({}^{th}\) columns in Fig. 3). Particularly, DEPLshows severe structural distortion of kidneys and tumors in the T2W sequence, which has greater inter-domain differences. MUNIT and StarGAN v2, while excelling in the diversity of generated images, fall short in maintaining cross-domain structural consistency, leading to distorted structures in the generated images (3\({}^{rd}\) and 6\({}^{th}\) columns in Fig. 3). The performance of CycleGAN is close to that of PSTUDA on FS T2W and DWI sequences, but it underperforms on the T2W sequence, as the translated images lose many important structural details, resulting in poor image alignment. In contrast, our PSTUDA prioritizes structural consistency in the cross-domain image translation process by separating content from style and progressively recombining content and style features in a point-wise manner. This approach produces images with higher fidelity, as demonstrated in the last column of Fig. 3, thereby significantly enhancing segmentation performance.

Results on KiTS19 and MSKTThis experiment utilizes CT images from KiTS19 as the source domain and four MRI sequences from MSKT as the target domain. Given the different imaging technologies employed by CT and MRI, the domain difference between them is significantly greater than that between the various MRI sequences. As shown in Table 2, most comparative methods show a reduction in kidney and tumor segmentation performance on FS T2W, T2W, and DWI when compared to the first set of experiments. Although the average _DSC_ of CycleGAN on the DWI is higher than that of PSTUDA (by about 1.9%), its performance is lower than that of PSTUDA by approximately 7.5% and 9.1% on FS T2W and T2W, respectively. Our PSTUDA almost achieves the

    &  &  &  &  \\   & & Kidney & Tumor & Avg. & Kidney & Tumor & Avg. & Kidney & Tumor & Avg. \\   & Supervised training & 90.14 & 88.73 & 89.44 & 87.53 & 80.68 & 84.11 & 90.20 & 84.76 & 87.48 \\  & W/o adaptation & 60.66 & 49.69 & 55.18 & 0.71 & 28.29 & 14.50 & 54.44 & 35.30 & 44.87 \\   & \(\) CycleGAN  & **86.50** & 74.00 & 80.25 & 71.97 & 51.94 & 61.96 & **87.60** & 71.88 & 79.74 \\  & \(\) MUNIT  & 80.96 & 53.00 & 66.98 & 75.78 & 48.34 & 62.06 & 78.29 & 51.87 & 65.08 \\  & \(\) SIFA  & 78.31 & 50.58 & 64.45 & 65.54 & 37.10 & 51.32 & 72.94 & 43.28 & 58.11 \\  & \(\) DEPL  & 85.22 & 68.20 & 76.71 & 23.44 & 23.16 & 23.30 & 83.63 & 64.26 & 73.95 \\  & \(\) StarGAN v2  & 73.58 & 41.77 & 57.68 & 42.87 & 28.68 & 35.78 & 75.26 & 41.29 & 58.28 \\   & PSTUDA(Ours) & 86.30 & **76.36** & **81.33** & **77.26** & **53.77** & **65.52** & 86.99 & **74.23** & **80.61** \\  _{95}\)_ (mm)_} & Supervised training & 2.47 & 1.78 & 2.13 & 3.49 & 8.25 & 5.87 & 2.04 & 3.36 & 2.70 \\  & W/o adaptation & 11.94 & 43.03 & 27.48 & 40.14 & 69.39 & 54.77 & 11.38 & 33.60 & 22.49 \\   & \(\) CycleGAN  & 4.92 & 31.54 & 18.23 & **5.37** & **6.34** & **5.86** & 4.57 & **24.62** & **14.60** \\   & \(\) MUNIT  & 6.08 & 31.95 & 19.01 & 7.36 & 25.58 & 16.47 & 6.43 & 39.09 & 22.76 \\   & \(\) SIFA  & 6.12 & 53.60 & 29.86 & 10.48 & 57.18 & 33.83 & 14.25 & 54.22 & 34.24 \\   & \(\) DEPL  & 6.75 & 36.87 & 21.81 & 17.65 & 70.65 & 44.15 & 11.14 & 25.43 & 18.28 \\   & \(\) StarGAN v2  & 5.39 & 33.20 & 19.29 & 15.43 & 68.98 & 42.20 & 13.70 & 26.84 & 20.27 \\   & PSTUDA(Ours) & **4.66** & **25.85** & **15.25** & 6.81 & 21.37 & 14.09 & **2.96** & 34.65 & 18.80 \\    \(\) implies that we report the results of our own runs according to the official code.

\(\) implies that the method is based on our implementation.

Table 1: Quantitative segmentation results of different comparative methods on the MSKT dataset.

Figure 3: Qualitative results for T1c \(\) FS T2W, T2W, and DWI on the MSKT dataset. Blue and red bounding boxes indicate the annotated boundaries of the kidney and tumor, respectively (Same below).

[MISSING_PAGE_FAIL:9]

Baseline with the M_SD results in a significant performance improvement. This can be attributed to the stability and representativeness of the learned style encodings. Moreover, the integration of the M_SD with PIN fully considers the details and local style variations of images, thereby yielding exceptional results.

Ablation of PIN and other normalization methodsWe compare PIN with AdaIN  and BIN , and as shown in Table 5, PIN outperforms the others. We attribute this superior performance to PIN's ability to account for local style differences, which is particularly advantageous for fine-grained segmentation tasks (_e.g._, kidney tumors, as abnormal pathological tissues, exhibit significant style differences). This capability enables PSTUDA to generate synthetic images that align more closely with the data distribution of the target domain.

## 5 Conclusion

In this work, we propose a novel and efficient _One-to-Multiple_ PSTUDA framework that utilizes a multi-level style dictionary to decouple and store style information. By employing multiple cascaded style fusion modules, our framework progressively recombines content and style features, thereby achieving superior cross-modal alignment and consistency of medical tissue structures. Experimental validation on both a private dataset and a public dataset demonstrates the significant advantages of our method in improving training efficiency for one-to-multiple domain adaptation tasks and enhancing the accuracy of multi-sequence kidney tumor segmentation.

   &  &  &  &  \\    & &  &  & Kidney & Tumor & Avg. & Kidney & Tumor & Avg. & Kidney & Tumor & Avg. \\   & & & 74.96 & 58.46 & 66.71 & 58.48 & 17.05 & 37.77 & 69.29 & 41.41 & 55.35 \\ ✓ & & & 83.75 & 71.48 & 77.62 & 65.82 & 30.45 & 48.14 & 77.38 & 54.99 & 66.19 \\ ✓ & ✓ & & **84.94** & **77.74** & **81.34** & **70.16** & **52.80** & **61.48** & **85.09** & **72.06** & **78.58** \\   & & & 5.33 & 59.68 & 32.50 & 11.44 & 93.41 & 52.42 & 14.55 & 68.95 & 41.75 \\ ✓ & & & **4.89** & 40.75 & 22.82 & 8.94 & 53.27 & 31.11 & 4.50 & 59.67 & 32.08 \\ ✓ & ✓ & & 5.70 & **33.03** & **19.36** & **8.45** & **52.71** & **30.58** & **4.22** & **10.04** & **7.13** \\  

Table 4: Ablation study of Multi-level Style Dictionary (M_SD) and Point-wise Instance Normalization (PIN) in PST on the MSKT Dataset. The Baseline model is StarGAN v2.

   &  &  &  \\  PST & MS\_D & & & Kidney & Tumor & Avg. & Kidney & Tumor & Avg. & Kidney & Tumor & Avg. \\   & & & 74.96 & 58.46 & 66.71 & 58.48 & 17.05 & 37.77 & 69.29 & 41.41 & 55.35 \\ ✓ & & _DSC_ (\%) & 84.94 & **77.74** & **81.34** & 70.16 & 52.80 & 61.48 & 85.09 & 72.06 & 78.58 \\ ✓ & ✓ & & **86.30** & 76.36 & 81.33 & **77.26** & **53.77** & **65.52** & **86.99** & **74.23** & **80.61** \\   & & & 5.33 & 59.68 & 32.50 & 11.44 & 93.41 & 52.42 & 14.55 & 68.95 & 41.75 \\ ✓ & & _HD\({}_{95}\)_ (mm) & 5.70 & 33.03 & 19.36 & 8.45 & 52.71 & 30.58 & 4.22 & **10.04** & **7.13** \\ ✓ & ✓ & & **4.66** & **25.85** & **15.25** & **6.81** & **21.37** & **14.09** & **2.96** & 34.65 & 18.80 \\  

Table 3: Ablation study of Progressive Style Transfer paradigm (PST) and Multi-Scale Discriminator (MS_D) on the MSKT dataset. The Baseline model is StarGAN v2.

   &  &  &  &  \\   & & & Kidney & Tumor & Avg. & Kidney & Tumor & Avg. & Kidney & Tumor & Avg. \\  AdaIN & & 85.05 & 62.11 & 73.58 & 75.40 & 43.32 & 59.36 & 83.69 & 64.44 & 74.07 \\ BIN & _DSC_ (\%) & 82.32 & 67.91 & 75.12 & 74.14 & 49.02 & 61.58 & 85.85 & 65.07 & 75.46 \\ PIN & & **86.30** & **76.36** & **81.33** & **77.26** & **53.77** & **65.52** & **86.99** & **74.23** & **80.61** \\  AdaIN & & 5.46 & **24.30** & **14.88** & **6.15** & 33.44 & 19.79 & 5.22 & 44.20 & 24.71 \\ BIN & _HD\({}_{95}\)_ (mm) & 9.71 & 53.73 & 31.72 & 7.05 & 36.79 & 21.92 & 5.36 & 61.74 & 33.55 \\ PIN & & **4.66** & 25.85 & 15.26 & 6.81 & **21.37** & **14.09** & **2.96** & **34.65** & **18.81** \\  

Table 5: Ablation study of PIN with AdaIN and BIN on the MSKT dataset.