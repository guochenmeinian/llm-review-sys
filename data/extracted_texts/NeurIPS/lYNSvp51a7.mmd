# Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory

Xin Cheng\({}^{1}\)  Di Luo\({}^{2}\)  Xiuying Chen\({}^{3}\)  Lemao Liu\({}^{4}\)  Dongyan Zhao\({}^{1}\)  Rui Yan\({}^{2}\)

\({}^{1}\) Peking University \({}^{2}\) Remin University of China

\({}^{3}\) KAUST \({}^{4}\) Tencent AI Lab

chengxin1998@stu.pku.edu.cn

###### Abstract

With direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation (we define this as _primal problem_). The traditional approach for memory retrieval involves selecting memory that exhibits the highest similarity to the input. However, this method is constrained by the quality of the fixed corpus from which memory is retrieved. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a novel framework, Selfmem, which addresses this limitation by iteratively employing a retrieval-augmented generator to create an unbounded memory pool and using a memory selector to choose one output as memory for the subsequent generation round. This enables the model to leverage its own output, referred to as self-memory, for improved generation. We evaluate the effectiveness of Selfmem on three distinct text generation tasks: neural machine translation, abstractive text summarization, and dialogue generation, under two generation paradigms: fine-tuned small model and few-shot LLM. Our approach achieves state-of-the-art results in four directions in JRC-Acquis translation dataset, 50.3 ROUGE-1 in XSum, and 62.9 ROUGE-1 in BigPatent, demonstrating the potential of self-memory in enhancing retrieval-augmented generation models. Furthermore, we conduct thorough analyses of each component in the Selfmem framework to identify current system bottlenecks and provide insights for future research1.

## 1 Introduction

In recent years, retrieval-augmented text generation has attracted growing interest across various fields, including neural machine translation, dialogue response generation, and language modeling. This innovative generation paradigm initially equips a fine-tuned small model or a large language model (LLM) with access to an external database (typically the training corpus) using information retrieval techniques. Subsequently, the generation process is conducted based on both the input text and the retrieved memory.

In this paradigm, the guiding principle for memory retrieval is to find the memory that exhibits the highest similarity to the current input . This aligns with the human intuition that a more similar demonstration sample typically offers more hints. As demonstrated in Figure 1, for a retrieval-augmented translation model, the memory similarity alone exhibits a strong correlation with the final translation quality, regardless of other factors that may influence translation quality (e.g.,polysemy, morphology, and coreference). We define this as the _primal problem_: **better memory prompts better generation**. Consequently, numerous studies have focused on how to retrieve better memory, ranging from sparse retrieval to dense retrieval [10; 63], from a fixed retriever to a learnable retriever [41; 8], and from sentence-level memory to more fine-grained token-level memory [36; 35].

However, a fundamental limitation exists in all previous works: the memory is retrieved from a fixed corpus and is constrained by the corpus's quality. Due to the finite retrieval space, bounded memory significantly restricts the potential of memory-augmented generation models . In this paper, we explore the duality of the _primal problem_, which posits that **better generation also prompts better memory**. We propose a novel framework called Selfmem, which iteratively employs a retrieval-augmented generator to create an unbounded memory pool and uses a memory selector to choose one output as memory for the subsequent generation round. By combining the _primal_ and _dual problem_, a retrieval-augmented generation model can elevate itself using its own output, referred to as self-memory. The key insight behind Selfmem is that the text more closely resembling the data distribution during inference is not the training data , but the model's own output.

Selfmem consists of two complementary components: a retrieval-augmented generator and a memory selector. The generator operates under two distinct paradigms: fine-tuning a small model or few-shot prompting an LLM. For the former, we train the generator with labeled data and retrieved memory, while for the latter, we employ a fixed black-box LLM exclusively for inference alongside retrieved in-context learning samples. We then use the generator's output to train a memory selector based on a specific performance metric. By simply replacing the retrieved memory with unbounded generated memory, we achieve higher-quality generation output (_primal problem_), which subsequently serves as memory for the next round after being refined by the memory selector (_dual problem_).

To evaluate the efficacy of the Selfmem, we carry out comprehensive experiments in three distinct text generation tasks: neural machine translation, abstractive text summarization, and dialogue generation. We witness substantial enhancements over robust baselines, attaining state-of-the-art outcomes in JRC-Acquis (four directions), XSum (50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1). To gain deeper insights into the Selfmem, we meticulously investigate each crucial component and pinpoint the existing system bottleneck to guide future research endeavors.

## 2 Related Work

### Retrieval-augmented Text Generation

Since the world is not a snapshot once the training corpus is collected, we can never expect an ever-large model to capture everything in its parameters, even for LLMs like GPT-4 . Therefore, it is crucial to equip these models with an external memory bank to store additional knowledge or useful demonstration examples for solving various NLP tasks[41; 78; 95].

In the translation domain, retrieval techniques have long been employed by the localization industry to enhance human translators' productivity and consistency even before the advent of machine translation . Early works on machine translation primarily focused on utilizing memory for statistical machine translation (SMT) systems [80; 50]. For neural machine translation (NMT),  were the first to use search engines to retrieve memory from the training set and incorporate it with an external memory network. Subsequent research explored various aspects of retrieval-augmented NMT, such as memory encoding methods [92; 93; 31], joint training of retrievers and generators with monolingual data , memory granularity , and memory diversity . For few-shot LLM generation, strategies for in-context example selection have been proposed to improve translation quality . Furthermore, in-context machine translation has been shown to be effective for on-the-fly adaptation . For dialogue response generation tasks, employing exemplar/template

Figure 1: Relation between memory and hypothesis on JRC-Acquis En\(\)De dataset. The hypothesis is generated by a retrieval-augmented translator whose memory is retrieved from the training set. The X-axis represents the similarity between memory and the reference.

retrieval as an intermediate step has proven advantageous for generating informative responses [89; 91; 6; 7]. In-context learning example retrieval also aids in controllable dialogue . Other applications include abstractive summarization [64; 14; 18; 15], code generation , paraphrase generation [34; 83], language modeling [36; 105], counterfactual data generation , open domain question answering [12; 33] and semantic parsing .

### Neural Text Reranking

By alleviating the discrepancy between training and inference (i.e., exposure bias) and directly optimizing desired metrics, two-stage reranking methods have facilitated significant progress in various text generation tasks. In machine translation, pioneering works by  and  introduced and popularized discriminative reranking for SMT. In the context of NMT, research has focused on two primary reranking approaches: generative reranking [56; 32; 88] and discriminative reranking [39; 71; 23]. For syntactic parsing,  were the first to employ a two-stage reranking method to select outputs from a base parser, while  introduced a maximum entropy reranker. In text summarization, RefSum  proposed a second-stage summarization framework to address train-test distribution mismatches. SimCLS  used pairwise Learning To Rank (LTR) to select candidates with the highest matching scores. SummaReranker  adopted a multi-task mixture-of-experts framework to leverage different metrics capturing various aspects of generated candidates. BRIO  reused the base model for a second round of fine-tuning with both cross-entropy loss and a candidate-level ranking loss. JGR  employed an alternate training paradigm to train the generator and reranker.

A key limitation of these reranking methods is that they only represent a one-way process, wherein the selected candidates become the system's final output. In contrast, our framework innovatively utilizes the chosen candidates as memory for the subsequent generation round of a retrieval-augmented generator, which can produce better candidates with enhanced memory.

## 3 Methods

In this section, we begin with a motivating experiment on _generation as memory_ (SS 3.1). Then, we introduce Selfmem, a framework comprising a _retrieval-augmented generator_ (SS 3.2) and a _memory selector_ (SS 3.3). The complete framework and algorithm are illustrated in Figure 2 and Algorithm 1.

### Generation as Memory

The primary motivation behind our framework stems from the observation that the memory, which is more similar in distribution to the data during inference, is not the training data (38.89 BLEU, as shown in the first row of Table 1). Instead, it is the model's own output (58.58 BLEU) within the unbounded generation space. One interesting exploration involves directly utilizing the generated output as memory in relation to the _primal problem_: better memory prompts better generation.

We conduct experiments on the JRC-Acquis En\(\)De dataset. The first row in Table 1 represents conventional retrieval-augmented training with retrieved memory and achieves a 58.58 BLEU score. However, directly incorporating beam output of this trained model as memory (Beam) back into the generation model does not yield any improvements (row 2), despite its higher similarity to the reference compared to the retrieved ones. We hypothesize two potential reasons for this: (1) the retrieval-augmented generator may not generalize effectively in this context due to the memory distribution shift (from 38.89 to 58.58), and (2) the beam memory does not offer any information gain compared to the retrieved one, even it exhibits more overlap with the references.

To investigate the first hypothesis, we conduct experiments under the oracle and random scenarios by using the reference as memory (Reference) and randomly sampled sentences as memory (Random). The result is shown in Table 1 and it illustrates that a retrieval-augmented generator (trained with

  
**Memory Source** & **Memory Quality** & **Hypothesis Quality** \\  Retrieval & 38.89 & 58.58 \\ Beam & 58.58 & 58.43 \\  Reference & 100 & 90.43 \\ Random & 1.14 & 49.08 \\   

Table 1: Experiments on the relation between memory quality and the final hypothesis quality, measured by the BLEU score with ground truth translation. The retrieval-augmented translator keeps fixed while the memory is obtained from different sources.

retrieved memory) has already learned to discriminate between different memories in both oracle and random scenarios, without updating the model weights.

To evaluate the second conjecture, we first define the token sets of the reference, retrieved memory, and beam memory as \(,\), and \(\), respectively. The overlap token set, denoted by \(\), is defined as the tokens that overlap with the references in the beam memory but not in the retrieved memory, which is represented as \(-\). \(\) is considered as the additional information provided by the beam memory. Inspired by the confidence analysis of NMT model , we compute the set confidence score, \(()\), as follows:

\[()=_{y^{i}}p(y_{i}|x,y_{<i})\] (1)

where \(p(y_{i}|x,y_{<i})\) is defined by the generation model. \(()\) measures the confidence with which the generation model generates the tokens. The value of \(()\) is 0.58, while that of \(\) is 0.76, indicating that the generator is relatively confident in generating tokens in \(\), and therefore does not need to resort to external memory . Beam search ranks generated candidates based on \(p(y|x)\), where the selected memory falls within the confidence region of the generator and consequently provides no information gain. This observation motivates us to select memory according to metrics other than \(p(y|x)\) in the memory selector (SS3.3).

### Retrieval-augmented Generator

Given a text pair \((x,y)\), where \(x=\{_{1},...,_{|x|}\}\) is the source, \(y=\{_{1},...,_{|y|}\}\) is the target. They could be (document, summary) in summarization, (context, response) in dialogue generation or (source, target) in machine translation. The retrieval-augmented generation would first use \(x\) to retrieve memory \(m\) from datastore \(\). Then the generator \(G_{}(x,m)\), parameterized by \(\), would take both \(x\) and \(m\) as input to generate the target sentence \(y\). In this paper, following standard practice, we choose the training set as \(=\{(x^{i},y^{i})\}_{i=1}^{||}\). For LLM as \(G_{}\), we use the standard in-context learning format to give \((x,y)\) as demonstration example. For tunable generator \(G_{}\), we only keep the target side of _top-1_ retrieval results as memory and we consider two commonly used architectures: **Joint-Encoder**[29; 87; 41] and **Dual-Encoder**[92; 8; 17].

Joint-EncoderThis architecture is the standard encoder-decoder-based model [3; 84]. The input is the concatenation of \(x\) and \(m\). The encoder would first map the input into the hidden states \(H\):

\[H=(xm)\] (2)

Figure 2: Overall framework. There are two components in Selfmem, a retrieval-augmented generator (a) and a memory selector (b). For the primal problem, (a) takes source and memory as input to generate candidates for (b). For the dual problem, (b) takes as input source and generated candidates to select memory for (a).

[MISSING_PAGE_FAIL:5]

legislative text of European Union Law. It is the benchmark dataset used in translation memory-augmented NMT task [28; 92; 8; 17]. We choose 4 translation directions, namely, Spanish\(\)English (Es\(\)En), German\(\)English (De\(\)En). **Summarization.** We evaluate on 2 summarization datasets: 1) XSum , extreme summarization, a single-document summarization dataset with highly abstractive articles from British Broadcasting Corporation. 2) BigPatent , consisting of 1.3 million records of U.S. patent documents along with human-written abstractive summaries. **Dialogue.** We experiment on DailyDialog , which contains multi-turn dialogs on daily life topics and is used by [13; 4; 103]. The detailed statistics for these datasets can be found in the Appendix A.

### Implementation Details

We utilize the BM25 algorithm  for retrieval purposes. For all tasks, the candidate generation method consists of beam search with a beam width of 50. The number of iterations is determined by the performance on the validation set. **For translation**, we follow the approach of [93; 8; 17], employing a randomly initialized Transformerbase architecture as \(G_{}\) for trainable small model and XGLM  for LLM in-context learning. Evaluation metrics include BLEU, TER, and chrF++ obtained from SacreBLEU. The memory selector \(S_{}\) utilizes an XLM-R\({}_{base}\) as backbone, with BLEU serving as \((,)\). **For summarization**, we initialize \(G_{}\) with \(_{}\) for BigPatent and employ BRIO  for XSum. The evaluation metric comprises ROUGE (R-1/2/L) . **For dialogue generation**, \(_{}\) serves as the backbone for \(G_{}\). Our dialogue system is evaluated using BLEU (B-1/2) and Distinct (D-1/2) scores . For both dialogue and summarization tasks, we adhere to the methods of [54; 26], adopting RoBERT\({}_{}\) as the backbone for \(S_{}\). The linear combination of B-1/2 is chosen as \((,)\) for Dialogue Generation, while R-1/2/L is used for Summarization, following . For further implementation details, please refer to the Appendix B and Appendix C for evaluation metrics.

## 5 Experimental Results

### Machine Translation

We select four translation directions and experiment with two generation paradigms: trainable small models and few-shot prompted LLMs [85; 20]. For trainable models, we explore two architectures (joint and dual, as detailed in SS3.2). The baselines comprise two types of translation systems: one being the vanilla sequence-to-sequence model [3; 84] without memory augmentation, and the other consisting of retrieval-augmented translation models focusing on memory encoding [28; 92], memory construction , memory retrieval , and memory diversity . Based on the experimental results2 shown in Table 2, Selfmem significantly enhances the performance of \(G_{}\) across four translation datasets and two different architectures. This is noteworthy, given that the parameters of the \(G_{}\) remain fixed, with the only variable being the input memory. This finding is consistent with the _primal problem_ which posits that improved memory typically leads to better generation results.

The _dual problem_ is revealed in Table 3. Self-memory, which essentially represents the model's own output, exhibits greater similarity with the ground truth and serves as a more effective memory for generating the final output. This observation highlights a key distinction between Selfmem and previous reranking works . Reranking aims to select candidates of higher quality than the beam output, whereas in Selfmem, the chosen candidates serve as memory for the retrieval-augmented generator and do not necessarily need to surpass the quality of the beam hypotheses.

In Table 4, we present the results of LLM with self-memory. We employ XGLM  as our backbone generator, with three different sizes ranging from 1.7B to 7.5B. We utilize the recommended prompt as described in . We select three in-context learning examples and report the average scores from three separate runs, taking into account the sensitivity of example selection in ICL . From the table, we first observe a general trend where few-shot translation performance improves as the

    & &  &  &  \\   & & Random & kNN & Self & Random & kNN & Self & Random & kNN & Self \\   & \(\) & 11.51 & 37.87 & 40.94 & 17.51 & 37.60 & 38.25 & 18.48 & 47.82 & 48.32 \\  & \(\) & 27.42 & 51.00 & 51.88 & 30.62 & 48.12 & 48.36 & 33.03 & 55.65 & 55.12 \\   & \(\) & 23.87 & 46.20 & 48.56 & 31.83 & 48.37 & 49.17 & 29.97 & 53.86 & 54.32 \\  & \(\) & 25.29 & 51.55 & 53.13 & 32.16 & 48.55 & 49.22 & 35.22 & 57.25 & 57.56 \\   

Table 4: Evaluation results of in-context learning with self-memory.

    &  &  &  &  \\   & Dev & Test & Dev & Test & Dev & Test & Dev & Test \\   \\  RNNsearch  & 55.02 & 59.34 & 50.54 & 50.48 & 50.20 & 49.74 & 44.94 & 43.98 \\ Transformer  & 64.08 & 64.63 & 62.02 & 61.80 & 60.18 & 60.16 & 54.65 & 55.43 \\   \\  SEG-NMT  & 60.28 & 59.34 & 57.62 & 57.27 & 55.63 & 55.33 & 49.26 & 48.80 \\ NMT-pieces  & 63.97 & 64.30 & 61.50 & 61.56 & 60.10 & 60.26 & 55.54 & 55.14 \\ G-TFM  & 66.37 & 66.21 & 62.50 & 62.76 & 61.85 & 61.72 & 57.43 & 56.88 \\ MonoNMT  & 67.73 & 67.42 & 64.18 & 63.86 & 64.48 & 64.62 & 58.77 & 58.42 \\ CMM  & 67.48 & 67.76 & 63.84 & 64.04 & 64.22 & 64.33 & 58.94 & 58.69 \\ Transformer\({}_{}\) & 66.87 & 67.12 & 63.14 & 63.54 & 64.09 & 63.36 & 58.69 & 58.06 \\ Transformer\({}_{}\) & 67.74 & 67.32 & 63.93 & 64.12 & 64.50 & 64.40 & 58.16 & 58.58 \\   \\  Transformer\({}_{}\) & **68.63\({}^{}\)** & **69.20\({}^{}\)** & 64.12\({}^{}\) & 64.67\({}^{}\) & 65.06\({}^{}\) & 64.98\({}^{}\) & 59.26\({}^{}\) & 59.49\({}^{}\) \\ Transformer\({}_{}\) & 68.26\({}^{}\) & 68.80\({}^{}\) & **66.07\({}^{}\)** & **65.94\({}^{}\)** & **65.32\({}^{}\)** & **65.65\({}^{}\)** & **59.88\({}^{}\)** & **60.11\({}^{}\)** \\   

Table 2: Results of translation task on JRC-Acquis measured by BLEU. Models denoted by the same symbol (\(\) and \(\)) have the same parameters and only differ in memory as input. The bolded numbers show the SOTA performance and the underlined numbers show the second-best result. \(\) denotes the system is significantly better than baselines with _p-value_ < 0.05 tested by .

    & &  &  \\   & & memory & hypothesis & memory & hypothesis \\   & \(\) & 38.89 & 58.58 & 57.92 & 60.11 \\  & \(\) & 42.56 & 64.40 & 64.32 & 65.65 \\   & \(\) & 40.67 & 64.12 & 63.57 & 65.94 \\  & \(\) & 43.05 & 67.32 & 67.78 & 68.80 \\   

Table 3: Comparison between retrieval memory and self-memory. The quality of memory and hypothesis is measured by the n-gram overlap with reference (BLEU). All experiments are conducted with Transformer\({}_{}\) on JRC-Acquis.

size of the model increases. Furthermore, we find that more similar translation demonstrations significantly enhance performance across all model sizes (from random, kNN to Self). This suggests that demonstration examples in in-context learning not only act as triggers for model ability but also adhere to the _primal problem_, where better demonstration example leads to better generation. Also, by comparing the results in Table 2 and Table 4, we can conclude that the cross-lingual LLM with designed examples still falls short of the supervised baselines in this task.

### Summarization

In this paper, we compare the performance of our trainable model with those of REINA , PEGASUS , and BART . The results are presented in Table5. Initially, it can be observed that memory has varying impacts on different datasets. The enhancement brought by memory in the BigPatent dataset is significantly larger than that in the XSum dataset. This can be attributed to the inherent characteristics of the BigPatent dataset, which consists of official patent documents that exhibit considerable similarity. Consequently, this greatly improves the summarization quality in accordance with the _primal problem_. Furthermore, we discovered that self-memory substantially enhances the performance of both BRIO (+1.2 R1) and BART (+18.5 R1), achieving state-of-the-art results on both datasets. We selected these baselines for a fair comparison, as they share the same base generator. Due to space constraints, additional comparisons and the confidence region of the SOTA model can be found in the Appendix E.

### Dialogue Generation

As demonstrated in Table 6, the self-memory significantly enhances the performance of the retrieval-augmented generator for dialogue generation tasks. By optimizing memory using BLEU as \((,)\), the self-memory improves the B-1,2 score over retrieved memory by 3.08 B-1 and 0.6 B-2 on BARTjoint. Intriguingly, although Selfmem surpasses the baselines in terms of B-1/2, it falls behind in D-1 and D-2, which can be attributed to the trade-off between BLEU score and Distinct score when evaluating a dialogue system . To address this issue, we opt for D-1,2 as \((,)\) when optimizing \(S_{}\), denoted as BARTjoint(D). The results in Table 6 highlight the remarkable flexibility of Selfmem by directly optimizing memory to achieve the desired attributes for diverse and informative dialogue.

## 6 Further Analysis

To gain a deeper insight into Selfmem, we first examine the impact of each key component, namely \(G_{}\) and \(S_{}\). Subsequently, we perform a detailed token-level analysis of the generated output concerning their frequency in the training set. Experiments are conducted on the JRC-Acquis En\(\)De dataset. We also include latency analysis and human evaluation on Appendix F and G.

Tuning \(S_{}\)We explored various \(S_{}\) by direct selection from the candidate pool based on gold rankings. As shown in Figure 2(a), both architectures with enhanced \(S_{}\) significantly outperform the current SOTA performance (60.11 BLEU). Moreover, we assessed the candidate pool quality during this iterative process using an oracle \(S_{}\), as displayed in Figure 2(b). A clear pattern emerges

  
**System** & **Memory** & **R-1** & **R-2** & **R-L** \\   \\  PEGASUS & None & 47.2 & 24.6 & 39.3 \\ BRIO & None & 49.1 & 25.6 & 40.4 \\ REINA (PG) & Retrieval & 48.2 & 26.0 & 40.2 \\ REINA (B) & Retrieval & 43.2 & 21.0 & 35.5 \\ REINA (L) & Retrieval & 46.5 & 24.1 & 38.6 \\ BRIOdual\({}^{}\) & Retrieval & 48.6 & 26.1 & 40.6 \\ BRIOjoint\({}^{}\) & Retrieval & 49.5 & 26.5 & 41.2 \\ BRIOdual\({}^{}\) & Self & 49.2 & 26.2 & 40.8 \\ BRIOjoint\({}^{}\) & Self & **50.3** & **26.7** & **41.6** \\   

Table 5: Results of summarization task on XSum and BigPatent measured by ROUGE.

in this boxplot, revealing improvements in the _oracle_, _quartile_, _average_, and _minimum_ scores of the candidate pool. These two experiments jointly clarify the Selfmem's underlying intuition: a retrieval-augmented generator profits from superior memory, which can be chosen from its own unbounded output, and subsequently, the generator with improved memory produces a higher-quality candidate pool for the next selection round. Consequently, the model lift itself up.

Tuning \(G_{}\)As discussed in SS3.1, we demonstrated that a trained retrieval-augmented generator, with fixed parameters, possesses the ability to distinguish between "good" and "bad" memory. This observation not only justifies our decision to maintain a fixed generator within our framework but also implies that the \(G_{}\) is not the current bottleneck of the Selfmem.

Frequency AnalysisWe conduct a comprehensive token-level analysis by computing the 1-gram F1 scores for generated translations and subsequently categorizing the tokens based on their frequency in the training set. The results are depicted in Figure 4. A noticeable pattern emerges, suggesting that the more frequently a model encounters a token during training, the higher the accuracy of the generated output . Moreover, our findings indicate that retrieval-augmented models, particularly those incorporating self-memory augmentation, exhibit superior performance in handling long-tail inputs which are challenges for parametric models [67; 57].

  
**System** & **Memory** & **B-1** & **B-2** & **D-1** & **D-2** \\  NCM  & None & 33.60 & 26.80 & 3.00 & 12.80 \\ iVAE  & None & 30.90 & 24.90 & 2.90 & 25.00 \\ PLATO-2  & None & 34.80 & 25.12 & 3.54 & 25.11 \\ DialoFlow  & None & 36.17 & 27.67 & 4.56 & 27.12 \\  BART & None & 20.72 & 11.36 & 3.92 & 19.44 \\ BART\({}_{}\) & Retrieval & 29.50 & 21.89 & 4.74 & 26.01 \\ BART\({}_{}\) & Retrieval & 36.72 & 31.55 & 6.13 & 35.65 \\ BART\({}_{}\) & Self & 33.43 & 22.85 & 4.66 & 26.16 \\ BART\({}_{}\) & Self & **39.80** & **32.15** & 5.84 & 32.16 \\  BART\({}_{}\) (D) & Self & 36.92 & 32.09 & **9.12** & **37.05** \\   

Table 6: Results of dialogue generation task on DailyDialog measured by B-1/2 and D-1/2. BARTjoint (D) denotes the metric \((,)\) for \(S_{}\) is the average of D-1 and D-2.

Figure 4: 1-gram F1 score sorted by training corpus frequency.

Figure 3: (a) shows generation quality in the iteration process with different \(S_{}\) in both trainable generator architectures. (b) shows candidates quality in the iteration process with an oracle \(S_{}\).

Conclusion

For the first time, we investigate the fundamental limitation of bounded memory in the current retrieval-augmented literature. We combine the _primal_ and _dual problems_ together and propose Selfmem, a general framework for retrieval-augmented text generation by uplifting generation model with its own output. We conduct comprehensive experiments across various text generation tasks and different generation paradigms, including trainable small model and few-shot prompted LLM. We surpass strong baselines and improve the state-of-the-art performance in serval datasets. We also meticulously investigate each crucial component and pinpoint the existing system bottleneck to guide future research endeavors.

## Limitations

We discuss the limitations of our framework as follows:

(1) Although Selfmem greatly improves the generation quality compared with other retrieval-augmented generation models, it requires more computational resources with respect to the memory selection process. For large dataset with long context (e.g., BigPatent), it would become a more crucial problem considering the quadratic time complexity of transformer architecture.

(2) This paper proposes a general idea for the retrieval-augmented generation. But we only experiment with transformer-based architecture for both generator and memory selector and the architecture of generator and memory selector keeps the same across all text generation tasks. We believe the task-specific design for the model architecture, training objective and generation methods in different text generation scenarios would further improve the performance.