# Explaining the Uncertain:

Stochastic Shapley Values for Gaussian Process Models

 Siu Lun Chau

CISPA

Helmholtz Center for Information Security

66123 Saarbrucken, Germany

siu-lun.chau@cispa.de

&Krikamol Muandet

CISPA

Helmholtz Center for Information Security

66123 Saarbrucken, Germany

muandet@cispa.de

&Dino Sejdinovic

School of Computer and Mathematical Sciences

University of Adelaide

Adelaide, South Australia, 5005 Australia

dino.sejdinovic@adelaide.edu.au

Equal contribution.

###### Abstract

We present a novel approach for explaining Gaussian processes (GPs) that can utilize the full analytical covariance structure present in GPs. Our method is based on the popular solution concept of Shapley values extended to stochastic cooperative games, resulting in explanations that are random variables. The GP explanations generated using our approach satisfy similar favorable axioms to standard Shapley values and possess a tractable covariance function across features and data observations. This covariance allows for quantifying explanation uncertainties and studying the statistical dependencies between explanations. We further extend our framework to the problem of _predictive explanation_, and propose a _Shapley prior_ over the explanation function to predict Shapley values for new data based on previously computed ones. Our illustrations demonstrate the effectiveness of the proposed approach.

## 1 Introduction

Shapley values , a solution concept derived from cooperative game theory, are an essential tool in explainable AI that helps understand and interpret the prediction of complex machine learning models. In particular, **SH**a**pley-**A**dditive-ex**P**lanation (SHAP)  algorithms have been applied in various fields, including finance , healthcare , and robotics . SHAP algorithms compute Shapley values by formulating cooperative game payoffs based on the function obtained from the learning algorithm, e.g., a regressor. This function is often treated as fixed, resulting in deterministic game payoffs. In contrast, we argue that uncertainty in model predictions also plays an equally important role for trustworthy machine learning models as it enables users to make more informed decisions , assess the model's confidence , and identify areas where more data or improved features are needed . Since uncertainty in model prediction can propagate to downstream explanation, explainability tools should also help users better understand the model's confidence or lack thereof in its predictions, allowing them to calibrate their trust in the explanations. By providing transparency into both the model's predictions and uncertainty, AI systems can become more reliable, interpretable, and trustworthy, facilitating broader adoption across domains where trust is imperative.

Gaussian processes (GPs)  are a natural model class for developing an explanation method that can account for predictive uncertainty. While there exist model-specific explanation methods for popular models such as LinearSHAP  for linear models, TreeSHAP  for trees, DeepSHAP  for deep networks, and RKHS-SHAP  for kernel methods, there is a lack of GP-specific Shapley value based explanation methods despite the ubiquity of GPs in machine learning. While feature importance can be studied via automatic relevance determination (ARD) lengthscales  of the covariance kernel, they only reveal global notions of importance which are not associated to the specific predictions, and are limited to the specific classes of kernels. Yoshikawa and Iwata  have proposed using a GP with a local linear regression component for interpretability, but this approach results in a specific interpretable GP model rather than a general GP explanation algorithm. Recently, Marx et al.  proposed to sample multiple realisations of the GP function and to apply a model-agnostic explanation algorithm to each sample, thus getting a distribution over explanations. While this in principle can give uncertainties in explanations, it is computationally expensive and not tailored for GPs. It does not take advantage of the various important properties that GPs enjoy, such as the fact that posterior means are reproducing kernel Hilbert space (RKHS)  functions and that we have a fully tractable covariance structure. These properties, as we later demonstrate, can lead to more effective estimation of Shapley values and help to understand the statistical dependencies across these explanations. Additionally, this covariance structure allows us to formulate explanations themselves as GPs and thus are able to predict explanations of unseen data.

To address this gap, we present the GP-SHAP algorithm. In Section 2, we contextualize our work by introducing the concepts of stochastic cooperative games and stochastic Shapley values. Both are studied thoroughly by game theorists [16; 17], but have not been utilized by the explanation community. Section 3 characterizes the stochastic cooperative game induced by GPs through the use of the conditional mean process [18; 8], a GP of conditional expectations. We use the weighted least squares formulation of Shapley values [19; 2] to express explanations as multivariate Gaussians with a tractable covariance structure. We then introduce the GP-SHAP algorithm as a method for estimating these stochastic Shapley values. In addition, we extend the GP-SHAP algorithm by combining it with the Bayesian weighted least squares approach by Slack et al. . The extension we call BayesGP-SHAP integrates two sources of uncertainty: GP predictive posterior uncertainty as well as the Shapley value estimation uncertainty (in the cases where Shapley values are too expensive to compute exactly). Section 4 expands beyond GP explanations, demonstrating the applicability of our framework to other predictive models. We consider the setting of predictive explanations and propose a _Shapley prior_ to represent explanations for more general predictive models as GPs. This allows us to predict explanations for new data without relying on the standard Shapley value procedure and provide predictive uncertainty to more general explanations beyond GP models, such as explanations from TreeSHAP, DeepSHAP, and KernelSHAP. Illustrations of the effectiveness of GP-SHAP, BayesGP-SHAP, and the Shapley prior for predictive explanations are provided in Section 5 and we conclude the paper in Section 6. All proofs in this paper are given in the appendix.

## 2 Stochastic cooperative games and their Shapley values

In this section, we review the concepts of stochastic cooperative games, denoted as s-games, and their corresponding stochastic Shapley values, referred to as SSVs. These concepts provide the necessary language to introduce GP-SHAP in the coming section. It is important to note that game theorists have studied s-games in various forms [16; 17; 21], but they have not been adequately introduced in the explanation literature. While Covert and Lee  briefly discussed s-games, they focused on computing deterministic Shapley values (DSVs) of the mean of the s-game, overlooking the inherent uncertainty in s-games and their potential application to uncertainty-aware explanations.

Problem formulation.The results presented below are analogous to the original work of Shapley's , but adapted to games with random outcomes. Formally, let \(\) denote the player set and let \(:2^{}_{2}()\) be a s-game with random variable payoff. We restrict our attention to stochastic payoffs with finite second moments as we aim to characterise the variance of the corresponding stochastic Shapley values. However, this is not a necessary condition to show existence and uniqueness of the SSVs. We further require \(()=_{_{0}}\) where \(_{_{0}}\) is the Dirac measure at some constant \(_{0}\). We introduce the following concepts to formalise the axioms for SSVs:

**Definition 1** (Carrier).: _A carrier of \(\) is any \(N\) such that \((S)=(N S)\) for all \(S\)._

**Definition 2** (Permutation s-game).: _Denote \(()\) the set of permutations on \(\). For \(()\), the induced permutation s-game is \(_{}( S):=(S)\) for all \(S\), where \( S\) is the image of \(S\) under \(\)._

**Definition 3** (Stochastic value allocation).: _Denote \(\) the space of s-games for \(\). We say \(:_{2}()^{||}\) is a value allocation of \(\) that assigns to each player \(i\) a stochastic value \(_{i}()\)._

The natural extension of the original axioms by Shapley are extended to s-games as follows:

1. **(s-symmetry)** For any \((),_{ i}(_{})=_{i}()\).
2. **(s-efficiency)** For each carrier \(N\) of \(\), \(_{i N}_{i}()=(N)\).
3. **(s-linearity)** For any \(,\), \((+)=()+()\) where addition of s-games are defined as \((+)(S):=(S)+(S)\) for all \(S\).

Note that all equality happens at the random variable level. Unsurprisingly, there is a unique stochastic value allocation that satisfies these three axioms:

**Theorem 4** (Stochastic Shapley values).: _The only stochastic value allocation \(\) of \(\) satisfying s-symmetry, s-efficiency, and s-linearity takes the following form,_

\[_{i}()=_{S N\{i\}}c_{|S|}((S i)-( S))\] (1)

_where \(N\) is the smallest carrier set of \(\), \(c_{|S|}=^{-1}\) and \(_{i}()\) is the \(i^{th}\)SSV of s-game \(\)._

Note that Equation (1) is equivalent to the DSVs, except it is written in terms of random variables. We emphasise that this result has been proven in Ma et al.  using a top-down approach, i.e. starting with Equation (1) and verify it satisfies the stochastic axioms and uniqueness. In the appendix, we offer a contrasting perspective where we mirror Shapley's original bottom-up derivation, i.e. began with the stochastic axioms and subsequently determined the unique solution.

### Variances of stochastic Shapley values are not Shapley values of variance games

As (1) is now defined by summing over a weighted differences between random variables, we can analyse the corresponding mean and variances across the SSVs. In the following, denote \(:}^{||}\) as the deterministic Shapley value allocation where \(}:=\{:\}\) is the space of deterministic cooperative games, referred to as d-games from now on.

**Proposition 5**.: _Given the player set \(\), let \(\) be a stochastic game, \(\) a stochastic Shapley value allocation, and \(\) a deterministic Shapley value allocation. Suppose that \([]\) and \([]\) are the corresponding mean and variance d-games, respectively. Then, \([()]=([])\), but \([()]([])\). In particular, the SSV variance is given by_

\[[_{i}()]=_{S N\{i\}}_{S^{}  N\{i\}}c_{|S|^{C}|S^{}|}[_{S  i},_{S^{} i}]-[_{S i},_{S^{}}]- [_{S},_{S^{} i}]+[_{S},_{S^{}} ],\]

_where \(_{S}=(S)\) and \(\) is the covariance function between the stochastic payoffs._

Proposition 5 highlights the difference between variances of stochastic Shapley values, which capture the propagated uncertainties through the s-game, and deterministic Shapley values of variance games , i.e. deterministic game \((S)=[(S)]\) for all \(S\) for some stochastic game \(\). Therefore, in the context of model explanations, variance-based allocation techniques such as the Shapley effects  and the work of Fryer et al. , where they computed Shapley values on games constructed using the variance and coefficient of determination \(R^{2}\) respectively, cannot be used to capture uncertainty of explanations, as those approaches are themselves explaining the model variance instead. Besides stochastic Shapley values, it is possible to leverage the framework of coalition interval games  and interval Shapley values  to provide confidence intervals for feature explanations, as Napolitano et al.  demonstrated. However, their approach is based on confidence interval of predictive models, thus this frequentist approach is not applicable to Bayesian models such as GPs and the obtained uncertainty around explanations have very different interpretations as well.

At first glance, computing the variance of each stochastic Shapley value seems cumbersome as it requires summing over all possible coalitions twice. However, there exists a compact expression for the full covariance matrix of stochastic Shapley values across players for stochastic games constructed using a Gaussian process model, which we demonstrate in the following section.

Explaining GPs with GP-SHAP

This section is dedicated to introducing GP-SHAP, where the key idea is about efficiently computing stochastic Shapley values for stochastic games that models conditional expectations of Gaussian processes. Specifically, in Section 3.1, we review the process of constructing Shapley values for model explanations using deterministic cooperative games. We then proceed to formalize the stochastic cooperative game induced by pushing the GP functions through the d-game and demonstrate that the resulting stochastic Shapley values follow a multivariate Gaussian distribution. Section 3.2 is dedicated to the estimation procedure and the introduction of two algorithms for computing GP explanations: GP-SHAP and BayesGP-SHAP. GP-SHAP provides explanations that incorporate the GP predictive uncertainty, while BayesGP-SHAP extends this further by accounting for the additional estimation uncertainty arising in the weighted least squares procedure. This extension is similar to how BayesSHAP introduced by Slack et al.  extends SHAP.

### Formulating stochastic Shapley values for GP models

Let \(X,Y\) be random variables taking values in the \(d\)-dimensional instance space \(^{d}\) and label space \(\) (could be in \(\) or discrete) respectively with the joint distribution \(p(X,Y)\). We use \([d]:=\{1,...,d\}\) to denote feature index set and \(S[d]\) as the feature index subset. For supervised learning with GPs, the usual goal is to model \(P(Y X=x)=(f())\) where \(f\) is the GP function of interest and \(\) is a problem-specific transformation, e.g., \(\) is the identity map for regression and sigmoid transformation for classification problem. We then posit a prior \(f(0,k)\) where \(k:\) is a covariance kernel, and compute the posterior distribution (or its approximation) \(p(f)\), which is typically also a GP with posterior mean \(\) and kernel \(\).

Explaining deterministic \(f\) with d-game.To explain a deterministic function \(f\), we create a d-game by treating each feature as a player and constructing the game with \(f\). The resulting DSVs then provide feature attributions that satisfy various favourable properties, such as the sum of the attribution equals to the prediction made at that specific point, identical features receive the same attribution, and null feature receives zero attribution . The d-game for local attribution on an observation \(\) typically involves computing the expected value of \(f(X)\) using a reference distribution \(r(X X_{S}=_{S})\) for different feature subsets \(S[d]\),

\[_{f}(,S):=_{r}[f(X) X_{S}=_{S}].\] (2)

This approach, also known as removal-based explanation , has been used in various explanation algorithms [2; 11; 12; 31]. These d-games determine the "worth" of features in \(S\) by looking at the expectation after "removing" the contribution of other features in \([d] S\) through integration. The choice of reference distribution leads to explanations with different properties, such as improved locality of estimation , promotion of fairness , or the incorporation of causal knowledge [34; 35]. In this paper, we focus on the scenario where \(r\) corresponds to the data distribution, i.e., \(r(X X_{S})=p(X X_{S})\), as it is one of the most frequently used approaches in the literature [36; 37; 12; 31]. Although different reference functions result in different estimation procedures, our formulation of s-games and SSVs for GPs still holds.

Induced s-game by stochastic \(f\).Now suppose \(f\) is a random function with a posterior distribution \(p(f)\). The corresponding stochastic cooperative game with players \([d]\) can be defined analogously to the deterministic case as \(_{p(f)}: 2^{[d]}_{2}( )\) such that, for a given observation \(\) and feature subset \(S[d]\), the stochastic payoff is

\[_{p(f)}(,S)=_{X}[f(X) X_{S}= _{S}].\] (3)

For brevity, we write \(_{p(f)}\) as \(_{f}\), bearing in mind that \(_{f}\) is a random function since \(f\) is a random function. In particular, when \(f\) is a GP with mean function \(\) and covariance function \(\), the stochastic payoff \(_{f}\) is also a GP indexed by \(\) and \(S\).

**Proposition 6** (Stochastic game \(_{f}\) as induced GP).: _Let \(f(,)\) with integrable sample paths, i.e. \(_{}|f|dp_{X}<\) almost surely. The stochastic payoff function \(_{f}\) induced by \(f\) is a Gaussian process with the following mean and covariance functions:_

\[m_{}(,S) :=_{X}[(X) X_{S}=_{S}],\] (4) \[k_{}((,S),(^{},S^{})) :=_{X,X^{}}[(X,X^{}) X_{S }=_{S},X^{}_{S^{}}=^{}_{S^{}} ].\] (5)In other words, the stochastic game \(_{f}\) now assigns a payoff distribution to each feature subset \(S\), where the variability arise from taking conditional expectation on a random function \(f\). We note that this GP is also known as the conditional mean process, previously studied in Chau et al. [8; 18].

Stochastic Shapley values as multivariate Gaussians for GPs.Given the s-game \(_{f}\), we can characterise the corresponding SSVs using the weighted least squares formulation of DSVs [19; 2]. For each coalition \(S_{j}[d]\), let \(z_{j}\{0,1\}^{d}\) be the binary vector such that \(z_{j}[i]=1\) if \(i S_{j}\) and \(\{0,1\}^{2^{d} d}\) the concatenation of all \(z_{j}\) vectors. When the context is clear, we use \(\) and \([d]\) interchangeably. Let \(^{2^{d} 2^{d}}\) be the diagonal matrix with entries \(W_{jj}=w(S_{j})=|})|S_{j}|(d-|S_{j}|)}\) for all subsets with size \(0<|S_{j}|<d\). When \(S_{j}\) has 0 or \(d\) elements, the weights are set to \(\) to enforce the efficiency axiom.

**Theorem 7** (Stochastic Shapley values of \(_{f}\)).: _Let \(_{f}\) be an induced stochastic game from the GP \(f(,)\) and denote \(_{}:=[_{f}(,S_{1}),_{f}(, S_{2^{d}})]^{}\) the vector of stochastic payoffs across all coalitions, then the corresponding stochastic Shapley values \((_{f}(,))\) follows a \(d\)-dimensional multivariate Gaussian distribution,_

\[(_{f}(,))([_{ }],[_{}]^{}) :=(^{})^{-1} ^{},\] (6)

_where \([_{x}]^{2^{d}}\) and \([_{x}]^{2^{d} 2^{d}}\) are the corresponding mean vector and covariance matrix of the payoffs._

The derivation is straightforward using the fact that the stochastic payoffs \(_{}\) are multivariate Gaussian random variables and the matrix \(\) is obtained from the weighted regression formulation of Shapley values. The resulting variance of \((_{f}(,))\) can then be interpreted as the uncertainties around the explanations, propagated from the posterior variance of \(f\).

### Estimation algorithms: the GP-SHAP and the BayesGP-SHAP

In this section, we introduce our main algorithm GP-SHAP and its variant BayesGP-SHAP to estimate the stochastic Shapley values for GP posterior distributions.

Formulating GP-SHAP.Given a set of observations \(=\{(_{i},y_{i})\}_{i=1}^{n}=(,)\) with GP prior \(f(0,k)\), we can obtain the posterior GP \(f(,)\) by using the Gaussian conditioning rule for regression or e.g. the variational or Laplace approximation for classification. In fact, the conditional expectations of mean and covariance function from a posterior GP can be estimated using conditional mean embeddings  without the need of explicit density estimations, following derivations of Chau et al. [8; 18]. We provide technical discussion on conditional mean embeddings in the appendix.

**Proposition 8** (Estimating \(_{f}\)).: _Given \(=(,)\) and the posterior GP \(f(,)\), the mean and covariance function of the stochastic cooperative game \(_{f}\) can be estimated as,_

\[_{}(,S)=(,S)^{}( ),_{}((,S),(^{},S^{ }))=(,S)^{}}_{ }(^{},S^{}),\] (7)

_where \((,S):=(_{_{S}_{S}}+ I )^{-1}k_{S}(_{S},_{S})\), \(()=[(_{1}),,( _{n})]^{}\), and \(k_{S}:_{S}_{S}\) is the kernel defined on the sub-feature space of \(\) and we write \(k_{S}(_{S},_{S}):=[k_{S}(_{S},_{1S}),...,k_{S}(_{S},_{nS})]\) and \(_{}\) and \(}_{}\) as the gram matrix of \(\) using kernel \(k\) and \(\) respectively. The parameter \(>0\) is a fixed hyperparameter to stabilise the inversion._

It is worth noting that the estimation \(_{}\) coincides with the one deployed in RKHS-SHAP  since we are also utilising conditional mean embeddings for the estimation of conditional expectation of RKHS functions. In addition, we obtain the analytical covariance function that is used to estimate the covariance of the stochastic Shapley values in the following proposition:

**Proposition 9** (Gp-Shap).: _Let the matrix \(\) be defined as in Theorem 7. The mean and covariance for the multivariate stochastic Shapley values can be estimated as,_

\[(_{f}(,))=(( ,[d])^{}(),(,[d])^{ }}_{}(,[d]) ^{})\] (8)

_where \((,[d])=[(,[d]_{1}),,( ,[d]_{2^{d}})]^{}\)._The complete algorithm, along with a discussion of computational techniques deployed to reduce computation time, such as vectorisation across \(\) using tensor operations and incorporation of the sparse GP formulation to speed up computations of quantities from Propositions 8 and 9, is provided in the appendix. It is worth noting that subsampling coalitions to reduce computational cost while estimating Shapley values is also a standard approach in SHAP algorithm implementations . Empirically, this procedure has been shown to give an unbiased estimate of the true DSVs, with the variance decreasing at a rate of \(()\), where \(\) is the number of coalition samples . However, this approach incurs an additional source of uncertainty, due to estimation, and we propose to incorporate that as well into our framework by utilising the BayesSHAP approach proposed by Slack et al. .

**Formulating BayesGP-SHAP.** To capture the estimation uncertainty, Slack et al.  proposed BayesSHAP and reformulated the WLS procedure as a hierarchical Bayesian weighted least square and studied the corresponding posterior distribution over the deterministic Shapley values. We provide the hierarchical data generation process below, with an abuse of notations, **for a deterministic**\(f\), we have

\[ z,,,f,^{ }z+   z(0,^{2}w(z)^{-1})\] (9) \[^{2}(0,^{2}I) ^{2}^{2}(_{0},_{0}^{2})\] (10)

where \(w\) and \(z\) are the weight function and binary vector respectively introduced in Theorem 7, and \(_{0}\), \(_{0}^{2}\) are two hyperparameters, typically set to small values to keep priors uninformative. Slack et al.  showed that the posterior distribution on \(^{2}\) and \(\) follows a scaled \(^{2}\) and normal respectively, due to their corresponding conjugacies with the likelihood.

**Proposition 10** (BayesSHAP ).: _Given the data generation above, the posterior distribution on \(\) and \(^{2}\) follows:_

\[^{2},_{},f,, (_{}}_{},(_{ }^{}_{}_{})^{-1}^{2})\] (11) \[^{2}_{},f,, ^{2}(_{0}+, _{0}^{2}+ s^{2}(}_{})}{_{0}+})\] (12)

_where \(\) is the number of coalitions \(}=\{S_{j}\}_{j=1}^{}\) we sample uniformly from \(2^{[d]}\), \(_{}\) is the binary matrix representing \(}\), and \(_{}\) is the corresponding weight matrix, and \(_{}=(_{}^{}_{}_{ })^{-1}_{}^{}_{}\) is the WLS matrix, \(}_{}=[_{f}(,S_{1}),...,_ {f}(,S_{})]^{}\) is the vector of deterministic payoffs, and_

\[s^{2}(}_{})=[(}_{ }-_{}_{}}_{})^ {}W_{}(}_{}-_{}_{ }}_{})+(_{}}_{ })^{}(_{}}_{})]\] (13)

_measures the average weighted error in the regression and the norm of the mean explanations._

It is important to highlight that while both GP-SHAP and BayesSHAP have a notion of the "posterior of Shapley values", the two sources of uncertainty which these approaches capture are very different: GP-SHAP corresponds to the predictive uncertainty induced by the GP posterior \(p(f)\), whereas BayesSHAP is the uncertainty due to having to estimate Shapley values when their exact computation is infeasible due to the exponential number of coalitions - and is in Slack et al.  proposed for a deterministic \(f\). Nonetheless, by integrating the BayesSHAP posterior of Shapley values through the posterior GP \(p(f)\), we can in fact incorporate both sources of uncertainty, leading to our second algorithm, BayesGP-SHAP.

**Proposition 11** (**BayesGP-SHAP**).: _Continuing from Propositions 9 and 10, the posterior distribution of the stochastic Shapley values can be estimated using the Bayesian WLS approach as,_

\[^{2},_{},, (_{}(,}))^ {}\!(),_{}(,})^{}}_{}(, })_{}^{}+(_{}^{} _{}_{})^{-1}^{2})\]

_where \(^{2}\) is sampled from \(^{2}_{}^{2}(_{0}+ ,_{0}^{2}+ s^{2}(}[_{ }])}{_{0}+})\)._

We note that in the above proposition, instead of integrating \(p(^{2}_{},f,,)\) with respect to the posterior GP, which leads to a complex scaled mixture of Gaussians, we simplify the expression and construct a scaled inverse chi-square distribution with \(s^{2}([_{x}])\) instead, which represents the error of the weighted regression with respect to the mean payoffs \([_{x}]\).

Conditionally on \(^{2}\), the posterior variance in BayesGP-SHAP is therefore the sum of the variance from GP-SHAP and BayesSHAP due to Gaussian conjugacies.

Predictive explanations using the Shapley prior

In this section, we move beyond standard GP explanations by formulating explanation functions of a broader class of models as Gaussian processes. We introduce a _Shapley prior_ over the space of vector-valued explanation functions \(:^{d}\), which correspond to Shapley values for arbitrary functions \(f\). By treating previously obtained Shapley values as regression labels, we can predict explanations for new data without relying on the standard procedure to compute Shapley values. Our framework is not limited to explanations generated from GP-SHAP, but can also be applied to other explanation methods such as TreeSHAP, DeepSHAP, or model-agnostic KernelSHAP. The proposed approach learns the direct mapping from \(\) to the space of Shapley values, without the need to access the underlying model \(f\) during training, which is different from previous predictive approaches such as FastSHAP . In contrast to prior work, Hill et al.  also considered fitting a GP regression model directly to explanation functions, but their focus is on incorporating the corresponding predictive uncertainty to explanations from black-box classifiers rather than predicting Shapley values for unseen data.

To achieve this, we leverage the key ideas behind the GP-SHAP algorithm, which uses the facts that GPs remain tractable under conditional expectations (to build the s-game) and linear combinations (to compute the stochastic Shapley values). By applying these properties to a GP prior \((0,k)\) instead of a posterior, we have an induced prior over the space of Shapley functions \(\).

**Proposition 12** (The Shapley prior over \(\)).: _The prior \(f(0,k)\) and the corresponding stochastic game \(_{f}(,S)=[f(X) X_{S}=_{S}]\) induce a vector-valued GP prior over the explanation functions \((0,)\) where \(:^{d d}\) is the matrix-valued covariance kernel_

\[(,^{})=()^{}(^{}),()=()^{}\] (14)

_where \(()=[k(,X) X_{S_{1}}=x_{S_{1}}],, [k(,X) X_{S_{2^{d}}}=x_{S_{2^{d}}}]\), and the \({}^{}\) sign refers to taking inner products in the RKHS of \(k\)._

By utilizing the Shapley function prior, we can apply our approach to predict explanations for a wide range of models, including trees, deep neural networks, or RKHS functions, by treating their explanations as noisy samples from this prior and using them as regression labels. Our approach is based on the perspective that there exists a true data generating mechanism \(f_{*}:\) and that any model \(f\) we use is an approximation of \(f_{*}\). Therefore, any explanations derived from \(f\) can be seen as an attempt to reveal the true explanations under \(f_{*}\). This perspective has also been adopted in the work of Chen et al.  and Marx et al. . We present the predictive posterior below.

**Proposition 13** (Predictive explanations as multi-output GPs).: _Given \(_{}=\{(_{i},_{i})\}_{i=1}^{n}=( ,})\) where \(_{i}^{d}\) are the Shapley values computed under predictive model \(f\) and \(}}=[_{1},...,_{ n}]^{}\), the predictive explanations for new data \(^{}\) is distributed as,_

\[(^{})_{}(_ {}(^{}),(^{},^{ })-(^{},)b_{}(^{},))\] (15)

_where \(_{}(^{})=b_{}(^{},)^{}(})\), \(b_{}(^{},):=(_{ }+_{}^{2}I)^{-1}(,^{})\), \(_{}\) is the gram matrix for kernel \(\) of size \(nd nd\), \((^{},)=[(^{},_{1}),,(^{},_{n})]\) is of size \(d nd\) and \(_{}^{2}\) is the noise parameter for regression._

In fact, we see that the posterior mean \(_{}(^{})\) from (15) are Shapley values of the following payoff vector \(}_{^{}}\), computed based on the observed explanations \(}}\),

**Proposition 14** (Posterior mean as Shapley values for payoff vector \(}_{^{}}\)).: _The posterior mean \(_{}(^{})\) corresponds to Shapley values for the payoff vector \(}_{^{}}\), i.e., \(_{}(^{})=}_{^{}}\), where \(}_{^{}}=_{i=1}^{n}(^{})^{ }(_{i})^{}_{i}\) and \(_{i}^{d}\) is the \([i,...,i+(d-1)]\) subvector of \((_{}+_{}^{2}I)^{-1}(}})\)._

## 5 Illustrations

We demonstrate the proposed approaches through three sets of illustrations. We first conduct an ablation study to examine how predictive and estimation uncertainty captured by our explanation algorithms vary under different configurations. We then move on to discuss various exploratory tools that can assist practitioners in utilizing the stochastic explanations. Finally, we demonstrate the effectiveness of the Shapley prior for the predictive explanations problem. Our code is included in the supplementary material, and we provide implementation details in the appendix. For all experiments, we pick the radial basis function (RBF) as our covariance kernel \(k\) for the GPs.

### Ablation study on different notions of uncertainties captured

We conducted a comparison between GP-SHAP, BayesSHAP, and BayesGP-SHAP to demonstrate the differences between model predictive uncertainty and estimation uncertainty captured in the stochastic Shapley values. BayesSHAP is applied to the predictive posterior mean of GP to obtain its explanation. For this purpose, we used the California housing dataset  from the StatLib repository, which includes \(20640\) instances and \(8\) numerical features, with the goal of predicting the median house value for California districts, expressed in hundreds of thousands of dollars. We trained our GP model using \(25\%\) and \(100\%\) of the data and calculated local stochastic explanations from GP-SHAP, BayesSHAP, and BayesGP-SHAP using \(50\%\) and \(100\%\) of coalitions. The results, shown in Figure 1, demonstrate that the magnitude of BayesSHAP uncertainties (green bars) are uniform across features as it is designed to capture the overall estimation performance and not feature-specific uncertainty. In contrast, GP-SHAP (red bars) exhibits varying uncertainties across features due to the propagation of variation from the posterior GP \(f\) to the attributions. This allows practitioners to make more granular statements about the uncertainty around specific feature explanations. We also observe that increasing the number of coalitions and training data generally leads to a decrease in both estimation and predictive uncertainties, but the estimation uncertainty drops more significantly. BayesGP-SHAP (yellow bars) consistently provides a more conservative uncertainty estimation by considering both predictive and estimation uncertainties.

### Exploratory analysis of the stochastic explanations

We propose several exploratory analysis methods to aid practitioners in comprehending the stochastic explanations generated for their downstream tasks. To this end, we use BayesGP-SHAP to explain a Gaussian process model trained on the breast cancer dataset . The dataset consists of \(569\) patients, \(30\) numeric features, and the objective is to predict whether a tumor is malignant or benign.

**Local explanation.** To visualize local explanations, we can plot the mean and standard deviations of stochastic Shapley values in a bar plot. This approach allows us to not only understand the degree of contribution a feature has to the prediction but also the corresponding credible interval as a measure of explanation uncertainty. Figure 1(a) displays that both "worst fractal dimension" and "worst perimeter" features have a similar contribution in terms of their absolute mean stochastic Shapley values. However, the model is much more uncertain about the former, allowing the user to calibrate their trust in model explanations.

Figure 1: Ablation study on different uncertainties captured by GP-SHAP, BayesSHAP, and BayesGP-SHAP when computing local explanations (SSVs) using the California housing dataset . 95% credible intervals around explanations are shown.

**Heuristic global explanation as means of absolute stochastic Shapley values.** It is common to compute the mean absolute DSVs as a proxy to global feature contributions  based on computed local explanations. Although this heuristic does not result in any explanations that are themselves DSVs of any game at the global level, they provide a quick summary to practitioners about the overall contributions. We can similarly compute the average of the absolute stochastic Shapley values, which are now distributed according to a folded multivariate Gaussian distribution  with a tractable covariance structure. However, it is important to highlight a key distinction from the previous approach: when computing the **mean of absolute SSVs**, we consider the uncertainty in the local stochastic explanations, while directly calculating the **absolute values of mean SSVs** (equivalent to computing absolute DSVs) disregards this uncertainty. We believe the former approach is more appropriate than the latter as practitioners utilising Gaussian processes would want their global explanations to take the predictive uncertainty into account. The blue and yellow bars in Figure 1(b) depict these two approaches, respectively. Notably, we observe that the blue bars suggest that the "worst fractal dimension" feature is considered more influential than "worst concave points" because it accounts for the higher variability in the former feature's explanation. In contrast, the yellow bar overlooks this variability, leading to a different conclusion.

**Correlations and dependencies across explanations.** As we have access to the explanation covariance, we can explore their correlation and visualise them as in Figure 1(c). For instance, the stochastic Shapley values of "concavity error" are less correlated with other features. Moreover, as our explanations are multivariate Gaussians, we can build an undirected Gaussian graphical model  using the precision matrix (inverse of the covariance matrix) to study the independence across the local stochastic explanations. We visualise the corresponding graphical model for explanations from patient \(1\) in Figure 1(d) by setting a sparsity threshold of \(90\%\), following the approach in .

### Predictive explanations

Finally, we showcase the effectiveness of our Shapley prior by comparing the regression performances of a multi-output GP using the Shapley prior with multi-output random forest and neural networks when predicting withheld explanations generated from GP-SHAP, Tree-SHAP, and DeepSHAP, respectively. We use the diabetes dataset  from the UCI repository, which contains \(442\) patients with \(10\) numerical features and the goal is to predict disease progression. We first train a GP, a random forest, and a neural network, to obtain the

Figure 2: Illustrations of possible exploratory analyses utilising the BayesGP-SHAP covariance structure, applied on the breast cancer dataset.

subsequent explanations from GP-SHAP, TreeSHAP, and DeepSHAP. Next, we feed \(70\%\) of the explanations to our predictive model as training data and the remaining \(30\%\) as test data.

We repeat this process over \(10\) seeds and compute the root mean squared error between the predicted explanation and the exact explanations for respective models, as shown in Figure 3. We observe that our GP-model with the Shapley prior consistently outperforms random forest and neural network model for predicting all three types of explanations. This demonstrates that the inductive bias that comes from our choice of covariance structure \(\) allows to build more accurate predictive explanation models. We also observe that the overall average prediction error for explanations generated from GP-SHAP is lower than that of TreeSHAP  and DeepSHAP , suggesting that GP-SHAP produce explanations that are easier to learn. This follows our intuition as GPs are typically smoother functions than trees and neural networks.

## 6 Discussion

In this work, we presented a novel and principled approach for explaining Gaussian process models using stochastic Shapley values. The proposed algorithm GP-SHAP, and its variant BayesGP-SHAP, allow practitioners to consider both predictive and estimation uncertainties when reasoning about the explanations and calibrate their trust in the model accordingly. Furthermore, we consider the setting of predictive explanations, where we introduced a Shapley prior over explanation functions, enabling us to model and predict Shapley values for a wider range of predictive models. We demonstrated the effectiveness of our methods through a number of illustrations and discussed various exploratory tools for practitioners to analyse the stochastic explanations.

**Limitations and future outlook.** While the general framework of using stochastic Shapley values for stochastic explanations can be applied beyond Gaussian process models, the specific estimation algorithms presented in this paper are tailored to GPs and may not be directly applicable to other probabilistic models like Bayesian neural networks. These alternative models would require different estimation procedures as the resulting explanations would not be GPs anymore. Another promising avenue for future research involves leveraging the uncertainty obtained from predicting explanations and incorporating it into Bayesian optimization for guiding experimental design. For example, this could allow practitioners to explore data regions that produce significant Shapley values for specific features. At last, we would like to highlight that although SHAP is popular, as an explanation tool it contains several limitations, specifically Kumar et al.  have shown that SHAP violates several expected properties of explanations from a human-centric perspective.