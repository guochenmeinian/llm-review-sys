# Disentangled Wasserstein Autoencoder for T-Cell Receptor Engineering

Tianxiao Li\({}^{1,2}\)1, Hongyu Guo\({}^{3}\), Filippo Grazioli\({}^{4}\), Mark Gerstein\({}^{2}\)2, Martin Renqiang Min\({}^{1}\)\({}^{}\)1

\({}^{1}\) NEC Laboratories America, \({}^{2}\) Yale University

\({}^{3}\) National Research Council Canada, \({}^{4}\) NEC Laboratories Europe

tianxiao.li@yale.edu, hongyu.guo@uottawa.ca, flpgrz@outlook.com

mark.gerstein@yale.edu, renqiang@nec-labs.com

Equal contribution. \({}^{}\)Corresponding author.

###### Abstract

In protein biophysics, the separation between the functionally important residues (forming the active site or binding surface) and those that create the overall structure (the fold) is a well-established and fundamental concept. Identifying and modifying those functional sites is critical for protein engineering but computationally non-trivial, and requires significant domain knowledge. To automate this process from a data-driven perspective, we propose a disentangled Wasserstein autoencoder with an auxiliary classifier, which isolates the function-related patterns from the rest with theoretical guarantees. This enables one-pass protein sequence editing and improves the understanding of the resulting sequences and editing actions involved. To demonstrate its effectiveness, we apply it to T-cell receptors (TCRs), a well-studied structure-function case. We show that our method can be used to alter the function of TCRs without changing the structural backbone, outperforming several competing methods in generation quality and efficiency, and requiring only 10% of the running time needed by baseline models. To our knowledge, this is the first approach that utilizes disentangled representations for TCR engineering.

## 1 Introduction

Decades of work in protein biology have shown the separation of the overall structure and the smaller "functional" site, such as the generic structure versus the active site in enzymes , and the characteristic immunoglobulin fold versus the antigen-binding complementarity-determining region (CDR) in immunoproteins . The latter usually defines the protein's key function, but cannot work on its own without the stabilizing effect of the former. This dichotomy is similar to the content-style separation in computer vision  and natural language processing . For efficient protein engineering, it is often desired that the overall structure is preserved while only the functionally relevant sites are modified. Traditional methods for this task require significant domain knowledge and are usually limited to specific scenarios. Several recent studies make use of deep generative models  or reinforcement learning  to learn from large-scale data the _implicit_ generation and editing policies to alter proteins. Here, we tackle the problem by utilizing _explicit_ functional features through the disentangled representation learning (DRL), where the protein sequence is separately embedded into a "functional" embedding and a "structural" embedding. This approach results in a more interpretable latent space and enables more efficient conditional generation and property manipulation for protein engineering.

DRL has been applied to the separation of "style" and "content" of images , or static and dynamic parts of videos  for tasks such as style transfer  and conditional generation .

Attaining the aforementioned disentangled embeddings in _discrete sequences_ such as protein sequences, however, is challenging because the functional residues can vary greatly across different proteins. To this end, several recent works on discrete sequences such as natural languages use adversarial objectives to achieve disentangled embeddings [14; 15]. Cheng, et al.  improves the disentanglement with a mutual information (MI) upper bound on the embedding space of a variational autoencoder (VAE). However, this approach relies on a complicated implementation of multiple losses that are approximated through various neural networks, and requires finding a dedicated trade-off among them, making the model difficult to train. To address these challenges, we propose a Wasserstein autoencoder (WAE)  framework that achieves disentangled embeddings with a theoretical guarantee, using a simpler loss function. Also, WAE could be trained deterministically, avoiding several practical challenges of VAE in general, especially on sequences [18; 19]. Our approach is proven to simultaneously maximize the mutual information (MI) between the data and the latent embedding space while minimizing the MI between the different parts of the embeddings, through minimizing the Wasserstein loss.

To demonstrate the effectiveness and utility of our method, we apply it to the engineering of T cell receptors (TCRs), which uses a similar structure fold as the immunoglobulin, one of the best-studied protein structures and a good example of separation of structure and functions. TCRs play an important role in the adaptive immune response  by specifically binding to peptide antigens  (Fig. 1A). Designing TCRs with higher affinity to the target peptide is thus of high interest in immunotherapy . Various data-driven methods have been proposed to enhance the accuracy of TCR binding prediction [23; 24; 25; 12; 13; 26; 27]. However, there has been limited research on leveraging machine learning for TCR engineering. A related application is the motif scaffolding problem [28; 29; 30; 31; 32] where a structural "scaffold" is generated supporting a fixed functional "motif". Here our goal is the opposite: to modify the functional parts of the sequence instead by directly introducing mutations to it.

We focus on the CDR3\(\) region of TCRs where there is sufficient data and a clearly defined functional role (peptide binding). Using a large TCR-peptide binding dataset, we empirically demonstrate that our method successfully separates key patterns related to binding ("functional" embedding) from generic structural backbones ("structural" embedding). Due to the lack of 3D structural data, we assume that similarity between the "structure-related" parts of the sequence indicates similarity in the structure. Furthermore, by modifying only the functional embedding, our approach is able to generate new TCRs with desired binding properties while preserving the structural backbone, requiring only 10% of the running time needed by baseline models in comparison. We also note from our TCR engineering results that mutations can be introduced throughout the sequence, which implies that the model learns higher-level functional and structural patterns that could span the whole sequence, instead of looking for a universal clear cut between a "functional segment" and a "structural segment".

We summarize our main contributions as follows:

* To our knowledge, we are the first to formulate computational protein design as a style transfer problem and leverage disentangled embeddings for TCR engineering, resulting in more interpretable and efficient conditional generation and property manipulation.
* We propose a disentangled Wasserstein autoencoder with an auxiliary classifier, which effectively isolates the function-related patterns from the rest with theoretical guarantees.
* We show that by modifying only the functional embedding, we can edit TCR sequences into desired properties while maintaining their backbones, running 10 times faster than baselines.

## 2 Methods

### Problem Formulation

We define our problem of TCR engineering task as follows: given a TCR sequence and a peptide it could not bind to, introduce a minimal number of mutations to the TCR so that it gains the ability to bind to the peptide. In the meantime, the modified TCR should remain a valid TCR, with no major changes in the structural backbone. Based on the assumption that only certain amino acids within the TCR should be responsible for peptide interactions, we can define two kinds of patterns in the TCR sequence: **functional patterns** and **structural patterns**. The former comprises the amino acidsthat define the peptide binding property. TCRs that bind to the same peptide should have similar functional patterns. The latter refers to all other patterns that do not relate to the function but could affect the validity. Following , we here limit our modeling to the CDR3\(\) region since it is the most active region for TCR binding. In the rest of the paper, we use "TCR" and "CDR3\(\)" interchangeably.

### Disentangled Wasserstein Autoencoder

Our proposed framework, named TCR-dMAE, leverages a disentangled Wasserstein autoencoder that learns embeddings corresponding to the functional and structural patterns. In this setting, the input data sample for the model is a triplet \(\{,,y\}\), where \(\) is the TCR sequence, \(\) is the peptide sequence, and \(y\) is the binary label indicating the interaction.

In detail, given an input triplet \(\{,,y\}\), the embedding space of \(\) is separated into two parts: \(=(_{f},_{s})\), where \(_{f}\) is the functional embedding, and \(_{s}\) is the structural embedding. The schematic of the model is shown in Fig. 1B.

#### 2.2.1 Encoders and Auxiliary Classifier

We use two separate encoders for the embeddings, respectively:

\[_{e}=_{e}(),\]

where \(e\{s,f\}\) correspond to "structure" and "function".

First, the functional embedding \(_{f}\) is encoded by the functional encoder \(_{f}()\). In order to make sure \(_{f}\) carries information about binding to the given peptide \(\), we introduce an auxiliary classifier \((_{f},)\) that takes \(_{f}\) and the peptide \(\) as input and predicts the probability of positive binding label \(q_{}(y_{f},)\),

\[=q_{}(Y=1_{f},)=(_{f}, ).\]

We define the binding prediction loss as binary cross entropy:

\[_{f\_cls}(,y)=-y-(1-y)(1-).\]

Figure 1: (A) Top: The TCR recognizes antigenic peptides provided by the major histocompatibility complex (MHC) with high specificity; bottom: the 3D structure of the TCR-peptide-MHC binding interface (PDB: 5HHO); the CDRs are highlighted. (B) The disentangled autoencoder framework, where the input \(x\), i.e., the CDR3\(\), is embedded into a functional embedding \(z_{f}\) (orange bar) and structural embedding \(z_{s}\) (green bar). (C) Method for sequence engineering with input \(x\). \(_{s}\) of the template sequence and a modified \(^{}_{f}\), which represents the desired peptide binding property, are fed to the decoder to generate engineered TCRs \(x^{}\).

Second, the structural embedding \(_{s}\) is encoded by the structural encoder \(_{s}()\). To enforce \(_{s}\) to contain all the information other than the peptide binding-related patterns, we leverage a sequence reconstruction loss, as will be discussed in detail in Section 2.2.3.

#### 2.2.2 Disentanglement of the Embeddings

To attain disentanglement between \(z_{f}\) and \(z_{s}\), we introduce a Wasserstein autoencoder regularization term in our loss function following , by minimizing the maximum mean discrepancy (MMD) between the distribution of the embeddings \( Q_{}\) where \(=(_{f},_{s})\) and an isotropic multivariate Gaussian prior \(_{0} P_{}\) where \(P_{}=(0,I_{d})\):

\[_{}()=(P_{},Q_{}).\] (1)

The MMD is estimated as follows: given the embeddings \(\{_{1},_{2},...,_{n}\}\) of an input batch of size \(n\), we randomly sample from the Gaussian prior \(\{}_{1},}_{2},...,}_{n}\}\) with the same sample size. We then use the linear time unbiased estimator proposed by  to estimate the MMD:

\[(P_{},Q_{})=_{i }^{ n/2}h((_{2i-1},}_{2i-1}),( _{2i},}_{2i})),\]

where \(h((_{i},}_{i}),(_{j},}_{ j}))=k(_{i},_{j})+k(}_{i},}_{ j})-k(_{i},}_{j})-k(_{j},}_{ i})\) and \(k\) is the kernel function. Here we use a radial basis function (RBF)  with \(=1\) as the kernel.

By minimizing this loss, the joint distribution of the embeddings matches \((0,I_{d})\), so that \(_{f}\) and \(_{s}\) are independent. Also, the Gaussian shape of the latent space facilitates generation [35; 17].

#### 2.2.3 Decoder and Overall Training Objective

The decoder \(\) takes \(_{f},_{s}\) and peptide \(\) as input and reconstructs the original sequence as \(^{}\). It also acts as a regularizer to enforce the structural embedding \(_{s}\) to contain all the information other than the peptide binding-related patterns. The reconstruction loss is the position-wise binary cross entropy between \(\) and \(^{}\) averaged across all positions of the sequence:

\[^{}=((_{s},_{f}, ))\]

\[_{recon}(,^{})=_{i}^{l}- ^{(i)}(^{(i)})-(1-^{(i)})(1- ^{(i)}),\]

where \(l\) is the length of the sequence and \(^{(i)}\) is the probability distribution over the amino acids at the \(i\)-th position.

Combining all these losses with weights \(_{1}\) and \(_{2}\), we have the final objective function, which then can be optimized through gradient descent in an end-to-end fashion:

\[=_{recon}+_{1}_{f\_cls}+_{2} _{}.\]

### Disentanglement Guarantee

To show how our method can guarantee the disentangled embeddings, we provide a novel perspective on the latent space of Wasserstein autoencoders  utilizing the variation of information following . We introduce a measurement of disentanglement as follows:

\[D(_{f},_{s};)=VI(_{s}; )+VI(_{f};)-VI( _{f};_{s}),\]

where \(VI\) is the variation of information, \(VI(;Y)=H()+H(Y)-2I(;Y)\), which is a measure of independence between two random variables. For simplicity, we omit the condition \(\) (peptide) in the following parts.

This measurement reaches 0 when \(_{f}\) and \(_{s}\) are totally independent, i.e. disentangled. It could further be simplified as:

\[VI(_{s};)+VI(_{f};)-VI( _{f};_{s})\] \[= 2H()+2[I(_{f};_{s})-I(; _{s})-I(;_{f})].\]Note that \(H()\) is a constant. Also, according to data processing inequality, as \(_{f} y\) forms a Markov chain, we have \(I(x;_{f}) I(y;_{f})\). Combining the results above, we have the upper bound of the disentanglement objective:

\[I(_{f};_{s})-I(;_{s})-I(; _{f}) I(_{f};_{s})-I(;_{ s})-I(Y;_{f}).\] (2)

Next, we show how our framework could minimize each part of the upper bound in (2).

**Maximizing \(I(;_{s})\)** Similar to , we have the following theorem:

**Theorem 2.1**: _Given the encoder \(Q_{}()\), decoder \(P_{}()\), prior \(P()\), and the data distribution \(P_{D}\)_

\[_{}(Q() P())=_{p_{ D}}[_{}(Q_{}() P( ))]-I(;),\]

_where \(Q()\) is the marginal distribution of the encoder when \( P_{D}\) and \( Q_{}()\)._

Theorem 2.1 shows that by minimizing the KL divergence between the marginal \(Q()\) and the prior \(P()\), we jointly maximize the mutual information between the data \(\) and the embedding \(\), and minimize the KL divergence between \(Q_{}()\) and the prior \(P()\). Detailed proof of the theorem can be found in the Appendix B.1. This also applies to the two separate parts of \(\), \(_{f}\) and \(_{s}\). In practice, because the marginal cannot be measured directly, we minimize the aforementioned kernel MMD (1) instead.

As a result, there is no need for additional constraints on the information content of \(_{s}\) because \(I(;_{f})\) is automatically maximized by the objective. We also empirically verify in Section 3.2.3 that supervision on \(_{s}\) does not improve the model performance.

**Maximizing \(I(Y;_{f})\)**\(I(Y;_{f})\) has an lower bound as follows:

\[I(Y;_{f}) H(Y)+_{p(Y,_{f})} q_{}(Y _{f}),\]

where \(q_{}(Y_{f})\) is the predicted probability by the auxiliary classifier \(\) (note we omitted the condition \(\) here). Thus, maximizing the performance of classifier \(\) would maximize \(I(Y;_{f})\).

**Minimizing \(I(_{f};_{s})\)** Minimization of the Wasserstein loss (1) forces the distribution of the embedding space \(\) to approach an isotropic multivariate Gaussian prior \(P_{}=(0,I_{d})\), where all the dimensions are independent. Thus, the dimensions of \(\) will be independent, which also minimizes the mutual information between the two parts of the embedding, \(_{f}\) and \(_{s}\).

As a conclusion, our objective can jointly minimize \(I(_{f};_{s})\) and maximize \(I(;_{s})\) and \(I(Y;_{f})\). The former ensures independence between the embeddings, and the latter enforces them to learn separate information, achieving disentanglement.

## 3 Experiments

### Setup

**Datasets** Interacting TCR-peptide pairs are obtained from VDJDB , merged with experimentally-validated negative pairs from NetTCR . We then expended the negatives to 5x the size of positives by adding randomly shuffled TCR-peptide pairs and only selected the peptides that could be well-classified by ERGO, a state-of-the-art method for TCR-binding prediction (Appendix A.2). The selected samples are then balanced resulting in 26262 positive and 26262 negative pairs with 2918 unique TCRs and 10 peptides. We also performed the same experiments on the McPAS-TCR dataset  (Appendix A.1). For each peptide, we select from VDJDB an additional set of 5000 TCR sequences that do not overlap with the training set. This set is used for the TCR engineering experiment.

**Implementation Details** The TCR-dWAE model uses two transformer encoders  for \(_{s},_{f}\) and a long short-term memory (LSTM) recurrent neural network decoder  for \(\). The auxiliary classifier \(\) is a 2- layer perceptron. Hyperparameters are selected through grid search. All results are averaged across five random seeds. See Appendix B.2 for more details.

### TCR Engineering

#### 3.2.1 Manipulating TCR Binding via Functional Embeddings

As shown in Fig. 1C, given any TCR sequence template \(\), we combine its original \(_{}\) and a new functional embedding \(^{}_{f}\) that is positive for the target peptide \(\) which is then fed to the decoder to generate a new TCR \(^{}\) that could potentially bind to \(\) while maintaining the backbone of \(\). For each generation, we obtain the \(^{}_{f}\) from a random positive sample in the dataset.

#### 3.2.2 Metrics and Baselines

We use the following metrics to evaluate whether the engineered sequence \(^{}\) (1) is a valid TCR sequence and (2) binds to the given peptide, which we denote as _validity score_ and _binding score_, respectively:

The _validity score_\(r_{v}\) evaluates whether the generated TCR follows similar generic patterns as naturally observed TCRs from TCRdb, an independent and much larger dataset . We train another LSTM-based autoencoder on TCRdb. If the generated sequence can be reconstructed successfully by the autoencoder and has a high likelihood in the distribution of the latent space, we consider it as a valid TCR from the same distribution as the known ones (Appendix D.2). We use the sum of the reconstruction accuracy and the likelihood as the validity score and show that this metric separates true TCRs from other protein segments and random sequences (Appendix D.3).

For the _binding score_, the engineered sequence \(^{}\) and the peptide \(\) are fed into the pre-trained ERGO classifier and binding probability \(r_{b}=(^{},)\) is calculated.

We compare TCR-ONAE with the following baselines (see C for more details):

**Random mutation-based** For each iteration, a new random mutation is added to the template. The best mutated sequence (one with the highest ERGO score) is selected on either a sample-level (greedy), population-level (genetic), or without any selection (naive rm), then goes into the next round.

**Generation-based** This includes Monte Carlo tree search (MCTS), where random residues are iteratively added until the sequence reaches a maximum length. For each generation process, the leaf with the highest ERGO score is added to the results.

**TCR-dVAE** We also include a VAE baseline inspired by the objectives of IDEL from , which minimizes a MI upper bound to achieve disentanglement in the embedding space (Appendix C.3).

#### 3.2.3 Main Results

We use peptides with AUROC \(>0.8\) by the classifier \(\) for the engineering task (SSYRRPVGI, TTPESANL, FRDYVDRFYKTLRAEQASQE, CTPYDINQM). The results are listed in Table 1. The cutoff for "valid" is \(r_{v} 1.25\), as it can successfully separate known TCRs from other protein sequences. The cutoff for "positive" TCR is \(r_{b}>0.5\) as the prediction scores by ERGO is rather extreme, either \(0\) or \(1\). The vast majority of the generated valid sequences (ranging from

    & \(}\) & \(}\) & \%valid & \#mut/len & \%positive valid \(\) \\  TCR-dWAE & 1.36\(\)0.03 & 0.45\(\)0.09 & 0.61\(\)0.06 & 0.49\(\)0.05 & **0.23\(\)0.02** \\ TCR-dVAE & 1.44\(\)0.02 & 0.24\(\)0.02 & 0.64\(\)0.05 & 0.4\(\)0.02 & 0.16\(\)0.01 \\ greedy & 0.34\(\)0.0 & 0.79\(\)0.0 & 0.02\(\)0.0 & 0.34\(\)0.0 & 0.02\(\)0.0 \\ genetic & 0.39\(\)0.03 & 1.0\(\)0.0 & 0.02\(\)0.0 & NA & 0.02\(\)0.0 \\ naive rm & 0.35\(\)0.0 & 0.2\(\)0.0 & 0.03\(\)0.0 & 0.35\(\)0.01 & 0.0\(\)0.0 \\ MCTS & -0.1\(\)0.0 & 0.95\(\)0.0 & 0.04\(\)0.0 & NA & 0.0\(\)0.0 \\ TCR-dWAE (null) & 1.51\(\)0.01 & 0.05\(\)0.03 & 0.85\(\)0.01 & 0.45\(\)0.07 & 0.04\(\)0.03 \\ original & 1.59 & 0.01 & 0.92 & NA & 0.01 \\   

Table 1: Performance comparison. original is the evaluation metrics on the template TCRs without any modification. Only unique sequences are counted. Results are averaged across selected peptides (i.e., SSYRRPVGI, TTPESANL, FRDYVDRFYKTLRAEQASQE, CTPYDINQM) and then averaged across five random seeds.

\(100\%\)) are unique and all are novel (Appendix Table 10). In general, TCR-dWARE-based methods generate more valid and positive sequences compared to other methods. We also include the results on McPAS-TCR in the Appendix Table 9, where the observations are similar.

One advantage of TCR-dWARE is that \(_{s}\) implicitly constrains the sequence backbone, ensuring validity. Methods like genetic and MCTS, on the contrary, generate much fewer valid sequences without explicit control on validity. As a result, they could produce high binding score sequences because they are guided to do so, but most of them are invalid TCRs, or out-of-distribution samples that ERGO cannot correctly predict. Also, they require calling of an external evaluation model during generation at every iteration. Adding a regularizer to enforce validity could potentially improve these baselines' generation quality, but the two objectives would be difficult to balance and will further increase the computational burden. TCR-dWARE, on the contrary, can perform sequence engineering in one pass, requiring 10x less time (Appendix Table 10). Compared to VAE-based models like TCR-dVAE, TCR-dWARE is a deterministic model with simpler objectives, circumventing some well-known challenges in the training of VAE which we've found to be rather sensitive to hyperparameters (see Appendix C.3 and E.5 for more comparisons between model architectures and generation modes).

#### 3.2.4 Analysis of Engineered TCRs

We define the first four and the last three residues as the conservative region and the rest as the hypervariable region. On average, the sequences generated by TCR-dAE have a similar conservative region motif as known TCR CDR3\(\)s  (Fig. 2A). The average attention score of the functional encoder concentrates on the hypervariable part (Fig. 2B, top), which has a similar distribution as the mutation frequency of the engineered sequences compared to their templates (Fig. 2B, bottom). On the other hand, we can also observe some, though less frequent, changes in the conservative regions through modifying the \(z_{f}\), while some residues in the hypervariable region are maintained (Fig. 2C). Furthermore, the attention patterns of the functional and structural encoders, despite having their own preferences, overlap at some positions and do not have a clear-cut separation. Neither pattern

Figure 2: (A) Consensus motif of the first 4 and last 3 residues for the engineered TCRs and known TCRs; the height of the alphabet indicates the frequency of the amino acid occurring at the position. (B) (top) Average attention patterns of the functional encoder for the positive TCRs; (middle) average attention patterns of the structural encoder for the positive TCRs;(bottom) mutation frequency of the optimized TCRs compared to the templates; averaged across sequences of length 15. (C) Pairwise Miyazawa-Jernigan energy for the positive optimized sequence and its template with respect to the target peptide (left: SSYRRPVGI; right: CTPYDINQM).

aligns with the hypervariable/conservative separation (Fig. 2B top and middle). These results indicate that both \(z_{f}\) and \(z_{s}\) learn to encode patterns from the entire sequence, and do not fit into the manual separation of a "functional" hypervariable region and a "structural" conservative region.

We also show some examples where residues introduced by the positive \(_{f}\) to the engineered sequence have the potential of forming lower-energy (i.e. more stable) interactions with the target peptide (Fig. 2C), using the classic Miyazawa-Jernigan contact energy , which is a knowledge-based pairwise energy matrix for amino acid interactions. These results demonstrate that the positive patterns carried by \(_{f}\) have learned some biologically relevant information such as the favored binding energy of the TCR-pMHC complex, and are successfully introduced to the engineered TCRs.

### Ablation Study

For a quantitative comparison of the disentanglement, we calculate a sample-based MMD between the embeddings of positive and negative samples. Ideally, a disentangled latent space would make \(_{f}\) of the positive and negative groups very different, and \(_{s}\) indistinguishable. As shown in Table 2, removing the Wasserstein loss (Wass\(-\)) or the auxiliary classifier (\(-\)) leads to both poorer performance and poorer disentanglement.

We also attempt to explicitly control the information content of \(_{s}\) by adding another decoder using only \(_{s}\) (\(L_{s}+\)). This in practice results in similar, even slightly better, disentanglement compared to the original model, but not necessarily better performance. These observations agree with our conclusions in Section 2.3.

### Analysis of the Embedding Space

We select a balanced subset from the test set (332 positives and 332 negatives) and obtain their \(_{f}\) and \(_{s}\) embeddings. T-SNE  visualization in Fig. 3A shows that for each peptide, the binding and non-binding TCRs can be well separated by \(_{f}\) but not \(_{s}\) (more examples in Appendix Fig. 5). Also, among the positive samples, \(_{f}\) show strong clustering patterns corresponding to their peptide targets (Fig. 2B). As a result of the Wasserstein loss constraint, there is minimal correlation between \(_{f}\) and \(_{s}\) (Fig. 3C).

Furthermore, as shown in Table 3, the reconstruction accuracy is the highest when both \((_{f},_{s})\) are provided, and significantly impaired when either embedding is missing. These results imply that the embedding space is properly disentangled as intended: \(_{f}\) and \(_{s}\) each encodes separate parts of the sequential patterns, and both are required for faithful reconstruction.

## 4 Conclusions and Outlook

In this work, we present TCR-dWAE, a disentangled Wasserstein autoencoder-based framework for massive TCR engineering. The disentanglement constraints separate key patterns related to binding

    & \%positive valid & \(_{f}\)\(\) & \(_{s}\)\(\) \\  full & 0.23\(\)0.02 & 0.163\(\)0.015 & 0.013\(\)0.004 \\ Wass- & 0.21\(\)0.02 & 0.040\(\)0.026 & 0.013\(\)0.006 \\ L\_s+ & 0.18\(\)0.01 & 0.238\(\)0.029 & 0.008\(\)0.000 \\ L\_i- & 0.13\(\)0.04 & 0.087\(\)0.067 & 0.011\(\)0.004 \\   

Table 2: Ablation study. Left: %positive valid for TCR engineering; Right: sample-based MMD between the embeddings of positive and negative samples. All means and standard deviations are calculated across five random seeds.

   Embedding & Average reconstruction accuracy \\  Original \((_{f},_{s})\) & 0.9044 \(\) 0.0020 \\ Random \(_{f}\) & 0.7535 \(\) 0.0085 \\ Fully random & 0.6016 \(\) 0.0030 \\   

Table 3: Reconstruction accuracy with altered embeddings (VDJDB).

("functional" embedding) from generic structural backbones ("structural" embedding). By modifying only the functional embedding, we are able to generate new TCRs with desired binding properties while preserving the backbone.

We are aware that our experiment is performed on a smaller subset selected by ERGO, whose limitations would bias the results. Also, our model assumes there is some "natural", yet unknown, disentanglement between features within the data. Thus, it could potentially be applied to several protein engineering tasks but would fail if the definition of functional terms is vague. With more high-throughput experimental data on TCR-peptide interaction, we will be able to train more comprehensive models on full TCR sequences. To our knowledge, ours is the first work that approaches protein sequence engineering from a style transfer perspective. As our framework mimics a vast amount of biological knowledge on function versus structure, we believe it can be further extended to a broader definition of protein functions or other molecular contexts.

## 5 Related Works

**TCR-peptide binding prediction** Most human TCRs are heterodimers comprised of a TCR\(\) chain and a TCR\(\) chain . Each chain contains three complementarity-determining regions (CDRs), CDR1, 2 and 3, which are highly variable due to gene recombination. A large amount of literature investigates the prediction of the highly specific interaction between TCRs and peptides. Early methods use consensus motifs or simple structural patterns like k-mers as predictive features [44; 45; 46; 24]. Other methods assess the binding energy for the TCR-peptide pair using physical or structural modeling [47; 48]. In recent years, several deep neural network architectures have been applied to this task [49; 26; 50; 23; 24; 51; 27], using both TCR and peptide sequences as input. Most studies

Figure 3: (A) T-SNE patterns of \(_{f}\) and \(_{s}\) for selected peptides, colored by ground-truth labels. (B) t-SNE of the \(_{f}\) of the positive TCRs. Colors correspond to their binding peptide. (C) Correlation between dimensions of \(_{f}\) and \(_{s}\), where the orange color corresponds to \(_{f}\) and green to \(_{s}\).

focus on the CDR3\(\) sequence and some also include other information like the CDR3\(\), depending on the availability of the data.

**Computational biological sequence design** Traditional methods of computational biological sequence design uses search algorithms, where random mutations are introduced and then selected by specified criteria . Deep generative models like variational autoencoder (VAE)  or generative adversarial network (GAN)  have also been successfully applied to biological sequences such as DNA. Other recent approaches include reinforcement learning (RL) , and iterative refinement . It is also possible to co-design sequences along with the 3D structure, such as in motif scaffolding problems .

**Disentangled representation learning** The formulation of DRL tasks depends on its nature, such as separating the "style" and the "content" of a piece of text  or a picture , or static (time-invariant) and dynamic (time-variant) elements of a video . The disentangled representations can be used for style transfer  and conditional generation . Several regularization methods have been used to achieve disentanglement with or without _a priori_ knowledge, including controls on the latent space capacity , adversarial loss , cyclic reconstruction , mutual information constraints , and self-supervising auxiliary tasks .