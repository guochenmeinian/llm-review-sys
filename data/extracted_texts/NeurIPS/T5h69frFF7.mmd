# UnSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures

Zhong-Qiu Wang and Shinji Watanabe

Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA

wang.zhongqiu41@gmail.com

###### Abstract

In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for unsupervised neural speech separation by leveraging over-determined training mixtures. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this loss can promote unsupervised separation of speakers. The linear filters are computed in each sub-band based on the mixture and DNN estimates through the forward convolutive prediction (FCP) algorithm. To address the frequency permutation problem incurred by using sub-band FCP, a loss term based on minimizing intra-source magnitude scattering is proposed. Although UNSSOR requires over-determined training mixtures, we can train DNNs to achieve under-determined separation (e.g., unsupervised monaural speech separation). Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR.

## 1 Introduction

In many machine learning and artificial intelligence applications, sensors, while recording, usually capture a mixture of desired and undesired signals. One example is the cocktail party problem (or speech separation) , where, given a recorded mixture of the concurrent speech by multiple speakers, the task is to separate the mixture to individual speaker signals. Speech separation  has been dramatically advanced by deep learning, since deep clustering  and permutation invariant training (PIT)  solved the label permutation problem. They (and their subsequent studies ) are based on supervised learning, requiring paired clean speech and its corrupted signal generated via simulation, where clean speech is mixed with, for example, various noises and competing speakers at diverse energy and reverberation levels in simulated rooms . The clean speech can provide an accurate, sample-level supervision for model training. Such simulated data, however, may not match the distribution of real-recorded test data in the target domain, and the resulting supervised learning based models would have generalization issues . How to train unsupervised neural speech separation systems on unlabelled target-domain mixtures is hence an important problem to study.

Training unsupervised speech separation models directly on monaural mixtures is an ill-posed task , since there is only one mixture signal observed but multiple speaker signals to reconstruct. The separation model would lack an accurate _supervision_ (or regularizer) to figure out what desired sound objects (e.g., clean speaker signals) are, as there are infinite solutions where in each solution theestimated sources can sum up to the mixture. Supposing that the separation model does not separate well and outputs a clean speaker signal plus some competing speech, noise or reverberation, would this output be viewed as a desired sound object? This is clear to humans, clear to supervised learning based models (by comparing the outputs with training labels), but not really clear to an unsupervised model. On the other hand, many studies [3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26] have observed that deep learning based supervised learning can achieve remarkable separation. In other words, with proper supervision, modern DNNs are capable of separating mixed speakers, but, in an unsupervised setup, there lacks an accurate supervision to unleash this capability. The key to successful unsupervised neural separation, we believe, is designing a clever _supervision_ that can inform the model what desired sound objects are, and penalize the model if its outputs are not good and reward otherwise.

Our insight is that, in multi-microphone over-determined conditions where the microphones out-number speakers, the ill-posed problem can be turned into a well-posed one, where a unique solution to the speakers exists (up to speaker permutation). This well-posed property (that a unique solution exists) can be leveraged as a _supervision_ (or regularizer) to design loss functions that could inform the unsupervised separation model what desired sound objects are and promote separation of speakers.

Equipped with this insight, we perform unsupervised neural speech separation by leveraging multi-microphone over-determined training mixtures. Our DNNs can be trained directly on over-determined mixtures to realize over- and under-determined separation. The proposed algorithm, named UNSSOR, obtains strong separation performance on two-speaker separation. Our contributions include:

* We enforce a linear-filter constraint between each speaker's reverberant images at each microphone pair, turning the ill-posed problem into a well-posed one that can promote separation of speakers.
* We formulate unsupervised neural speech separation as a blind deconvolution problem, where both the speaker images and linear filters need to be estimated. We design loss functions motivated by the blind deconvolution problem, and propose a DNN approach to optimize the loss functions, where the speaker images are estimated via DNNs and the linear filters are estimated via a sub-band linear prediction algorithm named FCP  based on the mixture and DNN estimates.
* We propose a loss term, which minimizes a metric named intra-source magnitude scattering, to address the frequency permutation problem incurred when using sub-band FCP.
* Based on over-determined training mixtures, UNSSOR can be trained to perform under-determined separation (e.g., monaural unsupervised speech separation).

## 2 Related work

Various unsupervised neural separation algorithms, which do not require labelled mixtures, have been proposed. The most notable one is mixture invariant training (MixIT) [30; 31; 32; 33; 34], which first synthesizes training mixtures, each by mixing two existing mixtures, and then trains a DNN to separate the resulting mixed mixtures to underlying sources such that the separated sources can be partitioned into two groups and the separated sources in each group can sum up to one of the two existing mixtures (used for mixing). Care needs to be taken when synthesizing mixtures of mixtures. First, the sources in an existing mixture could have similar characteristics (e.g., similar reverberation patterns as the sources in an existing mixture are recorded in the same room) that are informative about which sources belong to the same existing mixture, and this would prevent MixIT from separating the sources [30; 35]. Second, it is unclear how to mix existing multi-channel mixtures, which are usually recorded by devices with different microphone geometry and number of microphones. Third, mixing existing mixtures with different reverberation characteristics would create unrealistic mixtures.

UNSSOR avoids the above issues by training unsupervised neural separation models directly on existing mixtures rather than on synthesized mixtures of mixtures. An earlier study related to this direction is the reverberation as supervsion (RAS) algorithm , which addresses monaural two-speaker separation given binaural (two-channel) training mixtures. RAS performs magnitude-domain monaural separation directly on the left-ear mixture and then linearly filters the estimates through time-domain Wiener filtering so that the filtered estimates can approximate the right-ear mixture. RAS essentially does monaural separation and is effective at separating speakers in a semi-supervised learning setup, where a supervised PIT-based model is first trained and then used to bootstrap unsupervised training. It however fails completely in fully-unsupervised setup , unlike UNSSOR.

Conventional algorithms such as independent component analysis [37; 38; 39; 40; 41], independent vector analysis (IVA) [42; 43; 41; 44] and spatial clustering [45; 46; 47; 48] can perform unsupervised separation directly on existing mixtures. They perform separation based on a single test mixture at hand and are not designed to learn speech patterns from large training data, while UNSSOR leverages DNNs to model speech patterns through unsupervised learning, which could result in better separation. Another difference is that UNSSOR can be configured for monarual, under-determined separation, while ICA, IVA and spatial clustering cannot. There are studies  training DNNs to approximate pseudo-labels produced by conventional signal processing based separation models such as spatial clustering and blind source separation (BSS) algorithms. Their performance is however often limited since spatial clustering and BSS themselves are not good enough at separation. There are studies  training DNNs to separate multi-speaker mixtures such that the likelihood of observed mixtures under a probabilistic distribution derived based on the DNN separation results can be maximized. Such methods rely on statistical models for separation. They require costly iterative estimation of signal statistics at run time. In addition, the DNNs are only leveraged in estimating target magnitude, while phase estimation is only realized by spatial filtering.

## 3 Problem formulation

Given a \(P\)-microphone mixture with \(C\) speakers in reverberation conditions, the physical model can be formulated using a system of linear equations in the short-time Fourier transform (STFT) domain:

\[Y_{p}(t,f)=_{c=1}^{C}X_{p}(c,t,f)+_{p}(t,f),\;\,p\{1,,P\},\] (1)

where \(t\) indexes \(T\) frames, \(f\) indexes \(F\) frames, and at microphone \(p\), time \(t\) and frequency \(f\), \(Y_{p}(t,f)\), \(X_{p}(c,t,f)\) and \(_{p}(t,f)\) respectively denote the STFT coefficients of the mixture, reverberant image of speaker \(c\), and non-speech signals. In the rest of this paper, we refer to the corresponding spectrogram when dropping the index \(c\), \(p\), \(t\) or \(f\). We assume that \(\) is weak and stationary (e.g., a time-invariant Gaussian noise or simply modelling errors). Without loss of generality, we designate microphone \(1\) as the reference microphone. Our goal is to, in an unsupervised way, estimate each speaker's image at the reference microphone (i.e., \(X_{1}(c)\) for each speaker \(c\)) given the input mixture. We do not aim at dereverberation, instead targeting at maintaining the reverberation of each speaker.

Unsupervised separation based only on the observed mixture is difficult. There are infinite solutions to the above linear system where there are \(T F P\) equations (we have a mixture observation for each \(Y_{p}(t,f)\)) but \(T F P C\) unknowns (we have one unknown for each \(X_{p}(c,t,f)\)).

Our insight is that the number of unknowns can be dramatically reduced, if we enforce constraints to the speaker images at different microphones. Since \(X_{1}(c)\) and \(X_{p}(c)\) are both convolved versions of the dry signal of speaker \(c\), there exists a linear filter between them such that convolving \(X_{1}(c)\) with the filter would reproduce \(X_{p}(c)\). This convolutive relationship is a physical constraint, which can be leveraged to reduce the number of unknowns. Specifically, we formulate (1) as

\[Y_{1}(t,f)=_{c=1}^{C}X_{1}(c,t,f)+_{1}( t,f),\] \[Y_{p}(t,f)=_{c=1}^{C}_{p}(c,f)^{}\,}_{1}(c,t,f)+_{p}^{}(t,f),\;\,p\{2,,P\},\] (2)

where \(}_{1}(c,t,f)=[X_{1}(c,t-A,f),,X_{1}(c,t,f),, X_{1}(c,t+B,f)]^{}^{A+1+B}\) stacks a window of \(E=A+1+B\) T-F units, \(_{p}(c,f)^{E}\) is the _relative room impulse response_ (relative RIR) relating \(X_{1}(c)\) to \(X_{p}(c)\), and \(()^{}\) computes Hermitian transpose. \(_{p}(c,f)\) is not long (i.e., \(E\) is small)  if microphone \(1\) and \(p\) are placed close to each other, which is the case for compact arrays.

An implication of this constraint is that the number of unknowns is reduced from \(T F P C\) to \(T F C+F(P-1) E C\)1, which can be smaller than the number of equations (i.e., \(T F P\)) when \(P>C\) (i.e., over-determined conditions) and when \(T\) is sufficiently large (i.e., the input mixture is reasonably long). In other words, this formulation suggests that (1) there exists a solution for separation, which is most consistent with the above linear system; and (2) in over-determined cases, it is possible to estimate the speaker images in an unsupervised way.

As \(\) is assumed weak, time-invariant and Gaussian, one way to find the solution is to compute an estimate that is most consistent with the linear system in (2) by solving the following problem:

\[\::\:(,),X_{1}(,,)}{}_{t,f}Y_{1}(t,f)-_{c=1}^{C}X_{1}(c,t,f)^{2}+_{p=2}^{P} _{t,f}Y_{p}(t,f)-_{c=1}^{C}_{p}(c,f)^{}\: }_{1}(c,t,f)^{2}.\] (3)

This is a blind deconvolution problem , which is non-convex in nature and difficult to be solved if no prior knowledge is assumed about the relative RIRs or the speaker images, because both of them are unknown. In the next section, we propose a DNN-based approach, which can model speech patterns through unsupervised learning (and hence model speech priors), to tackle this problem.

## 4 Method

Fig. 1 illustrates the proposed system. The DNN takes in the mixture at all the \(P\) microphones or at the reference microphone \(1\) as input and produces an intermediate estimate \((c)\) for each speaker \(c\). FCP  is then performed on \((c)\) at each microphone \(p\) to compute a linear-filtering result, denoted as \(_{p}^{}(c)\), which, we will describe, is essentially an estimate of the speaker image \(X_{p}(c)\). After that, two loss functions are computed and combined for DNN training. This section describes the DNN configuration, loss functions, FCP filtering, and an extension for monaural separation.

### DNN configurations

The intermediate estimate \((c)\) for each speaker \(c\) is obtained via complex spectral mapping [23; 58], where we stack the real and imaginary (RI) parts of the input mixture as features for the DNN to predict the RI parts of \((c)\). For the DNN architecture, we employ TF-GridNet , which obtains strong results on supervised speech separation benchmarks. See Appendix I for more DNN details.

### Mixture-constraint loss on filtered estimates

Following formulation in (3), we propose _mixture-constraint_ (MC) loss, which is computed by filtering the DNN estimate \((c)\) of each speaker \(c\) to approximate the \(P\)-channel input mixture:

\[_{}=_{1}_{t,f}(Y_{1}(t,f),_{c=1 }^{C}(c,t,f))+_{p=2}^{P}_{p}_{t,f}(Y_{p}(t,f),_{c=1}^{C}}_{p}(c,f)^{}\: }(c,t,f)).\] (4)

In (4), \(}(c,t,f)\) stacks a window of T-F units around \((c,t,f)\), and \(}_{p}(c,f)\) is an estimated relative RIR computed based on \((c,,f)\) and the mixture \(Y_{p}(,f)\) through FCP . Both of them will be described in the next sub-section. \(_{p}\) is a weighting term for microphone \(p\). Following , \((,)\) in (4) computes an absolute loss on the estimated RI components and their magnitude:

\[Y_{p}(t,f),_{p}(t,f)= ,f^{}} Y_{p}(t^{},f^{ })}(Y_{p}(t,f))-(_{p} (t,f))\] \[+(Y_{p}(t,f))-(_{p}(t,f)) + Y_{p}(t,f)-_{p}(t,f) ,\] (5)

Figure 1: Illustration of UNSSOR (assuming \(P>C\) during training).

where \(()\) and \(()\) respectively extract RI components and \(||\) computes magnitude. The term \(1/_{t^{},f^{}}|Y_{p}(t^{},f^{})|\) balances the losses at different microphones and across training mixtures.

According to the discussion in Section 3, minimizing \(_{}\) would encourage separation of speakers. We illustrate the loss surface of \(_{}\) in Appendix B.

Compared to the mixture consistency term proposed in , our mixture-constraint loss has very different physical meanings and mathematical forms. See Appendix E for detailed discussions.

### FCP for relative RIR estimation

To compute \(_{}\), we need to first estimate each of the relative RIRs, \(}_{p}(c,f)\). In [29; 60], FCP is proposed to estimate the relative RIR relating direct-path signal to reverberant image for speech dereverberation. In this study, we employ FCP to estimate the relative RIR relating \((c)\) to the speaker image captured at each microphone \(p\) (i.e., \(X_{p}(c)\)).

Assuming speakers are non-moving, we estimate relative RIRs by solving the following problem:

\[}_{p}(c,f)=_{p}(c,f)}{}_{t}_{p}(c,t,f)}Y_{p}(t,f)- _{p}(c,f)^{}\,}(c,t,f)^{2},\] (6)

where \(_{p}(c,f)^{I+1+J}\) is a \(K\)-tap (with \(K=I+1+J\)) time-invariant FCP filter, \(}(c,t,f)=[(c,t-I,f),,(c,t,f),, (c,t+J,f)]^{}^{K}\) stacks \(I\) past and \(J\) future T-F units with the current one. Since the actual number of filter taps (i.e., \(A\) and \(B\) defined in the text below (2)) is unknown, we set them to \(I\) and \(J\), both of which are hyper-parameters to tune. \(_{p}(c,t,f)\) is a weighting term balancing the importance of each T-F unit. Following , we define it as \(_{p}(c,t,f)=(_{p^{}=1}^{P}|Y_{p^{ }}(t,f)|^{2})+_{p^{ }=1}^{P}|Y_{p^{}}|^{2}\), where \(\) (\(=10^{-4}\) in this study) is used to floor the weighting term and \(()\) extracts the maximum value of a spectrogram. (6) is a weighted linear regression problem. A closed-form solution can be readily computed:

\[}_{p}(c,f)=_{t}_{p} (c,t,f)}}(c,t,f)}(c,t,f)^{} ^{-1}_{t}_{p}(c,t,f)}}(c,t,f)(Y_{p}(t,f))^{*},\] (7)

where \(()^{*}\) computes complex conjugate. We then plug \(}_{p}(c,f)\) into (4) and compute the loss.

Note that to compute the relative RIR, ideally we should filter \((c)\) to approximate \(X_{p}(c)\) (i.e., replacing \(Y_{p}\) in (6) with \(X_{p}(c)\)), but \(X_{p}(c)\) is unknown. In (6), we instead linearly filter \((c)\) to approximate \(Y_{p}\), and earlier studies [29; 60] suggest that the resulting \(}_{p}(c,f)^{}\,}(c,t,f)\) would be an estimate of \(X_{p}(c,t,f)\), if \((c)\) is reasonably accurate (see Appendix C for the derivation). We name the speaker image estimated this way as _FCP-estimated image_:

\[_{p}^{}(c,t,f)=}_{p}(c,f)^{}\, }(c,t,f).\] (8)

It is therefore reasonable to sum up the FCP-estimated images of all the speakers and define a loss between the summation and \(Y_{p}\) as in (4).

Although (6) appears similar to multi-channel linear prediction (MCLP) [61; 62] which is popular in conventional speech separation algorithms, we point out that they have very different physical meanings. We consider that (6) does _forward filtering_, where source estimates are filtered to approximate mixtures so that relative RIRs can be estimated, while MCLP does _inverse filtering_, where mixtures are filtered to approximate target sources and the filters are designed to suppress non-target signals. This difference results in non-trivial changes of the physical meanings of the computed filters (see also discussions in Section V.C of ).

Although there were earlier efforts in estimating relative RIRs , they are based on conventional signal processing techniques and the performance is usually limited due to strong assumptions on, and inaccurate estimation of, signal statistics.

### Time alignment issues and alternative loss functions

In (4), we do not filter the DNN estimates when computing the loss on the first (reference) microphone. We expect this to result in a \((c)\) time-aligned with the speaker image \(X_{1}(c)\) (i.e., \((c)\) is an estimate of \(X_{1}(c)\)). Since the reference microphone may not be the microphone closest to speaker \(c\), it is best to use non-causal filters when filtering \((c)\) to approximate the reverberant image \(X_{p}(c)\) at non-reference microphones that are closer to source \(c\) than the reference microphone, and instead use causal filters for non-reference microphones that are farther2. Since estimating which non-reference microphones are closer or farther to a source than the reference microphone is not an easy task and doing this would complicate our system, we can just choose to use non-causal filters for all the non-reference microphones. This could, however, limit the DNN's capability at separating the speakers, because the relative RIRs for some non-reference microphones (farther to source \(c\) than the reference microphone) are causal, and it may not be a good idea to assume non-causal filters.

To address this issue, we make a simple modification to the loss function in (4):

\[_{}=_{p=1}^{P}_{p}_{p}=_{p=1}^{P}_{p}_{t,f}Y_{p} (t,f),_{c=1}^{C}}_{p}(c,f)^{}\, {}(c,t,f),\] (9)

where the difference is that we also filter the DNN estimates when computing the loss on the reference microphone, and we constrain \(}_{p}(c,f)\) to be causal and that \(}(c,t,f)\) only stacks current and past frames. This way, the resulting \((c)\) would not be time-aligned with the revereberant image captured at the reference microphone (i.e., \(X_{1}(c)\)) or any other non-reference microphones. Because of the causal filtering, \((c)\) would be more like an estimate of the reverberant image captured by a _virtual microphone_ that is closer to speaker \(c\) than all the \(P\) microphones (see Appendix G.1 for an interpretation). It would contain less reverberation of speaker \(c\) than any of the speaker images captured by the \(P\) microphones due to the causal filtering.

To produce an estimate that is time-aligned with the reverberant image at a microphone (e.g., \(X_{p}(c)\)), we use the FCP-estimated image computed in (8) (i.e., \(_{p}^{}(c)\)) as the output.

### Addressing frequency permutation problem

In (4) and (9), FCP is performed in each frequency independently from the others. Even though the speakers are separated at each frequency, the separation results of the same speaker at different frequencies may however not be grouped into the same output spectrogram (see an example in Appendix D). This is known as the _frequency permutation problem_, which has been studied for decades in frequency-domain blind source separation algorithms such as frequency-domain independent component analysis [37; 38; 39; 40; 41] and spatial clustering [45; 46; 47; 48]. Popular solutions for frequency alignment are designed by leveraging cross-frequency correlation of spectral patterns [47; 64] and direction-of-arrival estimation . However, these solutions are often empirical and have a complicated design. They can be used to post-process DNN estimates for frequency alignment, but it is not easy to integrate them with UNSSOR for joint training. This section proposes a loss term, with which the trained DNN can learn to produce target estimates without frequency permutation.

To deal with frequency permutation, IVA [42; 43; 44] assumes that, at each frame, the de-mixed outputs at all the frequencies follow a complex Gaussian distribution with a shared variance term across frequencies: \((c,f)^{}(t,f)(0,D(t,c))\), where \((c,f)^{P}\) is the de-mixing weight vector (in a time-invariant de-mixing matrix) for speaker \(c\) at frequency \(f\), and \(D(t,c)\) is the shared variance term, which is assumed time-variant. When maximum likelihood estimation is performed to estimate the de-mixing matrix, the variance term shared across all the frequencies is found very effective at solving the frequency permutation problem [41; 42; 43; 44].

Motivated by IVA, we design the following loss term, named _intra-source magnitude scattering_ (ISMS), to alleviate the frequency permutation problem in DNN outputs:

\[_{}=_{p=1}^{P}_{p}_{,p}= _{p=1}^{P}_{p}_{c=1}^{C} (|_{p}^{}(c,t,)|)}{_{t} {var}(|Y_{p}(t,)|)},\] (10)

where \(_{p}^{}\) is computed via (8), \(_{p}^{}(c,t,)^{F}\), and \(()\) computes the variance of the values in a vector. At each frame, we essentially want the the magnitudes of the estimated spectrogram of each speaker (i.e., \(_{p}^{}(c,t,)\)) to have a small intra-source variance. The rationale is that, when frequency permutation happens, \(_{p}^{}(c,t,)\) would contain multiple sources, and the resulting variance would be larger than that computed when \(_{p}^{}(c,t,)\) contains only one source. \(_{}\) echoes IVA's idea of assuming a shared variance term across all the frequencies. If the ratio in (10) becomes smaller, it indicates that the magnitudes of \(_{p}^{}(c,t,)\) are more centered around their mean. This is similar to optimizing the likelihood of \(_{p}^{}(c,t,)\) under a Gaussian distribution with a variance term shared across all the frequencies. In (10), a logarithmic compression is applied, since log-compressed magnitudes better follow Gaussian distribution than raw magnitudes .

We combine \(_{}\) with \(_{}\) in (9) for DNN training, using a weighting term \(\):

\[_{}=_{}+_ {}.\] (11)

### Training UNSSOR for monaural unsupervised separation

UNSSOR can be trained for monaural unsupervised separation by only feeding the mixture at the reference microphone to the DNN but still computing the loss on multiple microphones. Fig. 1 illustrates the idea. At run time, the trained system performs monaural under-determined separation, and multi-microphone over-determined mixtures are only required for DNN training. The loss computed at multiple microphones could guide the DNN to exploit monaural spectro-temporal patterns for separation, even in an unsupervised setup.

## 5 Experimental setup

We validate the proposed algorithms on two-speaker separation in reverberant conditions based on the six-channel SMS-WSJ dataset  (see Appendix A for its details). This section describes the baseline systems and evaluation setup. See Appendix I for miscellaneous system and DNN setup.

### Baselines

The baselines include conventional unsupervised separation algorithms, an improved version of RAS, MixIT, and supervised learning based models.

We include spatial clustering [46; 47; 48] for comparison. We use a public implementation3, which leverages complex angular-central Gaussian mixture models  for sub-band spatial clustering and exploits inter-frequency correlation of cluster posteriors [46; 64] for frequency alignment. The number of sources is set to three, one of which is used for garbage collection, following . After obtaining the estimates, we discard the one with the lowest energy. The STFT window size is tuned to \(128\) ms and hop size to \(16\) ms.

We include IVA [41; 44] for comparison. We use the public implementations provided by the _torchiva_ toolkit . We use the default spherical Laplacian model to model source distribution. In over-determined cases, the number of sources is set to three and we discard the estimate with the lowest energy, similarly to the setup in the spatial clustering baseline4. The STFT window size is tuned to \(256\) ms and hop size to \(32\) ms.

We propose a novel variant of the RAS algorithm  for comparison. Appendix H discusses the differences between UNSSOR and RAS. Since RAS cannot achieve unsupervised separation, we improve it by computing loss on multi-microphone mixtures, and name the new algorithm as improved RAS (iRAS). We employ the time-domain Wiener filtering (WF) technique in  to filter re-synthesized time-domain estimates \((c)=((c))\), where \((c)\) is produced by TF-GridNet. The loss is defined as:

\[_{}=_{p=1}^{P}_{p}_{ ,p}=_{p=1}^{P}_{p}\|_{1}} y_{p}-_{c=1}^{C}_{p}(c)*(c)_{1},\] (12)

with \(*\) denoting linear convolution, \(y_{p}\) the time-domain mixture at microphone \(p\), and \(_{p}(c)\) a time-domain Wiener filter computed by solving the following problem:

\[_{p}(c)=_{h_{p}(c)}\|y_{p}-h_{p}(c)*(c)\|_{2}^{2},\] (13)which is quadratic and has a closed-form solution. The separation result is computed as \(_{p}^{}(c)=_{p}(c)*(c)\). Following , we use \(512\) filter taps, and filter the future \(100\), the current, and the past \(411\) samples (i.e., non-causal filtering). We can also filter the current and the past \(511\) samples (i.e., causal filtering), and additionally experiment with a filter length (in time) same as the length of the FCP filters (see Appendix J).

For comparison, we also include MixIT , which requires using synthetic mixtures of mixtures.

We report the result of using supervised learning, where PIT  is used to address the permutation problem. This result can be viewed as a performance upper bound of unsupervised separation.

We use the same DNN and training configurations as those in UNSSOR for a fair comparison.

### Evaluation setup and metrics

We designate the first microphone as the reference microphone, and use the time-domain signal corresponding to \(X_{1}(c)\) of each speaker \(c\) for metric computation. The evaluation metrics include signal-to-distortion ratio (SDR) , scale-invariant SDR (SI-SDR) , perceptual evaluation of speech quality (PESQ) , and extended short-time objective intelligibility (eSTOI) . SI-SDR and SDR evaluate the sample-level accuracy of predicted signals, and PESQ and eSTOI are objective metrics of speech quality and intelligibility respectively. For all the metrics, the higher, the better.

## 6 Evaluation results

This section reports evaluation results on SMS-WSJ and compares the performance of various setups.

### Effectiveness of UNSSOR at promoting separation

Table 1 and 2 respectively report the results of using six- and three-microphone input and loss. After hyper-parameter tuning, in default we use the loss in (9) for DNN training, set \(I=19\) and \(J=0\) (defined below (6)) for FCP (i.e., causal FCP filtering with \(20\) taps), and set \(_{p}=1\) (meaning no

    & & & & Val. set &  \\ Row & Systems & \(I\) & \(J\) & Loss & SDR (dB) & SDR (dB) & SI-SDR (dB) & PESQ & eSTOI \\ 
0a & Mixture & - & - & - & \(0.1\) & \(0.1\) & \(0.0\) & \(1.87\) & \(0.603\) \\ 
1a & UNSSOR & \(19\) & \(0\) & \(_{}\) & \(12.5\) & \(11.9\) & \(10.2\) & \(2.61\) & \(0.735\) \\
1b & UNSSOR + Corr. based freq. align. & \(19\) & \(0\) & \(_{}\) & \(\) & \(\) & \(\) & \(\) & \(0.884\) \\
1c & UNSSOR + Oracle freq. align. & \(19\) & \(0\) & \(_{}\) & \(16.2\) & \(15.8\) & \(14.9\) & \(3.48\) & \(0.889\) \\ 
2a & UNSSOR & \(19\) & \(0\) & \(_{}\) & \(16.0\) & \(15.6\) & \(14.6\) & \(3.44\) & \(\) \\
2b & UNSSOR + Corr. based freq. align. & \(19\) & \(0\) & \(_{}\) & \(16.0\) & \(15.6\) & \(14.7\) & \(3.44\) & \(\) \\
2c & UNSSOR + Oracle freq. align. & \(19\) & \(0\) & \(_{}\) & \(16.0\) & \(15.6\) & \(14.7\) & \(3.44\) & \(0.886\) \\ 
3a & Spatial clustering + Corr. based freq. align.  & - & - & - & \(8.8\) & \(8.6\) & \(7.4\) & \(2.44\) & \(0.726\) \\
3b & IVA  & - & - & - & \(10.3\) & \(10.6\) & \(8.9\) & \(2.58\) & \(0.764\) \\
3c & iRAS w/ causal \(512\)-tap filters & - & \(_{}\) & \(7.8\) & \(7.6\) & \(5.7\) & \(2.14\) & \(0.642\) \\
3d & iRAS w/ non-causal \(512\)-tap filters (\(100\) future taps) & - & - & \(_{}\) & \(8.0\) & \(7.8\) & \(5.7\) & \(2.13\) & \(0.637\) \\ 
4a & PIT (supervised)  & - & - & - & \(19.9\) & \(19.4\) & \(18.9\) & \(4.08\) & \(0.949\) \\    _Notes: the rows shows in grey indicate using oracle information or using supervised models._

Table 1: Averaged results of 2-speaker separation on SMS-WSJ (6-channel input and loss).

    & & & & & Val. set &  \\ Row & Systems & \(I\) & \(J\) & Loss & SDR (dB) & SDR (dB) & SI-SDR (dB) & PESQ & eSTOI \\ 
0a & Mixture & - & - & - & \(0.1\) & \(0.1\) & \(0.0\) & \(1.87\) & \(0.603\) \\ 
1a & UNSSOR & \(19\) & \(0\) & \(_{}\) & \(9.9\) & \(9.4\) & \(7.4\) & \(2.12\) & \(0.672\) \\
1b & UNSSOR + Corr. based freq. align. & \(19\) & \(0\) & \(_{}\) & \(15.3\) & \(15.0\) & \(13.9\) & \(3.18\) & \(0.867\) \\
1c & UNSSOR + Oracle freq. align. & \(19\) & \(0\) & \(_{}\) & \(15.5\) & \(15.2\) & \(14.1\) & \(3.19\) & \(0.871\) \\ 
2a & UNSSOR & \(19\) & \(0\) & \(_{}\) & \(\) & \(\) & \(\) & \(\) & \(0.874\)weighting is applied for different microphones). For the 3-microphone case, we use the mixtures at the first, third, and fifth microphones for training and testing. Notice that for two-speaker separation, the cases with six or three microphones are both over-determined.

In both tables, from row 1a we observe that UNSSOR produces reasonable separation of speakers, improving the SDR from \(0.1\) to, for example, \(12.5\) dB in Table 1, but its output suffers from the frequency permutation problem (see Appendix D for an example). In row 1c, we use oracle target speech to obtain oracle frequency alignment and observe much better results over 1a. This shows the effectiveness of \(_{}\) at promoting separation of speakers and the severity of the frequency permutation problem. In row 1b, we use a frequency alignment algorithm (same as that used in the spatial clustering baseline) [46; 64] to post-process the separation results of 1a. This algorithm leads to impressive frequency alignment (see 1b vs. 1c), but it is empirical and has a complicated design.

### Effectiveness of ISMS loss at addressing frequency permutation problem

We train DNNs using \(_{}\) defined in (11). In each case (i.e, six- and three-microphone), we separately tune the weighting term \(\) in (11) based on the validation set. In both table, comparing row 2a-2c with 1a-1c, we observe that including \(_{}\) is very effective at dealing with the frequency permutation problem, yielding almost the same performance as using oracle frequency alignment.

### Results of training UNSSOR for monaural unsupervised separation

Table 3 and 4 use the mixture only at the reference microphone \(1\) as the network input, while computing the loss respectively on three and six microphones. We tune \(J\) to \(1\) (i.e., non-causal FCP filter), considering that, for a specific target speaker, the reference microphone may not be the microphone closest to that speaker5. See an interpretation on why non-causal filtering is needed in Appendix G.2. We still set the microphone weight \(_{p}\) to \(1.0\) for non-reference microphones (i.e., when \(p 1\)), but tune \(_{1}\) to a smaller value based on the validation set. Without using a smaller \(_{1}\), we found that the DNN easily overfits to microphone \(1\), as we use the mixture at microphone \(1\) as the only input and compute the MC loss also on the mixture at microphone \(1\). The DNN can just aggressively optimize \(_{,p}\) to zero at microphone \(1\) and not optimize that at other microphones.

From row 1a of both tables, strong performance is observed in this under-determined setup, indicating that the multi-microphone loss can inform the DNN what desired target sound objects are and the DNN can learn to model spectral patterns in speech for unsupervised separation. In addition, the result in 1a of Table 3 is better than that in Table 4. This indicates that using more microphones as constraints in the proposed MC loss can elicit better separation.

    & & & &  &  \\ Row & Systems & \(I\) & \(J\) & Loss & SDR (dB) & SDR (dB) & SI-SDR (dB) & PESQ & eSTOI \\ 
0a & Mixture & - & - & - & \(0.1\) & \(0.1\) & \(0.0\) & \(1.87\) & \(0.603\) \\ 
0a & Mixture & - & - & - & \(0.1\) & \(0.1\) & \(0.0\) & \(1.87\) & \(0.603\) \\ 
1a & UNSSOR & \(19\) & \(1\) & \(_{}\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ 
2a & iRAS w/ causal \(512\)-tap filters & - & - & \(_{}\) & \(-0.1\) & \(-0.3\) & \(-3.0\) & \(1.62\) & \(0.453\) \\
2b & iRAS w/ non-causal \(512\)-tap filters (\(100\) future taps) & - & - & \(_{}\) & \(11.0\) & \(10.7\) & \(9.9\) & \(2.81\) & \(0.783\) \\ 
3a & Monaural PIT (supervised)  & - & - & - & \(16.2\) & \(15.7\) & \(15.3\) & \(3.79\) & \(0.907\) \\   

Table 4: Averaged results of \(2\)-speaker separation on SMS-WSJ (\(1\)-channel input and \(3\)-channel loss).

    & & & &  &  \\ Row & Systems & \(I\) & \(J\) & Loss & SDR (dB) & SDR (dB) & SI-SDR (dB) & PESQ & eSTOI \\ 
0a & Mixture & - & - & - & \(0.1\) & \(0.1\) & \(0.0\) & \(1.87\) & \(0.603\) \\ 
1a & UNSSOR & \(19\) & \(1\) & \(_{}\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ 
2a & iRAS w/ causal \(512\)-tap filters & - & - & \(_{}\) & \(7.5\) & \(7.2\) & \(5.6\) & \(2.03\) & \(0.641\) \\
2b & iRAS w/ non-causal \(512\)-tap filters (\(100\) future taps) & - & - & \(_{}\) & \(10.7\) & \(10.5\) & \(9.7\) & \(2.80\) & \(0.778\) \\ 
3a & Monaural PIT (supervised)  & - & - & - & \(16.2\) & \(15.7\) & \(15.3\) & \(3.79\) & \(0.907\) \\   

Table 3: Averaged results of \(2\)-speaker separation on SMS-WSJ (\(1\)-channel input and \(6\)-channel loss).

### Comparison with other methods and supplementary results

In Table 1-4, we compare the performance of UNSSOR with spatial clustering, IVA, iRAS, and supervised PIT-based models. In Appendix J, we compare UNSSOR and iRAS when they use the same filter length (in time). UNSSOR shows better performance than previous unsupervised separation models that can be performed or trained directly on mixtures. A sound demo is available at this link6. UNSSOR is worse than supervised PIT but the performance is reasonably strong. For example, in row 2a of Table 2, UNSSOR obtains \(15.4\) dB SDR on the test set, which is close to the \(16.8\) dB result obtained by supervised PIT in 4a. In Appendix L, we compare the results of UNSSOR with MixIT  and observe better performance.

We think that the effectiveness of the proposed system comes from both the strong modeling capability of TF-GridNet and the UNSSOR mechanism itself. In Appendix K, we use UNSSOR with other separation models such as TCN-DenseUNet  and DPRNN , which are known to have weaker modelling capability than TF-GridNet in supervised separation tasks (see for example ). We observe that the separation performance is worse but still reasonable.

## 7 Limitations

Our study shows the strong potential of UNSSOR for unsupervised speech separation. There are, however, several weaknesses we need to address in future research. First, we assume that sources are directional point sources so that each relative RIR can be modelled using a short filter, and diffuse sources are not considered. Second, we assume that sources are non-moving within each utterance so that we can use time-invariant FCP filters. Third, we assume that the number of sources is known and the sources are fully-overlapped. Fourth, only measurement or modeling noise is considered and realistic directional or diffuse background noises with strong energy are not included. Although these assumptions are also made in many algorithms such as IVA, spatial clustering, RAS and iRAS, they need to be addressed to realize more practical and robust speech separation systems.

## 8 Conclusion

We have proposed UNSSOR for unsupervised neural speech separation. We show that it is possible to train unsupervised models directly on mixtures, if the mixtures are over-determined. We have proposed mixture-constraint loss functions, which leverage multi-microphone mixtures as constraints, to promote separation of speakers. We find that minimizing ISMS can alleviate the frequency permutation problem. Although UNSSOR requires over-determined training mixtures, it can be trained to perform under-determined unsupervised separation. Future research will combine UNSSOR with semi-supervised learning, evaluate it on real-recorded noisy-reverberant data such as CHiME-6 , AMI  and AliMeeting , and address the limitations described in Section 7.

In closing, we emphasize that a key scientific contribution of this paper is that the over-determined property afforded by having more microphones than speakers can narrow down the solutions to the underlying sources, and this property can be leveraged to design a supervision to train DNNs to model speech patterns via unsupervised learning and realize unsupervised separation. This meta-idea, we believe, would motivate the design of many algorithms in future research on neural source separation.