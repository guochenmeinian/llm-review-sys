# Homotopy-based training of NeuralODEs for accurate dynamics discovery

Joon-Hyuk Ko

Department of Physics & Astronomy

Seoul National University

Seoul, 08826, South Korea

jhko725@snu.ac.kr

Hankyul Koh

Department of Physics & Astronomy

Seoul National University

Seoul, 08826, South Korea

physics113@snu.ac.kr

Nojun Park

Department of Physics

Massachusetts Institute of Technology

MA, 02142, United States

bnj11526@mit.edu

Wonho Jhe

Department of Physics & Astronomy

Seoul National University

Seoul, 08826, South Korea

whjhe@snu.ac.kr

###### Abstract

Neural Ordinary Differential Equations (NeuralODEs) present an attractive way to extract dynamical laws from time series data, as they bridge neural networks with the differential equation-based modeling paradigm of the physical sciences. However, these models often display long training times and suboptimal results, especially for longer duration data. While a common strategy in the literature imposes strong constraints to the NeuralODE architecture to inherently promote stable model dynamics, such methods are ill-suited for dynamics discovery as the unknown governing equation is not guaranteed to satisfy the assumed constraints. In this paper, we develop a new training method for NeuralODEs, based on synchronization and homotopy optimization, that does not require changes to the model architecture. We show that synchronizing the model dynamics and the training data times the originally irregular loss landscape, which homotopy optimization can then leverage to enhance training. Through benchmark experiments, we demonstrate our method achieves competitive or better training loss while often requiring less than half the number of training epochs compared to other model-agnostic techniques. Furthermore, models trained with our method display better extrapolation capabilities, highlighting the effectiveness of our method.

## 1 Introduction

Predicting the evolution of a time varying system and discovering mathematical models that govern it is paramount to both deeper scientific understanding and potential engineering applications. The centuries-old paradigm to tackle this problem was to either ingeniously deduce empirical rules from experimental data, or mathematically derive physical laws from first principles. However, the complexities of systems of interest have grown so much that these traditional approaches are now often insufficient. This has led to a growing interest in using machine learning methods to infer dynamical laws from data.

One line of research stems from the dynamical systems literature, where the problem of interest was to determine the unknown coefficients of a given differential equation from data. While the task seems rather simple, it was found that naive optimization cannot properly recover the desired coefficientsdue to irregularities in the optimization landscape, which worsens with increasing nonlinearity and data length [64; 50; 56; 1].

A parallel development occurred in the study of recurrent neural networks (RNNs), which are neural network analogues of discrete maps. It was quickly found that training these models can be quite unstable and results in exploding or vanishing gradients. While earlier works in the field resulted in techniques such as teacher forcing or gradient clipping that are agnostic to the RNN architecture [14; 45], more recent approaches have been directed towards circumventing the problem entirely by incorporating specific mathematical forms into the model architecture to constrain the eigenvalues of the model Jacobian and stabilize RNN dynamics [4; 29; 9; 34; 42]. While these latter methods can be effective, for the purpose of discovering dynamical equations underlying the data, these approaches are inadequate as one cannot ascertain the hidden equations conform to the predetermined rigid mathematical structures - especially so if the dynamics to be predicted is incompatible with the constrained model architecture [57; 41].

In this work, we focus on Neural Ordinary Differential Equations (NeuralODEs) . These models are powerful tools in modeling natural phenomena, bridging the expressibility and flexibility of neural networks with the de facto mathematical language of the physical sciences. This has led to researchers amalgamating NeuralODEs with partial information about the governing equation to produce "grey-box" dynamics models [55; 71], and endowing NeuralODEs with mathematical structures that the system must satisfy [22; 18]. Despite their conceptual elegance, training NeuralODEs tend to result in long training times and sub-optimal results, a problem that is further exacerbated as the length of the training data grows [20; 17]. Similar to the case of RNNs, methods have been proposed to tackle the problem involve placing either strong [12; 25], or semi-strong constraints [17; 30] to the functional form the NeuralODE can take - something the underlying governing equation does not guarantee satisfying.

ContributionsWe introduce a method to accurately train NeuralODEs on long time series data by leveraging tools from the chaos and mathematical optimization literature: synchronization and homotopy optimization [1; 15]. Through loss landscape analyses, we show that longer training data lead to more irregular loss landscapes, which in turn lead to deteriorating performances of the trained models. We show that synchronizing NeuralODE dynamics with the data can smooth the loss landscape, on which homotopy optimization can be applied to enhance training. Through performance benchmarks, we show that not only does our model far outperform conventional gradient descent training, it also surpasses multiple shooting in terms of convergence speed and extrapolation accuracy.

## 2 Preliminaries

### Neural ordinary differential equations

A NeuralODE  is a model of the form,

\[}{dt}=(t,;), (t_{0})=_{0}.\] (1)

where \(_{0}^{n}\) is the initial condition or input given to the model, and \((...;):^{n} ^{n}\) is a neural network with parameters \(^{m}\) that governs the dynamics of the model state \((t)^{n}\). While this model can be used as a input-output mapping1, we are interested in the problem of training

Figure 1: Optimization trajectories for our homotopy method (**left**) and vanilla gradient descent (**right**). While convention training meanders on the pathological loss landscape, our method provides a series of relaxed landscapes that effectively guide the optimizer to the loss minimum.

NeuralODEs on time series data to learn the underlying governing equation and forecast future dynamics.

Given an monotonically increasing sequence of time points and the corresponding measurements \(\{t^{(i)},}^{(i)}\}_{i=0}^{N}\), NeuralODE training starts with using an ordinary differential equation (ODE) solver to numerically integrate Equation (1) to obtain the model state \(\) at given time points:

\[^{(i)}()=^{(0)}()+ _{t^{(0)}}^{t^{(i)}}(t,;)dt=((t,;),t^{(i)},_{0} ).\] (2)

Afterwards, gradient descent is performed on the loss function \(()=_{i}^{(i)} ()-}^{(i)}^{2}\) to arrive at the optimal parameters \(^{*}\).

### Homotopy optimization method

In topology, _homotopy_ refers to the continuous deformation of one function into another. Motivated by this concept, to find the minimizer \(^{*}^{m}\) of a complicated function \(()\), the homotopy optimization method3[67; 68; 3; 15] introduces an alternative objective function

\[(,)\ =\ \{ (),&if&=1\\ (),&if&=0.\] (3)

where \(()\) is an auxillary function whose minimum is easily found, and \((,):^{m} \) smoothly interpolates between \(\) and \(\) as the homotopy parameter \(\) varies from 1 to 0. While a commonly used construction is the convex homotopy \((,)=(1-)()+()\), more problem-specific forms can be employed.

Homotopy optimization shines in cases where the target function \(\) is non-convex and has a complicated landscape riddled with local minima. Similarly to simulated annealing , one starts out with a relaxed version of the more complicated problem at hand and finds a series of approximate solutions while slowly morphing the relaxed problem back into its original non-trivial form. This allows the optimization process to not get stuck in spurious sharp valleys and accurately converge to the minimum of interest.

### Synchronization of dynamical systems

Here, we briefly summarize the mathematical phenomenon of synchronization, as described in the literature [51; 28; 1; 49]. In later sections, we will show that synchronization can be used mitigate the difficulties arising in NeuralODE training. Consider two states \(},^{n}\), each evolving via

\[}}{dt}=}(t,};}),}{dt}=(t,; )-(-}).\] (4)

Here, \(},:^{n}^{n}\) are the dynamics of the two systems parameterized by \(},\), respectively, and \(=diag(k_{1},...,k_{n})\), is the diagonal coupling matrix between the two systems. The two systems are said to synchronize if the error between \(}(t)\) and \((t)\) vanishes with increasing time.

**Theorem 1** (Synchronization).: _Assuming \(=}\), \(}\), and \((t)\) in the vicinity of \(}(t)\), the elements of \(\) can be chosen so that the two states synchronize: that is, for error dynamics \((t)=(t)-}(t)\), \(||(t)||_{2}=||(t)-}(t)||_{2} 0\) as \(t\)._

Proof.: By subtracting the two state equations, the error, up to first order terms, can be shown to follow:

\[(t)}{dt}=(t)}{dt}-}(t)}{dt}=[(,;)}{}]_{=}}- ](t)=(t).\] (5)

Increasing the elements of \(\) causes the matrix \(\) to become negative definite --which in turn, causes the largest Lyapunov exponent of the system,

\[_{t}(t)||} {||(0)||}.\] (6)to also become negative as well. Then \(||(t)||_{2}=||(t)-}(t)||_{2} 0\) as \(t\), which means the dynamics of \(\) and \(}\) synchronize with increasing time regardless of the parameter mismatch. 

**Remark**.: _Our assumption of \(=}\), that the functional form of the data dynamics is the same as the equation to be fitted can also be thought to hold for NeuralODE training. This is because if the model is sufficiently expressive, universal approximation theorem will allow the model to match the data equation for some choice of model parameter._

#### 2.3.1 Constructing homotopy with synchronization

To combine homotopy optimization with synchronization, we slightly modify the coupling term of Equation (4) by multiplying it with the homotopy parameter \(\):

\[}{dt}=(t,;)- (-}).\] (7)

With this modification, homotopy optimization is introduced to the problem as follows. When \(=1\) and the coupling matrix \(\) has sufficiently large elements, synchronization occurs and the NeualODE prediction \((t)\) starts to converge to the data trajectory \(}(t)\) for large \(t\). As we will confirm in Section 4, this causes the resulting loss function \((,1)\) to become well-behaved, serving the role of the auxillary function \(\) in Equation (3). When \(=0\), the coupled Equation (4) reduces to the original model form of Equation (1). Accordingly, the corresponding loss function \(()=(,0)\) is the complicated loss function we need to ultimately minimize. Therefore, starting with \(=1\) and successively decreasing its value to 0, all the while optimizing for the coupled loss function \((,)\) allows one to leverage the well-behaved loss function landscape from synchronization while being able to properly uncover the system parameters [66; 58].

## 3 Related works

Synchronization in ODE parameter estimationAfter the discovery of synchronization in chaotic systems by Pecora & Carroll [47; 48], methods to leverage this phenomenon to identify unknown ODE parameters were first proposed by Parlitz , and later refined by Abarbanel et al. [2; 1]. Parameter identification in these earlier works would proceed by either designing an auxillary dynamics for the ODE parameters [44; 40], or including a regularization term on the coupling strength in the loss function [53; 1]. The idea of applying homotopy optimization to this problem was pioneered by Vyasarayani et al. [65; 66] and further expanded upon by later works [58; 63; 52].

Homotopy methods in machine learningAs homotopy methods can outperform gradient descent for complex optimization problems, it has been used in diverse fields, including Gaussian homotopy for non-convex optimization  and training feedforward networks [13; 24; 10]. Curriculum learning  is another example of homotopy optimization, with the each task in the curriculum corresponding to a particular value of the homotopy parameter as it is decreased to 0. Not to be confused with the method discussed in this paper is the homotopy perturbation technique  used in the dynamical systems literature, which is a method to obtain analytical solutions to nonlinear differential equations.

Improving the training of RNNs and NeuralODEsAs previously mentioned, many recent approaches to improve RNN and NeuralODE training involve constraining the model expressivity. The limitations of this strategy is investigated in , where it is demonstrated that such "stabilized" models cannot learn oscillatory or highly complex data that does not match the built-in restrictions. A recent development with a similar spirit as ours is Mikhaeil et al. , where the authors developed an architecture-agnostic training method for RNNs by revisiting the classical RNN training strategy of teacher forcing  and refining it via the chaos theory concept of Lyapunov exponents. Other aspects of NeuralODE training, such as better gradient calculation, have been studied to improve NeuralODE performance [30; 72; 31; 31]. Such methods are fully compatible with our proposed work, and we showcase an example in Appendix G, by changing the gradient calculation scheme to the "symplectic-adjoint-method" from .

Loss landscapes in learning ODEs

### Complicated loss landscapes in ODE parameter estimation

NeuralODEs suffer from the same training difficulties of ODE fitting and RNNs. Here we illustrate that the underlying cause for this difficulty is also identical by analyzing the loss landscape of NeuralODEs for increasing lengths of training data. The data were generated from the periodic Lotka-Volterra system4 and the landscapes were visualized by calculating the two dominant eigenvectors of the loss Hessian, then perturbing the model parameter along those directions . We also provide analogous analyses for ODE parameter estimation on the same system in Appendix B to further emphasize the similarities between the problem domains.

From Figure 2 (upper left), we find that the loss landscape is well-behaved for short data, but becomes pathological as the data lengthens. In fact, the landscape for \(t[0,9.1]\) features a stiff cliff, which is known to cause exploding gradients in RNN training [14; 45]. Indeed, this picture is also confirmed by an alternative principal component analysis-based visualization of the loss landscape and the optimization path 5 (Figure 1, right). We also observe that the overall magnitude of the loss increases with data length. This is a direct consequence of model dynamics \((t)\) being independent of the data dynamics \(}(t)\) and therefore tending to diverge away with time.

To further confirm our findings and compensate for the fact that our visualizations are low dimensional projections of the actual loss landscape, we computed the eigenvalue spectrum of the loss function hessian for increasing lengths of training data (Figure 2, lower left). The computed spectrums show that hessian eigenvalues spread away from zero with increasing length of training data, signalling that the loss morphs from a relatively flat to a more corrugated landscape. We also observe large positive eigenvalue outlier peaks, whose values also rapidly increase with the length of training data. As such large outliers have been report to slow down training , this also provides additional insights as to why NeuralODE training stagnates on long time series data.

### Loss landscape of synchronized dynamics

With coupling, the loss function \(\) between the model predictions and the data now becomes a function of both the model parameters \(\) and the coupling strength \(k\). Increasing the coupling strength \(k\) effectively acts as shortening the data because increased coupling causes the two trajectories to start synchronizing earlier in time, after which the error between the two will diminish and contribute negligibly to the total loss. Therefore, synchronizing the model and the data generating equations leads to a more favorable loss landscape that gradient descent based optimizers will not struggle on.

Figure 2: Irregular loss landscape in NeuralODE training. **(Upper left)** Loss landscape and **(Lower left)** eigenvalue spectrum of the loss Hessian for increasing lengths of train data. **(Upper right)** Loss landscape and **(Lower right)** eigenvalue spectrum of the loss Hessian with increasing coupling strength. For clarity, loss values were clipped above at 4000. All Hessian related information was calculated using the PyHessian package .

Once again, we confirm this point by visualizing the loss landscape and the Hessian eigenvalue spectrum for a long time series dataset while increasing the strength of the coupling. From Figure 2 (upper right) we find that with increasing coupling strength, the steep cliff and the flat basin of the original loss surface are tamed into a well-behaved slope. This is reflected in the eigenvalue spectrum as well (Figure 2, lower right), with the all of the spectrum gathering near zero as the coupling is turned up. In fact, when the coupling strength is increased too much, we find that all the eigenvalues bunch together at zero, meaning that the loss function becomes completely flat. Qualitatively, this corresponds to the case where the coupling completely dominates the synchronized dynamics and forces the model predictions onto the data regardless of the model parameters.

## 5 Homotopy optimization for NeuralODE training

Having established that applying synchronization between the model and data can tame the irregular loss landscape during training, we now turn towards adapting the synchronization-homotopy approach of Section 2.3.1 to a NeuralODE setting. In doing so, there are couple details that needs addressing to arrive at a specific implementations. We discuss such points below.

### Constructing the coupling term

The coupling term for synchronization \(-(-})\) in Equation (7) depends on the data dynamics \(}\), whose values are only available at discrete timestamps \(\{t^{(i)}\}_{i=0}^{N}\). This is at a disaccord with the ODE solvers used during training, which require evaluating the model at intermediate time points as well. Therefore, to construct a coupling term defined for arbitrary time, we used a cubic smoothing spline  of the data points to supply the \(}\) values at unobserved times. Smoothing was chosen over interpolation to increase the robustness of our approach to measurement noise.

Note that cubic spline is not the only solution - even defining the coupling term as a sequence of impulses is possible, only applying it where it is defined, and keeping it zero at other times. This is an interesting avenue of future research, as this scheme can be shown  to be mathematically equivalent to teacher forcing .

### A little more on the coupling matrix

In general, the coupling matrix \(\) can be any \(n n\) matrix, provided it renders the Jacobian of the error dynamics (matrix \(\) in Equation (6)) to be negative definite along the data trajectory \(}(t)\). However, naively following this approach leads to \(n^{2}\) hyperparameters just for the coupling matrix, which can quickly get out of hand for high dimensional systems. To circumvent this issue, in this work we have used a coupling matrix of the form \(=k_{n}\), where \(_{n}\) is the \(n n\) identity matrix, to arrive at single scalar hyperparameter characterizing coupling strength.

### Scheduling the homotopy parameter

The homotopy parameter \(\) can be decreased in various different ways, the simplest approach being constant steps. In our study, we instead used power law decrements of the form \(\{^{(j)}\}_{j=0}^{n_{step}-1}=^{j}/_{j=0}^{n_{step}-1} ^{j}\) where \(^{(j)}\) denotes the \(j\)-th decrement and \(n_{step}\) is number of homotopy steps. This was inspired from our observations of the synchronized loss landscape (Figure 2, right), where the overall shape change as well as the width of the eigenvalue spectrum seemed be a superlinear function of the coupling strength.

### Hyperparameter overview

Our final implementation of the homotopy training algorithm has five hyperparameters, as described below.

Coupling strength (\(k\))This determines how trivial the auxillary function for the homotopy optimization will be. Too small, and even the auxillary function will have a jagged landscape; too large, and the initial auxillary function will become flat (Figures 2 and 11, left panel, \(k=1.0,1.5\)) resulting in very slow parameter updates. We find good choices of \(k\) tend to be comparable to the scale of the measurement values.

Homotopy parameter decrement ratio (\(\))This determines how the homotopy parameter \(\) is decremented for each step. Values close to 1 cause \(\) to decrease in nearly equal decrements, whereas smaller values cause a large decrease of \(\) in the earlier parts of the training, followed by subtler decrements later on. We empirically find that \(\) values near 0.6 tend to work well.

Number of homotopy steps (\(s\))This determines how many relaxed problem the optimization process will pass through to get to the final solution. Similar to scheduling the temperature in simulated annealing, fewer steps results in the model becoming stuck in a sharp local minima, and too many steps makes the optimization process unnecessarily long. We find using values in the range of 6-8 or slightly larger values for more complex systems yields satisfactory results.

Epochs per homotopy step (\(n_{epoch}\))This determines how long the model will train on a given homotopy parameter value \(\). Too small, and the model lacks the time to properly converge on the loss function; too large, and the model overfits on the simpler loss landscape of \( 0\), resulting in a reduced final performance when \(=0\). We find for simpler monotonic or periodic systems, values of 100-150 work well; for more irregular systems, 200-300 are suitable.

Learning rate (\(\))This is as same as in conventional NeuralODE training. We found values in the range of 0.002-0.1 to be adequate for our experiments.

## 6 Experiments

To benchmark our homotopy training algorithm against existing methods, we trained multiple NeuralODE models across time-series datasets of differing complexities. The performance metrics we used in our experiments were mean squared error on the training dataset and on model rollouts of 100% the training data length, which from now on we refer to as interpolation and extrapolation MSE, respectively. For each training method, the hyperparameters were determined through a grid search performed prior to the main experiments. All experiments were repeated three times with different random seeds, and we report the corresponding means and standard errors whenever applicable. For conciseness, we outline the datasets, models and the baseline methods used in the subsequent sections, and refer interested readers to Appendices E and F for extensive details on data generation, model configuration and hyperparameter selection. We provide all of the code for this paper in https://github.com/Jhko725/NeuralODEHomotopy.

### Datasets and model architectures

Lotka-Volterra systemThe Lotka-Volterra system is a simplified model of prey and predator populations . The system dynamics are periodic, meaning even models trained on short trajectories should have strong extrapolation properties, provided the training data contains a full period of oscillation. We considered two types of models for this system: a "black-box" model of Equation (1), and a "gray-box" model used in  given by \(= x+U_{1}(x,y;_{1}),\ =- y+U_{2}(x,y; _{2})\).

Double pendulumFor this system, we used the real-world measurement data from Schmidt & Lipson . While the double pendulum system can exhibit chaotic behavior, we used the portion of the data corresponding to the aperiodic regime. We trained two types of models on this data: a black-box model and a model with second-order structure. Our motivation for including the latter model stems from the ubiquity of second-order systems in physics and from , which reports these models can match the performance of Hamiltonian neural networks.

Lorenz systemThe Lorenz system displays highly chaotic behavior, and serves as a stress test for time series prediction tasks. For this system, we only employed a black-box model.

### Baselines

As the focus of this paper is to effectively train NeuralODEs without employing any specialized model design or constraining model expressivity, this was also the criteria by which we chose the baseline training method for our experiments. Two training methods were chosen as our baseline: vanilla gradient descent, which refers to the standard method of training NeuralODEs, and multiple-shooting method. As the latter also stems from the ODE parameter estimation literature, and its inner working relevant to the discussion of our results, we briefly describe the multiple-shooting algorithm before discussing our results.

Multiple shootingThe multiple-shooting algorithm [62; 8; 5; 64] circumvents the difficulty of fitting models on long sequences by dividing the training data into shorter segments and solving multiple initial value problems. To keep the predictions for different segments in sync, the loss function is augmented with a continuity enforcing term. Unlike synchronization and homotopy, which we employ in NeuralODE training for the first time, works applying multiple shooting to NeuralODEs are scant but do exist [54; 38; 61]6. As the three previous references differ in their implementations, we chose the scheme of  for its simplicity and implemented a PyTorch adaptation to use for our experiments. Further details about our multiple-shooting implementation can be found in Appendix D.

## 7 Results

### Training NeuralODEs on time series data of increasing lengths

In the previous sections, we found that longer training data leads to a more irregular loss landscape. To confirm that this does translate to deteriorated optimization, we trained NeuralODEs on Lotka-Volterra system trajectories of different lengths. Inspecting the results (Figure 3, first panel) we find while all training methods show low interpolation MSE for shortest data, this is due to overfitting, as evidenced by the diverging extrapolation MSE7. As the length of the data grows and encompasses a full oscillation period, we find that the model performances from homotopy and multiple-shooting stabilize for both interpolation and extrapolation. This is clearly not the case for the vanilla gradient descent, whose model performances degrade exponentially with as data length increases.

Model capacity is not the problemThe reason NeuralODEs struggle on long sequences can easily be attributed to insufficient model capacity. We demonstrate this is not the case by repeating the previous experiment for the mid-length data while decreasing the model size used (Figure 3, second panel). In the case of vanilla gradient descent, the model size is directly correlated to its performance,lending the claim some support. However, we find that both homotopy and multiple shooting methods can effectively train much smaller models to match the accuracy of a larger model trained with vanilla gradient descent. This signifies that the cause of the problem is not the capacity itself, but rather the inability of training algorithms to fully capitalize on the model capacity.

Robustness against data degradationWe performed an additional set of experiments to evaluate the performances of different training methods against increasing data sparsity and noise. This is particularly of interest for our homotopy training as we employ smoothing cubic splines (Section 5.1) whose reliability can be adversely affected by the data quality. From the results (Figure 3, third, fourth panels) we find this is not the case and that that our algorithm performs well even for degraded data. One unexpected result is the drastic underperformance of the multiple shooting method with increased sampling period, which is a side effect from the fact we held the time interval fixed for this experiment. Due to the space constraint, we continue this discussion in Appendix G.3.

### Performance benchmarks of the training algorithms

Having confirmed that both the homotopy and multiple-shooting methods successfully mitigate the NeuralODE training problem, we now present a comprehensive benchmark in Figure 4.

From the results, it is clear both our homotopy method and multiple shooting can both boost the interpolation performance of NeuralODEs, with our method being slightly more performant. Inspecting the extrapolation performance, however, we do find a clear performance gap between our method and multiple shooting, with the latter even performing worse than vanilla gradient descent for the double pendulum dataset (Figure 4, center, Figure 5, left). This underperformance of multiple shooting for extrapolation tasks can be attributed to the nature of the algorithm: during training time, multiple shooting integrates NeuralODEs on shorter, subdivided time intervals, whereas during inference time the NeuralODE needs to be integrated for the entire data time points and then some more for extrapolation. This disparity between training and inference re

Figure 4: Performance benchmarks. **(Left)** Interpolation and **(Center)** extrapolation MSEs for each training method. **(Right)** Epochs where the minimum interpolation MSE was achieved. For all plots, the bar and the error bars denote the mean and standard errors across three runs.

Figure 3: Results for the Lotka-Volterra system. **(First)** Interpolation and extrapolation errors for increasing data length. **(Second)** Errors for the mid-length data with decreasing model capacity. **(Third)** Errors for fixed time interval but with increasing data sparsity. **(Fourth)** Errors for mid-length data with increasing noise.

multiple shooting to be less likely to compensate for error accumulation in long term predictions, resulting in poor extrapolation performance.

Another stark difference between our homotopy training and the other methods is in the number of epochs required to arrive at minimum interpolation MSE (Figure 4, right panel). While the exact amount of speedup is variable and seems to depend on the dataset difficulty, we find a 90%-40% reduction compared to vanilla training, highlighting the effectiveness of our approach. One unexpected observation from our experiments was that the epoch at which minimum interpolation MSE epoch was achieved did not always correspond to the homotopy parameter \(\) being zero (Appendix G.2). While seemingly counterintuitive, this is in line with the literature, where nonzero homotopy parameter has resulted in better generalization performances .

What makes homotopy training effective: inspecting training dynamicsTo obtain a complementary insight to the NeuralODE training dynamics and relate the effectiveness of our method with the properties of the loss landscape, we calculated the trace of the loss hessian during training (Figure 5, right panel). Small, positive trace values indicate more favorable training dynamics , and we find that our method results in the smallest trace, followed by multiple shooting then vanilla gradient descent. The fact that the hessian trace maintains a low value throughout the entire homotopy training, even as the loss surface is morphed back to its unsynchronized, irregular state, also confirms that our homotopy schedule works well and managed to gradually guide the optimizer into a well performing loss minimum.

## 8 Limitations and discussion

In this paper, we adapted the concepts of synchronization and homotopy optimization for the first time in the NeuralODE literature to stabilize NeuralODE training on long time-series data. Through loss landscape analysis, we established that the cause of the training problem is identical between differential equations, RNNs and NeuralODEs and that synchronization can effectively tame the irregular landscape. We demonstrated that our homotopy method can successfully train NeuralODE models with strong interpolation and extrapolation accuracies.

Note that our work serves as an initial confirmation that the methods developed in the ODE parameter estimation literature can be adapted to be competitive in the NeuralODE setting. As such, there are multiple opportunities for future research, including theoretical studies on the convergence properties of our algorithm and further testing out ideas in the synchronization literature, such as alternative parametrization for the coupling matrix , or incorporating the homotopy parameter into the loss function as a regularization term . Extending our algorithm to high-dimensional systems is another important direction, which we believe will be possible through the use of latent ODE models . Nevertheless, our work hold merit in that it lays the groundwork upon which ideas from ODE parameter estimation, control theory, and RNN/NeuralODE training can be assimilated, which we believe will facilliate further research connecting these domains.

#### Author Contributions

J. Ko and H. Koh contributed equally to the paper.

Figure 5: Second-order NeuralODE training results for the double pendulum dataset. **(Left)** Predicted trajectories. Lines and bands correspond to mean and standard errors for three training runs and the dashed line indicate the extrapolation start time. **(Right)** Estimation of the hessian trace during training. The homotopy curve stops early due to the difference in the number of maximum training curves used.

#### Acknowledgments

This work was supported by grants from the National Research Foundation of Korea (No. 2016R1A3B1908660) to W. Jhe.