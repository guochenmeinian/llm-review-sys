# DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph

Zhehao Zhang

Dartmouth College

zhehao.zhang.gr@dartmouth.edu &Jiaao Chen

Georgia Institute of Technology

jiaachen@gatech.edu &Diyi Yang

Stanford University

diyiy@cs.stanford.edu

###### Abstract

The current paradigm of evaluating Large Language Models (LLMs) through static benchmarks comes with significant limitations, such as vulnerability to data contamination and a lack of adaptability to the evolving capabilities of LLMs. Therefore, evaluation methods that can adapt and generate evaluation data with controlled complexity are urgently needed. In this work, we introduce **D**ynamic Evaluation of LLMs via **A**daptive **R**easoning **G**raph **E**volvement (DARG) to dynamically extend current benchmarks with controlled complexity and diversity. Specifically, we first extract the reasoning graphs of data points in current benchmarks and then perturb the reasoning graphs to generate novel testing data. Such newly generated test samples can have different levels of complexity while maintaining linguistic diversity similar to the original benchmarks. We further use a code-augmented LLM to ensure the label correctness of newly generated data. We apply our DARG framework to diverse reasoning tasks in four domains with 15 state-of-the-art LLMs. Experimental results show that almost all LLMs experience a performance decrease with increased complexity and certain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit more biases when being evaluated via the data generated by DARG with higher complexity levels. These observations provide useful insights into how to dynamically and adaptively evaluate LLMs. The code is available at https://github.com/SALT-NLP/DARG.

## 1 Introduction

Large language models (LLMs) have recently attained exceptional performance across a wide range of tasks  by showing substantial evaluation results on static benchmark datasets  where their test data points are open-sourced and unchanged. Although these widely used benchmarks are generally of high-quality, they may suffer from the following issues : (1) **Data contamination**, which refers to the potential overlap between LLMs' training corpus and benchmarks' data points. This raises concerns about whether LLMs are merely memorizing and overfitting these benchmarks instead of learning how to solve the tasks , which may lead to poor generalization. (2) Static datasets only have **fixed complexity** and lack the flexibility to evolve. As LLMs are developing and scaling up rapidly, existing static benchmarks may fail to align with their increasing capabilities, as the complexity of current benchmarks remains unchanged .

To address these issues, prior work has introduced template-based methods  to generate evaluation samples with different complexities for mathematical and logical reasoning tasks. However, these rule-based generated samples are synthetic and limited to a specific set of tasks, lacking linguisticdiversity compared to existing benchmarks. Another line of work involves prompting LLMs to directly modify the current evaluation data such as DyVal 2  and Benchmark Self-Evolving  which utilize LLMs with various prompting strategies to perturb existing data. Despite better adaptation to existing benchmarks, these methods usually have low controllability and suffer from LLMs' instability, which makes it difficult to verify the quality and correctness of the newly generated data points. Therefore, it remains a challenge to **dynamically and adaptively generate novel test samples with controlled complexity and diversity**.

To fill in this gap, in this work, we propose DARG, a **D**ynamic Evaluation of LLMs via **A**daptive **R**easoning **G**raph. Unlike previous work that generates test data through templates or designed prompts , we evolve existing benchmarks based on the **reasoning1 graphs** that represent the underlying structures of basic reasoning components necessary for problem-solving. Specifically, we first construct the reasoning graphs for data points in given benchmarks using LLMs (e.g., computational reasoning graphs for solving a math problem are shown in Figure 1). Next, we perform fine-grained graph perturbations based on various dimensions of the reasoning graph. As illustrated in the middle of Figure 1, we can dynamically increase the graph complexity by increasing its depth, width, and the numerical complexity of node values. Afterwards, we convert the reasoning graph back into the description that adapts the linguistic diversity as the original data. In order to ensure the correctness of the reasoning graph construction and graph-to-text generation, inspired by recent advances in tool-augmented LLMs , we use tool-augmented LLMs to verify the quality of reasoning graphs and generated text to produce valid test examples. In this way, novel test cases can be generated with controllable complexity, adapted linguistic diversity, and validated labels.

We evaluate 15 of the latest state-of-the-art (SOTA) LLMs with examples generated from our DARG on reasoning tasks across four different domains: math reasoning, social reasoning, spatial reasoning, and symbolic reasoning. We observe that: (1) All current LLMs show decreasing performances on these data generated by DARG with increasing complexity levels, demonstrating the unreliable assessment of LLMs' capabilities using static benchmarks and the need to evaluate LLMs dynamically and adaptively. (2) Additionally, in tasks involving social and spatial reasoning, we find an increase in biases reflected by LLMs as the complexity rises. (3) In general, larger models and mixture-of-experts (MOE) models with more active parameters demonstrate greater resistance to the changes in complexity, compared to smaller or non-MOE models. However, in tasks such as social reasoning, these powerful models such as GPT-4 Turbo and Gemini-1.5-Pro, have exhibited increased sensitivity to content involving protected groups as the complexity increases. In

Figure 1: Overview of our proposed DARG framework. We first use an LLM to construct internal reasoning graphs with rule-based supervision for label consistency. After that, we augment benchmarks through fine-grained graph interpolation based on different complexity dimensions. Finally, we decode the graph back into the original data format and use a code-augmented LLM agent to verify the label’s correctness.

summary, DARG sheds light on how to dynamically and adaptively evaluate LLMs and highlights the importance of developing better models that can adapt to diverse and dynamic evaluation scenarios.

## 2 Method: DARG

DARG aims to evolve the given test data into a novel example with controllable complexities, as shown in 1. Concretely, we will first extract the reasoning graph (Section 2.1, Section 2.2) for the given data. Subsequently, we conduct fine-grained graph perturbations to evolve the complexity of the reasoning graphs (Section 2.3 and then convert the graph into natural language descriptions that match the format of original data (Section 2.4).

### Reasoning Graph

The human problem-solving process can be conceptualized as a graph structure, where each vertex represents a partial solution and the edges represent the operators among them . Inspired by this, we represent each data in the form of a **Reasoning Graph**. Specifically, for a reasoning task, we define a reasoning graph, \(G^{R}=(V^{R},E^{R})\), which is a directed acyclic graph. The nodes \(v_{i} V^{R}\) represent the basic reasoning units, for example, numbers for math reasoning tasks. The edges \(e_{i,j} E^{R}\) represent the functions involved between the connected nodes, e.g., arithmetic operators for math reasoning tasks. A connection from \(v_{i}\) to \(v_{j}\) with edge \(e_{i,j}\) represents a partial solution to the problem where the operator \(e_{i,j}\) is applied to \(v_{i}\) to derive \(v_{j}\).

To quantify the complexity of the reasoning graph, we utilize (1) the **structural complexity** of the reasoning graph, including the _width of the graph_, which measures the maximum number of variables required to maintain in parallel during reasoning and _depth of the graph_ which measures the maximum level of reasoning steps required to solve the task; and (2) property and setup **complexity of nodes** in the reasoning graph, such as the numerical values of the nodes in math reasoning graphs. Based on the defined complexity measurements, we could then apply perturbations to vary the complexity of any given reasoning graph, such as increasing the numerical values of nodes or adding edges and nodes to increase the graph width and graph depth 2.

In this work, we use four widely used reasoning tasks including math reasoning, social reasoning, spatial reasoning, and symbolic reasoning as working examples, and the specific setup for nodes, edges, and complexity along with the example reasoning graphs are shown in Table 1. Note that even if the specific setups are different for different tasks, our reasoning graph definition can be easily applied and generalized to any given reasoning task.

### Reasoning Graph Construction

As current LLMs demonstrate increasing proficiency in in-context learning (ICL) [10; 70; 23], we leverage LLM with in-context exemplars to construct the reasoning graph for each data point. In the prompt, we manually define the nodes, edges, and their relationships with concrete examples and clear instructions as shown in Appendix F. However, constructing accurate reasoning graphs through

  
**Domain** & **Dataset** & **Node Definition** & **Edge Definition** & **Complexity** & **Example** \\  Math Reasoning & GSMSK  & Numbers & \(\{+,-,,,\}\) &  \# of digits in calculation \\ Width; Depth of calculations \\  & Fig. 1 \\  Social Reasoning & BBQ  & Persons, Attributes & Relations: ‘has’ & Attributes’ polarity & Fig. 18 \\  Spatial Reasoning & BBH Navigate  & Unit action & Sequential order & \# of actions & Fig. 11b \\  Symbolic Reasoning & BBH Dyck Language  & \(\{,\},\|,()\) & Sequential order & 
 \# of brackets in the input \\ \# of brackets in the label \\  & Fig. 11a \\   

Table 1: Overview of the tasks and reasoning domains investigated, along with their corresponding graph components, complexity definitions, and illustrative examples.

simple prompt engineering is non-trivial. Empirically, we find that even the most powerful model, GPT-4 Turbo, cannot accurately generate reasonable reasoning graphs for many arithmetic problems in one shot, even when using self-correction techniques [65; 105]. To resolve this instability, as shown in the leftmost part of Figure 1, we apply a rule-based function to use the graph structure to compute a label. This label is subsequently compared to the original label to verify the accuracy of the reasoning graph. If the computed label matches the original one, we consider the generated reasoning graph as accurate 3. Otherwise, we iteratively prompt the LLM using a high temperature until the computed label aligns with the original one.

### Reasoning Graph Perturbation

Reasoning graph perturbation involves systematically changing the structure of the reasoning graph based on different levels of complexity. Formally, for a given reasoning graph \(G^{R}=(V^{R},E^{R})\), we define a perturbation function \(P(G^{R},L,I)\), where \(L\) denotes the types of complexity and \(I\) represents the selected intervals. Inspired by DyVal's  approach to inject complexity, we use a rule-based function to modify the reasoning graph. This perturbation function \(P\) adjusts the nodes \(V^{R}\) and edges \(E^{R}\) according to the defined complexity and intervals, resulting in a new reasoning graph \(G^{R}_{p}\). For example, as illustrated in the middle part of Figure 1, we define a perturbation function \(P\) to alter the original reasoning graph to increase its structural complexity, including width and depth, and the node complexity such as numerical complexity of the nodes' values. Upon obtaining the modified graph, we apply the same label computation function as in the previous stage to determine the new label for this graph. Note that as we only use rule-based functions for graph interpolation without engaging LLMs, this stage does not introduce any noise.

### Testing Example Generation

**Graph-to-text Decoding** Prior work that uses template-based graph-to-text transformation  often suffers from limited linguistic diversity and lacks similarity to the original data point. In contrast, we use an LLM with original _(graph, text)_ pairs as in-context exemplars to conduct ICL for graph-to-text decoding. Specifically, given a reasoning graph \(G^{R}=(V^{R},E^{R})\) and an original text \(T\), we select \(k\) exemplars \(\{(G^{R}_{1},T_{1}),,(G^{R}_{k},T_{k})\}\) to guide the LLM in generating new text \(T^{}\). In this way, we can generate new data points that not only maintain a consistent language style but also encode the reasoning graph structure in the text in a similar manner.

**Data Verification** However, LLMs are notorious for their instability  and hallucinations [31; 44; 38]. Therefore, ensuring that the generated text aligns with the reasoning graph is critical. Inspired by recent advances in tool-augmented LLMs [106; 69; 28; 114; 86; 61], augmenting LLMs with tools such as code interpreters can significantly mitigate these hallucinations, thereby enhancing factuality and performance. For instance, GPT-4 equipped with a code interpreter has achieved a 97% accuracy on the GSM8K benchmark . Specifically, given a newly generated text \(T^{}\) from the reasoning graph \(G^{R}\), as illustrated in the rightmost of Figure 1, we use a code-augmented LLM agent that takes \(T^{}\) as input, generates code to solve the reasoning task, and utilizes an external code interpreter to compute the final answer \(A^{}\). We then compare this computed answer \(A^{}\) with the label \(A\) derived from the reasoning graph \(G^{R}\). If \(A^{}=A\), we consider the new data point correctly generated. If not, we iteratively provide the solving process and code output back to the LLM to refine its generation of new data points. Empirically, we find that using the code and code output as supervision signals significantly helps the LLM in reducing hallucinations during new data generation. All those prompt designs for graph generation and verification can be found in Appendix F

## 3 Experiment

For experiments, we use the following categories of LLMs4: (1) **Open-source vanilla transformer-based decoder-only LLMs**: phi3-mini ; Mistral-7B ; Llama-3-8B ; Llama-3-70B ; Command R+ ; (2) **Mixture of Experts(MoE) LLMs**: Mistral-8\(\)7B ; Mistral-8\(\)22B ; WizardLM-2-8\(\)22B ; (3) **Math-specific LLMs**: DeepSeekMath-7B ; (4) **Closed-source LLMs**: GPT-4 Turbo ; GPT-4-o ; Gemini-1.5-Pro ; Gemini-1.5-Flash ; Claude-3-Opus. Experiment setup details are available in the Appendix A. Unless otherwise stated, we use GPT-4 Turbo for graph construction and graph-to-text decoding across all tasks if needed 5. For all tasks, we use Chain-of-Thought (CoT)  prompting and Least-to-Most (LtM)  prompting, which are two of the most widely used prompting strategies in solving complex reasoning tasks.

We mainly apply DARG for four datasets in four representative reasoning tasks: **Mathematical Reasoning, Social Reasoning, Spatial Reasoning**, and **Symbolic Reasoning**, as case studies. For each of the tasks, we utilized the most used datasets, specifically, GSM8K  for math reasoning, BBQ  for social reasoning, BBH Navigate  dataset for spatial reasoning and BBH Dyck Language for symbolic reasoning, where recent LLMs seem to already solve these tasks by showing high performances (e.g., over 95% accuracy on GSM8K in zero-shot settings with GPT-4 ). However, by reevaluating the LLMs in the test data generated by our DARG on these datasets, we show that the current LLMs are still far from tackling these reasoning tasks. The graph setups for DARGin these tasks are illustrated in Table 1. Note that even though these graph setups are specific to datasets and tasks, the reasoning graph definitions and design patterns can be generalized to any reasoning datasets as stated in Section 2.1.

### Mathematical Reasoning: GSM8K

**Task and Graph Setup** To measure math reasoning abilities, we use the widely used GSM8K dataset , which contains high-quality, linguistically diverse school math word problems. Based on the definition of the reasoning graph in Section 2.1, for GSM8K, each node represents a number, and each edge serves as a math operator such as adding and dividing. The graph complexity and perturbation operations are defined as follows: **(1) Numerical Complexity** for the node complexity, which is defined as the number of unit additions in the calculations. We increase the numerical complexity at intervals of +2, +4, +6, +8. Based on the original reasoning graph, we randomly sample a set of new values for each node to meet the desired numerical complexity requirement. **(2) Depth of the Reasoning Graph** for structural complexity, which is defined as the number of nodes in the longest path from a leaf node to the answer node. We increment the depth of the original reasoning graphs at intervals of +1, +2, +3, +4. To increase the depth by 1, we identify the longest path in the original reasoning graph and then split the starting node into two new nodes with values that maintain the same numerical complexity. **(3) Width of the Reasoning Graph** for structural complexity, which is defined as the increased number of pairs of nodes added beyond the longest path in the graph. We increase the graph width at intervals of +1, +2, +3, and +4 by decomposing the starting nodes of non-longest paths, if they exist. Examples are shown in the middle part of Figure 1

**Evaluation** Apart from Pass@1 accuracy , to assess the robustness of LLMs in response to complexity increases within DARG, we additionally introduce the Complexity-Induced Accuracy Retention Rate (CIARR). Let \(A_{i}\) represent the accuracy of a model at complexity level \(i\) in a specific complexity dimension \(D\). The CIARR for a sequence of incremental complexity levels from \(0\) to \(n\) is defined as the average percentage retention in accuracy per complexity increment, given by:

Figure 2: Performance changes of 15 LLMs on GSM8K as the complexity level of the reasoning graph increases across three dimensions.

\[_{D}=_{i=1}^{n-1}(}{A_{i}})  100\%\] (1)

A higher value indicates greater robustness to complexity increases in that dimension.

**Results** Figure 2 shows the pass@1 accuracy on GSM8K with different complexity levels for each complexity dimension6 and Figure 9 visualizes the original accuracy and CIARR values from three complexity dimension. In general, the accuracy of all the models decreases as complexity increases across all three dimensions. For instance, as depth increases by 4, the performance for Claude-3-Opus significantly drops by 54.2% with different prompting strategies even though it achieves 95% accuracy on the original test set. This suggests that the superior performance on the existing static benchmark does not reflect the models' actual capabilities in reasoning, which might be partially due to the data contamination issues . We also observe that: (i) larger models with more active parameters demonstrate greater resilience to increasing complexity, for example, Llama3-70B is more resilient to complexity increases compared to Llama3-8B; (ii) MoE models are more resistant to complexity increases with similar amount of active parameters, e.g., Mistral-7B is less resistant to complexity increases than its MoE counterparts, Mistral-8x7B and Mistral-8x22B, suggesting the necessity of scaling up and MoE structures.

Following previous works [117; 15], we sampled 20 failure cases of GPT-4 Turbo from each complexity level and analyzed the types of errors involved in GSM8K. We categorize them into the following types: (1) Numerical Calculation Errors, where the model generates a correct problem-solving process but makes mistakes in arithmetic operations; (2) Reasoning Errors, which arise from incorrect reasoning or misapplication of mathematical concepts; (3) Other Errors, encompassing incorrect labels and other miscellaneous issues. Their distributions are visualized in Figure 3. We found that as the numerical complexity increases, the number of incorrect numerical calculations increases; as the reasoning graph's width and depth increase, there are more errors from incorrect reasoning processes. This suggests that current LLMs still lack the ability to handle larger numbers and math problems that require more reasoning steps. Case studies can be found in Appendix D.

### Social Reasoning: BBQ

**Task and Graph Setup** For social reasoning tasks, we use the BBQ dataset  which evaluates biases against nine protected groups through multiple-choice QA. The dataset includes two types of contexts: ambiguous (no clear evidence for an answer) and unambiguous (evidence supports a definite answer). Each question has three options: pro-bias, anti-bias, or neutral (_e.g., Camnot be determined_.). For BBQ, each node in the reasoning graph represents a person or an attribute, and the edges between different nodes represent the relation between them such as a person has an attribute. The graph complexity and perturbation operations are defined as follows: **(1) Attributes' polarity** for the node complexity, which describes whether a person's attributes are positive or negative. We examine if adding negative attributes to the pro-bias option and positive attributes to the anti-bias option influences LLMs to generate more biased output. **(2) Width of the reasoning Graph** for structural complexity, which is the number of attributes to people. An example is shown in Figure 18.

Figure 3: Distributions of different types of GPT-4’s errors in GSM8K with increasing complexity.

**Evaluation** Following previous works [75; 90], we evaluate performance using these metrics: (1) accuracy for ambiguous and unambiguous contexts (2) bias scores for both context types, with lower scores indicating less bias. We also observe that some SOTA LLMs are overly sensitive to contexts involving protected groups, often choosing _"Cannot be determined."_ even when clear evidence supports an answer. Therefore, we introduce an additional metric: (3) Overall Avoidance Rate, which measures how often this phenomenon occurs across all data points.

**Results** As shown in Figure 4, as the complexity of evaluation data increases by applying DARG, the overall accuracy tends to decline for all models. While closed-source models such as GPT-4 Turbo and Gemini-1.5-Pro show better overall accuracy, they lag behind many open-source models in disambiguous accuracy when we dig into ambiguous and disambiguous subcategories. Additionally, the overall avoidance rate in Figure 4 shows that GPT-4 Turbo and Gemini-1.5-Pro frequently opt for the _"Cannot be determined."_ even when there is clear evidence supporting an answer (shown in Appendix D). These two models with much higher overall accuracy actually exhibit a more severe issue of **over-sensitivity** to content involving protected groups compared to less powerful models such as GPT-3.5 Turbo. This might be due to the excessive alignment to avoid ethical issues. As the number of pairs of attributes increases, we observe that the bias scores in both ambiguous and disambiguous contexts generally increase, indicating that our DARGcan generate more challenging data to reveal biases in current models against vulnerable groups for more rigorous measurements of bias in LLMs.

### Spatial Reasoning: BBH Navigate

**Task and Graph Setup** We use the BBH Navigate dataset , which involves giving the LLM navigation steps to determine if the agent returns to the starting point. We construct reasoning graphs where nodes represent actions with attributes, including the number of steps and the direction, while directional edges indicate the order of actions. This forms a linear graph to model the task's reasoning structure. The graph complexity and perturbation operations are defined as the **depth of the Reasoning Graph for structural complexity**, i.e., the number of nodes in the linear reasoning graph. We increase the number of nodes by +2, +4, +8, and +16. To implement such a complexity increase, we randomly select an action node and split it into multiple nodes that

Figure 4: Comparison of different models’ performances with CoT as the number of attribute pairs increases on the BBQ dataset when applying DARG. All models show a decreasing trend in overall accuracy (\(\)) and an increasing trend in bias scores (\(\)) in both ambiguous and disambiguous contexts. Except for Mistral 7B, GPT-4 Turbo and Gemini-1.5-Pro demonstrate the highest overall avoidance (\(\)), indicating their over-sensitivity to contents with protected groups.

Figure 5: Models’ accuracy on BBH Navigate when applying DARG.

collectively have the same effect. We evaluate LLMs by overall accuracy and separate accuracies for "Yes" and "No" labeled data points, referred to as positive and negative accuracy, respectively.

**Results** As shown in Figure 5, there is a general trend of declining overall accuracy among all models with increasing complexities. More notably, as shown in Figure 12b 12a in the Appendix, all models exhibit a **dramatic decrease in positive accuracy** as the number of reasoning steps increases. Particularly, all models except GPT-4 Turbo show a decline of over 40 percent in positive accuracy when the number of nodes increases by 16, while negative accuracy remains relatively stable (examples are shown in Figure 16). This phenomenon might indicate **confirmation bias** in these LLMs, leading to an extremely unbalanced change in positive and negative performance.

### Symbolic Reasoning: BBH Dyck

**Task and Graph Setup**We use the BBH Dyck languages dataset , which requires the model to predict the sequence of closing parentheses for a Dyck-4 word missing its last few closing parentheses. Following Section 2.1, we construct reasoning graphs where each node represents a bracket of one of four types. There are three types of edges: those representing the order of actions, matches in the input, and expected matches between a bracket in the input and one in the output, as illustrated in Figure 11a. The entire reasoning graph can be divided into the input part and the output part. The input part is composed of nodes provided in the input, while the output part is composed of nodes in the ground truth label. The graph complexity and perturbation operations are defined as follows:

**(1) Depth of the graph's input part** for structure complexity, which is defined as the number of nodes in the input part of the graph, we increase the depth of the graph's input part by +2, +4, +8, and +16. **(2) Depth of the graph's output part** for structure complexity, which is defined as the number of nodes in the output part of the graph. To ensure unique output sequences, the number of input brackets must be greater than or equal to the number of brackets in the label. Thus, we increase the number of label nodes by \(+0.25\) (difference in number of nodes) and \(+0.5\) (difference in number of nodes). We use exact match accuracy as the evaluation metric.

**Results** As shown in Figure 6, when the number of nodes in the input increases to 4 and 8, GPT-4 and the Miktral 8\(\)22b model's accuracy even increases, while other models' performances show a significant decrease. When the number of nodes in the input increases to 16 and 32, all models' accuracy declines. Among all the models, GPT-4 Turbo and Mixtral 8\(\)22b are the best in terms of resilience to increasing input complexity. On the other hand, as the number of nodes in the expected output increases, almost all models' performances decrease. This suggests that LLMs still suffer from long context with either longer input or longer required output.

Figure 6: Comparison of different models’ accuracy on BBH Dyck Language with CoT as the number of brackets in the input (left) and label (right) increases. Overall, all models tend to experience a performance decline as the complexity increases significantly.

Figure 7: Results on GSM8K with increased complexity using Mixtral-7B and Llama2-7B, finetuned on GSM8K original data and DARG-generated ones.

### Fine-Tuning with DARG Generated Data

In this section, we demonstrate how the data generated by DARG can be further used to enhance LLMs by fine-tuning. Specifically, we first prompt GPT-4 Turbo with the novel questions and their corresponding reasoning graph to generate CoT reasoning steps. Then, we compare Mistral-7B and Llama2-7B on GSM8K test set evolved by DARG in different settings: (i) original model without any extra training, (ii) model fine-tuned with GSM8K training data and (iii) model fine-tuned with DARG generated data. The details are provided in Appendix A.

As shown in Figure 7, both models finetuned with DARG-generated data can outperform the one finetuned with an equivalent amount of GSM8K's original training data. This demonstrates DARG's potential not only to dynamically generate new test samples but also to produce training data that enables LLMs to adapt to various complexity levels.

## 4 Related Work

**Dynamic Evaluation.** A typical way to evaluate LLMs is constructing evaluation benchmarks [34; 55; 115; 14; 18; 16; 36; 35; 33; 87; 118; 41; 50]. However, these static benchmarks can have issues, such as data contamination [8; 56; 81; 51; 74; 43; 21; 30; 82; 52; 57; 47; 7; 54; 109; 24; 112] in LLMs, and may not be flexible enough to keep up with the rapid development of versatile LLMs. To resolve these problems, there are lines of work focusing on focus on human-centric evaluation [27; 80; 58; 108]. Another direction [48; 64] is to build crowdsourcing platforms to dynamically collect human-annotated data. Recently, DyVal  introduced a graph-informed method to dynamically generate evaluation samples with controllable complexities. However, the samples generated by this method tend to be rigid and explicitly described, e.g., _"The value of a is 9 and the value of \(b\) is 10; what is the value of \(c\)_ which is the same as \(a+b\)?". This approach lacks the linguistic diversity of existing benchmarks such as GSM8K , which may not align well with the evaluation objectives of LLMs in real-life usage. Besides, it only focuses on limited reasoning domains such as math and logical reasoning. DyVal 2  and Benchmark Self-Evolving  employ LLMs with prompting strategies such as paraphrasing to perturb current benchmarks. However, a significant issue is that LLMs are known for their instability, and merely prompting LLMs does not guarantee the stability of the labels nor does it achieve fine-grained complexity control. In contrast, our method enables fine-grained control over the complexity of extended benchmarks across various reasoning domains, verifying correct labels while preserving the same linguistic diversity as the original ones.

**Synthetic Data** Synthetic data has emerged as a promising solution by generating data that mimics real-world patterns [72; 59]. As LLMs demonstrate a powerful ability to generate high-quality data, an increasing number of methods have been proposed to generate synthetic data for LLM training [113; 39; 107; 32; 111; 89; 95; 6; 99; 102; 62; 83; 92; 94; 53; 88; 40], alignment [5; 97; 76; 93; 60; 22; 100; 110], and evaluation [77; 26; 114; 101; 42]. However, most previous works on synthetic data for LLM evaluation have focused on generating new data points from scratch, whereas our work concentrates on extending current benchmarks through fine-grained complexity control.

## 5 Conclusion

We presented DARG, a dynamic evaluation framework of LLMs via adaptive reasoning graph. Our method augments existing benchmarks by reconstructing the underlying reasoning structure of their problem-solving processes. DARG can generate new test samples across various complexity levels while maintaining linguistic diversity comparable to that of existing benchmarks. Our evaluation of 15 SOTA LLMs across four reasoning domains reveals that performance generally declines as task complexity increases, with varying degrees of resistance observed across different models. Additionally, we noted that LLMs exhibit increasing biases and excessive sensitivity to content involving protected groups. These findings shed light on how to dynamically and adaptively evaluate LLM and argue for moving beyond static benchmarking and adopting adaptive frameworks like DARG given the dynamic nature of LLM development and evaluation.

Our work has several limitations. (1) We focused on reasoning tasks and selected one representative dataset per task as case studies due to limited resources. But the reasoning graph definition in DARG are general and can be applied and extended to other tasks like natural language understanding tasks, which could be solved with a reasoning chain (e.g., Chain-of-Thoughts). (2) While we only fine-tuned two Mistral and LLAMA models on math reasoning datasets (GSM8K), we believe such improvements from training with DARG generated data would be consistent for other models andtasks as DARG could generate diverse and more complex examples than existing ones, which could also benefit weak-to-strong generalization . (3) The current graph extraction and data generation process heavily rely on closed-source LLMs (e.g., GPT-4). Although we added rule-based constraints and data verification modules, we have not explored whether open-source models could generate reasonable data in the absence of closed-source models.