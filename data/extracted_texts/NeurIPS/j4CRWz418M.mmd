# Are Large Language Models Good Statisticians?

Yizhang Zhu\({}^{1}\), Shiyin Du\({}^{1}\), Boyan Li\({}^{1}\), Yuyu Luo\({}^{1,2}\), Nan Tang\({}^{1,2}\)

\({}^{1}\)The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China

\({}^{2}\)The Hong Kong University of Science and Technology, Hong Kong SAR, China

{yzhu305, sdu164}@connect.hkust-gz.edu.cn

{boyanli, yuyuluo, nantang}@hkust-gz.edu.cn

 Yuyu Luo and Nan Tang are the corresponding authors.

###### Abstract

Large Language Models (LLMs) have demonstrated impressive capabilities across a range of scientific tasks including mathematics, physics, and chemistry. Despite their successes, the effectiveness of LLMs in handling complex statistical tasks remains systematically under-explored. To bridge this gap, we introduce StatQA, _a new benchmark designed for statistical analysis tasks._StatQA comprises 11,623 examples tailored to evaluate LLMs' proficiency in specialized statistical tasks and their applicability assessment capabilities, particularly for hypothesis testing methods. We systematically experiment with representative LLMs using various prompting strategies and show that even state-of-the-art models such as _GPT-4o_ achieve a best performance of only 64.83%, indicating significant room for improvement. Notably, while open-source LLMs (_e.g., LLaMA-3_) show limited capability, those fine-tuned ones exhibit marked improvements, outperforming all in-context learning-based methods (_e.g., GPT-4o_). Moreover, our comparative human experiments highlight a striking contrast in error types between LLMs and humans: LLMs primarily make applicability errors, whereas humans mostly make statistical task confusion errors. This divergence highlights distinct areas of proficiency and deficiency, suggesting that combining LLM and human expertise could lead to complementary strengths, inviting further investigation into their collaborative potential. Our source code and data are available at https://statqa.github.io/.

## 1 Introduction

Statistical analysis can capture data patterns and convert them into usable evidence, which is crucial to data science and machine learning applications [1; 2; 3; 4; 5; 6; 7; 8]. As shown in Figure 1, a typical statistical analysis task involves, given a table \(D\) and a statistical question \(Q\), a qualified statistician should be proficient in selecting relevant columns \(\), choosing the appropriate statistical methods \(\), and computing the results based on \(\) using \(\). Formally, the statistical analysis task can be divided into two stages: (1) identifying appropriate statistical methods and parameters (_e.g.,_ relevant columns in the table), and (2) computing the statistical results and deriving the conclusion. The first stage requires statistical expertise to assess the applicability of methods, considering factors such as data type, distribution characteristics, and sample size, which is the core of the statistical task. In contrast, the second stage (_i.e.,_ computation) can be easily aided by external tools  such as WolframAlpha .

Inspired by the extensive application of Large Language Models (LLMs) [11; 12; 13], we pose a critical question: _Do LLMs truly understand such "statistical literacy"?_ Specifically, can LLMs competently select relevant data, recognize prerequisites, and discern appropriate usage scenarios to assess the effectiveness of statistical methods?Currently, research on the mathematical reasoning abilities of LLMs has primarily focused on the accuracy of computational processes and results, involving conventional statistical methods [14; 15; 16; 17]. However, studies examining datasets requiring specialized statistical testing methods, particularly those assessing the applicability of these methods, are minimal. This gap in the literature motivated us to explore and address this area. Our study aims to answer the following critical questions.

* _**Q1:** How can we evaluate LLMs' performance in more complex and specialized statistical testing tasks?_ Constructing an appropriate benchmark is crucial for accurate performance evaluation. Given the challenges posed by the lack of datasets and scarcity of examples in this specialized field, how can we efficiently develop such a benchmark?
* _**Q2:** How capable are current LLMs in this field, and how can we improve their performance?_ This requires systematic experiments to evaluate the capabilities of current LLMs, exploring the impact of different models, prompting strategies, and fine-tuning methods on their performance.
* _**Q3:** How do humans perform compared to LLMs, and what are the differences in their performance?_ This involves a comparative study between humans and LLMs, aiming to analyze their respective strengths and weaknesses and explore potential complementary between human expertise and LLM capabilities.

Contributions.Our contributions are summarized as follows:

* **StatQA.** We propose StatQA, a new benchmark for statistical analysis tasks, particularly focusing on the applicability assessment of statistical methods. We introduce an automated pipeline to construct StatQA by synthesizing statistical tasks and their corresponding answers, which also provides insights for dataset construction in other specialized domains with scarce examples.
* **Systematic Evaluation.** We conduct extensive evaluations on widely used LLMs to establish benchmarks for statistical tasks. We also explore several strategies, including domain-specific prompts and fine-tuning, to better harness the capabilities of LLMs for these tasks.
* **Comparative Study between Humans and LLMs.** We organize group-based human experiments and comparatively analyze differences between humans and LLMs in performance and errors. Our findings highlight humans' and LLMs' distinct strengths and weaknesses and reveal their potential complementarity.
* **New Empirical Findings and Research Opportunities.** Based on the experiments and analysis above, we summarize six key findings and discuss research opportunities in this field.

## 2 StatQA

In this section, we will first discuss the design goal of the benchmark for statistical analysis tasks (Section 2.1). We will then describe the characteristics of StatQA (Section 2.2). Finally, we will elaborate on how to develop StatQA with low human cost while ensuring high quality (Section 2.3).

### Design Goals and Tasks Scope

Goals.We aim to develop a specialized dataset to address the current research gap. Our goals include: (G1) _Good Coverage of Statistical Tasks and Difficulties_: The benchmark should encompass representative and commonly used statistical analysis tasks and methods. It should be designed to be sufficiently discriminative, effectively distinguishing between the strengths and weaknesses

Figure 1: An Example of Statistical Analysis Task

of different LLMs; (G2) _Support for Evaluating Statistical Literacy_: The dataset should facilitate our core experiments, assessing whether LLMs can evaluate the applicability of statistical methods, select suitable methods, and identify relevant data columns; (G3) _Low Human-Cost_: Automating the dataset construction process to ensure sufficient scale and improved efficiency while maintaining high data quality; and (G4) _Extensibility_: The benchmark should be designed to accommodate future expansions and enhancements.

The Scope of Statistical Analysis Tasks.Statistical tasks can be broadly divided into descriptive statistics and inferential statistics, with inferential statistics primarily including regression analysis and hypothesis testing . Descriptive statistics and hypothesis testing represent two prevalent methods frequently employed in statistical analysis. The latter is often considered the most misunderstood in quantitative analysis due to its complex interdependencies between procedural components . Therefore, together with Descriptive Statistics (DS), we select four representative categories of statistical tasks in hypothesis testing to be covered in StatQA, along with commonly used methods: Correlation Analysis (CA); Contingency Table Test (CTT); Distribution Compliance Test (DCT); Variance Test (VT). The involved statistical tasks in each category are shown in Table 3 (Section B.2).

### StatQA Characteristics

Figure 2 shows an example in StatQA, which includes the dataset (tabular data), statistical question, task category, difficulty level, relevant column information, preliminary results, and ground truth. More examples are presented in Figure 7 in the appendix.

    &  &  &  &  \\   & Avg \#-Rows & Avg \#-Cols & Max & Min & Avg & Easy & Hard & StatQA & mini-StatQA \\ 
**Stats** & 6,228 & 14 & 346 & 21 & 113 & 7,401 & 4,222 & 11,623 & 1,163 \\   

Table 1: Statistics of StatQA

Figure 3: The Proportion of Statistical Tasks in StatQA. The inner ring represents the five types of statistical tasks mentioned, while the outer ring corresponds to the methods associated with each task.

Figure 2: An Example in StatQA. (1) Dataset (\(D\)) is the datasets for statistical tasks; (2) Question (\(Q^{*}()\)) is a statistical question, refined by GPT-3.5-Turbo; (3) Task indicates the statistical task category; (4) Difficulty includes easy and hard levels. (5) Relevant Columns (\(\)) are data columns relevant to the statistical question; (6) Results (\(\)) include applicable statistical methods (\(\)), the computational results, and preliminary conclusions; (7) Ground Truth includes relevant columns (\(\)) and all applicable methods (\(\)) for this statistical task;

Figure 3 and Table 1 show the proportion of different statistical task categories and the statistics of our StatQA, respectively. The StatQA benchmark contains 11,623 examples. To reduce testing costs and facilitate users with limited computational resources, we use the _stratified sampling_ strategy  to obtain the mini-StatQA (1,163 examples), ensuring mini-StatQA resembles the complete benchmark in terms of task and difficulty distribution. We also use mini-StatQA in subsequent experiments.

### StatQA Construction

Key Ideas for Developing StatQA.In conventional dataset construction, researchers collect a suitable dataset \(D\), formulate a question \(Q\), and manually annotate answers \(A\). While this method ensures high data quality, it is time-consuming, costly, and limits extensibility, especially in specialized domains with scarce examples. To alleviate these limitations, our **key idea** is to reverse this process by synthesizing the question \(Q\) based on target answers \(A\). We start with target answers \(A\) derived from the tabular data \(D\) and generate corresponding statistical questions \(Q\). This approach ensures precise alignment between questions and answers, enabling more efficient dataset construction.

To implement this, we design an efficient pipeline for constructing StatQA, as shown in Figure 4. Unlike traditional methods, we set target answers \(A\) based on tabular data \(D\) and then synthesize statistical questions \(Q\) in reverse. To ensure alignment between \(Q\) and \(A\), we incorporate automated prerequisite checks. To support the evaluation of statistical literacy, the target answers \(A\) include relevant columns \(\) and applicable statistical methods \(\), enabling the derivation of computational results \(\). Therefore, our pipeline can synthesize numerous examples of \((D,,,Q,)\) along with other supplementary information. Next, we will go through our pipeline step by step.

Tabular Data and Metadata Collection.We collect 78 tables from real-world applications, covering various domains including education, medicine, economy, etc., as shown in Section B.5. These tables are carefully gathered by post-graduate students in statistics from Kaggle  and Rdatasets . The metadata includes descriptive information about the tabular data and details of each data column. Specifically, it encompasses the column header, data type, normality, and description, which are crucial for prerequisite checks in statistical analysis. Descriptive information in metadata can be obtained from their sources, while the data types and normality of each column can be calculated and derived from the tabular data. For tables lacking metadata, we perform manual annotations. For tables with existing metadata, we conduct manual validation to ensure accuracy.

Step 1: Set Target Answers (Select Target Methods and Columns).As shown in Figure 4, our pipeline can reversely synthesize the statistical question \(Q\) based on statistical methods and involved data columns. The first step is to set the target methods \(\). Considering the parameter volume of the target methods, we select suitable columns from tabular data to obtain a set of data columns \(\).

Step 2: Prerequisites Check and Computation.Since statistical methods should be used under appropriate conditions, we perform a series of checks to ensure all prerequisites are met. This includes verifying features such as sample size, data type, and normality. Suppose data columns \(=\{C_{1},...,C_{m}\}\) fit with the prerequisites of methods \(=\{M_{1},...,M_{n}\}\). The data columns

Figure 4: The Pipeline for Synthesizing StatQA

can then be used as parameters in the target methods \(\) to compute the corresponding computational results and preliminary conclusions, noted as \(=\{M_{1}(),...,M_{n}()\}\).

Step 3: Statistical Question Synthesis.To ensure the quality of the questions, we use _hand-crafted_ question templates to synthesize preliminary statistical questions. The templates are determined by the target method, listing common question expressions with placeholders where relevant data columns are involved, as shown in Section B.3. By selecting a template \(T\) and substituting in previously chosen data columns \(\), we can obtain the preliminary statistical question \(Q()\).

Step 4: Difficulty Balancing and Dataset Splitting.For a statistical question, if the set of applicable methods constitutes a proper subset of the candidate methods (_i.e.,_ all methods for comparable scenarios), it indicates that certain prerequisites for some methods are not satisfied. In such cases, we label the question as "hard", reflecting the increased challenge of accurately assessing applicability to eliminate inapplicable methods. Otherwise, the question is labeled as "easy". Based on these labels, we expand the underrepresented categories and sample the abundant ones to obtain balanced synthesized examples in terms of task difficulty. Next, we split these synthesized examples to ensure no table overlaps in training and test sets: data synthesized from source tables No. 1 to 36 will be used for subsequent tests (_i.e.,_ StatQA), while tables No. 37 to 78 will be used for training (_i.e.,_ training set \(_{}\)). The \(_{}\) is used to fine-tuning LLMs.

Step 5: Statistical Question Refinement.Inspired by LLMs' capabilities in text comprehension and processing, we use GPT-3.5-Turbo to refine the phrasing and expression of statistical questions for the test data. We provide GPT-3.5-Turbo with original questions and their descriptive information, instructing GPT-3.5-Turbo to paraphrase and refine the question sentences without changing the meaning, aiming for more coherent and diverse expressions, which is noted as \(Q^{*}()\).

Discussion of Quality Control.We devise several strategies to ensure the high quality of our StatQA. (1) _Quality of Question Templates_: The quality of question templates is a focal point in our quality control. We categorize templates based on statistical tasks and applicable scenarios of methods. For each category, we meticulously prepare 10 to 20 templates for random selection to enhance diversity. To ensure the templates are representative, we recruited two post-graduate students in statistics to design and review them. (2) _Question Refinement_: The objectives of refinement are to correct potential grammar mistakes, improve semantical coherence, and increase the diversity of expressions for the questions generated by the templates. In practice, GPT-3.5-Turbo demonstrates a satisfactory level of English proficiency in achieving these goals and offers better cost-effectiveness; therefore, we employ GPT-3.5-Turbo as the refiner. Note that question refinement is exclusively performed on StatQA to increase diversity and ensure differences from the training set. The average BLEU , BERTScore  calculated between original questions and refined versions in StatQA is 0.126, 0.920 respectively. The low BLEU reflects notable vocabulary differences, while the high BERTScore suggests semantic consistency, collectively indicating adequate rephrasing that preserves the original meaning. (3) _Expert Reviews_: Last but most importantly, we conduct manual reviews by two post-graduate students in statistics to carefully check all examples in StatQA.

## 3 Experiments

### Setup

Experimental Protocols.We design experiments for LLMs similar to human statisticians' mindset, as presented in Figure 1, to evaluate the abilities of LLMs in statistical tasks. Because of the limitation of input tokens, we provide LLMs column information instead of the whole table, including the column headers, number of rows, data type, and normality. In the experiment, the LLMs need to pick headers of relevant data columns, assess the methods' applicability, select all statistical methods that fit the usage scenario and prerequisites as statisticians, and then respond in a specific format. Since LLM responses might be invalid or include irrelevant content, cleaning, and extraction are necessary, so the extracted answers should be compared to the ground truth for evaluation. In the human experiments, we use the same protocol for consistency and develop a testing platform to facilitate participant selection. _More details of experimental setups, including hyperparameters,prompts, procedures, and GUI of the testing platform used in human experiments, are provided in Section C._

Metrics.Accuracy of relevant data columns and applicable methods selections, noted as \(Acc(,)\), is used as our metrics to evaluate if LLMs or participants truly understand the question and the applicability of statistical methods. \(Acc(,)\) refers to the proportion of methods and column selections fully aligned with the ground truth without any omissions or incorrect selections:

\[Acc(,)=^{N}1(}_{i}= _{i},}_{i}=_{i})}{N},\] (1)

where \(N\) is the total number of test examples.

#### 3.1.1 Evaluation for LLMs

Non-fine-tuned LLMs.For open-source LLMs, we select and conduct experiments on LLaMA-2 models (Llama-2-7b-chat-hf, Llama-2-13b-chat-hf)  and LLaMA-3 models (Meta-Llama-3-8B, Meta-Llama-3-8B-Instruct) . For proprietary LLMs, we select representative and widely-used ChatGPT (GPT-3.5-turbo) , GPT-4  and newly released GPT-4o .

Prompting Strategies.Few-shot learning and Chain-of-Thought (CoT)  will be used as prompting strategies. In few-shot learning, we prepare one example of each task category for the LLMs to learn from. To prevent the leakage of actual task categories, we randomly select examples in the few-shot. Furthermore, we introduce a new strategy to include domain knowledge (DK) of statistical methods' applicability and prerequisites in the prompt.

Fine-tuned LLMs.We fine-tune three models: LLaMA-2 (LLaMA-2-7b-chat-hf) and LLaMA-3 (Meta-Llama-3-8B, Meta-Llama-3-8B-Instruct). For all fine-tuning, we use the LoRA  method, which is a Parameter-Efficient Fine-Tuning technique that adjusts only a small number of parameters. This method achieves an effect close to full-parameter fine-tuning on downstream tasks while reducing computation and storage costs. Note that StatQA is exclusively reserved for testing and evaluation. We use the training set, also generated by our dataset construction pipeline (see **Step 4** in Section 2.3), to fine-tune the LLMs.

#### 3.1.2 Human Experiments

Participants.Human experiments are conducted for a comparative study. We recruited 6 post-graduate students and grouped them based on their disciplinary backgrounds: Non-Statistics Background Group (Non-Stats, three STEM post-graduate students not in statistics major) and Statistics Background Group (Stats, three post-graduate students in statistics major).

Protocols.We use stratified sampling to extract 10% of the mini-StatQA for human experiments (117 examples). To ensure participants understand the task and are familiar with operations on our testing platform, they are required to watch a tutorial video before starting. Each participant needs to use 2 answering modes respectively during the experiments: (1) Closed-book: participants must answer independently; (2) Open-book: similar to introducing domain knowledge for LLMs, participants are provided with supplemental information. More details can be found in Section C.2.

### Experimental Results and Analysis

Different models display significant performance discrepancies on our benchmark, indicating sufficient discriminative ability. Table 2 presents the experimental results. Figure 5 shows the radar chart depicting performances across five task categories achieved by leading results within each model.

Overall Performance of LLMs.The LLaMA-2 models demonstrate weak performance, suggesting their inability. The newer LLaMA-3 shows remarkable improvement over the previous LLaMA-2, with the LLaMA-3-8b achieving an \(Acc(,)\) up to 36.1%, far surpassing all tested LLaMA-2 models, close to the performance of 0-shot GPT-3.5-Turbo (37.4%). However, it is still noticeably weaker than GPT-4 and GPT-4o. By fine-tuning, LLaMA-2-7b, LLaMA-3-8b, and LLaMA-3-8b-Instruct models achieve marked enhancement, with the fine-tuned LLaMA-3-8b model showing the best overall performance, but considerable room still remains for further improvement.

  
**Model** & **Strategy** & **Overall** & **CA** & **CTT** & **DCT** & **VT** & **DS** \\   \\   & 0-shot & 8.08 & 1.79 & 1.17 & 2.12 & 6.97 & 25.48 \\  & 1-shot & 14.96 & 0.60 & 6.25 & 5.93 & 19.26 & 37.07 \\ LLaMA-2 7B & 0-shot-CoT & 6.36 & 1.19 & 0.78 & 2.12 & 5.74 & 19.69 \\  & 1-shot-CoT & 14.45 & 1.79 & 4.30 & 8.48 & 19.67 & 33.21 \\  & 1-shot+DK & 16.08 & 0.60 & 7.42 & 9.32 & 18.44 & 38.61 \\   & 0-shot & 9.29 & 1.79 & 0.39 & 8.48 & 3.28 & 29.34 \\  & 1-shot & 17.97 & 9.52 & 5.47 & 9.32 & 2.05 & 58.69 \\ LLaMA-2 13B & 0-shot-CoT & 9.03 & 2.38 & 0.00 & 9.32 & 2.87 & 27.80 \\  & 1-shot-CoT & 17.63 & 6.55 & 9.38 & 9.75 & 0.41 & 56.37 \\  & 1-shot+DK & 20.29 & 8.33 & 7.03 & 16.53 & 11.48 & 52.90 \\   & 0-shot & 23.56 & 1.19 & 0.00 & 16.53 & 16.39 & 74.52 \\  & 1-shot & 31.90 & 17.86 & 8.20 & 18.64 & 25.41 & 82.63 \\ LLaMA-3 8B & 0-shot-CoT & 22.01 & 1.19 & 0.39 & 15.68 & 13.93 & 70.27 \\  & 1-shot-CoT & 32.24 & 14.29 & 5.86 & 19.92 & **29.10** & **84.17** \\  & 1-shot+DK & **36.11** & **26.79** & 20.31 & **29.24** & 15.98 & 83.01 \\   & 0-shot & 13.67 & 10.12 & 13.28 & 5.09 & 1.23 & 35.91 \\  & 1-shot & 28.20 & **26.79** & 12.11 & 13.56 & 9.84 & 75.68 \\ LLaMA-3 8B & 0-shot-CoT & 11.61 & 10.71 & 14.84 & 6.78 & 0.00 & 24.32 \\  & 1-shot-CoT & 28.29 & 26.19 & 16.80 & 16.10 & 9.84 & 69.50 \\  & 1-shot+DK & 27.77 & 19.64 & **22.27** & 20.34 & 8.20 & 63.71 \\   \\   & 0-shot & 37.40 & 47.02 & 31.25 & 26.27 & 12.71 & 70.66 \\ GPT-3.5-Turbo & 1-shot & 40.76 & 53.57 & 12.50 & 27.54 & 26.23 & 86.10 \\ GPT-3.5-Turbo & 0-shot-CoT & 38.17 & 45.24 & 33.59 & 25.85 & 13.93 & 72.20 \\  & 1-shot-CoT & 39.64 & 51.79 & 10.94 & 26.70 & 26.23 & 84.56 \\  & 1-shot+DK & 49.36 & 62.50 & 35.55 & 38.98 & 26.23 & 85.71 \\   & 0-shot & 42.39 & 66.67 & 20.70 & 45.76 & 2.46 & 82.63 \\  & 1-shot & 47.98 & 67.86 & 26.56 & 44.07 & 14.75 & 91.12 \\ GPT-4 & 0-shot-CoT & 43.34 & 67.86 & 23.44 & 46.19 & 1.64 & 83.78 \\  & 1-shot-CoT & 47.46 & 67.26 & 30.08 & 41.95 & 11.07 & 91.12 \\  & 1-shot+DK & 53.22 & 64.88 & 43.75 & 49.58 & 20.08 & 89.56 \\   & 0-shot & 44.23 & 62.50 & 19.53 & 25.00 & 31.56 & 86.49 \\  & 1-shot & 49.36 & **69.05** & 26.56 & 30.93 & 34.43 & 89.97 \\ GPT-4o & 0-shot-CoT & 44.71 & 63.10 & 20.70 & 24.58 & 32.38 & 86.49 \\  & 1-shot-CoT & 48.67 & 67.86 & 25.78 & 28.81 & 32.79 & **91.89** \\  & 1-shot+DK & **64.83** & 61.31 & **65.23** & **59.32** & **46.31** & 89.19 \\    \\  SFT LLaMA-2 7B & 0-shot & 66.72 & 69.05 & 35.94 & 83.48 & 54.51 & 91.89 \\ SFT LLaMA-3 8B & 0-shot & **77.13** & **79.76** & 65.23 & **88.56** & 55.33 & **97.30** \\ SFT LLaMA-3 8B & 0-shot & 75.92 & 69.64 & **68.75** & 85.17 & **57.38** & 96.14 \\   \\   & Closed-book & 18.10 & 5.88 & 3.85 & 8.70 & 0.00 & 65.39 \\  & Open-book & 34.48 & **52.94** & 0.00 & 30.44 & 8.33 & 84.62 \\   & Closed-book & 23.28 & 29.41 & 0.00 & 17.39 & 0.00 & 69.23 \\  & Open-book & **53.45** & 47.06 & **23.08** & **65.22** & **37.50** & **92.31** \\  

Table 2: Experimental Results of \(Acc(,)\)(%) on mini-StatQA. The 1st, 2nd, 3rd place results in all experiments are highlighted in red, blue, and green respectively. The **bold** results are the best in each section. The underlined results are the leading ones of each subgroup in overall \(Acc(,)\), whose performances in sub-tasks are shown in Figure 5. CA: Correlation Analysis; CTT: Contingency Table Test; DCT: Distribution Compliance Test; VT: Variance Test; DS: Descriptive Statistics.

Impact of Prompting Strategies.We observe that compared to 0-shot, 1-shot learning increased the \(Acc(,)\) for the LLaMA-2/3 models by an average of 9.6%, and by 4.7% for GPT models. However, CoT leads to a minor performance decrease in most cases of LLaMA-2/3, whereas its impact on larger GPT models is negligible. More analysis for CoT is shown in Section E.2. The introduced domain knowledge prompting has proven effective, for six out of the seven non-fine-tuned LLMs obtained their best results in our experiments. Notably, it has a more pronounced effect on larger LLMs with stronger comprehension abilities, particularly GPT-4o.

Finding 1. Few-shot learning and the inclusion of domain knowledge are helpful for LLMs in this task, whereas CoT is more likely to result in slight performance degradation in smaller models.

Human Performance.When completing the task independently without consulting any references, human participants demonstrate low accuracy. However, when provided with supplemental information of applicability, their performance improved significantly, particularly the participants in statistics, whose accuracy reached 53.4%, surpassing all non-fine-tuned LLMs with common prompting strategies. However, this is overshadowed by the fine-tuned models and the best performance of GPT-4o when domain knowledge is introduced in the prompt.

Finding 2. LLMs with prompt-based approaches remain behind people in statistics. However, the gap can be filled even surpassed by fine-tuning or introducing domain knowledge to a strong LLM.

Comparing LLMs and Human Performance.LLMs and humans perform best on straightforward and commonly seen descriptive statistics tasks. In hypothesis testing tasks, humans and most LLMs exhibit relatively strong performances in correlation analysis and distribution compliance tests. In contrast, LLMs and humans encounter major challenges in contingency table tests and variance tests. For these tasks where performance is relatively weak, introducing domain knowledge to larger proprietary LLMs can yield significant improvements, especially GPT-4o, whereas improvements on smaller open-source LLMs are less conspicuous. This may be due to larger proprietary LLMs having inherently stronger comprehension abilities, enabling them to utilize domain knowledge more effectively to enhance performance, but open-source models are comparatively weaker in this regard.

Finding 3. Humans and most LLMs are adept at descriptive statistics tasks but struggle with contingency tables and variance tests. Domain knowledge significantly boosts larger proprietary LLMs' performance, notably GPT-4o, but has limited impact on smaller open-source models.

### Errors Analysis

Errors Taxonomy.We categorize errors into four distinct types and mixed errors, with examples in Section E.1. The error categories are (1) Invalid Answer: meaningless responses or do not conform to the required format; (2) Column Selection Error: irrelevant or incorrect column selection; (3) Statistical Task Confusion: confusion regarding the category of statistical tasks, leading to incorrect selection of methods; (4) Applicability Error: no confusion on task category but failing to discern the usage scenarios and prerequisites, resulting in the selection of inapplicable methods; (5) Mixed Errors: valid answer but contain multiple types of errors.

Errors Analysis.We examine and summarize the proportions and distributions of the error types across different LLMs and experimental setups, as shown in Figure 6. We observe that except smaller

Figure 5: Best Results of Each Model in Five Sub-tasks (Section 2.1).

models like LLaMA-2-7b fail to understand the task more frequently, resulting in invalid responses, others seldom respond with invalid answers. Additionally, column selection errors account for a non-negligible proportion of the LLaMA-2/3 but are not a barrier for the GPT models.

Finding 4. LLaMA-3 and GPT models demonstrate a competent understanding of tasks, and the latter can accurately select data columns, but LLaMA-2 models have difficulties in these aspects.

Except for LLaMA-2, the primary error type observed for LLMs on our benchmark is applicability errors, while the percentage of errors associated with statistical task confusion is quite low. Strongly performed GPT and fine-tuned models also have a considerable proportion of applicability errors, even if we provide domain knowledge of applicability. This can be considered an inherent limitation of LLMs, indicating a deficiency in LLMs' ability to understand and assess methodological applicability. However, the situation is completely different for humans. Although participants in statistics outperform all non-fine-tuned LLMs in open-book experiments, in all human experimental groups, applicability errors account for a smaller proportion, while statistical task confusion errors constitute the highest proportion.

Finding 5. LLMs are good at distinguishing different statistical tasks and then selecting associated methods, but they struggle to utilize domain knowledge to assess method applicability effectively. Conversely, humans excel at discerning method applicability but are prone to task confusion.

Finding 6. Humans and LLMs have distinct proficiencies and weaknesses in different aspects of selecting applicable statistical tasks, highlighting the potential for complementary collaboration.

## 4 Research Opportunity

LLMs for Statistical Applications.Current LLMs struggle with accurately assessing the applicability of statistical methods, even when provided with explicit domain knowledge. This indicates a profound need for developing models that can better understand and utilize detailed methodological prerequisites and application contexts. Future research should focus on integrating more sophisticated reasoning mechanisms into LLMs or leveraging a multi-agent framework to enhance their comprehension and application of statistical methods.

Human-AI Collaboration in the Statistical Task.LLMs and humans exhibit distinct aspects of superior capability in statistical tasks. Therefore, an in-depth study into harnessing their unique strengths for complementary collaboration to achieve optimal performance can be a valuable endeavor. This approach could leverage the computational efficiency and data handling capabilities of LLMs alongside the nuanced understanding and domain knowledge of human experts, leading to more robust and accurate statistical analysis.

Figure 6: Distribution of Error Categories Across Experiments

Expanding the Benchmark Dataset.We establish StatQA, focusing on the evaluation of applicability rather than computational results, aiming to highlight the importance of discerning statistical method suitability. StatQA can be expanded to encompass a broader range of statistical tasks and methods, as discussed in Section G in the Appendix. Beyond the field of statistics, the evaluation of method applicability is also crucial in professional domains such as finance and operations research. Consequently, curating a more extensive benchmark represents a significant undertaking.

## 5 Related Work

Large Language Models.Proprietary LLMs like ChatGPT  and GPT-4  exhibit impressive capabilities in text comprehension and processing, and recent-released GPT-4o  is further elevated performance and speed. Open-source models are also favored for their flexibility and suitability for customization, among the most representative examples are the LLaMA series models, such as LLaMA-2  and the new LLaMA-3 . Meanwhile, prompting strategies  and fine-tuning methods like LoRA , enable LLMs to better adapt to specific tasks with less computational costs.

Relevant Benchmarks.Some benchmarks have been proposed to assess LLMs' problem-solving ability related to mathematical domains , but most are more focus on results reasoning and calculation instead of methods selection. Notable works like MATH  and GHOSTS  cover mathematical problems of varying difficulty, ranging from elementary to graduate levels; TheoremQA  and SciBench  involve multi-disciplinary problems; and recent studies like MathVista  and MathVerse  include visual tasks. DAEval  involves correlation analysis and distribution analysis as components of question concepts, but its coverage is relatively limited, and QRData  is for quantitative reasoning with data, but still more focus on calculation. Studies on datasets for more specialized statistical scenarios, particularly involving assessing the applicability of statistical methods, are minimal.

We also include a more detailed discussion about the related work in Section A in the Appendix.

## 6 Conclusion

In this paper, we propose StatQA, a benchmark designed for statistical analysis tasks. To evaluate the capabilities of LLMs on StatQA, we conduct systematic experiments with both open-source and closed-source LLMs to determine whether they can select proper statistical methods and relevant data columns by discerning prerequisites and assessing applicability, akin to competent statisticians. Furthermore, we conduct human experiments for a comparative study, discussing how humans and LLMs differ in their capabilities on StatQA and revealing their potential complementarity. Our findings suggest that while LLMs show promise, there is still significant room for improvement, especially in their ability to accurately assess the applicability of statistical methods. Future work could focus on enhancing the reasoning mechanisms of LLMs and exploring more effective human-AI collaboration strategies. We believe that StatQA fills a significant gap and provides a valuable resource for developing more advanced LLMs for statistical analysis tasks.

## 7 Acknowledgement

This paper is supported by NSF of China (62402409), Guangzhou Municipality Big Data Intelligence Key Lab (2023A03J0012), Guangdong Basic and Applied Basic Research Foundation (2023A1515110545), CCF-Huawei Populus Grove Fund (CCF-HuaweiDB202403), and the Red Bird Grant, Research Grant from The Hong Kong University of Science and Technology (Guangzhou).