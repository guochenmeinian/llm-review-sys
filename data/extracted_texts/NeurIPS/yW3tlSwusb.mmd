# Accelerating ERM for data-driven algorithm design using output-sensitive techniques

Maria-Florina Balcan

Carnegie Mellon University, ninamf@cs.cmu.edu.

Christopher Seiler

Work done by Christopher Seiler while he was at CMU.

Dravyansh Sharma

Corresponding author: dravy@ttic.edu. Work done by Dravyansh Sharma while he was at CMU.

###### Abstract

Data-driven algorithm design is a promising, learning-based approach for beyond worst-case analysis of algorithms with tunable parameters. An important open problem is the design of computationally efficient data-driven algorithms for combinatorial algorithm families with multiple parameters. As one fixes the problem instance and varies the parameters, the "dual" loss function typically has a piecewise-decomposable structure, i.e. is well-behaved except at certain sharp transition boundaries. Motivated by prior empirical work, we initiate the study of techniques to develop efficient ERM learning algorithms for data-driven algorithm design by enumerating the pieces of the sum dual loss functions for a collection of problem instances. The running time of our approach scales with the actual number of pieces that appear as opposed to worst case upper bounds on the number of pieces. Our approach involves two novel ingredients - an output-sensitive algorithm for enumerating polytopes induced by a set of hyperplanes using tools from computational geometry, and an _execution graph_ which compactly represents all the states the algorithm could attain for all possible parameter values. We illustrate our techniques by giving algorithms for pricing problems, linkage-based clustering and dynamic-programming based sequence alignment.

## 1 Introduction

The data-driven algorithm design paradigm captures a widely occuring scenario of solving multiple related problem instances and allows the design and analysis of algorithms that use machine learning to learn how to solve the instances which come from the same domain . Typically there are large (often infinite) parameterized algorithm families to choose from, and data-driven algorithm design approaches provide techniques to select algorithm parameters that provably perform well for instances from the same domain. Data-driven algorithms have been proposed and analyzed for a variety of combinatorial problems, including clustering, computational biology and mechanism design . But most of the prior work has focused on _sample efficiency_ of learning good algorithms i.e. the number of problem instances needed to learn algorithm parameters that perform well on a typical problem from the domain. A major open question for this line of research is to design _computationally efficient_ learning algorithms .

The parameterized family may occur naturally in well-known algorithms used in practice, or one could potentially design new families interpolating known heuristics. For the problem of aligning pairs of genomic sequences, one typically obtains the best alignment using a dynamic program with some costs or weights assigned to edits of different kinds, such as insertions, substitutions, or reduplications . These costs are the natural parameters for the alignment algorithm. The quality of the alignment can strongly depend on the parameters, and the best parameters vary depending on the application (e.g. aligning DNAs or RNAs), the pair of species being compared, orthe purpose of the alignment. Similarly, item prices are natural parameters in automated mechanism design . On the other hand, for linkage-based clustering, one usually chooses from a set of different available heuristics, such as single or complete linkage. Using an interpolation of these heuristics to design a parameterized family and tune the parameter, one can often obtain significant improvements in the quality of clustering .

A common property satisfied by a large number of interesting parameterized algorithm families is that the loss4 as a function of the real-valued parameters for any fixed problem instance--called the "dual class function" --is a piecewise structured function, i.e. the parameter space can be partitioned into "pieces" via sharp transition boundaries such that the loss function is well-behaved (e.g. constant or linear) within each piece . Prior work on data-driven algorithm design has largely focused on the sample complexity of the empirical risk minimization (ERM) algorithm which finds the loss-minimizing value of the parameter over a collection of problem instances drawn from some fixed unknown distribution. The ERM on a collection of problem instances can be implemented by enumerating the pieces of the sum dual class loss function. We will design algorithms for computationally efficient enumeration of these pieces, when the transition boundaries are linear, and our techniques can be used to learn good domain-specific values of these parameters given access to multiple problem instances from the problem domain. More precisely, we use techniques from computational geometry to obtain "output-sensitive" algorithms (formalized in Appendix C), that scale with the output-size \(R_{}\) (i.e. number of pieces in the partition) of the piece enumeration problem for the sum dual class function on a collection of problem instances. Often, on commonly occurring problem instances, the number of pieces is much smaller than worst case bounds on it  and our results imply significant gains in running time whenever \(R_{}\) is small.

Our contributions.We design novel approaches that use tools from computational geometry and lead to output-sensitive algorithms for learning good parameters by implementing the ERM (Empirical Risk Minimization) for several distinct data-driven design problems. The resulting learning algorithms scale polynomially with the number of sum dual class function pieces \(R_{}\) in the worst case (See Table 1) and are efficient for small constant \(d\).

1. We present a novel output-sensitive algorithm for enumerating the cells induced by a collection of hyperplanes. Our approach applies to any problem where the loss is piecewise linear, with convex polytopic piece boundaries. We achieve output-polynomial time by removing redundant constraints in any polytope using Clarkson's algorithm (from computational geometry)  and performing an implicit search over the graph of neighboring polytopes (formally Definition 2). Our results are useful in obtaining output-sensitive running times for several distinct data-driven algorithm design problems. Theorem 2.3 bounds the running time of the implementation of the

  Problem & Dim. & \(T_{S}\) (prior work) & \(T_{S}\) (ours) & \(T_{}\) (\(m\) instances) \\   Linkage-based & \(d=2\) & \(O(n^{18} n)\) & \(O(Rn^{3})\) & \(mT_{S}+(mn^{2}R_{})\) \\ clustering & any \(d\) & \(O(n^{8d+2} n)\) & \((R^{2}n^{3})\) & \(mT_{S}+(mn^{2}R_{}^{2})\) \\  DP-based sequence alignment & \(d=2\) & \(O(R^{2}+RT_{})\) & \(O(RT_{})\) & \(mT_{S}+(mT_{}R_{})\) \\  & any \(d\) & \(s^{O(sd)}T_{}\) & \((^{2L+1}T_{})\) & \(mT_{S}+(mT_{}R_{}^{2})\) \\  Two-part tariff & \(=1\) & \(O(K^{3})\) & \((R+K)\) & \(O(R_{}+mK mK)\) \\ pricing & any \(L^{}\) & \(K^{O(L^{})}\) & \((R^{2}K)\) & \(mT_{S}+(mKR_{}^{2})\) \\  

Table 1: Summary of running times of the proposed algorithms. \(T_{}\) denotes the running time for computing the pieces in the sum dual class function in terms of \(T_{S}\), the time for enumerating the pieces on a single problem instance (Theorem 2.3). \(R_{}\) (resp. \(R\)) denotes the number of pieces in the sum dual class function for given \(m\) problem instances (resp. dual class function for a single problem instance). \(n\) is the size of the clustering instance. \(s\) is the length of sequences to be aligned, \(L\) is the maximum number of subproblems in the sequence alignment DP update, \(\) is the maximum number of pieces in the dual class function over all subproblems, \(T_{DP}\) is the time to solve the DP on a single instance. \(K\) is the number of units of the item sold and \(\) is the menu length. The \(\) notation suppresses logarithmic terms and multiplicative terms that only depend on \(d\) or \(L\).

ERM for piecewise-structured duals with linear boundaries (Definition 1) that scales with \(R_{}\) provided we can enumerate the pieces of the dual class function for a single problem instance. It is therefore sufficient to enumerate the pieces of dual function for a single instance, for which Theorem 2.2 is useful.
2. We show how to learn multidimensional parameterized families of hierarchical clustering algorithms5. Our approach for enumerating the pieces of the dual class function on a single instance extends the _execution tree_ based approach introduced for the much simpler single-parameter family in  to multiple dimensions. The key idea is to track the convex polytopic subdivisions of the parameter space corresponding to a single merge step in the linkage procedure via nodes of the execution tree and apply our output-sensitive cell-enumeration algorithm at each node. 3. For dynamic programming (DP) based sequence alignment, our approach for enumerating pieces of the dual class function extends the execution tree to an _execution directed acyclic graph_ (DAG). For a small fixed \(d\), our algorithm is efficient for small constant \(L\), the maximum number of subproblems needed to compute any single DP update. Prior work  only provides an algorithm for computing the full partition for \(d=2\), and the naive approach of comparing all pairs of alignments takes exponential time.

Our main contribution is to show that when the number of pieces (\(R_{}\) in Table 1) is small we can improve over computational efficiency of prior work. Our techniques involves a novel application of Clarkson's algorithm (used to remove redundant inequalities from system of linear equations) to derive output-sensitive enumeration, and execution graphs which capture problem-specific structure.

Motivation from prior empirical work.For the clustering problem, we give theoretical bounds on the output-size \(R\) depending on the specific parametric family considered (e.g. Lemmas I.1, I.2). These bounds imply a strict improvement even for worst-case \(R\), but can be much faster for typical \(R\). Indeed, prior empirical work indicates \(R\) is much smaller than the worst case bounds in practice , where even for \(d=1\) there is a dramatic speed up of over \(10^{15}\) times by being output-sensitive. For the sequence alignment problem, again the upper bounds on R depend on the nature of cost functions involved. For example, Theorem 2.2 of  gives an upper bound on \(R\) for the 2-parameter problem with substitutions and deletions. For the TPT pricing problem we prove new theoretical bounds on \(R\) (Theorem G.3) that show worst-case improvements (cubic to quadratic) in the running time over prior work. In practice, \(R\) is usually much smaller than the worst case bounds, making our results even stronger. In prior experimental work on computational biology data (sequence alignment with \(d=2\), sequence length ~100),  observe that of a possible more than \(2^{200}\) alignments, only \(R=120\) appear as possible optimal sequence alignments (over \(^{2}\)) for a pair of immunoglobulin sequences (a speed-up of over 58 orders of magnitude).

Preliminaries.We follow the notation of . For a given algorithmic problem (say clustering, or sequence alignment), let \(\) denote the set of problem instances of interest. We also fix a (potentially infinite) family of algorithms \(\), parameterized by a set \(^{d}\). Here \(d\) is the number of real parameters, and will also be the 'fixed' parameter in the FPT (Fixed Parameter Tractability) sense (Appendix C). Let \(A_{}\) denote the algorithm in the family \(\) parameterized by \(\). The performance of any algorithm on any problem instance is given by a utility (or loss) function \(u:[0,H]\), i.e. \(u(x,)\) measures the performance on problem instance \(x\) of algorithm \(A_{}\). The utility of a fixed algorithm \(A_{}\) from the family is given by \(u_{}:[0,H]\), with \(u_{}(x)=u(x,)\). We are interested in the structure of the _dual class_ of functions \(u_{x}:[0,H]\), with \(u_{x}()=u_{}(x)\), which measure the performance of all algorithms of the family for a fixed problem instance \(x\). We will use \(\{\}\) to denote the 0-1 valued indicator function. For many parameterized algorithms, the dual class functions are piecewise structured in the following sense .

**Definition 1** (Piecewise structured with linear boundaries).: _A function class \(^{}\) that maps a domain \(^{d}\) to \(\) is \((,t)\)-piecewise decomposable for a class \(^{}\) of piece functions if the following holds: for every \(h\), there are \(t\) linear threshold functions \(g_{1},,g_{t}:\{0,1\}\), i.e. \(g_{i}(x)=\{a_{i}^{T}x+b_{i}\}\) and a piece function \(f_{}\) for each bit vector \(\{0,1\}^{t}\) such that for all \(y\), \(h(y)=f_{_{y}}(y)\) where \(_{y}=(g_{1}(y),,g_{t}(y))\{0,1\}^{t}\)._We will refer to connected subsets of the parameter space where the dual class function is a fixed piece function \(f_{c}\) as the _(dual) pieces_ or _regions_, when the dual class function is clear from the context. Past work  defines a similar definition for mechanism classes called \((d,t)\)_-delineable_ classes which are a special case of Definition 1, where the piece function class \(\) consists of linear functions, and focus on sample complexity of learning, i.e. the number of problem instances from the problem distribution that are sufficient learn near-optimal parameters with high confidence over the draw of the sample. Our techniques apply for a larger class, where \(\) is a class of convex functions. Past work  provides _sample complexity_ guarantees for problems that satisfy Definition 1, on the other hand we will focus on developing fast algorithms. We develop techniques that yield efficient algorithms for computing these pieces in the multiparameter setting, i.e. constant \(d>1\).The running times of our algorithms are polynomial in the output size (i.e, number of pieces). For \(i^{+}\), we will use \([i]\) to denote the set of positive integers \(\{1,,i\}\).

## 2 Output-sensitive cell enumeration and data-driven algorithm design

Suppose \(H\) is a collection of \(t\) hyperplanes in \(^{d}\). We consider the problem of enumerating the \(d\)-faces (henceforth _cells_) of the convex polyhedral regions induced by the hyperplanes. The enumerated cells will be represented as sign-patterns6 of facet-inducing hyperplanes. We will present an approach for enumerating these cells in OFPT time (Output-sensitive Fixed Parameter Tractable) Definition 3), which involves two key ingredients: (a) locality-sensitivity, and (b) output-sensitivity. By locality sensitivity, we mean that our algorithm exploits problem-specific local structure in the neighborhood of each cell to work with a smaller candidate set of hyperplanes which can potentially constitute the cell facets. This is abstracted out as a sub-routine ComputeLocallyRelevantSeparators which we will instantiate and analyse for each individual problem. In this section we will focus more on the output-sensitivity aspect.

To provide an output-sensitive guarantee for this enumeration problem, we compute only the _non-redundant_ hyperplanes which provide the boundary of each cell \(c\) in the partition induced by the hyperplanes. We denote the closed polytope bounding cell \(c\) by \(P_{c}\). A crucial ingredient for ensuring good output-sensitive runtime of our algorithm is Clarkson's algorithm for computing non-redundant constraints in a system of linear inequalities . A constraint is _redundant_ in a system if removing it does not change the set of solutions. The key idea is to maintain a set \(I\) of non-redundant constraints detected so far, and solve LPs that detect the redundancy of a remaining constraint (not in \(I\)) when added to \(I\). If the constraint is redundant relative to \(I\), it must also be redundant in the full system, otherwise we can add a (potentially different) non-redundant constraint to \(I\) (see Appendix D for details). The following runtime guarantee is known for the algorithm.

**Theorem 2.1** (Clarkson's algorithm).: _Given a list \(L\) of \(k\) half-space constraints in \(d\) dimensions, Clarkson's algorithm outputs the set \(I L\) of non-redundant constraints in \(L\) in time \(O(k(d,|I|+1))\), where \((v,c)\) is the time for solving an LP with \(v\) variables and \(c\) constraints._

Algorithm 1 uses AugmentedClarkson, which modifies Clarkson's algorithm with some additional bookkeeping (details in Appendix D) to facilitate a search for neighboring regions in our algorithm, while retaining the same asymptotic runtime complexity. Effectively, our algorithm can be seen as a breadth-first search over an implicit underlying graph (Definition 2), where the neighbors (and some auxiliary useful information) are computed dynamically by AugmentedClarkson.

**Definition 2** (Cell adjacency graph).: _Define the cell adjacency graph for a set \(H\) of hyperplanes in \(^{d}\), written \(G_{H}=(V_{H},E_{H})\), as:_

* _There is a vertex_ \(v V_{H}\) _for each cell in the partition_ \(\) _of_ \(^{d}\) _induced by the hyperplanes;_
* _For_ \(v,v^{}\!\!V_{H}\)_, add the edge_ \(\{v,v^{}\}\) _to_ \(E_{H}\) _if the corresponding polytopes intersect, i.e._ \(P_{v} P_{v^{}}\!\!\)_._

_This generalizes to a subdivision (Definition 6) of a polytope in \(^{d}\)._

This allows us to state the following guarantee about the runtime of Algorithm 1. In the notation of Table 1, we have \(R=|V_{H}|\) and \(|E_{H}|=O(R^{2})\).

**Theorem 2.2**.: _Let \(H\) be a set of \(t\) hyperplanes in \(^{d}\). Suppose that \(|E_{H}|=E\) and \(|V_{H}|=V\) in the cell adjacency graph \(G_{H}=(V_{H},E_{H})\) of \(H\); then if the domain \(\) is bounded by \(|P| t\) hyperplanes,Algorithm 1 computes the set \(V_{H}\) in time \((dE+VT_{}+t_{}_{c V_{H}}(d,|I_{c}|+1))\), where \((r,)\) denotes the time to solve an LP in \(r\) variables and \(s\) constraints, \(I_{c}\) denotes the number of facets for cell \(c V_{H}\), \(T_{}\) denotes the running time of \(}\) and \(t_{}\) denotes an upper bound on the number of locally relevant hyperplanes in Line 10.

We illustrate the significance of output-sensitivity and locality-sensitivity in our results with examples.

**Example 1**.: _The worst-case size of \(V_{H}\) is \(O(t^{d})\) and standard (output-insensitive) enumeration algorithms for computing \(V_{H}\) (e.g. ) take \(O(t^{d})\) time even when the output size may be much smaller. For example, if \(H\) is a collection of parallel planes in \(^{3}\), the running time of these approaches is \(O(t^{3})\). Even a naive implementation of \(}\) which always outputs the complete set \(H\) gives a better runtime of \(O(t^{2})\). By employing a straightforward algorithm which binary searches the closest hyperplanes to \(\) (in a pre-processed \(H\)) as \(}\) we have \(t_{}=O(1)\) and \(T_{}=(1)\), and Algorithm 1 attains a running time of \((t)\). Analogously, if \(H\) is a collection of \(t\) hyperplanes in \(^{d}\) with \(1 k<d\) distinct unit normal vectors, then output-sensitivity improves the runtime from \(O(t^{d})\) to \(O(t^{k+1})\), and locality-sensitivity can be used to further improve it to \((t^{k})\)._

ERM in the statistical learning setting.We now use Algorithm 1 to compute the sample mimimum (aka ERM, Empirical Risk Minimization) for the \((,t)\) piecewise-structured dual losses with linear boundaries (Definition 1) over a problem sample \(S^{m}\), provided piece functions in \(\) can be efficiently optimized over a polytope (typically the piece functions are constant or linear functions in our examples). Formally, we define search-ERM for a given parameterized algorithm family \(\) with parameter space \(\) and the dual class utility function being \((,t)\)-piecewise decomposable (Definition 1) as follows: given a set of \(m\) problem instances \(S^{m}\), compute the pieces, i.e. a partition of the parameter space into connected subsets such that the utility function is a fixed piece function in \(\) over each subset for each of the \(m\) problem instances. The following result gives a recipe for efficiently solving the search-ERM problem provided we can efficiently compute the dual function pieces in individual problem instances, and the the number of pieces in the sum dual class function over the sample \(S\) is not too large. The key idea is to apply Algorithm 1 for each problem instance, and once again for the search-ERM problem. In the notation of Table 1, we have \(R_{}=|V_{S}|\)the number of vertices in the cell adjacency graph corresponding to the polytopic pieces in the sum utility function \(_{i=1}^{m}u_{i}\). \(|E_{S}|=O(R_{})\) when \(d=2\) and \(O(R_{}^{2})\) for general \(d\).

**Theorem 2.3**.: _Let \(_{i}\) denote the cells partitioning the polytopic parameter space \(^{d}\) corresponding to pieces of the dual class utility function \(u_{i}\) on a single problem instance \(x_{i}\), from a collection \(s=\{x_{1},,x_{m}\}\) of \(m\) problem instances. Let \((V_{S},E_{S})\) be the cell adjacency graph corresponding to the polytopic pieces in the sum utility function \(_{i=1}^{m}u_{i}\). Then there is an algorithm for computing \(V_{S}\) given the cells \(_{i}\) in time \(((d+m)|E_{S}|+mt_{}_{c V_{S}}( d,|I_{c}|+1))\), where \(I_{c}\) denotes the number of facets for cell \(c V_{S}\), and \(t_{}\) is the number of locally relevant hyperplanes in a single instance._

An important consequence of the above result is an efficient output-sensitive algorithm for data-driven algorithm design when the dual class utility function is \((,t)\)-piecewise decomposable. In the following sections, we will instantiate the above results for various data-driven parameter selection problems where the dual class functions are piecewise-structured with linear boundaries (Definition 1). Prior work  has shown polynomial bounds on the sample complexity of learning near-optimal parameters via the ERM algorithm for these problems in the statistical learning setting, i.e. the problem instances are drawn from a fixed unknown distribution. In other words, ERM over polynomially large sample size \(m\) is sufficient for learning good parameters. In particular, we will design and analyze running time for problem-specific algorithms for computing locally relevant hyperplanes. Given Theorem 2.3, it will be sufficient to give an algorithm for computing the pieces of the dual class function for a single problem instance.

**Remark 1**.: _Our results in this section directly imply efficient algorithms for pricing problems in mechanism design which are known to be \((,t)\)-decomposable where \(\) is the class of linear functions (summarized in Table 1, see Appendix F for details)._

## 3 Linkage-based clustering

Clustering data into groups of similar points is a fundamental tool in data analysis and unsupervised machine learning. A variety of clustering algorithms have been introduced and studied but it is not clear which algorithms will work best on specific tasks. Also the quality of clustering is heavily dependent on the distance metric used to compare data points. Interpolating multiple metrics and clustering heuristics can result in significantly better clustering .

**Problem setup.** Let \(\) be the data domain. A clustering instance from the domain consists of a point set \(S=\{x_{1},,x_{n}\}\) and an (unknown) target clustering \(=(C_{1},,C_{k})\), where the sets \(C_{1},,C_{k}\) partition \(S\) into \(k\) clusters. Linkage-based clustering algorithms output a hierarchical clustering of the input data, represented by a cluster tree. We measure the agreement of a cluster tree \(T\) with the target clustering \(\) in terms of the Hamming distance between \(\) and the closest pruning of \(T\) that partitions it into \(k\) clusters (i.e., \(k\) disjoint subtrees that contain all the leaves of \(T\)). More formally, the loss \((T,)=_{P_{1},,P_{k}}_{ S_{n}}_{i=1}^{k}|C_{i} P_{_{i}}|\), where the first minimum is over all prunings \(P_{1},,P_{k}\) of the cluster tree \(T\) into \(k\) subtrees, and the second minimum is over all permutations of the \(k\) cluster indices.

A merge function \(D\) defines the distance between a pair of clusters \(C_{i},C_{j}\) in terms of the pairwise point distances given by a metric \(d\). Cluster pairs with smallest values of the merge function are merged first. For example, single linkage uses the merge function \(D_{}(C_{i},C_{j};d)=_{a C_{i},b C_{j}}d(a,b)\) and complete linkage uses \(D_{}(C_{i},C_{j};d)=_{a C_{i},b C_{j}}d(a,b)\). Instead of using extreme points to measure the distance between pairs of clusters, one may also use more central points, e.g. we define _median linkage_ as \(D_{}(C_{i},C_{j};d)=(\{d(a,b) a C_{i},b C _{j}\})\), where \(()\) is the usual statistical median of an ordered set \(S\)7. Appendix H provides synthetic clustering instances where one of single, complete or median linkage leads to significantly better clustering than the other two, illustrating the need to learn an interpolated procedure. Single,median and complete linkage are _2-point-based_ (Definition 4, Appendix I), i.e. the merge function \(D(A,B;d)\) only depends on the distance \(d(a,b)\) for two points \((a,b) A B\).

**Parameterized algorithm families.** Let \(=\{D_{1},,D_{l}\}\) denote a finite family of merge functions (measure distances between clusters) and \(=\{d_{1},,d_{m}\}\) be a finite collection of distance metrics (measure distances between points). We define a parameterized family of linkage-based clustering algorithms that allows us to learn both the merge function and the distance metric. It is given by the interpolated merge function \(D^{}_{}(A,B;)=_{D_{i},d_{j}}_{i,j}D_{i}(A,B;d_{j})\), where \(=\{_{i,j} i[l],j[m],_{i,j} 0\}\). In order to ensure linear boundary functions for the dual class function, our interpolated merge function \(D^{}_{}(A,B;)\) takes all pairs of distance metrics and linkage procedures. Due to invariance under constant multiplicative factors, we can set \(_{i,j}_{i,j}=1\) and obtain a set of parameters which allows \(\) to be parameterized by \(d=lm-1\) values8. Define the parameter space \(=^{d}=\{(^{ 0} )^{d}_{i}_{i} 1\}\); for any \(\) we get \(^{()}^{d+1}\) as \(^{()}_{i}=_{i}\) for \(i[d]\), \(^{()}_{d+1}=1-_{i[d]}_{i}\). We focus on learning the optimal \(\) for a single instance \((S,)\). With slight abuse of notation we will sometimes use \(D^{}_{}(A,B;)\) to denote the interpolated merge function \(D^{}_{^{()}}(A,B;)\). As a special case we have the family \(D^{}_{}(A,B;d_{0})\) that interpolates merge functions (from set \(\)) for different linkage procedures but the same distance metric \(d_{0}\). Another interesting family only interpolates the distance metrics, i.e. use a distance metric \(d_{}(a,b)=_{d_{j}}^{()}_{j}d_{j}(a,b)\) and use a fixed linkage procedure. We denote this by \(D^{1}_{}(A,B;)\).

We will extend the _execution tree_ approach introduced by  which computes the pieces (intervals) of _single-parameter_ linkage-based clustering. A formal treatment of the execution tree, and how it is extended to the multi-parameter setting, is deferred to Appendix I.1. Informally, for a single parameter, the execution tree is defined as the partition tree where each node represents an interval where the first \(t\) merges are identical, and edges correspond to the subset relationship between intervals obtained by refinement from a single merge. The execution, i.e. the sequence of merges, is unique along any path of this tree. The same properties, i.e. refinement of the partition with each merge and correspondence to the algorithm's execution, continue to hold in the multidimensional case, but with convex polytopes instead of intervals (see Definition 5). Computing the children of any node of the execution tree corresponds to computing the subdivision of a convex polytope into polytopic cells where the next merge step is fixed. The children of any node of the execution tree can be computed using Algorithm 1. We compute the cells by following the neighbors, keeping track of the cluster pairs merged for the computed cells to avoid recomputation. For any single cell, we find the bounding polytope along with cluster pairs corresponding to neighboring cells by computing the tight constraints in a system of linear inequalities. Theorem 2.2 gives the runtime complexity of the proposed algorithm for computing the children of any node of the execution tree. It only remains to specify ComputeLocallyRelevantSeparators. For a given \(=\) we find the next merge candidate in time \(O(dn^{2})\) by computing the merge function \(D^{}_{}(A,B;)\) for all pairs of candidate (unmerged) clusters \(A,B\). If \((A^{*},B^{*})\) minimizes the merge function, the locally relevant hyperplanes are given by \(D^{}_{}(A^{*},B^{*};) D^{}_{}(A^{},B^{ };)\) for \((A^{},B^{})(A^{*},B^{*})\) i.e. \(t_{} n^{2}\). Using Theorem 2.2, we give the following bound for the overall runtime of the algorithm (soft-O notation suppresses logarithmic terms and multiplicative constants in \(d\), proof in Appendix I.2).

**Theorem 3.1**.: _Let \(S\) be a clustering instance with \(|S|=n\), and let \(R_{i}=|_{i}|\) and \(R=R_{n}\). Let \(H_{t}=|\{(_{1},_{2})_{t}^{2} _{1}_{2}\}|\) denote the total number of adjacencies between any two pieces of \(_{i}\) and \(H=H_{n}\). Then, the leaves of the execution tree on \(S\) can be computed in time \((_{i=1}^{n}(H_{i}+R_{i}T_{M})(n-i+1)^{2})\), where \(T_{M}\) is the time to compute the merge function._

In the case of single, median, and complete linkage, we may assume \(T_{M}=O(d)\) by carefully maintaining a hashtable containing distances between every pair of clusters. Each merge requires overhead at most \(O(n_{t}^{2})\), \(n_{t}=n-t\) being the number of unmerged clusters at the node at depth \(t\), which is absorbed by the cost of solving the LP corresponding to the cell of the merge. We have the following corollary which states that our algorithm is output-linear for \(d=2\).

**Corollary 3.2**.: _For \(d=2\) the leaves of the execution tree of any clustering instance \(S\) with \(|S|=n\) can be computed in time \(O(RT_{M}n^{3})\)._

Above results yield bounds on \(T_{S}\), the enumeration time for dual function of the pieces in a single problem instance. Theorem 2.3 further implies bounds on the runtime of ERM (Table 1).

Dynamic Programming based sequence alignment

Sequence alignment is a fundamental combinatorial problem with applications to computational biology. For example, to compare two DNA, RNA or amino acid sequences the standard approach is to align two sequences to detect similar regions and compute the optimal alignment . However, the optimal alignment depends on the relative costs or weights used for specific substitutions, insertions/deletions, or _gaps_ (consecutive deletions) in the sequences. Given a set of weights, the optimal alignment computation is typically a simple dynamic program. Our goal is to learn the weights, such that the alignment produced by the dynamic program has application-specific desirable properties.

Problem setup.Given a pair of sequences \(s_{1},s_{2}\) over some alphabet \(\) of lengths \(m=|s_{1}|\) and \(n=|s_{2}|\), and a'space' character \(-\), a space-extension \(t\) of a sequence \(s\) over \(\) is a sequence over \(\{-\}\) such that removing all occurrences of \(-\) in \(t\) gives \(s\). A global alignment (or simply alignment) of \(s_{1},s_{2}\) is a pair of sequences \(t_{1},t_{2}\) such that \(|t_{1}|=|t_{2}|\), \(t_{1},t_{2}\) are space-extensions of \(s_{1},s_{2}\) respectively, and for no \(1 i|t_{1}|\) we have \(t_{1}[i]=t_{2}[i]=-\). Let \(s[i]\) denote the \(i\)-th character of a sequence \(s\) and \([:i]\) denote the first \(i\) characters of sequence \(s\). For \(1 i|t_{1}|\), if \(t_{1}[i]=t_{2}[i]\) we call it a _match_. If \(t_{1}[i] t_{2}[i]\), and one of \(t_{1}[i]\) or \(t_{2}[i]\) is the character \(-\) we call it a _space_, else it is a _mismatch_. A sequence of \(-\) characters (in \(t_{1}\) or \(t_{2}\)) is called a _gap_. Matches, mismatches, gaps and spaces are commonly used _features_ of an alignment, i.e. functions that map sequences and their alignments \((s_{1},s_{2},t_{1},t_{2})\) to \(_{ 0}\) (for example, the number of spaces). A common measure of _cost_ of an alignment is some linear combination of features. For example if there are \(d\) features given by \(l_{k}()\), \(k[d]\), the cost may be given by \(c(s_{1},s_{2},t_{1},t_{2},)=_{k=1}^{d}_{k}l_{k}(s_{1},s_{2},t_{1},t _{2})\) where \(=(_{1},,_{d})\) are the parameters that govern the relative weight of the features . Let \((s,s^{},)=*{argmin}_{t_{1},t_{2}}c(s,s^{},t_ {1},t_{2},)\) and \(C(s,s^{},)=_{t_{1},t_{2}}c(s,s^{},t_{1},t_{2},)\) denote the optimal alignment and its cost respectively.

A general DP update rule.For a fixed \(\), suppose the sequence alignment problem can be solved, i.e. we can find the alignment with the smallest cost, using a dynamic program \(A_{}\) with linear parameter dependence (described below). Our main application will be to the family of dynamic programs \(A_{}\) which compute the optimal alignment \((s_{1},s_{2},)\) given any pair of sequences \((s_{1},s_{2})^{m}^{n}=\) for any \(^{d}\), but we will proceed to provide a more general abstraction. See Section J.1 for example DPs using well-known features in computational biology, expressed using the abstraction below. For any problem \((s_{1},s_{2})\), the dynamic program \(A_{}\) (\(^{d}\), the set of parameters) solves a set \((s_{1},s_{2})=\{P_{i} i[k],P_{k}=(s_{1},s_{2})\}\) of \(k\) subproblems (typically, \((s_{1},s_{2})_{s_{1},s_{2}}=\{(s_{1}[:i^{}],s_{2}[:j^{ }]) i^{}\{0,,m\},j^{}\{0,,n\}\}\)) in some fixed order \(P_{1},,P_{k}=(s_{1},s_{2})\). Crucially, the subproblems sequence \(P_{1},,P_{k}\) do not depend on \(\)9. In particular, a problem \(P_{j}\) can be efficiently solved given optimal alignments and their costs \((_{i}(),C_{i}())\) for problems \(P_{i}\) for each \(i[j-1]\). Some initial problems in the sequence \(P_{1},,P_{k}\) of subproblems are _base case_ subproblems where the optimal alignment and its cost can be directly computed without referring to a previous subproblem. To solve a (non base case) subproblem \(P_{j}\), we consider \(V\) alternative _cases_\(q:[V]\), i.e. \(P_{j}\) belongs to exactly one of the \(V\) cases (e.g. if \(P_{j}=(s_{1}[:i^{}],s_{2}[:j^{}])\), we could have two cases corresponding to \(s_{1}[i^{}]=s_{2}[j^{}]\) and \(s_{1}[i^{}] s_{2}[j^{}]\)). Typically, \(V\) will be a small constant. For any case \(v=q(P_{j})[V]\) that \(P_{j}\) may belong to, the cost of the optimal alignment of \(P_{j}\) is given by a minimum over \(L_{v}\) terms of the form \(c_{v,l}(,P_{j})= w_{v,l}+_{v,l}(,P_{j})\), where \(l[L_{v}]\), \(w_{v,l}^{d}\), \(_{v,l}(,P_{j})=C_{t}()\{C_{1}(),,C_{j-1}()\}\) is the cost of some previously solved subproblem \(P_{t}=(s_{1}[:i^{}_{t}],s_{2}[:j^{}_{t}])=(s_{1}[:i^{}_{v,l,j}],s_{2}[:j^{}_{v,l,j}])\) (i.e. \(t\) depends on \(v,l,j\) but not on \(\)), and \(c_{v,l}(,P_{j})\) is the cost of alignment \(_{v,l}(,P_{j})=T_{v,l}(_{}())\) which extends the optimal alignment for subproblem \(P_{t}\) by a \(\)-independent transformation \(T_{v,l}()\). That is, the DP update for computing the cost of the optimal alignment takes the form

\[DP(,P_{j})=_{l}\{ w_{q(P_{j}),l}+_{q(P_{j}),l}(,P_{j})\},\] (1)

and the optimal alignment is given by \(DP^{}(,P_{j})=_{q(P_{j}),l^{*}}(,P_{j}),\) where \(l^{*}=*{argmin}_{l}\{ w_{q(P_{j}),l}+_{q(P_{j}),l}( ,P_{j})\}\). The DP specification is completed by including base cases \(\{C(s,s^{},)= w_{s,s^{}}(s,s^{}) (s_{1},s_{2})\}\) (or \(\{(s,s^{},)=_{s,s^{}}(s,s^{})(s _{1},s_{2})\}\) for the optimal alignment DP) corresponding to a set of base case subproblems \((s_{1},s_{2})_{s_{1},s_{2}}\). Let \(L=_{v[V]}L_{v}\) denote the maximum number of subproblems needed to compute a single DP update in any of the cases. \(L\) is often small, typically 2 or 3 (see examples in Section J.1). Our main result is to provide an algorithm for computing the polytopic pieces of the dual class functions efficiently for small constants \(d\) and \(L\).

As indicated above, we consider the family of dynamic programs \(A_{}\) which compute the optimal alignment \((s_{1},s_{2},)\) given any pair of sequences \((s_{1},s_{2})^{m}^{n}=\) for any \(^{d}\). For any alignment \((t_{1},t_{2})\), the algorithm has a fixed real-valued utility (different from the cost function above) which captures the quality of the alignment, i.e. the utility function \(u((s_{1},s_{2}),)\) only depends on the alignment \((s_{1},s_{2},)\). The dual class function is piecewise constant with convex polytopic pieces (Lemma J.5 in Appendix J.3). For any fixed problem \((s_{1},s_{2})\), the space of parameters \(\) can be partitioned into \(R\) convex polytopic regions where the optimal alignment is fixed. The optimal parameter can then be found by simply comparing the costs of the alignments in each of these pieces. For the rest of this section we consider the algorithmic problem of computing these \(R\) pieces efficiently.

For the clustering algorithm family, as we have seen in Section 3, we get a refinement of the parameter space with each new step (merge) performed by the algorithm. This does not hold for the sequence alignment problem. Instead we obtain the following DAG, from which the desired pieces can be obtained by looking at nodes with no out-edges (call these _terminal_ nodes). Intuitively, the DAG is built by iteratively adding nodes corresponding to subproblems \(P_{1},,P_{k}\) and adding edges directed towards \(P_{j}\) from all subproblems that appear in the DP update for it. That is, for _base case_ subproblems, we have singleton nodes with no incoming edges. Using the recurrence relation (1), we note that the optimal alignment for the pair of sequences \((s_{1}[:i],s_{2}[:j])\) can be obtained from the optimal alignments for subproblems \(\{(s_{1}[:i;i],s_{2}[:j^{}])\}_{l[L_{v^{}}]}\) where \(v^{}=q(s_{1}[:i],s_{2}[:j])\). The DAG for \((s_{1}[:i],s_{2}[:j])\) is therefore simply obtained by using the DAGs \(G_{v^{},l}\) for the subproblems and adding directed edges from the terminal nodes of \(G_{v^{},l}\) to new nodes \(v_{p,i,j}\) corresponding to each piece \(p\) of the partition \(P[i][j]\) of \(\) given by the set of pieces of \(u_{(s_{1}[:i],s_{2}[:j])}()\). A more compact representation of the execution graph would have only a single node \(v_{i,j}\) for each subproblem \((s_{1}[:i],s_{2}[:j])\) (the node stores the corresponding partition \(P[i][j]\)) and edges directed towards \(v_{i,j}\) from nodes of subproblems used to solve \((s_{1}[:i],s_{2}[:j])\). Note that the graph depends on the problem instance \((s_{1},s_{2})\) as the relevant DP cases \(v^{}=q(s_{1}[:i],s_{2}[:j])\) depend on the sequences \(s_{1},s_{2}\). A naive way to encode the execution graph would be an exponentially large tree corresponding to the recursion tree of the recurrence relation (1).

_Execution DAG._ Formally we define a _compact execution graph_\(G_{e}=(V_{e},E_{e})\) as follows. For the base cases, we have nodes labeled by \((s,s^{})(s_{1},s_{2})\) storing the base case solutions \((w_{s,s^{}},_{s,s^{}})\) over the unpartitioned parameter space \(=^{d}\). For \(i,j>0\), we have a node \(v_{i,j}\) labeled by \((s_{1}[:i],s_{2}[:j])\) and the corresponding partition \(P[i][j]\) of the parameter space, with incoming edges from nodes of the relevant subproblems \(\{(s_{1}[:i;v_{v^{},l}],s_{2}[:j^{},l])\}_{l[L_{v^{}}]}\) where \(v^{}=q(s_{1}[:i],s_{2}[:j])\). This graph is a DAG since every directed edge is from some node \(v_{i,j}\) to a node \(v_{i^{},j^{}}\) with \(i^{}+j^{}>i+j\) in typical sequence alignment dynamic programs (Appendix J.1). Algorithm 5 (Appendix J.2)) gives a procedure to compute the partition of the parameter space for any given problem instance \((s_{1},s_{2})\) using the compact execution DAG. We give intuitive overviews of the three main routines in Algorithm 5.

* ComputeOverlayDP computes an overlay \(_{i}\) of the input polytopic subdivisions \(\{_{s} s S_{i}\}\) and uses Clarkson's algorithm for intersecting polytopes with output-sensitive efficiency. We show that the overlay can be computed by solving at most \(^{L}\) linear programs (Algorithm 6, Lemma J.1).
* ComputeSubdivisionDP applies Algorithm 1, in each piece of the overlay we need to find the polytopic subdivision induced by \(O(L^{2})\) hyperplanes (the set of hyperplanes depends on the piece). This works because all relevant subproblems have the same solution within any piece of the overlay.
* Finally ResolveDegeneraciesDP merges pieces where the optimal alignment is identical using a simple search over the resulting subdivision.

For our implementation of the subroutines, we have the following guarantee for Algorithm 5.

**Theorem 4.1**.: _Let \(R_{i,j}\) denote the number of pieces in \(P[i][j]\), and \(=_{i m,j n}R_{i,j}\). If the time complexity for computing the optimal alignment is \(O(T_{})\), then Algorithm 5 can be used to compute the pieces for the dual class function for any problem instance \((s_{1},s_{2})\), in time \(O(d!L^{4d}^{2L+1}T_{})\)._

For the special case of \(d=2\), we show that (Theorem J.6, Appendix J.3) the pieces may be computed in \(O(RT_{})\) time using the ray search technique of .

Acknowledgments

We thank Dan DeBlasio for useful discussions on the computational biology literature. We also thank Avrim Blum and Mikhail Khodak for helpful feedback. This material is based on work supported by the National Science Foundation under grants CCF-1910321, IIS-1901403, and SES-1919453; the Defense Advanced Research Projects Agency under cooperative agreement HR00112020003; a Simons Investigator Award; an AWS Machine Learning Research Award; an Amazon Research Award; a Bloomberg Research Grant; a Microsoft Research Faculty Fellowship.