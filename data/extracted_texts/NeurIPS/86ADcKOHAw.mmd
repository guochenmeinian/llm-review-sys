# Nearest Neighbour with Bandit Feedback

Stephen Pasteris

The Alan Turing Institute

London UK

spasteris@turing.ac.uk

&Chris Hicks

The Alan Turing Institute

London UK

c.hicks@turing.ac.uk

&Vasilios Mavroudis

The Alan Turing Institute

London UK

vmavroudis@turing.ac.uk

###### Abstract

In this paper we adapt the nearest neighbour rule to the contextual bandit problem. Our algorithm handles the fully adversarial setting in which no assumptions at all are made about the data-generation process. When combined with a sufficiently fast data-structure for (perhaps approximate) adaptive nearest neighbour search, such as a navigating net, our algorithm is extremely efficient - having a per trial running time polylogarithmic in both the number of trials and actions, and taking only quasi-linear space. We give generic regret bounds for our algorithm and further analyse them when applied to the stochastic bandit problem in euclidean space. We note that our algorithm can also be applied to the online classification problem.

## 1 Introduction

In this paper we adapt the classic _nearest neighbour_ rule to the contextual bandit problem and develop an extremely efficient algorithm. The problem proceeds in trials, where on trial \(t\): (1) a _context_\(x_{t}\) is revealed to us, (2) we must select an _action_\(a_{t}\), and (3) the _loss_\(_{t,a_{t}}\) of action \(a_{t}\) on trial \(t\) is revealed to us. We assume that the contexts are points in a metric space and the distance between two contexts represents their similarity. A _policy_ is a mapping from contexts to actions and the inductive bias of our algorithm is towards learning policies that typically map similar contexts to similar actions. Our main result has absolutely no assumptions whatsoever about the generation of the context/loss sequence and has no restriction on what policies we can compare our algorithm to.

Our algorithm requires, as a subroutine, a data-structure that performs \(c\)-nearest neighbour search. This data-structure must be _adaptive_ - in that new contexts can be inserted into it over time. An example of such a data-structure is the _Navigating net_ which, given mild conditions on our metric and dataset, performs both search and insertion in polylogarithmic time. When utilising a data-structure of this speed our algorithm is extremely efficient - with a per-trial time complexity polylogarithmic in both the number of trials and actions, and requiring only quasi-linear space.

As an example we will apply our methodology to the special case of the contextual bandit problem in which the context sequence is drawn i.i.d. from a probability distribution over the \(d\)-dimensional hypercube. In this case, for any policy \(\) with a finite-volume decision boundary, our algorithm achieves \(}(T^{d/(d+1)}K^{1/(d+1)})\) regret w.r.t. \(\), where \(\) measures the magnitude of what is essentially the part of the decision boundary of \(\) that lies in the support of the probability distribution.

In the course of this paper we develop some novel algorithmic techniques, including a new algorithmic framework CanProp and efficient algorithms for searching in trees, which may find further application.

We now describe related works. The bandit problem  was first introduced in  but was originally studied in the stochastic setting in which all losses are drawn i.i.d. at random , , . However, our world is very often not i.i.d. stochastic. The work of  introduced the seminal Exp3 algorithm which handled the case in which the losses were selected arbitrarily. This work alsointroduced the Exp4 algorithm for contextual bandits. In general this algorithm is exponential time but in some situations can be implemented in polynomial time - such as their Exp3.S algorithm, which was a bandit version of the classic FixedShare algorithm . In  the Exp3.S setting was greatly generalised to the situation in which the contexts where vertices of a graph. They utilised the methodology of ,  and  in order to develop extremely efficient algorithms. Although inspiring this work, these algorithms cannot be utilised in our situation as they inherently require the set of queried contexts to be known a-priori. In the stochastic case another class of contextual bandit problems are _linear bandits_,  in which the contexts are mappings from the actions into \(^{d}\). Here the queried contexts need not be known in advance but the losses must be drawn i.i.d. from a distribution that has mean linear in the respective context. The \(k\) nearest neighbour algorithm was first analysed in . The work  utilised the \(k\) nearest neighbour methodology and the works  and  to handle a stochastic contextual bandit problem. However, their setting is extremely more restricted than ours. In particular, the context/loss pairs must be drawn i.i.d. at random and the probability distribution they are sampled from must obey certain strict conditions. In addition, on each trial the contexts seen so far must be ordered in increasing distance from the current context and operations must be performed on this sequence, making their algorithm exponentially slower than ours. Our algorithm utilises the works of  and  as subroutines. It should be noted that the later work, which was based on , was improved on in  - we leave it as an open problem as to whether we can utilise their work in our algorithm.

Algorithms for contextual bandits in metric spaces have been studied in [15; 19; 24; 25; 27; 29; 30; 31; 2; 26] but as far as we know ours is the first work to give a non-trivial bound for our problem in general (with no additional assumptions). As far as we are aware our above example bound for the fully stochastic case in euclidean space (with finite decision boundary) is also novel - the works [30; 24; 26] scale as \(}(T^{(d+1)/(d+2)})\) in general (and [24; 26] also require additional assumptions). As far as we are aware we are also the first work, stochastic or otherwise, to give a regret scaling as \(}(T^{1/2})\) when the contexts are drawn from well-separated clusters (i.e. there is a positive distance between all pairs of clusters) in a finite-dimensional metric space, and the comparator policy is constant on each cluster (we do, however, conjecture that in the stochastic case  can obtain such a regret scaling here - but, as stated above, it is exponentially slower).

## 2 Notation

Let \(\) be the set of natural numbers not including \(0\). Given a natural number \(m\) we define \([m]:=\{j j m\}\). Given a predicate \(p\) we define \( p:=1\) if \(p\) is true and \( p:=0\) otherwise. We define \(()\) and \(()\) to be the logarithms with base \(2\) and \(e\) respectively. Given sets \(\) and \(\) we denote by \(^{}\) the set of all functions \(f:\) and by \(2^{}\) the set of all subsets of \(\). We write \(x\) to mean that the value \(x\) is drawn from the uniform distribution on \(\).

All trees in this paper are considered rooted. Given a tree \(\) we denote its root by \(r()\), its vertex set by \(\), its leaves by \(^{}\), and its internal vertices by \(^{}\). Given a vertex \(v\) in a tree \(\) we denote its parent by \(_{}(v)\) and the subtree of all its descendants by \(_{}(v)\). Given an internal node \(v\) in a (full) binary tree \(\) we denote its left and right children by \(_{}(v)\) and \(_{}(v)\) respectively. Internal nodes \(v\) in a (full) ternary tree \(\) have an additional child \(_{}(v)\) called the _centre_ child. Given vertices \(v\) and \(v^{}\) in a tree \(\) we denote by \(_{}(v,v^{})\) the _least common ancestor_ of \(v\) and \(v^{}\): i.e. the vertex of maximum depth which is an ancestor of both \(v\) and \(v^{}\). We will drop the subscript \(\) in all these functions when unambiguous. Given a tree \(\), a _subtree_ of \(\) is a tree whose edge set is a subset of that of \(\).

## 3 Results

### The General Result

We consider the following game between _Learner_ (us) and _Nature_ (our adversary). We call this game the _similarity bandit problem_. We have \(K\)_actions_. Learning proceeds in \(T\) trials. A-priori Nature chooses a sequence \( n(t) t[T]\{1\}\) where for all \(t[T]\{1\}\) we have \(n(t)[t-1]\). A-priori Nature also chooses a sequence of loss vectors \(_{t} t[T]^{K}\), but does not reveal them to Learner. On the \(t\)-th trial the following happens:1. If \(t>1\) then Nature reveals \(n(t)\) to Learner.
2. Learner chooses some action \(a_{t}[K]\).
3. Nature reveals \(_{t,a_{t}}\) to Learner.

Intuitively, given a trial \(t[T]\{1\}\), \(n(t)\) is a _similar_ trial to \(t\). Our inductive bias is that if an action \(a[K]\) is good for trial \(t\) then it is likely that \(a\) will also be good for the similar trial \(n(t)\). We will measure our performance with respect to any policy \(\), where a policy is defined as a vector in \([K]^{T}\). Specifically, we wish to minimise the _\(\)-regret_, which is defined as the difference between the total cumulative loss suffered by Learner and that which Learner would have suffered if it had instead chosen \(a_{t}\) equal to \(y_{t}\) for all trials \(t\). Formally, this quantity is defined as follows:

**Definition 3.1**.: _Given a policy \([K]^{T}\) we define the \(\)-regret of Learner as:_

\[R():=_{t[T]}_{t,a_{t}}-_{t[T]}_{t,y_{t}}\,.\]

The following quantity quantifies how much a policy agrees with our inductive bias.

**Definition 3.2**.: _Given a policy \([K]^{T}\) we define the complexity of \(\) by:_

\[():=1+_{t[T]\{1\}} y_{t} y_{n(t)} \,.\]

We now state our main result:

**Theorem 3.3**.: _Consider the similarity bandit problem described above. Our algorithm CBNN takes a single parameter \(>0\) and, for all policies \([K]^{T}\) simultaneously, obtains an expected \(\)-regret bounded by:_

\[[R()]}((+)}{}))\]

_where the expectation is taken over the randomisation of the algorithm. CBNN needs no initialisation time and has a per-trial time complexity of:_

\[((T)^{2}(K))\,.\]

### Bandits in a Metric Space

We consider the following game between _Learner_ (us) and _Nature_ (our adversary). We call this game the _metric bandit problem_. We have \(K\)_actions_ and a metric space \((,)\) where \(\) is a (possibly infinite) set of _contexts_ and for all \(x,x^{}\) we have that \((x,x^{})\) is the _distance_ from \(x\) to \(x^{}\). We assume that Learner does not necessarily know \((,)\) a-priori but has access to an oracle for computing \((x,x^{})\) for any \(x,x^{}\). Learning proceeds in \(T\) trials. A-priori Nature chooses a sequence of contexts \( x_{t}\,|\,t[T]\) and a sequence of loss vectors \(_{t}\,|\,t[T]^{K}\), but does not reveal them to Learner. On the \(t\)-th trial the following happens:

1. Nature reveals \(x_{t}\) to Learner.
2. Learner chooses some action \(a_{t}[K]\).
3. Nature reveals \(_{t,a_{t}}\) to Learner.

Here, our inductive bias is that if an action \(a[K]\) is good for a context \(x\) then it is likely also good for contexts that are near to \(x\) with respect to the metric \(\). Our algorithm for this problem will be based on the concept of a \(c\)-nearest neighbour which is defined as follows.

**Definition 3.4**.: _Given some \(c 1\), a finite set \(\), and a context \(x\) we have that some \(\) is a \(c\)-nearest neighbour of \(x\) in the set \(\) if and only if:_

\[(x,) c_{x^{}}(x,x^{})\,.\]

In order to utilise CBNN for this problem we need a data-structure for _adaptive nearest neighbour search_. This problem is as follows. We maintain a finite set \(\). At any point in time we must either:* Insert a new context \(x\) into the set \(\) and update the data-structure.
* Given a context \(x\), utilise the data-structure to find a \(c\)-nearest neighbour of \(x\) in the set \(\).

An efficient example of such a data-structure is the _navigating net_.

We can now reduce the metric bandit problem to the similarity bandit problem as follows. On any trial \(t[T]\{1\}\) choose \(_{t}\) to be a \(c\)-nearest neighbour of \(x_{t}\) in the set \(\{x_{t^{}}\,|\,t^{}[t-1]\}\). Then choose \(n(t)[t-1]\) such that \(x_{n(t)}=_{t}\).

We will utilise the following definition in order to bound the complexity of policies when \(n()\) is chosen in this way.

**Definition 3.5**.: _Given a function \(:[K]\) and a set \(\) then for all \(x\) define:_

\[(x,,):=\{(x,x^{}) x^{} \,\,(x^{})(x)\}\,.\]

We can now bound the complexity of policies as follows, noting that by Theorem 3.3 this leads directly to a regret bound.

**Theorem 3.6**.: _Assume we have a sequence \( x_{t}\,|\,t[T]\) and a function \(:[K]\). Define \(:=\{x_{t}\,|\,t[T]\}\) and for all \(t[T]\) define \(y_{t}:=(x_{t})\). Assume that for all \(t[T]\{1\}\) we have that \(x_{n(t)}\) is a \(c\)-nearest neighbour of \(x_{t}\) in the set \(\{x_{t^{}}\,|\,t^{}[t-1]\}\). Then \(()\) is no greater than the minimum cardinality of any set \(\) in which for all \(x^{}\) there exists \(x\) with \((x,x^{})<(x,,)/3c\)._

A strength of our algorithm is that it can be combined with _binning_ algorithms, where the contexts \(x_{t}\) are partitioned into sets called _bins_ and, for each bin, all contexts in that bin are replaced by a single context called the _centre_ of the bin. The advantage of binning is that \((x,,)\) can increase, so that by Theorem 3.6 we may have that \(()\) decreases. However, when a context \(x_{t}\) is binned (i.e. replaced by its bin centre) its label \((x_{t})\) can change, increasing the final regret by \((1)\). In Section 3.3 we give an example of the utilisation of binning.

### Stochastic Bandits in Euclidean Space

As an example we now consider the utilisation of the above algorithms for the problem of stochastic bandits in \(^{d}\) for some arbitrary \(d\) which we view as a constant in our bounds. Here we will only focus on what happens in the limit \(T\). We focus on stochastic bandits for simplicity, but the same methodology can be used to study the limiting behaviour of adversarial bandits. In this problem we have an unknown probability density \(:^{d}^{K}\). We have \(T\) trials. On trial \(t\) the following happens:

1. Nature draws \((z_{t},_{t})\) from \(\).
2. Nature reveals \(z_{t}\) to Learner.
3. Learner chooses some action \(a_{t}[K]\).
4. Nature reveals \(_{t,a_{t}}\) to Learner.

To aid us in this problem we will first quantise the contexts \(z_{t}\) to a grid. Note that this is an example of binning. The grid is defined as follows:

**Definition 3.7**.: _Given \(q\) define \(_{q}^{d}\) to be the set of vectors in \(^{d}\) in which each component is an integer multiple of \(1/q\)._

On each trial \(t\) we will first quantise the vector \(z_{t}\) by defining \(x_{t}\) to be its nearest neighbour (w.r.t. the euclidean metric) in \(_{q}^{d}\), where \(q:=(T/K)^{1/(d+1)}\). Note that this can be done in constant time per trial. As in Section 3.2 we then use CBNN to solve the problem by defining \(n(t)\) such that \(x_{n(t)}\) is a \(c\)-nearest neighbour (w.r.t. the euclidean metric) of \(x_{t}\) in the set \(\{x_{t^{}}\,|\,t^{}[t-1]\}\). We consider \(c\) as a constant in our bounds.

As before, we will compare our cumulative loss to that of a policy that follows a function \(:^{d}[K]\). Our regret bound will be based on the following quantities:

**Definition 3.8**.: _Let \(\) be the marginal of \(\) with respect to its first argument and let \(\) be the euclidean metric on \(^{d}\). For all \(>0\) define:_

\[(,):=\{x^{d}\,|\,\,x^{}^{d} \,:\,(x^{}) 0\,\,(x,x^{})\}\]

_which is the set of contexts that are within distance \(\) of the support of \(\). Given \(:^{d}[K]\) we make the following definitions. For any \(>0\) define:_

\[(,,,):=\{x^{d}\,|\,\,x^{ }(,)\,:\,(x,x^{})\, \,(x)(x^{})\}\]

_which is the set of contexts that are at distance no more than \(\) from the intersection of the decision boundary of \(\) and \((,)\). We then define:_

\[(,):=_{ 0}_{ 0}_ {x(,,,)}1;( ,):=_{ 0}_{ 0}_{x (,,,)}(x)\]

_which are essentially the volumes of the part of the decision boundary of \(\) that lies in the support of \(\), with respect to the uniform density and the density \(\) respectively._

With these definitions in hand we now present our regret bound, which utilises Theorem 3.6 in its proof:

**Theorem 3.9**.: _Let \(q:=(T/K)^{1/(d+1)}\). For all \(t[T]\) let \(x_{t}\) be the nearest neighbour of \(z_{t}\) in the set \(_{q}^{d}\). For all \(t[T]\{1\}\) let \(n(t)\) be such that \(x_{n(t)}\) is a \(c\)-nearest neighbour of \(x_{t}\) in the set \(\{x_{t^{}}\,t^{}[t-1]\}\). Given some \(:^{d}[K]\) let \(y_{t}:=(z_{t})\) for all \(t[T]\). Then when \(:=q^{}\) CBNN gives us:_

\[[R()]}((1+(,)+ (,))T^{}K^{})\]

_as \(T\)._

We note that varying \(q\) and \(\) in Theorem 3.9 will allow us to trade off the values \(1\), \((,)\) and \((,)\) in different ways.

## 4 The Algorithm

In this section we describe our algorithm CBNN, for solving the similarity bandit problem, and give the pseudocode for the novel subroutines. In the appendix we give a more detailed description of how CBNN works and prove our theorems.

Instead of working directly with trial numbers we create a sequence of distinct _nodes_\( x_{t}\,|\,t[T]\) and define, for all \(t[T]\{1\}\), the node \(n(x_{t}):=x_{n(t)}\). Let \(:=\{x_{i} t[T]\}\). We can now represent policies as functions from \(\) into \([K]\). Hence, given some \(y:[K]\), we define the _\(y\)-regret_ and the _complexity_ of \(y\) as:

\[R(y):=_{t[T]}_{t,a_{t}}-_{t[T]}_{t,y(x_{t})}; (y):=1+_{x\{x_{1}\}}[\![y(x) y(n(x))]\!]\]

respectively.

### A Simple but Inefficient Algorithm

To give the reader intuition we first describe our initial idea - a simple algorithm which attains our desired regret bound but is exponentially slower - taking a per-trial time of \((KT)\). The algorithm is based on Exp4 which we now describe. On every trial \(t\) we maintain a weighting \(_{t}:[K]^{}\). We are free to choose the initial weighting \(_{1}\) to be any probability distribution. On each trial \(t\) the following happens:

1. For all \(a[K]\) set \(p_{t,a}_{y[K]^{}}[\![y(x_{t})=a]\!]_{t}(y)\,\).
2. Set \(a_{t} a\) with probability proportional to \(p_{t,a}\,\).
3. Receive \(_{t,a_{t}}\,\).

4. For all \(a[K]\) set \(_{t,a}[\![a=a_{t}]\!]_{t,a_{t}}||_{t}||_{1} /p_{t,a_{t}}\,.\)
5. For all \(y[K]^{}\) set \(_{t+1}(y)_{t}(y)(-_{t,y(x_{t})})\,.\)

For us we choose, for all \(y:[K]\,\), an initial weight of:

\[_{1}(y):=(1/K)(T(K-1))^{-(y)}\,(1-1/T)^{(T-1-(y))}\,\,.\]

Of course, we don't know \((y)\) a-priori, and hence we cannot implement Exp4 explicitly (and it would take exponential time even if we did know \((y)\) a-priori). Our crucial insight is the following. For any \(t[T]\) let \(_{t}:=\{x_{t^{}}\,|\,t^{}[t]\}\) and for any \(y:[K]\) let \(y^{t}\) be the restriction of \(y\) onto \(_{t}\). Then for any \(t[T]\) and for any \(y^{}:_{t}[K]\) we have:

\[_{y[K]^{}\,:\,y^{}=y^{}}_{1}(y) _{x_{t}\{x_{1}\}}((n(x))  y^{}(x)]\!]}{T(K-1)}+[\![y^{}(n(x))=y^{}(x)]\!](1- ))\,.\] (1)

Note that, for all \(t[T]\) and \(a[K]\,\), we can write \(p_{t,a}\) as follows:

\[p_{t,a} =_{y[K]^{}}[\![y(x_{t})=a]\!]_{1}(y) _{t^{}[t-1]}(-_{t,y(x_{t})})\] \[=_{y^{}[K]^{_{t}}}[\![y^{}(x_{t})=a ](_{t^{}[t-1]}(-_{t,y^{}(x_{t})}) )_{y[K]^{}\,:\,y^{}=y^{}}_{1}(y)\,.\]

By substituting in Equation (1) we have now brought \(p_{t,a}\) into a form that can be solved via _Belief propagation_ over the tree with vertex set \(_{t}\) and in which, for all \(t^{}[t]\{1\}\,\), the parent of \(x_{t^{}}\) is \(n(x_{t^{}})\).

It is well known that for any \(y:[K]\,\), Exp4 attains a \(y\)-regret of at most \((_{1}(y))/+ KT/2\). By setting \(:=/\) and noting our choice of \(_{1}\) we obtain our desired regret bound.

### Cancellation Propagation

In the remainder of this section we describe our algorithm CBNN, which is based on the same idea as the simple algorithm of Section 4.1.

In this subsection we describe a novel algorithmic framework CanProp for designing contextual bandit algorithms with a running time logarithmic in \(K\). It is inspired by Exp3, specialist algorithms  and online decision-tree pruning algorithms  but is certainly not a simple combination of these works. CBNN will be an efficient implementation of an instance of CanProp. Although in general CanProp requires a-priori knowledge, CBNN is designed in a way that, crucially, does not need it to be known.

We assume, without loss of generality, that \(K\) and \(T\) are integer powers of two. CanProp, which takes a parameter \(>0\), works on a full, balanced binary tree \(\) with leaves \(^{*}=[K]\). On every trial \(t\) each pair \((v,) 2^{}\) has a weight \(w_{t}(v,)\). These weights induce a function \(_{t}:\) defined by:

\[_{t}(v):=_{ 2^{}}[\![x_{t}]\!]w_{t }(v,)\,.\]

On each trial \(t\) a root-to-leaf path \(\{v_{t,j} j[(K)]\{0\}\}\) is sampled such that \(v_{t,0}:=r()\) and, given \(v_{t,j}\,\), we have that \(v_{t,(j+1)}\) is sampled from \(\{(v_{t,j}),(v_{t,j})\}\) with probability proportional to the value of \(_{t}\) when applied to each of these vertices. The action \(a_{t}\) is then chosen equal to \(v_{t,(K)}\). Once the loss has been observed we climb back up the root-to-leaf path, updating the function \(w_{t}\) to \(w_{t+1}\).

CanProp (at trial \(t\)) is given in Algorithm 1. We note that if \(w_{t+1}(v,)\) is not set in the pseudocode then it is defined to be equal to \(w_{t}(v,)\).

In Appendix B we give a general regret bound for CanProp. For CBNN we set:

\[:=\]and for all \((v,) 2^{}\) we set:

\[w_{1}(v,):=_{x\{x_{1}\}}( (x,)+(1-(x,))(1- ))\] (2)

where:

\[(x,):=[\![x]\!]\![n(x) ]\!].\]

This choice gives us the regret bound in Theorem 3.3. We note that CBNN will be implemented in such a way that \(n\) need not be known a-priori.

### Ternary Search Trees

As we shall see, CBNN works by storing a binary tree \((v)\) at each vertex \(v\). In order to perform efficient operations on these trees we will utilise the rebalancing data-structure defined in  which here we shall call a _ternary search tree_ (TST) due to the fact that it is a generalisation of the classic _binary search tree_ and, as we shall show, has searching applications. However, as for binary search trees, the applications of TSTs are more than just searching: we shall also utilise them for online belief propagation.

We now define what is meant by a TST. Suppose we have a full binary tree \(\). A TST of \(\) is a full ternary tree \(\) which satisfies the following. The vertex set of \(\) is partitioned into two sets \(^{}\) and \(^{}\) where each vertex \(s\) is associated with a vertex \((s)\) and every \(s^{}\) is also associated with a vertex \(^{}(s)((s))^{}\). In addition, each internal vertex \(s^{}\) is associated with a vertex \((s)\). For all \(u\) there exists an unique leaf \(_{}(u)^{}\) in which \((_{}(u))=u\).

Essentially, each vertex \(s\) corresponds to a subtree \(}(s)\) of \(\) where \(}(r())=\). Such a vertex \(s\) is a leaf of \(\) if and only if \(|}(s)|=1\). For each internal vertex \(s^{}\) the subtree \(}(s)\) is _split_ at the vertex \((s)\) into the subtrees \(}((s))\), \(}((s))\), and \(}((s))\) corresponding to the children of \(s\). The process continues recursively.

For completeness we now describe the rules that a TST \(\) of \(\) must satisfy. We have that \(r()^{}\) and \((r()):=r()\). Each vertex \(s\) represents a subtree \(}(s)\) of \(\). If \(s^{}\) then \(}(s):=((s))\) and otherwise \(}(s)\) is the set of all descendants of \((s)\) which are not proper descendants of \(^{}(s)\). Given that \(s^{}\) this subtree is _split_ at the vertex \((s)\) where if \(s^{}\) we have that \((s)\) lies on the path from \((s)\) to \(^{}(s)\). The children of \(s\) are then defined so that \(}((s))=}(s)( ((s)))\) and \(}((s))=}(s)( ((s)))\) and \(}((s))=}(s)(}((s))}((s)))\).

```
1:\(a_{t} v_{t,(K)}\)
2:\(_{t}_{j(K)}_{t}(v_{t,j})\)
3:\(_{t,(K)}(-_{t,a_{t}}/_{t})\)
4:for\(j=(K),((K)-1),,1\)do
5:\(_{t,(j-1)} 1-(1-_{t,j})_{t}(v_{t,j})\)
6:\(^{}_{t,j}_{t,j}/_{t,j-1}\)
7:if\(v_{t,j}=(v_{t,j-1})\)then
8:\(_{t,j}(v_{t,j-1})\)
9:else
10:\(_{t,j}(v_{t,j-1})\)
11:endif
12:for\( 2^{}:x_{t}\)do
13:\(w_{t+1}(v_{t,j},) w_{t}(v_{t,j},)^{}_{t,j}\)
14:\(w_{t+1}(_{t,j},) w_{t}(_{t,j},)/ _{t,j-1}\)
15:endfor
16:endfor ```

**Algorithm 1**CanProp at trial \(t\)

For completeness we now describe the rules that a TST \(\) of \(\) must satisfy. We have that \(r()^{}\) and \((r()):=r()\). Each vertex \(s\) represents a subtree \(}(s)\) of \(\). If \(s^{}\) then \(}(s):=((s))\) and otherwise \(}(s)\) is the set of all descendants of \((s)\) which are not proper descendants of \(^{}(s)\). Given that \(s^{}\) this subtree is _split_ at the vertex \((s)\) where if \(s^{}\) we have that \((s)\) lies on the path from \((s)\) to \(^{}(s)\). The children of \(s\) are then defined so that \(}((s))=}(s)( ((s)))\) and \(}((s))=}(s)( ((s)))\) and \(}((s))=}(s)(}((s))}((s)))\).

else

\(_{t,j}(v_{t,j-1})\)

\(v_{t,j+1}(v_{t,j})\)

\(v_{t,j+1}(v_{t,j})\)
For all binary trees \(\) in our algorithm we shall maintain a TST \(()\) of \(\) with height \(((||))\). Such trees \(\) are _dynamic_ in that on any trial it is possible that two vertices, \(u\) and \(u^{}\), are added to the tree \(\) such that \(u^{}\) is inserted between a non-root vertex of \(\) and its parent, and \(u\) is designated as a child of \(u^{}\). We define the subroutine \(((),u)\) as one which rebalances the TST \(()\) after this insertion, so that the height of \(()\) always remains in \(((||))\). The work of  describes how this subroutine can be implemented in a time of \(((||))\) and we refer the reader to this work for details (noting that they use different notation).

### Contractions

Define the quantity \(_{0}:=0\) and for all \(j\{0\}\) inductively define:

\[_{j+1}:=(1-)_{j}+(1-_{j})\,.\]

At any trial \(t\) the contexts in \(\{x_{s} s[t]\}\) naturally form a tree by designating \(n(x_{s})\) as the parent of \(x_{s}\). However, to utilise the TST data-structure we must only have binary trees. Hence, we will work with a (dynamic) full binary tree \(\) which, on trial \(t\), is a _binarisation_ of the above tree. The relationship between these two trees is given by a map \(:_{t}\{x_{s} s[t]\}\) where \(_{t}\) is the tree \(\) on trial \(t\). For all \(x\{x_{s} s[t]\}\) we will always have an unique leaf \((x)_{t}^{*}\) in which \(((x))=x\). We also maintain a balanced TST \(()\) of \(\).

Algorithm 2 gives the subroutine \(_{t}\) which updates \(\) at the start of trial \(t\). Note that \(_{t}\) also defines a function \(d:\) such that \(d(u)\) is the number of times the function \(n\) must be applied to \((u)\) to reach \(x_{1}\).

```
1:\(u(n(x_{t}))\)
2:\(u^{*}(u)\)
3:\(u^{}\)
4:\(u^{}\)
5:\((u^{}) n(x_{t})\)
6:\((u^{}) x_{t}\)
7:\((x_{t}) u^{}\)
8:if\(u=(u^{*})\)then
9:\((u^{*}) u^{}\) ```

**Algorithm 2**\(_{t}\) which works on \(\)

A _contraction_ (of \(\)) is defined as a full binary tree \(\) in which the following holds. (1) The vertices of \(\) are a subset of those of \(\). (2) \(r()=r()\). (3) Given a vertex \(u\) we have \(_{}(u)_{}(_{ }(u))\) and \(_{}(u)_{}(_{ }(u))\). (4) Any leaf of \(\) is a leaf of \(\).

CBNN will maintain, on every vertex \(v\), a contraction \((v)\) as well as a TST \(((v))\) of \((v)\). Given \(\) is one of these contractions, we also maintain, for all \(i,i^{}\{0,1\}\) and all \(u\), a value \(_{i,i^{}}(,u)_{+}\). Technically these quantities, which depend on the above function \(d\), define a _bayesian network_ on \(\) which is explained in Appendix C.3. For all \(i\{0,1\}\) and all \(u\) we also maintain a value \(_{i}(,u)\) initialised equal to \(1\).

On each of our contractions \(\) we will define, on trial \(t\), a subroutine \(_{t}()\) that simply modifies \(\) so that \((x_{t})\) is added to its leaves. This subroutine is only called on certain trials \(t\). Specifically, it is called on the contraction \((v)\) only when \(v\) is involved in CanProp on trial \(t\). Although the effect of this subroutine is simple to describe, its polylogarithmic-time implementation is quite complex. A function that is used many times during this subroutine is \(:\{\,\,,\}\) in which \((u,u^{})\) is equal to \(\), \(\) or \(\) if \(u^{}\) is contained in \(_{}(_{}(u))\), in \(_{}(_{}(u))\) or in neither, respectively. Algorithm 3 shows how to compute this function. Now that we have a subroutine for computing \(\) we can turn to the pseudocode for the subroutine \(_{t}()\) in Algorithm 4. In the appendix we give a full description of how and why this subroutine works.

```
1:\(()\)
2:if\(u=u^{}\)then
3:return\(\)
4:endif
5:\(_{}(u)\)
6:\(^{}_{}(u^{})\)
7:\(s^{*}_{}(,^{})\)
8:for\(s\{(s^{*}),\,(s^{*}),(s^{*})\}\)do
9:if\((s)\)then
10:\( s\)
11:endif
12:if\(^{}(s)\)then
13:\(^{} s\)
14:endif
15:endfor
16:if\((s^{*})\)then
17:return\(\)
18:endif
19:if\((s^{*})=u^{}=(s^{*})\)then
20:return\(\)
21:endif
22:if\((s^{*})=u^{}=(s^{*})\)then
23:endif
24:\(s\)
25:while True do
26:if\(s^{}\)then
27:return\(\)
28:elseif\(u=(s)(s)^{}\)then
29:return\(\)
30:elseif\(u=(s)(s)^{}\)then
31:return\(\)
32:endif
33:for\(s^{}\{(s),\,(s),(s)\}\)do
34:if\((s^{})\)then
35:\(s s^{}\)
36:endif
37:endif
38:endfor
39:endwhile ```

**Algorithm 3** Computing \((u,u^{})\) for \(u,u^{}\)

```
1:\(()\)
2:\(()\)
3:\(s r()\)
4:\(u_{t}(x_{t})\)
5:while\(s^{}\)do
6:if\(((s),u_{t})=\)then
7:\(s(s)\)
8:elseif\(((s),u_{t})=\)then
9:\(s(s)\)
10:elseif\(((s),u_{t})=\)then
11:\(s(s)\)
12:endif
13:endwhile ```

**Algorithm 4** The operation \(_{t}()\) on a contraction \(\) of \(\) at trial \(t\)

```
2:\(u^{*}(s)\)
3:\(u^{}_{}()\)
3:if\(=_{}(u^{})\)then
3:\(_{}(u^{}) u^{*}\)
3:else
4:\(_{}(u^{}) u^{*}\)
5:endif
6:if\((u^{*},)=\)then
7:\(_{}(u^{*})\)
8:\(_{}(u^{*}) u_{t}\)
9:else
10:\(_{}(u^{*})\)
11:\(_{}(u^{*}) u_{t}\)
12:endif
13:for\(i\{0,1\}\)do
14:\(_{i}(,u^{*}) 1\)
15:\(_{i}(,u_{t}) 1\)
16:endfor
17:for\(u\{u^{*},,u_{t}\}\)do
18:\((u) d(u)-d(_{}(u))\)
19:endfor
20:for\((i,i^{})\{0,1\}\{0,1\}\)do
21:if\(i=i^{}\)then
22:\(_{i,i^{}}(,u) 1-_{(u)}\)
23:else
24:\(_{i,i^{}}(,u)_{(u)}\)
25:endif
26:endfor
27:\(((),u_{t})\) ```

**Algorithm 5** The operator \(_{t}()\) on a contraction \(\) of \(\) at trial \(t\)

### Online Belief Propagation

In this subsection we utilise the work of  in order to be able to efficiently compute the function \(_{t}\) that appears in CanProp.

Given a vertex \(u\) in one of our contractions \(\) we define \((,u):=\{f\{0,1\}^{} f(u)=1\}\) and then define:

\[(,u):=_{f(,u)}\ _{u^{} \{r()\}}_{f(_{}(u^{ })),f(u^{})}(,u^{})_{f(u^{})}( ,u^{})\,.\]

As stated in the previous subsection, when a vertex \(v\) becomes involved in CanProp on trial \(t\), CBNN will add \((x_{t})\) to the leaves of \((v)\) via the operation Insert\({}_{t}((v))\). In the appendix we shall show that for each such \(v\) we then have:

\[_{t}(v)=((v),(x_{t}))/4\,.\]

We now outline how to compute this efficiently, deferring a full description for Appendix D.3. First note that for all contractions \(\) and all \(u\) we have that \((,u)\) is of the exact form to be solved by the classic _Belief propagation_ algorithm . The work of  shows how to compute this term in logarithmic time by maintaining a data-structure based on a balanced TST of \(\) - in our case the TST \(()\). Whenever, for some \(i\{0,1\}\) and \(u^{}\), the value \(_{i}(,u^{})\) changes, the data-structure is updated in logarithmic time. We define the subroutine Evidence\((,u^{})\) as that which updates this data-structure after \(_{i}(,u^{})\) changes. We also make sure that the data-structure is updated whenever \(}((),)\) is called. We then define the subroutine Marginal\((,u)\) as that which computes \((,u)/4\). Hence, the output of Marginal\(((v),(x_{t}))\) is equal to \(_{t}(v)\).

### Cbnn

Now that we have defined all our subroutines we give, in Algorithm 5, the algorithm CBNN which is an efficient implementation of CanProp with initial weighting given in Equation (2).

```
1:\(_{t}\)
2:\(u_{t}(x_{t})\)
3:\(v_{t,0} r()\)
4:for\(j=0,1,,((K)-1)\)do
5:for\(v\{ v_{(t,j)}, v_{(t,j)}\}\)do
6: Insert\({}_{t}((v))\)
7:\(_{t}(v)}((v),u_{t})\)
8:endfor
9:\(z_{t,j}_{t}((v_{t,j}))+_{t}((v_{t,j}))\)
10:for\(v\{ v_{(t,j)},(v_{t,j})\}\)do
11:\(_{t}(v)_{t}(v)/z_{t,j}\)
12:endfor
13:\(_{t,j}\)
14:if\(_{t,j}_{t}( v_{(t,j)})\)then
15:\(v_{t,j+1} v(v_{t,j})\)
16:else
17:\(v_{t,j+1}(v_{t,j})\)
18:endfor ```

**Algorithm 5** CBNN at trial \(t\)

## 5 Acknowledgments

We would like to thank Mark Herbster (University College London) for valuable discussions.

Research funded by the Defence Science and Technology Laboratory (Dstl) which is an executive agency of the UK Ministry of Defence providing world class expertise and delivering cutting-edge science and technology for the benefit of the nation and allies. The research supports the Autonomous Resilient Cyber Defence (ARCD) project within the Dstl Cyber Defence Enhancement programme.