# Harmonic: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection

Yuxin Wang

Sichuan University

Chengdu, China

wangyuxin1st@gmail.com&Duanyu Feng

Sichuan University

Chengdu, China

fengduanyuscu@stu.scu.edu.cn &Yongfu Dai

Sichuan University

Chengdu, China

wal.daishen@gmail.com &Zhengyu Chen

Wuhan University

Wuhan, China

2019302120293@whu.edu.cn &Jimin Huang

The Fin AI

Singapore

&Sophia Ananiadou

The University of Manchester

Manchester, UK

sophia.ananiadou@manchester.ac.uk &Qianqian Xie

The Fin AI

Singapore

&Hao Wang

Sichuan University

Chengdu, China

wangh@scu.edu.cn &Co-Corresponding Author.

###### Abstract

Data serves as the fundamental basis for advancing deep learning. The tabular data presented in a structured format is highly valuable for modeling and training. However, even in the era of LLM, obtaining tabular data from sensitive domains remains a challenge due to privacy or copyright concerns. Therefore, exploring the methods for effectively using models like LLMs to generate synthetic tabular data, which is privacy-preserving but similar to original one, is urgent. In this paper, we introduce a new framework HARMONIC for tabular data generation and evaluation by LLMs. In the data generation part of our framework, we employ fine-tuning to generate tabular data and enhance privacy rather than continued pre-training which is often used by previous small-scale LLM-based methods. In particular, we construct an instruction fine-tuning dataset based on the idea of the k-nearest neighbors algorithm to inspire LLMs to discover inter-row relationships. By such fine-tuning, LLMs are trained to remember the format and connections of the data rather than the data itself, which reduces the risk of privacy leakage. The experiments find that our tabular data generation achieves equivalent performance as existing methods but with better privacy by the metric of MLE, DCR, etc. In the evaluation part of our framework, we develop a specific privacy risk metric DLT for LLM synthetic data generation, which quantifies the extent to which the generator itself leaks data. We also developed LLE, a performance evaluation metric for downstream LLM tasks, which is more practical and credible than previous metrics. The experiments show that our data generation method outperform the previous methods in the metrics DLT and LLE.

Introduction

In the age of deep learning, tabular data is a predominant data format and a key element for building more effective algorithms to solve specific applications in various fields [1; 2]. However, in many sensitive domains such as business , healthcare , and governmental operations , there are significant limitations on the acquisition and use of tabular data. Tabular data in these domains involves personal privacy, business secrets, or state secrets. The collection and use of such data are strictly regulated by laws and regulations, and compliance with relevant data protection requirements is necessary. Unauthorized use or disclosure may result in serious privacy infringement or business losses. Therefore, generating data that ensures the effectiveness in modeling these data while preserving privacy in tabular data synthesis has always been a critical research area [6; 7; 8].

Traditionally, Tabular data synthesis often rely on methods like GANs [9; 10; 11], VAEs [12; 13], and Diffusion Models [14; 15; 16; 17]. However, the rise of Large Language Models (LLMs) with their impressive ability to generate data has shifted the paradigm. Methods like GReaT  and TabuLa  leverage LLMs for faster synthesis by converting tables to natural language and fine-tuning the LLMs through next-word prediction to get a generator. They often utilize smaller pre-trained models like GPT-2  for efficiency. Despite their powerful language understanding abilities, LLMs introduce significant privacy concerns [20; 21]. Continued pre-training methods may exacerbate this tendency to leak original data. Therefore, a crucial area of exploration lies in developing strategies to mitigate these privacy risks while harnessing the power of LLMs for tabular data synthesis.

To Harness LLMs for Tabular Data SyNthesis and PrIvacy ProteCtion, we develop a new framework, HARMONIC2, for the generation of tabular data by LLMs and its evaluation. For generation of the tabular data, we use existing larger-scale LLMs to leverage their in-context learning abilities for generating tabular data while ensuring privacy by fine-tuning. To be precise, we employ the idea of k-nearest neighbor algorithm (kNN)  to construct the instruction fine-tuning datasets. This allows the LLMs to see the relationship between multiple similar rows and construct the structural tabular synthetic data format. This dataset with this format then retain more structural information for LLMs to enhance the ability to generate synthetic data by fine-tuning but avoid the forced memorization of data with pre-training. For the comprehensive evaluation of the synthetic data generated by LLMs, especially its effectiveness and privacy, we introduce two novel metrics: LLE (LLM Effectiveness) and DLT (Data Leakage Test), where LLE evaluates the effectiveness of the synthetic data in downstream LLM tasks while DLT quantifies the privacy risk by comparing the perplexity of the generator on original data and synthetic data.

We assess our HARMONIC data generation framework together with existing methods of data generation by four datasets commonly used for classification tasks in tabular data synthesis, using both representative metrics and DLT and LLE. The results show that the data generated by HARMONIC performs comparably to existing methods in effectiveness but excels in privacy assessments. Crucially, HARMONIC's evaluation suggests that traditional synthetic data methods may be unsuitable for downstream LLM tasks and that pretraining-based synthetic data may pose greater privacy risks.

The main contributions of this study can be summarized as follows: 1) We recognize that it is crucial to not only focus on the strong data generation ability of LLM in this era, but also pay attention to the potential privacy risks it may bring. 2) We develop a framework, HARMONIC, for synthesizing tabular data based on LLM. The framework aims to minimize the risk of data leakage while ensuring the effectiveness of data synthesis using LLM. 3) Under the HARMONIC framework, a set of metrics is proposed for the effectiveness in downstream LLMs tasks and privacy risk evaluation of synthetic tabular data.

## 2 Related work

**Tabular Data Synthesis**. Prior to the rise of Large Language Models (LLMs), synthetic tabular data generation primarily relied on machine learning or classical neural network frameworks. These methods can be broadly categorized into three groups: Generative Adversarial Networks (GANs), Variational Autoencoder (VAE), and Diffusion Models. Building on VAE, TVAE  introduces a conditional generator with variational autoencoder (VAE) to generate tabular data. With the framework of GANs, CTAB-GAN  tackles data imbalance and long-tail issues. For Diffusion-based methods, TabDDPM  serves as a prominent benchmark, and TABSYN  offering faster synthesis compared with other such techniques. In addition to these three categories, early method like SMOTE  can also leverage linear interpolation for data generation. However, most of these methods utilize one-hot encoding for categorical data, which can exacerbate the "curse of dimensionality" for high-cardinality variables and fail to capture contextual information . Additionally, these methods overlook the semantic information present in tables.

LLMs have emerged as a compelling approach for synthetic data generation due to their exceptional capabilities in producing high-effectiveness, human-like data. LLM-based methods commonly employ a continued pre-training paradigm, and the original tabular data is converted into text format and fed into the LLM for learning. GreaT  exemplifies this approach, converting each tabular feature into the format "X is Y" and feeding the text into GPT-2  with training. REaLTabFormer  separate the table into parent table and child table as another format, and also use GPT-2 with continued pre-training and fine-tuning for synthetic data generation. Tabula  further uses the power of this pre-training and fine-tuning process, and prioritizes faster training speed by simplifying token sequences to "X Y". While LLM-based methods often outperform machine learning approaches due to their ability to leverage contextual information, limitations exist. Processing table data row-by-row hinders LLMs from fully exploiting relational information between samples. Furthermore, inherent security risks associated with data leakage plague LLMs . Pre-training method may make them vulnerable, potentially allowing an attacker with knowledge of one or two feature values in a row of original data to retrieve the entire original data record.

**Tabular Data Synthesis Protection.** To enhance the privacy protection for synthetic methods, most existing approaches incorporate differential privacy modules with the existing synthetic methods. For example, PrivBayes , CTAB-GAN+ , DP-TBART  and Mattern et al.  use the differential privacy with Bayes, CTABGAN, Bart and GPT-2 for synthetic data and its leakage protection, respectively. While these differential privacy methods offer a path towards privacy preservation on top of existing data synthesis methods, they often impose strict limitations on data features, leading to a significant decrease in downstream model performance. Therefore, we aim to enhance data leakage protection directly from the data synthesis method itself, minimizing the impact on downstream model effectiveness. Moreover, it's important to note that our method does not conflict with these differential privacy methods and can even be further integrated with them to achieve stronger privacy guarantees.

**Tabular Data Synthesis Evaluation**. Beyond evaluating the statistical distribution characteristics of synthetic data , existing evaluation methods for synthetic data, such as the MLE benchmarking system proposed by Xu et al. , primarily focus on assessing its performance as training data for machine learning models. However, as Kotelnikov et al.  argue, relying on weak classifiers for evaluation becomes outdated in light of the capabilities of advanced models like CatBoost . This underscores the need for more sophisticated evaluation techniques, especially considering the widespread adoption of LLMs in downstream applications .

Current privacy metrics for synthetic data, such as Distance to Closest Record (DCR)  and the NewRowSynthesis metric from SDMetrics , solely analyze the distance between synthetic data and original data. While these distance-based approaches provide valuable insights, they fall short when dealing with Large Language Models (LLMs). LLMs are particularly susceptible to data leakage due to their complex nature and training on massive datasets . However, existing privacy metrics based solely on tabular data feature distances fail to capture the unique learning and inference mechanisms of LLMs, which operate at the semantic and generative probability levels of embeddings. Consequently, these methods lack intuitive indicators of privacy leakage specific to LLMs .

## 3 HARMONIC Framework

This section is devoted to present the HARMONIC framework for tabular data synthesis powered by LLMs, encompassing both data generation and data evaluation.

### Synthetic Tabular Data Generation

We first present our synthetic tabular data generation approach, which utilizes fine-tuning LLMs for the generation of synthetic tabular data. It includes three key stages: (1) **Instruction dataset construction**: Construct an instruction fine-tuning dataset designed to fine-tune the generator model and a prompt dataset to facilitate data generation. (2) **Instruction tuning based tabular data synthesizer formation**: Fed the instruction fine-tuning dataset into an LLM for fine-tuning, as illustrated in Figure 1; (3) **Sampling for synthetic data generation**: Synthetic tabular data is generated by sampling from the fine-tuned LLM, with the sampling process described in Figure 2.

#### 3.1.1 Instruction Dataset Construction

**Construct the instruction fine-tuning dataset using kNN.** Our approach aims to allow LLMs to learn from a few original data instances and generate similar but distinct synthetic data. To achieve this goal, we use the kNN algorithm to identify neighboring data for each instance, enabling LLMs to learn to generate the original data from these neighbors by their in-context learning ability.

Specifically, for each sample in the training set (a row of data in the table), the kNN algorithm is used to find the \(k\) nearest neighbors (with a default value of 5) of the sample. This results in \(k\) input data points and one label (referred to as a \(k+1\) dataset).

To improve the effectiveness of the generated synthetic data, a filtering step is necessary. For each \(k+1\) dataset, if more than half of the input data have labels that are different from that of the sample, then this \(k+1\) data is discarded. Ultimately, this filtering process yields \(n\) sets of \(k+1\) data.

**Data format conversion.** Since LLMs are designed as sequence-to-sequence models, feeding tabular data into an LLM requires converting the structured data into a textual format. A straightforward approach would be to directly input a programming language readable data structure, such as Pandas DataFrame Loader for Python, line-separated JSON-file format, HTML code reflecting tables . In our work, each row of data \(s_{i}\) in a \(k+1\) data set obtained by kNN is converted into JSON dictionary format, preserving the original table structure and enabling the model to understand the semantics of each value.

Specifically, for a row of data \(s_{i}\) in each \(k+1\) data, it has feature names \(f_{1},f_{2},,f_{m}\), where the value of its \(j\)-th feature is \(v_{i,j}\). Then, the JSON-formatted data \(t_{i}\) corresponding to \(s_{i}\) is defined as

Figure 1: The fine-tuning step. After applying the kNN algorithm to the original table of data, we obtain \(n\) sets of \(k+1\) data points. Each set is structured according to the template shown in the gray table at the bottom left. These datasets are then changed to the instructions with the features of each table data shuffled, as shown in the white box above (a). Finally, the fine-tuning dataset is input into the pre-trained LLM for fine-tuning (b).

follows:

\[t_{i,j}=[f_{j}:v_{i,j}]  i\{1,,n(k+1)\},j\{1,,m\}, \] \[_{i}=\{t_{i,1},t_{i,2},,t_{i,m}\}  i\{1,,n(k+1)\}, \]

We concatenate \(k\) nearest neighbors JSON-formatted data sequentially, incorporating prompts as the input to elucidate the fine-tuning task. The left row of JSON-formatted data as the reference answer (the output).

In addition, when converting a tabular feature vector, we inadvertently introduce pseudo-positional information into the transformed tabular data. Because there is no inherent spatial ordering among features in tabular datasets . To maintain this independence of the order of the features, we randomly shuffle the order of the features within each row of JSON-formatted data \(_{i}\) in the input using a permutation. This operation results in a new sequence where the order of the features is randomized so that the model learns to be invariant to the order of the feature. A template for this instruction fine-tuning dataset is shown as Figure 1. 3

#### 3.1.2 Instruction Tuning Based Tabular Data Synthesizer Formation

We then fine-tune the LLM for the synthetic data generation task using the instruction dataset we constructed. After tokenizing our instruction dataset, the resulting token embeddings of one sample for the input and the output are denoted as \(=(x_{1},,x_{l})\) and \(=(y_{1},,y_{q})\), respectively. Here, \(l\) and \(q\) represent the lengths of the input and the output, respectively. Therefore, the objective of our fine-tuning strategy is to maximize the probability of generating the correct output sequence given the prompt describing the task and \(k\) input original data points. This objective function is formulated as:

\[p()=p(y_{1},,y_{q}|x_{1},,x_{l})=_{j=1} ^{q}p(y|x_{1},,x_{l},y_{1},,y_{j}). \]

The LLM is trained by optimizing the parameters to maximize the probability of all the \(p()\) sample, which only involves minimizing the loss of the output but avoids to learn the original data in the input. We denote the fine-tuned LLM as the generator \(\) for tabular data synthesis.

#### 3.1.3 Sampling for Synthetic Data Generation

To generate the synthetic data by LLMs, we construct a prompt dataset consistent in format with the fine-tuning dataset, where each data point consists of \(k\) original data that are randomly resampled from the original data. We emphasize that the data points should be different from those in the fine-tuning dataset which prevent the LLMs from reproducing the original data. Specifically, each data point in the prompt dataset is fed into \(\), yielding the distribution of subsequent tokens conditioned on the known input sequence. In the end, a full sequence of a synthetic tabular data will be generated. To generate the next token with more diversity and protect privacy, we adopt a weighted sampling strategy that incorporates a temperature coefficient \(T\). We set the default temperature coefficient \(T\) to 0.7. After generation, we utilize pattern-matching algorithms developed in , to reconvert the generated textual feature representations into a dataframe format, resulting in the final synthetic tabular dataset.

### Synthetic Tabular Data Evaluation

We introduce two new metrics to evaluate the effectiveness and privacy of synthetic data for LLM-based synthesis methods: LLM Effectiveness (LLE) and Data Leakage Test (DLT).

#### 3.2.1 LLE: LLM Effectiveness

With the advancement of LLMs, more and more studies believe that evaluating the effectiveness of synthetic data on the downstream tasks with weak classifiers is losing its practical value and credibility . Recent research find that the application of LLMs to tabular data processing yieldssignificant progress, with the possibility to rival or even surpass state-of-the-art machine learning approaches . Therefore, we propose the idea of using synthetic data to fine-tune a LLM to a classifier and then evaluate such classifier on the original test sets. We refer this evaluation metric as **LLM Effectiveness (LLE)**. In the current work, we choose LLaMA-2-7b-chat  as the base LLM for **LLE**.

#### 3.2.2 Dlt: Data Leakage Test

The commonly used data leakage metrics Distance to Closest Record (DCR)  and NewRowSynthesis (NRS)  focus on measuring the "distance" between synthetic data and original data. They do not take into account the extent to which the generator itself leaks the original data. Related research indicates that the LLMs are susceptible to data leakage issues to varying degrees . Attacks on LLMs of synthetic data generator have the potential to extract the complete training data, leading to severe privacy breaches. To address this issue, we propose a new metric for quantifying privacy protection named the **Data Leakage Test (DLT)**, which is inspired by the work of Skywork . This metric measures the level of which a generator leaks original data. The \(\) computes the perplexity (ppl) of the generator on both the synthetic and the original data to determine its data generation tendencies.

To compute the \(\), we first feed the training data into the generator to compute the ppl for each sample, then average these scores to determine the ppl on the training data, referred to as ppl-on-train. We then feed the synthetic data into the generator and obtain the ppl, referred to as ppl-on-syn. The DLT value is computed by subtracting ppl-on-syn from ppl-on-train. A larger DLT value indicates better privacy protection of the original data by the generator, whereas a smaller value indicates weaker privacy protection. The formula of DLT is shown as below, where the \(P(x)\) denotes the probability of generating a sentence.

\[=(_{})-(_ {}) \]

\[(_{})=_{}|}_{x_{}}P(x)^{-}=_{}|}_{x_{}}2^{ } \]

## 4 Experiment

In this section, we select four real-world datasets to compare the performance of HARMONIC with various types of data synthesis methods. The comparison is conducted in two aspects: the effectiveness of the synthesized data and its privacy protection. 4

Figure 2: The sampling step. It involves inputting a prompt, shown within the white box in the upper left corner (a), into the fine-tuned pretrained LLM. This results in a textual output (b), which is then converted into a table using pattern matching (c).

### Experimental Setup

**Datasets.** To evaluate the proposed method, we utilize four real-world datasets from various domains, namely _GM_ (German ), _AD_ (Adult Income ), _DI_ (Diabetes)5, _BU_ (Buddy)6, which are all open source datasets and don't contain any personal information such as names, phone numbers, addresses, or other sensitive data. These datasets, whose sizes range from fewer than 1,000 to tens of thousands of rows, also differ in feature types and the number of features. Some datasets include only categorical features, while others contain both numerical and categorical features. We divide each dataset into training, validation, and test sets in a ratio of 7:1:2.

**Baselines.** As discussed in related works, we select the most representative methods as our baselines, including: _SMOTE_, a simple interpolation method proposed for oversampling minority classes and can also be used for generating synthetic data ; _TVAE_, a state-of-the-art method for tabular data generation based on VAE; _CTABGAN_, a GAN-based model that performs exceptionally well across a diverse set of benchmarks ; _TabDDPM_, a famous benchmark for Diffusion-based Methods ; _TABSYN_, a faster synthesis compared with other diffusion-based techniques ; REaLTabFormer (_RTF_)  and _GReaT_, state-of-the-art tabular data synthesizers based on LLMs, to be precise, both are based on GPT-2 .

**Metrics.** We evaluate the effectiveness of the synthetic data from two aspects: the statistical distribution characteristics and the effectiveness on the downstream task. For the the statistical distribution characteristics, we employ the metrics "data_mismatch" (_DM_) to assess data type compatibility (0 indicates no datatype mismatch), "Wasserstein_dist" (_WD_) to quantify distributional differences (0 indicates identical distributions) and "CorrelationSimilarity" (_CS_) to evaluate the similarity of column-wise correlations (1 indicates that the pairwise correlations are identical). These are preliminary examinations of the effectiveness of synthetic data in previous work 7. For further evaluation with effectiveness on the downstream task, we use _MLE_ and _LLE_ just proposed, which train a machine learning model or a LLM on the original data and the synthetic data and compute the weighted F1 on the test data.

For the ability of privacy protection of the synthetic data, we use three different metrics to evaluate: Distance to Closest Record (DCR)  and NewRowSynthesis (NRS) , and our proposed _DLT_ metric. All three metrics are positively correlated with privacy, meaning that higher values indicate stronger ability of privacy protection.

**Implementation Details.** Our approach allows for the selection of any pre-trained generative LLM that supports fine-tuning, such as GPT-2 , LLaMA-2-7b-chat , Mistral , etc., as the base model. By default, our method choose LLaMA-2-7b-chat  as the base model due to its rich pre-training corpus, resulting in a stronger language understanding capability compared with GPT-2 . This enables LLaMA-2-7b-chat  to learn fine-tuning tasks more efficiently. Considering the time cost of the entire experiment, we choose LoRA  as the efficient fine-tuning method instead of full parameter adjustment. 8

### The Effectiveness of Synthetic Data

**Our method achieves effectiveness comparable to existing state-of-the-art approaches.** In Table 1, DM, WD, CS demonstrate that our method achieves state-of-the-art performance in terms of statistical distribution characteristics of generated data across all synthetic data generation methods, while remaining comparable to other LLM-based approaches in terms of its proximity and correlation between columns to the original distribution. These demonstrate our ability to effectively preserve the original data distribution. Moreover, these results show that our method can capture the relationships between columns, achieving comparable scores with other methods. Notably, on the DI dataset, our method (0.95) significantly outperforms GReaT (0.88), another LLM-based approach. These overall findings further validate the effectiveness of our method.

The performance on the downstream task also demonstrates the effectiveness of our method. Compared with other synthetic methods, our method displays promising results in many scenarios,particularly considering privacy protection in the subsequent section, resulting in a more balanced solution. While our method only achieves the best performance on the MLE metric of the BU dataset, it exhibits comparable results to current state-of-the-art generative methods in other datasets. This indicates that our data synthesis method is effective and performs on par with existing approaches. Even when prioritizing data leakage protection, our method may be a better choice. Compared with the original data, our method surpasses the original training set on the DI dataset, and on the remaining three datasets our performance only falls slightly short. The average decrease compared with the original data benchmark is less than 5%, which falls within an acceptable range for practical applications.

In conclusion, while our method may not achieve the absolute highest performance on every dataset, the results presented in this section overwhelmingly support its potential as a viable substitute for original data. The synthetic data generated by our method demonstrates both effectiveness and stability, making it a valuable tool for various LLM-based applications.

**Relying solely on MLE metrics may lead to inaccurate conclusions, and LLE is an important potential metric for synthetic data evaluation.** By examining our LLE metric, we observe that the MLE metric alone may not comprehensively reflect the effectiveness of different synthetic datasets. The evaluation results of LLE and MLE are not always consistent for the same method. For instance, TABSYN often performs better on LLE, while RTF excels on the MLE metric. This suggests that different synthetic data methods may have varying levels of effectiveness for downstream models (MLE and LLE), and a single evaluation metric may not adequately capture the true impact of a synthetic data approach.

More importantly, LLE highlights the potential of LLMs in utilizing synthetic tabular data, potentially surpassing traditional machine learning methods. This is particularly evident in the BU dataset. This finding suggests that leveraging LLMs to better accomplish tasks through synthetic data is a promising future direction. Therefore, the LLE metric holds significant potential in measuring the effectiveness of synthetic data.

### The Privacy of Synthetic Data

**The experimental results demonstrate that our method prioritizes privacy in the synthetic data generation.** This is particularly beneficial in situations where disclosing original data is not feasible due to privacy concerns. In such scenarios, our synthetic data serves as a reliable and secure substitute for original data, allowing downstream tasks to proceed without compromising sensitive information.

Table 2 presents three key privacy metric scores to quantify the privacy protection of our method. Analyzing the results in Table 2, it's evident that our method surpasses or comes in a close second for almost all datasets across all three metrics. This translates to demonstrably stronger privacy protection compared with existing methods.

   Dataset & Metric & Original & HARMONN & SMOTE & TVAE & CTAB & TABDDPM & TABSYN & RTF & GReaT \\   & DM & – & **0.00\({}_{}\)** & 0.14\({}_{}\) & 0.14\({}_{}\) & 0.14\({}_{}\) & 0.14\({}_{}\) & 0.14\({}_{}\) & 0.27\({}_{}\) & **0.00\({}_{}\)** & 0.14\({}_{}\) \\  & DM & – & 0.87\({}_{}\) & 0.85\({}_{}\) & 0.70\({}_{}\) & 0.77\({}_{}\) & 0.73\({}_{}\) & 0.94\({}_{}\) & **0.67\({}_{}\)** & 0.50\({}_{}\) \\  & OS & – & 0.96 & 0.97 & **0.99** & 0.98 & 0.90 & 0.98 & 0.98 & 0.98 \\   & MLE & 0.50\({}_{}\) & 0.50\({}_{}\) & 0.64\({}_{}\) & 0.61\({}_{}\) & 0.57\({}_{}\) & 0.64\({}_{}\) & 0.63\({}_{}\) & 0.62\({}_{}\) & 0.64\({}_{}\) & 0.64\({}_{}\) \\  & LLE & 0Moreover, besides the metrics, the design of our method inherently offers superior security in practice. If an attacker attempts to reconstruct a row of original data, he/she needs to know nearly the \(k\) rows of original data first. This includes knowing the sequence of each feature within a record and the specific order of these \(k\) samples. This significantly raises the bar for attackers compared with methods like GReaT, which exposes a vulnerability where an attacker with knowledge of just one or two feature values in an original record can potentially reconstruct the entire record.

## 5 Conclusion

In this paper, we introduce HARMONIC, a novel framework that leverages the power of LLMs for synthesizing tabular data but taking privacy concerns into account. It enables LLMs to capture both the internal feature relationships within individual row of data point and the broader connections among data point by instruction fine-tuning which is the key for the improvement in privacy protection. We also propose the metric named DLT specifically for evaluating the level of privacy protection in the synthetic data by LLM. Extensive evaluations across four real-world datasets of classification tasks showcase the ability of HARMONIC in the crucial balance of effectiveness and privacy protection.

**Limitations and Future Work**. We conclude the paper with the limitations and future work: (1) Compared with other GPT-2 based methods, our approach requires a longer processing time for larger LLMs. However, take a step forward, we believe our method will become more applicable to a wider range of contexts as hardware performance improves and cloud computing advances. (2) LLMs are less sensitive to numerical data and are better suited for classification tasks rather than regression tasks. As a result, our current work focuses primarily on tabular data used for classification tasks.9 (3) It would be interesting to carry out the the comparison and the integration of differential privacy with our method which may be a focus of our future work. (4) The ethical and potential biases of synthetic data are also critical concerns. Generating the synthetic data can inadvertently perpetuate existing biases. Addressing this challenge remains an open problem in the area of data synthesis.