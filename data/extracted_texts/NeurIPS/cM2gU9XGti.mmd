# Stopping Bayesian Optimization with

Probabilistic Regret Bounds

 James T. Wilson

Morgan Stanley, New York, USA

james.t.wilson@morganstanley.com

###### Abstract

Bayesian optimization is a popular framework for efficiently tackling black-box search problems. As a rule, these algorithms operate by iteratively choosing what to evaluate next until some predefined budget has been exhausted. We investigate replacing this de facto stopping rule with criteria based on the probability that a point satisfies a given set of conditions. We focus on the prototypical example of an \((,)\)-criterion: _stop when a solution has been found whose value is within \(>0\) of the optimum with probability at least \(1-\) under the model._ For Gaussian process priors, we show that Bayesian optimization satisfies this criterion under mild technical assumptions. Further, we give a practical algorithm for evaluating Monte Carlo stopping rules in a manner that is both sample efficient and robust to estimation error. These findings are accompanied by empirical results which demonstrate the strengths and weaknesses of the proposed approach.

## 1 Introduction

In the real world, we are often interested in finding high-quality solutions to black-box problems. Many of these problems are not only expensive to solve but difficult to reason about without extensive background knowledge--such as discovering new chemicals , designing better experiments , or configuring machine learning algorithms .

A common approach is therefore to construct models for these problems and use them to predict real-world outcomes. In recent years, Bayesian optimization (BO) has emerged as a leading approach for accomplishing these tasks. Precise definitions vary, but BO methods are frequently characterized by their use of probabilistic models to guide the search for good solutions. The idea is for these models to provide distributions over the performance of competing alternatives, which can then be used to simulate the usefulness of evaluating different things. For a recent review, see Garnett .

Despite the success of these algorithms, an ongoing issue for practitioners has been the continued lack of interpretable stopping rules. The vast majority of BO runs proceed until a predetermined budget (e.g., a number of evaluations or amount of resources) is exhausted. We highlight two likely reasons for this trend and then give a brief prospective for model-based alternatives. The first reason is that stopping rules often revolve around quantities like optimums that are difficult to work with, even when defined under a model. The second is that even the best models sometimes go astray; and, if the model is bad, then model-based stopping is liable to stop much too soon or far too late. To avoid potential disappointment, let us say upfront that this work addresses the former challenge and only provides mild commentary on the latter. We will revisit this topic in the closing sections.

At the same time, we argue there is much to be gained by using models to help us decide whether a given solution is "good-enough" for its intended purpose . One benefit of model-based stopping is its ability to adapt to the data. Sometimes, we will get lucky and stumble upon good solutions early on. Other times, our progress will be slow. If the model captures these events, then stopping canbe tailored to each run. Another benefit of model-based stopping is its ability to simplify the user experience by asking us to specify what we wish to find instead of how much we wish to spend.

The basic idea we pursue is that, if we can simulate whether a solution is good-enough, then we can stop once we find one that probably is. We focus on a prominent example of this framework, but stress that much of what follows holds for different choices of models and conditions. In particular, we investigate the setting where the user deems a solution sufficient if its performance is within \(>0\) of the optimum with probability at least \(1-\) under the model.

Our primary contributions are to: i) combine recent work on scalable sampling techniques with algorithms for cost-efficient statistical testing; ii) show how the resulting estimators can be used as the basis for robust stopping rules; and, iii) introduce the first model-based stopping rule for BO with convergence and performance guarantees (up to model error).

The remaining text is organized as follows. Section 2 presents notation background material. Section 3 introduces the proposed stopping rule and evaluation strategy. Section 4 analyzes this algorithm's convergence and correctness. Finally, Section 5 investigates its empirical performance under idealized and realistic circumstances.

## 2 Background

We use boldface symbols to indicate vectors (lowercase) and matrices (uppercase). Given a sequence \((_{i})\), we denote \(_{n}=[a_{1},,a_{n}]^{}\). Likewise, for a function \(f:\), we use the shorthand \(f(_{n})=[f(_{1}), f(_{n})]^{}\). By minor abuse of notation, we sometimes treat, e.g., \(_{n}\) as a set.

We focus on the task of sequentially querying a function \(f:\) in order to find a point \(\) whose value \(f()\) is within \(>0\) of the supremum. Such a point is said to be _\(\)-optimal_ if this condition holds and _\((,)\)-optimal_ if it holds with probability at least \(1-\). Throughout, we write \((_{t})\) for the sequence of query locations.

At any given time \(t_{0}\), our understanding of the target function's behavior is driven by domain knowledge and any data that we have already collected. We combine this information with the help of a Bayesian model by placing a prior on \(f\) and defining an observation model. Different types of models are eligible and techniques introduced in the sequel simply require that we are able to simulate the chosen stopping conditions (e.g., \(\)-optimality). We focus on the most popular family of models in this setting: Gaussian processes.

A Gaussian process (GP) is a random function \(f:\) such that, for any finite set \(\), the random variable \(f()^{||}\) is Gaussian in distribution. We write \(f(0,k)\) for a centered GP with covariance \(k:\) and model observations as function values corrupted by independent Gaussian noise, i.e. \(y(_{t}) f(_{t})f(_{t} ),^{2}\). Conditional on \(y(_{t})\), we therefore believe that \(f\) is distributed as \(f_{t}(_{t},k_{t})\), where \(=k(_{t},_{t})+^{2}\) is used to define

\[_{t}()=k(,_{t})^{-1}y(_{t} ) 56.905512ptk_{t}(,^{})=k(,^{})-k( ,_{t})^{-1}k(_{t},^{}).\] (1)

Finally, we assume that \(\) is compact and that \(_{t}\) and \(k_{t}\) are both continuous so their limits are attained on \(\). Among other things, this assumption allows us to write \(_{t}_{_{t}}_{t}()\) for a preferred solution at time \(t\), where \(_{t}\) is either the set of evaluated points \(_{t}\) or the search space \(\).

## 3 Method

Suppose Bayesian optimization terminates at time \(t_{0}\) and returns a point \(\) as the solution. Our _regret_ for having returned this point is defined as the distance between \(f()\) and the optimum. Under the model \(f_{t}\), this (simple) regret manifests as a random variable

\[r_{t}() =f_{t}^{*}-f_{t}() f_{t}^{*}=_{}f_{t}().\] (2)

Given a regret bound \(>0\) and a risk tolerance \(>0\), we would like to stop searching once we have found a point so that \(r_{t}()\) with probability at least \(1-\) and refer to this stopping rule as a _probabilistic regret bound_ (PRB). Probabilities of this sort are usually intractable and we will therefore estimate them via sampling. To this end, we denote the probability that a point \(\) is \(\)-optimal and an associated Monte Carlo estimator by

\[_{t}() =(r_{t}()) _{t}^{n}(;) =_{i=1}^{n}r_{t}^{i}() ,\] (3)

where \(r_{t}^{i}()\) is the \(i\)-th independent draw of the model-based regret (2). We will shortly explore how to construct estimators \(_{t}^{n}()\) and use them to decide whether \(_{t}()\) is above or below a level \(\) in a manner that is both cost efficient and robust to estimation errors. First, however, let us introduce some basic terminology that will help us reason about potential failure modes.

We will say the estimator produces a _false positive_ if \(_{t}^{n}()>_{t}()\) and a _true positive_ if \(_{t}^{n}()_{t}()\). Since either scenario may lead to an unsatisfactory solution, the level \(\) that we compare against must exceed \(1-\). Accordingly, let \(_{}\) and \(_{}\) be nonzero probabilities such that \(_{}+_{}\). By defining \(=1-_{}\), we will use \(_{}\) to limit the chance that a point \(\) is not \(\)-optimal even though \(_{t}^{n}()\) produced a true positive. Conversely, we will use \(_{}\) to control the probability of encountering a false positive (see Section 3.2). This pattern guarantees that if \(_{t}^{n}()\), then \(\) is \(\)-optimal with probability at least \(1-\) under the model.

Algorithm 1 sketches a typical BO loop with the proposed stopping rule. At each iteration, we obtain a model for the data. We then select candidate solutions \(\) and estimate their probabilities of being \(\)-optimal under the model. If an estimate is greater than \(1-_{}\), then the corresponding point satisfies the stopping conditions with probability at least \(1-\) and we terminate; otherwise, we press on.

The rest of this section examines two key questions: how to simulate model-based regrets \(r_{t}()\) when \(||\) is large (or infinite) and how to avoid false positives due to estimation error. Appendix A explores related topics such as how to choose \(\) and schedule \(_{}^{t}\).

Figure 1 shows how the proposed algorithm behaves for different choices of \(\) and \(\). Data was generated by running BO a hundred times and sampling \(r_{t}(_{t})\) a thousand times per step using the strategy from Section 3.1. Stopping decisions were then made by comparing estimators \(_{t}^{n}(_{t})\) with \(=1-_{}\), where \(_{}=}{{2}}\). These results do not take advantage of the testing paradigm introduced in Section 3.2, but accurately reflects the algorithm's behavior. In particular, we see that the number of function evaluations performed by each run automatically adapts to the definition of \((,)\)-optimality.

Figure 1: Overview of PRB stopping behavior when \(f:^{2}\) is drawn from a model with noise variance \(^{2}=10^{-4}\). Regret bounds \(>0\) dictate how close \(f()\) must be to the optimum \(f^{*}\) for \(\) to be satisfactory. Tolerances \(>0\) upper bound the chance of returning an unsatisfactory point. _Left:_ Percent of runs that stopped before time \(T=128\). _Middle:_ Percent of stopped runs that returned \(\)-optimal points. _Right:_ Median number of trials performed by stopped runs.

### How to simulate stopping conditions

This section describes how to simulate whether a point \(\) satisfies the chosen stopping conditions. For PRB, this amounts to sampling Bernoulli random variables \((r_{t}())\). We propose to generate this term by maximizing draws of \(f_{t}\). When dealing with parametric models, function draws are obtained by sampling parameter vectors. For GPs, analogous logic may be enacted by using a parametric approximation to the prior , as outlined below. This approximate sampling step is necessary because the time complexity for exactly simulating \(f_{t}()\) scales cubically in \(||\).

Let \(:^{m}\) be a finite-dimensional feature map so that, \(,^{}\), \(()^{}(^{}) k(,^{ })\). Note that feature maps of this sort are readily available for many popular covariance functions . Equipped with such a map, we may approximate a prior \(f(0,k)\) with a Bayesian linear model

\[() =()^{} (,).\] (4)

Letting \(=k(_{t},_{t})+^{2}\) and \((,^{2})\), this linear model may be used to generate draws from an approximate posterior by sampling \(\) from the prior and using Matheron's rule to write 

\[f_{t}()}{{}}()+k(, _{t})^{-1}_{t}-(_{t})- {}.\] (5)

For each draw of \(f_{t}\), the remaining problem is now to evaluate \((r_{t}())\). We suggest using multi-start gradient ascent. In our case, we performed an initial random search to identify promising starting locations and then used a quasi-Newton method  to optimize. A helpful insight is that we do not need to find \(f_{t}^{*}\) per se. Rather, it suffices to determine whether there exists a point \(^{}\) such that \(f_{t}(^{})-f_{t}()>\). This property can be exploited to accelerate simulating \((r_{t}())\); however, its benefits wane as \(_{t}()\) increases because \(r_{t}()\) implies that no such point \(^{}\) exists.

The right panel of Figure 2 compares different estimators for \(_{t}\). For simplicity, assume that \(f_{t}\) is sample continuous so that it almost surely attains its supremum on \(\). The goal of this plot is to highlight challenges inherent to conditioning on the maximum. We not only need to upper bound \(f_{t}\), but also account for the point(s) at which the maximum is achieved. This explains why the red estimator \(_{_{t}^{*}:f_{t}^{*}}[(f_{t}^{*}-f_{t}()  f_{t}(_{t}^{*})=f_{t}^{*},f_{t}() f_{t}^{*})]\) outperforms the orange one \(_{f_{t}^{*}}[(f_{t}^{*}-f_{t}())]\), while the green one \(_{f_{t}^{*}}[(f_{t}^{*}-f_{t}() f_{ t}() f_{t}^{*})]\) fails to do so. We opted to avoid these issues by sampling \(f_{t}()\) jointly with \(f_{t}^{*}\) rather than marginalizing it out. The resulting blue estimator is seen to more accurately follow the gold standard shown in black.

Lastly, it should be said that the suggested sampling procedure introduces a yet-to-be-determined amount of error in practice, since draws of \(f_{t}\) are not only approximate but non-convex. Initial results suggest these errors are small (see Figure 2), however we leave this as a topic for future investigation.

Figure 2: _Left:_ Posterior mean and two standard deviations of \(f\) (blue) given eight noisy observations (black dots). The goal is to find a point \(\) whose true function (black) value is within \(>0\) of the optimum \(f^{*}\) (orange star). _Middle:_ Draws of \(f_{t}(_{t},k_{t})\) and \(f_{t}^{*}\) (orange stars). _Right:_ Estimators for \(_{t}\). Ground truth (dashed black) was established using location-scale sampling on a dense grid. The joint-sampling strategy from Section 3.1 is shown in blue. Competing methods analytically integrated out \(f_{t}() f_{t}^{*}\) by approximating it with: \(f_{t}()\), \(f_{t}() f_{t}() f_{t}^{*}\), or \(f_{t}() f_{t}() f_{t}^{*} f_{t}(_{t}^{*})=f_{t}^ {*}\) where \(f_{t}^{*}\) and \(_{t}^{*}_{}f_{t}()\) were jointly sampled.

### How to efficiently make robust decisions with Monte Carlo estimates

This section discusses the general problem of using samples to decide whether the expectation of a random variable \(Z\{0,1\}\) exceeds a level \(\). For PRB, \(=1-_{}\) and each evaluation of the stopping rule corresponds to a unique \(Z=(r_{t}())\). We will show how to make probably-correct decisions using a minimal number of samples \(n\). In doing so, we first discuss confidence intervals for \([Z]\) based on a collection of i.i.d. draws \(_{n}=\{z_{i}:i=1,n\}\).

There are many techniques for generating intervals that contain \([Z]\) with (coverage) probability at least \(1-_{}\). Clopper & Pearson  gave an exact recipe for constructing confidence intervals for Bernoulli random variables \(Z\) as

\[(_{n};)=B;k,n-k+1 ,B1-;k+1,n-k,\] (6)

where \(B\) denotes the beta quantile function and \(k=_{i=1}^{n}z_{i}\) is the number of successes in \(n\) draws. It is also possible to take a Bayesian by placing a prior on \([Z]\). Differences between this Bayesian approach and (6) were observed to be minimal however, so we opted to avoid modeling \([Z]\). For further discussion, see Appendix A.1.

Given an estimate \(_{n}=_{i=1}^{n}z_{i}\) and a confidence interval \(_{n}=(_{n};_{})\), it follows that

\[[Z]_{n}\;\;\;\;_{n} \,[Z]}_{ }=_{n} }_{}.\] (7)

If \(_{n}\) collapses to a point as \(n\), then there exist sample sizes such that \(_{n}\), whereupon the conclusion from (7) holds with probability at least \(1-_{}\).1 Said differently, we can lower bound the probability that we correctly decide whether \([Z]\) by generating enough draws of \(Z\). With these details in mind, we now review an algorithm for adaptively choosing \(n\) in order to make probably-correct decisions using as few samples as possible--which is crucial when simulating \(Z\) is computationally intensive as in Section 3.1.

The general idea of Algorithm 2 is to perform a series of tests (each using more samples than the last), until a confidence interval for \([Z]\) is narrow enough for a decision to be made. To better understand this, start by defining two sequences: sample sizes \((n_{j})\) and risk tolerances \((d_{j})\). The sizes should be increasing, while the tolerances should be positive and satisfy \(_{j=1}^{}d_{j}_{}\).

Next, imagine that we generate draws of \(Z\) in batches of size \(n_{j}-n_{j-1}\), where \(n_{0}=0\). At each round of sampling \(j\), we construct an interval \(_{n_{j}}\) that contains \([Z]\) with probability at least \(1-d_{j}\). If \(_{n_{j}}\), we use \(_{n_{j}}\) to decide whether \([Z]\). Otherwise, we proceed to the next iteration.

Figure 3: _Left:_ Median number of draws used by Algorithm 2 to decide if the expectation of a Bernoulli random variable \(Z(p)\) exceeds \(=10^{-5}\) (chosen arbitrarily). _Middle:_ Empirical CDFs of \(_{t}^{n}\) when optimizing draws from known priors \((0,k)\) in two, four, and six dimensions with noise variance \(^{2}=10^{-6}\) (solid, \(\)) or \(^{2}=10^{-2}\) (dashed, \(\)). PRB parameters were set to \(=0.1\) and \(_{}=_{}=2.5\%\). _Right:_ Runtimes for Algorithm 2 using the generative strategy from Section 3.1 and a (wall) time limit of roughly one thousand seconds.

Per (7), this algorithm only makes an incorrect decision if the final interval fails to contain \([Z]\). By definition of \((d_{j})\) and the union bound however, the chance of any interval not containing \([Z]\) is at most \(_{}\). Hence, the algorithm makes the correct decision with probability at least \(1-_{}\).

Algorithm 2 was inspired by bandit methods, such as Mnih et al.  and references contained therein, who previously studied how concentration inequalities can be used to iteratively test whether \(_{n}-[Z] [Z] 1-_{}\). Algorithm 2 is closer to Braden et al.  however, who used a similar strategy to decide whether to accept Metropolis-Hastings proposals based on subsampled estimates of the data log-likelihood.

Extending our earlier argument, let \((_{}^{t})\) be a sequence of risk tolerances such that \(_{t=1}^{}_{}^{t}_{}\). If Algorithm 2 is run at each BO step \(t\) with schedule \((d_{j}^{t})\) such that \(_{j=1}^{}d_{j}^{t}_{}^{t}\), then the chance of encountering a false positive at any step is bounded from above by \(_{}\). Consequently, the decision to stop will be correct with probability at least \(1-_{}\).

We followed Mnih et al.  by defining \(d_{j}^{t}=j^{-}_{}^{t}\) and \(n_{j}=^{j-1}N\). We set \(=1.1\) so that \((d_{j}^{t})\) decayed slowly, \(=1.5\) such that \((n_{j})\) grew reasonably quickly, and \(N=64\) because smaller starting values took longer to run. These choices impact the algorithm's runtime, not its validity. Using a geometric schedule for \((n_{j})\) prevents \((d_{j}^{t})\) from rapidly shrinking due to a large number of tests being performed with very few samples. In exchange, this schedule can lead to nearly \(\) times too many samples being requested.

```
1:input point \(\), model \(f\) and parameters \(,_{},_{}>0\)
2:\((d_{j})_{d}(_{})\)
3:\((n_{j})_{n}()\)
4:\(\)
5:for\(j=1,2,\)do
6:while\(||<n_{j}\)do
7:\(f^{i}_{*}(f)\)
8:\(f^{i}_{*}_{}f^{i}()\)
9:\(1(f^{i}_{*}-f^{i}() )}\)
10:if\(1-_{}(,d_{j})\)then
11:break
12:return\(()\) ```

**Algorithm 2** Monte Carlo PRB

The left panel of Figure 3 shows how many samples Algorithm 2 used to decide whether \([Z]\) for \(Z(p)\). As \(p 1\), the distance between \(_{n}\) and \(\) tends to increase and decisions can be made with wider confidence intervals constructed using fewer draws. As \(_{} 1\), these intervals shrink and decisions can similarly be made using fewer samples. The middle panel visualizes the empirical CDF of estimates \(_{t}^{n}\) from BO experiments described in Section 5. For most of a typical BO run's life cycle, these estimates are far from \(=1-_{}\) so decisions can be made efficiently. This pattern is reflected in the rightmost panel, which illustrates the savings provided by Algorithm 2.

## 4 Analysis

We show that Bayesian optimization with the PRB stopping rule terminates under mild assumptions. Further, we prove that the given algorithm is correct in the sense that it returns an \((,)\)-optimal point under the model. We begin by discussing the assumptions made throughout this section, which are:

1. The search space \(=^{D}\) is a unit hypercube.
2. There exists a constant \(L_{k}>0\) so that, \(,^{}\), \(|k(,)-k(,^{})| L_{k}\|-^{} \|_{}\).
3. The sequence of query locations \((_{t})\) is almost surely dense in \(\).

A1 and A2 guarantee it is possible for the maximum posterior variance to become arbitrarily small given a finite number of observations. Note that if hyperparameters change over time, we only require that the (best) Lipschitz constant \(L_{k}\) and noise variance \(^{2}\) do not grow without bounds as \(t\). Combined with these assumptions, A3 implies that, for any \(C>0\), there exists a time \(T_{0}\) such that, \( t T\), \(_{}k_{t}(,) C\) with probability one. More generally, A3 is necessary to ensure convergence when all we known is that \(\) is compact and \(f\) is continuous .

When A1 and A2 hold, popular strategies often produce almost surely dense sequences \((_{t})\). For instance, Vazquez & Bect  proved Probability of Improvement  and Expected Improvement  exhibit this behavior for many covariance functions \(k\) when \(f\) is directly observed. In Appendix B, we show that this result holds for continuous acquisition functions that value informative queries over unambiguous ones. This family includes well-known acquisition functions such as Knowledge Gradient , Entropy Search , and variants thereof [21; 49]. Finally, dense sequences can be guaranteed by introducing a small chance for queries to be selected at random from an appropriately chosen distribution .

We first prove that points which maximize the posterior mean eventually satisfy the PRB criterion and then use this result to demonstrate convergence and correctness.

**Proposition 1**.: _Under assumptions A1-A3 and for all regret bounds \(>0\) and risk tolerances \(>0\), there almost surely exists \(T_{0}\) so that, at each time \(t T\), every \(_{t}_{}_{t}()\) satisfies_

\[_{t}(;)=(r_{t}(_{t})) 1-.\] (8)

_Sketch._ We sketch the proof below and provide full details in Appendix B for details. Consider the centered process \(g_{t}()=[f_{t}()-f_{t}(_{t})]+[_{t}(_{t})-_{t}( )]\). Since the second term is nonnegative, \(g_{t}^{*}=_{}g_{t}() r_{t}(_{t})=f_{t} ^{*}-f(_{t})\) and it suffices to upper bound the probability that \(g_{t}^{*}\). For \(>(g_{t}^{*})\), such a bound may be constructed by using the Borell-TIS inequality [8; 46] to write

\[(g_{t}^{*})\!(-\![(g_{t}^{*})}{_{t}}]^{2}) _{t}^{2}=_{}[g_{t}()].\] (9)

Since \((g_{t}^{*})\) and (9) both vanish as \(_{t}\) decreases, the claim holds so long as \(_{t}_{t}=0\). 

Similar ideas can be found in Grunewalder et al. , who proved that the expected supremums of centered process like \(g_{t}\) go to zero as \((_{t})\) becomes increasingly dense in \(\). In Appendix B, we extend this result to the setting where observations are corrupted by i.i.d. Gaussian noise and combine it with the Borell-TIS inequality to show the probability that \(r_{t}(_{t})\) vanishes. We also give a simple corollary for the case where solutions \(_{t}\) belong to \(_{t}\). Next, we show that BO not only stops when Algorithm 2 is used to evaluate the proposed rule but does so correctly.

**Proposition 2**.: _Suppose assumptions A1-A3 hold. Given a risk tolerance \(>0\), define nonzero probabilities \(_{}\) and \(_{}\) such that \(_{}+_{}\) and let \((_{}^{t})\) be a positive sequence so that \(_{t=0}^{}_{}^{t}_{}\). For any regret bound \(>0\), if Algorithm 2 is run at each step \(t_{0}\) with tolerance \(_{}^{t}\) to decide whether a point \(_{t}_{}_{t}()\) satisfies the stopping criterion_

\[_{t}(;)=(r_{t}(_{t})) 1- _{},\] (10)

_then BO almost surely terminates and returns an \((,)\)-optimal solution under the model._

Proof.: By Proposition 1, there almost surely exists an \(S_{0}\) so that \(t S_{t}(_{t}) 1-_{}\). Further, because \(_{t}^{n}(_{t})\) is unbiased, there exist times \(t T\) at which Algorithm 2 produces true positives \(_{t}^{n}(_{t}) 1-_{}_{t}(_{t})  1-_{}\). Hence, BO stops with probability one. If BO terminates at time \(T_{0}\), then the probability that \(_{T}\) is not \(\)-optimal is less than or equal to \(_{}\) in the event of a true positive and one otherwise. Since false positives \(_{t}^{n}(_{t}) 1-_{}>_{t}(_{t})\) occur with probability at most \(_{}\), it follows that \(_{T}\) is \(\)-optimal with probability at least \(1-\). 

In summary, we can design statistical tests to mitigate the risk of premature stopping due to random fluctuations in Monte Carlo estimators like \(_{t}^{n}\). Moreover, we can schedule these tests to ensure that points which pass them are sufficiently likely (under the model) to satisfy our stopping conditions. If the model is correct, we can therefore guarantee that a satisfactory solution is returned with high probability. Provided that one or more points almost surely satisfy the rule as \(t\), this result holds if we can simulate whether solutions are satisfactory and bound the error in the resulting estimator.

## 5 Experiments

To shed light on how our algorithm behaves in practice, we conducted a series of experiments. Focal questions here included: i) how does PRB perform in comparison to existing stopping rules, ii) how do these rules respond to different types of problems, and iii) what is the impact of model mismatch.

Experiments were performed by first running BO with conservatively chosen budgets \(T\). We then stepped through each saved run with different stopping rules to establish stopping times and terminal performance. This paradigm ensured fair comparisons and reduced compute overheads. We performed a hundred independent BO runs for all problems other than hyperparameter tuning for convolutional neural networks (CNNs) on MNIST , where only fifty runs were carried out.

Despite the general notation of the paper, all problems were defined as minimization tasks. Additional details and results can be found in Appendices C and D, respectively; and, code is available online at https://github.com/j-wilson/trieste_stopping.

Each BO run was tasked with finding an \(\)-optimal point with probability at least \(1-=95\%\). On the Rosenbrock-4 fine-tuning problem, we used a regret bound \(=10^{-4}\). For CNNs, we aimed to be within \(=0.5\%\) of the best test error (i.e., misclassification rate) seen across all runs, namely \(0.62\%\). Likewise, when fitting XGBoost classifiers  for income prediction , we sought to be within \(1\%\) of the best found test error of \(12.89\%\). For all other problems, we set \(=0.1\).

For PRB, we divided \(\) evenly between \(_{}\) and \(_{}\). Since experiments were carried out using preexisting BO runs that each began with five random trials and ended at times \(T\), we employed a constant schedule \(_{}^{t}=_{}\) for risk tolerances at steps \(t_{0}\). Parameter schedules for Algorithm 2 are discussed in Section 3.2.

As a practical concession, we limited each run of Algorithm 2 to a thousand draws of \(f_{t}\) and used the resulting estimate to decide whether to stop--even if the corresponding confidence interval was not narrow enough to afford guarantees. Results under this setup were consistent with preliminary experiments in which Algorithm 2 was run using a fifteen minute time limit. Finally, when optimizing draws from GP priors in six dimensions with noise \(^{2}=10^{-2}\), we evaluated PRB once every five steps to expedite these experiments.

### Baselines

We tested several baselines, some of which were granted access to information that would usually be unavailable (indicated by a dagger \(\)). We summarize these as follows2:

* Oracle\({}^{}\): stops once an \(\)-optimal point has been evaluated.
* Budget\({}^{}\): stops after a fixed number of trials chosen by an oracle for each problem.
* Acq [23; 35]: stops when the acquisition value of the next query is negligible.

   Problem & \(D\) & \(T\) & **Oracle\({}^{}\)** & **Budget\({}^{}\)** & **Acq** & \(}\) & \(}\) & **PRB** (ours) \\  \(^{}\)\(10^{-6}\) & 2 & 64 & \(10\,(100)\) & \(17\,(96)\) & \(28\,(100)\) & \(16\,(96)\) & \(22\,(99)\) & \(17\,(97)\) \\ \(^{}\)\(10^{-2}\) & 2 & 128 & \(11\,(100)\) & \(22\,(96)\) & \(78\,(100)\) & \(128\,(100)\) & \(54\,(100)\) & \(\) \\ \(^{}\)\(10^{-6}\) & 4 & 128 & \(27\,(100)\) & \(64\,(95)\) & \(90\,(100)\) & \(\) & \(93\,(100)\) & \(64\,(99)\) \\ \(^{}\)\(10^{-2}\) & 4 & 256 & \(30\,(100)\) & \(94\,(95)\) & \(106\,(98)\) & \(256\,(100)\) & \(144\,(97)\) & \(\) \\ \(^{}\)\(10^{-6}\) & 6 & 256 & \(40\,(99)\) & \(124\,(95)\) & \(142\,(98)\) & \(150\,(98)\) & \(256\,(99)\) & \(\) \\ \(^{}\)\(10^{-2}\) & 6 & 512 & \(65\,(100)\) & \(227\,(96)\) & \(\) & \(512\,(100)\) & \(278\,(99)\) & \(235\,(100)\) \\ \(\)\(10^{-6}\) & 4 & 128 & \(35\,(100)\) & \(79\,(95)\) & \(\) & \(41\,(66)\) & \(77\,(94)\) & \(61\,(88)\) \\ \(\)\(10^{-2}\) & 4 & 256 & \(51\,(100)\) & \(157\,(95)\) & \(\) & \(256\,(100)\) & \(160\,(96)\) & \(100\,(92)\) \\ \(\) & 2 & 128 & \(19\,(100)\) & \(25\,(95)\) & \(64\,(100)\) & \(36\,(100)\) & \(38\,(100)\) & \(\) \\ \(\) & 3 & 64 & \(14\,(100)\) & \(22\,(96)\) & \(26\,(100)\) & \(18\,(90)\) & \(21\,(97)\) & \(\) \\ \(\) & 6 & 64 & \(36\,(67)\) & \(256\,(67)\) & \(40\,(67)\) & \(38\,(67)\) & \(62\,(67)\) & \(40\,(64)\) \\ \(\) & 4 & 96 & \(34\,(100)\) & \(46\,(95)\) & \(95\,(100)\) & \(88\,(100)\) & \(98\,(100)\) & \(\) \\ \(\) & 4 & 256 & \(5\,(100)\) & \(11\,(96)\) & \(64\,(100)\) & \(64\,(100)\) & \(64\,(100)\) & \(\) \\ \(\) & 3 & 128 & \(4\,(100)\) & \(8\,(97)\) & \(128\,(100)\) & \(90\,(100)\) & \(51\,(100)\) & \(\) \\   

Table 1: Median stopping times and success rates when seeking \((,)\)-optimal points on \(=^{D}\) given an upper limit of \(T\) function evaluations. For GP objectives, number beside each name specify noise levels \(^{2}\). Superscripts \({}^{}\) indicate that model or stopping rule parameters were given by an oracle. For each problem, non-oracle methods that returned \(\)-optimal points at least \(1-\) percent of the time using the fewest function evaluations are shown in **blue**.

**B4**.: \(\)CB : stops once the gap between confidence bounds is less-equal to \(C>0\), i.e.

\[_{}_{t}()-_{^{} _{t}}_{t}(^{}) C\ \ \ \ \ [/]_{t}()=_{t}()k_{t}(,)}\]
**B5**.: \(\)ES : stops when an upper bound on \(|(f_{t}^{*})-(f_{t-1}^{*})|\) drops below a level.

B1 is the optimal stopping rule, but requires perfect information for \(f\). Likewise, B2 is the optimal fixed budget for each problem. These budgets were defined post-hoc as the minimum number of trials such that at least \(95\%\) percent of runs returned \(\)-optimal points (where possible).

The remaining methods are all model-based and stop when target quantities are sufficiently small. For the chosen acquisition function (see Appendix C.3), B3 can be interpreted as the expected improvement in solution quality given an additional trial, i.e. \(_{y_{t+1}}[_{_{t+1}}_{t+1}()-_{ t+1}(_{t})]\). Unfortunately, neither this quantity nor the change in the expected supremum used by B5 lend themselves to interpretation in terms of \((,)\)-optimality. B4 does admit such an interpretation for appropriate choice of constant \(_{t}\); however, these constants are often difficult to obtain in practice however, so we followed  by defining \(_{t}=Dt^{2}^{2}/6\).

To combat these issues, we gave baseline methods a competitive advantage by retroactively assigning cutoff values to ensure they achieved the desired success rate when optimizing draws from the model (denoted GP\({}^{}\)). Specifically, cutoff values for B3-B5 were obtained by dividing regret bounds \(\) by the smallest powers of two for which this condition held--explicitly: \(2^{15}\), \(2^{3}\), and \(2^{4}\) (respectively). Note that, in the absence of this fine tuning, these methods either proved unreliable or failed to stop within the allotted time depending on whether thresholds were too large or too small. For completion, additional results using \(\) as the cutoff value for B4 and B5 are presented in Appendix D.

The main results of this section are shown in Table 1 and key findings are discussed below.

### Results with true models

When optimizing functions drawn from known GP priors, denoted GP\({}^{}\), the proposed stopping rule performed exactly as advertised and consistently returned \((,)\)-optimal solutions. Moreover, PRB often requiring the fewest function evaluations. This result is not surprising when comparing with methods like B4 because an unbiased estimate to \((r_{t}())\) should exceed a level faster than a corresponding lower bound. In many cases, PRB achieved a higher success rate than the fixed budget oracle using a comparable or smaller number of trials. These gains occur because model-based stopping is able to exploit patterns in the data collected by individual runs.

Elsewhere, we observe that B4 struggled to terminate when faced with moderate noise levels \(^{2}=10^{-2}\). This pathology likely emerges because, similar to alternative estimators discussed in Section 3.1, the method does not fully account for dependencies between \(f_{t}^{*}\) and \(f_{t}()\). As an extreme example, B4 may fail to terminate when a point \(_{t}\) simultaneously maximizes upper and lower confidence bounds, despite the fact that \(r_{t}(_{t}^{*}=)=0\).

Not surprisingly, BO runs that took longer to query an \(\)-optimal point took longer to stop. However, the correlation between these terms paled in comparison to that of stopping times and \(\)-quantiles of regrets incurred by uniform random points (approximately, \(0.35\) vs. \(-0.75\)). Said differently, PRB stopped faster when \(f^{*}\) was an outlier. This pattern suggests that the one-step optimal strategy from Appendix C.3 is better at finding optimal solutions than verifying them. Future works may therefore wish to pursue stopping-aware approaches along the lines of McLeod et al.  or Cai et al. .

### Results with maximum a posteriori models

In the real world, the high-level assumptions that govern how the model behaves (i.e., its hyperparameters) are tuned online as additional data is collected using Type-II maximum likelihood. We are therefore interested in seeing how discrepancies between the model and reality influence stopping behavior.

Results here were similar to the synthetic setting, albeit with some blemishes. Interestingly, the most glaring example of the risks posed by model mismatch occurred on the popular Hartmann-6 test function. Here, \(33\%\) of BO runs overestimated the objective function's smoothness and converged to a local minimum of \(-3.20\) rather than the global minimum of \(-3.32\). It is worth noting, however,that if \(=0.1\) had been slightly larger, all stopping rules would have succeeded in at least \(95\%\) of cases (see Appendix D). Along similar lines, models occasionally underestimated the kernel variance when optimizing draws from GP priors and stopped prematurely.

These results also indicate that both hyperparameter tuning problems (CNN and XGBoost) were fairly easy and this may have masked potential failure modes. The fixed budget oracle's performance demonstrates that there was still room for model-based stopping rules to fail, but we nevertheless recommend that these results be taken with a grain of salt.

In additional experiments, we indeed found that it was easy to construct cases where poor model fits led to poor stopping behavior. This vulnerability was large due to our choice of hyperpriors (see Appendix C.1), which were purposefully broad and uninformative. Overall, we argue that these results are both highly encouraging and also highlight the importance of uncertainty calibration. Potential remedies for this issue are discussed below.

Based on these findings, we suggest that model-based stopping be used with more conservative priors that, e.g., favor smaller lengthscales and larger variances. Alternatively, calibration issues may be alleviated by marginalizing over hyperparameters  or utilizing more expressive models. These options help reduce the risk of overly confident models leading to premature stopping. Along the same lines, we recommend using a large fixed budget as an auxiliary stopping rule to avoid cases where poor model fits cause the algorithm to converge very slowly (see Appendix A.3 for discussion).

## 6 Conclusion

To the best of our knowledge, results presented here are among the first of their kind for Bayesian optimization. We have given a practical algorithm for verifying whether a set of stopping conditions holds with high probability under the model. For the proposed stopping rule, we have further shown that the algorithm correctly terminates under mild technical conditions. If data is generated according to the model, we can therefore guarantee that BO is likely to return a satisfactory solution.

The methods we have shared are largely generic. Echoing the introduction, if you can simulate it then you can use it for stopping. While this approach is not without limitations, we believe that it will ultimately allow others to design stopping rules as they see fit. To the extent that it does, model-based stopping may one day become as common place as model-based optimization.