# Learning Spatially-Aware

Language and Audio Embeddings

Bhavika Devanani\({}^{1,}\)1

Skyler Seto\({}^{2}\)   Zakaria Aldeneh\({}^{2}\)   Alessandro Toso\({}^{2}\)

**Elena Menyaylenko\({}^{2}\)   Barry-John Theobald\({}^{2}\)   Jonathan Sheaffer\({}^{2}\)   Miguel Sarabia\({}^{2}\)**

\({}^{1}\) Georgia Institute of Technology  \({}^{2}\) Apple

bdevanani3@gatech.edu, {sseto, zaldeneh, atoso}@apple.com

{elenam, bjtheobald, sheaffer, miguelsdc}@apple.com

###### Abstract

Humans can picture a sound scene given an imprecise natural language description. For example, it is easy to imagine an acoustic environment given a phrase like "the lion roar came from right behind me!". For a machine to have the same degree of comprehension, the machine must know what a lion is (semantic attribute), what the concept of "behind" is (spatial attribute) and how these pieces of linguistic information align with the semantic and spatial attributes of the sound (what a roar sounds like when its coming from behind). State-of-the-art audio foundation models, such as CLAP , which learn to map between audio scenes and natural textual descriptions, are trained on non-spatial audio and text pairs, and hence lack spatial awareness. In contrast, sound event localization and detection models are limited to recognizing sounds from a fixed number of classes, and they localize the source to absolute position (e.g., 0.2m) rather than a position described using natural language (e.g., "next to me"). To address these gaps, we present ELSA (Embeddings for Language and Spatial Audio), a spatially aware-audio and text embedding model trained using multimodal contrastive learning. ELSA supports non-spatial audio, spatial audio, and open vocabulary text captions describing both the spatial and semantic components of sound. To train ELSA: (a) we spatially augment the audio and captions of three open-source audio datasets totaling 4,738 hours and 890,038 samples of audio comprised from 8,972 simulated spatial configurations, and (b) we design an encoder to capture the semantics of non-spatial audio, and the semantics and spatial attributes of spatial audio using contrastive learning. ELSA is a single model that is competitive with state-of-the-art for both semantic retrieval and 3D source localization. In particular, ELSA achieves \(+2.8\%\) mean audio-to-text and text-to-audio R@1 above the LAION-CLAP  baseline, and outperforms by \(-11.6@math@degree\) mean-absolute-error in 3D source localization over the SeldNET  baseline on the TUT Sound Events 2018 benchmark . Moreover, we show that the representation-space of ELSA is structured, enabling swapping of direction of audio via vector arithmetic of two directional text embeddings.

## 1 Introduction

Humans use implicit context when communicating about and comprehending sounds in their environment. For instance, the instruction "Pull over if you hear a siren from behind you" is easily understood by most humans. However, a machine would need to not only recognize the source the sound, i.e., the siren (a semantic cue), but also interpret the spatial reference implied by "behind"relative to its own position (a spatial cue). The machine must then translate these linguistic cues into its understanding of spatial audio to accurately identify, locate, and conditionally respond to the sound. This degree of alignment between spatial audio and natural language is understudied in prior work.

Audio foundation models (AFMs), such as LAION-CLAP , have been used for multiple downstream applications, such as language guided audio editing , language guided audio and music generation , audio representations for image and text , setting a precedent for the wide applicability of audio representations aligned with natural language. However, these models, and similar state-of-the-art AFMs, such as Pengi , LTU , and SALMONN , cannot capture the _spatial attributes_ as the models are trained only on single-channel/non-spatial audio. Conversely, models such as SELDNet , and PILOT  are capable of precise spatial attribute classification and regression, but lack capability to generalize to natural language descriptions of spatial and semantic attributes.

To address these challenges, we introduce ELSA, a multimodal foundation model that learns a joint representation space for the spatial attributes and semantics of audio aligned with natural language descriptions of the audio scene. Learning a joint contrastive representation model enables and improves several tasks including: retrieval, multimodal QA, captioning, and generation . Prior work has shown the benefits of learning well-aligned encoders for multimodal tasks within the vision domain . In contrast, mapping between audio and natural language via a large language model, as in , can significantly improve language reasoning tasks, especially for zero-shot generalization of pre-trained models, however yields worse performance in classification and QA tasks when fine-tuned in the language domain . ELSA enables similar spatially-aware downstream applications, for instance, one can expand traditional language-guided audio editing to manipulate spatial elements using natural language commands like: "Remove the sound of the plane flying above" or "move the sound of the dog barking from left to right". In this paper we focus, for the first time, on devising and analyzing multimodal task-agnostic representations that capture both the semantics and the spatial attributes of audio aligned with natural language.

Noting a lack of paired spatial audio and language data that can enable training spatially aware audio-language models at scale, to train ELSA, we synthesize a spatial audio corpus consisting of 890,038 samples that span a variety of acoustic room properties, such as size and reverberation, from audio clips of the AudioSet  and Freesound  corpora. We also synthesize natural language spatial audio captions to match the spatial audio using a large-language model (LLM) to rephrase the initial captions. We demonstrate that ELSA captures spatial attributes and semantics of audio by identifying a set of tasks on which a standard AFM, such as LAION-CLAP, fails. Moreover, we show that ELSA achieves better zero-shot classification of spatial attributes than models trained only for that task. Finally, we show that ELSA maintains the ability to represent non-spatial audio by demonstrating performance competitive with existing state-of-the-art for a number of tasks.

Our key contributions are:

* We present and release a new synthetic dataset of 4738.55 hours, with 890,038 samples and corresponding spatial captions across 8,972 simulated rooms with accurate parametric labels for the room properties and sound source locations. Additionally, we also record a small spatial real-world dataset to verify transfer to the real-world (cf. Section 3).
* We provide ELSA, a multimodal spatial audio-language model that jointly performs semantic classification (sound detection, retrieval), spatial localization, and direction of arrival. ELSA consists of an audio encoder paired with a text encoder that jointly learns semantic and spatial attributes via contrastive learning (cf. Section 4).
* We show that ELSA effectively captures spatial attributes and semantics competitive with baselines. ELSA improves by \(-11.6^{}\) mean-absolute-error on 3D source localization, and by \(+2.9\%\) on text-to-audio and audio-to-text mAP@10 scores. (cf. Section 5).
* Further, we show that the representation-space of ELSA is structured, allowing for transposition of spatial sound direction via addition or subtraction of two spatially descriptive text embeddings. (cf. Section 5.4).

## 2 Related Work

We provide an overview of the architecture choices and corresponding datasets for training and evaluation of models that can capture the semantics and spatial attributes of audio, including AFMs.

Audio-language ApproachesPrior works (e.g., CLAP , LAION-CLAP , MULAN ) have extended the image-text contrastive pre-training approach introduced by CLIP  to link audio representations to textual descriptions. These models use two encoders, one for audio and another for text, to project the representations from the two modalities into a common embedding space. Once trained, the models enable zero-shot prediction and retrieval capabilities on unseen sounds and textual descriptions. Despite their utility, the methods do not capture the spatial attributes of the modeled signals, rather they capture only their semantics. Another line of work (e.g., Pengi , LTU , and SALMONN ) extends LLMs to enable audio understanding in open-vocabulary settings (e.g., audio captioning and audio question answering). Such models learn audio encoders to provide a prefix token to prompt a frozen pre-trained autoregressive LLM, which is then used to generate unconstrained text. These prior methods do not explicitly model the spatial attributes of the audio. Zheng et al.  introduced BAT, an audio-based LLM that combines binaural spatial sound perception with natural language understanding with an accompanying question-and-answer dataset that enables model training. BAT focuses on enabling LLMs to reason about binaural spatial audio, which depends on the head-related transfer-function. In contrast our focus is on a task-agnostic and device-agnostic representation of spatial audio aligned with text.

Audio-language DatasetsLearning audio-language models requires access to datasets that link the two modalities. Clotho  and AudioCaps  are popular audio captioning datasets for which the textual descriptions were collected by annotating sound event datasets (e.g., AudioSet , or Freesound ) through crowd-sourcing platforms. LAION-Audio-630K  is a large-scale audio-text dataset collected by downloading audio and relevant textual descriptions from publicly available websites. All three datasets focus on the semantic attributes of the audio signal and do not have labels for the spatial attributes. SPATIALSOUNDQA  is a dataset that consists of simulated binaural audio samples and question-answer pairs, which was used to train BAT . The audio samples were sourced from AudioSet , and the question-answer pairs were paraphrased using GPT-4. In contrast with the our environmental description captions, the text in SPATIALSOUNDQA is geared towards question-and-answer tasks. In addition, the dataset employs a binaural representation of spatial audio, rendering the data incompatible with ELSA. STARSS23  is a dataset of real-world multi-channel audio annotated with semantic labels for overlapping sound sources, and the equivalent annotations for the spatial attributes. However, a limitation is that the dataset lacks natural language descriptions of the sound scenes, which are required for aligning the spatial attributes with language descriptions.

## 3 Paired Spatial Audio and Text Datasets

Multimodal contrastive learning approaches, e.g., CLIP  and CLAP , use large amounts of multimodal data pairs: 413M and 634k for the LAION versions of both models [32; 44]. Training a model

Figure 1: Our pipeline for learning spatial-audio representations aligned with natural language.

capable of understanding spatial audio as natural language requires a spatial audio dataset annotated with natural language spatial descriptions (e.g., "a dog barking in the far left corner of a room"). To the best of our knowledge, no such dataset is available. Thus, we use a spatial augmentation pipeline composed of two steps: simulating spatial audio in synthetic rooms (cf. Section 3.2 and Fig. 0(a)), and caption rephrasing using terms that refer to spatial audio attributes (cf. Section 3.2 and Fig. 0(b)). We use AudioCaps , Clotho , and Freesound  as base datasets for our augmentation pipeline.

The training set ensures at least two spatial augmentations per data point, allowing for the model to see the same audio with at least two different spatial augmentations per epoch. We generate two different sized versions of the evaluation and test sets. The larger version consists, once more, of at least two augmentations per audio sample, whilst the smaller version has no repeated samples and, consequently, is the same size as the original test set. The smaller dataset allows reporting retrieval results on the same sized dataset as the original, as size uniformity is key to consistency in retrieval metrics. The size of the respective datasets is reported in Appendix A.1. For all datasets, we use first-order ambisonics (FOA) as the encoding of spatial audio, which we describe next.

### Spatial Audio Encoding: First Order Ambisonics

Monophonic, non-spatial audio, captures the spectral and temporal nature of sound, which carries a significant portion of context. Spatial audio provides additional context as it contains both spectral-temporal information and directional attributes, characterized by azimuth and elevation \((,)^{2}\), and distance. Binaural audio, a common spatial audio distribution format, mimics the signal entering the ear canals. Whilst binaural audio may be a natural choice for playback over headphones, it presents challenges for encoding, storing, and processing spatial information due to the presence of head-related transfer-functions in the signal . To facilitate more processing flexibility, microphone array signals are often encoded as ambisonics . This is accomplished by taking the spherical Fourier transform of the microphone signals and removing their radial component, which is equivalent to representing the spatial signal as a phase-coincident, infinite series in a spherical harmonic basis . In practice, to avoid spatial aliasing, this series is truncated at an order proportional to the number of microphones in the array, with higher orders corresponding to a higher spatial resolution. Ambisonics are linearly mappable into a variety of audio playback formats, including binaural. First-order ambisonics (FOA) can be recorded using readily available four-channel microphone arrays, and have been shown to carry significant spatial information . As such, we develop our models to ingest FOA signals. We leave generalization to higher orders for future work. It is worthwhile noting that once microphone array signals have been encoded into ambisonics, no a-priori knowledge on the structure of the capturing array is needed in order to perform any downstream spatial processing. Thus, ambisonics are agnostic to both recording and playback devices, making any embeddings derived from them equally generalizable.

### Spatial Augmentation of the Audio and Captions

Like TUT Sounds Events 2018  and BAT , we use a simulator to spatially augment non-spatial audio. The augmentation pipeline mirrors that of Spatial LibriSpeech . We specify room configurations parameterized by size, shape, and reverberation time, where reverberation time is a function of the room structure and materials with characteristic absorption and scattering coefficients. The simulator further allows specification of the placement and direction of the receiver microphones relative to the source of the sound (see Fig. 0(a)). For each sample we remove leading and trailing silences, and repeat the audio signal to ensure that samples are at least four seconds long before simulation. A randomly chosen room, placement for the microphone, and placement for the static source is then selected. We ensure that the room augmentations do not overlap between the train, evaluation, and test datasets. The rooms vary in size between 13.3m\({}^{2}\)and 277.4m\({}^{2}\), their full-band T30 reverberance ranges from 114.5ms to 2671.9ms. The full statistics of these synthetic rooms can be found in Appendix A.1.

Our caption augmentation pipeline converts raw numerical values associated with the spatial audio attributes of the room simulator (e.g., distance to the microphone) into natural language descriptors (e.g., "near" or "far"). Our caption augmentation pipeline is shown in Fig. 0(b). The full mapping from spatial audio attributes to natural language is given in Appendix A.2.

The original caption augmented with the spatial information makes up the input to LLaMA-13B , which is prompted to rephrase in the form of a spatially augmented caption. The prompt is:

The sound: <_original caption>_ is coming from the <_distance>_ <_elevation>_ <_direction>_ of a <_size>_ <_reverb>_ room. Rephrase as a short English sentence describing the sound and all the details of its source.

This template prompt overcomes challenges like non-English language in the original caption, missing spatial descriptors in the generated caption, and hallucinations that changed the meaning of the caption. We set the inference temperature of the LLM to 0.9 and the maximum tokens to 1,024. Appendix A.3 contains examples of the obtained spatial captions. We note that the caption re-writes can lead to hallucinations, which is discussed further in Appendix A.4. We leave the quantification and mitigation of hallucinations for future work.

### Spatial Real-World Dataset

Our training data consists of synthetically-augmented audio and captions, so we also recorded a small dataset to verify generalization to real-world data (refer to Sections 5.2 and 5.3 for analysis). Our spatial real-world dataset was recorded using a Zylia 19 microphone spherical array at 48kHz with a bit-depth of 24-bits per sample. The dataset contains environmental sounds typically found in an apartment. In total, we recorded 70 samples of spatial audio in five rooms. Each spatial audio sample in the dataset was captioned with the semantic content (e.g., "sound of a vacuum"), and the direction ("left", "right", "front", "back"), distance ("far", "near"), and elevation ("up", "down", "level"). For privacy, no personally identifiable information was included in the dataset.

## 4 ELSA Pretraining for Spatial Audio and Language

Our architecture is derived from LIAION-CLAP , which is composed of an audio encoder and a text encoder that aligns embeddings for similar samples across modalities whilst maintaining the original representational capabilities of the individual modalities.

### Audio Input Features

The audio encoder must capture both the semantics of the audio (e.g., "the sound of a fire alarm") and the spatial attributes (e.g., "the upper right of a reverberant room"). Following LAION-CLAP  and BAT , we translate the raw audio into the frequency domain. Consider a FOA signal represented by tensor, \(^{T F(N+1)^{2}}\), where \(N=1\) is the spherical-harmonics order, \(T\) the number of time frames and \(F\) the number of frequency bins. More information on the derivation of \(\) can be found in Appendix A.5. The corresponding real-valued log-mel spectrogram feature can be written as:

\[(t,)=(|(t,f)|^{2}_{} (f,)),\] (1)

where \(_{}\) is the corresponding filter, \(\) is the filter index, \(t\) is time, and \(f\) is frequency. As summarized in Table 1 of SALSA , both mel-spectrograms and intensity vectors (IVs) are effective spatial features for FOAs. We extract the IVs, \((t,f)\) as follows:

\[_{}(t,f)=[A_{0,0}^{*}(t,f)A_{1, 1}(t,f)\\ A_{1,0}(t,f)\\ A_{1,1}(t,f)],_{}(t,f)=[A_{0,0}^{*}(t,f)A_{1,1}(t,f)\\ A_{1,0}(t,f)\\ A_{1,1}(t,f)],\] (2)

where \(A_{n,m}\) are the \(n^{}\) and \(m^{}\) order and mode of the ambisonics signal corresponding to its omnidirectional (\(W\)) and three dipole \((Z,Y,X)\) components, and \(()^{*}\) denotes complex conjugation. Physical normalization constants are omitted here for brevity as IVs are scaled to unit-norm .

For ELSA to use semantic features from both non-spatial audio and FOAs, during training we use sample from both the spatially-augmented datasets and the original non-spatial dataset. Since first-order ambisonics has four channels, and non-spatial audio only one, we copy the single-channel non-spatial signal across all channels. Intensity vectors normalize the dipoles by the omni channel, and result in identical IVs for non-spatial audio. We let the model learn this condition. We ablate the effect of using both spatial audio and non-spatial audio in Appendix A.6 and find that using both improves semantic retrieval.

### Audio and Text Encoders

Our architecture is composed of an audio encoder and a text encoder. The audio encoder consists of two branches: the _semantic audio branch_, and the _spatial attributes branch_. See Appendix A.7 for a visualization of the full architecture.

For the semantic audio branch, we use HTSAT  since it was found to perform best in the LAION-CLAP evaluation . HTSAT is a transformer-based audio encoder with self-attention blocks to achieve high performance in audio classification tasks. We initialize HTSAT with weights provided by LAION-CLAP2. For spatial-audio input, we feed only the mel-spectrogram of the omni channel from the first-order ambisonics encoding. The omni channel does not contain spatial characteristics, so its role is equivalent to single channel, non-spatial audio. This branch has 30M parameters.

As far as we are aware, there is no existing established feature encoder for spatial audio. Thus, for our spatial attributes branch we propose a two-branched CNN based on the architecture of  that was trained on a multi-task regression loss for azimuth, elevation, distance, and third-octave direct-to-reverberant ratio. The branch was trained for 100 epochs on Spatial LibriSpeech, which uses FOA spatial audio and has enough samples to train the spatial attributes branch. Further details, along with the full training hyper-parameters are discussed in Appendix A.8. This branch is fed the active and reactive intensity vector features described in Eq. (2). This branch has 486k parameters.

The outputs of both the _semantic_ (768-dimensional) and the _spatial attributes_ (192-dimensional) branches are concatenated to form a 960-dimensional embedding. Using a two-layer multi-layer perceptron (MLP), they are subsequently projected down to a 512-dimensional embedding.

For the text branch, we follow the best performing model in LAION-CLAP , and use RoBERTa-base . RoBERTa is a general purpose bidirectional transformer , pretrained on a dynamically masked token prediction task, which employs byte-pair encoding  for tokenization. We use the same pre-trained model as  as the starting point3. The text encoder has 125M parameters, and the final embedding has a dimensionality of 712, which also is projected down to 512 by a two-layer MLP, matching the size of the audio encoder output.

### Pretraining Objectives

We learn aligned representations using batched contrastive loss (popularized by CLIP ). The loss function rewards the alignment of representations from the same sample but different modalities, and penalizes the alignment of representations from different samples (see Fig. 1c). Our loss (in common with CLIP , CLAP , and LAION-CLAP ) is derived from the InfoNCE loss , as we now describe. Given a set of embeddings of any modality \(X^{N D}\) where the \(i^{ th}\) entry, \(x_{i}^{D}\) is to be matched with \(y^{D}\), the following InfoNCE sample loss maximizes the similarity between the pair \(x_{i}\) and \(y\), and minimizes the similarity between all other \(x\) and \(y\) pairs:

\[_{ InfoNCE}(X,x_{i},y)=-(x_{i},y)}{_{x_{ j} X}f_{ sim}(x_{j},y)},\] (3)

where \(f_{ sim}(a,b)=(a b/)\) is a similarity function with a learnable temperature parameter \(\). Taking the average across all audio-text pairs in the batch, where entries at the \(i^{ th}\) position match each other, we arrive at the CLIP loss:

\[_{ CLIP}= (_{i=0}^{N}_{ InfoNCE }(Z^{a},z_{i}^{a},z_{i}^{t})+_{i=0}^{N}_{ InfoNCE} (Z^{t},z_{i}^{t},z_{i}^{a})),\] (4) \[= -_{i=0}^{N}((z_{i}^{a},z_{i}^{t})}{_{j=0}^{N}f_{ sim}(z_{j}^{a},z_{i}^{t})}+(z_{i}^{t},z_{i}^{a})}{_{j=0}^{N}f_{ sim}(z_{j}^{t},z_{i}^{a})}),\]

Since the rooms we use to spatially-augment the audio are parametric, we have accurate labels associated with spatial features of the audio source. We take advantage of these labels by adding three additional spatial regression objectives. We feed the generated 512 dimension audio embedding intothree 2-layer MLPs of 33k parameters, which respectively regress the direction of arrival (azimuth and elevation) of sound in 3D space, distance of the source to the receiver, and room floor area. These objectives, along with the CLIP loss in Eq. (4), define our final loss:

\[_{}=_{}+_{}+_{}+_{},\] (5)

where \(_{}\) is the cosine similarity between the predicted and target angles, and \(_{}\) and \(_{}\) is the mean-squared error between the predicted and target distances and room floor area respectively. We ablate the differences between \(_{}\) and \(_{}\) in Appendix A.6 and find that at a negligible cost (0.4%) to semantic retrieval, we get a 15.3% improvement to 3D localization capability and 12.3% improvement in distance estimation when using \(_{}\).

## 5 Experiments, Results, and Discussion

We demonstrate that ELSA jointly captures the semantics and spatial attributes of sound with either audio or text inputs by answering the following research questions:

**RQ1**: Does ELSA capture _spatial attributes_ in spatial audio (Section 5.2)?
**RQ2**: Does ELSA capture _semantic information_ in both text and audio (Section 5.3)?
**RQ3**: Does ELSA transfer to our real-world dataset? (Sections 5.2 and 5.3)?
**RQ4**: Does ELSA provide interpretable multimodal representations (Section 5.4)?
**RQ5**: Are ELSA embeddings capable of driving automatic captioning (Section 5.5)?

### Training and Evaluation of ELSA

As indicated in Section 4.2, we use pretrained weights for the semantic audio encoder, the spatial attributes encoder, and the text encoder. All components of the model are fine-tuned, which corresponds to 158M trainable parameters, an increase of 0.86% over LAION-CLAP.

For our best model, we train for 40 epochs on 12 nodes, each with 8 NVIDIA A100 GPUs and \(96\) CPU cores with a batch size of 2,304. Training converges within 17 hours. We use the Adam optimizer with a learning rate of \(5 10^{-5}\) and cosine scheduling. We select the checkpoint with the lowest mAP@10 retrieval on the spatially augmented captions.

### Spatial Attributes Evaluation

We show that ELSA captures the spatial attributes of sound (**RQ1**) by carrying out downstream regression and zero-shot spatial prompt classification. For regression to 3D sound localization, we a train two-layer MLP with 32,768

    & Semantic & Spatial & AudioCaps & REAL 3D \\  & Capabilities & Capabilities & mAP@10\(\) & Local(\({}^{}\))\(\) \\  SeldNET  & � Limited vocab. & ✓ & ✗ & 26.6 \\ PILOT  & � Limited vocab. & ✓ & ✗ & 4.2 \\  Spatial Librispeech  & ✗ & ✓ & ✗ & 12.4 \\ LAION-CLAP  & ✓ Open vocab. & ✗ & 43.8 & 95.29 \\ ELSA (ours) & ✓ Open vocab. & ✓ & **44.2** & 14.97 \\   

Table 1: Comparison of model capabilities and performance for retrieval of semantic captions from AudioCaps, and 3D sound localization for the REAL component TUT Sound Events 2018. ELSA is the only model that allows both open vocabulary language understanding and spatial localization, and performs comparably against the baselines for both tasks.

   Task & S-Clotho & S-AC & S-RWD \\  Distance (2-class) & 96.0\% & 92.9\% & 67.1\% \\ Direction (4-class) & 92.0\% & 92.8\% & 35.8\% \\ Elevation (2-class) & 100.0\% & 100.0\% & 72.1\% \\ Room area (2-class) & 76.6\% & 74.7\% & N/A \\ Reverberation (2-class) & 100.0\% & 83.3\% & N/A \\   

Table 2: Zero-shot classification accuracy using the cosine similarity between test set audio embeddings and templated probe caption embeddings. The template is “A sound coming from ‘spatial attribute>’ and a value for <spatial attribute>’ is substituted into the template representing the desired class (e.g., “near” or “far” for distance). A classification is correct if the attribute in the closest test sample matches the attribute in the template. We cannot provide comparisons with baselines since this is a new task.

parameters using the ELSA audio embeddings generated from the training set. We then evaluate on the REAL component of the TUT Sound Events 2018 dataset . Table 1 confirms that CLAP cannot encode spatial attributes (95.29\({}^{}\)), whereas ELSA achieves 14.97\({}^{}\)mean-absolute error (MAE) and maintains a higher mAP@10 for semantic retrieval tasks than CLAP. Appendix A.9 shows that there is little variability in the direction-of-arrival error across various spatial attributes. However, we note the errors tend to be higher at the extrema of the dimensions. When compared to methods designed explicitly for 3D sound localization, ELSA performs better than SeldNET (the baseline included with TUT Sound Events 2018) by +11.6\({}^{}\), and that achieves only -2.6\({}^{}\)MAE compared to the model in Spatial LibriSpeech4. ELSA does not reach the performance of PILOT (4.3\({}^{}\)), but this model was specifically-tuned only for 3D sound localization on data derived from TUT Sound Events 2018 .

To verify that the spatial attributes are aligned with language, we create new captions using the template: "A sound coming from <spatial attribute>", where <spatial attribute> can be distance, direction, elevation, room size, and reverberation. For instance, a caption for distance might be "A sound coming from far away". The ELSA text embeddings for such captions are extracted from the pre-trained encoder and compared in a zero-shot fashion with ELSA audio embeddings for samples from the test set using cosine similarity. We classify the match as correct if the spatial attribute in the closest audio sample matches the spatial attribute of the query caption, and we report accuracy in Table 2. ELSA achieves >90% correct retrieval for most spatial attributes. For room area, ELSA achieves 74.7% correct retrieval, which we hypothesize is due to the relatively small perceptual differences between small (<50m\({}^{2}\)) and large rooms (>100m\({}^{2}\)). We observe a transfer gap on retrieval scores when evaluating on our spatial real-world dataset, with ELSA achieving 67.1% (distance), 35.8% (direction) and 72.1% (elevation) correct retrieval. Part of this performance difference is because the spatial attributes were only estimates by the annotators during data capture. On the other hand, performance using pre-trained LAION-CLAP is close to random for all tasks (Appendix A.10), which is expected as CLAP was not trained with spatial audio or captions.

### Semantics Evaluation

Following LAION-CLAP , we calculate retrieval results when finding matches from audio-to-text and text-to-audio. To compute retrieval, we encode the test set for each modality, and for every sample we check whether the corresponding sample in the other modality has the closest cosine distance (R@1), is within the five closest samples (R@5), or within the ten closest samples (R@10). The results in Table 3 show that in addition to learning representations of spatial captions and spatial audio, ELSA also performs on par with LAION-CLAP on non-spatial tasks. Table A.7.8 in Appendix A.12 shows the retrieval results when using spatially-augmented versions of AudioCaps and Clotho. We remark that adding Freesound to the training set decreases the retrieval scores in Spatial AudioCaps, but improves retrieval scores in Clotho, due to Clotho being a differently-captioned subset of Freesound. We note that the spatial retrieval performance of ELSA is lower than the non-spatial retrieval performance (for instance, -9.4% and -13.3% on audio-to-text and text-to-audio R@10

    & &  \\  & &  &  &  &  \\ Model & Train Data & R@1 & R@5 & R@10 & R@1 & R@0 & R@1 & R@1 & R@5 & R@10 & R@10 & R@1 & R@5 & R@10 \\  CLAP (paper) & \(C,AC,LA\) & 34.7 & 70.5 & 83.2 & 45.3 & 79.5 & 89.2 & 16.4 & 39.0 & 51.0 & 21.8 & 44.6 & 60.1 \\  CLAP (local) & \(C,AC,FS\) & 32.7 & 68.8 & 81.5 & 40.7 & 74.0 & 84.7 & 14.4 & 37.6 & 50.7 & 18.3 & 40.5 & 55.1 \\ ELSA & \(C,AC,FS,C^{S},AC^{S},FS^{S}\) & **33.2** & 68.2 & 81.0 & **40.9** & **74.4** & **86.1** & **15.0** & 36.7 & **50.8** & **20.1** & **43.2** & **55.4** \\   

Table 3: Semantic retrieval (R@1, R@5, and R@10) for CLAP and ELSA calculated over the original (non-spatial) versions of Clotho and AudioCaps. Although ELSA is trained using a mixture of non-spatial and spatial audio, it conserves the retrieval performance on non-spatial audio of LAION-AI CLAP, which was trained on only non-spatial data. For the training data, read \(C\) as Clotho, \(AC\) as AudioCaps, \(LA\) as LAION-Audio-630K and \(FS\) as Freesound. A superscript \({}^{S}\) denotes the spatially-augmented equivalent dataset. We use Freesound, a subset of LAION-Audio-630K due to its more permissive licensing. For a fair comparison, we train a version of CLAP locally with Clotho, Audiocaps and Freesound, which is not reported in the CLAP paper.

AudioCaps). This reflects the fact that spatial captions are harder to match, since there is a larger number of attributes and since there are hard-negatives (same semantics, different spatial attributes). Still, ELSA achieves the highest retrieval scores on the spatial real-world dataset (Table A.7.9 in Appendix A.12), showcasing its ability to transfer to the real-world without fine-tuning. Note that we cannot provide comparisons with prior models since using these spatial augmentations is a new task.

### Interpreting the representation structure of ELSA

To confirm that directional characteristics in ELSA spatial caption embeddings are encoded in the same feature space as those in ELSA spatial audio embeddings, we train a direction regressor with a two-layer MLP using the _spatial audio embeddings_ in the training split. We subsequently regress the _spatial text embeddings_ to azimuth values using our trained regressor and affix direction labels ("left", "right", "front", "back") to each sample based on the azimuth values. We obtain 64.3% accuracy on the four-class problem, indicating alignment between the encoding of the modalities. Similarly, we obtain an accuracy of 76.5% for when classifying over distance labels ("far", "near") and 55.1% over elevation labels ("up", "down").

Besides using regression to confirm that ELSA embeddings capture spatial direction, we verify whether the ELSA embeddings can be clustered by spatial attributes. Fig. 2 shows a UMAP projection of the ELSA embeddings from the test sets of Spatial-AudioCaps and Spatial-Clotho. Note that the UMAP projection was guided with the embeddings and labels of the training sets of both datasets. The figure shows the embeddings cluster well with the direction labels, though there is some degree of confusion between "back" and "front". This is corroborated by the analysis in Appendix A.13, where we compute Wasserstein distances directly in the 512-dimensional space. We carried out a similar analysis for spatial distance and found the embeddings cluster clearly between "near" and "far".

We validate that ELSA audio embeddings capture implicit spatial attributes that are latent in the text-encoder by first training a classifier using the spatial audio in our training data, where the classes are broad directions, such as _above_ and _below_. We use LLaMA-13B  to generate descriptions of sounds that would typically come from each of these directions, e.g., "the rhythmic drumming of raindrops on a skylight" (for above) and "the faint creaking of an old house settling" (for below). Appendix A.11 lists all generated captions. Finally, we use the classifier trained on audio samples to classify ELSA embeddings for these generated captions with implicit directionality. We find that the classifier can correctly identify 68% of the sounds typically heard from above as being from above, showing that the latent space of the text encoder for ELSA is capturing directionality.

Lastly, we show that we can swap the spatial direction encoded by an ELSA audio embedding with a simple text caption. We first obtain ELSA prototypes for four directions ("left", "right", "front", "back") with the template: "A sound coming from the _direction_". Next, we train a 4-class direction classifier with a two-layer MLP using the spatial audio in the training splits of our spatially-augmented datasets. To swap the direction of the sound, we subtract the text prototype of the original direction and add prototype for the new direction. For evaluation, we swap the spatial direction of every sample in our spatially-augmented test set that was correctly classified by the 4-class direction classifier (96.7% of the audio embeddings). Our results show that 99.7% of the swapped samples are classified correctly with the new spatial direction, which highlights the strong alignment of spatial features across modalities, resulting in the ability to edit spatial attributes of

Figure 2: UMAP projection of ELSA embeddings of the test splits of Spatial-Clotho and Spatial-AudioCaps. Filled markers are obtained from spatial audio, and hollow markers are obtained from spatial captions. The UMAP projection was fitted with the train splits of Spatial-Clotho and Spatial-Audio caps, and we made use of supervised dimension reduction to highlight the _direction_ differences rather than the semantic differences in the embeddings.

existing spatial audio using text in embedding space. Further details about this experiment are described in Appendix A.14. These results also point to exciting avenues wherein text can condition the manipulation and generation of spatial characteristics of audio. We leave this application for future work.

### Spatial Audio Caption Generation

Decoding multimodal embeddings into natural language can be achieved by prefixing an autoregressive causal language model , where the prefix is constructed from a projection of the multimodal embeddings. To facilitate audio captioning using ELSA, we fine-tune a GPT-2 model  with 12 attention layers each having 12 heads (with 163M parameters). The ELSA embeddings are projected onto the prefix using a single dense layer (393k parameters). With the ELSA encoder frozen, we train the GPT-2 model on 150k spatial-audio embedding and caption pairs from Spatial-Clotho and Spatial-AudioCaps. We report caption generation metrics in Table 4 and show three generation samples in Appendix A.15. Overall, we find that automatic spatial audio captioning systems are viable though more work is needed to increase the vocabulary size of the generations.

## 6 Conclusions, Limitations, and Further Work

We have presented ELSA, an AFM that aligns representations of spatial audio and equivalent text descriptions. To train such representations we built a pipeline to spatially augmented the audio in existing non-spatial audio-text datasets, such as Clotho  and AudioCaps , and added spatial information to their respective captions. Our results show that ELSA embeddings capture _both_ the semantic contents and the spatial attributes of the audio, with ELSA achieving +2.8% higher scores in audio-to-text and text-to-audio retrieval scores than the state-of-the-art, and obtaining -2.6\({}^{}\)MAE in direction of arrival error with respect to an equivalent baseline. Interestingly, by mixing spatial and non-spatial audio and caption pairs, ELSA is able to represent non-spatial audio as well, Finally, we show that the representation space of ELSA is structured in that the directionality of a spatial audio sample can be transposed by simple addition or subtraction of two text representations. Future work will explore acoustic scenarios with overlapping sound sources and sound sources that are moving in the scene. ELSA will also benefit from advances in spatial attributes encoders. In this work, we used the augmented spatial captions as is, but further work should ensure consistency with the semantics before and after augmentation, which will further improve the representational power of ELSA.

Perceiving spatial audio is a fundamental aspect of human nature. As is linking perception with language. Using spatial audio and a contrastive multimodal training approach, ELSA bridges the gap between feature rich spatial audio and language, paving the way for more intuitive and effective human-machine interactions by allowing for richer understanding of the users' environment and generation of immersive sound scenes from natural language.

Broader ImpactOur research has the potential to be used in creation of immersive augmented or virtual reality environments. If not controlled well, these immersive experiences have the potential to become addictive, and thus impact the mental health of individuals or even society as a whole. Another danger is possibility of creating deepfakes of soundscapes, thus making it possible for generated 3D environments to sound very realistic. The proliferation of deepfake soundscapes could lead to misinformation and manipulation, undermining trust in audio media.

   Metric & Range & S-Clotho & S-ac \\  SPIDE & [0, 5.5] & 0.19 & 0.34 \\ FENSE & [-1.0, +1.0] & 0.59 & 0.68 \\ \# Unique words & [0, \(\)) & 1103 & 1258 \\   

Table 4: Evaluation of Spatial Audio Caption Generation. Metrics were obtained from the Audio Captioning task of the DCASE Challenge5 by comparing the generated captions produced from spatial audio and the ground-truth captions from the test splits of Spatial-AudioCaps (S-AC) and Spatial-Clotho.