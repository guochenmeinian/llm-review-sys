# Theoretical Foundations of Deep Selective State-Space Models

Nicola Muca Cirone

Department of Mathematics

Imperial College London

&Antonio Orvieto

MPI for Intelligent Systems,

Tubingen AI Center

ELLIS Institute Tubingen

&Benjamin Walker

Mathematical Institute

University of Oxford

Cristopher Salvi

Department of Mathematics

Imperial College London

&Terry Lyons

Mathematical Institute

University of Oxford

###### Abstract

Structured state-space models (SSMs) are gaining popularity as effective foundational architectures for sequential data, demonstrating outstanding performance across a diverse set of domains alongside desirable scalability properties. Recent developments show that if the linear recurrence powering SSMs allows for a selectivity mechanism leveraging multiplicative interactions between inputs and hidden states (e.g. Mamba, GLA, Hawk/Griffin, HGRN2), then the resulting architecture can surpass attention-powered foundation models trained on text in both accuracy and efficiency, at scales of billion parameters. In this paper, we give theoretical grounding to the selectivity mechanism, often linked to in-context learning, using tools from Rough Path Theory. We provide a framework for the theoretical analysis of generalized selective SSMs, fully characterizing their expressive power and identifying the gating mechanism as the crucial architectural choice. Our analysis provides a closed-form description of the expressive powers of modern SSMs, such as Mamba, quantifying theoretically the drastic improvement in performance from the previous generation of models, such as S4. Our theory not only motivates the success of modern selective state-space models, but also provides a solid framework to understand the expressive power of future SSM variants. In particular, it suggests cross-channel interactions could play a vital role in future improvements.

## 1 Introduction

Sequence-to-sequence blocks are fundamental components of modern deep learning models for language, images, video, audio, time series, and genomics. For the last five years,attention (Vaswani et al., 2017; Dosovitskiy et al., 2020) has been the dominant mechanism powering these architectures. However, competitive results have recently been achieved without attention, by using state-space models (SSMs): GPU-efficient linear recurrent sequence-to-sequence blocks stemming from S4 (Gu et al., 2021). SSMs achieve state-of-the-art results on long-range-reasoning benchmarks (Tay et al., 2020) and show outstanding performance in various domain including vision (Nguyen et al., 2022), audio (Goel et al., 2022), biological signals (Gu et al., 2021), reinforcement learning (Lu et al., 2023) and online learning (Zucchet et al., 2023). SSMs recently have gained significant interest in the community since their computational complexity scales linearly in sequence length, while attention scales quadratically; moreover, unlike other recurrent mechanisms such as LSTMs (Hochreiter and Schmidhuber, 1997) and GRUs (Cho et al., 2014), they can be efficiently parallelized on GPUs during training using parallel scans (Martin and Cundy, 2017; Smith et al., 2023).

While standard SSMs were shown to be particularly powerful on signal processing tasks, their computation power is limited: the core sequential mechanism of S4 is equivalent to a convolution (filtering) . This represents a drawback in challenging domains such as text and genetics, where the ability to select data efficiently in an input-dependent manner - i.e., perform content-based reasoning - is crucial (see ). Towards reaching this goal with recurrent models, various adaptations of S4 have been proposed in the last few months. Notably, Mamba  implements simple and efficient gating mechanisms on the S4 recurrence, unlocking input selectivity in the memory update. Mamba achieved state-of-the-art performance in various language modeling tasks while greatly improving the inference throughput. Similar ideas can be found in recent developments inspired by attention, such as RWKV , RetNet , Gateloop , Gated Linear Attention (GLA) , and HGRN2 . Very recently, De et al.  surpassed the performance of Mamba with a gated RNN architecture - Griffin - based on an improved version of the LRU , and  introduced minimal versions of GRU and LSTM as gated SSMs.

ContributionsAt the core of the models discussed above is a time-varying dynamical system, where reasoning is performed through an efficient and parallelizable update _linear_ in the hidden state. In this paper, we generalize the structure of such models, drawing a direct link to _controlled differential equations (CDEs)_ and use tools from _rough path theory_ to study expressivity.

1. In Sec. 3.1 we provide a framework for the analysis of (input-controlled) linear (in the hidden state) recurrences such as S4 and Mamba. This framework allows the use of powerful tools and results in the Rough Path Theory literature by casting a large family of SSMs as Linear CDEs driven by the two possibly nonlinear embeddings \(X^{X}\) and \(X^{X}\), defining gates. Appendices A and E provide a largely self-contained exposition of the key theoretical tools now available to us.
2. In Sec. 4 we fully characterize the closure (i.e. the class of functions which can be arbitrarily well approximated) of our generalized models. This provides a generalization of the results by Li et al. , Orvieto et al. , Wang and Xue , who only consider the case of S4. The Mamba setting is more rich, complex, and relevant given the rising interest in selective SSMs.
3. We show (Thm. 4.2) that full expressivity can be obtained by training only a linear layer on a Linear CDE with random parameters, providing a direct link to kernel methods and reservoirs.
4. We point out (Thm. 4.3) that if the recurrence is diagonal, as the case for Mamba, the closure is strictly smaller than in the general dense case. Interestingly though, the closure is a peculiar set of filters that unlock some specific context-dependent processing. Full expressive power is recovered by stacking multiple SSMs without MLPs in between (Prop. 4.5).

Our framework not only provides significant theoretical insight regarding some recently proposed SSM architectures, but we also envision it to be a useful tool in analysing, and perhaps developing, future architectural advances.

## 2 State-space Models

We describe here the structure of the main SSMs-based strategies for processing length-\(L\) input sequences of \(d\) dimensional tokens: \(x^{d L}\). We denote by \(x_{}\) the \(\)-th column of \(x\) (the \(\)-th token) and by \(x^{i}\) the \(i\)-th row of \(x\) (time series for the \(i\)-th channel). We will write \(A v\) for matrix-vector multiplication when this enhances comprehension, and use bold letters for "tensors" of order greater than 2 (such as \(_{i}^{N_{i} L}\) introduced below).

### Review of Modern SSMs

We start with a quick simplified recap of S4 , the first SSM proposed in the literature, and then describe recent improved variants such as Mamba (in particular, the S6 block) . We restrict our focus to the recurrent mechanism and invite the reader to refer to the original papers for a description of the token-wise operations following and preceding each block.

SSM basics and S4.Most1 SSMs (Gu et al., 2021, 2022) operate independently on input channels. Each time series \(x^{i}^{L}\) is seen as the result of sampling a latent continuous-time signal \(X^{i}:\) at multiples of a channel-dependent stepsize \(_{i}>0\): \(X^{i}_{_{i}}:=X^{i}(_{i})=x^{i}_{}\). In S4, each path \(X^{i}\) produces a complex-valued hidden state signal \(_{i}:^{N_{i}}\) as

\[d_{i;t}=A_{i}_{i;t}\;dt+B\;X^{i}_{t}dt,\] (1)

where \(A_{i}=(a_{i,1},a_{i,2}, a_{i,N})\) is channel-specific _diagonal_\(N_{i} N_{i}\) complex valued matrix and \(B^{N_{i}}\) is an input projection shared across input components \(i[d]\). SSMs are based on a stable discretization of the continuous system above: each input sequence channel \(x^{i}^{L}\) produces a sequence of hidden states \(_{i}=[_{i;1}|_{i;2}||_{i;L}] ^{N_{i} L}\) as follows:

\[_{i;}=_{i}_{i;-1}+_{i}x^{i}_{ },\] (2)

where \(_{i}\) and \(_{i}\) are determined by the discretization technique and the channel-dependent stepsize \(_{i}\). Under the commonly used Zero-Order Hold discretization2,

\[_{i}=(_{i}A_{i}),_{i}=(_{i}A_{i})^{-1}( (_{i}A_{i})-I)_{i}B_{i}B.\] (3)

Note from (2) that SSMs at inference time are equivalent to linear recurrent neural networks (RNNs). Yet, learning with gradient descent is performed on the continuous-time variables, unlocking stable signal propagation and alleviating vanishing gradients (Orvieto et al., 2023; Zucchet and Orvieto, 2024). Finally, at each channel \(i\), the sequence of hidden states is mapped back to real numbers, and linear projections \(C_{i}:^{N_{i}}\) are performed to produce an output a sequence of tokens \(y^{d L}\) with the same dimensions as \(x\):

\[y^{i}_{}:=C_{i}_{i;}\]

To conclude, we point out that the transition matrices \(A_{i}\) are often structured, i.e. initialized deterministically through HiPPO theory (Gu et al., 2020) in diagonal form. Common choices (Gu et al., 2022) are \(a_{.,n}=-+ n\) (S4D-Lin3) and \(a_{.,n}=-\) (S4D-Real).

**Mamba.** As done in practice, let us consider all channels' hidden dimensions \(N_{i}\) equal to \(N\). The Selective SSM (S6) powering the Mamba architecture (Gu and Dao, 2023) augments S4 with input-controlled matrices:

\[_{i;}=_{i}(x_{})_{i;-1}+_ {i}(x_{})x^{i}_{},\] (4)

where the most crucial component (see in-context learning argument by Gu and Dao (2023)) is the dependency of the _diagonal_ matrix \(_{i}(x_{})^{N N}\) at timestamp \(\) on _all_ input channels at timestamp \(\). This makes the operation \(_{i}(x_{})_{i;-1}\) effectively a _gate_. The dependency of \(_{i}:^{d}^{N N}\) on the input is achieved efficiently by letting \(_{i}\) in (3) be computed, at step \(\), as \(_{i}(x_{})\) where

\[_{i}:^{d},_{i}(x)=( _{i} x+_{i})\]

where \(\) is the scalar product and 4\(_{i}^{d}\), \(_{i}\). Further, \(_{i}:^{d}^{N}\) is computed via a, shared between channels, linear map \(B^{N d}\) via \(_{i}(x_{l})=(B x_{})\;_{i}(x_{})^{N}\). Finally, each \(_{i;}^{N}\) is projected to \(y^{i}_{}\) via a matrix \(C_{i}\). This step can also be done by means of output gating (\(C_{i}\) function of the input), but we avoid this complication here as it can be seen as an architectural component outside the recurrence.

_Remark 2.1_.: While each channel evolves separately, the laws of evolution are pointwise determined by all input features: \(A_{i}\) and \(B_{i}\) can be functions of \(x_{}\), and not just of \(x^{i}_{}\). We will discuss this in Sec. 4.3 after presenting our general results.

The RG-LRU (De et al., 2024) works similarly, yet processing all input channels at once with a diagonal recurrence. Gateloop (Katsch, 2023), GLA (Yang et al., 2023), and HGRN2 (Qin et al., 2024) leverage similar ideas, though they differ in parametrization and gating strategies.

### Known properties of (non-linear) recurrences

The expressiveness of standard nonlinear RNNs of the form \(z_{}=A(z_{-1})+Bx_{}\), where \(\) is a nonlinearity, has been extensively studied since the seminal work of Siegelmann and Sontag (1992), with recent contributions such as Korsky and Berwick (2019) and Hanson and Raginsky (2020). In particular, Hanson and Raginsky (2020) proved that wide enough non-linear RNNs can approximate up to vanishing precision non-linear time-homogeneous systems of differential equations driven by input paths. The argument used here is based on the celebrated Barron's theorem (Barron, 1993) for approximation of continuous functions with neural networks with one hidden layer. Indeed, note that non-linear RNNs are recurrent perceptrons with one hidden layer, acting both on the state and the input (Tallec and Ollivier, 2018). Instead, (selective) SSMs such as S4 and Mamba have transition map which is linear in the state - unlocking parallelization (Smith et al., 2023; Gu and Dao, 2023).

In the context of linear RNNs and non-selective SSMs, many results (classic and new) exist that characterize expressivity. Li et al. (2022) showed that linear RNNs (i.e. S4-like recurrences) can approximate arbitrary convolution filters in the width limit. Further, Hanson and Raginsky (2019) proved that stacking exponentially (in the sequence length) many temporal convolution filters, chained together with ReLU activations, leads to approximation of arbitrary non-linear filters. Recent works (Orvieto et al., 2023; Wang and Xue, 2023) prove the universality of linear recurrences (one layer) when equipped with a fixed (timestamp independent) point-wise MLP acting across the recurrence output, with intriguing connections to Volterra series (Boyd and Chua, 1985).

Mamba (alongside with gated linear attention variants e.g. Yang et al. (2023)) falls neither in the linear RNN nor the nonlinear RNN setting: its recurrence is linear on the hidden state (can be parallelized) but unlike S4, it is not linear time-invariant as the input controls the recurrence eigenvalues. In this paper, we are interested in this hybrid setting. It is worth noting that some work exploring Mamba's expressiveness has already been performed to study some interesting toy tasks (Jelassi et al., 2024) and to understand its limitation using the framework of formal language theory (Merrill et al., 2024). Compared to these works, which outline interesting failure cases, this paper studies a more general class of models, allowing to identify how architectural choices impact expressivity.

## 3 SSMs as Linear CDEs

The crucial component that unlocks in-context learning and selectivity in modern SSMs is _the input-dependent state-to-state transition matrix (Gu and Dao, 2023), gating the hidden state_ and thus allowing the system to filter out unnecessary context and remember relevant information indefinitely.

At the core of most modern SSMs is a a recurrence which is _linear in the hidden state, but potentially non-linear in the input_. This class includes many recent SSM-based or inspired models. Crucially, it does not contain classical RNNs, LSTMs, and GRUs - for which results are known and rely on the non-linear dependence of the hidden state in the update rule (Sec. 2.2). As we will shortly see, structure and features of (selective) SSMs can be studied within a unified, convenient, continuous-time framework (Linear Controlled Differential Equations). This allows to answer the following question:

"_What is the most one can achieve using a recurrence which is linear in the hidden state and potentially non-linear in the input?_"

### Linear CDEs

According to their continuous-time formulation, SSMs process input data sampled from a continuous path \(X:^{d}\), where \(d\) is the number of channels. By \(X^{i}_{t}\) we denote the input channel \(i\), evaluated at time \(t\). More formally, we consider input trajectories in the separable Banach space \(=C_{1,0}(;^{d})\) of absolutely continuous \(^{d}\) dimensional paths. In this space, one can write \(X=_{Q}^{t}_{s}\;ds\) since \(X L^{1}(;^{d})\); and the norm is \(\|X\|_{1;}:=_{0}^{1}|_{s}|\;ds\).

To model gates (see Mamba in (4)), we introduce two maps transforming the path \(X\), which output trajectories \(^{}\), \(^{}\) living in potentially higher dimensions: \(d_{}\) and \(d_{}\)

\[: C_{1,0}(;^{d_{}}),:  C_{1,0}(;^{d_{}})\]

where we used the shorthand notation \(^{}:=(X),^{}=(X)\). Akin the notation for the \(i\)-the channel of \(X\) (i.e. \(X^{i}\)), we denote by \(^{,i}\) the \(i\)-th channel in \(^{}\).

The role of the \(,\) functions - which we denote _gating functions_ - will be clear very soon.

**Definition 3.1** (Linear CDE).: Fix \(N\) (hidden-state dimension), matrices \(A_{1},...,A_{d_{}}^{N N}\) and \(B^{N d_{}}\). The hidden state \(Z^{}:=Z(X)\) is computed by the Linear CDE through a linear (in the hidden state) differential equation driven by \(,\) - functions of the input path:

\[dZ^{}_{t}=_{i=1}^{d_{}}A_{i}Z^{}_{t}d^{ ,i}_{t}+Bd^{}_{t}, Z^{}_{0}=Z_{0} ^{N}\] (5)

We show that both S4 and Mamba can be written in continuous-time as systems of parallel Linear CDEs. The key ingredient setting the two models apart is the choice of drivers \(\) and \(\). As in the preceding section, we will use the bold tensor notation to stress the parallel nature of these CDEs.

* this will have a crucial role in expressivity (see Sec. 4.2).
* **Mamba is a Linear CDE:** Recall from (4) that the recurrence inside Mamba (i.e. S6), can be written as \(_{i;}=_{i}(x_{})_{i;-1}+_{ i}(x_{})x^{i}_{}\) where for generic timestamp features \(x^{d}\), we have \(_{i}(x)=(_{i}(x)A_{i})\), \(_{i}(x)(B x)_{i}(x)\) and \(_{i}(x)=(_{i} x+_{i})\). Let us introduce a parameter \(>0\), and consider \(_{i}=_{i}/,_{i}=_{i}/\). Let us further approximate the softplus function with a ReLU (\((x)=(x)(1+e^{x})\)) to obtain \(_{i}(x)=(_{i} x+_{i})= (_{i} x+_{i})\). Therefore, as \( 0\), one has \(_{i}(x)=(_{i}(x)A_{i})=((_{i} x +_{i}) A_{i})}{{}}1+ (_{i} x+_{i}) A_{i}\), leading to the recurrence \[_{i;}=_{i;-1}+A_{i}_{i;-1}\; (_{i} x_{}+_{i})+(B x_{ })\;x^{i}_{}(_{i} x_{}+_{i} ).\] As we show formally in Appendix F, \(\) plays the role of the differential \(dt\). The equation above is the Euler discretization of the differential equation \[^{N} d^{}_{i;t}=A_{i}^{ }_{i;t}\;(_{i} X_{t}+_{i})dt+ (B X_{t})\;X^{i}_{t}(_{i} X_{t}+_ {i})dt\] where for each \(i\), \(^{}_{i}:^{N}\). These are Linear CDEs with carefully chosen \(\) and \(\): \[^{}_{i;t}=_{0}^{t}(_{i}  X_{s}+_{i})ds,^{ }_{i;t}=_{0}^{t}X_{s}\;X^{i}_{s}(_{i} X _{s}+_{i})ds^{d}.\] Note that here the \(_{i}\) depend on higher powers of \(X\)'s dimensions, an intriguing feature connected to input gating [Hochreiter and Schmidhuber, 1997, Cho et al., 2014].

_Remark 3.2_ (Are hidden state components recurrently mixed?).: In the Linear CDE defined by \(dZ^{}_{t}=[_{j=1}^{d_{}}A_{j}d^{,j}_{t}]  Z^{}_{t}+B d^{}_{t}\) channels of the transformed input path \(^{}\) are mixed in a linear way in the recurrent step and stored in a shared hidden state \(Z^{}\). This allows our framework to be more general compared to S4 and Mamba, where each channel of the input is processed individually, and _hidden states are later combined_. We know already from Merrill et al.  that this distinction between hidden state mixing strategies is crucial for expressivity. Our discussion in Sec. 4.3 provides an in-depth look at the effects of separate channel processing, achieved in our framework by choosing the \(A_{i}\)s to be diagonal and non-zero only around a channel-specific portion.

## 4 Expressivity of Linear CDEs

Having established the connection between SSMs and Linear CDEs, we now provide an explicit characterization of the _uniform closure_ of Linear CDEs, i.e. a description of all the functions from compact subsets of \(\) to \(\) that can be uniformly approximated at an arbitrary precision by a Linear CDE of the form given in (5).

### Characterization of the closure

The proof of the main theorem we present here (Thm. 4.1) is involved and requires tools from Rough Path theory; we provide a full derivation in Appendix B with tools reviewed in the self contained Appendix E, as well as an introduction to the Signature approach in Section 4.5 and Appendix A. While familiarity with these concepts is not necessary to grasp the main results as stated, it is essential for a thorough comprehension.

In this subsection, Linear CDEs are analyzed in full generality - i.e., in the dense setting. While for efficiency reasons non-diagonal recurrences are rarely used in SSMs, our results allow to precisely characterize the Linear CDE hypothesis class (i.e. the _closure_), thus to the answer the question _"What is the most we can achieve from recurrences which are linear in the hidden state?"_. We show that Linear CDEs can model, at fixed time \(t\), _arbitrary continuous functions_ of the _whole_ seen inputs.

Of course, understanding the diagonal setting with separate channel processing is of utmost importance in the current research landscape. The tools and results in this subsection allow us to directly discuss this case and compare it to the dense setting - this is presented in Sec. 4.3.

In this section, for any path \( C_{1,0}(,^{d_{}})\) and any sub-interval \([s,t]\), we denote by \(_{[s,t]} C_{1,0}(,^{d_{}})\) the path \(_{[s,t]}(u)=_{t u}-_{s u}\), where \(a b=(a,b)\).

**Theorem 4.1**.: _Let \( C_{1,0}(,^{d})\) be compact and choose continuous gating functions \(,\) such that 5\(_{t}^{X,1}=t\) and \(_{t}^{X,2}=t^{2}\). Consider the Linear CDE model (5). Let \(\), \(\) be generic continuous functions from paths to real vectors:_

\[:C_{1,0}(,^{d_{}}),:C_{1,0}(,^{d_{}})^{d_{}}.\]

_There exist **dense** matrices \(A_{1},...,A_{d_{}},B\) such that, after a fixed final linear projection \(C^{1 N}\), the output \(Y_{t}^{X}=CZ_{t}^{X}\) is arbitrarily close, uniformly on \(\), to_

\[(_{[0,t]}^{X})+_{0}^{t}(_{[s,t]}^{X}) d_{s}^{X}\] (7)

_where \(\) is the scalar product. Moreover \(Y_{t}^{X}=CZ_{t}^{X}\) is itself of form (7)._

In Theorem 4.1, the \(A_{i}\) are _dense_ matrices constructed _ad hoc_ for the proof. We show next that, with high probability, _random_ Glorot-initialized matrices (LeCun et al., 2012) provide enough expressivity - one only has to choose the appropriate matrix \(C\). This is a similar mechanism to the paradigm advocated in reservoir computing (Lukoveviius and Jaeger, 2009; Cucchiero et al., 2021; Compagnoni et al., 2023). The next result, however, is novel and of independent interest in Rough Path Theory.

**Theorem 4.2**.: _Under the hypothesis of Theorem 4.1, pick \([A_{j}]_{n,n^{}}(0,)\) and \([Z_{0}]_{n},[B]_{n,j}(0,1)\). For any functional \(F:\) of the form (7) and \(>0\) it holds that_

\[_{N} C^{1 N} _{(X,t)}|F(X,t)-CZ_{t}^{X}|} =1.\]

### Intuition on the closure result: role of gates

Roughly speaking, our result shows that dense Linear CDEs have drastically superior expressive power compared to dense linear RNNs. This contrast is to be attributed completely to the gate \(\).

Warmup - Linear RNNs.Thm 4.1 can be seen as a generalization of the _Universal Approximation for Linear RNNs_ presented by Li et al. (2022b)[Thm. 7] for generic gates \(,\). In fact, their setting is restricted to \(_{t}^{X}=t\) and \(_{t}^{X}=_{0}^{t}X_{s}ds\). This is also the case for S4, S5 and the LRU - which are linear RNNs at test time. Then the only information contained in \(_{[s,t]}\) is the increment \(t-s\) so that family (7) reduces to \((X,t)(t)+_{0}^{t}(t-s) X_{s}ds}\). This is, fundamentally, the set of linear filters on the input. As shown in Gu and Dao (2023), such processing is unable to adapt information flow in-context.

How does a Linear CDE process inputs?Take without loss in generality6\(_{t}^{}=X_{t}\). The first term in the function class \(\{(X_{[0,t]})+_{0}^{t}(X_{[s,t]}) d_{s}^{}\}\) is already enough to establish that the output \(CZ_{t}^{}\) is a nonlinear function of all previously seen inputs \(X_{[0,t]}-\) could be set to zero. The term \((X_{[0,t]})\) is however non-trivial only in the \(Z_{0} 0\) case (_cf_. Appendix B): picking \(_{t}^{}=0^{N}\) for all \(t\) indeed leads to \(dZ_{t}^{}=[_{i=1}^{d_{}}A_{i}d_{t}^{X,i}]Z_{ t}^{}\), which is evolving only if \(Z_{0} 0\). While this setting is interesting and suggests a clear direction for future research, a case more similar to Mamba (see Sec. 4.3) is \(Z_{0}=0\) and \(_{t}^{}=_{0}^{t}X_{s}ds\). Here, we can approximate arbitrarily well outputs of the form

\[\{(X,t)_{0}^{t}(X_{[s,t]}) X_{s}ds\}\] (8)

where \(\) is any continuous function of the input path, restricted to the portion \([s,t]\). This clearly shows that dense Linear CDEs are capable of _context-dependent filtering_: the output is again a linear combination of previously seen inputs, but weights are not predetermined as in Linear RNNs (S4) - they are a function of the context. A similar yet less powerful processing is happening in the diagonal setting, which we explore next.

### The Diagonal Case

The choice of diagonal weights considerably restricts the family of learnable functionals. Intuitively the diagonal choice corresponds to running \(N\) independent \(1\)-dimensional systems, this absence of mixing between the different hidden dimensions is the main culprit for the loss of expressivity. The full power can however be recovered by _chaining_ the diagonal schemes (_cf_. Prop. 4.5).

**Theorem 4.3** (Diagonal Case).: _If the matrices \(A_{1},...,A_{d_{}}\) are constrained to be diagonal, the requirements \(_{t}^{,1}=t\), \(_{t}^{,2}=t^{2}\) can be dropped and the closure reduces to_

\[\{(X,t)(_{t}^{})+_{0}^{t}(_{t}^{ }-_{s}^{}) d_{s}^{}\}\] (9)

_for continuous \(:^{d_{}}\) and \(:^{d_{}}^{d_{}}\)._

Compared to the dense setting, the effect of diagonality is pretty clear. Let us again assume \(_{t}^{}=X_{t}\) and \(_{t}^{}=_{0}^{t}X_{s}\ ds\) for simplicity. While \(_{0}^{t}(X_{[s,t]}) X_{s}\ ds\) unlocks filtering based on the entire trajectory \(X_{[s,t]}\), the diagonal case term \(_{0}^{t}(X_{t}-X_{s}) X_{s}\ ds\) indicates that filtering coefficients can only be chosen by comparing two elements of the (potentially transformed through \(\)) input sequence. While this precise and tight result reveals a pitfall of diagonal recurrence, it also brings about an interesting connection to attention (Vaswani et al., 2017), where only a finite number of tokens are compared at each layer. While a smart choice of gating functions \(,\) can improve on the learned nonlinear filtering strategy (e.g. based on filtered input versions as in Mamba), our theory reveals the fundamental processing discrepancy compared to the dense setting, a property which was also explored in recent literature (Merrill et al., 2024) using different tools.

As already noted, on top of diagonality, recent SSMs also mix inputs as linear combinations of independently run channel-dependent systems. This slightly modifies the function class as:

**Corollary 4.4** (Mamba Case).: _In the Mamba setting, the closure reduces to_

\[\{(X,t)_{i=1}^{d_{}}_{i}(_{t}^{,i})+ _{i=1}^{d_{}}_{0}^{t}_{i}(_{t}^{,i}-_{s} ^{,i})\ d_{s}^{,i}\}\] (10)

_for continuous \(_{i}:\) and \(_{i}:\)._

### Chaining Diagonal CDEs

Fortunately it is possible to re-gain expressivity without sacrificing the computational advantages of diagonal schemes through _chaining_. This means driving a new Linear CDE by the solution of a previous Linear CDE, and repeating this procedure \(K\) times (_cf_. Appendix C).

**Proposition 4.5**.: _Assume a compact \( C_{1,0}(;^{d})\). For any functional \(F:\) of the form (7) and \(>0\) there exists a sequence of linear maps \(W_{k}^{M_{k} N_{k}}\), diagonal weights \(A_{i}^{(k)}\) and \(B^{(k)}\) for the following family of chained diagonal Linear CDEs_

\[Z_{t}^{0,X} 0, dZ_{t}^{k+1,X}=_{i=1}^{d+M_{k}}A_{i}^{(k+1)}Z_{t}^ {k+1,X}dW_{k}Z^{k,X}\\ X_{t}^{i}+B^{(k+1)}dX_{t}^{N_{k+1}},\] (11)

_such that eventually, as \(k\), there exists \(C_{k}^{1 N_{k}}\) with \(_{(X,t)}|F(X,t)-C_{k}Z_{t}^{k,X}|\)._

Intuitively, with chaining one recovers the mixing between the input dimensions which was so important for the expressiveness of _dense_ Linear CDEs. This does not happen immediately but crucially depends on the length of the chain, the result in fact tells us that the recovery holds for _long enough_ chains (and big enough hidden states). In Appendix C.2.1 we argue that the same conclusions hold also in the Mamba setting with non-linear gates.

### Proof Idea - Signature expansion

In this section, we introduce the primary tools, objects, and techniques from Rough Path theory used in our proofs. Given their technical nature, we have chosen to present the main results of this paper without explicit reference to them, allowing readers to understand the work without needing specialized knowledge. For those interested in the finer details, here we provide a brief overview and refer to Appendix A for additional details and references.

To study the expressivity of Linear CDEs it is convenient to introduce the so-called _signature transform_(Lyons et al., 2007; Kidger et al., 2019; Fermanian et al., 2023), a classical path-transform from stochastic analysis. The main reason for doing so is that, as a simple consequence of the Stone-Weirestrass theorem, linear functionals on the signature provide the essential building blocks (analogous to monomials on Euclidean spaces) to approximate continuous functions on path space.

Consider a path \( C_{1,0}(;^{d_{}})\) and define as \(_{d_{}}\) the set of _words_ (_i.e._ ordered sequences) in \(\{1,,d_{}\}\)7. The signature transform is the following infinite collection of scalar iterated integrals

\[()_{s,t}:=(()_{s,t}^{(I)})_{I _{d_{}}},()_{s,t}^{(I)}:=_{s<u_{1}< <u_{n}<t}_{u_{1}}^{(i_{1})}..._{u_{n}}^{(i_{n})} du_{1}...du_{n}.\]

A classical result from rough path theory states that a Linear CDE can be expanded explicitly as an (infinite) linear combination of terms in the signature of the driving path.

**Proposition 4.6**.: _For any choice of matrices \(A_{1},...,A_{d_{}}\) and \(B\), the unique solution Linear CDE (5) is, using the notation \(A_{I}:=A_{i_{m}}...A_{i_{1}}\), given by_

\[Z_{t}^{X}=_{I_{d_{}}}A_{I}Z_{0}\;(^{X}) _{0,t}^{(I)}+_{i=1}^{d_{}}_{I_{d_{}}}A_{I}B_{i} \;_{0}^{t}(^{X})_{s,t}^{(I)}d_{s}^{X,i}^{ N}.\] (12)

Notice that the previous result _does not_ rely on any assumptions on the nature of \(Z_{0}\), \(A_{i}\), and \(B\); for any such choice the result is a _time-independent linear map_ on a _feature vector_\(T(X)_{0,t}:=((^{X})_{0,t}^{(I)},_{0}^{t}(^{ X})_{s,t}^{(I)}d_{s}^{X,i})_{(I,i)}\), where the index \((I,i)\) runs over \(_{d_{}}\{1,...,d_{}\}\).

The main takeaway is that any linear projection \(Y_{t}^{X}:=CZ_{t}^{X}\) is written as an (infinite) linear combination of the terms in \(T(X)_{0,t}\). This means that the expressive power of such schemes is almost _completely determined_ by the gate \(\), which is the only path of which high order information is taken into consideration through its full _signature_. It is evident then that the classical choice of input-independent \(\) (i.e. \(_{t}^{X}=t\)) then _precludes_ the use of higher order statistics of \(X\).

## 5 Path-to-Path Learning

In Section 4, we showed that Linear CDEs (and chained Mamba) can model, _at fixed time_\(t\), arbitrary continuous functions of the whole seen inputs. Assume wanting to learn a functional of type\((X,t)_{t}(^{}_{[0,t]})\) where the map \(_{t}\) is changing with time as well, this is an example of a general continuous path-to-path model. Note in particular how this is a more general family than the one of Theorem 4.1, where the maps \(\) and \(\) are fixed once and for all. Here, we discuss how passing the hidden state through a multi-layer perceptron (MLP), instead of just a linear readout (matrix \(C\)), allows us to efficiently approximate this richer class, interpolating between the \(_{t}\)s.

As shown in Orvieto et al. (2023), classical SSMs followed by an MLP are universal on sequences. Given their nature of input-independent convolutions, their construction defers all reasoning to the MLP acting on the output: in S4, the SSM is simply providing an input compression - with no added reasoning. In our setting, we instead characterized the processing power of input-controlled (dense or diagonal) SSMs precisely, showing how it greatly surpasses linear filtering. For this reason, _the computational burden for an MLP action on a general Linear CDE would be greatly diminished_ and its actual function reduced to an interpolation of the maps \(_{t}\). We defer the proof to Appendix D.

**Proposition 5.1**.: _Fix a compact set \(\) and continuous \(,\) with \(^{,1}_{t} t\). Then for all \(>0\) and all causal 8 continuous mapping \(G:C_{1,0}(,^{d_{}})\) there exist an integer \(N 0\), some MLP \(F:^{N}\), and parameters \(Z_{0}^{N},A_{i}^{N N},B^{N d _{}}\) such that_

\[_{(X,t)}|F(Z^{}_{t})-G(^{},t)|<.\] (13)

## 6 Empirical Validation

Code to reproduce all of our experiments can be found at:

https://github.com/Benjamin-Walker/selective-ssms-and-linear-codes

Datasets.The first task is based on a dataset from Walker et al. (2024) where the aim is to predict terms in the anti-symmetric part of the input path's signature. The dataset's objective aligns with the proofs of Theorem 4.1 and 4.2, which characterise the closure using the path's signature. We created two datasets with dimensions \(2\) and \(3\) respectively. The increment in each channel at each step is an integer-rounded sample from a standard Normal distribution. The 2D dataset's target is an area integral \(_{0}^{1}_{0}^{v}dX^{1}_{u}dX^{2}_{v}\), and the 3D dataset's target is a volume integral \(_{0}^{1}_{0}^{w}_{0}^{v}dX^{1}_{u}dX^{2}_{v}dX^{3}_{w}\).

The second task is the \(A_{5}\) benchmark from Merrill et al. (2024). It tests models on state-tracking, a crucial ability for tasks involving permutation composition, such as chess. The dataset comprises sequences from the group of even permutations on five elements, \(A_{5}\), where the target is the cumulative composition of all preceding permutations. Datasets vary by sequence length, ranging from \(3\) to \(20\).

Models.On the anti-symmetric signature task, we considered seven models: (i-ii) S4 or Mamba recurrence with linear readout, (iii-iv) two stacked S4 or Mamba recurrences with a linear mixing layer in-between and a linear readout, (v-vi) two stacked S4 or Mamba recurrences with a linear mixing layer + GeLU in-between, and a linear readout, (vii) a linear CDE with gates \(^{}_{t}=^{}_{t}=(t,X_{t})\) and a linear readout. All state space models have **trainable** matrices in their recurrences, whereas the linear CDE is using **fixed** matrices. All models use a hidden dimension of \(256\), with the state space models using a state dimension of \(256\). The state space models are trained using gradient descent with a batch size of \(32\) and Adam with a learning rate of \(10^{-4}\). The output from the linear CDE's recurrence is obtained using the Tsit5 adaptive ODE solver, with an absolute and relative tolerance of \(10^{-2}\). The linear CDEs linear readout is optimised via ordinary least squares.

On the \(A_{5}\) benchmark, we consider five models: a linear CDE, a RNN, a transformer, and S4 and Mamba recurrences. All models use an embedding layer followed by a series of blocks that combine the sequence-to-sequence model, linear mixing with a non-linear activation function, layer normalization, and a residual connection. Furthermore, all models have **trainable** matrices in their recurrences and are trained using batch gradient descent with a batch size of \(32\) and AdamW with a weight decay of \(0.01\). The RNN, transformer, S4, and Mamba use a hidden dimension of \(1024\), with the state space models using a state dimension of \(64\) and the transformer using \(64\) heads. Due to memory constraints, the linear CDE uses a hidden dimension of \(256\). Note that the IDS4 recurrence introduced by Merrill et al. (2024) corresponds directly to a linear CDE with dense transition matrices, hence we did not include the model as a baseline.

Results.Results on the anti-symmetric signature prediction task (Fig. 1) empirically demonstrate a number of the theoretical results presented in this paper. Firstly, as discussed in Sec. 4.2, recurrences which are linear in the input, such as S4, require a non-linearity in-between the layers to perform well. Furthermore, as stated in Thm. 4.3 and Prop. 4.5, even if the recurrence is non-linear in the input, such as Mamba, the expressivity of models with diagonal matrices is improved by stacking. Additionally, the inclusion of the non-linearity in-between the Mamba layers does not improve performance, as the recurrences themselves are expressive enough. Finally, as stated in Thm. 4.2, dense matrices can achieve strong expressivity with random initialization, no stacking, and only a trainable linear readout.

Fig. 2 is a plot of the results on the \(A_{5}\) benchmark. The figure shows that the number of blocks S4, Mamba, and the transformer require to achieve greater than \(90\%\) validation accuracy grows with the sequence length. In fact, given our model architecture, none of these models could achieve greater than \(90\%\) validation accuracy for sequences of length twenty9. On the other hand, the RNN and Linear CDE are able to achieve greater than \(90\%\) validation accuracy for all lengths considered using only one block. These empirical results further validate Thm. 4.3 and Prop. 4.5: there exists a gap in expressivity between diagonal and dense transition matrices, and stacking is required to recover the expressivity. Furthermore, they provide empirical evidence that even for simple state-tracking problems, the number of blocks required can grow quickly with sequence length.

## 7 Conclusions

This paper explores Linear CDEs, a model family that extends both classical and modern SSM architectures, including recent gated RNNs. Using Rough Paths theory, we have characterized their uniform closure, generalizing the results of Li et al. (2022b) for linear RNNs. We precisely identified the advantages of input-controlled transition dynamics, which allow you to capture high-order statistics of the input as opposed to just the linear ones extracted by convolutions. While dense models reach full expressiveness, imposing diagonality (e.g. Mamba) weakens the model capabilities. This is in direct contrast to S4, where dense and diagonal settings share the same closure. Our analysis lays the theoretical foundation for analyzing the expressive power of future SSM variants and hints at non-diagonality as a potential source of improvements. We believe that light and efficient channel mixing in the recurrent block might already unlock the modeling of higher-order statistics.

**Limitations.** The current framework examines expressivity from a continuous-time perspective with real-valued inputs. The RNN literature suggests that finite precision often plays a significant role in practice, making it an interesting direction for future exploration. Additionally, although dense transition matrices are shown to be theoretically and empirically more expressive than diagonal ones, their increased computational cost makes them impractical for large-scale models.

Figure 1: Comparison of the Linear CDE, Mamba, and S5 on the anti-symmetric signature prediction tasks. For each model, we plotted the mean and range of the validation accuracy over 5 independent runs.

Figure 2: For each sequence length, the plot shows the minimum number of blocks required to achieve at least \(90\%\) validation accuracy, with each grey band corresponding to a number of blocks. Missing points mean the model did not achieve at least \(90\%\) validation accuracy with \(4\) blocks or less.

[MISSING_PAGE_FAIL:11]

* Arora et al. (2023) Simran Arora, Sabri Eyuboglu, Aman Tamalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher Re. Zoology: Measuring and improving recall in efficient language models. _arXiv preprint arXiv:2312.04927_, 2023.
* Gu & Dao (2023) Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023.
* Peng et al. (2023) Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. _arXiv preprint arXiv:2305.13048_, 2023.
* Sun et al. (2023) Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models, 2023.
* Katsch (2023) Tobias Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling, 2023.
* Yang et al. (2023) Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. _arXiv preprint arXiv:2312.06635_, 2023.
* Qin et al. (2024) Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. _arXiv preprint arXiv:2404.07904_, 2024.
* De et al. (2024) Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024.
* Orvieto et al. (2023a) Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. _arXiv preprint arXiv:2303.06349_, 2023a.
* Feng et al. (2024) Leo Feng, Frederick Tung, Mohamed Osama Ahmed, Yoshua Bengio, and Hossein Hajimirsadegh. Were rnns all we needed?, 2024. URL https://arxiv.org/abs/2410.01201.
* Young (1936) Laurence C Young. An inequality of the holder type, connected with stieltjes integration. 1936.
* Lyons (1994) Terry Lyons. Differential equations driven by rough signals (i): An extension of an inequality of lc young. _Mathematical Research Letters_, 1(4):451-464, 1994.
* Kidger et al. (2020) Patrick Kidger, James Morrill, James Foster, and Terry Lyons. Neural controlled differential equations for irregular time series. _Advances in Neural Information Processing Systems_, 33:6696-6707, 2020.
* Morrill et al. (2021) James Morrill, Cristopher Salvi, Patrick Kidger, and James Foster. Neural rough differential equations for long time series. In _International Conference on Machine Learning_, pages 7829-7838. PMLR, 2021.
* Fermanian et al. (2021) Adeline Fermanian, Pierre Marion, Jean-Philippe Vert, and Gerard Biau. Framing rnn as a kernel method: A neural ode approach. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 3121-3134. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/18a9042b3fc5b02fe3d57fea87d6992f-Paper.pdf.
* Salvi et al. (2022) Cristopher Salvi, Maud Lemercier, and Andris Gerasimovics. Neural stochastic pdes: Resolution-invariant learning of continuous spatiotemporal dynamics. _Advances in Neural Information Processing Systems_, 35:1333-1344, 2022.
* Hoglund et al. (2023) Melker Hoglund, Emilio Ferrucci, Camilo Hernandez, Aitor Muguruza Gonzalez, Cristopher Salvi, Leandro Sanchez-Betancourt, and Yufei Zhang. A neural rde approach for continuous-time non-markovian stochastic control problems. _arXiv preprint arXiv:2306.14258_, 2023.
* Walker et al. (2024) Benjamin Walker, Andrew D. McLeod, Tiexin Qin, Yichuan Cheng, Haoliang Li, and Terry Lyons. Log neural controlled differential equations: The lie brackets make a difference. _International Conference on Machine Learning_, 2024.
* Wojcicki et al. (2020)Terry J Lyons, Michael Caruana, and Thierry Levy. _Differential equations driven by rough paths_. Springer, 2007.
* Li et al. [2022b] Zhong Li, Jiequn Han, E Weinan, and Qianxiao Li. Approximation and optimization theory for linear continuous-time recurrent neural networks. _J. Mach. Learn. Res._, 23:42-1, 2022b.
* Orvieto et al. [2023b] Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, and Samuel L Smith. On the universality of linear recurrences followed by nonlinear projections. _arXiv preprint arXiv:2307.11888_, 2023b.
* Wang and Xue  Shida Wang and Beichen Xue. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory. _arXiv preprint arXiv:2309.13414_, 2023.
* Gu et al.  Albert Gu, Ankit Gupta, Karan Goel, and Christopher Re. On the parameterization and initialization of diagonal state space models. _arXiv preprint arXiv:2206.11893_, 2022.
* Zucchet and Orvieto  Nicolas Zucchet and Antonio Orvieto. Recurrent neural networks: vanishing and exploding gradients are not the end of the story. _arXiv preprint arXiv:2405.21064_, 2024.
* Gu et al.  Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. Hippo: Recurrent memory with optimal polynomial projections. _Advances in neural information processing systems_, 33:1474-1487, 2020.
* Siegelmann and Sontag  Hava T Siegelmann and Eduardo D Sontag. On the computational power of neural nets. In _Proceedings of the fifth annual workshop on Computational learning theory_, pages 440-449, 1992.
* Korsky and Berwick  Samuel A Korsky and Robert C Berwick. On the computational power of rnns. _arXiv preprint arXiv:1906.06349_, 2019.
* Hanson and Raginsky  Joshua Hanson and Maxim Raginsky. Universal simulation of stable dynamical systems by recurrent neural nets. In _Learning for Dynamics and Control_, pages 384-392. PMLR, 2020.
* Barron  Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. _IEEE Transactions on Information theory_, 39(3):930-945, 1993.
* Tallec and Ollivier  Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time?, 2018.
* Hanson and Raginsky  Joshua Hanson and Maxim Raginsky. Universal approximation of input-output maps by temporal convolutional nets. _Advances in Neural Information Processing Systems_, 32, 2019.
* Boyd and Chua  Stephen Boyd and Leon Chua. Fading memory and the problem of approximating nonlinear operators with volterra series. _IEEE Transactions on circuits and systems_, 32(11):1150-1161, 1985.
* Jelassi et al.  Samy Jelassi, David Brandfonbrener, Sham M Kakade, and Eran Malach. Repeat after me: Transformers are better than state space models at copying. _arXiv preprint arXiv:2402.01032_, 2024.
* Merrill et al.  William Merrill, Jackson Petty, and Ashish Sabharwal. The illusion of state in state-space models. _arXiv preprint arXiv:2404.08819_, 2024.
* Hambly and Lyons  Ben Hambly and Terry Lyons. Uniqueness for the signature of a path of bounded variation and the reduced path group. _Annals of Mathematics_, pages 109-167, 2010.
* LeCun et al.  Yann A. LeCun, Leon Bottou, Genevieve B. Orr, and Klaus-Robert Muller. _Efficient BackProp_, pages 9-48. Springer Berlin Heidelberg, Berlin, Heidelberg, 2012. ISBN 978-3-642-35289-8. doi: 10.1007/978-3-642-35289-8_3. URL https://doi.org/10.1007/978-3-642-35289-8_3.
* Lukoveviius and Jaeger  Mantas Lukoveviius and Herbert Jaeger. Reservoir computing approaches to recurrent neural network training. _Comput. Sci. Rev._, 3:127-149, 2009. URL https://api.semanticscholar.org/Co rpusID:554006.
* Cuchiero et al. [2021a] Christa Cuchiero, Lukas Gonon, Lyudmila Grigoryeva, Juan-Pablo Ortega, and Josef Teichmann. Discrete-time signatures and randomness in reservoir computing. _IEEE Transactions on Neural Networks and Learning Systems_, Forthcoming, 04 2021a. doi: 10.1109/TNNLS.2021.3076777.
* Cichich et al. * Compagnoni et al.  Enea Monzio Compagnoni, Anna Scampicchio, Luca Biggio, Antonio Orvieto, Thomas Hofmann, and Josef Teichmann. On the effectiveness of randomized signatures as reservoir for learning rough dynamics. In _2023 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8. IEEE, 2023.
* Kidger et al.  Patrick Kidger, Patric Bonnier, Imanol Perez Arribas, Cristopher Salvi, and Terry Lyons. Deep signature transforms. _Advances in Neural Information Processing Systems_, 32, 2019.
* Fermanian et al.  Adeline Fermanian, Terry Lyons, James Morrill, and Cristopher Salvi. New directions in the applications of rough path theory. _IEEE BITS the Information Theory Magazine_, 2023.
* Cass and Salvi  Thomas Cass and Cristopher Salvi. Lecture notes on rough paths and applications to machine learning. _arXiv preprint arXiv:2404.06583_, 2024.
* Friz and Victoir  Peter K. Friz and Nicolas B. Victoir. _Multidimensional Stochastic Processes as Rough Paths: Theory and Applications_. Cambridge Studies in Advanced Mathematics. Cambridge University Press, 2010. doi: 10.1017/CBO9780511845079.
* Fermanian  Adeline Fermanian. Embedding and learning with signatures, 2020.
* Chen  Kuo-Tsai Chen. Integration of paths-a faithful representation of paths by noncommutative formal power series. _Transactions of the American Mathematical Society_, 89(2):395-407, 1958. ISSN 00029947. URL http://www.jstor.org/stable/1993193.
* Salvi et al. [2021a] Cristopher Salvi, Thomas Cass, James Foster, Terry Lyons, and Weixin Yang. The signature kernel is the solution of a goursat pde. _SIAM Journal on Mathematics of Data Science_, 3(3):873-899, 2021a. doi: 10.1137/20M1366794. URL https://doi.org/10.1137/20M1366794.
* Berlinet and Thomas-Agnan  A. Berlinet and C. Thomas-Agnan. _Reproducing Kernel Hilbert Spaces in Probability and Statistics_. Springer US, 2011. ISBN 9781441990969. URL https://books.google.co.uk/books?id=bX3TBwAAQBAJ.
* Lemercier et al.  Maud Lemercier, Cristopher Salvi, Theodoros Damoulas, Edwin V. Bonilla, and Terry Lyons. Distribution regression for sequential data, 2021.
* Salvi et al. [2021b] Cristopher Salvi, Maud Lemercier, Chong Liu, Blanka Horvath, Theodoros Damoulas, and Terry Lyons. Higher order kernel mean embeddings to capture filtrations of stochastic processes. _Advances in Neural Information Processing Systems_, 34:16635-16647, 2021b.
* Cochrane et al.  Thomas Cochrane, Peter Foster, Varun Chhabra, Maud Lemercier, Terry Lyons, and Cristopher Salvi. Sk-tree: a systematic malware detection algorithm on streaming trees via the signature kernel. In _2021 IEEE international conference on cyber security and resilience (CSR)_, pages 35-40. IEEE, 2021.
* Salvi et al. [2021c] Cristopher Salvi, Maud Lemercier, Thomas Cass, Edwin V Bonilla, Theodoros Damoulas, and Terry J Lyons. Siggdep: Scaling sparse gaussian processes on sequential data. In _International Conference on Machine Learning_, pages 6233-6242. PMLR, 2021c.
* Cirone et al.  Nicola Muca Cirone, Maud Lemercier, and Cristopher Salvi. Neural signature kernels as infinite-width-depth-limits of controlled resnets, 2023.
* Issa et al.  Zacharia Issa, Blanka Horvath, Maud Lemercier, and Cristopher Salvi. Non-adversarial training of neural sdes with signature kernel scores. _Advances in Neural Information Processing Systems_, 2023.
* Pannier and Salvi  Alexandre Pannier and Cristopher Salvi. A path-dependent pde solver based on signature kernels. _arXiv preprint arXiv:2403.11738_, 2024.
* Manten et al.  Georg Manten, Cecilia Casolo, Emilio Ferrucci, Soren Wengel Mogensen, Cristopher Salvi, and Niki Kilbertus. Signature kernel conditional independence tests in causal discovery for stochastic processes. _arXiv preprint arXiv:2402.18477_, 2024.
* Kidger  Patrick Kidger. On neural differential equations, 2022.
* Kessler et al. Guillaume Dubach and Yuval Peled. On words of non-Hermitian random matrices. _The Annals of Probability_, 49(4):1886 - 1916, 2021. doi: 10.1214/20-AOP1496. URL https://doi.org/10.1214/20-AOP1496.
* Dasgupta and Gupta (2003) Sanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of johnson and lindenstrauss. _Random Structures & Algorithms_, 22, 2003. URL https://api.semanticscholar.org/CorpusID:10327785.
* Cuchiero et al. (2021b) Christa Cuchiero, Lukas Gonon, Lyudmila Grigoryeva, Juan-Pablo Ortega, and Josef Teichmann. Expressive power of randomized signature. In _The Symbiosis of Deep Learning and Differential Equations_, 2021b.

Introduction to Signatures

This initial section of the Appendix is devoted to a brief introduction to the topic of Signature Transform. For a more in-depth account we refer the interested reader to Cass and Salvi (2024).

### Intuition - Controlled Differential Equations

In the simplest setting of smooth paths a CDE is a differential equation of form

\[}{dt}=F}{dt},Z_{t}, Z_{0}^{n}\]

where \(X:^{d}\) is a known smooth path to which we refer as _control_, \(Z_{0}\) the known initial condition and \(Z:^{n}\) the unknown solution.

The natural generalization is the following: assume to have two spaces \(^{d_{x}}\) and \(^{d_{x}}\), \(X C_{1}(;^{d_{x}})\), \(Z C_{1}(;^{d_{x}})\), \(F:^{d_{x}}(^{d_{x}},^{d_{x}})\) and \(Z_{0}^{d_{x}}\). We say that \((Z,X,F,Z_{0})\) satisfy the CDE

\[dZ_{t}=F(Z_{t})dX_{t}, Z_{0}^{d_{x}}\]

whenever

\[Z_{t}=Z_{0}+_{0}^{t}F(Z_{s})dX_{s}\]

The theory of _Rough Paths_ has its origins in the study of such types of differential equations and provides a theoretical framework to define and work in rough settings i.e. when \(X\) is not kust BV but even \(\)-Holder for \((0,1)\)_cf._ Friz and Victoir (2010).

Assume thus to have the CDE

\[dZ_{t}=_{i=1}^{d_{x}}V_{i}(Z_{t})dX_{t}^{i}, Z_{0}^{d_{ x}}\]

for sufficiently regular vector fields \(V_{i}\) and \(X C_{1}(;^{d_{x}})\). Given a smooth \(f:^{n}\), by the change of variable formula (_i.e._ fundamental theorem of calculus) we have

\[f(Z_{t})=f(Z_{0})+_{i=1}^{d_{x}}_{0}^{t}V_{i}f(Z_{s})dX_{s}^{i}\]

where \(V_{i}f(z):=df_{y}[V_{i}(z)]\). Iterating this procedure on the \(V_{i}f\)s, _i.e._ substituting in the previous equation the analogously obtained equality

\[V_{i}f(Z_{s})=V_{i}f(Z_{0})+_{j=1}^{d_{x}}_{0}^{s}V_{j}(V_{i}f)(Z_{u}) dX_{u}^{j},\]

we get

\[f(Z_{t})=f(Z_{0})+_{i=1}^{d}V_{i}f(Z_{0})_{0}^{t}dX_{s}^{i}+_{i,j= 1}^{d}_{0}^{t}_{0}^{s}V_{j}V_{i}f(Z_{u})dX_{u}^{j}dX_{s}^{i}\]

Keeping with this procedure for \(N\) steps we get

\[f(Z_{t})=f(Z_{0})+_{k=1}^{N}_{|I|=k}V_{I}f(Z_{0})< <u_{k}<t}{}dX_{u_{1}}^{i_{1}} dX_{u_{k}}^{i_{k}}+R_{ N}(t)\]

where \(I=(i_{1},,i_{k})\) runs through the multi-indices, \(V_{I}f:=V_{i_{1}}V_{i_{2}} V_{i_{k}}f\) and

\[R_{N}(t):=_{|J|=k+1}<<u_{k+1}<t}{}V _{J}f(Z_{u_{1}})dX_{u_{1}}^{j_{1}} dX_{u_{k+1}}^{j_{k+1}}\]

[MISSING_PAGE_EMPTY:17]

[MISSING_PAGE_EMPTY:18]

Proof.: The _if_ part is follows from \(()_{s,t}=(_{[s,t]})_{0,1}\). For the _only if_ part, If \(s=s^{}\) and \(t=t^{}\) the statement holds; this is because if the signatures over the time interval \([s,t]\) of two time-augmented paths are equal, then the two paths must be equal on \([s,t]\). We now show that augmenting the path with \(t^{2}\) and imposing equality of signatures, implies \(s=s^{}\) and \(t=t^{}\), which will in turn allow us to conclude the proof by the previous remark. Assume \(()_{s,t}=()_{s^{},t^{}}\), in particular we must have

\[_{s}^{t}d(r^{2})=t^{2}-s^{2}=(t^{})^{2}-(s^{})^{2} =_{s^{}}^{t^{}}d(r^{2})\] (21) \[_{s}^{t}d(r)=t-s=t^{}-s^{}=_{s^{}}^{t^ {}}d(r)\] (22)

which reduces to the system

\[t^{2}-s^{2}=(t^{})^{2}-(s^{})^{2}\\ t-s=t^{}-s^{}t+s=t^{}+s^{}\\ t-s=t^{}-s^{}2t=2t^{}\\ 2s=2s^{}\]

Hence it must be true that \(t=t^{}\) and \(s=s^{}\).

## Appendix B Expressivity

### Model Recap

In the body of the paper we have presented the main results with the simplified assumption of \(Z^{}_{0}=0\) or at best \(Z^{}_{0}=Z_{0}\)_i.e._ with an initial value independent from the input. In this appendix we will carry on the proofs in a more general setting in which \(Z^{}_{0}\) is allowed to be input-dependent, as previously discussed the choice of initial value is, in contrast to the classical setting, meaningful inasmuch it allows to approximate linear maps on the signature of \(^{}_{}\). In order to do so we have to introduce a new gate, the initial value gate, in the form of a map

\[()_{0}: ^{d_{0}}\] (23) \[X  X_{0}\]

Despite the notation, there is no reason why \((X)_{0}\) should be the initial value of the path \(X\), one should think of this map as the one summarizing the data which still matters for the task but which does not have a time-series nature.

To recapitulate, the general setting of our models is the following: a topological _input space_ space \(\),

\[()_{0}: ^{d_{0}},\] ( \[\] ) \[:  C_{1,0}(;^{d_{}}),\] ( \[\] -gate ) \[:  C_{1,0}(;^{d_{}}).\] ( \[\] -gate )

where all the gates are continuous functions on \(\). The space \(\) does not have to be a space of paths, a topological structure suffices, as long as the gates \(()_{0},,\) are well defined and continuous.

_Remark B.1_.: Typical examples for the choice of gates are \(\) space of paths and

\[(X)_{0}=0^{}_{t}=t^{}_{t}=_{0}^{t} X_{s}ds\] (S4)

\[(X)_{0}=0^{}_{t}=_{0}^{t}softplus( X_{s}+)ds ^{}_{t}=_{0}^{t}softplus( X_{s}+)X_{s}ds\] (Mamba)

Then the main object of study, "gated" Linear CDEs, are defined as:

**Definition B.2**.: Fix gates \(()_{0},,\) as above, \(N\), matrices \(\{A_{i}\}_{i=1,,d_{}}\) (\(A_{i}^{N N}\)), \(B^{N d_{}}\), \(C^{N d_{0}}\). The corresponding Linear CDE is the functional

\[Z: C_{1}(;^{N})\] (24)

\[Z^{}_{0}=CX_{0}, Z^{}_{t}=_{i=1}^{d_{}}A_{i }Z^{}_{t}d^{,i}_{t}+Bd^{}_{t}\] (25)

### Main Result - Statement and Strategy

Here we present the unified expressivity result in its most general form:

**Theorem B.3**.: _For any compact set \(\) and continuous gates \(()_{0},,\) with \(^{,1}_{t} t\) and \(^{,2}_{t} t^{2}\). For any \(>0\) and any_

\[F\{(X,t)(^{}_{[0,t]}) X_{0}+_{0}^{ t}(^{}_{[s,t]}) d^{}_{s}\}\] (26)

_where \( C^{0}(C_{1,0};^{d_{0}})\) and \( C^{0}(C_{1,0};^{d_{}})\), there exist a choice of hidden dimension \(N 1\) and parameters \(v^{N},A_{i}^{N N},B^{N d_{ }},C^{N d_{0}}\) such that_

\[_{(X,t)}|F(X,t)- v,Z^{}_{t}|\] (27)

_Moreover generic parameters suffice with high probability in the sense that under LeCun initialization_

\[[A_{i}]_{n,j}}{{}}(0,)  C_{n,j},B_{n,j}}{{}}(0,1)\]

[MISSING_PAGE_EMPTY:21]

**Definition B.6**.: Let \(_{d_{0},d_{},d_{}}\) be the set of words in the alphabet

\[_{d_{0},d_{},d_{}}:=\{_{i}\}_{i=1,,d_{0}}\{ _{j}^{}\}_{j=1,,d_{}}\{_{k}^{} \}_{k=1,,d_{}}\]

Fixed the gates \(()_{0},,\) we define \(T(X): l^{2}(_{d_{0},d_{},d_{}}) T(( _{d_{0},d_{},d_{}}))\) as the unique solution to:

\[T(X)_{0,t}=_{i=1}^{d}X_{0}^{i}_{i}+_{j=1}^{d_{}}_{t}^{ ,j}_{j}^{}+_{k=1}^{d_{}}_{0}^{t}T(X)_ {0,s}\ d_{s}^{,k}_{k}^{}\] (32)

In fact one readily sees that the only non-zero terms of \(T(X)_{0,t}\) defined as above are

\[ T(X)_{0,t},_{i}=X_{0}^{i}\] \[ T(X)_{0,t},_{i}_{Ik}^{} =_{0}^{t} T(X)_{0,s},_{i}_{I}^{ } d_{s}^{,k}=X_{0}^{i}\ (^{ })_{0,t}^{Ik}\] \[ T(X)_{0,t},_{j}^{}=_{t}^{ ,j}=_{0}^{t}d_{s}^{,j}\] \[ T(X)_{0,t},_{j}^{}_{ Ik}^{}=_{0}^{t} T(X)_{0,s},_{j}^{} _{I}^{} d_{s}^{,k}=_{s=0} ^{t}_{r=0}^{s}(^{})_{r,s}^{I}d_{r}^{ ,j}d_{s}^{,k}\] \[=_{r=0}^{t}_{s=r}^{t}(^{} )_{r,s}^{I}d_{s}^{,k}d_{r}^{,j}=_{0}^{t} (^{})_{r,t}^{I}\ d_{r}^{,j}\]

This is similar to the tensor-valued CDE defining the signature as a tensor _i.e._ Salvi et al. (2021a)

\[()_{0,t}=()+_{0}^{t}()_{0,s} d _{s}\]

with the addition of two terms to track \(X_{0}\) and \(^{}\). One could also understand \(T(X)_{s,t}\) as a sub-tensor of

\[X_{0}((^{},^{}))_{s,t}\]

but in doing this one would have to explicitly ignore most of the terms of this vector; the CDE (32) does exactly this, but implicitly. In any case the subtensor view shows that \(T:^{2} l^{2}(_{d_{0},d_{},d_{}})\) is well defined and continuous.

To the feature map \(T()_{0,t}\) with values in the Hilbert space \(l^{2}(_{d_{0},d_{},d_{}})\) is then associated a Reproducing Kernel Hilbert Space Berlinet and Thomas-Agnan (2011), where the Kernel is the one induced by the \(l^{2}\) product, which we denote by

\[_{t}^{()_{0},,} C^{0}(;)\] (33)

Classical RKHS theory tells us that we can characterize its elements as:

**Proposition B.7**.: _A map \(F()_{t}:\) is an element of \(_{t}^{()_{0},,}\) if and only if it is of the form_

\[F(x)_{t}=_{i=1}^{d_{0}}X_{0}^{i}_{i},(^{ })_{0,t}+_{j=1}^{d_{}}_{0}^{t}_{j}, (^{})_{s,t} d_{s}^{,j}\] (34)

_for \(_{i},_{j} l^{2}(_{d_{}})\). Moreover \(\|F()_{t}\|_{_{t}^{()_{0},,}}^{2}\) is equal to the minimal value of_

\[_{i=1}^{d_{0}}\|_{i}\|_{l^{2}(_{d_{}})}^{2}+_{j=1 }^{d_{}}\|_{j}\|_{l^{2}(_{d_{}})}^{2}\]

_taken over those \(,\) for which the above equality holds._

_Signature kernels_Salvi et al. (2021a) are a class of _universal kernels_ on sequential data which have received attention in recent years thanks to their efficiency in handling path-dependent problems Lemercier et al. (2021), Salvi et al. (2021b), Cochrane et al. (2021), Salvi et al. (2021c), Cirone et al. (2023), Issa et al. (2023), Pannier and Salvi (2024), Manten et al. (2024).

Just as signature kernels, the kernel associated to \(T(X)_{0,t}\) can be explicitly written as the solution of a two-parameter CDE:

**Lemma B.8**.: _Let \(^{}(s,t):= T(X)_{0,s},T(Y)_{0,t}^{2}\) then_

\[^{}(s,t)= X_{0},Y_{0}+^{} _{s},^{}_{t}+_{=0}^{s}_{=0}^{t}^{ }(,) d^{}_{},d^{}_{ }\] (35)

_or, directly in terms of Signature, also_

\[^{}(s,t)=  X_{0},Y_{0}(^{})_{ 0,s},(^{})_{0,t}+_{=0}^{s}_{=0 }^{t}(^{})_{,s},(^{})_{,t} d^{}_{},d^{}_{}\] (36)

Proof.: The first expression follows immediately from (32) the second one by summing the products of \(T(X)_{0,s}\)'s and \(T(Y)_{0,t}\)'s entries given above. 

**Definition B.9**.: Define the space \(^{()_{0},,}_{} C^{0}( ;)\) as the space of functions of form

\[(X,t) _{i=1}^{d_{0}}X^{i}_{0}_{i},(^{ })_{0,t}+_{j=1}^{d_{}}_{0}^{t}_{j}, (^{})_{s,t} d^{}_{s}\] (37)

for \(_{i},_{j} l^{2}(_{d_{}})\). Thus for all \(t\) and \(F^{()_{0},,}_{}\) it holds \(F(,t)^{()_{0},,}_{t}\).

#### b.3.3 Linear maps on \(Z^{}_{t}\) are close to the RKHS

The following proposition will show how linear maps on \(Z^{}_{t}\) cannot be more expressive than elements of the RKHS \(^{()_{0},,}_{}\) since their closure is in the closure of \(^{()_{0},,}_{}\). In this precise sense these spaces act like upper bounds to expressiveness.

**Proposition B.10**.: _Assume \(\) compact. Consider fixed the gates and \(A_{i}^{N N},B^{N d_{}}\) and \(C^{N d_{0}}\). Consider a linear readout \(v^{N}\). For any \(>0\) there exist choices of \(_{i},_{j} l^{2}(_{d_{}})\) such that_

\[_{(X,t)}| v,Z^{}_{t}-F(X,t)|\] (38)

_where \(F^{()_{0},,}_{}\). In other words, linear maps on the \(Z^{}_{t}\) are in the uniform closure of \(^{()_{0},,}_{}\)._

Proof.: Using (30) we see that \( v,Z^{}_{t}\) is a linear map on \(T(X)_{0,t}\) with coefficients

\[v^{}A_{I}B_{j} v^{}A_{I}C_{i}\]

using Cauchy-Schwartz it's moreover easy to see the existence of a constant \( 0\) such that for all \(I,i,j\) one has

\[|v^{}A_{I}B_{j}|^{|I|}|v^{}A_{I}C_{i}|^{|I |}.\]

Since \(|()_{s,t}^{I}|\|\|_{1-var,[s,t]}^{|I|}\) we have that, given an integer \(M 0\), the bound

\[R_{M}(t) :=|_{i=1}^{d_{0}}_{|I| M}v^{}A_{I}C_{i}\ X_{ 0}^{i}(^{})_{0,t}^{I}+_{j=1}^{d_{}}_{|I|  M}v^{}A_{I}B_{j}_{0}^{t}(^{})_{s,t}^{I}d ^{}_{s}|\] \[(\|X_{0}\|_{1}+\|^{}_{t}\|_{ 1})_{m=M}^{}d_{}^{m}\|^{} \|_{1-var,[0,t]}^{m}}{m!}\] \[(\|X_{0}\|_{1}+\|^{}_{t}\|_{ 1})_{m=M}^{}d_{}^{m}\|^{} \|_{1-var,}^{m}}{m!} K_{m=M}^{}K)^{m}}{m!}\]

where \(K 0\) is a constant which must exist by compactness of \(\) and continuity of the gates. Since \(K_{m=M}^{}K)^{m}}{m!}\) is just the tail of the taylor expansion of \(Ke^{ d_{}K}\) there must be an \(M\) such that \(_{t}R_{M}(t)\). But then the choice

\[_{i}^{I}:=v^{}A_{I}C_{i}\ (|I|<M)_{j}^{I}:=v^{}A_{I}B_{j} \ (|I|<M)\]

suffices for the required bound.

#### b.3.4 Uniform closure of the RKHS

Now that we have established the theoretical interest of the \(^{()_{0},,}_{}\) we proceed to characterize which maps \((X,t)\) can be uniformly approximated through them.

**Proposition B.11**.: _Fix a compact input set \(\) and continuous gates \(()_{0},,\) with \(^{,1}_{t} t\) and \(^{,2}_{t} t^{2}\). For any \(>0\) and any_

\[F\{(X,t)(^{}_{[0,t]}) X_{0}+_{0}^ {t}(^{}_{[s,t]}) d^{}_{s}\}\] (39)

_where \( C^{0}(C_{1,0};^{d_{0}})\) and \( C^{0}(C_{1,0};^{d_{}})\), there exist a \(G^{()_{0},,}_{}\) such that_

\[_{(X,t)}|F(X,t)-G(X,t)|\] (40)

Proof.: Note first that the map

\[ C_{1,0}(;^{d_{}}) (X,s,t)^{}_{[s,t]}\]

is a continuous map from a compact space, thus the image must be compact too. Moreover by Prop. A.8 the Signature separates the points in this image. Since any \(G\) as above has form

\[G(X,t) =_{i=1}^{d_{0}}X^{i}_{0}_{i},(^{ })_{0,t}+_{j=1}^{d_{}}_{0}^{t}_{j}, (^{})_{s,t} d^{,j}_{s}\] \[=_{i=1}^{d_{0}}X^{i}_{0}_{i},(^{ }_{[0,t]})_{0,1}+_{j=1}^{d_{}}_{0}^{t}_ {j},(^{}_{[s,t]})_{0,1} d^{,j}_{s}\]

the proof follows from the uniform density on compact sets of linear functionals on the (truncated) Signature (Thm. A.5), by also uniformly bounding thanks to compactness and continuity the norms of \(X_{0}\) and \(^{}_{1}\). 

_Remark B.12_.: The specific restriction of \(\) to subsets of \(\) is a crucial part of the result. The family of approximable maps _does not_ include _all_ path-to-path causal11 functions \(t Y^{}_{t}\) but a subset of them, of type \(t Y^{}_{t}:=(^{}_{[0,t]})\), satisfying the specific _time-homogeneity_ specified by the form of the restriction, akin to that in Li et al. (2022b).

#### b.3.5 Generic Weights are fully expressive

We have seen how linear maps on \(Z^{}_{t}\) are in the uniform closure of \(^{()_{0},;}_{}\), and we have explicitly characterized this closure. It is then natural to ask "how much" of this closure the \(Z^{}_{t}\) are able to "explore". The present section not only shows that the \(Z^{}_{t}\) "explore" all the closure, but also that a generic choice of weights is enough to eventually do this with high probability.

The fact that these maps are "universal" in the above sense is not surprising, since it is well known that Linear CDEs are universal for path-to-point tasks _cf._ Kidger (2022), what is surprising is that this universality can be achieved probabilistically with one of the _standard_ parametrizations used in ML practice (LeCun) 12.

**Theorem B.13**.: _Fix \(\) compact and \(>0\). For all \(F^{()_{0},,}_{}\) there exist a choice of hidden dimension \(N 1\) and parameters \(v^{N},A_{i}^{N N},B^{N d_{ }},C^{N d_{0}}\) such that_

\[_{(X,t)}|F(X,t)- v,Z^{}_{t}|\] (41)

_Moreover generic weight choices suffice with high probability, in the sense that under LeCun initialization_

\[[A_{j}]_{n,n^{}}}{{}}(0, )[C]_{n,i},[B]_{n,j}}{{}} (0,1)\]the following holds_

\[_{N} v^{N}:=1\]

We propose two proofs, the first one of a deterministic character concerns the first claim in the theorem, the second one is probabilistic and concerns the whole result. The deterministic proof follows the same arguments employed by Kidger (2022) and is included to highlight the main idea of the probabilistic result, which reduces to a "spin" on the central argument of this proof.

Deterministic Proof.: Any \(F_{}^{()_{0},,}\) has form

\[F(X,t)=_{i=1}^{d_{0}}X_{0}^{i}_{i},(^{})_{0,t}+_{j=1}^{d_{}}_{0}^{t}_{j},( ^{})_{s,t} d_{s}^{,j}\] (42)

for fixed \(_{i},_{j} l^{2}(_{d_{}})\). Consider an integer \(M 0\) such that

\[_{(x,t)}|F(X,t)-_{i=1}^{d_{0}}X_{0}^{i} _{M}_{i},(^{})_{0,t}-_{j=1}^{d_{ }}_{0}^{t}_{M}_{j},(^{})_{s,t}  d_{s}^{,j}|\] (43)

where \(_{M}\) is the truncation at length \(M\).

Fix \(d=d_{0}+d_{}+d_{}\). Consider \((M,d)\) such that \(^{(M,d)} T^{M}(^{d})\). We are going to write \(e_{I}^{(M,d)}\) to mean the image of \(e_{I} T^{M}(^{d})\) through this identification. Note that \(()_{M}e_{k}:T^{M}(^{d}) T^{M}(^{d})\) is a linear map, it does then correspond to a matrix \(_{k}^{(M,d)(M,d)}\). Write \(_{j}^{}:=e_{d_{0}+j}\) for \(j=1,,d_{}\) and \(_{k}^{}:=e_{d_{0}+d_{}+k}\) for \(k=1,,d_{}\). Then the solution to

\[^{(M,d)}_{t}=_{i=1}^{d_{0}}X_{0}^{i}e_{i}+_{ j=1}^{d_{}}_{t}^{,j}_{j}^{B}+_{k=1}^{d_{}} _{0}^{t}_{d_{0}+d_{}+k}_{s}d_{s}^{,k}\] (44)

is the object in \(^{(M,d)}\) corresponding to the truncated tensor \(_{M}(T(X)_{0,t})\).

This \(_{t}\) is of the form \(Z_{t}^{}\) with \(N=(M,d)\), \(A_{k}=_{d_{0}+d_{}+k}\), \(B=[_{1}^{}||_{d_{}}^{}]\) and \(C=[e_{1}||e_{d_{0}}]\).

In particular note how these matrices are such that

\[e_{J}^{T}A_{I}C_{i}=(e_{J}=e_{i}_{I}^{}),  e_{J}^{T}A_{I}B_{j}=(e_{J}=_{j}^{} _{I}^{}),\] (45)

since it holds that

\[A_{I}C_{i}=e_{i}_{I}^{}, A_{I}B_{j}=_ {j}^{}_{I}^{},\] (46)

and for all \(|I|>M\) one has necessarily \(A_{I}=0\).

Our strategy is that of using these equatities to create a vector \(v^{N}\) corresponding to the \(_{M}_{i}\) and \(_{M}_{j}\). Define the vector

\[v:=_{i=1}^{d_{0}}_{|I| M}_{i}^{I}\ e_{i}_ {I}^{}+_{j=1}^{d_{}}_{|I| M}_{j}^{I}\ _{j}^{}_{I}^{}^{(M,d)}\] (47)

Then expanding \(Z_{t}^{}\) as in (30) and using the equalities above one has

\[ v,Z_{t}^{}= _{i=1}^{d_{0}}_{I}v^{}A_{I}C_{i}\ X_{0}^{i}( ^{})_{0,t}+_{j=1}^{d_{}}_{I}v^{}A_{I}B_{j}\ _{0}^{t}(^{})_{s,t}d_{s}^{,j}\] \[= _{i=1}^{d_{0}}_{|I| M}_{i}^{I}X_{0}^{i}(^{})_{0,t}+_{j=1}^{d_{}}_{|I| M}_{j}^{I} _{0}^{t}(^{})_{s,t}d_{s}^{,j}\] \[= _{i=1}^{d_{0}}X_{0}^{i}_{M}_{i},( ^{})_{0,t}+_{j=1}^{d_{}}_{0}^{t}_{M} _{j},(^{})_{s,t} d_{s}^{,j}\]proving that for such \(v\) and \(Z^{}\) it holds

\[_{(x,t) X}|F(X,t)- v,Z^{}_{t}|\] (48)

The crucial ingredient for the success of this proof is the possibility to recreate the space \(T^{M}(^{d_{0}+d_{}+d_{}})\) as an euclidean space. To do this one needs \((M,d_{0}+d_{}+d_{})(d_{0}+d_{}+d_{})^{M}\) orthogonal vectors and a way to express them using the matrices \(A_{i},B\) and \(C\), the essential equations which capture this are given by (45).

The core idea of the following probabilistic proof of this same result is that of allowing for some error in (45), so the idea is that of exhibiting only _approximately_ orthogonal vectors. At the cost of losing exactness, one can leverage results of the Johnson-Lindenstrauss Dasgupta and Gupta (2003) type to find on the order of \( e^{^{2}N}\) vectors in \(^{N}\) orthogonal up to an \(\) error, using random projections. This idea in the context of Signature goes back to Cuchiero et al. (2021), and allows for much smaller hidden dimensions.

Proof.: (Probabilistic Proof) Any \(F^{()_{0},,}_{}\) has form

\[F(X,t)=_{i=1}^{d_{0}}X^{i}_{0}_{i},(^{ })_{0,t}+_{j=1}^{d_{}}_{0}^{t}_{j}, (^{})_{s,t} d^{,j}_{s}\] (49)

for fixed \(_{i},_{j} l^{2}(_{d_{}})\). Consider an integer \(M 0\) such that

\[_{(x,t) X}|F(X,t)-_{i=1}^{d_{0}}X^{i}_{0}_{M} _{i},(^{})_{0,t}-_{j=1}^{d_{}} _{0}^{t}_{M}_{j},(^{})_{s,t}  d^{,j}_{s}|\] (50)

where \(_{M}\) is the truncation at length \(M\).

From Cirone et al. (2023)[Appendix C] we know that

\[\|C^{}_{i}A^{}_{I}A_{J}C_{j}-^{jJ} _{iI}\|_{L^{2}}=(})2^{}(|I| +|J|)!!\] (51) \[\|C^{}_{i}A^{}_{I}A_{J}B_{j}\|_{L^{ 2}}=(})2^{}(|I|+|J|)!!\] (52) \[\|B^{}_{i}A^{}_{I}A_{J}B_{j}-^{jJ} _{iI}\|_{L^{2}}=(})2^{}(|I| +|J|)!!\] (53)

Our strategy is that of using these bounds to create a vector \(v^{N}\) "acting" like the \(_{M}_{i}\) and \(_{M}_{j}\). Define, noting that the \(A_{i},C_{i},B_{j}\) depend on \(N\), the vector

\[v^{}:=(_{i=1}^{d_{0}}_{|I| M}_{i}^ {I}\ A_{I}C_{i}+_{j=1}^{d_{}}_{|I| M}_{j}^{I}\ A_{I}B_{ j})\] (54)Then expanding \(Z_{i}^{}\) as in (30)

\[R_{M}:= \|_{(X,t)}| v^{},Z_{t}^{}-_{i=1}^{d_{0}}X_{0}^{i}_{M}_{i}, (^{})_{0,t}-_{j=1}^{d_{}}_{0}^{t} _{M}_{j},(^{})_{s,t} d_{s}^ {,j}|\|_{L^{2}}\] \[ _{i=1}^{d_{0}}_{|I| M}\|(v^{})^{}A_ {I}C_{i}-_{i}^{I}\|_{L^{2}}_{(X,t)}|X_{0 }^{i}(^{})_{0,t}^{I}|\] \[++_{i=1}^{d_{0}}_{|I|>M}\|(v^{})^{}A_ {I}C_{i}\|_{L^{2}}_{(X,t)}|X_{0}^{i}(^{})_{0,t}^{I}|\] \[+_{j=1}^{d_{}}_{|I| M}\|(v^{})^{ }A_{I}B_{j}-_{j}^{I}\|_{L^{2}}_{(X,t)}|_{0}^{t}Sig(^{})_{s,t}^{I}d_{s}^{,j}|\] \[++_{j=1}^{d_{}}_{|I|>M}\|(v^{})^{ }A_{I}B_{j}\|_{L^{2}}_{(X,t)}|_{0}^{t} Sig(^{})_{s,t}^{I}d_{s}^{,j}|\]

Note how for \(|I| M\) one has

\[\|(v^{})^{}A_{I}C_{i}-_{i}^{I}\|_{L^{2}} _{M}(})\]

and that similarly for \(|I|>M\)

\[\|(v^{})^{}A_{I}C_{i}\|_{L^{2}}_{M}( })^{2}(M+|I|)!!\]

Which leads, thanks to the same bounds of Cirone et al. (2023)[Appendix C], to

\[R_{M}=}_{M,}(1)\] (55)

But then by Markov's inequality it holds that

\[[_{(X,t)}| v^{},Z_{t}^{}-_{i=1}^{d_{0}}X_{0}^{i}_{M}_{i}, (^{})_{0,t}-_{j=1}^{d_{}}_{0}^{t} _{M}_{j},(^{})_{s,t} d_{s}^ {,j}|] 1\] (56)

and thus there must be a choice of \(N,\{A_{i}\},B,C\) such that the inequality holds, and we thus obtain using (50)

\[_{(X,t)}|F(X,t)- v^{},Z_{t}^{ }| 2\]

and we conclude by arbitrariness of \(\).

### The Diagonal Case

Here we study the particular, but empirically important, case where the matrices \(A_{i}\) are taken to be diagonal13.

What we'll discover is that the \(Z_{t}^{}\) cannot differentiate between \((^{})_{s,t}^{I}\) and other \((^{})_{s,t}^{(I)}\) for any permutation \(\) of the letters in the word \(I\).

#### b.4.1 Diagonal Expansion for \(Z^{}_{i}\)

**Proposition B.14**.: _For any choice of \(V^{N d_{}},B^{N d_{}}\) and \(C^{N d_{0}}\), writing \(A_{i}:=(V_{i})\), the unique solution to_

\[& dZ^{}_{t}=_{i=1}^{d_{}}A_{i}Z^{ }_{t}d^{,i}_{t}+Bd^{}_{t}\\ & Z^{}_{0}=CX_{0}^{N}\] (57)

_is given by_

\[Z^{}_{t}=e^{(V^{}_{t})}CX_{0}+_{0 }^{t}e^{(V(^{}_{t}-^{}_{s}))}Bd ^{}_{s}\] (58)

_which can be expanded as_

\[Z^{}_{t}=_{i=1}^{d_{0}}_{I_{d_{}}}A^{sym}_ {I}C_{i}\ X^{i}_{0}(^{})^{sym,I}_{0,t}+_{j=1}^{d _{}}_{I_{d_{}}}A^{sym}_{I}B_{j}_{0}^{t}(^{})^{sym,I}_{s,t}d^{,j}_{s}^{N}\] (59)

_where_

\[A^{sym}_{I}:=_{ S_{k}}A_{(I)}=A_{I} (^{})^{sym,I}_{s,t}:=_{ S _{k}}(^{})^{(I)}_{s,t}.\] (60)

Proof.: By Theorem E.1 and Theorem E.6 we know that the solution of

\[Z^{}_{t}=Z^{}_{0}+_{i=1}^{d_{}}_{0}^{t}A_{i}Z^{ }_{t}d^{,i}_{t}+_{0}^{t}Bd^{}_{t}\] (61)

is explicitly given by

\[Z^{}_{t}=W^{}_{0,t}Z^{}_{0}+_{0}^{t}W^{}_{ s,t}Bd^{}_{s}\] (62)

where \(W_{s,t}\) is the unique solution to

\[W^{}_{s,t}=Id+_{i=1}^{d_{}}_{s}^{t}A_{i}W^{}_{s, r}d^{,i}_{r}\] (63)

In case the \(A_{i}\)s are commuting matrices one can explicitly write the solution as

\[W^{}_{s,t}=(_{i=1}^{d_{}}_{s}^{t}A_{i}d^{ ,i}_{r})=((V(^{}_{t}- ^{}_{s})))\] (64)

since for fixed \(s\) one has, using commutativity, that

\[dW^{}_{s,t}=W^{}_{s,t}(_{i=1}^{d_{}}A_{i}d ^{,i}_{t})=_{i=1}^{d_{}}A_{i}W^{}_{s,t}d ^{,i}_{t}\]

On the other hand we know, Theorem E.2, that

\[W^{}_{s,t}=_{I_{d_{}}}A_{I}(^{ })^{I}_{s,t}=_{k=0}^{}_{I^{k}_{d_{}}} A_{I}(^{})^{I}_{s,t}\] (65)

The two views are reconciled by noticing that the symmetric group \(S_{k}\) acts on \(^{k}_{d_{}}\), the space of words of tenth \(k\), by permuting the letters and, by commutativity,

\[ S_{k}. I^{k}_{d_{}}.\ A_{I}=A_{ (I)}\]

Then we have

\[_{I^{k}_{d_{}}}A_{I}(^{})^{I}_{ s,t}=_{I^{k}_{d_{}}}_{ S_{k}}A_{ (I)}(^{})^{(I)}_{s,t}=_{I^{k}_{d_{}}}}{k!}_{ S_{k}}(^{ })^{(I)}_{s,t}\]recalling then how \(e_{I_{1}} e_{I_{k}}=_{ S_{k}}e_{(I)}\) we get to

\[_{I^{k}_{d_{}}}A_{I}}(^{ })^{I}_{s,t}\] \[=_{I^{k}_{d_{}}}}{k!}_{  S_{k}}}(^{})^{(I)}_{s,t}=_{I ^{k}_{d_{}}}}{k!}_{i=1}^{k}}( ^{})^{I_{i}}_{s,t}\] \[=_{I^{k}_{d_{}}}_{i=1}^{k }A_{I_{i}}}(^{})^{I_{i}}_{s,t}=( _{i=1}^{d_{}}A_{i}}(^{})^{i}_{s,t})^ {k}=(_{i=1}^{d_{}}_{s}^{t}A_{i}d^{,i}_{r})^{k}\]

In particular we see how in the commuting case

\[W^{}_{s,t}=_{I_{d_{}}}A^{sym}_{I}}( ^{})^{sym,I}_{s,t}\] (66)

where

\[A^{sym}_{I}:=_{ S_{k}}A_{(I)}=A_{I} }(^{})^{sym,I}_{s,t}:=_{ S _{k}}}(^{})^{(I)}_{s,t}.\]

#### b.4.2 Diagonal Expressiveness

**Theorem B.15**.: _Fix a compact input set \(\) and continuous gates \(()_{0},,\). For any \(>0\) and any_

\[F\{(X,t)(^{}_{t}) X_{0}+_{0}^{t} (^{}_{t}-^{}_{s}) d^{}_{s}\}\] (67)

_for \( C^{0}(^{d_{}};^{d_{}})\) and \( C^{0}(^{d_{}};^{d_{}})\), there exist a choice of hidden dimension \(N 1\) and parameters \(v^{N},B^{N d_{}},C^{N d _{}}\) and diagonal \(A_{i}^{N N}\) such that_

\[_{(X,t)}|F(X,t)- v,Z^{}_{t}|\] (68)

_Moreover the "reverse" also holds i.e. given any choice of matrices \(A_{i},B,C\) there is an \(\)-close map \(F\) in the family._

Proof.: This is just a repetition of the arguments used for the dense case with little more care to get the uniformity in time.

One defines the subset \(Sym(^{()_{0},,}_{})^{( )_{0},,}_{}\) of those \(F\) of type (37) defined by \(_{i},_{j} l^{2}(_{d_{}})\) such that for any word \(I\) and any permutation \((I)\) of it

\[_{i}^{I}=_{i}^{(I)}_{j}^{I}=_{j}^{(I)}\] (69)

The same argument of Proposition B.10 shows that the uniform closure of the space of linear maps on the \(Z^{}_{t}\) is contained in the uniform closure of \(Sym(^{()_{0},,}_{})\), and the same bounds show that this latter closure is the same as that of its subset composed of those \(F Sym(^{()_{0},,}_{})\) having entries eventually equal to \(0\).

Since

\[}(^{})^{sym,I}_{s,t}:=_{ S _{k}}}(^{})^{(I)}_{s,t}=_{i =1}^{|I|}(^{,I_{i}}_{t}-^{,I_{i}}_{s}),\]

such maps can be expressed exactly in the form

\[P(^{}_{t}) X_{0}+_{0}^{t}Q(^{}_{t}- ^{}_{s}) d^{}_{s}\]for polynomial maps \(P,Q\) fixed in time. The usual compactness and continuity argument, together with an application of Stone-Weiestrass, thus proves that the uniform closure of \(Sym(_{}^{()_{0},,})\) has the form needed.

The final ingredient is the density of the space of linear maps on the \(Z^{}_{t}\) in \(Sym(_{}^{()_{0},,})\); this is another consequence of Stone-Weiestrass as seen from Proposition B.18. 

_Remark B.16_.: Notice how here there is no need to augment the paths in creative ways in order to ensure separability of the points. The map \((,s,t)_{[s,t]} C_{1,0}(;^{d_{}})\) is replaced by \((,s,t)_{t}-_{s}^{d_{}}\) and the space of polynomials always separates points in \(^{d_{}}\).

_Remark B.17_.: It is not necessary to pass through \(Sym(_{}^{()_{0},,})\) to prove the previous result, since it directly follows from Proposition B.18. This choice of presentation has been motivated by the conviction of the usefulness of drawing parallels and comparisons.

**Proposition B.18**.: _Fix a compact set \(^{d}\) and a \(d\)-dimensional convex cone \(C\) containing the origin. The space_

\[:=Span( x e^{(,x)_{^{d}}} : C)\]

_is uniformly dense in \(C^{0}(;)\)._

Proof.: This is an application of Stone-Weiestrass: \(\) is a sub-algebra since

\[e^{(,x)}e^{(,x)_{^{d}}}=e^{(+,x)_{^ {d}}}\]

and \(, C+ C\) by convexity of the cone; \(\) contains the constant function \(e^{(0,x)}=1\) and is clearly point separating since the cone, being \(d\)-dimensional, it contains a basis of the whole space. 

_Remark B.19_.: The usefulness of stating the previous result in such a general setting is the following: with this formalism we can, for example, restrict to \( 0\), in this way we would have a method to control the stability (_cf._ Appendix C.1) of the Linear CDEs by choosing the gate with a.s. \(^{} 0\).

**Corollary B.20** (Mamba Case).: _In the Mamba setting, the closure reduces to_

\[\{(X,t)_{i=1}^{d_{}}_{i}(_{t}^{X,i})+_{i= 1}^{d_{}}_{0}^{t}_{i}(_{t}^{X,i}-_{s}^{X,i})\;d _{s}^{X,i}\}\] (70)

_for continuous \(_{i}:\) and \(_{i}:\)._

Proof.: In this setting one runs in parallel \(d_{}\) diagonal systems and then takes a linear combination of the stacked hidden state. The maps in the closure of the whole system are then just the sums of maps in the closure of the subsystems.

Stability and Chaining of Diagonal Systems

For this section consider, unless otherwise stated, a fixed \(N 0\), compact \(\) and gates \(()_{0},,\).

We will study the stability and chaining of diagonal systems defined by the choice of a matrix \(V^{N d_{}}\) such that \(A_{i}:=diag(V_{i})\), where \(V=[V_{1}||V_{d_{}}]\).

Note that the present discussion holds even for non-diagonal but _commuting_ matrices, since these can be simultaneously diagonalized (at the cost of considering the complex plane).

### Stability

Here we explore the stability of the dynamical system \(Z^{}\), thus we need to study the eigenvalues of the \(W^{}_{s,t}\). Recall how in this setting

\[W^{}_{s,t}=(_{i=1}^{d_{}}_{s}^{t}A_{i}d _{r}^{,i})=((_{s}^{t}Vd_{r}^{ }))=((V(_{t}^{}- _{s}^{})))\] (71)

Note that because \(^{}\) is continuous and of bounded variation, it can be reparameterised to be Lipschitz continuous, hence absolutely continuous. Thus we can assume that \(^{}\) is almost everywhere differentiable and its derivative \( L^{1}\).

The stability of the dynamical system then depends on the alignment between \(_{t}^{}-_{s}^{}\) and the singular vectors of \(V\). If \(V_{t}^{} 0\) for all times, where the inequality is coordinate-wise, then \(W^{}_{s,t}\) has eigenvalues all in \(\) thus the system is stable making training easier Orvieto et al. [2023a].

Consider the singular value decomposition (SVD) of the matrix \(V\)

\[V=_{k=1}^{K}_{k}\;v_{k}u_{k}^{}\] (72)

Then, a sufficient condition for stability is that for any \(k=1,...,K\)

\[0>_{k}, 0 v_{k}^{N},  u_{k},_{t}^{} 0\;t[0,T].\] (73)

#### c.1.1 The case of Mamba

In the case of Mamba Gu and Dao  the matrices are diagonal and

\[d_{t}^{}=softplus(Wx_{t}+)dt, d_{t}^{}=x_{t} d_{t}^{},\]

moreover the proposed choices of \(V\) are all of type

\[V=-_{d_{}}\]

for some choice of \(0^{N}\). Note that \(softmax\) is just a smooth approximation of \(ReLU\) and that \(Im(ReLU)\{w^{d_{}}:_{d_{}}, w 0\}\) hence mamba is implicitly ensuring that the dynamical system is approximately always well-conditioned.

### Chaining

The diagonal case differs from the general one not only in the fact that the class of approximable functions is much weaker but also in the necessity for the presence of \(^{}\) in order to obtain any path-dependence. The term

\[_{0}^{t}(_{t}^{}-_{s}^{}) d_{ s}^{}\]

becomes then a crucial component. At first sight one might think that such a term allows to recover at least level two components of the Signature of \((^{},^{})\), unfortunately things are not as easy as they may seem. Notice how inside of the integral time is "going backwards" from the perspective of \(^{}\), thus we can in general approximate terms of type

\[_{0}^{t}_{s}^{t}d_{r}^{,i}d_{s}^{,j}= _{1-t}^{1}_{1-t}^{r}d}_{r}^{,i}d }_{s}^{,j}=((}^{},}^{})) _{1-t,1}^{i j_{t}}\]

which are indeed terms of the Signature, but of the reverse paths \(}_{r}^{}=_{1-r}^{}\) and \(}_{s}=_{1-s}^{}\)!

**Proposition C.1**.: _Fix a compact input set \(\), continuous gates \(()_{0},,\) and \(X^{1}_{0}=1\). If the components of \(^{}\) are linear combinations of those of \(^{}\), with time-independent weights, then linear functionals on \(Z^{}_{t}\) can, uniformly in \(\), approximate arbitrarily well the following level \(2\) terms of \(Sig((^{},^{}))_{0,t}\):_

\[_{0}^{t}_{0}^{s}d^{,i}_{r}d^{,j}_{s}=Sig(( ^{},^{}))^{i_{u}j_{}}_{0,t}\]

Proof.: Under these hypotheses we know that linear functionals on \(Z^{}_{t}\) are uniformly dense, for continuous \(,\), in

\[\{(X,t)(^{}_{t}) X_{0}+_{0}^{t}( ^{}_{t}-^{}_{s}) d^{}_{s}\}.\]

Assume \(^{,j}_{s}=_{j},^{}_{t}\) and consider the choices

\[(x)=(x^{i}_{j},x,0,,0)^{},(x)=-(0, ,0,x^{i},0,,0).\] (74)

so that

\[(^{}_{t}) X_{0}=^{,i}_{t}^{,j }_{t}(^{}_{t}-^{}_{s}) d^{ {X}}_{s}=-(^{,i}_{t}-^{,i}_{s})d^{,j}_{ s}.\] (75)

To conclude note that

\[^{,i}_{t}^{,j}_{t}= _{s=0}^{t}_{r=0}^{t}d^{,i}_{r}d^{,j}_{s}=_{s=0}^{t}_{r=0}^{s}d^{,i}_{r}d^{,j}_ {s}+_{s=0}^{t}_{r=s}^{t}d^{,i}_{r}d^{,j}_{s}\] \[= _{0}^{t}_{0}^{s}d^{,i}_{r}d^{,j} _{s}+_{s=0}^{t}(^{,i}_{t}-^{,i}_{s})d^{ ,j}_{s}\]

hence

\[_{0}^{t}_{0}^{s}d^{,i}_{r}d^{,j}_{s}=^{ ,i}_{t}^{,j}_{t}-_{s=0}^{t}(^{,i}_{t}- ^{,i}_{s})d^{,j}_{s}=(^{}_{t}) X _{0}+_{0}^{t}(^{}_{t}-^{}_{s}) d^{ }_{s}\]

If \(X C_{1,0}(;^{d})\) we can use the previous result to compute its Signature entries by chaining _diagonal_ Linear CDEs.

**Theorem C.2**.: _Assume a compact input set \( C_{1,0}(;^{d})\). For any \(I_{d}\) with \(|I| 2\) and \(>0\) there is a sequence of linear maps \(W_{k}^{N_{k} 1}\) and weights for the following family of chained Linear CDEs_

\[dZ^{1,X}_{t}=_{i=1}^{d}A^{(1)}_{i}Z^{1,X}_{t}dX^{i}_{t}+B^{(1 )}dX_{t}^{N_{1}}, Z^{1,X}_{0}=Z^{1}_{0},\] (76) \[dZ^{k+1,X}_{t}=_{i=1}^{d+1}A^{(k+1)}_{i}Z^{k+1,X}_{t}d W_{k}Z^{k,X}\\ X_{t}^{i}+B^{(k+1)}dX_{t}^{N_{k+1}}, Z^{k+1,X}_{0 }=Z^{k+1}_{0},\] (77)

_such that for some \(v^{N_{|I|-1}}\) one has_

\[_{(X,t)}|Sig(X)^{I}_{0,t}- v,Z^{|I|-1,X}_{t} |\] (78)

Proof.: For \(|I|=2\) we can apply Prop. C.1. Assume the theorem holds for \(|I| k\) and let \(M:=_{X}\|X\|_{1-var}\). Fix \(|Ij|=k+1\) and \(W_{k-1}^{N_{k-1} 1}\) such that

\[_{(X,t)}|Sig(X)^{I}_{0,t}-W_{k-1}Z^{k-1,X}_{t}| .\]

Again by Prop. C.1 there are a \(N_{k}\) and \(v^{N_{k}}\) such that

\[_{(X,t)}|_{0}^{t}W_{k-1}Z^{k-1,X}_{s}dX^{j}_{s }- v,Z^{k,X}_{t}|.\]Then

\[|}(X)^{Ij}_{0,t}- v,Z^{k,}_{t}| |_{0}^{t}}(X)^{I}_{0,s}dX^{j}_{s}-_{0}^{t}W_{k-1 }Z^{k-1,}_{s}dX^{j}_{s}|+|_{0}^{t}W_{k-1}Z^{k-1,}_{s}dX^{j}_ {s}- v,Z^{k,}_{t}|\] \[ _{0}^{t}|}(X)^{I}_{0,s}-W_{k-1}Z^{k-1,}_{s} ||dX^{j}_{s}|+\] \[ _{0}^{t}|dX^{j}_{s}|+ 2\]

thus concluding the proof. 

Then Proposition 4.5 follows as a corollary by running in parallel the systems above to recover simultaneously multiple Signature entries.

#### c.2.1 \(ReLU\) activation choice

Models like _Lambda_ do not only use diagonal matrices but also consider controls of a specific kind:

\[^{}_{t}=_{0}^{t}ReLU(WX_{s}+b)ds\]

The choice of \(ReLU\) enforces \(_{t} 0\) for all times as seen above, but could, a priori, destroy information about \(X\) which allows for the recovery, after chaining, of its Signature.

Does this choice keep some expressivity? Fortunately almost all of it: since

\[ReLU(x)-ReLU(-x)=x\]

one can choose a linear map \(W\) which allows to linearly recover

\[^{}_{t}=_{0}^{t}X_{s}ds\]

from \(^{}_{t}\). By correspondingly modifying the form of \(\) and \(\) in (74) such that

\[(^{}_{t}) X_{0}=^{,i}_{t}^{ ,j}_{t}(^{}_{t}-^{}_{s}) d ^{}_{s}=-(^{,i}_{t}-^{,i}_{s})d^{,j}_{s}.\] (79)

one is able, through a similar chaining procedure, to recover arbitrarily deep entries of the Signature of \(^{}_{t}=_{0}^{t}X_{s}ds\).

Path-to-Path

**Definition D.1**.: A map \(G C^{0}(C_{1,0}(;^{d});)\) is _causal_ iff for all \(t\) and paths \(, C_{1,0}(;^{d})\) one has

\[|_{[0,t]}=|_{[0,t]} G(,t)=G(,t)\]

_i.e._\(\) is _causal_ if it does not look in the future.

**Proposition D.2**.: _Assume a compact input set \(\), continuous \(()_{0},,\), \(X_{0}^{1} 1\) and \(_{t}^{X,1} t\). Then for all \(>0\) and all causal\(G C^{0}(C_{1,0}(;^{d_{}});)\) there exist an integer \(N 0\), some Feed Forward neural network \(F:^{N}\), and parameters \(C^{N d_{0}},A_{i}^{N N},B^{ N d_{}}\) such that_

\[_{X}_{t}|F(Z_{t}^{})-G(^{},t)|<\] (80)

Proof.: Fix \(>0\). By B.7 the space \(_{}^{()_{0},;}\) contains all functionals of form

\[(X,t),(^{})_{0,t}\]

thus, by the properties of the signature and by compactness of \(\), for any fixed \(s_{0}\) there is some \(f_{}^{()_{0},,}\) such that

\[_{X}|f(X,s_{0})-G(^{},s_{0})|<\]

Using the fact that \(G C^{0}(;C^{0}(C_{1,0}(;^{d_{}});))\) and compactness of \(\), we find a finite set \(\{0 s_{0} s_{M} 1\}\) of points and \(f_{0},,f_{M}_{}^{()_{0},,}\) such that

\[_{(X,s)[s_{i-1},s_{i+1}]}|G(^{},s)-G(^{},s_{i})|<\] (81) \[_{X}|f_{i}(X,s_{i})-G(^{},s_{i}) |<\] (82) \[_{(X,s)[s_{i-1},s_{i+1}]}|f_{i}(X,s)-f_{i}( X,s_{i})|<\] (83)

for \(i=0,,M-1\). Notice then how for all \(X\) and \(s[s_{i-1},s_{i+1}]\)

\[|f_{i}(X,s)-G(^{},s)||f_{i}(X,s)-f_{i}(X,s_{i})|+|f_{i}(X, s_{i})-G(^{},s_{i})|+|G(^{},s_{i})-G(^{ },s)|\ 3\]

It follows that the map \(F C^{0}(;)\) linearly interpolating the \(f_{i}\) in time satisfies

\[_{X}_{t}|F(X)_{t}-G(^{},t)|<6\]

To conclude note that \(\) being compact, the \(f_{i}\) take values in a common compact set \(K\). There exist then a neural network \(: K^{M}\) such that

\[_{i,,M-1}_{s[s_{i},s_{i+1}]}|(t,z)-(-s}{s_{i+1}-s_{i}}z_{i}+}{s_{i+1}-s_{i}}z_{i+1})|<\]

which means that

\[_{X}_{t}|(t,f_{0}(X,t),,f_{M}(X,t))-F( X,t)|<\]

Recalling that \(_{t}^{,1}=t\) we get that \(X\{t t\}_{}^{()_{0},,}\) so that, given density of linear maps on \(Z^{}\) in the space, \((t,f_{0}(X,t),,f_{M}(X,t))\) can be uniformly approximated. Triangular inequality gives finally

\[_{X}_{t}|(t,f_{0}(X,t),,f_{M}(X,t))-G( ^{},t)|<7\]

which, by arbitrariness of \(\), gives the thesis.

The non-linearity is crucial for the path-to-path result. A map of type \((,t) t,}()_{0,t}\) cannot be approximated arbitrarily well by \((,t),}()_{0,t}\).

In any case, note that in the proof the role of the neural network is only that of interpolating the RKHS elements in the right order and at the right time. All the non-linear complexity of learning the particular \(G\) is offloaded and taken care of by the RKHS elements.

_Remark D.3_.: In the proof we have only considered the part of \(T(X)\) concerning \(^{}\), but \(T(X)_{t}\) depends linearly on \(X_{0}\) and \(^{}\) suggesting that neural networks on \(^{()_{0},,}_{}\) have stronger generalization properties. In fact one can prove that it is possible to approximate all continuous \(G(X_{0},^{},^{},t)\), this is done by reconstructing \(X_{0}\) and \(^{}_{}\) as in the classical SSM case _cf._Orvieto et al. [2023b].

Wronskian Matrix Theory

In this section we obtain a unified theory studying the solutions to general Linear CDEs. The results presented here are not new and can be found in different terms in the literature Friz and Victoir (2010), despite this we have decided to reproduce them from scratch for completeness, notational reasons and to present a self-contained theory.

**Theorem E.1**.: _For any choice \(\{A^{1},,A^{d}\} C^{0}(;^{N N})\) and \( C_{1}(;^{d})\) there exist a unique map \(W C^{0}(;^{N N})\) solving the following CDE_

\[W_{s,t}=Id_{N}+_{i=1}^{d}_{=s}^{t}A^{i}_{}W_{s,}d^{i} _{}\] (84)

Proof.: We will use Banach fixed point theorem leveraging the completeness of the space \(:=C^{0}(;^{N N})\) with the uniform norm

\[\|X\|_{}:=_{s,t}\|X_{s,t}\|_{op}\]

Define the map \(:\) as

\[(X)_{s,t}=Id_{N}+_{i=1}^{d}_{=s}^{t}A^{i}_{}X_{s,}d ^{i}_{}\]

One has, for \(X,Y\) and \(k\) setting \(^{0}=Id_{}\), that

\[^{k+1}(X)_{s,t}-^{k+1}(Y)_{s,t}=_{i=1}^{d}_{=s}^{t}A^{ i}_{}(^{k}(X)_{s,}-^{k}(Y)_{s,})d^{i}_{}\]

which iterated gives

\[^{k+1}(X)_{s,t}-^{k+1}(Y)_{s,t} =_{I_{d}\\ |I|=k+1}_{_{k+1}=s}^{t}_{_{1}=s }^{_{2}}(_{j=k+1}^{1}A^{I_{j}}_{_{j}^{j}})(X_{s,_ {1}}-Z_{s,_{1}})_{j=1}^{k+1}d^{I_{j}}_{_{j}^{j}}\] \[=_{I_{d}\\ |I|=k+1}_{^{k+1}_{[s,t]}}A^{I}_{ }(X_{s,_{1}}-Z_{s,_{1}})d^{I}_{}\]

where \(_{d}\) is the set of words in the alphabet \(\{1,,d\}\) and

\[^{k}_{[s,t]}:=\{(_{1},,_{k})^{k}: j 1,,k-1.\ \ _{j}_{j+1}\}\]

\[A^{I}_{}:=_{j=k+1}^{1}A^{I_{j}}_{_{j}} d^{I}_{}:= _{j=1}^{k+1}d^{I_{j}}_{_{j}}\]

By defining \(M=\{\|A^{i}\|_{}:i\{1,,d\}\}\) then one clearly has

\[\|^{k}(X)-^{k}(Y)\|_{})^{k}}{k!}\|X-Y\|_{}\] (85)

thus definitely (in \(k\)) the map \(^{k}\) is a contraction. By Banach fixed point there exist a unique fixed point \(W\). 

**Theorem E.2**.: _Under the assumptions of the previous theorem one can write \(W_{s,t}\) explicitly as_

\[W_{s,t}=_{I_{d}}_{^{|I|}_{[s,t]} }A^{I}_{}d^{I}_{}\] (86)

_moreover if for all \(i\) the matrix-valued maps are constant on all \(\) i.e. \(A^{i}_{t} A_{i}\) then_

\[W_{s,t}=_{I_{d}}A_{I}()^{I}_{s,t}\] (87)

_where \(()^{I}_{s,t}\) is the Signature of the path \(\)._Proof.: The second assertion follows from the first by definition of the Signature of a path.

Regarding the first notice how the series is absolutely convergent in \(^{N N}\), uniformly in \(s,t\) since

\[_{I_{d}}\|_{^{|I|}_{[,t]} }A^{I}_{}d^{I}_{}\|_{op} _{k=0}^{}d^{k}M^{k}^{k}}{k!}\] \[= e^{dM\|\|_{1-var,[s,t]}} e^{dM\|\|_{1-var,}}\]

thus for any \(s,t\) the series defines an element of \(_{s,t}^{N N}\).

Using the uniformity of this bound and the fact that for all \(I_{d}\) one has

\[^{I}_{s,t}:=_{^{|I|}_{[,t]}}A^{I}_{}d^ {I}_{}\]

as a function of \((s,t)\), which moreover is uniformly continuous

\[\|^{I}_{s_{1},t_{1}}-^{I}_{s_{2},t_{2}} \|_{op}= \|_{^{|I|}_{[s_{1} s_{2},t_{1} t _{2}]}}(_{^{|I|}_{[s_{1},t_{1}]}}-_{^{|I |}_{[s_{2},t_{2}]}})A^{I}_{}d^{I}_{}\|_{op}\] \[ M^{|I|}_{^{|I|}_{[s_{1} s_{2},s_{1} t _{2}]}}|_{^{|I|}_{[s_{1},t_{1}]}}-_{ ^{|I|}_{[s_{2},t_{2}]}}||d^{I}_{}|\] \[ M^{|I|}\|\|^{|I|}_{[s_{1} s_{2},s_{1} s_{2}] \{t_{1} t_{2},t_{1} t_{2}\}},\]

one concludes that \(_{s,t}\). Finally notice that \(_{s,t}\) is a fixed point of \(\)

\[(_{s,t})= Id_{N}+_{i=1}^{d}_{=s}^{t}A^{i}_{}(_{I _{d}}_{^{|I|}_{}}A^{I}_{}d^{I}_{ })d^{i}_{}\] \[= Id_{N}+_{I_{d}\\ |I| 1}_{^{|I|}_{}}A^{I}_{}d^ {I}_{}d^{i}_{}\] \[= _{I_{d}}_{^{|I|}_{}}A^{I}_ {}d^{I}_{}=_{s,t}\]

and conclude by uniqueness. 

**Proposition E.3**.: _Under the previous conditions, the unique solution of the \(N\)-dimensional CDE_

\[dX_{t}=X_{0}+_{i=1}^{d}_{=0}^{t}A^{i}_{}X_{}d^{i}_{}\] (88)

_is given by_

\[X_{t}=W_{0,t}X_{0}\] (89)

Proof.: The solutions are unique by standard results Friz and Victoir [Thm. 3.7], moreover

\[W_{0,t}X_{0}= (Id_{N}+_{i=1}^{d}_{=0}^{t}A^{i}_{}W_{0,}d ^{i}_{})X_{0}=X_{0}+_{i=1}^{d}_{=0}^{t}A^{i}_{ }(W_{0,}X_{0})d^{i}_{}\]

**Proposition E.4**.: _The Wronskian matrix has the following properties:_

1. \( r,s,t\)_._ \(W_{r,t}=W_{s,t}W_{r,s}\)__
2. \( s,t\)_._ \(W_{s,t}^{-1}=W_{t,s}\)__
3. \( s,t\)_._ \(W_{s,t}=Id_{N}+_{i=1}^{d}_{=s}^{t}W_{,t}A^{i}_{}d ^{i}_{}\)__Proof.: Regarding the first statement notice that for all \(X_{0}^{N}\) one has

\[_{t}:= W_{s,t}W_{r,s}X_{0}=(Id_{N}+_{i=1}^{d}_{=s}^{t}A _{}^{i}W_{s,}d_{}^{i})W_{r,s}X_{0}\] \[= W_{r,s}X_{0}+_{i=1}^{d}_{=s}^{t}A_{}^{i}(W_{s, }W_{r,s}X_{0})d_{}^{i}\] \[= W_{r,s}X_{0}+_{i=1}^{d}_{=s}^{t}A_{}^{i}_{}d_{}^{i}\]

and by the previous proposition also

\[X_{t}:=W_{r,t}X_{0}=W_{r,t}X_{0}+_{i=1}^{d}_{=r}^{t}A_{}^{i}X_ {}d_{}^{i}\]

thus \(X_{t}\) and \(_{t}\) solve the same \(CDE\) and coincide at time \(t=s\). This means, by uniqueness, that \(X_{t}\) and \(_{t}\) coincide for all times; hence \(W_{s,t}W_{r,s}\) and \(W_{r,t}\) coincide for all times too since for any choice of \(X_{0}\) one has \(W_{s,t}W_{r,s}X_{0}=W_{r,t}X_{0}\).

The second statement follows from the previous one setting first \(r=t\) and subsequently exchanging \(s\) and \(t\).

To prove the third equality note that

\[0=d_{s}(W_{s,t}W_{t,s})=(d_{s}W_{s,t})W_{t,s}+W_{s,t}(d_{s}W_{t,s})\]

hence

\[d_{s}W_{s,t}=-W_{s,t}(d_{s}W_{t,s})W_{t,s}^{-1}=-W_{s,t}(_{i=1}^{d}A_{s}^{ i}W_{t,s}d_{s}^{i})W_{t,s}^{-1}=-_{i=1}^{d}W_{s,t}A_{s}^{i}d_{s}^{i}\]

**Proposition E.5** (Liouville's Formula).: _Under the assumptions of the previous theorems, if \( C^{1}(;^{d})\) then_

\[det(W_{s,t})=1+_{i=1}^{d}_{=s}^{t}tr(A_{}^{i})det(W_{s,t})d _{}^{i}=(_{i=1}^{d}_{=s}^{t}tr(A_{}^{i})d _{}^{i})\] (90)

Proof.: This just follows from the classical case since we can write

\[_{i=1}^{d}_{=s}^{t}A_{}^{i}W_{s,}d_{}^{i}=_ {=s}^{t}(_{i=1}^{d}A_{}^{i}_{}^{i})W_{s,}d\]

We can now state the main result of the section:

**Theorem E.6**.: _Under the assumptions of the previous theorems, given continuous functions \(\{B^{1},,B^{t}\}(^{d})^{}\) the unique solution of the \(N\)-dimensional CDE_

\[X_{t}=X_{0}+_{i=1}^{d}_{=0}^{t}(A_{}^{i}X_{}+B_{} ^{i})d_{}^{i}\] (91)

_is given explicitly by_

\[X_{t}=W_{0,t}X_{0}+_{i=1}^{d}_{0}^{t}W_{s,t}B_{s}^{i}d_{s}^{i}\] (92)

_where \(W_{s,t} C^{0}(;^{N N})\) is the Wronskian matrix defined by_

\[W_{s,t}=_{I_{d}}_{_{[s,t]}^{[I]}}A_{}^ {I}d_{}^{I}\] (93)Proof.: Given the unique solution \(X_{t}\) one has

\[d_{s}(W_{s,t}X_{s})= d_{s}(W_{s,t})X_{s}+W_{s,t}d_{s}(X_{s})\] \[= _{i=1}^{d}(-W_{s,t}A_{s}^{i}X_{s}+W_{s,t}A_{s}^{i}X_{s}+W_ {s,t}B_{s}^{i})d_{s}^{i}\] \[= _{i=1}^{d}W_{s,t}B_{s}^{i}d_{s}^{i}\]

hence

\[X_{t}-W_{0,t}X_{0}=W_{t,t}X_{t}-W_{0,t}X_{0}=_{i=1}^{d}_{s=0}^{t}W_{s, t}B_{s}^{i}d_{s}^{i}\]ZOH and Exact Solutions

Consider a Linear CDE as the one of (25)

\[dZ_{t}=_{i=1}^{d_{w}}A_{i}Z_{t}d_{t}^{i}+Bd_{t}\]

and recall how the solution can be explicitly written, for times \(s<t\), as

\[Z_{t}=W_{s,t}Z_{s}+_{s}^{t}W_{r,t}Bd_{r}\]

Assume moreover that in the interval \([s,t]\) both drivers have constant derivative _i.e._

\[_{r}=_{s}+(r-s)_{r}=_{s}+(r-s)\]

Then if \(_{}:=_{i=1}^{d_{w}}A_{i}^{i}\) we get that \(W_{r,t}=e^{_{}(t-r)}\) thus

\[Z_{t}=e^{_{}(t-s)}Z_{s}+_{s}^{t}e^{_{}(t-r )}Bdr=e^{_{}(t-s)}Z_{s}+(_{s}^{t}e^{_ {}(t-r)}dr)B\] (94)

But the integral can be explicitly solved as

\[_{s}^{t}e^{_{}(t-r)}dr=(-_{}^{-1}e^{ _{}(t-r)}|_{r=s}^{t}=_{}^{-1}(e^{ _{}(t-s)}-)\] (95)

leaving us with

\[Z_{t}=e^{_{}(t-s)}Z_{s}+_{}^{-1}(e^{ _{}(t-s)}-)B\] (96)

which, setting \(=t-s\), can be rewritten as

\[Z_{t}=e^{_{}}Z_{s}+(_{} )^{-1}(e^{_{}}-)(B) {v}\] (97)

_i.e._ exactly the ZOH scheme.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the Introduction we list our contributions and refer, point by point, to the relevant sections of our work. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: We have discussed some limitations in the conclusion section.

Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All the proofs are found in the Appendices, the specific sections of interest are referred after the statements. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Code to reproduce experiments included in supplementary material. Guidelines:* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code to generate the data used and to run the experiments are included in the supplementary material alongside a README explaining how to run the experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/pub blic/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.

* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Details can be found in the section titled empirical validation, with additional details for reproducing the results included in the supplementary material. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Details included in the empirical validation section. Guidelines: * The answer NA means that the paper does not include experiments.

* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This is a theoretical work. We provide information sufficient for the reproduction of empirical results. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This is a theoretical work. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]Justification: This is a theoretical work.

Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?Answer: [NA] Justification: This is a theoretical work, not involving crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This is a theoretical work, not involving crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.