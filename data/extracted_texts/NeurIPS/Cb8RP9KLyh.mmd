# Regress, Don't Guess - A Regression-like Loss on

Number Tokens for Language Models

 Jonas Zausinger\({}^{1,2}\) Lars Pennig\({}^{1,2}\) Kacper Chlodny\({}^{1,2}\) Vincent Limbach\({}^{1,2}\) Anna Ketteler\({}^{1,2}\) Thorben Prein\({}^{1,2}\) Vishwa Mohan Singh\({}^{2,3}\) Michael Morris Danziger\({}^{4}\)

Jannis Born\({}^{4,*}\)

\({}^{1}\)TU Munich, Germany; \({}^{2}\)TUM.AI, Germany; \({}^{3}\)LMU Munich, Germany

\({}^{4}\)IBM Research Europe, Switzerland;

Corresponding author: jab@zurich.ibm.com

###### Abstract

While language models have exceptional capabilities at text generation, they lack a natural inductive bias for emitting numbers and thus struggle in tasks involving reasoning over quantities, especially arithmetics. This has particular relevance in scientific datasets where combinations of text and numerical data are abundant. One fundamental limitation is the nature of the CE loss, which assumes a nominal (categorical) scale and thus cannot convey proximity between generated number tokens. As a remedy, we here present two versions of a number token loss. The first is based on an \(L_{p}\) loss between the ground truth token value and the weighted sum of the predicted class probabilities. The second loss minimizes the Wasserstein-1 distance between the distribution of the predicted output probabilities and the ground truth distribution. These regression-like losses can easily be added to any language model and extend the CE objective during training. We compare the proposed schemes on a mathematics dataset against existing tokenization, encoding, and decoding schemes for improving number representation in language models. Our results reveal a significant improvement in numerical accuracy when equipping a standard T5 model with the proposed loss schemes.

## 1 Introduction

As coined by Thawani et al. , numbers in natural texts are _ubiquitous_ and _important_, yet systematically _neglected_ by language models (LMs). Even worse, while Transformers  were invented for NLP, they have permeated various scientific domains (chemistry, biology, etc ), where tabular/numerical data is more prevalent than in NLP and often even fundamental for constructing task definitions: Molecules are labeled with drug efficacy, chemical reactions with yield, and synthesis procedures are natural text interspersed with quantities and times. Still, LMs notoriously struggle even with simple arithmetic tasks like three-digit multiplication  for multiple reasons:

1. **Tokenization**: Standard subword tokenization splits numbers into arbitrary tokens, disrupting their structure. Mitigation strategies include scientific notation  or digit-level tokenization , which may also preserve the decimal order of each digit .

2. **Embedding**: Canonically, the model has to recover the structure of numbers from data because the embeddings of numerical tokens are learned like any other token. Countless flavors of numeracy-preserving word embeddings exist [13; 1; 7], often akin to positional encodings.
3. **Training objective**: The standard cross-entropy (CE) loss assumes a nominal scale, thus it fails to convey the proximity between numbers, effectively inducing a semi-supervised setting. For example, predicting a  instead of a  token will not generally induce lower loss than a . This problem has been surprisingly neglected and is the focus of this work.

Here, we aim to equip LMs with better inductive biases to handle combinations of textual and numerical data, such as math word problems or scientific datasets. In particular, we propose two versions of a regression loss on number tokens that respect numerical proximity (cf. Figure 1 right) and can be effectively combined with regular CE. The first version of this loss computes the Mean Squared Error (MSE) between the sum of the predicted class probabilities, weighted by their respective numerical token value, and the numerical token value of the label. The second version computes the Wasserstein distance between the distribution of the predicted number probabilities and the ground truth distribution, which is the one-hot encoding of the label. We integrate these improved training objectives with existing solutions for tokenization and embedding, in particular the Regression Transformer . We evaluate all methods on a subset of the mathematical-question-answer dataset from DeepMind .

Prior art for joint language-number modeling suggested the use of verifiers [3; 10], calculators (typically: Python interpreters), or chain-of-thought (CoT) reasoning  to yield improved performance in Large Language Models (LLMs). We argue that all such strategies avoid the fundamental, underlying problem (i.e., number representation in LMs is poor) by reformulating the task, trying to correct answers _a posteriori_ with calculators, or using significantly more compute (CoR). Therefore, we herein intentionally attempt to improve a classic, relatively small encoder-decoder LM with up to 220M parameters, namely T5 .

## 2 Methods

### Number Token Loss

The idea of the Number Token Loss (NTL) is to add an additional loss term to the CE, which is only applied to number tokens and takes their numerical proximity into account. To achieve this, we propose two versions.

Figure 1: _Left_: xVal  decodes numbers through a regression head carried alongside the regular token head, gated through the [NUM] token (figure reproduced with permission). _Right_: Instead, the Number Token Loss (NTL) circumvents the need for two heads and allows the computation of a regression loss directly on the token head. We propose two schemes to achieve this: \(_{}\) (right) leverages a dot product of the values of the number tokens and their class probabilities. The \(_{}\) (left) uses the Wasserstein-1 distance of the (sorted) number token labels and their class probabilities.

Number Token Loss with Mean Squared Error (NTL-MSE)This loss compares the numerical value of the ground truth token with the weighted sum of the respective numerical token values, with the weights corresponding to the predicted class probabilities (cf. Figure 1 right). Given a model \(f()\), input tokens \(_{ i}\) (where \(i N\)), the numerical value \(_{i}\) of ground truth token \(y_{i}\) and a vocab \(V\) consisting of tokens (with indices \(j,...,k\) representing the number tokens), we compute NTL-MSE:

\[_{}=_{i}^{N}(_{i}-f(_ { i})_{j:k} V_{j:k})^{2}\] (1)

Instead of a nominal-scale loss with regular CE, the NTL-MSE effectively conveys proximity between numbers. For example, if the label is  and the LM predicts a  instead of , the loss will be lower, matching our intuitive expectation, unlike the CE which gives constant loss no matter the proximity of the number (cf. Figure 2). This is sufficient for the vast majority of cases, however, since the NTL is not injective (like CE), it can return spuriously low loss for incorrect predictions. Consider e.g., a label  with \(50\%\) of the mass on  and \(50\%\) on  token, then NTL will be zero (Figure 3). While such cases are rare due to the softmax emphasizing logit differences, combining NTL with CE loss helps correct spurious cases, as CE continues refining predictions without reducing its value in these instances. However, to address this non-injectiveness, we propose a second version based on the Wasserstein-1 distance.

Number Token Loss with Wasserstein-1 distance (NTL-WAS)This loss calculates the Wasserstein-1 distance between the predicted probability distribution of the (sorted) number tokens and the ground truth probability distribution, which is 1 for the label token and 0 for all other tokens. Given the ground truth \(y_{i}\), a vocab \(V\) with number tokens ordered from indices \(j\) to \(k\) and the cumulative distribution function \(()\), we compute NTL-WAS:

\[_{}=_{i=1}^{N}| (y_{i})-(f(_{ i})_{j:k})|\] (2)

As one can see in Figure 2, this version of the NTL not only conveys proximity between numbers correctly but also eliminates the non-injectiveness problem, shown in Figure 3. Both versions of the NTL are scaled with \(\) (\(0.3\) unless mentioned otherwise) and added to the regular CE loss:

\[=_{CE}+_{NTL}\] (3)

Note that both versions of the NTL shall be 0 for all non-numerical tokens. By changing the \(p\)-order in NTL-MSE, different \(L_{p}\)-norm losses can be obtained (e.g., NTL-MAE). Huber loss is also compatible. In Appendix A.2, we provide pseudo-code for both versions of the NTL.

### Backbone T5 and model variants

We use a T5 backbone  (Appendix A.3) for our experiments and extend it with both versions of the NTL and the Regression-Transformer tokenization scheme, due to its flexible encoder-decoder architecture and its success in various natural language processing tasks.

Regression Transformer (RT).The Regression Transformer  tokenizes numbers on digit level, considering both the position and value of each digit. Since standard learnable embeddings may not adequately preserve the inherent structure of numbers, it leverages an inductive bias to account for the relative proximity of the numbers through numerical encodings, further explained in Appendix A.5.

xVal encoding and decoding scheme.The xVal method  encodes real numbers using a single [NUM] token multiplied by its numerical value. For decoding (see Figure 1), a number head predicts the value while the token head outputs the sequence, replacing [NUM] during inference. However, this scheme is incompatible with T5 (see Appendix A.6). We thus use the xVal encoder and masked language modeling in our experiments.

Integration of the Number Token LossBoth versions of our proposed NTL, depicted in the right panel of Figure 1, can be integrated into any model that treats numbers as clearly separated tokens of single digits by applying it as an additional loss term. Therefore, we adapt the tokenization scheme of the standard T5 model to tokenize all numbers on the digit level to make it compatible with the NTL. As RT already tokenizes numbers on digit level by default, we can integrate the NTL without any changes. Integrating NTL into xVal is not feasible, as xVal encodes every number with the same token. Moreover, xVal already uses both MSE and CE loss.

## 3 Experiments and results

To test the mathematical capabilities of the methods, we use a dataset with more than 25 million samples from the mathematical Q&A dataset from DeepMind . The dataset comes with two sets of tests: interpolation tests, one for each type of question occurring in the training set, and extrapolation tests, which measure generalization along various axes of difficulty beyond that seen during training. We provide more information about the dataset in Appendix A.4. We evaluate all five models on the two test sets of this dataset and report the accuracy (how often the model predicts the number exactly), as well as the Mean Absolute Error (MAE) and the R\({}^{2}\)-score. Since the dataset is skewed with some very high values, we perform a \(log_{10}\) transformation on the predicted and ground truth numbers before calculating MAE and R\({}^{2}\)-score.

All experiments except the one with xVal are built upon the T5 implementation and language modeling trainer based on the Hugging Face transformers library . We use the T5-base model as a pretrained base for our respective models. All models were trained for approximately one million steps with a batch size of 32 over a period of approximately 3 days. More details on the models' training hyperparameters can be found in Appendix A.7.

Table 1: Evaluation metrics on test data.

Figure 3: The heatmap plot shows the respective loss for a given combination of the class probabilities for token 3 and 5, where the ground truth is token 4. The behavior of the NTL-WAS is closest to the intuitive desired behavior of the loss function, while the NTL-MSE does not have a unique minimum.

Figure 2: CE, NTL-MSE, and NTL-WAS for different predicted number values with ground truth label . The underlying logit distribution over the simplified vocabulary (numbers 0-9) peaks at the respective predicted value and is uniform elsewhere.

The results can be seen in Table 1 and Figure 4. They show that vanilla T5 clearly benefits from both our loss variants. Indeed, accuracy increases by more than \(10\%\) for NTL-WAS compared to vanilla T5 in the interpolation tasks. The NTL-WAS was found to have the best performance across all three metrics and both interpolation and extrapolation tasks. This confirms our hypothesis that number representation in LMs can be effectively improved through a minor, architecture-agnostic modification of the loss function. The RT consistently surpasses vanilla T5 on interpolation, however no further benefit was found by augmenting RT tokenization with NTL-MSE, potentially due to the custom number embeddings conveying numerical proximity. The limited performance of xVal  is explained by the extensive range of numbers in the used dataset. The dynamic range of xVal is limited due to the combination of its scaling of the number token embeddings and the pre-layer-norm in the backbone . As a result, the effective number range of xVal is limited to [-5, 5]. To take this into account, we scale our dataset for xVal with \(log(1+x)\). However, this means that large numbers can no longer be adequately distinguished by the model, as their embeddings become very similar.

## 4 Conclusion

We introduced the Number Token Loss (NTL) for LMs to enhance their ability to handle numerical data by considering the numerical proximity between tokens. Our experiments unambiguously demonstrate the effectiveness of the NTL-WAS loss. This confirms our hypothesis that number representation in LMs can be effectively improved through a minor, architecture-agnostic modification of the loss function. By augmenting the standard CE loss with NTL, we provide a simple yet effective method that integrates seamlessly into existing architectures without requiring additional computational overhead. Experiments on the DeepMind Mathematics Dataset demonstrated that NTL significantly improves numerical reasoning, especially in models without specialized numerical embeddings. This approach offers a practical solution for enhancing language models in numerically rich domains, paving the way for more accurate and reliable applications in mathematics and science.

Figure 4: Comparison of evaluation metrics on interpolated and extrapolated test data.