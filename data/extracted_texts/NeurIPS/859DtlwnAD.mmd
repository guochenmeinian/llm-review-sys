# Pin-Tuning: Parameter-Efficient In-Context Tuning

for Few-Shot Molecular Property Prediction

 Liang Wang\({}^{1}\)\({}^{2}\)Qiang Liu\({}^{1}\)\({}^{2}\)\({}^{}\)Shaozhen Liu\({}^{1}\)Xin Sun\({}^{3}\)Shu Wu\({}^{1}\)\({}^{2}\)Liang Wang\({}^{1}\)\({}^{2}\)\({}^{3}\)

\({}^{1}\)New Laboratory of Pattern Recognition (NLPR)

State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS)

Institute of Automation, Chinese Academy of Sciences (CASIA)

\({}^{2}\) School of Artificial Intelligence, University of Chinese Academy of Sciences

\({}^{3}\) University of Science and Technology of China

liang.wang@cripac.ia.ac.cn, qiang.liu@nlpr.ia.ac.cn, liushaozhen2025@ia.ac.cn

sunxin000@mail.ustc.edu.cn, {shu.wu, wangliang}@nlpr.ia.ac.cn

###### Abstract

Molecular property prediction (MPP) is integral to drug discovery and material science, but often faces the challenge of data scarcity in real-world scenarios. Addressing this, few-shot molecular property prediction (FSMPP) has been developed. Unlike other few-shot tasks, FSMPP typically employs a pre-trained molecular encoder and a context-aware classifier, benefiting from molecular pre-training and molecular context information. Despite these advancements, existing methods struggle with the ineffective fine-tuning of pre-trained encoders. We attribute this issue to the imbalance between the abundance of tunable parameters and the scarcity of labeled molecules, and the lack of contextual perceptiveness in the encoders. To overcome this hurdle, we propose a parameter-efficient in-context tuning method, named Pin-Tuning. Specifically, we propose a lightweight adapter for pre-trained message passing layers (MP-Adapter) and Bayesian weight consolidation for pre-trained atom/bond embedding layers (Emb-BWC), to achieve parameter-efficient tuning while preventing over-fitting and catastrophic forgetting. Additionally, we enhance the MP-Adapters with contextual perceptiveness. This innovation allows for in-context tuning of the pre-trained encoder, thereby improving its adaptability for specific FSMPP tasks. When evaluated on public datasets, our method demonstrates superior tuning with fewer trainable parameters, improving few-shot predictive performance.1

## 1 Introduction

In the field of drug discovery and material science, molecular property prediction (MPP) stands as a pivotal task [5; 9; 63]. MPP involves the prediction of molecular properties like solubility and toxicity, based on their structural and physicochemical characteristics, which is integral to the development of new pharmaceuticals and materials. However, a major challenge encountered in real-world MPP scenarios is data scarcity. Obtaining extensive molecular data with well-characterized properties can be time-consuming and expensive. To address this, few-shot molecular property prediction (FSMPP) has emerged as a crucial approach, enabling predictions with limited labeled molecules [1; 41; 4].

The methodology for general MPP typically adheres to an encoder-classifier framework [71; 23; 27; 56], as illustrated in Figure 2(a). In this streamlined framework, the encoder converts molecularstructures into vectorized representations [12; 28; 50; 67; 2], and then the classifier uses these representations to predict molecular properties. In the context of few-shot scenarios, two significant discoveries have been instrumental in advancing this task. Firstly, _pre-trained molecular encoders_ have demonstrated consistent effectiveness in FSMPP tasks [20; 14; 58]. This indicates the utility of leveraging pre-acquired knowledge in dealing with data-limited scenarios. Secondly, unlike typical few-shot tasks such as image classification [57; 48], FSMPP tasks greatly benefits from _molecular context information_. This involves comprehending the seen many-to-many relationships between molecules and properties [58; 45; 73], as molecules are multi-labeled by various properties. These two discoveries have collectively led to the development of the widely used FSMPP framework that utilizes a pre-trained encoder followed by a context-aware classifier, as shown in Figure 2(b).

Despite the progress, there are observed limitations in the current approaches to FSMPP. Notably, while using a pre-trained molecular encoder generally outperforms training from scratch, fine-tuning the pre-trained encoder often leads to inferior results compared to keeping it frozen, which can be observed in Figure 1.

The observed ineffective fine-tuning can be attributed to two primary factors: (i) Imbalance between the abundance of tunable parameters and the scarcity of labeled molecules: fine-tuning all parameters of a pre-trained encoder with few labeled molecules leads to a disproportionate ratio of tunable parameters to available data. This imbalance often results in over-fitting and catastrophic forgetting [7; 6]. (ii) Limited contextual perceptiveness in the encoder: while molecular context is leveraged to enhance the classifier [58; 73], the encoder typically lacks the explicit capability to perceive this context, relying instead on implicit gradient-based optimization. This leads to the encoder not directly engaging with the nuanced molecular context information that is critical in FSMPP tasks. In summary, while significant strides have been made, the challenges of imbalance between the number of parameters and labeled data, along with the need for contextual perceptiveness in the encoder, necessitate more sophisticated methodologies in this domain.

Based on the aforementioned analysis, we propose the parameter-efficient in-context tuning method, named Pin-Tuning, to address the two primary challenges in FSMPP. To overcome the parameter-data imbalance, we propose a parameter-efficient chemical knowledge adaptation approach for pre-trained molecular encoders. A lightweight adapters (MP-Adapter) are designed to tune the pre-trained message passing layers efficiently. Additionally, we impose a Bayesian weight consolidation (Emb-BWC) on the pre-trained embedding layers to prevent aggressive parameter updates, thereby mitigating the risk of over-fitting and catastrophic forgetting. To address the second challenge, we further endow the MP-Adapter with the capability to perceive context. This innovation allows for in-context tuning of the pre-trained molecular encoders, enabling them to adapt more effectively to specific downstream tasks. Our approach is rigorously evaluated on public datasets. The experimental results demonstrate that our method achieves superior tuning performance with fewer trainable parameters, leading to enhanced performance in few-shot molecular property prediction.

The main contributions of our work are summarized as follows:

* We analyze the deficiencies of existing FSMPP approaches regarding the adaptation of pre-trained molecular encoders. The key issues include an imbalance between the number of tunable parameters and labeled molecules, as well as a lack of contextual perceptiveness in the encoders.
* We propose Pin-Tuning to adapt the pre-trained molecular encoders for FSMPP tasks. This includes the MP-Adapter for message passing layers and the Emb-BWC for embedding layers, facilitating parameter-efficient tuning of pre-trained molecular encoders.
* We further endow the MP-Adapter with the capability to perceive context to allows for in-context tuning, which provides more meaningful adaptation guidance during the tuning process.
* We conduct extensive experiments on benchmark datasets, which show that Pin-Tuning outperforms state-of-the-art methods on FSMPP by effectively tuning pre-trained molecular encoders.

Figure 1: Comparison of molecular encoders trained via different paradigms: train-from-scratch, pretrain-then-freeze, and pretrain-then-finetune. The evaluation is conducted across two datasets and three encoder architectures [20; 47; 66]. The results consistently demonstrate that while pretraining outperforms training from scratch, the current methods do not yet effectively facilitate finetuning.

Related work

**Few-shot molecular property prediction.** Few-shot molecular property prediction aims to accurately predict the properties of new molecules with limited training data . Early research applied general few-shot techniques to FSMPP. IterRefLSTM  is the pioneer work to leverage metric learning to solve FSMPP problem. Following this, Meta-GGNN  and Meta-MGNN  introduce meta-learning with graph neural networks, setting a foundational framework that subsequent studies have continued to build upon [39; 40; 4]. It is noteworthy that Meta-MGNN employs a _pre-trained molecular encoder_ and achieves superior results through fine-tuning in the meta-learning process compared to training from scratch. In fact, pre-trained graph neural networks [64; 36; 17; 54; 37] have shown promise in enhancing various graph-based downstream tasks [52; 13], including molecular property prediction [60; 62; 38; 72]. Recent efforts have shifted towards leveraging unique nature in FSMPP, such as the many-to-many relationships between molecules and properties arising from the multi-labeled nature of molecules, often referred to as the _molecular context_. PAR  initially employs graph structure learning [32; 55] to connect similar molecules through a homogeneous context graph. MHNs  introduces a large-scale external molecular library as context to augment the limited known information. GS-Meta  further incorporates auxiliary task to depict the many-to-many relationships.

**Parameter-efficient tuning.** As pre-training techniques have advanced, tuning of pre-trained models has become increasingly crucial. Traditional full fine-tuning approaches updates all parameters, often leading to high computational costs and the risk of over-fitting, especially when available data for downstream tasks are limited [33; 15]. This challenge has led to the emergence of parameter-efficient tuning [26; 29; 34]. The philosophy of parameter-efficient tuning is to optimize a small subset of parameters, reducing the computational costs while retaining or even improving performance on downstream tasks [19; 69]. Among the various strategies, the adapters [18; 42; 59] have gained prominence. Adapters are small modules inserted between the pre-trained layers. During the tuning process, only the parameters of these adapters are updated while the rest remains frozen, which not only improves tuning efficiency but also offers an elegant solution to the generalization [70; 30; 8]. By keeping the majority of the pre-trained parameters intact, adapters preserve the rich pre-trained knowledge. This attribute is particularly valuable in many real-world applications including FSMPP.

## 3 Preliminaries

### Problem formulation

Let \(\{\}\) be a collection of tasks, where each task \(\) involves the prediction of a property \(p\). The training set comprising multiple tasks \(\{_{}\}\), is represented as \(_{}=\{(m_{i},y_{i,t})|t\{_{}\}\}\), with \(m_{i}\) indicating a molecule and \(y_{i,t}\) its associated label for task \(t\). Correspondingly, the test set \(_{}\), formed by tasks \(\{_{}\}\), ensures a separation of properties between training and testing phases, as the property sets \(\{p_{}\}\) and \(\{p_{}\}\) are disjoint (\(\{p_{}\}\{p_{}\}=\)).

The goal of FSMPP is to train a model using \(_{}\) that can accurately infer new properties from a limited number of labeled molecules in \(_{}\). Episodic training has emerged as a promising strategy in meta-learning [10; 16] to deal with few-shot problem. Instead of retaining all \(\{_{}\}\) tasks in memory, episodes \(\{E_{t}\}_{t=1}^{B}\) are iteratively sampled throughout the training process. For each episode \(E_{t}\), a particular task \(_{t}\) is selected from the training set, along with corresponding support set \(_{t}\) and query set \(_{t}\). Typically, the prediction task involves classifying molecules into two classes: _positive_ (\(y=1\)) or _negative_ (\(y=0\)). Then a 2-way \(K\)-shot episode \(E_{t}=(_{t},_{t})\) is constructed. The support set \(_{t}=\{(m_{i}^{s},y_{i,t}^{s})\}_{i=1}^{2K}\) includes \(2K\) examples, each class contributing \(K\) molecules. The query set containing \(M\) molecules is denoted as \(_{t}=\{(m_{i}^{q},y_{i,t}^{q})\}_{i=1}^{M}\).

### Encoder-classifier framework for FSMPP

Encoder-classifier framework is widely adopted in FSMPP methods. As illustrated in Figure 2(a), given a molecule \(m\) whose property need to be predicted, a molecular encoder \(f()\) first learns the molecule's representation based on its structure, i.e., \(_{m}=f(m)^{d}\). The molecule \(m\) is generally represented as a graph \(m=(,,,)\), where \(\) denotes the nodes (atoms), \(\) represents the adjacent matrix defined by edges (chemical bonds), and \(,\) denote the original feature of atomsand bonds, then graph neural networks (GNNs) are employed as the molecular encoders [44; 51; 21]. Subsequently, the learned molecular representation is fed into a classifier \(g()\) to obtain the prediction \(=g(_{m})\). The model is trained by minimizing the discrepancy between \(\) and the ground truth \(y\).

Further, two key discoveries have been pivotal for FSMPP. The first is the proven effectiveness of pre-trained molecular encoders, while the second is the significant advantage gained from molecular context. Together, these discoveries have further reshaped the widely adopted FSMPP framework, which combines a _pre-trained encoder_ followed by a _context-aware classifier_, as shown in Figure 2(b).

### Pre-trained molecular encoders (PMEs)

Due to the scarcity of labeled data in molecular tasks, molecular pre-training has emerged as a crucial area, which involves training encoders on extensive molecular datasets to extract informative representations. Pre-GNN  is a classic pre-trained molecular encoder that has been widely used in addressing FSMPP tasks [14; 58; 73]. The backbone of Pre-GNN is a modified version of Graph Isomorphism Network (GIN)  tailored to molecules, which we call GIN-Mol, consisting of multiple atom/bond embedding layers and message passing layers.

**Atom/Bond embedding layers.** The raw atom features and bond features are both categorical vectors, denoted as \((i_{v,1},i_{v,2},,i_{v,|E_{n}|})\) and \((j_{e,1},j_{e,2},,j_{e,|E_{e}|})\) for atom \(v\) and bond \(e\), respectively. These categorical features are embedded as:

\[_{v}^{(0)}=_{a=1}^{|E_{n}|}_{a}(i_{v,a}), _{e}^{(l)}=_{b=1}^{|E_{e}|}_{b}^{(l) }(j_{e,b}),\] (1)

where \(_{a}()_{a\{1,,|E_{n}|\}}\) and \(_{b}()_{b\{1,,|E_{e}|\}}\) represent embedding operations that map integer indices to \(d\)-dimensional real vectors, i.e., \(_{v}^{(0)},_{e}^{(l)}^{d}\), \(l\{0,1,,L-1\}\) represents the index of encoder layers, and \(L\) is the number of encoder layers. The atom embedding layer is present only in the first encoder layer, while an bond embedding layer exists in each layer.

**Message passing layers.** At the \(l\)-th encoder layer, atom representations are updated by aggregating the features of neighboring atoms and chemical bonds:

\[_{v}^{(l)}=(^{(l)}(_{ u}_{u}^{(l-1)}+_{e=(v,u)}_{e}^{(l-1)})),\] (2)

where \(u(v)\{v\}\) is the set of atoms connected to \(v\), and \(_{v}^{(l)}^{d}\) is the learned representation of atom \(v\) at the \(l\)-th layer. \(()\) is implemented by 2-layer neural networks, in which the hidden dimension is \(d_{1}\). After \(\), batch normalization is applied right before the \(\). The molecule-level representation \(_{m}^{d}\) is obtained by averaging the atom representations at the final layer.

## 4 The proposed Pin-Tuning method

This section delves into our motivation and proposed method. Our framework for FSMPP is depicted in Figure 2(c). The details of our principal design, Pin-Tuning for PMEs, is present in Figure 2(d).

As shown in Figure 1, pretraining then finetuning molecular encoders is a common approach. However, fully fine-tuning yields results inferior to simply freezing them. Thus, the following question arises:

_How to effectively adapt pre-trained molecular encoders to downstream tasks, especially in few-shot scenarios?_

We analyze the reasons of observed ineffective fine-tuning issue, and attribute it to two primary factors: (i) imbalance between the abundance of tunable parameters and the scarcity of labeled molecules, and (ii) limited contextual perceptiveness in the encoder.

### Parameter-efficient tuning for PMEs

To address the first cause of observed ineffective tuning, we reform the tuning method for PMEs. Instead of conducting full fine-tuning for all parameters, we propose tuning strategies specifically tailored to the message passing layers and embedding layers in PMEs, respectively.

#### 4.1.1 MP-Adapter: message passing layer-oriented adapter

For message passing layers in PMEs, the number of parameters is disproportionately large compared to the training samples. To mitigate this imbalance, we design a lightweight adapter targeted at the message passing layers, called MP-Adapter. The pre-trained parameters in each message passing layer include parameters in the MLP and the following batch normalization. We freeze all pre-trained parameters in message passing layers and add a lightweight trainable adapter after MLP in each message passing layer. Formally, the adapter module for \(l\)-th layer can be represented as:

\[_{v}^{(l)}=_{}(_{v}^{(l)}) ^{d_{2}},\] (3)

\[_{v}^{(l)}=_{}((_{v}^{(l) }))^{d},\] (4)

\[}_{v}^{(l)}=(_{v}^{(l)}+_{v}^ {(l)})^{d},\] (5)

where \(()\) denotes feed forward layer and \(()\) denotes layer normalization. To limit the number of parameters, we introduce a bottleneck architecture. The adapters downscale the original features from \(d\) dimensions to a smaller dimension \(d_{2}\), apply nonlinearity \(\), then upscale back to \(d\) dimensions. By setting \(d_{2}\) smaller than \(d\), we can limit the number of parameters added. The adapter module has a skip-connection internally. With the skip-connection, we adopt the near-zero initialization for parameters in the adapter modules, so that the modules are initialized to approximate identity functions. Therefore, the encoder with initialized adapters is equivalent to the pre-trained encoder. Furthermore, we add a layer normalization after skip-connection for training stability.

#### 4.1.2 Emb-BWC: embedding layer-oriented Bayesian weight consolidation

Unlike message passing layers, embedding layers contain fewer parameters. Therefore, we directly fine-tune the parameters of the embedding layers, but impose a constraint to limit the magnitude of parameter updates, preventing aggressive optimization and catastrophic forgetting.

The parameters in an embedding layer consist of an embedding matrix used for lookups based on the indices of the original features. We stack the embedding matrices of all embedding layers to form \(^{E d}\), where \(E\) represents the total number of lookup entries. Further, \(_{i}^{d}\) denotes the \(i\)-th row's embedding vector, and \(_{i,j}\) represents the \(j\)-th dimensional value of \(_{i}\).

To avoid aggressive optimization of \(\), we derive a Bayesian weight consolidation framework tailored for embedding layers, called Emb-BWC, by applying Bayesian learning theory  to fine-tuning.

**Proposition 1**:: _(Emb-BWC ensures an appropriate stability-plasticity trade-off for pre-trained embedding layers.) Let \(^{E d}\) be the pre-trained embeddings before fine-tuning, and \(^{}^{E d}\) be the fine-tuned embeddings. Then, the embeddings can both retain the atom and bond properties

Figure 2: (a) The vanilla encoder-classifier framework for MPP. (b) The framework widely adopted by existing FSMPP methods, which contains a pre-trained molecular encoder and a context-aware property classifier. (c) Our proposed framework for FSMPP, in which we introduce a Pin-Tuning method to update the pre-trained molecular encoder followed by the property classifier. (d) The details of our proposed Pin-Tuning method for pre-trained molecular encoders. In (b) and (c), we use the property names like SR-HSE to denote the molecular context in episodes.

obtained from pre-training and be appropriately updated to adapt to downstream FSMPP tasks, by introducing the following Emb-BWC loss into objective during the fine-tuning process:_

\[_{}=-_{i=1}^{E}(_{i}^{}-_{ i})^{}(_{},_{i})(_{i}^{}-_{ i}),\] (6)

_where \((_{},_{i})^{d d}\) is the Hessian of the log likelihood \(_{}\) of pre-training dataset \(_{}\) at \(_{i}\)._

Details on the theoretical derivation of Eq. (6) are given in Appendix A. Since \((_{},_{i})\) is intractable to compute due to the great dimensionality of \(\), we adopt the diagonal approximation of Hessian. By approximating \(\) as a diagonal matrix, the \(j\)-th value on the diagonal of \(\) can be considered as the importance of the parameter \(_{i,j}\). The following three choices are considered.

_Identity matrix._ When using the identity matrix to approximate the negation of \(\), Eq. (6) is simplified to \(_{}^{}=_{i=1}^{E}_{j=1}^{d }(_{i,j}^{}-_{i,j})^{2}\), assigning equal importance to each parameter. This loss function is also known as L2 penalty with pre-trained model as the starting point (L2-SP) .

_Diagonal of Fisher information matrix._ The Fisher information matrix (FIM) \(\) is the negation of the expectation of the Hessian over the data distribution, i.e., \(=-_{_{}}[]\), and the FIM can be further simplified with a diagonal approximation. Then, the Eq. (6) is simplified to \(_{}^{}=_{i=1}^{E}}_{i}(_{i}^{}-_{i})^{2}\), where \(}_{i}^{d}\) is the diagonal of \((_{},_{i})^{d d}\) and the \(j\)-th value in \(}_{i}\) is computed as \(_{_{}}(_{}/ _{i,j})^{2}\). This is equivalent to elastic weight consolidation (EWC) .

_Diagonal of embedding-wise Fisher information matrix._ In different property prediction tasks, the impact of the same atoms and inter-atomic interactions may be significant or negligible. Therefore, we propose this choice to assign importance to parameters based on different embeddings, rather than treating each parameter individually. By defining \(_{i}=_{j}_{i,j}\), the total update of the embedding \(_{i}\) can be represented as \(_{i}=_{i}^{}-_{i}=_{j}(_{i,j} ^{}-_{i,j})\). Then, the Eq. (6) is reformulated to \(_{}^{}=_{i=1}^{E}}_{i}(_{i}^{}-_{i})^{2}\), where \(}_{i}=_{j}_{_{}}( _{}/_{i,j})^{2}\).

Detailed derivation is given in Appendix A. Intuitively, these three approximations employ different methods to assign importance to parameters. \(_{}^{}\) assigns the same importance to each parameter, \(_{}^{}\) assigns individual importance to each parameter, and \(_{}^{}\) assigns the same importance to parameters within the same embedding vector.

### Enabling contextual perceptiveness in MP-Adapter

For different property prediction tasks, the decisive substructures vary. As shown in Figure 2, the ester group in the given molecule determines the property SR-HSE, while the carbon-carbon triple bond determines the property SR-MMP. If fine-tuning can be guided by molecular context, encoding context-specific molecular representations allows for dynamic representations of molecules tailored to specific tasks and enables the modeling of the context-specific significance of substructures.

**Extracting molecular context information.** In each episode, we consider the labels of the support molecules on the target property and seen properties, as well as the labels of the query molecules on seen properties, as the context of this episode. We adopt the form of a graph to describe the context. Figure 3 demonstrates the transformation from original context data to a context graph. In the left table, the labels of molecules \(m_{1}^{q},m_{2}^{2}\) for property \(p_{t}\) are the prediction targets, and the other shaded values are the available context. The right side shows the context graph constructed based on the available context. Specifically, we construct context graph \(_{t}=(_{t},_{t},_{t})\) for episode \(E_{t}\). It contains \(M\) molecule nodes \(\{m\}\) and \(P\) property nodes \(\{p\}\). Three types of edges indicate different relationships between molecules and properties.

Then we employ a GNN-based context encoder: \(=(_{t},_{t},_{t})\), where \(^{(M+P) d_{2}}\) denotes the learned context representation matrix for \(E_{t}\). \(_{t}\) and \(_{t}\) denote the node set

Figure 3: Convert the context information of a 2-shot episode into a context graph.

and the adjacent matrix of the context graph, respectively, and \(_{t}\) denotes the initial features of nodes. The features of molecule nodes are initialized with a pre-trained molecular encoder. The property nodes are randomly initialized. When we make the prediction of molecule \(m\)'s target property \(p\), we take the learned representations of the this molecule \(_{m}\) and of the target property \(_{p}\) as the context vectors. Details about the context encoder are provided in Appendix F.2.

**In-context tuning with molecular context information.** After obtaining the context vectors, we consider enabling the molecular encoder to use the context as a condition, achieving conditional molecular encoding. To achieve this, we further refine our adapter module. While neural conditional encoding has been explored in some domains, such as cross-attention  and ControlNet  for conditional image generation, these methods often come with a significant increase in the number of parameters. This contradicts our motivation of parameter-efficient tuning for few-shot tasks. In this work, we adopt a simple yet effective method. We directly concatenate the context with the output of the message passing layer, and feed them into the downscaling feed-forward layer in the MP-Adapter. Formally, the downscaling process defined in Eq. (3) is reformulated as:

\[^{(l)}=_{}(_{v}^{(l)}||_{ m}\|_{p}),\] (7)

where \(\|\) denotes concatenation. Such learned molecular representations are more easily predicted on specific properties, verified in Section 5.5 and Appendix G.

### Optimization

Following MAML , a gradient descent strategy is adopted. Firstly, \(B\) episodes \(\{E_{t}\}_{t=1}^{B}\) are randomly sampled. For each episode, in the inner-loop optimization, the loss on the support set is computed as \(_{t,}^{cls}(f_{})\) and the parameters \(\) are updated by gradient descent:

\[_{t,}^{cls}(f_{})=-_{ _{t}}(y()+(1-y)(1-)),\] (8) \[^{}-_{inner}_{} _{t,}^{cls}(f_{}),\] (9)

where \(_{inner}\) is the learning rate. In the outer loop, the classification loss of query set is denoted as \(_{t,}^{cls}\). Together with our Emb-BWC regularizer, the meta-training loss \((f_{^{}})\) is computed and we do an outer-loop optimization with learning rate \(_{outer}\) across the mini-batch:

\[(f_{^{}})=_{t=1}^ {B}_{t,}^{cls}(f_{^{}})+_ {},\] (10) \[-_{outer}_{}( f_{^{}}),\] (11)

where \(\) is the weight of Emb-BWC regularizer. The pseudo-code is provided in Appendix B. We also provide more discussion of tunable parameter size and total model size in Appendix C.

## 5 Experiments

### Evaluation setups

**Datasets.** We use five common few-shot molecular property prediction datasets from the MoleculeNet : Tox21, SIDER, MUV, ToxCast, and PCBA. Standard data splits for FSMPP are adopted. Dataset statistics and more details of datasets can be found in Appendix D.

**Baselines.** For a comprehensive comparison, we adopt two types of baselines: (1) methods with molecular encoders trained from scratch, including Siamese Network , ProtoNet , MAML , TPN , EGNN , and IterRefLSTM ; and (2) methods which leverage pre-trained molecular encoders, including Pre-GNN , Meta-MGNN , PAR , and GS-Meta . More details about these baselines are in Appendix E.

**Metrics.** Following prior works [1; 58], ROC-AUC scores are calculated on the query set for each meta-testing task, to evaluate the performance of FSMPP. We run experiments 10 times with different random seeds and report the mean and standard deviations.

[MISSING_PAGE_FAIL:8]

For Emb-BWC, we verify the effectiveness of fine-tuning the embedding layers and regularizing them with different approximations of \(_{}\) (Table 3). Since the embedding layers have relatively few parameters, direct fine-tuning can also enhance performance. Applying our proposed regularizers to fine-tuning can further improve the effects. Among the three regularizers, the \(_{}^{}\) is the most effective. This indicates that keeping pre-trained parameters to some extent can better utilize pre-trained knowledge, but the parameters worth keeping in fine-tuning and the important parameters in pre-training revealed by Fisher information matrix are not completely consistent.

### Sensitivity analysis

**Effect of weight of Emb-BWC regularizer \(\).** Emb-BWC is applied on the embedding layers to limit the magnitude of parameter updates during fine-tuning. We vary the weight of this regularization \(\) from \(\{0.01,0.1,1,10\}\). The first subfigure in Figure 5 shows that the performance is best when \(=0.1\) or \(1\). When \(\) is too small, the parameters undergo too large updates on few-shot downstream datasets, leading to over-fitting and ineffectively utilizing the pre-trained knowledge. Too large \(\) causes the parameters of the embedding layers to be nearly frozen, which prevents effective adaptation.

**Effect of hidden dimension of MP-Adapter \(d_{2}\).** The results corresponding to different values of \(d_{2}\) from \(\{25,50,75,100,150\}\) are presented in the second subfigure of Figure 5. On the Tox21 dataset, we further analyze the impact of this hyper-parameter on the number of trainable parameters. As shown in Figure 5, the number of parameters that our method needs to train is significantly less than that required by the full fine-tuning method, such as GS-Meta, while our method also performs better in terms of ROC-AUC performance due to solving over-fitting and context perceptiveness issues. When \(d=50\), Pin-Tuning performs best on Tox21, and the number of parameters that need to train is only 14.2% of that required by traditional fine-tuning methods.

### Case study

We visualized the molecular representations learned by the GS-Meta and our Pin-Tuning's encoders in the 10-shot setting, respectively. As shown in Figure 6 and 7, Pin-Tuning can effectively adapt to different downstream tasks based on context information, generating property-specific molecular representations. Across different tasks, our method is more effective in eno

Figure 4: Effect of different hyper-parameters. The y-axis represents ROC-AUC scores (%) and the x-axis is the different hyper-parameters.

Figure 5: ROC-AUC (%) and number of trainable parameters of Pin-Tuning with varied value of \(d_{2}\) and full Fine-Tuning method (e.g., GS-Meta) on the Tox21 dataset.

   Fine-tune & Regularizer & Tox21 & SIDER & MUV & PCBA \\  - & - & 89.70 & 90.12 & 70.76 & 80.24 \\  ✓ & - & 90.17 & 92.06 & 72.37 & 80.74 \\ ✓ & \(_{}^{}\) & **91.56** & **93.41** & **73.22** & **81.26** \\ ✓ & \(_{}^{}\) & 90.93 & 90.09 & 72.17 & 80.78 \\ ✓ & \(_{}^{}\) & 91.32 & 90.31 & 72.78 & 81.22 \\   

Table 3: Ablation analysis on the Emb-BWC.

facilitate the prediction of the current property, reducing the difficulty of property prediction from the encoding representation aspect. More case studies are provided in Appendix G.

## 6 Conclusion

In this work, we propose a tuning method, Pin-Tuning, to address the ineffective fine-tuning of pre-trained molecular encoders in FSMPP tasks. Through the innovative parameter-efficient tuning and in-context tuning for pre-trained molecular encoders, our approach not only mitigates the issues of parameter-data imbalance but also enhances contextual perceptiveness. The promising results on public datasets underscore the potential of Pin-Tuning to advance this field, offering valuable insights for future research in drug discovery and material science.