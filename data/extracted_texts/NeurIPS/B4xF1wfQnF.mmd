# Optimal Time Complexities of

Parallel Stochastic Optimization Methods

Under a Fixed Computation Model

Alexander Tyurin

KAUST

Saudi Arabia

alexandertiurin@gmail.com

&Peter Richtarik

KAUST

Saudi Arabia

richtarik@gmail.com

###### Abstract

Parallelization is a popular strategy for improving the performance of iterative algorithms. Optimization methods are no exception: design of efficient parallel optimization methods and tight analysis of their theoretical properties are important research endeavors. While the minimax complexities are well known for sequential optimization methods, the theory of parallel optimization methods is less explored. In this paper, we propose a new protocol that generalizes the classical oracle framework approach. Using this protocol, we establish _minimax complexities for parallel optimization methods_ that have access to an unbiased stochastic gradient oracle with bounded variance. We consider a fixed computation model characterized by each worker requiring a fixed but worker-dependent time to calculate stochastic gradient. We prove lower bounds and develop optimal algorithms that attain them. Our results have surprising consequences for the literature of _asynchronous_ optimization methods.

## 1 Introduction

We consider the nonconvex optimization problem

\[_{x Q}f(x):=_{}[f(x;) ]}, \]

where \(f\,:\,^{d}_{}\), \(Q^{d}\), and \(\) is a random variable with some distribution \(\) on \(_{}\). In machine learning, \(_{}\) could be the space of all possible data, \(\) is the distribution of the training dataset, and \(f(,)\) is the loss of a data sample \(\). In this paper we address the following natural setup:

1. \(n\) workers are available to work in parallel,
2. the \(i^{}\) worker requires \(_{i}\) seconds1 to calculate a stochastic gradient of \(f\). 
The function \(f\) is \(L\)-smooth and lower-bounded (see Assumptions 7.1-7.2), and stochastic gradients are unbiased and \(^{2}\)-variance-bounded (see Assumption 7.3).

### Classical theory

In the nonconvex setting, gradient descent (GD) is an optimal method with respect to the number of gradient (\( f\)) calls (Lan, 2020; Nesterov, 2018; Carmon et al., 2020) for finding an approximately stationary point of \(f\). Obviously, a key issue with GD is that it requires access to the exact gradients\( f\) of the function \(f.\) However, in many practical applications, it can be infeasible to calculate the gradient of \([f(;)]\) analytically. Moreover, even if this is possible, e.g., if the distribution \(\) is described by \(m\) possible samples, so that \(_{}[f(;)]=(}{{m}}) _{i=1}^{m}(;_{i}),\,m\) can be huge (Krizhevsky et al., 2017), and gradient evaluation can be arbitrarily expensive.

**Stochastic Gradient Descent.** Due to the above-mentioned problem, machine learning literature is preoccupied with the study of algorithms that can work with stochastic gradients instead (Lan, 2020; Ghadimi and Lan, 2013). For all \(x^{d},\) we assume that the \(n\) workers have access to independent, unbiased, and \(^{2}\)-variance-bounded stochastic gradients \(f(x,)\) (see Assumption 7.3), where \(\) is a random sample from \(\). Under such assumptions, **with one worker**, stochastic gradient descent (SGD), i.e., the method \(x^{k+1}=x^{k}-f(x^{k};^{k}),\) where \(^{k}\) are i.i.d. random samples from \(\), is known to be optimal with respect to the number of stochastic gradient calls (Ghadimi and Lan, 2013; Arjevani et al., 2022). SGD guarantees convergence to an \(\)-stationary point in expectation after \((}{{}}+L }}{{^{2}}})\) stochastic gradient evaluations, where \(:=f(x^{0})-f^{*}\) and \(x^{0}^{d}\) is a starting point.

### Parallel optimization methods

Using the bounds from Section 1.1, one can easily _estimate_ the performance of these algorithms in real systems. For instance, if it takes \(_{1}\)_seconds_ to calculate a stochastic gradient **with one worker**, then SGD guarantees to return a solution after

\[(_{1}(+L }{^{2}}))\]

seconds. If instead of a single worker we can access \(n\) workers that can calculate stochastic gradients in parallel, we can consider the following classical parallel methods:

**Minibatch SGD.** The minibatch SGD method (Minibatch SGD), i.e., the iterative process

\[x^{k+1}=x^{k}-_{i=1}^{n}f(x^{k};_{i}^ {k}),\]

where \(\) is a stepsize, \(_{i}^{k}\) are i.i.d. samples from \(\), and the gradients \(f(x^{k};_{i}^{k})\) are calculated in parallel. This method converges after \((}{{}}+L }}{{n^{2}}})\) iterations (Cotter et al., 2011; Goyal et al., 2017; Gower et al., 2019) and after

\[(_{}(+L }{n^{2}})) \]

seconds, where \(_{}:=_{i[n]}_{i}\) is the processing time associated with the _slowest_ machine2.

Although the time complexity (2) of Minibatch SGD improves with the number of workers \(n\), in general, this does _not_ guarantee better performance due to the delay \(_{}\). In real systems, parallel computations can be very chaotic, e.g., they can be slow due to inconsistent network communications, or GPU computation delays (Dutta et al., 2018; Chen et al., 2016).

**Asynchronous SGD.** We now consider the asynchronous SGD method (Asynchronous SGD) (Recht et al., 2011; Nguyen et al., 2018; Arjevani et al., 2020; Feyzmahdavian et al., 2016) described by

1. Receive \(f(x^{k-_{k}};^{k-_{k}})\) from a worker,
2. \(x^{k+1}=x^{k}-^{k}f(x^{k-_{k}};^{k-_{k}}),\)
3. Ask the worker to calculate \(f(x^{k+1};^{k+1}),\)

where \(^{k}\) are i.i.d. samples from \(\), and \(_{k}\) are gradient iteration delays. This is an _asynchronous_ method: the workers work independently, finish calculations of stochastic gradients with potentially large and chaotic delays \(_{k}\), and the result of their computation is applied as soon as it is ready, without having to wait for other workers. Asynchronous SGD was also considered in the heterogeneous setting (see details in Section A.2).

Cohen et al. (2021); Mishchenko et al. (2022); Koloskova et al. (2022) provide the current state-of-the-art analysis of Asynchronous SGD. In particular, they prove that Asynchronous SGD converges after \((}{{}}+L }}{{^{2}}})\) iterations. To show the superiority of Asynchronous SGD, Mishchenko et al. (2022) consider the following _fixed computation model_: the \(i^{}\) worker requires \(_{i}\) seconds to calculate stochastic gradients. In this setting, Asynchronous SGD converges after

\[((_{i=1}^{n}})^{-1} (+L}{n^{2}} )) \]

seconds (we reprove this fact in Section L). Thus, Asynchronous SGD can be \(}{{n}}_{i=1}^{n}}}}{{_{i}}}\) times faster than Minibatch SGD.

Besides Asynchronous SGD, many other strategies utilize parallelization (Dutta et al., 2018; Woodworth et al., 2020; Wu et al., 2022), and can potentially improve over Minibatch SGD.

## 2 Problem and Contribution

In this paper, we seek to find the optimal time complexity in the setting from Section 1: our goal is to provide a lower bound and a method that attains it. Our main contributions are:

1. **Lower bound.** In Sections 4 & 5, we define new Protocols 2 & 3, and a new complexity measure \(_{}\) (see (6)), which we believe are more appropriate for the analysis of parallel optimization algorithms. In Section 6, we prove the time complexity lower bound for (nonconvex) functions and algorithms that work with parallel asynchronous oracles.
2. **Optimal method.** In Section 7, we develop a minimax optimal method--Rennala3 SGD--that attains this lower bound.

In addition, we investigate several other related questions. As an independent result, in Section 8 we prove that all methods which synchronize workers in each iteration (e.g., Minibatch SGD) have _provably_ worse time complexity than asynchronous methods (e.g., Rennala SGD (see Method 4), Asynchronous SGD). In Section A, we extend our theory to the _heterogeneous_ case, in which the workers have access to different distributions (datasets), and provide a lower bound and a new method that attains it. In Section B, we provide the optimal time complexities in the _convex_ setting.

    \\   &  \\  Minibatch SGD & \(_{n}(+L}{n^ {2}})\) \\  Asynchronous SGD (Cohen et al., 2021) & \((_{i=1}^{n}})^{-1}( +L}{n^{2}})\) \\ (Mishchenko et al., 2022) & \((_{i=1}^{n}})^{-1}( +L}{n^{2}})\) \\  
 Rennala SGD (Theorem 7.5) \\  & \(_{m[n]}[(_{i=1}^{n}})^{-1}(+L}{n ^{2}})]\) \\   Lower Bound (Theorem 6.4) & \(_{m[n]}[(_{i=1}^{n}})^{-1}(+L}{n ^{2}})]\) \\   

Table 1: **Homogeneous and Heterogeneous Case.** The required time to get an \(\)-stationary point (\([\| f()\|^{2}]\)) in the nonconvex setting, where \(i^{}\) worker requires \(_{i}\) seconds to calculate a stochastic gradient. We assume that \(0<_{1}_{n}\).

Classical Oracle Protocol

Let us recall the classical approach to obtaining lower bounds for optimization algorithms. We need to define a _function class_\(\), an _oracle class_\(\), and an _algorithm class_\(\). We then analyze the complexity of an algorithm \(A=\{A^{k}\}_{k=0}^{}\), using the following protocol:

```
1:Input: function \(f\), oracle and distribution \((O,)(f)\), algorithm \(A\)
2:for\(k=0,,\)do
3:\(x^{k}=A^{k}(g^{1},,g^{k})\)\(\)\(x^{0}=A^{0}\) for \(k=0\).
4:\(g^{k+1}=O(x^{k},^{k+1}),^{k+1}\)
5:endfor
```

**Protocol 1** Classical Oracle Protocol

More formally, in first-order stochastic optimization, the oracle class \(\) returns a random mapping \(O\ :\ ^{d}_{}^{d}\) based on a function \(f\) and a distribution \(\); we use the notation \((O,)(f)\). An algorithm \(A=\{A^{k}\}_{k=0}^{}\) is a sequence such that

\[A^{k}\,:\,^{d}^{d}}_{}^{d}\ \  k 1,A^{0}^{d}. \]

Typically, an oracle \(O\) returns an unbiased stochastic gradient that satisfies Assumption 7.3: \(O(x,)=f(x;)\) for all \(x^{d}\) and \(_{}\). Let us fix an oracle class \(\). Then, in the nonconvex first-order stochastic setting, we analyze the complexity measure

\[_{}(,):=_{A }_{f}_{(O,)(f)} \{k\,\,[\| f(x^{k})\|^ {2}]\}, \]

where the sequence \(\{x^{k}\}_{k}\) is generated by Protocol 1. Virtually all previous works are concerned with lower bounds of optimization problems using Protocol 1 and the complexity measure (5) (Nemirovskij and Yudin, 1983; Carmon et al., 2020; Arjevani et al., 2022; Nesterov, 2018).

## 4 Time Oracle Protocol

In the previous sections, we discuss the classical approach to estimating the complexities of algorithms. Briefly, these approaches seek to quantify the worst-case number of iterations or oracle calls that are required to find a solution (see (5)), which is very natural for _sequential_ methods. However, and this is a key observation of our work, this approach is not convenient if we want to analyze _parallel_ methods. We now propose an alternative protocol that can be more helpful in this situation:

```
1:Input: functions \(f\), oracle and distribution \((O,)(f)\), algorithm \(A\)
2:\(s^{0}=0\)
3:for\(k=0,,\)do
4:\((t^{k+1},x^{k})=A^{k}(g^{1},,g^{k})\), \( t^{k+1} t^{k}\)
5:\((s^{k+1},g^{k+1})=O(t^{k+1},x^{k},s^{k},^{k+1}),^{k+1}\)
6:endfor
```

**Protocol 2** Time Oracle Protocol

Protocol 2 is almost identical to Protocol 1 except for one key detail: Protocol 2 requires the algorithms to return a sequence \(\{t^{k+1}\}_{k=1}^{}\) such that \(t^{k+1} t^{k} 0\) for all \(k 0\). We assume that \(t^{0}=0\). We also assume that the oracles take to the input the states \(s^{k}\) and output them (the role of these states will be made clear later). In this case, we provide the following definition of an algorithm.

**Definition 4.1**.: An algorithm \(A=\{A^{k}\}_{k=0}^{}\) is a sequence such that

\[A^{k}\,:\,^{d}^{d}}_{}_{ 0}^{d} k 1,A^{0} _{ 0}^{d},\]and, for all \(k 1\) and \(g^{1},,g^{k}^{d}\), \(t^{k+1} t^{k},\) where \(t^{k+1}\) and \(t^{k}\) are defined as \((t^{k+1},)=A^{k}(g^{1},,g^{k})\) and \((t^{k},)=A^{k-1}(g^{1},,g^{k-1}).\)

Let us explain the role of the sequence \(\{t^{k}\}_{k}\). In Protocol 1, an algorithm outputs a point \(x^{k}\) and then asks the oracle: _Provide me a gradient at the point \(x^{k}\)_. In contrast, in Protocol 2 an algorithm outputs a point \(x^{k}\) and a time \(t^{k+1}\), and asks the oracle: _Start calculating a gradient at the point \(x^{k}\) at a time \(t^{k+1}\)_. We have a constraint that \(t^{k+1} t^{k}\) for all \(k 0,\) which means that the algorithm is not allowed to travel into the past.

Using Protocol 2, we propose to use another complexity measure instead of (5):

\[&_{}(, ):=_{A}_{f}_{( ,)(f)}\{t 0|\, [_{k S_{t}}\| f(x^{k})\|^{2}] .\},\\ & S_{t}:=\{k_{0}t^{k} t\},  \]

where the sequences \(t^{k}\) and \(x^{k}\) are generated by Protocol 2. In (5), we seek to find the _worst-case number of iterations_\(k\) required to get \([\| f(x^{k})\|^{2}]\) for any \(A.\) In (6), we seek to find the _worst-case case time_\(t\) required to find an \(\)-stationary point for any \(A.\)

We now provide an example, considering an oracle that calculates a stochastic gradient in \(\) seconds. Let us define the appropriate oracle for this problem:

\[O_{}^{ f}\,:\,_{ 0}}_{} ^{d}}_{}_{ 0} ^{d}\{0,1\})}_{}_{ }_{ 0}^{d}\{0,1\})}_{ }^{d}\]

\[O_{}^{ f}(t,x,(s_{t},s_{x},s_{q}),)=((t,x,1),& 0),&s_{q}=0,\\ ((s_{t},s_{x},1),&0),&s_{q}=1t<s_{t}+,\\ ((0,0,0),&f(s_{x};)),&s_{q}=1t s_{t}+,  \]

and \(f\) is a mapping such that \(f\,:\,^{d}_{}^{d}.\) Further, we additionally assume that \(f\) is an unbiased \(^{2}\)-variance-bounded stochastic gradient (see Assumption 7.3).

Note that the oracle \(O_{}^{ f}\) emulates the behavior of a real worker. Indeed, the oracle can return three different outputs. If \(s_{q}=0,\) it means that the oracle has been idle, then "starts the calculation" of the gradient at the point \(x,\) and changes the state \(s_{q}\) to \(1.\) Also, using the state, it remembers the time moment \(t\) when the calculation began and the point \(x.\) Next, if \(s_{q}=1\) and \(t<s_{t}+,\) it means the oracle is still calculating the gradient, so if an algorithm sends time \(t\) such that \(t<s_{t}+,\) then it receives the zero vector. Finally, if \(s_{q}=1,\) as soon as an algorithm sends time \(t\) such that \(t s_{t}+,\) then the oracle will be ready to provide the gradient. Note that the oracle provides the gradient calculated at the point \(x\) that was requested when the oracle was idle. Thus, the time between the request of an algorithm to get the gradient and the time when the algorithm gets the gradient is at least \(\) seconds.

In Protocol 2, we have a game between an algorithm \(A\) and an oracle class \(,\) where algorithms can decide the sequence of times \(t^{k}.\) Thus, an algorithm wants to find enough information from an oracle as soon as possible to obtain \(\)-stationary point.

Let us consider an example. For the oracle class \(\) that generates the oracle from (7), we can define the SGD method in the following way. We take any starting point \(x^{0}^{d},\) a step size \(=\{}{{L}},}{{2L^{2}}}\}\) (see Theorem D.8) and define \(A^{k}\,:\,^{d}^{d})}_{k}_{ 0}^{d}\) such that

\[A^{k}(g^{1},,g^{k})= k/2,x ^{0}-_{j=1}^{k}g^{k},&k\,\,(\,2)=0,\\ ( k/2+1),0,&k\,\,(\,2)=1, \]

for all \(k 1,\) and \(A^{0}=(0,x^{0}).\) Let us explain the behavior of the algorithm. In the first step of Protocol 2, when \(k=0,\) the algorithm requests the gradient at the point \(x^{0}\) at the time \(t^{1}=0\) since \(A^{0}=(0,x^{0})\). The oracle \(O\) changes the state from \(s^{0}_{q}=0\) to \(s^{1}_{q}=1\) and remembers the point \(x^{0}\) in the state \(s^{1}_{x}\). In the second step of the protocol, when \(k=1\), the algorithm calls the oracle at the time \((}{{2}}+1)=\). In the oracle, the condition \(t^{2} s^{1}_{t}+ 0+\) is satisfied, and it returns the gradient at the point \(x^{0}\). Note that this can only happen if an algorithm does the second call at a time that is greater or equal to \(\).

One can see that after \( K\) seconds, the algorithm returns the point \(x^{2K}=x^{0}-_{j=0}^{K-1}f(x^{2j};^{2j+1})\), where \(^{j}\) are i.i.d. random variables. The algorithm is equivalent to the SGD method that converges after \(K=(}{{}}+L }}{{^{2}}})\) steps for the function class \(_{,L}\) (see Definition 6.1) for \(x^{0}=0\). Thus, the complexity \(_{}(\{A\},_{,L})\) equals \(((}{{}}+L}}{{^{2}}})).\)

Actually, any algorithm that was designed for Protocol 1 can be used in Protocol 2 with the oracle (7). Assuming that we have mappings \(A^{k}:\,^{d}^{d}^{d}\) for all \(k 1\), we can define mappings \(^{k}:\,^{d}^{d} _{ 0}^{d}\) via

\[^{k}(g^{1},,g^{k})= k/2 ,A^{ k/2}(g^{2},g^{4},,g^{2k}),&k\ ( \ 2)=0,\\ ( k/2+1),0,&k\ ( \ 2)=1.\]

For \(k=0\), we define \(^{0}=(0,A^{0})\).

## 5 Time Multiple Oracles Protocol

The protocol framework from the previous section does not seem to be very powerful because one can easily find the time complexity (6) by knowing (5) and the amount of time that oracle needs to calculate a gradient. In fact, we provide Protocol 2 for simplicity only. We now consider a protocol that works with multiple oracles:

```
1:Input: function(s) \(f\), oracles and distributions \(((O_{1},...,O_{n}),(_{1},...,_{n})) (f)\), algorithm \(A\)
2:\(s^{0}_{i}=0\) for all \(i[n]\)
3:for\(k=0,,\)do
4:\((t^{k+1},}}{{x^{k}}})=A^{k}(g^{1},,g^{k})\), \(\)\(t^{k+1} t^{k}\)
5:\((s^{k+1}_{}}{{i}}},g^{k+1})=O_{}}{{i}}}(t^{ k+1},x^{k},s^{k}_{}}{{i}}},^{k+1}),^{k+1} _{}}{{i}}}\)\(s^{k+1}_{j}=s^{k}_{j} j}}{{i}}\)
6:endfor
```

**Protocol 3** Time Multiple Oracles Protocol

Compared to Protocol 2, Protocol 3 works with multiple oracles, and algorithms return the indices \(i^{k+1}\) of the oracle they want to call. This minor add-on to the protocol enables the possibility of analyzing parallel optimization methods. Also, each oracle \(O_{i}\) can have its own distribution \(_{i}\).

Let us consider an example with two oracles \(O_{1}=O_{_{1}}^{f}\) and \(O_{2}=O_{_{2}}^{f}\) from (7). One can see that a "wise" algorithm will first call the oracle \(O_{1}\) with the time \(t^{0}=0\), and then, in the second step, it will call the oracle \(O_{2}\) also with the time \(t^{1}=0\). Note that it is impossible to do the following steps: in the first step an algorithm calls the oracle \(O_{1}\) with the time \(t^{0}=0\), in the second step, the algorithm calls the oracle \(O_{1}\) with the time \(t^{1}=_{1}\) and receives the gradient, in the third step, the algorithm calls the oracle \(O_{2}\) with the time \(t^{2}=0\). Indeed, this can't happen because \(t^{2}<t^{1}\).

An example of a "non-wise" algorithm is an algorithm that, in the first step, calls the oracle \(O_{1}\) with the time \(t^{0}=0\). In the second step, the algorithm calls the oracle \(O_{1}\) with the time \(t^{1}=_{1}\) and receives the gradient. In the third step, the algorithm calls the oracle \(O_{2}\) with the time \(t^{2}=_{1}\). It would mean that the "non-wise" algorithm did not use the oracle \(O_{2}\) for \(_{1}\) seconds. Consequently, the "wise" algorithm can receive two gradients after \(\{_{1},_{2}\}\) seconds, while the "non-wise" algorithm can only receive two gradients after \(_{1}+_{2}\) seconds.

We believe that Protocol 3 and the complexity (6) is a better choice for analyzing the complexities of parallel methods than the classical Protocol 1. In the next section, we will use Protocol 3 to obtain lower bounds for parallel optimization methods.

Lower Bound for Parallel Optimization Methods

Considering Protocol 3, we define a special function class \(\), oracle class \(\), and algorithm class \(\). We consider the same function class as Nesterov (2018); Arjevani et al. (2022); Carmon et al. (2020):

**Definition 6.1** (Function Class \(_{,L}\)).: We assume that function \(f:^{d}\) is differentiable, \(L\)-smooth, i.e., \(\| f(x)- f(y)\| L\|x-y\| x, y^{d},\) and \(\)-bounded, i.e., \(f(0)-_{x^{d}}f(x)\). A set of all functions with such properties we denote by \(_{,L}\).

In this paper, we analyze the class of "zero-respecting" algorithms, defined next.

**Definition 6.2** (Algorithm Class \(_{}\)).: Let us consider Protocol 3. We say that an algorithm \(A\) from Definition 4.1 is a zero-respecting algorithm, if \((x^{k})_{j=1}^{k}(g^{j })\) for all \(k_{0},\) where \((x):=\{i[d]\,|\,x_{i} 0\}\). A set of all algorithms with this property we define as \(_{}\).

A zero-respecting algorithm does not try to change the coordinates for which no information was received from oracles. This family is considered by Arjevani et al. (2022); Carmon et al. (2020), and includes SGD, Minibatch and Asynchronous SGD, and Adam (Kingma and Ba, 2014).

**Definition 6.3** (Oracle Class \(_{_{1},,_{n}}^{^{2}}\)).: Let us consider an oracle class such that, for any \(f_{,L}\), it returns oracles \(O_{i}=O_{_{i}}^{ f}\) and distributions \(_{i}\) for all \(i[n],\) where \(f\) is an unbiased \(^{2}\)-variance-bounded mapping (see Assumption 7.3). The oracles \(O_{_{i}}^{ f}\) are defined in (7). We define such oracle class as \(_{_{1},,_{n}}^{^{2}}\). Without loss of generality, we assume that \(0<_{1}_{n}\).

We take \(_{_{1},,_{n}}^{^{2}}\) because it emulates the behavior of workers in real systems, where workers can have different processing times (delays) \(_{i}\). Note that \(_{_{1},,_{n}}^{^{2}}\) has the freedom to choose a mapping \(f\). We only assume that the mapping is unbiased and \(^{2}\)-variance-bounded. We are now ready to present our first result; a lower bound:

**Theorem 6.4**.: _Let us consider the oracle class \(_{_{1},,_{n}}^{^{2}}\) for some \(^{2}>0\) and \(0<_{1}_{n}\). We fix any \(L,>0\) and \(0< c^{}L\). In view Protocol 3, for any algorithm \(A_{}\), there exists a function \(f_{,L}\) and oracles and distributions \(((O_{1},,O_{n}),(_{1},,_{n}))_{_{1},,_{n}}^{^{2}}(f)\) such that \([_{k S_{t}}\| f(x^{k})\|^{2}]>,\) where \(S_{t}:=\{k_{0}t^{k} t\},\) and_

\[t=c_{m[n]}[(_{i=1}^{m}} )^{-1}(+L}{m ^{2}})].\]

_The quantities \(c^{}\) and \(c\) are universal constants._

Theorem 6.4 states that

\[_{}(_{},_{,L} )=(_{m[n]}[(_{i=1}^{m} {_{i}})^{-1}(+L }{m^{2}})]). \]

The interpretation behind this complexity will be discussed later in Section 7.3. No algorithms known to us attain (9). For instance, Asynchronous SGD has the time complexity (3). Let us assume that \(}}{{}} p\) and \(p[n]\). Then (lower bound from (9)) \(=((_{i=1}^{p}})^{-1} ())\). In this case, the lower bound in (9) will be at least \((_{i=1}^{n}})^{-1}/( _{i=1}^{p}})^{-1}\) times smaller. It means that either the obtained lower bound is not tight, or Asynchronous SGD is a suboptimal method. In the following section we provide a method that attains the lower bound. The obtained lower bound is valid even if an algorithm has the freedom to interrupt oracles. See details in Section F.

### Related work

For convex problems, Woodworth et al. (2018) proposed the graph oracle, which generalizes the classical gradient oracle (Nemirovskij and Yudin, 1983; Nesterov, 2018), and provided lower boundsfor a rather general family of parallel methods. Arjevani et al. (2020) analyzed the delayed gradient descent method, which is Asynchronous SGD when all iteration delays \(_{k}=\) are a constant.

As far as we know, Woodworth et al. (2018) provide the most suitable and tightest prior framework for analyzing lower bound complexities for problem (1). However, as we shall see, our framework us more powerful. Moreover, they only consider the convex case. In Section M, we use the framework of Woodworth et al. (2018) and analyze the fixed computation model, where \(i^{}\) worker requires \(_{i}\) seconds to calculate stochastic gradients. In Section B, we consider the convex setting and show that the lower bound obtained by their framework is not tight and can be improved. While the graph oracle framework by Woodworth et al. (2018) is related to the classical oracle protocol (Section 3) and also calculates the number of oracle calls in order to get lower bounds, our approach directly estimates the required time. For more details, see Section B and the discussion in Section B.1.1.

```
1:Input: starting point \(x^{0}\), stepsize \(\), batch size \(S\)
2:Run Method 5 in all workers
3:for\(k=0,1,,K-1\)do
4: Init \(g^{k}=0\) and \(s=1\)
5:while\(s S\)do
6: Wait for the next worker
7: Receive gradient and iteration index \((g,k^{})\)
8:if\(k^{}=k\)then
9:\(g^{k}=g^{k}+g\); \(s=s+1\)
10:endif
11: Send \((x^{k},k)\) to the worker
12:endwhile
13:\(x^{k+1}=x^{k}- g^{k}\)
14:endfor
```

**Method 4** Rennala SGD

## 7 Minimax Optimal Method

We now propose and analyze a new method: Rennala SGD (see Method 4). Methods with a similar structure were proposed previously (e.g., (Dutta et al., 2018)), but we are not aware of any method with precisely the same parameters and structure. For us, in this paper, the theoretical bounds are more important than the method itself.

Let us briefly describe the structure of the method. At the start, Method 4 asks all workers to run Method 5. Method 5 is a standard routine: the workers receive points \(x^{k}\) from the server, calculate stochastic gradients, and send them back to the server. Besides that, the workers receive and send the iteration counter \(k\) of the received points \(x^{k}\). At the server's side, in each iteration \(k\), Method 4 calculates \(g^{k}\) and performs the standard gradient-type step \(x^{k+1}=x^{k}- g^{k}\). The calculation of \(g^{k}\) is done in a loop. The server waits for the workers to receive a stochastic gradient and an iteration index. The most important part of the method is that the server ignores a stochastic gradient if its iteration index is not equal to the current iteration index. In fact, this means that \(g^{k}=(1/s)_{i=1}^{S}f(x^{k};_{i}),\) where \(_{i}\) are i.i.d. samples. In other words, the server ignores all stochastic gradients that were calculated at the points \(x^{0},,x^{k-1}\).

It may seem that Method 4 does not fully use the information due to ignoring some stochastic gradients. That contradicts the philosophy of Asynchronous SGD, which tries to use all stochastic gradients calculated in the previous points. Nevertheless, we show that Rennala SGD has _better time complexity_ than Asynchronous SGD, and this complexity matches the lower bound from Theorem 6.4. The fact that Rennala SGD ignores the previous iterates is motivated by the proof of the lower bound in Section 6. In the proof, any algorithm, on the constructed "worst case" function, does not progress to a stationary point if it calculates a stochastic gradient at a non-relevant point. This suggested to us to construct a method that would focus all workers on the last iterate.

### Assumptions

Let us consider the following assumptions.

**Assumption 7.1**.: \(f\) is differentiable & \(L\)-smooth, i.e., \(\| f(x)- f(y)\| L\|x-y\|, x,y ^{d}\).

**Assumption 7.2**.: There exist \(f^{*}\) such that \(f(x) f^{*}\) for all \(x^{d}\).

**Assumption 7.3**.: For all \(x^{d}\), stochastic gradients \(f(x;)\) are unbiased and \(^{2}\)-variance-bounded, i.e., \(_{}[f(x;)]= f(x)\) and \(_{}[\|f(x;)- f(x)\|^{2 }]^{2},\) where \(^{2} 0\).

### Analysis of Rennala SGD

**Theorem 7.4**.: _Assume that Assumptions 7.1, 7.2 and 7.3 hold. Let us take the batch size \(S=\{|.^{2}/|,1\},=\{,}\}= (}{{L}})\). Then after_

\[K\]

_iterations, the method guarantees that \(_{k=0}^{K-1}[\| f(x^{k})\|^{2} ]\)._

In the following theorem, we provide the time complexity of Method 4.

**Theorem 7.5**.: _Consider Theorem 7.4. We assume that \(i^{}\) worker returns a stochastic gradient every \(_{i}\) seconds for all \(i[n]\). Without loss of generality, we assume that \(0<_{1}_{n}\). Then after_

\[96_{m[n]}[(_{i=1}^{m}} )^{-1}(+L}{m ^{2}})] \]

_seconds, Method 4 guarantees to find an \(\)-stationary point._

This result with Theorem 6.4 state that

\[_{}(_{},_{,L} )=(_{m[n]}[(_{i=1}^{m}})^{-1}(+L }{m^{2}})]) \]

for Protocol 3 and and the oracle class \(_{_{1},,_{n}}^{^{2}}\) from Definition 6.3.

### Discussion

Theorem 7.5 and Theorem 6.4 state that Method 4 is _minimax optimal_ under the assumption that the delays of the workers are fixed and equal to \(_{i}\). Note that this assumption is required only in Theorem 7.5, and Theorem 7.4 holds without it.

In the same setup, the previous works (Cohen et al., 2021; Mishchenko et al., 2022; Koloskova et al., 2022) obtained the weaker time complexity (3). We do not rule out that it might be possible for the analysis, the parameters or the structure of Asynchronous SGD to be improved and obtain the optimal time complexity (10). We leave this to future work. However, instead, we developed Method 4 that has not only the optimal time complexity, but also a very simple structure and analysis (see Section D.4.1). Our claims are supported by experiments in Section J.

The reader can see that we provide the complexity in a nonconstructive way, as the minimization over the parameter \(m[n]\). Note that Method 4 _automatically finds the optimal \(m\)_ in (7.5), and it does not require the knowledge of the delays \(_{i}\) to do so! Let us explain the intuition behind the complexity (10). Let \(m^{*}\) be the optimal parameter of (10) with the smallest index. In Section D.4.3, we show that all workers with the delays \(_{i}\) for all \(i>m^{*}\) can be simply ignored since their delays are too large, and their inclusion would only harm the convergence time of the method. So, the method _automatically_ ignores them! However, in Asynchronous SGD, these harmful workers can contribute to the optimization process, which can be the reason for the suboptimality of Asynchronous SGD.

In general, there are two important regimes: \(}}{{}} n\) ("low noise/large # of workers") and \(}}{{}} n\) ("high noise/small # of workers"). Intuitively, in the "high noise/small # of workers" regime, (11) is minimized when \(m\) is close to \(n\). However, in the "low noise/large # of workers", the optimal \(m\) can be much smaller than \(n\).

Synchronized Start of Workers

In the previous sections, we obtain the time complexities for the case when the workers asynchronously compute stochastic gradients. It is important that the complexities are obtained assuming that the workers _can start_ their calculations asynchronously. However, in practice, it is common to train machine learning models with multiple workers/GPUs, so that all workers are _synchronized_ after each stochastic gradient calculation (Goyal et al., 2017; Sergeev and Balso, 2018). The simplest example of such a strategy is Minibatch SGD (see Section 1.2). We want to find an answer to the question: what is the best time complexity we can get if we assume that the workers start simultaneously? In Section G, we formalize this setting, and show that the time complexity equals to

\[_{}(_{},_{,L} )=(_{m[n]}[_{m}(+L}{m^{2}})]) \]

for Protocol 2 and the oracle class \(_{_{1},,_{n}}^{^{2},}\) from Definition G.1. Comparing (11) and (12), one can see that _methods that start the calculations of workers simultaneously are provably worse than methods that allow workers to start the calculations asynchronously_.

## 9 Future Work

In this work, we consider the setup where the times \(_{i}\) are fixed. In future work, one can consider natural, important, and more general scenarios where they can be random, follow some distribution, and/or depend on the random variables \(\) from Assumption 7.3 (be correlated with stochastic gradients).