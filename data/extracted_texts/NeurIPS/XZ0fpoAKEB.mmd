# No free delivery service

Epistemic limits of passive data collection

in complex social systems

**Maximilian Nickel**

FAIR, Meta

New York, NY

maxn@meta.com

**Abstract**

Rapid model validation via the train-test paradigm has been a key driver for the breathtaking progress in machine learning and AI. However, modern AI systems often depend on a combination of tasks and data collection practices that violate all assumptions ensuring test validity. Yet, without rigorous model validation we cannot ensure the intended outcomes of deployed AI systems, including positive social impact, nor continue to advance AI research in a scientifically sound way. In this paper, I will show that for widely considered inference settings in complex social systems the train-test paradigm does not only lack a justification but is indeed invalid for any risk estimator, including counterfactual and causal estimators, with high probability. These formal impossibility results highlight a fundamental epistemic issue, i.e., that for key tasks in modern AI we cannot know whether models are valid under current data collection practices. Importantly, this includes variants of both recommender systems and reasoning via large language models, and neither naive scaling nor limited benchmarks are suited to address this issue. I am illustrating these results via the widely used MovieLens benchmark and conclude by discussing the implications of these results for AI in social systems, including possible remedies such as participatory data curation and open science.

Figure 1: **Test validity in complex systems**. Given assumptions \(\), target distribution \(\), data set \(^{m}\) from a sampling distribution \(\), and quality metric \(\), an inference setting is _test-valid_ if the difference between \(\) and the true risk \(L^{}_{fh}\) can be bounded over the distribution of all possible worlds \(f\) consistent with \((,)\).

Introduction

Model validation, long taken to be "solved" via the train-test paradigm, has become one of the central challenges in modern machine learning and artificial intelligence. In unison with the dramatic increase of their capabilities, AI systems are now supposed to solve tasks of vastly expanded scope, including potentially AI-complete tasks such as open domain question answering, autonomous decision making, and ultimately, artificial general intelligence. Even before the recent triumphs of large language models and deep learning, Anderson  proclaimed "the end of theory" and the scientific method being obsolete due to the wonders of big data, large-scale computing, and data mining. At the same time, it is entirely unclear how to rigorously evaluate the quality of models for these ambitious tasks. This lack of proper evaluation can then materialize in persistent issues of deployed systems related to generalization, e.g., hallucination , out-of-distribution generalization , fairness , and generalization to the long-tail . Importantly, these issues do not only affect the accuracy of models in a vacuum, but can also affect their social impact if they are deployed in consequential social contexts . In this paper, I aim to connect the former developments with the latter issues through the lens of epistemology. More concretely, I ask:

**Research Question 1.** Given the ambitious tasks that we ask AI systems to solve and given

how we currently collect data, _can we know_ whether a model performs well for these tasks?

Answering RQ 1 positively is central not only for the deployment of machine learning systems, but also for scientific progress within artificial intelligence itself. After all, knowledge of a model's quality is a prerequisite to detect generalization issues and develop improved models. In deployed systems, a model's predictions are useless -- as good as they might be -- without knowing that they are, in fact, reliable. In social systems, where the consequences of model errors can be severe, having this knowledge is of even greater importance. Hence, the epistemic question of this work gets to the heart of various debates surrounding AI and its capabilities: How can we understand and measure the true capabilities of modern AI systems, which are so very impressive and yet lacking in fundamental ways at the same time ? What can we know about the quality of our models? Are our benchmarks suited to give insights into the intended tasks or do they project a false image of quality? How can we develop systems such that they work for everyone? Will naive scaling solve all these problems or do we need to invest into entirely new approaches for evaluation within the scope of modern AI?

A prerequisite to answering RQ 1 positively is the validity of model validation: Without model validation we can not know wether a model is good or bad and without a valid model validation procedure we can not attain this knowledge. The almost exclusively used method for model validation in machine learning and AI is the ubiquitous train-test paradigm, i.e., the practice of estimating the generalization performance of a model on a test set distinct from the training set. Arguably, much of the breathtaking progress in machine learning has been driven by the success of this single experimental paradigm as it allows for the rapid validation and, therefore, improvement of models . However, it is crucial to note that the train-test paradigm is inherently an inductive method that aims to _infer, not measure_, the generalization error of a model from its error on a test set. It is well known -- dating back at least to Hume  and formalized in the context of machine learning by Wolpert  -- that it is not possible to justify the validity of such inductive inferences without further assumptions. This raises the question: is the train-test paradigm still valid for the combination of tasks and data sets considered in modern AI and under what assumptions is this the case?

Importantly, such assumptions should be minimal in terms of ontological commitments, i.e., meet _ontological parsimony_ (or minimality), since (a) model validation results can not provide insights about validity in the real world if they are contingent on strong ontological assumptions (b) any assumptions that are required to ensure the validity of model validation can not be validated through the same method without circular reasoning. In traditional machine learning _settings_, these ontological commitments are placed entirely on the data collection process and, as such, the train-test paradigm is indeed suitable to _validate any model assumption_ outside the data collection process. More concretely, under _active data collection_, i.e., when we actively control the data collection process, we can create large enough test sets that are (approximately) sampled i.i.d. from the target distribution. Under these conditions, it is well known that the train-test paradigm allows us to validate models simply via their performance on this test set -- _without making any further ontological commitments_. This property is the beauty of the train-test paradigm and what makes it so valuable and successful.

However, domains in modern machine learning have become far too large to be covered via data sets in this active and controlled manner -- the required effort would be prohibitively difficult and costly.

In lieu, _passive data collection_ has become the predominant way to create data sets for modern AI systems. Here, data is collected without intervention from _some social system_ that generates data within the domain of interest. For instance, rather than meticulously collecting independent samples from all possible facts in a domain, training and validation corpora for QA models are gathered from what has been published on the internet. Similarly, preferences of users are collected over items that a recommender system has pre-selected, rather than sampling them i.i.d. over all possible user-item pairs. Importantly, these sample generating systems need not correspond to the target data generating process, have their own internal dynamics, and are driven by complex interactions of their parts and social processes, e.g., well-known phenomena such as popularity bias , homophily [22; 37], or feedback loops .

Hence, I will ground RQ 1 in these conditions of current machine learning practice: _Under passive data collection from a social system, can model validation be valid or not?_ To formalize the social systems with which an AI system interacts, I am taking a _complex systems_ perspective and describe them as networks with well-established sampling biases and degree distributions. For these properties, I will show how they affect _necessary conditions_ of test validity. These results can also be understood as a strengthening of the seminal _No Free Lunch_ (NFL) theorems for supervised learning  in the context of social systems. While the NFL theorems show the impossibility of an assumption-free general purpose learning algorithm, a common criticism is that they need to assume an induction-hostile universe, i.e., full ontological neutrality . In practice, where assuming a reasonably induction-friendly universe is common, the NFL theorems have had therefore limited impact. In contrast, the results of this work are grounded in current machine learning practice and considerably stronger: Even for non-trivial assumptions of an induction-friendly universe, model validation can be shown to be invalid when data is collected passively in social systems. In other words, there is _no free delivery service_ of data for model validation in complex social systems. To discuss the above results, I will provide a synthesis of results from learning theory, social science, and complex systems -- and combine them with new theoretical and empirical results on the validity of model validation. In particular, the _main contributions_ of this paper are as follows:

**Theorem 1** (Informal).: For passively collected data in complex social systems the train-test paradigm cannot be valid under ontological parsimony for the vast majority of the system. This includes widely considered variants of recommender systems and question answering.

**Corollary 2** (Informal).: Naive scaling and limited benchmarks are prohibitively inefficient to address theorem 1 and therefore not suited to attain test validity in these scenarios.

**Supporting evidence.** Theoretical results are supported via experiments on the popular MovieLens benchmark where widely considered recommendation tasks are shown to be test-invalid.

The remainder of this paper proceeds as follows: Sections 2 and 3 formalize passive data collection in social systems and connect it to test validity. Section 4 develops theorem 1, corollary 2, and supporting evidence. Sections 5 and 6 discuss related work and implications for AI in social systems.

## 2 Passive data collection and inference tasks in social systems

To construct validation data sets for large-scale domains, there exist currently two main practical approaches: (i) "scaling", i.e., indiscriminately collecting as much data as possible from some domain and (ii) manually constructing benchmarks of limited size that probe certain subareas of the domain. In the following, I will focus on formalizing (i) as passive data collection from social systems. Section 4 will then show that neither (i) nor (ii) can be solutions to the issues of this paper.

In sociology, a social system is often considered a pattern of networked interactions that exists between individuals, groups, or institutions . For the purposes of this paper, I will consider a _social system_ to be a pair \((f,)\) where \(f:\) is a _possible world of interactions_ such that \(=_{1}_{n}\) denotes the domain of interactions, \(\) denotes the set of outcomes (or labels) of an interaction, and \(:\) denotes the _sampling distribution_ of the system over interactions. Within this framework, _passive data collection_ refers to sampling directly from \(\). This is in contrast to _active data collection_ where we would aim to sample directly from the _target distribution_\(:\) for an inference task, e.g., via simple random sampling, stratified sampling, etc.

In complex social systems, \(\) is driven by social processes that lead to two characteristic properties of samples: (i) they are _biased_ and (ii) they follow _heavy-tailed_ or _power-law_ distributions. The earliestwork on (ii) is due to Simon , and has independently been discovered in multiple contexts. In fact, (ii) can often be understood as a consequence of (i), e.g., popularity bias leading to power-law distributions in social networks . See also supp. C for further discussion of these properties.

In the remainder, I will therefore focus on the presence of heavy-tailed distributions in \(\) to understand how this ubiquitous property of social systems affects test validity. For this purpose, I will first introduce the concept of a sample graph, i.e., the observed interactions that we receive from \(\):

**Definition 1** (Sample graph).: A data set \(^{m}_{1}_{2}\) of observed interactions induces a bipartite _sample graph_\(G=(_{1},_{2},)\) between entities of \(_{1}\) and \(_{2}\) where an edge indicates that the corresponding interaction has been observed. In the following, I will use \(\) and \(G\) interchangeably.

For higher arity relations, definition 1 can easily be generalized to hypergraphs. For simplicity, I will focus on bipartite graphs in the following. In sample graphs, the heavy-tailed property of complex systems materializes then through their degree distribution. While the exact nature of these distributions is disputed , I will follow Voitalov et al.  and assume that node degrees in \(\) follow a _regularly-varying power-law distribution_. Based on this observation, _passive data in complex social systems_ will then refer to the following:

**Definition 2** (Passive data in complex social systems).: Let \(^{m}\) be a sample graph drawn from sampling distribution \(\). Let \(K_{1}\), \(K_{2}\) denote random variables that model the degree distribution in \(\) of nodes in \(_{1}\) and \(\), respectively. For passively collected data from complex social systems, I will then assume that \(K_{1},K_{2}\) follow regularly-varying power-law distributions, i.e.,

\[(K_{1}>k)=u_{1}(k)k^{-_{1}}(K_{ 2}>k)=u_{2}(k)k^{-_{2}}\]

where \(_{i}>0\) are the tail indices and \(u_{i}\) are slowly varying functions such that \(_{x}u(rx)/u(x)=1\) for any \(r>0\). Higher arity relations are defined analogously. Next, I will show how passive data in social systems materializes in key inference settings (see also table 1).

**Example 1** (Recommender Systems).: Recommender systems are concerned with inferring the true preferences of a user over all items from a set of revealed preferences sampled from \(\). As such they are a typical example for \((f,)\) where the target distribution \(\) corresponds to the uniform distribution over all possible interactions. Importantly, \(\) is typically influenced by social processes and sampling bias as well as heavy-tailed distributions are well documented in recommender systems. For instance, an important factor for sampling biases are feedback loops, e.g., that past recommendations influence which recommendations are shown in the future . Another source of sampling bias is user feedback, which is often biased towards items with high ratings , as well as popularity bias . Popularity bias leads directly to heavy-tailed distributions in the degree distribution of the sample graph . See also fig. 1(a) for evidence of this property on MovieLens.

 p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt}}   & **Domain \(\)** & **Possible world \(f\)** & **Sample distribution \(\)** & **Target distribution \(\)** \\  Recommender systems & \(\) & User preferences & Probability of user interacting with item, heavy-tailed in \(\) and \(\) & Uniform, \(p_{T}(u,i)=1/||\) \\  Symbolic reasoning & \(\) & Truth value of factoids & Probability of observing factoid, heavy-tailed in \(\), \(\), and \(\) & Uniform, \(p_{T}(s,p,o)=1/||\) \\  

Table 1: **Inference settings based on passive data collection in complex social systems.**

Figure 2: (a) **Heavy-tailed samples** in recommender and reasoning datasets. (b) **Symbolic reasoning via LLMs.** To validate reasoning capabilities of LLMs, natural language has to be mapped to logical knowledge representations. This shows that validation of reasoning in LLMs is subject to the results of this paper. See also fig. 5(b) and supp. G.1.

**Example 2** (Symbolic reasoning and QA).: Reasoning and question answering over symbolic knowledge representations are another key example for \((f,)\). In this setting, factoids are represented in form of _(subject, predicate, object)_ triples and the task is to infer the truth value for _any_ unknown factoid, i.e., for a uniform target distribution \(\). Importantly, while facts about the world itself do not need to be influenced by social processes, our available knowledge about them, i.e., \(\), often is. In addition to aspects such as popularity bias, causes for this can range from which questions are studied in science [35; 36], over how data is collected , to who has access to the internet and the ability to contribute to knowledge . Consequently, heavy-tailed distributions are also well-documented in this setting. For instance, Steyvers et al.  showed that semantic networks typically follow heavy-tailed degree distributions. Similar distributions have been observed in large-scale knowledge graphs such as DBPedia, Yago , Freebase , and Wikidata. See also fig.2a for evidence of this property on FB15k. Importantly, this setting applies to any reasoning task over factoids in general -- irrespective of the data representation. For instance, the validation of reasoning capabilities for general purpose question answering in systems such as LLAMA [63; 21] and ChatGPT  needs to follow this blueprint. See also fig.2b for an illustration.

## 3 Test validity

To answer RQ 1, I will focus on the test validity of _inference settings_, i.e., whether task, assumptions, and data allow for _any_ valid validations at all. For this purpose, I will use a deductive approach: model validation is _valid_ if it is a logical consequence of its assumptions that the difference between its estimate and the true generalization error is bounded with high probability. To formalize this, let \(h,f:\) denote functions that map from sample domain \(\) to target domain \(\). For clarity, I will assume noise-free \(f\) and \(h\). Furthermore, let \(^{m}=\{x_{i}\}_{i=1}^{m}\) denote a data set of \(m\) samples drawn from a sampling distribution \(:\) and let \(=\{(x,f(x)):x\}\) denote its supervised extension. For notational convenience, I will also write \(^{m}\) when \(f\) is clear from context. In addition, let \(\{f f:\}\) be the set of all functions from \(\) to \(\) that are consistent with some set of assumptions on \(f\) such as being low-rank. Next, note that \(\) and \(\) then induce a set of possible worlds as follows:

**Definition 3** (Possible worlds).: Let \(\) be a set of assumptions, \(\) a set of observations, and \(f:\). The set of _possible worlds_\(\) is then the set of functions consistent with \(\) and \(\), i.e.,

\[=\{f f\ \ (x,y):f(x)=y\}.\]

Furthermore, I will consider an inference setting \((,,,)\) to be a set of assumptions \(\), a _fixed_ dataset \(^{m}\), a _target distribution_\(:\) for which we want to make inferences, and an assumed _distribution over possible worlds_\(\). Note that if \(\), \(\) can not be an i.i.d. sample from \(\). For further details and notation see supps. A and B.

Next, let \(X\) be a random variable over \(\) and let \(:_{+}\) be a positive loss function. The _risk_ of hypothesis \(h\) with respect to a _single_ world \(f\) is then denoted by

\[L^{}_{fh}=_{X-}[(h(X),f(X))].\]

Furthermore, let \(\) denote _any_ risk measure of a hypothesis \(h\) on some test set \(\). For instance, \(\) could denote the empirical risk or a re-weighted estimator such as the Horvitz-Thompson adjusted empirical risk (see also table 4 in the supp. material). Hence, \(\) does not only cover the standard Monte-Carlo estimator for the i.i.d. setting, but also estimators used in counterfactual and causal settings. To determine the test-validity of an inference setting, I am then interested in bounding the difference between the estimated risk (\(\)) and the true risk of h (\(L^{}_{fh}\)). Importantly, it is necessary to consider the risk of \(h\) relative to the distribution \(\) over all possible worlds since no world \(f\) can be excluded based on \(\) and \(\). Hence, test validity is defined as follows:

**Definition 4** (Test validity).: Let \(f\) denote a distribution over possible worlds \(\) and let \(\) denote a hypothesis class. Furthermore, let \(L^{}_{fh}\) denote the risk of hypothesis \(h\) for target distribution \(\) and possible world \(f\). Let \(_{+}\) denote any empirical risk measure of \(h\) on a test set. Then, \((,,,)\) is _\((,)\)-test-valid_ (_test-invalid_) if \(\)'s difference to \(L^{}_{fh}\) can (cannot) be bounded accordingly, i.e.,

\[(,,,)\{ \  h:_{f-}(|-L^{}_{fh}| )& 1-&(,)\\ \  h:_{f-}(| -L^{}_{fh}|>)&>.&( ,).\]The conditions in definition4 for a valid validation setting are very mild since it requires only a single hypothesis class in which \(\) for a single hypothesis has bounded difference to the true risk with high probability. Since invalidity follows directly from validity via complement rule and negation, the conditions for a validation setting to be invalid are strong: For _any possible hypothesis class_ it has to hold that the difference between \(\) and the true risk of _all hypotheses_ can not be bounded with sufficient probability. Importantly, both are statements about an inference setting, i.e., the combination of assumptions, observed data, and target distribution, and not about a specific hypothesis (class). Furthermore, note that definition4 implies realizability with regard to the assumptions: if \(\{f(x,y):f(x)=y\}=\), an inference setting is test-invalid since \(()=0\). However, definition4 imposes no realizability or any other constraints on \(\).

Next, note that definition4 implies straightforward necessary conditions for test validity:

**Corollary 1** (Necessary condition for test validity).: _Let \((,,,)\) be an inference setting, let \(:_{+}\) be a positive loss function, and let \(\) be a hypothesis class. Furthermore, let \(_{+}\) be any risk estimate for \(h\). Then, if \((,,,)\) is \((,)\)-test-valid, it must hold that_

\[\; h:_{f\;-\;}(L^{ }_{fh}+) 1-.\]

Proof sketch.: Corollary1 follows simply via the monotonicity of probability, i.e., it holds that \(1-_{f\;-\;}(|-L^{}_{fh}| )_{f\;-\;}(L^{}_{fh}+)\). This holds for any risk measure \(\), loss \(_{+}\) and hypothesis \(h\). See supp.D for proof details. 

## 4 Test validity under passive data collection in complex systems

In the following, I will provide an overview of the main results as well as high-level proof sketches. For clarity, I will consider only binary relations \(=_{1}_{2}\) and possible worlds over unbounded output domains \(f:\). For detailed proofs and discussion, as well as extensions to ternary relations and bounded domains, see supps. E to G. To meet ontological parsimony1 and get insights into the validity of the train-test paradigm, I will focus on F being the uniform distribution \(\) and \(\) imposing only minimal assumptions on \(f\).

Next, to derive bounds on the validity of inference settings in complex social systems, I will represent possible worlds \(f\) as partially observed matrices which are constructed as follows:

**Definition 5** (Matrix representation).: For a function \(f:_{1}_{2}\) over _finite_ sets of size \(|_{1}|=n_{1}\) and \(|_{2}|=n_{2}\), its _matrix representation_\(^{n_{1} n_{2}}\) is given via \(_{ij}=f(x_{i},x_{j})\) for all \((x_{i},x_{j})_{1}_{2}\).2 In the following, I will use \(f\) and \(\) interchangeably.

Using this matrix representation of a system, I will show in lemma2 that the train-test paradigm is invalid if the rank of \(f\), i.e., the complexity of the system, exceeds the \(k\)-connectivity of the sample graph \(\) and if \(f\) is chosen uniformly from \(\). Here, \(k\)-connectivity is defined as follows:

**Definition 6** (\(k\)-core and \(k\)-connectivity).: The \(k\)-core (or core of order \(k\)) of a graph is its maximal subgraph such that all vertices are at least of degree \(k\).3 A graph is \(k\)-connected _if and only if_ every vertex is in a core of order at least \(k\).

**Lemma 1** (Rank-\(k\) underdetermination).: _Let \(=\{f rank(f) k\}\). Then, if \(\) is not \(k\)-connected, the set of possible worlds \(\) forms a non-empty vector space._

Proof sketch.: Since \(\) is not \(k\)-connected, any \(f\) with \((f)=k\) can not be \(\)-isomeric. It then holds via [38, Lemma 5.1] that \(\), i.e., the set of matrices of rank \(k\) or less that are consistent with \(\), form a non-empty vector space. See supp.E for proof details. 

In the spirit of Occam's razor, higher ranks of \(f\) correspond to more complex possible worlds. Lemma1 establishes then that if the \(k\)-connectivity of \(\) does not match the complexity of the system \(f\), the observations \(\) do not constrain \(\) sufficiently and a randomly chosen possible world can be arbitrarily different on the non-observed entries. Via corollary1, lemma1 implies then that \(k\)-connectivity is necessary for test validity if \(\) belongs to the broad class of scalar Bregman divergences, i.e., widely used loss functions such as the square loss, the log loss, or the KL-divergence (see also table5 in the supplementary material).

**Lemma 2** (Rank-\(k\) test-invalidity).: _Let \(\) be identical to lemma1, let \(\) be a scalar Bregman divergence, let \(\) be the uniform distribution over \(\), and let \(\) be the uniform distribution over \(\). Furthermore, let \(_{+}\) be any risk estimator on a test set. Then, if \(\) is not \(k\)-connected, \((,,,)\) is test-invalid, i.e., it holds for any \(>0\) that_

\[\,:_{f}(| -L_{fh}^{}|)=0.\]

_Proof sketch._ If \(\) is not \(k\)-connected, \(\) is a vector space according to lemma1. Lemma2 follows then from corollary1 for uniformly sampled \(f\) and \(h\) via a simple volume argument. For \(h\), the result follows again from \(\) being a vector space via the generalized Pythagorean theorem for Bregman divergences [20, Eq. 2.3]. See supp. B.2 for proof details. 

The consequences of lemma2 are non-trivial. Under ontological parsimony, it shows that passive data from complex social systems, i.e., the foundation of basically all large-scale AI tasks, can not be used to validate the quality of models if \(\). Clearly, no subset of \(\), e.g., cross-validation, can fulfill this task either. Importantly, lemma2 holds not only for empirical risk, but for _any_ estimator on \(\), including counterfactual estimators, i.e., methods which are exactly meant to address \(\). This illustrates that lemma2 is not simply an out-of-distribution or counterfactual estimation problem. Rather, it is caused by a combination of out-of-distribution (\(\)) and insufficient data (\(k\)-connectivity \(<(f)\)). Next, I will connect these results to the main result of this work.

**Theorem 1** (Test validity in complex social systems).: _Let \((,,,)\) be identical to lemma2. Furthermore, let \(^{m}\) where \(\) follows power-law distributions such that the degrees of \(x_{i}\) in the sample graph \(\) are drawn i.i.d. from a regularly-varying power-law distribution \(((x)>k)=u(k)k^{-_{i}}\). Furthermore, let \(n_{i}=|_{i}|\) be the size of domain \(_{i}\). Then, the number \(V_{i}\) of nodes in \(_{i}\) for which test validity holds decreases with a power-law decay in \((f)=k\), i.e,_

\[[V_{i}] n_{i}u(k)k^{-_{i}}.\]

_Proof sketch._ Test validity requires the \(k\)-connectivity of \(\) to be greater or equal to \((f)\) via lemmas1 and 2. Hence, only subgraphs where all vertices are at least of degree \(k\) can be valid. Theorem1 follows then via the expected number of nodes with degree at least \(k\) in \(_{i}\), i.e., \([V_{i}]=_{x_{i}}((x) k)\). 

For heavy-tailed distributions, most nodes will be outside the required k-core for even moderately complex worlds. Hence, theorem1 shows that the train-test paradigm cannot be valid under ontological parsimony for the vast majority of nodes in realistic social systems. Table2 illustrates this using parameters that match the well-known Book Crossing dataset.

An immediate next question is then if the issues raised by theorem1 can simply be solved by scaling, i.e., by collecting more data from \(\) -- or via manually constructed benchmarks such as BigBench  to extrapolate from their results to the risk on \(\). Corollary2 answers both questions via lemma1 (see supp. H for a detailed discussion and proof): (i) For _scaling_, we can ask how many draws from \(\) would be necessary such that all nodes are within the \(k\)-core of \(\) with high probability, i.e., how many samples are needed until arriving at a valid test setting. While there exists no easily computable solution to this problem, we can compute a (weak) lower bound by asking how many samples from \(\) are needed to sample a random node in \(_{i}\)_once_. (ii) For _benchmarks_, we can ask how many nodes would need _at least one_ additional data point to arrive at a valid test setting, i.e., how much manual data collection is at least needed to create a benchmark that extrapolates to \(\).

    & & **Scaling** & **Benchmarks** & Nodes with less than 100 observations \\ 
2.5 & 5 & \(10^{7}\) & \(_{i}[T_{i}](||/2)^{+1}/( ax_{}^{})=2 10^{21}\) & \([N]=||(1-(x_{}/x)^{})>9.9 10^{6}\) \\   & \)} & |\)} & **Book Crossing** & **Book Crossing** \\  & & & Fraction of users with large enough degrees such that train-test measures and inferences are valid \\ 
2.38 & 8 & \(10^{5}\) & Rank 8: 100\%, Rank 10: 58.8\%, Rank 20: 11.3\%, Rank 100: 0.2\% \\   

Table 2: **Inefficiency of scaling and benchmarks; validity coverage for the Pareto distribution.**

**Corollary 2** (Inefficiency of scaling and benchmarks).: _Let \((,,,)\) and \(\) be identical to theorem1. Furthermore, let (i) \(T_{i}\) denote the expected number of samples from \(\) until node \(x_{i}\) is sampled, and let (ii) \(N_{j}\) denote the number of nodes in \(_{j}\) with less then \(k\) samples. Then, \(T_{i}\) scales at least polynomially and \(N_{j}\) scales linearly in the size of the domain \(||\). Specifically,_

\[_{i\{1,||\}}[T_{i}](||/2)^{ +1}/( x_{min}^{}),[N_{j}]=| |(1-(x_{min}/x)^{})\]

Clearly, sampling from \(\) is highly inefficient to overcome the issues raised by theorem1 since (i) it is extremely difficult to get successful samples from the heavy tail (rare events) and (ii) covering all nodes outside sufficiently large k-cores in selective benchmarks is prohibitively expensive. See also table2 for examples of these aspects for typical distributions in complex social systems.

Experimental evidenceTo illustrate the real consequences of the previous theoretical results, I will now provide experimental evidence based on the MovieLens 100k dataset , a critical benchmark that has, for years, been widely-used in recommender systems research. As predicted by lemma2, I will show that there exist possible worlds of low complexity that all explain the observed data equally well but are widely different on the unobserved data. Hence, _any_ quality metric that is inferred on this benchmark, or subsets of it, can not be informative about the true generalization error. For this purpose, I fit \(p=100\) matrices of rank \(k=50\) to the observed data \(\). All matrices, or possible worlds, fit the observed data and rank constraint with error below \(10^{-3}\) and \(10^{-2}\), respectively. See supp.1.1 for details. For a pair of possible worlds \((f,f^{})\), I compute then the normalized absolute error (NAE) for each _unobserved_ entry \((i,j)\) via \((f_{ij},f^{}_{ij})=|f_{ij}-f^{}_{ij}|/(f_{}-f _{})\). This informs us about how different pairs of possible worlds can be on the unobserved data. Figures3(a) and 3(b) shows the empirical CDF (eCDF) of the NAE over unobserved entries for such pairwise comparisons of possible worlds as well as the worst-case over all worlds per entry. From fig.3(a), it can be seen that the worst case error across possible worlds per entry is substantial for the vast majority of unobserved entries. For instance, for 50% of entries the NAE is above 77% of the worst case error. For arbitrary pairs of possible worlds, the situation is similar, where, depending on the particular pair of worlds, the NAE is between 23% to 49% for 50% of entries. Furthermore, the area over the eCDF curves in fig.3(b) corresponds directly to the risk for a pair of possible worlds and is again substantial for all pairs (see supp.1.2 for details). Since any possible world can be the "true" world this shows again that the test error for any subset of this benchmark can not be informative for the true generalization error of this task.

In addition to the NAE, fig.3(c) shows the cumulative distribution of users within cores of order \(k\) per demographic group for MovieLens 100k. It can be seen that the cumulative distribution can vary significantly between different demographics. For instance, while only 25% of "homemakers" are in

Figure 3: **M MovieLens 100k experiments (a) Empirical CDF (eCDF) of maximum NAE over possible worlds. Area over the curve (expected error) shaded. (b) eCDF of NAE for pairs of possible worlds. (c) eCDF k-coreper demographic group. (d) Proportion of users for which test-validity holds relative to the rank of \(f\).**a \(k\)-core larger than 50, 40% of "technicians" are in a \(k\)-core larger than 80. It follows from lemma2, that test-validity will therefore also vary significantly between demographic groups (if we assume that there are no significant differences in the complexity of preferences between groups). Figure3d illustrates this point by showing the proportion of users for which test-validity holds relative to the rank of a model. It can be seen that there exist clear differences already for moderately complex worlds. For instance, for a model of rank 60, test-validity would hold for 67% of "technicians" while it would only hold for 14% of "homemakers". Clearly, this has important implications for fairness, bias, and whether recommender systems _work for everyone_.

## 5 Related work

The no-free-lunch theorems for machine learning [68; 60] share important similarities to this work as both consider the expected risk over possible worlds. However, the results in this paper are stronger and directly applicable to current machine learning practice. While the NFL theorems consider the performance over all possible worlds without any restrictions -- an assumption that is too restrictive in most instances -- the results of this paper show that even for relatively strong assumptions about the set of possible worlds, e.g., low-rank structures, valid model validation is not generally possible for passive data collection in complex social systems. In motivation, this paper is also related to the works [19; 7; 25; 54; 41; 66] which study outcomes of underspecification in ML pipelines, model multiplicity and Rashomon sets. In the restricted context of personalized prediction, Monteiro Paes et al. , discusses related limits to testing and estimation. Schaeffer et al.  discuss whether seemingly emergent capabilities of LLMs are rather a result of insufficient metrics. In statistics, Meng  analyzed a scaling-related question similar to this paper: Given a carefully collected survey with low response rate (small data) or a large, self-reported dataset without data curation (big data), which dataset should one trust more to estimate population averages? Outside machine learning, validity theory has a long history in fields such as psychology and sociology. Here, test validity is considered a measure of the degree to which a test measures what it is intended to measure  and has been studied extensively in the context of psychological tests  and educational testing . Increasingly, these notions of validity, have also been considered in machine learning [17; 50; 2].

With regard to technical tools, this paper is also closely related to prior work in matrix completion. For instance,  studied the problem of unique and finite completability of matrices and derived similar \(k\)-core related bounds using determinantal varieties and algebraic geometry. Srebro et al.  studied the problem of matrix completion based on non-uniform samples such as power-laws but assume that \(=\). Meka et al.  focused on power-law samples for \(\) and, consistent with this work, require at least \(k\) samples per row and column to guarantee completability of a rank-\(k\) matrix. Cheng et al.  derive similar results based on graph \(k\)-connectivity. Related to non-i.i.d. observations,  developed a framework to provide necessary conditions for matrix completion under deterministic sampling. Lemma1 is based on these results. Different to these prior works, I provide formal impossibility results for test validity based on passive data in complex social systems. This allows to gain rigorous insights into the epistemic limits of what we can know based on this form of data collection. See also supp. J for further related work.

## 6 Discussion

The results in this paper provide new insights into the validity of the train-test paradigm when data is passively collected from complex social systems. In particular, I have shown that there exists _no free delivery service_ of data that allows for test validity on a global scale in this setting. While valid inferences are possible with respect to the sampling distribution \(\) and within high \(k\)-cores, they are unlikely if \(\) extends to the entirety of the system. Hence, test validity depends on the interplay between task (\(\)), the complexity of the system (\(\)), and the \(k\)-connectivity of the sample graph (\(\$\)) underlying the observed data (\(\)), what is a _combinatorial_ property of the data. These results are attained by establishing novel _necessary conditions_ for which validation is possible. As AI systems are increasingly applied in conditions for which sufficient conditions of validity are difficult to guarantee, understanding such minimal conditions can provide guidelines into developing better and more robust systems. Importantly, it can help to demarcate inference goals that are not meaningful from ones that are attainable. It helps to understand the limits of what we can know and which questions are futile to ask. This work provides a first step in this direction by establishing such epistemic limits of AI in complex social systems.

Furthermore, I have shown that the sub-system for which valid inferences are possible shrinks rapidly with the complexity of the system and that a naive application of the scaling paradigm is prohibitively inefficient to overcome these validity issues. As a consequence, solving many complex AI tasks are unlikely to come for free through scaling or for cheap through extrapolating from limited small-scale benchmarks. Instead, there exists an inherent trade-off between data quality, quantity, and task complexity. If we want to avoid asking AI systems to solve simpler tasks (e.g., non-out-of-distribution or smaller scope), new data curation efforts are likely needed. Due to the substantial amount of data that would have to be collected, centralized data collection is often infeasible to overcome the validity issues of this paper. Instead, decentralized methods such as _participatory data curation_ could provide a way forward. This aligns with insights from fairness which also highlight the need for participatory methods in data collection . Similar arguments apply to the importance of open science and open-source models in this context.

Importantly, the theoretical results of this paper also provide direct insights into how to improve data collection for model validation via its \(k\)-core conditions. In particular, lemma1 and corollary2 imply two clear objectives for targeted data collection: (a) collecting data points that increase the \(k\)-connectivity of the sample graph and (b) collecting data points that increase the size of the \((f)\)-core of the sample graph, where \((f)\) is the complexity of the world that we want to assume. Pursuing (a) would increase the complexity of the world that can be assumed such that model validation is still valid for the entire sample graph, while pursuing (b) would increase the size of the subgraph for which a \((f)=k\) assumption would still yield valid model validation. Hence, both objectives are based on the k-core conditions of this work and can be computed from a given sample graph. Creating new mechanisms for efficient data collection based on these insights is therefore a very promising avenue for future work.