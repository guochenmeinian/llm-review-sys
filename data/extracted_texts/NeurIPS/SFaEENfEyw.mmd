# The Closeness of In-Context Learning and Weight Shifting for Softmax Regression

Shuai Li

Shanghai Jiao Tong University

shuaili8@sjtu.edu.cn

&Zhao Song

Simons Institute for the Theory of Computing, UC Berkeley

magic.linuxkde@gmail.com

&Yu Xia

University of California, San Diego

yux078@ucsd.edu

&Tong Yu

Adobe Research

tyu@adobe.com

&Tianyi Zhou

University of Southern California

tzhou029@usc.edu

###### Abstract

Large language models (LLMs) are known for their exceptional performance in natural language processing, making them highly effective in many human life-related tasks. The attention mechanism in the Transformer architecture is a critical component of LLMs, as it allows the model to selectively focus on specific input parts. The softmax unit, which is a key part of the attention mechanism, normalizes the attention scores. Hence, the performance of LLMs in various NLP tasks depends significantly on the crucial role played by the attention mechanism with the softmax unit.

In-context learning is one of the celebrated abilities of recent LLMs. Without further parameter updates, Transformers can learn to predict based on few in-context examples. However, the reason why Transformers becomes in-context learners is not well understood. Recently, in-context learning has been studied from a mathematical perspective with simplified linear self-attention without softmax unit. Based on a linear regression formulation \(_{x}\|Ax-b\|_{2}\), existing works show linear Transformers' capability of learning linear functions in context. The capability of Transformers with softmax unit approaching full Transformers, however, remains unexplored.

In this work, we study the in-context learning based on a softmax regression formulation \(_{x}\|(Ax),_{n}^{-1}(Ax)-b\|_{2}\). We show the upper bounds of the data transformations induced by a single self-attention layer with softmax unit and by gradient-descent on a \(_{2}\) regression loss for softmax prediction function. Our theoretical results imply that when training self-attention-only Transformers for fundamental regression tasks, the models learned by gradient-descent and Transformers show great similarity.

## 1 Introduction

In recent years, there has been a significant increase in research and development in the field of Artificial Intelligence, with large language models (LLMs) emerging as an effective way to tackle complex tasks. Transformers have achieved state-of-the-art results in various NLP tasks, such as machine translation  and text generation . As a result, they have become the preferred architecture for NLP, where BERT , GPT-3 , PaLM  were proposed. They have demonstrated remarkable learning and reasoning capabilities and have proven to be more efficient than traditional models when processing natural language.

Additionally, LLMs can be fine-tuned for multiple purposes without requiring a new build from scratch, making them a versatile tool for AI applications. Moreover, recent studies on the in-context learning abilities of LLMs have demonstrated that even without further fine-tuning, LLMs can generalize to new tasks with only a few demonstration examples in the prompt. To understand how LLMs become in-context learners, recent works have studied the problem and provided mathematical explanations from the Transformer architecture perspective, showing a simplified linear self-attention layer of Transformer can learn linear functions similarly as a step of gradient descent . While such linear approximation of full Transformers is overly simplistic, studies on more complex Transformer architecture are needed to further explain the in-context learning phenomenon.

Transformers have a specific type of sequence-to-sequence neural network architecture. They utilize the attention mechanism  that allows them to capture long-range dependencies and context from input data effectively. The core of the attention mechanism is the attention matrix which is comprised of rows and columns, corresponding to individual words or "tokens". The attention matrix represents the relationships within the given text. It measures the importance of each token in a sequence as it relates to the desired output. During the training process, the attention matrix is learned and optimized to improve the accuracy of the model's predictions. Through the attention mechanism, each input token is evaluated based on its relevance to the desired output by assigning a token score. This score is determined by a similarity function that compares the current output state with input states.

Theoretically, the attention matrix is comprised of the query matrix \(Q^{n d}\), the key matrix \(K^{n d}\) and the value matrix \(V^{n d}\). Following , AS23, AS24, AS24, AS24, the computation of the normalized attention function is defined as \(D^{-1}(QK^{})V\). Following the transformer literature, we apply \(\) to a matrix entry-wise way. Here \(D^{n n}\) is diagonal matrix that defined as \(D=((QK^{})_{n})\). Intuitively, \(D\) denotes the softmax normalization matrix. A more general computation formulation can be written as

\[}{}}X^{})}_{n n}_{n d}_{d d},\]

where

\[D:=((XQK^{}X^{})_{n}).\]

In the above setting, we treat \(Q,K,V^{d d}\) as weights and \(X\) is the input sentence data that has length \(n\) and each word embedding size is \(d\). In the remaining of the part, we will switch \(X\) to notation \(A\) and use \(A\) to denote sentence. Mathematically, the attention computation problem can be formulated as a regression problem in the following sense

**Definition 1.1**.: _We consider the following problem_

\[_{X^{d d}}\|D^{-1}(AXA^{})-B\|_{F}\]

_where \(A^{n d}\) can be treated as a length-\(n\) document and each word has length-\(d\) embedding size. Here \(D=(AXA^{}_{n})\). For any given \(A^{n d}\) and \(B^{n n}\), the goal is to find some weight \(X\) to optimize the above objective function._

In contrast to the formulation in , the parameter \(X\) in Definition 1.1 is equivalent to the \(QK^{}^{d d}\) in the generalized version of  (e.g. replacing \(Q^{n d}\) by \(XQ\) where \(X^{n d}\) and \(Q^{d d}\). Similarly for \(K\) and \(V\). In such scenario, \(X\) can be viewed as a matrix representation of a length-\(n\) sentence.).

A number of work  study the in-context learning from mathematical perspective in a much simplified setting than Definition 1.1, which is linear regression formulation as in Definition 1.2. They show linear Transformer without softmax unit in its attention layer can mimic the ability of gradient descent in learning linear functions in context. While the softmax unit plays an important role in attention computations of full Transformers, their simplification of the softmax unit leaves a gap in explaining LLMs' in-context learning abilities.

**Definition 1.2**.: _Given a matrix \(A^{n d}\) and \(b^{n}\), the goal is to solve_

\[_{x}\|Ax-b\|_{2}\]Several theoretical transformer work have studied either exponential regression  or softmax regression problem . In this work, to take a step forward to understand the softmax unit in the attention scheme in LLMs. We consider the following softmax regression and study the in-context learning phenomena and its closeness to gradient descent from the data transformation perspective.

**Definition 1.3** (Softmax Regression).: _Given a \(A^{n d}\) and a vector \(b^{n}\), the goal is to solve_

\[_{x^{d}}\|(Ax),_{n}^{-1}(Ax) -b\|_{2}\]

We remark that the Definition 1.3 of Softmax Regression is a formulation in between Definition 1.2 and Definition 1.1.

We state our major result as follows:

**Theorem 1.4** (Bounded shift for in-context learning, informal version of the combination of Theorem 4.2 and Theorem 4.3).: _If the following conditions hold: Let \(A^{n d}\). Let \(b^{n}\). \(\|A\| R\). Let \(\|A(x_{t+1}-x_{t})\|_{}<0.01\). \(\|(A_{t+1}-A_{t})x\|_{}<0.01\). Let \(R 4\). Let \(M:=n^{1.5}(10R^{2})\). We consider the softmax regression (Definition 1.3) problem_

\[_{x}\|(Ax),_{n}^{-1}(Ax)-b\|_{2}.\]

* **Part 1.**_If we move the_ \(x_{t}\) _to_ \(x_{t+1}\)_, then we're solving a new softmax regression with_ \(_{x}\|(Ax),_{n}^{-1}(Ax)-\|_{2}\) _where_ \(|-b\|_{2} M\|x_{t+1}-x_{t}\|_{2}\)__
* **Part 2.**_If we move the_ \(A_{t}\) _to_ \(A_{t+1}\)_, then we're solving a new softmax regression with_ \(_{x}\|(Ax),_{n}^{-1}(Ax)-\|_{2}\) _where_ \(|-b\|_{2} M\|A_{t+1}-A_{t}\|\)__

Recall that \(A^{n d}\) denotes a length-\(n\) document and each word has the length-\(d\) embedding size and \(x\) denotes the simplified version of \(QK^{}\). One-step gradient descent can be treated as an update to the model's weight \(x\). Thus, part 1 of our result (Theorem 1.4) implies that the data transformation of \(b\) induced by gradient-descent on the \(_{2}\) regression loss is bounded by \(M\|x_{t+1}-x_{t}\|_{2}\).

According to , to do in-context learning, a self-attention layer update can be treated as an update to the tokenized document \(A\). For detailed derivation, please refer to . Thus, part 2 of our result (Theorem 1.4) implies that the data transformation of \(b\) induced by a single self-attention layer is bounded by \(M\|A_{t+1}-A_{t}\|\).

We remark that the data transformation of \(b\) induced by \(1)\) a single self-attention layer and by \(2)\) gradient-descent on the \(_{2}\) regression loss are both bounded. The bounded transformation of \(b\) implies that when training self-attention-only Transformers for fundamental regression tasks, the models learned by gradient-descent and Transformers show great similarity.

Roadmap.In Section 2, we give some preliminaries. In Section 3, we compute the gradient of the loss function with softmax function with respect to \(x\). Those functions include \((x)^{-1}\), \((x)\) and \(f(x)\). In Section 4, we give our formal theoretical results, validated by numerical experiments presented in Section 5. In Section 6, we conclude our paper.

## 2 Preliminary

In Section 2.1, we introduce the notations used in this paper. In Section 2.2, we give some facts about the basic algebra. In Section 2.3, we propose the lower bound on \((Ax),_{n}\).

### Notations

For a positive integer \(n\), we use \([n]\) to denote \(\{1,2,,n\}\), for any positive integer \(n\). We use \([]\) to denote expectation. We use \([]\) to denote probability. We use \(_{n}\) to denote the vector where all entries are one. We use \(_{0}\) to denote the vector where all entries are zero. The identity matrix of size \(n n\) is represented by \(I_{n}\) for a positive integer \(n\). The symbol \(\) refers to real numbers and \(_{ 0}\) represents non-negative real numbers. For any vector \(x^{n}\), \((x)^{n}\) denotes a vector where the \(i\)-th entry is \((x_{i})\) and \(\|x\|_{2}\) represents its \(_{2}\) norm, that is, \(\|x\|_{2}:=(_{i=1}^{n}x_{i}^{2})^{1/2}\). We use \(\|x\|_{}\) to denote \(_{i[n]}|x_{i}|\). For any vector \(x^{n}\) and vector \(y^{d}\), we use \( x,y\) to denote the inner product of vector \(x\) and \(y\). The notation \(B_{i}\) is used to indicate the \(i\)-th row of matrix \(B\). If \(a\) and \(b\) are two column vectors in \(^{n}\), then \(a b\) denotes a column vector where \((a b)_{i}=a_{i}b_{i}\). For a square and full rank matrix \(B\), we use \(B^{-1}\) to denote the true inverse of \(B\).

### Basic Algebras

**Fact 2.1**.: _For vectors \(x,y^{n}\), we have_

* \(\|x y\|_{2}\|x\|_{}\|y\|_{2}\)__
* \(\|x\|_{}\|x\|_{2}\|x\|_{}\)__
* \(\|(x)\|_{}(\|x\|_{2})\)__
* _For any_ \(\|x-y\|_{} 0.01\)_, we have_ \(\|(x)-(y)\|_{2}\|(x)\|_{2} 2\|x-y\|_{}\)__

**Fact 2.2**.: _For matrices \(X,Y\), we have_

* \(\|X^{}\|=\|X\|\)__
* \(\|X\|\|Y\|-\|X-Y\|\)__
* \(\|X+Y\|\|X\|+\|Y\|\)__
* \(\|X Y\|\|X\|\|Y\|\)__
* _If_ \(X Y\)_, then_ \(\|X\|\|Y\|\)__

### Lower bound on \(\)

**Lemma 2.3**.: _If the following conditions holds_

* \(\|A\| R\)__
* \(\|x\|_{2} R\)__
* _Let_ \(\) _be lower bound on_ \((Ax),_{n}\)__

_Then we have_

\[(-R^{2})\]

Proof.: We have

\[(Ax),_{n} =_{i=1}^{n}((Ax)_{i})\] \[_{i[n]}((Ax)_{i})\] \[_{i[n]}(-|(Ax)_{i}|)\] \[=(-_{i[n]}|(Ax)_{i}|)\] \[=(-\|Ax\|_{})\] \[(-\|Ax\|_{2})\] \[(-R^{2})\]

the 1st step follows from simple algebra, the 2nd step comes from simple algebra, the 3rd step follows from the fact that \((x)(-|x|)\), the 4th step follows from the fact that \((-x)\) is monotonically decreasing, the 5th step comes from definition of \(_{}\) norm, the 6th step follows from Fact 2.1, the 7th step follows from the assumption on \(A\) and \(x\)Softmax Function with Respect to \(x\)

In Section 3.1, we give the definitions used in the computation. In Section 3.2, we compute the gradient of the loss function with softmax function with respect to \(x\). Those functions includes \((x)^{-1}\), \((x)\) and \(f(x)\).

### Definitions

We define function softmax \(f\) as follows

**Definition 3.1** (Function \(f\), Definition 5.1 in ).: _Given a matrix \(A^{n d}\). Let \(_{n}\) denote a length-\(n\) vector that all entries are ones. We define prediction function \(f:^{d}^{n}\) as follows_

\[f(x):=(Ax),_{n}^{-1}(Ax).\]

**Definition 3.2** (Loss function \(L_{}\), Definition 5.3 in ).: _Given a matrix \(A^{n d}\) and a vector \(b^{n}\). We define loss function \(L_{}:^{d}\) as follows_

\[L_{}(x):=0.5\|(Ax),_{n}^{-1}(Ax)-b\| _{2}^{2}.\]

For convenient, we define two helpful notations \(\) and \(c\)

**Definition 3.3** (Normalized coefficients, Definition 5.4 in ).: _We define \(:^{d}\) as follows_

\[(x):=(Ax),_{n}.\]

_Then, we can rewrite \(f(x)\) (see Definition 3.1) and \(L_{}(x)\) (see Definition 3.2) as follows_

* \(f(x)=(x)^{-1}(Ax)\)_._
* \(L_{}(x)=0.5\|(x)^{-1}(Ax)-b\|_{2}^{2}\)_._
* \(L_{}(x)=0.5\|f(x)-b\|_{2}^{2}\)_._

**Definition 3.4** (Definition 5.5 in ).: _We define function \(c:^{d}^{n}\) as follows_

\[c(x):=f(x)-b.\]

_Then we can rewrite \(L_{}(x)\) (see Definition 3.2) as follows_

* \(L_{}(x)=0.5\|c(x)\|_{2}^{2}\)_._

### Gradient Computations

We state a lemma from previous work,

**Lemma 3.5** (Gradient, Lemma 5.6 in ).: _If the following conditions hold_

* _Given matrix_ \(A^{n d}\) _and a vector_ \(b^{n}\)_._
* _Let_ \((x)\) _be defined in Definition_ 3.3_._
* _Let_ \(f(x)\) _be defined in Definition_ 3.1_._
* _Let_ \(c(x)\) _be defined in Definition_ 3.4_._
* _Let_ \(L_{}(x)\) _be defined in Definition_ 3.2_._

_For each \(i[d]\), we have_

* _Part 1.__Part 2._ \[(Ax),_{n}}{x_{i}}= (Ax),A_{*,i}\]
* _Part 3._ \[(x)^{-1}}{x_{i}}=-(x)^{-1} f(x ),A_{*,i}\]
* _Part 4._ \[f(x)}{x_{i}}=c(x)}{x_{i}}= \,- f(x),A_{*,i} f(x)\,\,+f(x) A_{*,i}\]
* _Part 5._ \[L_{}(x)}{x_{i}}=^{}}_{ 1 n}_{n 1}_{}+(f(x))}_{n n} _{n 1}\]

## 4 Main Results

In Section 4.1, we show the lipschitz bound of function \(f\). In Section 4.2, we show our upper bound result of \(_{b}\) with respect to \(x\). In Section 4.3, we show our upper bound result of \(_{b}\) with respect to \(A\).

### Lipschitz Bound

To bound the shift of \(b\), we first show the Lipschitz property for the basic functions:

* \(\|(Ax)-(Ay)\|_{2} 2R(R^{2})\|x-y\|_{2}\)
* \(|(x)-(y)|\|(Ax)-(Ay)\|_{2}\)
* \(|(x)^{-1}-(y)^{-1}|^{-2}|(x)-(y)|\)

We can show that

**Lemma 4.1**.: _If the following conditions hold_

* _Let_ \((0,1)\)_._
* _Let_ \(_{b,1}^{n}\) _be defined as Definition_ B.3_._
* _Let_ \(_{b,2}^{n}\) _be defined as Definition_ B.3_._
* _Let_ \(_{b}=_{b,1}+_{b,2}\)_._
* _Let_ \(R 4\)_._

_We have_

* _Part 1._ \[\|_{b,1}\|_{2} 2^{-2}n^{1.5}(2R^{2})\|x_{t+1}-x_{t}\|_{2}\]
* _Part 2._ \[\|_{b,2}\|_{2} 2^{-1}R(R^{2})\|x_{t+1}-x_{t}\|_{2}\]
* _Part 3._ \[\|)-f(x_{t})}_{_{b}}\|_{2} 4^{-2}n^{1.5}R (2R^{2})\|x_{t+1}-x_{t}\|_{2}\]Proof.: **Proof of Part 1.** We have

\[\|_{b,1}\|_{2} |(x_{t+1})^{-1}-(x_{t})^{-1}|\|(Ax_{t+1}) \|_{2}\] \[|(x_{t+1})^{-1}-(x_{t})^{-1}| (R^{2})\] \[^{-2}|(x_{t+1})-(x_{t})| (R^{2})\] \[^{-2}\|(Ax_{t+1})-(Ax_{t})\|_ {2}(R^{2})\] \[^{-2} 2R(R^{2})\|x_{t+1}-x _{t}\|_{2}(R^{2})\] \[=2^{-2}n^{1.5}R(2R^{2})\|x_{t+1}-x_{t}\|_{2}\]

where the first step follows from definition, the second step follows from assumption on \(A\) and \(x\), the third step follows Lemma B.7, the forth step follows from Lemma B.6, the fifth step follows from Lemma B.5.

**Proof of Part 2.**

We have

\[\|_{b,2}\|_{2} |(x_{t+1})^{-1}|\|(Ax_{t+1})-(Ax_{t})\|_{2}\] \[^{-1}\|(Ax_{t+1})-(Ax_{t})\|_{2}\] \[^{-1} 2R(2R^{2})\|x_{t+1}-x_{t}\|_{2}\]

where the first step follows from definition, the 2nd step comes from Lemma B.5.

**Proof of Part 3.**

We have

\[\|_{b}\|_{2} =\|_{b,1}+_{b,2}\|_{2}\] \[\|_{b,1}\|_{2}+\|_{b,2}\|_{2}\] \[ 2^{-2}n^{1.5}R(2R^{2})\|x_{t+1}-x_{t}\|_{2}+2 ^{-1}n^{0.5}R(2R^{2})\|x_{t+1}-x_{t}\|_{2}\] \[ 2^{-2}n^{1.5}R(2R^{2})\|x_{t+1}-x_{t}\|_{2}+2 ^{-2}n^{1.5}R(2R^{2})\|x_{t+1}-x_{t}\|_{2}\] \[ 4^{-2}n^{1.5}R(2R^{2})\|x_{t+1}-x_{t}\|_{2}\]

where the 1st step follows from the definition of \(_{b}\), the 2nd step follows from triangle inequality, the 3rd step follows from the results in Part 1 and Part 2, the 4th step follows from the fact that \(n 1\) and \(^{-1} 1\), the 5th step follows from simple algebra. 

Similarly, we can show the Lipschitz property of function \(f\) with respect to \(A\) as the following

\[\|f(A_{t+1})-f(A_{t})\|_{2}\] \[ 4^{-2}n^{1.5}R(2R^{2})\|A_{t+1}-A_{t}\|_{2}\]

Due to space limitation, we defer formal lemma and proof to D.2.

### Shifting Weight Parameter \(x\)

**Theorem 4.2** (Bounded shift for shifting the weight parameter, formal of Theorem 1.4).: _If the following conditions hold_

* _Let_ \(A^{n d}\)__
* \(\|A\| R\)__
* \(\|A(x_{t+1}-x_{t})\|_{}<0.01\)__
* _Let_ \(R 4\)__
* _Let_ \(M:=n^{1.5}(10R^{2})\)_We consider the softmax regression problem_

\[_{x}\|(Ax),_{n}^{-1}(Ax)-b\|_{2}\]

_If we move the \(x_{t}\) to \(x_{t+1}\), then we're solving a new softmax regression problem with_

\[_{x}\|(Ax),_{n}^{-1}(Ax)-\|_{2}\]

_where_

\[\|-b\|_{2} M\|x_{t+1}-x_{t}\|_{2}\]

Proof.: We have

\[\|-b\|_{2}  4^{-2}n^{1.5}R(2R^{2})\|x_{t+1}-x_{t}\|_{2}\] \[ 4n^{1.5}R(2R^{2})(2R^{2})\|x_{t+1}-x_{t}\|_{2}\] \[ n^{1.5}(4R)(4R^{2})\|x_{t+1}-x_{t}\|_{2}\] \[ n^{1.5}(6R^{2})(4R^{2})\|x_{t+1}-x_{t}\|_{2}\] \[ n^{1.5}(10R^{2})\|x_{t+1}-x_{t}\|_{2}\] \[ M\|x_{t+1}-x_{t}\|_{2}\]

where the 1st step follows from Lemma 4.1, the 2nd step comes from Lemma 2.3, the 3rd step comes from simple algebra, the 4th step follows from simple algebra, the 5th step follows from simple algebra and the 6th step follows from the definition of \(M\). 

### Shifting Sentence Data \(A\)

**Theorem 4.3** (Bounded shift for in-context learning, formal of Theorem 1.4).: _If the following conditions hold_

* _Let_ \(A^{n d}\)__
* \(\|A\| R\)__
* \(\|(A_{t+1}-A_{t})x\|_{}<0.01\)__
* _Let_ \(R 4\)__
* _Let_ \(M:=n^{1.5}(10R^{2})\)_._

_We consider the softmax regression problem_

\[_{x}\|(Ax),_{n}^{-1}(Ax)-b\|_{2}\]

_If we move the \(A_{t}\) to \(A_{t+1}\) then we're solving a new softmax regression problem with_

\[_{x}\|(Ax),_{n}^{-1}(Ax)-\|_ {2}\]

_where_

\[\|-b\|_{2} M\|A_{t+1}-A_{t}\|.\]

Proof.: We have

\[\|-b\|_{2}  4^{-2}n^{1.5}R(2R^{2})\|A_{t+1}-A_{t}\|\] \[ 4n^{1.5}R(2R^{2})(2R^{2})\|A_{t+1}-A_{t}\|\] \[ n^{1.5}(4R)(4R^{2})\|A_{t+1}-A_{t}\|\] \[ n^{1.5}(6R^{2})(4R^{2})\|A_{t+1}-A_{t}\|\] \[ n^{1.5}(10R^{2})\|A_{t+1}-A_{t}\|\] \[ M\|A_{t+1}-A_{t}\|\]

where the 1st step follows from Lemma D.5, the 2nd step follows from Lemma 2.3, the 3rd step follows from simple algebra, the 4th step comes from simple algebra and the 6th step follows from the definition of \(M\).

Numerical Experiments

In this section, we present our numerical experiments to validate our theoretical results that when training self-attention-only Transformers for softmax regression tasks, the models learned by gradient-descent and Transformers show great similarity.

### Experiments Setup

According to Definition 1.3, we construct the synthetic softmax regression tasks consists of randomly sampled length-\(n\) documents \(A^{n d}\) where each word has the \(d\)-dimensional embedding and targets \(b^{n}\). Each document is generated from a unique random seed. In our experiments, we choose a set of different document length \(n\) and a set of different embedding size \(d\).

Following , we compare the following two models in our experiment

* a trained single self-attention (SA) layer with softmax unit approximating full Transformers.
* a softmax regression model trained with one-step gradient descent (GD).

The training objective for both models is defined as in Definition 1.3. For the single self-attention layer with a softmax unit, we choose the learning rate \(_{}=0.005\). For the softmax regression model, we determine the optimal learning rate \(_{}\) by minimizing the \(_{2}\) regression loss over a training set of \(10^{3}\) tasks through line search.

To compare the trained single self-attention layer with a softmax unit and the softmax regression model trained with one-step gradient descent, we sample \(10^{3}\) tasks and record the losses of two models. In addition, we follow  to record

* **Pred Diff**: the predictions difference measured with the \(_{2}\) norm: \[\|_{}(A)-_{}(x)\|_{2}\] where \(_{}(A)\) corresponds to the \(\) in Theorem 4.2, and \(_{}(x)\) corresponds to the \(\) in Theorem 4.3.
* **Model Cos**: the cosine similarity between the sensitivities of two models: \[(_{}(x)}{ x}, _{}(A)}{ A})\]
* **Model Diff**: the model sensitivity difference measured with the \(_{2}\) norm: \[\|_{}(x)}{ x}-_{}(A)}{ A}\|_{2}\]

All experiments run on a single NVIDIA RTX2080Ti GPU with 10 independent repetitions.

### Different Document Lengths

For synthetic softmax regression tasks of document length \(n\{200,1000\}\) and word embedding size \(d=20\), the comparison results between a trained single self-attention layer and one-step gradient descent are shown in Figure 1 and Figure 2. Due to space limitation, we present more results with different document length \(n\{25,50,100,200,400,1000\}\) in Appendix E.

We compare two models' losses over training steps of Transformers in Figure 0(a) and Figure 1(a). In Figure 0(b) and Figure 1(b), we show the differences and similarities of two models over the training steps. From the results, we find identical performances of the two models measured in losses. We also observe considerable alignment of the two models across tasks of different document lengths, indicated by decreasing prediction and model difference and increasing cosine similarity between models. Besides, comparing results with different \(n\), we observe that with larger document length, which is common in practical NLP tasks, more training steps are required for Transformers to exhibit such similarities. This shows the crucial role of pretraining stage of Transformers for their in-context learning ability.

### Different Word Embedding Sizes

We also compare trained single self-attention layer and one-step gradient descent on synthetic softmax regression tasks of different word embedding sizes and document length \(n=200\). Similarly, we measure two models' losses and similarities over training steps on each set of tasks. Due to space limitation, we follow  to show in Figure 3 the loss comparisons at the end of training over different embedding size \(d\{5,10,20,35,50\}\). The complete loss curves and measurements of model difference and similarity are presented in Appendix E.

From the results, we again observe similar performances and close alignment of the two models with different word embedding sizes.

To summarize, our numerical results validate our theoretical results in Section 4, showing that when training self-attention-only Transformers for softmax regression tasks, the models learned by gradient-descent and Transformers show great similarity. Note that due to the non-linearity of softmax regression, it is not expected for models to match exactly as implied in our theoretical results in Section 4, which is also observed in our numerical findings.

## 6 Conclusion

The attention mechanism that incorporates the softmax unit is a crucial aspect of Large Language Models (LLMs) and significantly contributes to their extraordinary performance in various Natural Language Processing (NLP) tasks. The ability to learn in-context is highly valued in recent LLMs, and comprehending this concept is vital when querying LLMs. In this study, taking a step further from prior works' studies on linear Transformer's ability of learning linear functions, we examined the in-context learning process from a softmax regression perspective of Transformer's attention mechanism. We established the bound on the data transformations brought about by a single self-attention layer with softmax unit and gradient descent on an L2 regression loss. Our findings suggest that the update acquired through gradient descent and in-context learning are highly similar when training self-attention-only Transformers for softmax regression tasks, which is also validated through our preliminary experimental results. These results offer insights into the theoretical underpinnings of in-context learning in Transformers and can aid in improving the understanding and performance of LLMs in various NLP tasks.