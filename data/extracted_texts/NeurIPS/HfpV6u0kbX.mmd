# Efficient Multi-task LLM Quantization and Serving

for Multiple LoRA Adapters

 Yifei Xia

Peking University

yifeixia@stu.pku.edu.cn

&Fangcheng Fu

Peking University

ccchengff@pku.edu.cn

&Wentao Zhang

Peking University

wentao.zhang@pku.edu.cn

&Jiawei Jiang

Wuhan University

jiawei.jiang@whu.edu.cn

&Bin Cui

Peking University

bin.cui@pku.edu.cn

###### Abstract

With the remarkable achievements of large language models (LLMs), the demand for fine-tuning and deploying LLMs in various downstream tasks has garnered widespread interest. Parameter-efficient fine-tuning techniques represented by LoRA and model quantization techniques represented by GPTQ and AWQ are of paramount significance. However, although these techniques have been widely adopted in single-task scenarios, research is scarce in multi-task scenarios. To be specific, we find that mainstream quantization methods would prevent the base LLM from being shared among tasks, so current LLM serving systems are infeasible to integrate LLM quantization with multiple LoRA adapters to achieve memory-efficient multi-task serving. Moreover, existing LLM serving systems lack support for dynamic task addition and overlook the workload differences among tasks, leading to inefficiencies in multi-task scenarios.

This work proposes _LoRA-Inlaid_, an efficient multi-task LLM serving system. On the one hand, _LoRA-Inlaid_ designs a flexible and efficient multi-task quantization algorithm (MLGPTQ) that facilitates the sharing of a single quantized model for multiple LoRA adapters, which significantly reduces the memory consumption for model deployment. Meanwhile, it supports adding LoRA adapters for new tasks on the fly, without sacrificing the stability of online services. On the other hand, _LoRA-Inlaid_ develops a novel multi-task scheduling algorithm guided by output length prediction and grouping among different tasks, which effectively shrinks the memory consumption and avoids frequent switching of LoRA adapters. Empirical results verify that _LoRA-Inlaid_ outperforms existing state-of-the-art LLM serving systems by up to 1.58\(\) in terms of throughput, 1.76\(\) in terms of average latency, 2\(\) in terms of job completion time, and 10\(\) in terms of SLO Attainment, while maintaining the same level of model quality.

## 1 Introduction

Large language models (LLMs) have demonstrated impressive effectiveness in various domains , and the demand of deploying LLMs in downstream tasks continues to grow . Given the explosive increase in model size and the limitations of hardware resources, "parameter-efficient fine-tuning" (PEFT) and "quantization-then-deployment" have become the most common pathways for deploying LLMs in downstream tasks . On the one hand, PEFT techniques, represented by LoRA (Low-Rank Adaptation) , only train small-scale adapters to adapt the base model to a specific task, significantly reducing the cost of model fine-tuning. On the other hand, low-bit quantization techniques like GPTQ and AWQ  can substantially reduce the memory requirements of model deployment and alleviate memory access overhead during inference, while maintaining model quality.

Although mainstream LLM serving systems like vLLM and TensorRT-LLM  have integrated support for the quantized deployment of fine-tuned models, these systems focus on single-task serving scenarios. With the rising demand for various downstream tasks, efficiently supporting multi-task servicing scenarios has become increasingly crucial. This has led to the emergence of multi-task serving systems supporting multiple LoRA adapters concurrently, such as S-LoRA and Punicia . These systems share a unified base model across different tasks and activate different LoRA adapters based on the incoming requests, enabling the simultaneous processing of multiple tasks in a single batch. However, in multi-task scenarios, existing systems still face three major challenges.

First, existing multi-task serving systems cannot effectively incorporate mainstream model quantization methods such as GPTQ and AWQ. Specifically, these quantization methods require calibration of numerical distributions using task-specific datasets, and the quantization process for each task necessitates activating the corresponding LoRA adapter. Consequently, the base models after quantization are divergent across different tasks, and thus it is infeasible to share a unified quantized model. This limitation leads to performance deficiencies or even unavailability in resource-constrained scenarios.

Second, in practical multi-task serving scenarios, it would be necessary to add new tasks in real time. However, existing systems only support a static number of tasks and are incapable of dynamically adding LoRA adapters. More importantly, after a quantized model is deployed, current solutions do not support any subsequent quantization and deployment for new tasks without affecting the existing tasks. In contrast, adding new tasks typically requires suspending and restarting the serving process, which severely harms the stability and robustness of online services.

Third, incoming requests for different tasks inevitably have workload variations (such as request length, processing time, etc.) and require loading different LoRA adapters for processing. Existing systems overlook these issues during the scheduling for multi-task requests, and thus necessitate loading a large number of adapters in a single scheduling step as well as frequently switching adapters between adjacent scheduling steps, leading to significant efficiency degradation.

To address these challenges, we develop _LoRA-Inlaid_, a resource-efficient and high-performance system for multi-task LLM serving. The main contributions of this paper are as follows.

To begin with, we propose an innovative multi-task quantization algorithm termed MLGPTQ (**M**ulti-**LoRA **GPTQ**), which utilizes multi-task data to perform joint quantization on the base model. This allows the quantized base model to be shared across multiple tasks. In addition, it supports incremental quantization for newly added tasks without impacting the performance of online services.

Subsequently, we introduce a novel multi-task scheduling strategy based on output length prediction and grouping. This effectively reduces memory consumption and memory swapping overhead in multi-task scenarios, significantly enhancing overall system performance.

Based on these two techniques, we develop a brand new multi-task LLM serving system, namely _LoRA-Inlaid_. As shown in Table 1, _LoRA-Inlaid_ integrates multi-task quantization, enables dynamic task addition, and employs the multi-task scheduling strategy, achieving high-performance and flexible multi-task LLM serving in resource-constrained environments.

   System & Multi-task & Multi-task & Dynamic Task & Multi-task \\  & Serving & Quantization & Addition & Scheduling \\  vLLM  \& TensorRT-LLM  & ✗ & ✗ & ✗ & ✗ \\ S-LoRA  \& Punicia  & ✓ & ✗ & ✗ & ✗ \\ _LoRA-Inlaid_ (this work) & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison of supported features of different LLM serving systemsFinally, experimental results demonstrate that, compared to existing systems, _LoRA-Inlaid_ can increase throughput by up to 1.58\(\), reduce average latency and job completion time by up to 1.76\(\) and 2\(\), improve SLO Attainment by up to 10\(\), and support larger-scale language models under the same resource constraints, all while maintaining nearly the same level of model quality.

## 2 Background and Related Works

Low-Rank Adaptation.LoRA , short for Low-Rank Adaptation, is one of the most widely used parameter-efficient fine-tuning (PEFT) techniques. Unlike full-parameter fine-tuning, LoRA fine-tunes only a small adapter, which consists of much fewer parameters than the base model, significantly reducing the training cost. The key idea behind LoRA is that the fine-tuning process should only introduce small changes to the weight matrix of the base model (denoted by \(^{m n}\)), so we can learn two small, low-rank matrices (denoted by \(^{r n},^{m r}\) where \(r m,n\)), and approximate such changes with the product of two matrices (i.e., \(\)).

Low-bit Quantization.Low-bit quantization [7; 8; 12; 22; 23; 42] shrinks the model size effectively and thus reduces the memory requirement when deploying the model. In addition, it usually helps to improve efficiency by decreasing the memory access overhead of the model weights. Consequently, it has been widely adopted in LLM serving. There are various quantization paradigms, with post-training quantization (PTQ) being among the most popular ones. Typically, PTQ computes \(X_{}=(\;(X_{R}/,Q_{},Q_{}))\), where \(X_{R}\) represents the real-valued parameters before quantization, \(X_{}\) represents the parameters after quantization to integers, \(Q_{min}\) and \(Q_{max}\) denote the minimum and maximum values of the quantization range, and \(\) represents the scaling factor. Various PTQ methods calculate the quantization knobs like \(\) with diverse approaches or implement different approximation methods. While mainstream PTQ methods (e.g., GPTQ , AWQ ) have a common ground that they need to calibrate the numerical distribution based on a small task-specific dataset (a.k.a. _the calibration set_), since numerous studies have revealed the accuracy after quantization with dataset calibration is usually significantly higher than that without dataset calibration . Therefore, this paper focuses on quantization with dataset calibration.

Scheduling in LLM Serving.With the explosive applications of LLMs, more and more studies try to evolve the scheduling strategies in LLM serving for better performance. Early systems like FasterTransformer  rely on request-level scheduling. Notably, Yu et al.  introduced Orca, the first iteration-level scheduling with first-come-first-serve (FCFS) order for better batching. Building on this, mainstream LLM serving systems leverage various batching approaches, such as continuous batching in vLLM  and in-flight batching in TensorRT-LLM . FastServe  takes the semi-information of requests (e.g., input length, processed time, etc.) into account and tries to minimize average job completion time. However, none of these scheduling strategies consider the characteristics of multi-task scenarios, as we will discuss in SS3.3.

Multi-task Serving Systems.Since the LoRA fine-tuning technique keeps the base model unaltered, it is feasible to share the same base model across multiple LoRA adapters, so that we can serve requests from multiple tasks within a single batch. Punica  and S-LoRA  are two notable multi-task serving systems, putting forward the initial efforts to support multi-task LLM serving with multiple LoRA adapters. Specific optimization techniques are proposed. For instance, the Segmented Gather Matrix-Vector (SGMV) kernel is developed to enhance memory and computation efficiency when processing requests from different tasks together. In addition, to allocate more GPU memory to intermediate results (typically, KV cache), existing systems maintain the LoRA adapters in CPU memory and only preserve a relatively small number of LoRA adapters in GPU memory. When a LoRA adapter outside GPU memory is needed, it is necessary to perform memory swapping between the CPU and GPU memory.

## 3 _LoRA-Inlaid_

The overview of _LoRA-Inlaid_ is depicted in Figure 1. Given an LLM with multiple LoRA adapters for various downstream tasks, _LoRA-Inlaid_ initiates a joint quantization process (SS3.1), which produces a unified quantized base model that can be shared across the adapters. During the online serving, if new tasks are to be included on the fly, _LoRA-Inlaid_ facilitates a dynamic task addition process (SS3.2) that efficiently conducts incremental re-quantization and seamlessly deploys the added tasks.

Furthermore, _LoRA-Inlaid_ employs a multi-task scheduling strategy (SS3.3) that takes the workload differences into account for better efficiency.

### Multi-task Joint Quantization

As introduced in SS2, mainstream quantization methods require task-specific datasets for calibration. In addition, they mostly follow the _Forward-Aggregate Info-Modify Weight-Quant_ paradigm in Figure 2. This paradigm first simulates the activation distribution for a given task through _Forward_ propagation and _Aggregates Information_ of this specific task. Subsequently, it uses the aggregated information to _Modify_ model _Weights_ to adapt to the task. Finally, the quantization knobs like scales \(\) are calculated based on the modified weights to _Quan_rize the base model.

However, in multi-task scenarios, since different tasks should provide diverse calibration sets and necessitate unique LoRA adapters for computation, the quantized models of different tasks are inevitably divergent. Intuitively, if we wish to tweak existing quantization methods to make the quantized model shareable across tasks, we should quantize the model without any LoRA adapters. In addition, we should either (i) quantize the model without calibration or (ii) quantize the model with a mixed calibration set consisting of the datasets from all tasks.

However, these approaches fail to accurately capture the unique numerical distribution of each task, and suffer from severe accuracy loss (as evaluated in SS4.2). Below we first elaborate on the reason why these approaches fail with the widely used GPTQ  and then propose our solution1.

**Drawbacks of GPTQ in multi-task scenarios.** Directly applying GPTQ in multi-task scenarios has the following drawbacks. First, as aforementioned, GPTQ can only quantize the model without any LoRA adapters, which is infeasible to accurately capture the correct activation information for multiple tasks during _Forward_. Second, in _Aggregate Info_, since the calibration sets from all tasks are mixed, GPTQ simply accumulates the information from different tasks into one Hessian matrix, making each task's specific information diluted and losing the emphasis on critical information from different tasks. Third, in _Modify Weight_, GPTQ relies on the naive, mix-aggregated Hessian matrix, overlooking the varying importance across tasks, which results in suboptimal outcomes. These drawbacks make the direct application of GPTQ in multi-task scenarios ineffective.

**Our MLGPTQ (Multi-LoRA GPTQ) Algorithm** To address these drawbacks, we propose a multi-task quantization algorithm termed MLGPTQ. Our algorithm enables joint quantization of multiple tasks to retain only one quantized base model, while effectively maintaining the model accuracy by capturing the numerical distributions of all tasks. The goal of MLGPTQ is to minimize the errors of activations before and after quantization, i.e.,

\[*{arg\,min}_{Q()}||_{t=1}^{T}((+_{t}_{t})_{t}-(Q()+_{t}_{t}) _{t})||_{2}^{2},\] (1)

Figure 1: Design overview of _LoRA-Inlaid_. The workflow is labeled with numbers in the diagram. 1 _Quaníze and Deploy_ indicates the initiation of the server performing the multi-task quantization and deploying the quantized model and LoRA online. 2 _Schedule_ involves utilizing a multi-task scheduling strategy for 3 _Inference_. If a new task is detected, it invokes 4 _Add Task_ to dynamically add the new task without interruping the ongoing services.

where \(T\) denotes the number of tasks, \(_{t}\) and \(_{t}\) are the low-rank adapter matrices of the \(t\)-th task, \(_{t}\) is the input of \(t\)-th task, and \(\) and \(Q()\) denote the original and quantized weights of a layer.

As shown in Figure 2, During _Forward_, MLGPTQ loads the corresponding LoRA adapters based on each task, accurately computing the activations. In _Aggregate Info_, unlike GPTQ's naive mix-aggregation that disrupts task-specific information, MLGPTQ derives the max-aggregation to solve the objective in Eq. 1 (the derivation can be found in the Appendix A), which has the following form:

\[=--Q(w_{q})}{(_{t}^{-1})_{qq}} _{t}^{-1}e_{q},t^{*}=*{arg\,max}_{t\{1,2,,T\}}( _{t}^{-1})_{qq},\] (2)

where \(_{t}\) denotes the Hessian matrix of the \(t\)-th task,\(w_{q}\) is the \(q\)-th parameter in \(\). To be formal, there are primarily two steps in _Aggregate Info_. First, it calculates the Hessian matrix information for each task individually (i.e., compute \(\{_{t}^{-1}\}_{t=1}^{T}\)) Second, it aggregates the most important information from each one into a max-aggregated Hessian matrix (i.e., \(_{tmp}=(\{_{t}^{-1}\}_{t=1}^{T})\)). In _Modify Weight_, MLGPTQ utilizes the max-aggregated Hessian matrix to adjust the weights according to Eq. 2. Finally in _Quant_, we utilize the modified weights for quantization. Due to space constraints, we only present the core concept of MLGPTQ here. Interested readers are referred to Appendix A for a complete derivation as well as the detailed algorithm.

### Dynamic Task Addition

In real-world online services, there is a need for dynamic task addition (i.e., adding new LoRA adapters). In single-task scenarios, adding new tasks typically requires launching more services with extra hardware resources, which does not affect the services for existing tasks. In multi-task scenarios, there would be interference since all tasks share the same base model. However, we find that none of the existing multi-task serving systems address this problem, lacking a proper solution.

Nevertheless, adding new LoRA adapters on the fly in _LoRA-Inlaid_ is inherently far from trivial since the multi-task quantization poses two challenges: _(1. Unseen Distributions)_ Since the MLGPTQ algorithm is invoked before the new tasks are involved, the quantized model has not captured the distribution information about the new tasks, making it infeasible to work with the new LoRA adapters directly. _(2. Serving Interruption)_ Directly re-quantizing the model requires a substantial amount of memory, so it necessitates pausing the ongoing serving for a while to reserve available space for re-quantization, harming the stability of online services. To support dynamic task addition in multi-task scenarios, _LoRA-Inlaid_ tackles these two obstacles, respectively.

To capture the information of new tasks, a naive solution is to perform full quantization once there are new tasks. Denote \(T_{1},T_{2}\) as numbers of existing and new tasks, respectively. The naive solution runs the two steps of _Aggregate Info_ above with \(T=T_{1}+T_{2}\). However, this leads to redundant computation of \(\{_{t}^{-1}\}_{t=1}^{T}\). In addition, given the commutative property of the \(\) operation, we have \((\{_{t}^{-1}\}_{t=1}^{T})=(\{ (\{_{t}^{-1}\}_{t=1}^{T})_{t=1}^{T_{2}}\},(\{_{t}^{-1}\}_{t=T_{1}+1}^{T_{2}})\), where the first term \((\{_{t}^{-1}\}_{t=1}^{T_{1}})\) has already been computed as \(_{tmp}\) in the previous quantization. Inspired by this, _LoRA-Inlaid_ caches \(_{tmp}\) so that the incremental quantization can be done as follows. In _Forward_, it capture the activation information of new task \(T_{1}+1,,T_{2}\). In _Aggregate

Figure 2: Process of MLGPTQ vs GPTQ. Both MLGPTQ and GPTQ follow the _Forward-Aggregate Info-Modify Weight-Quant_ paradigm. MLGPTQ primarily improves the first three steps, aiming to better gather and highlight critical information for all tasks.

_Info_, it computes the Hessian matrices for new tasks \(\{_{t}^{-1}\}_{t=T_{1}+1}^{T_{2}}\), and then max-aggregates the \(T_{2}+1\) matrices (i.e., \(\{_{t}^{-1}\}_{t=T_{1}+1}^{T_{2}}\) and the cached \(_{tmp}^{( cached)}\)). At last, it performs _Modify Weight_ and _Quant_, as introduced in SS3.1. By doing so, incremental quantization with \(T_{2}\) tasks is identical to full quantization with \(T_{1}+T_{2}\) tasks, while avoiding redundant computation.

To avoid halting the ongoing services, _LoRA-Inlaid_ spawns a background thread for incremental quantization. Moreover, it is done in a layer-by-layer manner to reduce the memory consumption -- for each (unquantized) model weight, we load it from CPU memory to GPU memory, perform incremental quantization, remove it from GPU memory, and proceed to the next model weight. The IO between CPU-GPU is overlapped with computation. Thus, _LoRA-Inlaid_ supports seamless task addition on the fly and has very little influence on the ongoing services, as evaluated in SS4.4.

Putting them together, _LoRA-Inlaid_ develops an asynchronous, layer-wise re-quantization mechanism, which accomplishes incremental quantization with the new tasks and cached Hessian matrices asynchronously, without interrupting the serving.

### Multi-task Scheduling

Despite extensive research on scheduling strategies for LLM serving, these approaches primarily focus on single-task serving, leaving the unique characteristics in the multi-task scenarios neglected. Below we analyze two limitations of existing scheduling strategies in multi-task serving. Besides, due to the space constraint, we briefly introduce the corresponding solutions in _LoRA-Inlaid_, while leaving the details of our multi-task scheduling algorithm in Appendix B.

_Limitation 1: Divergent Output Length Distributions Leading to High Average Completion Time._ As shown in Figure 4, the distributions of input and output lengths vary significantly across different tasks, while requests of the same task exhibit clustering effects. Current strategies mainly rely on semi-information (e.g., input length, processed time, etc.) to make the scheduling decisions, but do not consider the information of output length since it is not the prior knowledge. Intuitively, this may work fine for single-task scenarios where the vast majority of requests fall within the same workload and thus the clustering effect exists. However, it is unsuitable for multi-task scenarios due to the divergent output length distributions across different tasks. Eventually, we find that existing scheduling strategies suffer from heavy performance degradation when applied to multi-task serving.

_Solution 1: Scheduling Guided by Output Length Prediction._ Existing research has shown that the output lengths can be accurately predicted by a small, distilled model given the requests . Inspired by this, we leverage a number of small models, to predict the output lengths of incoming requests. Particularly, upon receiving a new request, we predict its output length on CPU using a small model (255MB). Note that the output length prediction takes about 16 milliseconds for one request on CPU, while it takes about 200 milliseconds or more to finish the inference of one request on GPU. Hence, we can completely overlap the prediction, without occupying any GPU computing resources. Based on the predictions, we employ a Shortest Remaining Time First (SRTF) scheduling, which prioritizes requests with the shortest remaining processing time and has been proven to minimize the average completion time in the field of job scheduling .

_Limitation 2: Excessive Tasks Involved in each Step Leading to Expensive Memory Access Overhead._ Due to the randomness and dynamicity of request arrivals, multiple tasks are to be scheduled in each step. However, owing to the lack of consideration upon the task for each request, existing scheduling

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

of tasks, and it encounters out-of-memory (OOM) errors in several cases. In contrast, _LoRA-Inlaid_ supports all cases well. More importantly, since _LoRA-Inlaid_ is able to reserve more memory for intermediate results (e.g., KV cache) in serving, it achieves higher performance than the baselines. For instance, _LoRA-Inlaid_ surpasses S-LoRA by 26.5%, 31.3%, 24.1% on average, and up to 58.1%, 76.3%, and 99.9%, in terms of throughput, latency, and JCT, respectively.

**SLO Attainment.** We also assess the SLO Attainment under different serving loads by varying the request rates and maximum request lengths. The results are shown in Figure 7. In short, compared to S-LoRA and vLLM, _LoRA-Inlaid_ improves the SLO Attainment by 3.9\(\), 8.5\(\) on average, and up to 10\(\), 38\(\), respectively. Furthermore, we observe that as the request rate or maximum sequence length increases, S-LoRA and vLLM experience a steep decline in SLO Attainment while _LoRA-Inlaid_ does not. This demonstrates the excellent adaptability of _LoRA-Inlaid_ to various serving loads.

### More Experiments

**Scalability.** We investigate the scalability w.r.t. number of tasks. As shown in Table 3, vLLM suffers from significant performance decline, dropping by 56%-73% when the number of tasks increases from 2 to 4, and eventually encountering out-of-memory (OOM) errors when the number of tasks reaches 5. In contrast, under all experimented request rates, the throughput of _LoRA-Inlaid_ hardly declines, even with 1000 tasks served simultaneously. S-LoRA also supports a large number of tasks, while _LoRA-Inlaid_ consistently achieves better performance across all kinds of workloads.

**Ablation Studies of Multi-task Scheduling and Multi-task Quantization.** We compare different scheduling strategies on _LoRA-Inlaid_. The results are shown in the left of Figure 8. "Ours (w/o group)", "Ours (w/o prediction)" and "Ours (w/o SRTF)" represent three variants of our multi-task scheduling strategy without task grouping, without output length prediction and without the prediction-based SRTF, respectively. "FIFO" is the strategy adopted in S-LoRA and vLLM, and "Skip-join

Figure 6: System performance in terms of throughput (higher is better), latency (lower is better), and JCT (lower is better) under various request rates (\(x\)-axis) and numbers of tasks (\(T\)).

Figure 7: SLO Attainment (higher is better) under various serving loads (RTX 4090).

 
**Task num** &  &  &  &  &  &  &  \\ 
**Rags rate** & **5** & **10** & **20** & **5** & **10** & **20** & **5** & **10** & **20** & **5** & **10** & **20** & **–** & **5** & **10** & **20** & **5** & **10** & **20** \\  _LoRA-Inlaid_ & 3.89 & 4.70 & 4.86 & 3.78 & 4.66 & 4.81 & 3.82 & 4.77 & 4.89 & 3.71 & 4.61 & 4.73 & — & 3.60 & 4.25 & 4.58 & 3.42 & 4.02 & 4.22 \\  S-LoRA & 2.93 & 3.45 & 3.51 & 2.97 & 3.86 & 3.54 & 2.91 & 3.36 & 3.58 & 2.97 & 3.40 & 3.55 & -2.87 & 3.35 & 3.36 & 2.78 & 3.26 & 3.28 \\  vLLM & 1.77 & 2.46 & 2.98 & 1.02 & 1.68 & 2.27 & 0.77 & 0.76 & 0.80 & OOM & OOM & OOM & — & OOM & OOM & OOM & OOM & OOM \\  

Table 3: Scalability comparison in terms of throughput (reqs/s, higher is better) under different request rates and number of LoRA adapters (LLAMA2-7B@RTX 4090).

MLFQ" represents the strategy in FastServe . It is evident that our multi-task scheduling strategy achieves the best performance in terms of SLO Attainment. The designs of task grouping, output length prediction, and SRTF increase the SLO Attainment by 1.16\(\), 1.23\(\) and 2.27\(\) on average, respectively. We also explore the individual impact of multi-task quantization as shown in the right of Figure 8. Specificically, we consider a variant of _LoRA-Inlaid_, which disables quantization (i.e., the served model is not quantized), denoted as "Ours (w/o quant)". The results show that multi-task quantization brings 39% improvement ("Ours" vs. "Ours (w/o quant)") when serving the 7B model. Additionally, without quantization, it will lead to OOM when serving the 13B model.

**Dynamic Task Addition.** We evaluate the ability of dynamic task addition in _LoRA-Inlaid_ by adding 1, 5, and 10 tasks to a heavily loaded service on the fly. The results in Figure 9 show that the throughput undergoes 10%-13% of degradation during the task addition, regardless of the number of tasks added. This is worthwhile given that the online service need not be interrupted. Meanwhile, to evaluate the time consumption of dynamic task addition, we conducted an experiment where there are 5 tasks in the ongoing service and another 5 tasks need to be added. We measured the time cost of three approaches: "Full Quant", which halts the serving and performs full quantization with 10 tasks, "Incr Quant offline", an offline variant (which halts the serving) of our incremental quantization on the 5 new tasks without layer-by-layer quantization, and "Incr Quant", our incremental quantization with the 5 new tasks, which works concurrently with the ongoing service. As shown in Table 4, by avoiding the redundant computation, the time cost of forward process and calculation of Hess matrix can be reduced greatly, accelerating quantization. Moreover, although the layer-by-layer mechanism slows down the quantization by 1.26 \(\) due to the extra IO, it reduces the memory greatly and does not halt the serving. These empirical results validate the flexibility and robustness of _LoRA-Inlaid_ for multi-task serving.

## 5 Conclusion and Limitations

In this work, we focused on LLM serving in multi-task scenarios and developed a multi-LoRA task serving system, namely _LoRA-Inlaid_. On one hand, we designed a flexible and efficient dynamic multi-task quantization algorithm that supports the joint quantization of models for multiple tasks, significantly reducing the memory requirements for model deployment. We also facilitated real-time dynamic task addition, enhancing the stability and flexibility of online services. On the other hand, we introduced a novel multi-task scheduling strategy based on output length prediction and grouping, effectively resolving the issues of high memory overhead and frequent memory swapping when applying existing strategies in multi-task scenarios. Extensive experiments demonstrated that _LoRA-Inlaid_ significantly outperforms existing LLM serving systems.

Despite the effectiveness of _LoRA-Inlaid_, it still has several limitations. First, our quantization does not detect the existence of malicious or poisoning tasks, which might be intentionally crafted to harm the other tasks. Second, our scheduling does not consider the fairness among tasks (e.g., balancing the total numbers of output tokens for all tasks), which may be essential for shared service platforms. Third, it only supports language tasks while requiring some system re-designs for multi-modal tasks. We wish to leave the exploration of these issues as future works.