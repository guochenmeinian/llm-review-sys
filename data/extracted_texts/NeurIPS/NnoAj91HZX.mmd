# No-Regret M\({}^{\natural}\)-Concave Function Maximization:

# No-Regret M\({}^{}\)-Concave Function Maximization:

Stochastic Bandit Algorithms and NP-Hardness of

Adversarial Full-Information Setting

 Taihei Oki

Hokkaido University

Hokkaido, Japan

oki@icredd.hokudai.ac.jp

&Shinsaku Sakaue

The University of Tokyo and RIKEN AIP

Tokyo, Japan

sakaue@mist.i.u-tokyo.ac.jp

Equal contribution, alphabetical order.

###### Abstract

M\({}^{}\)-concave functions, a.k.a. gross substitute valuation functions, play a fundamental role in many fields, including discrete mathematics and economics. In practice, perfect knowledge of M\({}^{}\)-concave functions is often unavailable a priori, and we can optimize them only interactively based on some feedback. Motivated by such situations, we study online M\({}^{}\)-concave function maximization problems, which are interactive versions of the problem studied by Murota and Shioura (1999). For the stochastic bandit setting, we present \(O(T^{-1/2})\)-simple regret and \(O(T^{2/3})\)-regret algorithms under \(T\) times access to unbiased noisy value oracles of M\({}^{}\)-concave functions. A key to proving these results is the robustness of the greedy algorithm to local errors in M\({}^{}\)-concave function maximization, which is one of our main technical results. While we obtain those positive results for the stochastic setting, another main result of our work is an impossibility in the adversarial setting. We prove that, even with full-information feedback, no algorithms that run in polynomial time per round can achieve \(O(T^{1-c})\) regret for any constant \(c>0\) unless \(=\). Our proof is based on a reduction from the matroid intersection problem for three matroids, which would be a novel idea in the context of online learning.

## 1 Introduction

M\({}^{}\)-concave functions form a fundamental function class in _discrete convex analysis_, and various combinatorial optimization problems are written as M\({}^{}\)-concave function maximization. In economics, M\({}^{}\)-concave functions (restricted to the unit-hypercube) are known as _gross substitute valuations_; in operations research, M\({}^{}\)-concave functions are often used in modeling resource allocation problems . Furthermore, M\({}^{}\)-concave functions form a theoretically interesting special case of (DR-)submodular functions that the greedy algorithm can _exactly_ maximize (see, Murota and Shioura , Murota [32, Note 6.21], and Soma [48, Remark 3.3.1]), while it is impossible for the submodular case  and the greedy algorithm can find only _approximately_ optimal solutions . Due to the wide-ranging applications and theoretical importance, efficient methods for maximizing M\({}^{}\)-concave functions have been extensively studied .

When it comes to maximizing M\({}^{}\)-concave functions in practice, we hardly have perfect knowledge of objective functions in advance. For example, it is difficult to know the exact utility an agent gains from some items, which is often modeled by a gross substitute valuation function. Similar issues are also prevalent in submodular function maximization, and researchers have addressed them by developing _no-approximate-regret_ algorithms in various settings, including stochastic/adversarial environmentsand full-information/bandit feedback [49; 14; 43; 54; 16; 36; 37; 38; 51; 55; 11; 40]. On the other hand, no-regret algorithms for \(^{}\)-concave function maximization have not been well studied, despite the aforementioned importance and practical relevance. Since the greedy algorithm can exactly solve \(^{}\)-concave function maximization, an interesting question is whether we can develop no-regret algorithms--in the standard sense _without approximation_--for \(^{}\)-concave function maximization.

### Our contribution

This paper studies online \(^{}\)-concave function maximization for the stochastic bandit and adversarial full-information settings. Below are details of our results.

In Section 4, we study the stochastic bandit setting, where we can only observe values of an underlying \(^{}\)-concave function perturbed by sub-Gaussian noise. We first consider the stochastic optimization setting and provide an \(O(T^{-1/2})\)-simple regret algorithm (Theorem 4.2), where \(T\) is the number of times we can access the noisy value oracle. We then convert it into an \(O(T^{2/3})\)-cumulative regret algorithm (Theorem 4.3), where \(T\) is the number of rounds, using the explore-then-commit technique. En route to developing these algorithms, we show that the greedy algorithm for \(^{}\)-concave function maximization is _robust to local errors_ (Theorem 3.1), which is one of our main technical contributions and is proved differently from related results in submodular and \(^{}\)-concave function maximization.

In Section 5, we establish the \(\)-hardness of no-regret learning for the adversarial full-information setting. Specifically, Theorem 5.2 shows that unless \(=\), no algorithms that run in polynomial time in each round can achieve \((N) T^{1-c}\) regret for any constant \(c>0\), where \((N)\) stands for any polynomial of \(N\), the per-round problem size. Our proof is based on the fact that maximizing the sum of three \(^{}\)-concave functions is at least as hard as the _matroid intersection_ problem for three matroids, which is known to be \(\)-hard.2 We carefully construct a concrete online \(^{}\)-concave function maximization instance that enables reduction from this \(\)-hard problem. Our high-level idea, namely, connecting sequential decision-making and finding a common base of three matroids, might be useful for proving hardness results in other online combinatorial optimization problems.

### Related work

There is a large stream of research on no-regret submodular function maximization. Our stochastic bandit algorithms are inspired by a line of work on explore-then-commit algorithms for stochastic bandit problems [37; 38; 11] and by a robustness analysis for extending the offline greedy algorithm to the online setting . However, unlike existing results for the submodular case, the guarantees of our algorithms in Section 4 involve no approximation factors. Moreover, while robustness properties similar to Theorem 3.1 are widely recognized in the submodular case, our proof for the \(^{}\)-concave case substantially differs from them. See Appendix A for a detailed discussion.

Combinatorial bandits with linear reward functions have been widely studied [6; 9; 8; 41], and many studies have also considered non-linear functions [7; 21; 15; 29]. However, the case of \(^{}\)-concave functions has not been well studied. Zhang et al.  studied stochastic minimization of \(L^{}\)_-convex_ functions, which form another important class in discrete convex analysis  but fundamentally differ from \(^{}\)-convex functions. Apart from online learning, a body of work has studied maximizing valuation functions approximately from samples to do with imperfect information [2; 3; 4].

Regarding hardness results in online learning, most arguments are typically information-theoretic. For instance, the minimax regret of hopeless games in partial monitoring is \((T)\)[24, Section 37.2]. By contrast, we establish the _NP-hardness_ of the adversarial full-information online \(^{}\)-concave function maximization, even though the offline \(^{}\)-concave function maximization is solvable in polynomial time. Such a situation is rare in online learning. One exception is the case studied by Bampis et al. . They showed that no polynomial-time algorithm can achieve sub-linear approximate regret for some online min-max discrete optimization problems unless \(=\), even though their offline counterparts are solvable in polynomial time. Despite the similarity in the situations, the problem class and proof techniques are completely different. Indeed, while their proof is based on the \(\)-hardness of determining the minimum size of a feasible solution, it can be done in polynomial time for \(^{}\)-concave function maximization [47, Corollary 4.2]. They also proved the \(\)-hardness of the _multi-instance_ setting, which is similar to the maximization of the sum of \(^{}\)-concave functions. However, they did not relate the hardness of multi-instance problems to that of no-regret learning.

## 2 Preliminaries

Let \(V=\{1,,N\}\) be a ground set of size \(N\). Let \(\) be the all-zero vector. For \(i V\), let \(e_{i}^{V}\) denote the \(i\)th standard vector, i.e., the \(i\)th element is \(1\) and the others are \(0\); let \(e_{0}=\) for convenience. For \(x^{V}\) and \(S V\), let \(x(S)=_{i S}x_{i}\). Slightly abusing notation, let \(x(i)=x(\{i\})=x_{i}\). For a function \(f:^{V}\{-\}\) on the integer lattice \(^{V}\), its _effective domain_ is defined as \(f\{\,x^{V}\,:\,f(x)>-\,\}\). A function \(f\) is called _proper_ if \(f\). We say a proper function \(f:^{V}\{-\}\) is _\(^{}\)-concave_ if for every \(x,yf\) and \(i V\) with \(x(i)>y(i)\), there exists \(j V\{0\}\) with \(x(j)<y(j)\) or \(j=0\) such that the following inequality holds:

\[f(x)+f(y) f(x-e_{i}+e_{j})+f(y+e_{i}-e_{j}).\] (1)

Similarly, we say \(f:^{V}\{+\}\) is _\(^{}\)-convex_ if \(-f\) is \(^{}\)-concave. If \(x(V) y(V)\), \(^{}\)-concave functions satisfy more detailed conditions, as follows.

**Proposition 2.1** (Corollary of Murota and Shioura [33, Theorem 4.2]).: _Let \(f:^{V}\{-\}\) be an \(^{}\)-concave function. Then, the following conditions hold for every \(x,yf\): (a) if \(x(V)<y(V)\), \( j V\) with \(x(j)<y(j)\), \(f(x)+f(y) f(x+e_{j})+f(y-e_{j})\) holds. (b) if \(x(V) y(V)\), \( i V\) with \(x(i)>y(i)\), \( j V\) with \(x(j)<y(j)\), (1) holds._

Let \([a,b]=\{\,x^{V}\,:\,a(i) x(i) b(i)\,\}\) be an _integer interval_ of \(a,b(\{\})^{V}\) and \(f\) be \(^{}\)-concave. If \(f[a,b]\), restricting \(f\) to \([a,b]\) preserves the \(^{}\)-concavity [32, Proposition 6.14]. The sum of \(^{}\)-concave functions is _not_ necessarily \(^{}\)-concave [32, Note 6.16]. In this paper, we do _not_ assume monotonicity, i.e., \(x y\) (element-wise) does not imply \(f(x) f(y)\).

### Examples of \(^{}\)-concave functions

Maximum-flow on bipartite graphs.Let \((V,W;E)\) be a bipartite graph, where the set \(V\) of \(N\) left-hand-side vertices is a ground set. Each edge \(ij E\) is associated with a weight \(w_{ij}\). Given sources \(x_{ 0}^{V}\) allocated to the vertices in \(V\), let \(f(x)\) be the maximum-flow value, i.e.,

\[f(x)=_{_{ 0}^{E},\,0\,\,E 0^{W}_{ 0}} _{ij E}w_{ij}_{ij}\,:\, i V,\,_{j:ij E}_ {ij}=x_{i};\, j W,\,_{i:ij E}_{ij}=y_{j}\,}.\]

This function \(f\) is \(^{}\)-concave; indeed, more general functions specified by convex-cost flow problems on networks are \(^{}\)-concave [32, Theorem 9.27]. If we restrict the domain to \(\{0,1\}^{V}\) and regard \(V\) as a set of items, \(W\) as a set of agents, and \(w_{ij} 0\) as the utility of matching an item \(i\) with an agent \(j\), the resulting set function \(f:\{0,1\}^{V}_{ 0}\) coincides with the _OXS_ valuation function known in combinatorial auctions , which is a special case of the following gross substitute valuation.

Gross substitute valuation.In economics, an agent's valuation (a non-negative monotone set function of items) is said to be _gross substitute_ (GS) if, whenever the prices of some items increase while the prices of the other items remain the same, the agent keeps demanding the same-priced items that were demanded before the price change . \(^{}\)-concave functions can be viewed as an extension of GS valuations to the integer lattice [32, Section 6.8]. Indeed, the class of \(^{}\)-concave functions restricted to \(\{0,1\}^{V}\) is equivalent to the class of GS valuations .

Resource allocation.M\({}^{}\)-concave functions also arise in resource allocation problems , which are extensively studied in the operations research community. For example, given \(n\) univariate concave functions \(f_{i}:\{-\}\) and a positive integer \(K\), a function \(f\) defined by \(f(x)=_{i=1}^{n}f_{i}(x(i))\) if \(x\) and \(x(V) K\) and \(f(x)=-\) otherwise is \(^{}\)-concave. More general examples of \(^{}\)-concave functions used in resource allocation are given in, e.g., Moriguchi et al. .

More examples can be found in Murota and Shioura [33, Section 2] and Murota [32, Section 6.3]. As shown above, \(^{}\)-concave functions are ubiquitous in various fields. However, those are often difficult to know perfectly in advance: we may neither know all edge weights in maximum-flow problems, exact valuations of agents, nor \(f_{i}\)s' values at all points in resource allocation. Such situations motivate us to study how to maximize them interactively by selecting solutions and observing some feedback.

```
1:\(x_{0}=\)
2:for\(k=1,,K\) :
3: Select \(i_{k} V\{0\}\)\(\) Standard greedy selects \(i_{k}_{i V\{0\}}f(x_{k-1}+e_{i})\).
4:\(x_{k} x_{k-1}+e_{i_{k}}\) ```

**Algorithm 1** Greedy-style procedure with possibly erroneous local updates

### Basic setup

Similar to bandit convex optimization , we consider a learner who interacts with a sequence of M\({}^{}\)-concave functions, \(f^{1},,f^{T}\), over \(T\) rounds. To avoid incurring \(f^{t}(x)=-\), we assume that \(f^{2},,f^{T}\) are identical to \(f^{1}\). We also assume \(f^{1}\) and \(f^{1}_{ 0}^{V}\), which are reasonable in all the examples in Section 2.1. We consider a constrained setting where the learner's action \(xf^{1}\) must satisfy \(x(V) K\). If \(f^{1}\{0,1\}^{V}\), this is equivalent to the cardinality constraint common in set function maximization. Let \(\,xf^{1}\,:\,x(V) K\, }\) denote the set of feasible actions, which the learner is told in advance. (More precisely, a \((N)\)-time membership oracle of \(\) is given.) Additional problem settings specific to stochastic bandit and adversarial full-information cases are provided in Sections 4 and 5, respectively.

## 3 Robustness of greedy M\({}^{}\)-concave function maximization to local errors

This section studies a greedy-style procedure with possibly erroneous local updates for M\({}^{}\)-concave function maximization, which will be useful for developing stochastic bandit algorithms in Section 4. Let \(f:^{V}\{-\}\) be an M\({}^{}\)-concave function such that \(f_{ 0}^{V}\), which we want to maximize under \(x(V) K\). Let \(x^{*}\{\,f(x)\,:\,xf,\,x(V) K\,\}\) be an optimal solution. We consider the procedure in Algorithm 1. If \(f\) is known a priori and \(i_{1},,i_{K}\) are selected as in the comment in Step 3, it coincides with the standard greedy algorithm for M\({}^{}\)-concave function maximization and returns an optimal solution . However, when \(f\) is unknown, we may select different \(i_{1},,i_{K}\) than those selected by the exact greedy algorithm. Given any \(x^{V}\) and update direction \(i V\{0\}\), we define the _local error_ of \(i\) at \(x\) as

\[(i\,|\,x)_{i^{} V\{0\}}f(x+e_{i^ {}})-f(x+e_{i}) 0,\] (2)

which quantifies how much direction \(i\) deviates from the choice of the exact greedy algorithm when \(x\) is given. The following result states that local errors affect the eventual suboptimality only additively, ensuring that Algorithm 1 applied to M\({}^{}\)-concave function maximization is robust to local errors.

**Theorem 3.1**.: _For any \(i_{1},,i_{K} V\{0\}\), it holds that \(f(x_{K}) f(x^{*})-_{k=1}^{K}(i_{k}\,|\,x_{k-1})\)._

Proof.: The claim is vacuously true if \((i_{k}\,|\,x_{k-1})=+\) occurs for some \(k K\). Below, we focus on the case with finite local errors. For \(k=0,1,,K\), we define

\[_{k}\{\,y\,:\,y x_{k},\;y(V) K-k+x_ {k}(V)\,\},\]

where \(y x_{k}\) is read element-wise. That is, \(_{k}\) consists of feasible points that can be reached from \(x_{k}\) by the remaining \(K-k\) updates (see Figure 1). Note that \(x^{*}_{0}\) and \(_{K}=\{x_{K}\}\) hold.

To prove the theorem, we will show that the following inequality holds for any \(k\{1,,K\}\):

\[_{y_{k}}f(y)_{y_{k-1}}f(y)- (i_{k}\,|\,x_{k-1}).\] (3)

Take \(y_{k-1}_{y_{k-1}}f(y)\) and \(y_{k}_{y_{k}}f(y)\). If \(f(y_{k}) f(y_{k-1})\), we are done since \((i_{k}\,|\,x_{k-1}) 0\). Thus, we assume \(f(y_{k})<f(y_{k-1})\), which implies \(y_{k-1}_{k-1}_{k}\). Then, we can prove the following helper claim by using the M\({}^{}\)-concavity of \(f\).

```
1:\(x_{0}=\)
2:for\(k=1,,K\)do
3: Select \(i_{k} V\{0\}\)\(\) Standard greedy selects \(i_{k}_{i V\{0\}}f(x_{k-1}+e_{i})\).
4:\(x_{k} x_{k-1}+e_{i_{k}}\) ```

**Algorithm 2** Greedy-style procedure with possibly erroneous local updates

Assuming the helper claim, we can easily obtain (3). Specifically, (i) \(f(y_{k-1}+e_{i_{k}}-e_{j}) f(y_{k})\) holds due to \(y_{k-1}+e_{i_{k}}-e_{j}_{k}\) and the choice of \(y_{k}\), and (ii) \((i_{k}\,|\,x_{k-1}) f(x_{k-1}+e_{j})-\)\(f(x_{k-1}+e_{i_{k}})=f(x_{k}-e_{i_{k}}+e_{j})-f(x_{k})\) holds due to the definition of the local error (2). Combining these with (4) and rearranging terms imply (3) as follows:

\[f(y_{k})}{}f(y_{k-1}+e_{i_{k}}-e_{j}) }{}f(y_{k-1})+f(x_{k})-f(x_{k}-e_{i_{k}}+e_{j})}{}f( y_{k-1})-(i_{k}\,|\,x_{k-1}).\]

Given (3), the theorem follows from a simple induction on \(k=1,,K\). For each \(k\), we will prove

\[_{y_{k}}f(y) f(x^{*})-_{k^{}=1}^{k} (i_{k^{}}\,|\,x_{k^{}-1}).\] (5)

The case of \(k=1\) follows from (3) since \(x^{*}_{0}\). If it is true for \(k-1\), (3) and the induction hypothesis imply \(_{y_{k}}f(y)_{y_{k-1}}f(y)- (i_{k}\,|\,x_{k-1}) f(x^{*})-_{k^{}=1}^{k-1} (i_{k^{}}\,|\,x_{k^{}-1})-(i_{ k}\,|\,x_{k-1})\), thus obtaining (5). Since \(_{K}=\{x_{K}\}\) holds, setting \(k=K\) in (5) yields Theorem 3.1.

The rest of the proof is dedicated to proving the helper claim, which we do by examining the following three cases. The middle (right) image in Figure 1 illustrates case 1 (cases 2 and 3).

**Case 1:**\(i_{k}=0\). Due to \(x_{k-1}=x_{k}\), \(y_{k-1}_{k-1}_{k}\) implies \(y_{k-1}(V)=K-(k-1)+x_{k-1}(V)\). Thus, \(x_{k}(V)=x_{k-1}(V)=y_{k-1}(V)-(K-k+1)<y_{k-1}(V)\) holds. From Proposition 2.1 (a), there exists \(j V\) with \(x_{k}(j)<y_{k-1}(j)\) that satisfies (4). Also, \(y_{k-1} x_{k-1}=x_{k}\), \(x_{k}(j)<y_{k-1}(j)\), and \((y_{k-1}-e_{j})(V)=K-k+x_{k-1}(V)=K-k+x_{k}(V)\) imply \(y_{k-1}-e_{j}_{k}\).

**Case 2:**\(i_{k} 0\) and \(x_{k}(V) y_{k-1}(V)\). In this case, \(y_{k-1}_{k-1}_{k}\) implies \(y_{k-1} x_{k-1}\) and \(y_{k-1} x_{k}=x_{k-1}+e_{i_{k}}\), hence \(x_{i}(i_{k})>y_{k-1}(i_{k})\). From Proposition 2.1 (b), there exists \(j V\) with \(x_{k}(j)<y_{k-1}(j)\) that satisfies (4). Since \(y_{k-1} x_{k-1}=x_{k}-e_{i_{k}}\) and \(x_{k}(j)<y_{k-1}(j)\), we have \(y_{k-1}+e_{i_{k}}-e_{j} x_{k}\). Also, we have \((y_{k-1}+e_{i_{k}}-e_{j})(V)=y_{k-1}(V) K-(k-1)+x_{k-1}(V)=K-k+x_{k}(V)\). Therefore, we have \(y_{k-1}+e_{i_{k}}-e_{j}_{k}\).

**Case 3:**\(i_{k} 0\) and \(x_{k}(V)>y_{k-1}(V)\). Since \(y_{k-1}_{k-1}\), we have \(y_{k-1} x_{k-1}\). We also have \(y_{k-1}(V)<x_{k}(V)=x_{k-1}(V)+1\). These imply \(y_{k-1}=x_{k-1}\). Therefore, (4) with \(j=0\) holds by equality, and \(y_{k-1}+e_{i_{k}}=x_{k}_{k}\) also holds. 

The robustness property in Theorem 3.1 plays a crucial role in developing stochastic bandit algorithms in Section 4. Furthermore, the robustness would be beneficial beyond the application to stochastic bandits since \(^{}\)-concave functions often involve uncertainty in practice, as discussed in Section 2.1. Note that Theorem 3.1 has not been known even in the field of discrete convex analysis and that the above proof substantially differs from the original proof for the greedy algorithm _without local errors_ for \(^{}\)-concave function maximization . Indeed, the original proof does not consider a set like \(_{k}\), which is crucial in our proof. It is also worth noting that Theorem 3.1 automatically implies the original result on the errorless case by setting \((i_{k}\,|\,x_{k-1})=0\) for all \(k\). We also emphasize that while Theorem 3.1 resembles robustness properties known in the submodular case , ours is different from them in that it involves no approximation factors and requires careful inspection of the solution space, as in Figure 1. See Appendix A for a detailed discussion.

Figure 1: Images of \(_{k}\) on \(^{2}\). The set of integer points in the trapezoid is the feasible region \(\). Left: the gray area represents \(_{k-1}\) consisting of points reachable from \(x_{k-1}\). Middle: if \(i_{k}=0\) (case 1), \(x_{k-1}=x_{k}\) holds and \(_{k-1}\) shrinks to \(_{k}\), the darker area, since the constraint on \(y(V)\) gets tighter. Right: if \(i_{k}=i_{1}\) (cases 2 and 3), the area, \(_{k}\), reachable from \(x_{k}=x_{k-1}+e_{i_{1}}\) shifts along \(e_{i_{1}}\).

Stochastic bandit algorithms

This section presents no-regret algorithms for the following stochastic bandit setting.

Problem setting.For \(t=1,,T\), the learner selects \(x^{t}\) and observes \(f^{t}(x^{t})=f^{*}(x^{t})+^{t}\), where \(f^{*}:^{V}\{-\}\) is an unknown M2-concave function and \((^{t})_{t=1}^{T}\) is a sequence of i.i.d. \(1\)-sub-Gaussian noises, i.e., \([(^{t})](^{2}/2)\) for \(\).3 Let \(x^{*}_{x}f^{*}(x)\) denote the offline best action. In Theorem 4.2, we will discuss the simple-regret minimization setting, where the learner selects \(x^{T+1}\) after the \(T\)th round to minimize the expected simple regret:

\[_{T} f^{*}(x^{*})-f^{*}(x^{T+1}).\]

Here, the expectation is about the learner's randomness, which may originate from noisy observations and possible randomness in their strategy. This is a stochastic bandit optimization setting, where the learner aims to find the best action without caring about the cumulative regret over the \(T\) rounds. On the other hand, Theorem 4.3 is about the standard regret minimization setting, where the learner aims to minimize the expected cumulative regret (or the pseudo-regret, strictly speaking):

\[_{T} T f^{*}(x^{*})-_{t=1}^{T }f^{*}(x^{t}).\]

In this section, we assume that \(T\) is large enough to satisfy \(T K(N+2)\) to simplify the discussion.

Pure-exploration algorithm.Below, we will use a UCB-type algorithm for pure exploration in the standard stochastic multi-armed bandit problem as a building block. The algorithm is based on _MOSS_ (Minimax Optimal Strategy in the Stochastic case) and is known to achieve an \(O(T^{-1/2})\) simple regret as follows. For completeness, we provide the proof and the pseudo-code in Appendix B.

**Proposition 4.1** (Lattimore and Szepesvari [24, Corollary 33.3]).: _Consider a stochastic multi-armed bandit instance with \(M\) arms and \(T^{}\) rounds, where \(T^{} M\). Assume that the reward of the \(t\)th arm in the \(t\)th round, denoted by \(Y_{i}^{t}\), satisfies the following conditions: \(_{i}[Y_{i}^{t}]\) and \(Y_{i}^{t}-_{i}\) is \(1\)-sub-Gaussian. Then, there is an algorithm that, after pulling arms \(T^{}\) times, randomly returns \(i\{1,,M\}\) with \(^{*}-[_{i}]=O(})\), where \(^{*}\{_{1},,_{M}\}\)._

Given this fact and our robustness result in Theorem 3.1, it is not difficult to develop an \(O(T^{-1/2})\)-simple regret algorithm; we select \(i_{k}\) in Algorithm 1 with the algorithm in Proposition 4.1 consuming \( T/K\) rounds and bound the simple regret by using Theorem 3.1, as detailed below. Also, given the \(O(T^{-1/2})\)-simple regret algorithm, an \(O(T^{2/3})\)-regret algorithm follows from the explore-then-commit technique, as described subsequently. Therefore, we think of these no-regret algorithms as byproducts and the robustness result in Theorem 3.1 as our main technical contribution on the positive side. Nevertheless, we believe those algorithms are beneficial since no-regret maximization of M3-concave functions have not been well studied, despite their ubiquity as discussed in Section 2.1.

The following theorem presents our result regarding an \(O(T^{-1/2})\)-simple regret algorithm.

**Theorem 4.2**.: _There is an algorithm that makes up to \(T\) queries to the noisy value oracle of \(f^{*}\) and outputs \(x^{T+1}\) such that \(_{T}=O(K^{3/2})\)._

Proof.: Based on Algorithm 1, we consider a randomized algorithm consisting of \(K\) phases. Fixing a realization of \(x_{k-1}\), we discuss the \(k\)th phase. We consider the following multi-armed bandit instance with at most \(N+1\) arms and \( T/K\) rounds. The arm set is \(\{\,i V\{0\}\,:\,x_{k-1}+e_{i}\,\}\), i.e., the collection of all feasible update directions; note that the learner can construct this arm set since \(\) is told in advance. In each round \(t\{1,, T/K\}\), the reward of an arm \(i V\{0\}\) is given by \(Y_{i}^{t}=f^{*}(x_{k-1}+e_{i})+^{t}\), where \(^{t}\) is the \(1\)-sub-Gaussian noise. Let \(_{i}=[Y_{i}^{t}]=f^{*}(x_{k-1}+e_{i})\) denote the mean reward of the arm \(i\) and \(^{*}=_{i V\{0\}}_{i}\) the optimal mean reward. If we apply the algorithm in Proposition 4.1 to this bandit instance, it randomly returns \(i_{k} V\{0\}\) such that \([\,(i_{k}\,|\,x_{k-1})\,|\,x_{k-1}\,]=^{*}-[\,_{i_{k}}\,|\,x_{k-1}\,]=O()\), consuming \( T/K\) queries.

Consider sequentially selecting \(i_{k}\) as above and setting \(x_{k}=x_{k-1}+e_{i_{k}}\), thus obtaining \(x_{1},,x_{K}\). For any realization of \(i_{1},,i_{K}\), Theorem 3.1 guarantees \(f^{*}(x_{K}) f^{*}(x^{*})-_{k=1}^{K}(i_{k}\,|\,x_{k- 1})\). By taking the expectations of both sides and using the law of total expectation, we obtain

\[f^{*}(x^{*})-[f^{*}(x_{K})]_{k=1}^{K} (i_{k}\,|\,x_{k-1})=_{k=1}^{ K}(i_{k}\,|\,x_{k-1})\,|\,x_{k-1} =O(K^{}).\]

Thus, \(x^{T+1}=x_{K}\) achieves the desired bound. The number of total queries is \(K T/K T\). 

We then convert the \(O(T^{-1/2})\)-simple regret algorithm into an \(O(T^{2/3})\)-regret algorithm by using the explore-then-commit technique as follows.

**Theorem 4.3**.: _There is an algorithm that achieves \(_{T}=O(KN^{1/3}T^{2/3})\)._

Proof.: Let \( T\) be the number of exploration rounds, which we will tune later. If we use the algorithm of Theorem 4.2 allowing \(\) queries, we can find \(x^{+1}\) with \(_{}=O(K^{3/2}})\). If we commit to \(x^{+1}\) in the remaining \(T-\) rounds, the total expected regret is

\[_{T}=_{t=1}^{}f^{*}(x^{*})- f^{*}(x^{t})+(T-)_{} +T_{}=O(+TK^{} }).\]

By setting \(=(KN^{1/3}T^{2/3})\), we obtain \(_{T}=O(KN^{1/3}T^{2/3})\). 

## 5 NP-hardness of adversarial full-information setting

This section discusses the NP-hardness of the following adversarial full-information setting.

Problem setting.An oblivious adversary chooses an arbitrary sequence of M*-concave functions, \(f^{1},,f^{T}\), where \(f^{t}:^{V}\{-\}\) for \(t=1,,T\), in secret from the learner. Then, for \(t=1,,T\), the learner selects \(x^{t}\) and observes \(f^{t}\), i.e., full-information feedback. More precisely, we suppose that the learner gets free access to a \((N)\)-time value oracle of \(f^{t}\) by observing \(f^{t}\) since M*-concave functions may not have polynomial-size representations in general. The learner aims to minimize the expected cumulative regret:

\[_{x}_{t=1}^{T}f^{t}(x)-_{t=1}^{T} f^{t}(x^{t}),\] (6)

where the expectation is about the learner's randomness. To simplify the subsequent discussion, we focus on the case where the constraint is specified by \(K=N\) and \(f^{1},,f^{T}\) are defined on \(\{0,1\}^{V}\); therefore, the set of feasible actions is \(=\,xf^{1}\,:\,x(V) K\,}=\{0, 1\}^{V}\).

For this setting, there is a simple no-regret algorithm that takes _exponential_ time per round. Specifically, regarding each \(x\) as an expert, we use the standard multiplicative weight update algorithm to select \(x_{1},,x_{T}\). Since the number of experts is \(||=2^{N}\), this attains an expected regret bound of \(O(|})(N)\), while taking prohibitively long \((N)|| 2^{N}\) time per round. An interesting question is whether a similar regret bound is achievable in polynomial time per round. Thus, we focus on the _polynomial-time randomized learner_, as with Bampis et al. .

**Definition 5.1** (Polynomial-time randomized learner).: We say an algorithm for computing \(x_{1},,x_{T}\) is a _polynomial-time randomized learner_ if, given \((N)\)-time value oracles of revealed functions, it runs in \((N,T)\) time in each round, regardless of realizations of the algorithm's randomness.4

Note that the per-round time complexity can depend polynomially on \(T\). Thus, the algorithm can use past actions, \(x_{1},,x_{t-1}\), as inputs for computing \(x_{t}\), as long as the per-round time complexity is polynomial in the input size. The following theorem is our main result on the negative side.

**Theorem 5.2**.: _In the adversarial full-information setting, for any constant \(c>0\), no polynomial-time randomized learner can achieve a regret bound of \((N) T^{1-c}\) in expectation unless \(=\).5_

### Proof of Theorem 5.2

As preparation for proving the theorem, we first show that it suffices to prove the hardness for any polynomial-time _deterministic_ learner and some distribution on input sequences of functions, which follows from celebrated Yao's principle . We include the proof in Appendix C for completeness.

**Proposition 5.3** (Yao ).: _Let \(\) be a finite set of all possible deterministic learning algorithms that run in polynomial time per round and \(^{1:T}\) a finite set of sequences of \(^{}\)-concave functions, \(f^{1},,f^{T}\). Let \(_{T}(a,f^{1:T})\) be the cumulative regret a deterministic learner \(a\) achieves on a sequence \(f^{1:T}=(f^{1},,f^{T})^{1:T}\). Then, for any polynomial-time randomized learner \(A\) and any distribution \(q\) on \(^{1:T}\), it holds that_

\[\,_{T}(A,f^{1:T})\,: \,f^{1:T}^{1:T}\,}\,_{f^{1: T} q}_{T}(a,f^{1:T})\,:\,a \,}.\]

Note that the left-hand side is nothing but the expected cumulative regret (6) of a polynomial-time randomized learner \(A\) on the worst-case input \(f^{1:T}\). Thus, to prove the theorem, it suffices to show that the right-hand side, i.e., the expected regret of the best polynomial-time deterministic learner on some input distribution \(q\), cannot be as small as \((N) T^{1-c}\) unless \(=\). To this end, we will construct a finite set \(^{1:T}\) of sequences of \(^{}\)-concave functions and a distribution on it.

The fundamental idea behind the subsequent construction of \(^{}\)-concave functions is that the matroid intersection problem for three matroids (the 3-matroid intersection problem, for short) is \(\)-hard.

3-matroid intersection problem.A _matroid_\(\) over \(V\) is defined by a non-empty set family \( 2^{V}\) such that for \(B_{1},B_{2}\) and \(i B_{1} B_{2}\), there exists \(j B_{2} B_{1}\) such that \(B_{1}\{i\}\{j\}\). Elements in \(\) are called _bases_. We suppose that, given a matroid, we can test whether a given \(S V\) is a base in \((N)\) time. (This is equivalent to the standard \((N)\)-time independence testing.) The 3-matroid intersection problem asks to determine whether three given matroids \(_{1}\), \(_{2}\), \(_{3}\) over a common ground set \(V\) have a common base \(B_{1}_{2}_{3}\) or not.

**Proposition 5.4** (cf. Schrijver [44, Chapter 41]).: _The 3-matroid intersection problem is \(\)-hard._

We construct \(^{}\)-concave functions that appropriately encode the 3-matroid intersection problem. Below, for any \(B V\), let \(_{B}\{0,1\}^{V}\) denote a vector such that \(_{B}(i)=1\) if and only if \(i B\).

**Lemma 5.5**.: _Let \(\) be a matroid over \(V\) and \( 2^{V}\) its base family. There is a function \(f:\{0,1\}^{V}\) such that (i) \(f(x)=1\) if and only if \(x=_{B}\) for some \(B\) and \(f(x) 1-1/N\) otherwise, (ii) \(f\) is \(^{}\)-concave, and (iii) \(f(x)\) can be computed in \((N)\) time at every \(x\{0,1\}^{V}\)._

Proof.: Let \(_{1}\) denote the \(_{1}\)-norm. We construct the function \(f:\{0,1\}^{V}\) as follows:

\[f(x) 1-_{B} x-_{B} _{1}(x\{0,1\}^{V}).\]

Since \(0 y_{1} N\) for \(y[-1,+1]^{V}\), \(f(x)\) takes values in \(\). Moreover, \(_{B} x-_{B}_{1}\) is zero if \(x=_{B}\) for some \(B\) and at least 1 otherwise, establishing (i). Below, we show that \(f\) is (ii) \(^{}\)-concave and (iii) computable in \((N)\) time.

We prove that \((x)_{B} x-_{B}_{1}\) is \(^{}\)-convex, which implies the \(^{}\)-concavity of \(f\). Let \(_{}:^{V}\{0,+\}\) be the indicator function of \(\), i.e., \(_{}(x)=0\) if \(x=_{B}\) for some \(B\) and \(+\) otherwise. Observe that \(\) is the _integer infimal convolution_ of \(_{1}\) and \(_{}\). Here, the integer infimal convolution of two functions \(f_{1},f_{2}:^{V}\{+\}\) is a function of \(x^{V}\) defined as \((f_{1}_{}f_{2})(x)\{f_{1}(x-y)+f_{2}(y):y ^{V}\}\), and the \(^{}\)-concavity is preserved under this operation [32, Theorem 6.15]. Thus, the \(^{}\)-convexity of \((x)=(_{1}_{}_{})(x)\) follows from the \(^{}\)-convexity of the \(_{1}\)-norm \(_{1}\)[32, Section 6.3] and the indicator function \(_{}\)[32, Section 4.1].

Next, we show that \((x)\) is computable in \((N)\) time for every \(x\{0,1\}^{V}\), which implies the \((N)\)-time computability of \(f(x)\). As described above, \(\) is the integer infimal convolution of\(\|\|_{1}\) and \(_{}\), i.e., \((x)=\{\|x-y\|_{1}+_{}(y):y^{V}\}\). Since the function \(y\|x-y\|_{1}\) is \(^{}\)-convex [32, Theorem 6.15], \((x)\) is the minimum value of the sum of the two \(^{}\)-convex functions. While the sum of two \(^{}\)-convex functions \(f_{1},f_{2}:^{V}_{ 0}\{+\}\) is no longer \(^{}\)-convex in general, we can minimize it via reduction to the _M-convex submodular flow_ problem [32, Note 9.30]. We can solve this by querying \(f_{1}\) and \(f_{2}\) values \((N, L, M)\) times, where \(L\) is the minimum of the \(_{}\)-diameter of \(f_{1}\) and \(f_{2}\) and \(M\) is an upper bound on function values . In our case of \(f_{1}(y)=\|x-y\|_{1}\) and \(f_{2}(y)=_{}(y)\), we have \(L=1\) and \(M N\), and we can compute \(f_{1}(y)\) and \(f_{2}(y)\) values in \((N)\) time (where the latter is due to the \((N)\)-time membership testing for \(\)). Therefore, \((x)\) is computable in \((N)\) time, and so is \(f(x)\). 

Now, we are ready to prove Theorem 5.2.

Proof of Theorem 5.2.: Let \(_{1}\), \(_{2}\), \(_{3}\) be three matroids over \(V\) and \(f_{1}\), \(f_{2}\), \(f_{3}\) functions defined as in Lemma 5.5, respectively. Let \(^{1:T}\) be a finite set such that each \(f^{t}\) (\(t=1,,T\)) is either \(f_{1}\), \(f_{2}\), or \(f_{3}\). Let \(q\) be a distribution on \(^{1:T}\) that sets each \(f^{t}\) to \(f_{1}\), \(f_{2}\), or \(f_{3}\) with equal probability.

Suppose for contradiction that some polynomial-time deterministic learner achieves \((N) T^{1-c}\) regret in expectation for the above distribution \(q\). Let \(T\) be the smallest integer such that the regret bound satisfies \((N) T^{1-c}< T>(3N (N))^{1/c}\). Note that \(T\) is polynomial in \(N\) since \(c>0\) is a constant. We consider the following procedure.

Run the polynomial-time deterministic learner on the distribution \(q\) and obtain \(x_{t}\) for \(t=1,,T\). If some \(x_{t}\) satisfies \(f_{1}(x_{t})=f_{2}(x_{t})=f_{3}(x_{t})=1\), output "Yes" and otherwise "No."

If \(_{1}\), \(_{2}\), \(_{3}\) have a common base \(B_{1}_{2}_{3}\), we have \(f_{1}(_{B})=f_{2}(_{B})=f_{3}(_{B})=1\). On the other hand, if \(x_{t}_{B}\) for every \(B_{1}_{2}_{3}\), \([f^{t}(x^{t})] 1-\) holds from Lemma 5.5 and the fact that \(f^{t}\) is drawn uniformly from \(\{f_{1},f_{2},f_{3}\}\). Thus, to achieve the \((N) T^{1-c}\) regret for \(T>(3N(N))^{1/c}\), the learner must return \(x_{t}\) corresponding to some common base at least once among \(T\) rounds. Consequently, the above procedure outputs "Yes." If \(_{1}\), \(_{2}\), \(_{3}\) have no common base, none of \(x_{1},,x_{T}\) can be a common base, and hence the procedure outputs "No." Therefore, the above procedure returns a correct answer to the 3-matroid intersection problem. Recall that \(T\) is polynomial in \(N\). Since the learner runs in \(T(N,T)\) time and we can check \(f_{1}(x_{t})=f_{2}(x_{t})=f_{3}(x_{t})=1\) for \(t=1,,T\) in \(T(N)\) time, the procedure runs in \((N)\) time. This contradicts the \(\)-hardness of the 3-matroid intersection problem (Proposition 5.4) unless \(=\). Therefore, no polynomial-time deterministic learner can achieve \((N) T^{1-c}\) regret in expectation. Finally, this regret lower bound applies to any polynomial-time randomized learner on the worst-case input due to Yao's principle (Proposition 5.3), completing the proof. 

**Remark 5.6**.: One might think that the hardness simply follows from the fact that no-regret learning in terms of (6) is too demanding. However, similar criteria are naturally met in other problems: there are efficient no-regret algorithms for online convex optimization and no-approximate-regret algorithms for online submodular function maximization. What makes online \(^{}\)-concave function maximization \(\)-hard is its connection to the 3-matroid intersection problem, as detailed in the proof. Consequently, even though offline \(^{}\)-concave function maximization is solvable in polynomial time, no polynomial-time randomized learner can achieve vanishing regret in the adversarial online setting.

## 6 Conclusion and discussion

This paper has studied no-regret \(^{}\)-concave function maximization. For the stochastic bandit setting, we have developed \(O(K^{3/2})\)-simple regret and \(O(KN^{1/3}T^{2/3})\)-regret algorithms. A crucial ingredient is the robustness of the greedy algorithm to local errors, which we have first established for the \(^{}\)-concave case. For the adversarial full-information setting, we have proved the \(\)-hardness of no-regret learning through a reduction from the 3-matroid intersection problem.

Our stochastic bandit algorithms are limited to the sub-Gaussian noise model, while our hardness result for the adversarial setting comes from a somewhat pessimistic analysis. Overcoming these limitations by exploring intermediate regimes between the two settings, such as stochastic bandits with adversarial corruptions , will be an exciting future direction from the perspective of _beyond the worst-case analysis_. We also expect that our stochastic bandit algorithms have room for improvement, considering existing regret lower bounds for stochastic combinatorial (semi-)bandits with linear reward functions. For top-\(K\) combinatorial bandits, there is a sample-complexity lower bound of \((N/^{2})\) for any \((,)\)-PAC algorithm . Since our \(O(K^{3/2})\)-simple regret bound implies that we can achieve an \(\)-error in expectation with \(O(K^{3}N/^{2})\) samples, our bound seems tight when \(K=O(1)\), while the \(K\) factors would be improvable. Regarding the cumulative regret bound, there is an \(()\) lower bound for stochastic combinatorial semi-bandits . Filling the gap between our \(O(KN^{1/3}T^{2/3})\) upper bound and the lower bound is an open problem. (Since we have assumed \(T=(KN)\) in Section 4, our upper bound does not contradict the lower bound.) We believe that our upper bound is essentially tight considering a recent minimax regret bound by Tajdini et al.  for bandit submodular maximization, which we discuss in detail below. Regarding the adversarial setting, it will be interesting to explore no-approximate-regret algorithms. If \(^{}\)-concave functions are restricted to \(\{0,1\}^{V}\), the resulting problem is a special case of online submodular function maximization and hence vanishing \(1/2\)-approximate regret is already possible . We may be able to improve the approximation factor by using the \(^{}\)-concavity.

Discussion on the tightness of the \(O(KN^{1/3}T^{2/3})\) bound.As mentioned above, obtaining a tight regret bound for stochastic bandit \(^{}\)-concave maximization is left open. Nevertheless, we conjecture that our \(O(KN^{1/3}T^{2/3})\) bound in Theorem 4.3 is tight unless we admit exponential factors in \(K\). The rationale behind this conjecture lies in a recent result by Tajdini et al. . They studied stochastic bandit monotone submodular maximization with a ground set of size \(N\) and a cardinality constraint of \(K\), and they showed that there is a lower bound of

\[(K-i)N^{1/3}T^{2/3}+T}\]

on _robust greedy regret_, which compares the learner's actual reward with the output of the greedy algorithm, denoted by \(S_{}\), applied to the underlying true submodular function. Here, \(i K\) is the largest positive integer with \(K^{6}}^{3} T\); see Tajdini et al. [50, Theorem 2.3] for details.6 This lower bound suggests that the \(O(KN^{1/3}T^{2/3})\) regret for stochastic bandit submodular maximization, which can also be achieved by the explore-then-commit strategy, is inevitable in general. We can interpret the \(T}\) term as the regret achieved by regrading all \(\) subsets as arms and using a UCB-type algorithm. Thus, the lower bound consists of the two regret terms achieved by the explore-then-commit and the UCB applied to exponentially many arms.

Currently, we have observed that the proof of the lower bound by Tajdini et al.  does not directly apply to our stochastic bandit \(^{}\)-concave maximization problem. Specifically, the function used in their proof for obtaining the lower bound is submodular but not \(^{}\)-concave. Nevertheless, the problem setting of Tajdini et al.  and our problem in Section 4, with the domain restricted to \(\{0,1\}^{V}\), have notable connections:

1. Since the greedy algorithm applied to the unknown true \(^{}\)-concave function \(f^{*}\) can find an optimal solution \(x^{*}\), we have \(x^{*}=S_{}\). Therefore, the notion of robust greedy regret in Tajdini et al.  essentially coincides with the standard regret in our case.
2. Both the \(O(KN^{1/3}T^{2/3})\) and \(O(T})\) regret bounds discussed above can also be achieved by the explore-then-commit and UCB strategies, respectively, in our \(^{}\)-concave case, where the former is exactly what our Theorem 4.3 states.

Considering these facts, we expect that we can construct a hard instance of stochastic bandit \(^{}\)-concave maximization similar to Tajdini et al.  to establish the same regret lower bound. Therefore, we conjecture that our \(O(KN^{1/3}T^{2/3})\) regret bound in Theorem 4.3 is tight in \(K\), \(N\), and \(T\), if we want to avoid the exponential factor, which generally scales as \(N^{K}\), regardless of the value of \(T\).