# ChatCam: Empowering Camera Control through Conversational AI

Xinhang Liu\({}^{1}\)   Yu-Wing Tai\({}^{2}\)   Chi-Keung Tang\({}^{1}\)

\({}^{1}\)HKUST  \({}^{2}\)Dartmouth College

###### Abstract

Cinematographers adeptly capture the essence of the world, crafting compelling visual narratives through intricate camera movements. Witnessing the strides made by large language models in perceiving and interacting with the 3D world, this study explores their capability to control cameras with human language guidance. We introduce _ChatCam_, a system that navigates camera movements through conversations with users, mimicking a professional cinematographer's workflow. To achieve this, we propose _CineGPT_, a GPT-based autoregressive model for text-conditioned camera trajectory generation. We also develop an _Anchor Determinator_ to ensure precise camera trajectory placement. _ChatCam_ understands user requests and employs our proposed tools to generate trajectories, which can be used to render high-quality video footage on radiance field representations. Our experiments, including comparisons to state-of-the-art approaches and user studies, demonstrate our approach's ability to interpret and execute complex instructions for camera operation, showing promising applications in real-world production settings. We will release the codebase upon paper acceptance.

## 1 Introduction

Cinematographers skillfully capture the essence of the 3D world by maneuvering their cameras, creating an array of compelling visual narratives . Achieving aesthetically pleasing results requires not only a deep understanding of scene elements and their interplay but also meticulous execution of techniques.

Recent progress of large language models (LLMs)  has marked a significant milestone in AI development, demonstrating their capability to understand and act within the 3D world . Witnessing this evolution, our work explores the feasibility of empowering camera control through conversational AI, thus enhancing the video production process across diverse domains such as documentary filmmaking, live event broadcasting, and virtual reality experiences.

Although the community has devoted considerable effort to controlling the trajectories of objects and cameras in video generation approaches for practical usage , or predicting similar sequences through autoregressive decoding processes , generating camera trajectories has yet to be explored. This task involves multiple elements such as language, images, 3D assets, and, beyond mere accuracy, necessitates visually pleasing rendered videos as the ultimate goal.

We propose _ChatCam_, a system that allows users to control camera operations through natural language interaction. As illustrated in Figure 1, leveraging an LLM agent to orchestrate camera operations, our method assists users in generating desired camera trajectories, which can be used to render videos on radiance field representations such as NeRF  or 3DGS .

At the core of our approach, we introduce _CineGPT_, a GPT-based autoregressive model that integrates language understanding with camera trajectory generation. We train this model using a paired text-trajectory dataset to equip it with the ability for text-conditioned trajectory generation. We alsopropose an _Anchor Determinator_, a module that identifies relevant objects within the 3D scene to serve as anchors, ensuring correct trajectory placement based on user specifications. Our LLM agent parses compositional natural language queries into semantic concepts. With these parsed sub-queries as inputs, the agent then calls our proposed _CineGPT_ and _Anchor Determinator_. It composes the final trajectory with the outputs from these tools, which can ultimately be used to render a video that fulfills the user's request.

With comprehensive evaluations and comparisons to other state-of-the-art methods, our method exhibits a pronounced ability to interpret and execute complex instructions for camera operation. Our user studies further demonstrate its promising application prospects in actual production settings. In summary, this paper's contributions are as follows:

* We introduce _ChatCam_, a system that, for the first time, enables users to operate cameras through natural language interactions. It simplifies sophisticated camera movements and reduces technical hurdles for creators.
* We develop _CineGPT_ for text-conditioned camera trajectory generation and an _Anchor Determinator_ for precise camera trajectory placement. Our LLM agent understands users' requests and leverages our proposed tools to complete the task.
* Extensive experiments demonstrate the effectiveness of our method, showing how AI can effectively collaborate with humans on complex tasks involving multiple elements such as language, images, 3D assets, and camera trajectories.

## 2 Related Work

**Multimodal Language Models.** Large-scale language models (LLMs) [9; 19; 17; 1; 70] enabled by extensive datasets and model size, have demonstrated surprising emerging abilities. The emergence of multimodal models [43; 42; 33] is captivating as they can process text alongside other modalities such as images , audio , and videos . Some unified models can perceive inputs and generate outputs in various combinations of text, images, videos, and audio [51; 68; 77; 86]. LLMs hold the potential to act as agents [76; 80; 62], allowing them to be driven by goals, reason about their objectives, devise plans, utilize tools, and interact with and gather feedback from the environment. Our proposed method involves multiple modalities including language, images, 3D fields, and camera trajectories, and utilizes LLMs as agents to assist users in operating cameras.

**Radiance Field Representations.** Utilizing continuous 3D fields modeled by MLPs and volumetric rendering, Neural Radiance Fields (NeRFs)  achieved breakthrough for novel view synthesis.

Figure 1: **Empowering camera control through conversational AI. Our proposed _ChatCam_ assists users in generating desired camera trajectories through natural language interactions. The generated trajectories can be used to render videos on radiance field representations such as NeRF  or 3DGS .**

Subsequent research has emerged to improve NeRFs and broaden their applications , including enhancing rendering quality , modeling dynamic scenes , improving computational efficiency , and facilitating 3D scene editing . Replacing the deep MLPs with a feature voxel grid network has demonstrated enhancement in both training and inference speed . More recently, 3D Gaussian Splatting  has further advanced visual quality and rendering efficiency. Compared to traditional 3D representations, radiance field representations offer superior photorealistic rendering quality, therefore, this study focuses on camera manipulation upon mainstream radiance field representations such as NeRF or 3DGS.

**3D Scene Understanding.** Early methods for 3D semantic understanding  primarily focused on the closed-set segmentation of point clouds or voxels. NeRF's capability to integrate information from multiple viewpoints has spurted its application in 3D semantic segmentation . Among these,  combine image embeddings from effective 2D image feature extractors  to achieve language-guided object localization, segmentation, and editing.  proposes semantic anisotropic Gaussians to simultaneously estimate geometry, appearance, and semantics in a single feed-forward pass. Another line of research integrates 3D with language models for tasks such as 3D question answering , localization , and captioning . Additionally,  propose 3D foundation models to handle various perception, reasoning, and action tasks in 3D environments. However, the AI-assisted operation of cameras within 3D scenes remains an unexplored area.

**Trajectory Control and Prediction.** Controlling the trajectories of objects and cameras is crucial to advance current video generation approaches for practical usage. TC4D  incorporates trajectory control for 4D scene generation with multiple dynamic objects. Direct-a-Video , MotionCtrl ,

Figure 2: **Overview of the ChatCam pipeline. Given a camera operation instruction, ChatCam reasons the user’s request and devises a plan to generate a trajectory using our proposed CineGPT and Anchor Determinator. The agent then utilizes the outputs from these tools to compose the complete trajectory and render a video.**

and CameraCtrl  manage camera pose during video generation; however, they are either limited to basic types or necessitate fine-tuning of the video diffusion model. Moreover, these approaches require user-provided trajectories, whereas we, for the first time, generate camera trajectories conditioned on text.

## 3 Method

Figure 2 provides an overview of our method's pipeline. ChatCam analyzes the user's camera operation instruction and devises a plan to generate a trajectory using our proposed CineGPT and Anchor Determinator. Finally, an AI agent utilizes the outputs from these tools to compose the complete trajectory.

### Text-Conditioned Trajectory Generation

To enable text-conditioned trajectory generation, we collect a text-trajectory dataset and introduce CineGPT, a GPT-based autoregressive model integrating language and camera trajectories. Illustrated in Figure 3 (a), our method quantizes camera trajectories into a sequence of trajectory tokens using a trajectory tokenizer. Subsequently, a multi-modal transformer decoder is employed to convert input tokens into output tokens. Upon training, our model adeptly generates token sequences based on user-provided text prompts. These sequences are then de-quantized to reconstruct the camera trajectory.

**Camera Trajectory Parameterization.** For each single frame, our camera parameters include rotation \(^{3 3}\), translation \(^{3}\), and intrinsic parameters \(^{3 3}\). We further convert the rotation matrix \(\) into the \(^{2}^{2}\) space  to facilitate computational efficiency and simplify the optimization process. The total \(M\)-frame camera trajectory is formulated as:

\[c_{1:M}=\{c_{i}\}_{i=1}^{M}=\{(_{i},_{i},_{i})\}_ {i=1}^{M}.\] (1)

To additionally model the velocity of camera movement, we introduce a global parameter \(t\) representing the total duration. Consequently, the instantaneous velocity of each frame can be approximated by the relative translation and rotation to the previous frame over unit time.

**Text-Trajectory Dataset.** Given the scarcity of readily available data on camera operations, we manually constructed approximately \(1000\) camera trajectories using Blender . These trajectories encompass a diverse range of movements, including various combinations of translations, rotations,

Figure 3: **(a) CineGPT. We quantize camera trajectories to sequences of tokens and adopt a GPT-based architecture to generate the tokens autoregressively. Learning trajectory and language jointly, CineGPT is capable of text-conditioned trajectory generation. (b) Anchor Determination. Given a prompt describing the image rendered from an anchor point, the anchor selector chooses the best matching input image. An anchor refinement procedure further fine-tunes the anchor position.**

focal lengths, and velocities. Each trajectory is accompanied by a human language description detailing the corresponding movements. This dataset spans various scenarios, capturing both simple pan-tilt-zoom motions and more complex trajectories mimicking real-world scenarios.

**Trajectory Tokenizer.** We leverage a trajectory tokenizer based on the Vector Quantized Variational Autoencoders (VQ-VAE) architecture  to represent camera trajectories as discrete tokens. Our trajectory tokenizer consists of an encoder \(\) and a decoder \(\). Given an \(M\)-frame camera trajectory \(c_{1:M}=\{c_{i}\}_{i=1}^{M}\), the encoder \(\) encodes it into \(L\) trajectory tokens \(z_{1:L}=\{z_{i}\}_{i=1}^{L}\), where \(L=M/l\) and \(l\) is the temporal downsampling rate. The decoder \(\) then decodes \(z_{1:L}\) back into the trajectory \(_{1:M}=\{_{i}\}_{i=1}^{M}\). Specifically, the encoder \(\) first encodes frame-wise camera parameters \(c_{1:M}\) into a latent vector \(_{1:L}=(c_{1:M})\), by performing 1D convolutions along the time dimension. We then transform \(_{1:L}\) into a collection of codebook entries \(z\) through discrete quantization. The learnable codebook \(Z=\{z_{i}\}_{i=1}^{K}\) consists of \(K\) latent embedding vectors, each with dimension \(d\). The quantization process \(Q()\) replaces each row vector with its nearest codebook entry, as follows:

\[z_{i}=Q(_{i})=_{z_{k}}||_{i}-z_{k}||_{2}^{2},\] (2)

where \(||||_{2}\) denotes the Euclidean distance. After quantization, the decoder projects \(z_{1:L}\) back to the trajectory space as the reconstructed trajectory \(_{1:M}=(z_{1:L})\). In addition to the reconstruction loss, we adopt embedding loss and commitment loss similar to those proposed in  to train our trajectory tokenizer. With a trained trajectory tokenizer, a camera trajectory \(c_{1:M}\) can be mapped to a sequence of trajectory tokens \(z_{1:L}\), facilitating the joint representation of camera trajectory and natural language for text-conditioned trajectory generation.

**Cross-Modal Transformer.** We utilize a cross-modal transformer decoder to generate output tokens from input tokens, which may consist of text tokens, trajectory tokens, or a combination of both. These output tokens are subsequently converted into the target space. To train our decoder-only transformer, we denote our source tokens as \(X_{s}=\{x_{s}^{i}\}_{i=1}^{N_{s}}\) and target tokens as \(X_{t}=\{x_{t}^{i}\}_{i=1}^{N_{t}}\). We feed source tokens into it to predict the probability distribution of the next potential token at each step \(p_{}(x_{t}|x_{s})=_{i}p_{}(x_{t}^{i}|x_{t}^{<i},x_{s})\). The objective function is formulated as:

\[_{}=-_{i=1}^{N_{t}} p_{}(x_{t}^{i}|x_{t}^{< i},x_{s}).\] (3)

By optimizing this objective, we aim to equip CineGPT with the ability to capture intricate patterns and relationships within the data distribution. We then fine-tune CineGPT on supervised trajectory-language translation leveraging our paired text-trajectory dataset, where the input for this stage can either be a camera trajectory or a text description, while the target is the opposite modality. During inference, CineGPT can generate camera trajectories solely from textual descriptions as inputs.

### Object-Centric Trajectory Placement with Anchors

While CineGPT enables text-conditioned trajectory generation, its generation process solely focuses on determining the camera's movements, without contextual connection to specific scenes. Consequently, CineGPT alone cannot effectively handle user prompts that involve object-centric descriptions, such as directives like "directly above the Sydney Opera House". In this light, we bridge trajectory generation with each underlying scene with "anchors" serving as reference points within the scene to achieve more accurate placement of trajectories, as illustrated in Figure 3 (b).

Our anchor determination procedure takes natural language descriptions of an image as input. This procedure identifies a set of camera parameters that can render an image that best matches the given description. Current 3D visual grounding approaches [57; 81] typically entail learning a 3D feature field [40; 37] and localizing objects within the scene, which often results in high computational costs. In contrast, our anchor determinator adopts a different strategy. Initially, it selects the input image that best matches the given text description as an initial anchor. Subsequently, an anchor refinement process is employed to iteratively improve upon this initial anchor, ultimately yielding the final anchor. This approach offers a more efficient alternative to traditional methods, reducing computational overhead while still achieving accurate scene anchoring.

**Initial Anchor Selector.** Since our method leverages radiance field representations to render videos, we naturally have access to the input images for training the 3D scene representations. We utilize an initial anchor selector based on CLIP  to choose the image from these input images that best matches the text prompt. To be specific, for \(i\)-th input image \(I_{i}\), we extract their CLIP image features and convert the text prompt \(T\) into a CLIP text feature. Next, we compute the cosine similarity between the CLIP text feature vector and each of the CLIP image feature vectors. We select the best matching image with the highest cosine similarity score as the initial anchor. This can be formulated as:

\[i_{}=_{i}}(I_{i}) f_{}(T)}{\|f_{}(I_{i})\|\|f_{}(T)\|},\] (4)

where \(f_{}()\) and \(f_{}()\) represent the image and text feature extractor, respectively.

**Anchor Refinement.** Using the camera parameters \(c_{}\) associated with the selected image as initialization, we further minimize the following objective to obtain the final anchor camera parameters:

\[_{c}_{}(c)=-}(R(c)) f_{ }(T)}{\|f_{}(R(c))\|\|f_{}(T)\|},\] (5)

where \(R()\) is the rendering function and \(c\) is initialized with \(c_{}\). The optimization of \(c\) is performed using gradient descent, with the update rule given by:

\[c_{t+1}=c_{t}-_{c}_{}(c_{t}),\] (6)

where \(\) is the learning rate. The optimization typically achieves convergence within 100 to 1000 steps. This refinement process ensures that the camera parameters are adjusted to better match the text prompts, handling cases where the initial input images do not align well with the prompts.

### Trajectory Generation through User-Friendly Interaction

With our proposed CineGPT and anchor determination, a large language model acts as an agent to interpret the user's requests, generates a plan to use various tools, and composes a final camera trajectory. We adopt GPT-4  to interpret users' natural language inputs and subsequently produce trajectory prompts. Specifically, we use a carefully designed prompt to instruct the LLM agent to reason about the user's requirements and devise a plan consisting of the following steps: 1) Break down the complex text query into sub-tasks that CineGPT and the Anchor Determinator can effectively handle. 2) Use these tools to generate atomic trajectories and determine anchor points. 3) Compose the final trajectory by concatenating atomic trajectories and ensuring they pass through the anchors.

**Observing, Reasoning, and Planning.** Research indicates that LLMs can be prompted to decompose complex goals into sub-tasks, essentially thinking step-by-step . As illustrated in Figure 2, we begin by instructing the agent to describe its observations, providing a summary of the current situation. The agent then uses this summary to reason and develop a mental scratchpad for high-level planning. Finally, it outlines specific steps to achieve the overarching goal of generating the user-required camera trajectory.

**Utilization of Proposed Tools.** We inform our agent of the expected input and output format, i.e., the APIs, of our proposed CineGPT and Anchor Determinator, and instruct the agent to interact with them following the given format. In its outlined specific steps to generate the user-required camera trajectory, it first calls CineGPT and Anchor Determinator to obtain atomic trajectories and anchor points, respectively. Note that both tools can be called multiple times, and multiple atomic trajectories can later be concatenated into final trajectories that pass through all anchor points correctly.

**Final Trajectory Composition.** Here we explain how to combine atomic trajectories from CineGPT with anchor points to form the final trajectory. The agent first decides the role of the anchors in the ultimate trajectory, either as a starting point or an ending point of some atomic trajectory. Then affine transformations are applied to the respective atomic trajectories to ensure that their starting or ending points align with the anchor points. For the remaining atomic trajectories not controlled by anchor points, affine transformations are applied to make the endpoint of the previous trajectory align with the starting point of the subsequent trajectory.

## 4 Experiments

We assess the performance of our proposed ChatCam for human language-guided camera operation across a series of challenging scenarios. Through ablation studies, we provide empirical evidence of the effectiveness of its fundamental components. We kindly refer the reader to our supplementary material for additional experimental results, including rendered **videos**.

### Experimental Setup

**Implementation Details.** We implement our approach using PyTorch  and conduct all the training and inference on a single NVIDIA RTX 4090 GPU with 24 GB RAM. The trajectory tokenizer has a codebook with \(K=256\) latent embedding vectors, each with dimension \(d=256\). The temporal downsampling rate of the trajectory encoder is \(l=4\). Our cross-modal transformer decoder consists of \(24\) layers, with attention mechanisms employing an inner dimensionality of \(64\).

Figure 4: **Qualitative results on indoor and outdoor scenes. Visualizations of our generated trajectories from input text descriptions and the frames in the final rendered video. Our method is capable of understanding and executing instructions and providing correct translations, rotations, and camera focal lengths. Additionally, our method can comprehend more specialized terms such as “dolly zoom”.**

_Swep across the boy in black, the keyboard, and the boy in white, then zoom out to frame the boys and the white guitar together._

The remaining sub-layers and embeddings have a dimensionality of \(256\). We train CineGPT using the Adam optimizer  with an initial learning rate of \(0.0001\). It takes approximately 30 hours to converge. Our anchor determination utilizes CLIP  with a ViT-B/32 Transformer architecture. The learning rate of anchor refinement is \(0.002\). By default, we use GPT-4  as our LLM agent, and its prompt will be released with our codebase. We render final videos using 3DGS  as the 3D representation.

**Tested Scenes.** We tested our method on scenes from a series of datasets suitable for 3D reconstruction with radiance field representations, including: (i) _mip-NeRF 360_, a real dataset with indoor and outdoor scenes. (ii) _OMMO_, a real dataset with large-scale outdoor scenes. (iii) _Hypersim_, a synthetic dataset for indoor scenes. (iv) _MannequinChallenge_, a real dataset for human-centric scenes. If camera poses associated with images were not provided, we used COLMAP  for camera pose estimation. For each scene, we reconstructed using all available images without train-test splitting.

**Baselines.** As the first method to enable human language-guided camera operation, there is no established direct baseline for comparison. Therefore, we adopt 3D understanding approaches based on radiance field representations to let the LLM agent attempt to select a series of images corresponding to the input text from input images and interpolate their camera poses to construct camera trajectories. These methods include LERF , utilizing CLIP embeddings, and SA3D , utilizing SAM embeddings.

**Evaluation Metrics.** To evaluate the accuracy of the generated trajectories, we manually construct ground truth trajectories and compute the mean squared errors (MSEs) of translations and rotations relative to them. Additionally, we conduct a user study to evaluate the rendered videos using generated camera trajectories, where users are asked to select the video with the best **visual quality** and best **alignment** with the input text.

Figure 5: **Qualitative results on human-centric scenes.** Visualizations of our generated trajectories from input text descriptions and the frames in the final rendered video. Our method performs effectively in scenes with multiple humans.

### Results

As shown in Figure 4, our method demonstrates the ability to understand and execute camera operation instructions on a range of complex indoor and outdoor scenes, giving appropriate translation, rotation, and focal length. Our method also understands more technical terms such as dolly zoom, which creates a special visual effect by zooming the camera out while adjusting the focus. In Figure 5 we further showcase the qualitative results of our method in human-centric scenes. Our method can correctly handle user instructions about specific people and create correct and vivid visual effects.

**Comparisons.** In Figure 6 we qualitatively compare our method with LLM agents utilizing SA3D or LERF to locate target objects. The baselines do simple interpolation of keyframes because they have no knowledge about camera trajectories and tend to move the camera to unreasonable spots (such as entering an object). Therefore, the video rendered by baselines contains artifacts and is not correctly consistent with the input text. However, our method achieves better visual quality and alignment with input texts. Quantitative comparisons in Table 1 further prove that our method has better performance and is preferred by users.

**Ablation Study.** We present our ablation study in Table 1. We evaluate the performance of our method using different LLMs as agents. Our approach achieved the best accuracy using GPT-4  as the agent, better than GPT-3  and LLaMA-2 .Without our proposed anchor determination, our method cannot correctly place trajectories within 3D scenes, thereby being less accurate than our full model.

## 5 Conclusion

This paper presents ChatCam, a system designed for camera operation through natural language interactions. By introducing CineGPT, we bridge the gap between human language guidance and camera control, achieving text-conditioned trajectory generation. Our proposed anchor determination procedure further ensures precise camera trajectory placement. Our LLM agent comprehends users' requests and effectively utilizes our proposed tools to compose the final trajectory. Through extensive experiments, we demonstrate the effectiveness of ChatCam, showcasing its ability to collaborate with humans on complex tasks involving language, images, 3D assets, and camera trajectories. ChatCam has the potential to simplify camera movements and reduce technical barriers for creators.

   Method & LLM Agent & Anchor Determination & Translation MSE (\(\)) & Rotation MSE (\(\)) & Visual Quality (\(\)) & Alignment (\(\)) \\  SA3D  & _GPT-4_ & - & 19.5 & 6.3 & 5.7 & 3.8 \\ LERF  & _GPT-4_ & - & 17.7 & 4.9 & 9.4 & 28.3 \\  ChatCam (Ours) & _LLaMA-2_ & ✓ & 6.4 & 3.6 & - & - \\ ChatCam (Ours) & _GPT-3-5_ & ✓ & 7.3 & 3.5 & - & - \\ ChatCam (Ours) & _GPT-4_ & ✗ & 16.2 & 8.5 & - & - \\
**ChatCam (Ours)** & _GPT-4_ & ✓ & **5.3** & **2.9** & **84.9** & **67.9** \\   

Table 1: **Quantitative comparisons and evaluations. Our full model performs better than baselines and variants in terms of trajectory accuracy, visual quality, and alignment with input text.**

Figure 6: **Qualitative comparisons. Our approach avoids moving the camera to unreasonable positions such as inside objects, obtaining videos with better visual effects, and aligning best with input texts.**