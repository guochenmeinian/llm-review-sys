# Soft-Unification in Deep Probabilistic Logic

Jaron Maene

KU Leuven

jaron.maene@kuleuven.be

&Luc De Raedt

KU Leuven & Orebro University

luc.deraedt@kuleuven.be

###### Abstract

A fundamental challenge in neuro-symbolic AI is to devise primitives that fuse the logical and neural concepts. The Neural Theorem Prover has proposed the notion of soft-unification to turn the symbolic comparison between terms (i.e. unification) into a comparison in embedding space. It has been shown that soft-unification is a powerful mechanism that can be used to learn logic rules in an end-to-end differentiable manner. We study soft-unification from a conceptual point and outline several desirable properties of this operation. These include non-redundancy in the proof, well-defined proof scores, and non-sparse gradients. Unfortunately, these properties are not satisfied by previous systems such as the Neural Theorem Prover. Therefore, we introduce a more principled framework called DeepSoftLog based on probabilistic rather than fuzzy semantics. Our experiments demonstrate that DeepSoftLog can outperform the state-of-the-art on neuro-symbolic benchmarks, highlighting the benefits of these properties.

## 1 Introduction

Deep learning has become the most prominent paradigm in machine learning but still suffers from limitations such as poor interpretability, reasoning skills, and difficulties in incorporating knowledge. Neuro-symbolic AI (NeSy) is an emerging field that seeks to address these limitations by combining the strengths of neural networks and symbolic systems .

A key design question in NeSy is the used _representation_. Neural networks operate on tensors while symbolic reasoning operates on symbolic structures. Previous systems have broadly used two ways to deal with this dilemma. The first is keeping the neural and symbolic representations completely separate (e.g. DeepProbLog ). The second is relaxing the symbolic part, making it continuous by mapping the symbols onto vector spaces (e.g. Logic Tensor Networks ). This allows - in principle - for a tighter integration, where both representations are retained and fused.

The Neural Theorem Prover (NTP)  is an early system that pioneered such an integration, by linking the logical and neural representations with a mechanism called soft-unification. For instance, in regular logic, matching \((,)\) with \((,)\) or \(()\) and \(()\) fails. However, by mapping \(\) and \(\), resp. \(\) and \(\) to vectors, they can be compared in an embedding space. The NTP is an end-to-end differentiable prover that learns these embeddings from data.

Inspired by the success of the NTP, we analyze its key concept - _soft-unification_ - from a theoretical perspective. We identify several natural and desirable properties of soft-unification, which are not always satisfied by previous works. This motivates us to propose an alternative more principled definition of soft-unification based on distances. Next, we introduce DeepSoftLog, which makes a principled integration of embeddings in probabilistic logic programming .

As an example, consider the DeepSoftLog program in listing 1. By associating the constants with learnable embeddings (denoted with the \(\) prefix), we turn a regular finite state machineimplementation into a differentiable model. So if the program would be trained on e.g. the \((01)^{*}\) language, it could learn to set \(\)prev_state\(1=\)state\(2\) and \(\)prev_state\(2=\)state\(1\).

In summary, this paper makes three contributions: (1) a theory for using learnable embeddings in logic through soft-unification, (2) DeepSoftLog, a neuro-symbolic system that includes embeddings inside an end-to-end probabilistic logic prover, and (3) a demonstration of the practical advantages of DeepSoftLog over state-of-the-art NeSy systems on several benchmarks.

``` accepts(X):-run(\(\)start_state,X). run(\(\)end_state,[]).%basecase+2statetransitions run(\(\)state1,[\(\)symbol1|T]):-run(\(\)prev_state1,T). run(\(\)state2,[\(\)symbol2|T]):-run(\(\)prev_state2,T). query(accepts([\(\)0,\(\)1])). ```

Listing 1: Example of a differentiable finite state machine implementation in DeepSoftLog.

## 2 Background

### Logic programming

We summarise the basic logic programming concepts. An in-depth discussion can be found in .

**Syntax** A term \( t\) is either a constant \( c\), a variable \( X\), or a structured term \( f(t_{1},...,t_{k})\), where \( f\) is a functor and \( t_{i}\) are terms. Atoms are expressions of the form \( q(t_{1},...,t_{k})\), where \( q\) is a predicate of arity \(k\) and \( t_{i}\) are terms. A rule is an expression \( h:-b_{1},...,b_{k}\) where \( h\) is an atom and \( b_{i}\) are atoms or negated atoms. The meaning of such a rule is that \( h\) holds whenever all the \( b_{i}\)'s hold. The atom \( h\) is the head of the rule, while the conjunction of \( b_{i}\)'s is the body. Facts are rules with an empty body.

A logic program is a set of rules. An expression is _ground_ if it does not contain variables. To ground an expression \( e\) we can apply a substitution \(=\{ V_{1} t_{1},..., V_{k} t_{k}\}\), creating an instantiated expression \( e\) where every occurrence of the variable \( V_{i}\) is replaced with \( t_{i}\). An important operation is _unification_. Two expressions \( t_{1}\) and \( t_{2}\) unify when there exists a substitution \(\) such that \( t_{1}= t_{2}\). For example, \( f(g(a), X)\) and \( f(g( Y), Z)\) have a unifier \(\{ X Z, Y a\}\).

**Semantics** The set of all possible ground terms in a program is called the _Herbrand universe_\(\). A _possible world_ (or Herbrand interpretation) is a subset of the possible ground atoms. A ground atom \(a\) is true in a possible world \(w\) when \(a w\). A possible world \(w\) is a model for a program \(T\), written \(w T\), when every grounding of every rule in the program is true in the possible world.

**Inference** Proving relies on the repeated application of rules to a goal, using _SLD resolution_. Given a rule \( h:-b_{1}, b_{n}\) and a goal \( g_{1}, g_{m}\), such that \( h\) and \( g_{1}\) unify with substitution \(\), SLD resolution derives the new goal \(( b_{1}, b_{n}, g_{2}, g_{m})\). A proof is a sequence of resolution steps that results in an empty goal.

### Probabilistic and Fuzzy Logic

ProbLog  extends logic programming with probabilistic facts \( p::f\), where a ground fact \( f\) is annotated with a probability \( p\). As an example, consider the well-known alarm Bayesian network:

``` 0.1::event(landslide).0.2::event(earthquake).0.5::hears_alarm(mary).0.4::hears_alarm(john).alarm:-event(landslide).alarm:-event(earthquake).calls(X):-alarm,hears_alarm(X). ```

**Semantics** Each ground probabilistic fact represents an independent Boolean random variable. Hence, the probability of a possible world \(w\) is a product over the choice of probabilistic facts in \(w\). The probability (or _success score_) of an atom \( a\) in a program \(T\) is the sum over all the models of \(T\) where \( a\) is true.

\[P(w)=_{f w}p_{f}_{f w}(1-p_{f}) P( a)=_{w: a w w|=T}P(w)\]In the example, a choice of probabilistic facts \(F=\{(),()\}\), has the corresponding model \(w=F\{,()\}\) with probability \(P(w)=0.1 0.5(1-0.2)(1-0.4)=0.024\).

**Inference** There are multiple ways to perform inference in ProbLog . We rely on a proving approach . Computing the success score \(P(q)\) of a query \(q\) consists of the following steps. (1) Generate the set of all proofs \(B(q)\) using SLD resolution. (2) Disjoin the proofs into a formula \(_{ B(q)}\). (3) Transform this formula using knowledge compilation , to ensure it can be evaluated correctly and efficiently. (4) Convert the compiled formula into an arithmetic circuit by replacing the OR and AND operations with SUM and PRODUCT operators from a semiring .

We demonstrate our inference approach on the query \(()\). The proofs: \(()()\) and \(()()\) result in the formula on the left of figure 1 when disjoined. When working with probabilities, we cannot simply evaluate this formula. Indeed, the proofs are not independent as they both contain \(()\), so \(P(A B) P(A)+P(B)\). Hence, we transform the formula into a circuit using knowledge compilation  (middle of figure 1). The resulting circuit supports many types of inference, by evaluating with the appropriate semiring . For probabilistic inference, we can simply use the standard \(+\) and \(\) operators, as shown on the right of figure 1.

**Neural Theorem Prover** We can also sketch the Neural Theorem Prover (NTP)  in this framework. The NTP is based on the Datalog subset of Prolog. So it does not include structured terms (e.g. lists) or negation. At the same time, rather than using the probabilistic semiring, the NTP uses the fuzzy semiring. Essentially, this means the fuzzy \(\) and \(\) operations are used instead of the probabilistic \(+\) and \(\). This is also known as the Godel t-norm or minimum t-norm. In contrast with probabilistic inference, disjunction in the fuzzy semiring is idempotent. This implies we can skip the expensive knowledge compilation step and evaluate the formula directly during proving .

**Approximation** As probabilistic inference is #P-hard, it is possible to approximate inference by only using the \(k\)-best proofs. The best proofs are those with the highest score. During proving, these can be found with \(A^{*}\) search on the proof tree . For instance, the \(1\)-best proof for the example is \(()()\), with a score of \(0.08\). Note that for the NTP, the \(1\)-best proof captures the full success score due to the \(\) aggregation of proofs. The NTP and Greedy NTP  introduced additional approximation methods to prune proofs during proving, such as imposing a maximum depth and maximum branching factor.

### Soft Unification

While in standard logic programming two constants landslide and earthquake do not unify (i.e. do not match), soft-unification returns a score based on how similar the symbols are in embedding space. We use the \(\) predicate to denote that two symbols are soft-unified (e.g. \(\)). The score of a soft-unification is determined by a soft-unification function \(s:\), so \(P(t_{1} t_{2})=s(t_{1},t_{2})\).

Algorithm 1 states the simplified soft-unification algorithm for atoms and terms without functors. The algorithm returns the set of variable substitutions, that make two atoms equal while disregarding the

Figure 1: The circuits used during inference of \(()\) in the alarm example. (left) AND/OR formula represented as a tree. (middle) Compiled formula. (right) Arithmetic circuit.

equality of the symbols. For example, the two atoms \((,,,)\) and \((,,,)\) do not unify, but soft-unify with the substitution \(\{X Z,Y d\}\). In practice, the algorithm usually also extracts the conjunction of the used soft-unifications (in this example \(()()\)). During inference, these can be conjoined with the proof just like probabilistic facts.

``` functionSoftUnify\((t_{1},t_{2})\) if\(t_{1}\) is a variable thenreturn\(\{t_{1} t_{2}\}\) elseif\(t_{2}\) is a variable thenreturn\(\{t_{2} t_{1}\}\) elseif\(t_{1}\) and \(t_{2}\) are constants thenreturn\(\) elseif\(t_{1}\) and \(t_{2}\) are atoms of the same arity\(k\)then return\(_{i\{1..k\}}\) SoftUnify(argument \(i\) of \(t_{1}\), argument \(i\) of \(t_{2}\)) elsereturn Failure endif endfunction ```

**Algorithm 1** Soft-unification

To illustrate soft-unification during proving, suppose that \(s(,)=0.5\). We can now delete the fact \(()\) from our example program and still derive the same probability for the query \()}\). The first proof for \(()\) (namely \(())}\)) is unaffected by the deletion of \(()\). But in the second proof, we need to derive \(()\) by applying \(()\), resulting in the proof \(()())}\).

## 3 Soft-unification properties

In principle, any function \(s\) can be used for soft-unification. However, we introduce some properties that make the soft-unification semantically meaningful and efficiently trainable.

**Definition 1**.: _The success score \(P(q)\) is well-defined iff for every soft-unification function \(s\), query \(q\), and pair of logically equivalent programs \(T_{1}\) and \(T_{2}\) (i.e. \( a:T_{1} a T_{2} a\)), the success score \(P(q)\) is the same for \(T_{1}\) and \(T_{2}\)._

**Definition 2**.: _The soft-unification function \(s\) is non-redundant, when given a proof \(\), every other entailed proof of the form \(^{}=(\{t_{i} t_{j}\})\{t_{i} t_{k},t_{k } t_{j}\}\) has at most the same score: \(P(^{}) P()\)._

We motivate definition 2 by returning to our running example. Recall that we made a proof where \(()\) was obtained using the soft-unification: \(()\). Suppose now that the program contains some extra rules: \(:()\) and \(():()\). These rules could be interjected in the proof so that we use: \(()()\). Definition 2 says that introducing these intermediary soft-unifications in the proof should not increase the proof score.

**Definition 3**.: _The soft-unification function \(s\) is connected, when for every \(x\) and \(z\) where \(x z\), it is possible to have \(y\) that is inbetween \(x\) and \(z\) (i.e. \(s(x,y)>s(x,z)\) and \(s(y,z)>s(x,z)\))._

Definition 3 is a statement on the expressivity of soft-unification. For example, it should be possible to model that white and grey are similar (e.g. \(s(,)=0.5\)), and grey and black are similar (e.g. \(s(,)=0.5\)), but that white and black are dissimilar (e.g. \(s(,)=0.25\)).

**Definition 4**.: _We call the proof score \(P(q)\) effectively optimizable when the soft-unification \(s\) is differentiable, and it is possible to have a gradient with respect to every proof \(\) in \(B(q)\)._

This last definition essentially asks that the soft-unification function should be trainable using gradient descent. It is important that every proof receives gradients, as updating a single proof at a time leads to local minima due to a lack of exploration [9; 24]. In short, if a wrong proof starts of with a higher score due to an unfortunate initialization, it should not get pushed up greedily disregarding the other proofs. Otherwise, the correct proof might never be discovered during training. Note that \(k\)-best approximate inference does not evaluate all possible proofs and hence cannot satisfy definition 4.

### Neural Theorem Prover

The current (neuro)-symbolic systems unfortunately do not always respect the abovementioned properties. We focus mostly on the Neural Theorem Prover, though table 1 gives an overview of several related (neuro-)symbolic systems, which we later discuss in the related work. We start by introducing some necessary concepts, before giving the proof that the NTP is not always well-defined.

An atom or term is _linear_ when no variable occurs multiple times. A rule is linear when its head is linear. A soft-unification function \(s\) is \(\)-_transitive_ when \(s(x,z) s(x,y) s(y,z)\), and a \(\)-_similarity_ when it is reflexive, symmetric, and \(\)-transitive. The specific meaning of the conjunction operation \(\) depends on the semantics. So \(\)-transitivity should be interpreted as \(\)-transitivity for the NTP and \(\)-transitivity for ProbLog. Omitted proofs can be found in appendix A.

**Theorem 1**.: _The Neural Theorem Prover is not well defined on programs with linear rules._

Proof.: We give a proof by example. Consider the following two programs:

\((,):=()\). \(()\). \(()\). \(()\). and \((,)\). \((,)\). \((,)\). \(()\). \(()\). \(()\).

These programs are logically equivalent but can have a different proof score for the query \(q=(,)\). In \(T_{1}\), we obtain \(P(q)=s(,)\) in the straightforward manner. But in the second proof, we obtain \(P(q)=(s(,),(s(,),s(,)))\), because we can also unify with \((,)\). So with the right choice of \(s\), the proof score \(P(q)\) is higher in \(T_{2}\) than in \(T_{1}\). 

This proof works in general for programs that contain linear rules. It might be tempting to suggest a fix by making \(s\)-transitive, but this does not resolve the problem when negation is considered. Transitivity is still desirable for another reason however, as it relates to definition 2.

**Theorem 2**.: _Definition 2 holds when the soft-unification function \(s\) is \(\)-transitive._

We note that the NTP implements the soft-unification function \(s\) using radial basis functions  (specifically the laplacian kernel \(e^{-\|x-y\|_{2}}\)), which are not \(\)-transitive. The situation in the NTP is further complicated as the choice of semantics links definition 2 and 3.

**Theorem 3**.: _Under the fuzzy semiring, definition 3 cannot hold if \(s\) is \(\)-transitive._

In other words, by choosing the minimum t-norm, definitions 2 and 3 are mutually exclusive. A further disadvantage is that the minimum t-norm also does not satisfy definition 4. By using the minimum to conjoin facts, the success score of a proof is essentially determined by the weakest soft-unification. This is a significant drawback as it heavily reduces training efficiency and leads to local minima .

### ProbLog

Motivated by the previous discussion, we will rely on probabilistic instead of fuzzy semantics. This allows us to satisfy all the stated properties.

**Theorem 4**.: _If the soft-unification function \(s\) is a \(\)-similarity with probabilistic semantics, we can satisfy all stated properties (from Def. 1, Def. 2, Def. 3 and Def. 4)._

**Theorem 5**.: _A soft-unification function \(s\) is a \(\)-similarity, iff \(s(x,y)=e^{-d((x),(y))}\), where \(d\) is a distance function on an embedding space and \(\) maps symbols to this space._

The last theorem shows that the soft-unification corresponds to a distance on a embedding space under the modest assumptions of reflexivity, symmetry, and transitivity. This brings us to the final question of what concrete embedding space to choose. We do this based on two considerations.

   Properties & Def. 1 & Def. 2 & Def. 3 & Def. 4 \\  Bousi\(\)Prolog  & Yes & Depends (\({}^{*}\)) & Depends (\({}^{*}\)) & No \\ WHIRL  & No & No & Yes & No \\ NTP  & No & No & Yes & No \\ LRNN  & No & No & Yes & Yes \\ DeepSoftLog (Ours) & Yes & Yes & Yes & Yes \\   

Table 1: Comparison of the soft-unification properties of different (neuro-)symbolic systems. (\({}^{*}\)): Depends on the choice of transitivity and t-norm.

[MISSING_PAGE_FAIL:6]

We note that this program transformation is only for semantics, and we still apply the soft-unification algorithm during inference. In appendix C, we prove that the soft-unification algorithm is equivalent to regular unification on the transformed program.

**Non-linear rules** The semantics of DeepSoftLog only allow for linear rules, but non-linear rules can be supported as syntactic sugar. Similar to Bousi\(\)Prolog , we linearize rules as follows. The variable X that occurs multiple times in the head is replaced by unique variables, and \(\) predicates are added to the body to perform the soft-unification. For example, the linearization of \((,):-()\) is \((_{1},_{2}):-_{1}_{2},\ (_{1})\). This construction does entail some extra grounding work on \(\). We need to state the \(\) fact for every constant \(\), to make sure \((,)\) in this example can still succeed with non-embedded constants. Similarly, we need rules \((_{1},..._{})( _{1},..._{}):-_{1}_{1 },..._{}_{}\) for every functor \(\) of arity \(k\).

**Inference** We use proof-based inference, as described in section 2.2. Exact inference can become intractable for large experiments, so we support approximate inference techniques (\(k\)-best, maximum search depth, and maximum branching factor), similar to the Greedy NTP . An important constraint we apply is that the soft-unification predicate \(\) needs to be ground upon evaluation, i.e. input mode (++,++) in Prolog. Otherwise, the number of possible unifications explodes. For example in \(()\), \(\) can unify with an infinite number of ground facts: \(()\), \((())\), \(((())),...\)

During training, we can use the gradient semiring of algebraic ProbLog to make the success score end-to-end differentiable . We train using the AdamW optimizer  with cross-entropy loss.

**Structure learning** As in the NTP, we can add templates to the program to do rule learning via parameter learning. For example, \((_{1},,):-(_{1},,),(_{1},, )\). is a transitive template that could be used for the facts \((,,)\).\((,,)\). by learning that \(=_{1}\). These templates can also parameterize the rule symbol \(_{1}\), similar to the conditional theorem prover : \((,,):-( _{1}(),,),(_{2}(),,)\).

## 5 Experiments

We present three sets of experiments. First, we compare DeepSoftLog to the NTP on _structure learning_ tasks, which requires learning rules from data. We show that DeepSoftLog significantly improves the performance over the NTP, and perform ablations to show that the properties introduced in section 3 are instrumental to this. The second experiment requires _learning the perception_, while the background knowledge is given. The last experiment is the most challenging setting, as both perception and rules need to be learned _jointly_. For all experiments, we report the mean and standard deviation over 10 seeds. All code is available on https://github.com/jjcmoon/DeepSoftLog.

### Countries

Countries is a knowledge graph with the locatedIn and neighborOf relations between the countries and continents on Earth. There are three tasks of increasing difficulty (S1, S2 and S3). For example in S2, we need to derive locatedIn(\(,\)) using facts such as locatedIn(\(,\)) and neighborOf(\(,\)). For the full experimental setup, we refer to previous work [26; 29]. To keep the comparison fair, we use the same rule templates, embedding dimensions, and training objective as the NTP. The hyperparameters can be found in appendix D.2. Table 2 reports the results and compares them with other state-of-the-art NeSy systems that support structure learning.

In figure 2, we perform ablation on the properties of DeepSoftLog, discussed in section 3. As expected, DeepSoftLog no longer properly converges when switching from probabilistic semantics to fuzzy semantics (violating property 4). Switching the soft-unification function \(s\) to a Gaussian kernel (violating property 2) also delays convergence. There is no ablation for property 3 as there is no straightforward way to violate it in DeepSoftLog.

### MNIST-addition

MNIST-addition is a popular neuro-symbolic benchmark where, given two numbers as lists of MNIST images, the task is to predict the sum (e.g. \(}

that it allows to increase the reasoning difficulty, by increasing the number of digits. We use the same experimental setup and neural network as previous works, to which we refer for more details .

Existing NeSy solutions to the MNIST-addition problem typically represent all the possible sums explicitly, which means inference becomes exponential in the number of digits. DeepSoftLog allows an alternative encoding of the problem which scales linearly, by using the embeddings to represent discrete probability distributions over the digits. In each iteration, the probability distribution of the carry and the digit classifier are used to calculate the probability distribution of the sum and the next carry. This factorization of the problem is similar to the one used in A-NeSI , although we do not approximate and use exact inference. The program and hyperparameters are included in appendix D.1. Results are summarized in table 3.

The performance of DeepSoftLog is in line with previous exact systems like DeepStochLog, and we note that the improvement over other exact systems such as DeepProbLog and NeurASP is likely due to their suboptimal hyperparameters. More importantly, DeepSoftLog outperforms the more specialized systems Embed2Sym and A-NeSI, while also scaling further.

   Countries & S1 & S2 & S3 \\  NTP  & \(90.93 15.4\) & \(87.40 11.7\) & \(56.68 17.6\) \\ GNTP  & \(99.98 0.05\) & \(90.82 0.88\) & \(87.70 4.79\) \\ DeepSoftLog (Ours) & \( 0.00\) & \( 0.98\) & \( 1.00\) \\  NeuralLP  & \( 0.0\) & \(75.1 0.3\) & \(92.2 0.2\) \\ CTP  & \( 0.00\) & \(91.81 1.07\) & \(94.78 0.0\) \\ MINERVA  & \( 0.00\) & \(92.36 2.41\) & \(95.10 1.2\) \\   

Table 2: AUC-PR on Countries link prediction . Results are adapted from . The three methods on top use templates, while the bottom ones do not.

   Digits per number & 1 & 2 & 4 & 15 & 100 \\  LTN  & \(80.5 23.3\) & \(77.5 35.6\) & timeout & \\ NeuPSL  & \(97.3 0.3\) & \(93.9 0.4\) & timeout & \\ DeepProbLog  & \(97.2 0.5\) & \(95.2 1.7\) & timeout & \\ NeurASP  & \(97.3 0.3\) & \(93.9 0.7\) & timeout & \\ DeepStochLog  & \(97.9 0.1\) & \(96.4 0.1\) & \(92.7 0.6\) & timeout \\ Embed2Sym  & \(97.6 0.3\) & \(93.8 1.4\) & \(91.7 0.6\) & \(60.5 20.4\) & timeout \\ A-NeSI  & \(97.7 0.2\) & \(96.0 0.4\) & \(92.6 0.8\) & \(75.9 2.2\) & overflow \\ DeepSoftLog (Ours) & \( 0.1\) & \( 0.3\) & \( 0.6\) & \( 1.6\) & \( 3.4\) \\   _Reference_ & \(98.17\) & \(96.37\) & \(92.87\) & \(75.78\) & \(15.43\) \\   

Table 3: Test accuracy of the sums on the MNIST addition problem. Results are adapted from . Reference equals \(0.9907^{2d}\) where \(d\) is the number of digits per number. This is a lower bound on the performance that can be achieved when single-digit supervision is available.

Figure 2: (Left): Ablations of DeepSoftLog on countries S2, with the corresponding properties that are violated. VP stands for violated properties. (Right): Loss curve during training of the ablations. The shaded area represents the min/max over 10 seeds, while the line is the mean.

### Differentiable Automata

We can implement a differentiable finite state machine in DeepSoftLog by taking a regular finite state machine in Prolog and making the states and symbols embedded (see listing 1). The resulting model can be trained end-to-end, jointly learning interpretable rules and a perception network.

As an example, consider the regular language \(()^{*}\) represented by MNIST images. We learn from positive examples (e.g. \(\) or \(\) ) and negative examples (e.g. \(\) or \(\) ). We train on sequences of lengths up to 4 and test the generalization by evaluating on sequences of length 8 with images not seen during training. Hyperparameters are in appendix D.3.

We pre-train the perception network with 20 images, for which the ground truth image labels are given for each digit. This small amount of concept supervision prevents reasoning shortcuts  and eases the optimization problem. As a baseline we use an RNN, which has the same perception network and receives the same pretraining.

Results are summarized in table 4. We repeat the experiment for different regular languages. Not only does DeepSoftLog achieve a higher AUC, but it is also highly interpretable as we can inspect the learned state transitions.

## 6 Related Work

There are several neuro-symbolic systems built on the principle of neural reparametrizations in a symbolic framework. DeepStochLog , NeurASP , and DeepProbLog  are neural extensions of stochastic grammars, answer set programming, and probabilistic logic programming respectively. Lifted Relational Neural Networks  and Logic Tensor Networks  are based on Datalog with fuzzy semantics (which cannot deal with structured terms) and use the logic program to construct the neural network. These systems make different trade-offs between expressivity and scalability, with fuzzy systems typically being more scalable and probabilistic systems being more expressive.

DeepSoftLog explores an alternative possibility, by neurally reparameterizing the matching mechanism between symbolic symbols instead of the symbols themselves. DeepSoftLog is most closely related to DeepProbLogA\({}^{*}\), as they both rely on probabilistic possible worlds semantics and backward proving. Hence, it is in principle possible to convert between the two.

The Neural Theorem Prover (NTP)  pioneered the association of learnable embeddings with symbols in a logic prover. However, the fuzzy semantics of the NTP leads to gradient sparsity and local minima, as discussed in section 3.1. Furthermore, the NTP lacks several key features of DeepSoftLog like neural functors, recursive rules, and negation. The Greedy Neural Theorem Prover  improved the scalability of the NTP. The Conditional Theorem Prover  extended the NTP to support rule learning without templates.

The soft matching of symbols in logic is already well-studied in the fuzzy logic programming community . A notable example is Bousi\(\)Prolog [17; 18], which just like the NTP uses soft-unification and fuzzy semantics. These systems still differ considerably from DeepSoftLog as they are not differentiable, and typically rely on the user to manually specify the soft-unification function. WHIRL  proposed to add embeddings in DataLog for more flexible data querying. Their soft-join operation can also be seen as an early form of soft-unification, albeit not a learnable one.

Many approaches towards embedding symbols have been investigated for knowledge graphs. Recently, some of these have also been combined with symbolic methods [11; 36]. Generally, these methods are

   Language & \(()^{*}\) & \(^{*}^{*}\) & \((^{*}^{*})^{*}\) \\  RNN & \(77.63 15.05\) & \(61.59 10.09\) & \(50.14 1.36\) \\ DeepSoftLog & \( 25.87\) & \( 7.18\) & \( 15.98\) \\   

Table 4: Results for the differentiable automata experiments. We evaluate with the AUC-PR and report the average and standard deviation over 10 seeds.

less expressive than DeepSoftLog but far more scalable. MINERVA  uses reinforcement learning to predict links by walking over a knowledge graph conditional on a query.

## 7 Limitations

DeepSoftLog inherits the advantages of probabilistic logic programming, but also the disadvantages. Probabilistic inference is #P-hard, so exact inference is often not tractable and we need to rely on approximate inference techniques (e.g. ). Crucially, some approximate inference methods like \(k\)-best reintroduce the local minima problem from the NTP. So in future work, we want to consider approximate methods that incorporate a form of exploration, such as sampling methods. What still could be investigated in further work is the use of embeddings to scale up inference in NeSy as in section 5.2.

## 8 Conclusion

We analyzed how a more principled integration of embeddings in logic can be achieved for neuro-symbolic learning, by discussing the properties of learnable soft-unification. We discussed how previous systems do not always satisfy these, and how this can lead to optimization problems. We applied our analysis to create a framework on top of probabilistic logic programming with embeddings, called DeepSoftLog. DeepSoftLog demonstrated that two common methods to neurally extend logic, that of neural facts (as done by DeepProbLog) or neural unification (as done by the NTP) are essentially interchangeable. Lastly, we showed that DeepSoftLog can outperform existing neuro-symbolic methods on both accuracy and scalability.