# Interpolating Item and User Fairness in Multi-Sided Recommendations

Qinyi Chen\({}^{1}\)  Jason Cheuk Nam Liang\({}^{1}\)  Negin Golrezaei\({}^{1}\)  Djallel Bouneffouf\({}^{2}\)

\({}^{1}\)Massachusetts Institute of Technology \({}^{2}\)IBM Research

{qinyic,jcnliang,golrezae}@mit.edu  djallel.bouneffouf@ibm.com

###### Abstract

Today's online platforms heavily lean on algorithmic recommendations for bolstering user engagement and driving revenue. However, these recommendations can impact multiple stakeholders simultaneously--the platform, items (sellers), and users (customers)--each with their unique objectives, making it difficult to find the right middle ground that accommodates all stakeholders. To address this, we introduce a novel fair recommendation framework, Problem (fair), that flexibly balances multi-stakeholder interests via a constrained optimization formulation. We next explore Problem (fair) in a dynamic online setting where data uncertainty further adds complexity, and propose a low-regret algorithm FORM that concurrently performs real-time learning and fair recommendations, two tasks that are often at odds. Via both theoretical analysis and a numerical case study on real-world data, we demonstrate the efficacy of our framework and method in maintaining platform revenue while ensuring desired levels of fairness for both items and users.

## 1 Introduction

Online recommendation systems have become essential components of various digital platforms and marketplaces, playing a critical role in user experience and revenue generation. These platforms usually operate as multi-sided entities, recommending items (services, products, contents) to users (consumers). Examples include e-commerce (Amazon, eBay, Etsy), service platforms (Airbnb, Google Local Services), job portals (LinkedIn, Indeed), streaming services (Netflix, Spotify), and social media (Facebook, TikTok). On the platform, there are typically two main groups of stakeholders: **items** (products, services, contents) and **users** (those engaging with items), with the **platform** itself acting as an independent stakeholder due to its unique objectives.

However, as these platforms play a more vital role in societal and economic realms, fairness concerns have prompted increasing regulatory action. Notably, the European Union proposed the Digital Markets Act , which emphasizes the need for contestable and fair markets in the digital sector, designating recommendation systems as a key area of focus; the U.S. Algorithmic Accountability Act  requires companies to assess the impacts of their automated decision systems to ensure they are not biased or discriminatory. It has also become increasingly evident, to the platforms, that upholding fairness across their stakeholder groups not only strengthens relationships with these stakeholders, but also enhances long-term platform sustainability .

Fairness concerns within digital platforms significantly impact both users and items, manifesting in various forms of disparities. For users, issues like racial discrimination in Airbnb's host-guest matching  and gender disparity in career ads  highlight the critical need for enforcing user fairness. Similarly, items on the platforms also experience unfair allocation of opportunities induced by algorithmic decisions, such as e-commerce platforms favoring their own private-label products  and social media prioritizing influencers' contents over others , indicating a disparity that affects items (sellers, content creators) alike. These underscore the necessity to address biases and create a more inclusive environment for all stakeholders involved.

Despite the many efforts by companies to mitigate fairness issues, most of the existing measures solely focus on one stakeholder group, sometimes even at the expense of another. For example, Airbnb has implemented strategies to mitigate racial discrimination among its users , yet their listings of comparable relevance might still not receive similar levels of visibility. Etsy implements measures to promote market diversity and supporting new entrants, which contributes to equality of opportunities for its items (sellers) [27; 13]; however, such a strategy may inadvertently disadvantage users by potentially overwhelming them with choices and hindering their ability to quickly find their preferred products. Similar to industrial practices, the majority of ongoing research on fairness in recommender systems (e.g., [13; 55; 71]) also focus solely on either users or items, but seldom both.

Addressing multi-sided fairness is inherently complex due to the competing interests and objectives of different stakeholders [39; 12]. If algorithms solely prioritize the platform's profits, this can lead to inequitable exposure for those more niche items and users who prefer them. In addition, what users perceive as fair may be viewed as unfair by items, and vice versa .

In face of these challenges, our work aims to answer the following two questions: (1) _What constitutes a fair recommendation within a multi-sided platform?_ and (2) _How would a platform implement a fair recommendation in a practical online setting?_ Our contributions are summarized as follows:

1. **A novel fair recommendation framework, Problem (fair).** Our fair recommendation problem, framed as a constrained optimization problem, adopts a novel multi-sided perspective that first achieves _within-group fairness_ among items/users, and then enforces _cross-group_ fairness that addresses the trade-offs _across_ groups. Notably, Problem (fair) enables the platform to (i) flexibly define fair solutions for items/users rather than relying on a single predefined notion (see Section 2.2); (ii) adjust trade-offs between its own business goals and fairness for stakeholders (see, e.g., our case study in Section 4); (iii) flexibly accommodate additional operational considerations.
2. **A Fair Online Recommendation algorithm for Multi-sided platforms (Form).** In Section 3, we study an online setting where the platform must ensure fairness for a sequence of arriving users amidst data uncertainty. We present FORM, a low-regret algorithm tailored for fair online recommendation, whose efficacy is further validated via a real-world case study (Section 4). The design of FORM contributes both methodologically and technically. (i) Methodologically, contrary to prior works (e.g., [55; 71]) that treats learning and enforcing fairness as two separate tasks, we recognize that these two tasks, when conducted simultaneously, are often at odds. FORM well balances learning and fairness via properly relaxing fairness constraints and introducing randomized exploration. (ii) Technically, FORM overcomes a non-trivial challenge of managing _uncertain fairness constraints_ when solving a constrained optimization problem in an online setting with bandit feedback. While existing works on online constrained optimization all would require certain access to constraint feedback (see discussion in Section 1.1), our setup disallows even verifying the satisfaction of item/user fairness constraints, demanding novel design in FORM.

### Related Works

Our work primarily contributes to the emerging area of algorithmic fairness in recommender systems. Additionally, from a technical aspect, our algorithm contributes to the field of constrained optimization with bandit feedback. We highlight the literature most relevant to our work in both areas.

**Algorithmic fairness in recommender systems.** Algorithmic fairness is an emerging topic [8; 2; 42] explored in various contexts such as supervised learning [14; 24], resource allocation [9; 37], opportunity allocation , scheduling , online matching , bandits , facility location , refugee assignment , assortment planning , online advertising , search , and online combinatorial optimization . Our research is primarily aligned with works investigating fairness in recommender systems; see [67; 20] for comprehensive surveys.

The prior works on fairness in recommender systems can be categorized into three streams based on their subjects: _item fairness_, _user fairness_ and _joint fairness_. Item fairness focuses on whether the decision treats items fairly, including fair ranking and display of search results [73; 11; 48; 10; 60; 66; 17; 72], similar prediction errors for items' ratings , and long-term fair exposure . User fairness examines whether the recommendation is fair to different users, encompassing aspects like similar recommendation quality [26; 45; 68] and comparable explainability  across users.

The third stream, also the stream that our work fits in, focuses on joint fairness that concerns whether both items and users are treated fairly. However, the number of works in this stream remain scarce, as suggested in . Existing works mainly focuses on achieving item and user fairness based on certain pre-specified fairness notions:  seeks to balance item and user neighborhoods;  proposes a method that guarantees maxmin fair exposure for items and envy-free fairness for users; [71; 52] promote fairness with respect to item exposures and user normalized discounted cumulative gains. Additionally,  proposes and compares a family of joint multi-sided fairness metrics to tackle systematic biases in content exposure, and  introduces a multi-objective optimization framework that jointly optimizes accuracy and fairness for consumers and producers.

Our work distinguishes from the above works on multi-sided fairness in several aspects. (i) We not only impose fairness for multi-stakeholders (items/users), but also take the platform's revenue, which is often neglected, into consideration. (ii) Our framework is not confined to a single, pre-specified fairness/outcome notion. (iii) Most importantly, unlike works that solely focused on multi-sided fairness, we recognize the challenge of jointly handling fairness and learning, and propose an algorithm with theoretical guarantees (see Section 3).

**Constrained optimization with bandit feedback.** Our work formulates the fair recommendation problem in an online setting as a constrained optimization with bandit feedback (see Section 3.1). Our proposed algorithm (Algorithm 1) makes a notable contribution to the literature of constrained optimization with bandit feedback by addressing the challenge of having _uncertain constraints_.

Previous research has explored constrained optimization with bandit feedback in various contexts. Notably, works on _online learning with knapsack_[38; 15; 61] involves maximizing total reward while adhering to a resource consumption constraint, using feedback on rewards and resource usage. Other relevant studies include online bidding [28; 22], online allocation , and safe sequential decision-making , which develop algorithms to monitor and maintain "constraint balances" or address adversarial objectives and constraints. Our setting crucially differs from prior works by not assuming availability of constraint feedback or even the ability to monitor constraint satisfaction, making our problem more challenging (see, also, Section 3.2 for more discussions).

## 2 Preliminaries

Throughout this paper, boldface symbols denote vectors or matrices, while regular symbols represent scalars. For matrix \(^{n m}\), \(A_{i,j}\) is the element at the \(i\)-th row and \(j\)-th column; \(_{i,:}\) and \(_{,j}\) are the \(i\)-th row and \(j\)-th column vectors, respectively. We set \([n]:=\{1,2,,n\}\), \(_{n}\) as the probability distribution space over \([n]\), and \(_{n}^{m}=_{n}_{n}\) (m times). For any \(v\), \((v)^{+}:=(v,0)\).

### Platform's Recommendation Problem

Consider a platform performing recommendations for \(T\) rounds, each indexed by \(t[T]\), where one user visits the platform at each round. There are \(N\) items on the platform, each indexed by \(i[N]\), and the users are classified into \(M\) types, each indexed by \(j[M]\). At each round \(t\), a type-\(J_{t}\) user arrives and the platform observes the user's type.1 Here, we consider a stochastic setting where the probability of having a type-\(j\) user arrival is \([J_{t}=j]=p_{j}\), where \(=(p_{j})_{j[M]}_{M}\) denotes the _arrival probabilities2_. The platform needs to select an item to display to the user, denoted by \(I_{t}\). If a type-\(j\) user is presented with item \(i\), he/she would choose/purchase the item with probability \(y_{i,j}(0,1)\), where \(=(y_{i,j})_{i[N]\\ j[M]}\) denotes the _purchase probabilities_. The platform then observes the user's purchase decision \(z_{t}\{0,1\}\). If item \(i\) gets chosen/purchased, it generates revenue \(r_{i}>0\), where \(=(r_{i})_{i[N]}\) denotes the _revenues_.

Let \(=(,,)\), where \(=_{M}(0,1)^{N M}^{N}\), define an instance of the platform's recommendation problem. Given the problem instance \(\), the platform needs to determine its recommendation probabilities \(_{M}^{N}\), where \(x_{i,j}\) is the probability of offering item \(i\) upon observing a type-\(j\) user's arrival.3 In Section 2, we first fix the problem instance \(\) and omit variablesdependency on \(\) whenever the context allows. Later, we will transition to an online setting where the problem instance \(\) becomes unknown (see Section 3) reestablish the dependency in our discussion.

Here, we focus on a single-item recommendation setting, where the platform highlights one item to each arriving user. This approach applies to various real-world scenarios, such as Spotify's "Song of the Day", Amazon's "Best Seller", Medium's daily "Must-Read" article, etc. Later in Section 3.6 and our case study on Amazon review data in Section 4, we will show that the core concepts of our model and approach can naturally extend to recommending an assortment of items.

### Single-Sided Fair Solutions: Within-Group Fairness for Items and Users

To properly address multi-sided fairness, we begin by first considering _within-group fairness_. That is, we seek to answer: _what constitutes a fair solution within our item/user group_? To address this question, it is important to understand the different _outcomes_ that items and users respectively care about, and the fairness notions they would like to consider.

**Single-Sided Item-Fair Solutions.** Let \(O^{1}_{i}()\) be the expected outcome received by item \(i\) at a round with recommendation probabilities \(\). We let \(O^{1}_{i}()\) take the following general form: \(O^{1}_{i}()=^{}_{i,:}_{i,:}\), where \(=(L_{i,j})_{}\) and \(L_{i,j}\) can be any proxy for the expected outcome received by item \(i\) from type-\(j\) user if it gets offered. One can consider any of the following metrics or their weighted combinations as the item's outcome: (1) _visibility_: \(L_{i,j}=p_{j}\) and \(O^{1}_{i}()=_{j}p_{j}x_{i,j}\); (2) _marksthare_: \(L_{i,j}=p_{j}y_{i,j}\) and \(O^{1}_{i}()=_{j}p_{j}y_{i,j}x_{i,j}\); (3) _expected revenue_: \(L_{i,j}=r_{i}p_{j}y_{i,j}\) and \(O^{1}_{i}()=_{j}r_{i}p_{j}y_{i,j}x_{i,j}\).

For items, a common way to achieve fairness is via the optimization of a _social welfare function_ (SWF), denoted as \(W\), which merges fairness and efficiency metrics into a singular objective (see ). The maximization of SWF can incorporate a broad spectrum of fairness notions commonly adopted in practice, including maxmin fairness , Kalai-Smorodinsky (K-S) bargaining solution , Hooker-Williams fairness , Nash bargaining solution , demographic parity, etc. (See Section A.2 for an expanded discussion of these fairness notions and their social welfare functions.)

To preserve the generality of our framework, we let the platform freely determine the outcome function and fairness notion for items, by broadly defining the _item-fair solution_ as follows.

**Definition 2.1** (Item-Fair Solution): _Given items' outcome matrix \(()\), and a SWF \(W:^{M}_{N}\), an item-fair solution is given by: \(^{1}_{^{M}_{N}}W(^{1}())\)._

One popular fairness notion that can be captured by Definition 2.1 is maxmin fairness, which ensures that the most disadvantaged item is allocated a fair portion of the outcome. A few prior works  on fair recommendation solely focus on achieving maxmin fairness for the visibilities (exposure) received by the items. Our model, however, can flexibly accommodate any outcome functions as listed above and any fairness notions supported via SWF maximization (see Section A.2).

**Single-Sided User-Fair Solutions.** Let \(O^{}_{j}()\) be the expected outcome received by a type-\(j\) user at a round with recommendation probabilities \(\). We again let \(O^{}_{j}()\) take a general form: \(O^{}_{j}()=^{}_{:,j}_{:,j}\), where \(=(U_{i,j})_{}\) and \(U_{i,j}\) is the expected outcome received by a type-\(j\) user if offered item \(i\). Here, the specific form of the user's outcome matrix \(()\) is determined by user's utility model, which can vary depending on contexts. See Section A.1 for some example forms of \(()\) based on discrete choice models (multinomial logit (MNL), probit) used in demand modeling  and valuation-based models used in online auction design . For generality, we do not impose restrictions on the form of \(()\), but merely assume knowledge of it.

For users, achieving fairness is more straightforward. Since the platform can personalize recommendations based on user types, given the users' outcome matrix \(()\), it is best for type-\(j\) users to consistently receive the item that offers the highest utility. We thus define the _user-fair solution_ as:

**Definition 2.2** (User-Fair Solution): _Given users' outcome matrix \(()\), the user-fair solution \(^{y}^{M}_{N}\) is given by \(^{y}_{i,j}=1\) if \(i=_{i}U_{i,j}\) and \(^{y}_{i,j}=0\) otherwise, for all \(i[N],j[M]\)._

[MISSING_PAGE_FAIL:5]

and (ii) the cost of implementing fairness constraints to balance its revenue and stakeholder interests, often known as the "price of fairness" (PoF). In Section C, we investigate the concept of PoF under our framework (Problem (fair)), and show a piecewise linear dependency of the PoF on both (i) the amount of misalignment in the platform's and its stakeholders' objectives, (ii) the fairness parameters \(^{},^{}\) (see Theorem C.1). We further provide guidelines on how a platform can effectively tune its fairness parameters based on its desired fairness level and acceptable PoF via online A/B experimentation (see Section C for an extended discussion)._

## 3 Fair Online Recommendation Algorithm for Multi-Sided Platforms

In practice, user data are often missing or inaccurate, so solving Problem (fair) with flawed data could inadvertently result in unfair outcomes. Our online setting (Section 3.1) allows the platform to improve its estimates of user data via a data collection process over time. However, this simultaneous process of learning to be fair and understanding user preferences also introduces new challenges. In this section, we address the more intricate online setting and introduce an algorithm for this scenario.

### Online Setting and Goals

In an online setting, the platform no longer has prior knowledge of user preference \(\) and arrival rates \(\). At each round \(t\), some non-anticipating algorithm \(\) generates the recommendation probabilities \(_{t}_{N}^{M}\), only based on past recommendation \(\{_{t^{}}\}_{t^{}<t}\) and purchase decisions \(\{z_{t^{}}\}_{t^{}<t}\). After observing the type of the arriving user \(J_{t}\), the platform offers item \(i\) with probability \(x_{t,i,J_{t}}\). Following this recommendation, the platform only observes the user's binary purchase decision \(z_{t}\{0,1\}\) for the offered item. We will measure the performance of our algorithm using the following metrics:

**Definition 3.1** (Revenue and fairness regrets): _Consider Problem (fair) under a problem instance \(\), item outcome function \(():^{N}\), user outcome function \(():^{M}\), item social welfare function \(W:^{N}\), and fairness parameters \(^{},^{}\). Let \(\) be a non-anticipating algorithm that generates recommendation probabilities \(_{t}\) at each round \(t[T]\). We define the **revenue regret**, denoted by \((T)\), and the **fairness regret**, denoted by \(_{F}(T)\), of \(\) respectively as_

\[(T)=_{t=1}^{T}(^{*},)-(_{t},) _{F}(T)=\{_{F}^{}(T),_{F}^{ }(T)\}\,,\]

_where \(_{F}^{}(T)=_{i}_{t=1}^{T}(^{ } O_{i}^{}(^{}(),)-O_{i}^{}(_{t},))^{+}\) and \(_{F}^{}(T)=_{j}_{t=1}^{T}( ^{} O_{j}^{}(^{}(),)-O_{j}^{}(_{t}, {}))^{+}\) are respectively the maximum time-averaged violation of item/user-fair constraints. Here, \((,)=_{i,j}r_{i}p_{j}y_{i,j}x_{i,j}\) is platform's expected revenue under instance \(\); \(O_{i}^{}(,)=_{i,:}()^{} _{i,:}\) and \(O_{j}^{}(,)=_{:,j}()^{} _{:,j}\) are the item/user outcomes under \(\); \(^{}()\) and \(^{}()\) are the item/user-fair solutions w.r.t. \((),()\) and SWF \(W\), per Definition 2.1 and 2.2._

Observe that in Definition 3.1, we reintroduce the dependency of our variables on the instance \(\). This is because in the online setting, we typically work with an estimated instance, which in turn impacts our estimates for the platform's revenue, item/user outcomes as well as item/user-fair solutions.

### Challenges of the Online Setting

Before proceeding, we highlight the two main challenges unique to the online setting.

**(1) Data uncertainty and partial feedback can interfere with evaluating fairness.** Due to lack of knowledge of the instance \(\) and limited feedback (as we only observe the purchase decision for the offered items), it is difficult to assess the quality of our recommendations \(_{t}\) at each round. In particular, verifying whether our fairness constraints are satisfied and/or measuring the amount of constraint violations require evaluating the item/user-fair solutions \(^{}(),^{}()\) and item/user outcomes \(O_{i}^{}(^{}(),),O_{i}^{ }(_{t},)\) and \(O_{j}^{}(^{}(),),O_{j}^ {}(_{t},)\), all of which heavily depend on \(\).

This sets our work apart from prior works on fairness in recommender systems, which assume full knowledge of the problem instance (e.g., [55; 71]), and from works on constrained optimization with bandit feedback (e.g., [38; 15; 61]), which rely on the availability of constraint feedback. As we discussed in Section 1.1, the latter works need to access the amount of constraint violation or verify constraint satisfaction after each decision to update their policies. For example, in online learning with knapsack, the constraint is a resource budget, so constraint violations can be directly evaluated. Our work contributes to the literature on constrained optimization with bandit feedback by directly handling _uncertain constraints_ and providing a sublinear regret bound (see Theorem 3.1).

**(2) Fairness can interfere with the quality of learning.** To ensure fairness for all stakeholder groups, some items (e.g., low-revenue or low-utility items) would necessarily receive lower recommendation probabilities than the others. Nonetheless, if we barely offer these items to the users, the lack of exploration could also lead to poor estimation for their purchase probability.

### Algorithm Description

We now present our algorithm, called FORM (**F**air **O**nline **R**ecommendation algorithm for **M**ulti-sided platforms), which handles the aforementioned challenges by adopting a relaxation-then-exploration technique, allowing it to achieve both low revenue regret and fairness regret. The design of FORM is outlined in Algorithm 1, consisting of the following.

**Input:** (i) \(N\) items with revenues \(^{N}\), item outcome \(():^{N M}\), item SWF \(W:_{N}^{M}\); (ii) \(M\) types of users with user outcome \(():^{N M}\); (iii) fairness parameters \(^{},^{0}\).

1. **Initialization.** Set \(_{1,i,j}=1/2\) and \(_{j}=1/M\) for \(i[N],j[M]\). Let the magnitude of exploration be \(_{t}=\{N^{-1},N^{-}t^{-}\}\), and define the magnitude of relaxation as \[_{t}=M(T)\{_{y,t},_{p,t}\}\,,\] (1) where \(_{y,t}=2(T)/\{1,t/(T)-\}}\) and \(_{p,t}=5\).
2. For \(t=1,,T\) 1. **Solve a relaxed version of Problem (fair) under data uncertainty.** Given estimated instance \(}_{t}=(}_{t},}_{t},)\) and relaxation \(_{t}\), let \(}_{t}\) be the optimal solution to Problem (fair-relax\((}_{t},_{t})\)). 2. **Recommend with randomized exploration.** * Let the recommendation probability be \(_{t,i,j}=(1-N_{t})}_{t,i,j}+_{t}\) for all \(i[N],j[M]\). * Observe the type of the arriving user \(J_{t}\) and offer item \(I_{t}\) based on probabilities \(I_{t}_{t,i,J_{t}}\). * Observe purchase decision \(z_{t}\{0,1\}\) and update the number of user arrivals: \(n_{J_{t},t}=n_{J_{t},t-1}+1\). 3. **Update estimates for purchase and arrival probabilities.** Let \[_{t+1,i,j}=}_{k=1}^{n_{j,t}}\{I_{_{ j,k}}=i,z_{_{j,k}}=1\}/x_{_{j,k},i,j},_{t+1,j}=n_{j,t}, }_{t+1}=(}_{t+1},}_{t+1},)\,,\] (2) where \(_{j,k}[T]\) denote the round at which the \(k\)th type-\(j\) user arrives.

**Solve a relaxed version of Problem (fair) under the estimated instance.** Recall, from our first challenge, that we cannot directly verify if the fairness constraints have been satisfied due to having data uncertainty and partial feedback. If the platform solves Problem (fair) using the estimated instance, the flaw in estimation could easily lead to failure of maintaining fairness for some stakeholders. In face of this, FORM solves a _relaxed_ version of Problem (fair), defined as follows:

\[_{_{N}^{M}} (,}_{t})  O_{i}^{}(,}_{t})^{ } O_{i}^{}(^{}(}_{t}), }_{t})-_{t}  i[N]\] \[O_{j}^{}(,}_{t})^{ } O_{j}^{}(^{}(}_{t}), }_{t})-_{t}  j[M]\,.\]

where \(}_{t}\) is the estimated instance at round \(t\) and \(_{t}>0\) is a parameter that regulates the magnitude of relaxation on our fairness constraints (Eq. (1)). Here, Problem (fair-relax\((}_{t},_{t})\)) differs from Problem (fair) in that (i) it uses the estimated instance \(}_{t}\) rather than the ground-truth instance \(\), and (ii) it relaxes all fairness constraints by the amount of \(_{t}\). At a high level, the relaxation here ensures that the solution fair to all stakeholders (i.e., \(^{}\)) would be captured even under data uncertainty. The magnitude of relaxation \(_{t}\) depends on \(_{y,t}\) and \(_{p,t}\), which are respectively confidence bounds associated with estimated purchase/arrival probabilities (see Definition D.1). As our estimates become more accurate, both confidence bounds shrink, hence decreasing the magnitude of fairness relaxation.

**Recommend with randomized exploration.** In order to handle the second challenge of some items being inadequately explored, we incorporate randomized exploration by sampling from a distribution that perturbs the estimated solution to Problem (fair), \(}_{t}\), by a carefully tailored amount \(_{t}\). This allows ongoing exploration of all items, with the magnitude of exploration \(_{t}\) also decreasing over time as our parameter estimates improve, shifting from exploration towards greater exploitation.

**Unbiased estimators for our problem instance.** In Algorithm 1, for simplicity, we estimate purchase probabilities \(\) using an inverse probability weighted estimator and estimate arrival probabilities \(\) with the sample mean (See Eq. (2)). For sufficiently large \(t\), our estimates \(}_{t}\) and \(}_{t}\) will be accurate with high probability (see Lemma D.2). In practice, the platform, potentially with access to historical data, can freely use any learning mechanism that yield unbiased estimators for user preferences and arrival rates. As long as the estimates get sufficiently accurate over time with high probability, FORM would ensure low revenue/fairness regrets, all while keeping the rest of its design unchanged.

### Theoretical Analysis

We theoretically analyze the performance of FORM, under a mild local Lipschitzness assumption.

**Assumption 3.1**: _Given instance \(\), there exists constants \(B,>0\) such that for any \(}\) where \(\|}-\|_{}\), \(\{\|()-(})\|_{},\|^ {I}()-^{I}(})\|_{}\} B\|-}\|_{}\,.\)_

Assumption 3.1 is well justified in practice. In terms of users' outcome matrix, the assumption readily holds for all prevalent users' choice models and valuation-based models (see examples in Section A.1) as long as \(\) is bounded away from the boundaries of \(\). For item-fair solutions \(^{I}()\), a wide range of standard outcome functions (visibility, revenue, etc.) and fairness notions (maxmin, K-S, etc.) readily induce locally Lipschitz item-fair solutions; see Section A.3 for some examples.

Theorem 3.1 is the main result of this section, which states that FORM achieves both sublinear revenue regret and fairness regret, as desired. The proof of Theorem 3.1 is deferred to Section D.

**Theorem 3.1** (Performance of Form): _Given any problem instance \(\) and assume that Assumption 3.1 holds, for \(T\) sufficiently large, we have that_

* _the revenue regret of_ FORM _is at most_ \([(T)](MN^{}T^{-})\)_;_
* _the fairness regret of_ FORM _is at most_ \([_{F}(T)](MN^{}T^{-})\)_._

### Computational complexity and scalability

In terms of the computational complexity of FORM, the dominant runtime cost in each iteration of FORM arises from solving Problem (fair-relax\((}_{t},_{t})\)). Note that for a wide variety of commonly used item-fairness notions (e.g., maxmin, K-S, demographic parity; see Table 1) and item outcome functions (e.g., visibility, revenue), solving Problem (fair-relax\((}_{t},_{t})\)) involves solving two linear programs with \(MN\) variables, which is solvable in polynomial runtime \(O^{*}((MN)^{2+1/18})\). The remaining operations in each iteration of FORM takes \(O(MN)\) time. Consequently, each iteration of FORM has a worst-case complexity of \(O^{*}((MN)^{2+1/18})\). In practice, however, much better performance can often be achieved by advanced LP solvers such as Gurobi and CPLEX.

For real-world deployments, FORM can be further adapted with scalability in consideration. First, in practice, there is no need to solve Problem (fair-relax\((}_{t},_{t})\)) at every user arrival. Instead, platforms can resolve the problem after a given number of user arrivals or periodically at fixed time intervals, while updating user data in real-time. This allows majority of the iterations to run in \(O(MN)\) time and removes the computational overhead.4 Second, we do not always encounter a large-scale optimization problem when applying our framework. Real-world recommendation systems often narrow down items through lightweight pre-filtering stages based on criteria like keywords or price range (e.g., ), allowing us to enforce fairness within smaller, context-specific subsets. Our fairness framework is also particularly impactful at this final stage, where items with similar attributes compete for visibility and a revenue-maximizing strategy could lead to extremely unfair outcomes.

### Extensions to Additional Setups

Our framework, Problem (fair), and the proposed algorithm can be extended to accommodate other variations of our setup. Below, we briefly introduce these extensions; see Section E for more details.

**Periodic arrivals.** While our current model focuses on stochastic arrivals with fixed user arrival probabilities \(\), real-world recommendation systems often observe non-stationary user arrivals with hourly, daily or weekly periodicity. In Section E.1, we show that by additionally integrating a sliding window mechanism, FORM can seamlessly accommodate periodic arrivals, and attain the same \((MN^{1/3}T^{-1/3})\) guarantees for both revenue and fairness regrets.

**Recommending an assortment.** As remarked in Section 2, while our model adopts a single-item recommendation setting, the high-level ideas behind our framework/method naturally extend to recommending an assortment of size at most \(K\). To extend our framework, Problem (fair), we will let \(\{q_{j}(S):S[N],|S| K,j[M]\}\) be the decision variables, where \(q_{j}(S)\) is the likelihood of proposing assortment \(S\) to a type-\(j\) user. We can then apply the same relaxation-then-exploration techniques to solve the fair recommendation problem in a dynamic online setting. See Section E.2 for details of our extension. In the case study that follows (Section 4), we also validate the efficacy of our framework/method in a real-world assortment recommendation problem.

## 4 Case Studies on Amazon Review Data

In our case study on Amazon review data, we act as an e-commerce platform displaying featured products to incoming users, aiming to maximize revenue while ensuring fairness for items and users. Our experiments numerically validate the efficacy of our framework/method. All algorithms were implemented in Python 3.7 and run on a MacBook with a 1.4 GHz Quad-Core Intel Core i5 processor.

**Data and setup.** We use an Amazon review dataset  from the "Clothing, Shoes and Jewelry" category. Product reviews provide relevance scores between each item and user, serving as a proxy for purchase likelihood. Users are classified into \(M=5\) types using matrix factorization and \(k\)-means clustering on user feature vectors. The arrival probability \(p_{j}\) is set to the proportion of type-\(j\) users. We select \(N=30\) items with the highest variance in relevance scores across user types, indicating a discrepancy between item and user interests and making this a challenging instance. Item revenues \(r_{i}\) are uniformly drawn from \([0.5,1.5]\). The purchase probability and users' utilities are defined based on the multinomial logit (MNL) model . For a type-\(j\) user presented with assortment \(S\), the probability of purchasing item \(i S\) is \(y_{i,j}=}}{1+_{i^{} S}e^{e_{i^{},j}}}\), where \(v_{i,j}\) is the relevance score. The user's perceived utility is \((1+_{i^{} S}e^{e_{i^{},j}})\). Each instance simulates \(T=2000\) user arrivals. Upon arrival, each type-\(j\) user is shown an assortment \(S\) of up to \(K=3\) items.

The platform's primary goal is to maximize its revenue while ensuring maxmin fairness for items w.r.t. item revenue, and fairness for users w.r.t. utilities from the MNL model. We apply the extension of FORM for recommending assortments, using relaxation-then-exploration techniques to produce fair recommendations while learning user data (see Section E.2). To establish generality of our framework/method, we have also performed additional experiments under alternative outcomes and fairness notions (see Section F.2) and an alternative movie recommendation setting using MovieLens data (see Section F.3). In all cases, our experiments yielded consistent results.

**Baselines.** We consider six baselines for comparisons. Since all baselines assume full knowledge of the instance and lack a learning phase, we let them use our unbiased estimator to update their estimated instance as they observe purchase decisions, and recommend based on these estimates. (i) _greedy_: offers \(K\) items with the highest expected revenue, prioritizing platform's goal; (ii) _max-utility_: offers \(K\) items with the highest user utilities, a user-centric approach; (iii) _min-revenue_: offers \(K\) items generating the least revenue so far, promoting maxmin fairness for items w.r.t. revenue; (iv) _random_: offers \(K\) items uniformly at random, promoting maxmin fairness for items w.r.t. visibility; (v) FairRec : an algorithm that addresses two-sided fairness in a _static_ setting. While it's not designed for online settings, we adapt it for online arrivals by duplicating users and using the single-shot recommendation solution. It ensures maxmin fairness for item visibility and envy-free fairness for users; (vi) TFROM : addresses two-sided fairness in an online setting, focusing on uniform item visibility and similar user normalized discounted cumulative gain. However, neither FairRec nor TFROM considers platform's revenue; see Section F.1 for more details on these two baselines.

**Platform's revenue.** We first assess FORM's efficacy in achieving a low-regret solution. Figure (a)a shows that the time-averaged revenue of FORM, i.e., \(_{t=1}^{T}(_{t})\), converges rapidly to the optimal revenue for Problem (fair), complementing Theorem 3.1. Figure (b)b compares the time-averaged revenue (normalized by opt-rev) of FORM with other baselines. As expected, _greedy_ achieves revenue close to opt-rev, while all other baselines result in significant revenue loss. FairRec and TFROM, in particular, reduce platform revenue by about 38% as their sole focus is on ensuring two-sided fairness. In contrast, FORM offers tunable parameters to balance platform and stakeholder interests. As shown in Figure (b)b, adjusting \(^{1}\) and \(^{0}\) allows platforms to control revenue loss (e.g., choosing \(^{1},^{0}=0.2\) keeps loss within 10%). See, also, Section C for how a platform can tune fairness parameters to control its "price of fairness" in practice.

**Item and user fairness.** Figure (c)c shows average outcomes for each item \(i\), normalized by the outcome under item-fair solution, \(\{O_{i}^{1}(_{t})/O_{i}^{1}(^{1}),1\}\). As expected, since our item-fair solution \(^{1}\) adopts maxmin fairness w.r.t. item revenues, _min-revenue_ achieves the highest level of item fairness, though at a high cost to the platform. Methods such as _random_, FairRec, TFROM also achieve high item fairness but with some discrepancies in maximum and minimum item outcomes. _greedy_ and _max-utility_ show extremely skewed allocations, with some items receiving minimal or no revenue. In comparison, our algorithm FORM strikes a good balance, ensuring all items nearly attain or surpass the specified fairness levels, whether the level is high (\(_{1}=0.8\)) or moderate (\(_{1}=0.2\)).

Figure (d)d shows the average outcomes for each user type, normalized by their outcome under the user-fair solution, \(\{O_{j}^{1}(_{T})/O_{j}^{1}(^{1}),1\}\). In the Amazon review data, user interests align well with the platform's objectives (as validated in Section C), leading to most baselines performing fairly well and achieving high levels of fairness for the users. FORM again ensures good user outcomes, all while maintaining high platform revenue and desired item fairness levels.

## 5 Conclusion and Future Directions

Our work introduced a novel fair recommendation framework that maintains platform revenue while addressing multi-stakeholder fairness, as well as a low-regret algorithm that effectively produces fair recommendations amidst data uncertainty. It is worth noting that the high-level ideas behind our versatile framework has the potential to be applied in settings beyond recommender systems, such as dynamic pricing and online advertising, ensuring fairness across different stakeholders and promoting stable market conditions in these applications.

There are several future directions worth investigating. (i) Our current framework calibrates recommendation policies within a shorter time period when user preferences and item attributes are relatively fixed. Future research can explore long-term effects of our method by developing adaptive fairness notions that account for evolving user and item attributes and quantifying the long-term multi-stakeholder fairness. (ii) It would be interesting to pursue real-world deployments of our framework/algorithm and evaluate their impact using an expanded set of metrics, such as user satisfaction, retention rates, and recommendation diversity.

Figure 1: Experiment results for Amazon review data. Fair-rev(\(^{1}\), \(^{0}\)) is the platform’s revenue from solving Problem (fair) in hindsight with fairness parameters \(^{1}\), \(^{0}\) and FORM(\(^{1}\), \(^{0}\)) is FORM when adopting fairness parameters \(^{1},^{0}\). In Figures (c)c and (d)d, item (user) outcomes are shown in ascending order. All results are averaged over \(10\) simulations, with the line indicating the mean and shaded region showing mean \(\) std/\(\).