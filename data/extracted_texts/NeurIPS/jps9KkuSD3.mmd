# Sequential Harmful Shift Detection Without Labels

Salim I. Amoukou  Tom Bewley Saumitra Mishra

**Freddy Lecue Daniele Magazzeni Manuela Veloso**

J.P. Morgan AI Research

Correspondence to: Salim I. Amoukou <salim.ibrahimamoukou@jpmorgan.com>

###### Abstract

We introduce a novel approach for detecting distribution shifts that negatively impact the performance of machine learning models in continuous production environments, which requires no access to ground truth data labels. It builds upon the work of Podkopaev and Ramdas (2022), who address scenarios where labels are available for tracking model errors over time. Our solution extends this framework to work in the absence of labels, by employing a proxy for the true error. This proxy is derived using the predictions of a trained error estimator. Experiments show that our method has high power and false alarm control under various distribution shifts, including covariate and label shifts and natural shifts over geography and time.

## 1 Introduction

When deploying a machine learning model in production, it is common to encounter changes in the data distribution, such as shifts in covariates (Shimodaira, 2000), labels (Saerens et al., 2002; Lipton et al., 2018) or concepts (Goncalves Jr et al., 2014). Many methods exist for detecting such distribution shifts. However, a distinct but equally important challenge is assessing whether a shift has a harmful impact on the prediction error of a given model, which may necessitate interventions such as ceasing production or retraining the model. Not all distribution shifts are harmful, but traditional methods for shift detection are unable to distinguish harmful and benign shifts.

While some approaches address the specific issue of performance shift, most require access to ground truth data labels in the production environment (Gama et al., 2013, 2014; Bayram et al., 2022). In scenarios where predictions concern future outcomes, such as medical diagnosis or credit scoring, immediate access to labels in production is not feasible. This work focuses on the challenge of detecting harmful distribution shifts -- those that increase model error in production -- without requiring access to labels. As Trivedi et al. (2023) note, current methods for harmful shift detection without labels rely on disparate heuristics, often lacking a solid theoretical foundation. Such methods include proxies based on aggregate dataset-level statistics (Deng and Zheng, 2021), optimal transport mappings between training and production distributions (Koebler et al., 2023), and model-specific metrics such as input margins (Mouton et al., 2023), perturbation-sensitivity (Ng et al., 2023), disagreement-metrics (Chen et al., 2023; Ginsberg et al., 2022), and prediction confidence (Guillory et al., 2021; Garg et al., 2022). While such methods may be practically effective in certain contexts, they rely on assumptions and correlations that do not hold universally, so can provide no guarantees.

Furthermore, conventional methods rely on two-sample or batch testing, which involves comparing the statistical properties of a production dataset with those of a control sample. These methods have inherent limitations, as the sample size is prespecified. This is a problem because the necessary amount of data to detect any given shift is unknown beforehand. Furthermore, in real-world scenarios, data typically arrive sequentially over time and shifts may occur either suddenly or gradually. In such scenarios, it may be desirable to detect harmful shifts as early as possible. Batch testing is ill-suited to the sequential context (Maharaj et al., 2023), as it does not accommodate the collection of additional data for retesting without adjusting for multiple testing, leading to diminished power.

The most principled and relevant work to our problem is that of Podkopaev and Ramdas (2022), which tackles the problem of sequential harmful shift detection with false alarm control but assumes the availability of ground truth labels in production. Our work builds on the foundation established by Podkopaev and Ramdas (2022), extending the methodology to detect harmful shifts in unlabeled production data while effectively managing false alarms.

Our approach leverages a secondary model to estimate the errors of the primary model. While learning such a model might seem challenging at first, consider a situation where the primary model performs well overall but struggles with specific data subgroups. Sagawa et al. (2019) demonstrate that this phenomenon can occur in natural distributions. In such cases, learning to predict "error given X" might be easier than the primary task of predicting "Y given X", because the error estimator only needs to identify those subgroups where the primary model struggles. This approach has shown promise in recent studies (Zrnic and Candes, 2024; Amoukou and Brunel, 2023). More generally, Zrnic and Candes (2024) note that predicting the magnitude of the error, rather than its direction, is often easier. Furthermore, our approach is based on estimating the proportion of high-error observations over time. For this task, the error estimator does not need to be very accurate; it only needs to assign higher values to observations with higher errors. That is, the estimator only needs to correctly order most observations from low to high error, which is easier than precisely predicting the error itself. We demonstrate in Section 4.1 that even a relatively inaccurate error estimator can be effective at identifying high-error observations, and thus provides the functionality required by our framework. Although this paper uses a learned error estimator's predictions as a proxy for error, we note that any scalar function correlated with error could suffice to isolate high-error observations. For example, for a well-calibrated binary classification model, we could instead use that model's predicted probability, tracking observations with predictions near 0.5 to identify uncertain predictions.

Figure 1 gives an overview of our approach. We first fit the secondary error estimator model to predict the error of the primary model, then use labeled data to calibrate an estimated error threshold () that separates observations with low () and high () true error as fully as possible. We run the error estimator on all observations encountered in production and continually monitor the proportion of observations whose estimated error falls above the threshold. We raise an alarm when this exceeds the rate of high-error observations () in the calibration set plus a tolerance threshold \(_{}\) and correction terms to deal with the sequential setting and account for uncertainty in the estimates. In the example shown, this occurs at time \(t=10\).

The rest of this paper is organized as follows. Section 2 outlines the problem definition and Section 3 provides an overview of the foundational work of Podkopaev and Ramdas (2022) as background. Section 4 is dedicated to the presentation and theoretical analysis of our sequential statistical test. Section 5 demonstrates the empirical efficacy of our method, showcasing its strong detection capabilities and controlled false alarm rates across various types of harmful shift.

## 2 Problem Definition

Let \(\) and \(\) be input and label spaces, \(f:\) be a predictive model, and \(:^{2}\) be a measurable and bounded error function that is selected for monitoring purposes. The model's error on a specific observation \((,Y)\), drawn from a joint distribution \(P_{(,Y)}=P_{}P_{Y|}\), is represented by the random variable \(E=(f(),Y)\). The probability distribution of the error is denoted by \(P_{E}\). As discussed above, our focus is not on detecting shifts in covariates or labels per se, but rather changes in the error distribution \(P_{E}\). Error changes can be caused by various types of shift in the underlying joint distribution, including changes in \(P_{X}\) while the conditional label distribution \(P_{Y|}\) remains constant (covariate shift) or changes in \(P_{Y}\) while \(P_{|Y}\) remains constant (label shift). We assume access to a dataset \(_{n}=\{(_{i}^{0},Y_{i}^{0})\}_{i=1}^{n}\), sampled independently from a _source_ distribution \(P_{(,Y)}^{0}\). In addition, we have a sequence of data \((_{t},Y_{t})_{t 1}\) drawn independently from a time

Figure 1: Overview of the proposed approach. **Left:** calibrating an estimated error threshold to separate low/high true errors. **Right:** sequentially tracking production data exceeding the threshold and raising an alarm upon a significant increase.

varying distribution encountered by the model in production, \(P^{t}_{(,Y)}\). We model the occurrence of a shift in production by assuming this distribution is equal to the source before some time \(T\{\}\) (i.e., \(P^{t}_{(,Y)}=P^{0}_{(,Y)}, t<T\)) and different thereafter (i.e., \(P^{t}_{(,Y)} P^{0}_{(,Y)}, t T\)).

Our goal whenever there is a shift, (i.e., \(T<\)), is to decide if this shift is harmful to the model error. To formalize this, we introduce \(:()^{+}\) as a mapping from probability distributions on the error space \(\) to a real-valued parameter. This mapping could, for instance, map the distribution to its mean or a certain quantile. We aim to construct a sequential test for the following pair of hypotheses:

\[H_{0}: t 1,\ (_{k=1}^{t}(P ^{k}_{E}))(P^{0}_{E})+_{};\] (1) \[H_{1}: t T:(_{k=1}^{t}(P ^{k}_{E}))>(P^{0}_{E})+_{},\] (2)

where \(P^{k}_{E}\) denotes the error distribution at at time \(k\), the _running risk_\(_{k=1}^{t}(P^{k}_{E})\) is the average value of the error parameter up to time \(t\), and \(_{} 0\) is a tolerance level. Intuitively, \(H_{0}\) holds if the running risk remains below that of the source distribution (\(+_{}\)) for all time throughout production, and \(H_{1}\) holds if this condition is violated.

**Objective.** Construct a \(\)-level sequential test, defined by an _alarm_ function \(:_{k=1}^{}^{k}\{0,1\}\), which at time \(t\) uses the first \(t\) observations \(_{1},,_{t}\) to output 0 (no harmful shift so far) or 1 (harmful shift; raise an alarm) with a controlled false alarm rate and high power, i.e.,

\[_{H_{0}}( t 1:\ (X_{1},,X_{t})=1) ,_{H_{1}}( t 1:\ (X_{1},,X_{t})=1)  1.\] (3)

We refer to this problem definition as _sequential harmful shift detection_ (SHSD).

## 3 SHSD with Production Labels

A work closely related to ours is that of Podkopaev and Ramdas (2022), which offers a solution for scenarios where the ground truth labels of the production data are available. This method leverages confidence sequences (Darling and Robbins, 1967; Jennison and Turnbull, 1984; Johari et al., 2015; Jamieson and Jain, 2018), which are time-uniform (i.e., valid for any time) confidence intervals, allowing for the ongoing monitoring of any bounded random variable. With access to labels, it is possible to calculate the true errors on the production data over time and monitor the running risk.

Choosing the mean as the error parameter i.e. \((P^{k}_{E})=_{P^{k}}[E]\), Podkopaev and Ramdas (2022) use the empirical production errors \(E_{1}=(f(_{1}),Y_{1}),,E_{t}=(f(_{t}),Y_{t})\), to construct a confidence sequence lower bound \(\) for the running risk, satisfying a chosen miscoverage level \(_{}(0,1)\):

\[ t 1,\ _{k=1}^{t}(P ^{k}_{E})(E_{1},,E_{t}) 1-_{}.\] (4)

This equation guarantees that the lower bound remains valid over time with high probability. Furthermore, given the errors on the source data \(E^{0}_{1}=(f(^{0}_{1}),Y^{0}_{1}),,E^{0}_{n}=(f(^{0}_{ n}),Y^{0}_{n})\), either another confidence sequence or a traditional confidence interval method (Howard et al., 2021; Waudby-Smith and Ramdas, 2020) can be used to construct a fixed-time upper confidence bound \(\) for the mean error \((P^{0}_{E})\). For a miscoverage level \(_{}(0,1)\), \(\) satisfies the following condition:

\[ n 1,\ ((P^{0}_{E})(E^{0}_{1}, ,E^{0}_{n})) 1-_{}.\] (5)

An alarm is raised when the lower bound of the running risk in production exceeds the upper bound of the source error plus a tolerance \(_{}\). Formally, this equates to defining the function \(\) as follows:

\[_{m}(E_{1},,E_{t})=\{(E_{1},,E_{t})>(E^{0}_{1},,E^{0}_{n})+_{}\},\] (6)

where the subscripted \(m\) denotes that the mean is the error parameter being tracked. This methodology provides uniform control over the false alarm rate across time, i.e.,

\[_{H_{0}}( t 1:\ _{m}(E_{1},,E_{t})=1) _{}+_{}.\] (7)

It also makes no assumptions about the data distribution or the type of shift. However, the reliance on immediate access to ground truth production labels at each time \(t\) limits the method's practical applicability. We now propose a solution that avoids the need for production labels.

Sequential Harmful Shift Detection without Production Labels

This section consists of two subsections, each detailing one of the two stages of our proposal. The initial stage consists of fitting an error estimator and calibrating it to identify high-error observations with few mistakes. Following this, we apply confidence sequence methods to track the proportion of high errors over time in production, and develop a test for raising an alarm based on this proportion.

### Fitting and Calibrating the Error Estimator

The primary drawback of the Podkopaev and Ramdas (2022) method is its reliance on having ground truth labels for the production data, which are often unavailable in real-world scenarios. A straightforward solution is to use a _plug-in_ approach: replace the true error in production with an estimated error obtained from a secondary predictive model, denoted as \(:\). This model trained to predict the true error of the primary model using any available labeled data. We can then reformulate the alarm function of Equation 6 to deal with unlabeled production data as follows:

\[_{m}(_{1},,_{t})=\,\{((_{1}),,(_{t}))>(E^{0}_{1},,E^{0}_{n})+ _{}\}\] (8)

If \(()\) is sufficiently accurate, the performance of this alarm mechanism should align closely with what would be achieved if ground truth labels were available. However, even if the estimator \(\) exhibits strong performance on its training distribution, the absence of labels in production makes it difficult to conclusively determine the alarm's reliability in a shifting production environment.

Our strategy to address this issue consists of using a calibration step to derive a more reliable statistic from the imperfect estimator \(()\). Specifically, we propose to track the proportion of observations above a carefully-selected quantile of estimated error, rather than the mean value as in the original method of Podkopaev and Ramdas (2022). The fundamental hypothesis here is that an estimator, even if not particularly accurate at predicting error magnitudes, may still effectively distinguish between the lowest and highest errors across a dataset, thereby preserving most ordinal relationships between observations. For example, if \(()\) has correctly represented some underlying patterns to predict the errors, and if \(k\)-th and \(l\)-th ranked errors are significantly different, then it is highly probable that \((_{(k)})(_{(l)})\). Focusing on the aggregate distinction of low and high errors rather than the prediction of specific magnitudes allows us to utilize an imperfect estimator \(\) more effectively.

Our proposed calibration process is as follows. Given the labeled source data \(_{n}\) and a trained error estimator \(\), we identify an empirical quantile of the true errors, \(q=(p,\{E^{0}_{i}\}_{i=1}^{n}),p[0.5,1)\), and an empirical quantile for the estimated errors \(=(,\{(^{0}_{i}):^{0}_{i} _{n})\},(0,1)\), such that the selector function \(S_{,}()=\{()>\}\) reliably distinguishes between observations with true error below and above \(q\). Specifically, we seek to balance the statistical power and false discovery proportion (FDP) of the selector, which are defined as follows:

\[=^{n}S_{,}(^{0}_{i}) \{E^{0}_{i}>q\}}{_{i=1}^{n}\{E^{0}_{i}>q\}}; =^{n}S_{,}(^{0}_{i}) \{E^{0}_{i} q\}}{_{i=1}^{n}S_{,}(^{0}_{i })}.\] (9)

We search over a uniform grid of quantile pairs \((p,)\), compute the associated thresholds \((q,)\), and identify those that achieve an FDP below a maximum value. Among these qualifying pairs, we select the one that maximizes the power. Figure 2 illustrates this process for a toy example. In this case, thresholds are found that achieve a selector power of \(0.72\) while keeping FDP below the specified maximum of \(0.2\).

We now present empirical evidence that it is possible to achieve high power and a controlled FDP in realistic settings,

Figure 2: Calibration toy example. **Left:** threshold grid created by sweeping \(p[0.5,0.95]\) at increments of \(0.05\) and \([0.1,0.9]\) at increments of \(0.1\). **Middle:** FDP of selector for each \((p,)\) pair. Black outline indicates pairs for which FDP \(<0.2\). **Right:** selector power for each \((p,)\) pair. Green dotted outline indicates the pair that maximises power subject to the FDP \(<0.2\) limit. Corresponding thresholds \((q,)\) shown as thick lines in left plot.

using the California house prices (Dua and Graff, 2017), Bike sharing demand (Fanaee-T, 2013), HELOC (FICO, 2018) and Nhanesi (CDC, 1999-2022) datasets. We partition each dataset into training (60%), test (20%) and calibration (20%) sets and use the training data to train random forests (RFs) as the primary models. However, we first ablate the training data in various ways to ensure the models perform poorly on certain subgroups. The ablation is done on a per-feature basis. For continuous features, we exclude 80% of observations with values either above or below the median. For categorical features, we exclude data from one category. We then simulate production environments by gradually reintroducing these previously excluded observations alongside the test set. For each dataset, the number of distribution shifts studied equals the number of features times the number of splits: two for continuous features and the number of categories for discrete ones. We use half of the calibration sets to train RF regressors as the error estimators, then use the remainder to calibrate true and estimated error thresholds using the grid search process described above.

In Figure 3, we present the distribution of the FDP and power across all datasets and shifts, relative to the performance of the error estimator, as measured by the R-squared score on the source/calibration data. The R-squared score is binned into quantiles, with 10 bins used. We observe that the estimators are generally highly imperfect, with R-squared values consistently below 0.3. Despite these low predictive accuracies, we can still find threshold pairs that achieve an FDP below 0.2 in the source data (shown next to the red boxplot). The power ranges from 0.4 for the least accurate estimators to 0.9 for the most accurate. Crucially, when we apply the calibrated thresholds in the production environments, we achieve similarly low FDP values (shown in red), almost always below 0.25 (though some reach \(0.4\)), while the power remains similar to the source data, ranging from \(0.4\) to \(0.9\). This consistency of the FDP/power even when error estimators are not particularly accurate is promising for shift detection.

### Sequential Testing Framework and Performance Guarantees

We can now state the specific objective of our sequential testing framework. During production, we propose to test if there is an increase in the proportion of observations exceeding the true error quantile \(q\) obtained in calibration. This is formalized in terms of the following hypotheses:

\[H_{0}: t 1,\ _{k=1}^{t}_{P^{k}} (E>q)_{P^{0}}(E>q)+_{},\] (10) \[H_{1}: t T:_{k=1}^{t}_{P^{k }}(E>q)>_{P^{0}}(E>q)+_{},\] (11)

where \(_{P^{k}}\) denotes a probability taken under distribution \(P^{k}\). Note that this is a special case of the general test in Equations 1 and 2, with the probability \((P_{E}^{k})=_{P^{k}}(E>q)\) as the error parameter.

Since we cannot observe production errors directly, we use the selector function \(S_{,}()\) as a proxy for a check on the true error \(E>q\). The effectiveness of the sequential test under this substitution depends on how well the selector's power and FDP properties generalize from the source distribution to the production environment. In particular, we can show that the method outlined below provably controls the false alarm rate given in Equation 3 if the following assumption holds:

**Assumption 4.1**.: \(\ t 1\)_, \(_{k=1}^{t}_{P^{k}}(S_{,}() =1,E q)_{P^{0}}(S_{,}()=1,E  q)\)._

Referring back to the example in Figure 2 (left), this assumption implies that at all times during production, the proportion of data observed so far falling the quadrant above and to the left of the calibrated thresholds (\(\)) does not exceed that observed under the source distribution. While we do not claim that this assumption always holds exactly, we find that it is only violated to a small extent in realistic settings (see Appendix A for more discussion and experimental analysis). If this is the case, and thresholds \((q,)\) have been found that yield a small number of false discoveries in calibration, \(_{P^{0}}(S_{,}()=1,E q)\), then the number of false discoveries in production will also remain low. A substantial increase in false discoveries in production would require a shift specifically targeting those rare observations with low error but high estimated error.

With this foundation established, we can now describe our testing methodology. Following a similar approach to that used by Podkopaev and Ramdas (2022), we construct:

Figure 3: Selector FDP (**left**) and power (**right**) vs estimator accuracy. Results on source data in blue; results on production data in red.

1. A lower bound of \(_{k=1}^{t}_{P^{k}}(E>q)\) using a confidence sequence.
2. An upper bound of \(_{P^{0}}(E>q)\) using a traditional confidence interval.

To construct the lower bound, we rewrite the target quantity as follows:

\[_{k=1}^{t}_{P^{k}}(E>q) =_{k=1}^{t}_{P^{k}}(S_{,}( )=1,E>q)+_{P^{k}}(S_{,}()= 0,E>q)\] (12) \[_{k=1}^{t}_{P^{k}}(S_{, }()=1,E>q)\] (13) \[=_{k=1}^{t}_{P^{k}}(S_{, }()=1)-_{k=1}^{t}_{P^{k}}(S_{, }()=1,E q)\] (14) \[_{k=1}^{t}_{P^{k}}(S_{,}()=1)-_{P^{0}}(S_{,}()= 1,E q).\] (15)

The last inequality uses Assumption 4.1 to substitute the probability of a false discovery in production with the probability on the source. As we can empirically estimate both \(_{P^{k}}(S_{,}()=1)\) and \(_{P^{0}}(S_{,}()=1,E q)\) (via the labeled source data \(_{n}\)), we can use a confidence sequence to construct a valid time-uniform lower bound of their sum. Specifically, we define the bound \(_{q}\) as

\[_{q}=_{k=1}^{t}\{S_{,}( _{k})=1\}-_{i=1}^{n}\{S_{ ,}(_{i}^{0})=1,E_{i}^{0} q\}-w_{t}-w_{n},\] (16)

where \(w_{t}\) and \(w_{n}\) are the widths of the lower and upper bounds of \(_{k=1}^{t}_{P^{t}}(S_{,}() =1)\) and \(_{P^{0}}(S_{,}()=1,E q)\) with miscoverage levels \(_{1}\) and \(_{2}\) respectively, such that for a total miscoverage level \(_{}=_{1}+_{2}(0,1)\),

\[( t 1:\ _{k=1}^{t}_{P^{k}}(E>q )_{q}) 1-_{}.\] (17)

The specific values of \(w_{t}\) and \(w_{n}\) used in our experiments are given in Appendix B. Respectively, these choices correspond to the predictably-mixed empirical-Bernstein (PM-EB) confidence sequence described by Podkopaev and Ramdas (2022), and the classic Hoeffding interval.

We similarly compute an upper bound \(_{q}\) for \(_{P^{0}}(E>q)\) as follows:

\[_{q}=_{i=1}^{n}\{E_{i}^{0}>q\}+w_{n},\] (18)

where \(w_{n}\) is the same as above. This bound satisfies a miscoverage level \(_{}(0,1)\), such that

\[(_{P^{0}}(E>q)_{q}) 1-_{ }.\] (19)

Finally, we define our sequential test using the following alarm function:

\[_{q}(_{1},,_{t})=\{_{q}>_{q}+_{}\},\] (20)

where the subscripted \(q\) denotes that we are now detecting shifts in error across a particular quantile, rather than the mean. In Appendix C, we provide a proof of the following statement:

**Theorem 4.2**.: _Under Assumption 4.1, \(_{q}\) and \(_{q}\) satisfy Equations 17 and 19. Therefore, the function \(_{q}\) has false alarm control, i.e.,_

\[_{H_{0}}( t 1:\ _{q}(X_{1},,X_{t})=1) _{}+_{}.\] (21)

While a controlled false alarm rate is a desirable property, the power of \(_{q}\) may be limited if the degree of error change is not large. Noting that \((1/t)_{k=1}^{t}_{P^{k}}(E>q)\) is lower-bounded by \((1/t)_{k=1}^{t}_{P^{k}}(S_{,}()=1,E>q)\), detecting a change requires this probability to exceed \(_{P^{0}}(E>q)\). Thus, we also propose to compare \(_{k=1}^{t}_{P^{k}}(S_{,}() =1,E>q)\) directly with \(_{P^{0}}(S_{,}()=1,E>q)\). This leads to a second test with higher power. It uses an upper bound of \(_{P^{0}}(S_{,}()=1,E>q)\), defined as:

\[_{q}^{2}=_{i=1}^{n}\{S_{,}( _{i}^{0})=1,E_{i}^{0}>q\}+w_{n},\] (22)

satisfying

\[(_{P^{0}}(S_{,}()=1,E>q) _{q}^{2}) 1-_{}.\] (23)

The alarm function for the second test is defined as:

\[_{q}^{2}(_{1},,_{t})=\{_{q}>_{q}^{2}+_{}\}.\] (24)

Through an almost identical proof, we can similarly show that \(_{q}^{2}\) also has false alarm control for comparing \((1/t)_{k=1}^{t}_{P^{k}}(S_{,}()=1,E>q)\) with \(_{P^{0}}(S_{,}()=1,E>q)\).

Experiments

In this section, we compare the performance of the plug-in approach of Podkopaev and Ramdas (2022)'s method (Equation 8), which is designed to detect a change in the mean error, and our approach, which determines if an increasing number of observations fall beyond a certain quantile. We focus on the second test (Equation 24) to simplify the comparison with the mean detector and because it consistently outperforms the first statistics. Results for the first test are reported in Appendix E. We conduct three experiments using a variety of datasets and setups. The first experiment aims to illustrate the different approaches and demonstrate the applicability of our method to image data and deep learning models. The second experiment returns to the tabular datasets studied in Section 4.1, going into more detail by comparing the mean and quantile detection approaches in terms of power and FDP on the numerous generated shifts. The final experiment also consists of a large-scale evaluation of the approaches, in this case on natural shifts due to temporal and geographical changes. Although the focus of this paper is on the sequential or online setting, we provide an analysis using state-of-the-art methods in the batch setting in Appendix F.

### Illustrative Example on an Image Dataset

The first experiment replicates the setup of Saerens et al. (2002) using the CelebA dataset (Liu et al., 2015). They demonstrate that a ResNet50 model (He et al., 2016) trained on this dataset performs poorly on "males with blond hair" due to spurious correlations. We split this dataset into a training set (60%), test set (20%) and calibration set (20%), and train a ResNet50 on the training set. Using half of the calibration set, we train another ResNet50 (with a regression head) as an error estimator. The remaining half is employed to determine the empirical quantiles \(p[0.5,1),(0,1)\) at which we achieve maximum power while keeping the FDP below 0.2. We create a harmful shift in production as follows. For each time step up to \(t=4990\), we sample an observation uniform-randomly from the test set. Thereafter, we begin to oversample instances of males with blond hair, sampling such an observation with probability \(_{t}=1/(1+(-(t-4990)))\), and a random observation otherwise.

The objective of this experiment is to visually observe how the methods can be used to monitor performance shift over time and to evaluate how each method compares to an idealised version with access to true production errors. Both Podkopaev and Ramdas (2022)'s method (mean detector) and our approach (quantile detector) involve comparing a lower bound to an upper bound. For both methods, Figure 4 displays the lower bound in blue and the version calculated with true production errors in gray. For the quantile detector, the blue line corresponds to \(_{q}\) of Equation 16, which is the estimated lower bound of \(}_{k=1}^{}_{P^{k}}(E>q)\) with estimated production errors. The gray line represents the lower bound of this same quantity, except computed using the true errors. The blue lower bound of the plug-in approach of the mean detector is defined as \(((_{1}),,(_{t}))\). The gray line represents \((E_{1},,E_{t})\), the lower bound of the original mean detector using true errors. The upper bound that needs to be surpassed for each method to raise an alarm is depicted in red. For the quantile detector, this is the second lower bound \(_{q}^{2}\), and for the mean detector, it is \(((_{1}),,(_{t}))\). For the quantile detector, we also plot in pink the upper bound of the first statistic \(_{q}\) (Equation 20).

The R-squared value of the error estimator on the source distribution is \(0.35\), which is not especially high. By analyzing the upper bounds for each method in Figure 4, we observe that all bounds remain roughly constant before the shift starts. Unsurprisingly, we observe the mean detector using true

Figure 4: Evolution of bounds in production for mean detector **(left)** and quantile detector **(right).**

production errors quickly detects the shift (gray line). In contrast, its plug-in version raises an alarm with a significantly delayed detection. For the quantile detector, there is a much smaller difference between the lower bound of the plug-in and the one using true production errors. This observation validates our expectation that the FDP remains relatively stable post-shift. Additionally, as expected, the plot shows that the lower bound of the quantile detector crosses the upper bound of the second statistic (red line) much earlier than that of the first statistic (pink line).

This experiment suggests that in scenarios with a less accurate error estimator, targeting quantile changes is more effective for detecting harmful shifts than focusing on mean change. Additional experiments on image datasets confirming this observation can be found in Appendix D. Larger-scale analyses in the following subsections examine the advantages of the quantile detector in more depth.

### Synthetic Shifts on Tabular Datasets

In this section, we conduct a large-scale experiment to evaluate the effectiveness of both methods in detecting harmful shifts while maintaining their ability to control false alarms. We also analyze how these metrics relate to the performance of the error estimator. We use two regression datasets (California house prices and bike sharing demand) as well as two classification datasets (HELOC and Nhanesi). We follow the feature-splitting setup of Section 4.1 to generate synthetic distribution shifts, excluding splits that result in subsets with fewer than 10 observations, and repeat each split 50 times with different random seeds.

Table 1 shows the number of generated shifts and the number of harmful shifts detected by each method using the true errors (H-M for mean detector and H-Q for quantile detector). A shift is considered harmful by each method as soon as the lower bound exceeds the upper bound plus \(_{tol}=0\).

The left plot of Figure 5 displays the aggregated results across all distribution shifts for mean detection (red) and quantile detection (green) on the different datasets. The points labeled "all-[method]" represent the average results across the datasets. The quantile method achieves a significantly better power-FDP balance: (power 0.83, FDP 0.11) compared to the mean method: (power 0.67, FDP 0.41) across all experiments. An exception is observed for the Nhanesi dataset, where the mean detection shows slightly better power. However, overall, the quantile detection demonstrates a superior trade-off between power and false alarms. A similar trend is observed in the middle plot, which analyzes the absolute difference in detection time between each method using estimated errors and the same method with access to true errors. In the right plot, we compute how the power across datasets varies when we increase the threshold at which we consider the true shift as harmful (\(_{tol}\)). Across varying intensities of shift, the quantile detector consistently outperforms the mean detector, with false alarm rates at \(_{tol}=0\) being 0.41 and 0.11, respectively.

In Figure 6, we further investigate the relationship between the power (top row) and FDP (bottom row) of each method and the error estimator's performance binned into 10 quantiles for each dataset. The error estimator performance, measured by R-squared values, is generally low across all experiments (0.10 - 0.26). Notably, the quantile detector consistently maintains a lower FDP compared to the

  
**Data** & **\# Generated Shifts** & **H-M** & **H-Q** \\  california & 62 & 10 & 48 \\ bike & 2129 & 57 & 961 \\ helic & 3385 & 774 & 1283 \\ nhanesi & 1697 & 377 & 679 \\   

Table 1: Description of the shifts generated.

Figure 5: **Left:** Power/FDP when \(_{tol}=0\) for all datasets. **Middle:** Absolute detection time difference vs. the methods using true errors. **Right:** Power values for different harmfulness thresholds (\(_{tol}\)).

mean detector across all error estimator values. Regarding power, excluding the Nhanesi dataset, the quantile detector performs better than or equal to the mean detector.

### Natural Shifts on a Tabular Dataset

In the last experiment, we conduct another large-scale evaluation of our approach on natural shifts within the Folktables dataset . This dataset is derived from US Census data spanning all fifty states within the US (plus Puerto Rico), each with a unique data distribution. Furthermore, it includes data from multiple years (from 2014 to 2018), introducing a form of temporal distribution shift in addition to the variations between states. We select the income feature as the target label, specifically predicting whether income exceeds \(\$50,000\). We first split the dataset of each state in the year 2014 into training (\(50\%\)), and calibration (\(50\%\)). Then, we train a separate RF classifier in each state in the year 2014, and an RF regressor to learn the error of the primary model on the calibration set. Subsequently, we evaluate the model's error on all the remaining 50 states over 5 years, effectively creating 250 production datasets. We consider a shift to be harmful if the model's error in production exceeds the error on the calibration dataset plus \(_{}=0\). We introduce the shift in all datasets starting at time \(t=3300\).

Table 2 summarizes the results for both methods, demonstrating that the quantile detector consistently outperforms the mean detector across all metrics.

Figure 7 plots the sensitivity of each method relative to the shift harmfulness threshold. We observe that the quantile detector maintains superior performance across all threshold values.

Overall, this experiment provides good evidence that our proposed method is effective under the kinds of natural shift encountered in realistic production environments.

## 6 Conclusion

We have introduced an approach to identifying harmful distribution shifts in continuous production environments where ground truth labels are unavailable. Utilizing a plug-in strategy that substitutes true errors with estimated errors, alongside a threshold calibration step, our method effectively controls false alarms without relying on perfect error predictions. Experiments on real-world datasets demonstrate that our approach is effective in terms of detection power, false alarm control and detection time across various shifts, including covariate, label, and temporal shifts. In future work, we plan to apply interpretability techniques to the quantile detector to understand where and how the data are shifting in the input space, and to use this information to improve the primary model itself.

   Method & Power & FDP & Mean detection time \\  Quantile detector & 0.48 & 0.019 & 3727 \\ Mean detector & 0.01 & 0.19 & 4945 \\   

Table 2: Comparison of detection methods on Folktables data.

Figure 6: Power and FDP by error across all datasets.

Figure 7: Power corresponding to different levels of the harmfulness threshold (\(_{tol}\)) on Folktables.

## Disclaimer

This paper was prepared for informational purposes by the Artificial Intelligence Research group of JPMorgan Chase & Co and its affiliates ("J.P. Morgan") and is not a product of the Research Department of J.P. Morgan. J.P. Morgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein. This document is not intended as investment research or investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful.