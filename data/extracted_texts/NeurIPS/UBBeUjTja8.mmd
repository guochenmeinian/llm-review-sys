# Cross-modal Active Complementary Learning

with Self-refining Correspondence

 Yang Qin\({}^{1}\), Yuan Sun\({}^{1}\), Dezhong Peng\({}^{1,3,4}\), Joey Tianyi Zhou\({}^{2}\),

**Xi Peng\({}^{1}\), Peng Hu\({}^{1}\)\({}^{*}\)**

\({}^{1}\) College of Computer Science, Sichuan University, Chengdu, China.

\({}^{2}\) Centre for Frontier AI Research (CFAR) and Institute of High

Performance Computing (IHPC), A*STAR, Singapore.

\({}^{3}\) Chengdu Ruibei Yingte Information Technology Co., Ltd, Chengdu, China.

\({}^{4}\) Sichuan Zhiqian Technology Co., Ltd, Chengdu, China.

{qinyang.gm,joey.tianyi.zhou,pengx.gm,penghu.ml}@gmail.com, sunyuan_work@163.com, pengdz@scu.edu.cn

Corresponding author.

###### Abstract

Recently, image-text matching has attracted more and more attention from academia and industry, which is fundamental to understanding the latent correspondence across visual and textual modalities. However, most existing methods implicitly assume the training pairs are well-aligned while ignoring the ubiquitous annotation noise, _a.k.a_ noisy correspondence (NC), thereby inevitably leading to a performance drop. Although some methods attempt to address such noise, they still face two challenging problems: excessive memorizing/overfitting and unreliable correction for NC, especially under high noise. To address the two problems, we propose a generalized Cross-modal Robust Complementary Learning framework (CRCL), which benefits from a novel Active Complementary Loss (ACL) and an efficient Self-refining Correspondence Correction (SCC) to improve the robustness of existing methods. Specifically, ACL exploits active and complementary learning losses to reduce the risk of providing erroneous supervision, leading to theoretically and experimentally demonstrated robustness against NC. SCC utilizes multiple self-refining processes with momentum correction to enlarge the receptive field for correcting correspondences, thereby alleviating error accumulation and achieving accurate and stable corrections. We carry out extensive experiments on three image-text benchmarks, _i.e_., Flickr30K, MS-COCO, and CC152K, to verify the superior robustness of our CRCL against synthetic and real-world noisy correspondences. Code is available at https://github.com/QinYang79/CRCL.

## 1 Introduction

Image-text matching aims to search the most relevant samples across different modalities, which is fundamental for most cross-modal tasks . The core of image-text matching is how to accurately measure the similarity between distinct modalities, however, which is challenging due to the visual-textual discrepancy. To tackle the challenge, numerous deep methods are presented to learn the visual-semantic associations of image-text pairs and achieve remarkable progress, thanks to the powerful representation ability of Deep Neural Networks (DNNs) and some well-designed similarity inference architectures . They could be roughly divided into two groups, _i.e_., global-level methods  and local-level methods , which aim at learning the image-to-sentence and region-to-word correlation to infer the cross-modal similarity, respectively. Although these methodsachieved promising matching performance, most of them implicitly require large-scale well-aligned data for training, which is expensive or even impossible to collect due to ubiquitous noise in real-world scenarios [10; 11]. Therefore, there is inevitably imperfect alignment luring in the data, _i.e_., noisy correspondence (NC) , resulting in inferior performance.

To handle the NC problem, some prior arts are presented to alleviate the adverse impact of NC in various tasks, _e.g_., partially view-aligned clustering [13; 14; 15; 16], video-text retrieval , visible-infrared person re-identification , and image-text matching [12; 19; 20]. Specifically, inspired by learning with noisy labels [21; 22], some works [12; 23; 24] are proposed to alleviate the negative impact brought by NC. These works attempt to leverage the memorization effect of DNNs  to gradually distinguish the noisy image-text pairs for robust learning in a co-teaching manner. Furthermore, the predicted soft correspondence is used to recast a soft margin to replace the scalar margin of triplet ranking loss , which helps to avoid misleading the model by mismatched pairs. However, the soft-margin ranking loss is experimentally found to provide only limited robustness against NC, especially under high noise (as shown in Table 1), due to unstable division based on inaccurate predictions. In contrast, some works [19; 20] aim to enhance the robustness of cross-modal methods against NC by starting with a robust loss function, _i.e_., avoiding over-amplification of wrong supervision information to reduce misleading risk. However, the lack of explicitly mitigating the effect of easily separable noise makes them hard to further improve the performance.

To address the aforementioned issues, we propose a generalized robust framework, dubbed CRCL, for learning with noisy correspondences. CRCL could be easily migrated into existing image-text matching methods to enhance their robustness against NC. Our framework introduces a novel Active Complementary Loss (ACL) that applies active and complementary learning to mutually boost robustness against NC. Specifically, we present a robust complementary learning loss that employs complementary pairs, such as "input image-text pairs are irrelevant", to conduct indirect cross-modal learning with exponential normalization. Due to the low likelihood of selecting incorrect complementary pairs, robust complementary learning could reduce the risk of providing incorrect supervision and smooth the losses, thus embracing robustness against NC. However, the robust loss will face the underfitting problem, leading to suboptimal performance. To overcome this issue, a weighted Active learning loss is proposed to enforce the model focus on more reliable positive pairs in addition to only complementary pairs. In addition, we propose a Self-refining Correspondence Correction paradigm (SCC) to obtain stable and accurate correspondence correction. SCC utilizes Momentum Correction (MC) to aggregate historical predictions for stable and accurate correspondence corrections. By combining multiple Self-Refining processes (SR) throughout the entire training process, we alleviate over-memorization for NCs. In summary, the key contributions and innovations of this work are as follows:

* We propose a generalized Cross-modal Robust Contrastive Learning framework (CRCL) to address a pressing and widely-exist problem in image-text matching, _i.e_., noisy correspondence. CRCL empowers existing methods with strong robustness through the perspectives of robust loss and correction techniques.
* A novel Active Complementary Loss (ACL) is presented to balance active and complementary learning, mutually enhancing robustness against NC while encapsulating correct cross-modal associations in the latent common space.
* We design an effective Self-refining Correspondence Correction paradigm (SCC) to achieve accurate and stable soft correspondences, which enables the prediction-based corrections to perceive larger fields and self-refine from the historically learned knowledge.
* Extensive experiments verify the effectiveness and superiority of our framework on three benchmark image-text datasets: Flickr30K, MS-COCO, and CC152K. Additionally, comprehensive ablation studies and insightful analyses demonstrate the reliability and practicability of the proposed CRCL.

## 2 The Proposed Method

### Preliminaries and Problem Statement

To be specific, we first provide some definitions of instance-level image-text matching so as to conveniently study noisy correspondence. Let \(=\{,,\}\) be an image-text dataset, where \(\{I_{i}\}_{i=1}^{N}\) and \(=\{T_{i}\}_{i=1}^{N}\) are the training image and text set with size of \(N\). The correspondence label space is defined as \(=\{y_{ij}|i=1,,N;j=1,,N\}\), where \(y_{ij}\) represents the correspondence of pair \((I_{i},T_{j})\), _i.e._, if \(I_{i}\) and \(T_{j}\) are matched (_i.e._, positive pair), \(y_{ij}=1\) otherwise \(y_{ij}=0\). We assume each pair with the same indices has matched correspondence, _i.e._, \(y_{ii}=1,i=1,,N\). However, due to the ubiquitous noise during data collection, some negative pairs are mismatched as positives, _a.k.a_ noisy correspondence (NC) [12; 19], which would introduce wrong supervisory information and misguide model training, leading to performance degradation. Mathematically, we define NC as shown in Definition 1.

**Definition 1**.: _Due to the existence of NC, the learner only has access to the noisy training data (\(_{}\)), instead of clean data (\(\)). Thus, the correspondence label for pair \((I_{i},T_{j})\) is reconsidered as_

\[_{ij}=\{y_{ij}&(1-_{ij}),\\ 1-y_{ik}&_{ik}, k j..\] (1)

_For all pairs, conditioned on that if \(i=j\) then \(y_{ij}=1\) else \(y_{ij}=0\), we have \(_{j k}_{ik}=_{ij}\). Similar to the definitions of noisy labels , we assume that NC is uniform, i.e., \(_{ij}=\) and \(_{ik}=, k j\), where \(\) is a constant to represent the noise rate._

The key to learning with noisy correspondence is to alleviate the misguiding impact and rectify the mismatched pairs. One direct solution is to enhance the robustness of loss function \(\) against noisy pairs, which can help prevent overfitting on mismatched pairs. The second aims at using the memorization effect of DNNs  to discriminate the mismatched pairs, thus removing unreliable supervision from the training data.

For image-text matching, images and texts are first projected into a shared representation space by two modality-specific networks, denoted as \(v\) and \(g\), respectively. The cross-modal similarity of a given pair \((I_{i},T_{j})\) is then computed as \(S(v(I_{i}),g(T_{j}))\), where \(S(*)\) could be the cosine function [7; 9] or a relevance inference module [8; 4]. For brevity, \(S(v(I_{i}),g(T_{j}))\) is denoted as \(S(I_{i},T_{j})\) or \(S_{ij}\) in the following. The computed similarities could be considered as supporting evidence for retrieved results. Thus, the learning objective of image-text matching is to maximize the cross-modal similarities of positive pairs while minimizing those of negatives in the latent common space, which is commonly achieved by using contrastive learning [7; 8; 4].

The widely-used triplet ranking loss  has shown excellent performance in cross-modal contrastive learning tasks [7; 27]. However, recent research  has demonstrated that this loss function fails to perform well in image-text data with NCs, especially when using the hardest negative samples as comparison items. To address this issue, some works proposed an adaptive soft margin approach to improve the robustness of the ranking loss [12; 24; 23], which is defined as follows:

\[_{soft}(I_{i},T_{i})=[_{i}-S(I_{i },T_{i})+S(I_{i},_{h})]_{+}+[_ {i}-S(I_{i},T_{i})+S(_{h},T_{i})]_{+},\] (2)

where \(_{h}\) and \(_{h}\) denote the hardest cross-modal samples in a mini-batch, \(_{i}\) is a soft margin adaptively computed by \(_{i}=_{ii}-1}}{m-1}\), \(\) is a constant margin, \(m\) is a curve parameter, and \(_{ii}\) is the rectified correspondence between \(I_{i}\) and \(T_{i}\). However, this approach has two disadvantages: 1) The margin setting \(\) may not be consistent with the empirical setting under NC scenarios. 2) The inaccurate prediction \(_{ii}\) can still easily produce the risk of misleading gradient, which can cause trivial solutions to fail, especially in the case of high noise (_e.g._, the results of NCR  and BiCro  on Flickr30K with 80% noise). To overcome these limitations, we propose a novel Active Complementary Loss (ACL) under the risk minimization theory [28; 26] to provide noise tolerance for noisy pairs while ensuring discriminative learning.

### Active Complementary Loss

For the image-text dataset \(\), our goal is to learn a cross-modal model (\(\)) that can discriminatively identify the positive (matched) and negative (unmatched) pairs well for retrieval, which is intuitively equivalent to maximizing bidirectional matching probabilities of positives. The bidirectional matching probabilities for pair \((I_{i},T_{j})\) are defined as:

\[p_{ij}^{o}=f(I_{i},T_{j})=/)}}{_{l=1}^{N}e^{(S_{il}/ )}},\ \ p_{ij}^{o}=f(T_{j},I_{i})=/)}}{_{l=1}^{N}e^{(S_{ij}/ )}},\] (3)where \(\) is a temperature parameter , \(f\) is regarded as the cross-modal decision function, and "\(\), \(\) " means the two retrieval directions, _i.e._, image-to-text and text-to-image, respectively. For any loss function \(\), the matching risk of \(f\) for image-text matching can be defined as

\[R_{}(f)=_{(I_{i},y_{i}.)}[ (f(I_{i},T.),y_{i}.)]+_{(T_{i},y.)}[ (f(T_{i},I.),y_{i}.)],\] (4)

where \([]\) represents the expectation operator. Considering noisy correspondences, the risk of \(f\) in noisy data \(_{}\) could be formulated as follows:

\[R_{}^{}(f)=_{(I_{i},_{i}.)_{ }}[(f(I_{i},T.),_{i}.)]+_{(T_{i },.)_{}}[(f(T_{i},I.),_{ i}.)],\] (5)

where \(_{ij}\) is the noisy correspondence label as shown in Definition 1. Thus, the cross-modal learning objective is to learn a model \(_{}^{}\) with a global minimizer \(f_{}^{*}\) of \(R_{}^{}(f)\). To achieve robustness, \(f_{}^{*}\) should be also the global minimizer of \(R_{}(f)\) on the noise-free data.

Inspired by complementary contrastive learning , we propose to optimize the matching probabilities of all negative pairs for learning with noisy data indirectly, thereby avoiding fast overfitting to NC. Simultaneously, to further improve the noise tolerance, we introduce an exponential normalization to smooth the complementary loss. Hence, the robust complementary loss for pair \((I_{i},T_{i})\) could be formulated as:

\[_{r}(I_{i},T_{i},q)&= _{r}^{}(I_{i},T_{i},q)+_{r}^{}(T_{i},I_{i},q) \\ &=_{j i}^{N}(p_{ij}^{})/_{k=1}^{N} (p_{ik}^{})^{q}+_{j i}^{N}(p_{ji}^{})/ _{k=1}^{N}(p_{ki}^{})^{q},\] (6)

where \(()\) is the tan function and \(q\) is a regulatory factor. Theoretically, for any input \((I_{i},T_{i})\) under noise rate \(\), we can show (see proofs in supplementary material)

\[C R_{_{r}}(f^{*})-R_{_{r}}(f_{}^{*}) 0,\] (7)

where \(C=2(A_{}^{(1-q)}-A_{}^{(1-q)})/(1-) 0\). \(C\) increases as \(q\) increases and when \(q=1\), \(C\) takes the maximum value \(0\). \(A_{}\) and \(A_{}\) are the maximum and minimum values of \(_{j=1}^{N}(p_{ij})\) under the condition \(_{j=1}^{N}p_{ij}=1\), where \(1<A_{}<A_{}\), and \(0 p_{ij} 1\) (\(p_{ij}=p_{ij}^{}\) or \(p_{ij}^{}\)). \(f^{*}\) and \(f_{}^{*}\) are the global minimizers of \(R_{_{r}}(f)\) and \(R_{_{r}}^{}(f)\), respectively.

**Analysis:** The larger the \(q\) is, \(C 0\), the tighter the bound of Equation (7) is. When \(q=0\), \(_{r}\) is a standard complementary contrastive loss . In the extreme case of \(q=1\), \(R_{_{r}}(f^{*})=R_{_{r}}(f_{}^{*})\), _i.e._, \(_{r}\) is noise tolerant since \(f_{}^{*}\) and \(f^{*}\) are simultaneously the minimizers of \(R_{_{r}}(f)\) (It can also be obtained from Lemma 1 that \(_{r}\) is robust under \(q=1\)). Put another way, as \(q\) approaches 1, the optimum \(f_{}^{*}\) of the noisy risk will be close to \(f^{*}\) on the clean data more likely, which implies noise tolerance.

Like most robust learning methods , we should focus more on reliable data, while less on unreliable data. In other words, a smaller value of \(q\) is used for more convincing pairs (with larger \(_{ii}\)), while a larger value of \(q\) is for less convincing pairs (with smaller \(_{ii}\)). Thus, we could empirically utilize the soft corrected label \(_{ii}\) to recast \(q\) like the soft margin used in NCR , _i.e._, \(q=1-_{ii}\). However, indirect learning will face the underfitting problem, resulting in suboptimal/insufficient performance. To address this issue, we introduce a weighted active learning loss \(_{d}\) to make the model pay more attention to positive/matched pairs, _i.e._, \(_{d}(I_{i},T_{i},_{ii})=-_{ii}( p_{ii}^{ }+ p_{ii}^{})\). This positive learning will mine discrimination from direct supervision, which complements the complementary learning loss. By combining active and complementary learning losses, our active complementary loss is defined as:

\[_{acl}(I_{i},T_{i},_{ii})=_{d}(I_{i},T_{i}, _{ii})+_{r}(I_{i},T_{i},1-_{ii}),\] (8)

where \(\) is a scale factor to prevent \(_{d}\) from dominating the cross-modal training and quickly overfitting NC. As shown in Equation (8), when \((I_{i},T_{i})\) is a noisy pair and \(_{ii}\) ideally approaches 0, the loss emphasizes robust complementary learning, thus mitigating overfitting to NC. Conversely, when \((I_{i},T_{i})\) is a clean pair and \(_{ii}\) ideally approaches 1, the loss focuses on discriminative learning, thereby facilitating the accurate acquisition of visual-semantic associations. However, due to computational resource constraints, we cannot use the entire training set to perform cross-modal learning. Therefore,we relax \(N\) to the size \(K\) of the mini-batch \(\) by Monte Carlo sampling. Without loss of generality, the final loss for cross-modal learning is given by:

\[_{acl}()=_{i=1}^{K}_{acl}(I_{i}, T_{i},_{ii}).\] (9)

**Lemma 1**.: _In an instance-level cross-modal matching problem, under uniform NC with noise rate \(\), when \(q=1\), \(_{r}\) is noise tolerant._

Proof.: The proofs of Equation (7) and Lemma 1 can be found in the supplementary material. 

### Self-refining Correspondence Correction

Another key to solving NC is how to obtain accurate correspondence estimations so as to reduce the adverse effects of NC. To this end, we propose an effective Self-refining Correspondence Correction paradigm (SCC). SCC leverages Momentum Correction (MC) to aggregate historical predictions, providing stable and accurate correspondence estimations while alleviating the over-memorization to NC. To eliminate the error accumulation against NC, we combine multiple independent Self-Refining (SR) in the entire training process. Specifically, the MC for the correspondence of \((I_{i},T_{i})\) at the \(t\)-th epoch is defined as follows:

\[y_{ii}^{t}= y_{ii}^{t-1}+(1-)^{t}(I_{i},T_{i}),\] (10)

where \((0,1)\) represents the momentum coefficient, \(^{t}(I_{i},T_{i})=(p_{ii}^{}+p_{ii}^{})/2\) denotes the average matching probability at the \(t\)-th epoch. Through adequate cross-modal training, our CRCL will obtain a more stable and accurate soft correspondence label by smoothly evolving and expanding the receptive field of correction based on historical predictions with MC. Notably, as training progresses, some pairs would always be incorrectly distinguished as clean or noisy ones, resulting in the error accumulation of the estimated labels (see Figure 1c). Additionally, even though the updates performed by MC help reduce the negative influence of NC, the initial correspondence label \((y_{ii}^{0})\) still greatly affects the quality of subsequent smooth corrections. In other words, providing more accurate initial correspondences weakens the DNN's memorization against NC, thereby reducing the risk of error accumulation.

``` Input: A noisy training dataset \(_{}\), image-text matching model \(()\); Initialize: \(\); for\(e^{j}\) in \([e_{1},e_{2},,e_{m}]\)do for\(t\) in \([1,2,,e^{j}]\)do for\(\) in batchesdo  Obtain the bidirectional matching probabilities of \(\) with Equation (3);  Update the correspondence labels with Equation (11);  Obtain the corrected labels with Equation (12);  Compute the overall loss \(_{acl}()\) with Equation (9); \(=(,_{acl}())\);  end for  end for Re-initialize \(\);  end for Output: The learned parameters \(\); ```

**Algorithm 1**The pseudo-code of CRCL

To achieve accurate initial correspondence estimations, SCC refines updated correspondence labels historically in epochs using MC through multiple concatenated SR pieces during the entire training process. Subsequent SR pieces could gradually aggregate the learned correspondence knowledge from previous pieces, thus improving the quality of estimated correspondences progressively. Furthermore, each SR piece is trained from scratch, which aims to clear accumulated error/noise that has been memorized, thus providing more accurate correspondence predictions for subsequent training. Mathematically, SCC consists of multiple SR pieces (\([e_{1},,e_{m}]\)) based on MC, where each piece undergoes robust learning for \(e_{j}\) (\(j\{1,,m\}\)) epochs. Thus, during the \(j\)-th SR training, the estimated soft label of the \(i\)-th pair at \(t\)-th epoch is reconsidered as follows:

\[y_{ii}^{(j,t)}=\{y_{ii}^{(j-1,e_{j-1})},&$},\\ ^{j,(t-1)}(I_{i},T_{i}),&+1)$},\\  y_{ii}^{(j,t-1)}+(1-)^{(j,t-1)}(I_{i},T_{i}),&,.\] (11)

where \(^{(j,t-1)}(*)\) is the average matching probability of pair \((I_{i},T_{i})\) at the \((t-1)\)-th epoch during the \(j\)-th training piece, \(e_{f}\) denotes the number of epochs to freeze the correspondence label, preventing insufficient model training in the early stage from affecting the correction quality. In our experiments, we set all initial labels to 1, assuming that all training pairs are matched at the beginning of the first SR piece. In practice, we assign the label of the confident noisy pair as 0 to reduce the risk of producing erroneous supervision information. Therefore, the final corrected correspondence label used for \(_{acl}\) is defined as:

\[_{ii}=\{0,&^{(j,t)}<$},\\ y_{ii}^{(j,t)},&,.\] (12)

where \(=0.1\) is a fixed threshold used to filter the confident noisy pairs in experiments.

## 3 Experiments

In this section, comprehensive experiments are conducted on three widely used benchmarks to demonstrate the robustness and effectiveness of our CRCL under multiple scenarios, including synthetic noise, real-world noise, and well-annotated correspondence.

### Datasets and Protocols

**Datasets:** For an extensive evaluation, we use three benchmark datasets (_i.e_., Flickr30K , MSCOCO  and CC152K ) in our experiments. More specifically, Flickr30K is a widely-used image-text dataset collected from the Flickr website, which comprises 31,000 images and each one has 5 corresponding textual descriptions. Following , 30,000 images are employed for training, 1,000 images for validation, and 1,000 images for testing in our experiments. MS-COCO is a large-scale image-text dataset, which has 123,287 images, and 5 captions are given to describe each image. We follow the split of [36; 8] to carry out our experiments, _i.e_., 5000 validation images, 5000 test images, and the rest for training. CC152K is a subset of Conceptual Captions (CC)  collected in the real world, which is selected by . Due to the absence of manual annotation, there are about 3% \(\) 20% incorrect correspondences in CC, _i.e_., real-world noisy correspondences. CC152K contains 150,000 image-text pairs for training, 1,000 pairs for validation, and 1,000 pairs for testing.

**Evaluation Protocols:** Recall at K (R@K=1, 5, and 10) is used to measure the performance of bidirectional retrievals, which is defined as the proportion of the queries with the correct item in the top K retrieved results. Besides, flowing , we also take the sum of all Recalls to evaluate the overall performance, _i.e_., rSum.

### Implementation Details

Our CRCL is a generalized robust framework that could extend existing methods to confront noisy correspondences. To demonstrate the effectiveness and robustness of CRCL, we extend two representative methods, _i.e_., VSE\(\) and SGRAF (SGR and SAF) , to perform robust image-text matching, respectively. Specifically, the shared hyper-parameters are set as the same as the original works [4; 9], _e.g_., the batch size is 128, the word embedding size is 300, and the joint embedding dimensionality is 1,024. More specific hyper-parameters and implementation details are given in our supplementary material due to the space limitation.

### Comparison with State-of-the-Arts

In this section, we evaluate our CRCL by comparing it with 7 state-of-the-art methods on three benchmarks, _i.e_., SCAN (ECCV'18) , SGRAF (SGR and SAF, AAAI'21) , VSE\(\) (CVPR'21) , NCR (NeurIPS'21) , DECL (ACM MM'22) , BiCro (CVPR'23)  and MSCN(CVPR 23) . For a fair comparison, all tested approaches adopt the same visual features (BUTD features)  and textual backbone Bi-GRU . To comprehensively investigate the robustness of our method, we artificially inject synthetic false correspondence of different ratios by proportionally shuffling the captions on Flickr30K and MS-COCO like , _i.e._, 20%, 40%, 60%, and 80% noise rates. In addition to synthetic noise, we also evaluate the robustness of tested methods against the real-world noisy correspondences on CC152K. Due to the space limitation, we only provide the results on MS-COCO 5K under well-annotated correspondences in Table 2. For fairness, like [19; 23], the ensemble results of CRCL-SGRAF are reported in the paper. More extensive comparison results are provided in the supplementary material to fully demonstrate the superiority of CRCL.

#### 3.3.1 Results under Synthetic Noisy Correspondences

For quantitative evaluation under specific noise levels, we conduct all tested methods under four different noise rates (_i.e._, 20%, 40%, 60%, and 80%) of synthetic noisy correspondences on the Flickr30K and MS-COCO datasets. Quantitative results on Flickr30K and MSCOCO 1K test set are shown in Table 1. For MS-COCO, the results are computed by a averaging over 5 folds of 1K test images. From the results, one can see that our CRCL could remarkably outperform the robust baselines (NCR, DECL, BiCro, and MSCN) on most of the metrics, which demonstrates the superior robustness of CRCL against NC. Moreover, our CRCL not only performs well in low noise but also achieves the best performance under high noise, especially 80% noise, which provides strong evidence for the stability and robustness of our method.

   &  &  \\  &  &  &  &  &  & \\  Noise & Methods & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & fSum & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & fSum \\   & SCAN & 56.4 & 81.7 & 89.3 & 34.2 & 65.1 & 75.6 & 402.3 & 28.9 & 64.5 & 79.5 & 20.6 & 55.6 & 73.5 & 322.6 \\  & SAF & 51.8 & 79.5 & 88.3 & 38.1 & 66.8 & 76.6 & 401.1 & 41.0 & 78.4 & 89.4 & 38.2 & 74.0 & 85.5 & 406.5 \\  & SGR & 61.2 & 84.3 & 91.5 & 44.5 & 72.1 & 80.2 & 433.8 & 49.1 & 83.8 & 92.7 & 42.5 & 77.7 & 88.2 & 434.0 \\  & VESco & 69.0 & 89.2 & 94.8 & 48.8 & 76.3 & 83.8 & 461.9 & 73.5 & 93.3 & 97.0 & 57.4 & 86.5 & 92.8 & 500.5 \\  & NCR* & 76.7 & 93.9 & 96.9 & 57.5 & 82.8 & 89.2 & 497.0 & 77.0 & 95.6 & 98.1 & 61.5 & 89.3 & 95.1 & 51.6 \\  & DECL* & 75.6 & 93.8 & 97.4 & 58.5 & 82.9 & 89.4 & 497.6 & 77.1 & 95.9 & 98.4 & 61.6 & 89.1 & 95.2 & 517.3 \\  & BiCro* & **78.1** & 94.4 & 97.5 & 60.4 & 84.4 & 89.9 & 504.7 & 78.8 & 96.1 & 98.6 & 63.7 & 90.3 & 95.7 & 523.2 \\  & MSCN* & 77.4 & 94.9 & 97.6 & 59.6 & 83.2 & 89.2 & 501.9 & 78.1 & **97.2** & **98.8** & 64.3 & 90.4 & 95.8 & 524.6 \\  & **CRCL* & 77.9 & **95.4** & **98.3** & **60.9** & **84.7** & **90.6** & **507.8** & **79.6** & 96.1 & 79.8 & **74.7** & **90.6** & **95.9** & **525.6** \\   & SCAN & 29.9 & 60.5 & 72.5 & 16.4 & 38.5 & 48.6 & 266.4 & 30.1 & 65.2 & 79.2 & 18.9 & 51.1 & 69.9 & 314.4 \\  & SAF & 34.3 & 65.6 & 78.4 & 30.1 & 58.0 & 68.5 & 334.9 & 36.0 & 74.4 & 87.0 & 33.7 & 69.4 & 82.5 & 383.0 \\  & SGR & 47.2 & 76.4 & 83.2 & 34.5 & 60.3 & 70.5 & 372.1 & 43.9 & 78.3 & 89.3 & 70.7 & 62.8 & 85.1 & 406.4 \\  & VESco & 30.2 & 58.3 & 70.2 & 22.3 & 49.6 & 62.7 & 293.3 & 53.3 & 84.3 & 92.1 & 31.4 & 63.8 & 75.0 & 399.9 \\  & NCR* & 75.3 & 92.1 & 95.2 & 56.2 & 80.6 & 87.4 & 486.8 & 76.5 & 95.0 & 98.2 & 60.7 & 88.5 & 95.0 & 513.9 \\  & DECL* & 72.5 & 93.1 & 97.0 & 55.8 & 81.2 & 88.1 & 488.7 & 77.1 & 95.7 & **98.3** & 61.5 & 89.2 & 95.3 & 517.1 \\  & BiCro* & 74.6 & 92.7 & 96.2 & 55.5 & 81.1 & 87.4 & 485.7 & 77.0 & **95.9** & **98.3** & 61.8 & 89.2 & 94.9 & 517.1 \\  & MSCN* & 74.4 & 94.4 & 96.9 & 57.2 & 81.7 & 87.6 & 492.2 & 74.8 & 94.9 & 98.0 & 60.3 & 88.5 & 94.4 & 510.9 \\  & **CRCL* & **77.8** & **95.2** & **98.0** & **60.0** & **84.0** & **90.2** & **505.2** & **78.2** & 95.7 & **98.3** & **63.3** & **90.3** & **95.7** & **521.5** \\   & SCAN & 16.9 & 39.3 & 53.9 & 2.8 & 7.4 & 11.4 & 131.7 & 27.8 & 59.8 & 74.8 & 16.8 & 47.8 & 66.4 & 293.4 \\  & SAF & 28.3 & 54.5 & 67.5 & 22.1 & 47.3 & 59.0 & 27.87 & 28.2 & 63.9 & 79.4 & 31.1 & 65.6 & 80.5 & 348.7 \\  & SGR & 28.7 & 58.0 & 71.0 & 23.8 & 49.5 & 60.7 & 291.7 & 37.6 & 73.3 & 86.3 & 33.8 & 68.6 & 81.7 & 381.3 \\   & VESco & 18.0 & 44.0 & 55.7 & 15.1 & 38.5 & 51.8 & 223.1 & 33.4 & 64.8 & 79.1 & 26.0 & 60.1 & 76.3 & 339.7 \\   & NCR* & 68.7 & 89.9 & 95.5 & 52.0 & 77.6 & 84.9 & 468.6 & 72.7 & 94.0 & 97.6 & 57.9 & 87.0 & 94.1 & 503.3 \\   & DECL* & 69.4 & 89.4 & 95.2 & 52.6 & 78.8 & 85.9 & 471.3 & 73.8 & 94.7 & 97.7 & 59.6 & 87.9 & 94.5 & 508.2 \\   & Bicro* & 67.6 & 90.8 & 94.4 & 51.2 & 77.6 & 84.7 & 466.3 & 73.9 & 94.4 & 97.8 & 58.3 & 87.2 & 93.9 &

#### 3.3.2 Results under Real Noisy Correspondences

In addition to synthetic noise, we carry the comparison experiments on the real-world noisy dataset CC152K. The quantitative results are shown in Table 2. From the results, one can see that our CRCL is superior to all baselines with the best overall performance of 373.7%, which indicates that the proposed method is robust against real-world noise. Specifically, CRCL outperforms the best baseline BiCro, with absolute performance improvement of 1.0%, 0.2%, 0.4%, 0.4%, and 2.0% across different metrics, except for R@1 in text-to-image retrieval.

#### 3.3.3 Results under Well-annotated Correspondences

Besides noisy correspondences, we also evaluate the tested methods trained on the well-aligned MS-COCO dataset for a comprehensive comparison. The results on MS-COCO 5K are shown in Table 2, wherein the results of all baselines are reported by the original papers for a fair comparison, except for the reproduced results of BiCro. From the table, our CRCL remarkably outperforms all baselines in terms of rSum. Specifically, CRCL prevails over the best baselines by 8.2% on overall performance (_i.e_., rSum) absolutely, which shows that our CRCL is not only suitable for noisy cases but also performs well in well-aligned ones.

### Comparison to pre-trained model

In this section, we compare our CRCL to the pre-trained model CLIP  to further evaluate its effectiveness in handling NC. CLIP is a well-known large pre-trained model that is trained from scratch on a dataset of 400 million image-text pairs collected from the Internet, which includes a large number of training pairs with real NCs. More specifically, following , we report the zero-shot results and fine-tuning results under 20% noise and compare them with that of CRCL-SGRAF in Table 3. From the results, CLIP shows a significant performance drop during fine-tuning under NC. On the contrary, the performance of our CRCL under 20% noise is even better than the zero-shot result of CLIP (ViT-L/14), which shows the strength and potential of our CRCL in dealing with NC.

### Ablation Study

In this section, we present ablation studies conducted on Flickr30K with 60% noise, as shown in Table 4. From the table, one can find that the full version of our CRCL achieves the best performance, which indicates that each component contributes to our method for performance improvement. By recasting \(q\) with the rectified label \(_{ii}\) using SCC, \(_{r}\) shows higher potential against NC. Moreover,

    &  &  \\  &  &  &  &  & \\  Methods & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & rSum & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & rSum \\  SCAN & 30.5 & 55.3 & 65.3 & 26.9 & 53.0 & 64.7 & 295.7 & 44.7 & 75.9 & 86.6 & 33.3 & 63.5 & 75.4 & 379.4 \\ VSE\(\) & 34.0 & 64.5 & **77.0** & 12.9 & 19.2 & 21.6 & 229.2 & 56.6 & 83.6 & 91.4 & 39.3 & 69.9 & 81.1 & 421.9 \\ SGRAF & 32.5 & 59.5 & 70.0 & 32.5 & 60.7 & 68.7 & 323.9 & 58.8 & 84.8 & 92.1 & 41.6 & 70.9 & 81.5 & 429.7 \\ NCR* & 39.5 & 64.5 & 73.5 & 40.3 & 64.6 & 73.2 & 355.6 & 58.2 & 84.2 & 91.5 & 41.7 & 71.0 & 81.3 & 427.9 \\ DECL* & 39.0 & 66.1 & 75.5 & 40.7 & 66.3 & 76.7 & 366.4 & 59.2 & 84.5 & 91.5 & 41.7 & 70.6 & 81.1 & 428.6 \\ MSCN* & 40.1 & 65.7 & 76.6 & 40.6 & 67.4 & 76.3 & 366.7 & - & - & - & - & - & - \\ BiCro* & 40.8 & 67.2 & 76.1 & **42.1** & 67.6 & 76.4 & 370.2 & 59.0 & 84.4 & 91.7 & 42.4 & 71.2 & 81.7 & 430.4 \\
**CRCL\({}^{*}\)** & **41.8** & **67.4** & 76.5 & 41.6 & **68.0** & **78.4** & **373.7** & **61.3** & **85.8** & **92.7** & **43.5** & **72.6** & **82.7** & **438.6** \\   

Table 2: Performance comparison on CC152K and MS-COCO 5K.

    & &  &  & \\  Noise Rate & Methods & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & rSum \\ 
0\%, Zero-Shot & 
 CLIP (ViT-L/14) \\ CLIP (ViT-B/32) \\  & 58.4 & 81.5 & 88.1 & 37.8 & 62.4 & 72.2 & 400.4 \\
50.2 & 74.6 & 83.6 & 30.4 & 56.0 & 66.8 & 361.6 \\ 
20\%, Fine-tune & 
 CLIP (ViT-L/14) \\ CLIP (ViT-B/32) \\  & 36.1 & 61.3 & 72.5 & 22.6 & 43.2 & 53.7 & 289.4 \\
21.4 & 49.6 & 63.3 & 14.8 & 37.6 & 49.6 & 236.3 \\
**Our CRCL** & **59.3** & **85.2** & **91.9** & **42.9** & **71.9** & **82.1** & **433.3** \\   

Table 3: Comparison with CLIP on MS-COCO 5K.

through the combination of active learning and robust complementary learning, there is further performance improvement, which indicates the effectiveness of our ACL. Note that these results are obtained from a single model for a comprehensive evaluation, and are not ensemble results as shown in Tables 1 and 2.

### Visualization Analysis

To further investigate the generalization and robustness of CRCL, we visually study the retrieval performance of our CRCL-VSE\(\), CRCL-SAF, and CRCL-SGR on Flickr30K and MS-COCO with varying noise rates in Figure 1(a,d). From the figure, one can see that CRCL can enhance the robustness of the original methods and perform well even under high noise. In addition, we provide visualization to intuitively showcase the effectiveness of the proposed SCC. Specifically, we visualize the distributions of both cross-modal similarities and corrected correspondences for noisy training pairs using CRCL-SGR on Flickr30K with 60% NCs. For comparison, we also visualize the results without SCC, where we replace \(_{ii}\) with the prediction \(^{t}(I_{i},T_{i})\) for the current iteration. The visualizations clearly indicate that SCC prevents the over-accumulation of errors in correspondence correction, thus verifying its effectiveness in mitigating the impact of NC.

## 4 Conclusion

In this paper, we propose a generalized robust framework CRCL to endow traditional methods with robustness against noisy correspondences. To alleviate the harmful effects brought by NC, we present

   Configuration &  &  &  \\ loss & SCC & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & rSum \\  \(_{acl}\) & ✓ & **70.5** & **91.3** & **95.6** & **52.5** & **79.4** & **86.8** & **476.1** \\ \(_{d}\) & ✓ & 69.3 & 91.1 & 95.2 & 51.4 & 78.2 & 86.0 & 471.2 \\ \(_{r}\) & ✓ & 67.7 & 91.2 & 95.0 & 52.3 & 79.0 & 86.4 & 471.6 \\ \(_{r}(q=1)\) & 25.2 & 53.9 & 65.6 & 20.3 & 45.0 & 57.2 & 267.2 \\ \(_{r}(q=0)\) & 63.5 & 89.7 & 93.8 & 48.2 & 74.5 & 82.0 & 451.7 \\ \(_{d}\) & & 65.6 & 87.5 & 93.2 & 47.7 & 74.6 & 82.7 & 451.3 \\   

Table 4: Ablation studies on Flickr30K with 60% noise.

Figure 1: (a,d) The performance on Flickr30K and MS-COCO with varying noise rates; (b,c/e,f) The similarities and corrected correspondences of training pairs after learning without/with SCC.

an Active Complementary Loss (ACL) and a Self-refining Correspondence Correction technique (SCC). These techniques enable effective learning of visual-semantic associations and progressive correction of correspondences based on historical predictions, thus boosting the robustness of image-text matching. Extensive experiments demonstrate that our CRCL achieves state-of-the-art robustness to both synthetic and real-world NCs. Furthermore, ablation studies and visualization analyses further verify the effectiveness and reliability of the proposed components.

## 5 Limitations and Broader Impact Statement

Despite the promising performance of our proposed CRCL, there are some limitations that should be acknowledged. First, we only study the NC problem between two modalities, _i.e_., image and text. Second, we did not take into account category-level noise. The proposed CRCL likely impacts various applications that require robust image-text matching, _e.g_., multimedia retrieval, and image annotation. We encourage further study to understand and mitigate the biases and risks potentially brought by image-text matching.