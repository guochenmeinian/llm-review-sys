# Adaptive and Optimal Second-order Optimistic Methods for Minimax Optimization

Ruichen Jiang

ECE department, UT Austin

rjiang@utexas.edu

&Ali Kavis

ECE department, UT Austin

kavis@austin.utexas.edu

&Qiujiang Jin

ECE department, UT Austin

qiujiangjin0@gmail.com

&Sujay Sanghavi

ECE department, UT Austin

sanghavi@mail.utexas.edu

&Aryan Mokhtari

ECE department, UT Austin

mokhtari@austin.utexas.edu

###### Abstract

We propose adaptive, line search-free second-order methods with optimal rate of convergence for solving convex-concave min-max problems. By means of an adaptive step size, our algorithms feature a simple update rule that requires solving only one linear system per iteration, eliminating the need for line search or backtracking mechanisms. Specifically, we base our algorithms on the optimistic method and appropriately combine it with second-order information. Moreover, distinct from common adaptive schemes, we define the step size recursively as a function of the gradient norm and the prediction error in the optimistic update. We first analyze a variant where the step size requires knowledge of the Lipschitz constant of the Hessian. Under the additional assumption of Lipschitz continuous gradients, we further design a parameter-free version by tracking the Hessian Lipschitz constant locally and ensuring the iterates remain bounded. We also evaluate the practical performance of our algorithm by comparing it to existing second-order algorithms for minimax optimization.

## 1 Introduction

In this paper, we consider the min-max optimization problem, also known as the saddle point problem:

\[_{^{m}}\ _{^{n}}\ f( ,),\] (1)

where the objective function \(f:^{m}^{n}\) is twice differentiable and convex-concave, i.e., \(f(,)\) is convex for any fixed \(^{n}\) and \(f(,)\) is concave for any fixed \(^{m}\). The saddle point problem (1) is a fundamental formulation in machine learning and optimization and naturally emerges in several applications, including constrained and primal-dual optimization [1; 2], (multi-agent) games , reinforcement learning , and generative adversarial networks [5; 6]. The saddle point problem, which can be interpreted as a particular instance of variational inequalities and monotone inclusion problems , has a rich history dating back to . We often solve (1) using iterative, first-order methods due to their simplicity and low per-iteration complexity. Over the past decades, various first-order algorithms have been proposed and analyzed for different settings [1; 8; 9; 10; 11; 12; 13; 14; 15]. Under the assumption that the gradient of \(f\) is Lipschitz, the aforementioned methods converge at a rate of \((1/T)\), where \(T\) is the number of iterations. This rate is optimal for first-order methods [16; 10; 17].

Recently, there has been a surge of interest in higher-order methods for solving (1) [18; 19; 20; 21; 22; 23], mirroring the trend in convex minimization literature [24; 25; 26; 27; 28]. In general, these methods exploit higher-order derivatives of \(f\) to achieve faster convergence rates. From a practical viewpoint, any methodinvolving third and higher-order derivatives is essentially a _conceptual_ framework; it is unknown how to efficiently solve auxiliary problems involving higher-order derivatives, making it virtually impossible to efficiently implement methods beyond second-order . Therefore, we focus on second-order methods and review the literature accordingly.

The existing literature on second-order algorithms for minimax optimization, capable of achieving the optimal convergence rate of \((1/T^{1.5})\), falls into two categories. The first group requires solving a linear system of equations (or matrix inversion) for their updates but needs a "line search" scheme to select the step size properly. This includes methods such as the Newton proximal extragradient method [18; 19], second-order extensions of the mirror-prox algorithm , and the second-order optimistic method . These methods impose a cyclic and implicit relationship between the step size and the next iterate, necessitating line search mechanisms to compute a valid selection that meets the specified conditions.

The second group, which includes [22; 23], does not require a line search scheme and bypasses the implicit definitions and search subroutines. They follow the template of the cubic regularized Newton method  for convex minimization and solve an analogous "cubic variational inequality sub-problem" per iteration. Despite having explicit parameter definitions, these methods require specialized sub-solvers to obtain approximate solutions to the auxiliary problem, increasing the per-iteration complexity. Moreover, both groups of algorithms rely vitally on the precise knowledge of the objective's Hessian Lipschitz constant.

While the above frameworks achieve the optimal iteration complexity for second-order methods, their requirement for performing a line search or solving a cubic sub-problem limits their applicability. Recently, the authors in  proposed a method with optimal iteration complexity that requires neither the line search nor the solution of an auxiliary sub-problem. In each iteration, they compute a "candidate" next point \(_{t}\) from the base point \(_{t-1}\). However, unless the step size satisfies a "large step condition", which requires the exact knowledge of the Hessian's Lipschitz constant, the base point remains the same for the next iteration, slowing down the convergence in practice. Therefore, it remains an open problem to design a simple, efficient, and optimal second-order method without the need for line search, auxiliary sub-problems, and the knowledge of the Hessian's Lipschitz constant.

**Our Contributions.** Motivated by the aforementioned shortcomings in the literature, our proposed framework completely eliminates the need for line search and backtracking by providing a closed-form, explicit, simple iterate recursion with a data-adaptive step size that adjusts according to local information. In doing so, we develop a parameter-free method that does not require any problem parameters, such as the Lipschitz constant of the Hessian. The key to our simple, parameter-free algorithm is a careful combination of the second-order optimistic algorithm and adaptive regularization of the second-order update. We summarize the highlights of our work as follows:

1. We first present an adaptive second-order optimistic method that achieves the optimal rate of \((1/T^{1.5})\) without requiring any form of line search, assuming the Hessian is Lipschitz and its associated constant is known. We introduce a recursive, adaptive update rule for the step size as a function of the gradient and the Hessian at the current and previous iterations. Our step size satisfies a specific error condition, ensuring sufficient progress while growing at a favorable rate to establish optimal convergence rates.
2. Under the additional, mild assumption that the gradient is Lipschitz, we propose a _parameter-free_ version with the same optimal rates which _adaptively_ adjusts the regularization factor by means of a local curvature estimator. This method is completely oblivious to any problem-dependent parameter including Lipschitz constant(s) and the initialization. Importantly, we achieve this parameter-free guarantee without artificially imposing bounded iterates, which is a common yet restrictive assumption in the study of adaptive methods in minimization [31; 32; 33] and min-max [34; 35] literature.

## 2 Preliminaries

An optimal solution of (1) denoted by \((^{*},^{*})\) is called a _saddle point_ of \(f\), as it satisfies the property \(f(^{*},) f(^{*},^{*}) f( ,^{*})\) for any \(^{m}\), \(^{n}\). Given this notion of optimality, one can measure the suboptimality of any \((,)\) using the primal-dual gap, i.e., \((,):=_{}^{n}}f(,})-_{}^{n}}f( },)\). However, it could be vacuous if not restricted to a bounded region. For instance, when \(f(,)=,\), this measure is always \((,)=+\), except at the saddle point \((0,0)\). To remedy this issue, we consider the restricted primal-dual gap function:

\[_{}(,):=_ {}}f(,})-_{ }}f(},),\] (Gap)

where \(^{m}\) and \(^{n}\) are two compact sets containing the optimal solutions of problem (1). The restricted gap function is a valid merit function (see [1; 11]), and has been used as a measure of suboptimality for min-max optimization . Next, we state our assumptions on Problem (1).

**Assumption 2.1**.: _The objective \(f\) is convex-concave, i.e., \(f(,)\) is convex for any fixed \(^{n}\) and \(f(,)\) is concave for any fixed \(^{m}\)._

**Assumption 2.2**.: _The Hessian of \(f\) is \(L_{2}\)-Lipschitz, i.e., \(\|^{2}f(_{1},_{1})-^{2}f(_{2}, _{2})\| L_{2}\|(_{1}-_{2},_{1}- _{2})\|\) for any \((_{1},_{1}),(_{2},_{2})^ {m}^{n}\)._

Assumptions 2.1 and 2.2 are standard in the study of second-order methods in min-max optimization and constitute our core assumption set. That said, _only_ for the parameter-free version of our proposed algorithm, we will require the additional condition that the gradient of \(f\) is \(L_{1}\)-Lipschitz.

**Assumption 2.3**.: _The gradient of \(f\) is \(L_{1}\)-Lipschitz, i.e., \(\| f(_{1},_{1})- f(_{2},_ {2})\| L_{1}\|(_{1}-_{2},_{1}-_{2})\|\) for any \((_{1},_{1}),(_{2},_{2})^ {m}^{n}\)._

To simplify the notation, we define the concatenated vector of variables as \(=(,)^{m}^{n}\), and define the operator \(:^{m+n}^{m+n}\) at \(=(,)\) as

\[()=[_{}f(,);- _{}f(,)].\] (2)

Under Assumption 2.1, the operator \(\) is _monotone_, i.e., \((_{1})-(_{2}),_{1}- _{2} 0\) for any \(_{1},_{2}^{m}^{n}\). Moreover, Assumption 2.2 implies that the Jacobian of \(\), denoted by \(^{}\), is \(L_{2}\)-Lipschitz, i.e., for any \(_{1},_{2}^{m}^{n}\) we have \(\|^{}(_{1})-^{}(_{2})\|_{ }}{2}\|_{1}-_{2}\|\). This is referred to as _second-order_ smoothness [20; 21]. Similarly, Assumption 2.3 implies that the operator \(\) itself is \(L_{1}\)-Lipschitz, i.e., \(\|(_{1})-(_{2})\| L_{1}\|_{1}-_{2}\|\) for any \(_{1},_{2}^{m}^{n}\).

Finally, the following classic lemma plays a key role in our convergence analysis, as it provides an upper bound on the restricted primal-dual gap at the averaged iterate. Proof can be found in .

**Lemma 2.1**.: _Suppose Assumption 2.1 holds. Consider \(_{1},,_{T} 0\) with \(_{t=1}^{T}_{t}=1\) and \(_{1}\!=\!(_{1},_{1}),,_{T}\!=\!( _{T},_{T})^{m}\!\!^{n}\). Define the average iterates as \(}_{T}=_{t=1}^{T}_{t}_{t}\) and \(}_{T}=_{t=1}^{T}_{t}_{t}\). Then, \(f(}_{T},)-f(,}_{T}) _{t=1}^{T}_{t}(_{t}),_{t}- \) for any \((,)^{m}^{n}\)._

For simplicity and ease of delivery, our algorithm and analysis are based on the operator representation of Problem (1). By means of Lemma 2.1, our derivations with respect to the operator \(\) imply convergence in terms of the (restricted) primal-dual (Gap) function.

## 3 Background on optimistic methods

At its core, our algorithm is a second-order variant of the optimistic scheme for solving min-max problems [12; 14; 15; 21]. As discussed in [36; 37], the optimistic framework can be considered as an approximation of the proximal point method (PPM) [38; 39], which is given by \(_{t+1}=_{t}-_{t}(_{t+1})\). To highlight this connection, note that PPM is an implicit method since the operator \(\) is evaluated at the _next_ iterate \(_{t+1}\). The first-order optimistic method approximates PPM by a careful combination of gradients in two consecutive iterates. The second-order variant , however, jointly uses first and second-order information, which we describe next. Its key idea is to approximate the "implicit gradient" \((_{t+1})\) in PPM by its linear approximation \((_{t})+^{}(_{t})(- _{t})\) around the current point \(_{t}\), and to correct this "prediction" with the error associated with the previous iteration. Specifically, the correction term, denoted by \(_{t}:=(_{t})-(_{t-1})- ^{}(_{t-1})(_{t}-_{t-1})\), is the difference between \((_{t})\) and its prediction at \(t-1\). To express in a formal way,

\[_{t}(_{t+1})[( _{t})+^{}(_{t})(_{t+1}\!-\! _{t})]}_{}+[( _{t})\!-\!(_{t-1})\!-\!^{}( _{t-1})(_{t}\!-\!_{t-1})]}_{}.\] (3)

The rationale behind the optimism is that if the prediction errors in two consecutive rounds do not vary much, i.e., \(_{t}_{t+1}_{t-1}_{t}\), then the correction term should help reduce the approximation error and thus lead to a faster convergence rate. Replacing \(_{t}(_{t+1})\) by its approximation in (3) and rearranging the terms leads to the update rule of the second-order optimistic method:

\[_{t+1}=_{t}-(+_{t}^{}( _{t}))^{-1}(_{t}(_{t})+_{t-1} _{t}).\] (4)

The key challenge is to control the discrepancy between the second-order optimistic method and PPM. This is equivalent to managing the deviation between the updates of the second-order optimistic method and the PPM update. We achieve this by checking an additional condition denoted by

\[_{t}\|_{t+1}\|:=_{t}\|(_{t+1})-(_{t})-^{}(_{t})(_{t+1}- _{t})\|\|_{t+1}-_{t}\|,\] (5)

where \((0,0.5)\). Note that if the prediction term perfectly predicts the prox step, we recover the PPM update and the condition holds with \(=0\). For the standard second-order optimistic algorithm in (4), we need to select \( 0.5\). The condition in (5) emerges solely from the convergence analysis.

While the above method successfully achieves the optimal complexity of \((1/T^{1.5})\), there remains a major challenge in selecting \(_{t}\). A naive choice guided by the condition in (5) results in an _implicit_ parameter update. Specifically, note that the error condition in (5) involves both \(_{t}\) and the next iterate \(_{t+1}\), but \(_{t+1}\) is computed only _after_ the step size \(_{t}\) is determined. Consequently, we can test whether the condition in (5) is satisfied only after selecting the step size \(_{t}\). The authors in  tackled this challenge with a direct approach and proposed a "line search scheme", where \(_{k}\) is backtracked until (5) is satisfied. While their line search scheme requires only a constant number of backtracking steps on average, it is desirable to design simpler line search-free algorithms for practical and efficiency purposes.

## 4 Proposed algorithms

As discussed, the current theory of second-order optimistic methods requires line search due to the implicit structure of (5). In this section, we address this issue and present a class of second-order methods that, without any line search scheme, are capable of achieving the optimal complexity for convex-concave min-max setting. To begin, we first present a general version of the second-order optimistic method by introducing an additional scaling parameter \(_{t}\). Specifically, the update is

\[_{t+1}=_{t}-(_{t}+_{t}^{}(_{t}))^{-1}(_{t}(_{t} )+_{t-1}_{t}).\] (6)

when \(_{t}=1\), we recover the update in (4). Crucially, the regularization factor \(_{t}\) enables flexibility in choosing the parameters of our proposed algorithm and plays a vital role in achieving the parameter-free design, which does not need the knowledge of the Lipschitz constant. What remains to be shown is the update rule for \(_{t}\) and \(_{t}\). In the following sections, we present two adaptive update policies for these parameters. The first policy is line-search-free, explicit, and only requires knowledge of \(L_{2}\). The second approach does not require knowledge of \(L_{2}\) and is completely parameter-free, but it requires an additional assumption that F is \(L_{1}\)-Lipschitz, which is satisfied when \( f\) is \(L_{1}\)-Lipschitz (see Assumption 2.3).

**Adaptive and line search-free second-order optimistic method (Option I).** In our first proposed method, we set the parameter \(_{t}\) to be a fixed value \(\) and update the parameter \(_{t}\) using the policy:

\[_{t}=}{_{t-1}L_{2}\|_{t}\|+L_{2}\|_{t}\|)^{2}+8^{2}L_{2}\|( _{t})\|}}.\] (7)As we observe, \(_{t}\) only depends on the information that is available at time \(t\), including the error term norm \(\|_{t}\|\) and the operator norm \(\|(_{t})\|\). Hence, the update is explicit and does not require any form of backtracking or line search. That said, it requires the knowledge of the Lipschitz constant of the Jacobian \(^{}\) denoted by \(L_{2}\). We should note that \(>0\) in this case is a free parameter, and we set it as \(=L_{2}\) to be consistent with the parameter-free method in the next section. The update for \(_{t}\) might seem counter-intuitive at first glance, but as we elaborate upon its derivation in the next section, it is fully justified by optimizing the upper bounds corresponding to the optimistic method.

**Parameter-free adaptive second-order optimistic method (Option II).** While the expression for step size \(_{t}\) in (7) is explicit and adaptive to the optimization process, however, it depends on the Hessian's Lipschitz constant \(L_{2}\). Next, we discuss how to make the method parameter-free, so that the algorithm parameters \(_{t}\) and \(_{t}\) do not depend on the smoothness constant(s) or any problem-dependent parameters. Specifically, we propose the following update for \(_{t}\) and \(_{t}\):

\[_{t}=}{_{t-1}\|_{t}\|+^{2}\|_{t}\|^{2}+4_{t}\|(_{t})} },_{t}=\{_{t-1},\,_{t}\|}{\| _{t-1}-_{t}\|^{2}}\}.\] (8)

These updates are explicit, adaptive, and parameter-free. In the next section, we justify these updates.

## 5 Main ideas behind the suggested updates

Before we delve into the convergence theorems, we proceed by explaining the particular choice of algorithm parameters and the derivation process behind their design, through which we will motivate how we eliminate the need for iterative line search.

**Rationale behind the update of Option I.** First, we motivate the design process for updating \(_{t}\) and \(\) in Option **(I)**, guided by the convergence analysis. We illustrate the technical details leading to the parameter choices in Step 4 by introducing a template equality that forms the basis of our analysis.

**Proposition 5.1**.: _Let \(\{_{t}\}_{t=0}^{T+1}\) be generated by Algorithm 1. Define the "approximation error" as \(_{t+1}(_{t+1})-(_ {t})-^{}(_{t})(_{t+1}-_{t})\). Then for any \(^{d}\), we have_

\[_{t=1}^{T}_{t}(_{t+1}), _{t+1}-= _{t=1}^{T}}{2}(\|_{t}- \|^{2}-\|_{t+1}-\|^{2})-_{t=1}^{T} }{2}\|_{t}-_{t+1}\|^{2}\] \[+_{T+1},_{T+1}- }_{(A)}+_{t=1}^{T}_ {t},_{t}-_{t+1}}_{(B)}.\] (9)

As we observe in the above bound, if we set \(_{t}\) to be constant (\(_{t}=\)), then the first summation term on the right-hand side will telescope. On top of that, if we apply the Cauchy-Schwarz inequality and Young's inequality on terms (A) and (B) and regroup the matching expressions, we would obtain \(_{t=1}^{T}_{t}(_{t+1}),_{t+1}- \|_{1}-\|^{2}-\|_{T+1}-\|^{2}+_{t=1}^{T}(^{2}}{}\|_{t+1}\|^{2}-\|_{t}- _{t+1}\|^{2})\). We make two remarks regarding the inequality above. (_i_) By using Lemma 2.1 with \(_{t}=}{_{t=1}^{T}_{t}}\) for \(1 t T\), the left-hand side can be lower bounded by \((_{t=1}^{T}_{t})(f(}_{T+1},)-f( ,}_{T+1}))\), where the averaged iterate \(}_{T+1}=(}_{T+1},}_{T+1})\) is given by \(}_{T+1}=^{T}_{t}}_{t=1}^{T}_{t} _{t+1}\). (_ii_) If we can show that the summation on the right-hand side is non-positive and divide both sides by \(_{t=1}^{T}_{t}\), we obtain a convergence rate of \((1/_{t=1}^{T}_{t})\) for (Gap) at the averaged iterate.

To obtain the optimal rate of \((1/T^{1.5})\), the analysis guides us to be more conservative with the latter point and ensure that the summation on the right-hand side is strictly negative (see Section 6 for further details). Specifically, we require each error term in the summation to satisfy \(^{2}}{}\|_{t+1}\|^{2}-\| _{t}-_{t+1}\|^{2}-(-^{2}) \|_{t}-_{t+1}\|^{2}\) for a given \((0,)\). Rearranging the expressions we obtain \(_{t}^{2}\|_{t+1}\|^{2}^{2}^{2}\|_{t}- _{t+1}\|^{2}\), and we retrieve an analog of the error condition (5) by simply taking the square root of both sides. A naive approach would be to choose \(_{t}\) small enough to satisfy the condition. However, since our convergence rate is of the form \(_{t=1}^{T}_{t}\), this approach would also slow down the convergence of our algorithm and achieve a sub-optimal rate.

Hence, our goal is to _select the largest possible \(_{t}\) that satisfies the condition in (5)_. Next, we will explain how we come up with an explicit update rule for step size \(_{t}\) that achieves this goal. Our strategy is quite simple; we first rewrite the inequality of interest as

\[\|_{t+1}\|}{\|_{t}-_{t +1}\|} 1.\] (10)

Then, we derive an upper bound for the term on the left-hand side that depends only on quantities available at iteration \(t\). A sufficient condition for (10) would be showing that the upper bound of \(\|_{t+1}\|}{\|_{t}-_{ t+1}\|}\) is less than \(1\). Note that by Assumption 2.2, we can upper bound \(\|_{t+1}\|\) and write \(\|_{t+1}\|}{\|_{t}- _{t+1}\|}L_{2}\|_{t}-_{t+1}\|^{2}}{2 \|_{t}-_{t+1}\|}=L_{2}\| _{t}-_{t+1}\|}{2}\). As the final component, we derive an upper bound for \(\|_{t}-_{t+1}\|\) that only depends on the information available at time \(t\). In the next lemma, which follows from the update rule and the fact that \(\) is monotone, we accomplish this goal. The proof is in Appendix A.2.

**Lemma 5.2**.: _Suppose that Assumption 2.1 holds. Then, the update rule in Step 5 in Algorithm 1 implies \(\|_{t}-_{t+1}\|}_{t}\|(_{t})\|+}_{t-1}\|_{t}\|\)._

We combine Lemma 5.2 for \(_{t}=\) with the previous expression and rearrange the terms to obtain

\[L_{2}\|_{t}-_{t+1}\|}{2} L_{2}(_{t}\|(_{t})\|+_{t-1}\|_{t}\|)}{2^{2}}.\] (11)

Hence, we obtained an _explicit_ upper bound for the left hand side of (10) that only depends on terms at iteration \(t\) or before. Therefore, a sufficient condition for satisfying (10) is ensuring that \(L_{2}(_{t}\|(_{t})\|+_{t-1}\| _{t}\|)}{2^{2}} 1\). Since we aim for the largest possible choice of \(_{t}\), we intend to satisfy this condition with equality. After rearranging, we end up with the following expression:

\[_{t}(_{t}\|(_{t})\|+_{t-1}\|_{t}\|) =}{L_{2}}.\] (12)

The expression in (12) is a quadratic equation in \(_{t}\) and it is an _explicit_ expression where all the terms are available at the beginning of iteration \(t\). Solving for \(_{t}\) leads to the expression in (7).

**Rationale behind the update of Option II.** Choosing the regularization parameter \(_{t}\) properly is the key piece of the puzzle. First, recall the error term \(_{t=1}^{T}}{2}(\|_{t}-\|^{2}-\| _{t+1}-\|^{2})\) from Proposition 5.1. When \(_{t}\) is time-varying, this summation no longer telescopes. A standard technique in adaptive gradient methods to resolve this issue (see, e.g., [40, Theorem 2.13]) involves selecting \(_{t}\) to be monotonically non-decreasing and showing that the iterates \(\{_{t}\}_{t 0}\) are bounded. We follow this approach, and in the next proposition, we investigate the possibility of ensuring that the distance of the iterates to the optimal solution, \(\|_{t}-^{*}\|^{2}\), remains bounded.

**Proposition 5.3**.: _Let \(\{_{t}\}_{t=0}^{T+1}\) be generated by Algorithm 1 and \(^{*}^{m}^{n}\) be a solution to Problem (1). Then,_

\[\|_{T+1}-^{*}\|^{2} \|_{1}-^{*}\|^{2}-_{t=1}^{ T}\|_{t}-_{t+1}\|^{2}+_{t=1}^{T}}{_{t}}_{t},_{t}-_{t+1}}_{(A)}\] (13) \[+}{_{T}}_{ T+1},_{T+1}-^{*}}_{(B)}+_{t=2}^{T}}-}_{t-1} _{t},_{t}-^{*}}_{(C)}.\]

To derive the boundedness of \(\{_{t}\}_{t 1}\) from (13), all error terms (A), (B), and (C) in (13) should be upper bounded. As detailed in the proof of Lemma C.1, we can apply Assumption 2.3 to control the second term (B) and it does not impose restrictions on our choice of \(_{t}\) and \(_{t}\). To control term (A) and (C), we apply Cauchy-Schwarz and Young's inequalities individually; we get \(}{_{t}}_{t},_{t}-_{t+1}^{2}}{_{t}^{2}}\|_{t}\|^{ 2}+\|_{t}-_{t+1}\|^{2}\), and also \((}-})_{t-1}_{t}, _{t}-^{*}^{2}}{_{t}^{2}} \|_{t}\|^{2}+(}{_{t-1}}-1)^{2}\| _{t}-^{*}\|^{2}\), respectively. Combining the new terms obtained from (A) and (C) and summing from \(t=1\) to \(T\), we obtain \(_{t=1}^{T}^{2}}{_{t+1}^{2}}\|_{t+1}\|^{2}+ _{t=1}^{T}\|_{t}-_{t+1}\|^{2}+_{t=1}^{ T}(}{_{t-1}}-1)^{2}\|_{t}-^{*}\|^{2}\). The last term will remain in the recursive formula, hence manageable. On the other hand, we need to make sure that the first two terms can be canceled out by the negative terms we have in (13). Thus, we need to enforce the condition \(^{2}}{_{t+1}^{2}}\|_{t+1}\|^{2}+\| _{t}-_{t+1}\|^{2}-\|_{t}-_{ t+1}\|^{2}-(-2^{2})\|_{t}-_{t+1 }\|^{2}\), where \((0,})\). This condition can be simplified as

\[^{2}\|_{t+1}\|^{2}}{_{t+1}^{2}\| _{t}-_{t+1}\|^{2}}^{2} \|_{t+1}\|}{_{t+1}\|_{t}- _{t+1}\|} 1.\] (14)

Comparing with (11), we observe that the difference is that \(\) is replaced by \(_{t+1}\). Thus, we propose to follow a similar update rule for \(_{t}\) as in (7). However, recall that \(L_{2}\) appears in the update rule of (7), yet we do not have the knowledge of \(L_{2}\) in this setting. Hence, we assume that we can compute a sequence of Lipschitz constant estimates \(\{_{2}^{(t)}\}\) at each iteration \(t\). The construction of such Lipschitz estimates will be evident later from our analysis. Specifically, in the update rule of (8), we will replace \(\) by \(_{t}\) and replace \(L_{2}\) by \(_{2}^{(t)}\), leading to the expression

\[_{t}=^{2}}{_{t-1}\|_{t }\|+_{2}^{(t)}\|_{t}\|)^{2}+8_{ t}^{2}_{2}^{(t)}\|(_{t})\|}}.\] (15)

By relying on Lemma 5.2 and following similar arguments, we can show that

\[\|_{t+1}\|}{_{t+1}\|_{t}-_{t+1}\|}}{_{t+1}}^{(t+1)} }{_{2}^{(t)}},\] (16)

where \(L_{2}^{(t+1)}=_{t+1}\|}{\|_{t+1}-_{t} \|^{2}}\) can be regarded as a "local" estimate of the Hessian's Lipschitz constant. Thus, to satisfy the condition in (14), the natural strategy would be to set \(_{t}=_{2}^{(t)}\) and ensure that \(L_{2}^{(t+1)}_{2}^{(t+1)}=_{t+1}\). Finally, recall that the sequence \(\{_{t}\}\) should be monotonically non-decreasing, i.e., \(_{t+1}_{t}\) for \(t[T]\), leading to our update rule for \(_{t+1}\) as shown in (8). This way, the right-hand side of (16) becomes \(^{(t+1)}}{_{2}^{(t+1)}} 1\) and thus the error condition (14) is satisfied. By replacing \(_{2}^{(t)}\) with \(_{t}\) and simplifying the expression, we arrive at the update rule for \(_{t}\) in (8).

## 6 Convergence analysis

In this section, we present our convergence analysis for different variants of Algorithm 1. We first present the final convergence result for Option **(I)** of our proposed method. Besides the convergence bound in terms of (Gap), we provide a complementary convergence bound with respect to the norm of the operator, evaluated at the "best" iterate.

**Theorem 6.1**.: _Suppose Assumptions 2.1 and 2.2 hold and let \(\{_{t}\}_{t=0}^{T+1}\) be generated by Algorithm 1, where \(_{t}=L_{2}\) (Option **(I)**) and \(=0.25\). Then \(\|_{t}-^{*}\|}\|_{1}- ^{*}\|\) for all \(t 1\). Moreover,_

\[_{}(}_{T+1}) }\| _{1}-\|^{2}\ \|(_{1})\|+36.25L_{2}^{2}\| _{0}-^{*}\|^{2}}}{T^{1.5}},\] (17) \[_{t\{2,,T+1\}}\|(_{t})\| _{1}-^{*}\|\| (_{1})\|+290L_{2}^{2}\|_{1}-^{*}\|^{ 2}}}{T}.\] (18)

Theorem 6.1 guarantees that the iterates \(\{_{t}\}_{t 0}\) always stay in a compact set \(\{^{d}:\|-^{*}\|}\|_{1}-^{*}\|\}\). Moreover, it demonstrates that the gap function at the weighted averaged iterate \(}_{T+1}\) converges at the rate of \((T^{-1.5})\), which is optimal and matches the lower bound in . Finally, the convergence rate in (18) in terms of the operator norm also matches the state-of-the-art rate achieved by second-order methods , [41, Theorem 3.7], [30, Theorem 4.9 (a)].

Proof Sketch of Theorem 6.1.: We begin with the convergence with respect to (Gap) in (17). The proof consists of the following steps.

**Step 1:** As mentioned in Section 5, the choice of \(_{t}\) in (7) guarantees that \(_{t}\|_{t+1}\|\|_{t}-_{t+1}\|\). This allows us to prove that the right-hand side of (9) is bounded by \(\|_{1}-\|^{2}=}{2}\| _{1}-\|^{2}\). Hence, using Lemma 2.1, we have \(_{}(}_{T+1})_{ }}{2}\|_{1}- \|^{2}(_{t=1}^{T}_{t})^{-1}\).

**Step 2:** Next, our goal is to lower bound \(_{t=1}^{T}_{t}\). By using the expression of \(_{t}\) in (7) we can show a lower bound on \(_{t}\) in terms of \(\|_{t}\|\) and \(\|(_{t})\|\) as (formalized in Lemma B.2)

\[_{t} 2(_{t-1}^{2}\|_{t}\|^{2}+2 \|(_{t})\|)^{-}( \|_{t}-_{t-1}\|^{2}+\| (_{t})\|)^{-}.\] (19)

Additionally, we need to establish an upper bound on \(\|(_{t})\|\). By leveraging the update rule in (4) and Assumption 2.2, we show that \(\|(_{t})\|}\| _{t}-_{t-1}\|+}\| _{t-1}-_{t-2}\|\) for \(t 2\) in Lemma B.2. Thus, \(\|(_{t})\|\) can be bounded in terms of \(\|_{t}-_{t-1}\|\), \(\|_{t-1}-_{t-2}\|\) and \(_{t-1}\). In addition, by using Proposition 5.3, we can establish that \(_{t=1}^{T}\|_{t+1}-_{t}\|^{2}=(\|_{1}-^{*}\|^{2})\).

**Step 3:** By combining the ingredients above, with some algebraic manipulations we can show that \(_{t=1}^{T}^{2}}=(\|_{1}- ^{*}\|^{2}+\|(_{1})\|)\) (check Lemma B.3). Hence, using Holder's inequality, it holds that \(_{t=0}^{T}_{t} T^{1.5}(_{t=0}^{T}(1/_{t}^{2}))^{-1/2}\). This finishes the proof for (17).

Finally, we prove the convergence rate with respect to the operator norm in (18). Essentially, we reuse the results we have established previously. By using the upper bounds on \(\|()\|\) and \(_{t=1}^{T}}\) from Lemmas B.2 and B.3 respectively, and combining them with the bound \(_{t=1}^{T}\|_{t+1}-_{t}\|^{2}=(\|_{1}-^{*}\|^{2})\) (see Proposition B.1), we show that \(_{t=2}^{T+1}\|(_{t})\|=(\| _{1}-^{*}\|^{T}^{2}}} )=(\|_{1}-^{*}\|\| _{1}-^{*}\|^{2}+\|(_{1})\|} ).\) Then the bound follows from the simple fact that \(_{\{2,,T+1\}}\|(_{t})\|_{t=2} ^{T+1}\|(_{t})\|\). 

Next, we proceed to present the convergence results for Option **(II)** of our proposed method that is parameter-free. Note that if the initial scaling parameter \(_{1}\) overestimates the Lipschitz constant \(L_{2}\), we have \(_{t}=_{1}\) for all \(t 1\). This is because we have \(_{t}\|}{\|_{1}-_{t-1}\|} L_{2}\) by Assumption 2.2, and thus in this case the maximum in (8) will be always \(_{t-1}\). As a result, \(_{t}\) stays constant and the convergence analysis for Option **(I)** also applies here. Given this argument, in the following, we focus on the case where the initial scaling parameter \(_{1}\) underestimates \(L_{2}\), i.e., \(_{1}<L_{2}\). Moreover, it is rather trivial to establish that \(_{t}<L_{2}\) for all \(t 1\) using induction.

**Theorem 6.2**.: _Suppose Assumptions 2.1,2.2, and 2.3 hold and let \(\{_{t}\}_{t=0}^{T+1}\) be generated by Algorithm 1, where \(_{t}\) is given by (8) (Option **(II)**) and \(=0.25\). Assume that \(_{1}<L_{2}\). Then we have \(\|_{t}-^{*}\| D\) for all \(t 1\), where \(D^{2}=^{2}}{_{1}^{2}}+^{2}}{_{1}^{2}}\| _{1}-^{*}\|^{2}\). Moreover, it holds that_

\[_{}(}_{T+1}) (_{} \|-^{*}\|^{2}+D^{2})\,(_{1})\|}{_{1}}+145\|_{1}-^{*} \|^{2}}}{T^{1.5}},\] (20) \[_{t\{2,,T+1\}}\|(_{t})\| D(_{1})\|}{ _{1}}+72.5\|_{1}-^{*}\|^{2}}}{T}.\] (21)

Under the additional assumption of a Lipschitz operator, Theorem 6.2 guarantees that the iterates stay bounded. This is the main technical difficulty in the analysis, as most previous works on adaptive methods assume a compact set. On the contrary, we prove that iterates remain bounded within a set of diameter \(D=(}{_{1}}+}{}\|_{1}- ^{*}\|)\). Compared to Option **(I)** in Theorem 6.1, the diameter increases by a factor of \(}{_{1}}\), i.e., the ratio between \(L_{2}\) and our initial parameter \(_{1}\). Moreover, Theorem 6.2 guarantees the same convergence rate of \((T^{-1.5})\). In terms of constants, compared to Theorem 6.1, the difference is no more than \((}{_{1}})^{2.5}\). Thus, with a reasonable underestimate of the Lipschitz constant, \(_{1}=cL_{2}\) for some absolute constant \(c<1\), the bound worsens only by a constant factor.

_Proof Sketch of Theorem 6.2_.: We begin with the convergence with respect to (Gap) in (20). The proof consists of the following three steps.

**Step 1:** By using Proposition 5.3, we first establish the following recursive inequality:

\[\|_{t+1}-^{}\|^{2}^{2}}{_{t}^{2}}+2 \|_{1}-^{}\|^{2}+_{s=2}^{t} }{_{s-1}}-1^{2}\|_{s}-^{}\|^{2}- {2}_{s=1}^{t}\|_{s+1}-_{s}\|^{2},\] (22)

as shown in Lemma C.1 in the Appendix. Note that this upper bound for \(\|_{t+1}-^{}\|^{2}\) on the right-hand side depends on \(\|_{s}-^{}\|^{2}\) for all \(s t\). By analyzing this recursive relation, we obtain \(\|_{t+1}-^{}\| D\) and \(_{s=0}^{t}\|_{s}-_{s+1}\|^{2} 2D^{2}\) for all \(t 1\), where \(D^{2}\!=\!^{2}}{_{1}^{4}}+^{2}}{_{1}^{ 2}}\|_{1}-^{}\|^{2}\); see Lemma C.2 for details.

**Step 2:** After showing a uniform upper bound on \(\|_{t+1}-^{}\|\), Proposition C.3 establishes the adaptive convergence bound \(_{}(}_{T+1}) L_{2} (_{}\|- ^{}\|^{2}+D^{2})_{t=0}^{T}_{t}^{-1}\).

**Step 3:** Following similar arguments as in the proof of Theorem 6.1, we can show \(_{t=1}^{T}^{2}}=(D^{2}+}\| (_{1})\|)\) (check Lemma C.5). By applying the Holder's inequality \(_{t=0}^{T}_{t} T^{1.5}(_{t=0}^{T}(1/_{t}^{2}))^{-1/2}\), we obtain the final convergence rate.

Finally, along the same lines as Theorem 6.1, we can show that \(_{t=2}^{T+1}}\|(_{t})\|=(D^{T}^{2}}})= (D+}\|(_{1})\|})\). Since \(_{t} L_{2}\) and \(_{\{2,,T+1\}}\|(_{t})\|_{t=2} ^{T+1}\|(_{t})\|\), we obtain the result in (21). 

_Remark 6.1_.: Our results can be extended to the more general problem of monotone inclusion with proper modification to the algorithm. We chose to focus on the unconstrained min-max problem for ease of presentation, so that we can better highlight the key novelties and make it accessible to a broader audience. In future work, we plan to extend our results for the monotone inclusion problem.

_Remark 6.2_.: Our proposed algorithm with Option **(II)** achieves the same rate as Option **(I)** but does not require prior knowledge of the Hessian's Lipschitz constant, making it fully parameter-free. However, since it requires an additional assumption on Lipschitz gradients (Assumption 2.3), the existing lower bound  does not directly apply to certify its optimality. That said, we hypothesize that the \(L_{1}\)-Lipschitz gradient assumption should not improve the lower bound based on the existing evidence from the convex minimization setting. Specifically,  proves that for convex minimization with \(L_{1}\)-Lipschitz gradient and \(L_{2}\)-Lipschitz Hessian, the optimal rate is \(\!(\!\{D^{2}}{T^{2}},D^{3}}{T^{ 3.5}}\})\), where \(D\) is the initial distance. When \(T\) is sufficiently large, the second term will become the smaller one, showing that the Lipschitz gradient assumption does not improve the optimal rate. While their construction does not imply an analogous rate for our setting in Theorem 6.2, we conjecture that the Lipschitz gradient assumption should not improve the lower bound of \((1/T^{1.5})\).

## 7 Numerical experiments

In this section, we present numerical results for implementing both variants of our algorithm: the version with \(_{t}=L_{2}\) (Adaptive SOM I) and the parameter-free variant (Adaptive SOM II). We also compare these with the homotopy inexact proximal-Newton extragradient (HIPNEX) method  and the optimistic second-order method with line search (Optimal SOM) . To assess convergence toward the solution \(^{}\), we plot \(\|(_{T})\|^{2}/\|(_{0})\|^{2}\). For complete details, check Appendix D.

**Synthetic min-max problem:** We first consider the min-max problem in [21; 30], given by

\[_{^{n}}_{^{n}}f(,)=(-)^{}+(L_{2}/6)\| \|^{3},\]

which satisfies Assumptions 2.1 and 2.2. Let \(=(,)^{d}\), with \(d=2n\), and recall that \(()\) is defined in (2). Following the setup in , we generate the matrix \(^{d d}\) to ensure a condition number of 20. The vector \(^{d}\) is generated randomly according to \((0,)\). We report results across various values of \(L_{2}\) and problem dimension \(d\) (for complete details, see Appendix D) and present a representative subset here. Focusing on large dimensions highlights computational efficiency, as shown in Fig. 1, where our line-search-free methods outperform both the optimal SOM and HIPNEX in runtime. The performance gap with the optimal SOM widens as the dimension grows: line search demands more steps, especially with larger \(L_{2}\), with each step becoming increasingly costly due to the Hessian computation and inversion in high dimensions.

**AUC maximization problem:** We consider a second problem where we maximize the Area Under the Receiver Operating Characteristic Curve (AUC), where we want to find a classifier \(^{d}\) with a small error and a large AUC. This problem could be formulated as a min-max problem as in :

\[_{=(,u,v)}_{y} _{i=1}^{N}(,_ {i}-u)^{2}[b_{i}=1]+_{ i=1}^{N}(,_{i}-v)^{2}[b_{i}=-1 ]\] \[+_{i=1}^{N},_{i}(p[b_{i}=-1]-(1-p)[b_{i}=1 ])+\|\|^{3}-p(1-p)y^{2},\]

where \(u,v\) are auxiliary variables, \(\{(_{i},b_{i})\}_{i=1}^{N}\) denote the (data, label) pairs (\(_{i}^{d}\) and \(b_{i}\{1,-1\}\)), \([]\) is the indicator function, and \(p\) is the ratio of positive labels. Similar to the observations above, Fig. 2 demonstrates that both of our methods outperform the optimal SOM and HIPNEX in terms of runtime, particularly in the early stages of the execution.

## 8 Conclusion and limitations

We proposed the first parameter-free and line-search-free second-order method for solving convex-concave min-max optimization problems. Our methods eliminate the need for line-search and backtracking mechanisms by identifying a sufficient condition on the approximation error and designing a data-adaptive update rule for step size \(_{t}\) that satisfies this condition. Notably, distinct from conventional approaches, our adaptive step size rule can be non-monotonic. Additionally, we removed the requirement to know the Lipschitz constant of the Hessian by appropriately regularizing the Hessian matrix with an adaptive scaling parameter \(_{t}\).

The convergence rate for our fully parameter-free method was established under the additional assumption that the gradient is Lipschitz continuous. This assumption helps control the prediction error without imposing artificial boundedness conditions. Our method ensures that the generated sequence remains bounded even without access to any Lipschitz parameters. Extending these parameter-free guarantees without the Lipschitz gradient assumption remains an open problem worth exploring.

Figure 1: Synthetic min-max problem: Runtimes under large dimension regime with \(L_{2}=10^{4}\).

Figure 2: AUC maximization: Runtimes under large Lipschitz (\(L_{2}\)) regime with dimension \(d=10^{4}\).