# Hierarchical Programmatic Option Framework

Yu-An Lin Chen-Tao Lee Chih-Han Yang Guan-Ting Liu Shao-Hua Sun

National Taiwan University

{b06204039, b06703027, b10902069, f07944014, shaohuas}@ntu.edu.tw

Equal contribution. Correspondence to: Shao-Hua Sun <shaohuas@ntu.edu.tw>

###### Abstract

Deep reinforcement learning aims to learn deep neural network policies to solve large-scale decision-making problems. However, approximating policies using deep neural networks makes it difficult to interpret the learned decision-making process. To address this issue, prior works  proposed to use human-readable programs as policies to increase the interpretability of the decision-making pipeline. Nevertheless, programmatic policies generated by these methods struggle to effectively solve long and repetitive RL tasks and cannot generalize to even longer horizons during testing. To solve these problems, we propose the Hierarchical Programmatic Option framework (HIPO), which aims to solve long and repetitive RL problems with human-readable programs as options (low-level policies). Specifically, we propose a method that retrieves a set of effective, diverse, and compatible programs as options. Then, we learn a high-level policy to effectively reuse these programmatic options to solve reoccurring subtasks. Our proposed framework outperforms programmatic RL and deep RL baselines on various tasks. Ablation studies justify the effectiveness of our proposed search algorithm for retrieving a set of programmatic options.

## 1 Introduction

Deep reinforcement learning (deep RL) has recently achieved tremendous success in various domains, such as controlling robots , playing strategy board games , and mastering video games . However, neural network policies learned by deep RL methods are not human-interpretable, and their black-box nature poses challenges in scrutinizing model decisions and establishing user trust . Moreover, deep RL policies often suffer from overfitting and struggle to generalize to novel scenarios , limiting their applicability in the context of most real-world applications.

To address these issues, programmatic RL frameworks  were proposed to represent policies as programs that detail task-solving procedures in a formal programming language. Particularly, Trivedi et al.  and Liu et al.  synthesize programs from continuous latent spaces, whereas Carvalho et al.  search programs directly from discrete programmatic spaces. Such _program policies_ are human-readable and demonstrate significantly improved zero-shot generalizability from smaller state spaces to larger ones.

Despite encouraging results, prior programmatic RL frameworks are limited to generating concise programs that can only tackle short-horizon tasks. Particularly for long-horizon RL problems like robotic manipulation  and autonomous driving , these tasks consist of reoccurring subtasks and sparse rewards, necessitating a significant number of actions to be fully resolved. Therefore, the RL agent needs to learn a diverse and reusable set of skills to solve such tasks effectively.

Aiming to solve long and repetitive tasks with better policy interpretability, we borrow ideas from the option frameworks [4; 51; 66; 72] and latent-space-based programmatic RL frameworks [46; 74]. With that in mind, we propose the **H**ierarchical **P**rogrammatic **O**ption framework (HIPO), which utilizes a set of programs with diverse skills as options (programmatic options) and learn a high-level policy to determine which programmatic option to be used based on the current state and the current option. By switching between these programmatic options, HIPO can reuse the skills encapsulated in these options to tackle long-horizon tasks with an arbitrary number of repeating subroutines.

Our framework contains three stages. (1) **Constructing a program embedding space**: To establish a program embedding space that smoothly and continuously parameterizes programs with diverse behaviors, we adopt the method proposed by Trivedi et al. . (2) **Retrieving a diverse set of effective and reusable programmatic options**: We introduce a searching algorithm to retrieve a set of programmatic options from the learned program embedding space. Each programmatic option can be executed in the MDP and achieve satisfactory performance; more importantly, these programs are compatible and can be sequentially executed in any order. (3) **Learning a high-level policy**: To alter between a set of programmatic options, the high-level policy represented by neural networks takes the current environment state and the current programmatic option as input to predict the next programmatic option. This high-level policy can be learned using RL algorithms with the goal of maximizing the task return from the MDP.

To evaluate our proposed HIPO framework, we adopt the Karel domain , which characterizes an agent that navigates a grid world and interacts with objects. HIPO outperforms prior programmatic reinforcement learning and deep RL baselines on existing benchmarks [46; 74]. To further evaluate the performance and generalization ability to even longer horizons, we design a new set of tasks consisting of an arbitrary number of subtasks. Our framework shows better generalization in testing environments of different lengths. Ablation studies are also conducted to demonstrate the effectiveness of the proposed programmatic options retrieving process.

## 2 Related work

**Program synthesis.** Program synthesis techniques revolve around program generation to convert given inputs into desired outputs. These methods have demonstrated notable successes across diverse domains such as array and tensor manipulation [5; 22], string transformation [20; 29; 85], generating computer commands  and code [11; 42], graphics and 3D shape modeling [48; 73; 81], and describing agent behaviors [9; 13; 43; 69; 70]. Most program synthesis methods focus on task specifications such as input/output pairs, demonstrations, or language descriptions; in contrast, this work aims to synthesize human-readable programs as options to solve reinforcement learning tasks.

**Programmatic reinforcement learning.** Programmatic reinforcement learning methods [16; 80] explore structured representations for representing RL policies, including decision trees [7; 35], state machines , symbolic expressions [19; 33; 49; 50; 83], and programs [1; 75; 76]. Liu et al. , Medeiros et al. , Moraes and Lelis , Trivedi et al. , and Carvalho et al.  attempted to produce policies described by domain-specific language programs to solve simple RL tasks. We aim to take a step toward addressing complex, long-horizon, repetitive tasks.

**Hierarchical reinforcement learning.** Hierarchical Reinforcement Learning (HRL) frameworks [4; 6; 40; 72; 77] focus on learning and operating across different levels of temporal abstraction, enhancing the efficiency of learning and exploration, particularly in sparse-reward environments. In this work, our proposed HIPO shares the same spirit with HRL frameworks [18; 21; 24; 25; 62; 63] that learn reusable skills as options. Instead of learning uninterpretable options as low-level policies, our framework aims to retrieve reusable and interpretable programs as options.

## 3 Problem formulation

Our goal is to devise a framework that generates a set of programs as options (low-level policies) and integrates them with high-level policies to tackle complex, long-term tasks defined by Markov Decision Processes (MDPs). To this end, we first synthesize a set of task-solving, diverse, compatible programs, then train a high-level policy to iteratively select and execute programs.

**Domain specific language.** This work adopts the domain-specific language (DSL) of the Karel domain [9; 13; 74], as illustrated in Figure 1. This DSL describes the control flows as well as the perception and actions of the Karel agent. Actions including move, turnRight, and putMarker define how the agent can interact with the environment. Perceptions, such as frontIsClear and markerPresent, formulate how the agent observes the environment. Control flows, _e.g._, if, else, while, enable representing divergent and repetitive behaviors. Furthermore, Boolean and logical operators like and, or, and not allow for composing more intricate conditions. This work uses programs structured in this DSL to construct programmatic options.

**Markov Decision Process (MDP).** The tasks considered in this work can be formulated as finite-horizon discounted Markov Decision Processes (MDPs). The performance of HIPO is evaluated based on the execution traces of a series of programs (programmatic options) selected by its high-level policy. The rollout of a program \(\) consists of a \(T\)-step sequence of state-action pairs \(\{(s_{t},a_{t})\}_{t=1,\,,T}\) obtained from a program executor EXEC\(()\) that executes program \(\) to interact with an environment, resulting in the discounted return \(_{t=0}^{T}^{t}(r_{t})\), where \(r_{t}=(s_{t},a_{t})\) denotes the reward function. We aim to maximize the total rewards by executing a series of programs following the high-level policy.

**Hierarchical Programmatic Option Framework.** The proposed hierarchical framework consists of a set of programmatic options \(M=\{m_{k}\}_{k=1,\,,M|}\) as low-level policies and a high-level policy \(f_{}\) that sequentially chooses one option at a time. Each option \(m_{i}\) encapsulates a human-readable program \(_{m_{i}}\) that will be executed if selected by the high-level policy \(f_{}\). On the other hand, the high-level policy \(f_{}(m^{i},s_{T^{i}}^{i})\) outputs the probability distribution over all programmatic options \(M\), given the last selected programmatic option \(m^{i}\) at timestep \(T^{i}\) and the current MDP state \(s_{T^{i}}^{i}\). If the next option \(m^{i+1}\) sampled from the distribution is the termination mode \(m_{}\), the rollout will be terminated. Otherwise, the corresponding programmatic option \(_{m^{i+1}}\) will be executed and generates a sequence of state-action pairs \(\{(s_{t}^{i+1},a_{t}^{i+1})\}_{t=1,\,,\,T^{i+1}}\) before the high-level policy \(f_{}\) selects the next programmatic option \(m^{i+2}\).

## 4 Approach

We design a three-stage framework to search programmatic options and train a high-level policy represented by neural networks. The main goal is to maximize the return given a task described by an MDP. Firstly, as introduced in Section 4.1, we construct a program embedding space parameterizing programs smoothly and continuously with diverse behaviors. Then, Section 4.2 presents a method that retrieves a set of effective, diverse, and compatible programmatic options. Given retrieved programmatic options, Section 4.3 describes learning the high-level policy to determine probability distributions for options sampling. An overview of the proposed framework is illustrated in Figure 2.

### Constructing program embedding space

We follow the approach and the program dataset specified in Trivedi et al.  to learn a program embedding space that smoothly and continuously parameterizes programs with diverse behaviors. The training objectives include a VAE loss and two losses that encourage learning a behaviorally smooth program embedding space. Once trained, we can use the learned decoder \(p_{}\) to map any program embedding \(z\) to a program \(_{z}=p_{}(z)\) consisting of a sequence of program tokens. Details on the program dataset generation and the encoder-decoder training can be found in Section E.1.1.

### Retrieving programmatic options

With a program embedding space, we aim to retrieve a set of programs (programmatic options) given a task. This set of programs should have the following properties.

Figure 1: **Karel domain-specific language (DSL), designed for describing the Karel agent’s behaviors.*** **Effectiveness**: Each program can solve the task to some extent.
* **Diversity**: The more behaviorally diverse the programs are, the richer behaviors can be captured.
* **Compatibility**: Sequentially executing some programs with specific orders can potentially lead to improved task performance.

#### 4.2.1 Retrieving effective programs

To obtain a task-solving program, we can apply the Cross-Entropy Method , iteratively searching in a learned program embedding space  in the following procedure:

1. Randomly initialize a program embedding vector \(z_{r}\) as the search center.
2. Add random noises to \(z_{r}\) to generate a population of program embeddings \(Z=\{z_{i}\}_{i=1,,n}\), where \(n\) denotes the population size.
3. Evaluate every program embedding \(z Z\) with the evaluation function \(G\) to get a list of fitness score \([G(z_{i})]_{i=1,,n}\).
4. Average the top k program embeddings in \(Z\) according to fitness scores \([G(z_{i})]_{i=1,,n}\) and assign it to the search center \(z_{r}\).
5. Repeat (2) to (4) until the fitness score \(G(z_{r})\) of \(z_{r}\) converges or the maximum number of steps is reached.

Since we aim to retrieve a set of effective programs, we can define the evaluation function as the program execution return of a decoded program embedding, _i.e._, \(G(z)=_{t=0}^{T}^{t}_{(s_{t},a_{t})(_{t} )}[r_{t}]\). To retrieve a set of \(|M|\) programmatic options for a high-level policy, we deploy this CEM search \(N\) times, take \(|M|\) best program embeddings, and obtain the decoded program set \(\{_{z_{r_{i}}}=p_{}(z_{r_{i}})\}_{i=1,,|M|}\). Please refer to Section A.1 for more details and the CEM search pseudo-code.

#### 4.2.2 Retrieving effective, diverse programs

In our proposal, we retrieve a set of programmatic options for a high-level policy. Hence, the diversity of behaviors of the retreived program set endow HIPO with versatility. However, by

Figure 2: **Hierarchical Programmatic Option Framework.****(a): Retrieving programmatic options.** After learning the program embedding space, we propose an advanced search scheme built upon the Cross-Entropy Method (CEM) to search programs \(_{m_{1}},...,_{m_{k}},_{m_{k+1}}\) of different skills. While searching for the next program \(_{m_{k+1}}\), we consider its compatibility with predetermined programs \(_{m_{1}},...,_{m_{k}}\) by randomly sampling a sequence of programs. We also consider the diversity among all programs using the _diversity multiplier_. **(b): Learning the high-level policy.** Given the current environment state \(s\) and the current programmatic option \(m_{i}\), the high-level policy outputs a probability distribution over all programmatic options, aiming to maximize the total accumulative reward from the environment.

running the CEM search for \(|M|\) times, the obtained program set can have low diversity, making the multiskill-demanding tasks unsolvable.

To address this issue, we propose the _diversity multiplier_ that accounts previous search results to encourage diversity among the retrieved programs. The evaluation of program employing the _diversity multiplier_ is illustrated in Figure 2. Specifically, during the \((k+1)\)st CEM search, each program embedding \(z\) is evaluated by \(G(z,Z_{k})=(_{t=0}^{T}^{t}_{(s_{t},a_{t})( _{s})}[r_{t}]) diversity(z,Z_{k})\), where \(diversity(z,Z_{k})\) is the proposed _diversity multiplier_ defined as \(Sigmoid(-_{z_{i} Z_{k}}}{\|z\|\|_{z_{i}}\|})\). Thus, the program execution return is scaled down by \(diversity(z,Z_{k})\) based on the maximum cosine similarity between \(z\) and the retrieved program embeddings \(Z_{k}=\{z_{i}\}_{i=1,,k}\) from the previous \(k\) CEM searches, diverging the current program embedding from previously retrieved programs.

To retrieve a set of \(|M|\) programmatic options for our high-level policy, we deploy this CEM+diversity search \(N\) times, take \(|M|\) best program embeddings, and obtain the decoded program set. The procedure and the search trajectory visualization can be found in Section A.2.

#### 4.2.3 Retrieving effective, diverse, compatible programs

The proposed HIPO executes a sequence of programmatic options determined by a high-level policy. Therefore, these programs shall be compatible with each other, _i.e_., executing a program following the execution of other programs could improve task performance. Yet, CEM+diversity discussed in Section 4.2.2 searches every program independently.

Accounting for the compatibility among programs while searching, we propose an evaluation method, CEM+diversity+compatibility. To evaluate the program embedding \(z\), we take the decoded program \(_{z}\) as the \((k+1)\)st option. Then, lists of programs \(_{i,i=1,,D}\) are sampled with replacements from determined \(k\) options and the \((k+1)\)st option. Each program list \(_{i}\) contains at least one \((k+1)\)st option to consider the compatibility between the \((k+1)\)st and previously determined \(k\) options. The return is computed by sequentially executing these \(D\) sequences of programs and multiply the result with the _diversity multiplier_ proposed in Section 4.2.2. As the result, the evaluation function is \(G(z,Z_{k})=_{i=1}^{D}R_{_{i}} diversity(z,Z_{k})\), where \(R_{_{i}}\) is the normalized reward obtained from executing all programs in the specified program sequence \(_{i}\):

\[R_{_{i}}=|}_{j=1}^{|_{i}|}_{t=0}^{T^{j}} ^{t}_{(s_{t},a_{t})(_{i},|j)}[r_{t}],\] (1)

with \(|_{i}|\) being the number of programs in the program sequence \(_{i}\), \(_{i}[j]\) being the \(j\)-th program, and \(\) as the discount factor.

The search method with the specified evaluation is deployed \(|M|\) times to obtain a set of programs that are effective, diverse, and compatible with each other, adopted by the high-level policy as programmatic options. Please refer to Section A.3 for more details and the thorough procedure.

### Learning high-level policy with programmatic options

Given a set of programmatic options \(M=\{m_{k}\}_{k=1,\ ,|M|}\), we formulate learning a high-level policy \(f_{}\) represented by neural networks, as a reinforcement learning problem aiming to maximize the task return. At the \(i\)-th high-level step, given the latest selected programmatic option \(m^{i}\) and the current environment state \(s\), the high-level policy \(f_{}(m^{i},s)\) outputs the probability distribution of programmatic options for the next option \(m^{i+1} m_{}\{m_{k}\}_{k=1,\ ,|M|}\), where \(m_{}\) denotes the termination option that ends the episode once selected. Otherwise, the corresponding program \(_{m^{i+1}}\) is executed, yielding the next state \(s^{i+1}_{T^{i+1}}\) and the cumulative reward \(r^{i+1}=_{t=1}^{T^{i+1}}r^{i+1}_{t}\). Note that the last state \(s^{i+1}_{T^{i+1}}\) of the state sequence is returned by EXEC\((_{m^{i+1}})\), with \(T^{i+1}\) denoting the horizon of the \(i+1\)-th program execution. Also, the cumulative reward \(r^{i+1}\) obtained within a single program execution is not discounted.

Note that a single program execution EXEC\(()\) will terminate after complete execution or the number of function calls emitted during EXEC\(()\) reaches 220, which aligns to the setting in Trivedi et al.

. This iterative process stops once the termination option is sampled or the maximum number of the option-selection steps is reached. Please refer to Section E.1.3 for training details.

To further enhance the explainability of the high-level policy, we apply the approach proposed by Koul et al.  to extract the state machine structure from the learned high-level policy. Combining the retrieved set of programmatic options and the extracted state machine structure, our framework is capable of solving long-horizon tasks while being self-explanatory. Examples of extracted state machines are illustrated in Section D.

## 5 Experiments

We aim to answer the following questions with the experiments and ablation studies. (1) Can our proposed _diversity multiplier_ introduced in Section 4.2.2 enhance CEM and yield programs with improved performance? (2) Can our proposed CEM+diversity+compatibility introduced in Section 4.2.3 retrieve a set of programs that are diverse yet compatible with each other? (3) Can the proposed framework outperforms existing methods on long-horizon tasks?

Please refer to Section E for hyperparameter settings for the following experiments.

### Karel problem sets

To this end, we consider the Karel domain , which is widely adopted in program synthesis [9; 13; 65; 70] and programmatic reinforcement learning [46; 74]. Specifically, we utilize the Karel problem set  and the Karel-Hard problem set . The Karel problem set includes six basic tasks, each of which can be solved by a short program (less than \(45\) tokens), with a horizon shorter than \(200\) steps per episode. On the other hand, the four tasks introduced in the Karel-Hard problem require longer, more complex programs (_i.e._, \(45\) to \(120\) tokens) in longer execution horizons (_i.e._, up to \(500\) actions). Details on two problem sets is provided in Section F and Section G.

**Karel-Long problem set.** Since most of the tasks in the Karel and Karel-Hard problem sets are short-horizon tasks (_i.e._, can be finished in less than 500 timesteps), they are not suitable for evaluating long-horizon task-solving ability (_i.e._, tasks requiring more than 3000 timesteps to finish). Hence, we introduce a newly designed Karel-Long problem set as the benchmark to evaluate the capability of HIPO.

As illustrated in Figure 3, the tasks require the agent to fulfill extra constraints (_e.g._, not placing multiple markers on the same spot in Farmer, receiving penalties imposed for not moving along the stairs in Up-N-Down) and conduct extended exploration (_e.g._, repetitively locating and collecting markers in Seesaw, Inf-DoowKey, and Inf-Harvester). More details on the Karel-Long tasks can be found in Section H.

### Cross-entropy method with diversity multiplier

We aim to investigate whether our proposed _diversity multiplier_ can enhance CEM and yield programs with improved performance. To this end, for each Karel or Karel-Hard task, we use CEM and

Figure 3: **Karel-Long problem set**: This work introduces a new set of tasks in the Karel domain. These tasks necessitate learning diverse, repetitive, and task-specific skills. For example, in our designed Inf-Harvester, the agent needs to traverse the whole map and pick nearly 400 markers to solve the tasks since the environment randomly generates markers; in contrast, the Harvester from the Karel problem set  can be solved by picking merely 36 markers.

CEM+diversity to find 10 programs. Then, for each task, we evaluate all the programs and report the best performance in Table 1. The results suggest that our proposed CEM+diversity achieves better performance on most of the tasks, highlighting the improved search quality induced by covering wider regions in the search space with the _diversity multiplier_. Visualized search trajectories of CEM+diversity can be found in Section A.2.

### Ablation study

We propose CEM+diversity+compatibility to retrieve a set of effective, diverse, compatible programmatic options for our high-level policy. In this section, we compare a variety of implementations regarding the diversity and the compatibility of programs.

* **CEM**\(|M|\): Conduct the CEM search described in Section 4.2.1\(|M|\) times and take the resulting \(|M|\) programs as the set of programmatic options.
* **CEM+diversity top \(k\), \(k=|M|\)**: Conduct the CEM search with the _diversity multiplier_ described in Section 4.2.2\(N=10\) times and take the top \(|M|\) results as the set of programmatic options.
* **CEM+diversity**\(|M|\): Conduct the CEM search with the _diversity multiplier_ described in Section 4.2.2\(N=10\) times and select the best program as the \(i^{th}\) option. Repeat this process \(|M|\) times to extract \(|M|\) programs as the set of programmatic options.
* **CEM+compatibility\(|M|\)**: Conduct the CEM search by executing programs in the specified program sequence \(_{i}\) described in Section 4.2.3, excluding the _diversity multiplier_. Iteratively perform this search \(|M|\) times and take the resulting \(|M|\) programs as the set of programmatic options.
* **HIPO (ours)**: Conduct CEM+diversity+compatibility (_i.e._, CEM with the _diversity multiplier_ and \(R_{}\) as described in Section 4.2.3) for \(N=10\) times and select the best result as the \(i^{th}\) option. Repeat the above process \(|M|\) times and take all \(|M|\) results as the set of programmatic options.

The number of programmatic options \(|M|\) is 3 for Seesaw, Up-N-Down, and Inf-Harvester and 5 for Farmer and Inf-DoorKey. We assess the quality of the retrieved programmatic options by evaluating the performance of the high-level policy learned with these option sets on the Karel-Long tasks. The results presented in Table 2 indicate that our proposed framework, HIPO, outperforms its variants that ignore diversity or compatibility among programmatic options on all the tasks. This justifies our proposed CEM+diversity+compatibility method for retrieving a set of effective, diverse, compatible programs as options for the high-level policy.

### Comparing with deep RL and programmatic RL Methods

We compare our proposed framework and its variant to state-of-the-art deep RL and programmatic RL methods on the Karel-Long tasks.

* **Random transition** uses the same set of programmatic options as our method but with a random high-level policy (_i.e._, uniformly randomly select the next option at each step). The performance of this method examines the necessity of learning a high-level policy.

    & Foux & Top & Clean & Stain &  &  &  &  &  &  \\  & CoWER & OP & House & CHDER & & & & & & & \\  CEM & 0.45 \(\) 0.40 & 0.81 \(\) 0.07 & 0.18 \(\) 0.14 & **1.00 \(\) 0.00** & 0.45 \(\) 0.28 & **1.00 \(\) 0.00** & 0.50 \(\) 0.19 & 0.51 \(\) 0.21 & 0.21 \(\) 0.15 \\ CEM+diversity & **1.00**\(\) 0.00 & **1.00**\(\) 0.00 & 0.37 \(\) 0.06 & **1.00 \(\) 0.00** & 0.80 \(\) 0.07 & **1.00 \(\) 0.00** & 0.50 \(\) 0.00 & 0.62 \(\) 0.01 & 0.69 \(\) 0.07 & 0.36 \(\) 0.02 \\  DRL & 0.29 \(\) 0.05 & 0.32 \(\) 0.07 & 0.00 \(\) 0.00 & **1.00 \(\) 0.00** & 0.90 \(\) 0.10 & **1.00 \(\) 0.00** & 0.48 \(\) 0.03 & **0.90 \(\) 0.04** & 0.96 \(\) 0.02 & **0.47 \(\) 0.17 \\ LEAPS & 0.45 \(\) 0.40 & 0.81 \(\) 0.07 & 0.18 \(\) 0.14 & **1.00 \(\) 0.00** & 0.45 \(\) 0.28 & **1.00 \(\) 0.00** & 0.50 \(\) 0.00 & 0.65 \(\) 0.19 & 0.51 \(\) 0.21 & 0.21 \(\) 0.15 \\ HPRL & **1.00**\(\) 0.00 & **1.00**\(\) 0.00 & **1.00 \(\) 0.00** & **1.00 \(\) 0.00** & **1.00 \(\) 0.00** & **1.00 \(\) 0.00** & 0.50 \(\) 0.00 & 0.80 \(\) 0.02 & 0.58 \(\) 0.07 & 0.28 \(\) 0.11 \\  HIPO (Ours) & **1.00**\(\) 0.00 & **1.00**\(\) 0.00 & **1.00**\(\) 0.00 & **1.00**\(\) 0.00 & **1.00**\(\) 0.00 & **1.00**\(\) 0.00 & **1.00**\(\) 0.00 & 0.62 \(\) 0.01 & **0.97**\(\) 0.02 & 0.36 \(\) 0.02 \\   

Table 1: **Evaluation on Karel and Karel-Hard tasks.** Mean return and standard deviation of all methods across the Karel and Karel-Hard problem set, evaluated over five random seeds. CEM+diversity outperforms CEM with significantly smaller standard deviations across 8 out of 10 tasks, highlighting the effectiveness and stability of CEM+diversity. In addition, HIPO outperforms LEAPS and HPRL on 8 out of 10 tasks.

* **Primitive actions as options (PAO)** learns a high-level policy similar to HIPO, which takes the current option and environment state as input and predicts the next option. However, it utilizes primitive actions (_e_.\(g\)., move, pickMarker) as options. This baseline highlights the necessity of retrieving programs with higher-level behaviors as options.
* **DRL** represents a policy as a neural network and is learned using PPO . The policy takes raw states (_i_.\(e\)., Karel grids) as input and predicts the probability distribution over the set of primitive actions, (_e_.\(g\)., move, pickMarker).
* **Option-Critic** represents a policy that both high-level and low-level policies are neural networks and is learned using the option-critic architecture . The policy takes raw states (_i_.\(e\)., Karel grids) as input, and each option predicts the probability distribution over the set of primitive actions (_e_.\(g\)., move, pickMarker).
* **Learning Embeddings for Latent Program Synthesis (LEAPS)** searches for a single task-solving program using the vanilla CEM in a learned program embedding space.
* **Hierarchical Programmatic Reinforcement Learning (HPRL)** learns a meta-policy, whose action space is a learned program embedding space, to compose a series of programs as the policy.
* **Hill Climbing (HC)** is a stochastic search technique that operates directly within the program space. The process begins by randomly modifying portions of the current program to generate a set of neighboring candidates. The program that performs best among these neighbors is selected as the next candidate for exploration.

As Table 1 shows, HIPO outperforms LEAPS and HPRL on 8 out of 10 tasks from the Karel and Karel-Hard tasks, indicating that the retrieved programs are truly effective at solving short horizon

   Method & Sessaw & Up-N-Down & Farmer & Inf-DoorKey & INF-Harvester \\  CEM \(|M|\) & \(0.06 0.10\) & 0.39 \(\) 0.36 & 0.03 \(\) 0.00 & 0.11 \(\) 0.14 & 0.41\(\) 0.17 \\ CEM+diversity top \(k\)\(\)\(=|M|\) & 0.15 \(\) 0.21 & 0.25 \(\) 0.35 & 0.03 \(\) 0.00 & 0.13 \(\) 0.16 & 0.42\(\) 0.19 \\ CEM+diversity \(|M|\) & 0.28 \(\) 0.23 & 0.58 \(\) 0.31 & 0.03 \(\) 0.00 & 0.36 \(\) 0.26 & 0.47\(\) 0.23 \\ CEM+compatibility \(|M|\) & 0.23 \(\) 0.32 & 0.43 \(\) 0.34 & 0.14 \(\) 0.22 & 0.57 \(\) 0.3 & 0.66 \(\) 0.08 \\  Random Transition & 0.01 \(\) 0.00 & 0.02 \(\) 0.01 & 0.01 \(\) 0.00 & 0.01 \(\) 0.01 & 0.15\(\) 0.04 \\ PAO & 0.01 \(\) 0.01 & 0.00 \(\) 0.00 & 0.43 \(\) 0.23 & 0.34 \(\) 0.45 & 0.60 \(\) 0.04 \\ DRL & 0.00 \(\) 0.01 & 0.00 \(\) 0.00 & 0.38 \(\) 0.25 & 0.17 \(\) 0.36 & 0.74 \(\) 0.05 \\ Option-Critic & 0.00 \(\) 0.00 & 0.00 \(\) 0.00 & 0.00 \(\) 0.00 & 0.00 \(\) 0.00 & 0.47 \(\) 0.01 \\ LEAPS & 0.01 \(\) 0.01 & 0.02 \(\) 0.01 & 0.03 \(\) 0.00 & 0.01 \(\) 0.00 & 0.12 \(\) 0.00 \\ HPRL & 0.00 \(\) 0.00 & 0.00 \(\) 0.00 & 0.01 \(\) 0.00 & 0.00 \(\) 0.00 & 0.45 \(\) 0.03 \\ HC & 0.22 \(\) 0.08 & 0.31 \(\) 0.38 & 0.19 \(\) 0.03 & 0.14 \(\) 0.16 & **0.88 \(\)** 0.00 \\  HIPO (Ours) & **0.53**\(\) 0.10 & **0.76**\(\) 0.02 & **0.62**\(\) 0.02 & **0.66**\(\) 0.07 & 0.79 \(\) 0.02 \\   

Table 2: **Karel-Long performance.** Mean return and standard deviation of all methods across the Karel-Long problem set, evaluated over five random seeds. Our proposed framework achieves the best mean reward across most of the tasks by learning a high-level policy with a set of effective, diverse, and compatible programs.

Figure 4: (a) **Program sample efficiency.** The training curves of HIPO and other programmatic RL approaches, where the x-axis is the total number of executed programs for interacting with the environment, and the y-axis is the maximum validation return. This demonstrates that our proposed framework has better program sample efficiency and converges to better performance. (b) **Inductive generalization performance.** We evaluate and report the performance drop in the testing environments with an extended horizon, where the x-axis is the extended horizon length compared to the horizon of the training environments, and the y-axis is the performance drop in percentage. Our proposed framework can inductively generalize to longer horizons without any fine-tuning.

tasks (_i_.\(e\)., less than 500 actions). For long-horizon tasks that require more than 3000 actions to solve, Table 2 shows that HIPO excels on four tasks, with better performance on Farmer and particular prowess in Seesaw, Up-N-Down, and Inf-Dowkey.

Two of these tasks require distinct skills (_e_.\(g\)., pick and put markers in Farmer; go up and downstairs in Up-N-Down) and the capability to persistently execute one skill for an extended period before transitioning to another. HIPO adeptly addresses this challenge due to the consideration of diversity while seeking programmatic options, ensuring the acquisition of both skills concurrently.

Unlike the other tasks, Seesaw and Inf-Dowkey require an extended traverse to collect markers, leading to a sparser reward distribution. During the searching phase of programmatic options, emphasizing compatibility enables HIPO to secure a set of mutually compatible options that work together effectively to accomplish the extended traversal.

Retrieved programs are provided in Appendix (Figure 22, Figure 23, Figure 24, and Figure 25). Experimental results on programmatic policy baselines over 32 seeds are provided in Table 3.

### Program sample efficiency

To accurately evaluate the sample efficiency of programmatic RL methods, we propose the _program sample efficiency_ metric, measuring the total number of program executions required to learn a program policy. We report the program sample efficiency of LEAPS, HPRL, HC, and HIPO on Farmer and Inf-Harvester in Figure 4a. As the results show, HIPO demonstrates program sample efficiency than LEAPS and HPRL, indicating that our framework requires fewer program interactions with the environment and lower computational costs compared to existing latent-space-based programmatic RL frameworks. More details and the action sample efficiency can be found in Section B and Figure 8.

### Inductive generalization

We aim to compare the inductive generalization ability among all the methods, generalizing to out-of-distributionally (_i_.\(e\)., unseen during training) long task instances . To this end, we increase the expected horizons of Farmer and Inf-Harvester by \(2\), \(4\), \(8\), and \(16\). Then, we report the performance drop compared to the original task performance of selected baselines in Figure 4b. More details on extending task horizons are provided in Section C.

The results show that HIPO suffers a fewer decline in performance in testing environments with significantly extended horizons than LEAPS and HPRL, suggesting that HIPO exhibits better inductive generalization in these tasks. The longest execution of HIPO ran up to \(48\)k environment steps.

### Interpretability

In the proposed framework, we retrieve a set of programmatic options that can be reused by the high-level policy to solve long and repetitive tasks. For example, in Inf-DoorKey (detailed in Section H), the retrieved programmatic options are presented in Figure 24. Based on these programmatic options, the high-level policy can reuse them to guide the agent to traverse all four chambers with the following sequence of programmatic options: \(_{m}=\{m_{5},m_{3},m_{5},m_{3},m_{1},m_{4},m_{4},m_{2},m_{5},m_{4},m_{5}\}\).

Inf-Doorkey requires three different skills to solve: picking a marker to open the door in some chambers, placing a marker to open the door in some other chambers, and navigating between the chambers. Specifically, the programs provided in Figure 24 show that Option 1 and Option 5 fulfill the first skill, while Option 3, Option 4, and Option 5 meet the requirements for the second, and Option 2 satisfy the third. Along with the observation, we could fully interpret the agent policy with the control flows, perceptions, and actions specified in the program. On the other hand, the high-level policy is a neural network by itself, making it hard to interpret the option transition dynamic learned by the high-level policy.

To improve the interpretability of our high-level policy, we extract state machine structures from the high-level policy to visualize the option transition dynamics. More details and examples of extracted state machines can be found in Section D.

Conclusion

This work aims to construct reinforcement learning policies that are human-interpretable and generalizable, bridging from hierarchical reinforcement learning to programmatic options. Consequently, we propose the Hierarchical Programmatic Option framework (HIPO) to represent complex behaviors and address long-horizon tasks. Specifically, we introduce a method that can retrieve a set of effective, diverse, compatible programs by modifying the Cross Entropy Method (CEM). Following this, these programs are applied as options by the high-level policy learned with reinforcement learning. To evaluate HIPO's ability in extended horizons, we design a set of tasks that require thousands of steps in the Karel domain. Our framework HIPO outperforms various deep RL and programmatic RL methods on various tasks. In addition, HIPO demonstrates good performance in inductive generalization to even longer horizons without fine-tuning. Last but not least, extensive ablation studies justify the effectiveness of our proposed search algorithm in retrieving programmatic options.