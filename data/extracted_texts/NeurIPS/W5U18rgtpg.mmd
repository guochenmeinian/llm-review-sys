# Re-evaluating Retrosynthesis Algorithms

with Synthesus

 Krzysztof Maziarz\({}^{1}\)  Austin Tripp\({}^{2}\)\({}^{*}\)  Guoqing Liu\({}^{1}\)  Megan Stanley\({}^{1}\)

**Shufang Xie\({}^{1}\) Piotr Gainski\({}^{3}\)\({}^{}\) Philipp Seidl\({}^{4}\)\({}^{}\) Marwin Segler\({}^{1}\)\({}^{*}\)**

\({}^{1}\)Microsoft Research AI4Science \({}^{2}\)University of Cambridge

\({}^{3}\)Jagiellonian University \({}^{4}\)Johannes Kepler University Linz

Correspondence to {krmaziar, marwinsegler}@microsoft.com and ajit212@cam.ac.ukThese authors contributed during their internships at Microsoft Research AI4Science

###### Abstract

The planning of how to synthesize molecules, also known as retrosynthesis, has been a growing focus of the machine learning and chemistry communities in recent years. Despite the appearance of steady progress, we argue that imperfect benchmarks and inconsistent comparisons mask systematic shortcomings of existing techniques. To remedy this, we present a benchmarking library called synthesus which promotes best practice by default, enabling consistent meaningful evaluation of single-step and multi-step retrosynthesis algorithms. We use synthesus to re-evaluate a number of previous retrosynthesis algorithms, and find that the ranking of state-of-the-art models changes when evaluated carefully. We end with guidance for future works in this area. We encourage the reader to view the full version of this paper at https://arxiv.org/abs/2310.19796.

## 1 Introduction

Over the past five years, the use of machine learning, and generative models in particular, has led to renewed interest in the automated computational design of novel molecules (Segler et al., 2017; Gomez-Bombarelli et al., 2018; Meyers et al., 2021; Maziarz et al., 2022). Although such approaches can help to discover compounds with the desired property profiles more efficiently, most existing methods do not explicitly account for synthesizability, and therefore often output molecules which are hard to synthesize in a wet-lab (Klebe, 2009). This motivates the development of fast and reliable computer-aided synthesis planning (CASP) algorithms which check for synthesizability by explicitly designing synthesis routes for an input molecule (Strieth-Kalthoff et al., 2020), also known as _retrosynthesis_.

Retrosynthesis works by recursively decomposing a target molecule into increasingly simpler molecules using formally reversed chemical reactions, until a set of purchasable or known building block molecules is found. Starting with the building blocks, the reactions in the forward direction provide a recipe of how to synthesize the target. Most work in the area has focused on studying these two components, single-step retrosynthesis models and multi-step planning algorithms, independently.

In _single-step_ retrosynthesis, models are given a molecule and output reactions which produce that molecule in one step (Segler and Waller, 2017; Coley et al., 2017; Liu et al., 2017; Dai et al., 2019; Tetko et al., 2020; Yan et al., 2022; Wang et al., 2023). Recent work in this area has generally focused on training neural networks to predict reactions extracted from the scientific literature or patents (Lowe, 2012; Zhong et al., 2023). In _multi-step_ planning, given a target molecule, a set of purchasable molecules, and a single-step retrosynthesis model, the goal is to produce complete synthesis routes. This is challenging as the search space is extremely large compared to the number of solutions. Recent work in this area has used Monte Carlo Tree Search (MCTS), ReinforcementLearning (RL), or heuristic-guided search algorithms, to selectively explore a tree of possible reactions from the starting molecule (Segler et al., 2018; Coley et al., 2019; Schwaller et al., 2020; Chen et al., 2020; Xie et al., 2022; Tripp et al., 2022; Liu et al., 2023; Tripp et al., 2023).

In this work, we take a closer look at the commonly used metrics for single and multi-step retrosynthesis. First, it is not clear how metrics used when benchmarking single-step and multi-step in isolation should be interpreted in the context of an end-to-end retrosynthesis pipeline. Second, model comparison and metrics use in prior work has been inconsistent. The goal of this paper is to specify best practices for evaluating retrosynthesis algorithms and perform a rigorous re-evaluation and analysis of prior work. To facilitate this, we developed a python package called synthesesus which allows researchers to evaluate their approaches in a consistent way. The paper is organized as follows. Section 2 examines how retrosynthesis was evaluated in previous works, and points out shortcomings in this practice. We find several previously reported results to be understated, overstated, or otherwise not comparable to each other. We then present best practices for the field. In Section 3 we present our python package synthesesus. It supports consistent evaluation of single-step and multi-step retrosynthesis algorithms, with best practice enforced by default. In Section 4 we use synthesesus to re-evaluate existing single-step and multi-step methods across many settings in an attempt to "set the record straight". We refrain from making a final recommendation about which models or algorithms are the best, as we argue currently existing metrics are not sufficient. Our results instead serve as a starting point for future research, which we hope synthesesus to accelerate.

## 2 Pitfalls and Best Practice for Retrosynthesis Evaluation

Evaluation in retrosynthesis is largely constrained by two realities. First, actually performing synthesis in the lab is costly, time-consuming, and requires significant expertise; it is therefore infeasible for most researchers who work on algorithm development, and should not be a requirement, even though experimental validation is clearly important. Second, because of the division into single-step and multi-step, most works seek to evaluate one part of the retrosynthesis pipeline in isolation rather than holistically, while the key to real-world adoption lies in end-to-end performance. Keeping this in mind, in this section we survey the merits and shortcomings of existing evaluation practices.

### Single-Step Models

Single-step retrosynthesis models have several functions in CASP programs: (1) defining which reactions are _feasible_ ways of obtaining a given molecule, effectively defining the search environment; and (2) _ranking_ or otherwise expressing preference over these reactions, effectively acting as a policy or heuristic to guide the search. Most single-step retrosynthesis models output a list of reactions, and are trained using supervised learning to output reactions which were used in real synthesis routes and furthermore to rank these reactions highly. The most common evaluation strategy is to evaluate the top-\(k\) accuracy on a held-out test set, i.e. the fraction of molecules where the reaction which occurred in the dataset is ranked in the first \(k\) outputs (Segler and Waller, 2017; Liu et al., 2017; Coley et al., 2017). This parallels evaluation commonly used in computer vision (Deng et al., 2009; Krizhevsky et al., 2017). In this section, we explain how this evaluation metric does not fully measure the utility of single-step models in CASP programs, and how subtle differences in evaluation have distorted the numbers reported in prior works. We suggest best practice for each of these points.

Pitfall S1: measuring recall rather than precisionBy measuring how often reactions from the dataset occur in the model outputs, top-\(k\) accuracy essentially tests the model's ability to _recall_ the dataset. Unless \(k=1\) and the top-1 accuracy is nearly 100%, a multi-step search algorithm using this single-step model will almost certainly use reactions not contained in the dataset for planning. If these reactions have low quality or feasibility then routes using them will not be useful. On the other hand, in many cases there are several possible ways to make a particular molecule. Therefore, as previously argued by Schwaller et al. (2020), top-\(k\)_precision_ of a single-step model (what fraction of the top \(k\) reactions are feasible) is arguably equally or more important than recall for multi-step search. Unfortunately, without an expert chemist or a wet-lab, precision is hard to measure. Nonetheless, this suggests that models with a higher top-\(k\) accuracy are not necessarily more useful in CASP programs.

**Best practice:** Although it is not clear how this can be done, we believe authors should strive to evaluate the precision of their models, at the very least through a visual check of several examples. Some prior works use round-trip accuracy using a forward reaction model (also referred to as back translation) to measure feasibility of reactions that are not necessarily ground-truth (Schwaller et al., 2019; Chen and Jung, 2021). However, we note the inconsistent use of the term "round-trip accuracy" in prior work: Chen and Jung (2021) compute it in a top-\(k\) fashion where _at least one_ of the top-\(k\) results has to round-trip in order for the prediction to count as successful, which does _not_ measure precision; in (Schwaller et al., 2020) this metric is called _coverage_. Round-trip accuracy also relies on a fixed forward model, which is usually only trained on real reactions (i.e. is given sets of reactants that actually react as input) without the presence of negative data; it is unclear whether such a model can be used to evaluate reaction feasibility more broadly. In summary, while the best way to evaluate model precision is not clear, we think this needs more attention and thought from the community.

Best practice S2: use consistent and realistic post-processingMost prior works perform some amount of post-processing of model outputs when measuring accuracy. Unfortunately, this has not been done consistently by previous papers, distorting comparisons between methods. In general, the evaluation post-processing should match the post-processing that would be performed if the model was used in a CASP program. We identify several instances of this below and suggest best practice.

* **Invalid outputs:** some models can output invalid molecules (e.g. a syntactically invalid SMILES string) (Irwin et al., 2022). When computing top-\(k\) accuracy, some prior works include invalid molecules in the top-\(k\), whereas other works filter them and consider the top-\(k\)_valid_ molecules. Because the validity of molecules is generally easy to check,1 a well-engineered CASP program would discard invalid molecules instead of considering them during search. Therefore, we believe best practice should be to only consider valid molecules when computing top-\(k\) accuracy. * **Duplicate outputs:** some models can produce the same result (i.e. same set of reactants) multiple times. Clearly, a well-engineered CASP program would remove duplicate reactions, because they are redundant for search. However, this has not been done consistently in prior work. For example, we found that the published top-5 accuracy of GLN (Dai et al., 2019) on USPTO-50K can be increased by as much as 5.8% by applying simple deduplication. Therefore, we think best practice is to measure accuracy _after_ deduplicating the outputs.
* **Stereochemistry:** in general, the stereochemistry of chiral molecules is important for chemical reactivity;2 for this reason, many prior works require an exact match of stereochemistry in order for a prediction to count as correct. In popular datasets like USPTO (Schneider et al., 2016) stereochemistry is often unlabelled or mislabelled, which motivated the authors of LocalRetro (Chen and Jung, 2021) to measure a relaxed notion of accuracy where a prediction can be deemed correct even if its stereochemistry is different to the dataset.3 However, this practice was not applied to baselines LocalRetro was compared to, and subsequent authors copied the result from (Chen and Jung, 2021) unaware that it uses a different definition of success. In our re-evaluation we found that using a relaxed comparison significantly boosted the reported accuracy of LocalRetro on USPTO-50K (e.g. +1.3% top-1 and +2.6% top-50); same is true for RetroKNN (Xie et al., 2023a) which built upon LocalRetro and re-used their evaluation code. While some datasets like USPTO-50K indeed contain chirality errors, in real-world scenarios CASP programs should not discard it; we therefore believe that best practice is to report the standard exact match (although additional reporting of results with stereochemistry removed could still provide valuable insight).

Best practice S3: report inference timeIn contemporary ML works, it is common to give little attention to inference time, and focus on pushing the quantitative model performance. However, in retrosynthesis prediction, the purpose of a single-step model is to act as an environment during multi-step search. In practice, having a drastically faster single-step model can translate to doing a much more extensive search, thus single-step model speed is directly tied to quantitative performance downstream. Due to that, we believe future research should give more attention to accurately reporting inference speed, reasoning in terms of a speed-accuracy Pareto front rather than accuracy alone. At the very least, we believe best practice is to report inference time in addition to accuracy.

Best practice S4: focus on prediction with unknown reaction typeMost single-step works using USPTO report two sets of metrics: one for when the reaction type is not known, and another one for when the reaction type is given as auxiliary input; a practice started by (Liu et al., 2017). The rationale for the latter usually involves an interactive setting where a chemist may prefer one reaction type over another. In the context of multi-step search this information would not be available, and it is unlikely that a given reaction type is universally preferred across the entire search tree. In any case, none of the popular multi-step search algorithms add reactions conditioned on a particular reaction type, so this "conditional reaction prediction" would not be used by existing approaches. Thus, our recommendation is for researchers to focus on the "reaction type unknown" setting, as this is the one most directly applicable to multi-step search.

Best practice S5: avoid data leakage through atom mappingsSome results on USPTO-50K were later found to be flawed due to unexpected behaviour of rdkit canonicalization after removing the atom mapping from test molecules (Yan et al., 2020; Gao et al., 2022). While this problem is known to many practitioners, we mention it for completeness. To avoid this pitfall, the input molecules should be provided to the model with the atom mapping information removed, and they have to be re-canonicalized _after_ said removal.

### Multi-Step Search

The role of a multi-step search algorithm is to use a single-step model, a set of purchasable molecules, and optionally some heuristic functions, in order to find synthesis routes. Most prior works evaluate multi-step search algorithms by reporting the fraction of test molecules solved in a given time window, where time is often measured with the number of calls to the reaction model. As in the previous section, here we explain the pitfalls and abuses of this metric and suggest best practice going forward.

Pitfall M1: changing the single-step modelMany algorithms use single-step reaction models not only to define the search environment, but also use the rankings or probabilities from a single-step model as a policy, cost function, or to otherwise guide the search (Segler et al., 2018; Kishimoto et al., 2019; Chen et al., 2020). Naturally this has led some works to modify the single-step model in order to improve search performance (Kim et al., 2021; Yu et al., 2022). These modifications not only change the relative rankings, but also the set of produced reactions. We see two pitfalls with the way this has been used in practice. First, unless the single-step model is separately validated, it is not clear whether it still outputs realistic reactions: for example, a change in the solution rate could just be the result of new unrealistic reactions being outputted by the model. Second, even disregarding model quality, comparing search algorithms with different single-step models is essentially comparing two algorithms in different environments, which is not a meaningful comparison. We think that best practice in this aspect should be training a policy model to _re-rank_ the top-\(k\) outputs of a fixed single-step model without changing the set of feasible reactions. This allows for meaningful improvement while still keeping the same accuracy guarantees and comparability of using the original single-step model. We note that this strategy was recently used by Liu et al. (2023).

Pitfall M2: using search success rate to compare single-step modelsSome works (Hassen et al., 2022; Torren-Peraire et al., 2023) run search using various single-step models and use the success of such search to rank the models themselves. While we agree that single-step models should be benchmarked as part of search, inferring that a model is better solely because it allows for finding more routes can lead to flawed conclusions: an overly permissive single-step model may yield many routes simply because it lets search make unrealistic retrosynthetic steps, as demonstrated in the baseline experiments in Segler et al. (2018). Instead, success rate should be treated as an initial metric; a final determination of whether one end-to-end retrosynthesis pipeline is better than another is only possible if the quality of routes found is properly assessed. Outside of actually running synthesis, this could also be achieved using a reaction feasibility model; however, training such models in a generalizable way is so far an underexplored research direction.

Best practice M3: carefully choose how search experiments are capped if varying the single-step modelExisting works differ in how search experiments are limited: some use number of calls to the reaction model (Tripp et al., 2022), while others combine this with a wall-clock time limit (Hassen et al., 2022). Capping the number of model calls is a reliable choice if the single-step model is kept fixed; however, varying the single-step model can lead to some models being allocated vastly more resources_ (e.g. time) than others (Torren-Peraire et al., 2023). This may be justified if one believes the model speed is subject to change, and that perhaps all compared models can be optimized to eventually take a similar amount of time per call, but in the absence of such belief we recommend limiting search using a measure that treats the algorithm as a black-box (e.g. wall-clock time or memory consumption), as such approach also more directly reflects downstream use in CASP systems.

Best practice M4: cache calls to the reaction modelIf the same molecule is encountered twice during search, a naive implementation will call the reaction model twice. As calling the reaction model is expensive, a well-engineered CASP system would clearly _cache_ the outputs of the reaction model to avoid duplicate computation. Therefore, we believe it is best practice to use a cache for the single-step model when evaluating multi-step algorithms. This may sound like a minor implementation detail, but it actually has a significant impact on the evaluation: often large sub-trees can occur in multiple places during search;4 without a cache, expanding each occurrence of these subtrees will count against an algorithm's time budget, whereas with a cache these expansions are effectively free (Tripp et al., 2022).

Best practice M5: evaluate the diversity of proposed routesWhile previous works emphasize finding a single synthesis route quickly, because outputs of CASP programs may not work in the wet lab it is preferable to return multiple routes, and that these routes be _diverse_. Put another way, once an algorithm is able to find a single route, it is desirable to evaluate its ability to find additional ones which differ from the one already found. There are many ways to measure diversity, but we think that a good diversity metric must be monotonic with respect to input routes (otherwise algorithms could be penalized for finding more routes). One such metric is the packing number, also called #Circles (Xie et al., 2023b), which can be instantiated as the number of synthesis routes with no overlapping reactions.5

Best practice M6: quality assessment by chemistsFinally, we recommend practitioners perform qualitative assessment of the discovered routes by expert chemists, similarly to Segler et al. (2018). This is closer to experimental validation than commonly used metrics, and has the potential to catch many pitfalls, including (but not limited to) most of those described above. Additionally, it can capture poor synthesis strategies e.g. repetitions of similar steps, redundant (de)protection chemistry, or poor choice of linear vs convergent synthesis routes, which are difficult to spot with computational metrics.

## 3 Syntheses

To encourage and promote the principles and practices discussed in Section 2, we built a benchmarking library called syntheseus. Syntheseus is designed to be a platform for researchers developing methods for retrosynthesis, rather than a specific set of models or tasks. Currently, there is no generic package for retrosynthesis evaluation, forcing researchers to either write evaluation code themselves (which can be subtly inconsistent with prior work) or directly copy code from prior works (which have not followed the best practices from Section 2). Syntheseus provides a working end-to-end retrosynthesis pipeline which is modular and extensible for both novel single-step models and novel multi-step search algorithms; this allows researchers to plug their methods into a well-tested evaluation pipeline which implements best practice by default. We highlight key features of syntheseus in Appendix A, and discuss related work in Appendix B.

## 4 Experiments: re-evaluation of existing methods

We use syntheseus to re-evaluate many existing single-step models in conjunction with popular search algorithms, providing a holistic view of the existing methods, and in many instances _correcting the numbers from the literature_. We did not re-implement any of the models, and used open-source codebases wrapped into syntheseus's single-step model interface, demonstrating its flexibility. Crucially, _results in this paper were produced by our evaluation framework with no numbers copied from previous work_, which ensures a fair comparison immune to many issues discussed in Section 2.

### Single-Step

DatasetsAs a starting point we use the USPTO-50K dataset (Schneider et al., 2016) split by Dai et al. (2019), as all of the models we consider report results on this dataset, allowing us to contrast the published numbers with ones obtained from our re-evaluation. There is a newer version of this dataset available (Lin et al., 2022), but since our aim is to correct the existing results, we focus on the more established version and leave using the newer one for future work. Moreover, USPTO-50K is a small dataset, and it may not be representative of the full data distribution. Thus, we also use the proprietary Pistachio dataset (Mayfield et al., 2017) (more than 15.6M raw reactions; 3.7M samples after preprocessing), and evaluate out-of-distribution generalization of the model checkpoints trained on USPTO-50K. To the best of our knowledge this has not been done before; while some works also make use of Pistachio (Jiang et al., 2023), it is rather used as a _pretraining_ dataset before fine-tuning on USPTO. As most researchers do not have access to Pistachio, by reporting generalization we aim to gain insight into how USPTO-trained models work across a wider input distribution they may be exposed to during multi-step search. We performed significant cleaning and preprocessing on the Pistachio data to ensure the test set is of high quality, and also to limit overlap between the USPTO training set and Pistachio test set; see Appendix C for the details of our preprocessing pipeline.

ModelsWe re-evaluate established single-step models where either the code is publicly available (GLN (Dai et al., 2019), MEGAN (Sacha et al., 2021), MHNreact (Seidl et al., 2021), LocalRetro (Chen & Jung, 2021), RootAligned (Zhong et al., 2022) and Chemformer (Irwin et al., 2022)) or we were able to obtain it from the authors (RetroKNN (Xie et al., 2023a)). We omit Dual-{TB,TF} (Sun et al., 2020) as we have no access to the code; even though these models reported promising performance, we were unable to verify it under our framework. For all models we used the provided checkpoint if one using the right data split was available, and trained a new model using the original training code otherwise. We used the original implementations adapted to our shared interface.6

MetricsWe compute top-\(k\) accuracy for \(k\) up to \(50\) and Mean Reciprocal Rank (MRR). It is not clear what value of \(k\) is the most relevant metric to consider, but given the target use of single-step models in search, it is desirable for \(k\) to be roughly similar to the expected or desired breadth of the search tree (number of children visited for a typical internal node); thus, \(k=1\) would be too narrow. Typically, values beyond \(k=50\) are not reported, as models tend to saturate past this point. Several CASP programs also restrict the expansion beyond the top-50 (Segler et al., 2018; Genheden et al., 2020). We highlight \(k=5\) as a reasonable middle-ground, and defer extended results to Appendix D.

SetupWe queried all models for \(n=100\) outputs (see Appendix E for a discussion on how obtaining multiple results is handled for different model types). Note that we measure top-\(k\) only up to \(k=50\) but set \(n>k\) to account for deduplication. We used a fixed batch size of \(1\) for all models. While all models could easily handle larger batches, batch size used during search typically cannot be set arbitrarily, and in most cases it is equal to \(1\) as usually search is not parallelized. Thus, speed under batch size of \(1\) directly translates to the maximum number of model calls that can be performed during search with a fixed time budget. All inference time measurements used a single V100 GPU.

Figure 1: Trade-off between top-5 accuracy and inference speed. Circle area is proportional to the number of parameters; color denotes whether a model predicts a graph edit (blue) or produces the output from scratch (red). Dashed gray line shows the Pareto front (best result for any time budget). Exact results for Chemformer are not shown as they fall below the plot boundary. We show in-distribution results on USPTO-50K (left) and out-of-distribution generalization on Pistachio (right).

ResultsWe present top-5 accuracy results on both datasets in Figure 1. First, we note that two of the models (RootAligned, Chemformer) predict the reactants SMILES from scratch using a Transformer decoder (Vaswani et al., 2017), while the other models predict the graph rewrite to apply to the product. Across datasets and metrics, models of the former type tend to be slower, and while they show good performance in top-1 accuracy, they are surpassed by the graph-transformation-based models for higher \(k\). We hypothesize that, due to more explicit grounding in the set of transformations occurring in training data, transformation-based models tend to produce a more complete coverage of the data distribution. Second, many of the USPTO-50K results we report are better than the numbers from the literature (see Table 1 in Appendix D for a detailed breakdown), especially in terms of top-\(k\) accuracy for \(k>1\), which is affected by deduplication. This also changes some of the model rankings, e.g. LocalRetro was originally reported to have a better top-1 accuracy than GLN, but we find that to not be the case. Surprisingly, model ranking on USPTO-50K transfers to Pistachio quite well, although

Figure 2: Multi-step search results on the Retro* Hard target set with different single-step models. **Left:** Time until first solution was found (or \(\) if a molecule was not solved). Orange line represents the median, box represents 25th and 75th percentile, whiskers represent 5th and 95th percentile, points outside this range are shown as dots. **Right:** Approximate number of non-overlapping routes present in the search graph (tracked over time and aggregated across target molecules). Solid line represents the median, shaded area shows the 40th and 60th percentile. On the right hand side we note the average number of calls made by the model within the allotted time limit.

all results are substantially degraded, e.g. in terms of top-50 accuracy all models still fall below \(55\%\), compared to nearly \(100\%\) on USPTO. While for template-based models this is a result of insufficient coverage, we note that some of the models tested here are template-free, and yet they fail to generalize better than their template-based counterparts (this is similar to the findings of Tu et al. (2022)). To further ground our Pistachio results, we note that Jiang et al. (2023) report 66.1% top-5 accuracy when training on Pistachio directly (compared to our transfer results of up to 45%); however, these values are not fully comparable due to differences in preprocessing. Finally, RetroKNN is best or close to best on all metrics on both datasets, while also being one of the faster models in our re-evaluation. However, we caution the reader to not treat this as a final recommendation; as discussed in Section 2, existing single-step metrics provide a useful but incomplete view of the performance of single-step models.

### Multi-Step

We also ran search experiments combining various single-step models and search algorithms. As our primary objectives are to outline good practices, correct established numbers, and showcase synthesis, we only show preliminary multi-step results; we leave a final determination of which end-to-end pipeline is best to future work building on top of our framework. It is worth noting that the single-step models considered here use various (usually conflicting) versions of deep learning frameworks and other libraries, yet due to minimalistic dependencies synthesesu can be combined with any of them.

SetupWe followed one of the experimental setups from Tripp et al. (2022) and used the 190 target molecules from the Retro* Hard set (Chen et al., 2020). We combined each of the seven single-step models with two search algorithms: an MCTS variant and Retro* (Chen et al., 2020). All runs were limited to \(10\) minutes per molecule. Our single-step model wrappers expose the underlying output probabilities, which are used by both algorithms to guide the search. To ensure a fair comparison, the hyperparameters of each algorithm-model combination were tuned separately (see Appendix F for the exact procedure; qualitatively this step was especially important for MCTS).

ResultsWe show the results in Figure 2, tracking when the first solution was found as well as the maximum number of non-overlapping routes that can be extracted from the search graph. For LocalRetro, MEGAN, RetroKNN and RootAligned, both search algorithms are able to find several disjoint routes for most molecules, while for other models search can only solve a minority of the targets. Notably, RootAligned obtains promising results despite making less than 30 calls on average (due to its high computational cost). However, as discussed in Section 2.2, these results should not be treated as a final comparison of the models and rather serve as a starting point towards future research.

## 5 Conclusion and Future Work

In this paper we presented an analysis of pitfalls and best practices for evaluating retrosynthesis programs (Section 2), a software package called synthesesu to help researchers benchmark their methods following these best practices (Section 3), and used synthesesu to re-evaluate many existing models and algorithms (Section 4). These results "set the record straight" regarding the performance of existing algorithms, and the standardized evaluation protocol of synthesesu can ensure that future works do not continue to make the same mistakes. We encourage members of the community to contribute new models, algorithms, and metrics to synthesesu (see maintenance plan in Appendix G).

Despite this, several important issues remain in the field, which we plan to resolve with synthesesu in future iterations. As we argue in Section 2, existing metrics of recall (S1) and solve rate (M1/M2) are not ideal for comparing arbitrary end-to-end retrosynthesis pipelines. Assuming evaluation by chemists (M6) is not possible, we believe the most plausible substitute is to develop reaction "feasibility" models to estimate whether reactions will succeed. If such models were used post-hoc (not available during training or search), they could be used to evaluate the precision of single-step models (resolving S1) and assign a feasibility score to entire routes (resolving M1/M2). We designed synthesesu with this in mind and have a clear way to support feasibility models in both single-step and multi-step evaluation. However, how to train a high-quality feasibility model is an open research question which we leave to future work. Finally, the lack of information on reaction conditions, required quality of the starting materials, required equipment, and purification, is a significant barrier to actually executing the synthesis plans from CASP systems which synthesesu does not address. We encourage the community to work together with us on these challenges in the future.

Author ContributionsThe single-step reaction prediction workstream was led by Krzysztof, who built the majority of the codebase, ran the experiments, and integrated LocalRetro and GLN. Others contributed to specific components: Guoqing (parts of evaluation pipeline, Pistachio data preprocessing, integrating RootAligned), Megan (parts of evaluation pipeline, integrating MEGAN and Chemformer), Shufang (integrating RetroKNN), Piotr (analysing the utility of Chemformer as a feasibility model), Philipp (integrating MHNreact) and Marwin (Pistachio data analysis and preprocessing, template extraction). The multi-step search workstream was led by Austin, who implemented all of the search algorithms and ran initial experiments, advised jointly by Krzysztof and Marwin. Experiments with search algorithms were continued by Krzysztof, who performed additional tuning and produced the final results. Writing was done together by Krzysztof and Austin, with help from Marwin and Guoqing, as well as comments from the other authors. Both workstreams were supervised by Marwin.

AcknowledgmentsThe authors would like to thank Hannes Schulz, Maik Riechert, as well as the larger Microsoft Research team, for help with computing infrastructure that made this work possible.