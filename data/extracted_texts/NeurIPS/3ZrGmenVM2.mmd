# Deep Learning with Kernels through RKHM and the Perron-Frobenius Operator

Yuka Hashimoto

NTT Network Service Systems Laboratories /

RIKEN AIP,

Tokyo, Japan

yuka.hashimoto@ntt.com

&Masahiro Ikeda

RIKEN AIP / Keio University,

Tokyo, Japan

masahiro.ikeda@riken.jp

Hachem Kadri

Aix-Marseille University, CNRS, LIS,

Marseille, France

hachem.kadri@lis-lab.fr

###### Abstract

Reproducing kernel Hilbert \(C^{*}\)-module (RKHM) is a generalization of reproducing kernel Hilbert space (RKHS) by means of \(C^{*}\)-algebra, and the Perron-Frobenius operator is a linear operator related to the composition of functions. Combining these two concepts, we present deep RKHM, a deep learning framework for kernel methods. We derive a new Rademacher generalization bound in this setting and provide a theoretical interpretation of benign overfitting by means of Perron-Frobenius operators. By virtue of \(C^{*}\)-algebra, the dependency of the bound on output dimension is milder than existing bounds. We show that \(C^{*}\)-algebra is a suitable tool for deep learning with kernels, enabling us to take advantage of the product structure of operators and to provide a clear connection with convolutional neural networks. Our theoretical analysis provides a new lens through which one can design and analyze deep kernel methods.

## 1 Introduction

Kernel methods and deep neural networks are two major topics in machine learning. Originally, they had been investigated independently. However, their interactions have been researched recently. One important perspective is deep kernel learning . In this framework, we construct a function with the composition of functions in RKHSs, which is learned by given training data. Representer theorems were shown for deep kernel methods, which guarantee the representation of solutions of minimization problems only with given training data . We can combine the flexibility of deep neural networks with the representation power and solid theoretical understanding of kernel methods. Other important perspectives are neural tangent kernel  and convolutional kernel , which enable us to understand neural networks using the theory of kernel methods. In addition, Bietti et al.  proposed a regularization of deep neural network through a kernel perspective.

The generalization properties of kernel methods and deep neural networks have been investigated. One typical technique for bounding generalization errors is to use the Rademacher complexity . For kernel methods, generalization bounds based on the Rademacher complexity can be derived by the reproducing property. Bounds for deep kernel methods and vector-valued RKHSs (vvRKHSs) were also derived . Table 1 shows existing Rademacher generalization bounds for kernel methods. Generalization bounds for deep neural networks have also been actively studied . Recently, analyzing the generalization property using the concept of benign overfittinghas emerged [20; 21]. Unlike the classical interpretation of overfitting, which is called catastrophic overfitting, it explains the phenomenon that the model fits both training and test data. For kernel regression, it has been shown that the type of overfitting can be described by an integral operator associated with the kernel .

In this paper, we propose deep RKHM to make the deep kernel methods more powerful. RKHM is a generalization of RKHS by means of \(C^{*}\)-algebra [23; 24; 25], where \(C^{*}\)-algebra is a generalization of the space of complex numbers, regarded as space of operators. We focus on the \(C^{*}\)-algebra of matrices in this paper. Applications of RKHMs to kernel methods have been investigated recently [26; 27]. We generalize the concept of deep kernel learning to RKHM, which constructs a map from an operator to an operator as the composition of functions in RKHMs. The product structure of operators induces interactions among elements of matrices, which enables us to capture relations between data components. Then, we derive a generalization bound of the proposed deep RKHM. By virtue of \(C^{*}\)-algebras, we can use the operator norm, which alleviates the dependency of the generalization bound on the output dimension.

We also use Perron-Frobenius operators, which are linear operators describing the composition of functions and have been applied to analyzing dynamical systems [28; 29; 30; 31], to derive the bound. The compositions in the deep RKHM are effectively analyzed by the Perron-Frobenius operators.

\(C^{*}\)-algebra and Perron-Frobenius operator are powerful tools that provide connections of the proposed deep RKHM with existing studies. Since the norm of the Perron-Frobenius operator is described by the Gram matrix associated with the kernel, our bound shows a connection of the deep RKHM with benign overfitting. In addition, the product structure of a \(C^{*}\)-algebra enables us to provide a connection between the deep RKHMs and convolutional neural networks (CNNs).

Our main contributions are as follows.

* We propose deep RKHM, which is a generalization of deep kernel method by means of \(C^{*}\)-algebra. We can make use of the products in \(C^{*}\)-algebra to induce interactions among data components. We also show a representer theorem to guarantee the representation of solutions only with given data.
* We derive a generalization bound for deep RKHM. The dependency of the bound on the output dimension is milder than existing bounds by virtue of \(C^{*}\)-algebras. In addition, the Perron-Frobenius operators provide a connection of our bound with benign overfitting.
* We show connections of our study with existing studies such as CNNs and neural tangent kernel. We emphasize that our theoretical analysis using \(C^{*}\)-algebra and Perron-Frobenius operators gives a new and powerful lens through which one can design and analyze kernel methods.

## 2 Preliminaries

### \(C^{*}\)-algebra and reproducing kernel \(C^{*}\)-module

\(C^{*}\)-algebra, which is denoted by \(\) in the following, is a Banach space equipped with a product and an involution satisfying the \(C^{*}\) identity (condition 3 below).

**Definition 2.1** (\(C^{*}\)-algebra): _A set \(\) is called a \(C^{*}\)-algebra if it satisfies the following conditions:_

1. \(\) _is an algebra over_ \(\) _and equipped with a bijection_ \(()^{*}:\) _that satisfies the following conditions for_ \(,\) _and_ \(a,b\)_:_ \[\;( a+ b)^{*}=a^{*}+b^{*}, \;(ab)^{*}=b^{*}a^{*},\;(a^{*})^{*}=a.\]

 Reproducing space & Output dimension & Shallow & Deep \\  RKHS & 1 & \(O()\) & \(O(A^{L})\) \\ vvRKHS & \(d\) & \(O()\)[12; 13] & \(O(A^{L})\) \\ RKHM (existing) & \(d\) & \(O()\) & \(-\) \\ RKHM (ours) & \(d\) & \(O(d^{1/4}/)\) & \(O(B^{L}d^{1/4})\) \\ 

Table 1: Existing generalization bounds for kernel methods based on the Rademacher complexity and our bound (\(n\): sample size, \(A\): Lipschitz constant regarding the positive definite kernel, \(B\): The norm of the Perronâ€“Frobenius operator)2. \(\) _is a normed space endowed with_ \(\|\|_{}\)_, and for_ \(a,b\)_,_ \(\|ab\|_{}\|a\|_{}\|b\|_{}\) _holds. In addition,_ \(\) _is complete with respect to_ \(\|\|_{}\)_._
3. _For_ \(a\)_, the_ \(C^{*}\) _identity_ \(\|a^{*}a\|_{}=\|a\|_{}^{2}\) _holds._

**Example 2.2**: _A typical example of \(C^{*}\)-algebras is the \(C^{*}\)-algebra of \(d\) by \(d\) circulant matrices. Another example is the \(C^{*}\)-algebra of \(d\) by \(d\) block matrices with \(M\) blocks and their block sizes are \(=(m_{1},,m_{M})\). We denote them by \(Circ(d)\) and \(Block(,d)\), respectively. See  for more details about these examples._

We now define RKHM. Let \(\) be a non-empty set for data.

**Definition 2.3** (\(\)-valued positive definite kernel): _An \(\)-valued map \(k:\) is called a positive definite kernel if it satisfies the following conditions: \(\)\(k(x,y)=k(y,x)^{*}\) for \(x,y\), \(\)\(_{i,j=1}^{n}c_{i}^{*}k(x_{i},x_{j})c_{j}\) is positive semi-definite for \(n\), \(c_{i}\), \(x_{i}\)._

Let \(:^{}\) be the _feature map_ associated with \(k\), defined as \((x)=k(,x)\) for \(x\) and let \(_{k,0}=\{_{i=1}^{n}(x_{i})c_{i}|\ n,\ c_{i} ,\ x_{i}\ (i=1,,n)\}\). We can define an \(\)-valued map \(,_{_{k}}:_{k,0} _{k,0}\) as

\[_{i=1}^{n}(x_{i})c_{i},_{j=1}^{l}(y_{j})b_{j} _{_{k}}:=_{i=1}^{n}_{j=1}^{l}c_{i}^{*}k(x_{i},y_{j})b_{j},\]

which enjoys the reproducing property \((x),f_{_{k}}=f(x)\) for \(f_{k,0}\) and \(x\). The _reproducing kernel Hilbert \(\)-module (RKHM)_\(_{k}\) associated with \(k\) is defined as the completion of \(_{k,0}\). See, for example, the references  for more details about \(C^{*}\)-algebra and RKHM.

### Perron-Frobenius operator on RKHM

We introduce Perron-Frobenius operator on RKHM . Let \(_{1}\) and \(_{2}\) be nonempty sets and let \(k_{1}\) and \(k_{2}\) be \(\)-valued positive definite kernels. Let \(_{1}\) and \(_{2}\) be RKHMs associated with \(k_{1}\) and \(k_{2}\), respectively. Let \(_{1}\) and \(_{2}\) be the feature maps of \(_{1}\) and \(_{2}\), respectively. We begin with the standard notion of linearity in RKHMs.

**Definition 2.4** (\(\)-linear): _A linear map \(A:_{1}_{2}\) is called \(\)-linear if for any \(a\) and \(w_{1}\), we have \(A(wa)=(Aw)a\)._

**Definition 2.5** (Perron-Frobenius operator): _Let \(f:_{1}_{2}\) be a map. The Perron-Frobenius operator with respect to \(f\) is an \(\)-linear map \(P_{f}:_{1}_{2}\) satisfying_

\[P_{f}_{1}(x)=_{2}(f(x)).\]

Note that a Perron-Frobenius operator is not always well-defined since \(ac=bc\) for \(a,b\) and nonzero \(c\) does not always mean \(a=b\).

**Definition 2.6** (\(\)-linearly independent): _The set \(\{_{1}(x)\ |\ x\}\) is called \(\)-linearly independent if it satisfies the following condition: For any \(n\), \(x_{1},,x_{n}\), and \(c_{1},,c_{n}\), "\(_{i=1}^{n}_{1}(x_{i})c_{i}=0\)" is equivalent to "\(c_{i}=0\) for all \(i=1,,n\)"._

**Lemma 2.7**: _If \(\{_{1}(x)\ |\ x\}\) is \(\)-linearly independent, then \(P_{f}\) is well-defined._

The following lemma gives a sufficient condition for \(\)-linearly independence.

**Lemma 2.8**: _Let \(k_{1}=a\), i.e., \(k\) is separable, for an invertible operator \(a\) and a complex-valued kernel \(\). Assume \(\{(x)\ |\ x\}\) is linearly independent (e.g. \(\) is Gaussian or Laplacian), where \(\) is the feature map associated with \(\). Then, \(\{_{1}(x)\ |\ x\}\) is \(\)-linearly independent._

Note that separable kernels are widely used in existing literature of vvRKHS (see e.g. ). Lemma 2.8 guarantees the validity of the analysis with Perron-Frobenius operators, at least in the separable case. This provides a condition for "good kernels" by means of the well-definedness of the Perron-Frobenius operator.

NotationWe denote the Euclidean inner product and norm by \(,\) and \(\|\|\) without the subscript. For \(a\), let \(|a|_{}\) be the unique element in \(\) satisfying \(|a|_{}^{2}=a^{*}a\). If \(a\) is a matrix, \(\|a\|_{}\) is the Hilbert-Schmidt norm of \(a\). The operator norm of a linear operator \(A\) on an RKHM is denoted by \(\|A\|_{}\). All the technical proofs are documented in the supplementary.

## 3 Deep RKHM

We now construct an \(L\)-layer deep RKHM. Let \(=^{d d}\) be the \(C^{*}\)-algebra of \(d\) by \(d\) matrices. Let \(_{0},,_{L}\) be \(C^{*}\)-subalgebras of \(\) and for \(j=1,,L\) and \(k_{j}:_{j-1}_{j-1}_{j}\) be an \(_{j}\)-valued positive definite kernel for the \(j\)th layer. For \(j=1,,L\), we denote by \(_{j}\) the RKHM over \(_{j}\) associated with \(k_{j}\). In addition, we denote by \(}_{j}\) the RKHM over \(\) associated with \(k_{j}\). Note that \(_{j}\) is a subspace of \(}_{j}\) and for \(u,v_{j}\), we have \( u,v_{_{j}}= u,v_{_{j}}\). We set the function space corresponding to each layer as \(_{L}=\{f_{L}\ \ f(x)^{d d}\) for any \(x_{L-1},\ \|f\|_{_{L}} B_{L}\}\) and \(_{j}=\{f_{j}\ \ f(x)^{d d}\) for any \(x_{j-1},\ \|P_{f}\|_{} B_{j}\}\) for \(j=1,,L-1\). Here, for \(f_{j}\) with \(j=1,,L-1\), \(P_{f}\) is the Perron-Frobenius operator with respect to \(f\) from \(}_{j}\) to \(}_{j+1}\). We assume the well-definedness of these operators. Then, we set the class of deep RKHM as

\[_{L}^{}=\{f_{L} f_{1}\ \ f_{j} _{j}\ (j=1,,L)\}.\]

Figure 1 schematically shows the structure of the deep RKHM.

**Example 3.1**: _We can set \(_{j}=Block((m_{1,j},,m_{M_{l},j}),d)\), the \(C^{*}\)-algebra of block diagonal matrices. By setting \(M_{1} M_{l}\) and \(M_{l} M_{L}\) for \(l<L\), the number of nonzero elements in \(_{j}\) decreases during \(1 j l\) and increases during \(l j L\). This construction is regarded as an autoencoder, where the \(1 l\)th layer corresponds to the encoder and the \(l+1 L\)th layer corresponds to the decoder._

Advantage over existing deep kernel methodsNote that deep kernel methods with RKHSs and vvRKHSs have been proposed [1; 4; 2]. Autoencoders using RKHSs and vvRKHSs were also proposed [3; 5]. We have at least three advantages of deep RKHM over deep vvRKHSs or RKHSs: 1) useful structures of matrices stated in Remark 5.2, 2) availability of the operator norm in the generalization bound stated in Section 4, 3) connection with CNNs stated in Subsection 6.1.

## 4 Generalization Bound

We derive a generalization error for deep RKHM. To derive the bound, we bound the Rademacher complexity, which is one of the typical methods on deriving generalization bounds . Let \(\) be a probability space equipped with a probability measure \(P\). We denote the integral \(_{}s()P()\) of a measurable function \(s\) on \(\) by \([s]\). Let \(x_{1},,x_{n}\) and \(y_{1},,y_{n}\) be input and output samples from the distributions of \(_{0}\) and \(_{L}\)-valued random variables \(x\) and \(y\), respectively. Let \(_{i,j}\ (i=1,,d,\ j=1,,n)\) be i.i.d. Rademacher variables. Let \(_{j}=(_{1,j},,_{d,j})\). For an \(^{d}\)-valued function class \(\) and \(=(x_{1},,x_{n})\), the empirical Rademacher complexity \(_{n}(,)\) is defined by \(_{n}(,):=[_{f}_{ i=1}^{n}_{i},f(x_{i})]/n\).

### Bound for shallow RKHMs

We use the operator norm to derive a bound, whose dependency on output dimension is milder than existing bounds. Availability of the operator norm is one of the advantages of considering

Figure 1: Overview of the proposed deep RKHM. The small blue squares represent matrix elements. In the case of the autoencoder (see Example 3.1), \(f_{1} f_{2}\) is the encoder, and \(f_{3} f_{4}\) is the decoder.

\(C^{*}\)-algebras (RKHMs) instead of vectors (vvRKHSs). Note that although we can also use the Hilbert-Schmidt norm for matrices, it tends to be large as the dimension \(d\) becomes large. On the other hand, the operator norm is defined independently of the dimension \(d\). Indeed, the Hilbert-Schmidt norm of a matrix \(a^{d d}\) is calculated as \((_{i=1}^{d}s_{i}^{2})^{1/2}\), where \(s_{i}\) is the \(i\)th singular value of \(a\). The operator norm of \(a\) is the largest singular value of \(a\).

To see the derivation of the bound, we first focus on the case of \(L=1\), i.e., the network is shallow. Let \(E>0\). For a space \(\) of \(\)-valued functions on \(_{0}\), let \(()=\{(x,y) f(x)-y\;\;f,\|y\|_{ } E\}\). The following theorem shows a bound for RKHMs with the operator norm.

**Theorem 4.1**: _Assume there exists \(D>0\) such that \(\|k_{1}(x,x)\|_{} D\) for any \(x_{0}\). Let \(=4(B_{1}+E)B_{1}\) and \(=6(B_{1}+E)^{2}\). Let \((0,1)\). Then, for any \(g(_{1})\), where \(_{1}\) is defined in Section 3, with probability at least \(1-\), we have_

\[\|[|g(x,y)|_{}^{2}]\|_{}\| _{i=1}^{n}|g(x_{i},y_{i})|_{}^{2}\|_{}+}{n}_{i=1}^{n}\,k_{1}(x_{i},x_{i})^{1/ 2}+}.\]

Theorem 4.1 is derived by the following lemmas. We first fix a vector \(p^{d}\) and consider the operator-valued loss function acting on \(p\). We first show a relation between the generalization error and the Rademacher complexity of vector-valued functions. Then, we bound the Rademacher complexity. Since the bound does not depend on \(p\), we can finally remove the dependency on \(p\).

**Lemma 4.2**: _Let \(\) be a function class of \(^{d d}\)-valued functions on \(_{0}\) bounded by \(C\) (i.e., \(\|f(x)\|_{} C\) for any \(x_{0}\)). Let \(}(,p)=\{(x,y)\|(f(x)-y)p\|^{2}\;\;f ,\|y\|_{} E\}\) and \(M=2(C+E)^{2}\). Let \(p^{d}\) satisfy \(\|p\|=1\) and let \((0,1)\). Then, for any \(g}(,p)\), with probability at least \(1-\), we have_

\[\|[|g(x,y)|_{}^{2}]^{1/2}p\|^{2}\| _{i=1}^{n}|g(x_{i},y_{i})|_{}^{2}\|_{}+2 _{n}(,}(,p))+3M}.\]

**Lemma 4.3**: _With the same notations in Lemma 4.2, let \(K=2(C+E)\). Then, we have \(_{n}(,(,p)) K_{n}(,p)\), where \(p=\{x f(x)p\;\;f\}\)._

**Lemma 4.4**: _Let \(p^{d}\) satisfy \(\|p\|=1\). For \(_{1}\) defined in Section 3, we have_

\[_{n}(,_{1}p)}{n}_{i=1}^{ n}(k(x_{i},x_{i}))^{1/2}.\]

### Bound for deep RKHMs

We now generalize Theorem 4.1 to the deep setting (\(L 2\)) using the Perron-Frobenius operators.

**Theorem 4.5**: _Assume there exists \(D>0\) such that \(\|k_{L}(x,x)\|_{} D\) for any \(x_{0}\). Let \(=4(B_{L}+E)B_{1} B_{L}\) and \(=6(B_{L}+E)^{2}\). Let \((0,1)\). Then, for any \(g(_{L}^{})\), with probability at least \(1-\), we have_

\[\|[|g(x,y)|_{}^{2}]\|_{}\| _{i=1}^{n}|g(x_{i},y_{i})|_{}^{2}\|_{}+}{n}_{i=1}^{n}\,k_{1}(x_{i},x_{i})^{1/ 2}+}.\]

We use the following proposition and Lemmas 4.2 and 4.3 to show Theorem 4.5. The key idea of the proof is that by the reproducing property and the definition of the Perron-Frobenius operator, we get \(f_{L} f_{1}(x)=_{L}(f_{L-1} f_{1}(x) ),f_{L}_{}_{L}}=P_{f_{L-1}} P_{f_ {1}}(x),f_{L}_{}_{L}}\).

**Proposition 4.6**: _Let \(p^{d}\) satisfy \(\|p\|=1\). Then, we have_

\[_{n}(,_{L}^{}p)_{(f _{j}_{j})_{j}}\|P_{f_{L-1}} P_{f_{1}}|_{}( )}\|_{}\;\|f_{L}\|_{_{L}}\;_{i=1}^ {n}(k_{1}(x_{i},x_{i}))^{1/2}.\]

_Here, \(}()\) is the submodule of \(}_{1}\) generated by \(_{1}(x_{1}),_{1}(x_{n})\)._

**Corollary 4.7**: _Let \(p^{d}\) satisfy \(\|p\|=1\). Then, we have_

\[_{n}(,_{L}^{ deep}p)B_{1} B_{ L}_{i=1}^{n}(k_{1}(x_{i},x_{i}))^{1/2}.\]

Comparison to deep vvRKHSWe can also regard \(^{d d}\) as the Hilbert space equipped with the Hilbert-Schmidt inner product, i.e., we can flatten matrices and get \(d^{2}\)-dimensional Hilbert space. In this case, the corresponding operator-valued kernel is the multiplication operator of \(k(x,y)\), which we denote by \(M_{k(x,y)}\). Then, we can apply existing results for vvRKHSs [5; 12], which involve the term \((_{i=1}^{n}(M_{k(x_{i},x_{i})}))^{1/2}\). It is calculated as

\[_{i=1}^{n}(M_{k(x_{i},x_{i})})=_{i=1}^{n}_{j,l=1} ^{d} e_{jl},k(x_{i},x_{i})e_{jl}_{ HS}=_{i=1}^ {n}_{j,l=1}^{d}k(x_{i},x_{i})_{l,l}=d_{i=1}^{n}k(x_{ i},x_{i}).\]

Thus, using the existing approaches, we have the factor \((d_{i=1}^{n}k(x_{i},x_{i}))^{1/2}\). On the other hand, we have the smaller factor \((_{i=1}^{n}k(x_{i},x_{i}))^{1/2}\) in Theorems 4.1 and 4.5. Using the operator norm, we can reduce the dependency on the dimension \(d\).

## 5 Learning Deep RKHMs

We focus on the practical learning problem. To learn deep RKHMs, we consider the following minimization problem based on the generalization bound derived in Section 4:

\[_{(f_{j}_{j})_{j}}\|_{i=1}^{n}|f_{L}  f_{1}(x_{i})-y_{i}|_{}^{2}\|_{}+ _{1}\|P_{f_{L-1}} P_{f_{1}}|_{()}\|_{ op}+ _{2}\|f_{L}\|_{_{L}}. \]

The second term regarding the Perron-Frobenius operators comes from the bound in Proposition 4.6. We try to reduce the generalization error by reducing the magnitude of the norm of the Perron-Frobenius operators.

### Representer theorem

We first show a representer theorem to guarantee that a solution of the minimization problem (1) is represented only with given samples.

**Proposition 5.1**: _Let \(h:^{n}^{n}_{+}\) be an error function, let \(g_{1}\) be an \(_{+}\)-valued function on the space of bounded linear operators on \(}_{1}\), and let \(g_{2}:_{+}_{+}\) satisfy \(g_{2}(a) g_{2}(b)\) for \(a b\). Assume the following minimization problem has a solution:_

\[_{(f_{j}_{j})_{j}}h(f_{L} f_{1}(x_{1}),,f_{L} f_{1}(x_{n}))+g_{1}(P_{f_{L-1}} P_{f_{L}}|_{()})+g_{2}(\|f_{L}\|_{_{L}}).\]

_Then, there exists a solution admitting a representation of the form \(f_{j}=_{i=1}^{n}_{j}(x_{i}^{j-1})c_{i,j}\) for some \(c_{1,j},,c_{n,j}\) and for \(j=1,,L\). Here, \(x_{i}^{j}=f_{j} f_{1}(x_{i})\) for \(j=1,,L\) and \(x_{i}^{0}=x_{i}\)._

**Remark 5.2**: _An advantage of deep RKHM compared to deep vvRKHS is that we can make use of the structure of matrices. For example, the product of two diagonal matrices is calculated by the element-wise product of diagonal elements. Thus, when \(_{1}==_{L}=Block((1,,1),d)\), interactions among elements in an input are induced only by the kernels, not by the product \(k_{j}(x,x_{i}^{j-1}) c_{i,j}\). That is, the form of interactions does not depend on the learning parameter \(c_{i,j}\). On the other hand, if we set \(_{j}=Block(_{j},d)\) with \(_{j}=(m_{1,j},,m_{M_{j},j})(1,,1)\), then at the \(j\)th layer, we get interactions among elements in the same block through the product \(k_{j}(x,x_{i}^{j-1}) c_{i,j}\). In this case, the form of interactions is learned through \(c_{i,j}\). For example, the part \(M_{1} M_{l}\) (the encoder) in Example 3.1 tries to gradually reduce the dependency among elements in the output of each layer and describe the input with a small number of variables. The part \(M_{l} M_{L}\) (the decoder) tries to increase the dependency._

### Computing the norm of the Perron-Frobenius operator

We discuss the practical computation and the role of the factor \(\|P_{f_{L-1}} P_{f_{1}}|_{}()}\|_{}\) in Eq. (1). Let \(G_{j}^{n n}\) be the Gram matrix whose \((i,l)\)-entry is \(k_{j}(x_{i}^{j-1},x_{l}^{j-1})\).

**Proposition 5.3**: _For \(j=1,L\), let \([_{j}(x_{1}^{j-1}),,_{j}(x_{n}^{j-1})]R_{j}=Q_{j}\) be the QR decomposition of \([_{j}(x_{1}^{j-1}),,_{j}(x_{n}^{j-1})]\). Then, we have \(\|P_{f_{L-1}} P_{f_{1}}|_{}()}\|_{} =\|R_{L}^{*}G_{L}R_{1}\|_{}\)._

The computational cost of \(\|R_{L}^{*}G_{L}R_{1}\|_{}\) is expensive if \(n\) is large since computing \(R_{1}\) and \(R_{L}\) is expensive. Thus, we consider upper bounding \(\|R_{L}^{*}G_{L}R_{1}\|_{}\) by a computationally efficient value.

**Proposition 5.4**: _Assume \(G_{L}\) is invertible. Then, we have_

\[\|P_{f_{L-1}} P_{f_{1}}|_{}()}\|_{} \|G_{L}^{-1}\|_{}^{1/2}\|G_{L}\|_{}\|G_{1}^{-1}\|_{ }^{1/2}. \]

Since \(G_{1}\) is independent of \(f_{1},,f_{L}\), to make the value \(\|P_{f_{L-1}} P_{f_{1}}|_{}()}\|_{}\) small, we try to make the norm of \(G_{L}\) and \(G_{L}^{-1}\) small according to Proposition 5.4. For example, instead of the second term regarding the Perron-Frobenius operators in Eq. (1), we can consider the following term with \(>0\):

\[_{1}(\|( I+G_{L})^{-1}\|_{}+\|G_{L}\|_{}).\]

Note that the term depends on the training samples \(x_{1},,x_{n}\). The situation is different from the third term in Eq. (1) since the third term does not depend on the training samples before applying the representer theorem. If we try to minimize a value depending on the training samples, the model seems to be more specific for the training samples, and it may cause overfitting. Thus, the connection between the minimization of the second term in Eq. (1) and generalization cannot be explained by the classical argument about generalization and regularization. However, as we will see in Subsection 6.2, it is related to benign overfitting and has a good effect on generalization.

**Remark 5.5**: _The inequality (2) implies that as \(\{_{L}(x_{1}^{L-1}),,_{L}(x_{n}^{L-1})\}\) becomes nearly linearly dependent, the Rademacher complexity becomes large. By the term \(\|( I+G_{L})^{-1}\|\) in Eq. (1), the function \(f_{L-1} f_{1}\) is learned so that it separates \(x_{1},,x_{n}\) well._

**Remark 5.6**: _To evaluate \(f_{1} f_{L}(x_{i})\) for \(i=1,,n\), we have to construct a Gram matrix \(G_{j}_{j}^{n n}\), and compute the product of the Gram matrix and a vector \(c_{j}_{j}^{n}\) for \(j=1,,L\). The computational cost of the construction of the Gram matrices does not depend on \(d\). The cost of computing \(G_{j}c_{j}\) is \(O(n^{2}_{j})\), where \(_{j}\) is the number of nonzero elements in the matrices in \(_{j}\). Note that if \(_{j}\) is the \(C^{*}\)-algebra of block diagonal matrices, then we have \(_{j} d^{2}\). Regarding the cost with respect to \(n\), if the positive definite kernel is separable, we can use the random Fourier features  to replace the factor \(n^{2}\) with \(mn\) for a small integer \(m n\)._

## 6 Connection and Comparison with Existing Studies

The proposed deep RKHM is deeply related to existing studies by virtue of \(C^{*}\)-algebra and the Perron-Frobenius operators. We discuss the connection below.

### Connection with CNN

The proposed deep RKHM has a duality with CNNs. Let \(_{0}==_{L}=Circ(d)\), the \(C^{*}\)-algebra of \(d\) by \(d\) circulant matrices. For \(j=1,,L\), let \(a_{j}_{j}\). Let \(k_{j}\) be an \(_{j}\)-valued positive definite kernel defined as \(k_{j}(x,y)=_{j}(a_{j}x,a_{j}y)\), where \(_{j}\) is an \(_{j}\)-valued function. The \(_{j}\)-valued positive definite kernel makes the output of each layer become a circulant matrix, which enables us to apply the convolution as the product of the output and a parameter. Then, \(f_{j}\) is represented as \(f_{j}(x)=_{i=1}^{n}_{j}(a_{j}x,a_{j}x_{i}^{j-1})c_{i,j}\) for some \(c_{i,j}_{j}\). Thus, at the \(j\)th layer, the input \(x\) is multiplied by \(a_{j}\) and is transformed nonlinearly by \(_{j}(x)=_{i=1}^{n}k_{j}(x,a_{j}x_{i}^{j-1})c_{i,j}\). Since the product of two circulant matrices corresponds to the convolution, \(a_{j}\) corresponds to a filter. Inaddition, \(_{j}\) corresponds to the activation function at the \(j\)th layer. Thus, the deep RKHM with the above setting corresponds to a CNN. The difference between the deep RKHM and the CNN is the parameters that we learn. Whereas for the deep RKHM, we learn the coefficients \(c_{1,j},,c_{n,j}\), for the CNN, we learn the parameter \(a_{j}\). In other words, whereas for the deep RKHM, we learn the activation function \(_{j}\), for the CNN, we learn the filter \(a_{j}\). It seems reasonable to interpret this difference as a consequence of solving the problem in the primal or in the dual. In the primal, the number of the learning parameters depends on the dimension of the data (or the filter for convolution), while in the dual, it depends on the size of the data.

**Remark 6.1**: _The connection between CNNs and shallow RKHMs has already been studied . However, the existing study does not provide the connection between the filter and activation function. The above investigation shows a more clear layer-wise connection of deep RKHMs with CNNs._

### Connection with benign overfitting

Benign overfitting is a phenomenon that the model fits any amount of data yet generalizes well [20; 21]. For kernel regression, Mallinar et al.  showed that if the eigenvalues of the integral operator associated with the kernel function over the data distribution decay slower than any powerlaw decay, than the model exhibits benign overfitting. The Gram matrix is obtained by replacing the integral with the sum over the finite samples. The inequality (2) suggests that the generalization error becomes smaller as the smallest and the largest eigenvalues of the Gram matrix get closer, which means the eigenvalue decay is slower. Combining the observation in Remark 5.5, we can interpret that as the right-hand side of the inequality (2) becomes smaller, the outputs of noisy training data at the \(L-1\)th layer tend to be more separated from the other outputs. In other words, for the random variable \(x\) following the data distribution, \(f_{L-1} f_{1}\) is learned so that the distribution of \(f_{L-1} f_{1}(x)\) generates the integral operator with more separated eigenvalues, which appreciates benign overfitting. We will also observe this phenomenon numerically in Section 7 and Appendix C.3.2. Since the generalization bound for deep vrRKHSs  is described by the Lipschitz constants of the feature maps and the norm of \(f_{j}\) for \(j=1,,L\), this type of theoretical interpretation regarding benign overfitting is not available for the existing bound for deep vvRKHSs.

**Remark 6.2**: _The above arguments about benign overfitting are valid only for deep RKHMs, i.e., the case of \(L 2\). If \(L=1\) (shallow RKHM), then the Gram matrix \(G_{L}=[k(x_{i},x_{j})]_{i,j}\) is fixed and determined only by the training data and the kernel. On the other hand, if \(L 2\) (deep RKHM), then \(G_{L}=[k_{L}(f_{L-1} f_{1}(x_{i}),f_{L-1} f_{1 }(x_{j}))]_{i,j}\) depends also on \(f_{1},,f_{L-1}\). As a result, by adding the term using \(G_{L}\) to the loss function, we can learn proper \(f_{1},,f_{L-1}\) so that they make the right-hand side of Eq. (2) small, and the whole network overfits benignly. As \(L\) becomes large, the function \(f_{L-1} f_{1}\) changes more flexibly to attain a smaller value of the term. This is an advantage of considering a large \(L\)._

### Connection with neural tangent kernel

Neural tangent kernel has been investigated to understand neural networks using the theory of kernel methods [6; 7]. Generalizing neural networks to \(C^{*}\)-algebra, which is called the \(C^{*}\)-algebra network, is also investigated [32; 38]. We define a neural tangent kernel for the \(C^{*}\)-algebra network and develop a theory for combining neural networks and deep RKHMs as an analogy of the existing studies. Consider the \(C^{*}\)-algebra network \(f:^{N_{0}}\) over \(\) with \(\)-valued weight matrices \(W_{j}^{N_{j} N_{j-1}}\) and element-wise activation functions \(_{j}\): \(f(x)=W_{L}_{L-1}(W_{L-1}_{1}(W_{1}x))\). The \((i,j)\)-entry of \(f(x)\) is \(f_{i}(_{j})=_{L,i}_{L-1}(W_{L-1}_{1}(W_{ 1}_{j}))\), where \(_{j}\) is the \(i\)th column of \(x\) regarded as \(x^{M_{0} d}\) and \(_{L,i}\) is the \(i\)th row of \(W_{L}\) regarded as \(W_{L}^{d h_{L-1}}\). Thus, the \((i,j)\)-entry of \(f(x)\) corresponds to the output of the network \(f_{i}(_{j})\). We can consider the neural tangent kernel for each \(f_{i}\). Chen and Xu  showed that the RKHS associated with the neural tangent kernel restricted to the sphere is the same set as that associated with the Laplacian kernel. Therefore, the \(i\)th row of \(f(x)\) is described by the shallow RKHS associated with the neural tangent kernel \(k_{i}^{ NT}\). Let \(k^{ NN}\) be the \(\)-valued positive definite kernel whose \((i,j)\)-entry is \(k_{i}^{ NT}\) for \(i=j\) and \(0\) for \(i j\). Then, for any function \(g_{k^{ NN}}\), the elements in the \(i\)th row of \(g\) are in the RKHS associated with \(k_{i}^{ NT}\), i.e., associated with the Laplacian kernel. Thus, \(f\) is described by this shallow RKHM.

**Remark 6.3**: _We can combine the deep RKHM and existing neural networks by replacing some \(f_{j}\) in our model with an existing neural network. The above observation enables us to apply our results in Section 4 to the combined network._

### Comparison to bounds for classical neural networks

Existing bounds for classical neural networks typically depend on the product or sum of matrix \((p,q)\) norm of all the weight matrices \(W_{j}\)[14; 15; 16; 17]. Typical bound is \(O(_{j=1}^{L}\|W_{j}\|_{})\). Note that the Hilbert-Schmidt norm is the matrix \((2,2)\) norm. Unlike the operator norm, the matrix \((p,q)\) norm tends to be large as the width of the layers becomes large. On the other hand, the dependency of our bound on the width of the layer is not affected by the number of layers in the case where the kernels are separable. Indeed, assume we set \(k_{1}=_{1}a_{1}\) and \(k_{L}=_{L}a_{L}\) for some complex-valued kernels \(_{1}\) and \(_{2}\), \(a_{1}_{1}\), and \(a_{L}_{L}\). Then by Proposition 5.3, the factor \((L):=\|P_{f_{L-1}} P_{f_{1}}\|_{()}\|_{}\) is written as \(\|_{L}^{*}_{L}_{1} a_{L}^{2}a_{1}\|_{}=\|_{L}^{*}_{L}_{1}\|_{}\ \|a_{L}^{2}a_{1}\|_{ }\) for some \(_{L},_{L},_{1}^{n n}\). Thus, it is independent of \(d\). The only part depending on \(d\) is \(\,k_{1}(x_{i},x_{i})\), which results in the bound \(O((L))\). Note that the width of the \(j\)th layer corresponds to the number of nonzero elements in a matrix in \(_{j}\). We also discuss in Appendix B the connection of our bound with the bound by Koopman operators, the adjoints of the Perron-Frobenius operators .

## 7 Numerical Results

We numerically confirm our theory and the validity of the proposed deep RKHM.

Comparison to vvRKHSWe compared the generalization property of the deep RKHM to the deep vvRKHS with the same positive definite kernel. For \(d=10\) and \(n=10\), we set \(x_{i}=(az_{i})^{2}+_{i}\) as input samples, where \(a^{100 10}\) and \(z_{i}^{10}\) are randomly generated by \((0,0.1)\), the normal distribution of mean 0 and standard deviation 0.1, \(()^{2}\) is the elementwise product, and \(_{i}\) is the random noise drawn from \((0,1e-3)\). We reshaped \(x_{i}\) to a \(10\) by \(10\) matrix. We set \(L=3\) and \(k_{j}=I\) for \(j=1,2,3\), where \(\) is the Laplacian kernel. For RKHMs, we set \(_{1}=Block((1,,1),d)\), \(_{2}=Block((2,,2),d)\), and \(_{3}=^{d d}\). This is the autoencoder mentioned in Example 3.1. For vvRKHSs, we set the corresponding Hilbert spaces with the Hilbert-Schmidt inner product. We set the loss function as \(1/n\|_{i=1}^{n}|f(x_{i})-x_{i}|_{}^{2}\|_{}\) for the deep RKHM and as \(1/n_{i=1}^{n}\|f(x_{i})-x_{i}\|_{}^{2}\) for the deep vvRKHS. Here, \(f=f_{3} f_{2} f_{1}\). We did not add any terms to the loss function to see how the loss function with the operator norm affects the generalization performance. We computed the same value \(\|[|f(x)-x|_{}^{2}|]\|_{}-1/n\|_{i=1}^{n}|f (x_{i})-x_{i}|_{}^{2}\|_{}\) for both RKHM and vvRKHS. Figure 2 (a) shows the results. We can see that the deep RKHM generalizes better than the deep vvRKHS, only with the loss function and without any additional terms.

Observation about benign overfittingWe analyzed the overfitting numerically. For \(d=10\) and \(n=1000\), we randomly sampled \(d\) by \(d\) diagonal matrices \(x_{1},,x_{n}_{0}\) from the normal distribution \((0,0.1)\). We set \(y_{i}=x_{i}^{2}+_{i}\) for \(i=1,,n\), where \(_{i}\) is a noise drawn from the normal distribution \((0,0.001)\). The magnitude of the noise is \(10\)% of \(x_{i}^{2}\). In addition, we set \(L=2\), \(_{1}=^{d d}\), \(_{2}=Block((1,,1),d)\), and \(k_{j}\) as the same kernel as the above experiment. The additional term to the loss function is set as \(_{1}(\|( I+G_{L})^{-1}\|_{}+\|G_{L}\|_{})+ _{2}\|f_{L}\|_{_{L}}^{2}\), where \(=0.01\) and \(_{2}=0.01\) according to Subsection 5.2. We computed the generalization error for the cases of \(_{1}=0\) and \(_{1}=10^{2}\). Figure 2 (b) shows the result. We can see that the generalization error saturates without the additional term motivated by the Perron-Frobenius operator. On the other hand, with the additional term, the generalization error becomes small, which is the effect of benign overfitting.

Comparison to CNNWe compared the deep RKHM to a CNN on the classification task with MNIST . We set \(d=28\) and \(n=20\). We constructed a deep RKHM combined with a neural network with 2 dense layers. For the deep RKHM, we set \(L=2\), \(_{0}=^{d d}\), \(_{1}=Block((7,7,7,7),d)\), and \(_{2}=Block((4,,4),d)\). Then, two dense layers are added. See Subsection 6.3 about combining the deep RKHM with neural networks. Regarding the additional term to the loss function, we set the same term with the previous experiment with \(_{2}=0.001\) andset \(_{1}=1\) or \(_{1}=0\). To compare the deep RKHM to CNNs, we also constructed a network by replacing the deep RKHM with a CNN. The CNN is composed of 2 layers with \(7 7\) and \(4 4\) filters. Figure 2 (c) shows the test accuracy of these networks. We can see that the deep RKHM outperforms the CNN. In addition, we can see that the test accuracy is higher if we add the term regarding the Perron-Frobenius operators. We discuss the memory consumption and the computational cost of each of the deep RKHM and the CNN in Appendix C.3, and we empirically show that the deep RKHM outperforms the CNN that has the same size of learning parameters as the deep RKHM. We also show additional results about benign overfitting in Appendix C.3.

## 8 Conclusion and Limitations

In this paper, we proposed deep RKHM and analyzed it through \(C^{*}\)-algebra and the Perron-Frobenius operators. We derived a generalization bound, whose dependency on the output dimension is alleviated by the operator norm, and which is related to benign overfitting. We showed a representer theorem about the proposed deep RKHM, and connections with existing studies such as CNNs and neural tangent kernel. Our theoretical analysis shows that \(C^{*}\)-algebra and Perron-Frobenius operators are effective tools for analyzing deep kernel methods. The main contributions of this paper are our theoretical results with \(C^{*}\)-algebra and the Perron-Frobenius operators. More practical investigations are required for further progress. For example, although we numerically showed the validity of our method for the case where the number of samples is limited (the last experiment in Section 7), more experimental results for the case where the number of samples is large are useful for further analysis. Also, although we can apply random Fourier features (Remark 5.6) to reduce the computational costs, studying more efficient methods specific to deep RKHM remains to be investigated in future work. As for the theoretical topic, we assumed the well-definedness of the Perron-Frobenius operators. Though separable kernels with invertible matrices, which are typical examples of kernels, satisfy the assumption, generalization of our analysis to other kernels should also be studied in future work.