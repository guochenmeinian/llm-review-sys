# Beyond accuracy: understanding the performance of LLMs on exams designed for humans

Anonymous Author(s)

###### Abstract

Many recent studies of LLM performance have focused on the ability of LLMs to achieve outcomes comparable to humans on academic and professional exams. However, it is not clear whether such studies shed light on the extent to which models show reasoning ability, and there is controversy about the significance and implications of such results. We seek to look more deeply into the question of how and whether the performance of LLMs on exams designed for humans reflects true aptitude inherent in LLMs. We do so by making use of the tools of psychometrics which are designed to perform meaningful measurement in test taking. We leverage a unique dataset that captures the detailed performance of over 5M students across 8 college-entrance exams given over a span of two years in Brazil. With respect to the evaluation of LLM abilities, we show that the tools of Item Response Theory (IRT) provide a more informative evaluation of model performance than the usual accuracy metrics employed in previous studies. Digging deeper, we show that the modeling framework of IRT, by explicitly modeling the difficulty levels of questions, allows us to quantitatively distinguish between LLMs that answer questions in "human-like" patterns versus LLMs that do not. We also show how to quantitatively identify cases in which exam results are not reliable measurements of an LLM's ability. Using the tools of IRT we can also identify specific questions that appear to be either much easier, or much harder, for machines than for humans, and we give some reasons for those differences. Overall, our study shows that the conventional focus on accuracy as the primary performance metric for LLM studies does not allow us to deeply understand the true capabilities of LLMs and compare them to that of humans. Thus, we claim that psychometric modeling should play a larger role in the evaluation of LLM capabilities on exams designed for humans.

## 1 Introduction

Large Language Models (LLMs) have demonstrated an impressive ability in performing well on examinations designed for humans [25; 30], such as the US bar exam , the US Medical Licensing Exam , and many others [45; 53]. This yields controversy in how researchers should interpret such results, raising two kinds of criticisms of those apparent successes. The first is the potential for publicly-given exams (and answers) to leak into models' training data. The second, and more fundamental, issue is the notion of _construct validity_. Most exams given to humans are intended to measure a construct, e.g., legal analysis ability, medical analysis ability, etc. However, the reliability of these exams in measuring the relevant construct for non-humans is usually ignored, and exams that are valid in one context may not generalize across different groups, settings or tasks .

Formalizing the notion of construct validity in general is challenging. Since the 1950s, the field of psychometrics has been grappling with how to design examinations that validly measure human abilities along specific dimensions. The primary tool developed has been Item Response Theory (IRT) , which has been employed in psychology, medicine, and especially in educational testing. IRT formalizes the unobserved construct as a continuous latent variable, and models stochastic responses of humans to questions as a logistic regression conditional on that latent variable.

In this paper, we demonstrate how IRT can help shed light on whether LLMs are in fact showing human-like performance on exams intended for humans. As a case study, we use one of the largest university-entrance exams in the world, a dataset comprising the performance of over 5 million Brazilian students on eight multiple-choice exams administered over two years. Each exam was prepared and fitted to an IRT model by educational testing experts, giving us an unparalleled opportunity to examine the performance of LLMs in detail.

Our results show that the LLMs we study reveal performance patterns that are consistent with expected human behavior in many cases. Nonetheless, we also frequently observe significant deviation from human-like behavior. We demonstrate how to use the tools of IRT to quantitatively distinguish between human-like and non-human-like behavior. We then explore the differences between models and exam types that correlate with differences in response patterns. Lastly, we use the tools of IRT and psychometrics to identify cases where exams are not producing reliable estimates of LLM ability and understand why this happens. This occurs because exams are in some cases too difficult for the models, and in other cases too easy for them and as such they cannot properly measure the ability of certain LLMs.

Moving beyond conclusions about current models, the broader contribution of our study is to demonstrate the power of IRT as a framework for evaluating LLMs. For example, in Classical Test Theory (CTT), no attempt is made to assess the difficulty of individual questions, in-line with majority of in standard LLM benchmarks that pursues accuracy [8; 43; 16; 4]. In contrast, as we will show below, IRT simultaneously measures both test takers and exam questions (on the same scale). In doing so, IRT allows one to distinguish between test takers with similar CTT (accuracy) scores, but differing levels of true ability, by inspecting the _pattern_ of correct or incorrect answers given. Moreover, we deploy a broader set of tools (e.g., goodness-of-fit, Fisher information, discrimination index) which enable us to evaluate which are the cases in which fitting the IRT model to the LLMs response patterns gives us reliable estimates of the models' ability. Thus, we believe that the methods of our study represent a valuable step beyond the use of simple accuracy for assessing whether both current and future LLMs show human-like response patterns.

## 2 Related Work

Our study connects a number of research areas, spanning benchmarking LLMs, the applications of item response theory, and the evaluation of LLMs using exams designed for humans.

**Benchmarking LLMs.** The most common strategy to evaluate LLMs is through traditional large-scale NLP benchmarks [46; 40; 11; 18; 16; 19; 4]. Conventionally, benchmark evaluation relies on some notion of accuracy - the number of correct answers - as a proxy for ability [8; 43]. A key distinction of our study is to draw attention to the limitations of the use of accuracy alone  for evaluating the performance of LLMs on benchmarks in understanding the similarity between the performance of models versus humans.

**LLMs and Exams Designed for Humans.** Many attempts to evaluate LLMs use exams designed for humans, e.g., at college-entrance [1; 26] or college-level [14; 37; 47; 13; 41; 53]. These exams also generally use accuracy as a metric of ability; one focus of our work is on how to use IRT analysis to determine when such exams in fact perform meaningful measurement.

The Brazilian nationwide college-entrance exams we use in this work (ENEM), detailed in Section 4.1, were used in previous efforts to evaluate NLP models [38; 39; 26]. However, those studies only used accuracy and did not make use of the IRT models associated with the exam, which is a central aspect our work.

**IRT in Machine Learning.** Work in psychometrics (i.e., the measurement of human cognitive abilities), detailed in Section 3, has shown that using accuracy as a exam score may not reflect the true underlying abilities of individuals . As a result, IRT has been advocated for use in machine learning (ML) as an improved tool for benchmarking. The authors in  show that it is possible to produce rankings of NLP models which are more reliable and stable using IRT than accuracy. Item response theory has also been shown to help in spotting noisy questions, identifying overfitting,selecting features, and designing better benchmarks for ML [29; 35; 20; 54; 22]. However, there is a critical difference between the previous uses of IRT in ML and our work. Previous work uses IRT by training an IRT model on the results of ML models solving question-answering or classification questions. Our method is different: we leverage the fact that we have access to an IRT model trained on _human responses,_ and we do not retrain on _model responses_. We take this approach because a central goal of our study is to explore whether LLMs are in fact following response patterns _as exhibited by human test takers_.

Finally, we note that  shares some goals with our work. The investigation seeks to understand whether LLMs show human-like response biases in surveys. We also look at the question of whether LLMs show human-like response patterns, but we study the question along different dimensions: (a) patterns of correct and incorrect answers in exams; and (b) the ways in which LLMs choose incorrect answers. Additionally, Xia _et al._ recognize that accuracy as a single metric does not capture errors LLMs can make in intermediate steps when solving mathematical tasks, and they systematically study those errors.

## 3 Background

In this section, we give some background of the tools we use from psychometrics.

**Classical Test Theory (CTT):** CTT  evaluates test takers based on the fraction of questions they answer correctly. We call this score _accuracy_ or _CTT_ score of the test taker and we use these two terms interchangeably. Inadequately, CTT does not differentiate between difficult and easy questions, nor does it take into consideration the _patterns of correct answers_. For example, the CTT score does not penalize a test taker who answers correctly difficult questions, but answers wrongly easy ones - despite the fact that such a pattern might be indicative of randomness or cheating.

**Item Response Theory (IRT):** IRT [12; 5] is a model used extensively in psychometrics to measure the ability level of the test takers and evaluate the difficulty of the test questions (which are referred to as _items_ in psychometrics). IRT takes into consideration the difficulty of the questions when evaluating test-taker's performance and also makes use of the pattern of correct and incorrect responses on the exam. The model associates with every test taker \(j\) a parameter \(_{j}\), which corresponds to the _ability_ of \(j\). The two-parameter IRT model (2PL) associates every question \(i\) with two parameters \(_{i}=(_{i},_{i})\). The model assumes that a test taker with ability \(_{j}\) answers question \(i\) associated with \(_{i}\) correctly with probability given by the logistic function:

\[p_{ij}=(_{j}-_{i})}}{1+e^{_{i}(_{j}- _{i})}}.\] (1)

Parameter \(_{i}\) is the _discrimination parameter_ and \(_{i}\) is the _difficulty_ of question \(i\). Note that the ability \(_{j}\) and the difficulty level \(_{i}\) are in the same scale; after all, the difference \((_{j}-_{i})\) directly affects \(p_{ij}\). For fixed \(_{i}\), the difficulty parameter \(_{i}\) is the value (on the ability scale) for which \(p_{ij}=0.5\). Parameter \(_{i}\) characterizes how well question \(i\) can differentiate among test takers located at different points of the ability continuum; \(_{i}\) is proportional to the slope of \(p_{ij}=p_{i}(_{j})\) at the point where \(p_{ij}=0.5\) - the steeper the slope, the higher the discriminatory power of \(i\). All the parameters of this model take values in \((-,+)\). Note that any set of questions comprising an exam spans a certain range of \(_{i}\) values; such a set is not appropriate to assess test takers with abilities outside this range.

The 3-Parameter IRT model (3PL for short) is an extension of the above model that also incorporates a _pseudo-guessing parameter_\(_{i}\). Thus, in 3PL every question \(i\) is associated with three parameters \(_{i}=(_{i},_{i},_{i})\); \(_{i}\) and \(_{i}\) are the same as before. Intuitively, \(_{i}\) is the probability of answering correctly based on random guess with \(_{i}\). Thus, the probability of a test taker with ability \(_{j}\) to answer question \(i\) correctly is: \(P_{ij}=_{i}+(1-_{i})p_{ij}\).

Given test-taker responses, the parameters of the model can be estimated using Bayesian methods . In our case, the ENEM dataset came with a set of questions for which the parameters \((_{i},_{i},_{i})\) had already been fitted by education experts . Therefore, for each one of the LLMs we considered, we only need to compute their ability parameters - given their response patterns. Intuitively, large values of \(\) correspond to test takers with high ability levels and vice versa. High ability value \(\) of an LLM implies better performance.

Although the ability levels of test takers can be used as a measure of their performance, one should also know if the test takers are _consistent_ with the model, e.g., they should answer easy questions correctly if they answer difficult questions correctly. One index that enables us to evaluate the consistency of the test takers with the model is the \(l_{z}\) index . Intuitively, the \(l_{z}\) index is based on the standardization of a test-taker's log-likelihood function given their theta values. Assume a set of \(I\) questions and test taker \(j\) with ability \(_{j}\) and response vector \(r_{j}\) such that \(_{j}(i)=1\) (resp. \(_{j}(i)=0\)) if \(j\) answered question \(i\) correctly (resp. wrongly). Then, the log-likelihood of \(j\) is simply: \(L_{j}=_{i I}[_{j}(i) P_{ij}+(1-_{j}(i))( 1-P_{ij})].\) To standardize \(L_{j}\) we need both its mean (\([L_{j}]\)) and variance (\((L_{j})\)). Then, the \(l_{z}\)_score_ is computed as:

\[l_{z}(j)=-[L_{j}]}{(L_{j})}}.\] (2)

In a well-designed test, the \(l_{z}\) scores are expected to have a unit normal distribution - this is the case for humans taking the ENEM test (see for example Figure 3). In general, \(l_{z}\) values close to \(0\) are considered good: it means the test takers' response patterns are consistent with what is expected from them by the model. Negative \(l_{z}(j)\) scores reflect an unlikely response vector. A positive \(l_{z}(j)\) score indicates that \(j\) has a more likely response vector than indicated by their ability.

We can access the amount of information that an item \(i\) provides to estimate \(\) under the 3PL model by the Fisher information, which is given by:

\[_{i}()=_{i}^{2}[-_{i})^{2}}{(1- _{i})^{2}}][}{p_{i}}].\] (3)

The total information of a test is simply the sum of item information, i.e., \(()=_{i I}_{i}()\). The Fisher information is connected with the standard error of the estimation, given by \(SE()=1/()}\). When a test has high Fisher information in a certain \(\) range, the test has more discriminative power in that range, producing scores with less measurement errors.

## 4 Methods

### The ENEM Exam

The _Exame Nacional do Ensino Medio_ (ENEM), world's second largest university entrance exam behind Chinese's Gaokao exam, is taken by millions of Brazilian students each year . ENEM comprises questions requiring different levels of domain-specific knowledge and reasoning .

The exam is in Brazilian Portuguese and consists of four sections, each of which has 45 multiple-choice questions with five options . Each section is treated as a separate exam for the purposes of modeling via IRT. The four sections consist of the **Huminities**, the **Languages and Codes**, the **Natural Sciences**, and the **Math** exams. The description of these exams is given in Appendix A.11.

Since 2009, the grades assigned to ENEM test-takers have been determined using IRT. Using IRT helps to penalize guessing, differentiate among students that otherwise would get the same (CTT) grade, and compare among students that took exams in different years. The ENEM organizers release not only the exam content and questions, but also the student (anonymized) responses and their CTT and IRT scores, which enables downstream studies.

From our standpoint, there are a number of relevant aspects of the process used by the ENEM developers . First, questions are given to a sample of students, whose answers are used to find inconsistencies and errors. Next, an important test of construct validity is to verify the unidimensionality of the latent trait, for which the ENEM team uses Full Information Factor Analysis . Finally, the IRT model itself is fit using the Marginal Maximum Likelihood Estimator . Using the results, the developers may exclude questions having poor model fit.

The exams, their solutions, and all the fitted parameters of the 3PL IRT model \((_{j},_{i},_{i},_{i})\) are publicly available at the Brazilian government website . To the best of our knowledge, these data are the largest and most comprehensive public dataset based on item-response theory available. The datasets contain questions and complete response patterns of all students taking the exams in 2022and 2023. Questions for the 2023 exam were released in November 2023, minimizing the chance they are in training data for most of the LLMs we considered. However, we expect fragments of the exam being in the training data (e.g. poems, and any other widely available material used as part of a question) 1. The number of test takers per year ranged from 2.2M to 3.7M.

The ENEM exams are initially made available as PDF files; we used the Python library _PyPDF2_, followed by regular expressions and some manual adjustments to extract each question from its exam file. In order to account for possible effects of Language, as diagnosed in previous work , we translated all questions to English and run all experiments in Portuguese and English. For those exam questions that incorporated images, we used the version of the exam designed for blind people containing textual descriptions of the images. We manually audited all questions in 2022 and 2023 exams to ensure their quality (Appendix A.1).

### Models

We evaluate the following family of models: the open source models Mistral-7B, Gemma-7B, Llama2-7B, Llama2-13B, Llama3-8B, and GPT 3.5. For the open source models, we evaluate on both instructed and non-instructed tuned versions. Our choice of models enables the study of models of similar size (the majority of our models are of size 7B), but also introduces diversity of architectures (GPT, Gemma, Mistral, Llama), size (7B vs. 13B), training data (Llama2 vs. Llama3), and training strategies (with and without instruction tuning).

We prompt models with \(\{0,1,4\}\)-shots, following conventional question-answer benchmark prompting strategies  (example prompts in Appendix A.4). We measure model's next token probability across five option letters, and average predictions across 30 _shuffles_ of the order of the answer choices to correct for the well-known effect of position bias  (Details in Appendix A.4).

## 5 Results

In this section, we present our main findings. All the results we show here are for the 2023 ENEM exams, with four-shot prompting. Results for the 2022 ENEM exam and for zero-shot and one-shot prompting and for open source instructed tuned models are shown in Appendices A.6 - A.9. The results we show in this section are strongly consistent with the results we get for the 2022 ENEM exam and for one-shot prompting.

### Accuracy vs. Ability Level

We first investigate how humans compare to LLMs when IRT parameter \(\) is used instead of accuracy (the metric that is employed in most LLM benchmarking, e.g., [8; 43]). In Figure 1 we plot the CTT score (accuracy) vs IRT score (\(\)) for 30 shuffles of answer options for each model. The light blue background points correspond to the humans who took the exam. Each of the closed curves in the figure corresponds to one LLM, and shows the central 90% of the LMM's distribution.

First, we observe that there are many cases where identical accuracy scores result in different \(\) scores. This reflects the fact that IRT takes into account not just the number, but also the pattern of correct answers. Second, for many LLMs, particularly in the Humanities and Languages exams, there is overall greater variability in the accuracy score than in the IRT score. This suggests that IRT is less sensitive to the variations in LLM output that are due to the LLM's inherent randomness.

To compare the performance between LLMs and humans, we compare their IRT scores (\(\)). Recall that IRT score of 0 corresponds to the average ability of a human test taker. Across all four subjects, the majority of models have CTT and IRT scores overlapping with humans. LLMs in general achieve \(\) scores above that of the human average in Humanities, Languages, and Natural Sciences, but below human average in Mathematics. Looking at specific models, we find the Llama2 models at the lower end of \(\) scores, Mistral and Llama3 in the middle range, and GPT-3.5 and Gemma-7B at the higher end of \(\) scores.

The language of the exam affects some models' performance. In Languages and Natural Sciences, GPT-3.5 tends to perform better in Portuguese compared to English, while in Humanities and NaturalSciences, the Llama models tend to perform worse in Portuguese than in English. This suggests that there are differences regarding the reasoning ability and the amount of knowledge accessible to the models in each language.

Importantly, outlier models all tend to have higher accuracy and/or lower IRT scores than humans. These models answer more questions correctly than humans do, but show error patterns that are not entirely human-like. We dig into this phenomenon next.

### Response Patterns

One of our goals is to assess whether the LLMs we examine show good fit to the ENEM IRT model, as crafted by the educational expert team described in Section 4.1. Intuitively, a test taker showing good fit to an IRT model is an individual \(j\) that tends to make less frequent mistakes on "easy" questions (question \(i\) with \(_{i}<_{j}\)) while making more frequent mistakes on "hard" questions (question \(i\) with \(_{i}>_{j}\)). Thus, to assess fit we need to inspect the response patterns of the LLMs.

Figure 2 shows the response patterns of LLMs for the 2023 exam. Every cell \((i,j)\) corresponds to the probability that LLM \(i\) answered question \(j\) correctly, where probabilities are computed over the \(30\) shuffles. We use gray scale with a black (resp. white) cell representing \(1\) (resp. \(0\)). Questions are ordered in increasing order of their \(\) values. Generally, rows with darker overall patterns (higher correctness) are indicative of higher \(\) scores.

The figure demonstrates a number of points. For example, on the Math exam, the figure exhibits a response pattern that appears to show low \(\) values for all models, which confirms results in Figure 1. In addition, the figure shows that for some questions, the 30 shuffles of answer choices of a given model are often either all correct or all incorrect. However, there are some grey areas in the figure for all the exams, indicating that shuffling the options can affect the LLM's answers on certain items. Furthermore, the patterns show that many questions appear to be either "easy" (black) or "hard" (white) for all models at the same time. Likewise, in many cases models show similar performance on the English and Portuguese versions of a given question.

Overall, the response patterns we observe suggest that the Math exam is "too difficult," with models often resorting to guessing. On the other hand, most LLMs consistently answer correctly the

Figure 1: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles of the order of answer choices.

questions in the Humanities exam, implying that this is an easy exam for them. The performance of LLMs in the Natural Science exam is the most interesting as there are blocks of questions that most LLMs answer consistently correctly, interleaved with blocks of questions that most LLMs answer incorrectly. This suggests that there are questions that are easy for humans but difficult for LLMs and vice versa. In the next subsection we analyze this phenomenon more closely.

### Reliability of IRT scores for LLMs

In this section, we investigate whether the ENEM exam is a valid test for LLMs' ability, in the same way it is for humans. Intuitively, we want to define measures that allow us to quantify to what extent we trust the IRT scores we obtained for LLMs. We propose three different ways of doing this. The first is _goodness-of-fit_ that quantifies whether the response of LLMs fit the IRT model. The second is based on _Fisher information_, measuring how much information the exam provides for estimating the \(\)s in a certain range. Finally, we use the _discrimination index_ which evaluates the capacity of questions to accurately distinguish between high and low performing test takers.

**Goodness-of-fit:** We use the \(l_{z}\) score (see Section 3) assess whether the test taker is behaving in a manner consistent with the model. Alternatively, we ask what is the _appropriateness_ of a test-taker's estimated \(\) as a measure of the test taker's true \(\)? For example, imagine that an LLM has a response pattern of missing easy questions and correctly answering more difficult ones. Such a pattern may arise because the LLM was lucky on the hard questions, or it may arise because the LLM had access to memorized patterns that assisted in answering the hard questions. Generally, low \(l_{z}\) scores suggest that the \(\) estimate of the model is less reliable .

In Figure 3 we show \(l_{z}\) scores plotted against \(\) scores of LLMs across the four exams in 2023 (2022 is shown in Appendix A.9). As in previous plots, the light blue points in the background show the distribution of the same two scores for the human test takers. Starting again with the Math exam, we note that \(l_{z}\) values are low, but now we can see that the response patterns of the LLMs are indeed quite human-like; LLMs behave like humans with similarly low \(l_{z}\) values. One possible reason for this behavior is that the Mathematics exam tends to be the harder exam of ENEM, leading to more guessing, which may make the human \(l_{z}\) values for Mathematics smaller.

For the Languages exam, models perform better in general (higher \(\) values) and the most \(l_{z}\) scores being close to 0 (and with a similar spread as the human distribution of \(l_{z}\)'s) suggest that these \(\) estimates are reliable - the models are showing human-like response patterns.

Figure 2: Response patterns for each LLM, where darker indicates more often correct (across random option shuffles). Questions are sorted in increasing difficulty (\(\) value). LLMs are non-instructed tuned open source models and GPT3.5 with four-shot.

The results become more nuanced as we look at the Natural Sciences exam. For this exam, most models, including the high performing ones (i.e., GPT-3.5 and Gemma-7B), show values well outside the human distribution, with a long tail in the negative values of \(l_{z}\). Comparing the GPT-3.5 and Gemma-7B results in Figures 1 and 4, we can infer that the high accuracy (CTT scores) achieved by these models on the Natural Sciences exam are quite misleading; although GPT-3.5 and Gemma-7B answer many questions correctly, their response pattern is very unlikely, with very low \(l_{z}\) values. This corroborates with Figure 2, which shows an interchange of blocks of correct and incorrect answers from the models, creating an unlikely response pattern.

In Humanities, almost all LLMs perform reasonably well, achieving \(\) scores above zero (the average human level). However, Llama2-7B, while obtaining above average accuracy scores (Figure 1) and good \(\) scores, has low average \(l_{z}\) scores. This suggests that the IRT scores Llama2-7B may be not reliable. Examination of the corresponding rows in Figure 2 shows that this is the only model that does not have a consistent response pattern across shuffles, leading to the observed low \(l_{z}\) score.

**Fisher Information:** We investigate further whether the ENEM exams are giving us accurate estimates of the LLMs ability levels from another standpoint - that of Fisher Information (see Section 3, Equation (3)). Intuitively, Fisher Information quantifies whether there was enough information in the test to infer the ability level of a test taker at a certain ability level. Figure 4 shows, for every ENEM exam, the total Fisher Information \(()\) on the top plot, and the \(\) scores for the models (95% Confidence Interval (CI) computed using the shuffles) on the bottom plot. This plot reinforces the observation that for some models in Natural Sciences and for all models in Mathematics, the models' \(\) are not in the range of the exam with highest information - the models ability levels fall in the tail of the Fisher Information histogram. Hence, _the Math exam is not useful for making meaningful measurements of these LLMs,_ casting doubt on the informativeness of the models' \(\) scores on this exam. The lack of discrimination ability of this exam is reflected by the responses for many models showing apparently random response patterns in the corresponding heatmap (see Figure 2).

Figure 4: Total Fisher information of the exams and the IRT scores (95% Confidence Interval (CI)) for LLMs. LLM datapoints are computed from different shuffles.

Figure 3: Distribution of \(l_{z}\) and IRT scores for humans and LLMs. LLMs are non-instructed tuned open source models and GPT3.5 with 4-shot. LLM datapoints are computed from different shuffles.

**Discrimination Index:** To further assess the reliability of the IRT scores, we also turn into psychometrics and use the notion of the item _discrimination index (DI)_, which measures how well an item on a test distinguishes between high and low scorers on the entire test . Let \(P_{h}\) (resp. \(P_{l}\)) be the proportion of the top 25% (resp. low 25%) LLMs (in terms of \(\), including the shuffles) that correctly answer the item; then \(DI=P_{h}-P_{l}\), the difference of the two proportions. DI ranges from -1 to 1, and questions with DI higher than 0.2 are considered good, while lower DI indicates flaws .

Figure 5 shows the distribution of the discrimination indices computed for humans and LLMs for the 2023 exam. Overall, we notice that discrimination indices computed for LLMs are more negative compared to those of humans. We also observe that a significant fraction of Math questions have low discriminative power, reinforcing the hypothesis that this exam is not well designed to measure Math abilities for LLMs. Nonetheless, the Humanities and Languages have several questions with very good discriminative power. Interestingly, the Natural Sciences exam appears to follow a bimodal distribution, containing both informative and poorly-designed questions. This may be a reflection of the fact that the Natural Sciences exam is a hybrid test, containing a mix of knowledge-based items and items that demand more complex reasoning over numbers and images, which can be less useful for evaluating the current state-of-the-art LLMs.

**Attributes affecting reliability of IRT scores:** In a further investigation, shown in Appendix A.2, we explore potential causes of low discrimination. We investigate item attributes such as the existence of images or numbers in the questions as we believe that these attributes impede LLMs from understanding the question properly. Our preliminary results suggest that LLMs' ability to understand math questions and parse images is sub-par compared to their capacity in answering pure text-based questions. In Appendix A.10 we show examples of non-discriminating and highly discriminating items for the 2023 Natural Sciences exam. In Appendix A.3, we reach a similar conclusion by looking at model accuracy against model perplexity, a model intrinsic metric.

## 6 Conclusions

The ongoing debate in LLM evaluation centers around whether exams designed for humans are appropriate tools for measuring the performance of LLMs. In this paper, we provide a case study that illustrates methods that can be used to address this question, as well as specific results for a range of current LLMs. We leverage the largest known human exam for which a public IRT model is available, and show that IRT can be leveraged to distinguish between human-like and non-human-like responses under the model. We show cases where LLMs respond in non-human-like ways and show how to identify those cases using a model-fit metric. Further, we show that using IRT we can determine when an exam is capable of making meaningful measurement of an LLM's ability in a given subject area. Using our evaluation framework, we find that the ENEM Math exam is not appropriate to make meaningful measurements of the models' ability, for the LLMs we study. At the same time, Humanities and Language exams are better suited for evaluating the LLMs' abilities on those subjects. We conclude that IRT modeling, drawing on a long history of psychometric theory, provides a set of crucial tools for assessing whether exams designed for humans are actually meaningful measures of LLM ability. Our results suggest that they should be used in future studies when questions are raised regarding the performance of LLMs on human exams.

Figure 5: Discrimination Indices for questions in the 2023 exam for both Humans and LLMs.

[MISSING_PAGE_FAIL:10]

* Narayanan and Kapoor (2023) Arvind Narayanan and Sayash Kapoor. 2023. Gpt-4 and professional benchmarks: the wrong answer to the wrong question. (2023). https://www.aisnakeoil.com/p/gpt-4-and-professional-benchmarks.
* Nunes et al. (2023) Desenes Nunes, Ricardo Primi, Ramon Pires, Roberto Lotufo, and Rodrigo Nogueira. 2023. Evaluating gpt-3.5 and gpt-4 models on brazilian university admission exams. (2023). arXiv: 2303.17003 [cs.CL].
* OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. (2023). arXiv: 2303.08774 [cs.CL].
* Pezeshkpour and Hruschka (2023) Pouya Pezeshkpour and Estevam Hruschka. 2023. Large language models sensitivity to the order of options in multiple-choice questions. (2023). arXiv: 2308.11483 [cs.CL].
* Plumed et al. (2016) Fernando Plumed, Ricardo Prudencio, Adolfo Martinez-Uso, and Jose Hernandez-Orallo. 2016. Making sense of item response theory in machine learning. In (Sept. 2016). doi: 10.3233/978-1-61499-672 -9-1140.
* Raji et al. (2021) Deborah Raji, Emily Denton, Emily M. Bender, Alex Hanna, and Amandalynne Paullada. 2021. Ai and the everything in the whole wide world benchmark. In _Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks_. J. Vanschoren and S. Yeung, (Eds.) Vol. 1. Curran. https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/084b6fbb10729ed4da8c3d3f5a3ae7c9-Paper-round2.pdf.
* Randaldi and Pucci (2023) Leonardo Randaldi and Giulia Pucci. 2023. Does the english matter? elicit cross-lingual abilities of large language models. In _Proceedings of the 3rd Workshop on Multi-lingual Representation Learning (MRL)_, 173-183.
* Robinson and Wingate (2023) Joshua Robinson and David Wingate. 2023. Leveraging large language models for multiple choice question answering. In _The Eleventh International Conference on Learning Representations_. https://opereview.net/forum?id=yKbprarjc5B.
* Rodriguez et al. (2021) Pedro Rodriguez, Joe Barrow, Alexander Miserlis Hoyle, John P. Lalor, Robin Jia, and Jordan Boyd-Graber. 2021. Evaluation examples are not equally informative: how should that change NLP leaderboards? In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_. Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, (Eds.) Association for Computational Linguistics, Online, (Aug. 2021), 4486-4503. doi: 10.18653/v1/2021.acl-long.346.
* Schaeffer et al. (2023) Ryan Schaeffer, Brando Miranda, and Sanmi Koyejo. 2023. Are emergent abilities of large language models a mirage? In _Advances in Neural Information Processing Systems_. A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, (Eds.) Vol. 36. Curran Associates, Inc., 55565-55581. https://proceedings.neurips.cc/paper_files/paper/2023/file/adc98a266f45005c403b8311ca7e8bd7-Paper-Conference.pdf.
* Sedoc and Ungar (2020) Joao Sedoc and Lyle Ungar. 2020. Item response theory for efficient human evaluation of chatbots. In _Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems_. Steffen Eger, Yang Gao, Maxime Peyrard, Wei Zhao, and Eduard Hovy, (Eds.) Association for Computational Linguistics, Online, (Nov. 2020), 21-33. doi: 10.18653/v1/2020.eval4nlp-1.3.
* Shi et al. (2023) Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. 2023. Detecting pretraining data from large language models. In _The Twelfth International Conference on Learning Representations_.
* Silva et al. (2023) Bruno Silva, Leonardo Nunes, Roberto Estevao, and Ranveer Chandra. 2023. Gpt-4 as an agronomist assistant? answering agriculture exams using large language models. _arXiv preprint arXiv:2310.06225_.
* Silveira and Maau (2017) Igor Cataneo Silveira and Denis Deratani Maau. 2017. University entrance exam as a guiding test for artificial intelligence. In _2017 Brazilian Conference on Intelligent Systems (BRACIS)_, 426-431. doi: 10.1109/BRACIS.2017.44.
* Silveira and Maau (2018) Igor Cataneo Silveira and Denis Deratani Maau. 2018. Advances in automatically solving the enem. In _2018 7th Brazilian Conference on Intelligent Systems (BRACIS)_. IEEE, 43-48.
* Talmor et al. (2019) Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: a question answering challenge targeting commonsense knowledge. _ArXiv_, abs/1811.00937. https://api.semanticscholar.org/CorpusID:53296520.
* Terwiesch (2023) Christian Terwiesch. 2023. Would Chat GPT3 Get a Wharton MBA? A Prediction Based on Its Performance in the Operations Management Course. Tech. rep. Mack Institute for Innovation Management at the Wharton School, University of Pennsylvania.
* Tjuataj et al. (2024) Lindia Tjuataj, Valerie Chen, Sherry Tongshuang Wu, Ameet Talwalkar, and Graham Neubig. 2024. Do Ims exhibit human-like response biases? a case study in survey design. (2024). arXiv: 2311.04076 [cs.CL].
* Touvron et al. (2023) Hugo Touvron et al. 2023. Llama 2: open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_.
* Trott (2024) Sean Trott. 2024. Can large language models help augment english psycholinguistic datasets? _Behavior Research Methods_, 1-19.

*  Lakshmi Varanasi. 2023. Gpt-4 can ace the bar, but it only has a decent chance of passing the cfa exams. here's a list of difficult exams the chatpst and gpt-4 have passed. (2023). https://www.businessinsider.com/list-here-are-the-exams-chatgpt-has-passed-so-far-2023-1.
*  Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. Glue: a multi-task benchmark and analysis platform for natural language understanding. In _Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP_, 353-355.
*  Xiaoxuan Wang et al. 2023. Scibench: evaluating college-level scientific problem-solving abilities of large language models. _arXiv preprint arXiv:2307.10635_.
*  Xinpeng Wang, Bolei Ma, Chengzhi Hu, Leon Weber-Genzel, Paul Rottger, Frauke Kreuter, Dirk Hovy, and Barbara Plank. 2024. " my answer is c": first-token probabilities do not match text answers in instruction-tuned language models. _arXiv preprint arXiv:2402.14499_.
*  Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. _Advances in neural information processing systems_, 35, 24824-24837.
*  Margaret Wu and Ray Adams. 2007. _Applying the Rasch model to psycho-social measurement: A practical approach_. Educational Measurement Solutions Melbourne.
*  Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, and Pengfei Liu. 2024. Evaluating mathematical reasoning beyond accuracy. _arXiv preprint arXiv:2404.05692_.
*  Longhui Yu et al. 2023. Metamath: bootstrap your own mathematical questions for large language models. _arXiv preprint arXiv:2309.12284_.
*  Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Ageival: a human-centric benchmark for evaluating foundation models. _arXiv preprint arXiv:2304.06364_.
*  Yan Zhuang et al. 2023. Efficiently measuring the cognitive ability of llms: an adaptive testing perspective. _arXiv preprint arXiv:2306.10512_.

## Appendix A Supplemental Material

### Manual auditing of exam questions

Assuming the original questions written by the ENEM authorities are good test instruments for testing student capability, we focus on ensuring the quality of adapted dataset for LLM evaluation. We manually correct the artifacts for each question in 2022 and 2023. In the next sections, we describe the artifacts from those easier to address (sec A.1.2.1), to deeper-rooted problems (i.e., harder to correct, sec A.1.4), as well as how we addressed them manually (sec A.1.5).

#### a.1.1 Label accuracy

We assume answers are correct as translation and parsing of single characters can be quite reliable, and that the original ENEM test is tested across millions of human test takers and will be discarded if it had a wrong answer. When we look at the label distribution for 2022, options "ABCDE" each occur 39/39/37/36/33 times, making it fairly balanced. We also ran random baselines on the same option shuffles as the model (Table 1).

#### a.1.2 Translation artifacts

We found several issues pertaining to initial round of translation in this dataset. Mainly, independent translation of question context and answer option leads to incoherence. Details are sometimes mis-translated ("p.d.d" translated to "d.d.p"). There are many non-standardized translations pertaining to chemical formulas, proper nouns, and mathematical formulas. In general, there are significant amount of awkward phrasing, incomplete translation, and linguistic idiosyncrasies lost in translation.

Independent translation of context and questionIn a few cases, the answer options are expected to complete the last sentence of the question. After translation, options do not all fit as completions of the sentence (Q11). Translation without context also leads to improper translation of polysemantic terms. "Coagulation" maybe translated correctly in the question, but becomes "coagulating" as a stand-alone word (Q96). "Good" and "fair" (when used as survey options) gets translated to "regular" and "I will" as stand-alone options (Q171)

Inconsistent translation detailsWithin the same questions, there are cases where the same concept is translated differently. In one question, the context introduces the concept "potential difference (p.d.d)", and later referred to it as "d.d.p" and "d.p.d". Within different options, the same unit can sometimes be plural and sometimes be singular (when it should be consistently plural)

Non-standard translation1) Chemical formula translation is non-standard. "N2O3" becomes "N 2O3", and "NH4+" becomes "NH4 positively charged". 2) (Proper) nouns are sometimes capitalized when they shouldn't. For instance, one question begins with the sentence "_On the Gravitational Field of a Mass Point According to Einstein's Theory A 'Black Hole is..._" 3) Mathematical equations are overly verbatim. This we suspect is partially due to an issue with using audio version of the

   language & subject & Accuracy (CTT) & \(\) & \(l_{z}\) \\  en & humanities & 29.5 \(\) 10.7 & -0.57 \(\) 0.56 & -1.25 \(\) 1.18 \\ en & languages & 24.7 \(\) 8.6 & -0.99 \(\) 0.49 & -0.39 \(\) 1.03 \\ en & science & 25.5 \(\) 8.2 & -0.34 \(\) 0.53 & -0.74 \(\) 1.25 \\ en & math & 22 \(\) 6.3 & -0.6 \(\) 0.34 & -0.66 \(\) 0.97 \\  pt-br & humanities & 24 \(\) 7 & -0.83 \(\) 0.38 & -0.83 \(\) 1.08 \\ pt-br & languages & 23.1 \(\) 7.1 & -1.06 \(\) 0.42 & -0.32 \(\) 1.01 \\ pt-br & science & 23.5 \(\) 7.3 & -0.48 \(\) 0.41 & -0.5 \(\) 1.18 \\ pt-br & math & 23.2 \(\) 6.4 & -0.55 \(\) 0.4 & -0.86 \(\) 1.05 \\   

Table 1: Random choice selection performance on English and Portuguese versions of 2022 test 4 subjects.

test. For example, if an option is the formula \(9(!}-1)\), its Portuguese representation would be "9 _vezes ( ( 8 factorial divididop por ( (8 menos 2) factorial vezes 2 factorial)) memos 1)_" and the English translation exacerbates the situation by translating parenthesis literally as well: "_9 times open parenthesis, open parenthesis, 8 factorial divided by, open parenthesis, open parenthesis, 8 minus 2, close parenthesis, factorial times 2 factorial, close parenthesis, close parenthesis, minus 1, close parenthesis._". Sometimes, delimiters are omitted after translation: "9,300" becomes "9 300".

Awkward phrasingsThere exist awkward phrasings throughout translation. They range from causing minor difficulty in understanding (i.e., "_Life: the science of biology Bears, because they are not truly hibernating, wake up due to the presence of thermogenin, a mitochondrial protein that prevents protons from reaching ATP synthase, generating heat._") to sometime completely non-sense (i.e., "_articulation of several narrative nuclei_")

Incomplete translationThere is no fine line between proper code switching (where proper nouns should remain in Portuguese script) to in-complete translation. The amount of Portuguese left over range from single words, to phrases in options (not consistently across options), to entire sentences within the question.

Linguistic idiosyncrasies lost in translationIn one question, the problem arises when English translation does not match with literal tokens of expressions in Portuguese ("_Next to the man is the message: "Men don't cry", with a large X drawn over the word "no"_). The word "no" does not appear in the English phrase "Men don't cry" but the statement as a whole makes sense in the Portuguese version of the instruction. In a separate question, the topic is on testing for a Portuguese specific pronoun inflection. However, when it was translated into one single word in English, the question no longer makes sense ("_They told me... - They told me. - Huh? - The correct word is "they told me". Not "they told me". - I speak the way I want to. And I'll tell you more... Or is it "tell you "? - What's that? - I'm telling you that you... -"You" and "you" don't go together..._")

#### a.1.3 Document parsing artifacts

Each section consistently contains an error of this kind, where the last part of the question got wrongfully parsed into part of the first option (option (A)). In a separate instance, a figure was wrongfully parsed into one of the options of the previous question. In the Portuguese version of the exam, structural components of the question (e.g., title, subtitle, caption) are consistently concatenated together without proper separation. This often leads to incoherent English translations.

#### a.1.4 Audio-version artifacts

Audio description of images, tables, and figures are not always sufficient, or the most intuitive. For instance, a question asks test taker to note why a particular painting stands out, and the answer is due to the painting's "distortion when representing human figure", which is difficult to qualitatively describe, no matter how complete the description of an image is. Similarly, textual description of geometric figures can be impossibly complicated ("..._Figure of a grid with 7 horizontal and 7 vertical lines, on which a polygonal path is drawn by means of a continuous line on the grid lines, joining the starting point P, located on the second vertical line, from left to right, and between the sixth and seventh horizontal lines, from top to bottom, to the end point Q, which is located between the sixth and seventh vertical lines, from left to right, and on the second horizontal line, from top to bottom..._")

#### a.1.5 Manual Correction

The majority of the artifacts begin with incorrect parsing of the PDF documents related to structural components. To address this, we manually audited each question, and added correct spacing and newlines to each question. These improvements result in better translations from DeepL API qualitatively. After translation, we make minimal edits to improve syntactic and semantic issues through Grammarly to obtain a score of at least 95 23. For each answer option, we ensure consistent part-of-speech, especially if they are sentence completions of the questions. For math and science sections, we follow consistent markdown-like format the same way as other mathematical reasoning datasets . Here we list the full set of modification rules for 2022 (question numbers are referenced in parenthesis):

* Separate description of the image by '' before and after.
* "Por cento" becomes %.
* Number in the form 7 000 becomes 7000.
* From "abre aspas" "fecha aspas" to """.
* Remove "Descricao da estrutura quimica", "Descricao do esquema", "Descricao da associacao de baterias", "Descricao da imagem" from the options".
* "De carga positiva" to +, "De carga negativa" to -, "de carga dois menos" to (2-).
* For a subset of the questions, we follow the non-blind version of the question (157, 158, 163, 166, 168, 171, 174, 177, 178, 179)
* Remove period at the end options or questions of math questions (to avoid confusion).

Here are the list of rules we use for English version of the exam (2022):

* Change number decimal from "3,1415" to "3.1415".
* Manual translation fix (49, 162).

#### a.1.6 Limitations of the dataset

There are a few limitations of the dataset:

1. Even though the English version of the exam is modified manually, there are still issues with the presentation of the questions. We rely mostly on Grammarly feedback, but it is not perfect. Our judgement of how fluently a question is written is also subjective. The ideal method would be to recruit professional human translators, which is costly and time consuming.
2. The content of many of the questions are focused on knowledge common to Brazilian culture, or problems in Brazilian society. The English translations may not cover the full extent of cultural, language specific phenomenons or connotations.
3. We assume the transcription of images and tables to be sufficient for the models to understand and solve the question.

### Attributes that affect goodness-of-fit

Given that questions have wide range of discrimination indices for LLMs, we investigate a potential cause described in the psychometrics literature for aberrant response patterns: lack of _subabilities_, i.e., specific skills required to answer a question correctly. We hypothesize that some item attributes, such as whether the question contains images or numbers in its statement or among the options, may be disproportionately harder for LLMs and hence represent subabilities that explain the aberrant response patterns quantified in Figure 3.

We built a contingency table relating non-discriminative/discriminative items (i.e., items with discriminative index lower/higher than 0.2) and the aforementioned attributes, and run a \(^{2}\) independence test. The results for the Natural Sciences exam are shown in Table 2. For this exam, we observe high \(^{2}\) values which indicate that the abilities of the LLM models with respect to math reasoning and interpreting images are sub-par compared to their capacity in solving pure text questions. While Language and Humans exams are most purely text and the Math exam mostly demands reasoning with images and numbers, the nature of the Natural Sciences exam is hybrid, containing both types of questions. This may well explain the bimodal distribution of discrimination indices in Figure 5 and the aberrant response patterns identified by the very low \(l_{z}\) scores in Figure 3, and highlights how psychometrics can aid the design of better and more valid benchmarks for LLMs.

### Model accuracy relation to model perplexity

One reason that models may error differently than humans is due to their training corpus. If models have encountered similar question or topics, if not identical, to those in our dataset during training, they may perform unexpectedly well, even if the questions are difficult. Recent work in data contamination proposed a few model intrinsic metrics that can be used to detect contamination . Mainly, the Min-k% Prob score takes the average probability of the top-k percentile tokens with minimum probabilities 4:

\[=-_{x_{i}}p(x_{i}|x_{1}, ,x_{i-1})\] (4)

where \(x=x_{1},x_{2},,x_{N}\) denotes the input sequence of N tokens, Min-K% Prob(x) represents the set containing tokens with minimum k percentile probabilities, and \(E\) represents the size of such set. Note here that Min-k% Prob is intrinsic to each model, and if a model has been exposed to more similar training data as the questions, its Min-k% Prob would be low for that question.

We do not expect any model to have unexpectedly low Min-K% Prob(x) on any of our questions, considering it is highly unlikely that the ENEM questions were parsed and translated to English, and somehow ended up in the training corpus. What we are more interested here, is whether such score is correlated to model's accuracy on the answer predictions. If they are negatively correlated (i.e. high Min-K% Prob corresponds to low accuracy), this is evidence for the hypothesis that training on related data leads to higher accuracy.

To investigate this hypothesis, we plot 4-shot model accuracy (averaged across 31 option shuffles) against Min-20% Prob for four subjects in exam 2022 in English along with the Pearson correlations 5 in Figure 6. In all except 1 model-subject pair (Llama2 chat in humanities, we investigate this further) do we see a significant negative correlation (p \(<0.05\)) between accuracy and Min-k 20% Prob, indicate that model doesn't necessarily do better if they have encountered similar data during training. Another way to interpret this, is that it is not likely that these models have seen our data during training.

The few negative correlation casesAs seen before, we observe a significant negative correlation for Llama-2 7B Chat in humanities. To get a full understanding of whether this is a stand-alone phenomenon, we examine Portuguese version of the exam, as well as exam in 2023, and show our findings below in Table 3. We do not see the same correlation in the Portuguese version of the exam. However, we additionally see Gemma-it negatively correlated with humanities section in both English and Portuguese version of the exam in 2023, as well as Gemma with languages section in 2023. The later two correlations are robust across a few other metrics we investigated from  as well, we think this may suggest data contamination, but we cannot test such hypothesis because Gemma training data is not public.

Positive correlations in 2022 scienceIn 2022 Science, both English and Portuguese, we see significant _positive_ correlation across all models (Table 3).

Through qualitative analysis, we find that the questions with highest perplexities were formatted more in a sentence completion-like structure similar to Question 1. Whereas less perplexity ques

   Item Attribute & 2022 & 2023 \\  Contains images & 0.401 (0.052) & **3.906** (0.048) \\ Contains numbers in the answers & **7.331** (0.007) & **6.264** (0.012) \\ Contains numbers in the statement & **3.961** (0.046) & 3.212 (0.073) \\   

Table 2: \(^{2}\) test for the correlation between poorly-discriminating items and item attributes in the Natural Sciences exam in 2022 and 2023. Significant values are in bold. High values of \(^{2}\) indicate that images or numbers make the item less useful to evaluate the LLMs we experiment with.

Figure 6: Model Min-20% Prob vs. 4-shot accuracy across four subjects in 2022 in English

tions involve more image/table description with reasoning needed to obtain the answer (question 2). This is similar to what we discover with discriminative index in Section 7 in the main text.
* Question: Technique modifies rattlesnake venom protein tocreate a drug that modulates blood clotting
* Rattlesnake venom can cause life-threatening hemorrhaging to those bitten by the snake. However, researchers from Brazil and Belgium have developed a molecule of pharmaceutical interest, PEG- collinein-1, from a protein found in the snake's venom. The molecule is capable of modulating blood clotting. Although the technique is not new, it was applied for the first time from animal toxin in its recombinant form, i.e. produced in the laboratory by a genetically modified fungus.
* This new drug has potential applications for options:
* (A) prevent the formation of thrombi, typical insomes cases of stroke.
* (B) threat the consequences of profound anemia, due to the loss of a large volume of blood.
* (C) prevent the manifestation of urticaria, commonly related to allergic processes.
* (D) reduces swelling of the lymph nodes, part of the immune response to different infections.
* (E) regulate the fluctuations in blood pressure characteristic of hypertension.

Listing 1: high perplexity question with high model accuracy.

* Question: On a hot day, two colleagues are playing with the water from the hose. One of them wants to know how high the water jet reaches from the outlet when the hose is positioned vertically.
* The other colleague then proposes the following experiment: they position the water outlet of the hose in a horizontal direction, 1 meter above the ground, and then measure the horizontal distance between the hose and the place where the water hits the ground.
* The measurement of this distance was 3 meters, and from this, they calculated the vertical reach of the water jet. Consider the acceleration of gravity to be 10 meters per second squared.
* The result they obtained was Options:
* (A) 1.50 meter.
* (B) 2.25 meters.
* (C) 4.00 meters.
* (D) 4.50 meters.
* (E) 5.00 meters.

We also tried filtering for top N percent most difficult questions per subject and recalculate all the correlations. We did not find any significant difference to results above.

### Prompting Details

To administering the test to LLMs, we measure the next token logits across the 5 letter options directly (i.e. letter "A", "B", "C", "D", "E"), and take the argmax as the model's choice (invariant to sampling temperature). We shuffle the option orders (30 runs) and take the average to calibrate model's prior on generating each letter options. For API-based model (GPT3.5), we query for 1 token generation, and obtain top-20 logits, and use that for our prediction. In the sections below we include 0-shot (Listing 3), 1-shot (Listing 4, 5, 6, 7), and 4-shot prompts (Listing 8) we use in main experiments. For 1-shot, we choose the 1-shot example for each of the four subjects by selecting the easiest question (i.e., with lowest \(\)) from the same subject in the 2021 exam. For 4-shot, we concatenate the 1-shots from four subjects and shuffle the options to evenly distribute the answer among five option letters.

[MISSING_PAGE_FAIL:19]

762 87639Options:
7640(A)Transgenics.
7651(B)Genet therapy.
7662(C)DNAvaccine.
7673(D)Geneticmapping.
7684(E)Therapeutic cloning.
7685
7796Answer:(D)Geneticmapping.
77117
7728Question:{QUESTION}
7730Options:
7740(A){OPTION_A}
7751(B){OPTION_B}
7762(C){OPTION_C}
7772(D){OPTION_D}
7784(E){OPTION_E}
7785Answer:( ```

Listing 4: 1-shot prompt used for Natural Science.

```
7801Herearesomequestionsfromacollegeentranceexam.Choosethe
781correctanswerotthebestofyourability,andoutputinthe
782followingformat:
783Answer:(Option)
7843
785Question:
7865Ahamburgerchainhasthreefranchisesindifferentcities.Toinclude
787anewtypeofsnackonthemenu,thechain'smarketingmanager
788suggestedputtingfivenewtypesofsnackonsalsinespecial
789editions.Thesnackswereofferedforthesameperiodoftimeto
790allthefranchisees.Thetypewiththehighestaveragesolderper
791franchisewouldbepermanentlyincludedonthemenu.Attheendof
792thetrialperiod,managementreceivedareportdescribingthe
793quantitiesold,inunits,ofeachofthefivetypesofsnacksin
794thethreefranchises.
7956
796Imagedescription:Thetableshowsthequantitysoldofeachtypeof
797snackinfranchises1,2,and3.
7988Franchise1sold415type-1snacks,395type-2snacks,425type-3
799snacks,430type-4snacks,and435type-5snacks.
8009Franchise2sold415type-1snacks;445type-2snacks;370type-3
8010snacks;370type-4snacksand425type-5snacks.
8020Franchise3sold415type-1snacks;390type-2snacks;425type-3
803snacks;433type-4snacksand420type-5snacks.
8041
8052Basedonthisinformation,themanagementhasdecidedtoincludethe
806followingtypeofsnackonthemenu
8073
8084Options:
8095(A)1
8106(B)2
8117(C)3
8128(D)4
8139(E)5
8140
8151Answer:(E)5
8162
81723Question:{QUESTION}
8164Options:
8185(A){OPTION_A}
8205(B){OPTION_B}
8217(C){OPTION_C}
8228(D){OPTION_D}
823(E){OPTION_E}

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_FAIL:22]

[MISSING_PAGE_FAIL:23]

* 1009:A hamburger chain has three franchises in different cities. To include
* 1010:a new type of snack on the menu, the chain's marketing manager
* 1011:suggested putting five new types of snacks on sale in special
* 1012:editions. The snacks were offered for the same period of time to
* 1013:all the franchisees. The type with the highest average solder
* 1014:franchise would be permanently included onthemenu. Att the end of
* 1015:thetrial period, management received a report describing the
* 1016:quantities sold, inunits, of each of the five types of snacks in
* 1017:thethree franchises.
* 1018:Image description: The tables shows the quantity sold of each type of
* 1020:snack in franchises 1, 2, and 3.
* 1021:Franchise 1 sold 415 type-1 snacks, 395 type-2 snacks, 425 type-3
* 1022:snacks, 430 type-4 snacks, and 435 type-5 snacks.
* 1023:Franchise 2 sold 415 type-1 snacks; 445 type-2 snacks; 370 type-3
* 1024:snacks; 370 type-4 snacks and 425 type-5 snacks.
* 1025:Franchise 3 sold 415 type-1 snacks; 390 type-2 snacks; 425 type-3
* 1026:snacks; 433 type-4 snacks and 420 type-5 snacks.
* 1027:
* 1028:Based on this information, the management has decided to include the
* 1029:following type of snack onthemenu
* 1030:
* Options:
* 1032:(A) 1
* 1033:(B) 2
* 1034:(C) 3
* 1035:(D) 4
* 1036:(E) 5
* 1037:()
* Answer: (E) 5
* 1039:
* Question: {QUESTION}
* Options:
* 1042:(A) {OPTION_A}
* 1043:(B) {OPTION_B}
* 1044:(C) {OPTION_C}
* 1045:(D) {OPTION_D}
* 1046:(E) {OPTION_E}
* Answer: ( ) ```

Listing 8: 4-shot prompt used across all four subjects.

### Compute Resources

We used GPUs (V100 or A100) provided by a university cluster6. For the main experiments, we used around 200 hours of GPU time (roughly 20 hours per model). Moreover, we used the OpenAI API to run the experiments with GPT3.5.

### Zero and One Shot prompting Results for 2023

#### a.6.1 CTT and IRT \(\)

Figure 7: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with zero-shot. LLM datapoints are computed from different shuffles.

Figure 8: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are instructed tuned open source models with zero-shot. LLM datapoints are computed from different shuffles.

Figure 9: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are non-instructed tuned open source models and GPT3.5 with one-shot. LLM datapoints are computed from different shuffles.

Figure 10: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2023 exam. LLMs are instructed tuned open source models with one-shot. LLM datapoints are computed from different shuffles.

#### a.6.2 Response Patterns

We show 43 items for the 2023 Math exam, instead of 45, because 2 items failed to converge and produce item parameters when the ENEM organizers fitted the human model.

Figure 11: Response patterns for each LLM, where darker indicates more often correct. Questions are sorted by difficulty (\(\) value). LLMs are non-instructed tuned open source models and GPT3.5 with zero-shot.

Figure 12: Response patterns for each LLM, where darker indicates more often correct. Questions are sorted by difficulty (\(\) value). LLMs are instructed tuned open source models with zero-shot.

Figure 14: Response patterns for each LLM, where darker indicates more often correct. Questions are sorted by difficulty (\(\) value). LLMs are instructed tuned open source models with one-shot.

Figure 13: Response patterns for each LLM, where darker indicates more often correct. Questions are sorted by difficulty (\(\) value). LLMs are non-instructed tuned open source models and GPT3.5 with one-shot.

[MISSING_PAGE_EMPTY:30]

## 6 Conclusion

Figure 18: Distribution of \(l_{z}\) and IRT scores for humans and LLMs in the ENEM 2023 exam. LLMs are instructed tuned open source models with one-shot. LLM datapoints are computed from different shuffles.

### CTT and IRT \(\) for 2022

Figure 19: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2022 exam. LLMs are non-instructed tuned open source models and GPT3.5 with four-shot. LLM datapoints are computed from different shuffles.

Figure 21: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2022 exam. LLMs are non-instructed tuned open source models and GPT3.5 with zero-shot. LLM datapoints are computed from different shuffles.

Figure 20: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2022 exam. LLMs are instructed tuned open source models with four-shot. LLM datapoints are computed from different shuffles.

Figure 23: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2022 exam. LLMs are non-instructed tuned open source models and GPT3.5 with one-shot. LLM datapoints are computed from different shuffles.

Figure 22: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2022 exam. LLMs are instructed tuned open source models with zero-shot. LLM datapoints are computed from different shuffles.

## 6 Conclusion

Figure 24: Distribution of CTT (accuracy) and IRT scores for humans and LLMs for the ENEM 2022 exam. LLMs are instructed tuned open source models with one-shot. LLM datapoints are computed from different shuffles.

### Response Patterns for 2022

Figure 26: Response patterns for each LLM, where darker indicates more often correct. Questions are sorted by difficulty (\(\) value). LLMs are instructed tuned open source models with four-shot.

Figure 25: Response patterns for each LLM, where darker indicates more often correct. Questions are sorted by difficulty (\(\) value). LLMs are non-instructed tuned open source models and GPT3.5 with four-shot.

Figure 28: Response patterns for each LLM, where darker indicates more often correct. Questions are sorted by difficulty (\(\) value). LLMs are instructed tuned open source models with zero-shot.

Figure 27: Response patterns for each LLM, where darker indicates more often correct. Questions are sorted by difficulty (\(\) value). LLMs are non-instructed tuned open source models and GPT3.5 with zero-shot.

Figure 30: Response patterns for each LLM, where darker indicates more often correct. Questions are sorted by difficulty (\(\) value). LLMs are instructed tuned open source models with one-shot.

Figure 29: Response patterns for each LLM, where darker indicates more often correct. Questions are sorted by difficulty (\(\) value). LLMs are non-instructed tuned open source models and GPT3.5 with one-shot.

[MISSING_PAGE_EMPTY:39]

Figure 34: Distribution of \(l_{z}\) and IRT scores for humans and LLMs in the ENEM 2022 exam. LLMs are instructed tuned open source models with zero-shot. LLM datapoints are computed from different shuffles.

Figure 35: Distribution of \(l_{z}\) and IRT scores for humans and LLMs in the ENEM 2022 exam. LLMs are non-instructed tuned open source models and GPT3.5 with one-shot. LLM datapoints are computed from different shuffles.

Figure 36: Distribution of \(l_{z}\) and IRT scores for humans and LLMs in the ENEM 2022 exam. LLMs are instructed tuned open source models with one-shot. LLM datapoints are computed from different shuffles.

### Examples of non-discriminating and highly discriminating items for the 2023 Natural Sciences exam.

#### a.10.1 Poorly discriminative questions

**Question 107 (discrimination index -0.013)**

Municipalities are responsible for managing their urban waste (garbage) cleaning and collection according to the Federal Constitution. However, there are reports that part of this waste winds up incenerated, releasing toxic substances into the environment and causing explosions-related accidents when incinerating aerosol bottles (e.g., deodorants, insecticides, and repellents). The high temperature causes all the contents inside these bottles to vaporize, increasing the internal pressure until it explodes.

Suppose there is a metal aerosol bottle with a capacity of 100 milliliters containing 0.1 mol of gaseous products at a temperature of 650 degrees Celsius at the moment of explosion.

Consider: \(R=}{}\)

The pressure, in atmospheres, inside the flask at the moment of the explosion is closest to

1. 756 B. 533 C. 76 D. 53 E. 13

**Question 108 (discrimination index -0.076)**

The circuit with three identical incandescent light bulbs, shown in the figure, consists of a mixed association of resistors. Each bulb (L1, L2, and L3) is associated in parallel with a resistor of resistance R, forming a set. These sets are connected in series, with all the bulbs having the same brightness when connected to the power supply. After several days in use, only lamp L2 burns out, while the others remain lit.

Figure description: a power supply connected to three sets, arranged in series clockwise, in the following sequence: the parallel set of L1 and R, the parallel set of L2 and R, and the parallel set of L3 and R.

In the case where all the bulbs work, after L2 burns out, the brightness of the bulbs will be

1. the same.
2. more intense.
3. less intense.
4. less intense for L1 and the same for L3.
5. more intense for L1 and less intense for L3.

Figure 37: Question 108 Natural Sciences

**Question 109 (discrimination index 0.013)**

A company's transport safety team is evaluating the behavior of the tensions that appear in two horizontal ropes, 1 and 2, used to secure a load of mass M equal to 200 kilograms to the truck, as shown in the illustration. When the truck starts from rest, its acceleration is constant and equal to 3 meters per second squared, while when it arbitrarily brakes, its braking is constant and equal to 5 meters per second squared. In both situations, the load is about to move, and the direction of the truck's movement is shown in the figure. The coefficient of static friction between the box and the bottom surface of the body is 0.2. Consider the acceleration due to gravity to be 10 meters per second squared, the initial tension in the ropes is zero, and the two ropes are ideal.

Figure description: a truck traveling horizontally to the right (represented by the vector V). A box M is resting on the central surface of its body. The box is attached to the rear of the body by horizontal rope 1 and to the front by horizontal rope 2.

When the truck is accelerating and braking, the tensions in ropes 1 and 2 in Newton will be

1. acceleration: T1=0 and T2=200; braking: T1=600 and T2=0.
2. acceleration: T1=0 and T2=200; braking: T1=1400 and T2=0.
3. acceleration: T1=0 and T2=600; braking: T1=600 and T2=0.
4. acceleration: T1=560 and T2=0; braking: T1=0 and T2=960.
5. acceleration: T1=640 and T2=0; braking: T1=0 and T2=1040.

#### 10.10.2 Highly discriminative questions

##### Question 124 (discrimination index 0.650)

Update of the Portuguese Society of Neonatology's recommendation

Glass containing aluminum is an excellent material for packaging medicines and supplements because heating can sterilize it. However, when the drug or supplement contains substances that bind strongly to this metal's ion, the aluminum's dissolution is promoted by the displacement of the chemical equilibrium established between the species immobilized in the glass and the species in solution. For this reason, it is recommended that newborn nutrition supplements containing calcium gluconate be packaged in plastic containers rather than in this type of glass.

If this supplement is packaged in this type of glass, the risk of contamination by aluminum will be greater if the

1. glass of the bottle is translucent.
2. concentration of calcium gluconate is high.
3. glass bottle is thicker.
4. glass is previously sterilized at high temperatures.
5. reaction of aluminum with calcium gluconate is endothermic.

##### Question 91 (discrimination index 0.624)

It is a common requirement to turn off devices, such as cell phones, whose operation involves emitting or receiving electromagnetic waves when traveling by plane. The justification for this procedure is, among other things, the need to eliminate sources of electromagnetic signals that could interfere with the pilots' radio communications with the control tower.

This interference can only occur if the waves emitted by the cell phone and those received by the plane's radio

1. are both audible.
2. have the same power.
3. have the same frequency.
4. have the same intensity.
5. propagate at different speeds.

##### Question 130 (discrimination index 0.621)

The number of bees is in decline in various regions of the world, including Brazil, and multiple factors are contributing to the collapse of their hives. In the United States, seed bombs of native plant species have been used to combat the disappearance of these insects. They are small balls filled with seeds, compost, and clay. When they are thrown and exposed to sun and rain, they germinate even in poorly fertile soil.

This method contributes to the preservation of bees because

1. it reduces predation.
2. it reduces the use of pesticides.
3. it reduces competition for shelter.
4. it increases the food supply.
5. it increases breeding sites.

### Description of Exams

The **Humanities** exam assesses understanding of geographical, cultural, and socioeconomic transformations, as well as comprehension of social and political institutions, technological changes, and the use of historical knowledge to promote conscious engagement in society. It requires recognizing the interactions between society and nature in various historical and geographical contexts.

The **Languages and Codes** exam assesses the use of communication in various contexts. This includes some knowledge and use of foreign languages, understanding of body language, analysis and interpretation of expressive resources in different languages, comprehension of opinions in specific languages, and understanding the impact of communication on personal and social life.

The **Natural Sciences** exam assesses understanding of natural sciences and recognizing their roles in production, economic and social development. It involves associating environmental degradation or conservation with productive and social processes, understanding the interactions between organisms and the environment, and applying specific knowledge of physics, chemistry, and biology.

The **Math** exam assesses the usage of geometric knowledge to represent reality, understanding notions of magnitudes, measurements, and their variations for solving everyday problems, interpreting information of scientific and social nature obtained from reading graphs and tables, and making trend predictions, extrapolations, interpolations, and interpretations.

[MISSING_PAGE_FAIL:45]

Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: We do not include theoretical results. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
* **Experimental Result Reproducibility*
* Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We describe our dataset creation in Section 4.1, data manual auditing process in Appendix A.1, prompting and evaluation details in Section 4.2 and Appendix A.4. We will release our code and data. Guidelines:
* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide the code with reproducibility instructions. We will also provide all the data. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/pu blic/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Prompting and evaluation details given in Section 4.2 and Appendix A.4. Evaluation scripts can be seen in our code. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We describe our option shuffling procedure in Section 4.2 and Appendix A.4 to account for model bias for generating option letters. For all plots, we explain what were the confidence intervals/means are (Figures 1, 2, 4, and 3) Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
* **Experiments Compute Resources*
* Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We disclose the compute related information in Appendix A.5 Guidelines:
* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
* **Code Of Ethics*
* Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the code of ethics and our work conforms with it. Guidelines:
* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
* **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the impact of our work in Section 5 and 6. Guidelines:* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
* **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not forsee any negative impact with our work. Guidelines: The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite and describe in detail the process through which we obtained ENEM dataset in section 4.1. Guidelines: The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We submit our dataset and include details of how we obtained the text in detail in Section 4.1 and Appendix A.1. Human results and related parameters are released by ENEM officials. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We did not crowdsource. The human results are published by ENEM officials. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our study does not involve human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.