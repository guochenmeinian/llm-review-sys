# HourVideo: 1-Hour Video-Language Understanding

Keshigeyan Chandrasegaran

Agrim Gupta

Lea M. Hadzic

Taran Kota

Jimming He

Cristobal Eyzaguirre

Zane Durante

Manling Li

Jiajun Wu

Li Fei-Fei

hourvideo.stanford.edu

###### Abstract

We present **HourVideo**, a benchmark dataset for hour-long video-language understanding. Our dataset consists of a novel task suite comprising summarization, perception (_recall, tracking_), visual reasoning (_spatial, temporal, predictive, causal, counterfactual_), and navigation (_room-to-room, object retrieval_) tasks. HourVideo includes 500 manually curated egocentric videos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and features **12,976 high-quality, five-way multiple-choice questions**. Benchmarking results reveal that multimodal models, including GPT-4 and LLaVA-NeXT, achieve marginal improvements over random chance. In stark contrast, human experts significantly outperform the state-of-the-art long-context multimodal model, Gemini Pro 1.5 (85.0% vs. 37.3%), highlighting a substantial gap in multimodal capabilities. Our benchmark, evaluation toolkit, prompts, and documentation are available at hourvideo.stanford.edu.

+
Footnote †: Correspondence to {keshik,agrim}@stanford.edu

## 1 Introduction

Humans demonstrate a remarkable ability to process visual stimuli over long time horizons, enabling them to perceive, plan and act in the real world. Consider the routine task of cooking a meal. This activity involves a continuous and adaptive visual process: identifying and using ingredients and tools, monitoring state changes of various dishes, and adjusting cooking duration/techniques based on visual cues such as color and texture. Such sustained visual processing is crucial to achieving the desired culinary outcomes. Naturally, endowing autonomous agents with this capability has been a long-standing goal in the field of Artificial Intelligence.

In recent years, large multimodal models [1; 2; 3] have emerged as a promising approach toward achieving this goal. Typically, these models are evaluated using multiple datasets that test capabilities such as object recognition [4; 5], image comprehension [6; 7; 8], and action recognition . However, these benchmarks are often restricted to single images or short video clips, usually lasting from a few seconds to no more than three minutes [9; 10; 11; 12]. While these benchmarks have spurred significant advancements, a deeper exploration into long-form video-language understanding is essential to develop multimodal systems that can form the basis for future autonomous agents and assistants.

A significant challenge in evaluating long-form video-language understanding capabilities is designing tasks that genuinely necessitate _long-term_ comprehension, i.e., tasks that require long-range dependencies. Merely posing questions that can be answered by watching a brief segment of a lengthy video effectively reduces the task to a combination of temporal localization and short-clip understanding. Furthermore, while intriguing narrative inquiries can certainly be formulated for long-form videos such as television shows and films, it is imperative to ensure that the questions are not trivially answerable due to the vast prior knowledge encoded in modern large language models.

In this work, we introduce **HourVideo**--a benchmark dataset designed for long-form video-language understanding. To design tasks that require _long-term_ comprehension, we first propose a novel tasksuite (Tab. 1), comprising **summarization**, **perception** (_recall_, _tracking_), **visual reasoning** (_spatial_, _temporal_, _predictive_, _causal_, _counterfactual_), and **navigation** (_room-to-room_, _object retrieval_) tasks. For each task, we manually create question prototypes designed to ensure that correctly answering them requires identification and synthesis of information across multiple temporal segments within the long-form videos. Guided by our task suite, we curated 500 egocentric videos from the Ego4D dataset --covering 77 unique everyday activities and ranging from 20 to 120 minutes--to generate questions based on our prototypes. The combination of our comprehensive task suite and everyday mundane egocentric videos provides a robust framework to rigorously evaluate multimodal models' capabilities in understanding long-form videos. Finally, we developed a question-answer generation pipeline utilizing the expertise of trained human annotators (800+ hours of effort) and large language models (LLMs), resulting in a collection of 12,976 high-quality, five-way multiple-choice questions.

We comprehensively evaluate state-of-the-art multimodal models on HourVideo (Tab. 2, Fig. 4), including GPT-4V , Gemini 1.5 Pro , and LLaVA-NeXT  in a zero-shot setting. Our findings reveal that GPT-4V and LLaVA-NeXT achieve only marginal improvements over a random predictor (20%), obtaining accuracies of 25.7% and 22.3%, respectively. Gemini 1.5 Pro, designed specifically for long-context multimodal understanding, obtains an accuracy of 37.3%, which, while better, is still substantially lower than the average performance of human experts at 85.0%. These results suggest that while the multimodal community has made meaningful progress, a significant gap remains to be bridged before these systems can match human-level long-form video understanding capabilities. Progress in long-form video understanding could enable new applications including AR assistants, embodied agents, and interactive video platforms. We hope that HourVideo will serve as a benchmark to facilitate research in this direction and enable the development of multimodal models that can understand endless streams of visual data.

## 2 Benchmark Design and Construction

While open-ended question answering closely emulates human interaction, automating the evaluation of free-form natural language responses remains challenging. Given that our primary goal is to assess long-form video-language understanding capabilities, we opt for a five-way multiple-choice question-answering (MCQ) task. This approach simplifies the evaluation process by allowing to calculate an aggregate question-answering accuracy metric. In the following section, we describe our task suite and question-answer generation pipeline in detail, both of which are designed to curate diverse high-quality five-way multiple-choice questions (MCQs).

### Task Suite

Creating a comprehensive benchmark for long-form video-language understanding is challenging, primarily because formulating meaningful questions that require processing and synthesizing information across various temporal segments is highly nontrivial, even for expert human annotators. Moreover, we note that even benchmarks for image or short video clip understanding are difficult to construct. As a result, we typically observe two common strategies for benchmark creation: (1) pre-defined label spaces testing for a specific skill or within narrow domains (e.g., Kinetics  and Something-Something ); or (2) gluing together different datasets, each designed to test a specific model capability [16; 17; 18; 19]. In contrast, a single benchmark that can comprehensively test a suite of model capabilities can significantly benefit the research community.

We draw inspiration from both lines of research methodologies and introduce a novel suite of tasks designed to benchmark long-form video-language understanding capabilities for one-hour-long videos. Our task suite encompasses a comprehensive set of perceptual and cognitive tasks, including summarization, perception (recall, tracking), visual reasoning (spatial, temporal, predictive, causal, counterfactual), and navigation (room-to-room, object retrieval) tasks. Our strategy draws inspiration from the two common approaches previously discussed: (1) designing narrowly focused question prototypes to significantly streamline the question-answer creation process, and (2) creating a diverse suite of tasks that holistically evaluate a broad spectrum of multimodal capabilities. Our task suite with manually designed question prototypes are shown in Table 1. In particular, there are 18 sub-tasks in our proposed task suite and example MCQs from HourVideo are shown in Fig. 1.

[MISSING_PAGE_EMPTY:3]

### Dataset Generation Pipeline

In this section, we provide an overview of the question-answer creation pipeline that we developed to create HourVideo. The pipeline is summarized in Fig. 2.

**Video curation, Stage 1.** A crucial design consideration for this benchmark is the selection of video sources and types. We chose the Ego4D  dataset for our videos for multiple reasons: (1) its egocentric perspective aligns well with the typical visual input for autonomous agents and assistants; (2) it features extensive visual narrations, which aid in creating diverse multiple-choice questions; and (3) it is readily accessible under the Ego4D license. We manually reviewed 1,470 videos, ranging from 20 to 120 minutes, from the Ego4D dataset, assessing their potential to generate relevant questions for various tasks in our task suite. We engaged five human experts for video curation. Following this process, we curated 500 egocentric videos.

 p{284.5pt}}    & **Summarization** \\ 
**Key Events/ Objects** & _Summarize the key interactions of the camera wearer in the [supermarket]_. \\ 
**Temporal Sequencing** & _Describe the sequence of activities performed by the camera wearer to [prepare the desser1]_. \\ 
**Compare/ Contrast** & _How did the camera wearer’s activities in the [apartment] differ from those in the [restaurant]?_ \\   & **Perception** \\  
**Information Retrieval** & \\  \(\) Factual Recall & _What [dairy products] did the camera wearer [pick up] in the [supermarket]?_ \\  \(\) Sequence Recall & _What did the camera wearer do immediately after [weighing tomatoes] at the [supermarket]?_ \\  \(\) Temporal Distance & _How long after starting to [eat pizza] did the camera wearer [dispose of the pizza box]?_ \\ 
**Tracking** & _List the unique [individuals] the camera wearer interacted with at the [dragstore]_. \\ }{**Visual Reasoning**} \\ 
**Spatial** & \\  \(\) Relationship & _Where was the [microwave] placed in relation to the [stove] in the [kitchen]?_ \\  \(\) Proximity & _Is the [microwave] closer to the [fridge] compared to the [sink]?_ \\  \(\) Layout & _Which is the correct [IMAGE] depicting the layout of the camera wearer’s [apartment]?_ \\ 
**Temporal** & \\  \(\) Duration & _Which activity did the camera wearer spend more time on: [cooking] or [playing the piano]?_ \\  \(\) Frequency & _Did the camera wearer use the [circular saw] or [crosscut saw] more frequently to [cut wood]?_ \\  \(\) Pre-requisites & _What preparation steps did the camera wearer take before [baking cookies]?_ \\ 
**Predictive** & _What is the most likely activity the camera wearer will do next after [doing laundry]?_ \\ 
**Causal** & _Why did the camera wearer [leave the garage for the second time]?_ \\ 
**Counterfactual** & _What if the camera wearer used the [oven] to [cook mashed potatoes]?_ \\   & **Navigation** \\ 
**Room-to-Room** & _How did the camera wearer get from the [building entrance] to the [apartment]?_ \\ 
**Object Retrieval** & _How can the camera wearer retrieve the [TV remote] if they are in the [kitchen]?_ \\   

Table 1: **Our proposed task suite with question prototypes.** This table shows all 4 tasks and 18 sub-tasks proposed in **HourVideo**, along with the corresponding handcrafted question prototypes designed to evaluate long-form video-language understanding capabilities.

**Candidate MCQ Generation, Stage 2.** The objective of this stage is to produce high-quality MCQs for each task, requiring analysis and synthesis of information across multiple temporal segments in a long-form video. Initially, we manually develop question template(s) for each task in the suite. As shown in Table 1, transforming a question template into an actual question involves incorporating video-specific information tailored to the task and template. To facilitate this, we utilize the detailed narrations from the Ego4D dataset, transforming them into a structured format that can be processed by an LLM. Specifically, we segment the video at 20-minute intervals, with each segment's representation including a summary and a list of tools, food items, technology, humans, pets, and physical locations encountered by the camera wearer in the video. We note that synthesizing a structured representation and a question template into a valid question with correct and incorrect answers presents a significant challenge, even for advanced LLMs. Consequently, for each task, we formulate detailed prompts that offer question prototypes, comprehensive instructions, in-context examples, and step-by-step guidance on how to transform a question template into a valid candidate \(_{2}\). In total, we developed 25 task-specific prompts.

MCQ **Refinement with LLMs using Human Feedback, Stage 3.** The purpose of this phase is to refine \(_{2}\), created in the previous stage. \(_{2}\) may contain invalid questions, incorrect answers, trivial incorrect options, and various other issues. We identified that a significant source of these issues stemmed from relying on the noisy narrations in Ego4D. For example, different narrators within the same video could refer to a dishwasher as a "plate rack" or use other terms, and an individual might be described as an "adult," "person with a red and white shirt," "man Y," or "teenager" at various times in the narration. These inconsistencies, combined with our automatic question generation in the first stage, could lead to generation of invalid \(\)s. To address noisy \(\)s, we implement a human feedback system where trained annotators are tasked with: 1) assessing the validity of each question to ensure it aligns with the video content, 2) verifying the accuracy of the given answer--if found incorrect, they provide the correct answer in free-form text, 3) ensuring that all incorrect options are factually wrong and clearly distinguishable from the correct answer. We gather human feedback for all \(_{2}\), involving over 400 hours of human effort. We then design prompts, to automatically refine \(_{2}\) using this human feedback to produce \(_{3}\). We engaged seven trained annotators in this stage.

**Blind filtering, Stage 4.** Modern LLMs possess extensive prior knowledge and can thus easily answer certain questions without needing to analyze the videos. The objective of this phase is to eliminate questions that can be answered through prior knowledge or can be trivially answered without requiring any information from the video. To address this, we do blind filtering of \(_{3}\), utilizing two separate blind LLMs (GPT-4-turbo and GPT-4). Specifically, we exclude any MCQ that is correctly answered by at least one LLM without video input. Although this method may aggressively remove MCQs, it ensures that the remaining \(_{4}\) are of high quality and specifically tailored to test long-form video-language understanding.

Figure 2: **Our dataset generation pipeline.** We develop a dataset generation pipeline consisting of five stages to create HourVideo. We leverage over _800 hours of human effort_ in total corresponding to Video curation (Stage 1), \(\) Refinement using Human Feedback (Stage 3) and Expert MCQ Refinement (Stage 5) stages. We use LLMs for \(\) Generation (Stage 2), \(\) Refinement using Human Feedback (Stage 3) and Blind Filtering (Stage 4). We note that causal, counterfactual and navigation questions are manually generated by human experts (See Sec. 2.2 for details).

[MISSING_PAGE_FAIL:6]

Experiments

### Evaluation Protocol

HourVideo includes five-way multiple-choice questions, for which we report accuracies per task and in aggregate across the entire dataset. A significant challenge in evaluating MCQs over long videos is preventing information leakage across questions. Ideally, each MCQ should be evaluated independently to avoid this issue, but unfortunately, this approach is computationally expensive and time-consuming. Therefore, for our evaluation, we assess the questions in batches, with each batch containing all questions related to a specific task or sub-task. For predictive tasks (reasoning), we provide precise timestamps to trim the videos for targeted evaluation. Details on tasks and sub-tasks requiring independent evaluation are provided in Supplementary B.

### Baselines

In this section, we compare the performance of different multimodal models on understanding long videos in a zero-shot setting. Specifically, we evaluate three classes of models: (1) Blind LLMs, (2) Socratic Models , and (3) Native multimodal models. All these models operate under a common function \(A=M(V,,Q)\) where \(V,,Q,M,A\) refer to the long-form video input, prompt (instruction), multiple-choice question, multimodal model, and text output respectively.

**Blind LLMs.** Modern LLMs possess extensive prior knowledge, enabling them to easily answer certain questions without the need to analyze videos. Furthermore, it is likely that some questions can be trivially answered by exploiting biases in the question-answer pairs. The 'blind' LLM baseline is designed to evaluate this by asking the LLM to answer the multiple-choice question without considering any visual information from the video, i.e., \(A=M(,Q)\), where \(\) is a generic task-agnostic prompt prepended to the question \(Q\). We use GPT-4  as our LLM for this baseline.

**Socratic Models.** Most current state-of-the-art multimodal models are unable to process very long videos. Therefore, to benchmark these models, we use the Socratic models approach . In this approach, the video \(V\), with a total duration of \(t\) minutes, is segmented into one-minute intervals, each denoted as \(V[i]\) for minute \(i\). Each segment \(V[i]\) is independently captioned, yielding a sequence of captions \(z_{1},z_{2},z_{3},,z_{t}\), where \(z_{i}=(V[i])\). These captions are aggregated to form a comprehensive language-based representation of the video, referred to as the world state history, which includes timestamps. This textual representation, along with a generic task-agnostic prompt \(\), serves as the input for long-form video-question answering: \(A=M([,z_{1},z_{2},,z_{t},Q])\). We sample one-minute video clips at a rate of 0.5 fps and a resolution of 512\(\)384. We test using both GPT-4  and LLaVA-NeXT-34B-DPO  as the Video-Captioner. Finally, we use GPT-4 for actual question answering, as LLaVA-NeXT-34B-DPO does not support the extended context length required to process our world state history.

**Native Multimodal Models.** Multimodal video models, such as Gemini 1.5 Pro , are trained _jointly_ on multimodal data, including audio, video, images, and text. These models are particularly adept at handling very long context lengths (2M+), making them ideal for end-to-end evaluation using our benchmark. Evaluating these models is straightforward, as they can directly process hour-long videos as \(A=M(V,,Q)\). For all experiments, we use a sampling rate of 0.5 frames per second, a resolution of 512 \(\) 384, and a temperature setting of 0.1.

**Human performance.** Due to the high costs associated with human evaluations, we sampled 14 videos from our benchmark, which included more than 18 scenarios in total including crafting/painting, cooking, construction/renovation, gardening, cleaning/laundry and yard work. We ask three human experts to conduct evaluations on 11.2 hours of video content, encompassing a total of 213 MCQs. To prevent any contamination, we ensured that human experts who evaluated videos were not involved in the annotation of the same videos at any earlier stage (Stages 3 and 5 discussed in Sec. 2). The human experts achieve an accuracy of **85.0%**. The results are shown in Fig. 4.

### Results

We report all task and sub-task level quantitative results in Tab. 2. Qualitative evaluations, including human evaluation numbers, are presented in Fig. 4. We remark that random guessing corresponds to 20% accuracy. Below, we discuss our key observations.

[MISSING_PAGE_FAIL:8]

**Socratic models vs. Native Multimodal Models.** Gemini 1.5 Pro outperforms Socratic models by a considerable margin across all 4 tasks-summarization, perception, visual reasoning, and navigation-indicating that similar models may be promising avenues toward long-form video-language understanding. On aggregate, Gemini 1.5 Pro outperforms the GPT-4-based Socratic model by 11.6%. Despite these significant improvements, it is important to note that Gemini's performance, at 37.3%, still lags significantly behind that of human experts, who achieve 85.0%.

**Independent vs. Task-level MCQ evaluation.** To investigate the validity of our proposed task/sub-task level evaluation method, we conducted an ablation study where each multiple-choice question (MCQ) was evaluated independently. For this, we used 15.9 hours of video and 570 MCQs across 25 randomly selected videos. We used Gemini 1.5 Pro, which demonstrated the highest performance on HourVideo (37.3%). The results and evaluation costs are shown in Tab. 3. There is a minor drop (2.1%) in performance when evaluating each MCQ independently; however, the associated costs increase by more than threefold. These results highlight the efficiency and validity of our proposed task-level/subtask level evaluation method. We will require benchmark submissions to indicate whether they used task-level or individual MCQ evaluation when submitting their results, allowing for greater transparency and comparability between methods.

## 4 Related Work

Dataset Comparison.Existing video benchmarks [23; 24; 25; 26; 27; 28; 29; 30; 31; 32], primarily focus on specific domains or short videos, which limit their ability to assess long-form video understanding comprehensively. Efforts like WebVid10M , InternVid , and Panda-70M  include detailed captions to provide video pretraining data but consist primarily of short video clips less than one minute in length and do not provide QA pairs. Recent works have introduced several benchmarks specifically designed for long video understanding, such as Next-QA , Next-GQA , VideoChatGPT , EgoSchema , MovieChat-1K  and MovieNet-QA .  introduced benchmarks for evaluating relational space-time query tasks. Perception Test  proposed a diagnostic benchmark for multimodal models, probing for memory, abstraction, physics, and semantic capabilities using short video clips (23s average duration). However, the average video length in these datasets is still relatively short, with Ego-Schema having an average duration of 3 minutes. In contrast, we focus on hour-long video-language understanding, with videos averaging 45.7 minutes in duration (Table 4) and tasks requiring long-term comprehension.

Video Understanding Tasks.Significant efforts have been made to design tasks appropriate for evaluating multimodal large language models (MLLMs) [43; 44; 45; 46; 47; 48; 49; 50; 51]. The evaluation of vision-language models (VLMs) focuses mainly on visual perception tasks such as image-text matching, retrieval, captioning, object detection, and visual grounding tasks) [45; 46; 47]. Methods revoking around contrastive learning on image-text pairs have proven to be effective methods for learning transferable representations for these visual tasks [52; 53; 54], and have been shown to be effective in more specific domains such as multi-disciplinary scientific understanding [50; 55] and multi-modal mathematical reasoning [48; 49]. Later work has improved upon the visual reasoning capabilities of VLMs [1; 56; 57; 58; 59; 60; 61; 62; 63] and their ability to reason across complex spatio-temporal video data [64; 65; 66; 67; 68; 69; 70; 71]. To better evaluate spatio-temporal abilities, specific benchmarks [12; 72; 73; 31; 72] have been developed. However, the questions in

  
**Benchmark** & **\# Videos** & **Avg. len. (mins)** & **\# Questions** \\  MSRVTC-QA  & 2,990 & 0.25 & 72,821 \\ ActivityNet-QA  & 800 & 1.85 & 8,000 \\ TVQA  & 2,179 & 0.19 & 15,253 \\ How2QA  & 1,166 & 0.25 & 2,852 \\ NExt-QA  & 1,000 & 0.66 & 8,564 \\ EgoSchema  & 5,063 & 3.0 & 5,063 \\ 
**HourVideo** & **500** & **45.7** & **12,976** \\   

Table 4: **Dataset statistics comparison between video understanding benchmarks.**

   & **Performance** & **Total Tokens** & **Evaluation Cost** \\  Task-level & 38.9\% & 120,818,343 & 8846 \\  Individual & 36.8\% & 374,396,885 & 52621 \\  

Table 3: Performance and evaluation cost comparison for our proposed task/sub-task level vs. individual MCQ evaluation.

many of these datasets are often not challenging enough to fully evaluate the capabilities of models in understanding long-form video content and can often be answered from only a single frame . In contrast, our benchmark focuses on evaluating the capabilities needed to reason over a significantly longer duration and with more sophisticated reasoning. The questions in our dataset are designed to be highly challenging, with novel video question categories such as navigation highlighting our benchmark's ability to effectively assess the limitations of current state-of-the-art multimodal models in comprehending long-form videos.

Long-Form Video Understanding.To extend video-language models  to long-form videos, the main challenge lies in efficiently encoding the temporal and spatial dynamics over a long horizon. One widely used strategy is to maintain a memory bank to store history information in long videos . Alternatively, other methods have been proposed to compact spatio-temporal tokens into a smaller set of compressed or merged tokens to reduce redundancy and alleviate computational burden . Another line of work leverages language as a bridge by first generating textual descriptions for shorter video clips sub-sampled from the longer video and then employing an LLM to aggregate the short captions for longer video understanding . In contrast, approaches like TimeChat  and VTimeLLM  aim to enhance temporal localization capabilities by encoding timestamp knowledge into visual tokens or using multi-stage training methods. Despite these extensive efforts, long-form video understanding remains a significant challenge for the current generation of multimodal models.

## 5 Conclusion

We introduce **HourVideo**, a novel benchmark dataset designed to rigorously evaluate the capabilities of multimodal models to comprehend one-hour-long videos. Our dataset consists of a novel task suite comprising summarization, perception (_recall, tracking_), visual reasoning (_spatial, temporal, predictive, causal, counterfactual_), and navigation (_room-to-room, object retrieval_) tasks. This benchmark includes 500 egocentric videos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and features 12,976 high-quality five-way multiple-choice questions. Our zero-shot evaluation on HourVideo reveal that multimodal models such as GPT-4V and LLaVA-NeXT exhibit performance levels only slightly better than random guessing. In stark contrast, human expert performance substantially surpasses state-of-the-art long-context multimodal model Gemini 1.5 Pro (85.0% accuracy versus 37.3%), highlighting significant research gap. We aim to establish HourVideo as a benchmark challenge to spur the development of advanced multimodal models capable of truly understanding endless streams of visual data.

Limitations and future work.Despite our substantial efforts to create a high-quality benchmark dataset, there may still be some inconsistencies within the multiple-choice questions. Additionally, while this is currently the largest long-form video-language understanding benchmark of its kind to the best of our knowledge, we acknowledge the need for more holistic benchmarks that include diverse video sources such as sports and YouTube videos. Lastly, we note that incorporating support for the audio modality is essential for more comprehensive evaluation of multimodal models. We also remark that our world extends beyond visual and auditory stimuli to include other sensory modalities (e.g., tactile), suggesting opportunities to explore these additional modalities in future work. We discuss broader impact of HourVideo in Supplementary E.