# Are Graph Neural Networks Optimal Approximation Algorithms?

Morris Yau

MIT CSAIL

morrisy@mit.edu &Nikolaos Karalias

MIT CSAIL

stalence@mit.edu &Eric Lu

Harvard University

ericlu01@g.harvard.edu &Jessica Xu

Independent researcher formerly at MIT

jessica.peng.xu@gmail.com &Stefanie Jegelka

TUM1 and MIT2

stefje@mit.edu

CIT, MCML, MDSI

EECS and CSAIL

###### Abstract

In this work we design graph neural network architectures that capture optimal approximation algorithms for a large class of combinatorial optimization problems, using powerful algorithmic tools from semidefinite programming (SDP). Concretely, we prove that polynomial-sized message-passing GNN's can learn the most powerful polynomial time algorithms for Max Constraint Satisfaction Problems assuming the Unique Games Conjecture. We leverage this result to construct efficient graph neural network architectures, OptGNN, that obtain high-quality approximate solutions on landmark combinatorial optimization problems such as Max-Cut, Min-Vertex-Cover, and Max-3-SAT. Our approach achieves strong empirical results across a wide range of real-world and synthetic datasets against solvers and neural baselines. Finally, we take advantage of OptGNN's ability to capture convex relaxations to design an algorithm for producing bounds on the optimal solution from the learned embeddings of OptGNN.

## 1 Introduction

Combinatorial Optimization (CO) is the class of problems that optimize functions subject to constraints over discrete search spaces. They are often NP-hard to solve and to approximate, owing to their typically exponential search spaces over nonconvex domains. Nevertheless, their important applications in science and engineering (Gardiner et al., 2000; Zaki et al., 1997; Smith et al., 2004; Du et al., 2017) has engendered a long history of study rooted in the following simple insight. In practice, CO instances are endowed with domain-specific structure that can be exploited by specialized algorithms (Hespe et al., 2020; Walteros & Buchanan, 2019; Ganesh & Vardi, 2020). In this context, neural networks are natural candidates for learning and then exploiting patterns in the data distribution over CO instances.

The emerging field at the intersection of machine learning (ML) and combinatorial optimization (CO) has led to novel algorithms with promising empirical results for several CO problems. However, similar to classical approaches to CO, ML pipelines have to manage a tradeoff between efficiency and optimality. Indeed, prominent works in this line of research forego optimality and focus on parametrizing heuristics (Li et al., 2018; Khalil et al., 2017; Yolcu & Poczos, 2019; Chen & Tian, 2019) or by employing specialized models (Zhang et al., 2023; Nazari et al., 2018; Toenshoff et al., 2019; Xu et al., 2021; Min et al., 2022) and task-specific loss functions (Amizadeh et al., 2018;Karalias & Loukas, 2020; Wang et al., 2022; Karalias et al., 2022; Sun et al., 2022). Exact ML solvers that can guarantee optimality often leverage general techniques like branch and bound (Gasse et al., 2019; Paulus et al., 2022) and constraint programming (Parjadis et al., 2021; Cappart et al., 2019), which offer the additional benefit of providing approximate solutions together with a bound on the distance to the optimal solution. The downside of those methods is their exponential worst-case time complexity. Striking the right balance between efficiency and optimality is quite challenging, which leads us to the central question of this paper:

_Are there neural architectures for efficient combinatorial optimization that can learn to adapt to a data distribution over instances yet capture algorithms with **optimal** worst-case approximation guarantees?_

To answer this question, we build on the extensive literature on approximation algorithms and semidefinite programming (SDP) which has led to breakthrough results for NP-hard combinatorial problems, such as the Goemans-Williamson approximation algorithm for Max-Cut (Goemans & Williamson, 1995) and the use of the Lovasz theta function to find the maximum independent set on perfect graphs (Lovasz, 1979; Grotschel et al., 1981). For several problems, it is known that if the Unique Games Conjecture (UGC) is true, then the approximation guarantees obtained through SDP relaxations are indeed the best that can be achieved (Raghavendra, 2008; Barak & Steurer, 2014). The key insight of our work is that a polynomial time message-passing algorithm (see Definition G) approximates the solution of an SDP with the optimal distribution of the solution.

Our contributions can be summarized as follows:

* We construct a polynomial-time message passing algorithm (see Definition G) for solving the SDP of Raghavendra (2008) for the broad class of maximum constraint satisfaction problems (including Max-Cut, Max-SAT, etc.), that is optimal barring the possibility of significant breakthroughs in the field of approximation algorithms.
* We construct a graph neural network architecture which we call OptGNN, to capture this message-passing algorithm. We show that OptGNN is PAC-learnable with polynomial sample complexity.
* We describe how to generate optimality certificates from the learned representations of OptGNN i.e., provable bounds on the optimal solution.
* Empirically, OptGNN is simple to implement (see pseudocode in Appendix 5) and we show that it achieves competitive results on 3 landmark CO problems and several datasets against classical heuristics, solvers and state-of-the-art neural baselines 3. Furthermore, we provide out-of-distribution (OOD) evaluations and ablation studies for OptGNN that further validate our parametrized message-passing approach.

Figure 1: Schematic representation of OptGNN. During training, OptGNN produces node embeddings \(\) using message passing updates on the graph \(G\). These embeddings are used to compute the penalized objective \(_{p}(;G)\). OptGNN is trained by minimizing the average loss over the training set. At inference time, the fractional solutions (embeddings) \(\) for an input graph \(G\) produced by OptGNN are rounded using randomized rounding.

Background and Related Work

**Optimal Approximation Algorithms.** Proving that an algorithm achieves the best approximation guarantee for NP-hard problems is an enormous scientific challenge as it requires ruling out that better algorithms exist (i.e., a hardness result). The Unique Games Conjecture (UGC) (Khot, 2002) is a striking development in the theory of approximation algorithms because it addresses this obstacle. If true, it implies approximation hardness results for a large number of hard combinatorial problems that match the approximation guarantees of the best-known algorithms (Raghavendra and Steurer, 2009b; Raghavendra et al., 2012). For that reason, those algorithms are also sometimes referred to as UGC-optimal. More importantly, the UGC implies that for all Max-CSPs there is a general UGC-optimal algorithm based on semidefinite programming (Raghavendra, 2008). For a complete exposition on the topic of UGC and approximation algorithms, we refer the reader to Barak and Steurer (2014).

**Neural approximation algorithms and their limitations.** There has been important progress in characterizing the combinatorial capabilities of modern deep learning architectures, including bounds on the approximation guarantees achievable by GNNs on bounded degree graphs (Sato et al., 2019, 2021) and conditional impossibility results for solving classic combinatorial problems such as Max-Independent-Set and Min-Spanning-Tree (Loukas, 2019). It has been shown that a GNN can implement a distributed local algorithm that straightforwardly obtains a 1/2-approximation for Max-SAT (Liu et al., 2021), which is also achievable through a simple randomized algorithm (Johnson, 1973). Recent work proves there are barriers to the approximation power of GNNs for combinatorial problems including Max-Cut and Min-Vertex-Cover (Gamarnik, 2023) for constant depth GNNs. Other approaches to obtaining approximation guarantees propose avoiding the dependence of the model on the size of the instance with a divide-and-conquer strategy; the problem is subdivided into smaller problems which are then solved with a neural network (McCarty et al., 2021; Kahng et al., 2023).

**Convex Relaxations and Machine Learning.** Convex relaxations are crucial in the design of approximation algorithms. In this work, we show how SDP-based approximation algorithms can be incorporated into the architecture of neural networks. We draw inspiration from the algorithms that are used for solving low-rank SDPs (Burer and Monteiro, 2003; Wang et al., 2017; Wang and Kolter, 2019; Boumal et al., 2020). Beyond approximation algorithms, there is work on designing differentiable Max-SAT solvers via SDPs to facilitate symbolic reasoning in neural architectures (Wang et al., 2019). This approach uses a fixed algorithm for solving a novel SDP relaxation for Max-SAT, and aims to learn the structure of the SAT instance. In our case, the instance is given, but our algorithm is learnable, and we seek to predict the solution. SDPs have found numerous applications in machine learning including neural network verification (Brown et al., 2022), differentiable learning with discrete functions (Karalias et al., 2022), kernel methods (Rudi et al., 2020; Jethava et al., 2013) and quantum information tasks (Krivachy et al., 2021). Convex relaxation are also instrumental in integer programming where branch-and-bound tree search is guaranteed to terminate with optimal integral solutions to Mixed Integer Linear Programs (MILP). Proposals for incorporating neural networks into the MILP pipeline include providing a "warm start" (Benidis et al., 2023) to the solver, learning branching heuristics (Gasse et al., 2019; Nair et al., 2020; Gupta et al., 2020; Paulus et al., 2022), and learning cutting plane protocols (Paulus et al., 2022). A recent line of work studies the capabilities of neural networks to solve linear programs (Chen et al., 2022; Qian et al., 2023). It is shown that GNNs can represent LP solvers, which may in turn explain the success of learning branching heuristics (Qian et al., 2023). In a similar line of work, neural nets are used to learn branching heuristics for CDCL SAT solvers (Selsam and Bjorner, 2019; Kurin et al., 2020; Wang et al., 2021). Finally, convex optimization has also found applications (Numeroso et al., 2023) in the neural algorithmic reasoning paradigm (Velickovic et al., 2022) where neural networks are trained to solve problems by learning to emulate discrete algorithms in higher dimensional spaces.

**Learning frameworks for CO.** A common approach to neural CO is to use supervision either in the form of execution traces of expert algorithms or labeled solutions (Li et al., 2018; Selsam et al., 2018; Prates et al., 2019; Vinyals et al., 2015; Joshi et al., 2019, 2020; Gasse et al., 2019; Ibarz et al., 2022; Georgiev et al., 2023). Obtaining labels for combinatorial problems can be computationally costly which has led to the development of neural network pipelines that can be trained without access to labels or partial solutions. This includes approaches based on Reinforcement Learning (Ahn et al., 2020; Bother et al., 2022; Barrett et al., 2020, 2022; Bello et al., 2016; Khalil et al., 2017; Yolcu and Poczos, 2019; Chen and Tian, 2019), and other self-supervised methods (Brusca et al., 2023; Karalias et al., 2022; Karalias and Loukas, 2020; Tonshoff et al., 2022; Schuetz et al., 2022a, b; Amizadeh et al., 2019; Dai et al., 2020; Sun et al., 2022; Wang et al., 2022; Amizadeh et al., 2018; Gaile et al., 2022). Our work falls into the latter category since only the problem instance is sufficient for training and supervision signals are not required. For a complete overview of the field, we refer the reader to the relevant survey papers (Cappart et al., 2023; Bengio et al., 2021).

## 3 Optimal Approximation Algorithms with Neural Networks

We begin by showing that solving the Max-Cut problem using a vector (low-rank SDP) relaxation and a simple projected gradient descent scheme amounts to executing a message-passing algorithm on the input graph. We then generalize this insight to the entire class of Max-CSPs. We reformulate the UGC-optimal SDP for Max-CSP in SDP 1. Our main Theorem 3.1 exhibits a message passing algorithm (Algorithm 1) for solving SDP 1. We then capture Algorithm 1 via a message passing GNN with learnable weights (see Definition G for definition of Message Passing GNN). Thus, by construction OptGNN captures algorithms with UGC-optimal approximation guarantees for Max-CSP. Furthermore, we prove that OptGNN is efficiently PAC-learnable (see Lemma 3.1) as a step towards explaining its empirical performance.

### Solving Combinatorial Optimization Problems with Message Passing

In the Max-Cut problem, we are given a graph \(G=(V,E)\) with \(N\) vertices \(V\) and edges \(E\). The goal is to divide the vertices into two sets that maximize the number of edges going between them. This corresponds to the quadratic integer program

\[_{x_{1},x_{2},,x_{N}}\ \ _{(i,j) E}(1-x_{i}x_{j}) \ \ \ \ \ \ x_{i}^{2}=1\ \ \  i[N].\]

The global optimum of the integer program is the Max-Cut. Noting that discrete variables are not amenable to the tools of continuous optimization, a standard technique is to 'lift' the quadratic integer problem: replace the integer variables \(x_{i}\) with vectors \(v_{i}^{r}\) and constrain \(v_{i}\) to lie on the unit sphere

\[_{v_{1},v_{2},,v_{N}}\ \ -_{(i,j) E}(1-  v_{i},v_{j})\ \ \ \ \ \ \|v_{i}\|=1\ \ \ \  i[N]\ \ \ \ v_{i}^{r}.\] (1)

This nonconvex relaxation of the problem admits an efficient algorithm Burer and Monteiro (2003). Furthermore, all local minima are approximately global minima (Ge et al., 2016) and variations of stochastic gradient descent converge to its optimum (Bhojanapalli et al., 2018; Jin et al., 2017) under a variety of smoothness and compactness assumptions. Specifically, for large enough \(r\)(Boumal et al., 2020; O'Carroll et al., 2022), simple algorithms such as block coordinate descent (Erdogdu et al., 2019) can find an approximate global optimum of the objective. Projected gradient descent is a natural approach for solving the minimization problem in equation 1. In iteration \(t\) (for \(T\) iterations), update vector \(v_{i}\) as

\[v_{i}^{t+1}=(v_{i}^{t}-_{j  N(i)}v_{j}^{t}),\] (2)

where NORMALize enforces unit Euclidean norm, \(^{+}\) is an adjustable step size, and \(N(i)\) the neighborhood of node \(i\). The gradient updates to the vectors are local, each vector is updated by aggregating information from its neighboring vectors (i.e., it is a message-passing algorithm).

**OptGNN for Max-Cut.** Our main contribution in this paper builds on the following observation. We may generalize the dynamics described above by considering an overparametrized version of the gradient descent updates in equations 2. Let \(M_{1},M_{2},...,M_{T}^{r 2r}\) be a set of \(T\) learnable matrices corresponding to \(T\) layers of a neural network. Then for layer \(t\) and embedding \(v_{i}\) we define a GNN update

\[v_{i}^{t+1}:=(M_{t}([_{j  N(i)}v_{j}^{t}])).\] (3)More generally, we can write the dynamics as \(v_{i}^{t+1}:=(M_{t}((v_{i}^{t},\{v_{j}^{t}\}_{j N(i)}))\), where AGG is a function of the node embedding and its neighboring embeddings. We will present a message passing algorithm 1 that generalizes the dynamics of 2 to the entire class of Max-CSPs (see example derivations in Appendix A and Appendix B), which provably converges in polynomial iterations for a reformulation of the canonical SDP relaxation of Raghavendra (2008) (see SDP 1). Parametrizing this general message-passing algorithm will lead to the definition of OptGNN (see Definition 3.3).

### Message Passing for Max-CSPs

Given a set of constraints over variables, Max-CSP asks to find a variable assignment that maximizes the number of satisfied constraints. Formally, a Constraint Satisfaction Problem \(=(,,q)\) consists of a set of \(N\) variables \(:=\{x_{i}\}_{i[N]}\) each taking values in an alphabet \([q]\) and a set of predicates \(:=\{P_{z}\}_{z}\) where each predicate is a payoff function over \(k\) variables \(X_{z}=\{x_{i_{1}},x_{i_{2}},...,x_{i_{k}}\}\). Here we refer to \(k\) as the arity of the Max-k-CSP. We adopt the normalization that each predicate \(P_{z}\) returns outputs in \(\). We index each predicate \(P_{z}\) by its domain \(z\). The goal of Max-k-CSP is to maximize the payoff of the predicates.

\[():=_{(x_{1},,x_{N})[q]^{N}} |}_{P_{z}}P_{z}(X_{z}),\] (4)

where we normalize by the number of constraints so that the total payoff is in \(\). Therefore we can unambiguously define an \(\)-approximate assignment as an assignment achieving a payoff of \(-\). Since our result depends on a message-passing algorithm, we will need to define an appropriate graph structure over which messages will be propagated. To that end, we will leverage the constraint graph of the CSP instance: Given a Max-k-CSP instance \(=(,,q)\) a _constraint graph_\(G_{}=(V,E)\) is comprised of vertices \(V=\{v_{,}\}\) for every subset of variables \( z\) for every predicate \(P_{z}\) and every assignment \([q]^{|z|}\) to the variables in \(z\). The edges \(E\) are between any pair of vectors \(v_{,}\) and \(v_{^{},^{}}\) such that the variables in \(\) and \(^{}\) appear in a predicate together. For instance, for a SAT clause \((x_{1} x_{2}) x_{1} x_{3}\) there are four nodes \(v_{1},v_{12},v_{3}\) and \(v_{}\) with a complete graph between \(\{v_{1},v_{12},v_{}\}\) and \(v_{3}\) an isolated node.

Let \(()\) be the optimal value of the SDP 1 on instance \(\). The _approximation ratio_ for the Max-k-CSP problem achieved by the SDP 1 is \(_{}()}{()}\), where the minimization is taken over all instances \(\) with arity \(k\). There is no polynomial time algorithm that can achieve a larger approximation ratio assuming the truth of the UGC Raghavendra (2008). We construct our message passing algorithm as follows. First we introduce the definition of the vector form SDP and its associated quadratically penalized Lagrangian.

**Definition** (Quadratically Penalized Lagrangian).: Any standard form \(\,\) can be expressed as the following vector form SDP for some matrix \(V=[v_{1},v_{2},,v_{N}] R^{N N}\).

\[_{V^{N N}} C,V^{T}V A_{i},V^{T}V=b_{i} i[].\] (5)

For any SDP in vector form we define the \(\) quadratically penalized Lagrangian to be

\[_{}(V):= C,V^{T}V+_{i}(  A_{i},V^{T}V-b_{i})^{2}.\] (6)

We show that gradient descent on this Lagrangian \(_{}(V)\) for the Max-CSP SDP 1 takes the form of a message-passing algorithm on the constraint graph that can provably converge to an approximate global optimum for the SDP (see algorithm 1). We see that gradient descent on \(_{}\) takes the form of a simultaneous message passing update on the constraint graph. See equation 60 and algorithm 3 for analytic form of the Max-CSP message passing update. See appendix A and B for analytic form of Min-Vertex-Cover and Max-3-SAT message passing updates. Our main theorem is then following.

**Theorem 3.1**.: _[Informal] Given a Max-k-CSP instance \(\) represented in space \(=O(||q^{k})\), there is a message passing Algorithm 3 on constraint graph \(G_{}\) with a per iteration update time of \(O()\) that computes in \(O(^{-4}^{4})\) iterations an \(\)-approximate solution (solution satisfies constraints to error \(\) achieving objective value within \(\) of optimum) to SDP 1. For the formal theorem and proof see Theorem C.1._

### OptGNN for Max-CSP

Next we define OptGNN for Max-CSP. See A and B for OptGNN for Vertex Cover and 3-SAT.

**Definition** (OptGNN for Max-CSP).: Let \(\) be a Max-CSP instance on a constraint graph \(G_{}\) with \(N\) nodes. Let \(U\) be an input matrix of dimension \(r N\) for \(N\) nodes with embedding dimension \(r\). Let \(_{}\) be the penalized lagrangian loss defined as in equation 6 associated with the Max-CSP instance \(\). Let \(M\) be the OptGNN weights which are a set of matrices \(M:=\{M_{1},M_{2},...,M_{T}\}^{r 2r}\). Let \(_{M_{t}}:^{r N}^{r N}\) be the function \(_{M_{t}}(U)=M_{i}((U))\), where

\(:^{r N}^{2r N}\) is the aggregation function \((U):=[U,_{}(U)]\). We define \(_{(M,)}:^{r N}\) to be the function

\[_{(M,)}(U)=_{}_{M_{T}} _{M_{1}}(U).\] (7)

The per iteration update time of OptGNN is \(O( r^{})\) where \(\) is the matrix multiplication exponent. We update the parameters of OptGNN by inputting the output of the final layer \(_{M_{T}}\) into the Lagrangian \(_{}\) and backpropagate its derivatives. We emphasize the data is the instance \(\) and not the collection of vectors \(U\) which can be chosen entirely at random. The form of the gradient \(_{}\) is a message passing algorithm over the nodes of the constraint graph \(G_{}\). Therefore, OptGNN is a message passing GNN over \(G_{}\) (see Definition G). This point is of practical importance as it is what informs out implementation of the OptGNN architecture. We then arrive at the following corollary.

**Corollary 1** (Informal).: Given a Max-k-CSP instance \(\) represented in space \(=O(||q^{k})\), there is an OptGNN\({}_{(M,)}\) with \(T=O(^{-4}^{4})\) layers, and embeddings of dimension \(\) such that there is an instantiation of learnable parameters \(M=\{M_{t}\}_{t[T]}\) that outputs a set of vectors \(V\) satisfying the constraints of SDP 1 and approximating its optimum to error \(\). See formal statement 2

Moving on from our result on representing approximation algorithms, we ask whether OptGNN is learnable. That is to say, does OptGNN approximate the value of SDP 1 when given a polynomial amount of data? We provide a perturbation analysis to establish the polynomial sample complexity of PAC-learning OptGNN. The key idea is to bound the smoothness of the polynomial circuit AGG used in the OptGNN layer which is a cubic polynomial analogous to linear attention. We state the informal version below. For the formal version see Lemma E.5.

**Lemma 3.1** (PAC learning).: Let \(\) be a dataset of Max-k-CSP instances over an alphabet of size \([q]\) with each instance represented in space \(\). Here the dataset \(:=_{1},_{2},...,_{}\) is drawn i.i.d from a distribution over instances \(\). Let \(M\) be a set of parameters \(M=\{M_{1},M_{2},...,M_{T}\}\) in a parameter space \(\). Then for \(T=O(^{-4}^{4})\), for \(=O(^{-4}^{6}^{4}(^{-1}))\), let \(\) be the empirical loss minimizing weights for arbitrary choice of initial embeddings \(U\) in a bounded norm ball. Then we have that OptGNN is \((,,)\)-PAC learnable. That is to say the empirical loss minimizer \(()\) is within \(\) from the distributional loss with probability greater than \(1-\):

\[[|()-_{}[ _{(,)}(U)]|] 1-.\]

We note that this style of perturbation analysis is akin to the VC theory on neural networks adapted to our unsupervised setting. Although it's insufficient to explain the empirical success of backprop, we believe our analysis sheds light on how architectures that capture gradient iterations can successfully generalize.

**OptGNN in practice**. Figure 1 summarizes the OptGNN pipeline for solving CO problems. OptGNN computes node embeddings \(V^{N r}\) as per equation 7 which feeds into the loss \(_{}\). For pseudocode, please refer to the appendix (Max-Cut: algorithm 4 and general SDPs: algorithm 5). We use Adam (Kingma & Ba, 2014) to update the parameter matrices \(M\). Given a training distribution \(\), the network is trained in a completely unsupervised fashion by minimizing \(_{G}[(;G)]\) with a standard automatic differentiation package like PyTorch (Paszke et al., 2019). A practical benefit of our approach is that users do not need to reimplement the network to handle each new problem. Users need only implement the appropriate loss, and our implementation uses automatic differentiation to compute the messages in the forward pass. At inference time, the output embeddings must be rounded to a discrete solution. To do this, we select a random hyperplane vector \(y^{r}\), and for each node with embedding vector \(v_{i}\), we calculate its discrete assignment \(x_{i}\{-1,1\}\) as \(x_{i}=(v_{i}^{}y)\). We use multiple hyperplanes and pick the best resulting solution.

heuristics, solvers, and neural baselines. We then describe a model ablation study, an out-of-distribution performance (OOD) study, and an experiment with our neural certification scheme. We empirically test the performance of OptGNN on NP-Hard combinatorial optimization problems: _Maximum Cut_, _Minimum Vertex Cover_, and _Maximum 3-SAT_. We obtain results for several datasets and compare against greedy algorithms, local search, a state-of-the-art MIP solver (Gurobi), and various neural baselines. For Max-3-SAT and Min-Vertex-Cover we adopt the quadratically penalized Lagrangian loss of their respective SDP relaxations. For details of the setup see Appendix D.

**Min-Vertex-Cover experiments.** We evaluated OptGNN on forced RB instances, which are hard vertex cover instances from the RB random CSP model that contain hidden optimal solutions (Xu et al., 2007). We use two distributions specified in prior work (Wang and Li, 2023), RB200 and RB500. The results are in Figure 2b, which also includes the performance of several neural and classical baselines as reported in (Wang and Li, 2023; Brusca et al., 2023). OptGNN consistently outperforms state-of-the-art unsupervised baselines on this task and is able to match the performance of Gurobi with a 0.5s time limit.

**Max-3-SAT experiment and ablation.** The purpose of this experiment is twofold: to demonstrate the viability of OptGNN on the Max-3-SAT problem and to examine the role of overparameterization in OptGNN. We generate 3-SAT formulae on the fly with 100 variables and a random number of clauses in the \(\) interval, and train OptGNN for 100,000 iterations. We then test on instances with 100 variables and \(\{400,415,430\}\) clauses. The results are Table 2. We compare with WalkSAT, a classic local search algorithm for SAT, a low-rank SDP solver (Wang et al., 2019), Survey Propagation (Braunstein et al., 2005), and ErdosGNN Karalias & Loukas (2020), a neural baseline trained in the same way. We also compare with a simple baseline reported as "Autograd" that directly employs gradient descent on the penalized Lagrangian using the autograd functionality of Pytorch. For details see D.2. OptGNN is able to outperform ErdosGNN consistently and improves significantly over Autograd, which supports the overparameterized message passing of OptGNN. OptGNN performs better than the low-rank SDP solver, though does not beat WalkSAT/Survey Prop. It is worth noting that the performance of OptGNN could likely be further improved without significant computational cost by applying a local search post-processing step to its solutions but we did not pursue this further in order to emphasize the simplicity of our approach.

**Max-Cut experiments.** Table 1 presents a comparison between OptGNN, a greedy algorithm, and Gurobi for Max-Cut. OptGNN clearly outperforms greedy on all datasets and is competitive with Gurobi when Gurobi is restricted to a similar runtime. For results on more datasets see subsection D.3. Following the experimental setup of ANYCSP Tonshoff et al. (2022), we also tested OptGNN on the GSET benchmark instances (Benlic & Hao, 2013). We trained an OptGNN for 20k iterations on generated Erdos-Renyi graphs \(_{n,p}\) for \(n\) and \(p=0.15\). Figure 1(a) shows the results. We have included an additional low-rank SDP baseline to the results, while the rest of the baselines are reproduced as they appear in the original ANYCSP paper. These include state-of-the-art neural baselines, the Goemans-Williamson algorithm, and a greedy heuristic. We can see that OptGNN outperforms the SDP algorithms and the greedy algorithm, while also being competitive with the

   Dataset & OptGNN & Greedy & Gurobi & Gurobi \\  & & & 0.1s & 1.0s \\  BA\({}^{}\) (400,500) & 2197.99 (66) & 1255.22 & 2208.11 & 2208.11 \\ ER\({}^{}\) (400,500) & 16387.46 (225) & 8622.34 & 16476.72 & 16491.60 \\ HK\({}^{}\) (400,500) & 2159.90 (61) & 1230.98 & 2169.46 & 2169.46 \\ WC\({}^{}\) (400,500) & 1166.47 (78) & 690.19 & 1173.45 & 1175.97 \\  ENZYMES\({}^{}\) & 81.37 (14) & 48.53 & 81.45 & 81.45 \\ COLLAB\({}^{}\) & 2622.41 (22) & 1345.70 & 2624.32 & 2624.57 \\  REDDIT-M-12K\({}^{}\) & 568.00 (89) & 358.40 & 567.71 & 568.91 \\ REDDIT-M-5K\({}^{}\) & 786.09 (133) & 495.02 & 785.44 & 787.48 \\   

Table 1: Performance of OptGNN, Greedy, and Gurobi 0.1s, 1s, and 8s on Maximum Cut. For each approach and dataset, we report the average cut size measured on the test slice. Here, higher score is better. In parentheses, we include the average runtime in _milliseconds_ for OptGNN.

   Dataset & \(r=\) 4.00 & \(r=\) 4.15 & \(r=\) 4.30 \\  ErdősGNN & 5.46\({}^{ 1.91}\) (0.01) & 6.14\({}^{ 2.01}\) (0.01) & 6.79\({}^{ 2.03}\) (0.01) \\  Walksat (100 restarts) & 0.14\({}^{ 0.36}\) (0.12) & 0.36\({}^{ 0.52}\) (0.25) & 0.68\({}^{ 0.65}\) (0.40) \\ Walksat (1 restart) & 0.94\({}^{ 0.92}\) (0.01) & 1.46\({}^{ 1.11}\) (0.01) & 1.97\({}^{ 1.28}\) (0.01) \\ Survey Propagation & 3.32\({}^{ 0.81}\) (0.001) & 3.87\({}^{ 0.79}\) (0.001) & 3.94 \({}^{ 0.93}\) (0.001) \\  OptGNN & 4.46\({}^{ 1.68}\) (0.01) & 5.15\({}^{ 1.76}\) (0.01) & 5.84\({}^{ 2.18}\) (0.01) \\  Autograd SDP & 6.85\({}^{ 2.33}\) (6.80) & 7.52\({}^{ 2.38}\) (6.75) & 8.32\({}^{ 2.50}\) (6.77) \\ Low-Rank SDP ((Wang & Kolter, 2019)) & 12.38\({}^{ 1.06}\) (0.66) & 13.32\({}^{ 1.09}\) (0.67) & 14.27\({}^{ 1.08}\) (0.69) \\   

Table 2: Average number of unsatisfied clauses for Max-3-SAT on random instances with \(N=100\) variables and clause ratios \(r=4.00,4.15,4.30\). Standard deviation of the ratio over the test set is reported in superscript. In parentheses, we report the average time per instance on the test set in seconds.

neural baselines. However, OptGNN does not manage to outperform ANYCSP, which to the best of our knowledge achieves the current state-of-the-art results for neural networks.

**Out of distribution generalization.** We test OptGNN's ability to perform on data distributions (for the same optimization problem) that it's not trained on. The results can be seen in table 7 and subsection D.6. The results show that the model is capable of performing well even on datasets it has not been trained on.

Figure 2: Results for Max-Cut and Minimum Vertex Cover.

**Model ablation.** We train modern GNN architectures from the literature with the same loss function and compare them against OptGNN. Please see Appendix D.4 for more details and results on multiple datasets for two different problems. Overall, OptGNN is the best performing model on both problems across all datasets.

**Experimental demonstration of neural certificates.** Next, we provide a simple experimental example of our neural certificate scheme on small synthetic instances. Deploying this scheme on Max-Cut on random graphs, we find this dual certificate to be remarkably tight. figure 3 shows an example. For \(100\) node graphs with \(1000\) edges our certificates deviate from the SDP certificate by about \(20\) nodes but are dramatically faster to produce. The runtime is dominated by the feedforward of OptGNN which is \(0.02\) seconds vs. the SDP solve time which is \(0.5\) seconds on cvxpy. See C.1 for extensive discussion and additional results.

## 5 Conclusion

We presented OptGNN, a GNN that can capture provably optimal message passing algorithms for a large class of combinatorial optimization problems. OptGNN achieves the appealing combination of obtaining approximation guarantees while also being able to adapt to the data to achieve improved results. Empirically, we observed that the OptGNN architecture achieves strong performance on a wide range of datasets and on multiple problems. OptGNN opens up several directions for future exploration, such as the design of powerful rounding procedures that can secure approximation guarantees, the construction of neural certificates that improve upon the ones we described in Appendix C.1, and the design of neural SDP-based branch and bound solvers.

## 6 Acknowledgment

The authors would like to thank Ankur Moitra, Sirui Li, and Zhongxia Yan for insightful discussions in the preparation of this work. Nikolaos Karalias is funded by the SNSF, in the context of the project "General neural solvers for combinatorial optimization and algorithmic reasoning" (SNSF grant number: P500PT_217999). Stefanie Jegelka and Nikolaos Karalias acknowledge support from NSF AI Institute TILOS (NSF CCF-2112665). Stefanie Jegelka acknowledges support from NSF award 2134108.

Figure 3: Experimental comparison of SDP versus OptGNN Dual Certificates on random graphs of 100 nodes for the Max-Cut problem. Our OptGNN certificates track closely with the SDP certificates while taking considerably less time to generate.