# Stepping on the Edge:

Curvature Aware Learning Rate Tuners

 Vincent Roulet

Google DeepMind

vroulet@google.com

&Atish Agarwala

Google DeepMind

thetish@google.com

&Jean-Bastien Grill

Google DeepMind

jbgrill@google.com

&Grzegorz Swirszcz

Google DeepMind

swirszcz@google.com

&Mathieu Blondel

Google DeepMind

mblondel@google.com

&Fabian Pedregosa

Google DeepMind

pedregosa@google.com

Equal contribution.

###### Abstract

Curvature information - particularly, the largest eigenvalue of the loss Hessian, known as the sharpness - often forms the basis for learning rate tuners. However, recent work has shown that the curvature information undergoes complex dynamics during training, going from a phase of increasing sharpness to eventual stabilization. We analyze the closed-loop feedback effect between learning rate tuning and curvature. We find that classical learning rate tuners may yield greater one-step loss reduction, yet they ultimately underperform in the long term when compared to constant learning rates in the full batch regime. These models break the stabilization of the sharpness, which we explain using a simplified model of the joint dynamics of the learning rate and the curvature. To further investigate these effects, we introduce a new learning rate tuning method, Curvature Dynamics Aware Tuning (CDAT), which prioritizes long term curvature stabilization over instantaneous progress on the objective. In the full batch regime, CDAT shows behavior akin to prefixed warm-up schedules on deep learning objectives, outperforming tuned constant learning rates. In the mini batch regime, we observe that stochasticity introduces confounding effects that explain the previous success of some learning rate tuners at appropriate batch sizes. Our findings highlight the critical role of understanding the joint dynamics of the learning rate and curvature, beyond greedy minimization, to diagnose failures and design effective adaptive learning rate tuners.

## 1 Introduction

The learning rate, a.k.a. stepsize, is the main hyperparameter controlling the efficiency and stability of gradient-based training of deep neural networks. The learning rate is typically adjusted through a predetermined schedule - often consisting of a warm-up phase, where the learning rate is gradually increased to a peak, followed by an annealing phase, where it is decreased to zero (Goyal et al., 2017; Loshchilov and Hutter, 2016). Tuning the shape of the schedule (warm-up time, peak learning rate, decay scale and shape) is essential for good performance. Despite recent efforts to understand their effectiveness, the optimal shape of these schedules remains an area of active research (Liu et al., 2020; Shi et al., 2023). The cost of tuning these schedules has led to interest in automatic selection of these hyperparameters with _learning rate tuners_ - methods which aim to automatically adjust the learning rate through training.

These methods have roots in traditional optimization theory, including inexact linesearch with Armijo-Goldstein criterion (Armijo, 1966; Nocedal and Wright, 1999) and Polyak stepsizes (Polyak, 1964), which select the learning rate via estimates of the gap to optimality of the objective. The Armijo-Goldstein criterion is a crucial component of popular full-batch convex optimizers, such as L-BFGS (Liu and Nocedal, 1989). Recent efforts have adapted linesearches to stochastic optimization, with some partial empirical successes and with some approaches offering convergence guarantees (Galli et al., 2023; Mutschler and Zell, 2020; Vaswani et al., 2019). Similar efforts have been made for Polyak stepsizes (Berrada et al., 2020; Loizou et al., 2021), in addition to new methods which combine distance to optimality with online learning convergence bounds (Cutkosky et al., 2023; Defazio and Mishchenko, 2023; Ivyi et al., 2023; Mishchenko and Defazio, 2023).

Classically-inspired methods, however, have generally struggled to gain traction in deep learning. This is partly due to their design, which prioritizes convex, Lipschitz-continuous, and/or smooth (Lipschitz-continuous gradients) objectives. In contrast, the loss landscape of deep networks is known to be non-convex (Li et al., 2018), and non-Lipschitz continuous (Hochreiter et al., 2001). Moreover, non-linear models, especially neural networks, will commonly undergo dramatic changes in geometry during training (Arora et al., 2022; Jastrzebski et al., 2019; Jastrzebski et al., 2020; Kalra et al., 2023; Kopitkov and Indelman, 2020; Lewkowycz et al., 2020; Wu et al., 2018). In particular, most models undergo a phase of _progressive sharpening_ - where the sharpness, the largest eigenvalue of the Hessian, increases during training (Cohen et al., 2021). These potentially detrimental effects are mitigated by non-linear stabilization arising from the discreteness of the dynamics - namely, the _edge of stability_ (EOS) phenomenon (Cohen et al., 2021). This causes large Hessian eigenvalues to stabilize at the critical value for a given learning rate in an equivalent smooth setting (for example, max Hessian eigenvalue stabilizes at \(_{}=2/\) for learning rate \(\)) (Cohen et al., 2023, 2021). The early training time behavior corresponds to the regime where there is the most feature learning (Cohen et al., 2023, 2021), and is the main focus of this work; at late times, the large eigenvalues of the Hessian usually drop below the edge of stability. Gilmer et al. (2022) considered EOS stabilization as a leading candidate for the necessity of the warm-up procedure; as the learning rate \(\) increases, \(_{}\) is effectively annealed.

This raises some natural questions. _How do these sharpness dynamics affect the performance of learning rate tuners? What insights can we gain to design better tuners for deep learning?_ Our work takes a first step at answering these questions, starting with a study of some classical learning rate tuners: a linesearch ensuring sufficient decrease and an approximately greedy method that minimizes a quadratic approximation of the objective. Specifically, we find the following.

* We empirically observe that classical learning rate tuners qualitatively underperform their constant learning rate counterparts across several deep learning benchmarks, in the full batch regime, for which these methods were originally designed.
* Our empirical analysis of curvature dynamics reveals that classical learning rate tuners generally undershoot the edge of stability. This undershooting creates a snowball effect of ever-increasing sharpness and ever-decreasing learning rates.
* We propose a theoretical model that effectively captures these empirically observed failures.

Our analysis suggests that stabilizing the sharpness may be a more important goal for the long-term success of training, compared to greedily optimizing the objective. To explore this idea, we propose the Curvature Dynamics Aware Tuning (CDAT) method, which dynamically drives the learning rate to the EOS. In our exploration, we find the following.

* We observe empirically that the proposed learning rate tuner can outperform fine-tuned constant learning rate counterparts in a full batch regime.
* We analyze the sharpness dynamics induced by CDAT in these examples and observe that the progressive sharpening is mitigated by the tuner, increasing learning rates at early times before stabilizing, akin to an automatic warm-up schedule.
* We propose a theoretical model that clarifies the dynamical mechanisms by which CDAT maintains proximity to the EOS, while highlighting the limitations of existing models of curvature dynamics.

Our work suggests that the design of learning rate tuners benefits from exploiting curvature stabilization rather than focusing on loss decrease. The introduction of simple learning rate tuners can also refine our understanding of sharpness dynamics through feedback loop effects. Additional experiments and experimental details are presented in Appendix B and Appendix C respectively.

## 2 The Interplay Between Learning Rate Tuners and Curvature Dynamics

A leitmotif in the design of learning rate tuners has been to select the learning rate to ensure a maximal or sufficient decrease of the objective at each iteration. We focus here on two canonical examples. Polyak stepsizes and hyper-gradient descent are also briefly examined in Appendix B, Fig. 13.

### Canonical learning rate tuners failures in deep learning

The first classical approach we consider is a **linesearch** (ls) method that selects the learning rate \(\) such that the objective \(f\) satisfies a certain decrease criterion (Armijo, 1966; Nocedal and Wright, 1999). Formally, given current parameters \(w_{t}\) and an update direction \(u_{t}\), the learning rate \(_{t}^{}\) is chosen such that

\[f(w_{t}+_{t}^{}u_{t}) f(w_{t})+c\,_{t}^{}u_{t}^{ } f(w_{t})\,.\] (1)

This rule assumes that \(u_{t}\) is a descent direction (\( f(w_{t})^{}u_{t}<0\)), which ensures the existence of a learning rate satisfying (1). This holds true for simple Gradient Descent (GD) or preconditioned variants like RMSProp (Hinton et al., 2012). In the criterion (1), \(c\) is usually a small constant set to \(10^{-4}\) or \(0\). A valid learning rate is searched with a usual backtracking linesearch (Appendix C).

The second method we consider involves selecting the learning rate at each iteration to minimize a quadratic approximation of the objective. Formally, the objective \(f\) at parameters \(w_{t}\) can be approximated along an update direction \(u_{t}\) by a quadratic approximation \(q_{f}\) as

\[f(w_{t}+ u_{t}) q_{f}(;w_{t},u_{t}) f(w_{t})+  f(w_{t})^{}u_{t}+^{2}u_{t}^{}^{2}f(w_{t}) u_{t}.\] (2)

Provided that this quadratic approximation is strongly convex in \(\) (\(u_{t}^{}^{2}f(w_{t})u_{t}>0\)), the minimum of the quadratic approximation \(q_{f}(;w_{t},u_{t})\) is reached for the **quadratically greedy** (qg) learning rate \(^{}\) given by

\[_{t}^{}=)^{}u_{t}}{u_{t}^{}^{2 }f(w_{t})u_{t}}\,.\] (3)

Setting the learning rate by minimizing the quadratic approximations (3) is a simple intuitive idea studied for example by Schaul et al. (2013), Martens and Grosse (2015, Section 6.4). This approach as well as linesearches are effective on simple linear problems (Fig. 2). While their rationale originates in non-stochastic optimization, they have been analyzed in the context of stochastic optimization for deep learning (Schaul et al., 2013; Vaswani et al., 2019).

Figure 1: **Simple learning rates tuners qualitatively underperform their constant learning rate counterparts.** Gradient descent or RMSProp with a tuned constant learning rate versus self-tuned gradient descent by a linesearch method (1), or a quadratically greedy rule (3) on various datasets, architectures and losses in a full batch regime. The linesearch may perform better at early times but stalls in the long term.

Figure 2: Classical learning rate tuners can be effective on linear models.

### Analyzing learning rate tuners through curvature dynamics

Full batch regime.We revisit the performance of the learning tuners presented in Section 2.1 in the full batch regime on deep learning problems in Fig. 1. As demonstrated in Fig. 1, a linesearch (1) or the quadratically greedy rule (3) qualitatively underperform their constant learning rate counterpart in the deep learning benchmarks considered. Notably, all these results are obtained despite being in a full batch regime, for which these methods are originally designed. To understand the failures of these approaches, we consider several measures presented in Fig. 3 (see also Fig. 12).

First, we observe a consistent decrease in the chosen learning rate over time, spanning several orders of magnitude (\(1^{}\) panel of Fig. 3). This is surprising, as none of these approaches explicitly encode a decreasing learning rate mechanism. Specifically, the linesearch always initiates its search with a guess larger than the previously selected learning rate (see Appendix C for implementation details). Decreasing learning rates are theoretically optimal for non-smooth objectives (Nesterov et al., 2018), such as the ones induced by using the ReLU activation; however in our example, the gradient norm does not increase beyond one order of magnitude (\(4^{}\) panel of Fig. 3). This suggests both that an increase in gradient norm is not the primary cause of learning rate decrease, and also explains why the learning rate decrease is correlated with slower progress on the training loss.

Following the work of Cohen et al. (2021), we analyze the dynamics of the sharpness, that is the largest eigenvalue of the Hessian, \(_{}(^{2}f(w_{t}))\). In the \(2^{}\) panel of Fig. 3, we observe that while sharpness stabilizes for gradient descent, it does not exhibit the same behavior for the considered learning rate tuners. By plotting the product of the learning rate \(_{t}\) and the sharpness (\(3^{}\) panel of Fig. 3), we find that this product can exceed the stability threshold of \(2\), eventually stabilizing below this threshold for constant learning rate gradient descent. In contrast, for the learning rate tuners, this product neither surpasses the stability threshold nor stabilizes around \(2\) in the long run. Therefore, these classical learning rate tuners do not operate at the edge of stability.

From a theoretical perspective, objectives are typically classified as either smooth or non-smooth. Smooth objectives have gradients that are Lipschitz-continuous, at least locally around any point. Non-smooth objectives, on the other hand, may contain points with kinks (non-differentiable points). However, this taxonomy might not fully capture the curvature dynamics observed by Cohen et al. (2023, 2021) for constant learning rates, and in Fig. 1 for the classical learning rate tuners. In particular, the concept of smoothness might not be entirely relevant in the context of deep learning, where its local estimate (the spectral norm of the Hessian, also known as sharpness) can continue to increase throughout training. To push the limits of classical smoothness assumptions, we consider in Section 3 a learning rate tuner that propels the optimizer at the edge of stability or above, a regime that usual smoothness assumptions would theoretically prohibit.

Mini-batch regime.The results presented in Fig. 1 in the full batch regime _do not contradict_ the success of linesearches at medium batch size observed by Vaswani et al. (2019) in the stochastic regime. This observation is illustrated in Fig. 14, and was previously reported by Roulet et al. (2023). We simply point out that the success of linesearches observed by Vaswani et al. (2019) may not be entirely attributable to the method's original rationale.

Figure 3: **Classical learning rate tuners can undershoot the edge of stability. Learning rate, sharpness, their product, and the gradient norm evolution of a constant learning rate and learning rate tuners, full batch gradient descent. Learning rate decreases by \(3\) orders of magnitude for tuners (\(1^{}\) panel) while sharpness increases (\(2^{}\) panel). Their product remains relatively steady, just below the edge of stability (\(3^{}\) panel). The gradient norm increases by less than a factor of \(10\), consistent with slow training at late times (\(4^{}\) panel).**The actual success of linesearches in a stochastic regime may instead be explained by the attenuated progressive sharpening observed in such a regime (Agarwala and Pennington, 2024; Cohen et al., 2021; Jastrzebski et al., 2017). Moreover, linesearches applied to mini-batches tend to select larger learning rates than they would in a full-batch regime (Mutschler and Zell, 2020) potentially allowing them to avoid undershooting the full objective's edge of stability.

### Theoretical analysis

The sharpening effects can be understood theoretically. Previous work has shown that the stabilization provided by EOS is due to non-linear interaction between the component of the gradient in the largest eigendirection, and the dynamics of the largest eigenvalues themselves (Agarwala et al., 2023; Damian et al., 2023). We can use these analyses to understand why there is no stabilization for some classical learning rate tuners.

We start with the model from Damian et al. (2023), which focuses on the dynamics in the largest eigendirection of the Hessian. We consider a unique eigenvector for simplicity; we don't observe degeneracy in the eigenspace of the largest eigenvalue in any practical models. Given an objective \(f\) parameterized by parameters \(w_{t}\), let \(_{t}\) be the largest eigenvalue of the Hessian \(^{2}f(w_{t})\), i.e., \(_{t}(w_{t})_{}(^{2}f(w_{t}))\). Let \(v\) be its normalized eigenvector; the model assumes slow eigenvector change, so it is treated as a fixed direction. The joint dynamics of \(_{t}\) and the projection \(x_{t} v^{}w_{t}\) can then be written as

\[x_{t+1}=(1-_{t}_{t})x_{t},\ _{t+1}=_{t}(a-bx_{t}^{2})+ _{t}\,.\] (4)

Here, \(a-(w)^{} f(w)\) corresponds to the instantaneous change of \(\) along the negative gradient (the update direction), and \(b\|(w)\|^{2}\) encodes the non-linear negative feedback between \(x_{t}\) and \(_{t}\). Both \(a\) and \(b\) are considered constant along iterations. These equations are derived by Damian et al. (2023) using a Taylor expansion of the iterates combined with a coupling argument. We provide intuition for the model in Appendix A.1.

In the original model, the learning rate \(_{t}\) is also fixed to \(\). This leads to the following dynamics: while \(_{t}<2\), the magnitude of \(x_{t}\) decreases. This, in turn, leads to an increase in \(_{t}\). Eventually, \(_{t}>2\) and \(|x_{t}|\) increases. This eventually leads to the \(bx_{t}^{2}\) term becoming large, which decreases \(_{t}\). There is a range of learning rates over which this dynamic leads to quasi-stable oscillations of \(_{t}\) around the edge of stability value \(2/\) (Fig. 4, blue curves).

When using a learning rate tuner, \(_{t}\) is also a dynamical variable. This introduces the additional complication of a shifting edge of stability. Therefore, it is advantageous to analyze the dynamical system using normalized variables (Agarwala et al., 2023). We define \(y_{t}_{t}_{t}-2\), where \(y=0\) corresponds to the EOS, and \(p_{t} x_{t}^{2}\). This gives us the dynamical equations (Appendix A.2)

\[p_{t+1}=(1+y_{t})^{2}p_{t},\ y_{t+1}=_{t+1}[_{t}(a-bp_{t} )]+(}{_{t}})y_{t}+2[}{_{t}}-1].\] (5)

We must then supply a rule for \(_{t+1}\). In Fig. 3, we observed that in the full batch setting, the learning rate multiplied by the sharpness appears to quickly approach a threshold of \(2-\) (corresponding to \(y=-\)), and then varies slowly below the EOS threshold.

Figure 4: **The poor performance of classical learning rate tuners, understood in a simplified model.** The dynamics of learning rate \(\), sharpness \(_{}\), and normalized centered sharpness \(y=_{}-2\) are examined in the simplified model (5). With a constant \(\), \(_{}\) stabilizes and \(y\) oscillates around \(0\) (blue). Classical learning rate tuners often quickly equilibrate around \(y_{t}=-\), which we model using \(=1.9_{}\) (orange). This equilibration of \(y\) away from zero prevents stabilization in \(_{}\), leading to an increase in \(_{}\), and a corresponding decrease in \(\).

We model the varying learning rate as

\[_{t} 2(1-)/_{t}\,.\] (6)

This maintains \(y_{t}=-\). Notably, this schedule was explicitly proposed by Cohen et al. (2021) (see also Fig. 15). In this regime, \(p_{t}\) decreases monotonically, aligning with the original goal of these methods to decrease the loss (Fig. 10). However, this eliminates feedback for controlling the increase in \(_{t}\), resulting in significant progressive sharpening (Fig. 4, orange curve).

Consequently, when attempting to enforce monotonicity, learning rate tuners may inadvertently disrupt the non-linear stabilization that makes gradient descent robust and effective for training deep neural networks. Continually undershooting the EOS triggers a snowball effect of decreasing learning rate and increasing sharpness. If there is no corresponding increase in gradient norms, this causes optimization to slow down.

The poor performance of the classical learning rate tuners in Fig. 1 therefore appear strongly correlated with their tendency to _undershoot_ the edge of stability in the normalized sharpness coordinate \(y\). In the following, we focus on understanding tuners that prioritize training at or near the edge of stability.

## 3 Optimizing on the Edge of Stability

Based on our observations in Section 2, we design learning rate tuners that position the underlying optimizer **on the edge** of stability (\(y=0\)). We analyze a tuner capable of operating both slightly below and slightly above the EOS in order to exploit nonlinear stabilization.

Formally, we investigate a generalization of the quadratically greedy rule from Section 2, which sought \(_{t}\) to minimize the quadratic approximation \(q_{f}\) in (2). We instead choose the learning rate to be _on edge_ by seeking the largest value of \(\) such that \(q_{f}\) is smaller or equal to the original value of \(f\),

\[_{t}^{}\{ 0:q_{f}(;w_{t},u_{t}) f(w_{t })\}=-2)^{}u_{t}}{u_{t}^{}^{2}f(w_{t})u_{t} }\,,\] (7)

where the last formula holds provided that \(u_{t}^{}^{2}f(w_{t})u_{t}>0\) (convex quadratic) and \( f(w_{t})^{}u_{t}<0\) (\(u_{t}\) is a descent direction).

Figure 5: **Enforcing optimizers to stay on edge (\(=2.0\)) improves performance over greedy approximation (\(=1.0\)). Train loss and learning rate behaviors for fine-tuned optimizers vs self-tuned counterparts with CDAT on various datasets, architectures, losses in a full batch regime. Tuning the learning rate “on edge” (\( 2\)) improves performance over greedy tuning (\(=1\)) as well as constant learning rate.**

For \(u_{t}=- f(w_{t})\), and if \(- f(w_{t})\) is aligned with the eigendirection \(v_{}\) associated with the largest eigenvalue \(_{}\) of \(H\), we recover the familiar \(_{t}^{}=2/_{}\). Note however, that contrarily to using directly \(_{t}=2/_{}\), the on-edge rule can naturally take into account the alignment with \(v_{}\) (see Fig. 15). We note that we recover the edge of stability even when the updates are given by the gradient multiplied by a preconditioner, e.g. \(u_{t}=-P^{-1} f(w_{t})\) for a matrix \(P\). In this case, we have \(-u^{}Hu/u^{}g=g^{}P^{-1}HP^{-1}g/g^{}P^{-1}g\) for \(H=^{2}f(w_{t})\), \(g= f(w_{t})\). This is maximized when \(P^{-1/2}g\) lies in the largest eigendirection of the PSD matrix \( P^{-1/2}HP^{-1/2}\), which for \(=2\) gives us the learning rate \(_{t}=2/_{}\), where \(_{}\) is the maximum eigenvalue of \(\). This is exactly the edge of stability for adaptive methods (Cohen et al., 2023).

We note that the only difference between this and the quadratically greedy rule is a factor of \(2\) in the numerator. Inspired by this observation, and with an eye towards robustness, we define our _Curvature Dynamics Aware Tuning_ (CDAT) rule by:

\[_{t}^{}=}{d_{t}},n_{t}=\{-  f(w_{t})^{}u_{t},0\},\;d_{t}=|u_{t}^{}^{2}f(w_{t})u_{t}| +.\] (8)

The scaling factor \(\) lets us interpolate between greedy (\(=1\)) and on-edge (\(=2\)). We are most interested in the behavior near \(=2\), (also studied in Rosca et al. (2023)). In (8), the \(\) function takes care of the case where \(u_{t}\) is an ascent direction (\( f(w_{t})^{}u_{t}>0\)), the absolute value takes care of cases where the objective has negative curvature in the update directions (see Appendix C for additional justification), and we simply set \(=0\) as we always observed non-negligible positive curvature. The definitions of the numerator \(n_{t}\) and the denominator \(d_{t}\) allow for the possibility of exponential moving averages (EMA) of each quantity such as \(_{t+1}=(1-_{})n_{t}+_{}_{t}\) for \(_{}\) referred to as the CDAT EMA parameter thereafter. We observed that smoothing the estimates of \(n_{t}\) and \(d_{t}\) by an EMA is particularly relevant when the updates are themselves defined through an exponential moving average as in Adam, or when using the proposed rule in a stochastic setting.

CDAT has two major advantages: it is sensitive to information from all eigenvalues of \(^{2}f(w_{t})\), and it depends on updates \(u_{t}\) coming from any base optimizer. We will take advantage of these properties to explore the behavior of "on edge" optimization in a variety of settings.

### On edge optimizers in practice

Full batch regime.Fig. 5 presents results for training with CDAT across various optimizers, architectures, datasets, and losses. Overall, selecting the learning rate to be on edge (\(=2\)) is on par with or better than a fine-tuned constant learning rate and is always better than a quadratically greedy approach (\(=1\)). This observation holds even though the quadratically greedy rule ensures larger instantaneous decrease (Fig. 16). One notes that targeting slightly above the edge (\(=2.0625\)) provides even better performance than the on edge rule (\(=2\)) on all examples except the MLP Mixer on CIFAR10. However, targeting higher above the edge (\(=2.5\)) generally gives diverging results in the short or long terms. To integrate the proposed rule with the Adam optimizer, we also observed that the estimation of the curvatures through \(n_{t}\), \(d_{t}\) in (8) was necessary.

Figure 6: **Optimizing on edge induces different curvature dynamics.** Sharpness, product between learning rate and sharpness, and gradient norm evolutions for gradient descent with CDAT. By putting the learning rate on edge (\( 2\)), the sharpness does not ever increase and actually decreases slightly over time. GD with CDAT operates slightly above the edge constantly during training. Its gradient norm evolution is akin to a fine-tuned constant learning rate baseline.

Remarkably, all choices around the edge (\(1.9375,2.0,20625\)) show a progressive increase of the learning rate that results generally in a better performance than the constant learning rate counterparts, except for RMSProp on the NanoLM experiment. The increasing learning rate behavior is akin to the warm-up phase generally hard-coded by a scheduler. In Fig. 19, we observe that the CDAT rule displays similar behavior as warm-up schedules, yet it may not fully capture the benefits of prefixed schedules.

In Fig. 6, we analyze the dynamics of the curvature when optimizing on edge. We observe that the sharpness can be pushed to reduce over the iterations (\(1^{}\) panel of Fig. 6). The CDAT rule may operate constantly slightly above the edge (\(2^{}\) panel of Fig. 6). By reducing the sharpness, the algorithm may be able to take larger stepsizes and converge faster. Sensitivity to architecure's width and depth, as well as weight decay, are also analyzed in Fig. 18.

Mini batch regime.The CDAT rule can be used in a stochastic regime by replacing \(f\) in (8) by its stochastic counterpart \(f^{(m_{t})}\) on a mini-batch \(m_{t}\). However, two difficulties may arise.

First, the on edge rule is motivated by the sharpening effects of the overall objective, which can be overestimated or underestimated by a single mini-batch. Previous work shows that the trace of the Hessian may best capture the sharpening and stabilization effects in a stochastic regime (Agarwala and Pennington, 2024; Wu and Su, 2023); it is unclear what function of the Hessian spectrum, the CDAT rule captures in the stochastic regime. As a result the optimal scaling factor may vary with the mini-batch. In Fig. 7, we observe that the optimal scaling of the on-edge rule is proportional to the batch size up to some size. In particular, at specific batch sizes, we observe that the greedy rule (\(=1\)) outperforms the on-edge rule. This result is consistent with the good performance of linesearches or greedy rules in a mini-batch regime previously mentioned and observed in Fig. 14. We also observe in Fig. 7, that integrating an EMA into the estimation of the edge in (8) smooth out the selection of the optimal scaling factor.

Finally, the sharpening effects are known to be generally mitigated in the stochastic regime (Agarwala and Pennington, 2024; Cohen et al., 2021; Jastrzebski et al., 2017). The benefits of the on edge rule appear also subdued in this regime (Fig. 8, Fig. 20, Fig. 21).

### Modeling CDAT dynamics

The classical optimization framework is insufficient to fully explain the benefits of CDAT. For example, on a convex quadratic objective, \(=1\) is the optimal choice, and \(>2\) results (in the worst case) in a divergent algorithm. However, we can use a simplified model to begin understanding the joint dynamics of the learning rate and sharpness under CDAT.

We approximate the gradients around a stationary point \(w_{}\), where \( f(w_{})=0\), as \( f(w_{t}) H_{t}\) for \(_{t} w_{t}-w_{}\), and \(H\) being a symmetric matrix. In this scenario, the learning rate given by CDAT is \(_{t}^{}=(_{t}^{}\,H^{2}_{t})/(_{ t}^{}\,H^{3}_{t})\). Consider the case where \(H\) has two eigenvalues \(\) and \(\), with \(> 0\). In this case the CDAT learning rate can be written as

\[_{t}^{}=p_{t}+^{2}g_{t}}{^{3}p_ {t}+^{3}g_{t}}=p_{t}/g_{t}+^{2}}{^{3}p_{t }/g_{t}+^{3}}\,.\] (9)

Figure 7: **Stochasticity shifts the optimal scaling.** Normalized performance of gradient descent with momentum equipped with CDAT in a stochastic regime with varying batch sizes. In a mini-batch regime, the optimal scale decreases as the batch size decreases. Using an exponential moving average smooths out the performance of the CDAT rule over batch sizes.

Here \(p_{t},g_{t}\) are the projections \(p_{t}:=(_{t}^{}v)^{2},g_{t}:=(_{t}^{}v_{})^{2}\), respectively onto the eigendirections \(v\) and \(v_{}\) associated with \(\), \(\). Therefore, \(_{t}^{}\) interpolates between its minimum value \(/\) to the larger value \(/\), depending on the alignment ratio \(p_{t}/g_{t}\). For \(2/<<2\), this rule can achieve learning rates both above and below the EOS.

We can gain additional insight by modeling a dynamical \(_{t}\), extending the model of Section 2.3. While model (5) captures the dynamics in the largest eigendirection \(v\), here we aim to model the dynamics in the orthogonal subspace. To simplify, we consider the eigendirections \(v,v_{}\), and small eigenvalue \(\) fixed. We then model the gradients as \( f(w_{t}) H_{t}_{t}\) with \(H_{t}=_{t}vv^{}+ v_{}v_{}^{}\). If we update \(w\) in the direction \(v_{}\) using gradient descent on \( g_{t}\), we obtain the following dynamical system describing the CDAT learning rate tuner:

\[_{t+1}=^{2}p_{t}+^{2}g_{t}}{_{t}^{3}p_{t} +^{3}g_{t}}, g_{t+1}=(1-_{t}_{t})^{2}g_{t}, p_{t+1}=(1+y_{ t})^{2}p_{t}\,.\] (10)

Combining this with the update rule for \(y_{t}\) given in (5) completes the model.

There are two important regimes of behavior in this model. First, if \(y_{t}>0\), \(p_{t}\) will increase and eventually \(y_{t}\) will decrease as in the normal EOS case. If \(y_{t}<0\), the key threshold is \(y_{t}<-_{t}_{t}\). In this case, the ratio \(p_{t}/g_{t}\)_decreases_ - leading to an increase in \(_{t}\) according to the on edge rule. If \(a-bp_{t}>0\) (as it is if \(p_{t}\) has become small due to \(y_{t}<0\)), then we see from (5) that this leads to an _increase_ in \(y_{t}\). This suggests that CDAT has a tendency to push \(y_{t}\) closer to the EOS - sending \(y\) towards \(0\) if the learning rate is driven by the eigendirections corresponding to smaller eigenvalues.

Numerical simulations on this model (Fig. 9) suggest that this effect can indeed cause remarkably small values of \(y\) (\(3^{}\) panel of Fig. 9). We emphasize that this is due to the _joint dynamics_ of \(_{t}\) (induced by the learning rate tuner), and \(_{t}\), \(p_{t}\), and \(g_{t}\) (induced by GD). There are also important limitations in this model's ability to fully explain CDAT's behavior. For example, the model predicts runaway sharpening for \(<2\) (\(2^{}\) panel of Fig. 9), and divergence for \(>2\). In practice, we saw a range of stable and useful settings for scale centered around \(2\). This modeling limitation likely stems from neglecting the dynamics orthogonal to \(v\) as well as higher-order terms, which empirically tend to stabilize EOS dynamics (Agarwala et al., 2023).

Figure 8: **The performance of CDAT is subdued in the stochastic regime.** Fine-tuned constant, scheduled, and self-tuned with CDAT learning rates in a stochastic regime. In a stochastic regime, CDAT can also exhibit a form of learning rate warm-up (top figure). However, the interplay between sharpening and learning rate are known to be mitigated in a stochastic regime which may explain the underperformance of CDAT in this regime (bottom figure).

## 4 Conclusion and Future Directions

Summary.Our empirical results showed that simple linesearches and approximate greedy learning rate tuners underperform constant learning rate approaches in the full batch regime - despite being better on individual steps. The idea that "locally greedy" methods perform poorly on long time scales has been shown in other settings as well, including evolutionary dynamics Agarwal and Fisher (2019). Our experiments and theoretical work suggest the failure of these classical tuners is due to the fact that they suppress the feedback which stabilizes sharpness in the fixed learning rate setting. As the sharpness increases, tuners are forced to take smaller steps, which ends up leading to slower learning.

We find, in contrast, that prioritizing stability of the sharpness yields tangible benefits. Our CDAT method pushes the network towards the edge of stability via a dynamically driven process. It also naturally displays some form of progressive increase of the learning rate akin to prefixed warm-up schedules. CDAT also sheds light on the more complicated dynamics in small mini batch regime, where estimation of a locally greedy rule may actually place the optimizer on the edge of stability of the full batch objective.

Limitations and future directions.We explored some limitations of the current modeling framework in Section 2.3 - in particular, the failure to capture stabilization due to higher order terms. Developing improved models (either analytically or numerically) would allow for powerful tools from other disciplines to aid algorithm design - particularly, methods from control theory. For example, state feedback schemes can be designed through the analysis of nonlinear dynamical systems to ensure asymptotic stabilization (Isidori, 1995, Chapter 7). We believe a cross disciplinary approach will be useful for designing the next generation of learning rate tuners.

The proposed CDAT rule may also help to understand and refine the design of learning rate schedules through scaling ladders (Wortsman et al., 2024). Recent work has shown that transfer of learning rates over different scales is related to consistency of curvature dynamics (Noci et al., 2024); this suggests that approaches like ours may be useful to increase predictability of optimal learning rates across scale.

Acknowledgements.We thank James Martens and Mihaela Rosca for fruitful discussions on related ideas. We also thank the reviewers for their insightful comments that helped us refine the manuscript.

Figure 9: **A simple model partially captures the benefits induced by the proposed CDAT rule.** Dynamics of theoretical model of CDAT (10). For \(=2\), feedback stabilizes \(y\) close to the EOS (\(y=0\)), which stabilizes \(_{}\) (orange). For \(=2-\) and small \(\) (blue, \(=0.1\)), model predicts that \(_{}\) slowly grows (middle), but predicts that \(y\) stabilizes to a value \(- y_{t}<0\) (right).