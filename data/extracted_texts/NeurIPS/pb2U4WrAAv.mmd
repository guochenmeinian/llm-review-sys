# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

building block for constructing discriminative models that are fast, gradient-free, and Bayesian.

## 1 Introduction

Modern machine learning methods attempt to learn functions of complex data (e.g., images, audio, text) to predict information associated with that data, such as discrete labels (Bernardo et al., 2007). Deep neural networks (DNNs) have demonstrated success in this domain, owing to their universal function approximation properties (Park and Sandberg, 1991) and scalable optimization algorithms for training them (Amari, 1993). Despite their performance and scalability, DNNs do not provide well calibrated predictions and uncertainty estimates (Wang et al., 2021; Shao et al., 2020). This limits the applicability and reliability of using DNNs in safety-critical applications like autonomous driving, medicine, and disaster response (Papamarkou et al., 2024).

Here we introduce a gradient-free variational learning algorithm for a probabilistic variant of a two-layer, feedforward neural network -- the conditional mixture network or CMN -- and measure its performance on supervised learning benchmarks. This method rests on coordinate ascent variational inference (CAVI) (Wainwright et al., 2008; Hoffman et al., 2013) and hence we name it CAVI-CMN. CAVI-CMN maintains the predictive accuracy and scalability of an architecture-matched feedforward neural network fit with maximum likelihood estimation, while maintaining full distributions over its parameters and generating calibrated predictions, as measured in relationship to state-of-the-art Bayesian methods like the No U-Turn Sampler (NUTS) algorithm for Hamiltonian Monte Carlo (Hoffman et al., 2014) and black-box variational inference (Ranganath et al., 2014).

We summarize the contributions of this work below:

* Introduce and derive a coordinate ascent variational inference scheme for the conditional mixture network, which we term CAVI-CMN.
* CAVI-CMN matches, and sometimes exceeds, the performance of maximum likelihood estimation (MLE) in terms of predictive accuracy, while maintaining probabilistic benefits like high log predictive density and low calibration error. This is shown across a suite of 8 different supervised classification tasks.
* CAVI-CMN requires drastically less time to converge and overall runtime than the other state-of-the-art Bayesian methods like NUTS and BBVI.

## 2 Methods

Here, we introduce a variant of the Mixture-of-Experts (MoE) model (Jacobs et al., 1991) that makes its parameters amenable to gradient-free Bayesian learning. Jacobs et al. (1991) originally introduced MoEs as a way to improve the performance of neural networks by combining the strengths of multiple specialized models (Gormley and Fruhwirth-Schnatter, 2019). Non-Bayesian approaches to MoE typically rely on maximum likelihood estimation (MLE) (Jacobs et al., 1991), which can suffer from overfitting and poor generalization due to the lack of regularization mechanisms (Bishop and Svenskn, 2003).

The approach we propose, CAVI-CMN takes advantage of the conditional conjugacy of a mixture of linear experts, along with Polya-Gamma (PG) augmentation (Polson et al., 2013) for the softmax layers, to make all parameters amenable to variational Bayesian inference. We use coordinate ascent variational inference (CAVI) to obtain posteriors over the weights of both the individual experts and the gating network (Bishop and Nasrabadi, 2006; Blei et al., 2017), without resorting to costly gradient or sampling computations.

### The conditional mixture network

The conditional mixture network maps from a continuous input vector \(_{0}^{d}\) to its label \(y\{1,,L\}\). This is achieved with two layers: a conditional mixture of linear experts, which outputs a joint continuous-discrete latent \(_{1}^{h},z_{1}\{1,,K\}\) and a multinomial logistic regression or softmax layer, which maps from the continuous latent \(_{1}\) to the corresponding label \(y\). Given a dataset of input-label pairs \((_{0},Y)=\{_{0}^{n},y^{n}\}_{n=1}^{N}\), the CMN defines a joint distribution over labels \(Y\), latents \(_{1},Z_{1}\), and parameters \(\):\[p(,_{1},_{1},|_{0}) =p()_{n=1}^{N}p_{_{1}}(y^ {n}|_{1})p_{_{1}}(_{1}^{n}|_{0}^{n},z _{1}^{n})p_{_{0}}(z_{1}^{n}|_{0}^{n})\] (1) \[p() =p(_{1})p(_{0})p( _{1})\]

where \(p_{_{1}}(_{0}^{n}|_{0}^{n},z_{1}^{n})\) refers to a mixture of linear models with parameters \(_{1}=_{1:K},_{1:K}^{-1}\), while \(p_{_{0}}(z_{1}^{n}|_{0}^{n})\)is a multinomial logistic (softmax) layer that outputs a probability over discrete latent \(z_{1}^{n}\), which selects which of \(K\) experts to use in predicting \(_{1}^{n}\). \(p_{_{1}}(y^{n}|_{1})\) parameterizes a final (softmax) likelihood over the label \(y^{n}\). A Bayesian network representation of the two layer CMN architecture is shown in Figure 1.

### Coordinate ascent variational inference with conjugate priors

In this section we summarize a variational approach for inverting the probabilistic model described in Equation (1). We posit the following approximate posterior over latents and parameters:

\[p(_{1},_{1},|Y,) q()_{n=1}^{N}q(z_{1}^{n})q(_{1}^{n}|z_{1 }^{n}) q()=q(_{1} )q(_{0})q(_{1})\] (2)

where \(q(_{1}^{n},z_{1}^{n})\) corresponds to an approximate posterior over continuous \(_{1}\) and discrete \(z_{1}\) latent variables.

The mean-field factorized form of the approximate posterior (Svensen, 2003), combined with conjugate priors over the parameters \(\) (see Appendix A for their form), allows us to derive conditionally-conjugate updates for \(q(_{1},Z_{1})\) and \(q()\). We use an iterative update scheme for the parameters of the approximate posterior, often referred to as variational Bayesian expectation maximisation (VBEM) (Beal, 2003) or coordinate ascent variational inference (CAVI) (Bishop and Nasrabadi, 2006). This consists in alternating updates to the posterior over latents and the posterior over parameters, split into a variational E-step and a variational M-step. Each step maximizes the evidence lower bound (ELBO), conditioned on the current setting of the other factor (i.e., \(q(_{1},Z_{1})\) or \(q()\)).

Figure 1: A Bayesian network representation of the two-layer conditional mixture network, with an input-output pair \(_{0}^{n},y^{n}\) and latent variables \(_{1}^{n},z_{1}^{n}\). Observations are shaded nodes, while latents and parameters are transparent.

Update to latents ('E-step')

\[q_{t}(_{1}^{n},z_{1}^{n}) \{_{q_{t-1}()}[ p_{}(y^{n},_{1}^{n},z_{1}^{n}|_{0}^{n})]\}\] (3) Update to parameters ('M-step')

\[q_{t}() \{_{q_{t-1}(_{1}^{n},z_{1}^{n})} [ p_{}(y^{n},_{1}^{n},z_{1}^{n}|_{0}^{n})]\}\]

The functional forms of these equations and the PG augmentation scheme needed to turn them into conditionally-conjugate updates, are given in detail in Appendix A and Appendix B.

## 3 Results

We fit CAVI-CMN on several real and synthetic datasets and compared it to three alternative inference methods for fitting the parameters of the CMN:

**MLE**: -- We obtained point estimates for the parameters \(\) of the CMN using maximum-likelihood estimation (backpropagation to minimize the negative log likelihood).
**NUTS-HMC**: -- The No-U-Turn Sampler (NUTS), an extension to Hamiltonian Monte Carlo (HMC) that incorporates adaptive step sizes (Hoffman et al., 2014). This provides samples from a posterior distribution over \(\).
**BBVI**: -- Black-Box Variational Inference (BBVI) method (Ranganath et al., 2014). BBVI maximizes the evidence lower bound (ELBO) using stochastic estimation of its gradients with respect to variational parameters.

Appendix C contains details of the hyperparameters used for each inference algorithm.

### Predictive performance and efficiency

We fit all the inference algorithms on the Pinwheels dataset (Johnson et al., 2016) and 7 datasets from the UCI Machine Learning repository (Kelly et al., 2024). The upper row of Figure 2 visualizes three

Figure 2: Performance and runtime results of the different inference algorithms on the ‘Pinwheel’ dataset from Johnson et al. (2016). The standard deviation (vertical lines) of the performance metric is depicted together with the mean estimate (circles) over different model initializations. The top row of subplots show test accuracy (top left); log predictive density (top center), and expected calibration error (top right) as a function of training set size. The bottom row shows runtime metrics as a function of increasing training set size: the number of iterations required to achieve convergence (lower left); and the total runtime (in seconds, lower right). See Appendix G for details on run-time metrics.

different performance metrics for the Pinwheels dataset as a function of the size of the training set. The CAVI-based approach achieves competitive test accuracy to MLE, as well as comparable log predictive density (LPD) and expected calibration error (ECE) to the other two Bayesian methods; all three Bayesian approaches outperform maximum likelihood estimation in LPD and ECE. This finding holds for 6 of the 7 datasets we tested (see Appendix E), and also holds across training set sizes, indicating robust sample efficiency and calibration. To further study the probabilistic performance of CAVI-CMN, we computed the widely applicable information criterion (WAIC), an approximate estimate of leave-one-out cross-validation (Vehtari et al., 2017; Watanabe and Opper, 2010). Table 1 shows the WAIC scores for all methods evaluated on 7 UCI datasets. The CAVI-CMN approach consistently provided higher WAIC scores compared to the MLE algorithm, and WAIC scores that were on par with BBVI and NUTS.

The bottom row of Figure 2 shows that across training set sizes, all three gradient-based algorithms 4 exhibit an increase in runtime as the number of training data increases (which also scales the number of parameters for BBVI and CAVI). However the rate of increase varies significantly across different algorithms, with CAVI-CMN approach showing the best scaling behavior, both in terms of steps-to-convergence and absolute runtime. CAVI-CMN's runtime also scales competitively with MLE and BBVI along two other dimensions of model complexity: input dimension \(d\) and number of expert learners \(K\) (see Appendix F).

Thus, CAVI-CMN retains the probabilistic benefits of state-of-the-art Bayesian methods, as measured by metrics like test accuracy, LPD, and ECE, while also offering substantial advantages in terms of computational efficiency.

## 4 Conclusion

We introduced CAVI-CMN, a computationally efficient Bayesian approach for conditional mixture networks (CMN) that outperforms maximum likelihood estimation (MLE) in terms of predictive performance and calibration, as measured by LPD and ECE, and is competitive in terms of test accuracy on held out data. CAVI-CMN offers significant computational advantages over other Bayesian methods like Black-Box Variational Inference (BBVI) and the No-U-Turn Sampler (NUTS). While NUTS excels in inference quality, its computational cost is prohibitive for complex models. BBVI, though efficient, converges slower and has overall slower run-time than CAVI when applied to the CMN model.

The benchmark results demonstrate that CAVI-CMN matches the performance of BBVI and NUTS in terms of predictive accuracy, log-predictive density, and expected calibration error, while being considerably faster. The conjugate-exponential form of CAVI-CMN also makes it amenable to online learning with mini-batches of data, suggesting the extension of CAVI-CMN to deeper architectures and larger datasets. Overall, CAVI-CMN presents a promising tool for building fast, gradient-free and scalable Bayesian machine learning models.