# The Challenges of the Nonlinear Regime for

Physics-Informed Neural Networks

 Andrea Bonfanti

BMW AG, Digital Campus Munich

Basque Center for Applied Mathematics

University of the Basque Country

abonfanti001@ikasle.ehu.eus

Giuseppe Bruno

BMW AG, Digital Campus Munich

Giuseppe.GB.Bruno@bmw.de

Cristina Cipriani

Technical University of Munich

Munich Center for Machine Learning

Munich Data Science Institute

cristina.cipriani@ma.tum.de

###### Abstract

The Neural Tangent Kernel (NTK) viewpoint is widely employed to analyze the training dynamics of overparameterized Physics-Informed Neural Networks (PINNs). However, unlike the case of linear Partial Differential Equations (PDEs), we show how the NTK perspective falls short in the nonlinear scenario. Specifically, we establish that the NTK yields a random matrix at initialization that is not constant during training, contrary to conventional belief. Another significant difference from the linear regime is that, even in the idealistic infinite-width limit, the Hessian does not vanish and hence it cannot be disregarded during training. This motivates the adoption of second-order optimization methods. We explore the convergence guarantees of such methods in both linear and nonlinear cases, addressing challenges such as spectral bias and slow convergence. Every theoretical result is supported by numerical examples with both linear and nonlinear PDEs, and we highlight the benefits of second-order methods in benchmark test cases.

## 1 Introduction

PINNs have became ubiquitous in the scientific research community as a meshless and practical alternative tool for solving PDEs. The first attempts to exploit machine learning models for PDE solutions can be traced back to two articles from the 90s , while the model acquired its name and popularity through a later publication . Due to the flexible structure of the architecture, PINNs can be used for forward and inverse problems  and efficiently exploited for more complex engineering practice such as constrained shape and topology optimization, and surrogate modeling . However, the usability of PINNs for such applications is often hindered by their slow training and occasional failure to converge to acceptable solutions. Due to the black-box nature of PINNs, it is challenging to analyze their training dynamics and convergence properties mathematically . Nonetheless, rapid training and reliable convergence are crucial aspects of any PDE solver intended for engineering applications.

Related works.In this context, the NTK  viewpoint has yielded intriguing insights, particularly in the realm of linear PDEs . Although based on the assumption of overparameterized networks, this perspective has proven valuable in highlighting various intrinsic pathologies in PINN training,such as spectral bias [39; 2; 29], the complexity of the loss landscape generated by the PDE residuals  and the nuanced interplay among components of the loss function . The salient characteristics of the NTK in the infinite-width limit are the fact that is deterministic at initialization, constant during training, and it linearizes the training dynamics due to the sparsity of the Hessian of PDE residuals [22; 23].

Our contributions.In this paper, we delineate the profound theoretical distinctions between the application of PINNs to linear versus nonlinear PDEs, elucidating the differences in their NTK behavior. We show that, even under the idealistic assumption of the infinite-width limit, the NTK framework fails in the nonlinear domain. Our novel contribution lies in demonstrating that the NTK is stochastic at initialization, it is dynamic during training, and is accompanied by a non-vanishing Hessian. Given the evolution of the Hessian throughout training, we emphasize the need of employing second-order methods for nonlinear PDEs. Furthermore, we analyze their convergence guarantees, revealing that even in linear scenarios, the utilization of second-order methods proves advantageous in mitigating the issue of spectral bias. As a second-order method, we employ Levenberg-Marquardt algorithm, a stabilized version of the well-known Gauss-Newton algorithm, which approximates the Hessian to make it computationally feasible even for large networks. It is important to note that our goal is not to propose a novel training algorithm but to demonstrate the benefits of using _any_ second-order method. The reason is twofold: in the nonlinear regime, we achieve faster and better convergence, while in the linear regime, where fast convergence can be achieved by first-order methods, the advantage of second-order methods lies in their ability to alleviate spectral bias.

Our work is organized as follows: Section 2 introduces PINNs, and Section 3 covers the NTK theory, comparing its dynamics in linear and nonlinear PDEs. Section 4 examines the convergence guarantees of second-order optimization methods. Finally, Section 5 presents numerical experiments that validate our theoretical insights.

## 2 Physics-Informed Neural Networks

We address the following PDE formulated on a bounded domain \(^{d_{}}\),

\[u(x) =f(x), x,\] (1) \[u(x) =g(x), x.\]

Here, the PDE is defined with respect to the differential operator \(\), while the boundary and initial conditions are collected in the function \(g\). Notice that \(\) can be either a spatial or spatio-temporal domain, depending on whether the PDE is time-dependent or not. PINNs aim to approximate the PDE solution \(u:^{d_{}}\) with a neural network \(u_{}\) parametrized by \(\), which is a vector containing all the parameters of the network. The "Physics-Informed" nature of the neural network \(u_{}\) lies in the choice of the loss function employed for training

\[()=_{}|u_{}(x)-f(x)|^{2 }dx+_{}|u_{}(x)-g(x)|^{2}d(x),\]

where \(\) denotes a measure on the surface \(\). In this work, we specifically focus on scenarios where the PDE involves a nonlinear differential operator. Moreover, without loss of generality we consider the case where \(f(x)=0\). Since the function \(f(x)\) does not depend on the parametrization, all of our results hold also for the case when it is nonzero. Moreover, we express (1) as

\[R([u](x))=0, x,\] (2) \[u(x)=g(x), x,\]

where \([u]:^{d_{}}^{k d_{}}\), defined as

\[[u](x)=[u(x),\,_{x}u(x),\,_{x}^{2}u(x),,\,_ {x}^{k}u(x)],\] (3)

denotes a vector encompassing all (possibly mixed) derivatives of \(u\) until order \(k\), while \(R:^{k d_{}}\) represents a differentiable function of the components of \([u]\).

**Remark 2.1**.: _The importance of the function \(R\) lies in its ability to completely encode the nonlinearity of the PDE, while the term \(\) remains linear. Furthermore, for numerous well-known nonlinear PDEs (such as Burgers' or Navier-Stokes equations), the function \(R\) exhibits a distinctive structure as it takes the form of a second-order polynomial._To illustrate this, we consider the example of the inviscid Burgers' equation, which for \((,x)\) is expressed as \(_{}u+u\,_{x}u=0\), where \(\) represents time and \(x\) the space variable. It follows that

\[[u](,x) =[u(,x),\,_{}u(,x),\,_{x}u(,x)],\] \[R(z_{1},z_{2},z_{3}) =z_{2}+z_{1}z_{3}.\]

## 3 Neural Tangent Kernel for PINNs

We now introduce and develop the NTK for PINNs, inspired by the definition in . We employ a fully-connected neural network featuring a single hidden layer, as follows

\[u_{}(x):=}W^{1}(W^{0}x+b^{0})+b^{1},\] (4)

for any \(x^{d_{u}}\). Here, \(W^{0}^{m d_{u}}\) and \(b^{0}^{m}\) denote the weights matrix and bias vector of the hidden layer, while \(W^{1}^{d_{an} m}\) and \(b^{1}^{d_{an}}\) are the corresponding parameters of the outer layer. Additionally, \(:\) is a smooth coordinate-wise activation function, such as the hyperbolic tangent, which is a common choice for PINNs. Furthermore, we adopt the NTK rescaling \(}\) to adhere to the methodology introduced in the original work . This is crucial for achieving a consistent asymptotic behavior of neural networks as the width of the hidden layer approaches infinity. In the following, for brevity, we denote with \(\) the collection of all the trainable parameters of the network, i.e. \(W^{1},W^{0},b^{1},b^{0}\).

**Remark 3.1**.: _For the sake of brevity, we focus on the case of neural networks with a single hidden layer. However, the outcomes derived in this scenario may be extended to deep networks. We leave this extension to future works and refer to  for results on finite networks with multiple hidden layers._

We consider the discrete loss on the collocation points \(x_{i}^{r}\) and the boundary points \(x_{i}^{b}\),

\[L()=}_{i=1}^{N_{r}}|r_{}(x_{i}^{r})|^{2}+}_{i=1}^{N_{b}}|u_{}(x_{i}^{b})-g(x_{i}^{b})|^{2},\] (5)

where \(r_{}(x_{i}^{r})=R([u_{}](x_{i}^{r}))\) indicates the residual term. Furthermore, \(N_{r}\) and \(N_{b}\) denote the batch size of, respectively, the collection of \(^{r}=\{x_{i}^{r}\}_{i=1}^{N_{r}}\) and \(^{b}=\{x_{i}^{b}\}_{i=1}^{N_{b}}\), which are the discrete data used for training. We now consider the minimization of (5) as the gradient flow

\[_{t}(t)=- L((t)).\] (6)

Using the following notation

\[u_{}(^{b})=\{u_{(t)}(x_{i}^{b})\}_{i=1}^{N_{b }}, 28.452756ptr_{}(^{r})=\{r_{(t)}(x_{i}^{r} )\}_{i=1}^{N_{r}},\] (7)

we can characterize how these quantities evolve during the gradient flow, through the NTK perspective.

**Lemma 3.2**.: _Given the data (7) and the gradient flow (6), then \(u_{}\) and \(r_{}\) satisfy the following_

\[_{t}u_{(t)}(^{b})\\ _{t}r_{(t)}(^{r})=-K(t)u_{ (t)}(^{b})-g(^{b})\\ r_{(t)}(^{r}),\] (8)

_where \(K(t)=J(t)J(t)^{T}\) and_

\[J(t)=_{}u_{(t)}(^{b})\\ _{}r_{(t)}(^{r}).\] (9)

Proof.: The proof is presented in . 

We provide more details about the construction of \(J(t)\) in Appendix A. The matrix \(K\) is also referred to as Gram matrix. The analysis of Gram matrices and their behavior in the infinite-width limit  yields results akin to the NTK analysis. It is important to note that Lemma 3.2 is applicable to any type of sufficiently regular differential operator.

### The difference between linear and nonlinear PDEs

In the work , PINNs have been thoroughly investigated using the NTK, but only in the case of linear PDEs. Additionally,  extensively explores the similar case of standard neural networks with linear output. In particular, they show that in the infinite-width limit, the NTK is deterministic under proper random initialization and stays constant during training. Thereby, the dynamics in (8) is equivalent to kernel regression and has an analytical solution expressed in terms of the kernel. As noted in , the constancy of the NTK during training is equivalent to the linearity of the model. This characteristic is related to the vanishing of the (norm of the) Hessian of the network's output in the infinite-width limit. These well-known results are reported in Appendix B. In , the same convergence results for Gram matrices hold for nonlinear PDEs when using networks as in (4) with a scaling of \(}\), where \(s>\). However, this scaling is inconsistent with the NTK model, so we focus on the unexplored case where \(s=\). The novel contribution of our paper lies in demonstrating that in this regime this phenomenon does not hold true when dealing with nonlinear PDEs, which we prove in this section. The network architecture and its associated assumptions are relatively standard, so we refer to Assumption B.2 in Appendix B. However, it is essential to delineate the specific assumptions related to the nonlinear PDE.

**Assumption 3.3** (on \(\)).: _The differential operator \(\) is nonlinear, hence the function \(R\) is nonlinear. Moreover, the gradient \( R\) is continuous._

The first distinction with linear PDEs arises in the convergence as \(m\) of the NTK at initialization.

**Theorem 3.4**.: _Consider a fully-connected neural network given by (4) satisfying Assumption B.2. Moreover, the PDE satisfies Assumption 3.3. Then, under a Gaussian random initialization \((0)\), it holds_

\[K(0)}{}m,\]

_where the limit is in distribution and \(\) is not deterministic, but its law can be explicitly characterized._

Proof.: A detailed proof is in Appendix C. However, the basic idea is to reformulate the kernel as

\[K(0)=_{R}(0)\,K_{}(0)\,_{R}(0)^{T},\]

where the matrix \(K_{}(0)\) enclose the linear components of \(\), hence the derivatives of the network's output, while the matrix \(_{R}(0)\) depends on the gradient of \(R\) (so its contribution is relevant just in the nonlinear case). We can establish the convergence in probability of \(K_{}(0)\) to a deterministic matrix by taking advantage of the linearity of the operator \(\) and commuting \(\) and \(_{}\) (see Lemma C.2). The matrix \(_{R}(0)\) only converges in distribution, since it is a function of the network output and its derivatives, whose limits are Gaussian Processes at initialization by Proposition C.1. 

Next, we focus on the NTK behavior during training.

**Proposition 3.5**.: _Under Assumption B.2 on the network, and Assumption 3.3 on the PDE, assume additionally that \(R\) is a real analytic function. Let \(u_{*}\) be a solution of the corresponding PDE and suppose that for every \(m\) there exists \(t_{m}\) such that_

\[\|u_{}(t_{m})-u_{*}\|_{^{k}}_{m},_{m} 0\ m.\] (10)

_Finally, let \((t)\) be obtained through gradient flow as defined in (6) and denote by \(K(t)\) the corresponding NTK. For \((0)(0,I_{m})\), the following holds:_

\[_{m}_{t[0,T]}\|K(t)-K(0)\|>0\]

Proof.: The proof can be found in Appendix D. 

**Remark 3.6**.: _It is worth noticing that our result holds under the assumption that a neural network with \(m\) can adequately approximate the solution \(u^{}\) of the PDE (1), and that the training process is successful in achieving this approximation. The first assumption is justified by results such as the universal approximation theorem for neural networks . Despite this optimistic training scenario, as demonstrated in Proposition 3.5, the constancy of the kernel is unattainable._In the context of nonlinear PDEs, converging to a linear regime is unattainable, even in the infinite-width limit, and this inability stems from the spectral norm of \(H_{r}\), which is the Hessian of the residuals \(r_{}\) with respect to the parameters \(\). Indeed, in the linear scenario, the convergence of \(\|H_{r}\|\) to \(0\) as \(m\) is crucial for demonstrating convergence to the linear regime, as established in Proposition B.3. Similar conclusions have been drawn in  for various deep learning architectures. However, we now show that this property does not hold for nonlinear PDEs.

**Proposition 3.7**.: _Under Assumptions B.2 and 3.3 on the network and on the PDE, let us further assume that \(R\) is a second-order polynomial. Then, the Hessian of the residuals \(H_{r}\) is not sparse and_

\[_{m}\|H_{r}\|,\]

_where the constant \(\) does not depend on \(m\)._

Proof.: The proof can be found in Appendix E, together with an explicit formula for \(\). 

**Remark 3.8**.: _For the latter result, we additionally require that \(R\) is a second-order polynomial, which includes many classic nonlinear PDEs like Burgers' or Navier-Stokes equations._

We summarize all our results and provide a comparison with the linear case in Table 1. Motivated by the fact that the Hessian is not negligible, we shift our attention to second-order optimization methods and explore their convergence capabilities.

## 4 Convergence results

Before delving into second-order methods, let us revisit a convergence result for first-order ones. Traditional analyses of the gradient descent (6) often rely on the smoothness and convexity of the loss, assumptions that may not hold in the context of deep learning. As an alternative, numerous results concentrate on the infinite-width limit, particularly in connection with the NTK analysis. While we refrain from presenting a formal proof, we highlight the notable result below.

**Theorem 4.1**.: _Under Assumption B.1 on the PDE and Assumption B.2 on the network defined by (4), consider the scenario where \(m\) is sufficiently large. With high probability on the random initialization, there exists a constant \(>0\), depending on the eigenvalues of \(K\), such that gradient descent, employing a sufficiently small step size \(\), converges to a global minimizer of (5) with an exponential convergence rate, i.e._

\[L((t))(1-)^{t}L((0)).\]

Proof.: See , and others. 

It is noteworthy that this result is presented at the level of gradient descent, i.e. the discretization of the gradient flow (6), which explains the constant \(\) representing its step size. Theorem 4.1 has also been extended to various types of architectures in . We emphasize that this convergence result is rooted in the applicability of the Polyak-Lojasiewicz condition which, in turn, is linked to the smallest eigenvalue of the tangent kernel (denoted with \(_{}\)). In the case of linear PDEs, the tangent kernel \(K(t)\) is positive definite  for any \(t[0,T]\), leading to positive eigenvalues. The key finding in this context is that if \(m\) is sufficiently large, \(K(t)\), where \(\) is a deterministic matrix, which only

   & **Linear PDEs** & **Nonlinear PDEs** \\  NTK at initialization & Deterministic & Random _(Theorem 3.4)_ \\  NTK during training & Constant & Dynamic _(Proposition 3.5)_ \\  Hessian \(H_{r}\) & Sparse & Not sparse _(Proposition 3.7)_ \\  First-order convergence bound & \(_{}(K)\) & \( 0\) or \(_{}(K(t))\) \\  Second-order convergence bound & \( 1\) & \( 0\) or \(1\)_(Theorem 4.2)_ \\  

Table 1: Comparison of the theoretical results for linear and nonlinear PDEs.

depends on the training input and not on the network's parameters \(\). As a result, in the infinite-width regime, the dynamics (8) can be approximated by

\[_{t}u_{}(^{b})\\ _{t}r_{}(^{r})- u_{}(^{b})-g(^{b})\\ r_{}(^{r}).\] (11)

In the linear case, the key steps (i.e. the fact that the NTK is deterministic and constant) of the convergence proof of Theorem 4.1 cannot be adapted to nonlinear PDEs. Indeed, the stochasticity of the matrix and its dynamic behavior during training make the reasoning of  or  inapplicable, and it is challenging to show that the eigenvalues of \(K(t)\) in the nonlinear case are uniformly bounded away from zero over training time. Nevertheless, we believe this question warrants further investigation.

Another issue linked to the NTK's eigenvalues is the phenomenon recognized as _spectral bias_ by [39; 2; 29]. This is related to the fast decay of the NTK's eigenvalues, which characterize the rate at which the training error diminishes. The presence of small or unbalanced eigenvalues leads to slow convergence, particularly for high-frequency components of the PDE solution, or even to training failure. This occurs regardless of the linearity of the PDE differential operator \(R\). In the next section, we show that under certain assumptions, second-order methods can help mitigate both problems.

### Second-Order Optimization Methods

Due to all the aforementioned reasons and Proposition 3.7, our focus turns to the investigation of second-order optimization methods. These are powerful algorithms that leverage both the gradient and the Hessian of the loss function. Within this category, Quasi-Newton methods stand out as the most natural and widely known, relying on the Newton update rule

\[(t+1)=(t)-[^{2}L((t))]^{-1} L( (t)).\] (12)

However, the application of this update step relies on second-order derivatives, which are prohibitively expensive to compute as the number of parameters in the model increases. Indeed, the core idea behind Quasi-Newton methods involves utilizing an approximation of the Hessian as follows

\[^{2}L()=J^{T}(t)J(t)+H_{r}r_{(t)} J^{T}(t)J(t)\] (13)

in the formula (12). Here, \(J(t)^{n p}\) represents the Jacobian of the loss at the training time \(t\), and it aligns with the definition in (9). Since the Jacobian \(J(t)\) is part of the evaluation of the gradient, the approximation (13) does not necessitate the computation of higher-order derivatives.

We now tackle the issues of spectral bias and slow convergence by presenting a result applicable to the Gauss-Newton method. In practice, when the number of parameters \(p\) is larger than the number of samples \(n\), the matrix \(J^{T}(t)J(t)\) is surely singular. In this case, we consider the generalized inverse \((J^{T}(t)J(t))^{}\), instead of the inverse.

**Theorem 4.2**.: _Consider the parameter \((t)\) obtained by the Gauss-Newton flow below_

\[_{t}(t)=-(J^{T}(t)J(t))^{} L((t)).\] (14)

_Then, the following holds_

\[_{t}u_{(t)}(^{b})\\ _{t}r_{(t)}(^{r})=-U(t)D(t)U(t)^{T} u_{(t)}(^{b})\\ r_{(t)}(^{r}),\] (15)

_where \(U(t)^{n n}\) is a unitary matrix and \(D^{n n}\) is a diagonal matrix with entries \(0\) or \(1\). In particular, if \(J(t)\) is full-rank for any \(t[0,T]\), then convergence to a global minimum is attained._

Proof.: The proof is presented in Appendix F. 

This result is significant as it indicates that when utilizing second-order methods via (14), convergence no longer depends on the eigenvalues of \(K(t)\) as in (11), but rather on the elements of the diagonal matrix \(D(t)\). Consequently, the training process becomes nearly spectrally unbiased, as the nonzero eigenvalues of the controlling matrix in (15) are all \(1\)s. Let us now compare the cases of linear and nonlinear PDEs, in relation to the assumption of full-rankness of \(J(t)\) and, consequently, the NTK.

**Linear PDEs:** recent research  has theoretically confirmed that the NTK has full-rank in this case. Hence, convergence of second-order methods is achieved with _all_ eigenvalues equal to \(1\), offering a notable advantage over (11) since the training method is unaffected by the spectral bias.
* **Nonlinear PDEs:** showing theoretically the full-rankness is a complicated task, particularly in light of Theorem 3.5, which highlights the stochastic and dynamic nature of the NTK. Similarly, verifying numerically the full-rankness of \(J(t)\) is impractical due to the matrix's ill-conditioning, as mentioned in . However, even if \(J(t)\) is not full-rank, it holds that, although some singular values are zero, fast convergence for the remaining ones is attained.

Moreover, let us stress that the result in Theorem 4.2 applies to any network, including those with finite width. Thus, while the NTK model motivates the use of second-order methods, the key insights about spectral bias and convergence hold without assuming infinite width.

**Remark 4.3**.: _In practice, the Gauss-Newton method becomes less computationally expensive when combined with inexact techniques such as Krylov subspace methods, conjugate gradient, BFGS, or LBFGS . It has been shown that BFGS and LBFGS asymptotically approach the exact Hessian under certain conditions . To extend our findings to more practical inexact methods, we can leverage these asymptotic convergence properties. However, while this approach is theoretically sound, the speed of convergence of quasi-Newton methods to the exact Newton method -- specifically their matrix approximation accuracy -- depends on the minimum eigenvalue of the Hessian [Theorem 6]. As discussed in our paper, the Hessian in PINNs is typically very poorly conditioned. As a result, quasi-Newton methods may require an impractically large number of training steps to converge to the true inverse Hessian and, thus, to begin training higher modes._

## 5 Numerical Experiments

### Empirical validation of our NTK results

First of all, we aim at numerically validate the results presented above, by comparing the NTK in case of linear and nonlinear PDEs. Our experiments are conducted on the following linear equation: \(_{x}^{2}u(x)=}(x)\). Meanwhile, as nonlinear PDE, we consider \(u(x)_{x}u(x)=}(x)\). Notably, these results exhibit consistency across various equations and experimental setups.

The result in Theorem 3.4 is confirmed by the numerical experiments depicted in Figure 1, part (a): in the linear case the NTK at initialization converges to a deterministic matrix when \(m\), while this does not happen in the nonlinear case. The statement of Proposition 3.5 is confirmed in part (b) of Figure 1 by showing that the constancy of the NTK during training is not attainable in the nonlinear case. Moreover, the result in Proposition 3.7 is supported by part (a) of Figure 2, where we compare the sparsity of the Hessian at initialization \(H_{r}(0)\) in both the linear and nonlinear case. Moreover, we observe that in the linear scenario \(\|H_{r}\|\) decays as \(m\) grows, contrarily to the nonlinear example. Similarly, we refer to Figure 2, part (b) for a comparison of the eigenvalues when training with first-order or second-order methods on Burgers' equation.

Figure 1: **(a) Mean and standard deviation of the spectral norm of \(K(0)\) as a function of the number of neurons \(m\) for \(10\) independent experiments. **Left:** linear case. **Right:** nonlinear case. **(b)** Mean and standard deviation of \( K(t):=\) over the network’s width \(m\), for \(10\) independent experiments. **Left:** linear case. **Right:** nonlinear case.

### Employment of second-order methods

Among all second-order methods, in our numerical experiments we make use of an existing variant of the Levenberg-Marquardt (LM) algorithm, as it offers further stability through the update rule

\[(t+1)=(t)-[J^{T}(t)J(t)+_{p}]^{-1}  L((t)),\]

where \(\) is a damping parameter adjusted by the algorithm. In practice, the iterative step of LM can be considered as an average, weighted by \(\), between the Gradient Descent step and a Gauss-Newton method. This aspect of the LM algorithm represents its crucial advantage over other Quasi-Newton methods such as Gauss-Newton or BFGS. Indeed, Quasi-Newton methods show good performance when the initial guess of the solution \(u_{}\) is close to the correct one. The update rule of LM avoids this issue by relying on similar-gradient descent steps at early iteration. Moreover, the parameter \(\) typically decreases during training, in order to converge to a Quasi-Newton method when close to the optimum. Our primary aim is to showcase the effectiveness of second-order methods for nonlinear PINNs, a point which has been supported by findings such as those in : their approach also employs a second-order method, akin to a Gauss-Newton method in function spaces. For details on the modified LM algorithm, along with pseudocode, we refer to Appendix G.

Details on the NetworksThe neural network architectures adopted in the experiments are standard Vanilla PINNs with hyperbolic tangent as activation function. All of the PINNs trained in our analysis are characterized by 5 hidden layers with 20 neurons each. Every training is performed for 10 independent neural networks initialized with Xavier normal distribution . All models are implemented in PyTorch  and trained on a single NVIDIA A10 GPU.

Test CasesWe assess our theoretical findings on the following equations:

* _Wave/Poisson/Convection Equation_: despite being linear PDEs, they represent a suitable scenario to showcase the detrimental effect of the spectral bias on the training of PINNs, due to the presence of high-frequency components in the solution.
* _Burgers' Equation_: this nonlinear PDE is commonly used to test PINNs, and usually they reach a valid solution even with a first-order optimizer, due to the PDE's simplicity.
* _Navier-Stokes Equation_: it poses challenges for both PINNs and classical methods, being a difficult nonlinear PDEs. We test the case of the fluid flow in the wake of a 2D cylinder .

For the sake of compactness, we refer to Appendix G for detailed descriptions of the mentioned PDEs, and to Appendix H for supplementary numerical experiments not included in the main text. We compare results obtained by the LM algorithm with those from commonly used optimizers for training PINNs, such as Adam  and L-BFGS . Where not stated otherwise, Adam is trained for \(10^{5}\) iterations and LM for \(10^{3}\) iterations. Additionally, we provide a comparison with other methods that are ad-hoc enhancements of PINNs, such as loss balancing  (also known as NTK rescaling), Random Fourier Features (RFF) , and curriculum training (CT) . Our performance metric is the relative \(L^{2}\) loss on the test set, detailed in Appendix H formula (28).

Figure 2: **(a) Left:** in yellow the non-zero components of the Hessian matrix at initialization (up in the linear case, down the nonlinear one). **Center:** mean and standard deviation of the spectral norm of the \(H_{r}(0)\) over \(m\) in the linear case (for \(10\) independent experiments). **Right:** same as Center, but for a nonlinear PDE. **(b)** Eigenvalues of \(K(0)\) for a first-order optimizer and \(D(0)\) for a second-order method applied to Burgers’ equation.

Linear PDEs affected by spectral biasIn Figure 3, we demonstrate the effectiveness of second-order methods in handling equations with high spectral bias. Part (a) of Figure 3 focuses on the Poisson equation with high-frequency components, for which is common to use RFF . On the left, we show that Adam requires RFF to converge to a reasonable solution. On the right, we observe that LM not only significantly outperforms Adam combined with RFF, but also that incorporating RFF with LM leads to remarkable loss reduction from the very first iterations. In Part (b) of Figure 3, we investigate the effect of high convection coefficients \(\) in the convection equation as discussed in , where it is shown that a PINN trained with Adam necessitates of curriculum training to achieve meaningful results on such a spectrally biased PDE. However, we show on the left Figure 3, part (b), that the LM optimizer can handle higher values of \(\), especially when curriculum training is introduced. Remarkably, on the right of Figure 3(b), we show that a PINN trained with LM, without any other enhancements, achieves high accuracy with \(\) values up to 100. This level of accuracy is not feasible with Adam and curriculum training alone, which, as noted in , manages coefficients only up to 20.

Nonlinear PDEsFirstly, we consider the case of Burgers' equation, where convergence is achievable even with first-order methods. To address concerns about the additional computational time required by second-order methods, in Figure 4, part (a), we display the relative \(L^{2}\) loss over wall time when training on Burgers' equation. All training methods can reach a reasonable solution, however, while the precision of PINNs trained with Adam and L-BFGS is approximately \(10^{-3}\), PINNs trained with LM can consistently attain precision around \(10^{-5}\) in few iterations and very short GPU time. Figure 4 also provides a qualitative estimate of the runtime of LM in comparison to Adam and L-BFGS. The intermediate performance of L-BFGS, falling between first- and second-order methods, is explained in Remark 4.3. Lastly, a similar outcome can be seen in part (b) of Figure 4, where we demonstrate that employing the LM optimizer makes it possible to obtain a reasonable solution even for Navier-Stokes equation in terms of relative \(L^{2}\) loss over PDE time. Notice that in this case, we employ causality training  for both Adam and LM.

Figure 4: **(a) Burgers’ equation: mean and standard deviation of the relative \(L^{2}\) loss for various optimizers over wall time (repetitions over 10 independent runs). (b) Navier-Stokes equation: mean and standard deviation of the relative \(L^{2}\) loss over the PDE time \(\) for PINNs trained with Adam and LM (\(10\) independent runs). Both optimization methods are enhanced with causality training.**

Figure 3: **(a) Poisson equation: median and standard deviation of the relative \(L^{2}\) loss for different optimizers over training iterations (repetitions over 10 independent runs). (b) Convection equation: median and standard deviation of the \(L^{2}\) loss after \(1000\) iterations achieved over 5 independent runs with and without CT for different values of the convection coefficient \(\) (left) and solution obtained with LM (and no other enhancement) after 5000 iterations with \(\) = 100 (right).**

### Limitations and possible solutions

The major limitation of our findings is related to scalability. Traditionally, second-order methods have been avoided for machine learning models due to their poor scaling with an increasing number of parameters. However, one can adopt classical PDE solution approaches, such as domain decomposition, to utilize a collection of smaller networks instead of a single large one. Similarly, one can embrace machine learning-based solutions such as ensemble models  or mixture of experts . We advocate that existing models such as [14; 25; 37] could already be strongly enhanced with the usage of second-order methods for training. In the scenario where these approaches are impractical, one could also resort to techniques in the field of optimization to enable the scalability of the method. For medium to large-sized networks, the challenge of storing the matrix \(J^{T}J\) in GPU memory becomes infeasible. This can be addressed through an inexact LM method, which involves solving the equivalent system \(\|J-r_{}\|=0\) using a Krylov subspace iterative method (LSQR or LSMR) [27; 7]. These methods only require Jacobian-vector products, which can be efficiently computed through backpropagation.

## 6 Conclusion

In this paper, we conduct an in-depth analysis of PINNs training utilizing the NTK framework. We elucidate the distinction between linear and nonlinear cases, and reveal that even in the optimistic infinite-width limit, favorable outcomes observed with NTK in linear cases do not extend to nonlinear PDEs. Motivated by the NTK anaylsis, we emphasize the significant advantage of employing second-order methods. These seem to mitigate the spectral bias issue and to improve convergence even for challenging nonlinear PDEs. Second-order methods, such as LM, consistently achieve a precision comparable or even better than the state-of-the-art presented in . Notably, our findings demonstrate that convergence is attainable without resorting to typical training protocols aimed at enhancing PINNs. However, combining these enhancements with second-order training methods can further improve accuracy while reducing computational time, as demonstrated in our numerical experiments. Accuracy and convergence guarantees are indeed two crucial components for the majority of real-world applications of PDE solvers. In practice, second-order methods may be preferable when the solution contains high frequencies, when the application demands high accuracy, or when the target PDE is nonlinear. A key objective of our paper is to highlight that, despite their scalability challenges, second-order methods could help bridge the gap between black-box machine learning models and PDE solutions in scientific machine learning.