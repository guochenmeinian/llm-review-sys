# Decision-Driven Calibration for Cost-Sensitive Uncertainty Quantification

Gregory Canal

The Johns Hopkins University Applied Physics Laboratory

{greg.canal,vladimir.leung,philip.sage,i-jeng.wang}@jhuapl.edu

Vladimir Leung

The Johns Hopkins University Applied Physics Laboratory

{greg.canal,vladimir.leung,philip.sage,i-jeng.wang}@jhuapl.edu

John J. Guererrio

Completed during an internship at JHU/APL.Dartmouth College

Philip Sage

The Johns Hopkins University Applied Physics Laboratory

{greg.canal,vladimir.leung,philip.sage,i-jeng.wang}@jhuapl.edu

I-Jeng Wang

The Johns Hopkins University Applied Physics Laboratory

{greg.canal,vladimir.leung,philip.sage,i-jeng.wang}@jhuapl.edu

###### Abstract

In recent years, the ability of artificial intelligence (AI) systems to quantity their uncertainty has become paramount in building trustworthy AI. In standard uncertainty quantification (UQ), AI uncertainty is calibrated such that the confidence of its predictions matches the statistics of the underlying data distribution. However, this method of calibration does not take into consideration the direct influence of UQ on the subsequent actions taken by downstream decision-makers. Here we demonstrate an alternate, decision-driven method of UQ calibration that explicitly minimizes the incurred costs of downstream decisions. After formulating decision-driven calibration as an optimization problem with respect to a known decision-maker, we show in a simulated search-and-rescue scenario how decision-driven temperature scaling can lead to lower incurred decision costs.

## 1 Introduction

Artificial intelligence (AI) has become an integral component in decision-making pipelines in many settings across society, ranging from medicine to scientific discovery. Rather than providing only a single predicted output in these systems, it has become common for AI to also supplement their predicted outputs with quantified notions of _uncertainty_ describing the level of confidence in predictions, i.e., to perform _uncertainty quantification_ (UQ). For a decision-maker taking actions in downstream tasks, only considering the AI's predicted output in isolation runs the risk of discounting critical information about possible sources of ambiguity in the AI's output. In contrast, taking the full UQ distribution into consideration can better enable informed decision-making that trades off the potential costs and benefits of various actions . For instance, a doctor aided by an AI diagnosis system can better weigh the risks of various patient treatment options if the AI provides a confidence level for its diagnosis, only moving ahead with risky treatments if the AI is confident enough.

Rather than serving only as a passive aid, in general we expect the specific UQ outputs generated by the AI to have a significant impact on the actions actually taken by downstream decision-makers consuming these outputs. Continuing with the above example, if the doctor only proceeds with a treatment if the AI's confidence exceeds a particular threshold (e.g., 90%), then the difference of only a few confidence percentage points may have a drastic effect on the patient's outcome. This implies that the selection and tuning of the UQ algorithm (e.g., temperature scaling , conformal prediction ) can potentially result in different behaviors by the downstream decision-maker .

Based on these observations, we note that common UQ calibration techniques minimizing distributional metrics such as Expected Calibration Error (ECE) do not necessarily improve decision-making performance in the downstream application of interest, as measured by incurred decision costs. Recognizing the critical role of UQ in the overall decision-making pipeline, in this work we introduce _Decision-Driven Calibration_ (DDC) as a means of optimizing UQ parameters to explicitly minimize expected downstream decision costs. Although aspects of decision-aware calibration have been explored in prior work, to our knowledge this work is the first to optimize UQ for minimization of incurred costs with respect to a given decision-cost function and a (potentially non-ideal) decision-maker profile. We demonstrate in a simulated search-and-rescue task how DDC can reduce decision costs in comparison to standard temperature scaling-based calibration.

Related workUQ comprises a vast body of work, providing a multitude of methods for learning uncertainty estimates from data [5; 6]; here we specifically focus on prior work leveraging knowledge of the downstream decision process to inform UQ selection and calibration.  focuses calibration in output probability regions that have the most impact on diagnostic decision making, but considers only the scenario of an unknown decision cost function in narrow settings. Another line of work introduces the notion of Decision Calibration Error (DCE), measuring the discrepancy between the estimated decision costs incurred by an output probability distribution in comparison to the true costs incurred under the true data distribution [8; 9; 10]. While optimizing UQ to minimize DCE does ensure that decision costs computed under the UQ distribution are reliable estimates of the true expected cost, this is distinct from our goal of optimizing UQ to _minimize the incurred decision costs themselves_.  does optimize UQ to minimize downstream decision costs while taking into account a given decision-maker model, but is limited to the case of binary decisions.  uses a fixed decision-maker model to optimize conformal prediction for minimizing decision-maker classification error; however, this approach is limited to classification actions.  explicitly optimizes UQ to minimize incurred decision-making costs, but the decision space is limited to the decision-maker either accepting the AI's recommendation or solving the task themselves.

## 2 Methods

Let \(\) denote a data domain (e.g., \(=^{d}\)) with individual examples denoted by \(\). Here we consider the scenario of multi-class classification where each \(\) is associated with a label \(y 1 C\), but the concepts demonstrated here could be applied to other settings (e.g., regression). Consider an AI model given by the function \(f_{}^{C}\) mapping from examples to "logit" vectors \(=f_{}()\). In standard classification, the AI model's prediction would be taken as \(=*{arg\,max}_{y^{}}f_{}^{(y^{})}(x)\), where \(f^{(i)}\) denotes the \(i\)th entry of \(f\). More generally, logit \(\) can be used to generate a UQ output such as a probability distribution (via the softmax function) or a conformal set. Adopting notation from , for generality let \(g_{}^{C}\) denote a generic UQ function parameterized by \(\) and mapping to an uncertainty representation space \(\). For instance, for softmax class probabilities we have \(g^{C}^{C-1}\) given by \(g()=()\) where \(\) is the softmax function.

We suppose that a decision-maker observes \(g_{}(f_{}())\) and chooses an action \(a\) from some set of \(K\) actions \(\), noting that it need not be the case that \(=1 C\) or even for \(K=C\). Without loss of generality we model the decision-maker as a known conditional probability distribution \(^{K-1}\) and assume that the action \(a\) for data point \(\) is sampled from \((g_{}(f_{}()))\). We assume that taking action \(a\) on example \(\) with ground-truth label \(y\) incurs a cost of \(c(y,a)\), where \(c(,)\) is a known cost function. For data \(,y\) distributed according to \(_{X,Y}\), we define the expected decision cost as \(C()=_{,y_{X,Y}}\ _{a(g_{}(f_{ }()))}[c(y,a)]\). For optimal decision-making (in an average sense), the expected incurred cost \(C\) should be as low as possible.

Decision-agnostic calibrationIn typical calibration, the parameters of \(g_{}\) are adjusted such that the UQ output matches the underlying statistics of the data distribution. In practice this is typically achieved by optimizing a scoring rule \(\) over a calibration set \(S_{}\) sampled from \(_{XY}\):

\[^{*}=*{arg\,min}_{}\ }|}_{,y  S_{}}(g_{}(f_{}()),y).\] (1)

In this work we focus on standard Temperature Scaling (TS) applied to a label distribution  optimized with Negative Log-likelihood (NLL) as the UQ method and associated calibration criteria.

TS adjusts the model's output probabilities by dividing the logits by a scalar "temperature" parameter \(T\) which smooths the probability distribution without altering the ranking of predicted classes. In our notation, this corresponds to a UQ function \(g_{T}()=(}{{T}})\), where the only UQ parameter \(\) to be optimized is the temperature \(T\). Letting \(g^{(i)}\) denote the \(i\)th entry of \(g\), the NLL loss is given by \(_{}(g_{T}(),y)=- g_{T}^{(y)}()\) which can be optimized in (1) over the calibration set.

Decision-driven calibrationWhile performing standard calibration as in (1) will encourage the UQ outputs \(g\) to match the underlying statistics of the data distribution, this method of calibration does not necessarily ensure a reduced downstream decision cost \(C\). Instead, as a loss function in (1) we utilize a decision-driven calibration loss, computed as the expected decision cost incurred with respect to a fixed decision-maker \(\) (denoting the \(i\)th entry of \(\) by \(^{(i)}\)):

\[_{}(g(),y)=_{a}^{(a)}(g() )\,c(y,a).\] (2)

We refer to the optimization of (1) with \(_{}\) as _decision-driven calibration_ (DDC). Although DDC assumes that both \(\) and \(c\) are fixed and known, it is possible to learn \(\) from observed decision-maker behavior ([11; 12]), and in many settings numeric costs can be assigned to various action outcomes.

Decision-maker modelsWe consider two possible decision-maker models for optimization in (2) that accept as input a label distribution and generate a decision:

_Smooth Bayes' Optimal:_ It is well-known that the average-case optimal action is the _Bayes' optimal_ action minimizing the expected incurred cost, i.e., \(a^{*}=_{a}_{y^{}}p^{(y^{})}\,c(y^{},a)\), where \(p^{(y)}\) is the _true_ conditional probability of an example having label \(y\). Since the decision-maker only has access to \(g()\) and not \(p\), they can instead take a Bayes' optimal action with respect to \(g()\) as \(=_{a}_{y^{}}g^{(y^{})}()\,c(y^{},a)\), or in vector notation as \(=_{a=}\), i.e., the one-hot vector at \(\). To make this decision-maker differentiable, we apply the softmax approximation \(_{i=_{i}^{(i)}}()\) and express the \(\) in \(\) as a negated \(\). This leads to \(=(-^{T}g())\) where \(L\) is the \(C K\) matrix whose \(L[i,j]\) entry is given by \(c(i,j)\).

_Proportional:_ We model the decision-maker as first selecting a label \(y^{}\) with probability \(g^{(y^{})}()\), and then taking the optimal action for that class. This corresponds to a probability of action \(a\) given by \(_{y^{}}[y^{},a]\,g^{(y^{})}()\) where \(\) is the \(C K\) matrix given by \([i,j]=1\) for \(j=_{a}c(i,a)\) and \([i,j]=0\) otherwise. We can represent this decision-maker in vector form as \(=^{T}g()\).

## 3 Simulated decision-making

We aim to demonstrate in a realistic decision-making setting how DDC can result in reduced test-time decision costs when applied to TS, in comparison to standard calibration. To do so, we simulate an AI-assisted search-and-rescue scenario where a drone is tasked with patrolling a section of beach and assisting swimmers in distress. We assume the drone will encounter objects belonging to three classes: boat, swimmer without a life jacket, and swimmer with a life jacket. In this example we assume that all swimmers without a life jacket are in distress, while all swimmers with a life jacket are not in distress. Upon encountering a new object, the drone can take one of three actions: continue its route without intervention (_Do Nothing_), "mark" the object to return to later (_Mark Object_), or drop a flotation device and call for help (_Rescue_). For each class-action pair we assign a nominal numerical "cost" characterizing the quality of each decision, presented in Table 1 and justified as follows: the most severe error is ignoring a swimmer without a life jacket (i.e., in distress).

  Ground-truth class (\(y\)) &  \\  & _Do Nothing_ & _Mark Object_ & _Rescue_ \\   boat & 0 & 15 & 50 \\ swimmer w/o life jacket & 100 & 75 & 0 \\ swimmer w/ life jacket & 10 & 5 & 30 \\  

Table 1: Search-and-rescue decision cost matrix (\(c(y,a)\)).

Dataset and model trainingWe simulate this decision-making process on the multi-object tracking (MOT) subset of SeaDronesSee , a large-scale dataset of overhead aerial drone footage (see example images in Figure 3). This dataset consists of 22 video clips, encompassing a total of \(\)50k frames and \(\)400k annotations and contains objects in the classes described above.2 We split SeaDronesSee-MOT as follows: for our "test" set we use the original "valiation" split. To construct a "calibration" set, within each video we partition the original MOT "training" set by allocating the first 75% for model training and the final 25% for calibration. To focus only on the task of classification, we crop each image to the ground-truth bounding boxes of objects in the scene. This results in \(\)120k training instances, \(\)40k calibration instances, and \(\)50k testing instances.

For the AI predictive model \(f_{}\) mapping from cropped images to logits \(\), we train a single linear layer as the classification head on a ViT-B/16 backbone . The ViT backbone was pretrained on the SWAG dataset and fine-tuned end-to-end on ImageNet-1k . After freezing the backbone, we fine-tune the classification head on our custom SeaDronesSee-MOT training split (excluding the calibration set). We used a learning rate of 0.003, Cross-Entropy loss with a class re-weighting scheme from , and optimized with AdamW . As a UQ output, we then apply TS to the model Softmax outputs \(()\), optimizing over the calibration split using either NLL or DDC calibration loss. Further training details can be found in Appendix A.

Numerical resultsOur main objective is to compare the average decision cost at test-time incurred by decision-driven calibration TS in comparison to decision-agnostic (i.e., standard) TS. For decision-driven calibration, we refer to the calibration method according to the decision-maker assumed during calibration (e.g., Smooth Bayes TS, Proportional TS). For a given temperature \(T\) (as optimized by one of the calibration methods above), we measure the incurred cost over a test set \(S_{}\) as \(C(T)=}|}_{,y S_{}}_{ {DDC}}(g_{T}(f_{}()),y)\), where \(_{}\) is computed with respect to a test-time decision-maker \(_{}\) not necessarily equal to the one used during decision-driven calibration (when applicable). In particular, we evaluate all TS methods (Decision-agnostic TS, Smooth Bayes TS, and Proportional TS) against both the Smooth Bayes and Proportional decision-makers (Figure 1). When computing \(C(T)\) for temperatures obtained from Smooth-Bayes DDC, we observe a smaller incurred decision cost than Decision-agnostic TS. However, we find that calibrating according to a mismatched Proportional decision-maker incurs a worse cost than decision-agnostic TS. Similarly, when evaluating against a Proportional decision-maker at test-time, Proportional TS incurs a smaller cost than Decision-agnostic TS, which itself incurs a lower cost than Smooth Bayes TS. These results indicate that performing decision-driven calibration with temperature scaling does lead to reduced costs at test time, but a mismatch in decision-maker between calibration and test-time can actually _increase_ decision costs.

Figure 1: test-time expected decision cost.

The difference in incurred cost between the various calibration methods becomes even more apparent when decomposing average cost by ground-truth class. In the case of a Smooth Bayes decision-maker, calibrating with respect to the same decision model results in a much lower average cost incurred on the Swimmer class compared to Decision-agnostic and Proportional calibration (Figure 2). This is a desirable outcome in our search-and-rescue scenario, as swimmers without life jackets are the most critical class to protect. Interestingly, a tradeoff is made by slightly _increasing_ average cost when conditioned on the other ground-truth classes. This could be attributed to the structure of the cost matrix (Table 1), where taking incorrect actions on a swimmer without a life jacket results in significantly larger penalties than incorrect actions on other classes. Similar observations can be made when examining classwise costs when evaluated with a Proportional decision-maker (Figure 4).

When examining the optimized temperature values for each calibration method, we observe that decision-agnostic temperature scaling and Smooth Bayes temperature scaling actually adjust the temperature in _opposite_ directions (i.e., less or greater than 1.0), resulting in opposite effects on model confidence (Figure 5): decision-agnostic temperature scaling _increases_ the confidence of model predictions (decreased temperature), while Smooth Bayes temperature scaling _decreases_ the confidence of model predictions (increased temperature). Future research will be crucial in understanding how the magnitude of specific action costs and the balance of costs between classes affects decision-driven calibration performance. Additional results comparing the calibration performance of decision-agnostic and decision-driven calibration can be found in Appendix B, including a comparison of test-time ECE and NLL (Figure 6) and reliability diagrams (Figure 7).

## 4 Conclusion

Overall, this work introduces decision-driven calibration as a means of directly optimizing UQ to minimize incurred costs for a known decision-maker, and to our knowledge is the first work to perform this type of calibration on a general decision cost function. We demonstrate the promise of DDC in comparison to standard temperature scaling for reducing incurred costs in a simulated scenario on a large-scale search-and-rescue dataset. Our results also demonstrate the importance of calibrating according to a decision-maker matched to the one taking actions at test-time, and motivates the approach in [11; 12] of accurately estimating a decision-maker before calibration occurs. Important future avenues of work include a theoretical study of the robustness of DDC to decision-maker mismatch or shifts in decision-making behavior, and the utilization of active learning  to efficiently estimate a decision-maker model from a minimal number of queries.