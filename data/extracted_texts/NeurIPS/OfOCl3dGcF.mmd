# ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs

Irene Huang\({}^{*1}\)

Wei Lin\({}^{*2}\)

M. Jehanzeb Mirza\({}^{*1}\)\({}^{1}\)

Jacob A. Hansen\({}^{1}\)

Sivan Doveh\({}^{3}\)

Victor Ion Butoi\({}^{1}\)

Roei Herzig\({}^{4}\)

Assaf Arbelle\({}^{3}\)

Hilde Kuehne\({}^{5,6}\)

Trevor Darrell\({}^{4}\)

Chuang Gan\({}^{5,7}\)

Aude Oliva\({}^{1,5}\)

Rogerio Feris\({}^{5}\)

Leonid Karlinsky\({}^{5}\)

\({}^{*}\)Equally Contributing Authors.

\({}^{1}\)MIT, USA. \({}^{2}\)JKU, Austria. \({}^{3}\)IBM Research, Israel. \({}^{4}\)UC Berkeley, USA.

\({}^{5}\)MIT-IBM, USA. \({}^{6}\)Tuebingen AI Center, Germany. \({}^{7}\)UMass Amherst, USA.

###### Abstract

Compositional Reasoning (CR) entails grasping the significance of attributes, relations, and word order. Recent Vision-Language Models (VLMs), comprising a visual encoder and a Large Language Model (LLM) decoder, have demonstrated remarkable proficiency in such reasoning tasks. This prompts a crucial question: have VLMs effectively tackled the CR challenge? We conjecture that existing CR benchmarks may not adequately push the boundaries of modern VLMs due to the reliance on an _LLM only_ negative text generation pipeline. Consequently, the negatives produced either appear as outliers from the natural language distribution learned by VLMs' LLM decoders or as improbable within the corresponding image context. To address these limitations, we introduce ConMe1 - a compositional reasoning benchmark and a novel data generation pipeline leveraging VLMs to produce 'hard CR Q&A'. Through a new concept of VLMs conversing with each other to collaboratively expose their weaknesses, our pipeline autonomously generates, evaluates, and selects challenging compositional reasoning questions, establishing a robust CR benchmark, also subsequently validated manually. Our benchmark provokes a noteworthy, up to 33%, decrease in CR performance compared to preceding benchmarks, reinstating the CR challenge even for state-of-the-art VLMs.

## 1 Introduction

Present day Vision-Language Models (VLMs)  have recently emerged as the default choice for many computer vision tasks. However, these models also have their Achilles' heel. Several recent studies have highlighted important VLM failure modes, especially their lacking ability to perform Compositional Reasoning (CR) . CR is the ability of the VLM to recognize and attend to the language concepts beyond objects (_i.e.,_ nouns), such as attributes, relations, fine-grained object alternatives, and more, in both the image and text of a VL pair. As noted in , earlier dual-encoder VLMs (_e.g.,_ CLIP ) have especially low, even close to chance, CR performance. However, more modern VLMs, which combine a pre-trained vision encoder with a strong LLM decoder (_e.g.,_ LLaVA ) and employ both architectural (projection layer/MLP, tuningof the LLM decoder) and instruction-tuning-based alignment [18; 19], demonstrate much stronger performance on compositional reasoning task when evaluated on present CR benchmarks [14; 15; 16]. Most CR benchmarks [14; 15; 16] have been formed from collections of text-image pairs, grouped by the presence of certain CR concepts, such as relations, attributes, _e.t.c._, by a process of randomly "flipping" the present CR concept in the positive text to form a "negative alternative" text (having the CR concept wrong). The VLM's preference for the resulting negative is then compared to the true positive source text thus testing the VLM's ability to entail the correct text from the image.

Originally, simple word substitution or ordering changes were used for this CR concept flipping [14; 15; 16], relying on simple language augmentation heuristics and tools. This simple approach was able to elegantly illustrate the CR fail modes of dual-encoder VLMs [1; 20; 21]. This can be intuitively explained due to their contrastive pretraining, for which representing only the objects (nouns) is sufficient to disambiguate all the text-image pairs in a random batch of limited size. However, modern VLMs, _e.g.,_, demonstrate significantly higher performance on such benchmarks. We conjecture that their increased CR performance stems from two factors: (i) the negative synthesis heuristic may generate "out-of-natural-language-distribution" samples and is not powerful enough to "fool" the LLM decoders of the VLMs; (ii) even if the language of the produced negative is in-distribution, the produced CR concept manipulation that forms the negative text might be unlikely for the scene observed in the corresponding image. As we observe in our evaluations in Section 4.2, even the SugarCrepe benchmark  designed specifically to generate in-language-distribution samples (to thus not suffer from factor (i)) by using LLMs for negative synthesis and applying language-side debiasing, likely suffers from factor (ii), as not looking at the image can produce unlikely negatives (w.r.t. the image). This naturally leads us to ask - did the modern VLMs relying on LLM decoders solve the previous issue with the low CR performance of dual-encoder VLMs?

We propose a new CR benchmark ConMe, generated through our novel automated data generation pipeline utilizing GPT-4V  with a combination of open-source modern VLMs to answer this question in the negative. Our pipeline gradually discovers what the stronger VLM (GPT-4V) sees/knows and other (evaluated) VLMs do not, to produce plausible and difficult question-and-answer options for CR testing. In a way, our pipeline creates a 'conversation' among VLM agents to collaboratively expose their weaknesses to GPT-4V, as shown in Figure 1. Using VLMs in the pipeline instead of relying on LLM generation, we propose a new method of incorporating both image and language context during the benchmark curation, thus avoiding suffering from factors (i) or (ii) as mentioned above. Finally, we show that our conclusions on hardness generalize to strong unseen VLMs likely due to the similar nature of their Visual Instruction tuning alignment methodology.

To summarize, our contributions are as follows: (i) Through extensive experiments we show that compositional reasoning is still a significant problem for present-day VLMs and their CR performance can be evaluated more closely than currently possible by existing CR benchmarks; (ii) We propose a novel CR data generation pipeline that incorporates GPT-4V and contemporary open-source VLMs which can potentially be used to generate abundant challenging CR data; and (iii) We also contribute a challenging CR benchmark ConMe, which leads to up to 33% decrease in CR performance of SOTA VLMs as compared to the preceding benchmark. Moreover, we accompany ConMe with an LLM-based analysis tool for automatic mining of insights on VLMs' CR weaknesses.

## 2 Background and Related Work

**Vision-Language Models.** There has been a remarkable surge in VLMs, which have shown impressive performance for many vision-language understanding tasks, _e.g._, zero-shot classification, visual question-answering (VQA), image captioning, _e.t.c_. In a broader sense, the present day VLMs can be

Figure 1: We propose a new concept of VLMs conversing with each other to collaboratively expose their weaknesses. Our pipeline autonomously generates, evaluates, and selects challenging Compositional Reasoning (CR) questions, to establish a robust CR benchmark â€“ **ConMe**.

divided into two families. One family of methods relies on dual-encoders (vision and text encoder) and usually trains the encoders with a contrastive objective by using a large corpus of paired image-text data scraped from the web. Most common among these methods are CLIP , ALIGN  and OpenCLIP . Many different ideas have been explored to improve these models, _e.g.,_ by using off-the-shelf object detectors [25; 26; 27], using cross-attention and additional regularization objectives [28; 29; 30; 31], filtering noisy captions (_e.g.,_ BLIP ), employing textual nearest-neighbors , using geometrically consistent representations , caption augmentations [32; 33]. In parallel, some other methods employ few-shot supervision [34; 35; 36] and label-free finetuning [37; 38; 39; 40]. The other family of methods aligns the visual modality with a frozen LLM. BLIP-2  bridges the modality gap between a pre-trained visual encoder and an LLM by using a Querying Transformer. InstructBLIP  proposes to improve  by employing instruction tuning. MiniGPT  grounds a vision encoder with a frozen LLM (Vicuna ) by only using a (trainable) linear projection layer between the two. MiniGPT-V2  replaces the LLM with LLaMA-2  and also proposes to unfreeze it during the training and finetuning phases. LLaVA  also grounds an LLM with a pre-trained visual encoder and proposes Visual Instruction Tuning by carefully curating instruction-response pairs, to enhance the performance, with further improvements proposed in . Some other works [46; 47; 48; 49; 50; 51] also explore similar ideas and propose certain improvements.

**Compositional Reasoning Benchmarks.** Compositionality Reasoning is the VLM's ability to attend to more complex concepts in natural language, beyond only objects (_i.e.,_ nouns). Recently, many benchmarks have been proposed to evaluate CR in VLMs. Winoground  proposes a simple task to evaluate the CR ability of VLMs - given two images and captions, the goal is to correctly match them, where the captions contain the same words but in different orders. They show that the SOTA VLMs only perform slightly better than chance. COLA  proposes a benchmark where the task for the VLM is to retrieve correct images (based on the correct configuration of attributes and objects) from a database, where _distractor_ images are present. Crepe  also evaluates several SOTA VLMs and highlights the lack of CR abilities. The evaluation is performed on a proposed benchmark inspired by cognitive science literature, specifically testing for systematicity and productivity. Attribution, Relation, and Order (ARO) benchmark  is a large-scale dataset specifically designed to evaluate the VLM's ability to understand the relational and order abilities. SugarCrepe  shows that the current benchmarks proposed to test the VLM's ability for compositionality are easily _hackable_. They evaluate _blind_ models having no access to image data and show that they outperform modern VLMs on these benchmarks. To fix this, they forego the rule-based hard negative generation by employing LLMs and propose adversarial refinement. However, they mostly focus on "syntactic correctness" and language model-based hackability and de-biasing, also without checking VLM task hardness. As a result (as demonstrated in Section 4.2) modern VLMs (employing strong LLM decoders) experience no significant challenge on SugarCrepe , even compared to Crepe, its predecessor. To counter these shortcomings in the previous benchmarks, we propose ConMe which is generated through an automated pipeline that foregoes relying only on the LLMs by employing VLMs to generate new CR question-and-answer options by leveraging a multi-turn conversation between stronger (GPT-4V) and weaker VLMs to increase question difficulty by gradually uncovering what weaker models are blind to, as well as incorporating image context into the hard negative formation.

**V&L Benchmarks QA Analysis Taxonomies** Taxonomies, if available, in V&L Benchmarks are acquired either by manual verification [53; 54; 55] or from the existing annotations of the image source [15; 16; 17]. In our proposed ConMe curation, a group of VLMs (including GPT-4V) communicate with each other collaboratively uncovering their CR weaknesses, such that GPT-4V is able to generate CR QA that are difficult for all VLMs (including unseen) and GPT-4V itself. The available VLM error taxonomies related to our generated CR QA in ConMe are therefore dynamic, scalable, and adaptive. This necessitates an automatic LLM-based taxonomy analysis tool for generating interesting insights using our ConMe benchmark. We contribute such a tool in this work (Section 5.2).

## 3 ConMe: A Compositional Reasoning Benchmark

In contrast to the previous benchmarks [16; 17] which are created either by rule-based manipulation of the text to create the negatives or use LLMs for this task, our ConMe is harnessed by employing additional image context using 'a conversation' between state-of-the-art VLMs. Our negative text generation pipeline can create abundant challenging CR data, given only a random collection of images. In this paper, we use our proposed text generation pipeline on the images present in the SugarCrepe  dataset and come up with a challenging CR benchmark labeled ConMe. In the following, we provide details about our proposed automated CR data generation pipeline. A brief overview of the SugarCrepe dataset partitions is provided below and in the Appendix Section E.

### Hard Compositional Reasoning (CR) QA Generation Pipeline

Our proposed hard negative mining pipeline consists of GPT-4V and several open VLMs (LLaVA 1.6-7b , LLaVA 1.5-7b , InstructBLIP Flan-T5 , and InstructBLIP Vicuna-7b ) deployed in a multi-stage setup. An overview of all the stages is provided in Figure 2 and next we describe them in detail. More details and the full prompts used are provided in the Appendix Section A.

**GPT-4V Generated Description (Stage 1)** In the first stage, we prompt the GPT-4V to generate a detailed description of the input image. We treat this as the "ground truth" description, to capture as many details that the model deems itself confident in describing.

**Downstream VLM Generated Descriptions (Stage 2)** In stage 2, we prompt each targeted downstream VLM to generate a detailed description for the same input image, by similarly prompting the VLMs as in stage 1. This discloses to GPT-4V what the open VLMs'see' (or 'pay attention to') on this input image, which is helpful for comparison in the next stages.

**Iteration 1 of GPT-4V Generated Questions (Stage 3)** In this stage, we provide the GPT-4V with the descriptions that it generated (in stage 1), along with descriptions from the individual downstream VLMs, and prompt it to generate multiple (_e.g.,_ 10) challenging CR questions based on these generated descriptions. Furthermore, we also prompt the model to provide an answer (positive) and multiple corresponding negatives to that positive. The goal here is to provide GPT-4V with the descriptions that the downstream VLMs generated so that it can come up with targeted reasoning questions about the details that the open-source VLMs missed.

**VLM Inference Evaluation to Iteration 1 Questions (Stage 4)** Each CR question-answer from stage 3 is now framed as a binary multiple-choice selection between two answer options, one of which is the correct answer and the other is the negative option. We employ the 'generate' evaluation mode (asking the VLM to generate the index of the correct choice) to evaluate each of the four downstream VLMs and record the resulting accuracies for each model. Later, we perform an intermediate filtering step. Specifically, for each collected data sample (_i.e.,_ input image, question, correct answer, and one negative option), we evaluate all the downstream VLMs (_i.e.,_ LLaVA 1.6-7b, LLaVA 1.5-7b, InstructBLIP Flan-T5, InstructBLIP Vicuna-7b). If all four VLMs answered correctly, we discard this data sample; conversely, if at least one VLM answered incorrectly, we keep the sample. After filtering, we obtain the updated accuracy values for each of the four models.

**Open-Ended Answer Generation to Iteration 1 Questions (Stage 5)** Given the first iteration of generated CR questions, we pass these to the four open VLMs to obtain their open-ended answers intending to utilize the resulting responses to provide GPT-4V with additional context on what image details the models do not perceive (failing on).

Figure 2: Our proposed CR data generation framework employs multiple open VLMs in a multi-stage â€˜conversationâ€™ setup. Given an image, first, GPT-4V and the VLMs are prompted to describe the image in detail. Then, providing all the generated descriptions from the VLMs and the GPT-4V itself as context, GPT-4V is tasked with the generation of the first iteration of CR questions, and the VLMs are evaluated on these questions and also prompted to generate open-ended answers. Finally, GPT-4V is again employed and prompted to generate _more_ challenging CR questions with the additional context from the previous iterations output resulting in challenging CR questions, and their correct answers (positives) and confusing wrong answers (negatives).

**Iteration 2 of GPT-4V Generated Questions (Stage 6)** In this stage, we again prompt the GPT-4V with additional context asking it to generate more challenging CR data. Specifically, we prompt the GPT-4V with the set of 10 CR questions it generated (_cf_. stage 3) and also the open-ended answers from the open VLMs and instruct it to generate more challenging CR questions. The intuition behind prompting the model in this manner is to make GPT-4V reflect upon its own generated data and also have the additional context of the details, the downstream VLMs focused upon, in their answers.

**VLM Inference Evaluation to Iteration 2 Questions (Stage 7)** This stage is concerned with the final evaluation performed on the second iteration of GPT-4V-generated questions. Similarly to stage 4, we again use the 'generate' inference evaluation with the questions framed as a binary multiple-choice selection. Also similar to stage 4, we again filter out any samples that all four VLMs answer correctly. Accordingly, we record the evaluation results for the resulting filtered dataset (ConMe) - the final curated dataset produced by our pipeline.

Our proposed pipeline curates a new dataset for a set of input images from the \(3\) SugarCrepe  partitions, which consists of 919 total images2 - 333 from the _Replace-Att_ partition, 333 from _Replace-Object_, 253 from _Replace-Relation_. However, our automated hard CR QA pipeline can generate challenging compositional reasoning QA on an arbitrary set of images. Extensive experimentation in Section 4 highlights the challenging nature of our proposed ConMe dataset which also generalizes beyond the set of 4 open VLMs employed during ConMe curation.

## 4 Experiments

We first provide implementation details we employ for our ConMe curation pipeline, then introduce the dataset partitions and later discuss the results of 7 VLMs (including GPT-4V itself) on our contributed ConMe dataset.

**Implementation Details** We employ GPT-4V  through the OpenAI API, using the gpt-4-vision-preview endpoint. We use the default setting for the image resolution and limit the number of new tokens generated by the model to \(2000\). We use four open VLMs for ConMe curation: LLaVA 1.6-7b , LLaVA 1.5-7b , InstructBLIP Flan-T5 , and InstructBLIP Vicuna-7b . For open-ended text generation, for the two LLaVA models, we use temperature 0 and max new tokens 500; for the two InstructBLIP models, we use temperature 1.0, max new tokens 500, top 90% probability mass, repetition penalty 1.5, length penalty 1.0, and number of beams 5. When prompting the VLMs to answer binary multiple choice questions, for both LLaVA models, we use temperature 0 and max new tokens 128; for both InstructBLIP models, we use temperature 1.0, max new tokens 10, top 90% probability mass, repetition penalty 1.0, length penalty 1.0, and number of beams 5. The above settings are directly taken from the respective publications and empirically validated for best performance. For a fair comparison, for generation inference mode when prompting these VLMs to answer binary multiple-choice questions, we use the same generation parameters as those we used for evaluating the InstructBLIP models. Furthermore, we also validate that the same conclusions are obtained by switching to 'perplexity' inference for multiple choice questions (choosing the answer by minimal CLM loss value computed by the LLM decoder [58; 59]).

### Datasets

The three SugarCrepe partitions are structured to target a particular CR aspect (_i.e.,_ attributes, objects, or relations) within an image, and provide only a single question per aspect. On the other hand, to construct ConMe, we generate various CR questions for each image, thus resulting in a larger dataset in terms of sample count. The total sample size for the SugarCrepe and ConMe datasets is listed in Table 1. Thanks to our proposed automated ConMe curation pipeline, this dataset can be further expanded by incorporating an arbitrary image set.

  
**Benchmarks** & **replace-att** & **replace-obj** & **replace-rel** & **Total** \\   SugarCrepe & 788 & 1652 & 1406 & 3846 \\ ConMe & 8863 & 8691 & 6793 & 24347 \\   

Table 1: Total number of samples per partition in the SugarCrepe and our ConMe benchmarks.

### Results

We evaluate 6 strong open VLMs - the four used in ConMe curation and two 'unseen' models, on the baseline SugarCrepe dataset and the ConMe dataset curated through our proposed CR QA generation pipeline (Sec. 3). The 'unseen open VLMs': InternLM-XComposer2-VL-7b  and Idefics2-8b - recent improvement of Idefics1 , were used to evaluate the generalization of ConMe to models not seen during its curation. Furthermore, we also evaluate GPT-4V itself on ConMe showing its performance significantly decreases compared to the original SugarCrepe. Surprisingly, using our proposed ConMe pipeline that employs GPT-4V for hard CR QA generation - GPT-4V is shown to 'fool' itself! For comparison between the baseline and pipeline datasets, we calculate the evaluation accuracy for all 3 partitions of the original SugarCrepe and average the 3 numbers into the final metric reported in Table 2. We observe a substantial performance drop of 23.3% from original SugarCrepe when averaged over the 7 evaluated VLMs. When comparing the results for the \(4\)'seen' (during ConMe curation) models on the baseline SugarCrepe dataset and our curated ConMe benchmark, we see even more significant performance drops as expected. For example, we observe a performance drop of up to \(31.7\%\) and \(33.0\%\) on the LLaVA and InstructBLIP families of models, when evaluated on the more challenging ConMe benchmark as compared to the original SugarCrepe.

Since our ConMe curation pipeline is employing VLMs (that 'converse' with GPT-4V), it is also important to evaluate our ConMe benchmark on unseen VLMs to test its generalization ability. For this purpose, we evaluate \(2\) state-of-the-art open VLMs: InterLM-XComposer, Idefics2. And we also report results for GPT-4V itself as it was not targeted for the hard CR QA production (rather, it generated them). We observe from the results in Table 2 that our ConMe also generalizes to unseen VLMs and is even challenging for GPT-4V which is often considered as the strongest VLM currently available. We see that for unseen VLMs our ConMe benchmark provokes a performance drop of up to 15.4%. Notably, we also observe a performance drop of 11.2% when evaluating GPT-4V. These results show that our proposed ConMe benchmark is not only challenging for the VLMs employed in our CR QA generation pipeline but can also generalize to unseen SOTA VLMs.

**Manually Verified ConMe Partition.** Our CR QA generation pipeline employs a conversation between multiple generative models, thus it can be prone to issues like hallucinations in text, resulting in unfair model evaluations. To analyze such issues, we manually verified a subset of \(1000\) samples3 from the ConMe dataset and also reported the accuracy on this subset in Table 2. Manually verified partition is contributed as part of ConMe. Comparing the evaluation on the entire ConMe vs its manually-verified partition (Tab. 2), the performance is almost the same, even with the manual-verified partition being 0.5% 'harder' on average. These results show that our CR QA generation pipeline is robust to common generative-models-based errors and (automatically curated) ConMe predictions, in terms of CR strengths and weaknesses analysis of seen and unseen VLMs, are trustworthy. We attribute this to the effective multi-stage filtering performed in our generation pipeline.

  
**Models** & **Seen** & **SugarCrepe** & **ConMe** & **Manual Subset** & **Performance Drop** \\   LLaVA 1.5-7b & âœ“ & 88.5 & 57.7 & 56.2 & -30.8 \\ LLaVA 1.6-7b & âœ“ & 89.2 & 57.5 & 54.9 & -31.7 \\ InstructBLIP Flan-T5 & âœ“ & 91.4 & 58.5 & 59.8 & -33.0 \\ InstructBLIP Vicuna-7b & âœ“ & 82.5 & 53.6 & 46.2 & -28.9 \\ InterLM-XComposer2-VL-7b & âœ— & 92.0 & 79.7 & 82.1 & -12.3 \\ Idefics2-8b & âœ— & 85.5 & 70.1 & 72.1 & -15.4 \\  GPT-4V & & 91.2 & 80.1 & 81.8 & -11.2 \\  Mean & & 88.6 & 65.3 & 64.8 & -23.3 \\   

Table 2: Average accuracy (%) across baseline and our generated data partitions using generate inference evaluation mode. **ConMe** results refer to the evaluation of all the data samples, while **Manual Subset** results are obtained from a manually verified subset of \(1000\) samples.

## 5 Analysis and Ablations

This section provides detailed ablations and analysis of our proposed benchmark. We first ablate the effect of multiple stages of filtering performed in our ConMe curation framework, then compare the generation and perplexity-based inference method found in the literature, and finally provide insights into the error taxonomy contributed as part of ConMe and automatically applied to analyze all the evaluated VLMs drawing interesting conclusions and insights on their strength and weaknesses. For completeness, we also delegate some qualitative examples to the Appendix Section D.

### Comparison with Perplexity Inference Evaluation

For modern VLMs, two different types of evaluation methods are used by the community. Namely, the generation and the perplexity:

**Generation:** For generation evaluation, we use the forward call of the model to output a letter option, A or B, and we record sample accuracy based on this generated output letter, after string-comparing it with the ground truth. The main results in Table 2 are obtained by evaluating the models in the generate mode.

**Perplexity:** For perplexity evaluation, we calculate the loss score (perplexity score) associated with each answer option, and we determine the sample accuracy by selecting the letter associated with the smaller loss value [58; 59]. More formally, denoting the VLM visual encoder by \(_{V}\)4 and the LLM decoder by \(_{L}\) the perplexity score \(P(I,T)\) for an image \(I\) and a text \(T\) (\(T\) can include a prompt prefix) is defined as:

\[-log(T|I)=-log(_{i=1}^{|T|}( _{L}(_{V}(I),T_{[1:i-1]}),T_{i})),\] (1)

For completeness, the prompt templates used for perplexity inference, as some validation evidence supporting the prompts used are provided in the Appendix Section B.

In Table 3 we provide the detailed results with both the evaluation protocols on the three partitions from our curated ConMe benchmark. We see that both evaluation protocols provide consistent results for the different partitions. Furthermore, we also see the effect of multi-stage filtering proposed in our ConMe curation framework. With each successive filtering step, the benchmark becomes more challenging. For example, the accuracy drops by \( 30\%\) on average (after stage \(2\) filtering) for the four models as compared to the average accuracy obtained in the first step of the framework. Moreover, the difference between the results obtained from the perplexity and generate inference mode is only \( 2\%\) on average, signifying the consistency of the obtained results regardless of the inference approach. We also ablate the baseline SugarCrepe dataset for these two evaluation modes and also for the VQAScore  and provide the results in the Appendix Section B.

    &  &  &  &  &  \\   & & **Perplexity** & **Generate** & **Perplexity** & **Generate** & **Perplexity** & **Generate** & **Perplexity** & **Generate** \\    & replace-att & 82.2 & 82.2 & 69.0 & 64.7 & 79.3 & 79.2 & 56.5 & 57.9 \\  & replace-obj & 83.5 & 83.2 & 70.7 & 65.7 & 79.9 & 80.1 & 51.6 & 57.0 \\  & replace-rel & 82.9 & 82.9 & 70.5 & 65.6 & 79.7 & 79.5 & 57.8 & 57.6 \\   & replace-att & 79.5 & 79.5 & 64.3 & 59.2 & 73.5 & 73.8 & 58.2 & 58.5 \\  & replace-obj & 80.4 & 80.7 & 65.1 & 60.4 & 73.9 & 73.6 & 51.3 & 56.8 \\  & replace-rel & 79.6 & 80.0 & 64.8 & 59.6 & 73.6 & 73.4 & 56.5 & 57.8 \\   & replace-att & 78.7 & 78.1 & 62.8 & 56.3 & 76.4 & 77.1 & 62.1 & 59.8 \\  & replace-obj & 79.3 & 78.5 & 63.1 & 56.1 & 76.0 & 75.9 & 61.1 & 57.4 \\  & replace-rel & 78.4 & 78.0 & 62.6 & 55.6 & 75.8 & 75.7 & 58.2 \\   & replace-att & 61.0 & 69.4 & 31.9 & 39.1 & 58.3 & 64.4 & 45.9 & 52.1 \\  & replace-obj & 61.5 & 70.2 & 31.5 & 38.9 & 58.8 & 66.8 & 47.4 & 54.7 \\   & replace-rel & 60.6 & 70.1 & 32.0 & 39.6 & 58.7 & 65.6 & 56.1 & 54.0 \\   

Table 3: Accuracy (%) using perplexity and generate inference evaluation modes while comparing performance across the generated partitions for iterations 1 and 2 of GPT-4V generated CR questions.

### Error Taxonomy Analysis

We complement the ConMe curation pipeline - our automatic framework for mining hard CR QA, by also contributing an LLM-based analysis tool for automatic categorization of VLM mistakes according to human-specified error taxonomies provided simply by natural language description. This analysis pipeline is necessary, as our ConMe curation pipeline is automatic, adaptive (to the target VLMs being analyzed), and scalable (in a sense we can scale arbitrarily by providing the ConMe curation framework with more images). Hence, the generated CR QA in ConMe need to be analyzed (categorized) automatically in order to dynamically mine insights on the evaluated VLM weaknesses in terms of the relative distribution of their errors across error categories or other CR QA insights specified by the taxonomies. For our analysis tool, we utilized LLaMa-3 8B  to categorize our human-filtered ConMe partition. We leverage two natural language taxonomy specifications provided in Tables 5 and 6 in the Appendix. The complete LLaMa-3 8B prompts are provided in Appendix Section C. In Figure 3 we plot the mistake rate for the 7 VLMs which are partitioned by the _error category type_. Similarly, Figure 4 provides the breakdown of VLM mistakes according to _CR QA formats_. We observe notable actionable insights (for future work) and interesting conclusions for different models. For example, LLaVA 1.6-7B showed a large improvement in 'emotion understanding' compared to LLaVA 1.5-7B (decreasing emotion error from 40% to 27%), but suffered a large decrease in 'counting ability' (counting mistakes rose from 26% to 53%). We also find that InternLM XComposer2 VL 7B struggles with proximity assessment, Idefics2-8B with emotion recognition, and InstructBLIP Flan T5 with counting. Interestingly, GPT-4V CR errors are more evenly distributed among error categories, peaking at proximity assessment errors, while our analysis also identified it's somewhat higher tendency to misconception and hallucination in terms of CR QA formats taxonomy. We delegate further error analysis to the Appendix Section C but in summary, our findings can lead to actionable improvement targets for each model. For example,

Figure 4: Distribution of mistake rates of various VLMs across different CR QA formats automatically obtained by our proposed analysis framework. Tab. 5 in the Appendix specifies each CR QA format.

Figure 3: Distribution of mistake rates of various VLMs across different error categories automatically obtained by our proposed analysis framework. Table 6 in the Appendix specifies each error category.

LLaVA 1.6 can benefit from more instructional data targeting improving the capability of fine detail analysis (like object attention and counting) while avoiding misconception or hallucination; Idefics2-8B could enhance higher-level reasoning such as emotion recognition, leveraging additional datasets in that area; InternLM XComposer2 VL 7B could benefit from additional training data focused on proximity detection. Such insights, instrumented by our ConMe and its analysis tool, are crucial for guiding future developments in VLM architectures, instruction data collection, and training methodologies, ensuring more robust compositional reasoning capabilities across diverse visual and textual contexts.

## 6 Conclusions and Limitations

We have presented a fully automated hard negative generation framework and a curated dataset ConMe for evaluating and analyzing CR performance of modern VLMs, which include an LLM decoder component and hence are more sensitive to language mistakes and learn to better interpret the provided image context. With thorough evaluation and analysis, we have found that our proposed approach is significantly more effective in detecting and targeting compositional reasoning failure modes of the state-of-the-art VLMs. Our work provides a pathway to building increasingly difficult (and adaptive to VLM evolution) benchmarks for even modern VLMs, as our proposed methodology can easily be employed to generate challenging CR data sources, given any arbitrary image collection. In the future, the proposed negative generation pipeline can also be extended to curate large-scale training datasets for finetuning models to improve the compositional reasoning aspects of these models, in addition to further analysis of the types of failure modes most common to modern VLMs.

LimitationsLike any other research, our work also comes with certain limitations. The quality and robustness of the curated dataset rely strongly on the VLM used to generate the CR questions. In this regard, we selected GPT-4V for its demonstrated success in generative capabilities when processing both image and text inputs concurrently. Nevertheless, it is not perfect, and our proposed generation framework can introduce errors. However, the evaluations on the human-verified subset hint that the errors introduced in our ConMe dataset are uniformly distributed and the difference (in accuracy) as compared to the entire ConMe dataset is almost negligible. In the future, as the models become better, our CR data generation pipeline will also directly benefit. Furthermore, another way to address this limitation could also target analyzing the images associated with manually verified errors, to capture aspects of images that could be best used for applying the proposed pipeline to other image data collections.