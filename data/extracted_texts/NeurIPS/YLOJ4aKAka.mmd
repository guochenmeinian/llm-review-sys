# Connecting Pre-trained Language Models and Downstream Tasks via Properties of Representations

Chenwei Wu

Duke University

cwwu@cs.duke.edu

&Holden Lee

Johns Hopkins University

hlee283@jhu.edu

&Rong Ge

Duke University

rongge@cs.duke.edu

###### Abstract

Recently, researchers have found that representations learned by large-scale pre-trained language models are useful in various downstream tasks. However, there is little theoretical understanding of how pre-training performance is related to downstream task performance. In this paper, we analyze how this performance transfer depends on the properties of the downstream task and the structure of the representations. We consider a log-linear model where a word can be predicted from its context through a network having softmax as its last layer. We show that even if the downstream task is highly structured and depends on a simple function of the hidden representation, there are still cases when a low pre-training loss cannot guarantee good performance on the downstream task. On the other hand, we propose and empirically validate the existence of an "anchor vector" in the representation space, and show that this assumption, together with properties of the downstream task, guarantees performance transfer.

## 1 Introduction

Large-scale pre-trained language models have achieved strong performance in a wide range of downstream tasks, including natural language inference (Devlin et al., 2018) and reading comprehension (Brown et al., 2020). For many of these tasks, training a linear classifier on top of the hidden-layer representations generated by the pre-trained models can already provide near state-of-the-art results (Belinkov et al., 2017). Despite many empirical investigations about the zero-shot applications of these pre-trained models, there is little theoretical understanding of their empirical success. In this paper, we aim to theoretically investigate this core question:

_When can the **representations** from pre-trained models transfer to downstream tasks that are **very different** from the pre-training task?_

This is a fundamental question in understanding why good performance in pre-training leads to good performance on downstream tasks. Unlike the notion of generalization in traditional learning theory where the models are evaluated in the same task and the test data are sampled from the same distribution as the training data, here the downstream tasks are usually very different from pre-training. For instance, people can pre-train large language models using cross-entropy loss on a language modeling task with webpage data, and evaluate the models using classification accuracy on text classification in news articles. The differences between pre-training and downstream tasks make it challenging to explain the success of these language models.

To overcome this challenge, we need a way to model the relationship between the pre-training and downstream tasks. Previous research has taken several approaches in this direction: Wei et al. (2021) assumes a latent-variable generative model for the data and a downstream task depending on the latent variables; Saunshi et al. (2021) formulates the downstream classification task as a language modeling task which is similar to the pre-training task. These works either rely on strong explicit assumptions about the structure of the data (i.e., assuming the data is generated from a simple generative model) or treat the entire pre-trained model as a black box.

### Our contributions

In this paper, we consider a very general model for the data and open the black box of the pre-trained model at the last layer. Specifically, for an input sequence \(x=(x_{1},,x_{L})\) where the entries comes from a dictionary \(\{1,,n\}\), we assume the observation probability of \(x_{i}\) satisfies a log-linear model

\[p^{*}(x_{i}=j|x_{-i})( v^{*}_{-i}(x_{-i}),v^{*}_{j}),\]

where \(x_{-i}\) is the sequence \(x\) without \(x_{i}\), \(v^{*}_{j}\) is a vector only depending on word \(j\), and \(v^{*}_{-i}\) can be an arbitrary function. This aligns with commonly used networks whose last layer is usually a softmax layer. Moreover, since our model does not put any constraint on the function \(v^{*}_{-i}\), it can be arbitrarily complicated, such as a huge transformer model such as BERT (Devlin et al., 2018) or GPT-3 (Brown et al., 2020). We also allow the distribution of the input to be different in pre-training and downstream tasks. This makes our setting more general than previous latent models Wei et al. (2021); Arora et al. (2016).

We assume the pre-training task is to predict a word from its context. During pre-training, for every input sequence \(x\), we want our model to predict the "label" \(x_{i}\) from \(x_{-i}\). The model we use in training (which we call the "student model") has the same log-linear structure: \(p(x_{i}=j|x_{-i})( v_{-i}(x_{-i}),v_{j})\).

For the downstream task, for simplicity we focus on binary sequence classification, e.g., sentiment classification. To define downstream tasks, let the "logits" of the ground-truth model be defined as \(z^{*}:=( v^{*}_{-i}(x_{-i}),v^{*}_{j})_{j=1}^{n}\) (these are just the outputs before the softmax computation), and assume that the downstream task is specified by a function of the logits, \(f^{*}(z^{*})\).

In reality, we do not have access to the ground-truth model \(v^{*}_{-i}\) and \(v^{*}_{j}\). Instead, we only have access to the student model \(v_{-i}\) and \(v_{j}\) that achieves low pre-training loss. We can define the student logits as \(z:=( v_{-i}(x_{-i}),v_{j})_{j=1}^{n}\). In some sense, \(z\) is the representation learned by the pre-training step. A natural idea for solving the downstream task would be to learn a function \(f(z)\). More details about this model will be provided in Section 2.

Our goal is to understand the properties of the learned representation. In order to simplify the problem, we assume that the student model can be optimized to achieve a small KL divergence with the true word probabilities during training. Under this setting, the question we ask above becomes:

_If the downstream task depends on a simple function of the logits \(f^{*}(z^{*})\) and we have access to a student model \(p\) such that \(_{x}[D_{}(p^{*}(x_{i}|x_{-i})|[p(x_{i}|x_{-i}))]\) is small, under what conditions is there a function \(f\) such that \(f(z) f^{*}(z^{*})\)?_

A priori, one might imagine if the function \(f^{*}\) is very simple (e.g., a linear function), then it should be easy to find a function \(f\). However, we give two counter-examples that show there are additional properties that \(f^{*}\) needs to satisfy: (i) \(f^{*}\) should not distinguish between words with small probabilities and words with super-small probabilities, and (ii) the hidden representations must have some structure that deals with the shift-invariance of the softmax function (that is, the result of softmax does not change if all the logits are shifted by the same constant). We present the counterexamples in Section 3.

To further investigate the structure of the hidden representations and see how we can deal with the shift-invariance property of softmax, in Section 4, we propose and empirically verified the "anchor vector hypothesis": there exists an "anchor vector" in the representation space that can be used to estimate the _bulk partition function_, which we define to be the sum of the exponential of all logits except the largest few, i.e., \(_{j:z^{*}_{j}}e^{z^{*}_{j}}\). We show how the anchor vector can be used to address the shift-invariance of softmax.

Based on the observation that anchor vectors exist, in Section 5, we give sufficient conditions that shows when a sparse one-hidden-layer ReLU network \(f^{*}\) can be learned by our student model \(f\). Specifically, assuming that \(f^{*}\) is a one-hidden-layer ReLU network depending on a small set of words and the downstream task is binary classification depending on \(f^{*}(z^{*})\), the existence of the anchor vector enables us to upper bound the loss on the downstream task attained by the student model \(f(z)\), in terms of its KL divergence in pre-training. In other words, a small pre-training loss is guaranteed to transfer to a small downstream classification error.

### Related works

**Theoretical understanding why pre-training helps downstream tasks**: Most of the relevant existing works rely on latent variable models and show that pre-training could recover some form of the latent variables. Arora et al. (2016) proposed the RAND-WALK latent model and explains the empirical success of word embedding methods such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). Arora et al. (2017) extended the previous model to justify sentence embeddings, and Arora et al. (2018) explained sentence embedding via compressed sensing. Other models are also used in this line of work, e.g., hidden Markov models (Wei et al., 2021) and graphical models (Zhang and Hashimoto, 2021). Lee et al. (2021) and Tosh et al. (2021) assume conditional independence or multi-view structure in the pre-training data and prove that training an additional linear layer on top of learned representations can perform well in downstream tasks.

The problem setting in our paper is similar to that of Saunshi et al. (2021), which also analyzes the performance transfer of pre-trained language models to binary classification downstream tasks. They treat the pre-trained model as a black box and assume that the downstream task can be formulated as a sentence completion task, while we open the black box at the last layer and connect pre-training with downstream tasks by the "anchor vector" and function \(f^{*}\). Moreover, they focus on the prediction probabilities of the pre-trained model while we instead focus on the representations.

**Applications and analysis of hidden representations from large-scale language models**: The hidden representations produced by large language models such as BERT (Devlin et al., 2018) or ELMo (Peters et al., 2018) have been very useful in various NLP tasks. A standard method is to train a linear classifier on these representations, though there are other methods such as using the normalized mean of concatenated word embeddings (Tanaka et al., 2020). To understand why these word embeddings are useful, people have empirically showed that BERT word embeddings contain information about sentence-level context (Miaschi and Dell'Orletta, 2020), word sense (Wiedemann et al., 2019), and syntactic phenomena (Tenney et al., 2019) including parse trees (Hewitt and Manning, 2019; Kim et al., 2020). Other empirical explanations include the flatness of local minima achieved by pre-training (Hao et al., 2019), connection to deep metric learning (Tschannen et al., 2019), and the attention patterns across different heads (Kovaleva et al., 2019).

Language modeling can also be considered as a way of using the hidden representations because the next word probability is usually the softmax of the product of the representation and the dictionary matrix. Therefore, any zero-shot application of pre-trained auto-regressive language models, e.g., GPT-3 (Brown et al., 2020) and T5 (Raffel et al., 2020), is a specific method of using the hidden representations.

Some previous works have found that the word embeddings learned by language models can lie in a narrow cone: Gao et al. (2019) empirically found this phenomena for the learned word embeddings in LSTM and vanilla transformers. Ethayarajh (2019); Cai et al. (2020) found similar phenomena for contextualized embeddings in BERT-like models.

## 2 Problem setup

Notations.We use \([n]\) to denote the set \(\{1,2,,n\}\). For an input sequence \(x=(x_{1},,x_{L})\), we use \(x_{-i}\) to denote the input sequence without the \(i\)-th entry where \(i[L]\), i.e., \(x_{-i}:=(x_{1},,x_{i-1},x_{i+1},,x_{L})\). We let \(D_{KL}(p||q)\) be the KL-divergence between distributions \(p\) and \(q\) and define \(H(p)\) to be the entropy of distribution \(p\).

Ground-truth model.We consider the following model: There is a set of words \([n]\), each with a fixed corresponding vector \(v^{*}_{j}^{d}\) (\(j[n]\)). We refer to each \(v^{*}_{j}\) as an atom and the matrix \([v^{*}_{1},,v^{*}_{n}]^{d n}\) as the dictionary. At each position \(i\), let \(x_{i}\) be the value of the word at that position; then \(x_{i}[n]\). Assume that the probability of \(x_{i}\) given \(x_{-i}\) follows a log-linear model, i.e.,

\[p^{*}(x_{i}=j|x_{-i})( v^{*}_{-i}(x_{-i}),v^{*}_{j}),\] (1)

where \(v^{*}_{-i}()\) is a function that encodes the remaining sequence \(x_{-i}\) into a vector in \(^{d}\).

We also use \(z^{*}_{j}(x,i):= v^{*}_{-i}(x_{-i}),v^{*}_{j}\) to denote the \(j\)-th logit and \(Z^{*}(x,i):=_{j=1}^{n}(z^{*}_{j}(x,i))\) to denote the partition function, i.e., the normalization factor of equation (1). In other words,

\[ j[n], p^{*}(x_{i}=j|x_{-i})=_{j}(x,i))}{Z^{*} (x,i)}=_{-i}(x_{-i}),v^{*}_{j})}{Z^{*}(x,i)}.\] (2)

Student model.We use a black-box neural network model whose penultimate layer outputs a \(d^{}\)-dimensional vector \(v_{-i}(x_{-i})^{d^{}}\) and the last layer is a fully-connected layer with weight matrix \([v_{1},v_{2},,v_{n}]^{d^{} n}\) followed by the softmax function. In other words, the model output is

\[p(x_{i}=j|x_{-i})( v_{-i}(x_{-i}),v_{j}).\] (3)

Similar to the ground-truth model, we use \(z_{j}(x,i):= v_{-i}(x_{-i}),v_{j}\) to denote the \(j\)-th logit and \(Z(x,i):=_{j=1}^{n}(z_{j}(x,i))\) to denote the partition function of equation (3), so

\[ j[n], p(x_{i}=j|x_{-i})=(x_{-i}),v_{ j})}{Z(x,i)}.\] (4)

Pre-training.For self-supervised pre-training, we are given (data, "label") pairs \((x_{-i},x_{i})\), and want our model to predict the "label" \(x_{i}\) given \(x_{-i}\). The pre-training loss we use here is cross-entropy loss:

\[(v_{-i})=_{x}[-p^{*}(x_{i}|x_{-i}) p(x_{i}|x_{-i})]=_{x}[D_{KL}(p^{*}(x_{i}|x_{-i})||p(x_{i}|x_{-i}))]+_{x}[H(p^{*}(x_{i}| x_{-i}))].\]

Note that \(_{x}[H(p^{*}(x_{i}|x_{-i}))]\) is a constant, and we assume that our student model achieves a small loss value so that the KL-divergence term \(_{x}[D_{KL}(p^{*}(x_{i}|x_{-i})||p(x_{i}|x_{-i}))]_{ KL}\) for some \(_{ KL}\).

Downstream task.The downstream task we are considering is binary sequence classification, e.g., sentiment classification. For instance, given a sentence, we (probably after adding some prompts) use our pre-trained model to predict the missing word \(x_{i}\) from the given input \(x_{-i}\). We assume that there is a perfect classifier \(f^{*}(x,i)\) only depending on \(v^{*}_{-i}\) that can distinguish between positive and negative samples. In other words, for all \(x,f^{*}(x,i)>0\) and for all \(x,\,f^{*}(x,i)<0\). Here \(\) and \(\) are the set of positive and negative input sequences, respectively. We choose to focus on binary classification for theoretical analysis for simplicity, but most of the ideas can be extended to a multi-class classification setting.

A simple downstream task is one whose classifier is linear in \(v^{*}_{-i}\), that is, \(f^{*}(x,i):= v^{*}_{-i}(x_{-i}),\,u^{*}\) for some \(u^{*}^{d}\). One might also expect more structures in the vectors \(u^{*}\) and \(v^{*}_{-i}\). For \(u^{*}\), the downstream task usually depends on only a small set of the words which are related to this task. For example, for sentiment classification, the sentiment of a sentence depends mostly on words similar to "positive" or "negative". This can be formalized as the following assumption:

**Assumption 1** (\(u^{*}\) is a \(k\)-sparse combination of \(\{v^{*}_{j}\}_{j=1}^{n}\)).: _Assume \(u^{*}\) is a sparse combination of at most \(k\) vectors in \(\{v^{*}_{j}\}_{j=1}^{n}\) and without loss of generality assume these \(k\) vectors are \(\{v_{1},,v_{k}\}\), i.e., there exist coefficients \(\{c^{*}_{j}\}_{j=1}^{k}^{k}\) such that \(u^{*}=_{j=1}^{k}c^{*}_{j}v^{*}_{j}\)._

Note that the fixed representations \(\{v_{1},,v_{k}\}\) correspond to the weights of the last layer in large language models instead of the token representations \(v^{*}_{-i}(x_{-i})\). In other words, the token representations can still be different depending on their context.

There are usually differences between input distributions of pre-training and downstream tasks. The difference may be due to different data sources. For example, the samples in the downstream task may only contain movie reviews while the pre-training dataset can include all kinds of texts on the Internet. It can also result from prompting, which has become the dominant way of using large language models for downstream tasks (Brown et al., 2020; Radford et al., 2019). For instance, for movie review classification, appending "This movie was" to the original input could improve the classification accuracy. Similar to the assumption made in Saunshi et al. (2021), we use \((0,1]\) to capture this difference. A smaller \(\) indicates a larger difference between the two distributions, and \(=1\) if and only if these two distributions are the same. In most tasks, the two distributions are not too different and we would expect a reasonable value of \(\).

**Assumption 2** (Difference between pre-training and downstream distribution).: _Let \(p_{pre}\) and \(p_{DS}\) be the probability density functions of the pre-training and downstream task, respectively. We assume that there exists \((0,1]\) such that_

\[ i, x,\ p_{pre}(x_{-i})  p_{DS}(x_{-i}).\]

If the ground truth classifier \(f^{*}\) can be too close to 0, it would not be robust to small perturbations. We use the standard margin assumption to avoid such cases:

**Assumption 3** (Margin for downstream task).: _There exists a margin \(^{+}\) such that at any position \(i\), if \(x\), then \(f^{*}(x,i)\), and if \(x\), then \(f^{*}(x,i)-\), where \(\) and \(\) are the sets of positive and negative samples, respectively._

Ideally, we want to show that such simple downstream tasks can also be solved well with the representations learned by our student model, i.e., \(v_{-i}\) and \(\{v_{j}\}_{j=1}^{k}\). However, as we will see in Section 3, our current model with this margin assumption still doesn't guarantee good downstream task performances. More structures in the model and the downstream task is necessary to make sure that the pre-trained representations are useful for the downstream task.

## 3 Cases when learned representations are insufficient for downstream tasks

The problem setting in Section 2 seems reasonable at first sight, but in the following subsections, we will show that this model is not enough to guarantee good pre-training performance to generalize to downstream tasks. In other words, there are ways for the student model to approximate the ground-truth probabilities very well in terms of KL divergence but perform very badly at the downstream task. Therefore, we need to put further constraints on the ground-truth model and the downstream task.

### Downstream tasks sensitive to words with super-small probability

Intuitively, KL divergence is a weighted log probability difference between two distributions where the weight is the ground-truth probability. Therefore, for the entries with small ground-truth probabilities, a large log probability difference will not result in a large KL divergence. However, the log probability difference is proportional to the difference in the value of \(f^{*}(x,i)\). This makes it possible for the student model to flip the sign of \(f^{*}(x,i)\) without incurring a large KL divergence, as presented in Theorem 1 whose proof is given in Appendix A.

**Theorem 1**.: _Suppose the downstream task performance depends only on a function \(f^{*}(x,i)= v^{*}_{-i}(x_{-i}),u^{*}=_{t=1}^{k}c^{*}_{t}  v^{*}_{-i}(x_{-i}),v^{*}_{t}\). For \(t^{-}[k]\), define \(p^{-}:=p^{*}(x_{i}=t^{-}|x_{-i})\), and assume \(p^{-}\). Then for all \(s^{+}\), there exist functions \(v_{-i}\) and \(\{v_{t}\}_{t=1}^{k}\) such that \(D_{}(p^{*}(x_{i}|x_{-i})||p(x_{i}|x_{-i})) 2sp^{-}\) and \(f(x,i):=_{t=1}^{k}c^{*}_{t} v_{-i}(x_{-i}),v_{t} f^{*}( x,i)-s c^{*}_{t^{-}}\)._

Theorem 1 shows that if there is some word of interest \(t^{-}\) that has a small probability \(p^{-}\), then it is possible to have a model with small KL divergence in pre-training but bad downstream performance. This is because changing the KL divergence by only \(2p^{-}(x,i)}{c^{*}_{t^{-}}}\) is enough to change the label of the downstream prediction. In other words, as long as the KL divergence is higher than the threshold \(2p^{-}(x,i)}{c^{*}_{t^{-}}}\), we cannot distinguish between the case where the student model makes an already small probability even smaller (which can hurt the downstream task performance) and the case where random approximation errors are spread across the entries. In this case, a small KL divergence does not necessarily imply good downstream performance.

Note that this sensitivity of the downstream task to very small logits is not natural. For the downstream tasks in practice, after conditioning on the context, whether a word has a probability of \(10^{-5}\) or \(10^{-10}\) should not influence the label of the sequence. Thus, we need to impose additional structure on our model. We make the downstream task ignore super-small entries by setting a threshold for the logits and ignoring the logits smaller than that threshold. In this case, making the logits smaller when they are already small will have no influence on the downstream task performance. Concretely, the enhanced model will be (\((x):=\{x,0\}\) is the ReLU function):

\[f^{*}(x,i)=_{j=1}^{k}a_{j}^{*}(z_{j}^{*}(x,i)-b_{j}^{*})=_{j=1}^{ k}a_{j}^{*}( v_{-i}^{*}(x_{-i}),v_{j}^{*}-b_{j}^{*}).\] (5)

### Representations are not shift-invariant

The softmax function is invariant under shift, i.e., the output stays the same if we add the same value to every coordinate of the input. In the current model, we have no control over the shift of student model logits on unseen data. Consequently, even if we get a student model that performs well on the training data for the downstream task, we cannot guarantee the performance of this model on new data. This can be formalized in the following theorem.

**Theorem 2**.: _Assume \(z^{*}(x,i)\) is bounded. For any function \(f^{*}(x,i)=_{j=1}^{n}a_{j}(z_{j}^{*}(x,i)-b_{j})\), there exist functions \(\{_{j}(x,i)\}_{j=1}^{n}\) such that for all \(x\) and \(i\), we have \((x_{i}|x_{-i})=p^{*}(x_{i}|x_{-i})\) and \((x,i):=_{j=1}^{n}a_{j}^{*}(_{j}(x,i)-b_{j}^{*})\) is always equal to \(0\). In other words, the pre-training loss of the model \(\{_{j}(x,i)\}_{j=1}^{n}\) is the same as \(\{z_{j}(x,i)\}_{j=1}^{n}\), but its logits are useless for the downstream task._

Proof.: We choose \(\) such that \( x,i,<_{j[n]}b_{j}^{*}-_{j[n]}z_{j}^{*}(x,i)\), and \( x,i, j[n]\), we set \(_{j}(x,i):=z_{j}^{*}(x,i)+\), then

\[ j[n],_{j}(x,i)-b_{j}^{*}<z_{j}^{*}(x,i)+_{j[n]}b_{j}^ {*}-_{j[n]}z_{j}^{*}(x,i)-b_{j}^{*} 0,\] (6)

which implies that \((_{j}(x,i)-b_{j}^{*})=0\). Therefore, \( x,i\), we have \((x,i)=0\). 

Theorem 2 indicates that without any structure in the representations, the student model is able to shift the logits for any sample and keep the pre-training loss unchanged. In the worst case, it can shift the logits for unseen data drastically, resulting in a bad downstream performance. Therefore, a theoretical guarantee for downstream performance requires structure in the representations learned by the pre-trained model.

## 4 "Anchor vector" hypothesis and empirical verifications

In Section 3.2, we showed that the shift-invariance of the softmax function can potentially make the student logits useless for the downstream task. Therefore, to understand why the downstream tasks benefit from representations from the pre-trained model, we need to understand the structure of these representations, and this structure must be able to handle the shift-invariance problem.

### "Anchor vector" hypothesis

There are different ways to prevent the shift-invariance of softmax from influencing the performance of the downstream tasks. One way of doing this is to keep the partition function stable. Recall that in (4), the probability of a word is the exponential of the corresponding logit divided by the partition function. If the partition function is constant for different samples, the logits can be uniquely determined by the probabilities, which solves the shift-invariance problem. Arora et al. (2016) showed that when both the word embeddings and the latent representation are uniformly distributed on a sphere, the partition function is close to a constant with high probability. They also empirically verified this uniformity of word embeddings trained using GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) on English Wikipedia. However, as we will see in later experiments in Sections 4.2 and F, this is not true for recent large-scale pre-trained language models.

Instead of uniformity of word embeddings, in large pre-trained models such as GPT-2 (Radford et al., 2019), we observe that if we remove several most frequent words from the computation of the log partition function, the remaining part can be well approximated by the inner product between the hidden representation \(v_{-i}(x)\) and a fixed vector. This motivates us to have the following "anchor vector" hypothesis:

**Definition 1**.: _For a sample \(x\) and position \(i\), we could select a set of bulk words \(B(x,i)[n]\), and we define the bulk partition functions as \(Z^{*}_{}(x,i):=_{j B(x,i)}( v^{*}_{-i}(x_{-i}),v ^{*}_{j})\) and \(Z_{}(x,i):=_{j B(x,i)}( v_{-i}(x_{-i}),v_{j} )\)._

The selection of bulk words \(B(x,i)\) can usually be selected manually or by a simple algorithm. For instance, we can construct \(B(x,i)\) by taking out the words corresponding to the largest entries in \(p(x_{i}|x_{-i})\). We can also manually select all the words that are irrelevant to the downstream task.

**Hypothesis 1** ("Anchor vector" hypothesis).: _There exists \(v_{0}^{d}\) such that_

\[ v_{-i}(x_{-i}),v_{0} Z_{}(x,i).\]

If our hypothesis holds, we can use \(v_{0}\) as an anchor for the logits because it can be used to estimate the partition function, and this does not change the form of the downstream task by much. In the next subsection, we will show that Hypothesis 1 holds for most popular language models and provide some discussions. More details about these models are provided in Appendix E.

### Empirical verification of "anchor vector" hypothesis

Figure 0(a) plots the mean squared approximation error of the log bulk partition function. We use different versions of pre-trained GPT-2 (Radford et al., 2019) and OPT (Zhang et al., 2022), and use the first \(1/4\) of WikiText-2 (Merity et al., 2016) as the input text. The hidden representations we use in this experiment are the last hidden states of these models, i.e., the output of the penultimate layer. The dimension of the hidden representations ranges from 768 to 2048, and the number of tokens is about 70k. We choose the bulk words to be all the words except those having top-\(k\) probabilities and compute the optimal anchor vector using the closed-form least-squares solution. In our experiments, we use the mean squared error (MSE) to measure the approximation quality. Formally, the MSE is defined as

\[_{}=_{v_{0}}_{x,i}[( v_{-i}(x_{-i}),v _{0}- Z_{}(x,i))^{2}].\]

The values of the bulk partition functions are around 10 (with comparable standard deviation), and we can see from Figure 0(a) that the MSE is usually several orders of magnitude smaller. Therefore, the inner product between the hidden representation and the optimal anchor vector can usually approximate the log bulk partition function well, and the approximation improves as \(k\) increases, i.e., when we ignore more top words. This validates our "anchor vector" hypothesis.

The existence of anchor vector is quite surprising. Although there are some settings where the partition function can become easy to predict, e.g., when the word embeddings are uniformly distributed on a sphere, we have empirically shown that the large-scale language models

Figure 1: Empirical verifications of “anchor vector” hypothesis

do not fall under these settings. Figure 0(b) shows that the word embeddings are far from uniformly distributed on a sphere, and Figure 0(c) indicates that the values of log partition functions and log bulk partition functions vary greatly for different samples. More experiments and discussions are provided in Section F.

## 5 Anchor vector guarantees performance transfer from pre-training to downstream tasks

Based on the counterexamples in Section 3 and the observations in Section 4, we now give sufficient conditions on the downstream tasks so that the downstream classification accuracy provably benefits from pre-trained representations.

### Model and assumptions

For huge language models, the anchor vector hypothesis states that there exists a vector \(v_{0}^{*}\) such that its inner product with the hidden state \(v_{-i}^{*}(x_{-i})\) well approximates the logarithm of the bulk partition function. Therefore, we can use \(v_{0}^{*}\) as an anchor to handle the shift invariance problem of the softmax function, i.e., we want to subtract \( v_{-i}^{*}(x_{-i}),v_{0}^{*}\) from \( v_{-i}^{*}(x_{-i}),v_{j}^{*}\). As a result, we modify our model for the downstream task to be

\[f^{*}(x,i):=_{j=1}^{k}a_{j}^{*}( v_{-i}^{*}(x_{-i}),v_{j}^{*} -v_{0}^{*}-b_{j}^{*}).\] (7)

The student model is also modified accordingly:

\[f(x,i):=_{j=1}^{k}a_{j}( v_{-i}(x),v_{j}-v_{0} -b_{j}).\] (8)

Note that in the above model we have already adapted Assumption 1 which was originally stated for the linear model. Therefore, we also update the assumption and restate it below:

**Assumption 4** (At most \(k\) words of interest).: _Assume there are at most \(k\) vectors in \(\{v_{j}^{*}\}_{j=1}^{n}\) whose logits are relevant to the downstream task and WLOG assume these \(k\) vectors are \(\{v_{1}^{*},,v_{k}^{*}\}\). In other words, we assume there exist coefficients \(\{a_{j}^{*}\}_{j=1}^{k}^{k}\) such that \(f^{*}(x,i):=_{j=1}^{k}a_{j}^{*}( v_{-i}^{*}(x_{-i}),v_{j}^{*} -v_{0}^{*}-b_{j}^{*})\)._

We are still assuming the teacher-student setting and the log-linear word production model for the word probabilities, as restated below:

\[p^{*}(x_{i}=j|x_{-i})=^{*}(x),v_{j}^{* })}{Z^{*}}, p(x_{i}=j|x_{-i})=(x),v_{j} )}{Z}.\] (9)

Under the modified model defined above, we will make some additional assumptions. As noticed in the experiments, the log bulk partition function (as defined in Definition 1) can be linearly approximated by the hidden state. Normally, the bulk only contains words that are not related to the downstream task, and every single word usually has a small probability, but the total probability of the bulk words is not negligible, as reflected in the following two assumptions. Note that the set of bulk words can be strictly contained in the complement of the set of words of interest. This is useful especially when we are not certain about which words are important for the downstream task.

**Assumption 5** (Bulk contains no words of interest).: _For all \(x\) and \(i\), \(B(x,i)\{1,,k\}=\)._

**Assumption 6** (Lower bound of bulk probability).: _For all \(x\) and \(i\),_

\[}^{*}(x,i)}{Z^{*}(x,i)} p_{b}.\]

The "anchor" vector \(v_{0}^{*}(i)\) is used to handle the instability of the partition function, and we formalize our anchor vector hypothesis as the following assumption:

**Assumption 7** (Linear approximation of log bulk partition function).: _There exist \(v_{0},v_{0}^{*}^{d},_{b}^{+}\), s.t.,_

\[ x,\{| v_{-i}^{*}(x),v_{0}^{*}- Z_{}^{ *}(x,i)|,| v_{-i}(x),v_{0}- Z_{}(x,i)|\} _{b}.\]

_Furthermore, we assume \(_{b}|a_{j}^{*}|}\)._

For notational simplicity, we will use all the notations (including \(f^{*}\), \(f\), \(v_{0}^{*}\), and \(v_{0}\)) without \(i\) when the selection of \(i\) is clear from the context.

### Main theorem and interpretations

In our model, the ground-truth function \(f^{*}\) contains \(k\) terms with coefficients \(\{a_{j}^{*}\}_{j=1}^{k}\) and we define the margin \(\) as the margin for \(f^{*}\). If we scale \(\{a_{j}^{*}\}_{j=1}^{k}\) or increase \(k\) by adding duplicated terms to \(f^{*}\), we can scale \(\) arbitrarily without changing the pre-training performance or the downstream prediction of the student model. To construct a quantity that better indicates the difficulty of the downstream task, we introduce the following definition of the normalized margin that is invariant to the scaling of \(k\) and \(\{a_{j}^{*}\}_{j=1}^{k}\):

**Definition 2**.: _The normalized margin is defined as \(:=|a_{j}^{*}|}\)._

Then we are ready to state our main result. The proof and further discussions for this theorem will be provided in Section B.

**Theorem 3**.: _Let \(_{}:=_{x_{pre}}[D_{}(p^ {*}(x)||p(x))]\) be the pre-training loss, and \(_{CLS}:=_{x_{DS}}[f(x) f^{*}(x)<0]\) be the downstream classification error rate, where \(_{pre}\) and \(_{DS}\) are the input distributions of pre-training and downstream data. Under Assumptions 2-7, further assuming \(_{j[k]}b_{j}^{*}_{b}-(4k)\) and \(8_{b}<<6\), there exists a set of parameters \((a_{j})_{j=1}^{n},(b_{j})_{j=1}^{n}\) such that_

\[_{CLS}_{}^{2} ^{2}}.\] (10)

This theorem shows that when an "anchor vector" exists, we can upper bound the downstream classification error by the KL divergence of the student model during pre-training. This upper bound becomes smaller when the distribution of downstream input is close to that of pre-training, the bulk probability is non-negligible, and the normalized margin of the ground-truth classifier is large. Here we discuss these ways to decrease the upper bound and their corresponding intuitions.

**Large \(\).**\(\) is larger when the data distributions of pre-training and the downstream task are closer, which helps with the performance transfer.

**Large \(p_{b}\).** The bulk probability \(}^{*}}{Z_{}^{*}}\) is usually at least a constant in practice. When the bulk probability becomes larger, the anchor vector plays a more important role in the partition function and the downstream task.

**Large \(\).** A larger normalized margin makes it harder for the student model to make mistakes in downstream prediction.

**Remark 1**.: _For ease of presentation, we are making additional assumptions in Theorem 3, e.g., \(_{j[k]}b_{j}^{*}_{b}-(4k)\) and \(8_{b}<<3\). More general cases are covered in Lemma 1. We also discuss potential ways to improve the bound in Section B.1._

## 6 Conclusions, limitations and future work

In this paper, we analyzed when and how the representations generated by pre-trained large-scale language models can be used in downstream classification tasks. We found two necessary conditions for guaranteeing the usefulness of these representations: the insensitivity of the downstream task to super-small probability words, and the underlying structure in the representations to handle the shift-invariance of softmax. We also provide a sufficient condition, i.e., the existence of an anchor vector, that can guarantee the representations from pre-trained language model to help with downstream tasks. We verify this existence empirically in various large language models and believe that this is an important reason why recent large-scale language models can adapt to different downstream tasks.

We made some assumptions to provide theoretical understanding on how representations can be helpful for downstream applications, and these assumptions may not always hold in practice. For instance, some downstream tasks require multiple rounds of reasoning and do not rely on a small set of words, and the "anchor vector" hypothesis can be weaker for some models such as OPT-350M and GPT-2 Medium. Relaxing these assumptions is a meaningful future direction.

While our work showed the existence of the anchor vector, it remains unclear why this vector exists in most large language models. It might be related to the initialization, optimization, and structure of the neural networks, especially transformers, and it could also be related to the underlying structure of the training data. Digging deeper into this may reveal more fundamental properties of these models. Our analysis of downstream tasks focuses on classification tasks. Since these large networks perform well in various types of downstream tasks, another future direction would be to analyze other kinds of downstream tasks.

We only consider using the last layer representation without fine-tuning the whole model, which is usually weaker in performance. Furthermore, we model the network before the softmax function as a black box, ignoring its inner structure. Further opening up this black box and consider fine-tuning the whole model are important for deeper understanding of the structures in the learned representations from pre-training.