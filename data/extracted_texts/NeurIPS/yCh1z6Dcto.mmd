# Stepping Forward on the Last Mile

Chen Feng

Qualcomm AI Research

Qualcomm Canada ULC

chenf@qti.qualcomm.com &Shaojie Zhuo

Qualcomm AI Research

Qualcomm Canada ULC

shaojiez@qti.qualcomm.com &Xiaopeng Zhang

Qualcomm AI Research

Qualcomm Canada ULC

xiaopeng@qti.qualcomm.com &Ramchalam Kinattharkara Ramakrishnan

Qualcomm AI Research

Qualcomm Canada ULC

rkinatti@qti.qualcomm.com &Zhaocong Yuan

Qualcomm AI Research

Qualcomm Canada ULC

zhaocong@qti.qualcomm.com &Andrew Zou Li

University of Toronto

andrewzou.li@mail.utoronto.ca

Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.

###### Abstract

Continuously adapting pre-trained models to local data on resource constrained edge devices is the _last mile_ for model deployment. However, as models increase in size and depth, backpropagation requires a large amount of memory, which becomes prohibitive for edge devices. In addition, most existing low power neural processing engines (e.g., NPUs, DSPs, MCUs, etc.) are designed as fixed-point inference accelerators, without training capabilities. Forward gradients, solely based on directional derivatives computed from two forward calls, have been recently used for model training, with substantial savings in computation and memory. However, the performance of quantized training with fixed-point forward gradients remains unclear. In this paper, we investigate the feasibility of on-device training using fixed-point forward gradients, by conducting comprehensive experiments across a variety of deep learning benchmark tasks in both vision and audio domains. We propose a series of algorithm enhancements that further reduce the memory footprint, and the accuracy gap compared to backpropagation. An empirical study on how training with forward gradients navigates in the loss landscape is further explored. Our results demonstrate that on the last mile of model customization on edge devices, training with fixed-point forward gradients is a feasible and practical approach.

## 1 Introduction

On-device training allows pre-trained models to be continuously adapted to newly collected personal data after deployment. Moving model training from the cloud to local devices is essential for model customization and protecting users' privacy (Moon et al. (2024)). However, the constraint on power and memory makes training on edge devices extremely challenging (Dhar et al. (2019)). Traditional backpropagation involves a forward step, which computes activations given an input, and a backward step which computes the gradients. Intermediate activation values must be stored in memory prior tothe gradient of a certain layer is computed (Baldi and Sadowski (2016)). As models increase in size and depth, this process requires a prohibitive amount of memory for most existing edge devices.

To avoid large memory consumption, recent studies have re-examined the procedure of computing _Forward Gradients_ as an alternative to standard backpropagation (Fournier et al. (2023)). As introduced by Baydin et al. (2022), a forward gradient, computed through a random, isotropic directional derivative, is an unbiased approximation of a weight gradient. Forward gradients can be further estimated solely with two forward calls of a neural network (Liu et al. (2020)), which saves computation and memory substantially. The work of MeZO (Malladi et al. (2023)) applies forward gradients on fine-tuning Large Lanugage Models (LLMs), and shows a success on diverse downstream tasks, with the same memory footprint as inference.

Despite the aforementioned benefits, forward gradients may encounter the curse of dimensionality as the size of trainable parameters increases. Gradient approximations from two forward calls may be noisy and with large variance (Ren et al. (2023)), resulting in less effective training of large networks. Moreover, most existing low power neural processing engines (e.g., NPUs, DSPs, MCUs, etc.) are designed as efficient fixed-point inference accelerators. The feasibility of utilizing fixed-point forward gradients for quantized training remains uncertain. Our goal is to gain deeper insights into whether training with fixed-point forward gradients can still result in competitive models while preserving the memory and computation benefits. To answer the question, we conduct comprehensive experiments across a variety of deep learning benchmark tasks in both vision and audio domains. A series of algorithm enhancements are proposed to further reduce the memory footprint, and accuracy gap compared to backpropagation. We believe our study to be of high interest in making model personalization happen locally on edge devices.

**Contributions.****(a)** We formulate the computation of forward gradients in the quantized space. Weight perturbations and gradient calculations are all in fixed-point precision during model training or adaptation (see Figure 1 and Section 3). **(b)** We demonstrate the feasibility of on-device training with fixed-point forward gradients, through comprehensive experiments across a variety of deep learning benchmark tasks in both vision and audio domains. Although the method is model architecture agnostic, the experiments cover most typical model types (e.g., CNN, RNN, ViT-based) and parameter sizes (\(100\)K to \(80\)M). **(c)** We propose a series of algorithm enhancements that further reduce the memory footprint and accuracy gap compared to backpropagation, leading to a practical solution for model adaptation on edge devices. **(d)** Finally, we visualize the neural loss landscape and trajectories of training with forward gradients, and show its dynamics and characteristics.

## 2 Related Work

### Memory Efficient Training through Backpropagation

With an increasing number of applications using large neural networks on device, there is a demand of moving model training from the cloud to local devices. However, the key bottleneck for efficient on-device training is the limitation of memory resources. For example, training a simple Convolutional Recurrent model (CRNN, Keren and Schuller (2017)) with a parameter size of \(250\)kB, requires \(11.5\)MB (\(46\)) memory to store activations. Training memory is primarily attributed to activations

Figure 1: An overview of fixed-point forward gradient learning. The pipeline includes quantized weights perturbation, quantized forward gradient calculation through two forward calls with perturbed weights, and quantized weights update. Each process is explained in details in section 3.3.

rather than parameters. Studies on algorithms to reduce resource consumption during training have been published, with a trade-off between memory usage and model accuracy. Parameter-efficient fine-tuning techniques such as LoRA (Hu et al. (2021)) and prefix tuning (Li and Liang (2021)) are proposed to train a model with reduced parameters. Dynamic sparse representation (Mostafa and Wang (2019)) is proposed to reduce memory requirements by making the weight and activation values sparse during training. Low precision training (Micikevicius et al. (2018)) reduces model sizes and computation requirements by adopting \(16\)-bit float precision instead of \(32\)-bit. The work of Lin et al. (2022) pushes conventional convolutional neural network training on devices with only \(256\)kB by pruning the training graph during compilation time. These methods mainly focus on reducing the trainable parameters or activation sizes, thus reduce the peak memory required for training a neural network. However, due to the inherent nature of backpropagation, intermediate activations across all layers must be retained until loss is backpropagated and gradients are calculated. Therefore, as models increase in size and depth, parameter-efficient techniques do not fundamentally resolve the training memory problem.

### Forward Gradients through Zeroth-order Optimization

Forward gradient has been recently brought to attention by Baydin et al. (2022) and Silver et al. (2022), which showed that gradients can be computed solely based on the directional derivatives using the forward mode of auto-differentiation only. The forward gradients can be estimated via two forward calls using zeroth-order optimization (Liu et al. (2020)) by incorporating random perturbations on weights, entirely eliminating the need for backpropagation in gradient descent. The work of Ren et al. (2023) shows that it is possible to substantially reduce the variance of the forward gradient estimation by applying perturbations to activations rather than weights. Considering the memory required for storage of intermediate activations, only weight-perturbed forward gradient estimator can be deployed on low resource constrained devices. While research by Belouze (2022) claimed shortcomings of forward gradients in high dimensions, the work of MeZO (Malladi et al. (2023)) proposes a contradictory perspective by showing the lower bounds of such zeroth-order optimization is conditioned on loss landscape instead of number of trainable parameters. MeZO further applies forward gradients on fine-tuning LLMs, and shows a success on diverse downstream tasks.

### Quantized Training and Quantized Gradients

There is limited literature on gradient computation in the quantized space. Quantization-aware training (QAT Nagel et al. (2021)) has been widely used to simulate the potential quantization loss in the training stage. However, most existing low power neural processors (e.g., NPUs, DSPs, MCUs, etc.) are designed and optimized for fixed-point inference. Direct training in the quantized space will fundamentally bridge the gap between training and inference, thus being essential for model adaptation on edge devices. However, the work of Lin et al. (2022) observed that the quantization process distorts backward gradients, resulting in significantly lower accuracy in model training through backpropagation. Quantization-aware scaling (QAS) is proposed to address this problem. It remains uncertain whether training with quantized forward gradients through zeroth-order optimization can still lead to competitive models on device, while preserving the memory and computation benefits.

## 3 Quantized Forward Gradient Learning

Forward gradients utilize directional derivatives to bypass backpropagation, while retaining unbiased estimations of true gradients. In the following, we first review the technique of forward-mode autodifferentiation (AD Baydin et al. (2022)), alongside a practical implementation known as Simultaneous Perturbation Stochastic Approximation (SPSA) for zeroth-order gradient estimation (Spall (1992)). We then propose sign-m-SPSA, a variant of SPSA to alleviate the noisy component of forward gradients estimated by SPSA, which leads to stable performance in many use cases. Once the gradients are estimated, optimizers such as SGD, Adam etc. can be applied to update the weights. Finally, we formulate the Quantized Zeroth-order Forward Gradient (QZO-FF) estimator, mapping the processes of weights perturbation, gradients estimation and weights update in the fixed-point space. An overview of the QZO-FF algorithm is illustrated in Algorithm 1.

### Forward Gradients

**Definition 1** (Forward Gradients).: Consider a machine learning function \(f(w):^{n}\), where \(w^{n}\) is the trainable parameters that the gradients are evaluated. _Forward gradients_\(g:^{n}^{n}\) is defined as:

\[g(w)=( f(w) z)z\] (1)

where \(z^{n}\) is a perturbation vector taken as multivariate random variable \(z p(z)\) such that \(z^{}s\) scalar components \(z_{i}\) are independent and have zero-mean and unit variance for all \(i\). \( f(w) z\), the Jacobian matrix-vector product, defines the directional derivative of \(f\) at point \(w\) in direction \(z\).

### Zeroth-order Optimization

In order to have runtime advantage over backpropagation, a classical zeroth-order estimator, SPSA can be used to estimate the forward gradients by evaluating \(f\) in forward path \(m\) times, where \(m n\).

**Definition 2** (SPSA).: Given a model \(f\) with parameters \(w^{n}\) and a loss function \((w)\), SPSA estimates the gradient as:

\[(w)=(w+ z)-(w- z)}{2}z\] (2)

where \(z(0,_{n})\) is a weighted vector over all parameter dimensions, randomly sampled from normal distribution with zero-mean and standard deviation. The perturbation scale \(\) is a small constant value (e.g., \(1e-3\)). For each sampled \(z\), SPSA only requires two forward calls through the model, with positive and negative perturbed weights respectively, to estimate the gradients.

Gradient magnitude defined in (2) is determined by loss difference of two forward calls based on a random perturbation applied on weights, which easily becomes noisy. Inspired by many popular optimizers, such as sign-SGD and RMSProp (Bernstein et al. (2018)), updating weights through a sign-based method achieves good practical performance for many gradient compression use cases. In order to mitigate the noisy component of forward gradients estimated by SPSA, we propose sign-m-SPSA by only taking the direction of loss difference under a certain perturbation, while disregarding the magnitude component. The estimation can be improved by averaging \((w)\) over \(m\) randomly sampled \(z\) (\(m n\)), with an increased number of training iterations.

**Definition 3** (Sign-m-SPSA).: \[(w)=_{i=1}^{m}sign((w+ z_{i})-(w- z_{i}))z_{i}\] (3)

The intuition behind sign-m-SPSA is that during the training, the estimator samples a random perturbation direction \(z_{i},i\{1,..,m\}\), and tests how it aligns with the true gradient by examining the loss change, and then multiplies the alignment direction with the perturbation direction. Weights will be updated along the sampled direction that leads to a decrease in loss. This design is also quantization-friendly, constraining the range of gradient values to be the same as perturbation for static quantization. Our later experiments show that \(8\)-bit quantization of perturbation and forward gradient is sufficient for preserving the model accuracy across many use cases.

**Definition 4** (Sign-m-SPSA-SGD).: With \((w)\) as the forward gradients estimated through sign-m-SPSA, similar to backpropagation, an optimizer such as SGD with learning rate \(\) can be used to update model parameters:

\[w_{t+1}=w_{t}-(w)\] (4)

### Quantized Weights Perturbation and Forward Gradients

Sign-m-SPSA in (3) estimates forward gradients through a minimum of two forward calls, with positive and negative perturbed weights in float precision, respectively. For low power devices with fixed-point computation engines, model weights are quantized in low bit precision. Therefore, the random perturbation needs to be quantized prior to apply on weights.

For a given model, consider \(w\) as the floating point weights of a certain layer. Assume model is per-tensor quantized with symmetric quantization in \(b\)-bit, the quantized weights can be represented by:

\[w_{q}=}\] (5)where \(_{w}\), denoted as the quantization scaling factor, is calculated by \(_{w}=w_{max}/(2^{b-1}-1)\), where \(w_{max}\) is the maximum absolute value in \(w\) found by a quantization method (i.e., TF, MSE, AdaRound, etc., Nagel et al. (2021)). \(\) represents for the rounding operation.

**Quantized Perturbation.** With the given quantization method in 5, the quantized weights perturbation can be defined and calculated as:

\[ w z&=w 1.0 z \\ &_{w}w_{q}_{z}_{q}_{w} _{q}_{z}z_{q}\\ &=_{w}_{z}(w_{q}_{q}_{q}  z_{q})}{{}}_{w}  w_{q^{}}\] (6)

Since weights \(w\) and perturbation \(z\) have different quantization scaling factors and possibly different bit-width used, we quantize \(1.0\) with the scaling factor of \(z\), and quantize \(\) with the scaling factor of \(w\), prior to direct adding the quantized values in accumulator. \(_{q}=}{_{z}}\), represents for the quantized value of floating point \(1.0\) with \(_{z}\) as its scaling factor. Similarly, \(_{q}=}\), represents for the quantized value of \(\) with \(_{w}\) as its scaling factor.

The random perturbation vector \(z\) is sampled from normal distribution with zero-mean and standard deviation \((0,_{n})\), we can use static quantization with a pre-determined \(z_{max}\) to pre-calculate \(_{z}\). For example, in the case of \(z_{max}=3.5\), with \(8\)-bit symmetric quantization, \(_{z}=0.0276\), and \(_{q}=36\). Similarly, \(_{q}\) can be pre-calculated, if a pre-trained model with \(w_{max}\) is given. It is noted that \(\) is a very small value (e.g., \(1e-3\)). Therefore, we require \(16\)-bit to be used for weight quantization, such that \(\) can be properly represented by the minimum representation power of \(_{w}\) without clipping loss, and small perturbation can be reflected on the weights change in the quantized space.

In (6), two values with \(16\)-bit and \(8\)-bit are multiplied, and then fed to a quantized add/subtract operation. In hardware, a \(32\)-bit accumulator is used to hold the result. The result is then re-quantized to \(16\)-bit by a multiply and a shift operation through a post-processing block (Appendix A), using the original weight scaling factor \(_{w}\). The quantized perturbed weights are denoted as \((_{w},w_{q^{+}})\) and \((_{w},w_{q^{-}})\). The above formulation is derived under per-tensor quantization, however, per-channel quantization can be similarly derived with finer granularity.

**Quantized Forward Gradients.** Based on the quantization method in (5), quantized forward gradients, estimated from sign-m-SPSA, can be calculated as:

\[_{f}&=_{i=1}^{m }sign((w+ z_{i})-(w- z_{i}))z_{i}\\ &_{i=1}^{m}sign((w_{q^{+}})- (w_{q^{-}}))_{z}z_{q}\\ &=_{z}g_{q}\] (7)

where \(g_{q}\) represents for the quantized gradients, and it is using the same quantization scaling factor and bit-width as perturbation vector \(z\).

**Quantized Weights Update.** We can further quantize the learning rate \(\) to a quantized value of \(1\), using quantization scaling factor of \(_{}=\). Finally, quantized weights update can be computed by:

\[ w_{t+1}&=w_{t}-_{f}\\ &_{w}w_{q}-_{}1_{z}g_{q}\\ &_{w}w_{q}-_{w} _{z}}{_{w}}g_{q}\\ &=_{w}(w_{q}-_{q})\] (8)

where \(_{q}\) represents for the change of weights in the quantized space, with \(_{w}\) as the re-quantized scaling factor (Appendix A). \(_{}\) can be pre-calculated. In our experiments, we find that it is important to keep weights in \(16\)-bit, while the perturbation \(z\) and gradient \(g\) can be in \(8\)-bit representations.

### QZO-FF enhancement

**Momentum Guided Sampling.** Besides naive SGD, quantized forward gradient learning can also be combined with other optimizers such as Adam or SGD with momentum, with slight overhead to store the gradient history. Similarly, by allocating additional memory to store the perturbation history, momentum can be used to guide the sampling process. Instead of sampling solely from a zero-centered Gaussian distribution, perturbations are computed from a combination of a momentum-centered and a zero-centered Gaussian distribution. Mathematically, \(z_{1}(0,_{n}*)\), \(z_{2}(z_{t},_{n}*)\), and \(z_{t+1}=*z_{1}+(1-)*z_{2}\). Here, \(\) is a smoothing parameter; \(\) and \(\) can be adaptively adjusted during training. For example, during the initial training stage, random perturbations are applied with \(=1\). As training progresses, a history of the momentum \(z_{t}\) is incorporated to guide the new sampling process.

**Sharpness-aware Perturbation.** Motivated by the connection between sharpness of the loss landscape and model generalization, we can perturb parameter values from its neighborhood location. This is done by performing an additional step of directional gradient ascent through parameter perturbation and loss evaluation, prior to QZO-FF, as illustrated in Figure 2. This process helps to prevent the model from converging to a sharp minimum.

**Sparse Update.** To further reduce memory consumption, the forward gradient learning can be combined with a sparsity algorithm such that only a subset of the weights are selected from the network for updating. Examples of sparsity algorithm may include pruning by top-\(k\) magnitude, randomized pruning, pruning values beyond a specified threshold, to determine the importance of the weights. Our experiments show that incorporating sparsity with forward gradient learning allows for a \(90\%\) reduction in the size of trainable parameters, with only minor decrease in accuracy, as well as slight improvement in convergence speed.

**Kernel-wise Normalization.** In (3), forward gradients are estimated through sign-m-SPSA. In addition, we can also apply a kernel-wise normalization to scale the gradient adaptively. \(z\) is normalized by the norm of \(w\) in each layer.

\[(w^{i})=sign((w+ z)-(w- z))z^{i}/ \|z^{i}\|\|w^{i}\|\] (9)Experiments

### Few-shot learning

We first apply forward gradient learning in the setting of few-shot learning, targeting to adapt a base-learner to a new task for which only a few labeled samples are available. Experiments across a variety of challenging few-shot learning benchmarks in both vision and audio domains are explored. Models are trained for each dataset individually and then evaluated with the corresponding test split.

To address whether forward gradient learning (FF) could match the performance of backpropagation (BP), we explore classification tasks on training models with full fine-tuning (FT) and linear probing (LP), utilizing float16 (fp16) precision. Training accuracy with quantized FF (16-bit weights and 8-bit activations, 16w8a) is also evaluated and compared with that of fp16 precision. Details and analysis on memory usage during training are reported in Appendix B - E.

**Vision Benchmark.** Image classification models are compared across commonly used \(5\) few-shot learning benchmark datasets (Table 1). Training methods are evaluated on \(3\) network backbones (modified Resnet12 Ye et al. (2020), Resnet18 He et al. (2015) and ViT tiny Dosovitskiy et al. (2020)), with ProtoNets Snell et al. (2017) as few-shot classifier.

Table 2 demonstrates the classification accuracy on vision benchmarks. We first show that FF significantly improves over zero-shot performance across model types and tasks. Given that FF solely utilizes directional derivatives for gradient estimation, it is expected that BP generally outperforms FF in most tasks. The accuracy gap between BP and FF can vary based on factors such as backbone architecture, dataset and task difficulty. The largest accuracy degradation is observed when training Resnet12 on Cifar-100 dataset with an input resolution of \(32 32\). However, using a stronger backbone such as ViT, can help bridge this accuracy gap. This indicates that while FF may show some degradation with smaller architectures and low-resolution inputs, performance improvements can be achieved with more advanced models. Overall, FF achieves comparable performance (accuracy within \(5\%\)) to BP in \(26\) out of \(30\) comparable experiments. A minimal accuracy drop is observed in quantized FF training, when a strong backbone such as ViT tiny is used. These promising results

   Name & Setting & No. Classes (train/val/test) & No. Samples & Resolution \\  CUB & Bird Species & 200 (140/30/30) & 11,788 & 84\(\) 84 \\ Omniglot & Handwritten characters & 1623 (100/000/200423) & 32,460 & 28\(\) 28 \\ Cifar100\_fs & Color & 100 (64/16/20) & 60,000 & 32\(\) 32 \\ miniImageNet & Natural images & 100 (64/16/20) & 60,000 & 84\(\) 84 \\ tieredImageNet & Natural images & 608 (351/97/160) & 779,165 & 84\(\) 84 \\   

Table 1: Vision datasets used for few-shot learning

   Backbone & Training & CUB & Omniglot & Cifar100\_fs & minimizeNet & tieredImageNet \\   & Zero-shot & 68.46 & 92.00 & 60.44 & 84.44 & 80.92 \\   & BP. FT & **85.32** & **99.62** & **82.32** & 87.34 & **82.54** \\  & BP, LP & 84.14 & 98.64 & 72.42 & **87.46** & 81.96 \\   & FF, FT & 80.58 (+3.42) & 97.44 (+1.18) & 71.24 (+1.00) & 87.36 (+0.02) & 82.12 (+4.43) \\  & FF, LP & 79.02 (+1.21) & 96.62 (+2.76) & 70.32 (+1.22) & 87.30 (+0.11) & 82.22 (+0.02) \\  & FF, LP, Quant & 77.42 & 96.08 & 68.54 & 87.00 & 81.64 \\    & Zero-shot & 59.96 & 86.68 & 74.60 & 82.58 & 80.44 \\   & BP, FT & **79.28** & **98.54** & **86.34** & 86.96 & **86.78** \\   & BP, LP & 78.92 & 96.48 & 84.88 & 87.42 & 84.68 \\   & FF, FT & 76.34 (+1.54) & 94.70 (+3.54) & 82.20 (+1.54) & **87.66 (+0.30)** & 85.88 (+0.98) \\  & FF, LP & 73.64 (+2.52) & 95.56 (+4.95) & 82.32 (+2.59) & 87.14 (+0.32) & 83.02 (+1.69) \\  & FF, LP, Quant & 70.54 & 95.86 & 74.92 & 85.74 & 81.00 \\    & Zero-shot & 90.60 & 90.96 & 82.28 & 98.78 & 94.30 \\   & BP, FT & 93.08 & **99.88** & **90.88** & 98.46 & **96.04** \\   & BP, LP & 93.90 & 95.78 (+0.98) & 84.42 & 98.40 & 95.32 \\   & FF, FT & **93.58 (+0.98)** & 96.96 (+1.52) & 88.66 (+1.22) & **99.08 (+0.05)** & 95.50 (+0.46) \\   & FF, LP & 92.26 (+1.14) & 95.00 (+7.78) & 84.48 (+0.46) & 99.02 (+0.15) & 95.18 (+1.46) \\   & FF, LP, Quant & 92.24 & 95.04 & 84.40 & 99.00 & 95.18 \\  

Table 2: Vision tasks: few-shot learning accuracy (%) with Forward (FF) and Backward (BP) gradients. The averaged accuracy over \(100\) testing tasks is reported. FT: full fine-tuning; LP-; linear probing; Quant: 16w8a with symmetric quantization. FF outperforms zero-shot across the board, and achieves comparable performance (accuracy within \(5\%\)) to BP on \(26\) out of \(30\) tasks.

[MISSING_PAGE_FAIL:8]

boosts the accuracy by \(1.22\%\). However, there is a trade-off between model accuracy and training efficiency.

**Quantization bit-width.** Experiments show that \(8\)-bit weights quantization (8w8a) does not lead to model convergence. Therefore, \(16\)-bit weights quantization is necessary to capture the small perturbation, while the perturbation \(z\) and gradients can use 8-bit.

**Perturbation sampling.** The random perturbation \(z\) in Equation (2) is sampled from a normal distribution with zero-mean and standard deviation \((0,_{n})\). Other distibutions, such as Binomial distribution, also works well for forward gradient learning.

**QZO-FF enhancement.** FF can be extended with sharpness-aware scheme, where a perturbation is performed at a neighborhood location through an extra step of gradient ascent. Together with kernel-wise normalization, this technique results in the closest performance to BP in both training methods. Although obtaining the norm of weights involves a trade-off between computation and accuracy, efficient implementations using _gemm_ and _sqrt_ operations can minimize the overhead on hardware.

**Loss landscape.** It is believed that the convergence and generalization property of perturbation-based learning, such as forward gradient learning, depends on the loss landscape instead of number of parameters. Visualization of loss landscape has the potential to help us answer several important questions about how a neural network is trained, and why do the resulting minima generalize under certain training approach. Utilizing the tool provided in Li et al. (2018), we show the 2D contours of loss landscape of ViT tiny network under the task of cross-domain adaptation, together with the loss trajectory during training, providing an empirical characterization of neural loss functions, and exploring how training with forward gradients navigates in the loss landscape (See Appendix E).

### In-domain OOD Adaptation

On-device model adaptation often involves fine-tuning on data that is out-of-distribution (OOD). To evaluate the performance of FF, we pretrain a ViT tiny backbone on Cifar10, and fine-tune the decoder on Cifar10-C (Hendrycks and Dietterich (2019)), where \(15\) types of corruptions, such as Gaussian noise or pixelation, of varying severity are applied. We take the lowest (easy), middle (medium), and highest (hard) corruption severity from the dataset as separate benchmarks for fine-tuning. Fine-tuning techniques include LP with 1 linear decoder layer, LP with 3 linear decoder layers, and D-VPT (Jia et al. (2022)). Additionally, we explore the impact of sparsity by pruning \(90\%\) of the trainable parameters using a zero-order method (Chen et al. (2024)). Table 5 shows a comparison of accuracy on the test set between BP, FF, quantized FF and Sparsed FF, alongside different fine-tuning methods. Detailed training hyper-parameters are listed in Appendix D.

As the number of trainable parameters increases, forward gradient learning improves the model accuracy on OOD dataset. Even with a sparsity level of \(90\%\), FF can still achieve comparable accuracy levels to those of BP. The largest accuracy disparity between the two is \(6.98\%\), observed on the Cifar10-C (hard) category using the LP method for \(3\) decoder layers. As corruption intensifies,

Figure 2: Ablation studies on cross-domain adaptation. The accuracy numbers (with standard deviation) are averaged over 5 runs.

the loss surface becomes less smooth, potentially causing FF to be impacted more from the noisy gradient estimation.

## 5 Conclusion

Continuously updating pre-trained models to local data on the edge is the last mile for model adaptation and customization. To overcome the memory limitation of most existing low power devices, forward gradients are used for model adaptation. We have formulated the forward gradient learning in the quantized space, where weight perturbations and gradient calculations are all in fixed-point during model training. To investigate the feasibility of on-device training with fixed-point forward gradients, we have extensively conducted experiments across a variety of deep learning benchmark tasks in both vision and audio domains. Model adaptation to cross-domain dataset and in-domain OOD datasets are further evaluated and analyzed.We further explore 2D contours of loss landscape, together with loss trajectory during training, providing an empirical explanation on how the model is trained. We have shown that quantized forward gradient learning with 16w8a can effectively adapt most typical model architectures (e.g., Resnet, ViT-tiny, CRNN, AST) and scales. With minimum accuracy reduction, fixed-point forward gradients allows model adaptation using the same memory footprint and operation support as inference, as opposed to backpropagation. Therefore, it has the potential to enable model fine-tuning on existing edge devices with limited memory and backpropagation support, without requiring additional hardware adaptation.

    & Training & Cifar10-C (easy) & Cifar10-C (median) & Cifar10-C (hard) \\   & Zero-shot & 82.48 & 74.59 & 62.40 \\   & BP & 83.75 \(\) (0.67) & 77.88 \(\) (0.85) & 70.03 \(\) (1.20) \\  & FF & 83.37 \(\) (0.60) & 77.04 \(\) (0.66) & 68.65 \(\) (0.70) \\  & FF, Sparse & 83.34 \(\) (0.59) & 77.11 \(\) (0.68) & 68.63 \(\) (0.95) \\  & FF, Quant & 83.23 \(\) (0.57) & 76.73 \(\) (0.75) & 68.28 \(\) (0.87) \\   & Zero-shot & 85.83 & 77.77 & 62.25 \\   & BP & 86.99 \(\) (0.41) & 81.57 \(\) (0.78) & 74.76 \(\) (0.90) \\  & FF & 86.11 \(\) (0.59) & 79.17 \(\) (0.70) & 67.78 \(\) (0.72) \\  & FF, Sparse & 86.10 \(\) (0.58) & 79.24 \(\) (0.63) & 68.06 \(\) (1.11) \\  & FF, Quant & 85.77 \(\) (0.55) & 78.67 \(\) (0.63) & 67.25 \(\) (0.42) \\   & Zero-shot & 89.52 & 82.24 & 68.95 \\   & BP & 91.66 \(\) (0.50) & 88.90 \(\) (0.46) & 84.54 \(\) (0.42) \\   & FF & 90.58 \(\) (0.53) & 86.21 \(\) (0.49) & 78.38 \(\) (0.80) \\   & FF, Sparse & 90.56 \(\) (0.48) & 86.18 \(\) (0.51) & 78.24 \(\) (0.81) \\    & FF, Quant & 90.41 \(\) (0.49) & 85.77 \(\) (0.43) & 77.45 \(\) (0.64) \\   

Table 5: Accuracy (%) of model adaptation to in-domain OOD dataset with Forward (FF) and Backward (BP) gradients. I. LN: linear layer of decoder, 3.N: 3 linear layer of decoder, Quant: 16w8a. Sparse: 90% weights pruned. The accuracy numbers (with standard deviation) are averaged over 5 runs.