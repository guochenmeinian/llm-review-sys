# PureGen: Universal Data Purification for Train-Time Poison Defense via Generative Model Dynamics

Sunay Bhat

sunaybhat@ucla.edu

&Jeffrey Jiang

jeffrey.jiang@ucla.edu

&Omead Pooladzandi

opooladz@caltech.edu

&Alexander Branch

alexrbranch@ucla.edu

&Gregory Pottie

pottie@ee.ucla.edu

Affiliation for all authors: University of California, Los AngelesEqual ContributionsGitHub: https://github.com/SunayBhat1/PureGen_PoisonDefense.

###### Abstract

Train-time data poisoning attacks threaten machine learning models by introducing adversarial examples during training, leading to misclassification. Current defense methods often reduce generalization performance, are attack-specific, and impose significant training overhead. To address this, we introduce a set of universal data purification methods using a stochastic transform, \((x)\), realized via iterative Langevin dynamics of Energy-Based Models (EBMs), Denoising Diffusion Probabilistic Models (DDPMs), or both. These approaches purify poisoned data with minimal impact on classifier generalization. Our specially trained EBMs and DDPMs provide state-of-the-art defense against various attacks (including Narcissus, Bullseye Polytope, Gradient Matching) on CIFAR-10, Tiny-ImageNet, and CINIC-10, without needing attack or classifier-specific information. We discuss performance trade-offs and show that our methods remain highly effective even with poisoned or distributionally shifted generative model training data.

## 1 Introduction

Large datasets enable modern deep learning models but are vulnerable to data poisoning, where adversaries inject imperceptible poisoned images to manipulate model behavior at test time. Poisons can be created with or without knowledge of the model's architecture or training settings. As deep learning models grow in capability and usage, securing them against such attacks while preserving accuracy is critical.

Numerous methods of poisoning deep learning systems to create backdoors have been proposed in recent years. These disruptive techniques typically fall into two distinct categories: explicit backdoor, triggered data poisoning, or triggerless poisoning attacks. Triggered attacks conceal an imperceptible trigger pattern in the samples of the training data leading to the misclassification of test-time samples containing the hidden trigger . In contrast, triggerless poisoning attacks involve introducing slight, bounded perturbations to individual images that align them with target images of another class within the feature or gradient space resulting in the misclassification of specific instances without necessitating further modification during inference . Alternatively, data availability attacks pose a training challenge by preventing model learning at train time, but do not introduce any latent backdoors that can be exploited at inference time . In all these scenarios, poisoned examples often appear benign and correctly labeled making them challenging for observers or algorithms to detect.

Current defense strategies against data poisoning exhibit significant limitations. While some methods rely on anomaly detection through techniques such as nearest neighbor analysis, training loss minimization, singular-value decomposition, feature activation or gradient clustering [13; 14; 15; 16; 17; 18; 19], others resort to robust training strategies including data augmentation, randomized smoothing, ensembling, adversarial training and maximal noise augmentation [20; 21; 22; 23; 24; 25; 26]. However, these approaches either undermine the model's generalization performance [27; 18], offer protection only against specific attack types [27; 17; 15], or prove computationally prohibitive for standard deep learning workflows [22; 16; 28; 18; 27; 17; 26]. There remains a critical need for more effective and practical defense mechanisms in the realm of deep learning security.

Generative models have been used for robust/adversarial training, but not for train-time backdoor attacks, to the best of our knowledge. Recent works have demonstrated the effectiveness of both EBM dynamics and Diffusion models to purify datasets against inference or availability attacks [29; 30; 31], but train-time backdoor attacks present additional challenges in both evaluation and application, requiring training using Public Out-of-Distribution (POOD) datasets and methods to avoid cumbersome computation or setup for classifier training.

We propose PureGen, a set of powerful stochastic preprocessing defense techniques, \(_{T}(x)\), against train-time poisoning attacks. PureGen-EBM uses EBM-guided Markov Chain Monte Carlo (MCMC) sampling to purify poisoned images, while PureGen-DDPM uses a limited forward/reverse diffusion process, specifically for purification. Training DDPM models on a subset of the noise schedule improves purification by dedicating more model capacity to'restoration' rather than generation. We further find that the energy of poisoned images is significantly higher than the baseline images, for a trained EBM, and PureGen techniques move poisoned samples to a lower-energy, natural data manifold with minimal accuracy loss. The PureGen pipeline, sample energy distributions, and purification on a sample image can be seen in Figure 1.PureGen significantly outperforms current defenses in all tested scenarios. Our key contributions in this work are as follows.

* A set of state-of-the-art (SoTA) stochastic preprocessing defenses \((x)\) against adversarial poisons using MCMC dynamics of EBMs and DDPMs trained specifically for purification

Figure 1: **Top** The full PureGen pipeline is shown where we apply our method as a preprocessing step with no further downstream changes to the classifier training or inference. _Poisoned images are moderately exaggerated to show visually._**Bottom Left** Energy distributions of clean, poisoned, and PureGen purified images. Our methods push poisoned images via purification into the natural,clean image energy manifold. **Bottom Right** The removal of poison artifacts and the similarity of clean and poisoned images after purification using PureGen EBM and DDPM dynamics. The purified dataset results in SoTA defense and high classifier natural accuracy.

named PureGen-EBM and PureGen-DDPM with analysis providing further intuition on effectiveness
* Experimental results showing the broad application of \((x)\) with minimal tuning and no prior knowledge needed of the poison type and classification model
* Results showing SoTA performance can be maintained even when PureGen models' training data includes poisons or is from a significantly different distribution than the classifier/attacked train data distribution
* Results showing even further performance gains from combinations of PureGen-EBM and PureGen-DDPM and robustness to defense-aware poisons

## 2 Related Work

### Targeted Data Poisoning Attack

Poisoning of a dataset occurs when an attacker injects small adversarial perturbations \(\) (where \(\|\|_{}\) and typically \(=8\) or \(16/255\)) into a small fraction, \(\), of training images, making poisoning incredibly difficult to detect. These train-time attacks introduce _local sharp regions_ with a considerably higher _training loss_. A successful attack occurs when, after SGD optimizes the cross-entropy training objective on these poisoned datasets, invisible backdoor vulnerabilities are baked into a classifier, without a noticeable change in overall test accuracy. This is in contrast to inference-time or other adversarial scenarios where an attacker might be defense or model-aware. The goal in train-time attacks is "stealth" via minimal impact to the dataset and training and testing curves while creating backdoors to exploit at deployment.

In the realm of deep network poison security, we encounter two primary categories of attacks: triggered and triggerless attacks. Triggered attacks, often referred to as backdoor attacks, involve contaminating a limited number of training data samples with a specific trigger (often a patch) \(\) (similarly constrained \(\|\|_{}\)) that corresponds to a target label, \(y^{}\). After training, a successful backdoor attack misclassifies when the perturbation \(\) is added:

\[F(x)=y&x\{x:(x,y)_{test}\}\\ y^{}&x\{x+:(x,y)_{test},y y^{}\} \] (1)

Early backdoor attacks were characterized by their use of non-clean labels [32; 1; 33; 3], but more recent iterations of backdoor attacks have evolved to produce poisoned examples that lack a visible trigger [2; 34; 4].

On the other hand, triggerless poisoning attacks involve the addition of subtle adversarial perturbations to base images \(\|\|_{}\), aiming to align their feature representations or gradients with those of target images of another class, causing target misclassification [5; 6; 7; 8; 9]. These poisoned images are virtually undetectable by external observers. Remarkably, they do not necessitate any alterations to the target images or labels during the inference stage. For a poison targeting a group of target images \(=\{(x^{},y^{})\}\) to be misclassified as \(y^{}\), an ideal triggerless attack would produce a resultant function:

\[F(x)=y&x\{x:(x,y)_{test}\}\\ y^{}&x\{x:(x,y)\}\] (2)

Background for data availability attacks can be found in . We include results for one leading data availability attack Neural Tangent Gradient Attack (NTGA) , but we do not focus on such attacks since they are realized in model results during training. They do not pose a latent security risk in deployed models, and arguably have ethical applications within data privacy and content creator protections as discussed in App. 6.

The current leading poisoning attacks that we assess our defense against are listed below. More details about their generation can be found in App. A.1.

* **Bullseye Polytope (BP):** BP crafts poisoned samples that position the target near the center of their convex hull in a feature space .
* **Gradient Matching (GM):** GM generates poisoned data by approximating a bi-level objective by aligning the gradients of clean-label poisoned data with those of the adversarially-labeled target . This attack has shown effectiveness against data augmentation and differential privacy.

* **Narcissus (NS):** NS is a clean-label backdoor attack that operates with minimal knowledge of the training set, instead using a larger natural dataset, evading state-of-the-art defenses by synthesizing persistent trigger features for a given target class. .
* **Neural Tangent Generalization Attacks (NTGA):** NTGA is a clean-label, black-box data availability attack that can collapse model test accuracy .

### Train-Time Poison Defense Strategies

Poison defense categories broadly take two primary approaches: filtering and robust training techniques. Filtering methods identify outliers in the feature space through methods such as thresholding , nearest neighbor analysis , activation space inspection , or by examining the covariance matrix of features . These defenses often assume that only a small subset of the data is poisoned, making them vulnerable to attacks involving a higher concentration of poisoned points. Furthermore, these methods substantially increase training time, as they require training with poisoned data, followed by computationally expensive filtering and model retraining [16; 17; 14; 15].

On the other hand, robust training methods involve techniques like randomized smoothing , extensive data augmentation , model ensembling , gradient magnitude and direction constraints , poison detection through gradient ascent , and adversarial training [27; 28; 25]. Additionally, differentially private (DP) training methods have been explored as a defense against data poisoning [22; 38]. Robust training techniques often require a trade-off between generalization and poison success rate [22; 37; 24; 28; 25; 26] and can be computationally intensive [27; 28]. Some methods use optimized noise constructed via Generative Adversarial Networks (GANs) or Stochastic Gradient Descent methods to make noise that defends against attacks [39; 26].

Recently Yang et al.  proposed EPIC, a coreset selection method that rejects poisoned images that are isolated in the gradient space while training, and Liu et al.  proposed FrieNDs, a per-image preprocessing transformation that solves a min-max problem to stochastically add \(_{}\) norm \(\)-bound 'friendly noise' (typically 16/255) to combat adversarial perturbations (of 8/255) [18; 26].

These two methods are the previous SoTA and will serve as a benchmark for our PureGen methods in the experimental results. Finally, simple compression JPEG has been shown to defend against a variety of other adversarial attacks, and we apply it as a baseline defense in train-time poison attacks here as well, finding that it often outperforms previous SoTA methods .

## 3 PureGen: Purifying Generative Dynamics against Poisoning Attacks

### Energy-Based Models and PureGen-EBM

An Energy-Based Model (EBM) is formulated as a Gibbs-Boltzmann density, as introduced in . This model can be mathematically represented as:

\[p_{}(x)=(-_{}(x))q(x),\] (3)

where \(x^{D}\) denotes an image signal, and \(q(x)\) is a reference measure, often a uniform or standard normal distribution. Here, \(_{}\) signifies the energy potential, parameterized by a ConvNet with parameters \(\).

The EBM \(_{}(x)\) can be interpreted as an unnormalized probability of how natural the image is to the dataset. Thus, we can use \(_{}(x)\) to filter images based on their likelihood of being poisoned. Furthermore, the EBM can be used as a generator. Given a starting clear or purified image \(x_{}\), we use Markov Chain Monte Carlo (MCMC) Langevin dynamics to iteratively generate more natural images via Equation 4.

\[x_{+}=x_{}-_{x_{}}_{}( x_{})+_{},\] (4)

where \(_{k}(0;_{D})\), \(\) indexes the time step of the Langevin dynamics, and \(\) is the discretization of time . \(_{x}_{}(x)=_{}(x)/ x\) can be obtained by back-propagation. Intuitively, the EBM informs a noisy stochastic gradient descent toward natural images. More details on the convergent contrastive learning mechanism of the EBM and mid-run generative dynamics that makes purification possible can be found in App. A.2.1. Ultimately, the training modifications of using realistic imagesto initialize the MCMC runs of negative samples produces mid-run, meta-stable EBM dynamics which can be leveraged for better purification. Further intuition is in Section 3.4.

### Diffusion Models and PureGen-DDPM

Denoising Diffusion Probabilistic Models (DDPMs) are a class of generative models proposed by (Ho et al., 2020) where the key idea is to define a forward diffusion process that adds noise until the image reaches a noise prior and then learn a reverse process that removes noise to generate samples as discussed further in App. A.3(Han et al., 2019). For purification, we are interested in the stochastic "restoration" of the reverse process, where the forward process can degrade the image enough to remove adversarial perturbations. We find that only training the DDPM with a subset of the standard \(_{t}\) schedule, where the original image never reaches the prior, sacrifices generative capabilities for slightly improved poison defense while reducing training costs. Thus we introduce PureGen-DDPM which makes the simple adjustment of only training DDPMs for an initial portion of the standard forward process, improving purification capabilities. For our experiments, we find models trained up to 250 steps outperformed models in terms of poison purification than those trained on higher steps, up to the standard 1000 steps. We show visualizations and empirical evidence of this in Figure 2 below. In App. E.2.2 we show that pre-trained, standard DDPMs can offer comparable defense performance, but with added training cost.

### Classification with Stochastic Transformation

Let \(_{T}:^{D}^{D}\) be a stochastic pre-processing transformation. In this work, \(_{T}(x)\), is the random variable of a fixed image \(x\), and we define \(T=(T_{},T_{},T_{})^{3}\), hyperparameters specifying the number of EBM MCMC steps, the number of diffusion steps, and the number of times these steps are repeated, respectively. Then, \(T_{}=(T_{},0,1)\) and \(T_{}=(0,T_{},1)\).

Figure 2: **Top** We compare PureGen-DDPM forward steps with the standard DDPM where 250 steps _degrades images for purification but does not reach a noise prior. Note that all model are trained with the same linear \(\) schedule_. **Bottom Left** Generated images from models with 250, 750, and 1000 (Standard) train forward steps _where it is clear 250 steps does not generate realistic images_ **Bottom Right** Significantly improved poison defense performance of PureGen-DDPM with 250 train steps indicating a trade-off between data purification and generative capabilities.

We compose a stochastic transformation \(_{T,k}(x)\) with a randomly initialized deterministic classifier \(f_{}(x)^{J}\) (for us, a naturally trained classifier) to define a new deterministic classifier \(F_{}(x)^{J}\) as

\[F_{}(x)=_{_{T,k}(x)}[f_{_{0}}(_{T,k}(x))]\] (5)

which is trained with cross-entropy loss via SGD to realize \(F_{}(x)\). As this is computationally infeasible we take \(f_{}(_{T,k}(x))\) as the point estimate of \(F_{}(x)\), which is valid because \(_{T,k}(x)\) has low variance.

### Erasing Poison Signals via Mid-Run MCMC

The stochastic transform \(_{T}(x)\) is an iterative process. PureGen-EBM is akin to a noisy gradient descent over the unconditional energy landscape of a learned data distribution. This is more implicit in the PureGen-DDPM dynamics. As \(T\) increases, poisoned images move from their initial higher energy towards more realistic lower-energy samples that lack poison perturbations. As shown in Figure 1, the energy distributions of poisoned images are much higher, pushing the poisons away from the likely manifold of natural images. By using Langevin dynamics of EBMs and DDPMs, we transport poisoned images back toward the center of the energy basin.

In from-scratch \(=8\) poison scenarios, 150 EBM Langevin steps or 75 DDPM steps fully purifies the majority of the dataset with minimal feature loss to the original image. In Figure 3, we explore the Langevin trajectory's impacts on \(_{2}\) distance of both purified clean and poisoned images from the initial clean image (\(\|x-_{T}(x)\|_{2}\) and \(\|x-_{T}(x+)\|_{2}\)), and the purified poisoned image's trajectory away from its poisoned starting point (\(\|(x+)-_{T}(x+)\|_{2}\)). Both poisoned and clean distance trajectories converge to similar distances away from the original clean image (\(_{T}\|x-_{T}(x)\|_{2}=_{T}\|x-_{T}(x+) \|_{2}\)), and the intersection where \(\|(x+)-_{T}(x+)\|_{2}>\|x-_{T}(x+)\|_{2}\) (indicated by the dotted red line), occurs at \(\)150 EBM and 75 DDPM steps, indicating when purification has moved the poisoned image closer to the original clean image than the poisoned version of the image.

These dynamics provide a **concrete intuition for choosing step counts** that best balance poison defense with natural accuracy (given a poison \(\)), hence why we use 150-1000 EBM steps of 75-125 (specifically 150 EBM, 75 DDPM steps in from-scratch scenarios) shown in App. D.2. Further, PureGen-EBM dynamics stay closer to the original images, while PureGen-DDPM moves further away as we increase the steps as the EBM has explicitly learned a probable data distribution, while the DDPM restoration is highly dependent on the conditional information in the degraded image. More experiments comparing the two are shown in App. G.2. These dynamics align with empirical results showing that EBMs better maintain natural accuracy and poison defense with smaller perturbations and across larger distributional shifts, but DDPM dynamics are better suited for larger poison perturbations. Finally, we note the purify times in the \(x\)-axes of Fig. 3, where PureGen-EBM is much faster for the same step counts to highlight the computational differences for the two methods, which we further explore Section 4.5.

Ultimately, one can think of PureGen as sampling from a "close" region in the pixel space around the original image where proximity is determined by a stochastic process that is initialized at the image

Figure 3: Plot of \(_{2}\) distances for PureGen-EBM (**Left**) and PureGen-DDPM (**Right**) between clean images and clean purified (blue), clean images and poisoned purified (green), and poisoned images and poisoned purified images (orange) at points on the Langevin dynamics trajectory. Purifying poisoned images for less than 250 steps moves a poisoned image closer to its clean image with a minimum around 150, preserving the natural image while removing the adversarial features.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

the cost becomes negligible when the purified dataset is used multiple times relative to baselines like FrieNDs which require retraining for each specific task and poison scenario (while still utilizing the full dataset unlike EPIC). PureGen-EDM generally has lower purification times compared to PureGen-DDPM, making it more suitable for subtle and rare perturbations. Conversely, PureGen-DDPM can handle more severe perturbations but at a higher computational cost and potential reduction in natural accuracy.

Training the generative models for PureGen involves substantial computational cost and data requirements. However, as shown in Table 3 and Figure 4, these models remain effective even when trained on poisoned or out-of-distribution data. This universal applicability justifies the initial training cost, as the models can defend against diverse poisoning scenarios. So while JPEG is a fairly effective baseline, the added benefits of PureGen start to outweigh the compute as the use cases of the dataset increase. While PureGen combinations (PureGen-Reps and PureGen-Filt) show enhanced performance on higher power attacks (Table 4), further research is needed to fully exploit the strengths of both PureGen-EBM and PureGen-DDPM.

## 5 Conclusion

Poisoning has the potential to become one of the greatest attack vectors to AI models, decreasing model security and eroding public trust. In this work, we introduce PureGen, a suite of universal data purification methods that leverage the stochastic dynamics of Energy-Based Models (EBMs) and Denoising Diffusion Probabilistic Models (DDPMs) to defend against train-time data poisoning attacks. PureGen-EBM and PureGen-DDPM effectively purify poisoned datasets by iteratively transforming poisoned samples into the natural data manifold, thus mitigating adversarial perturbations. Our extensive experiments demonstrate that these methods achieve state-of-the-art performance against a range of leading poisoning attacks and can maintain SoTA performance in the face of poisoned or distributionally shifted generative model training data. These versatile and efficient methods set a new standard in protecting machine learning models against evolving data poisoning threats, potentially inspiring greater trust in AI applications.

## 6 Potential Social Impacts

Poisoning represents one of the greatest emerging threats to AI systems, particularly as foundation models increasingly rely on large, diverse datasets without rigorous quality control against imperceptible perturbations. This vulnerability is especially concerning in high-stakes domains like healthcare, security, and autonomous vehicles, where model integrity is crucial and erroneous outputs could have catastrophic consequences. Our research provides a universal defense method that can be implemented with minimal impact to existing training infrastructure, enabling practitioners to preemptively secure their datasets against state-of-the-art poisoning attacks.

While we acknowledge that the poison defense space can promote an 'arms race' of increasingly sophisticated attacks and defenses, our approach's universality poses a fundamentally harder challenge for attackers, even when using defense-aware crafting E.2.1. We specifically focus on defending against latent backdoor vulnerabilities rather than data availability attacks, as the latter can serve legitimate purposes in protecting content creators' rights. By providing robust defense against malicious poisoning while preserving natural model performance, our method helps build trust in AI systems for increasingly consequential real-world applications.

   \\   & **Single Classifier** & **Gradient Matching** & **Narcissus** \\  & **(Median)** & **100 Classifiers** & **10 Classifiers** \\  None, JPEG & **3690** & 3673\({}_{}\) & **5288\({}_{}\)** \\ EPIC & 3650 & 3624\({}_{}\) & 5212\({}_{}\) \\ FrieNDs & 11502 & 11872\({}_{}\) & 1286\({}_{}\) \\ PureGen-EBM\({}_{}\) & 4613 & 3699\({}_{}\) & **5380\({}_{}\)** \\ PureGen-DDPM\({}_{}\) & 7871 & 3731\({}_{}\) & 5706\({}_{}\) \\  

Table 5: PureGen and baseline Timing Analysis on TPU V3Acknowledgments

This work is supported with Cloud TPUs from Google's Tensorflow Research Cloud (TFRC). We would like to acknowledge Jonathan Mitchell, Mitch Hill, Yuan Du and Kathrine Abreu for support and discussion on base EBM and Diffusion code, and Yunzheng Zhu for his help in crafting poisons.