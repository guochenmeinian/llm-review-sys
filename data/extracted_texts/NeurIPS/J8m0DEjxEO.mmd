# AttnGCG: Enhancing Adversarial Attacks on Language Models with Attention Manipulation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

This paper studies the vulnerabilities of transformer-based Large Language Models (LLMs) to jailbreaking attacks, with a particular focus on the optimization-based Greedy Coordinate Gradient (GCG) strategy. Noting a positive correlation between the effectiveness of attacks and the internal behaviors of models--for instance, attacks are less effective when models robustly focus on system instructions specialized for mitigating harmful behaviors and ensuring safety alignment--we introduce an enhanced method that additionally manipulates models' attention scores to enhance the large language model (LLM) jailbreaking. We term this novel strategy AttnGCG. Empirically, AttnGCG demonstrates consistent performance enhancements across diverse LLMs, with an average improvement of 7% in the Llama-2 series and 10% in the Gemma series. This strategy also exhibits stronger attack transferability when testing on unknown or closed-sourced LLMs, such as GPT-3.5 and GPT-4. Moreover, we show that AttnGCG is able to offer enhanced interpretability by visualizing models' attention scores across different input components, thus providing clear insights into how targeted attention manipulation contributes to more successful jailbreaking.

## 1 Introduction

Transformer-based large language models (LLMs)  have enabled tremendous success in the field of natural language processing (NLP), propelling these systems toward near human-level intelligence . Nevertheless, to ensure these sophisticated systems remain safe and ethical, preventing them from generating harmful responses, LLMs typically undergo comprehensive safety training . This critical training process enables models to refuse inappropriate requests and produce responses that are socially acceptable and contextually suitable, which aims at significantly enhancing their functional utility in real-world NLP applications .

However, despite these established safety protocols, aligned LLMs remain vulnerable to adversarial attacks that can provoke toxic responses , particularly those that employ optimization-based approaches. These attacks typically exploit the model's inherent security flaw by optimizing for malicious adversarial input, including optimization-based gradient-searching methods , approaches that adapt genetic algorithms  and LLM-aided jailbreakings , collectively highlighting the ongoing security '_bugs_' of advanced LLMs.

In this paper, we focus on the optimization-based attack, whose target is to maximize the probability of generating harmful textual content. We notice two limitations with existing optimization-based attacks. First, these methods heavily rely on the likelihood of generating target tokens as an indicator of a successful jailbreak. Although many techniques are developed for maximizing this targeted probability , the recent study points out that a high probability of harmful tokensdoes not necessarily equate to a successful jailbreak . For example, as shown in figure 1, the adversarial prompt crafted by the popular Greedy Coordinate Gradient (GCG)  may initially cause the LLM to generate the target tokens, but subsequently, the model could reject the request, rendering the jailbreak unsuccessful. Second, existing attack frameworks predominantly operate at the output layer of LLMs, evaluating performance based on metrics like the probability of target tokens  or using scores from auxiliary evaluation models . These approaches neglect the internal workings of LLMs, resulting in a lack of interpretability. Without this understanding, it would be challenging to grasp why some attacks succeed while others fail, limiting the development of more effective attack strategies.

To mitigate these issues, we advocate for a deeper understanding of the underlying factors that contribute to the success of LLM jailbreaks. Our key insight is the crucial role of the attention score, which reveals where the model focuses during token generation, in achieving successful jailbreaking. Specifically, in aligned LLMs, the input typically consists of two parts: a system prompt (though, for some LLMs, defaulted to be empty) and a user prompt, assuming a zero-shot scenario. In the jailbreaking context, the user prompt can be further segmented into the goal prompt, representing the user's intent, and the adversarial suffix, which our method targets for optimization to facilitate jailbreak. In Figure 2, we illustrate the distribution of the model's attention score across these three input components during the jailbreak process. Interestingly, our findings indicate that the model's attention score on the adversarial suffix could serve as a strong indicator of the jailbreaking performance, _e.g._, a successful jailbreak typically corresponds with a high attention score on the adversarial suffix. The rationale behind this is that as the attention score on the adversarial suffix increases, the model's focus on the system prompt and the input goal would decrease, thereby diminishing their effectiveness in safeguarding the system and giving the adversarial suffix a higher chance to circumvent the model's safety protocols.

Building upon this key observation, we introduce AttnGCG. AttnGCG leverages the attention score as an additional optimization target, creating adversarial suffixes that are more challenging for LLMs to defend against. Specifically, we augment the traditional GCG objective with an auxiliary attention loss function, which gradually increases the importance of the suffix during adversarial training. By pivoting the optimization focus from solely targeting token probabilities to also manipulating models' attention scores, we concentrate the model's attention more effectively on the adversarial suffix, thereby enhancing jailbreak success, resulting in an increase in attack success rate from an average of 64.3% to 70.6% over 7 aligned LLMs. Furthermore, we demonstrate the versatility of our method by adding it to other existing jailbreaking techniques, which register an average of 5.3% ASR increases over the vanilla GCG. Additionally, AttnGCG exhibits stronger transferability to closed-source LLMs, achieving an average improvement of 2.8% over the GCG baseline. Qualitative visualizations are also provided to showcase that the the attention scores of the model to adversarial suffixes can serve as an interpretable indicator of jailbreaking performance, providing new insights for evaluating and enhancing the quality of an adversarial prompt.

Figure 1: A higher attention score on the adversarial suffix can indicate a higher attack success rate. We show that the original GCG  is not sufficient for jailbreak, as the model may generate the first few target tokens, but may still fail to fulfill the request. In contrast, our method, AttnGCG, successfully bypasses the safety protocols rooted in LLMs by increasing the attention score on the adversarial suffix.

## 2 Method

In this section, we first give a brief introduction of the GCG method , which our AttnGCG is built on. Then, we formalize attention scores of different LLM input components, and finally the core contribution in this paper: attention loss, which greatly enhances the learning of the adversarial suffix.

### Greedy Coordinate Gradient

The Greedy Coordinate Gradient (GCG)  is a pioneering method for eliciting malicious text outputs from aligned LLMs by employing a discrete token-level optimization. In this approach, an LLM is considered as a mapping from a sequence of \(n\) tokens \(x_{1:n}\) to a distribution over the next token \(x_{n+1}\). In the jailbreaking scenario, the first \(n\) tokens \(x_{1:n}\) fed to the language model contains both the goal which the user aim to achieve \(x_{}=x_{1:j}\) as well as an adversarial suffix \(x_{}=x_{j+1:n}\) that we aim to optimize. The objective of GCG is to find an adversarial suffix \(x_{}\) that minimizes the negative log probability of a target sequence of tokens \(x_{n+1:n+L}^{*}\), representing affirmative response (_i.e._, "Sure, here is...") Under this context, GCG uses a target loss function \(_{t}\):

\[_{t}(x_{1:n})=- p(x_{n+1:n+L}^{*}|x_{1:n}). \]

Formally, the optimization problem of GCG can be expressed as:

\[_{x_{}\{1,...,V\}^{||}}_{t}(x_{1:n}), \]

where \(\) denotes the indices of the adversarial suffix tokens in the LLM input, and \(V\) denotes the vocabulary size. This objective is optimized by the Greedy Coordinate Gradient (GCG)  algorithm (Algorithm 1) to find the optimal adversarial suffix.

### Attention Loss

As current LLMs are mostly attention-based architecture, we can assume that when generating the next tokens, the model will generate an attention matrix indicating the importance of all previous tokens \(x_{1:n}\) to the next token \(x_{n+1}\). As we are calculating the loss in Eq. 1 using a sequence of target tokens \(x_{n+1:n+L}\), we can obtain the attention weight matrix \(w\) with the shape of \((n+L)(n+L)\) where \(w_{i,j}\) represents the attention weight of token \(x_{j}\) on the token of \(x_{i}\). In our implementation, we use the attention matrix from the last decoder layer. We define the attention score \(s_{j}\) on the token \(x_{j}\) as the average of the attention weights of token \(x_{j}\) on the output tokens \(x_{n+1:n+L}\):

\[s_{j}=_{i[n+1:n+L]}}{L}. \]

Figure 2: The attention scores and attack success rate (ASR) of GCG  (_left_) and our method (_left_) on Llama-2-Chat-7B. We observe that (1) the attention score on adversarial suffix can grow simultaneously with the ASR. (2) Meanwhile, attention scores on the goal and system prompt can decrease as the training continues.

Similarly, we can represent the attention score of the adversarial suffix \(x_{}\) as:

\[s_{}=_{i}}{||}, \]

where \(\) denotes the indices of the adversarial suffix tokens in the LLM input. Based on our insight of that the model's attention score on the adversarial suffix can indicate the jailbreaking performance, we can directly optimizing for the below objective:

\[_{x_{}\{1,...,V\}^{|x_{}|}}s_{}. \]

We can rewrite this as a loss function:

\[_{a}(x_{1:n})=-s_{}. \]

We can easily integrate this new loss function into the original GCG loss by a weighted sum \(_{t+a}(x_{1:n})=w_{t}_{t}(x_{1:n})+w_{a}_{a}( x_{1:n})\), where \(w_{t}\) and \(w_{a}\) are the weights. Therefore, the optimization objective of AttnGCG can be written as:

\[_{x_{}\{1,...,V\}^{||}}_{t+a}(x_{1:n}). \]

We use the Greedy Coordinate Gradient method  to optimize this objective augmented with attention loss.Algorithm 2 and 1 shows a comparison of our AttnGCG to the original GCG method.

```
Input:Initial prompt \(x_{1:n}\), modifiable subset \(\), iterations \(T\), \(k\), batch size \(B\), loss \(_{t}\)(only target loss) repeat
1for\(i\)do
2\(_{i}:=\)Top-k(\(-_{e_{x_{i}}}_{t+a}(x_{1:n})\))
3for\(b\)=1,...,\(B\)do
4\(_{1:n}^{(b)}:=x_{1:n}\)
5\(_{i}^{(b)}:=Uniform(_{i})\), where \(i=Uniform()\)
6\(x_{1:n}:=_{1:n}^{(b^{*})}\), where \(b^{*}=argmin_{b}_{t}(_{1:n}^{(b)})\)
7until\(T\)times;
8Output:Optimized prompt \(x_{1:n}\)
```

**Algorithm 1**GCG

## 3 Experiments

In this section, we first introduce the experimental setup. Second, we present ans analyze results of AttnGCG across different white-box LLMs compared with the original GCG. Then, we validate the universality of our method by connecting AttnGCG with other attacks. Finally, we conduct transfer attacks on black-box LLMs to validate the transferability of prompts generated by AttnGCG.

Figure 3: The components of prompts fed into LLMs. ‘System’ is the default system prompt of the model, ‘Goal’ and ‘Suffix’ make up the user prompt where ‘Goal’ describes the actual user request, and ‘Suffix’ is an adversarial prompt our method will optimize for. ‘Target’ is the model’s generation, on which we calculate the loss function to generate the desired output.

### Experimental Setups

**Dataset.** We employ the AdvBench Harmful Behaviors benchmark  to assess jailbreak attacks. This dataset comprises \(520\) requests spanning profanity, graphic depictions, threatening behavior, misinformation, discrimination, cybercrime, and dangerous or illegal suggestions. We randomly sample \(100\) behaviors from this dataset for our evaluation.

**Language Models** In this paper, we attempt to jailbreak both open-source and closed-source LLMs. For open-source LLMs, we test the LLaMA , Gemma , and Mistral  series of seven aligned models, particularly including Mistral-8x7B-Instruct , the open-source MoE model that outperforms GPT-3.5, and LLaMA-3 , the most capable openly available LLM to date. For closed-source LLMs, we mainly focus on GPT-3.5 , GPT-4 , and the Gemini  series, due to their widespread use. For each of these target models, we use a temperature of zero for deterministic generation. For a list of all system prompts used in this paper, see Table 8.

**Baselines and hyperparameters.** We mainly adopt the simple and effective GCG  as our baseline on both direct attack on white-box LLMs and the transfer attack. We also incorporate optimization-free method ICA  and AutoDAN  as baselines and use their generated suffix as the initialization of our training. For evaluation, we conduct comparisons under the condition of training for \(500\) steps, using the same implementation and parameters as our method, i.e., at the same query count. Detailed parameters can be found in the Appendix A.

**Evaluation** To comprehensively access our proposed attack, we use two types of evaluation protocols, one is the keyword-detection method introduced in Zou et al.  which assesses harmfulness by checking the presence of predefined keywords in the response. Another evaluation leverages LLMs as the judge , we utilize GPT-4  as our judge to determine whether or not the attack is successful, which is proven to better align with the attacker's interests .

### Base Experiments on GCG and AttnGCG

**Main results and analysis.** We first conduct white-box attack of GCG and our AttnGCG where we can optimize the adversarial suffix directly based on LLM probabilities. Results from Table 1 suggest that, incorporating attention loss (Eq. 6) into the optimization objective leads to a general improvement in attack performance. This observation is well supported as the proposed AttnGCG consistently outperforms GCG across various models, showcasing an average improvement of 6.3% with GPT-4 judge and 3.9% using the keyword detection. Note that, our statistics also indicate the 'false jailbreak' of current LLMs mentioned by Chao et al. , as the ASR measured by generating target tokens is 9.8% higher than GPT-4-aided evaluation (_i.e._ 64.3% _vs._ 74.1%). Our AttnGCG with additional attention loss can handle such situation by generating complete responses given malicious input. For example, on the popular Gemma models, AttnGCG narrows the gap between two evaluation aspects by 8% from 23.5% to 15.5% on average. These results validate that our proposed AttnGCG is a competent jailbreaking strategy for aligned LLMs.

    &  &  \\   & GPT-4 judge & keyword-detection & GPT-4 judge & keyword-detection \\  Llama-2-Chat-7B & 48.0\% & 51.0\% & 58.0\% \(+\)10.0\% & 60.0\% \(+\)9.0\% \\ Llama-2-Chat-13B & 47.0\% & 47.0\% & 51.0\% \(+\)4.0\% & 52.0\% \(+\)5.0\% \\ Llama-3-8B-Instruct & 42.0\% & 50.0\% & 45.0\% \(+\)3.0\% & 51.0\% \(+\)1.0\% \\ Gemma-2B-it & 73.0\% & 93.0\% & 81.0\% \(+\)8.0\% & 95.0\% \(+\)2.0\% \\ Gemma-7B-it & 63.0\% & 90.0\% & 75.0\% \(+\)12.0\% & 92.0\% \(+\)2.0\% \\ Mistral-7B-Instruct-v0.2 & 94.0\% & 95.0\% & 95.0\% \(+\)1.0\% & 98.0\% \(+\)3.0\% \\ Mistral-8x7B-Instruct-v0.1 & 83.0\% & 93.0\% & 89.0\% \(+\)6.0\% & 98.0\% \(+\)5.0\% \\  Average & 64.3\% & 74.1\% & 70.6\% \(+\)6.3\% & 78.0\% \(+\)3.9\% \\   

Table 1: We present Attack Success Rates (ASR) measured by both GPT-4 (GPT-4 judge) and the keyword detection technique (keyword-detection). The ASR changes of AttnGCG relative to GCG are marked in red.

[MISSING_PAGE_EMPTY:6]

#### Transfer AttnGCG to other methods

From Table 4, we observe that:

1. Using AttnGCG to further optimize the prompts generated by ICA and AutoDAN leads to additional enhancement over the performance of these methods. Moreover, the average improvement is 5% higher than that achieved by GCG, demonstrating that optimizing adversarial prompts based on attention can effectively further enhance existing methods.
2. ICA+AttnGCG and AutoDAN+AttnGCG both outperform AttnGCG alone, indicating the significance of properly initializing prompts in AttnGCG. A good initialization can reduce the search space. The standard for evaluating the quality of initialization can be referenced in the Table3, from which we can observe that AutoDAN prompts have the smallest attention score on the goal part. Furthermore, AutoDAN+AttnGCG achieves the highest performance. Therefore, prompts with smaller attention score on the goal part, _i.e._, prompts that can shift more attention of the model away from the goal, serve as better initializations.

   Model & Method & GPT-4 judge & keyword-detection \\  Llama-2-Chat-7B & Vanilla & 0.0 & 0.0 \\ Llama-2-Chat-7B & GCG & 48.0 & 51.0 \\ Llama-2-Chat-7B & AttnGCG & 58.0 & 60.0 \\  Llama-2-Chat-7B & AutoDAN-HGA & 35.0 & 56.0 \\ Llama-2-Chat-7B & AutoDAN-HGA+GCG & 86.0 & 87.0 \\ Llama-2-Chat-7B & AutoDAN-HGA+AttnGCG & 91.0 & 92.0 \\  Llama-2-Chat-7B & ICA & 0.0 & 0.0 \\ Llama-2-Chat-7B & ICA+GCG & 56.0 & 56.0 \\ Llama-2-Chat-7B & ICA+AttnGCG & 61.0 & 62.0 \\   

Table 4: We compared the effects of further adding GCG and AttnGCG to different base methods(ICA, AutoDAN). AttnGCG consistently enhances base methods and provides a greater improvement compared to GCG.

Figure 5: Attention heatmaps for prompts derived by ICA and AutoDAN. The top two images depict the attention heatmaps from the input prompt (\(x\)-axis) to the output (\(y\)-axis), with the score of the goal input highlighted. The attention scores on the goal prompt are presented in Table 3.

### Transfer to Closed-Source Models

Since our method relies on data from the model's internal workings and requires outputting attention weights during the jailbreak process, our direct victim models are limited to open-source ones. However, as successful jailbreaks developed for one large language model can often be reused on another model , our method offers the possibility of attacking closed-source models. Therefore, we also tested the transferability of AttnGCG to unknown models.

From table 5, we observe that the prompts created by AttnGCG show greater transferability to closed-source models compared to GCG. AttnGCG surpasses GCG by an average of 2.8% on GPT-4 judge and by an average of 2.4% on keyword detection.

We also test the transfer performance on the latest models such as Gemini-1.5-Pro-latest, Gemini-1.5-Flash, and GPT-4o. However, both GCG and AttnGCG exhibited very low transferability to these models. For instance, on Gemini-1.5-Flash, the average attack success rate (as judged by GPT-4) for GCG is 0.5%, and for AttnGCG, it is 1%. We believe that conclusions drawn under such low ASR conditions are not representative and that more future studies are needed.

## 4 Related Work

Optimization-based JailbreakingOptimization-based method design a criteria to find the most effective adversarial prompts for jailbraking LLMs. This paradigm is initially explored with gradient-based optimization and introduced by GCG , which employs a combination of greedy and gradient-based search techniques for both white-box and black-box LLM jailbreaking. PGD  revisit Projected Gradient Descent (PGD) on the continuously relaxed input prompt for creating adversarial prompts. Genetic-based methods [17; 19] leverage the genetic algorithm to produce universal and transferrable prompts to attack black-box LLMs.  propose to first manually design an adversarial template, then use random search to maximize the target probability for jailbreaking black

    &  &  \\   & GPT-4 judge & keyword-detection & GPT-4 judge & keyword-detection \\   \\  Llama-2-Chat-7B & 40.0\% & 49.0\% & 40.0\%\({}_{+0.0\%}\) & 58.0\%\({}_{+9.0\%}\) \\ Mikrtal-8x7B-Instruct-v0.1 & 11.0\% & 19.0\% & 16.0\%\({}_{+5.0\%}\) & 21.0\%\({}_{+2.0\%}\) \\   \\  Llama-2-Chat-7B & 74.0\% & 82.0\% & 78.0\%\({}_{+4.0\%}\) & 82.0\%\({}_{+0.0\%}\) \\ Mikrtal-8x7B-Instruct-v0.1 & 45.0\% & 56.0\% & 51.0\%\({}_{+6.0\%}\) & 60.0\%\({}_{+4.0\%}\) \\   \\  Llama-2-Chat-7B & 82.0\% & 87.0\% & 83.0\%\({}_{+1.0\%}\) & 88.0\%\({}_{+1.0\%}\) \\ Mikrtal-8x7B-Instruct-v0.1 & 43.0\% & 55.0\% & 54.0\%\({}_{+11.0\%}\) & 61.0\%\({}_{+6.0\%}\) \\   \\  Llama-2-Chat-7B & 99.0\% & 100.0\% & 100.0\%\({}_{+1.0\%}\) & 100.0\%\({}_{+0.0\%}\) \\ Mikrtal-8x7B-Instruct-v0.1 & 94.0\% & 100.0\% & 96.0\%\({}_{+2.0\%}\) & 100.0\%\({}_{+0.0\%}\) \\   \\  Llama-2-Chat-7B & 3.0\% & 9.0\% & 4.0\%\({}_{+1.0\%}\) & 11.0\%\({}_{+2.0\%}\) \\ Mikrtal-8x7B-Instruct-v0.1 & 1.0\% & 1.0\% & 1.0\%\({}_{+0.0\%}\) & 4.0\%\({}_{+3.0\%}\) \\   \\  Llama-2-Chat-7B & 15.0\% & 24.0\% & 18.0\%\({}_{+3.0\%}\) & 24.0\%\({}_{+0.0\%}\) \\ Mikrtal-8x7B-Instruct-v0.1 & 5.0\% & 7.0\% & 5.0\%\({}_{+0.0\%}\) & 9.0\%\({}_{+2.0\%}\) \\  Average & 42.7\% & 49.1\% & 45.5\%\({}_{+2.8\%}\) & 51.5\%\({}_{+2.4\%}\) \\   

Table 5: We compared the transferability of prompts generated by GCG and AttnGCG, using GPT-3.5, GPT-4 and Gemini-Pro as transfer targets. The ASR improvements of AttnGCG relative to GCG are marked in red.

box LLMs. Prompt Automatic Iterative Refinement (PAIR)  use an attacker LLM to generate jailbreaks for the targeted LLM with iterative queries for the target LLM to update and refine a candidate jailbreak. Building upon PAIR,  propose a refined version for adversarial prompt searching, which employ a tree-based search method. Our AttnGCG belongs to the optimization-based category and employ the attention score as an additional objective for enhanced jailbreaking.

**Optimization-free Jailbreaking** Optimization-free jailbreakings generally attack models by twitching the input prompts. Early attack strategies are tested on ChatGPT since its initial release, users realized that by "delicately" design the input prompts, the aligned ChatGPT always chooses to answer malicious questions without refusal . Since this kind of attack method only requires adjust the model input, it has drawn huge attention from role play  to semi-subversion of the safety training objective . A main trend for producing the malicious textual prompt is by leveraging another LLM. Shah et al.  employ an LLM that is guided by persona modulation to generate jailbreaking prompts. GPTFuzzer  demonstrate an iterative jailbreaking enhancement over human-written templates with LLMs. Zeng et al.  and Takemoto  chose to refine the input adversarial examples using stronger LLMs (_e.g._, fintuned GPT-3.5) and high-quality prompts. Deng et al.  propose a novel attacking using reverse engineering and an LLM as the automatic prompt generator. Besides, by interpolating rare linguistic knowledge, Yuan et al.  discover the intriguing fact that conducting chats in cipher can bypass the LLM safety alignment. ICA  successfully attack LLMs by adapting the in-context technique that contain a few examples of harmful question-answer pairs.

## 5 Conclusion

In this paper, we study the jailbreaking attacks of transformer-based LLMs. Our exploration results in an insight of the effectiveness of the jailbreaking attack and the model's internal behaviors -- the attention on the adversarial suffix matters for successful jailbreaking. Based on this insight, we proposed a novel method termed AttnGCG that directly manipulates the model's attention score to optimize for a enhanced jailbreaking suffix. Our experiments have shown an impressive improvement in both white box attacks and transfer attacks. Furthermore, we demonstrate that by visualizing the model's attention score, we can provide a clear insight on how jailbreaking is achieved by manipulating the attention distributions. We believe our work can inspire future works on the attack and defense of LLMs.

**Limitations** The transfer attack performance of AttnGCG is unsatisfactory on the latest models, including Gemini-1.5-Pro-latest, Gemini-1.5-Flash, and GPT-4o, necessitating further research to address this issue. The results are presented in the Table 12. Nonetheless, our method still consistently perform well on models released before January 25, 2024.

Due to the limited availability of high-quality red teaming datasets, we only conduct the experiment on the most widely used redteaming benchmark, _AdvBench Harmful Behaviors_, where our method demonstrates consistently strong performance. This highlights the general issues faced by current adversarial attacks, underscoring the necessity for a more comprehensive redteaming benchmark and emphasizing the requirement for further efforts in this regard.

## 6 Ethics Statement

Operating within a white-box setting, our proposed jailbreak targets open-sourced LLMs derived from unaligned models like Llama2-7B for Llama2-7B-Chat. Adversaries can manipulate these base models directly, rather than use our specific prompt.

Looking ahead, while we acknowledge that our method, like previous jailbreak studies, has limited immediate harm, it prompts further investigation into stronger defense measures. We argue that openly discussing attack methods at this stage of LLM advancement is beneficial, as it allows for the enhancement of future LLM iterations with improved security measures if necessary.