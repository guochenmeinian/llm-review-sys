# Robust group and simultaneous inferences for high-dimensional single index model

Weichao Yang

School of Statistics

Beijing Normal University, Beijing, China

yangweichao@mail.bnu.edu.cn &Hongwei Shi

School of Statistics

Beijing Normal University, Beijing, China

shihongwei21@mail.bnu.edu.cn &Xu Guo

School of Statistics

Beijing Normal University, Beijing, China

xustat12@bnu.edu.cn &Changliang Zou

NITFID, School of Statistics and Data Science,

LPMC and KLMDASR and LEBPS,

Nankai University, Tianjin, China

zoucl@nankai.edu.cn

The first two authors contribute equally and are co-first authors. Xu Guo is the corresponding author. Guo was supported by National Key R & D Program of China (Grant Nos. 2023YFA1008702, 2023YFA1011100), National Natural Science Foundation of China Grant (Nos. 12322112, 12071038), and the Fundamental Research Funds for the Central Universities. Zou was supported by the National Key R & D Program of China (Grant Nos. 2022YFA1003703, 2022YFA1003800), the National Natural Science Foundation of China (Grant Nos. 11925106, 12231011, 11931001, 12226007, 12326325).

proposed a decorrelated score method which is a general procedure and is applicable to a large family of penalized M-estimators.  introduced a recursive online-score estimation for high-dimensional generalized linear model.  proposed the nuisance penalized regression which does not penalize the parameters of interest.  and  studied inference problems for the high-dimensional Cox model and longitudinal data, respectively. Logistic regression model was investigated in . Other recent developments include , , , and .

Most of current inference procedures focus on parametric regression models. To address the potential mis-specification issue of parametric models, the single index model (SIM) which is one of the most popular semiparametric modelling techniques, has received extensive attention and in-depth research in the past decade. Let \(Y\) be the response variable along with predictor vector \(=(X_{1},,X_{p})^{}^{p}\). We consider a general SIM,

\[Y=g(^{},) ,\] (1.1)

where \(=(_{1},,_{p})^{}\), the link function \(g()\) is unknown and \(\) means independence. Equivalently, we have

\[Y^{}.\]

The above statement is that, given \(^{}\) which is called index, the response \(Y\) is independent of the predictor vector \(\). Clearly, model (1.1) covers linear models, generalized linear models and also the classical SIM , that is, \(Y=g_{1}(^{})+\) with \(g_{1}()\) being an unknown function. It relaxes restrictive assumptions on parametric models and is flexible enough to capture complex relationship between the response and the predictors.

For high-dimensional SIMs, variable selection has been considered by many authors. Examples include , , , , , , ,  and . Our primary interest is to detect whether a set of predictors contributes to the response \(Y\) or not given the other predictors, that is testing the following group inference problem:

\[_{0,}:_{j}=0\;j _{1,}:_{j} 0\;j,\] (1.2)

where \(\) is a prespecified subset of \(\{1,2,,p\}\) with \(p_{0}=||\). Such a hypothesis naturally arises in the high-dimensional setting. For example, researchers may want to test whether a gene pathway, consisting of multiple genes for the same biological functions, is important for certain clinical outcome The above group inference problem also includes global significance testing as a special case when we set \(=\{1,2,,p\}\) and \(p_{0}=p\). The group inference problem is more difficult than purely global significance testing. Actually, for group inference problem, we are dealing with a high-dimensional interested parametric vector with a high-dimensional nuisance parameter. Such a group inference problem was considered by  for high-dimensional linear models. See also , and . All of , , and  did not establish asymptotical honesty of their procedures, which is critical for reliable inference in high-dimensional setting.

For SIM, due to the existence of the unknown link function \(g()\), the group inference problem is more difficult. Actually, the parameter of interest and the nonparametric function are bundled together, which makes the inference being challenging. For the model (1.1),  developed a debiased LASSO procedure for individual coefficient, while  considered simultaneous confidence interval for optimal treatment regimes under the classical SIM . Their procedures, as well as many other existing works, rely heavily on the sub-Gaussian assumption about the error term. In practice, data with heavy-tailed distribution or in the presence of outliers are very common , and the efficiency of those procedures developed under sub-Gaussian assumption would be largely deteriorated. Recently  investigated the robust inference problem of high-dimensional SIM by adopting the Huber loss with original response. However, their procedure still requires bounded first moment condition of the error term and needs to select a suitable robustification parameter. In this paper, we aim to develop robust group inference methods for high-dimensional SIM without any moment condition on the error term and without choosing robustification parameter.

Interestingly, under a mild condition on the predictors, which is called linearity condition in the literature of sufficient dimension reduction , we find that the SIM can be recast into a pseudo-linear model with transformed response, allowing us to make robust inference in a simple fashion. With a specific "response-distribution" transformation, the sub-Gaussian response assumption required in existing methods is totally avoided, and accordingly our proposed procedures are robust to outliers or heavy-tailed error distributions.

Besides group inference of a prespecified subset of predictors, we are also interested in identifying relevant predictors. To this aim, large-scale simultaneous hypotheses are considered

\[_{0j}:_{j}=0_{1j}:_{j}  0, 1 j p.\]

Apart from identifying as many nonzero \(_{j}\) as possible, to obtain results with uncertainty quantification, we would like to control the false discovery rate (FDR) which is an extremely popular tool to maintain the ability to reliably detect true alternatives without excessive false positive results when large-scale hypotheses are simultaneously tested . , ,  and  considered FDR control in high-dimensional regression models. We follow this line and develop a FDR control procedure for high-dimensional SIM.

Our major contributions are listed from the following three aspects.

* We extend the rank-LASSO procedure in  to include both convex and non-convex penalties and establish error bound of any local optimum of the empirical objective. These theoretical results are summarized in subsection 3.1.
* In Section 2, we provide asymptotically honest group inference procedures based on the idea of orthogonalization for testing the joint effect of many predictors, which enjoys the feature that it does not require the zero and nonzero coefficients to be well-separated. We demonstrate the superiority of our test procedures both theoretically and empirically. Please see Section 3 for theoretical justification and Section 4 for numerical studies respectively.
* We develop a multiple testing procedure for determining if the individual coefficients are relevant simultaneously, and show that it is able to control the FDR asymptotically. To this end, we develop suitable multiple testing procedures and show that the proposed methods can control the false discovery rate (FDR, ) both theoretically and empirically. We refer the readers to Appendix A.3 and A.6 for details.

Notation.For a \(d\)-dimension vector \(\), we write \(\|\|_{r}=(_{k=1}^{d}U_{k}^{r})^{1/r}\) and \(\|\|_{}=_{1 k d} U_{k}\) to denote \(l_{r}\) and \(l_{}\) norms of \(\). Further we define \(\|\|_{0}=\#\{k:U_{k} 0\}\). A random variable \(X\) is _sub-Gaussian_ if the moment generating function (MFG) of \(X^{2}\) is bounded at some point, namely \((X^{2}/K^{2}) 2\), where \(K\) is a positive constant. A random variable \(Y\) is _sub-Exponential_ if the MGF of \( Y\) is bounded at some point, namely \(( Y/K^{}) 2\), where \(K^{}\) is a positive constant. For \(a,b\), we write \(a b=\{a,b\}\).

## 2 Group inference based on distribution transformation

Without loss of generality, assume that \(()=\), and \(=()>0\). Now let \(_{h}=\{,h(Y)\}\) for a given transformation function \(h()\) of the response. Define \(_{h}=^{-1}_{h}\). We then have the following result.

**Proposition 2.1**.: _Assume that \((^{})\) is a linear function of \(^{}\). Then \(_{h}\) is proportional to \(\), that is, \(_{h}=_{h}\) for some constant \(_{h}\)._

The above proposition follows directly from Theorem 2.1 in . The assumption in above proposition is known as linearity condition (LC) for predictors. It is satisfied when \(\) has an elliptical distribution and widely assumed in the sufficient dimension reduction literature [31; 14].  showed that in high-dimensional setting, the LC holds to a reasonable approximation. Throughout of the paper, we assume that \(_{h} 0\). This assumption is mild. In fact, when \(h()\) is monotone and \(g(,)\) is monotone with respect to the first argument, this assumption is satisfied.

From the definition of \(_{h}\), we can write

\[h(Y)=_{h}^{}+e,\] (2.3)

where the error term \(e\) must satisfy \((e)=0\). Different from the existing literature which usually imposes independence between the regression error and the predictors, the regression error \(e\) in the transformed model is only uncorrelated with the predictors. This implies that under LC, we can recast the general SIM into a pseudo-linear model with transformed response \(h(Y)\). Therefore, by Proposition 2.1, testing \(_{0,}\) is equivalent to

\[_{0,}^{}:_{hj}=0\;\;j _{1,}^{}:_ {hj} 0\;\;j.\]This reformulation is important, allowing us to circumvent the issue of estimating the unknown link function \(g(,)\) and to make inference of \(_{hj},j\) in a linear regression model with transformed response instead of \(_{j}\) in SIM. As we show later, this reformulation greatly facilitates the construction of our test statistic and simplifies the computation as well.

In practice, we need to choose a suitable transformation function \(h()\). We note that 's procedure also relies on the above proposition and they essentially work with \(h(Y)=Y\). For robustness consideration, throughout this paper, we consider the distribution function of \(Y\), denoted by \(F(Y)\) as the transformation function. Actually with the equation (2.3), given the widely imposed subgaussian assumption on the predictors, any bounded transformation function \(h(Y)\) would lead the transformed error term \(e\) being subgaussian, even if the original error term \(\) in the single index model \(Y=g(^{},)\) comes from Cauchy distribution. Further as noted by , in the empirical distribution function, the term \(_{j=1}^{n}I(Y_{j} Y_{i})\) is the rank of \(Y_{i}\). Since statistics with ranks such as Wilcoxon test and the Kruskal-Wallis ANOVA test, are well-known to be robust, this then intuitively explains why our procedures with response-distribution transformation are robust with respect to outliers in response. Moreover the distribution function is very easy to estimate and thus our approach is straightforward to implement and understand.

Our test statistic for group inference of \(^{}_{0j}\) relies on individual inference of \(^{}_{0j}:_{hj}=0\). Now we first consider individual hypothesis \(^{}_{0j}\). Let \(_{j}\) be the subvector of \(\) without \(X_{j}\), and \(_{j}\) be the subvector of \(_{h}\) without \(_{hj}\). Suppose that \(\{_{i},Y_{i}\}_{i=1}^{n}\) is a random sample from the population \((,Y)\). Similarly, we denote \(_{ij}\) as the sample of \(_{j}\). Define

\[_{h}=(^{})^{-1}[ \{F(Y)-1/2\}]_{j}=(_{j}_{j}^{})^{-1}(_{j}X_{j}),\]

where \(_{h}\) is the regression coefficient of the model (2.3) with \(h=F\).

Our approach is based on the idea of orthogonalization. The main idea of orthogonalization is to construct a statistic for target parameter which is locally insensitive to the nuisance parameters. Dealing with high-dimensional models, it plays an important role to make the statistic of target parameter immune to the bias from the estimators of high-dimensional nuisance parameters, which in turn enables the statistical inference of parameter of interest. For relevant references, see for example, ,  and .

As pointed by , the adoption of orthogonalization is nontrivial for high-dimensional semiparametric setting, particularly for index model, where the challenge of bundled parameter arises. Fortunately, with the Proposition 2.1, the SIM can be recast into a pseudo-linear model with transformed response (2.3). Note that under the null hypothesis \(_{0j}\)

\[[\{F(Y)-1/2-_{j}^{}_{j}\} (X_{j}-_{j}^{}_{j})]=0.\]

Further the above equation has the orthogonality property

\[_{j}}[\{F(Y)-1/2- _{j}^{}_{j}\}(X_{j}-_{j}^{ }_{j})]=[_{j}(X_{j}- _{j}^{}_{j})]=0.\]

This then motivates us to consider the following quantity

\[T_{nj}^{}=}_{i=1}^{n}\{F(Y_{i})-1/2-_{ij}^{}_{j}\}(X_{ij}-_{ij}^{}_{j}).\] (2.4)

In practice, we need to use suitable estimates of \(F\), \(_{j}\) and \(_{j}\) in (2.4) as those quantities are unknown. Naturally, \(F_{n}(y)=n^{-1}_{i=1}^{n}I(Y_{i} y)\), the empirical distribution of \(Y_{1},,Y_{n}\), can be used to estimate \(F(Y)\). For \(_{j}\), we estimate it by the penalized least-squares method

\[}_{j}=_{_{j}^{p-1}}_{i=1}^{n}(X_{ij}-_{ij}^{}_{j})^{2}+ _{l=1}^{p-1}p_{_{X}}(|_{jl}|),\] (2.5)

where \(p_{_{X}}()\) is a penalty function with a tuning parameter \(_{X}\). Similarly, for \(_{j}\), we adopt the following penalized least-squares

\[}_{h}=_{_{h}^{p}}_{n }(_{h})+_{l=1}^{p}p_{_{Y}}(|_{hl}|),\] (2.6)where \(_{n}(_{h})=(2n)^{-1}_{i=1}^{n}\{F_{n}(Y_{i})-1/2- _{i}^{}_{h}\}^{2}\) and \(p_{_{Y}}()\) is a penalty function with a tuning parameter \(_{Y}\). The \(}_{j}\) is then set as the subvector of \(}_{h}\) without \(_{hj}\). Accordingly, for each individual hypothesis \(_{0j}^{}:_{hj}=0\) we define the standardized test statistic

\[_{nj}=_{j}}_{i=1}^{n}\{F_{n }(Y_{i})-1/2-_{ij}^{}}_{j}\}(X_{ij}- _{ij}^{}}_{j}),\] (2.7)

where

\[_{j}^{2}=_{i=1}^{n}\{(X_{ij}- _{ij}^{}}_{j})_{ij}+_{j}(Y_{i}) \}^{2},\] (2.8)

\(_{ij}=F_{n}(Y_{i})-1/2-_{ij}^{}}_{j}\) and \(_{j}(y)=n^{-1}_{i=1}^{n}(X_{ij}-_{ij}^{}}_{j})\{I(Y_{i} y)-F_{n}(Y_{i})\}\). Note that \(F_{n}(Y_{i})=n^{-1}_{j=1}^{n}I(Y_{j} Y_{i})\). Then given predictors \(_{i}\)'s being fixed, perturbations in the responses would not make the value of \(_{nj}\) change as long as the ranks of \(Y_{i}\)'s remain unchanged.

Denote \(}_{n,}=(_{nj})_{j}\). To test the null hypothesis \(_{0,}\), we consider test statistic based on the max norm of \(}_{n,}\). That is,

\[M_{n,}=_{j}_{nj}^{2}.\] (2.9)

Based on the limiting null distribution obtained in subsection 3.2, we can reject null hypothesis \(_{0,}\) at the significant level \(\) if and only if \(M_{n,} c_{}()\), where \(c_{}()=2 p_{0}- p_{0}+q_{}\) and \(q_{}\) is the \(1-\) quantile of the Gumbel distribution with the cumulative distribution function \(\{-}(-x/2)\}\), that is,

\[q_{}=-()-2(1-)^{-1}.\] (2.10)

Our inference procedure is summrized in the Algothrim 1 as follows.

```
0: Covariates data \(\{_{i}\}_{i=1}^{n}\), response data \(\{Y_{i}\}_{i=1}^{n}\).
0: Testing results for group inference problem (1.2).
1: Compute the penalized least-square estimator \(}_{h}\) defined in (2.6);
2:for\(j\)do
3: compute estimator \(}_{j}\) and \(}_{j}\) defined in (2.5);
4: calculate the standard statistic \(_{nj}\) defined in (2.7);
5: calculate the test statistic \(M_{n,}\) defined in (2.9) ;
6:if\(M_{n,} c_{}()\), reject \(_{0,}\); otherwise accept \(_{0,}\). ```

**Algorithm 1**Group inference based on distribution transformation

**Remark 2.1**.: _The use of quantiles of limiting null distribution in \(c_{}()\) is attractive from a computational point of view. On the other hand, the validity of limiting null distribution requires additional assumptions regarding the dependence structure of the components of the covariates \(\). See Assumption A.5 in Appendix A.8 for details. Besides, it is well known that this weak convergence is typically slow. To solve these problems, we propose a multiplier bootstrap approach and show its validity in theory, please see Appendix A.1 for details._

## 3 Theoretical properties

### Estimation error bound

The theoretical analysis of \(}_{h}\) requires a substantial modification of the proof technique as compared to existing works on high-dimensional inference. It is related to the fact that empirical distribution \(F_{n}(Y_{i})\) are dependent, and thus \(_{n}(_{h})\) is a sum of dependent random variables. Note that  considered a similar penalized procedure with LASSO penalty. While  and  considered non-convex penalties such as SCAD and MCP to reduce estimation bias incurred by LASSO. However,their results only allow the dimension to be polynomial order of the sample size and thus cannot work in ultrahigh-dimensional setting. Further their results only provide guarantees for global optima. In this paper, we consider both convex and non-convex penalties and establish error bound of any local optimum of the empirical objective.

To be specific, assume that \(}_{h}\) satisfies the first-order necessary condition to be a local minimum of 2.6, that is,

\[_{n}(}_{h})+ p_{_{Y}} (}_{h}),-}_{h} 0, \ ^{p}.\] (3.11)

Let \(s_{Y}\) be the sparsity level for \(_{h}\), i.e., \(s_{Y}=\|_{h}\|_{0}\). For any vector \(}_{h}\) satisfying the condition (3.11), we have the following result.

**Theorem 3.1**.: _Under Assumptions A.1 and A.2 in Appendix A.8, the \(}_{h}\) defined in (3.11) satisfies_

\[\|}_{h}-_{h}\|_{2} c_{Y}} and \|}_{h}-_{h}\|_{1} c^{}_{Y\,SY}\]

_with probability at least \(1-c_{1}(-c_{2} p)\). Here, \((c,c^{},c_{1},c_{2})\) are universal constants and \(_{X},_{Y}\)._

In Theorem 3.1 we establish error bound of penalized least-squares estimators with empirical distribution function of the response. For consistency in \(L_{2}\)-loss, we require the sparsity level \(s_{Y}\) satisfy that \(s_{Y}=o(n/ p)\). While in terms of \(L_{1}\)-loss, the limitation becomes \(s_{Y}=o()\). Clearly the sparsity level is allowed to be diverging. Our results unify both convex  and non-convex penalties [60; 51]. Compared with the results in  and , the dimension can be exponential order of the sample size, and no minimal signal condition is imposed to obtain those error bounds. In the proof, we modify Hoeffding's inequality by a probability inequality for \(U\)-statistic to handle tail probability in dependent case, which may be interesting in their own rights.

### Asymptotic null distribution

In this subsection, we derive the asymptotic null distribution for statistic defined in (2.9). Denote \(=\{_{h}^{p}:\|_{h}\|_{0}_ {1 j p}\|_{j}\|_{0} s\}\) and \(_{h}=(_{hj})_{j}\). We consider the following parameter space for \(_{}\)

\[_{}^{0}=\{_{h}^{p}:_{h }=\}.\]

**Theorem 3.2**.: _Suppose that Assumptions A.1-A.6 in Appendix A.8 and LC condition hold. If \(s=o(/( p p_{0}))\), then for given \(t\) we have_

\[_{(n,p_{0})}_{_{h}_{}^{0}}| M_{n,}-2 p_{0}+ p_{0} t- -}(-t/2)}|=0.\]

Theorem 3.2 implies that the type I error of the proposed test statistic \(M_{n,}\) converges to any pre-specified significance level uniformly over \(_{h}_{}^{0}\). The hypothesis test with such uniform convergence property is called _honest_ test. The advantage of honest test is that the limiting distribution of our procedure is uniformly valid over \(s\)-sparse high-dimensional models despite the possible imperfect model selection via estimator \(}_{h}\). An immediate implication is that it relaxes the assumption on signal strength and does not require the zero and nonzero effects to be well-separated. In particular, this procedure does not require the initial estimator to select zero and nonzero signals perfectly, which is nearly impossible in practice. Due to these excellent statistical properties, honest test has recently drawn lots of attention. See , , , , ,  for further examples.

On the basis of Theorem 3.2, we can reject null hypothesis \(_{0,}\) at the significant level \(\) if and only if \(M_{n,} c_{}()\), where \(c_{}()=2 p_{0}- p_{0}+q_{}\) and \(q_{}\) is defined in (2.10).

### Power analysis

We next to consider the asymptotic power analysis of the \(M_{n,}\). In this section, we show that our test statistic is powerful in the sense that the separation rate is of order \() p_{0}/n}\) for some \(_{0}>0\) when \(p_{0}\). At the beginning we define the following parameter space for \(_{1,}\)

\[_{}^{1}(c_{0})=\{_{h}^{p}:_{j }_{j}}{_{j}} }{n}}\},\]where \(c_{0}\) is a positive constant and \(_{j}=X_{j}^{2}-X_{j}_{j} ^{}_{j}_{j}^{}^{-1} _{j}X_{j}\).

**Theorem 3.3**.: _Suppose that conditions in Theorem 3.2 are satisfied, we have for some \(_{0}>0\),_

\[_{(n,p_{0})}_{_{h}_{0}^{1}(2+ _{0})}M_{n,} c_{}()=1.\] (3.12)

Theorem 3.3 implies that our test procedure can still be powerful even when there exists few components of \(_{}\) with a magnitude being larger than \() p_{0}/n}\). Therefore our testing procedure is powerful against "sparse" alternative aforementioned. This separation rate is widely discussed in the literature, such as  and . Theorem 3.3 shows our tests based on distribution transformation can also achieve this lower bound.

**Remark 3.1**.: _In the proof of Theorem 3.3, we show that_

\[M_{n,} M_{}^{0}+_{j} _{j}}{_{j}}^{2},\]

_where the \(M_{}^{0}\) has the type I extreme limiting distribution, which is mentioned in Theorem 3.2. Then the power is largely determined by the second term_

\[(M_{n,})=_{j}_{j}}{_{j}}^{2},\] (3.13)

_and it can be explained as the maximum of the signal-to-noise ratio for \(j\). The power of the test based on \(M_{n,}\) increases with the growth of \((M_{n,})\). When \((M_{n,})(2+_{0}) p_{0}/n\), the asymptotic power is tending to 1._

In this paper, we focus on the distribution transformation of the response. In the following we make some comparisons between this choice and another natural one. Actually, same to , one may consider use \(h(Y)=Y\) to conduct the following test statistic

\[_{nj}^{+}=_{j}^{+}}_{i=1}^{n}(Y_ {i}-_{ij}^{}}_{j}^{+})(X_{ij}-_{ij}^{}}_{j}).\] (3.14)

Here \(}_{j}^{+}\) is the subvector of \(}_{h}^{+}\) without \(j\)-th component, and \(}_{h}^{+}\) is a penalized least-squares estimator of \(_{h}^{+}=(^{})^{-1} (Y)\) as follows:

\[}_{h}^{+}=_{_{h}^{+} ^{p}}_{i=1}^{n}(Y_{i}-_{i}^{} {}_{h}^{+})^{2}+_{l=1}^{p}p_{_{Y}}(|_{hl}^{+}|).\]

\(}_{j}\) is estimated by (2.5). And \(_{j}^{+2}\) is an appropriate estimator of \(_{j}^{+2}\)\(:=\)\(\{(X_{j}-_{j}^{}_{j})^{2}(Y-_{j}^{}_{j}^{+})^{2}\}\). For the group inference problem (1.2), it's natural to consider the following testing statistic:

\[M_{n,}^{+}=_{j}(_{nj}^{+})^{2},\]

Now we discuss the power properties of \(M_{n,}^{+}\). By similar arguments as Theorem 3.3, the power of the test based on \(M_{n,}^{+}\) is determined by

\[(M_{n,}^{+})=_{j}^{+}_{j}}{_{j}^{+}}^{2}.\] (3.15)

For the convenience of illustration, we consider the classical SIM, that is, \(Y=g_{1}(^{})+\) and \(\) is independent of \(\). To compare the theoretical power of \(M_{n,}\) and \(M_{n,}^{+}\), it suffices to compare the SNRs aforementioned. In the classical SIM case, the ratio of SNR can be simplified as

\[(M_{n,})}{(M_{n,}^{+})} c _{3}(),\]

where \(c_{3}\) is a constant depending on \((,Y)\). The detailed form of \(c_{3}\) is presented in the Appendix A.2. When \(()>c_{3}^{-1}\), \(M_{n,}\) is more powerful than \(M_{n,}^{+}\). Actually when \(\) follows a heavy-tail distribution, \(()\) can be very large even be infinite.

The above results illustrate the robustness of our test statistic \(M_{n,}\). In fact, with distribution transformation, no moment assumption on \(\) is required. Even if the \(\) comes from Cauchy distribution, our test procedure still works well but the \(M_{n,}^{+}\) requires sub-Gaussian assumption on the error term and thus in this situation would fail.

Numerical studies

In this section, extensive simulation studies are carried out to evaluate the numerical performance of the proposed methods for the global inference problem described in (1.2). We consider the high-dimensional SIM in equation (1.1) and generate the data from the following models:

**Model 1**: Linear model: \(Y=^{}+\).

**Model 2**: Non-linear model: \(Y=(^{}+)\).

Here, the predictors \(_{i},i=1,,n\) are generated from multivariate normal distribution \(N_{p}(_{p},)\). The covariance matrix \(^{p p}\) is block diagonal, that is, \(=(_{1},,_{1 0})\), where \(_{1}\) is \(\) dimensional identity matrix, and each \(_{k}\) is \(\) dimensional with an AR\((1)\) correlation structure, that is \((_{k})_{ij}=(0.1k-0.1)^{|i-j|},k=2,,10\); \(i,j=1,,p/10\). We consider two different error distributions for \(\) which is independent of \(\): (1) standard normal distribution \(N(0,1)\); (2) Cauchy distribution or equivalently Student's \(t\) distribution with 1 degree of freedom, \(t(1)\). Without loss of generality, we set the active set be \(\{1,2,,6\}\). The regression coefficients \(\) are generated from an arithmetic sequence from \(0.1\) to \(2\), that is \(_{j}=0.2+0.038(j-1)\) for \(1 j 6\) and \(_{j}=0\) otherwise. We consider \(n=200,500\) and \(p=800\). We use Intel(R) Xeon(R) Silver 4208 CPU @ 2.10GHz.

In order to explore the robustness behaviors of our proposed statistics, we add outliers to pollute the observations: \(p_{}\) of the responses are picked at random and increased by \(m_{}\)-times maximum of original responses, shorted as \(p_{}+m_{}()\). Here \(p_{}\) is the proportion of outliers and \(m_{}\) is a pre-set constant standing for outlier strength. Throughout the simulation study, we analyze the results with both the original data (shorted as Ori.) and the data with outliers (shorted as Out.).

We assess the empirical type I error and the empirical power of group inference with the nominal level \(=0.05\) based on 500 simulation runs. We consider the following five different choices for \(\) to test the hypothesis (1.2):

\[_{1} =\{10,11,p-4,p-3,p-2,p-1,p\},\] \[_{2} =\{3,,6\}_{1},\] \[_{3} =\{10,,59,p-149,,p\},\] \[_{4} =\{3,,6\}_{3}.\]

Note that \(_{1}\) and \(_{2}\) are small groups, \(_{3}\) and \(_{4}\) are large groups. \(_{1}\) and \(_{3}\) consist of only zero coefficients, while \(_{2}\) and \(_{4}\) includes nonzero elements.

Then we report the numerical results of empirical rejection rate (ERR) for different scenarios, where ERR is the proportion of rejected hypotheses among the total 500 simulations. For \(_{1}\) and \(_{3}\), the ERR is the empirical type I error; For \(_{2}\) and \(_{4}\), the ERR is the empirical power.

From the numerical results for all scenarios displayed in table 1, we have the following findings. Firstly, when there are outliers or heavy errors, our methods still have very high powers and control empirical sizes well. Secondly, for both linear model and nonlinear model, small groups and large groups, our procedures have similar satisfactory performance in this experiment. Thirdly, we find that the dimensionality of all predictors \(p\) does not effect the empirical performances. Our test procedures with LASSO penalty perform similarly to those with SCAD or MCP. Therefore, we only present the results of test statistics with LASSO penalty. We use the R-package ncvreg.

Moreover, we compare the proposed methods with the three-step testing procedure based on the studentized statistics in , denoted as ST. Here, we use R package SILM to implement the three-step testing procedure, set the number of bootstrap replications as 500 and choose the splitting proportion of 30% for screening.

Table 2 summarizes the results of empirical type I errors and powers with \(=0.05\) for different methods and models based on 500 simulation runs (due to the computation limit, we only run 200 times for ST). Here we consider \((n,p)=(200,400)\), and other settings are same as before. It is obvious that our methods outperform the ST. Firstly, when there are no outliers and heavy errors, all methods have empirical power 1 for linear model. But for non-linear model, our test procedures \(T_{}^{(1)}\) and \(T_{}^{(2)}\) are more powerful than ST. For instance, under non-linear model with original data and \( N(0,1)\), the empirical powers of ST for \(_{2}\) and \(_{4}\) are 0.785 and 0.430, while the corresponding

[MISSING_PAGE_FAIL:9]

robustness. It is also of interest to make inference for partially linear single-index regression model . We will pursue these challenging problems in near future.