# A Near-optimal Algorithm for Learning Margin Halfspaces with Massart Noise

Ilias Diakonikolas

Department of Computer Sciences

UW-Madison

Madison, WI

ilias@cs.wisc.edu

&Nikos Zarifis

Department of Computer Sciences

UW-Madison

Madison, WI

zarifis@wisc.edu

###### Abstract

We study the problem of PAC learning \(\)-margin halfspaces in the presence of Massart noise. Without computational considerations, the sample complexity of this learning problem is known to be \((1/(^{2}))\). Prior computationally efficient algorithms for the problem incur sample complexity \((1/(^{4}^{3}))\) and achieve 0-1 error of \(+\), where \(<1/2\) is the upper bound on the noise rate. Recent work gave evidence of an information-computation tradeoff, suggesting that a quadratic dependence on \(1/\) is required for computationally efficient algorithms. Our main result is a computationally efficient learner with sample complexity \((1/(^{2}^{2}))\), nearly matching this lower bound. In addition, our algorithm is simple and practical, relying on online SGD on a carefully selected sequence of convex losses.

## 1 Introduction

This work studies the algorithmic task of learning margin halfspaces in the presence of Massart noise (aka bounded label noise)  with a focus on fine-grained complexity analysis. A halfspace or Linear Threshold Function (LTF) is any Boolean-valued function \(h:^{d}\{ 1\}\) of the form \(h()=(-)\), where \(^{d}\) is the weight vector and \(\) is the threshold. The function \(:\{ 1\}\) is defined as \((t)=1\) if \(t 0\) and \((t)=-1\) otherwise. The problem of learning halfspaces with a margin -- i.e., under the assumption that no example lies too close to the separating hyperplane -- is one of the earliest algorithmic problems studied in machine learning, going back to the Perceptron algorithm .

In the realizable PAC model  (i.e., with clean labels), the sample complexity of learning \(\)-margin halfspaces on the unit ball in \(^{d}\) is \((1/(^{2}))\), where \(>0\) is the desired 0-1 error; see, e.g., 1. Moreover, the Perceptron algorithm is a computationally efficient learner achieving this sample complexity. That is, without label noise, there is a sample-optimal and computationally efficient learner for margin halfspaces.

In this paper, we study the same problem in the Massart noise model that we now define.

**Definition 1.1** (PAC Learning with Massart Noise).: Let \(D\) be a distribution over \(\{ 1\}\), and let \(\) be a class of Boolean-valued functions over \(\). We say that \(D\) satisfies the \(\)-Massart noise condition with respect to \(\), for some \(<1/2\), if there exists a concept \(f\) and an unknown noise function \(():[0,]\) such that for \((,y) D\), the label \(y\) satisfies: with probability \(1-()\), \(y=f()\); and \(y=-f()\) otherwise. Given i.i.d. samples from \(D\), the goal of thelearner is to output a hypothesis \(h:\{ 1\}\) such that with high probability the 0-1 error \(_{D}(h)}}{{=}}_{( ,y) D}[h() y]\) is small.

The concept class of halfspaces with a margin is defined as follows.

**Definition 1.2** (\(\)-Margin Halfspaces).: Let \(D\) be a distribution over \(^{d-1}\{ 1\}\), where \(^{d-1}\) is the unit sphere in \(^{d}\). Let \(^{*}^{d-1}\) and \((0,1)\). We say that the distribution \(D\) satisfies the \(\)-margin condition with respect the halfspace \((^{*})^{2}\), if (i) for \((,y) D\), we have that \(y=(^{*})\), and (ii) \(_{(,y) D}[|^{*}|< ]=0\). The parameter \(\) is called the margin of the halfspace \((^{*})\).

Information-theoretically, the best possible 0-1 error attainable for learning a concept class with Massart noise is \(:=_{ D_{}}[()]\). Since \(()\) is uniformly bounded above by \(\), it follows that \(\); also note that it may well be the case that \(\). Focusing on the class of \(\)-margin halfspaces, it follows from  that there exists a (computationally inefficient) estimator achieving error \(+\) with sample complexity \((1/((1-2)^{2}))\); and moreover that this sample upper bound is nearly best possible (within a logarithmic factor) for any estimator. (That is, the sample complexity of the Massart learning problem is essentially the same as in the realizable case, as long as \(\) is bounded from \(1/2\).)

Taking computational considerations into account, the feasibility landscape of the problem changes. Prior work  has provided strong evidence that achieving error better than \(+\) is not possible in polynomial time. Consequently, algorithmic research has been focusing on achieving the qualitatively weaker error guarantee of \(+\). We note that efficiently obtaining any non-trivial guarantee had remained open since the 80s; see Appendix A.1 for a discussion. The first algorithmic progress for this problem is due to , who gave a polynomial-time algorithm achieving error of \(+\) with sample complexity \((1/,1/)\). Subsequent work  gave an efficient algorithm with improved sample complexity of \((1/(^{4}^{3}))\). Prior to the current work, this remained the best known sample upper bound for efficient algorithms.

In summary, known computationally efficient algorithms for learning margin halfspaces with Massart noise require significantly more samples--namely, \((1/(^{4}^{3}))\)--than the information-theoretic minimum of \(_{}(1/(^{2}))\). It is thus natural to ask whether a polynomial-time algorithm with optimal (or near-optimal, i.e., within logarithmic factors) sample complexity exists. Recall that the answer to this question is affirmative in the realizable setting, where the Perceptron algorithm is optimal. Perhaps surprisingly, recent work  (see also ) gave evidence for the existence of inherent _information-computation tradeoffs_ in the Massart noise model--in fact, even in the simpler model of Random Classification Noise (RCN) 3. Specifically, they showed that any efficient Statistical Query (SQ) algorithm or low-degree polynomial tasks requires \((1/^{2})\) samples--a near quadratic blow-up compared to the \((1/)\) information-theoretic upper bound. This discussion serves as the motivation for the following question:

_What is the optimal_ computational sample complexity _of the problem of_

_learning \(\)-margin halfspaces with Massart noise?_

By the term "computational sample complexity" above, we mean the sample complexity of polynomial-time algorithms for the problem. Given the fundamental nature of this learning problem, we believe that a fine-grained sample complexity versus computational complexity analysis is interesting on its own merits. _In this work, we develop a computationally efficient algorithm with sample complexity of \((1/(^{2}^{2}))\)._ Given the aforementioned information-computation tradeoffs, there is evidence that this upper bound is close to best possible. As a bonus, our algorithm is also simple and practical, relying on online SGD. (In fact, our algorithm runs in sample linear time, excluding a final testing step that slightly increases the runtime.)

### Our Result and Techniques

Our main result is the following:

**Theorem 1.3** (Main Result, Informal).: _Let \(D\) be a distribution on \(^{d-1}\{ 1\}\) that satisfies the \(\)-Massart noise condition with respect to an unknown \(\)-margin halfspace \(f()=(^{*})\). There is algorithm that draws \(n=(1/(^{2}^{2}))\) samples from \(D\), runs in time \((dn/)\), and with probability at least \(9/10\) returns a vector \(}\) such that \(_{D}(})+\)._

The sample upper bound of Theorem 1.3 nearly matches the computational sample complexity of the problem (for SQ algorithms and low-degree polynomial tests), which was shown to be \((1/(^{2})+1/(^{2}))\). That is, Theorem 1.3 comes close to resolving the fine-grained complexity of this basic task. Moreover, it matches known algorithmic guarantees for the easier case of Random Classification Noise .

Independent WorkIndependent work  obtained a learning algorithm for \(\)-margin halfspaces with essentially the same sample and computational complexity as ours.

Brief Overview of TechniquesHere we provide a brief summary of our approach in tandem with a comparison to prior work. The algorithm of  adaptively partitions the space into polyhedral regions and uses a different linear classifier in each region, each achieving error \(+\) within the corresponding region. Their approach leverages the LeakyReLU loss (see (1)) as a convex proxy to the 0-1 loss. At a high-level, their approach reweights the samples in order to accurately classify a non-trivial fraction of points.  uses the LeakyReLU loss to efficiently identify a region where the value of the loss conditioned on this region is sub-optimal; they then use this procedure as a separation oracle along with online convex optimization (see also ) to output a linear classifier with 0-1 error at most \(+\). Both of these approaches inherently require \((1/^{3})\) samples for the following reason: they both need to condition on a region where the probability mass of the distribution can be as small as \(()\). Thus, even estimating the error of the loss would require at least \((1/^{2})\) conditional samples. Beyond the dependence on \(1/\), the sample complexity achieved in these prior works is also suboptimal in the margin parameter \(\); namely, \((1/^{4})\). This dependence follows from the facts that both of these works require estimating the loss in each iteration within error of at most \(\), and that their algorithmic approaches require \((1/^{2})\) iterations.

To circumvent these issues, novel ideas are required. At a high-level, we design a uniform approach to decrease the "global" error, as opposed to the local error (as was done in prior work). Specifically, we construct a different sequence of convex loss functions, each of which attempts to accurately simulate the 0-1 objective. We note that a similar sequence of loss functions was used in the recent work  in a related, but significantly different, adversarial online setting. Interestingly, a similar reweighting scheme was used in  for learning general Massart halfspaces. Beyond this similarity, these works have no implications for the sample complexity of our problem. (See Appendix A.2 for a detailed comparison.) Via this approach, we obtain an iterative algorithm which uses only \(O_{}(1/^{2})\) samples in order to estimate the loss in each iterative step.

In more detail, note that the 0-1 loss can be written in the form \(-[y_{}{||}}]\). We convexify this objective by considering, in each step, the loss \((,)=-\,[y_{} {||}}]\), where \(\) is independent of \(\); this loss is convex with respect to \(\). Observe that \((,)\) is proportional to the zero-one loss of \(\). Unfortunately, it is possible that no optimal vector \(^{*}\) (under 0-1 loss) minimizes \((^{*},)\). For this reason, we consider the objective \(_{}(,)=[(\,\{y( )\}--)||/| |]\). This new objective satisfies the following: \(_{}(^{*},)<-\) for any vector \(\) and any \(^{*}\) that minimizes the 0-1 objective; and \(_{}(,)\) as long as \(\) incurs 0-1 error at least \(+\). By the convexity of \(_{}(,)\), this allows us to construct a separation oracle. Namely, we draw enough samples so that \(_{}(,)-_{}(^{*},)/2\), where \(\) is the emprical version of the loss. Due to the nature of these objectives, \(O_{}(1/^{2})\) samples per iteration suffice for this purpose. This in turn implies that the cutting planes method efficiently finds a near-optimal weight vector after \(O((1/)/^{2})\) iterations. Overall, this approach leads to an efficient algorithm with sample complexity \(_{}(1/^{2})\). To get the desired sample complexity of \((1/(^{2}^{2}))\), more ideas are needed.

In the previous paragraph, we hid an obstacle that makes the above approach fail. Specifically, it may be possible that, for many points \(\), the value of \(||\) is arbitrarily small. To fix this issue, we consider a clipped reweighting as follows: \(^{}_{}(,)=[(\,\{y {sign}()\}--)|}{(||,)}]\). This clipping step is not a problem for us, because the target halfspace \((^{*})\) was assumed to have margin \(\). This guarantees that the difference between the expected (over \(y\)) pointwise losses at \((,)\) and \((^{*},)\) is at least \(\) on the points \(\) where \(||\). Indeed, when this is the case, then \(|^{*}|/|| 1\). Overall, this suffices to guarantee that \(^{}_{}(,)-^{}_{}(^{*}, )\).

### Notation

For \(n_{+}\), let \([n]}{{=}}\{1,,n\}\). We use small boldface characters for vectors. For \(^{d}\) and \(i[d]\), \(_{i}\) denotes the \(i\)-th coordinate of \(\), and \(\|\|_{2}}{{=}}(_{i=1}^{d} _{i}^{2})^{1/2}\) denotes the \(_{2}\)-norm of \(\). We will use \(\) for the inner product of \(,^{d}\). For a subset \(S^{d}\), we define the \(_{S}\) operator that maps a point \(^{d}\) to the closest point in the set \(S\). For \(a,b\), we denote \(W(a,b)}{{=}}1/(a,b)\). We will use \(_{A}\) to denote the characteristic function of the set \(A\), i.e., \(\{ A\}=1\) if \( A\), and \(\{ A\}=0\) if \( A\). For \(A,B\), we write \(A B\) (resp. \(A B\)) to denote that there exists a universal constant \(C>0\), such that \(A CB\) (resp. \(A CB\)).

We use \(_{x D}[x]\) for the expectation of the random variable \(x\) with respect to the distribution \(D\) and \([]\) for the probability of event \(\). For simplicity, we may omit the distribution when it is clear from the context. For \((,y) D\), we use \(D_{}\) for the marginal distribution of \(\) and \(D_{y}()\) for the distribution of \(y\) conditioned on \(\). We use \(_{N}\) to denote the empirical distribution obtained by drawing \(N\) i.i.d. samples from \(D\). We use \(_{D}()\) to denote the 0-1 error of the halfspace defined by the weight vector \(\) with respect to the distribution \(D\), i.e., \(_{D}()}{{=}} _{(,y) D}[( ) y]\). We will use \((,)\) for the 0-1 error of \(()\) conditioned on \(\), i.e., \((,):=_{ D_{y}( )}[() y]\). Note that \(_{D}()=_{ D_{} }[(,)]\). If \(D\) satisfies the \(\)-Massart noise condition with respect to the halfspace \(()\), then \((,)=()\{ ()=(^{*})\}+(1-())\{( )(^{*} )\}\).

## 2 Our Algorithm and its Analysis: Proof of Theorem 1.3

In this section, we prove our main result. Algorithm 1 efficiently learns the class of margin halfspaces on the unit ball, in the presence of Massart noise, with sample complexity nearly matching the information-computation limit. Additionally, its runtime is linear in the sample size, excluding a final testing step to select the best hypothesis.

At a high-level, our algorithm leverages a carefully selected convex loss (or, more precisely, a sequence of convex losses) -- serving as a proxy to the 0-1 error. A common loss function, introduced in this context by  and leveraged in , is the LeakyReLU function. This is the univariate function \(_{}(t)=(1-)\{t 0\}t+ \{t<0\}t\), where \((0,1)\) is the leakage parameter (that needs to be selected carefully). Roughly speaking, the convex function \(_{}(,,y)=_{}(-y( ))\) can be viewed as a reasonable proxy to the 0-1 loss of the halfspace \(()\) on the point \((,y)\). To see this, note that (see, e.g., Claim C.1)

\[_{}(,,y)=(\{( ) y\}-)||\.\] (1)

Observe that a point \(\) that is classified correctly by the halfspace \(()\) will satisfy

\[_{y D_{y}()}[\{( ) y\}]-||= (()-)||\]

which is non-positive for \(()\). Since the only guarantee we have is that \(()\), this suggests that we need to select \(\). It turns out that \(:=\) is the optimal choice. We fix the choice of \(:=\) throughout. On the other hand, if (the halfspace defined by) \(\) misclassifies the point \(\), this term becomes non-negative.

The factor \(||\) in Equation (1) reweights the 0-1 error so that points \(\) for which \(||\) is sufficiently large (i.e., close to \(1\)) have to be classified correctly by a minimizer of \(_{(,y) D}[_{}(,,y)]\). On the other hand, points closer to the separating hyperplane defined by \(\), or points where \(()\) is close to \(=\), are not guaranteed to be classified correctly by the minimizer of this loss. We leverage this insight to construct a sequence of loss functions that reweight the points so that, to minimize the regret, we need to classify a large fraction of points; this leads to the desired error of \(+\) with near-optimal sample complexity.

We now provide some intuition justifying our choice of surrogate loss functions. Observe that if we instead could minimize the function

\[,y) D}{}[_{}(, ,y)/||]=,y) D}{ }[(\{() y \}-)]\,\] (2)

with respect to \(\), we would obtain a halfspace with minimum 0-1 error; unfortunately, this reweighted loss is just a shift of the 0-1 loss, hence non-convex. To fix this issue, instead of reweighting by\(1/||\), we will reweight by \(W(,)}}{{=}}1/ (||,)\), where \(\) is the margin parameter and \(\) is an appropriately chosen vector that is independent of \(\). The new loss is defined as follows:

\[_{,}()}}{{=}},y) D}{}[_{ }(,,y)W(,/2)]\;,\] (3)

where for technical reasons we use \(/2\) instead of \(\) in the maximum.

Since the parameter \(\) is independent of \(\), the loss \(_{,}()\) remains convex in \(\). At the same time, by carefully choosing \(\), we can accurately simulate the non-convex 0-1 loss. Note that our reweighting term is a maximum over two terms. The reason for this choice is that, for some points \(\), the quantity \(||\) can be arbitrarily small; taking the maximum avoids the loss becoming very large. In particular, the loss \(_{,}()\) will be guaranteed to remain in a bounded length interval.

Our algorithm proceeds in a sequence of iterations. In the \((t+1)\)-st iteration, it sets \(\) to be \(^{t}\), where \(^{t}\) is the weight vector of step \(t\). This choice attempts to simulate the 0-1 error at \(^{t}\), as is suggested by Equation (2). Assume for simplicity that our current hypothesis is the halfspace defined by \(\) and is such that \(_{ D_{}}[\{| |/2\}]=0\). Note this implies that \(W(,/2)=1/||\). By combining Equations (2) and (3), we get that \(_{,}()=_{D}()-\); note that as long as \(_{D}()+\), we have that \(_{,}()\). On the other hand, the optimal halfspace \(^{*}\) achieves a non-positive loss; from Equations (1) and (2), we have that

\[_{,}(^{*}) =,y) D}{}[(\{ (^{*}) y\}-)|^{*} |W(,/2)]\] \[= D_{}}{}[(( )-)|^{*}|W(,/2)] 0\;,\]

where the inequality follows from the fact that \(()\). Recalling that \(_{,}()\) is convex, if we run an Online Convex Optimization (OCO) algorithm, after \(T\) steps we are guaranteed to find a vector \(\) such that \(_{,}()-_{,}( ^{*}) O(1/)\). For \(T=O(1/^{2})\), this gives that \(_{,}()</2\); and therefore we would have \(_{D}()<+\). We provide an approach using this idea and the cutting planes algorithm in Appendix B that achieves sample complexity \((1/(^{2}^{4}))\).

Our algorithm and its analysis work only with the gradient of \(_{,}()\). The key novelty is the analysis of the sample complexity. The gradient of \(_{}(,,y)W(,)\) with respect to \(\) has the following explicit form:

\[_{,}(,,,y)}}{{=}}((1-2)()-y)W (,)\,=()-y)}{(||, )}\,.\]

Furthermore, we denote by \(_{D}(,,,)=_{(,y)  D}[_{,}(,,,y)]\).

Before describing our algorithm and proving Theorem 2.1, we simplify our notation. We will omit the parameters \(,\) from the function input (as they are fixed throughout). Therefore, we use \(_{_{N}^{}}(,)_{ _{N}^{}}(,,,)\) and \((,,,y)_{,/2}( ,,,y)\).

Our algorithm is described in pseudocode below.

Algorithm 1 employs online SGD applied to a sequence of convex loss functions. We show that, after a certain number of iterations, the algorithm will find a weight vector achieving 0-1 error at most \(+\). Since the desired vector may not be the last iterate, in the end, our algorithm returns the halfspace that achieves the smallest empirical 0-1 error.

We establish the following result, which implies Theorem 1.3.

**Theorem 2.1** (Main Result).: _Let \(D\) be a distribution on \(^{d-1}\{ 1\}\) satisfying the \(\)-Massart noise condition with respect to the \(\)-margin halfspace \(f()=(^{*})\). Given \(N=((1/())/(1-2))\) and \(T=((1/)/(^{2}^{2}))\), Algorithm 1 returns a vector \(}\) such that \(_{D}(})+\) with probability at least \(1-\). The algorithm draws \(n=O(N+T)\) samples from \(D\) and runs in \(O(dNT)\) time._

The rest of this section is devoted to the proof of Theorem 2.1.

Our algorithm sets \(=^{t}\) in each round, therefore for the rest of the section we proceed by setting \(=\) as arguments of \(\) and \(\).

**Input:** Sample access to a distribution \(D\) supported in \(^{d-1}\{ 1\}\) corrupted with \(\)-Massart noise with respect to a halfspace \((^{*})\) that satisfies the \(\)-margin condition; parameters \(,(0,1)\), and \(N,T_{+}\).

**Output:** Weight vector \(}\) such that \(_{D}(})+\) with probability at least \(1-\).

1. Let \(c>0\) be a sufficiently small universal constant.
2. \(t 0\), \(^{0}_{1}=(1,0,,0)\), and \(T=(1/c)(1/)/(^{2}^{2})\).
3. While \(t T\) do 1. Draw \((^{(t)},y^{(t)})\) sample from \(D\). 2. Set \(_{t} c^{2}\). 3. Update \(^{t}\) as follows: \(\) Update and project in the unit ball \[^{t+1}^{t}-_{t}(^{t },^{t},^{(t)},y^{(t)})^{t+1} ^{t+1}}{(\|^{t+1}\|_{2},1)}\] 4. \(t t+1\).
4. Draw \(N\) samples from \(D\) and construct the empirical distribution \(_{N}\).
5. Return \(}=_{t[T+1]}_{_{N} }(^{t})\).

**Algorithm 1**Learning Margin Halfspaces with Massart Noise

We decompose the stochastic gradient \((,,,y)\) into two parts: \((,,,y)=^{1}(, )+^{2}(,,y)\), where

\[^{1}(,)=(1-2)()-()}{}[y]W( ,/2)\]

and

\[^{2}(,,y)=()}{}[y]-yW(,/2)\.\]

We also use \(^{1}_{_{N}}()\) and \(^{2}_{_{N}}()\) for the same decomposition after taking the empirical expectation, i.e., \(^{1}_{_{N}}()=_{( _{N})}[^{1}(,)]\) and \(^{2}_{_{N}}()=_{(,y) _{N}}[^{2}(,,y)]\).

This serves to decompose the gradient into two parts: one containing the population expectation over the random variable \(y\), and the other containing the error between the empirical estimation of \(y\) and the population version of \(y\). The vector \(^{1}_{_{N}}()\) contains the direction that will decrease the distance between \(\) and \(^{*}\), while \(^{2}_{_{N}}()\) contains the estimation error. To see this, observe that if we take the population expectation of \(^{2}(,,y)\), we will have:

\[,y) D}{}[^{2}(, ,y)]= D_{}}{} (1-2())(^{*})- ()}{}[y]W( ,/2)=0\,\]

where we used that \(_{y D_{y}()}[y]=(1-2())( ^{*})\).

We start by bounding the contribution of \(^{1}_{_{N}}()\) in the direction \(-^{*}\). We show that if instead of the corrupted label \(y\) at the point \(\), we had access to \(_{y D_{y}()}[y]=(1-2())( ^{*})\), then the gradient has a large component in the direction of \(-^{*}\). This effectively implies that \(^{1}_{_{N}}()\) can be used as a separation oracle, separating all the halfspaces with 0-1 error more than \(+\) from the ones with smaller error.

**Lemma 2.2** (Structural Lemma).: _Let \(N_{+}\) and let \(D\) be a distribution on \(^{d-1}\{ 1\}\) satisfying the \(\)-Massart condition with respect to the optimal classifier \(f()=(^{*})\). Let \(^{d}\) be such that \(\|\|_{2} 1\) and let \(\{^{(i)}\}_{i=1}^{N}\) be a multiset of \(N\) i.i.d. samples from \(D_{}\). Then, it holds \(^{1}_{_{N}}()(-^{*})  2(_{_{N}}()-)\,\) where \(_{N}\) is the corresponding empirical distribution._Proof.: We partition \(^{d}\) into two subsets \(R_{1},R_{2}\) as follows: \(R_{1}\) contains the points that lie sufficiently far away from the separating hyperplane \(=0\), i.e., \(R_{1}}}{{=}}\{^{d}:| |/2\}\). \(R_{2}\) contains the remaining points, i.e., \(R_{2}}}{{=}}\{^{d}: ||</2\}\).

We first show that for any \( R_{1}\), the vector \(^{1}(,)\) has a large component parallel to the direction \(-^{*}\). The proof of the claim below can be found in Appendix C.

**Claim 2.3**.: _For any \(^{(i)} R_{1}\), we have that \(^{1}(,^{(i)})(-^{*})  2((,^{(i)})-)\)._

It remains to show that the same holds for all the points in \(R_{2}\). The proof of the claim below can be found in Appendix C.

**Claim 2.4**.: _For any \(^{(i)} R_{2}\), we have that \(^{1}(,^{(i)})(-^{*})  2((,^{(i)})-)\)._

Applying Claim 2.3 and Claim 2.4 for each sample in the set \(\{^{(i)}\}_{i=1}^{N}\), we get that

\[_{i=1}^{N}^{1}(,^{(i)})( -^{*})_{i=1}^{N}((,^{(i)})-)\.\]

This completes the proof of Lemma 2.2. 

By Lemma 2.2, the gradient points towards the direction \(^{t}-^{*}\), in the \(t\)-th iteration. This means that, in fact, the gradient is a subgradient of the potential loss \(()=\|-^{*}\|_{2}^{2}\). This allows us to show convergence, even though it is generally not possible in a sequence of loss functions in the stochastic setting. We are now ready to prove our main result.

Proof of Theorem 2.1.: Let \(T\) be the maximum number of iterations of Algorithm 1. Denote by \(^{t}:=\{(^{(t)},y^{(t)})\}\) the i.i.d. sample drawn from \(D\) in the \(t\)-th iteration, \(t[T]\). Furthermore, let \(_{1},,_{T}\) be the filtration with respect to the \(\)-algebra generated by \(^{1},,^{T}\). We denote by \(H_{t}\) the event that \(_{D}(^{t})+\).

Recall that Algorithm 1 uses the following update rule (see Step (3c)):

\[^{t+1}=_{\{^{d}:\|\|_{ 2} 1\}}(^{t}-_{t}(^{t},^{t}, ^{(t)},y^{(t)}))\,\]

with \(_{t}=c^{2}\), for some sufficiently small absolute constant \(c>0\).

We begin by bounding from above the distance between \(^{t+1}\) and \(^{*}\) from the previous distance between \(^{t}\) and \(^{*}\). We have that

\[\|^{t+1}-^{*}\|_{2}^{2} =\|_{\{^{d}:\|\|_{ 2} 1\}}(^{t}-_{t}(^{t},^{t}, ^{(t)},y^{(t)})-^{*}\|_{2}^{2}\] \[\|^{t}-_{t}(^{t}, ^{t},^{(t)},y^{(t)})-^{*}\|_{2}^{2}\] \[=\|^{t}-^{*}\|_{2}^{2}-2_{t} (^{t},^{t},^{(t)},y^{(t)})(^{t}- ^{*})+_{t}^{2}\|(^{t},^{t}, ^{(t)},y^{(t)})\|_{2}^{2}\,\] (4)

where in the first inequality we used the projection inequality, i.e., \(\|_{B}()-_{B}()\|_{2}\| -\|_{2}\) for any set \(B\). We will decouple the mean of the random variable \((^{t},^{t},,y)\) and make it zero-mean.

To simplify the notation, we denote by \(_{t}:=((^{t},^{t},^{(t)},y^{( t)})-_{D}^{1}(^{t}))(^{t}-^{*})\) and note that \(_{t}\) is a zero-mean random variable over the sample \((^{(t)},y^{(t)})\). Adding and subtracting \(_{D}^{1}(^{t})\) onto Inequality (4) a we get that

\[\|^{t+1}-^{*}\|_{2}^{2}\|^{t}-^{*} \|_{2}^{2}_{D}^{1}(^{t})( ^{t}-^{*})+_{t}^{2}\|(^{t}, ^{t},^{(t)},y^{(t)})\|_{2}^{2}}_{I}_{t}}_{_{t}}\.\] (5)

We now outline the main steps of our analysis. Instead of accurately estimating the gradients in each round, we denote by \(_{t}\) the estimation error from which we bound above their sum. We first add and subtract the population gradient to obtain the \(I\) term, which is the decreasing direction. In this way, we decouple the expected decrease and the error of the approximation (see Claim 2.5). After that, we bound the contribution of the estimation error in Lemma 2.8. Observe that \(_{t}\) is a random variable that corresponds to the estimation error of the gradient. We will argue that with high probability the contribution of \(_{t=1}^{T}_{t}\) is bounded; therefore, our algorithm will converge to an accurate solution.

Lemma 2.2 shows that \(^{1}_{_{N}^{}}(^{t})\) (and therefore the same holds for \(^{1}_{D}(^{t})\)) contains substantial contribution towards to the direction \(^{t}-^{*}\), depending of the current error. We show that our choice of step size guarantees a decreasing direction. To this end, we prove the following:

**Claim 2.5**.: _Assume that the event \(H_{t}\) happens, i.e., \(_{D}(^{t})+\). If \(_{t}^{2}/8\), then \(I-_{t}(_{D}(^{t})-)\)._

Proof of Claim 2.5.: Recall that \(I=-2_{t}^{1}_{D}(^{t})(^{t}- ^{*})+_{t}^{2}\|(^{t},^{t}, ^{(t)},y^{(t)})\|_{2}^{2}\). By Lemma 2.2, we get that \(^{1}_{_{N}}(^{t})(^{t}- ^{*}) 2(_{_{N}}(^{t})-)\); hence, by taking expectations over the samples, we also have \(^{1}_{D}(^{t})(^{t}-^{*}) 2( _{D}(^{t})-)\). Furthermore, we have that \(\|(^{t},^{t},^{(t)},y^{(t)})\|_{2}^{2 } 8/^{2}\). Hence, \(I-2_{t}(_{D}(^{t})-)+8(_{t}^{2}/ ^{2})\). The claim follows by noting that if \(_{t}^{2}/8\), then \(-_{t}(_{D}(^{t})-)+8(_{t}^{2}/^{ 2}) 0\). Therefore, we obtain

\[I-_{t}(_{D}(^{t})-)\.\]

This completes the proof of Claim 2.5. 

Therefore, our choice of parameters guarantees that \(_{t}^{2}/8\). Using Claim 2.5 onto Inequality (5), we have that

\[\|^{t+1}-^{*}\|_{2}^{2}\|^{t}- ^{*}\|_{2}^{2}-_{t}(_{D}(^{t})-)+ _{t}\.\] (6)

Using Claim 2.5 and Inequality (6), we have that

\[\|^{T+1}-^{*}\|_{2}^{2} \|^{T}-^{*}\|_{2}^{2}-_{T}( _{D}(^{T})-)+_{T}\] \[\|^{0}-^{*}\|_{2}^{2}-_{t=0}^{T} _{t}(_{D}(^{t})-)+_{t=0}^{T}_ {t}\.\] (7)

To complete the proof of Theorem 2.1, we need to bound the estimation error that corresponds to the random variable \(_{t}\). We show that \(_{t}\) does not increase the error by a lot. Recall that \(_{t}=-2_{t}_{t}\).

Before proceeding, we provide some basic background on subgaussian random variables.

**Definition 2.6** (Subgaussian Random Variable).: For \(>0\), a zero-mean random variable \(X\) is called \(\)-subgaussian, if for any \(\) it holds \(([( X)])^{2}^{2}\).

Note that any zero-mean bounded random variable is subgaussian. Specifically, we have the following:

**Fact 2.7** (Hoeffding's lemma, see, e.g., ).: _Let \(X\) be a zero-mean random variable such that \(|X|\) for some \(>0\). Then \(X\) is \(C\)-subgaussian, where \(C>0\) is a universal constant._

Equipped with the above context, we show the following:

**Lemma 2.8**.: _With probability at least \(1-\) over the random samples, it holds that \(_{t=0}^{T}_{t} C^{2}^{2}T+(1/)\), where \(C>0\) is an absolute constant._

Proof.: We first show that \(_{t}\) is a subgaussian random variable.

**Claim 2.9**.: _The random vector \(_{t}\) is \((16/)\)-subgaussian._

Proof of Claim 2.9.: Note that \(_{t}=((^{t},^{t},^{(t)},y^{(t)})- _{(,y) D}[(^{t},^{t}, ,y)])(^{t}-^{*})\) and that by construction \(\|(^{t},^{t},,y)\|_{2} 4/\). Therefore, it holds that \(|(^{t},^{t},^{(t)},y^{(t)})( ^{t}-^{*})| 8/\), where we used that \(\|^{t}-^{*}\|_{2} 2\) as both of these vectors lie in the unit ball. Hence, by Fact 2.7, we have that \(_{t}\) is \((16/)\)-subgaussian.

Using Claim 2.9 and Definition 2.6 with parameter \(=-2_{t}\) and \(X=_{t}\), we have that

\[[(_{t})]=[(-2_{t}_{t})]  C(_{t}^{2}/^{2})\,\]

where \(C>0\) is a universal constant. To bound the contribution of \(_{t=0}^{T}_{t}\), we use Markov's inequality with respect to the filtration \(_{1},,_{T}\). We have that for any \(Z\), it holds that

\[}_{^{1},,^{T} D }[_{t=0}^{T}_{t} Z] =}_{^{1},,^{T} D }[(_{t=0}^{T}_{t})(Z)]\] \[}_{^{1},,^{T}  D}[(_{t=0}^{T}_{t})](-Z)\] \[=_{t=1}^{T}}_{^{t} D} [_{t}_{t}](-Z)(C _{t=0}^{T}^{2}}{^{2}}-Z)\,\]

where in the second inequality we use the independence of \(_{t}\) with \(\{_{k}\}_{k=1}^{t-1}\) with respect to the filtration \(_{t}\). Recalling that \(_{t}=c^{2}\), where \(c>0\) is a sufficiently small universal constant, we have that

\[}_{^{1},,^{T} D}[ _{t=0}^{T}_{t} Z](Cc^{2}^{2}^ {2}T-Z)(Cc^{2}^{2}^{2}T-Z)\.\]

Setting \(Z=Cc^{2}^{2}^{2}T+(1/)\) and taking \(c\) to be a sufficiently small absolute constant (as is done in our algorithm), we get that \(}_{^{1},,^{T} D}[ _{t=0}^{T}_{t} Z]\). This completes the proof of Lemma 2.8. 

Assume that until the round \(T\) the event \(H_{T}\) holds, i.e., for all \(i[T]\) we have that \(_{D}(^{i})+\). Using Lemma 2.8 onto Inequality (7), with probability at least \(1-\), we have that:

\[\|^{T+1}-^{*}\|_{2}^{2} \|^{0}-^{*}\|_{2}^{2}-_{t=0}^{T} _{t}(_{D}(^{t})-)+_{t=0}^{T} _{t}\] \[\|^{0}-^{*}\|_{2}^{2}-cT^{2} ^{2}+(1/)\.\]

Running the algorithm for \(T=((1/)/(^{2}^{2}))\) iterations guarantees that with probability at least \(1-\), we will have that \(\|^{T+1}-^{*}\|_{2}^{2} 0\), which means \(^{T+1}=^{*}\). In that case, i.e., in the case where all the events \(H_{i}\) for \(i[T]\) hold, \(^{T+1}\) achieves the same error as the optimal halfspace, thus it has 0-1 error of at most \(+\). Therefore, at least one vector \(^{t^{}}\) with \(t^{}[T+1]\) achieves 0-1 error of at most \(+\). The algorithm, in Step (5), returns a vector \(}\) that has 0-1 error at most \(_{D}(})_{t[T+1]} _{D}(^{t})++2\). The algorithm requires \(N=O((T/)/((1-2)))\) samples for Step (5), due to . The algorithm draws a sample in each round and runs for at most \(T\) rounds. Therefore, Algorithm 1 draws \(n=N+T=((1/)/(^{2}^{2}))\) samples. The algorithm needs to test each of the \(T\) hypotheses with \(N\) samples to find the closest one. Therefore, the total runtime is \(O(dTN)\) (as in the other subroutines the algorithm uses the samples only to estimate the gradients \(\), which requires \(O(1)\) additions of \(d\)-dimenional vectors). This completes the proof of Theorem 2.1. 

## 3 Conclusions and Open Problems

In this paper, we give the first sample near-optimal and computationally efficient algorithm for learning margin halfspaces in the presence of Massart noise. Specifically, the sample complexity of our algorithm nearly matches the computational sample complexity of the problem and its computational complexity is polynomial in the sample size. An interesting direction for future work is to develop a sample near-optimal and computationally efficient learner for general halfspaces (i.e., without the margin assumption). While our approach can likely be leveraged to obtain an efficient algorithm with sample complexity \((d)/^{2}\), the sample dependence on the dimension \(d\) would be suboptimal. Obtaining the right dependence on the dimension seems to require novel ideas, as prior works rely on fairly sophisticated methods  to effectively reduce to the large margin case.