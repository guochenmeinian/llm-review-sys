# Data Selection for Language Models

via Importance Resampling

 Sang Michael Xie, Shibani Santurkar, Tengyu Ma, Percy Liang

Department of Computer Science

Stanford University

{xie, shibani, tengyuma, pliang}@cs.stanford.edu

###### Abstract

Selecting a suitable pretraining dataset is crucial for both general-domain (e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We formalize this problem as selecting a subset of a large raw unlabeled dataset to match a desired target distribution given unlabeled target samples. Due to the scale and dimensionality of the raw text data, existing methods use simple heuristics or require human experts to manually curate data. Instead, we extend the classic importance resampling approach used in low-dimensions for LM data selection. We propose _Data Selection with Importance Resampling (DSIR)_, an efficient and scalable framework that estimates importance weights in a reduced feature space for tractability and selects data with importance resampling according to these weights. We instantiate the DSIR framework with hashed n-gram features for efficiency, enabling the selection of 100M documents from the full Pile dataset in 4.5 hours. To measure whether hashed n-gram features preserve the aspects of the data that are relevant to the target, we define _KL reduction_, a data metric that measures the proximity between the selected pretraining data and the target on some feature space. Across 8 data selection methods (including expert selection), KL reduction on hashed n-gram features highly correlates with average downstream accuracy (\(r=0.82\)). When selecting data for continued pretraining on a specific domain, DSIR performs comparably to expert curation across 8 target distributions. When pretraining general-domain models (target is Wikipedia and books), DSIR improves over random selection and heuristic filtering baselines by 2-2.5% on the GLUE benchmark.1

## 1 Introduction

Given a fixed compute budget, the choice of pretraining data is critical for the performance of language models (LMs) [18; 10; 24; 64; 27]. Existing works rely on heuristics to select training data. For example, GPT-3  and PaLM  filter web data for examples that are closer to formal text from Wikipedia and books as a proxy for high quality, a method which we call _heuristic classification_. Specifically, they train a binary classifier to discriminate formal text from web data and select web examples that have a predicted probability above a noisy threshold [10; 18; 21]. However, heuristic classification does not guarantee that the selected data is distributed like formal text. As a second example, domain-specific LMs such as Minerva  and Codex  (math and code LMs, respectively) employ domain-adaptive pretraining (DAPT) , where the model is initialized from a base LM and continues to be pretrained on a domain-specific dataset to achieve gains over the base LM on that domain. The domain-specific datasets are typically manually curated, but a framework for automating data selection could save effort and increase the amount of relevant training data.

In this paper, we consider the problem of **data selection**: given a large and diverse raw dataset (e.g., The Pile ) and a smaller dataset sampled from a desired target distribution, choose a subset of the raw data that is distributed similarly to the target (Figure 1). While a natural approach is to resample the raw data according to importance weights (importance resampling ), estimating importance weights on high dimensional data such as text is often statistically intractable [6; 75].

Instead, our **D**ata **S**election with **I**Importance **R**esampling (DSIR) framework efficiently estimates importance weights over a featurization of the raw and target distributions (Section 3). Our framework first maps the raw and target data onto some feature space and resamples a subset of raw data according to importance weights computed in this feature space. DSIR is extensible via the choice of feature space and importance estimator, which specify what aspects of the data we care about.

What is a feature space that both allows for efficient computation and preserves aspects of the data that are relevant for the target? In Section 4, we instantiate DSIR with hashed n-gram features, where n-grams are hashed onto a fixed number of buckets, for efficiency and scalability. The importance estimator is parameterized by bag-of-words generative models on the hashed n-grams, learned by simply counting the the hash bucket frequencies. DSIR with hashed n-grams enables the selection of 100M documents from the full Pile dataset in 4.5 hours.

To evaluate how well hashed n-grams preserves the aspects of the data that are relevant for the target, in Section 6 we define _KL reduction_, a data metric that measures how much a selected dataset reduces the Kullback-Leibler (KL) divergence to the target (in some feature space) over random selection \(((\|)-(\|))\). We show in Section 5 that KL reduction highly correlates with average downstream performance (Pearson \(r=0.82\)) across 8 data selection methods, including expert selection.

We consider selecting data from The Pile (1.6B examples) for continued pretraining of domain-specific LMs and training general-domain LMs from scratch. First, we select data for continued pretraining  of domain-specific LMs in a controlled setting where the target samples are unlabeled training inputs from a downstream dataset (Section 5). We perform continued pretraining on the selected data starting from RoBERTa  and evaluate by fine-tuning on the downstream dataset (whose unlabeled inputs were also used as the target for data selection). On 8 datasets from 4 domains (CS papers, biomedical papers, news, reviews), DSIR improves over RoBERTa (no continued pretraining) by 2% on average and is even comparable to continued pretraining on expert-curated data from Gururangan et al. .

For general-domain LMs (Section 7), the data selection target is formal, clean text from Wikipedia and books, following GPT-3 . We train a masked language model (MLM) from scratch on the selected data and evaluate by fine-tuning on GLUE . In controlled experiments, heuristic classification performs comparably to random sampling from The Pile, possibly because The Pile is already filtered

Figure 1: Given a large raw dataset such as The Pile  and a smaller target dataset (e.g., Wikipedia + books), we aim to select a subset of the raw data that is distributed like the target in some feature space. Our method, DSIR, first estimates importance weights using raw and target data in an n-gram feature space. The importance weights are used to resample a subset of the raw dataset.

[MISSING_PAGE_EMPTY:3]

introduces some noise due to collisions, we find that this is a simple and effective way to incorporate both unigram and bigram information.

Bag of hashed n-grams model.We parameterize the raw and target feature distributions \(p_{}\) and \(q_{}\) as bag-of-ngrams models. The bag-of-ngrams model has parameters \(^{m}\), which is a vector of probabilities on the hash buckets that sums to 1, Under this model, the probability of a feature vector \(z^{m}\) is

\[(z;\!)=_{j=1}^{m}[j]^{z[j]}\] (1)

where the bracket notation selects the corresponding index in the vector. Given some featurized examples \(_{1},...,_{s}\) sampled from a feature distribution, we estimate the parameters by counting: \(\!=\!^{s}1^{+}_{i}}_{j=1}^{s} _{j}\).

Speed benchmark on The Pile.To test the scalability of the framework, we benchmark DSIR with hashed n-gram features on selecting data from the full Pile dataset . For this test, we do not preprocess the data other than decompressing the text files for faster I/O. We use hashed n-gram features with 10k buckets, fit the raw feature distribution with 1B hashed indices from the Pile, and fit the target feature distribution with the full target dataset (ChemProt ). DSIR selects 100M documents from the full Pile dataset in 4.5 hours on 1 CPU node with 96 cores. Almost all of the time (4.36 hours) is spent computing the importance weights on the raw dataset, while fitting the feature distributions (1 minutes) and resampling (6 minutes) were much faster. Increasing the number of CPU cores can further decrease the runtime.

## 5 Selecting Data for Domain-Specific Continued Pretraining

In this section, we use DSIR to select domain-specific data for continued pretraining. We compare DSIR to 7 other data selection methods in this continued pretraining setting.

Setup.We select data for 8 target distributions in the setting of Gururangan et al. , where we perform continued pretraining of domain-specific LMs. Here, the target is a specific downstream unlabeled data distribution and we select examples from The Pile (the raw data). For each downstream dataset, we select data for continued pretraining starting from RoBERTa  (see Appendix H). Following Gururangan et al. , we consider 8 downstream datasets across 4 domains: Computer Science papers (ACL-ARC , Sci-ERC ), Biomedicine (ChemProt , RCT ) News (AGNews , HyperParisan ), and Reviews (Helpfulness , IMDB ).

Baselines.Beyond random selection (without replacement) and heuristic classification, we also compare against manual curation  and a top-\(k\) variant of heuristic classification. In manual curation, we simply fine-tune from domain-adaptive pretraining (DAPT) checkpoints , which are the result of continued pretraining on manually-curated data. In top-\(k\) heuristic classification, we select the top-\(k\)-scoring examples according to the binary classifier used in heuristic classification. All methods select data from The Pile except for manual curation, which uses domain-specific data sources .

We perform a controlled comparison by equalizing the amount of LM training compute for all methods, measured by the number of tokens processed during training, following the compute budget in Gururangan et al. . For random selection, heuristic classification, and DSIR using n-gram features (defined in Section 4), we control the number of selected examples (25M examples with fixed token length 256) and the training protocol. We standardize the fine-tuning for all models and average all results over 5 random seeds (see Appendix H for details). All the models initialize from RoBERTa-base. Before data selection via DSIR or heuristic classification, we remove extremely short (<40 words) or repetitive documents that tend to be uninformative (Appendix J).

Automatic data selection with DSIR can replace manual curation.Table 1 shows the comparison between the data selection methods. To summarize:

* On average, DSIR improves over random selection by 1.2% and manually curated data (DAPT) by 0.3%, showing the potential to replace manual curation.

* DSIR improves over heuristic classification by 0.9% and is comparable to top-\(k\) heuristic classification. We note that top-\(k\) heuristic classification is not typically used in this setting, but we find that it may be particularly suited for domain-specific data selection, where diversity may be less important than the general-domain setting.
* Random selection improves by 0.4% on average over no continued pretraining at all, showing that additional data generally improves the downstream F1 score. All the targeted data selection methods improve over random selection.

Discriminative importance weight estimators underperform generative estimators.We experiment with replacing components of DSIR in Table 2. First, we consider using the binary classifier from heuristic classification (which takes pretrained fasttext word vectors as input) as the importance weight estimator in DSIR. For input \(x_{i}\), the classifier predicts the probability of target \(f(x_{i})\). We use this to estimate importance weights \()}{1-f(x_{i})}\), then resample according to these weights. This approach (DSIR (fasttext discriminative)) improves F1 by 0.3% over heuristic classification. However, this approach still underperforms DSIR by 0.6% on average, even with regularization and calibration.

We consider another discriminative version of DSIR that uses hashed n-gram features as input to a logistic regression binary classifier for importance weight estimation. This differs from heuristic classification, which initializes with pretrained fasttext feature vectors and fine-tunes the features along with the classifer. This approach (DSIR (n-gram discriminative)) underperforms DSIR by 0.7%, even with regularization and calibration. These results suggest that a generative approach is better suited (or easier to tune) for importance resampling. However, the discriminative approaches still outperform random selection by 0.6%.

Selecting with n-grams improves downstream performance over unigrams.DSIR uses both unigram and bigram information to compute hashed n-gram features. We ablate the role of bigram information in hashed n-grams by using hashed unigram features (with 10k buckets) for DSIR. In Table 2, we find that DSIR with unigram features underperforms DSIR with n-grams by 0.26%, though still achieving comparable F1 score to manual curation. Overall, selecting data with unigrams is effective, but including bigrams further improves the relevance of the selected data.

Cross-domain analysis and the effect of the choice of pretraining data.DSIR assumes knowledge of the target distribution, but what happens if the target dataset is not representative of the target

    & ACL-ARC & Sci-ERC & ChemProt & RCT & HyperPartisian & AGNews & Helpfulness & IMDB & Avg \\  RoBERTa (no continued pretraining) & 66.80\(\) & 80.14\({}_{-2.5}\) & 82.31\({}_{-0.5}\) & 86.86\({}_{-0.4}\) & 88.25\({}_{-2.9}\) & 93.35\({}_{-0.5}\) & 65.08\({}_{-2.9}\) & 94.38\({}_{-0.3}\) & 82.20 \\ Random selection & 67.51\({}_{-2.6}\) & **80.53\({}_{-0.5}\)** & 83.14\({}_{-0.2}\) & 86.85\({}_{-0.1}\) & 86.42\({}_{-0.3}\) & 93.52\({}_{-0.1}\) & 65.15\({}_{-1.3}\) & 94.49\({}_{-2.9}\) & 82.58 \\ Manual curation/DAPT  & 71.84\({}_{-2.7}\) & 80.42\({}_{-0.5}\) & 84.17\({}_{-0.5}\) & 87.16\({}_{-1.0}\) & 87.23\({}_{-0.5}\) & 93.61\({}_{-1.2}\) & 68.21\({}_{-1.0}\) & **95.08\({}_{-1.1}\)** & 83.46 \\ Heuristic classification & 69.94\({}_{-2.6}\) & 80.52\({}_{-0.2}\) & 83.35\({}_{-0.5}\) & 86.78\({}_{-0.7}\) & 85.71\({}_{-0.9}\) & 93.54\({}_{-1.0}\) & 85.08\({}_{-0.9}\) & 94.66\({}_{-2.8}\) & 82.88 \\ Top-6 Heuristic classification & 71.70\({}_{-2.1}\) & 80.22\({}_{-0.8}\) & 84.11\({}_{-0.7}\) & 87.06\({}_{-1.1}\) & 88.26\({}_{-2.8}\) & 83.68\({}_{-1.4}\) & **69.76\({}_{-1.7}\)** & 94.90\({}_{-1.0}\) & 83.65 \\ DSIR & **72.86\({}_{-2.7}\)** & 80.44\({}_{-1.3}\) & **85.41\({}_{-0.4}\)** & **87.41\({}_{-0.3}\)** & 87.01\({}_{-1.3}\) & 87.01\({}_{-1.5}\) & 93.62\({}_{-1.0}\) & 68.95\({}_{-0.5}\) & 94.56\({}_{-0.3}\) & **83.76** \\   

Table 1: F1 scores for continued pretraining from the RoBERTa checkpoint  on 8 downstream datasets from 4 domains (CS, Biomed, News, and Reviews). Random selection, heuristic classification, and DSIR train on 25M selected examples from The Pile. Heuristic classification and DSIR create a different pretraining dataset for every downstream dataset. All models (including DAPT ) use the same amount of training compute and results are averaged over 5 seeds, with standard deviations in subscripts. All datasets use macro-F1 except ChemProt and RCT, which use micro-F1.

    & **ACL-ARC** & **Sci-ERC** & **ChemProt** & **RCT** & HyperPartisian & AGNews & Helpfulness & IMDB & Avg \\  Heuristic classification & **69.94\({}_{-1.0}\)** & **80.52\({}_{-2.9}\)** & 83.51\({}_{-0.7}\) & 86.76\({}_{-0.17}\) & 85.71\({}_{-0.0}\) & **93.54\({}_{-1.0}\)** & **68.50\({}_{-0.9}\)** & 94.66\({}_{-0.2}\) & 82.88 \\ DSIR (n-gram generative) & **72.86\({}_{-2.7}\)** & **80.44\({}_{-1.3}\)** & **85.51\({}_{-0.6}\)** & **87.14\({}_{-1.3}\)** & 87.01\({}_{-1.5}\) & 93.62\({}_{-1.0}\) & **68.95\({}_{-0.1}\)** & 94.56\({}_{-0.1}\) & **83.76** \\  DSIR (fasttext discriminative) & 68.46\({}_{-1.5}\) & 79.00\({}_{-1.0}\) & **84.57\({}_{-0.6}\)** & **87.09\({}_{-0.0}\)** & **89.18\({}_{-0.0}\)** & **93.54\({}_{-1.4}\)** & 68.41\({}_{-1.5}\) & **94.95\({}_{-0.2}\)** & **83.15** \\ DSIR (n-gram discriminative) & 70.35\({}_{-2.9}\) & 80.21\({}_{-0.5}\) & 85.03\({}_{-1.8}\) & 87.04\({}_{-1.9}\) & 85.49\({}_{-2.9}\) & **93.74\({}_{-0.7}\)** & 68.79\({}_{-1.2}\) & **94.84\({}_{-2.4}\)** & 83.19 \\ DSIR (unigram generative) & 69.53\({}_{-0.6}\) & 79.69\({}_{-1.3}\) & 85.24\({}_{-0.4}\) & 87.05\({}_{-0.0}\) & **90.11\({}_{-0.9}\)** & 93.42\({}_{-0.1}\) & 68.55\({}_{-0.5}\) & 94.39\({}_{-0.3}\) & 83.50 \\   

Table 2: F1 scores for continued pretraining from RoBERTa, testing different on heuristic classification and DSIR methods. For heuristic classification, replacing the Pareto noisy threshold with calibration and importance resampling (DSIR (fasttext discriminative)) improves F1. Generative importance weight estimators outperform discriminative importance weight estimators for DSIR. All results average over 5 seeds, with standard deviations in subscripts.

distribution? To test the effect of varying the target distribution on downstream performance, we consider every pair of pretraining dataset, which is selected by DSIR for target downstream task X, and downstream task Y. Figure 2 provides the full matrix of results. We find a 6% average drop in F1 when we choose the worst pairing for each downstream task instead of matching the pretraining and downstream data. In the worst case, the F1-score on HyperPartisan drops by 30%. Thus, the choice of target distribution can have a large effect on downstream performance.

Pretraining data transfers better for targets within the same domain.In practice, we may have access to some target datasets in the relevant domain and hope to select pretraining data that can improve performance on other tasks in that domain. The 8 target/downstream tasks we use come from 4 domains, with 2 tasks from each domain. We define within-domain F1 as the average F1 of the pairs of pretraining and fine-tuning data from the same domain, but excluding pairs where the pretraining data is selected for the fine-tuning task. We compute this by averaging the off-diagonal elements in the 2\(\)2 diagonal blocks of the matrix in Figure 2. We find that the within-domain F1 (82.9%) is 1.7% higher on average than the cross-domain F1 (81.2%), where the pretraining data is selected for a target from a different domain.

## 6 KL Reduction on Hashed N-grams Predicts Downstream Performance

When designing a feature extractor for DSIR, how do we measure whether the features preserve the information for selecting relevant pretraining data? To answer this question, we propose a data metric, _KL reduction_, which measures how much data selection reduces distance to the target over random selection in a feature space. We find that KL reduction on hashed n-gram features strongly correlates with downstream performance across various data selection methods, including those that do not involve n-grams, such as manual curation.

Figure 2: F1 scores of DSIR for all pairs of pretraining data target distributions (rows) and downstream tasks (columns). The cells are colored by its per-column ranking, with better rankings (higher F1 scores) having darker colors. While using the pretraining data selected specifically for the downstream task is typically strong, choosing the worst pretraining dataset for the downstream task reduces F1 by 6% on average. All results are averaged over 5 seeds.

KL reduction metric.We define _KL reduction_ as the average reduction in empirical KL divergence from doing data selection over random selection over a set of target feature distributions \(\):

\[(p^{}{}_{};_{},) =|}_{_{}}( _{}\|_{})-(_{}\|p ^{}{}_{})\] (2)

where \(p^{}{}_{}\) is the empirical feature distribution of the selected data, \(_{}\) is a empirical target feature distribution, \(_{}\) is the empirical raw feature distribution. KL reduction depends on the raw distribution \(_{}\) and the set of target distributions \(\) as hyperparameters. In our continued pretraining setting, \(_{}\) is the feature distribution of the Pile and \(\) consists of the feature distributions from the 8 downstream tasks from Section 5.

KL reduction on hashed n-grams predicts downstream performance.We show that when computed on the hashed n-gram feature space, KL reduction of a selected dataset highly correlates with the downstream performance of a model trained on that data. Figure 3 plots KL reduction against average downstream performance over 8 target distributions for 8 data selection methods from The Pile , where the distribution parameters are estimated using 100k samples from each dataset. The average downstream F1 score is highly correlated with the KL reduction (Pearson \(r\) = 0.82). This agrees with the results of Razeghi et al.  for in-context learning  and extends the preliminary evidence from Gururangan et al.  on one selection method that better unigram overlap improves downstream performance. DSIR with hashed n-gram features achieves the highest KL reduction and the best average downstream F1.

While some of the original pretraining datasets for DAPT  were not publicly available, we downloaded the public versions as an approximation. Our results suggest that hashed n-gram features preserve most of the information needed for selecting data relevant to the target. Since KL reduction highly correlates with downstream performance and can be cheaply computed without training an LM, KL reduction can be used as a sanity check for future data selection methods.

## 7 Selecting Data for Training General-Domain LMs

In this section, we consider selecting formal text (as a proxy for high-quality text) for training general-domain LMs from scratch. We use Wikipedia and books as the target distribution.

Baselines and setup.We compare the following methods for selecting data from the Pile: 1) Random selection, 2) Heuristic classification (GPT-3/Pile/PaLM method), and 3) DSIR. As ablations, we consider top-\(k\) variants of heuristic classification and DSIR (take the top-\(k\) examples according to importance weights instead of resampling). We use each method to select 51.2M examples, which corresponds to 4 epochs with our compute budget. For heuristic classification and DSIR, we select 96% of the examples from domains excluding Wikipedia and books. This is done to reduce the bias towards selecting data from Wikipedia and books (the target distribution). We choose the other 4%

Figure 3: Plot of average KL reduction on the n-gram feature space, defined as how much the selected dataset reduces KL divergence to the target distribution over just random sampling from The Pile, against average downstream F1 score over the 8 continued pretraining datasets in Table 1. There is a strong correlation between KL reduction and downstream performance (Pearson \(r\!=\!0.82\)).

uniformly from Wikipedia and books, and did not tune these proportions (Appendix F). We apply a quality filter for extremely short or repetitive examples before heuristic classification and DSIR selection (Appendix J). For each dataset, we perform MLM pretraining for 50k steps with a large batch size (4096) and short token length (128), following Izsak et al. . All the models use the BERT-base architecture . We evaluate the models on the GLUE dev set, averaged over 5 fine-tuning runs . Fine-tuning hyperparameters such as the number of epochs and batch size are fixed for each dataset, following reasonable defaults from the RoBERTa codebase .

DSIR qualitatively selects more formal text.Figure 4 shows the beginning characters of 20 random examples selected by random selection, heuristic classification, and DSIR. The random sample contains many code examples that are not similar to text from Wikipedia and books. Heuristic classification seems slightly too diverse, which suggests that the variance of the Pareto distribution added to the classifier scores may be too high. Note that we use the setting of the Pareto shape hyperparameter used in GPT-3 . Qualitatively, DSIR selects the most formal text. By doing importance resampling to match the target distribution, DSIR trades off the relevance and diversity of the selected data automatically.

DSIR improves GLUE performance.Table 3 shows results on the GLUE dev set. DSIR achieves 82.3% average GLUE accuracy, improving over random selection by 2% and heuristic classification by 2.5%. Heuristic classification leads to 0.5% lower accuracy than random selection from The Pile. We hypothesize this is because The Pile has already been filtered once with heuristic classification.

Resampling outperforms top-\(k\) selection.Top-\(k\) heuristic classification and top-\(k\) DSIR have similar performance across datasets, with some tradeoffs compared to DSIR without top-\(k\). DSIR without top-\(k\) is competitive with these variants on all datasets and achieves a 0.8-0.9% higher average over top-\(k\) variants. All the top accuracies across datasets are achieved by DSIR or top-\(k\) DSIR.

Figure 4: Beginning characters of 20 random examples (each line is a different example) selected by random selection, heuristic classification, and DSIR, where the target is formal text from Wikipedia + books. Qualitatively, DSIR selects more formal text than random selection and heuristic classification.

   & MNLI & QNLI & QQP & RTE & SST-2 & MRPC & CoLA & STS-B & Avg \\  Random selection & 82.63\({}_{0.41}\) & 86.90\({}_{0.28}\) & 89.57\({}_{0.30}\) & 67.37\({}_{1.60}\) & 90.05\({}_{0.04}\) & 87.40\({}_{1.08}\) & 49.41\({}_{1.47}\) & 88.63\({}_{0.21}\) & 80.25 \\ Heuristic classification & 82.69\({}_{0.17}\) & 85.95\({}_{0.29}\) & 89.77\({}_{0.32}\) & 65.95\({}_{1.75}\) & 88.94\({}_{0.98}\) & 86.63\({}_{0.58}\) & 84.17\({}_{1.93}\) & 88.62\({}_{0.22}\) & 79.85 \\ Top-\(k\) Heuristic classification & 83.84\({}_{0.24}\) & 85.62\({}_{0.38}\) & 89.89\({}_{1.90}\) & 70.04\({}_{0.99}\) & 91.15\({}_{0.76}\) & 86.37\({}_{0.50}\) & 53.02\({}_{0.56}\) & 89.30\({}_{1.11}\) & 81.47 \\  DSIR & 83.07\({}_{0.29}\) & **89.11\({}_{0.11}\)** & 89.80\({}_{0.37}\) & **75.09\({}_{2.76}\)** & 90.48\({}_{0.37}\) & **87.70\({}_{0.68}\)** & **54.00\({}_{0.34}\)** & 89.17\({}_{1.03}\) & **82.30** \\ Top-\(k\) DSIR & **83.39\({}_{0.06}\)** & 88.63\({}_{0.38}\) & **89.94\({}_{1.7}\)** & 72.49\({}_{1.29}\) & **91.01\({}_{0.79}\)** & 86.18\({}_{1.12}\) & 49.90\({}_{1.30}\) & **89.52\({}_{0.21}\)** & 81.38 \\  

Table 3: Accuracies on the GLUE  dev set for a BERT-style masked language model  trained on data selected from The Pile . Following RoBERTa , for RTE, STS, and MRPC we fine-tune starting from the MNLI model instead of from scratch. DSIR outperforms heuristic classification (used by GPT-3 and PaLM) and random selection by over 2% on average. All results are averaged over 5 seeds and standard deviations are in subscripts.

Related Work

Effect of pretraining data on LMs.The pretraining data has a large effect on LM performance. Lee et al. ; Hernandez et al.  show that deduplicating data improves LMs, and Baevski et al. ; Yang et al.  compare using a large web corpus versus Wikipedia. Raffel et al.  shows that heuristically filtered data (filtering out short and duplicated examples) improves T5 and Du et al.  shows that heuristic classification improves downstream few-shot performance for GLaM. We provide extensive controlled experiments comparing the effect of data selection methods on downstream performance.

Retrieval.Yao et al.  use keyword-based retrieval (BM25) to select data for semi-supervised learning. In preliminary tests, we found that out of 6.1M documents retrieved by BM25, there were only 1.8M unique documents (70% were exact duplicates). These duplicate examples can hurt performance [44; 26]. Selecting a desired number of unique documents involves oversampling and de-duplication. Instead, we consider top-\(k\) heuristic classification, which has similarities to cosine similarity-based retrieval (since heuristic classification uses an inner product score between pretrained word embeddings and a learned class vector) and avoids retrieving repeated examples.

Data selection in classical NLP.Moore-Lewis selection [56; 2; 20] takes the top-\(k\) examples in cross-entropy difference between n-gram LMs trained on target and raw data to score examples, which could over-sample examples from the mode of the target distribution. In Section 7, we found that top-\(k\) DSIR, which is a form of Moore-Lewis selection with hashed n-gram LMs, underperforms DSIR by 0.9% on GLUE. DSIR naturally balances diversity and relevance for use in both domain-specific and general-domain cases, since it uses importance resampling to match the target distribution. Feature-space/n-gram discrepancy measures [29; 69; 47] have also been used in selecting data in the domain adaptation setting. Overall, these methods do not consider importance resampling and do not address the gap between pretraining and downstream tasks: pretraining has a different objective to fine-tuning, pretraining uses unlabeled data that is not task-formatted, and the influence of pretraining data is separated from the final model by the fine-tuning step. Beyond the preliminary evidence that unigram similarity metrics are related to downstream performance in Gururangan et al. , we show comprehensively and quantitatively on 8 selection methods that despite the pretrain-downstream gap, n-gram KL reduction on pretraining datasets highly correlates with downstream performance.

Data selection in deep learning.Many works show the importance of data selection in the supervised or semi-supervised learning setting in vision [76; 54; 33; 36; 35; 37; 83; 85; 61; 55; 15; 71; 5] and in language finetuning [15; 54]. While most select image data from CIFAR or ImageNet, which have up to 1-10M examples, we consider selecting text data from The Pile, which has over 1.6B examples (of 128 whitespace-delimited words each). At this scale, previous methods become quite expensive since they typically require running a neural network forward pass to get embeddings [71; 76; 37], taking gradients [35; 36; 83; 61; 55], or training a reference model . In contrast, we construct a simple n-gram-based selection method that easily scales to internet-scale datasets. Coleman et al.  select data with high uncertainty under a smaller proxy neural model. They do not consider using a target dataset for estimating importance weights. However, using a neural model could be a complementary strategy for importance resampling. Other works [70; 50; 32] focus on choosing a subset that approximates training with the original dataset and require selecting data online during training. We aim to select a targeted dataset (once, before training) with different properties from the raw data (restricting the data to formal text or a specific domain). Our work also differs from active learning methods [72; 19; 84; 90; 80], which query an annotator for more labeled data. Instead, we select data for self-supervised pretraining.

Importance weighting and domain adaptation.Many methods tackle the high-dimensional importance weight estimation problem [79; 67; 12; 13]. In particular, importance weighting is classically used in domain adaptation [74; 78], where unlabeled target examples are used to adapt a model trained on labeled source data, for reweighting the loss function. However, in many modern applications the source and target are often disjoint (e.g., sketches vs. natural images), causing undefined importance weights [62; 42; 73]. We side-step high-dimensional importance weight estimation by instead working in a reduced feature space where the support of the massive web corpus should cover the target.

Discussion and Limitations

Feature space for importance resampling.Finding an appropriate feature space is important for DSIR. Although we find a tight correlation between downstream performance and our data metric compute using hashed n-gram features, n-grams only capture a superficial word-level overlap. Other feature extractors, such as neural models, may produce features that better capture semantics. We consider a variant of DSIR which estimates importance weights on a neural feature space in Appendix B, and find that this variant also improves by 1-1.5% over random selection and heuristic classification on GLUE, but our preliminary version does not improve over DSIR with hashed n-gram features. However, extracting these features is much more computationally expensive (on the order of \(D\) times more FLOPs for a \(D\)-parameter neural model), and importance weight estimation on this continuous feature space may be more difficult.

Parameterization of the importance weight estimator.In principle, both generative and discriminative approaches to estimating the importance weights should work. In a discriminative approach, regularization and calibration should be used to combat overfitting and make the predicted probabilities useful for importance resampling. We find that a generative approach requires less tuning and could also be better when the number of target examples is small, as Ng and Jordan  finds that Naive Bayes often performs better than logistic regression in low-sample regimes.

What is the right target distribution?When developing a domain-specific model such as Codex , the target dataset should be representative of the coding tasks we expect the model to be used on. However, it's unclear how exactly to collect this dataset and how much to weight each task in the target distribution. Developing better procedures for collecting the target dataset can ultimately improve the data selected by DSIR. For general-domain LMs, we follow GPT-3, the Pile, and PaLM in using formal text from Wikipedia and books as a proxy for high quality text [10; 21; 18; 14]. However, this is just a heuristic. We leave the exploration of other target distributions for general-domain LMs to future work.

Broader impacts.The impact of DSIR depends on the properties of the target data. While DSIR could amplify biases present in the target examples, with the appropriate target data, DSIR can be used to collect data that improve the training efficiency, alignment, or bias of LMs [59; 4; 40]. These benefits could reduce the environmental impact of LMs [77; 43; 46; 60] and reduce their biases and risks [9; 1; 22; 8]. For example, DSIR can be used to collect more data on underrepresented subpopulations and fine-tune the model on this data to improve model fairness.

## 10 Conclusion

We provide a cheap and scalable data selection framework based on importance resampling for improving the downstream performance of LMs. We also find a data metric, KL reduction, that strongly correlates with downstream performance and can provide a sanity check for data selection methods without training a model. Our work provides a step in understanding the choice of pretraining data for downstream transfer in LMs.

## 11 Acknowledgements

We thank Neil Band, Hong Liu, Garrett Thomas, and anonymous reviewers for their feedback. This work was supported by an Open Philanthropy Project Award and NSF IIS 2211780. SMX was supported by a NDSEG Fellowship. SS is supported by an Open Philanthropy Graduate Fellowship.