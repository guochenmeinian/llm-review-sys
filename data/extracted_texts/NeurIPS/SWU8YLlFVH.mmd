# Neural Sampling in Hierarchical Exponential-family Energy-based Models

Xingsi Dong\({}^{1,2,3}\)

dxs19980605@pku.edu.cn

Si Wu\({}^{1,2,3}\)

siwu@pku.edu.cn

1. PKU-Tsinghua Center for Life Sciences, Academy for Advanced Interdisciplinary Studies.

2. School of Psychological and Cognitive Sciences,

Beijing Key Laboratory of Behavior and Mental Health, Peking University.

3. IDG/McGovern Institute for Brain Research. Center of Quantitative Biology, Peking University.

###### Abstract

Bayesian brain theory suggests that the brain employs generative models to understand the external world. The sampling-based perspective posits that the brain infers the posterior distribution through samples of stochastic neuronal responses. Additionally, the brain continually updates its generative model to approach the true distribution of the external world. In this study, we introduce the Hierarchical Exponential-family Energy-based (HEE) model, which captures the dynamics of inference and learning. In the HEE model, we decompose the partition function into individual layers and leverage a group of neurons with shorter time constants to sample the gradient of the decomposed normalization term. This allows our model to estimate the partition function and perform inference simultaneously, circumventing the negative phase encountered in conventional energy-based models (EBMs). As a result, the learning process is localized both in time and space, and the model is easy to converge. To match the brain's rapid computation, we demonstrate that neural adaptation can serve as a momentum term, significantly accelerating the inference process. On natural image datasets, our model exhibits representations akin to those observed in the biological visual system. Furthermore, for the machine learning community, our model can generate observations through joint or marginal generation. We show that marginal generation outperforms joint generation and achieves performance on par with other EBMs.

## 1 Introduction

Human behavioral studies [1; 2; 3] and animal neurophysiological studies [4; 5] have suggested that the brain performs statistically optimal Bayesian inference to interpret the external world [6; 7; 8; 9]. One promising theory for implementing Bayesian inference in the brain is to interpret the variability of neural responses as Monte Carlo sampling of the posterior distribution . This perspective naturally accounts for the irregular firing patterns and other response properties observed in sensory cortex neurons [11; 12; 13]. Numerous sampling-based models [10; 14; 15; 16; 17; 12; 13; 18] have been proposed to elucidate neural dynamics and the underlying mechanisms of Bayesian inference.

To facilitate the brain's ability to derive meaningful representations from sensory input, it must continually update its generative model to approximate the true distribution of the external world . Previous approaches often neglect this critical learning process. They either maintain fixed parameters for their generative model [15; 17; 16], or they employ biologically implausible methods like the variational approach with backpropagation (BP) for training . Is there a generative model that integrates sampling-based inference with the capability to learn locally in both time and space?Energy-based models (EBMs)  provide a framework for inference with sampling method and learning with spatially localized rules. The reason for non-locality in time is the need to estimate the partition function. To estimate the global partition function, the network has to perform a top-down pass to obtain a negative sample. This process is referred to as the negative phase. During this pass, it disrupts the neural network's stored inference results, which is essentially the same reason for the temporal non-locality as in the case of BP. Recently, predictive coding networks (PCNs) [21; 22; 23; 24] use the Gaussian distribution to avoid the negative phase since the partition function of Gaussian is constant. But further study  reveals that the Gaussian assumption is restrictive when dealing with complex probability distributions. Moreover, setting aside biological constraints, estimating the partition function itself poses a significant challenge. In the field of machine learning, various sampling methods, such as amortized generation  and implicit generation , have been proposed to tackle this issue. However, both of these methods involve the use of the negative phase, which is known for being challenging to converge. Generative adversarial networks (GANs) address this issue by utilizing a discriminative model, but GANs are notoriously difficult to train in practice.

Besides, the inference dynamic of Gibbs sampling [28; 20] or Langevin sampling  in EBMs essentially perform random walks in local regions rather than the whole posterior space, which is too slow to be compatible with brain functions . Therefore, it is crucial to investigate whether neural circuits in the brain have the capacity of realizing sampling-based inference rapidly.

**Summary of the work.** In Sec.2, we propose that our brain holds an EBM as the intrinsic generative model to interpret the external world. The neural dynamics employ a sampling-based method for Bayesian inference. Simultaneously, the learning dynamic aims to minimize the discrepancy between the observed distribution of intrinsic model and the real world. In Section 3, we introduce the Hierarchical Exponential-family Energy-based (HEE) model, which allocates the partition function across each layer. This allocation shifts the estimation of the total sample space required for calculating the partition function from a product of individual layer spaces to a sum of layer spaces. This approach enhances the convergence of our model. Furthermore, we efficiently sample the normalization term of the exponential-family in each layer using a group of neurons with fast dynamics. This localizes the learning process in both time and space. In Sec.4, we find that incorporating noisy adaptation, a generic feature of neuronal responses, into the inference dynamics effectively yields a second-order Langevin dynamic. In Sec.5, we validate the capabilities of the HEE model using _2D synthetic datasets_ and _FashionMNIST_. Then, we incorporate receptive field as the biological constrains to the HEE model training on _CIFAR10_. We show that the HEE model can achieve a performance comparable with previous EBMs . We also investigate the neural representation of semantic information, including orientation, color and category, which exhibit similarities to biological visual systems. And the neural adaptation can trigger neural phenomena including oscillations and transient which are widely observed in biological systems. In Sec.6, we discuss several related theories and models.

**Main Contributions.** We propose a hierarchical EBM whose learning process is localized both in time and space, which could potentially serve as a mechanism for the brain to utilize changes in synaptic strength for learning. And our brain-inspired EBM also presents a technique for estimating the partition function, which is a challenging problem within the machine learning community.

## 2 The intrinsic generative model

In this section, we propose that our brain holds energy-based models (EBMs) as the intrinsic generative model consisting of two components: inference and learning. Inference is believed to be carried out through neural sampling [10; 20], while learning is accomplished through long-term synaptic plasticity .

Let \(\) be the observation received by our brain, and let \(\) be the latent variable represented by neurons. The joint distribution of the EBMs is written as \(p_{}(,)=p_{}(|)p_{} ()\), where \(\) are stored in the connection weights of neurons (Fig.1A). The EBMs aims to minimize the difference between the intrinsic marginal distribution \(p_{}()\) and the true distribution of the external world \(p_{}()\), which is described by the Kullback-Leibler divergence,

\[_{}D_{}[p_{}() p_{ }()].\] (1)The neural system can adopts the gradient based learning method such as gradient decent. And the gradients is calculated as (see SI for detailed proof),

\[_{}D_{}[p_{}() p_{} ()]=-_{ p_{}()}_{ p_{}(|)}[_{} p_{ }(,)].\] (2)

The second expectation of the above equation requires the posterior of a given observation \(\), which is calculated as \(p_{}(|)=p_{}(,)/p_{ }()\). Practically, the posterior is intractable for the denominator \(p_{}()\) requires complex integral. Variational inference is usually used to solve this problem (such as VAE ). And the EBMs adopts the sampling method to avoid complex calculations. By leveraging the relationship \(_{} p_{}(|)=_{}  p_{}(,)\),the samples of posterior can be offered by the neural dynamic (Langevin sampling),

\[_{z}}{t}=_{} p_{ }(,)+},\] (3)

where \(\) is Gaussian white noise and \(_{z}\) is the time constant. The stationary distribution of the above dynamic is our target distribution \(p_{}(|)\). The sampling algorithm, along with the joint distribution, determines the connections of neurons and their inference dynamic (Fig.1B).

For the learning dynamic, the brain receives the observations from the real world continuously (\(_{ p_{}()}\)), and the neural dynamic mentioned above can produce samples of their posterior simultaneously (\(_{ p_{}(|)}\)). The parameters can be updated according to the gradient, which is often implemented by Hebbian learning rules(Fig.1B),

\[_{}}{t}=-_{}D_{}[p_{}() p_{}() ]=_{} p_{}(,),\] (4)

where \(_{}\) is the time constant of synapses.

For neuroscience, this is the end of the story. Nevertheless, EBMs can also generate observations which the machine learning society follow with interest. In the generation process, the observation is not fixed but undergoes the Langevin sampling,

\[_{x}}{t}=_{} p_{ }(,)+}.\] (5)

The latent variable \(\) can follow the inference dynamic Eq.(3). In this case, the observation \(\) and latent variable \(\) together follow the joint distribution \(p_{}(,)\). Thus, the observation \(\) can produce samples following \(p_{}()\), which is called joint generation (Fig.1C). However, in order to get the marginal distribution \(p_{}()\), the latent variable \(\) just needs to follow the prior distribution \(p_{}()\) rather than reaching the posterior. In this case, the generation dynamic of latent variable is written as,

\[_{z}}{t}=_{} p_{ }()+}.\] (6)

Figure 1: (A) The directed graphical model of the energy-based model (EBM). (B) The latent variable \(\) receives the likelihood information \(p_{}(|)\) from the observation \(\) and combines it with the prior knowledge \(p_{}()\) to perform the inference dynamic. And the connected weights \(\) changes following the learning dynamic (dashed line). (C) The latent variable \(\) performs inference dynamic and the observation \(\) performs generation dynamic, which leads to the distribution of \(, p_{}(,)\). And the connection weights \(\) are fixed (solid line). (D) The latent variable doesn’t receive likelihood information from observation and \( p_{}(),\  p_{}()\) which is called marginal generation.

And the stationary distribution of Eq.(5) performed by observation \(\) still equals to \(p_{}()\), which is called marginal generation (Fig.1D). In Sec.5, we will see that the marginal generation performs better than joint generation. Here is an informal understanding. The process of sampling \((,)\) can be understood as searching for a specific pair. We assume that the size of the \(\)-space is \(O(m)\) and the size of the \(\)-space is \((n)\). In joint generation, the search is conducted simultaneously in both the \(\)-space and \(\)-space, resulting in a required search space of \(O(n*m)\). In marginal generation, the process involves initially searching in the \(\)-space according to \(p_{}()\). Once \(\) is found, it is fixed. This step's search space size is \(O(n)\). Then, \(\) is searched based on \(p_{}(|)\) in the \(\)-space. This step's search space size is \(O(m)\), leading to a combined required search space size of \(O(n+m)\).

## 3 Exponential-family energy-based model

In this section, we provide a neural implementation of the HEE model and outline the specific dynamics involved in inference, learning, and generation, as discussed in Section 2. Approximating the target distribution \(p_{}()\) which is diverse and complex requires a good representation ability of the model. Exponential families include many of the most common distributions (such as normal, Poisson, gamma distribution and so on). Moreover, exponential families can be easily parameterized, allowing for generalization and flexibility in modeling various types of distributions.

Let \(_{0}^{n_{0}}\) be the observation (such as an image) received by our brain. And there are \(L\) layers of neurons representing the latent variables \(_{1:L}=\{_{1},_{2},...,_{L}\},\ _{l} ^{n_{l}}\). The joint distribution is a Markov chain (Fig.2A) starting with \(p(_{L})=[_{L}^{T}(_{L})+g( _{L})-A(_{L})]\),

\[p_{}(_{0:L})=p(_{l})_{l=0}^{L-1}p_{}( _{l}|_{l+1}),\ \ \ \ p_{}(_{l}|_{l+1})=[ _{l}^{T}(_{l})+g(_{l})-A( _{l})].\] (7)

The natural parameter \(_{l}^{n_{l}}\) is a function of \(_{l+1}\) with parameters \(_{l}^{n_{l} n_{l+1}}\), which is written as \(_{l}=_{l}f(_{l+1})\), where \(f()\) is the activation function. And \(_{L}\) is constant. The sufficient statistic \((_{l})^{n_{l}}\) and the base measure \(g(_{l})\) is the function of \(_{l}\). \(A(_{l})\) is the normalize term (log-partition function) to make sure the sum of the probability equals to 1. In order to get the inference dynamic, we substitute the joint distribution Eq.(7) into the Langevin dynamic Eq.(3) obtaining,

\[_{z}_{l}}{t}=f^{}(_{l })_{l-1}^{T}[(_{l-1})-A^{}(_{l -1})]+^{}(_{l})_{l}+g^{}( _{l})+}_{l},\] (8)

where \(f^{}(_{l}),^{}(_{l})^{n_{l}  n_{l}}\) are diagonal matrices. The derivative of log-partition \(A^{}(_{l-1})\) is intractable for it needs complex integral. Here, we use a group of interneurons \(_{l-1}^{n_{l}-1}\) to represent the term \((_{l-1})-A^{}(_{l-1})\). It can be proved that \(A^{}(_{l-1})=E_{_{l-1} p_{}( _{l-1}|_{l})}[(_{l-1})]\) (See SI for detailed proof). Thus, in order to calculate \(A^{}(_{l-1})\), the interneurons need to produce samples \(_{l-1}\) following the distribution \(p_{}(_{l-1}|_{l})\) in a short time compared with the inference dynamic. Therefore, the dynamic of \(_{l-1}\) can be written as,

\[_{l-1}=(_{l-1})-(_{l-1}), \ \ \ _{u}_{l-1}}{t}=^{}( _{l-1})_{l-1}+g^{}(_{l-1})+}_{u}.\] (9)

By setting the time constant \(_{u}_{z}\)1, we can ensure that \(_{l-1}\) converges much faster than \(_{l}\), and the stationary distribution of \(_{l-1}\) corresponds to \(p_{}(_{l-1}|_{l})\). This leads to \(_{l-1}=(_{l-1})-A^{}(_{l-1})\).

Now, we can rewrite the inference dynamic Eq.(8) into,

\[_{z}_{l}}{t}=f^{}(_{l}) _{l-1}^{T}_{l-1}+^{}(_{l}) _{l}+g^{}(_{l})+}_{l}.\] (10)

The first term on the right side of the dynamic equation indicates that neurons \(_{l}\) receive feedback from interneurons \(_{l-1}\), which provides likelihood information \(p_{}(_{l-1}|_{l})\). The second term showsthat neurons \(_{l}\) receive prior knowledge \(p_{}(_{l}|_{l+1})\) from interneurons \(_{l}\) through a feedforward loop. The term \(g^{}(_{l})\) controls the self-connections within layer \(l\) (Fig.2B). \(_{l-1}\) is also called the error term in PCNs. Here, we show that the predictions in PCNs essentially estimate the log-partition.

Then, we can obtain the learning dynamic by substituting the joint distribution Eq.(7) into the gradient decent dynamic Eq.(4),

\[_{}_{l}}{t}=[(_ {l})-A^{}(_{l})]f(_{l+1})^{T}= _{l}f(_{l+1})^{T}.\] (11)

The derivatives of the log-partition function are stored in the interneurons \(_{l}\). As a result, the synaptic changes are determined solely by local neurons, adhering to Hebbian rules.

After inference and learning, our model can also generate observations. During joint generation, the dynamics of neurons \(_{1:L}\) follow the same principles as the inference dynamics described by Eq.(10). By substituting the joint distribution Eq.(7) into the Langevin dynamic Eq.(5), we can generate new samples from the model by,

\[_{x}_{0}}{t}=^{}( _{0})_{0}+g^{}(_{0})+} _{0}\] (12)

And for marginal generation, we can substitute the prior distribution described in Eq.(7) into Eq.(6) obtaining the dynamic of neurons \(_{1:L}\),

\[_{z}_{l}}{t}=^{}( _{l})_{l}+g^{}(_{l})+} _{l}.\] (13)

## 4 Neural adaptation accelerate the sampling process

Langevin sampling essentially performs random walks in local regions rather than the whole posterior space , because the drift term \(_{} p_{}(,)\) in Eq.(3) will vanish near the local minima and only noise term remains (Fig.3A). Sampling the entire posterior space is a time-consuming process and does not align with the brain's ability to perform tasks quickly. Therefore, it is important to implement a faster sampling algorithms for our model.

In this section, we show that by including noisy adaptation, the network is able to speed up the inference dynamic significantly. Adaptation is a common phenomenon observed in neural systems,

Figure 2: (A) The directed graphical model of the hierarchical exponential-family energy-based (HEE) model. (B) The inference and learning dynamic of HEE model. The red arrows represent the likelihood information and the black arrows represent the prior information. Neurons \(_{l}\) receive the likelihood information from \(_{l-1}\) and receive prior information \(_{l}\) from \(_{l}\). The interneurons \(_{l-1}\) receive the natural parameter \(_{l-1}\) from neurons \(_{l}\) and perform the Langevin sampling dynamic to approximate \(A^{}(_{l-1})\). Then, the interneurons compare it with the the sufficient statistic \((_{l-1})\) received from \(_{l-1}\) to calculate the value of \(_{l-1}\). The dashed line shows that the connection weights \(\) will perform gradient decent in the learning dynamic. (C) The neural connection diagram between layer \(l\) and layer \(l+1\).

where negative feedback mechanisms are employed to suppress neuronal responses when they reach high levels. Here, we show that the neural adaptation \(\) can introduce an auxiliary variable to implement a faster sampling algorithm, which is called second-order Langevin dynamics (SLD),

\[_{z}}{t} =_{} p_{}(|)-+},\] (14) \[_{v}}{t} =-}{2}+m},\] (15)

where \(m>0\) controls the adaptation strength. The auxiliary variable \(\) can keep the network moving while reaching the local minima, which essentially serves as the momentum term (Fig.3B). When there is no adaptation (\(m=0\)), the above dynamic will degenerate to the Langevin sampling (LS) described in Eq.(3). And the stationary distribution of the above dynamic remains to be \(p_{}(|)\) (See SI for proof). By substituting the joint distribution of exponential family Eq.(7) into the above dynamic, we can obtain the SLD inference dynamic,

\[_{z}_{l}}{t} =f^{}(_{l})_{l-1}^{T}_{ l-1}+^{}(_{l})_{l}+g^{}(_{l})- _{l}+}_{l},\] (16) \[_{v}_{l}}{t} =-_{l}}{2}+m}_{l}.\] (17)

Here we exemplify the neural adaptation with spike frequency adaptation (SFA) . In the neural system, the adaptation current \(_{l}\) accumulates the noise term \(_{l}\) coming from the ion concentrations, release of neural transmitters, activation/inactivation of ion channels and so on, which is described by Eq.(17). The adaptation current induces suppression on neurons \(_{l}\), acting as momentum variables to accelerate the sampling process (Eq.(16)).

We conduct further investigation to elucidate the precise mechanism by which noisy adaptation facilitates the acceleration of the sampling process in the HEE model. Considering that the energy function \(- p_{}(_{1:L}|)\) is non-convex, the sampling process can be divided into two parts. In the first part, the network needs to find a local minima and samples near the local minima, which takes a certain amount of time called recurrence time \(_{rec}\) (Fig.3C). In the second part, the network needs to leave the local minima and find a new one, which takes a certain amount of time called the escape time \(_{esc}\). Typically, we have \(_{rec}_{esc}\). The total time to get stationary distribution can be approximated by \(=_{esc}+_{rec}\). It can be proved that  the recurrence time is bounded by \(_{rec}=(1/_{1}(H_{J}))\). \(_{1}(H_{J})\) is the smallest eigenvalue of the matrix \(H_{J}\),

\[H_{J}=(H/_{z}&I/_{z}\\ 0&mI/(2_{v}))\] (18)

where \(H\) is the Hessien matrix of the energy function \(- p_{}(_{1:L}|_{0})\). The smallest eigenvalue of \(H_{J}\) is calculated as \(_{1}(H_{J})=\{_{1}(H)/_{z},m/(2_{v})\}\). Thus, in the case \(m>2_{1}(H)_{v}/_{z}\), the SLD can reduce the recurrence time \(_{rec}\) to accelerate the sampling process. And the escape

Figure 3: (A)(B) The sampling trace of Langevin dynamic and second-order Langevin dynamic of a mixture of 2D Gaussian distribution. (C) Illustration of the sampling process. States with lower energy indicates a higher probability, which needs to be stay longer. And the network needs to cross the energy barrier to reach a new local minima for the non-convex of the energy function. (D) \(_{1}\) and \((H)\) increases and decreases, respectively, with the changes of layers \(L\) and the total number of the neurons fixed (\(_{l}n_{l}=10000\)).

time is bounded by \(_{esc}=()})\). The determinant of \(H_{J}\) is calculated as \((H_{J})=(H)(m/2_{v})^{ n_{l}}\). Thus, \((H_{J})\) is monotonically increaseing with \(m\), indicating that the larger \(m\) is, the shorter the time it takes to escape from the local minima.

The analysis presented above demonstrates that the convergence speed of the inference dynamic is determined by the values of \(_{1}(H)\) and \((H)\). This valuable insight can be leveraged to guide the design of the network architecture, enabling the creation of more efficient and effective models. Specifically, with a fixed total number of neurons \(_{l}n_{l}\), increasing the number of layers \(L\) results in a deeper network, while decreasing the number of layers results in a wider network. Practically, we show that \((H)\) will decrease with layers \(L\) while \(_{1}\) will increase (Fig.3D), which indicates that there is a trade-off between the recurrence time \(_{rec}\) and the escaping time \(_{esc}\) with different layers \(L\) (See SI for detailed setting and analysis).

## 5 Experiment

In this section, we firstly validate the capability of HEE model for approaching complex distribution by examining the quality of generation. Then, we show that our model demonstrates similarity in the representation of natural images to the biological visual system. And adaptation can induce oscillatory behavior and transient overshoots in neurons during the inference phase.

### Generation

Firstly, we conducted experiments using three variations of the HEE model, each with different \((x)\) functions and sampling methods (Tabel 1), to evaluate their capabilities. The experiments were performed on both _2D synthetic datasets_ and the _FashionMNIST_. We use the fully connected architecture, i.e., \(_{l}\) has no zero elements. The results (second and third column) show that HEE with linear statistic (HEE-L) struggles to capture the complex distribution. Moreover, we theoretically prove that HEE-L can only approach unimodal distributions (See SI for detailed proof). For HEE-NL, some modes are missing while using the joint generation. And when the spacing between modes is large, there is an issue of non-uniformity among different modes. And the marginal generation converge much faster than the joint generation. In _FashionMNIST_, it takes less time for marginal method to get the generation of high-quality images.

Figure 4: Evaluation on _2D synthetic datasets_ and _FashionMNIST_: a mixture of four Gaussian distribution (first line), a mixture of four banana-shaped distribution (second line), pinwheel-shaped distribution (third line).

Then, we employ the HEE-NL-A with layers \(L=10\) on the _CIFAR10_ unconditional. The sparse connection is employed as a method to mimic the receptive field behavior found in biological systems. We quantitatively evaluate image quality of HEE-NL-A with Inception score  and FID score  in Tabel 2. Overall, we achieve a performance comparable with the previous EBMs. And the generation quality of the marginal method is better than joint method, which agrees with the previous results .

### Inference

We further use the HEE-NL-A trained on _CIFAR10_ to explore the relationship between the latent features and semantic information, including orientation, color and category.

_Orientation:_ Simple cells  and complex cells  are the most prominent and widely observed neurons in the biological visual system that exhibit tuning to orientation. They are found in the primary visual cortex (V1) of numerous animal species . We present the model with gabor images of different orientations commonly used in experiments (Fig.6A) and compute the mean and variance of the neural responses for each neuron. Then, we use a Gaussian curve with bandwidth limited from 20\({}^{}\) to 90\({}^{}\) and a two-modes Gaussian curve to fit the simple cell and complex cell, respectively (Fig.6A&B). We find that the proportion of simple cells and complex cells remains relatively consistent across each layer and both decrease with increasing layers in our model (Fig.6C).

_End stopping:_ In the HEE model, interneurons essentially represent the error term in the PCNs. We have observed the phenomenon of 'end stopping' in interneurons (Fig.6D), which aligns with the end-stopping behavior observed in error neurons in the PCNs .

_Color:_ Recent study shows that there is a hierarchical representation for chromatic processing across the ventral pathway of macaque . We present our model using reshaped natural images  and employ principal component analysis to demonstrate that the middle layer's neural representation's most informative dimensions carry chromatic information (Fig.6E).

_Category:_ Visual object recognition is believed to be solved by the brain hierarchically . A recent study  demonstrate that the inferotemporal cortex, situated in the deeper layer of the visual pathway, is capable of constructing a linear map of the object space. For each layer in our model, we employed a linear support vector machine (SVM) to classify the ten labels of the _CIFAR10_. The SVM was trained using the average neural responses as features. Fig.6F illustrates the projection of the features in the last layer onto the SVM weights corresponding to the "cat" and "dog" labels. Furthermore, we observe that the classification accuracy improves as we move up the layers of our model (Fig.6G), which is consistent with findings in both biological visual systems  and the artificial neural networks .

_Phenomena:_ Oscillations  and transients  are two kinds of spatial-temporal dynamic features in neural systems, which play a crucial role during the sampling process . Here, we show that by adjusting the adaptation strength \(m\), the oscillation frequency of the HEE model can span within the

   Model & IS & FID \\  HEE-NL-A (Joint) & 5.95 & 43.21 \\ EBM (single)  & 6.02 & 40.58 \\ HEE-NL-A (Marginal) & 6.47 & 37.05 \\ MEG (Generator)  & 6.49 & 35.02 \\ EBM (10 ensemble) & 6.78 & 38.20 \\ MEG (MCMC) & 7.31 & 33.18 \\   

Table 2: Table of Inception and FID scores.

   Model & \((x)\) & Sampling method \\  HEE-L & \(x\) & LS \\ HEE-NL & \(sigmoid(x)\) & LS \\ HEE-NL-A & \(sigmoid(x)\) & SLD \\   

Table 1: Table of different HEE models.

Figure 5: Marginal generation of HEE-NL-A on CIFAR10.

range of 20-80 Hz (gamma band), which is widely observed in visual systems  (Fig.6H). And stimulus-onset transients of the firing rate can also be enhanced by the adaptation (Fig.6I).

## 6 Discussion

The present study investigates the sampling-based inference and learning dynamic within the framework of an intrinsic generative model. We introduce the HEE model as a neural implementation that utilizes neural dynamics and Hebbian learning. Additionally, we demonstrate that the inclusion of

Figure 6: (A)(B) The red dots show the average firing rate of two neurons in \(_{1}\) with different gabor-like stimulus. Different tuning curves are used to fit simple cells and complex cells. (C) The proportion of simple cells and complex cells in each layer. (D) We show horizontal bars of varying lengths to the HEE. The red dots show the average firing rate of a neuron in \(_{1}\) whose corresponding neuron in \(_{1}\) is a simple cell preferring 0 degree. (E) The images are plotted at the location corresponding to the projection of their average neural response in layer 5 onto the first two principal components. Red images are located in the upper left corner, while blue-green images are located in the lower right corner. (F) The true label of cat, dog, and other categories should be respectively located in the second, fourth, and third quadrants. (G) The classification accuracy of the SVM in each layer. (H) We sampled and statistically analyzed the distribution of the highest firing rate frequencies of all neurons during the inference phase in the first 100\(_{z}\) for different values of \(m\). The gamma band is centered around the dashed line. (I) We sampled and statistically assessed the maximum change in firing rates of all neurons during the inference phase in the first 100\(_{z}\) for different values of \(m\). We refer to the mean of the maximum change values as the ‘average step size’. We utilize the average step size of the neurons during the sampling process as an indicator of transients.

neural adaptation can significantly accelerate the sampling process and give rise to various dynamic phenomena throughout the network. In this section, we will discuss several related theories and models.

**Probabilistic Population Code (PPC)** is another theory that explains how the brain perform Bayesian inference, in which neural responses are interpreted as the parameters of the probability distributions. We adopted the idea  that PPC theory incorporates two generative models. In the framework of PPC, the experimenter presents the subject with observation \(\) (gabor image) based on the semantic information \(\) (orientation), which actually defines an external generative model from \(\) to \(\). And the subjects holds an intrinsic generative model with latent variable \(\) represented by neural response to interpret the observation \(\). PPC theory integrates two generative models into a single generative model, in which the neural response \(\) is regarded as the observation generated from the semantic information \(\). We propose that the learning dynamic occurs exclusively within the intrinsic generative model, as the brain is not aware of the external generative model.

**Energy-based models (EBMs)** When EBMs were initially proposed , they had latent variables corresponding to neurons. Later, to enhance the model's expressive power, hierarchical structures were introduced . Such EBMs could typically ensure local learning in space. As artificial neural networks have become increasingly powerful, it has been observed that for generative tasks, there's no need to explicitly introduce neurons as latent variables within EBMs. Instead, one can directly employ a neural network to represent the energy . Training such EBMs often involves utilizing BP. The distinction between these EBMs and traditional EBMs is akin to the difference between dynamic systems and recurrent neural networks.

Regardless of whether it's the traditional EBMs or the new type of EBMs, both involve the challenge of estimating the partition function. This difficulty arises from the fact that as the depth of the energy function increases, the total sample space required multiplies the space for each layer. In the case of HEE, we allocate the partition function across each layer. As a result, the total sample space required is the sum of the spaces for each layer. This significantly reduces the required sample space.

**Predictive coding networks (PCNs)** The interneurons in the HEE model serve a similar role to the prediction error in PCNs. And our theoretical analysis shows that the predictions in PCNs essentially represent the decomposed log-partition function. And PCNs don't stress the sampling-based inference, which requires them to approximate the energy function using variation inference by delta function. Sampling-based inference can assist the network in exploring the posterior probability space, leading to a more accurate estimation of the energy function. Additionally, it can account for the observed neural variability in experiments.

**Diffusion models (DDPMs)** The HEE model and DDPMs share the same joint distribution and both exhibit a hierarchical Markov structure, which may contribute to the HEE model's strong expressive potential. The marginal generation is also called latent space MCMC , which is similar to the generation process of DDPMs. While DDPMs unfolds the Markov chain over time, the HEE model unfold it between layers of neurons. However, in order to reach a better performance, DDPMs use a fixed diffusion process as the inference dynamic, which may not be adopted by our brain since the latent variables in our brain carry semantic information (such as simple cells ).

**Speed up sampling** In previous work, inhibition neurons were used to serve as momentum terms to accelerate sampling . However, this approach required a one-to-one correspondence between inhibition neurons and excitatory neurons. In our approach, we consider the adaptive properties inherent in each neuron itself to serve as momentum, naturally resolving the one-to-one correspondence issue. Furthermore, our consideration extends to sampling in a non-convex energy space, which differs from the prior focus solely on convex space convergence properties .