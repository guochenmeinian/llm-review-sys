# Protecting Your LLMs with Information Bottleneck

Zichuan Liu1,2, Zefan Wang3, Linjie Xu2,4, Jinyu Wang2,

Lei Song2\({}^{}\), Tianchun Wang5, Chunlin Chen1, Wei Cheng6, Jiang Bian2

\({}^{1}\)Nanjing University, \({}^{2}\)Microsoft Research Asia,

\({}^{3}\)Tsinghua University, \({}^{4}\)Queen Mary University of London,

\({}^{5}\)Pennsylvania State University, \({}^{6}\)NEC Laboratories America

zichuanliu@smail.nju.edu.cn, wang-zf20@mails.tsinghua.edu.cn, linjie.xu@qmul.ac.uk, {wang.jinyu, lei.song, jiang.bian}@microsoft.com,

tkw5356@psu.edu, clchen@nju.edu.cn, weicheng@nec-labs.com

Authors contributed equally.\(\) Corresponding author.

###### Abstract

The advent of large language models (LLMs) has revolutionized the field of natural language processing, yet they might be attacked to produce harmful content. Despite efforts to ethically align LLMs, these are often fragile and can be circumvented by jailbreaking attacks through optimized or manual adversarial prompts. To address this, we introduce the Information Bottleneck Protector (IBProtector), a defense mechanism grounded in the information bottleneck principle, and we modify the objective to avoid trivial solutions. The IBProtector selectively compresses and perturbs prompts, facilitated by a lightweight and trainable extractor, preserving only essential information for the target LLMs to respond with the expected answer. Moreover, we further consider a situation where the gradient is not visible to be compatible with any LLM. Our empirical evaluations show that IBProtector outperforms current defense methods in mitigating jailbreak attempts, without overly affecting response quality or inference speed. Its effectiveness and adaptability across various attack methods and target LLMs underscore the potential of IBProtector as a novel, transferable defense that bolsters the security of LLMs without requiring modifications to the underlying models.

**WARNING: This paper contains AI-generated text that is offensive in nature.**

## 1 Introduction

The recent advancement of large language models (LLMs) has positioned them as the pivotal foundation in the computational understanding and generation of human language. Trained on extensive and diverse corpora, LLMs exhibit unparalleled proficiency in widespread applications . However, the vast pre-training datasets inherently introduce the risk of objectionable responses, including misinformation, hate speech, and other forms of harmful communication . Despite tremendous efforts to align LLMs  with societal norms and ethical guidelines, the jailbreak scenarios  still raise critical concerns. These jailbreak attacks, intending to bypass LLMs' safety alignment, not only compromise the integrity of LLM applications but also pose significant risks to users and society at large, undermining trust in generative AI technologies.

As a result, there is a growing imperative for the development of defense mechanisms that can neutralize jailbreaks. A typical strategy for this task is applying token-level random perturbations toprompts [7; 8] without the need for LLM fine-tuning or the detection of jailbreaking prompts. The key idea behind this approach is that adversarial prompts are sensitive to perturbations while benign prompts are less affected. However, the random perturbations might not consistently target tokens triggering jailbreaks. Furthermore, for some prompt-level attacks  (Figure 1 left), the effectiveness of random perturbations against semantic jailbreaks still has room for optimization. More recent approaches  filter the input by randomly applying semantic parsing functions, which are usually executed by LLMs and thus require specific prompts and have a high cost. These limitations prompt the question: Can we perturb adversarial prompts more effectively beyond mere randomness?

We answer this question by proposing Information Bottleneck Protector (IBProtector) as shown in Figure 1 right. This method selectively highlights informative prompt tokens and perturbs other tokens to suffice the Information Bottleneck (IB) principle [11; 12]. The rationale of IBProtector lies in competing the prompt to a minimal and explanatory form, with sufficient information for an answer and filtering out irrelevant content. To achieve this, we introduce a trainable, lightweight extractor as the IB, optimized to minimize mutual information between the original prompt and the perturbed one. Our empirical evaluations demonstrate that IBProtector possesses significantly superior attack mitigation capabilities compared to existing methods on GCG  and PAIR  jailbreaking. We also find that the defense efficacy of IBProtector does not compromise sensitivity to benign questions nor increase inference time consumption excessively. Moreover, IBProtector robustly generalizes across different attack methods and LLMs, even those not encountered during the training phase.

We summarize the contribution of our work as follows:

* We propose IBProtector, the first LLM jailbreak defending method based on the IB principle in the perspective of information compression, and give a tractable objective function. The process is lightweight and requires no modifications to the LLMs.
* The proposed IBProtector is empirically generalizable to different attack strategies and target LLMs, highlighting its potential as a transferable defense mechanism.
* We evaluate IBProtector on token-level and prompt-level jailbreaking datasets. The results show that IBProtector can successfully defend against adversarial prompts without substantially affecting LLMs' responsiveness and inference consumption.

## 2 Related Works

**Jailbreaking Attacks on LLMs.** With LLMs iterating rapidly in terms of complexity and functional capabilities, they have a potential influence on the attack surface [13; 14]. Recent literature has explored jailbreaking methods to generate harmful responses, which require manually-generated test cases , prompt engineering , or adversarial training [16; 3]. With these limitations, two classes of automated attack tools have gained prominence. The first is the _token-level_ jailbreak [6; 13; 2] that involves optimizing the set of tokens as input to the target LLM. The second class of _prompt-level_ jailbreaks employs semantic deception [17; 9; 18] and social engineering  to coax LLMs into

Figure 1: (left) Normal jailbreak process attacks aligned LLMs, where red parts represent an example of adversarial prefix and suffix. (right) Our IBProtector extracts compression information related to expected responses to mitigate jailbreaking attacks on LLMs.

generating objectionable content. Therefore, in this work, we employ the two most common and effective attack prompts mentioned above as the original jailbreaks for defense.

**LLM Defenses.** To prevent adversarial attacks, recent literature [4; 20; 21] has delved into several defense strategies for aligning LLMs with human values. Specifically, some perturbations [7; 8] do this by enumerating all possible content that may be partially erased and using security filters to provide a verifiable safety. Other methods to determine harmfulness are through checking [22; 23] or self-examination  if the perplexity is greater than a threshold, and also by summarizing and parsing  perturbation functions for semantic-level checking. Moreover, fine-tuning alignment is the other dominant domain for learning human values, including reinforcement learning from human feedback [25; 4], unlearning fine-tuning , and preference optimization . However, such alignments are usually vulnerable to being bypassed or requiring an expensive fine-tuning process. Different from them, we effectively prevent jailbreaking using only the retraining of a small model.

**Information Bottleneck.** IB [11; 12], originally proposed for signal processing, is able to reduce the length of sequences while retaining meaningful information. On this foundation, Alemi et al.  have introduced variational inference in the information bottleneck principle, bridging the gap between deep learning and IB for the first time. Thus, it has been wildly adapted in different sequence applications such as parsing , summarization , and explaining subspace [31; 32]. However, to the best of our knowledge, there are currently no applications for them due to the intractability of mutual information. We are the first to introduce IB to the domain of LLM alignment, providing explanatory phrases for LLMs that ensure security security at a small cost.

## 3 Preliminaries

### Large Language Models

A jailbreaking attack aims to circumvent the safeguard of a well-aligned LLM by embedding adversarial prompts within an original malicious question. Let \(X_{}\) represent a malicious question and let \(X\) denote the adversarial prompt generated by a red team function \(f_{}\), such that \(X f_{}(|X_{})\). A target LLM, denoted as \(f_{}\), generates a textual response \( f_{}(|X)\). While this LLM may be aligned to resist and deny malicious requests, it can still produce affirmative answers under the influence of an adversarial prompt  due to prompt manipulations like the _prefix_ and _suffix_. Thus, our goal is of no prevent the LLM from such jailbreaks. In this work, we introduce an auxiliary language model \(p_{}\), functioning as a front protector. This model serves to compress an adversarial prompt \(X\) into a sub-prompt \(X_{}\), thereby facilitating the target model to defend the manipulated input. Alternatively, when an input is safe, we preserve valid information for answering.

### Information Bottleneck Principle

Finding adverse relevant subsentences in text response tasks has unique challenges. Due to the variety of attacks from the red team, target LLMs often encounter inputs of various lengths and are influenced by disruptions like prefix and suffix interference. Inspired by the Information Bottleneck (IB) principle  that facilitates learning compacted representations while retaining relevant information for effective predictions, we explore its utility in the LLM context. The principle is based on the concept that an optimal representation contains _minimal_ and _sufficient_ relevant information necessary for the downstream task. The key is that \(X_{}\), a compressed subsentence extracted from the source sentence \(X\), encapsulates only the information useful for the expected response \(Y\). Formally, the objective of finding the optimal subsentence \(X^{*}_{}\) is defined by:

\[X^{*}_{}(X_{}|X)}{ }\;})}_{}- })}_{},\] (1)

where \(I(;)\) denotes mutual information between sentence pairs and \(\) is a hyperparameter that governs the trade-off between minimality and sufficiency constraints. For clarity, we call the former part in Eq. (1) the compactness quantifier, and the latter part the prediction quantifier. Notably, from the definition of mutual information, we have \(I(Y;X_{})=H(Y)-H(Y|X_{})\), where the entropy \(H(Y)\) is a statistic of the protected large model and remains constant independent of the defense process. Therefore, the process of maximizing the mutual information between the compressed subsentence \(X_{}\) and the target response \(Y\) can be reformulated into minimizing the conditional entropy of \(Y\) given \(X_{}\):

\[X_{}^{*}=*{arg\,min}_{(X_{}|X)}  I(X;X_{})+H(Y|X_{}).\] (2)

## 4 Information Bottleneck Protector

In this section, we introduce Information Bottleneck Protector (IBPProtector) to defend against potential adversarial prompts, which consists of a trainable extractor \(p_{}()\) and a frozen predictor \(f()\). The IBPProtector framework is depicted in Figure 2, where the extractor is responsible for compacting \(X\) into \(X_{}\). Meanwhile, the predictor facilitates the optimization of the compaction's informativeness. However, directly employing the IB principle in Eq. (2) is complicated due to the high-dimensional input of long texts . In the following subsections, we identify a tractable objective to address this challenge.

### A Tractable Objective for Compactness \(I(x;X_{})\)

As mentioned in Preliminaries 3.2, the term \(I(X;X_{})\) is included in IB to ensure that the sub-prompt \(X_{}\) is extracted from the adversarial prompt \(X\) with minimal information. To achieve this, IBPProtector trains an extractor \(p_{}_{}\) with the parameter \(\) to extract a proper \(X_{}\). However, the mutual information term \(I(X;X_{})\) might be biased when measuring the extraction ability of \(_{}(X)\). This term can occasionally favor low-entropy stop words to appear in \(X_{}\) rather than high-entropy informative words, thereby contravening the principle of compactness. This phenomenon was also discussed in . To address this problem, we modify the compactness quantifier by adapting a method from . We adjust their approach to suit the context of defending LLMs, considering an upper bound as follows:

\[I(X;X_{})_{X}[D_{}[_{} (X_{}|X)\|(X_{})]],\] (3)

where \(D_{}\) is the Kullback-Leibler divergence. The formulation in Eq. (3) provides a basis for further simplification by defining specific forms for \(_{}\) and \(\), enabling enhanced practical functionality through these definitions.

**Compactness Quantifier Implementation.** IBProtector leverages a perturbation mask \(M\) to extract sub-prompts, utilizing the extractor \(p_{}:^{T}\), where \(\{p_{}(X_{ t})=_{t}|t[T]\}\) and \(T\) is the length of the tokenized \(X\). An alternative operation is that autoregressive sampling predicts attribution scores representing the continuous probability of the mask based on previously sampled masks, and we defer this discussion to E.2. To facilitate the training process, for each forward pass we sample stochastic masks from a Bernoulli distribution that is parameterized by

Figure 2: The model framework of IBPProtector, where fire and snowflake denote frozen and trained parameters, respectively, and the small language model is optional. Given an input prompt, the extractor can extract the most informative parts for the predictor to respond to.

\(=[_{t}]_{t=1}^{T}\), i.e., \(M_{}(M|X)=_{t=1}^{T}(_{t})\). Therefore, our extraction goal is shifted from obtaining \(X_{ sub}\) to generating textually relevant attribution scores \(\) by optimizing \(\), i.e., \(_{_{t}}_{X}[D_{ KL}[_{}(M|X)|| (M)]]\), where \(X_{ sub}=X M\) and \(\) is the element-wise multiplication. As done in [31; 33], we define \((M)\) as the Bernoulli distribution with a sparsity parameter \(r(0,1)\). This choice regulates the generation of \(\), aligning it with the desired distribution of \((M)_{t=1}^{T}(r)\). Thus, the original compactness quantifier term \(I(X;X_{ sub})\) is transformed into a more tractable compactness loss \(_{M}\) as follows:

\[_{M}=_{t=1}^{T}[_{t}(}{r})+(1-_{t} )(}{1-r})].\] (4)

The derivation for simplifying the compactness objective is detailed in Appendix B. As investigated in , the parameterization process of the extraction target \(_{}\) and \(\) effectively limits the average number of non-zero elements in the mask and discourages high-entropy masks whose value is 'almost' deterministic, i.e., \(_{t} 1\) or \(_{t} 0\).

Furthermore, to penalize fragmented and non-contiguous sub-prompts, we introduce a continuity loss \(_{ con}\) to enhance the coherence of \(X_{ sub}\):

\[_{ con}=_{t=1}^{T-1}- _{t})^{2}}.\] (5)

We implement the extractor \(p_{}\) by a compact language model (any pre-trained language model suffices) and an MLP layer with a sigmoid activation to map the encoded prompt embeddings into \(^{T}\). The fine-tuning of the parameter \(\) is adjustable, typically restricted to the very last layer of the language model and the MLP. This approach significantly reduces the computational cost, with further details discussed in Section 5.5.

**Discretizing Attribution Scores.** Masks must be discrete due to the nature of the tokenization of the target model. If \(M\) were continuous, for instance, with a token \(X_{t}=21\) and a corresponding mask value \(M_{t}=0.3\), the product \(M_{t} X_{t}\) would result in a non-existent token in the vocabulary and could not be processed by the embedding layer of \(f_{ tar}\), leading to issues with token recognition by the LLMs. However, discretization blocks the gradient flow of the model. Therefore, we adopt a straight-through estimator (STE)  to implement the backward pass of Bernoulli sampling during training. As presented in , STEs employ a surrogate function to estimate the gradient for operations in the forward propagation.

### Modifying the Prediction Quantifier \(I(Y;X_{ sub})\)

Here we articulate how to generate the sub-prompt \(X_{ sub}:=X M\) and how to let it capture sufficient information for LLMs to generate the expected response \(Y\) in \(f()\), where masks \(M\) highlight essential tokens on \(X\).

**Applying Masks to Target LLMs.** A common practice for producing \(\) is to 'pad' the masked positions in sub-prompt \(X_{ sub}\) with non-informative tokens as perturbations . The reason for keeping perturbations instead of simply removing the token at masked positions is that we need gradients to flow at these positions for more effective optimization updates. Formally, we consider an uninformative token \(\) in text processing. Then, the 'padded' prompt instance is produced by a multiplicative replacement as:

\[=M X+(1-M).\] (6)

Note that during training we are padding at the level of token embedding of sentences rather than the level of token IDs. For the selection of \(\), we use a period ('\(\)') by default because there are no truly meaningless tokens in the vocabulary of target models under the decoder-only architecture (e.g., generative pre-trained transformer). We discuss the effect of different perturbation tokens in Appendix E.5.

**The Informativeness Predictor.** When utilizing the target LLM \(f_{ tar}\) for response generation, the masked prompt must contain information about \(Y\). To facilitate this, we assume access to gradient information from the embedding layer within \(f_{ tar}\), and that \(f_{ tar}\) shares the same tokenizer with \(p_{}\). This assumption enables end-to-end gradient backpropagation necessary for the optimization process.

We also give a reinforcement learning solution for the fully black-boxed model in Section 6. As discussed in Eq. (2), the prediction quantifier is shifted to minimize the conditional entropy when optimizing \(M\), where \(H(Y|X_{})=-_{X,Y}p(X M,Y) p(Y|X M)\). This loss is similar to the vanilla cross entropy function in supervised fine-tuning except for probability weights. However, as shown in previous literature [35; 33], minimizing this form of IB loss sometimes produces _signaling_ issues, leading to perceptually unrealistic explanations. Also, empirical findings  suggest that this approach does not hold up well to the performance of normal queries. To address these problems, we minimize an upper bound of the conditional entropy. This objective contains the cross entropy between the output of \(\{,Y\}\) and \(Y\), bounding the conditional entropy. Additionally, it includes the divergence between the output probabilities of \(\{X,Y\}\) and \(\{,Y\}\) within the response part, preventing the extraction result from deviating too far. This method is similar to the existing works in RLHF [4; 27], with the difference that we optimize the mask \(M\) rather than \(f_{}\). Formally, let \(f_{}(X,Y_{<t}):=(Y_{t}|X,Y_{<t})\) be the probability distribution of the position at \(Y_{t}\) generated by target model \(f_{}\) and prefix \(Y_{<t}:=[Y_{1},,Y_{t-1}]\), and then we adopt the following modified informativeness loss:

\[_{}=-_{t=1}^{|Y|} p(Y_{t}|,Y_{<t })+_{t=1}^{|Y|}D_{}[f_{}(,Y_{<t} )\|f_{}(X,Y_{<t})],\] (7)

where \(Y\) is an expected response of \(X\). The modified IB avoids the signaling issues by providing a general indicator of how well \(Y\) is predicted from \(\).

### Overall Learning Objective

IBProtector is optimized end-to-end and the learning objective is trained by minimizing the total loss:

\[=_{}+(_{M}+_{}),\] (8)

where \(\{,\}\) are weighting hyperparameters for the compactness loss and continuity loss, respectively. Selecting an appropriate value for sparsity \(r\) for regulating masks is critical. We choose \(r=0.5\) by default and provide further analysis in Section 5.5. The compaction requirement is more pronounced as \(\) gets larger (more \(\) tokens in \(\)). When \(=0\), there is still no guarantee that \(=X\) because the jailbreaking prompt \(X\) is suboptimal to minimize the informativeness loss. Overall, the philosophy of IBPProtector is that when a malicious prompt enters, we highlight informative tokens likely to be unsafe so that the target LLM itself can recognize them. On the other hand, namely, when a prompt is safe, IBPProtector keeps its information to normal reply. Furthermore, our approach is lightweight and the computational cost is deferred to Appendix E.8. We summarize the pseudo-code of IBPProtector in Appendix C.

## 5 Experiments

In this section, we evaluate the efficacy of our IBPProtector for defending against malicious attacks. In the following, we first outline the original attack methods, evaluation metrics, and baselines. Then, we give a detailed analysis of white-box attacks, transferability, adaptive attacks, and ablation studies.

### Experimental Setup

**Datasets and Attack Methods.** We mainly evaluate our IBProtector on three datasets: AdvBench , TriviaQA , and EasyJailbreak . The AdvBench dataset contains 520 examples of harmful or toxic behaviors, including profanity, graphic depictions, threatening behavior, misinformation, discrimination, cybercrime, and dangerous or illegal suggestions. We use two representative methods on this dataset as original attacks: PAIR  and GCG , which represent prompt-level and token-level jailbreak, respectively. TriviaQA is a normal question-answering dataset for reading comprehension, and we sample \(230\) examples for our benign answer evaluation. The target models are two open-sourced LLMs: LLaMA-2  and Vicuna . To explore the transferability, we select three other highly effective attacks on the EasyJailbreak dataset: Autodan , ReNellm , and GPTFuzz . We sample \(138\) and \(185\) adversarial prompts for LLaMA-2 and Vicuna, respectively.

**Baselines and Metrics.** We compare our defense approach with the following six representative baselines: Fine-tuning , Unlearning LLM , Self Defense , Smooth LLM , RA-LLM ,and Semantic Smooth . We use greedy decoding for LLM inference in our experiments by default for better reproducibility. As for metrics, we employ Attack Success Rate (ASR), Harm Score, and GPT-4 Score to assess IBProtector's effectiveness and adaptability in defense comprehensively , where lower is better. To examine if the defense methods refuse to answer benign prompts or not , we also employ Benign Answering Rate (BAR) in the normal TriviaQA tasks, where higher is better. For each evaluation metric, we mark **bold** and underline as the best and second result, respectively. More experimental details about baselines, implementations, and metrics are available in Appendix D.

### Main Results

Table 1 summarizes the results of previous state-of-the-art methods and our defenses for prompt-level and token-level jailbreaks. IBProtector consistently outperforms other state-of-the-art methods across three metrics on PAIR and GCG. Specifically, the prompt-level PAIR jailbreak leads to high ASRs of \(87.5\%\) and \(67.5\%\) on the two target LLMs, respectively. The existing defense methods perform poorly, while employing IBProtector dramatically reduces ASRs of PAIR to \(19.2\%\) and \(16.7\%\). Similarly, in the case of the GCG attack, which incorporates an added suffix, IBProtector reduces ASRs from \(82.5\%\) and \(27.5\%\) to \(1.7\%\) and \(0.8\%\). These compelling results highlight the efficacy of our IBProtector in mitigating adversarial prompts, far surpassing current methods. Besides the ASR, we also assess the harmfulness of outputs and the GPT-4 score, both demonstrating that our method identifies adversarial prompts with superior results than other methods. Furthermore, on the normal TriviaQA dataset, IBProtector has little negative effect on LLMs' response rate to benign prompts, witnessing an insignificant reduction of BARs. We give an example of IBProtector in Figure 3 to illustrate the defense process. Some tokens with an intuitively low density of information are perturbed with '.', and the expected response of the denial statement is retained for LLM judgment. We also explore the effect of different perturbation tokens and evaluate the informativeness between \(X_{}\) and \(X\) using METEOR  and BERTScore . More details and real cases can be found in Appendix E.

    &  &  &  \\  Model & Method & ASR \(\) & Harm \(\) & GPT-4 \(\) & ASR \(\) & Harm \(\) & GPT-4 \(\) & BAR \(\) \\   & Original Attack & 87.5\% & 4.034 & 3.008 & 82.5\% & 0.244 & 4.300 & 97.8\% \\  & Fine-tuning & 62.5\% & 2.854 & 2.457 & 32.5\% & 0.089 & 2.114 & 94.8\% \\  & Unlearning LLM & 66.7\% & 2.928 & 2.496 & 40.8\% & 0.123 & 2.537 & 92.2\% \\  & Self Defense & 44.2\% & 2.585 & 1.692 & 12.5\% & -1.170 & 1.400 & 79.6\% \\  & Smooth LLM & 68.3\% & 3.115 & 2.642 & 24.2\% & -1.252 & 1.767 & 90.9\% \\  & RA-LLM & 34.2\% & 2.446 & 1.832 & 8.3\% & -1.133 & 1.411 & 95.2\% \\  & Semantic Smooth & 20.0\% & 2.170 & 1.525 & **1.7\%** & -0.842 & 1.058 & 95.7\% \\   & IBProtector & **19.2\%** & **1.971** & **1.483** & **1.7\%** & **-1.763** & **1.042** & **96.5\%** \\    & Original Attack & 67.5\% & 3.852 & 1.617 & 27.5\% & 0.325 & 2.517 & 98.7\% \\  & Fine-tuning & 47.5\% & 2.551 & 1.392 & 12.5\% & -0.024 & 1.233 & 97.0\% \\  & Unlearning LLM & 49.2\% & 2.507 & 1.383 & 12.5\% & -0.084 & 1.258 & **97.4\%** \\   & Self Defense & 45.0\% & 2.682 & 1.525 & 11.7\% & 0.208 & 1.492 & 92.6\% \\   & Smooth LLM & 43.3\% & 2.394 & 1.342 & 4.2\% & -0.189 & 1.100 & 95.2\% \\   & RA-LLM & 40.0\% & 2.493 & 1.362 & 4.2\% & -0.070 & 1.116 & 97.0\% \\   & Semantic Smooth & 40.8\% & 2.250 & 1.333 & 10.0\% & -0.141 & 1.417 & 96.5\% \\    & IBProtector & **16.7\%** & **1.315** & **1.125** & **0.8\%** & **-1.024** & **1.000** & 97.0\% \\   

Table 1: Defense results of state-of-the-art methods and IBProtector on AdvBench.

Figure 3: An example of the IBProtector. We use ’.’ perturbation in the adversarial prompt thus all uninformative tokens are replaced with ’.’.’.

### Transferability

We conduct transfer experiments on the trained extractor of IBProtector to answer the following two questions: whether the IBProtector can defend against _other attack methods_ unseen during training or protect _other target models_ that have not been detected during training.

We first attack various competitors with three different types of stealthier adversarial from the EasyJailbreak datasets, while the IBProtector is only trained on the PAIR and GCG datasets with the corresponding target LLMs. As shown in Table 2, the results show that IBProtector is the best defense on 5/6 (three metrics on two LLMs). These adversarial prompts have stealthy jailbreak information, and thus defense methods like Smooth LLM fail to transfer, almost losing defense ability and even increasing ASRs. Our IBProtector can still suppress the jailbreaking phenomenon significantly, which reduces the ASR from \(88.6\%\) and \(29.0\%\) to \(18.9\%\) and \(0.7\%\). Consistent with the results in Table 1, we observe that LLMa-2 has better intrinsic defense ability than Vicuna due to being safety alignment, so it easily recognizes harmful prompts when they are highlighted by IBProtector. We also conduct an additional experiment to evaluate the effectiveness of defense methods against cipher-based attacks , which is deferred to Appendix E.3.

Moreover, we transfer the extractor of the IBProtector trained on Vicuna-13B to defend adversarial prompts for other target models. We measure ASRs on an assortment of comparably-sized target models, including Vicuna-7B-v1.5 , LLMaMA-2 (7b-chat-hf) , ChatGLM3-6B , Mistral-7B-v0.1 , ChatGPT (gpt-3.5-turbo-0301), GPT-4 (gpt-4-0613) , with greedy decoding as default for more deterministic results. We use the adversarial prompts of PAIR in EasyJailbreak datasets and apply each LLM's default conversation template when attacking them. Figure 4 summarizes the comparison between IBProtector and Smooth LLMs, where the number of copies indicates the ensemble number for Smooth LLM. The masks generated by IBProtector's extractor are still evidently effective in mitigating jailbreaks for unseen target models, while random masks with perturbations (Smooth LLMs) are not. This fact suggests that IBProtector extracts effective harmful information still identifiable by other target LLMs' intrinsic defense ability.

    &  &  \\  Method & ASR \(\) & Harm \(\) & GPT-4 \(\) & ASR \(\) & Harm \(\) & GPT-4 \(\) \\  Original Attack & 88.6\% & 2.337 & 4.225 & 29.0\% & 2.167 & 1.883 \\ Fine-tuning & 26.8\% & 1.124 & 1.772 & 5.1\% & 1.597 & 1.192 \\ Unlearning LLM & 28.3\% & 1.127 & 1.815 & 5.1\% & 1.534 & 1.233 \\ Self Defense & 28.7\% & 1.291 & **1.725** & 8.7\% & 1.439 & 1.792 \\ Smooth LLM & 81.1\% & 1.673 & 2.168 & 35.5\% & 1.720 & 1.992 \\ RA-LLM & 54.1\% & 1.027 & 1.892 & 2.2\% & 1.484 & 1.253 \\ Semantic Smooth & 49.2\% & 0.417 & 2.022 & 5.1\% & 1.116 & 1.101 \\  IBProtector & **18.9\%** & **0.031** & 1.854 & **0.7\%** & **0.608** & **1.036** \\   

Table 2: Evaluating the transferability of defensibility on EasyJailbreak datasets, where original adversarial prompts are generated by Autodan, GPTFuzz, and ReNellm.

Figure 4: IBProtector’s extractor and Smooth LLMs defense results from other target models, where a lower ASR is better. IBProtector is interpreted as masking the most useless information, whereas Smooth is interpreted as randomizing masks where the number of copies is a way of ensemble masks.

### Adaptive Attacks

Adaptive attacks rely on a red team to manually adapt an existing attack to exploit the weak points of each defense. While rule-based or longtail Encoding mutations are insufficient for adapted attacks as they are fixed, we select a prompt optimization, PAIR, to explore the iteration number of successful jailbreaks with or without defense mechanisms. If the number of iterations is large, it is difficult to be jailbreaking by adapted attacks. We compare several methods where the filter exists: Smooth LLM, RA-LLM, Semantic Smooth, and IBProtector. We set the maximum number of iterations to \(20\), with three mutants per iteration. As shown in Table 3, the experimental results indicate that IBProtector can mitigate adaptive attacks and make them more costly compared with other baselines.

### Ablation Study

In this section, we analyze the effect of the three hyperparameters in our IBProtector's loss: the compactness weight \(\), the continuity weight \(\), and most importantly the sparsity \(r\). Our default parameter set is \(=0.5\), \(=1.0\), and \(r=0.5\). The ablation study is conducted on Vicuna-13B with PAIR attacks, using ASR, BAR, and Harm scores as indicators. The results are shown in Figure 5. The choice of different \(\) does not seem to have a significant effect on mitigating attacks, indicating the robustness of our IBProtector. Note that the ASR significantly when \(\) is large, a.k.a, when the compactness term dominates, this is because the information about \(\) cannot be accurately predicted, and thus the BAR also decreases dramatically. Furthermore, the sparsity parameter \(r\) casts a strong effect on the performance. When \(r\) decreases, generated masks are sparser and more tokens are replaced by the meaningless \(\), resulting in a decrease in both ASR and BAR. This comes with the trade-off that a low \(r\) could lead to the target model considering the highlighted prompts \(\) meaningless and answering with sentences like "I'm sorry, but it's unclear...". Therefore, we adopt \(r=0.5\) as the default parameter for the information bottleneck, which is also consistent with the sparsity constraint in most work [31; 33]. In addition, we explore the model size and fine-tuning parameters of extractors \(p_{}()\), which is deferred to Appendix E.6.

## 6 Further Discussion

Since this training objective requires knowledge of the tokenizer of the target LLM, it is not possible to directly train Eq. (7) in some non-open-source models, such as GPT-4. Whereas the \(\) of transferability extracted by \(p_{}()\) may not satisfy the new target LLM distribution, which may lead to incorrect token outputs. Inspired by  but differently generating adversarial prompts, we can use

Figure 5: Ablation study of the PAIR attacks on Vicuna-13B.

    &  &  \\  Method & Iteration \(\) & ASR \(\) & Iteration \(\) & ASR \(\) \\  Original Attack & 6.06\(\)6.17 & 92.0\% & 13.76\(\)7.04 & 52.0\% \\ Smooth LLM & 5.86\(\)4.73 & 96.0\% & 14.06\(\)6.91 & 52.0\% \\ RA-LLM & 6.38\(\)5.69 & 90.0\% & 13.32\(\)7.09 & 58.0\% \\ Semantic Smooth & 8.40\(\)6.62 & 86.0\% & 14.28\(\)7.61 & 44.0\% \\  IBProtector & **15.60\(\)5.64** & **52.0\%** & **16.18\(\)6.06** & **36.0\%** \\   

Table 3: Average number and attack success rate of iterations required for a successful jailbreak by an adaptive attack on 50 instances.

reinforcement learning (RL) to align reply target information to further fine-tune the extractor \(p_{}()\). Thus the IB objective function in Eq. (2) can be written in the form of maximizing the expected reward \(_{X_{} p_{}(X), f(|X_{})}[(Y;)]\) to avoid the gradient propagation hassle, where \(Y\) an expected response of \(X\). Besides, it is common practice  to augment it with a Kullback-Leibler divergence penalty \(D_{}(p_{}||p_{}^{})\) to encourage the generation to stay a reference policy \(p_{}^{}\). Formally, the training objective of maximizing \(I(Y;X_{})- I(X;X_{})\) in Eq. (8) is expressed as:

\[_{}[(Y;)]- D_{}[p_{ }(X)||p_{}^{}(X)]}_{}-_{M}+_{})}_{},\] (9)

where \(\) is a penalty weight as suggested by . The design of the reward function \((;)\) is important. Our target needs to contain information about the expected responses, thus cosine similarity  can be employed for labeling consistency alignment as follows:

\[(Y;)=-)}{\|(Y)\|^{2}\| ()\|^{2}},\] (10)

where \( f_{}(|)\) (\(\) is given in Eq. (6)) is the generated text from the target LLM \(f_{}\) during training up to the current iteration, and \(()\) represents the sentence embedding model in .

However, the current low success rate of attacks on non-open-source models (see Figure 4), coupled with the need for a large amount of instance data for RL training , prevents us from testing Eq. (9). This can be left for future work.

## 7 Limitation and Impact Statement

**Limitations.** Although IBProtector operates as an extractor and is not confined to a specific task, our experiments predominantly focus on protecting the target model from jailbreaking attempts. The role of IBProtector is to merely highlight the more harmful and informative parts, serving as explanations within harmful instances, while the primary defense relies on the target large model itself. Consequently, a well-aligned LLM with better performance may be better equipped to recognize these highlighted explanation parts. Furthermore, extracted sub-prompts merely act as intermediate results lacking fluency and coherence, and there is no guarantee that their integration into responses would be in context, i.e., perturbations in filling may result in other target LLMs being out-of-distribution. Thus the corresponding protector is the best, if the target LLM is a black-box, see the solution given in Section 6. Finally, high dimensionality challenges, such as images and toxic documents, have not been validated due to our lack of benchmarks, which can be left in the future.

**Impact Statement.** The development of robust countermeasures against the malicious exploitation of LLMs is of paramount importance for the human community and society at large. As these models become increasingly integral to a wide range of applications, the potential for them to be used in ways that deviate from human ethical standards grows. Efforts to adapt LLMs to reduce the spread of toxic content are critical. This underscores the need for a robust framework that LLMs align with societal values and operate within ethical boundaries, fostering trust and safety. Moreover, our efforts contribute to the responsible advancement of technology, preventing the erosion of social norms, and offering highlighted phrases as an explanation of harmful information. This approach will ensure that technology enhances the defenses of LLMs, ultimately enabling more informed and harmless decisions across all sectors of society.

## 8 Conclusion

In this work, we proposed IBProtector, the first defense against LLM jailbreak attacks based on the IB principle. The design allows a tractable optimization to produce compacted and label-preserving sub-prompts. Our method is lightweight and does not require modifications for expensive retraining or fine-tuning of the target LLM, and its trained extractors have transferable attack-mitigation ability. We further give a solution for which gradient search is not available in IBProtector. Relevant comparative studies confirm that IBProtector surpasses existing defense methods without substantially affecting the LLM's ability to reply and reasoning speed. A potential issue is that extracted information only highlights the most harmful parts as intermediate results that are difficult for humans to understand. Hence, it will be interesting to explore generating fluent sub-prompts.