# KuaiSim: A Comprehensive Simulator for Recommender Systems

Kesen Zhao\({}^{1}\), Shuchang Liu\({}^{2}\), Qingpeng Cai\({}^{2}\), Xiangyu Zhao\({}^{1}\),

Ziru Liu\({}^{1}\), Dong Zheng\({}^{2}\), Peng Jiang\({}^{2}\), Kun Gai\({}^{3}\)

\({}^{1}\)City University of Hong Kong, \({}^{2}\)Kuaishou Technology, \({}^{3}\)Unaffiliated

{kesenzhao2-c,ziruliu2-c}@my.cityu.edu.hk

{liushuchang, caiqingpeng, zhengdong, jiangpeng}@kuaishou.com

xianzhao@cityu.edu.hk, gai.kun@qq.com

The first two authors contributed equally to this workCorresponding author

###### Abstract

Reinforcement Learning (RL)-based recommender systems (RSs) have garnered considerable attention due to their ability to learn optimal recommendation policies and maximize long-term user rewards. However, deploying RL models directly in online environments and generating authentic data through A/B tests can pose challenges and require substantial resources. Simulators offer an alternative approach by providing training and evaluation environments for RS models, reducing reliance on real-world data. Existing simulators have shown promising results but also have limitations such as simplified user feedback, lack of consistency with real-world data, the challenge of simulator evaluation, and difficulties in migration and expansion across RSs. To address these challenges, we propose KuaiSim, a comprehensive user environment that provides user feedback with multi-behavior and cross-session responses. The resulting simulator can support three levels of recommendation problems: the request level list-wise recommendation task, the whole-session level sequential recommendation task, and the cross-session level retention optimization task. For each task, KuaiSim also provides evaluation protocols and baseline recommendation algorithms that further serve as benchmarks for future research. We also restructure existing competitive simulators on the KuaiRand Dataset and compare them against KuaiSim to further assess their performance and behavioral differences. Furthermore, to showcase KuaiSim's flexibility in accommodating different datasets, we demonstrate its versatility and robustness when deploying it on the ML-1m dataset. The implementation code is available online to ease reproducibility 3.

## 1 Introduction

Reinforcement Learning (RL)-based recommender systems (RSs) have drawn considerable attention in both academia and industry . They regard the recommendation procedures as sequential interactions between users and agents, and learn an optimal recommendation policy that maximizes the long-term cumulative reward from users. While it is theoretically superior to standard learning-to-rank methods , it is usually sub-optimal to evaluate the RL model using offline data, because of the missing online exploration and the impossibility of counterfactual evaluation. As an alternative, one may directly deploy RL models in online environments and let them interact with real users. In practice, this turns out to be challenging and resource-intensive , since an untrained or premature recommendation model can adversely impact the user experience and lead to undesirable real-time data, subsequently affecting the model's training performance . In order tomitigate these challenges, the research of user simulators has emerged recently to serve as a pre-online verification method for RL models [8; 14; 11; 37; 44; 40; 57]. Intuitively, simulators offer a practical solution for simulating user responses, allowing for the training and evaluation of RS models that are challenging to assess using offline data alone. And it enables researchers to iteratively improve RS models without relying exclusively on real-time user interactions.

Despite the effectiveness of existing simulators in some specially designed recommendation scenarios [14; 44], they are still far from the real-world environment in several aspects, which limits their effectiveness in evaluating online learning models: 1) While the most standard setup simulates single immediate feedback, user responses in most web services are multi-behavior in essence, for example, in short-video recommendation platforms, users can click, like, forward, or download. 2) In addition to the aforementioned immediate responses, a user may leave the app and then come back, generating a leave signal and a retention signal, but existing simulators do not address these long-term or delayed behaviors. The retention signal, in particular, is closely related to some crucial evaluation metrics (i.e., the daily average users) of online systems [54; 27], which could be identified as an essential building block that is still seriously underestimated in the research field. 3) Many rule-based simulators  are specially constructed for certain recommendation scenarios that may restrict their accommodation towards other recommendation services. 4) More recent simulators [44; 41] addressed the previous issue through pretraining of simulators according to log data, but during online interactions, they need to sample a user according to a pretrained user generator in addition to the user response model. This may further amplify the inconsistency between the simulated environment and the real-world data distribution. 5) Finally, simulators are capable of evaluating recommendation models, but there is still limited research on how to evaluate a simulator. In other words, we can only be certain of the superiority of an RL method when it achieves better results on the simulator and the simulator is consistent with the real-world environment.

To meet this demand, we propose KuaiSim, a comprehensive simulator that provides a user response environment at three distinct task levels: the request level recommendation task addresses the multi-behavior feedback and list-wise evaluation, the whole-session level sequential recommendation task addresses the long-term reward optimization under the standard RL setting, and cross-session level recommendation task address the retention optimization problem. The resulting simulator consists of a user immediate response model that generates feedback for each recommendation, a user leave model that specifies the end of a session, and a user retention model that determines how long the user returns to the system and starts a new session. To ensure its consistency with the real-world environment, KuaiSim uses the log data to pretrain the user response models and engage user sampling during simulation. This also allows for flexible adaptation on other datasets as long as the required data format suffices. In summary, our contribution can be summarized as follows:

* We propose a comprehensive simulator, KuaiSim, that covers three levels of recommendation tasks to encompass a majority of recommendation challenges. We also provide comparisons of various competitive algorithms for each task as benchmarks to boost future research work.
* Our refined simulator construction and evaluation process is user-friendly and flexible, and we showcase the data migration capabilities of KuaiSim on both the original dataset KuaiRand and the widely available ML-1m benchmark dataset.
* Additionally, we conduct a comparative analysis of KuaiSim against existing simulators, including RecoGym , RecSim , RL4RS , and VirtualTaobao . The results demonstrate that our simulator excels in its ability to approximate real-world environments.

## 2 Preliminary

### List-wise Recommendation

The list-wise recommendation problem focuses on the interconnected nature of items within a presented list, as highlighted in our previous studies [56; 59; 60]. The main objective is to understand and learn from the distinctions between including or excluding specific items in the exposed list [2; 32]. Recent research has indicated that in multi-stage recommendation systems, re-ranking models can effectively capture item correlations due to the reduced candidate set size, enabling the utilization of powerful neural models . In the past few years, there has been an ongoing discussion regarding the generative perspective of list-wise recommendation [16; 29]. To address the challenge of the vast combinatorial output space for lists, the generative approach directly models the distribution of recommended lists and generates entire lists using deep generative models. For instance, ListCVAE  employs Conditional Variational Autoencoders (CVAE) to capture positional biases and item interdependencies within the list distribution. However, our previous studies  have revealed that ListCVAE struggles with accuracy-diversity trade-offs. One limitation of these methods is that they fail to consider the broader impact of a single recommendation on the entire session.

### Sequential Recommendation

The session-based recommendation (SBR) considers the beginning and end of an interaction session. Our ultimate objective is to optimize the overall future reward of the user session, which aligns more with the notion of partial SBR as defined in . In our settings, the prevailing solution relies on the Markov Chain assumption to model the dynamic transitions in user-system interactions. The primary challenge in this line of research is how to construct a representative user state based on extensive historical data [22; 25]. Initial solutions to the recommendation problem utilized collaborative filtering techniques [7; 18; 24; 35], while subsequent approaches incorporated deep neural networks  such as Recurrent Neural Networks , Convolution Networks  and Self-Attention [42; 53; 21] to enhance the model's expressive power. These techniques aim to capture the rich and intricate information contained in user and item features, as well as their interaction histories. However, the common underlying principle of these methods is to accurately encode long-term histories without directly optimizing for long-term user rewards.

### Retention Optimization

The objective of the next session recommendation is to suggest the next session, based on the analysis of inter-session dependencies within historical sessions . In addition to considering dependencies within individual sessions, the inclusion of inter-session dependencies, such as user retention, has been proven to be crucial for improving the overall performance of recommendations by our previous works [54; 3]. Multi-level session data exhibits a hierarchical structure, encompassing multiple levels, including interaction and session levels. Within this framework, both the dependencies within each level and those across different levels play a pivotal role in influencing subsequent recommendations. For instance, the recommended results from the previous session can impact the time interval before the user initiates the next session (return time). Consequently, effectively capturing and accurately modeling inter-level dependencies, while optimizing user retention, poses a significant challenge . Unfortunately, there is a scarcity of research addressing this area. In response to this need, our simulator offers a unique capability to predict the return time of users, providing a valuable tool for conducting in-depth research in this under-explored domain.

### Existing Simulators

The efficacy of reinforcement learning in recommendation systems is attributable to its ability to learn from user feedback and adapt to evolving user preferences over extended periods [1; 12; 30; 61]. However, the application of reinforcement learning to real-world tasks can be challenging due to the large volume of data involved, which can result in high sampling costs . To address this challenge, researchers have developed simulators for recommender systems using reinforcement learning (RL) algorithms. In this paper, we provide an overview of the existing four simulators.

**RecoGym** RecoGym overcomes the issue of exploding variation and allows for the simulation of user reactions to arbitrary recommendation policies. It is based on a cycle of collecting performance data, developing a new version of the recommendation model, A/B testing, and rolling out, which shares similarities with the RL setup.

**RecSim** Recsim is a configurable simulation platform, which supports sequential interaction with users and allows for easy configuration of environments with varying assumptions about user preferences, item familiarity, user latent state and dynamics, and choice models.

    &  &  \\   & & Request-level & Whole-session & Cross-session \\  RecoGym & & & ✓ & \\ RecSim & & & ✓ & \\ RL4RS & ✓ & ✓ & ✓ & \\ Virtual-Taobao & ✓ & & ✓ & \\  KuaiSim & ✓ & ✓ & ✓ & ✓ \\   

Table 1: A comparison between KuaiSim and other simulators.

**RL4RS** Current research on RL-based recommender systems (RL-based RS) lacks a properly validated simulation environment, advanced evaluation methods, and a real dataset, leading to a reality gap. To address this, the RL4RS dataset is introduced, which aims to bridge the reality gap in current RL-based RS research. The dataset is collected from a popular game by NetEase Games and is anonymized using a three-phase anonymization procedure.

**Virtual-Taobao** Virtual-Taobao is a simulator that is trained using historical customer behavior data. To enhance the simulation accuracy, it utilizes GAN-SD (GAN for Simulating Distributions) and MAIL (Multi-agent Adversarial Imitation Learning) for generating customer features with better distribution matching and more generalizable customer actions.

But all these simulators ignore the optimization of long-term feedback, such as user retention, and mostly support only a single recommended task. In Table 1, we summarize the characteristics of existing works and our KuaiSim in terms of datasets and tasks. It can be seen that our KuaiSim benchmark is the only one that meets all requirements.

## 3 Methodology

This section begins by providing a comprehensive explanation of the fundamental definitions and settings pertaining to our three tasks: request level, whole-session level, and cross-session level. Subsequently, we delve into the intricacies of constructing our simulator, offering a detailed account of the process. Lastly, we present an introduction to the dataset we employ and conduct a thorough analysis of the KuaiRand dataset.

### Multi-level Reinforcement Learning-based Recommendation

We consider the RL-based recommendation task as a sequential interaction between a recommender system (agent) and users (environment) and emphasize the multi-behavior and cross-session nature of this interaction. Specifically, users may provide short-term feedback in different formats such as clicks, views, and likes; and they may also provide delayed long-term feedback like the retention signal that can only be observed when users return to the system. We provide the overview of this comprehensive interaction paradigm as Figure 1, which is a cross-session multi-behavior user-system interaction flow. Formally, in the \(t\)-th interaction step in a specific session, the recommendation system receives a user request \(_{t}\) and generates a recommendation \(_{t}\) as action. Here we assume that each user request consists of a set of static user profile features \(\) and the most recent interaction history \(_{t-1}\) that dynamically changes through time. We also assume a candidate item set \(\), so a typical recommendation action has \(_{t}^{K}\), which is a list of size \(K\). And a user session typically refers to the continuous interactions of the user from the starting point when opening the app until the exit. On the other side of the flow, the user receives the recommendation and provides the user feedback \(_{t}\) of three types: 1) The immediate feedback \(_{t}^{(I)}^{b K}\) directly reveals the user's preference of the recommended items, and \(b\) represents the number of behavior signal types; 2) The leave signal \(_{t}^{(L)}\{0,1\}\) specifies whether the user stops the current session and exits the app; 3) If the user leaves the current session, there is an additional return time (i.e. retention) signal \(_{t}^{(R)}\) that indicates how long the user will come back and start a new session. Then, we can formulate the three-level learning tasks with different learning goals and each focuses on a specific research question, namely, the request level, the session-level, and the cross-session level.

Figure 1: The workflow of simulator and a common MDP setting for user-system interaction.

**Request level** This problem focuses on single request optimization where the recommendation list could be optimized in a list-wise way. Different from the standard learning-to-rank paradigm where a point-wise model is trained to predict the ranking score of each individual item, the list-wise recommendation assumes that each recommendation action corresponds to a list of items that are exposed to the user in order, and it addresses the existence of item correlations within recommendation lists . In this case, conventional point-wise evaluations like Recall and NDCG metrics become ineffective in evaluating these cross-item patterns, and simulators are introduced to fill in the gap.

**Whole-session level** When we look at each user session as a whole and want to optimize the overall recommendation performance of the entire session, cross-request influences are included in the picture and the problem corresponds to the most standard MDP setting for RL-based models: Each recommendation drives a user state transition and thus the long-term performance in the session, which could be effectively modeled as a Markov Decision Process .

**Cross-session level** Further expanding our view to the entire interaction sequences across sessions, an extra user return time signal becomes observable, which refers to the time interval between the last request of a session and the first request of the subsequent session. This introduces an additional learning goal of retention optimization, which aims to learn a policy that provides satisfactory recommendations that can reduce the user's return time. Note that this learning goal is directly related to daily average users (DAU) which is the core evaluation metric for many web services. However, its optimization is extremely challenging for its uncertainty and uncontrollable delay .

### Simulator Building Blocks

As we would simulate the online environment and provide a training and evaluation platform for all aforementioned tasks, our simulator aims to provide the multi-behavior immediate responses, the user leave model, and the user retention model. Formally, the user simulator is a response generation function for recommendation actions \(:_{t},_{t}_{t}\). We provide a detailed workflow of this function in Algorithm 1 which consists of three feedback generation modules:

```
InputFormat: observation \(\), \(_{:t-1}\), and recommendation action \(_{t}\) Output: immediate feedback \(_{t}^{(I)}\), leave signal \(_{t}^{(L)}\), and retention signal \(_{t}^{(R)}\) (cross-session) The user immediate response module:
1: User history encoding \(_{t}\) Transformer (\(\), \(_{:t-1}\))
2: Ground truth user state \(_{t}_{t}\)
3: Behavior attention \(w_{t}\) DNN(\(_{t}\))
4: Behavior likelihood \(p(y|_{t}) w_{t}_{t}-(_{t})\)
5: Sample final immediate feedback \(_{t}^{(I)} p(y|_{t})\) The user leave module:
6: Immediate reward \(r_{t}\) reward_func(\(_{t}^{(I)}\))
7: User temper \(\) user temper - immediate reward
8: Leave signal \(_{t}^{(L)} 1\) if user temper \(\) T; 0 otherwise The user retention module:
9: Personal retention bias \(b_{u}\) DNN(\(_{t}\))
10: Response retention bias \(b_{r}_{1}r_{t}\)
11: Next day return probability \(p_{} b_{u}+b_{r}+_{2}b\), where \(b\) is the global retention bias
12: Return time \(_{t}^{(R)}(p_{})\) if \(_{t}^{(L)}=1\), otherwise \(_{t}^{(R)}=0\) Post processing module:
13: Update user history \(_{:t}_{:t-1}(_{t},_ {t}^{(I)},_{t}^{(L)},_{t}^{(R)})\)
14: If \(_{t}^{(L)}\) == 1, the user leave the current session
15: Else if not reaching the max session number, then continue.
16: Otherwise, sample a new user \(\), \(\) from data and replace the current user.
```

**Algorithm 1** Step -- the detail workflow of KuaiSim.

**User immediate response module (UIRM)** is responsible for generating the user's immediate feedback \(_{t}^{(I)}\). It first infers a ground-truth user state \(_{t}\) (assumed implicitly from RL models), then outputs the behavioral likelihood for each immediate feedback type. Specifically, \(\) represents concatenation and \(\) represents dot product. The item_correlation function is introduced to suppress the positive behaviors for items with higher correlation with other items in the same recommendationlist. This behavior simulates the user's demands for item diversity since lower item correlation induces a higher chance of positive interactions. In order to ensure the validity of the UIRM model, we require pretraining on the log data, and for each immediate feedback type, we can adapt the standard point-wise learning using binary cross-entropy. As a result, any datasets that provide logs of user-wise recommendation-feedback sequences would sufficiently support this module.

**User leave module** maintains a user temper/patience factor that directly determines the leave signal. We assume a maximum length of each user session, and initialize the user temper as equal to this maximum length. Then the user gradually loses the temper during the interactions and finally leaves the session when the temper is too low. In each step, the \(^{(I)}\) inferred by the UIRM is reused to calculate the immediate reward that represents the user's satisfaction with the \(_{t}\). We assume that less satisfactory recommendations make users lose their temper faster. The initial temper value, the decrease rate of temper, and the leave threshold are all adjustable hyperparameters.

**User retention module** is specifically designed for cross-session tasks, which predicts the user's return time. The user's return time typically follows a geometric distribution (as we will further discuss in section 3.3), so we only predict a next-day return probability \(p_{}\) to simulate this behavior. Specifically, \(p_{}\) combines the global retention bias, a personal retention bias, and a response retention bias. The personal retention bias reflects the differences in user's activity level, e.g., highly active users may use the system every day with next-day return probability, but lower-activity users may come back to the system after several weeks. The response retention bias assumes that better recommendations also increase the user's returning probability since users are more satisfactory to the system. We tune the global bias to fit the geometric distribution of the data and restrict the return day signal in \(^{(R)}_{t}\{1,,D\}\) after each session. Here, we set \(D=10\) in our simulation because the percentage of requests returned after the tenth day is almost zero.

### Dataset Analysis

In order to maintain distributional consistency between the simulator and the real-world environment, we build KuaiSim with the support of log data. The data will be used to pretrain the UIRM and sample the user's initial observation during online interactions as the post-processing module in algorithm 1. In this section, we further describe the example datasets used in building up the simulators.

**Dataset description** We construct our simulator KuaiSim on the KuaiRand  dataset, a large-scale unbiased sequential recommendation dataset collected from the Kuaishou4 App. It elicits user preferences on the randomly exposed items to collect unbiased data. You can find detailed data collection process in Appendix A.1. The availability of unbiased sequential data allows us to conduct unbiased offline evaluations, which greatly aids in constructing our simulator. We specifically choose the KuaiRand-Pure version. By doing so, we maintain a focused dataset for constructing our simulator. The KuaiRand dataset encompasses 12 distinct feedback signals. We utilize six positive feedback signals (i.e., 'click', 'view time', 'like', 'comment', 'follow', and 'forward'). Additionally, we incorporate two negative feedback signals (i.e., 'hate' and 'leave'). The other feedback signals occur infrequently and are therefore excluded from our analysis. Furthermore, we broaden the scope of our simulator by incorporating the ML-1m5 dataset. This dataset serves as a benchmark commonly used in RSs and includes ratings provided by 6,014 users for 3,417 movies. In our analysis of the ML-1m dataset, we consider movies with user ratings higher than 3 as positive samples, representing a 'like,' while the rest are deemed negative samples, representing a 'hate.'

**Dataset process** We standardize both datasets by preprocessing them into a cohesive format, ensuring that each record follows a sequential order of (user features, user history, exposed items, user feedback, and timestamp). The detailed statistics of the resulting dataset can be found in Table 2. To guarantee the data quality of the KuaiRand dataset, we apply 50-core filtering where videos with less than 50 occurrences are removed. To generate session data, we segment each day chronologically, treating each day as an individual session.

   Datasets & Users & Items & Interactions & Sessions & Density \\  KuaiRand-Pure & 27077 & 7551 & 1,436,609 & 246738 & 0.70\% \\ ML-1m & 6,400 & 3,706 & 1,000,208 & 16629 & 4.22\% \\   

Table 2: Statistics of datasets.

**Primary data analysis** We present our primary data analysis on the KuaiRand-Pure dataset in Figure 2. A full analysis procedure and more detailed results can be found in Appendix A.2. In our observation: 1) Figure 2(a) displays the distribution of user feedback, showcasing the frequency of occurrences for the six positive and one negative ('hate') 0/1 feedback signals. Notably, the negative feedback signal, 'is hate,' appears the least frequently. Among the positive feedback signals, 'is click' exhibits the highest frequency, while 'is forward' has the lowest occurrence. 2) In Figure 2(b) and 2(c), we present the distribution of same day and cross day request gaps, respectively. These gaps represent the time intervals, in hours, between consecutive requests. The occurrences of different request time gaps follow a geometric distribution pattern. 3) By analyzing the frequency of session returns and plotting the return time distribution in Figure 2(d), we observe that the return time also adheres to a geometric distribution, i.e., given \(p_{}\), the \(d\)-th day return probability is \((1-p_{})^{d-1}p_{}\). And note that the percentage of return time greater than 10 is nearly negligible.

## 4 Baselines and Evaluation Protocol

### Baseline Methods

**List-wise recommendation with request level simulator**: For list-wise recommendation, we first consider a standard collaborative filtering baseline **CF**. It adopts a point-wise approach, where the user-item scoring function is trained through binary cross entropy. We then include a generative approach **ListCVAE**, which captures the distribution of recommended lists using a conditional VAE. We also include a reranking approach **PRM**, which is built upon a CF-based initial ranker and utilizes a transformer-based re-ranker to encode the intermediate candidate set.

**Sequential recommendation with whole-session simulator**: With the standard MDP formulation, we include the following RL methods as comparisons: **A2C** is a synchronized version of A3C  that applies the policy gradient algorithm on the actor and TD error learning on the critic. **SA2C** combines a negative sampling strategy with supervised sequential learning to enhance reward signal. **DDPG** is a deep deterministic policy gradient algorithm and the continuous action space is used to represent the recommendation list, for both the actors and critics. **TD3** is an extension of DDPG that improves the stability of training by incorporating the clipped double Q-learning. **HAC** is also an extension of DDPG that decomposes the generation of item lists into a hyper-action(embedding) inference step and an effect-action(item list) selection step.

**Retention optimization with cross-session simulator**: The retention optimization task is essentially a hyper-parameter search problem. We first consider **CEM** as a black-box optimization method. It iteratively samples and updates a population of candidate hyper-parameters based on their performance. We then include **TD3** method similar to that in whole-session level, but use return time as the reward. We also include **RLUR** as the state-of-the-art method. It uses intrinsic rewards  to enhance policy learning and introduces a novel soft regularization method to address the trade-off between sample efficiency and stability.

   Algorithm & Average L-reward & Max L-reward & Coverage & ILD \\  CF & **2.253 \(\) 0.024** & 4.039 \(\) 0.001 & 100.969 \(\) 7.193 & 0.543 \(\) 0.007 \\ ListCVAE & 2.075 \(\) 0.039 & **4.042 \(\) 0.001** & **446.100 \(\) 15.648** & **0.565 \(\) 0.004** \\ PRM & 2.174 \(\) 0.017 & 3.811 \(\) 0.003 & 27.520 \(\) 3.210 & 0.538 \(\) 0.004 \\   

Table 3: Benchmarks for the request level task of KuaiSim. Best values are in bold.

Figure 2: Data analysis on KuaiRand dataset. (a) User response distribution. (b) Same day request gap distribution. (c) Cross day request time gap distribution. (d) Return time distribution

### Evaluation Protocol

**List-wise recommendation with request level simulator**: **List-wise reward (L-reward)** is the average of item-wise immediate reward. We use both the average L-reward and the max L-reward across user requests in a mini-batch. **Coverage** describes the number of distinct items exposed in a mini-batch. **Intra-list diversity (ILD)** estimates the dissimilarity between items in each recommended list, based on item embedding.

**Sequential recommendation with whole-session simulator**: Besides coverage and ILD, we also use other metrics. **Whole-session reward**: total reward is the average sum of immediate rewards for each session. The average reward is the average of total reward for each request. **Depth** represents how many interactions before the user leaves.

**Retention optimization with cross-session simulator**: **Return day** is the average time gap (day) between the last request of session and the first request of session. **User retention** is the average ratio of visiting the system again.

## 5 Benchmark Results and Analysis

We provide benchmark results for all three tasks in this section. To ensure the reproducibility and reliability of results, we computer averages across five distinct sets of results. The \(\) represents the standard deviation of five results. Detailed experiment implementation and hyper-parameter settings are provided in Appendix B.

**List-wise recommendation with request level simulator**: As shown in Table 3, among the evaluated models, ListCVAE demonstrates the best performance in terms of maximum reward and diversity. Its ability to generate diverse and high-reward recommendations makes it an effective choice for list-wise recommendation tasks. On the other hand, PRM exhibits the worst performance among the evaluated models. A significant challenge in this task lies in efficiently searching the extensive combinatorial space of list actions. A promising avenue for future research involves addressing this challenge by simultaneously improving the diversity of recommendation results and reducing the complexity of the combination space.

**Sequential recommendation with whole-session simulator**: As shown in Table 4, the HAC framework consistently demonstrates superior performance across long-term metrics, showcasing the effectiveness of its hyper-actions and the efficiency of its learning method. Notably, HAC outperforms other frameworks, indicating its high level of expressiveness and efficacy in recommendation tasks. On the other hand, A2C exhibits the poorest performance and appears to be the most unstable learning framework among the evaluated methods. SA2C outperforms A2C by capitalizing on an improved reward signal generated through negative sampling and supervised training. While slightly trailing behind HAC, the DDPG framework also delivers commendable results. Current methods in this task often overlook the importance of incorporating long-term feedback. Therefore, a promising research direction lies in exploring how to model complex inter-session relationships effectively.

   Algorithm & Depth & Average reward & Total reward & Coverage & ILD \\  TD3 & 14.63 \(\) 0.03 & 0.6476 \(\) 0.0028 & 9.4326 \(\) 0.0756 & 24.20 \(\) 2.55 & 0.9864 \(\) 0.0004 \\ A2C & 14.02 \(\) 0.02 & 0.5950 \(\) 0.0019 & 8.3905 \(\) 0.1026 & 27.41 \(\) 1.08 & 0.9870 \(\) 0.0002 \\ SA2C & 14.34 \(\) 0.02 & 0.6251 \(\) 0.0014 & 8.9547 \(\) 0.0241 & 27.14 \(\) 2.01 & 0.9872 \(\) 0.0002 \\ DDPG & 14.89 \(\) 0.04 & 0.6841 \(\) 0.0013 & 10.0873 \(\) 0.0571 & 20.95 \(\) 3.27 & 0.9850 \(\) 0.0006 \\ HAC & **14.98 \(\) 0.03** & **0.6895 \(\) 0.0017** & **10.1742 \(\) 0.0634** & **35.70 \(\) 1.22** & **0.9874 \(\) 0.0004** \\   

Table 4: Benchmarks for the whole-session task of KuaiSim. Best values are in bold.

   Algorithm & Return day \(\) & User retention \(\) \\  CEM & 3.573 \(\) 0.012 & 0.572 \(\) 0.002 \\ TD3 & 3.556 \(\) 0.010 & 0.581 \(\) 0.001 \\ RLUR & **3.481 \(\) 0.010** & **0.607 \(\) 0.002** \\    \(\): the higher the better; \(\): the lower the better.

Table 5: Benchmarks for the cross-session task of KuaiSim. Best values are in bold.

**Retention optimization with cross-session simulator**: As shown in Table 5, TD3 demonstrates better performance than CEM in terms of both metrics, showcasing the effectiveness of reinforcement learning techniques. However, RLUR surpasses both TD3 and CEM significantly, indicating its superior performance in the evaluated tasks. Indeed, the exploration of this task is still in its early stages. While some research has attempted to model user retention on a day-by-day basis, capturing more enduring user feedback has considerable potential in exploring approaches.

## 6 Discussion

**Comparing with other simulators** In section 2.4, we compare our KuaiSim with other simulators qualitatively in terms of datasets and tasks, showing that our KuaiSim is the only one that meets all requirements. In this section, we compare our KuaiSim with other simulators quantitatively. We reconstruct some competitive simulators on the KuaiRand dataset for the whole-session task to illustrate the effectiveness of our KuaiSim. In order to evaluate the simulator, we divide it into two planes of evaluation. On the one hand, how accurately the simulator fits the environment. Since some simulators have only one feedback signal, click, we compare the AUC predicted for this signal. On the other hand, the agent is trained using the simulator. We utilize three metrics proposed in the whole-session evaluation protocol, depth, average reward, and total reward. As shown in Table 6, we utilize the DDPG algorithm to train an agent with different simulators. Among these simulators, KuaiSim outperforms the others by a significant margin across all evaluation metrics. This outcome demonstrates that KuaiSim accurately aligns with the environment, resulting in superior agent training. Compared to rule-based simulators like RecSim and RecoGym, our simulator can be trained in a supervised manner to fit the real environment better. Additionally, compared to Virtual-Taobao and RL4RS, KuaiSim directly samples users from datasets without the need for fitting user states, avoiding potential errors. These distinctions highlight the strengths of our simulator, KuaiSim, which contributes to its enhanced accuracy and reliability in simulating user behavior and preferences.

**Data migration analysis** To showcase the data migration capabilities of our construction process, we implement KuaiSim on the ML-1m dataset and evaluate its performance on the whole session task. The benchmark results, as presented in Table 7, indicate that HAC continues to outperform other methods in all metrics, except for coverage. Notably, DDPG exhibits the highest coverage and achieves the best diversity in the recommended results. On the other hand, A2C outperforms TD3, positioning itself as a stronger-performing model compared to the latter. TD3 exhibits the poorest performance among the evaluated models. In a nutshell, these findings show that our KuaiSim can work well on ML-1m and emphasize the effectiveness of KuaiSim in adapting to different datasets.

**Parameter analysis** We further investigate the effect of some vital hyper-parameters for training the Algorithm TD3 with our KuaiSim. We examine the influence of max time step in the range of [5; 10; 15; 20; 25; 30], the results are presented in Figure 3 (a). Optimal model performance is achieved when the max time step is set to 20, and the return day fluctuates between 2.2 and 2.3 for other settings. Additionally, we delve into the effect of slate size, varying it across [5; 10; 15; 20;

   Simulators & Depth & Average reward & Total reward & AUC \\  RL4RS & 14.39 \(\) 0.02 & 0.640 \(\) 0.015 & 9.235 \(\) 0.122 & 0.6929 \(\) 0.0019 \\ Recogym & 13.55 \(\) 0.01 & 0.535 \(\) 0.013 & 7.194 \(\) 0.109 & 0.6729 \(\) 0.0026 \\ RecSim & 14.05 \(\) 0.02 & 0.588 \(\) 0.006 & 9.347 \(\) 0.143 & 0.6842 \(\) 0.0031 \\ VirtualTaobao & 14.45 \(\) 0.02 & 0.646 \(\) 0.009 & 9.570 \(\) 0.077 & 0.6866 \(\) 0.0014 \\ KuaiSim & **14.86*** \(\) 0.01** & **0.679******* \(\) **0.011** & **10.081******* \(\) **0.116** & **0.7234*** \(\) **0.0021** \\   

* indicates the statistically significant improvements (i.e., two-sided t-test with \(p<0.05\)) over the best baseline.

Table 6: A comparison between KuaiSim and other simulators. Best values are in bold.

   Algorithm & Depth & Average Reward & Total reward & Coverage & ILD \\  TD3 & 13.50 \(\) 0.01 & 0.5388 \(\) 0.007 & 7.4035 \(\) 0.0152 & 44.18 \(\) 2.19 & 0.9866 \(\) 0.0001 \\ A2C & 13.55 \(\) 0.01 & 0.5468 \(\) 0.002 & 7.4487 \(\) 0.0171 & 27.31 \(\) 1.4425, 30]. the results are shown in Figure 3 (b). The best model results are obtained when the slate size is set to 20. The performance exhibits a decline as the slate size deviated from this optimum point, either increasing or decreasing. This is because when the slate size is excessively small, the recommendation list fails to encompass items that align with the user's preferences. Conversely, when the slate size is overly large, the recommendation list contains too much noise that is not of interest to the user. This overload of information not only diverts the user's focus but also tests their patience. The model's outcomes demonstrate a consistent and stable pattern across different parameter settings, underscoring the robust stability of our KuaiSim training agent.

**Limitations and potential direction for solutions** There remain certain areas within our RecSim framework that warrant refinement. **Uncovered recommendation problems:** While we propose three levels of recommendation tasks to encompass a majority of recommendation challenges, numerous others exist, including explainable recommender systems , diversity recommendations , and fairness recommendations . Incorporating the interpretability, diversity, fairness, and other indicators of the recommendation results into the modeling of the simulator is a promising strategy to broaden our simulator's scope. **Expanding to diverse domains:** While both the KuaiRand and ML-1m datasets center around video recommendations, it's worth noting that the KuaiSim framework can be extended to encompass a variety of domains such as music, news, and e-commerce. The iterative refinement process we've undertaken for constructing and evaluating the simulator greatly facilitates such expansions.

**Ethical considerations and potential societal impact** We categorize our ethical considerations and potential societal impact into three key aspects. **Safeguarding user privacy:** While our approach involves training the simulator using real user logs, it's important to note that the datasets we employed have been stripped of any privacy-sensitive information. The user characteristics utilized in our study are comprehensively presented in Appendix A.1 Table 2. These characteristics reflect the user's interactions with the application, and each user is denoted by an anonymous aid. Thus, it's essential to emphasize that we do not leverage any private user information, effectively mitigating concerns related to privacy breaches. **Unbiased KuaiRand dataset:** Because online A/B test usually consumes much time and money, which makes it impractical for academic researchers to conduct the evaluation online. However, offline evaluation will cause bias because of massive missing data, i.e., the user-item pairs that have not occurred in the test set. One fundamental approach to address this issue in offline evaluation is to collect unbiased data, i.e., to elicit user preferences on the randomly exposed items. This unbiased dataset empowers us to conduct unbiased offline evaluations without compromising user privacy. The detailed data collection process can be found in Appendix A.1. **Promoting field advancement:** We provide various competitive algorithms for three task levels as benchmarks, coupled with a meticulously enhanced process for constructing and evaluating simulators. This framework not only facilitates the advancement of the field but also presents a promising avenue for research by extending these simulators to encompass a broader spectrum of tasks and fields.

## 7 Conclusion

In summary, KuaiSim stands as a comprehensive and sophisticated simulator that encompasses multiple task levels, establishing benchmarks and enabling thorough evaluations in the realm of recommendation systems. Through its refined construction and evaluation process, as well as its effectiveness in replicating user behaviors, KuaiSim contributes to the development and advancement of recommendation system techniques and methodologies.

Figure 3: Parameter sensitive analysis on KuaiRand dataset. (a) Max time step. (b) Slate size.