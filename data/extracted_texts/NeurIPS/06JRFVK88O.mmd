# Mimicking To Dominate: Imitation Learning Strategies for Success in Multiagent Games

The Viet Bui

Singapore Management University, Singapore

theviet.bui.2023@phdcs.smu.edu.sg

Tien Mai

Singapore Management University, Singapore

atmai@smu.edu.sg

Thanh Hong Nguyen

University of Oregon Eugene, Oregon, United States

thanhhng@cs.uoregon.edu

###### Abstract

Training agents in multi-agent games presents significant challenges due to their intricate nature. These challenges are exacerbated by dynamics influenced not only by the environment but also by strategies of opponents. Existing methods often struggle with slow convergence and instability. To address these challenges, we harness the potential of imitation learning (IL) to comprehend and anticipate actions of the opponents, aiming to mitigate uncertainties with respect to the game dynamics. Our key contributions include: (i) a new multi-agent IL model for predicting next moves of the opponents -- our model works with hidden actions of opponents and local observations; (ii) a new multi-agent reinforcement learning (MARL) algorithm that combines our IL model and policy training into one single training process; and (iii) extensive experiments in three challenging game environments, including an advanced version of the Star-Craft multi-agent challenge (i.e., SMACv2). Experimental results show that our approach achieves superior performance compared to state-of-the-art MARL algorithms.

## 1 Introduction

Recent works in MARL have made a significant progress in developing new effective algorithms that can perform well in complex multi-agent environments including SMAC . Among these works, centralized training and decentralized execution (CTDE)  has attracted a great attention from the RL community due to its advantage of leveraging global information to train a centralized critic (i.e., actor-critic methods ) or a joint Q-function (i.e., value-decomposition methods ). This approach enables a more efficient and stable learning process while allowing agents to act in a decentralized manner. Under this CTDE framework, off-policy methods such as MADDPG  and QMIX  have become very popular due to their data efficiency and state-of-the-art (SOTA) results on a wide range of benchmarks. On the other hand, on-policy gradient methods have been under-explored in MARL due to their data consuming and difficulty in transferring knowledge from single-agent to multi-agent settings. However, a recent work shows that on-policy methods (such as MAPPO, a multi-agent version of proximal policy optimization) outperforms all other SOTA methods including MADDPG and QMIX in various multi-agent benchmarks, and especially workswell in complex SMAC settings . Motivated by this promising result, we focus on improving the performance of policy gradient methods in MARL.

We consider a partially observable MDP environment in which there are agents attempting to form an alliance to play against a team of opponents, where allied agents have to make decision independently without communicating with other members. We aim to enhance the performance of PPO in MARL with the introduction of a novel opponent-imitation component. This new component is then integrated into the MAPPO framework to enhance the policy learning of allied agents. A key challenge in our problem setting is that allied agents are unaware of actions taken by their opponents. In addition, each allied agent only has local observations of opponents locating in the current neighborhood of the agent -- the locations and neighborhoods of all players are changing over time depending on actions taken by players and the dynamics of the environment. Lastly, learning to imitate opponents occurs during the policy learning process of the allied agents. The inter-dependency between these two learning components makes the entire learning process significantly challenging.

We address these challenges while providing the following key contributions. _First_, we convert the problem of imitating the opponent policy into predicting their next states. The outcome of this next state prediction is an indirect implication of the opponent policy. We then cast the problem of opponent next-state prediction as a new multi-agent imitation learning (IL) problem. We propose a new multi-agent IL algorithm, which is an adaptation of IQ-Learn  (a SOTA IL algorithm), that only considers local opponent-state-only observations. Especially, instead of imitating the opponents' policy, our IL algorithm targets the prediction of next states of the neighboring opponents. _Second_, we provide a comprehensive theoretical analysis which provides bounds on the impact of the changing policy of the allied agents (as a result of the policy learning process) on our IL outcomes.

_Third_, we present a unified MARL algorithmic framework in which we incorporate our IL component into MAPPO. Our idea is to combine each allied agent's local observations with the next-state prediction of neighboring opponents of that agent, creating an augmented input based on which to improve the decision making of the allied agent at every state. This novel integration results in a new MARL algorithm, which we name _Imitation-enhanced **M**ulti-**A**gent **EX**tended PPO**_ (IMAX-PPO).

_Finally_, we conduct extensive experiments in several benchmarks ranging from complex to simple ones, including: SMACv2 (an advanced version of the Star-Craft multi-agent challenge) , Google research football (GRF) , and Gold Miner . Our empirical results show that our new algorithm consistently outperforms SOTA algorithms significantly accross all these benchmarks.

## 2 Related Work

**MARL.** The literature on MARL includes both centralized and decentralized algorithms. While centralized algorithms  learn a single joint policy to produce joint actions of all the agents, decentralized learning  optimizes each agent's local policy independently. There are also algorithms based on _centralized training and decentralized execution_ (CTDE). For example, methods in [18; 5] adopt actor-critic structures and learn a centralized critic that takes global information as input. Value-decomposition (VD) is a class of methods that represent the joint Q-function as a function of agents' local Q-functions [29; 23]. Alternatively, the use of policy-gradient methods, such as PPO , has also been investigated in multi-agent RL. For example,  propose independent PPO (IPPO), a decentralized MARL, that can achieve high success rates in several hard SMAC maps. IPPO is, however, overall worse than QMIX , a method based on factorizing Q function to facilitate CTDE. Later methods based on factorized Q-learning include QTRAN  and QPLEX , where QPLEX has been shown to achive better performance than QMIX and QTRAN. Recently,  develop MAPPO, a PPO-based MARL algorithm that outperforms QMIX and QPLEX on some popular multi-agent environments such as SMAC [25; 4] and GRF . To the best of our knowledge, MAPPO is currently a SOTA method for MARL. Our work integrates a new opponent imitation model into MAPPO, resulting in a new MARL algorithm that outperforms SOTA methods on various challenging game tasks.

**Imitation Learning (IL).** In this study, we employ IL to anticipate the opponents' moves. IL is known as a compelling approach for sequential decision-making [20; 1]. In IL, a collection of expert trajectories is provided, with the objective of learning a policy that emulates behavior similar to the expert's policy. One of the simplest IL methods is Behavioral Cloning (BC), which aims to maximize the likelihood of the expert's actions under the learned policy. BC disregards environmental dynamics, rendering it suitable only for uncomplicated environments. Several advanced IL techniques, encompassing environmental dynamics, have been proposed [24; 8; 12]. While these methods operate in complex and continuous domains, they involve adversarial learning, making them prone to instability and sensitivity to hyperparameters. The IQ-learn  stands as a cutting-edge IL algorithm with distinct advantages, specifically its incorporation of dynamics awareness and non-adversarial training. It's important to note that all the aforementioned IL methods were designed for single-agent RL. In contrast, the literature on multi-agent RL is limited, with only a handful of studies addressing IL in multi-agent RL. For instance,  presents an adversarial training-based algorithm, named Multi-agent Generative Adversarial IL. It's worth noting that all the IL algorithms mentioned above are established on the premise that expert (aka. opponent in our case) actions are either observable or can be accessed via sampling, which implies that no existing algorithm can be directly applied to our multi-agent game settings with _local state-only_ observations.

**Opponent Modeling.** Many existing works in MARL attempt to capture the learning process of opponents and incorporate it into the learning of the agent's policy [6; 14; 33]. For example, the LOLA algorithm  considers the impact of one agent's policy on the parameter update of other opponents while Meta-MAPG  combines LOLA with meta-learning, accounting for continuous adaptation. Another important line of research on opponent modelling follows hierarchical reasoning, considering each agent holds a belief about the other agents according to varying levels of reasoning ability [32; 31; 35; 19]. As an example, in , they introduce a probabilistic recursive reasoning framework in which variational Bayes methods are used to approximate the opponents' conditional policies. Depart from these two lines of research, there are many other works that attempt to learn a representation for the opponent's policy or to consider the prediction of opponents' actions as an an auxiliary task that can be trained simultaneously with the RL part [11; 13; 22; 10; 11]. For example,  use variational encoder to model the opponents' fixed policies while  apply behavioral cloning together with agent identification to learn a hybrid generative-discriminative representation for the opponents' policy. All the aforementioned related works require having access to opponent's observations and actions during training and/or execution. In our work, on the other hand, the opponent modeling can be only trained based on local observations of each agent in the allied team. These local observations contain limited information about current states of nearby opponents while opponents' actions are unobservable. As a result, existing methods on opponent modeling are not applicable in our multi-agent setting.

## 3 Multi-Agent POMDP Setting

We consider a multi-player Markov game in which there are multiple agents forming an alliance to play against some opponent agents. We present the Markov game as a tuple \(\{,_{},_{},^{ },^{},P,R\}\), where \(\) is the set of global states shared by all the agents, \(_{}\) and \(_{}\) are the set of ally and enemy agents, \(^{}=_{i_{}}_{i}^{}\) is the set of joint actions and \(^{}=_{j_{}}_{j}^{}\) is the set of joint actions of all the ally agents, \(P\) is the transition dynamics of the game environment, and \(R\) is a reward function that takes inputs as states and actions of all agents and returns the corresponding rewards. At each time step where the global state is \(S\), each ally agent \(i_{}\) makes an action \(a_{i}^{}\) according to a policy \(_{i}^{}(a_{i}^{}|o_{i}^{})\), where \(o_{i}^{}\) is the observation of ally agent \(i\) given state \(S\). The joint action of allied agents can be now defined as \(A^{}=\{a_{i}^{}\,|\,i_{}\}\), and the joint policy is defined accordingly:

\[^{}(A^{}|S)=_{i^{}}_{i}^{}(a _{i}^{}|o_{i}^{}).\]

The enemy agents, at the same time, make a joint action \(A^{}=\{a_{j}^{}\,|\,j_{}\}\) with the probability:

\[^{}(A^{}|S)=_{j^{}}_{j}^{ }(a_{j}^{}|o_{j}^{}).\]

After all agents make decisions, the global state transits to a new state \(S^{}\) with the probability \(P(S^{}|A^{},A^{},S)\). In our setting, the enemies' policies \(^{}\) are fixed and thus can be treated as a part of the environment dynamics, as follows:

\[P(S^{}|A^{},S)=_{A^{}}(A^{}|S)P(S ^{}|A^{},A^{},S)\]

Our goal is to find a policy that optimizes the allies' expected joint reward, formulated as follows:1

\[_{^{}}_{(A^{},S)^{}}R^{ }(S,A^{})\] (1)The game dynamics involve both the environment dynamics and the joint policy of enemies, making the training costly to converge. We aim to migrate uncertainties associated with these game dynamics by first predicting the opponent policy based on the allies' past observations and leveraging this prediction into guiding the policy training for the allies.

## 4 Opponent Policy Imitation

The key challenge in our problem is that actions taken by opponents are hidden from allied agents. Moreover, each allied agent has limited observations of other agents; they can only obtain information about nearby opponents. For example, in the SMAC environment, for each allied agent, besides information about the agent itself, the allied agent is also aware of the relative position and health point, etc. of the neighboring opponents.2 Therefore, instead of directly predicting opponents' next moves, we focus on anticipating next states of opponents -- this next-state prediction can be used as an implication of what actions have been taken by the neighboring opponents. Our key contributions include: (i) a novel representation of the opponent next-state prediction in the form of multi-agent IL; (ii) a new adaptation of IQ-Learn to solve our new IL problem; (iii) a comprehensive theoretical analysis on the influence of policy learning of allied agents on the next-state prediction outcomes; and (iv) a practical multi-agent IL algorithm which is tailored to local observations of allied agents.

Here, it is important to note that prior works on IL with state-only observations all assume that actions are not available in the expert demonstrations but can be accessed via sampling, which is not the case in our context. Alternatively, one could apply standard supervised learning for this opponent-next-state prediction task. However, a well-known drawback of this approach is that it disregards environment dynamics and often struggles with distribution shifts . As shown later in our experiments, our IL approach significantly outperforms this supervised-learning approach.

### Multi-Agent IL with Unobservable Actions

We now present our IL formulation and our adaptation of IQ-Learn for solving our new IL problem. For the sake of theoretical analysis, this section focuses on IL with _global_ state-only observations. We then introduce a new practical algorithm later which addresses local observations of allied agents.

Opponent Next-State Prediction as an IL.To formulate the problem as an IL that accounts for the action-unobservable issue, we introduce a new notion of the "expert" state in our IL problem as a pair \(W=(S,A^{}_{-})\) which comprises of the original state \(S\) and the joint action of the allies \(A^{}_{-}\) taken in the previous step that leads to state \(S\). The action space of the "expert" is equivalent to the original state space \(\). We then introduce a new notion of a reward function for the expert as \(R^{}(W,S^{})\). This action (\(S^{}\)) of the expert is basically a resulting state of joint actions of the allies \(A^{}\) and _hidden_ joint actions of the enemies \(A^{}\) taken at state \(S\). Altogether with the reward function \(R^{}(W,S^{})\), we introduce a new notion of joint policy for the expert, \(^{}(S^{}|W)\) (or \(^{}(S^{}|S,A^{}_{-})\)), which is essentially the probability of ending up at a global state \(S^{}\) from state \(S\). The dynamics in this IL setting becomes \(P(W^{}\,|W,S^{})=P((S^{},A)\,|\,(S,A^{}_{-}),S^{ })=^{}(A\,|\,S)\) (which is the allies' policy) where \(W^{}=(S^{},A)\) and \(A\) is the action taken by the allies at state \(S\).

Let \(=\{:^{ },\ _{S^{}}(S^{}|S,A^{}_{-})=1,\  S,S^{},A^{} ^{}\}\) be the support set of the imitating policy. We now introduce the maximum-entropy inverse RL framework  w.r.t the new notions of the expert's reward and policy \((R^{}(S^{}|W),^{}(S^{}|W))\):

\[_{R^{}}_{}L(,R^{})= _{^{,}}[R^{}(W,S^{})-_{ ^{,}}[R^{}(W,S^{})]+_{^{,}} [(S^{}|W)]}\] (2)

where \(^{,}\) is the occupancy measure of \((W,S^{})\) given by the expert policy \(^{}\) and the ally joint policy \(^{}\), and \(^{,}\) the occupancy measure of \((W,S^{})\) given by the imitation and ally policies. In particular, \(^{,}\) can be computed as follows:

\[^{,}(S^{},W)= (1-)^{}(S^{}\,|\,W){}_{t=0}^{ }\,^{t}P(W_{t}=W\,|\,^{},^{})\]

An Adaptation of IQ-Learn.Drawing inspiration from the SOTA IL algorithm, IQ-learn, we construct our IL algorithm which is an adaptation of IQ-Learn tailored to our multi-agent environment.

The main idea of IQ-learn is to convert a reward learning problem into a Q-function learning one. To apply IQ-Learn to our setting, we present the following new _soft and inverse soft_ Bellman operators, which is as adaption from the original ones introduced in :

\[^{,R^{}}_{Q^{}}(W,S^{}) =R^{}(W,S^{})+_{W^{}}[V^{ }_{}(W^{})]\] (3) \[^{}_{Q^{}}(W,S^{}) =Q^{}(W,S^{})-_{W^{}}[V^{ }_{}(W^{})]\] (4)

where the state value function is computed as follows:

\[V^{}_{}(W)\!=\!_{S^{}}\!Q^{} (W,S^{})-((S^{}|W))\]

First, it is clear that \(^{,R^{}}_{Q^{}}\) is contractive, thus defining a unique fixed point solution \(Q^{*}\) such that \(^{,R^{}}_{Q^{*}}=Q^{*}\). Let us further define the following function of \(\) and \(Q^{}\) for the Q-function learning:

\[J(,Q^{})=_{^{},}[^{}_{Q^{ }}(W,S^{})]-_{^{,}}[^{}_{ Q^{}}(W,S^{})]+_{^{,}}[(S^{}|W)]\]

We obtain a theoretical result on a connection between the learning reward and learning Q-functions:3

**Proposition 1**.: _For any reward function \(R^{}\), let \(Q^{*}\) be the unique fixed point solution to the soft Bellman equation \(^{,R^{}}_{Q^{*}}=Q^{*}\), then: \(L(,R^{})=J(,Q^{*})\), and for any \(Q^{}\), \(J(,Q^{})=L(,^{}_{Q^{}})\)._

Proposition (1) indicates that the IL problem in (2) is equivalently to the Q-value-based IL problem:

\[_{Q^{}}_{}J(,Q^{})\] (5)

Suppose \(Q^{*}\) is a solution to (5), then rewards can be recovered by taking \(R^{}(W,S^{})=Q^{*}(W,S^{})-_{W^{}}[ V^{}_{}(W^{})]\). Under this viewpoint, Prop. 2 shows that key properties of original IQ-learn still hold in our multi-agent setting with missing observations, making our IL algorithm convenient to use.

**Proposition 2**.: _The IL problem (5) is equivalent to the maximization \(_{Q^{}}J(^{Q},Q^{})\) where the imitation policy can be computed based on \(Q^{}\) as follows:_

\[^{Q}(S^{}|W)=(W,S^{}))}{_{S^{ }}(Q^{}(W,S^{}))}\]

_Moreover, the function \(J(^{Q},Q^{})\) is concave in \(Q^{}\)._

### Affect of Allies' Policies on Imitation Learning

The above IL algorithm is trained during the training of the allies' policies, which means the dynamics \(P(W^{}|W,S^{})=^{}(A^{}|S)\) change during the IL process. Therefore, we aim to analyze the impact of these changes on the imitation policy. According to Proposition 2, given \(Q^{}\), we can compute corresponding optimal imitation policy as \(^{Q}\). Therefore, the value function \(V^{}_{}(W)\) can be alternatively write as \(V^{}_{Q}(W)\). We now can denote the loss function of the imitation model (Eq. 8) as a function of the allies' joint policy explicitly: \((^{}|Q^{}) J(,Q^{})\).

We first present our results about bounds on the impact of the allies' changing policies on the IL loss function (Prop. 3). Based on these results, we then provide a bound on the IL learning outcome accordingly (Corollary 4). Let us denote by \(\!=\!_{(W,S^{})}Q^{}(W,S^{})\) an upper bound of \(Q^{}\).

**Proposition 3**.: _Given two allies' joint policies \(^{}\) and \(^{}\) such that \((^{}(|S)||^{}(|S))\) for any state \(S\), the following inequality holds:_

\[|(^{}|Q^{})-(^{}|Q^{ })|(+||) \]

_where \(=}+}{1-}+(3-)\), and \(=}{(1-)}+3-\)._

Let \((^{}|Q^{},)\) be the objective of IL \(J(,Q^{})\), written as a function of the allies' joint policy \(^{}\). The following proposition establishes a bound for the variation of \(|(^{}|Q^{},)-(^{}|Q^{},)|\) as function of \((^{}|^{})\), for any pair of allies' joint policies \((^{},^{})\).

Prop. 3 allows us to establish an upper bound for the IL when the allies' joint policy changes.

**Corollary 4**.: _Given two allies' policies \(^{}\) and \(^{}\) with \((^{}(|S)||^{}(|S)\), \( S\!\!\), then:_

\[_{Q^{}}\!(^{}|Q^{})}- _{Q^{}}\!(^{}|Q^{})} ()\]

Since the allies' joint policy \(^{}\) will be changing during our policy learning process, the above result implies that the imitating policy will be stable if \(^{}\) becomes stable, and if \(^{}\) is converging to a target policy \(^{*}\), then the imitator's policy also converges to the one that is trained with the target ally policy with a rate of \((^{}||^{})}\). That is, if the actual policy is within a \(()\) neighborhood of the target policy (i.e., \((^{}||^{})\)) then the expected return of the imitating policy is within a \(()\) neighborhood of the desired "_expected return_" given by the target policy.

## 5 IMAX-PPO: Imitation-enhanced Multi-Agent EXtended PPO Algorithm

We present our MARL algorithm for the competive game setting. We first focus on a practical implementation of an IL algorithm taking into account local observations. We then show how to integrate this into our MARL algorithm. We call our algorithm as IMAX-PPO, standing for _Imitation-enhanced **M**ulti-**A**gent EXtended PPO_ algorithm.

### Imitation Learning with Local Observations

In previous section, we present our new IL algorithm (which is an adaptation of IQ-Learn) to learn an expert policy \(^{}(S^{}|W)=^{}(S^{}|S,A ^{})\) that behaves similarly to the probabilities of ending up at state \(S^{}\) when the current global state is \(S\) and the allies' joint action is \(A^{}\). From the allies' perspective, to run this IL algorithm, it requires the allies to have access to the global state \(S\). However, each ally agent \(i\) can only observe local states of its neighboring enemies (such as their locations, speeds, etc.). Therefore, we adapt our IL algorithm in accordance with such local information. The goal is to predict next states of enemies in the neighborhood of each allied agent \(i\), denoted by \(^{}(S^{,next}}_{i}|w^{}_{i})\), where local information \(w^{}_{i}=(o^{}_{i},a^{}_{i,-})\) with \(o^{}_{i}\) is an observation vector of agent \(i\), containing the local states of the agent \(i\) itself and of all the agents in the neighborhood that are observable by agent \(i\). In particular, given that \(s^{}_{i_{k}}\) is the local state of an ally agent \(i_{k}\) and \(s^{}_{j_{k}}\) is of an enemy agent \(j_{k}\) and \(N(i)\) is the neighborhood of \(i\), we have:

\[o^{}_{i}=\{s^{}_{i}\}\{s^{}_{i_{k}} i_{k}\!\!N (i)^{}\}\{s^{}_{j_{k}} j_{k}\!\!N(i )^{}\}\]

To apply our IL algorithm to this local observation setting, we build a common policy network \(^{}_{_{}}\) and Q network \(^{}_{_{Q}}\) for all the agents where \(_{}\) and \(_{Q}\) are the network parameters. The IL objective

Figure 1: An overview of our IMAX-PPO algorithm. Each local observation \(o^{}_{i}\) of an ally agent \(i\) includes information about itself, as well as enemy and ally agents in its neighborhood (which changes over time). The output of the IL component is the predicted next states of neighboring enemy agents (predictions for the non-neighbor enemies will be masked out).

function can be reformulated according to local observations of the alleles as follows:

\[J(^{}_{_{}},^{}_{ _{Q}})=_{i^{}}_{(S^{}_{i},w^{}_{i})^{,}}[^{}(S^{ }_{i},w^{}_{i}).\] (6) \[.-_{w^{,}_{i}}[V^{}_{}(w^{,}_{i})]]-(1-)_{w^{ }_{i} P^{0},^{}}V^{}_{}(w^{}_{i0})\]

where \(w^{}_{i}=(o^{}_{i},a^{}_{i,-})\), \(w^{,}_{i}=(o^{,}_{i},a^{}_{i})\) (\(a^{}_{i}\) is the action taken by agent \(i\) at observation \(o^{}_{i}\), resulting in next observation \(o^{,}_{i}\)), \(S^{}_{i}=\{s^{}_{j_{k}}:j_{k} N(i) ^{}\}\) is the next states of enemies in the current neighborhood of the agent \(i\). In addition, the value functions are re-formulated as follows:

\[V^{}_{}(w^{}_{i})=_{(S^{}_{i})^{}_{_{}}}[^{}_{ _{Q}}(S^{}_{i},w^{}_{i})-(^{}_{_{}}(S^{}_{i}|w^{}_{i}))]\]

In the end, we can update \(^{}_{_{}}\) and \(^{}_{_{Q}}\) by the following actor-critic rule: for a fixed \(^{}_{_{Q}}\), we update \(_{Q}\) to maximize \(J(^{}_{_{}},^{}_{_{Q}})\), and for a fixed \(^{}_{_{}}\), we apply soft actor-critic (SAC) to update \(_{}\).

### IMAX-PPO Algorithm

We now combine the local observation \(o^{}_{i}\) of each allied agent \(i\) with the next-state prediction \(S^{}_{i}\) of its neighboring enemies (obtained by our IL algorithm) to create an augmented input. This augmented input is used to improve the policy learning of the allied agent \(i\). That is, we aim to optimize the allies' policy \(^{}_{}(a^{}_{i}|o^{}_{i},S^{}_{i}),i ^{}\) that optimizes the long-term expected joint reward:

\[_{^{}_{}}_{(a^{}_{i},o^{}_{ i},S^{}_{i})^{}_{}}[_{i ^{}}R^{}_{i}(o^{}_{i},a^{}_{i})]\]

where \(o^{}_{i}\) is an observation vector of agent \(i\), \(S^{}_{i}\) is the information derived from the imitator for agent \(i\), \(a^{}_{i}^{}_{i}\) is a local action of agent \(i\). To facilitate the training and integration of the imitation learning policy into the MARL algorithm, for every ally agent \(i\), we gather game trajectories following the structure \((o_{i},a^{}_{i},S^{}_{i})\). These gathered observations are then stored in a replay buffer to train the imitation policy \(^{}_{_{}}(S^{}_{i}|o^{}_{i },a^{}_{i})\).

In the IMAX-PPO framework, at each game state \(S\), considering the current actor policy \(^{}\) and the imitating policy \(^{}\), for each agent \(i^{}\), we draw a sample for the allied agents' joint action \(^{}^{}\). Corresponding local observation \(o^{}_{i}\) and action \(^{}_{i}\) of each agent \(i\) are then fed as inputs into the imitation policy to predict the subsequent state \(S^{}_{i}^{}(|o^{}_{i}, ^{}_{i})\). Once the predicted local states \(\{S^{}_{i},\;i^{}\}\) are available, it is used as input to the actor policy \(^{}_{}\) in order to generate new actions for the allied agents. In simpler terms, we select a next local action \(a^{}_{i}^{}_{}(|o^{}_{i},S^{ }_{i})\). Beside the allies' policy network, we also use a centralized value network \(V^{}_{_{i}}(S)\) and update it together with the policy network in an actor-critic manner, similarly to MAPPO. The actor-network is trained by optimizing the following objective:

\[L^{}()=_{i^{}}_{o^{ }_{i},a^{}_{i},S^{}_{i}}[\,\{r_{i}() ,(r_{i}(),1-,1+)\}]\] (7)

where \(r_{i}()=_{}(a^{}_{i}|o^{}_{i},S^{ }_{i})}{^{}_{_{}}(a^{}_{i}|o^{} _{i},S^{}_{i})}\) and \(\) is the advantage function, calculated by Generalized Advantage Estimation (GAE). The critic network is trained by optimizing

\[^{}(_{v})=_{S}\{[V^{}_{_{ v}}(S)-(S)]^{2},[V^{}_{_{v},_{v,old}}(S)-(S)]^{2} \}\]

where \((S)=+V^{}_{_{v},old}(S)\) and \(V^{}_{_{v},_{v,old}}(S)=(V^{}_{_{ v}}(S),V^{}_{_{v,old}}(S)-,V^{}_{_{v,old}}(S)+)\). We provide the key stages in Algorithm 1. Additionally, Fig. 1 serves as an illustration of our IMAX-PPO.

## 6 Experiments

We evaluate the performance of our **IMAX-PPO** algorithm (Algo. 1) in comparison with some standard and SOTA multi-agent RL algorithms: **IPPO**, **MAPPO**, **QMIX** and **QPLEX**. In addition, to examine the impact of our multi-agent IL model on the performance of **IMAX-PPO**, we include two versions of **IMAX-PPO** where opponent's next states are predicted by (i) an adaption of the GAILalgorithm , denoted as **IMAX-PPO (GAIL)** and (ii) the IQ-learn adaption (i.e. Algorithm 1, denoted as **IMAX-PPO (InQ)**. Moreover, to compare out IL-based approach with supervise learning, we include an approach based on MAPPO where the opponent's next states are learned and predicted by _standard supervising learning_. We denote this approach as **Sup-MAPPO**.

The details of **Sup-MAPPO** and **IMAX-PPO (GAIL)** are provided in the appendix. We run extensive experiments in three multi-agent competitive environments: SMACv2, Google Research Football (GRF), and Miner. Each reported value is computed based on \(32\) different rounds of game playing (each corresponds to a different random seed).

SMACv2.SMACv2  is an advanced variant of SMAC, driven by the aim to present a more challenging setting for the assessment of cooperative MARL algorithms. In SMACv2, scenarios are procedurally generated, which require agents to generalize to previously unseen settings (from the same distribution) during evaluation. This benchmark consists of \(15\) sub-tasks where the number of agents varies from \(5\) to \(20\). The agents can play with opponents of different difficulty levels. In

    &  &  &  &  &  &  &  \\  & & & & & & & **MAPPO** & **GAIL** & **InQ** \\   & 5\_vs\_5 & 58.0 & 54.6 & 70.2 & 53.3 & 71.8 & 68.1 & **78.7** \\  & 10\_vs\_10 & 58.3 & 58.0 & 69.0 & 53.7 & 67.3 & 59.6 & **79.8** \\  & 10\_vs\_11 & 18.2 & 20.3 & 42.5 & 22.8 & 36.7 & 21.3 & **48.7** \\  & 20\_vs\_20 & 38.1 & 44.5 & 69.7 & 27.2 & 71.1 & 76.3 & **80.6** \\  & 20\_vs\_23 & 5.1 & 4.1 & 16.5 & 4.8 & 21.9 & 11.8 & **24.2** \\   & 5\_vs\_5 & 52.0 & 56.2 & 58.4 & **70.0** & 55.8 & 53.3 & 69.9 \\  & 10\_vs\_10 & 58.1 & 57.3 & 65.8 & 66.1 & 54.1 & 58.4 & **72.2** \\  & 10\_vs\_11 & 28.6 & 31.0 & 39.4 & 41.4 & 26.9 & 28.4 & **53.9** \\  & 20\_vs\_20 & 52.8 & 49.6 & 57.6 & 23.9 & 38.6 & 35.9 & **65.4** \\  & 20\_vs\_23 & 11.2 & 10.0 & 10.0 & 7.0 & 11.2 & 4.7 & **17.7** \\   & 5\_vs\_5 & 41.0 & 37.2 & 37.2 & 47.8 & 52.5 & 48.6 & **55.0** \\  & 10\_vs\_10 & 39.1 & 49.4 & 40.8 & 41.6 & 57.4 & 50.6 & **57.6** \\  & 10\_vs\_11 & 31.2 & 26.0 & 28.0 & 31.1 & 38.1 & 34.8 & **41.5** \\  & 20\_vs\_20 & 31.9 & 31.2 & 30.4 & 15.8 & **44.3** & 26.7 & 43.3 \\  & 20\_vs\_23 & 15.8 & 8.3 & 10.1 & 6.7 & 13.6 & 8.2 & **21.3** \\   & easy & 48.9 & 49.3 & 57.2 & 59.8 & 47.1 & 54.5 & **61.8** \\  & medium & 40.6 & 39.5 & 47.3 & 50.4 & 39.4 & 39.3 & **55.0** \\  & hard & 31.2 & 31.2 & 41.7 & 43.5 & 31.3 & 29.7 & **49.8** \\   & 3\_vs\_1 & 88.0 & 82.7 & 8.1 & 90.2 & 96.1 & 96.4 & **98.1** \\  & easy & 87.8 & 84.1 & 16.0 & 94.9 & 89.7 & 64.1 & **95.0** \\   & hard & 77.4 & 70.9 & 3.2 & 95.1 & 10.7 & 15.2 & **97.3** \\   

Table 1: Win-rates (percentage).

comparison to SMACv1 , SMACv2 stands apart by permitting randomized team compositions, varied starting positions, and an emphasis on augmenting diversity.

Figure 2 shows the performance of the five algorithms during the training process across \(15\) sub-tasks. The x-axis is the number of training steps and the y-axis is the winning rates averaged over \(32\) rounds of evaluations. In Figure 2, **IMAX-PPO (InQ)** consistently and significantly outperforms other baselines. Our algorithm frequently attains quicker convergence; it achieves high win rates at earlier training stages. This could be attributed to the incorporation of our IL component, which facilitates faster comprehension of opponents throughout the game. In particular, the **IMAX-PPO (InQ)** outperforms the two other variants **IMAX-PPO (GAIL)** and **Sup-MAPPO**, indicating the advantage of our inverse-Q approach over other IL (i.e. GAIL) and traditional supervising learning methods. Details of win rates at the end of training are shown in Table 1.

Google Research Football (GRF).This is a challenge on Kaggle competitions made by Google Research team . We focus on three main sub-tasks, sorted based on increasing difficulty levels: (i) _academy-3-vs-1-with-keeper_: three allies try to score against a goal-keeping opponent; (ii) _academy-counterattack-easy_: four allies versus a counter-attack opponent and a goal-keeping opponent; and (iii) _academy-counterattack-hard_: four allies versus two counter-attack opponents and a goalkeeper.

By default, the representations of all agents' observations are RGB pixels in GRF, so we pre-process this information by distilling some important features such as object positions, object directions, distances between objects, etc. The final win rates are in Table 1, which shows that **IMAX-PPO (InQ)** achieves nearly 100% win-rates, and significantly outperforms other baselines.4

Gold Miner .This is another competitive multi-agent game for evaluating our methods, originating from a MARL competition. Multiple miners navigate in a 2D terrain containing obstacles and repositories of gold. Players get points according to the volume of gold they successfully extract. This game is challenging to win as the agents have to learn playing against extremely well-designed heuristic-based enemies. In this game, the ally agents win if the allied team's average mined gold is higher than that of the enemy team.

We customized the original environment into three sub-tasks (between two allies against two enemies) of three difficulty levels: (i) _Easy (easy_2_vs_2)_: The enemies' greedy strategy is to find the shortest way to the golds; (ii) _Medium (medium_2_vs_2)_: One enemy is greedy, and the other follows the algorithm of the second-ranking team in the competition; and (iii) _Hard (hard_2_vs_2)_: The enemies are the first- and second-ranking teams in the competition.

Figure 2: Win-rate curves on SMACv2 environment.

For this environment, the win rates are in Table 1. Again, **IMAX-PPO (InQ)** obtained superior win rates across all three tasks. Especially, in the hard-level task, our algorithm manages to win more than \(50\%\) of the time against the first and second-ranking teams in the competition.

## 7 Conclusion

We introduced a novel principled framework for enhancing agent training in multi-agent environments through IL. Our new IL model, adapted from IQ-learn, can predict opponents' policy using only local state observations. By integrating this model into a multi-agent PPO algorithm, our IMAX-PPO algorithm consistently outperforms previous SOTA algorithms such as QMIX and MAPPO. This improvement is observed across various challenging multi-agent tasks, including SMACv2 and GRF. A possible **limitation** of our work is that it relies on the assumption that the enemies do not update their policies during training (even though this is a standard setting in cooperative multi-agent reinforcement learning [4; 25]). A future direction would be to delve into these aspects to develop efficient MARL algorithms for cooperative-competitive multi-agent RL.