# Anytime-Competitive Reinforcement Learning

with Policy Prior

 Jianyi Yang

UC Riverside

Riverside, CA, USA

jyang239@ucr.edu

&Pengfei Li

UC Riverside

Riverside, CA, USA

pli081@ucr.edu

&Tongxin Li

UC Riverside

Shenzhen, Guangdong, China

litongxin@cuhk.edu.cn

&Adam Wierman

Caltech

Pasadena, CA, USA

adamw@caltech.edu

&Shaolei Ren

UC Riverside

Riverside, CA, USA

shaolei@ucr.edu

###### Abstract

This paper studies the problem of Anytime-Competitive Markov Decision Process (A-CMDP). Existing works on Constrained Markov Decision Processes (CMDPs) aim to optimize the expected reward while constraining the _expected_ cost over random dynamics, but the cost in a specific episode can still be unsatisfactorily high. In contrast, the goal of A-CMDP is to optimize the expected reward while guaranteeing a bounded cost in _each_ round of _any_ episode against a policy prior. We propose a new algorithm, called Anytime-Competitive Reinforcement Learning (\(\)), which provably guarantees the anytime cost constraints. The regret analysis shows the policy asymptotically matches the optimal reward achievable under the anytime competitive constraints. Experiments on the application of carbon-intelligent computing verify the reward performance and cost constraint guarantee of \(\).

## 1 Introduction

In mission-critical online decision-making problems such as cloud workload scheduling [49; 20], cooling control in datacenters [63; 16; 46], battery management for Electrical Vehicle (EV) charging [57; 36], and voltage control in smart grids [54; 73], there is always a need to improve the reward performances while meeting the requirements for some important cost metrics. In these mission-critical systems, there always exist some policy priors that meet the critical cost requirements, but they may not perform well in terms of the rewards. In the application of cooling control, for example, some rule-based heuristics [19; 46] have been programmed into the real systems for a long time and have verified performance in maintaining a safe temperature range, but they may not achieve a high energy efficiency. In this paper, we design a Reinforcement Learning (RL) algorithm with the goal of optimizing the reward performance under the guarantee of cost constraints against a policy prior for any round in any episode.

Constrained RL algorithms have been designed to solve various Constrained MDPs (CMDPs) with reward objective and cost constraints. Among them, some are designed to guarantee the expected cost constraints [61; 62], some can guarantee the cost constraints with a high probability (w.h.p.) , and others guarantee a bounded violation of the cost constraints [26; 29; 21; 22; 1]. In addition, conservative RL algorithms [27; 68; 31; 65] consider constraints that require the performance of a learning agent is no worse than a known baseline policy in expectation or with a high probability.  points out that it is impossible to guarantee the constraints for any round with probabilityone while such guarantees are desirable. In real mission-critical applications, the cost constraints are often required to be satisfied at each round in any episode even in the worst case, but such anytime constraints are not guaranteed by the existing constrained/conservative RL policies. Recently, learning-augmented online control algorithms [37; 38; 69; 51; 17; 35] have been developed to exploit machine learning predictions with the worst-case control performance guarantee. Nonetheless, the learning-augmented control algorithms require the full knowledge of the dynamic models, which limits their applications in many systems with unknown random dynamic models. A summary of most relevant works is given in Table 1.

To fill in this technical blank, we model the mission-critical decision-making problem as a new Markov Decision Process (MDP) which is called the Anytime-Competitive MDP (A-CMDP). In A-CMDP, the environment feeds back a reward and a cost corresponding to the selected action at each round. The next state is updated based on a random dynamic model which is a function of the current action and state and is not known to the agent. The distribution of the dynamic model is also unknown to the agent and needs to be learned. Importantly, at each round \(h\) in any episode, the policy of A-CMDP must guarantee that the cumulative cost \(J_{h}\) is upper bounded by a scaled cumulative cost of the policy prior \(^{}\) plus a relaxation, i.e. \(J_{h}(1+)J_{h}^{}+hb\) with \(,b>0\), which is called an _anytime_ competitive constraint or _anytime_ competitiveness. The term "competitive" or "competitiveness" is used to signify the performance comparison with a policy prior. Under the anytime cost competitiveness for all rounds, the RL agent explores the policy to optimize the expected reward.

The anytime competitiveness guarantee is more strict than the constraints in typical constrained or conservative MDPs, which presents new challenges for the RL algorithm design. First of all, the anytime competitive constraints are required to be satisfied for any episode, even for the early episodes when the collected sequence samples are not enough. Also, to guarantee the constraints for each round, we need to design a safe action set for each round to ensure that feasible actions exist to meet the constraints in subsequent rounds. Last but not least, without knowing the full transition model, the agent has no access to the action sets defined by the anytime competitive constraints. Thus, in comparison to the control settings with known transition models , ensuring the anytime competitiveness for MDPs is more challenging.

**Contributions**. In this paper, we design algorithms to solve the novel problem of A-CMDP. The contributions are summarized as follows. First, we propose an Anytime-Competitive Decision-making (ACD) algorithm to provably guarantee the anytime competitive constraints for each episode. The key design in ACD is a projection to a safe action set in each round. The safe action set is updated at each round according to a designed rule to gain as much flexibility as possible to optimize the reward. Then, we develop a new model-based RL algorithm (ACRL) to learn the optimal ML model used in ACD. The proposed model-based RL can effectively improve the reward performance based on the new dynamic defined by ACD. Last but not least, we give rigorous analysis on the reward regret of ACRL compared with the optimal-unconstrained policy. The analysis shows that the learned policy performs as well as the optimal ACD policy and there exists a fundamental trade-off between the optimization of the average reward and the satisfaction of the anytime competitive constraints.

## 2 Related Work

**Constrained RL**. Compared with the existing literature on constrained RL [68; 1; 67; 10; 13; 2; 21; 29; 26; 22; 61], our study has important differences. Concretely, the existing constrained RL works consider an _average_ constraint with or without constraint violation. In addition, existing conservative RL works  consider an _average_ constraint compared with a policy prior. However, the constraints can be violated especially at early exploration episodes. In sharp contrast, our anytime competitive

    & **Unknown** & **Expected constraints or constraints w.h.p.** & **Any-episode** & **Anytime-competitive** \\  & **dynamic** & **With violation** & **No violation** & **constraints** & **constraints** \\  Learning-augmentation & ✗ & N/A & N/A & [38; 51; 17] & ✗ \\ Constrained RL & ✗ & [29; 21; 22; 1, 67; 26] & [62; 61] & ✗ & ✗ \\ Conservative RL & ✗ & N/A & [68; 27; 31; 65] & ✗ & ✗ \\ ACRL (**this work**) & ✗ & N/A & ✗ & ✗ & ✗ \\   

Table 1: Comparison between ACRL and most related works.

constraint ensures a strict constraint for any round in each episode, which has not been studied in the existing literature as shown in Table 1. In fact, with the same policy prior, our anytime competitive policy can also meet the average constraint without violation in conservative/constrained RL [68; 25].  considers MDPs that satisfy safety constraint with probability one and proposes an approach with a high empirical performance. However, there is no theoretical guarantee for constraint satisfaction. Comparably, our method satisfies the constraint with theoretical guarantee, which is essential to deploy AI for mission-critical applications.

Our study is also relevant to safe RL. Some studies on safe RL [10; 2; 13] focus on constraining that the system state or action at each time \(h\) cannot fall into certain pre-determined restricted regions (often with a high probability), which is orthogonal to our anytime competitiveness requirement that constrains the cumulative cost at each round of an episode. Our study is related to RL with safety constraints [10; 39], but is highlighted by the strict constraint guarantee for each round in each episode. In a study on safe RL , the number of safety violation events is constrained almost surely by a budget given in advance, but the safety violation value can still be unbounded. By contrast, our work strictly guarantees the anytime competitive constraints by designing the safety action sets. In a recent study , the safety requirement is formulated as the single-round cost constraints. Differently, we consider cumulative cost constraints which have direct motivation from mission-critical applications.

**Learning-augmented online decision-making**. Learning-based policies can usually achieve good average performance but suffer from unbounded worst-case performance. To meet the requirements for the worst-case performance of learning-based policies, learning-augmented algorithms are developed for online control/optimization problems [38; 51; 17; 35; 34]. To guarantee the performance for each problem instance, learning-augmented algorithm can perform an online switch between ML policy and prior , combine the ML policy and prior with an adaptive parameter , or project the ML actions into safe action sets relying on the prior actions . Compared with learning-augmented algorithms, we consider more general online settings without knowing the exact dynamic model. Also, our problem can guarantee the cost performance for each round in any episode compared with a policy prior, which has not been studied by existing learning-augmented algorithms.

## 3 Problem Formulation

### Anytime-Competitive MDP

In this section, we introduce the setting of a novel MDP problem called Anytime-Competitive Markov Decision Process (A-CMDP), denoted as \((,,,g,H,r,c,,^{})\). In A-CMDP, each episode has \(H\) rounds. The state at each round is denoted as \(x_{h},h[H]\). At each round of an episode, the agent selects an action \(a_{h}\) from an action set \(\). The environment generates a reward \(r_{h}(x_{h},a_{h})\) and a cost \(c_{h}(x_{h},a_{h})\) with \(r_{h}\) and \(c_{h}\). We model the dynamics as \(x_{h+1}=f_{h}(x_{h},a_{h})\) where \(f_{h}\) is a random transition function drawn from an unknown distribution \(g(f_{h})\) with the density \(g\). The agent has no access to the random function \(f_{h}\) but can observe the state \(x_{h}\) at each round \(h\). Note that we model the dynamics in a function style for ease of presentation, and this dynamic model can be translated into the transition probability in standard MDP models [59; 5] as \((x_{h+1} x_{h},a_{h})=_{f_{h}}(f_{h}(x_{h},a_{h}) =x_{h+1})g(f_{h})\). A policy \(\) is a function which gives the action \(a_{h}\) for each round \(h[H]\). Let \(V_{h}^{}(x)=[_{i=h}^{H}r_{i}(x_{i},a_{i})) x_{ h}=x]\) denote the expected value of the total reward from round \(h\) by policy \(\). One objective of A-CMDP is to maximize the expected total reward starting from the first round which is denoted as \(_{x_{1}}[V_{1}^{}(x_{1})]=[_{h=1}^ {H}r_{h}(x_{h},a_{h}))\).

Besides optimizing the expected total reward as in existing MDPs, A-CMDP also guarantees the anytime competitive cost constraints compared with a policy prior \(^{}\). The policy prior can be a policy that has verified cost performance in real systems or a heuristic policy with strong empirically-guaranted cost performance, for which concrete examples will be given in the next section. Denote \(y_{h}=(f_{h},c_{h},r_{h})\), and \(y_{1:H}=\{y_{h}\}_{h=1}^{H}= \) is a sampled sequence of the models in an A-CMDP. Let \(J_{h}^{}(y_{1:H})=_{i=1}^{h}c_{i}(x_{i},a_{i})\) be the cost up to round \(h[H]\) with states \(x_{i},i[h]\) and actions \(a_{i},i[h]\) of a policy \(\). Also, let \(J_{h}^{^{}}(y_{1:H})=_{i=1}^{h}c_{i}(x_{i}^{},a_{i}^{ })\) be the cost of the prior with states \(x_{i}^{},i[h]\) and actions \(a_{i}^{},i[h]\) of the prior \(^{}\). The anytime competitive constraints are defined as below.

**Definition 3.1** (Anytime competitive constraints).: If a policy \(\) satisfies \((,b)-\)anytime competitive-ness, the cost of \(\) never exceeds the cost of the policy prior \(^{}\) relaxed by parameters \( 0\) and \(b 0\), i.e. for any round \(h\) in any model sequence \(y_{1:H}\), it holds that \(J_{h}^{}(y_{1:H})(1+)J_{h}^{^{}}(y_{1:H})+hb\).

Now, we can formally express the objective of A-CMDP with \(\) being the policy space as

\[_{}_{x_{1}}[V_{1}^{}(x_{1})], s.t. \ \ J_{h}^{}(y_{1:H})(1+)J_{h}^{^{}}(y_ {1:H})+hb,\ \  h[H], y_{1:H}.\] (1)

Let \(_{,b}\) be the collection of policies that satisfy the anytime competitive constraints in (1). We design an anytime-competitive RL algorithm that explores the policy space \(_{,b}\) in \(K\) episodes to optimize the expected reward \(_{x_{1}}[V_{1}^{}(x_{1})]\). Note that different from constrained/conservative MDPs , the anytime competitive constraints in (1) must be satisfied for any round in any sampled episode \(y_{1:H}\) given relaxed parameters \(,b 0\). To evaluate the performance of the learned policy \(^{k}_{,b},k[K]\) and the impact of the anytime competitive constraints, we consider the regret performance metric defined as

\[(K)=_{k=1}^{K}_{x_{1}}[V_{1}^{^{*}}(x_{1}) -V_{1}^{^{k}}(x_{1})],\ ^{k}_{,b}\] (2)

where \(^{*}=_{}_{x_{1}}[V_{1}^{}(x_{1})]\) is an optimal policy without considering the anytime competitive constraints. When \(\) or \(b\) becomes larger, the constraints get less strict and the algorithm has more flexibility to minimize the regret in (2). Thus, the regret analysis will show the trade-off between optimizing the expected reward and satisfying the anytime cost competitiveness.

In this paper, we make additional assumptions on the cost functions, transition functions, and the prior policy which are important for the anytime-competitive algorithm design and analysis.

**Assumption 3.2**.: All the cost functions in the space \(\) have a minimum value \( 0\), i.e. \((x,a), h[H],c_{h}(x,a) 0\), and are \(L_{c}\)-Lipschitz continuous with respect to action \(a_{h}\) and the state \(x_{h}\). All the transition functions in the space \(\) are \(L_{f}\)-Lipschitz continuous with respect to action \(a_{h}\) and the state \(x_{h}\). The parameters \(,L_{c}\) and \(L_{f}\) are known to the agent.

The Lipschitz continuity of cost and transition functions can also be found in other works on model-based MDP . The Lispchitz assumptions actually apply to many continuous mission-critical systems like cooling systems , power systems  and carbon-aware datacenters . In these systems, the agents have no access to concrete cost and transition functions, but they can evaluate the Lipschitz constants of cost and dynamic functions based on the prior knowledge of the systems. The minimum cost value can be as low as zero, but the knowledge of a positive minimum cost \(\) can be utilized to improve the reward performance which will be discussed in Section 5.1.

**Definition 3.3** (Telescoping policy).: A policy \(\) satisfies the telescoping property if the policy is applied from round \(h_{1}\) to \(h_{2}\) with initialized states \(x_{h_{1}}\) and \(x_{h_{1}}^{}\), it holds for the corresponding states \(x_{h_{2}}\) and \(x_{h_{2}}^{}\) at round \(h_{2}\) that

\[\|x_{h_{2}}-x_{h_{2}}^{}\| p(h_{2}-h_{1})\|x_{h_{1}}-x_{h_{1}}^{ }\|,\] (3)

where \(p(h)\) is called a perturbation function with \(h\) and \(p(0)=1\).

**Assumption 3.4**.: The prior policy \(^{}\) satisfies the telescoping property with some perturbation function \(p\). Furthermore, \(^{}\) is Lipschitz continuous.

The telescoping property in Definition 3.3 indicates that with an initial state perturbation at a fixed round, the maximum divergence of the states afterwards is bounded. Thus, the perturbation function \(p\) measures the sensitivity of the state perturbation with respect to a policy prior. The telescoping property is satisfied for many policy priors . It is also assumed for perturbation analysis in model predictive control .

Note that in A-CMDP, the constraints are required to be satisfied for any round in any sequence, which is much more stringent than constraint satisfaction in expectation or with a high probability. In fact, the any-time constraints cannot be theoretically guaranteed without further knowledge on the system . This paper firstly shows that Assumption 3.2 and Assumption 3.4, which are reasonable for many mission-critical applications , are enough to guarantee the anytime competitive constraints, thus advancing the deployment of RL in mission-critical applications.

### Motivating Examples

The anytime competitiveness has direct motivations from many mission-critical control systems. We present two examples in this section and defer other examples to the appendix.

**Safe cooling control in data centers.** In mission-critical infrastructures like data centers, the agent needs to make decisions on cooling equipment management to maintain a temperature range and achieve a high energy efficiency. Over many years, rule-based policies have been used in cooling systems and have verified cooling performance in maintaining a suitable temperature for computing . Recently, RL algorithms are developed for cooling control in data centers to optimize the energy efficiency [63; 16; 46]. The safety concerns of RL policies, however, hinder their deployment in real systems. In data centers, an unreliable cooling policy can overheat devices and denial critical services, causing a huge loss [19; 46]. The safety risk is especially high at the early exploration stage of RL in the real environment. Therefore, it is crucial to guarantee the constraints on cooling performance at anytime in any episode for safety. With the reliable rule-based policies as control priors, A-CMDP can accurately model the critical parts of the cooling control problem, opening a path towards learning reliable cooling policies for data centers.

**Workload scheduling in carbon-intelligent computing.** The world is witnessing a growing demand for computing power due to new computing applications. The large carbon footprint of computing has become a problem that cannot be ignored [49; 64; 52; 23]. Studies find that the amount of carbon emission per kilowatt-hour on electricity grid varies with time and locations due to the various types of electricity generation [33; 32; 9]. Exploiting the time-varying property of carbon efficiency, recent studies are developing workload scheduling policies (e.g. delay some temporally flexible workloads) to optimize the total carbon efficiency . However, an unreliable workload scheduling policy in data centers can cause a large computing latency, resulting in an unsatisfactory Quality of Service (QoS). Thus, to achieve a high carbon efficiency while guaranteeing a low computing latency, we need to solve an A-CMDP which leverages RL to improve the carbon efficiency while guaranteeing the QoS constraints compared with a policy prior targeting at computing latency [20; 30; 12; 72; 71]. This also resembles the practice of carbon-intelligent computing adopted by Google .

## 4 Methods

In this section, we first propose an algorithm to guarantee the anytime competitive constraints for any episode, and then give an RL algorithm to achieve a high expected reward under the guarantee of the anytime competitive constraints.

### Guarantee the Anytime Constraints

It is challenging to guarantee the anytime competitive constraints in (1) for an RL policy in any episode due to the following. First of all, in MDPs, the agent can only observe the _real_ states \(\{x_{h}\}_{h=1}^{H}\) corresponding to the truly-selected actions \(\{a_{h}\}_{h=1}^{H}\). The agent does not select the actions \(a_{h}^{}\) of the prior, so the states of the prior \(x_{h}^{}\) are _virtual_ states that are not observed. Thus, the agent cannot evaluate the prior cost \(J_{h}^{^{}}\) which is in the anytime competitive constraint at each round \(h\). Also, the action at each round \(h\) has an impact on the costs in the future rounds \(i,i>h\) based on the random transition models \(f_{i},i h\). Thus, besides satisfying the constraints in the current round, we need to have a good planning for the future rounds to avoid any possible constraint violations even without the exact knowledge of transition and/or cost models. Additionally, the RL policy may be arbitrarily bad in the environment and can give high costs (especially when very limited training data is available), making constraint satisfaction even harder.

Despite the challenges, we design safe action sets \(\{_{h},h[H]\}\) to guarantee the anytime competitive constraints: if action \(a_{h}\) is strictly selected from \(_{h}\) for each round \(h\), the anytime competitive constraints for all rounds are guaranteed. As discussed above, the anytime competitive constraints cannot be evaluated at any time since the policy prior's state and cost information is not available. Thus, we propose to convert the original anytime competitive constraints into constraints that only depend on the known parameters and the action differences between the real policy and the policy prior. We give the design of the safe action sets based on the next proposition. For the ease of presentation, we denote \(c_{i}=c_{i}(x_{i},a_{i})\) as the real cost and \(c_{i}^{}=c_{i}(x_{i}^{},(x_{i}^{}))\) as the cost of the policy prior at round \(i\).

**Proposition 4.1**.: _Suppose that Assumption 3.2 and 3.4 are satisfied. At round \(h\) with costs \(\{c_{i}\}_{i=1}^{h-1}\) observed, the anytime competitive constraints \(J_{h^{}}^{^{}}(1+)J_{h^{}}^{^{}}+h^{ }b\) for rounds \(h^{}=h,,H\) are satisfied if for all subsequent rounds \(h^{}=h,,H\),_

\[_{j=h}^{h^{}}_{j,j}\|a_{j}-^{}(x_{j})\| G_{h,h^ {}},\, h^{}=h,,H,\] (4)

_where \(_{j,n}=_{i=n}^{H}q_{j,i},(j[H], n j)\), with \(q_{j,i}=L_{c}(j=i)+L_{c}(1+L_{^{}})L_{f}p(i-1-j)(j<i),( j[H],i j),\) relying on known parameters, and \(G_{h,h^{}}\) is called the allowed deviation which is expressed as_

\[G_{h,h^{}}=_{i=1}^{h-1}((1+)_{i}^{}-c_{i} -_{i,h}d_{i})+(h^{}-h+1)(+b),\] (5)

_where \(_{i}^{}=\{,c_{i}-_{j=1}^{i}q_{j,i}d_{j} \},( i[H]),\) is the lower bound of of \(c_{i}^{}\), and \(d_{j}=\|a_{j}-^{}(x_{j})\|, j[H]\) is the action difference at round \(j\)._

At each round \(h[H]\), Proposition 4.1 provides a sufficient condition for satisfying all the anytime competitive constraints from round \(h\) to round \(H\) given in (1). The meanings of the parameters in Proposition 4.1 are explained as follows. The weight \(q_{j,i}\) measures the impact of action deviation at round \(j\) on the cost difference \(|c_{i}-c_{i}^{}|\) at round \(i j\), and the weight \(_{j,n}\) indicates the total impact of the action deviation at round \(j\) on the sum of the cost differences from rounds \(n\) to round \(H\). Based on the definition of \(q_{j,i}\), we get \(_{i}^{}\) as a lower bound of the prior cost \(c_{i}^{}\). With these bounds, we can calculate the maximum allowed total action deviation compared with the prior actions \(^{}(x_{j})\) from round \(j=h\) to \(h^{}\) as \(G_{h,h^{}}\)

By applying Proposition 4.1 at initialization, we can guarantee the anytime competitive constraints for all rounds \(h^{}[H]\) if we ensure that for all rounds \(h^{}[H]\), \(_{j=1}^{h}_{j,j}\|a_{j}-^{}(x_{j})\| G_{1,h^{}} =h^{}(+b)\). This sufficient condition is a long-term constraint relying on the relaxation parameters \(\) and \(b\). Although we can guarantee the anytime competitive constraints by the sufficient condition obtained at initialization, we apply Proposition 4.1 at all the subsequent rounds with the cost feedback information to get larger action sets and more flexibility to optimize the average reward. In this way, we can update the allowed deviation according to the next corollary.

**Corollary 4.2**.: _At round 1, we initialize the allowed deviation as \(D_{1}=+b\). At round \(h,h>1\), the allowed deviation is updated as_

\[D_{h}=\{D_{h-1}++b-_{h-1,h-1}d_{h-1},\,R_{h-1}+ +b\}\] (6)

_where \(R_{h-1}=_{i=1}^{h-1}((1+)_{i}^{}-c_{i}-_{i,h}d_{i})\) with notations defined in Proposition 4.1. The \((,b)-\)anytime competitive constraints in Definition 3.1 are satisfied if it holds at each round \(h\) that \(_{h,h}\|a_{h}-^{}(x_{h})\| D_{h}\)._

Corollary 4.2 gives a direct way to calculate the allowed action deviation at each round. In the update rule (6) of the allowed deviation, the first term of the maximum operation is based on the deviation calculation at round \(h-1\) while the second term is obtained by applying Proposition 4.1 for round \(h\).

We can find that the conditions to satisfy the anytime competitive constraints can be controlled by parameters \(\) and \(b\). With larger \(\) and \(b\), the anytime competitive constraints are relaxed and the conditions in Corollary 4.2 get less stringent. Also, the conditions in Corollary 4.2 rely on the minimum cost value \(\) and other system parameters including Lipschitz constants \(L_{c},L_{f},L_{^{}}\) and telescoping parameters \(p\) through \(_{h,h}\). Since \(_{h,h}\) increases with the Lipschitz and telescoping parameters, even if the estimated Lipschitz constants and the telescoping parameters are higher than the actual values or the estimated minimum cost is lower than the actual value, the obtained condition by Corollary 4.2 is sufficient to guarantee the anytime competitive constraints, although it is more stringent than the condition calculated by the actual parameters.

By Corollary 4.2, we can define the safe action set at each round \(h\) as

\[_{h}(D_{h})=\{a_{h,h}\|a-^{}(x_{h})\| D _{h}\}.\] (7)With the safe action set design in (7), we propose a projection-based algorithm called \(\) in Algorithm 1. We first initialize an allowed deviation as \(D_{1}=+b\). When the output \(_{h}\) of the ML model is obtained at each round \(h\), it is projected into a safe action set \(_{h}(D_{h})\) depending on the allowed deviation \(D_{h}\), i.e. \(a_{h}=P_{_{h}(D_{h})}(_{h})=_{a_{h}( D_{h})}\|a-_{h}\|\). The projection can be efficiently solved by many existing methods on constrained policy learning [67; 11; 4; 41; 24]. The allowed deviation is then updated based on Corollary 4.2. Intuitively, if the actions are closer to the prior actions before \(h\), i.e. the action deviations \(\{d_{i}\}_{i=1}^{h-1}\) get smaller, then \(R_{h-1}\) becomes larger and \(D_{h}\) becomes larger, leaving more flexibility to deviate from \(a_{i}^{},i h\) in subsequent rounds.

### Anytime-Competitive RL

The anytime competitive constraints have been satisfied by Algorithm 1, but it remains to design an RL algorithm to optimize the average reward under the anytime competitive cost constraints, which is given in this section.

The anytime-competitive decision-making algorithm in Algorithm 1 defines a new MDP, with an additional set of allowed deviations \(\) to the A-MDP defined in Section 3.1, denoted as \(}(,,,,g,H,r,c, ,^{})\). In the new MDP, we define an augmented state \(s_{h}\) which include the original state \(x_{h}\), the allowed deviation \(D_{h}\), and history information \(\{c_{i}\}_{i=1}^{h-1}\) and \(\{d_{i}\}_{i=1}^{h-1}\). The transition of \(x_{h}\) is defined by \(f_{h}\) in Section 3.1 and needs to be learned while the transition of \(D_{h}\) is defined in (6) and is known to the agent. The ML policy \(\) gives an output \(_{h}\) and the selected action is the projected action \(a_{h}=P_{_{h}(D_{h})}(_{h})\). Then the environment generates a reward \(r_{h}(x_{h},P_{_{h}(D_{h})}(_{h}))\) and a cost \(c_{h}(x_{h},P_{_{h}(D_{h})}(_{h}))\). Thus, the value function corresponding to the ML policy \(\) can be expressed as \(_{h}^{}(s_{h})=[_{i=h}^{H}r_{i}(x_{i}, P_{_{h}(D_{h})}(_{h}))]\) with \(_{h}\) being the output of the ML policy \(\). For notation convenience, we sometimes write the actions of \(^{*}\) and \(^{}\) as \(^{*}(s)\) and \(^{}(s)\) even though they only reply on the original state \(x\) in \(s\).

To solve the MDP, we propose a model-based RL algorithm called \(\) in Algorithm 2. Different from the existing model-based RL algorithms [48; 6; 70], \(\) utilizes the dynamic model of A-CMDP and \(\) (Algorithm 1) to optimize the average reward. Given a transition distribution \(g\) at episode \(k\), we perform value iteration to update \(\) functions for \(h=1,,H\).

\[&_{h}^{k}(s_{h},_{h})=r_{h}(x_{h},a_{h}) +_{g}[_{h+1}^{k}(s_{h+1}) s_{h},a_{h}],\, _{h}^{k}(s_{h})=_{a}_{h}^{k}(s_{h},a),\\ &_{g}[_{h+1}^{k}(s_{h+1}) s_{h},a_{h} ]=_{f}_{h+1}^{k}(s_{h+1})g(f),\] (8)

where \(a_{h}=P_{_{h}(D_{h})}(_{h})\), \(_{H+1,k}(s,a)=0,_{H+1,k}(s)=0\). The transition model \(g\) is estimated as

\[^{k}=_{g}_{i=1}^{k-1}_{h=1}^{H}( _{g}[_{h+1}^{i}(s_{h+1}) s_{h},a_{h}]- _{h+1}^{i}(s_{h+1}))^{2}.\] (9)

Based on the transition estimation, we can calculate the confidence set of the transition model as

\[_{k}\!=\!\{g|_{i=1}^{k-1}_{h=1}^{H} (_{g}[_{h+1}^{i}(s_{h+1}) s_{h},a_{h}]- _{^{k}}[_{h+1}^{i}(s_{h+1}) s_{h},a_{h} ])^{2}_{k}.\},\] (10)

where \(_{k}>0\) is a confidence parameter.

With a learned ML policy \(^{k}\) at each episode \(k\), the policy used for action selection is the \(\) policy \(^{k}\). Given the optimal ML policy \(^{*}=*{arg\,max}_{}_{1 }^{^{*}}(s_{1})\) with \(\) being the ML policy space, the optimal \(\) policy is denoted as \(^{}\). For state \(s_{h}\) at round \(h\), \(^{k}\) and \(^{}\) select actions as

\[^{k}(s_{h})=P_{_{h}(D_{h})}(^{k}(s_{h})),\;^{ }(s_{h})=P_{_{h}(D_{h})}(^{*}(s_{h})).\] (11)

In the definition of A-CMDP, the dimension of the augmented state \(s_{h}\) increases with the length of the horizon \(H\), which cloud cause a scalability issue for implementation. The scalability issues also exit in other RL works with history-dependent states [58; 14]. In practice, tractable methods can be designed through feature aggregation  or PODMP .

## 5 Performance Analysis

In this section, we analyze the reward regret of \(\) to show the impacts of anytime cost constraints on the average reward.

### Regret due to Constraint Guarantee

Intuitively, due to the anytime competitive constraints in Eqn. (1), there always exists an unavoidable reward gap between an \(\) policy and the optimal-unconstrained policy \(^{*}\). In this section, to quantify this unavoidable gap, we bound the regret of the optimal \(\) policy \(^{}\), highlighting the impact of anytime competitive cost constraints on the average reward performance.

**Theorem 5.1**.: _Assume that the optimal-unconstrained policy \(^{*}\) has a value function \(Q_{h}^{^{*}}(x,a)\) which is \(L_{Q,h}\)-Lipschitz continuous with respect to the action \(a\) for all \(x\). The regret between the optimal \(\) policy \(^{}\) that satisfies \((,b)-\)anytime competitiveness and the optimal-unconstrained policy \(^{*}\) is bounded as_

\[_{x_{1}}[V_{1}^{^{*}}(x_{1})-V_{1}^{^{}}(x_{1}) ]_{y_{1,H}}\{_{h=1}^{H}L_{Q,h}[-}(+b+ G_{h})]^{+}\},\] (12)

_where \(=_{x}\|^{*}(x)-^{}(x))\|\) is the maximum action discrepancy between the policy prior \(^{}\) and optimal-unconstrained policy \(^{*}\); \(_{h,h}\) is defined in Proposition 4.1; \( G_{h}=[R_{h-1}]^{+}\) is the gain of the allowed deviation by applying Proposition 4.1 at round \(h\)._

The regret bound stated in Theorem 5.1 is intrinsic and inevitable, due to the committed assurance of satisfying the anytime competitive constraints. Such a bound cannot be improved via policy learning, i.e., converge to \(0\) when the number of episodes \(K\). This is because to satisfy the \((,b)-\)anytime competitiveness, the feasible policy set \(_{,b}\) defined under (1) is a subset of the original policy set \(\), and the derived regret is an upper bound of \(_{}_{x_{1}}[V_{1}^{}(x_{1})]-_{ _{,b}}_{x_{1}}[V_{1}^{}(x_{1})]\). Moreover, the regret bound relies on the action discrepancy \(\). This is because if the optimal-unconstrained policy \(^{*}\) is more different from the prior \(^{}\), its actions are altered to a larger extent to guarantee the constraints, resulting in a larger degradation of the reward performance. More importantly, the regret bound indicates the trade-offbetween the reward optimization and anytime competitive constraint satisfaction governed by the parameters \(\) and \(b\). When \(\) or \(b\) becomes larger, we can get a smaller regret because the anytime competitive constraints in (1) are relaxed to have more flexibility to optimize the average reward. In the extreme cases when \(\) or \(b\) is large enough, all the policies in \(\) can satisfy the anytime competitive constraints, so we can get zero regret.

Moreover, the regret bound shows that the update of allowed deviation by applying Proposition 4.1 based on the cost feedback at each round will benefit the reward optimization. By the definition of \(R_{h-1}\) in Corollary 4.2, if the real actions deviate more from the prior actions before \(h\), the gain \( G_{i}\) for \(i h\) can be smaller, so the actions must be closer to the prior actions in the subsequent rounds, potentially causing a larger regret. Thus, it is important to have a good planing of the action differences \(\{d_{i}\}_{i=1}^{H}\) to get larger allowed action deviations for reward optimization. Exploiting the representation power of machine learning, ACRL can learn a good planning of the action differences, and the \(\) policy \(^{}\) corresponding to the optimal ML policy \(^{*}\) can achieve the optimal planing of the action differences.

Last but not least, Theorem 5.1 shows the effects of the systems parameters in Assumption 3.2 and Assumption 3.4 on the regret through \(_{h,h}\) defined in Proposition 4.1 and the minimum cost \(\). Observing that \(_{h,h}\) increases with the systems parameters including the Lipschitz parameters \(L_{f},L_{c},L_{^{}}\) and telescoping parameters \(p\), a higher estimation of the Lipschitz parameters and telescoping parameters can cause a higher regret. Also, a lower estimation of the minimum cost value can cause a higher regret. Therefore, although knowing the upper bound of the Lipschitz parameters and telescoping parameters and the lower bound of the minimum cost value is enough to guarantee the anytime competitive cost constraints by Proposition 4.1, a lower reward regret can be obtained with a more accurate estimation of these system parameters.

### Regret of Acrl

To quantify the regret defined in Eqn. (2), it remains to bound the reward gap between the \(\) policy \(^{k}\) and the optimal \(\) policy \(^{}\). In this section, we show that \(^{k}\) by ACRL approaches the optimal one \(^{}\) as episode \(K\) by bounding the pseudo regret

\[(K)=_{x_{1}}[_{k=1}^{K}(V_{1}^{^{ }}(s_{1})-V_{1}^{^{k}}(s_{1}))].\] (13)

**Theorem 5.2**.: _Assume that the value function is bounded by \(\). Denote a set of function as_

\[=\{q g,(s,a,v) ,q(s,a,v)=_{f g}[v(s^{ }) s,a]\}.\] (14)

_If \(_{k}=2(H)^{2}((,,\| \|_{})}{})+CH\) with \(=1/(KH(KH/))\), \(\) being a constant, and \((,,\|\|_{})\) being the covering number of \(\), with probability at least \(1-\), the pseudo regret of Algorithm 2 is bounded as_

\[(K) 1+d_{}H+4}_{K} KH}+H,\] (15)

_where \(d_{}=_{E}(,)\) is the Eluder dimension of \(\) defined in ._

Theorem 5.2 bounds the pseudo regret for each episode \(k\). The confidence parameter \(_{k}\) to balance the exploration and exploitation is chosen to get the pseudo regret bound as shown in Theorem 5.2. A higher \(_{k}\) is chosen to encourage the exploration if the covering number of the function space \(\), the episode length, or the maximum value becomes larger. Also, the pseudo regret relies on the size of the function space \(\) through \(d_{}\) and \(_{K}\). With smaller \(\) or \(b\), less actions satisfy Corollary 4.2 given a state, and so a smaller state-action space \(\) is obtained, which results in a smaller size of the function space \(\) and thus a smaller regret.

To get more insights, we also present the overall regret bound when the transition model \(g\) can be represented by a linear kernel as in , i.e. \(g(f)=(f),\) with dimension of \(\) as \(d_{}\), the reward regret in Eqn.2 is bounded as

\[(K)\!\!K_{y_{1:H}}\{_{h=1}^{H}L_{Q,h} -}(+b+ G_{h})^{+ }\!\}\!+\!(^{2}K(1/)}),\] (16)where \(L_{Q,h}\), \(\), \(_{h,h}\), and \( G_{h}\) are all defined in Theorem 5.1. The overall regret bound is obtained because under the assumption of linear transition kernel, we have \(_{K}=O((H)^{2}((,, \|\|_{})))=((H)^{2}(d_{}+(1/)))\), and the Eluder dimension is \(d_{}=(d_{})\). Thus the pseudo regret is \((K)=(V^{2}K(1/)})\) which is sublinear in terms of \(K\). With the sublinear pseudo regret \((K)\), the \(\) policy \(^{k}\) performs as asymptotically well as the optimal \(\) policy \(^{}\) when \(K\). Combining with the regret of the optimal \(\) policy in Theorem 5.1, we can bound the overall regret of \(\). Since in the definition of regret, \(\) policy is compared with the optimal-unconstrained policy \(^{*}\), the regret bound also includes an unavoidable linear term due to the commitment to satisfy the anytime competitive constraints. The linear term indicates the trade-off between the reward optimization and the anytime competitive constraint satisfaction.

## 6 Empirical Results

We experiment with the application of resource management for carbon-aware computing  to empirically show the benefits of \(\). The aim of the problem is to jointly optimize carbon efficiency and revenue while guaranteeing the constraints on the quality-of-service (QoS). In this problem, there exists a policy prior \(^{}\) which directly optimizes QoS based on estimated models. In our experiment, we apply \(\) to optimize the expected reward and guarantee that the real QoS cost is no worse than that of the policy prior. The concrete settings can be found in Appendix A.

Figure 1(a) gives the regret changing the in first 500 episodes. Figure 1(b) shows the regret with different \(\) and \(b\), demonstrating the trade-off between reward optimization and the satisfaction of anytime competitive constraints. Figure 1(c) shows the probability of the violation of the anytime competitive constraints by RL and constrained RL. \(\) and ML models with \(\) have no violation of anytime competitive constraints. More analysis about the results are provided in Appendix A due to space limitations.

## 7 Concluding Remarks

This paper considers a novel MDP setting called A-CMDP where the goal is to optimize the average reward while guaranteeing the anytime competitive constraints which require the cost of a learned policy never exceed that of a policy prior \(^{}\) for any round \(h\) in any episode. To guarantee the anytime competitive constraints, we design \(\), which projects the output of an ML policy into a safe action set at each round. Then, we formulate the decision process of \(\) as a new MDP and propose a model-based RL algorithm \(\) to optimize the average reward under the anytime competitive constraints. Our performance analysis shows the tradeoff between the reward optimization and the satisfaction of the anytime competitive constraints.

**Future directions.** Our results are based on the assumptions on the Lipschitz continuity of the cost, dynamic functions, and the policy prior, as well as the telescoping properties of the policy prior, which are also supposed or verified in other literature [5; 28; 60; 40]. In addition, to guarantee the anytime competitive constraints, the agent is assumed to have access to the Lipschitz constants, the minimum cost value, and the perturbation function. However, since the anytime competitive constraints are much stricter than the expected constraints or the constraints with a high probability, there is no way to guarantee them without any knowledge of the key properties of a mission-critical system. Our work presents the first policy design to solve A-CMDP, but it would be interesting to design anytime-competitive policies with milder assumptions in the future.

Figure 1: Regret and cost violation rate of different algorithms. Shadows in Figure 2(b) show the range of the regret.