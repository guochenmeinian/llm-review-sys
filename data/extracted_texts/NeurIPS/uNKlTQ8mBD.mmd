# Learning Formal Mathematics

From Intrinsic Motivation

 Gabriel Poesia\({}^{1}\) &David Broman\({}^{4}\) &Nick Haber\({}^{1,3}\) &Noah D. Goodman\({}^{1,2}\)

{poesia,nhaber,ngoodman}@stanford.edu &dbro@kth.se

Departments of Computer Science\({}^{1}\), Psychology\({}^{2}\) and Education\({}^{3}\), Stanford University, USA

EECS and Digital Futures\({}^{4}\), KTH Royal Institute of Technology, Sweden

###### Abstract

How did humanity coax mathematics from the other? We explore the Platonic view that mathematics can be discovered from its axioms--a game of conjecture and proof. We describe Minimo (Mathematics from Intrinsic Motivation): an agent that jointly learns to pose challenging problems for itself (_conjecturing_) and solve them (_theorem proving_). Given a mathematical domain axiomatized in dependent type theory, we first combine methods for constrained decoding and type-directed synthesis to sample valid conjectures from a language model. Our method guarantees well-formed conjectures by construction, even as we start with a randomly initialized model. We use the same model to represent a policy and value function for guiding proof search. Our agent targets generating hard but provable conjectures--a moving target, since its own theorem proving ability also improves as it trains. We propose novel methods for hindsight relabeling on proof search trees to significantly improve the agent's sample efficiency in both tasks. Experiments on 3 axiomatic domains (propositional logic, arithmetic and group theory) demonstrate that our agent can bootstrap from _only_ the axioms, self-improving in generating true and challenging conjectures and in finding proofs.

## 1 Introduction

Mathematical reasoning stands as a grand challenge for Artificial Intelligence (AI) research since the birth of the field . Artificial agents capable of general mathematical reasoning have the potential to drastically impact both mathematics itself and areas where mathematical proof plays a key role, such as program and hardware verification . While this goal has received significant attention from the AI community , it still remains far from the breakthroughs that areas such as general game playing , image  generation or protein folding  have witnessed.

Prior work has reflected two main visions of how AI might achieve general mathematical reasoning abilities. One such strategy is to leverage all of the available human-produced mathematical knowledge as a starting point . This knowledge is encoded in source as varied as textbooks, online forums, academic papers, as well as formal proofs written in computer languages such as Lean, Isabelle, Coq or Metamath . Large Language Models (LLMs) can ingest all such sources of knowledge in a unified manner, and provide a foundation for tasks in both formal and informal mathematics. Benchmarks of mathematical problem solving in natural language, such as GSM8k  and MATH , have measured rapid progress over the years, but they remain limited to problems where the final answer is a number, due to the challenge of assessing the correctness of mathematical _arguments_ written in natural language. This difficulty is not a challenge in formal theorem proving, where we can automatically verify the correctness of _proofs_ in arbitrarily high-level mathematics. But benchmarks of formal theorem proving (such as miniZf  and LeanDojo ), even with rapid advances in LLMs, have not yet witnessed the same breakthroughs. In fact, these benchmarks remainfar from solved even though all of the theorems and problems in them are known mathematical facts, often already presented informally in publicly available training data.

An alternative vision of how AI might master mathematical reasoning sees mathematics through the lens of game playing, observing that the rules of the "game of mathematics" can be encoded in a variety of formal systems, such as dependent type theory, using a small set of axioms . In principle, this can allows us to potentially borrow the successes of general game-playing agents, such as AlphaZero , that have achieved remarkable success in mastering complex games entirely from experience. Notably, AlphaZero achieves super-human performance in a range of games without leveraging any human examples. Instead, it learns entirely from experience given only an environment where it can play the game by itself. If this approach could be transported to mathematics, it would bypass the dependency on human examples, and allow us to explore mathematical domains -- both known and new, without distinction -- by utilizing large scale compute and the potential of axioms to produce infinite data.

However, there is a crucial and often neglected difference between mathematics and traditional board games where game-playing AI has succeeded: mathematics is a game with _intrinsic rewards_. Board games, such as Go or Chess, have a fixed starting configuration, and their rules determine the outcome of the game unequivocally. Mastering the game then amounts to learning a policy that optimizes for the _extrinsic_ signal of winning. In theorem proving, a starting configuration is given by a theorem statement, and the correctness of a proof can be assessed objectively in a formal system. But the choice to work on a particular statement -- a _conjecture_, before it is proved -- is not given a priori by the rules of the game of mathematics. Instead, these goals come from the _intrinsically motivated_ agents  who are playing the game. Thus, a key skill developed by human mathematicians is to decide which conjectures are worth considering. In stark contrast, current benchmarks of mathematical reasoning abilities, both formal  and informal , all measure performance on an extrinsically defined set of problems, without space for further discovery.

In this paper, we make the first step towards creating _intrinsically motivated_ mathematical agents by proposing Minimo -- Mathematics from Intrinsic Motivation --, a method to jointly learn conjecturing and theorem proving, starting from _nothing but the axioms of a given mathematical domain_, represented in dependent type theory . We borrow inspiration from the literature of _intrinsic motivation_ in Reinforcement Learning (RL) , where agents can learn from interacting with an environment even when no specific goals are given. Intrinsic motivation objectives have been instrumental for RL in hard exploration environments, where rewards are too sparse to seek directly . The sparsity of rewards is also a major challenge in theorem proving, making this connection especially attractive. We thus define the objective of conjecturing as generating new problems that are challenging for the current agent but still provable within its given search budget. Since the agent also learns from the solutions it finds, the conjecture has to continuously increase the difficulty of the problems it generates.

Minimo performs both conjecturing and proof search with a Transformer  language model (LM), which starts randomly initialized. To sample conjectures even from a model that starts with no prior knowledge, we combine methods from type-directed program synthesis and constrained generation from language models, enabling us to get valid conjectures _by construction_ -- concretely, conjectures are simply terms of type prop, the type of mathematical propositions in our type theory. Then, we perform proof search in the Peano environment , which provides a finite action space for search

Figure 1: We train mathematical reasoning agents starting only from the axioms of a given formal domain, where they jointly learn to pose challenging but provable conjectures and to find their proofs.

in a dependent type theory, guiding search using the LM as a policy and value function. When a proof is found, proof search generates training data for improving the policy and values; it also provides data to improve conjecturing, since we then know how hard the problem was. We use this to alternate between conjecturing and theorem proving, in a self-improving loop. However, successful proofs are sparse. We thus adapt the idea of _hindsight relabeling_ to reinterpret failed trajectories as successful ones by rewriting their goals a posteriori. This significantly accelerates learning, allowing us to extract hundreds of new (true) conjectures, and their proofs, even from failed proof searches. In this way, even unprovable conjectures can be highly useful for the agent's learning. We evaluate our system on three axiomatic mathematical domains--propositional logic, natural number arithmetic, and group theory--showing both that agents self-improve successfully in proving theorems in all domains. We also find that they improve at an extrinsic evaluation of theorems (from a classical textbook on logic , and the Lean Natural Number Game ), _not given in training_. In summary, we make the following contributions:

* We introduce a method for conjecturing using LMs that generates valid conjectures by construction in an arbitrary theory, using constrained decoding and type-directed synthesis.
* We define a hindsight relabeling method that simultaneously generates training data for _conjecturing_ and _theorem proving_.
* We combine these methods in a learning loop where a mathematical agent can self-improve in a given formal domain given _only_ the axioms.
* We evaluate agents trained on axioms for propositional logic, group theory and arithmetic, showing that they improve in both intrinsic and extrinsic evaluations.

## 2 Related Work

Our work is primarily related to prior work on mathematical conjecturing, learning to prove theorems, and on intrinsic motivation in Reinforcement Learning. To the best of our knowledge, our work is the first attempt to unify insights from these areas for training mathematical reasoning agents.

Mathematical conjecturing.The task of _discovering_ mathematical facts was the subject of the influential Automated Mathematician (AM), developed by Lenat in the 1970s . AM was able to conjecture several known mathematical facts and concepts (such as the definition of prime numbers, and the unique factorization theorem). Unlike our system, AM did not aim at _proving_ the conjectures it formulated -- instead, it proposed and judged them based on a set of principles and on empirical evidence collected by AM itself. More recently, other works have revisited the idea of generating conjectures by training language models on human-written theorem statements [39; 41]. Unlike our approach, this relies on pre-training data, and does not readily extend to conjecturing in new domains.

Learning to prove theorems from human data.A large body of recent work has used Large Language Models to guide formal theorem proving in a number of proof assistants, such as Lean , Isabelle [22; 18] and Metamath [31; 42]. Typically, these systems pre-train an LLM on large-scale Internet corpora and fine-tune on human-written formal mathematical proofs. Work on scaling laws for LLMs has shown that they generally improve when their parameter count and dataset size both increase in similar proportion. But the scarcity of formal proofs for training creates a challenge for this approach for learning formal theorem proving: even the largest datasets to date, such as the ProofPile, which aggregates libraries from 5 proof assistants, form relatively small datasets (e.g., 500MB of formal proofs on the ProofPile, contrasting to terabytes of Python code on Github).

Learning to prove theorems from synthetic data.One recent success in automated mathematical reasoning was AlphaGeometry , which was highly effective in solving olympiad-level geometry problems. AlphaGeometry, like our method, was trained entirely on synthetic data. Crucial to its approach is a method for generating both problems and solutions using only the axioms of geometry and a domain-specific deductive closure solver. This allows AlphaGeometry to synthesize and train on hundreds of millions of problems: many orders of magnitude more than existing human-created datasets of mathematical problems. Our approach shares the goal of AlphaGeometry of only using data derived from the axioms of the domain, with the difference that our method (a) is agnostic to the underlying axiomatic system and (b) does not rely on a domain-specific solver. Another line of work, including TacticZero  and rICoP , has explored learning to prove theorems from reinforcement learning only, in a tabula rasa fashion, but still using a human-written set of problems for training (and manually-engineered features, in the case of rICoP).

Intrinsic motivationWe leverage inspiration from the literature on training reinforcement learning agents with intrinsic motivation objectives, allowing an agent to learn without pre-specified goals [26; 34; 33; 13; 27; 7; 3; 37]. Our setup is conceptually close to AMIGO , where agents attempt to generate challenging but achievable next goals. While AMIGO was demonstrated in a simple grid-world environment with a simple goal structure (any point in the grid gives a valid goal), we operate on a much richer setting, where the space of goals is unbounded -- all conjectures in a formal mathematical theory. To sample conjectures, we use Synchronesh's Constrained Semantic Decoding algorithm , and guide it with type constraints.

Self-improvement of language modelsA significant line of recent work has explored the idea that LLMs can "self-improve": increase performance on a given task by fine-tuning on their own generated reasoning traces, which are selected by some criterion that ensures their quality. STaR  fine-tuned on reasoning traces that reached the correct answer on mathematical and multiple choice questions; LMSI  was able to obtain self-improvement on a question-only dataset, sampling multiple rationales and training on those that agree with the majority answer. More related to our work, but less explored, is the direction of having LLMs also generate their own _problems_ for training: this has been explored for self-improving on solving programming problems [14; 37], where code execution provides signal about correctness. In Minimo, since our problems are formal conjectures and our solutions are formal proofs, we can guarantee correctness in a much stronger form than self-generated test cases.

## 3 Minimo

Most recent work on AI for mathematical reasoning assumes a target set of problems to be solved. We deviate from this paradigm by having _the agent itself_ propose problems for it to try to solve and learn from. Our goal is to target increasingly harder problems in a given mathematical domain, where the domain is specified as a set of axioms given in dependent type theory.

Our agent is represented by a language model, which we will use to encode (a) a proof search policy \(_{}(a|s)\), (b) a value function \(V_{}(s)\), and (c) a difficulty-conditioned _conjecturer_\(P_{}(c d)\), where \(d\) is a discretized measure of difficulty and \(c\) is a mathematical statement (a string). Minimo consists of training both components in a loop that alternates between generating conjectures, trying to target hard but provable ones, and doing proof search, as we depict in Figure 1. As we describe in this section, proof search generates training data both for the conjecture and the prover components -- training on that data thus yields a self-improvement loop as the agent interacts with the environment. We now describe the first step in this loop: generating conjectures.

### Conjecturing

We aim to sample conjectures from a language model, conditioned on a target difficulty. By construction, an autoregressive LM gives a distribution over all strings. But if the LM does not have prior knowledge about the domain, it is unlikely to put non-negligible probability mass on valid mathematical statements. We now address the challenge of sampling valid conjectures.

To that end, our main insight is to leverage _constrained decoding_ to obtain valid conjectures by construction. Our method will turn any language model -- including a randomly initialized one, which is what we start with -- into a probability distribution over _strings that represent well-formed conjectures_ in dependent type theory over a given set of axioms. Ultimately, we will also train the LM to generate conjectures given a target difficulty. We use this ability to attempt to generate increasingly difficult problems for training, according to the agent's current ability to prove theorems.

To reason about constraining the LM's outputs, we leverage the abstraction of a _completion engine_, first introduced in the context of code generation with language models . Assuming \(\) is the set of all valid conjectures, a completion engine will allow us to use any LM to sample strings from \(\) in an autoregressive fashion. Mathematically, \(\) is a function \(f_{}:^{*}(^{*})\), taking a string \(s^{*}\) and returning a set of strings \(f_{}(s)\)1. Intuitively, we will sample conjectures from our LM by constraining it to strings that can be generated with iterated calls to \(f_{}\). Concretely, we will start with \(s=""\), and query \(f_{}(s)\) to obtain the set of valid ways to begin to state a conjecture. After we choose one of those \(s_{1} f_{}(s)\), we can then query \(f_{}(s_{1})\) to obtain the valid ways to proceed, and repeat until we have a complete conjecture. Our main challenge here is to construct a suitable \(f_{}\) that it is _sound_ (all conjectures obtained by this procedure are valid) and _complete_ (all valid conjectures can be obtained by this procedure). After we define \(f_{}\), we can sample from any LM while guaranteeing that the output will belong to \(\) by using the Constrained Semantic Decoding (CSD) algorithm .

To construct \(f_{}\), we analyze the minimal formulation of dependent type theory backing Peano  - essentially the classical Calculus of Constructions (CoC) . In the CoC, mathematical propositions are constructions of type prop. Since generating a proposition might involve generating objects of arbitrary other types, depending on the axioms of the given domain, we will define a more general family of completion engines, \(f_{t}\), which will constrain the LM to sample an object of an arbitrary type \(t\). At the end, we obtain \(f_{}=f_{prop}\).

To sample an object of type \(t\), we leverage the simple grammar of terms in Peano  to guide a recursive type-directed synthesis algorithm. Syntactically, terms in Peano are defined by the grammar \(e::=\) type \(\) prop \( x(e\ e)\) (lambda \(x:e,e)\) [\((x:e) e\)]. The first two production rules give the names of two built-in base types, type and prop. We then have variables, function application, lambda functions, and function types (denoted in square brackets). As conventional in type theory, let \(\) be our _typing context_: a sequence of pairs of names and their types. For example, we might have \(=[nat:type,z:nat,succ:[nat nat]]\), a context with three objects: a type nat, an object z having that type, and a function succ from nat to nat. Given a context, to obtain an object of type \(t\), it suffices to consider the formation rules in CoC to guide generation:

* If our target type is \(t=\)type, we can generate either one of the names in \(\) having type \(t=\)type (e.g., nat, for the example context above), or a function type.
* If our target type is \(t=\)prop, we can generate either one of the objects in \(\) having type prop, or a function type where the output type has type prop.
* If our target type is a function type, we must start by generating a square bracket; then, we (recursively) iteratively generate a type for the next parameter, or, if we already have at least one parameter, we can also generate the final output type.
* An object of any type \(t\) can be formed by function application of a function \(f\) chosen from \(\), provided that the output type of \(f\) can be unified with \(t\).

These rules allow us to determine the possible valid tokens at any point during generation. Besides the base types type and prop, objects of all other types are either in \(\) or the result of a function application. We use a recursive descent parser to parse the (incomplete) term we have so far (as originally done in ), and compute the target type \(t\) at the current point in generation. Then, we enumerate the possible next tokens for the LM by using the rules above, return the union of the sets of choices allowed by each rules as the output of \(f_{t}\).

Note that this is a general procedure for searching for objects of any given type \(t\) (i.e., _inhabitants_ of that type). This is undecidable in general (theorem proving is a special case of type inhabitation), so this procedure might not terminate. Therefore, we set a maximum number of tokens \(K\) (150, in our experiments), and ignore samples where the LM fails to generate a conjecture after \(K\) tokens. In practice, we find the rejection rate for generating _propositions_ to be low, \(<\) 10% of the generations.

The above procedure forces the LM to generate a _valid_ conjecture, but the LM still assigns a distribution over those. During training, we also aim to generate conjectures that are provable, but hard to prove. The signal on both success and difficulty is generated by running proof search (as we describe next) and, in cases where a proof is found, measuring it's log-likelihood under the current policy, which correlates with how many iterations MCTS takes to find the proof (see Appendix).

### Proof Search

Having a conjecture represented by a target type \(t\), we then perform proof search using Monte Carlo Tree Search (MCTS; , ), guided by a learned policy \(_{}\) and value function \(V_{}\). We represent both \(_{}\) and \(V_{}\) using the same underlying language model that we use for conjecturing. We use Peano  as the environment for proof search. Peano exposes a finite action space, so we don't need to _generate_ actions using \(_{}\) -- it suffices to be able to evaluate them. More precisely, at a given state \(s\) where we have actions \(a_{i}^{(s)}\) available, we compute the distribution \(_{}(a_{i}^{(s)}|s)\) by evaluating the likelihood of each \(a_{i}^{(s)}\) as the completion to a string of the form "STATE: csy; POLICY:". We read out the value of a state in a similar way -- by considering the likelihood of \(1\) or \(0\) as being the next token following "STATE: csy; VALUE:". In both cases, the probability of the next token is normalized over only the choices that can lead to a valid completion.

States and actions in Peano are similar to several other interactive theorem provers, such as Lean and Coq. The state consists of a set of typed objects, along with a set of open proof goals (which are types). Objects in the state whose type is a proposition type are interpreted as evidence for that proposition (either a proof or an assumption). Actions might add new objects to the state (e.g., take the successor of one existing natural number, or use an equality to rewrite an existing proposition into a new one), change the goal (by backward chaining), or both (e.g., if the goal is to prove \(A B\), which is used to represent both logical implication and universal quantification, the intro action will add object of type \(A\) to the state and change the goal to \(B\)).

When proof search succeeds, we can extract examples to train both the policy and the value function. For the policy, we simply extract the actions that lead to the proof as language modeling examples, using the same format we use to query \(_{}\). For the value function, we take the states in the subtree that complete the proof as examples with value \(1\), and a random set of other states as examples with value \(0\) for training. When proof search fails, however, this naive procedure does not extract any training examples. This happens often at the beginning, since our model initially generates a large number of conjectures it cannot prove (either because they are false, or because naive proof search times out). But forward actions in the proof search tree often construct proofs for other propositions, even if they are irrelevant for proving the original goal. We leverage this fact for generating training data by hindsight relabeling, as we describe next.

### Hindsight Relabeling

Even a conjecture that fails to be proven can be highly informative about the domain. During proof search, forward actions that apply functions whose result type is a proposition type (e.g., concluding A from (and A B)) produce proofs, even when those proofs might not be useful for proving the original conjecture. In Reinforcement Learning, the well-known method of Hindsight Experience Replay  extracts training data for the policy from such failed trajectories by relabeling the trajectories with goals that were in fact achieved, as opposed to the original goal. For those alternative goals, the trajectory then represents a successful sequence of actions. We apply this idea to extract training examples for both the policy and value functions from proof search trees, by picking nodes after forward actions that produced a proof, and walking upwards in the tree until we find a backward action (since those change the goal). That path then becomes a successful trajectory after we relabel the goal. Two important steps to improve data quality are (1) we clean up the solutions by eliminating steps irrelevant for the proof of the new goal, and (2) we only add proofs of goals never seen before, to avoid oversampling trivial facts that are rediscovered extremely often (such as \(0=0\)).

We go one step further and observe that hindsight relabeling can also be useful for training the _conjecturer_. Concretely, the procedure we described above produces a set of proofs \(p_{i}\) for relabeled statements \(g_{i}\). All of these statements are therefore true in the mathematical domain, and we use them as examples of true conjectures. As a concrete example, in arithmetic, the agent will often conjecture simple expressions such as \(2+1=0\). Most equalities generated at random will be false. However, by applying the Peano Axioms and facts about equality in succession, the agent eventually finds a proof of \(2+1=3\). Evaluating the likelihood of the proof under \(_{}\) gives a measure of the difficulty of this alternative statement for the current policy. This insight allows our agent to learn about hundreds of new unique, true statements in each proof search. As our experiments show, we find hindsight relabeling to be crucial for enabling the agent to steadily conjecture harder statements that it is able to prove.

### Training loop

Finally, we tie all components together by alternating between (a) generating a batch of conjectures, (b) running proof search on them, (c) extracting training examples from the search trees, and finally (d) training the underlying LM using the standard cross-entropy objective. When a proof is found, either directly or by hindsight relabeling, we first compute a continuous difficulty metric of the statement by taking the log-likelihood of the proof under the current policy. To discretize difficulties, we then consider difficulty percentiles of the last batch of conjectures: we take the 10% least likely proofs to be associated with "hard" conjectures, the 50% most likely to be considered "trivial", and the remaining are taken as "easy". When put together, these components form a self-improving loop that starts only from the axioms of the given mathematical domain, as our experiments show.

## 4 Experiments

We now evaluate Minimo2 on three mathematical domains. We experiment with axiomatic systems for (a) propositional logic, (b) arithmetic, and (c) abstract groups. All the axioms are given in the Appendix. We then train agents over 5 iterations of conjecturing and theorem proving, generating 200 conjectures in each batch, running MCTS for proof search with 1000 expansions, and evaluate our agents guided by the following research questions:

**RQ1:**: Can our conjecturing method generate increasingly harder conjectures as training progresses?
**RQ2:**: Do agents improve at theorem proving as they train on their own generated problems?
**RQ3:**: Is hindsight relabeling effective at improving conjecturing and theorem proving?
**RQ4:**: Do our agents improve at proving an externally given set of human-selected theorems, even if these are not given during training?

The first three questions require _intrinsic evaluations_ -- they ask whether the learning dynamics of agents trained with Minimo matches the desiderata of self-improving at both conjecturing and theorem proving while given only the axioms. The last question is an extrinsic assessment of what our agents learn -- we evaluate whether the learning progresses in a way that aligns with an external criteria -- namely, the proficiency of the agent at proving theorems from human sources (a textbook and a Lean game).

### Learning dynamics

We first investigate RQ1 and RQ2. Figure 2 shows the progression of difficulty across 5 iterations of the Minimo learning loop (first conjecturing, then running proof search, and training on collected

Figure 2: Difficulty of proved conjectures found in each iteration of Minimo, evaluate as the log-probability of the proof under the policy checkpoints after each iteration of the training loop (with standard error bands for runs with 3 random seeds).

examples). Here, we evaluate the average log-likelihood (y-axis) of conjectures proven at each conjecturing iteration (x-axis) under the policy after each iteration (line color). Policy 0 is the initial (randomly initialized) policy, while subsequent policies were trained on the examples obtained during proof search, with hindsight relabeling, up to the previous iteration. Lower log-likelihood generally means harder conjectures (i.e., they tend to take more search iterations, see Appendix).

Difficulty increases as training progresses (RQ1).Fixing an early policy, the log-likelihood of proofs under that policy steadily decreases across training iterations. This is reflected in the negative slope of difficulty across iterations when the policy is fixed. In particular, the policy at iteration 0 serves as a naive search baseline, since it is essentially uniform. We observe that, as training progresses, this policy struggles increasingly more with each new batch of conjectures. The same pattern repeats for subsequent policies when we consider conjectures generated in future iterations, giving a generally downward trend in log-likelihood of the solutions for any given policy, showing that conjectures get increasingly more challenging. This provides positive evidence for answering RQ1: in all 3 domains, the conjecture is able to increasingly propose harder provable conjectures as training progresses.

Proof policy improves as training progresses (RQ2).At each iteration, we sample a new set of unseen conjectures for training. From Figure 2, we see that later policies generally perform better than earlier ones, at a fixed conjecture iteration. This reflects the fact that each new policy assigns higher likelihood to the proofs, even for unseen conjectures at each iteration. For example, after training on the data generated from the first iteration, the policy on iteration 1 has higher log-likelihood for the proofs to the _new conjectures_ found at iteration 1, when compared to the initial policy from iteration 0. This pattern repeats at each iteration, showing that the prover is also progressing and generalizing to unseen problems, though with diminishing gains in the final iterations. This suggests a positive answer to our second research question: our agents effectively self-improve in their ability to prove new statements.

Hindsight relabeling is necessary for joint self-improvement (RQ3).The results so far all used hindsight relabeling on all proof searches--successful or not--to extract training data for the policy and value function, as well as conjecturing. To evaluate whether our agents would still show the same continuous self-improvement regardless of the data efficiency gains from hindsight relabeling, Figure 3 compares agents trained with and without hindsight relabeling across 5 iterations over the same 3 axiomatic domains. Here, we look at the ability of the agent to achieve the goal of constantly proposing provable but challenging conjectures for itself. Ideally, the difficulty of conjectures should not fluctuate significantly during the course of training: we would like the agent to always find new challenging conjectures that it nevertheless still proves. We find that, in all 3 domains, the agent fails to achieve this goal when not trained with hindsight relabeling. Instead, as it trains on its own proofs, the agent's conjectures fail to remain challenging--all provable conjectures end up with high log-likelihood as training progresses, and the conjecture is unable to leave that regime. We attribute this to the volume of signal that the conjecture receives: at each initial batch, only around 10-20% of the conjectures are proven. When not using hindsight relabeling, the only feedback that the conjecturer receives is that proof search timed out on these statements. On the other hand, with hindsight relabeling, even these failures lead the conjecturer to observe hundreds of actual true statements in each domain (along with their proofs), leading to better learning dynamics. This provides positive evidence for RQ3: hindsight relabeling significantly helps the agent to jointly improve in conjecturing and theorem proving--without it, training tends to collapse to by proposing only easy conjectures.

### Proving human-written theorems (RQ4)

Finally, we evaluate whether our agent, trained only on problems that it proposes to itself, also improves in solving problems that are of interest to humans. Since our agent does not grow its library of theorems over time, starting every new proof from the axioms, a meaningful evaluation requires theorems that can be reasonably proven straight from the axioms, without lemmas. We thus use two human-written sources of theorems meeting this criterion, for the domains of propositional logic and arithmetic. For logic, we take the set of 35 theorems of propositional logic from Stephen Kleene's textbook "Introduction to Metamathematics" . Precisely, in Theorem 41, Kleene states (and proves a subset of) 35 useful statements of Propositional Logic (such as contraposition rules, commutativity and transitivity laws of logical connectives, and properties of double negation). For arithmetic, we use the Natural Number Game , a popular game used to introduce formal mathematics in the Lean theorem prover. We take levels of the game that are (a) theorems about natural numbers, and (b) do not refer to previous lemmas, only the axioms; this results in 10 levels spanning the Tutorial, Addition, and Multiplication worlds. We translate the statements into Peano, and evaluate our agents on their success rate on those problems within 2000 MCTS expansions.

Figure 4 shows the results. We find that, as our agents train on their self-generated problems, they steadily become more successful at proving theorems from both Kleene's book and the Natural Number Game. This happens _even though these theorems are not targeted_ during training, since our agent only uses its own conjectures. Four theorems in Propositional Logic are only proved after the last iteration of training: commutativity and transitivity of the "if and only if" logical connector, a law connecting double negation and implication (\((A B), A B\)), and the "currying law" of the conjunction -- \((A B) C A(B C)\). In the Natural Number game, only the final agent proves \( n, m,succ(n)+m=succ(n+m)\), a theorem requiring induction on the correct variable (m) and a non-trivial sequence of rewrites in the inductive case (we include the full proof in the Appendix). While admitedly small scale, these results suggest a positive answer to our last research question: agents trained on their own conjectures can also improve at solving human-written problems, which are not given during training.

## 5 Limitations and Conclusion

We present Minimo: an approach to training agents for formal mathematical reasoning starting from only the axioms of a given domain. The agent jointly learns to propose challenging conjectures and to prove them. Our experiments show evidence of Minimo improving its performance across training iterations. In the Groups domain, the average proof length it finds on _generated_ conjectures (i.e.,

Figure 4: Success rate of our agents at proving theorems from the “Introduction to Metamathematics” textbook and the Natural Number Game. As agents train on their own conjectures, they also improve at solving problems from these two human-written sources.

Figure 3: Difficulty of proved conjectures proposed in each iteration under the current policy at that same iteration, comparing when using and not using hindsight relabeling to generate new proofs and conjectures, with standard error bands for runs with 3 random seeds. Ideal behavior would be a flat line, representing constant relative difficulty. Hindsight significantly helps the agent conjecture propose harder problems.

not found by hindsight) increased from 2.67 steps in the first iteration, when the model is randomly initialized, to 5 steps by iteration 4, with proofs for 'hard' conjectures growing from 3.67 to 6.10 steps. The longest proofs found grew from 4 steps to 9 steps from the first to last iteration. Similar trends appear in Propositional Logic (average length from 2.75 to 4.21 steps, longest proofs from 5 to 11 steps) and Arithmetic (average from 2.36 to 3.35 steps, longest from 4 to 7 steps).

However, Minimo currently has two crucial limitations that prevent it from (a) discovering deep mathematical theories, and (b) scaling up to large theories. First, even if the agent's policy improves, its library remains fixed to the definitions and axioms that it starts with. Proofs that do not use lemmas (_cut-free_ proofs in logic) can be exponentially longer than equivalent ones that do, and thus quickly grow beyond the reach of search. With this constraint, our agent most often finds harder conjectures by making the statements longer and more complicated. For example, in Groups, early conjectures include trivial statements like \(e=e\); by the last iteration, the conjecture requiring the longest proof reads as \( g G,e=(g^{-1})^{2}\) then \(e^{2}=e(e(e((g^{-1})^{2})))\) (proved in 9 steps). In contrast, human mathematicians develop deep theories by accumulating results and definitions along the way, in such a way that even very high-level results can be described succinctly at the right level of abstraction. Understanding how to bootstrap such cumulative learning in an agent (e.g., exploring notions of _usefulness_ or _interestingness_, several of which have been posited ) will be a key direction for future work.

Another bottleneck in our current setup is that a large library can cause the current action enumeration algorithm in Peano to become prohibitively slow (a finite action space can still become intractable). A method that scales unboundedly should incorporate some form of _premise selection_. In past work, premise selection has either been based on symbolic heuristics or in learning useful premises from human-written proofs. We believe that developing a premise selection method that _bootstraps_ together with the other learned components will be as important as understanding how to grow the agent's library.

Together, lifting these limitations from our method might lead to a completely compute-bound, self-improving agent for formal mathematics capable of discovering deep mathematical theories starting only from basic axioms -- the rules of the game of mathematics.

#### Acknowledgments

We thank Daniel Selsam and Laetitia Teodorescu for insightful discussions on preliminary ideas related to this work, as well as the anonymous reviewers for their helpful feedback. This work was supported by the NSF Expeditions Grant with Award Number (FAIN) 1918771, and by NSF grant #2302701. GP was also supported by the Stanford Interdisciplinary Graduate Fellowship (SIGF). This work was also partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.