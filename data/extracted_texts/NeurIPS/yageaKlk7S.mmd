# Understanding the Detrimental Class-level

Effects of Data Augmentation

 Polina Kirichenko\({}^{1,2}\) Mark Ibrahim\({}^{2}\) Randall Balestriero\({}^{2}\) Diane Bouchacourt\({}^{2}\)

Ramakrishna Vedantam\({}^{2}\) Hamed Firooz\({}^{2}\) Andrew Gordon Wilson\({}^{1}\)

\({}^{1}\)New York University \({}^{2}\)Meta AI

###### Abstract

Data augmentation (DA) encodes invariance and provides implicit regularization critical to a model's performance in image classification tasks. However, while DA improves average accuracy, recent studies have shown that its impact can be highly class dependent: achieving optimal average accuracy comes at the cost of significantly hurting individual class accuracy by as much as \(20\%\) on ImageNet. There has been little progress in resolving class-level accuracy drops due to a limited understanding of these effects. In this work, we present a framework for understanding how DA interacts with class-level learning dynamics. Using higher-quality multi-label annotations on ImageNet, we systematically categorize the affected classes and find that the majority are inherently ambiguous, co-occur, or involve fine-grained distinctions, while DA controls the model's bias towards one of the closely related classes. While many of the previously reported performance drops are explained by multi-label annotations, our analysis of class confusions reveals other sources of accuracy degradation. We show that simple class-conditional augmentation strategies informed by our framework improve performance on the negatively affected classes.

## 1 Introduction

Data augmentation (DA) provides numerous benefits for training of deep neural networks including promoting invariance and providing regularization. In particular, DA significantly improves the generalization performance in image classification problems when measured by average accuracy . However, Balestriero et al.  and Bouchacourt et al.  showed that strong DA, in particular, Random Resized Crop (RRC) used in training of most modern computer vision models, may disproportionately hurt accuracies on some classes, e.g. with up to \(20\%\) class-level degradation on ImageNet compared to milder augmentation settings (see Figure 1 left). Performance degradation even on a small set of classes might result in poor generalization on downstream tasks related to the affected classes , while in other applications it would be unethical to sacrifice accuracy on some classes for improvements in average accuracy .

Balestriero et al.  attempted to address class-level performance degradation by applying DA selectively to the classes where the accuracy improves with DA strength. Surprisingly, they found that this augmentation policy did not address the issue and the performance on non-augmented classes still degraded with augmentation strength. In this work we perform detailed analysis and explore the mechanisms causing the class-level performance degradation. In particular, we identify the _interactions between class-conditional data distributions_ as the cause of the class-level performance degradation with augmentation: DA creates an overlap between the data distributions associated with different classes. As a simple illustrative example, in Figure 1 (right) we show that the standard RRC operation creates an overlap between the "car" and "wheel" classes. As a result, the model learns to predict label "car" on "wheel" images, and the performance on the "wheel" class drops. Importantly,if we want to improve the performance on the "wheel" class, we need to modify the augmentation policy on the class "car" and not "wheel" as was done in prior work . We summarize our findings in Figure 1. In particular, our contributions are the following:

* We refine the analysis of class-level effects of data augmentations by correcting for label ambiguity. Specifically, we use multi-label annotations on ImageNet  and measure the effects of data augmentation for each class in terms of the original and multi-label accuracy. Through this analysis, we find that class-level performance degradation reported in Balestriero et al.  and Bouchacourt et al.  is overestimated (Section 4).
* We systematically categorize the class confusions exacerbated by strong augmentation and find that many affected classes are ambiguous or co-occurring and are often affected by label noise (Figure 1 middle and Section 5). We focus on addressing the remaining fine-grained and non-trivial class confusions.
* We show that for addressing DA biases it is important to consider the classes with an increasing number of _false positive mistakes_, and not only the classes negatively affected in accuracy. By taking into account our observations on DA affecting class interactions, we propose a simple class-conditional data augmentation strategy that leads to improvement on the affected group of classes by \(2.5\%\) on ImageNet (Section 6). This improvement is in contrast to the previously explored class-conditional DA in Balestriero et al.  which failed to improve class-level accuracy.
* We confirm our findings across multiple computer vision architectures including ResNet-50 , EfficientNet  and ViT  (Section F), multiple data augmentation transformations including Random Resized Crop, mixup , RandAugment  and colojitter (Section G), as well as two additional datasets besides ImageNet (Section G).

Figure 1: **We show that the classes negatively affected by data augmentation are often ambiguous, co-occurring or fine-grained categories and analyze how data augmentation exacerbates class confusions. Left: Average accuracy of ResNet-50 on ImageNet against Random Resized Crop (RRC) data augmentation strength: average of all classes (blue), average of the \(50\) classes on which stronger RRC hurts accuracy the most (red), and the average of the remaining \(950\) classes (green). Yellow line indicates the default RRC setting used in training of most computer vision models. Middle: We systematically categorize the types of class confusions exacerbated by strong data augmentation: while some of them include ambiguous or correlated classes, there is a number of fine-grained and non-trivial confusions. Right: Often the class-level accuracy drops due to overlap with other classes after applying augmentation: e.g. heavily augmented samples from “car” class can look like typical images from “wheel” class. As a result, the model learns to predict “car” on “wheel” images, and the accuracy on the “wheel” class drops. To resolve the negative effect of strong augmentation on classes like “wheel”, we should modify augmentation strength of classes like “car”.**

Related work

Understanding data augmentation, invariance and regularization.Hernandez-Garcia and Konig  analyzed the DA from the perspective of implicit regularization. Botev et al.  propose an explicit regularizer that encourages invariance and show that it leads to improved generalization. Balestriero et al.  derive an explicit regularizer to simulate DA to quantify its benefits and limitations and estimate the number of samples for learning invariance. Gontijo-Lopes et al.  and Geiping et al.  study the mechanisms behind the effectiveness of DA, which include data diversity, exchange rates between real and augmented data, additional stochasticity and distribution shift. Bouchacourt et al.  measure the learned invariances using DA. Lin et al.  studied how data augmentation induces implicit spectral regularization which improves generalization.

Biases of data augmentations.While DA is commonly applied to improve generalization and robustness, a number of prior works identified its potential negative effects. Hermann et al.  showed that decreasing minimum crop size in Random Resized Crops leads to increased texture bias. Shah et al.  showed that using standard DA amplifies model's reliance on spurious features compared to models trained without augmentations. Idrissi et al.  provided a thorough analysis on how the strength of DA for different transformations has a disparate effect on subgroups of data corresponding to different factors of variation. Kapoor et al.  suggested that DA can cause models to misinterpret uncertainty. Izmailov et al.  showed that DA can hurt the quality of learned features on some classification tasks with spurious correlations. Balestriero et al.  and Bouchacourt et al.  showed that strong DA may disproportionately hurt accuracies on some classes on ImageNet, and in this work we focus on understanding this class-level performance degradation through the lens of interactions between classes.

Multi-label annotations on ImageNet.A number of prior works identified that ImageNet dataset contains label noise such as ambiguous classes, multi-object images and mislabeled examples . Tsipras et al.  found that nearly \(20\%\) of ImageNet validation set images contain objects from multiple classes. Hooker et al.  ran a human study and showed that examples most affected by pruning a neural network are often mislabeled, multi-object or fine-grained. Yun et al.  generate pixel-level multi-label annotations for ImageNet train set using a large-scale computer vision model. Beyer et al.  provide re-assessed (ReaL) multi-label annotations for ImageNet validation set which aim to resolve label noise issues, and we use ReaL labels in our analysis to refine the understanding of per-class effects of DA.

Adaptive and learnable data augmentation.Xu et al.  showed that data augmentation may exacerbate data bias which may lead to model' suboptimal performance on the original data distribution. They propose to train the model on a mix of augmented and unaugmented samples and then fine-tune it on unaugmented data after training which showed improved performance on CIFAR dataset. Raghunathan et al.  showed standard error in linear regression could increase when training with original data and data augmentation, even when data augmentation is label-preserving. Rey-Area et al.  and Ratner et al.  learn DA transformation using GAN framework, while Hu and Li  study the bias of GAN-learned data augmentation. Fujii et al.  take into account the distances between classes to adapt mixed-sample DA. Hauberg et al.  learn class-specific DA on MNIST. Numerous works, e.g. Cubuk et al. , Lim et al. , Ho et al. , Hataya et al. , Li et al. , Cubuk et al. , Tang et al. , Muller and Hutter  and Zheng et al.  find dataset-dependent augmentation strategies. Benton et al.  proposed Augerino framework to learn augmentation form training data. Zhou et al. , Cheung and Yeung , Mahan et al.  and Miao et al.  learn class- or input-dependent augmentation policies. Yao et al.  propose to modify mixed-sample augmentation to improve out-of-domain generalization.

Robustness and model evaluation beyond average accuracy.While Miller et al.  showed that model's average accuracy is strongly correlated with its out-of-distribution performance, there is a number of works that showed that only evaluating average performance can be deceptive. Teney et al.  showed counter-examples for "accuracy-on-the-line" phenomenon. Kaplan et al.  show that while model's average accuracy improves during training, it may decrease on a subset of examples. Sagawa et al.  show that training with Empirical Risk Minimization may lead to suboptimal performance in the worst case. Bitterwolf et al.  evaluated ImageNet models' performance in terms of a number of metrics beyond average accuracy, including worst-class accuracy and precision. Richards et al.  demonstrate that improved average accuracy on ImageNet and standard out-of-distribution ImageNet variants may lead to exacerbated geographical disparities.

Evaluation setup and notation

Since we aim to understand class-level accuracy degradation emerging with strong data augmentation reported in Balestriero et al. , we closely follow their experimental setup. We focus on ResNet-50 models  trained on ImageNet  and study how average and class-level performance changes depending on the Random Resized Crop augmentation strength: it is by far the most widely adopted augmentation which leads to significant average accuracy improvements and is used for training the state-of-the-art computer vision models [58; 65; 38]. We train ResNet-50 for \(88\) epochs using label smoothing with \(=0.1\). We use image resolution \(R_{train}=176\) during training and evaluate on images with resolution \(R_{test}=224\) following Balestirero et al. , Touvron et al.  and torchvision training recipe1. More implementation details can be found in Appendix A.

**Data augmentation.** We apply random horizontal flips and Random Resized Crop (RRC) DA when training our models. In particular, for an input image of size \(h w\) the RRC transformation samples the crop scale \(s U[s_{low},s_{up}]\) and the aspect ratio \(r U[t_{low},r_{up}]\), where \(U[a,b]\) denotes a uniform distribution between \(a\) and \(b\). RRC then takes a random crop of size \(\) and resizes it to a chosen resolution \(R R\). We use the standard values for \(s_{up}=100\%\) and aspect ratios \(r_{low}=3/4,r_{up}=4/3\), and vary the lower bound of the crop scale \(s_{low}\) (for simplicity, we will further use \(s\)) between \(8\%\) and \(100\%\) which controls _the strength of augmentation_: \(s=8\%\) corresponds to the strongest augmentation (note this is the default value in pytorch RRC implementation) and \(s=100\%\) corresponds no cropping hence no augmentation. For each augmentation strength, we train \(10\) models with different random seeds.

**ReaL labels.**  Beyer et al.  used large-scale vision models to generate new label proposals for ImageNet validation set which were then evaluated by human annotators. These Reassessed Labels (ReaL) correct the label noise present in the original labels including mislabeled examples, multi-object images and ambiguous classes. Since there are possibly multiple ReaL labels for each image, model's prediction is considered correct if it matches one of the plausible labels. Further we will use \(l_{ReaL}(x)\) to denote the set of ReaL labels of example \(x\).

**Evaluation metrics.** We aim to measure performance of model \(f_{s}(x)\) as a function of DA strength, i.e. RRC lower bound of the crop scale \(s\). We measure average accuracy \(a(s)\), and per-class accuracy \(a_{k}(s)\) with respect to both original ImageNet labels and ReaL multi-label annotations given by:

\[a_{k}^{or}(s)=1/|X_{k}|_{x X_{k}}I[f_{s}(x)=k] a_{ k}^{ReaL}(s)=1/|X_{k}|_{x X_{k}}I[f_{s}(x) l_{ReaL}(x)],\]

where \(X_{k}\) are images from class \(k\) in validation set. We will refer to \(a^{or}\) and \(a^{ReaL}\) as _original accuracy_ and _ReaL accuracy_, respectively. To quantify the class accuracy drops, Balestirero et al.  compare the per-class accuracy of models trained with the strongest DA (\(s=8\%\)) and models trained without augmentation (\(s=100\%\) which effectively just resizes input images without cropping), while Bouchacourt et al.  compared class-level accuracy of models trained with RRC with \(s=8\%\) and models trained with fixed size Center Crop. In our analysis, we evaluate per-class accuracy drops comparing the maximum accuracy attained on a particular class \(k\) across all augmentation levels \(_{s}a_{k}(s)\) and accuracy on that class when training with strongest DA \(a_{k}(s=8\%)\). We will refer to the classes with the highest accuracy degradation computed by:

\[ a_{k}=_{s}a_{k}(s)-a_{k}(s=8\%)\]

as the classes _most negatively affected_ by DA (with respect to either original labels or multi-label ReaL annotations). To summarize performance on the affected classes, we will evaluate average accuracy of classes with the highest \( a_{k}\) (in many cases focusing on \(5\%\) of ImageNet classes with the highest accuracy drop following Balestirero et al. ). In the following sections, we also highlight the importance of measuring other metrics beyond average and per-class accuracy which comprise a more thorough evaluation of DA biases.

While we focus on the analysis of ResNet-50 on ImageNet with RRC augmentation as the main setup following prior work [1; 9], we additionally confirm our observations on other architectures (EfficientNet  and ViT ) in Appendix F, other data augmentation transformations (RandAugment , colorjitter and mixup ), and other datasets (CIFAR-100 and Flowers102 ) in Appendix G.

Per-class accuracy degradation with strong data augmentation is overestimated due to label ambiguity

Previous studies reported that the performance of ImageNet models is effectively better when evaluated using re-assessed multi-label annotations which address label noise issues in ImageNet [4; 57; 69]. These works showed that recent performance improvements on ImageNet might be saturating, but the effects of this label noise on granular per-class performance has not been previously studied. In particular, it is unclear how correcting for label ambiguity would affect the results of Balestriero et al.  and Bouchacourt et al.  on the effects of DA on class-level performance.

We observe that **for many classes with severe drops in accuracy on original labels, the class-level Real multi-label accuracy is considerably less affected.** The right panel of Figure 2 shows the distributions of per-class accuracy drops \( a_{k}^{or}\) and \( a_{k}^{ReaL}\), and we note that the distribution of \( a_{k}^{or}\) has a much heavier tail. Using multi-label accuracy in evaluation reveals there are much fewer classes which have severe effective performance drop: e.g. only \(37\) classes with \( a_{k}^{ReaL}>4\%\) as opposed to \(83\) classes with \( a_{k}^{or}>4\%\), moreover, there are no classes with \( a_{k}^{ReaL}>11\%\).

On the left panel of Figure 2, we show how multi-label accuracy evaluation impacts the average and individual class performance across different augmentation strengths \(s\). In particular, in the top row plots we see that while the average accuracy of all classes follows a similar trend when evaluated with either original or ReaL labels, the average accuracy of \(50\) classes most negatively affected in original accuracy only decreases by \(1\%\) with ReaL labels as opposed to more significant \(5\%\) drop with original labels. The bottom row shows the accuracy for "barn spider", "overskirt" and "academic gown" classes which have the highest \( a_{k}^{or}\), and accuracy trends for all \(50\) most negatively affected classes are shown in Appendix C. For many of these classes which are hurt in original accuracy by using stronger DA, the ReaL accuracy is much less affected. For example, for the class "barn spider" the original accuracy is decreased from \(63\%\) to \(47\%\) if we use the model trained with RRC \(s=8\%\) compared to \(s=70\%\), while the highest ReaL accuracy is achieved by the strongest augmentation setting on this class.

However, there are still classes for which the ReaL accuracy degrades with stronger augmentation, and in Appendix C we show ReaL accuracy trends against augmentation strength \(s\) for \(50\) classes with the highest \( a^{ReaL}\). While some of them (especially classes from the "animal" categories)

Figure 2: **We find that for many classes the negative effects of strong data augmentation are muted if we use high-quality multi-label annotations. Left: Average and per-class accuracy of ResNet-50 trained on ImageNet evaluated with original and ReaL labels as a function of Random Resized Crop augmentation strength (\(s=8\%\) corresponds to the strongest and default augmentation). The top row shows the average accuracy of all ImageNet classes, the 50 classes with the highest original accuracy degradation and the remaining 950 classes. The bottom row shows the accuracy of 3 individual classes most significantly affected in original accuracy when using strong augmentation. Right: Distribution of per-class accuracy drops \( a_{k}\) for original and ReaL labels. The distribution of \( a_{k}^{or}\) has a heavier tail compared to the one computed with ReaL labels.**

may still be affected by the remaining label noise [69; 68; 57; 39; 4], for other classes the strongest DA leads to suboptimal accuracy. In the next section, we aim to understand why strong DA hurts the performance on these classes. We analyze model's predictions and consistent confusions on these classes and find that the degradation in performance is caused by the interactions between class-conditional distributions induced by DA.

Data augmentation most significantly affects classification of ambiguous, co-occurring and fine-grained categories

In this section, we aim to understand the reasons behind per-class accuracy degradation when using stronger data augmentation by analyzing the most common mistakes the models make on the affected classes and how they evolve as we vary the data augmentation strength. We consider the classes most affected by strong DA (see Figures in Appendix C) which do not belong to the "animal" subtree category in the WordNet hierarchy  since fine-grained animal classes were reported to have higher label noise in previous studies [68; 57; 39; 4]. We focus on the \(50\) classes with the highest \( a_{k}^{or}\) (corresponding to \( a_{k}^{or}>5\%\)), and \(50\) classes with the highest \( a_{k}^{ReaL}\) (corresponding to \( a_{k}^{ReaL}>4\%\)). For a pair of classes \(k\) and \(l\) we define the confusion rate (CR) as:

\[CR_{k l}(s)=1/|X_{k}|_{x X_{k}}I[f_{s}(x)=l],\]

i.e. the ratio of examples from class \(k\) misclassified as \(l\). For each affected class, we identify most common confusions and track the CR against the RRC crop scale lower bound \(s\). We also analyze the reverse confusion rate \(CR_{l k}(s)\).

We observe that in many cases DA strength controls the model's preference in predicting one or another plausible ReaL label, or preference among semantically similar classes. We roughly outline the most common types of confusions on the classes which are significantly affected by DA. The different types of confusion differ in the extent to which the accuracy degradation can be attributed to label noise versus the presence of DA. We also characterize how DA effectively changes the data distribution of these classes leading to changes in performance. These categories are closely related to common mistake types on ImageNet identified by Beyer et al.  and Vasudevan et al. , but we focus on class-level interactions as opposed to instance-level mistakes and particularly connect them to the impact of DA. We use _semantic similarity_ and _ReaL labels co-occurence_ as a criteria to identify a confusion category for a pair of classes. We can measure semantic similarity by (a) WordNet class similarity, given by the Wu-Palmer score which relies on the most specific common ancestor of the class pair in the WordNet tree, and (b) similarity of the class name embeddings2. To estimate the intrinsic distribution overlap of the classes, we compute Real labels co-occurrence between classes \(k\) and \(l\) as:

\[C_{kl}=_{x X}I[k l_{ReaL}(x)] I[l l_{ReaL}(x)]/_{x X }I[k l_{ReaL}(x)],\]

which is the ratio of examples that have both labels \(k\) and \(l\) among the examples with the label \(k\). Using these metrics, depending on a higher or lower semantic similarity and higher or lower ReaL labels overlap, we categorize confused class pairs as _ambiguous_, _co-occurring_, _fine-grained_ or _semantically unrelated_. Below we discuss each category in detail, and the examples are shown in Figure 3 and Appendix Figure 6. We provide more details on computing the metrics for identifying the confusion type and categorize the confusions of all affected classes in Appendix D.

**Class-conditional distributions induced by DA.** To aid our understanding of the class-specific effects of DA, it is helpful to reason about how a parametrized class of DA transformations \(_{s}()\) changes the distributions of each class in the training data \(p_{k}(x)\). We denote the augmented class distributions by \(_{s}(p_{k})\). In particular, if supports of the distributions \(_{s}(p_{k})\) and \(_{s}(p_{l})\) for two classes \(k\) and \(l\) overlap, the model is trained to predict different labels \(k\) and \(l\) on similar inputs corresponding to features from both classes \(k\) and \(l\) which will lead to performance degradation. Some class distributions \(p_{k}\) and \(p_{l}\) are intrinsically almost coinciding or highly overlapping in the ImageNet dataset, while others have distinct supports, but in all cases the parameters of DA \(s\) will control the overlap of the induced class distributions \(_{s}(p_{k})\) and \(_{s}(p_{l})\), and thus the biases of the model when making predictions on such classes.

**Intrinsically ambiguous or semantically (almost) identical classes.** Prior works [e.g. 4, 57, 69, 67] identified that some pairs of ImageNet classes are practically indistinguishable, e.g. "sunglasses" and "sunglasss", "monitor" and "screen", "maillot" and "maillot, tank suit". These pairs of classes generally have higher semantic similarity and higher ReaL labels overlap \(C_{kl}\). We observe that in many cases the accuracy on one class within the ambiguous pair degrades with stronger augmentations, while the accuracy on another class improves. The supports of distributions of these class pairs \(p_{k}\) and \(p_{l}\) highly overlap or even coincide, but with varying \(s\) depending on how the supports of \(_{s}(p_{k})\) and \(_{s}(p_{l})\) overlap the model would be biased towards predicting one of the classes. In Figure 3 top left panel, we show how the frequencies of most commonly predicted labels change on an ambiguous pair of classes "sunglasss" and "sunglasses" as we vary the crop scale parameter. The Real labels for these classes overlap with \(C_{kl}=87\%\), and the majority of confusions are resolved after accounting for multi-label annotations. We note that for images from both classes the frequency of "sunglasses" label increases with stronger DA while "sunglass" predictions have the opposite trend. Models trained on ImageNet often achieve a better-than-random-guess accuracy when classifying between these classes due to overfitting to marginal statistical differences and idiosyncrasies of the data labeling pipelines. While DA strength controls model's bias towards predicting one or another plausible label, the models are not effectively making mistakes when confusing such classes.

For the remaining categories described below, the class distributions become more overlapping when strong DA is applied during training, and data augmentation amplifies or causes problematic misclassification.

**Co-occurring or overlapping classes.** There is a number of classes in ImageNet which correspond to semantically different objects which often appear together, e.g. "academic gown" and "mortarboard", "Windsor tie" and "suit", "assault rifle" and "military uniform", "seashore" and "sandbar". These pairs of classes have rather high overlap in ReaL labels depending on the spurious correlation strength, and their semantic similarity can vary, but generally it would be lower than for ambiguous classes. The class distributions of co-occurring classes inherently overlap, however, stronger DA may increase the overlap in class distribution supports. For example, with RRC we may augment the

Figure 3: **Types of class confusions affected by data augmentation with varied semantic similarity and data distribution overlap.** Each panel shows a pair of confused classes which we categorize into: _ambiguous, co-occurring_, _fine-grained_ and _semantically unrelated_, depending on the inherent class overlap and semantic similarity. For each confused class pair, the left subplot corresponds to the class \(k\) whose accuracy decreases with strong data augmentation (DA), e.g. “sunglasss” on top left panel: the ratio of validation samples from that class which are classified correctly decreases with stronger DA, while the confusion rate with another class \(l\) (e.g. class “sunglasses” on top left panel) increases. The right subplot shows the percent of examples from class \(l\) that get classified as \(k\) or \(l\) against DA strength.

sample such that only the spuriously co-occurring object, but not the main object, is left in the image, and the model would still be trained to predict the original label: e.g. we can crop just the mortarboard in an image labeled as "academic gown". It was previously shown that RRC can increase model's reliance on spurious correlations  which can lead to meaningful mistakes, not explained by label ambiguity. In Figure 3 top right panel, we show how DA strength impacts model's bias towards predicting "sandbar" or "seashoe" class (these classes co-occur with \(C_{kl}=64\%\) and the majority of confusions are resolved by accounting for Real labels). We emphasize that unlike the ambiguous classes, the co-occuring classes cause meaningful mistakes on the test data, which are not always resolved by multi-label annotations. For example, the model will be biased to predict "academic gown" even when shown an image of just the mortarboard.

**Fine-grained categories.** There is a number of semantically related class pairs like "tobacco shop" and "barbershop", "frying pan" and "wok", "violin" and "cello", where objects appear in related contexts, share some visually similar features and generally represent fine-grained categories of a similar object type. These classes have high semantic similarity and do not have a significant ReaL label overlap (sometimes such examples are affected by mislabeling but such classes generally do not co-occur). The class distributions for such categories are close to each other or slightly overlapping, but strong DA pulls them closer, and \((p_{i})\) and \((p_{k})\) would be more overlapping due to e.g. RRC resulting in the crops of the visually similar features or shared contexts in the augmented images from different categories. In Figure 3 bottom left panel, we show how model's confusion rates change depending on RRC crop scale for fine-grained classes "frying pan" and "wok" (these classes rarely overlap with only \(C_{kl}=9\%\)).

**Semantically unrelated.** In the rare but most problematic cases, the stronger DA will result in confusion of semantically unrelated classes. While they share similar low-level visual features, they are semantically different, their distributions \(p_{k}\) and \(p_{l}\) and ReaL labels do not overlap, and they get confused with one another specifically because of strong DA), for example, categories like "muzzle" and "sandal", "bath towel" and "pillow". Figure 3 bottom right panel shows how confusions between unrelated classes "bath towel" and "pillow" emerge with stronger DA.

In Appendix D we show a larger selection of example pairs from each confusion category. Among the confusions on the most significantly affected classes, approximately \(53\%\) are fine-grained, \(21\%\) are co-occurring, \(16\%\) are ambiguous and the remaining \(10\%\) are semantically unrelated. While the confusion of semantically unrelated categories is the most rare, it is potentially most concerning since it corresponds to more severe mistakes.

While in some cases we can intuitively explain why the model becomes biased to predict one of the two closely related classes with stronger DA (such as with "car" and "wheel" classes in Figure 1), in other cases the bias arises due to statistical differences in the training data such as data imbalance. ImageNet train set is not perfectly balanced with \(104\) classes out of \(1000\) containing less training examples than the remaining classes. We observed that \(21\%\) of these less represented classes were among the ones most significantly affected in original or ReaL per-class accuracy. Since DA can push class-conditional distributions of related classes closer together, if one of such classes is less represented in training data it may be more likely to be impacted by stronger augmentation. In Appendix C we show how average accuracy on the underrepresented classes changes depending on the data augmentation strength.

We observe that using other data augmentation transformations such as RandAugment  or colorjitter on ImageNet, or mixup  on CIFAR-100 results in similar effects of exacerbated confusions of related categories (see Appendix section G). In particular, RandAugment and colorjitter often affect class pairs which are distinct in their color or texture, while mixup on CIFAR-100 may amplify confusions of classes within the same superclass.

## 6 Class-conditional data augmentation policy interventions

In Section 5, we showed that class-level performance degradation occurs with stronger augmentation because of the interactions between different classes. The models tend to consistently misclassify images of one class to another related, e.g. co-occurring, class. In this section, we use these insights to develop a class-conditional augmentation policy and improve the performance on the classes negatively affected by DA.

Balestitero et al.  identified that DA leads to degradation in accuracy for some classes and also showed that a naive class-conditional augmentation approach is not sufficient for removing these negative effects. In particular, using oracle knowledge of validation accuracies of models pretrained with different augmentation levels, they evaluated a DA strategy where augmentation is applied to all classes except the ones with degraded accuracy (i.e. classes with \( a_{k}^{or}>0\)) which are instead processed with Center Crop. Since this approach didn't recover the accuracy on the affected classes, they hypothesized that DA induces a general invariance or an implicit bias that still negatively affects classes that are not augmented when DA is applied to the majority of the training data.

We explore a simple class-conditional augmentation strategy based on our insights regarding class confusions. By changing the augmentation strength for as few as \(1\%\) to \(5\%\) of classes, we observe substantial improvements on the classes negatively affected by the standard DA. We also provide an alternative explanation for why the class-conditional augmentation approach from Balestriero et al.  was not effective. In particular, we found in many cases that as we train models with stronger augmentation, a class \(k\) negatively affected by DA consistently gets misclassified as a related class \(l\) (see Section 5). We can precisely describe these confusions in terms of _False Negative_ (FN) mistakes for class \(k\) (not recognizing an instance from class \(k\)) and _False Positive_ (FP) mistakes for class \(l\) (misclassifying an instance from another class as class \(l\)):

\[FN_{k}^{or}(s)=_{x X_{k}}I[f_{s}(x) k] FP_{l} ^{or}(s)=_{(x X)(x X_{l})}I[f_{s}(x)=l].\]

We argue that to address the degraded accuracy of class \(k\) it is also important to consider DA effect on class \(l\). In Appendix E, we show the number of class-level False Positive mistakes as a function of DA strength for the set of classes with the highest \( FP_{l}=FP_{l}(s=8\%)-_{s}FP_{l}(s)\) (i.e. the classes for which the number of FP mistakes increased the most with standard augmentation). Note that these classes are often semantically related to and confused with the ones affected in accuracy, for example "stage" is confused with "guitar", "barybershop" with "tobacco shop".

We explore a simple adaptation of DA policy informed by the following observations: (1) generally stronger DA is helpful for the majority of classes and leads to learning more diverse features, (2) a substantially increased number of False Positive mistakes for a particular class likely indicates that its augmented data distribution overlaps with other classes and it might negatively affect accuracy of those related classes. Further, we discuss training the model from scratch using class-conditional augmentation policy, and in Appendix E we consider fine-tuning the model using class-conditional augmentation starting from a checkpoint pre-trained with the strongest augmentation strength.

Class-conditional augmentation policy. In general, for a class that is not closely related to other classes, strong RRC augmentation should lead to learning diverse features correlated with the class label and improving accuracy. Thus, by default we set the strongest data augmentation value \(s=8\%\) for the majority of classes, and change augmentation level for a small subset of classes. We change the augmentation strength for \(m\) classes for which False Positive mistakes grew the most with stronger DA (i.e. the classes with the highest \( FP_{l}\)). However, completely removing augmentation from these classes would hurt their accuracy (or equivalently increase the number

    & Avg acc &  Avg acc of \\ 50 classes \\  & 
 Avg acc of \\ 950 classes \\  \\   & \(s=8\%\) & \(76.79_{ 0.03}\) & \(53.93_{ 0.20}\) & \(77.99_{ 0.02}\) \\  & \(s=60\%\) & \(74.65_{ 0.03}\) & \(59.11_{ 0.20}\) & \(75.47_{ 0.02}\) \\   & \(76.11_{ 0.05}\) & \(43.02_{ 0.28}\) & \(77.85_{ 0.04}\) \\   & \(m=10\) & \(76.70_{ 0.03}\) & \(54.99_{ 0.15}\) & \(77.84_{ 0.03}\) \\  & \(m=30\) & \(76.70_{ 0.03}\) & \(55.48_{ 0.23}\) & \(77.82_{ 0.03}\) \\   & \(m=50\) & \(76.68_{ 0.04}\) & \(56.34_{ 0.14}\) & \(77.75_{ 0.04}\) \\   

Table 1: **Class-conditional data augmentation policy informed by our insights improves performance on the negatively affected classes while maintaining high overall accuracy. Average accuracy of different augmentation policies on all ImageNet classes, negatively affected classes and remaining majority of classes.**of False Negative mistakes) so we need to balance the tradeoff between learning diverse features and avoiding class confusions. As a heuristic, we set augmentation strength for each class \(l\) to \(s^{*}=*{arg\,min}_{s}F{P_{l}}(s)+FN_{l}(s)\) corresponding to the minimum of the total number of class-related mistakes across augmentation levels.

We vary the number of classes for which we change augmentations in the range \(m\{10,30,50\}\). In Table 1 we show the results where the parameters of the augmentation policy are defined using original ImageNet labels and in Appendix E we use Real multi-label annotations. We compare this intervention to the baseline model trained with the strongest augmentation \(s=8\%\), mild augmentation level \(s=60\%\) optimal for average accuracy on the affected set of classes, and the class-conditional augmentation approach studied in Balestriero et al.  where we remove augmentation from the negatively affected classes. We find existing approaches sacrifice accuracy on the subset of negatively affected classes for overall average accuracy or vice versa. For example, as we previously observed the default model trained with \(s=8\%\) achieves high average accuracy on the majority of classes but suboptimal accuracy on the 50 classes affected by strong augmentation. Optimal performance on these \(50\) classes is attained by the model trained with \(s=60\%\), but the overall average accuracy significantly degrades. Removing augmentation from the negatively affected classes only exacerbates the effect and decreases the accuracy both on the affected set and on average. At the same time, tuning down augmentation level on \(1\) to \(5\%\) of classes with the highest FP mistake increase improves the accuracy on the affected classes by \(2.5\%\) for \(m=50\), and taking into account the tradeoff between False Positive and False Negative mistakes helps to maintain high average accuracy overall and on majority of classes (the average accuracy decreased by \(0.1\%\) for \(m=50\)). These results support our hypothesis and demonstrate how a simple intervention on a small number of classes informed by the appropriate metrics can substantially improve performance on the negatively affected data.

In Appendix F we study the class-level accuracy degradation of the ViT-S  model trained on ImageNet with varied RRC augmentation strength. We observe that a similar set of classes is negatively affected in accuracy with stronger augmentation and multi-label annotations resolve some cases of accuracy degradation. Similarly, we identify the same categories of class confusions. By conducting class-conditional augmentation intervention and adapting augmentation strength for \(m=10\) classes with the highest increase in False Positive mistakes, we improve the average accuracy on the degraded classes by over \(3\%\): from \(52.28\% 0.18\%\) to \(55.49\% 0.07\%\).

## 7 Discussion

In this work, we provide new insights into the class-level accuracy degradation on ImageNet when using standard data augmentation. We show that it is important to consider the interactions among class-conditional data distributions, and how data augmentation affects these interactions. We systematically categorize the most significantly affected classes as inherently ambiguous, co-occurring, or involving fine-grained distinctions. These categories often suffer from label noise and thus the overall negative effect is significantly muted when evaluating performance with cleaner multi-label annotations. However, we also identify non-trivial cases, where the negative effects of data augmentation cannot be explained by label noise such as fine-grained categories. Finally, in contrast to prior attempts, we show that a simple class-conditional data augmentation policy based on our insights can significantly improve performance on the classes negatively affected by standard data augmentation.

**Practical recommendations.** When evaluating model performance, one should not only check average accuracy, which may conceal class-level learning dynamics. Instead, we recommend researchers also consider other metrics such as False Negative and False Positive mistakes to better detect which confusions data augmentation introduces or exacerbates. In particular, when training a model with strong augmentations, one should train another model with milder augmentation to check whether these finer-grained metrics degraded as an indicator that augmentation is biasing the model's predictions. We can then design targeted augmentation policies to improve performance on the groups negatively affected by standard augmentation.