# Textual Training for the Hassle-Free Removal of Unwanted Visual Data :

Case Studies on OOD and Hateful Image Detection

Saehyung Lee\({}^{1}\)1Jisoo Mok\({}^{1}\) Sangha Park\({}^{1}\) Yongho Shin\({}^{2}\) Dahuin Jung\({}^{3}\)1 Sungroh Yoon\({}^{1,4}\)2

\({}^{1}\)Department of Electrical and Computer Engineering, Seoul National University

\({}^{2}\)Qualcomm Korea YH, Seoul, South Korea

\({}^{3}\)School of Computer Science and Engineering, Soongsil University

\({}^{4}\)Interdisciplinary Program in Artificial Intelligence, Seoul National University

{halo8218, magicshop1118, wiarae}@snu.ac.kr,

yshin@qti.qualcomm.com, dahuin.jung@ssu.ac.kr, sryoon@snu.ac.kr

###### Abstract

In our study, we explore methods for detecting unwanted content lurking in visual datasets. We provide a theoretical analysis demonstrating that a model capable of successfully partitioning visual data can be obtained using only textual data. Based on the analysis, we propose Hassle-Free Textual Training (HFTT), a streamlined method capable of acquiring detectors for unwanted visual content, using only synthetic textual data in conjunction with pre-trained vision-language models. HFTT features an innovative objective function that significantly reduces the necessity for human involvement in data annotation. Furthermore, HFTT employs a clever textual data synthesis method, effectively emulating the integration of unknown visual data distribution into the training process at no extra cost. The unique characteristics of HFTT extend its utility beyond traditional out-of-distribution detection, making it applicable to tasks that address more abstract concepts. We complement our analyses with experiments in out-of-distribution detection and hateful image detection. Our codes are available at https://github.com/Saehyung-Lee/HFTT

## 1 Introduction

We are currently in the midst of what is known as the large-scale AI era. The growth in both the size of deep neural networks and training datasets led to unparalleled achievements in a wide array of tasks . However, this transition to large-scale AI presents new, unforeseen challenges. Particularly, the recent reports on the biased behavior of large AI models raise significant concerns surrounding the continuously expanding size of training datasets without proper quality control and regulation . The massive scale of visual training datasets necessary to train large-scale models presents a challenge in curating data to ensure unbiased and safe datasets. This is primarily due to the impracticality of manually selecting and removing unwanted content from such an extensive collection of images. This issue of data curation has traditionally been addressed by: (i) creating a supervised dataset for a specific objective; (ii) training a model on this dataset; and then (iii) utilizing the model to develop a larger dataset . However, this approach exploits considerable human labor and needs to be re-initiated from the beginning whenever there are changes in the training objective.

The field of out-of-distribution (OOD) detection, which aims identify OOD data that lie outside the training data distribution, can be considered a sub-branch of data curation research. Recent works in OOD detection utilize vision-language models (VLMs) [40; 29; 30] to take advantage of the rich and human-aligned representations learned by these models. For instance, Esmaeilpour et al.  augmented the pre-trained CLIP model  with an additional decoder module, trained on a vision-language dataset, for visual OOD detection. In a similar vein, Wang et al.  incorporated an extra "no" text encoder, trained on a vision-language dataset, into CLIP. These previous approaches, however, suffer from a significant limitation: They require a vast amount of additional vision-language data. Using data samples targeted for detection can improve sample efficiency, but this leads to the dilemma of needing to collect unwanted data for the purpose of removing them.

In this work, we propose a novel method that no longer relies on additional visual data or a computationally expensive training process. We first outline our theoretical rationale, particularly demonstrating that with a successfully trained model on a bimodal dataset, like CLIP, one can obtain a classifier to partition data from one mode using solely the data from the other mode. Building upon our motivation, we propose a method called Hassle-Free Textual Training (HFTT). HFTT consists of a newly proposed loss and a clever textual data synthesis method, updating trainable parameters defined in the joint embedding space to improve the detection of undesirable visual content. Specifically, we decompose the weighted cross-entropy loss into a formula that includes a regularization term, which is tailored to our use. Additionally, to achieve higher detection accuracy, we introduce the concept of focal loss . Moreover, our textual data synthesis method, which combines prompt templates and words, can effectively imitate the involvement of the entire visual data distribution in the training process. We illustrate an overview of our proposed method in Figure 1.

The proposed loss function brings considerable convenience in achieving our objective. To train an unwanted data detection model, it is necessary to define out-distribution for a given data distribution (in-distribution), which is not always straightforward due to vague boundaries between the two. For instance, in hateful content detection tasks , the divide between what is hateful and what is not is influenced by various contexts, _e.g._, historical and social backgrounds. Our proposed loss eliminates the need for human labor to annotate out-distribution data because it does not involve a clearly defined set of out-distribution data. Furthermore, our textual data synthesis method requires no cost, employing a rule-based approach using only prompt templates and a set of words.

Based on the principle that HFTT can detect out-distribution samples by merely defining the in-distribution in natural language, we propose that this method can be extended to tasks beyond traditional OOD detection, including hateful image detection. Current OOD detection methods often fail in such extended tasks for two main reasons: firstly, they are based on the assumption of a distinct boundary between in- and out-distributions, which is unsuitable for tasks with abstract concepts.

Figure 1: Overview of our proposed method. **Task embeddings** define the task to be performed. For example, in the case of hateful image detection, hate speeches would serve as task embeddings, while in OOD detection, the names of classes from the training distribution would be the task embeddings. **Trainable embeddings** are the only parameters that are trained in our method, defined in the joint embedding space. During the training phase, only textual data are used, and in the testing phase, these trained parameters are employed to classify images. Detailed explanations are provided in Sections 3.

Secondly, methods requiring training images may lead to ethical concerns. Our proposed method, however, is not subject to these limitations.

Through empirical validation, we verify that HFTT can enhance the performance of VLMs in identifying unwanted visual data, whether it be OOD instances or hateful images. Additionally, we demonstrate through feature visualization results that HFTT, despite not observing any visual data in the training phase, appears to have been trained as if it had. Furthermore, we provide various analyses of HFTT, including comparative results of using different textual data synthesis methods.

In summary, our **contributions** are as follows: (i) We theoretically demonstrate how textual data can serve as a substitute for visual data in our scenario; (ii) We introduce a new loss function. Its use eliminates the need for labor in annotating out-distribution data; (iii) We propose a textual data synthesis method that can efficiently imitate the visual data distribution in our training; (iv) We empirically analyze HFTT, a method composed of the above proposals. Our experiments show that HFTT is effective in a range of scenarios, from traditional OOD detection to situations involving abstract concepts, like the identification of hateful images.

## 2 Related Work

Vision-language models.With the advancements in deep learning, tackling sophisticated tasks that demand an understanding of both vision and language modalities has become viable. The methodologies employed to encode image and text data exhibit notable distinctions owing to their inherent differences. Prominent within this domain are dual-stream models exemplified by CLIP , ALIGN , and FILIP . These models employ separate encoders for text and image data, optimizing them through contrastive objectives to align semantically similar features across heterogeneous modalities. Primarily, VLMs integrate transformer-based encoders for text data, while a variety of architectures, encompassing convolutional neural networks [25; 15] and vision transformers , are deployed for image encoding. The success of CLIP-like models has spurred numerous subsequent inquiries, with a focus on enhancing data efficiency and adaptability for diverse downstream tasks.

Out-of-distribution detection.Traditionally, OOD detection has evolved by defining post-hoc OOD scores [18; 32; 27; 34] or formulating learning algorithms based on outlier exposure methods [26; 20; 10]. With the advancement of VLMs, methods for OOD detection that leverage both image and text embeddings have also progressed. Post-hoc OOD score methods based on VLM typically involve utilizing the OOD class name [12; 11] or defining OOD scores using the top similarity values between images and class names . In the case of outlier exposure, which requires a training algorithm, some approaches employ prompt learning or fine-tune the image encoder  of models like CLIP. Transitioning from conventional methods to VLM-based approaches, none of these methods have attempted text-only training and subsequently applied their techniques to tasks such as hateful image detection.

Text-only training for vision tasks.Given the progress in VLMs, there have been numerous studies aimed at replacing images with textual representations in vision and multimodal tasks. Textual data presents the advantage of being easily collectible compared to visual data. Previous studies have demonstrated the effectiveness of using only textual information for various vision tasks, including image classification , image captioning , and multi-label image recognition . Our work represents a pioneering effort in applying text-only supervision to unwanted visual data detection.

Figure 2: Overview of Section 3.1. The red and blue colors symbolize the two classes \(-1\) and \(+1\), respectively. In our theoretical model, \(u\) and \(v\) can be interpreted as text and image, respectively.

Method

In this section, we propose a new textual training method for the convenient and successful removal of unwanted visual data. In Section 3.1, we theoretically demonstrate, through a motivating example, that when there is a well-trained model on a bimodal dataset, such as CLIP, it is possible to train a binary classifier successfully partitioning data from one mode using only data from the other. This theoretical revelation leads us to our novel loss function that allows hassle-free training of an unwanted visual data detector in Section 3.2. Lastly, in Section 3.3, we present our proposed method that includes a simple yet effective synthesis of textual data. The proposed method is executable even without access to the parameters of the backbone model, making it lightweight and applicable to black-box foundational models.

### A Motivating Example

We theoretically demonstrate that when there exists a well-trained bimodal model \(F:\) for a given bimodal data distribution, it is possible to train a classifier successfully partitioning data from one mode (\(\)) using only the dataset from the other mode (\(\)). To align with the operations of VLMs like CLIP in our scenario, we assume that the output vectors of \(F\) are normalized. We define a bimodal dataset \(D\) as follows:

\[D=\{(g_{i},h_{i},y_{i})\}_{i=1}^{N},y^{u.u.i.}{}}r.}\{-1,+1\}(g,h)i.i.}{}}_{y}_{y}.\]

\((g_{i},h_{i})\) represents the input vectors from the two modes for a given data sample, where \(y_{i}\) denotes the binary class of the \(i\)-th sample. We can partition the dataset \(D\) as follows:

\[D=D_{-1} D_{+1},D_{y}=\{(g_{i},h_{i})\,|y=y_{i}\}.\]

We assume that samples belonging to the same class in the dataset \(D\) exhibit similar semantic patterns. Given \(F\) that successfully builds the joint embedding space for the bimodal data distribution, we can posit the following:

\[u_{+1}^{}v_{+1}>u_{+1}^{}v_{-1}u_{-1}^{}v_{+1}<u_{-1}^{ }v_{-1},u_{y}=}_{u U_{y}}[u], \,v_{y}=}_{v V_{y}}[v],\]

\[U_{y}=\{F(g_{i})|g_{i} D_{y}\},V_{y}=\{F(h_{i} )|h_{i} D_{y}\}.\]

\(U_{y}\) and \(V_{y}\) are class-conditional embedding sets for each of the two modes, respectively. For simplicity, we assume that the variances of the angular distributions relative to their mean vectors are equal for sets \(U_{-1}\) and \(U_{+1}\), as well as for sets \(V_{-1}\) and \(V_{+1}\).

We investigate whether the cosine-similarity classifier \(^{}\) trained solely on the unimodal dataset \((g_{i},y_{i})_{i=1}^{N}\) using \(F\) can successfully be applied to \((h_{i},y_{i})_{i=1}^{N}\). We establish the following theorem:

**Theorem 1**.: _For the quadratic loss function \(L(u,y;)=(1-y^{}u)^{2}\), the optimal cosine-similarity classifier \(^{}\) that classifies sets \(U_{-1}\) and \(U_{+1}\) is_

\[*{arg\,min}_{}}_{u U_{-1}}[L (u,-1;)]+}_{u U_{+1}}[L( u,+1;)]=-u_{-1}}{\|u_{+1}-u_{-1}\|}.\]

Proofs are in Appendix A. Theorem 1 demonstrates the optimal classifier \(^{}\) is orthogonal to \(u_{-1}+u_{+1}\). We present an illustration in Figure 2 to enhance understanding of both the problem under investigation and the results of our analysis.

Applying the classifier \(^{}\) trained to classify \(U_{-1}\) and \(U_{+1}\) to distinguish between \(V_{-1}\) and \(V_{+1}\) leads to the following:

**Corollary 1**.: _The classifier \(^{}\), with respect to \(V_{-1}\) and \(V_{+1}\), satisfies the double inequalities of_

\[}_{v V_{-1}}[^{}{}^{}v]<0< }_{v V_{+1}}[^{}{}^{}v].\]

This implies that we can successfully classify \(V_{-1}\) and \(V_{+1}\) by observing the cosine similarities with \(^{}\). Motivated by these theoretical examples, we hypothesize that classifiers obtained solely using textual data can operate on visual data as well. Section 4 empirically demonstrates that the arguments developed based on our theoretical model can be applied to modern machine-learning settings.

### Our Proposed Loss Function

Our objective is to distinguish in-distribution data samples (\(D_{}\)), conforming to given data distribution, from out-distribution data samples (\(D_{}\)). The development of our new loss function begins with defining the binary cross-entropy loss \(L\) as follows:

\[L(u,y)=- p(u)-(1-p(u)).\]

We employ the notations introduced in Section 3.1. \(p(u)\) denotes the probability that the label of an embedding \(u\) is \(+1\), where \(+1\) signifies an out-distribution. With respect to datasets \(U_{-1}\) and \(U_{+1}\), we minimize

\[_{u U_{-1}} L(u,-1)+_{u U_{+1}}(1- )L(u,+1).\] (1)

We introduce a hyper-parameter, \(\), to adjust the balance between in-distribution learning (the first term) and out-distribution learning (the second term). Equation (1) can be reformulated as

\[=_{u U_{-1}}L(u,-1)-_{u U_{-1}}(1- )L(u,-1)+_{u U_{+1}}(1-)L(u,+1 ).\] (2)

The second term can be understood as regularization for in-distribution learning. As \(\) approaches 0, in-distribution learning is more heavily impeded. Rather than employing the original regularization term, \(-_{u U_{-1}}(1-)L(u,-1)\), we propose changing it to

\[_{u U_{-1}}(1-)L(u,+1).\]

Before analyzing the significance of this modification to the objective function, we first examine the effects resulting from this change. Along with the modification, our objective function can be formulated as follows:

\[_{u U_{-1}}L (u,-1)+_{u U_{-1}}(1-)L( u,+1)+_{u U_{+1}}(1-)L(u,+1)\] (3) \[=_{u U_{-1}}L(u,-1)+_{u U_{-1} U_{+1 }}(1-)L(u,+1).\]

To minimize Eq. 2, it is imperative to distinguish between the in-distribution dataset and the out-distribution dataset. In-distribution data aligns with the objective of the given task, and any data not included in it becomes out-distribution data. However, in real-world scenarios, distinguishing between these distributions is not straightforward. For instance, if we consider \(U\) as the text embedding space, collecting out-distribution texts for a given set of in-distribution texts involves considerations such as homonyms, synonyms, and various forms of linguistic variations. Particularly, in tasks where the boundaries between in-distribution and out-distribution are ambiguous, as seen in challenges such as hate content detection , constructing a dataset for Eq. 2 becomes difficult and requires considerable human labor. However, the utilization of Eq. 3 alleviates us from such challenges. In other words, the union of the two sets \(U_{-1} U_{+1}\) in Eq. 3 allows us to treat all data samples as out-distribution without the need to ponder their relationship with the in-distribution, providing a solution to the intricacies involved in dataset construction. The distinction between Eq. 2 and 3 becomes evident when comparing the gradient signals produced by the two different regularization terms. Gradients of the regularization terms in Eq. 2 and 3 can be computed as follows:

(original) \[-_{u U_{-1}}=_{u U_{-1}},\] (proposed) \[_{u U_{-1}}=_{u U_{-1}}.\]

The original regularization term weakly regularizes in-distribution samples that were sufficiently learned by the model (_i.e._, samples with low \(p(u)\)). However, the proposed regularization term does the exact opposite; it imposes stronger regularization on in-distribution samples with low\(p(u)\). In essence, our proposed regularization prevents the model from exhibiting high confidence in in-distribution samples and enforces the decision boundary to be created near the in-distribution.

Recent studies show that the closer the decision boundary of the out-distribution data detector is to the in-distribution, the more effective the detector is at identifying various out-distribution data [26; 19; 10; 39]. Subsequent research efforts have been directed at obtaining out-distribution samples that reside close to the in-distribution while training a detector to bring its decision boundary closer to the in-distribution. For instance, Lee et al.  utilizes a generative adversarial network to acquire samples placed on the in-distribution boundary. Du et al.  models the in-distribution using a Gaussian distribution and samples embeddings from the low-likelihood regions of the defined Gaussian distribution. Likewise, we focus on training samples situated in the region close to the in-distribution by incorporating an additional focal loss.

The focal loss was initially proposed to forcefully suppress the gradients for background pixels, which dominate the image, and intensify the learning signals from foreground pixels. Under our scenario, the in-distribution, like foreground pixels, tends to inhabit a small portion of the entire embedding space. In light of the similarity between the in-distribution and foreground pixels, we utilize the focal loss to restrain the loss from far out-distribution samples and amplify learning signals from samples near the in-distribution. The proposed loss can thus be defined as:

**Definition 1**.: _Let \(B_{-1}=\{x_{i}\}_{i=1}^{N}\) and \(B=\{_{i}\}_{i=1}^{N}\) denote mini-batches that are respectively drawn from the specified in-distribution and the overall data distribution. Let \(L\) be the cross-entropy loss. Then, our proposed loss function is_

\[_{x_{i} B_{-1}}L(x_{i},-1)+(1-)_{x_{j } B}_{j}L(x_{j},+1);\ \ _{j}=}{_{x_{k} B}_{k}}_{j}= (1-p(x_{j}))^{}.\] (4)

\(p(x)\) is the predictive probability of \(x\) belonging in out-distribution. \( 0\) is treated as a hyperparameter of the focal loss. In Section C, we compare the results of using loss terms in Eqs. 2 and 3 and demonstrate the particular effectiveness of the focal loss.

### Hassle-Free Textual Training (HFTT)

So far, we have assumed access to data sampled from the out-distribution. However, we may not always be able to anticipate the out-distribution in advance, and even if we can, sampling a subset of data that is representative of the entire distribution is not a straightforward problem in nature. In our scenario, we solely utilize textual data to learn an unwanted visual data detector. Therefore, the proposed scenario requires texts to define the in-distribution and a comprehensive corpus of textual data that can replace the entirety of visual data. VLMs, such as CLIP, obtained impressive zero-shot classification accuracy on diverse visual data benchmarks through the usage of prompts, _e.g._, "a photo of a \(\{\}\)." Inspired by the success of prompting in VLMs, we conjecture that all visual data can be expressed textually through prompts. This assumption allows the textual dataset to replace the unknown visual data distribution by integrating words associated with the visual data into prompts, drastically simplifying the process of textual data sampling in our method. One example of a prompt design utilized in our approach is: "This is a photo of a \(\{\}\)." To emulate the effect of using the entire visual data distribution, we adopt a word set3 that includes approximately 370k English words. We report the results of using other prompt designs or textual data acquisition processes in Appendix C. While the optimization procedure in our method additionally requires in-distribution textual data according to Eq. 4, these can be obtained with minimal effort by creating arbitrary sentences or prompts related to the given task. Section 4 details how in-distribution textual data are attained for each experimental setting. Even though the task of unwanted visual data detection is a type of binary classification problem, learning a plain linear classifier in the embedding space of pre-trained VLMs through approaches like linear probing is not necessarily compatible with our task because the in- and out-distributions are not expected to be linearly separable. To accurately estimate the probability that input \(x\) belongs in the out-distribution \(p(x)\), we must take advantage of the informative signals in the text encoder of pre-trained VLMs. In our method, \(p(x)\) is computed as follows:

1. Obtaining embeddings \(\{w_{i}^{}\}_{i=1}^{K}\) for \(K\) texts that effectively represent in-distribution visual data is equivalent to defining the task. This process is akin to obtaining zero-shot classifiers using VLMs. We will refer to these text embeddings as **task embeddings**.

[MISSING_PAGE_FAIL:7]

involves obtaining cosine similarities between trainable embeddings and input embeddings. This cost amounts to 2\(\)(batch size)\(\)(embedding dimension)\(\)(the number of trainable embeddings) FLOPS, which is negligible compared to the inference cost of the entire model.

### Out-of-Distribution Detection

In OOD detection experiments, we utilize weights of zero-shot classifiers of pre-trained VLMs as task embeddings for HFTT. In-distribution textual data are obtained via combinations of various prompt templates and class names of in-distribution data, which, in our experimental setting, are equivalent to 1,000 ImageNet classes. We adopt the prompt set released by OpenAI for prompt ensembling4 as prompt templates. The comparison results are reported in Table 1. Despite the simple and lightweight nature of the proposed method, it outperforms even strong baselines that utilize images, on average.

To understand how the trained embeddings provide the task embeddings with informative signals for identifying OOD data points, we visually analyze the joint embedding space of CLIP in Figure 3. Even though visual OOD data were not involved in the training process, the trained embeddings are positioned on a sub-region of the embedding space inhabited by OOD data and thus can function as additional pointers for where OOD data lie in the joint embedding space. Therefore, this deliberate positioning of trained embeddings precludes the task embeddings from accidentally confusing out-distribution data as in-distribution by refining the decision boundary to intricately separate in-distribution and out-distribution regions.

    &   \\  \\  } &   \\  } &  \\   & FPR & AUROC & FPR & AUROC & FPR & AUROC & FPR & AUROC & FPR & AUROC \\   \\ Mahalanobis & 99.33 & 55.89 & 99.41 & 59.94 & 98.54 & 65.96 & 98.46 & 64.23 & 98.94 & 61.51 \\ MSP & 40.17 & 89.76 & 63.99 & 79.40 & 63.50 & 80.19 & 67.01 & 79.33 & 58.67 & 82.17 \\ KNN & 29.17 & 94.52 & 35.62 & 92.67 & **39.61** & **91.02** & 64.35 & 85.67 & 42.19 & 90.97 \\ NPOS & **16.58** & **96.19** & 43.77 & 90.44 & 45.27 & 89.44 & 46.12 & **88.80** & 37.94 & 91.22 \\   \\ Energy & 34.70 & 90.55 & 32.33 & 90.58 & 40.29 & 89.32 & 51.24 & 72.36 & 39.64 & 85.70 \\ MaxLogit & 35.03 & 89.46 & 32.86 & 90.33 & 41.15 & 89.60 & 68.17 & 75.63 & 44.30 & 86.26 \\ ZOC & 87.30 & 86.09 & 81.51 & 81.20 & 73.06 & 83.39 & 89.90 & 76.46 & 85.19 & 81.79 \\ MCM & 34.33 & 91.36 & 32.27 & 91.86 & 47.48 & 88.68 & 50.90 & 87.52 & 41.25 & 89.86 \\
**HFTT (ours)** & 27.44 & 93.27 & **19.24** & **95.28** & 43.54 & 90.26 & **43.08** & 88.23 & **33.33** & **91.76** \\   

Table 1: Comparison of HFTT and competitive baselines with and without in-distribution image requirements on the ImageNet-1K dataset. The best and second-best results are indicated in bold and underlined, respectively. Our method surpasses even strong baselines that utilize in-distribution images. This complements our analysis in Section 3.1, demonstrating that textual data can substitute for visual data in such tasks.

Figure 3: UMAP  visualization of the joint embedding space of CLIP. The dispersed, transparent markers represent the OOD data samples used in our experiment (iNatural: brown; SUN: grey; Places: pink; Texture: purple; NINCO : red). The trained embeddings (blue stars) are located in a sub-region of the embedding space occupied by OOD data. We trained 2000 embeddings for this plot. It is important to note that these trainable embeddings did not incorporate any information about the OOD data during their training time.

This visualization result can be attributed to two methodological characteristics that are unique to our method. First, our method directly optimizes the trainable embeddings and is not bounded by the modality gap between texts and images. Second, our method successfully places the trained embeddings on top of OOD data only through textual data; this consolidates that textual data can replace visual data, providing strong empirical support for the theory presented in Section 3.1. Together, these two aspects of HFTT yield the joint embedding space as illustrated in Figure 3.

### Hateful Image Detection

In this task, the hateful data that contains offensive content against Muslims and Jews is treated as in-distribution data, whereas innocuous data void of such content is treated as out-distribution data. Consequently, embeddings of distinct phrases from a collection of offensive and hateful phrases, provided as part of the Hate dataset, are utilized as task embeddings. The entire set of offensive and hateful phrases is employed as in-distribution textual data.

The Mahalanobis, MSP, KNN, and NPOS methods necessitate the construction of an in-distribution image dataset. Therefore, they should not be used as methods for unethical image detection tasks such as hate image detection, as doing so would require the construction of unethical image datasets, leading to numerous ethical problems such as director or indirect leakage of sensitive information. In contrast, HFTT requires no usage of any image, thus it can be applied to any unethical image detection task without ethical concerns. To highlight the differences between traditional OOD detection methods and HFTT, we include two additional baselines [49; 24] and one extra dataset .

In Table 2, we can observe that most OOD detection methods show significantly lower performance compared to HFTT. These results arise because OOD detection methods assume a classification problem with clear distinctions between classes. In tasks dealing with abstract concepts, the boundaries between data clusters within the in-distribution are ambiguous, which results in the underperformance of existing OOD detection methods. NegLabel shows different results compared to traditional OOD detection methods but still falls short of our proposed approach. We provide a further comparison of our method to CLIPN  and NegLabel  in Appendix B.

To further study the generalizability of HFTT, we observe the effectiveness of HFTT in low-quality image detection  and within the medical image domain [6; 16; 48]. The findings demonstrate the potential extension of HFTT's applicability beyond conventional OOD detection tasks. The results of these experiments and an ablation study on hyper-parameters are provided in Appendices B and C.

## 5 Conclusion and Limitation

In this paper, we proposed a novel methodology for identifying undesirable content hidden within visual datasets. Close theoretical scrutiny of the joint embedding space of VLMs led to the development of HFTT, an efficient framework for training detectors to automatically identify unwanted visual content by leveraging solely textual data together with pre-trained VLMs. HFTT is comprised of a creative objective function that markedly diminishes human involvement in data annotation and the textual data synthesis technique in HFTT that can simulate the usage of unknown visual

    &  &  \\  & iNaturalist &  &  &  &  &  \\   & FPR & AUROC & FPR & AUROC & FPR & AUROC & FPR & AUROC & FPR & AUROC & FPR & AUROC \\  Energy & 12.30 & 97.53 & 3.88 & 98.51 & 13.87 & 97.40 & 39.70 & 94.98 & 26.89 & 96.12 & 17.43 & 97.10 \\ MaxLogit & 23.65 & 96.89 & 18.49 & 97.49 & 27.84 & 96.48 & 33.33 & 96.00 & 33.03 & 95.99 & 25.82 & 96.71 \\ ZOC & 87.76 & 71.05 & 66.51 & 85.23 & 69.96 & 82.57 & 65.48 & 83.22 & 81.06 & 78.36 & 74.15 & 84.09 \\ MCM & 80.53 & 76.70 & 87.54 & 69.38 & 81.37 & 74.12 & 60.39 & 84.97 & 81.95 & 78.00 & 77.45 & 76.29 \\ CLIPN & 47.71 & 92.78 & 36.36 & 95.16 & 40.62 & 94.52 & 53.36 & 92.36 & 68.40 & 89.58 & 49.29 & 92.88 \\ NegLabel & **0.03** & **99.84** & 1.09 & 99.10 & 5.16 & 98.50 & 3.56 & 98.82 & 12.62 & 97.86 & 4.49 & 98.82 \\
**HFTT (ours)** & 0.17 & 99.44 & **1.05** & **99.13** & **4.38** & **98.60** & **1.73** & **99.08** & **4.18** & **98.52** & **1.83** & **99.06** \\   

Table 2: Comparison of HFTT with state-of-the-art methods for OOD detection that do not require in-distribution images, conducted on the Hate dataset. The best result in each column is in bold. HFTT outperforms baseline approaches, showing that it can effectively be used for the general purpose of unwanted data detection.

data distributions in the training process without additional cost. The distinctive attributes of HFTT broaden its applicability from a clearly-scoped OOD detection task to a far more general set of tasks that are more abstract. Because HFTT requires some type of VLM as the base model, its capabilities are bounded by the representative capacity of pre-trained VLMs. This dependency on pre-trained VLMs makes the use of HFTT in tasks that VLMs struggle with challenging.

Impact Statements.This paper contributes to the growing field of data curation and selection research. As datasets for training large AI models expand without adequate safeguards, identifying unwanted data points, such as biased or offensive content, from training datasets is becoming crucial. We believe our work will make a positive contribution to this area, opening up new possibilities for the effortless removal of unwanted visual data. While our method could potentially be misused for content censorship, we believe the positive impact it provides significantly outweighs these concerns.

This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) [No.RS-2021-II211343, 2022-0-00959, RS-2022-II220959, Artificial Intelligence Graduate School Program (Seoul National University)], the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2022R1A3B1077720, 2022R1A5A708390811), and the BK21 FOUR program of the Education and the Research Program for Future ICT Pioneers, Seoul National University in 2024.