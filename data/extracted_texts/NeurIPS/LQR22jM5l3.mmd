# Mind the Graph When Balancing Data for Fairness or Robustness

Jessica Schrouff

Google DeepMind

schrouff@google.com

Alexis Bellot

Google DeepMind

Amal Rannen-Triki

Google DeepMind

Alan Malek

Google DeepMind

Isabela Albuquerque

Google DeepMind

Arthur Gretton

Google DeepMind

Alexander D'Amour

Google DeepMind

Silvia Chiappa

Google DeepMind

###### Abstract

Failures of fairness or robustness in machine learning predictive settings can be due to undesired dependencies between covariates, outcomes and auxiliary factors of variation. A common strategy to mitigate these failures is data balancing, which attempts to remove those undesired dependencies. In this work, we define conditions on the training distribution for data balancing to lead to fair or robust models. Our results display that, in many cases, the balanced distribution does not correspond to selectively removing the undesired dependencies in a causal graph of the task, leading to multiple failure modes and even interference with other mitigation techniques such as regularization. Overall, our results highlight the importance of taking the causal graph into account before performing data balancing.

## 1 Introduction

When training prediction models, practitioners often desire that the model's outputs display safety properties in addition to high performance, such as being fair across demographic subgroups [29; 51] or being robust to distribution shifts [e.g. 19; 59]. These objectives can be difficult to attain if there are undesired dependencies between covariates \(X\), labels \(Y\), and auxiliary factors of variation \(Z\), such as confounding factors or hidden stratification [26; 27]. A commonly referenced example is that of an animal classification task from wildlife pictures [e.g. 64]: the model might identify patterns in the background of the images that are indicative of the type of animal (e.g. the presence of snow for polar bears or grass for cows), which might lead to the model failing to recognize the same animal when it is on another background. When the auxiliary factors relate to demographic attributes, the deployment of such models can have societal implications, e.g. patients not being assigned medical resources due to factors related to race .

Multiple mitigation strategies have been proposed to remove undesired dependencies pre-, in- or post-processing. Amongst them, balancing the training data is typically considered a straightforward approach and has been used or researched in various settings [e.g. 37; 39; 60; 8; 33; 40; 2]. This approach modifies the training distribution, indicated with \(P^{t}(X,Y,Z)\), into a new, balanced distribution (which we refer to as \(Q(X,Y,Z)\)) that aims to approximate an 'idealized' training distribution in which the undesired dependencies are absent [48; 14; 77]. Models are then trained on this balanced distribution to attain different fairness or robustness criteria. A popular approach to construct a balanced distribution is by balancing classes (resp. groups), leading to a uniform distribution over \(Y\) (resp. \(Z\)). While successful for addressing failures of robustness [e.g. 33] or of fairness due to under-representation of certain groups [e.g. 75], this approach does not induce independence between\(Y\) and \(Z\). To approximate independence, a 'joint' balancing on \((Y,Z)\) is often performed (e.g. 48, 8). Joint balancing can be implemented by matching the numbers of samples in all \((y,z)\) groups (only feasible when \(Y\) and \(Z\) have small, discrete domains) via subsampling the majority groups (e.g. 8), upsampling the minority groups (e.g. 63), resampling the data with weights proportional to \(P^{t}(Y)P^{t}(Z)/P^{t}(Y,Z)\), or reweighting the loss . Our work focuses on joint balancing given its suitability to mitigate a marginal dependence between \(Y\) and \(Z\).1 While the choice of the method for jointly balancing can impact the results [11; 65; 33], these methods can be all seen as modifying \(P^{t}\) as described in Definition 1.1.

**Definition 1.1** (Jointly balanced distribution).: We say that the distribution \(Q(X,Y,Z)\) is a jointly balanced version of \(P^{t}(X,Y,Z)\) if \(Q(X,Y,Z)=P^{t}(X,Y,Z)(Y)P^{t}(Z)}{P^{t}(Y,Z)}\).

In some cases, data balancing has proven to be an effective mitigation strategy for undesired dependencies, performing on-par with other, more complex mitigation techniques . Recently, data balancing has also shown promises for mitigation during fine-tuning or partial retraining [41; 44; 49; 79; 75], which is relevant to the settings of training large-scale models and with large amounts of data. Nevertheless, data balancing has also displayed failure modes in which the obtained models were not fair, robust or optimal [76; 48; 58; 2]. These failure modes have not been thoroughly characterized and can be difficult to predict. Furthermore, the impact of data balancing on other mitigation strategies has not been studied extensively.

Given data balancing's popularity as a baseline mitigation strategy for undesired dependencies, we aim to formalize some of its promises and pitfalls. Our analysis relies on a causal graphical framework, which allows investigating the impact of data balancing in different data generating processes. Our contributions can be summarized as follows: (1) we display failure modes of data balancing in semi-synthetic tasks and highlight how predicting these failures can be challenging; (2) we introduce conditions for data balancing to attain invariance to undesired dependencies as defined by fairness or robustness criteria; (3) we prove that data balancing does not correspond to'removing' undesired dependencies from a causal perspective, and can negatively impact fairness or robustness criteria when combined with regularization strategies; and (4) we illustrate how our findings can be used to distinguish between failure modes and identify next steps.

## 2 Preliminaries

Let \(X\), \(Y\), \(Z\) be discrete random variables with \(X\) corresponding to a set of covariates (e.g. tabular, images or text), \(Y\) to an outcome to be predicted, and \(Z\) to an auxiliary factor of variation, such as a sensitive attribute or the type of background of an image, that displays statistical dependence with \(Y\). We assume access to data sampled from distribution \(P^{t}(X,Y,Z)\), where \(P^{t}\) is the true data-generating distribution. We consider a family of models \(\) that will be trained on data from \(P^{t}(X,Y,Z)\) to minimize the risk \(R_{P^{t}}(f):=_{X,Y P^{t}}[(f;X,Y)]\) where \(\) is a loss function. We define \(f^{*}\) to be the _optimal_ model, i.e. one where the risk attains the minimum on \(P^{t}\). We assume that \(_{Q}[Y|X]=f^{*}(X)\), which occurs, for example, if \(\) is the square loss or cross-entropy loss.

**Definition 2.1** (Optimality).: A prediction model \(f\) is optimal w.r.t. \(P^{t}\) if \(f=*{argmin}_{f^{}}R_{P^{t}}(f^{})\).

### Desired criteria on a model's predictions

Due to undesired independencies, while a model may be optimal on \(P^{t}\), it might not be optimal on another distribution of interest \(P^{}(X,Y,Z)\) (e.g. in deployment), and/or might display disparities across subsets of the data (e.g. \(P^{t}(X,Y\,|\,Z=z)\)) . To mitigate this issue, multiple safety criteria have been defined in the fields of _fairness_ and _robustness_.

**Fairness:** Fairness criteria can be defined in terms of the dependence between the model's output \(f(X)\) and the auxiliary factor of variation \(Z\). We consider established fairness criteria [5; 51], including _demographic parity_[\(f(X)\!\!\! Z\), 23], _equalized odds_[\(f(X)\!\!\! Z\,|\,Y\), 29] and _predictive parity_[\(Y\!\!\! Z\,|\,f(X)\), 24]. Beyond fairness of \(f(X)\), we also consider fairness of intermediate _representations_\((X)\), e.g. \((X)\!\!\! Z\), for their usage in downstream tasks.

**Robustness:** In this field, the focus is typically on finding models \(f_{}\) parameterized by \(\) that provide the lowest risk across a _family of target distributions_\(\). For instance, the 'worst group performance' criterion aims to select parameters such that the performance on a 'worst' distribution \(P^{}\) is optimized, i.e. \(^{*}=_{}\{_{P^{}}R_{P^{ }}(f_{})\}\). \(\) can be defined so that each distribution \(P^{}\) represents a specific subpopulation , to minimize the loss in each subgroup, or aiming for an invariance of \(R_{P^{}}\) across subgroups [_risk-invariance_, 48].

**Definition 2.2** (Risk-invariance).: A prediction model \(f\) is risk-invariant w.r.t. a family of distributions \(\) if \(R_{P}(f)=R_{P^{}}(f)\  P,P^{}\).

If a model is optimal on \(P^{t}\) and risk-invariant w.r.t. \(\), it is also optimal w.r.t. \(\). The choice of \(\) is context-specific and reflects some domain knowledge about shifts that are likely to arise in a given application. For instance, a plausible family of target distributions could imply a shift in the dependence between \(Y\) and \(Z\), also known as a _correlation shift_, and be expressed as \(=\{P^{}(X,Y,Z)=P^{t}(X\,|\,Y,Z)P^{}(Z\,|\,Y)P^{t}(Y),  P^{}(Z\,|\,Y)\}\). Alternatively, we can define \(\) using a causal framework (see Section 2.2) when the data generation process is known .

We acknowledge that selecting amongst those criteria is context-dependent and do not advocate for a specific choice. We call a prediction model \(f\)_invariant_ to undesired dependencies, denoted with \(f_{inv}\), if it satisfies one of such criteria. For brevity, we focus on risk-invariance in the main text and consider fairness criteria in Appendix. Obtaining an invariant model can be performed in different ways, with data balancing being a popular approach.

### Causal framework to analyse data balancing

To understand the effects of data balancing, we need to investigate its impact on the distribution \(P^{t}\). A causal formalization is useful for studying how distributions change under different interventions. To analyse the implications of data balancing, we use the framework of _causal Bayesian networks_ (CBNs) [e.g. 71, 13, 52, 74, 25, 48]. A Bayesian network  is a pair \(,P^{t}\), in which \(\) is a directed acyclic graph whose nodes \(X^{1},,X^{D}\) represent random variables and in which \(P^{t}\) is a joint distribution over the nodes. The absence of edges in \(\) implies a set of statistical independence assumptions satisfied by \(P^{t}\) that can be expressed by the factorization \(P^{t}(X^{1},,X^{D})=_{d=1}^{D}P^{t}(X^{d}\,|\,(X^{d}))\), where \((X^{d})\) denote the _parents_ of \(X^{d}\), namely the nodes with an edge into \(X^{d}\) (we say that \(P^{t}\)_factorizes according to \(\)_). A CBN is a Bayesian network in which an edge expresses causal influence, so that \((X^{d})\) are _direct causes_ of \(X^{d}\). A directed path between \(X^{i}\) and \(X^{j}\) in a CBN is also called a _causal path_. A non-directed path, also called _non-causal path_, expresses statistical dependence of non-causal nature. We refer to the statistical dependence between \(X^{i}\) and \(X^{j}\) that arises only due to the presence of non-causal paths as _purely spurious_. In our setting \(X^{1} X^{D}=X Y Z\) where \(\) are unobserved variables. Inspired by prior work , we make a decomposition assumption on the form of the covariates \(X\).

**Assumption 2.3** (Decomposition of \(X\)).: The covariates \(X\) can be decomposed into three unobserved random variables \(X_{Z}^{},X_{Y}^{}\) and \(X_{Y Z}\) such that: 1) \(X_{Z}^{}\) does not have causal paths to/from \(Z\) but has causal paths to/from \(Y\), 2) \(X_{Y}^{}\) does not have causal paths to/from \(Y\) but has causal paths to/from \(Z\), 3) \(X_{Y Z}\) has causal paths to/from both \(Y\) and \(Z\), representing _entangled_ signals, and 4) \(X\) is measurable w.r.t. \((X_{Z}^{},X_{Y}^{},X_{Y Z})\), the joint \(\)-algebra. In particular, there exists a function \(g\) such that \(X=g(X_{Z}^{},X_{Y}^{},X_{Y Z})\) almost everywhere and \(P^{t}(X_{Z}^{},X_{Y}^{},X_{Y Z},Y,Z,)=P^{t}(g(X_{Z}^{ },X_{Y}^{},X_{Y Z}),Y,Z,)\).

In the animal classification example, \(X_{Z}^{}\) would correspond to the animal pixels, \(X_{Y}^{}\) to the background pixels (e.g. snowy or grassy landscape), and \(X_{Y Z}\) to characteristics of the animal that depend on its environment (e.g. color of the fur pixels in bears). Intuitively, we want to build a prediction model \(f\) that only depends on the animal pixels. While the decomposition may be readily available when a causal graph of the application is available and the data is tabular, we typically do not have direct access to the different functions of \(X\) and these need to be isolated algorithmically.

Following Scholkopf et al. , we consider both the case in which \(X_{Z}^{} X_{Y Z}\) are direct causes of the label \(Y\) (_causal task_) e.g. estimating the helpfulness of a text review, and the case in which \(Y\) is a direct cause of \(X_{Z}^{} X_{Y Z}\) (_anti-causal task_) as in object detection tasks in computer vision. Figures 1(a-b) display examples of anti-causal and causal tasks with a purely spurious dependence

[MISSING_PAGE_FAIL:4]

Models trained with confounded data (95/10) display biased outputs (Table 2), with low worst group performance and high equalized odds. Performance on \(P^{0}\) is also lower compared to that on \(P^{t}\) (\(0.937 0.002\)), showing that these models are not risk-invariant w.r.t. \(\). Models trained from balanced data obtain high overall performance and worst group accuracy, as well as low equalized odds. In addition, we were not able to decode \(Z\) from the model representation \((X)\) at the penultimate layer, suggesting that the model has not learned \(X_{Y}^{}\).

**Causal task: helpfulness of reviews with Amazon reviews **. Inspired by Veitch et al. , we refer to the causal task of predicting the helpfulness rating of an Amazon review (thumbs up or down, \(Y\)) from its text (\(X\)). We add a synthetic factor of variation \(Z\) such that words like 'the' or'my' are replaced by 'thexxxx' and'myxxxx' (\(Z=0\)) or 'theyyyy' and'myyyyy' (\(Z=1\)). We train a BERT  model on a class-balanced version of the data for reference (due to high class imbalance), and compare to a model trained on jointly balanced data, both evaluated on their training distribution and on a distribution \(P^{0}\) with no association.

In this case, jointly balancing improves fairness and risk-invariance, with the model's performance on the training distribution (acc.: \(0.574 0.016\)) being similar to that on \(P^{0}\) (Table 2). This however comes at a high performance cost when compared to the class balanced model's performance on \(P^{t}\) (acc: \(0.658 0.015\)). Therefore, data balancing might not lead to optimality for this causal task.

Using the same framework, we can replicate the failure modes due to another confounder described in Wang et al. , Alabdulmohsin et al.  as well as that from Puli et al. .

**Anti-causal task with another factor of variation \(V\).** It is common for multiple auxiliary factors to influence the data generating process, and they tend to correlate with each other [e.g. 21]. To emulate this case, we introduce more unobserved variables \(U_{2},U_{3}\) as well as a factor of variation \(V\) which affects the data through \(X_{V}\) (Figure 1(c)). We modify the MNIST data generation to include \(X_{V}\) depicted by a green cross on the top left or top right of the image and jointly balance the data on (\(Y,Z\)) before training the model. We evaluate the obtained predictor on a distribution where \(V\) and \(Z\) are not correlated with \(Y\) and observe (Table 2) a large gap between worst group accuracy and overall performance, as well as non-null equalized odds. These results suggest that the model is not fair or robust.

**Anti-causal task with entangled data.** We map the work in Puli et al.  to our decomposition of \(X\) and propose the example graph in Figure 1(d) where \(X_{Y Z}\) represents an entangled function of \(X\). To match this data generating process, the color of the noise in MNIST samples is defined by \((Y,Z)\) and the evaluation distribution is the disentangled \(P^{0}\) with no dependence between \(Y\) and \(Z\). Once again, the obtained model is not fair, robust or optimal (Table 2). Appendix A.2 discusses this case further.

Motivated by these examples of both success and failures, we define conditions for the success of data balancing, and highlight when the cases above fail to meet these conditions.

## 4 Conditions for data balancing to produce an invariant and optimal model

In this section, we introduce a sufficient condition on the data generative process and a necessary condition on the trained model that, taken together, lead to a risk-invariant and optimal prediction model after training on \(Q\) (proofs in Appendix B.1). In Appendix B.2, we derive similar conditions

    &  &  & \)} & \)} \\  & & & Acc. (\(\)) & Acc. (\(\)) & Worst Grp (\(\)) & Encoding (\( 0.5\)) & Equ. Odds (\(\)) \\   & & (a) & 95/10 & \(0.937 0.002\) & \(0.717 0.027\) & \(0.380 0.062\) & \(0.996 0.004\) & \(0.539 0.015\) \\ Figure & 1: & (a) & Balanced & \(0.871 0.008\) & \(0.880 0.006\) & \(0.836 0.075\) & \(0.486 0.005\) & \(0.018 0.008\) \\  MNIST &  & (b) & Class bal. & \(0.658 0.015\) & \(0.558 0.015\) & \(0.092 0.015\) & \(0.690 0.113\) & \(0.542 0.098\) \\  & & (b) & Jointly bal. & \(0.574 0.016\) & \(0.583 0.017\) & \(0.399 0.014\) & \(0.545 0.037\) & \(0.600 0.046\) \\   & & (c) & With l & \(0.769 0.001\) & \(0.647 0.023\) & \(0.555 0.031\) & \(0.665 0.134\) & \(0.094 0.035\) \\  & & (d) & Entangled & \(0.903 0.011\) & \(0.672 0.004\) & \(0.000 0.001\) & \(0.881 0.223\) & \(0.554 0.028\) \\   

Table 2: Model performance on semi-synthetic data, for the tasks in Figure 1. ‘Acc’ refers to accuracy, ‘Worst Grp’ to worst group accuracy, ‘Encoding’ to confounder encoding as measured by transfer learning and ‘Equ. Odds’ refers to equalized odds between \(Z\) subgroups. \(\) (resp. \(\)) means the higher (resp. lower), the better.

for fairness criteria. Throughout the rest of the paper, we use an subscript to indicate under which of \(P^{t}\) or \(Q\) a statistical independence holds, e.g. \(Y\!\!\!_{P^{t}}Z\) to indicate \(P^{t}(Y\,|\,Z)=P^{t}(Y)\).

We consider the criterion of risk-invariance (Definition 2.2) under correlation shift, i.e. \(=\{P^{}(X,Y,X)=P^{t}(X|Y,Z)P^{}(Z|Y)P^{t}(Y)\}\). According to our decomposition of \(X\), the risk-minimizing function \(f(X):=_{Q}[Y\,|\,X]\) should only be a function of \(X_{Z}^{}\) and not of \(X_{Y}^{}\) or \(X_{Y Z}\). To achieve this result with data balancing, we build on a prior result by Makar et al. , which shows that a model trained on a balanced distribution only depends on \(X_{Z}^{}\) if \(X_{Z}^{}\) represents a _sufficient statistic_ for \(Y\), i.e. no other part of \(X\) influences \(Y\).

**Definition 4.1**.: (Sufficient Statistic) We say that \(X_{Z}^{}\) is a sufficient statistic for \(Y\) in \(Q\) if \(_{Q}[Y\,|\,X]=_{Q}[Y\,|\,X_{Z}^{}]\) (note that \(X_{Z}^{}\) is a function of \(X\)).

Definition 4.1 implies that the risk-minimizing function \(f\) for \(Q\) does not vary with \(X_{Y}^{},X_{Y Z}\). However, this condition is not sufficient on its own to ensure that \(f\) is risk-invariant w.r.t. \(\), as \(X_{Z}^{}\) or \(Y\) may have non-causal relationships with \(Z\). To ensure optimality and risk-invariance w.r.t. \(\), we derive the sufficient condition in Proposition 4.2.

**Proposition 4.2**.: _If \(X_{Z}^{}_{Q}Z\,|\,Y\) and \(X_{Z}^{}\) is a sufficient statistic for \(Y\) in \(Q\), then the risk-minimizer \(f(X):=_{Q}[Y\,|\,X]\) is risk-invariant and optimal w.r.t. \(\)._

The conditions of Proposition 4.2 concern \(Q\). However, it would be of interest to express them in \(P^{t}\) if it is possible to observe all covariates (e.g. in the case of tabular data). Based on our expression for \(Q\), we can derive sufficient conditions on \(P^{t}\), expressed in Corollary 4.3. Let's denote \(\{X_{Y}^{},X_{Y Z}\}\) by \(R\).

**Corollary 4.3**.: _If \(R\!\!\!_{P^{t}}\{Y,X_{Z}^{}\}\,|\,Z\) and \(X_{Z}^{}\!\!\!_{P^{t}}Z\,|\,Y\), then the risk-minimizer \(f(X):=_{Q}[Y\,|\,X]\) is risk-invariant and optimal w.r.t. \(\)._

In general, we can expect that anti-causal tasks with purely spurious correlations will satisfy these conditions, as per their definition. However, this would not be the case for most causal tasks as \(X_{Z}^{}_{P^{t}}Z\,|\,Y\). This result is in line with our findings in Section 3, as the MNIST data generated from the graph in Figure 1(a) validates Corollary 4.3, but the Amazon reviews data generated from Figure 1(b) does not.

It may be less obvious, but the conditions for a sufficient statistic are not met in Figures 1(c,d) as \(X_{V}_{P^{t}}\{Y,X_{Z}^{}\}\,|\,Z\) in the case of another factor of variation \(V\), and \(X_{Y Z}_{P^{t}}\{Y,X_{Z}^{}\}\,|\,Z\) in the case of entangled data. We hence see that when a causal graph of the application is available, Corollary 4.3 can provide indicators on when data balancing might succeed or fail, with the caveat that it is not a necessary condition.

While Proposition 4.2 and Corollary 4.3 provide conditions on the data generating process, prior work (e.g. 10, 31) has demonstrated that the learning strategy also influences the model's fairness and robustness characteristics.

**Proposition 4.4**.: _Let \(\) be some fitted model and \(>0\). Assume that, for all \(P^{},P^{}\), we have \(|_{P^{}}[Y(X,Y)]-_{P^{}}[Y X _{Z}^{}]|\). Then \(\) is \(\)-risk invariant, meaning that_

\[_{P^{},P^{}}R_{P^{}}()-R_{P^{ }}().\]

Proposition 4.4 states that the learned function \(\) needs to be nearly optimal over \(\). This statement, while straightforward, implies that (i) \((X)\) needs to preserve all the information about the expectation of \(Y\) in \(X_{Z}^{}\), and that (ii) \((X)\) changes with \(X_{Y}^{}\) or \(X_{Y Z}\) only marginally. Let's rewrite \((X)=h((X))\), where \(h\) is'simple' function and \((X)\) is a model representation. This case could correspond to the last layers of a neural network or when learning a model based on a representation \((X)\) (e.g. embeddings, transfer learning). Based on Proposition 4.4, \((X)\) must be disentangled in the sense that the simple function \(h\) eliminates any dependence on \(X_{Y}^{}\) or \(X_{Y Z}\). For example, if \(h\) is a linear function, it must be possible to linearly project out all dependence on \(X_{Y}^{}\) and \(X_{Y Z}\). We note that such a representation can be obtained even if the data is entangled, e.g. by dropping modes of variation during training. Unlike other strategies , data balancing cannot enforce this property on its own and a disentangled representation would be necessary. This condition hence suggests another failure mode of data balancing when the conditions on the data are validated, but the representation is of low quality. We believe this failure mode is displayed in Kirichenko et al. , as the success of their data balancing mitigation only holds when using models pre-trained on large datasets.

In this section, we have identified conditions for data balancing to be successful. In the next section, we go one step further to understand how data balancing impacts the data generating process, and how it interacts with other mitigation strategies for undesired dependencies, focusing on regularization.

## 5 Impact of data balancing on the CBN

Joint data balancing is assumed to _remove_ statistical dependence between \(Y\) and \(Z\) while keeping other relationships in the CBN of the task unaffected (e.g. 48; 77; 14). This could be interpreted as 'dropping' edges in the undesired paths in \(\), e.g. removing the influence of \(U\) on \(Y\) and/or \(Z\) in Figure 1(a), leading to a new graph \(^{0}\). While this interpretation is correct for joint balancing in the case of Figure 1(a), Proposition 5.1 below (proof in Appendix C) shows that it can be erroneous in general: the distribution \(Q\) underlying the balanced data might not factorize according to \(^{0}\) and therefore might not obey the statistical dependence relationships implied by \(^{0}\). Therefore, balancing data to make \(Z\) and \(Y\) statistically independent, i.e. selecting samples in proportion to \(P^{t}(Z)P^{t}(Y)/P^{t}(Z,Y)\), is not equivalent to generating data from a distribution that factorises according to \(^{0}\) in general. This factorization is important because downstream distributions \(P^{}(X,Y,Z)\) are often assumed to follow this factorization; in fact, this assumption underlies a number recommendations for applying regularization methodologies such as in .

**Proposition 5.1**.: _Let \(,P^{t}\) be the CBN underlying the data, where \(\) contains an undesired path between \(Z\) and \(Y\), and let \(^{0}\) be a modification of \(\) in which the undesired path has been removed. The distribution \(Q\) obtained by jointly balancing the data need not factorize according to \(^{0}\)._

Proposition 5.1 shows that statistical (in)dependencies that we assumed would remain fixed (i.e. the black edges on the graph) can be modified by the process of joint balancing. As a consequence, further interventions on \(Q\) (e.g. the addition of a regularizer) should not be motivated by \(^{0}\), and we show below that combining data balancing with other mitigation strategies can lead to unexpected results.

### Data balancing can hinder regularization and vice-versa

When confronted with a failure mode, it is reasonable to ask whether an additional fairness or robustness regularizer on the training loss might be beneficial. Based on Proposition 5.1, we see that this question might have a different answer if we are in \(P^{t}\) or in \(Q\). Below, we consider each failure mode and ask whether performing an additional regularization motivated by the literature would mitigate the undesired dependencies in \(Q\). The results are summarized in Table 1, with suggested next steps. In Appendix C.1.2, we discuss when balancing with regularization is sufficient for different fairness criteria.

**Anti-causal task.** In the case of an anti-causal task with a dependence between \(Y\) and \(Z\) (Figures 1(a,c,d)), Veitch et al.  recommend to impose an independence between \(f(X)\) and \(Z\) conditioned

Figure 2: Accuracy across different values of the MMD hyper-parameter for models trained on balanced data and evaluated on their respective training distribution (dashed) and \(P^{0}\) (solid line) averaged across replicates. We consider anti-causal tasks: (left) purely spurious case, (middle) when another confounder \(V\) is present, and (right) the entangled dataset. Worst group performance on \(P^{0}\) is displayed in red. Markers display individual replicates.

on \(Y\). If we consider both the purely spurious correlation and the entangled case, we see that regularization and data balancing would have the same effects of blocking any dependence between \(\{Y,X_{}^{}\}\) and \(\{Z,X_{}^{},X_{Y Z}\}\). We demonstrate that \(X_{}^{} Z\,|\,Y\) in both \(P^{t}\) and \(Q\) (see Appendix C.1), and this regularization is sensible under both distributions. This means that performing the regularization provides the sufficient conditions for a risk-invariant model, whether or not joint data balancing is performed. In theory, data balancing is not needed but is also not harmful. In the case of an added confounder, we have that \(X_{V}\) depends on both \(Y\) and \(Z\) due to non-causal paths through \(V\). Therefore, imposing that \(f(X)_{Q}Z Y\) might lead to results whereby the model only depends on \(V\) or is trivial (e.g. predicts a constant) as the regularization encourages the removal of any dependence on \(Z\), which is related to \(Y\) via \(X_{V}\). This behavior would be observed in both \(P^{t}\) and \(Q\), but data balancing on its own might be less detrimental than regularization in terms of predictive power even though it does not resolve all undesired dependencies. In this case, regularization hinders data balancing.

Based on the balanced data from Section 3, we add a conditional Maximum Mean Discrepancy [MMD, 28] to encourage \(f(X)_{Q}Z Y\) during training, varying the strength of this regularizer via a hyper-parameter. In the case of the purely spurious statistical dependence between \(Y\) and \(Z\) (Figure 1(a)), there is little variation between the metrics across MMD strengths, and the model is fair and robust (Figure 2(left)). In the entangled case (Figure 2(right)), the model's performance on \(Q\) and \(P^{0}\) are close for medium values of the hyper-parameter (before MMD overpowers the training) and worst group performance improves markedly. This result suggests that, with the added regularizer, \(f\) only varies with \(X_{Z}^{}\). Performing the same regularization in the presence of another confounder (Figure 2(middle)) leads to a plateau in performance on \(Q\), but low performance on \(P^{0}\) and chance-level worst group performance. In this case, we posit that the model relies exclusively on \(X_{V}\) for its predictions, and the regularizer is detrimental compared to data balancing on its own (MMD=0 on the plot).

**Causal task.** Finally, let us consider the causal task in Figure 1(b). In a similar case, Veitch et al.  suggests a regularizer such that \(f(X)_{P^{t}}Z\), which would encourage the model \(f(X)\) to vary only with \(X_{Z}^{}\) as \(X_{Z}^{}_{P^{t}}Z\). However, data balancing induces a dependence between \(X_{Z}^{}\) and \(Z\), as expressed below:

\[Q(X_{Z}^{}\,|\,Z)=^{},Y}P^{t}(X_{Z}^{},X_{Y}^ {}\,|\,Z,Y)P^{t}(Z)P^{t}(Y)}{_{X_{Y}^{},X_{Z}^{},Y}P^{t}(X _{Z}^{},X_{Y}^{}\,|\,Z,Y)P^{t}(Z)P^{t}(Y)}=_{Y}P^{t}(X_{Z}^{ }\,|\,Z,Y)P^{t}(Y),\]

The RHS cannot be simplified further because \(X_{Z}^{}_{P^{t}}Z Y\), because \(Y\) is a collider under \(P^{t}\). Thus, the left hand side is a function of \(Z\) in general (see Appendix C.1 for further details and a numerical simulation). In this case, regularizing to enforce \(f(X)_{Q}Z\) would destroy information in \(X_{Z}^{}\), whereas the same regularization under \(P^{t}\) would have enabled \(f(X)\) to use all of the information in \(X_{Z}^{}\). Therefore, data balancing may hinder regularization.

We illustrate this result on the Amazon reviews dataset from Section 3 by imposing a marginal MMD regularization \(f(X) Z\) during training and evaluating risk-invariance across multiple \(P^{}\). When training on \(P^{t}\), we observe that the regularization allows to 'flatten' the curve, such that from

Figure 3: Accuracy across different values of the confounder strength (i.e. different \(P^{}\)), for each value of MMD regularization considered (displayed by the color gradient). (a) Models trained on \(P^{t}\). (b) Models trained on \(Q\). Results are averaged across seeds for clarity. Notice the different y-scales. (c) Displays the mean and standard deviation across seeds for MMD=16.

medium to high values of MMD regularization, the model is risk-invariant (Figure 3(a)). On the jointly balanced data, medium values of the regularization degrade risk-invariance (see green curves on Figure 3(b)). Overall, model performance is also lower for the models trained on \(Q\) compared to models trained on \(P^{t}\) across test sets from \(P^{}\), at similar levels of regularization (see Figure 3(c) for MMD=16). This result displays that \(X_{Z}^{}\) is not a sufficient statistic for \(Y\) in \(Q\).

## 6 Case study: distinguishing between failure modes in CelebA

In this section, we show that when \(Y\) and \(Z\) are available at training time, we can try to distinguish between failure modes of data balancing by using our different observations, even in the absence of a full causal graph. We illustrate this using the benchmark task of detecting blond hair in pictures of celebrities in the CelebA  dataset. This label has a strong correlation with perceived gender: half of the non-males have blond hair, while only \( 7\%\) of males do. We consider a balanced, subsampled dataset (train: \(n=4,096\), test/valid: \(n=400\)) and the original, confounded dataset. We train a VGG  and four Vision Transformer [ViT, 18] architectures, with number of parameters ranging from 17 to 690 millions.

We observe that, while training with balanced data leads to higher worst group accuracy and lower equalized odds scores than training with the historical data (Table 3), an important gap remains between the overall and worst group performances. These results show that data balancing leads to improvements in downstream fairness and robustness metrics, but does not provide a risk-invariant or fair model on its own. Therefore, it is likely that one of the conditions for data balancing to be sufficient is not fulfilled and understanding which condition is violated can guide our selection of another technique.

**Distinguishing between failure modes.** We first assume that the task is anti-causal. We then aim to understand whether there is another confounder, the data is entangled, or the representation is entangled (Proposition 4.4). As per Kirichenko et al. , we first attempt to improve our representation by pre-training the VGG with ImageNet . While we observe an increase in performance with pre-training, there is no clear decrease in equalized odds. This result suggests that the failure may lie elsewhere. We then train models with MMD on \(P^{t}\), with the expectation that we would observe a plateau for entangled data when the model learns \(f(X_{Z}^{})\), or a stark decrease in worst group performance in the presence of another confounder. While there is no major pattern of correlation between \(Y\) and another attribute in the balanced data (see Appendix E.2.2), small effects might combine, or there might be other, unobserved attributes that influence \(Y\). For a medium value of the regularization hyper-parameter, the model displays a plateau in performance and poor worst group performance. This result suggests an effect of another confounder and next steps can include methods such as Alabdulmohsin et al. , which controls for all (observed) auxiliary factors of variation.

## 7 Related works

**Balanced data as mitigation for invariant models.** Our results extend those of Makar et al.  which considered a single causal graph. Wang et al.  displayed that balancing data did not lead to a reduction in bias amplification. The authors posit that this failure of balanced data to correct for spurious signals is due to unobserved confounding factors which is confirmed in Alabdulmohsin et al. . Rolf et al.  investigated upsampling by relying on a scaling law per group, focusing on the question of fairness vs performance trade-off . Focusing on causal NLP settings, Joshi et al.  investigated causal and non-causal features, concluding that data balancing does not help in all cases. Closer to our work is that of Puli et al. , in which the authors showed that having \(Y\!\!\!_{Q}Z\) does not imply that \(Y\!\!\!_{Q}Z X\) and the model can learn signals related to \(Z\). Puli et al.  propose a method to learn a representation \(r\) such that \(Y\!\!\! Z r(X)\). Our work provides a framework to understand these different failure modes and proposes strategies to distinguish between them. While we focus on pre-processing mitigation with a fixed distribution \(Q(X,Y,Z)\), another line of work considers dynamic resampling in-processing [e.g. 35; 61; 12]. As the resampling converges towards a fixed distribution \(P^{}(Z|Y)\), we would expect failure modes in the presence of entangled data or of another confounder. Nevertheless, the variation in \(P^{}(Z|Y)\) at the early stages of training might be beneficial, e.g. by disentangling the representation. We leave this investigation for future work.

**Causal feature selection.** Some works have used a causal framing to select features such that \(f(X)\) has robustness and/or fairness properties [e.g. 47; 71; 69; 25; 67; 38]. Similarly, our work defines independence conditions on covariates to obtain an optimal, invariant model, and can be used to select features. Two major distinctions between feature selection works and ours reside in the fact that we consider the case in which we do not observe \(X_{}^{}\) explicitly and that we investigate the impact of data balancing.

## 8 Discussion

In this work, we uncover important results to guide the use of data balancing for mitigating undesired dependencies between covariates, outcomes and auxiliary factors of variation. We first show (Section 3) that joint data balancing might not achieve the desired fairness or robustness criteria, and that the failures may seem difficult to predict. Motivated by these results, we introduce conditions under which data balancing leads to a robust or fair model (Sections 4, B.2). Importantly, we show that data balancing is not equivalent to 'dropping an edge' in the causal graph and can lead to distributions that do not factorize according to the desired graph (Section 5). This can have downstream consequences if further mitigation strategies are motivated by the causal graph and highlights why regularization and data balancing might not go 'hand in hand'. This last result shows that data balancing should not be performed as a 'default', and mitigation strategies should be based on the causal graph of the application. Finally, even in the absence of a causal graph, our findings may help to pinpoint which condition(s) are not fulfilled, and guide further mitigation (Section 6).

**Limitations.** The conditions defined in Section 4 for risk-invariance depend on the expression of \(\) as a correlation shift [48; 62]. Other expressions or shifts are likely to lead to other conditions. In our experiments, we have mostly subsampled datasets to obtain balanced distributions. We would expect similar results for other joint balancing methods. Variations are, however, possible due to the finite-set nature of the computations , e.g. with reweighting displaying more variance , potentially under-performing in overparametrized settings [11; 65]. We also note that, while we aimed to provide upper bounds for the effectiveness of data balancing, we did not use additional training strategies for mitigation beyond regularization. We believe that our causal framework can be a useful tool to analyze other pre- or in-processing methods that enforce independence between variables in the data generating process [e.g. 1; 58]. On the other hand, our framework might not be suited to analyze the effects of other mitigation strategies, e.g. hyper-parameter optimization . We discuss the broader societal impacts of our work in Appendix E.2.2.

**Future work.** This work considered a variety of causal graphs in order to provide general insights rather than task-specific conditions. However, investigating specific graphs could enable to leverage further strategies including other balancing techniques [e.g. 38; 72]. We believe that our causal framing could then be a useful resource to analyze the effect of these strategies on downstream fairness and robustness criteria. Finally, we illustrate our propositions with binary classification tasks and confounders. While our reasoning applies to more complex settings, there might be further considerations to account for when generalizing beyond binary variables, especially with respect to estimation.