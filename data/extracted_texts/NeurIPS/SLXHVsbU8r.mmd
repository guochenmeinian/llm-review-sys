# End-to-End Autonomous Driving without Costly Modularization and 3D Manual Annotation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We propose UAD, a method for vision-based end-to-end autonomous driving (E2EAD), achieving the best open-loop evaluation performance in nuScenes, meanwhile showing robust closed-loop driving quality in CARLA. Our motivation stems from the observation that current E2EAD models still mimic the modular architecture in typical driving stacks, with carefully designed **supervised** perception and prediction subtasks to provide environment information for oriented planning. Although achieving groundbreaking progress, such design has certain drawbacks: 1) preceding subtasks require massive high-quality 3D annotations as supervision, posing a significant impediment to scaling the training data; 2) each submodule entails substantial computation overhead in both training and inference. To this end, we propose UAD, an E2EAD framework with an **unsupervised1** proxy to address all these issues. Firstly, we design a novel Angular Perception Pretext to eliminate the annotation requirement. The pretext models the driving scene by predicting the angular-wise spatial objectness and temporal dynamics, without manual annotation. Secondly, a self-supervised training strategy, which learns the consistency of the predicted trajectories under different augment views, is proposed to enhance the planning robustness in steering scenarios. Our UAD achieves 38.7% relative improvements over UniAD on the average collision rate in nuScenes and surpasses VAD for 6.40 points on the driving score in CARLA's Town05 Long benchmark. Moreover, the proposed method only consumes 44.3% training resources of UniAD and runs \(3.4\) faster in inference. Our innovative design not only for the first time demonstrates unarguable performance advantages over supervised counterparts, but also enjoys unprecedented efficiency in data, training, and inference.

## 1 Introduction

Recent decades have witnessed breakthrough achievements in autonomous driving. The end-to-end paradigm, which seeks to integrate perception, prediction, and planning tasks into a unified framework, stands as a representative branch . The latest advances in end-to-end autonomous driving significantly piped researchers' interest . However, handcrafted and resource-intensive supervised sub-tasks for perception and prediction, which have previously proved their utility in environment modeling , continue to be indispensable, as shown in Fig. 1a.

Then what insights have we gained from the recent advances? It has come to our attention that one of the most enlightening innovations lies in the Transformer-based pipeline, in which the queries act as a connective thread, seamlessly bridging various tasks. Besides, the capability for environment modeling has also seen a significant boost, primarily due to complicated interactions of supervisedsub-tasks. However, every coin has two sides. In comparison to the vanilla design  (see Fig. 1a), modularized methods incur unavoidable computation and annotation overhead. As illustrated in Fig. 1b, the training of the recent method UniAD  takes 48 GPU days while running at only 2.1 frames per second (FPS). Moreover, modules in existing perception and prediction design require large quantities of high-quality annotated data. The financial overhead for human annotation significantly impedes the scalability of such modularized methods with supervised subtasks to leverage massive data. As proved by large foundation models [24; 31], scaling up the data volume is the key to bringing the model capabilities to the next level. Thus we ask ourselves the question: _Is it viable to devise an efficient and robust E2EAD framework while alleviating the reliance on 3D annotation?_

In this work, we show the answer is affirmative by proposing an innovative **U**nsupervised pretext task for end-to-end **A**utonomous **D**riving (**UAD**), which seeks to efficiently model the environment. The pretext task consists of an angular-wise perception module to learn _spatial_ information by predicting the objectness of each sector region in BEV space, and an angular-wise dreaming decoder to absorb _temporal_ knowledge by predicting inaccessible future states. The introduced angular queries link the two modules as a whole pretext task to perceive the driving scene. Notably, our method shines by completely eliminating the annotation requirement for perception and prediction. Such data efficiency is not attainable for current methods with complex supervised modularization [21; 22]. The supervision for learning spatial objectness is obtained by projecting the 2D region of interests (ROIs) from an off-the-shelf open-set detector  to BEV space. While utilizing the publicly available open-set 2D detector pre-trained with manual annotation from other domains (_e.g._ COCO ), we avoid the need for any additional 3D labels within our paradigm and target domains (_e.g._ nuScenes  and CARLA ), thereby creating a pragmatically unsupervised setting . Furthermore, we introduce a self-supervised direction-aware learning strategy to train the planning model. Specifically, the visual observations are augmented with different rotation angles, and the consistency loss is applied to the predictions for robust planning. Without bells and whistles, the proposed UAD outperforms UniAD for 0.13m in nuScenes Avg. L2 error, and surpasses VAD  for 9.92 points in CARLA route completion score. Such unprecedented performance gain is achieved with a \(3.4\) inference speed, a mere 44.3% training budget of UniAD, and zero annotations, as illustrated in Fig. 1.

In summary, our contributions are as follows: **1)** We propose an unsupervised pretext task to discard the requirement of 3D manual annotation in end-to-end autonomous driving, potentially making it more feasible to scale the training data to billions level without any labeling overload; **2)** We introduce a novel self-supervised direction-aware learning strategy to maximize the consistency of the predicted trajectories under different augment views, which enhances planning robustness in steering scenarios; **3)** Our method shows superiority in both open- and closed-loop evaluation compared with other vision-based E2EAD methods, with much lower computation and annotation cost.

## 2 Related Work

### End-to-End Autonomous Driving

End-to-end autonomous driving can be dated back to 1988, when the ALVINN  proposed by Carnegie Mellon University could successfully navigate a vehicle over 400 meters. After that, to

Figure 1: **(a)** End-to-end autonomous driving paradigms. 1) The vanilla architecture that directly predicts control command. 2) The modularized design that combines various preceding tasks. 3) Our proposed framework with unsupervised pretext task. **(b)** Comparison of training cost, inference speed and average L2 error between our method and [21; 22] on 8 NVIDIA Tesla A100 GPUs.

improve the robustness of E2EAD, a series of modern approaches such as NEAT , P3 , MP3 , ST-P3  introduce the design of more dedicated modularization, which integrate auxiliary information such as HD maps, and additional tasks like bird's-eye view (BEV) segmentation. Most recently, embracing advanced architectures like Transfromer  and visual occupancy prediction , UniAD  and VAD  demonstrate impressive performance in open-loop evaluation. In this work, instead of integrating complex supervised modular sub-tasks, we innovatively propose another path proving that an efficient unsupervised pretext task without any human annotation like 3D bounding boxes and point cloud categories, can achieve even superior performance than recent state-of-the-arts.

### World Model

In pursuit of understanding the dynamic changes in environments, researchers in the fields of gaming and robotics have proposed various world models [13; 14; 15; 16]. Recently, the autonomous driving community introduces world models for safer maneuvering [32; 18; 12; 38]. MILE  considers the environment as a high-level embedding and tends to predict its future state with historical observations. Drive-WM  proposes a framework to integrate world models with existing E2E methods to improve planning robustness. In this work, we propose an auto-regressive mechanism, tailored to our unsupervised pretext, to capture angular-wise temporal dynamics within each sector.

## 3 Method

### Overview

As illustrated in Fig. 2, our **UAD** framework consists of two essential components: 1) the Angular Perception Pretext, aims to liberate E2EAD from costly modularized tasks in an unsupervised fashion; 2) the Direction-Aware Planning, learns self-supervised consistency of the augmented trajectories.

Specifically, UAD first models the driving environment with the pretext. The _spatial_ knowledge is acquired by estimating the objectness of each sector region within the BEV space. The angular queries, each responsible for a sector, are introduced to extract features and predict the objectness. The supervision label is generated by projecting the 2D regions of interests (ROIs) to the BEV space, which are predicted with an available open-set detector GroundingDINO . This way not only eliminates the 3D annotation requirement, but also greatly reduces the training budget. Moreover, as driving is inherently a dynamic and continuous process, we thus propose an angular-wise dreaming decoder to encode the _temporal_ knowledge. The dreaming decoder can be viewed as an augmented world model  capable of auto-regressively predicting the future states.

Subsequently, direction-aware planning is introduced to train the planning module. The raw BEV feature is augmented with different rotation angles, yielding rotated BEV representations and ego trajectories. We apply self-supervised consistency loss to the predicted trajectories of each augmented view, which is expected to improve the robustness for directional change and input noises. The learning strategy can also be regarded as a novel data augmentation technique customized for end-to-end autonomous driving, which enhances the diversity of trajectory distribution.

Figure 2: The architecture of our UAD. The inference pipeline is marked by black arrows with blue background, which plans ego trajectory based on the input multi-view images. The training pipeline consists of Angular Perception Pretext (orange arrows with khaki background) and Direction-Aware Planning (orange arrows with purple background). “F” in BEV feature indicates the driving direction.

### Angular Perception Pretext

**Spatial Representation Learning.** Our model attempts to acquire spatial knowledge of the driving scene by predicting the objectness of each sector region within the BEV space. Specifically, taking multi-view images \(\{_{i}\!\!^{H_{i}W_{i}\!\!3}\}\) as input, the BEV encoder  first extracts visual information into the BEV feature \(_{}\!\!^{H_{} W_{}  C}\). Then, \(_{}\) is partitioned into \(K\) sectors with a uniform angle \(\) centered around ego car. Each sector contains several feature points in BEV space. Denoting feature of a sector as \(\!\!^{N C}\), where \(N\) is the maximum number of feature points in all sectors, we derive angular BEV feature \(_{}\!\!^{K N C}\). Zero-padding is applied on sectors with fewer than \(N\) points.

Then why do we partition the rectangular BEV feature to angular-wise formatting? The underlying reason is that, in the absence of depth information, the region in BEV space corresponding to an ROI in 2D image is a sector. As illustrated in Fig. 2(a), by projecting 3D sampling points to images and verifying their presence in 2D ROIs, a BEV object mask \(\!\!^{H_{} W_{}\!1}\) is generated, representing the objectness in BEV space. Specifically, the sampling points falling within 2D ROIs are set to 1, while the others are 0. It is noticed that the positive sectors are irregularly and sparsely distributed in BEV space. To make the objectness label more compact, similar to the BEV feature partition, we uniformly divide \(\) into \(K\) equal parts. The segments overlapped with positive sectors are assigned with 1, constituting the angular objectness label \(_{}\!\!^{K\!\!1}\). Thanks to the rapid development of open-set detection, it's now convenient to obtain 2D ROIs for the input multi-view images by feeding the pre-defined prompts (_e.g._, vehicle, pedestrian, and barrier) to a 2D open-set detector like GroundingDINO . Such design is the key in reducing annotation cost and scaling up the dataset.

To predict the objectness score of each sector, we define angular queries \(_{}\!\!^{K\!\!C}\) to summarize \(_{}\). Each angular query \(_{}\!\!^{1\!\!C}\) in \(_{}\) will interact with corresponding \(\) by cross attention ,

\[_{}=(_{},\, ),\] (1)

Finally, we map \(_{}\) to the objectness scores \(_{}\!\!^{K 1}\) with a linear layer, which is supervised by \(_{}\) with binary cross-entropy loss (denoted as \(_{}\)).

**Temporal Representation Learning.** We propose to capture the temporal information of driving scenarios with the angular-wise dreaming decoder. As shown in Fig. 2(b), the decoder auto-regressively learns transition dynamics of each sector in a similar way of world model . Assuming the planning module predicts the trajectories of future \(T\) steps, the dreaming decoder accordingly comprises \(T\) layers, where each updates the input angular queries \(_{}\) and angular BEV feature \(_{}\) based on the learned temporal dynamics. At step \(t\), the queries \(_{}^{t-1}\) first grasp environmental dynamics from the observation feature \(_{}^{t}\) with a gated recurrent unit (GRU) , which generates \(_{}^{t}\) (hidden state),

\[_{}^{t}=(_{}^{t-1},_{}^{t}),\] (2)

In previous world models, the hidden state \(\) is solely used for perceiving observed scenes. The GRU iteration thus ends at \(t\) with the final observation \(_{}^{t}\). In our framework, \(\) is also used for predicting ego trajectories in the future. Yet, the future observation, _e.g._, \(_{}^{t+1}\), is unavailable, as the world model  is designed for forecasting the future with only current observation. To obtain \(_{}^{t+1}\), we first propose to update \(_{}^{t}\) to provide pseudo observations \(}_{}^{t+1}\),

\[}_{}^{t+1}=(_{ }^{t},\,_{}^{t}).\] (3)

Then \(_{}^{t+1}\) can be generated with Eq. 2 and inputs of \(}_{}^{t+1}\) and \(_{}^{t}\).

Following the loss design in world models [14; 15; 16], we respectively map \(_{}^{t-1}\) and \(_{}^{t}\) to distributions of \(\{_{}^{t-1},_{}^{t-1}\!\!^{K C}\}\) and \(\{_{}^{t},_{}^{t}\!\!^{K C}\}\), and then minimize their KL divergence.

Figure 3: **(a) Label generation for angular perception pretext. (b) Illustration of dreaming decoder.**For the prior distribution from \(_{}^{t-1}\), it's regarded as a prediction of the future dynamics without observation. In contrast, the posterior distribution from \(_{}^{t}\) represents the future dynamics with the observation \(_{}^{t}\). The KL divergence between the two distributions measures the gap between the imagined future (prior) and the true future (posterior). We expect to enhance the capability of future prediction for long-term driving safety, which is realized by optimizing the dreaming loss \(_{}\),

\[_{}=(\{_{}^{ t},_{}^{t}\}||\{_{}^{t-1},_{}^{t-1}\}),\] (4)

### Direction-Aware Planning

**Planning Head.** The outputs of angular perception pretext contain a group of angular queries \(\{_{}^{t}\,(t=1,...,T)\}\). For planning, we correspondingly initialize \(T\) ego queries \(\{_{}^{t}\!\!^{1 C}\,(t=1,...,T)\}\) to extract planning-relevant information and predict the ego trajectory of each future time step. The interaction between ego queries and angular queries is performed with cross attention,

\[_{}^{t}=(_{ }^{t},\,_{}^{t}).\] (5)

The output ego queries \(\{_{}^{t}\}\) are then used to predict the ego trajectories of future \(T\) steps. Following previous works [21; 22], a high-level driving signal \(c\) (_turn left_, _turn right_ or _go straight_) is provided as prior knowledge. The planning head takes the concatenated ego feature \(_{}\!\!^{T C}\) from \(\{_{}^{t}\}\) and the driving command \(c\) as inputs, and outputs the planning trajectory \(_{}\!\!^{T 2}\),

\[_{}=(_{},c),\] (6)

where the \(\) is the same as UniAD . We apply \(_{1}\) loss to minimize the distance between the predicted ego trajectory \(_{}\) and the ground truth \(_{}\), denoted as \(_{}\). Notably, \(_{}\) is easy to obtain, and manual annotation is not required in practical scenarios.

**Directional Augmentation.** Observed that the training data is predominated by the _go straight_ scenarios, we propose a directional augmentation strategy to balance the distribution. As shown in Fig. 4, the BEV feature \(_{}\) is rotated with different angles \(r\!\!R\!=\!\{90^{},180^{},270^{}\}\), yielding the rotated representations \(\{_{}^{r}\}\). The augmented features will also be used for the pretext and planning task, and supervised by the aforementioned loss functions (_e.g.,_\(_{}\)). Notably, the BEV object mask \(\) and the ground truth ego trajectory \(_{}\) are also rotated to provide corresponding supervision labels.

Furthermore, we propose an auxiliary task to enhance the steering capability. In specific, we predict the planning direction that the ego car intends to maneuver (_i.e., left_, _straight_ or _right_) based on the ego query \(_{}^{t}\), which is mapped to the probabilities of three directions \(_{}^{t}\!\!^{1 3}\). The direction label \(_{}^{t}\) is generated by comparing the x-axis value of ground truth \(_{}^{t}(x)\) with the threshold \(\). Specifically, \(_{}^{t}\) is assigned to \(straight\) if \(-<_{}^{t}(x)<\), otherwise \(_{}^{t}=left/right\) for \(_{}^{t}(x)\!\!-/_{}^{t}(x)\!\!\), respectively. We use the cross-entropy loss to minimize the gap between the direction prediction \(_{}^{t}\) and the direction label \(_{}^{t}\), denoted as \(_{}\).

**Directional Consistency.** Tailored to the introduced directional augmentation, we propose a directional consistency loss to improve the augmented plan training in a self-supervised manner. It should be noticed that the augmented trajectory predictions \(_{}^{t,r}\) incorporate the same scene information as the original one \(_{}^{t,r=0}\), _i.e.,_ BEV features with different rotation angles. Therefore, it's reasonable to consider the consistency among the predictions and regulate the noises caused by the rotation. The planning head is expected to be more robust to directional change and input distractors. Specifically, \(_{}^{t,r}\) are first rotated back to the original scene direction, then \(_{1}\) loss is applied with \(_{}^{t,r=0}\),

\[_{}=_{t=1}^{T}_{r}^{R}|| (_{}^{t,r})-_{}^{t,r =0}||_{1},\] (7)

where \(\) is the inverse rotation.

To summarize, the overall objective for our UAD contains spatial objectness loss, dreaming loss from the pretext, and imitation learning loss, direction loss, consistency loss from the planning task,

\[=_{1}_{}+_{2}_{}+_{3}_{}+_{4}_{ }+_{5}_{},\] (8)

where \(_{1},_{2},_{3},_{4},_{5}\) are the weight coefficients.

Figure 4: Illustration of direction-aware learning strategy.

[MISSING_PAGE_FAIL:6]

resolution in VAD-Tiny, runs at the fastest speed of 18.9FPS while clearly outperforming VAD-Tiny and even achieving comparable performance with VAD-Base. This again proves the superiority of our design. More detailed runtime comparisons and analyses are presented in the appendix. We adopt the \(\) evaluation protocol in the following ablation experiments unless otherwise specified. Recent works discuss the effect of using ego status in the planning module [22; 26]. Following this trend, we also fairly compare the ego status equipped version of our model with these works. It shows that the superiority of our UAD is still preserved, which also achieves the best performance against the compared methods. Moreover, BEV-Planner  introduces a new metric named "interaction" for better evaluating the performance of E2EAD methods. As shown in Tab. 1, our model obtains the average interaction rate of 1.13%, obviously outperforming other methods. This again proves the effectiveness of our UAD. On the other hand, this demonstrates the importance of designing a suitable pretext for perceiving the environment. Only using ego status is not enough for safe driving.

**Closed-loop Evaluation.** The simulation results in CARLA  are shown in Tab. 2. Our UAD achieves better performance compared with recent E2E planners ST-P3  and VAD  in all scenarios, proving the effectiveness. Notably, on challenging Town05 Long benchmark, UAD greatly outperforms recent E2E method VAD by 6.40 points on the driving score and 9.92 points on route completion, respectively. This proves the reliability of our UAD for long-term autonomous driving.

### Component-wise Ablation

**Loss Functions.** We first analyze the influence of different loss functions that correspond to the proposed pretext task and self-supervised trajectory learning strategy. The experiments are conducted on the validation split of the nuScenes , as shown in Tab. 3. The model with single imitation loss \(_{}\) is considered as the baseline (1). With the enhanced perception capability by the spatial objectness loss \(_{}\), the average L2 error and collision rate are clearly improved to 1.00m and 0.71% from 3.18m and 2.43%, respectively (2 _v.s._ 1). The dreaming loss \(_{}\), direction loss \(_{}\) and consistency loss \(_{}\) also respectively bring considerable gains on the average L2 error for 1.98m, 1.58m, 1.77m over the baseline model (3,4,5 _v.s._ 1). The loss functions are finally combined to construct our UAD (6), which obtains the average L2 error of 0.90m and average collision rate of 0.19%. The results demonstrate the effectiveness of each proposed component.

**Temporal Learning with Dreaming Decoder.** The temporal learning with the proposed dreaming decoder is realized by Circular Update and Dreaming Loss. The circular update is in charge of both extracting information from observed scenes (Eq. 2) and generating pseudo observations to predict the ego trajectories of future frames (Eq. 3). We study the influence of each module in Tab. 4. Circular Update and Dreaming Loss respectively bring performance gains of 0.70m/0.78m on the average L2 error (2,3,5 ), proving the effectiveness of our designs. Applying both two modules (4) achieves the best performance, showing their complementarity for temporal representation learning.

**Direction Aware Learning Strategy.** Directional Augmentation and Directional Consistency are the two core components of the proposed direction-aware learning strategy. We prove their effectiveness in Tab. 5. It shows that the Directional Augmentation improves the average L2 error for considerable

    &  Distance \\ Driving \\ Score \({}^{}\) \\  } &  12 (m) \(\) \\ 1s 2s 3s \\  } &  Collision (\%) \(\) \\ 1s 2s 3s \\  } &  Collision (\%) \(\) \\ 3s \\  } \\  \(\) & - & - & 0.98 & 1.73 & 2.74 & 1.82 & 0.43 & 0.85 & 1.71 & 1.00 \\ \(\) & - & - & 0.50 & 0.98 & 1.87 & 1.12 & 0.27 & 0.60 & 1.37 & 0.75 \\ \(\) & - & - & 0.44 & 0.96 & 1.73 & 0.04 & 0.03 & 0.35 & 1.13 & 0.52 \\ \(\) & - & - & **0.39** & **0.81** & **1.50** & **0.96** & **0.01** & **0.12** & **0.43** & **0.19** \\   

Table 4: Ablation on the dreaming decoder. Table 5: Ablation on direction-aware learning strategy.

    &  Distance \\ Driving \\ Score \({}^{}\) \\  } &  Distance \\ Driving \\ Score \({}^{}\) \\  } &  Time \\ Driving \\ Score \({}^{}\) \\  } & _{v.s.}\)). One interesting observation is that applying the augmentation brings more gains for long-term planning than short-term ones, _i.e.,_ the L2 error of 1s/3s decreases for 0.01m/0.08m compared with 1, which proves the effectiveness of our augmentation on enhancing longer temporal information. The Directional Consistency further reduces the average collision rate for impressive 0.13% (\(_{v.s.}\)), which enhances the robustness for driving directional change.

**Angular Design.** We further explore the influence of the proposed angular design by removing the angular partition and angular queries. Specifically, the BEV feature is directly fed into the dreaming decoder to predict pixel-wise objectness, which is supervised by the BEV object mask (see Fig. 2) with binary cross-entropy loss. Besides, the ego query directly interacts with the BEV feature by cross-attention to extract environmental information. The results are presented in Tab. 6. When discarding the angular design, the average L2 error degrades for 0.47m, and the average collision rate consistently degrades for 1.18%. This demonstrates the effectiveness of our angular design in perceiving complex environments and planning robust driving routes.

### Further Analysis

**Planning Performance in Different Driving Scenes.** The direction-aware learning strategy is designed to enhance the planning performance in scenarios of vehicle steering. We demonstrate the superiority of our proposed model by evaluating the metrics of different driving scenes in Tab. 7. According to the given driving command (_i.e., go straight_, _turn left_ and _turn right_), we divide the 6,019 validation samples in nuScenes  into three parts, which contain 5,309, 301 and 409 ones, respectively. Not surprisingly, all methods perform better under _go straight_ scenes than the steering scenes, proving the necessity of augmenting the imbalanced training data for robust planning. When applying the proposed direction-aware learning strategy, our UAD achieves considerable gains on the average collision rate of _turn left_ and _turn right_ scenes (UAD _v.s._ UAD\({}^{*}\)). Notably, our model outperforms UniAD and VAD by a large margin in steering scenes, proving its effectiveness.

**Visualization of Angular Perception and Planning.** The angular perception pretext is designed to perceive the objects in each sector region. We show its capability by visualizing the predicted objectness in nuScenes  in Fig. 4(a). For a better view, we transform the discrete objectness scores and ground truth to a pseudo-BEV mask. It shows that our model can successfully capture surrounding objects. Fig. 4(a) also shows the open-loop planning results of recent SOTA UniAD , VAD  and our UAD, proving the effectiveness of our method to plan a more reasonable ego trajectory. Fig. 4(b) compares the closed-loop driving routes between Transfuser , ST-P3  and our UAD in CARLA . Our method successfully notices the person and drives in a much safer manner, proving the reliability of our UAD in handling safe-critical issues under complex scenarios.

Due to limited space, we present more analyses in the appendix, including **1)** the influence of partition angle \(\), **2)** the influence of direction threshold \(\), **3)** different backbones and pre-trained weights, **4)** replacing 2D ROIs from GroundingDINO with 2D GT boxes, **5)** different settings of GroundingDINO to generate 2D ROIs, **6)** the influence of pre-training to previous method UniAD and our UAD, **7)** runtime analysis of each module in our UAD and modularized UniAD, **8)** more visualizations, _etc._

### Discussion

**Ego Status and Open-loop Planning Evaluation.** As revealed by [26; 40], it's not a challenge to acquire decent performance of L2 error and collision rate (the original metrics in nuScenes ) in the open-loop evaluation of nuScenes by using ego status in the planning module (see Tab. 1). The question is: _is open-loop evaluation meaningless?_ Our answer is **NO**. Firstly, the inherent reason for the observation is that the simple cases of _go straight_ dominate the nuScenes testing dataset. In these

    &  &  &  \\  & Design & 1s & 2s & 3s & AvgE & 1s & 2s & 3s & AvgE \\  \(\) & - & 0.78 & 1.31 & 2.01 & 1.37 & 0.61 & 1.39 & 2.12 & 1.37 \\ \(\) & **2** & **✓** & **0.39** & **0.81** & **1.50** & **0.90** & **0.01** & **0.12** & **0.43** & **0.19** \\   

Table 6: Ablation on the angular design.

    &  &  &  &  \\  & (\(_{v.s.}\)) &  &  &  \\   & Avg. L2 (m) & Avg. Col. (\%) & Avg. L2 (m) & Avg. Col. (\%) & Avg. L2 (m) & Avg. Col. (\%) & Avg. L2 (m) & Avg. Col. (\%) \\  UniAD  & 0.98 & 0.26 & 1.48 & 0.55 & 1.27 & 0.73 & 1.03 & 0.31 \\ VAD-Base  & 1.19 & 0.37 & 1.47 & 0.78 & 1.39 & 0.81 & 1.22 & 0.43 \\ UAD (V)  & 0.89 & 0.28 & 1.55 & 0.43 & 1.51 & 0.65 & 0.97 & 0.32 \\ UAD (Ours) & **0.84** & **0.17** & **1.39** & **0.22** & **1.16** & **0.33** & **0.90** & **0.19** \\   

Table 7: Performances under different driving scenes. \({}^{*}\) denotes not using direction-aware learning.

cases, even a linear extrapolation of motion being sufficient for planning is not surprising. However, as shown in Tab. 7, in more challenging cases like _turn right_ and _turn left_, the open-loop metrics can still clearly indicate the difficulty of steering scenarios and the differences in methods, which is also proved in . Therefore, open-loop evaluation is not meaningless, while the crux is the distribution of the testing data and the metrics. Secondly, the advantage of open-loop evaluation is its efficiency, which benefits the fast development of algorithms. This view is also revealed by a recent simulator design study , which tries to transform the closed-loop evaluation into an open-loop fashion.

In our work, we thoroughly compare our model with other methods, which shows consistent improvements against previous works under various driving scenarios (straight or steering), different usage of ego status (_w/_ or _w/o_.), diverse evaluation metrics (L2 error, collision rate or intersection rate from ), and different evaluation types (open- or closed-loop). It thus again proves the importance of designing suitable pretext tasks for end-to-end autonomous driving.

**How to Guarantee Safety in Current Auto-Drive System?** Safety is the first requirement of autonomous driving systems in practical products, especially for L4-level auto-vehicles. To guarantee safety, offline collision check with predicted 3D boxes is an inevitable post-process under current technological conditions. Then, a question naturally arises: _how to safely apply our model to current auto-driving systems?_ Before answering this question, we reaffirm our claim that we believe discarding 3D labels is an efficient, attractive, and potential direction for E2EAD, but it doesn't mean we refuse to use any 3D labels if the relatively cheap ones are available in practical product engineering. For instance, solely annotating bounding boxes without object identity for tracking is much cheaper than labeling other elements like HD-map, and point-cloud segmentation labels for occupancy. Therefore, we provide a degraded version of our method by arranging an additional 3D detection head. Then our model can seamlessly integrate into auto-drive products, and offline collision check is achievable. As shown in Tab. 8, integrating the 3D detection head doesn't bring additional improvements, which again proves the design of our method has sufficiently encoded 3D information to the planning module.

In a nutshell, **1)** our work can easily integrate other 3D tasks if they are inevitable under current technical conditions; **2)** the experiments again prove from the side that our spatial-temporal module has already encoded important 3D clues for planning; **3)** we hope our frontier work can eliminate some inessential 3D sub-tasks for both research and engineer usage of E2EAD models. An era of cheap, laboratory-affordable but robust, practical E2EAD design will eventually come!

## 5 Conclusion

Our work seeks to liberate E2EAD from costly modularization and 3D manual annotation. With this goal, we propose the unsupervised pretext task to perceive the environment by predicting angular-wise objectness and future dynamics. To improve the robustness in steering scenarios, we introduce the direction-aware training strategy for planning. Experiments demonstrate the effectiveness and efficiency of our method. As discussed, although the ego trajectories are easily obtained, it is almost impossible to collect billion-level precisely annotated data with perception labels. This impedes the further development of end-to-end autonomous driving. We believe our work provides a potential solution to this barrier and may push performance to the next level when massive data are available.

Figure 5: **(a) Qualitative results in nuScenes. (b) Qualitative results in CARLA.**

   \)} &  &  &  \\  & & 1s & 2s & 3s & AvgE & 1s & 2s & 3s & AvgE \\  \(\) & \(\) & **0.39** & **0.81** & **1.50** & **0.90** & **0.01** & **0.12** & **0.43** & **0.19** \\  \(\) & \(\) & 0.37 & 0.86 & 1.57 & 0.93 & 0.02 & 0.17 & 0.55 & 0.25 \\   

Table 8: Ablation on the 3D detection head.

[MISSING_PAGE_FAIL:10]

*  Tarasha Khurana, Peiyun Hu, Achal Dave, Jason Ziglar, David Held, and Deva Ramanan. Differentiable raycasting for self-supervised occupancy forecasting. In _ECCV_, 2022.
*  Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.
*  Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer: Learning bird's-eye-view representation from multi-camera images via spatiotemporal transformers. In _ECCV_. Springer, 2022.
*  Zhiqi Li, Zhiding Yu, Shiyi Lan, Jiahan Li, Jan Kautz, Tong Lu, and Jose M Alvarez. Is ego status all you need for open-loop end-to-end autonomous driving? In _CVPR_, 2024.
*  Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _ECCV_. Springer, 2014.
*  Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. _arXiv preprint arXiv:2303.05499_, 2023.
*  Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In _CVPR_, 2019.
*  Mahyar Najibi, Jingwei Ji, Yin Zhou, Charles R Qi, Xinchen Yan, Scott Ettinger, and Dragomir Anguelov. Unsupervised 3d perception with 2d vision-language distillation for autonomous driving. In _ICCV_, 2023.
*  OpenAI. Chatgpt [large language model]. _https://chat.openai.com_, 2023.
*  Minting Pan, Xiangming Zhu, Yunbo Wang, and Xiaokang Yang. Iso-dream: Isolating and leveraging noncontrollable visual dynamics in world models. _NeurIPS_, 2022.
*  Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. _NeurIPS_, 1988.
*  Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multi-modal fusion transformer for end-to-end autonomous driving. In _CVPR_, 2021.
*  Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu, Pranaab Dhawan, and Raquel Urtasun. Perceive, predict, and plan: Safe motion planning through interpretable semantic representations. In _ECCV_. Springer, 2020.
*  Wenwen Tong, Chonghao Sima, Tai Wang, Li Chen, Silei Wu, Hanming Deng, Yi Gu, Lewei Lu, Ping Luo, Dahua Lin, et al. Scene as occupancy. In _ICCV_, 2023.
*  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _NeurIPS_, 2017.
*  Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. _CVPR_, 2024.
*  Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin Yang, Sergio Casas, and Raquel Urtasun. End-to-end interpretable neural motion planner. In _CVPR_, 2019.
*  Jiang-Tian Zhai, Ze Feng, Jinhao Du, Yongqiang Mao, Jiang-Jiang Liu, Zichang Tan, Yifu Zhang, Xiaoqing Ye, and Jingdong Wang. Rethinking the open-loop evaluation of end-to-end autonomous driving in nuscenes. _arXiv preprint arXiv:2305.10430_, 2023.

[MISSING_PAGE_FAIL:12]

### Different Direction Thresholds

The direction prediction that the ego car intends to maneuver (_i.e., left_, straight_ and _right_) is proposed to enhance the steering capability for autonomous driving. The label is generated with the threshold \(\) (see Eq. 7 in the manuscript), which determines the ground-truth direction of each waypoint in the expert trajectory. Here we explore the influence by ablating different thresholds, as shown in Tab. 10. Experimental results show that the L2 error gradually increases with the direction threshold. The model with \(\) of 0.5m (\(\)) achieves the lowest L2 error of 0.86m. It reveals that a smaller threshold will force the planner to fit the expert navigation, leading to a closer distance between the predicted trajectory and the ground truth. In contrast, the collision rate benefits more from larger thresholds. The model with \(\) of 2.0m obtains the best collision rate at 2s of 0.08% (\(\)), showing the effectiveness for robust planning. Notably, the threshold of 1.2m contributes to a great balance with the average L2 error of 0.90m and average collision rate of 0.19%.

### Different Backbones and Pre-trained Weights

As a common sense, pre-training the backbone network with fundamental tasks like image classification on ImageNet  will benefit the sub-tasks. The previous method UniAD  uses the pre-trained weights of BEVFormer . What surprised us is that when replacing the pre-trained weights with the one learned on ImageNet, the performance of UniAD dramatically degraded (see "Influence of Pre-training" for more details). This inspires us to explore the influence of backbone settings on our framework. As shown in Tab. 11, interestingly, even without any pre-training, our model still outperforms UniAD with pre-trained ResNet101 and VAD with pre-trained ResNet50. This verifies the effectiveness of our unsupervised pretext task on modeling the driving scenes. We also use publicly available pre-trained weights on detection datasets like COCO  and nulmages  to train our model, which shows better performance. These experimental results and observations demonstrate that a potentially promising topic is _how to pre-train a model for end-to-end autonomous driving_. We leave this to future research.

### Objectness Label Generation with GT Boxes

As mentioned in the manuscript, the essence of generating the angular objectness label lies in the 2D ROIs, which come from the open-set 2D detector GroundingDINO . Here we explore the influence of using the ground-truth 2D boxes as ROIs, which provide more high-quality samples for the representation learning in the angular perception pretext. Tab. 12 shows that training with GT boxes achieves consistent performance gains on both ResNet50  and ResNet101  (\(\),\(\),\(\)). This reveals that accurate annotation does help to learn better spatio-temporal knowledge and improve ego planning. Considering the cost in real-world deployment, training with accessible

    &  &  &  &  &  \\  & & Box & & & & & & & & & \\  \)} &  &  & Pseudo & 0.41 & 0.90 & 1.66 & 0.99 & 0.03 & 0.32 & 0.80 & 0.38 & \\  & & GT & 0.41 & 0.87 & 1.61 & 0.96 & 0.03 & 0.30 & 0.71 & 0.35 & \\  \)} &  & Pseudo & 0.39 & 0.81 & 1.50 & 0.90 & 0.01 & **0.12** & 0.43 & 0.19 & \\  & & GT & **0.37** & **0.79** & **1.45** & **0.84** & **0.01** & 0.13 & **0.39** & **0.18** & \\   

Table 12: Ablation on 2D object boxes in pretext label generation.

    &  &  &  &  &  \\  & & Weight & & & 1s & 2s & 3s & Avg. & 1s & 2s & 3s & Avg. & \\  \)} &  & None & 0.43 & 0.94 & 1.65 & 1.01 & 0.03 & 0.37 & 0.86 & 0.42 & 9.6 \\  & & ImageNet & 0.41 & 0.90 & 1.66 & 0.99 & 0.03 & 0.32 & 0.80 & 0.38 & 9.6 \\  \)} &  & None & 0.40 & 0.87 & 1.59 & 0.95 & 0.02 & 0.23 & 0.59 & 0.28 & \\  & & ImageNet & 0.37 & 0.84 & 1.53 & 0.91 & 0.01 & 0.18 & 0.50 & 0.23 & \\   & & COCO & **0.36** & 0.83 & 1.51 & 0.90 & 0.01 & 0.16 & 0.45 & 0.21 & \\   & & Nulmages & 0.39 & **0.81** & **1.50** & **0.90** & **0.01** & **0.12** & **0.43** & **0.19** & \\   

Table 11: Ablation on different backbones and pre-trained weights.

pseudo labels is a more efficient way compared with the manual annotation, which also shows comparable performance in autonomous driving (1 _v.s._ 2 and 3 _v.s._ 4).

### Settings for ROI Generation.

The quality of learned spatio-temporal knowledge highly relies on the generated ROIs by the open-set 2D detector GroundingDINO , which are then projected as the BEV objectness label for training the angular perception pretext. We explore the influence of generated ROIs with different settings, as shown in Tab. 13. We take the setting with the confidence score of 0.35, prompt word of _vehicle_ and without the Rule Filter, as the baseline (1). By appending more prompt words (_e.g., pedestrian, barrier_), the planning performance gradually improves (3,2 _v.s._ 1), showing the enhanced perception capability with more diversified objects. Filtering the ROIs with overlarge size (_i.e.,_ Rule Filter) brings considerable gains for the average L2 error of 0.07m and average collision rate of 0.10% (4\(@sectionsign\)_v.s._ 5). One interesting observation is that decreasing the confidence threshold would slightly improve the L2 error while causing higher collision rate (5\(@sectionsign\)_v.s._ 4). In contrast, increasing the threshold obtains lower average collision rate of 0.17% and higher average L2 error of 0.98m. This reveals the importance of providing diversified ROIs for angular perception learning as well as ensuring high quality. The model with the confidence score of 0.35, all prompt words and Rule Filter achieves balanced performance with the average L2 error of 0.90m and average collision rate of 0.19%.

### Different Image Sizes and BEV Resolution

For safe autonomous driving, increasing the input size of the multi-view images and the resolution of the built BEV representation is an effective way, which provide more detailed environmental information. While benefiting perception and planning, it inevitably brings heavy computation cost. We then ablate the image size and BEV resolution of our UAD to find a balanced version between performance and efficiency, as shown in Tab. 14. The results show that our UAD with ResNet-101 , image size of 1600\(\)900, BEV resolution of 200\(\)200, achieves the best performance compared with previous methods UniAD  and VAD-Base  while running faster with 7.2FPS (8). By replacing the backbone with ResNet-50, our UAD is more efficient with little performance degradation (5 _v.s._ 6). We further align the settings of VAD-Tiny, which has an inference speed of outstanding 17.6FPS (2), to explore the influence of much smaller input sizes. Tab. 14 shows that our UAD still achieves excellent performance even compared with VAD-Base of high-resolution inputs (4 _v.s._ 3). Notably, our UAD of this version has the fastest inference speed of 18.9FPS. This

    & Conf. & Prompt & Rule &  &  \\  & Thresh & Words & Filter & 1s & 2s & 3s & Avg. & 1s & 2s & 3s & Avg. \\  1 & 0.35 & \{_vehicle_\} & - & 0.48 & 0.98 & 1.75 & 1.07 & 0.08 & 0.38 & 0.80 & 0.42 \\ 2 & 0.35 & \{_vehicle,pedestrian_\} & - & 0.47 & 0.94 & 1.69 & 1.03 & 0.04 & 0.27 & 0.71 & 0.34 \\ 3 & 0.35 & \{_vehicle,pedestrian,barrier_\} & - & 0.43 & 0.88 & 1.60 & 0.97 & 0.03 & 0.23 & 0.60 & 0.29 \\ 4 & 0.35 & \{_vehicle,pedestrian,barrier_\} & \(\) & **0.39** & **0.81** & 1.50 & 0.90 & **0.01** & **0.12** & 0.43 & 0.19 \\ 5 & 0.30 & \{_vehicle,pedestrian,barrier_\} & \(\) & 0.39 & 0.82 & **1.45** & **0.89** & 0.01 & 0.21 & 0.51 & 0.24 \\ 6 & 0.40 & \{_vehicle,pedestrian,barrier_\} & \(\) & 0.46 & 0.90 & 1.57 & 0.98 & 0.01 & 0.13 & **0.37** & **0.17** \\   

Table 13: Ablation on the settings of ROI generation. The Conf. Thresh denotes the confidence threshold in GroundingDINO  to filter unreliable predictions. _vehicle,pedestrian,barrier_ represent the used prompt words to obtain ROIs of corresponding classes. Rule Filter indicates filtering the ROIs that are more than half of the length or width of the image.

    &  &  &  &  &  &  \\  & & & & & 1s & 2s & 3s & Avg. & 1s & 2s & 3s & Avg. \\  2 & 1 & UniAD  & R101 & 1600\(\)900 & 200\(\)200 & 0.48 & 0.96 & 1.65 & 1.03 & 0.05 & 0.17 & 0.71 & 0.31 & 2.1 \\  3 & 1 & VAD-Tiny  & R50 & 640\(\)360 & 100\(\)100 & 0.60 & 1.23 & 2.06 & 1.30 & 0.33 & 1.33 & 2.21 & 1.29 & 17.6 \\ 3 & VAD-Base  & R50 & 1280\(\)720 & 200\(\)200 & 0.54 & 1.15 & 1.98 & 1.22 & 0.10 & 0.24 & 0.96 & 0.43 & 5.3 \\  4 & 1 & UAD (Ours) & R50 & 640\(\)360 & 100\(\)100 & 0.47 & 0.99 & 1.71 & 1.06 & 0.08 & 0.39 & 0.90 & 0.46 & 18.9 \\ 5 & 1 & UAD (Ours) & R50 & 1600\(\)900 & 200\(\)200 & 0.41 & 0.90 & 1.66 & 0.99 & 0.03 & 0.32 & 0.80 & 0.38 & 9.6 \\ 6 & 1 & UAD (Ours) & R101 & 1600\(\)900 & 200\(\)200 & **0.39** & **0.81** & **1.50** & **0.90** & **0.01** & **0.12** & **0.43** & **0.19** & 7.2 \\   

Table 14: Comparison with different backbones, image sizes and BEV resolutions.

again proves the effectiveness of our method in performing fine-grained perception, as well as the robustness to fit the inputs of different sizes.

### Runtime Analysis

Tab. 15 compares the runtime of each module between the modularized method UniAD  and our UAD. As we adopt the Backbone and BEV Encoder from BEVFormer  that are the same in UniAD, the latency of feature extraction is similar with little difference due to different pre-processing. The modular sub-tasks in UniAD consume most of the runtime, _i.e.,_ significant 71.8% for Det&Track (31.2%), Map (19.8%), Motion (10.9%) and Occupancy (9.9%), respectively. In contrast, our UAD performs simple Angular Partition and Dreaming Decoder, which take only 14.0% (19.3ms) to model the complex environment. This demonstrates our insight that it's a necessity to liberate end-to-end autonomous driving from costly modularization. The downstream Planning Head takes negligible 1.5ms to plan the ego trajectory, compared with 9.7ms in UniAD. Finally, our UAD finishes the inference with a total runtime of 138.3ms, 3.4\(\) faster than the 465.1ms of UniAD, showing the efficiency of our design.

### Classification of Angular Perception

The proposed angular perception pretext learns spatio-temporal knowledge of the driving scene by predicting the objectness of each sector region, which is supervised by the generated binary angular-wise label. We show the perception ability by evaluating the classification metrics based on the validation split of the nuScenes  dataset. Fig. 6 draws the Precision-Recall (PR) curve and Receiver-Operating-Characteristic (ROC) curve in different driving scenes (_i.e., turn left, go straight_ and _turn right_). In the PR curve, our UAD achieves balanced precision and recall scores in different driving scenes, showing the effectiveness of our pretext task to perceive the surrounding objects. Notably, the performance of _go straight_ scenes is slightly better than the steering ones under all thresholds. This proves our insight to design tailored direction-aware learning strategy for improving the safety-critical _turn left_ and _turn right_ scenes. The ROC curve shows the robustness of our angular perception pretext to classify the objects from complex environmental observations.

    Model \\ Partition \\  } &  &  \\  &  &  &  &  \\  &  &  &  &  \\   Feature \\ Extraction \\  } & Backbone & 13.1\(\) & 8.2\(\) & 28.6\(\) & Backbone & 36.0\(\) & 3.6\(\) & 26.9\(\) \\  & BEV & 4.4\(\) & 0.7\(\) & 17.9\(\) & BEV & 8.1\(\) & 0.4\(\) & 58.9\(\) \\   & Backbone & 14.5\(\) & 31.3\(\) & 21.0\(\) & Angular & 1.1\(\) & 0.8\(\) \\   & Map & 92.1\(\) & 0.7\(\) & 19.8\(\) & Permion & 1.1\(\) & 0.8\(\) \\   & Motion & 36.0\(\) & 6.1\(\) & 19.9\(\) & Permion & 18.2\(\) & 0.1\(\) & 13.2\(\) \\   & Oxapapap & 4.9\(\) & 0.4\(\) & 9.9\(\) & Decoder & 18.2\(\) & 0.1\(\) & 1.1\(\) \\   & Planning & 9.7\(\) & 0.3\(\) & 2.1\(\) & Planning & 1.5\(\) & 0.1\(\) & 1.1\(\) \\   & Head & & & & & & & \\   Prediction \\  } & Total & - & 465.1\(\) & 43.0\(\) & 100\% & - & 138.3\(\) & 1.1 & 100.0\% \\   

Table 15: Module runtime comparison between UniAD  and our UAD. The inference is measured on an NVIDIA Tesla A100 GPU.

Figure 6: Visualization of the PR and ROC curves for the angular-wise objectness prediction in different driving scenes.

Figure 7: Optimization of UniAD **(a)** and our UAD **(b)** with different pre-trained backbone weights.

### Influence of Pre-training

Pre-training the backbone network with fundamental tasks is a commonly used metric to benefit representation learning. As mentioned in "Different Backbones and Pre-trained Weights" of Sec. 4.4 in the manuscript, the performance of the previous SOTA method UniAD  dramatically degrades without the pre-trained weights from BEVFormer . Here we further detail the influence by comparing the training losses and planning performances with different pre-trained weights in Fig. 7. Fig. 7a shows that the training losses increase by about 20 on average when replaced with the pre-trained weights from ImageNet . Correspondingly, the average L2 error is significantly higher than the one with the pre-trained weights from BEVFormer. This reveals that UniAD heavily relies on the perceptive pre-training in BEVFormer to optimize modularized sub-tasks. In contrast, our UAD performs comparably even without any pre-training (see Fig. 7b), proving the effectiveness of our designs for robust optimization.

Figure 8: Visualization of the angular perception.

Figure 9: Visualization of the planning results. The first two rows show the success of our method in safe planning in complex scenarios, while the third row exhibits a failure case of our planner when no temporal information could be acquired when \(t\!=\!0\).

### More Visualizations

Open-loop PlanningWe provide more visualizations about the predicted angular-wise objectness and planning results on nuScenes . Fig. 8 compares the discrete objectness scores and ground truth, proving the effectiveness of our angular perception pretext to perceive the objects in each sector region. The planning results of previous SOTA methods (_i.e.,_ UniAD  and VAD ) and our UAD are shown in Fig. 9. With the designed pretext and tailored training strategy, our method could plan a more reasonable ego trajectory under different driving scenarios, proving the effectiveness of our work. The third row shows the failure case of our planner. In this case, the ego car is given the "_Turn Right_" command when \(t=0\) (_i.e.,_ the first frame of the driving scenario), leading to ineffectiveness of our planner in learning helpful temporal information. A possible solution to deal with this is to apply an auxiliary trajectory prior for the first several frames, and we leave this to future work.

Closed-loop SimulationFig. 10 visualizes the predicted objectness and planning results in the Town05 Long benchmark of CARLA . Following the setting of ST-P3  in closed-loop evaluation, we collect visual observations from the cameras of "CAM_FRONT", "CAM_FRONT_LEFT", "CAM_FRONT_RIGHT" and "CAM_BACK". It shows that the sector regions in which the surrounding objects exist are successfully captured by our UAD, proving the effectiveness and robustness of our design. Notably, the missed objects by GroundingDINO , _e.g.,_ the black car in the camera of "CAM_FRONT_LEFT" at \(t=145\), are surprisingly perceived and marked in the corresponding sector. This demonstrates our method has the capability of learning perceive knowledge in a data-driven manner, even with coarse supervision by the generated 2D pseudo boxes from GroundingDINO.

Figure 10: Visualization of angular perception and planning in Carla.

NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions and scope, please see Sec. 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations of the work are discussed in this paper, please see Sec. 4.5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: This paper does not include theoretical results. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All the information needed to reproduce the main experimental results of the paper is disclosed, please see Sec. 4.1. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: This paper provides open access to the data and code to reproduce the main experimental results, please see Sec. 4.1. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the training and test details are specified in this paper, please see Sec. 4.1. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: This paper reports information about the statistical significance of experiments, please see Sec. 4. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: This paper provides sufficient information on the computer resources, please see Sec. 4.1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The conducted research conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: This paper discusses both potential positive societal impacts and negative societal impacts of the work, please see Sec. 4.5. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The assets used in this paper are credited and the license is respected. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.