# Fast Attention Requires Bounded Entries

Josh Alman

josh@cs.columbia.edu. Columbia University.

Zhao Song

zsong@adobe.com. Adobe Research.

###### Abstract

In modern machine learning, inner product attention computation is a fundamental task for training large language models such as Transformer, GPT-1, BERT, GPT-2, GPT-3 and ChatGPT. Formally, in this problem, one is given as input three matrices \(Q,K,V[-B,B]^{n d}\), and the goal is to construct the matrix \((Q,K,V):=(A_{n})^{-1}AV^{n  d}\), where \(A=(QK^{}/d)\) is the 'attention matrix', and \(\) is applied entry-wise. Straightforward methods for this problem explicitly compute the \(n n\) attention matrix \(A\), and hence require time \((n^{2})\) even when \(d=n^{o(1)}\) is small.

In this paper, we investigate whether faster algorithms are possible by _implicitly_ making use of the matrix \(A\). We present two results, showing that there is a sharp transition at \(B=()\).

* If \(d=O( n)\) and \(B=o()\), there is an \(n^{1+o(1)}\) time algorithm to approximate \((Q,K,V)\) up to \(1/(n)\) additive error.
* If \(d=O( n)\) and \(B=()\), assuming the Strong Exponential Time Hypothesis from fine-grained complexity theory, it is impossible to approximate \((Q,K,V)\) up to \(1/(n)\) additive error in truly subquadratic time \(n^{2-(1)}\).

This gives a theoretical explanation for the phenomenon observed in practice that attention computation is much more efficient when the input matrices have smaller entries.

## 1 Introduction

Large language models (LLMs) such as Transformer , BERT , GPT-3 , PaLM , and OPT  can process natural language more effectively than smaller models or traditional algorithms. This means that they can understand and generate more complex and nuanced language, which can be useful for a variety of tasks such as language translation, question answering, and sentiment analysis. LLMs can also be adapted to multiple purposes without needing to be retained from scratch. Their power is particularly exemplified by the recent success of ChatGPT, a chat software by OpenAI built on top of GPT-3 .

The key technical backbone of LLMs is the _attention matrix_. An attention matrix is a square matrix whose rows and columns correspond to words or "tokens", and whose entries correspond to the correlations between these tokens in natural text. The attention matrix is then used to calculate the importance of each input token in a sequence when producing an output. In an attention mechanism, each input token is given a weight or score, which reflects its importance or relevance to the current output being generated. These scores are calculated based on a comparison between the current output state and the input states, using a similarity function.

More formally, the attention matrix is defined as follows. Let \(Q^{n d}\) be the matrix of query tokens, and \(K^{n d}\) be the matrix of key tokens. (We focus here on the case when \(d=n^{o(1)}\), so \(d n\).) The attention matrix is an \(n n\) matrix \(A\) where the rows and columns correspond to the input tokens in the sequence. Each entry in the matrix represents the attention weight or score between a particular input token (query token \(Q\)) and a particular output token (key token \(K\)). The diagonal entries of the matrix represent self-attention scores, which measure the importance of each token with respect to itself.

The major bottleneck to speeding up LLM operations (in the case of modeling long sequences with large \(n\)) is the time to perform attention matrix computations . These computations ask us to multiply the attention matrix \(A\) with another value token matrix \(V^{n d}\).

We formally define Attention computation as follows. Throughout this paper, we write \(\) to denote the _entry-wise_ exponential for matrices.

**Definition 1.1** (Exact Attention Computation \((n,d)\)).: _Given three matrices \(Q,K,V^{n d}\), output the \(n d\) matrix \((Q,K,V)\) defined by_

\[(Q,K,V):=D^{-1}AV\]

_where \(A^{n n}\) and diagonal matrix \(D^{n n}\) are defined as_

\[A:=(QK^{}/d),\ \ \ \ \ D:=(A_{n}).\]

The straightforward algorithm for this problem computes the matrix \(A\) and then performs the multiplications \(D^{-1}AV\), in time \(n^{2+o(1)}\). Since \(A\) is an \(n n\) matrix with \(n^{2}\) entries, it is impossible to improve on this much while explicitly computing the matrix \(A\). However, the input to the problem is not \(A\), but rather the three matrices \(Q,K,V\) which each have only \(n^{1+o(1)}\) entries. An algorithm which only _implicitly_ makes use of \(A\), without explicitly computing all its entries, could hope to run in almost linear time!

In this paper, we investigate the possibility of accelerating attention computations in this way. The two main questions we address are:

* **Q1.** When can we perform attention computations in almost linear time \(n^{1+o(1)}\)?
* **Q2.** When can we prove that subquadratic-time algorithms for attention computations are _impossible_?

In most LLMs, it suffices to _approximately_ perform attention computations throughout the inference process as long as there are reasonable precision guarantees . We therefore focus here on approximate attention computation, which can potentially be performed even faster than exact computation. Mathematically, we define the _approximate_ version of \(\) as follows.

**Definition 1.2** (Approximate Attention Computation \((n,d,B,_{a})\)).: _Let \(_{a}>0\) and \(B>0\) be parameters. Given three matrices \(Q,K,V^{n d}\), with the guarantees that \(\|Q\|_{} B\), \(\|K\|_{} B\), and \(\|V\|_{} B\), output a matrix \(T^{n d}\) which is approximately equal to \(D^{-1}AV\), meaning,_

\[\|T-D^{-1}AV\|_{}_{a}.\]

_Here, for a matrix \(M^{n n}\), we write \(\|M\|_{}:=_{i,j}|M_{i,j}|\)._

Again, the straightforward algorithm for this problem runs in time \(O(n^{2}d) n^{2+o(1)}\), but the input size is only \(O(nd) n^{1+o(1)}\). Our goal is to investigate when faster algorithms are possible in terms of the parameters \(d,B\), and \(_{a}\).

### Our Results

We focus on the natural setting where \(d=O( n)\) (the setting where we model long sequences) and \(_{a}=1/(n)\) (low enough error so that attention computations over an entire network can be combined). Our main results show that whether or not there is a fast algorithm for \(\) critically depends on \(B\), the magnitudes of the entries in the input matrices.

We first show a lower bound, that when \(B()\), it is impossible to design a truly subquadratic-time algorithm. Our lower bound makes use of the Strong Exponential Time Hypothesis (SETH) , a popular conjecture  from the area of fine-grained complexity regarding the time required to solve \(k\)-SAT. (See Section 4 below where we discuss SETH in more detail.)

**Theorem 1.3** (Lower bound, informal version of Theorem 4.6).: _Assuming_ SETH_, for every \(q>0\), there are constants \(C,C_{a},C_{b}>0\) such that: there is no \(O(n^{2-q})\) time algorithm for the problem \((n,d=C n,B=C_{b},_{a}=n^{-C_{a}})\)._

Our second complementary result is a new algorithm, showing that when \(B<o()\), the problem can be solved very efficiently, in almost linear time.

**Theorem 1.4** (Upper bound, informal version of Theorem 3.8).: _There is an algorithm (Algorithm 1) that solves \((n,d=O( n),B=o(),_{a}=1/(n))\) in time \(n^{1+o(1)}\)._

Our Theorems 1.3 and 1.4 show that the attention computation problem \(\) exhibits a very tight transition at \(B=()\) from almost linear time to trivial quadratic time. When \(B<o()\) is smaller, the problem can be solved in almost linear time \(n^{1+o(1)}\) in the input size, using our algorithm for Theorem 1.4. When \(B()\) is greater, our algorithm from Theorem 1.4 no longer applies, and furthermore our lower bound from Theorem 1.3 shows that it is _impossible_ to solve the problem in truly subquadratic time, no matter what algorithmic techniques one uses (assuming SETH).

It has been observed in LLM implementations in practice that computations are much faster when one assumes that the matrix entries are bounded or can be well-approximated using a small number of bits (see, e.g., [14, Section 2] and [17, Section 3.2.1]). Our work can be viewed as giving a theoretical explanation for this phenomenon, and helping to explain why techniques like quantization  and low-degree polynomial approximation  have been so effective in practice.

**Related Work.**

A recent work by Zandieh, Han, Daliri, and Karbasi  was the first to give an algorithm with provable guarantees for attention approximation. Their algorithm makes use of locality sensitive hashing (LSH) techniques  which, as we will discuss next, is quite different from our algorithm for Theorem 1.4 which uses the polynomial method .

In the case when \(d=o(^{2}n)\), they achieve a running time of roughly \(O(n^{1.17} d/_{r}^{2})\), where \(_{r}\) is a _relative_ error parameter (which is similar, though not exactly the same, as our \(_{a}\) from Definition 1.2). In particular, their algorithm applies for larger \(d\) than ours (we require \(d=O( n)\)), but we achieve almost linear time \(n^{1+o(1)}\) (whereas their running time is bounded below by \((n^{1.17})\)), and our algorithm can handle any polynomial error \(_{a}=1/(n)\) (whereas they require \(_{r} 1/n^{o(1)}\) to not increase the running time by a polynomial factor).

It is natural to wonder whether further improvements are possible by combining our techniques with those of . However, our lower bound of Theorem 1.3 shows that our algorithm of Theorem 1.4 is already essentially tight and cannot be substantially improved.

Another recent work by Keles, Wijewardena, and Hedge  was the first to prove a lower bound for attention computation assuming SETH. They prove, among other results, that \(\) cannot be solved in truly subquadratic time in the case when \(d=( n)\). Our Theorem 1.3 improves their result to also hold for \(d=( n)\), and to show how the complexity changes with the magnitude of entries \(B\) (which is not studied by ). As we discuss more shortly, both our lower bound proof and  use the high-level technique of , although our more fine-grained analysis of the parameters \(d,B\) requires a more intricate analysis and the use of other techniques from fine-grained complexity related to approximate nearest neighbor search  and the polynomial method .

### Technique Overview

Our high-level approach is to make use of similarities between attention computation and other computational problems related to Kernel Density Estimation (KDE). Such a relationship was investigated by recent work . In particular,  was inspired to apply LSH techniques to attention computation because of the prevalence of LSH in KDE algorithms . The main conceptual idea behind our results is that different techniques from the KDE literature, other than LSH, can be modified to apply in this setting and yield tight algoriths and lower bounds.

To design our algorithm for Theorem 1.3, we instead build off of a different line of work on KDE which makes use of the 'polynomial method in algorithm design'. Suppose \(M^{n n}\) is a matrix, \(f:\) is a function, and let \(f(M)\) denote the matrix one gets by applying \(f\) entry-wise to \(M\). The polynomial method is a technique for finding low-rank approximations of \(f(M)\). It shows that if \(M\) has low rank, and if \(f\) can be approximated by a low-degree polynomial, then the matrix \(f(M)\) is very close to a low-rank matrix whose low-rank decomposition can be computed efficiently.

To use this to solve \(\), we make use of a recent result which bounds the degree required to approximate the exponential function by a polynomial  in order to find a low-rank approximation of the attention matrix \(A\). Prior work  applied these polynomials in a similar way to solve the Gaussian KDE problem; our main observation is that by an appropriate rescaling, this approach can be modified to apply to \(\) as well.

The proof of our lower bound Theorem 1.3 builds off of another line of work on the fine-grained complexity of KDE problems . The main idea is to give a fine-grained reduction from the well-studied problem of Approximate Nearest Neighbor search \(\). In \(\), one is given as input \(n\) vectors of dimension \(d\), and an error parameter \(>0\), and the goal is to find a pair of vectors whose distance is at most \((1+)\) times the _minimum_ distance between any pair of the vectors. The straightforward algorithm for \(\) runs in quadratic time, and it is known that it is impossible to solve \(\) in truly subquadratic time assuming \(\).

In order to prove our lower bound, we show that \(\) can be used to solve \(\). The key idea is that, if the matrices \(Q\) and \(K\) from \(\) are formed by concatenating the input vectors to the \(\) problem, then the nearest neighbor vectors correspond to the largest entries of the attention matrix \(A\). It is not immediately clear that \(\) can be used to detect large entries of \(A\), since the output is rescaled by the matrix \(D^{-1}\), but we show that this can be overcome with some modifications to the input vectors which approximately balance the rows of \(A\). Prior work  used a very similar approach to give lower bounds for KDE problems, although KDE doesn't involve any rescaling factors.

#### Roadmap.

In Section 2, we introduce relevant notation and tools from prior work. In Section 3, we present and analyze our attention algorithm. In Section 4, we prove our fine-grained attention lower bound. In Section 5, we provide a conclusion for this paper.

## 2 Preliminaries

We work in the standard real-RAM model and assume arithmetic operations on real numbers can be performed in constant time in our algorithms.

We use \(_{}(a,b,c)\) to denote the time to multiply an \(a b\) matrix with another \(b c\) matrix. In fact, we will only make use of the straightforward, practical bound \(_{}(a,b,c) O(abc)\). In principle, fast theoretical matrix multiplication algorithms could be used instead to improve this bound and speed up our algorithms here (in exchange for making them less practical). That said, because of our parameter settings3, we will see that faster matrix multiplication could only improve low-order terms in our running times.

For any positive integer, we use \([n]\) to denote set \(\{1,2,,n\}\).

For a matrix \(M\), we write \(\|M\|_{}\) to denote its \(_{}\) norm, i.e., \(\|M\|_{}:=_{i,j}|M_{i,j}|\). For a matrix \(M\), we use \(M^{}\) to denote its transpose.

We use \(_{n}\) to denote a length-\(n\) vector whose entries are all \(1\)s. We use \(_{n}\) to denote a length-\(n\) vector whose entries are all \(0\)s.

For any matrix \(A^{n n}\), we use \((A)^{n n}\) to denote the matrix where \((A)_{i,j}=(A_{i,j})\). In other words, all the \(()\) operators in this paper are applied entry-wise to matrices. In particular, we will not use matrix exponentials in this paper.

For a vector \(x^{n}\), we use \(\|x\|_{0}\) to denote its number of non-zero entries, we use \(\|x\|_{1}\) to denote its \(_{1}\) norm, i.e., \(\|x\|_{1}:=_{i=1}^{n}|x_{i}|\), and we use \(\|x\|_{2}\) to denote its \(_{2}\) norm, i.e., \(\|x\|_{2}:=(_{i=1}^{n}|x_{i}|^{2})^{1/2}\). For a vector \(x\), we use \(x^{}\) to denote its transpose.

### Additive Error for Polynomial Approximation

Our algorithm for attention computation will critically make use of a polynomial approximation for the exponential function. In particular, we use the following tight construction from previous work .

**Lemma 2.1** ().: _Let \(B>1\) and let \((0,0.1)\). There is a polynomial \(P:\) of degree \(g:=(\{,B \})\) such that for all \(x[0,B]\), we have_

\[|P(x)-(x)|<.\]

_Moreover, \(P\) can be computed efficiently: its coefficients are rational numbers with \((g)\)-bit integer numerators and denominators which can be computed in \((g)\) time._

### From Additive Error to Relative Error

We note that in our setting, Lemma 2.1 can be used to give a relative error approximation as well:

**Corollary 2.2**.: _Let \(B>1\) and let \((0,0.1)\). There is a polynomial \(P:\) of degree \(g:=(\{,B\})\) such that for all \(x[-B,B]\), we have_

\[|P(x)-(x)|<(x).\]

Proof.: By Lemma 2.1, there is a polynomial \(Q:\) of degree \(g=(\{,B\})\) such that, for all \(y[0,2B]\) we have \(|Q(y)-(y)|\). Our desired polynomial is the rescaled \(P(x):=Q(x+B)/(B)\). Indeed, for any \(x[-B,B]\), we have \((x)(-B)\), and so

\[|P(x)-(x)| =|Q(x+B)/(B)-(x)|\] \[=|Q(x+B)-(x+B)|/(B)\] \[/(B)\] \[(x),\]

as desired. 

## 3 Attention Algorithm

In this section, we show how to use polynomial approximations for the exponential function in order to approximately perform attention computations. In Section 3.1, we define the type of low-rank matrix approximation which we will use. In Section 3.2, we show how polynomial approximations can give rise to such low-rank matrix approximations. In Section 3.3, we bound the entries of the matrix \(QK^{}^{n n}\) (before converting it to the attention matrix) to confirm that our polynomial approximation applies. In Section 3.4, we state our main technique for approximating the attention matrix. In Section 3.5, we show how to control the error propagation from \(A\) to the rescaling matrix \(D\). In Section 3.6, we further explain how to control the error propagation from \(D\) and \(A\) to the resulting attention matrix. Finally, in Section 3.7, we conclude our general algorithm, and in Section 3.8, we appropriately select the parameters to achieve almost linear time.

### Matrix Low-Rank Approximation

**Definition 3.1**.: _Let \(r 1\) denote a positive integer. Let \((0,0.1)\) denote an accuracy parameter. Given a matrix \(A_{ 0}^{n n}\), we say \(_{ 0}^{n n}\) is an \((,r)\)-approximation of \(A\) if_* \(=U_{1} U_{2}^{}\) _for some matrices_ \(U_{1},U_{2}^{n r}\) _(i.e.,_ \(\) _has rank at most_ \(r\)_), and_
* \(|_{i,j}-A_{i,j}| A_{i,j}\) _for all_ \((i,j)[n]^{2}\)_._

### From Low Degree Polynomials to Low Rank Matrices

**Lemma 3.2**.: _Let \(M=XY^{}^{n n}\) denote a matrix with \(X,Y^{n d}\). Let \(P(x)\) denote a degree-\(g\) polynomial, and define \(r=\)._

_There is an algorithm that runs in \(O(nrg)\) time and, given as input the matrix \(X,Y\), constructs matrices \(U_{1},U_{2}^{n r}\) such that \(P(M)=U_{1}U_{2}^{}\). (Here, \(P(M)\) denotes the entry-wise application of \(P\) to \(M\).)_

Due to space limitation, we defer the proof of Lemma 3.2 to Appendix A.

### Matrix \(Qk^{}\) Has Bounded Entries

**Lemma 3.3** (Bounded entry).: _Suppose \(B 1\) and matrices \(Q,K^{n d}\) have \(\|Q\|_{} B\) and \(\|K\|_{} B\). Then, we have_

\[\|QK^{}/d\|_{} B^{2}.\]

Proof.: For each \((i,j)[n][n]\), we have

\[|(QK^{})_{i,j}| =|_{l=1}^{d}Q_{i,l}K_{j,l}|\] \[ d\|Q\|_{}\|K\|_{}\] \[ d B^{2},\]

as desired. 

### Key Lemma

Our key lemma shows that, even though the attention matrix \(A\) may have full rank, it has a low-rank approximation that is easy to compute:

**Lemma 3.4**.: _Suppose \(Q,K^{n d}\), with \(\|Q\|_{} B\), and \(\|K\|_{} B\). Let \(A:=(QK^{}/d)^{n n}\). For accuracy parameter \((0,1)\), there is a positive integer \(g\) bounded above by_

\[g=O)}, B^{2}},\]

_and a positive integer \(r\) bounded above by_

\[r\]

_such that: There is a matrix \(^{n n}\) that is an \((,r)\)-approximation (Definition 3.1) of \(A^{n n}\). Furthermore, the matrices \(U_{1}\) and \(U_{2}\) defining \(\) can be computed in \(O(n r)\) time._

Proof.: Let \(M:=QK^{}/d\). From Lemma 3.3, we know that \(\|M\|_{} B^{2}\). Thus, applying Corollary 2.2 (with bound \(B^{2}\) on its entries), there is a degree-\(g\) polynomial \(P\) such that the matrix \(=P(M)\) is an \((,r)\)-approximation to \(A\) (See the definition of \((,r)\)-approximation in Definition 3.1.) We can then compute \(U_{1},U_{2}\) using Lemma 3.2, which gives the bound

\[r.\]

This completes the proof.

### From \(A\) to \(D\)

**Lemma 3.5**.: _Let \(A^{n n}\) be any matrix whose entires are all positive and \(_{A}(0,0.1)\) be any parameter. Let \(^{n n}\) be any matrix such that, for all \((i,j)[n][n]\), we have_

\[|_{i,j}-A_{i,j}|_{A} A_{i,j}.\]

_Define the matrices \(D,^{n n}\) by \(D=(A_{n})\) and \(=(_{n})\). Then, for all \(i[n]\) we have_

\[|_{i,i}-D_{i,i}|_{A} D_{i,i}.\]

Due to space limitation, we defer the proof of Lemma 3.5 into Appendix A.

### From \(A\) and \(D\) to Attention Matrix

**Lemma 3.6**.: _Let \(_{A},_{D}(0,0.1)\) and \(B>1\) be parameters, and let \(V^{n d}\) denote a matrix with \(\|V\|_{} B\). Let \(A^{n n}\) be any matrix whose entires are all positive, and let \(^{n n}\) be a matrix such that, for all \((i,j)[n][n]\) we have_

\[|_{i,j}-A_{i,j}|_{A} A_{i,j}.\]

_Let \(D,^{n n}\) be any diagonal matrices with positive entries on their diagonals, with the property that, for all \(i[n]\), we have_

\[|_{i,i}-D_{i,i}|_{D} D_{i,i}.\]

_Then, we have_

\[\|^{-1}V-D^{-1}AV\|_{}(_{A}+ _{D}) B.\]

Due to space limitation, we delay the proof of Lemma 3.6 to Appendix A.

### Main Upper Bound

**Theorem 3.7**.: _For positive integers \(n,d\) and real parameters \(>0\) and \(B>1\), there are positive integers \(g=(\{)},B^{2}\})\) and \(r=\) such that: There is an algorithm (Algorithm 1) that runs in \(O(_{}(n,r,d)+nrg)\) time to solve \((n,d,B,)\) (Definition 1.2)._

Proof.: The running time of each step is shown in Algorithm 1; its running time follows from Lemma 3.4. Its correctness follows from Lemma 3.5 and Lemma 3.6. 

```
1:procedurePolyAttention(\(Q^{n d},K^{n d},V^{n d},n,d,B,\)) \(\) Theorem 1.4
2:\(\)\(\) is the accuracy output
3:\(\)\(\|Q\|_{},\|K\|_{},\|V\|_{} B\)
4:\(g O(\{)},B^{2}\})\)
5:\(r\)
6: Construct \(U_{1},U_{2}^{n r}\) via Lemma 3.4 \(\)\(O(nrg)\) time
7:\( U_{1}(U_{2}^{}_{n})\)\(\)\(O(nr)\) time
8:\(^{-1}=(^{-1})\)\(\)\(O(n)\) time
9: Compute \(U_{2}^{}V^{r d}\)\(\) Takes \(_{}(r,n,d)\) time
10: Compute \(U_{1}(U_{2}^{}V)\)\(\)\(_{}(n,r,d)\) time
11:\(T^{-1}(U_{1}(U_{2}^{}V))\)\(\)\(O(nd)\) time
12:return\(T\)\(\)\(T^{n d}\)
13:endprocedure ```

**Algorithm 1** Our Algorithm

```
1:procedurePolyAttention(\(Q^{n d},K^{n d},V^{n d},n,d,B,\)) \(\) Theorem 1.4
2:\(\)\(\) is the accuracy output
3:\(\)\(\|Q\|_{},\|K\|_{},\|V\|_{} B\)
4:\(g O(\{)},B^{2}\})\)
5:\(r\)
6: Construct \(U_{1},U_{2}^{n r}\) via Lemma 3.4 \(\)\(O(nrg)\) time
7:\( U_{1}(U_{2}^{}_{n})\)\(\)\(O(nr)\) time
8:\(^{-1}=(^{-1})\)\(\)\(O(n)\) time
9: Compute \(U_{2}^{}V^{r d}\)\(\) Takes \(_{}(r,n,d)\) time
10: Compute \(U_{1}(U_{2}^{}V)\)\(\)\(_{}(n,r,d)\) time
11:\(T^{-1}(U_{1}(U_{2}^{}V))\)\(\)\(O(nd)\) time
12:return\(T\)\(\)\(T^{n d}\)
13:endprocedure ```

**Algorithm 2** Our Algorithm

### Proof of Theorem 1.4

**Theorem 3.8** (Upper bound, formal statement of Theorem 1.4).: \((n,d=O( n),B=o(),_{a}=1/(n))\) _can be solved in time \(_{}(n,n^{o(1)},d)=n^{1+o(1)}\)._

Proof.: If we select the parameters

\[B=o(),\ \ =1/(n),\ \ \ d=O( n)\]

in Theorem 3.7, then we see that

\[g =O(\{)},B^ {2}\})\] \[=O(\{)},B^{2}\})\] \[=O(\{,o( n)\})\] \[=o( n),\]

where the second step follows from \(=1/(n)\) and the third step follows from \(B=o()\). Since \(g=o( n)\), let us write \(g=( n)/f\) for some \(f=(1)\). We thus have that

\[r=()^{2g} 2^{O(g((  n)/g))} 2^{O( n(f)/f)}<2^{o( n)}<n^{o(1)}.\]

The second step follows from the generic bound \((ea/b)^{b}\) for \(1 b a\), and the third step uses that \(d=O( n)\).

Since \(d,r,g\) are all bounded by \(n^{o(1)}\), our final running time is \(n^{1+o(1)}\) as desired. 

## 4 Hardness

In this section, we prove our fine-grained lower bound for attention computation. In Section 4.1, we state the Strong Exponential Time Hypothesis (\(\)), the main hardness assumption we will use. In Section 4.2, we define the approximate nearest neighbor search problem, and its known hardness assuming \(\). Finally, in Section 4.3, we give a reduction from approximate nearest neighbor search to attention computation, which implies our hardness result.

### Fine-Grained Hypotheses

The Strong Exponential Time Hypothesis (\(\)) was introduced by Impagliazzo and Paturi  over 20 years ago. It is a strengthening of the \(\) conjecture, which asserts that our current best \(\) algorithms are roughly optimal:

**Hypothesis 4.1** (Strong Exponential Time Hypothesis (\(\))).: _For every \(>0\) there is a positive integer \(k 3\) such that \(k\) on formulas with \(n\) variables cannot be solved in \(O(2^{(1-)n})\) time, even by a randomized algorithm._

\(\) is a popular conjecture which has been used to prove fine-grained lower bounds for a wide variety algorithmic problems. See, for instance, the survey .

### Nearest Neighbor Search

We will make use of a known relationship between \(\) and approximate nearest neighbor search.

**Definition 4.2** (Approximate Hamming Nearest Neighbor Search \(()\)).: _For a parameter \(>0\), in the \((1+)\)-Approximate Hamming Nearest Neighbor Search problem for \(n\) vectors of dimension \(d\), we are given as input two sets \(A,B\{0,1\}^{d}\) with \(|A|=|B|=n\), and our goal is to find an \(a^{*} A\) and \(b^{*} B\) satisfying \(\|a^{*}-b^{*}\|_{0}(1+)_{a A,b B}\|a-b\|_{0}\)._(This is sometimes called the 'bichromatic' \(\) problem, and a monochromatic version has also been studied; see, for instance, .) Rubinstein  showed that for certain parameters, it is impossible to substantially improve on the straightforward quadratic-time algorithm for \(\) assuming \(\):

**Lemma 4.3** ().: _Assuming \(\), for every \(q>0\), there are \((0,1)\) and \(C>0\) such that \((1+)\)-Approximate Hamming Nearest Neighbor Search in dimension \(d=C n\) requires \((n^{2-q})\) time._

**Remark 4.4**.: _We may assume that 4.3 holds even in the special case where each input vector from \(A\) and \(B\) has half its entries equal to \(0\) and half equal to \(1\). Indeed, for any vector \(a\{0,1\}^{d}\), we can construct a new vector \(\{0,1\}^{2d}\) given by \(=a^{}&^{}^{}\). Here \(\{0,1\}^{d}\) is the binary complement of vector \(a\), i.e., \(_{i}=1-a_{i}\) for all \(i[d]\). Thus, \(\|\|_{0}=d\). We can similarly construct a new vector \(\{0,1\}^{2d}\) for each \(b B\). After this transformation, for any \(a A\) and \(b B\), we have \(\|-\|_{0}=2\|a-b\|_{0}\), so it suffices to find an approximate nearest neighbor among these transformed vectors._

For convenience in our the analysis, we define a gap version of approximate nearest neighbor search problem \(-(n,d,t,)\).

**Definition 4.5** (Gap approximate nearest neighbor search (\(-(n,d,t,)\))).: _Let \(n,d\) denote two positive integers. Let \(t>0\) denote a threshold parameter. Let \(\) denote a accuracy parameter. Given two sets of points \(A=\{a_{1},,a_{n}\}\{0,1\}^{d}\) and \(B=\{b_{1},,a_{n}\}\{0,1\}^{d}\): For each \(i[n]\), we need to distinguish the following two cases_

* _Case 1. There exists a_ \(j[n]\) _such that_ \(\|a_{i}-b_{j}\|_{0}<t\)_._
* _Case 2. For all_ \(j[n]\) _we have_ \(\|a_{i}-b_{j}\|_{2}^{2}(1+) t\)_._

An algorithm for \(-(n,d,t,)\) can be called \((nd)\) times to binary search for the answer to \(\), so Lemma 4.3 holds as well for \(-(n,d,t,)\).

### Hardness Result

In the remainder of this section, we prove our lower bound for attention computation:

**Theorem 4.6** (Main Result, formal version of Theorem 1.3).: _Assuming \(\), for every sufficiently small \(q>0\), there are constants \(C>0\) and \(C_{}>0\) and \(C_{}>1\) such that Approximate Attention Computation \(\) (Definition 1.2) for parameters \((n,d=C n,B=C_{},_{a}=n^{-C_{}})\) requires \((n^{2-q})\) time._

Proof.: This follows from combining Lemma 4.3 (hardness for approximation nearest neighbor search) and Lemma 4.7 (a reduction from approximate nearest neighbor search to approximate attention computation) which we prove below. 

**Lemma 4.7**.: _For any constant \(C_{}(0,0.1)\): For every \(>0\) and \(C>0\), there exist constants \(C_{a}>0\) and \(C_{b}>0\) and such that, if \(\) (Definition 1.2) for parameters \((2n,d=2C n,B=C_{b},_{a}=n^{-C_{a}})\) can be solved in time \(T\), then \(-(n,d=C n,t,)\) (Definition 4.5) can be solved in time \(O(T+n^{2-C_{}})\)._

Due to space limitation, we defer the proof of Lemma 4.7 to Appendix B.

## 5 Conclusion

In this work, we showed that how quickly one can perform attention computation depends critically on the magnitude, \(B\), of the entries of the input matrices. Our main idea was to make use of similarities between attention computation and KDE, and to show how many known techniques for KDE can also be used in this setting. Since KDE is a very well-studied problem, it would be exciting to see what other techniques can be applied to attention computation as well. One limitation of our lower bound result is, it is a conditional lower bound which is based on a well-known conjecture SETH in the area of complexity. It would be interesting to show unconditional lower bound for future work. As far as we are aware, our work does not have negative societal impacts.

AcknowledgementsJosh Alman was partly supported by a grant from the Simons Foundation (Grant Number 825870 JA). The authors would like to thank Beidi Chen for helpful discussions related to LLMs, and Feyza Duman, Chinmay Hegde, and Piotr Indyk for helpful comments on an earlier draft. The authors would like to appreciate very constructable feedbacks for NeurIPS 2023 Reviewers. The authors would like to thanks Lichen Zhang and Ruizhe Zhang for useful and helpful suggestions about proof-reading the paper.