# Dynamic Regret of Adversarial Linear Mixture MDPs

Long-Fei Li, Peng Zhao, Zhi-Hua Zhou

National Key Laboratory for Novel Software Technology, Nanjing University, China

School of Artificial Intelligence, Nanjing University, China

{lilf, zhaop, zhouzh}@lamda.nju.edu.cn

###### Abstract

We study reinforcement learning in episodic inhomogeneous MDPs with adversarial full-information rewards and the unknown transition kernel. We consider the linear mixture MDPs whose transition kernel is a linear mixture model and choose the _dynamic regret_ as the performance measure. Denote by \(d\) the dimension of the feature mapping, \(H\) the length of each episode, \(K\) the number of episodes, \(P_{T}\) the non-stationary measure, we propose a novel algorithm that enjoys an \(}H^{3}K}+(K+P_{T})(1+P_{T})} \) dynamic regret under the condition that \(P_{T}\) is known, which improves previously best-known dynamic regret for adversarial linear mixture MDP and adversarial tabular MDPs. We also establish an \(H^{3}K}+)}\) lower bound, indicating our algorithm is _optimal_ in \(K\) and \(P_{T}\). Furthermore, when the non-stationary measure \(P_{T}\) is unknown, we design an online ensemble algorithm with a meta-base structure, which is proved to achieve an \(}H^{3}K}+(K+P_{T})(1+P_{T}) +H^{2}_{T}^{2}}\) dynamic regret and here \(S_{T}\) is the expected switching number of the best base-learner. The result can be optimal under certain regimes.

## 1 Introduction

Reinforcement Learning (RL) aims to learn a policy that maximizes the cumulative reward through interacting with the environment, which has achieved tremendous successes in various fields, including games , robotic control , and dialogue generation . In reinforcement learning, the Markov Decision Process (MDP)  is the most widely used model to describe the environment.

Traditional MDPs assume that the reward functions are stochastic and the number of actions and states is small. However, in many real-world applications, the reward functions may be adversarially changing and the state and action spaces are large or even infinite. Previous work studies these two problems separately. To deal with the adversarial reward functions, Even-Dar et al.  first consider learning _adversarial_ MDPs with known transition and full-information reward feedback. They propose the MDP-E algorithm that enjoys \(}(T})\) regret, where \(\) is the mixing time and \(T\) is the number of steps. There is a line of subsequent work studying adversarial MDPs , which studies various settings depending on whether the transition kernel is known, and whether the feedback is full-information or bandit. To overcome the large state and action space issue, a widely used method is _linear function approximation_, which reparameterizes the value function as a linear function over some feature mapping that maps the state and action to a low-dimensional space. Amongst these work, linear MDPs  and linear mixture MDPs  are two of the most popular MDP models with linear function approximation. In particular, He et al.  and Zhou et al.  attain the minimax optimal \(}(H^{3}K})\) regret for linear MDPs and linear mixture MDPs with stochastic rewards respectively.

Recent studies try to combine two lines of work to establish the theoretical foundation of adversarial MDPs with large state and action space. In particular, Cai et al.  study adversarial linear mixtureMDPs and propose the OPPO algorithm that enjoys an \(}(H^{4}K})\) regret. He et al.  improve the result to \(}(H^{3}K})\) and show it is (nearly) optimal by presenting a matching lower bound. However, these studies choose _static regret_ as the performance measure, defined as the performance difference between the learner's policy and that of the best-fixed policy in hindsight, namely,

\[(K)=_{}_{k=1}^{K}V_{k,1}^{}(s_{k,1})-_{ k=1}^{K}V_{k,1}^{_{k}}(s_{k,1}),\] (1)

where \(V\) is the value function and \(\) is the set of all stochastic policies. One caveat in (1) is that the best _fixed_ policy may behave poorly in non-stationary environments. To this end, we introduce _dynamic regret_, which benchmarks the learner's performance with changing policies, defined as

\[(K)=_{k=1}^{K}V_{k,1}^{_{k}^{c}}(s_{k,1})-_{k=1}^ {K}V_{k,1}^{_{k}}(s_{k,1}),\] (2)

where \(_{1}^{c},,_{K}^{c}\) is compared policies. Define \(P_{T}=_{k=2}^{K}d(_{k}^{c},_{k-1}^{c})\) with a certain distance measure \(d(,)\) as the non-stationary measure. A favorable dynamic regret should scale with \(P_{T}\).

Dynamic regret is a more appropriate measure in non-stationary environments, but it is more challenging to optimize such that few studies focus on it in the literature. Zhao et al.  investigate the dynamic regret of adversarial tabular MDPs with the _known_ transition kernel and present an algorithm with optimal dynamic regret. Importantly, their algorithm does not require the non-stationarity measure \(P_{T}\) as the algorithmic input ahead of time. For the unknown transition setting, Fei et al.  study adversarial tabular MDPs and propose an algorithm with dynamic regret guarantee. Zhong et al.  further extend the algorithm of Fei et al.  to accommodate non-stationary transition kernels with linear function approximation. Both algorithms of Fei et al.  and Zhong et al.  require the quantity of \(P_{T}\) as the input. Moreover, their dynamic regret bounds are suboptimal in terms of \(K\) and \(P_{T}\) as demonstrated by the lower bound established by our work (see Theorem 4).

This work investigates the dynamic regret of adversarial linear mixture MDPs, with a focus on the full-information feedback and the unknown transition. We first propose POWERS-FixShare algorithm when \(P_{T}\) is known, an algorithm combining optimistic policy optimization with a Bernstein bonus and fixed-share mechanism. We show it enjoys an \(}(H^{3}K}+(K+P_{T})(1+P_{T})})\) dynamic regret, where \(d\) is the dimension of the feature mapping, \(H\) is the length of each episode, \(K\) is the number of episodes, \(P_{T}\) is the non-stationary measure. We also establish a dynamic regret lower bound of \((H^{3}K}+)})\). We stress four remarks regarding our results:

1. Our result can recover the \(}(H^{3}K})\) minimax optimal static regret in He et al. .
2. Our result improves upon the previously best-known \(}(dH^{7/4}K^{3/4}+H^{2}{K^{2/3}P_{T}}^{1/3})\) dynamic regret for the fixed transition of Zhong et al. [36, Theorem 4.6] in terms of \(H\), \(K\) and \(P_{T}\).
3. Our result can imply an \(}(S^{2}AK}+H^{2})(1+P_{T})})\) dynamic regret for adversarial tabular MDPs, strictly improving upon the previously best-known \(}(S^{2}AK}+H^{2}{K^{2/3}P_{T}}^{1/3})\) dynamic regret of Fei et al. [35, Theorem 1], in terms of both \(K\) and \(P_{T}\).
4. As the lower bound suggests, our result is the first optimal regarding the dependence on \(d\), \(K\) and \(P_{T}\) and can be optimal in terms of \(H\) under certain regimes (\(H d\) and \(P_{T} d^{2}/H\)).

Furthermore, we study the case when \(P_{T}\) is unknown and design a novel algorithm equipped with the dynamic regret guarantee by the meta-base two-layer structure. Our algorithm enjoys an \(}(H^{3}K}+(K+P_{T})(1+P_{T})+H^{2 }S_{T}^{2}})\) dynamic regret, where \(S_{T}\) is the expected switching number of the best base-learner. Though \(S_{T}\) is a data-dependent quantity, it also reflects the degree of environmental non-stationarity to some extent. Moreover, under specific regimes, the magnitude of \(S_{T}\) may be relatively negligible, resulting in our results still being optimal. Indeed, given that \(S_{T}\) is a data-dependent quantity, its inclusion in the regret bound is not ideal. Deriving bounds that rely exclusively on problem-dependent quantities, like \(P_{T}\), remains an open challenge. We discuss the technical difficulty of removing \(S_{T}\) in Section 5 and take this issue for future work.

Finally, we also highlight the main technical challenges and our solutions as follows.

* We first show that the dynamic regret depends on the inverse of the minimal probability over the action space of our policies, which can be arbitrarily small. To this end, we propose a novel algorithm with the _fixed-share_ mechanism . While this mechanism is proved to enjoy favorable dynamic regret in online convex optimization , it suffers an additional term that can be regarded as the weighted sum of the difference between the occupancy measure of the compared policies in online MDPs. To overcome the difficulty, we exploit the _multiplicative stability_ to bound this term, eliminating the need for a restart strategy to handle the environmental non-stationarity as in previous studies [35; 36] and allows us to attain the dynamic regret optimal in terms of \(K\) and \(P_{T}\).
* We show the dynamic regret of online MDPs can be written as the weighted average of "multi-armed bandits" problems over all states, where the weight for each state is the _unknown_ and _changing_ probability of being visited by \(_{1}^{c},,_{K}^{c}\). For the unknown \(P_{T}\) case, we first show the standard two-layer structure used in non-stationary online learning studies [39; 40; 41] fails to achieve a favorable dynamic regret guarantee, which characterizes the unique difficulty of online MDPs. Then, we present an initial attempt to address this issue by a specifically designed two-layer structure. We prove our algorithm enjoys nice dynamic regret guarantees under certain regimes.

Notations.We denote by \(()\) the set of probability distributions on a set \(\) and denote the KL-divergence by \(D_{}(p||p^{})=_{a}p(a)\) for any \(p,p^{}()\). We define \((,H)=\{\{_{h}()\}_{h=1}^{H} _{h}( x)(), s,h[ H]\}\) for any set \(\) and \(H_{+}\). Further, for any \(,^{},^{}(,H)\), we define \(_{}[_{}(^{}\|^{} )]=_{}[_{h=1}^{H}D_{}(^{}_{h}( s _{h})\|^{}_{h}( s_{h}))]\). For any policy pair \(_{h},^{}_{h}\), we define \(\|_{h}-^{}_{h}\|_{1,}=_{s}\|_{h}(  s)-^{}_{h}( s)\|_{1}\). For any \(a,b,x\) with \(a b\), let \([x]_{[a,b]}\) denote \(\{(x,a),b\}\). \(}()\) omits the logarithmic factors.

## 2 Related Work

RL with adversarial rewards.There are many studies on learning adversarial MDPs where the reward functions are adversarially chosen, yielding fruitful results that can be categorized into three lines [8; 9; 10; 11; 12; 13; 14; 15; 16]. In particular, the first line of work considers the infinite-horizon MDPs with uniform mixing assumption. In the known transition and full-information setting, the seminal work of Even-Dar et al.  proposes MDP-E algorithm that achieves the \(}(T})\) regret, where \(\) is the mixing time and \(T\) is the number of steps. Another concurrent work of Yu et al.  achieves \(}(T^{2/3})\) in the same setting. In the known transition and bandit-feedback setting, Neu et al.  propose MDP-EXP3 algorithm that attains \(}(T^{2/3})\) regret. The second line of work considers the episodic loop-free MDPs. Neu et al.  first study this problem under the known transition setting and propose algorithms that achieve \(}()\) and \(}()\) for full-information and bandit-feedback respectively, where \(\) is the lower bound of the probability of all states under any policy. Zimin and Neu  propose O-REPS algorithm that enjoys \(}()\) regret in both full-information and bandit-feedback setting without any additional assumption. Rosenberg and Mansour  and Jin et al.  further consider the harder unknown transition and bandit-feedback setting. The last line of work studies the episodic Stochastic Shortest Path (SSP) setting [15; 16]. In this paper, we focus on episodic MDPs with the unknown transition and full-information setting.

RL with linear function approximation.To design RL algorithms in large state and action space scenarios, recent works focus on solving MDPs with linear function approximation. In general, these works can be divided into three lines based on the specific assumption of the underlying MDP. The first line of work considers the low Bellman-rank assumption [42; 43; 44; 45], which assumes the Bellman error matrix has a low-rank factorization. The second line of work is based on the linear MDP assumption [17; 18; 19; 20; 21; 22; 23], where both the transition kernel and reward functions can be parameterized as linear functions of given state-action feature mappings \(:^{d}\). The last line of work studies the linear mixture MDP [24; 25; 26; 28; 29; 30; 31], where the transition kernel can be parameterized as a linear function of a feature mapping \(:^{d}\) but without the linear reward functions assumption. Amongst these works, He et al.  and Zhou et al.  attain the minimax optimal \(}(H^{3}K})\) regret for both episodic linear MDPs and linear mixture MDPs respectively. However, all the above studies consider the stochastic reward setting. In this paper, we study the episodic linear mixture MDP setting but with adversarial reward functions.

Non-stationary RL.Another related line of research is online non-stationary MDPs. In contrast to adversarial MDPs where the reward functions are generated in an adversarial manner, online non-stationary MDPs consider the setting where the reward functions are generated stochastically according to some distributions that may vary over time. Jaksch et al.  study the piecewise stationary setting where the transitions and rewards are allowed to change certain times and propose UCRL2 with restart technique to deal with the non-stationarity. Later, Ortner et al.  consider the generalized setting where the changes are allowed to take place every step. However, the above studies need prior knowledge about the magnitude of non-stationary. To address this issue, Cheung et al.  propose the Bandit-over-RL algorithm to remove this requirement. A recent breakthrough by Wei and Luo  introduces a black-box method that can convert any algorithm satisfying specific conditions and enjoying optimal static regret in stationary environments into another with optimal dynamic regret in non-stationary environments, without requiring prior knowledge about the degree of non-stationarity. However, this approach does not apply to the adversarial setting. Specifically, their reduction requires the base algorithm to satisfy a certain property enjoyed by typical UCB-type algorithms. When a new instance of the base algorithm surpasses the optimistic estimator, it can be inferred that the environment has changed, prompting a restart of the algorithm to disregard prior information. However, this approach of constructing an optimistic estimator by a UCB-type algorithm can only be applied to a _stochastic_ setting. In the _adversarial_ setting, where no model assumptions are made and comparators can be arbitrary, this approach encounters significant difficulties.

Dynamic Regret.Dynamic regret of RL with adversarial rewards is only recently studied in the literature [34; 35; 36]. Zhao et al.  investigate the dynamic regret of adversarial tabular MDPs with the _known_ transition kernel and present an algorithm with optimal dynamic regret. Importantly, their algorithm does not require the non-stationarity measure \(P_{T}\) as the algorithmic input ahead of time. For the unknown transition setting, Fei et al.  study adversarial tabular MDPs and propose an algorithm with dynamic regret guarantees. Zhong et al.  further extend the algorithm of Fei et al.  to accommodate non-stationary transition kernels with linear function approximation. Both algorithms [35; 36] require the quantity of \(P_{T}\) as the input. Moreover, their dynamic regret bounds are suboptimal in \(K\) and \(P_{T}\) as shown by the lower bound established in our work. In this work, we first design an _optimal_ algorithm in terms of \(K\) and \(P_{T}\) when \(P_{T}\) is known. Further, we develop the first algorithm to handle the unknown \(P_{T}\) issue in adversarial MDPs with unknown transition.

## 3 Problem Setup

We focus on episodic inhomogeneous MDPs with full-information reward functions and the unknown transition kernel. Denote by \(M=\{,,H,\{r_{k,h}\}_{k[K],h[H]},\{_{h}\} _{h[H]}\}\) an episodic inhomogeneous MDP, where \(\) is the state space, \(\) is the action space, \(K\) is the number of episodes, \(H\) is the horizon, \(r_{k,h}:\) is the reward function, \(_{h}(,): \) is the transition kernel. We assume the rewards are deterministic without loss of generality and extending our results to stochastic rewards is straightforward. Let \(S=||\) and \(A=||\).

The learner interacts with the MDP through \(K\) episodes without knowledge of transition kernel \(\{_{h}\}_{h[H]}\). In each episode \(k\), the environment chooses the reward function \(\{r_{k,h}\}_{h[H]}\) and decides the initial state \(s_{k,1}\), where the reward function may be chosen in an adversarial manner and depend on the history of the past \((k-1)\) episodes. Simultaneously, the learner decides a policy \(_{k}=\{_{k,h}\}_{h[H]}\) where each \(_{k,h}:()\) is a function that maps a state \(s\) to a distribution over action space \(\). In the \(h\) stage in episode \(k\), the learner observes current state \(s_{k,h}\), chooses an action \(a_{k,h}_{k,h}( s_{k,h})\), and transits to the next state \(s_{k,h+1}_{h}( s_{k,h},a_{k,h})\). Then the learner obtains the reward \(r_{k,h}(s_{k,h},a_{k,h})\) and observes the reward function \(r_{k,h}\) as we consider the full-information setting. At stage \(H+1\), the learner observes the final state \(s_{k,H+1}\) but does not take any action, and the episode \(k\) terminates. Denote by \(T=KH\) the total steps throughout \(K\) episodes.

Linear Mixture MDPs.In this work, we focus on a special class of MDPs called _linear mixture MDPs_, a setting initiated by Ayoub et al.  and further studied in the subsequent work [32; 31; 33]. In this setup, the transition kernel can be parameterized as a linear function of a feature mapping \(:^{d}\). The formal definition of linear mixture MDPs is as follows.

**Definition 1** (Linear Mixture MDPs).: An MDP \(M=\{,,H,\{r_{k,h}\}_{k[K],h[H]},\{_{h}\} _{h[H]}\}\) is called an inhomogeneous, episode \(B\)-bounded linear mixture MDP, if there exist a _known_ feature mapping \((s^{} s,a): ^{d}\) and an _unknown_ vector \(_{h}^{s}^{d}\) with \(\|_{h}^{s}\|_{2} B\), \( h[H]\) such that (i) \(_{h}(s^{} s,a)=(s^{} s,a)^{}_{h}^{s}\) for all \((s,a,s^{})\) and \(h[H]\), (ii) \(\|_{V}(s,a)\|_{2}\|_{s^{}}(s^{}  s,a)V(s^{})\|_{2} 1\) for any \((s,a)\) and function \(V:\).

**Dynamic Regret.** For any policy \(=\{_{h}\}_{h[H]}\) and any \((k,h,s,a)[K][H]\), we define the action-value function \(Q_{k,h}^{}\) and value function \(V_{k,h}^{}\) as

\[Q_{k,h}^{}(s,a)=_{}[_{b^{}=h}^{H}r_{k,h^{}}( s_{h^{}},a_{h^{}})\,\,s_{h}=s,a_{h}=a],V_{k,h}^{}(s)= _{}[_{b^{}=h}^{H}r_{k,h^{}}(s_{h^{}},a _{h^{}})\,\,s_{h}=s].\]

The Bellman equation is given by \(Q_{k,h}^{}=r_{k,h}+_{h}V_{k,h+1}^{}\), and \(V_{k,h}^{}(s)=_{a_{h}(\,\,|\,s)}[Q_{k,h}^{}(s,a)]\) with \(V_{k,H+1}^{}=0\). For simplicity, for any function \(V:\), we define the operator

\[[_{h}V](s,a)=_{s^{}_{h}(\,\,|\,s,a)}V(s^{}),[_{h}V](s,a)=[_{h}V^{2}](s,a)-([ _{h}V](s,a))^{2}.\] (3)

As stated in Section 1, dynamic regret is a more appropriate measure compared with static regret for the adversarial environments, which is defined in (2) and we rewrite it below for clarity:

\[(K)=_{k=1}^{K}V_{k,1}^{_{k}^{c}}(s_{k,1})-_{k=1}^ {K}V_{k,1}^{_{k}}(s_{k,1}),\] (4)

where \(_{1}^{c},,_{K}^{c}\) is any sequence of compared policies. We define \(_{0}^{c}=_{1}^{c}\) to simplify the notation. The non-stationarity measure is defined as \(P_{T}=_{k=1}^{K}_{h=1}^{H}_{k,h}^{c}-_{k-1,h}^{c}_{1,}\).

## 4 Optimal Dynamic Regret with Known \(P_{t}\)

We present our proposed algorithm in Algorithm 1. Similar to previous studies, the algorithm consists of (i) policy improvement phase, and (ii) policy evaluation phase. We introduce the details below. In Sections 4.1, we first consider the case when the transition is _known_ to highlight the challenges even under the ideal setting. Then, we extend the results to the _unknown_ transition setting in Section 4.2.

### Policy Improvement Phase

In the policy improvement phase, the algorithm updates \(_{k}\) based on \(_{k-1}\) using the proximal policy optimization (PPO) method . Specifically, at episode \(k\), we define the following linear function:

\[L_{k-1}()=V_{k,1}^{_{k-1}}(s_{k,1})+_{_{k-1}} [_{h=1}^{H} Q_{k-1,h}^{_{k-1}},_{h}( s_{h})-_ {k-1,h}( s_{h})\ \ s_{1}=s_{k,1}],\]

which is the first-order Taylor approximation of \(V_{k-1,1}^{}(s_{k,1})\) around \(_{k-1}\). Then, we update \(_{k}\) by

\[_{k}=*{arg\,max}_{( ,H)}L_{k-1}()-_{_{k-1}}[_{h= 1}^{H}D_{}_{h}( s_{h})\|_{k-1,h}( s _{h})],\] (5)

where \(>0\) is the stepsize and the KL-divergence encourages \(_{k}\) to be close to \(_{k-1}\) so that \(L_{k-1}()\) is a good approximation of \(V_{k-1,1}^{}(s_{k,1})\). The update rule in (5) takes the following closed form,

\[_{k,h}( s)_{k-1,h}( s)  Q_{k-1,h}^{_{k-1}}(s,), h[H],s .\] (6)

We show the update rule in (6) ensures the following guarantee and the proof is in Appendix C.1.

**Lemma 1**.: _The update rule in (6) ensures the following dynamic regret guarantee:_

\[(K)}{2}+ _{k=1}^{K}_{_{k}^{c}}[_{}(_ {k}^{c}\|_{k})-_{}(_{k}^{c}\|_{k+ 1})].\] (7)

Note that the expectation in the last term in (7) is taken over \(_{k}^{c}\) which may _change_ over episode \(k\). For static regret, i.e., \(_{1}^{c}==_{K}^{c}=^{*}\), we can control this term by a standard telescoping argument, which is not viable for dynamic regret analysis. Fei et al.  propose a restart strategy to handle this term. Specifically, they restart the algorithm every certain number of steps and decompose the above expectation into \(_{_{k}^{c}}[]=_{_{k_{0}}^{c}}[]+_{_{k}^{c}-_{k_{0}}^{c}}[]\) where \(k_{0}<k\) is the episode in which restart takes place most recently before episode \(k\). For the first expectation \(_{_{k_{0}}^{c}}[]\), they apply a customized telescoping argument to each period as the expectation is taken over the fixed policy. The second expectation \(_{_{k}^{c}-_{k_{0}}^{c}}[]\) involves the difference \(_{k}^{c}-_{k_{0}}^{c}\) and can be bounded by \(P_{T}\). However, as we will show in Theorem 4, their regret bound is suboptimal in terms of \(K\) and \(P_{T}\).

We introduce our approach below. Let us first consider taking expectations over any fixed policy \(\). Denote by \(\) the minimal probability over any action at any state for policies \(_{1},,_{K}\), i.e., \(=_{k[K]}_{k}(a s), a,s\), the last term in (7) can be upper bounded by \(_{k=1}^{K}_{}[_{}(_{k}^{c}\| _{k})-_{}(_{k}^{c}\|_{k+1})]  H A+P_{T}\), showing that we need to control the minimal value of \(\) to obtain a favorable dynamic regret bound. To this end, we slightly modify the update rule in (6) and add a uniform distribution \(^{u}( s)=\), \( s\). That is, the policy \(^{u}\) chooses each action with equal probability at any state. Thus, the update rule in (6) is modified as:

\[_{k,h}^{}( s)_{k-1,h}( s) ( Q_{k-1,h}^{_{k-1}}(s,)),_{k,h}( s)=(1- )_{k,h}^{}( s)+^{u}( s)\] (8)

for any \(s,h[H]\), where \( 0\) is the exploration parameter. This update is called the _fixed-share_ mechanism in online learning literature . While the fixed-share mechanism is standard to obtain dynamic regret in modern online learning , several important new challenges arise in online MDPs due to taking expectations over the policy sequence of _changing_ policies \(_{1}^{c},,_{K}^{c}\). In particular, we prove that performing (8) ensures the following dynamic regret.

**Lemma 2**.: _Set \(_{1}\) as uniform distribution on \(\) for any state \(s\). The update rule in (8) ensures_

\[(K)}{2}+(P _{T}+KH)\] \[+_{k=1}^{K}_{h=1}^{H}_{_{k}^{c} }[_{a}(_{k-1,h}^{c}(a s_{h})^{}(a s_{h})}-_{k,h}^{c}(a s_{h})^{}(a s_{h})})]\] (9)

The proof can be found in Appendix C.2. In the dynamic regret analysis in online learning, the last term in (9) is usually canceled out through telescoping since we do not need to take expectations .

However, this is _not_ the case in online MDPs. Since the expectation is taken over the policy sequence of _changing_ policies \(^{c}_{1},,^{c}_{K}\), this term cannot be canceled out, which requires a more refined analysis. To address this issue, we decompose one step of the expectation in (9) as follows.1

\[(_{^{c}_{k-1}}[_{a}^{c}_{k-1,h} _{k,h}}]-_{^{c}_{k}}[_{a }^{c}_{k,h}_{k+1,h}}])+ _{^{c}_{k-n}^{c}_{k-1}}[_{a}^{c}_{k-1, h}_{k,h}}].\]

With this decomposition, the first term can be canceled out through telescoping, yet it remains to control the second term -- the weighted difference between the state-action occupancy measures of policy \(^{c}_{k}\) and \(^{c}_{k-1}\) with weight \(-^{c}_{k-1,h}(a s_{h})^{}_{k,h}(a s_{h})\) for state-action \((s_{h},a)\). To control it, we need to (i) ensure the weight is upper bounded by some universal constant, and (ii) bound the unweighted difference between the state-action occupancy measures, which are new challenges that arose in online MDPs compared with standard online learning.

For the first challenge, note that the weight \(-^{c}_{k-1,h}(a s_{h})^{}_{k,h}(a s_{h})\) can be large or even infinite since \(^{}_{k,h}\) is the policy before uniform exploration and \(^{}_{k,h}(a s_{h})\) can be arbitrarily small. Fortunately, \(^{}_{k,h}\) is obtained by one-step descent from \(_{k-1,h}\), which is the policy after uniform exploration and can be lower bounded. We provide the following _multiplicative stability lemma_ for the one-step update, which shows \(^{}_{k,h}\) is not far from \(_{k-1,h}\) and thus is also lower bounded.

**Lemma 3** (Multiplicative Stability).: _For any distributions \(p()\) with \(p(a)>0\), for all \(a A\), and any function \(Q:[0,H]\), it holds for \(p^{}()\) with \(p^{}(a) p(a)( Q(s,a))\) and \( 1/H\) that \(p^{}(a)[p(a)/4,4p(a)]\), for all \(a\)._

For the second challenge, we show the unweighted difference between the state-action occupancy measures can be bounded by the path length of policies. In particular, we have the following lemma.

**Lemma 4**.: _For any policy sequence \(^{c}_{1},,^{c}_{K}\), it holds that_

\[_{k=1}^{K}(_{^{c}_{k}}-_{^{c}_{k-1}} )[_{h=1}^{H}(s_{h}) s_{1}=s_{k,1}] _{k=1}^{K}_{h=1}^{H}_{i=1}^{h}^{c}_{k,i}-^{c}_{k-1,i} _{1,}=HP_{T}.\]

**Remark 1**.: We note that a similar argument is also used in Fei et al. [35, Appendix B.2.2]. However, they prove this lemma by imposing an additional smooth visitation measures assumption [35, Assumption 1], which is not required in our analysis.

The proofs for Lemmas 3 and 4 can be found in Appendices C.3 and C.4 respectively. Combining Lemmas 2, 3 and 4, we can prove the guarantee for update rule (8). The proof is in Appendix C.5.

**Theorem 1**.: _Set \(_{1}\) as uniform distribution on \(\) for any state \(s\). The update rule in (8) ensures_

\[(K)}{2}+(H  A+(1+H)P_{T}+KH).\]

**Remark 2**.: Considering the static regret where \(^{c}_{1}==^{c}_{K}=^{*}\), we can recover the \((K A})\) static regret in Cai et al.  under the stationary scenario by setting \(=0\), that is, without uniform exploration. However, when \(=0\), the dynamic regret is not bounded as there lacks an upper bound for \(-\), showing the necessity of the fixed-share mechanism.

### Policy Evaluation Phase

Sections 4.1 focus on the simplified scenario where the transition is known. In this subsection, we further consider the unknown transition setting such that it is necessary to evaluate the policy \(_{k}\) based on the \((k-1)\) historical trajectories. To see how the model estimation error enters the dynamic regret, we decompose the dynamic regret in the following lemma.

**Lemma 5** (Fei et al. [35, Lemma 1]).: _Define the model prediction error as \(_{k,h}=r_{k,h}+_{h}V_{k,h+1}-Q_{k,h}\), the dynamic regret \((K)=_{k=1}^{K}V_{k,1}^{^{c}_{k}}(s_{k,1})- _{k=1}^{K}V_{k,1}^{_{k}}(s_{k,1})\) can be written as_

\[_{k,h}_{^{c}_{k}} Q_{k,h}(s_{h}, ),^{c}_{k,h}( s_{h})-_{k,h}( s_{h})+ _{K,H}+_{k,h}_{^{c}_{k}}[_{k,h}(s_{h},a _{h})]-_{k,h}(s_{k,h},a_{k,h}),\]

_where \(_{K,H}=_{k=1}^{K}_{h=1}^{H}M_{k,h}\) is a martingale that satisfies \(M_{k,h} 4H\), \( k[k],h[H]\)._

**Remark 3**.: Lemma 5 is independent of the structure of MDPs. The first term in Lemma 5 is the dynamic regret over the estimated action-value function \(Q_{k,h}\), which can be upper bounded by Theorem 5. The second term is a martingale, which can be bounded by Azuma-Hoeffding inequality. The third term is the model estimation error, which is the main focus of this section. Note the model prediction error \(t_{k,h}(s_{h},a_{h})\) can be large for the state-action pairs that are less visited or even unseen. The general approach is incorporating the bonus function into the estimated \(Q\)-function such that \(_{k,h}(s_{h},a_{h}) 0\) for all \(s,a\) (i.e., \(_{_{k}^{c}}[_{k,h}(s_{h},a_{h})] 0\)) and we only need to control \(-_{k,h}(s_{k,h},a_{k,h})\), which is the model estimation error at the visited state-action pair \((s_{k,h},a_{k,h})\).

When applied to linear mixture MDPs, the key idea is learning the unknown parameter \(_{h}^{*}\) of the linear mixture MDP and using the learned parameter \(_{k,h}\) to build an optimistic estimator \(Q_{k,h}(,)\) such that the model prediction error is non-positive, which is more or less standard. From the definition of linear mixture MDP, for the learned value function \(V_{k,h}()\), we have \([_{h}V_{k,h+1}](s,a)=_{s^{}}(s^{} s, a)V_{k,h+1}(s^{}),_{h}^{*}= V_{k,h+1}(s,a), _{h}^{*}\). Inspired by recent advances in policy evaluation for linear mixture MDPs , we adopt the _weighted ridge regression_ to estimate the parameter \(_{h}^{*}\), that is, we construct the estimator \(_{k,h}\) by solving the following weighted ridge regression problem:

\[_{k,h}=*{arg\,min}_{^{d}} _{j=1}^{k-1}[_{V_{j,h+1}}(s_{j,h},a_{j }),-V_{j,h+1}(s_{j,h+1})]^{2}/ _{j,h}^{2}+\|\|_{2}^{2}.\]

Here, \(_{j,h}^{2}\) is the upper confidence bound of the variance \([_{h}V_{j,h+1}](s_{j,h},a_{j,h})\), and we set it as \(_{k,h}=/d,[_{k,h}V_{k,h+1}](s_{k,h },a_{k,h})+E_{k,h}\}}\), where \([_{k,h}V_{k,h+1}](s_{k,h},a_{k,h})\) is a scalar-valued empirical estimate for the variance of the value function \(V_{k,h+1}\) under the transition probability \(_{h}( s_{k},a_{k})\), and \(E_{k,h}\) is the bonus term to guarantee that the true variance \([_{k,h}V_{k,h+1}](s_{k,h},a_{k,h})\) is upper bounded by \([_{k,h}V_{k,h+1}](s_{k,h},a_{k,h})+E_{k,h}\) with high probability. Then, the confidence set \(}_{k,h}\) is constructed as follows:

\[}_{k,h}=\|_{k,h}^{1/2}( -_{k,h})\|_{2}_{k}}.\] (10)

where \(_{k,h}\) is a covariance matrix based on the observed data, and \(_{k}\) is a radius of the confidence set. Given \(}_{k,h}\), we estimate the \(Q\)-function following the principle of "optimism in the face of uncertainty"  and set it as \(Q_{k,h}(,)=[r_{k,h}(,)+_{}_{k,h}},_{V_{k,h+1}}(,)]_{[0,H-h+1]}\).

It remains to estimate the variance \([_{h}V_{k,h+1}](s_{k,h},a_{k,h})\). By the definition of linear mixture MDPs, we have \([_{h}V_{k,h+1}](s_{k,h},a_{k,h})=_{V_{k,h+1}^{2}}(s_{k,h },a_{k,h}),_{h}^{*}-[_{V_{k,h+1}}(s_{k,h},a_{k,h}), _{h}^{*}]^{2}\). Therefore, we estimate \([_{k,h}V_{k,h+1}](s_{k,h},a_{k,h})\) by the expression below

\[[_{V_{k,h+1}^{2}}(s_{k,h},a_{k,h}),_{k,h}]_{[0,H^{2}]}-_{V_{k,h +1}}(s_{k,h},a_{k,h}),_{k,h}_{[ 0,H]}^{2},\] (11)

where \(_{k,h}=*{arg\,min}_{^{d}} _{j=1}^{k-1}[_{V_{j,h+1}^{2}}(s_{j,h},a_{j,h}),-V_{ j,h+1}^{2}(s_{j,h+1})]^{2}+\|\|_{2}^{2}\). The details are summarized in Lines 10-20 of Algorithm 1 and we provide the following guarantee.

**Theorem 2**.: _Set the parameters as in Lemma 8, with probability at least \(1-\), we have_

\[_{k=1}^{K}(V_{k,1}(s_{1}^{k})-V_{k,1}^{^{k}}(s_{1}^{ k}))=_{K,H}-_{k=1}^{K}_{h=1}^{H}_{k,h}(s_{k,h},a_{k,h}) }(K+d^{2}H^{3}K}).\]

The proof is given in Appendix C.6. Theorem 2 shows the model estimation error can be bounded. Combining Theorems 1, 2 and Lemma 5, we present the dynamic regret bound in the next section.

### Regret Guarantee: Upper and Lower Bounds

In this section, we provide the regret bound for our algorithm and present a lower bound of the dynamic regret for any algorithm for adversarial linear mixture MDPs with the unknown transition.

**Theorem 3**.: _Set \(=\{+ A)/K},1\}/H\), \(=1/(KH)\) and \(_{k}\) as in Lemma 8, then with probability at least \(1-\), it holds_

\[(K)}K+d^{2}H^{3}K }+(K+P_{T})(1+P_{T})},\] (12)

_where \(P_{T}=_{k=1}^{K}_{h=1}^{H}\|_{k,h}^{c}-_{k-1,h}^{c}\|_{1,}\) is the path length of the compared policies._

**Remark 4** (recovering static regret).: Since static regret is a special case with \(_{k}^{c}=^{*}, k\), our result can recover the optimal \(}(H^{3}K})\) static regret when \(H d\), same as the result in He et al. .

**Remark 5** (improving linear mixture case).: Our result improves upon the previously best-known \(}(dH^{7/4}K^{3/4}+H^{2}K^{2/3}{P_{T}}^{1/3})\) dynamic regret for adversarial linear mixture MDPs of Zhong et al. [36, Theorem 4.6] in terms of the dependence on \(H\), \(K\) and \(P_{T}\).

**Remark 6** (improving tabular case).: For the adversarial tabular MDPs, our result implies an \(}(S^{2}AK}+H^{2})(1+P_{T})})\) dynamic regret. This improves upon the best-known \(}(S^{2}AK}+H^{2}K^{2/3}{P_{T}}^{1/3})\) result of Fei et al. [35, Theorem 1]. The details are in Appendix B.

We finally establish the lower bound of this problem. The proof can be found in Appendix C.8.

**Theorem 4**.: _Suppose \(B 2,d 4,H 3,K(d-1)^{2}H/2\), for any algorithm and any constant \([0,2KH]\), there exists an episodic \(B\)-bounded adversarial linear mixture MDP and compared policies \(_{1}^{c},,_{K}^{c}\) such that \(P_{T},\) and \((K)(H^{3}K}+)\)._

When \(H d\), the upper bound is \(}(H^{3}K}+(K+P_{T})(1+P_{T})})\). Combining it with Theorem 4, we discuss the optimality of our result. We consider the following three regimes.

* Small \(P_{T}\): when \(0 P_{T} d^{2}/H\), the upper bound (12) can be simplified as \(}(H^{3}K})\), and the lower bound is \(}(H^{3}K})\), hence our result is optimal in terms of \(d\), \(H\) and \(K\).
* Moderate \(P_{T}\): when \(d^{2}/H P_{T} K\), the upper bound (12) can be simplified as \(}(H^{3}K}+K(1+P_{T})})\), and it is minimax optimal in \(d\), \(K\) and \(P_{T}\) but looses a factor of \(H^{}\).
* Large \(P_{T}\): when \(P_{T} K\), any algorithm suffers at most \((HK)\) dynamic regret, while the lower bound is \((K)\). So our result is minimax optimal in \(K\) but looses a factor of \(\).

## 5 Towards Optimal Dynamic Regret with Unknown \(P_{t}\)

This section further considers the case when the non-stationarity measure \(P_{T}\) is unknown. By Theorem 1, we need to tune the step size \(\) optimally to balance the number of episodes \(K\) and \(P_{T}\) to achieve a favorable dynamic regret. To address the difficulty of not knowing \(P_{T}\) ahead of time, we develop an online ensemble method to handle this uncertainty, in which a two-layer meta-base structure is maintained. While the methodology can be standard in recent non-stationary online learning , new challenges arise in online MDPs. We introduce the details below.

By the performance difference lemma in Cai et al. [32, Lemma 3.2] (as restate in Lemma 13), we can rewrite the dynamic regret as

\[_{k=1}^{K}[V_{k,1}^{_{k}^{c}}(s_{1}^{k})-V_{k,1}^{_ {k}}(s_{1}^{k})]=_{k=1}^{K}_{_{k}^{c}}[ _{h=1}^{H} Q_{k,h}^{_{k}}(s_{h},),_{k,h}^{c}(  s_{h})-_{k,h}( s_{h})],\]

where the expectation is taken over the randomness of the state trajectory sampled according to \(_{k}^{c}\). The dynamic regret of online MDPs can be written as the weighted average of some "multi-armed bandits" problems over all states, where the weight for each state is the _unknown_ and _changing_ probability of being visited by \(_{1}^{c},,_{K}^{c}\). As the optimal step size depends on the unknown non-stationarity measure \(P_{T}\) as shown in Section 4, a natural idea is to the two-layer structure to learn the optimal step size as in recent online convex optimization literature .

The general idea is constructing a step size pool \(=\{_{1},,_{N}\}\) to discretize the value range of the optimal step size; and then maintaining multiple base-learners \(_{1},,_{N}\), each of which works with a specific step size \(_{i}\). Finally, a meta-algorithm is used to track the best base-learner and yield the final policy. Then, the dynamic regret can be decomposed as follows (omit \(( s_{h})\) for simplicity):

\[_{k=1}^{K}_{_{k}^{c}}[_{h=1}^{H} Q_{k,h}^ {_{k}},_{k,h}^{c}-_{k,h}^{i}]+_{k=1}^{K} _{_{k}^{c}}[_{h=1}^{H} Q_{k,h}^{_{k}},_{k,h}^{i}-_{k,h}].\]

Since the above decomposition holds for any index \(i[N]\), we can always choose the base-learner with optimal step size to analyze and the first term is easy to control. The challenge is to control thesecond term, which is the regret of the meta-algorithm. Different from the standard "Prediction with Expert Advice" problem, it involves an additional expectation over the randomness of states sampled according to \(_{k}^{i}\). This poses a grand challenge compared to conventional online convex optimization where the expectation is not required. Although we can bound this term by \(P_{T}\) again, optimal tuning of the meta-algorithm is hindered as \(P_{T}\) is unknown. Consequently, we opt to upper bound it by the worst-case dynamic regret , that is, benchmarking the performance with the best choice of each round, which in turn introduces the dependence on the switching number of the best base-learner.

We introduce our approach as follows. We maintain multiple base-learners, each of which works with a specific step size \(_{i}\). All base-learners update their policies according to the same action-value function \(Q_{k-1,h}(s_{h},)\) of the combined policy \(_{k-1}\), that is, the base-learner \(_{i}\) updates policy by

\[_{k,h}^{i,}( s)_{k-1,h}^{i}( s)( _{i}Q_{k-1,h}(s,)),_{k,h}^{i}( s)=(1-)_{k,h}^{i, }( s)+^{u}( s),\] (13)

Then, the meta-algorithm chooses the base-learner by measuring the quality of each base-learner. In our approach, we choose the best base-learner at the last episode, that is,

\[_{k,h}( s)=_{k,h}^{i^{*}_{k-1,h}}( s)i^{*}_{k-1,h}(s)=_{i[N]} Q_{k-1,h}(s,),_{k-1,h}^{i}(  s).\] (14)

The details are summarized in Algorithm 2 of Appendix A and the guarantee is as follows.

**Theorem 5**.: _Set \(=1/(KH)\), step size pool \(=\{_{i}=(2^{i}/H) i[N]\}\) with \(N=()\). Algorithm 2 ensures_

\[(K)}K+d^{2}H^{3 }K}+(K+P_{T})(1+P_{T})+H^{2}S_{T}^{2}},\]

_where \(P_{T}=_{k=1}^{K}_{h=1}^{H}_{k,h}^{c}-_{k-1,h}^{c}_{1,}\) is the path length of the compared policies, \(S_{T}=_{k=1}^{K}_{h=1}^{H}_{_{k}^{i}}[i^{*}_{k,h}(s_{h}) i^{*}_{k-1,h}(s_{h})]\) is the expected switching number of best base-learner._

Combining it with Theorem 3, we discuss the optimality of our result. We consider two regimes.

* Small \(S_{T}\): when \(S_{T}\{d,H)(1+P_{T})}\}\), the term \(S_{T}\) can be subsumed by other terms. In this case, the upper bound in Theorem 5 is entirely the _same_ as that in Theorem 3. This implies we maintain the same guarantees without \(P_{T}\) as algorithmic input.
* Large \(S_{T}\): when \(S_{T}>\{d,H)(1+P_{T})}\}\), our result looses a factor of \(HS_{T}\) compared with the result in Theorem 3 for the known \(P_{T}\) setting.

By the above discussion, our result can be optimal in terms of \(K\) and \(P_{T}\) under certain regimes when \(P_{T}\) is unknown. In comparison, the regret bounds achieved via the restart mechanism [35; 36] remain sub-optimal across all regimes even \(P_{T}\) is known. Note that we introduce the notation \(S_{T}\) in the regret analysis, which also reflects the degree of environmental non-stationarity to some extent. Consider the following two examples: (i) in the stationary environment, \(S_{T}\) could be relatively small as the best base-learner would seldom change, and (ii) in the piecewise-stationary environment, \(S_{T}\) would align with the frequency of environmental changes. Indeed, given that \(S_{T}\) is a data-dependent quantity, its inclusion in the regret analysis is not ideal. Deriving bounds that rely exclusively on problem-dependent quantities, like \(P_{T}\), remains a significant open challenge.

## 6 Conclusion and Future Work

In this work, we study the dynamic regret of adversarial linear mixture MDPs with the unknown transition. For the case when \(P_{T}\) is known, we propose a novel policy optimization algorithm that incorporates a _fixed-share_ mechanism without the need for restarts. We show it enjoys a dynamic regret of \(}H^{3}K}+(K+P_{T})(1+P_{T})} \), strictly improving the previously best-known result of Zhong et al.  for the same setting and Fei et al.  when specialized to tabular MDPs. We also establish an \(H^{3}K}+)}\) lower bound, indicating that our algorithm is optimal regarding \(d\), \(K\) and \(P_{T}\) and can be optimal in terms of \(H\) under certain regimes. Moreover, we explore the more complex scenario where \(P_{T}\) is unknown. We show this setting presents unique challenges that distinguish online MDPs from conventional online convex optimization. We introduce a novel two-layer algorithm and show its dynamic regret guarantee is attractive under certain regimes.

There are several important future works to investigate. First, how to remove the dependence on the switching number \(S_{T}\) is an important open question. Moreover, we focus on the full-information feedback in this work, it remains an open problem to extend the results to the bandit feedback.