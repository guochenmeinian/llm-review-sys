# Overfitting Behaviour of Gaussian Kernel Ridgeless Regression: Varying Bandwidth or Dimensionality

Marko Medvedev

The University of Chicago

medvedev@uchicago.edu

&Gal Vardi

Weizmann Institute of Science

gal.vardi@weizmann.ac.il

&Nathan Srebro

TTI-Chicago

nati@ttic.edu

###### Abstract

We consider the overfitting behavior of minimum norm interpolating solutions of Gaussian kernel ridge regression (i.e. kernel ridgeless regression), when the bandwidth or input dimension varies with the sample size. For fixed dimensions, we show that even with varying or tuned bandwidth, the ridgeless solution is never consistent and, at least with large enough noise, always worse than the null predictor. For increasing dimension, we give a generic characterization of the overfitting behavior for any scaling of the dimension with sample size. We use this to provide the first example of benign overfitting using the Gaussian kernel with sub-polynomial scaling dimension. All our results are under the Gaussian universality ansatz and the (non-rigorous) risk predictions in terms of the kernel eigenstructure.

## 1 Introduction

A central question in learning theory is how learning algorithms can generalize well even when returning models that perfectly fit (i.e. interpolate) noisy training data. This phenomenon was observed empirically by Zhang et al. , and does not align with the traditional belief from statistical learning theory that overfitting to noise leads to poor generalization. Consequently, it attracted significant interest in recent years, and there has been much effort to understand the overfitting behavior of linear models, kernel methods, and neural networks.

In this paper, we study the overfitting behavior of Kernel Ridge Regression (KRR) with Gaussian kernel, namely, the behavior of the limiting test error when training on noisy data as the number of samples tends to infinity by insisting on interpolation (achieving zero training error). When the input dimension and bandwidth are fixed, the overfitting behavior is known to be "catastrophic" , i.e. for any nonzero noise, the test risk tends to infinity as the sample size increases. However, this is not how Gaussian Kernel Ridge Regression is typically used in practice. In fixed dimension, the bandwidth is tuned, that is decreased, when the sample size increases . Additionally, it makes sense to study the behaviour when the input dimension increases with sample size (as in, e.g. linear models with proportional scaling ). This could be because when more data is available, more input features are used, even with a kernel; because as more resources are available we scale up both the input dimension and amount of data used; or to capture the fact that very large scale problems typically involve both more samples and higher input dimension. But unlike with linear models, where the dimension must scale linearly with the number of samples in order to allow for interpolation, when a kernel is used we can study the behaviour even when the input dimensionality increases much slower, and ask how slowly it could increase without catastrophic overfitting. Previous studies on kernel ridgeless regression considered polynomial increasing dimension (i.e. dimension \(\) sample-size\({}^{a}\), for \(0<a 1\)) , but not subpolynomial scaling.

We aim to provide a more comprehensive picture of overfitting with Gaussian KRR by studying the overfitting behavior with varying bandwidth or with arbitrarily varying dimension, includingsub-polynomially. In particular, we show that for fixed dimension, even with varying bandwidth, the interpolation learning is never consistent and generally not better than the null predictor (either the test error tends to infinity or is finite but it is almost always not better than the null predictor). For increasing dimension, we give an upper and lower bound on the test risk for any scaling of the dimension with sample size, which indicates in many cases whether the overfitting is catastrophic (test error tends to infinity), tempered (test error tends to a constant), or benign (consistent). Our result agrees with the polynomial scaling of the dimension with sample size, showing tempered overfitting for an exponent that is a reciprocal of an integer and benign overfitting for any other exponent . Moreover, our result goes further, as we show the first example of sub-polynomially scaling dimension that achieves benign overfitting for the Gaussian kernel. Additionally, we show that a class of dot-product kernels on the sphere is inconsistent when the dimension scales logarithmically with sample size. All our results are under the Gaussian universality ansatz and the non-rigorous but well-established risk predictions in terms of the kernel eigenstructure .

#### Related work

The test performance of overfitting models has been extensively studied for linear regression , linear classification , neural networks , and kernel methods. Below we focus on overfitting in kernel ridge regression.

Overfitting in fixed dimension with fixed kernel.In Mallinar et al. , the authors show that the minimum norm interpolating solution for Gaussian kernel with fixed bandwidth overfits catastrophically. In , the authors derive bounds on the test risk of minimum norm interpolant with a fixed kernel under various assumptions. Our result for fixed dimension will only apply to the Gaussian kernel, but it allows for any varying or adaptively chosen bandwidth.

Inconsistency of Kernel Ridge Regression.The case of varying bandwidth has been considered in Beaglehole et al. , Rakhlin and Zhai , Haas et al. . In Beaglehole et al. , the authors show that there exists a specific data distribution for which the minimum norm interpolating solution for a particular set of translation invariant kernels is not consistent. In Rakhlin and Zhai , the authors show that for input distributions on the unit ball, the Laplace kernel is inconsistent, even with varying bandwidth. In Haas et al. , the authors show that under different assumptions on the data distribution, for a general class of (potentially varying) kernels in fixed dimension, any differentiable function that overfits the data and is not much different from the minimum norm interpolant is inconsistent. All of these works only consider whether we can achieve consistency. None of these results apply in the case of data distributions that we are considering, and even if the predictor is not consistent, we ask how bad is it by comparing it to the null predictor and whether it might be tempered.

Overfitting in increasing dimensions.Many papers have studied the setup where the dimension increases with sample size , in particular when the dimension is a function of sample size (or vice versa), but they all consider only the case of a polynomial scaling of dimension and sample size. It was shown that in this case the minimum norm interpolating solution of dot product kernels on the sphere can be benign, depending on if the exponent is not an integer. We generalize these results to any scaling of the dimension and sample size. Our results recover the existing results in the case of polynomially scaling dimension and show benign overfitting in a certain sub-polynomial scaling. Having sub-polynomimal scaling of the dimension allows us to expand the set of possible target functions from only polynomials of a bounded degree, as in the case of polynomial scaling of dimension , to, in our case of sub-polynomially increasing dimension, polynomials of any degree and even non-polynomial functions.

## 2 Problem formulation and assumptions

Kernel ridge(less) regression and the Gaussian kernel.Let \(\) be an unknown distribution over \(^{d}\) and let \(\{(x_{i},y_{i})\}_{i=1}^{m}^{m}\) be a dataset consisting of \(m\) samples. For simplicity, we will assume that the distribution of the target is given by a target function \(f^{*}\) of the input \(x\)with zero mean independent noise \(\) with variance \(^{2}\), that is \(y f^{*}(x)+\). We note that our results can be extended to a distribution agnostic setting, as analyzed by Zhou et al. .

Let \(K:\) be a positive semi-definite kernel function. Let \(\|f\|_{K}\) be the norm of \(f\) in the RKHS \(_{K}\) corresponding to \(K\). For a predictor \(f\), let \(R(f)\) and \((f)\) be the test and training risk of \(f\),

\[(f)=_{i=1}^{m}(f(x_{i})-y_{i})^{2}\;\;\;\;R(f)= _{}[(f(x)-y)^{2}].\]

Two important risks to consider are the risk of the null predictor, \(f 0\), which we will denote by \(R(0)=_{}[(y)^{2}]\), and the Bayes (or irreducible) risk, which we will denote \(^{2}\) or \(R(f^{*})\). Using this notation, the risk of the null predictor is \(R(0)=^{2}+_{}[(y)^{2}]\). Bayes risk represents the minimum possible risk that can be achieved by any predictor. For a regularization parameter \(\), the regularized ridge solution \(_{}\) is given by

\[_{}=_{f_{K}}(f)+\|f\|_{K}^{2}.\]

We are interested in the minimum norm interpolating (ridgeless) solution \(_{0}=_{ 0+}_{}\), namely

\[_{0}=_{(f)=0;f_{K}}\|f\|_{K}^{2}.\]

We will focus on the Gaussian kernel, which is given by \(K(x_{1},x_{2})=(--x_{2}\|_{2}^{2}}{_{m}^{2}})\), where \(m\) is the sample size and \(_{m}\) is a predetermined bandwidth parameter that can vary with sample size. The Gaussian kernel is widely used and achieves good error rates for a variety of learning tasks . Gaussian KRR achieves optimal convergence to the best possible (Bayes) error for learning any function in a Besov space of high enough order (essentially bounded and twice differentiable in the weak sense) under very mild assumptions on the distribution of the input and target \(\). For ridge regression with the Gaussian kernel and under a standard data distribution assumption, the minimum distance between samples decreases with sample size so it makes sense to also decrease \(_{m}\). Additionally, decreasing \(_{m}\) with sample size helps to achieve good convergence rates theoretically .

Main question.We will consider the problem of learning using the minimum norm interpolating solution \(_{0}\) of KRR. We want to understand the limiting behavior of test risk \(R(_{0})\) as the sample size increases \(m\), that is \(_{m}R(_{0})\). It suffices to understand \(_{m}R(_{0})\) and \(_{m}R(_{0})\) and this way we do not assume the existence of the limit. In this work, we use the taxonomy of benign, tempered, and catastrophic overfitting from Mallinar et al. , which indicates whether \(_{m}R(_{0})\) is the Bayes (optimal) error, a non-optimal but constant error, or infinity. Note that in this taxonomy, the null predictor can be classified as tempered. Therefore, we will compare the limiting risk to the risk of the null predictor \(R(0)\) in order to understand whether the performance of the interpolating solution is non-trivial.

Main tool: Eigenframework and a closed form of the test risk.Our main tool will be the closed form of the test risk predicted by the _eigenframework_. Under the eigenframework, we can write down the closed form of the test risk using Mercer's theorem decomposition of a kernel function \(K\).

Given a positive semi-definite kernel function \(K:\), we can decompose it as

\[K(x_{1},x_{2})=_{k=1}^{}_{k}_{k}(x_{1})_{k}(x_{2}),\] (1)

where \(_{k}\) and \(_{k}\) are the eigenvalues and eigenfunctions of the integral operator associated to \(K\). The eigenfunctions \(\{_{k}\}\) are an orthonormal basis of \(L^{2}_{_{}}()\), where \(_{}\) is the marginal distribution of \(\). We denote the Bayes optimal target function by \(f^{*}\), and expand it in the kernel basis \(\{_{i}\}_{i=1}^{}\) as \(f^{*}(x)=_{i=1}^{}_{i}_{i}(x)\). To state the close form of the test risk we will introduce a few quantities. Let _effective regularization_, \(_{}\), be the solution to \(_{i=1}^{}}{_{i}+_{}}+}=m\). Furthermore, let and \(_{}=^{n}_{i,}^{2}}\). Then, the _predicted risk_, i.e. the predicted closed form of the test risk of \(_{}\), is given by

\[(_{})=_{}(_{i=1}^{}( 1-_{i,})^{2}_{i}^{2}+^{2}),\] (2)

where \(^{2}\) is the Bayes error of \(\). Equation (2) was initially heuristically derived using the replica method or continuous approximations to the learning curves inspired by the Gaussian process literature . In , it is derived using a conservation law. Note that  shows that the predicted closed form of the test risk from  extends to general target distributions. There is strong evidence that the predicted risk is a good estimate of the true test risk, namely \(R(_{})(_{})\). Indeed, a number of works use the predicted risk closed form to estimate the test risk of KRR [58; 8; 66]. The following assumption on Gaussian design ansatz is used by all of these works.

**Assumption 1** (Gaussian design ansatz, cf. Zhou et al. ).: When sampling \((x,)\), we have that the Gaussian universality holds for the eigenfunctions in the sense that the expected risk is unchanged if we replace \(\) with \(\), where \(\) is Gaussian with appropriate parameters, i.e. \((0,\{_{i}\})\).

This assumption appears to hold for real datasets as well, namely, the predictions computed for Gaussian design agree well with the experiments on kernel regression using real data [8; 50; 57]. As discussed in Zhou et al. , under this assumption, the equivalence \(R(_{})(_{})\) holds in a few ways. First, in an appropriate asymptotic limit in which the sample size \(m\) and the number of eigenmodes in a given eigenvalue grow proportionally, the equivalence holds [27; 1]. Second, if the eigenstructure of the task is fixed, the error between the two can be bounded by a decaying function of \(m\). Finally, various numerical experiments show that the error between the two is small even for a small sample size \(m\)[8; 50]. Specifically, Canatar et al.  (see Figure 5 there) gives empirical evidence that the predicted risk closely approximates the true risk for the Gaussian kernel with data uniform on a sphere, which is the setting that we consider in some of our results.

There has been some recent progress in bounding the error between \(R(_{})\) and \((_{})\) unconditionally. Misiakiewicz and Saeed  shows that the error will tend to zero if the dimension \(d\) grows fast enough with the sample size. Additionally, they provide strong empirical evidence that the predicted risk is close to the test risk for a real dataset (MNIST) and Gaussian kernel, see Figure 1 in .

Formally, we will prove results about the predicted risk \((_{})\), but as previously presented evidence suggests, treating \(R(_{})(_{})\) as equivalence is sufficient for understanding the behavior of KRR.

We note that using the eigenframework might introduce restrictions for which kernels some of our results apply, as concurrent work showed that the eigenframework prediction might not hold for the NTK in fixed dimension [4; 15]. Our two main results concern the Gaussian kernel, for which we described ample empirical evidence that the eigenlearning predictions hold [8; 41]. Understanding the limitations of the eigenframework is an important future research direction.

## 3 Fixed dimension: Gaussian kernel with varying bandwidth

We will assume that the source distribution is uniform on a \(d\) dimensional sphere, that the target function is square integrable, and that the target distribution is given by the target function with an independent noise.

**Assumption 2** (Target function and data distribution).: Let \(\) be the distribution over \(=^{d-1}\), such that the \(\) marginal, denoted by \(_{}\), is \((^{d-1})\). We will assume that for a target function \(f^{*} L^{2}_{_{}}(^{d-1})\), the marginal \(\) distribution is given by \(y f^{*}(x)+\), where \(\) has mean zero and variance \(^{2}>0\). We write \(f^{*}=_{i}_{i}_{i}\), where \(\{_{i}\}\) is the \(L^{2}_{_{}}(^{d-1})\) eigenbasis corresponding to the kernel \(K\) (Equation (1)). If we write \(=(_{1},_{2},)\), then we have that \(\|\|_{2}^{2}=_{_{}}((f^{*}(x))^{2})\). We will use the notation \(\|f^{*}\|^{2}=\|\|_{2}^{2}\).

The assumption on the distribution on \(\) is common in the literature on KRR [38; 4; 64; 22]. The assumption that \(x(^{d-1})\) can be relaxed to a more general setting where \(x\) is uniformly distributed on other manifolds that are diffeomorphic to the sphere using the results of Li et al. , although that will not be the focus of this paper.

Note that here we vary the bandwidth \(_{m}\) so both \(_{0}\) and \(R(_{0})\) will depend on the bandwidth \(_{m}\) as well as \(m\). We will identify three different regimes of bandwidth scaling. We will show that the minimum norm interpolating solution exhibits either tempered or catastrophic overfitting, and we will argue that it is almost always worse than the risk of the null predictor.

**Theorem 3** (Overfitting behavior of Gaussian kernel in fixed dimension).: Under Assumption 2, the following bounds hold for the predicted risk \((_{0})\) of the minimum norm interpolating solution of Gaussian KRR:

1. If \(_{m}=o(m^{-})\), then \((0)}_{}( _{0})}_{}( _{0})}<\). More precisely, if \(_{m} m^{-}t(m)\), where \(t(m) 0\) as \(m\), then there is a scalar \(c_{d}\) that depends only on the dimension and \(m_{0}\) that depends on \(t(m)\) such that for all \(m>m_{0}\) we have \((_{0})>^{2}+(1-c_{d}t(m)^{})\|f^{*}\|^{2}\).
2. If \(_{m}=(m^{-})\), then \(_{}(_{0})}=\). Hence, for large enough \(m\) we have \((_{0})>(0)\).
3. If \(_{m}=(m^{-})\), then \(_{}R(_{0})<\). Moreover, Suppose that \(C_{1}m^{-}_{m} C_{2}m^{-}\) for some constants \(C_{1}\) and \(C_{2}\), then there exist \(,>0\) that depend only on \(d\), \(C_{1}\), and \(C_{2}\), such that for all \(m\) we have \((_{0})}>\|f^{*}\|^{2}+(1+)}\). Consequently, \((_{0})}>(0)}\) as long as \(}>}{}\|f^{*}\|^{2}\).

Theorem 3 shows that the minimum norm interpolating solution of Gaussian KRR cannot be consistent when data is distributed uniformly on the sphere, even with varying or adaptively chosen bandwidth. Additionally, in the first two modes of bandwidth change, the minimum norm interpolating solution is never better than the null predictor. In the third case, the interpolating solution is worse than null for noise that is not too small. This shows that even though the minimum norm interpolating predictor is classified as tempered in the first and third cases of scaling of the bandwidth, it is still worse than the trivial null predictor. Note that our analysis does not exclude the possibility that for \(_{m}=(m^{})\) there exists small enough \(^{2}\) for which the interpolating solution is better than the null predictor. We leave this as an open question. In Appendix A, we provide further empirical justification for Theorem 3.

## 4 Increasing dimension

For the case of increasing dimension, we consider the problem of learning a sequence of distributions \(^{(d)}\) over \(=^{d}\) given by \(y f_{d}^{*}(x)+_{d}\) using a sequence of kernels \(K^{(d)}\). Here, \(_{d}\) is independent noise with mean \(0\) and variance \(^{2}>0\). Formally, the kernel and the target function can change with the dimension \(d\), but we will think of it as the same kernel and target with higher dimensional input. Furthermore, \(d\) will increase with sample size \(m\), i.e. \(d=d(m)\) (or analogously \(m\) will increase with \(d\)). A common assumption, which we also adopt, is that the projections of the target function \(f_{d}^{*}\) onto the eigenfunctions \(_{k}^{(d)}\) of the kernels \(K^{(d)}\) are uniformly bounded .

**Assumption 4** (Target function and distribution in increasing dimension).: Consider learning a sequence of target functions \(f_{d}^{*}\) with a sequence of kernels \(K^{(d)}\). Let the target function \(f_{d}^{*}\) have only \(S_{d}\) nonzero coefficients (where \(S_{d}\) can change with \(d\)), so \(f_{d}^{*}=_{i=1}^{S_{d}}_{i}^{(d)}_{i}^{(d)}\) where \(_{i}^{(d)}\) are from Equation (1) and \(|_{i}^{(d)}| B\), i.e. \(\|\|_{} B\), for \(B\) that is independent of \(d\).

The functions that can be represented in this form depend on the number of nonzero coefficients, \(S_{d}\), and the kernel that we are using. In particular, for dot-product kernels on the sphere it includes all polynomials of degree \(k k_{d}\) where \(k_{d}\) is such that the multiplicities of first \(k_{d}\) eigenfunctions are at most \(S_{d}\). If \(S_{d}\) grows with \(d\), then this set will include much more general functions. See Remark 14 for further discussion.

First, we will consider a general kernel \(K\) since Theorem 7 and Theorem 9 will apply more generally. Then, we will apply these results to the cases of dot-product and Gaussian kernels.

The multiplicities of eigenvalues will play an important role in bounding the test risk, both from above and below, so we will introduce the following related notation.

**Definition 5** (Lower and upper index).: Let \(_{k}\) be the \(k\)-th non-repeating eigenvalue of a kernel \(K\) and let \(N(k)\) be its multiplicity. Let \(N_{k}=N(1)++N(k)\). Let \(m\) be the sample size. Let \(k_{m}\) be defined as the maximal \(k\) such that there is less than \(m\) eigenvalues with index \(k\), i.e. \(k_{m}=\{k|N(1)++N(k)<m\}\). Define the lower index \(L_{m}\) and the upper index \(U_{m}\) as follows \(L_{m}=N(1)++N(k_{m})\) and \(U_{m}=N(1)++N(k_{m}+1)\). When the dimension changes with sample size, we sometimes denote \(N(k)\) by \(N(d,k)\).

We will first state a generic bound on the test risk for any data distribution, kernel with a bounded sum of eigenvalues, sample size, and dimension. This bound will be informative when we scale the dimension \(d\) with sample size \(m\), but it holds for any kernel that satisfies the following assumption.

**Assumption 6** (Bounded sum of eigenvalues).: Assume that the kernel \(K\) has a bounded sum of eigenvalues, i.e. there is a constant \(A\) such that \(_{i=1}^{}N(i)_{i} A\). For a sequence of kernels \(K^{(d)}\), assume that all such \(A^{(d)}\) are bounded by some constant \(A\).1

This is a reasonable assumption for most dot-product kernels, as we show in Appendix C.4. It also implicitly sets the scale of the kernel.

**Theorem 7** (Test risk upper bound for kernel ridgeless regression).: Let \(d\) and \(m\) be any dimension and sample size. Define \(L_{m}\), \(U_{m}\), \(k_{m}\), \(N(i)\), \(N_{l}\), and \(_{k}\) as in Definition 5. Consider KRR with a kernel \(K\) satisfying Assumption 6 for some \(A\). Assume that for some integer \(l\), the target function \(f^{*}\) satisfies Assumption 4 with at most \(N_{l}\) nonzero coefficients. Then, the predicted risk of the minimum norm interpolating solution is bounded by the following:

\[(_{0}) (1-}{m})^{-1}(1-} )^{-1}^{2}\] (3) \[+B^{2}(1-}{m})^{-1}(1-})^{-1}}{m^{2}}(_{i=1}^{l}N(i)_{i}^{2}})\.\] (4)

Alternatively, we can bound the risk using \((_{i=1}^{l}N(i)_{i}})\) instead of \((_{i=1}^{l}N(i)_{i}^{2}})\)(see Theorem 20 in the appendix).

We also establish a generic inconsistency result for any data distribution, kernel with a bounded sum of eigenvalues, sample size, and dimension based on the upper and lower indices from Definition 5. We will further need to assume that the eigenvalues are bounded away from zero. Similarly, this will be useful when scaling dimension \(d\) with sample size, but it holds generally.

**Assumption 8** (Lower bound on eigenvalues).: Assume that the kernel \(K\) has eigenvalues that are not too small, i.e. there is a constant \(b\) such that \(_{i k_{m}}(_{i}})<}{b}\). For a sequence of kernels \(K^{(d)}\), assume that for the corresponding \(m=m(d)\) (since \(d=d(m)\), we can also "invert" the dependence) all such \(b^{(d)}\) are bounded below by some \(b\).

This assumption will hold for most dot-product kernels and we will show it for Gaussian kernel in Appendix C.4.

**Theorem 9** (Test risk lower bound for any kernel ridgeless regression).: Let \(k_{m}\) and \(L_{m}\) be as in Definition 5. Consider learning a target function \(f^{*}\), with some sample size \(m\). Let \(K\) be a kernel satisfying Assumption 6 and Assumption 8 for some \(A\) and \(b\). Consider the minimum norm interpolating solution of KRR (with any data distribution) with kernel \(K\). Then, for the predicted risk of minimum norm interpolating solution, the following lower bound holds:

\[(_{0})>(1-()^{2}}{m} )^{-1}^{2}.\]

To apply Theorem 7 for varying dimension \(d\), we would additionally require that \(A\) is uniformly bounded for all \(d\) and kernel \(K^{(d)}\) and also that \(l=l(d)\) changes with \(d\) such that Assumption 4 holds with \(S_{d}=N_{l(d)}\). For dot-product kernels \(K^{(d)}\) on the sphere, if we let \(K^{(d)}(x,y)=h^{(d)}(\|x-y\|)\)we will have \(A=_{d}h^{(d)}(0)\), so if \(h^{(d)}\) does not change with \(d\) we can take \(A=h(0)\) (see Appendix C.4 for more details). Specifically, this holds for the Gaussian kernel on the sphere with \(A=1\).

To apply Theorem 9 to the case of increasing dimension, we would require that the bounds \(_{i}N(d,i)_{i} A\) and \(_{i k_{m}}(})<}{b}\) hold for all \(d\) and kernels \(K^{(d)}\). Usually, the condition \(_{i k_{m}}(})<}{b}\) will be satisfied for \(b=1\). We will show it for the two cases of sub-polynomially scaling dimensions with a Gaussian kernel. For the polynomial scaling dimension, it is reasonable to assume it for general dot-product kernels, as discussed in Appendix C.4.

Now, we will show that using Theorem 7 and Theorem 9 we can recover the behavior of the minimum norm interpolating solution for polynomially increasing dimension , i.e. tempered overfitting for integer exponent and benign for non-integer exponent. Here, we will need to additionally assume that the eigenvalue decay is not too fast.

**Assumption 10** (Eigenvalue decay).: The eigenvalues do not decrease too quickly, i.e. for \(k_{m}\) as in Definition 5, we have that there is a constant \(c\) such that \(_{i k_{m}}(}) cN(k_{m})\). For increasing dimension, we require that \(_{i k_{m}}(}) cN(d,k_{m})\) for all \(m\) (i.e. all \(d\), as \(d\) and \(m\) both increase).

This assumption is stronger than Assumption 8, but as we show in Appendix C.4, it is reasonable for dot-product kernels on the sphere and even the NTK.

**Corollary 11** (Dot-product kernels with polynomially increasing dimension, recovering the results of ).: Consider the problem of learning a sequence of target functions \(f_{d}^{*}\) satisfying Assumption 4 with \(S_{d}(d^{})\) with a dot-product kernel \(K(x,y)=h(\|x-y\|)\) with \(h(0)=1\) on the sphere \(^{d-1}\) (where \(h\) does not depend on \(d\), i.e. \(A=1\) from Assumption 6)) that further satisfies Assumption 10. Let \(}{m}=(1)\) for \((0,)\). Then the overfitting behavior of the minimum norm interpolating solution is benign if \(\) is not an integer and tempered if \(\) is an integer.

Additionally, we will show that for \(d= m\), we cannot get benign overfitting, i.e. consistency with a class of dot-product kernels on the sphere. Similarly, as in the previous corollary, this will hold for any sequence where \(d= m\) even only asymptotically.

**Corollary 12** (Inconsistency with dot-product kernels in logarithmically scaling dimension).: Let \(K^{(d)}\) be a sequence of dot-product kernels on \(^{d-1}\) that satisfy Assumption 8. Let the dimension \(d\) grows with sample size as \(d=_{2}m\) (i.e. \(m=2^{d}\)). Then, the minimum norm interpolant cannot exhibit benign overfitting for any such sequence \(K^{(d)}\), i.e. there exists an absolute constant \(>0\) such that for all \(d,m\), \((_{0})>(1+)^{2}\).

On the other hand, using Theorem 7, we will establish the first case of sub-polynomial scaling dimension with benign overfitting using the Gaussian kernel and data on the sphere. We will use \(d=()\).

**Corollary 13** (Benign overfitting with Gaussian kernel and sub-polynomial dimension).: Let \(K\) be the Gaussian kernel on the sphere \(^{d-1}\) with a fixed bandwidth, and take a sequence of dimensions \(d\) and sample sizes \(m\) that scale as \(d=()\) (in particular, we take \(l\) such that \(d=2^{2^{l}}\) and \(m=2^{2^{2l}}\) with \(l=1,2,3\)). Consider learning a sequence of target functions \(f_{d}^{*}\) as in Assumption 4 with \(S_{d} m^{}\). Then, we have that the minimum norm interpolating solution achieves the Bayes error in the limit \((m,d)\). In particular, for \(d 4\) and \(m 16\) we have

\[(_{0})(1-)^{-1}(1- (-0.89))^{-1}^{2}+2B^{2}.\]

**Remark 14** (Allowed target functions).: The set of allowed target functions \(f_{d}^{*}\) in Corollary 13, i.e. with sub-polynomial scaling dimension, is strictly larger than the set of allowed target functions for polynomially scaling dimension, as in Corollary 11 and . In particular, for polynomially scaling dimension \(}{m}=(1)\), the result holds only if the target function is a polynomial of degree at most \(\). On the other hand, Corollary 13 shows that sub-polynomially scaling dimension allows for the target function to be a polynomial of arbitrary degree, as well as non-polynomial functions. In particular, in dimension \(d\), we can represent polynomials of degree up to \((^{2}d)\).

Proofs outline

In this section, we discuss the main proof ideas. All formal proofs are provided in the appendix.

By Equation (2), to understand how the test risk of the minimum norm interpolating solution behaves, it suffices to understand how the eigenvalues corresponding to the kernel \(K\) and thus the quantities \(_{0}\), \(_{i,0}\) behave. In Zhou et al. , the authors show that \(_{0}\) is bounded both above and below in terms of the effective rank of the systems of eigenvalue \(\{\}_{i=1}^{}\), defined by \(r_{k}:=^{}_{i}}{_{k+1}}\). We will use this, along with directly bounding \(_{i,0}\).

If \(K\) is a dot-product kernel on the sphere (such as the Gaussian kernel), we can take the eigenfunctions \(_{i}\) to be the spherical harmonics \(Y_{ks}\), where \(k 0\) and \(s[1,N(d,k)]\). Here \(N(d,k)=\) is the multiplicity of the \(k\)-th spherical harmonic. All \(Y_{ks}\) for the same \(k\) will have the same eigenvalue, which we will denote \(_{k}\). In this case, we can write a closed-form expression for the eigenvalues of Gaussian kernel, \(_{k}\), in terms of the bandwidth \(_{m}\) and modified Bessel functions of the first kind \(I_{v}(x)\) (see Appendix D). Using the closed form of \(_{i}\) and the multiplicities of eigenvalues, we can understand how the test risk of the minimum norm interpolating solution \(R(_{0})\) behaves as \(m\), which tells us its overfitting behavior.

Additionally, \(_{0}\) appearing in Equation (2), will be informative. For example, if \(_{m}_{0}>1\) then the overfitting cannot be benign. The following bound using the effective rank \(r_{k}\) holds : For \(k<m\) such that \(r_{k}+k>m\) we have

\[_{0}(1-)^{-1}(1-})^{-1}.\] (5)

For \(k m\) it holds that

\[_{0}(})}.\] (6)

Proof sketch of Theorem 3.We will focus on the lower bounds in this case, as the result is negative. The key elements to understanding the effective rank \(r_{k}\) and the test risk Equation (2) is to understand how the ratios of eigenvalues \(_{k+1}}{_{k}}\) and the multiplicities \(N(d,k)\) behave. Using the closed form of the eigenvalues of Gaussian kernel and the properties of modified Bessel functions [46; 48], with some computation (see Theorem 28 in the appendix for the computations), we can derive the following bounds on ratios of eigenvalues

\[^{2}})}{2(k+ )+(^{2}})}<_{k+1}}{ _{k}}<^{2}})}{(k+ {d}{2}-)+(^{2}})}\,.\] (7)

From these bounds, we can derive tight bounds on \(_{k+j}}{_{k}}\) for indices \(k\) and \(j\) using simple but long calculations (see Theorem 28 in the appendix). If \(k=o(})\) and \(j=o(})\), then \(_{k+j}}{_{k}} 1\). If \(k(})\) and \(j=(})\), then \(_{k+j}}{_{k}}=(1)\). If \(k=(})\), then \(_{k+j}}{_{k}}=o(})\) for any integer \(n\) (i.e. it deceases super-polynomial). For \(N(d,l)\) it holds that \(N(d,l)=(l^{d-2})\) and \(N_{l}:=N(d,1)++N(d,l)=(l^{d-1})\). Therefore if \(\) is the index such that \(_{}=_{l}\), we have that \(=(l^{})\).

For \(_{m}(m^{-})\), we will take \(l=(1+})m\) and show that \(r_{l}=o(m)\). Then, the bound in Equation (6) will imply that \(_{0} 1+\), so from Equation (2), \((_{0})>_{0}^{2}=^{2}\). Note that for \(l=(1+})m\), we have that \(=(m^{})=(})\). Note that for \(r_{l-1}\), we have that

\[r_{l-1}=_{i=0}^{}}{_{l}}<N(d, )+_{i=1}^{}N(d,+i)_{ +i}}{_{}}<(m^{})(1+(1) ),\]since \(N(d,+i)<N(d,j)i^{d-2}\) and \(_{+i}}{_{}}<}\). So indeed \(r_{l-1}=o(m)\) which implies \(r_{l}=o(m)\).

For \(_{m}=o(m^{-})\) and \(_{m}=(m^{-})\), we will directly analyze Equation (2). For \(k=(})\), we have that \(_{}}{_{1}}>\) for all \(i k\). Let \(_{i}=_{i}}{_{i}+_{0}}\). Note that \(_{i}>_{1}\) for \(i k\). Note that

\[m=_{i}N(d,i)_{i}>_{1}(N(d,1)++ N(d,k))>((})^{d-1}) _{1}.\]

So, we have that for all \(i\) that \(_{i}<_{1}<} )^{d-1})}\). From Equation (2), we have that

\[(_{0})=_{0}_{i}N(d,i)(1-_{i})^{2} _{i}^{2}+_{0}^{2}>_{0}(1-})^{d-1})})^{2}\|f^{*}\| ^{2}+_{0}^{2}.\]

For \(_{m}=o(m^{})\) this is sufficient. Additionally, by a similar computation as above, in this case, \(r_{1}=(m)\), so \(_{0}\) is bounded by Equation (5). For \(_{m}=(m^{})\), using Equation (5) and Equation (6), by showing that for \(l=(m)\), \(r_{l}=(m)\), we have \((1)>_{0}>1+(1)\).

Proof sketch of Theorem 7.Let \(_{i}=_{i}}{_{i}+_{0}}\). Note that \((1-_{i})^{2}=^{2}}{(_{0}+_{i} )^{2}}\). Then, Equation (2) can be rewritten and bounded as (with an abuse of notation for \(_{i}\))

\[(_{0})=_{0}_{i}N(d,i)(1-_{i})^{2} _{i}^{2}+_{0}^{2}_{0}B^{2}_{i=1}^{l} N(d,i)^{2}}{(_{0}+_{i})^{2}}+_{0} ^{2}.\]

Note that \(^{2}}{(_{0}+_{i})^{2}}<^ {2}}{_{i}^{2}}\) and also \(_{i}_{i}}{_{0}}\). Therefore, we have that \((_{0})_{0}B^{2}_{0}^{2}(_{i=1}^{l }N(d,i)_{i}^{2}})+_{0}^{2}\). Note that \(m=_{i}N(d,i)_{i}<_{i}N(d,i)_{i}}{ _{0}}<}\), so \(_{0}<\). Finally, to bound \(_{0}\), note that in Equation (5) we can choose \(k=L_{m}<m\), then \(r_{k}+k>U_{m}>m\), so \(_{0}(1-}{m})^{-1}(1-} )^{-1}\). Combining these with \(_{0}<\) gives the claim of Theorem 7. The proof of the alternative bound is harder and will be delayed to the appendix.

Proof sketch of Theorem 9.By Theorem \(3\) from Zhou et al. , if \(k\) is the first \(k<m\) such that \(k+br_{k} m\), then \(_{0}(1-()^{2})^{-1}\). Since \(_{i k_{m}}(_{i}})<}{b}\), for \(l<N(d,1)++N(d,k_{m})\), we have that \(br_{l}+l N(d,1)++N(d,k_{m})-1+b_{i k_{m}}(_{i}})<L_{m}+m-L_{m} m\), so the first \(l\) for which \(r_{l}+l>m\) is \(l=L_{m}=N(d,1)++N(d,k_{m})\). Plugging in \(k=L_{m}\) we get that \(_{0}(1-()}{m})^{-1}\), so from Equation (6), we have \((_{0})(1-()^{2}}{m} )^{-1}^{2}\).

Proof sketch of Corollary 11.Note that an analogous proof holds for any \(}{m} c\), for a constant \(c\), but we take equality for simplicity. If \(k\) is a constant, i.e. it does not change with the dimension, we have that \(N(d,k)=(d^{k})\). Therefore, \(k_{m}=\) if \(\) is a non-integer and \(-1\) if \(\) is an integer. If \(\) is a non-integer, then \(L_{m}=N(d,k_{m})++N(d,1)=(d^{})\) and \(U_{m}=N(d,k_{m}+1)++N(d,1)=(d^{+1})\). So we have that \(}{m}=d^{-}\), \(}=d^{--1}\), \(N(d,)=d^{}\). Note that \(k_{m}=\). We have from Assumption 10 that \(_{i k_{m}}_{i}}=O(N(d,k_{m}))\). Note that Theorem 7 holds with \((_{i=1}^{l}N(d,i)_{i}})\) instead of \((_{i=1}^{l}N(d,i)_{i}^{2}})\). Therefore, we have \(}_{l k_{m}}(}N(d,i)) O(d^{ (2-2)})\). This gives

\[(_{0}) (1-d^{-)^{-1}(1-d^ {--1})^{-1}^{2}\] \[+O(B^{2}(1-d^{-})^{-1} (1-d^{--1})^{-1}d^{(2 -2)}).\]

Therefore, \(_{m}(_{0})=^{2}\). So, if \(\) is not an integer, we get benign overfitting. If \(\) is an integer, note that \(N(d,1)++N(d,-1)=o(d^{})\), so \(k_{m}=\). Then there are \(c_{u},c_{l}(0,1)\) such that \(c_{l}<}{m} c_{u}<1\). So, Theorem 9 holds for \(b=\), so \((_{0})>}{9}}^{2}\). This shows that \(_{m}(_{0})}{9}} ^{2}>^{2}\). The upper bound follows as above. Since we assumed that \(_{i k_{m}}}=O(N(d,k_{m}))\), and \(_{i k_{m}}N(d,i)=N(d,k_{m})=(d^{})\), we have that \(_{i=1}^{k_{m}}N(d,i)}=(d^{2})\), so

\[(_{0})(1-c_{l})^{-1}(1-d^{-1})^{-1} ^{2}+(B^{2}(1-d^{-1})^{-1}).\]

So, we conclude that for integer \(\) we have tempered overfitting and for non-integer \(\) we have benign overfitting. This recovers results of Ghorbani et al. , Mei et al. , Misiakiewicz , Barzilai and Shamir , Zhang et al. .

Proof sketch of Corollary 12.Note that this holds for any scaling of \(d\) and \(m\) where \(d=( m)\), but we take this particular one for concreteness. As we show in the appendix (Theorem 21), it is not hard to see that \(L_{m}_{l}m\) for some constant \(_{l}<1\) and \(L_{m}_{u}m\) for some constant \(_{u}>1\). Theorem 9 implies that we cannot get benign overfitting in this case, i.e. for all \(m\), \((_{0})>)^{2}_{l}} ^{2}\). This shows that \(_{m}(_{0}) )^{2}_{l}}^{2}>^{2}\).

Proof sketch of Corollary 13.We have that for the Gaussian kernel on the sphere, Theorem 7 holds with \(A=1\) (see Appendix C.4 for further details). First, we will compute \(k_{m}\) and hence \(L_{m}\) and \(U_{m}\). After some tedious calculation (Theorem 23 in the appendix), we see that for \(k 2^{l}+l-1\) we have \(N(d,1)++N(d,k)=o(m)\), but for \(k=2^{l}+l\) we have \(N(d,1)++N(d,k)>m\). This shows that \(k_{m}=2^{l}+l-1\) and so again after long calculations \(L_{m}=()\) and \(U_{m}>(md^{0.89})\). To estimate \(_{i=1}^{k}N(d,i)_{i}\), note that eigenvalues are decreasing and \(iN(d,i)=iN(d,i+1)=o(N(d,i+1))\) (take \(k d\)), so it suffices to estimate \(N(d,k)}\). Now, from Equation (7) and an estimate on the size of the first eigenvalue (Corollary 31) \(_{1}=(})\), for fixed \(>0\), we can estimate the size \(_{k}\). We have that

\[_{k}}<_{1}}(_{m} )^{k}(+k+^{2}})^{k}<d^{k(1+ )},\]

for arbitrarily small \(\). For \(k=}{24}\), from additional calculation (Theorem 23 in the appendix), we have that \(m^{} N(d,k) m^{}\) and \(d^{k}<m^{}\). So \(_{i=1}^{k_{m}}N(d,i)_{i}^{2}}<m\). Plugging these back into Theorem 7 gives the desired result.

## 6 Summary

In this paper, we considered the minimum norm interpolating solution of kernel ridge regression in fixed dimension with Gaussian kernel and varying or adaptively chosen bandwidth, and in increasing dimension with various kernels. In fixed dimension, we showed that if the source distribution is uniform on the sphere, then the minimum norm interpolating solution is inconsistent for any choice of bandwidth, and usually worse than the null predictor, except possibly in one particular scaling of bandwidth and with small noise. Furthermore, we showed a general upper and lower bound on the test risk, which we applied in the case of increasing dimension to recover the currently known results about polynomially increasing dimension and show two new results: We showed that no dot-product kernel on the sphere can achieve consistency for logarithmic scaling of the dimension, and obtained the first case of sub-polynomially scaling dimension where the minimum norm interpolating solution exhibits benign overfitting, namely with Gaussian kernel and dimension scaling as \(d=()\)