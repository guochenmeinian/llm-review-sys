# Mitigating Backdoor Attack by Injecting Proactive Defensive Backdoor

Shaokui Wei1 Hongyuan Zha1,2 Baoyuan Wu1

1School of Data Science,

The Chinese University of Hong Kong, Shenzhen, Guangdong, 518172, P.R. China

2Shenzhen Key Laboratory of Crowd Intelligence Empowered Low-Carbon Energy Network

###### Abstract

Data-poisoning backdoor attacks are serious security threats to machine learning models, where an adversary can manipulate the training dataset to inject backdoors into models. In this paper, we focus on in-training backdoor defense, aiming to train a clean model even when the dataset may be potentially poisoned. Unlike most existing methods that primarily detect and remove/unlearn suspicious samples to mitigate malicious backdoor attacks, we propose a novel defense approach called PDB (**P**roactive **D**efensive **B**ackdoor). Specifically, PDB leverages the "home field" advantage of defenders by proactively injecting a defensive backdoor into the model during training. Taking advantage of controlling the training process, the defensive backdoor is designed to suppress the malicious backdoor effectively while remaining secret to attackers. In addition, we introduce a reversible mapping to determine the defensive target label. During inference, PDB embeds a defensive trigger in the inputs and reverses the model's prediction, suppressing malicious backdoor and ensuring the model's utility on the original task. Experimental results across various datasets and models demonstrate that our approach achieves state-of-the-art defense performance against a wide range of backdoor attacks. The code is available at https://github.com/shawkui/Proactive_Defensive_Backdoor.

## 1 Introduction

In recent years, deep neural networks (DNNs) have become ubiquitous across diverse fields, powering applications such as face recognition, self-driving vehicles, and medical image analysis [1; 13; 25; 38]. However, alongside these advancements, the vulnerability of DNNs to malicious attacks presents a critical challenge to their safety and reliability. A particularly alarming threat arises from backdoor attacks, where adversaries secretly introduce backdoors into DNN models during training by subtly altering a fraction of the dataset. This manipulation ensures the model's standard performance on uncontaminated data but erroneously assigns a pre-determined label to any input carrying a specific trigger. Considering its real threats to machine learning systems, especially in security-critical scenarios, it's a practical necessity to investigate and propose effective defense strategies against such attacks to safeguard real-world applications.

To mitigate the threats posed by backdoor attacks, researchers have actively explored various backdoor defense techniques throughout the life cycle of machine learning systems . In this paper, we specifically delve into **in-training backdoor defense**[44; 45; 46], which aims to train machine learning models using datasets that may be contaminated with poisoned data. Most existing methods in this field primarily focuses on identifying suspicious samples through various means, along with mitigating the backdoor effect by directly removing [3; 48] or applying some techniques (\(e.g.\), unlearning [20; 4], or relabel [15; 26; 60]) to the suspicious samples. Despite achieving remarkable performance inbackdoor defense, these methods face certain limitations and challenges. First, most existing works rely on specific assumptions such as the latent separability  or the early learning of poisoned samples [20; 60; 51] to identify the poisoned samples. However, these assumptions may not hold under more sophisticated attacks . As accurately detecting poisoned samples is crucial for those methods, any deviation from their underlying assumptions could lead to performance degradation and compromise their effectiveness. Second, some methods, such as DBD , NAB , and V&B , necessitate complex modifications to the training process, resulting in a substantial increase in training costs.

In this paper, instead of following the traditional detection-and-mitigation pipeline, we propose a proactive approach that leverages the "_home field_" advantage of defenders. Our method, called PDB (short for **P**roactive **D**efensive **B**ackdoor), aims to fight malicious backdoor attacks by injecting a proactive defensive backdoor introduced by the defenders themselves. The primary objective of PDB is to suppress the malicious backdoor with a defensive backdoor while keeping the model's utility on original task. Specifically, When the defensive trigger is presented, the defensive backdoor will dominate the prediction of the proactively backdoored model, effectively suppressing the malicious backdoor's impact. Importantly, our defensive backdoor allows for the restoration of the ground truth label to maintain the model's utility on the original task. To achieve this goal, we first analyze the objective for an effective defensive backdoor and introduce four essential design principles, including **reversibility**, **inaccessibility to attackers**, **minimal impact on model performance**, and **resistance against other backdoors**. Then, we construct an additional defensive poisoned dataset, subsequently utilizing such dataset and the whole poisoned dataset to train the model. Consequently, if only the malicious trigger is present, the model remains under the control of the malicious backdoor. However, when the defensive trigger appears, the defensive backdoor is activated, mitigating the malicious backdoor effect. To evaluate its effectiveness, we compare PDB with five state-of-the-art (SOTA) in-training defense methods across seven SOTA data-poisoning backdoor attack methods involving different model structures and datasets. Our experimental results demonstrate that PDB achieves comparable or even superior performance compared to existing baselines.

Our main contributions are threefold: **1)** We break away from the traditional detection-and-mitigation pipeline by proposing a novel mechanism that injects a proactive defensive backdoor during training, which suppresses the malicious backdoor while preserving the model's utility on the original task, without any specific assumptions about potential malicious backdoor attacks. **2)** By analyzing the primary objective, we introduce essential design principles for an effective defensive backdoor and propose a practical algorithm to implement the defensive backdoor. **3)** We conduct extensive experiments to evaluate the effectiveness of our method and compare it with five SOTA defense methods across seven challenging backdoor attacks, spanning diverse model structures and datasets, demonstrating the superior performance of the proposed method.

## 2 Related work

Backdoor attacks.DNNs face significant security threats from backdoor attacks, which are designed to maintain normal performance on regular inputs while forcing the network to output a predetermined target when a specific trigger is introduced. These attacks can be generally categorized into two types based on the property of the trigger: static-pattern backdoor attacks and dynamic-pattern backdoor attacks. The seminal instance of static-pattern backdoors, BadNets , employed fixed triggers like white squares. To enhance trigger stealthness, the Blended approach  was introduced, which merges the trigger with the host image in a subtle manner. Recognizing the potential for detection in fixed-pattern triggers, the research has pivoted towards dynamic-pattern backdoor attacks. Innovations in this direction, such as SSBA , WaNet , LF , WPDA , IRBA , VSSC  and TAT , have focused on crafting sample-specific triggers that are more challenging to identify. Techniques to refine the stealthness of triggers have been furthered by works like Sleeper-agent  and Lira , which optimize the output to be more covert. The sophistication of backdoor attacks has recently been advanced by strategies for learning-based poisoning sample selection  and re-activation attack . To execute attacks without altering the consistency between the image and its label, 'clean label' attacks have been introduced. For example, LC  and SIG  employed counterfactual methods and additional techniques to modify the image while maintaining label consistency subtly.

Backdoor defenses.The main purpose of backdoor defense is to alleviate the vulnerabilities of DNNs to backdoor attacks by employing various strategies during different stages of the model lifecycle. Therefore, backdoor defenses are typically categorized into three types: pre-training, in-training, and post-training. Pre-training defenses concentrate on the detection and removal of poisoned data points before training. For example, AC  leverages unusual activation patterns to weed out poisoned data, while Confusion Training  relies on a model trained specifically to recognize poisoned instances. VDC  utilizes the capabilities of large multimodal language models for the same purpose. Post-training defenses are applied after a model has been trained. A line of works in this direction focusing on pruning [24; 47; 53; 52; 23] or fine-tuning [55; 28] to neutralize the backdoor. Besides, I-BAU , NPD , and SAU  reverse potential backdoor triggers by adversarial techniques to cleanse the model. NAD  employs a slightly poisoned model to assist in retraining a heavily compromised one.

This paper mainly focuses on the in-training defenses that aim to prevent backdoor insertion during the training phase. Along this direction, ABL  utilizes the observation that the poisoned samples are easier to learn than normal samples, resulting in the different learning speeds between benign and poisoned samples, to detect and unlearn the poisoned samples. Based on similar observation, V&B  first trains a backdoored model to capture the backdoor effect and utilizes the backdoored model to train a benign model by detecting and applying a series of operations on the suspicious samples. Similarly, CBD  first trains a backdoored model for a few epochs and trains a benign model by reweighting the samples and deconfounding the representation. DBD  splits the training process into three steps and employs self-supervised learning techniques to detect suspicious samples and train a benign model. D-ST  leverages the fact that benign samples are less sensitive to image transformations to detect suspicious samples and employs semi-supervised learning to train a benign model. Recently, a few attempts have been made to defend against malicious attacks by incorporating proactive attacks [54; 26]. The work most closely aligned with our approach is NAB , which first identifies and then relabels potentially poisoned samples in the dataset, subsequently embedding non-adversarial triggers into the suspicious samples to mitigate the backdoor effect. In contrast to their methodology, our technique offers a more straightforward solution, eliminating the need for costly detection and relabeling processes, thus reducing overall costs and complexity. In essence, we demonstrate that injecting a defensive backdoor alone is sufficient to defend against backdoor attacks without requiring detection and relabeling of the poisoned samples. We refer readers to  for more defense in adversarial machine learning.

## 3 Method

In Section 3.1, we introduce the essential notations and define the threat model in this paper. Subsequently, we explore the principles behind effective defensive backdoors, illustrated with practical examples in Section 3.2. We present the overall pipeline for our proposed method in Section 3.3.

### Problem setting

Notations.Considering a sample \(\) with label \(y\), a DNN model \(f_{}\) parameterized by \(\) is trained to classify \(\). The space \(=[1,,K]\) denotes the space of candidate labels (\(K 2\)), and \(\) represents the sample space. In the context of backdoor attack, we denote the trigger by \(\) and the trigger injection operator by \(\). Consequently, given a benign sample \(\), the poisoned sample can be generated by \(\). It's important to note that the injection operator \(\) can vary according to the type of trigger \(\).

Threat model.We consider a data poisoning scenario for **malicious backdoor** attack where the attacker can only manipulate a portion of the training dataset to plant trigge but cannot control the training process. By poisoning the dataset, the model trained on the manipulated dataset \(_{tr}\) would normally perform for benign input but classify the inputs with malicious trigger \(\) to predefined target \(\). Besides, we define the portion of manipulated samples as the **poisoning ratio** of backdoor attack.

The defender faces a situation where a potentially poisoned dataset is given. The defender aims to train a model where the malicious backdoor fails to be activated by the malicious trigger, and the model's utility on the original task is maintained. We assume a small benign dataset \(_{cl}\) is reserved for the defender, which can be obtained by various means, including but not limited to purchase from reputable data vendors, generation via state-of-the-art generative models [7; 11; 16], collection from the internet, or applying data cleansing methods . Moreover, we assume that the defender does not have knowledge of either the malicious trigger \(\) or the malicious target label \(\).

### Proactive defensive backdoor

In this paper, we aim to defend the unknown malicious backdoor with the trigger \(\), by inserting a proactive defensive backdoor with a trigger \(_{1}\) into the model. Our primary objective is to ensure that when \(_{1}\) is presented, the model's output will be controlled by \(_{1}\) rather than \(\), thereby suppressing the malicious backdoor. Besides, the model's utility on the original task should be preserved, \(i.e.\), user can still get the true prediction of the benign sample with the defensive trigger. To achieve such a defense goal, the desired defensive backdoor attack should follow the principles below:

* **Principle 1: Reversibility.** The defensive backdoor must be reversible, such that the ground truth label can be restored from the prediction of benign samples attached with \(_{1}\). Such a requirement is crucial for preserving the model performance on benign inputs with \(_{1}\).
* **Principle 2: Inaccessibility to attackers.** The defensive trigger \(_{1}\) should be meticulously designed to be non-replicable and undiscoverable by potential attackers. By doing so, we prevent adversaries from exploiting the same trigger or using inversion techniques to identify it.
* **Principle 3: Minimal impact on model performance.** While stealth is not a strict requirement for the defensive trigger, modified samples should retain sufficient characteristics of the original data. This ensures accurate label recovery from the model's predictions in the presence of \(_{1}\).
* **Principle 4: Resistance against other backdoors.** To effectively mitigate malicious backdoors, the defensive backdoor should be resistant to various backdoor attacks, not only known attacks but also potential future backdoors.

In light of the principles outlined above, we delve into the practical design of our defensive backdoor2.

Following Principle 1.For the first principle, we propose to assign the target label by a bijective mapping \(h:\), such that the target label of a sample with label \(y\) is \(h(y)\) and the ground truth label of a poisoned image with label \(y\) is \(h^{-1}(y)\). A typical choice of \(h\) and \(h^{-1}\) is \(h(y)=(y+1) K\) and \(h^{-1}(y)=(y-1) K\) where \( K\) represents the modulo operation and \(K\) is the number of classes. It's worth noting that in the context of DNNs, \(h\) can also be formulated as a function of logits or features such as \(h(())=-()\) and \(h^{-1}(())=-()\) where \(()\) corresponds to the features or logits of input. This flexibility allows for a broader range of target label assignment strategies.

Following Principle 2 & 3.To follow the second and third principles, the design of the trigger is essential. Consider the patched trigger as an illustrative example, which can be constructed by carefully determining its position and pattern. Regarding the trigger's position, it should be crafted to preserve the core visual patterns of the original image, ensuring that the primary content remains unaltered. As for the trigger's pattern, we leverage the _"home field"_ advantage of the defender, designing a trigger that operates beyond the conventional pixel space. Specifically, for an image with pixel values in the range of \(\), the trigger is engineered to modify regions

Figure 1: Illustration of bijective mapping with \(h(y)=(y+1) K\), with \(K=4\).

Figure 2: Demonstration of generating a defensive poisoned sample. \(V\) is the pixel value of trigger, \(\) is the element-wise product. For the mask, \(0\) is represented by black, while \(1\) is represented by white.

to values beyond this range. This modification renders the trigger infeasible and not invertible by attackers, given the natural constraints of image data.

Following Principle 4.Following the fourth principle, the defensive backdoor is required to be resistant against other backdoors in the dataset. To meet such requirements, the key point is that the defender can control the training process, a _"home filed"_ advantage that attackers lack. On the one hand, the defender can design a strong defensive backdoor, \(e.g.\), adopting a large trigger. On the other hand, the defensive backdoor can be further enhanced by controlling the training process, \(e.g.\), applying data augmentation or adjusting the weight of defensive poisoned samples. More discussion and empirical findings are presented in **Appendix C.7**.

### Backdoor injection

As depicted in Figure 3, our proposed method involves three key steps:

Data preparation.Given a well-designed defensive backdoor with trigger \(_{1}\) and a target label mapping \(h\), a defensive poisoned dataset is first constructed by

\[}_{def}=\{(_{1},h(y))|(,y) _{cl}\}.\] (1)

Model training.Now, a model can be trained on the combination of the malicious poisoned dataset \(_{tr}\) and the defensive poisoned dataset \(}_{def}\). Then, a well-trained model will normally perform for benign inputs while controlled by the corresponding backdoor when either the trigger \(\) or \(_{1}\) is presented. However, if both \(\) and \(_{1}\) are simultaneously presented, the model may become confused due to the lack of such samples in the training dataset. As aforementioned, to ensure that the defensive trigger \(_{1}\) effectively defeats an unknown trigger \(\), some _backdoor enhancement strategies_ such as data augmentation or increasing sample weight can be adopted to enhance the defensive backdoor. In summary, the overall training objective is formulated as follows:

\[_{}_{(,y)_{tr}}L_{0}(f_{}( ),y)+_{(,y)}_{def}}_{1}L_{1}(f_{ {}}(),y)+_{2}L_{2}(f_{}(()),y),\] (2)

where \(_{tr}\) and \(}_{def}\) are the maliciously poisoned training dataset and the defensive poisoned dataset, respectively. The operation \(\) enhances the defensive backdoor by applying operation on the defensive poisoned samples (\(e.g.\), adding noise: \(()=+\) with \((0,1)\)).

Figure 3: Overview of the proposed method. The trigger of the malicious backdoor is a white square, and its target label is \(0\). The trigger of the defensive backdoor is represented by a white shield, and the target label mapping is \(h(y)=(y+1) 10\) and \(h^{-1}(y)=(y-1) 10\).

In \((2)\), the first term stands for the loss on the poisoned dataset, the second term stands for the loss of injecting our defensive backdoor, and the third loss aims to enhance the defensive backdoor. We use \(L_{0}\), \(L_{1}\), and \(L_{2}\) to represent the loss function for each term, which are usually Cross-Entropy losses if not specified. The parameters \(_{1}\) and \(_{2}\) are introduced to balance the contributions of the respective loss components. More details for the model training and implementation can be found in **Appendix** A.

Inference.During the inference, each input sample \(\) is initially embedded with the defensive trigger, and the model's prediction \(f_{}(_{1})\) is obtained. Subsequently, the authentic prediction is reconstructed via the inverse mapping \(h^{-1}(f_{}(_{1}))\).

Below, we provide a high-level pseudocode representation of our proposed method for training and inference:

``` Input: Model \(f_{}\), poisoned training set \(_{tr}\), reserved benign dataset \(_{cl}\), defensive trigger \(_{1}\), defensive target mapping \(h\), max iteration number \(T\).  Initialize \(f_{}\). \(\) Data preparation  Construct the defensive poisoned dataset \(}_{def}=\{(_{1},h(y)|(,y)_{cl})\). \(\) Model training for\(t=0,...,T-1\)do for each mini-batch in \(_{tr}}_{def}\)do  Update \(\) w.r.t. objective in (2). endfor endfor \(\) Inference for each input sample \(\)do  Predict its label by \(h^{-1}(f_{}(_{1}))\). endfor ```

**Algorithm 1** Proactive Defensive Backdoor (PDB)

## 4 Experiments

### Experiment setting

Backdoor attack.To assess our method, we consider seven leading backdoor attacks: BadNets , Blended method , Sinusoidal Signal (SIG) attacks , Sample-Specific Backdoor Attacks (SSBA) , WaNet , BPP attack  and TrojanNN attack . Note that to expand our evaluation scope, we have modified certain attacks originally intended for training-controllable scenarios by excluding their training control components and we postpone the details to **Appendix** A. For a consistent and reliable evaluation, we utilize configurations from the BackdoorBench framework [44; 46], which offers a standardized backdoor attack assessment platform. Each attack is implemented with a 5% poisoning rate, targeting the \(0^{th}\) label if not specified. The performance of these attacks is measured across three benchmark dataset, \(i.e.\), CIFAR-10 , Tiny ImageNet , and GTSRB , and analyzed using three neural network architectures, \(i.e.\), PreAct-ResNet18  VGG19-BN  and ViT-B-16 . Due to limitations in space, we present results for GTSRB and VGG19-BN in **Appendix** B. It is important to note that the clean label attack SIG is only applicable to CIFAR-10 with the set poisoning ratio. Additional information on these attacks is available in **Appendix** A.

Backdoor defense.In this paper, we benchmark our approach against popular and advanced backdoor defense methods, including AC , Spectral signatures , ABL , DBD , NAB . For a fair comparison, we adopt the configurations recommended by the BackdoorBench framework [44; 46]. Note that we were unable to achieve satisfactory results for DBD on Tiny ImageNet with ViT-B-16, so it has been excluded in this case. For our method, we set the reserved dataset size to 10% of the training dataset unless otherwise specified. The chosen parameters are \(_{1}=1\) and \(_{2}=1\). To enhance the defensive backdoor, each defensive poisoned sample is sampled five times in an epoch, and we set \(()=+0.1\) with \((0,1)\). The defensive backdoor utilizes a target mapping function \(h(y)=(y+1) K\), along with a patch trigger with pixel

[MISSING_PAGE_FAIL:7]

poisoned samples. AC identifies poisoned samples through clustering in the latent space, considering smaller clusters as likely to contain poisoned data. Spectral detects outliers in the latent space to identify such samples. However, with a poisoning ratio of 5% for Tiny ImageNet (200 classes, each class accounts for 0.5%), the poisoned samples become the majority within the target class, breaking the underlying assumptions of both methods and resulting in high ASR values. Additionally, while ABL, DBD, and NAB can defend against certain attacks, they fall short against more sophisticated adversaries, highlighting PDB's robust defense performance.

**PDB achieves an excellent balance between defense performance and model utility.** Apart from its robust defensive performance, PDB distinguishes itself through its ability to preserve benign accuracy. Unlike ABL, DBD, and NAB, which often sacrifice considerable benign accuracy in exchange for reduced ASR, leading to lower DER values, PDB maintains a high DER by effectively managing this trade-off. The preservation of model utility, without compromising defense effectiveness, further solidifies PDB's status as a promising strategy in backdoor defense.

The results demonstrate the superiority of PDB in defending against backdoor attacks. By effectively reducing ASR and maintaining a high DER, PDB stands out as a valuable defense approach for backdoor attack.

### Analysis

Understanding the effect of PDB.To elucidate the underlying mechanism of PDB, we delve into the impact of the defensive backdoor by analyzing the T-SNE embeddings and the Trigger Activation Change (TAC). TAC, adapted from Zheng et al. , is designed to measure the change of activation values for each neuron when comparing maliciously poisoned samples to their benign counterparts. Let \(\) be a feature extractor which maps an input image \(x\) to the latent activations. For an input image \(x\), we can construct the malicious poisoned sample \(x\). In PDB, a defensive trigger is added to the malicious poisoned sample, crafting sample \(x_{1}\), aiming to suppress the malicious backdoor. Therefore, for dataset \(D\), we define

\[_{1}=((x)- (x))}{|D|},\] (4) \[_{1}=((x _{1})-(x))}{|D|}.\] (5)

In Figure 4, we present the visualization results for the BadNets attack on the CIFAR-10 dataset, utilizing a poisoning ratio of 5% alongside a PreAct-ResNet architecture. The illustration reveals that planting a defensive trigger to the inputs prompts a shift in the feature space, resulting in the formation of new clusters and effectively alleviating the backdoor effect. Moreover, the TAC analysis for both the initial and final blocks demonstrates that the incorporation of a defensive trigger substantially mitigates the activation changes triggered by the malicious backdoor.

Figure 4: Visualization of T-SNE and TAC for the BadNets attack on CIFAR-10 with a poisoning ratio of 5% and PreAct-ResNet. The T-SNE visualizes features in the 4th block of PreAct-ResNet18, and TAC is calculated for both the 1st and the 4th blocks (4 blocks in total). Neurons are indexed in descending order based on their TAC values without \(_{1}\).

Defense effectiveness under different poisoning ratios.To investigate the influence of poisoning ratios on defense performance, we evaluate our method against attacks with poisoning ratios ranging from 1% to 40% on CIFAR-10 with PreAct-ResNet18. The results are summarized in Table 3, from which we can find that the proposed method can consistently mitigate malicious backdoor effect across a wide range of poisoning ratios. For a more comprehensive evaluation of the influence of the poisoning ratio, please refer to **Appendix** B.

Training cost comparison.We first analyze the training complexity of PDB and we refer readers to BackdoorBench for the training complexity of other methods. Let \(C_{sl}\) be the supervised training complexity. Then, we denote the size of the training dataset and the size of the defensive poisoned dataset by \(N_{tr}\) and \(N_{def}\), respectively. Let \(F\) be the frequency of sampling defensive poisoned samples. The training complexity of PDB is given by: \(O((1+}{N_{tr}}) C_{sl})\).

To evaluate the empirical runtime, \(i.e.\), training time of different defense methods, we conduct experiments against the BadNets attack for the PreAct-ResNet18 architecture on CIFAR-10 and GTSRB, ViT-B-16 for Tiny ImageNet, all with a poisoning ratio of 5%. The experiments are conducted on an RTX 4090Ti GPU, and the results are summarized in Table 4. From Table 4, We can find since \(}{N_{tr}}\) is set as a small value, the runtime of PDB is not much larger than the baseline (\(i.e.\), No Defense). In contrast, the runtime of DBD and NAB are significantly higher due to their reliance on self-supervised and semi-supervised training techniques.

Resistance to ALL2ALL attack.We also evaluate PDB for ALL2ALL attacks on CIFAR-10 using PreAct-ResNet18. The poisoning ratio is set to 5% and the target labels for samples with labels \(y\) are \((y+2) K\) (different from the defensive target). The experimental results are summarized in Table 5. Notably, PDB achieves the best defending performance, demonstrating superior effectiveness in defending against backdoor attacks with multiple targets.

Resistance to adaptive attack.In our previous experiments, we assumed that attackers had no knowledge of the defense method. However, when attackers are aware of the deployment of PDB, they may design adaptive attacks to bypass the defense. One straightforward approach is to strengthen the malicious backdoor to counteract the defensive backdoor. To assess our method's resistance to

   Defense \(\) & No Defense & AC  &  & ABL  & DBD  & NAB  &  \\  Attack \(\) & ACC & ASR & ACC & ASR & DER & ACC & ASR & DER & ACC & ASR & DER & ACC & ASR & DER & ACC & ASR & DER \\  BadNets  & 92.50 & 61.33 & 90.10 & 53.7 & 52.61 & **92.33** & 57.33 & 51.72 & 52.46 & 59.96 & 30.66 & 87.10 & 4.52 & 75.50 & 80.51 & 62.74 & 44.00 & 90.68 & **2.72** & **78.40** \\ Blended  & 93.51 & 83.87 & 91.36 & 78.56 & 51.58 & **93.72** & 84.66 & 50.00 & 68.04 & 35.62 & 61.39 & 75.24 & 28.62 & 69.49 & 90.34 & 79.09 & 50.80 & 91.87 & **3.95** & **89.14** \\ SIG  & 93.52 & 88.15 & 91.49 & 83.07 & 51.52 & **84.02** & 88.77 & 50.00 & 67.20 & 59.67 & 51.08 & 76.19 & 20.236 & 75.28 & 82.65 & 83.19 & 47.04 & 91.73 & **3.13** & **91.62** \\   

Table 4: Running time (s) comparison of defense methods.

    &  &  &  &  &  \\  Defense \(\) & No Defense & PDB (**Ours**) & No Defense & PDB (**Ours**) & No Defense & PDB (**Ours**) & No Defense & PDB (**Ours**) & No Defense & PDB (**Ours**) & **PDB (**Ours**) & No Defense & PDB (**Ours**) \\  Attack \(\) & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR \\  BadNets  & 93.14 & 74.73 & 91.59 & 0.31 & 92.64 & 88.74 & 91.08 & 0.38 & 91.32 & 95.03 & 90.25 & 0.40 & 90.17 & 96.12 & 89.18 & 0.16 & 86.16 & 97.77 & 86.32 & 0.28 \\ Blended  & 93.76 & 94.88 & 91.77 & 1.20 & 93.67 & 99.61 & 91.36 & 70.90 & 93.47 & 99.92 & 91.21 & 92.92 & 99.92 & 99.90 & 90.74 & 1.51 & 91.74 & 99.98 & 89.04 & 0.27 \\ BPP  & 90.81 & 87.23 & 90.73 & 1.38 & 91.47 & 93.94 & 90.43 & 1.90 & 90.69 & 99.78 & 90.47 & 1.11 & 91.45 & 99.71 & 90.44 & 1.29 & 90.66 & 99.99 & 89.22 & 0.49 \\ Average & 92.57 & 85.61 & 91.36 & 0.96 & 92.59 & 95.90 & 90.96 & 99.13 & 99.82 & 90.64 & 0.81 & 91.51 & 98.58 & 90.12 & 0.99 & 89.52 & 99.25 & 88.19 & 0.34 \\   

Table 3: Defense results (%) under different poisoning ratios on CIFAR-10 and PreAct-ResNet18.

such adaptive attacks, we evaluate it against BadNets with varying trigger sizes and poisoning ratios, representing different strengths of backdoor attacks. The results, summarized in Table 6, demonstrate that PDB can consistently mitigate backdoor against adaptive attacks with various malicious trigger size and poisoning ratios. Note that to keep the stealthness of malicious backdoor, its poisoning ratio and trigger size is expected to be constrained. However, the defensive backdoor can utilize a large trigger size and high sampling frequency to meet the Principle 4, therefore, mitigating the malicious backdoor.

Appendix structure.Due to page limitations, more experiments and analyses have been moved to the Appendix. The Appendix is structured as follows: In **Appendix**A, we provide the details for the experiments, including the implementation of our method, the parameters, and the setting for all attacks and defense methods. In **Appendix**B, we provide a more comprehensive comparison between our method and baselines across different datasets, poisoning ratios, and model structures. In **Appendix**C, we discuss the influence of key components for PDB, such as triggers, targets, and reserved datasets, and make comparisons to more baselines.

## 5 Conclusion

In this paper, we propose a proactive approach to defend against malicious backdoor attacks in DNNs. Rather than relying on traditional detection and mitigation pipeline, our method, PDB, leverages the _"home field"_ advantage of defenders to inject a defensive backdoor to fight against malicious backdoor. To achieve such a goal, we introduce four essential design properties for an effective defensive backdoor: reversibility, inaccessibility to attackers, minimal impact on model performance, and resistance to other backdoors. By incorporating a defensive backdoor during training, we suppress the impact of malicious backdoors when the defensive trigger is present. Our approach offers several advantages over existing methods. First, it does not rely on accurate detection of poisoned samples and any assumption for attacks, avoiding performance degradation when some poisoned samples evade detection. Second, PDB does not require complex modifications to the training process, minimizing training cost. In summary, PDB provides a novel and effective defense method against backdoor attacks, enhancing the safety and reliability of DNNs.

Limitations and future work.Currently, PDB faces several key limitations. First, its reliance on clean samples presents a practical challenge, prompting the exploration of alternative sources, such as generated data. Second, investigating PDB across diverse machine learning tasks is essential for broader applicability. Addressing these limitations through future research will enhance the defense's effectiveness and facilitate its widespread adoption in safeguarding machine learning systems against backdoor attacks.

Broader impacts.The broader impacts can be considered from both positive and negative perspectives. On the positive side, PDB enhances the security and reliability of DNNs, thereby contributing to the trustworthiness of AI technologies. However, there are potential negative implications that should be considered. The technique could potentially be misused if it falls into the wrong hands, who might use the defensive backdoor mechanism for nefarious purposes.

    &  &  &  \\  Defense \(\) & No Defense & PDB (**Ours**) & No Defense & PDB (**Ours**) & No Defense & PDB (**Ours**) \\  Malicious trigger size & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR & ACC & ASR \\ 
4x4 & 92.39 & 96.83 & 90.66 & 0.18 & 91.14 & 97.67 & 90.01 & 0.21 & 90.38 & 98.13 & 89.65 & 0.49 \\
5x5 & 93.11 & 97.69 & 91.28 & 0.29 & 92.79 & 97.98 & 90.97 & 0.28 & 92.20 & 98.30 & 90.02 & 0.56 \\
6x6 & 93.26 & 98.16 & 91.62 & 0.27 & 92.48 & 98.68 & 90.83 & 0.33 & 92.01 & 98.83 & 90.03 & 0.69 \\
7x7 & 93.65 & 98.66 & 91.46 & 0.31 & 93.07 & 99.03 & 91.03 & 0.56 & 92.56 & 99.23 & 90.48 & 0.67 \\
8x8 & 93.51 & 99.24 & 91.16 & 0.37 & 92.82 & 99.83 & 91.14 & 95.82 & 93.59 & 90.20 & 0.74 \\
9.9 & 93.45 & 99.53 & 91.12 & 0.51 & 92.26 & 99.67 & 90.84 & 0.56 & 92.15 & 99.72 & 90.39 & 0.67 \\
10x10 & 93.20 & 99.66 & 91.37 & 0.54 & 93.17 & 99.74 & 90.76 & 0.78 & 92.58 & 99.81 & 90.45 & 0.82 \\   

Table 6: Defense results (%) against adaptive attacks with different poisoning ratios.