# Robust Model Reasoning and Fitting via

Dual Sparsity Pursuit

 Xingyu Jiang\({}^{1,2}\) &Jiayi Ma\({}^{2}\)

\({}^{1}\)Huazhong University of Science and Technology, \({}^{2}\)Wuhan University

{jiangxy998,jyma2010}@gmail.com

Corresponding author

###### Abstract

In this paper, we contribute to solving a threefold problem: outlier rejection, true model reasoning and parameter estimation with a unified optimization modeling. To this end, we first pose this task as a sparse subspace recovering problem, to search a maximum of independent bases under an over-embedded data space. Then we convert the objective into a continuous optimization paradigm that estimates sparse solutions for both bases and errors. Wherein a fast and robust solver is proposed to accurately estimate the sparse subspace parameters and error entries, which is implemented by a proximal approximation method under the alternating optimization framework with the "optimal" sub-gradient descent. Extensive experiments regarding known and unknown model fitting on synthetic and challenging real datasets have demonstrated the superiority of our method against the state-of-the-art. We also apply our method to multi-class multi-model fitting and loop closure detection, and achieve promising results both in accuracy and efficiency.

_Code is released at:_https://github.com/StaRainJ/DSP.

## 1 Introduction

Geometric model estimation is a fundamental problem in computer vision, serving as the core of many high-level tasks including structure-from-motion , SLAM , and data alignment . The models to specific data can be explained as line/ellipse/circle for 2D point sets, plane/cylinder/sphere for 3D point sets, or fundamental/homography/affine model for point correspondences extracted from two-view images, _etc._ Estimating the parameters of a predefined model from inliers only is well studied . But real-world inputs unavoidably contain severe noises and outliers, posing great challenges for accurate model estimation. Besides, another key problem rarely considered is _how to recovery the model parameters without knowing the model type_.

Recent researches mainly focus on proposing robust estimators to tackle the impact of noise and outliers, such as regarding it as a Consensus Maximization (CM) problem. This problem can be well solved with **Sample Consensus** (SAC) methods, such as RANSAC  and its variants. They commonly sample a smallest inlier set, to best fit a given geometry model following a hypothesize-and-verify strategy . Inside this loop, the model is actually estimated by Direct Linear Transform (DLT) method with a Least-Square (LS) solution. SAC methods can provide probabilistic guarantee of hitting an all-inlier subset. However, this scheme succeeds only if given a predefined model type with sufficient time budget . Another popular strategy is to pose it in a **Global Optimization** framework, which formulates the fitting task as a subspace learning problem . Specifically, the representative method, Dual Principal Component Pursuit (DPCP) , tries to minimize an \(_{1}\) co-sparse objective on the sphere. This is further applied to estimate specific models , which typically embeds the model as a single hyperplane, or the intersection of multiple hyperplanes, and optimizes globally. This type of methods are admitted withefficient implementations and strong theoretical guarantees, thus arising great research interest in recent years. **Deep Learning** has also stimulated numerous methods for geometric model learning, which extract deep geometric cues from sparse points using multilayer perceptrons [49; 43; 50], convolutional  or graph networks . They can fast output potential inliers once trained, but still require robust estimator such as RANSAC as postprocess for accurate model estimation.

The above mentioned methods can only work on a fact that: _one is certain about the true geometric models, then uses them to guide the formulation for parameter estimation_. But in fact, we can always fit a model less constrained to obtain higher inlier count [31; 37]. Specifically, for homography data, estimating a fundamental matrix may return more consensus points, but many are outliers. While homography estimation can only find the largest plane structure in 3D scene, missing considerable inliers for full motion data. Fitting an unknown model for heavily contaminated data is much challenging and is in general NP hard. Existing methods solve it with model selection criteria [1; 42; 44], which first fit all possible models, then select the best one with a geometric verification, such as widely used GRIC metric [44; 32]. This is what is done in SfM or SLAM pipeline for fundamental matrix and homography identification . However, such strategy is limited by the greedy selection strategy thus requiring huge computational burden. In addition, the used insufficient information would easily cause wrong selection of the true model for constrained motions of camera [37; 31; 32].

In this paper, we will simultaneously solve _i) outlier rejection, ii) true model reasoning_ and _iii) parameter estimation_ in a unified optimization modeling. To this end, we start with introducing a common paradigm for exact model fitting, then derive our sparse subspace learning theory, which makes it possible to estimate the true model without knowing the model type, and robust to outliers. On this basis, we convert the objective into a continuous optimization paradigm that estimates sparse solutions for both bases and outlier entries. Wherein a fast and robust solution is proposed, that is based on superiorities of projected sub-gradient method (PSGM) and alternating optimization strategy. Finally, the true model and the inliers are directly extracted from our solutions. **Contributions**: i) We are the first to propose a continuous optimization modeling for geometric model fitting with unknown model type and dominant outliers. ii) We propose sparse subspace recovery theory, which is a novel formulation for model reasoning and fitting. iii) We integrate the proximal approximation strategy and sub-gradient descent method into the alternating optimization paradigm, which solves our dual sparsity problem with a convergence rate of \((1/k^{2})\). iv) Extensive experiments on known/unknown model fitting and two visual applications are designed to validate the superiority of our method.

## 2 Methodology

This paper aims to explore a valid solution for geometric model reasoning and robust fitting. Before this, we first give a brief review of the widely-used solution for exact model estimation, which helps to derive our concerned unknown model fitting problem.

### Geometry Preliminaries and New Insights

In the community of model fitting, the objective with geometric error is extremely hard to optimize due to the highly non-linear nature. In contrast, if the data are properly normalized, using linearized error (_i.e._, algebraic error) to construct objective would show great efficiency to find the optimal solution . For algebraic objective, DLT is known as an efficient method. To be specific, suppose we are given a set of feature correspondences \(=\{_{i}=(_{i},^{}_{i})\}_{i=1}^ {N}\), where \(_{i}=(u_{i},v_{i},1)^{}\) and \(^{}_{i}=(u^{}_{i},v^{}_{i},1)^{}\) are column vectors denoting the homogeneous coordinates of feature points extracted from two-view images. Our goal is to recover the underlying geometric structure including _Fundamental matrix_, _Homography_ and _Affine_. This two-view problem is essential in 3D vision applications, and is the main focus of this paper.

**Fundamental Matrix \(^{3 3}\)** describes the entire epipolar geometry \(^{}_{i}_{i}=0\). It is suggested to be represented by single equation living in the polynomial space

\[_{}(_{i},^{}_{i})^{}vec()=0,\] (1)

where

\[_{}(_{i},^{}_{i})^{}=(u^{}_ {i}u_{i},u^{}_{i}v_{i},u^{}_{i},v^{}_{i}u_{i},v^{}_{i}v _{i},v^{}_{i},u_{i},1),\] (2)

is the embedding of correspondence \((_{i},^{}_{i})\) under epipolar constraint, and \(vec()^{9}\) is the vector form of matrix \(\) in row-major order. \(N 8\) correspondences can uniquely determine \(\) up to scale.

**Homography \(^{3 3}\)** describes the pure rotation or plane projection, which claims that \(_{i}^{}\) and \(_{i}\) are co-linear, implying \([_{i}^{}]_{}_{i}=,\) where \([_{i}^{}]_{}\) is a skew-symmetric matrix. In DLT solution, it usually converts to the following constraint:

\[_{}(_{i},_{i}^{})^{}vec()=,\] (3)

where \(vec()^{9}\), similarly. And \(_{}(_{i},_{i}^{})\) denotes the homography embedding:

\[_{}(_{i},_{i}^{})^{}\!=\! u_{i}&v_{i}&1&0&0&0&-u_{i}^{}u_{i}&-u_{i}^{}v_{i}&-u_{i}^{ }\\ 0&0&0&u_{i}&v_{i}&1&-v_{i}^{}u_{i}&-v_{i}^{}v_{i}&-v_{i}^{} .\] (4)

This constraint suggests that, \(\) can be estimated from given at least 4 correspondences.

**Affine Matrix \(^{3 3}\)** is the degraded case of \(\), with the last row of \(\) being \(\). This model implies a linear transformation \(_{i}^{}=_{i}\). The affine constraint is also represented as:

\[_{}(_{i},_{i}^{})^{}vec()=,\] (5)

where \(vec()=[a_{11},a_{12},a_{13},a_{21},a_{22},a_{23},1]^{}\), and

\[_{}(_{i},_{i}^{})^{}= u_{i}&v_{i}&1&0&0&0&-u_{i}^{}\\ 0&0&0&u_{i}&v_{i}&1&-v_{i}^{}\] (6)

is the affine embedding. Using Eq. (5), the solution can be given by at least \(3\) correspondences.

Given sufficient inliers, DLT method provides an efficient solution for geometric model fitting, which converts to solving the following minimal least-square problem:

\[_{}\|^{}\|_{2}^{2}, s.t.\|\|_{ 2}=1,\] (7)

where \(\) is the data embedding matrix for specific model \(\), _i.e.,_\(_{i}=_{}(_{i},_{i}^{})\) with \(\{,,\},\) as defined in Eqs. (2), (4) and (6). \(=vec()^{D}\) indicates the parameter vector of \(\), and \(\|\|_{2}=1\) restricts it in a sphere thus avoiding the trivial solution. The optima solution \(^{*}\) is exactly the right singular vector corresponding to the smallest singular value of \(\). However, DLT only works on the outlier-free case, due to the use of \(_{2}\) norm. Thus, it is usually applied in SAC paradigm by sampling a clean subset to fit a given model. Another practical formulation is using latent variable inside (7) to indicate the inlier, or using truncated or \(_{1}\) loss . In contrast, the \(_{1}\) objective is easier to optimize due to the convex nature by solving

\[_{}\|^{}\|_{1}, s.t.\|\|_{2}=1.\] (8)

This is actually a subspace learning problem as introduced in DPCP series [45; 52; 13], which can be efficiently solved by a projected sub-gradient method (PSGM) . Besides,  gives comprehensive comparisons of above mentioned losses under different model cases. These works also demonstrate the robust property of (8), which roughly states that, the estimation task can even tolerate \(O(m^{2})\) outliers (\(m\) is the inlier number).

**New Insights.** Above formulations almost work on knowing the exact information of model type, such that the embedding matrix \(\) is correct, _i.e.,_ choosing \(\) from \(\{,,\}\) to generate \(_{}\). Practically, predefining a correct model is much difficult. First, the camera type and motion, or the scene in shooting is not always known. Besides, we cannot manually define the correct model for each image pair in large-scale vision tasks. In this regard, this paper tends to simultaneously seek the true model and robustly estimate the model parameters, by proposing a general continuous optimization framework with dual sparsity constraints.

### Dual Sparsity Formulation

In this paper, we aim to estimate the model parameters without knowing the model type. That means we should find a common data embedding \(()\) thus avoiding exact \(_{}()\) for each model. Instead it converts to constructing model embedding \(()\), to generate a common constraint as:

\[(_{i},_{i}^{})^{}( )=,\{,,\}.\] (9)

Eq. (9) can provide great advantages for our model reasoning and fitting task. First, it avoids the requirement for exact model type to construct \(_{}()\) before estimation. Moreover, the solution would have the form of \(()\), which directly guides the identification of different models. Eq. (9) essentially relies on the following _Sparse Subspace Recovery_ (SSR) theory.

**Theorem 1**: _(SSR) Geometric model fitting can be seen as a subspace recovery problem represented by the intersection of multiple sparse hyperplanes under an over embedded data space._

To better interpret the proposed _Theorem_ 1, we first give an example on \(2\)**point set fitting**. Given a set of clean \(2\) points \(=\{_{i}=(x_{i},y_{i})\}_{i=1}^{N_{i}}\) that are sampled from a structure of _line (L), parabola (P)_ or _ellipse (E)_, _i.e., \(\{L,P,E\}\)_. The model parameters can be estimated by exact data embedding \(_{}(x_{i},y_{i})\) with DLT method (7). For ellipse estimation, \(_{E}(x_{i},y_{i})=[1,x_{i},y_{i},x_{i}^{2},y_{i}^{2}]^{}\), while for a line, \(_{L}(x_{i},y_{i})=[1,x_{i},y_{i}]^{}\). But in our sparse theory, with a higher-order embedding, such as fixing \((x_{i},y_{i})=_{E}(x_{i},y_{i})\), we can obtain a sparse solution \(=[a,b,c,0,0]^{}\) for a line model, and \(=[a,b,c,d,0]^{}\) for \([a,b,c,0,e]^{}\) for a parabola model. See Fig. 2, or refer to _Append.D and Fig. 6_ for details. Thus, the SSR task can be formulated as:

\[_{}\ \ \|\|_{0},\ \ ^{}=,\| \|_{2}=1,\] (10)

where \(\) is the over embedding for input data with \(_{i}=(x_{i},y_{i})=_{E}(x_{i},y_{i})\), and we denote \(\) as a sparse basis that indicates a hyperplane under SSR theory. The \(_{0}\) norm in the objective suggests to use less parameters to fit the given data. Note that, \(_{i}\) can be composed by more polynomials such as \(x_{i}y_{i}\), \(x_{i}^{3},y_{i}^{3}\) or higher-order forms, but it is not necessary for the model pool \(\{L,P,E\}\). Problem (10) can be illustrated by a point model fitting \(x=t\) as in Fig. 1. The middle plot shows that, the solution \(^{*}\) or \(-^{*}\) is ideally vertical to the embedding vector \(}_{i}\) of inliers. With higher-order embedding \([x_{i},1,x_{i}^{2}]^{}\), the solution would be any vector located in the plane \(\) that is vertical to \(}_{i}\) and through the origin. However, with sparsity constraint, the solution reduces to be the intersection of plane \(\) and one of coordinate planes \(\{_{xoy},_{xoz},_{yoz}\}\). Since \(x_{i}^{2}\) causes higher complexity, the final solution is exactly \(_{xoy}\), _i.e., \(^{*}=}}[1,-t,0]^{}\)_._

Denoting \(G_{}=(D,d,r,s)\) as the geometry relationship for a model \(\), where \(D\) is the ambient dimension of embedded data, \(d\) is the subspace dimension, \(r\) denotes the number of basis to be estimated, while \(s\) means the minimum sparsity in each basis (_i.e.,_ the least number of zero parameters), the relations for 2D models are \(G_{L}=(5,4,1,2)\), \(G_{P}=(5,4,1,1)\), and \(G_{E}=(5,4,1,0)\). Next, we emphatically introduce the new formulation for two-view models, which is outlined in _Append.D_.

Extending to **two-view geometry**, our SSR task can directly benefit from (8), which is a special case in subspace recovery , with \(d=D-1\). In subspace learning theory, the learning problem is significantly easier when the relative dimension \(d/D\) is small . For such purpose, and together with our sparse theory, we can obtain the following new formulations. First, for \(\) estimation, since its geometric constraint derives only one single hyperplane living in a \(9\)-dimensional space, thus Eq. (9) has the same form with Eq. (1). And the geometric relation is clearly \(G_{}=(9,8,1,0)\).

**Proposition 1**: _Given \(n 4\) clean correspondences conforming to a homography transformation \(\), the model estimation can be converted to recovering a subspace with dimension no more than 6, which consists of \(3\) sparse hyperplanes with sparsity \(s 3\) under the 9-D embedding \(=_{F}(_{i},_{i}^{})\)._

_Proof._  Homography model actually derives \(r=3\) equations, and each of them indicates a sparse hyperplane with \((_{i},_{i}^{})^{}()= \), where \(()^{9 3}\) is the embedding of model \(\), which consists of three sparse intersected hyperplanes (_i.e.,_ orthometric bases) as:

\[()=_{1 3},_{1}^{},- _{2}^{}\\ -_{3}^{},_{1 3},_{1}^{}\\ _{2}^{},-_{1}^{},_{1 3}^{},\] (11)

Figure 1: Illustration of fitting a point model (\(x=t\)). (a) original points, (b) embedding under DLT solution, (c) embedding under our SSR theory. \(^{*}\) or \(-^{*}\) is the optimal solution.

with \(=[_{1}^{};_{2}^{};_{3}^{}]\). Since \(()\) is of rank 3, the dimension of subspace \(d 6\). And the solution for each basis would have at least \(s=3\) zeros, thus the geometric relation is \(G_{}=(9,6,3,3)\). 

**Proposition 2**: _Given \(n 3\) clean correspondences conforming to an affine transformation \(\), the model estimation can be converted to recovering a subspace with dimension no more than 7, which consists of \(2\) sparse hyperplanes with sparsity \(s 5\) under the 9-D embedding \(=_{F}(_{i},_{i}^{})\)._

_Proof._ Eq. (5) derives \(r=2\) linear equations, and each of them indicates a sparse hyperplane with \((_{i},_{i}^{})^{}()=\), where \(()^{9 2}\) is the embedding of model \(\), which consists of two sparse intersected hyperplanes as:

\[()=_{9 2}^{}=0,0,a_{11},0,0, a_{12},-1,0,a_{13}\\ 0,0,a_{21},0,0,a_{22},0,-1,a_{23}^{}.\] (12)

Because \(()\) is of rank 2, the dimension of subspace is no more than 7. And the solution for each basis would have at least \(s=5\) zeros, thus the geometric relation is \(G_{}=(9,7,2,5)\). 

The above two propositions have well interpreted _Theorem_1. To be specific, \(\) and \(\) estimation can be performed as a hyperplane fitting problem with Eq. (3) and Eq. (5). Alternatively, they can be respectively solved as recovering a 6-dimensional subspace and a 7-dimensional subspace in a common 9-dimensional data embedding space. For the purpose of our unknown model fitting, the advantages of this conversion have been explained in Eq. (9). Besides, the low-relative dimension \(d/D\) significantly makes the subspace learning task easier [17; 18]. In the following, we will give a general formulation for unknown model fitting, then explore its efficient solution.

**Formulation for Unknown Model Fitting.** With no outliers and noise, unknown model fitting can convert to finding all \(r\) independent sparse bases \(=[_{1},_{2},,_{r}]^{D r}\) by solving

\[_{^{D r}}\ \ \|\|_{0},\ \ \ \ ^{}=,()=r,\] (13)

where \(\) is the over embedding matrix of data with \(_{i}=(_{i})\). Rank \(r\) constraint of \(\) asks for \(r\) independent bases. But \(r\) is generally unknown in advance, thus its estimation is also necessary.

**Proposition 3**: _Given sufficient inliers with no noise, if the conformed model derives \(r\) independent bases, then the solution of SSR satisfies \((())=r\)._

_Proof._ Denoting \(^{*}=[_{1},_{2},,_{r}] ^{D r}\) the optimal solution, any subset of \(^{*}\) is also a feasible solution with rank no more than \(r\). For \(\)\(_{r+1}\) with \(\|_{r+1}\|_{2}=1,(^{*})^{}_{r+1}=\), we have \(^{}_{r+1}\). Hence, \(_{r+1}\) is not a valid basis, and we can conclude \((())=r\). \(\)

_Proposition 3_ suggests that the target of unknown model fitting can be converted to finding a maximum number of orthometric bases with sparsity constraint. Considering the outliers and noise, the ideal constraints can be written as \(^{}--=\), where \(\) and \(\) are noise and outlier entries, respectively. Following a common perception, \(\) is assumed to be Gaussian, which suggests to be constricted by _Frobenius_ norm \(\|\|_{F}\). While outlier matrix \(\) is sparse in row, and any \(\|_{i,i}\|_{2} 0\) indicates that the \(i\)-th sample can be considered as an outlier, thus we use \(_{2,0}\) to constrain this property. Then our problem is transformed into a multi-objective optimization task, including minimizing the fitting error, the parameter number, the outlier number, and maximizing the basis number. Hence, the general formulation for robust model reasoning and fitting can be written as a unified form:

\[_{,,r} \ \ \ \|\|_{F}^{2}+\|\|_{0}+ \|\|_{2,0}-(),\] (14) _s.t._ \[\ \ ^{}--=, ^{}=_{r r},\]

where \(\) is an identity matrix of size \(r r\) with \(r=()\). It restricts that the bases are all orthometric. \(,,>0\) are hyper-parameters to balance different loss items. Problem (14) ultimately outputs an exact sparse subspace \(^{D r}\), helping to identify the true model \(\) with \(()\). In the following, we will explore an efficient solution for this multi-objective problem.

### Solution

Problem (14) involves \(_{0}\) minimization, which is the holy grail of sparse approximation. However, \(_{0}\) optimization is NP-hard, and the non-linear objective makes it even more difficult. A practical way is to use \(_{1}\) norm as the best convex approximation instead. Thus, Problem (14) can be relaxed into:

\[_{,,r}&\|^{} -\|_{F}^{2}+\|\|_{1}+\|\|_ {2,1}-(),\\ &&\|_{i}\|_{2}=1,_{i}^{}_{j}=0,\ \ \ i,j=1,2,...,r,\ i j.\] (15)

Another troublesome issue is to maximize \(()\). Consider that Problem (15) actually aims to obtain \(r\) orthometric bases \([}_{1},}_{2},,}_{r}]\) and a sparse outlier matrix \(}=[}_{1},}_{2},,}_{r}]\) satisfying \((,}_{i},}_{i})=\|^{}}_{i}-}_{i}\|_{2}^{2}+ \|}_{i}\|_{1}+\|}_{i}\|_{1}<,  i\{1,2,,r\}\). Thus we decompose (15) into progressively estimating a new sparse bases orthometric to all given bases up to \((,}_{i},}_{i})<\) not holds. For any given bases \(\{=(_{j})|j=1,2,,i-1\}\) (those have been estimated), a new sparse basis \(_{i}(i>1)\) can be estimated by solving

\[_{,}(,,)= \|^{}-\|_{2}^{2}+\| \|_{1}+\|\|_{1},\ \|\|_{2}=1,\ ^{}=0,\  .\] (16)

Since there exist two variables (\(\), \(\)) to be estimated, it is impractical to optimize them directly with gradient descent method as used in . A valid way is to alternatively optimize \(\) and \(\) in an iterative pipeline, which encourages us to compute the closed-form solution for \((,)\) in each iteration. For such purpose, we first calculate the derivatives of objective _w.r.t._ variables, then make them equal to zero. In the \(k\)-th iteration and for given \(^{k-1}\), we can easily obtain

\[^{k}=^{}^{k-1}-(^{k})=_{}(^{}^{k-1}),\] (17)

where \(()\) is a sign function. \(^{k}\) is in fact solved by a standard threshold shrinkage operation \(_{}\) introduced in the _Lemma_ of . (Refer to _Append.B_ for details)

**Given \(^{k}\) Update \(^{k}\).** Due to the coexistence of quadratic and sparse constraints, it is hard to directly optimize. Fortunately,  provides efficient solution for such linear inverse problem with theoretical and practical verification. We exploit its basic idea into our dual sparsity problem, that is to build at each iteration a regularization of the linearized differentiable function, then obtain

\[^{k}=_{}\|-( ^{k-1}- f(^{k-1}))\|_{2}^{2}+\| \|_{1}},\] (18)

where the smallest Lipschitz constraint of the gradient \( f()\) can be calculated by \(L(f)=_{}(^{})\), and \(_{}()\) indicates the maximum eigenvalue of a matrix, and we denote

\[_{L}(^{k-1})=^{k-1}- f(^{k-1}),\] (19)

where \( f(^{k-1})=(^{}^{k-1}- ^{k})\). In this case, Problem (18) is essentially similar to solving \(\), and we can obtain the update formula of \(\) with \(^{k}=_{}(_{L}(^{k-1}))\). (_Proof is in Append.C_)

The above solution for \(\) actually reduces to a subgradient method , which converges at a rate no worse than \((1/k)\). However, it was proven in  that there exists a gradient method with an \((1/k^{2})\) complexity which is an 'optimal' first order method for smooth problems . The core strategy is that the iterative shrinkage operation performs at the point \(}^{k-1}\) which uses a specific linear combination of previous two points \((^{k-1},^{k-2})\) as \(}^{k-1}=^{k-1}+(-1}{t_{k}} )\)(\(^{k-1}-^{k-2}\)) with \(t_{k}=(1+^{2}})\). Then updating \(\) with convergence rate \((1/k^{2})\) is (See Fig. 2)

\[^{k}=_{}(_{L}(}^ {k-1})).\] (20)

Next, we further consider constraints of Problem (16). It can be easily solved by a projection operation  in each iteration, including a sphere projection \(}{\|\|_{2}}\), and an orthogonal projection \((-^{})\), because the projector of orthogonal complement is \(-^{}\).

Eqs. (17) and (20) provide iterative optimization for searching sparse solution of \(\) and \(\), thus we term the whole algorithm as _Dual Sparsity Pursuit_ (DSP). Repeatedly using DSP, we can successfully address the general unknown model fitting task, and the whole process is concluded in _Append.D_. Note that, in two-view problem, each model has at least \(r=1\) basis (_e.g._\(\)) and at most \(r=3\) bases (_e.g._\(\)), thus we denote \(R\) as the max number of bases to be estimated, with \(R=3\).

**Model Reasoning Analysis.** A critical problem is how to reason out the model type and estimate the parameters under a general case. Several methods  have been early proposed to use a selection strategy , but it is not always clear due their ambiguities . This would be well mitigated in our DSP. _To be specific, for 2D point set fitting, we can directly distinguish the correct model from \(\{L,P,E\}\) through the sparsity of estimated \(\), as shown in Fig. 2. As for two-view models, we can also distinguish them via the sparsity and rank of estimated \(\), referring to the model embedding \(()\) or their geometric relations \(G_{}(D,d,r,s)\)._ The following remark may deliver good understanding of this property, which is illustrated in Fig. 4.

**Remark 1**: _If the true model is \(\), the solution for \((_{2},_{2})\) would result in a larger fitting error than \((_{1},_{1})\), i.e., \(\|^{}_{1}\|_{2}\|^{}_{2}\|_ {2}\). In other words, the number of inliers detected by \((_{2},_{2})\) would be much smaller than \((_{1},_{1})\). Since the second basis is essentially to find a homography or affine structure. On the contrary, if the true model is \(\) or \(\), the detected inliers between these two solutions would have small difference, as the orthometric and sparse bases indeed exist._

## 3 Experiment

**Implementation Details: (1) Parameter setting.** In DSP, \(\) and \(\) are two hyper-parameters. Based on , we set \(=0.005(4N)[1,1,0.5,1,1,0.5,0.5,0.5,0.1]^{}\) as default (for 2D model, we set \(=(2N)[0.01,0.1,0.1,1]^{}\)). In addition, we set \(=0.06\) at the beginning, then update it with \(0.98\) for each twenty iterations, and constrain \(_{}=0.02\). Moreover, we set the max iteration as \(2k\), and stop it if \(=\|_{k}-_{k-1}\|_{2} 1e{-}6\). As for \(\), it controls the number of estimated basis, _i.e.,_\(r\). We set \(=(,_{i},_{i})/(,_{i-1},_{i-1})\), and describe its distribution on all real data as in Fig. 3. Based on the best \(\), we set \(=1.2(,_{i-1},_{i-1})\) during the estimation of \(_{i}\). **(2) Synthesized Data.** We synthesize \(300\) image pairs for each model, which consist of different outlier ratio (OR = \(20\%,50\%,80\%\)) and noise level (NL = \(0.2,0.5,0.8,1.0,1.5\) in pixel). **(3) Real Image Data.** 8 public datasets  are used, and we divide them into two groups including **Fund**: kusvod2, CPC, TUM, KITTI, T&T; and **Homo**: homogr, EVD, Hpatch. **(4) Comparisons.** 6 methods are used for evaluation, including three SAC methods RNASAC , USAC , and MAGSAC++ ; one globally optimized method EAS ; and two deep methods OANet , SuperGlue . **(5) Metrics**. All methods applied the normalized 4-point and 8-point algorithms for \(\) and \(\) estimation (which are applied as post-process  in EAS, DSP and two deep methods with \(100\) iterations). To measure the accuracy, we use Geometrical Error (GE, in pixel), which is defined as the re-projection error and Sampson distance  for \(\) and \(\) estimation, respectively. We defined the failed case as that the GE is larger than 5 pixels, or the model is wrongly identified. To avoid the failed cases for a dataset, we use the medium value \(E_{med}\) of GE as measurement. (More details are in _Append.E_).

**Model Reasoning and Robust Fitting Test: (1) Test on Synthetic Data.** In the simulation test, an advanced robust estimator EAS  is used to estimate all possible models first. Then, three widely used model selection criteria, _i.e.,_ AIC , BIC  and GRIC , are equipped. The accuracy (\(\%\)) of model selection are reported in Tab. 1. AIC can achieve good performance for homography and affine models, but not for epipolar cases, particularly for high OR. BIC shows poor performance in fundamental model selection, since it prefers under-fitting in theory. GRIC fully considers the properties of vision problem performing well in most cases. As for our DSP, it can identify the geometric models with the best accuracy even in case of high outlier ratio.

    & Data &  & BIC & GRIC & DSP (ours) \\   & & & & & \\   & F.100 & 100 & 62 & 100 & 100 \\  & F.100 & 100 & 98 & 100 & 100 \\  & A.100 & 100 & 96 & 100 & 100 \\   & F.100 & 100 & 0 & 100 & 100 \\  & F.100 & 100 & 95 & 100 & 100 \\  & A.100 & 99 & 97 & 98 & 100 \\   & F.100 & 85 & 0 & 93 & 1000 \\  & A.100 & 95 & 96 & 98 & 99 \\   & A.100 & 96 & 92 & 95 & 98 \\   & 97.2 & 70.7 & 98.4 & 99.7 \\    & & & & & \\ 

Table 1: Model reasoning results on syn. data _w.r.t_ different OR. Each value \(x\) indicates that \(x\%\) models are correctly selected.

Figure 3: Distribution of \(\) on \(\) data and \(\) data.

**(2) Test on Real Image Data.** In this part, those datasets are divided into Fund and Homo based on their model type. In fact, during real applications, it is more urgent to make sure whether the true model is \(\) or \(\), thus our used real image data are sufficient to verify the performance for real cases. In addition, considering the competitive performance in Tab. 1, we only use GRIC as comparison in model selection stage. Six representative estimators are used for comparison. That is, first using those robust estimators to obtain all possible models, then using GRIC to select the best one. **Qualitative results** on two representative image pairs are shown in Fig. 4, which obviously reveal the mechanism of our DSP as we assumed in Remark 1. In addition, our algebra results are somewhat coarse, but they can be well refined by a post-processing . **Quantitative results** are evaluated in Tab. 2, including \(E_{med}\), _Run Time_ (Time) and failure rate (_FR_). From this table, we find that the other estimators typically achieve poor model selection even if using the best criterion GRIC. Three SAC methods get poor model identification, particularly for Homo data, since many image pairs are from extreme view change with high outlier ratios. OANet and SuperGlue similarly achieve low model reasoning accuracy, since some datasets are from complicated scenarios, such as EVD, T&T and CPC, which are not seen by these two deep methods in their training. Their model fitting performances directly affect the model reasoning accuracy. Overall speaking and comparing with GRIC, our DSP reasons out the true model directly from its solutions with much higher accuracy. As for running time, OANet and USAC almost achieve the best efficiency. Because OANet is a deep model with GPU acceleration (SuperGlue is slower since it takes descriptors as input to generate matches), and USAC integrates the local optimization and fast model verification strategy in its universal framework, but sacrifices the accuracy to some extent.

**Known Model Estimation Test:** Known model estimation is the main focus of current researches, thus we also test on those real image pairs by giving the priori of model type, and provide extensive comparisons. The statistic results are shown in Fig. 5, which only contains four challenging datasets, while others are in _Append.E.3_. Statistic results show that the performance of SAC-based methods would heavily degrade with more time consuming and less accuracy on these challenging datasets. This is because they intrinsically have to sample an outlier-free subset to best fit the given model, this is much difficult for these datasets. OANet and SuperGlue achieve poor fitting accuracy due to the same reason particularly for EVD datasets. As for EAS and our DSP, they commonly optimize from global formulation, thus showing similar top accuracy. However, our DSP performs better comparing with EAS, due to the specified modeling to restrain noise and outliers. And it is much faster, because the use of an acceleration strategy that makes the convergence rate closed to \((1/k^{2})\).

  DataMethods & RANSAC* & USAC* & MAGSAC++* & EAS* & OANet* & SuperGlue* & DSP (ours) \\   & \(E\_med\) & 0.8478 & 0.6545 & 0.6260 & 0.6900 & 0.7579 & 0.8926 & **0.6017** \\  & \(Time(s)\) & 0.5666 & 0.0239 & 0.3918 & 0.2037 & **0.0152** & 0.0721 & 0.0551 \\  & \(FR\) & 0.2925 & 0.1992 & 0.2077 & 0.2142 & 0.2632 & 0.2709 & **0.1136** \\   & \(E\_med\) & 1.0428 & 0.8744 & 0.8978 & 0.8472 & 0.8480 & 0.9392 & **0.8227** \\  & \(Time(s)\) & 2.010 & 0.0550 & 1.3419 & 0.4463 & **0.0342** & 0.0716 & 0.2794 \\  & \(FR\) & 0.3021 & 0.2604 & 0.1424 & 0.0972 & 0.0903 & 0.1181 & **0.066** \\    & \(E\_med\) & 0.8794 & 0.6711 & 0.6383 & 0.7091 & 0.7670 & 0.9026 & **0.6249** \\  & \(Time(s)\) & 0.6638 & 0.0260 & 0.4558 & 0.22 & **0.0130** & 0.0721 & 0.0703 \\   & \(FR\) & 0.2932 & 0.2043 & 0.2033 & 0.2064 & 0.2515 & 0.2606 & **0.1123** \\  

Table 2: Results of model reasoning and fitting on real image pairs. All datasets are divided into Fund. and Homo. based on the model type. Method with * means first estimating all possible models, then using GRIC to select the “best” one. **Bold** and underline indicate the best and second, respectively.

Figure 4: Detected inliers of DSP for real data: \(\)(left) and \(\)(right). Top to bottom: input data, the first \((_{1},_{1})\), second basis \((_{2},_{2})\), and refined results. For visibility, at most \(200\) random selected matches are shown (blue = true positive, green = false negative, red = false positive).

[MISSING_PAGE_FAIL:9]

## Broader Impact

The proposed method enjoys great potential to improve a wide range of industrial applications including image registration and retrieval, camera pose estimation and localisation, 3D reconstruction, _etc._ To be specific, the proposed method achieves significant accuracy gains of model reasoning and fitting on challenging matching benchmarks, which strongly indicates that it will directly benefit many related fields including robotics and autonomous driving, where the geometric model estimation is the foundation. However, our method does have some unwelcome repercussions like many other machine learning techniques. The accurate and robust model reasoning and estimation can be illegally used for a person or property without permission. It may even be weaponized to guide a UAV to perform self-location then carry out a terrorism attack. But these negative impacts are more related to the fields of application rather than the technology itself. And we believe, under strict supervision, that our work will bring more benefits than harms to society.