# Improved Best-of-Both-Worlds Guarantees for Multi-Armed Bandits: FTRL with General Regularizers and Multiple Optimal Arms

Improved Best-of-Both-Worlds Guarantees for Multi-Armed Bandits: FTRL with General Regularizers and Multiple Optimal Arms

 Tiancheng Jin

University of Southern California

tiancheng.jin@usc.edu

&Junyan Liu

University of California, San Diego

jul037@ucsd.edu

&Haipeng Luo

University of Southern California

haipengl@usc.edu

Equal contribution, in alphabetical order.

###### Abstract

We study the problem of designing adaptive multi-armed bandit algorithms that perform optimally in both the stochastic setting and the adversarial setting simultaneously (often known as a best-of-both-world guarantee). A line of recent works shows that when configured and analyzed properly, the Follow-the-Regularized-Leader (FTRL) algorithm, originally designed for the adversarial setting, can in fact optimally adapt to the stochastic setting as well. Such results, however, critically rely on an assumption that there exists one unique optimal arm. Recently, Ito (2021) took the first step to remove such an undesirable uniqueness assumption for one particular FTRL algorithm with the \(}{{2}}\)-Tsallis entropy regularizer. In this work, we significantly improve and generalize this result, showing that uniqueness is unnecessary for FTRL with a broad family of regularizers and a new learning rate schedule. For some regularizers, our regret bounds also improve upon prior results even when uniqueness holds. We further provide an application of our results to the decoupled exploration and exploitation problem, demonstrating that our techniques are broadly applicable.

## 1 Introduction

We study the problem of multi-armed bandits (MAB) where a learner sequentially interacts with an environment for \(T\) rounds. In each round, the learner selects one of the \(K\) arms and observes its loss. The goal of the learner is to minimize her regret, which measures the difference between her total loss and that of the best fixed arm in hindsight. Depending on how the losses are generated, two settings have been heavily studied in the literature: the _stochastic_ setting, where the loss of each arm at each round is an i.i.d. sample of a fixed and unknown distribution, and the _adversarial_ setting, where the losses can be arbitrarily decided by an adversary. In the stochastic setting, the UCB algorithm (Lai and Robbins, 1985; Auer et al., 2002a) attains the instance-optimal regret \((_{i:_{i}>0}})\), where the sub-optimality gap \(_{i}\) is the difference between the expected loss of arm \(i\) and that of the optimal arm. On the other hand, in the adversarial setting, the minimax-optimal regret is known to be of order \(()\)(Auer et al., 2002b; Audibert and Bubeck, 2009), achieved via the well-known Follow-the-Regularized-Leader (FTRL) framework.

Given the rather different algorithmic ideas in UCB and FTRL, it is natural to ask whether there exists an adaptive algorithm that achieves the _best of both worlds_ (BOBW), simultaneously enjoying the instance-optimal \(( T)\)-regret in the stochastic setting and minimax-optimal \(\)-regret in the adversarial setting. This question was first answered affirmatively in (Bubeck and Slivkins, 2012), followed by a sequence of improvements and extensions in the past decade. Among these works, a somewhat surprising result by Wei and Luo (2018) shows that, when configured and analyzed properly, FTRL, an algorithm originally designed for the adversarial setting to achieve \(\)-type regret, in fact can also achieve \(( T)\)-type regret in the stochastic setting automatically. This result was latter significantly improved to optimal by Zimmert and Seldin (2019, 2021) using the \(}{{2}}\)-Tsallis entropy regularizer and extended to many other problems. In fact, these algorithms not only achieve BOBW, but also automatically adapt to intermediate settings with a regret bound that interpolates smoothly between the two extremes.

A key drawback of such FTRL-based approaches, however, is that their analysis for the stochastic setting critically relies on a uniqueness assumption, that is, there exists one unique optimal arm (with \(_{i}=0\)). A recent work by Ito (2021) took the first step to address this issue and proposed a novel analysis showing that the exact same Tsallis-INF algorithm of (Zimmert and Seldin, 2019) in fact works even without this assumption. Unfortunately, his analysis is specific to the \(}{{2}}\)-Tsallis entropy regularizer with an arm-independent learning rate, and it is highly unclear how to extend it to other regularizers which often require an arm-dependent learning rate. For example, extending it to the log-barrier regularizer was explicitly mentioned as an open problem in (Ito, 2021).

In this work, we significantly improve and generalize the analysis of (Ito, 2021), greatly deepening our understanding on using FTRL to achieve BOBW. Our improved analysis allows us to obtain a suite of new results, all achieved _without_ the uniqueness assumption. Specifically, we consider a new and unified arm-dependent learning rate schedule and the following regularizers (plus a small amount of extra log-barrier); see also Table 1 for a summary.

* Log-barrier: with our new learning rate schedule, we show a regret bound of order \(( T)\) for the stochastic setting and \(()\) for the adversarial setting. Here, \(=}}}+_{i V}}\) measures the difficulty of the instance, with \(U=\{i:_{i}=0\}\) being the set of optimal-arms, \(V\) being the set of all remaining sub-optimal arms, and \(_{}}=_{i V}_{i}\) being the minimum non-zero sub-optimality gap. Our bound for the stochastic setting improves those in (Wei and Luo, 2018; Ito, 2021), both of which require the uniqueness assumption.
* Shannon entropy: we show a regret bound of order \((( T)^{2})\) for the stochastic setting and \(( T)\) for the adversarial setting. This improves (Ito et al., 2022) (when applying their more general results to MAB) in two ways: first, their result requires the uniqueness assumption while ours does not; second, their \(\) is defined as \(}}}\), strictly larger than ours.
* \(\)-Tsallis entropy: we also consider Tsallis entropy with a general parameter \((0,1)\), and show a regret bound of order \(( T}{(1-)})\) for the stochastic setting and \(}{(1-)}} \) for the adversarial setting. The only prior work that uses \(\)-Tsallis entropy for BOBW in MAB is (Zimmert and Seldin, 2021), but their algorithm is _infeasible_ (unless \(=}{{2}}\)) since the learning rate is tuned in terms of the unknown sub-optimality gaps. We not only address this issue with our new learning rate schedule, but also remove the uniqueness assumption. While \(=}{{2}}\) leads to be best bounds in MAB, following (Rouyer and Seldin, 2020), we showcase the importance of other values of \(\) (specifically, \(=}{{3}}\)) in the so-called Decoupled Exploration and Exploitation (DEE-MAB) setting, and again significantly improve their results (see Table 2).

It is worth noting that, as it is common for FTRL-based approaches, our algorithms also automatically adapt to more general corrupted settings (or the so-called _adversarial regime with a self-bounding constraint_(Zimmert and Seldin, 2021)). The complete statements of our results can be found in Section 3.

TechniquesInspired by (Ito, 2021), we decompose the regret into three parts: the regret related to the sub-optimal arms, the regret related to the optimal arms, and the residual regret. Bounding each of them requires new ideas, as discussed below (see Section 4 for details).

To bound the regret related to the sub-optimal arms by a so-called self-bounding quantity (the key to achieve BOBW using FTRL), we design a novel arm-dependent learning rate schedule (which is also our key algorithmic contribution). For example, when using \(\)-Tsallis entropy, this schedule balances the corresponding stability term and penalty term of a sub-optimal arm \(i\) to \((_{t=1}^{T}(p_{i}^{t})^{1-}/_{i}^{t+1})\), which is then bounded by a self-bounding quantity. Apart from removing the uniqueness assumption, as mentioned this learning rate schedule also enables us to achieve the first BOBW guarantees for \(\)-Tsallis entropy with any value of \(\), and also to improve the bound of Ito et al. (2022) for Shannon entropy, which are notable results on their own.

Then, to bound the regret related to the optimal arms, we greatly extend the idea of Ito (2021) that is highly specific to the simple form of \(}{{2}}\)-Tsallis entropy with an arm-independent learning rate. Specifically, we develop a new analysis based on a key observation of a certain monotonicity of Bregman divergences. Such monotonicity only requires two mild conditions on the regularizer that are usually satisfied, allowing us to apply it to a broad spectrum of regularizers.

Our arm-dependent learning rate does make the residual regret much more complicated compared to Ito (2021). To handle it, we carefully consider two cases and show that in both cases it can be related to some self-bounding quantities.

Finally, we note that various places of our analysis require the learner's distribution over arms to be stable in a multiplicative sense between two consecutive rounds. We achieve this by adding an extra small amount of log-barrier, a technique first proposed in Bubeck et al. (2018). While we do not know how to prove the same results without this extra tweak, we conjecture that it is indeed unnecessary.

   Regularizer1  & \(,\) & Regret (w/o uniqueness) & Comments \\  Log-barrier & \(=0\) & Sto. \(( T)\) & First to remove uniqueness \\ \(-_{i}_{i}^{t} p_{i}\) & \(=}\) & Adv. \(()\) & for log-barrier \\  \(\)-Tsallis entropy & \(=\) & Sto. \(( T}{(1-)})\) & First BOBW result \\ \(-_{i}_{i}^{t}p_{i}^{}\) & \(=}\) & Adv. \((}{(1-)}})\) & for \(}{{2}}\) (even with uniqueness) \\  Shannon entropy & \(=1\) & Sto. \((( T)^{2})\) & Improve Ito et al. (2022) \\ \(_{i}_{i}^{t}p_{i}(}{e})\) & \(=}\) & Adv. \(()\) & which defines \(\) as \(K/_{}\) and requires uniqueness2  \\   

Table 1: Overview of our BOBW results for MAB, all achieved via Algorithm 1 and a unified learning rate \(_{i}^{t}=(\{p_{i}^{}, }{{T}}\})^{1-2}}\) for arm \(i\) in round \(t\), with \(p_{i}^{}\) being the probability of picking arm \(i\) in round \(\) and the values of \(\) and \(\) specified in the table. “Sto.” and “Adv.” denote respectively the stochastic and the adversarial setting. \(\) is defined as \(}}+_{i V}}\), where \(U\) is the set of optimal-arms, \(V\) is the set of sub-optimal arms, and \(_{}=_{i V}_{i}\).

Related workFor early results solely for the stochastic setting or solely for the adversarial setting, we refer the readers to the systematic survey in (Lattimore and Szepesvari, 2020). The study of BOBW for MAB starts from the pioneering work of Bubeck and Slivkins (2012), followed by many improvements via different approaches (Seldin and Slivkins, 2014; Auer and Chiang, 2016; Seldin and Lugosi, 2017; Wei and Luo, 2018; Lykouris et al., 2018; Gupta et al., 2019; Zimmert and Seldin, 2019, 2021) and many extensions from MAB to other problems such as semi-bandits (Zimmert et al., 2019), linear bandits (Lee et al., 2021), MAB with feedback graphs (Ito et al., 2022; Erez and Koren, 2021; Rouyer et al., 2022), MAB with switching cost (Rouyer et al., 2021; Amir et al., 2022), model-selection (Pacchiano et al., 2022), partial monitoring (Tsuchiya et al., 2023), and Markov Decision Process (MDP) (Lykouris et al., 2019; Jin and Luo, 2020; Jin et al., 2021; Chen et al., 2021). Among these works, the FTRL-based approach is particularly appealing since it is simple in both the algorithm design and the analysis, and also extends seamlessly to other more general settings (such as the corrupted setting). The uniqueness assumption used to be critical for the analysis of this approach, but plays no role in other methods such as (Auer and Chiang, 2016; Seldin and Lugosi, 2017). Following the first step by (Ito, 2021), our work further demonstrates that this was merely due to the lack of a better analysis (and sometimes a better learning rate schedule). We believe that our techniques shed light on removing the uniqueness assumption for using FTRL in more complicated problems such as semi-bandits and MDPs.

## 2 Preliminaries

In multi-armed bandits (MAB), a learner is given a fixed set of arms \([K]=\{1,2,,K\}\) and has to interact with an environment for \(T K\) rounds. In each round \(t\), the learner chooses an arm \(i^{t}[K]\) while simultaneously the environment decides a loss vector \(^{t}^{K}\). The learner then suffers and observes the loss \(^{t}_{i^{t}}\) of the selected arm for this round. The goal of the learner is to minimize her (pseudo) regret, which measures the difference between her expected cumulative loss and that of the best arm in hindsight. Formally, the regret is defined as \(^{T}=_{t=1}^{T}^{t}_{i^{t}}-_{t=1}^{T} ^{t}_{i^{*}}\), where \(i^{*}*{argmin}_{i[K]}_{t=1}^{T}^{ t}_{i}\) is one of the best arms in hindsight, and \([]\) denotes the expectation with respect to the internal randomness of both the algorithm and the environment.

Adversarial setting versus stochastic settingWe consider two different settings according to how the loss vectors are decided by the environment. In the adversarial setting, the environment decides the loss vectors in an arbitrary way with the knowledge of the learner's algorithm. In this case, the minimax optimal regret is known to be \(()\)(Audibert and Bubeck, 2009).

In the stochastic setting, following prior work such as (Zimmert and Seldin, 2021), we consider a situation much more general than the vanilla i.i.d. case (sometimes called the adversarial regime with a self-bounding constraint). Formally, we assume that the loss vectors satisfy the following condition: there exists a gap vector \(^{K}\) and a constant \(C 0\) such that

\[^{T}[_{t=1}^{T}_{i[K]}[i^{t}=i ]_{i}]-C,\] (1)

where \([i^{t}=i]\) denotes the learner's probability of taking arm \(i\) in round \(t\). This condition subsumes the well-studied i.i.d. setting (discussed in Section 1) where the loss vectors are independently sampled from a fixed but unknown distribution and thus Condition (1) holds with equality, \(C=0\), and \(_{i}=[^{t}_{i}-^{t}_{i^{*}}]\) being the sub-optimality gap of arm \(i\) (independent of \(t\)). In this case, the instance-optimal regret is \((_{i:_{i}>0}})\), achieved by the UCB algorithm (Auer et al., 2002a). More generally, Condition (1) covers the corrupted i.i.d. setting where the loss vectors are first sampled from a fixed distribution and then corrupted by an adversary in an arbitrary way as long as the expected cumulative \(_{}\) distance between the corrupted loss vector and the original one is bounded by \(C[0,T]\). While these two examples both involve iidness, note that Condition (1) itself is much more general and does not necessarily require that.

Uniqueness assumptionPrior work using FTRL to achieve BOBW crucially relies on a uniqueness assumption when analyzing the regret under Condition (1). Specifically, it is assumed that there exists one and only one arm \(\) with \(_{i}=0\). In the special i.i.d. case, this simply means that there exists a unique optimal arm (\(=i^{}\)). Under this uniqueness assumption, the Tsallis-INF algorithm of (Zimmer and Seldin, 2021) achieves \(^{T}=(_{i i}}+}}})\). The recent work (Ito, 2021) takes the first step to remove such an assumption for the Tsallis-INF algorithm and develops a refined analysis with regret bound \((_{i:_{i}>0}}+>0}}}+D+K)\), where \(D\) is such that \([_{t=1}^{T}_{i:_{i}=0}^{t}[_{i} ^{t}-_{i^{}}^{t}]] D\), and \(^{t}[]\) is the conditional expectation with respect to the history before round \(t\). Note that, in the i.i.d. setting, \(D\) is simply \(0\), while in the corrupted setting, \(D\) is at most \(C\). We significantly generalize and improve this result to a borad family of algorithms, and all our results hold _without_ the uniqueness assumption.

We denote by \(U=\{i[K]:_{i}=0\}\) the set of arms with a zero gap, and \(V=[K] U\) the set of arms with a positive gap. In the i.i.d. setting, \(U\) is simply the set of optimal arms and \(V\) is the set of sub-optimal arms. We also define \(_{}=_{i V}_{i}\) to be the minimum nonzero gap.

## 3 Algorithms and Results

The pseudocode of our algorithm is presented in Algorithm 1. It is based on the general FTRL framework, which finds \(p^{t}\), the distribution of selecting arms in around \(t\), via solving the optimization problem \(p^{t}=*{argmin}_{p_{K}} p,_{<t} ^{}+^{t}(p)\). Here, \(_{K}\) is the set of all possible distributions over \(K\) arms, \(^{}\) is an loss estimator for \(^{}\), and \(^{t}\) is a regularizer. The learner then samples arm \(i^{t}\) from the distribution \(p^{t}\) and observes the suffered loss \(_{i^{t}}^{t}\). With this feedback, the algorithm constructs the standard unbiased importance-weighted loss estimator: \(_{i}^{t}=\{i^{t}=i\}_{i}^{t}}{p_{i}^{t}},\  i[K]\), where \(\{\}\) denotes the indicator function.

``` Input: coefficient \(\), learning rate \(\), Tsallis entropy parameter \(\), log-barrier coefficient \(C_{}\), number of arms \(K\), number of rounds \(T\) (not necessary if a doubling trick is applied; see (Ito, 2021, Section 5.3)) for\(t=1,2,,T\)do  Define regularizer \(^{t}(p)=}_{i[K]} p_{i}}_{}+-_{i[K]}_{i}^{t}p_{i},&\\ -_{i[K]}_{i}^{t}p_{i}(p_{i}/e),&\\ _{i[K]}_{i}^{t}p_{i}(p_{i}/e),&\) with learning rate \(_{i}^{t}=(\{p_{i}^{},}{{T}}\})^{1-2}}\).  Compute \(p^{t}=*{argmin}_{p_{K}}\{ p,_{< t}^{}+^{t}(p)\}\) where \(_{K}\) is the simplex. Draw arm \(i^{t} p^{t}\), suffer and observe loss \(_{i^{t}}^{t}\). Construct \(^{t}\) as an unbiased estimator of \(_{t}\) with \(_{i}^{t}=\{i^{t}=i\}_{i}^{t}}{p_{i}^{t}},\  i[K]\). ```

**Algorithm 1** FTRL for BOBW without Uniqueness

We consider three different reuglarizers \(^{t}\): log-barrier, \(\)-Tsallis entropy, and Shannon entropy; see Algorithm 1 for definitions. By now, they are standard reuglarizers used extensively in the MAB literature, each with different useful properties, but there are some small tweaks in our definitions: 1) a linear term (from \(p_{i}(p_{i}/e)=p_{i} p_{i}-p_{i}\)) is added to the canonical form of Shannon entropy, which is critical to ensure a certain type of monotonicity of Bregman divergences (see Section 4); 2) for technical reasons, we also incorporate a small amount of extra log-barrier (with coefficient \(C_{}\)), which ensures multiplicative stability of the algorithm.

More importantly, we propose the following unified arm-dependent learning rate:

\[_{i}^{t}=^{t-1}(\{p_{i}^{}, }{{T}}\})^{1-2}}, i[K],\ t[T],\] (2)where \(\) and \(_{>0}\) are parameters (set differently for different regularizers). The clipping of \(p_{i}^{}\) to \(1/T\) is because \({(p_{i}^{t})}^{1-2}\) itself could be unbounded for \(}{{2}},1]}\) when \(p_{i}^{t}\) is too small. This learning rate is not only conceptually simpler than those in  and important for removing the uniqueness assumption, but also leads to better bounds in some cases as we discuss below.

Main resultsWe now present our main results. Our regret bounds in the stochastic setting are expressed in terms of an instance complexity measure \(=}}+_{i V}}\), which is of the same order as the standard complexity measure \(_{i V}}\) when \(|U|=(1)\) (in particular, this is the case when the uniqueness assumption holds). Similar to , our bounds are also in terms of the constant \(D\) defined in Section2. We start with the following result for the log-barrier regularizer.

**Theorem 3.1**.: _When using the log-barrier regularizer with \(C_{}=162\), \(=0\), and \(=}{{ T}}}\), Algorithm1 ensures \(^{T}=()\) always, and simultaneously the following regret bound when Condition (1) holds: \(^{T}=( T+  T}+}{}+K T+D)\)._

Log-barrier was first used to achieve BOBW in  and later improved in , both of which require the uniqueness assumption. The \(()\) bound for the adversarial setting is almost minimax optimal except for the extra \(\) factor (a common caveat for log-barrier). On the other hand, the bound under Condition (1) matches that of  when uniqueness holds and generalizes it otherwise.2 It is worth noting that this bound (and the same for our other results) suffers an \((D)\) term, which unfortunately can be as large as \(C\), making the bound weaker than those always with only \(\) dependence under the uniqueness assumption. It is unclear to us whether such \((D)\) dependence is necessary when we do not make the uniqueness assumption.

Next, we present our results for Shannon entropy.

**Theorem 3.2**.: _When using the Shannon entropy regularizer with \(C_{}=162 K\), \(=1\), and \(=}{{ T}}}\), Algorithm1 ensures \(^{T}=\) always, and simultaneously the following regret bound under Condition (1): \(^{T}=( T)^{2}+ ( T)^{2}}+K^{2}^{}{{2}}}T+D \)._

The recent work  is the first to discover that Shannon entropy, used in the very first adversarial MAB algorithm EXP3 , in fact also achieves BOBW when configured and analyzed properly (assuming uniqueness). Their results are for the more general setting of MAB with a feedback graph, and when specified to standard MAB, their regret bound under Condition (1) is worse than ours with \(\) defined as \(K/_{}\). The key of our improvement comes from our very different and arm-dependent learning rate schedule. Note that there are extra \( T\) factors in the regret for both the adversarial setting and the stochastic setting, which is also the case in . While this makes the bounds worse compared to other regularizers, in the more general setting with a feedback graph, Shannon entropy is known to be critical for achieving the right dependence on the independence number of the feedback graph, and we believe that our results shed light on how to remove the uniqueness requirement in this more general setting using Shannon entropy.

Finally, we present our results for Tsallis entropy.

**Theorem 3.3**.: _For any \((0,1)\), when using the \(\)-Tsallis entropy regularizer with \(C_{}=\), \(=\), and \(=\), Algorithm1 ensures \(^{T}=KT(  T)^{}{{}}}}\) always, and simultaneously the following regret bound under Condition (1): \(^{T}=( T}{(1-)}+  T}{(1-)}}+D+}{(1- )^{}{{}}}}+)\)._

When \(=}{{2}}\), our learning rate \(_{i}^{t}\) simply becomes \(\) (which is arm-independent), and our algorithm exactly recovers Tsallis-INF . In this case, our result is essentially the same as what the improved analysis of  shows, which does not require uniqueness. For \(}{{2}}\), while such regularizers were also analyzed in (Zimmer and Seldin, 2021) (under uniqueness), their algorithm is infeasible since the learning rates are tuned based on the unknown \(_{i}\)'s. On the other hand, our algorithm not only uses a simple and feasible learning rate schedule, but also works without uniqueness. The bound for the adversarial setting has an extra \(\) factor when \(}{{2}}\) though, which we conjecture can be removed (as it is the case when using a fixed learning rate (Audibert and Bubeck, 2009; Abernethy et al., 2015)); see Remark C.2.2. We find it surprising that our learning rate exhibits totally different behavior when \(<}{{2}}\) versus when \(>}{{2}}\) (recall that \(\) is set to \(\)): in the former, \(_{i}^{t}\) increases in the previous \(p_{i}^{}\) (\(<t\)), while in the latter, it decreases in \(p_{i}^{}\).

It might not be clear at this point what the value is to consider \(}{{2}}\) -- after all, our bounds are minimized when \(=}{{2}}\). It turns out that, however, other values of \(\) play important roles in other problems, as for example demonstrated by Rouyer and Seldin (2020) in a decoupled exploration and exploitation setting. Below, we generalize our results to this setting, showcasing the broad applicability of our techniques.

Decoupled exploration and exploitationThe Decoupled Exploration and Exploitation MAB (DEE-MAB) problem, first considered in (Avner et al., 2012), is a variant of MAB where in each round \(t\), the learner picks an arm \(i^{t}\) to exploit and an arm \(j^{t}\) to explore, and then suffers the loss \(_{i^{t}}^{t}\) while observing the feedback \(_{j^{t}}^{t}\). The performance of the learner is still measured by the same regret definition in terms of the exploitation arms \(i^{1},,i^{T}\). The standard MAB can be seen as a special case where \(i^{t}\) and \(j^{t}\) must be the same. In DEE-MAB, it turns out that the adversarial setting is as difficult as standard MAB with a lower bound \(()\), but one can do much better in the stochastic setting with a _\(T\)-independent_ regret bound. For example, (Rouyer and Seldin, 2020) uses FTRL with \(}{{3}}\)-Tsallis entropy to achieve \(()\) in the adversarial setting and simultaneously \(_{}}}\) in the i.i.d. setting assuming a unique optimal arm.

Using our techniques, we not only remove the uniqueness requirement, but also improve their bounds. Specifically, we consider the exact same algorithm as theirs, which can be described using the framework of Algorithm 1: take \(C_{}=0\), \(=K^{}{{6}}}\), \(=}{{2}}\), and \(=}{{3}}\) for the Tsallis entropy regularizer; sample the exploitation arm \(i^{t}\) according to \(p^{t}\) as before and the exploration arm \(j^{t}\) according to a different distribution \(g^{t}\) with \(g_{i}^{t}p_{i}^{t}^{}{{3}}}\); finally, construct the importance-weighted estimator using the exploration information: \(_{i}^{t}=j^{t}=i}_{i}^{t}}{g_{i}^{t}}\). Our results are as follows.

**Theorem 3.4**.: _For the DEE-MAB problem, the algorithm described above ensures \(^{T}=()\) always, and simultaneously the following regret bound under Condition (1): \(^{T}=^{2}}}+_{i V}^{2}}^{ }{{4}}}+K+D\)._

Note that in the i.i.d. setting (where \(C=D=0\)), we improve their bound from \(_{}}}\) to \(^{2}}}\) (in addition to removing the uniqueness assumption).

## 4 Analysis

In this section, we take \(\)-Tsallis entropy as an example to illustrate the key ideas of our analysis in proving the MAB results under Condition (1). As in all prior work, the analysis relies on a so-called _self-bounding_ technique. Specifically, our goal is to bound the regret as follows (ignoring all minor terms, including the dependence on \(\)):

\[^{T}[_{i V}^{T}p_{i}^{t}}+_{t=1}^{T}p_{i}^{t}} ],\] (3)

where the two terms above enjoy a self-bounding property since they can be related back to the regret under Condition (1). To see this, we apply AM-GM inequality followed by Condition (1) to show the following for any \(z 0\):

\[[_{i V}^{T}p_{i}^{t}} ][_{i V}(}+z _{t=1}^{T}p_{i}^{t}_{i})] z(^{T}+C )+_{i V}},\] \[[_{t=1}^{T}p_{i}^{ t}}][}}+z_{i V} _{t=1}^{T}p_{i}^{t}_{}] z(^{T}+C )+}}.\]

Rearranging and picking the optimal \(z\) yields the regret bound under Condition (1) in Theorem3.3.

The key is thus to prove Eq.3, which is not that difficult under uniqueness (when \(|V|=K-1\)) but turns out to be much more complicated without uniqueness. To proceed, we start with some key concepts and ideas from  which we follow. First, define the _skewed Bregman divergence_ for two time steps \(s,t\) as

\[D^{s,t}(x,y)=^{s}(x)-^{t}(y)-^{t}(y),x-y,\] (4)

and its variant restricted to any subset \([K]\) as \(D_{}^{s,t}(x,y)=_{}^{s}(x)-_{}^{t}( y)-_{}^{t}(y),x-y,\) where \(_{}^{t}(x)=-C_{}_{i} x _{i}-_{i}_{i}^{t}_{i}^{}\) (that is, \(^{t}\) restricted to \(\)). The standard Bregman divergence associated with \(^{t}\), which we denote by \(D^{t}(x,y)\), is then a shorthand for \(D^{t,t}(x,y)\). One key idea of  is to carefully choose the right benchmark for the algorithm -- when there is a unique optimal arm, the benchmark basically has to be this unique optimal arm, but when multiple optimal arms exist, the benchmark can now be any distribution over these arms, and it can even be varying over time. Indeed, for round \(t\), the following benchmark was used in :

\[q^{t}=*{argmin}_{p_{U}}\{ p,_{ <t}^{}+^{t}(p)\}=*{ argmin}_{p_{U}}D^{t}(p,p^{t}),\] (5)

which follows the same definition of \(p^{t}\) but is restricted to \(_{U}=\{p_{K}:_{i U}p_{i}=1\}\), the set of distributions over the zero-gap arms. As the second equality shows, \(q^{t}\) is also the projection of \(p^{t}\) onto \(_{U}\) w.r.t. the Bregman divergence \(D^{t}\). With these time-varying benchmarks, Ito  proves

\[^{T}[_{t=1}^{T}D^{t,t+1}(p^{t},p^{t+1})-D _{U}^{t,t+1}(q^{t},q^{t+1})]+D.\] (6)

The rest of the analysis is where we start to deviate from that of  (thought still largely inspired by it), which is critical for our algorithms that use arm-dependent learning rates. First, we introduce an important intermediate point \(^{t+1}=^{t+1}_{U}+^{t+1}_{V}\) where \(^{t+1}_{U}\) and \(^{t+1}_{V}\) are defined as

\[^{t+1}_{U}= *{argmin}_{x^{K}_{  0},\ _{i V}x_{i}=0,\\ _{i V}x_{i}=_{i U}p^{t}_{i}} x,_{  t}^{}+^{t+1}_{U}(x),\] (7) \[^{t+1}_{V}= *{argmin}_{x^{K}_{  0},\ _{i V}x_{i}=0,\\ _{i V}x_{i}=_{i V}p^{t}_{i}} x,_{  t}^{}+^{t+1}_{V}(x).\]

By definition, \(^{t+1}\) is obtained from \(p^{t}\) by redistributing the weights among arms in \(U\) and those in \(V\), in a way that minimizes an FTRL objective similar to that of \(p^{t+1}\). We note that Ito  also uses the same \(^{t+1}_{U}\) is his analysis, but we introduce \(^{t+1}_{V}\) (and thus \(^{t+1}\)) as well since it importantly allows us to decompose each Bregman divergence difference term in Eq.6 as follows.

**Lemma 4.1**.: _For any \(t\), \(D^{t,t+1}(p^{t},p^{t+1})-D_{U}^{t,t+1}(q^{t},q^{t+1})\) is bounded by_

\[^{t,t+1}(p^{t},^{t+1})}_{}+ ^{t,t+1}(p^{t},^{t+1})-D_{U}^{t,t+1}(q^{t},q^{t+1})}_{ }+(^{t+1},p^{t+1})}_{ }.\] (8)

In the rest of this section, we proceed to bound each of the three terms in Eq.44 (see also Table3 for a summary of bounds for each of these terms and each of the three regularizers).

Regret on Sub-Optimal ArmsThe regret related to the sub-optimal arms (or more formally arms in \(V\)) is the most straightforward to deal with, since our objective is to arrive at the self-bounding terms in Eq.3 which are exactly only in terms of arms in \(V\). Indeed, we can write this term as (with \(p_{V}^{t}\) being the vector with the same value as \(p^{t}\) for coordinates in \(V\) and \(0\) for coordinates in \(U\))

\[D_{V}^{t,t+1}(p^{t},^{t+1})=^{ t}-_{V}^{t+1},^{}-D_{V}^{t}(^{ t+1},p^{t})}_{}+^{t}(^{t+1})-_{V}^{t+1} (^{t+1})}_{},\] (9)

and then apply standard arguments to show that the stability term is of order \(_{i V}(p_{i}^{t})^{1-}/_{i}^{t+1}\) while the penalty term is of order \(_{i V}(_{i}^{t+1}-_{i}^{t})(p_{i}^{t})^{}/ \!_{1-}\). In the Tsallis-INF algorithm, we have \(=}{{2}}\) and \(_{i}^{t}=\), and thus the stability term and the penalty term are of the same order. This inspires us to design a learning rate for general \(\) with the same objective. Indeed, it can be verified that our particular learning rate schedule makes sure that the penalty term is of the same order as the stability term, meaning \(D_{V}^{t,t+1}(p^{t},^{t+1})=(_{i V}(p_{i}^{ t})^{1-}/_{i}^{t+1})\). Further plugging in the learning rate and summing over \(t\), we arrive at the following with \(z_{i}^{t}=\{p_{i}^{t},}{{T}}\}\):

\[_{t=1}^{T}_{i V}^{t})^{1-}}{ _{i}^{t+1}}}_{i V}_{t=1}^{T} ^{t})^{1-}}{^{t}(z_{i}^{k} )^{1-2}}}.\] (10)

Finally, applying the following technical lemma shows that the above is of the same order as the first term in our objective Eq.3. More details can be found in SectionA.2.

**Lemma 4.2**.: _Let \(\{x_{t}\}_{t=1}^{T}\) be a sequence with \(x_{t}>0\) for all \(t\). Then, for any \(\), we have_

\[_{t=1}^{T}^{1-}}{^{t}x_{s}^{1-2}} }(^{T}x_{t})(1+_ {t=1}^{T}x_{t}^{1-2})}).\] (11)

Regret on Optimal ArmsNext, we show that the regret on optimal arms (or more formally arms in \(U\)), \(D_{U}^{t,t+1}(p^{t},^{t+1})-D_{U}^{t,t+1}(q^{t},q^{t+1})\), is nonpositive, which corresponds to the intuition that pulling optimal arms incur no regret. Ito  proves something similar for \(}{{2}}\)-Tsallis entropy via a certain monotonicity property of Bregman divergence, but his proof is highly specific to \(}{{2}}\)-Tsallis entropy. Instead, we develop the following general monotonicity theorem which applies to a broad spectrum of regularizers as long as they satisfy two mild conditions.

**Theorem 4.3** (Monotonicity of Bregman divergence).: _For any \(t\), let \(f^{t}:\) be a continuously-differentiable and strictly-convex function defined on \(\). Suppose that the following two conditions hold for all \(z\): (i) \((f^{t})^{}(z)\) is differentiable and concave; (ii) \((f^{t+1})^{}(z)(f^{t})^{}(z)\). Then, for any \(x,m\) with \(x m\), and \(y,n\) such that \((f^{t+1})^{}(y)-(f^{t})^{}(x)=(f^{t+1})^{}(n)-(f^{t})^{ }(m)=\) for a fixed scalar \(\), we have \(D^{t,t+1}(x,y) D^{t,t+1}(m,n)\), where \(D^{t,t+1}(u,v)=f^{t}(u)-f^{t+1}(v)-(u-v)(f^{t+1})^{}(v)\) is the skewed Bregman divergence._

While we state the theorem for the one-dimensional case, it trivially extends to multi-dimensional regularizers as long as they decompose over the coordinates (which is the case for all our regularizers).

Take Tsallis entropy as an example: we only need to apply the theorem with \(f^{t}(z)=-^{t}z^{}}{1-}\) for each \(i\) and then sum up the conclusions. The two conditions stated in the theorem also hold for all regularizers we consider. In particular, Condition2 holds as long as the learning rate \(_{i}^{t}\) is non-decreasing in \(t\) and the regularizer itself is non-increasing (thus with nonpositive first derivative). This explains the additional linear term in our definition of Shannon entropy: this way it is strictly decreasing. Note that Condition2 also trivially holds if \(f^{t}\) is independent of \(t\), in which case the theorem states the monotonicity for the standard (non-skewed) Bregman divergence.

After verifying the conditions, we can now apply this theorem to show \(D_{U}^{t,t+1}(p^{t},^{t+1}) D_{U}^{t,t+1}(q^{t},q^{t+1})\). For each \(i U\), we take \(x=p_{i}^{t}\) and \(m=q_{i}^{t}\). Since by definition \(q^{t}\) is obtained by projecting \(p^{t}\) onto \(_{U}\), it can be shown via KKT conditions that \(p_{i}^{t} q_{i}^{t}\) indeed holds for all \(i U\). Then, we define an intermediate point \(z\) such that \(_{U}^{t+1}(z)-_{U}^{t}(p^{t})=_{U}^{t+1}(q^{t+1} )-_{U}^{t}(q^{t})\) and show \(D_{U}^{t,t+1}(p^{t},^{t+1}) D_{U}^{t,t+1}(p^{t},z)\). Finally, taking \(y=z_{i}\) and \(n=q_{i}^{t+1}\) and applying Theorem4.3 finishes the proof; see SectionA.3 for details.

Residual RegretFinally, bounding the residual regret \(D^{t+1}(^{t+1},p^{t+1})\) by the self-bounding terms in Eq.3 that are only in terms of arms in \(V\) is another key challenge in our analysis, especially given the arm-dependent learning rates. We start by developing a new analysis that leads to tighter bounds compared to  on the Lagrangian multipliers associated with Eq.7, which reveals that the key to analyze \(D^{t+1}(^{t+1},p^{t+1})\) is to bound the following term (or terms of a similar form)

\[^{t})^{2-}}{ _{i}^{t}})(_{i U}^{t})^{3-2 }}{(_{i[K]}^{t})^{2-}}{_{i }^{t}})(_{i U}^{t})^{2-}}{ _{i}^{t}})})}{(_{i[K]}^{t})^{2- }}{_{i}^{t}})(_{i U}^{t})^ {2-}}{_{i}^{t}})}.\] (12)

Again, for the case of \(}{{2}}\)-Tsallis entropy with an arm-independent learning rate \(_{i}^{t}=\), removing all dependence on \(i U\) in Eq.12 is relatively straightforward as shown by Ito . Indeed, in this case, Eq.12 simplifies to \(}(p_{i}^{t})^{3/2}) (_{i U}(p_{i}^{t})^{2})}{(_{i[K]} (p_{i}^{t})^{3/2})(_{i U}(p_{i}^{t})^{ 3/2})}\). By splitting \(_{i[K]}(p_{i}^{t})^{}{{2}}}\) into two summations, one over \(i V\) and another over \(i U\), and further applying \(x+y x^{}{{3}}}y^{}{{3}}}\) for any \(x,y>0\), we have \(_{i[K]}(p_{i}^{t})^{}{{2}}}(_{i  V}(p_{i}^{t})^{}{{2}}})^{}{{ 3}}}(_{i U}(p_{i}^{t})^{}{{2}}})^{ }{{3}}}\) and thus Eq.12 is bounded by

\[}(p_{i}^{t})^{3/2}) (_{i U}(p_{i}^{t})^{2})}{(_{i V} (p_{i}^{t})^{3/2})^{}{{3}}}}}(_{i V}(p_{i}^{t})^{}{{2}}})^{ }{{3}}}^{t}}}{},\]

where importantly, in the second step we use the fact \(\|x\|_{2}\|x\|_{}{{2}}}\) to drop all dependence on \(i U\), eventually arriving at the self-bounding term of Eq.3.

Unfortunately, with an arm-dependent learning rate, it is unclear to us how to analyze Eq.12 in a similar way. Instead, we propose a different analysis with the following rough idea: we propose a condition under which Eq.12 can be bounded by \(_{i V}(p_{i}^{t})^{2-}}{{_{i}^{t}}}\) and then further related to a self-bounding term similarly to the analysis of the regret on sub-optimal arms. If, on the other hand, the condition does not hold, then we show that the probability \(p_{i}^{t}\) of selecting an optimal arm \(i U\) must be no more than the total probability of selecting sub-optimal arms \(_{j V}p_{j}^{t}\). Using this fact again allows us to convert dependence on \(i U\) to \(i V\). Since we apply this technique to all \(i U\), it leads to the extra \(|U|\) factor in the second self-bounding term of Eq.3, which eventually translates to \(}\) in our instance complexity \(\). All details can be found in SectionA.4.

## 5 Conclusions

In this work, we improve and generalize the analysis of , showing that many FTRL algorithms can achieve BOBW without the uniqueness assumption. Specifically, we propose a unified arm-dependent learning rate schedule and novel analytical techniques to remove the uniqueness assumption for a broad family of regularizers, including log-barrier, \(\)-Tsallis entropy, and Shannon entropy. With these new techniques, our regret bounds improve upon prior results even when the uniqueness assumption holds. We further apply our results to the decoupled exploration and exploitation setting, showing that our techniques are broadly applicable.

There are many natural future directions, including (1) removing the \(}}\) term in our regret bounds; (2) improving the dependence on \(D\) (which as mentioned could be as large as the corruption level \(C\)); (3) understanding whether the extra the log-barrier regularizer is necessary or not; (4) and finally generalizing our results to other problems such as semi-bandits and Markov Decision Processes.