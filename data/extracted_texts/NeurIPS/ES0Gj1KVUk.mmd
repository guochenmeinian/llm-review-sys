# Data subsampling for Poisson regression

with \(p\)th-root-link

 Han Cheng Lie

Institut fur Mathematik

Universitat Potsdam

Germany

hanlie@uni-potsdam.de &Alexander Munteanu

Department of Statistics

TU Dortmund University

Germany

alexander.munteanu@tu-dortmund.de

###### Abstract

We develop and analyze data subsampling techniques for Poisson regression, the standard model for count data \(y\). In particular, we consider the Poisson generalized linear model with ID- and square root-link functions. We consider the method of _coresets_, which are small weighted subsets that approximate the loss function of Poisson regression up to a factor of \(1\). We show \((n)\) lower bounds against coresets for Poisson regression that continue to hold against arbitrary data reduction techniques up to logarithmic factors. By introducing a novel complexity parameter and a domain shifting approach, we show that sublinear coresets with \(1\) approximation guarantee exist when the complexity parameter is small. In particular, the dependence on the number of input points can be reduced to polylogarithmic. We show that the dependence on other input parameters can also be bounded sublinearly, though not always logarithmically. In particular, we show that the square root-link admits an \(O((y_{}))\) dependence, where \(y_{}\) denotes the largest count presented in the data, while the ID-link requires a \((/(y_{})})\) dependence. As an auxiliary result for proving the tightness of the bound with respect to \(y_{}\) in the case of the ID-link, we show an improved bound on the principal branch of the Lambert \(W_{0}\) function, which may be of independent interest. We further show the limitations of our analysis when \(p\)th degree root-link functions for \(p 3\) are considered, which indicate that other analytical or computational methods would be required if such a generalization is even possible.

## 1 Introduction

Random sampling is arguably one of the most popular approaches to reduce large amounts of data to save memory, runtime, and further downstream resources such as communication bandwidth and energy. In contrast, classic statistical learning theory often uses uniform sampling and provides only asymptotic approximation guarantees. These guarantees often require strict assumptions such as i.i.d. data and for model assumptions to be met exactly. However, data collected from real applications often violate these conditions: only finite samples are available, independence might not be satisfied, and the model may deviate from reality. When model fitting algorithms are applied to such data, we are not only interested in reducing the above-mentioned resource requirements, but also in providing rigorous worst-case guarantees on approximation.

Arguably, the most popular approach is the Sensitivity Framework , which provides a general-purpose importance sampling scheme that yields a weighted subsample -- or _coreset_ -- that given a data matrix \(X\) approximates some loss function \(f(X)\) within a factor \((1)\) for any query point \(\). This guarantee can be stated as follows: a significantly smaller subset \(K X,k:=|K||X|\)together with corresponding weights \(w^{k}\) is a \((1)\)-coreset for \(X\) if it satisfies

\[^{d}|f(X)-f_{w}(K)|  f(X).\] (1)

We point the interested reader to  for a gentle introduction and overview on coresets.

Our aim is of course not only to obtain good approximation accuracy as stated in Equation (1), but also for the subsample achieving the bound to be sublinear in the input size. Unfortunately, most generalized linear models do not admit strongly sublinear data summaries with reasonable approximation guarantees . This also holds for the Poisson models considered in this paper.

To go beyond the worst-case setting and enable meaningful data reduction, a natural approach is to parameterize the analysis with a quantity that captures the fit of data to the statistical model and quantifies the achievable size of succinct data summaries . Another ingredient that is commonly used to tackle data reduction for generalized linear models is to relate their loss to \(_{p}\) norms, for which \(_{p}\) sensitivities or leverage scores yield viable importance sampling distributions .

### Our contributions

We provide the first rigorous analysis for \((1)\)-approximate data reduction for Poisson models:

1. We show \((n)\) lower bounds against coresets for Poisson regression (Lemma 6.1), showing that changing the link function alone does not resolve the problem of bounding the complexity of coresets for the log-link, which is incompressible. Our lower bound extends to arbitrary data reduction techniques up to a \((n)\) factor (Lemma 6.2).
2. We introduce a novel complexity parameter \(\) that captures the compressibility of data under a Poisson \(p\)th-root-link model (Equation (2)). This parameter corresponds naturally to the statistical model assumptions and establishes a relationship between these assumptions and an optimization perspective of the compressibility problem.
3. We conduct a parameterized analysis, showing that sublinear coresets exist under the statistically natural assumption of small \(\) parameter (Theorem 3.8), and using a novel domain shift idea for their optimization (Theorem 4.2).
4. We prove a square root upper bound for the Lambert \(W_{0}\) function over \([-1/e,0)\) (Lemma 6.3) that allows us to prove tight bounds for the slope of a linear lower envelope (Lemma 6.4). This justifies an \((})\)1 dependence for the ID-link, which is contrasted by an \(O((y_{}))\) dependence for the square root-link. 5. We show the limitation of our domain shifting approach, showing that the error of this method cannot be bounded to give the required \((1+)\) approximation for \(p 3\) (Lemma 6.5). This indicates a limitation to the common choices \(p\{1,2\}\), and suggests that different techniques than the ones we develop below may be needed to overcome this limitation.

### Our techniques

The general outline of our analysis follows the established method of sensitivity sampling . Several steps along this outline require novel ideas due to peculiarities of the Poisson loss function defined in Equations (3) and (4) below. A VC dimension bound of \(d^{2}\) is easy to obtain by counting the number of arithmetic operations required to compare an individual loss to a given threshold as a measure of complexity . A near-linear \((d/)\) bound is obtained by a more fine-grained analysis, by grouping and rounding the associated count data (respectively, sensitivity scores) to powers of \((1+)\) (resp. to powers of \(2\)). This results in a surrogate loss function that admits group-wise linear VC dimension, by splitting the domain of each loss function at its unique global minimum into two regions such that the restriction of the function to each region is monotone, and connecting the resulting construction to hyperplane classifiers. Approximating the surrogate finally implies the desired \((1)\) approximation for the original loss as well.

For bounding the sensitivities, the domain of the loss function \(g(z)\) is split into three intervals: 1) one interval consisting of'moderate' values of \(z\), such that the \(g(z)\) values are bounded above and below within constants and can be treated using simple uniform sampling; 2) one interval where \(z\) has large values, in which case \(g(z)\) is closely related to \(z^{p}\); 3) one interval, where \(z\) is close to \(0\), in which case the negative logarithm dominates the loss.

Tackling the interval in 2) requires relating the loss function \(g_{y}(z)\) to \(z^{p}\). Specifically we would like to bound \(z^{p} g_{y}(z) z^{p}/\) for sufficiently large \(z\) and for some value of \(\). This requires special care, since the loss function \(g_{y}(z)\) is translated polynomially towards larger values of \(z\) with growing \(y\), but the minimum of \(g_{y}(z)\) grows only logarithmically with \(y\). Informally speaking, the loss function 'widens', and its minimum'moves mainly to the right', so for large \(y\), we would need a very flat lower bound, which requires large \( y\) (ignoring polylogarithmic terms). However, this is undesirable, since \(\) appears to be a crucial parameter for bounding the subsample size. Specifically, this would yield a \(_{i[n]}y_{i}=(n)\) dependence in the coreset size. So instead, we bound the loss roughly as \(z^{p} g_{y}(z)(z-y^{1/p})^{p}/\), which amounts to translating the lower envelope with growing \(y\) as well. Additionally, we introduce a novel complexity parameter \(\) which balances the translated \((z-y^{1/p})^{p}\) lower bound with the \(z^{p}\) upper bound. We stress that these translation and balancing arguments are not artificial or just used to make calculations go through, but they are naturally consistent with the statistical model (see the discussion below Equation (2)). This proof strategy eventually captures the loss function within interval 2) more closely and yields sublinear bounds for \(\) as well.

We remark that in contrast to \(\)-complexity in previous work [36; 34], a bounded balancing complexity parameter \(\) does _not_ handle the asymmetry between intervals 2) and 3). Tackling the interval in 3) thus also requires completely new ideas, as the negative logarithm has an infinite asymptote at \(0\), which we exploit to prove \((n)\) lower bounds on subsample size in Lemma 6.1 and Lemma 6.2. Such asymptotes have not been mentioned or analyzed in previous related work on sensitivity sampling for GLMs. To circumvent the lower bound, we avoid this interval by introducing a novel _domain shifting_ approach, requiring all feasible solutions to satisfy \(z>\) for a suitable \(>0\) for optimization. Choosing \(\) in the order of \(\), we can argue that the solution in the shifted domain is a \((1+)\) approximation. Avoiding the asymptote enables a coreset construction for the shifted domain.

We believe that the domain shifting approach is necessary: if an instance consists of the extreme points on the convex hull, and all but a small (sublinear) number of points are separated by an \(\) distance to the boundary, then required structure is already in the data. But if non-extremal points are allowed to be arbitrarily close to the boundary, and we do not shift the domain, then we will not avoid high sensitivity points that are strictly inside the convex hull. Then the coreset size would necessarily depend on the distance of these non-extremal points to the boundary, and crucially on the number of points that are very close to the boundary of the convex hull, which again can be \((n)\).

Exploiting the asymmetry between the intervals 2) and 3) where the loss exhibits \(z^{p}\) and \(-(z)\) growth respectively, we prove \((n)\) lower bounds on subsample size, by adapting known reduction techniques [31; 36; 41]. We also provide lower bounds on parameters used in our analysis, showing their tightness. In particular, the aforementioned \(\) slope parameter is of size \(()\) in the case \(p=1\). The proof is conducted by an exact characterization of the tangent point between our linear lower envelope and the loss function. Since this requires balancing between \(z\) and \((z)\) and examining the \((y!)\) function, further bounds rely on Stirling's approximation and the principal branch of the Lambert \(W_{0}\) function. Recent bounds in  for the Lambert \(W_{0}\) function imply our square root upper bound but only a cubic root lower bound. We thus significantly tighten their bound in an appropriate interval. This result may be of independent interest, since the Lambert function cannot be expressed in terms of elementary functions and has many important applications in various fields. As a result, we obtain a matching square root lower bound on \(\) in our context.

### Related work

Classic work on data subsampling started with linear \(_{2}\) regression  and was extended to linear \(_{p}\) regression . More recently, the study continued with non-linear transformations such as in generalized linear regression models. The first guaranteed finite subsample bounds for logistic regression appeared in , while impossibility results for Poisson regression were given in . Research on generalized linear models was continued for Probit regression . Asymptotic properties of subsampling for generalized linear models, including for Poisson regression, were studied in [3; 27]. A finite-sample size result is given in [3; Theorem 5] that exhibits \(O()\) approximation error. studied a sampling-based feature space reduction for a wide array of generalized linear models with additive errors. However, parts of their assumptions specifically do not apply to Poisson regression.

## 2 Preliminaries and the Poisson \(p\)th-root-link model

Poisson regression models aim to predict a count variable \(Y_{0}\) using a generalized linear model with link function \(h:\), i.e.,

\[h((Y x))=x\,,\]

where \(x=(1,x^{(1)},,x^{(d-1)})^{d}\) is a _row_ vector, and \(^{d}\) is a column vector carrying the model parameters that in particular include an intercept \(_{1}\). Common choices for \(h\) are the canonical log-link \(h(v)=(v)\), the ID-link \(h(v)=v,\) and the root-link \(h(v)=v^{1/2}\). The latter two can be cast into a general framework by introducing the \(p\)th-root-link \(h(v)=v^{1/p}\), for any \( p 1\), where the ID-link and root-link correspond to \(p\{1,2\}\).

Subsampling for the log-link is not possible with the multiplicative \((1+)\) error guarantees that we aim for, since it entails preserving the \((x)\) function . We will also show impossibility results for the \(p\)th-root-link. However, we parameterize our analysis with a data-dependent parameter that reflects naturally how well the realized data distribution is captured by the Poisson model. This parameter is inspired from previous work : we will refer to data \(X,y\) as being '\(\)-complex', if there exists a \(0<<\) such that denoting by \(x_{j}\) the \(j\)-th row of the data matrix \(X^{n d}\) and by \(y_{j}\) the \(j\)-th entry of the vector \(y_{0}^{n}\), it holds that

\[_{^{d}}^{n}|x_{j}|^{p}}{_{j=1}^ {n}|x_{j}-y_{j}^{1/p}|^{p}}.\] (2)

We may interpret the parameter \(\) as follows. The rate parameter of the predicted Poisson distribution is \((Y x)=(x)^{p}\). Hence, the mean and variance of \(Y_{j}\) given \(x_{j}\) is \((x_{j})^{p}\), for each \(j=1,,n\). To obtain the maximum likelihood estimator of \(\), we seek \(\) such that for every \(j=1,,n,x_{j}\) is as close as possible to \(y_{j}^{1/p}\), since \(y_{j}^{1/p}\) minimizes the \(j\)th summand \(g_{y_{j}}\) of the loss function specified in Equations (3) and (4) below. However, choosing \(\) so that \(|x_{j}-y_{j}^{1/p}|\) is small will imply that each summand in the numerator of Equation (2) will be close to \(y_{j}^{1/p}\). In this case, the variance of the \((Y_{j})_{j=1}^{n}\) will not be captured effectively by the Poisson model, and \(\) will be large. Thus, smaller values of \(\), i.e., values of \(\) that are closer to 1, indicate that the true data distribution is better captured by the Poisson model. Thus the \(\) parameter in Equation (2) plays a similar role of quantifying model fit as the \(_{w}(X)\) parameter from [36, Section 2]; see in particular the comments at the end of that section.

Assuming that the value of \(\) is small allows us to use the proximity of the negative log-likelihood to \(_{p}\) norms, together with some novel optimization ideas involving a shifted domain. This yields the first provable finite and sublinear subsample size with rigorous \((1+)\) approximation guarantee. We focus on the special cases \(p\{1,2\}\) since they are the most popular (in fact the only practical) alternatives to the intractable log-link [10; 31]. The ID-link has been used in epidemiology [40; 29]. The root-link function has been applied to forecasting for queueing systems  and to account for misspecification bias in maximum likelihood estimation . When the estimated mean count of the data is zero, then the canonical log-link causes problems that can be avoided by using the root-link; see e.g. [28, Section 5.4]. We also discuss in our lower bounds section other choices for \(p\), and show that for any natural number \(p 3\) the bound implied by our novel shifting idea must fail. This bound is crucial to obtain our final approximation, indicating that other methods would be required to tackle a generalization for \(p 3\), if this is even possible.

Given parameters \(\) and an input \(x\), the rate parameter of the predicted Poisson distribution is

\[(Y x)=(x)^{p},\]

which corresponds to its mean and variance. Its probability mass function is \((Y=y)=(y x)=e^{-}}{y!}= {(x)^{p^{y}}e^{-(x)^{p}}}{y!}\). Given a set of i.i.d. observations expressed as the rows \(x_{i}\) of a data matrix \(X^{n d}\) with corresponding labels \(y_{0}^{n}\) we can obtain a maximum likelihood estimate of the parameter \(\) by minimizing the negative log-likelihood, which takes the form

\[f_{y}(X)_{i=1}^{n}g_{y_{i}}(x_{i})= _{i=1}^{n}(x_{i})^{p}-py_{i}(x_{i})+(y_{i}!)\] (3)where

\[g_{y_{i}}(x_{i})(x_{i})^{p}-py_{i}(x_{i})+(y_{i}!).\] (4)

For any \(p\)th-root-link, the loss function includes a \((x)\) term, which restricts the feasible set to all \(\) such that for all \(x_{i},i[n]\) it holds that \(x_{i}>0\). We note that for summands corresponding to \(y_{i}=0\), the function \(g_{0}(z)\) simplifies to \(g_{0}(z)=z^{p}>0\) with well-known properties of the \(_{p}\) norm. We thus focus on summands \(g_{y_{i}}\) for \(y_{i}\) below.

For arbitrary \(y\), the function \(g_{y}(z)=z^{p}-py(z)+(y)\) on \(_{>0}\) is strictly convex with first and second derivatives \(g^{}_{y}(z)=pz^{p-1}-\) and \(g^{}_{y}(z)=p(p-1)z^{p-2}+}>0\) respectively. Thus \(g_{y}(z)\) decreases on the interval \(z(0,y^{1/p})\), increases on \(z(y^{1/p},)\), and has a unique minimizer at \(z^{*}=y^{1/p}\) with corresponding value \(y-y(y)+(y!)\ (y)+(1) 1\) by Stirling's approximation. We shall use the following lower bounds to capture the \(y\)-dependence.

**Lemma 2.1**.: _It holds for all \(z_{>0},p[1,),y\) that_

\[g_{y}(z)=z^{p}-py(z)+(y!)\{1,(1+p(z)) \}.\]

The next two results bound the individual loss contributions from above and below by roughly a value of \(z^{p}\). For the lower bound, however, we note that as the value of \(y\) grows, the loss function is translated polynomially towards larger \(z\) values, since its minimum is attained at \(z^{*}=y^{1/p}\). However, by Lemma 2.1, and the properties above, the increase of \(g_{y}(z^{*})\) is only logarithmic in \(z^{*}\), and thus also logarithmic in \(y\). Denote by \(\) the scaling parameter of the lower bound on \(g_{y}\) given in Lemma 2.2. Then the logarithmic growth of \(g_{y}(z^{*})\) implies that we would need \( y/(y)\). Unfortunately, the value of \(\) will affect the coreset size, which is undesirable, since it can become linear simply due to large values of \(y\). We thus require a sublinear dependence on \(y_{}\), i.e., the largest value of \(y\) presented in the data. To this end, we shift the lower envelope by the minimizer \(z^{*}=y^{1/p}\). The value of \(\) can subsequently be bounded in a desirable way, but differs significantly depending on the value of \(p\): in the case \(p=1\) we prove \( O()\) to be sufficient, while the case \(p=2\) even constant \(=1\) will suffice. In Section 6, we will show a separation by a superconstant and matching square root lower bound on the value of \(\) in the dominating case \(p=1\).

**Lemma 2.2**.: _For any \(p 1\) and \(y\) it holds that \(z^{p} g_{y}(z)\) for \(z>y^{1/p}\). If \(p=1\), then for some \( O()\), it holds that \(g_{y}(z))^{p}}{}\) for \(z>y^{1/p}\)._

**Lemma 2.3**.: _Let \(p 2\) and \(y\), and \(=1\). Then \(g_{y}(z))^{p}}{}\) for \(z>y^{1/p}\)._

## 3 Coreset construction

We begin by summarizing some key aspects of the sensitivity framework. Formal definitions for the sensitivity framework (including sensitivities, the VC dimension, and the main subsampling theorem) are given in Appendix A. In the sensitivity framework, the goal is to obtain coresets using importance sampling techniques to approximate loss functions . Given a loss function whose value depends on a collection of input points, the main idea of the framework is to measure the sensitivity of any input point in terms of its worst-case contribution to the loss function. More precisely, given a point \(x_{j}\), its sensitivity for the loss function of the form \(f(X)=_{j[n]}g(x_{j})\) is given by

\[_{j}=_{})}{f(X)}.\]

The main subsampling theorem, Proposition A.5, combines the sensitivities together with the theory of VC dimension. The idea is to sample points according to probabilities that are proportional to the sensitivities, in order to create an appropriately reweighted subsample of the initial collection of points. Suppose the total sensitivity \(S=_{j[n]}_{j}\) of the points and the VC dimension \(\) associated with a set system based on the summands \(g(x_{i})\) in the loss function \(f(X)\) are bounded, and choose a failure probability \(\). If the subsample is of size \(k=O(}((S)+()))\), then it is in fact a \((1+)\)-coreset  with probability at least \(1-\). Unfortunately, it is often just as difficult to compute the exact sensitivities as it is to solve the original problem. The remedy is to exploit the fact that one does not need the exact sensitivities themselves: it suffices to use any upper bounds on the sensitivities, provided that the upper bounds are not too loose, since a larger upper bound will lead to a larger coreset. Thus, we can reduce the task of coreset construction to two tasks: control of the VC dimension and sensitivity estimation of the loss function. This is handled in the following sections.

### Bounding the VC dimension

We prove two different bounds on the VC dimension. The first one is a simple quadratic bound of \(O(d^{2})\). Our proof in the appendix simply counts the number of operations required to compare the loss function to a given threshold. The VC dimension bound then follows from a standard result in the context of bounding the VC dimension of neural networks [4, Thm. 8.14].

**Lemma 3.1**.: _The VC dimension of the range space associated with the class of Poisson loss functions as in Equation (3) is bounded by \((_{^{*}}) O(d^{2})\)._

It is noteworthy that the quadratic dependence is implied already only from one single application of the exponential function, which is sufficient but likely not necessary in our context. Hence, we show in the remainder a more refined near-linear bound of \(O((n)(y_{}))=(d/)\), while keeping the dependence on other input parameters--namely, on \(n\) and \(y_{}\)--logarithmic.

To this end, we subdivide the set of input functions into groups of growing values of their response parameter \(y_{i}\), and of their sensitivity \(_{i}\) in a geometric progression. By rounding these values in each group to their next power in the geometric progression, we obtain disjoint sets of functions that closely approximate the original weighted loss functions, and whose VC-dimension can be bounded in \(O(d)\). Since there is only a logarithmic number of groups in both progressions, we obtain the claimed VC dimension bound.

Recall the responses \((y_{i})_{i}\) are nonnegative integers. We define their largest value (for a given input) to be \(y_{}=\{y_{i}\ |\ i[n]\}\). Therefore, they are naturally bounded between \(0 y_{i} y_{}\) for all \(i[n]\) and there are at most \(y_{}+1\) different values of \(y_{i}\). Also note that the sensitivity values are naturally bounded by \(0_{i} 1\), but since they are continuous, they must be bounded away from \(0\) in order for the geometric progression to end in a finite (logarithmic) number of steps. If we increase each sensitivity by \(1/n\) then the total sensitivity grows only by a constant, since \(S^{}=_{i[n]}(_{i}+1/n)=S+1\). For these reasons, we can thus assume that \(1/n_{i} 1\).

Now, we would like to increase the sensitivities even more to their next power of \(2\), which will clearly increase the total sensitivity by no more than

\[2S^{}=2S+2 3S.\] (5)

By our above upper and lower bounds on the sensitivities, this will result in \(O((n))\) groups, where in each group all sensitivities are equal. Note that in Proposition A.5, the reweighting of points depends only on fixed terms except for the sensitivities. Thus, in each group, all weights are constant. We have the following bound that applies to each group and for any fixed \(y_{0}\).

**Lemma 3.2**.: _The VC dimension of the range space induced by the set of functions \(_{c}=\{g_{i}()=c g_{y_{i}}(x_{i})\ |\ i[n]\}\) with equal weight \(c_{ 0}\), and equal \(y_{i}=y_{0}\) for all \(i[n]\) satisfies \((_{_{c}})=O(d)\)._

For the values of \(y_{i}\), we proceed in a very similar way. However, unlike the sensitivities, a constant approximation as in Equation (5) provided by powers of \(2\) will not suffice. Instead, we group the values of \(y_{i}\) into powers of \((1+)\) and round all \(y_{i}\) that belong to the same group to the next larger power. We argue that this preserves a \((1 O())\) approximation to the original loss function. Indeed, this claim even holds for each summand \(g_{y_{i}}\) if \(y_{i}\) is large enough, as the following lemma shows.

**Lemma 3.3**.: _Let \(y 8\) and \(1>0\). Let \(y<y^{}(1+)y_{i}\). Then for arbitrary \(z>0\) it holds that \((1-3)g_{y}(z) g_{y^{}}(z)(1+3)g_{y}(z)\)._

A direct consequence of Lemma 3.3 is that any coreset for the rounded version is a coreset for the original loss function and vice versa, up to an additional \((1 O())\) error. We can therefore work with the rounded version of the loss function, which yields better bounds for the VC dimension.

A general theorem for bounding the VC dimension of the union or intersection of \(t\) range spaces, each of bounded VC dimension at most \(D\), was given in . Their result yields \(O(tD(t))\). Here, we give a bound of \(O(tD)\) for the special case that the range spaces are disjoint2.

**Lemma 3.4**.: _Let \(\) be any family of functions, and let \(F_{1},,F_{t}\) be nonempty sets that form a partition of \(\), i.e., their disjoint union satisfies \(_{i[t]}=\). Let the VC dimension of the range space induced by \(F_{i}\) be bounded by \(D\) for all \(i[t]\). Then the VC dimension of the range space induced by \(\) satisfies \((_{}) tD\)._

As a result of our previous partition into groups and the \(O(d)\) bound on each group, we obtain the desired result.

**Lemma 3.5**.: _Let \(\) be the set of functions in the Poisson model. We can round and group the values of \(y_{i}\) and the associated sensitivities \(_{i}\) to obtain \(^{*}\) such that each function in \(^{*}\) is weighted by \(0<w_{i} W:=\{u_{1},,u_{t}\}\) for \(t O(^{-1}(n)(y_{}))\). The range space induced by \(^{*}\) satisfies \((_{^{*}}) O((n)(y_{}))\)._

As a direct corollary of Lemmas 3.1 and 3.5, we obtain the following combined bound.

**Corollary 3.6**.: _The VC dimension \((_{^{*}})\) of the range space associated with the class of Poisson loss functions as in Equation (3) is bounded by_

\[(_{^{*}}) O(d\{ d,)}{}\}).\]

### Bounding the sensitivities

We split the loss function into two parts:

\[f_{y}(X)_{i:x,}g_{y_{i}}(x_{i})+_{i:x,>}g_{y_{i}}(x_{i})\] (6)

We will ignore the first sum, since we will see later in the main approximation of Section 4, that by shifting the hyperplanes defined by parameter vectors in the solution space, everything can be shifted to the second sum where \(x_{i}\). In this way, we preserve a \((1+)\)-approximation, if \(=()\) is small enough. We note that this shifting technique still requires the extreme points on the convex hull \((X)\) to be maintained; we address this issue in Section 5. We will focus on bounding the sensitivities for the remaining points with \(x_{i}\). In the next lemma we require the concept of a well-conditioned-basis . Let \(q\{2,\}\) denote the dual norm of \(p\{1,2\}\), respectively. We say that \(U\) is a '\((,,p)\)-well-conditioned basis' for the column span of \(X=UR\) if \(U^{n d}\) satisfies

\[\|U\|_{p},, z^{d}:\|z\|_{q}\|Uz \|_{p}.\] (7)

**Lemma 3.7**.: _Let \(X^{n d},y_{0}^{n}\) be a \(\)-complex dataset, i.e., Equation (2) holds. Let \(p\{1,2\}\). Let \( 1\) be the slope parameter from either Lemma 2.2 or Lemma 2.3 depending on the value of \(p\). Let \(\) be a conditioning parameter and \(>0\) be arbitrary. Then the sensitivity for each \(x_{i}\) with \(x_{i}>\) is bounded by \(_{i}^{p}\|U_{i}\|_{p}^{1}+2/n\). Their total sensitivity is bounded by \( O( d/(y_{})}+(1/))\) for \(p=1\), and \( O( d+(y_{})+(1/))\) for \(p=2\)._

### Combining the results into the sensitivity framework

Putting all steps (VC dimension, total sensitivity) together into the sensitivity framework, Proposition A.5 yields the following computational result, where in particular \(_{p}\) well-conditioning is established constructively using \(_{2}\) subspace embeddings , resp. using \(_{1}\) spanning sets .

**Theorem 3.8**.: _Let \(X^{n d},y_{0}^{n}\) be a \(\)-complex dataset, i.e., Equation (2) holds. We can compute a weighted coreset \((K,w)^{k d}_{ 0}^{k}\) for the \(p\)th-root-link Poisson regression problem with \(p\{1,2\}\) on \(D()\{^{d}\ :\  i,\ x_{i}>\}\). The size of the coreset is bounded by \(k=(^{-2}d\{d,^{-1}(n)(y_{} )\} m)\), where_

\[m= d(d)/(y_{})}+(1/ )&p=1\\  d+(y_{})+(1/)&p=2.\]Main approximation result

In the previous section, we developed a coreset for the sum of individual losses where \(x_{i}\), i.e., for the second sum of Equation (6). Since we cannot bound the remaining first sum where \(x_{i}<\), we choose to simply avoid it instead, by shifting each solution by \(\). Define for any \( 0\)

\[D()\{^{d}\;:\; i,\;x_{i}>\}.@note{ footnote}{Such domain restrictions do not lead to feasibility issues, as discussed in Appendix F due to page limitations.}\]

The original domain of optimization is \(D(0)\) and the shifted domain is \(D()\). Shifting the domain does not remove the need to store the extreme points on the convex hull of the input, since we need these points to determine the feasible domain \(D()\) during optimization. However, shifting removes the need to approximate the first sum in Equation (6) over points with unbounded sensitivity located in a small slab of width \(\) within the convex hull.

Since we have a \((1)\) coreset for \(D()\), we need to find a suitable choice for \(\) and show that the optimizer \((^{})^{*} D()\) is a \((1+O())\) approximation for the optimizer in the original domain \(^{*} D(0)\). We thus define

\[^{*}_{^{} D()}f(X^{ }),^{*}_{ D(0)}f(X).\] (8)

**Lemma 4.1**.: _It holds for sufficiently small \(>0\) that_

\[f(X^{*}) f(X^{*})(1+O())f(X^{*}).\]

Now we combine the preceding results, namely the coreset and the domain shifting bound, for our main theorem.

**Theorem 4.2**.: _Let \((0,1/14)\). Let \((C,w)\) be a coreset according to Theorem 3.8. Let \(:=*{argmin}_{ D()}f_{w}(C)\), \(^{*}:=*{argmin}_{ D(0)}f(X)\). Then_

\[f(X^{*}) f(X)(1+)f(X^{*}).\]

## 5 Extreme points on the convex hull

In the previous sections, we argued that for obtaining a \((1+)\) approximation, it suffices to calculate a coreset that is valid for all \( D()\), and then to minimize our loss function over \( D()\) using the coreset instead of the full data. Note that for the optimizer to stay in the feasible set \(D()\), one must store the extreme points on the convex hull of the input points denoted by \((X)\). This is true even when \(=0\), i.e., even when the original function is considered and no shifting occurs. Note that there exist datasets such as our 'points on a circle' example considered in the lower bounds of Section 6, such that \(|(X)|=n\).

There are several ways to either characterize \(|(X)|\) for _typical_ inputs in a sublinear way, or to approximate the convex hull by a smaller sublinear subset, called an \(\)-kernel, with an error of at most \(\). Since these methods are usually relative to the diameter of the data, and since we need an additive error for our shifting approach, we first normalize the data to be within the unit ball. We note that this does not change the value of the loss function, since this involves scaling by a fixed value \(C 1\), and since we can use the fact that

\[f(X)=f(()(C)),\]

as well as the one-to-one correspondence between any \(\) and \(C\). Thus, we can run the algorithm on the rescaled data, obtain a good or optimal solution \(C^{*}\), and rescale \(C^{*}\) to obtain the corresponding \(^{*}\). Rescaling steps such as normalizing data to zero mean and unit variance are standard in statistical data analysis .

**Smoothed complexity of the convex hull** Instead of the worst case \(|(X)|\) over all possible datasets \(X^{n d}\), in smoothed complexity we consider

\[_{X^{n d}}_{}| (X+)|,\]where \(\) denotes the distribution over matrices \(\) with the same dimensions as \(X\) and with i.i.d. Gaussian entries with mean \(0\) and variance \(^{2}\), i.e., \(_{i,j} N(0,^{2})\) for all \(i[n],j[d]\). This is motivated by the fact that many datasets are recorded with measurement errors, which can often be assumed to be Gaussian. Specifically for the convex hull, [11, Chapter 4] showed that for normalized data in the unit cube, the supremum above is bounded by

\[O(d-1}(n)}{^{d}}+^{d-1}(n)),\]

which is sublinear in \(n\), though exponential in \(d\).

\(\)**-kernels** The purpose of \(\)-kernels is to approximate the extent of a point set up to an error of \(1-\) for any direction in \(^{d}\) based on a subset of the data. They were introduced by  and improved by  to optimal \((1/^{(d-1)/2})\) size, see the survey . Since we assume our data to be normalized within the unit ball, this translates to an additive error that is bounded by \(\). Thus, the boundary of the convex hull of the \(\)-kernel can be smaller than the boundary of the original convex hull by at most \(\).

**Improvement for structured data** It is known that \(\)-kernels can have size up to \((1/^{(d-1)/2})\) in the worst case. Beyond the worst case, the structure of the given data may allow for much smaller \(\)-kernels to exist, even in high dimensions. Motivated by this,  developed a 'greedy clustering' approach that produces an \(\)-kernel of size \(O(k_{}/^{2})\), where \(k_{}=k_{}(X,)\) denotes the smallest possible size of a subset that gives the required \(\)-kernel guarantee for the original input \(X\).

**Using \(\)-kernels for optimizing Poisson models** The above options include the possibility to calculate the convex hull and rely on the sublinear smoothed complexity bound, if this is reasonable in the given context. Otherwise, if we have access to any of the above \(\)-kernel constructions, we shift the hyperplane away from the approximation of the convex hull, provided by the \(\)-kernel, by a distance of \(=2\) instead of just \(\). As a result, the hyperplane will be shifted away from the original convex hull by at least \(\) and at most \(2\). Thus, we still compute a \((1+())\) approximation in this way.

## 6 Lower bounds

We complement our coreset constructions for the variants of the Poisson model by a series of lower bounds. Our bounds in Lemma 6.1 and Lemma 6.2 are specifically for the \(p\)th-root-link, and not for the log-link, which was studied in . We use similar constructions of the bad dataset as those used frequently in previous literature, e.g., in [31, Theorem 6], and in . However, each of these references require specific adaptations to the respective loss function that do not directly apply to our setting. Our arguments are thus adapted to the Poisson \(p\)th-root-link model to show that it does not admit sublinear coresets without imposing assumptions on the data or restricting the model.

The hard instance consists of \(n\) equidistant points on the unit circle. Recall that in the Poisson regression formulation, every point has an additional intercept coordinate which is \(1\), and the corresponding parameter of \(\) determines the affine translation; see Section 2. For every \(i[n]\), let \(x_{i}=(1,(),())\), and \(y_{i}=1\). Recall that any feasible \(\) must satisfy \(x_{i}>0, i[n]\). The hyperplane parameterized by \(\) is thus always outside the point set and \(\) points in the direction of the \((x_{i})_{i[n]}\).

The idea for showing that any point has sensitivity \(1\) is that if \(\) points to the center of the point set, and the hyperplane is translated to just 'touch' point \(x_{i}\), then \(x_{i}\) is arbitrarily close to \(0\), implying that the cost is arbitrarily large. All other points are sufficiently bounded away from the hyperplane, but also not too far away, so their cost is bounded. This implies that the sensitivity of point \(x_{i}\) can be made arbitrarily close to \(1\). By symmetry of our construction, this holds for any point.

**Lemma 6.1**.: _Consider a number \(n 8\) of points equidistant on a unit circle in a 2-dimensional affine subspace embedded in \(^{d},d 3\), each with label \(y_{i}=1\). Then the sensitivity of each point for the Poisson model with \(p\{1,2\}\) is arbitrarily close to \(1\). Consequently, any coreset for the Poisson regression model must comprise all \((n)\) input points._

Below, we prove an even more interesting statement, i.e., that no compression is possible below \((n)\)_bits_. While this statement appears to give a weaker \((n/(n))\) bound against coresets, it in fact gives a stronger bound in some sense. This is because the bound holds against _any_ possible data reduction algorithm and against _any_ data structure that answers negative log-likelihood queriesto within a small error, independent of what (possibly randomized) operations the data reduction algorithm performs. For example, the algorithm could subsample, it could select input points as coreset constructions do, or it could take linear combinations as in linear sketching. More generally, the bound holds against any sort of bit encoding that represents the reduced data. The reduction is based on the same data example of equidistant points on a circle embedded in \(d 3\) dimensions.

**Lemma 6.2**.: _Let \(_{D}\) be a data structure for \(D=[X,y]^{n d}^{n}\), \(d 3\), that approximates negative log-likelihood queries \(_{D}()\) for Poisson regression with the \(p\)th-root-link for \(p\{1,2\}\), such that for some \( 1\) it holds that_

\[^{d}:f(X)_{D}() f (X).\]

_If \(<\) then \(_{D}\) requires \((n)\) bits of memory._

Next, we prove that the parts of our analysis that are specific to the Poisson model are tight. In particular, the scale parameter \(\) is only a constant for \(p=2\), but in the case \(p=1\) we only have a \(\) upper bound. Since \(y_{}\), i.e., the largest \(y\), can potentially be very large, one may ask if we can do better. Our next result exactly characterizes the smallest possible parameter \(\) such that our linear lower envelope approximation is tangent to the actual loss function, in order to show a tight \(=()\) bound. This characterization of tangent points relies on properties of the principal branch \(W_{0}\) of the Lambert function, which is defined by the equation

\[W_{0}(x)e^{W_{0}(x)}=x,x-1/e.\]

The only approximations that the final bounds obey follow from Stirling's approximation and from the following upper bound on the Lambert function \(W_{0}\) by a square root function. This upper bound improves the recent cubic root bound of [38, Theorem 3.2] within a small region, and is crucial to obtaining a tight square root (rather than cubic root) lower bound on \(\).

**Lemma 6.3**.: _For all \(x[-1/e,0)\), it holds that \(W_{0}(x)-1\)._

The above bound on the \(W_{0}\) function is novel and may be of independent interest. In our context, it allows us to prove the following tight bound on \(\) that resembles the same asymptotic upper bound as in Lemma 2.2, and establishes a matching lower bound.

**Lemma 6.4**.: _Let \(y\) be arbitrary, \(p=1\), and \(=y^{1/p}\) in the definition Equation (4) of \(g_{y}\). Let \(h_{}(z))^{p}}{}\) for \(z>0\). Then \(g_{y}\) and \(h_{}\) are tangent to each other if and only if \(=^{*}(y)=(W_{0}((2)})+1)^{-1}\), in which case the unique tangent point is \(z^{*}(y)=(y)}{^{*}(y)-1}\). In addition, \(^{*}(y)=(/(y_{})})\)._

The next lemma shows for \(p 3\) that there exists no constant \(C\) such that the domain shifting approach that we developed in Section 4 yields a \(1+C\) error bound. As this error bound is a crucial sufficient condition for our main approximation results given in Lemma 4.1 and Theorem 4.2 to hold, the lemma suggests that different techniques may be needed. It indicates a limitation of our analysis to the most common values \(p\{1,2\}\), which are the main parameterizations considered in our work.

**Lemma 6.5**.: _Let \(p\), \(p 3\). Then there does not exist an absolute constant \(C 0\) such that for all sufficiently small \(>0\) and for all \( D(0)\), \(^{}+ e_{1} D()\) satisfies_

\[f(X^{}) f(X)+^{p}n+ Cf(X).\] (9)

## 7 Concluding remarks

In Section 1.3, we recalled that previous finite sample size results had either unbounded or \(O()\) error instead of our \((1+)\) approximation. Our lower bounds on the parameters, together with linear VC dimension, linear sensitivity, and linear \(\) dependence in our main quantitative bounds of Theorem 3.8, leave no room for improvement (up to polylogarithmic factors) if one uses a black-box application of the sensitivity framework. We remark that recent improvements on \(_{p}\) sensitivity sampling  suggest that the dimension dependence can be improved to linear as well. Exploiting the fact that our coreset gives a guarantee for all \( D()\), it would be interesting to extend the statistical treatment to the Bayesian setting, inferring the distribution of parameters over this (sub-)domain, as was recently accomplished in the case of logistic and probit regression .