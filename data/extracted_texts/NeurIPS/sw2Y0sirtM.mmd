# A Unified, Scalable Framework for

Neural Population Decoding

 Mehdi Azabou\({}^{1,}\)1, Vinam Arora\({}^{1}\), Venkataramana Ganesh\({}^{1}\), Ximeng Mao\({}^{2,3}\),

**Santosh Nachimuthu\({}^{1}\), Michael J. Mendelson\({}^{1}\), Blake Richards\({}^{2,4}\), Matthew G. Perich\({}^{2,3}\), Guillaume Lajoie\({}^{2,3}\), Eva L. Dyer\({}^{1,}\)1**

\({}^{1}\) Georgia Tech, \({}^{2}\) Mila, \({}^{3}\) Universite de Montreal, \({}^{4}\) McGill University

Contact: {mazabou,evadyer}@gatech.edu. Project page and code: https://poyo-brain.github.io

###### Abstract

Our ability to use deep learning approaches to decipher neural activity would likely benefit from greater scale, in terms of both model size and datasets. However, the integration of many neural recordings into one unified model is challenging, as each recording contains the activity of different neurons from different individual animals. In this paper, we introduce a training framework and architecture designed to model the population dynamics of neural activity across diverse, large-scale neural recordings. Our method first tokenizes individual spikes within the dataset to build an efficient representation of neural events that captures the fine temporal structure of neural activity. We then employ cross-attention and a PerceiverIO backbone to further construct a latent tokenization of neural population activities. Utilizing this architecture and training framework, we construct a large-scale multi-session model trained on large datasets from seven nonhuman primates, spanning over 158 different sessions of recording from over 27,373 neural units and over 100 hours of recordings. In a number of different tasks, we demonstrate that our pretrained model can be rapidly adapted to new, unseen sessions with unspecified neuron correspondence, enabling few-shot performance with minimal labels. This work presents a powerful new approach for building deep learning tools to analyze neural data and stakes out a clear path to training at scale.

## 1 Introduction

Recent advances in machine learning, particularly in the context of large-scale pretrained models like GPT , have showcased the immense potential of scaling up, both the terms of the size of datasets and models . Similarly, in neuroscience, there is a growing need for a foundational model that can bridge the gaps between diverse datasets, experiments, and individuals, allowing for a more holistic understanding of brain function and information processing . The development of such a model would allow researchers to uncover underlying patterns and interactions within neural populations , and potentially allow for more robust decoding of brain states.

However, creating a large-scale neural decoding model that can effectively combine spiking datasets from various sources is a complex challenge . One of the central challenges is the lack of a shared "vocabulary" in neural recordings. Unlike the case for text--wherein every document written in a given language shares a basic lexicon for tokenization--there is no one-to-one correspondence between neurons in different individuals. As such, every recording from a different individual involves a unique set of neurons that cannot be easily aligned with another set. An additional core challenge lies in the inherent variability of neuron sets observed across different days . Even when monitoring the same individual, variability in electrode/tissue interface can lead to distinctneuron sets which, despite advanced sorting methods, can lead to inconsistencies in input channels across sessions [11; 12; 9]. Overall, this lack of correspondence across recordings complicates the integration of information from different experiments and individuals, ultimately hampering efforts to construct a unified perspective on population-level interactions and dynamics in the brain.

In response to these challenges, we propose a new framework for large-scale training on neural spiking data called POYO (**P**re-training **O**n man**Y** neur**O**ns).2 This framework is designed to enable scalable and efficient training across multiple sessions of neural recordings, even when spanning different sets of neurons with no known correspondence. Our approach centers around a novel tokenization scheme that transforms individual neural action potentials, or "spikes", into discrete tokens, thereby preserving the neural code's finest temporal structure while simultaneously enhancing computational efficiency. The resulting tokenization not only allows for a more effective representation of neural activity but also paves the way for training on larger volumes of data. We combine this input tokenization method with an architecture that builds on the PerceiverIO  to compress the input spikes into a latent space and learns interactions across spikes in time and across neurons.

We evaluate the performance of our proposed approach on data from over 158 sessions from open electrophysiology datasets from seven non-human primates (NHPs), spanning over 27,373 units and 100 hours of recordings. We demonstrate that through pretraining on large amounts of data, we can transfer with very few samples (few-shot learning) and thus improve overall brain decoding performance. Our work not only presents an innovative framework for training large models on neuroscience datasets, but also offers insights into the scaling laws that govern decoding from neural populations. By enabling the development of large pretrained models for neural decoding, our approach advances the field of brain-machine interfaces and other decoding applications.

The main contributions of this work include:

* _A framework for large-scale training on neural recordings:_ We present a novel framework for training transformer models end-to-end on multi-session and across-individual electrophysiology datasets derived from neural populations, enabling efficient and effective decoding from a diverse range of neural recordings.
* _Innovative spike-based tokenization strategies:_ We introduce a fundamentally different way to tokenize neural population activity. Our approach tokenizes individual spikes (events) across neural populations, preserving fine temporal structure and enhancing computational efficiency by adopting a sparse representation of the data.
* _Pre-trained models for neural decoding:_ We build two large pretrained models (POYO-1, POYO-mp) that can be fine-tuned on new sessions and across recordings from different animals and new behavioral tasks. We will make the weights and code available, and provide both pretrained models as a resource to the community.

## 2 Approach

The transformer architecture , originally introduced in the context of natural language processing (NLP), has shown remarkable flexibility and effectiveness in various domains, especially in the presence of large and diverse datasets [15; 3]. In this work, we explore how to leverage this versatility in the neural data domain.

### Tokenizing neural population activity

Neurons communicate asynchronously using electrical impulses called spikes. The timing and frequency of spikes encode signals that convey information about the external world and coordinate the internal dialogue within the brain. In many neuroscience experiments, neural activity is recorded from the brain through multiple electrodes, and then processed  to extract the spiking events for a set of neural "units" .3 The resulting data are multi-variate event-based sequences that are typically very sparse relative to the number of recorded time points.

Neurons and their spikes are of course not independent of one another. Rather, they are part of a complex, interwoven tapestry of neural activity, with each spike contributing to a collective dialogue across billions of neurons. The true challenge and opportunity lie in interpreting these spikes not in isolation, but in the context of this broader "conversation."

When trying to scale up and train on many sessions of data, we are faced with the challenge of having recordings of neural conversations with completely different set of "speakers" for which we don't know the identity or their functional tuning (or what they respond to). This is because each time we record from the brain, we are tuning into a conversation between a new set of speakers. However, as with language, there is some reason to think that neurons are ultimately communicating on similar topics, e.g. sensations from the external world, internal states of the body, muscle commands, etc. In other words, the lexicon may be different, but everyone is talking about similar subjects. Despite the variability of our data, our aim is to decipher these "conversations" in a way that generalizes to new neural datasets without fixed or known correspondence across their inputs.

The neural tokenizer.Based upon these motivations, we propose a novel approach for tokenizing neural population activity where _each spike is represented as a token_ (see Figure 1). In this case, each token can be defined in terms of _which unit it came from_ (via a learnable embedding) and the time that the spike event was detected. By representing our data in this way, we never define or fix the expected number of units, and the model can ingest populations of arbitrary size, and thus can be trained across many datasets. At the same time, this approach also avoids having to specify a specific time resolution for the neural code, as is the case with binning, and gets rid of the accumulation of a lot of sparse tokens containing no events.

More concretely, we assign each unit a unique identifier and a corresponding \(D\)-dimensional learnable embedding. Let \(()\) denote a lookup table that associates each unit to its unit embedding. Akin to word embeddings that encode semantic meaning and relationship between words, the unit embedding space encodes something about the "speaker's" identity and role in neural computation. A unit fires a sequence of spikes in a context window of time \([0,T]\). Each spike will be represented by a token characterized by \((_{i},t_{i})\), where

\[_{i}=(s\;unit})\]

is the learned embedding associated with the unit that emitted the spike and \(t_{i}\) is the event timestamp.

Collectively, and for an arbitrary set of units, we combine all the spikes into a sequence of length \(M\). The neural population activity is represented by \([_{1},,_{M}]\) and their corresponding times \([t_{1},,t_{M}]\). While the size of our context window \(T\) stays fixed, \(M\) will vary depending on the number of units in the population and their firing rates (see Figure 1). Note that all spikes from a specific unit have the same unit embedding, and only differ in their timing.

Figure 1: _Overview of our approach._ Input spike tokens are compressed into a smaller set of latent tokens which are processed through multiple self-attention blocks, finally time-varying outputs are predicted by querying the latent space. All tokens in this model are assigned a timestamp, which are used towards rotary position encoding.

### Building the latent space

**Compressing the input sequence.** Rather than processing the input sequences with self-attention, which is quadratic in sequence length and can be expensive for long sequences, we use a perceiver encoder module  that summarizes the input sequence into a shortened "latent" sequence. Let's consider a sequence of \(M\) input tokens \(=[_{1},,_{M}]\) and a sequence of learned latent tokens \(_{0}=[_{0,1},,_{0,N}]\), where \(_{0,i}^{D}\) and \(N M\). We use cross-attention to pull information from the input sequence into a compressed latent sequence. The queries \(=_{q}_{0}\) are a projection of the learned latent tokens \(_{0}\) and the keys and values are a projection of the input tokens: \(=_{k}\) and \(=_{v}\), respectively. The cross-attention operation can be expressed as follows:

\[_{1}(,\,,\, )=(^{T}}{ }}).\] (1)

We use the standard transformer block with pre-normalization layers and feed-forward nets.

**Attention in the latent space.** Following the compression of the input sequence through cross-attention, we apply multiple self-attention blocks on the latent token sequence, which now costs \(O(N^{2})\) instead of \(O(M^{2})\) with \(N M\). Let \(_{l}\) denote the embedding of the latent tokens at the \(l\)th layer. The self-attention operation can be expressed as follows:

\[_{l+1}(,\,,\, )=(^{T}}{}})\] (2)

We denote the final latent sequence obtained after \(L\) layers as \(_{L}=[_{L,1},,_{L,N}]\).

### Encoding relative timing information

To incorporate timing information, we leverage rotary position encoding (RoPE)  across all attention layers in the architecture. Unlike traditional position encoding methods that inject absolute position information into the token's embedding, RoPE applies a rotation operation in the query, key embedding space, allowing each token in a sequence to attend to others based on its relative positional information.

Recall that each input token \(i\) has an associated timestamp \(t_{i}\). We will also assign a timestamp to each of the latent tokens: we divide the latent tokens into groups of equal size and spread each group uniformly over the context window \([0,T]\). This method allows us to capture temporal relationships between the latent tokens and the input tokens, enabling a temporal understanding of the encoded sequence. By distributing the latent tokens evenly within the context window \([0,T]\), we create a structured temporal representation, which preserves and propagates temporal information all the way through the model. A detailed formulation of the relative encoding can be found in Appendix A.1.

### Querying the latent space

Having built a latent space which can encode and model any population of units, we now want a flexible way to readout behavioral variables. The output of the latent encoder is the latent sequence of size \(N\), while the desired output can be any arbitrary sequence of length \(P\). Let us consider the task of hand velocity decoding for example, in the context window \([0,T]\), the length of the output sequence will depend on the sampling frequency. Since we aspire to train on datasets sourced from various labs, the sampling rate can differ significantly. We thus need a flexible mechanism for predicting outputs of varying lengths and querying from the neural activity at specific points in time.

We define a sequence of output tokens \(_{0}=[_{0,1},,_{0,P}]\), where \(P\) is the number of output time points, which can change across sequences. Each output token is defined by \((_{0,i},t_{i}^{out})\). Initially all the output tokens within a session are set to the same learned embedding \(_{0,i}=_{0}^{D}\)\( i\). The output \(\) is obtained by querying the latent space through cross-attention:

\[(,\,,\,)= (^{T}}{}} )\] (3)Since we use rotatary embeddings, and both the latent and output tokens have assigned timestamps, the querying mechanism can leverage the relative position of latent and output tokens to extract the temporally relevant context that enable prediction of the behavioral variable of interest.

Session embeddings.To account for variability of the experimental setups in real world settings, we propose to define a learnable session embedding which captures the hidden experimental variables. This information is injected into the output query \(_{0}\), where again we use a lookup table to register each new session we encounter. To illustrate how we probe the model to generate an output, we can think about it as asking a question of the form:

_"Predict [BEHAVIOR] at time [TIMESTAMP] under the experimental conditions of [SESSION ID]"._

This produces a set of output tokens of the dimension of the number of queries which we then pass through an MLP to generate the behavioral variables of the desired dimension (e.g., 2D for hand velocities in a planar movement task).

### Unit identification in new sessions

Our design of the unit embedding space allows our model to learn latent information about the units it encounters, as well as capture the relationship between units in the population. Given a new recording with unidentified units, we can transfer our model by mapping these new units into the unit embedding space. To do this, we introduce an approach that we call "unit identification", which leverages gradient descent to learn the embeddings of new units. In this approach, we freeze all existing weights of the model and simply add new rows to the \(()\) lookup table for each of the new units, as well as a new session embedding. Notably, the bulk of our model which maps the neural population activity to behavior is unchanged and is simply transferred to the new dataset. In our experiments, we find that this approach is surprisingly effective and allows us to rapidly integrate new datasets into the same underlying model. Further details and analysis on the robustness of this approach are presented in Appendix C.2.

## 3 Experiments

In this section, we demonstrate the promise of our approach for large-scale training and examine the benefit of scaling in neural population decoding.

### Datasets and experiment setup

One of the key advantages of our approach is its ability to scale to handle large amounts of neural data, including sessions from different numbers of neurons, across different tasks and recording setups, and from different animals. Thus we set out to build a diverse dataset large enough to test our approach. We curated a multi-lab dataset with electrophysiological recordings from motor cortical regions, where neural population activity has been extensively studied , and deep learning tools and benchmarks have recently been established . In total, we aggregated 178 sessions worth of data, spanning 29,453 units from the primary motor (M1), premotor (PMd), and primary somatosensory (S1) regions in the cortex of 9 nonhuman primates (see Table 1). We place this in the context of standard analyses within a single lab or paper which typically involve 10's of sessions and a few hundred neurons.

All of these neural recordings were collected while the animals performed various motor tasks that vary in their inherent complexity (see Figure 2A-B). The center-out (CO) task is relatively stereotyped, with the animal making a reach to one of eight targets after receiving a go cue, and

**Study** & **Regions** & **\# Indiv** & **\# Sess** & **\# Units** & **\# In** & **\# Out** & **Tasks** \\  Perich et al.  & M1, PMd & 4 & 117 & 11,557 & 143M & 20M & CO, RT \\ Churchland et al.  & M1 & 2 & 9 & 1,728 & 706M & 87M & CO \\ Makin et al.  & M1, S1 & 2 & 47 & 14,899 & 123M & 15M & RT \\ Flint et al.  & M1 & 1 & 5 & 957 & 7.9M & 0.3M & CO \\ NLB-Maze  & M1 & 1 & 1 & 182 & 3.6M & 6.8M & Maze \\ NLB-RTT  & M1 & 1 & 1 & 130 & 1.5M & 2.8M & RT \\  

Table 1: _Datasets used in this work._ CO: Center-Out, RT: Random Target.

then returning to the center. In contrast, the random target (RT) task is significantly more complex. The animals make continuous and self-paced movements with new targets appearing in succession at random locations in the workspace. In addition to the greater exploration of the workspace and heterogeneity in movements, this task allows individual to plan their next movement while finishing the execution of the current movement leading to greater complexity in neural dynamics. Other sources of variability that we find across labs include the: choice of pre-processing algorithms (spike sorting or threshold crossing analysis), type of controller (manipulandum, touch screen), and sampling rate when recording behavior (100Hz to 1kHz). We do not re-process the data or attempt to standardize it across labs or tasks. Further details on the datasets are provided in Appendix B.

**Experiment setup.** Throughout all of our experiments, we use a context window of \(1s\) and do not segment data into trials during training. We only use the trial structure when reporting the decoding performance, in particular, center-out sessions are evaluated during the reaching movement. We train the model with \(N=512\) latent tokens and a dimension \(D=128\). We use the LAMB optimizer , and employ a cosine decay of the learning rate at the end of training. For every session, we holdout \(20\%\) of the trials for testing, and \(10\%\) for validation. We use 1-GPU and 8-GPU setups for single-session and multi-session models, respectively. All details can be found in Appendix A.

### Testing the model on single sessions

To first investigate the performance of our architecture when trained on a single session, we trained single-session models on 100 different recording sessions acquired from three nonhuman primates (Monkey C, Monkey M, Monkey Ja), each containing anywhere from 10 to 106 minutes of data . In all of these recordings, the same behavioral task setup, behavioral recording apparatus (10 ms temporal resolution), and spike sorting procedure was used.

Across all 100 single-session models, we obtained an average R2 of 0.9347 on CO and 0.8402 for RT sessions. When we compared these single-session results with existing models for neural decoding, including a Wiener Filter, GRU and MLP , we found that our approach consistently outperforms these baselines, with even greater improvements observed on the RT task. Additionally, we found that our single-session models are stable for a wide range of hyperparameters (Appendix C.3).

### Poyq-mp: Building a large pretrained across-animal, multi-session model

To investigate the question of how training with more sessions of data can improve brain decoding, we trained a large model (POYO-mp, 24 layers) on all 100 sessions that we studied in our single-session analysis. In total, we used 9,789 units and 4,367 neuron-hours (number of neurons \(\) amount of time recorded) to train our POYO-mp model.

Figure 2: _Building a multi-session model spanning multiple animals and tasks._ (A) Center-out reach and (B) Random target task , along with examples of true and predicted behavior (x,y velocities). In (C-D), we show the decoding performance for single-session models (gray) and the POYO-mp multi-session model (green).

[MISSING_PAGE_FAIL:7]

of our model and its flexibility to accommodate fresh data with even a simple input mapping (unit identification).

We compare with the single session model and a GRU baseline (Figure 4A) with our multi-session model trained on the same number of trials. Both our single-session and finetuning approach achieve good performance scaling as we increase the number of samples, with the finetuning maintaining a gap over the single session models over the entire sampling space.

**Results on a new animal performing the same tasks.** Next we tested our model on 12 sessions from a completely new animal (6 CO sessions, 6 RT sessions from Monkey T) that was not included in the training of the model (Table 2, Right). When we applied our unit identification approach on the multi-session model (POY0-mp + Unit ID), we see a bit of a dip in overall accuracy from the single-session model (POY0-[Single-session]); however, when we fine-tune the weights of the model further, we achieve an accuracy of 0.9379 on CO, which is a significant improvement over all of the other baselines. For the RT task, the single session model is 0.7569 and with unit identification we achieve 0.7669 and with full fine-tuning we get up to 0.7916 (Figure 4B). These results are promising and show that we can use our pretrained model on new animals with only a few minutes of labeled data.

**Performance on the Neural Latents Benchmark (NLB).** To understand how well our pre-trained model performs on data collected from new animals performing novel tasks with different equipment (example: touch screen vs. manipulandum), we applied our pretrained model to the MC-Maze (Monkey L) and MC-RTT (Monkey I) datasets from the NLB (Table 3). The NLB serves as a benchmark for neural representation learning and decoding and thus we can include other single-session baselines to our comparisons, including self-supervised models AutoLFADS  and NDT  which produce denoised firing rate estimates over which we fit a linear layer (+ Linear), a supervised variant of NDT  (NDT-Sup), and EIT . Both datasets contain single sessions from new animals performing movement tasks that we haven't seen during training.

On the NLB-Maze dataset, we obtain a R2 of 0.8952 after unit identification, which is competitive with the baselines. These results are surprising since we do not modify the model's weights, and yet our pretrained model yields competitive results on a dataset collected under very different conditions. When we finetune the model, we boost the performance even further establishing a 4.4% gap over the best baseline. Similar trends can be observed for the RTT task (Monkey I), with even larger (2%) improvement after finetuning.

### Poyq-1: A multi-lab, multi-task model for neural decoding

Given the impressive transfer results of POY0-mp to datasets from different labs, we ask whether we can use our approach to build a model that spans even more diverse recording setups that we expect

Figure 4: _Sample and compute efficiency for training, unit identification, and fine-tuning approaches._ On the left, we show the sample and compute efficiency for a heldout CO session. On the right, we plot the sample and compute efficiency for a new animal not seen during training.

   & Method & NLB-Maze & NLB-RTT \\   & Wiener Filter & 0.7485 & 0.5438 \\  & GRU & 0.8887 & 0.5951 \\  & MLP & 0.8794 & **0.6953** \\  & AutoLFADS + Linear  & 0.9062 & 0.5931 \\  & NDT + Linear  & 0.8929 & 0.5895 \\  & NDT-Sup  & 0.8708 & 0.4621 \\  & EIT  & 0.8791 & 0.4691 \\  & POY0-[Single-session] & **0.9470** & 0.6850 \\   & POY0-mp + Unit ID & 0.8962 & 0.7107 \\  & POY0-mp + Finetune & 0.9466 & 0.7318 \\   & POY0-1 + Unit ID & 0.9329 & 0.7294 \\   & POY0-1 + Finetune & **0.9482** & **0.7378** \\  

Table 3: _Behavioral decoding results for datasets from the Neural Latents Benchmark ._ Best performing model is in bold and second best model is underlined.

to encounter when trying to unify data from many sources. In total, we used datasets from seven non-human primates spanning three different labs, with a total of 27,373 units and 16,473 neuron-hours for training our model. We call this pretrained multi-lab, multi-task model POYO-1.

Even in light of the high amounts of variability across these different datasets, POYO-1 provides consistent improvements over the single-session models (Figure 5C). When tested on a number of different transfer tasks (Tables 2, 3), we again find that the unit identification and finetuning approaches provide effective strategies for adapting our pretrained model to new datasets. We obtain notable performance on the NLB-Maze, where we find that we obtain a \(^{2}\) of 0.9329 with only unit identification remapping, an almost 4% improvement over the unit identification result for our POYO-mp pretrained model, suggesting that we cover more of the task space with POYO-1.

When comparing the POYO-1 model with POYO-mp model, it is clear that both methods have their strengths. POYO-mp excels on the datasets sourced from the same lab, with 0.8788 on RT sessions compared to POYO-1's 0.8664. On the other hand, both models exhibit great transfer capabilities, with POYO-1 having the edge especially when using unit identification, indicating its ability to generalize better across diverse experimental conditions. This flexibility and scalability make these methods promising tools for future research in analyzing neural data from diverse sources.

**Visualizing the session embedding space.** We visualized the learned task embeddings to see if the model learns relationships between sessions seen during training (Figure 5D), even though it was not explicitly trained with this information. This analysis revealed clusters corresponding to different data sources and tasks, suggesting that our model has not only learned to identify patterns within each session but also recognized overarching structures across sessions. In particular, we find that the datasets collected in the Miller Lab (C,M,J) used to train the POYO-mp model are mapped into similar regions of the session latent space, and (I and L) sessions are mapped to their own distinct clusters.

## 4 Related Work

**Transformer architectures for time-series data.** Transformers have emerged as a powerful model for processing time-series data due to their ability to capture long-range dependencies [34; 35]. The key to their success lies in the self-attention mechanism , which allows the model to weigh the importance of each time step in the input sequence when producing an output. A typical approach in applying transformers to time-series data involves discretizing the continuous time domain into discrete bins , and treating the binned data as a sequence of tokens. Each bin is then linearly projected into a high-dimensional embedding, which is input to the transformer model. The Perceiver framework [18; 13] moves away from the idea of patches, by directly processing the input bytes,

Figure 5: _Scaling up to more tasks and diverse neural and behavioral recording conditions._ (A) Random target task from Makin et al. included in training of POYO-1 and (B) Maze task from NLB heldout for transfer testing. In (C) decoding accuracy for the single-session (gray) and the POYO-1 model (blue). (D) PCA projection of the learned session embeddings.

using cross-attention layers to reduce the complexity of attention. In addition to regular timeseries, there has been interest in applying transformers to model irregular timeseries including event stream and point process data [36; 37; 38]. However, many of these models work on univariate event streams and when they extend to multivariate cases, they assume fixed and known input channels.

Transformers applied to neural data.With the recent advances in transformers for sequence modeling in many different time-series, transformers have also recently found successful applications in building representations of neural population activity [32; 33; 39]. In these models, the spiking activity is first binned and then the estimated bin counts are tokenized. In the neural data transformer (NDT) model , the firing rates of all neurons in the population are embedded jointly in one token (time step or bin). In the embedded interaction transformer (EIT) , neurons are considered independently in one stage of processing and at a population level in a second stage, and thus the whole dataset is tokenized over both neurons and time. In the spatiotemporal (STNDT) model , two different tokenizations are also considered, one in space and one in time, and two representations are learned jointly for both tokenizations. In all cases, binned data are used and the models are trained on a single session and fixed set of neurons.

Multi-session training and alignment.The idea of decoding across multiple sessions has been explored in previous work [41; 42; 43; 44]. In many of these works, an initial baseline representation is formed on one day and alignment-based approaches are used to transfer a model trained on one session across recording days [23; 45; 12; 46; 47]. A subset of these methods [41; 43] can be trained on many sessions jointly, but rely on the assumption of shared dynamics or structure of a single task to achieve alignment. To the best of our knowledge, our work is the first to demonstrate multi-session transfer across subjects performing different tasks, and the first to demonstrate scaling across different data and model sizes.

## 5 Discussion

In this paper, we introduce a novel framework for training transformers on large multi-session, multi-task neural activity datasets. To tackle the challenges of training on such large heterogeneous sources, we introduce a novel spike-level tokenization scheme and architecture that enables the model to learn from populations with varying numbers of neurons. We show that training a single unified model on multiple recordings is possible, and find that it leads to improved decoding performance. Finally, we build two large pretrained models (POY0-1, POY0-mp) that can be efficiently fine-tuned on new datasets, and make them available as a resource to the community.

In contrast to models trained on a single dataset, the pretrained models that we have developed provide a potential way to compare and contrast datasets, and also understand common motifs of activity and dynamics that may be shared across different sessions, tasks, and individuals. Thus, it will be critical to develop tools to probe the patterns and motifs learned by such models and characterize the neural mechanisms underlying different tasks and computations. In particular, we look to understand how spike tokens are grouped across different latent tokens and how the dynamics of the population are modeled in this latent space. Additionally, our proposed unit embedding space allows us to map units into a high-dimensional space; thus understanding how unit projections are organized might help reveal the similarities between different neurons and the nature of their interactions. Similarly, we can analyse the session embeddings to glean insights into inter-session and across-animal differences.

Our work shows how pretraining on diverse data, including datasets from animals performing different tasks and across different laboratories, can all help to improve our ability to decode from novel and unseen neural datasets. Already, our results demonstrate the positive effect of scale for neural data analysis. However, to scale this approach further and integrate even more diverse brain regions and tasks, it will be critical to move toward a self-supervised objective. Thus, our current architecture and multi-session framework could be also extended to self-supervised tasks like generative next-event prediction or masked modeling to allow for even larger datasets to be ingested.

This framework has the potential to advance neuroscience research in several ways. By enabling the development of large pretrained models that can be fine-tuned for various downstream tasks, our work can accelerate progress in brain-machine interfaces and disease modeling applications. The ability to quickly and effectively decode neural activity from diverse datasets and modalities can have significant implications for improving our understanding of the brain and developing new therapeutic interventions.

#### Acknowledgements

We would like to thank Patrick Mineault for providing a lot of great feedback on the work, and Jingyun Xiao for helping to run baselines for NLB. This project was supported by NIH award 1R01EB029852-01, NSF award IIS-2146072 as well as generous gifts from the Alfred Sloan Foundation (ED), the McKnight Foundation (ED), and the CIFAR Azrieli Global Scholars Program (ED). BAR acknowledges support from NSERC (Discovery Grant: RGPIN-2020-05105; Discovery Accelerator Supplement: RGPAS-2020-00031; Arthur B. McDonald Fellowship: 566355-2022) and CIFAR (Canada AI Chair; Learning in Machine and Brains Fellowship). MGP acknowledges support the Fonds de recherche du Quebec Sante (Grant: Chercheurs-boursiers en intelligence artificielle). GL acknowledges support from the Canada CIFAR AI Chair program, the Canada Research Chair for Neural Computation and Interfacing, and NSERC Discovery grant RGPIN-2018-04821.