# StyleDrop: Text-to-Image Generation in Any Style

Kihyuk Sohn Nataniel Ruiz Kimin Lee Daniel Castro Chin Irina Blok Huiwen Chang Jarred Barber Lu Jiang Glenn Entis Yuanzhen Li Yuan Hao Irfan Essa Michael Rubinstein Dilip Krishnan Google Research

Now at Korea Advanced Institute of Science and Technology (KAIST).Now at OpenAI.Now at OpenAI.

###### Abstract

Pre-trained large text-to-image models synthesize impressive images with an appropriate use of text prompts. However, ambiguities inherent in natural language and out-of-distribution effects make it hard to synthesize image styles, that leverage a specific design pattern, texture or material. In this paper, we introduce _StyleDrop_, a method that enables the synthesis of images that faithfully follow a specific style using a text-to-image model. The proposed method is extremely versatile and captures nuances and details of a user-provided style, such as color schemes, shading, design patterns, and local and global effects. It efficiently learns a new style by fine-tuning very few trainable parameters (less than \(1\%\) of total model parameters) and improving the quality via iterative training with either human or automated feedback. Better yet, StyleDrop is able to deliver impressive results even when the user supplies only a _single_ image that specifies the desired style. An extensive study shows that, for the task of style tuning text-to-image models, StyleDrop implemented on Muse  convincingly outperforms other methods, including DreamBooth  and textual inversion  on Imagen  or Stable Diffusion . More results are available at our project website: https://styledrop.github.io.

Figure 1: **Visualization of StyleDrop** outputs for \(18\) different styles. Each model is tuned on a _single_ style reference image, which is shown in the white insert box of each image. The per-style text descriptor is appended to the content text prompt: “_A fluffy baby cloth with a knitted hat trying to figure out a laptop, close up_”. Generated images capture many nuances such as colors, shading, textures and 3D appearance.

Introduction

Text-to-image models trained on large image and text pairs have enabled the creation of rich and diverse images encompassing many genres and themes [2; 5; 33; 35; 43]. The resulting creations have become a sensation, with Midjourney  reportedly being the largest Discord server in the world . The styles of famous artists, such as Vincent Van Gogh, might be captured due to the presence of their work in the training data. Moreover, popular styles such as "anime" or "steampunk", when added to the input text prompt, may translate to specific visual outputs based on the training data. While many efforts have been put into "prompt engineering", a wide range of styles are simply hard to describe in text form, due to the nuances of color schemes, illumination and other characteristics. As an example, Van Gogh has paintings in different styles (_e.g._, Fig. 1, top row, rightmost three columns). Thus, a text prompt that simply says "Van Gogh" may either result in one specific style (selected at random), or in an unpredictable mix of several styles. Neither of these is a desirable outcome.

In this paper, we introduce StyleDrop3 which allows significantly higher level of stylized text-to-image synthesis, using as few as _one_ image as an example of a given style. Our experiments (Fig. 1) show that StyleDrop achieves unprecedented accuracy and fidelity in stylized image synthesis. StyleDrop is built on a few crucial components: (1) a transformer-based text-to-image generation model ; (2) adapter tuning ; and (3) iterative training with feedback. For the first component, we find that Muse , a transformer modeling a discrete visual token sequence, shows an advantage over diffusion models such as Imagen  and Stable Diffusion  for learning fine-grained styles from single images. For the second component, we employ adapter tuning  to style-tune a large text-to-image transformer efficiently. Specifically, we construct a text input of a style reference image by composing content and style text descriptors to promote content-style disentanglement, which is crucial for compositional image synthesis [37; 32; 41]. Finally, for the third component, we propose an iterative training framework, which trains a new adapter on images sampled from a previously trained adapter. We find that, when trained on a small set of high-quality synthesized images, iterative training effectively alleviates overfitting, a prevalent issue for fine-tuning a text-to-image model on a very few (_e.g._, one) images. We study high-quality sample selection methods using CLIP score (_e.g._, image-text alignment) and human feedback in Sec. 4.4.3, verifying the complementary benefit.

In addition to handling various styles, we extend our approach to customize not only style but also content (_e.g._, the identifying/distinctive features of a given object or subject), leveraging DreamBooth . We propose a novel approach that samples an image of _my content in my style_ from two adapters trained for content and style independently. This compositional approach voids the need to jointly optimize on both content and style images [20; 13] and is therefore very flexible. We show in Fig. 5 that this approach produces compelling results that combines personalized generation respecting both object identity and object style.

We test StyleDrop on Muse on a diverse set of style reference images, as shown in Fig. 1. We compare with other recent methods including DreamBooth  and Textual Inversion , using Imagen  and Stable Diffusion  as pre-trained text-to-image backbones. An extensive evaluation based on prompt and style fidelity metrics using CLIP  and a user study shows the superiority of StyleDrop to other methods. Please visit our website and Appendix for more results.

## 2 Related Work

**Personalized Text-to-Image Synthesis** has been studied to edit images of personal assets by leveraging the power of pre-trained text-to-image models. Textual inversion  and Hard prompt made easy (PEZ)  find text representations (_e.g._, embedding, token) corresponding to a set of images of an object without changing parameters of the text-to-image model.

DreamBooth  fine-tunes an entire text-to-image model on a few images describing the subject of interest. As such, it is more expressive and captures the subject with greater details. Parameter-efficient fine-tuning (PEFT) methods, such as LoRA  or adapter tuning , are adopted to improve its efficiency [3; 27]. Custom diffusion  and SVDiff  have extended DreamBooth to synthesize multiple subjects simultaneously. Inversion-based Style Transfer  presents a one-shot style tuning of text-to-image diffusion models. Unlike these methods built on text-to-image diffusion models, we build StyleDrop on Muse , a generative vision transformer. [11; 39; 20] have shown learning styles with text-to-image diffusion models, but from a handful or a dozen of style reference images, and are limited to painting styles. We demonstrate on a wide variety of visual styles, including 3d rendering, design illustration, and sculpture, using a single style reference image.

**Neural Style Transfer (NST).** A large body of work [12; 18; 24; 7] has investigated style transfer using deep networks by solving a composite objective of style and content consistency . Recently,  has shown that quantizing the latent space leads to improved visual and style fidelity of NST compared to continuous latent spaces. MaskSketch  converts sketch images into natural images via structure-guided parallel decoding of a masked image generation model. While both output stylized images, StyleDrop is different from NST in many ways; ours is based on text-to-image models to generate content, whereas NST uses an image to guide content (_e.g._, spatial structure) for synthesis; we use adapters to capture fine-grained visual style properties; we incorporate feedback signals to refine the style from a single input image.

**Parameter Efficient Fine Tuning (PEFT)** is a new paradigm for fine-tuning of deep learning models by only tuning a much smaller number of parameters, instead of the entire model. These parameters are either subsets of the original trained model, or small number of parameters that are added for the fine-tuning stage. PEFT has been introduced in the context of large language models [15; 23; 16], and then applied to text-to-image diffusion models [35; 33] with LoRA  or adapter tuning . Fine-tuning of autoregressive (AR) [10; 43; 21] and non-autoregressive (NAR) [6; 5; 38] generative vision transformers has been studied recently , but without the text modality.

## 3 StyleDrop: Style Tuning for Text-to-Image Synthesis

StyleDrop is built on Muse , reviewed in Sec. 3.1. There are two key parts. The parameter-efficient fine-tuning of a generative vision transformer (Sec. 3.2) and an iterative training with feedback (Sec. 3.3). Finally, we discuss how to synthesize images from two fine-tuned models in Sec. 3.4.

### Preliminary: Muse , a masked Transformer for Text-to-Image Synthesis

Muse  is a state-of-the-art text-to-image synthesis model based on the masked generative image transformer, or MaskGIT . It contains two synthesis modules for base image generation (\(256 256\)) and super-resolution (\(512 512\) or \(1024 1024\)). Each module is composed of a text encoder T, a transformer G, a sampler S, an image encoder E, and decoder D. T maps a text prompt \(t\) to a continuous embedding space \(\). G processes a text embedding \(e\) to generate logits \(l\) for the visual token sequence. S draws a sequence of visual tokens \(v\) from logits via iterative decoding [6; 5], which runs a few steps of transformer inference conditioned on the text embeddings \(e\) and visual tokens decoded from previous steps. Finally, D maps the sequence of discrete tokens to pixel space \(\).4 To summarize, given a text prompt \(t\), an image \(I\) is synthesized as follows:

\[I=(,(t))\,\ l_{k}=(v_{k},(t))+ (v_{k},(t))-(v_{k},(n)) ,\] (1)

where \(n\) is a negative prompt, \(\) is a guidance scale, \(k\) is the synthesis step, and \(l_{k}\)'s are logits, from which the next set of visual tokens \(v_{k+1}\)'s are sampled. We refer to [6; 5] for details on the iterative decoding process. The T5-XXL  encoder for T and VQGAN [10; 42] for E and D are used. G is trained on a large (image, text) pairs \(\) using masked visual token modeling loss :

\[L=_{(x,t),}_{}((x), ),(t),(x),\] (2)

where M is a masking operator that applies masks to the tokens in \(v_{i}\). \(_{}\) is a weighted cross-entropy calculated by summing only over the unmasked tokens.

### Parameter-Efficient Fine-Tuning of Text-to-Image Generative Vision Transformers

Now we present a unified framework for parameter-efficient fine-tuning of generative vision transformers. The proposed framework is not limited to a specific model and application, and is easily applied to the fine-tuning of text-to-image (_e.g._, Muse , Paella , Parti , RQ-Transformer ) and text-to-video (_e.g._, Phenaki , CogVideo ) transformers, with a variety of PEFT methods, such as prompt tuning , LoRA , or adapter tuning , as in . Nonetheless, we focus on Muse , an NAR text-to-image transformer, using adapter tuning .

Following , we are interested in adapting a transformer \(\), while the rest (\(\), \(\), \(\)) remain fixed. Let \(}:\) a modified version of a transformer \(\) that takes learnable parameters \(\) as an additional input. Here, \(\) would represent parameters for learnable soft prompts of prompt tuning or weights of adapter tuning. Fig. 2 provides an intuitive description of \(}\) with adapter tuning.

Fine-tuning of the transformer \(}\) involves learning of newly introduced parameters \(\), while existing parameters of \(\) (_e.g._, parameters of self-attention and cross-attention layers) remain fixed, with the learning objective as follows:

\[=*{arg\,min}_{}L_ {}\,\ L_{}=_{(x,t)_{}, }_{}}((x),),( t),,(x),\] (3)

where \(_{}\) contains a few (image, text) pairs for fine-tuning. Unlike DreamBooth  where the same text prompt is used to represent a set of training images, we use different text prompts for each input image to better disentangle content and style. Once trained, similarly to the procedure in Eq. (2), we synthesize images from the generation distribution of \(}(,,)\). Specifically, at each decoding step \(k\), we generate logits \(l_{k}\) as follows:

\[l_{k}=}(v_{k},(t),) +_{}}(v_{k},(t), )-(v_{k},(t))+ _{}(v_{k},(t))- (v_{k},(n)),\] (4)

where \(_{}\) controls the level of adaptation to the target distribution by contrasting the two generation distributions, one that is fine-tuned \(}(v_{k},(t),)\) and another that is not \((v_{k},(t))\), and \(_{}\) controls the textual alignment by contrasting the positive (\(t\)) and negative (\(n\)) text prompts.

#### 3.2.1 Constructing Text Prompts

To train \(\), we require training data \(_{}=\{(I_{i},t_{i})\}_{i=1}^{N}\) composed of (image, text) pairs for style reference. In many scenarios, we may be given only images as a style reference. In such cases, we need to manually append text prompts.

We propose a simple, templated approach to construct text prompts, consisting of the description of a content (_e.g._, object, scene) followed by the phrase describing the style. For example, we use a "cat" to describe an object in Tab. 1 and append "watercolor painting" as a style descriptor. Incorporating descriptions of both content and style in the text prompt is critical, as it helps to disentangle the content from style and let learned parameters \(\) model the style, which is our primary goal. While we find that using a rare token identifier  in place of a style descriptor (_e.g._, "watercolor painting") works as well, having such a descriptive style descriptor provides an extra flexibility of style property editing, which will be shown in Sec. 4.4.2 and Fig. 7.

### Iterative Training with Feedback

While our framework is generic and works well even on small training sets, the generation quality of the style-tuned model from a single image can sometimes be sub-optimal. The text construction method in Sec. 3.2.1 helps the quality, but we still find that overfitting to content is a concern. As in red boxes of Fig. 3 where the same house is rendered in the background, it is hard to perfectly avoid the content leakage. However, we see that many of the rendered images successfully disentangle style from content, as shown in the blue boxes of Fig. 3.

For such a scenario, we leverage this finding of high precision when successful and introduce an iterative training (IT) of StyleDrop using synthesized images by StyleDrop trained at an earlier stage to improve the recall (more disentanglement). We opt for a simple solution: construct a new training

Figure 2: A simplified architecture of transformer layers of Muse  with modification to support parameter-efficient fine-tuning (PEFT) with adapter . \(L\) layers of transformers are used to process a sequence of visual tokens in green conditioned on the text embedding _e._ Learnable parameters \(\) are used to construct weights for adapter tuning. See Appendix B.1.1 for details on adapter architecture.

set with a few dozen successful (image, text) pairs (_e.g._, images in blue box of Fig. 3) while using the same objective in Eq. (3). IT results in an immediate improvement with a reduced content leakage, as in Fig. 3 green box. The key question is how to assess the quality of synthesized images.

**CLIP score** measures the image-text alignment. As such, it could be used to assess the quality of generated images by measuring the CLIP score (_i.e._, cosine similarity of visual and textual CLIP embeddings). We select images with the highest CLIP scores and we call this method an iterative training with CLIP feedback (CF). In our experiments, we find that the CLIP score to assess the quality of synthesized images is an efficient way of improving the recall (_i.e._, textual fidelity) without losing too much style fidelity. On the other hand, CLIP score may not be perfectly aligned with the human intention [22; 40] and would not capture the subtle style property.

**Human Feedback** (HF) is a more direct way of injecting user intention into the quality evaluation of synthetic images. HF is shown to be powerful and effective in LLM fine-tuning with reinforcement learning . In our case, HF could be used to compensate the CLIP score not being able to capture subtle style properties. Empirically, selecting less than a dozen images is enough for IT, and it only takes about 3 minutes per style. As shown in Sec. 4.4.4 and Fig. 9, HF is critical for some applications, such as illustration designs, where capturing subtle differences is important to correctly reflect the designer's intention. Nevertheless, due to human selection bias, style may drift or be reduced.

### Sampling from Two \(\)'s

There has been an extensive study on personalization of text-to-image diffusion models to synthesize images containing multiple personal assets [20; 26; 13]. In this section, we show how to combine DreamBooth and StyleDrop in a simple manner, thereby enabling personalization of both _style and content_. Inspired by the idea of diffusion as energy-based models for compositional visual generation [9; 25; 8], we sample from two modified generation distributions, guided by \(_{}\) for style and \(_{}\) for content, each of which are adapter parameters trained independently on style and content reference images, respectively. Unlike existing works [20; 13], our approach does not require joint training of learnable parameters on multiple concepts, leading to a greater compositional power with pre-trained adapters, which are separately trained on individual subject and style assets.

Our overall sampling procedure follows the iterative decoding of Eq. (1), with differences in how we sample logits at each decoding step. Let \(t\) be the text prompt and \(c\) be the text prompt without the style descriptor.5 We compute logits at step \(k\) as follows: \(l_{k}=(1-)l_{k}^{s}+ l_{k}^{c}\), where

\[l_{k}^{s} =}(v_{k},(t),_{})+_{}} (v_{k},(t),_{})- (v_{k},(t))+_{}(v_{k},(t))-(v_{k},(n)) \] (5) \[l_{k}^{c} =}(v_{k},(c),_{})+_{}} (v_{k},(c),_{})- (v_{k},(c))+_{}(v_{k},(c))-(v_{k},(n)) \] (6)

where \(\) balances the StyleDrop and DreamBooth - if \(\) is 0, we get StyleDrop, and DreamBooth if 1. By properly setting \(\) (_e.g._, \(0.5 0.7\)), we get images of _my content in my style_ (see Fig. 5).

## 4 Experiments

We report results of StyleDrop on a variety of styles and compare with existing methods in Sec. 4.2. In Sec. 4.3 we show results on "my object in my style" combining the capability of DreamBooth and StyleDrop. Finally, we conduct an ablation study on the design choices of StyleDrop in Sec. 4.4.

### Experimental Setting

To the best of our knowledge, there has not been an extensive study of style-tuning for text-to-image generation models. As such, we suggest a new experimental protocol.

Figure 3: Iterative Training with Feedback. When trained on a single style reference image (orange box), some generated images by StyleDrop may exhibit leaked content from the style reference image (red box, images contain in the background a similar-looking house as in the style image), while other images (blue box) have better dismantlement of style from content. Iterative training of StyleDrop with the good samples (blue box) results in an overall better balance between style and text fidelity (green box).

**Data collection.** We collect a few dozen images of various styles, from watercolor and oil painting, flat illustrations, 3d rendering to sculptures with varying materials. While painting styles have been a major focus for neural style transfer research [12; 7], we go beyond and include a more diverse set of visual styles in our experiments. We provide image sources in Tab. S1 and attribute their ownership.

**Model configuration.** As in Sec. 3.2, we base StyleDrop on Muse  using adapter tuning [15; 36]. For all experiments, we update adapter weights for \(1000\) steps using Adam optimizer  with a learning rate of \(0.00003\). Unless otherwise stated, we use "StyleDrop" to denote the second round model trained on as many as 10 synthetic images with human feedback, as in Sec. 3.3. Nevertheless, to mitigate confusion, we append "HF" (human feedback), "CF" (CLIP feedback) or "R1" (first round model) to StyleDrop whenever there needs a clarity. More training details are in Appendix B.1.

**Evaluation.** We report quantitative metrics based on CLIP  that measures the style consistency and textual alignment. In addition, we conduct the user preference study to assess style consistency and textual alignment. Appendix B.2 summarizes details on the human evaluation protocol.

### StyleDrop Results

Fig. 1 shows results of our default approach on the 18 different style images that we collected, for the same text prompt. We see that StyleDrop is able to capture nuances of texture, shading, and structure across a wide range of styles, significantly better than previous approaches, enabling significantly more control over style than previously possible. Fig. 4 shows synthesized images of StyleDrop using 3 different style reference images. For comparison, we also present results of (b) DreamBooth  on Imagen , (c) a LoRA implementation of DreamBooth [34; 3; 16] and (d) textual inversion , both on Stable Diffusion .6\({}^{,}\)7 More results are available in Figs. S9 to S14.

Figure 4: Qualitative comparison of style-tuned text-to-image synthesis on various styles, including “melting golden 3d rendering”, “3d rendering”, “3d rendering”, “ “flat cartoon illustration”, and “cartoon line drawing”, shown on the first column. Text prompts used for synthesis are “the Golden Gate bridge”, “the letter “G”, and “a man riding a snowboard”. Image sources are in Tab. S1. We see that StyleDrop (HF) consistently captures nuances such as the “melting” effect in the top row.

For baselines, we follow instructions from the respective papers and open-source implementations, but with a few modifications. For example, instead of using a rare token (_e.g._, "a watermelon slice in [V*] style"), we use the style descriptor (_e.g._, "a watermelon slice in 3d rendering style"), similarly to StyleDrop. We train DreamBooth on Imagen for 300 steps after performing grid-search. This is less than 1000 steps recommended in , but is chosen to alleviate overfitting to image content and to better capture style. For LoRA DreamBooth on Stable Diffusion, we train for 400 steps with learning rates of \(0.0002\) for UNet and \(0.000005\) for CLIP. We do not adopt the iterative training for baselines in Fig. 4. StyleDrop results without iterative training are in Sec. 4.4.3. It is clear from Fig. 4 that StyleDrop on Muse convincingly outperforms other methods that are geared towards solving subject-driven personalization of text-to-image synthesis using diffusion models.

We see that style-tuning on Stable Diffusion with LoRA DreamBooth (Fig. 4(c)) and textual inversion (Fig. 4(d)) show poor style consistency to reference images. While DreamBooth on Imagen (Fig. 4(b)) improves over those on Stable Diffusion, it still lacks the style consistency over StyleDrop on Muse across text prompts and style references. It is interesting to see such a difference as both Muse  and Imagen  are trained on the same set of image/text pairs using the same text encoder (T5-XXL ). We provide an ablation study to understand where the difference comes from in Sec. 4.4.1.

#### 4.2.1 Quantitative Results

For quantitative evaluation, we synthesize images from a subset of . This includes 190 text prompts of basic text compositions, while removing some categories such as abstract, arts, people or world knowledge. We test on 6 style reference images from Fig. 1.8

**CLIP scores.** We employ two metrics using CLIP , (Text) and (Style) scores. For Text score, we measure the cosine similarity between image and text embeddings. For Style score, we measure the cosine similarity between embeddings of style reference and synthesized images. We generate 8 images per prompt for 190 text prompts, 1520 images in total. While we desire high scores, these metrics are not perfect. For example, Style score can easily get to \(1.0\) if mode collapses.

StyleDrop results in competitive Text scores to Muse (_e.g._, \(0.323\) vs \(0.322\) of StyleDrop (HF)) while achieving significantly higher Style scores (_e.g._, \(0.556\) vs \(0.694\) of StyleDrop (HF)), implying that synthesized images by StyleDrop are consistent in style with style reference images, without losing text-to-image generation capability. For the 6 styles we test, we see a light mode collapse from the first round of StyleDrop, resulting in a slightly reduced Text score. Iterative training (IT) improves the Text score, which is aligned with our motivation. As a trade-off, however, they show reduced Style scores over Round 1 models, as they are trained on synthetic images and styles may have been drifted due to a selection bias.

DreamBooth on Imagen falls short of StyleDrop in Style score (\(0.644\) vs \(0.694\) of HF). We note that the increment in Style score for DreamBooth on Imagen is less significant (\(0.569\!\!0.644\)) than StyleDrop on Muse (\(0.556\!\!0.694\)). We think that the fine-tuning for style on Muse is more effective than that on Imagen. We revisit this in Sec. 4.4.1.

**Human evaluation.** We formulate 3 binary comparison tasks for user preference among StyleDrop (R1), StyleDrop with different feedback signals, and DreamBooth on Imagen. Users are asked to select their preferred result in terms of style and text fidelity between images generated from two

    & SDRP (R1) & tie & DB on Imagen & SDRP (R1) & tie & SDRP (HF) & SDRP (HF) & tie & SDRP (CF) \\  Text & 31.7\% & **45.0**\% & 23.3\% & 20.7\% & **56.0**\% & 23.3\% & 19.4\% & **58.2**\% & 22.4\% \\ Style & **86.0**\% & 4.3\% & 9.7\% & **62.3**\% & 7.4\% & 30.3\% & **60.9**\% & 8.4\% & 30.8\% \\    &  &  &  &  \\    & & & & Round 1 & HF & CF & Random \\  Text (\(\)) & **0.337\({}_{ 0.001}\)** & 0.335\({}_{ 0.001}\) & 0.322\({}_{ 0.001}\) & 0.313\({}_{ 0.001}\) & 0.322\({}_{ 0.001}\) & 0.329\({}_{ 0.001}\) & 0.316\({}_{ 0.001}\) \\ Style (\(\)) & **0.569\({}_{ 0.002}\)** & **0.644\({}_{ 0.002}\)** & **0.556\({}_{ 0.001}\)** & **0.705\({}_{ 0.002}\)** & **0.694\({}_{ 0.001}\)** & 0.673\({}_{ 0.001}\) & 0.678\({}_{ 0.001}\) \\   

Table 2: Evaluation metrics of (top) human evaluation and (bottom) CLIP scores  for image-text alignment (Text) and visual style alignment (Style). We test on 6 styles from Fig. 1. For human evaluation, preferences are reported. For CLIP scores, we report the mean and standard error. We report scores for Muse  and Imagen  with styles guided by the text prompt. DB: DreamBooth, SDRP: StyleDrop, and iterative training with human feedback (HF), CLIP feedback (CF), and random selection (Random).

different models (_i.e_., an A/B test), while given a style reference image and the text prompt. Details on the study is in Appendix B.2. Results are in Tab. 2 (top). Compared to DreamBooth on Imagen, images by StyleDrop are significantly more preferred by users in Style score. The user study also shows style drifting more clearly when comparing StyleDrop (R1) and StyleDrop IT either by HF or CF. Between HF and CF, HF retains better Style and CLIP retained better Text. Overall, we find that CLIP scores are a good proxy to the user study.

### My Object in My Style

We show in Fig. 5 synthesized images by sampling from two personalized generation distributions, one for an object and another for the style, as described in Sec. 3.4. To learn object adapters, we use 5\(\)6 images per object.9 Style adapters from Sec. 4.2 are used without any modification. The value of \(\) (to balance the contribution of object and style adapters) is chosen in the range \(0.5\)-\(0.7\). We show synthesized images from (a) object adapter only (_i.e_., DreamBooth), (b) style adapter only (_i.e_., StyleDrop), and (c) both object and style adapters. We see from Fig. 5(a) that text prompts are not sufficient to generate images with styles we desire. From Fig. 5(b), though StyleDrop gets style correct, it generates objects that are inconsistent with reference subjects. The proposed sampling method from two distributions successfully captures both _my object_ and _my style_, as in Fig. 5(c).

### Ablations

We conduct ablations to better understand StyleDrop. In Sec. 4.4.1 we compare the behavior of the Imagen and Muse models. In Sec. 4.4.2 we highlight the importance of a style descriptor. In Sec. 4.4.3, we compare choices of feedback signals for iterative training. In Sec. 4.4.4, we show to what extent StyleDrop learns distinctive styles properties from reference images.

#### 4.4.1 Comparative Study of DreamBooth on Imagen and StyleDrop on Muse

We see in Sec. 4.2 that StyleDrop on Muse convincingly outperforms DreamBooth on Imagen. To better understand where the difference comes from, we conduct some control experiments.

**Impact of training text prompt.** We note that both experiments in Sec. 4.2 are carried out using the proposed descriptive style descriptors. To understand the contribution of _fine-tuning_, rather than the prompt engineering, we conduct control experiments, one with a rare token as in  (_i.e_., "A flower in [V*] style") and another with descriptive style prompt.

Figure 5: Qualitative comparison of (a) DreamBooth, (b) StyleDrop, and (c) DreamBooth + StyleDrop. For DreamBooth and StyleDrop, style and subject are guided by text prompts, respectively, whereas DreamBooth + StyleDrop, both style (blue inset box at bottom left) and subject (red inset box at top right) are guided by respective reference images. Image sources are in Tab. S1.

Results on Muse are in Fig. 7. Comparing (a) and (c), we do not find a substantial change, suggesting that the style can be _learned_ via an adapter tuning without too much help of a text prior. On the other hand, as seen in Fig. 6, comparing (a) and (c) with Imagen as a backbone model, we see a notable difference. For example, "melting" property only appears for some images synthesized from a model trained with the descriptive style descriptor. This suggests that the learning capability of fine-tuning on Imagen may not be as powerful as that of Muse, when given only a few training images.

**Data efficiency.** Next, we study whether the quality of the fine-tuning on Imagen could be improved with more training data. In this study, we train a DreamBooth on Imagen using 10 human selected, synthetic images from StyleDrop on Muse. Results are in Fig. 6. Two models are trained with (b) a rare token and (d) a descriptive style descriptor. We see that the style consistency improves a lot when comparing (c) and (d) of Fig. 6, both in terms melting and golden properties. However, when using a rare token, we do not see any notable improvement from (a) to (b). This suggests that the superiority of StyleDrop on Muse may be coming from its extraordinary fine-tuning data efficiency.

#### 4.4.2 Style Property Edit with Concept Disentanglement

We show in Sec. 4.4.1 that StyleDrop on Muse is able to learn the style using a rare token identifier. Then, what is the benefit of descriptive style descriptor? We argue that not all styles are described in a single word and the user may want to learn style properties selectively. For example, the style of an image in Fig. 7 may be written as a composite of "melting", "golden", and "3d rendering", but the user may want to learn its "golden 3d rendering" style without "melting".

We show that such a _style property edit_ can be naturally done with a descriptive style descriptor. As in Fig. 7(d), learning with a descriptive style descriptor provides an extra knob to edit a style by omitting certain words (_e.g._, "melting") from the style descriptor at synthesis. This clearly shows the benefit of descriptive style descriptors in disentangling visual concepts and creating a new style based on an existing one. This is less amenable when trained with the rare token, as in Fig. 7(b).

#### 4.4.3 Iterative Training with Different Feedback Signals

We study how different feedback signals affects the performance of StyleDrop. We compare three feedback signals, including human, CLIP, and random. For CLIP and random signals, we synthesize

Figure 8: Qualitative comparison of StyleDrop. (a) Round 1, (b) IT with random selection, (c) CLIP and (d) Human feedback. Generated images of “a banana” and “a bottle” in “3d rendering style” are visualized. While StyleDrop Round 1 model captures the style very well, it often suffer from a content leakage (_e.g._, a banana and women are mixed). (c, d) IT with a careful selection of synthetic images reduces content leakage and improves.

Figure 6: Ablation study using Imagen . (a, b) are trained with a rare token and (c, d) are trained with a style descriptor. (a, c) are trained on a single style reference image. (b, d) are trained on 10 synthetic images from StyleDrop. With Imagen, we need 10 images and a descriptive style descriptor to capture the style, as in (d).

Figure 7: StyleDrop on Muse. All models are trained on 10 images from StyleDrop, which in turn was trained on a single style image. (a, b) are trained with a rare token and (c, d) are trained with a style descriptor. When trained with a descriptive style descriptor, StyleDrop can support additional applications such as _style editing_ (d), here removing the “melting” component of the reference style.

64 images per prompt from 30 text prompts and select one image per prompt. For human, we select 10 images from the same pool, which takes about 3 minutes per style. See Appendix B.2 for details.

Qualitative results are in Fig. 8. We observe that some images in (a) from a Round 1 model show a mix of banana or bottle with a human. Such concept leakage is alleviated with IT, though we still see a banana with arms and legs with Random strategy. The reduction in concept leakage could be verified with the Text score, achieving (a) \(0.303\), (b) \(0.322\), (c) \(0.339\), and (d) \(0.328\). On the other hand, Style score, (a) \(0.560\), (b) \(0.567\), (c) \(0.542\), and (d) \(0.549\), could be misleading in this case, as we compute the visual similarity to the style reference image, favoring a content leakage. Between CLIP and human feedback, we see a clear trade-off between text and style fidelity from quantitative metrics.

#### 4.4.4 Fine-Grained Style Control with User Intention

Moreover, human feedback is more critical when trying to capture subtle style properties. In this study, we conduct experiments on four images in Fig. 9 inside orange boxes, created by the same designer with varying style properties, such as color offset (Fig. 9(b)), gradation (Fig. 9(c)), and sharp corners (Fig. 9(d)). We train two more rounds of StyleDrop with human feedback. We use the same style descriptor of "minimal flat illustration style" to make sure the same text prior is given to all experiments. As in Fig. 9, style properties such as color offset, gradation, and corner shape are captured correctly. This suggests that StyleDrop offers the control of fine-grained style variations.

## 5 Conclusion

We have presented StyleDrop, a novel approach to enable synthesis of any style through the use of a few user-provided images of that style and a text description. Built on Muse  using adapter tuning , StyleDrop achieves remarkable style consistency at text-to-image synthesis. Training StyleDrop is efficient both in the number of learnable parameters (_e.g._, \(<\) 1%) and the number of style samples (_e.g._, 1) required.

**Limitations.** Visual styles are of course even more diverse than what is possible to explore in our paper. More study with a well-defined system of visual styles, including, but not limited to, the formal attributes (_e.g._, use of color, composition, shading), media (_e.g._, line drawing, etching, oil painting), history and era (_e.g._, Renaissance painting, medieval mosaics, Art Deco), and style of art (_e.g._, Cubism, Minimalism, Pop Art), would broaden the scope. While we show in part the superiority of a generative vision transformer to diffusion models at few-shot transfer learning, it is by no means conclusive. We leave an in-depth study among text-to-image generation models as a future work.

**Societal impact.** As illustrated in Fig. 4, StyleDrop could be used to improve the productivity and creativity of art directors and graphic designers when generating various visual assets in their own style. StyleDrop makes it easy to reproduce many personalized visual assets from as little as one seed image. We recognize potential pitfalls such as the ability to copy individual artists' styles without their consent, and urge the responsible use of our technology.

**Acknowledgement.** We thank Varun Jampani, Jason Baldridge, Forrester Cole, Jose Lezama, Steven Hickson, Kfir Aherman for their valuable feedback on our manuscript.

Figure 9: Fine-grained style control. StyleDrop captures subtle style differences, such as (b) color offset, (c) gradation, or (d) sharp corner, reflecting designer’s intention in text-to-image synthesis.