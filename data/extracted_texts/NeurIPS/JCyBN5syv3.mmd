# SimGen: Simulator-conditioned Driving Scene Generation

Yunsong Zhou\({}^{1,2*}\) Michael Simon\({}^{1}\) Zhenghao Peng\({}^{1}\) Sicheng Mo\({}^{1}\)

**Hongzi Zhu\({}^{2}\) Minyi Guo\({}^{2}\) Bolei Zhou\({}^{1}\)**

\({}^{1}\) University of California, Los Angeles \({}^{2}\) Shanghai Jiao Tong University

https://metadriverse.github.io/simgen/

###### Abstract

Controllable synthetic data generation can substantially lower the annotation cost of training data. Prior works use diffusion models to generate driving images conditioned on the 3D object layout. However, those models are trained on small-scale datasets like nuScenes, which lack appearance and layout diversity. Moreover, overfitting often happens, where the trained models can only generate images based on the layout data from the validation set of the same dataset. In this work, we introduce a simulator-conditioned scene generation framework called SimGen that can learn to generate diverse driving scenes by mixing data from the simulator and the real world. It uses a novel cascade diffusion pipeline to address challenging sim-to-real gaps and multi-condition conflicts. A driving video dataset DIVA is collected to enhance the generative diversity of SimGen, which contains over 147.5 hours of real-world driving videos from 73 locations worldwide and simulated driving data from the MetaDrive simulator. SimGen achieves superior generation quality and diversity while preserving controllability based on the text prompt and the layout pulled from a simulator. We further demonstrate the improvements brought by SimGen for synthetic data augmentation on the BEV detection and segmentation task and showcase its capability in safety-critical data generation.

Figure 1: **SimGen** is a _controllable_ scene generation paradigm conditioned on a simulator. It learns from real-world and simulated data and then can generate diverse driving scenes based on the simulatorâ€™s control conditions and text prompt.

Introduction

A high-quality and diverse training data corpus is crucial for autonomous driving research and development. However, it is costly and laborious to annotate the data. Synthetic data generation is a promising alternative to harvest annotated training data, which brings realistic images and notable performance improvements across tasks like object detection  and semantic segmentation . Besides the _realism_ of the generated images, there are two necessary conditions to consider for a practical synthetic data generator for autonomous driving: 1) Appearance diversity, which ensures the synthetic data can cover a spectrum of weather, environmental, and geographical conditions. 2) Layout diversity, namely the distribution of objects, should cover different traffic scenarios, including safety-critical situations that are rare to collect in the real world.

Recent diffusion-based generative models show promising results to generate realistic driving images from text prompts , BEV road maps , and object boxes [70; 72; 20; 79]. Despite generating coherent images, these attempts lack the generalizability of generating new and diverse real-world appearances and traffic scenarios due to data limitations. They are confined to learning on small-scale datasets [29; 32; 44; 71] with limited scenarios such as only urban streets  or restricted weather conditions . In addition, the driving behaviors in the available driving datasets like nuScenes are tedious and lack complex or safety-critical situations. Another option for collecting synthetic data is from driving simulators, which can effortlessly generate scenes encompassing various behaviors with its physics and graphics engines [16; 36; 58; 61; 66]. Simulators also provide accurate control over all objects and their spatial locations, thus can easily generate a huge amount of traffic layout maps. However, open-source simulators usually only contain a limited amount of 3D assets, and they lack a realistic visual appearance. Thus, the models trained on simulator-generated data can easily overfit, also known as the Simulation to Reality (Sim2Real) gap.

We take the best of two worlds by integrating the data-driven generative models with a simulator to obtain both the appearance diversity of real-world data and the layout controllability of simulated data. To this end, we introduce **SimGen**, a simulator-conditioned diffusion model, which follows the layout guidance from the simulator and rich text prompts to generate diverse driving scene images. One naive approach is to guide an image generation model with the depth and semantic images from the simulator via training a control branch through ControlNet . Yet, as the simulator has limited assets and cannot fully capture the variations in the real world, the simulated conditions and the underlying real-world conditions that guide a diffusion model to generate real-world images might have conflicts. To tackle this, SimGen adopts a cascade design. The model first injects noise-added simulated conditions such as depth and semantic images into the intermediate sampling process of a pre-trained text-to-real-condition diffusion network. The network then converts simulated conditions into more realistic conditions via continuous denoising, free of additional training on simulated conditions beyond this diffusion network. After that, a second diffusion module utilizes an adapter to integrate multimodal conditions and uses masks to filter conflicting data. SimGen thus achieves outstanding generation quality and diversity while preserving layout controllability by connecting with the simulator.

We construct a dataset called **DIVA** to obtain the appearance and layout diversity of the training data. DIVA comprises two parts: the web data and the synthesized data from the simulator. On the one hand, web data covers a worldwide range of geography, weather, scenes, and traffic elements, preserving the appearance diversity of a wide range of traffic participants. We design a data curation pipeline to collect and label YouTube driving videos. On the other hand, virtual driving videos with the traffic flow replayed from trajectory datasets or generated by a safety-critical scenario generator  are collected from a driving simulator . In short, _DIVA dataset blends real-world appearances and virtual layouts_, consisting of 147.5 hours of **D**iverse **I**n-the-wild and **V**irtual driving data**A**.

We summarize our contributions as follows: 1) a novel controllable image generation model SimGen incorporating a driving simulator to generate realistic driving scenarios with appearance and layout diversity; 2) a new dataset DIVA containing massive web and simulated driving videos that ensures diverse scene generation and advances simulation-to-reality research; 3) SimGen improves over counterparts like BEVGen , MagicDrive , Panacea , DrivingDiffusion , _i.e._, in terms of image quality, diversity, and controllability of scene generation.

## 2 Appearance Diversity and Layout Diversity from DIVA Dataset

We introduce a large-scale DIVA dataset containing diverse driving scenes in the real world and the simulation. It facilitates the training of generative models and tackles the simulation-to-reality (Sim2Real) challenge. Tab. 1 displays the statistics, composition, and annotation of the data, which comprises about 147.5 hours of driving videos. The data is collected from a vast corpus of high-quality YouTube driving videos and simulation environments in the MetaDrive simulator . We use DIVA-Real and DIVA-Sim to denote the web data downloaded from YouTube and the data from the MetaDrive simulator, respectively. Comparisons with other datasets, license, and privacy considerations are detailed in Appendix B.

### DIVA-Real: Appearance Diversity in Web Data

**Collecting web videos.** As shown in Fig. 2 (left), to streamline the process and minimize manual effort, we begin by searching for relevant keywords on YouTube to identify a batch of driving video channels. The videos are downloaded from these identified YouTube channels. We filter out unsuitable videos based on their length and resolution and proceed to download the appropriate ones. This yields hundreds of first-person driving videos, each with an average duration of one hour. Next, we sample the videos into frames at 10Hz, excluding the initial and final 30 seconds to eliminate user channel information. This process yields over 4.3 million frames, awaiting further data cleaning.

**Data cleaning and autolabeling.**

Data cleaning is vital for ensuring data quality, but manual inspection of each image is impractical. Inspired by , we implement an automated data-cleaning workflow to expedite the process. With the remarkable image understanding capabilities of the vision-language model (VLM), _i.e._ LLaMA-Adapter V2 , we are able to conduct the quality checks via VLM with a checklist including criteria such as non-front view, video transition, black screens, _etc_, to identify nonconforming images. Driving videos are chunked into five-frame batches. For each batch, the VLM chooses and assesses a random image; if this single image fails to pass checks, the entire batch of five frames will be discarded. In the autolabeling process, pre-trained models for various tasks, including BLIP2-flant5 , ZoeDepth , and Segformer , are used to generate annotations of text, depth, and semantic segmentation, respectively. Eventually, over 120 hours of driving videos with rich annotations are collected.

### DIVA-Sim: Layout Diversity from the Simulator

Simulators are capable of faithfully reconstructing real-world scenes and hence obtaining training data with layout diversity. Also, after loading the driving scenarios such as map topology from the dataset, the simulator allows changing the motions and states of the traffic participants with pre-defined rules or interactive policies that differ from the original ones. This inspires us to build **Sim2Real data** from the simulator. The Sim2Real data is induced from the same real-world scenarios, in which we can obtain real-world map topology, layout, and raw sensor data. At the same time, we can reconstruct the paired data from those scenarios but with reconstructed sensor data and even with altered layout and traffic flows. DIVA-Sim utilizes the MetaDrive simulator  and ScenarioNet  to gather 5.5 hours of virtual driving videos from nuScenes layouts  and another 22 hours from procedurally generated behaviors. It includes a set of safety-critical driving data through interactions introduced by an adversarial traffic generation method , further improving the diversity of our dataset.

**Scene layout construction.** We utilize ScenarioNet  to transform scenes into a unified description format suitable for simulators, known as _scene records_, logging map elements and objects. As illustrated by the example scene in Fig. 2 (right), loading _scene records_, MetaDrive  can reconstruct

    &  &  &  &  &  \\   & (hours) & & & & Text & Depth & Seg. & Virt. \\  KITTI  & 1.4 & 15k & 1 & 1 & & âœ“ & âœ“ \\ CityScapes  & 0.5 & 25k & 3 & 50 & & & âœ“ \\ Waymo  & 11 & 300k & 1 & 3 & & âœ“ \\ Aryoverse  & 4.2 & 300k & 1 & 6 & & \\ nuPlan*  & 120 & 4.0M & 2 & 4 & & \\ Honda-HAD  & 32 & 1.2M & 1 & - & âœ“ & & \\ nuScenes  & 5.5 & 241k & 2 & 2 & & âœ“ & \\  DIVA-Real & 120 & 43M & 19 & 71 & âœ“ & âœ“ & âœ“ \\ DIVA-Sim & 27.5* & 998k* & 3 & 5 & âœ“ & âœ“ & âœ“ \\
**DIVA (All)** & **147.5** & **5.3M** & **22** & **76** & âœ“ & âœ“ & âœ“ \\   

Table 1: **Comparing DIVA with relevant datasets on scale, diversity, and annotations. \({}^{*}\): perception subset. \({}^{+}\): including procedural generation  and safety-critical  data. Cts: countries; Seg: segmentation; Virt: virtual image.**roads, blocks, and intersections, and place corresponding 3D models like vehicles, cyclists, and pedestrians, based on the recorded positions and orientations. We will reasonably select representative 3D models based on the category and dimensions of the objects. And the model's shape is scaled based on the real dimensions to replicate the objects in the nuScenes dataset accurately. By doing so, the digital twin scenario can be faithfully reconstructed in the simulator.

**Obtaining images via trajectory replay and rendering pipeline.** The _control policy_ determines the motion dynamics, while the sensors generate multimodal image data at any desired location. To create nuScenes digital twins, ReplayPolicy is applied to replay logged trajectories of all objects. Our cameras are placed in the exact pose of the nuScenes front camera, with the camera's field of view adjusted to match that of nuScenes closely. The camera attribute can be set to multiple types to obtain a variety of sensor data. In summary, we can obtain the following conditions through the simulator: rendered RGB, depth, semantic segmentation, instance segmentation, and top-down views.

**Creation of safety-critical data.** Besides building digital twins of the real-world data, we can harness the simulator to continue growing the safety-critical data and enhance layout diversity. We apply the CAT method  to generate safety-critical data based on real-world scenarios. Specifically, we first randomly sample one scenario from the Waymo Open dataset . A traffic vehicle is perturbed to attempt colliding with the ego-vehicle via adversarial interaction learning . Thus, we harvest many safety-critical scenarios with adversarial driving behaviors, which might be challenging to collect in the real world. This scalable creation of the safety-critical data from the simulator is also one of the strengths of our method.

## 3 SimGen Framework

SimGen aims to generate realistic driving images based on the text prompt and the spatial conditions including semantic and depth maps from real-world datasets and the driving simulator. We incorporate a driving simulator into the data generation pipeline to achieve controllable and diverse image generation. Incorporating the simulator provides access to diverse layouts and behaviors of traffic participants, thus better closing the Sim2Real gap. However, if just conditioning the diffusion model on synthesized data from a simulator, the diffusion model will result in bad image quality due to the limited assets and the artificial rendering. We propose a cascade generative model that first transforms the simulated spatial conditions to realistic conditions as those in the dataset, then uses those realistic conditions to guide the first-view image diffusion model.

Illustrated in Fig. 3, SimGen first samples a driving scenario and a text prompt from the dataset and invokes the driving simulator MetaDrive  to render _simulated conditions (SimCond)_, the

Figure 2: **Constructing DIVA dataset. DIVA-Real (left) comprises driving videos collected from YouTube. We apply a Vision Language Model to filter out noisy images via a checklist and utilize off-the-shelf models to annotate text, depth, and semantic labels. Meanwhile, DIVA-Sim (right) employs scene records and control policies in a simulator to create map elements and objects. It can generate digital twins of real-world data and safety-critical scenes. Then various kinds of sensors placed in the simulation produce multimodal images. Ren.rendered; T.D. : top-down view. Numbers and letters indicate the sequence of processes.**

synthesized depth and segmentation images. Then, the SimCond and text features are fed into a lightweight diffusion model **CondDiff** (Sec. 3.1) that converts simulated conditions into _realistic conditions (RealCond)_, that resembles the real-world depth and segmentation images from YouTube and nuScenes datasets. Finally, a diffusion model called **ImgDiff** (Sec. 3.2) generates a driving scene according to multi-modal conditions, including RealCond, textual prompts, and optional simulated spatial conditions, including RGB images, instance maps, and top-down views, _etc_.

### Sim-to-Real Condition Transformation

While we strive to align the simulator settings with real data, such as intrinsic and extrinsic parameters of the camera, there is still a disparity between RealCond and SimCond. The disparity arises from image mismatch, inherent flaws of the 3D models, and the simulator's lack of background details (Appendix C.1.1). Consequently, simulator conditions require transformation to closely resemble real ones. An easy solution is to use domain adaptation  and consider the SimCond and RealCond as different image styles. However, training a domain transfer model that can generalize to novel scenarios requires paired SimCond and RealCond data far exceeding public datasets like nuScenes. Thus, it's necessary to have an adaptation-free approach for Sim2Real transformation without additional training on SimCond. To achieve that, we first use data from DIVA-Real to train a diffusion model, CondDiff, that generates RealCond purely from text prompts. The training does not contain data rendered from simulators. During inference, CondDiff injects noise-added SimCond into the intermediate sampling process and converts it into realistic conditions via continuous denoising.

**Learning to generate conditions from text inputs.** To facilitate the learning process of CondDiff, we initiate this stage with text-to-RealCond generation. Concretely, we utilize Stable Diffusion 2.1 (SD-2.1) , a large-scale latent diffusion model for text-to-image generation. It is implemented as a denoising UNet, denoted by \(_{}\), with multiple stacked convolutional and attention blocks, which learns to synthesize images by denoising latent noise. Let \(_{0}\) represents a latent feature from the data distribution \(p()\). Starting from \(_{0}\), the training process involves gradually adding noise to procedure \(_{t}\) for \(t(0,1]\) until \(_{t}\) transforms into Gaussian noise, namely forward stochastic differential equation (SDE) . The model is optimized by minimizing the mean-square error:

\[_{t}=_{t}_{0}+_{t}, (,),_{0} p(),\] (1) \[ t,\ _{}||-_{}( _{t};,t)||_{2}^{2},\] (2)

where \(_{t}\) is a scalar function that describes the magnitude of the noise \(\) at denoising step \(t\), \(_{t}\) is a scalar function that denotes the magnitude of the data \(_{0}\), \(\) parameterizes the denoiser model \(_{}\), \(\) is the added noise, and \(\) is the text condition that guides the denoising process. The learning occurs in a compressed latent space \(\) instead of the pixel space . During sampling, the model iteratively denoises the final step prediction from the standard Gaussian noise to generate images.

Figure 3: **Illustration of SimGen. SimGen processes text and scene record as inputs. The text is feature-encoded and utilized in the subsequent modules, whereas the scene record undergoes a simulator rendering into simulated depth and segmentation (SimCond) and extra conditions (ExtraCond). SimCond, coupled with the text features, is fed into the CondDiff module that converts SimCond into RealCond, representing real depth and segmentation. Eventually, the text features, RealCond, and ExtraCond are inputted into the ImgDiff module, where an Adapter merges multi-source conditions into a unified control condition and generates driving scene images.**

The original SD-2.1 is trained on data from various domains unrelated to the depth and semantic images in driving scenes. As depicted in the CondDiff in the upper right of Fig. 3, we fine-tune the SD-2.1 to be a text-to-RealCond model using the triplets of text, depth and segmentation data from DIVA-Real and nuScenes, with the objective of Eq. (2). After loading the SD-2.1 checkpoint, all parameters \(\) of the Unet are fine-tuned at this stage, while the CLIP text encoder  and autoencoder  remain frozen. The depth and segmentation data is autolabelled by a set of perception models as discussed in Sec. 2.1.

Adaptation-free sim-to-real transformation.Now, we have a model CondDiff that generates RealCond purely from text prompts. We will then use the conditions from simulator _SimCond_ to guide the sampling process so that we can transform SimCond to RealCond. According to SDEdit , the reverse SDE, where the diffusion model iteratively denoises standard Gaussian noise to generate images, can start from any intermediate time. This inspires us to insert noise-added SimCond into the intermediate time of the sampling process, and the model will use them as guidance to generate RealCond with the SimCond layouts. In detail, the module first encodes the SimCond into latent space to get \(^{}\). It selects a specific time \(t_{s}(0,1)\) and perturbs the input \(^{}\) using a Gaussian noise of standard deviation \(_{t_{s}}^{2}\) as follows:

\[\ ^{}(^{};_{t_{s}}^{2}).\] (3)

The perturbing process will effectively remove low-level details like pixel information while preserving high-level cues like rough color strokes . The noise-processed image \(^{}\) seamlessly substitutes the diffusion model's state at time \(t_{s}\) during denoising. Thus, the intermediate state \(_{t_{s}}=^{}\) serves as a guidance to solve the corresponding reverse SDE as follows:

\[p_{}(_{t_{s}-1}|_{t_{s}})=(_{t_ {s}-1};_{}(_{t_{s}},t),_{}(_{t_{s}}, t_{s})),\] (4)

where \(_{}\) and \(_{}\) are determined by CondDiff \(_{}\). The above equation iterates until the model generates a synthesized image \(_{0}\) like RealCond at \(t_{s}=0\). Throughout this process, all parameters of CondDiff remain frozen, with only SimCond \(^{}\), text \(\), and noise affecting the sampling process.

### Controllable Image Generation with Multimodal Conditions

In the second stage, we will use a diffusion-based model to synthesize diverse driving images by integrating various control conditions (Tab. 2), including the RealCond from the data or generated from SimCond by CondDiff, the textual prompt, and some extra conditions ExtraCond such as rendered RGB, instance segmentation, and top-down views from the simulator. ExtraCond offers additional information for the output image, including road typology and object attributes (orientation, outlines, and 3D locations), highlighting the necessity of incorporating them into model control.

However, there exist conflicts among multimodal conditions (Appendix C.1.2): 1) _Modal discrepancy_: The nuScene dataset contains a full set of RealCond, SimCond, and ExtraCond, while YouTube only includes RealCond. This might impact the quality of images generated based on nuScenes layouts due to the data bias for diffusion models . 2) _Condition disparity_: The lack of rich background information in simulated conditions compared to real ones results in a struggle between the two modalities. In real-world images, the background might contain urban buildings with drastically different facades and street trees of different species. Although CondDiff can convert SimCond to RealCond, the domain gap prevents the same transformation for ExtraCond (_e.g._, rendered RGB, instance segmentation, and top-down views) from the simulator. Thus, we propose using a unified adapter in ImgDiff to address these issues. Its essence lies in mapping variable conditions into fixed-length vectors, overcoming the misalignment of low-level features, and enabling a unified control input interface for the diffusion model.

Mitigating condition conflicts with adapters.Adapters are essential at the guiding branch of image generation to ensure the model learns necessary, unique, non-conflicting information from all conditions. Inspired by UniControl , we devised a set of convolutional modules as the adapters to capture features from various modalities, as shown in the ImgDiff in the lower right of Fig. 3. For a set of input conditions \(=\{^{1},^{2},...,^{K}\}\), each condition undergoes feature extraction via the

   Dataset & RealCond & SimCond & ExtraCond \\  nuScenes & âœ“ & & \\  DIVA-Real & âœ“ & & \\ DIVA-Sim & & âœ“ & âœ“ \\   

Table 2: **Formats of conditions.** Real/SimCond: depth and segmentation; ExtraCond: rendered RGB, instance maps, and top-down views.

unified adapter \(_{}\) represented as:

\[_{}(^{k}):=_{i=1}^{K}_{i=k}_{}^{(i)}(_{}^{(i)}(^{k} ^{k})),\] (5)

where \(\) is the indicator function, \(^{k}\) is the \(k\)-th condition image, and \(_{}^{(i)}\), \(_{}^{(i)}\) are the convolution layers of the \(i\)-th module of the adapter. \(^{k}\) is the valid mask for each condition.

The valid mask is the key to mitigating conflicts. The entire mask will be padded with 0 if a condition is missing or not provided. For simulator-generated conditions, we set the masks of backgrounds to 0 based on the semantic labels, preventing unwanted constraints on background generation. Since top-down view conditions don't belong to the frontal perspective, all information is retained. Ultimately, two convolutional layers process the concatenated condition features, max pooling them into a fixed-length feature vector for control.

**Controllable image generation.** We utilize the ControlNet  to guide image generation. After the feature extraction by \(_{}\), conditions are encoded into the UNet model. Then, the model injects control information into each UNet layer through residual connections. All parameters in UNet's input and middle layers are frozen, and we only fine-tune the output layers and the control branch.

## 4 Experiments

**Setup and protocols.** SimGen is learned in two stages on DIVA and nuScenes dataset . The performance is evaluated based on image quality, controllability, and diversity. The Frame-wise Frechet Inception Distance (FID) evaluates the synthesized data's quality. SimGen's controllability corresponds to how well the generated images align with ground truths from the nuScenes validation set. The controllability is measured by the 3D detection metrics (AP) and BEV segmentation metrics (mIoU) when applying out-of-the-box perception models on the generated images. Lastly, diversity is measured using the pixel variance of the generated images. More details on training, sampling, and evaluation metrics are provided in Appendix C and Appendix D.

### Comparison to nuScenes-specific models.

**Comparison to nuScenes-specific models.** We compare SimGen with the most recently available data generation approaches exclusively trained on nuScenes. Tab. 3 shows that SimGen surpasses all previous methods in image quality (FID) and diversity (\(D_{}\)). Specifically, SimGen significantly increases \(D_{}\) by **+6.5** compared to DrivingDiffusion . For fair comparisons, we train a model variant (SimGen-nuSc) on the nuScenes dataset only. We find that although SimGen-nuSc performs on par with SimGen on nuScenes, its performance in diversity is less than ideal, and it struggles to generalize to novel appearances like Desert, Mountains, and Blizzard, where the generation degrades to the nuScenes visual pattern. In contrast, SimGen trained on DIVA exhibits strong generalization ability across appearances as shown in Fig. 4.

**Controllability for autonomous driving.** The controllability of our method is quantitatively assessed based on the perception performance metrics obtained using a single-frame version of BEVFusion . We feed the data from nuScenes validation set into SimGen and generate the driving images. Then, the perception performance of pre-trained BEVFusion, involving map segmentation (mIoU) and 3D object detection (AP), is recorded. Compared to the perception scores on the raw nuScenes data, the relative performance metrics serve as the indicators of the alignment between the generated images and the conditions. As depicted in Tab. 4, SimGen achieves a relative performance of **-3.3** on map segmentation of vehicles, underscoring a robust alignment of the generated samples.

   Method & Dataset & FID\(\) & \(D_{}\) \(\) \\  BEVGen  & & 25.5 & 17.0 \\ BEVControl  & & 24.9 & - \\ MagicDrive  & nuScenes & 16.6 & 19.7 \\ Panacea  & & 17.0 & - \\ DrivingDiffusion  & & 15.9 & 20.1 \\  SimGen-nuSc & nuScenes & **15.6** & 20.5 \\
**SimGen** & **DIVA** & **15.6** & **26.6** \\   

Table 3: **Generation quality and diversity compared to nuScenes experts.** The FID and \(D_{}\) indicate the image quality and pixel diversity, respectively. gray\(\) main metric. **bold**: best results.

**Data augmentation via synthetic data.** SimGen can produce augmented data with accurate annotation controls, enhancing the training for perception tasks, _e.g._, map segmentation, and 3D object detection. For these tasks, we augment an equal number of images as in nuScenes dataset, ensuring consistent training iterations and batch sizes for fair comparisons to the baseline. Tab. 5 indicates that blending generated with real data can elevate the singe-frame version of BEVFusion's vehicle mIoU to **39.0**, a **+4.4** uptick compared to models trained purely on real data. These outcomes reinforce SimGen's validity as a controllable synthetic data generator for enhancing perception models.

Figure 4: **Generating diverse appearances conditioned on simulatorâ€™s conditions and texts.** We show the generation results of SimGen (blue boxes) and SimGen-nuSc (gray boxes) under the same conditions. Compared to models confined to limited datasets, SimGen exhibits a stronger ability to generate more realistic and diverse driving scenarios. Reference is not used for inference.

### Ablation Study

The ablation is conducted by training each variant of our model on a DIVA subset with 30K frames, and we report FID and average precision of cars (AP\({}_{}\)) as the quality and controllability metrics. We gradually introduce our proposed components and conditions, starting with a ControlNet baseline  that directly takes SimCont as input. As shown in Tab. 6, by introducing a cascade pipeline to transform SimCond into RealCond, the FID significantly reduces by **-2.3**, as the transformed conditions closely resemble real scenarios. Including simulator-pulled ExtraCond to the control conditions improves the alignment of the generated images with the target layouts, effectively enhancing the AP\({}_{}\) by **+1.3**. However, a slight deterioration in the FID metric (**+0.5**) may result from condition conflicts. Lastly, using a Unified Adapter helps alleviate conflicts, significantly improving generated image quality by **-0.8**. The effectiveness of ExtraCond is exhibited in Fig. 6, where the addition of instance map, rendered RGB, and top-down view enables the model to better handle object boundaries, orientation angles, and occlusions in these cases. To the best of image quality, we only use depth and segmentation conditions in subsequent experiments.

### Discussions

**Extension to video generation.** SimGen is not designed for video generation. But the high-quality image generation brings a potential for video generation, which is important for interactive scene generation and closed-loop planning. We have a preliminary attempt by integrating temporal attention layers into UNet similar to , and then conducting subsequent training stages focusing solely on learning the newly added layers while freezing the original parameters. This shows a promising result of temporal consistency across frames, as compared with video generation models in Appendix D.2.

**Generating safety-critical scenarios.** The key innovation of SimGen is the controllability of layouts brought by connecting to a driving simulator. Building upon video generation, we showcase SimGen's

   Ablation & FID\(\) & AP\({}_{}\) \\  Baseline & 19.5 & 45.7 \\ + Cascade Pipeline & 17.2 & 46.3 \\ + ExtraCond & 17.7 & 47.6 \\ + Unified Adapter & **16.9** & 48.2 \\   

Table 6: **Ablation on designs in SimGen. All proposed designs contribute to the final performance.**

    &  &  \\   & mIoU\({}_{}\) & mIoU\({}_{}\) & AP\({}_{}\) & AP\({}_{}\) \\  Oracle & 72.2 & \(34.6\) & \(47.0\) & 21.4 \\ BEVGen  & 50.1 (-21.1) & **5.9** (-28.7) & **24.7** (-22.3) & **9.1** (-15.0) \\ MagicD.  & 58.6 (-13.6) & **29.5** (-5.1) & **37.3** (-9.7) & **17.3** (-4.1) \\  SimGen-nuSc & 60.6 (-11.6) & **29.9** (-47.7) & **39.1** (-7.9) & **18.1** (-3.3) \\
**SimGen** & **62.9** (-9.3) & **31.2** (-34) & **41.0** (-6.9) & **19.6** (-1.8) \\   

Table 4: **Generation controllability for perception tasks. Oracle: a single-frame version of BEVFu-sion . In blue is the relative drop compared to standard nuScenes validation data.**

Figure 5: **Generating safety-critical scenes. SimGen can also recreate image sequences of safety-critical scenes where risky driving behaviors like sudden braking and merging happen.**generalized capabilities in novel layouts, specifically in safety-critical scenarios in Fig. 5. The visualized layout is initialized from a scenario sampled from the Waymo Open dataset  and then populated with risky behaviors via an adversarial interaction traffic flow generation method . SimGen can transform safety-critical driving scenarios from the simulator into realistic sequential images, including risky behaviors like sudden braking, crossroad meeting, merging, sharp turning, _etc_. This application is impossible with existing models, which are only trained and conditioned on a given static real-world dataset that lacks records of dangerous driving behaviors. This brings new opportunities for closed-loop data generation capabilities (Appendix D.2).

## 5 Related Work

**Diffusion-based generative Models.** Diffusion models have made significant strides in image generation [15; 45; 49; 51; 57; 62] and video generation [4; 24]. Recent works incorporate additional control signals beyond text prompts [23; 39; 47]. ControlNet  integrates a trainable copy of the SD encoder for control signals. Studies like Uni-ControlNet  and UniControl  have also focused on fusing multimodal inputs into a unified control condition using input-level adapter structures. Our method distinguishes itself in its capability of multimodal conditioned generation by addressing the sim-to-real gap and condition conflicts in the complex realm of driving scenarios.

**Controllable generation for autonomous driving.** Autonomous driving research heavily relies on paired data and layout ground truths, spurring numerous studies on their generation [11; 44]. Some works [21; 28; 78] utilize diffusion models to generate future driving scenes based on historical information, but they lack the ability to control scenes through layout. Other generative methods, like BEVGen  and BEVControl , use BEV layouts to create synthetic single or multi-view images. Recent innovative method Panacea  generates panoramic and controllable videos, while MagicDrive  offers diverse 3D controls and tailored encoding strategies. Lastly, DriveDreamer  and DrivingDiffusion  employ diffusion models for realistic multi-view video generation and environment representation. Yet, these works are confined to limited appearances and layouts of static datasets, restraining their real-world applicability and the controllability over the layouts that deviate from the dataset, such as the safety-critical scenarios.

**Scenario generation via simulators.** Driving simulators [16; 36] are fundamental to autonomous driving development, providing controlled simulations that mimic reality. Notable studies include SYNTHIA , AIODrive , and GTA-V  that generate virtual images and annotations. SHIFT  diversifies with environmental changes, while CAT  creates safety-critical scenarios for targeted training from real-world logs. Despite their layout diversity and attempts at photorealism enhancement , the simulated images lack realism. In this work, we bridge the two worlds to obtain both the appearance diversity from diffusion models and the layout controllability from simulators.

## 6 Conclusion

We propose a simulator-conditioned diffusion model, SimGen, that learns to generate diverse driving scenarios by mixing data from the simulator and the real world. A novel dataset containing massive web and simulated driving videos is collected to ensure diverse scene generation and mitigate simulation-to-reality gap. By obtaining diversity in appearance and layout, SimGen exhibits superior data augmentation and zero-shot generalization capabilities in generating diverse and novel scenes.

**Limitations and future work.** SimGen currently does not support multi-view generation, limiting its application in Bird's Eye View models. Inheriting the drawbacks of diffusion models, SimGen suffers from long inference time, which may impact the applications like closed-loop training. The study of extending SimGen to video generation is left for future work.

Figure 6: **Ablation study of simulator conditions.**