# Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation

Wenhao Ding\({}^{1}\)   Laixi Shi\({}^{2}\)   Yuejie Chi\({}^{1}\)   Ding Zhao\({}^{1}\)

\({}^{1}\)Carnegie Mellon University  \({}^{2}\)California Institute of Technology

{wenhood,laixis,yuejice}@andrew.cmu.edu, dingzhao@cmu.edu

Equal contribution.

###### Abstract

Robustness has been extensively studied in reinforcement learning (RL) to handle various forms of uncertainty such as random perturbations, rare events, and malicious attacks. In this work, we consider one critical type of robustness against spurious correlation, where different portions of the state do not have correlations induced by unobserved confounders. These spurious correlations are ubiquitous in real-world tasks, for instance, a self-driving car usually observes heavy traffic in the daytime and light traffic at night due to unobservable human activity. A model that learns such useless or even harmful correlation could catastrophically fail when the confounder in the test case deviates from the training one. Although motivated, enabling robustness against spurious correlation poses significant challenges since the uncertainty set, shaped by the unobserved confounder and causal structure, is difficult to characterize and identify. Existing robust algorithms that assume simple and unstructured uncertainty sets are therefore inadequate to address this challenge. To solve this issue, we propose _Robust State-Confounded Markov Decision Processes_ (RSC-MDPs) and theoretically demonstrate its superiority in avoiding learning spurious correlations compared with other robust RL counterparts. We also design an empirical algorithm to learn the robust optimal policy for RSC-MDPs, which outperforms all baselines in eight realistic self-driving and manipulation tasks. Please refer to the website for more details.

## 1 Introduction

Reinforcement learning (RL), aiming to learn a policy to maximize cumulative reward through interactions, has been successfully applied to a wide range of tasks such as language generation , game playing , autonomous driving , etc. While standard RL has achieved remarkable success in simulated environments, a growing trend in RL is to address another critical concern - **robustness** - with the hope that the learned policy still performs well when the deployed (test) environment deviates from the nominal one used for training . Robustness is highly desirable since the performance of the learned policy could significantly deteriorate due to the uncertainty and variations of the test environment induced by random perturbation, rare events, or even malicious attacks .

Despite various types of uncertainty that have been investigated in RL, this work focuses on the uncertainty of the environment with semantic meanings resulting from some unobserved underlying variables. Such environment uncertainty, denoted as **structured uncertainty**, is motivated by innumerable real-world applications but still receives little attention in sequential decision-making tasks . To specify the phenomenon of structured uncertainty, let us consider a concrete example (illustrated in Figure 1) in a driving scenario, where a shift between training and test environments caused by an unobserved confounder can potentially lead to a severe safety issue. Specifically, the observations _brightness_ and _traffic density_ do not have cause and effect on each other but arecontrolled by a confounder (i.e. _sun_ and _human activity_) that is usually unobserved 2 to the agent. During training, the agent could memorize the **spurious state correlation** between _brightness_ and _traffic density_, i.e., the traffic is heavy during the daytime but light at night. However, such correlation could be problematic during testing when the value of the confounder deviates from the training one, e.g., the traffic becomes heavy at night due to special events (_human activity_ changes), as shown at the bottom of Figure 1. Consequently, the policy dominated by the spurious correlation in training fails on out-of-distribution samples (observations of heavy traffic at night) in the test scenarios.

The failure of the driving example in Figure 1 is attributed to the widespread and harmful spurious correlation, namely, the learned policy is not robust to the structured uncertainty of the test environment caused by the unobserved confounder. However, ensuring robustness to structured uncertainty is challenging since the targeted uncertain region - the structured uncertainty set of the environment - is carved by the unknown causal effect of the unobserved confounder, and thus hard to characterize. In contrast, prior works concerning robustness in RL  usually consider a homogeneous and structure-agnostic uncertainty set around the state [9; 6; 10], action [11; 12], or the training environment [13; 14; 15] measured by some heuristic functions [9; 15; 8] to account for unstructured random noise or small perturbations. Consequently, these prior works could not cope with the structured uncertainty since their uncertainty set is different from and cannot tightly cover the desired structured uncertainty set, which could be heterogeneous and allow for potentially large deviations between the training and test environments.

In this work, to address the structured uncertainty, we first propose a general RL formulation called State-confounded Markov decision processes (SC-MDPs), which model the possible causal effect of the unobserved confounder in an RL task from a causal perspective. SC-MDPs better explain the reason for semantic shifts in the state space than traditional MDPs. Then, we formulate the problem of seeking robustness to structured uncertainty as solving Robust SC-MDPs (RSC-MDPs), which optimizes the worst performance when the distribution of the unobserved confounder lies in some uncertainty set. The key contributions of this work are summarized as follows.

* We propose a new type of robustness with respect to structured uncertainty to address spurious correlation in RL and provide a formal mathematical formulation called RSC-MDPs, which are well-motivated by ubiquitous real-world applications.
* We theoretically justify the advantage of the proposed RSC-MDP framework against structured uncertainty over the prior formulation in robust RL without semantic information.
* We implement an empirical algorithm to find the optimal policy of RSC-MDPs and show that it outperforms the baselines on eight real-world tasks in manipulation and self-driving.

## 2 Preliminary and Limitations of Robust RL

In this section, we first introduce the preliminary formulation of standard RL and then discuss a natural type of robustness that is widely considered in the RL literature and most related to this work - robust RL.

**Standard Markov decision processes (MDPs).** An episodic finite-horizon standard MDP is represented by \(=,,T,r,P}\), where \(^{n}\) and \(^{d_{A}}\) are the state and action spaces, respectively, with \(n/d_{A}\) being the dimension of state/action. Here, \(T\) is the length of the horizon; \(P=\{P_{t}\}_{1 t T}\), where \(P_{t}:()\) denotes the probability transition kernel at time step \(t\), for all \(1 t T\); and \(r=\{r_{t}\}_{1 t T}\) denotes the reward function, where \(r_{t}:\) represents the deterministic immediate reward function. A policy (action selection rule) is denoted by \(=\{_{t}\}_{1 t T}\), namely, the policy at time step \(t\) is \(_{t}:()\) based on the current state \(s_{t}\) as \(_{t}(\,|\,s_{t})\). To represent the long-term cumulative

Figure 1: A model trained only with heavy traffic in the daytime learns the spurious correlation between brightness and traffic density and could fail to drive in light traffic in the daytime.

reward, the value function \(V_{t}^{,P}:\) and Q-value function \(Q_{t}^{,P}:\) associated with policy \(\) at step \(t\) are defined as \(V_{t}^{,P}(s)=_{,P}_{k=t}^{T}r_{k}(s_{k},a_{k}) s _{k}=s\) and \(Q_{t}^{,P}(s,a)=_{,P}_{k=t}^{T}r_{k}(s_{k},a_{k})  s_{t}=s,a_{t}=a\), where the expectation is taken over the sample trajectory \(\{(s_{t},a_{t})\}_{1 t T}\) generated following \(a_{t}_{t}( s_{t})\) and \(s_{t+1} P_{t}( s_{t},a_{t})\).

**Robust Markov decision processes (RMDPs).** As a robust variant of standard MDPs motivated by distributionally robust optimization, RMDP is a natural formulation to promote robustness to the uncertainty of the transition probability kernel [13; 15], represented as \(_{}=,,T,r,^{ }(P^{0})}\). Here, we reuse the definitions of \(,,T,r\) in standard MDPs, and denote \(^{}(P^{0})\) as an uncertainty set of probability transition kernels centered around a nominal transition kernel \(P^{0}=\{P^{0}_{t}\}_{1 t T}\) measured by some 'distance' function \(\) with radius \(\). In particular, the uncertainty set obeying the \((s,a)\)-rectangularity  can be defined over all \((s,a)\) state-action pairs at each time step \(t\) as

\[^{}(P^{0})\,^{}(P^{0}_{t,s,a}),^{}(P^{0}_{t,s,a})P_{t,s,a} ():(P_{t,s,a},P^{0}_{t,s,a})}\,,\] (1)

where \(\) denotes the Cartesian product. Here, \(P_{t,s,a} P_{t}(\,|\,s,a)()\) and \(P^{0}_{t,s,a} P^{0}_{t}(\,|\,s,a)()\) denote the transition kernel \(P_{t}\) or \(P^{0}_{t}\) at each state-action pair \((s,a)\) respectively. Consequently, the next state \(s_{t+1}\) follows \(s_{t+1} P_{t}(\,|\,s_{t},a_{t})\) for any \(P_{t}^{}(P^{0}_{t,s_{t},a_{t}})\), namely, \(s_{t+1}\) can be generated from any transition kernel belonging to the uncertainty set \(^{}(P^{0}_{t,s_{t},a_{t}})\) rather than a fixed one in standard MDPs. As a result, for any policy \(\), the corresponding _robust value function_ and _robust Q function_ are defined as

\[V_{t}^{,}(s)_{P^{}(P^{0})}V_{t}^{ ,P}(s), Q_{t}^{,}(s,a)_{P^{ }(P^{0})}Q_{t}^{,P}(s,a),\] (2)

which characterize the cumulative reward in the worst case when the transition kernel is within the uncertainty set \(^{}(P^{0})\). Using samples generated from the nominal transition kernel \(P^{0}\), the goal of RMDPs is to find an optimal robust policy that maximizes \(V_{1}^{,}\) when \(t=1\), i.e., perform optimally in the worst case when the transition kernel of the test environment lies in a prescribed uncertainty set \(^{}(P^{0})\).

**Lack of semantic information in RMDPs.** In spite of the rich literature on robustness in RL, prior works usually hedge against the uncertainty induced by unstructured random noise or small perturbations, specified as a small and homogeneous uncertainty set around the nominal one. For instance, in RMDPs, people usually prescribe the uncertainty set of the transition kernel using a heuristic and simple function \(\) with a relatively small \(\). However, the unknown uncertainty in the real world could have a complicated and semantic structure that cannot be well-covered by a homogeneous ball regardless of the choice of the uncertainty radius \(\), leading to either over conservative policy (when \(\) is large) or insufficient robustness (when \(\) is small). Altogether, we obtain the natural motivation of this work: _How to formulate such structured uncertainty and ensure robustness against it?_

## 3 Robust RL against Structured Uncertainty from a Causal Perspective

To describe structured uncertainty, we choose to study MDPs from a causal perspective with a basic concept called the structural causal model (SCM). Armed with the concept, we formulate State-confounded MDPs - a broader set of MDPs in the face of the unobserved confounder in the state space. Next, we provide the main formulation considered in this work - robust state-confounded MDPs, which promote robustness to structured uncertainty.

**Structural causal model.** We denote a structural causal model (SCM)  by a tuple \(\{X,Y,F,P^{x}\}\), where \(X\) is the set of exogenous (unobserved) variables, \(Y\) is the set of endogenous (observed) variables, and \(P^{x}\) is the distribution of all the exogenous variables. Here, \(F\) is the set of structural functions capturing the causal relations between \(X\) and \(Y\) such that for each variable \(y_{i} Y\), \(f_{i} F\) is defined as \(y_{i} f_{i}((y_{i}),x_{i})\), where \(x_{i} X\) and \((y_{i}) Y y_{i}\) denotes the parents of the node \(y_{i}\). We say that a pair of variables \(y_{i}\) and \(y_{j}\) are confounded by a variable \(C\) (confounder) if they are both caused by \(C\), i.e., \(C(y_{i})\) and \(C(y_{j})\). When two variables \(y_{i}\) and \(y_{j}\) do not have direct causality, they are still correlated if they are confounded, in which case this correlation is called spurious correlation.

### State-confounded MDPs (SC-MDPs)

We now present state-confounded MDPs (SC-MDPs), whose probabilistic graph is illustrated in Figure 2(a) with a comparison to standard MDPs in Figure 2(b). Besides the components in standard MDPs \(=\{,,T,r\}\), we introduce a set of unobserved confounder \(C_{}=\{c_{t}\}_{1 t T}\), where \(c_{t}\) denotes the confounder that is generated from some unknown but fixed distribution \(P_{t}^{c}\) at time step \(t\), i.e., \(c_{t} P_{t}^{c}()\).

To characterize the causal effect of the confounder \(C_{}\) on the state dynamic, we resort to an SCM, where \(C_{}\) is the set of exogenous (unobserved) confounder and endogenous variables include all dimensions of states \(\{s_{t}^{i}\}_{1 i n,1 t T}\), and actions \(\{a_{t}\}_{1 t T}\). Specifically, the structural function \(F\) is considered as \(\{_{t}^{i}\}_{1 i n,1 t T}\) - the transition from the current state \(s_{t}\), action \(a_{t}\) and the confounder \(c_{t}\) to each dimension of the next state \(s_{t+1}^{i}\) for all time steps, i.e., \(s_{t+1}^{i}_{t}^{i}(\,|\,s_{t},a_{t},c_{t})\). Notably, the specified SCM does not confound the reward, i.e., \(r_{t}(s_{t},a_{t})\) does not depend on the confounder \(c_{t}\).

Armed with the above SCM, denoting \(P^{c}\{P_{t}^{c}\}\), we can introduce state-confounded MDPs (SC-MDPs) represented by \(_{}=\{,,T,r,,\{ _{t}^{i}\},P^{c}\}\) (Figure 2(a)). A policy is denoted as \(=\{_{t}\}\), where each \(_{t}\) results in an intervention (possibly stochastic) that sets \(a_{t}_{t}(\,|\,s_{t})\) at time step \(t\) regardless of the value of the confounder.

**State-confounded value function and optimal policy.** Given \(s_{t}\), the causal effect of \(a_{t}\) on the next state \(s_{t+1}\) plays an important role in characterizing value function/Q-function. To ensure the identifiability of the causal effect, the confounder \(c_{t}\) are assumed to obey the backdoor criterion [17; 18], leading to the following _state-confounded value function_ (SC-value function) and _state-confounded Q-function_ (SC-Q function) :

\[_{t}^{,P^{c}}(s) =_{,P^{c}}[_{k=t}^{T}r_{k}(s_{k},a_{k}) s _{t}=s;c_{k} P_{k}^{c},s_{k+1}^{i}_{k}^{i}(\,|\,s_{k},a_{k},c_{k})],\] \[_{t}^{,P^{c}}(s,a) =_{,P^{c}}[_{k=t}^{T}r_{k}(s_{k},a_{k}) s _{t}=s,a_{t}=a;c_{k} P_{k}^{c},s_{k+1}^{i}_{k}^{i}(\,| \,s_{k},a_{k},c_{k})].\] (3)

_Remark_ 1. Note that the proposed SC-MDPs serve as a general formulation for a broad family of RL problems that include standard MDPs as a special case. Specifically, any standard MDP \(=\{,,P,T,r\}\) can be equivalently represented by at least one SC-MDP \(_{}=\{,,T,r,.\)\(.,\{_{t}^{i}\},P^{c}\}\) as long as \(_{c_{t} P_{t}^{c}}[_{t}^{i}(\,|\,s_{t},a_{t },c_{t})]=[P(\,|\,s_{t},a_{t})]_{i}\) for all \(1 i n,1 t T\).

### Robust state-confounded MDPs (RSC-MDPs)

In this work, we consider robust state-confounded MDPs (RSC-MDPs) - a variant of SC-MDPs promoting the robustness to the uncertainty of the unobserved confounder distribution \(P^{c}\), denoted by \(_{}=\{,,T,r,,\{ _{t}^{i}\},^{}(P^{c})\}\). Here, the perturbed distribution of the unobserved confounder is assumed in an uncertainty set \(^{}(P^{c})\) centered around the nominal distribution

Figure 2: The probabilistic graphs of our formulation (SC-MDP) and other related formulations (specified in Appendix B.1 due to the limited space). \(s_{t}^{1}\) means the first dimension of \(s_{t}\). \(s_{t^{}}\) is a shorthand for \(s_{t+1}\). In SC-MDP, the orange line represents the backdoor path from state \(s_{t^{}}^{1}\) to action \(a_{t^{}}\) opened by the confounder \(c_{t}\), which makes the learned policy \(\) rely on the value of \(c_{t}\).

with radius \(\) measured by some 'distance' function \(:()()^{+}\), i.e.,

\[^{}(P^{c})^{}(P^{c}_{t}), ^{}(P^{c}_{t})\{P(): (P,P^{c}_{t})\}\,.\] (4)

Consequently, the corresponding _robust SC-value function_ and _robust SC-Q function_ are defined as

\[^{,}_{t}(s)_{P^{}(P^{ c})}^{,P}_{t}(s),^{,}_{t}(s,a) _{P^{}(P^{c})}^{,P}_{t}(s,a),\] (5)

representing the worst-case cumulative rewards when the confounder distribution lies in the uncertainty set \(^{}(P^{c})\).

Then a natural question is: does there exist an optimal policy that maximizes the robust SC-value function \(^{,}_{t}\) for any RSC-MDP so that we can target to learn? To answer this, we introduce the following theorem that ensures the existence of the optimal policy for all RSC-MDPs. The proof can be found in Appendix C.1.

**Theorem 1** (Existence of an optimal policy).: _Let \(\) be the set of all non-stationary and stochastic policies. Consider any RSC-MDP, there exists at least one optimal policy \(^{,}=\{^{,}_{t}\}_{1 t T}\) such that for all \((s,a)\) and \(1 t T\), one has_

\[^{^{,},}_{t}(s)=^{, }_{t}(s)_{}^{,}_{t}(s) ^{^{,},}_{t}(s,a)= ^{,}_{t}(s,a)_{}^ {,}_{t}(s,a).\]

In addition, RSC-MDPs also possess benign properties similar to RMDPs such that for any policy \(\) and the robust optimal policy \(^{,}\), the corresponding _robust SC Bellman consistency equation_ and _robust SC Bellman optimality equation_ are also satisfied (specified in Appendix C.3.3).

**Goal.** Based on all the definitions and analysis above, this work aims to find an optimal policy for RSC-MDPs that maximizes the robust SC-value function in (5), yielding optimal performance in the worst case when the unobserved confounder distribution falls into an uncertainty set \(^{}(P^{c})\).

### Advantages of RSC-MDPs over traditional RMDPs

The most relevant robust RL formulation to ours is RMDPs, which has been introduced in Section 2. Here, we provide a thorough comparison between RMDPs and our RSC-MDPs with theoretical justifications, and leave the comparisons and connections to other related formulations in Figure 2 and Appendix B.1 due to space limits.

To begin, at each time step \(t\), RMDPs explicitly introduce uncertainty to the transition probability kernels, while our RSC-MDPs add uncertainty to the transition kernels in a latent (and hence more structured) manner via perturbing the unobserved confounder that partly determines the transition kernels. As an example, imagining the true uncertainty set encountered in the real world is illustrated as the blue region in Figure 3, which could have a complicated structure. Since the uncertainty set in RMDPs is homogeneous (illustrated by the green circles), one often faces the dilemma of being either too conservative (when \(\) is large) or too reckless (when \(\) is small). In contrast, the proposed RSC-MDPs - shown in Figure 3 - take advantage of the structured uncertainty set (illustrated by the orange region) enabled by the underlying SCM, which can potentially lead to much better estimation of the true uncertainty set. Specifically, the varying unobserved confounder induces diverse perturbation to different portions of the state through the structural causal function, enabling heterogeneous and structural uncertainty sets over the state space.

**Theoretical guarantees of RSC-MDPs: advantages of structured uncertainty.** To theoretically understand the advantages of the proposed robust formulation of RSC-MDPs with comparison to prior works, especially RMDPs, the following theorem verifies that RSC-MDPs enable additional robustness against semantic attack besides small model perturbation or noise considered in RMDPs. The proof is postponed to Appendix C.2.

Figure 3: (a) RMDPs add homogeneous noise to states, while (b) RSC-MDPs perturb the confounder to influence states, resulting in a subset of the valid space.

**Theorem 2**.: _Consider any \(T 2\). Consider some standard MDPs \(=\{,,P^{0},T,r\}\), equivalently represented as an SC-MDP \(_{}=\{,,T,r,,\{ ^{i}_{t}\},P^{c}\}\) with \(\{0,1\}\), and total variation as the 'distance' function \(\) to measure the uncertainty set (the admissible uncertainty level obeys \(\)). For the corresponding RMDP \(_{}\) with the uncertainty set \(^{_{1}}(P^{0})\), and the proposed RSC-MDP \(_{}=\{,,T,r,,\{^{i}_{t}\},^{_{2}}(P^{c})\}\), the optimal robust policy \(^{,_{1}}_{}\) associated with \(_{}\) and \(^{,_{2}}_{}\) associated with \(_{}\) obey: given \(_{2}(,1]\), there exist RSC-MDPs with some initial state distribution \(\) such that_

\[_{1}^{^{,_{2}}_{},_{2}}()- _{1}^{^{,_{1}}_{},_{2}}() ,_{1}.\] (6)

In words, Theorem 2 reveals a fact about the proposed RSC-MDPs: _RSC-MDPs could succeed in intense semantic attacks while RMDPs fail_. As shown by (6), when fierce semantic shifts appear between the training and test scenarios - perturbing the unobserved confounder in a large uncertainty set \(^{_{2}}(P^{c})\), solving RSC-MDPs with \(^{,_{2}}_{}\) succeeds in testing while \(^{,_{1}}_{}\) trained by solving RMDPs can fail catastrophically. The proof is achieved by constructing hard instances of RSC-MDPs that RMDPs could not cope with due to inherent limitations. Moreover, this advantage of RSC-MDPs is consistent with and verified by the empirical performance evaluation in Section 5.3 R1.

## 4 An Empirical Algorithm to Solve RSC-MDPs: RSC-SAC

``` Input: policy \(\), data buffer \(\), transition model \(P_{}\), ratio of modified data \(\) for\(t[1,T]\)do  Sample action \(a_{t}(|s_{t})\) \((s_{t+1},r_{t})(s_{t},a_{t})\) Add buffer \(=\{s_{t},a_{t},s_{t+1},r_{t}\}\) forsample batch \(\)do  Randomly select \(\%\) data in \(\)  Modify \(s_{t}\) in selected data with (7) \((_{t+1},_{t}) P_{}(s_{t},a_{t},_{})\)  Replace data with \((s_{t},a_{t},_{t+1},_{t})\) \(=\|s_{t+1}-_{t+1}\|_{2}^{2}+\|r_{t}-_{t}\|_{2}^{2}\)  Update \(\) and \(\) with \(+\|\|_{p}\)  Update \(\) with SAC algorithm ```

**Algorithm 1**RSC-SAC Training

When addressing distributionally robust problems in RMDPs, the worst case is typically defined within a prescribed uncertainty set in a clear and implementation-friendly manner, allowing for iterative or analytical solutions. However, solving RSC-MDPs could be challenging as the structured uncertainty set is induced by the causal effect of perturbing the confounder. The precise characterization of this structured uncertainty set is difficult since neither the unobserved confounder nor the true causal graph of the observable variables is accessible, both of which are necessary for intervention or counterfactual reasoning. Therefore, we choose to approximate the causal effect of perturbing the confounder by learning from the data collected during training.

In this section, we propose an intuitive yet effective empirical approach named RSC-SAC for solving RSC-MDPs, which is outlined in Algorithm 1. We first estimate the effect of perturbing the distribution \(P^{c}\) of the confounder to generate new states (Section 4.1). Then, we learn the structural causal model \(^{i}_{t}\) to predict rewards and the next states given the perturbed states (Section 4.2). By combining these two components, we construct a data generator capable of simulating novel transitions \((s_{t},a_{t},r_{t},s_{t+1})\) from the structured uncertainty set. To learn the optimal policy, we construct the data buffer with a mixture of the original data and the generated data and then use the Soft Actor-Critic (SAC) algorithm  to optimize the policy.

### Distribution of confounder

As we have no prior knowledge about the confounders, we choose to approximate the effect of perturbing them without explicitly estimating the distribution \(P^{c}\). We first randomly select a dimension \(i\) from the state \(s_{t}\) to apply perturbation and then assign the dimension \(i\) of \(s_{t}\) with a heuristic rule. We select the value from another sample \(s_{k}\) that has the most different value from \(s_{t}\) in dimension \(i\) and the most similar value to \(s_{t}\) in the remaining dimensions. Formally, this process solves the following optimization problem to select sample \(k\) from a batch of \(K\) samples:

\[s_{t}^{i} s_{k}^{i},\ k=^{i}-s_{k}^{i}\|_{2}^{2}} {_{ i}\|s_{t}^{ i}-s_{k}^{ i}\|_{2}^{2}},\ k\{1,...,K\}\] (7)

where \(s_{t}^{i}\) and \(s_{t}^{ i}\) means dimension \(i\) of \(s_{t}\) and other dimensions of \(s_{t}\) except for \(i\), respectively. Intuitively, permuting the dimension of two samples breaks the spurious correlation and remains themost semantic meaning of the state space. However, this permutation sometimes also breaks the true cause and effect between dimensions, leading to a performance drop. The trade-off between robustness and performance  is a long-standing dilemma in the robust optimization framework, which we will leave to future work.

### Learning of structural causal model

With the perturbed state \(s_{t}\), we then learn an SCM to predict the next state and reward considering the effect of the action on the previous state. This model contains a causal graph to achieve better generalization to unseen state-action pairs. Specifically, we simultaneously learn the model parameter and discover the underlying causal graph in a fully differentiable way with \((_{t+1},_{t}) P_{}(s_{t},a_{t},_{})\), where \(\) is the parameter of the neural network of the dynamic model and \(^{(n+d_{})(n+1)}\) is the parameter to represent causal graph \(\) between \(\{s_{t},a_{t}\}\) and \(\{s_{t+1},r_{t}\}\). This graph is represented by a binary adjacency matrix \(\), where \(1/0\) means the existence/absence of an edge. \(P_{}\) has an encoder-decoder structure with matrix \(\) as an intermediate linear transformation. The encoder takes state and action in and outputs features \(f_{e}^{(n+d_{}) d_{f}}\) for each dimension, where \(d_{f}\) is the dimension of the feature. Then, the causal graph is multiplied to generate the feature for the decoder \(f_{d}=f_{e}^{T}^{d_{f}(n+1)}\). The decoder takes in \(f_{d}\) and outputs the next state and reward. The detailed architecture of this causal transition model can be found in Appendix D.1.

The objective for training this model consists of two parts, one is the supervision signal from collected data \(\|s_{t+1}-_{t+1}\|_{2}^{2}+\|r_{t}-_{t}\|_{2}^{2}\), and the other is a penalty term \(\|\|_{p}\) with weight \(\) to encourage the sparsity of the matrix \(\). The penalty is important to break the spurious correlation between dimensions of state since it forces the model to eliminate unnecessary inputs for prediction.

## 5 Experiments and Evaluation

In this section, we first provide a benchmark consisting of eight environments with spurious correlations, which may be of independent interest to robust RL. Then we evaluate the proposed algorithm RSC-SAC with comparisons to prior robust algorithms in RL.

### Tasks with spurious correlation

To the best of our knowledge, no existing benchmark addresses the issues of spurious correlation in the state space of RL. To bridge the gap, we design a benchmark consisting of eight novel tasks in self-driving and manipulation domains using the Carla  and Robosuite  platforms (shown in Figure 4). Tasks are designed to include spurious correlations in terms of human common sense, which is ubiquitous in decision-making applications and could cause safety issues. We categorize the tasks into _distraction correlation_ and _composition correlation_ according to the type of spurious correlation. We specify these two types of correlation below and leave the full descriptions of the tasks in Appendix D.2.

* **Distraction correlation** is between task-relevant and task-irrelevant portions of the state. The task-irrelevant part could distract the policy model from learning important features and lead to a performance drop. A typical method to avoid distraction is background augmentation [24; 25]. We design four tasks with this category of correlation, i.e., _Lift_, _Wipe_, _Brightness_, and _CarType_.
* **Composition correlation** is between two task-relevant portions of the state. This correlation usually exists in compositional generalization, where states are re-composed to form novel tasks during testing. Typical examples are multi-task RL [26; 27] and hierarchical RL [28; 29]. We design four tasks with this category of correlation, i.e., _Stack_, _Door_, _Behavior_, and _Crossing_.

Figure 4: Two tasks used in experiments. _Door_ is a composition task implemented in Robosuite with a spurious correlation between the positions of the handle and the door. _Brightness_ is a distraction task implemented in Carla with a spurious correlation between the number vehicles and day/night.

### Baselines

Robustness in RL has been explored in terms of diverse uncertainty set over state, action, or transition kernels. Regarding this, we use a non-robust RL and four representative algorithms of robust RL as baselines, all of which are implemented on top of the SAC  algorithm. **Non-robust RL (SAC):** This serves as a basic baseline without considering any robustness during training; **Solving robust MDP:** We generate the samples to cover the uncertainty set over the state space by adding perturbation around the nominal states that follows two distribution, i.e., uniform distribution (RMDP-U) and Gaussian distribution (RMDP-G). **Solving SA-MDP:** We compare ATLA , a strong algorithm that generates adversarial states using an optimal adversary within the uncertainty set. **Invariant feature learning:** We choose DBC  that learns invariant features using the bi-simulation metric  and  (Active) that actively sample uncertain transitions to reduce causal confusion. **Counterfactual data augmentation:** We select MoCoDA , which identifies local causality to switch components and generate counterfactual samples to cover the targeted uncertainty set. We adapt this algorithm using an approximate causal graph rather than the true causal graph.

### Results and Analysis

To comprehensively evaluate the performance of the proposed method RSC-SAC, we conduct experiments to answer the following questions: **Q1.** Can RSC-SAC eliminate the harmful effect of spurious correlation in learned policy? **Q2.** Does the robustness of RSC-SAC only come from the sparsity of model? **Q3.** How does RSC-SAC perform in the nominal environments compared to non-robust algorithms? **Q4.** Which module is critical in our empirical algorithm? **Q5.** Is RSC-SAC robust to other types of uncertainty and model perturbation? **Q6.** How does RSC-SAC balance the tradeoff between performance and robustness? We analyze the results and answer these questions in the following.

   Method & Brightness & Behavior & Crossing & CarType & Lift & Stack & Wipe & Door \\  SAC & 0.56\(\)0.13 & 0.13\(\)0.03 & 0.81\(\)0.13 & 0.63\(\)0.14 & 0.58\(\)0.13 & 0.26\(\)0.12 & 0.16\(\)0.20 & 0.08\(\)0.07 \\ RMDP-G & 0.55\(\)0.15 & 0.16\(\)0.04 & 0.47\(\)0.13 & 0.53\(\)0.16 & 0.31\(\)0.08 & 0.33\(\)0.15 & 0.06\(\)0.17 & 0.07\(\)0.03 \\ RMDP-U & 0.54\(\)0.19 & 0.13\(\)0.05 & 0.60\(\)0.15 & 0.39\(\)0.13 & 0.51\(\)0.17 & 0.23\(\)0.11 & 0.06\(\)0.17 & 0.10\(\)0.13 \\ MoCoDA & 0.50\(\)0.14 & 0.16\(\)0.05 & 0.22\(\)0.14 & 0.23\(\)0.12 & 0.46\(\)0.14 & 0.29\(\)0.11 & 0.01\(\)0.24 & 0.09\(\)0.14 \\ ATLA & 0.48\(\)0.11 & 0.14\(\)0.03 & 0.61\(\)0.14 & 0.52\(\)0.14 & 0.61\(\)0.18 & 0.21\(\)0.12 & 0.29\(\)0.18 & 0.28\(\)0.19 \\ DBC & 0.52\(\)0.18 & 0.16\(\)0.03 & 0.68\(\)0.12 & 0.45\(\)0.10 & 0.12\(\)0.02 & 0.03\(\)0.02 & 0.19\(\)0.35 & 0.01\(\)0.01 \\ Active & 0.47\(\)0.14 & 0.14\(\)0.03 & 0.83\(\)0.09 & 0.77\(\)0.14 & 0.35\(\)0.09 & 0.24\(\)0.12 & 0.17\(\)0.17 & 0.05\(\)0.02 \\  RSC-SAC & **0.99\(\)0.11** & **1.02\(\)0.09** & **1.04\(\)0.02** & **1.03\(\)0.02** & **0.98\(\)0.04** & **0.77\(\)0.20** & **0.85\(\)0.12** & **0.61\(\)0.17** \\   

Table 1: Testing reward on shifted environments. Bold font means the best reward.

Figure 5: The first row shows the testing reward on the nominal environments, while the second row shows the testing reward on the shifted environments.

**R1.** RSC-SAC **is robust against spurious correlation.** The testing results of our proposed method with comparisons to the baselines are presented in Table 1, where the rewards are normalized by the episode reward of SAC in the nominal environment. The results reveal that RSC-SAC significantly outperforms other baselines in shifted test environments, exhibiting comparable performance to that of vanilla SAC on the nominal environment in 5 out of 8 tasks. An interesting and even surprising finding, as shown in Table 1, is that although RMDP-G, RMDP-U, and ATLA are trained desired to be robust against small perturbations, their performance tends to drop more than non-robust SAC in some tasks. This indicates that using the samples generated from the traditional robust algorithms could harm the policy performance when the test environment is outside of the prescribed uncertainty set considered in the robust algorithms.

**R2. Sparsity of the model is only one reason for the robustness of RSC-SAC.** As existing literature shows , sparsity regularization benefits the elimination of spurious correlation and causal confusion. Therefore, we compare our method with a sparse version of SAC (SAC-Sparse): we add an additional penalty \(|W|_{1}\) during the optimization, where \(W\) is the parameter of the first linear layer of the policy and value networks and \(\) is the weight. The results of both _Distraction_ and _Composition_ are shown in Figure 6. We have two important findings based on the results: (1) The sparsity improves the robustness of SAC in the setting of distraction spurious correlation, which is consistent with the findings in . (2) The sparsity does not help with the composition type of spurious correlation, which indicates that purely using sparsity regularization cannot explain the improvement of our RSC-SAC. In fact, the semantic perturbation in our method plays an important role in augmenting the composition generalization.

**R3.** RSC-SAC **maintains great performance in the nominal environments.** Previous literature  finds out that there usually exists a trade-off between the performance in the nominal environment and the robustness against uncertainty. To evaluate the performance of RSC-SAC in the nominal environment, we conduct experiments and summarize results in Table 2, which shows that RSC-SAC still performs well in the training environment. Additionally, the training curves are displayed in Figure 5, showing that RSC-SAC achieves similar rewards compared to non-robust SAC although converges slower than it.

**R4. Both the distribution of confounder and the structural causal model are critical.** To assess the impact of each module in our algorithm, we conduct three additional ablation studies (in Table 3), where we remove the causal graph \(_{}\), the transition model \(P_{}\), and the distribution of the confounder \(P^{c}\) respectively. The results demonstrate that the learnable causal graph \(_{}\) is critical for the performance that enhances the prediction of

   Method & Brightness & Behavior & Crossing & CarType & Lift & Stack & Wipe & Door \\  SAC & 1.00\(\)0.09 & 1.00\(\)0.08 & 1.00\(\)0.02 & 1.00\(\)0.03 & 1.00\(\)0.03 & 1.00\(\)0.09 & 1.00\(\)0.12 & 1.00\(\)0.03 \\ RMDP-G & 1.04\(\)0.09 & 1.00\(\)0.11 & 0.78\(\)0.05 & 0.79\(\)0.05 & 0.92\(\)0.07 & 0.86\(\)0.14 & 0.99\(\)0.13 & 0.99\(\)0.06 \\ RMDP-U & 1.02\(\)0.09 & 1.04\(\)0.07 & 0.90\(\)0.03 & 0.88\(\)0.03 & 0.97\(\)0.05 & 0.92\(\)0.12 & 0.97\(\)0.14 & 0.88\(\)0.31 \\ MoCoDA & 0.65\(\)0.17 & 0.78\(\)0.15 & 0.57\(\)0.07 & 0.55\(\)0.13 & 0.79\(\)0.17 & 0.27\(\)0.08 & 0.69\(\)0.13 & 0.41\(\)0.22 \\ ATLA & 0.99\(\)0.11 & 0.98\(\)0.11 & 0.89\(\)0.05 & 0.88\(\)0.04 & 0.94\(\)0.08 & 0.88\(\)0.10 & 0.96\(\)0.12 & 0.97\(\)0.05 \\ DBC & 0.75\(\)0.12 & 0.78\(\)0.10 & 0.85\(\)0.08 & 0.86\(\)0.06 & 0.27\(\)0.04 & 0.12\(\)0.08 & 0.31\(\)0.21 & 0.01\(\)0.01 \\ Active & 1.02\(\)0.10 & 1.08\(\)0.06 & 1.00\(\)0.02 & 1.00\(\)0.02 & 0.99\(\)0.03 & 0.90\(\)0.12 & 0.93\(\)0.20 & 0.99\(\)0.05 \\  RSC-SAC & 0.92\(\)0.31 & 1.06\(\)0.07 & 0.96\(\)0.03 & 0.96\(\)0.03 & 0.96\(\)0.05 & 1.04\(\)0.08 & 0.92\(\)0.14 & 0.98\(\)0.05 \\   

Table 2: Testing reward on nominal environments. Underline means the reward is over 0.9.

   Method & Lift & Behavior & Crossing \\  w/o \(_{}\) & 0.79\(\)0.15 & 0.51\(\)0.24 & 0.87\(\)0.10 \\ w/o \(P_{}\) & 0.75\(\)0.13 & 0.41\(\)0.28 & 0.89\(\)0.08 \\ w/o \(P^{c}\) & 0.90\(\)0.09 & 0.66\(\)0.21 & 0.96\(\)0.04 \\  Full model & 0.98\(\)0.04 & 1.02\(\)0.09 & 1.04\(\)0.02 \\   

Table 3: Influence of modulesthe next state and reward, thereby facilitating the generation of high-quality next states with current perturbed states. The transition model without \(_{}\) may still retain numerous spurious correlations, resulting in a performance drop similar to the one without \(P_{}\), which does not alter the next state and reward. In the third row of Table 3, the performance drop indicates that the confounder \(P^{c}\) also plays a crucial role in preserving semantic meaning and avoiding policy training distractions.

**R5. RSC-SAC is also robust to random perturbation.** The final investigation aims to assess the generalizability of our method to cope with random perturbation that is widely considered in robust RL (RMDPs). Towards this, we evaluate the proposed algorithm in the test environments added with random noise under the Gaussian distribution with two varying scales in the _Lift_ environment. In Table 4, _Lift-0_ indicates the nominal training environment, while _Lift-0.01_ and _Lift-0.1_ represent the environments perturbed by the Gaussian noise with standard derivation \(0.01\) and \(0.1\), respectively. The results indicate that our RSC-SAC achieves comparable robustness compared to RMDP-0.01 in both large and small perturbation settings and outperforms RMDP methods in the nominal training environment.

**R6. RSC-SAC keeps good performance and robustness for a wide range of \(\).** As shown in Figure 7, the proposed RSC-SAC performs well in both nominal and shifted settings - keeping good performance in the nominal setting and achieving robustness, for a large range of (20%-70%). When the ratio of perturbed data is very small (1%), RSC-SAC almost achieves the same results as vanilla SAC in nominal settings and there is no robustness in shifted settings. As it increases (considering more robustness), the performance of RSC-SAC in the nominal setting gradually gets worse, while reversely gets better in the shifted settings (more robust). However, when the ratio is too large (>80%), the performances of RSC-SAC in both settings degrade a lot, since the policy is too conservative so that fails in all environments.

## 6 Conclusion and Limitation

This work focuses on robust reinforcement learning against spurious correlation in state space, which broadly exists in (sequential) decision-making tasks. We propose robust SC-MDPs as a general framework to break spurious correlations by perturbing the value of unobserved confounders. We not only theoretically show the advantages of the framework compared to existing robust works in RL, but also design an empirical algorithm to solve robust SC-MDPs by approximating the causal effect of the confounder perturbation. The experimental results demonstrate that our algorithm is robust to spurious correlation - outperforms the baselines when the value of the confounder in the test environment derivates from the training one. It is important to note that the empirical algorithm we propose is evaluated only for low-dimensional states.However, the entire framework can be extended in the future to accommodate high-dimensional states by leveraging powerful generative models with disentanglement capabilities  and state abstraction techniques .