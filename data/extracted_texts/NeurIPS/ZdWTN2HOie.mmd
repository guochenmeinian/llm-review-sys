# Stochastic Amortization: A Unified Approach to Accelerate Feature and Data Attribution

Ian Covert

Stanford University

icovert@stanford.edu

&Chanwoo Kim

University of Washington

chanwkim@uw.edu

&Su-In Lee

University of Washington

suinlee@uw.edu

&James Zou

Stanford University

jamesz@stanford.edu

&Tatsunori Hashimoto

Stanford University

thashim@stanford.edu

Equal contribution. Equal advising.

###### Abstract

Many tasks in explainable machine learning, such as data valuation and feature attribution, perform expensive computation for each data point and are intractable for large datasets. These methods require efficient approximations, and although amortizing the process by learning a network to directly predict the desired output is a promising solution, training such models with exact labels is often infeasible. We therefore explore training amortized models with noisy labels, and we find that this is inexpensive and surprisingly effective. Through theoretical analysis of the label noise and experiments with various models and datasets, we show that this approach tolerates high noise levels and significantly accelerates several feature attribution and data valuation methods, often yielding an order of magnitude speedup over existing approaches.

## 1 Introduction

Many tasks in explainable machine learning (XML) perform some form of costly computation for every data point in a dataset. For example, common tasks include assessing individual data points' impact on a model's accuracy , or quantifying each input feature's influence on individual model predictions . Many of these techniques are prohibitively expensive: in particular, those with game-theoretic formulations have exponential complexity in the number of features or data points, making their exact calculation intractable .

Accelerating these methods is therefore a topic of great practical importance. This has been addressed primarily with Monte Carlo approximations , which are faster than brute-force calculations but can be slow to converge and impractical for large datasets. Alternatively, a promising idea is to _amortize_ the computation, or to approximate each data point's output with a learned model, typically a deep neural network . For example, in the feature attribution context, we can train an _explainer model_ to predict Shapley values that describe how each feature affects a classifier's prediction .

There are several reasons why amortization is appealing, particularly with neural networks: similar data points often have similar outputs, pretrained networks extract relevant features and can be efficiently fine-tuned, and if the combined training and inference time is low then amortization can be faster than computing the object of interest (e.g., data valuation scores) for the entire dataset. However, it is not obvious how to train such amortized models, because standard supervised learning requires a dataset of ground truth labels that can be intractable to generate. Our goal here is therefore to explore efficiently training amortized models when exact labels are costly. Our main insight is thatamortization is surprisingly effective with noisy labels: we train with inexpensive estimates of the true labels, and we find that this is theoretically justified when the estimates are unbiased.

We refer to this approach as _stochastic amortization_ (Figure 1), and we find that it is applicable to a variety of XML tasks. In particular, we show that it is effective for feature attribution with Shapley values , Banzhaf values  and LIME ; for several formulations of data valuation ; and to data attribution with dammodels . Our experiments demonstrate significant speedups for several of these tasks: we find that amortizing across an entire dataset with noisy labels is often more efficient than current per-example approximations, especially for large datasets, and that amortized feature and data attribution models generalize well to unseen examples.

Our contributions in this work are the following:

* We present the idea of stochastic amortization, or training amortized models with noisy labels. We analyze the role of noise from the label generation process and show theoretically that it is sufficient to use unbiased estimates of the ground truth labels (Section 3). We find that non-zero bias in the labels leads to learning an incorrect function, but that variance in the labels plays a more benign role of slowing optimization.
* We identify a range of applications for stochastic amortization in XML. Our theory only requires unbiased estimation of the task's true labels, and we find that such estimators exist for several feature attribution and data valuation methods (Section 4).
* Experimentally, we test multiple estimators for Shapley value feature attributions and find that amortization works when the labels are unbiased (Section 5). We also verify that amortization is effective for Banzhaf values and LIME. For data valuation, we apply amortization to Data Shapley and show that it allows us to scale this approach to larger datasets than in previous works.
* Throughout our experiments, we also analyze the scaling behavior with respect to the amount of training data and the quality of the noisy labels. In general, we find that amortization is more efficient than per-example computation when the training set used for amortization contains at least a moderate number of data points (e.g., \(>\)1K for data valuation).

Overall, our work shows the potential for accelerating many computationally intensive XML tasks with the same simple approach: amortizing with noisy, unbiased labels.

## 2 Background

We first introduce the basic idea of amortization in ML, which we discuss in a general setting before considering any specific XML tasks. Consider a scenario where we repeat similar computation for a large number of data points. We represent this general setup with a context variable \(b\) and a per-context output \(a(b)\). For example, these can be an image and its data valuation score, or an image and its feature attributions. Amortization can be used with arbitrary domains, but we assume Euclidean spaces, or \(^{m}\) and \(^{d}\), because many XML tasks involve real-valued outputs.

Figure 1: Diagram of stochastic amortization. Left: using a dataset with noisy labels \((b)\) (e.g., images and data valuation estimates), we can train an amortized model that accurately estimates the true outputs \(a(b)\) (e.g., data valuation scores). Right: the default approach of running an expensive approximation algorithm for each example (e.g., a Monte Carlo estimator with many samples ).

The computation performed for each context \(b\) can be arbitrary as well. In some cases \(a(b)\) is an intractable expectation, in which case we would typically approximate it with a Monte Carlo estimator [96; 33]. In other situations it is the solution to an optimization problem [91; 2], in which case we can define \(a(b)\) via a parametric objective \(h:\):

\[a(b)*{arg\,min}_{a^{}}\ h(a^{}; b).\] (1)

We do not require a specific formulation for \(a(b)\) in this work, but we will see that both the expectation and optimization setups provide useful perspectives on our proposal of training with noisy labels.

In situations with repeated computation, our two options are generally to (i) perform the computation separately for each \(b\), or (ii) amortize the computation by predicting the output with a learned model. We typically implement the latter with a neural network \(a(b;)\), and our goal is to train it such that \(a(b;) a(b)\). In training these models, the main challenge occurs when the per-instance computation is costly: specifically, it is not obvious how to train \(a(b;)\) without a dataset of ground truth solutions \(a(b)\), which can be too slow to generate for many XML methods [68; 33]. We address this challenge in Section3, where we prove that amortization tolerates training with noisy labels.

### Related work

The general idea of amortized computation captures many tasks arising in physics, engineering, control and ML . For example, amortization is prominent in variational inference [56; 82], meta learning [38; 29; 80] and reinforcement learning [45; 64; 39]. In the XML context, many recent works have explored amortization to accelerate costly per-datapoint calculations [12; 108; 49; 50; 18; 19]. Some of these works are reviewed by , and we offer a detailed overview in AppendixA.

For feature attributions, two works propose predicting Shapley values by training with a custom weighted least squares loss [50; 18]; our simpler approach can use any unbiased estimator and resembles a standard regression task. Two other works suggest modeling feature attributions with supervised learning [87; 14]; these recommend training with exact or high-quality labels, whereas we recognize the potential to use noisy labels that can be generated orders of magnitude faster. Concurrently, Zhang et al.  proposed training with a custom estimator for Shapley value feature attributions; our work is similar but derives stochastic amortization algorithms for a range of settings, including the use of any unbiased estimator (Section3) and usage for various XML tasks including data valuation (Section4).

For data valuation, two works consider predicting data valuation scores with supervised learning [35; 36], but these also use near-exact labels that limit the applicability to large datasets. Concurrently, Li and Yu  propose a family of data valuation estimators and a learning-based approach analogous to  but for data valuation; our approach uses a simpler training loss that works with any unbiased estimator, and unlike  its memory usage does not scale with the dataset size. Separately, another line of work focuses on accelerating the model retraining step underlying most data valuation methods [57; 104; 107], and these are complementary to our approach. Finally, while there are works that accelerate data attribution with dammodels [78; 27], we are not aware of any that use amortization.2

More broadly, our proposal to train amortized models with noisy labels can be viewed as a version of stochastic optimization, or training with noisy gradients. This fundamental idea is widely used in machine learning , but to our knowledge we are the first to study the broad applicability of noisy labels for accelerating diverse XML tasks.

## 3 Stochastic Amortization

We now discuss how to efficiently train amortized models with noisy labels. Following Section2, we present this as a general approach before focusing on a specific XML task. One natural idea is to treat amortization like a standard supervised learning problem: we can parameterize a model \(a(b;)\), adopt a distribution \(p(b)\) over the context variable, and then train our model with the following objective,

\[_{}()=[\|a(b;)-a(b) \|^{2}].\] (2)This approach is called _regression-based amortization_ because it reduces the problem to a simple regression task. The challenge is that this approach cannot be used when we lack a large dataset of exact labels \(a(b)\), which is common for computationally intensive XML methods (Section 4).

A relaxation of this idea is to train the model with inexact labels (see Figure 1). We assume that these are generated by a noisy oracle \((b)\), which is characterized by a distribution of outputs for each context \(b\). For example, the noisy oracle could be a statistical estimator of a data valuation score . With this, we can train the model using a modified version of Eq. (2), where we consider the loss in expectation over both \(p(b)\) and the noisy labels \((b)\):

\[}_{}()=[\|a(b; )-(b)\|^{2}].\] (3)

It is not immediately obvious when this approach is worthwhile: if the noisy oracle is too inaccurate we will learn the wrong function, so it is important to choose the oracle carefully. We find that there are two properties of the oracle that matter, and these relate to its systematic error and noise level, or more intuitively its bias and variance. We denote these quantities as follows for a specific value \(b\),

\[( b)=\|a(b)-[(b) b] \|^{2},( b)=[\| (b)-[(b) b]\|^{2} b],\]

and based on these we can also define the global measures \(()_{p}[( b)]\) and \(()_{p}[( b)]\) for the distribution over context variables \(p(b)\).3 These terms are useful because they reveal a relationship between the two amortization objectives. In general, the objectives are related by the following two-sided bound (see proof in Appendix D):

\[(}_{}()-()} -()})^{2}_{}( )(}_{}()-( {a})}+()})^{2}.\] (4)

This relationship shows that reducing \(}_{}()\) towards its minimum value \(()\) is similar to training with \(_{}()\), only with a disconnect introduced by the bias \(()\). The bias represents a source of irreducible error, because in the limit \(}_{}()-() 0\) we have \(_{}()=()\). On the other hand, when \(()=0\) we can see that \(_{}()=}_{}()- ()\), which means that training with the noisy loss is equivalent and will recover the correct function asymptotically. This last equality is easy to see given an unbiased noisy oracle \((b)\), but the more general relationship in Eq. (4) emphasizes how non-zero bias can be problematic and lead to learning an incorrect function.

Aside from the bias, the variance plays a role as well, not in determining the function we learn but in making the model's optimization unstable or require more noisy labels. To illustrate the role of variance, we present a theoretical result considering the simplest case of a linear model \(a(b;)\) trained with SGD, which shows that high label noise slows convergence (see proof in Appendix D).

**Theorem 1**.: _Consider a noisy oracle \((b)\) that satisfies \([(b) b]=b\) with parameters \(^{m d}\) such that \(\|\|_{F} D\). Given a distribution \(p(b)\), define the norm-weighted distribution \(q(b) p(b)\|b\|^{2}\) and the terms \(_{p}_{p}[bb^{}]\) and \(_{q}_{q}[bb^{}]\). If we train a linear model \(a(;b)= b\) with the noisy objective \(}_{}()\) using SGD with step size \(_{t}=\), then the averaged iterate \(_{T}=_{t=1}^{T}_{t}\) at step \(T\) satisfies_

\[[}_{}(_{T})]-( )(_{p})(_{q}( {a})+4_{}(_{q})D^{2})}{_{}(_{p})(T+1)},\]

_where \(_{q}()_{q}[( b)]\) is the noisy oracle's norm-weighted variance, and \(_{}()\), \(_{}()\) are the maximum and minimum eigenvalues._

The bound in Theorem 1 shows that noise slows convergence by its presence in the numerator, and interestingly, it appears in the form of a weighted version \(_{q}()\) that puts more weight on values with large norm \(\|b\|\); this is a consequence of assuming a linear model, but we expect the general conclusion of label variance slowing convergence to hold even for neural networks. As a corollary, we can see that the rate in Theorem 1 applies directly to \(_{}()\) when the noisy oracle is unbiased (see Appendix D). We note that very high noise levels can in principle prevent effective optimization, and this can be mitigated by either reducing the noisy oracle's variance or taking more steps \(T\).

Overall, our analysis shows that amortization with noisy labels is possible, although perhaps more difficult to optimize than training with exact labels. We next show that unbiased estimates are available for many XML tasks (Section4), and we later find that this form of amortization is consistently effective with the noise levels observed in practice (Section5), even providing better accuracy than per-example estimation in compute-matched comparisons. As a shorthand, we refer to training with the noisy objective \(}_{}()\) in Eq.3 as _stochastic amortization_.

## 4 Applications to Explainable ML

We now consider XML tasks that can be accelerated with stochastic amortization. Rather than using generic variables \(b\) and \(a(b)\), this section uses an input variable \(x\), a response variable \(y\), and a model \(f\) or a measure of its performance. As we describe below, each application of stochastic amortization is instantiated by a noisy oracle that generates labels for the given task.

### Shapley value feature attribution

One of the most common tasks in XML is feature attribution, which aims to quantify each feature's influence on an individual prediction. The Shapley value has gained popularity because of its origins in game theory [89; 68], and like many feature attribution methods is based on querying the model while removing different feature sets . Given a model \(f\) and input \(x\) that consists of \(d\) separate features \(x=(x_{1},,x_{d})\), we assume that we can calculate the prediction \(f(x_{S})\) for any feature set \(S[d]\).4 With this setup, the Shapley values \(_{i}(x)\) for each feature \(i[d]\) are defined as:

\[_{i}(x)=_{S[d]\{i\}}(d-1\\ |S|)^{-1}(f(x_{S\{i\}})-f(x_{S})).\] (5)

These scores satisfy several desirable properties , but they are impractical to calculate due to the exponential summation over feature subsets. Our goal is therefore to learn an amortized model \((x;)^{d}\) to directly predict feature attribution scores, and for this we require a noisy oracle.

Many recent works have studied efficient Shapley value estimation , and we first consider noisy oracles derived from Eq.5, which defines the attribution as the feature's expected marginal contribution. There are several unbiased statistical estimators that rely on sampling feature subsets or permutations [6; 77; 74; 58], and following Section3 we can use any of these for stochastic amortization. Our experiments use the classic permutation sampling estimator [96; 74], which approximates the values \(_{i}(x)\) as an expectation across feature orderings. We defer the precise definition of this noisy oracle to AppendixE, along with the other estimators used in our experiments.

Next, we also consider noisy oracles derived from an optimization perspective on the Shapley value. A famous result from Charnes et al.  shows that the Shapley values are the solution to the following problem (with abuse of notation we discard the solution's intercept term),

\[(x)=*{arg\,min}_{a^{d+1}}\ _{S[d]}(S) (f(x_{S})-a_{0}-_{i S}a_{i})^{2},\] (6)

where we use a least squares weighting kernel defined as \(^{-1}(S)=|S|(d-|S|)\). Several works have proposed approximating Shapley values by solving this problem with sampled subsets, either using projected gradient descent  or analytic solutions [68; 16]. Among these, we use KernelSHAP  and SGD-Shapley  as noisy oracles in our experiments. The first is an M-estimator whose bias shrinks as the number of sampled subsets grows , so we expect it to lead to effective amortization; the latter has been shown to have non-negligible bias , so our theory in Section3 suggests that it should lead to learning an incorrect function when used for amortization.

### Alternative feature attributions

Next, we consider two alternative feature attribution methods: Banzhaf values [4; 11] and LIME . These are closely related to Shapley values and are similarly intractable [26; 40], but we find that they offer statistical estimators that can be used for stochastic amortization.

First, Banzhaf values assign the following scores to each feature for a prediction \(f(x)\):

\[_{i}(x)=}_{S[d]\{i\}}(f(x_{S \{i\}})-f(x_{S})).\] (7)

These differ from Shapley values only in their choice of weighting function, and they admit a range of similar statistical estimators. One option is the MSR estimator from , which is unbiased and re-uses all model evaluations for each feature attribution estimate. We adopt this as a noisy oracle in our experiments (see a precise definition in E), but several other options are available.

Second, LIME defines its attribution scores \(_{i}(x)\) as the solution to the following optimization problem, given a weighting kernel \((S)\) and penalty term \(\):5

\[*{arg\,min}_{a^{d+1}}\;_{S[d]}(S) (f(x_{S})-a_{0}-_{i S}a_{i})^{2}+(a).\] (8)

As our noisy oracle for LIME, we use the popular approach of solving the above problem for subsets sampled according to \((S)\). Similar to KernelSHAP , this is an M-estimator whose bias shrinks to zero as the sample size grows , so we expect it to lead to successful amortization.

Aside from these methods, other costly feature interpretation methods rely on unbiased statistical estimators and can be amortized in a similar fashion [98; 62; 32]. We leave further investigation of these methods to future work.

### Data valuation

Next, data valuation aims to quantify how much each training example affects a model's accuracy. We consider labeled examples \(z=(x,y)\) and a training dataset \(=\{z_{i}\}_{i=1}^{n}\), and we analyze each data point's value by fitting models to subsampled datasets \(_{T}\) with \(T[n]\) and calculating a measure of the model's performance \(v(_{T})\) (e.g., its 0-1 accuracy). This general approach was introduced by Ghorbani and Zou , who defined the Data Shapley scores \((z_{i})\) as follows:

\[(z_{i})=_{T[n]\{i\}}^{- 1}(v(_{T}\{z_{i}\})-v(_{T})).\] (9)

Subsequent work generalized the approach in different ways, which we briefly summarize before considering amortization. For example, Wang and Jia  used the Banzhaf value rather than Shapley value, and Kwon and Zou  considered the case of arbitrary semivalues . These correspond to adopting a different weighting over subsets in (9), and the general case can be written as follows for a normalized weighting function \(w(k)\):6

\[(z_{i})=_{T[n]\{i\}}w(|T|)(v(_{T} \{z_{i}\})-v(_{T})).\] (10)

Next, another extension is the case of _distributional data valuation_. Ghorbani et al.  incorporate an expectation over the original dataset \(\), which they show leads to well defined scores even for data points \(z=(x,y)\) outside the training set. Given a distribution over datasets of size \(||=n-1\) and a weighting function \(w(k)\), this version defines the score \((z)\) for arbitrary \(z\) as follows:

\[(z)=_{}_{T[n-1]}w(|T|)(v(_{T}\{z\})-v(_{T})).\] (11)

When using any of these methods in practice, the scores are difficult to calculate due to the intractable expectation across datasets. However, a crucial property they share is that they can all be estimated in an unbiased fashion, and these estimates can therefore be used as noisy labels for stochastic amortization. We focus on Data Shapley and Distributional Data Shapley in our experiments [33; 35], and we use the Monte Carlo estimator from Ghorbani and Zou  as a noisy oracle (see the precise definition in Appendix E). Doing so allows us to train an amortized valuation model \((z;)\) that accelerates valuation within our training dataset, and that can also be applied to external data, e.g., when selecting informative new data points for active learning .

Finally, Appendix B discusses amortization for the datamodels data attribution technique . This method measures how much each training data point \(z_{i}\) affects the prediction for an inference example \(x\), and we show that the scores are equivalent to a simple expectation that can be estimated in an unbiased fashion. The datamodels scores can therefore be amortized by adopting these estimates as noisy labels, but we leave further investigation of this approach to future work.

## 5 Experiments

Our experiments apply stochastic amortization to several of the tasks discussed in Section4. We consider both feature attribution and data valuation, for image and tabular datasets, and using multiple architectures for our amortized models, including fully-connected networks (FCNs), ResNets  and ViTs . Full details are provided in Appendix F, including our exact models and hyperparameters.

Our goal in each experiment is to perform feature attribution or data valuation for an entire dataset, and to compare the accuracy of stochastic amortization to running existing estimators on each point. We adopt a noisy oracle for each task (e.g., a Monte Carlo estimator of data valuation scores), we then fit an amortized network with one noisy label per training example, and our evaluation focuses on the accuracy of the amortized predictions relative to the ground truth. Our ground truth is obtained by running the noisy oracle to near-convergence for a large number of samples: for example, we run KernelSHAP  for feature attribution with 1M samples, and the TMC estimator  for data valuation with 10K samples. We test amortization when using different numbers of training examples, and for both training and unseen external data to evaluate the model's generalization. We find that amortization often denoises and strongly improves upon the noisy labels, leading to a significant accuracy improvement for the same computational budget.

### Feature attribution

We first consider Shapley value feature attributions. This task offers a diverse set of noisy oracles, and we consider three options: KernelSHAP , permutation sampling  and SGD-Shapley . Among these, our theory from Section3 suggests that the first two will be effective for amortization, while the third may not because it is not an unbiased estimator. We follow the setup from  and implement our amortized network with a pretrained ViT-B architecture , and we use the ImageNette dataset  with \(224 224\) images partitioned into \(196\) patches of size \(14 14\).

As a first result, Figure2 compares our training targets to the amortized model's predictions. The predicted attributions are significantly more accurate than the labels, even for the noisiest setting with just \(512\) KernelSHAP samples. Figure3 (left) quantifies the improvement from amortization, and we observe similar results for both KernelSHAP and permutation sampling (see Appendix G): in both cases the error is significantly lower than that of the noisy labels, and it remains lower and improves as the labels become more accurate. To contextualize our amortized model's accuracy, we find that the error is similar to that of running KernelSHAP for 10-40K samples, even though our labels use an order of magnitude fewer samples (see Appendix G). We also find that the model generalizes to external data points (see Appendix G). In addition, we

Figure 2: Stochastic amortization for Shapley value feature attributions. We compare the predicted attributions to the noisy labels and ground truth, which are generated using KernelSHAP with \(512\) and 1M samples, respectively.

show results for SGD-Shapley, where we confirm that it leads to poor amortization results due to its non-negligible bias (see Appendix G).

Next, we investigate the compute trade-off between calculating attributions separately (e.g., with KernelSHAP) and using amortization. Figure 3 (center-right) shows two results regarding this tradeoff. First, we measure the error as a function of FLOPs, where we account for the cost of generating labels and training the amortized model (see Appendix F). The FLOPs incurred by training are negligible compared to the KernelSHAP estimates, and we find that stopping at any time to fit an amortized model yields significantly better estimates. This suggests that amortization is an inexpensive final denoising step, regardless of how much compute was used for the noisy estimates.

Second, we test the effectiveness of amortization for different dataset sizes. We match the compute between the two scenarios, using \(2440\) KernelSHAP samples for per-example computation7 and \(2257\) for amortization to account for the cost of training. This compute-matched comparison shows that amortization achieves lower estimation error for datasets ranging from 250-10K data points (Figure 3 right); it becomes more effective as the dataset grows, but it is useful even for small datasets.

Finally, Appendix G shows a comparison between stochastic amortization and FastSHAP , an existing approach to amortized Shapley value estimation. We observe similar estimation accuracy in compute-matched comparisons, and find that both methods are significantly more accurate than per-example estimation (similar to Figure 3 center). Appendix G also show results for amortizing Banzhaf values and LIME: we find that amortization is more difficult for these methods due to the inconsistent scale of attributions between inputs, but we nonetheless observe an improvement in our amortized estimates versus the noisy labels.

### Data valuation

Next, we consider data valuation with Data Shapley. For our noisy oracle, we obtain training labels by running the TMC estimator with different numbers of samples . We first test our approach with the adult census and MiniBooNE particle physics datasets , and following prior work we conduct experiments using versions of each dataset with different numbers of data points . Our valuation model must predict scores for each example \(z=(x,y)\), so we train FCNs that output scores for all classes and use only the relevant output for each data point.

As a first result, Figure 4 (left-center) shows the estimation accuracy for the MiniBooNE dataset when using 1K and 10K data points. The noisy estimates converge as we use more Monte Carlo samples, but we see that the amortized estimates are always more accurate in terms of both squared error and correlation with the ground truth. The improvement is largest for the noisiest estimates, where the amortized predictions have correlation \(>\)\(0.9\) when using only \(50\) samples. Amortization

Figure 3: Amortized Shapley value feature attributions using KernelSHAP as a noisy oracle. Left: squared error relative to the ground truth attributions when using noisy labels with different numbers of samples (different noise levels). Center: estimation error as a function of FLOPs, where KernelSHAP incurs FLOPs via classifier predictions used to estimate the attributions, and amortization incurs additional FLOPs from training (training appears as a vertical line because the FLOPs are relatively low, and endpoints represent results from the final epoch). Right: estimation error with different training dataset sizes given equivalent compute per data point (matched by using fewer KernelSHAP samples when generating noisy labels for amortization and allowing up to \(50\) epochs of training).

is more beneficial for the 10K dataset, which suggests that training with more noisy labels can be a substitute for high-quality labels. \(\)G shows similar results with the adult census dataset.

Next, Figure 4 (right) considers the role of training dataset size for the estimates with \(50\) Monte Carlo samples. For both the adult census and MiniBooNE datasets, we see that the benefits of amortization are small with \(250\) data points but grow as we approach 10K data points. This number of samples is enough to maintain \(0.9\) correlation with the ground truth when using amortization, whereas the raw estimates become increasingly inaccurate. Stochastic amortization is therefore promising to scale data valuation beyond previous works, which typically focus on \(<\)1K data points .

### Distributional data valuation

Finally, we consider Distributional Data Shapley , which is similar to the previous experiments but defines valuation scores even for points outside the training dataset; this allows us to test the generalization to unseen data. We use the CIFAR-10 dataset , which contains 50K training examples, and for our valuation model we train a ResNet-18 that outputs scores for each class.

Similar to the previous experiments, Figure 5 (left-center) evaluates the estimation accuracy for different numbers of Monte Carlo samples. We observe that the distributional scores converge faster because we use a smaller maximum dataset cardinality (see \(\)F), but that amortization still provides a significant benefit. The improvement is largest for the noisiest estimates, which in this case use just \(5\) samples: amortization achieves correlation \(0.81\) with the ground truth, versus \(0.58\) for the Monte Carlo estimates. The improvement is consistent across several measures of accuracy, including squared error, Pearson correlation and Spearman correlation (see \(\)G).

Figure 4: Amortized data valuation accuracy for tabular datasets. Left: mean squared error relative to the ground truth for the MiniBooNE dataset, normalized so that the mean valuation score has error equal to 1 (for 1K and 10K data points). The x-axis indicates how many Monte Carlo samples were used for each data point. Center: Pearson correlation with the ground truth for the MiniBooNE dataset (for 1K and 10K data points). Right: estimation accuracy for the MiniBooNE and adult census datasets as a function of dataset size (250 to 10K data points); we use \(50\) Monte Carlo samples per data point for all results and show the Pearson correlation with the ground truth.

Figure 5: Distributional data valuation for CIFAR-10. Left: estimation error when using different numbers of samples for the noisy label estimates. Center: Pearson correlation with the ground truth for different numbers of noisy samples. Right: estimation error as a function of dataset size, where all results use \(5\) Monte Carlo samples per data points; we compare the error for amortized estimates on internal (training) and external (unseen) data points, demonstrating strong generalization.

Next, we study generalization and the role of dataset size. We focus on the noisiest estimates with \(5\) Monte Carlo samples, because this low-sample regime is most relevant for larger datasets with millions of examples. We train the valuation model with different portions of the 50K training set, and we measure the estimation accuracy for both internal and unseen external data. Figure 5 (right) shows large improvements in squared error with as few as 1K data points, and we also observe improvement in correlation when we use at least 5K data points (10% of the dataset, see Appendix G). We additionally observe a small generalization gap, suggesting that the valuation model can be trained with a subset of the data and reliably applied to unseen examples.

Lastly, we test the usage of amortized valuation scores in downstream tasks. Following , the tasks we consider are identifying mislabeled examples and improving the model by removing low-value examples. These results are shown in Appendix G, and we find that the amortized estimates identify mislabeled examples more reliably than Monte Carlo estimates, and that filtering the dataset based on our estimates leads to improved performance.

## 6 Conclusion

This work explored the idea of stochastic amortization, or training amortized models with noisy labels. Our main finding is that fast, noisy supervision provides substantial compute and accuracy gains over existing XML approximations. This approach makes several feature attribution and data valuation methods more practical for large datasets and real-time applications, and it may have broader applications to amortization beyond XML . Our proposal has certain limitations, including that stochastic amortization may become ineffective with sufficiently high noise levels, and that it is difficult to know a priori how much compute is necessary for label generation to achieve a desired error level in the amortized predictions.

Our work suggests multiple directions for future research. One direction is to study the trade-off between using a larger number of noisy labels or a smaller number of more accurate labels, which is a key difference from prior work that uses near-exact labels for amortization . Other directions include scaling to datasets with millions of examples to test the limits of noisy supervision, leveraging more sophisticated data valuation estimators , using alternative model retraining primitives , and exploring amortization for other methods like datamodels (discussed in Appendix B).

## Code

We provide two repositories to reproduce each our results:

**Feature attribution** & https://github.com/chanwkimlab/amortized-attribution \\
**Data valuation** & https://github.com/iancovert/amortized-valuation \\