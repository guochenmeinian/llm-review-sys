# SpikeReveal: Unlocking Temporal Sequences from

Real Blurry Inputs with Spike Streams

 Kang Chen\({}^{1,2}\)1 &Shiyan Chen\({}^{1,2,3}\)1 &Jiyuan Zhang\({}^{1,2}\) &Baoyue Zhang\({}^{1,2}\) &Yajing Zheng\({}^{1,28}\) &Tiejun Huang\({}^{1,2,3}\) &Zhaofei Yu\({}^{1,2,38}\)

\({}^{1}\) School of Computer Science, Peking University

\({}^{2}\) State Key Laboratory for Multimedia Information Processing, Peking University

\({}^{3}\) Institute for Artificial Intelligence, Peking University

{mrchenkang,strerichia002p,jyzhang,byzhang}@stu.pku.edu.cn

{yj.zheng,tjhuang,yuzf12}@pku.edu.cn

Equal contributors.Corresponding authors.

###### Abstract

Reconstructing a sequence of sharp images from the blurry input is crucial for enhancing our insights into the captured scene and poses a significant challenge due to the limited temporal features embedded in the blurry image. Spike cameras, sampling at rates up to 40,000 Hz, have proven effective in capturing motion features and beneficial for solving this ill-posed problem. Nonetheless, existing methods fall into the supervised learning paradigm, which suffers from notable performance degradation when applied to real-world scenarios that diverge from the synthetic training data domain. To address this challenge, we propose the first self-supervised framework for the task of spike-guided motion deblurring. Our approach begins with the formulation of a spike-guided deblurring model that explores the theoretical relationships among spike streams, blurry images, and their corresponding sharp sequences. We subsequently develop a self-supervised cascaded framework to alleviate the issues of spike noise and spatial-resolution mismatching encountered in the deblurring model. With knowledge distillation and reblur loss, we further design a lightweight deblur network to restore high-quality sequences with brightness and texture consistency with the original input. Quantitative and qualitative experiments conducted on our real-world and synthetic datasets with spikes validate the superior generalization of the proposed framework. Our code, data and trained models are available at [https://github.com/chenkang455/S-SDM](https://github.com/chenkang455/S-SDM).

## 1 Introduction

Traditional cameras, constrained by their exposure-based imaging mechanism, often produce blurry images when capturing fast-moving objects or during camera movement throughout the exposure process . While these blurry images lose significant details, the ability to recover dynamic motion trajectories from the static blurry input becomes critically important. However, the inherent challenge lies in the limited motion features available within blurry frames, leading to potential ambiguities such as multiple motion trajectories corresponding to the same blurry input. This is exemplified by scenarios where two objects move along the same trajectory but in opposite directions , rendering the task of motion deblurring ill-posed. Recent advancements in learning-based approaches  seek to address this challenge by establishing direct mappings fromblurry inputs to sharp sequences in a supervised learning manner. Despite these efforts, traditional cameras struggle to capture fine details in high-speed motion due to their exposure constraints, thus limiting the effectiveness of these methods in scenarios not covered by the training datasets.

In recent years, neuromorphic cameras [9; 10], leveraging their ultra-high temporal resolution and high dynamic range, have found widespread use in many fields, including computer vision and robotics. These cameras, including event and spike cameras, are distinguished by their ability to produce high temporal resolution outputs directly tied to changes in light intensity. Specifically, event cameras generate events in areas where light intensity changes , while spike cameras capture the absolute brightness of the scene at each pixel, offering a stream of spikes as output . This distinctive feature endows spike cameras with a significant advantage [47; 45; 48; 5; 41; 6] in capturing and recovering sharp texture from scenes with rapid motion.

Recent studies [2; 8] have explored the potential of RGB-Spike fusion, _i.e._, harnessing the strengths of both traditional and spike cameras to reconstruct sharp sequences from blurry inputs. However, their frameworks are constrained within the supervised learning paradigm, which necessitates extensive datasets comprising pairs of blurry and sharp images, as well as spike sequences. While synthetically acquiring such paired data, as demonstrated in previous studies [8; 4; 24], is feasible, collecting them in real-world scenarios presents the following challenges: (1) high-speed cameras are prohibitively expensive and not readily deployable in many settings; (2) spatial-temporal calibration between spike cameras and high-speed RGB cameras complicates the data collection process. These problems render the fine-tuning of supervised methods on real-world datasets challenging, further leading to their performance deterioration in such environments as shown in Fig. 1. The resulting degradation in image quality, manifesting as color distortion, brightness inconsistency, and inaccurate texture restoration, is mainly caused by the disparity between synthetic and real-world datasets, especially in terms of the density of spike stream, spike generation mechanism, and blurry image generation. Moreover, the effectiveness of supervised methods is inherently limited by the ground truth sequences created through motion analysis interpolation algorithms [22; 11], which inherently differs from the real-world scene and thus affects the model's generalization ability.

To overcome these issues, we propose the first-of-its-kind Self-supervised **S**pike-guided **D**eblurring **M**odel **(S-SDM)**, capable of recovering the continuous sharp sequence from a single blurry input with the assistance of low-resolution spike streams. We begin with a theoretical analysis of the

Figure 1: Illustration of the superiority of our self-supervised framework (S-SDM) over supervised methods. Supervised methods, while effective on synthetic datasets, suffer from a significant performance decline when applied to real-world datasets, primarily due to data distribution discrepancies. In contrast, our self-supervised framework, necessitating no Ground Truth (GT) for training, seamlessly bridges this dataset gap through fine-tuning on real-world datasets.

relationship between spike streams, blurry images, and sharp sequences, leading to the development of our Spike-guided Deblurring Model (SDM). We further construct a self-supervised processing pipeline by cascading the denoising network and the super-resolution network to reduce the sensitivity of the SDM to spike noise and its reliance on spatial-resolution matching between the two modalities. To reduce the computational cost and enhance the utilization of spatial-temporal spike information within this pipeline, we further design a Lightweight Deblurring Network (LDN) and train it based on pseudo-labels from the teacher model, \(i.e.\), the established self-supervised processing pipeline. Further introducing reblur loss during LDN training, we achieve better restoration performance and faster processing speed than the processing-lengthy and structure-complicated teacher model. To validate the performance of our S-SDM across various scenarios, we build an RGB-Spike binocular system and propose the first spatially-temporally calibrated Real-world Spike Blur (RSB) dataset in this community. Quantitative and qualitative experiments conducted on the real-world and synthetic datasets validate the superiority of our method. In summary, our key contributions are:

* We develop a self-supervised spike-guided image deblurring framework, addressing the performance degradation due to the synthetic-real domain gap in supervised methods.
* We perform an in-depth theoretical analysis of the fusion between the spike stream and blurry image, leading to the development of the SDM.
* We propose a real-world dataset RSB and experiments on GOPRO and RSB datasets validate the superior generalization of our S-SDM.

## 2 Related Work

**Spike Camera.** The spike camera, inspired by the primate retina, stands apart from conventional cameras with its ability to generate synchronous spike streams for each pixel at extremely low latency. This distinct feature provides significant advantages in various applications such as high-speed imaging [47; 45; 48; 5; 41; 6; 33], optical flow estimation , object detection , 3D reconstruction , depth estimation , motion deblurring [8; 32], and occlusion removal .

**Spike-guided Motion Deblurring.** While the spike camera boasts an ultra-high temporal resolution, its development is currently impeded by the low spatial resolution. Additionally, the single-channel output from the spike camera restricts previous methods from recovering the image color information. To address these issues, a promising approach is establishing an RGB-Spike hybrid imaging system . The binocular system achieves the multi-modality fusion of High-spatial/Low-temporal RGB blurry input and High-temporal/Low-spatial spike stream, thereby also serving as a spike-guided motion deblurring method . However, to the best of our knowledge, existing spike-guided deblurring methods [2; 8] predominantly rely on supervised training on synthetic datasets. This reliance results in significant performance degradation when these methods are evaluated in real-world scenarios due to the domain discrepancies between synthetic and real datasets as illustrated in Fig. 1.

**Event-based Motion Deblurring.** Event camera  can asynchronously generate events that record log-intensity changes at the pixel level with minimal latency, which contains a rich set of motion features beneficial for motion deblurring tasks. Numerous supervised methods [16; 4; 3; 16; 23; 24] have been proposed to learn the mapping from the blurry input, events to the sharp outcome. Despite these advancements, a major hurdle remains in obtaining real blurry-sharp image pairs for training. To overcome the domain gap between synthetic and real-world datasets, recent methods [29; 38; 39] explored the mutual constraint between the blurry image and event stream, enabling the training of networks on real-world blur datasets.

## 3 Method

### Preliminaries

**Spike Camera Mechanism.** Consider \((t)\) to represent the latent sharp frame at time \(t\). Each pixel \(p\) in the spike camera  has an integrator that accumulates the incoming photons at a high frequency. Once the cumulative intensity exceeds a predefined threshold \(C\) at time \(t_{e}\), pixel \(p\) emits a spike, and the accumulation of photons is reset to zero. This process can be mathematically described as follows:

\[_{t_{s}}^{t_{s}}(t)dt C, \]

where \(t_{s}\) denotes the firing time of the previous spike. While the spike camera is capable of generating asynchronous spike streams akin to that of event cameras, its effectiveness is constrained by the inherent limitations of its physical circuitry, which necessitates reading spikes at a predetermined sampling rate. We denote the generated spike stream as \(\{0,1\}^{K 1 H W}\), where \(H\) and \(W\) signify the height and width of the image, and \(K\) represents the length of the spike sequence.

**Problem Formulation.** In traditional photography, motion blur occurs when there is relative movement between the camera and the scene during the exposure period. According to the motion blur physical model , the blurry image \(\) can be represented as the average of the latent frame \((t)\) over the exposure \(\), _i.e._:

\[=_{t}(t)dt, \]

where \(T\) represents the exposure period. Despite the spike camera's superior temporal resolution, its spatial resolution remains comparatively low. This limitation is primarily attributed to the constraints in data transmission bandwidth and the challenges inherent in the manufacturing process. Here, we postulate that the spatial resolution of the spike camera is approximately one-quarter that of a conventional RGB camera.

In this paper, we aim to enhance the High-spatial/Low-temporal resolution blurry input \(^{1 3 H W}\) into a sequence of High-Quality images \(\{(t_{i})\}_{i=1}^{K}^{K 3 H W}\) with the aiding of High-temporal/Low-spatial resolution spike stream \(_{}\{0,1\}^{K 1}\), which can be mathematically formulated as:

\[\{(t_{i})\}_{i=1}^{K}=(t_{i};,_{ }). \]

In Eq. (3), \(()\) represents the Spike-guided Deblur-Net as shown in Fig. 1, \(i\) refers to the \(i\)-th frame in the spike stream \(_{}\), and \(t_{i}\) is the timestamp associated with this frame.

### Theoretical Analysis

The spike camera, with its photodetector tailored to capture the single-channel light intensity, faces difficulties in obtaining the color information that the multi-channel RGB camera can effortlessly capture. Therefore, we modify the color intensity \((t)\) in Eq. (1) to the grayscale value \(_{g}(t)\) for further analysis:

\[_{t_{s}}^{t_{e}}_{g}(t)dt C. \]

In this formulation, \(_{g}(t)=w_{r}_{r}(t)+w_{gre}_{gre}(t)+ w_{b}_{b}(t)\), where \(_{c}(t),w_{c}\) denote the intensity and weight of channel \(c\{r,gre,b\}\) respectively.

Given the blurry input \(\) and its corresponding spike stream \(_{}\), we incorporate Eq. (4) into the motion blur model presented in Eq. (2). This integration formulates a link between the two modalities as outlined below:

\[_{g}=}}{T}, \]

where \(_{g}\) denotes the grayscale version of the blurry input, and \(N_{}\) denotes the total number of spikes accumulated over the exposure period. The accumulation \(N_{}\) is calculated as \(N_{}=_{i=1}^{K}[i]\), with \([i]\) indicating the \(i\)-th frame of the spike stream.

Within the exposure \(\), we consider a shorter spike sequence centered around the \(t\) moment \(_{^{}}\{0,1\}^{K^{} 1 H W}\), satisfying \(K^{} K\), \(t\), and \(^{}\). Similar to Eq. (5), we can derive the relationship between the short-exposure gray image \(_{g}(t,^{})\) and the short spike stream \(_{^{}}\) as follows:

\[_{g}(t,^{})=}_{s ^{}}_{g}(s)ds=^{}} }{T^{}}, \]

where \(T^{} T\) represents the short exposure period.

Given the observation that the color information of adjacent pixels often exhibits similarity under the premise of minor motion amplitude, we postulate that the colors of the blurry input \(\) and the short-exposure image \((t,^{})\) are identical. This assumption implies that the intensity proportion among RGB channels in the blurry input \(_{c}^{}\) and the short-exposure image \(_{c}^{}(t,^{})\) is approximately equivalent, satisfying \(_{c}^{}=_{g}/_{c}\) and \(_{c}^{}(t,^{})=_{g}(t,^ {})/_{c}(t,^{})\). More details can be found in the supplementary materials.

Upon establishing it, we move forward to build a mathematical relation between the blurry image and the spike stream. By substituting the gray channel \(g\) with color channel \(c\) and dividing Eq. (5) by Eq. (6), we efficiently eliminate the unknown threshold \(C\) and weights \(_{c}^{}/_{c}^{}(t,^{})\), leading to the following equation:

\[_{c}(t,^{})=_{c}^{}}}{N_{}}}. \]

By applying Eq. (7) to RGB three channels, we explicitly establish the relationship between the color blurry input \(\), the color short-exposure image \((t,^{})\) and the spike stream \(_{}\) as shown in Fig. 18. Since the exposure period \(^{}\) is relatively short, it is reasonable to assume that the scene remains static. In this context, we interpret the short-exposure image \((t,^{})\) as the latent sharp frame \((t)\), allowing us to modify Eq. (7) as follows:

\[(t)=^{}}}{N_{}} }. \]

To this end, we have conducted a comprehensive theoretical analysis of the spike-guided motion deblurring task, which is neglected in prior learning-based motion deblur methodologies . For further discussion readability, we refer to Eq. (8) as the **Spike-guided Deblurring Model (SDM)**, which is analogous to the baseline motion deblur model EDI  in event camera.

### Self-supervised Spike-guided Deblurring Model

#### 3.3.1 Processing Pipeline

While SDM theoretically allows for the fusion of the blurry image and the spike stream, its practical deployment faces the following obstacles:

* The deblurred image \((t,^{})\) suffers from noise-related degradation due to the lack of adequate spike information during the short exposure \(^{}\).

Figure 2: The schematic diagram of our proposed distillation self-supervised framework. The “\(\)” indicates that certain computations are executed in a non-network manner.

* The spatial resolution of the spike camera is approximately one-quarter of the RGB camera, rendering the SDM implementation impractical.

To overcome these limitations, we further cascade the self-supervised denoising network to eliminate the spike noise in \(N_{^{}}\) and super-resolution network to match spatial resolutions of the blurry image and spike stream, with the processing pipeline illustrated in the bottom of Fig. 2.

**Denoising Network.** We leverage the Blind Spot Network (BSN) [13; 1; 27; 5; 14; 7] to predict the clean spike accumulation \(N_{^{}}\) from the input short-exposure spike stream \(_{^{}}\). The core idea of BSN is to design the blind-spot strategy that compels the convolutional layer to estimate the clean value of each pixel solely based on its surrounding pixels.

Under the premise that the spike stochastic thermal noise is independent identically distributed , the BSN is trained to deduce sharp spike frames from the input, with the loss function formulated as:

\[_{}=||(_{^{}}; _{1})-N_{^{}})||_{2}^{2}, \]

where the denoised spike frame \((_{^{}};_{1})\) is denoted by \(_{}(t)\) for further analysis.

**Super-Resolution Network.** In this task, we observe that the blurry input \(_{g}\) and the long-exposure spike frame \(N_{}\) exhibit the same texture features as shown in Eq. (5). This observation motivates us to train the Super-Resolution (SR) network based on pairs of the blurry images and the long-exposure spike frames.

We leverage the well-explored Enhanced Deep Super-Resolution network (EDSR)  as the backbone of our SR network, with the loss function formulated as follows:

\[_{}=||(N_{};_{2})- _{g}||_{2}^{2}. \]

With the training of the SR network completed, we freeze its parameters and apply it to the denoised spike frame \(_{}(t)\), yielding the resolution-enhanced spike frame \(_{}^{}(t)\).

#### 3.3.2 Knowledge Distillation Framework

While the aforementioned processing pipeline achieves the multi-modality fusion of the blurry input and the spike stream, several aspects still need refinement:

* The framework is lengthy and computationally demanding, which hinders its suitability for real-time system deployment.
* The blind-spot strategy of the BSN limits the full utilization of the spatial information inherent in the spike stream.
* The representation of the short-exposure image does not fully reflect the advantages of the high temporal resolution inherent in the spike stream.

To improve them, we further build a knowledge distillation framework building upon the existing processing pipeline. This pipeline serves as the teacher model, providing the reconstructed sequence as pseudo-labels for the training of the student model LDN, as illustrated in Fig. 2.

**Lightweight Deblur Network.** LDN adheres to a similar input and output pattern as previous research , _i.e_., taking the blurry input \(\) and the short spike stream \(_{^{}}\) centered around moment \(t\) as inputs, with the output being the reconstructed sharp image \(_{}(t)\), mathematically formulated as follows:

\[_{}(t)=(,_{^{ }};_{3}), \]

Full details regarding the LDN structure are available in the supplementary materials.

To avoid the scenario where the LDN exactly replicates the mapping of the teacher model, we design the teacher loss \(_{}\) based on the LPIPS  loss and further introduce the blur reconstruction loss. The reblur loss \(_{}\) measures the difference between the blurry input \(\) and the re-synthesized blurry image \(}\), satisfying:

\[}=_{m=1}^{M}_{}(t_{m}), \]

where \(_{}(t_{m})\) represents the \(m\)-th recovered image within the exposure period \(\) and \(M\) is the total number of reconstructed images. Finally, we sum up two loss functions with the weighting parameter \(\), and the final loss function is formulated as follows:

\[ =_{}+_{} \] \[=_{m=1}^{M}_{}(_{} ^{}(t_{m}),_{}(t_{m}))+_{ }(},). \]

## 4 Experiments

### Dataset

**Synthetic Data.** For quantitative analysis of our spike-guided motion deblurring task, we construct the synthetic dataset based on the widely employed GOPRO  dataset. We initially utilize the interpolation algorithm XVFI  to augment the video frame by interpolating additional \(7\) frames between each pair of consecutive sharp images. To generate the spike stream that mimics reality closely, we downsample the interpolated video to the resolution of \(320 180\) and simulate the spike

Figure 4: Qualitative comparison for the sequence reconstruction on the RSB dataset.

Figure 3: Qualitative comparison for the single frame restoration on the RSB dataset.

stream based on the spike simulator . To replicate real-world motion blur, we synthesize each blurry input by averaging \(97\) frames from interpolated video sequences.

**Real-world Data.** We construct an RGB-Spike binocular system and propose the first **Re**al-world **S**pike-guided **B**lur dataset (**RSB**) in this community. This system consists of a fixed-exposure RGB camera (Basler acA1920-150uc) and a spike camera , enabling us to capture the blurry image and the corresponding spike stream simultaneously. Further details about our RSB and the spatial-temporal calibration for our binocular system are provided in the supplementary materials.

### Experimental Results

We conduct both quantitative and qualitative comparisons of our S-SDM against state-of-the-art (SOTA) motion deblurring methods, including frame-based Motion-ETR , LEVS , video-based BiT , event-based TRMD , REFID , RED  and spike-based SpkDeblurNet  on the GOPRO and RSB datasets. For event-based methods, we replace the event stream with the spike stream and adopt the same input representation in these methods . We further cascade the image super-resolution technique DASR  as in  for the deblurred sequence to overcome the modality resolution inconsistency which is not considered in these methods. We reconstruct \(7\) images from one blurry input for sequence restoration evaluation  as listed in Tab. 1.

    &  & \)=1} & \)=2} & \)=4} &  \\    & & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & \\  LEVS & \(\) & 21.155 & 0.601 & 21.155 & 0.601 & 21.155 & 0.601 & 4.97M \\ Motion-ETR & \(\) & 21.955 & 0.610 & 21.955 & 0.610 & 21.955 & 0.610 & 6.55M \\ BiT & \(\) & 23.644 & 0.698 & 23.644 & 0.698 & 23.644 & 0.698 & 11.3M \\ TRMD+DASR & ✓ & 27.323 & 0.784 & 21.198 & 0.601 & 18.567 & 0.523 & 19.3M \\ RED+DASR & ✓ & 24.456 & 0.741 & 23.178 & 0.674 & 21.942 & 0.608 & 9.76M \\ REFID+DASR & ✓ & 28.124 & 0.819 & 15.288 & 0.339 & 13.623 & 0.274 & 15.9M \\ SpkDeblurNet & ✓ & **28.307** & **0.834** & 14.406 & 0.299 & 11.621 & 0.202 & 13.4M \\  S-SDM (Ours) & ✓ & 26.893 & 0.757 & **26.367** & **0.740** & **25.433** & **0.699** & **0.23M** \\   

Table 1: Quantitative comparison of the sequence reconstruction task on the GOPRO dataset.

Figure 5: Visual comparison of our S-SDM against other methods on the GOPRO dataset.

**Results on GOPRO.** To simulate the spike density domain gap as depicted in Fig. 1, we train all supervised spike-based deblurring methods on the GOPRO dataset under spike threshold \(V_{th}=1\) and evaluate them on datasets with spike thresholds \(V_{th}=1,2,4\). Quantitative comparison results are listed in Tab. 1 and the visual comparison is demonstrated in Fig. 5. Given that the principal contribution of our S-SDM is self-supervised learning, it is foreseeable that our method might be slightly inferior to the supervised methods SpkddDeblurNet on the dataset with \(V_{th}=1\). While these supervised methods deteriorate on datasets with \(V_{th} 1\), our method achieves great generalization benefiting from the self-supervision design. Specifically, SpkDeblurNet tends to produce darker and blurrier reconstructions on images with high thresholds as shown in Fig. 5. Besides, our method achieves better restoration performance than the self-supervised method RED due to our consideration of the spatial-resolution mismatch between two modalities and the designed teacher loss, which imposes a stronger constraint than the optical loss in RED.

**Results on RSB.** We further present visualizations of single frame and sequence reconstruction comparisons on the real-world RSB dataset as depicted in Fig. 3 and 4. Both frame-based and video-based approaches fail to replicate fine textures and detailed elements present in the blurry input. While SpkDeblurNet is capable of recovering structural details and the motion trajectory, it is deteriorated by significant noise and color distortion under conditions with lower spike density than those simulated in the GOPRO dataset. Similarly, in scenarios of higher spike stream densities, the restoration of SpkDeblurNet tends to exhibit over-exposure, resulting in brightness inconsistency compared to the blurry input. This over-exposure affects the dynamic range of the reconstructed image, ultimately compromising the overall perceptual quality and uniformity of the restored sequence. Our method addresses these challenges by finetuning on the RSB dataset, ensuring that the restored sequence aligns with the real-captured blurry input and spike stream. More comparative experiments and analyses are accessible in the supplementary material.

### Ablation Study

We perform ablation experiments on the GOPRO and RSB datasets to evaluate the performance of each module within S-SDM, the validity of our designed network architecture, as well as the overall effectiveness of our distillation learning framework. In this subsection, we evaluate the performance based on the single middle frame for simplicity.

**Modules Cascading.** Building upon the SDM, we sequentially cascade the BSN, the SR, and the LDN to evaluate their respective effectiveness, with quantitative results on GOPRO presented in Tab. 3. We employ bilinear interpolation as a substitute for the SR network in experiments I-1 and I-2 to align the spatial resolution of two modalities.

Qualitative ablation experiments are illustrated in Fig. 6. These comparisons reveal that while the SDM effectively removes motion blur, it struggles with significant noise and detail loss due to the spike noise and the low resolution of the spike stream. While the BSN mitigates noise and the SR network improves spatial resolution explicitly, the LDN trained via distillation learning further refines these enhancements, enabling the recognition of intricate textural features in the images, such as the license plate and the door number shown in Fig. 6.

**Network Architecture.** While our designed LDN mirrors the Scale-aware Network (SAN) proposed in the GEM , we replace the SAN with the LDN to compare the performance difference between the two architectures as depicted in Tab. 2. Despite the simple design of our LRN, which consists only of convolutional layers, ResBlocks, and basic modules such as CBAM , it outperforms SAN in both PSNR and SSIM while requiring fewer parameters and less computation, demonstrating that LDN is both sufficient and efficient for this task. This conclusion is consistent with our previous discussion, _i.e._, the restoration performance of the self-supervised learning framework primarily depends on the quality of pseudo-labels rather than the network architecture.

   Methods & PSNR \(\) & SSIM \(\) & Params (M) \(\) & Flops (G) \(\) \\  SAN  & 27.283 & 0.773 & 2.36 & 107.84 \\ LDN (Ours) & **27.928** & **0.786** & **0.234** & **33.60** \\   

Table 2: Performance comparison between the SAN in GEM  and our designed LDN.

**Distillation Learning.** We focus on analyzing the contribution of the teacher loss \(_{}\) and the reblur loss \(_{}\) within the distillation framework, with quantitative results listed in Tab. 4. Without the teacher loss \(_{}\), the LDN tends toward learning identity mapping from the blurry input. While under the guidance of the teacher model, the reblur loss \(_{}\) not only enforces motion consistency in the reconstructed sequence but also enriches the LDN with high-resolution details from the non-blurry regions of the input, thus improving the performance on GOPRO as listed in Tab. 4.

We further apply the LDN trained on GOPRO to the real-world dataset RSB, with qualitative visualization illustrated in Fig. 7. As observed in the figure, the absence of the reblur loss \(_{}\) leads to significant noise in the recovered image, which predominantly arises from the disparity in spike density and generation mechanism between the simulated and real-captured spike stream. This discrepancy causes the LDN to overestimate the spike number, resulting in black holes in regions with lower spike density than simulated, which reflects the drawback inherent in the supervised learning strategies as discussed in Sec. 1. Increasing the weight of the reblur loss \(_{}\) allows the LDN to incorporate more information from the blurry input, thereby mitigating this issue. However, this adjustment also leads to the presence of blurry edges. We follow the parameters set in Experiment II-6 and retrain the LDN on the RSB dataset (referred to as Experiment II-7). The retrained LDN effectively recovers the sharp edge of the calibration board and suppresses the spike noise in the background, validating the feasibility of our self-supervised framework in real-world scenarios.

## 5 Conclusion

In this paper, we introduce a novel self-supervised spike-guided motion deblurring framework S-SDM, which reconstructs sequences of sharp images from real-world blurry inputs with the spike stream. Additionally, we construct an RGB-Spike binocular system and propose the first spatially-temporally calibrated real-world dataset RSB in this community. Quantitative and qualitative experiments validate the superior generalization capabilities of our proposed S-SDM.

**Limitation.** The limitation of our S-SDM lies in its dependence on strict spatial-temporal calibration. Misalignment will lead to color shifts and quality degradation in the deblurred sequence.

   ID & \(_{}\) & \(_{}\) & \(\) & PSNR\(\) & SSIM\(\) \\  II-1 & \(\) & & & 10 & 23.102 & 0.441 \\ II-2 & \(\) & \(\) & & / & 26.563 & 0.723 \\ II-3 & \(\) & \(\) & & 10 & 27.345 & 0.762 \\ II-4 & \(\) & \(\) & & 50 & 27.742 & 0.778 \\ II-5 & \(\) & \(\) & & 100 & **27.928** & **0.786** \\ II-6 & \(\) & \(\) & & 200 & 27.620 & 0.783 \\   

Table 4: Distillation learning ablation on GOPRO.

Figure 6: Modules cascading comparisons on GOPRO. Experiments ID can be viewed on Tab. 3.

   ID & BSN & SR & LDN & PSNR\(\) & SSIM\(\) \\  I-1 & \(\) & & & 23.012 & 0.486 \\ I-2 & \(\) & & & 24.634 & 0.661 \\ I-3 & \(\) & \(\) & & 26.144 & 0.708 \\ I-4 & \(\) & \(\) & \(\) & **27.928** & **0.786** \\   

Table 3: Modules cascading ablation on GOPRO.

Figure 7: Distillation learning comparisons on RSB. Experiments ID can be viewed on Tab. 4