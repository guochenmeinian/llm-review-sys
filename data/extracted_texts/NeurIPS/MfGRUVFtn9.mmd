# Unveiling and Mitigating Backdoor Vulnerabilities based on Unlearning Weight Changes and Backdoor Activeness

Unveiling and Mitigating Backdoor Vulnerabilities based on Unlearning Weight Changes and Backdoor Activeness

Weilin Lin\({}^{1}\)  Li Liu\({}^{1}\)1  Shaokui Wei\({}^{2}\)  Jianze Li\({}^{3,4,2}\)  Hui Xiong\({}^{1}\)

\({}^{1}\)The Hong Kong University of Science and Technology (Guangzhou)

\({}^{2}\)The Chinese University of Hong Kong, Shenzhen

\({}^{3}\)Shenzhen International Center for Industrial and Applied Mathematics

\({}^{4}\)Shenzhen Research Institute of Big Data

###### Abstract

The security threat of backdoor attacks is a central concern for deep neural networks (DNNs). Recently, without poisoned data, unlearning models with clean data and then learning a pruning mask have contributed to backdoor defense. Additionally, vanilla fine-tuning with those clean data can help recover the lost clean accuracy. However, the behavior of clean unlearning is still under-explored, and vanilla fine-tuning unintentionally induces back the backdoor effect. In this work, we first investigate model unlearning from the perspective of weight changes and gradient norms, and find two interesting observations in the backdoored model: 1) the weight changes between poison and clean unlearning are positively correlated, making it possible for us to identify the backdoored-related neurons without using poisoned data; 2) the neurons of the backdoored model are more active (_i.e._, larger gradient norm) than those in the clean model, suggesting the need to suppress the gradient norm during fine-tuning. Then, we propose an effective two-stage defense method. In the first stage, an efficient _Neuron Weight Change (NWC)-based Backdoor Reinitialization_ is proposed based on observation 1). In the second stage, based on observation 2), we design an _Activeness-Aware Fine-Tuning_ to replace the vanilla fine-tuning. Extensive experiments, involving eight backdoor attacks on three benchmark datasets, demonstrate the superior performance of our proposed method compared to recent state-of-the-art backdoor defense approaches. The code is available at https://github.com/linweiii/TSBD.git.

## 1 Introduction

Over the past few years, _deep neural networks_ (DNNs) have achieved surprising success in several real-world applications, such as _face recognition_, _medical image processing_, and _autonomous driving_, _etc_. However, DNNs are susceptible to malicious attacks that can compromise their security and reliability. One typical example is the _backdoor attack_, where the adversary maliciously manipulates the training dataset or training process to produce a backdoored model, which performs normally on clean data while predicting any sample with a particular trigger pattern to a pre-defined target label. In this work, we focus on the _post-training defense_ scenario where, given a backdoored model and a small set of clean training samples, one aims to mitigate the backdoor effect while maintaining the performance on clean data, thereby obtaining a benign model.

Up to now, several important methods have been developed for _backdoor defense_. One promising approach is _poison unlearning_, which involves updating a backdoored model by unlearning from poisoned data. This technique has been utilized in various backdoor defenses such as ABL , D-BR , _Neural Cleanse_ (NC) , and i-BAU , _etc_. To avoid approximating poisoned data, another approach called _clean unlearning_ was conducted by RNP . This technique only uses clean data for unlearning and then prunes the backdoored model, which has been proven to be effective. Through relevant experiments, we find an interesting connection between poison unlearning and clean unlearning, as illustrated in **Observation 1** of Figure 1. Specifically, by calculating the weight changes of each neuron during the two unlearning processes on the backdoored models2, we find that they exhibit a strong positive correlation, _i.e._, the neurons exhibiting significant weight changes during clean unlearning also tend to play crucial roles in poison unlearning, indicating a stronger association with backdoor-related activities. Moreover, we further investigate the **backdoor activeness** during learning processes3, _i.e._, comparing the average gradient norm for each neuron in both the backdoored and clean models. The results are shown in **Observation 2** of Figure 1, revealing that neurons in the backdoored model are always more active compared to those in the clean model.

Inspired by the above two observations regarding the backdoored model, we propose **T**wo-**S**tage **B**ackdoor **D**efense (**TSBD**), consisting of stage 1) _Neuron Weight Change-based Backdoor Reinitialization_ and stage 2) _Activeness-Aware Fine-tuning_. In the first stage, we first conduct clean unlearning on the backdoored model, followed by the neuron weight change calculation, where both the changes of each subweight4 and neuron are recorded. Then, we conduct zero reinitialization to mitigate the backdoor effect by reinitializing the most-changed subweights among the top-\(n\%\) most-changed neurons as \(0\) in the original backdoored model. In the second stage, we adopt activeness-aware fine-tuning with gradient-norm regulation to recover clean accuracy and suppress the reactivation of the backdoor effect. Extensive experiments demonstrate the superior defense performance of the proposed method compared to state-of-the-art (SOTA) backdoor defense methods.

To summarize, our main contributions are three-fold. **(1) Novel Insight:** We are the first to uncover the strong positive correlation between neuron weight changes in clean unlearning and poison unlearning. We also reveal the high backdoor activeness in the backdoored model during the learning process. **(2) Effective Defense Method:** We further develop an effective two-stage defense method based

Figure 1: Illustration of two observations. Figures for Observation 1 show distributions of neuron weight changes during clean unlearning and poison unlearning. Figures for Observation 2 compare the average gradient norm for each neuron on the backdoored model and clean model, which are calculated with one-epoch clean unlearning. Being “more active” means a larger gradient norm. Experiments are conducted on PreAct-ResNet18  using CIFAR-10  for the clean model, along with additional attacks using 10% poisoning ratio for the backdoored model. The last convolutional layers are chosen for illustration.

on unlearning weight changes and backdoor activeness, considering both backdoor mitigation and clean-accuracy recovery, respectively. **(3) SOTA Performance:** Experimental results and analysis show that our proposed method achieves SOTA performance in backdoor defense.

## 2 Related Work

### Backdoor Attack

In the literature, various backdoor attacks on DNNs have been proposed, which can be categorized into _data poisoning attacks_ and _training-controllable attacks_. BadNets  is one of the earliest data poisoning attacks in this field. In this attack, a small proportion of the original data is selected and patched with a pre-defined pattern, known as a _trigger_. The labels of these patched data points are then modified to a target label. The mixed dataset, containing both clean and poisoned data, is used to train the DNNs, resulting in the implantation of the backdoor. Under a similar procedure, Blended  was proposed as a stronger attack by blending an entire pre-defined image into the original clean data with controllable transparency. Recently, more advanced and stealthy attacks have been proposed to enhance the trigger, such as SIG , label consistent attacks [27; 28], SSBA , _etc_. Another category is training-controllable attacks [23; 24; 29; 30; 31], where the attackers design triggers with permission to control the training process. Two significant examples are WaNet  and Input-aware , which generate unique triggers for different input data by incorporating an injection function into the model training process. This approach makes these attacks more difficult to detect compared to previous attacks with fixed triggers.

### Backdoor Defense

According to the different stages of model training, backdoor defense methods can be classified into two types: _training-stage defenses_ and _post-training defenses_.

**Training-stage Defenses.** In training-stage defenses, defenders have access to a mixed training dataset containing both clean data and poisoned data with triggers. ABL  discovers that the loss-dropping speed of poisoned data during the early stages of model training is faster, and thus isolates them for poison unlearning. DBD  splits the training process into three steps to separate the training of feature extraction from that of the subsequent classifier to evade the learning of trigger-label correlation. Similarly, D-ST/D-BR  observes that the transformations of poisoned-data feature representations are more sensitive than clean ones, and thus proposes to modularize the training process.

**Post-training Defenses.** In post-training defenses [33; 34; 35; 36], defenders aim to erase the backdoor effect in the learned DNNs using a small portion of clean data. FP  is one of the earliest defense methods, which observes that poisoned data and clean data activate different neurons in a backdoored DNN, and thus keeps pruning the less-activated neurons in response to clean data until a significant

Figure 2: Overview of the proposed Two-Stage Backdoor Defense framework.

drop in accuracy occurs. After that, vanilla fine-tuning is employed to recover the lost clean accuracy. Using the pruning strategy [38; 39; 40], ANP  observes that the backdoor-related neurons exhibit higher sensitivity to adversarial perturbations compared to others, and thus trains a pruning mask using minimax optimization. Continuing along this line, AWM  and RNP  use a similar mask training process with main modifications in neuron perturbations to data perturbations and _clean unlearning_, respectively. Different strategies are also proposed for defense. For example, NC  proposes to recover the trigger before the subsequent backdoor removal. NAD , for the first time, adopts model distillation to guide the learning of a benign student model. Additionally, employing unlearning techniques, SAU  treats backdoor triggers as a form of adversarial perturbation, and generates poisoned data through optimization on clean data, which are then used in _poison unlearning_.

**Unlearning for Backdoor Defense.**_Model unlearning_ can be considered as an opposite process against learning, aiming to remove the impact of a training subset from a trained model . In the field of backdoor defense, unlearning the possible poisoned data (_i.e._, _poison unlearning_) is an effective way to remove the learned backdoor. NC  and BTI-DBF  try to generate the possible poisoned data with either trigger inversion or poison-data generator; ABL  and D-BR  focus on filtering out the poisoned data from the training dataset according to their attributes during training; i-BAU  and SAU  assume the adversarial perturbation as a type of trigger and generate poisoned data with adversarial example. To avoid inducing bias, recent work tries to directly unlearn the available clean data (_i.e._, _clean unlearning_) for defense. RNP  finds that a clean-unlearned model can help expose the backdoor neurons for the subsequent pruning-mask learning.

However, there exist some limitations among those techniques, _e.g._, clean unlearning is still under-explored, and the vanilla fine-tuning unintentionally increases the attack success rate. In this paper, we propose a comprehensive two-stage defense method breaking through the two limitations.

## 3 Methods

### Problem Formulation

**Threat Model.** We assume that the attacker has full access to the training data. Their goal is to poison a portion of the dataset by injecting triggers into the data so that the trained model misclassifies the poisoned data to the target class while still performing normally on clean data. The _poisoning ratio_ (_e.g._, 10%) is used to depict the proportion of poisoned data within the entire dataset. We denote the parameters of the backdoored model as \(_{bd}=\{_{bd}^{(l)}\}_{1 l L}\) satisfying \(_{bd}^{(l)}^{K^{(l)} I^{(l)}}\), where \(=\{K^{(l)}\}_{1 l L}\) and \(=\{I^{(l)}\}_{1 l L}\) represent the neuron numbers and learnable subweight numbers, respectively. Specifically, for the \(l^{th}\{1,,L\}\) layer, there are \(K^{(l)}\) neurons in total and \(I^{(l)}\) subweights for each neuron.

**Defense Setting.** The defender's goal is to remove the backdoor effect, which causes poisoned data to be misclassified to the target class, from the backdoored model while minimizing the impact on the prediction accuracy for clean data. Following the previous defense setting [37; 41], we assume that the defender knows nothing about the poisoned data and possesses only 5% of the total dataset as clean data, denoted as \(_{c}\).

### Neuron Weight Change & Suggestions Given by the Two Observations

In this subsection, we provide more details on the unlearning formulation and offer suggestions based on the two observed observations.

**Model Unlearning.** Model Unlearning can be defined as the reverse process of model training , which involves maximizing the loss value on a given dataset. Given a DNN model \(f\) parameterized as \(\) and a dataset \(\) for unlearning, the maximization problem can be formulated as:

\[_{}_{(,y)}[(f( {x};),y)],\] (1)

where \((,y)\) represents the images and their corresponding labels, and \(\) denotes the loss function used in this task, _e.g._, cross-entropy loss.

Intuitively, by maximizing the loss expectation, the unlearned model, parameterized as \(_{ul}\), is prone to fail at the task specified in \(\). In this paper, we term the process as _clean unlearning_ when all the data in \(\) are clean, denoted as \(_{c}\). On the other hand, _poison unlearning_ refers to the scenario where all the data in \(\) are poisoned with a trigger. By default, both clean and poison unlearning are terminated when the model performs poorly on the corresponding tasks, such as achieving only 10% clean accuracy or attack success rate.

**Neuron Weight Change.** To comprehensively quantify the weight changes of a neuron during the entire unlearning process, we define the _Neuron Weight Change_ (NWC), where the \(L_{1}\) norm is calculated on every neuron's weight differences. The NWC for the \(k^{th}\{1,,K^{(l)}\}\) neuron in layer \(l\{1,,L\}\) can be formulated as:

\[^{(l)k}=_{i=0}^{I^{(l)}}\|_{ul}^{(l)ki}-_{bd}^{(l)ki}\|_{1},\] (2)

where \(_{i=0}^{I^{(l)}}_{1}\) is to calculate the \(L_{1}\) norm for the differences on a neuron with totally \(I^{(l)}\) subweights, \(_{ul}^{(l)ki}\) and \(_{bd}^{(l)ki}\) denote the \(i^{th}\{1,,I^{(l)}\}\) subweights of \(k^{th}\) neuron after and before the entire unlearning process, respectively. A larger \(^{k}\) indicates more significant changes occurring in neuron \(k\) during the unlearning process. Similarly, in Equation (2), the term \(\|_{ul}^{(l)ki}-_{bd}^{(l)ki}\|_{1}\) represents the changes in the \(i^{th}\) subweight of neuron \(k\) in layer \(l\), _i.e._, defined as _Subweight Change_.

**Suggestions Given by the Two Observations.** As demonstrated in Section 1, we have two interesting observations regarding the backdoored model. **Observation 1** shows that the neurons exhibiting significant weight changes during clean unlearning also tend to play crucial roles in poison unlearning. It suggests that we can employ clean unlearning to identify and eliminate backdoor-related neurons using NWC, at the expense of reducing clean accuracy. On the other hand, **Observation 2** reveals that neurons in the backdoored model are always more active compared to those in the clean model. It suggests that we should suppress the gradient norm during the learning process if we want to recover it to a clean model. These two suggestions act as the main supports to our proposed TSBD.

### Further Investigations & Insights

Here, we offer insights from the perspective of neuron activations, trying to answer two important questions: **[Q1]** What causes the clean unlearning NWCs to exhibit a positive correlation with those in poison unlearning, and **[Q2]** What motivates the neurons more active in the backdoored model.

**Neuron Activations & Activation Rise.** The neuron activation is determined by computing the average value of all inputs to the specific neuron, _e.g._, \(h^{(l)k}(^{(l)k}^{(l-1)})\) for simplicity, where \(()\) is the activation function. In line with the terminology used in FP , _clean activation_ denotes the scenario where all the input samples are clean while _poison activation_ refers to the presence of poisoned inputs. To better observe the changes in activation during unlearning, we calculate the activation rise from the original model to the unlearned model, _i.e._, \( h^{(l)k}=h^{(l)k}_{ul}-h^{(l)k}_{bd}\). A positive value indicates an increase in activation, while a negative value signifies a decrease.

Figure 3: Illustration of clean and poison activations of each neuron. (a) and (b) represent the activations on the original clean and backdoored model, respectively. (c) shows the activation changes during the clean and poison unlearning on backdoored model. Activations are captured from the last convolutional layer with an additional _Relu_ activation function on PreAct-ResNet18 .

**Relationship between NWC and Activation Change.** Considering that a backdoored model has learned two tasks from the clean and poisoned data , the main influence of NWC on a neuron can be roughly attributed to its activation change on both clean and poisoned inputs. For neuron \(k\) in layer \(l\), we can formulate it as \(^{(l)k}| h_{c}^{(l)k}|+| h_{p}^{(l)k}|\), where \( h_{c}^{(l)k}\) and \( h_{p}^{(l)k}\) represent the activation rise on clean and poisoned inputs, respectively.

Figure 3 illustrates the clean and poison activations in (a) the original clean model, (b) the original backdoored model, and (c) the backdoored-model unlearning. We now try to answer the above two questions from these observations. **[A1]** We can observe that poison activations are the main factors affected during both clean unlearning (increase) and poison unlearning (decrease), while clean activations are only slightly influenced (see Figure 3 (c)), _i.e._, \(^{(l)k}_{}| h_{c}^{(l)k}|_{}+|  h_{p}^{(l)k}|_{}\). Also, the growing NWC during clean unlearning can indicate larger poison and clean activations (where \(h_{p}^{(l)k}>h_{c}^{(l)k}\) to some extent (see Figure 3 (b)). Thus, we deduce that the co-function of clean and poison activations dominates the performance on both tasks, while the higher values of poison activation in the backdoored model make it an easier target for modification. In this case, the neurons with higher poison activations tend to decrease their values during poison unlearning, thereby reducing the attack success rate. Conversely, during clean unlearning, these neurons increase poison activations, which suppresses the function of clean activations and reduces clean accuracy. **[A2]** Similarly, the significantly lower values of mixed clean and poison activations (maximum: 0.5676) on the clean model (see Figure 3 (a)) indicate that it is less active compared to the backdoored model (maximum: 1.7053), where a similar pattern can also be seen on the bottom left of Figure 3 (b).

### Two-Stage Backdoor Defense Framework

Based on the above observations, we now propose a defense framework incorporating _Neuron Weight Change-based Backdoor Reinitialization_ (including _Clean Unlearning_, _Neuron Weight Change Calculation_ and _Zero Reinitialization_), and _Activeness-aware Fine-tuning_. The detailed defense process is illustrated in Figure 2 and Algorithm 1 (found in Appendix A).

**Stage 1) Neuron Weight Change-based Backdoor Reinitialization.** We aim to mitigate the backdoor effect with acceptable clean-accuracy sacrificed in this stage. _[a. Clean Unlearning.]_ To identify the backdoor-related neurons, we first conduct a full clean unlearning using the available clean data \(_{c}\) on the backdoored model. _[b. Neuron Weight Change Calculation.]_ Then, we record the subweight changes and calculate the NWC for each neuron as described in Section 3.2. The resulting sorted order of neurons reflects the backdoor strength. _[c. Zero Reinitialization.]_ After that, we can now eliminate the backdoor effect through zero reinitialization. Based on the NWC neuron order, we identify the top-\(n\%\) neurons as strongly backdoor-related. As suggested in Section 3.3, high-NWC neurons may also contribute to clean accuracy to some extent. Therefore, we further choose to reinitialize the subweights of the most-changing \(m\%\) among the selected neurons to zero in the backdoored model, while leaving the others unchanged. The reinitialized model parameter is denoted as \(}\).

**Stage 2) Activeness-Aware Fine-tuning.** To further repair the reinitialized subweights and avoid recovering the backdoor effect again, we conduct activeness-aware fine-tuning on the reinitialized model (\(}\)) using the clean dataset, \(_{c}\). This involves incorporating gradient-norm regulation into the original loss function, such as the cross-entropy loss \(_{ce}\), to penalize high gradient values. This regulation serves to suppress neuron activity during fine-tuning. The final loss function is:

\[_{ft}(})=_{ce}(})+ \|_{}}_{ce}(})\|_{2},\] (3)

where \(\|_{}}_{ce}(})\|_{2}\) represents the \(L_{2}\) norm of gradients, and \(\) is the _penalty coefficient_ controlling its impact. Hence, the objective of fine-tuning is to minimize the loss function \(_{ft}(})\) using the available clean data \(_{c}\):

\[_{}}_{(_{c},y_{c})_{c}}[ _{ft}(f(_{c};}),y_{c})].\] (4)

During practical optimization for computational efficiency, we adopt the approximation scheme in , which can be formulated as:

\[_{}}_{ft}(})(1-) _{}}_{ce}(})+_{ }}_{ce}(}+r}}_{ce}(})}{\|_{}} _{ce}(})\|_{2}}).\] (5)

[MISSING_PAGE_FAIL:7]

a 10% poisoning ratio on PreAct-ResNet18 for illustration, which is shown in Table 1 and Table 2. More results on GTSRB and VGG19-BN can be found in Appendix D and E, respectively.

**Results on CIFAR-10.** Table 1 shows the results on CIFAR-10. Results show that our TSBD outperforms all the other SOTA defenses on the average of ASR (2.18%) and DER (97.09%), as well as a promising ACC (91.70%) higher than the original "No Defense" models (91.34%), which indicates its effectiveness in removing the backdoor effect with the least cost. Though most defenses fail in strong attacks Blended, LF, or SSBA, _e.g._, FT, FP, NAD, ANP, CLP, i-BAU, and RNP, our proposed TSBD success with the best ASR and DER on Blended and second best ASR and DER on LF and SSBA. FT and NAD perform similarly on each attack with a promising ACC, but they also fail on WaNet with a high ASR except for the mentioned strong attacks. FP and ANP perform well in ACC among all the defenses, while the defense performances on ASR and DER are unstable, which may be due to the unsuccessful backdoor-related neuron locating. CLP fails on most of the attacks with high ASR and low DER, which may be due to the structure constraint of computing channel Lipschitz only on the convolutional-batch normalization layer combination. i-BAU performs the second best on average ASR and DER, with failures on three attacks. TSBD outperforms RNP on almost all performances, which indicates that using clean unlearning with NWC is more effective than the unlearn-recovery process in RNP.

**Results on Tiny ImageNet.** Table 2 presents the results on Tiny ImageNet with PreAct-ResNet18. We observe that most defenses also fail on Blended, SSBA, and LF with high ASR. Similar performances of other attacks are shown on most of the defenses compared to CIFAR-10, where FT and FP also perform well in ACC and CLP fails on most attacks. Although i-BAU can successfully defend against most of the attacks in CIFAR-10, it fails on all attacks here, indicating that its adversarial training fails with large classification categories. For RNP, though it performs well in ASR on almost all attacks, the ACC is sacrificed too much to be unacceptable, indicating an unbalanced defense performance. In comparison, our TSBD can achieve SOTA on the average of DER, and perform second best on the average of ASR, which validates its superior defense performance.

### Ablation Studies

**Effectiveness of NWC order for Backdoor Strength.** To verify the effectiveness of employing clean-unlearning NWC order in gauging the backdoor strength, we borrow the _Trigger-activated Change_ (TAC)  order as the ground truth and compare the neuron coverage ratio on TAC under different proportions, _i.e._, measuring the overlap of the selected neurons on both metrics. Specifically, TAC measures the change in neuron activation before and after the input image is attached with a trigger, where the larger value indicates a stronger backdoor effect. We select the following SOTA metrics for comparison: 1) the average neuron activations in FP ; 2) the channel Lipschitz in CLP ; 2) the perturb-recovery learned mask in ANP ; 3) the unlearn-recovery learned mask in RNP . The result is illustrated in Figure 4, where the x-axis represents the (reinitializing/pruning) neuron ratio and the y-axis represents the neuron coverage ratio on TAC. The higher values on the y-axis indicate a better matching of the current metric and the TAC metric,

  Backdoor &  &  &  &  \\  Attacks & ACC & ASR & DER & ACC & ASR & DER & ACC & ASR & DER & ACC & ASR & DER & ACC & ASR & DER \\   BaResNet  & 56.23 & 100.00 & -55.38 & 0.09 & **99.0** & 51.35 & 99.99 & 47.66 & 43.67 & 0.27 & 94.93 & 48.26 & 0.10 & 95.96 \\ Blended  & 56.03 & 99.71 & -55.04 & 97.73 & 50.49 & 50.19 & 59.94 & 49.81 & 46.99 & 59.00 & 47.79 & 52.55 & 93.91 & 51.51 \\ Input-aware  & 57.45 & 98.85 & -57.25 & 16.58 & 98.60 & 55.28 & 62.92 & 68.88 & 47.91 & 1.86 & 93.73 & 56.20 & 0.09 & **98.76** \\ LF  & 55.97 & 98.57 & -54.39 & 94.87 & 51.26 & 54.14 & 93.52 & 49.40 & 45.45 & 54.09 & 63.78 & 52.99 & 35.65 & 55.22 \\ SSAB  & 52.52 & 97.71 & -54.08 & 91.97 & 52.16 & 50.47 & 88.87 & 52.04 & 45.52 & 57.73 & 65.25 & 52.47 & 53.47 & 70.75 \\ Trojan  & 55.88 & 99.98 & -54.20 & 0.59 & **99.50** & 50.22 & 88.82 & 97.24 & 48.88 & 68.33 & 95.87 & 62.99 & 41.59 & 89.31 \\ WaNet  & 56.78 & 99.49 & -56.74 & 0.19 & **99.63** & 53.34 & 3.94 & 96.30 & 49.68 & 0.43 & 52.13 & 0.23 & 97.40 \\  Average & 56.22 & 99.19 & -55.03 & 69.74 & 78.32 & 52.12 & 63.71 & 64.99 & 46.77 & 27.96 & 30.14 & 52.30 & 32.36 & 81.70 \\  Backdoor &  &  &  &  &  \\  Attacks & ACC & ASR & DER & ACC & ASR & DER & ACC & ASR & DER & ACC & ASR & DER & ACC & ASR & DER \\  BaResNet  & 50.55 & 77.74 & 93.29 & 50.00 & 100.48 & 49.86 & 43.48 & 97.36 & 43.95 & 21.91 & **60.00** & **25.00** & 53.04 & 53.72 & 0.34 & 98.58 \\ Blended  & 54.99 & 84.61 & 57.93 & **55.70** & 99.68 & 49.85 & 53.03 & 51.90 & 52.40 & 34.60 & **61.11** & 80.06 & 53.63 & 2.30 & **97.50** \\ Input-aware  & 53.17 & 01.72 & 97.20 & **57.75** & 99.80 & 50.04 & 52.78 & 69.05 & 41.75 & **55.00** & **73.56** & 53.83 & 0.10 & 98.34 \\ LF  & 54.66 & 59.39 & 50.94 & **55.61** & 98.49 & 48.96 & 51.13 & 55.32 & 54.21 & 49.18 & **60.00** & 55.89 & 52.47 & 1.90 & **56.59** \\ SSAB  & 52.38 & 91.44 & 51.94 & **55.17** & 97.65 & 50.01 & 49.86 & 81.90 & 55.22 & 37.64 & **0.00** & 50.06 & 52.93 & 1.38 & **97.22** \\ Trojan  & 50.37 & 1.40 & 96.53 & **55.86** & 83.69 & 59.78 & 52.65 & 98.49 & 49.12 & 46.27 & **60.00** & **59.15** & 53.66 & 0.31 & 98.72 \\ WaNet  & 53.87 & 0.75 & 99.71 & 56.21 & 90.50 & 50.21 & 33.71 & 75.23 & 66.00 & 20.50 & **60.00** & **80.10** & 55.00 & 0.71 & 98.50 \\  Average & 52.92 & 40.21 & 77.83 & **56.03** & 86.04 & 55.12 & 50.65 & 86.17 & 54.42 & 32.24 & **0.02** & 87.59 & 53.83 & 1.01 & **97.89** \\  

Table 2: Comparison with the SOTA defenses on Tiny ImageNet dataset with PreAct-ResNet18 (%).

Figure 4: Comparison of neuron coverage ratio on TAC under different neuron ratios.

[MISSING_PAGE_EMPTY:9]

**Performance on Different \(r\) and \(\) for Activeness-Aware Fine-tuning.** To test the hyper-parameters sensitivity for stage 2, _i.e._, \(r\) and \(\) in Activeness-Aware Fine-tuning, we follow the tuning range in  under our experimental settings. The results are shown in Teble 5, which are obtained from a BadNets-attacked PreAct-ResNet18. We observe that the performance is insensitive (Changes <2% in ACC and <1% in ASR) across different hyper-parameter settings, maintaining a high level of performance.

**Performance on Different Poisoning Ratio.** We further investigate the performance of TSBD on different poisoning ratios, _e.g._, 10%, 5%, and 1%. Note that a larger poisoning ratio represents a stronger attack mode. We test the performance with six attacks on these three ratios. Figure 6 shows the performances of ACC and ASR. We can observe that TSBD successfully defends all the attacks on 10% and 5% with a low ASR and a high ACC while performing less effectively on 1%. A possible reason is that the unlearning weight changes of backdoor neurons are less obvious in the weak attack mode compared to the strong attack mode with 10% or 5% poisoning ratios.

**More Experiments and Analysis.** Due to the space limit, we postpone the detailed discussion of the clean data ratio and the fine-tuning learning rate to Appendix H and Appendix I, respectively. We also evaluate the defense performance on the clean model in Appendix J and on the ViT in Appendix K. Further, we provide the computational overhead in terms of runtime in Appendix L.

## 5 Conclusion

In this work, we propose an effective two-stage backdoor defense method, TSBD, to eliminate the backdoor effect in DNNs. Our research reveals two important observations regarding the backdoored models to support our method. First, there is a positive correlation between weight changes during poison and clean unlearning in backdoored models. This finding enables us to identify and eliminate backdoor-related neurons through clean unlearning and zero reinitialization. Second, neurons in backdoored models are more active compared to those in clean models, which suggests regulating the gradient norm during fine-tuning. Furthermore, we also provide insights into these two observations from the perspective of neuron activations, which may be a valuable contribution to the field of backdoor defense. Extensive experiments demonstrate the superiority of our method over recent defenses. One current challenge as well as promising future work involves defending against backdoor attacks without any accessible clean data. The data generation techniques and data-free techniques may be the potential solutions.

## 6 Acknowledgements

This work was supported in part by the Guangzhou Municipal Science and Technology Project: Basic and Applied Basic research projects (No. 2024A04J4232), National Natural Science Foundation of China (No. 62101351), Guangzhou-HKUST(GZ) Joint Funding Program (Grant No.2023A03J0008), Education Bureau of Guangzhou Municipality, and Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone Project (No.HZQSWS-KCCYB-2024016).