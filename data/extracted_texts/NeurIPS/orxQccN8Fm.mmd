# Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment

Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment

 Jiaxiang Li

University of Minnesota

Minneapolis, MN, USA

li003755@umn.edu

&Siliang Zeng

University of Minnesota

Minneapolis, MN, USA

zeng0176@umn.edu

&Hoi-To Wai

Chinese University of Hong Kong

Hong Kong

htwai@se.cuhk.edu.hk

&Chenliang Li

Texas A&M University

College Station, TX, USA

chenliangli@tamu.edu

&Alfredo Garcia

Texas A&M University

College Station, TX, USA

alfredo.garcia@tamu.edu

&Mingyi Hong

University of Minnesota

Minneapolis, MN, USA

mhong@umn.edu

###### Abstract

Aligning human preference and value is an important requirement for contemporary foundation models. State-of-the-art techniques such as Reinforcement Learning from Human Feedback (RLHF) often consist of two stages: 1) supervised fine-tuning (SFT), where the model is fine-tuned by learning from human demonstration data; 2) Preference learning, where preference data is used to learn a reward model, which is in turn used by a reinforcement learning (RL) step to fine-tune the model. Such reward model serves as a proxy to human preference, and it is critical to guide the RL step towards improving the model quality. In this work, we argue that the SFT stage significantly benefits from learning a reward model as well. Instead of using the human demonstration data directly via supervised learning, we propose to leverage an Inverse Reinforcement Learning (IRL) technique to _simultaneously_ build an reward model and a policy model. This approach leads to new SFT algorithms that are not only efficient to implement, but are robust to the presence of low-quality supervised learning data. Moreover, we discover a connection between the proposed IRL based approach, and a recent line of works called Self-Play Fine-tune (SPIN, Chen et al. (2024)). Theoretically, we show that the proposed algorithms converge to the stationary solutions of the IRL problem. Empirically, we align 1B and 7B models using proposed methods and evaluate them on a reward benchmark model and the HuggingFace Open LLM Leaderboard. The proposed methods show significant performance improvement over existing SFT approaches. Our results indicate that it is beneficial to leverage reward learning throughout the entire alignment process. Our code is available at https://github.com/JasonJiaxiangLi/Reward_learning_SFT.

## 1 Introduction

Large Language Models (LLMs) have become the cornerstone of modern artificial intelligence applications. They are believed to lead the way towards artificial general intelligence (Bubeck et al., 2023), also have shown great capabilities towards specialized domains such as math problem solving (Cobbe et al., 2021; Trinh et al., 2024; Wei et al., 2022; Lewkowycz et al., 2022), code generation (Chen et al., 2021; Austin et al., 2021; Li et al., 2022), text generation (Anil et al., 2023; Touvron et al., 2023; Thoppilan et al., 2022), etc. Usually, researchers need to align the pre-trained LLMswith certain exquisitely prepared human-labeled data to achieve desired performance over certain tasks, a process which is thus known as alignment or fine-tuning. The alignment datasets can be categorized into two classes: the demonstration data, with the input prompt and a human response; and the preference data, with the input prompt and two responses, where human labeler will pick a chosen one and a rejected one. With the alignment datasets, one could employ methods like supervised fine-tune (SFT, Ouyang et al. (2022); Tunstall et al. (2023); Chung et al. (2024)) for aligning demonstration datasets, and reinforcement learning from human feedback (RLHF, Christiano et al. (2017); Ouyang et al. (2022)) and direct preference optimization (DPO, Rafailov et al. (2024)) for aligning preference datasets. More specifically, RLHF _explicitly_ trains a reward model and uses reinforcement learning (in particular, policy optimization) methods to obtain a fine-tuned version of the LLM; on the other hand, DPO and many of its extensions simplifies the RLHF by training the LLM policy model directly, while _implicitly_ learns the reward model via log of the ratio of likelihood between the learned model and a reference model. In practice, both types of methods exhibit better performance over SFT on the demonstration datasets, and they are adopted by state-of-the-art LLMs, for example ChatGPT benefited from RLHF (see Ouyang et al. (2022)), zephyr benefited from DPO (see Tunstall et al. (2023)).

It is interesting to observe that, when dealing with preference data, state-of-the-art methods usually build an (explicit or implicit) reward model to evaluate the quality of responses for a given prompt. On the contrary, typically no reward modeling is done for demonstration datasets. Why this is the case? One may argue that, for a given set of prompts, preference datasets contain explicit preference information which is not found in the demonstration datasets; since this kind of information is harder to extract, it motivates the use of complicated methods such as reward modeling. However, since _human preferences_ are also implicit in the demonstration data, one can argue that training a reward model that encodes human value distilled from these datasets may help to boost the alignment capability of the LLM. Indeed, in the RL literature, it is known that if the agents are given a set of demonstration data, then the so-called inverse RL methods (which learns the reward and policy simultaneously) can outperform the behavior cloning methods (which corresponds to supervised fine-tune in LLM alignments) by a large margin. In a Markov decision process (MDP), it is likely that supervised learning methods which naively fit the demonstration data will suffer from the distribution shift problem - the fine-tuned policy from supervised learning can produce unsatisfactory generations in certain states which were unseen in the training dataset (Ross et al., 2011). Through formulating the learning from demonstration problem in a MDP setting, typical inverse reinforcement learning methods (Ziebart et al., 2008; Ross et al., 2011; Zeng et al., 2022) can alleviate such distribution shift issues. Witnessing the success in ChatGPT, where the alignment of LLMs is modelled in the MDP setting due to the auto-regressive process, one would expect that the LLM alignment with demonstration datasets can be improved as well through deploying imitation learning / inverse reinforcement learning methods.

Inspired by the above observation, we pose the following question:

Does building a reward model using the demonstration data benefit the alignment process?

**Contribution of this work.** This paper answers the above question affirmatively. Specifically, by developing a framework based on certain IRL technique, we show that building a reward from demonstration datasets can significantly improve the quality of the resulting model, as compared to that obtained by standard reward-free SFT (1). Our main contributions are listed as below:

* We develop a new reward-based SFT approach, which takes the form of a _bilevel_ optimization, where in the _lower-level_, LLM policy is learned via policy optimization for a given reward, while in the _upper-level_, the reward model is optimized so to maximize the likelihood for observing the demonstration data.
* Based on the above formulation, we propose two alignment algorithms, one learns the reward model explicitly, and the other implicitly. For the first algorithm, we show that the reward learned from only demonstration data already possesses strong capabilities in distinguishing between chosen and rejected responses; see Figure 1 and our experiment for details. For the second algorithm, we made an interesting observation that implicitly learning a reward is equivalent to improving the model by comparing the demonstration data with the _synthetic_ data generated by the past models. Somewhat surprisingly, the resulting algorithm is closely related to the self-play fine-tune (SPIN, Chen et al. (2024)) algorithm, recently proposed from a completely different viewpoint. It is worth pointing out that unlike SPIN, our proposed algorithms have finite-time convergence guarantees.

* We demonstrate the power of the proposed approach theoretically and numerically. We prove that our implicit reward learning algorithm converges to some stationary point of our proposed formulation. We show that the proposed algorithms outperform vanilla SFT in almost all cases we have tested, for example the model performance on HuggingFace Open LLM Leaderboard increases from 59.47% to 61.03%. To our knowledge, this is the first work that formally demonstrate the power of reward learning when dealing with demonstration data for LLM alignment.

**Notations.** We use \((y|x)\) to denote the LLM output probability for continuation \(y\) with input prompt \(x\), and we refer to \(\) as the policy. We use the notation \((y|x;)\) if the model \(\) is directly parameterized by parameters \(\). For the case when \(\) is indirectly determined by parameter \(\), we use notation \(_{}(y|x)\). We use \(=\{(x,y)\}\) to denote the demonstration dataset and \(=\{(x,y_{w},y_{l})\}\) for the preference dataset, where \(y_{w}\) is preferred over \(y_{l}\). Since we assume that the demonstration continuations \(y\) are collected from a human expert distribution, we also denote \((x,y)\) as \(x,y^{E}(|x)\) when taking the expectations, where \(\) is the distribution of the input prompts when collecting the data. We similarly have the notation \(x,(y_{l} y_{w})^{P}(|x)\) for the preference dataset.

## 2 Preliminaries

Consider a Large Language Model (LLM) parameterized by \(\) and denote the output probability by \((y|x;)\) where \(x=[x_{1},...,x_{n}]\) is the sequence of input prompts and \(y=[y_{1},...,y_{m}]\) is the sequence of output continuation. Typical LLM is an auto-regressive model, meaning that it predicts the output probability of the \(y_{j}\) given all tokens in \(x\) and \(y_{<j}:=[y_{1},...,y_{j-1}]\) (\(y_{<1}\) is null), i.e.

\[(y|x;)=_{j=1}^{m}(y_{j}|x,y_{<j};).\]

In this paper, we do not focus on the architecture design of LLMs. We will fix the LLM architecture and always denote it as a probability model \((y|x;)\). The following discussions review two common procedures for fine-tuning \(\): (1) supervised fine tuning (SFT) over demonstration dataset, (2) reinforcement learning with human feedback (RLHF) over preference dataset that consists of two steps: LLM alignment/fine-tuning based on a reward model using policy optimization; and reward learning process to learn the optimal reward for the preference dataset.

**SFT.** Given a _demonstration dataset_\(:=\{(x,y)\}\), the SFT optimizes the following problem:

\[_{}\ _{}():=_{(x,y) }[(y|x;)].\] (1)

It is easy to see that the above problem shares the same optimal solutions as \(_{}\ _{x}[D_{}(^{E}(|x )\|(|x;))]\). The latter shows that SFT aims at imitating the demonstration dataset via minimizing the KL divergence. It is worth noting that the SFT stage described

Figure 1: **Left: Difference between SFT and the two proposed methods: RFT (Algorithm 1) and IRFT (Algorithm 2); Right: Log probability gap between the chosen/preferred continuation and the rejected/non-preferred continuations for different methods. All methods _only_ consume the chosen/preferred data, but RFT and IRFT can effectively distinguish between chosen and rejected continuations; see Example 2 in Sec. 3 for the detailed settings.**

here is closely related to the imitation learning approach used in the RL literature for learning from demonstration (Osa et al., 2018), whose goal is to mimic the policy of an expert.

**RLHF.** Suppose that we have a reward model \(r(x,y;)\) (parameterized by \(\) and to be defined later) for any given input and output pair \((x,y)\), the LLM can be fine tuned by the following RL problem:

\[_{}\ _{}():=_{x,y (|x;)}[r(x,y;)]-_{x }[D_{}((|x;)\|_{} (|x))],\] (2)

where \(_{}\) is a fixed reference model. Note that the KL regularization term in (2) is not computable given the sheer amount of possible output \(y\) (which could be _corpus_size_max_sequence_length_ in most language model tasks), therefore (2) is usually solved by standard policy optimization techniques such as REINFORCE (Ahmadian et al., 2024) or PPO (Schulman et al., 2017).

To find an appropriate reward model \(r(x,y;)\), RLHF (see e.g., Christiano et al. (2017)) leverages a set of _preference dataset_\(:=\{(x,y_{w},y_{l})\}\), where each data contains a pair of output \(y_{w},y_{l}\), and \(y_{w}\) is preferred over \(y_{l}\) by human labeler (denoted as \(y_{w} y_{l}\)). The Bradley-Terry model (Bradley and Terry, 1952) assumes that the probability of choosing \(y_{w}\) over \(y_{l}\) is

\[(y_{w} y_{l} x)=;x))}{(r( y_{w};x))+(r(y_{l};x))}=(r(y_{w};x)-r (y_{l};x)).\]

One could formulate the following problem to find the reward model:

\[_{}\ _{}():=_{x,(y_{l}  y_{w})^{p}(|x)}r(x,y_{w}; {})-r(x,y_{l};).\] (3)

It is widely observed in the literature that, models trained via episodically learning the policy (2) and learning the reward (3) typically outperforms those that are only trained using SFT (Ouyang et al., 2022). The reward model guides the performance of the LLM and allows a better generalization ability via the consistent input of the preference data from human labeler. Follow up works such as DPO proposes to incorporate reward learning implicitly by utilizing the structure of the optimal solution of the RL problem (2); for more details about the DPO, see Rafailov et al. (2024).

**Discussion.** At this point, let us take a step back and think about the above process. The LLM alignment problem takes human labeled demonstration and preference data to produce an _aligned_ model. Clearly, both kinds of data encode information about how human would like the LLM output to be, but the processes of extracting such information is quite different (i.e., supervised learning vs RL). A series of questions naturally arises: Is supervised learning the best way to extract human inclination from the demonstration data? Can we also learn a reward model from the demonstration data to gauge human preference? Will policy model learned via such reward improve the supervised learning approach? In the next section, we will dive deep to carefully address these questions.

## 3 Reward Learning and Policy Fine Tuning from Demonstration Data

In this section, we argue that reward learning from the demonstration dataset can benefit the LLM alignment problem. To do so, we develop a joint reward learning and policy fine tuning formulation and understand its capabilities in improving the LLM policy. The new formulation inspired us to develop two reward learning paradigms: _i)_ Explicit reward learning, where a (parameterized) reward model is learned together with the language model policy, and _ii)_ Implicit reward learning, where the reward model is learned implicitly through directly optimizing the policy, avoiding learning _two_ models simultaneously.

### Joint Reward-learning and Policy Fine-tuning by Inverse RL

A challenge with learning only from the demonstration dataset is that the Bradley-Terry model (3) can no longer be used due to the lack of _pairs_ of preference data. However, all is not lost as we recall that it is the _value_ of the reward model that should be used in the fine-tuning process (2). Therefore, with only demonstration data \(\), a reasonable formulation is to combine the supervised learning problem (1) with the optimal policy generation problem (2), by requiring that the generated policy to'match' with the demonstration dataset. With this intuition in mind, we consider the _joint_ reward and policy learning problem via a maximum likelihood inverse reinforcement learning (ML-IRL) formulation (Ziebart et al., 2008, 2013; Zeng et al., 2022):

\[_{}\ ():=_{x,y^{ }(|x)}[_{}(y x)]\] (4) \[\ _{}:=_{}_{x,y (|x)}[r(x,y;)- D_{} (|x)\|_{}(|x)].\]The above problem has a _bilevel_ structure which trains a reward model \(r(x,y;)\). At the upper level, its objective is similar to that of SFT (1), but is evaluated on the policy \(_{}\) induced by the reward model \(r(x,y;)\); meanwhile, this policy \(_{}\) is found in the lower level using the RL objective (2).

There are several advantages of the bilevel formulation (4) over standard SFT (1). First, we notice formulating SFT as a RL / IRL problem can alleviate distribution shift and improve the generalization power (Ross et al., 2011). In fact, we observe that (4) tends to give a less extreme policy even when the demonstration dataset is extreme. The latter is observed in the following stylized example.

**Example 1**.: Suppose we have only one state (input prompt) \(x\) and three actions (continuations) \(y_{1},y_{2},y_{3}\). Let the reference model \(_{}\) be a uniform distribution over all continuations, and the demonstration dataset is \(=\{y_{3}\}\). One could easily compute the optimal solution for (1) and (4) by first-order optimality conditions. From Table 1 we can see that SFT (imitation learning) pushes all the likelihood toward the demonstration dataset, whereas ML-IRL (4) maintains non-zero weights for unseen data in the demonstration datasets. This is particular useful when we want to fine-tune from a pre-trained model, which is presumed to be powerful and have useful information already. 

Second, since the lower level problem in (4) encapsulates a generation process, it is anticipated that the proposed method can better distinguish between the preferred and non-preferred data than SFT, even if it is only trained on the demonstration dataset. The following numerical example highlights this point:

**Example 2**.: We compare the solution of SFT (1) and IRL (4) numerically, where the latter is solved using two algorithms RFT and IRFT (to be introduced shortly). We choose the preference based dataset Anthropic-HH and only keep the preferred continuation to form a demonstration dataset \(}=\{(x,y_{w})\}\) to implement SFT and IRL. We then compute the log probability gap \(((y_{w}|x))-((y_{l}|x))\) between the preferred \(y_{w}\) and non-preferred \(y_{l}\) on the test dataset; see Figure 1 right side. We observe that although all three methods are not exposed to the non-preferred data \(y_{l}\) during the training process, the IRL-based methods effectively distinguish the preferred continuation over the non-preferred one, while SFT assigns larger probability to the non-preferred continuation (see Section 5 for the details of the implementation).

Comparing to SFT (1), the bilevel problem (4) appears to be more complicated. In particular, solving standard bilevel optimization problem typically involves computation of Hessian matrices, which is too expensive for LLM related applications (Liu et al., 2023). Fortunately, in our next result, we show that the bilevel problem can be significantly simplified (proof in Appendix B):

**Lemma 3.1**.: _Problem (4) is equivalent to the following minimax optimization problem:_

\[_{}_{}_{x,y^{}(|x ),(|x)}[)-r(x,;)}{}+D_{}(|x)\|_{}(|x) ].\] (5)

The above reformulation is remarkable. First, minimax problem is much easier to solve as compared with bilevel problem, e.g., a simple alternating minimization can yield reasonably good solution; see Algorithm 1 for such an algorithm, and Sec. 3.3 for its theoretical analysis. **More importantly**, it shows that even only the demonstration data is available, the reward optimization problem takes a similar form as what has been used in RLHF (3), where not one but _two_ reward functions are contrasted. The key difference here is that one reward is evaluated on the continuation \(y\) in \(\), the other is evaluated on \(\), which is the continuation _generated_ from the current policy \((|x)\). We believe that such contrast is the key reason that enables the IRL based formulation to distinguish the preferred continuations over the non-preferred ones; see Example 2 and Figure 1.

Now that we have turned the original bilevel problem (4) into a minimax optimization problem (5), we can naturally develop a gradient-descent-ascent type algorithm for (5), which alternates between updating the policy according to the current reward, and updating the reward based on the current policy -- an algorithm that we call _Reward-learning Fine-tune_ (RFT), see Algorithm 1. Note that in

   Action & \(y_{1}\) & \(y_{2}\) & \(y_{3}\) \\  \(_{}\) & \(0.33\) & \(0.33\) & \(0.33\) \\ \(\) & & \(\{y_{3}\}\) & \\  \(_{}\) & \(0.0\) & \(0.0\) & \(1.0\) \\  \(_{}\) & \(}\) & \(}\) & \(}{2+e^{R/}}\) \\   

Table 1: A state-less counter-example with three actions where IRL-based fine-tune (4) shows regularization effect over SFT (1) to maintain weights over unseen data in the demonstration dataset \(\). Here we assume \(r[0,R]\).

the data sampling step, we sample the response from the current model for the next \(K\) iterations. If we take \(K=1\) and \(T=_{*}}{_{*}}*\) epoch, the sampling process would be done for every iteration. In practice however we take a relative small \(T\) and large \(K\), because frequent on-line sampling is time consuming; see Section 4 for the implementation details.

### Implicit Reward-learning Fine-tuning via Self-generation

So far we have seen that (4) (equivalently (5)) can efficiently utilize the demonstration dataset for better alignment. However, the computation cost for training two models (reward and policy) is significantly higher than the standard SFT. It turns out that (4) can be simplified into a supervised learning problem. Observe the following property (see Appendix B for proof):

**Lemma 3.2**.: _For the loss function \(\) in (4), we have:_

\[_{}()=_{x,y^{ E} (|x),_{}(|x)}[_{} }(y|x)}{_{}(y|x)}-_{} }(|x)}{_{}(|x)}].\] (6)

The proof of the above lemma uses the identity \(r(x,y;)=}(y|x)}{_{}(y| x)}+ Z_{}(x)\) for some constants \(Z_{}(x)\); see, e.g. Rafailov et al. (2024). Again it is remarkable that, despite the fact that the IRL formulation only consumes the demonstration data \(=x,y\), the gradient of the IRL loss takes the form as the difference of two gradients, one related to the demonstration data, the other related to the data generated by the current policy.

Lemma 3.2 leads to a simple scheme for _implicit reward-based supervised fine-tune_ (IRFT) - for each training batch, it samples the response from the current model, and construct the gradient estimator (6) directly to update the parameters \(\). This results in Algorithm 2, which is an SGD type algorithm for (4). In Algorithm 2, we use a double loop since generation at each step might again significantly take more time, similar to Algorithm 1. If \(K=1\) we get a single loop algorithm where we generate for every training step on the input batch.

``` Input: Initialize reward parameter \(_{0}(_{-1,K}=_{0})\) and policy model \(^{0}\), the stepsize of reward update \(_{t}\), and \(T\), \(K\) the outer and inner iterations. for\(t=0,1,,T-1\)do  Take \(_{t,0}=_{t-1,K}\)  Data Sample: Sample state \(x_{t,k}\), an expert response \(y_{t,k}^{ E}(|x_{t,k})\) and agent response \(_{t,k}^{t}(|x_{t,k})\), for \(k=0,1,...,K-1\) for\(k=0,1,...,K-1\)do  Estimate Gradient: Calculate the stochastic gradient \(g_{t,k}\) w.r.t. \(\) via \(g_{t,k}=_{}r(x_{t,k},y_{t,k};_{t,k})- _{}r(x_{t,k},_{t,k};_{t,k})\)  Reward Alignment:\(_{t,k+1}:=_{t,k}+_{t}g_{t,k}\) endfor  Policy Alignment: Update the optimal \(^{t}(y|x)(r(x,y;_{t,K}))\) according to (9) endfor ```

**Algorithm 1**_Reward-learning Fine-Tune_ (RFT)

### Convergence Theory

We conclude the section by theoretically inspecting the proposed algorithms. Note that details and proofs of convergence theorem are moved to Appendix B due to page limits. We observe:

**Theorem 3.1**.: _Under Assumption B.1, for Algorithm 1 and 2 with \(_{t}=(1/)\) we have_

\[_{t=1,...,T,\;k=1,...,K}[\|(_{t,k})\|^{2}] (1/+1/T).\]

Theorem 3.1 indicates that the convergence dependency is \((1/)\) (assuming \(T>K\)), which indicates that the algorithm could converge to stationary point if we take both the inner loop and outer loop reasonably large. This is slightly contrary to the intuition since with larger inner loop number \(K\), we are having more biased estimators. This theorem shows that this biasedness actually wouldn't harm the final convergence, thus validate our practice of having a relative large inner loop number \(K\) in practice (since generating at each training iteration is time-consuming).

## 4 Discussions

**Implementation details of RFT**. As mentioned, training a reward model and a policy at the same time is costly. In our experiments, we discovered that the reward alignment step can be completely separated from the policy alignment step. In particular, we take \(T=1\) and \(K=}{}*\) so that we train the reward over the entire dataset and then switch to the policy alignment. In our experiments, we indeed observe that only one round of above procedure can readily show superior performance over SFT and implicit reward-learning methods for pythia-1.4b model.

**Implementation details of IRFT**. It is worth noticing that in (6), the policy \(\) is not parameterized by \(\) directly. In our numerical experiment, we directly parameterize the LLM \(\) by \(\), making (6) the gradient of an supervised optimization problem itself. Meanwhile, it is not straightforward to calculate the self-generation gradient (6) directly, thus we need to design a loss function for back-propagation in main-stream packages such as PyTorch and TensorFlow. In practice, at each training iteration we first sample \((|x;)\) and pass the following loss function

\[h()}{_{}(y|x)}- |x;)}{_{}(|x)})\] (7)

into the standard optimizers (such as SGD or Adam) for back-propagation. Here \(h\) is a nonlinear function. We take \(h=\) where \(\) is the logistic loss function \((t):=(1+(-t))\) as in Rafailov et al. (2024), Chen et al. (2024) for its non-negativity, smoothness and exponentially decaying tail to avoid excessive growth in the absolute value of the log-likelihood.

**Discussion on the computational costs**. For Algorithm 1, we need to maintain a reward model and a policy model (which is the LLM), and this is doubling the standard LLM fine-tuning. Thus the memory consumption and computation time of Algorithm 1 is similar to the standard RLHF process (RLHF = reward learning + policy optimization); For Algorithm 2, we simply need to maintain the policy (LLM) model, and the memory consumption would be exactly the same as the standard SFT, whereas the computation time would involving generating for the entire training sample, which would be of similar level as the standard policy optimization process (same computational time as SPIN). Note that standard policy optimization process is equivalent to the time of standard SFT and a generation process toward all training input prompts.

We summarize the memory consumption and the computational time of the proposed methods in Table 2, assuming that the reward and policy models are of same size. Here "Forward" means the memory required for storing a model in inference mode, and "Backward" is the memory required for storing a model in training mode, including weights, activations and gradients; also "SFT" means the computational time as standard

   Method & Peak Memory & Computation Time \\  Algorithm 1 & Forward+Backward & 2SFT+Generation \\  Algorithm 2 & Backward & SFT+Generation \\   

Table 2: Table summarizing the computational costs of proposed methods.

SFT, and "Generation" means the time to generate continuations for each of the input training prompts. Therefore "2SFT+Generation" is roughly the same time as standard RLHF.

**Comparison to SPIN**. We discuss here the connection between our proposed algorithms with the self-play fine-tune algorithm (SPIN in Chen et al. (2024)), which also maximizes the gap between two rewards. First, SPIN is motivated by certain two-player games, while in our case, we show that the difference of two rewards in (5) naturally comes from _a single_, _reward learning_ agent; see (4).

Second, IRFT covers SPIN as a special case. In particular, if we take \(T=1\) and \(K\) as the total number of training iterations, the IRFT algorithm is equivalent to SPIN. In practice, we tested on different choices of \(T\) and show that a reasonable generation frequency can results in a strong model performance.

Finally, since SPIN does not involve explicit reward learning, its connection to RFT is relatively remote. It is worth noting that the relation between the proposed Algorithm 1 and Algorithm 2 is similar to that of RLHF to DPO. There has been intensive discussions regarding whether reward-based or reward-free algorithm gives better model performances, but this topic is beyond the scope of the current paper. We refer to Xu et al. (2024) for a comprehensive study.

## 5 Numerical experiments

In this section we study the proposed Algorithm 1 and 2 numerically. Our experiment mainly show the advantages of the proposed methods in the following aspects: (1) Reward learning is key to improve over standard SFT, even if we do not have preference dataset; (2) The double loop design in both Algorithm 1 and 2 enable us to explore appropriate parameter settings that could break the performance limits of the state-of-the-art methods, including SFT and SPIN.

### Experiment Setup

**Model and Datasets.** Since reward-based methods can be costly by training two models at the same time, we mainly test Algorithm 1 on pythia-1b reward model and pythia-1.4b policy model (Biderman et al., 2023). We tested pythia on Anthropic-HH dataset (Bai et al., 2022). Anthropic-HH is a preference dataset that provide two continuations based on helpfulness and harmlessness, and we only pick 10k chosen/preferred continuation data to form the demonstration dataset, which enable us to check the log likelihood of the non-preferred continuation without feeding the model with such data. At each iteration, we train our model for 2 epochs (seeing each data for two times).

Algorithm 2 is tested on two models: pythia-1.4b and zephyr-7b-sft-full (Tunstall et al., 2023). We tested on Ultrachat200k dataset by HuggingFace, which is a subset of the high quality demonstration UltraChat dataset(Ding et al., 2023) for text generation and dialogue. For Ultrachat200k, we adopt the same strategy as Chen et al. (2024) to pick up 50k data for training. At each iteration, we again train our model for 2 epochs.

**Evaluation.** For the Anthropic-HH dataset, we show the reward evaluated by the PKU-Alignment/beaver-7b-v3.0-reward (Dai et al., 2024; Ji et al., 2023) model which is a popular 7b model fine-tuned from meta-llama/llama-2-7b tailored for evaluating human preferences regarding helpfulness and harmlessness. We also record win rate of the two proposed methods over base model and SFT model. For the Ultrachat200k dataset, we follow the widely used HuggingFace Open LLM Leaderboard (Beeching et al., 2023). This evaluation package assess an LLM based on six tasks: LLMs on commonsense reasoning (Arc Clark et al. (2018), HellaSwag Zellers et al. (2019), Winogrande Sakaguchi et al. (2021)), multi-task language understanding (MMLU Hendrycks et al. (2020)), human falsehood mimic (TruthfulQA Lin et al. (2021)) and math problem solving (GSM8K, Cobbe et al. (2021)). See the appendix for more implementation details.

### Results of RFT (Algorithm 1)

We present the result of Algorithm 1 over Anthropic-HH dataset. We first fine-tuned pythia-1.4b using supervised fine-tune over the entire dataset (160k training data in total) using only the preferred/chosen data for 10 epochs and pick up the checkpoint with the best testing accuracy as our base model. We then use PKU-Alignment/beaver-7b-v3.0-reward model as our ground truth reward model. We use this model to pick 10k data from Anthropic-HH dataset with the highest reward scores. Next, we fine-tune the base model using SFT and Algorithm 1. Figure 2 shows the experiment results on averaged reward and win rate, where we record the average score (byPKU-Alignment/beaver-7b-v3.0-reward) of the continuation generated for test datasets, also the win rate (ratio of samples where the reward of our model's generation is higher than the model compared) of the proposed Algorithm 1 over the full SFT base model and the top 10k SFT model. The figures show that the proposed algorithm improves over the SFT models in terms of effectively improve the helpfulness and harmlessness of the model continuation.

We remind the readers that the advantage of Algorithm 1 over SFT in Figure 2 can be partially explained by Figure 1 right side: despite the fact that SFT, Algorithm 1 and 2 are only observing chosen/preferred data, the latter two still outperforms SFT since they discourage the likelihood of the synthetic non-preferred data, thus bringing better performance and robustness for the model.

### Results of IRFT (Algorithm 2)

Different from the time consuming Algorithm 1, Algorithm 2 is more capable of handling large data and models. We first present the result for pythia-1.4b models over Ultrachat200k data. We remind the reader again that \(T=1\) in Algorithm 2 is equivalent to SPIN (Chen et al., 2024)1. We tested on different choices of \(T\) and identify that \(T=5\) to \(8\) gives the best performance in the Open LLM Leaderboard evaluations.

The Open LLM Leaderboard result is presented in Table 3. We have the following main observations based on the results in Table 3:

1. SFT is not efficient in terms of boosting the pre-trained model performance on downstream tasks comparing to methods which promote the decreasing of the likelihood of synthetic data, namely SPIN and IRFT;
2. SPIN and IRFT (Algorithm 2) are both capable of further improving the performance of pythia model over downstream tasks, whereas IRFT shows better results due to more frequent generation comparing to SPIN. IRFT with \(T>1\) outperforms both SFT and SPIN on most of the tasks as well as the average score;
3. More frequent generation might also result in more variances, therefore a reasonable \(T\) (around 5) results in the best evaluation performance. Careful hyperparameter tuning might be needed for different models and datasets when applying our method, while we recommend using \(T=5\) as the default setting.

Apparently 1b model is not strong enough to handle hard tasks, e.g. GSM8k and all model performances are not desirable. Now we present the result for zephyr-7b-sft-full. We remind the reader that this is a fully SFT-ed model and further SFT would only detriment the model performance (see Chen et al. ). The results are presented in Table 4 where we can see that similar to the 1b case, both SPIN and IRFT could effectively improve the performance of SFT-ed model and the average performance of IRFT with \(T=5\) stands out. The success of IRFT and SPIN further suggest that reward learning is indeed beneficial for aligning with demonstration data.

Figure 2: Algorithm 1 fine-tuning result of pythia-1.4b over Anthropic-HH (with top 10k data picked by PKU-Alignment/beaver-7b-v3.0-reward). We record the average score of test dataset on the left figure and the win rate of Algorithm 1 over the (full SFT) base model and the SFT model.

## 6 Conclusions and Limitations

In this paper we proposed reward-learning approaches for aligning LLMs with demonstration datasets. We show both theoretically and numerically the great potential of reward-learning for alignment even without preference dataset. Our theory only indicate the convergence of the proposed algorithm to stationary point, and it is not clear what the policy converges to. The additional computation resources required for tuning two models or generate synthetic data in our algorithms are not negligible. Future works include exploring reward-learning for larger models and more complicated demonstration tasks, boosting the algorithm efficiency, and understanding how synthetic negative sample helps the LLMs to distinguish the preference dataset, etc.