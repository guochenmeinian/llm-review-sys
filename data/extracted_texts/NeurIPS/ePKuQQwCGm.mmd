# Targeted Unlearning with Single Layer Unlearning Gradient

Zikui Cai

zikui@umd.edu

Yaoteng Tan

ytan082@ucr.edu

M. Salman Asif

sasif@ucr.edu University of Maryland, College Park, MD

###### Abstract

The unauthorized generation of privacy-related and copyright-infringing content using generative-AI is becoming a significant concern for society, raising ethical, legal, and privacy issues that demand urgent attention. Recently, machine unlearning techniques have arisen that attempt to eliminate the influence of sensitive content used during model training, but they often require extensive updates in the model, reduce the utility of the models for unrelated content, and/or incur substantial computational costs. In this work, we propose a novel and efficient method called Single Layer Unlearning Gradient (SLUG), that can unlearn targeted information by updating a single targeted layer of a model using a one-time gradient computation. We introduce two metrics: layer importance and gradient alignment, to identify the appropriate layers for unlearning targeted information. Our method is highly modular and enables selective removal of multiple concepts from the generated outputs of widely used foundation models (e.g., CLIP), generative models (e.g., Stable Diffusion) and Vision-Language models. Our method shows effectiveness on a broad spectrum of concepts ranging from concrete (e.g., celebrity name, intellectual property figure, and object) to abstract (e.g., novel concept and artistic style). Our code is available at [https://github.com/CSIPlab/SLUG](https://github.com/CSIPlab/SLUG).

## 1 Introduction

Modern generative models, including large language models (LLMs) , Stable Diffusion (SD) , and vision language mdoels (VLMs)  leverage vast amounts of data for training. While these large unsupervised datasets enhance performance under scaling law , they also raise serious data privacy and legal compliance  concerns. Completely abandoning trained model weights and re-training large models from scratch using scrutinized dataset is prohibitively expensive, highlighting the need for efficient unlearning methods.

Machine unlearning (MU)  refers to a set of techniques designed to reverse the learning process, which aims to efficiently remove targeted information from a trained model without re-training the model from scratch. MU has three main objectives: **(1) Low computational cost**, as the naive approach of re-training models usually achieves the best result (exact unlearning) at the expense of large computational cost. **(2) Effective unlearning**, to ensure that the model forgets the intended data completely. **(3) Utility retention**, maintainingthe original model performance, in terms of accuracy and utility on data/tasks unrelated to the intended forgotten data. Current MU methods often fall short of meeting all these objectives simultaneously. Traditional methods like fine-tuning (FT)  and gradient ascent (GA)  struggle to balance effective forgetting with utility preservation. More recent techniques such as saliency unlearning (SalUn)  and selective synaptic dampening (SSD)  attempt to address this by identifying and updating only salient parameters. While these methods improve overall unlearning performance, they still face the following challenges: (1) they usually involve iterative updates over the model parameters, resulting in high computational costs . (2) The significant weights targeted for updates are often spread throughout the model, offering limited insight into the model's structure. (3) They require careful hyperparameter tuning, including learning rate, number of iterations, and parameters for selecting suitable masks in saliency approaches.

In this work, we propose a novel and efficient method for unlearning targeted information, namely **Single Layer Unlearning Gradient (SLUG)**. Figure 1 provides an overview of our proposed framework. SLUG performs three main steps using given unlearning query with retain and forget sets: (1) calculate one-time gradients for the forget and retain losses; (2) identify a single layer with high importance to the forget set and low relevance to the retain set; (3) update the targeted layer along a linear path using one-time calculated gradient. In addition to its efficiency and effectiveness, our methods offers higher modularity and better interpretability compared to . SLUG precisely identifies layers associated with distinct concepts, which provides insights into the features learned by different layers and their functionalities, offering generalized guidance for new tasks and model architectures design.

## 2 Background

### Machine unlearning preliminaries

The goal of MU is to remove the influence of a specific subset of training data, \(D_{} D\), on a pre-trained model \(F_{}(D)\) with parameters \(\). The challenge is to make this process more efficient than re-training the model on the retain set \(D_{}=D D_{}\). The unlearning algorithm \(U\) should produce an unlearned model \(F_{_{}}=U(F_{}(D),D,D_{})\) that is functionally equivalent to a model retrained only on \(D_{}\) (i.e., \(F_{_{}}(D_{})\)), which can be formalized as:

\[_{}}}_{(x_{},y_{ }) D_{}}(F_{}(x_{}),y_{})} _{_{}}-}}_{ (x_{},y_{}) D_{}}(F_{}(x_{}),y_{})}_{_{}} \]

Figure 1: Overview of our proposed Single Layer Unlearning Gradient (SLUG) framework. Given an unlearning query, we curate a forget set and retain set, then compute corresponding gradients. The gradient alignment guide identifying single layer updates for unlearning. A binary search helps determine the step size \(\), effectively erasing specified concepts while preserving the model’s utility.

where \(N\) is the number of elements in \(D\), \(\) is a balancing factor, and \(\) is the loss function.

#### 2.1.1 Vision language alignment

Traditional MU approaches struggle with high computational costs and limited scalability, which restricts their application to small-scale image classification models [16; 9]. In contrast, our method breaks away from these constraints by offering superior scalability and flexibility that is suitable for large multi-modal foundation models such as CLIP, SD, and VLMs.

CLIP , in particular, is pivotal in advancing multi-modal models by aligning visual and textual representations through contrastive loss :

\[=_{i=1}^{N}(_{i2i}(i)+_{i2i}(i)), \]

\[_{i2i}(i)=-_{i},_{i})/)}{_ {j=1}^{N}((_{i},_{j})/)},\ \ _{i2i}(i)=- _{i},_{i})/)}{_{j=1}^{N}( (_{i},_{j})/)}. \]

Here, \(_{i}\) is the normalized image embedding from the vision model \(f_{}\), and \(_{i}\) is the normalized text embedding from the text model \(f_{}\). The temperature \(\) controls the sharpness of the softmax probability distribution, while cosine similarity is defined as \((_{i},_{j})=_{i}_{j}\). Minimizing this contrastive loss aligns the vision and language representations in the embedding space. In unlearning, our goal is to break these learned alignments.

#### 2.1.2 Loss functions for gradient calculation

Selection of an appropriate loss functions to perform unlearning is critical. In the scenario for contrastive learning we focus on contrastive loss. The loss for retain and forget sets are defined as follows:

\[_{}=_{i=1}^{N}(_{i2i}(i)+ _{i2i}(i)),\ \ \ \ _{}(_{i},_{j})=1- (_{i},_{j}) \]

Retain loss is the original contrastive loss as in equation 2. For the forget loss, we employ the cosine embedding loss that directly pushes the embeddings of positive pairs away while not tampering with the embeddings of negative pairs. Using the original contrastive loss as forget loss will result in ineffective unlearning.

## 3 Single layer unlearning gradient (SLUG)

Our approach improves the state-of-the-art along three axes: (1) low computational cost, (2) effective unlearning, and (3) high retention of general utility. The framework is illustrated in Figure 1.

### Layer identification

SLUG is inspired by the nature of different layers in deep networks learn distinct features [41; 30; 12]. To efficiently unlearn, our goal is to identify the layers most critical to unlearn targeted concepts while preserving the model's functionality. To achieve this, we perform unlearning within the "nullspace" of the retain set, focusing on layers that minimally impact retained data performance while effectively removing the targeted features.

To measure the influence of each parameter, similar to [3; 9], we use the Fisher information matrix[18; 14; 19], approximated by its diagonal for computational feasibility:

\[_{D}()=-[}{^{2 }} L(;D)]=[( L(;D))( L(;D ))^{}] \]

The diagonal elements reflect the sensitivity of the log-likelihood to parameter changes, and we extend this to layers by aggregating sensitivities. The importance of a layer is determined by the ratio of the \(_{2}\) norm of the forget loss gradients to the \(_{2}\) norm of the layer's parameters:

\[(l)=_{D_{L}}( _{l})}}{\|_{l}\|_{2}}=}_{ }(,D_{})\|_{2}}{\|_{l}\|_{2}} \]

Importance of layer alone is insufficient. We also ensure that forget gradients are nearly orthogonal to retain gradients by minimizing the gradient alignment:

\[(l)=_{_{l}} _{}(,D_{}),_{_{l}}_{}(,D_{}) \]

Small alignment would prevent unlearning updates from negatively affecting the retain set. To balance both objectives, we use the concept of a Pareto optimal set , optimizing both importance and gradient orthogonality. Figure 2 illustrates the Pareto front for unlearning a person identity from CLIP ViT-B-32, showing layers that unlearn the forget set without harming retain data.

### Linearizing unlearning trajectory

Existing unlearning methods calculate gradients at each iteration to update model parameters, which significantly increases computational complexity. However, inspired by task arithmetic  and the linear nature of many optimization problems , we observe that repeated gradient calculations may be redundant. Instead, we propose calculating the gradient only once for the initialized model and updating the parameters \(_{l}\) of any layer \(l\) in a weight-arithmetic fashion. Specifically, the weights are updated along a fixed gradient direction in every iteration:

\[_{l}^{(t)}_{l}^{(0)}-^{(t)}_{_{l}} _{}(,D_{})_{=^{(0)}} \]

Figure 2: Layer identification (a,d) and unlearning with a single gradient (b,e). The first column shows gradient alignment and importance metrics for vision and language models from CLIP ViT-B-32, highlighting layers on the Pareto front for unlearning an identity. The second column demonstrates effective unlearning by updating identified layers along a single gradient direction without significantly impacting retain set performance. The third column shows that iterative methods (GA and GAFT) offer no advantage over a single gradient and require early stopping to prevent over-unlearning.

Here, \(_{l}^{(t)}\) represents the parameters of layer \(l\) at iteration \(t\), with \(_{l}^{(0)}\) being the initial parameters. The gradient \(_{_{l}}_{}(,D_{})_{= ^{(0)}}\) is calculated once, based on the forget loss \(_{}\) evaluated on the forget set \(D_{}\). The step size \(^{(t)}\) controls the update magnitude.

Instead of recalculating gradients, we use the initial gradient direction for updates, effectively linearizing the unlearning trajectory. This approach reduces computational complexity yet ensuring effective unlearning. To search a proper step size \(^{(t)}\), we perform binary search along the linearized path, halting when the evaluation metric indicates satisfactory unlearning without harming performance on the retain set. This method strikes a balance between efficiency and precision, maintaining model utility while achieving unlearning goals.

### Generalization to Stable Diffusion and VLMs

By harnessing effective unlearning in CLIP models, SLUG can be extended to larger generative models built on CLIP, such as Stable Diffusion (SD) and Vision-Language Models (VLMs).

**Unlearn SD**. Diffusion models, known for generating high-quality images from text, use a text encoder (e.g., CLIP ViT-H/14 in SDv2.1) to embed prompts into a high-dimensional space. The text embedding guides the denoising process through cross-attention, starting from an initial noise \(_{T}\) and iteratively denoising at each step:

\[_{t-1}=}(_{t}-_{t}_{ } p(_{t}|))+}_{t} \]

where \(_{t}\) is the noisy image at step \(t\), \(_{t}\) is the noise added at step \(t\), \(_{t}\) is a time-dependent parameter controlling the noise balance, \(_{t}\) is the learning rate, \(=f_{}()\) is the text embedding, and \(_{} p(_{t}|)\) is the gradient of the log-probability of the noisy image given the text embedding, guiding the denoising process. We freeze the CLIP vision model and only update the language model to achieve unlearning.

**Unlearn VLMs**. VLMs enable LLMs to process multi-modal information. LLaVA-1.5  uses a pretrained CLIP vision encoder ViT-L/14-336px to extract the visual features \(=f_{}()\), which are projected as visual tokens \(_{}=\) through an MLP \(\). These tokens are then concatenated with language tokens \(_{}\) as input \(=[_{};_{}]\) to the language model. Since VLMs rely on the vision encoder, unlearning specific concepts in the CLIP vision model can directly influence the language model's output.

## 4 Experiment

### Experiment setup

**Models.** We mainly experiments on CLIP and CLIP-based generative model SD, VLMs. For CLIP, we used architecture ViT-B-32 trained on LAION-400M dataset , and pre-trained weights from the OpenCLIP . For SD, we used the SDv2.1 from StabilityAI that built on the CLIP-ViT-H-14 text encoder, pre-trained on the LAION-5B dataset.

**Datasets.** We used publicly-available datasets to construct the forget, retain, and evaluation sets. For unlearning target identities, we curated the forget set by filtering the LAION-400M dataset to isolate 1,000 to 6,000 image-text pairs per identity. The retain set consists of a single shard from LAION-400M, containing approximately 7,900 images (due to expiring URLs). To assess unlearning effectiveness, we used the CelebA dataset , sampling 100 frequently appearing celebrities from LAION-400M. Post-unlearning, model utility was evaluated using the ImageNet dataset for zero-shot classification.

**Evaluation metrics.** For CLIP, we measure unlearn performance using forget accuracy, defined as the zero-shot classification accuracy on unlearned content. Following the standard zero-shot paradigm , predictions are based on the highest cosine similarity between image and text embeddings. The model utility retention is assessed via zero-shot accuracy on ImageNet and CelebA.

**Comparing methods.** We compare with the state-of-the-art methods along with classical methods. For unlearning in CLIP models, we compare with classical fine tuning (FT) , gradient ascent (GA) / negative gradient (NG) , and recent salient parameters based saliency unlearning (SalUn) , and selective synaptic dampening (SSD) .

### Unlearning for CLIP

**Unlearning identities.** We demonstrate that modifying a single layer suffices to unlearn an identity or concept while preserving the model's overall utility. Figure 3 presents an example of unlearning targeted identity "Elon Musk" on CLIP. Each cell in matrices shows the cosine similarity between the embeddings of an image-text pair. Before unlearning (Figure 2(a)), high similarity values are observed along the diagonal, indicating strong alignment between images and corresponding text descriptions across all identities. After unlearning (Figure 2(b)), the similarity of targeted identity image-text pairs decrease, while other identities remain largely unaffected. This demonstrates SLUG's precision in selectively removing specific information. Additional studies on other identities and model architectures are presented in the Appendix Figure 6 and 8. Moreover, Figure 8 showcases SLUG's capability to simultaneously unlearn multiple identities, highlighting its scalability and flexibility.

**Unlearning without losing utility.** A key advantage of SLUG is that performance on unrelated tasks remains intact. Table 1 presents quantitative performance comparisons of various methods for classification on ImageNet and CelebA. For CelebA, unlearning an identity slightly reduces accuracy due to its close relationship with the data distribution. As shown in Table 1, our method outperforms others in forget and retain accuracy while maintaining minimal computational complexity, requiring only a one-time gradient

   Method & FA@1 (↓) & FA@5 (↓) & TA\_IN@1 (↑) & TA\_CA@1 (↑) & Compute Time (\(\)) \\  Original & 73.05 & 92.22 & 60.12 & 61.38 & \((K N)\) \\    &  & \) / \(10^{-7}\)} \\  & & & & & \\    &  &  &  &  & (k N_{})\)} \\  & & & & & \\    &  &  &  &  & (k(N_{}+N_{}))\)} \\  & & & & & \\    &  &  &  &  & (N_{})+(k(N_{}+N_{}))\)} \\   SSD  & 0.00 & 0.00 & 51.84 & 35.96 & \((N_{}+N_{})\) \\ SLUG (ours) & 0.00 & 0.00 & 59.96 & 58.32 & \((N_{}+N_{})\) \\   

Table 1: Performance comparison of different unlearning methods on CLIP zero-shot classification. FA@ stands for top- forget accuracy (%), i.e., accuracy of unlearned identity. TA\_IN@1 and TA\_CA@1 stands for the top-1 test accuracy (%) on ImageNet and CelebA dataset, respectively. \(K\) and \(k\) denotes the number of epochs for training and iterations for unlearning, respectively (\(K=32\) and \(k=10\) in our experiments). \(N\) is the size of whole training set, which is much larger than our sampled forget set (\(N_{}\)) and retain set (\(N_{}\)).

Figure 3: Cosine similarity matrix of image-text pairs before & after unlearning "Elon Musk" as an example. (a) original CLIP correctly associate images and text of distinct identities with high similarity. (b) after unlearning, the image-text pair of “Elon Musk” is no longer matched, while other identities are only slightly affected.

computation (\((N_{t}+N_{})\)) for unlearning. In contrast, other methods need \(k\) iterative gradient calculations and careful hyperparameter tuning, such as learning rate, to balance unlearning effectiveness and utility preservation. For instance, a high learning rate (e.g., \(10^{-6}\)) compromises utility, while a low rate (e.g., \(10^{-7}\)) reduces unlearning effectiveness.

**Localizing layers.** Our method efficiently identifies critical layers for unlearning, reducing the search space from hundreds to just a few. Figures 2, 7, and 12 show which layers are selected for unlearning different identities. This is achieved by combining two key metrics: layer importance, which measures how sensitive the forget loss is to changes in each layer, and gradient alignment, ensuring updates minimally affect the retain set. These metrics help identify Pareto-optimal layers that balance effective unlearning with preserving model utility (explained further in Section 3). The late attention layers in vision models and early attention layers in language models are targeted for updates because they play critical roles in refining high-level features and establishing foundational understanding, respectively. In vision transformers, late layers focus on contextually rich features, while in language models, early layers process key sequential and contextual dependencies.

### Unlearning for Stable Diffusion

**Unlearning identity.** Stable Diffusion (SD) models excel in text comprehension and image generation, producing high-fidelity results such as "a portrait of Elon Musk." Adjusting the prompt can create imaginative content, such as "Elon Musk on Mars." However, their potential misuse raises concerns about harm to data providers . This study demonstrates how to erase personal information from an image generation model, ensuring prompts for the erased individual produce inaccurate results. Figure 4 shows examples before and after unlearning. Our method, when applied to Elon Musk, generates electronic circuits consistently, without reducing the model's ability to produce diverse objects. In contrast, other methods degrade both the portraits of others and the quality of unrelated images. We provide additional results on unlearning more celebrity identities, and other case studies on unlearning copyright-protected content and novel concept, in Section G.

Figure 4: Images generated by different SDs using column captions as prompts. First row: images generated by the original pretrained SD. Second row: outputs of the SD after “Elon Musk” is unlearned using SLUG. We can see that “Elon Musk” is effectively unlearned, whereas other objects are unaffected. Bottom two rows: outputs of the SDs after “Elon Musk” is unlearned by existing methods (SalUn and ESD). We observe images generated for other unrelated prompts are also affected to some degree.

### Unlearning for VLMs

VLMs excel in visual understanding and question answering, accurately identifying individuals in images. Figure 5 shows that when given an image of Elon Musk and asked, "What's the name of the person in this image?", the model correctly names him.

Our experiments focused on the VLM model LLaVA-1.5, which uses a pre-trained CLIP visual encoder to extract visual features from images. These features are transformed into a format compatible with the language model using a neural network layer that projects them into the word embeddings space. The resulting visual tokens are combined with language tokens and fed into the model to generate responses. The key insight of our method is that the vision capability of VLMs heavily relies on the visual encoder. Therefore, by unlearning certain concepts from the CLIP vision model, we can influence the language model's understanding and generation of responses without directly modifying the language model itself. Figure 5 demonstrates the effectiveness of our approach. When given an image of Elon Musk and asked to identify the person, the original model correctly names him. After applying our unlearning method, the model incorrectly identifies Elon Musk as Michael Jackson, indicating that the specific identity information has been successfully removed. This alteration does not significantly impact the model's overall utility. Additional examples of this phenomenon are discussed in Section H.

## 5 Conclusion

In this work, we introduced SLUG, an efficient machine unlearning method that requires just a single gradient computation and updates only one layer of the model. SLUG enhances unlearning feasibility on large models, especially with constrained hardware, while preserving overall model utility. Our experiments with CLIP, and Stable Diffusion show that SLUG outperforms existing methods, particularly in unrelated tasks, with minimal computational overhead. The key innovation of SLUG is its ability to identify and update only the most relevant layers for the desired unlearning concepts, which also provides new insights into the internal representations learned by different parts of neural networks. This contributes to the ongoing effort to improve the interpretability and transparency of AI systems.