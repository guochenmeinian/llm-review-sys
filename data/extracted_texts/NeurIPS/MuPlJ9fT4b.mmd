# Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning

Wuyang Chen

Simon Fraser University

&Jialin Song

Simon Fraser University

&Pu Ren

Lawrence Berkeley National Laboratory

&Shashank Subramanian

Lawrence Berkeley National Laboratory

&Dmitriy Morozov

Lawrence Berkeley National Laboratory

&Michael W. Mahoney

International Computer Science Institute

Lawrence Berkeley National Laboratory

University of California, Berkeley

Equal contribution.

###### Abstract

Recent years have witnessed the promise of coupling machine learning methods and physical domain-specific insights for solving scientific problems based on partial differential equations (PDEs). However, being data-intensive, these methods still require a large amount of PDE data. This reintroduces the need for expensive numerical PDE solutions, partially undermining the original goal of avoiding these expensive simulations. In this work, seeking data efficiency, we design unsupervised pretraining for PDE operator learning. To reduce the need for training data with heavy simulation costs, we mine unlabeled PDE data without simulated solutions, and we pretrain neural operators with physics-inspired reconstruction-based proxy tasks. To improve out-of-distribution performance, we further assist neural operators in flexibly leveraging a similarity-based method that learns in-context examples, without incurring extra training costs or designs. Extensive empirical evaluations on a diverse set of PDEs demonstrate that our method is highly data-efficient, more generalizable, and even outperforms conventional vision-pretrained models. We provide our code at https://github.com/delta-lab-ai/data_efficient_nopt.

## 1 Introduction

Recent advancements in machine learning methodology have shown promise in solving partial differential equations (PDEs) . A significant development in this area is the concept of operator learning for PDEs. This approach differs from traditional neural network methods, which are restricted to fixed-dimension input and output, since neural operators focus on learning mappings between function spaces . Like other neural network methods, neural operators are recognized to be universal approximators for any continuous operator , enabling them to approximate any physical operator, including solution operators for various parametric PDE families. A solution operator is defined as a function that maps physical inputs to output solutions. Previous work has shown that in simple settings, neural operators can effectively capture complex, multi-scale dynamic processes .

However, neural operators tend to suffer from a problem common to other deep networks, namely the _need for enormous quantities of data_. Limited availability of data is common in science andengineering. High-fidelity numerical simulations are computationally costly or even infeasible for many applications . For example, an extreme-scale simulation of magnitude 7.0 earthquake at frequencies up to 10 Hz in San Francisco requires 3600 Summit GPU nodes and 42.7 hour .

Motivated by this data-efficiency challenge, recent works, particularly in natural language processing (NLP) and computer vision (CV), have focused on unsupervised (or self-supervised) pretraining2 to reduce the cost of collecting or generating labeled data. Such pretrained models have been shown to be highly data-efficient in downstream fine-tuning , and they can even become few-shot learners without any downstream data . In CV, researchers collect large amounts of natural images without any manual labels, and then pretrain visual encoders with proxy tasks, such as Noise-Contrastive Estimation (NCE) , masked reconstruction , rotation and jigsaw prediction . In NLP, people typically pretrain models via next-word prediction or masked tokens .

However, unsupervised pretraining is still largely underexplored in Scientific Machine Learning (SciML). Therefore, our core question is: _How can we design unsupervised pretraining for operator learning to reduce the data simulation costs?_

In this work, we resort to unsupervised pretraining for neural operator learning to achieve data efficiency in SciML. We overview our framework in Figure 1. First, we define unlabeled data for PDEs, which avoids heavy computation costs for simulating PDE solutions. We propose two physics-inspired reconstruction-based proxy tasks, and we pretrain neural operators on unlabeled PDE data. We demonstrate that with unsupervised pretraining, our neural operators not only improve counterparts trained with more simulated data, but also they outperform off-the-shelf pretrained checkpoints from other popular domains (such as CV) that are ready for fine-tuning. Then, to further improve the data efficiency during out-of-distribution (OOD) inference, we design a similarity-based method that learns in-context examples . This approach introduces zero overhead during training: one just maintains the standard training pipeline, and it can be seamlessly plugged in for OOD inference, without further fine-tuning. In more detail, we summarize our main contributions:

1. We introduce unlabeled PDE data and unsupervised pretraining for data-efficient neural operator learning. We show that our method can achieve better performance than models trained with more simulated PDE solutions, or fine-tuned from public checkpoints pretrained on other benchmarks, demonstrating the importance of unsupervised pretraining on domain-specific PDE data.
2. We propose a similarity-based method to improve the OOD generalization of neural operators, which is flexible and can scale up to a large number of unseen in-context examples ("demos").
3. We provide detailed empirical evaluations on both diverse PDE benchmarks and also several real-world scenarios, demonstrating that we can achieve both strong forward modeling performance and significant savings in PDE simulations.

Figure 1: Overview of our framework for data-efficient neural operator learning (with our contributions highlighted in red). Stage 1: Unsupervised pretraining only on unlabeled PDE data. Stage 2: Fine-tuning with reduced simulation costs of PDE data. Stage 3: Test-time in-context examples can improve the neural operator’s out-of-distribution performance, without additional training costs.

Related Works

### Machine Learning for Scientific Modeling

There has been a long history of using learning-based methods to model physical scientific phenomena [41; 42; 6; 5]. A representative line of work is so-called physics-informed neural networks (PINNs) [67; 87; 19; 18; 69], which try to incorporate physics in neural networks by including the differential form of the PDE as an additional physics loss regularization term. However, this paradigm is confined to specific PDE scenarios (e.g., fixed PDE coefficients), instead of being more physics-agnostic. Moreover, recent work has highlighted several fundamental "issues" with PINN-based methods [40; 15]. On the other hand, operator learning methods, including Fourier Neural Operators [45; 44; 38] and Deep Operator Network , have achieved progress in approximating the solution operators of PDEs. Although these data-driven approaches show promise in learning PDE solutions, they (like many neural network-based methods) rely on vast quantities of high-fidelity labeled data. For operator learning, such data are usually computationally expensive to simulate [67; 2; 84]. More recently, people have tried to generate synthetic PDE solutions to train SciML models . In contrast, our method works on unlabeled PDE data. This approach is distinct from the generation of synthetic PDE data, and it could be further combined as a semi-supervised learning strategy.

### Unsupervised Pretraining and Foundation Models

Unsupervised (or self-supervised) pretraining is a key method in CV and NLP to achieve meaningful representations , data-efficient fine-tuning , and foundation models . In CV, contrastive learning learns meaningful features by distinguishing between similar (positive) and different (negative) samples [59; 76; 7; 56; 29]. Masked Autoencoder (MAE)  uses a reconstructive approach where parts of the input are masked and the model learns to predict masked parts. In NLP, among the most prominent works are large language models (LLMs) such as GPT [3; 64; 65] and BERT [12; 66], which leverage token predictions for pretraining. Similar directions also show progress in SciML. For example,  and  propose to create augmented views in the solution space via Lie Symmetries;  study the scaling behavior of supervised pretraining and OOD generalization, charting directions for foundational models for SciML;  target learning astronomical foundation models with cross-modal contrastive learning; and  build large task-agnostic models with a broad understanding of common physical behavior to serve as foundation models for SciML.

### In-Context Learning (ICL)

In-context learning (ICL) is a promising paradigm that helps deep networks generalize to unseen domains with a few in-context examples. Early works in CV seek to learn feature-level correspondence between the target and a few "shots," such that models can generalize to open-set unseen objects [17; 83]. In NLP, people find LLMs are naturally few-shot learners , and thus tuning or optimizing prompts becomes extremely important to improve the in-context learning performance of LLMs [85; 72]. More recently, within SciML, a different operator learning strategy, termed "in-context operator learning," has been proposed [80; 81; 47]. During both training and inference, the neural operator is asked to make predictions by explicitly leveraging a predefined number of so-called "demo" examples (pairs of physical parameters and simulated solutions). This approach provides a balance between model generalization and addressing data scarcity in the scenario of OOD testing.

## 3 Methods

In this section, we introduce our framework (outlined in Figure 1). We propose first to pretrain the model with unsupervised pretraining (Sec. 3.1), which will contribute to the data efficiency and reduced PDE simulation costs during standard training of neural operators. When we move to OOD scenarios during inference, we test our models with in-context examples (Sec. 3.2) to avoid further fine-tuning costs.

### Unsupervised Pretraining

The core idea in unsupervised (or self-supervised) pretraining is to train a neural network with properly designed proxy tasks. These proxy tasks do not require labeled data, but they are designed to be highly related to the supervised learning objectives of interest. While popular in CV and NLP, unsupervised pretraining on unlabeled data has never been explored in PDE operator learning in SciML. This is due to _two unresolved questions_: 1) What kinds of unlabeled data can we use to train neural operators? 2) How to design proxy tasks for PDE data? We address these questions below.

#### 3.1.1 Unlabeled PDE Data

How to Define Unlabeled PDE Data?In general, when a neural operator is trained on PDE datasets , it learns to map inputs (physical parameters, coordinates, forcing functions, initial conditions, etc.) to PDE solutions. Therefore, given a set of PDE data (collected via simulations or observations), its unlabeled version is defined as the one without PDE solutions. Our unlabeled PDE data is a broader concept of related inputs in modeling PDE systems. Let us consider the second-order linear differential equation as a general example. It is formulated as

\[_{i,j=1}^{n}a_{ij}(x)u_{x_{i}x_{j}}+_{i=1}^{n}b_{i}(x)u_{x_{i}}+c(x)u=f( x),\]

where \(x^{n}\) represents physical space that varies in different systems (e.g. \(n=3\) for 2D time-dependent PDEs); the coefficients (or physical parameters) \(a_{ij},b_{i},c\) are known from the physical process; \(u\) is the target solution; and \(f\) denotes an external forcing function . We can consider two situations where solutions are unavailable:

* Time-independent equations: following , our unlabeled PDE data include physical parameters (\(a_{ij},b_{i},c\)), forcing functions (\(f\)), and coordinates (grids of the discrete physical space).
* Time-dependent equations (e.g., forecasting problems ): without simulating the temporal dynamics, our unlabeled PDE data only include initial snapshot \(u_{0}(x)\) that defines PDE systems. Note that collecting snapshots with temporal dynamics in large-scale scenes is more complex than capturing individual snapshots. For example, weather forecasting  and smoke dispersion  require continuous monitoring and multiple sensors, whereas single measurements are simpler and less resource-intensive. Long-term data collection often involves extensive networks and processing, unlike one-time measurements.

For concrete examples of different PDEs and unlabeled data that we will study, see Appendix A. There are two main reasons for pretraining only on unlabeled PDE data, as discussed below.

Cheap Generation of Unlabeled PDE Data.One critical reason that leads to expensive computational costs when collecting PDE data is the time marching scheme  in numerical simulation. However, only generating unlabeled PDE data and snapshots without temporal dynamics will be much cheaper than simulating solutions (Table 4), making our unsupervised pretraining highly feasible in practice. Our pretraining strategy will be very data-efficient, and can avoid the heavy computational cost of simulating complex time-dependent equations for massive high-fidelity labeled solutions.

Benefits of Pretraining on Unlabeled PDE Data.Beyond the cheap generation, pretraining on unlabeled PDE data has the following benefits. First, _regularization against overfitting_. Unsupervised pretraining can strongly regularize the model towards better generalization. Second, _faster convergence_. Pretraining on unlabeled PDE data will provide neural operators with domain-adapted initializations and can accelerate training speed. Third, _meaningful representations_. Pretraining on unlabeled PDE data can help models extract useful representations for subsequent operator learning. We defer our results and experimental details in Figure 4 in Sec. 4.1.

#### 3.1.2 Proxy Tasks

To illustrate our general approach of constructing proxy tasks, we choose two variants of reconstruction as our core proxy tasks. In particular, we will input unlabeled PDE data to our neural operators, and after a decoder network, we will force the output to be close to the input. We consider two perturbation variants (or augmented views). They are inspired by real-world settings when people collect scientific data, and they are important invariances we need to introduce to SciML models.

Masked Autoencoder.Masked autoencoders (MAEs) have been shown to be scalable self-supervised learners . The method is conceptually simple: remove a portion of the input data, and learn to predict the removed content. These methods enable training in NLP and CV of generalizable models containing over one hundred billion parameters [12; 3]. Here, we investigate the potential of MAE for scientific modeling.

Motivation: PDE dynamics are invariant to sparse sensing of the full field scientific data. It is very common [4; 16; 32] that scientific data need to be collected from sparse sensors, and that people need to reconstruct or generate data for domains without sensors. We enforce our model to learn sensor invariance via random masking, and we extract the invariant features over distorted views of the same unlabeled PDE data. The invariance to sparse sensing of scientific data will facilitate the robustness of the representations from MAE.

Therefore, we consider MAE as a proxy task. Specifically, our MAE is a straightforward autoencoding approach that reconstructs the original signal, given its partial observation. Like all autoencoders, our approach has an encoder that maps the observed signal to a latent representation, and a decoder that reconstructs the original signal from the latent representation. We randomly sample masks according to a certain masking ratio (i.e., the ratio of removed areas), and the values of masked areas of unlabeled PDE data are set to zero. Our loss function computes the mean squared error (MSE) between the reconstructed and original input. We compute the loss only on masked areas; if no mask is applied (i.e., a vanilla autoencoder), the MSE loss will be applied to all spatial areas.

Super-resolution.Super-resolution (SR) techniques have emerged as powerful tools for enhancing data resolution, improving the overall quality and fidelity of data representation, and retrieving fine-scale structures. SR is a task that involves recovering fine-scale data from corresponding coarse-grained data. It is also a popular task on PDE learning [9; 68], which is to train SciML models to preserve the inherent physical properties of scientific data.

Motivation: Numerical solutions of PDEs are expected to exhibit invariance to filtering blur or different resolutions of inputs [37; 79]. For instance, in turbulence simulations, the traditional numerical methods always fail to model the expected physical phenomenon with low-resolution meshes due to substantial numerical errors. SR has emerged as a powerful tool for subgrid modeling of PDE dynamics, especially helping to capture the critical patterns of turbulence [51; 37]. Given a specific input distribution, after fitting the SR objective, neural operators are expected to preserve the inherent physical properties and exhibit invariance to filtering blur.

Therefore, we introduce SR as another proxy task. Our objective shares the same motivation as recent SciML works for SR . Specifically, we enforce the model to learn invariant features of unlabeled PDE data that are immune to resolution and blur. Blurry snapshots often occur when the resolution is too low to accurately represent the details in the original content. To do so, we apply a Gaussian filter to blur the unlabeled PDE data, and the autoencoder will reconstruct the high-resolution input with fine-grained details. Instead of applying a fixed blurring, we randomly sample the variance of the Gaussian filter from a certain range as augmentations.

#### 3.1.3 PDEs

After pretraining on unlabeled PDE data, we fine-tune neural operators on simulated solutions of PDEs. We study two time-independent PDEs (Poisson, Helmholtz) and two time-dependent PDEs (Reaction-Diffusion, Navier-Stokes). We include details of these PDEs in Appendix A.

#### 3.1.4 Model Architectures

We consider two popular architectures for fair comparisons with previous works. These are encoder-decoder architectures designed to reconstruct the original input given partial observations, where the encoder maps observed unlabeled PDE data to a latent space, and the decoder reconstructs the original conditions. We include visualizations of these architectures in Appendix D.

Figure 2: Overview: unsupervised pretraining via MAE and super-resolution. During pre-training, in the input unlabeled PDE data, a random subset (e.g., 70%) of spatial locations are masked, followed by a Gaussian blur. After the encoder and decoder, the full set of input is required to be reconstructed.

Fourier Neural Operator.Fourier Neural Operator (FNO) targets learning PDE data in the Fourier space. The original model backbone (encoder) employs Fourier transform and learns lower Fourier modes with linear transforms. The FNO backbone outputs features back to the spatial domain (i.e., the embeddings are on the pixel level). We refer readers to the original paper for details [45; 46].

* _Pretraining_: We build the decoder to be identical to the encoder (except for the input/output dimension). Unlabeled PDE data are randomly masked at the pixel level.
* _Fine-tuning_: After the pretraining, we discard the decoder, and we follow the original design to append two fully-connected layers (with ReLU activations) to predict final spatial-wise solutions.

Transformer.Transformers, which mainly employ self-attention and linear transform blocks, have shown promise in both NLP and CV [78; 13]. Different from FNO, which directly operates on grids, transformers tokenize and group grids into patches, i.e., each tokenized patch embeds a local neighborhood of subgrids. We follow the 3D transformer architecture of Video-MAE [77; 52]. Our encoder embeds patches by a linear projection with added positional embeddings (just as in a standard ViT), and it then processes the resulting set via a series of Transformer blocks. For the transformer, the unlabeled PDE data are randomly masked at the patch level.

* _Pretraining_: For the transformer encoder, we only apply it on the subset of tokens that are visible (i.e., unmasked patches), and masked patches are removed. This allows us to train encoders efficiently. The input to the MAE decoder is the full set of tokens consisting of (i) encoded visible patches, and (ii) the mask token. The mask token is a shared and learned vector that indicates the presence of a missing patch to be predicted. We add positional embeddings to all tokens in this full set. Without this, mask tokens would have no information about their location in the input. Following [77; 28], we adopt an asymmetric design where the decoder is more lightweight (shallower and narrower) than the encoder.
* _Fine-tuning_: After the pretraining, the decoder is preserved during fine-tuning, since we need to reconstruct the tokenized patches back to the input.

### Similarity-based Mining of In-Context Examples

Out-of-distribution (OOD) generalization is a critical technical challenge, not only in SciML but also across multiple domains in AI for science . To improve the OOD generalizability of neural operators and to reduce the extra effort of downstream fine-tuning, the following inference paradigm has been proposed: given a query input, the model is also provided with a few supporting examples (dubbed "demos"), together with their ground-truth solutions, to make the final prediction. This approach enables the "open-set" generalization of the model to make predictions on unseen samples.

Originally, in the literature on few-shot learning [82; 83; 55; 70; 35; 48; 61], people developed delicate architectures to find a correspondence between the input and the supporting examples. The purpose of this extra architecture/training design is twofold: first, _find similarities_ between the target input and supporting examples; and second, _aggregate labels_ of supporting examples for the final predictions. Recent ICL works [80; 81; 47] on learning PDE data also adopt this strategy, with transformers and cross-attention layers.

However, the ICL (in-context learning) of LLMs (large language models) enables a different strategy. The pretraining is still standard and simple (next/masked token prediction), without additional training costs. During inference, LLMs can auto-regressively take any number of few-shot examples, finding similarities between tokens in few-shot examples and those in the target query (via self-attention), and then generate responses by aggregating embeddings of tokens in few-shot examples. This ICL strategy used in LLM is highly scalable and training-efficient.

Motivated by this, we propose to leverage in-context examples via two steps (Algorithm 1).

Similarity by Prediction.We find spatial-wise and temporal-wise similar demos by calculating their distance in the output space. That means, for two input locations over the spatial and temporal domains, if we find their outputs of the trained neural operator similar, then we treat them as similar samples. Following [80; 81], we assume demos share the same distribution of physical parameters with the query.

Aggregation.For each spatial-temporal location of the query, after finding its similar samples in demos, we aggregate and average their solutions as the prediction.

## 4 Empirical Results

To illustrate the benefits of our approach, we perform empirical evaluations on both PDE benchmarks and real-world observations. _Most importantly_, our unsupervised pretraining (on unlabeled PDE data, followed by fine-tuning) outperforms neural operators trained from scratch, while requiring fewer PDE simulations (Sec. 4.1 and Sec. 4.2). _Moreover_, our in-context examples can help the model generalize better to OOD cases (Sec. 4.3). In our experiments, we trained models three times with different random seeds for statistical significance. For more experimental details, see Appendix B. We also include ablation studies about pretraining hyperparameters in Appendix J. For visualizations of our pretraining, see Appendix K.

Figure 3: Pretraining neural operators on unlabeled PDE data improves its performance and data efficiency on Poisson (**a**), Helmholtz (**b**), Reaction-Diffusion (**c**), and Navier-Stokes (**d** and **e**, with relative errors at different unrolled steps shown on **f**). “random init.”: models are trained from scratch with random initialization. “vision pretrained (SSv2)”: fine-tuning from the publicly available checkpoint for Video-MAE (pretrained on computer vision dataset SSV2  for video understanding). Savings of the number of simulated PDE data (when “random init.” achieves the best test error) are shown in red.

### Unsupervised Pretraining Enables Data-Efficient Operator Learning

Data Efficiency.We first demonstrate that, by leveraging unsupervised pretraining, neural operators can achieve improved errors with less simulated data. In Figure 3, on each PDE, compared with directly training from scratch ("random init."), pretraining on unlabeled PDE data can help neural operators achieve better performance, which can further help reduce the amount of simulated data. Specifically, in our experiments, when we target achieving the best test error of the baseline ("random init."), our method can save \(5 10^{2} 8 10^{5}\) simulated solutions across diverse PDE systems.

Among all PDEs, we find that Helmholtz (Figure 3 (b)) is the most challenging. Both two curves failed to improve the error until we increased the number of simulated data points to over 1024. Meanwhile, the generalization gaps remain high (Figure 12 (b)), indicating low training errors. We suspect that learning on the Helmholtz equation may be exhibiting the grokking issue , where the network quickly memorizes the training data, but the improvement in generalizability is delayed.

Unsupervised Pretraining Outperforms Off-the-Shelf Pretrained Checkpoints.Pretraining on unlabeled PDE data is not the only way to save simulation costs. As pretraining is widely adopted in CV, pretrained checkpoints on vision data become publicly available and are ready for fine-tuning. We choose to compare with Video-MAE  for state-of-the-art video understanding pretrained on SSV2 . As shown in Figure 3 (e), vision-pretrained Video-MAE can only outperform the random initialization with high volumes of simulated data, while its performance suffers when fine-tuned with limited simulations. In contrast, our unsupervised pretraining on unlabeled PDE data can save a significant amount of simulated data.

As errors during testing may quickly accumulate with further timestamps, we also report results with more unrolled steps. We use checkpoints trained with the largest amount of simulated data from above for this study. As shown in Figure 3 (f), at each rollout step, our unsupervised pretraining achieves much better performance. Similarly, vision-pretrained Video-MAE is eventually outperformed by the random initialization at the long rollout step.

Benefits of Pretraining on Unlabeled PDE Data.Pretraining on unlabeled PDE data is beneficial beyond achieving better performance with reduced simulations. _First_, when training on extremely low volumes of data, neural operators tend to overfit, resulting in poor generalization. In Figure 4-left, pretraining on unlabeled PDE data can reduce the generalization gap (testing error \(-\) training error). We further show that better generalization gaps persist across all PDEs we studied (see Figure 12). _Second_, pretraining on unlabeled PDE data can lead to faster convergence during fine-tuning. Unlike standard random initializations from Gaussian distributions, pretraining on unlabeled PDE data will provide neural operators with domain-adapted initializations and facilitate a much faster convergence rate, as shown in Figure 4-middle3. _Third_, unsupervised pretraining can also help models extract useful representations for subsequent operator learning. From Figure 4-right, we find that even with pre-extracted features (i.e., fixed encoder and only fine-tuned decoder), our neural operators can still outperform the baselines where both the encoder and decoder are updated during fine-tuning.

Figure 4: Benefits of our unsupervised pretraining. **Reduced overfitting** (left): our method consistently leads to smaller generalization gaps (test error \(-\) training error) across all PDEs we studied (Fig. 12). **Faster convergence** (middle): our unsupervised pretraining can accelerate model convergence than both random initialization and vision-pretrained checkpoint. **Meaningful representations** (right): fine-tuning Video-MAE with fixed encoder (pretrained on unlabeled PDE data, red line) can extract meaningful features and outperform the baseline and the vision-pretrained model (both encoder and decoder are updated during fine-tuning).

### More Comprehensive Experiments on Real-World Data

We now move to a broader range of benchmarks. We will study real-world and noisy data instead of toy datasets, providing even more comprehensive experiments. These benchmarks are widely studied in previous works [60; 62; 52].

Datasets.We brief the background, with more details in Appendix L and visualizations in Fig. 15.

* ECMWF Reanalysis v5 (ERA5)  is a public extensive dataset, which delivers hourly data on multiple atmospheric variables spanning from 1979 to today. ERA5 represents a type of atmospheric reanalysis dataset , integrating observations from a variety of measurement sources with numerical models through data assimilation . Essentially, it reconstructs the most accurate estimation of the Earth's atmospheric conditions over time. This dataset has been extensively utilized in prior SciML studies [60; 68]. We focus on forecasting the important and challenging _temperature_ atmospheric variable.
* ScalarFlow  is a reconstruction of real-world smoke plumes. It assembles the first large-scale dataset of realistic turbulent flows. The availability of a large, volumetric data set opens up a wide range of applications, including re-simulations, novel visualization, and metric evaluations. The dataset contains 104 real-world smoke flow density reconstructions. Each reconstruction is captured from five different viewpoints for 150 temporal frames spanning 2.5 seconds.
* Airfoil  is a large-scale dataset that contains the pressure and velocity simulations of the flow around real airfoils. This dataset has 53880 samples for training and 90 samples for testing. Each sample contains 6 channels. The 3 input channels include the binary spatial mask of the airfoil and the initial velocity of the freestream condition in the x and y directions. The output channels include the pressure and the x and y velocity components from the simulation at the steady state.

Model and Training.For time-dependent ERA5 (Sec. L.1) and ScalarFlow (Sec. L.2) datasets, we adopt the same VideoMAE architecture  used in Sec. 4.1. We use 15 consecutive temporal snapshots to forecast the next time step. We train VideoMAE with Adam, with other hyperparameters the same as in Table 3 column "N.S. (PDEBench)". For the time-independent steady-state Airfoil dataset (Sec. L.3), we adopt the 2D-FNO architecture. We train 2D-FNO with Adam, with other hyperparameters the same as in Table 3 column "Poisson".

Results.As shown in Figure 5, compared with directly training operators from scratch ("random init."), pretraining on unlabeled data (2D snapshots of ERA5/ScalarFlow without temporal dynamics, or freestream velocities of Airfoil) can help neural operators achieve better performance on both temperature/flow forecasting and predictions of the steady-state pressure and velocity around airfoils.

### In-Context Examples Enable Data-Efficient OOD Generalization

We now move to OOD settings, where models will be tested on PDE data simulated with physical parameters unseen during fine-tuning/training. We include how to simulate OOD samples in

Figure 5: For **real-world scientific problems**, pretraining neural operators on unlabeled PDE data improves its performance and data efficiency. We study VideoMAE  pretrained with unlabeled snapshots (no temporal dynamics), and then fine-tune across different numbers of temporal snapshots on ERA5 (left) and ScalarFlow (middle). We also pretrain 2D-FNO  on freestream velocities and fine-tune on time-independent steady-state airflow pressure and velocities (right). “random init.”: models are trained from scratch with random initialization.

Appendix B.2. Neural operators suffer from poor OOD generalization [73; 80]. Traditionally, improvements heavily depend on further fine-tuning on simulated data, which requires extra simulation and training costs. We study the benefits of leveraging test-time in-context examples. As shown in Figure 6, when we flexibly scale up the number of demos, we can keep improving FNO's OOD generalization on diverse PDEs. We follow [80; 81] that demos are randomly sampled from the same distribution used to generate the OOD test set. When the number of demos is 0, we have the baseline in the OOD setting. Notably, we introduce zero training overhead: we keep the standard training pipeline, and our mining of in-context examples can be seamlessly plugged in during OOD inference.

We further provide a baseline, which uses features extracted by the backbone of the neural operator (high-dimensional features before the final output layer) to find similar samples. As we can see, this baseline is worse than our method (both performance and confidence), indicating that the final output of the neural operator can more accurately indicate true similar samples.

See Appendix M for further discussions about the benefits of leveraging in-context examples, and Appendix N for visualizations.

## 5 Conclusion

In this work, we focus on improving the data efficiency of solving partial differential equations (PDEs) using deep learning, with a particular emphasis on unsupervised pretraining and in-context learning (ICL) methods. Our key contributions include introducing unsupervised pretraining for operator learning and a flexible ICL approach that enhances out-of-distribution (OOD) generalization without increasing training costs. Through extensive evaluations, we demonstrate that our method is not only more data-efficient, but it also achieves greater generalizability compared to existing approaches. By improving the data efficiency of neural operators for solving PDEs, our approach can significantly reduce the computational costs and energy demands of high-fidelity numerical PDE simulations. Additionally, by making advanced PDE solutions more accessible through efficient pretraining, our method has the potential to accelerate scientific and engineering progress across various fields, ultimately benefiting society. We hope our work will inspire the scientific machine learning (SciML) community to further address the high simulation costs and limited OOD generalization of neural operators, contributing to advancements that support both scientific innovation and environmental sustainability.

## 6 Limitations

Current limitations of our work: 1) We could design more physics-inspired proxy tasks and data augmentation methods for scientific data; 2) We could study more PDE systems in our unsupervised pretraining and in-context learning; 3) We could consider more different neural operator architectures. We expect that addressing these limitations will lead to broader impacts in future works.

Figure 6: In-context examples for OOD testing. **Our method (blue) improves both \(_{2}\) errors and confidence as we increase the number of demos.** “Ours (Similarity by FNO Output)”: we leverage the output (prediction) of neural operators to find similar samples. “Baseline (Similarity by Backbone Feature)”: the baseline uses features extracted by the backbone of the neural operator (high-dimensional features before the final output layer) to find similar samples.