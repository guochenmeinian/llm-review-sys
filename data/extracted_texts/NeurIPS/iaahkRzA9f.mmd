# Map It Anywhere (Mia): Empowering Bird's Eye View Mapping using Large-scale Public Data

Cherie Ho \({}^{1}\)1 Jiaye Zou \({}^{1}\)1 Omar Alama \({}^{1}\)1 Sai Mitheran Jagadesh Kumar \({}^{1}\)

Benjamin Chiang \({}^{1}\) Taneesh Gupta \({}^{1}\) Chen Wang \({}^{2}\) Nikhil Keetha \({}^{1}\)

Katia Sycara \({}^{1}\) Sebastian Scherer \({}^{1}\)

\({}^{1}\)Carnegie Mellon University \({}^{2}\)University at Buffalo

###### Abstract

Top-down Bird's Eye View (BEV) maps are a popular perceptual representation for ground robot navigation due to their richness and flexibility for downstream tasks. While recent methods have shown promise for predicting BEV maps from First-Person View (FPV) images, their generalizability is limited to small regions captured by current autonomous vehicle-based datasets. In this context, we show that a more scalable approach towards generalizable map prediction can be enabled by using two large-scale crowd-sourced mapping platforms, Mapillary for FPV images and OpenStreetMap for BEV semantic maps. We introduce Map It Anywhere (Mia), a data engine that enables seamless curation and modeling of labeled map prediction data from existing open-source map platforms. Using our Mia data engine, we display the ease of _automatically_ collecting a dataset of _1.2 million_ pairs of FPV images & BEV maps encompassing diverse geographies, landscapes, environmental factors, camera models & capture scenarios. We further train a simple camera model-agnostic model on this data for BEV map prediction. Extensive evaluations using established benchmarks and our dataset show that the data curated by Mia enables _effective pretraining for generalizable BEV map prediction_, with zero-shot performance far exceeding baselines trained on existing datasets by 35%. Our analysis highlights the promise of using large-scale public maps for developing & testing generalizable BEV perception, paving the way for more robust autonomous navigation. Website: mapitanywhere.github.io

Figure 1: Our **Map It Anywhere (Mia)** data engine empowers _generalizable_ Bird’s Eye View (BEV) map prediction from First-Person View (FPV) images. _Left:_Mia enables seamless automatic curation of quality FPV & semantic BEV map data from _crowd-sourced_ platforms, Mapillary & OpenStreetMap. _Right:_ Both as a tool for training & benchmarking, Mia enables research towards _anywhere_ map prediction. A simple model (Mapper) trained on data from Mia better generalizes on both held-out cities (Mia-Odd) & existing benchmarks, while state-of-the-art baselines trained on conventional autonomous vehicle datasets struggle.

Introduction

Bird's Eye View (BEV) maps are an important perceptual representation for localization, mapping , and decision-making tasks , particularly for ground robots. They provide a rich, efficient metric representation of the world, enabling spatial reasoning directly in the plane ground robots move in. Given these advantages, autonomous driving systems often employ BEV maps as their primary perceptual representation . Beyond on-road driving, BEV mapping is widely used in other robot domains, such as offroad driving, mobile manipulation , and exploration . To enable wide deployment of BEV maps as a perceptual representation, there is a requirement for a _general map prediction building block_ that performs robustly across different domains and supports effective adaptation to specific tasks/environments.

Despite tremendous advancements in predicting BEV maps from First Person View (FPV) images , we find that achieving good out-of-the-box predictions across diverse scenarios remains challenging. This shortcoming mainly stems from the current training & testing paradigm on limited-scale datasets collected using autonomous vehicle (AV) platforms . While these benchmarks have massively propelled the field, they are _principally limited_ in capturing large-scale diversity due to the time and cost associated with manual labeling, the limited deployment range, and finally, the use of specific sensor configurations on current AV stacks.

We believe that a complementary training & testing paradigm is necessary to assess the generalizability & robustness of BEV mapping, paving the way for _anywhere_ deployment. Hence, starting from first principles, we formulate the key requirements for generalizable BEV mapping as: (a) being able to provide top-down information of key navigation classes, (b) ability to be used by different agents and across different operating regimes, for example, sidewalk prediction is more critical for autonomous wheelchairs, (c) perform reasonably out-of-the-box in unseen locations supporting quick adaptation, and (d) easily adaptable to different hardware configurations such as camera models.

In this context, we explore the question of **"How can one collect a dataset to empower generalizable BEV mapping?"** Specifically, to support research on generalizable BEV mapping, such a dataset needs to (a) contain diverse geographies, terrain types, time of day, and seasons, (b) capture scenarios beyond on-the-road driving, (c) support various camera models, and (d) consists of well-distributed classes and labels for supporting navigation.

To construct such a dataset, our key insight is to leverage two disjoint, crowd-sourced, and _world-scale_ public mapping platforms: Mapillary for First-Person View (FPV) images and OpenStreetMap for Bird's Eye View (BEV) semantic maps. Both open-source platforms provide the tools necessary to associate crowd-sourced FPV images with semantic raster maps used for everyday human navigation. We introduce MIA, a data engine which taps into the potential of these mapping platforms to enable seamless curation and modeling of labeled data for generalizable BEV map prediction. Specifically, our data engine enables an evergrowing BEV dataset and benchmark, which exhibits world-scale diversity and supports research on both universal & environment-specific deployment.

In this paper, to showcase the potential of our data engine, we make the following key contributions:

1. We open-source our MIA data engine for supporting automation curation of paired world-scale FPV & BEV data, which can be readily used for BEV semantic map prediction research.
2. Using our MIA data engine, we release a dataset containing \(\)1.2 million high quality FPV image and BEV map pairs covering 470 \(km^{2}\), thereby facilitating future map prediction research on generalizability and robustness.
3. We show that training a simple camera intrinsics-agnostic model with our released datasets results in superior zero-shot performance over existing state-of-the-art baselines on key static classes, such as roads and sidewalks.
4. Through analysis of current performance in urban and rural domains of our benchmark, we show that significant research remains to enable generalizable BEV map prediction.

Overall, MIA establishes a diverse evergrowing dataset & benchmark for map prediction research and showcases how commodity public maps can empower generalizable BEV perception tasks (Fig. 1).

## 2 Related Work

**Bird's Eye View Map Prediction:** BEV map prediction involves predicting top-down semantic maps from various sensory modalities to facilitate downstream robotic tasks. Some works rely solely on LiDAR , others on multi-view cameras , and some on both [24; 46]. These approaches rely on modalities that are expensive and difficult to calibrate. Recently, a growing number of works use _monocular_ cameras [14; 15; 25; 26; 31; 32], as they are attractive for their ease of deployment, reduced cost, and higher scalability. However, all these works still rely on current autonomous driving datasets for labels, which limits the scalability of data collection. To address this limitation, SkyEye  uses more available front-view semantics to build map predictors without explicit BEV maps. However, this method relies on ground truth FPV semantic masks, which are costly to annotate and scale. In contrast, MIA leverages two readily available world-scale databases to provide diverse and accurate supervision, avoiding the high cost of equipment and manual labor.

Another line of related work is the task of matching FPV images with BEV maps that can be satellite-based [17; 45], planimetric [33; 41], or multi-modal . They often employ techniques to predict a BEV feature map given an FPV image. While useful for retrieval or localization, these feature maps cannot benefit downstream tasks without complex learned decoders, unlike predicted semantic BEV maps, which downstream algorithms like path planning can readily consume.

**Datasets for BEV Map Prediction:** Existing BEV map prediction datasets are often derived from multi-modal autonomous driving datasets [4; 5; 14; 15; 35; 39] that target various tasks, including BEV prediction. These pioneering works depend on manually collected data from costly LiDARs, hence requiring careful calibration with camera setups to ensure accurate correspondence between FPV & BEV data. The BEV is generated by accumulating semantically labeled LiDAR point clouds & then splatting them to BEV, allowing them to capture dynamic and static classes. However, these approaches are **principally limited** in both **scale** and **diversity** due to their high cost, hindering model generalizability. In contrast, MIA uses data available on crowd-sourced platforms and can thus obtain FPV-BEV pairs globally, achieving broader diversity & scale as shown in Table 1.

**Crowd-sourced Datasets for Learning Geometric Tasks:** Crowd-sourced platforms enable open-source contributors to upload diverse in-the-wild data, significantly empowering generalizability in geometric learning tasks. One such notable platform is Mapillary , which hosts over 2 billion (and growing) crowd-sourced street-level images from various locations worldwide, captured by different cameras across all seasons and times. Mapillary has been notably used for tasks such as depth estimation , lifelong place recognition , and visual localization [19; 33]. Most related to our work, OrienterNet  addresses visual localization within a large top-down map, curating a large-scale localization dataset provided by crowd-sourced platforms, Mapillary  for FPV images and OpenStreetMap  (OSM) for large BEV maps. However, the OrienterNet pipeline for rasterizing maps is not suitable for BEV prediction as it renders large maps, mimics OSM style, and includes graphical elements/labels irrelevant to the map prediction task. We illustrate the qualitative differences in Figs. 11 and 12 (shared in the Appendix). MIA further refines the OrienterNet pipeline by enabling automatic curation and collection of semantic maps, alignment of BEV renders with satellite images, and inference of missing OSM sidewalk geometries, thus providing rich semantic maps that are ready for BEV map prediction.

    &  & **km\({}^{2}\)** & \# Annotated & \# Camera &  &  & **Suitable for** & **Automatic** \\  & & **covered** & **BEV Frames** & **Models** & U & S & R & O & Car & Bik & Ped & **Map Pred.** & **Curation** \\   & 2 in US & 1.6  & 22K & 2 & ✓ & X & X & X & ✓ & X & ✓ & X \\  & 6 in US & \(\)100K  & 2 & ✓ & X & X & ✓ & X & ✓ & X & ✓ & X \\  & KITTI-360-BEV  & 1 in DE & 5.3 & 83K & 2 & X & ✓ & ✓ & X & ✓ & X & ✓ & X \\  & BusS & 5.6 & 40K & 2 & ✓ & ✓ & X & ✓ & X & ✓ & X & ✓ & X \\  & BusS & 76  & 230K & 2 & ✓ & ✓ & X & ✓ & X & ✓ & X & ✓ & X \\  & 3 in US & 76  & 230K & 2 & ✓ & ✓ & X & ✓ & X & ✓ & X & ✓ & X \\  &  &  &  &  &  &  &  &  &  &  &  &  &  \\   

Table 1: **Statistics Showcasing the broader scale of MIA in comparison to prior BEV Datasets.** Taxonomy: U: Urban, S: Suburban, R: Rural, O: Offroad, BN: Boston, SP: Singapore. “-”: Attributes are not available. “**: MCI is a BEV localization dataset and does not provide semantic BEV maps suitable for map prediction. “# BEV Annotated Frames”: Readily available BEV data. “Automatic Curation”: No human intervention in the collection and annotation of the dataset.

Mia Data Engine

To construct a dataset for generalizable BEV mapping, we develop a scalable data engine that generates high-quality, diverse FPV-BEV pairs with rich semantic labels. This process, summarized in Fig. 2 and detailed below, follows the criteria discussed in Section 1.

### First Person View (FPV) Retrieval

**Mapillary:** For FPV retrieval, we leverage Mapillary , a massive public database, licensed under _CC BY-SA_, with over 2 billion crowd sourced images. The images span various weather and lighting conditions collected using diverse camera models and focal lengths. Furthermore, images are taken by pedestrians, vehicles, bicyclists, etc. This diversity enables the collection of more dynamic and difficult scenarios critical for _anywhere_ map prediction. However, this massive pool of data is not readily amenable to deep learning as it contains many noisy, distorted, and incorrectly registered instances. Thus far, few works, such as , have leveraged such data providing an impressive retrieval and undistortion pipeline. However, the work relied on careful and manual curation of limited camera models and scenarios. Such approaches are not scalable and cannot leverage the powerful quantity and diversity of Mapillary. Hence, we further refine the OrienterNet  data curation framework and develop a fully automated curation pipeline that can harness the full potential of the extensive Mapillary database. We describe the pipeline in the following sections.

**FPV Pipeline:** As demonstrated in Fig. 2, the FPV pipeline starts by manually inputting a list of locations of interest, which can be as simple as inputting the name of the location or as specific as specifying the GPS bounds. The geographical bounds are then fetched using the Nominatim API  if needed. The pipeline then converts these bounds to a list of zoom-14 tiles and uses the public Mapillary APIs to query for the image instances within these tiles. Only the retrieved instances that lie within the geographical boundaries of interest are kept. Given the retrieved image IDs, we use another Mapillary endpoint to retrieve image metadata, which includes coordinates rectified through structure from motion, camera information, poses, and timestamps, amongst other details that we use for filtering in the subsequent stage. For more information on the raw data used from Mapillary, please refer to Supplemental Information Q8.

We develop the filtering pipeline by observing hundreds of FPV & BEV pairs and identifying the correlations between good-quality FPVs and their corresponding metadata. The criteria we used included a recency filter, a camera model filter spanning 19 camera models with good RGB quality, a location/angle discrepancy filter that computes the difference between Structure from Motion (SfM) computed and recorded poses, both obtained from Mapillary API, as a proxy for measuring the quality of the geo-registration, and a camera type filter that only includes perspective and fisheye. To promote spatial diversity over sheer quantity, we filter out images within a 4-meter radius of another image from the same sequence. After filtering, we retrieve the RGB images from Mapillary

Figure 2: **Overview of how the MIA data engine enables automatic curation of FPV & BEV data.** Given names of cities as input from the left, the top row shows FPV processing, while the bottom row depicts BEV processing. Both pipelines converge on the right, producing FPV, BEV, and pose tuples.

and feed them through an undistortion pipeline adapted from . The undistortion is critical for fisheye images to ensure their pixel-aligned features can be correctly lifted into BEV space. Using this pipeline, we can retrieve high-quality images from anywhere in the world, tapping into the power of the Mapillary platform.

### Birds Eye View (BEV) Retrieval

**Open Street Map (OSM):** For BEV retrieval, we leverage OSM , a global crowd-sourced mapping platform open-sourced under Open Data Commons Open Database License (ODbL). OSM provides rich vectorized annotations for streets, sidewalks, buildings, etc. However, OSM data cannot be easily used for map prediction as (a) OSM often does not encode critical sidewalk geometry, (b) structured formats like OSM maps prove difficult to train on, (c) off-the-shelf rendering pipelines target human consumption often encoding information irrelevant for BEV prediction (such as textual labels) and do not care about pixel aligning maps with satellite imagery, thereby encoding inaccurate road widths in many instances. Recognizing the need to create our own rasterization pipeline, we build on top of the MIT-licensed MapMachine project  and study hundreds of satellite/map pairs to achieve rasterization that is more pixel-aligned with satellite imagery. We further carefully map the hundreds of elements in OSM to a handful of informative dominant semantic labels.

**BEV Generation Pipeline:** BEV retrieval starts after the filtering stage in FPV retrieval as illustrated in Fig. 2. Given coordinates in the World Geodetic System (WGS-84) frame for each image, we project each point onto a Cartesian UTM coordinate frame, estimated separately for each city/location. Next, we calculate an ego-centric bounding box of size \((++)^{2}\) at a resolution of \(\) meters per pixel. Here, \(\) represents the requested image dimension, \(=-)}\) is the padding added to accommodate rotations without introducing empty space, and \(\) is the padding added to avoid missing any OSM elements that may not fall within the original box. To adhere to the OSM API, we project boxes back to WGS-84 coordinates before retrieving OSM data for every image. We then utilize our version of MapMachine (enhanced to infer missing sidewalks from OSM) with a carefully tuned, satellite-aligned map style to render the data into SVG format. Next, we rotate the rendered image so that the robot is looking 'up' in the BEV image plane, aligning it with the forward direction of the FPV plane. Finally, we rasterize the SVG into a semantic mask containing six static classes (Road, Parking, Sidewalk, Crossing, Building & Terrain), as shown in Fig. 3, to produce the final BEV.

## 4 Empowering Map Prediction with the Mia Data Engine

### Sampling the Mia Dataset

We show the utility of the MIA data engine by sampling six different urban-centered locations, extending to the suburbs. We selected highly populated cities - New York, Chicago, Houston, and Los Angeles - to collect challenging scenarios with diverse and dense traffic. Additionally, we included Pittsburgh and San Francisco for their unique topologies. For BEV retrieval, we set \(=224\), \(=50\), and \(=0.5\), resulting in what we believe is the largest public BEV prediction dataset, comprising

Figure 3: **Comparison of default MapMachine-style rendering with the MIA-style**. The figure shows our rendering removes irrelevant information, clusters key semantic categories, aligns better with satellite and is able to provide more accurate sidewalk geometry correctly. Satellite imagery is not part of the MIA data engine and was obtained from  only for tuning map rendering.

approximately 1.2 million FPV-BEV pairs, as shown in Table 1. To illustrate the diversity of the data, we adapt the coverage metric proposed by  in which each image instance covers a radius of 150 meters around its pose. For our sampled dataset, we calculate the coverage at a radius of 112 meters consistent with our chosen \(\) value. As shown in Table 1, our sampled dataset covers \(~{}470~{}km^{2}\), far surpassing all existing BEV prediction datasets by \(6\). This highlights the immense potential of our _scalable_ MIA data engine to produce large quantities of annotated FPV-BEV pairs covering extensive geographies and with varying camera models and focal lengths, as highlighted in Fig. 4.

To further benchmark the generalization capability of map prediction models in more extreme settings, we further sample a small (\( 1.1K\)) rural/remote dataset, which we denote as MIA-Rural. This dataset has a distribution very different from the urban-centered samples. We selected locations with distinct visual appearances, namely Willow (Alaska), Ely (Nevada), and Owen Springs (Australia). To push the boundaries of generalization testing, we disabled the camera model filter for this test set, thereby incorporating a variety of challenging camera models into this extreme dataset.

### Mapper: Training a camera intrinsics-agnostic baseline model

We train a model, Mapper, with MIA to validate the need for such a large-scale and diverse dataset by testing its generalization capability. Leveraging the diversity of the Mapillary dataset requires a model architecture capable of handling various image characteristics, such as focal lengths and image size. Additionally, following OrienterNet , it is reasonable to assume that the robot or phone has orientation information (IMU), and we aim to incorporate this information into our map predictions.

Our goal is to learn a model that takes in a monocular image \(\) to predict a gravity-aligned BEV semantic map \(\). Formally, given an image \(^{3 H W}\), its intrinsic matrix \(^{3 3}\), and its extrinsic matrix \(^{3 4}\), we seek to produce a multi-label binary semantic map in gravity-aligned frame \(^{N Z K}\) where \(K\) is the number of semantic classes. To achieve this, we build on OrienterNet , designed for top-down map localization, as the front-end architecture. This choice accommodates different camera characteristics and pose information from IMUs, thereby leveraging the orientation data from OSM and Mapillary. We add a decoder head on the BEV features (obtained post 2D-to-3D lifting of FPV features) to predict a semantic map, thereby maintaining simplicity in the model architecture. Furthermore, to improve generalization capability, we replace the ResNet encoder with the DINOv2 encoder . For training, we resize and pad images to a 512 x 512 square, applying weighted Dice and Binary Cross Entropy Loss to the BEV pixels within the image frustum. We use DINOv2 ViT-B/14  with registers  as the image encoder. We further augment the dataset with brightness, contrast, and color jittering. We train with a batch size of 128 for 15 epochs, which takes approximately 4 hours using 4 NVIDIA-H100 GPUs. The supplementary material provides more details on the model and training, along with a figure of the model pipeline.

## 5 Experimental Setup

To evaluate generalizability, we test our Mapper model and baselines on multiple datasets, including our diverse MIA dataset and conventional map prediction datasets.

Figure 4: **Samples from the MIA dataset**: Highlighting diversity in time of day, seasons, weather and capture scenarios from vehicles & pedestrians.

**Conventional Datasets:** We evaluate our model on BEV map segmentation benchmarks: NuScenes  and KITTI360-BEV , to demonstrate the generalizability of a model trained on MIA when applied to established datasets. We follow the BEV generation procedure in  for NuScenes. Both datasets are collected from on-road vehicles and present challenges such as occlusions, different times of day, and varying weather conditions. We adhere to the conventional(geographically non-overlapping) data set splits: Roddick et al.'s  split for NuScenes and Gosala et al.'s split  for KITTI360-BEV. Since we focus on static class map prediction, we exclude dynamic elements from the dataset labels before training. For NuScenes, we use only the static map layers, and for KITTI360-BEV, we remove dynamic object labels. In the experiments, we use the front-facing camera data to evaluate monocular camera BEV map prediction. Class mappings between the datasets are provided in Table 6 of the appendix.

MIA **Dataset:** We utilize the MIA dataset to assess performance in diverse urban and rural settings not captured by previous datasets, including held-out environments. Specifically, we split our dataset into two settings: MIA-ID (New York, Los Angeles, San Francisco, Chicago) represents in-distribution urban areas, MIA-OOD (Houston, Pittsburgh) tests the generalization ability in held-out urban settings. Furthermore, we use MIA-Rural to provide a more challenging out-of-distribution evaluation. For each location, we generate an 80% train / 10% validation / 10% test split, ensuring the splits are geographically disparate as illustrated in Fig. 10 of the appendix.

**Metrics:** We adhere to standard conventions [15; 32] to calculate the Intersection-over-Union (IoU) score using binarized predictions with a threshold of 0.5. The IoU score is computed over the observable area as defined by the visibility mask, based on LiDAR observations or the visibility frustum. Since there is no LiDAR sensing in the MIA dataset, we generate a heuristic-based visibility mask by raycasting from the robot's position, ending 4 pixels into a building. To be consistent with SkyEye evaluations , in KITTI360-BEV, we also include occluded areas within image frustum for IoU calculation. For all datasets, we performed comparisons over a 50m x 50m area with a resolution of 0.5m/pixel. Consistent with prior work [15; 32], we report the macro-mean IoU over all classes. In addition, for a fair comparison across different methods and datasets, we report the macro-mean IoU over the two classes common in all datasets, _road & sidewalk_.

**Baselines:** We compare our results with previously published methods that focus on the monocular single-camera setting, specifically Translating Images into Maps (TIIM) , which was trained and tested on NuScenes , and SkyEye, which was tested on KITTI360-BEV . While newer methods have been proposed [6; 44], TIIM is the most recent method with available code, to our knowledge. We follow the training protocols described in the respective papers and code, with slight modifications to train for static classes. For evaluation, images are processed to meet the requirements of each method. To test the baseline models on datasets they were not trained on, we follow Gosala et al.  and resize the image to match the focal length of the model's training dataset. More details on baseline implementation are provided in supplementary material and appendix.

## 6 Results & Discussion

We firstly evaluate Mapper zero-shot against the baselines. Next, we test MIA's effectiveness for pre-training by finetuning Mapper on limited data from an existing dataset. Finally, we stress test all models in extreme out-of-distribution scenarios to highlight future opportunities enabled by MIA.

### MIA can go more "anywhere" out-of-the-box

Table 2 demonstrates the generalizability of Mapper, trained with the MIA-ID dataset, over both zero-shot & fully-supervised baselines, in particular on the NuScenes and MIA-OOD environments. Fig. 5 visually compares the model predictions, where Mapper provides more realistic predictions across the datasets compared to the zero-shot baseline, which often fails due to the distribution change caused by unseen location, different camera models, or severe weather conditions. When comparing average IoU of Road and Sidewalk, Mapper achieves superior zero-shot result in NuScenes _val_ and MIA-OOD, with improvements of 33% and 144%, respectively. Notably, in NuScenes and MIA-OOD, Mapper performs comparably to fully supervised methods (trained with in-domain data) in road and sidewalk classes. While Mapper performs consistently with _TIIM_ (trained on NuScenes) when tested on KITTI360-BEV , Mapper provides more realistic predictions overall, especially in non-road regions where TIIM tends to overpredict roads. However, due to the limitations ofKITTI360-BEV, where much of the map is unlabeled and IoU is only measured in labeled regions, as illustrated in Fig. 6, it is challenging to perform effective benchmarking and comparisons.

### Pre-training on MIA enables effective fine-tuning with limited data

We also test if the MIA dataset can provide effective pretraining for new map prediction tasks. Specifically, we finetune Mapper with 10% and 1% of NuScenes data. For comparison, we use the same data split to train TIIM . To fine-tune Mapper, we map the new training dataset classes to MIA classes as described in Table 6 of appendix. Details on training, fine-tuning and data splits are available in the appendix and supplementary material. Table 2(d) shows that Mapper can be effectively fine-tuned on specific environments with limited new map prediction data. Notably, the experiment suggests that MIA provides effective pretraining for map prediction, as fine-tuning the pre-trained Mapper yields improved results compared to training TIIM solely on the data subset.

### MIA provides challenging settings for future work on anywhere map prediction

To test model generalizability, we further curate MIA-Rural, which is far from the training distribution. Fig. 7 shows an example predictions from highway images where all models, including our proposed Mapper, fail to generalize. Quantitatively, on the entirety of MIA-Rural, the average IoU between road and sidewalks (Avg. R, S) is 21.04 for Mapper, 20.55 for TIIM  and 18.62 for SkyEye . This further illustrates the research need for an _anywhere_ map prediction dataset.

Figure 5: Mapper **consistently provides more precise & realistic zero-shot predictions across all the datasets.** Notably, Mapper, empowered by MIA data, can produce zero-shot predictions which are comparable to the fully-supervised baselines which have been trained on in-domain data.

Figure 6: **Lack of complete labels in KITTI360-BEV  dilutes quality of benchmarking.** For example, while Mapper predicts sidewalk and road reasonably in this frame, the lack of sidewalk labels in the ground truth results in a misrepresentative IoU. Meanwhile, TIIM’s  road IoU is artificially higher, despite the incorrect road prediction on the left.

### Mia data engine can also effortlessly extract data beyond U.S. cities

To show that our MIA data engine can effortlessly curate data from other global locations, we curate MIA-non-US from 6 non-US cities spanning 3 continents: Europe (London, Stockholm, Berlin,

Table 2: Benchmarking across all the datasets in both zero-shot & finetuning setups.

Figure 8: Example pairs of First-Person-View images & Birds-Eye-View semantic maps effortlessly extracted by our MIA data engine from non-US cities.

Figure 7: **Challenging scenarios mined using the MIA data engine: We curate highway images far from urban environments to stress test models. This showcases our ability to extract challenging and high-impact test scenarios where current models, including Mapper, do not perform well.**

Figure 9: Zero-shot predictions in non-US cities. Mapper consistently provides more accurate & realistic zero-shot predictions.

Zurich), Asia (Tokyo), and Australia (Sydney). Fig. 8 shows example data pairs. Furthermore, we leveraged this data to evaluate the generalizability of existing methods and Mapper. Results in Tab. 3 and Fig. 9 show that our model Mapper consistently provides more accurate and realistic zero-shot predictions compared to baselines trained on conventional datasets.

## 7 Conclusion

In this work, we propose MIA, a data curation pipeline aimed at empowering _anywhere_ BEV map prediction from FPV images. We release a large MIA dataset obtained through the data engine giving the research community access to \(\)1.2M FPV-BEV pairs to accelerate _anywhere_ map prediction research. Results from training on the dataset show impressive generalization performance across conventional map prediction datasets, while on the other hand, provides challenging test cases for the research community. Our approach departs from the traditional and expensive autonomous vehicle data collection and labeling paradigm, towards _automatic curation of readily-available crowd-sourced data_. We believe this work seeds the first step towards _anywhere_ map prediction.

### Limitations, Biases, Social Impact

While we show promising generalization performance on conventional datasets, we note that label noise inherently exists, to a higher degree than manually collected data, in crowd-sourced data, in both pose correspondence and in BEV map labeling. Such noise is common across large-scale automatically scraped/curated benchmarks such as ImageNet . Moreover, our approach does not capture dynamic classes as they do not exist in static maps. However, we see our approach as indispensable for scale and diversity yet complementary to conventionally obtained datasets.

**Negative Societal Impact:** Our work relies heavily on crowd-sourced data, which places the burden of data collection on open-source contributors. Additionally, while the FPV images and metadata from Mapillary are desensitized, there remains a potential risk of reconstructing private information.