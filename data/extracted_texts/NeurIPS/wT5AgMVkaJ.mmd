# Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms

Miaosen Zhang\({}^{1}\) Yixuan Wei\({}^{2}\) Zhen Xing\({}^{3}\) Yifei Ma\({}^{4}\) Zuxuan Wu\({}^{3}\) Ji Li\({}^{4}\)

Zheng Zhang\({}^{4}\) Qi Dai\({}^{4}\)\({}^{}\) Chong Luo\({}^{4}\) Xin Geng\({}^{1}\)\({}^{}\) Baining Guo\({}^{1}\)\({}^{}\)

\({}^{1}\)Southeast University \({}^{2}\)Tsinghua University \({}^{3}\)Fudan University \({}^{4}\)Microsoft

{miazhang,xgeng,307000167}@seu.edu.cn qid@microsoft.com

###### Abstract

Modern vision models are trained on very large noisy datasets. While these models acquire strong capabilities, they may not follow the user's intent to output the desired results in certain aspects, e.g., visual aesthetic, preferred style, and responsibility. In this paper, we target the realm of visual aesthetics and aim to align vision models with human aesthetic standards in a retrieval system. Advanced retrieval systems usually adopt a cascade of aesthetic models as re-rankers or filters, which are limited to low-level features like saturation and perform poorly when stylistic, cultural or knowledge contexts are involved. We find that utilizing the reasoning ability of large language models (LLMs) to rephrase the search query and extend the aesthetic expectations can make up for this shortcoming. Based on the above findings, we propose a preference-based reinforcement learning method that fine-tunes the vision models to distill the knowledge from both LLMs reasoning and the aesthetic models to better align the vision models with human aesthetics. Meanwhile, with rare benchmarks designed for evaluating retrieval systems, we leverage large multi-modality model (LMM) to evaluate the aesthetic performance with their strong abilities. As aesthetic assessment is one of the most subjective tasks, to validate the robustness of LMM, we further propose a novel dataset named HPIR to benchmark the alignment with human aesthetics. Experiments demonstrate that our method significantly enhances the aesthetic behaviors of the vision models, under several metrics. We believe the proposed algorithm can be a general practice for aligning vision models with human values.

## 1 Introduction

Large-scale data pretrained models, e.g. CLIP , have been applied to a broad range of fields, e.g. visual generation , understanding , LMMs . They are trained on very large image-text pair datasets, e.g. LAION  and DataComp , rather than the traditional ImageNet . These datasets contain noisy labels, and exhibit diverse data quality. As a result, though models trained on such datasets demonstrate strong capabilities on semantic matching in the wild, they may prefer samples that violate the intents from users, as shown in Fig. 1. For example, using a vision-language model as an one stage retrieval system, with a huge amount of images in the database, the model may pick the images that exactly match the search query but with unappealing visual appearance. Moreover, it may provides harmful results that disrupt the principle of responsible AI (RAI). Existing retrieval benchmarks  also lack evaluation for aesthetics and RAI.

These problems are crucial in real retrieval engines, and are lacking investigation in the research. Among the products of the industrial community (_e.g._, Google search, Bing search, etc.), such problems are mitigated by a multi-stage approach, _i.e._, a cascade of semantic search and multiple quality filters or re-rankers. However, multi-stage approach may introduce extra latency and a cascade of model biases, and it requires more manpower and resources to maintain, debug and A/B test. Therefore, integrating human preferences into model features and simplifying retrieval into an end-to-end system shows great research value and engineering importance, especially in the scenarios where on-end devices and large-scale API services are arranged.

Luckily, in the field of natural language processing , the problem of misalignment has been extensively studied. Supervised fine-tuning and reinforcement learning from human feedback (RLHF)  have been proven to be effective, which significantly improve the quality of model outputs. Similar method is also widely adopted in some vision-language tasks, primarily in image captioning , and has recently been extended to non-textual vision tasks . Nevertheless, the utilization of RL for subjective preferences in pure vision tasks has not yet been explored.

In this paper, we target the realm of visual aesthetics as a representative of human preference and aim to align the pre-trained vision models with human aesthetics. In Oxford dictionary , "Aesthetic" has two explanations: (1) "Connected with beauty and art and the understanding of beautiful things." (2) "Made in an artistic way and beautiful to look at." We re-express this concept in Fig. 2. High level understanding of aesthetic may involve the cultural or symbolic aspects that require reasoning related to the object. Low level part of aesthetic is related to image resolution, layout, saturation, etc. Particularly, this visual appeal (low level part) is considered as some statistical prior information, which can be learned by an end-to-end neural network to a great extent.

Based on the above understanding, we build our pipeline as in Fig. 2, which first leverages the strong reasoning ability of LLMs to extend the query with expectations of implicitly containing the understanding of beauty. We find that using this rephrased query in retrieval drastically boosts the aesthetic quality more than we ever expected. Then, we utilize public aesthetic models to re-rank the retrieved images, resulting in a high quality image sequence that contains the inductive bias of both mechanisms and agrees with both aspects of aesthetic. Finally, a preference-based reinforcement learning method, adapted from DPO , is proposed to align the model with the sequence.

Former well-known open-source aesthetic datasets (_e.g._, ) were mainly designed for image aesthetic assessment (IAA) task, which cannot be used for aesthetic retrieval evaluation without adaptations. Thus, we propose two methods to evaluate models. For system level evaluation, we use GPT-4V as a judge to simulate users to choose a favored retrieval system within two candidates. Due to the subjective nature of aesthetic, we further construct a novel dataset (named HPIR) labeled by humans for model evaluation, and validating the reliability of the GPT-4V judge.

We make several contributions in this work: (1) We benchmark the alignment of human aesthetics with two methods, using both a novel dataset and using GPT-4V as a judge, which also investigates

Figure 1: Alignment examples. W/o alignment, the models may prefer samples violating user intents.

how to prompt those large multi-modality models toward better aesthetic judgement. (2) We present that LLMs rephrasing of queries can significantly improve the aesthetic scores. (3) We propose a preference-based reinforcement learning method to align existing vision models with human aesthetics. Last but not least, aesthetics is one of the most subjective components of human preferences, and hence we believe our method can be easily generalized to other aspects of human preference.

## 2 Method for Aesthetic Alignment

### Model Pretraining

We use both self-pretrained model and open source models [38; 8] for alignment fine-tuning. We pretrain our vision-language model using the adapted CLIP contrastive loss , which can be formulated as follows:

\[_{}=-_{i=1}^{N}_{j=1}^{N}l_{i}^{ }(/)}{_{k=1}^{N}(s_{ik}/)} ),\] (1) \[_{}=-_{j=1}^{N}_{i=1}^{N}l_{j}^{ }(/)}{_{k=1}^{N}(s_{kj}/)} ),\] (2)

where \(s_{ij}=}_{i}^{}}_{j}\) is the cosine similarity between the embeddings \(}_{i},}_{j}\) of the corresponding image and text. \(\) is the temperature parameter and \(l_{i}^{}=(1-) l_{i}+/N\) indicates a smoothed version of label \(l_{i}\) with a factor of \(\). The final pretraining loss \(_{pt}\) is the sum of the text and image losses:

\[_{pt}=_{}+_{}.\] (3)

The vision model and language model are initialized from the Swin-V2-L  and Roberta-L , respectively. We leverage several advanced techniques [57; 9; 55] and we leave our detailed insights into the pretraining process and data composition in Appendix. A.

### Aesthetically Query Rephrasing with LLMs

According to the explanation of "aesthetic", a query with explicit understanding of aesthetic will potentially benefit the quality of retrieved images. While a typical user's text query can be quite plain, we expect to leverage LLMs (e.g., GPT-3.5-turbo) to enrich such concepts and contents. The participation of LLMs or LMMs is crucial because high level of aesthetic understanding requires their strong reasoning ability. Existing aesthetic models do well to differentiate high and low quality images when they have a great quality gap, but when the gap is small, LMMs surpass all of them significantly via reasoning (Table 13 and 10 in Appendix). This indicates that to make further development of aesthetic understanding, reasoning is essential. While directly labeling a training dataset by LMMs is not acceptable in both latency and cost, using LLMs to reason and extend the query becomes a nice

Figure 2: The concept of aesthetic, which inspires our pipeline of alignment. The specific and technical details are shown in Fig. 4.

substitute. In addition, LLMs can further refine the query with the following advantages: (1) Enrich queries with visual details, yielding more aesthetically appealing and expectation-aligned results. (2) Mitigate issues stemming from user search text style, misspellings, and incorrect emphasis.

The impact of the above enhancements will be quantified in Sec. 4.2. Our utilized prompt template can be found in Appendix. F.1, in which a'method' indicating the rules that query should obey is required. We evaluate four distinct method prompts in Sec. 4.2, and finally advocate the one termed as <k list>:

Generate a comma-separated list of succinct object descriptions, visual details, or stylistic elements to extend the aesthetic understanding and expectations, ordered from the most to the least significant.

An example of query rephrasing is shown in Fig. 3. When we think about "Instagram style", we usually have an imagination of light scene with clean and a little minimalist design. LLM rephrasing adds these elements directly to the query, resulting in a more satisfying retrieval results. In addition to aligning with user's implicit imagination, when the standard of beauty is associated with context of cultural or knowledge, LLM rephrasing can also significantly boost the results. More cases are presented in Appendix. C.

### Aesthetic Alignment Fine-tuning

We aim to directly align the retrieval model with human aesthetics, eliminating the multi-stage retrieval system with re-ranker. To obtain the training data for fine-tuning, we leverage public aesthetic models to build a two-stage retrieval system, generating sorted high-quality image sequences. Particularly, given the images retrieved by pretrained model, we utilize well-known semantic (_e.g._, CLIP ) and aesthetic (_e.g._, CLIPIQA , IAP  and MANIQA ) models as the re-ranker. Although these models are not exactly in the same category (e.g., MANIQA is a no-reference image quality assessment model instead of image aesthetic quality assessment model strictly speaking), we choose these models because during our engineering testing, they perform well as second stage re-rankers. Note that in real world engineering, we can adapt this pipeline to multi-stage system that may further leverage information like click rate, making the pipeline become RLHF.

**Data preprocessing.** We collect our training data using a four-step process:

* We analyze the topic distribution of user queries and employ GPT-3.5-turbo to synthesize \(N=80,000\) pseudo queries that mirrored the distribution of authentic user queries. This procedure can protect user privacy well.
* Each generated query is subjected to a rephrasing process as described in Sec. 2.2. The modified outputs are regarded as rephrased queries.

Figure 3: Effect of LLM rephrasing. All images are retrieved from the same fixed engine. The advancement of LLM rephrasing has clearly enhanced the aesthetic quality of outputs, particularly in expressing abstract notions and stylistic elements.

* For each query, we utilize our pretrained model along with an Approximate Nearest Neighbor (ANN) search algorithm  to quickly retrieve the top \(K=400\) images from a 75 million subset of DataComp, using the rephrased query.
* We compute the score of re-ranker (semantic and aesthetic models) for each image in search results.

The final training dataset \(\) is structured as follows:

\[=\{(q_{i},_{i},_{i})|i=1,,N;_{i}=[ _{i}^{(1)},_{i}^{(2)},,_{i}^{(K)}]\},\] (4)

where \(q_{i}\) and \(_{i}\) are pseudo query and rephrased pseudo query, and each \(_{i}^{(j)}\) is defined as a tuple that contains image \(y^{(j)}\) and the re-ranking scores:

\[_{i}^{(j)}=(y^{(j)},\ S_{}^{(j)}).\] (5)

**Fine-tuning from AI feedback.** We model the retrieval problem as a reinforcement learning problem: for a given search query \(q\) and an image database \(=\{y_{n}\}\), we denote the retrieval system with learnable parameters as the policy model \(_{}(y|q;)\). For some of the retrieved images, e.g., \(y_{i}\) and \(y_{j}\), we can establish a preference \(y_{i}>y_{j}\) (or \(y_{i}<y_{j}\)) to signify that image \(y_{i}\) (or \(y_{j}\)) is a preferred retrieval response. Assume that these preferences are generated by some underlying reward model \(r_{}(y,q)\), e.g., human/AI scorer, and aesthetic model. Reinforcement learning maximizes the expectation of rewards while using KL divergence for regularization to prevent training collapse and overfitting:

\[_{_{}}_{q,y_{}(y|q; )}[r_{}(y,q)]-_{KL}[_{}(y|q; )\|_{ref}(y|q;)].\] (6)

Here, \(_{ref}\) is the reference model (i.e., the pretrained model). Following DPO , by choosing Bradley-Terry model  to formulate preference distribution, we can use the following policy objective loss to maximize the reward:

\[_{dp_{0}}=-_{(q,y_{w},y_{l})_{po}}[ ((y_{w}|q;)}{_{ref}(y_ {w}|q;)}-(y_{l}|q;)}{_{ref} (y_{l}|q;)})],\] (7)

where \(y_{w}\) is the preferred sample compared to \(y_{l}\). In retrieval scenario, given a user search query \(q\), we build the partially ordered dataset \(_{po}\) for training by establishing those ordered pairs: \(_{po}=\{(q,y_{i},y_{j})|y_{i}<y_{j}\}\). The probability that multimodal based policy model return response \(y_{i}\) is given by the normalized cosine similarity:

\[_{}(y_{i}|q;)=^{}(y_{i}),f_{l}^{ }(q))}{_{y_{k}}(f_{v}^{}(y_{k}),f_{l}^{ }(q))}.\] (8)

Here, \(f_{v}^{}\) and \(f_{l}^{}\) represent the vision and language encoders of the multimodal model, respectively. It is easy to observe that the denominator part of \(_{}(y|q;)\) will be cancelled out in \(_{dp_{0}}\), thus in actual calculations, we only need to calculate the cosine similarity of the corresponding images and queries, which also makes \(_{dp_{0}}\) independent of the image database \(\). Compared to DPO , we utilize an ordered sequence to obtain samples for adapting the DPO loss, allowing producing \(O(n^{2})\) preference pairs from a sequence of length \(n\). This approach significantly enhances the data utilization rate, making the modified DPO algorithm more scalable.

Following InstructGPT , we also integrate the pre-training loss \(_{pt}\) to stabilize the training process and maintain retrieval capability. Consequently, the composite loss function formulated for fine-tuning is expressed as:

\[=_{dp_{0}}+w_{pt}_{pt}.\] (9)

**Construction of \(_{po}\).** We illustrate the construction of the partially ordered dataset \(_{po}\) in Fig. 4. For each query, images are intermittently selected from the retrieved results of the rephrased query at intervals defined as stride, aiming to amplify the quality discrepancy among the chosen images. Subsequently, these images are arranged into a matrix with \(u\) rows and \(v\) columns, following a row-major format. Samples within each row are sorted according to the score of the re-ranker. The re-ranker, which is finally designed by assembling open-source models, is evaluated in Appendix. E. We then define the column dimension as the aesthetic dimension, since the samples in each row are sorted aesthetically. The row dimension is defined as the semantic dimension because semantic relevance varies across different rows. We extract all rows and columns to obtain \(u+v\) ordered sequences with the form \(y_{1}>y_{2}>>y_{k}\), resulting in \(C_{k}^{2}\) pairs of \((y_{i},y_{j})\) in each sequence. To this end, a total of \(uC_{v}^{2}+vC_{u}^{2}\) partial order pairs can be produced for each query. Note that numerous operations in this process can be tensorized and executed in parallel, thus the time cost is low.

## 3 Benchmarking Human Preference

Standard retrieval benchmarks, including MSCOCO  and Flickr30k , lack the aesthetic evaluation. Aesthetic datasets, _e.g._ AVA , can only be used to evaluate the accuracy of an aesthetic prediction model, which needs additional efforts for retrieval system evaluation. Therefore, we introduce the following two novel benchmarks to assess whether a retrieval model can align with human aesthetics well: (1) testing model preference by human-labeled dataset, and (2) using GPT-4V  to determine the relative performance of two systems.

### Human Preference of Image Retrieval (HPIR)

We introduce HPIR, a test set of human preference alignment. It leverages 150 pseudo queries for testing, which are generated using LLMs by requesting an aligned distribution with the user's topics. For each query, we combine the results of multiple search engines and obtain 10 images, which are divided into two groups (A and B). Human labelers are asked to evaluate the two groups of images, determining which group is more precise and visually appealing. To ensure robustness, each comparison is annotated for 30 times. We also scrutinize the annotation time and order consistency (Sec. 3.2) to guarantee the quality. The label that predominated in the 30 annotations is designated as the golden label. Let \(N_{pos}\) be denoted as the number of labelers that assign the golden label, and \(N_{neg}\) as the remaining number. We define the confidence score \(w_{c}\) (exemplified in Fig. 14 of Appendix) of this annotation as:

\[w_{c}=}{N_{pos}+N_{neg}}-1.\] (10)

This confidence level has a similar effect to variance, and the variance formula for human labelers in aesthetic annotations can be easily calculated as follows:

\[var=N_{neg}}{(N_{pos}+N_{neg})^{2}}.\] (11)

To evaluate a model/search engine, we task it with discerning the better group between A and B for all queries, based on a designated criterion. Then the HPIR metric \(M_{asp}\) (\(asp\) stands for either accuracy or aesthetics.) is assessed by comparing the selections of the model/engine to the human-annotated golden labels. \(M_{asp}\) is formulated as a confidence-weighted average over the queries:

\[M_{asp}=w_{c}\{choice=golden\_label\}}{ _{query}w_{c}},\] (12)

where \(\{choice=golden\_label\}\) is an indicator that equals 1 when the model's choice matches the golden label, and 0 otherwise. This method can effectively assess the degree of alignment between a model and human preferences. For instance, to evaluate the CLIP , we simply compute the average CLIP similarities (to query) on group A and B, choosing the group with the higher average as the model's choice. More details about data distribution, baseline results and aesthetic model evaluation can be found in Sec. 4.1 and in Appendix. E.

Figure 4: An example illustration for the construction of partially ordered pair dataset.

### GPT-4V Win Rate

LMMs have shown their strong abilities across numerous tasks. Thus, we directly compare two retrieval models/systems using GPT-4V . Emulating the AlpacaEval  approach from LLMs, we first conduct image searches for a collection of queries using two retrieval systems, R1 and R2. We then concatenate results in R1 and R2 into one large image and employ GPT-4V as the judge to assess which system performs better. We note that GPT-4V tends to prefer the first row when the results from both systems are comparable, a tendency that mirrors human behavior. To address the bias, we introduce an **order-consistency** (OC) strategy, where we initially place images from R1 on the first row and images from R2 on the second for evaluation, then invert their positions for a separate assessment. A visualization and more detailed description is provided in Appendix. D.1. If two assessments have conflicting conclusions, we say the two results are similar. Lets denote the number of R1 wins as \(N_{w}\), loses as \(N_{l}\) and similar as \(N_{s}\). System R2 serves as the baseline. We define the win rate of R1 as \(R_{win}\), and a win-and-similar rate as \(R_{win\&similar}\):

\[R_{win}=}{N_{w}+N_{l}},\] (13) \[R_{win\&similar}=+N_{s}}{N_{w}+N_{s}+N_{l}}=1-}{N_{w}+N_{s}+N_{l}}.\] (14)

Unlike HPIR, this approach lacks the supervision of human labelers, necessitating meticulous design and validation to ensure its soundness. We thus leverage HPIR feedback to filter various prompts and evaluation methods, ultimately selecting a prompt format referred to as <ranker> (see Appendix. G). Detailed experiments, shown in Appendix. D.2, yield that with this prompt and order-consistency, GPT-4V can present comparable aesthetic judgments to humans.

## 4 Experiments

In this section, we present our main experiments. More results, including ablations, benchmark evaluations (HPIR and GPT-4V judge), and LLM rephrasing, are in Appendix. B - F.

### Details and Evaluations of Alignment Fine-tuning

In the alignment fine-tuning loss, the \(_{pt}\) component is configured identically to the pretraining phase described in Sec. 2.1, encompassing batch size, temperature, and data, with a weight of \(w_{pt}=1.0\). For the remaining components, each batch comprises 128 queries. The overall learning rate is fixed to \(lr=5 10^{-5}\). The partially ordered set \(_{po}\), as discussed in Sec. 2.3, is derived using \(u=v=5\), and a stride of 10.

We conduct the experiments with two other state-of-the-art models: CLIP  and DataComp . The image encoders of CLIP and DataComp are ViT-L/14 models, trained on a private 400M dataset and on the DataComp-1B dataset, respectively. We report their performance on classic retrieval benchmarks (ImageNet1K  zero-shot classification, MSCOCO  T2I retrieval Recall@1, and Flickr30K  T2I retrieval Recall@1) and on the proposed HPIR in Table 1. It is not surprising that our model performs worse to DataComp on MSCOCO and Flickr30K, since our training budget is much smaller than DataComp. We can further observe that our alignment fine-tuning does not significantly impact the retrieval capability, but it greatly enhances the aesthetic scores of the retrieval results, surpassing both original CLIP and DataComp. While fine-tuning on the CLIP and DataComp, a similar increase is observed in aesthetic ability, demonstrating the generalization of our method.

In Table 2, we report the system-level comparison results with other models, approaches and two commercial search engines (Bing image search  and Getty search ). We report the win

    &  &  \\  Model & ImageNet1K-ZS & MSCOCO & Flickr30K & Accuracy & Aesthetic \\  CLIP & 76.2 & 37.8 & 68.7 & 68.1 & 62.1 \\ DataComp & 79.2 & 45.7 & 73.4 & 71.8 & 62.7 \\ Ours-PT & 82.1 & 40.2 & 66.5 & 66.2 & 59.4 \\  CLIP + RLFT & 79.4 & 38.1 & 67.2 & 73.1 & **71.7** \\ DataComp + RLFT & 81.7 & 45.1 & 72.2 & 74.4 & **71.9** \\ Ours-RLFT & 82.2 & 40.2 & 66.4 & 71.7 & **67.6** \\   

Table 1: Performance on traditional retrieval benchmarks and our proposed aesthetic alignment dataset. PT indicates pre-training, and RLFT indicates our alignment fine-tuning.

and-similar rate here because it is more in line with the user's thinking when choosing products (see other indices and details in Appendix. D.3). Experiments are conducted on a database of 15M images extracted from DataComp-1B and our internal database with 8M images. The experiments demonstrate the effectiveness of our alignment fine-tuning, the superiority of our final model, and the gap with commercial web-scale products. Our finetuned model shows comparable performance with 2-stage approach, yielding a possible latency and pipeline optimization to real world products (see win rate in Appendix. D.3). Systematically speaking, despite the vast difference in the size of the databases, our model still achieved a win-and-similar rate of 50%\(\)60% in comparisons with Bing image search and Getty search. Additional human user evaluations of selected experiments also validate the reliability of the GPT-4V judge. These labeling processes can be seen as real world A/B test user studies.

### Effect of LLM Rephrasing

Here, we test different method prompts, which correspond to {method} as outlined in the template in Sec. 2.2, for search query rephrasing. We task GPT-3.5-turbo with the job of rephrasing queries to an approximate word count of 50. The method <k list>, as introduced in Sec. 2.2, enumerates additional descriptions for the query. The <detail> method encourages the model to elaborate with more specifics, while <kw dict> instructs the LLM to enumerate keywords followed by detailed expansions for each. We employ two verification metrics as described in this paper: HPIR and GPT-4V win rate, with the latter benchmarked against the original query's baseline results. The detailed prompts are described in the Appendix. F.2.

Table 3 reports the evaluation results, including the average scores from aesthetic models. The empirical evidence from all three metrics suggests that the utilization of LLMs for query rephrasing has enhanced the aesthetic appeal of the search results. We choose win rate to report because it brings clearer clues of which prompt is better (see other indices and details in Appendix. D.4).

To investigate the mechanisms by which query rephrasing yields enhancements, we further evaluate two additional rephrasing methods for extending the length of the original query: the "repeat" method, which involves duplicating the original query \(n\) times, and the <reorg> method, which entails prompting the LLM to reformulate the query in diverse linguistic styles, then repeating it \(n\) times

    &  &  &  \\  ID & Name & Database & Stages & Name & Database & Stages & Accuracy &  \\ 
1 & Ours-FT & Datacomp-15M & 1 & Ours-PT & Datacomp-15M & 1 & 72.0 & 78.7 \\
2 & Ours-FT & Datacomp-15M & 1 & CLIP & Datacomp-15M & 1 & 74.0 & 77.3 \\
3 & Ours-FT & Datacomp-15M & 1 & Datacomp-Datoom-15M & 1 & 69.3 & 67.3 \\
4 & Ours-FT & internal-8M & 1 & Ours-PT + Rernak & internal-8M & 2 & 68.0 & 70.0 \\
5 & Ours-FT \(\) & internal-8M & 1 & Bing search & web & 2 & 45.3 & 57.3 \\
6 & Ours-FT \(\) & internal-8M & 1 & Getty search & Getty Images & 2 & 62.7 & 62.7 \\   \\ 
1’ & Ours-FT & Datacomp-15M & 1 & Ours-PT & Datacomp-15M & 1 & 65.3 & 74.7 \\
4’ & Ours-FT & internal-8M & 1 & Ours-PT + Rernak & internal-8M & 2 & 66.9 & 71.4 \\
5’ & Ours-FT \(\) & internal-8M & 1 & Bing search & web & 2 & 49.1 & 56.6 \\
6’ & Ours-FT \(\) & internal-8M & 1 & Getty search & Getty Images & 2 & 63.3 & 61.3 \\   

Table 2: System level comparison results. Mark \(\) indicates using LLM rephrasing. DataComp-15M is a 15 million subset of DataComp-1B dataset .

    &  &  &  \\  prompt & CLIPIQA & IAP & MANIQA & Accuracy & Aesthetic & Accuracy & Aesthetic \\  original & 0.8544 & 4.8047 & 0.4296 & 66.19 & 59.36 & - & - \\ detail> & 0.8787 & 5.0698 & 0.4279 & 65.25 & 68.24 & 63.51 & 68.75 \\ k list> & 0.8768 & 5.0772 & 0.4384 & 68.95 & 72.42 & 53.37 & 67.86 \\ kw dict> & 0.8678 & 5.0140 & 0.4345 & 69.17 & 69.93 & 53.57 & 63.83 \\ repeat & 0.8554 & 4.9525 & 0.4395 & 67.96 & 65.67 & 62.00 & 63.16 \\ reorg> & 0.8742 & 4.9925 & 0.4463 & 68.33 & 67.85 & 59.65 & 64.44 \\   

Table 3: Evaluation of different method prompts for LLM rephrasing on HPIR and GPT-4V win rate. Scores from aesthetic models are also provided.

without incorporating additional details. As shown in Table 3, simply enlarging the length of the query, even in the absence of new details, can enhance the aesthetic performance. Leveraging LLMs to deepen the comprehension of the query and enrich the visual specifics allows for further aesthetic improvement in retrieval tasks. We further summarize 2 possible reasons for this phenomenon in Appendix. F.3.

## 5 Cases Study and Qualitative Comparison

Fig. 5 shows the qualitative comparison between our fine-tuned model and pretrained model, where we retrieve top-4 images from the 75M subset of DataComp. It can be observed that the alignment fine-tuning endows the model with the capability to retrieve images with vivid background, rich texture details, and dynamic color contrast, leading to more aesthetically pleasing search engine.

More comparison result and analysis with and without LLM rephrasing using our fine-tuned model can be found in Appendix. C. With LLM rephrasing, the retrieved images exhibit remarkable improvement on visual coherence and enriched details. The styles of the images become more consistent with the search intent, capturing samples that align closely with human expectation.

## 6 Related Work

**Vision Language Models.** The availability of web-scale image-text pairs has sparked the research of vision-language models . The pioneering works, CLIP  and ALIGN , utilize contrastive loss during training to achieve remarkable generalization capabilities. Subsequent works  have expanded image-text contrastive to a wider scope. BeiT3 , CoCa  and BLIP  further explore other pretraining methods. More recently, several large multi-modal models have emerged . While most methods have shown strong retrieval capabilities, they often overlook aesthetics, frequently retrieving results with poor visual quality. Our work aims to fill this gap, focusing on designing a vision-language model with aligned aesthetics with humans.

**Reinforcement Learning from Human Feedback (RLHF).** RLHF has been widely adopted in LLMs . Typically, a reward model is trained as a proxy for human preferences, providing feedback for model tuning. Recently, researchers focus and apply RLHF technique to computer vision . In image generation, Lee _et al._ and ImageReward  utilize the reward modeling for text-to-image tasks. To the best of our knowledge, our work is the first to focus on aligning human intents with the fundamental task of text-image retrieval.

Figure 5: Qualitative comparison of top-4 retrieval results between models with and without our proposed alignment fine-tuning.

**Image Aesthetics Assessment.** Numerous aesthetic evaluation protocols have been proposed [17; 47; 11; 22; 34; 42]. Kong _et al._ propose relative ranking to model photo aesthetics. NIMA  assigns a score distribution to each photo, which captures the subjective variations in human aesthetic preferences. MANIQA  considers the no-reference image quality assessment task and employs a multi-scale attention mechanism to analyze images across various scales and regions. CLIPIA  tries to understand image content and trains the model by comparing the quality of images. In this work, we adopt a weighted combination of existing models [62; 50; 42; 38] to provide supervision for our training. In addition, several IAA datasets (AVA , PN , and CHUNK-PQ ) have been proposed for evaluating aesthetic models. However, in retrieval setting, we need to compare the aesthetic levels of two image groups corresponding to a shared query. Existing datasets cannot satisfy the needs in retrieval scenario.

## 7 Potential Social Impacts and Risks

We state that all labelers (mostly university students) were informed of all the uses of their labels, and they agreed on the use. All experiments were conducted with open-source datasets and proprietary data for which we own the full copyright.

Our work may cause potential social impacts and risks:

* Diversity and quality trade-off: The LLM rephrasing stage may increase the quality while decrease the diversity, which generally does more good than harm, but in cases where more diversity is required, several simple adjustments can be made to adapt. For example, repeating the rephrasing may benefit both direct inference and training models. In addition, increasing the pre-training loss weight during fine-tuning may also be an option to balance the trade-off.
* Potential biases: LLMs are trained with massive internet-scale data and may therefore contain potential biases. This may reinforce the biases of the retrieval system. For example, when searching for the word 'nurse', the vector search system using the CLIP feature returns approximately 90% female images and 10% male images. However, when this system uses LLM rephrasing as preprocessing, it results in nearly 100% female images. This is because the rephrased sentence describes a female nurse. However, our approach also provides an opportunity to supervise and eliminate some biases. A vector search system using the CLIP feature is a black box, and we cannot predict or control when it may retrieve harmful content. By using LLM rephrasing, we can trace intermediate information and debug the system. We can further improve LLMs or implement a text-based filter.
* Specific RAI problems: As we mentioned, our algorithm can be adapted to other areas of RAI problems, such as nationality and race. While our approach may benefit many general cases, it may potentially ignore minorities, as aesthetics and ethics may differ across various cultures and regions. Specific adaptations should be made to both LLMs and vision models in accordance with the 'no free lunch' theorem.

## 8 Conclusion

In this paper, we attempted to align image retrieval models with human aesthetics. We presented a preference-based reinforcement learning method to align retrieval models with human aesthetics by distilling knowledge from LLMs reasoning and aesthetic models. Extensive experiments demonstrated the effectiveness of our method, showing a possible alternative to the multi-stage retrieval pipeline. Finally, we discussed the potential social impacts and risks. We believe our proposed approach can become a general practice for other misalignment problems in computer vision.