# Contrasting Sequence with Structure:

Pre-training Graph Representations with PLMs

 Louis Robinson

InstaDeep

Timothy Atkinson

InstaDeep

Liviu Copoiu

InstaDeep

&Patrick Bordes

InstaDeep

&Thomas Pierrot

InstaDeep

&Thomas D. Barrett

InstaDeep

###### Abstract

Understanding protein function is vital for drug discovery, disease diagnosis, and protein engineering. While Protein Language Models (PLMs) pre-trained on vast protein sequence datasets have achieved remarkable success, equivalent Protein Structure Models (PSMs) remain underrepresented. We attribute this to the relative lack of high-confidence structural data and suitable pre-training objectives. In this context, we introduce BioCLIP, a contrastive learning framework that pre-trains PSMs by leveraging PLMs, generating meaningful per-residue and per-chain structural representations. When evaluated on tasks such as protein-protein interaction, Gene Ontology annotation, and Enzyme Commission number prediction, BioCLIP-trained PSMs consistently outperform models trained from scratch and further enhance performance when merged with sequence embeddings. Notably, BioCLIP approaches, or exceeds, specialized methods across all benchmarks using its singular pre-trained design. Our work addresses the challenges of obtaining quality structural data and designing self-supervised objectives, setting the stage for more comprehensive models of protein function. Source code is publicly available2.

## 1 Introduction

The study of proteins, central to cellular function, impacts fields such as medicine, biotechnology, and computational biology. While the amino acid sequence of a protein carries vital information, its 3D structure often holds the key to understanding its function and putative interactions. Machine learning, especially Protein Language Models (PLMs), has recently revolutionized protein modelling. PLMs pre-trained on extensive sequence data have demonstrated the ability to capture intrinsic relationships in amino acid sequences  in rich representations that can be leveraged for various downstream applications  - mirroring the trends observed in other domains such as natural language processing [3; 4] and computer vision . However, despite the success of machine learning in protein structure prediction, epitomized by AlphaFold , general pre-trained protein structure models (PSMs) have not yet found the same ubiquitous application as their sequence-based counterparts. We attribute this to two main challenges; (i) data scarcity and (ii) objective complexity.

High-quality protein structure data is hard to come by and often expensive. Methods such as X-ray crystallography and cryo-electron microscopy, though very insightful, are not without limitations. Whilst recent tools such as AlphaFold and ESMFold have enabled the generation of massive protein structure datasets, they only predict the 3D coordinates, and can still be imperfect especially for multi-state proteins or proteins with shallow MSAs [8; 9]. However, objective formulation remains a significant hurdle. While masked sequence prediction has proven highly effective for pre-training PLMs, defining self-supervised objectives for structure data is far more challenging due to the continuous and multi-dimensional nature of protein structures.

Motivated by this, we introduce BioCLIP, a self-supervised contrastive learning framework for building latent representations of protein structures and sequences. The core intuition behind BioCLIP is to leverage the quality of pre-trained PLM embeddings trained on abundant sequence data to facilitate the training of a PSM. The model employs a loss function inspired by Contrastive Language-Image Pretraining (CLIP) , incorporating both per-residue and per-chain embeddings to create a comprehensive representation of protein structures.

We validate BioCLIP's efficacy through tests on protein-protein interaction prediction, GO-term annotation, and Enzyme Commission number prediction. Our findings underscore three points: (1) BioCLIP's pre-trained Graph Neural Network (GNN) surpasses conventional training methods, (2) structural embeddings enhance sequence embeddings and usually boost performance when combined, and (3) BioCLIP approaches or outperforms specialized methods.

## 2 Related Work

Protein Language ModelsRecent advances in scaleable transformer architectures  have facilitated an explosion in pre-trained language models, e.g. [12; 13; 4]. Correspondingly, a number of pre-trained _protein language models_ have emerged, employing the same principles of auto-regressive or masked-prediction while targeting the vast available protein sequence data. For example, the ESM family of BERT-like models are trained to do masked prediction  which can be used for a variety of downstream tasks [15; 16; 17], ProtGPT2 is an autoregressive model capable of generating novel, realistic protein sequences . While these PLMs in many ways represent a state-of-the-art, the inclusion of additional structural information may improve performance in practice .

Protein Structure ModelsProtein structure is a key modality in modelling of protein function, with a variety of research interest in both predicting protein structure [6; 20] and leveraging predicted or experimentally obtained structural information for protein modelling . With the view that protein structure, represented as a set of residue coordinates in 3D space, can directly be mapped to a graph structure, there is a clear affinity with the subject of graph neural networks. In particular, a

Figure 1: Pre-training, fine-tuning and downstream task illustrations. The modules in light blue are tuned, the modules in green are fixed.

number of GNN architectures have emerged in the past decade which specifically target invariance and equivariance with respect to the 3D coordinate system [22; 23; 24], some of which have shown promise in protein tasks such as prediction of solubility , function  and binding affinity . However these _protein structure models_ (PSMs) face bespoke challenges, such as the well-studied 'over-smoothing' problem for GNNs  that limits the size and depth of these models. Additionally, there is a relative lack of available structural data; consider that at the time of writing, the Protein Data Bank  has hundreds of thousands of experimentally obtained structures, in comparison to the millions of available protein sequences , although we note that this issue can increasingly be mitigated using predicted structures e.g. [31; 32].

Pre-trained Structure ModelsRecently, a variety of techniques for contrastive learning on graph representations have been proposed. Typically, these incorporate some form of structural augmentation , structure masking  or network perturbation  to create neighborhoods of structure representations for contrastive learning. Of particular relevance to this work, GearNet  and MolCLR  propose graph-augmentation approaches to contrastive learning of protein and molecule structures, respectively.  propose a mask-prediction method where a GNN is trained to reconstruct pairwise distances and angles between residues. However, as these techniques focus on contrastive learning between graph structures, they are, in isolation, unable to leverage the vast available protein sequence data to improve their representations.

Multi-modal Protein EmbeddingsAs both sequence and structure are considered key modalities for modelling protein function, a number of works naturally consider combined representations for downstream tasks. For example, a number of works aims for a 'best of both worlds' by incorporating amino acid embeddings, obtained from a pre-trained PLM, as node features to a PSM that is then trained to predict protein function [39; 40]. However, these approaches do not incorporate unsupervised learning into their PSM components. The approach taken in this work can be motivated by , which presents experimental results showing that representations of sequences obtained via unsupervised learning, specifically with the ELMo model , are more effective in downstream tasks than hand-crafted representations. Further motivating the unsupervised learning of complimentary structural representations,  find that although the embeddings are obtained from sequence alone, they do not benefit from including hand-crafted structural representations. In contrast to these directions, a number of works provide avenues for multi-modal representations through large-scale contrastive learning, particularly in the case of image-text modalities [10; 43]. These methods provide a systematic way to build multi-modal representations of data, which we leverage here, alongside the ubiquitous success of PLMs, to achieve effective pre-training of data-scarce PSMs.

## 3 BioCLIP

BioCLIP is visualised in Figure 1. The core idea is to pass as input both a sequence and a structure representation of a given protein through a PLM and a PSM, respectively, to obtain per-residue and per-protein level embeddings. For a given sequence embedding \(S_{a}\) and structure embedding \(G_{b}\), the scaled cosine distance metric can be used to measure the similarity of the embeddings,

\[d(S_{a},G_{b})= G_{b}}{||S_{a}||||G_{b}||}.\] (1)

For a batch of sequence embeddings \(S=\{S_{1} S_{N}\}\) and structure embeddings \(G=\{G_{1} G_{N}\}\) where \(S_{i}\) corresponds to \(G_{i}\), a CLIP-style loss can be employed,

\[L_{S}=-_{a=1}^{N},G_{a})}}{_{b=1}^{N}e ^{d(S_{a},G_{b})}},\] (2)

such that by minimising the contrastive loss term for a given batch of proteins, the model is trained to produce aligned embeddings for paired sequence-structure inputs, which are far away from all other embeddings produced by the model. An equivalent method is used at the level of per-residue embeddings across all proteins in the batch. These learned embeddings can then be used for a variety of down-stream tasks; for example, for a given protein with sequence and structure representation,the outputs of the PSM and the PLM can be concatenated and passed through an MLP to predict whether that protein has a given Enzyme Commission number.

In principle, while a separate BioCLIP model can be trained end-to-end at the level of both proteins and their component amino acids, these tasks are inherently related. Each structure or sequence is by definition a composition of its component residue nodes and amino acid types, respectively. There is a vast literature demonstrating that training a single backbone model for multiple, related, tasks typically yields better generalisation performance . It is therefore beneficial to treat both sequence and amino-acid representations as heads of the same underlying sequence encoder, and similarly treat structure and residue representations as heads of the same underlying structure encoder. Then by minimising the sum of contrastive losses, we simultaneously minimise both the protein-level loss and the residue-level loss and benefit from the commonalities in the two problems.

We leverage the available massive pre-trained PLMs which already provide robust representations of the protein sequences, e.g. [45; 46; 18; 47; 48]. By appropriately choosing a pre-trained PLM, we are able to fix the sequence and amino-acid representations and then derive robust structural representations from limited sequence-structure pairs, reflecting the availability of substantially larger datasets of protein sequences. In our experiments, we use an instance of ESM  which is a state-of-the-art BERT-style PLM. The PSM used is a type of SE(3)-invariant graph neural network based on prior work . By design, all node and edge features passed to the network are SE(3)-invariant, and any message passing applied on top of them inherently maintains this. Within the message passing mechanism, we use a type of graph attention network  with multi-head dot-product attention .

BioCLIP is pre-trained on a dataset of approximately 500,000 sequence-structure pairs obtained from the RCSB PDB databank . Once the model has been pre-trained it can then be used for a variety of downstream tasks, such as those described in Section 4. Full details of the implementation used here are given in Appendix A.

## 4 Experiments

### Tasks

To investigate whether BioCLIP is capable of learning meaningful structural representations that offer novel benefits on top of those already available from the underlying PLM, we empirically evaluate their performance when used as a basis for three downstream tasks. These tasks are visualised in Figure 1. Full details of the downstream tasks and their configurations are provided in Appendix B.

* **Function Prediction:** A binary protein classification task, based on datasets used in , where the goal is to predict enzyme-commission numbers and three gene ontology (GO) tasks: biological-process (BP), molecular-function (MF) and cellular component (CC).
* **Protein-Protein Interaction:** A binary classification task where, given two proteins, the objective is to predict whether or not they interact. We study the Human and S.cerevisiae tasks that are introduced in .
* **Per-Residue Protein-Protein Interaction:** A binary classification task where, given two biological sub-units within two distinct biological molecules, the objective is to predict whether or not they interact. This task is taken from .

### Models

Across all tasks we utilise the same BioCLIP pre-trained GNN. During fine-tuning we take the structure representations obtained from the GNN _before_ the application of a final MLP, such that initially, the structure representation differs from the sequence representation by a non-linear transformation. These structure embeddings are then concatenated with sequence embeddings from ESM and passed into an (initially untrained) predictor MLP model. The parameters of the GNN the final three layers of the ESM model and the final MLP are all fine-tuned for each specific task.

AblationsTo ablate BioCLIP's component parts and identify their contributions, we consider four variants of the method in fine-tuning:* **GNN (random)**: The same GNN architecture as used in our BioCLIP pre-trained, but initialised with random weights. This variant is used as a control to measure the effectiveness of pre-training of the GNN. All parameters in the GNN and the final MLP are fine-tuned.
* **GNN (pretrained)**: The GNN architecture, pre-trained to align to the ESM model. This variant is used to measure the effectiveness of pre-training the structure model in isolation. Again, all parameters in the GNN and final MLP are fine-tuned.
* **ESM**: The PLM model used in pre-training, with its final three layers fine-tuned. This variant is used as a control to measure the effectiveness of introducing the BioCLIP-aligned GNN for downstream tasks. All parameters in the last three layers of the ESM model and the final MLP are fine-tuned. Note that we did experiment with different methods of fine-tuning but found that it made little difference to the final performance.

Task-specific BaselinesFor each of the downstream tasks, we further identify a recent method that represents the state-of-the-art, or close, for that task:

* **DeepFRI  (GO-term tasks)**: This model represents the protein structure as contact maps, and employs a frozen protein language model to provide input node features for a three-layer graph convolutional network , which is pre-trained on 10 million Pfam protein sequences and uses an LSTM architecture with 512 hidden units. A sum operation is used to pool the per-residue representations into a single protein representation, which is finally passed through an MLP. The authors expand the training dataset by using homology models from SWISS-MODEL which they show boosts performance significantly, the dataset uses 30k non-redundant experimental PDB structures and 220k non-redundant homology models from SWISS-MODEL.
* **Jha et. al.  (protein-protein tasks)**: The authors use a GCN and a GAT to predict interactions between proteins. They use two pre-trained protein language models: SeqVec (LSTM) and ProtBert (Transformer) to obtain feature vectors for each residue. The SeqVec embedder produces a sequence representation by summing three representations: a 1-character convolution (CharNN), and bi-directional LSTM layers. The second PLM, ProtBert, is a BERT model is trained on the BFD-100 dataset , which has 2.1 million protein sequences. Finally, the per protein representation is obtained by averaging over residue activations. The PPI datasets used are from two organisms: Human and S. cerevisiae. The Pan's human dataset  is modified to remove duplicates and apply some filtering, see the original paper for details .
* **PeSTo  (per-residue protein interaction)**: The authors employ a deep GAT network on the atoms of a protein structure. Latent atom representations are obtained through 32 SO-3 invariant message-passing layers with increasing neighborhood sizes. Finally, cross attention is used from the atoms in each residue, to the corresponding residue.

### Results

The results of the downstream evaluation experiments are summarised in Figure 2, with corresponding exact numerical values found in Appendix C. For each experimental configuration, we present results after 1, 2 and 3 epochs and also the final epoch from each fine-tuning experiment. From our experiments, we draw three important conclusions:

1. **BioCLIP's pre-trained structure representations are informative :** Observing the results for GNN (random) and GNN (pretrained), we find that for all 7 tasks and at every epoch, the pre-trained GNN provides better performance. This result is particularly stark in the Protein-Protein Interaction tasks, where the pre-trained GNN significantly outperforms the randomly initialised GNN in early epochs. From these observations, we conclude that pre-training a GNN via the BioCLIP method can result in meaningful representations of protein structure that can aid in downstream tasks.
2. **Aligned pre-trained structure and sequence representations are additive:** We find that in 25/28 cases, the results for fine-tuning of the full BioCLIP system outperforms the ESM model alone, and in 23/28 cases the pre-trained GNN alone. We argue that, although the GNN and ESM models are aligned, they provide different inductive biases which are mutually beneficial for fine-tuning. We note that in the cases where we did not observe a benefit in combining the representations, the drop in performance was very marginal.

3. **BioCLIP is competitive with state-of-the-art methods:** In 4 of the 7 problems considered, we find that BioCLIP is able to out-perform or match a recent, state-of-the-art, method designed specifically for that task. In the GO: Molecular Function and Enzyme Commission problems, fine-tuning BioCLIP surpasses the results achieved by DeepFRI. Further, on the GO: Cellular Component and GO: Biological Process tasks, BioCLIP's performance reaches within 7% of the performance achieved by DeepFRI. For the two PPI tasks, we find that BioCLIP is able to match the performance of Jha et. al. Additionally, we find that BioCLIP is able to reach very close to the performance of Jha et. al. within only a few epochs. We also note that while BioCLIP struggled to reach the performance of PeSTo on per-residue protein interaction, it did outperform all of the methods that PeSTo was compared to in its original publication 3.

## 5 Conclusion & Future Work

This work introduces BioCLIP, a contrastive learning framework for learning structure and sequence representations of proteins, which we have demonstrated in a practical setting by aligning a GNN to a pre-trained PLM. We have carried out numerous empirical evaluations on a variety of downstream tasks for protein function prediction. We show that the representations derived from BioClip are meaningful, complementary to existing sequence embeddings and can be used to obtain competitive performance in comparison to task-specific methods. Overall, we believe that BioCLIP addresses the problem of limited structural data for pre-training in a systematic way, and provides a general template for models of protein function based on their full sequential and structural representations.

There are a number of areas for future work further developing the ideas presented here. Firstly, recent work  has demonstrated that a CLIP-style model can be effectively trained using a sigmoid loss, rather than a softmax loss, which may pave the way for training of BioCLIP with substantially larger batch sizes. An empirical investigation in this direction may yield improved performance on downstream tasks. We also note that the PSM used in this work was substantially smaller than the PLM, which may limit the richness of the structural embedding obtained. This reflects the broader problem within the geometric deep learning community of over smoothing in deeper networks . Investigation into alternative models of structure, such as the EvoFormer module used in AlphaFold2  and may therefore allow for larger, richer representations of structure.

Figure 2: Performance metrics across 7 downstream tasks for: Random GNN, pre-trained GNN with the contrastive BioCLIP objective, ESM 150M with the last three layers tuned, and the BioCLIP model which combines ESM and the pre-trained GNN. For each task, we also provide a recent, task-specific, benchmark represented as a dashed line.