# AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models

AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models

 Haiquan Lu\({}^{1}\)1, Yefan Zhou\({}^{2}\)2, Shiwei Liu\({}^{3}\), Zhangyang Wang\({}^{4}\), Michael W. Mahoney\({}^{5,6,7}\), Yaoqing Yang\({}^{2}\)

\({}^{1}\)Nankai University, \({}^{2}\)Dartmouth College, \({}^{3}\)University of Oxford

\({}^{4}\)University of Texas at Austin, \({}^{5}\)International Computer Science Institute

\({}^{6}\)Lawrence Berkeley National Laboratory, \({}^{7}\)University of California at Berkeley

###### Abstract

Recent work on pruning large language models (LLMs) has shown that one can eliminate a large number of parameters without compromising performance, making pruning a promising strategy to reduce LLM model size. Existing LLM pruning strategies typically assign uniform pruning ratios across layers, limiting overall pruning ability; and recent work on layerwise pruning of LLMs is often based on heuristics that can easily lead to suboptimal performance. In this paper, we leverage Heavy-Tailed Self-Regularization (HT-SR) Theory, in particular the _shape_ of empirical spectral densities (ESDs) of weight matrices, to design improved layerwise pruning ratios for LLMs. Our analysis reveals a wide variability in how well-trained, and thus relatively low prunable, different layers of an LLM are. Based on this, we propose AlphaPruning, which uses shape metrics to allocate layerwise sparsity ratios in a more theoretically-principled manner. AlphaPruning can be used in conjunction with multiple existing LLM pruning methods. Our empirical results show that AlphaPruning prunes LLaMA-7B to 80% sparsity while maintaining reasonable perplexity, marking a first in the literature on LLMs. We have open-sourced our code.3

## 1 Introduction

Recent work on pruning large language models (LLMs) (Jaiswal et al., 2023; Frantar and Alistarh, 2023; Sun et al., 2023) has shown the ability to reduce the number of parameters significantly, without compromising performance, resulting in notable savings in memory footprint, computing time, and energy consumption. Unlike pre-LLM pruning methods (Sanh et al., 2020; Kurtic et al., 2022), existing LLM pruning approaches typically allocate the "sparsity budget" (i.e., the number of pruned parameters or pruning ratios) uniformly across layers, making it difficult to increase sparsity to very high levels. Relatively little effort has been put into developing theoretically-principled ways to compute layerwise pruning ratios. For example, the Outlier Weighed Layerwise sparsity (OWL) method (Yin et al., 2023) uses a nonuniform layerwise sparsity based on the distribution of outlier activations. However, OWL relies on heuristics related to the presence of outliers (Kovaleva et al., 2021; Puccetti et al., 2022; Dettmers et al., 2022). This can lead to suboptimal performance in the absence of outliers, and this can make it difficult to achieve very aggressive levels of sparsity. For example, Yin et al. (2023) shows that pruning LLMs to 80% sparsity often significantly degrades the prediction performance of LLMs.

In developing a principled approach to allocate sparsity budgets across layers, we draw inspiration from Heavy-Tailed Self-Regularization (HT-SR) Theory (Martin et al., 2021; Martin and Mahoney,2021a,b, 2019, 2017, 2020, Yang et al., 2023, Zhou et al., 2023, Qing et al., 2024]. HT-SR theory analyzes the weight matrices of models to derive quantities (related to the shape of the weight matrix eigenspectrum), which help characterize model capacity and quality. Applications of HT-SR to model selection [Martin and Mahoney, 2019, 2020, 2021a, Martin et al., 2021, Yang et al., 2023] and layer-wise adaptive training [Zhou et al., 2023] demonstrate the effectiveness of the theory in estimating model and layer quality. Furthermore, in the context of pruning memory/computation-efficient LLMs, using this kind of low-cost weight analysis is advantageous because it requires no data or gradient backpropagation.

Our study consists of two main parts. In the first part, we evaluate the effectiveness of various weight matrix-based metrics for allocating layer-wise sparsity. Our primary finding indicates that _shape metrics_ generally outperform _scale metrics_ in determining layer importance for pruning. Shape metrics capture the shape properties of the empirical spectral densities (ESDs) of layer weight matrices, whereas scale metrics, like matrix norms, reflect the size or scale of ESDs. This offers a novel perspective since shape metrics are less frequently used in the literature than scale metrics, which have been used to create regularizers [Yoshida and Miyato, 2017] and inform pruning [Han et al., 2015].

In the second part of the paper, we introduce a theoretically principled layer-wise sparsity allocation method, AlphaPruning, based on metrics that quantify a unique heavy-tailed (HTed) shape of ESDs. According to HT-SR theory, well-trained models display strong correlations among the weight matrix elements, leading to HT structures in layer weight matrices' ESDs [Martin and Mahoney, 2019, 2020, 2021a]. Moreover, layers with more pronounced HT properties are typically better trained than others. We quantify the HT properties by fitting a power law (PL) distribution [Alstott et al., 2014, Clauset et al., 2009] to ESD and using the PL exponent as the HT metric PL_Alpha_Hill.3 The principle of AlphaPruning is to allocate less sparsity to more well-trained (more HTed) layers, as indicated by lower PL_Alpha_Hill values, thereby preserving their quality during pruning. Figure 1 illustrates the pipeline of AlphaPruning.

We conducted a comprehensive empirical evaluation to assess the generalizability of AlphaPruning in LLM pruning. This evaluation involved comparisons with various baseline methods, integration with existing techniques, and testing across different architectures. We also conducted sanity checks to ensure that AlphaPruning indeed reduces the variation of PL_Alpha_Hill values across layers. Our key contributions are summarized below.

* This paper is the first to study principled layer-wise sparsity allocation from the HT-SR perspective. We systematically evaluate multiple weight matrix-based metrics to compute sparsity based on their effectiveness in estimating layer quality, discovering an interesting finding: shape metrics outperform scale metrics in allocating sparsity, despite the latter being more commonly used.

Figure 1: **The pipeline diagram of AlphaPruning. Our post-training layer-wise pruning method involves the following steps: (i) Performing ESD analysis on all weight matrices of a base LLM and (ii) employing PL fitting to derive the layer-wise metric values (that measures the HT exponent). Then, (iii) using the layer-wise metric values, we assign layer-wise pruning ratios to each layer through a linear assignment function.**

* We introduce a novel sparsity allocation method, AlphaPruning, inspired by HT-SR theory, which demonstrates superior performance in LLM pruning. This method assigns sparsities based on the heavy-tailed shape of ESDs in layer weight matrices, a previously unexplored concept. Our empirical evaluations span a range of LLM architectures, including the LLaMA V1-3 families , OPT families , Vicuna-7B , and Mistral-7B . The results show that AlphaPruning outperforms OWL , the current SOTA non-uniform sparsity allocation method for LLMs, reducing perplexity by 304.31 and achieving an average accuracy gain of 4.6% over 7 zero-shot tasks at 80% sparsity, while providing a 3.06\(\) end-to-end speedup on CPUs for LLaMA-7B on the DeepSparse  inference engine. AlphaPruning also outperforms six layer-wise allocation methods, including global thresholding , ER , ER-Plus , LAMP , rank selection , and layer-wise error thresholding .
* AlphaPruning provides layer-wise budget allocation (e.g., sparsity), demonstrating remarkable generalizability and can be integrated with multiple LLM compression techniques to enhance performance. This includes unstructured pruning , SparseGPT ), with or without fine-tuning , semi-structured (DominoSearch ), structured pruning (LLMPruner , OSSCAR ), mixed-precision quantization . Additionally, we have extended this method to large Computer Vision (CV) architectures, such as Vision Transformers (ViT) , and ConvNext . In this case, OWL significantly underperforms AlphaPruning due to the lack of outlier features. This indicates HT metrics are more general than outlier metrics.
* AlphaPruning is theoretically driven, and its improvements can be interpreted by HT-SR metrics. We demonstrate that model performance correlates with model-wise and layer-wise changes in PLAlphaHill before and after pruning. Furthermore, compared to baseline pruning methods, AlphaPruning not only improves model performance but also achieves a lower mean of layer-wise PLAlphaHill. HT-SR theory suggests that AlphaPruning preserves the quality of model layers "on average", minimizing the damage caused by pruning.

We provide an overview of related work in Appendix C.

## 2 Background and Setup

### Notation

Consider a NN with \(L\) layers, \(_{i}\) is one of the weight matrices extracted from the \(i\)-th layer with shape \(m n\) (\(m n\)). We note that the "layer" used in this work refers to the transformer block (layer), and each block contains multiple weight matrices, such as the attention layer weight matrix, and projection layer weight matrix. The correlation matrix \(_{i}=_{i}^{}_{i}\) is an \(n n\) symmetric matrix, and the ESD of \(_{i}\) is formulated as

\[_{_{i}}:=_{j=1}^{n}_{_{j}(_ {i})}\] (1)

where \(_{1}(_{i})_{n}(_{i})\) are the eigenvalues of \(_{i}\) and \(\) is the Dirac delta function. The ESD is a probability measure, which can be viewed as a distribution of the eigenvalues of \(_{i}\).

### HT-SR theory and metrics

Here, we provide a brief overview of HT-SR theory. HR-SR theory originated as a semi-empirical theory, with early seminal work  examining the empirical spectral density (ESD) of weight matrices, specifically the eigenspectrum of the correlation matrix \(_{i}=_{i}^{}_{i}\). This research found that the structures of the ESDs strongly correlate with training quality. These findings are rooted in statistical physics and Random Matrix Theory , as detailed in Table 1 of . It is well-known  that spikes in ESD represent "signals," while the bulk represents noise, which follows the Marchenko-Pastur law. In the theoretical setting of , the signal or the spike aligns with ground-truth features from the teacher model, and that corresponds to increased correlations in weight elements. Furthermore,  show that heavy tails in ESDoriginate from the interaction between spikes and bulk, which can be quantified precisely using recent advances in the free-probability theory (Landau et al., 2023), and the interaction characterizes the "bulk-decay" phase in the five-plus-one phase model in Martin and Mahoney (2019), a critical phase between classical "bulk+spike" model and heavy-tail models.

To quantify the structure of ESDs, HT-SR theory provides several metrics, collectively known as HT-SR metrics. These metrics are typically categorized into two groups: scale metrics and shape metrics.

**Scale metrics.** Scale metrics refer to those obtained from measuring various norms of weight matrices. As demonstrated empirically in Yang et al. (2023), these metrics are often strongly correlated with the generalization gap (which is the gap between training and test performance), instead of the quality of the models. In this paper, we mainly study two scale metrics, Frobenius_Norm and Spectral_Norm. The Frobenius_Norm metric is calculated by the squared Frobenius norm of the weight matrix \(\|\|_{F}^{2}\); and the Spectral_Norm metric can be calculated by the square of the spectral radius of the weight matrix \(\|\|_{2}^{2}\).

**Shape metrics.** Drawing analytic methods from Random Matrix Theory, HT-SR work analyzes poorly trained and well-trained models and concludes that the performance of these models usually correlates with shapes emerging in their ESDs, such as "bulk+spike" shape or "heavy-tailed" shape. The metrics used to characterize these ESD shapes are called shape metrics, and we mainly studied four of them: PL_Alpha_Hill, Alpha_Hat, Stable_Rank, and Entropy. PL_Alpha_Hill is the main metric used in our method, and we define it in Section 3.2. The definitions of other shape metrics (including Alpha_Hat, Stable_Rank, Entropy) can be found in Appendix D.

## 3 Alpha-Pruning

In this section, we first outline the motivation behind AlphaPruning, followed by an introduction to the layer-wise importance metric, PL_Alpha_Hill, and the sparsity allocation function, as shown in Figure 1. Our empirical analysis reveals that shape metrics generally outperform scale metrics in guiding sparsity allocation with the same function. Notably, PL_Alpha_Hill, the shape metric used in AlphaPruning, achieves the best results in preliminary evaluations.

### Rationale

HT-SR theory, introduced in Section 2.2, examines the ESD of weight matrices and finds a strong correlation between heavy-tailed structures in the ESD and training quality. It suggests that heavy-tailed structures emerge from feature learning, where useful correlations are extracted during optimization. Layers with more heavy-tailed ESDs tend to capture more signals, indicating better training quality. Inspired by these findings, we propose to assign sparsity based on the heavy-tailed properties of each layer's ESD. Layers with more heavy-tailed ESDs, which contain more learned signals, are assigned lower sparsity, while layers with light-tailed ESDs are assigned higher sparsity. In practice, the heavy-tailed structure is measured by fitting a PL distribution to the ESD, and extracting the PL exponent \(\) as the indicator. This is why our method is named AlphaPruning.

### Estimating layer quality by HT metric

AlphaPruning relies on estimating the layer quality based on the HT characteristic of the layer ESDs, which is quantified by HT metric PL_Alpha_Hill. Given an ESD \(_{_{i}}\) of a weight matrix's correlation matrix, we fit a PL density function \(p()\) on it, taking values within an interval (\(_{}\), \(_{}\)), formally defined as:

\[p()^{-},_{}<<_{ }.\] (2)

The estimated exponent \(\) is then used as a metric to characterize the HT extent of the ESD, with a lower value means more HTed. We estimate the PL coefficient using the Hill estimator (Hill, 1975; Xiao et al., 2023; Zhou et al., 2023), and we refer to it as the PL_Alpha_Hill metric. The Hill estimator is defined as:

\[=1+^{k}}{ _{n-k}})},\] (3)

where \(\{_{i}\}_{i=1}^{n}\) is sorted in ascending order, and \(k\) is a tunable parameter that adjusts the lower eigenvalue threshold \(_{}\) for (truncated) PL estimation. We adopt the Fix-finger method (Yanget al., 2023) to select the \(k\), which sets \(k\) such that \(_{}\) aligns with the peak of the ESD. Note that PL_Alpha_Hill and other scale/shape metrics are calculated for each weight matrix individually.

### Allocating sparsity based on the layer quality

AlphaPruning allocates sparsity for each layer (transformer block) by using a mapping function \(:^{L}^{L}\) to map a sequence of layer quality measures \(=(q_{1},q_{2},..,q_{L})\) into corresponding sparsities \(()\).

\[()_{i}=[-q_{}}{q_{}-q_{ }}(s_{2}-s_{1})+s_{1}].\] (4)

Here, \(()_{i}\) represents the \(i\)-th element of the resulting vector \(()\), \(q_{i}\) represents the \(i\)-th element of the input vector \(\), and \(q_{}\), \(q_{}\) represent the minimum and maximum values of \(\). The normalization factor \(\) adjusts the sparsity levels to achieve the target global sparsity \(S\). Each layer's sparsity is normalized within the interval \([ s_{1}, s_{2}]\). \(\) is calculated using the equation \(_{i=1}^{L}()_{i}d_{i}=S_{i=1}^{L}d_{i}\), in which \(d_{i}\) is the number of parameters of \(_{i}\). Both sides of the equation represent the total number of remaining parameters. The \((s_{1},s_{2})\) are tunable hyperparameters that adjust the non-uniformity of the sparsity distribution. We note that sparsity allocation is executed on a per-block basis, averaging the HT-SR metric across all matrices within a block to determine \(q_{i}\). This design is supported by an ablation study, presented in Appendix F, which shows that it yields superior performance over a per-matrix allocation. Hyperparameter settings for all experiments are provided in Appendix G.

### Shape vs. scale metrics for sparsity allocation

In this study, we evaluated various HT-SR metrics, as defined in Section 2.2, to assess their effectiveness in estimating layer quality \(\) for computing layer-wise sparsity. Our preliminary experiments involved pruning the LLaMA-7B model to 70% sparsity and assessing its performance through WikiText perplexity and accuracy across seven zero-shot tasks, with results presented in Table 1. We applied each HT-SR metric in conjunction with three intra-layer pruning techniques (which only determine which matrix elements to prune), such as Magnitude, Wanda, and SparseGPT, to thoroughly evaluate their efficacy. Further experiments on Vision Transformers (ViT) are described in Appendix I.1. Across all tests, shape metrics consistently outperformed scale metrics in assigning layer-wise sparsities. This finding suggests that shape metrics are more robust and yield more reliable predictions of layer quality, which extends previous research (Yang et al., 2023; Zhou et al., 2023; Martin et al., 2021) on estimating model quality. Notably, the shape metric PL_Alpha_Hill that focuses on estimating the HT shape, proved to be the most effective. Consequently, we have adopted PL_Alpha_Hill as the primary metric in our proposed method, AlphaPruning.

   Metric used for &  &  \\ layerwise pruning ratios & Magnitude & Wanda & SparseGPT & Magnitude & Wanda & SparseGPT \\  Uniform & 48419.13 & 85.77 & 26.30 & 32.30 & 36.73 & 41.52 \\  Frobenius\_Norm & 30136.37 & 59.82 & 24.95 & 33.23 & 37.62 & 43.67 \\ Spectral\_Norm & 48073.99 & 246.84 & 29.01 & 32.81 & 33.15 & 41.14 \\  Entropy & 3716.07 & 41.15 & 22.02 & 33.58 & 39.39 & 43.53 \\ Stable\_Rank & 851.65 & 41.24 & 24.91 & 34.91 & 39.74 & 42.97 \\ Alpha\_Hat & 1256.02 & 27.60 & 20.02 & 33.48 & 43.57 & 43.83 \\ PL\_Alpha\_Hill & **231.01** & **23.86** & **18.54** & **35.67** & **44.42** & **45.48** \\   

Table 1: **Evaluating shape metrics versus scale metrics on allocating layerwise sparsities on LLMs.**_Shape metrics_ are obtained from the shapes of the ESDs. _Scale metrics_ are norm-based metrics measuring the scale of weights matrices (which can also be obtained from the ESD). The results are conducted on LLaMA-7B at 70% sparsity. We show WikiText validation perplexity and average accuracy on seven different zero-shot tasks as evaluation metrics. We observe that shape metrics outperform scale metrics and PL_Alpha_Hill performs the best.

Empirical results

In this section, we evaluate the performance, generalizability, and interpretability of AlphaPruning. Section 4.1 outlines our experimental setup. In Section 4.2, we evaluate AlphaPruning's performance by comparing it to the SOTA method OWL and five other baseline methods, and we analyze the efficiency of LLMs pruned by AlphaPruning using practical metrics such as FLOPs and latency. Section 4.3 evaluates the generalizability of AlphaPruning by integrating it with various LLM compression techniques including post-pruning fine-tuning, semi-structured pruning, structured pruning, and mixed-precision quantization. Additionally, we extend its application to CV tasks. Section 4.4 provides an analysis of the layer-wise sparsities and the PL_Alpha_Hill distribution to further elucidate the effectiveness and implications of AlphaPruning.

### Experimental setup

**Models and Evaluation.** We evaluate AlphaPruning on the three most widely adopted LLM model families: LLaMA 7B/13B/30B/65B [Touvron et al., 2023a], LLaMA-2 7B/13B/70B [Touvron et al., 2023b], OPT 125M/350M/2.7B/6.7B, and other advanced LLMs: LLaMA-3-8B, Vicuna-7B, Mistral-7B. Our evaluation protocol aligns with established methodologies for LLM pruning [Xiao et al., 2023a], including assessments of language modeling proficiency and zero-shot capabilities. Specifically, we evaluate the perplexity on the held-out WikiText [Merity et al., 2016] validation set, and use seven tasks, including BoolQ [Clark et al., 2019], RTE [Wang et al., 2018], HellaSwag [Zellers et al., 2019], WinoGrande [Sakaguchi et al., 2021], ARC Easy and Challenge [Clark et al., 2018] and OpenbookQA [Mihaylov et al., 2018] for downstream zero-shot evaluation [Gao et al., 2023].

**Baselines.** We apply the layer-wise sparsities determined by AlphaPruning to three LLM pruning methods, including Magnitude [Han et al., 2015], SparseGPT [Frantar and Alistarh, 2023b] and Wanda [Sun et al., 2023]. Magnitude-based pruning is a simple and strong baseline in which weights are discarded based on their magnitudes. Wanda and SparseGPT are two strong LLM pruning baselines due to their capability to sustain reasonable performance even at relatively high sparsity levels (around 50%). All these methods originally used uniform layerwise sparsity. We incorporate AlphaPruning directly into these baselines, and we demonstrate that this results in improved performance. Besides, we also compare AlphaPruning with OWL [Yin et al., 2023], a recently proposed non-uniform LLM pruning method and six layer-wise pruning methods, including global thresholding [Frankle and Carbin, 2018], ER [Mocanu et al., 2018], ER-Plus [Liu et al., 2022a], LAMP [Lee et al., 2020], rank selection [Kuzmin et al., 2019, El Halabi et al., 2022], and layer-wise error thresholding [Ye et al., 2020, Zhuang et al., 2018].

### Main results

**Language Modeling.** In Table 2, we report the perplexity of the pruned LLaMA and LLaMA-2 models at 70% sparsity. We provide results for more sparsity levels in Figure 2 and Appendix H. AlphaPruning, as a general layerwise sparsity method, consistently demonstrates performance improvements when used in conjunction with various pruning methods. For example, in the case of LLaMA-7B with a sparsity of 70%, AlphaPruning produces sparse networks with a perplexity of 231.01, significantly outperforming the Magnitude-based pruning baseline of 48419.13. Notably, when applied to Wanda and SparseGPT, two robust LLM pruning methods, AlphaPruning still achieves substantial perplexity reductions, evidenced by a decrease of 61.91 for Wanda and 7.76 for SparseGPT, in the case of LLaMA-7B with a sparsity of 70%.

**Zero-shot tasks.** We conducted empirical evaluations to determine the zero-shot ability of pruned LLMs on diverse zero-shot downstream tasks with prompting. The results are shown in Table 3, where we show the mean zero-shot accuracy on 7 zero-shot tasks of pruned LLaMA and LLaMA-2 models at sparsity of 70%. AlphaPruning consistently improves accuracy across all settings. For example, AlphaPruning achieves an average accuracy gain of 8.79, 6.05, and 2.61 over 7 tasks and 7 models compared to Magnitude, Wanda, and SparseGPT alone, respectively. These results highlight the promise of AlphaPruning for more challenging zero-shot downstream tasks.

**More baseline comparison.** For allocating layerwise sparsity ratios, we compare AlphaPruning with other allocation methods. The experiments involve pruning the LLaMA-7B model and LLaMA-13B to various sparsities. We use Wanda as the basic pruning method, with results presented in Figure 2 and Figure 3. Results indicate that AlphaPruning significantly outperforms all baselinemethods in relatively high-sparsity regimes. For LLaMA-7B and LLaMA-13B pruned to 80% sparsity, AlphaPruning reduces perplexity by 304.31 and 200.54 compared to OWL, respectively. Achieving high levels of sparsity is crucial for unstructured sparsity to yield significant speedups on GPUs by leveraging existing sparse kernels. Sparse kernels such as Flash-LLM [Xia et al., 2023] and Sputnik [Gale et al., 2020] have shown that unstructured sparsity outperforms dense computation in terms of performance, but only when sparsity levels reach 60% and 71%, respectively. The importance of achieving high sparsity is further substantiated in the following section, which demonstrates that high sparsity levels facilitate significant end-to-end inference speedup. Additional comparisons with rank selection and layer-wise error thresholding, as well as results using SparseGPT as the pruning method and low sparsity results, can be found in Appendices H.

    &  &  &  \\  & & 7B & 13B & 30B & 65B & 7B & 13B & 70B \\  Dense model & - & 5.68 & 5.09 & 4.77 & 3.56 & 5.12 & 4.57 & 3.12 \\   & Uniform & 48419.13 & 84527.45 & 977.76 & 46.91 & 499111.45 & 214.04 & 1481.95 \\  & OWL & 19527.58 & 11464.69 & 242.57 & **15.16** & 59176.42 & 57.55 & 17.18 \\  & Ours & **231.01** & **2029.20** & **62.39** & 16.01 & **8900.32** & **31.89** & **15.27** \\   & Uniform & 85.77 & 54.03 & 17.35 & 15.17 & 74.26 & 45.36 & 10.56 \\  & OWL & 24.57 & 17.17 & 10.75 & 8.61 & 30.38 & 20.70 & 8.52 \\  & Ours & **23.86** & **14.21** & **9.68** & **7.86** & **28.87** & **14.16** & **7.83** \\   & Uniform & 26.30 & 18.85 & 12.95 & 10.14 & 27.65 & 19.77 & 9.28 \\  & OWL & 19.49 & 14.55 & 10.28 & 8.28 & 20.40 & 15.27 & 7.65 \\   & Ours & **18.54** & **12.81** & **9.77** & **7.83** & **19.34** & **12.20** & **7.57** \\   

Table 2: WikiText validation perplexity for pruned LLaMA and LLaMA-2 models at 70% sparsity. Our method (AlphaPruning) is compared to uniform layerwise sparsity and OWL, each paired with magnitude-based pruning, Wanda, and SparseGPT. Lower perplexity indicates improved model performance.

Efficiency measure.To verify the sparse LLM pruned by our method can indeed achieve speedups when deployed on the CPU, we provide new results in Table 4. We apply our method to Llama2-7B-Chat-hf, prune it to different sparsities, and then test its end-to-end decode latency using DeepSparse (Kurtic et al., 2023) inference engine on an Intel Xeon Gold 6126 CPU with 24 cores. The results indicate that when the global sparsity reaches 80%, the speedup reaches 3.06\(\).

In Appendix I.2, we evaluate the pruned LLM by efficiency metrics other than sparsity, such as FLOPs, Compared with uniform sparsity ratios, our approach is able to achieve better performance-FLOPs trade-off. In Appendix I.3, we show that AlphaPruning can control the minimum layer sparsity without losing the performance advantage to meet the hardware requirements of a memory-limited device. In Appendix I.4, we report the runtime of AlphaPruning to show that the computational overhead is reasonable. The computational complexity is not large because the most computation-intensive aspect of our method involves performing SVD decomposition on weight matrices.

### Corroborating results

To demonstrate the generalizability of AlphaPruning, we first evaluate if the performance of the model pruned by AlphaPruning can be well recovered by fine-tuning. Then we apply AlphaPruning to other LLM compression techniques (semi-structured, structured pruning, and quantization), and CV models.

Fine-tuning.We show the performance of LLMs pruned by AlphaPruning can be well recovered by fine-tuning. We investigate the parameter-efficient strategies for fine-tuning LLMs: LoRA fine-tuning (Hu et al., 2021). Fine-tuning is conducted on the C4 training set (Raffel et al., 2020) with the pre-training auto-regressive loss. The pruned mask is fixed during fine-tuning. The low-rank (\(r\) = 8) adapter is applied to the query and value projection matrices in the attention layers. We fine-tune LLaMA-7B pruned by SparseGPT at various sparsities. Table 6 summarizes the results for perplexity and mean zero-shot accuracy after fine-tuning pruned LLaMA-7B models. We can see the performance of pruned LLMs can be notably improved with very light LoRA fine-tuning. In Appendix I.8, we further compare AlphaPruning with baselines. The results show that the advantages of AlphaPruning don't diminish after fine-tuning.

More LLM architectures.To demonstrate that the effectiveness of AlphaPruning is robust across various more advanced LLMs, we also apply AlphaPruning to LLaMA-3-7B, Vicuna-7B (Zheng et al., 2024), and Mistral-7B (Jiang et al., 2023). The results in Table 5 show that, as a general method, AlphaPruning can consistently achieve performance improvement across different architectures. Results for OPT families can be found in Appendix I.5.

   Method & Sparsity & Samples & Perplexity (\(\)) & Accuracy (\(\)) \\  – & Dense & – & 5.68 & 60.08 \\  Uniform & 50\% & 20K & 6.86 & 54.85 \\ Ours & 50\% & 20K & **6.79** & **55.58** \\  Uniform & 60\% & 20K & 8.32 & 51.81 \\ Ours & 60\% & 20K & **8.23** & **53.39** \\  Uniform & 70\% & 30K & 11.21 & 49.00 \\ Ours & 70\% & 30K & **10.95** & **49.51** \\  Uniform & 80\% & 100K & 20.11 & 39.32 \\ Ours & 80\% & 100K & **18.53** & **40.02** \\   

Table 6: WikiText validation perplexity and zero-shot tasks accuracy of SparseGPT pruned LLaMA-7B at various sparsities after LoRA fine-tuning on C4 dataset samples.

   Sparsity & Dense & 10\% & 20\% & 30\% & 40\% & 50\% & 60\% & 70\% & 80\% & 90\% \\  Latency (ms) & 307.46 & 306.27 & 304.87 & 293.26 & 264.41 & 177.55 & 148.16 & 133.76 & 100.35 & 81.40 \\ Throughput (tokens/sec) & 3.25 & 3.26 & 3.28 & 3.41 & 3.78 & 5.63 & 6.74 & 7.47 & 9.96 & 12.28 \\ Speedup & 1.00x & 1.00x & 1.01x & 1.05x & 1.16x & 1.73x & 2.07x & 2.30x & 3.06x & 3.78x \\   

Table 4: End-to-end decode latency and speedup of AlphaPruning measured on the DeepSparse inference engine.

   Method & Model & 60\% & 70\% & 80\% \\  Uniform & LLaMA-V3-7B & 23.35 & 126.57 & 880.57 \\ Ours & LLaMA-V3-7B & **18.91** & **101.73** & **560.22** \\  Uniform & Vicuna-7B & 12.64 & 110.05 & 6667.68 \\ Ours & Vicuna-7B & **11.14** & **35.87** & **1081.76** \\  Uniform & Mistral-7B & 11.47 & 60.08 & 353.62 \\ Ours & Mistral-7B & **10.49** & **39.39** & **201.62** \\   

Table 5: WikiText validation perplexity (\(\)) of more LLMs pruned by uniform sparsity and our method combined with Wanda.

Integration with other compression techniques.To demonstrate the generalizability of our non-uniform layerwise sparsity, we integrated AlphaPruning with three prominent LLM compression methods: N:M sparsity, structured pruning, and quantization. In Appendix I.6, we examine a mixed N:8 sparsity setup using DominoSearch (Sun et al., 2021) as well as combine AlphaPruning with structured pruning methods LLM-Pruner (Ma et al., 2023) and OSSCAR (Meng et al., 2024). In Appendix I.7, we merge our method with mixed-precision quantization as (Tang et al., 2022). Across these configurations, AlphaPruning consistently enhances the performance of baseline methods.

Vision models.To illustrate AlphaPruning in a broader context, we study how it performs against other methods for determining layerwise sparsity on CV tasks, where non-uniform layerwise sparsity has been widely used. We consider two modern CV architectures: ViT (Dosovitskiy et al., 2020) and ConvNext (Liu et al., 2022). We adopt Wanda as the pruning approach and compare AlphaPruning with uniform layerwise sparsity and OWL. We present the results on ConvNext in Figure 4 and provide more results on DeiT and ViT models in Appendix I.9. These results affirm that AlphaPruning effectively allocates layerwise sparsities in CV tasks as well, where OWL significantly underperforms AlphaPruning due to the lack of outlier features. This indicates HT metrics are more general than outlier metrics.

### Analyzing LLM pruning via HT-SR perspective

We study how the PL_Alpha_Hill metric as a measure of model quality (based on HT-SR theory) changes before and after pruning. In particular, we show that the proposed method, AlphaPruning, effectively controls the damage of pruning to model quality, resulting in a more favorable distribution (lower mean) of PL_Alpha_Hill among the model layers compared to the baseline pruning method.

Analyzing PL_Alpha_Hill affected by pruning.We investigate how PL_Alpha_Hill values change before and after pruning. According to HT-SR Theory (Yang et al., 2023, Martin et al., 2021), models or layers of higher quality typically exhibit lower PL_Alpha_Hill values. As observed in Figure 4(a), dense LLaMA models (depicted in gray) consistently show a lower mean PL_Alpha_Hill, with larger models demonstrating an even lower mean. Furthermore, we can see both pruning methods lead to increments of the metric value, as pruning is often seen as damaging the quality of the model, the changes are well correlated with the perplexity. More interestingly, AlphaPruning not only outperforms the Uniform pruning in perplexity, but it also leads to a smaller mean of PL_Alpha_Hill. Figure 4(b) delves deeper into this phenomenon by visualizing the metric value in a layer-wise manner. It is noticeable that AlphaPruning leads to lower PL_Alpha_Hill than the uniform pruning among the first several blocks. This is due to the mechanism (Figure 8) by which our method prunes the model based on the layer-wise PL_Alpha_Hill, and prunes less on these more heavy-tailed layers.

Based on these results, we can conclude that: (1) pruning a model to a larger sparsity generally hurts the model's task performance (e.g., perplexity), and this coincides with decreased model quality, as measured by PL_Alpha_Hill; and (2) a better pruning method such as AlphaPruning can obtain a sparse model with a smaller mean of layer-wise PL_Alpha_Hill, and this is achieved by pruning less aggressively in the dense layers with lower PL_Alpha_Hill layers.

Here we provide more analytical results in Appendix E. We show analyses on other LLMs provided in Appendix E.1. We compare our HT-SR metrics with other layer quality metrics in Appendix E.2. Additionally, in Appendix E.3, we investigate the distribution of layerwise sparsities allocated by the heavy-tailed metric PL_Alpha_Hill. In Appendix E.4, we also examine the apparent connection between our method, AlphaPruning, and Low-Rank Approximation (LRA) (Zhang et al., 2015; Wen et al., 2017; Xu et al., 2019; Barsbey et al., 2021) from two perspectives. First, we study the relationship between the ESD used in our method and the low-rank properties often used in LRA, where we use PL_Alpha_Hill to measure HT and Stable_Rank to measure low-rank properties. Second, we study the differing layer-wise assignment strategies adopted by the two methods. Finally, we discuss

Figure 4: ImageNet-1K accuracy (\(\)) of the sparse ConvNext model pruned to various sparsity levels by AlphaPruning and other baseline methods, without fine-tuning.

how our findings relate to those of Barshey et al. (2021), a paper closely related to ours, showing that the results from both studies complement each other, offering different yet compatible insights.

## 5 Conclusion

We have used methods from HT-SR Theory to develop improved methods for pruning LLMs. The basic idea is to analyze the ESDs of trained weight matrices and to use shape metrics from these ESDs to measure how much to prune a given layer, with less well-trained layers, as measured by these shape metrics from HT-SR Theory, being pruned more aggressively. Our extensive empirical evaluation demonstrates that AlphaPruning offers a straightforward yet effective way of determining the layer-wise sparsity ratios. Our analysis reveals that different layers of an LLM are not equally trained (typically, the ESDs of early layers are more HT and thus are more well-trained, compared to later layers), and that shape-based ESD metrics work better for layer quality prediction in pruning pipelines than scale-based ESD metrics. AlphaPruning achieves higher sparsity, without severely hurting performance, and also smaller values of PL_Alpha_Hill after pruning. AlphaPruning is also compatible with multiple existing LLM pruning methods and is expected to be integrated with future ones, as long as the methods allow specifying layerwise ratios.