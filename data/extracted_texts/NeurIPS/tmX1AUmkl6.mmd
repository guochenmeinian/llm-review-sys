# Evaluation of Text-to-Video Generation Models: A Dynamics Perspective

Mingxiang Liao\({}^{1}\)1  Hannan Lu\({}^{2}\)1  Xinyu Zhang\({}^{3,4}\)1  Fang Wan\({}^{1}\)2  Tianyu Wang\({}^{1}\)

**Yuzhong Zhao\({}^{1}\)**  Wangmeng Zuo\({}^{2}\)**  Qixiang Ye\({}^{1}\)**  Jingdong Wang\({}^{4}\)

\({}^{1}\)University of Chinese Academy of Sciences \({}^{2}\)Harbin Institute of Technology

\({}^{3}\)The University of Adelaide \({}^{4}\)Baidu Inc.

{liaomingxiang20, wangtianyu21, zhaoyuzhong20}@mails.ucas.ac.cn

{wanfang, qxye}@ucas.ac.cn, {luhannan, wmzuo}@hit.edu.cn

xinyu.zhang02@adelaide.edu.au, wangjingdong@baidu.com

 Equal contribution. \(\) Corresponding Author.

###### Abstract

Comprehensive and constructive evaluation protocols play an important role when developing sophisticated text-to-video (T2V) generation models. Existing evaluation protocols primarily focus on temporal consistency and content continuity, yet largely ignore dynamics of video content. Such dynamics is an essential dimension measuring the visual vividness and the honesty of video content to text prompts. In this study, we propose an effective evaluation protocol, termed DEVIL, which centers on the dynamics dimension to evaluate T2V generation models, as well as improving existing evaluation metrics. In practice, we define a set of dynamics scores corresponding to multiple temporal granularities, and a new benchmark of text prompts under multiple dynamics grades. Upon the text prompt benchmark, we assess the generation capacity of T2V models, characterized by metrics of dynamics ranges and T2V alignment. Moreover, we analyze the relevance of existing metrics to dynamics metrics, improving them from the perspective of dynamics. Experiments show that DEVIL evaluation metrics enjoy up to about 90% consistency with human ratings, demonstrating the potential to advance T2V generation models. Project page: t2veval.github.io/DEVIL/.

## 1 Introduction

With the rapid progress of video generation technology, the demand of comprehensively evaluating model performance continues to grow. Recent benchmarks [27; 24] have included various metrics, \(e.g.\), generation quality, video-text alignment degree, and video content continuity, to evaluate T2V generation models. Despite of the great efforts made, an essential characteristic of video, \(i.e.\), _dynamics_, remains ignored.

Dynamics is a crucial dimension when evaluating video generation models for the following two reasons: \((i)\) In practical applications, it is expected that generated video content is honest to text prompts, \(e.g.\), dramatic text prompts result in videos of high dynamics. \((ii)\) In real-world scenarios, dynamics are negatively relevant to commonly used evaluation metrics, as observed by recent benchmark studies [24; 27]. This allows T2V models to 'cheat' by generating low-dynamic video content in many cases to achieve high scores upon these metrics.

In this study, we introduce DEVIL, a comprehensive evaluation protocol, which assesses T2V generation models from a perspective of dynamics. DEVIL treats _dynamics_ as a primary dimension for T2V model evaluation, as well as enhancing the completeness of existing metrics. To fulfillthis purpose, we first establish a benchmark incorporating text prompts under multiple dynamics grades. The text prompts are collected from commonly used datasets [7; 6; 46; 41] and categorized using a Large Language Model (LLM), GPT-4 , with further manual refinement. We then define a set of dynamics scores, which are aggregated into two dynamics metrics to reveal the temporal characteristics of generated videos. In addition, we conduct a user study to synthesize the proposed metrics into an overall dynamics score, facilitating a comprehensive evaluation of dynamics capacity, characterized by dynamics ranges and T2V alignment.

To enhance the completeness of existing evaluation metrics, we introduce a bi-variate analysis strategy. Specifically, we use dynamics as an additional dimension to examine the distribution of existing metrics, such as aesthetics, continuity, and consistency. Through bi-variate analysis, we identify those metrics negatively related to dynamics, and update them by incorporating the dynamics factor. We also introduce a metric to evaluate the naturalness based on a multimodal large language model (MLLM), \(e.g.\), Gemini-1.5 Pro .

With the proposed DEVIL protocol, we evaluate state-of-the-art T2V models and commonly used benchmarks and find the following problems. (_i_) Existing generation models typically generate slow-motion videos, as most videos in existing benchmarks are of low dynamics. (_ii_)The text prompts in commonly used T2V benchmarks can not reflect the degrees of video dynamics. If such prompts are used to generate videos, they cause poor T2V alignment \(w.r.t.\) dynamics. (_iii_) By experiments, we observe that the naturalness of generated video decreases with video dynamics, which implies that the capability of simulating real-world scenarios remains to be elaborated.

The contributions of this study are summarized as follows,

* We propose a novel evaluation protocol, termed DEVIL, which benchmarks T2V generation models by integrating dynamics metrics. Together with existing evaluation metrics, DEVIL builds a more comprehensive evaluation protocol.
* We establish a text prompt benchmark \(w.r.t.\) dynamics grades and propose a set of metrics to evaluate video dynamics across temporal granularities, facilitating the assessment of dynamics range and T2V alignment.
* Extensive evaluation of existing T2V generation models allows us to thoroughly analyze the capabilities of T2V models through the proposed protocol and benchmarks. The results would inspire sophisticated T2V generation models.

## 2 Related Work

### Text-to-Video Generation Model

As a recent breakthrough in artificial intelligence, diffusion models have pushed video generation technology to a new height. Earlier studies [22; 21] explored the 3D U-Net and cascaded models for

Figure 1: Evaluation of video dynamics. (a) Illustration of dynamics at multiple temporal granularities. (b) Video quality distribution \(w.r.t.\) dynamic scores. (Best viewed in color)

diffusion within pixel space. Recent solutions [13; 33] employed latent diffusion models to efficiently manage the diffusion process within a compressed latent space. Following these studies, a variety of approaches [40; 10; 26; 43; 16; 42; 47; 29; 25] updated and improved this paradigm. Building on these advancements, subsequent methods further explored generating videos of higher quality and extended duration. The Videocrafter approach  pursued high-quality video generation through disentangling spatial and temporal learning and tuning spatial modules using high-quality images. In a similar way, commercial models such as Pika  and GEN-2  demonstrated substantial improvements, showcasing videos with exceptional visual clarity. For longer video generation, Gen-L-Video  aggregated short clips generated by base T2V models using temporal co-denoising to enhance continuity. Freenoise  extended pre-trained T2V models through rescheduling noise for longer-duration video inference. StreamingT2V  enhanced long-term content consistency by integrating short-term and long-term memory blocks.

The rapid development of T2V models poses a growing demand for quality evaluation protocols. Unfortunately, existing protocols primarily focus on temporal consistency and content continuity, yet largely ignore temporal dynamics. This hinders the exploitation of video content vividness and the honesty of video content to text prompts.

### Evaluation Protocol

Early evaluation protocols  primarily relied on class labels to T2V models. For example, they commonly used video clips from the UCF-101 dataset and human-annotated video captions from the MSR-VTT  dataset as the evaluation data. For a more specific assessment, FETV  assigned fine-grained category labels to prompts and calculated the CLIP-SIM score for each category.

However, conventional quality assessment metrics such as Inception Score (IS) , Frechet Inception Distance (FID) , Frechet Video Distance (FVD) , and CLIP-SIM typically operate on a single dimension while can not provide a comprehensive evaluation. When addressing the limitation, EvalCrafter  expanded both the prompt scale and the number of evaluation metrics so that the text-video alignment degree and the quality of generated videos can be better evaluated. Additionally, VBench  proposed a multi-dimensional, multi-category evaluation suite that not only considered the diversity of prompts but also encompassed a variety of assessment metrics.

Despite of the evolution of evaluation metrics, we argue an essential characteristic of video, \(i.e.\), dynamics, remains ignored. In this study, we introduce the dynamics dimension to evaluate T2V generation models, as well as enhance the completeness of existing metrics.

## 3 Dynamics Evaluation Protocol

As shown in Fig. 2, we first establish a benchmark incorporating text prompts under multiple dynamics grades. The text prompts are collected from commonly used datasets [7; 6; 46; 41] and categorized to dynamics grades using GPT-4  and human refinement. We then define a set of dynamics

Figure 2: Flowchart to calculate dynamics metrics based on dynamics scores and text prompts.

scores corresponding to multiple temporal granularities, to reveal the video characteristics at multiple temporal levels. We finally conduct a user study to aggregate the proposed dynamics into overall dynamics metrics about ranges of dynamics and T2V alignment.

### Text Prompt Benchmark

Building the benchmark includes a coarse categorization step and a post-processing step. In the coarse categorization step, the GPT-4 model is used to categorize approximately 50k text prompts collected from existing benchmarks into five grades. These benchmarks include VidProm , WebVid , MSR-VTT , Didemo , etc. The five dynamics grades are summarized as follows:

**Static video**: Video content is nearly stationary. _Example_: A man is laying on the ground.

**Low dynamics**: Video content has slow and slight changes. _Example_: A male fencer adjusts his epee mask and prepares to duel with his sparring partner in slow motion.

**Medium dynamics**: Noticeable activity and changes, but relatively smooth overall. _Example_: Tilt up of shirtless sportsman doing pull-ups on bars during cross-training workout at gym.

**High dynamics**: Fast actions and changes. _Example_: A runner explodes out of the starting blocks, racing down the track.

**Very high dynamics**: Extremely rapid and frequent video content changes. _Example_: A medieval siege with catapults launching, walls breaking, soldiers charging, and arrows raining down.

In the post-processing step, we recruit six human annotators to refine the dynamics grades following same criterion. Finally, we sample 800 text prompts at different dynamics grades for a uniform distribution along the grades.

Fig. 3 shows the statistics of the DEVIL benchmark, which contains approximately 800 text prompts, and each dynamics grade includes 19 object categories and 4 scene categories. Unless otherwise specified, all experiments in this paper are conducted on the DEVIL benchmark.

### Dynamics Scores

As illustrated in Fig. 4, video dynamics can be classified into three categories based on their temporal granularities: **(i) Inter-frame dynamics**, which describes the variations between successive frames. The dynamics score at this level reflects rapid and prominent content variations. **(ii) Inter-segment dynamics**, which refers to the changes between video segments which contains \(K\) video frames.

   Granularity & Dynamics scores & Formulation \\   & Optical Flow Strength & \(_{ofs}=_{i=1}^{N-1}(f_{i})\) \\   & Structural Dynamics Score & \(_{sd}=1-_{i=1}^{N-1}(f_{i},f_{i+1})\) \\   & Perceptual Dynamics Score & \(_{pd}=_{i=1}^{N-1}(f_{i},f_{i+1})\) \\   & Patch-level Aperiodicity & \(_{pa}=1-_{h,w}(\{F_{i,h,w}\}_{i=1}^{N})\) \\   & Global Aperiodicity & \(_{ga}=1-_{j i}^{[rN]}(F_{i}^{r},F _{j}^{r})\) \\   & Temporal Entropy & \(_{te}=(f_{1},f_{2},,f_{N}|f_{1})\) \\   & Temporal Semantic Diversity & \(_{tsd}=_{i=1}^{N}\|F_{i}-\|^{2}\) \\   

Table 1: Formulations of dynamics scores at different temporal granularities.

Figure 3: (a) Distribution of dynamics grades for text prompts from DEVIL, Vbench , and EvalCrafter . (b) Word cloud of the text prompt benchmark of DEVIL.

Define on a middle-level, this score captures middle-speed transitions and motion patterns. **(iii) Video-level dynamics**, which encompasses the overall content diversity and the frequency of changes throughout the video.

**(i) Inter-frame Dynamics Score.** This is further categorized to _optical flow strength_, _structural dynamics_ and _perceptual dynamics_.

_Optical flow strength._ We first employ RAFT  to estimate the optical flow  for each video frame. The mean optical flow magnitudes of each frame are averaged to calculate the optical flow strength of this frame. Averaging the optical flow strength values of all video frames, we have the optical flow strength \(_{ofs}\) of the video, as

\[_{ofs}=_{i=1}^{N-1}(f_{i}),\] (1)

where FLOW calculate the mean optical flow strength values of frame \(f_{i}\).

_Structural dynamics score._ While optical flow excels in capturing motion, it is less effective when detecting structural dynamics such as lighting conditions. To capture such information, we calculate the average structural similarity index (SSIM) between consecutive frames from all frame pairs to quantify inter-frame structural variations of the video, as

\[_{sd}=1-_{i=1}^{N-1}(f_{i},f_{i+1})\}_{i =1}^{N-1},\] (2)

_Perceptual dynamics._ The human visual system is sensitive to changes in low-frequency regions of video frames. To reflect this characteristic, we introduce a perceptual dynamics metric that measures the difference between the perceptual hashes  of consecutive frames. The perceptual distance \(D_{pa}\) is defined as the mean of all frame pairs, as

\[_{pd}=_{i=1}^{N-1}(f_{i},f_{i+1})\}_{ i=1}^{N-1},\] (3)

where PHASHD denotes the Hamming distance  between the perceptual hash of \(f_{i}\) and \(f_{i+1}\).

**(ii) Inter-segment Dynamics Score.** This is further categorized into _patch-level aperiodicity_ and _global aperiodicity_, which measure the dynamics between video segments.

_Patch-level aperiodicity_. We first calculate inter-segment dynamics at the patch level using the auto-correlation factor , to evaluate the scene and temporal pattern dynamics. The auto-correlation factor measures the feature similarity of a time series, revealing periodicity and changing trends of features. Given features at position \((h,w)\) across \(N\) frames, \(\{F_{i,h,w}\}_{i=1}^{N}\), the auto-correlation factor of the features is defined as

\[(\{F_{i,h,w}\}_{i=1}^{N})=}_{k=K_{0}}^{N} _{i=1}^{k}(F_{i,h,w},F_{N-k+i,h,w}),\] (4)

where **SIM** represents the cosine similarity between two feature vectors. The minimal segment length \(K_{0}\) is empirically set to \( N/8\), as most generated videos contain more than 8 frames. \(H\) and \(W\) are the height and width of the feature map, respectively. With auto-correlation factors of all patches, we define the patch-level aperiodicity of the video, as

\[_{pa}=1-_{h,w}(\{F_{i,h,w}\}_{i=1}^{N}\}).\] (5)

Figure 4: Video dynamics at different temporal granularities: (a) Inter-frame Dynamics, (b) Inter-segment Dynamics, and (c) Video-level Dynamics.

_Global aperiodicity_. In addition to patch-level dynamics, we employ a global aperiodicity metric to measure the diversity of patterns between video segments. Specifically, we divide the video into segments. Each segment has a length \(rN\), where \(r\) is a proportion factor, empirically set to 0.25. We use ViCLIP  to extract the spatial-temporal features for each segment. The features are denoted as \(\{F_{i}^{r}\}_{i=1}^{ rN}\). We then calculate the similarity of these features to assess the variation in spatial-temporal patterns across segments, as

\[_{ga}=1-_{i=1}^{ rN} _{j i}(F_{i}^{r},F_{j}^{r}).\] (6)

**(iii) Video-level Dynamics.** The dynamics of a whole video sequence is defined upon the _temporal entropy_ and _temporal semantic dynamics_.

_Temporal entropy_. To evaluate the dynamics at the video level, we first measure the temporal information of each video. The temporal information is defined as the conditional entropy of the entire video sequence given the first frame

\[_{te}=(f_{1},f_{2},,f_{N}|f_{1}).\] (7)

To estimate the conditional entropy \(D_{te}\), we employ the video encoding toolbox FFmpeg .

_Temporal Semantic Dynamics_. Beyond low-level dynamics, we further introduce a semantic diversity score to assess high-level dynamics across the whole video. The semantic diversity score \(_{tsd}\) is computed to reflect semantic-level dynamics and is defined as the variance of DINO  features \(\{F_{i}\}_{i=1}^{N}\) of each frame, as

\[_{tsd}=_{i=1}^{N}\|F_{i}-\|^{2},\] (8)

where \(=_{i=1}^{N}F_{i}\) denotes the mean feature vector of all frames.

### Human Aligned Dynamics Scores

To establish a reliable and robust assessment, we introduce a human alignment module, Fig. 2, to refine the empirically defined dynamcis scores. It utilizes human ratings to provide ground-truth, based on which we fit a linear regression model at each temporal level, as

\[_{f} =_{_{f}}(_{ofs},_{sd},_{pd}),\] (9) \[_{s} =_{_{s}}(_{pa},_{ga}),\] (10) \[_{v} =_{_{v}}(_{te},_{tsd}),\] (11)

where \(_{f},_{s},_{v}\) respectively denote the model parameters of linear regression at each scale. 2 The overall dynamics score of the video is then defined as the average of aligned dynamics scores from all three levels, as

\[=(_{f}+_{s}+_{v}).\] (12)

Through this learnable human alignment procedure, the empirically defined dynamics scores are more consistent with human perception, as validated in Sec. 5.1.

### Dynamics Metrics

After calculating the aligned dynamics scores of all generated videos at inter-frame, inter-segment, and video levels, we combine these scores together to obtain the following two evaluation metrics.

**(i) Dynamics Range.** The metric evaluates model's capability to generate videos with vivid dynamics. A larger dynamics range implies higher dynamics capability. In detail, we determine the dynamics range \(_{range}\) by identifying the extremes of the dynamic scores over the benchmark, while excluding the top and bottom 1% scores to mitigate the influence of outliers. This is formulated as

\[_{range}=_{0.99}-_{0.01},\] (13)where \(_{0.99}\) and \(_{0.01}\) denote the 99th and 1st percentile values of the dynamics scores for videos generated with the DEVIL benchmark, respectively. This metric reflects a realistic spread of dynamics, excluding atypical extremes.

**(i) Dynamics Controllability.** Let \(^{(i)},^{(j)}\) respectively denote the ground-truth dynamics grades of prompt \(i\) and \(j\), and \(^{(i)},^{(j)}\) the predicted dynamics scores by prompt \(i\) and \(j\). For \(^{(i)}>^{(j)}\), we should have \(^{(i)}>^{(j)}\) so that the dynamics scores of the generated videos are consistent with the dynamics grades of text prompts. Accordingly, we can calculate the dynamics controllability metric by

\[_{control}=_{i=1}^{|T|}}_{j:P_ {j} P_{i}}(^{(i)}-^{(j)})( ^{(i)}-^{(j)}),\] (14)

where \(|T|\) is the total prompt number and \(T_{i}\) denotes the number of prompts at dynamics grade \(^{(i)}\).

## 4 Improving Existing Metrics with Dynamics

As observed by our experiments, existing metrics have negative relevance to video dynamics. To identify these metrics, we calculate the correlation, \(e.g.\), Pearson and Kendal correlation coefficients, between dynamics scores and existing metrics, Table 11. These metrics include naturalness, motion smoothness, subject consistency, and background consistency. Under these metrics, models might 'cheat' for high-quality scores by generating low-dynamic videos.

We improve the identified metrics by incorporating our proposed human-aligned dynamics score \(\). In specific, we propose to equally divide the human-aligned dynamics score into \(L=13\) intervals. Within each interval, we calculate the mean metric values. The mean values of the \(L\) intervals are further averaged as the improved metrics. Upon the improved metrics, to have a high score, the generated videos should spread all dynamics intervals, which implies a large dynamics range. 3

**Naturalness.** In addition to the improved metrics, we introduce the _Naturalness_ metric, which reflects how much the generated videos are like camera-captured ones. This is done by using the MLLM, \(i.e.\), Gemini-1.5 Pro , to calculate a naturalness score for each video. The scores are categorized into five grades: "Almost Real" (100 points), "Slightly Unrealistic" (75 points), "Moderately Unrealistic" (50 points), "Noticeably Unrealistic" (25 points), and "Completely Fictitious" (0 points). The overall naturalness is then determined by averaging the scores of all videos. For evaluation, we invited five users to rate the naturalness of the generated videos and then perform a correlation analysis between human ratings and model scores. A high correlation (larger correlation coefficients) indicates the plausibility of the naturalness metric.

## 5 Experiment

### Human Alignment Assessment

To evaluate the plausibility of the proposed dynamics metrics and the naturalness metric, we conduct the following human alignment experiments.

**Ground-truth Annotation**. We first generate videos using six state-of-the-art (SOTA) T2V models, including GEN-2 , Pika , VideoCrafter2 , Open-Sora , StreamingT2V  and FreeNoise-Lavie  and DEVIL text prompts. For the generated videos, we collect human evaluated

   Evaluation Metrics & PC & KC \\  Naturalness (Nat) & -51.8 & -44.2 \\ Visual Quality (VQ) & -24.8 & -18.6 \\ Motion Smoothness (MS) & -64.0 & -54.6 \\ Subject Consistency (SC) & -88.9 & -74.9 \\ Background Consistency (BC) & -79.4 & -61.4 \\   

Table 2: Correlation between the dynamics metric with the existing metrics including **N**aturalness (Nat), **V**isual **Q**uality  (VQ), **M**otion **S**moothness (MS) , **S**ubject **C**onsistency(SC)  and **B**ackground **C**onsistency(BC) . ’PC’ denotes Pearson’s correlation, and ’KC’ denotes Kendall’s correlation.

dynamics and naturalness as the ground-truth. Six persons are recruited to assess each video's grade of dynamics under three temporal levels (Frame, Segment, and Video). For each temporal level, evaluators are required to rate the grade of dynamics from "static" to "very high dynamics". To guide the annotation process, we provide specific prompts for each temporal level. 4. We conduct between-group correlation analyses using Pearson's correlation, Kendall's correlation, and the win ratio to evaluate the consistency of dynamics scores with respect to human ratings.

The evaluation of the naturalness metric follows the same process, where a higher human assigned grade indicates a greater degree of naturalness.

**Evaluation**. We calculate dynamics grades and naturalness for generated videos on the proposed DEVIL benchmark. For dynamics scores at multiple temporal levels, we integrate them using the linear regression model defined by Eq. 12. For each linear regression model, it takes the human evaluation results as ground-truths, trained upon 75% of the randomly selected videos and tests on the remaining 25% videos. During testing, the human alignment performance is reflected by the correlation \(e.g.\), Pearson and Kendall correlation coefficients and win ratio, between predicted and human-evaluated dynamics grades. The win ratio involves comparing each video against others with different grades of dynamics. For instance, a video rated as "high dynamics" by evaluators should score lower in dynamics than any video rated as "Very high dynamics" but higher than those rated as "static".

Table 3 shows the assessment results of the six T2V models. It can be seen that the dynamics metrics and the naturalness metric exhibit a strong alignment with human evaluation. The improved metrics (\(_{f}\), \(_{s}\), \(_{v}\) defined in Sec. 3.3) further enhance the alignment with human evaluations.

### Influence of Frame Rate.

    &  &  \\   & \(_{range}^{f}\) & \(_{range}^{s}\) & \(_{range}^{v}\) & \(_{range}\) & \(_{control}\) \\  GEN-2  & 18.8 & 49.7 & 21.4 & 27.8 & 80.8 \\ Pika  & 36.2 & 56.3 & 29.1 & 36.8 & 72.2 \\ VideoCrafter2  & 38.9 & 43.2 & 17.0 & 29.4 & 56.6 \\ OpenSora  & 65.2 & 80.1 & 36.4 & 55.3 & 61.7 \\ StreamingT2V  & 59.8 & 80.0 & 69.9 & 61.4 & 64.0 \\ FreeNoise-Lavie  & 67.6 & 71.7 & 66.3 & 63.4 & 58.2 \\ VideoCrafter1  & 62.0 & 60.3 & 28.5 & 44.6 & 63.7 \\ Hotshot-XL  & 52.2 & 56.8 & 17.6 & 36.1 & 59.0 \\ Show-1  & 55.5 & 62.4 & 37.0 & 45.0 & 74.2 \\ ModelScope  & 72.1 & 78.1 & 40.3 & 56.0 & 63.7 \\ ZeroScope  & 24.9 & 46.8 & 19.3 & 28.5 & 66.4 \\   

Table 4: Evaluation of dynamics across text-to-video models at multiple temporal levels. Metrics include inter-frame (\(_{range}^{f}\)), inter-segment (\(_{range}^{s}\)), and video-level (\(_{range}^{v}\)) dynamics range and overall dynamics range (\(_{range}\)) also shown. Dynamics ranges and dynamics controllability (\(_{control}\)) are from 0 to 100, where higher scores indicate better performance.

   Scores &  &  &  \\   & \(S_{ofs}\) & 93.1 & 89.9 & 79.2 \\  & \(S_{sd}\) & 91.7 & 88.0 & 78.1 \\  & \(S_{pd}\) & 96.4 & 93.2 & 86.1 \\  & \(S_{f}\) & **96.5** & **93.5** & **86.5** \\   & \(S_{pa}\) & 95.1 & 94.3 & 87.0 \\  & \(S_{g}\) & 94.6 & 93.0 & 85.6 \\  & \(S_{s}\) & **95.8** & **94.8** & **87.7** \\   & \(S_{te}\) & 96.4 & 93.7 & 83.5 \\  & \(S_{tsd}\) & 97.7 & 96.4 & 90.5 \\  & \(S_{v}\) & **98.0** & **97.2** & **91.4** \\   Naturalness & & 79.0 & 75.5 & 52.4 \\   

Table 3: Human alignment by correlation between dynamics scores and human ratings on the proposed DEVIL benchmark. Video generation is based on text prompts in DEVIL. “PC” denotes Pearson’s correlation, “KC” Kendall’s correlation, and “WR” the win ratio.

Table 5 demonstrates how frame rate influences the correlation between dynamics scores and human evaluations. Experiments indicate that our dynamics scores maintain a high correlation (>0.9) with human ratings across various frame rates. To mitigate the impact of frame rate variations on dynamics, we standardized the frame rate of each video to 8 FPS.

### Computation Efficiency

Our dynamics metrics offer high computational efficiency, achieving around 10 frames per second on a single NVIDIA A100 GPU, and are scalable to multiple GPUs.

### Evaluation of Video Dynamics

We evaluate the dynamics range \(_{range}\), and dynamics controllability \(_{control}\) of T2V models on the proposed DEVIL benchmark. We also evaluate dynamics ranges at different temporal scales: inter-segment dynamics range\(_{range}^{f}\), inter-segment dynamics range \(_{range}^{s}\), and video level dynamics range \(_{range}^{v}\). The results are shown in Table 4. The GEN-2  and Pika  models score high in dynamics controllability but low in range due to their generation of low-dynamic videos. Conversely, the FreeNoise-Lavie method  attains high range but low controllability, suggesting it produces videos with dynamics that do not align well with text prompts.

### Improved Evaluation Metrics

As shown in Table 6, the existing metrics exhibit an obvious negative correlation when embedded with dynamics, indicating that these models can achieve high scores on these metrics by generating low-dynamic videos rather than high-quality content.

### Insights from Video Dynamics Analysis

**Biased Dynamics Distribution of Existing Dataset.** The distribution of dynamics of the video datasets (such as WebVid2M ) is biased. The statistical result is shown in Fig. 5. It can be seen that most of the videos have a small dynamics score (\( 20\)). The limited number of videos with high dynamics scores restricts the model's ability to generate dynamics-rich videos which are common in

    &  &  \\   & MS & BC & SC & Nat & MS & BC & SC & Nat \\  GEN-2  & 99.4 & 97.2 & 95.6 & 81.6 & 57.9 & 55.6 & 53.3 & 38.0 \\ Pika  & 99.4 & 96.5 & 93.2 & 69.0 & 57.7 & 55.8 & 53.1 & 32.4 \\ VideoCrafter2  & 97.7 & 97.4 & 95.5 & 70.8 & 48.1 & 47.7 & 46.5 & 33.1 \\ OpenSora  & 95.4 & 94.3 & 88.7 & 63.6 & 69.9 & 69.5 & 63.1 & 37.1 \\ StreamingT2V  & 94.9 & 91.0 & 85.2 & 55.2 & 70.7 & 68.0 & 61.2 & 33.2 \\ FreeNoise-Lavie  & 95.5 & 94.1 & 90.1 & 73.4 & 77.4 & 68.1 & 76.3 & 51.4 \\ VideoCrafter1  & 95.8 & 95.3 & 92.8 & 75.4 & 61.3 & 60.7 & 57.9 & 39.7 \\ Hotshot-XL  & 97.0 & 95.5 & 93.4 & 85.6 & 55.7 & 54.7 & 52.1 & 45.1 \\ Show-1  & 97.1 & 94.4 & 91.5 & 74.4 & 62.9 & 61.8 & 58.6 & 43.0 \\ ModelScope  & 95.8 & 93.5 & 89.3 & 71.6 & 70.5 & 68.2 & 62.6 & 44.9 \\ ZeroScope  & 98.2 & 94.8 & 89.3 & 74.6 & 49.1 & 46.8 & 45.6 & 34.2 \\   

Table 6: Evaluation of existing metrics and improved metrics. These metrics include **M**otion **S**moothness (MS), **B**ackground **C**onsistency(BC), **S**ubject **C**onsistency(SC), and **N**aturalness (Nat).

Figure 5: Video quantity density \(w.r.t.\) dynamics score of the WebVid-2M dataset.

   Dynamics & 4FPS & 8FPS & 16FPS & Origin \\  Inter-frame & 0.952 & 0.950 & 0.946 & 0.951 \\ Inter-Segm & 0.952 & 0.954 & 0.954 & 0.953 \\ Video & 0.967 & 0.967 & 0.967 & 0.967 \\   

Table 5: Influence of frame rate on the consistency of dynamics scores with human evaluation (measured by Pearson’s correlation).

practical applications. Therefore, existing datasets should be expanded in terms of dynamics, and the proposed metrics can provide guidance for this expansion.

**Prompt-Video Bias of Existing Datasets.** We used the dynamics controllability metric to evaluate two popular datasets, WebVid2M  and MSR-VTT , by using the ground-truth text prompts and videos. Unfortunately, they respectively achieve alignment scores of 36.31 and 52.98. The poor performance indicates that the two datasets can not provide sufficient information/guidance while training the video generation models. To train better video generation models, the text prompt of these datasets requires to be elaborated on aspects of dynamics.

**Limited Real-World Simulation Ability of Existing Methods.** As shown in Fig. 6, we performed a statistical analysis of frequency, visual quality, motion smoothness and naturalness metric scores for SOTA methods based on the distribution of dynamics score. When the dynamics score is low, videos generated by these SOTA models perform well across the four metrics mentioned. As the dynamics score rises, these metrics, particularly naturalness, tend to decrease significantly. This decline may be due to the models' focus on optimizing the generation of simple, slow-motion content, with dynamics not considered in the evaluation metrics.

## 6 Conclusion

We proposed DEVIL, a comprehensive and constructive evaluation protocol for T2V generation models. In the protocol, we defined a set of dynamics metrics corresponding to multiple temporal granularities, and a new benchmark of text prompts under multiple levels of dynamics. Based on the distribution of dynamics scores over the benchmark, we assessed the generation capacity of T2V models, characterized by dynamic ranges and degree of T2V alignment. Experiments show that DEVIL enjoys 90% consistency with human evaluation results, demonstrating the potential to be a powerful tool for advancing T2V generation models.

**Limitations.** At present, the grades of dynamics remain limited, which should be improved to more fine-grained grades. Furthermore, only a limited number of T2V models are evaluated using the proposed protocol. A more comprehensive evaluation of T2V models should be done in future work.

## 7 Acknowledgment

This work was supported by National Natural Science Foundation of China (NSFC) under Grant 62472402 and 62225208, and the Fundamental Research Funds for the Central Universities. This work was also supported by the Centre for Augmented Reasoning, an initiative by the Department of Education, Australian Government.

Figure 6: Distributions of video quantity and quality scores along the dynamics score for various video generation models including: GEN-2 , Pika , VideoCrafter2(VC-2) , Open-Sora(OS) , StreamingT2V  and FreeNoise-Lavie(FN) . Subplot (a) shows video quantity distribution. Subplots (b) display the distribution of quality score of generated videos in terms of Background Consistency, Motion Smoothness, and Naturalness, respectively. All videos are generated based on our text prompt benchmark.