# PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks

Ian Char

Machine Learning Department

Carnegie Mellon University

Pittsburgh, PA 15213

ichar@cs.cmu.edu

&Jeff Schneider

Machine Learning Department, Robotics Institute

Carnegie Mellon University

Pittsburgh, PA 15213

schneide@cs.cmu.edu

###### Abstract

Deep reinforcement learning (RL) has shown immense potential for learning to control systems through data alone. However, one challenge deep RL faces is that the full state of the system is often not observable. When this is the case, the policy needs to leverage the history of observations to infer the current state. At the same time, differences between the training and testing environments makes it critical for the policy not to overfit to the sequence of observations it sees at training time. As such, there is an important balancing act between having the history encoder be flexible enough to extract relevant information, yet be robust to changes in the environment. To strike this balance, we look to the PID controller for inspiration. We assert the PID controller's success shows that only summing and differencing are needed to accumulate information over time for many control tasks. Following this principle, we propose two architectures for encoding history: one that directly uses PID features and another that extends these core ideas and can be used in arbitrary control tasks. When compared with prior approaches, our encoders produce policies that are often more robust and achieve better performance on a variety of tracking tasks. Going beyond tracking tasks, our policies achieve 1.7x better performance on average over previous state-of-the-art methods on a suite of locomotion control tasks. 1

## 1 Introduction

Deep reinforcement learning (RL) holds great potential for solving complex tasks through data alone, and there have already been exciting applications of RL in video game playing , language model tuning , and robotic control . Despite these successes, there still remain significant challenges in controlling real-world systems that stand in the way of realizing RL's full potential . One major hurdle is the issue of partial observability, resulting in a Partially Observable Markov Decision Process (POMDP). In this case, the true state of the system is unknown and the policy must leverage its history of observations. Another hurdle stems from the fact that policies are often trained in an imperfect simulator, which is likely different from the true environment. Combining these two challenges necessitates striking a balance between extracting useful information from the history and avoiding overfitting to modelling error. Therefore, introducing the right inductive biases to the training procedure is crucial.

The use of recurrent network architectures in deep RL for POMDPs was one of the initial proposed solutions  and remains a prominent approach for control tasks . Theses architectures are certainly flexible; however, it is unclear whether they are the best choice for control tasks,especially since they were originally designed with other applications in mind such as natural language processing.

In contrast with deep RL methods, the Proportional-Integral-Derivative (PID) controller remains a cornerstone of modern control systems despite its simplicity and the fact it is over 100 years old [5; 48]. PID controllers are single-input single-output (SISO) feedback controllers designed for tracking problems, where the goal is to maintain a signal at a given reference value. The controller adjusts a single actuator based on the weighted sum of three terms: the current error between the signal and its reference, the integral of this error over time, and the temporal derivative of this error. PID controllers are far simpler than recurrent architectures and yet are still able to perform well in SISO tracking problems despite having no model for the system's dynamics. We assert that PID's success teaches us that in many cases only two operations are needed for successful control: summing and differencing.

To investigate this assertion, we conduct experiments on a variety of SISO and multi-input multi-output (MIMO) tracking problems using the same featurizations as a PID controller to encode history. We find that this encoding often achieves superior performance and is significantly more resilient to changes in the dynamics during test time. The biggest shortcoming with this method, however, is that it can only be used for tracking problems. As such, we propose an architecture that is built on the same principles as the PID controller, but is general enough to be applied to arbitrary control problems. Not only does this architecture exhibit similar robustness benefits, but policies trained with it achieve an average of _1.7x better performance_ than previous state-of-the-art methods on a suite of locomotion control tasks.

## 2 Preliminaries

The MDP and POMDPWe define the discrete time, infinite horizon Markov Decision Process (MDP) to be the tuple \((,,r,T,T_{0},)\), where \(\) is the state space, \(\) is the action space, \(r:\) is the reward function, \(T:()\) is the transition function, \(T_{0}()\) is the initial state distribution, and \(\) is the discount factor. We use \(()\) to denote the space of distributions over \(\). Importantly, the Markov property holds for the transition function, i.e. the distribution over a next state \(s^{}\) depends only on the current state, \(s\), and current action, \(a\). Knowing previous states and actions does not provide any more information. The objective is to learn a policy \(:()\) that maximizes the objective \(J()=[_{t=0}^{}^{t}r(s_{t},a_{t},s_{t+1})]\), where \(s_{0} T_{0}\), \(a_{t}(s_{t})\), and \(s_{t+1} T(s_{t},a_{t})\). When learning a policy, it is often key to learn a corresponding value function, \(Q^{}:\), which outputs the expected discounted returns after playing action \(a\) at state \(s\) and then following \(\) afterwards.

In a Partially Observable Markov Decision Process (POMDP), the observations that the policy receives are not the true states of the process. In control this may happen for a variety of reasons such as noisy observations made by sensors, but in this work we specifically focus on the case where aspects of the state space remain unmeasured. In any case, the POMDP is defined as the tuple \((,,r,T,T_{0},,,)\), where \(\) is the space of possible observations, \(:()\) is the conditional distribution of seeing an observation, and the rest of the elements of the tuple remain the same as before. The objective remains the same as the MDP, but now the policy and value functions are not allowed access to the state.

Crucially, the Markov property does not hold for observations in the POMDP. That is, where \(o_{1:t+1}:=o_{1},o_{2},,o_{t+1}\) are observations seen at times \(1\) through \(t+1\), \(o_{1:t-1} o_{t+1}|o_{t},a_{t}\). A naive solution to this problem is to instead have the policy take in the history of the episode so far. Of course, it is usually infeasible to learn a policy that takes in the entire history for long episodes since the space of possible histories grows exponentially with the length of the episode. Instead, one can encode the information into a more compact representation. In particular, one can use an encoder \(\) which outputs an encoding \(z_{t}=(o_{1:t},a_{1:t-1},r_{1:t-1})\) (note that encoders need not always take in the actions and rewards). Then, the policy and Q-value functions are augmented to take in \((o_{t},z_{t})\) and \((o_{t},a_{t},z_{t})\), respectively.

Tracking Problems and PID Controllers.We first focus on the tracking problem, in which there are a set of signals that we wish to maintain at given reference values. For example, in espresso machines the temperature of the boiler (i.e. the signal) must be maintained at a constant reference temperature, and a controller is used to vary the boiler's on-off time so the temperature is maintained at that value . Casting tracking problems as discrete time POMDPs, we let \(o_{t}=(x_{t}^{(1)},,x_{t}^{(M)},_{t}^{(1)},,_{t}^{(M )})\) be the observation at time \(t\), where \(x_{t}^{(i)}\) and \(_{t}^{(i)}\) are the \(i^{}\) signal and corresponding reference value, respectively. The reward at time \(t\) is simply the negative error summed across dimensions, i.e. \(-_{m=1}^{M}|x_{t}^{(m)}-_{t}^{(m)}|\).

When dealing with a single-input single-output (SISO) system (with one signal and one actuator that influences the signal), one often uses a Proportional-Integral-Derivative (PID) controller: a feedback controller that is often paired with feedforward control. This controller requires no knowledge of the dynamics, and simply sets the action via a linear combination of three terms: the error (P), the integral of the error (I), and the derivative of the error (D). When comparing other architectures to the PID controller, we will use orange colored text and blue colored text to highlight similarities between the I and D terms, respectively. Concretely, the policy corresponding to a discrete-time PID controller is defined as

\[^{}(o_{t})=K_{P}(x_{t}^{(1)}-_{t}^{(1)})+K_{I} }(x_{i}^{(1)}-_{i}^{(1)})dt+K_{D} ^{(1)}-_{t}^{(1)})-(x_{t-1}^{(1)}-_{t- 1}^{(1)})}{dt}\] (1)

where \(K_{P}\), \(K_{I}\), and \(K_{D}\) are scalar values known as gains that must be tuned. PID controllers are designed for SISO control problems, but many real-world systems are multi-input multi-output (MIMO). In the case of MIMO tracking problems, where there are \(M\) signals with \(M\) corresponding actuators, one can control the system with \(M\) separate PID controllers. However, this assumes there is a clear breakdown of which actuator influences which signal. Additionally, there are often interactions between the different signals, which the PID controllers do not account for. Beyond tracking problems, it is less clear how to use PID controllers without substantial engineering efforts.

## 3 Methodology

To motivate the following, consider the task of controlling a tokamak: a toroidal device that magnetically confines plasma and is used for nuclear fusion. Nuclear fusion holds the promise of providing an energy source with few drawbacks and an abundant fuel source. As such, there has recently been a surge of interest in applying machine learning [1; 13], and especially RL [14; 45; 17; 71; 65; 64; 44], for tokamak control. However, applying deep RL has the same problems as mentioned earlier; the state is partially observable since there are aspects of the plasma's state that cannot be measured in real time, and the policy must be trained before-hand on an imperfect simulator since operation of the actual device is extremely expensive.

How should one choose a historical encoder with these challenges in mind? Previous works [50; 46] suggest using Long Short Term Memory (LSTM) , Gated Recurrent Units , or transformers . These architectures have been shown to be powerful tools in natural language processing, where there exist complicated relationships between words and how they are positioned with respect to each other. However, do the same complex temporal relationships exist in something like tokamak control? The fact that PID controllers have been successfully applied for feedback control on tokamaks suggests this may not be the case [72; 27]. In reality, the extra flexibility of these architectures may become a hindrance when deployed on the physical device if they overfit to quirks in the simulator.

In this section, we present two historical encoders that we believe have good inductive biases for control. They are inspired by the PID controller in that they only sum and difference in order to combine information throughout time. Following this, in Section 5, we empirically show the benefits of these encoders on a number of control tasks including tokamak control.

The PID Encoder.Under the framework of a policy that uses a history encoder, the standard PID controller (1) is simply a linear policy with an encoder that outputs the tracking error, the integral of the tracking error, and the derivative of the tracking error. This notion can be extended to MIMO problems and arbitrary policy classes, resulting in the _PID-Encoder_ (PIDE). Given input \(o_{1:t}\), this encoder outputs a \(3M\) dimensional vector consisting of \((x_{t}^{(m)}-_{t}^{(m)})\), \(_{i=1}^{t}(x_{i}^{(m)}-_{i}^{(m)})dt\), and \(^{(m)}-_{i}^{(m)})-(x_{t-1}^{(m)}-_{t- 1}^{(m)})}{dt} m=1,,M\). For SISO problems, policies with this encoder have access to the same information as a PID controller. However, for MIMO problems the policy hasaccess to all the information that each PID controller, acting in isolation, would have. Ideally a sophisticated policy would coordinate each actuator setting well.

The Generalized PID Encoder.A shortcoming of PIDE is that it is only applicable to tracking problems since it operates over tracking error explicitly. A more general encoder should instead accumulate information over arbitrary features of each observation. With this in mind, we introduce the _Generalized-PID-Encoder_ (GPIDE).

GPIDE consists of a number of "heads", each accumulating information about the history in a different manner. When there are \(H\) heads, GPIDE forms history encoding, \(z_{t}\), through the following:

\[v_{i}^{h} =f_{}^{h}((o_{i-1},a_{i-1},r_{i-1},o_{i} -o_{i-1}))  i\{1,,t\},h\{1,H\}\] \[w_{t}^{h} =^{}(v_{1:t}^{h})  h\{1,H\}\] \[z_{t} =g_{}((w_{t}^{1},w_{t}^{2},,w_{t}^{h }))\]

Here, GPIDE is parameterized by \(\). For head \(h\), \(f_{}^{h}\) is a linear projection of the previous observation, action, reward, and difference between the current and previous observation to \(^{D}\), and \(^{}\) is a weighted summation of these projections. \(g_{}\) is a decoder which combines all of the information from the heads. A diagram of this process is shown in Figure 1. Note that \(\) is trained along with the policy and Q networks end-to-end with a gradient based optimizer.

Notice that the key aspects of the PID controller are present here. The difference in observations is explicitly taken before the linear projection \(f_{}^{h}\). We found that this simple method works best for representing differences when the observations are scalar descriptions of the state (e.g. joint positions). Although we do not consider image observations in this work, we imagine a similar technique could be done by taking the differences in image encodings. Like the integral term of the PID, \(^{}\) also accumulates information over time. In the following, we consider several possibilities for \(^{h}\), and we will refer to these different choices as "head types" throughout this work. We omit the superscript \(h\) below for notational convenience.

**Summation.** Most in line with PID, the projections can be summed, i.e. \((v_{1:t})=_{i=1}^{t}v_{i}\).

**Exponential Smoothing.** In order to weight recent observations more heavily, exponential smoothing can be used. That is, \((v_{1:t})=(1-)^{t-1}v_{1}+_{i=2}^{t}(1-)^{t-i}v_{i}\), where \(0 1\) is the smoothing parameter. Unlike summation, this head type cannot accumulate information in the same way because it is a convex combination.

**Attention.** Instead of hard-coding a weighted summation of the projections, this weighting can be learned through attention . Attention is one of the key components of transformers because of its ability to learn relationships between tokens. To implement this, two additional linear functions should be learned that project \((o_{i-1},a_{i-1},r_{i-1},o_{i}-o_{i-1})\) to \(^{D}\). These new projections are referred to as they key and query vectors, denoted as \(k_{i}\) and \(q_{i}\) respectively. The softmax between their inner products is then used to form the weighting scheme for \(v_{1:t}\). We can rewrite the first two

Figure 1: **Architecture for GPIDE. The diagram shows how one encoding, \(z_{t}\), is formed. Each of the gray, rounded boxes corresponds to one of the heads that makes up GPIDE. Each \(\) shows a function to be learned from data, and the \(\) shows the weighted summation of all previous vectors, \(v_{1:t}^{h}\). We write the difference in observations in blue text to highlight the part of GPIDE that relates to a PID controller’s \(z_{t}\). D term. Note that \(q_{1:t}^{h}\) and \(k_{1:t}^{h}\) only play a role in this process if head \(h\) uses attention; as such, we write these terms in gray text.**

steps of GPIDE for a head that uses attention as

\[v_{i},k_{i},q_{i} =f_{}((o_{i-1},a_{i-1},r_{i-1},o_{i}-o_{i-1}))  i\{1,,t\}\] \[w_{1:t} =(q_{1:t},k_{1:t},v_{1:t})=(k _{1:t}^{T}}{})v_{1:t}\]

Here, \(q_{1:t}\), \(k_{1:t}\), and \(v_{1:t}\) are treated as \(t D\) dimensional matrices. Since it results in a convex combination, attention has the capacity to reproduce exponential smoothing but not summation.

To anchor the GPIDE architecture back to the PID controller, we note that the P, I, and D terms can be formed exactly. At a high level, this is achieved when \(f_{}^{h}\) simply subtracts the target from the state measurement and when using summation and exponential smoothing heads with \(=1\). We write down the specific instance of GPIDE that results in these terms in Appendix A.1. While it is trivial for GPIDE to reconstruct the P, I, and D terms, it is less clear how an LSTM or GRU would achieve this, especially because of the I term. At the same time, GPIDE is much more flexible than the PID-representation since altering \(f_{}^{h}\) results in different representations at each time step and altering the type of head results in different temporal relationships.

## 4 Related Work

A control task may be partially observable for a myriad of reasons including unmeasured state variables [28; 75; 29], sensor noise, and unmeasured system parameters [76; 54]. When there are unmeasured system parameters, this is usually framed as a meta-reinforcement learning (MetaRL)  problem. This is a specific subclass of POMDPs where there is a collection of MDPs, and each episode, an MDP is sampled from this collection. Although these works do consider system parameters varying between episodes, the primary focus of the experiments usually tends to be on the multi-task setting (i.e. different reward functions instead of transition functions) [77; 18; 60]. We consider not only differing system parameters but also the presence of unmeasured state variables; therefore, the class of POMDPs considered in this paper is broader than the one studied in MetaRL.

Using recurrent networks has long been an approach for tackling POMDPs , and is still a common way to do so in a wide variety of settings [19; 73; 70; 50; 47; 75; 66; 11; 2]. Moreover implementations are publicly available both for on-policy [41; 30] and off-policy [50; 75; 11] algorithms, making it an easy pick for those wanting a quick solution. Some works [32; 77; 28; 18; 3] use recurrent networks to estimate the belief state , which is a distribution over the agent's true state. However, Ni et al.  recently showed that well-implemented, recurrent versions of SAC  and TD3  perform competitively with many of these specialized algorithms. In either case, we believe works that estimate the belief state are not in conflict with our own since their architectures can be modified to use GPIDE instead of a recurrent unit.

Beyond recurrent networks, there has been a surge of interest in applying transformers to reinforcement learning . However, we were unable to find many instances of transformers being used as history encoders in the online setting, perhaps because of their difficulty to train. Parisotto et al.  introduced a new architecture to remedy these difficulties; however, Melo  applied transformers to MetaRL and asserted that careful weight initialization is the only thing needed for stability in training. We note that GPIDE with only attention heads is similar to a single multi-headed self-attention block that appears in many transformer architectures; however, we show that attention is the least important type of head in GPIDE and often hurts performance (see Section 5.3).

Perhaps closest to our proposed architecture is PEARL , which does a multiplicative combination of Gaussian distributions corresponding to each state-action-reward tuple. However, their algorithm is designed for the MetaRL setting specifically. Additionally, we note that the idea of summations and averaging has been shown to be powerful in prior works. Specifically, Oliva et al.  introduced the Statistical Recurrent Unit, an alternative architecture to LSTMs and GRUs that leverages moving averages and performs competitively across several supervised learning tasks.

There are many facets of RL where improvements can be made to robustness, and many works focus on altering the training procedure. They use techniques such as optimizing the policy's worst-case performance [59; 36] or using variational information bottleneck (VIB)  to limit the information used by the policy [42; 33; 21]. In contrast, our work specifically focuses on how architecture choices of history encoders affect robustness, but we note our developments can be used in conjunctions with these other directions, possibly resulting in improved robustness. We perform additional experiments that consider VIB in Appendix F.1.

Lastly, we note that there is a plethora of work interested in the intersection of reinforcement learning and PID control . These works focus on using reinforcement learning to tune the coefficients of PID controllers (often in MIMO settings). We view these as important works on how to improve PID control using reinforcement learning; however, we view our own work as how to improve deep reinforcement learning by leveraging ideas from PID control.

## 5 Experiments

In this section, we experimentally compare PIDE and GPIDE against recurrent and transformer encoders. In particular, we explore the following questions:

* How does the performance of a policy using PIDE or GPIDE do on tracking problems? In addition, how well can policies adapt to different system parameters and how robust to modelling error are they on these problems? (Section 5.1)
* Going beyond tracking problems, how well does GPIDE perform on higher dimensional locomotion control tasks (Section 5.2)
* How important is each type of head in GPIDE? (Section 5.3)

For the following tracking problems we use the Soft Actor Critic (SAC)  algorithm with each of the different methods for encoding observation history. Following Ni et al. , we make two separate instantiations of the encoders for the policy and value networks, respectively. Since the tracking problems are relatively simple, we use a small policy network consisting of 1 hidden layer with 24 units; however, we found that we still needed to use a relatively large Q network consisting of 2 hidden layers with 256 units each to solve the problems. All hyperparameters remain fixed across baselines and tracking tasks; only the history encoders change.

For the recurrent encoder, we use a GRU and follow the implementation of Ni et al.  closely. Our transformer encoder closely resembles the GPT2 architecture , and it also includes positional encodings for the observation history. For GPIDE, we use \(H=6\) heads: one summation head, two attention heads, and three exponential smoothing heads (with \(=0.25,0.5,1.0\)). This choice was not optimized, but rather was picked so that all types of heads were included and so that GPIDE has roughly the same amount of parameters as our GRU baseline. As a reference point for these RL methods, we also evaluate the performance of a tuned PID controller. Not only do PID controllers have an incredibly small number of parameters compared to the other RL-based controllers, but the training procedure is also much more straightforward since it can be posed as a black-box optimization over the returns. While there exists many sophisticated extensions of the PID controller (especially in MIMO systems ), we only consider the vanilla PID controller since we believe it serves as a good reference point. All methods are built on top of the rlikit library . More details about implementations, hyperparameters, and computation can be found in Appendices B, C, and D, respectively. We also include additional experiments regarding variational information bottleneck (VIB) and lookback size ablations in Appendices F.1 and F.2. Implementations can be found at https://github.com/TanChar/GPIDE.

### Tracking Problems

In this subsection we consider a number of tracking problems. For each environment, the observation consists of the current signals, the reference values, and additional information about the last action made. Unless stated otherwise, the reward is as described in Section 2. More information about environments can be found in Appendix E. To make a fair comparison against PID controls, we choose to only encode the history of observations. For evaluation, we use 100 fixed settings of the environment (each setting consists of targets and system parameters). To avoid overfitting to these 100 settings, we used a separate set of 100 settings and averaged over 3 seeds when developing our methods. We evaluate policies throughout training, but report the average over the last 10% of evaluations as the final returns. We allow each policy to collect one million environment transitions, and all scores are averaged over 5 seeds. Lastly, each table shows scores formed by scaling the returns by the best and worst average returns across all methods in a particular variant of the environment, where scores of 0 and 100 correspond to the worst and best returns respectively.

Mass Spring Damper TrackingThe first tracking task is the control of a classic 1D toy physics system in which there is a mass attached to a wall by a spring and damper. The goal is then to apply a force to the mass in order to move it to a given reference location. There are three system parameters to consider here: the mass, spring constant, and damping factor. We also consider the substantially more difficult problem in which there are two masses sandwiched between two walls, and the masses are connected to the walls and each other by springs and dampers (see Appendix E.1 for a diagram of this). Overall there are eight system parameters (three spring constants, three damping factors, and two masses) and two actuators (a force applied to each mass). We refer to the first problem as Mass-Spring-Damper (MSD) and the second problem as Double-Mass-Spring-Damper (DMSD).

Additionally, we test how adaptive these policies are by changing system parameters in a MetaRL-type fashion (i.e. for each episode we randomly select system parameters and then fix them for the rest of the episode). Similar to Packer et al. , we train the policies on three versions of the environment: one with no variation in system parameters, one with a small amount of variation, and one with a large amount of variation. We evaluate all policies on the version of the environment with large system parameter variation to test generalization capabilities.

Table 1 shows the scores achieved for each of the settings. While GRU and transformers seem to do a good job at encoding history for the MSD environment, both are significantly worse on the more complex DMSD task when compared to our proposed encoders. This is true especially for GRU, which performs worse than two independent PID controllers for every configuration. Additionally, while it seems that GRU can generalize to large amounts of variation in system parameters when a small amount is present, it fails horribly when trained on fixed system parameters. On the other hand, transformers are able to generalize surprisingly well when trained on both fixed system parameters and with small variation. We hypothesize the autoregressive nature of GRU may make it particularly susceptible to overfitting. Comparing PIDE and GPIDE, we see that PIDE tends to shine in the straightforward cases where there is little change in system parameters, whereas GPIDE is able to adapt when there is a large variation in parameters since it has additional capacity.

Navigation EnvironmentTo emulate the setting where the policy is trained on an imperfect simulator, we consider an environment in which the agent is tasked with moving itself across a surface to a specified 2D target as quickly and efficiently as possible. At every point in time, the agent can apply some force to move itself, but a penalty term proportional to the magnitude of the force is subtracted from the reward. Suppose that we have access to a simulator of the environment that is perfect except for the fact that it does not model friction between the agent and the surface. We refer to this simulator and the real environment as the "No Friction" and "Friction" environment, respectively. In both environments, the mass of the agent is treated as a system parameter that is sampled for each episode; however, the Friction environment has a larger range of masses and also randomly samples the coefficient of friction each episode.

Figure 2 shows the average returns recorded during training for both navigation environments and when the policies trained in No Friction are evaluated in Friction. A table of final scores can be found in Appendix G.3. One can see that GPIDE not only achieves the best returns in the environments it was trained in, but is also robust when going from the frictionless environment to the one with friction. On the other hand, PIDE has less capacity and therefore cannot achieve the same results; however, it

  Environment (Train/Test) & PID Controller & GRU & Transformer & PIDE & GPIDE \\  MSD Fixed/Fixed & \(0.00 3.96\) & \(83.73 3.48\) & \(85.79 1.98\) & \(\) & \(83.72 2.86\) \\ MSD Small/Small & \(0.00 5.58\) & \(\) & \(73.27 4.98\) & \(75.51 1.31\) & \(80.21 8.59\) \\ MSD Fixed/Large & \(36.58 2.86\) & \(0.00 3.42\) & \(\) & \(34.92 0.93\) & \(29.55 2.32\) \\ MSD Small/Large & \(43.52 2.82\) & \(\) & \(81.44 0.82\) & \(53.21 1.31\) & \(68.03 4.43\) \\ MSD Large/Large & \(45.60 1.71\) & \(\) & \(92.60 1.49\) & \(69.88 0.69\) & \(93.03 1.27\) \\  Average & 25.14 & 74.27 & **77.36** & 66.70 & 70.91 \\   DMSD Fixed/Fixed & \(24.33 3.97\) & \(0.00 8.69\) & \(22.05 3.58\) & \(\) & \(76.23 6.26\) \\ DMSD Small/Small & \(16.17 3.09\) & \(0.00 7.79\) & \(43.74 3.70\) & \(\) & \(86.74 3.94\) \\ DMSD Fixed/Large & \(63.59 2.91\) & \(0.00 2.28\) & \(59.84 1.13\) & \(\) & \(63.89 2.16\) \\ DMSD Small/Large & \(70.35 1.44\) & \(39.26 2.37\) & \(73.81 1.60\) & \(88.52 0.83\) & \(\) \\ DMSD Large/Large & \(78.77 1.97\) & \(52.01 2.01\) & \(84.45 1.41\) & \(86.90 9.18\) & \(\) \\  Average & 50.64 & 18.25 & 56.78 & **90.84** & 83.30 \\   Total Average & 37.89 & 46.26 & 67.07 & **78.77** & 77.11 \\  

Table 1: **Mass Spring Damper Task Results**. The scores presented are averaged over five seeds and we show the standard error for each score.

is immediately more robust than the other methods, although it begins to overfit over time. It is also clear that using GRU is less sample efficient and less robust to changes in the test environment.

Tokamak ControlFor our last tracking experiment we return to tokamak control. In particular, we focus on the DIII-D tokamak, a device operated by General Atomics in San Diego, California. We aim to control two quantities: \(_{N}\), the normalized ratio between plasma and magnetic pressure, and rotation, i.e. how fast the plasma is spinning around the toroid. These are important quantities to track because \(_{N}\) serves as an approximate economic indicator and rotation control of the plasma has been suggested to be key for stability [7; 67; 10; 61; 56]. The policy has control over the eight neutral beams , which are able to inject power and torque by blasting neutrally charged particles into the plasma. Importantly, two of the eight beams can be oriented in the opposite direction from the others, which decouples the total combined power and torque to some extent (see Figure 3).

To emulate the sim-to-real training experience, we create a simulator based on the equations described in Boyer et al.  and Scoville et al. . This simulator has two major shortcomings: it assumes that certain states of the plasma (e.g. its shape) are fixed for entire episodes, and it assumes that there are no events that cause loss of confinement of the plasma. We make up for part of the former by randomly sampling plasma states each episode. The approximate

   Environment (Train/Test) & PID Controller & GRU & Transformer & PIDE & GPIDE \\  \(_{N}\)-Track Sim/Sim & \(40.69 0.32\) & \(\) & \(97.56 0.19\) & \(0.00 1.05\) & \(98.33 0.41\) \\ \(_{N}\)-Track Sim/Real & \(\) & \(40.96 5.45\) & \(40.05 11.91\) & \(0.00 21.04\) & \(55.21 4.44\) \\ \(_{N}\)-Track Real/Real & \(98.45 0.77\) & \(98.24 0.38\) & \(98.74 0.29\) & \(\) & \(99.30 0.64\) \\  Average & 76.10 & 79.73 & 78.79 & 33.33 & **84.28** \\   \(_{N}\)-Rot-Track Sim/Sim & \(0.00 0.83\) & \(99.06 0.22\) & \(96.22 0.94\) & \(67.98 0.50\) & \(\) \\ \(_{N}\)-Rot-Track Sim/Real & \(\) & \(39.76 5.84\) & \(33.31 0.69\) & \(0.00 8.89\) & \(51.00 1.92\) \\ \(_{N}\)-Rot-Track Real/Real & \(92.02 0.84\) & \(98.34 0.52\) & \(96.32 0.31\) & \(98.21 0.23\) & \(\) \\  Average & 58.58 & 79.05 & 75.28 & 55.40 & **83.67** \\   Total Average & 67.34 & 79.39 & 77.03 & 44.36 & **83.97** \\   

Table 2: **Tokamak Control Task Results. The scores presented are averaged over five seeds and we show the standard error for each score.**

Figure 3: **Illustration of DIII-D from Above.  Each beamline in the figure contains two independent beams (yellow boxes). The plasma is rotating counter-clockwise and the two beams in the bottom left of the figure are oriented in the counter-current direction, allowing power and torque to be decoupled. This figure gives a rough idea of beam positioning but is not physically accurate.**

Figure 2: **Average Returns for Navigation Environments. The curves show the average over five seeds and the shaded region shows the standard error. For this plot, we allowed for 5x the normal amount budget to allow all methods to converge. We omit the PID controllers from this plot since it gets substantially worse returns.**

"real" environment addresses these shortcomings by using a data-driven simulator. This approach to simulating has been shown to be relatively accurate [14; 65; 64; 1], and we use an adapted version of the simulator appearing in Char et al.  for our work. This simulator accounts for a greater set of the plasma's state, and the additional information is rich enough that loss of confinement events play a role in the dynamics.

We consider two versions of this task: the first is a SISO task where total power is controlled to achieve a \(_{N}\) target, and the second is a MIMO task where total power and torque is controlled to achieve \(_{N}\) and rotation targets. The results for both of these tasks are shown in Table 2. Most of the RL techniques are able to do well if tested in the same environment they were trained in; the exception of this is PIDE, which curiously is unable to perform well in the simulator environment. While no reinforcement learning method matches the robustness of a PID controller, policies trained with GPIDE fare significantly better.

### PyBullet Locomotion Tasks

Moving past tracking problems, we evaluate GPIDE on the PyBullet  benchmark proposed by Han et al.  and adapted in Ni et al. . The benchmark has four robots: halfcheetah, hopper, walker, and ant. For each of these, either the current position information or velocity information is hidden from the agent. Except for GPIDE and transformer encoder, we use all of the performance traces given by Ni et al. . In addition to SAC, they also train using PPO , A2C , TD3 , and VRM , a variational method that uses recurrent units to estimate the belief state. We reproduce as much of the training and evaluation procedure as possible, including using the same hyperparameters in the SAC algorithm and giving the history encoders access to actions and rewards. For more information see Appendix C.2. Table 3 shows the performance of GPIDE along with a subset of best performing methods (more results can be found in Appendix G.5). These results make it clear that GPIDE is powerful in arbitrary control tasks besides tracking since the average score achieved across all tasks is a 73% improvement over TD3-GRU, which we believe is the previous state-of-the-art for this benchmark at the time of this work.

Moreover, GPIDE dominates performance for every robot except HalfCheetah. The only setting where GPIDE achieves significantly worse performance is HalfCheetah-V. This setting seemed to cause stability issues in some seeds, lowering the average score. We believe that these stability issues stem from the attention heads, and we found that removing these heads fixed stability issues and resulted in a competitive average score (see Appendix G.5).

### GPIDE Ablations

To investigate the role of each type of head, we reran all experiments with three variants of GPIDE: one with six exponential smoothing heads (ES), one with five exponential smoothing heads and

    & MSD & DMSD & Navigation & \(_{N}\) Track & \(_{N}\)-Rot Track & PyBullet \\  ES & +2.69\% & -11.14\% & -0.11\% & +2.57\% & +0.29\% & +5.81\% \\ ES + Sum & -8.33\% & +5.49\% & -1.65\% & +4.22\% & +0.76\% & +11.00\% \\ Attention & -0.36\% & -54.95\% & -3.91\% & -8.85\% & -7.55\% & -39.44\% \\   

Table 4: **GPIDE Ablation Percent Difference for Average Scores. All final scores can be found in Appendix G.**

   Environment & PPO-GRU & TD3-GRU & VRM & SAC-Transformer & SAC-GPIDE \\  HalfCheetah-P & \(27.09 7.85\) & \(\) & \(-107.00 1.39\) & \(37.00 9.97\) & \(82.63 3.46\) \\ Hopper-P & \(49.00 5.22\) & \(84.63 8.33\) & \(3.53 1.63\) & \(59.54 19.64\) & \(\) \\ Walker-P & \(1.67 4.39\) & \(29.08 9.67\) & \(-3.89 1.25\) & \(24.89 14.80\) & \(\) \\ Ant-P & \(39.48 3.74\) & \(-36.36 3.35\) & \(-36.39 0.17\) & \(-10.57 2.34\) & \(\) \\ HalfCheetah-V & \(19.68 11.71\) & \(\) & \(-80.49 2.97\) & \(-41.31 26.15\) & \(20.39 29.60\) \\ Hopper-V & \(13.86 4.80\) & \(57.43 8.63\) & \(10.08 3.51\) & \(0.28 8.49\) & \(\) \\ Walker-V & \(8.12 5.43\) & \(-4.63 1.30\) & \(-1.80 0.70\) & \(-8.21 1.31\) & \(\) \\ Ant-V & \(1.43 3.26\) & \(17.03 6.55\) & \(-13.41 0.12\) & \(0.81 1.31\) & \(\) \\  Average & 20.04 & 36.50 & -28.67 & 7.80 & **63.18** \\   

Table 3: **PyBullet Task Results. Each score is averaged over four seeds and we report the standard errors. Unlike before, we scale the returns by the returns of an oracle policy (i.e. one which sees position and velocity) and a policy which does not encode any history. For the environment names, “P” and “V” denote only position or only velocity in the observation, resepctively.**one summation head (ES + Sum), and one with six attention heads (see Appendix C.3 for details). We choose these three configurations specifically to better understand the roles that attention and summation play.

Table 4 shows the differences in the average scores for each environment. The first notable takeaway is that having summation is often important in some of the more complex environments. The other takeaway is that much of the heavy lifting is being done by the exponential smoothing. GPIDE fares far worse when only having attention heads, especially in DMSD and the PyBullet environments.

We visualize some of the attention schemes learned by GPIDE for MSD with small variation and HalfCheetah (Figure 4). While the attention scheme learned for MSD could potentially be useful since it recalls information from near the beginning of the episode when the most movement is happening, it appears that the attention scheme for HalfCheetah is simply a poor reproduction of exponential smoothing, making it redundant and suboptimal. In fact, we found this phenomenon to be true across all attention heads and PyBullet tasks. We believe that the periodicity that appears here is due to the oscillatory nature of the problem and lack of positional encoding (although we found including positional encoding degrades performance).

## 6 Discussion

In this work, we introduced the PIDE and GPIDE history encoders to be used for reinforcement learning in partially observable control tasks. Although both are far simpler than prior methods of encoding, they often result in powerful yet robust controllers. We hope that this work inspires the research community to think about how pre-existing control methods can inform architecture choices.

LimitationsThere are many different ways a control task may be partially observable, and we do not believe that our proposed methods are solutions to all of them. For example, we do not think GPIDE is necessarily suited for tasks where the agent needs to remember events (e.g. picking up a key to unlock a door).

As with any bias, the PID-inspired biases that we propose in this work come at the cost of flexibility. For the experiments considered in this work, this trade off is beneficial and results in better policies. However, it is unclear whether this trade off is always worth making. It is possible that in higher dimensional environments or environments with more complex dynamics that having more flexibility is preferable to our proposed architecture.

Lastly, some tasks may require the policy to act on images as observations. We are optimistic that PIDE and GPIDE are still useful architectures in this setting, but we speculate that this is contingent on training an image encoder that is well-suited for these architectures, and we leave this research direction for future work.

## 7 Acknowledgements

We would like to thank Conor Igoe for his helpful discussions and advice on this work. We would also like to thank the NeurIPS 2023 reviewers assigned to our paper for their discussion and feedback.

This work was supported in part by US Department of Energy grants under contract numbers DE-SC0021414 and DE-AC02-09CH1146. This work is also supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE1745016 and DGE2140739. Any opinions, findings, and conclusions recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.

Figure 4: **Averaged Attention Schemes for MSD-Small and HalfCheetah-P. Each y-position on the grid corresponds to an amount of history being recorded, and each x-position corresponds to a time point in that history. As such, each of the left-most points are the oldest observation in the history, and the diagonals correspond to the most recent observation. The darker the blue, the greater the weight that is assigned to that time point.**