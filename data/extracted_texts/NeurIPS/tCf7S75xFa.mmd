# Physics-Informed Variational State-Space

Gaussian Processes

 Oliver Hamelijnck

University of Warwick

oliver.hamelijnck@warwick.ac.uk

Arno Solin

Aalto University

arno.solin@aalto.fi

&Theodoros Damoulas

University of Warwick

t.damoulas@warwick.ac.uk

###### Abstract

Differential equations are important mechanistic models that are integral to many scientific and engineering applications. With the abundance of available data there has been a growing interest in data-driven physics-informed models. Gaussian processes (GPs) are particularly suited to this task as they can model complex, non-linear phenomena whilst incorporating prior knowledge and quantifying uncertainty. Current approaches have found some success but are limited as they either achieve poor computational scalings or focus only on the temporal setting. This work addresses these issues by introducing a variational spatio-temporal state-space GP that handles linear and non-linear physical constraints while achieving efficient linear-in-time computation costs. We demonstrate our methods in a range of synthetic and real-world settings and outperform the current state-of-the-art in both predictive and computational performance.

## 1 Introduction

Physical modelling is integral in modern science and engineering with applications from climate modelling  to options pricing . Here, the key formalism to inject mechanistic physical knowledge are differential equations (DEs), which given initial and/or boundary values, are typically solved numerically . In contrast machine learning is data-driven, and aims to learn latent functions from observations. However the increasing availability of data has spurred interest in combining these traditional mechanistic models with data-driven methods through physics-informed machine learning. These hybrids approaches aim to improve predictive accuracy, computational efficiency by leveraging both physical inductive biases with observations .

A principled way to incorporate prior physical knowledge is through Gaussian processes (gps). gps are stochastic processes and are a data-centric approach that facilitates the quantification of uncertainty. Recently autoip was proposed in order to integrate non-linear physics into gps, where solutions to ordinary and partial differential equations (ODEs, PDEs) are observed at a finite set of collocation points. This is an extension of the probabilistic meshless method (pmm, ) to the variational setting such that non linear equations can be incorporated. Similarly,  introduced helmholtz-gp, that constructs gp priors that adhere to curl and divergence-free constraints. Such properties are required for the

Figure 1: The state-space formalism allows for linear-time inference in the temporal dimension.

successful modelling of electromagnetic fields  and ocean currents through the Helmholtz decomposition . These approaches enable the incorporation of physics but incur a cubic computational complexity from needlessly computing full covariance matrices, as illustrated in Fig. 1. For ODEs (time-series setting), extended Kalman smoothers incorporate non-linear physics (eks) [65; 34] and recover popular ODE solvers whilst achieving linear-in-time complexity through state-space gps [58; 25].

In this work we propose a unified physics informed state-space gp (physs-gp) that is a probabilistic models where mechanistic/physics knowledge is incorporated as an inductive bias. We can handle both linear and non-linear PDEs and ODEs whilst maintaining linear-in-time computational efficiency. We additionally derive a state-space variational inference algorithm that further reduces the computational cost in the spatial dimension. We recover eks, pmm, and helmholtz-gp as special cases, and outperform autoip in terms of computational efficiency and predictive performance. In summary:

1. We derive a state-space gp that can handle spatio-temporal derivatives with a computational complexity that is linear in the temporal dimension.
2. With this we derive a unifying state-space variational inference framework that allows the incorporation of both linear and non-linear PDEs whilst achieving a linear-in-time complexity and recovering state-of-the-art methods such as eks, pmm and helmholtz-gp.
3. We further explore three approximations, namely a structured variational posterior, spatial sparsity, and spatial minibatching, that reduce the cubic spatial computational costs to linear.
4. We showcase our methods on a variety of synthetic and real-world experiments and outperform the current state-of-the-art methods autoip and helmholtz-gp both in terms computational and predictive performance.

Code to reproduce experiments is available at https://github.com/ohamelijnck/physs_gp.

## 2 Background on Gaussian Processes

Gaussian processesA GP is a distribution on an infinite collection of random variables such that any finite subset is jointly Gaussian . Given observations \(^{N F}\) and \(^{N}\) then

\[p(,f\,|\,)=_{n}^{N}p(y_{n}\,|\,f( _{n}),)\,p(f\,|\,)\] (1)

is a joint model where \(p(f\,|\,)\) is a zero mean GP prior with kernel \((,)\), \(f() p(f()\,|\,0,(,))\), and \(\) are (hyper) parameters. We are primarily concerned with the spatio-temporal setting where we observe \(N_{}\) temporal and \(N_{}\) spatial observations \(x_{,s}\), \(y_{,s}\) on a spatio-temporal grid. Under a Gaussian likelihood, all quantities for inference and training are available analytically and, naively, carry a dominant computational cost of \(((N_{}\,N_{})^{3})\). For time series data, an efficient way to construct a GP over \(f\) (and its time derivatives) is through the state-space representation of GPs. Given a Markov kernel, the temporal GP prior can be written as the solution of a discretised linear time-invariant stochastic differential equation (lti-sde), which at time \(k\) is

\[}_{k+1}=\,}_{k}+q_{k}  y_{k}\,|\,}_{k} p(y_{k}\,|\,}\,}_{k}),\] (2)

where \(\) is a transition matrix, \(q_{k}\) is Gaussian noise, \(\) is an observation matrix, and \(}\) is a \(d\)-dimensional vector of temporal derivatives \(=[f(),,f( )}{ x^{2}},]^{}\). With appropriately designed states, matrices and densities, SDEs of this form represent a large class of gp models, and Kalman smoothing enables inference in \((N_{}\,d^{3})\), see . In the spatio-temporal setting, when the kernel matrix decomposes as a Kronecker product \(=_{t}_{s}\), then with a Markov time kernel, a state space form is admitted. This takes a particularly convenient form where the state is \(}_{t}=[((_{s})_{1},t),,((_{s})_{Ns},t)]^{}\), and inference requires \((N_{}(N_{}\,d)^{3})\), see .

Derivative Gaussian processesOne main appeal of gps is that they are closed under linear operators. Let \([]=_{}\,_{}[]\) be linear functional that computes \(D=d_{}\,d_{s}\) space-time derivatives with \(_{}[]=[,,}{ t^{2}},]\) and \(_{}[]=[,,}{ s^{2}},]\), then at a finite set of index points, the joint prior between \(\) and its time and spatial derivatives is

\[p(())=(\,\,\,\,|\,\, ,\,\,(,)\,^{*}\,)\] (3)where \(()=\,f()\) and \(^{*}\) is the adjoint of \(\), meaning it operates on the second argument of the kernel . When jointly modelling a single time and space derivative (\(d_{t}=d_{s}=1\)) the latent functions are \(}=[,}}{ s},}}{ t},}{ t }}{ s}]^{}\) and the kernel is

\[}=\,(,)\,^ {*}=[&-&-&-\\ \,&\,\,^{}&-&-\\ \,&\,\,^{}&\,\,&-\\ }{ t\, s}\,&}{  t\, s}\,\,^{}& }{ t\, s}\,\,&}{ t\, s}\,}{ t\, s}^{}].\]

This is a multi-output prior whose samples are paths of \(f\) with its corresponding derivatives. This prior is commonly known as a derivative gp and has found applications in monotonic GPs , input-dependent noise [41; 67] and explicitly modelling derivatives [59; 17; 43]. State-space gps can be employed in the temporal setting since the underlying state computes \(f()\) with its corresponding time derivatives. In Sec. 3.1, we extend this to the spatio-temporal setting.

## 3 Physics-Informed State-Space Gaussian Processes (physs-gp)

We now propose a flexible generative model for incorporating information from both data observations and (non-linear) physical mechanics. We consider general non-linear evolution equations of the form

\[g(_{}\,f)=-_{ }\,f=0\] (4)

with appropriate boundary conditions, where \(f:^{F}\) is the latent quantity of interest and \(_{}\) is a non-linear differential operator . We assume that \(g:^{P D}\) is measurable, and is well-defined such that there are sensible solutions to the differential equation . We wish to place a gp prior over \(f\) and update our beliefs after 'observing' that it should follow the solution of the differential equation. In general this is intractable and can only be handled approximately. By viewing Eqn. (4) as a loss function that measures the residual between \(\) and the operator \(_{}\,f\) then the right hand side (\(0\)) are virtual observations. The PDE can now be observed at a finite set of locations known as collocation points. This is a soft constraint (_i.e._\(\) is not guaranteed to follow the differential equation), but it can handle non-linear and linear mechanisms. However, there are special cases, namely curl and divergence-free constraints, that can be solved exactly. This follows from properties of vectors fields, where \(f\) defines a potential function where linear combinations of its partial derivatives define vector fields that enforce these properties. To handle both of these situations we propose the following generative model

\[_{n}=\,_{q}( _{n})\,^{}}_{},\,\,\,_{q} (,}_{q}),\] (5) \[_{n}^{()}=_{}\,_{n}+_{}}_{},\,\,_{n}^{()}=g(_{n})}_{},\,\,\, _{n}^{()}=_{}\,_{n}+_{}}_{},\] (6)

where \(_{q}\) are derivative gps (see Eqn. (3)) that are linearly mixed by \(^{(\,D)(\,D)}\), and \(^{()},^{()}^{ }\) are observations and collocation points over the P outputs and \(^{()}^{(\,D)}\) are boundary values over the derivatives of each output. The observation matrices \(_{},_{}\) simply select the relevant parts of \(_{n}\). For further details on notation see App. A. In many case we want to observe the solution of the differential equation exactly, however in some cases it may be required to add observation noise \(_{}\) to the collocation points, whether for numerical reasons or to model inexact mechanics. This is a flexible generative model where different assumptions and approximations will lead to various physics informed methods such as autoip, eks, pmm, and helmholtz-gp that we will develop state space algorithms for. Additionally it is possible to learn missing physics by parameterising unknown terms in Eqn. (4) through the gp priors in Eqn. (6) (see App. B.2).

**Example 3.1** (eks Prior and pmm).: We recover eks style generative models (see Hennig et al. ) when the mixing weight is identity \(=\), and \(_{},_{} 0\), and the non-linear transform \(g\) is linearised. Let the prior be Markov \(p(})=_{k}^{N_{}}p(}_{k}\,|\,}_{k-1})\) with marginals \(p(}_{k})=(\,}_{k}\,|\,\,_ {k}^{-},\,\,_{k}^{-}\,\,)\). By taking a first-order Taylor linearisation \(g(}_{k}) g(_{k}^{-})+_ {k}^{-})}{_{k}^{-}}\,}_{k}\) with \(}_{k}(\,,\,_{k}^{-}\,)\) the joint is

\[p(}_{k}\\ _{k})(}_{k}\\ _{k}\,\,|\,\,_{k}^{-}\\ g_{k}(_{k}^{-}),\,\,\\ _{k}^{-})}{_{k}^{-}} \,\,_{k}^{-}\,\,\\ _{k}^{-})}{_{k}^{-}} )^{}.\] (7)

This is now a form that can directly be implemented into an extended Kalman smoothing algorithm . When \(>1\) the state \(}\) is constructed by stacking the individual states of each latent . With linear ODEs eks coincides with pmm.

**Example 3.2** (helmholtz-gp and Curl and Divergence-Free Vector Fields in \(2\)D).: Let \(=[v_{t},v_{s_{1}},v_{s_{2}}]\) denote a 3D-vector field, then curl indicates the tendency of a vector field to rotate and divergence at a specific point indicates the tendency of the field to spread out. Curl and divergence-free fields follow

\[=0\,\,\,(),\,\,\, \,\,\,\,\,\,=0\,\,\,()\] (8)

where \(=[,},}]\). Two basic properties of vector fields state that the divergence of a curl field and the curl of a derivative field are zero . Let \([f_{1},f_{2}]\) be scalar potential functions then

\[_{}= f_{1}\,\,\,(),\,\,\,_{ }=\,f_{2}\,\,\,()\] (9)

define curl and divergence-free fields. In \(2\)D this simplifies to using the _grad_ and _rot_ operators over \(=[v_{s_{1}},v_{s_{2}}]\) (see ). Placing gp priors over \(f_{q}\) we incorporate this into Eqn. (6) by defining

\[_{}=1&0\\ 0&1\,\,,\,\,\,_{}= 0&1\\ -1&0\,\,\,\,\,\,\,\,\,\,\,\,\,\,[},}].\] (10)

helmholtz-gp is defined as the sum of gp priors over \(2\)D curl and divergence-free fields .

### A Spatio-Temporal State-Space Prior

The generative model in Eqn. (6) contains two complications: i) it includes potential non-lineararities, and ii) the independent priors are defined over latent functions with their partial derivatives which substantially increases the computational complexity. We wish to tackle both issues through state-space algorithms that are linear-in-time. We begin by deriving a state-space model that observes derivatives across space and time (see App. A.3 for the simpler time-series setting). In Sec. 3.2 we further derive a state-space variational lower bound that will enable computational speeds up in the spatial dimension.

First, we show how Kronecker structure in the kernel allows us to rewrite the model as the solution to an lti-sde. From the definition of \(\), the separable covariance matrix has a repetitive structure that can be represented through a Kronecker product. The gram matrix is

\[\,(,)\,^{*}=_{ }^{}(_{},_{})\, \,\,_{}^{}(_{}, _{})\] (11)

where \(_{}^{}\,=_{}&_{}}^{*}\\ }\,_{}&}\,_{} }^{*}\) and \(}[]=([])_{1:}\) excludes the underlying latent function.

To find a Kronecker form of the gram matrix over \(\), we will exploit the fact that \(\) is on a spatio-temporal grid and that the kernel is separable. Due to the separable structure a derivative over either the spatio (or temporal) dimension only affects the corresponding kernel, and so when considering \(\), the gram matrix is still Kronecker structured:

\[\,(,)=_{t} (,)\,_{s}( ,)\,( ,)=_{t}(_{t},_{t}) \,_{s}(_{s},_{s}).\] (12)

The full prior over (a permuted) \(\) is now given as

\[p(())\,\,(\,,\,_ {}^{}(_{t},_{t})_{s}^{ }(_{s},_{s})\,).\]

This is the form of a spatio-temporal Gaussian process with derivative kernels that can be immediately cast into a state-space form as in Eqn. (2) where \(=\), as we want to observe the whole state, not just \(\). The marginal likelihood and the gp posterior can now be computed using standard Kalman filtering and smoothing algorithms with a computational time of \(O(N_{}(N_{} d_{s} d)^{3})\). Inference in physs-gp now follows Ex. 3.1 by recognising that the filtering state consists of the spatial points with there spatio-temporal derivatives. The eks prior in Ex. 3.1 can now be simply extended to the PDE setting by placing colocation points on a spatio-temporal grid .

### A State-Space Variational Lower Bound (physs-vgp and physs-eks)

We now derive a variational lower bound for physs-gp that maintains the computational benefits of state-space gps. This acts as an alternative way of handling the non-linearity of \(g\) in Eqn. (6), and will also enable the reduction of the cubic spatial computation complexity in Sec. 4. We start by focusing on the single latent function setting \((=1)\) and collect all terms that relate to observations in Eqn. (6) with \(p(\,|\,})=_{n}^{N}p(_{n}^{( )}|_{}\,_{n})\,p(_{n}^{( )}|g(_{n}))\,p(_{n}^{()}|_{}\,_{n})\). vi frames inference as the minimisation of the Kullback-Leibler divergence between the true posterior and an approximate posterior, which leads the optimisation of the elbo:

\[*{arg\,max}_{q(}\,|\,)}\,= _{\,q(})}\,\,|\,})\,p(})}{q(})}\,\] (13)

where we define the approximate posterior \(q(\,|\,)(\,\,|\,\, ,\,\,)\) as a free-form Gaussian with \(=(,)\) and \(^{D\,N 1}\), \(^{D\,N D\,N}\). The aim is to represent the approximate posterior as a state-space gp posterior, which will enable efficient computation of the whole evidence lower bound (elbo). We will achieve this through the use of natural gradients. The natural gradient preconditions the standard gradient with the inverse Fisher matrix, meaning the information geometry of the parameter space is taken into account, leading to faster convergence and superior performance . For Gaussian approximate posteriors the natural gradient has a simple form 

\[_{k}=_{k-1}+\,}{ _{k}}=(1-)\,}_{k-1}+\,}{_{k}}+=}+\] (14)

where \(=(^{-1},}{{2}}\,^{-1})\) and \(=(,\,^{}+)\) are the natural and expectation parameterisations. This is known as conjugate variational inference (cvi) as \(}\) represent the natural parameters for the conjugate prior \(\). For now, we will assume that the likelihood is conjugate to ensure that \([_{k}]_{2}\) is _p.s.d_, this will be relaxed in Sec. 5. The derivative of the ell is

\[}{[]_{2}}=_{}^{N_{t },N_{s}}]_{2}}\,_{\,q}\, p (_{()}\,|\,}_{()})\,\,,\] (15)

where the expectation is under \(q(}_{()})\), a \(D\) dimensional Gaussian over the spatio-temporal derivatives at location \(_{}\). Within the sum, the only elements of \([]_{2}\) whose gradient will propagate through the expectation are the \(D D\) elements corresponding to these locations. These points are unique and so \(}{[]_{2}}\) has some (permutated) block-diagonal structure, hence Eqn. (14) can be written as

\[q(})_{t}^{Nt}[( }_{t}\,|\,}_{t},}_{t})]\,p(})\] (16)

where \(}_{t}\) is \(D\)-dimensional. The natural gradient update, _i.e._\(q(}_{t})\) in moment parameterisation, can now be computed using Kalman smoothing in \((N_{t}(N_{s} d_{s} d)^{3})\). Collecting \(}=(\,[}_{t}]\, ),}=(\,[}_{ t}]\,)\), then the elbo can also be computed efficiently by substituting this form of \(q(}_{t})\) in

\[=_{}^{N_{t},N_{s}}\,_{\,q(} _{()})}\, p(_{()}\,|\,}_{()})\,-_{t}^{Nt}_{\,q(}_{t})}\,(}_{t}\,|\,}_{t},}_{t})\,+ p(}\,|\,})\] (17)

where the first two terms only depend on \(q(\,_{t})\) and the final term is simply a by-product of running the Kalman filter, leading to a dominant computational complexity of \((N(N_{} d_{s} d)^{3})\). This cost is linear in the datapoints (\(N\)) because the expected log likelihood above decomposes across all spatio-temporal locations. In summary we have shown that natural gradient is equivalent updating a block-diagonal likelihood that decomposes across time; hence the approximate posterior is computable via Kalman smoothing algorithms. Extending to multiple latent functions (\(>1\)) we define a full Gaussian approximate posterior that captures all correlations between the latent functions \(q(}_{1},,}_{}) (\,}_{1},,}_{}\,|\,\,,\,\,)\) where \(^{(N Q) 1},^{(N Q) (N Q)}\). All the observation models in Eqn. (6) decompose across data points, hence Eqn. (16) is still block-diagonal and decomposes across time, except now each component is of dimension \( N_{}\) as it encodes the correlations of spatial points and their spatio-temporal derivatives across the latent functions. We denote this model as physs-vgp and physs-eks when using a eks prior (see Ex. 3.1).

**Theorem 3.1**.: _Let the approximate posterior be (full) Gaussian \(q(}_{1},,}_{}) (\,}_{1},,}_{},\,\,)\) where \(^{(N Q) 1},^{(N Q) (N Q)}\). When \(g\) is linear a single natural gradient step with \(=1\) recovers the optimal solution \(p(}_{1},,}_{})\)._

We prove this in App. A.5.4. This result not only demonstrates the optimality of our proposed inference scheme in the linear Gaussian setting, but confirms that we recover batch models like pmm and helmholtz-gp, as well as eks (see Ex. 3.1).

## 4 Reducing the Spatial Computational Complexity

We now propose three approaches that reduce the cubic computational complexity in the number of spatial derivatives and locations. The first augments the process with inducing points that alleviate cubic costs associated with \(N_{}\). The second is a structured variational approximation that defines the approximate posterior only over the temporal prior and alleviates cubic costs associated with \(d_{s}\). Finally, we introduce spatial mini-batching that alleviates linear \(N_{}\) costs. When used in conjunction, the dominant computation cost is \((N_{} d_{s}(M_{s} d_{t})^{3})\). These approximations are not only useful for the state-space setting and can readily be applied to reduce the computational complexity for batch variational models (such as autoip). See App. B.1 for more details.

Spatio-Temporal Inducing Points (physs-svgp)In this first approximation, denoted by physs-svgp, we augment the full prior \(p(})\) with inducing points. By defining these inducing points on a spatio-temporal grid, we will show that we can still exploit Markov conjugate operations through natural gradients. Let \(}=\,^{M D}\) be inducing points at locations \(^{M F}\). From the standard svgp formulation , the elbo is

\[=_{\,q(},})}[\, \,|\,})\,p(})}{q(})}\,]\] (18)

where \(q(},}) p(}\,|\,})\,q(})\). By defining the inducing points on a spatio-temporal grid at temporal locations \(_{}_{}^{N}\) and spatial \(_{}^{M_{s}(F-1)}\) then the marginal \(p(}\,|\,})\) is Gaussian with mean

\[_{\,|\,}=[\,_{s}^{ }(_{},_{})\,(_{s }^{}(_{},_{}))^{-1}\, ]\,}\] (19)

and variance given in Eqn. (41). This Kronecker structure allows us to again 'decouple' space and time, leading to natural gradient updates with block size \(M_{s} D\), reducing the computational complexity to \((N\,(M_{s} d_{s} d)^{3})\). For full details, see App. A.5.1.

Structured Variational Inference (physs-svgp\({}_{}}\))This second approximation, denoted as physs-svgp\({}_{}}\), defines the inducing points _only_ over the temporal derivatives. This is a useful approximation as it can drastically reduce the size of the filter state, making it more computationally and memory efficient. We begin by defining the joint prior as

\[p(,_{}\,)=p(\,|\,_{}\,)\,p(_{}\,)\]

where \(p(\,|\,_{}\,)\) is a Gaussian conditional with mean

\[[\,|\,_{}\,]= [\,}_{s}^{}(_{ },_{})\,_{s}(_{s}, _{})^{-1}\,]_{}\,,\,\,\,\,\,}_{s}^{}(_{},_{ })=_{s}(_{}, _{})\\ _{}\,_{s}(_{},_{ }).\]

We then define a structured variational posterior

\[q(},_{}\,) p( \,|\,_{}\,)\,q(_{}\, ).\]

Substituting this into the elbo we see that all the terms with the prior spatial derivatives cancel

\[_{\,q}[\,\,|\,})\,p(}|_{}})\,p(_{}\,)}{p(|\,\!+\!_{}\,\!})\,q(_{}\,)}\,]\]

Again, the marginal \(q(_{}\,)\) maintains Kronecker structure, enabling Markov conjugate operations, leading to a computational cost of \((N d_{s}(N_{} d)^{3})\), see App. A.5.2. These variational approximations can simply be applied to non-state-space variational approximation, see App. B.1.

Spatial Mini-BatchingA standard approach for handling big data is through mini-batching where the ell is approximated using only a data subsample . Directly appling mini-batching would be of little computation benefit because computation of the elbo requires running a Kalman smoother that iterates through all time points. Instead, we mini-batch by subsampling \(B_{}\) spatial points

\[}_{}^{N_{}}}}{B_ {}}_{i}^{B_{}}_{q}[\, p(_ {,}\,|\,}_{i,i})\,]\] (20)

where \(i\) is uniformly sampled. We used in conjunction with physs-svgp and physs-svgp\({}_{}\), this results in dominant costs of \((N_{}\,(M_{s} d_{s} d)^{3})\) and \((N_{} d_{s}(M_{s} d)^{3})\) when \(B_{s} N_{}\).

## 5 Handling the PSD Constraint

As discussed in Sec. 3.2 when the differential equation is non-linear, the model is no longer conjugate and the resulting natural gradients are not guaranteed to result in _p.s.d_ updates. This issue has received some attention in the literature [53; 64; 39], but these approaches do not maintain an efficient conjugate representation. One distinction is , which uses the Gauss-Newton approximation to maintain conjugate operations. We now extend this to support spatial inducing points and non-linear transformations. Due to space we focus on physs-svgp, but see App. A.5.3 for further details. The troublesome term for the natural gradient update in Eqn. (14) is the Jacobian of the ell_w.r.t._ to the second expectation parameter; which is not guaranteed to be _p.s.d_ unless the ell is log convex . Focusing at a single location \(n=(t,s)\):

\[}}{[_{k}]_{}}=_{u}}_{q(}_{i})}[_ {p(}_{n}|}_{i})}[\, p(_{n}\,| \,}_{n})\,]]\]

we apply the Bonnet's and Price's theorem  to bring the differential inside the expectation and make a Gauss-Newton  approximation ensuring that the Jacobian is _p.s.d_

\[}}{[_{t}]_{}}_{n, p}^{N}_{q(}_{i})}[\,_{n,p}^{}\, _{n,p}\,_{n,p}\,],\,_{n,p}=(_{n})}{ }_{t}},\,\,\,_{n,p}=^{2}\!  p(_{n}\,|\,g_{n})}{^{2}g_{n}},\] (21)

and \(g_{n,p}=g(}_{n})\) (Eqn. (4)) and \(_{n}\) is the mean of \(p(}_{n}\,|\,}_{t})\) (Eqn. (19)). When using spatial mini-batching Eqn. (21) is also subsampled.

## 6 Related Work

From the optimality of natural gradients, in the conjugate setting, we exactly recover batch gp based models such as [68; 29; 4]. Our inference scheme also applies to models that do not require derivative information i.e. in \(d_{t}=d_{s}=1\). As a special case, we recover , but we have extended the inference scheme to support spatial mini-batching, allowing big spatial datasets to be used. The linear weighting matrix can be used to define a linear model of coregionalisation and its variants [7; 77; 42; 66] and through appropriately designed functionals also non-linear variants .

In Alvarez et al.  gp priors over the solution of differential equations are obtained through a stochastic forcing term but they only consider situations where the Greens function is available. In [22; 23; 57; 33], efficient state-space algorithms are derived but are limited to the temporal setting only. Similarly, Heinonen et al. , learn a 'free-form ODE'. In the spatio-temporal setting Kramer et al.  and Duffin et al.  (which builds ) derive extended Kalman filter algorithms. Additionally there are approaches to constraining gps by linear differential equations [37; 1; 5]. More generally than  in  gp priors over the solutions to linear PDEs with constant coefficients are derived.

Beyond gp based models, physics informed neural networks (PINNs) incorporate physics by constructing a loss function between the network and the differential equation at a finite set of collocation points . This amounts to a highly complex optimisation problem  bringing difficulties for training [70; 71] and uncertainty quantification (UQ) . Current approaches to quantifying uncertainty in PINNs are based on dropout  and conformal predictions . In recent years UQ and deep learning has received much attention however is limited by its computational cost .

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

points and a spatial mini-batch size of \(10\), and plot results in Fig. 4. Our predictions are in excellent agreement with the test data, achieving an rmse of \(0.14\), nlpd of \(-0.52\), crps of \(0.078\), and an average run-time of \(1.86(s)\) per epoch.

## 8 Conclusion

We introduced a physics-informed state-space gp that integrates observational data with physical knowledge. Within the variational inference framework, we derived a computationally efficient algorithm that uses Kalman smoothing to achieve linear-in-time costs. To gain further computational speed-ups, we proposed three approximations with inducing points, spatial mini batching and structured variational posteriors. When used in conjunction, they allow us to handle large-scale spatiotemporal problems. The bottleneck is always the state size, where nearest neighbours gps [13; 74] could be explored. For highly non-linear problems, future directions could explore deep approaches  or more flexible kernel families . One limitation is the use of the collocation method which is only enforcing the differential equation point wise, whilst future work could look at the more general methods of weighted residuals .