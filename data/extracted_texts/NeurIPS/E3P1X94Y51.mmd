# SemFlow: Binding Semantic Segmentation and Image Synthesis via Rectified Flow

Chaoyang Wang\({}^{1}\) Xiangtai Li\({}^{1}\) Lu Qi\({}^{2}\) Henghui Ding\({}^{3}\)

**Yunhai Tong\({}^{1}\) Ming-Hsuan Yang\({}^{2}\)**

\({}^{1}\)School of Intelligence Science and Technology, Peking University

\({}^{2}\)UC, Merced \({}^{3}\)Institute of Big Data, Fudan University

Project page: https://wang-chaoyang.github.io/project/semflow

cywang@stu.pku.edu.cn, qqlu1992@gmail.com, xiangtai94@gmail.com

###### Abstract

Semantic segmentation and semantic image synthesis are two representative tasks in visual perception and generation. While existing methods consider them as two distinct tasks, we propose a unified framework (SemFlow) and model them as a pair of reverse problems. Specifically, motivated by rectified flow theory, we train an ordinary differential equation (ODE) model to transport between the distributions of real images and semantic masks. As the training object is symmetric, samples belonging to the two distributions, images and semantic masks, can be effortlessly transferred reversibly. For semantic segmentation, our approach solves the contradiction between the randomness of diffusion outputs and the uniqueness of segmentation results. For image synthesis, we propose a finite perturbation approach to enhance the diversity of generated results without changing the semantic categories. Experiments show that our SemFlow achieves competitive results on semantic segmentation and semantic image synthesis tasks. We hope this simple framework will motivate people to rethink the unification of low-level and high-level vision.

+
Footnote â€ : Corresponding author: Lu Qi and Xiangtai Li.

## 1 Introduction

Understanding semantic content and creating images from semantic conditions are fundamental research topics in computer vision. Semantic segmentation [45; 7; 59; 85; 11; 10] and image synthesis [8; 32; 28; 72; 90] are two representative dense prediction tasks and inspires various downstream applications, including autonomous driving  and medical image analysis . The former aims to assign a category label to each pixel in the image, while the latter aims to generate realistic images given semantic layouts.

Although semantic segmentation and image synthesis constitute a pair of reverse problems, existing works typically solve them using two distinct methodologies. On the one hand, segmentation models mostly follow the spirit of discriminative models. Specifically, a pre-trained backbone is employed to extract multi-scale features, and then task-specific decoders are used for dense prediction. On the other hand, semantic image synthesis frameworks are mainly built upon generative adversarial networks (GAN) [19; 52; 57; 62] or diffusion models (DM) [63; 64; 25]. GAN-based methods [90; 72; 28] typically take semantic layouts as inputs and adopt a discriminator for adversarial training. Meanwhile, DM-based methods  generate from noise with semantic layouts functioning as control signals.

An intuitive solution is to represent the segmentation mask as a colormap and model it as a conditional image generation task . However, there are several implementation challenges. Overall, most ofthe discriminative segmentation models do not apply to this problem due to their irreversibility. For generative adversarial networks, their generators are typically unidirectional. Jointly training multiple unidirectional models to achieve bidirectional generation  is not the concern of this paper. Latent diffusion models (LDMs) have recently demonstrated great potential in generative tasks. Beyond generative tasks, several works [56; 69] attempt to apply diffusion models for segmentation, with images functioning as conditions, but their applicable tasks are limited to be class-agnostic. There are three main problems for existing LDM-based segmentation frameworks: 1) The contradiction between the randomness of generation outputs and the certainty of segmentation labels. 2) The huge cost brought by multiple inference steps. 3) The irreversibility between semantic masks and images.

In this paper, we use rectified flow [40; 42; 17] to enable LDM as a unified framework for semantic segmentation and semantic image synthesis. Our key idea is summarized in Fig. 1. Starting from an LDM  framework, we solve the above problems with three methodologies. First, we redefine the mapping function to address the randomness. Previous works  aim to learn the mapping from the joint distribution of Gaussian noise and images to the segmentation masks. We argue that Gaussian noise in this mapping function is redundant and negatively affects the determinism of semantic segmentation results. Instead, our model _directly_ learns the mapping from images to masks. Secondly, we make the mapping reversible via rectified flow. Rectified flow is an ordinary differential equation (ODE) framework with a time-symmetric training object. This feature allows the model to be trained once to obtain bi-directional transmission capabilities and can be solved numerically using simple ODE solvers such as Euler. Finally, we propose a finite perturbation method to enable multi-modal image synthesis, as the mapping is one-to-one in semantic segmentation but one-to-many in semantic synthesis. We add amplitude-limited noise, enabling masks to be sampled from a collection of semantic-invariant distributions rather than a fixed value, and further improving the quality of synthesized results. Moreover, our model needs fewer inference steps than traditional diffusion models because the transport trajectory is straight, significantly reducing the gap between LDM and traditional discriminative models in segmentation.

The main contributions are as follows: 1) We introduce SemFlow, a new unified framework that binds semantic segmentation and image synthesis with rectified flow, effectively leveraging the length of the generative models. 2) We propose specialized designs of SemFlow, including pseudo mask modeling, bi-directional training of segmentation and generation, and a finite perturbation strategy. 3) We validate SemFlow on several popular benchmarks. For semantic segmentation, SemFlow dramatically narrows the gap between diffusion models and discriminative models in terms of accuracy and inference speed with a more elegant framework. Meanwhile, SemFlow also performs decently in semantic image synthesis tasks.

## 2 Related Work

**Diffusion Models and Rectified Flow.** Diffusion models [25; 63; 64] have shown impressive results in the field of generation, such as image generation [58; 55; 83; 27; 13; 54], video generation ,

Figure 1: **Rectified flow bridges semantic segmentation (SS) and semantic image synthesis (SIS).** SS and SIS are modeled as a pair of transportation problems between the distributions of images and masks. They share the same ODE and only differ in the direction of the velocity field. We propose a finite perturbation operation on the mask to enable multi-modal generation without changing the semantic labels. _Grey dots_ represent data samples. _Colored dots_ represent semantic centroids, also known as anchors in Eq. 7. _Colored bubbles_ represent the scale of perturbation.

image editing [4; 46; 60; 51; 23], image super resolution [61; 26] and point cloud [47; 50; 88; 82; 48]. Most of these methods are based on stochastic differential equations (SDEs) and need multiple steps for generation. Recently, some works [40; 39; 38; 1; 22] propose to model with probability flow ordinary differential equations (ODEs) to reduce the inference steps. Specifically, rectified flow [40; 42] defines the forward process as straight paths and uses reflow to minimize the sampling steps to one. Although rectified flow has demonstrated decent results on image generation and image transfer tasks, they are limited to low-level vision and lack an exploration of the unification of segmentation and generation tasks.

**Semantic Segmentation** is one of the core tasks in visual perception, aiming to assign each pixel of the given image a semantic category. Previous semantic segmentation approaches are typically built upon discriminative modeling, consisting of a strong backbone [21; 16; 43; 44; 33] for feature extraction and a task-specific decoder head [7; 59; 85; 11; 10; 87; 65; 81; 15; 14] for mask prediction. Recently, some works [34; 86; 18; 77; 75; 3; 70; 9; 20; 2; 29; 76; 71] exploit diffusion models for segmentation. They typically follow the spirit of discriminative models and employ the diffusion model as a feature extractor. Although UniGS  and LDMSeg  attempt to use a plain Stable Diffusion framework for segmentation, their applicable tasks are limited to be class-agnostic. In practice, reconciling the stochastic outputs of diffusion models with the deterministic results of semantic segmentation is difficult. We rethink this problem and re-model it with rectified flow.

**Semantic Image Synthesis** is the reverse problem of semantic segmentation, aiming to generate realistic images given semantic layouts [8; 53; 41; 74; 91; 66; 67; 79; 32; 35; 49]. Several studies have delved into semantic synthesis through two methodologies. One methodology [90; 28; 72; 6; 68] is based on GAN and trained with adversarial loss and reconstruction loss. However, some of these methods can only generate unimodal outputs. Although some methods [67; 92] have been developed to address the diversity issues, GAN-based frameworks typically suffer from unstable training and need careful parameter tuning. Another methodology  employs diffusion models and regards it as a conditional image generation task, where semantic masks function as control signals. Some works further add a greater variety of control signals to enhance the consistency of synthesized results, like textual prompts  and bounding box . Despite these methods achieves good synthetic results, their architecture is usually asymmetric, and the generator is usually unidirectional. This hinders the exploration of the unification of semantic segmentation and image synthesis.

## 3 Method

In this section, we first review diffusion models and the differential equations of diffusion-based segmentation models (DSMs) and then analyze the disadvantages of existing approaches. Afterward, we propose our SemFlow, which is inspired by rectified flow theory. It solves the randomness problem in the existing DSM and unifies semantic segmentation and image synthesis with _one_ model. We will elaborate on each phase of our method.

### Diffusion Model and Segmentation Modeling.

Diffusion models are a class of likelihood-based models that define a Markov chain of forward and backward processes. In the forward process, the noise is gradually added to the real data \(x_{0}\) to form the noisy data \(x_{t}\):

\[q(x_{t}|x_{0})=(x_{t};}x_{0},(1-_{t})I), t [0..T],\] (1)

where \(_{t}\) is functions of \(t\). During training, the model \(_{}\) learns to estimate the noise by minimizing the objective with L2 loss:

\[=_{x_{0} q(x_{0}),(0,I),t} [_{}(}x_{0}+}_{t})-_{t}_{2}^{2}],\] (2)

In the backward process, the model starts from Gaussian noise and gradually denoises to generate realistic data  via,

\[x_{t-1}=}(-}_{ }(x_{t},t)}{}})+-_{t}^{2 }}_{}(x_{t},t)+_{t}_{t}.\] (3)

Luckily, Eq. 3 provides a unified solution for DDPM and DDIM, where the former belongs to an SDE modeling and the latter is an ODE modeling. We parameterized it as

\[x_{0}=f(x_{T},),\] (4)where \(\) controls the way of modeling:

\[_{t}=\{)(1-_{t})}/_{t-1}}&\\ 0&.\] (5)

In the image generation task, \(x_{0}\) represents the real images, and \(x_{T}\) represents the Gaussian noise. We extend the boundaries of Eq. 4 and apply it to semantic segmentation problems. An intuitive way is to encode the segmentation mask into colormaps and model segmentation as a conditional image generation task. Given image \(I\), we rewrite the projection \(f\) into a formulation of conditional generation,

\[x_{0}=f(x_{T},,I).\] (6)

Eq. 6 formulates the existing diffusion-based segmentation models with generative modeling. However, this modeling approach suffers from the following problems: **1)** The randomness of initial noise and the determinism of the segmentation mask are contradictory. **2)** From a transmission point of view, the transfer from noise to the segmentation mask does not conform to the paradigm of semantic segmentation task, where noise is redundant. **3)** The images in this approach only serve as the condition, which is non-causal in reverse transfer. Thus, the reversed transfer is a separate problem.

### Unify Segmentation and Synthesis with Rectified Flow

**Task-agnostic Framework.** We employ the _standard Stable Diffusion_ (SD) framework for our task without any task-specific decoder head or well-designed text prompts. To this end, we design the network architecture using the following three steps. First, we convert the semantic segmentation masks to 3-channel pseudo mask \(M=(m_{0},m_{1},m_{2})\) to align with the images \(I\). Assume that the valid region \(\) (for real images) is divided into \(k\) parts with a spacing of \(s\), the pseudo mask corresponding to category index \(c\) is formulated as:

\[m^{}_{0}= c/k^{2},\ m^{}_{1}=(c-m^{}_{0 }*k^{2})/k,\ m^{}_{2}=c-m^{}_{0}*k^{2}-m^{}_{1}*k,\ M=s*(m^{ }_{0},m^{}_{1},m^{}_{2}),\] (7)

where \(s*(k-1)<255\) and \(\) means the floor operator. The \((m^{i}_{0},m^{i}_{1},m^{i}_{2})\) are called anchors.

After the transformation, we adopt a VAE encoder \(\) to compress the images and pseudo masks into the latent space. The corresponding VAE decoder \(\) restores the latent variables to the pixel space.

\[z_{0}=(I), z_{1}=(M),=( }),=(}),\] (8)

where \(\) means the output of the UNet.

_Discussion._ Although previous works  argue that segmentation masks are lower in entropy and specifically re-train a lighter network, we still adopt the off-the-shelf VAE from SD. The reasons are as follows: 1) The number of parameters in VAE is negligible compared to the UNet (84M vs. 860M). 2) The VAE specifically trained for segmentation masks can not be applied to images, thus destroying the bi-directional transportation. 3) Specific training creates spatial priors, such as clustering, which hinders our exploration of LDM's segmentation capability itself.

Note that we do not use image captions or image features as prompts. As our model unifies semantic segmentation and image synthesis with _one_ model, the usage of captions contradicts the definition of semantic segmentation task while the features are non-causal for image synthesis. Thus, in this work, we set the prompt as empty.

**Bi-directional Training and Inference.** Contrary to the conventional approach, we propose modeling the segmentation task with rectified flow. It is an ODE framework that aims to learn the mapping between two distributions through straight trajectories.

Denote \(_{0}\) and \(_{1}\) represent the distribution of the latent variables of images and masks, respectively. Denote \(z_{0}_{0}\) and \(z_{1}_{1}\), the trajectory from \(z_{0}\) to \(z_{1}\) can be formulated as \(z_{t}=_{t}(z_{0},z_{1})\):

\[z_{t}}{t}=(z_{0},z_{1})}{  t}.\] (9)

When \(\) is the trajectory of rectified flow , \(z_{t}\) can be reformulated as the linear interpolation process between \(z_{0}\) and \(z_{1}\) as \(z_{t}=(1-t)z_{0}+tz_{1}\). We aim to learn the velocity field using neural networks \(v_{}(z_{t},t)\) and solve it with optimization methods,

\[ =_{0}^{1}_{(z_{0},z_{1})}[||v_ {}(z_{t},t)-(z_{0},z_{1})}{ t}| |^{2}]t,\] (10) \[=_{0}^{1}_{(z_{0},z_{1})}[| |v_{}(z_{t},t)-(z_{1}-z_{0})||^{2}]t,\]

where \(\) indicates the coupling of images and their corresponding pseudo masks.

Eq. 10 is our training loss. Upon training completed, the transfer from \(z_{0}\) to \(z_{1}\) can be described via an ODE:

\[z_{t}}{t}=v_{}(z_{t},t), t.\] (11)

So far, we have constructed a mapping from the distribution of images to that of masks. Compared with DSMs in Eq. 6, our approach avoids the interference of randomness, enabling the application of diffusion models to semantic segmentation tasks.

Moreover, Eq. 10 has a time-symmetric form, which results in an equivalent problem by exchanging \(z_{0}\) and \(z_{1}\) and flipping the sign of \(v_{}\). Interestingly, the transportation problem from \(_{1}\) to \(_{0}\) indicates the semantic image synthesis task. This means that semantic segmentation and semantic image synthesis essentially become a pair of mutually reverse problems that share the same ODE (Eq. 11) and have solutions with opposite signs.

In the inference stage, we can obtain the approximate results by numerical solvers like the forward Euler method, which can be formulated as follows,

Semantic segmentation is regarded as the transportation from \(z_{0}\) to \(z_{1}\), which we call _forward flow_. The ODE is Eq. 11 and the numerical solution is,

\[z_{t+}=z_{t}+v_{}(z_{t},t), t\{0,1,...,N-1 \}/N.\] (12)

After \(\) is restored with Eq. 8, we calculate the L2 distance between \(\) and anchors and obtain the segmentation mask.

Correspondingly, the semantic image synthesis task is considered to transfer in a reverse direction, which we call the _reverse flow_. Specifically, this transfer can be described as \(z_{t}}{t}=-v_{}(z_{t},t),\) and the solution is,

\[z_{t-}=z_{t}-v_{}(z_{t},t), t\{N,N-1,...,1 \}/N.\] (13)

### Finite Perturbation

In Eq. 10, the model is configured to learn the one-to-one mapping between images and masks. However, assigning a fixed mask to each image brings about several problems. First, we find that constant \(z_{1}\) in Eq. 13 hinders the multi-modal generation for the image synthesis task. Moreover, the pseudo masks are low in entropy. We hypothesize that the low-entropy distribution of masks hinders the training process and may finally spoil the quality of synthesis results.

To this end, we propose to add finite perturbation on the pseudo masks. Specifically, given pseudo masks \(M\) which has a spacing of \(s\), we add a noise with a limited amplitude on \(M\),

\[M^{}=M+,(-,),\] (14)

where U is a uniform distribution. We set \(0<<s/2\) to ensure that the semantic label of each pixel does not change. Therefore, we propose to replace \(z_{1}\) with \(z_{1}^{}=(M^{})\) in Eq. 10 and Eq. 13. We show the effectiveness of this design in Sec. 4.3.

## 4 Experiments

### Experimental Settings

Dataset and Metrics.We study SemFlow using three popular datasets: COCO-Stuff , CelebAMask-HQ , and Cityscapes . They contain 171, 19, and 19 categories, respectively. For semantic segmentation, we evaluate with mean intersection over union (mIoU). For semantic image synthesis, we assess with the Frechet inception distance (FID)  and learned perceptual image patch similarity (LPIPS) .

**Implementation Details.** The SemFlow model is built upon Stable Diffusion UNet and initialized with weights from the pre-trained SD 1.5. Images and semantic masks in COCO-Stuff, CelebAMask-HQ, and Cityscapes are resized and cropped into \(512 512\), \(512 512\), and \(512 1024\), respectively. We use the off-the-shelf VAE of the corresponding Stable Diffusion model. The spacing \(s\) is set as 50, and the division coefficient \(k\) is 6. The amplitude of perturbation is 6. Note that there is no need to train two models for segmentation and synthesis separately since the optimization target in Eq. 10 is time-symmetric.

**Baselines.** We seek to unify semantic segmentation (SS) and semantic image synthesis (SIS) into a pair of reverse problems and train _one_ model for a solution. As few previous works achieve this goal, we thus take different baselines for SS and SIS, respectively. For SS, we design two baseline models, DSMs , which follow the principle of diffusion-based conditional generation modeling in Sec. 3.1. The network structure and hyperparameters of DSM follow SemFlow, except that the inputs of UNet are eight channels. Specifically, the noise and images are concatenated in the channel dimension. DSM-DDIM and DSM-DDPM differ in differential equation modeling (ODE vs. SDE). For SIS, we take pix2pixHD , SPADE , SC-GAN , BBDM  and CycleGAN  as the baselines.

### Main Results

**Semantic Segmentation.** Fig. 2 shows the qualitative comparison between SemFlow and DSMs. Our SemFlow demonstrates satisfactory performance in a range of scenarios and exhibits high accuracy in classifying the semantic labels of targets. In contrast, DSM fails in semantic segmentation tasks, regardless of SDE or ODE modeling. First, DSM is inferior to SemFlow in discriminating different semantic categories. Moreover, the outputs of DSM-DDIM and DSM-DDPM change dramatically with different random seeds. As discussed in Sec. 3.1, DSM is susceptible to the randomness inherent in diffusion models, making it unable to produce deterministic results.

Tab. 1 shows the quantitative results on the COCO-Stuff dataset. We compare SemFlow with two variants of DSM and MaskFormer, which is a classical discriminative segmentation model. Regarding accuracy, our SemFlow achieves 38.6 mIoU and outperforms DSMs by a remarkable margin. Moreover, SemFlow only uses a simple sampler, forward Euler method, and fewer inference steps than DSMs. Further reducing the inference steps brings about faster generation but witnesses a slight drop in performance, which is analyzed in Sec. 4.3.

**Semantic Image Synthesis.** We compare our approach with other specialist models on semantic image synthesis tasks in Tab. 1. On CelebAMask-HQ, SemFlow achieves a performance of 32.6 FID and 0.393 LPIPS.

Beyond specialist models, we also qualitatively compare our approach with CycleGAN  in Fig. 3 to demonstrate the _overall_ performance on SS and SIS tasks. All results of SemFlow are generated with _one_ model. In other words, the ODE modeling of SS and SIS share the same velocity

    &  &  & COCO-Stuff &  \\  & SS & SIS & & mIoU (SS) & FID (SIS) & LPIPS (SIS) \\  MaskFormer  & âœ“ & & - & 41.9 & - & - \\ DSM  & âœ“ & & DDIM-200 & 16.1 & - & - \\ DSM  & âœ“ & & DDPM-200 & 20.2 & - & - \\ pix2pixHD  & & âœ“ & - & - & 54.7 & 0.529 \\ SPADE  & âœ“ & - & - & 42.2 & 0.487 \\ SC-GAN  & & âœ“ & - & - & 19.2 & 0.395 \\ BBDM  & & âœ“ & BBDM-200 & - & 21.4 & 0.370 \\ SemFlow (ours) & âœ“ & âœ“ & Euler-25 & 38.6 & 32.6 & 0.393 \\   

Table 1: **Semantic segmentation results on COCO-Stuff dataset. SS and SIS represents semantic segmentation and semantic image synthesis, respectively. Sampler-N means the usage of a specific sampler with N inference steps.**field and only differ in the sign. The first and third rows show the image synthesis results given semantic layouts, while the second and fourth row shows the segmentation results. Our synthesis and segmentation results are inspiring and significantly outperform CycleGAN. Note that CycleGAN essentially trains two unidirectional generators while we employ only _one_ model for the two tasks.

Figure 3: **Semantic segmentation and semantic image synthesis results on Cityscapes dataset.** The color black in the ground truth indicates the ignored region. The segmentation results of SemFlow are colored following .

Figure 2: **Semantic segmentation results on COCO-Stuff dataset.** For the ground truth, each color reflects the value of anchors (Eq. 7), which corresponds to one semantic category, and the color white indicates the ignored regions. The predictions of DSM vary considerably under different random seeds.

[MISSING_PAGE_FAIL:8]

tradiction between the randomness of diffusion models and the certainty of semantic segmentation results, we propose to model semantic segmentation as a transport problem between image and mask distributions. We then employ rectified flow to learn the transfer function, which brings the benefits of reversible transportation. We propose a finite perturbation method to enable multi-modal generation, which also greatly improves the quality of synthesized results. With straight trajectory modeling, our model can sample with much fewer steps. Experimental results show that even with a weak sampler, our model still achieves comparable or even better results than specialist models. We hope our research can inspire the findings on unified generative model design for the community.

**Acknowledgement.** This work was supported by the National Key Research and Development Program of China (No. 2023YFC3807600).

   Method\&Sampler & 1 & 2 & 5 & 10 & 25 & 50 & 100 & 200 \\   & DDIM & - & 0.1 & 4.0 & 9.5 & 13.6 & 14.9 & 15.7 & 16.1 \\  & DDPM & - & 0.1 & 5.3 & 12.3 & 17.5 & 18.9 & 19.6 & 20.2 \\  SemFlow & Euler & 28.3 & 31.0 & 36.9 & 38.4 & 38.6 & 38.4 & 38.3 & 38.3 \\   

Table 2: **Semantic segmentation results with different inference steps on COCO-Stuff dataset.** mIoU is used as the metric.

Figure 5: **Image synthesis results with different inference steps.** We use the forward Euler method to get numerical solutions. Our approach obtains competitive results even with only one inference step.

Figure 6: **Visualization of latent variables on the trajectory from \(z_{1}\) to \(z_{0}\) (Semantic image synthesis).** Top row: COCO-Stuff. Bottom row: Cityscapes.

Figure 7: **Visualization of latent variables on the trajectory from \(z_{0}\) to \(z_{1}\) (Semantic segmentation).** Top row: COCO-Stuff. Bottom row: Cityscapes.