# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

Introduction

Visual scene understanding has shown significant progress in extracting semantic information from images and videos using deep learning algorithms [1; 2; 3]. Building upon the significant progress in visual scene understanding using deep learning algorithms [4; 5], video scene graph generation (VidSGG) extends the concept of Scene Graph Generation (SGG) from static images to dynamic video, representing object relationships within a graph structure that evolves over time. VidSGG [6; 7; 8] focus on the temporal dimension by constructing a dynamic graph structure, encapsulating the spatial and temporal relationships among object interactions across frames. This helps in understanding human-object interactions [9; 10], temporal events [11; 12; 13], and reasoning [14; 15]. However, drone-captured videos present unique challenges due to larger image sizes and higher object density in Unmanned Aerial Vehicle (UAV) datasets [16; 17; 18]. Despite recent advances in tiny object detection [19; 20; 21], current algorithms still need to effectively model object interactions and their temporal evolution in aerial videos, which have various applications in surveillance, disaster response.

In this paper, we introduce AeroEye, the first dataset for Video Scene Graph Generation in drone-captured videos featuring **Aerial-Ob**lique-**E**ye views. AeroEye distinguishes itself by showcasing a rich tapestry of aerial videos and an extensive set of predicates describing the intricate relations and positions of multi-objects. To address multi-object relationship modeling in aerial videos from the AeroEye dataset, we propose the Cyclic Graph Transformer (CYCLO). This new approach can establish circular connectivity among frames and enables the model to capture direct and long-range temporal relationships. By continuously updating history across a ring topology, CYCLO allows the model to handle sequences with inherent cyclic patterns, facilitating the processing of object relationships in the correct temporal order. Furthermore, CYCLO provides several advantages to VidSGG, including the ability to model periodic and overlapping relationships, predict object interactions by reasoning from previous cycles, facilitate information transfer across frames, and efficiently utilize long sequences, addressing the limitations of prior methods [22; 8]. They usually struggle with long-term dependencies due to the diminishing influence of inputs over time.

**Contributions of this Work.** There are three main contributions to this work. First, we introduce a new _AeroEye_ dataset for VidSGG in drone videos, augmented with numerous predicates and diverse scenes to capture the complex relationships in aerial videos. Second, we propose the CYCLO approach, utilizing circular connectivity among frames to enable periodic and overlapping relationships. It allows the model to capture long-range dependencies and process object interactions in the appropriate temporal arrangement. Finally, the proposed CYCLO approach outperforms prior methods on two large-scale in-the-wild VidSGG datasets, including PVSG  and ASPIRe . Interestingly, using the same method (_e.g_., our CYCLO), the ratio of correct predictions to incorrect predictions (R/mR) on AeroEye is higher than PVSG (Tables 4 and 6), despite having more predicates (Table 1) and tiny objects. This suggests that our dataset is _less visually ambiguous_ than PVSG.

## 2 Related Work

In this section, we review the existing datasets and benchmarks for Visual Scene Graph Generation, followed by a summary of the key challenges and issues related to Video Scene Graph Generation.

### Visual Scene Graph Generation Datasets and Benchmarks

**Datasets.** VisDrone , DOTA , and SODA-A  image datasets, along with UAVid , UAVDT , and MAVREC  video datasets, offer high-resolution UAV datasets that enable precise object detection in dynamic scenes. While these UAV datasets focus on object detection, the Visual Genome  pioneered image-based SGG, and the Action Genome  dataset extended this concept to capture dynamic interactions within videos. Recently, ASPIRe  and SportsHHI  emphasize diverse human-object relationships and sports-specific player interactions. Additionally, PSG-4D  expands the VidSGG to encompass the 4D domain, bridging the gap between raw visual data and high-level understanding. In Table 1, we present a comparative overview of UAV-based and SGG datasets for images and videos, emphasizing their unique characteristics and advantages.

**Benchmarks.** The existing benchmark focuses on Image Scene Graph Generation (ImgSGG) and Video Scene Graph Generation (VidSGG). _ImgSGG_ identifies and categorizes relationships between objects within an image into predefined relational categories, including Transformer-basedmethods [27; 48; 49; 50] and generative-based models [51; 52; 53]. _VidSGG_ leverages the dynamic nature of object interactions over time to better identify relationships, as the temporal dimension of videos provides a richer context for understanding semantic connections within the scene. Current methods using hierarchical structures [54; 8] or Transformer architectures [22; 55; 56; 57] excel at capturing long-range dependencies and complex interactions, advancing video understanding in video captioning[11; 12; 13], visual question answering [14; 15], and video grounding [58; 59; 60].

### Video Scene Graph Generation

VidSGG can be categorized into two main types based on the granularity of its graph representation. _Video-level SGG_ represents object trajectories as graph nodes, capturing constant relations between objects for a video. Various methods have been proposed to address this problem, incorporating Conditional Random Fields , abstracting videos , and iterative relation inference techniques  on fully connected spatio-temporal. However, focusing primarily on recognizing video-level relations directly based on object-tracking [64; 65; 66] results and neglecting frame-level scene graphs results in a cumbersome pipeline highly dependent on tracking accuracy. In contrast, _Frame-level SGG_ defines the graph at the frame level, allowing relations to change over time. The releases of the benchmark datasets [67; 7; 8] have prompted the development of VidSGG models. TRACE , for instance, employs a hierarchical relation tree to capture spatio-temporal context information, while CSTTran  uses a spatio-temporal transformer to solve the problem. Recently, hierarchical interlacement graph (HIG)  abstracts relationship evolution using a sequence of hierarchical graphs.

### Discussions

In this subsection, we conceptually compare our proposed approach with relationship modeling concepts discussed in Section 2.2 as illustrated in Fig. 2. In addition, we highlight the advantages of our approach and discuss the properties that distinguish it from these existing methods.

    &  &  &  &  &  &  \\  & & & &  & &  & &  & &  &  &  \\ 
**Visual Genome ** & - & **108K** & - & ✓ & ✓ & **3.8M** & **33K** & **42K** & - & ✓ & ✗ & ✗ & ✗ & ✗ \\ VG-150  & - & 88K & - & ✓ & ✓ & 2.8M & 150 & 50 & - & ✓ & ✗ & ✗ & ✗ & ✗ \\ VFW-35  & - & 95K & - & ✓ & ✓ & 282.4K & 15.8K & 117 & ✓ & ✗ & ✗ & ✗ & ✗ \\ GQA  & - & 85K & - & ✓ & ✓ & 1.7K & 310 & - & ✓ & ✗ & ✗ & ✗ & ✗ \\ FSG  & - & 49K & - & ✓ & ✓ & 538.2K & 80 & 56 & - & ✓ & ✗ & ✗ & ✗ \\ 
**VidVPon2008** & 1K & **296.2K** & **1920 \(\) 1080** & ✓ & ✓ & 15.1K & 35 & 132 & ✓ & ✗ & ✗ & ✗ & ✗ \\ Action Genome  & **10K** & 234.3K & 1280 \(\) 720 & ✓ & ✓ & **476.3K** & 25 & 25 & ✓ & ✗ & ✗ & ✗ & ✗ \\
**ASPIR ** & 1.5K & 1.6M & 1280 \(\) 720 & ✓ & ✓ & 1167.8K & **833** & **45.4K** & 7 & ✓ & ✗ & ✗ & ✗ & ✗ \\
**SportIIIB**  & 80 & 11.4K & 1280 \(\) 720 & ✓ & ✓ & 118.1K & 1 & 34 & 2 & ✓ & ✗ & ✗ & ✗ & ✗ \\ VidOR  & **10K** & 55.4K & 640 \(\) 360 & ✓ & ✓ & 50K & 80 & 50 & 1 & ✓ & ✗ & ✗ & ✗ & ✗ \\ VidSTG  & **55.4K** & 640 \(\) 360 & ✓ & ✓ & 50K & 80 & 50 & 1 & ✓ & ✗ & ✗ & ✗ & ✗ \\ EPIC-XITEmens  & 700 & 11.5K & **1920 \(\) 1080** & ✓ & ✓ & 454.3K & 21 & 13 & 1 & ✓ & ✓ & ✗ & ✗ & ✗ \\ PVSG  & 400 & 155K & **1920 \(\) 1080** & ✓ & ✓ & 7.6K & 126 & 57 & ✓ & ✓ & ✗ & ✗ & ✗ \\   DOTA  & - & 11.3K & 1490 \(\) 957 & ✓ & � & 1.8M & 18 & - & ✗ & ✗ & ✓ & ✗ & ✗ \\ AI-TOD  & - & 28.1K & 800 \(\) 800 & ✓ & � & 700.6K & 8 & - & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ DIOR-B  & - & 23.5K & 800 \(\) 800 & ✓ & � & 192.5K & **20** & - & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ MONet  & - & **53K** & - & **7.4K** & 1628 \(\) & - & - & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ SODA-A  & - & 2.5K & **74.61K** & **2721 \(\)** & ✓ & ✗ & **872.1K** & 9 & - & ✗ & ✗ & ✓ & ✗ & ✓ & ✗ \\    & 288.61K & 3840 \(\) 1260 & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ UAVDT  & 100 & 40.7K & 1080 \(\) 540 & ✓ & ✗ & ✗ & 6 & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ Stanford Drome  & **10K** & **29.5K** & - & **7.4K** & - & 6 & - & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ UT-Aprone  & 51 & 206.2K & 1920 \(\) 1080 & ✓ & � & 210.5K & 8 & - & - & ✗ & ✗ & ✗ & ✗ & ✗ \\ ERA  & 29K & 343.7K & 640 \(\) 640 & ✓ & � & ✗ & - & - & **25** & � & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ MORE-LAV  & 30 & 10.9K & 1920 \(\) 1080 & ✓ & � & 89.8K & 2 & - & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ AU-AIR  & - & 32.8K & 1920 \(\) 1080 & ✓ & � & 132K & 8 & - & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\
**ProustFRF ** & 200 & 411.5K & 1280 \(\) 720 & ✓ & � & 786K & 1 & - & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ MindDrone  & 38 & 23.3K & 224 \(\) 247 & ✓ & - & - & - & 1 & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\
**Brutal Running ** & - & 1K & 227 \(\) 227 & ✓ & � & - & - & - & 1 & ✗ & ✗ & ✗ & ✓ & ✗ & ✗ \\
**VAVA4**  & 30 & 300 & **4096 \(\) 2160** & ✓ & � & 8 & - & � & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\
**MAVREC ** & 24 & 537K & 3840 \(\) 2160 & ✓ & � & 1.1M & **10** & - & 4 & ✗ & ✗ & ✗ & ✓ & ✓ \\ 
**AeroEye (Ours)** & **2.3K** & **261.5K** & **3840 \(\) 2160** & ✓ & ✓ & **1.8M** & **57** & **384** & **29** & **✗ & � & ✓ & ✓ \\   

Table 1: Comparison of available datasets for scene graph generation 

**Concepts.** The _progressive_ approach fuses pairwise features between object pairs at each frame, encoding the object relationship at that specific step, followed by a fully connected layer to classify the predicate types. However, it processes frames independently without considering the temporal context. The _batch-progressive_ approach employs a transformer block with positional embeddings on the fused query features. The _hierarchical_ approach represents a video as a sequence of graphs, integrating temporal and spatial information at different levels. The node and edge features are updated at each hierarchical level based on the previous level to capture evolving object relationships.

**Limitations.** While the _batch-progressive_ approach considers temporal information, both these _progressive_ and _batch-progressive_ approaches have limitations in modeling the full complexity of temporal dynamics and dependencies in the video. In contrast, the _hierarchical_ graph approach can capture complex interactions and relationships between objects by considering the temporal evolution of graphs at different granularity levels. However, the hierarchical graph requires analyzing the entire video before constructing the graph (_i.e_. offline method). These limitations underscore the need for more advanced approaches to efficiently model temporal dynamics, adaptively update memory to handle evolving video data, and accurately capture the intricate relationships between objects.

**Advantages of Our Design.** Inspired by previous work [68; 69], which processes temporal features through iterative feedback loops and circular updating, we propose the CYCLO approach that circularly incorporates an updated history of relationships. In contrast to these methods, which focus on frame-level updates influenced by global features, our approach constructs and refines scene graphs for each frame, capturing static spatial relationships between objects and their dynamic evolution over time. By leveraging circular connectivity, CYCLO establishes a continuous loop of temporal information, ensuring no temporal edge is treated as a boundary. It enables the Transformer to operate online and then capture and update relationships between objects more effectively, correcting erroneous connections. The theoretical properties are included in Section B of Appendices.

## 3 The Proposed AeroEye Dataset

In this section, we detail the AeroEye dataset annotation process and provide the dataset statistics.

### Dataset Collection

**Data Preparation.** We leverage videos from the ERA  and MAVREC  datasets to construct our AeroEye dataset. ERA consists of diverse scenes ranging from rural to urban environments in extreme conditions (_e.g_. earthquake, flood, fire, mudslide), daily activities (_e.g_. harvesting, plowing, party, traffic collision), and sports activities (_e.g_. soccer, basketball, baseball, running, swimming). MAVREC features sparse and dense object distributions and contains typical outdoor activities characterized by many vehicle classes, incorporating viewpoint changes and varying illumination.

Figure 2: Comparisons of CYCLO and existing relationship modeling: (a) _progression_[27; 63]: frame-wise fusion and classification; (b) _Batch-progression_[22; 55; 56]: temporal transformer; (c) _Hierarchy_: spatiotemporal graph; (d) Our CYCLO approach: circular connectivity for capturing temporal dependencies.

**Relationship Classes and Instance Formulation.** In Fig. 3, we focus on two aspects of object relationships: positions (_e.g_. in front of, behind, next to) and relations, which consist of movement actions (_e.g_. chasing, towing, overtaking) and collision actions (_e.g_. hitting, crashing, colliding). These relationships are semantically complex and require detailed spatio-temporal context reasoning for recognition. Following previous PVSG benchmark datasets [6; 7; 8], we define relationship instances at the frame level, considering the long-term spatial-temporal context. Each instance is formulated as a triplet \(<\)\(s,o,p\)\(>\), where \(s\) and \(o\) denote the bounding boxes of the subject and object, and \(p\) represents the predicate (_i.e_. position and relation), included in Tables A.8, A.9 which are summarized in Fig. 4. In addition, Fig. A.11 presents selected samples from our AeroEye dataset.

### Data Specification

**Data Annotation.** We annotate keyframes at 5FPS to capture frequent and rapid changes in positions and relations in aerial videos, reducing redundancy while keeping up with interaction changes. Our two-stage annotation pipeline first performs _object localization and tracking_ and then _relationship instance annotation_. To generate diverse predicates, we leverage the GPT4RoI  model, which combines visual and linguistic data to generate detailed descriptions of object relationships within specified regions of interest. Although we annotate relationship instances frame by frame, as illustrated in Fig. 3, we easily create relationship tubes using the provided object tracking ID by connecting the same pair of objects with the same relationship predicate across consecutive frames. The annotation file includes object information (_i.e_., bounding boxes, category names, and tracking IDs) and relationships within each frame. Details on the quality control process and annotation examples can be found in Section A.2 of the Appendices.

Figure 4: Relationship word cloud on AeroEye dataset.

Figure 5: Statistics for each scene on the AeroEye dataset.

Figure 3: Example annotation in our dataset. In Fig. 2(b), straight arrows denote relationships between objects, while curved arrows indicate the positions of the objects. Nodes of the same color represent the same object, and the labels on the edges specify the predicate of each relationship. (Best viewed in colors)

**Data Statistics.** The AeroEye dataset is a collection of 2,260 videos with 261,503 frames, annotated with over 2.2 million bounding boxes across 56 object categories typically observed from _aerial_, _oblique_, and _ground_ perspectives captured by drone. Specifically, our AeroEye dataset consists of 384 predicates divided into two relationship categories: 135 positions and 249 relations. The key strength of AeroEye is its richness in relationships. On average, each video in the dataset has 127 frames, providing moderate temporal depth for capturing detailed interactions. The average number of frames per scene is 8,970, indicating substantial variability and complexity. AeroEye is rich in relationships, with more than 43 million relationship instances. In Table 1, we provide a detailed comparison with related datasets, while Fig. 5 presents statistics on the AeroEye dataset. In addition, predicate definitions and additional statistics are discussed in Sections A.1 and A.4 of the Appendices.

## 4 The Proposed Approach

In this section, we present our CYCLO approach, including the _Spatial Attention Graph_ and the _Cyclic Temporal Graph Transformer_. The _Spatial Attention Graph_ captures spatial dependencies and interactions between objects within the frame. In contrast, the _Cyclic Temporal Graph Transformer_ models temporal relationships across frames, capturing short-term and long-term dynamics.

### Problem Formulation

Given an input video with \(T\) frames, we construct dynamic scene graphs \(\{_{t}\}_{t=1}^{T}\) that encode the relationships among objects within these frames. Each graph \(_{t}(_{t},_{t})\) captures static relationships, where node \(\) consists of objects and edge \(\) denotes the relationship between objects. Each object \(v_{i}\) has an object category \(v_{i}^{c}_{v}\) and box coordinates \(v_{i}^{b}^{4}\). Each relationship \(e_{j}\) represents the \(j\)-th triplet (\(s_{j}\), \(o_{j}\), \(p_{j}\)), where subject \(s_{j}\) and object \(o_{j}\) and predicate \(p_{j}_{p}\).

### Spatial Attention Graph

Self-attention mechanisms in one-stage object detectors [71; 19] model relationships between objects, allowing insights into the dynamics between entities without relying on additional contextual information. For example, in an aerial parking lot video with cars, vans, and people, if the self-attention layer strongly connects the queries representing the person and the car, it suggests an interaction, such as the person entering the vehicle. Inspired by , in our CYCLO approach, to construct the static graph in each frame \(t\), we utilize the DETR decoder to establish bidirectional connections among object queries. In particular, we compute relational representations at each layer \(l\) by concatenating the query and key vectors, \(Q_{t}^{l}\) and \(K_{t}^{l}\), for every object pair. This process ensures the layer \(l-1\) output seamlessly transitions as input to layer \(l\). We omit the superscript \(L\) related to the final layer to simplify the following discussion. At each frame, \(_{a,t}^{l}\) captures the dynamic interplay of relations at layer \(l\), utilizing their query and key vectors. Furthermore, \(_{z,t}\) leverages object queries in the final layer for object detection. These relationships are formally defined in Eqn. (1).

\[_{a,t}^{l}=[Q_{t}^{l}_{W_{z}^{l}};K_{t}^{l}_{W_{z}^{l}}], _{z,t}=[_{t}_{W_{z}};_{t}_{W_{ z}}]\] (1)

Here, \(_{W_{z}}\) and \(_{W_{o}}\) are the linear transformations that process subject and object features, enabling the model to consider both object characteristics and their interrelations comprehensively. In addition, gating mechanisms \(g_{a,t}^{l}\) and \(g_{z,t}\) dynamically modulate the contributions from different layers. These gated representations from all layers are then integrated to construct a relation matrix:

\[_{a,t}^{l}=(_{a,t}^{l}_{W_{G}}), _{z,t}=(_{z,t}_{W_{G}})\] (2) \[_{t}=_{l=1}^{L}(_{a,t}^{l} _{a,t}^{l})+_{z,t}_{z,t}\]

where \(_{W_{G}}\) denotes the linear weight applied during the gating process. Finally, the relation matrix is fed into the three-layer perception (MLP) with ReLU activation and a sigmoid function \(\), which predicts multiple relationships \((p_{j})\) between pairs of objects \((s_{j},o_{j})\). Mathematically, \(_{t}=((_{t}))\) is the graph representation at frame \(t\)-th, where \(_{t}^{N N|_{p}|}\).

**Discussion.** Transformer-based approaches to VidSGG effectively capture interactions and temporal changes through self-attention mechanisms, creating detailed scene graphs that reflect video dynamicrelationships. However, these models often struggle to represent the directional and historical aspects of the relationship accurately. While effective at identifying token correlations, the scaled dot-product fails to consider their temporal or spatial ordering. This oversight is particularly critical in videos, where understanding the historical context of relationships is essential. For example, the sequence of relationships leading up to a car crash, including speeding, lane changing, and passing, must be considered. Each interaction change provides crucial historical information that contextualizes the final relationships, vital for enhancing prediction and interpretation. In addition, traditional self-attention  does not adequately capture this essential sequential, _directional_, and _historical information_. It highlights the need for advancements in transformer architectures to more effectively integrate the direction and historical sequence of interactions within videos.

### Cyclic Temporal Graph Transformer

We present the Cyclic Spatial-Temporal Graph Transformer to refine the spatial attention graph in each scene, capturing temporal dependencies via subject-object relationships across adjacent frames.

**Cyclic Attention.** As mentioned in Section 4.2, self-attention does not adequately capture _directional_ and _historical information_. Therefore, we propose the cyclic attention (CA), defined as in Eqn. (3).

\[(Q_{t},K_{t})=_{i=0}^{T-1}((K_{(t+i) T )}^{}}{}})\] (3)

In Eqn. (3), \(\) is a shift term enabling cyclical indexing via \( T\). The cyclical indexing, illustrated in Fig. 6, allows for continuous sequence processing by connecting the end to the beginning, which is crucial for predicting movements in dynamic interactions where past events influence future actions (_e.g_. a car navigating a roundabout). Cyclical indexing differs from standard self-attention as it is a permutation equivariant without positional encodings. In contrast, cyclical attention is non-permutation equivariant, which depends on the original sequence order. This property is crucial for multi-object relationship modeling, where maintaining the chronological order of interactions is essential. For instance, in a surveillance scenario, the sequence of a car stopping for a pedestrian must preserve the order of events, ensuring the vehicle stops before the pedestrian appears.

**Temporal Graph Transformer.** Our Temporal Graph Transformer refines spatial attention graphs, \(\{_{t}\}_{t=1}^{T}\), into a sequence of dynamic graphs \(\{_{t}\}_{t=1}^{T}\), leveraging the temporal dynamics and spatial interactions of objects across video frames. Our approach employs a series of cyclic attention blocks configured within multi-head attention to refine object representations by integrating features from adjacent frames. The core of our approach is the integration of cyclic attention into a multi-head structure, which processes the sequence of input features \(=\{_{t}\}_{t=1}^{T}\), represented as in Eqn. (4).

\[& Z^{}=_{W_{e}}([h_{0};h_{1};;h_{e-1} ]), Z^{}_{t}=_{W_{e}}([h_{0}(t);h_{1}(t);;h_{e-1}(t)]),\\ & h_{i}(t)=(_{W^{i}_{q}}(_{t}),_{W^{ j}_{k}}()), i\{0,1,,e-1\},\] (4)

where \(_{W_{e}}\), \(_{W^{i}_{q}}\), and \(_{W^{j}_{k}}\) denote the linear transformations. Each head \(h_{i}(t)\) computes the cyclic attention, integrating information across the video to enhance the temporal relationship at each frame. The outputs from various heads at each frame are integrated into \(Z^{}_{t}\), derived from concatenating all attention head outputs. These heads process features cyclically across different representation subspaces to capture the temporal evolution of relationships in the video. Then, \(Z\) is obtained by applying layer normalization (LN) and a skip connection to the aggregated features \(Z^{}_{t}\), where \(Z=(Z^{}+)\). This step ensures that \(Z^{}_{t}\) is stabilized and effectively integrated with the original features \(\), thus dynamically updating the scene graph and ensuring temporal coherence.

In addition, \(Z_{t}\) is utilized to construct a new relation matrix \(R_{t}\) by applying the transformations in Eqn. (1) and (2). This updated matrix \(R_{t}\) refines the relationship dynamics captured in static frames

Figure 6: Illustration of cyclic interactions in the Cyclic Spatial-Temporal Graph Transformer. Each frame, represented by a colored block (where the first frame, \(t=1\) and the last frame, \(T=4\)), undergoes spatial attention to obtain queries (\(Q_{t}\)) and keys (\(_{t}\)).

by correcting spurious or incomplete relationships and incorporating previously omitted ones using the temporal context from the frame sequence. As a result, \(G_{t}\) comprehensively represents persistent and transient interactions, including their direction and historical sequence within the video.

**Loss Function.** Visual object relationships involve predicates that may appear quite similar, such as "parking next to" and "stopping next to". Thereby, we utilize a multi-label margin loss, as in :

\[_{p}(r,^{+},^{-})=_{p^{+}} _{q^{-}}(0,1-(r,p)+(r,q))\] (5)

In Eqn. (5), \(r\) represents a subject-object pair, and \(^{+}\) and \(^{-}\) correspond to positive and negative predicates, respectively. The term \((r,p)\) measures the compatibility of the pair \(r\) with the predicate \(p\). Additionally, object distributions are modeled using neural networks with ReLU activation and batch normalization. Cross-entropy loss is applied during the learning process. The total loss is a combination of the multi-label margin loss \(_{p}\) and the cross-entropy loss \(_{ce}\), defined as:

\[_{total}=_{p}+_{ce}\] (6)

where \(\) is a weight balancing the contribution of the cross-entropy loss \(_{ce}\).

## 5 Experimental Results

In this section, we discuss the benchmark dataset evaluations and comparisons with SOTA methods.

### Implementation Details

**Dataset.** We use 10-fold cross-validation on the AeroEye dataset, including 1,797 videos for training and 463 videos for testing. We also evaluate our performance on PVSG  and ASPIRe  datasets.

**Settings.** We employ DINO  to extract the spatial attention graphs (in Section 4.2). DINO is trained with ResNet-50 backbone and \(1500\) queries on MAVREC, achieving \(92.35\) mAP on the validation set. The pre-trained detector is applied to baselines, and parameters are fixed during subsequent task training. Our model is trained on 8 \(\) A6000 GPUs using 12 epochs with AdamW optimizer (initial learning rate of \(1e^{-5}\) and a batch size of 1), gradient clipping (max norm of \(5\)).

**Evaluation Metrics.** We evaluate models on two standard tasks in image-based scene graph generation followed by previous work [74; 7] that are predicate classification (_PredCls_), scene graph classification (_SGCls_), and scene graph detection (_SGDet_). While _SGCls_ predicts relationships given ground truth objects, _SGDet_ involves detecting objects and predicting relationships. These tasks are evaluated using Recall (R@K) and mean Recall (mR@\(K\)), where \(K\{20,50,100\}\).

### Ablation Study

**Semantic Dynamics in Cyclic Attention.** By altering \(\) (in Eqn. (3)), we consider the permutation or non-equivariance equivariance. If the predictions systematically adapt to the shifts induced by

    &  &  &  \\  & **R/mR@20** & **R/m@50** & **R/m@100** & **R/m@20** & **R/m@50** & **R/m@100** & **R/m@20** & **R/m@50** & **R/m@100** \\ 
**1** & **56.20 / 19.23** & **61.62 / 20.67** & **62.40 / 21.19** & **54.55 / 16.22** & **59.59 / 18.20** & **60.37 / 18.38** & **43.53 / 13.29** & **47.93 / 13.69** & **48.94 / 13.86** \\
**2** & 55.01 / 18.01 & 60.02 / 19.02 & 61.08 / 19.53 & 53.04 / 15.05 & 58.60 / 17.07 & 59.08 / 17.25 & 60.29 / 12.10 & 46.11 / 25.64 / 12.73 & 42.71 / 12.78 \\
**3** & 54.12 / 17.11 & 59.13 / 18.21 & 60.14 / 18.16 & 53.17 / 14.18 & 53.17 / 19.16 & 52.82 / 16.14 & 42.18 / 13.30 & 43.18 / 11.82 & 46.32 / 12.04 \\
**4** & 54.55 / 17.55 & 59.56 / 18.56 & 60.57 / 19.07 & 52.59 / 14.60 & 57.61 / 16.62 & 58.63 / 16.84 & 47.15 / 11.76 & 45.77 / 12.28 & 46.79 / 12.50 \\
**5** & 54.33 / 17.34 & 59.35 / 18.36 & 60.37 / 18.85 & 52.40 / 14.41 & 57.42 / 16.63 & 58.44 / 16.65 & 41.50 / 11.51 & 45.53 / 12.03 & 46.55 / 12.26 \\   

Table 2: Our performance (%) on AeroEye with shift values (\(\) in Eqn. (3)) at Recall (R) and mean Recall (mR).

    &  &  &  \\  & **R/m@R20** & **R/m@50** & **R/m@100** & **R/m@20** & **R/m@50** & **R/m@100** & **R/m@20** & **R/m@50** & **R/m@100** \\ 
**1** & **56.20 / 19.23** & **61.43 / 20.67** & **62.40 / 21.19** & **54.55 / 16.22** & **59.59 / 18.20** & **60.37 / 18.38** & **43.53 / 13.29** & **47.93 / 13.69** & **48.94 / 13.86** \\
**2** & 55.05 / 18.82 & 60.39 / 19.20 & 61.15 / 20.50 & 53.07 / 15.00 & 58.40 / 17.84 & 59.16 / 18.01 & 42.66 / 13.02 & 46.97 / 13.41 & 47.96 / 13.58 \\
**3** & 53.98 / 18.42 & 59.18 / 19.85 & 59.93 / 20.34 & 51.99 / 15.58 & 57.23 / 17.49 & 57.97 / 17.67 & 41.81 / 12.75 & 46.03 / 13.14 & 47.00 / 13.30 \\
**4** & 52.90 / 18.04 & 57.99 / 19.46 & 58.73 / 19.93 & 50.95 / 15.37 & 56.08 / 17.14 & 56.81 / 17.30 & 40.97 / 12.90 & 45.11 / 12.87 & 46.06 / 13.03 \\
**5** & 51.84 / 17.66 & 56.83 / 19.08 & 57.55 / 19.53 & 49.93 / 14.96 & 54.96 / 16.80 & 55.68 / 16.96 & 40.17 / 12.24 & 44.23 / 12.61 & 45.14 / 12.76 \\   

Table 3: Our performance (%) on AeroEye for varying frames per video at Recall (R) and mean Recall (mR).

[MISSING_PAGE_FAIL:9]

improves the mean Recall, as the experimental results demonstrate. These results highlight the robustness of our CYCLO approach in recognizing and handling periodic actions, such as cooking, washing, cleaning, exercising, and other routine tasks frequently represented on the PVSG dataset.

**Performance on _ASPIRe_.** The ASPIRe dataset includes five distinct interactivity types. However, we focus on position and relation to ensure a fair comparison. As shown in Table 7, CYCLO consistently outperforms existing models across multiple recall and mean recall thresholds. Notably, at R@20, our CYCLO approach outperforms the HIG method, the second-best model, by 0.69% in position and a more substantial 5.03% in relation. Additionally, CYCLO shows significant gains in mean Recall across all evaluated thresholds, demonstrating its effectiveness in tackling the long-tail distribution.

## 6 Conclusions

We have introduced CYCLO, a novel approach that effectively captures periodic and overlapping relationships, handles extended sequences, and minimizes information loss, making it suitable for complex temporal modeling. In addition, we presented AeroEye, a comprehensive and diverse dataset composed of drone-captured scenes, specifically designed to represent intricate object relationships and spatial positions in aerial videos. Through extensive experiments on the AeroEye dataset and two large-scale in-the-wild datasets (_i.e_. ASPIRe and PVSG), we demonstrated the robustness and effectiveness of CYCLO in capturing dynamic interactions and evolving relationships over time.

**Limitations.** Although our CYCLO approach has achieved impressive performance, it may reveal limitations when dealing with incomplete or discontinuous videos. The periodic and cyclic attention mechanisms, crucial for capturing temporal and spatial object relationships, heavily rely on video continuity and completeness. Interruptions in the sequence, such as missing or discontinuous frames, disrupt the formation of accurate cyclical references, leading to inconsistent and incorrect predictions.

**Broader Impacts.** The proposed approach improves the capture of object interactions and temporal evolution in aerial and in-the-wild videos, which is critical for surveillance, disaster response, traffic management, and precision agriculture applications. By modeling object interactions over time, CYCLO supports more informed decision-making, leading to safer and more sustainable practices. This advancement opens the door for future work developing surveillance systems that can model complex relationships from drone videos. However, it is important to recognize the potential risks associated with this approach, particularly the possibility of using it for unauthorized surveillance.

**Acknowledgment.** This work is partly supported by J.B. Hunt Transport Services (JBHunt), NSF Data Science and Data Analytics that are Robust and Trusted (DART), NSF SBIR Phase 2, and Arkansas Biosciences Institute (ABI) grants. We also acknowledge Thanh-Dat Truong for invaluable discussions and the Arkansas High-Performance Computing Center (AHCC) for providing GPUs.

Figure 7: Scene graphs generated by the CYCLO model on the AeroEye dataset, illustrating dynamic relationships between objects and agents across UAV-captured frames. (Best viewed in colors)