# Can Language Models Perform Robust Reasoning

in Chain-of-thought Prompting with Noisy Rationales?

Zhanke Zhou\({}^{1}\)  Rong Tao\({}^{1}\)  Jianing Zhu\({}^{1}\)  Yiwen Luo\({}^{2}\)  Zengmao Wang\({}^{2}\)  Bo Han\({}^{1}\)

\({}^{1}\)TMLR Group, Hong Kong Baptist University \({}^{2}\)Wuhan University \(\{\)cszzzhou, csrongtao, csjnzhu, bhanmll}@comp.hkbu.edu.hk

{luoyiwen, wangzengmao}@whu.edu.cn

Correspondence to Bo Han (bhanmll@comp.hkbu.edu.hk).

###### Abstract

This paper investigates an under-explored challenge in large language models (LLMs): chain-of-thought prompting with _noisy rationales_, which include irrelevant or inaccurate reasoning thoughts within examples used for in-context learning. We construct NoRa dataset that is tailored to evaluate the robustness of reasoning in the presence of noisy rationales. Our findings on NoRa dataset reveal a prevalent vulnerability to such noise among current LLMs, with existing robust methods like self-correction and self-consistency showing limited efficacy. Notably, compared to prompting with clean rationales, GPT-3.5 drops by \(1.4\%\)-\(19.8\%\) in accuracy with irrelevant thoughts and more drastically by \(2.2\%\)-\(40.4\%\) with inaccurate thoughts.

Addressing this challenge necessitates external supervision that should be accessible in practice. Here, we propose the method of contrastive denoising with noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning capabilities by contrasting noisy rationales with _only one clean rationale_, which can be the minimal requirement for denoising-purpose prompting. This method follows a principle of exploration and exploitation: (1) rephrasing and selecting rationales in the input space to achieve explicit denoising and (2) exploring diverse reasoning paths and voting on answers in the output space. Empirically, CD-CoT demonstrates an average improvement of \(17.8\%\) in accuracy over the base model and shows significantly stronger denoising capabilities than baseline methods. The source code is publicly available at: [https://github.com/tmlr-group/NoisyRationales](https://github.com/tmlr-group/NoisyRationales).

**Question-1 (Q1):** In base-9, what is 86+57?

**Reunion-1 (R1):** In base-9, the digits are

"012345678". We have \(6+7=13\) in base-10. Since we're in base-9, that exceeds the maximum value of 8 for a single digit. 13 mod 9 = 4, so the digit is 4 and the carry is 1. We have \(8+5+1=14\) in base 10. 14 mod 9 = 5, so the digit is 5 and the carry is 1. A leading digit 1. So the answer is 154.

**Answer-1 (A1):** 154.

...Q2, R2, A2, Q3, R3, A3...

**Test Question:** In base-9, what is 62+58?

**Question-1 (Q1):** In base-9, what is 86+57?

**Question-1 (Q1):** In base-9, what is 86+57?

**Rationale-1 (R1):** In base-9, the digits are

"012345678". We have \(6+7=13\) in base-10. Since we're in base-9, that exceeds the maximum value of 8 for a

single digit.13 mod 9 = 4, so the digit is 4 and the carry is 1. We have \(8+5+1=14\) in base 10. 14 mod 9 = 5, so the digit is 5 and the carry is 1. 5 + 9 = 14. A leading digit is

1. So the answer is 154.

**Answer-1 (A1):** 154.

...Q2, R2, A2, Q3, R3, A3...

**Test Question:** In base-9, what is 62+58?

Figure 1: Exemplars of noisy questions  and _noisy rationales_ (our new research problem). Each input includes three prompting examples and one test question. Notably, the test question asks about base-9 calculation, while the misguiding base-10 information is given in noisy questions or rationales.

## 1 Introduction

In-context learning (ICL) is a common approach in large language models (LLMs), enabling models to extrapolate from a few examples and adapt without fine-tuning [4; 84; 16]. However, ICL's efficacy is closely tied to the quality and clarity of the prompting examples, particularly in the prevailing chain-of-thought (CoT) strategy that provides rationales, _i.e._, intermediate reasoning steps to solve a question . Recent research has shown that LLMs struggle with noisy questions: they are easily distracted by irrelevant context and exhibit instability with slight input modifications [68; 78; 107].

Notably, this work shifts focus from the well-studied noisy questions (Noisy-Q) problem to the under-explored _noisy rationales_ (Noisy-R) problem, wherein _factually inaccurate or irrelevant reasoning steps_ are paired with valid question-answer examples, as illustrated in Fig. 1. Here, the emphasis on Noisy-R is due to its practical challenges, with examples drawn from diverse sources such as crowd-sourced platforms, dialogue systems, and machine-generated data 2[25; 45; 73; 2; 77; 48]. However, the robustness of LLMs against Noisy-R remains unknown. A new benchmarking dataset is needed to conduct a systematic evaluation of current LLMs and verify the corresponding countermeasures.

In this work, we first construct the NoRa (**No**isy **R**ationales) dataset, a comprehensive testbed to evaluate the robustness of LLM reasoning against noisy rationales across various reasoning domains (in Sec. 3). The NoRa contains a total of \(26391\) questions, covering three types of reasoning tasks: mathematical, symbolic, and commonsense. We uniformly formalize the generation of noisy rationales by inserting irrelevant or inaccurate thoughts, controlling reasoning difficulty through noise ratios, and guaranteeing the overall prompting correctness without modifying the question or answer.

With the NoRa dataset, we evaluate several LLMs and reveal that all of them are _intrinsically vulnerable_ to noisy rationales (in Sec. 4). For example, compared to prompting with clean rationales, GPT-3.5 exhibits an average \(3.0\%\) - \(33.3\%\) decrease in accuracy with noisy rationales, as in Fig. 2. Besides, only limited improvements are achieved with existing robust methods based on the model's intrinsic denoising ability, _e.g._, self-consistency  and self-denoise . We show that Noisy-R is much more challenging than Noisy-Q, requiring context-specific knowledge to guide the denoising.

To solve this, we propose to rectify the rationales with _only one clean CoT demonstration_ that can be the most attainable supervision in practice (in Sec. 5). We assume that LLMs can rectify rationales by _contrasting_ a noisy rationale with a clean one, as in Fig. 3. Guided by this principle, we design the framework of **C**ontrastive **D**enoising with noisy **CoT** (CD-CoT) with four steps: rationale rephrasing, rationale selecting, rationale exploring, and answer voting. Technically, the first two steps aim to achieve explicit denoising, while the last two steps are for diverse reasoning paths. Empirically, CD-CoT achieves an average improvement of \(17.8\%\) in accuracy _w.r.t._ the base model (refer to Tab. 8). Notably, it presents much stronger denoising power than baselines in rectifying the rationales.

**Contributions.** To our best knowledge, we are the _first_ to investigate the problem of noisy rationales.

* We formalize the under-explored noisy rationale problem in the prevailing chain-of-thought prompting and construct the NoRa dataset to benchmark the robustness of LLMs against noisy rationales (Sec. 3).
* We systematically evaluate LLMs with NoRa dataset and extract several insightful observations, _e.g._, the unsatisfactory robustness and limited denoising power of LLMs under noisy rationales (Sec. 4).
* We propose to rectify the noisy rationales with only one clean CoT demonstration, design a simple yet effective method, CD-CoT, and verify its effectiveness through comprehensive experiments (Sec. 5).

Figure 3: Chain modeling of the noisy rationale problem: Recovering chain (3) from chain (1) with the guidance of chain (2). From question \(x_{i}\) to answer \(y_{i}\), the rationale of chain (3) includes clean thoughts \(T_{i}^{(j)}\) and noisy thoughts \(_{i}^{(j)}\).

Figure 2: Results of GPT-3.5 with 0-shot, 3-shot clean rationales, and 3-shot noisy rationales: Both inaccurate and irrelevant rationales degenerate performance significantly, while the proposed CD-CoT improves robustness against noisy rationales.

Related Work

**Limitations of in-context learning (ICL).** Though effective, ICL suffers from the susceptibility to manual generation and selection of demonstrations (examples), where the ultimate performance is closely tied to the demonstrations' quality and clarity. Recent investigations on _noisy questions_ have shown that (i) LLMs can be distracted by irrelevant or adversarial context, as they are designed to pay close attention to the context provided in the prompt [32; 58; 68; 78] and (ii) LLM reasoning is unstable, namely, small modifications to the prompt could potentially cause large variations in the model's output [102; 107]. Besides, another line of research regarding _noisy answers_[42; 18] justifies the feasibility of misleading an LLM to agree factual errors such as "1+1=3" in base-10 calculation.

**Countermeasures.** Two intrinsic traits of LLMs are desirable for addressing the above limitations:

* _Self-correction_, wherein LLMs attempt to correct their initial responses based solely on their inherent capabilities without external feedback, _e.g._, by refining prompts through iterative corrections of responses or question trajectories [91; 89]. Although LLMs can learn to ignore irrelevant information by examples or instructions , they are proved to be still struggling to correct their responses without external feedback, and at times, their performance might even degrade after self-correction [29; 81].
* _Self-consistency_ aims to obtain a consistent answer against input perturbations. This is achieved by generating multiple responses via randomized smoothing on input questions  or diverse paths for answering one question  followed by the answer aggregation. This strategy brings improvements with extra costs for repeated reasoning. Moreover, it cannot explicitly rectify questions or rationales.

**Noisy rationales**, as the research focus of this work, mainly originates from (1) the inherent imperfections, inconsistencies, and inaccuracies of humans' cognitive processes [53; 10] and (2) the diversity, unpredictability, and hallucination of the LLMs' generative mechanisms [103; 30; 101]. A detailed literature review and discussion of noise rationales are in Appendix B and C, respectively.

## 3 The NoRa Dataset

In this section, we introduce the NoRa (**N**oisy **R**ationales) dataset for benchmarking the robustness against noisy rationales. NoRa consists of \(26391\) questions and \(5\) subsets, covering mathematical, symbolic, and commonsense reasoning tasks, where ICL and CoT demonstrations play a crucial role.

### Definition of Noisy Rationales

We start by formalizing the ICL and CoT demonstrations. Given a test question \(x_{}\) and an LLM \(f_{}\), one expects to get the correct answer \(y_{}\) as \(f_{}(x_{})\!\!y_{}\). This zero-shot manner cannot guarantee effectiveness, especially when encountering unfamiliar contexts or scenarios. To boost effectiveness, the ICL techniques prompt the LLM with a few examples \(S_{n}\!=\!\{(x_{i},y_{i})\}_{i=1}^{n}\) collected in the current context, each composed of a question \(x_{i}\) and answer \(y_{i}\), and then construct the new input \(x_{}\) as

\[x_{}=[S_{n},x_{}]=[x_{1},y_{1},,x_{ n},y_{n},\ x_{}]. \]

The guidance by \(S_{n}\) makes \(f_{}(x_{})\!\!y_{}\) much easier than \(f_{}(x_{})\!\!y_{}\). Then, the CoT further refines \(x_{}\) by constructing the step-by-step rationale \(_{i}\), consisting of several thoughts \(T_{i}^{(j)}\), namely,

\[x_{}=[x_{1},_{1},y_{1},,x_{n},_{n}, y_{n},\ x_{}],\ \ \ \ _{i}=[T_{i}^{(1)},T_{i}^{(2)},T_{i}^{(3)},,T_{i}^{(k)}]. \]

However, as aforementioned, the thoughts in CoT (Eqn. 2) can be noisy in practice. This noise can be attributed to (1) _irrelevant thoughts_, which are irrelevant but correct, or (2) _inaccurate thoughts_, which are relevant but factually wrong. Here, we _uniformly_ formalize these two kinds of noise as

\[_{i}}=[T_{i}^{(1)},_{i}^{(1)},T_{i}^{(2)},_{ i}^{(2)},,T_{i}^{(k)},_{i}^{(k)}], \]

where \(_{i}^{(j)}\) represents a noisy thought (irrelevant or inaccurate) that is coherent with the previous clean thought \(T_{i}^{(j)}\) (relevant and correct). The following introduces the definition of noisy thoughts.

**Irrelevant thoughts** refer to incorporating irrelevant information unhelpful for solving the question, _e.g._, discussing the genetic overlap of siblings when the task is to deduce family roles in relationship reasoning. Redundant information may be introduced by the LLM's diverse response generation or 

[MISSING_PAGE_FAIL:4]

## 4 Evaluating Language Models on NoRa dataset

In this section, we comprehensively evaluate representative LLMs and robust methods on the newly constructed NoRa dataset. We first introduce the basic evaluation setups and then present several observations on the _unsatisfactory robustness_ of current LLMs and methods under noisy rationales.

**Baseline methods.** We select five representative methods as baselines to ensure a comprehensive assessment that encompasses the two traits of self-correction and self-consistency. ISC  and SP  exemplify self-correction, focusing on response rectification and prompt rephrasing, respectively. SM , SD , and SC  fall under self-consistency: SM  injects perturbations into prompts for robustness, SD  masks prompts and asks LLMs to reconstruct them, while SC directly samples outputs without denoising. These methods are further introduced in Appendix E.1.

**LLM basis.** We employ GPT-3.5-turbo-0613  as our base LLM (denoted as \(}\)) for the analyses presented in this study. In addition, we conduct evaluations on three supplementary models, including Gemini-Pro (Jan. 2024) , Llama2-70B , and Mixtral-8x7B . For all baselines, we consistently set the temperature parameter \(\) to the value of 1. In order to obtain consistent results, we evaluate 300 questions for each task and repeat the model reasoning five times for each question.

**Evaluation metric.** Given a set of test question \(=\{(x_{},y_{})\}\) and a set of CoT-prompting examples \(=[x_{1},_{1},y_{1},,x_{n},_{n},y_{n}]\), we define the accuracy of the denoising method \(\) with a specific LLM \(f_{}\), namely, \((,,)=_{(x_{})}}{{2}}( ,x_{})]}}{{[]}}\). We report the results in percentage (%) with one decimal point. Therein, \((,,_{})\), \((,,_{})\), and \((,,_{})\) indicate accuracy with clean, irrelevant, and inaccurate rationales, respectively. When there is no prompting example, _i.e._, \(=\), then \((,,)\) represents the zero-shot result.

**Unreliability revealing with noisy rationales.** We conduct the reasoning tasks on LLM with Noisy-R and summarize the results in Tab. 3. _Overall, the base LLM with all the existing reasoning methods is severely affected by irrelevant or inaccurate noise, with overall showing a \(0.2\%\)-\(25.3\%\) decrease with irrelevant noise and a more drastic \(0.1\%\)-\(54.0\%\) decrease with inaccurate noise compared with clean rationales._ While robust methods like SP and SD exhibit resilience to noise on partial tasks, their performance remains inconsistent and often declines. To further reveal the unreliability, we start by analyzing the two categories of robust methods mentioned above in the following observations.

    & \)} & ,,_{})\)} & (,,_{})\)} & (,,_{})\)} \\  & & & Easy & Medium & Hard & Avg. & Easy & Medium & Hard & Avg. \\   & Base & 46.4 & 39.3 & 30.3 & 26.6 & 32.1 & 28.2 & 10.1 & 6.0 & 13.1 \\  & w/ ISC  & 24.3 & 17.7 & 14.7 & 12.7 & 15.0 & 18.4 & 13.7 & 12.3 & 14.8 \\  & w/ SP  & 26.2 & 25.5 & 25.5 & 21.9 & 24.3 & 20.0 & 18.4 & **14.3** & 17.6 \\ Base-9 & w/ SM  & 37.4 & 30.0 & 22.7 & 16.5 & 23.1 & 24.7 & **19.2** & 12.4 & **18.8** \\  & w/ SD  & 47.9 & 37.2 & 25.4 & 24.7 & 29.1 & 29.3 & 12.5 & 8.7 & 16.8 \\  & w/ SC  & **61.5** & **51.1** & **39.0** & **36.2** & **42.1** & **32.7** & 15.3 & 7.5 & 18.5 \\   & Base & 23.9 & 19.1 & 13.6 & 10.7 & 14.5 & 14.0 & 6.7 & 3.6 & 8.1 \\  & w/ ISC  & 11.2 & 8.3 & 7.8 & 6.0 & 7.4 & 6.5 & 5.2 & 4.7 & 5.5 \\  & w/ SP  & 20.7 & 17.5 & 16.7 & 14.0 & 16.0 & 14.1 & **10.7** & **10.8** & **11.9** \\ Base-11 & w/ SM  & 16.3 & 12.0 & 6.0 & 5.7 & 7.9 & 12.0 & 9.3 & 7.7 & 9.7 \\  & w/ SD  & 17.9 & 12.3 & 12.0 & 13.3 & 12.5 & 17.0 & 8.7 & 5.3 & 10.3 \\  & w/ SC  & **33.7** & **25.3** & 16.3 & **15.0** & **18.9** & **19.7** & 9.3 & 3.3 & 10.8 \\   & Base & 32.7 & 28.1 & 25.1 & 23.0 & 25.4 & 29.1 & 26.1 & 22.7 & 26.0 \\  & w/ ISC  & 23.9 & 20.0 & 16.3 & 15.5 & 17.3 & 19.2 & 18.3 & 18.1 & 18.5 \\  & w/ SP  & 23.2 & 23.0 & 22.6 & 22.7 & 22.8 & 23.7 & 22.5 & 23.2 \\  & w/ SM  & 25.0 & 20.7 & 19.7 & 16.7 & 19.0 & 21.0 & 20.3 & 20.0 & 20.4 \\  & w/ SD  & 9.9 & 10.1 & 10.9 & 10.3 & 10.4 & 10.1 & 10.9 & 10.4 & 10.5 \\  & w/ SC  & **35.3** & **31.0** & **28.3** & **27.0** & **28.8** & **33.3** & **30.7** & **26.0** & **30.0** \\   & Base & 9.2 & 6.3 & 7.2 & 6.0 & 6.5 & 7.0 & 6.8 & 6.0 & 6.6 \\  & w/ ISC  & 4.9 & 4.6 & 2.7 & 3.7 & 3.7 & 3.4 & 4.3 & 3.5 & 3.7 \\   & w/ SP  & 5.1 & 4.3 & 4.1 & 3.9 & 4.1 & 4.9 & 4.0 & 4.5 & 4.5 \\   & w/ SM  & 1.7 & 0.7 & 0.7 & 1.3 & 1.0 & 1.3 & 0.7 & 0.3 & 0.8 \\   & w/ SD  & 0.1 & 0.1 & 0.1 & 0.2 & 0.1 & 0.1 & 0.3 & 0.0 & 0.1 \\   & w/ SC  & **13.0** & **7.7** & **9.0** & **6.3** & **7.7** & **8.0** & **8.0** & **8.7** & **8.2** \\   & Base & 45.7 & 44.3 & 42.3 & 41.4 & 42.7 & 36.7 & 33.4 & 28.3 & 23.8 \\  & w/ ISC  & 21.8 & 24.3 & 22.5 & 21.4 & 22.7 & 20.3 & 26.5 & 24.0 & 24.6 \\   & w/ SP  & 47.9 & 48.2 & 46.7 & 48.1 & 47.7 & 49.6 & 46.6 & 45.5 & 47.6 \\   & w/ SM  & 53.3 & 50.3 & 50.0 & 36.7 & 49.0 & 47.7 & 49.0 & 49.3 & **48.7** \\   & w/ SD  & **54.0** & **58.3** & **57.3** & **57.7** & **57.8** & **57.0** & **58.3** & **53.7** & **56.3** \\   & w/ SC  & 52.0 & 46.3 & 45.0 & 44.7 & 45.3 & 44.7 & 44.7 & 38.0 & 42.5 \\   

Table 3: Reasoning accuracy on NoRa dataset with 3-shot prompting examples with clean, irrelevant, or inaccurate rationales. The **boldface** numbers mean the best results, while the underlines numbers indicate the second-best results. Note the referenced results of \(}\) are highlighted in gray.

_Observation 4.1_.: **Self-correction methods perform poorly on most tasks with noisy rationales.** Therein, ISC  and SP  rely on the inherent capabilities of LLMs to enhance the quality of generated responses. However, in the absence of external feedback, the model's self-correction ability in reasoning tasks is limited, often resulting in the miscorrection of the given content (see Tab. 12). SP can only slightly improve the accuracy of commonsense tasks, while ISC performs unsatisfactorily across all tasks. As can be seen from Tab. 3, these methods perform even worse than the base model.

_Observation 4.2_.: **Self-consistency methods can improve robustness without true denoising.** Two self-consistency approaches, SM  and SD , are originally proposed to address Noisy-Q issues. When applied to our Noisy-R scenarios, they tend to easily disrupt the intrinsic logical coherence within the thought chain. Although these methods utilizing smooth strategies (_e.g._, random smoothing or masking) perform well on the commonsense dataset, they can hardly handle the more difficult reasoning tasks and even degenerate close to 0%, _e.g._, in the Symbolic Longer task. Another method, SC , performs better than the base model in all tasks, improving both clean and noisy reasoning performance. However, SC does not conduct explicit denoising on rationales during its reasoning procedure. In addition, SC also requires a high computation cost (refer to Appendix F.2).

Besides these methods, next, we analyze LLMs' _intrinsic_ properties under noisy rationales as follows.

_Observation 4.3_.: **Adjusting model temperature can help reasoning under noisy rationales.** In Tab. 4, we evaluate the base LLM using different temperatures on 3-shot demonstrations. Overall, reducing temperature can enhance the model's accuracy under both noisy and clean rationale reasoning, compared to the default temperature of 1. However, the relationship between temperature and accuracy is not linear for noisy reasoning; instead, there are multiple peaks in accuracy within the temperature range of \(0\) to \(1\). Additionally, it is found that excessively low temperatures (_e.g._, 0) tend to result in verbose and repeated responses, which cause the model to exceed token limits up to \(30\%\) in symbolic tasks where the length of expected answers is quite variable among different questions.

_Observation 4.4_.: **Prompting with more noisy examples boosts reasoning accuracy on most tasks.** In Tab. 5, we evaluate the model using different numbers of exemplars while keeping the temperature at 1. In general, the LLM's accuracy will still improve as the number of noisy examples increases in the clean and noisy settings. However, it should be noted that in tasks with high-level noise from NoRa-Math, increasing prompting examples can degenerate accuracy. For example, in the base-9 inaccurate-hard task, prompting with noisy rationales is even worse than the 0-shot accuracy of \(7.2\%\). Further, we provide a deeper analysis of increasing the number of noisy examples in Appendix F.6.

   Task & Setting &  &  \\  & & 0 & 0.3 & 0.5 & 0.7 & 1 \\   & clean & **61.0** & 69.0 & 57.5 & 55.3 & 46.4 \\  & ina. easy & 78.0 & 78.0 & 69.7 & 55.3 & 46.4 \\  & ina. hard & 5.0 & 78.0 & 69.7 & 55.3 & 46.4 \\   & clean & **34.9** & 33.8 & 31.6 & 29.8 & 23.9 \\  & inr. early & 21.7 & 23.1 & 23.3 & 23.3 & 24.9 \\  & inr. hard & 21.0 & **17.5** & 15.5 & 14.1 & 10.7 \\   & clean & 34.2 & **35.8** & 35.7 & 34.6 & 32.7 \\  & inr. easy & 26.8 & 34.5 & 29.2 & 29.1 & 28.1 \\  & inr. hard & 27.0 & 26.1 & 22.4 & 20.4 & 22.7 \\   & clean & 6.3 & 8.3 & 8.9 & **9.3** \\  & ina. easy & 5.0 & 7.3 & **8.6** & 8.7 & 7.0 \\  & inr. hard & 4.0 & 6.1 & **6.3** & 8.2 & 60.0 \\   & clean & **34.9** & 33.8 & 31.6 & 29.8 \\  & inr. early & 21.7 & 13.5 & 29.1 & 27.6 \\   & inr. hard & 21.0 & **17.5** & 15.5 & 14.1 & 10.7 \\   & clean & 34.2 & **35.8** & 35.7 & 34.6 & 32.7 \\   & inr. easy & 26.8 & 34.5 & 29.2 & 29.1 & 28.1 \\   & inr. hard & 27.0 & 26.1 & 22.4 & 20.4 & 22.7 \\   & clean & 2.7 & 7.9 & 31.3 & **12.2** \\   & inr. easy & 2.3 & 5.4 & 7.0 & 8.3 & 8.9 \\   & inr. hard & 1.9 & 4.0 & **6.3** & **6.3** \\   

Table 4: Comparing performances of the base model with different temperatures. a varying number of examples Sym.(E)/(L) are symbolic tasks. (“—” denotes over token limit).

   Task & Zero-shot & Few-shot (No Shuffle) & Shuffle Questions \(x_{i}\) & Shuffle Rationales \(_{i}\) & Shuffle Answers \(y_{i}\) \\  Math Base-9 & 7.2 & **46.4** & 45.5 (0.9\%\(\)) & 34.5 (11.9\%\(\)) & 35.7 (10.7\%\(\)) \\  Math Base-11 & 5.5 & **—** & 23.9 & **1** & **24.8** (0.9\%\(\)) & 21.6 (2.3\%\(\)) & 21.1 (11.7\%\(\)) \\  Symbolic Equal & 8.8 & **—** & 32.7 & **32.7** (0.0\%\(\)) & **32.8** (0.1\%\(\)) & 32.3 (0.4\%\(\)) \\  Symbolic Longer & 0.0 & **—** & **9.2** & **1** & **7.0** (2.2\%\(\)) & 6.2 (3.0\%\(\)) & 6.3 (2.9\%\(\)) \\  Commonsense & 40.0 & **—** & **45.7** & **1** & **38.7** (0.7\%\(\)) & 39.7 (6.0\%\(\)) & 39.8 (5.9\%\(\)) \\   

Table 7: Performance (in accuracy%) on NoRa dataset under different few-shot shuffle configurations.

**Observation 4.5**.: **Different LLMs are generally vulnerable to noisy rationales.** In Tab. 6, we evaluate different LLMs across three settings: 0-shot CoT, 3-shot clean rationales, and 3-shot medium-level noisy rationales. Notably, Gemini-Pro outperforms GPT-3.5 in overall performance. However, it demonstrates a similar degree of sensitivity to noise, with a \(2.4\%\)-\(15.7\%\) performance decline with irrelevant rationales and a \(7.8\%\)-\(66.8\%\) decline with inaccurate rationales compared to clean rationales. While Mixtral 8x7B shows a slight underperformance compared to GPT-3.5, it also manifests a vulnerability to noise, incurring a \(1.4\%\)-\(11.2\%\) loss with irrelevant rationales and a greater \(4.2\%\)-\(23.8\%\) loss with inaccurate rationales. By contrast, Llama2-70B performs suboptimally, with a \(0.4\%\)-\(2.0\%\) drop for irrelevant thoughts and a larger \(1.0\%\)-\(2.2\%\) drop for inaccurate thoughts.

**Further investigation.** Inspired by Min et al. , we further explore the mapping among questions, rationales, and answers through shuffling experiments. Specifically, given the 3-shot prompting examples \(\{(x_{1},_{1},y_{1}),(x_{2},_{2},y_{2}),(x_{3},_{3},y_{3})\}\), we test three configurations, _i.e._, shuffle questions \(\{(x_{1},_{3},y_{3}),(x_{2},_{1},y_{1}),(x_{3},_{2},y_{2})\}\), shuffle answers \(\{(x_{1},_{1},y_{3}),(x_{2},_{2},y_{1}),(x_{3},_{3},y_{2})\}\), and shuffle rationales \(\{(x_{1},_{3},y_{1}),(x_{2},_{1},y_{2}),(x_{3},_{2},y_{3})\}\). These break the original mappings. The results under these configurations are shown in Tab. 7, which induces the following observation.

**Observation 4.6**.: **Shuffling the mappings of prompting examples degenerates the reasoning but still performs better than without prompting.** This means that while LLMs may not heavily rely on the exact mapping (of question, rationale, and answer), they still benefit from demonstrating information even with shuffling. Notably, this finding is consistent with the conclusions of  that LLMs learn more abstract task information from the demonstrations rather than simply memorizing question-answer pairs. _More importantly, LLMs are less vulnerable to shuffled mappings than noisy rationales_. Unlike shuffling, the irrelevant or inaccurate information in noisy rationales introduces misleading elements that significantly interfere with the model's ability to learn _correct_ task patterns, thereby resulting in more severe performance degradation. This extends 's finding and shows that the quality of reasoning steps can be more crucial than the exact mapping of prompting examples.

## 5 Method

This section aims to enable LLMs to discern and remove noisy thoughts. The observations in Sec. 4 and previous works show that current LLMs cannot achieve this with their intrinsic denoising ability, even enhanced with self-denoising methods. Therefore, we would claim that the _external supervision_ is necessary for enhancement, which should be sufficient for denoising and accessible in practice. Existing methods with external supervision  require (1) oracle feedback on the test question, (2) human feedback of errors on specific tokens or positions, or (3) expert knowledge to construct detailed descriptions of specific tasks. By contrast, we believe that _a clean CoT demonstration_ is more attainable and practical, which can be the _minimal requirement_ for denoising-purpose prompting.

Therein, we assume that LLMs _can_ identify noisy thoughts by _contrasting_ a pair of noisy and clean rationales and discerning their differences, similar to contrastive learning . Here, the denoising power could come from the abilities of the instruction following and step-by-step reasoning . Hence, we propose the framework of CD-CoT, Contrastive **D**enoising with noisy **CoT**. The design principle is to explore and then exploit, _i.e._, (1) rephrasing and selecting rationales in input space to achieve explicit denoising, and then (2) exploring diverse rationales and voting answers in output space for deriving the final answer, as in Figs. 4 & 5. The details are as follows.

Figure 4: CD-CoT’s first two steps for data denoising. First, it rephrases the \(i\)-th noisy example by contrasting it with the clean example. Then, with the obtained \(N\) rephrased examples, it selects the \(M\) qualified candidates by checking the validity of the rephrased answers \(_{i1},,_{iN}\)_w.r.t._\(y_{i}\).

Figure 5: CD-CoT constructs \(M\) inputs (\(K\)-shot) by allocating the \(K M\) rephrased rationales. These inputs are concatenated with the clean example and test question and then fed to an LLM for reasoning separately. The obtained \(D\) answers are equally voted to obtain the final answer \(y\).

```
1:an LLM \(f_{}\), the prompt of contrastive denoising \(_{}\), one test question \(x_{}\), one clean example \((x_{},_{},y_{})\), K prompting examples \(S_{n}=\{(x_{i},_{i},y_{i})\}_{i=1}^{K}\), hyper-parameters \(N,M\), and reasoning budget \(\{B_{i}\}_{i=1}^{M}\) (satisfies that \(_{i=1}^{M}B_{i}=D\), where \(D\) is the total budget).
2:for\(i=1 K\)do
3: initialize the set of rephrased results of \(i\)-th example \(_{i}\).
4:for\(j=1 N\)do
5:\(\#\) Step-1: Rationale Rephrasing via Supervised Contrasting
6: obtain a rephrased example as \((x_{i},}_{i},_{i}) f_{}_{}(x_{},_{},y_{},x_{i}, _{i},y_{i})\).
7:if match answer \(_{i}=y_{i}\), then store the rephrased example as \(_{i}_{i}\{(x_{i},}_{i}, _{i})\}\).
8:endfor
9: randomly select \(M\) rephrased examples from \(_{i}\) and obtain \(}_{i}=\{(x_{is},}_{is},_{is})\}_{s=1} ^{M}\).
10:endfor
11:\(\#\) Step-3: Rationale Exploration
12: initialize the set of answers \(\).
13:for\(i=1 M\)do
14: construct an input \(_{i}\{(x_{ji},}_{ji},_{ji})\}_{j=1} ^{K}\), where \((x_{ji},}_{ji},_{ji})\) is the \(i\)-th element of \(}_{j}\).
15: concatenate \(_{i}\) with the clean example and test question as \(_{i}_{i}\{(x_{},_{ {C}},y_{}),x_{}\}\).
16:for\(j=1 B_{M}\)do
17: get one answer by LLM reasoning as \(y_{j} f_{}(_{i})\).
18: store the answer as \(\{y_{j}\}\).
19:endfor
20:endfor
21:\(\#\) Step-4: Answer Voting
22: initialize the dictionary of answer count \(\) that \( y_{j},[y_{j}]=0\).
23:for\(j=1 D\)do
24: update \([y_{j}]([y_{j}]+1)\).
25:endfor
26: get the final answer \(y\) with maximum counts as \(y_{y}[y]\).
27:return the answer \(y\).
```

**Algorithm 1** CD-CoT: Contrastive Denoising with Noisy Chain-of-Thought.

### Implementation

**Step-1: Rephrasing via Supervised Contrasting (\(1\) to \(N\)).** First, we establish a general prompt of contrastive rephrasing to construct a pair of contrastive examples, as shown in the template below. This steers the model towards learning from the clean example and then rephrasing and rectifying the noisy examples. To be specific, given one clean example and \(K\) noisy examples, we generate \(N\) rephrased rationales for each noisy example independently and obtain \(K N\) rephrased rationales.

```
1:\(\#\) Step-1: Rationale Rephrasing via Supervised Contrasting (\(N\) to \(N\)).
2:for\(j=1 D\)do
3: update \([y_{j}]([y_{j}]+1)\).
4:endfor
5:get the final answer \(y\) with maximum counts as \(y_{y}[y]\).
6:return the answer \(y\).
```

**Algorithm 2** T-Convergence of Contrastive Rephrasing via Supervised Contrasting

**Step-2: Rationale Selection (\(N\) to \(M\), \(N M\)).** Next, we employ answer matching to select those rephrased examples with unchanged answers, leaving behind a refined candidate pool. Subsequently, we randomly select \(M\) rephrased rationales from the pool and concatenate them to form the contexts.

**Step-3: Rationale Exploration (\(M\) to \(D\), \(M D\)).** For the \(M\) different contexts, we explore rationales by repeated reasoning with the budget of \(D\) reasoning repetitions. Notably, a higher temperature parameter, _e.g._, 1, is set to introduce more randomness in generating diverse rationales.

**Step-4: Answer Voting (\(D\) to \(1\)).** Ultimately, all the \(D\) answers are equally voted into a final answer.

**Instantiation.** By tuning the hyper-parameters \(N\), \(M\), and \(D\), we balance exploration and exploitation in the input and output space. The overall procedure of our proposed CD-CoT is presented in Algorithm 1. Besides, we further explain the details of each step of this algorithm in Appendix E.2.

**Theoretical analysis.** To understand the underlying mechanism of CD-CoT, we also conduct the theoretical analysis based on the distinguishability  of in-context learning. The full analysis is in Appendix D, where we find that the noisy demonstration in ICL can decrease the distinguishability of in-context matching with the clean-prompt distribution, while our method can mitigate this issue. Besides, we build a self-supervised variant of CD-CoT and empirically evaluate it in Appendix F.7.

### Empirical Study

In this part, we empirically verify the effectiveness of CD-CoT and start by introducing the baselines.

**Baseline methods.** We employ three methods that require _additional information_: (1) Self-Correction with Oracle Feedback (SCO)  utilizes the _ground truth answers_ of test questions to determine when to terminate the self-correction loop; (2) Backtracking (BT)  guides self-correction by providing the model with the _position_ of the first noisy thought; (3) Contrastive Chain-of-Thought (CC)  conducts direct reasoning with all the noisy or _clean examples_ without implicit or explicit denoising.

**Main results.** As in Tab. 8, CD-CoT demonstrates a significant performance improvement across all datasets, with an average improvement of \(17.8\%\) compared with the base model under noisy settings. Notably, on Math-Base-9, Math-Base-11, and Symbolic-Equal, CD-CoT surpasses all baseline methods by a significant margin. On Symbolic-Longer and Commonsense, CD-CoT only slightly lags behind SCO. However, SCO requires the ground truth answer to the test question, which should be unknown in practice, as pointed out in . In comparison, CD-CoT only necessitates an additional clean demonstration, making it much more practical to apply across realistic scenarios. _Notably, CD-CoT outperforms SCO in \(20\) out of \(30\) settings and surpasses BT, CC in all \(30\) settings_.

Besides, CD-CoT displays _remarkable resistance_ to the magnitude of noise. Therein, CD-CoT demonstrates enhanced resilience against inaccurate noise on mathematical tasks, which are quite challenging. For instance, on Math Base-9 with inaccurate rationales, the average accuracies of SCO and BT decline significantly by \(28.8\%\) and \(26.3\%\) compared to the accuracies with clean rationales. In contrast, CD-CoT exhibits a more modest decline of \(7.0\%\). An ablation study of components in Appendix F.3 demonstrates the denoising power and performance gain of CD-CoT, attributed to its contrastive denoising with rationale rephrasing as well as repeated reasoning with voting components.

**Ablation study of varying hyper-parameters.** By manipulating the values of \(N,\,M,\,D,\,\,C\), we generate diverse algorithm instances. Here, \(D\) denotes the reasoning times allocated to the \(M\) inputs, while \(C\) signifies whether the clean example is used in step 3. As demonstrated in Tab. 9, the clean example utilized by CD-CoT during the reasoning process plays a pivotal role. The omission of this clean example results in an average decrease of \(3.3\%\) and \(4.5\%\) in accuracy under irrelevant noise and inaccurate noise, respectively. Besides, the accuracy exhibits subtle variations when employing different algorithm instances, with the highest average accuracy observed at \(51.3\%\) and the lowest average accuracy at \(49.3\%\). Further, Tab. 10 presents the average number of tokens used in reasoning. We set \(M\!=\!2\) to strike a balance. Please refer to Appendix E.3 for detailed hyper-parameter selection.

    & \)} &  & ,\,,_{})\)} & ,\,,_{})\)} \\  & & & & Easy & Medium & Hard & Arg. & Easy & Medium & Hard & Arg. \\   & Base &  & Base & - & 46.4 & 39.3 & 30.3 & 26.6 & 32.1 & 23.2 & 10.1 & 6.0 & 13.1 \\  & & w/SD  & Ground Truth & 53.6 & 46.3 & 39.6 & 36.4 & 40.8 & 34.7 & 22.0 & 1.7 & 24.8 \\  & & w/SD  & Noise Position & 27.2 & 37.2 & 34.2 & 29.9 & 34.4 & 30.1 & 18.4 & 14.1 & 20.9 \\  & & w/CC  & Clean Demo & 44.9 & 43.3 & 44.6 & 45.5 & 44.3 & 37.2 & 31.7 & 30.7 & 33.2 \\  & & w/CD-CoT (ours) & Clean Demo & **60.7** & **59.7** & **60.7** & **57.2** & **59.2** & **54.0** & **58.7** & **48.4** & **58.7** \\   & Base &  & Base & 23.9 & 19.1 & 13.6 & 10.7 & 14.5 & 14.0 & 6.7 & 3.8 & 8.1 \\  & & w/SD  & Ground Truth & **33.0** & 29.2 & 24.0 & 20.0 & 24.4 & **29.2** & 20.0 & 17.2 & 22.1 \\  & & w/SD  & Noise Position & 24.3 & **17.5** & 17.2 & 15.7 & 13.5 & 12.8 & 9.2 & 6.8 & 9.6 \\  & & w/CD  & Clean Demo & 22.3 & 19.1 & 18.4 & 18.2 & 18.6 & 19.0 & 15.3 & 14.6 & 16.3 \\  & & w/CD-CoT (ours) & Clean Demo & 31.0 & **33.7** & **32.7** & **34.7** & **33.7** & **20.0** & **20.7** & **25.3** & **28.3** \\   & Base &  & Base & - & 32.7 & 28.1 & 25.1 & 23.0 & 25.4 & 29.1 & 26.1 & 22.7 & 26.0 \\  & & w/SD  & Ground Truth & 38.5 & 34.9 & 31.4 & 22.7 & 31.7 & 34.0 & 34.1 & 34.5 & 34.2 \\  & & w/SD  & Noise Position & 37.8 & 35.0 & 27.7 & 27.6 & 37.8 & 35.2 & **27.9** & **25.0** \\  & & w/CD  & Clean Demo & 37.8 & 33.8 & 32.7 & 32.0 & 32.8 & 31.3 & 33.0 & 29.9 & 31.4 \\  & & w/CD-CoT (ours) & Clean Demo & **42.7** & **44.7** & **44.0** & **43.8** & **42.6** & **41.3** & **42.7** & **42.2** \\   & Base &  & Base &  & - & 9.2 & 6.3 & 7.2 & 6.0 & 6.5 & 7.0 & 6.8 & 6.0 & 6.6 \\  & & w/SD  & Ground Truth & **18.7** & **12.1** & 10.5 & 11.3 & 11.3 & **15.2** & **15.9** & **9.8** & **13.6** \\  & & w/SD  & Noise Position & 7.2 & 3.4 & 3.5 & 7.5 & 7.5 & 3.8 & 3.6 & 3.6 & 3.7 \\  & & w/CD  & Clean Demo & 9.4 & 9.8 & 7.9 & 7.9 & 8.5 & 8.5 & 7.4 & 6.5 & 7.5 \\  & & w/CD-CoT (ours) & Clean Demo & 12.3 & 12.0 & **12.0** & **13.0** & **12.3** & 12.0 & 10.0 & 11.1 \\   & Base &  & - & 45.7 & 44.3 & 42.3 & 41.4 & 42.7 & 36.7 & 33.4 & 28.3 & 32.8 \\  & & w/SD  & Ground Truth & **63.5** & 40.4 & 56.4 & 60.8 & 56.4 & 58.6 & 55.9 & 57.8 \\  & & w/SD  & Noise Position & 47.7 & 25.8 & 25.3 & 25.8 & 11.6 & 11.0 & 15.8 & 12.8 \\  & & w/CD  & Clean Demo & 48.3 & 45.7 & 43.6 & 44.0 & 44.4 & 42.1 & 40.8 & 40.5 & 41.1 \\  & & w/CD-CoT (ours) & Clean Demo & 49.0 & 50.2 & 54.7 & 50.3 & 51.8 & 51.0 & 49.7 & 49.7 & 50.1 \\   

Table 8: Performance of denoising methods that require additional information for supervision.

**Ablation study with different LLMs.** We examine the generalization of CD-CoT across different LLMs. As shown in Tab. 11, with comparably more powerful LLMs, _e.g._, GPT-3.5-turbo and Gemini-Pro, CD-CoT demonstrates notable improvements in average accuracy. It respectively achieves increases of \(23.4\%\) and \(21.6\%\) in accuracy compared to base models and surpasses all the baselines.

**Case Study.** We illustrate the denoising effects of various robust methods using Math Base-9 as an example. The results in Tab. 12 indicate that the introduction of random characters by SM disrupts the logic of the rationale. SD fails to eliminate all the noise while recovering the input content, and SP alters the original rationale's reasoning process even when noise removal is successful. In contrast, CD-CoT significantly removes noisy thoughts and ensures format alignment with the original rationale. More denoising examples and an entire case study are in Appendix F.9 and G, respectively.

## 6 Conclusion

In this work, we investigate the under-explored problem of noisy rationales in LLMs. We introduce the NoRa dataset, which tests LLMs against irrelevant or inaccurate thoughts in question-answer scenarios. Our findings show LLMs' vulnerability to noisy rationales is inadequately mitigated by existing robust methods. We thereby design the CD-CoT method to enhance the robustness via contrastive denoising. The extension advocates for advancing LLMs by strategies, _e.g._, external knowledge bases with a retrieval-augmented framework, robust inductive reasoning to extract rules from noisy examples, and multi-modal data integration to enhance the robustness of LLM reasoning.

    &  &  \\ \(N\) & \(M\) & \(D\) & \(C\) & Base-9 & Sym.(E) & Com. & Base-9 & Sym.(E) & Com. \\ 
5 & 1 & 5 & Y & 1440 & 3162 & 788 & 1428 & 3170 & 798 \\
5 & 1 & 5 & N & 1301 & 2685 & 660 & 1295 & 2732 & 667 \\
5 & 2 & 2-3 & Y & 2175 & 4934 & 1269 & 2156 & 4989 & 1311 \\
5 & 2 & 2-3 & N & 1864 & 4044 & 1005 & 1842 & 4087 & 1039 \\
5 & 3 & 1+2+2 & Y & 2902 & 6704 & 1772 & 2878 & 6785 & 1821 \\
5 & 3 & 1+2+2 & Z & 2146 & 5360 & 1372 & 2399 & 5443 & 1420 \\
5 & 5 & 1 & Y & 3468 & 10340 & 2764 & 4339 & 10514 & 2105 \\
5 & 5 & 1 & N & 3535 & 8099 & 2088 & 3506 & 8303 & 2163 \\   

Table 10: Comparison of #tokens on medium-level tasks.

    &  Acc(\(\), \(\), \(_{}\)) \\ Base-9 & Sym.(E) \\  } &  Acc(\(\), \(\), \(_{}\)) \\ Base-9 & Sym.(E) \\  } \\   & Base & 30.3 & 25.1 & 42.3 & 10.1 & 26.1 & 33.4 \\  & SC & 36.6 & 28.3 & 45.0 & 17.3 & 30.7 & 44.7 \\  & BT & 34.2 & 22.7 & 28.3 & 18.4 & 22.7 & 11.0 \\  & CD-CoT & 44.3 & 32.7 & 43.6 & 31.7 & 33.0 & 40.8 \\  & CD-CoT & **60.7** & **42.7** & **54.7** & **58.7** & **41.3** & **49.7** \\   & Base & 72.3 & 38.9 & 53.2 & 21.2 & 36.7 & 33.5 \\  & SC & 80.3 & 43.3 & 60.0 & 32.3 & 45.0 & 42.7 \\  & BT & 82.4 & 29.3 & 37.8 & 26.7 & 28.7 & 33.3 \\  & CC & 67.5 & 37.3 & 50.2 & 43.6 & 35.0 & 45.6 \\  & CD-CoT & **92.7** & **49.3** & **57.7** & **76.2** & **53.3** & **55.7** \\   & Base & 2.8 & 8.7 & 41.9 & 2.7 & 9.1 & 40.2 \\  & SC & **5.0** & 10.3 & **46.7** & **3.0** & 9.7 & **46.0** \\  & BT & 1.4 & 11.2 & 36.1 & 0.9 & 12.5 & 36.2 \\  & CC & 1.1 & **16.3** & 29.9 & 2.8 & **14.0** & 28.3 \\  & CD-CoT & 4.0 & 9.7 & 39.3 & 2.7 & 9.7 & 39.7 \\   & Base & 16.3 & 17.9 & 34.9 & 3.7 & 15.1 & 31.1 \\  & SC & 20.0 & 21.7 & 37.0 & 2.7 & 18.0 & 37.7 \\  & BT & 4.1 & 9.7 & 6.2 & 2.4 & 10.1 & 10.5 \\  & CC & **24.4** & 18.5 & 36.0 & **12.5** & 18.3 & 35.7 \\  & CD-CoT & 8.7 & **22.7** & **40.3** & 4.7 & **21.3** & **40.3** \\   

Table 11: Comparing methods with different LLMs.

    &  &  \\ \(N\) & \(M\) & \(D\) & \(C\) & Base-9 & Sym.(E) & Com. & Base-9 & Sym.(E) & Com. \\ 
5 & 1 & 5 & Y & 1440 & 3162 & 788 & 1428 & 3170 & 798 \\
5 & 2 & 2-3 & Y & 1301 & 2685 & 660 & 1295 & 2732 & 667 \\
5 & 2 & 2-3 & Y & 2175 & 4934 & 1269 & 2156 & 4989 & 1311 \\
5 & 2 & 2-3 & N & 1864 & 4044 & 1005 & 1842 & 4087 & 1039 \\
5 & 3 & 1+2+2 & Y & 2902 & 6704 & 1772 & 2878 & 6785 & 1821 \\
5 & 3 & 1+2+2 & Z & 2146 & 5360 & 1372 & 2399 & 5443 & 1420 \\
5 & 5 & 1 & Y & 3468 & 10340 & 2764 & 4339 & 10514 & 2105 \\
5 & 5 & 1 & N & 3535 & 8099 & 2088 & 3506 & 8303 & 2163 \\   

Table 9: Comparison of accuracy on medium-level tasks.