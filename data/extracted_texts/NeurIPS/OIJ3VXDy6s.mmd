# RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability

Chuning Zhu

University of Washington

Seattle, WA 98105

zchuning@cs.washington.edu

&Max Simchowitz

Massachusetts Institute of Technology

Boston, MA 02139

msimchow@mit.edu

&Siri Gadipudi

University of Washington

Seattle, WA 98105

sg06@uw.edu

&Abhishek Gupta

University of Washington

Seattle, WA 98105

abhgupta@cs.washington.edu

###### Abstract

Visual model-based RL methods typically encode image observations into low-dimensional representations in a manner that does not eliminate redundant information. This leaves them susceptible to _spurious variations_ - changes in task-irrelevant components such as background distractors or lighting conditions. In this paper, we propose a visual model-based RL method that learns a latent representation resilient to such spurious variations. Our training objective encourages the representation to be maximally predictive of dynamics and reward, while constraining the information flow from the observation to the latent representation. We demonstrate that this objective significantly bolsters the resilience of visual model-based RL methods to visual distractors, allowing them to operate in dynamic environments. We then show that while the learned encoder is resilient to spurious variations, it is not invariant under significant distribution shift. To address this, we propose a simple reward-free alignment procedure that enables test time adaptation of the encoder. This allows for quick adaptation to widely differing environments without having to relearn the dynamics and policy. Our effort is a step towards making model-based RL a practical and useful tool for dynamic, diverse domains. We show its effectiveness in simulation benchmarks with significant spurious variations as well as a real-world egocentric navigation task with noisy TVs in the background. Videos and code: https://zchuning.github.io/repo-website/.

## 1 Introduction

Consider the difference between training a single robot arm against a plain background with reinforcement learning (RL), and learning to operate the same arm amidst of plentiful dynamic distractors - uncontrollable elements such as changing lighting and disturbances in the scene. The latter must contend with _spurious variations_ - differences in environments which are irrelevant for the task but potentially confusing for a vision-based RL agent - resilience to which is indispensable for truly versatile embodied agents deployed in real world settings.

Standard end-to-end techniques for visual RL struggle in the presence of spurious variations , in part because they fail to discard task-irrelevant elements. To improve generalization , self-supervised representation learning methods  pre-train visual encoders that compress visual observations. These methods aim for lossless compression of how image observations evolve in time (e.g. by minimizing reconstruction error). Unaware of the demands of downstreamtasks, these methods also cannot determine which elements of an environment can be discarded. As such, they often struggle in dynamic and diverse scenes [64; 48; 17] - ones where significant portions of the observations are both unpredictable and irrelevant - despite being remarkably successful in static domains.

This paper proposes Resilient Model-Based RL by **R**egularizing **P**osteior Predictability (RePo) - an algorithm for learning lossy latent representations resilient to spurious variations. A representation is satisfactory if it (a) predicts its own dynamics and (b) accurately predicts the reward. To satisfy these criteria, RePo jointly learns (i) a visual encoder mapping high-dimensional observations to intermediate image "encodings" (ii) a latent encoder which compresses histories of intermediate image encodings into compressed _latent representations_ (iii) a dynamics model in the latent representation space, and (iv) a reward predictor to most accurately predict current and future rewards. What distinguishes us from past work [63; 12; 17] is a new desideratum of _predictability:_ that, conditioned on past latents and actions, future latent dynamics should look _as deterministic as possible_. This is because an agent should try to maximize its control over task-relevant parts of the state, whilst neglecting aspects of the environment that it cannot influence [20; 60]. RePo optimizes a novel loss which encourages _predictability_, thereby discarding a broad range of spurious variations in aspects of the environment which are out of the agents control (e.g. changes in background, lighting, or visual traffic in the background). At the same time, by penalizing reward prediction error, we capture the _task-relevant_ aspects of the dynamics necessary for learning performant policies.

RePo implements a deceptively simple modification to recurrent state-space models for model-based RL [17; 59; 46]. We maximize mutual information (MI) between the current representation and _all_ future rewards, while minimizing the mutual information between the representation and observation. Instead of minimizing image reconstruction error, we optimize a variational lower bound on the MI-objective which tractably enforces that the learned observation encoder, latent dynamics and reward predictors are highly informative of reward, while ensuring latents are as _predictable_ as possible (in the sense described above). We demonstrate that the representations, and the policies built thereupon, learned through RePo succeed in environments with significant amounts of dynamic and uncontrollable distractors, as well as across domains with significant amounts of variability and complexity. Through ablations, we also validate the necessity of our careful algorithm design and optimization decisions.

While these learned representations enable more effective reinforcement learning in dynamic, complex environments, the visual encoders (point (i) above) mapping from observations into intermediate encodings suffer from distribution shift in new environments with novel visual features (e.g. a new background not seen at train time.) We propose a simple test-time adaptation scheme which uses (mostly) unlabeled test-time data to adapt the _visual encoders_ only, whilst keeping all other aspects of the RePo model fixed. Because RePo ensures resilience of the compressed latent representation at training time, modifying only the test-time visual encoders to match training time representations allows representations to recover optimal performance with only minor amounts of adaptation.

Concretely, the key contributions of this work are: **(1)** We propose a simple representation learning algorithm RePo for learning representations that are informative of rewards, while being as predictable as possible. This allows model-based RL to scale to dynamic, cluttered environments, avoiding reconstruction. **(2)** We show that while the learned encoders may be susceptible to distribution shift, they are amenable to a simple test-time adaptation scheme that can allow for quick adaptation in new environments. **(3)** We demonstrate the efficacy of RePo on a number of simulation and real-world domains with dynamic and diverse environments.

## 2 Related Work

Our work is related to a number of techniques for visual model-based reinforcement learning, but differs in crucial elements that allow it to scale to dynamic environments with spurious variations.

Figure 1: Reinforcement learning in environments with spurious variations - including dynamic elements like humans, changes in lighting and training across a range of visual appearances.

**Model-Based RL.** Though model-based RL began with low-dimensional, compact state spaces [26; 37; 27; 57], advances in visual model-based reinforcement learning [17; 19; 18; 44; 42; 21] learn latent representations and dynamics models from high dimensional visual feedback (typically via recurrent state-space models). Perhaps most relevant to RePo is Dreamer . Section 4 explains the salient differences between Dreamer and RePo; notably, we escher a reconstruction loss in pursuit of resilience to spurious variations. A closely related work is TD-MPC , which learns a task-oriented latent representation by predicting the value function. However, its representation may not discard irrelevant information and necessarily contains information about the policy.

**Representation Learning for Control.** There is a plethora of techniques for pretraining visual representations using unsupervised learning objectives [38; 34; 30; 32; 41; 49; 47; 13]. While these can be effective on certain domains, they do not take downstream tasks into account. Task-relevant representation learning for RL uses the reward function to guide representation learning, typically in pursuit of _value-equivalence_ (e.g. via bisimulation) [63; 8; 62; 12; 50; 22]. However, these approaches do little to explicitly counteract spurious variations. Our work aligns with a line of work that disentangles task-relevant and task-irrelevant components of the MDP. [7; 6] obtain provable guarantees for representation learning with exogeneous distractors - parts of the state space whose dynamics is independent of the agent's actions.  introduces a more granular decomposition of the MDP across the task relevance and controllability axes. Our work, in contrast, does not impose a specific form on the spurious variations.

**Domain Adaptation.** Unsupervised domain adaptation adapts representations across visually different source and target domains [66; 58; 45; 11; 25]. These techniques predominantly adapt visual encoders by minimizing a distribution measure across source and training distributions, such as MMD [5; 33; 53; 24], KL divergence [67; 35] or Jensen-Shannon divergence [11; 52]. In , distribution matching was extended to sequential decision making. While domain adaptation settings typically assume that the source and target share an underlying marginal or joint distribution in a latent space, this assumption does not hold in online RL because the data is being collected incrementally through exploration, and hence the marginals may not match. Hence, our test-time adaptation technique, as outlined in Section 4.1, introduces a novel support matching objective that enforces the test distribution to be in support of the train distribution, without trying to make the distributions identical.

## 3 Preliminaries

**MDPs.** A (discounted) MDP \(=(,,,P,P_{0},r)\) consists of a state-space \(\), action space \(\), discount factor, \((0,1)\), transition and \(P(,):()\), initial state distribution \(P_{0}()\), and reward function \(r(,):\) (assumed deterministic for simplicity). A policy \(:()\) is a mapping from states to distributions over actions.We let \(_{}^{}\) denote expectations under \(s_{0} P_{0}\), \(a_{t}(s_{t})\), and \(s_{t+1} P(s_{t},a_{t})\); the value is \(V_{}^{}(s):=_{}^{}[_{t=0}^{ }^{h}r(s_{t},a_{t}) s_{0}=s]\), and \(V_{}^{}=_{s_{0} P_{0}}[V_{}^{}(s_{0})]\). The goal is to learn a policy \(\) that maximizes the sum of expected returns \(_{}^{}[_{t=0}^{}^{h}r(s_{t},a_{t} ) s_{0}=s]\), as in most RL problems, but we do so based on a belief state as explained below.

**Visual RL and Representations.** For our purposes, we take states \(s_{t}\) to be visual observations \(s_{t} o_{t}\); for simplicity, we avoid explicitly describing a POMDP formulation - this can be subsumed either by introducing a belief-state , or by assuming that images (or sequences thereof, e.g. to estimate velocities) are sufficient to determine rewards and transitions . The states \(o_{t}\) may be high-dimensional, so we learn encoders \(h:\) to an encoding space \(\). We compress these encodings \(x_{t}\) further into latent states \(z_{t}\), described at length in our method in Section 4.

**Spurious variation.** By _spurious variation_, we informally mean the presence of features of the states \(s_{t}\) which are irrelevant to our task, but which do vary across trajectories. These can take the form of explicit _distractors_ - either _static_ objects (e.g. background wall-paper) or _dynamic_ processes (e.g. video coming from a television) that do not affect the part of the state space involved in our task [7; 6]. Spurious variation can also encompass processes which are not so easy to disentangle with the state: for example, lighting conditions will affect all observations, and hence will affect the appearance of transition dynamics.

Consider the following canonical example: an MDP with state space \(_{1}_{2}\), where for \(s=(s^{(1)},s^{(2)})_{1}_{2}\), the reward \(r(s,a)\) is a function \((s^{(1)},a)\) only of the projection onto \(^{1}\). Moreover, suppose that \([(s^{+})^{(1)} s,a]\), where \(s^{+} P(s,a)\), is a distribution \((s^{(1)},a)\) again only depending on \(s^{(1)}\). Then, the states \(s^{(2)}\) can be viewed as spuriously various. For example, if\(s^{(1)}\) is a Lagrangian state and \(s^{(2)}\) is a static background, then it is clear that transitions of Lagrangian state and reward do not depend on \(s^{(2)}\). Our template also encompasses dynamic distractors; e.g. a television show in the background has its own dynamics, and these also do not affect reward or physical dynamics. Even varying lighting conditions can be encompassed in this framework: the shadows in a scene or brightness of the environment should not affect reward or physics, even though these visual features themselves evolve dynamically in response to actions and changes in state. That is, there are examples of spurious variation where \(s^{(1)}\) (e.g. Lagrangian state) affect \(s^{(2)}\) (e.g. certain visual features), but not the other way round. In all cases, "spurious" implies that states \((s^{(2)}_{t})_{t 0}\), and their possible variations due to different environments, have no bearing on optimal actions.

## 4 RePo: Parsimonious Representation Learning without Reconstruction

We propose a simple technique for learning task-relevant representations that encourages parsimony by removing all information that is neither pertinent to the reward nor the dynamics. Such representations discard information about spurious variations, while retaining the information actually needed for decision making.

To describe our method formally, we introduce some notation (which is also shown in Fig 2). Let \(\) be the space of image observations, \(\) the space of encoded observations, where \(h:\) represents the encoding function from images observations to encoded observations, and \(\) the space of latent representations. Note that \(x_{t+1}\) is simply the instantaneous encoding of the image \(o_{t+1}\) as \(x_{t+1}=h(o_{t+1})\), but the latent representation \(z_{t+1}\) at time step \(t+1\) is an aggregation of the current encoding \(x_{t+1}\) and previous latent \(z_{t}\) and action \(a_{t}\). Let \(_{}\) denote the space of "posteriors" on latent dynamics \(z\) of the form \(p(z_{t+1} z_{t},a_{t},x_{t+1})\), where \(z_{t},z_{t+1}\), \(a_{t}\), \(x_{t+1}\), and where and \(z_{0} p_{0}\) has some initial distribution \(p_{0}\). In words, the latent posterior use past latent state and action, in addition to _current encoding_ to determine current latent. Control policies and learned dynamics models act on this latent representation \(z_{t+1}\), and not simply the image encoding \(x_{t+1}\) so as to incorporate historical information.

Let \(_{}\) denote the distribution over experienced actions, observations and rewards from the environment (\((a_{1:T},o_{1:T},r_{1:T})_{}\)). For \(p_{}\), let \(_{p,h}\) denote expectation of \((a_{1:T},o_{1:T},r_{1:T})_{}\), \(x_{t}=h(o_{t})\) and the latents \(z_{t+1} p( z_{t},a_{t},x_{t+1})\) drawn from the latent posterior, with the initial latent \(z_{0} p_{0}\). Our starting proposal is to optimize the latent posterior \(p\) and image encoder \(h\) such that information between the latent representation and future reward is maximized, while bottleneck  the information between the latent and the observation:

\[_{p,h}_{p,h}(z_{1:T};r_{1:T} a_{1:T})\;\;\;\; _{p,h}(z_{1:T};o_{1:T} a_{1:T})<.\] (4.1)

Above, \(_{p,h}(z_{1:T};r_{1:T} a_{1:T})\) denotes mutual information between latents and rewards conditioned actions under the \(_{p,h}\) distribution, and distribution \(_{p,h}(z_{1:T};o_{1:T} a_{1:T})\) measures information

Figure 2: RePo learns a latent representation resilient to spurious variations by predicting the dynamics and the reward while constraining the information flow from images.

between latents and observations under \(_{p,h}\) as well. Thus, (4.1) aims to preserve large mutual information with rewards whilst minimizing information stored from observations.

Optimizing mutual information is intractable in general, so we propose two variational relaxations of both objects (proven in Appendix B)

\[_{p,h}(z_{1:T};r_{1:T} a_{1:T}) _{p,h}[_{t=1}^{T} q_{}(r_{t}  z_{t})]\] (4.2) \[_{p,h}(z_{1:T};o_{1:T} a_{1:T}) _{p,h}[_{t=0}^{T-1}_{ }(p( z_{t},a_{t},x_{t+1}) q_{}( z_{t},a_{ t}))],\] (4.3)

where \(q_{}\) and \(q_{}\) are variational families representing beliefs over rewards \(r_{t}\) and latent representations \(z_{t+1}\), respectively. We refer to \(z_{t+1} p( z_{t},a_{t},x_{t+1})\) as the _latent posterior_, because it conditions on the latest encoded observation \(x_{t+1}=h(o_{t+1})\). We call the variational approximation \(q_{}( z_{t},a_{t})\) the _latent prior_ because it does not use the current observation \(o_{t+1}\) (or it's encoding \(x_{t+1}\)) to determine \(z_{t+1}\). Note that the right hand side of Eq. (4.3) depends on \(h\) through \(x_{t+1}=h(o_{t+1})\), and thus gradients of this expression incorporate gradients through \(h\).

**The magic of Eq. (4.3).** The upper bound in (4.3) reveals a striking feature which is at the core of our method: that, in order to reduce extraneous information in the latents \(z_{t}\) about observations \(o_{t}\), it is enough to match the latent posterior \(z_{t+1} p( z_{t},a_{t},x_{t+1})\) to our latent prior \(q_{}( z_{t},a_{t})\) that _does not condition on current \(x_{t+1}\)_. Elements that are spurious variations can be captured by \(p( z_{t},a_{t},x_{t+1})\), but not by \(q_{}( z_{t},a_{t})\), since \(q_{}\) is not informed by the latest observation encoding \(x_{t+1}\), and spurious variations are not predictable. To match the latent posterior and the latent prior, the latent representation must omit these spurious variations. For example, in an environment with a TV in the background, removing the TV images reduces next-step stochasticity of the environment. Thus, (4.3) encourages representations to omit television images.

**The relaxed bottleneck.** The above discussion may make it seem as if we suffer in the presence of task-relevant stochasticity. However, by replacing the terms in Eq. (4.1) with their relaxations in Eqs. (4.2) and (4.3), we only omit the stochasticity that is not useful for reward-prediction. We make these substitutions, and move to a penalty-formulation amenable to constrained optimization methods like dual-gradient descent . The resulting objective we optimize to learn the latent posterior \(p\), latent prior \(q_{}\), reward predictor \(q_{}\) and observation encoder \(h\) jointly is:

\[_{p,q_{},q_{},h}_{}_{p,h}[ _{t=1}^{T} q_{}(r_{t} z_{t})]+(_{p,h}[_{t=0}^{T-1}_{}(p( z_{t},a_{t},x_{t +1}) q_{}( z_{t},a_{t}))]-).\] (4.4)

**Implementation details.** We parameterize \(p\) and \(q\) using a recurrent state-space model (RSSM) . The RSSM consists of an encoder \(h_{}(x_{t} o_{t})\), a latent dynamics model \(q_{}(z_{t+1} z_{t},a_{t})\) corresponding to the prior, a representation model \(p_{}(z_{t+1} z_{t},a_{t},x_{t+1})\) corresponding to the posterior, and a reward predictor \(q_{}(r_{t} z_{t})\). We optimize (4.4) using dual gradient descent. In addition, we use the KL balancing technique introduced in Dreamer V2  to balance the learning of the prior and the posterior. Concretely, we compute the KL divergence in Eq. (4.4) as \(_{}(p q)=_{}( p  q)+(1-)_{}(p q )\), where \(\) denotes the stop gradient operator and \(\) is the balancing parameter. With the removal of reconstruction, the KL balancing parameters becomes especially important as shown by our ablation in Sec. 5.

**Policy learning** As is common in the literature on model-based reinforcement learning , our training procedure alternates between (1) _Representation Learning:_ learning a representation \(z\) by solving the optimization problem outlined in Eq. (4.4) to infer a latent posterior \(p(z_{t+1} z_{t},a_{t},x_{t+1})\), a latent prior \(q_{}(z_{t+1} z_{t},a_{t})\), an encoder \(x_{t}=h(o_{t})\) and a reward predictor \(q_{}(r_{t} z_{t})\), and (2) _Policy Learning:_ using the inferred representation, dynamics model and reward predictor to learn a policy \(_{}(a_{t} z_{t})\) for control. With the latent representation and dynamics model, we perform actor-critic policy learning  by rolling out trajectories in the latent space. The critic \(V_{}(z)\) is trained to predict the discounted cumulative reward given a latent state, and the actor \(_{}(a z)\) is trained to take the action that maximizes the critic's prediction. While policy learning is carried out entirely using the latent prior as the dynamics model, during policy execution (referred to as inference in Fig. 2), we infer the posterior distribution \(p(z_{t+1} z_{t},a_{t},x_{t+1})\) over latent representations from the current observation, and use this to condition the policy acting in the world. We refer readers to Appendix C for further details.

**Comparison to Dreamer, DeepMDP, and Bisimulation.** Dreamer  was first derived to optimize pixel-reconstruction, leading to high-fidelity dynamics but susceptibility to spurious variations. Naively removing pixel reconstruction from dreamer, however, leads to poor performance . Our objective can be interpreted as modifying Dreamer so as to maintain sufficiently accurate dynamics, but without the fragility of pixel-reconstruction. DeepMDP  sets the latents \(z_{t}\) to exactly the image encodings \(x_{t}=h(o_{t})\). It learns a dynamics \(:()\) such that the distribution \(_{t+1}(h(o_{t}),a_{t})\) is close to \(x_{t+1} h(o_{t+1})\), \(o_{t+1} P^{}(o_{t},a_{t})\), where \(P^{}\) denotes a ground-truth transition dynamics; this enforces consistency of dynamics under encoding. The above distributions are viewed as conditional on _past_ observation and action, and as a result, highly non-parsimonious representations such as the identity are valid under this objective. Bisimulation  learns an optimal representation in the sense that a perfect bisimulation metric does not discard any relevant information about an MDP. However, there is no guarantee that it will disregard irrelevant information. Indeed, the identity mapping induces a trivial bisimulation metric. Hence, Bisimulation compress only by reducing the dimensionality of the latent space. In contrast, we further compress the encodings \(x_{t}\) into latents \(z_{t}\) so as to enforce the latent prior \(q_{}( a_{t},z_{t})\) is close to the latest observation-dependent posterior distribution \(p( z_{t},a_{t},x_{t+1})\). As mentioned in Eq. (4.3), this ensures information compression and invalidates degenerate representations such as the identity mapping.

### Transferring Invariant Latent Representations via Test-Time Adaptation

While resilient to spurious variations seen during training, our learned latents \(z_{t}\) - and hence the policies which depend on them - may not generalize to new environment which exhibit systematic distribution shift, e.g. lighting changes or background changes. The main source of degradation is that encoder \(h:\) may observe images that it has not seen at train time; thus the latent, which depend on observations through \(x_{t}=h(o_{t})\), may behave erratically, even when system dynamics remain unchanged.

Relying on the resilience of our posteriors \(p\) over latents \(z_{t}\) introduced by RePo, we propose a test-time adaption strategy to only adjust the encoder \(h\) to the new environment, whilst leaving \(p\) fixed. A natural approach is to apply unsupervised domain adaptation methods [66; 58] to adapt the visual encoder \(h\) to \(h_{}\). These domain adaptation techniques typically operate in supervised learning settings, and impose distributional constraints between source and target domains [61; 25], where the distributions of training and test data are stationary and assumed to be the same in _some_ feature space. A distribution matching constraint would be:

\[_{h_{}()}(_{} _{})_{}=h_{} _{},_{}=h_{ }.\] (4.5)

In Eq. (4.5), we consider matching the distributions over encodings \(x\) of observations \(o\). Specifically, we assume \(_{}\) and \(_{}\) denote training and test-buffer distributions over observations \(o\), \(_{}=h_{}_{}\) denotes the distribution of \(x=h_{}(o)\) where \(o_{}\) is encoded by the train-time

Figure 3: Depiction of test-time adaptation scheme for latent alignment via support constraints. During exploration, the marginal distributions may not match perfectly, so we match the supports of the latent features instead, using a _reweighted_ distribution constraint.

encoder \(h_{}\), and \(_{}=h_{}_{}\) denotes encodings under a test-time encoder \(h_{}()\) over which we optimize. Here, \((,)\) denotes an \(f\)-divergence, such as the \(^{2}\)-divergence.

**Support Constraint.** (4.5) fails to capture that the encoded distributions at train and test time _differ_ at the start of our adaption phase: suboptimal encoder performance at the start of the adaptation phase causes the policy to visit sub-optimal regions of state space not seen at train time. Thus, it may be impossible to match the distribution as in standard unsupervised domain adaptation. We therefore propose to replace (4.5) with a _support constraint_, enforcing that the distribution of \(h_{}_{}\) is contained in the _support_ of \(h_{}_{}\). We consider the following idealized objective:

\[_{() 0,h_{}()}( _{}_{})\;\;\;\;_{x_{}}[(x)]=1.\] (4.6)

Here, by \(_{}\), we mean the re-weighted density of \(_{}=h_{}_{}\) by a function \((x)\). The constraints \(_{_{}}[(x)]=1\) and \(() 0\) ensures this reweighted distribution is also a valid probability distribution. The reweighting operation \(_{}\) seems intractable at first, but we show that if we take \((,)=^{2}(,)\) to be the \(^{2}\) divergence, then Eq. (4.6) admits the following tractable Lagrangian formulation (we refer readers to  and Appendix B for a thorough derivation)

\[_{() 0,h_{}()}_{f(),} _{_{}}[(x) f(x)]-_{ _{}}[f(x)+f(x)^{2}]+( _{_{}}[(x)]-1),\] (4.7)

where above, \(\), \(f:\), and the objective depends on \(h_{}\) through the definition \(_{}=h_{}_{}\). This objective is now a tractable saddle point optimization, which can be solved with standard stochastic optimization techniques. The optimization alternates between optimizing the reweighting \(\) and the visual encoder \(h_{}\), and the dual variables \(f,\). Throughout adaptation, we freeze all other parts of the recurrent state space model and only optimize the encoder. We provide more intuition for the support constraint in Appendix E.

**Calibration.** We note that naively reweighting by \(()\) can cause degenerate encodings that collapse into one point. To prevent this, we regularize the support constraint by also ensuring that some set of paired "calibration" states across training and testing domains share the same encoding. We collect paired trajectories in the training and testing domains using actions generated by an exploration policy, and minimize the \(_{2}\) loss between the training and testing encoding of each pair of observations. We defer the details of the complete optimization to Appendix C.

## 5 Experimental Evaluation

We conduct empirical experiments to answer the following research questions: (1) Does RePo enable learning in dynamic, distracted environments with spurious variations? (2) Do representations learned by RePo quickly adapt to new environments with test time adaptation? (3) Does RePo help learning in static, but diverse and cluttered environments?

Figure 4: Depiction of the environments being used for evaluation. **(Left):** the Distracted DeepMind Control suite , **(Top Right)**: Maniskill2  environments with realistic backgrounds from Matterport . **(Bottom Right)**: TurtleBot environment with two TVs playing random videos in the background.

Evaluation domainsWe evaluate our method primarily in three different settings. (1) **Distracted DeepMind Control Suite**[64; 63] is a variant of DeepMind Control Suite where the static background is replaced with natural videos (Fig. 4). For adaptation experiments, we train agents on static undistracted backgrounds and adapt them to distracted variants. (2) **Realistic Maniskill** is a benchmark we constructed based on the Maniskill2 benchmark , but with realistic backgrounds from  to simulate learning in a diverse range of human homes. We solve three tasks - LiftCube, PushCube, and TurnFaucet in a variety of background settings. (3) **Lazy TurtleBot** is a real-world robotic setup where a TurtleBot has to reach some goal location from egocentric observations in a furnished room. However, there are two TVs playing random videos to distract the "lazy" robot. We provide more details about evaluation domains in Appendix D.

BaselinesWe compare our method with a number of techniques that explicitly learn representations and use them for learning control policies. (1) **Dreamer** is a state-of-the-art visual model-based RL method that learns a latent representation by reconstructing images. (2) **TIA** renders Dreamer more robust to visual distractors by using a separate dynamics model to capture the task-irrelevant components in the environment. (3) **Denoised MDP** further learns a factorized latent dynamics model that disentangles controllability and reward relevance. (4) **TD-MPC** trains a latent dynamics model to predict the value function and uses a hybrid planning method to extract a policy. (5) **DeepMDP** is a model-free method that learns a representation by predicting dynamics and reward, and then performs actor-critic policy learning on the learned representation. (6) Deep Bisimulation for Control **DBC** is model-free algorithm which encodes images into a latent space that preserves the bisimulation metric.

We also compare with a number of techniques for test-time adaptation of these representations. (1) **calibrated distribution matching**, a variant of the method proposed in Section 4.1, using a distribution matching constraint rather than a support matching one, (2) **uncalibrated support matching**, a variant of the method proposed in Section 4.1, using a support matching constraint but without using paired examples, (3) **uncalibrated distribution matching**, a variant of the method proposed in Section 4.1, using a distribution matching constraint, but without using paired examples, (4) invariance through latent alignment **ILA**, a technique for test-time adaptation of representations with distribution matching and enforcing consistency in latent dynamics, (5) **calibration**, a baseline that only matches the encodings of paired examples.

**Does RePo learn behaviors in environments with spurious variations?** We evaluate our method's ability to ignore spurious variations on a suite of simulated benchmark environments with dynamic visual backgrounds (Fig. 4); these are challenging because uncontrollable elements of the environment visually dominate a significant portion of the scene. Fig. 5 shows our method outperforms the baselines across six Distracted DeepMind Control environments, both in terms of learning speed and asymptotic performance. This implies that our method successfully learns latent representations resilient to spurious variations. Dreamer  attempts to reconstruct the dynamic visual distractors which is challenging in these domains. TIA  and Denoised MDP  see occasional success when

Figure 5: Results on distracted DeepMind control environments. These environments have spurious variations, and RePo is able to successfully learn in all of them, both faster and achieving higher asymptotic returns than prior representation learning methods.

they dissociate the task-relevant and irrelevant components, but they suffer from high variance and optimization failures. TD-MPC  is affected by spurious variations as its representations are not minimal. The model-free baselines DeepMDP  and DBC  exhibit lower sample efficiency on the more complex domains despite performing well on simpler ones.

To further validate RePo's ability to handle spurious variations in the real world, we evaluate its performance on Lazy TurtleBot, where a mobile robot has to navigate around a furnished room to reach the goal from egocentric observations (Fig. 4). To introduce spurious variations, we place two TVs playing random Youtube videos along the critical paths to the goal. As shown in Table. 1, RePo is able to reach the goal with nontrivial success within 15K environment steps, whereas Dreamer fails to reach the goal. We provide details about the setup in Appendix. D.

**Do representations learned by RePo transfer under distribution shift?** We evaluate the effectiveness of the test-time adaptation method described in Section 4.1 on three DeepMind Control domains: Walker Stand, Walker Walk, and Cheetah Run. We train the representation in environments with _static backgrounds_, and adapt the representation to domains with _natural video distractors_ (as shown in Fig. 4). For methods that use calibration between the source and target environments, we collect 10 trajectories of paired observations. Results are shown in Fig. 6. RePo shows the ability to adapt quickly across all three domains, nearly recovering the full training performance within 50k steps. Performance degrades if we replace the support constraint with a distribution matching objective, as it is infeasible to match distributions with the test-time distribution having insufficient exploration. We also observe that by removing the calibration examples, both support constraint and distribution perform worse as the distributions tend to collapse. We found the addition of dynamics consistency in ILA to be ineffective. Nor is calibration alone sufficient for adaptation.

**Does RePo learn across diverse environments with varying visual features?** While the previous two sections studied learning and adaptation in dynamic environments with uncontrollable elements, we also evaluate RePo on it's ability to learn in a _diverse_ range of environments, each with a realistic and cluttered static background. Being able to learn more effectively in these domains suggests that RePo focuses it's representation capacity on the important elements of the task across environments, rather than trying to reconstruct the entire background for every environment.

    & Success & Return \\  RePo (Ours) & **62.5\%** & **-24.3** \\  Dreamer  & 0.0\% & -61.7 \\   

Table 1: Results on Lazy TurtleBot at 15K environment steps. RePo achieves nontrivial success whereas Dreamer fails to reach the goal.

Figure 6: Results on adaptation from static environments to dynamic environments in Deepmind control. RePo with calibrated support constraints outperforms ablations and previous techniques for domain adaptation.

Figure 7: Results of training agents on varying static environments in Maniskill . RePo is able to learn more quickly and efficiently than alternatives even in static domains.

We test on three robotic manipulation tasks - LiftCube, PushCube, and TurnFaucet with realistic backgrounds depicted in Fig. 4. As shown in Fig. 7, our method achieves saturating performance across all three tasks. Dreamer  spends its representation capacity memorizing backgrounds and is unable to reach optimal task performance. TIA  suffers from high variance and occasionally fails to dissociate task-relevant from task-irrelevant features. Denoised MDP , TD-MPC , and DBC  learn to ignore the background in two of the tasks but generally lag behind RePo in terms of sample efficiency. DeepMDP  fails to learn meaningful behavior in any task.

Visualizing representations learned by RePoTo decipher our representation learning objective, we probe the learned representations by post-hoc training a separate image decoder to reconstruct image observations from the latents. We visualize the results in Fig. 9 and compare them with Dreamer reconstructions . Our representation contains little information about background but is capable of reconstructing the agent, implying that it contains only task-relevant information.

In addition to probing, we qualitatively compare the latent states of RePo and Dreamer by visualizing their top two principal components. We collect the same trajectory across all backgrounds in Maniskill and visualize the final recurrent latent state inferred by RePo and Dreamer respectively. As shown in Fig. 10, RePo produces more compact latent representations than Dreamer, meaning the latent states encode less information about background variations. This enables RePo to share data across different backgrounds, explaining its superior sample efficiency compared to baselines.

Ablation experimentsWe conduct ablation experiments to determine the effect of hyperparameters in Fig. 8. As we can see, the performance is crucially dependent on the information bottleneck \(\), as well as KL balancing. We refer readers to Appendix E for a more thorough discussion.

## 6 Discussion

This work presents RePo, a technique for learning parsimonious representations that are resilient to spurious variations. Our representation is effective on learning in dynamic, distracted environments. And while the representation is subject to degradation under distribution shift, it can be quickly adapted to new domains by a semi-supervised test-time adaptation procedure. A limitation of our method is that the learned dynamics model is no longer task-agnostic, as it only captures task-relevant information. This can be potentially addressed by simultaneously predicting multiple reward objectives. Our framework opens up several interesting directions for future research, such as: can a multi-task variant of RePo allow for representations applicable to a some distribution of tasks? Can we apply our algorithm in a continual learning setup? We believe our method holds promise in these more general settings, especially for real robots deployed into dynamic, human-centric environments.

Figure 8: Ablating objectives showing the importance of information bottleneck and KL balancing described in Section 4.