# On Class Distributions Induced by Nearest Neighbor Graphs for Node Classification of Tabular Data

Federico Errica

NEC Laboratories Europe

Heidelberg, Germany

###### Abstract

Researchers have used nearest neighbor graphs to transform classical machine learning problems on tabular data into node classification tasks to solve with graph representation learning methods. Such artificial structures often reflect the homophily assumption, believed to be a key factor in the performances of deep graph networks. In light of recent results demystifying these beliefs, we introduce a theoretical framework to understand the benefits of Nearest Neighbor (NN) graphs when a graph structure is missing. We formally analyze the Cross-Class Neighborhood Similarity (CCNS), used to empirically evaluate the usefulness of structures, in the context of nearest neighbor graphs. Moreover, we study the class separability induced by deep graph networks on a \(k\)-NN graph. Motivated by the theory, our quantitative experiments demonstrate that, under full supervision, employing a \(k\)-NN graph offers no benefits compared to a structure-agnostic baseline. Qualitative analyses suggest that our framework is good at estimating the CCNS and hint at \(k\)-NN graphs never being useful for such classification tasks under full supervision, thus advocating for the study of alternative graph construction techniques in combination with deep graph networks.

## 1 Introduction

The pursuit of understanding real-world phenomena has often led researchers to model the system of interest as a set of interdependent constituents, which influence each other in complex ways. In disciplines such as chemistry, physics, and network science, graphs are a convenient and well-studied mathematical object to represent such interacting entities and their attributes. In machine learning, the term "graph representation learning" refers to methods that can automatically leverage graph-structured data to solve tasks such as entity (or node), link, and whole-graph predictions .

Most of these methods assume that the relational information, that is the connections between entities, naturally emerges from the domain of the problem and is thus known. There is also broad consensus that connected entities typically share characteristics, behavioral patterns, or affiliation, something known as the homophily assumption . This is possibly why, when the structure is _not_ available, researchers have tried to artificially build Nearest Neighbor (NN) graphs from tabular data, by connecting entities based on some attribute similarity criterion, with applications in healthcare , fake news and spam detection , biology , and document classification  to name a few. From an information-theoretic perspective, the creation of such graphs does not add new information as it depends on the available data; that said, what makes their use plausible is that the graph construction is a form of feature engineering that often encodes the homophily assumption. Combined with the inductive bias of Deep Graph Networks (DGNs) , this strategy aims at improving the generalization performances on tabular data compared to structure-agnostic baselines, for example, a Multi-Layer Perceptron (MLP).

Indeed, using a \(k\)-NN graph  has recently improved the node classification performances _under the scarcity of training labels_[24; 23]. This is also known as the semi-supervised setting, where one can access the features of all nodes but the class labels are available for a handful of those. A potential explanation for these results is that, by incorporating neighboring values into each entity's representation, the neighborhood aggregation performed by DGNs acts as a regularization strategy that prevents the classifier from overfitting the attributes of the few labeled nodes, similar to input jittering [57; 62; 6]. However, it is still unclear what happens _when one has access to all training labels_ (hereinafter the fully-supervised setting), namely if these graph-building strategies grant a statistically significant advantage in generalization compared to a structure-agnostic baseline. In this respect, proper comparisons against such baselines are often lacking or unclear in previous works, an issue that has also been reported in recent papers about the reproducibility of node and graph classification experiments [61; 22; 54].

In addition, it was recently shown  that homophily is not required to achieve good classification performances in node classification tasks; rather, what truly matters is how different the neighborhood class label distributions of nodes of separate classes are. This resulted in the definition of the _empirical_ Cross-Class Neighborhood Similarity (CCNS) [43; 13], an object that estimates such similarities based on the _available_ connectivity structure. Yet, whether or not artificially built graphs can be useful for the task at hand has mainly remained an empirical question, and more theoretical conditions for which this happens are still not understood.

In this paper, we introduce a theoretical framework to approach this question, and we provide two analyses of independent interest. Inspired by the CCNS, Section 3.1 studies the neighboring class label distribution of nearest neighbor graphs. Then Section 3.2 deals with the distribution of entity embeddings induced by DGNs on a \(k\)-NN graph, and we use it to quantify class separability in both the input and the embedding spaces. Overall, our theoretical results suggest that building a \(k\)-NN graph is not a good idea. To validate the theory with empirical evidence, we robustly compare (Sections 4 and 5) four baselines across 11 tabular datasets to check that the \(k\)-NN graph construction does not give statistically significant advantages in the fully-supervised setting. In addition, we reverse-engineer the theory to learn data distributions that would make a \(k\)-NN graph useful in practice when combined with DGNs. From the empirical results, we conjecture that this is never the case. Therefore, we hope to raise awareness in the graph machine learning community about the need for alternative (either NN-based or not) graph construction techniques.

In summary, our contributions are: _i)_ under some assumptions, we can estimate the CCNS for nearest neighbor graphs and provide its first lower bound; _ii)_ we study how applying a simple DGN to an artificial \(k\)-NN graph affects the class separability of the input data; _iii)_ we carry out a robust comparison between structure-agnostic and structure-aware baselines on a set of 11 datasets that is in agreement with our theoretical results; _iv)_ qualitative analyses based on the theory further suggest that using the \(k\)-NN graph might not be advisable.

## 2 Related Work

The early days of graph representation learning date back to the end of the previous century, when researchers proposed backpropagation through structures to process directed acyclic graphs [63; 25]. These ideas laid the foundations for the adaptive processing of cyclic graphs by the recurrent Graph Neural Network  and the feedforward Neural Network for Graphs , which today's DGNs build upon. Both methods iteratively compute embeddings of the graphs' entities (also called nodes) via a local and iterative message passing mechanism  that propagates the information through the graph. In recent years, many neural and probabilistic DGNs have emerged [50; 36; 31; 69; 74; 3] bridging ideas from different fields of machine learning; we set up our analysis in the context of these message-passing architectures. Even more recently, transformer models have begun to appear in graph-related tasks as well [19; 37; 75]. Akin to kernel methods for graphs [71; 38], this class of methods mainly relies on feature engineering to extract rich information from the input graph, and some perform very well at molecular tasks . However, the architecture of transformers is not intrinsically more expressive than DGNs, and their effectiveness depends on the specific encodings used . Therefore, gaining a better understanding of the inductive bias of DGNs remains a compelling research question.

The construction of NN graphs found recent application in predicting the mortality of patients, by connecting them according to specific attributes of the electronic health records . In addition, it was used in natural language processing to connect messages and news with similar contents to tackle spam and fake news detection, respectively . In both cases, the authors computed similarity based on some embedding representation of the text, and the terms' frequency in a document was the graph-building criterion for a generic document classification task . Finally, \(k\)-NN graphs have also been built based on chest computerized tomography similarity for early diagnoses of COVID-19  or to understand if debts will be repaid in time .

Most of the theoretical works on DGNs deal with the problems of over-smoothing  and over-squashing  of learned representations, as well as the discriminative power of such models . In this context, researchers mostly agreed that DGNs based on message passing perform favorably for homophilic graphs and not so much for heterophilic ones . However, recent works suggest a different perspective; the generalization performances depend more on the neighborhood distributions of nodes belonging to different classes  and on a good choice of the model's weights . The Cross-Class Neighborhood Similarity (CCNS) Ma et al.  stands out as an effective (but purely empirical) strategy to understand whether a graph structure is useful or not for a node classification task. In our work, we take inspiration from the CCNS to study the behavior of the neighborhood class label distributions around nodes and compute the first approximation and lower bound of the CCNS for NN graphs without the need to explicitly build them.

Structure learning and graph rewiring are also related but orthogonal topics. Rather than pre-computing a fixed structure, these approaches discover latent dependencies between samples  and can enrich the original graph structure when this is available . They have been applied in contexts of scarce supervision, where a \(k\)-NN graph proved to be an effective baseline when combined with DGNs . At the same time, the combinatorial nature of graphs makes it difficult and expensive to explore the space of all possible structures, making the a priori construction of the graph a sensible and efficient alternative.

## 3 Methodology

We begin by introducing notions and assumptions that will be used throughout the paper. Our starting point is a classification task over a set of classes \(\), where each sample \(u\) is associated with a vector of \(D\) attributes \(_{u}^{D}\) and a target class label \(y_{u}\).

A graph \(g\) of size \(N\) is a quadruple \((,,,)\), where \(=\{1,,N\}\) represents the set of nodes and \(\) is the set of _directed_ edges \((u,v)\) from node \(u\) to node \(v\). The symbol \(=\{X_{u}, u\}\) defines the set of random variables (_r.v._) with realizations \(_{u}^{D},u\). The same definition applies to the set of target variables \(=\{Y_{u}, u\}\) and their realizations \(y_{u},u\). When we talk about the subset of nodes with target label \(c\), we use the symbol \(_{c}\); also, we define the _neighborhood_ of node \(u\) as \(_{u}=\{v|(v,u)\}\).

A Gaussian (or normal) univariate distribution with mean and standard deviation \(,,>0\) is represented as \((;,^{2})\), using \(^{D},^{D D}\) for the multivariate case. The probability density function (_p.d.f._) of a univariate normal _r.v._ parametrized by \(,\) is denoted by \(()\) together with its cumulative density function (_c.d.f._) \(F(w)=()=(1+(}}))\), where erf is the error function. Subscripts will denote quantities related to a specific random variable.

We want to transform the initial task into a node classification problem, where different _i.i.d._ samples become the nodes of a single graph, and the edges are computed by some nearest neighbor algorithm. We assume (_assumption_\(1\)) the true data distribution \(p(X=)\) of each sample (thus abstracting from \(u\)) is defined by the graphical model of Figure 1 (left). This model reflects the common assumption that observables of a given class share the same distribution , and we choose this distribution to be a mixture of Gaussian distributions. Such a choice is not restrictive since, in principle, with enough mixture components it is possible to approximate almost any continuous density . Note that it is also possible to sample discrete attribute values by rounding the samples of a properly parametrized Gaussian mixture. Formally, we consider \(||\) latent classes modeled by a categorical _r.v._\(C\) with prior distribution \(p(C=c)\) and \(||\) mixtures for each class modeled by \(M p(M=m|C=c),m,c\), such that \(p(|c)=_{m=1}^{}p(|m,c)p(m|c)\). In Section 3.1, we further require (_assumption_\(2\)) that the \(D\) observables are conditionally independent when the class \(c\) and the mixture \(m\) are known, i.e., \(p(X=)=_{c=1}^{|C|}p(c)_{m=1}^{||}p(m|c)_{f=1}^{D} p(x_{f}|m,c)\). We refer the reader to Section 3.3 for a discussion about this assumption. Figure 1 (right) depicts an example of one such data distribution for \(D=1\). Hereinafter we shall have \(p(X_{f}=x_{f}|m,c)=(x_{f};_{cmf},_{cmf}^{2})\) for the individual attributes and \(p(X=|m,c)=(;_{mc},_{mc})\) for the multivariate case. We will also use diagonal covariance matrices denoted as \(_{mc}=diag(_{mc1}^{2},,_{mcD}^{2})\). Finally, akin to , we assume that the neighbors of node \(v\) are _i.i.d._ (_assumption_\(3\)), meaning their attributes and class distributions only depend on the properties of node \(v\); such an assumption is especially reasonable when one considers how nearest neighbor structures are typically built, by picking a set of neighbors based only on their distance to the target node \(v\).

To talk about the surroundings of a point in space we will use the notion of hypercubes (or D-cubes). A hypercube of dimension \(D\) centered at point \(^{D}\) of side length \(>0\) is the set of points given by the Cartesian product \(H_{}()=[x_{1}-,x_{1}+][x_{D}-,x_{D}+]\).

### Analytical Computation of the CCNS for Nearest Neighbor Graphs

The CCNS  computes how similar the neighborhoods of two distinct nodes are in terms of class label distribution, and it provides an aggregated result over pairs of target classes. Intuitively, if nodes belonging to distinct classes happen to have similar neighboring class label distributions, then it will be unlikely that a classifier will correctly discriminate between these two nodes after a message passing operation because the nodes' embeddings will look very similar. On the other hand, nodes of different classes with very different neighboring class label distributions will be easier to separate. This intuition implies that nodes of different classes typically have different attribute values.

**Definition 3.1** (Cross Class Neighborhood Similarity, extended from ).: Given a graph \(g\), the cross-class neighborhood similarity between classes \(c,c^{}\) is given by

\[s(c,c^{})=_{p(|c)p(^{}|c^{})}[( q_{c}(),q_{c^{}}(^{}))]\] (1)

where \(\) computes a (dis)similarity score between vectors and the function \(q_{c}:^{D}^{||}\) (resp \(q_{c^{}}\)) computes the probability vector that a node of class \(c\) (resp. \(c^{}\)) with attributes \(\) (resp. \(^{}\)) has a neighbor of class \(c^{}\), for every \(c^{}\).

The definition of \(q_{c}\) and \(q_{c^{}}\) is the key ingredient of Equation 1. In the following, we are going to show that it is possible to analytically compute these quantities when we assume an NN structure. For every norm-induced metric, which is also convex, the lower bound of Equation 1 follows from Jensen's inequality and from the linearity of expectation:

\[s(c,c^{})||\ _{p(|c)p(^{}|c^{})}[q_{ c}()-q_{c^{}}(^{})]\ ||=||\ _{p(|c)}[q_{c}()]-_{p(^{}|c^{ })}[q_{c^{}}(^{})]\ ||.\] (2)

This bound assigns non-zero values to the _inter-class_ neighborhood similarity, but as we shall see one can resort to Monte Carlo approximations of Equation 1 to estimate both the _inter_ and _intra-class_ scores. From now on, we will use the Euclidean distance as our \(\).

Figure 1: _Left:_ we represent assumption \(1\) on the data distribution through a graphical model, i.e., a hierarchical mixture of distributions, where observed variables are shaded and latent ones are white circles. _Right:_ a visual example of a data distribution generated by the graphical model, with an intuitive visualization of the class posterior mass around a point \(_{u}\).

We first study the class label distribution in the surroundings of some node \(u\). In the example of Figure 1 (right), we consider a binary classification problem with \(||=2\) and depict the conditional distributions \(p(_{u}|C=0)\), \(p(_{u}|C=1)\) with green and blue curves, respectively. A dashed black line, instead, represents \(p()\) assuming a non-informative class prior. If the neighbors of \(u\) belong to the hypercube \(H_{}(_{u})\) for some \(\), then the probability that a neighbor will belong to class \(c\) depends on how much class-specific probability mass, that is the shaded green and blue areas, there is in the hypercube. Since the blue area is larger than the green one, finding a neighbor of class \(1\) is more likely to happen. Formally, we define the (unnormalized) probability of a neighbor belonging to class \(c\) in a given hypercube as the weighted posterior mass of \(C\) contained in that hypercube.

**Definition 3.2** (Posterior Mass \(M_{}(c,)\) Around Point \(\)).: Given a hypercube \(H_{}()\) centered at point \(^{D}\), and a class \(c\), the posterior mass \(M_{}(c,)\) is the unnormalized probability that a point in the hypercube has class \(c\):

\[M_{}(c,)=_{ H_{}()}[p(c| )]=_{ H_{}()}p(c|)p()d= p(c)_{ H_{}()}p(|c)d,\] (3)

where the last equality follows from Bayes' theorem.

The following proposition shows how to compute \(M_{}(c,)\) analytically. The interested reader can find all proofs of our paper in Section A.3.

**Proposition 3.3**.: _Under assumptions 1,2, \(M_{}(c,)\) has the following analytical form_

\[p(c)_{m=1}^{||}p(m|c)_{f=1}^{D}F_{Z_{ cmf}}(x_{f}+)-F_{Z_{cmf}}(x_{f}-),\] (4) \[Z_{cmf}(;_{cmf},_{cmf}^{2}).\]

To reason about an entire class rather than individual samples, therefore being able to compute the two quantities on the right-hand side of Equation 2, we extend the previous definition by taking into account all samples of class \(c^{}\). Thus, we seek to compute the average unnormalized probability that a sample belongs to class \(c\) in the hypercubes centered around samples of class \(c^{}\).

**Definition 3.4** (Expected Class \(c\) Posterior Mass \(M_{c^{}}(c,)\) for Samples of Class \(c^{}\)).: Given a hypercube length \(\) and a class \(c^{}\), the unnormalized probability that a sample of class \(c^{}\) has another sample of class \(c\) lying in its hypercube is defined as

\[M_{c^{}}(c,)}{=}_{ p (|c^{})}[M_{}(c,)].\] (5)

It is also possible to compute \(M_{c^{}}(c,)\) in closed form, which we show in the next theorem.

**Theorem 3.5**.: _Under assumptions 1,2, \(M_{c^{}}(c,)\) has the following analytical form_

\[p(c)_{m=1\\ m^{}=1}^{||}p(m^{}|c^{})p(m|c) _{f=1}^{D}((m^{}f}+-_{cmf}}{^{2}+_{c^{}m^{} f}^{2}}})-(m^{}f}--_{cmf}}{^{2}+_{c^{}m^{} f}^{2}}})).\] (6)

Thanks to Theorem 3.5, we know how much class-\(c\) posterior probability mass we have, on average, around samples of class \(c^{}\). To get a proper class \(c^{}\)-specific distribution over neighboring class labels, we apply a normalization step using the fact that \(M_{c^{}}(c,) 0\;\; c\).

**Definition 3.6** (\(\)-Neighboring Class Distribution).: Given a class \(c^{}\) and an \(>0\), the neighboring class distribution around samples of class \(c^{}\) is modeled as

\[p_{c^{}}(c,)=}(c,)}{_{i=1}^{| |}M_{c^{}}(i,)}.\] (7)

This distribution formalizes the notion that, in a neighborhood around points of class \(c^{}\), the probability that points belong to class \(c\) does not necessarily match the true prior distribution \(p(C=c)\). However, this becomes false when we consider an infinitely-large hyper-cube, as we show in the next result.

**Proposition 3.7**.: _Let the first \(D\) derivatives of \(_{i=1}^{|c|}M_{c^{}}(i)\) be different from 0 in an open interval \(I\) around \(=0\) and not tending to infinity for \( 0\). Under assumptions 1,2, Equation 7 has limits_

\[_{ 0}p_{c^{}}(c,)=}^{||}p(m|c)p(m^{}|c^{})_{f=1}^{D}_{Z_{ emm^{}f}}(0)}{_{i}^{||}p(i)_{m,m^{}}^{||}p(m|i)p( m^{}|c^{})_{f=1}^{D}_{Z_{imm^{}f}}(0)},_{ }p_{c^{}}(c,)=p(c)\] (8)

_where \(Z_{imm^{}f}\) has distribution \((;-a_{imm^{}f},b_{imm^{}f}^{2})\),_

\[a_{imm^{}f}=2(_{c^{}m^{}f}-_{imf})b_{imm^{}f}=2^{2}+_{c^{}m^{}f}^{2}}.\]

The choice of \(\), which implicitly encodes our definition of "nearest" neighbor, plays a crucial role in determining the distribution of a neighbor's class label. When the hypercube is too big, the probability that a neighbor has class \(c\) matches the true prior \(p(c)\) regardless of the class \(c^{}\), that is we make a crude assumption about the neighbor's class distribution of a sample. If we instead consider a smaller hypercube, then we observe a less trivial behavior and the probability \(p_{c^{}}(c,)\) is directly proportional to the distances between the means \(_{c^{}m^{}}\) and \(_{cm}\), as one would intuitively expect for simple examples.

To summarize, we can use \(p_{c^{}}(c,)\) as an approximation for \(_{p(|c^{})}[q_{c}^{}()]\) of Equation 2; similarly, we can use a normalized version of \(M_{c}()\) in place of \(q_{c}()\) to estimate Equation 1 via Monte Carlo sampling without the need of building a NN graph. For space reasons, in Appendix A.5, we qualitatively discuss the results for a 1-dimensional instance of a data distribution, and we investigate the quality of the CCNS approximation while varying \(k\).

### Class Separability Induced by DGNs on a _k_-NN Graph

This section formally investigates the properties of the embedding space created by DGNs under the \(k\)-NN graph. Our goal is to understand whether using such a graph can improve the separability between samples belonging to different classes or not. Provided that our assumptions hold, both conclusions would be interesting: if the \(k\)-NN graph helps, then we know the conditions for that to happen; if that is not the case, we have identified and formalized the need for new graph construction mechanisms.

Akin to previous works [40; 16; 5; 43], we consider a linear 1-layer DGN with the following neighborhood aggregation scheme that computes node embeddings \(_{u}^{D}\  u\):

\[_{u}=_{v_{u}}_{v}.\] (9)

The node embedding of sample \(u\) is then fed into a standard machine learning classifier, e.g., an MLP. As done in previous works, we assume that a linear (learnable) transformation \(^{D D}\) of the input \(_{v}\), often used in DGN models such as the Neural Network for Graphs  and the Graph Convolutional Network , is absorbed by the subsequent classifier.

Mimicking the behavior of the \(k\)-NN algorithm, which connects _similar_ entities together, we model the attribute distribution of a neighbor \(v_{u}\) as a normal distribution \((_{v};_{u},diag(^{2},,^{2})\), where \(^{2}\) is a hyper-parameter that ensures it is highly unlikely to sample neighbors outside of \(H_{}(_{u})\); from now on we use the symbol \(_{}\) to make this connection clear. Under assumption \(3\), neighbors' sampling is independently repeated \(k\) times and the attributes are averaged together. Therefore, we use statistical properties of normal distributions (please refer to Lemmas A.2 and A.1) to compute the resulting node \(u\)'s embedding distribution:

\[p(_{u}|_{u})=(_{u};_{u},_{ }),_{}=diag(_{}^{2}/k, ,_{}^{2}/k).\] (10)

Therefore, the more neighbors a node has (i.e., higher \(k\)) the more skewed the resulting distribution is around \(_{u}\) (i.e., lower variance). This is a fairly reasonable assumption if we think of an infinitely large dataset; informally, each of the \(k\) neighbors will be much more likely to lie near \(_{u}\) than near the surface of the hypercube, hence \(_{u}\) will be close to \(_{u}\) with high probability.

To understand how Equation 9 affects the separability of samples belonging to different classes, we compute a divergence score between the distributions \(p(|c)\) and \(p(|c^{})\)[68; 51; 20; 8; 52]. Whenthis divergence is higher than that of the distributions \(p(x|c)\) and \(p(x|c^{})\), then the \(k\)-NN structure and the inductive bias of DGNs are helpful for our task. Below, we show that for two mixtures of Gaussians we can obtain the analytical form of their Squared Error Distance (SED), the simplest symmetric divergence defined as \(SED(p,q)=(p(x)-q(x))^{2}dx\). This provides a concrete strategy to understand, regardless of training, if it would make sense to build a \(k\)-NN graph for our problem.1

**Proposition 3.8**.: _Under assumptions 1,3, let us assume that the entity embeddings of a 1-layer DGN applied on an artificially built \(k\)-NN graph follow the distribution of Equation 10. Then the Squared Error Distances \(SED(p(|c),p(|c^{}))\) and \(SED(p(|c),p(|c^{}))\) have analytical forms._

In the interest of space, we have deferred the analytic formulas to the appendix.

As an immediate but important corollary, a \(k\)-NN graph improves the ability to distinguish samples of different classes \(c,c^{}\) if it holds that \(SED(p(|c),p(|c^{}))>SED(p(|c),p(|c^{}))\). Indeed, if class distributions diverge more in the embedding space, which has the same dimensionality as the input space, then they will be easier to separate by a universal approximator such as an MLP. We use this corollary in our experiments, by reverse-engineering the theory to find out "good" data models.

### Limitations and Future Directions

It is extremely challenging to prove that a \(k\)-NN graph _never_ helps to improve the classification performances using a DGN. The reason is that theoretical results of this kind rely, as is common , on assumptions of the true data generating distribution and on the specific assignments of parameters such as \(\) and \(\). The true data distribution is typically unknown and hard to find , but assumptions 1 and 3 remain quite reasonable due to the flexibility of mixture models and the nature of the nearest neighbors construction. In addition, assumption 2 has been widely used in the graphical models literature because it is particularly attractive from a theoretical perspective , despite it cannot cover all real-world cases; we note, however, that it plays no role in the analysis of Section 3.2. Finally, as done in the past [40; 15; 5; 43], we have ignored the non-linearity in the neighborhood aggregation of Equation 9. As we shall see in our empirical results, the intuitions we gained from the theoretical analysis seem to hold for non-linear models as well.

Our analysis has considered hyper-cubes in the interest of simplicity, but nearest neighbor graphs typically compute hyper-balls centered at a point. In high-dimensional spaces, the volume of hyper-cubes and hyper-balls can differ in non-trivial ways , so future work should improve on our results by replacing hyper-cubes with hyper-balls. Similarly, the results of Section 3.1 could be used to guide the definition of new graph construction strategies based on attribute similarity criteria, for instance by proposing a good \(\) for the data at hand. We leave this idea to future investigations.

## 4 Experiments

We conduct quantitative and qualitative experiments to support our theoretical insights. For all experiments we used a server with \(32\) cores, \(128\) GBs of RAM, and \(4\) GPUs with \(11\) GBs of memory.

Quantitative ExperimentsQuantitatively speaking, we want to compare a structure-agnostic baseline, namely an MLP , against different graph machine learning models, such as a simple DGN that implements Equation 9 followed by an MLP classifier, the Graph Isomorphism Network (GIN)  and the Graph Convolutional Network (GCN) . The goal is to show that in the fully-supervised setting using a \(k\)-NN graph does not offer any concrete benefit. We consider \(11\) datasets, eight of which were taken from the UCI repository , namely Abalone, Adult, Dry Bean, Electrical Grid Stability, Isolet, Musk v2, Occupancy Detection, Waveform Database Generator v2, as well as the citation networks Cora, Citeseer , and Pubmed . For each dataset, we build a \(k\)-NN graph, where \(k\) is a hyper-parameter, using the node attributes' similarity to find neighbors (discarding the original structure in the citation networks). We report some datasets statistics in Table 2, and the metric to optimize is the F1 score. Note that it is difficult to ascertain if our assumptions hold for these datasets because we have no access to the true data distribution; however, a robust comparison for this topic is lacking in the literature and can help us gather further insights on whether our ideas might also hold in a more general setting. For every dataset and model, we follow the rigorous and fair evaluation setup of : we perform a \(10\)-fold cross validation for risk assessment,with a hold-out model selection (90% training/10% validation) for each of the \(10\) external training folds. We report the hyper-parameters for the model selection phase in Table 3. For each of the 10 best models, we perform \(3\) final re-training runs and average their test performances on the corresponding external fold to mitigate bad initializations. The final score is the average of these 10 test results.

Qualitative ExperimentsWe provide four qualitative experiments. First, we study how \(p_{c}(c^{},)\) changes for varying values of \(\) and different priors \(p(c)\) under specific class-conditional data distributions. Second, we analyze the first limit of Proposition 3.7 under the same conditional data distributions as before (\(D=1\) hence assumptions are trivially satisfied). We translated one of the distributions by different values and computed the SED between them, together with the quantity \(p_{c}(c^{},)\) under different priors. The goal here is to show the relation between the SED and the limit. Third, we reverse-engineer the theoretical results to find, if possible, data distributions satisfying \(SED(p(|c),p(|c^{}))>SED(p(|c),p(|c^{}))\). There is no dataset associated with this experiment, rather we learn the parameters of the graphical model of Figure 1 (left) together with \(,k,_{}\) (\(k\) is treated as a continuous value during the optimization) in order to minimize the following objective: \(_{SED}-_{CCNS}\), where \(\) is a hyper-parameter. \(_{SED}\) sums \(SED(p(|c),p(|c^{}))-SED(p(|c),p(|c^{}))\) for all pairs of distinct classes \(c,c^{}\), whereas \(_{CCNS}\) computes the lower bound for the inter-class similarity, thus acting as a regularizer that avoids the trivial solution \(p(|c)=p(|c^{})\) for class pairs \(c,c^{}\). The set of configurations for this experiment is reported in Table 4, and we use Adam  as optimizer. In the last qualitative experiment, we compute the true CCNS and its approximations for the specific example of Figure 1 (right), showing that the quantities obtained in Proposition 3.3 and Theorem 3.5 are good representatives of the CCNS for nearest neighbor graphs. Please refer to Appendix A.5 for an additional study of this kind. 2

## 5 Empirical Results

We now present our empirical results to support the theory. The interested reader can find statistics on the chosen hyper-parameters and additional ablation studies in Appendix A.6.

Quantitative ResultsTable 1 details the empirical comparison between the structure-agnostic MLP and the structure-aware baselines sDGN, GIN, and GCN. We observe that in terms of Micro F1 score, which is the metric all models are optimized against, there is _no statistically significant improvement_ in performance (unpaired two samples t-test with p-value \(0.05\)) for the different DGNs when using an artificial \(k\)-NN graph. When looking at accuracy one can observe a similar trend, but the results fluctuate more because accuracy is not the metric to be optimized. These results further confirm that using \(k\)-NN graphs does not bring a substantial contribution to the generalization performances. On the contrary, it would seem the \(k\)-NN graph can even be considered harmful for the performances of GIN and GCN (e.g., DryBean, Musk, eGrid, Waveform). We hypothesize that the information flow

    &  &  &  &  \\  & F1 & ACC & F1 & ACC & F1 & ACC & F1 & ACC \\  Abalone & **0.55\(_{0.02}\)** & 55.5\(_{1.5}\) & **0.55\(_{0.01}\)** & 55.3\(_{1.2}\) & 0.54\(_{0.03}\) & 54.2\(_{1.7}\) & 0.54\(_{0.02}\) & 54.4\(_{1.7}\) \\ Adult & 0.70\(_{0.02}\) & 77.6\(_{1.7}\) & **0.72\(_{0.02}\)** & 77.8\(_{1.9}\) & 0.69\(_{0.02}\) & 76.2\(_{1.3}\) & 0.70\(_{0.02}\) & 78.6\(_{0.9}\) \\ DryBean & **0.78\(_{0.04}\)** & 77.4\(_{0.4}\) & 0.77\(_{0.06}\) & 77.3\(_{5.4}\) & 0.75\(_{0.03}\) & 74.4\(_{3.2}\) & 0.71\(_{0.07}\) & 72.6\(_{4.9}\) \\ eGrid & **0.95\(_{0.01}\)** & 95.6\(_{0.6}\) & **9.05\(_{0.01}\)** & 95.8\(_{0.4}\) & 0.93\(_{0.01}\) & 94.0\(_{0.6}\) & 0.82\(_{0.02}\) & 83.1\(_{1.4}\) \\ Isolet & **0.95\(_{0.01}\)** & 95.4\(_{0.5}\) & **0.95\(_{0.00}\)** & 95.4\(_{0.3}\) & **0.95\(_{0.01}\)** & 95.3\(_{0.7}\) & 0.92\(_{0.01}\) & 91.7\(_{1.4}\) \\ Musk & **0.99\(_{0.01}\)** & 99.3\(_{0.3}\) & **0.99\(_{0.01}\)** & 99.5\(_{0.3}\) & 0.94\(_{0.05}\) & 97.0\(_{2.2}\) & 0.94\(_{0.02}\) & 96.7\(_{0.9}\) \\ Occupancy & **0.99\(_{0.01}\)** & 98.9\(_{0.4}\) & **0.99\(_{0.01}\)** & 98.9\(_{0.5}\) & 0.98\(_{0.01}\) & 98.6\(_{0.6}\) & 0.90\(_{0.17}\) & 95.8\(_{0.0}\) \\ Waveform & **0.85\(_{0.02}\)** & 85.5\(_{1.7}\) & **0.85\(_{0.02}\)** & 85.3\(_{1.5}\) & 0.84\(_{0.02}\) & 84.5\(_{1.7}\) & 0.84\(_{0.03}\) & 84.0\(_{2.2}\) \\ Cora & 0.71\(_{0.04}\) & 74.3\(_{2.4}\) & 0.71\(_{0.03}\) & 74.1\(_{2.5}\) & **0.72\(_{0.03}\)** & 74.9\(_{2.4}\) & 0.70\(_{0.04}\) & 73.0\(_{2.6}\) \\ Citeseer & 0.69\(_{0.02}\) & 71.8\(_{1.9}\) & 0.69\(_{0.03}\) & 72.9\(_{2.7}\) & 0.69\(_{0.02}\) & 72.1\(_{2.2}\) & **0.71\(_{0.03}\)** & 74.4\(_{2.4}\) \\ Pubmed & **0.87\(_{0.01}\)** & 87.6\(_{0.7}\) & **0.87\(_{0.01}\)** & 87.6\(_{0.6}\) & **0.87\(_{0.01}\)** & 86.6\(_{0.9}\) & 0.83\(_{0.01}\) & 83.5\(_{0.9}\) \\   

Table 1: Node classification mean and standard deviation results for a structure-agnostic baseline (MLP) compared to graph machine methods applied to artificially built \(k\)-NN graphs. We performed model selection w.r.t. the F1 score, but we also include the accuracy for completeness.

of message passing does not act anymore as a regularizer (we refer the reader to the discussion in Section 1) as it might happen the scarce supervision scenario , rather it becomes harmful noise for the learning process. This is possibly the reason why sDGN, which is also the simplest of the DGNs considered, tends to overfit less such noise and always performs on par with the MLP.

Therefore, one of the main takeaways of the quantitative analysis is that the \(k\)-NN graph might generally not be a good artificial structure for addressing the classification of tabular data using graph representation learning methods. Different artificial structures, possibly inspired by our theoretical results, should be proposed to create graphs that help DGNs to better generalize in this context. We note that this research direction is broader than tabular classification, but tabular data offers a great starting point since samples are typically assumed _i.i.d._ in ML and are thus easier to manipulate. Also, in Appendix A.4, we discuss the impact of a different \(\) (the cosine similarity) in the construction of the \(k\)-NN graphs.

These results also substantiate our conjecture that, under assumptions \(1,3\), the \(k\)-NN graph _never_ induces a better class separability in the embedding space: since \(p(|c)\) is a mixture of distributions with the same mean but higher variance than \(p(|c)\) (proof of Proposition 3.8), in a low-dimensional space intuition suggests that the distributions of different classes will always overlap more in the embedding space than in the input space.

Qualitative ResultsWe present the main qualitative results in Figures 2, 3 and 4. First, as we can see from Figure 2, when \(\) is very small the probability that a neighbor is of the same class is high for both classes (but not the same), since there is less overlap between samples of the green (\(C=0\)) and blue (\(C=1\)) distributions. As we increase the side length of the hypercube (or equivalently, we consider more and more nearest neighbors), the curves converge to the priors that we set for this example. This result is in agreement with the second limit of Proposition 3.7.

As regards the first limit of Proposition 3.7, in Figure 3 we observe that the behavior of \(_{ 0}p_{c}(c^{},)\) is non-trivial when the two curves still overlap. In particular, the greater the overlap the closer \(p_{c}(c,)\) is to \(p(c)\), which makes sense since it becomes harder to distinguish samples of different classes. On the other hand, the behavior becomes trivial when the curves are distant enough from each other: there, in the limit, every node will only have neighbors of the same class.

In the reverse-engineering experiment of Figure 4 (left), for each possible configuration of the models tried in Table 4, we have computed the \(_{SED}\) for varying values of \(k\). We have then normalized all values for readability, by dividing each value of \(SED(p(|c),p(|c^{}))\) by \(SED(p(|c),p(|c^{}))\), as the latter is independent of \(k\). This particular figure depicts the curves for the binary classification cases, but the conclusions do not change for multi-class classification, for instance with 5 classes. We find that the normalized value of \(SED(p(|c),p(|c^{}))\) on the \(y\)-axis is always upper-bounded by \(1\), meaning that \(SED(p(|c),p(|c^{}))<SED(p(|c),p(|c^{}))\) for all the configurations tried, even in higher dimensions. This result further indicates that it could be unlikely that a \(k\)-NN graph induces better class separability in the embedding space of a DGN. We ask machine learning practitioners to take these insights into consideration when building artificial nearest neighbor graphs.

Lastly, we go back to the data distribution of Figure 1 (right) and show how to approximate the true CCNS under the \(k\)-NN graph with our theoretical framework. We first generate \(10000\) data points and set _w.l.o.g._\(k=5\) to connect them. We compute the lower bound of the CCNS in the first

Figure 2: We show how \(p_{c}(c^{},)\) varies for different values of \(\) and prior distributions assuming the class-conditional data distributions on the left.

heatmap (a), a Monte Carlo (MC) approximation of Equation 1 (\(1000\) samples per class pair) in the heatmap (b), and the true CCNS in the heatmap (c). In this case the lower bound has computed a good approximation of the inter-class CCNS, and the MC estimate is very close to the true CCNS. In Appendix A.6, we also show that the MC approximation is better for higher values of \(k\). It is important to recall that both heatmaps (a) and (b) are computed using the theoretical results of Section 3.1 and an \(\) close to the empirical one, hence it seems that the theory is in accord with the evidence.

## 6 Conclusions

We introduced a new theoretical tool to understand how much nearest neighbor graphs can help in the classification of tabular data. Our theory and empirical evidence suggest that some attribute-based graph construction mechanisms, namely the \(k\)-NN algorithm, are not promising strategies for obtaining better DGNs' generalization performances. This is a particularly troubling result since researchers often used nearest neighbor graphs when the graph structure was unavailable. We recommend using great care in future empirical evaluations of such techniques by always comparing against structure-agnostic baselines to ascertain real improvements from fictitious ones. Moreover, we provided a theoretically principled way to model the CCNS for nearest neighbor graphs, showing its approximation in a practical example. We hope that the results of this work will foster better strategies for building artificial graphs or as a guideline for new structure learning methods.

Figure 4: _Left:_ Each curve specifies the variation of \(SED(p(|c),p(|c^{}))\) against different values of \(k\) for different hyperparametersâ€™ configurations that have been trained to minimize the learning objective. The curves are normalized w.r.t their corresponding \(SED(p(|c),p(|c^{}))\). _Right:_ The CCNS lower bound (a), its Monte Carlo approximation for \( 0.1\), obtained without creating any graph (b), and the true CCNS (c) for the example of Figure 1 and a 5-NN graph. Heatmaps (a) and (b) are computed using the theory of Section 3.1. Please refer to the text for more details.

Figure 3: We show how \(p_{c}(c^{},)\) varies for different translations of the conditional data distributions \(p(x|0)\) and different priors as \(\) approaches \(0\).