# SanFlow: Semantic-Aware Normalizing Flow for Anomaly Detection and Localization

Daehyun Kim1 Sungyong Baik2 Tae Hyun Kim3

Dept. of Artificial Intelligence1, Dept. of Data Science2, Dept. of Computer Science3

Hanyang University

{daehyun, dsybaik, taehyunkim}@hanyang.ac.kr

Correspondence to: Tae Hyun Kim <taehyunkim@hanyang.ac.kr>.

###### Abstract

Visual anomaly detection, the task of detecting abnormal characteristics in images, is challenging due to the rarity and unpredictability of anomalies. In order to reliably model the distribution of normality and detect anomalies, a few works have attempted to exploit the density estimation ability of normalizing flow (NF). However, previous NF-based methods forcibly transform the distribution of all features into a single distribution (e.g., unit normal distribution), even when the features can have locally distinct semantic information and thus follow different distributions. We claim that forcibly learning to transform such diverse distributions to a single distribution with a single network will cause the learning difficulty, thereby limiting the capacity of a network to discriminate between normal and abnormal data. As such, we propose to transform the distribution of features at each location of a given input image to different distributions. Specifically, we train NF to map the feature distributions of normal data to different distributions at each location in the given image. Furthermore, to enhance the discriminability, we also train NF to map the distribution of abnormal data to a distribution significantly different from that of normal data. The experimental results highlight the efficacy of the proposed framework in improving the density modeling and thus anomaly detection performance.

## 1 Introduction

Abnormal events that deviate significantly from expected and typical characteristics are often unwanted and are referred to as anomalies. The objective of anomaly detection is to identify such abnormal events. As such, anomaly detection is applicable across domains (e.g., video-surveillance, product defect detection, medical image analysis, etc.), where abnormal events can indicate or lead to critical issues. However, abnormal events occur rarely and can appear in various forms that cannot be known in advance, making it infeasible to collect a large amount of abnormal data in many real-world scenarios. The difficulty in collecting abnormal data has prevented anomaly detection from exploiting the recent breakthrough in supervised learning, leaving the field still challenging.

Acknowledging the difficulty of collecting abnormal data, many works have formulated anomaly detection as unsupervised learning, one-class learning, or few-shot learning , in which the goal is to accurately model a distribution of normal data. Then, any data that deviates from the learned distribution of normal data is considered as an anomaly. The approaches to unsupervised anomaly detection differ by how they recognize the differences between normal and abnormal data. Reconstruction-based methods learn to reconstruct normal images and thus distinguish input images as abnormal if the reconstruction error is high . Meanwhile, representation-based approaches aim to learn a feature space, where normal images are brought close to each otherand abnormal images are placed far away from normal images [11; 8; 36]. Another line of works aims to simulate abnormalities through data augmentation [54; 26]. On the other hand, a recently emerging trend is to employ normalizing flow (NF) for a more reliable estimation of normal data distribution [51; 20; 37].

NF aims to achieve better density estimation by learning a sequence of reversible functions to map a complex distribution of input data into a simple distribution (e.g., normal distribution). The goal of NF aligns with that of anomaly detection in that a reliable density estimation of normal samples leads to the accurate detection of anomalies. However, previous NF-based methods have solely relied on the capability of NF to model complex distributions as simple ones. Although features at different locations, scales and images can follow different distributions, previous works have attempted to map such multi-modal distribution to simply a single normal distribution.

Considering that features at different locations and images may follow different distributions, we propose to adaptively embed the features at each location of each image into different distributions accordingly. We make two contributions in this work: First, we propose a novel NF-based framework that transforms the feature distribution of normal data at each location into Gaussian distributions with zero mean but different variances. The variance at each location is estimated by an external network that is conditioned on image features and semantics. Second, to enhance the detection of anomalies, we train NF to transform abnormal features into a distinct Gaussian distribution with a mean that is distant from the distributions of normal features. By embedding the features to locally different distributions, we facilitate the training of NF. We demonstrate that the estimated variance is lower for simple regions such as the background, while it increases for more complex regions. A description of the proposed approach is shown in Figure 1 and evaluation examples by category are shown in Figure 2.

The proposed framework demonstrates outstanding performance in both anomaly detection and localization. The strong empirical results underline the effectiveness of the proposed method in learning a more reliable density estimation, suggesting the importance of learning different target distributions for abnormal features and different normal features.

## 2 Related Work

Unsupervised anomaly detection tackles a challenging scenario, in which training images only consist of normal images that are free of anomalies. Under such challenging scenarios, standard supervised learning algorithms fail to work since training data is available for only one class (normal data), therefore often referred to as one-class learning. Methodologies that have emerged to overcome this issue largely differ by how they recognize the differences between normal data and abnormal data .

**Reconstruction based.** Reconstruction based approaches are hinged on the motivation that reconstruction models can reconstruct images well, if images are drawn from the same distribution as training images. Thus, these approaches distinguish images with high reconstruction error as abnormal. Several methods perform reconstruction by using auto-encoder architecture [41; 57; 6; 55; 32; 48; 45; 5] or the generator part of GAN models [1; 10; 31; 40].

Figure 1: Motivation Overview: Different semantic features can be observed in different regions. Abnormal regions (red box) exhibit significantly distinct features compared to normal features (orange and blue boxes). Moreover, normal regions also display different semantic features based on the complexity of the region. Upon our observation, we propose Semantic-Aware Normalizing Flow (SANFlow) that enables more accurate density estimation by adaptively embedding semantic feature distribution into corresponding but different base distributions.

**Data augmentation based.** A few methods have proposed to generate abnormal images through data augmentation that is based on either cut-out , random erasing , noise generation , or geometric transformations [17; 16; 21]. In particular, CutPaste  introduces a new data augmentation to generate more fine-grained and subtle abnormal local patches in normal images. Furthermore, other research areas use synthesized abnormal images to improve performance such as semantic segmentation [23; 19; 7] or image classification .

**Representation based.** Another recent line of research has focused on designing systems that can leverage the strong representation power of pre-trained convolutional neural networks. [11; 36; 8; 49; 46; 56; 34]. Then, anomaly detection is performed by measuring the distances between test image features and normal image features. The methods differ by how the features of normal image are maintained or modeled and how the distance is measured. SPADE  employs k-nearest neighbor (k-NN) for normal image features retrieval and feature correspondence for measuring the differences. To mitigate the complexity issue with nearest neighbor search, PatchCore  maintains a subset of features via coreset subsampling. Meanwhile, PaDiM  models the normal features as a multivariate Gaussian distribution and uses Mahalanobis distance  as a distance measure, instead of the k-NN algorithm, to improve the inference complexity.

**Normalizing flow based.** Recently, a few works have shifted the attention to employing normalizing flow (NF) to better estimate the distribution of the features of normal patches. Through a sequence of learnable and invertible transformations, NF embeds the feature distribution into a simple normal distribution. DifferNet  is one of the first to employ NF to transform the distribution of CNN features to a normal distribution, where anomaly detection is performed at image level by measuring the likelihood of samples. Then, CFLOW-AD  extends NF-based methodology to pixel-level anomaly detection. FastFlow  further improves the performance by utilizing normalizing flow in two-dimensional space, unlike previous one-dimensional NF-based anomaly detection algorithms. In addition, CS-Flow  proposed a new kind of NF model that jointly processes multiple feature maps across different scales. However, they forcefully embed different feature distributions of semantic patches to a single normal distribution, despite recent findings in classification that it is beneficial to transform images of different semantics into different distributions [22; 3]. By contrast, we propose to train NF to transform the distributions at different locations to simple, but different distributions to better exploit the density estimation capability of NF by assuming locally varying base distributions.

## 3 Proposed method

In this section, we describe the overall pipeline of our framework, dubbed Semantic-Aware Normalizing Flow (SANFlow), as depicted in Figure 3. Given an image \(\), a pre-trained feature extractor \(f\) is used to obtain features \(_{i} V_{i}\) at the \(i\)-th position. Then, we employ normalizing flow (NF) \(g^{-1}:V_{i} Z_{i}\) to embed the feature distribution \(V_{i}\) (a.k.a. target distribution) to simple and well-known distribution \(Z_{i}\) (a.k.a. base distribution). To further guide NF to map feature distributions \(V_{i}\) into different base distributions \(Z_{i}\) upon semantics, we introduce an external network \(h\) which allows us to predict statistics of \(_{i}\) for each feature \(_{i}\). Moreover, similar to , we employ data

Figure 2: Anomaly localization results by our proposed framework on MVTec benchmark dataset. From top to bottom: anomaly localization prediction, GT mask, anomaly score map and estimated variance map of input image. Variance maps are derived from mode values of \(\) and \(\) of each pixel and bright areas represent high variance and dark areas represent low variance values for corresponding distribution.

augmentation to synthesize local anomalies within the input image in order to train NF to embed the distribution of anomaly features into a distribution \(Z_{i}^{a}\) that is distinct from the distributions of normal features \(Z_{i}^{n}\).

Then, we modify the standard objective function of NF to account for different base distributions (Section 3.5). Finally, at test time, we use the trained NF to estimate the probability of given features to be normal, the negation of which is used as an anomaly score function to detect anomalies in a given image (Section 3.6).

### Background on normalizing flow

Normalizing flow (NF)  aims to learn a function \(g\) that transforms the latent variable \(\), which follows a simple and tractable base distribution \(Z\), into the variable \(\), which follows a complex target distribution \(V\). To do so, NF formulates \(g\) as the composition of invertible functions \(\{g^{l}\}_{l=1}^{l}\), i.e., \(g=g^{L} g^{L-1} g^{1}\), where \(L\) is the number of invertible functions. Such transformation of a complex distribution into a simple one allows for efficient and exact density estimation, serving as a powerful tool in density estimation. By applying the change of variables theorem in a sequence, the log-likelihood of the target distribution \(p_{V}()\) can be expressed in terms of the log-likelihood of the base distribution \(p_{Z}()\):

\[ p_{V}()= p_{Z}()-_{l=1}^{L} \,}{d^{l-1}},\] (1)

where \(^{l}\) represents the resulting random variable after applying up to the \(l\)-th function \(g^{l}\) on \(\). Note that \(^{0}\) corresponds to \(\) while \(^{L}\) corresponds to \(\). For efficient computation of the log-likelihood, functions \(\{g^{l}\}_{l=1}^{L}\) need to be invertible and have easy-to-compute Jacobian determinant. To meet such requirements, one of popular NF instances, RealNVP , formulated each function \(g^{l}\) as an affine coupling block. In an affine coupling block, the input is first divided into two equal-dimension parts. Then, each part undergoes affine transformation, whose parameters are generated by a network conditioned on the other part. The transformed parts, in turn, are put back together via concatenation.

Then, the log-likelihood in Equation 1 is maximized (or the negative log-likelihood is minimized) to train the parameters of transformation function. A base distribution \(Z\) is commonly chosen as a normal distribution \((,)\) in previous NF-based anomaly detection algorithms [20; 51; 37], resulting in a loss function (i.e., negative log-likelihood) as follows:

\[_{}=||_{2}^{2}}{2}+_{l=1}^{L} \,}{d^{l-1}}.\] (2)

### Synthetic anomaly generation

In unsupervised anomaly detection scenarios, abnormal training images are typically not available. Therefore, we synthesize abnormal images to facilitate training of NF. Abnormal images usually differ from normal images only in local regions, which are semantically or structurally similar to surrounding normal regions. To generate such abnormal data, CutPaste  introduces data augmentation technique that disturbs local regions of normal images with semantically similar patches. The extracted patches, in turn, undergo transformations (e.g., random flip, rotate, blur, and color jittering) and replace the region of other normal images of the same category at random location. In this work, we use the CutPaste method to synthesize abnormal images. We modify this method slightly to generate more realistic abnormal images by blurring the borders of extracted patches and applying diverse color jittering values to these patches. Also, we make abnormal patches by taking small rectangular patches like CutPaste or newly used circular patches. Size of patches are random and from a normal image to retain diversity of abnormal regions and use the same ratio of all types of patches and normal data for training. Then, during training, our NF model handles normal and abnormal regions of each input image \(\) differently by assuming locally different base distributions. To achieve this, we introduce a binary mask \(\) as shown in Figure 3 for each synthetic anomaly image \(\) to denote whether each pixel at pixel location \((w,h)\) belongs to the normal region (\(_{w,h}=1\)) or the abnormal region (\(_{w,h}=0\)).

### Feature extractor

Recently, features extracted by convolutional neural networks (CNN) pre-trained on ImageNet  have been shown to be helpful in anomaly detection, by providing useful semantic information [11; 5; 8; 20; 48]. In particular, several methods have employed a multi-scale feature pyramid  to handle anomalies of diverse sizes [20; 48; 8]. This is because features of different scales capture information about regions of corresponding sizes (i.e., effective receptive field) in the image . Motivated by previous findings, we also employ a pre-trained CNN [11; 20; 36] to obtain a \(K\)-level feature pyramid. The feature map at the \(k\)-th scale is denoted by \(^{k}^{C_{k} H_{k} W_{k}}\) where \(C_{k}\) is the number of channels and \(H_{k}\) and \(W_{k}\) are the height and width of the feature maps, respectively.

### Semantic-aware normalizing flow

#### 3.4.1 Scale- and spatial-aware normalizing flow

We train NF to map our feature vectors computed by CNN to latents which follow simple base distributions. However, features can follow complex distributions at each scale and spatial location of feature map. Thus, using a single NF model to map such complex features to a single base distribution can be difficult. Aiming to bridge the gap, CFLOW-AD  applied two modifications to a standard NF baseline model RealNVP : independent NF models for each scale and conditioning NF on position embedding vector [15; 44], similar to . Inspired by CFLOW-AD, we also employ \(K\) independent NF models to handle features across different scales as we have \(K\)-level pyramid features and position embedding vector conditioning. Moreover, we train the NF model to map features to latents that follow a spatially varying underlying distribution. This allows NF to consider semantic information, such as feature complexity, for corresponding locations in the image.

To be specific, we create a set of vectors by taking a feature vector \(_{i}^{k}\) at the \(i\)-th position (i.e., 2D coordinate) of the \(k\)-th scale feature map \(^{k}\), as done in the previous work . Note that, we omit the scale index \(k\) from this point on to avoid cluttered notation. To retain the spatial information, each feature vector \(_{i}\) is concatenated with the corresponding position embedding vector \(_{i}^{D}\), whose values are composed of sinusoidal harmonics and unique to each \(i\)-th position [15; 44]. The resulting vector \(_{i}=(_{i},_{i})\) is now used to condition each transformation function \(g^{l}\) in NF, making it process position-aware . Then, the log-likelihood of feature distribution becomes:

\[ p_{V_{i}}(_{i})= p_{Z_{i}}(_{i})-_{l=1}^{L}| }{d_{i}^{l-1}}|,\] (3)

Figure 3: The overview of our proposed SANFlow framework. Firstly, with synthetic augmentation process, make binary mask \(\) and anomaly image \(\) from normal image. Then, a pre-trained feature extractor \(f\) is applied to extract multi-scale feature maps (\(K\)=\(3\), number of scales in this work) for \(\). For each \(i\)-th position at feature map of \(k\)-th level, each feature vector \(_{i}^{k}\) is concatenated with a position embedding vector \(_{i}^{k}\) before being fed into NF, which is independently trained for each scale. To be specific, NF consists of 8 coupling blocks (i.e., \(L\)=\(8\)), which maps given feature distributions to base distributions. Each base distribution is different in terms of statistics (variance in this work) which are dependent on semantic features of the corresponding location in the given image. In particular, we condition our statistics prediction network \(h\) on the input image to generate parameters of inverse Gamma distribution such as \(\) and \(\), to estimate the variances of base distributions, which are assumed to follow inverse Gamma distributions.

where \(Z_{i}\) is a simple, yet locally varying base distribution corresponding to a feature vector \(_{i}\).

However, just making NF scale- and spatial-aware is insufficient to handle variations that exist within the same object category. This is because objects are not perfectly aligned across different images of the same category, resulting in images with different semantic information even at the identical spatial location and scale. Thus, in order to enhance the density estimation of our NF model, we propose to embed feature distributions into different base distributions, leveraging the semantic information encoded in the features. Further details on this approach are provided in the following section.

#### 3.4.2 Semantic-aware base distribution

In this study, we instantiate semantic-dependent base distributions as Gaussian distributions with statistics that are estimated based on semantic information of features. Inspired by an algorithm that estimates the statistics of non-i.i.d. noise , we employ a lightweight statistics prediction network \(h\) to estimate the statistics for the given feature \(_{i}\). We condition the network, parameterized by \(_{h}\), on the semantic features to be able to estimate appropriate statistics of corresponding base distributions. As we will discuss in our experiments, we empirically observe that estimating both mean (\(_{i}\)) and variance (\(_{i}^{2}\)) of the base distribution is difficult and that estimating variance (\(_{i}^{2}\)) alone is beneficial (Section 4.3). Therefore, we fix \(_{i}\) to be \(0\) for normal regions and \(1\) for abnormal regions. In doing so, we map abnormal features to a base distribution with minimal overlap with that of normal features during training, thereby aiding NF in mapping normal and abnormal regions into distinct distributions. When samples are non-i.i.d., the distribution of sample variance can be assumed to follow the inverse Gamma distribution . Since image pixels and semantic features are non-i.i.d., we formulate the variances of the corresponding base distributions to follow the inverse Gamma distribution as IG(\(|,\)) with parameters \(\) and \(\). We set \(\) to be \((}{2}-1)\) and \(\) to be \(}{2}\), where \(p^{2}\) denotes the area of the corresponding receptive field of size \(p p\) and \(\) is the mode of the inverse Gamma distribution . In this work, \(\) is a hyperparameter, the value of which is empirically found to be \(0.1\). Upon the assumption, we train the network \(h\) to estimate \(_{i}\) and \(_{i}\) for each \(_{i}\), resulting in the following regularization loss as:

\[_{KL} =D_{KL}(IG(_{i},_{i})\,|\,IG(,))\] \[=_{i=1}^{H_{k} W_{k}}\{(_{i}-)(_ {i})+(()-(_{i}))+(_{i}- )+_{i}(}-1)\},\] (4)

where \(D_{KL}(IG(_{i},_{i})\,|\,IG(,))\) computes a KL divergence between the estimated distribution by the network \(h\) and the distribution of variance \(_{i}^{2}\), which we assume to follow \(IG(,)\) as mentioned above; \(\) is a digamma function and \(\) is a gamma function. Notably, \(_{KL}\) with \(\) and \(\) acts as regularization to guide the statistics prediction network for stable training. The derivation details and detailed explanations can be found in the supplementary material.

### Loss function

In this section, we delineate the overall objective function which our model is trained to minimize. First, we need to modify the log-likelihood of \(_{i}\) corresponding to feature \(_{i}\) in Equation 3 to account for handling both normal and abnormal features differently:

\[ p_{Z_{i}}(_{i})=m_{i} p_{Z_{i}^{n}}(_{i})+(1-m_{i})  p_{Z_{i}^{n}}(_{i}),\] (5)

where the binary indicator \(m_{i}\) is set to be \(1\) when the corresponding location in the binary mask \(\) is \(1\), and \(0\), otherwise. Note that, \(Z_{i}^{n}\) represents the corresponding base distribution when \(_{i}\) is a normal feature, while \(Z_{i}^{a}\) represents the base distribution when \(_{i}\) is an abnormal feature. In practice, to compute the likelihood at the \(k\)-th scale, the binary mask \(\) is resized to match the size of the feature map \(^{k}\), which has height \(H_{k}\) and width \(W_{k}\). The resizing is done using nearest-neighbor interpolation to preserve the binary nature of the mask. In turn, each log-likelihood term can be formulated as follows (detailed derivations can be found in the supplementary material):\[ p_{Z_{i}^{n}}(_{i})=- 2-(_{i}- (_{i}))-}{2_{i}}||_{i}||_{2}^{2},\] (6)

and,

\[ p_{Z_{i}^{o}}(_{i})=- 2-(_{i}- (_{i}))-}{2_{i}}||_{i}-1||_{2}^{2}.\] (7)

Moreover, to further aid NF in distinguishing normal and abnormal features, we can also train NF to perform binary classification using the loss function as follows:

\[_{}=(s(_{i}),m_{i}),\] (8)

where \(\) denotes the conventional binary cross-entropy loss, while \(s(_{i})\) measures the probability of \(_{i}\) being classified as normal (i.e., \(s(_{i})=^{n}}(_{i})}{p_{Z_{i}^{n}}(_{i})+p_{Z_ {i}^{n}}(_{i})}\)). Note that the log-likelihood of \(_{i}\) being abnormal (Equation 7) can be used only during training such that our framework learns to map abnormal features to base distribution \(Z^{a}\) that has small overlap with the base distributions of normal features \(Z^{n}\). This allows NF to transform normal and abnormal features to two distinct base distributions, thereby enhancing the discriminability.

Together with \(_{}\) from Equation 4, the overall objective function \(_{}\) to minimize is given by,

\[_{}=_{}+_{1} _{}+_{2}_{},\] (9)

where \(_{1}\) and \(_{2}\) are hyperparameters to adjust the associated weights of \(_{}\) and \(_{}\).

### Anomaly score functions

During inference, given a test image \(^{H W 3}\), we first compute the log-likelihood of each feature \(_{i}\) at each \(k\)-th scale via Equation 3, where the log-likelihood of \(_{i}\) is computed with Equation 6. Then, the likelihood map \(^{k}^{H_{k} W_{k}}\) for the \(k\)-th scale is obtained by taking exponential of the log-likelihood 6. We obtain the final likelihood map \(^{H W}\) by summing the likelihood map \(^{k}\) from all scales. These likelihood maps are first upsampled to the image resolution \(^{H W}\) using bilinear interpolation, following the approach described in . Finally, the anomaly score map is computed as the negative of \(\), resulting in \(-\).

## 4 Experiments

In this section, we evaluate our framework to validate its capability in both pixel-level anomaly localization and image-level anomaly detection. We will release our code and data upon acceptance, and more details and results can be found in the supplementary material.

### Experimental settings

**Dataset.** The experiments are conducted on two commonly used datasets for unsupervised anomaly detection: STC (ShanghaiTech Campus) dataset  and MVTec dataset . STC is a video surveillance dataset, which provides static videos of 13 different scenes of \(856 480\) resolution. It contains \(274,515\) frames for training and \(42,883\) frames for evaluation. The training set consists of only normal sequences, while the evaluation set consists of \(300,308\) regular frames and \(42,883\) irregular frames. MVTec is a dataset that consists of images of industrial products categorized into 5 texture categories and 10 object categories. To evaluate the unsupervised anomaly detection performance, the training set includes only defect-free (e.g., normal) images: \(3,629\) normal images are available for training while \(1,725\) normal and abnormal images are used as test set. Among the test images, \(1,258\) images contain defects (e.g., abnormal images). For data augmentation, abnormal patches undergo random vertical, horizontal flip and rotation during training, as described in 3.2. We evaluate and compare algorithms in terms of area under the receiver operating characteristic curve (AUROC) and area under the per-region-overlap curve (AUPRO), as used in [4; 20]. AUPRO scores can be found in the supplementary.

**Implementation details.** Following other works [4; 20; 36], our models are trained and evaluated separately for each category. Similar to CFLOW-AD , we use \(K\)=3 scales for feature pyramid; normalizing flow consists of \(L\)=8 transformation blocks; the dimension of position embedding vector \(D\) is \(512\); Adam optimizer with a learning rate of 5e-4 and 80 train epochs for training. The loss weight hyperparameters \(_{1}\) and \(_{2}\) are set to be \(1.0\) and \(0.2\), respectively. The statistics prediction network \(h\) consists of five convolutional layers with a skip connection as illustrated in Figure 3. At last, we use NVIDIA RTX8000 for training and the model shows near real-time performance by running at 13fps.

### Experimental results

We provide the anomaly localization and detection performance on the STC dataset in Table 1 while the results on MVTec dataset are reported in Table 2 and Table 3. All results presented in this study are obtained using the same backbone architecture, WRN-50 , while additional results on MVTec are reported for PatchCore  and our method with a larger backbone, WRN-101 . The experimental results demonstrate that the performance of our proposed method is comparable with other state-of-the-art methods. In Table 1, compare to other anomaly detection models that are not video targeted, our method shows outstanding performances in both detection and localization.

    & **FramePred** & **MemAE** & **SPADE** & **PaDiM** & **CFLOW-AD** & **PatchCore-10** & **SANFlow (Ours)** \\  Image-wise & 72.8 & 71.2 & 71.9 & - & 72.63 & - & **76.1** \\ Pixel-wise & - & - & 89.9 & 91.2 & 94.48 & 91.8 & **94.8** \\   

Table 1: Anomaly detection and localization results w.r.t. AUROC metric by various methods with WRN-50 backbone for feature extraction on STC .

    & \(}{}\) & \(\)-VAE+grad & PatchSVDD & PathSM & CutBase & CFLOW-AD & PatchCore-10 & PatchCore-10 & **SANFlow** & **SANFlow** \\  & WRN-50 & WRN-50 & WRN-50 & WRN-50 & WRN-50 & WRN-50 & WRN-50 & WRN-50 & WRN-101 & WRN-50 & WRN-101 \\  bottle & 89.0 & 93.1 & 98.1 & 98.3 & 97.6 & 98.6 & 98.6 & 98.6 & 98.6 & 98.6 & **99.1** \\ cable & 82.0 & 88.0 & 96.8 & 96.7 & 90.0 & 97.6 & 98.5 & 98.4 & 98.5 & **98.8** \\ capsule & 94.0 & 91.7 & 95.8 & 98.5 & 97.4 & 98.8 & 98.9 & **99.1** & **99.1** & 98.9 \\ carpet & 87.0 & 72.7 & 92.6 & 99.1 & 98.3 & 99.2 & 99.1 & 98.7 & 99.3 & **99.4** \\ grid & 94.0 & 97.9 & 96.2 & 97.3 & 97.5 & 96.8 & 98.7 & 98.7 & 98.5 & **99.3** \\ hazelnut & 97.0 & 98.8 & 97.5 & 98.2 & 97.3 & 98.2 & 98.7 & 98.8 & **99.2** & 99.0 \\ leather & 78.0 & 89.7 & 97.4 & 99.2 & 99.5 & 99.6 & 99.3 & 99.3 & 99.6 & **99.8** \\ metal nut & 89.0 & 91.4 & 98.0 & 97.2 & 93.1 & 98.56 & 98.4 & **98.8** & 98.5 & 98.7 \\ pill & 91.0 & 93.5 & 95.1 & 95.7 & 95.7 & 98.9 & 97.6 & 97.8 & **99.2** & 99.1 \\ screw & 96.0 & 97.2 & 95.7 & 98.5 & 96.7 & 98.0 & **99.4** & 99.3 & 99.0 & 99.2 \\ tile & 59.0 & 58.1 & 91.4 & 94.1 & 90.5 & 97.1 & 95.9 & 96.1 & 98.9 & **99.1** \\ toothnush & 92.0 & 98.3 & 98.1 & 98.8 & 98.1 & 98.5 & 98.7 & 98.8 & 98.9 & **99.2** \\ transistor & 90.0 & 93.1 & 97.0 & **97.5** & 93.0 & 93.2 & 96.4 & 96.4 & 94.4 & 95.1 \\ wood & 73.0 & 80.9 & 90.8 & **94.9** & 95.5 & 94.49 & 95.1 & 95.1 & 96.4 & **97.9** \\ zipper & 88.0 & 87.1 & 95.1 & 98.5 & 99.3 & 98.41 & 98.9 & 98.9 & 98.9 & **99.6** \\  average & 87.0 & 88.8 & 95.7 & 97.5 & 96.0 & 97.9 & 98.1 & 98.2 & 98.5 & **98.8** \\   

Table 2: Quantitative comparisons on anomaly localization performance in MVTec dataset  with respect to AUROC metric. For each category, the best and second best performance is **bolded** and underlined, respectively.

    & GANonly & OCSVM & PatchSVDD & DiffeNet & PalDM & CFLOW-AD & CutBase & PatchCore-10 & PatchCore-10 & **SANFlow** & **SANFlow** \\  & WRN-50 & WRN-50 & WRN-50 & WRN-50 & WRN-50 & WRN-50 & WRN-50 & WRN-50 & WRN-101 & WRN-50 & WRN-101 \\  bottle & 89.2 & 99 & 98.6 & 99.0 & - & **100** & 98.2 & **100** & **100** & **100** & **100** \\ cable & 75.7 & 80.3 & 90.3 & 95.9 & - & 97.59 & 81.2 & 94.4 & 99.6 & 99.4 & **99.7** \\ capsule & 73.2 & 54.4 & 76.7 & 86.9 & - & 97.68 & 95.2 & 97.8 & 95.2 & 97.7 & **95.9** \\ carpet & 69.9 & 62.7 & 92.9 & 92.9 & - & 98.73 & 93.9 & 98.7 & 98.4 & 98.8 &Plus, Table 2 and Table 3 reports that our proposed method performs better than existing NF-based anomaly detection algorithms (CFLOW-AD  and DifferNet ) and other anomaly detection algorithms such as \(AE_{SSIM}\), \(\)-VAE+grad , PatchSVDD , PaDiM  and PatchCore-1, 10 . In particular, the results demonstrate the effectiveness of our proposed semantic-aware base distribution mapping approach in enhancing the density estimation performance of the proposed NF model.

### Ablation study

We conduct ablation experiments to evaluate the efficacy of each component of our framework. Specifically, we perform ablation studies on the loss function (Table 4), synthetic anomaly generation (Table 5), and statistics estimated by our statistics prediction network \(h\) (Table 6). All ablation experiments are reported with respect to AUROC, using the WRN-50 backbone, and all average AUROC values over all categories are reported for image-wise anomaly detection.

In Table 4, we analyze the efficacy of our proposed loss functions. To do so, we first compare against a baseline NF model trained using the standard negative log-likelihood loss of NF in Equation 2 (Model **(1a)**). Compared to Model **(1a)**, SANFlow brings substantial improvement, thereby validating the effectiveness of all three proposed loss functions (\(_{}\) in Equation 3 using Equation 5, \(_{}\), and \(_{}\)). We validate the efficacy of each loss function separately by applying only \(_{}\) using Equation 3 and 5 (Model **(1b)**) or disabling either \(_{}\) (Model **(1c)**) or \(_{}\) (Model **(1d)**). While each variant leads to performance improvement, using all three proposed loss functions leads to the largest improvement, suggesting that all three proposed loss functions coordinate and play critical roles in our overall framework.

In Table 5, we evaluate the impact of synthetic anomaly augmentation (generation) on the final performance of our framework. Training without anomaly augmentation and thus without any anomaly data results in performance degradation (Model **(2a)**). Also, Model **(1d)** (Table 4) which has only difference at doing anomaly augmentation in training, performs better than Model **(2a)**. Thus, the ablation result suggests that not only embedding semantic features to different base distributions but training NF with synthetic anomalies is also an important process. For better understanding, Figure 4 visualizes anomaly score maps based on estimated distribution \(Z^{a}\) **(b)** for test images. As described in Section 3.6, Figure 4 **(a)** can be found with Equation 3 and 6 while Figure 4 **(b)** can be found with Equation 3 and 7. We note that both maps agree on the abnormal region (highlighted with bright colors), validating that training NF to map normal and abnormal features to distant base distributions helps the model discriminate abnormal regions from normal regions.

   & \(_{i}\) & \(_{i}^{2}\) & STC & MVTec \\  Model **(3a)** & fixed & fixed & 75.9 & 97.6 \\ Model **(3b)** & estimated & fixed & 75.1 & 97.0 \\ Model **(3c)** & estimated & estimated & 74.5 & 98.1 \\
**SAFNlow (Ours)** & fixed & estimated & **76.1** & **98.7** \\  

Table 6: Ablation study on statistics estimation in MVTec and STC datasets.

Figure 4: Visualization of anomaly score map based on **(a)**\(Z^{n}\) and **(b)**\(Z^{a}\) on MVTec dataset.

   & \(C_{}\) & \(_{}\) & \(_{}\) & STC & MVTec \\  Model **(1d)** & \(_{}\)(2) & ✗ & ✗ & ✗ & 72.6 & 98.3 \\ Model **(1e)** & Eq. **(3)**(3)**(5) & ✗ & ✗ & 73.5 & 98.2 \\ Model **(1e)** & Eq. **(3)**(3)**(5) & ✗ & ✗ & 73.1 & 98.3 \\ Model **(1e)** & Eq. **(3)**(3)**(5) & ✗ & ✓ & 74.2 & 98.5 \\
**SAFNlow** & ✗ & ✓ & ✓ & **76.1** & **98.7** \\  

Table 4: Ablation study on loss functions in MVTec and STC datasets.

Table 6 reports ablation results after fixing either mean (\(_{i}\)), variance (\(_{i}^{2}\)), or both statistics of base distributions. If \(_{i}\) is fixed, it is set to be \(0\) for normal features and \(1\) for abnormal features. If \(_{i}^{2}\) is fixed, it is empirically set to be \(0.1\) for both normal and abnormal features. The performance is observed to decrease noticeably when \(_{i}\) is estimated alone (Model **(3b)**) or together with \(_{i}^{2}\) (Model **(3c)**). On the other hand, estimating only \(_{i}^{2}\) while fixing \(_{i}\) brings significant performance improvement. The performance degradation from estimating \(_{i}\) may be due to learning complexity. Furthermore, to enhance the persuasiveness of the mean margin, we conducted additional experiments on the MVTec dataset, comparing performance at margin values of 0.5, 1.0, and 1.5 in WRN-50 backbone. For margin values 0.5, 1.0, and 1.5, the proposing method shows 98.6, 98.7, and 98.1 AUROC for image-wise and 98.4, 98.5, and 98.3 AUROC for pixel-wise. As the margin of 1.0 yields the highest results in both image- and pixel- wise, we could confirm that the mean margin of 1.0 is a persuasive choice.

### Experiments on other datasets

Although the MVTec dataset is a benchmark that is widely used for anomaly detection and localization, there is difficulty in identifying the advantage of the proposing method with it due to performance saturation. Therefore, we provide a performance comparison in the VisA dataset . Our proposed method with a WRN-50 backbone achieves an image-wise AUROC of 93.4, 98.6, and a PRO score of 89.4, while CFLOW-AD  and PaDiM  achieve 91.5, 59.8, and 89.1, 85.9 of an image-wise AUROC and a PRO score, respectively. The results show that our approach demonstrates competitive performance against other methods, including other flow-based approaches, demonstrating the effectiveness of our framework in greatly improving the density estimation capability of normalizing flow for anomaly detection and localization. Plus, while our approach excels at detecting anomalies within images, its effectiveness in tasks involving image-wise semantic outlier detection, like CutPaste  and CFLOW-AD, is limited. Nonetheless, we conducted an additional experiment with CIFAR-10 , where our proposed method achieved an AUROC of 80.8, outperforming performances of CutPaste (69.4) and CFLOW-AD (79.32).

## 5 Conclusion

In this work, we propose to improve the density estimation of normalizing flow (NF) for anomaly detection, by training NF to dynamically embed given feature distribution to different base distributions. In particular, base distributions are Gaussian distribution with statistics estimated by our statistics prediction network. As a result, our NF framework learns to map not only diverse normal features but also abnormal features to corresponding yet different base distributions, enhancing the density estimation capability.

**Limitation** While SANFlow has the capability of mapping different semantic features to different base distributions, it requires anomaly augmentation to map anomaly features to a base distribution distant from base distributions of normal features for better anomaly detection. However, augmentations require domain knowledge and do not cover new anomalies that may occur. But, we would like to note that our ablation study reveals that our framework brings improvements even without anomaly augmentation. Nonetheless, reducing the dependence on anomaly augmentations in the proposed framework is an interesting future research direction.

**Broader Impacts** Finding anomalies in product during fabrication is important to ensure the quality of products. Anomaly detection algorithms can automate the process, which can offload the burden from human workers. This will allow human workers to focus on other important aspects of fabrication, improving the overall process of fabrication and thus the quality of products. Furthermore, anomaly detection can be used to frequently check anomalies of objects that may be difficult for humans but crucial to human safety (e.g., tall buildings, bridges, nuclear power plants, etc.).