# Matrix Denoising with Doubly Heteroscedastic Noise: Fundamental Limits and Optimal Spectral Methods

Yihan Zhang

Institute of Science and Technology Austria

zephyr.z798@gmail.com &Marco Mondelli

Institute of Science and Technology Austria

marco.mondelli@ist.ac.at

###### Abstract

We study the matrix denoising problem of estimating the singular vectors of a rank-\(1\) signal corrupted by noise with both column and row correlations. Existing works are either unable to pinpoint the exact asymptotic estimation error or, when they do so, the resulting approaches (e.g., based on whitening or singular value shrinkage) remain vastly suboptimal. On top of this, most of the literature has focused on the special case of estimating the left singular vector of the signal when the noise only possesses row correlation (one-sided heteroscedasticity). In contrast, our work establishes the information-theoretic and algorithmic limits of matrix denoising with doubly heteroscedastic noise. We characterize the exact asymptotic minimum mean square error, and design a novel spectral estimator with rigorous optimality guarantees: under a technical condition, it attains positive correlation with the signals whenever information-theoretically possible and, for one-sided heteroscedasticity, it also achieves the Bayes-optimal error. Numerical experiments demonstrate the significant advantage of our theoretically principled method with the state of the art. The proofs draw connections with statistical physics and approximate message passing, departing drastically from standard random matrix theory techniques.

## 1 Introduction

Matrix denoising is a central primitive in statistics and machine learning, and the problem is to recover a signal \(X^{n d}\) from an observation \(A=X+W\) corrupted by additive noise \(W\). This finds applications across multiple domains of sciences, e.g., imaging , biology  and astronomy . When \(X\) has low rank and \(W\) i.i.d. entries, \(A\) is the standard model for principal component analysis, typically referred to as the Johnstone spiked covariance model . When \(n,d\) are both large and proportional, which corresponds to the most sample-efficient regime, its Bayes-optimal limits are well understood , and it has been established how to achieve them efficiently . Minimax/non-asymptotic guarantees are also available in special cases, such as sparse PCA , Gaussian mixtures  and certain joint scalings of \((n,d)\).

However, in most applications, noise is highly structured and correlated, thereby calling for more realistic assumptions on \(W\) than having i.i.d. entries. A recent line of work addresses this concern by studying matrix denoising with heteroscedastic noise , resting on two basic ideas: whitening and singular value shrinkage. Whitening refers to multiplying the data matrix by the square root of the inverse covariance, in order to reduce the model to one with i.i.d. noise; and singular value shrinkage retains the singular vectors of the data while deflating the singular values to correct for the noise. Though the exact asymptotic performance of these algorithms has been derived , their optimality is yet to be determined from a Bayesian standpoint. In fact, we will prove that whitening and shrinkage are _not_ the correct way to approach Bayes optimality.

Main contributions.We focus on the prototypical model \(A=X+W\), where \(X=}{v^{*}}^{}\) is a rank-\(1\) signal, \(\) is the signal-to-noise ratio (SNR), and \(W=^{1/2}^{1/2}\) is doubly heterogeneous noise. Here \(u^{*},v^{*}\) follow i.i.d. priors; \(\) contains i.i.d. Gaussian entries; the covariance matrices \(,\) capture column and row correlations; and we consider the typical high-dimensional regime in which \(n,d\) are both large and proportional. Our main results are summarized below.

1. We design an efficient spectral estimator to recover \(u^{*},v^{*}\), and we provide a precise asymptotic analysis of its performance, see Theorem 5.1. This estimator is given by the top singular vectors of a matrix obtained by carefully pre-processing \(A\), see (5.3).
2. When the priors of \(u^{*},v^{*}\) are standard Gaussian, we show in Corollary 5.2 that the spectral estimator above is optimal in the following sense: _(i)_ under a technical condition, it achieves the _optimal weak recovery threshold_, namely its mean square error is non-trivial as soon as this is information-theoretically possible; _(ii)_ it achieves the _Bayes-optimal error_ for \(u^{*}\) (resp. \(v^{*}\)) when \(\) (resp. \(\)) is the identity. These optimality guarantees follow from rigorously obtaining the asymptotic minimum mean square error (MMSE) for the estimation of the whitened signals \(^{-1/2}u^{*}\) and \(^{-1/2}v^{*}\), see Theorem 4.2.

Our spectral estimator only involves matrix multiplication and computing principal singular vectors. Practically, this can be efficiently done using standard SVD algorithms or power iteration . For both one-sided and double heteroscedasticity, numerical experiments in Figures 2 and 3 show significant advantage of our spectral estimator for moderate SNRs over HeteroPCA  and shrinkage-based methods, i.e., Whiten-Shrink-reColor [44; 45], OptShrink , and ScreeNOT .

Proof techniques.We take a completely different route from classical approaches in statistics and random matrix theory (e.g., whitening and shrinkage), and instead exploit tools from statistical physics and the theory of approximate message passing. In particular, the MMSE for the whitened signals \(^{-1/2}u^{*},^{-1/2}v^{*}\) is obtained via an interpolation argument [10; 52; 53]. This result allows us to derive the weak recovery threshold for estimating the true signals \(u^{*},v^{*}\). Moreover, for one-sided heteroscedasticity, this MMSE coincides with that for estimating the true signal on the homoscedastic side. Evaluating the Bayes-optimal estimators requires solving high-dimensional integrals that are computationally intractable. To circumvent this issue, we propose an efficient spectral method that still enjoys optimality guarantees. Its design and analysis draw connections with a family of iterative algorithms called approximate message passing (AMP) [11; 32]. All our results are mathematically rigorous, with the only technical condition being "(5.1) implies \(_{2}^{*}<1\)" in Theorem 5.1 that we only managed to verify numerically, but not analytically; see Remark 5.1.

## 2 Related work

Research on matrix denoising in the homoscedastic case (\(=I_{n},=I_{d}\)) has a rich history, and in random matrix theory properties of the spectrum and eigenspaces of \(A\) have been studied exhaustively. Most prominently, the BBP phase transition phenomenon  (and its finite-sample counterpart ) unveils a threshold of the SNR \(\) above which a pair of outlier singular value and singular vector emerge. Under i.i.d. priors, the asymptotic Bayes-optimal estimation error has been derived [52; 53], rigorously justifying predictions from statistical physics . The proof uses the interpolation method due to Guerra , originally developed in the context of mean-field spin glasses. Besides low-rank matrix estimation, this method (including its adaptive variant  and the Aizenman-Sims-Starr scheme ) has also been applied to a range of problems, including spiked tensor estimation , generalized linear models , stochastic block models  and group synchronization .

Moving to the heteroscedastic case, an active line of work concerns optimal singular value shrinkage methods [44; 33; 45; 69; 59; 26]. These methods can be regarded as a special family of rotationally invariant estimators, which apply a univariate function \(_{ 0}\) to each empirical singular value. An example widely employed by practitioners is the thresholding function \(_{}(y)=y\{y>\}\). In the presence of noise heteroscedasticity, most of these results are based on whitening . Another model of noise heterogeneity common in the literature takes \(W=^{0/2}\), where \(\) has i.i.d. Gaussian entries, \(\) is a deterministic block matrix with fixed (i.e., constant with respect to \(n,d\)) number of blocks, and \(\) denotes the element-wise product. This means that the entries of the noise are independent but non-identically distributed, and they follow the variance profile \(\). The corresponding low-rank perturbation \(A\), known as a spiked inhomogeneous matrix, has attracted attention from both the information-theoretic [12; 66; 36] and the algorithmic sides [38; 50; 61].

Spiked inhomogeneous matrices have some connections with the model considered in this paper: if \(\) has rank \(1\), such \(A\) can be realized by taking \(,\) to be diagonal with suitable block structures. Finally, non-asymptotic results for the heteroscedastic and the inhomogeneous models have been derived in varying generality in [76; 82; 23; 1; 17]. We highlight that our paper is the _first_ to establish information-theoretic and algorithmic limits for doubly heteroscedastic noise.

Our characterization of the spectral estimator relies on an AMP algorithm that converges to it by performing power iteration. AMP refers to a family of iterative procedures, whose performance in the high-dimensional limit is precisely characterized by a low-dimensional deterministic recursion called state evolution [11; 15]. Originally introduced for compressed sensing , AMP algorithms have been developed for various settings, including low-rank estimation [56; 31; 6] and inference in generalized linear models [64; 65; 71]. Beyond statistical estimation, AMP proves its versatility as both an efficient algorithm and a proof technique for studying e.g. posterior sampling , spectral universality , first order methods with random data , mismatched estimation , spectral estimators for generalized linear models [79; 80] and their combination with linear estimators .

## 3 Problem setup

Consider the following rank-\(1\) rectangular matrix estimation problem with doubly heteroscedastic noise where we observe

\[A=u^{*}{v^{*}}^{}+W^{n d},\] (3.1)

and aim to estimate \(u^{*},v^{*}\). The following assumptions are imposed throughout the paper. The dimensions \(n,d\) obey the proportional scaling \(n/d(0,)\), where \(\) is the aspect ratio. The SNR \([0,)\) is a known constant (relative to \(n,d\)). The signals \((u^{*},v^{*}) P^{ n} Q^{ d}\) have i.i.d. priors, where \(P,Q\) are distributions on \(\) with mean \(0\) and variance \(1\). The unknown noise matrix has the form \(W=^{1/2}^{1/2}^{n d}\), with \(_{i,j}}}{{}}(0,1/n)\) independent of \((u^{*},v^{*})\). The covariances \(^{n n},^{d d}\) are known, deterministic,1 strictly positive definite and satisfy

\[_{n}()=_{d} {d}()=1.\] (3.2)

Their empirical spectral distributions (ESD) converge (as \(n,d\) s.t. \(n/d\)) weakly to the laws of the random variables \(\) and \(\). Furthermore, \(\|\|_{2},\|\|_{2}\) are uniformly bounded over \(d\). The supports of \(,\) are compact subsets of \((0,)\). For all \(>0\), there exists \(d_{0}\) s.t. for all \(d d_{0}\),

\[(())( )+[-,],( ())()+[- ,].\] (3.3)

The trace assumption (3.2) on the covariances is for normalization purposes since the values of the traces, if not \(1\), can be absorbed into \(\). The support assumption (3.3) excludes outliers in the spectra of covariances which may contribute to undesirable spikes in \(A\).

## 4 Information-theoretic limits

For mathematical convenience, in this section, we switch to an equivalent rescaled model

\[YA=}u^{*}{v^{*}}^{}+^{1/2}Z ^{1/2}^{n d},\] (4.1)

where \(^{2}\) and \(Z=\) contains i.i.d. elements \(Z_{i,j}}}{{}}(0,1)\). Abusing terminology, we refer to \(\) as the SNR of \(Y\). Define also \( 1/(0,)\) so that \(d/n\). The scaling of the parameters in (4.1) turns out to be more convenient for presenting the results in this section. Results for \(Y\) can be easily translated to \(A\) by a change of variables.

Let \(^{*}^{-1/2}u^{*}\) and \(^{*}^{-1/2}v^{*}\) denote the whitened signals. The main result of this section is Theorem 4.2, which characterizes the performance of the matrix minimum mean square error (MMSE) associated to the estimation of \(^{*}(^{*})^{},^{*}(^{*})^ {}\) and \(^{*}(^{*})^{}\), via the corresponding Bayes-optimal estimators:

\[_{n}() ^{*}( ^{*})^{}-^{*}(^{* })^{}\,Y_{}^{2},\] (4.2) \[_{n}^{u}() }^{* }(^{*})^{}-(^{*})^ {}\,Y_{}^{2},\] (4.3) \[_{n}^{v}() }^{* }(^{*})^{}-^{*}(^ {*})^{}\,Y_{}^{2}.\] (4.4)

Our characterization involves a pair of parameters \((q_{u}^{*},q_{v}^{*})^{2}_{ 0}\) defined as the largest solution to

\[q_{u}=^{-2}}{1+  q_{v}^{-1}}, q_{v}= ^{-2}}{1+ q_{u}^ {-1}}.\] (4.5)

Here and throughout the paper, all expectations involving \(,\) are computed as integrals against the limiting spectral distributions of \(,\).

The proposition below, proved in Appendix A, justifies the existence of the solution to (4.5) and identifies when a non-trivial solution emerges.

**Proposition 4.1**.: _The fixed point equation (4.5) always has a trivial solution \((0,0)\). There exists a non-trivial solution \((q_{u}^{*},q_{v}^{*})^{2}_{>0}\) if and only if_

\[^{2}^{-2} ^{-2}>1,\] (4.6)

_in which case the non-trivial solution is unique._

We are now ready to state our main result on the MMSE.

**Theorem 4.2**.: _Assume \(P=Q=(0,1)\). For almost every \(>0\),_

\[_{n}_{n}()= ^{-1}^{-1}-q_{u }^{*}q_{v}^{*},\] (4.7) \[_{n}_{n}^{u}()= ^{-1}^{2}-q_{u}^{*\,2},_{n}_{n}^{v}()=^{-1}^{2}-q_{v}^{*\,2}.\] (4.8)

We note that

\[_{n}^{*}( ^{*})^{}_{}^{2}=_{n} ^{*}_{2}^{2} ^{*}_{2}^{2}= ^{-1}^{-1},\] (4.9)

where the last step follows from Proposition G.2. This quantity represents the trivial error in the estimation of \(^{*}(^{*})^{}\), which is achieved by the all-0 estimator. Analogous considerations hold for \(^{*}(^{*})^{}\) and \(^{*}(^{*})^{}\), for which the trivial estimation error is \(^{-1}^{2}\) and \(^{-1}^{2}\), respectively. Thus, Proposition 4.1 and Theorem 4.2 identify (4.6) as the condition for non-trivial estimation, and the smallest \(\) that satisfies (4.6) gives the _weak recovery threshold_.

We show below that the weak recovery threshold is the same for the estimation of the true signals \(u^{*}{v^{*}}^{},u^{*}{u^{*}}^{}\) and \(v^{*}{v^{*}}^{}\). In this case, since the signal priors are Gaussian, using the same passages as in (4.9) one has that the trivial estimation error for \(u^{*}{v^{*}}^{},u^{*}{u^{*}}^{}\) and \(v^{*}{v^{*}}^{}\) is always equal to \(1\).

**Corollary 4.3**.: _Assume \(P=Q=(0,1)\). The MMSE associated to the estimation of \(u^{*}{v^{*}}^{}\) is non-trivial, i.e,_

\[_{n}}^{}}- }^{}}\,Y_{}^ {2}<1\] (4.10)

_if and only if (4.6) holds. The same result holds for the MMSE of \({u^{*}{u^{*}}^{}}\) and \({v^{*}{v^{*}}^{}}\)._

Proof strategy.To derive the characterizations in Theorem 4.2, we write the posterior distribution of \(u^{*},v^{*}\) given \(Y\) in a Gibbs form, i.e., its density is the exponential of a Hamiltonian normalized by a partition function. The interpolation argument relates the log-partition function (also referred to as the 'free energy') of the posterior to that of the posteriors of two Gaussian location models. Since i.i.d. Gaussianity is key to this approach, the challenge is to handle noise covariances. Our idea is 

[MISSING_PAGE_FAIL:5]

_Remark 4.2_ (Gaussian priors).: Theorem 4.4 crucially relies on having Gaussian priors \(P,Q\). This assumption is mainly used to derive single-letter (i.e., dimension-free) expressions of the free energy of the vector models in (4.17) which, under Gaussian priors, are nothing but Gaussian integrals. The free energy, and hence the MMSE, are expected to be sensitive to the priors. Indeed, this is already the case in the homoscedastic setting \(=I_{n},=I_{d}\). An extension towards general i.i.d. priors is a challenging open problem and, in fact, without posing additional assumptions on \(,\), it is unclear whether a single-letter expression for free energy and MMSE is possible.

At this point, the MMSE can be derived from the above characterization of free energy. Indeed, let

\[()\{>0:(q_{u}^{*},q_{v}^{*})(,)\}.\]

The envelope theorem [51, Corollary 4] ensures that \(()\) is equal to \(_{>0}\) up to a countable set. Using algebraic relations between free energy and MMSE, we prove (4.7) and (4.8) for all \(()\) (and, thus, for almost every \(>0\)). Then, using the Nishimori identity (Proposition G.4) and the fact that the ESDs of \(,\) are upper and lower bounded by constants independent of \(n\) and \(d\), Corollary 4.3 also follows. The formal arguments are contained in Appendix D.

## 5 Spectral estimator

This section introduces a spectral estimator that meets the weak recovery threshold and, for one-sided heteroscedasticity, attains the Bayes-optimal error. Suppose that the following condition holds

\[}{}^{-2} ^{-2}>1,\] (5.1)

which is equivalent to (4.6). Under this condition, the fixed point equations (4.5) have a unique pair of positive solutions \((q_{u}^{*},q_{v}^{*})\). For convenience, we also define the rescalings \(^{*} q_{v}^{*}/,^{*} q_{u}^{*}\), and the auxiliary quantities

\[b^{*}+}, c^{*}+}.\] (5.2)

Now, we pre-process the data matrix \(A\) as

\[A^{*}((^{*}+b^{*})I_{n}+)^{-1/2}^{-1/2}A ^{-1/2}((^{*}+c^{*})I_{d}+)^{-1/2},\] (5.3)

from which we obtain the spectral estimators

\[ _{u}((^{*}+b^{*})I _{n}+)^{-1/2}(^{*}I_{n}+)u_{1}(A^{*})}{^{1/2}( (^{*}+b^{*})I_{n}+)^{-1/2}(^{*}I_{n}+)u_{1}(A^{*}) _{2}},\] (5.4a) \[ _{v}((^{*}+c^{*} )I_{d}+)^{-1/2}(^{*}I_{d}+)v_{1}(A^{*})}{^{ 1/2}((^{*}+c^{*})I_{d}+)^{-1/2}(^{*}I_{d}+)v_{ 1}(A^{*})_{2}},\] (5.4b)

where \(u_{1}()/v_{1}()\) denote the top left/right singular vectors and

\[_{u}}{^{*}+1}},_{v }}{^{*}+1}}.\] (5.5)

Note that \(_{u},_{v}>0\), provided that (5.1) holds. The pre-processing of \(A\) in (5.3) and the form of the spectral estimators in (5.4) come from the derivation of a suitable AMP algorithm, and they are discussed at the end of the section. We finally defer to Appendix E.3 the definition of the scalar quantity \(_{2}^{*}\) obtained via a fixed point equation depending only on \(,,,\), see (E.26) for details.

Our main result, Theorem 5.1, shows that, under the criticality condition (5.1), the matrix \(A^{*}\) exhibits a spectral gap between the top two singular values, and it characterizes the performance of the spectral estimators in (5.4), proving that they achieve weak recovery of \(u^{*}\) and \(v^{*}\), respectively.

**Theorem 5.1**.: _Suppose that (5.1) holds and that, for any \(c>0\),_

\[_{ s}^{*}}{-c ^{*}}=_{ s}[( ^{*}}{-c^{*}})^{2} ]=,_{(^{*})}[^{*}}{-^{*}} ]=,\] (5.6)_where \(^{*}+b^{*})+}\). \(^{*}+c^{*})+}\) and \(s c(^{*})\). Let \(A^{*},,,_{2}^{*}\) be defined in (5.3), (5.4) and (E.26), and \(_{i}(A^{*})\) denote the \(i\)-th largest singular value of \(A^{*}\). Then, if \(_{2}^{*}<1\), the following limits hold in probability:_

\[_{n}_{1}(A^{*})=1>_{2}^{*}=_{n }_{2}(A^{*}),\] (5.7) \[_{n},u^{*}|}{\| \|_{2}\|u^{*}\|_{2}}=_{u},_{d},v^{*}|}{\|\|_{2}\|v^{*}\|_{2}}=_{v}\] (5.8) \[_{n}}u^{*}{u^{*}}^{}- ^{}_{}^{2}=1-_{u}^{4}, _{d}}}^{}}- ^{}_{}^{2}=1-_{v}^{4},\] (5.9) \[_{n}}^{}}- ^{}_{}^{2}=1-_{u}^{2}_{v} ^{2}.\] (5.10)

_Remark 5.1_ (Assumptions).: To guarantee a spectral gap for \(A^{*}\) and the weak recoverability of \(u^{*},v^{*}\) via the proposed spectral method, we also require the algebraic condition \(_{2}^{*}<1\). We conjecture that this condition is implied by (5.1), and we have verified that this is the case in all our numerical experiments (see Figure 1 for two concrete examples). The additional assumption (5.6) is a mild regularity condition on the covariances. It ensures that the densities of \(^{*},^{*}\) decay sufficiently slowly at the edges of the support, so that \(_{2}^{*}\) is well-posed .

_Remark 5.2_ (Signal priors).: Theorem 5.1 does not require the prior distributions \(P,Q\) to be Gaussian, and it is valid for any i.i.d. prior with mean \(0\) and variance \(1\).

On the one hand, Corollary 4.3 shows that, if (5.1) is violated, the problem is information-theoretically impossible, i.e., no estimator achieves non-trivial error. On the other hand, Theorem 5.1 exhibits a pair of estimators that achieves non-trivial error as soon as (5.1) holds - under the additional assumption \(_{2}^{*}<1\) which we conjecture to be implied by (5.1). Thus, the spectral method in (5.4) is optimal in terms of weak recovery threshold. Though such estimators do not attain the optimal error, when both priors are Gaussian and \(=I_{n}\), \(^{}\) is the Bayes-optimal estimate for \(u^{*}{u^{*}}^{}\).

**Corollary 5.2**.: _Assume \(P=Q=(0,1)\), and consider the setting of Theorem 5.1 with the additional assumption \(=I_{n}\). Then, \(_{u}=}^{*}}\), i.e., \(^{}\) achieves the MMSE for \(u^{*}{u^{*}}^{}\)._

The claim readily follows by noting that, when \(=I_{n}\), the first equation in (4.5) becomes

\[q_{u}^{*}=^{*}}{1+ q_{v}^{*}}=/)(}/)}{1+(^{2}/)(}/)}=}}{1+}}=_{u}^{2},\]

where the last equality is by the definition (5.5) of \(_{u}\). Let us highlight that, even if \(=I_{n}\), \(\) still makes non-trivial use of the other covariance \(^{1/2}\). At the information-theoretic level, this is reflected by the fact that \(^{1/2}\) enters \(q_{u}^{*}\) through the fixed point equations (4.5). Therefore, even though the matrix model in (4.1) is equivalent to a pair of uncorrelated vector models in (4.17) in the sense of the free energy, the tasks of estimating \(u^{*}\) and \(v^{*}\) cannot be decoupled.

Figure 1: Top two singular values of \(A^{*}\) in (5.3), where \(d=4000,=4\) and each simulation is averaged over \(10\) i.i.d. trials. The singular values computed experimentally (‘sim’ in the legends and \(\) in the plots) closely match our theoretical prediction in (5.7) (‘thy’ in the legends and solid curves with the same color in the plots). The threshold \(^{*}\) is such that equality holds in (5.1). We note that the green curve corresponding to \(_{2}^{*}\) is smaller than \(1\) for \(>^{*}\), i.e., when (5.1) holds.

[MISSING_PAGE_FAIL:8]

\[v^{t+1}=^{-1}A^{}^{-1}^{t}-c_{t}^{-1}^{t},^{t+1}=f^{*}_{t+1}(v^{t+1}), b_{t+1}= (( f^{*}_{t+1}(v^{t+1}))^{-1}),\]

where \(\) denotes the Jacobian matrix, and the functions \(g^{*}_{t},f^{*}_{t+1}\) are specified below in (5.12). As common in AMP algorithms, the iterates (5.11) are accompanied with a state evolution which accurately tracks their behavior via a simple deterministic recursion: the joint empirical distribution of \((u^{*},v^{*},u^{t},v^{t+1})\) converges to the random variables \((U^{*},V^{*},U_{t},V_{t+1})\), see Proposition E.1 for a formal statement and the recursive description of the laws of such random variables. Then, the name 'Bayes-AMP' is motivated by the fact that \(g^{*}_{t},f^{*}_{t+1}\) are the posterior-mean denoisers given by

\[g^{*}_{t}(u)[U^{*}\,|\,U_{t}=u], f^{*}_{t+1}(v) [V^{*}\,|\,V_{t+1}=v].\] (5.12)

Remarkably, Bayes-AMP operates on \(^{-1}A^{-1}\), as opposed to the widely adopted ansatz of considering the whitened matrix \(^{-1/2}A^{-1/2}\). The advantage of operating on \(^{-1}A^{-1}\) is that the fixed point of the corresponding state evolution matches the extremizers of the free energy in (4.5). This would _not_ be the case if Bayes-AMP used the whitening \(^{-1/2}A^{-1/2}\). Indeed, one can repeat the analysis of an AMP that operates on \(^{-1/2}A^{-1/2}\). The fixed point equations of the resulting state evolution do not match the information-theoretically optimal one in (4.5). In particular, the weak recovery threshold coming out of this approach is strictly larger than the optimal one in (4.6), as long as at least one of \(,\) is not a multiple of the identity. Since these derivations led to suboptimal results, the details were left out from the paper.

The design of Bayes-AMP and the proof of its state evolution follow a two-step reduction detailed in Appendix F. Using a change of variables, we show in Appendix F.2 that Bayes-AMP can be realized by an auxiliary AMP with non-separable denoising functions (meaning that \(g_{t},f_{t+1}\) cannot be written as univariate functions applied component-wise) operating on \(^{-1/2}A^{-1/2}=^{*}(^{* })^{}+\). Then, in Appendix F.1 we simulate the auxiliary AMP using a standard AMP operating on the i.i.d. Gaussian matrix \(\), whose state evolution has been established in .

However, Bayes-AMP by itself is not a practical algorithm since it needs a warm start, i.e., an initialization that achieves non-trivial error. Thus, the _second step_ is to design a spectral estimator that solves the fixed point equation of Bayes-AMP, which turns out to be an eigen-equation for \(A^{*}\).

To offer the readers an intuition on how the spectral estimators arise from Bayes-AMP, we now heuristically derive the form (5.3) of \(A^{*}\) and the expression (5.4) of the spectral estimator. To do so, we note that the large-\(n\) limits of \(c_{t},b_{t+1}\) coincide with the auxiliary quantities \(c^{*},b^{*}\) defined in (5.2).

Figure 4: Spectra of \(A\) and \(A^{*}\) averaged over \(10\) i.i.d. trials, where \(d=4000,=4\). An outlier singular value emerges in the spectrum of \(A^{*}\) due to the pre-processing on \(A\).

Furthermore, when the priors of \(u^{*},v^{*}\) are Gaussian, (5.12) reduces to

\[g^{*}_{t}(u)=(^{*}^{-1}+I_{n})^{-1}u, f^{*}_{t+1}(v)= (^{*}^{-1}+I_{d})^{-1}v,\]

where we recall that \(^{*}= q^{*}_{v}/\) and \(^{*}= q^{*}_{u}\) are rescalings of the non-trivial solution \((q^{*}_{u},q^{*}_{v})\) of (4.5). Denoting by \(u,v\) the fixed points of the iteration (5.11), after some manipulations we have

\[()u=A^{*}()v,()v=A^{* ^{*}}\,()u,\]

where \(A^{*}\) is given in (5.3) and

\[() ((^{*}+b^{*})I_{n}+)^{1/2}( ^{*}I_{n}+)^{-1}^{1/2},\] \[() ((^{*}+c^{*})I_{d}+)^{1/ 2}(^{*}I_{d}+)^{-1}^{1/2}.\]

This suggests that \(A^{*}\) has top singular value equal to \(1\) and \((()u,()v)\) are aligned with the corresponding singular vectors \((u_{1}(A^{*}),v_{1}(A^{*}))\). Moreover, state evolution implies that the distribution of the fixed point \((u,v)\) is close to that of

\[(^{*}^{-1}u^{*}+/}w_{u},^{*}^{-1}v^{*}+ /}w_{v}),\]

with \((w_{u},w_{v})(0_{n},^{-1})(0_{d},^{ -1})\) independent of \(u^{*},v^{*}\). Thus, to obtain estimates of \((u^{*},v^{*})\), we take \((()^{-1}u_{1}(A^{*}),()^{-1}v_{1}( A^{*}))\) and suitably rescale their norm, which leads to the expressions in (5.4). More details on the above heuristics are discussed in Appendix E.2.

The most outstanding step remains to make the heuristics rigorous. This involves proving that \( u^{t}, v^{t+1}\) are aligned with the proposed spectral estimator, which allows for a performance characterization via state evolution. The formal argument is carried out in Appendix E.4.

## 6 Concluding remarks

In this work, we establish information-theoretic limits and propose an efficient spectral method with optimality guarantees, for matrix estimation with doubly heteroscedastic noise. On the one hand, under Gaussian priors, we give a rigorous characterization of the MMSE; on the other hand, we present a spectral estimator that _(i)_ achieves the information-theoretic weak recovery threshold, and _(ii)_ is Bayes-optimal for the estimation of one of the signals, when the noise is heteroscedastic only on the other side. While our analysis focuses on rank-1 estimation, we expect that all results admit proper extensions to rank-\(r\) signals, where \(r\) is a constant independent on \(n,d\).

The design and analysis of the spectral estimator draws connections with approximate message passing and, along the way, we introduce a Bayes-AMP algorithm which could be of independent interest. In this paper, we employ Bayes-AMP solely as a proof technique. However, one could use the spectral method designed here as an initialization of Bayes-AMP itself, after suitably correcting its iterates. This strategy has been successfully carried out for i.i.d. Gaussian noise in  and for rotationally invariant noise in [55; 81]. Bayes-AMP is well equipped to exploit signal priors more informative than the Gaussian one, and AMP algorithms are known to achieve the information-theoretically optimal estimation error for low-rank matrix inference [56; 6]. Nevertheless, we point out two obstacles towards doing so in the presence of doubly heteroscedastic noise. First, for general priors, establishing the information-theoretic limits remains a challenging open problem, and it is unclear whether a low-dimensional characterization of the free energy (and, hence, of the MMSE) is possible. Second, even for Gaussian priors, Bayes-AMP reduces to the proposed spectral estimator, which is not Bayes-optimal for the general case of doubly heteroscedastic noise.

Finally, the proposed spectral estimator makes non-trivial use of the covariances \(,\), which are assumed to be known. When such matrices possess additional structure - e.g., they are sparse , their inverses are sparse  or they are circulant or Toeplitz  - their consistent estimation is possible, see also the survey . However, in general, \(,\) cannot be consistently estimated from the data when \(n\) and \(d\) grow proportionally. Thus, a challenging open problem is to construct estimators that retain comparable performance without knowing the noise covariances. The paper  addresses the challenge of unknown covariances by considering a modified model where one additionally observes an independent copy of noise. The statistician can then estimate the covariance from the noise-only observation and use it as a surrogate of the true covariance for estimating the signals from the spiked model. It is possible to derive similar results in the doubly heteroskedastic setting considered in our paper. If the covariances are completely unknown, then our model (with Gaussian priors) is equivalent to a spiked matrix model with a certain bi-rotationally invariant noise. This problem is expected to exhibit rather different behaviors than when covariances are known, see [7; 29] for recent progress on understanding the statistical and computational limits for such models.