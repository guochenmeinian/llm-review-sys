# Cal-QL: Calibrated Offline RL Pre-Training for

Efficient Online Fine-Tuning

 Mitsuhiko Nakamoto\({}^{1}\)1 Yuexiang Zhai\({}^{1}\)1 Anikait Singh\({}^{1}\) Max Sobol Mark\({}^{2}\)

Yi Ma\({}^{1}\) Chelsea Finn\({}^{2}\) Aviral Kumar\({}^{1}\) Sergey Levine\({}^{1}\)

\({}^{1}\)UC Berkeley Stanford University

###### Abstract

A compelling use case of offline reinforcement learning (RL) is to obtain a policy initialization from existing datasets followed by fast online fine-tuning with limited interaction. However, existing offline RL methods tend to behave poorly during fine-tuning. In this paper, we study the fine-tuning problem in the context of conservative offline RL methods and we devise an approach for learning an effective initialization from offline data that also enables fast online fine-tuning capabilities. Our approach, calibrated Q-learning (Cal-QL), accomplishes this by learning a conservative value function initialization that underestimates the value of the learned policy from offline data, while also ensuring that the learned Q-values are at a reasonable scale. We refer to this property as calibration, and define it formally as providing a lower bound on the true value function of the learned policy and an upper bound on the value of some other (suboptimal) reference policy, which may simply be the behavior policy. We show that a conservative offline RL algorithm that also learns a calibrated value function leads to effective online fine-tuning, enabling us to take the benefits of offline initializations in online fine-tuning. In practice, Cal-QL can be implemented on top of the conservative Q learning (CQL)  for offline RL within a one-line code change. Empirically, Cal-QL outperforms state-of-the-art methods on **9/11** fine-tuning benchmark tasks that we study in this paper. Code and video are available at https://nakamoto.github.io/Cal-QL

## 1 Introduction

Modern machine learning successes follow a common recipe: pre-training models on general-purpose, Internet-scale data, followed by fine-tuning the pre-trained initialization on a limited amount of data for the task of interest . How can we translate such a recipe to sequential decision-making problems? A natural way to instantiate this paradigm is to utilize offline reinforcement learning (RL)  for initializing value functions and policies from static datasets, followed by online fine-tuning to improve this initialization with limited active interaction. If successful, such a recipe might enable efficient online RL with much fewer samples than current methods that learn from scratch.

Many algorithms for offline RL have been applied to online fine-tuning. Empirical results across such works suggest a counter-intuitive trend: policy initializations obtained from more effective offline RL methods tend to exhibit worse online fine-tuning performance, even within the same task (see Table 2 of Kostrikov et al.  & Figure 4 of Xiao et al. ). On the other end, online RL methods training from scratch (or RL from demonstrations , where the replay buffer is seeded with the offline data) seem to improve online at a significantly faster rate. However,these online methods require actively collecting data by rolling out policies from scratch, which inherits similar limitations to naive online RL methods in problems where data collection is expensive or dangerous. Overall, these results suggest that it is challenging to devise an offline RL algorithm that both acquires a good initialization from prior data and also enables efficient fine-tuning.

How can we devise a method to learn an effective policy initialization that also improves during fine-tuning? Prior work [32; 6] shows that one can learn a good offline initialization by optimizing the policy against a _conservative_ value function obtained from an offline dataset. But, as we show in Section 4.1, conservatism alone is insufficient for efficient online fine-tuning. Conservative methods often tend to "unlearn" the policy initialization learned from offline data and waste samples collected via online interaction in recovering this initialization. We find that the "unlearning" phenomenon is a consequence of the fact that value estimates produced via conservative methods can be significantly lower than the ground-truth return of _any_ valid policy. Having Q-value estimates that do not lie on a similar scale as the return of a valid policy is problematic. Because once fine-tuning begins, actions executed in the environment for exploration that are actually worse than the policy learned from offline data could erroneously appear better, if their ground-truth return value is larger than the learned conservative value estimate. Hence, subsequent policy optimization will degrade the policy performance until the method recovers.

If we can ensure that the conservative value estimates learned using the offline data are _calibrated_, meaning that these estimates are on a similar scale as the true return values, then we can avoid the unlearning phenomenon caused by conservative methods (see the formal definition in 4.1). Of course, we cannot enforce such a condition perfectly, since it would require eliminating all errors in the value function. Instead, we devise a method for ensuring that the learned values upper bound the true values of some _reference policy_ whose values can be estimated more easily (e.g., the behavior policy), while still lower bounding the values of the learned policy. Though this does not perfectly ensure that the learned values are correct, we show that it still leads to sample-efficient online fine-tuning. Thus, our practical method, **calibrated Q-learning (Cal-QL)**, learns conservative value functions that are "calibrated" against the behavior policy, via a simple modification to existing conservative methods.

The main contribution of this paper is Cal-QL, a method for acquiring an offline initialization that facilitates online fine-tuning. Cal-QL aims to learn conservative value functions that are calibrated with respect to a reference policy (e.g., the behavior policy). Our analysis of Cal-QL shows that Cal-QL attains stronger guarantees on cumulative regret during fine-tuning. In practice, Cal-QL can be implemented on top of conservative Q-learning , a prior offline RL method, without any additional hyperparameters. We evaluate Cal-QL across a range of benchmark tasks from ,  and , including robotic manipulation and navigation. We show that Cal-QL matches or outperforms the best methods on all tasks, in some cases by 30-40%.

## 2 Related Work

Several prior works suggest that online RL methods typically require a large number of samples [50; 54; 61; 26; 64; 18; 38] to learn from scratch. We can utilize offline data to accelerate online RL algorithms. Prior works do this in a variety of ways: incorporating the offline data into the replay buffer of online RL [48; 53; 23; 52], utilizing auxiliary behavioral cloning losses with policy gradients [46; 27; 67; 66], or extracting a high-level skill space for downstream online RL [17; 1]. While these methods improve the sample efficiency of online RL from scratch, as we will also show in our results, they do not eliminate the need to actively roll out poor policies for data collection.

To address this issue, a different line of work first runs offline RL for learning a good policy and value initialization from the offline data, followed by online fine-tuning [45; 30; 41; 3; 56; 36; 42]. These

Figure 1: We study **offline RL pre-training followed by online RL fine-tuning**. Some prior offline RL methods tend to exhibit slow performance improvement in this setting (yellow), resulting in worse asymptotic performance. Others suffer from initial performance degradation once online fine-tuning begins (red), resulting in a high cumulative regret. We develop an approach that “_calibrates_” the learned value function to attain a fast improvement with a smaller regret (blue).

approaches typically employ offline RL methods based on policy constraints or pessimism [12; 49; 16; 15; 30; 51; 36] on the offline data, then continue training with the same method on a combination of offline and online data once fine-tuning begins [43; 28; 62; 32; 4]. Although pessimism is crucial for offline RL [25; 6], using pessimism or constraints for fine-tuning [45; 30; 41] slows down fine-tuning or leads to initial unlearning, as we will show in Section 4.1. In effect, these prior methods either fail to improve as fast as online RL or lose the initialization from offline RL. We aim to address this limitation by understanding some conditions on the offline initialization that enable fast fine-tuning.

Our work is most related to methods that utilize a pessimistic RL algorithm for offline training but incorporate exploration in fine-tuning [36; 42; 56]. In contrast to these works, our method aims to learn a better offline initialization that enables standard online fine-tuning. Our approach fine-tunes naively without ensembles  or exploration  and, as we show in our experiments, this alone is enough to outperform approaches that employ explicit optimism during data collection.

## 3 Preliminaries and Background

The goal in RL is to learn the optimal policy for an MDP \(=(,,P,r,,)\). \(,\) denote the state and action spaces. \(P(s^{}|s,a)\) and \(r(s,a)\) are the dynamics and reward functions. \((s)\) denotes the initial state distribution. \((0,1)\) denotes the discount factor. Formally, the goal is to learn a policy \(:\) that maximizes cumulative discounted value function, denoted by \(V^{}(s)=_{t}_{a_{t}(s_{t})}[ ^{t}r(s_{t},a_{t})|s_{0}=s]\). The Q-function of a given policy \(\) is defined as \(Q^{}(s,a)=_{t}_{a_{t}(s_{t})}[ ^{t}r(s_{t},a_{t})|s_{0}=s,a_{0}=a]\), and we use \(Q^{}_{}\) to denote the estimate of the Q-function of a policy \(\) as obtained via a neural network with parameters \(\).

Given access to an offline dataset \(=\{(s,a,r,s^{})\}\) collected using a behavior policy \(_{}\), we aim to first train a good policy and value function using the offline dataset \(\) alone, followed by an online phase that utilizes online interaction in \(\). Our goal during fine-tuning is to obtain the optimal policy with the smallest number of online samples. This can be expressed as minimizing the **cumulative regret** over rounds of online interaction: \((K):=_{s}_{k=1}^{K}[V^{}(s)-V^{ ^{k}}(s)]\). As we demonstrate in Section 7, existing methods face challenges in this setting.

Our approach will build on the conservative Q-learning (CQL)  algorithm. CQL imposes an additional regularizer that penalizes the learned Q-function on out-of-distribution (OOD) actions while compensating for this pessimism on actions seen within the training dataset. Assuming that the value function is represented by a function, \(Q_{}\), the training objective of CQL is given by

\[_{}_{s,a} [Q_{}(s,a)]-_{s,a}[Q_{}( s,a)])}_{()}+_{s,a,s^{}} Q_{}(s,a)-^{}(s,a)^{2},\] (3.1)

where \(^{}(s,a)\) is the backup operator applied to a delayed target Q-network, \(\): \(^{}(s,a):=r(s,a)+_{a^{}(a^{ }|s^{})}[(s^{},a^{})]\). The second term is the standard TD error [40; 13; 20]. The first term \(()\) (in blue) is a conservative regularizer that aims to prevent overestimation in the Q-values for OOD actions by minimizing the Q-values under the policy \((a|s)\), and counterbalances by maximizing the Q-values of the actions in the dataset following the behavior policy \(_{}\).

## 4 When Can Offline RL Initializations Enable Fast Online Fine-Tuning?

A starting point for offline pre-training and online fine-tuning is to simply initialize the value function with one that is produced by an existing offline RL method and then perform fine-tuning. However, we empirically find that initializations learned by many offline RL algorithms can perform poorly during fine-tuning. We will study the reasons for this poor performance for the subset of conservative methods to motivate and develop our approach for online fine-tuning, calibrated Q-learning.

### Empirical Analysis

Offline RL followed by online fine-tuning typically poses non-trivial challenges for a variety of methods. While analysis in prior work  notes challenges for a subset of offline RL methods, in Figure 2, we evaluate the fine-tuning performance of a variety of prior offline RL methods (CQL , IQL , TD3+BC , AWAC ) on a particular diagnostic instance of a visual pick-and-place task with a distractor object and sparse binary rewards , and find that all methods struggle to attain the best possible performance, quickly. More details about this task are in Appendix B.

While the offline Q-function initialization obtained from all methods attains a similar (normalized) return of around 0.5, they suffer from difficulties during fine-tuning: TD3+BC, IQL, AWAC attain slow asymptotic performance and CQL unlearns the offline initialization, followed by spending a large amount of online interaction to recover the offline performance again, before any further improvement. This initial unlearning appears in multiple tasks as we show in Appendix F. In this work, we focus on developing effective fine-tuning strategies on top of conservative methods like CQL. To do so, we next aim to understand the potential reason behind the initial unlearning in CQL.

**Why does CQL unlearn initially?** To understand why CQL unlearns initially, we inspect the learned Q-values averaged over the dataset in Figure 3. Observe that the Q-values learned by CQL in the offline phase are _much_ smaller than their ground-truth value (as expected), but these Q-values drastically jump and adjust in scale when fine-tuning begins. In fact, we observe that performance recovery (red segment in Figure 3) _coincides_ with a period where the range of Q-values changes to match the true range. This is as expected: as a conservative Q-function experiences new online data, actions much worse than the offline policy on the rollout states appear to attain higher rewards compared to the highly underestimated offline Q-function, which in turn deceives the policy optimizer into unlearning the initial policy. We illustrate this idea visually in Figure 4. Once the Q-function has adjusted and the range of Q-values closely matches the true range, then fine-tuning can proceed normally, after the dip.

**To summarize,** our empirical analysis indicates that methods existing fine-tuning methods suffer from difficulties such as initial unlearning or poor asymptotic performance. In particular, we observed that conservative methods can attain good asymptotic performance, but "waste" samples to correct the learned Q-function. Thus, in this paper, we attempt to develop a good fine-tuning method that builds on top of an existing conservative offline RL method, CQL, but aims to "calibrate" the Q-function so that the initial dip in performance can be avoided.

### Conditions on the Offline Initialization that Enable Fast Fine-Tuning

Our observations from the preceding discussion motivate two conclusions in regard to the offline Q-initialization for fast fine-tuning: **(a)** methods that learn **conservative** Q-functions can attain good asymptotic performance, and **(b)** if the learned Q-values closely match the range of ground-truth Q-values on the task, then online fine-tuning does not need to devote samples to unlearn and then recover the offline initialization. One approach to formalize this intuition of Q-values lying on a similar scale as the ground-truth Q-function is via the requirement that the conservative Q-values learned by the conservative offline RL method must be lower-bounded by the ground-truth Q-value of a sub-optimal reference policy. This will prevent conservatism from learning overly small Q-values. We will refer to this property as "calibration" with respect to the reference policy.

**Definition 4.1** (Calibration).: _An estimated Q-function \(Q_{}^{}\) for a given policy \(\) is said to be calibrated with respect to a reference policy \(\) if \(_{a}[Q_{}^{}(s,a)]_{a }[Q^{}(s,a)]:=V^{}(s), s D\)._

If the learned Q-function \(Q_{}^{}\) is calibrated with respect to a policy \(\) that is worse than \(\), it would prevent unlearning during fine-tuning that we observed in the case of CQL. This is because the

Figure 3: **The evolution of the average Q-value and the success rate of CQL over the course of offline pre-training and online fine-tuning.** Fine-tuning begins at 50K steps. The red-colored part denotes the period of performance recovery which also coincides with the period of Q-value adjustment.

Figure 2: **Multiple prior offline RL algorithms suffer from difficulties during fine-tuning including poor asymptotic performance and initial unlearning.**

policy optimizer would not unlearn \(\) in favor of a policy that is worse than the reference policy \(\) upon observing new online data as the expected value of \(\) is constrained to be larger than \(V^{}\): \(_{a}[Q_{}^{}(s,a)] V^{}(s)\). Our practical approach Cal-QL will enforce calibration with respect to a policy \(\) whose ground-truth value, \(V^{}(s)\), can be estimated reliably without bootstrapping error (e.g., the behavior policy induced by the dataset). This is the key idea behind our method (as we will discuss next) and is visually illustrated in Figure 4.

## 5 Cal-QL: Calibrated Q-Learning

Our approach, calibrated Q-learning (Cal-QL) aims to learn a conservative and calibrated value function initializations from an offline dataset. To this end, Cal-QL builds on CQL  and then constrains the learned Q-function to produce Q-values larger than the Q-value of a reference policy \(\) per Definition 4.1. In principle, our approach can utilize many different choices of reference policies, but for developing a practical method, we simply utilize the behavior policy as our reference policy.

**Calibrating CQL.** We can constrain the learned Q-function \(Q_{}^{}\) to be larger than \(V^{}\) via a simple change to the CQL training objective shown in Equation 3.1: masking out the push down of the learned Q-value on out-of-distribution (OOD) actions in CQL if the Q-function is not calibrated, i.e., if \(_{a}[Q_{}^{}(s,a)] V^{}(s)\). Cal-QL modifies the CQL regularizer, \(()\) in this manner:

\[_{s,a}[(Q_{}(s,a),V^{} (s))]-_{s,a}[Q_{}(s,a)],\] (5.1)

where the changes from standard CQL are depicted in red. As long as \(\) (in Equation 3.1) is large, for any state-action pair where the learned Q-value is smaller than \(Q^{}\), the Q-function in Equation 5.1 will upper bound \(Q^{}\) in a tabular setting. Of course, as with any practical RL method, with function approximators and gradient-based optimizers, we cannot guarantee that we can enforce this condition for every state-action pair, but in our experiments, we find that Equation 5.1 is sufficient to enforce the calibration in expectation over the states in the dataset.

**Pseudo-code and implementation details.** Our implementation of Cal-QL directly builds on the implementation of CQL from Geng . We present a pseudo-code for Cal-QL in Appendix A. Additionally, we list the hyperparameters \(\) for the CQL algorithm and our baselines for each suite of tasks in Appendix C. Following the protocol in prior work [30; 52], the practical implementation of Cal-QL trains on a mixture of the offline data and the new online data, weighted in some proportion during fine-tuning. To get \(V^{}(s)\), we can fit a function approximator \(Q_{}^{}\) or \(V_{}^{}\) to the return-to-go values via regression, but we observed that also simply utilizing the return-to-go estimates for tasks that end in a terminal was sufficient for our use case. We show in Section 7, how this simple _one-line_ change to the objective drastically improves over prior fine-tuning results.

Figure 4: **Intuition behind policy unlearning with CQL and the idea behind Cal-QL. The plot visualizes a slice of the learned Q-function and the ground-truth values for a given state. Erroneous peaks on suboptimal actions (x-axis) arise when updating CQL Q-functions with online data. This in turn can lead the policy to deviate away from high-reward actions covered by the dataset in favor of erroneous new actions, resulting in deterioration of the pre-trained policy. In contrast, Cal-QL corrects the scale of the learned Q-values by using a reference value function, such that actions with worse Q-values than the reference value function do not erroneously appear optimal in fine-tuning.**

Theoretical Analysis of Cal-QL

We will now analyze the cumulative regret attained over online fine-tuning, when the value function is pre-trained with Cal-QL, and show that enforcing calibration (Defintion 4.1) leads to a favorable regret bound during the online phase. Our analysis utilizes tools from Song et al. (2019), but studies the impact of calibration on fine-tuning. We also remark that we simplify the treatment of certain aspects (e.g., how to incorporate pessimism) as it allows us to cleanly demonstrate the benefits of calibration.

**Notation & terminology.** In our analysis, we will consider an idealized version of Cal-QL for simplicity. Specifically, following prior work (Song et al., 2019) under the bilinear model (Beng et al., 2019), we will operate in a finite-horizon setting with a horizon \(H\). We denote the learned Q-function at each learning iteration \(k\) for a given \((s,a)\) pair and time-step \(h\) by \(Q_{}^{k}(s,a)\). For any given policy \(\), let \(C_{} 1\) denote the concentrability coefficient such that \(C_{}:=_{f}^{H-1}_{s,a h_{ }^{}}[f_{h+1}(s,a)-f_{h}(s,a)]}{^{H-1} _{s,a h_{}}(f_{h+1}(s,a)-f_{h}(s,a))^{2}}}\), i.e., a coefficient that quantifies the distribution shift between the policy \(\) and the dataset \(\), in terms of the ratio of Bellman errors averaged under \(\) and the dataset \(\). Note that \(\) represents the Q-function class and we assume \(\) has a bellman-bilinear rank (Beng et al., 2019) of \(d\). We also use \(C_{}^{}\) to denote the concentrability coefficient over a subset _calibrated_ Q-functions w.r.t. a reference policy \(\): \(C_{}^{}:=_{f,f(s,a) Q^{}(s,a)}^{H-1}_{s,a h_{}^{}}[f_{h+1}(s,a)-f_{h}( s,a)]}{^{H-1}_{s,a h_{}}(f_{h+1}(s,a)-f_{h}( s,a))^{2}}}\), which provides \(C_{}^{} C_{}\).

Similar to \(\), let \(d_{}\) denote the bellman bilinear rank of \(_{}\) - the calibrated Q-function class w.r.t. the reference policy \(\). Intuitively, we have \(_{}\), which implies that \(d_{} d\). The formal definitions are provided in Appendix H.2. We will use \(^{k}\) to denote the arg-max policy induced by \(Q_{}^{k}\).

**Intuition.** We intuitively discuss how calibration and conservatism enable Cal-QL to attain a smaller regret compared to not imposing calibration. Our goal is to bound the cumulative regret of online fine-tuning, \(_{k}_{s_{0}}[V^{^{}}(s_{0})-V^{^{k}}(s_{0})]\). We can decompose this expression into two terms:

\[(K)=^{K}_{s_{0}}V^ {}(s_{0})-_{a}Q_{}^{k}(s_{0},a)}_{(i):=}+ ^{K}_{s_{0}}[_{a}Q_{}^{ k}(s_{0},a)-V^{^{k}}(s_{0})]}_{(ii):=}.\] (6.1)

This decomposition of regret into terms (i) and (ii) is instructive. Term (ii) corresponds to the amount of over-estimation in the learned value function, which is expected to be small if a conservative RL algorithm is used for training. Term (i) is the difference between the ground-truth value of the optimal policy and the learned Q-function and is negative if the learned Q-function were calibrated against the optimal policy (per Definition 4.1). Of course, this is not always possible because we do not know \(V^{}\) a priori. But note that when Cal-QL utilizes a reference policy \(\) with a high value \(V^{}\), close to \(V^{}\), then the learned Q-function \(Q_{}\) is calibrated with respect to \(Q^{}\) per Condition 4.1 and term (i) can still be controlled. Therefore, controlling this regret requires striking a balance between learning a calibrated (term (i)) and conservative (term (ii)) Q-function. We now formalize this intuition and defer the detailed proof to Appendix H.6.

**Theorem 6.1** (Informal regret bound of Cal-QL).: _With high probability, Cal-QL obtains the following bound on total regret accumulated during online fine-tuning:_

\[(K)=C_{^{}}^{}H|)}},\ K_{}[V^{}(s_{0})-V^{}(s_{0})]+H K|)}}},\]

_where \(\) is the functional class of the Q-function._

**Comparison to Song et al. (2019).** Song et al. (2019) analyzes an online RL algorithm that utilizes offline data without imposing conservatism or calibration. We now compare Theorem 6.1 to Theorem 1 of Song et al. (2019) to understand the impact of these conditions on the final regret guarantee. Theorem 1 of Song et al. (2019) presents a regret bound: \((K)=(C_{^{}}H|) }})\) and we note some improvements in our guarantee, that we also verify via experiments in Section 7.3: **(a)** for the setting where the reference policy \(\) contains near-optimal behavior, i.e., \(V^{}-V^{} O(H|)}/K})\), Cal-QL can enable a tighter regret guarantee compared to Song et al. (2019); **(b)** as we show in Appendix H.3, the concentrability coefficient \(C_{^{}}^{}\) appearing in our guarantee is no larger than the one that appears in Theorem 1 of Song et al. (2019), providing another source of improvement; and **(c)** finally, in the case where the reference policy has broad coverage _and_ is highly sub-optimal, Cal-QL reverts back to the guarantee from (Song et al., 2019), meaning that Cal-QL improves upon this prior work.

## 7 Experimental Evaluation

The goal of our experimental evaluation is to study how well Cal-QL can facilitate sample-efficient online fine-tuning. To this end, we compare Cal-QL with several other state-of-the-art fine-tuning methods on a variety of offline RL benchmark tasks from D4RL , Singh et al. , and Nair et al. , evaluating performance before and after fine-tuning. We also study the effectiveness of Cal-QL on higher-dimensional tasks, where the policy and value function must process raw image observations. Finally, we perform empirical studies to understand the efficacy of Cal-QL with different dataset compositions and the impact of errors in the reference value function estimation.

**Offline RL tasks and datasets.** We evaluate Cal-QL on a number of benchmark tasks and datasets used by prior works [30; 45] to evaluate fine-tuning performance: **(1)** The AntMaze tasks from D4RL  that require controlling an ant quadruped robot to navigate from a starting point to a desired goal location in a maze. The reward is +1 if the agent reaches within a pre-specified small radius around the goal and 0 otherwise. **(2)** The FrankaKitchen tasks from D4RL require controlling a 9-DoF Franka robot to attain a desired configuration of a kitchen. To succeed, a policy must complete four sub-tasks in the kitchen within a single rollout, and it receives a binary reward of +1/0 for every sub-task it completes. **(3)** Three Adroit dexterous manipulation tasks [47; 30; 45] that require learning complex manipulation skills on a 28-DoF five-fingered hand to **(a)** manipulate a pen in-hand to a desired configuration (pen-binary), **(b)** open a door by unlatching the handle (door-binary), and **(c)** relocating a ball to a desired location (relocate-binary). The agent obtains a sparse binary +1/0 reward if it succeeds in solving the task. Each of these tasks only provides a narrow offline dataset consisting of 25 demonstrations collected via human teleoperation and additional trajectories collected by a BC policy. Finally, to evaluate the efficacy of Cal-QL on a task where we learn from raw visual observations, we study **(4)** a pick-and-place task from prior work [51; 34] that requires learning to pick a ball and place it in a bowl, in the presence of distractors. Additionally, we compared Cal-QL on D4RL locomotion tasks (halfcheetah, hopper, walker) in Appendix D.

Figure 5: **Tasks: We evaluate Cal-QL on a diverse set of benchmark problems: AntMaze and Franakkitchen domains from , Adroit tasks from  and a vision-based robotic manipulation task from .**

Figure 6: **Online fine-tuning after offline initialization on the benchmark tasks. The plots show the online fine-tuning phase _after_ pre-training for each method (except SAC-based approaches which are not pre-trained). Observe that Cal-QL consistently matches or exceeds the speed and final performance of the best prior method and is the only algorithm to do so across all tasks. (6 seeds)**Comparisons, prior methods, and evaluation protocol.We compare Cal-QL to running online SAC  from scratch, as well as prior approaches that leverage offline data. This includes naively fine-tuning offline RL methods such as CQL  and IQL , as well as fine-tuning with AWAC , O3F  and online decision transformer (ODT) , methods specifically designed for offline RL followed by online fine-tuning. In addition, we also compare to a baseline that trains SAC  using both online data and offline data (denoted by "SAC + offline data") that mimics DDPGfD  but utilizes SAC instead of DDPG. We also compare to Hybrid RL , a recently proposed method that improves the sample efficiency of the "SAC + offline data" approach, and "CQL+SAC", which first pre-train with CQL and then run fine-tuning with SAC on a mixture of offline and online data without conservatism. More details of each method can be found in Appendix C. We present learning curves for online fine-tuning and also quantitatively evaluate each method on its ability to improve the initialization learned from offline data measured in terms of **(i)** final performance after a pre-defined number of steps per domain and **(ii)** the cumulative regret over the course of online fine-tuning. In Section 7.2, we run Cal-QL with a higher update-to-data (UTD) ratio and compare it to RLPD , a more sample-efficient version of "SAC + offline data".

### Empirical Results

We first present a comparison of Cal-QL in terms of the normalized performance before and after fine-tuning in Table 1 and the cumulative regret in a fixed number of online steps in Table 2. Following the protocol of , we normalize the average return values for each domain with respect to the highest possible return (+4 in FrankaKitchen; +1 in other tasks; see Appendix C.1 for more details).

**Cal-QL improves the offline initialization significantly.** Observe in Table 1 and Figure 6 that while the performance of offline initialization acquired by Cal-QL is comparable to that of other methods such as CQL and IQL, Cal-QL is able to improve over its offline initialization the most by **106.9%** in aggregate and achieve the best fine-tuned performance in **9 out of 11** tasks.

**Cal-QL enables fast fine-tuning.** Observe in Table 2 that Cal-QL achieves the smallest regret on **8 out of 11** tasks, attaining an average regret of 0.22 which improves over the next best method (IQL) by **42%**. Intuitively, this means that Cal-QL does not require running highly sub-optimal policies. In tasks such as relocate-binary, Cal-QL enjoys the fast online learning benefits associated with naive online RL methods that incorporate the offline data in the replay buffer (SAC + offline data and Cal-QL are the only two methods to attain a score of \(\) 90% on this task) unlike prior offline RL methods. As shown in Figure 6, in the kitchen and antmaze domains, Cal-QL brings the benefits of fast online learning together with a good offline initialization, improving drastically on the regret metric. Finally, observe that the initial unlearning at the beginning of fine-tuning with conservative methods observed in Section 4.1 is greatly alleviated in all tasks (see Appendix F for details).

### Cal-QL With High Update-to-Data (UTD) Ratio

We can further enhance the online sample efficiency of Cal-QL by increasing the number of gradient steps per environment step made by the algorithm. The number of updates per environment step is usually called the update-to-data (UTD) ratio. In standard online RL, running off-policy Q-learning with a high UTD value (e.g., 20, compared to the typical value of 1) often results in challenges pertaining to overfitting [39; 5; 2; 8]. As expected, we noticed that running Cal-QL with a high UTD value also leads these overfitting challenges. To address these challenges in high UTD settings, we combine Cal-QL with the Q-function architecture in recent work, RLPD  (i.e., we utilized layer normalization in the Q-function and ensembles akin to Chen et al. ), that attempts to tackle

 Task & COL & IQL & AWAC & OSF & OOT & COL+SAC & Hybrid SRL & SAC+ & Cal-QL (Ours) \\   large-diverse & \(25 87\) & \(40 59\) & \(0 00\) & \(59 28\) & \(00 01\) & \(36 00\) & \( 00\) & \( 00\) & \( 00\) & \(33 95\) \\ large-play & \(34 76\) & \(41 51\) & \(0 00\) & \(68 01\) & \(00 00\) & \(21 00\) & \( 00\) & \( 00\) & \( 00\) & \(26 90\) \\ medium-diverse & \(65 98\) & \(70 92\) & \(00 00\) & \(92 97\) & \(00 03\) & \(64 98\) & \( 02\) & \( 68\) & \( 00\) & \(75 98\) \\ medium-play & \(62 98\) & \(72 94\) & \(0 00\) & \(89 99\) & \(00 05\) & \(67 98\) & \( 25\) & \( 96\) & \( 00\) & \(54 97\) \\  partial & \(71 75\) & \(40 60\) & \(01 13\) & \(11 22\) & - & \(71 00\) & \(00 00\) & \( 00\) & \( 00\) & \(67 79\) \\ mixed & \(56 50\) & \(48 48\) & \(02 12\) & \(06 33\) & \(59 01\) & \(01 01\) & \(00 00\) & \( 02\) & \(38 80\) \\ complete & \(13 34\) & \(57 50\) & \(01 08\) & \(71 41\) & - & \(21 06\) & \(00 00\) & \( 05\) & \( 06\) & \(22 68\) \\  pen & \(35 13\) & \(88 92\) & \(83 92\) & \(91 98\) & - & \(48 10\) & - & \(54 17\) & \(-11\) & \(79 99\) \\ _door_ & \(22 88\) & \(41 88\) & \(29 13\) & \(04 08\) & - & \(29 66\) & \(88\) & \(-39\) & \(17 17\) & \(35 92\) \\ relocate & \(66 69\) & \(06 45\) & \(06 08\) & \(03 35\) & - & \(01 00\) & \(99 16\) & \( 00\) & \(03 98\) \\  manipulation & \(30 97\) & \(49 81\) & \(30 73\) & - & - & \(42 41\) & - \(00\) & \( 01\) & \(-01\) & \(49 99\) \\  
**average** & \(42 71\) & \(50 69\) & \(16 20\) & \(44 45\) & \(00 02\) & \(42 29\) & \( 24\) & \( 23\) & \( 04\) & \(44 90\) \\ 
**improvement** & \(+71.0\%\) & \(+37.7\%\) & \(+23.7\%\) & \(+3.0\%\) & N/A & \(-30.3\%\) & N/A & N/A & N/A & **+ 106.9\%** \\ 

Table 1: **Normalized score before & after online fine-tuning.** Observe that Cal-QL improves over the best prior fine-tuning method and attains a much larger performance improvement over the course of online fine-tuning. The numbers represent the normalized score out of 100 following the convention in .

overfitting challenges. Note that Cal-QL still first pre-trains on the offline dataset using Equation 5.1 followed by online fine-tuning, unlike RLPD that runs online RL right from the start. In Figure 7, we compare Cal-QL (\(=20\)) with RLPD  (\(=20\)) and also Cal-QL (\(=1\)) as a baseline. Observe that Cal-QL (\(=20\)) improves over Cal-QL (\(=1\)) and training from scratch (RLPD).

### Understanding the Behavior of Cal-QL

In this section, we aim to understand the behavior of Cal-QL by performing controlled experiments that modify the dataset composition, and by investigating various metrics to understand the properties of scenarios where utilizing Cal-QL is especially important for online fine-tuning.

**Effect of data composition.** To understand the efficacy of Cal-QL with different data compositions, we ran it on a newly constructed fine-tuning task on the medium-size AntMaze domain with a low-coverage offline dataset, which is generated via a scripted controller that starts from a fixed initial position and navigates the ant to a fixed goal position. In Figure 8, we plot the performance of Cal-QL and baseline CQL (for comparison) on this task, alongside the trend of average Q-values over the course of offline pre-training (to the left of the dashed vertical line, before 250 training epochs) and online fine-tuning (to the right of the vertical dashed line, after 250 training epochs), and the trend of _bounding rate_, i.e., the fraction of transitions in the data-buffer for which the constraint in Cal-QL actively lower-bounds the learned Q-function with the reference value. For comparison, we also plot these quantities for a diverse dataset with high coverage on the task (we use the antmaze-medium-diverse from Fu et al.  as a representative diverse dataset) in Figure 8.

Observe that for the diverse dataset, both naive CQL and Cal-QL perform similarly, and indeed, the learned Q-values behave similarly for both of these methods. In this setting, online learning doesn't spend samples to correct the Q-function when fine-tuning begins leading to a low bounding rate, almost always close to 0. Instead, with the narrow dataset, we observe that the Q-values learned by

 Task &  &  & AWAC & O3F &  &  &  &  &  &  \\   large-diverse & 0.35 & 0.46 & 1.00 & 0.62 & 0.98 & 0.99 & 1.00 & 1.00 & 1.00 & **0.20** \\ large-play & 0.32 & 0.52 & 1.00 & 0.91 & 1.00 & 0.99 & 1.00 & 1.00 & 1.00 & **0.28** \\ medium-diverse & 0.06 & 0.08 & 0.99 & **0.03** & 0.95 & 0.06 & 0.98 & 0.77 & 1.00 & 0.05 \\ medium-play & 0.09 & 0.10 & 0.99 & **0.04** & 0.96 & 0.06 & 0.90 & 0.47 & 1.00 & 0.07 \\  partial & 0.31 & 0.49 & 0.89 & 0.78 & - & 0.97 & 0.98 & 0.98 & 0.92 & **0.27** \\ mixed & 0.55 & 0.60 & 0.88 & 0.72 & - & 0.97 & 0.99 & 1.00 & 0.91 & **0.27** \\ complete & 0.70 & 0.53 & 0.97 & 0.66 & - & 0.99 & 0.99 & 0.96 & 0.91 & **0.44** \\  pen & 0.86 & **0.11** & 0.12 & 0.13 & - & 0.90 & 0.56 & 0.75 & 0.87 & **0.11** \\ door & 0.36 & 0.25 & 0.81 & 0.82 & - & **0.23** & 0.35 & 0.60 & 0.94 & **0.23** \\ relocate & 0.71 & 0.74 & 0.95 & 0.71 & - & 0.86 & **0.30** & 0.89 & 1.00 & 0.43 \\  manipulation & 0.15 & 0.32 & 0.38 & - & - & 0.61 & 1.00 & 1.00 & 0.99 & **0.11** \\  
**average** & 0.41 & 0.38 & 0.82 & 0.54 & 0.97 & 0.69 & 0.82 & 0.86 & 0.96 & **0.22** \\ 

Table 2: **Cumulative regret averaged over the steps of fine-tuning.** The smaller the better and 1.00 is the worst. Cal-QL attains the smallest overall regret, achieving the best performance among 8 / 11 tasks.

Figure 7: **Cal-QL with UTD=20**. Incorporating design choices from RLPD enables Cal-QL to achieve sample-efficient fine-tuning with UTD=20. Specifically, Cal-QL generally attains similar or higher asymptotic performance as RLPD, while also exhibiting a smaller cumulative regret. (3 seeds)

n a drop in performance (solid blue line on left), and naive CQL is unable to recover from this drop. Cal-QL  which calibrates the scale of the Q-function for many more samples in the dataset, stably transitions to online fine-tuning with no unlearning (solid red line on left).

This suggests that in settings with narrow datasets (e.g., in the experiment above and in the adroit and visual-manipulation domains from Figure 6), Q-values learned by naive conservative methods are more likely to be smaller than the ground-truth Q-function of the behavior policy due to function approximation errors. Hence utilizing Cal-QL to calibrate the Q-function against the behavior policy can be significantly helpful. On the other hand, with significantly high-coverage datasets, especially in problems where the behavior policy is also random and sub-optimal, Q-values learned by naive methods are likely to already be calibrated with respect to those of the behavior policy. Therefore no explicit calibration might be needed (and indeed, the bounding rate tends to be very close to 0 as shown in Figure 8). In this case, Cal-QL will revert back to standard CQL, as we observe in the case of the diverse dataset above. This intuition is also reflected in Theorem 6.1: when the reference policy \(\) is close to a narrow, expert policy, we would expect Cal-QL to be especially effective in controlling the efficiency of online fine-tuning.

**Estimation errors in the reference value function do not affect performance significantly.** In our experiments, we compute the reference value functions using Monte-Carlo return estimates. However, this may not be available in all tasks. How does Cal-QL behave when reference value functions must be estimated using the offline dataset itself? To answer this, we ran an experiment on the kitchen domain, where instead of using an estimate for \(Q^{}\) based on the Monte-Carlo return, we train a neural network function approximator \(Q^{}_{}\) to approximate \(Q^{}\) via supervised regression on to Monte-Carlo return, which is then utilized by Cal-QL. Observe in Figure 9, that the performance of Cal-QL largely remains unaltered. This implies as long as we can obtain a reasonable function approximator to estimate the Q-function of the reference policy (in this case, the behavior policy), errors in the reference Q-function do not affect the performance of Cal-QL significantly.

## 8 Discussion, Future Directions, and Limitations

In this work we developed Cal-QL  a method for acquiring conservative offline initializations that facilitate fast online fine-tuning. Cal-QL learns conservative value functions that are constrained to be larger than the value function of a reference policy. This form of calibration allows us to avoid initial unlearning when fine-tuning with conservative methods, while also retaining the effective asymptotic performance that these methods exhibit. Our theoretical and experimental results highlight the benefit of Cal-QL in enabling fast online fine-tuning. While Cal-QL outperforms prior methods, we believe that we can develop even more effective methods by adjusting calibration and conservatism more carefully. A limitation of our work is that we do not consider fine-tuning setups where pre-training and fine-tuning tasks are different, but this is an interesting avenue for future work.

Figure 8: **Performance of Cal-QL with data compositions.** Cal-QL is most effective with narrow datasets, where Q-values need to be corrected at the beginning of fine-tuning.

Figure 9: **Using a neural network approximator for the reference value function performs comparable to using the Monte-Carlo return.** This indicates that errors in the reference Q-function do not negatively impact the performance.