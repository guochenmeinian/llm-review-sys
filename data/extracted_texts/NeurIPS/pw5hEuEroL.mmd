# Unified Enhancement of Privacy Bounds for Mixture Mechanisms via \(f\)-Differential Privacy

Chendi Wang

University of Pennsylvania

Philadelphia, PA, USA, 19104 &

Shenzhen Research Institute of Big data

Shenzhen, Guangdong, China, 518000

chendi@wharton.upenn.edu

&Buxin Su

Department of Mathematics

University of Pennsylvania

Philadelphia, PA, USA, 19104

subuxin@sas.upenn.edu

equal contribution

&Jiayuan Ye

Department of Computer Science

National University of Singapore

Singapore

jiayuan@comp.nus.edu.sg

&Reza Shokri

Department of Computer Science

National University of Singapore

Singapore

reza@comp.nus.edu.sg

&Weijie J. Su

Wharton Statistics and Data Science Department

University of Pennsylvania

Philadelphia, PA, USA, 19104

suw@wharton.upenn.edu

###### Abstract

Differentially private (DP) machine learning algorithms incur many sources of randomness, such as random initialization, random batch subsampling, and shuffling. However, such randomness is difficult to take into account when proving differential privacy bounds because it induces mixture distributions for the algorithm's output that are difficult to analyze. This paper focuses on improving privacy bounds for shuffling models and one-iteration differentially private gradient descent (DP-GD) with random initializations using \(f\)-DP. We derive a closed-form expression of the trade-off function for shuffling models that outperforms the most up-to-date results based on \((,)\)-DP. Moreover, we investigate the effects of random initialization on the privacy of one-iteration DP-GD. Our numerical computations of the trade-off function indicate that random initialization can enhance the privacy of DP-GD. Our analysis of \(f\)-DP guarantees for these mixture mechanisms relies on an inequality for trade-off functions introduced in this paper. This inequality implies the joint convexity of \(F\)-divergences. Finally, we study an \(f\)-DP analog of the advanced joint convexity of the hockey-stick divergence related to \((,)\)-DP and apply it to analyze the privacy of mixture mechanisms.

## 1 Introduction

Differential privacy (DP, [16; 17]) is a rigorous mathematical framework for ensuring data privacy and has become a cornerstone of privacy-preserving data analysis over the past two decades. DP has found widespread applications in various data science fields, such as machine learning [12; 6; 44],query answering [18; 15], and synthetic data generation [37; 48; 29; 28]. A randomized mechanism is considered differentially private if the outputs of two neighboring datasets that differ in at most one element are indistinguishable from each other. The closeness of these outputs can be measured in various ways, resulting in the definition of \((,)\)-DP in  and its various relaxations.

The distinguishability between the outputs can be measured by statistical divergences. For example, \((,)\)-DP is associated with the so-called hockey-stick divergence . Another divergence relevant to differential privacy is the Renyi divergence [20; 38] which leads to Renyi DP [30; 10] and concentrated DP . In addition to divergence-based DP, a hypothesis testing perspective on differential privacy was proposed in . More recently,  established \(f\)-DP for differential privacy where the privacy is measured by the trade-off function of type I and type II errors.

In real-world applications of differential privacy, including differentially private machine learning, it is common to analyze the privacy budget of mechanisms that involve mixture distributions, where the mixture is introduced by stochastic components in the algorithm. Examples of such mechanisms include sub-sampled mechanisms [4; 52; 40; 31], shuffled mechanisms [13; 22; 23], and variants of the differentially private stochastic gradient descent (DP-SGD) algorithm [1; 9; 26; 3; 45] that involves random initialization and multiple rounds of mini-batch sampling. Recently, privacy amplification by iteration  has drawn much attention as it can be used to analyze the privacy bounds for DP-SGD [45; 2]) which leads to tighter privacy bounds compared to classical analysis based on the composition theorem [33; 39; 50].

While mixture mechanisms are essential in differentially private machine learning, the absence of an \(f\)-DP guarantee for their analysis remains a significant challenge. Moreover, existing divergence-based DP bounds for most of these mechanisms are not tight. This is primarily because the complex distribution resulting from the mixture makes it challenging to accurately quantify privacy guarantees. In order to illustrate this perspective, we consider the examples of shuffling models and DP gradient descent (DP-GD) with random initialization, as follows.

* In shuffling models, each user's data record is locally privatized using a local DP algorithm . Subsequently, a curator shuffles the dataset containing all users' data. The shuffling procedure introduces additional mixtures of binomial noise , thereby potentially amplifying the privacy provided by the local randomizer. Shuffling is commonly employed in machine learning algorithms for batch generation [45; 47]. To deal with this mixture, Hoeffding's inequality was used in previous literature [22; 23] that leads to the loss of information. Using \(f\)-DP in this paper, we derive an exact analytical trade-off function for the mixture of binomial distributions which is sharp.
* In deep learning, random initialization is usually adopted in the stochastic gradient descent to enhance the performance of deep neural networks . Intuitively, the inherent randomness introduced by initialization should contribute to the privacy amplification of DP-GD. However, Renyi differential privacy (DP) falls short in quantitatively measuring this randomness, even when applied to the simplest linear model. In this paper, we demonstrate how \(f\)-DP can effectively evaluate and quantify this inherent randomness from initialization.

**Our contributions.** This paper makes a two-fold contribution. Firstly, we propose a unified theory to analyze the privacy of mixture mechanisms within the framework of \(f\)-DP. Precisely, we derive an \(f\)-DP inequality for mixture distributions which implies the joint convexity of \(F\)-divergences for any convex \(F\). We name this result the "joint concavity of trade-off functions", as it is a lower bound for trade-off functions. The tightness of the joint concavity is also investigated. Moreover, we propose the "advanced joint concavity of trade-off functions" which is an \(f\)-DP analog of the advanced joint convexity of the hockey-stick divergence and results in sharper bounds in certain cases.

Building on our inequality, we have refined the privacy analysis of both shuffling models and DP-GD with random initialization using \(f\)-DP. Specifically, for shuffling models, we obtain trade-off functions in a closed-form representation, leading to tighter bounds compared to existing state-of-the-art results based on \((,)\)-DP. As for DP-GD, given the challenges in the trajectory analysis of multi-step iterations, we have chosen to explore a more straightforward one-iteration DP-GD. We demonstrate that using random initialization significantly enhances the privacy of the output from a single iteration.

## 2 Preliminaries on differential privacy

Let \(=\{z_{i}\}_{i=1}^{n}\) be a fixed dataset of size \(n\). Consider a randomized algorithm \(:^{n}\) that maps a dataset \(\) to \(()\) in some probability space \(\). Differential privacy requires that the change of one element in a dataset has a restricted impact on the output of \(\). Mathematically, we say \(\) satisfies \((,)\)-DP for some \( 0\) and \(0 1\) if

\[[(_{0}) S] e^{}[ (_{1}) S]+,\]

for any event \(S\) and any neighboring datasets \(_{0}\) and \(_{1}\). When \(=0\), we simply call \((,0)\)-DP as \(\)-DP. Based on the definition, we see that for small values of \(\) and \(\), it is challenging to distinguish between \(_{0}\) and \(_{1}\) based on the outputs of \((_{0})\) and \((_{1})\), as the distribution of \((_{0})\) closely resembles that of \((_{1})\).

The definition of \((,)\)-DP corresponds to the hockey-stick divergence. Let \(P\) and \(Q\) be two distributions with probability density functions (pdfs) \(p\) and \(q\), respectively. The hockey-stick divergence between \(P\) and \(Q\) is defined by \(H_{}(P\|Q)=(p(x)- q(x))_{+}dx\) for \( 1\) with \(()_{+}=\{0,\}\). With a little bit abuse of notations, in this paper, we define the divergence (or the trade-off function) between two random variables as the divergence (or the trade-off function) between their distributions. Then, a mechanism \(\) is \((,)\)-DP if and only if \(H_{e^{}}((_{0})\|(_{ 1}))\) for any neighboring datasets \(_{0}\) and \(_{1}\), which also implies \(H_{e^{}}((_{1})\|(_{0}))\).

The Renyi-DP (RDP) is defined based-on the Renyi divergence. The Renyi divergence of order \(>1\) between \(P\) and \(Q\) is given by

\[R_{}(P\|Q)=-1}( (x)})^{}q(x)dx.\]

For \(=1\) or \(+\), \(R_{1}\) or \(R_{}\) is the limit of \(R_{}\) as \(\) tends to \(1\) or \(+\). A mechanism \(\) is said to satisfy \((,)\)-RDP if \(R_{}((_{0})\|(_{ 1}))\) for any neighboring \(_{0}\) and \(_{1}\).

The distinguishability between \((_{0})\) and \((_{1})\) can be quantified using hypothesis testing, which aligns with the concept of \(f\)-DP. Consider a hypothesis testing problem \(H_{0}:P\) v.s. \(H_{1}:Q\) and a rejection rule \(\). We define the type I error as \(_{}=_{P}[]\), which is the probability that we reject the null hypothesis \(H_{0}\) by mistake. The type II error \(_{}=1-_{Q}[]\) is the probability that we accept the alternative \(H_{1}\) wrongly.

The trade-off function \(T(P,Q)\) is the minimal type II error at level \(\) of the type I error, that is,

\[T(P,Q)()=_{}\{_{}:_{}\}.\]

We say a mechanism \(\) satisfies \(f\)-DP if \(T((_{0}),(_{1})) f\) for any neighboring datasets \(_{0}\) and \(_{1}\). In particular, \(\) is said to satisfy \(\)-GDP if it is \(G_{}\)-DP, where \(G_{}(x)=(^{-1}(1-x)-)\), for \( 0\), is the Gaussian trade-off function with \(\) being the cumulative distribution function (cdf) of \((0,1)\). \(\) is considered to be more private if the corresponding trade-off function takes larger values. When \(\) achieves perfect privacy and \((_{0})\) and \((_{1})\) become completely indistinguishable, the trade-off function is \((x)=1-x\). Consequently, for any trade-off function \(f\), we have \(f\).

We say a trade-off function is symmetric if \(T(P,Q)=T(Q,P)\). Note that a trade-off function \(f\) may not necessarily be symmetric. But one can symmetrize it as shown in . The symmetrization of a trade-off function will be used when we analyze the shuffled mechanisms.

## 3 Joint concavity of trade-off functions

Let \(\{P_{i}\}_{i=1}^{m}\) and \(\{Q_{i}\}_{i=1}^{m}\) be two sequences of probability distributions. Denote the probability density functions (pdfs) of \(P_{i}\) and \(Q_{i}\) as \(p_{i}\) and \(q_{i}\), respectively. Consider the mixture distributions \(P_{}\) and \(Q_{}\) with pdfs \(p_{}=_{i=1}^{m}w_{i}p_{i}\) and \(q_{}=_{i=1}^{m}w_{i}q_{i}\), where the weight \(=(w_{1},,w_{m})\) is such that \(w_{i} 0\) and \(_{i=1}^{m}w_{i}=1\). The following lemma is to bound the trade-off function \(T(P_{},Q_{})\). Upon finalizing this paper, we noted that Lemma 3.1 and Proposition 3.2 appeared independently in another paper [42, Theorem 8], where they served different applications.

**Lemma 3.1** (Joint concavity of trade-off functions).: _For two mixture distributions \(P_{}\) and \(Q_{}\), it holds_

\[T(P_{},Q_{})((t,c))_{i=1}^{m}w_{ i}T(P_{i},Q_{i})(_{i}(t,c)),\]

_where \(_{i}(t,c)=_{X P_{i}}[}{p_{i}}(X)>t]+ c_{X P_{i}}[}{p_{i}}(X)=t]\) is the type I error for testing \(P_{i}\) v.s. \(Q_{i}\) using the likelihood ratio test and \((t,c)=_{i=1}^{m}w_{i}_{i}(t,c)\)._

The main idea of the proof is to make the mixture distributions more distinguishable by releasing the indices. Precisely, for \(X P_{}\) and \(Y Q_{}\), let \(X|I\) be a random variable such that \(X|I=i P_{i}\) with \(I\) being the indices, i.e., \([I=i]=w_{i}\). Let \((X|I,I)\) be a random variable where we observe both \(X|I\) and the indices \(I\). Then, the right hand side of Lemma 3.1 is the trade-off function \(T((X|I,I),(Y|I,I))\) between two joint distributions. This is a lower bound for the trade-off function between mixture distributions because \((X|I,I) X\) is a data-independent post-processing procedure that only removes the observation of indices \(I\), and DP is immune to post-processing [14; 19].

Under the setting of \(f\)-DP, we usually require that the trade-off function is symmetric. The symmetry of the trade-off function in Lemma 3.1 is guaranteed by the following proposition.

**Proposition 3.2**.: _Suppose that for each \(i\), \(T(P_{i},Q_{i})\) is a symmetric trade-off function. Then the trade-off function \(T((X|I,I),(Y|I,I))\) is symmetric._

The joint convexity of \(F\)-divergences plays an important role in the analysis of divergence-based DP for mixture mechanisms [4; 22]. We now show that Lemma 3.1 is an extension of the joint convexity of \(F\)-divergences, including the scaled exponentiation of the Renyi divergence and the hockey-stick divergence, to trade-off functions. A trade-off function is always convex and is thus differentiable almost everywhere. Thus, without loss of generality, we consider \(f_{i}\) that is differentiable, symmetric, with \(f_{i}(0)=1\).

**Proposition 3.3** (An application of Lemma 3.1 to the \(F\)-divergences).: _Let \(D_{F}(P\|Q)= F(p(x)/q(x))q(x)dx\) be an \(F\)-divergence between any two distributions \(P\) and \(Q\) with some convex \(F\). Then, for \(f_{i}=T(P_{i},Q_{i})\), we have_

\[D_{F}(P_{}\|Q_{})_{i=1}^{m}_{0} ^{1}F(^{}(x)|})|f_{i}^{}(x)|\,dx=_{ i=1}^{m}w_{i}D_{F}(P_{i}\|Q_{i}).\]

Conversion from a trade-off function to \(F\)-divergences is straightforward using Section B in . However, conversion from an \(F\)-divergence to a trade-off function is highly non-trivial. In fact, \(F\)-divergence is an integral of a functional of the trade-off function over the whole space while Lemma 3.1 holds pointwisely, which is a local property. This explains why the divergence-based DP is not as informative as \(f\)-DP since some information is lost due to the integration.

## 4 Privacy analysis of the shuffled mechanisms

In this section, we explore the \(f\)-DP analysis of shuffled mechanisms. Drawing upon [22; 23], the shuffling procedure incorporates a mixture of binomial noise. This noise can be tightly bounded using our \(f\)-DP inequality for mixture distributions.

### Theoretical privacy guarantee

In shuffling models, the record of each user is privatized by some local randomizer (such as a randomized response mechanism ) and all records are then shuffled by a curator. Mathematically, consider a dataset \(=\{z_{i}\}_{i=1}^{n}\) of size \(n\) and each data point \(z_{i}\) is privatized by an local randomizer \(_{0}::}\) that satisfies \(_{0}\)-DP. Then, the mechanism \(:^{n}_{}}^{n}\) that maps \(\) to \(}=\{_{0}(z_{i})\}_{i=1}^{n}\) is \(_{0}\)-DP. A shuffler \(_{}\) takes the privatized dataset \(}\) as input and applies a uniformly random permutation to \(}\), which introduces the mixture of binomial noise to \(\) and results in privacy amplification.

As noted in , the shuffling procedure introduces mixtures of binomial distributions. More specifically, the outputs generated by the shuffled mechanism for two neighboring datasets result from post-processing random variables \(X P\) and \(Y Q\) with \(P=(1-w)P_{0}+wQ_{0}\) and \(Q=(1-w)Q_{0}+wP_{0}\), where the weight \(w=}+1}\), and the distributions \(P_{0}\) and \(Q_{0}\) are defined as \((A+1,C-A) P_{0}\), and \((A,C-A+1) Q_{0}\) with \(A(C,1/2)\) and \(C(n-1,2/(e^{_{0}}+1))\). It is easy to see that \(P_{0}\) is the mixture of \(\{(A_{i}+1,i-A_{i})\}_{i=0}^{n-1}\) with weights \(w_{i}^{0}:=[C=i]\) and \(Q_{0}\) is the mixture of \(\{(A_{i},i-A_{i}+1)\}_{i=0}^{n-1}\) with the same weights. In this context, \((k,p)\) is a binomial distribution with parameters \(k\) and \(p\) and each \(A_{i}\) is distributed as \((i,1/2)\). Advancing our analysis, we adopt the joint concavity, as outlined in Lemma 3.1, to establish a lossless bound for the trade-off function \(T(P_{0},Q_{0})\).

**Proposition 4.1**.: _Let \(F_{i}\) be the distribution function of \((i,1/2)\) and let \(w_{i}^{0}=[C=i]\) for \(C(n-1,2/(e^{_{0}}+1))\). Then, we have \(T(P_{0},Q_{0})\) is a piecewise linear function with_

\[T(P_{0},Q_{0})((t))=_{i=0}^{n-1}w_{i}^{0}\{1-F_{i}[F_{i}^{ -1}(_{i}(t))+1]\},\]

_for each knot \((t)=_{i=0}^{n-1}w_{i}^{0}_{i}(t):=_{i=0}^{n-1}w_{i}^{0}F_ {i}(i-).\)_

**Remark**.: Proposition 4.1 holds with equality and the bound for \(T(P_{0},Q_{0})\) is sharp.

Before stating our results for \(T(P,Q)\), we define some notations related to \(f\)-DP. For a function \(g:\), let \(g^{*}(y):=_{x}\{xy-g(x)\}\) be its convex conjugate. For a trade-off function \(f\), let \((f)=\{f,f^{-1}\}^{**}\) be its symmetrization, where \(f^{-1}\) is the left inverse function of \(f\), i.e., \(f^{-1} f(x)=x\).

**Theorem 4.2**.: _The shuffled mechanism \(_{}\) is \((f_{})\)-DP. Here \(f_{}((t))\) is a piecewise linear function where each knot \((t)\) has the form_

\[(t)=_{i=0}^{n-1}w_{i}^{0}_{i}(t):=_{i=0}^{n-1}w_{i}^{0}F_{ i}(i-),t 0,\]

_with \(F_{i}\) being the distribution function of \((i,1/2)\) and \(w_{i}^{0}=[C=i]\) for \(C(n-1,2/(e^{_{0}}+1))\), and the value of \(f_{}\) at a knot \((t)\) is_

\[f_{}((t))=2w((t))+(1-2w)[ _{i=0}^{n-1}w_{i}^{0}\{1-F_{i}[F_{i}^{-1}(_{i}(t))+1] \}],\]

_with \(w=}}\) and \((x)=1-x\) being the identity trade-off function._

Remark.The bound in Theorem 4.2 is near-optimal. In fact, the proof of Theorem 4.2 is based on a post-processing procedure in , joint concavity (Proposition 4.1), and advanced joint concavity (Proposition 6.4). The post-processing procedure is sharp for specific mechanisms, such as the randomized response mechanism, as shown by Theorem 5.2 and Theorem 5.3 in . Proposition 4.1 holds with equality and is optimal. The advanced joint concavity, which is an \(f\)-DP analog of the advanced joint convexity in , is optimal for specific distributions. Compared to existing analysis of shuffled mechanisms (e.g., ), the main advantage of using \(f\)-DP is that we avoid the use of Hoeffding's inequality and the Chernoff bound to bound the distance between \(P_{0}\) and \(Q_{0}\) in Proposition 4.1, which is adopted in  and leads to loose bounds, to bound the mixture of binomial distributions. Moreover, Theorem 3.2 in  holds with an assumption \(_{0}(-1)\), which is removed by using \(f\)-DP in our paper.

To convert \(f\)-DP to \((,)\)-DP, we use the primal-dual perspective in  and obtain the following Corollary.

**Corollary 4.3**.: _Let \(l(t):=-^{n-1}w_{i}^{0}p_{i}( i+1- )}{_{i=0}^{n-1}w_{i}^{0}p_{i}(  i-)}\) with \(p_{i}\) being the probability mass function of \((i,1/2)\). Then, we have \(_{}\) is \((,_{f}())\)-DP for any \(>0\) with_

\[_{f}()=(-e^{}+2w)[_{i=0}^{n-1}w_{i}^{0 }F_{i}(i-+1})]+(1-2w)[_{i=0}^{ n-1}w_{i}^{0}F_{i}(i+1-+1})],\]

_where \(t_{}=\{t:-2w+(1-2w)l(t)-e^{}\}\) and \(w=}+1}\)._

### Numerical results and comparisons

To the best of our understanding, the leading privacy analysis for shuffled mechanisms is given in . In this section, we compare the privacy bounds from our Theorem 4.2 and Corollary 4.3 with those found in Theorem 3.2 of . Additionally, we assess the tightness of our bound against the empirical lower bounds obtained through binary search.

Specifically, Figure 1 presents a comparison of the trade-off function derived from our Theorem 4.2 to that of . This comparison clearly illustrates that \(f\)-DP offers tighter privacy bounds, given that its trade-off function aligns closer to the identity trade-off function.

In our Table 1, we compare the values of \(_{f}()\), as derived from Corollary 4.3 with \(()\) in . The results indicate that \(_{f}()\) is significantly smaller than \(()\).

In Table 2, we present \(_{f}\) alongside the numerical upper bound of \(\) from  and the numerical lower bound determined by binary search. Given its closeness to the lower bound, our Theorem 4.2 can be considered near-optimal.

We compare \(_{f}\) obtained in Corollary 4.3 with the corresponding numerical upper bound \(\) derived from  using a fixed value of \(_{0}=4.444\) and \(n=10000\). For \(<10^{-7}\), the bound in  fails as the assumption \(_{0}(-1)\) is violated while our theory removes this assumption and holds for all \(_{0}\). Moreover, we compare our theoretical upper bound with the empirical lower bound obtained by binary search in  which shows that our bound is near-optimal.

   \(\) & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 & 1.0 \\  \(\) in  & \(0.9494\) & \(0.3764\) & \(0.1038\) & \(0.0181\) & \(0.0018\) & \(8 10^{-5}\) \\  \(_{f}\) (ours) & \(3 10^{-6}\) & \(10^{-7}\) & \(4 10^{-9}\) & \(9 10^{-11}\) & \(2 10^{-12}\) & \(2 10^{-14}\) \\   

Table 1: Comparisons with 

   \(\) & \(5 10^{-5}\) & \(3 10^{-6}\) & \(10^{-7}\) & \(4 10^{-9}\) & \(9 10^{-11}\) \\  \(_{f}\) (ours) & \(0.4\) & 0.5 & 0.6 & 0.7 & 0.8 \\  Numerical \(\) upper bound in  & \(1.014\) & \(1.085\) & \(_{0}\) & \(_{0}\) & \(_{0}\) \\  Numerical \(\) lower bound & \(0.369\) & \(0.470\) & \(0.575\) & \(0.664\) & \(0.758\) \\   

Table 2: Comparisons with numerical results in In summary, our non-asymptotic privacy bound for shuffled mechanisms outperforms Theorem 3.2 in . This improvement is a result of our Proposition 4.1, which optimally refines Lemma A.4 in . Besides Proposition 4.1, the remainder of our proof of Theorem 4.2 closely adheres to the methodology presented in . Our near-optimal result is complicated due to its tightness. Thus, it is difficult to compare our result with the asymptotic bound in  analytically.

## 5 Privacy analysis of one-iteration DP-GD with random initialization

A significant challenge in the privacy analysis of the last-iteration model of DP-SGD lies in accounting for multiple randomization techniques used during iterations. This includes aspects like initialization, iterative steps, and sub-sampling. Since these techniques incorporate a mixture of random noise, the joint convexity of \(F\)-divergence becomes crucial in the privacy analysis of DP-SGD . Our Lemma 3.1, which provides a unified perspective on these convexity notations, has driven us to include it in the privacy analysis of DP-GD. Nevertheless, analyzing the trajectories from multi-step iterations remains complex. Therefore, our initial exploration is to investigate the effects of random initialization on a one-step iterate. It's noteworthy that in machine learning, training a deep neural network using (stochastic) gradient descent combined with random initialization is widely adopted . The significance of random initialization in noisy gradient descent is also emphasized by  within the framework of Kullback-Leibler privacy.

Consider a dataset \(=\{(x_{i},y_{i})\}_{i=1}^{n}\) with \(x_{i}\) being the features and \(y_{i}\) being the labels. Let \((,)\) be a loss function and let \(g(,)\) be the gradient of \(\) with respect to \(\). The output of one-step iteration of DP-GD initialized at \(_{0}\) with step-size 1 is given by

\[()=_{0}-(g(_{0},)+(0, ^{2})).\] (1)

In the setting of random initialization, \(_{0}\) is chosen as a Gaussian random variable. Without loss of generality, we consider \(_{0}=I(0,1)\) and rewrite \(()=s_{I}()+(0,^{2})\) with \(s_{I}()=I-g(I,)\). \(()\) is a Gaussian random variable when the initialization \(I\) is given, that is, \(()|I=i(s_{i}(),^{2})\). Thus, we can regard \(()\) as an infinite mixture of Gaussian distributions with continuous Gaussian weights \(\{(i)\}_{i}\), where \(\) is the pdf of \(I\) and the corresponding trade-off function \(T((_{0}),(_{1}))\) can be bounded using the joint concavity.

For simplicity, we define \(()|I\) as a random variable with a given initialization \(I\). For two neighboring datasets \(_{0}\) and \(_{1}\), it holds

\[T(((_{0})|I,I)\,,((_{1})|I,I))=T((X|I,I),(Y| I,I))\]

with \(X|I(0,1)\) and \(Y|I(_{I},1)\) for \(I(0,1)\), where \(_{I}=(g(I,_{1})-g(I,_{0}))/\).

**Theorem 5.1**.: _Let \((_{0})\) and \((_{1})\) be defined in (1) for neighboring datasets \(_{0}\) and \(_{1}\). Then, we have_

\[T((_{0}),(_{1}))((t))_{I} [(-t_{I}+_{I})_{[_{I} 0]}+(t_{I}-_{I}) _{[_{I}>0]}]\]

_with \(t_{I}=-}+}{2}\) and \((t)=_{I}[(t_{I})_{[_{I} 0]}+ (-t_{I})_{[_{I}>0]}].\) Here \(\) is the cumulative distribution function of \((0,1)\) and the expectation is taken with respect to \(I\)._

**Remark**.: Note that Theorem 5.1 is instance-based privacy guarantee as it relies on the datasets. To extend it to the worst case, we let \(_{I}^{}=_{_{0},_{1}}\{|g(I, _{1}^{})-g(I,_{0}^{})|/\}\) be the sensitivity of the gradient with a given initialization \(I\). As a result, \(()\) output by one-step DP-GD is \(f\)-DP with \(f((t))=_{I}[(t_{I}^{}-_{I}^{})],\) where \(t_{I}^{}=-^{}}+^{}}{2}\) and \((t)=_{I}[(-t_{I}^{})].\) The worst case trade-off function is bounded for strongly convex loss functions with a bounded data domain.

To numerically evaluate the trade-off function in Theorem 5.1, we consider an example \(_{0}=\{(x_{i},y_{i})\}_{i=1}^{n}\) with \(y_{i}=ax_{i}\) and \(x_{i}^{2}=1\) for some constant \(a\) and we defined \(_{1}\) by removing an arbitrary element in \(_{0}\). Moreover, we assume that \(=1\). Note that for this example without gradient clipping, the gradient is linear in \(I\) and \((_{0})\) is the sum of two Gaussian random variables which is Gaussian. Thus, the trade-off function has a closed-form representation. In general, the output is non-Gaussian and we should adopt Theorem 5.1. For example, if we consider gradient clipping  and replace \(g(,)\) by the clipped gradient

\[g_{c}(,)=_{i=1}^{n}()}{\{1,\|g^{(i) }()\|_{2}/c\}},g^{(i)}()=(y_{i}- x_{i})(-x_{i}),\]where the gradient of each data point \(g^{(i)}\) is cut off by some constant \(c>0\), then \(_{I}^{}\) is given by

\[_{I}^{}=\{a-I,&|a-I| c,\\ c,&a-I c,\\ -c,&a-I-c,.\]

which is not Gaussian. In this example \(g_{c}(,)+(0,1)\) is considered as \(c\)-GDP if we disregard the effects of random initialization since the sensitivity of \(g_{c}\) is \(c\).

We illustrate the trade-off function of Theorem 5.1 computed numerically in Figure 2, where we also compare it with \(c\)-GDP for \(a=1\) and varying values of \(c\). Overall, the figure suggests that random initialization can amplify the privacy of DP-GD, as our bounds outperform those of \(c\)-GDP, which does not take into account the randomness of initialization. Furthermore, we observe that as \(c\) increases, the amplification effect caused by random initialization becomes more significant, since the difference between \(T((X|I,I),(Y|I,I))\) and \(c\)-GDP also increases. This is reasonable, since the randomness resulting from initialization comes from \(I\) such that \(|a-I| c\), whereas for \(|a-I|>c\), \(_{I}\) remains constant and no randomness is introduced. Thus, the random initialization introduces greater levels of randomness as \(c\) increases.

It is worth noting that in this example, without gradient clipping, we have \(_{I}^{}=a-I\) and the dominate pair are two Gaussian distributions \((0,1)\) and \((0,2)\). The Renyi DP fails to measure the privacy of initialization. In fact, it holds \(R_{}((0,1)\|(0,2))=\) for \(\) large enough.

## 6 Optimality of joint concavity and advanced joint concavity

In this section, we first explore the sufficient and necessary conditions under which Lemma 3.1 holds with equality. While Lemma 3.1 is generally not sharp, we introduce an \(f\)-DP analog of the advanced joint convexity of the hockey-stick divergence from , yielding tighter bounds in certain applications.

Recall the distributions \(P=(1-w)P_{0}+wQ_{0}\) and \(Q=(1-w)Q_{0}+wP_{0}\) that appear in the shuffled mechanisms. Bounding the trade-off function \(T(P,Q)\) directly using the joint concavity leads to a loose bound (cf., Figure (b)b). For the scenarios where Lemma 3.1 is not tight, we introduce the \(f\)-DP analog of the advanced joint convexity of \((,)\)-DP  that may lead to tighter bounds and we term it the "advanced joint concavity of trade-off functions".

The following proposition presents a necessary and sufficient condition for Lemma 3.1 to hold with equality.

**Proposition 6.1**.: _For \(m=2\), Lemma 3.1 holds with equality if and only if \(p_{1}+w_{2}p_{2}}{w_{1}q_{1}+w_{2}q_{2}}(X)}}{{=}}w_{1}}{q_{1}}(X)+w_{2}}{q_{2}}(X)\) with \(X P_{}\), where for \(p_{i}(X)/q_{i}(X)=0/0\) and \(p_{j}(X)/q_{j}(X) 0/0\) with \(i j\), we set \(p_{i}(X)/q_{i}(X)=p_{j}(X)/q_{j}(X)\)._

It is not difficult to see that \(P_{0}\) and \(Q_{0}\) in shuffling models satisfy this necessary and sufficient condition when \(n=2\).

As we discussed, Lemma 3.1 may not be sharp in general. The following lemma is about the advanced joint convexity of the hockey-stick divergence, which is a slight generalization of Theorem 2 in .

Figure 2: Trade-off functions for linear models with \(a=1\).

**Lemma 6.2**.: _For any non-negative \(^{},_{0},_{1},\), and \(\) satisfying \((^{})=(1-w)(_{0})+w(_{1})\) and \((_{0})(1-w)+(_{1})w=(^{})w,\) we have_

\[& H_{e^{c^{}}}((1-w)P_{1}+wP_{2}\|(1-w)Q_{ 1}+wQ_{2})\\ &\ (1-w)H_{e^{c_{0}}}(P_{1}\|(1-)Q_{1}+ Q_{2} )+wH_{e^{*_{1}}}(P_{2}\|(1-)Q_{1}+ Q_{2}).\] (2)

Lemma 6.2 is reduced to the advanced joint convexity of the hockey-stick divergence in  when \(P_{1}=Q_{1},\) by minimizing the right-hand-side of (2) with respect to \(,,_{0},\) and \(_{1}.\)

Recall the convex conjugate \(g^{*}\) of a function \(g\) defined by \(g^{*}(y)=_{x}\{xy-g(x)\}\) and \((f)=\{f,f^{-1}\}^{**}\) which is the symmetrization of \(f\). We have the following advanced joint concavity of trade-off functions.

**Lemma 6.3** (Advanced joint concavity).: _Suppose that \(T(P_{i},Q_{i})\) is symmetric for each \(i\). Then, for \(0 w 1\), we have_

\[T((1-w) P_{1}+wP_{2},(1-w)Q_{1}+wQ_{2})\] \[(1-w)(1-)F_{1,1}^{*}+w(1- )F_{2,1}^{*}+(1-w) F_{1,2}^{*}+w F_{2,2}^{*}^{*}\]

_for arbitrary \(0<w< 1\), where \(F_{i,j}(x)\) is given by \(F_{1,i}(x):=f_{1,i}(),\) and \(F_{2,i}(x):=f_{2,i}(),\) and the trade-off functions are defined as\(f_{i,j}=T(P_{i},Q_{j})\) for \(1 i,j 2\). Moreover, for \(==w\), it holds_

\[T((1-w)P_{1}+wP_{2}, (1-w)Q_{1}+wQ_{2})\] \[((1-w)T(P_{1},(1-w)Q_{1}+wQ_{2})+ wT(P_{2},(1-w)Q_{1}+wQ_{2})).\]

Determining the trade-off functions using advanced joint concavity can be challenging in many practical situations. In fact, to apply the advanced joint concavity, one need to specify the choice of \(,\) by maximizing the right-hand-side of Lemma 6.3. Therefore, in real-world applications, we often rely on both joint concavity and advanced joint concavity.

For \(P=(1-w)P_{0}+wQ_{0}\) and \(Q=(1-w)Q_{0}+wP_{0}\) in shuffling models, we have the following bound derived from Lemma 6.3.

**Proposition 6.4**.: _For \(P=(1-w)P_{0}+wQ_{0}\) and \(Q=(1-w)Q_{0}+wP_{0}\) with some weight \(0 w 1/2\), we have \(T(P,Q)(2w+(1-2w)T(P_{0},Q_{0})).\)_

The equality in Proposition 6.4 does not hold exactly. However, this lower bound is almost the tightest closed-form expression. One may refer to Section E.1.1 in the appendix for the proof details.

Figure 3: Comparison between joint concavity (Lemma 3.1) and advanced joint concavity (Lemma 6.3).

## 7 Discussion

This paper provides refined privacy bounds for mixture mechanisms, including shuffling models and DP-GD with random initialization. For shuffling models, we present a bound that is tighter than existing results based on \((,)\)-DP. In the study of DP-GD, we demonstrate how random initialization can amplify privacy concerns. These bounds are derived using a unified \(f\)-DP approach based on the joint concavity and advanced joint concavity of trade-off functions. We also investigate the sharpness and other properties of these concavity notions.

In our future work, we plan to extend our analysis from one-step DP-GD to multi-step DP-SGD. For DP-SGD with multiple iterations, it is crucial to consider subsampling and privacy amplification by iteration in the privacy accountant, in addition to the randomness introduced by shuffling and random initialization. While there is an \(f\)-DP bound for subsampling provided in an independent work , as far as we know, there is limited research on \(f\)-DP results regarding privacy amplification by iteration.

Beyond DP-SGD, we intend to extend our theory to the privacy analysis of other key applications that involves various randomization techniques. These include the shuffled Gaussian mechanism for federated learning, as discussed in , and the composition of mixture mechanisms. For extending our theory to federated learning, we might adopt the \(f\)-DP framework outlined in . Addressing the composition of mixture mechanisms demands examination of the tensor product between trade-off functions. This is a complex task, even when dealing with the simplest mixture mechanisms like sub-sampling, as highlighted in .