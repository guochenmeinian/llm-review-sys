# General Munchausen Reinforcement Learning with Tsallis Kullback-Leibler Divergence

Lingwei Zhu

University of Alberta

lingwei4@ualberta.ca &Zheng Chen

Osaka University

chenz@sanken.osaka-u.ac.jp &Matthew Schlegel

University of Alberta

mkschleg@ualberta.ca &Martha White

University of Alberta

CIFAR Canada AI Chair, Amii

whitem@ualberta.ca

###### Abstract

Many policy optimization approaches in reinforcement learning incorporate a Kullback-Leibler (KL) divergence to the previous policy, to prevent the policy from changing too quickly. This idea was initially proposed in a seminal paper on Conservative Policy Iteration, with approximations given by algorithms like TRPO and Munchausen Value Iteration (MVI). We continue this line of work by investigating a generalized KL divergence--called the Tsallis KL divergence. Tsallis KL defined by the \(q\)-logarithm is a strict generalization, as \(q=1\) corresponds to the standard KL divergence; \(q>1\) provides a range of new options. We characterize the types of policies learned under the Tsallis KL, and motivate when \(q>1\) could be beneficial. To obtain a practical algorithm that incorporates Tsallis KL regularization, we extend MVI, which is one of the simplest approaches to incorporate KL regularization. We show that this generalized MVI(\(q\)) obtains significant improvements over the standard MVI(\(q=1\)) across 35 Atari games.

## 1 Introduction

There is ample theoretical evidence that it is useful to incorporate KL regularization into policy optimization in reinforcement learning. The most basic approach is to regularize towards a uniform policy, resulting in entropy regularization. More effective, however, is to regularize towards the previous policy. By choosing KL regularization between consecutively updated policies, the optimal policy becomes a softmax over a uniform average of the full history of action value estimates (Vieillard et al., 2020). This averaging smooths out noise, allowing for better theoretical results (Azar et al., 2012; Kozuno et al., 2019; Vieillard et al., 2020; Kozuno et al., 2022).

Despite these theoretical benefits, there are some issues with using KL regularization in practice. It is well-known that the uniform average is susceptible to outliers; this issue is inherent to KL divergence (Futami et al., 2018). In practice, heuristics such as assigning vanishing regularization coefficients to some estimates have been implemented widely to increase robustness and accelerate learning (Grau-Moya et al., 2019; Haarnoja et al., 2018; Kitamura et al., 2021). However, theoretical guarantees no longer hold for those heuristics (Vieillard et al., 2020; Kozuno et al., 2022). A natural question is what alternatives we can consider to this KL divergence regularization, that allows us to overcome some of these disadvantages while maintaining the benefits associate with restricting aggressive policy changes and smoothing errors.

In this work, we explore one possible direction by generalizing to Tsallis KL divergences. Tsallis KL divergences were introduced for physics (Tsallis, 1988, 2009) using a simple idea: replacing the use of the logarithm with the deformed \(q\)-logarithm. The implications for policy optimization, however, are that we get quite a different form for the resulting policy. Tsallis _entropy_ with \(q=2\) has actually already been considered for policy optimization (Chow et al., 2018; Lee et al., 2018), by replacing Shannon entropy with Tsallis entropy to maintain stochasticity in the policy. The resulting policies are called _sparsemax_ policies, because they concentrate the probability on higher-valued actions and truncate the probability to zero for lower-valued actions. Intuitively, this should have the benefit of maintaining stochasticity, but only amongst the most promising actions, unlike the Boltzmann policy which maintains nonzero probability on all actions. Unfortunately, using only Tsallis entropy did not provide significant benefits, and in fact often performed worse than existing methods. We find, however, that using a Tsallis KL divergence to the previous policy does provide notable gains.

We first show how to incorporate Tsallis KL regularization into the standard value iteration updates, and prove that we maintain convergence under this generalization from KL regularization to Tsallis KL regularization. We then characterize the types of policies learned under Tsallis KL, highlighting that there is now a more complex relationship to past action-values than a simple uniform average. We then show how to extend Munchausen Value Iteration (MVI) (Vieillard et al., 2020), to use Tsallis KL regularization, which we call MVI(\(q\)). We use this naming convention to highlight that this is a strict generalization of MVI: by setting \(q=1\), we exactly recover MVI. We then compare MVI(\(q=2\)) with MVI (namely the standard choice where \(q=1\)), and find that we obtain significant performance improvements in Atari.

**Remark:** There is a growing body of literature studying generalizations of KL divergence in RL (Nachum et al., 2019; Zhang et al., 2020). Futami et al. (2018) discussed the inherent drawback of KL divergence in generative modeling and proposed to use \(\)- and \(\)-divergence to allow for weighted average of sample contribution. These divergences fall under the category known as the \(f\)-divergence (Sason and Verdu, 2016), commonly used in other machine learning domains including generative modeling (Nowozin et al., 2016; Wan et al., 2020; Yu et al., 2020) and imitation learning (Ghasemipour et al., 2019; Ke et al., 2019). In RL, Wang et al. (2018) discussed using tail adaptive \(f\)-divergence to enforce the mass-covering property. Belousov and Peters (2019) discussed the use of \(\)-divergence. Tsallis KL divergence, however, has not yet been studied in RL.

## 2 Problem Setting

We focus on discrete-time discounted Markov Decision Processes (MDPs) expressed by the tuple \((,,d,P,r,)\), where \(\) and \(\) denote state space and finite action space, respectively. Let \(()\) denote the set of probability distributions over \(\). \(d()\) denotes the initial state distribution. \(P:()\) denotes the transition probability function, and \(r(s,a)\) defines the reward associated with that transition. \((0,1)\) is the discount factor. A policy \(:()\) is a mapping from the state space to distributions over actions. We define the action value function following policy \(\) and starting from \(s_{0} d()\) with action \(a_{0}\) taken as \(Q_{}(s,a)=_{}[_{t=0}^{}^{t}r_{t}|s_{0}=s,a_ {0}=a]\). A standard approach to find the optimal value function \(Q_{*}\) is value iteration. To define the formulas for value iteration, it will be convenient to write the action value function as a matrix \(Q_{}^{||||}\). For notational convenience, we define the inner product for any two functions \(F_{1},F_{2}^{||||}\) over actions as \( F_{1},F_{2}^{||}\).

We are interested in the entropy-regularized MDPs where the recursion is augmented with \(()\):

\[_{k+1}=_{}(,Q_{k}-() )\,,\\ Q_{k+1}=r+ P(_{k+1},Q_{k}-(_{k+1}))\] (1)

This modified recursion is guaranteed to converge if \(\) is concave in \(\). For standard (Shannon) entropy regularization, we use \(()=-()=,\). The resulting optimal policy has \(_{k+1}(^{-1}Q_{k})\), where \(\) indicates _proportional to_ up to a constant not depending on actions.

More generally, we can consider a broad class of regularizers known as \(f\)-divergences (Sason and Verdu, 2016): \(()=D_{f}(||):=,f(/)\), where \(f\) is a convex function. For example, the KL divergence \(D_{}(\,|\,)=,-\) can be recovered by \(f(t)=- t\). In this work, when we say KL regularization, we mean the standard choice of setting \(=_{k}\), the estimate from the previous update. Therefore, \(D_{}\) serves as a penalty to penalize aggressive policy changes. The optimal policy in this case takes the form \(_{k+1}_{k}(^{-1}Q_{k})\). By induction, we can show this KL-regularized optimal policy \(_{k+1}\) is a softmax over a uniform average over the history of action value estimates (Vieillard et al., 2020): \(_{k+1}_{k}(^{-1}Q_{k}) (^{-1}_{j=1}^{k}Q_{j})\). Using KL regularization has been shown to be theoretically superior to entropy regularization in terms of error tolerance (Azar et al., 2012; Vieillard et al., 2020; Kozuno et al., 2022; Chan et al., 2022).

The definitions of \(()\) and \(D_{}(||)\) rely on the standard logarithm and both induce softmax policies as an exponential (inverse function) over (weighted) action-values (Hiriart-Urruty and Lemarechal, 2004; Nachum and Dai, 2020). Convergence properties of the resulting regularized algorithms have been well studied (Kozuno et al., 2019; Geist et al., 2019; Vieillard et al., 2020). In this paper, we investigate Tsallis entropy and Tsallis KL divergence as the regularizer, which generalize Shannon entropy and KL divergence respectively.

## 3 Generalizing to Tsallis Regularization

We can easily incorporate other regularizers in to the value iteration recursion, and maintain convergence as long as those regularizers are strongly convex in \(\). We characterize the types of policies that arise from using this regularizer, and prove the convergence of resulting regularized recursion.

### Tsallis Entropy Regularization

Tsallis entropy was first proposed by Tsallis (1988) and is defined by the \(q\)-logarithm. The \(q\)-logarithm and its unique inverse function, the \(q\)-exponential, are defined as:

\[_{q}x:=-1}{1-q},_{q}x:=[1+(1-q)x]_{+}^{ },q\{1\}\] (2)

where \([]_{+}:=\{,0\}\). We define \(_{1}=_{1}=\), as in the limit \(q 1\), the formulas in Eq. (2) approach these functions. Tsallis entropy can be defined by \(S_{q}():=p-^{q},_{q},p\)(Suyari and Tsukada, 2005). We visualize the \(q\)-logarithm, \(q\)-exponential and Tsallis entropy for different \(q\) in Figure 1. As \(q\) gets larger, \(q\)-logarithm (and hence Tsallis entropy) becomes more flat and \(q\)-exponential more steep1. Note that \(_{q}\) is only invertible for \(x>\).

Tsallis policies have a similar form to softmax, but using the \(q\)-exponential instead. Let us provide some intuition for these policies. When \(p=,q=2\), \(S_{2}()=,1-\), the optimization problem \(_{(A)},Q+S_{2}()=_{ (A)}|-Q|_{2}^{2}\) is known to be the Euclidean projection onto the probability simplex. Its solution \([Q-]_{+}\) is called the sparsemax (Martins and Astudillo, 2016; Lee et al., 2018) and has sparse support (Duchi et al., 2008; Condat, 2016; Blondel et al., 2020). \(:\) is the unique function satisfying \(,[Q-]_{+}=1\).

As our first result, we unify the Tsallis entropy regularized policies for all \(q_{+}\) with the \(q\)-exponential, and show that \(q\) and \(\) are interchangeable for controlling the truncation.

Figure 1: \(_{q}x\), \(_{q}x\) and Tsallis entropy component \(-^{q}_{q}\) for \(q=1\) to \(5\). When \(q=1\) they respectively recover their standard counterpart. \(\) is chosen to be Gaussian \((2,1)\). As \(q\) gets larger \(_{q}x\) (and hence Tsallis entropy) becomes more flat and \(_{q}x\) more steep.

**Theorem 1**.: _Let \(()=-S_{q}()\) in Eq. (1). Then the regularized optimal policies can be expressed:_

\[(a|s)=[1-q]{[-_{q}()]_{+}(1-q)}=_{q}(-_{q }())\] (3)

_where \(_{q}=_{q}+\). Additionally, for an arbitrary \((q,)\) pair with \(q>1\), the same truncation effect (support) can be achieved using \((q=2,)\)._

Proof.: See Appendix B for the full proof. 

Theorem 1 characterizes the role played by \(q\): controlling the degree of truncation. We show the truncation effect when \(q=2\) and \(q=50\) in Figure 2, confirming that Tsallis policies tend to truncate more as \(q\) gets larger. The theorem also highlights that we can set \(q=2\) and still get more or less truncation using different \(\), helping to explain why in our experiments \(q=2\) is a generally effective choice.

Unfortunately, the threshold \(_{q}\) (and \(_{q}\)) does not have a closed-form solution for \(q 1,2,\). Note that \(q=1\) corresponds to Shannon entropy and \(q=\) to no regularization. However, we can resort to Taylor's expansion to obtain _approximate sparsemax policies_.

**Theorem 2**.: _For \(q 1,\), we can obtain approximate threshold \(_{q}_{q}\) using Taylor's expansion, and therefore an approximate policy:_

\[(a|s)_{q}(-_{q}( )),\ _{q}() -1}{|K(s)|}+1.\] (4)

\(K(s)\) _is the set of highest-valued actions, satisfying the relation \(1+i)}{}>_{j=1}^{i})}{}\), where \(a_{(j)}\) indicates the action with \(j\)th largest action value. The sparsemax policy sets the probabilities of lowest-valued actions to zero: \((a_{(i)}|s)=0,i=z+1,,||\) where \(Q(s,a_{(z)})>^{-1}_{q}()>Q(s,a _{(z+1)})\). When \(q=2\), \(_{q}\) recovers \(_{q}\)._

Proof.: See Appendix B for the full proof. 

Lee et al. (2020) also used \(_{q}\) to represent policies but they consider the continuous action setting and do not give any computable threshold. By contrast, Theorem 2 presents an easily computable \(_{q}\) for all \(q\{1,\}\).

### Tsallis KL Regularization and Convergence Results

The Tsallis KL divergence is defined as \(D^{q}_{KL}(\,\|\,):=,-_{q}\)(Furuichi et al., 2004). It is a member of \(f\)-divergence and can be recovered by choosing \(f(t)=-_{q}t\). As a divergence penalty,

Figure 2: (Left) Tsallis KL component \(-_{1}_{q}}{_{1}}\) between two Gaussian policies \(_{1}=(2.75,1),_{2}=(3.25,1)\) for \(q=1\) to \(5\). When \(q=1\) TKL recovers KL. For \(q>1\), TKL is more mode-covering than KL. (Mid) The sparsemax operator acting on a Boltzmann policy when \(q=2\). (Right) The sparsemax when \(q=50\). Truncation gets stronger as \(q\) gets larger. The same effect can be also controlled by \(\).

it is required that \(q>0\) since \(f(t)\) should be convex. We further assume that \(q>1\) to align with standard divergences; i.e. penalize large value of \(\), since for \(0<q<1\) the regularization would penalize \(\) instead. In practice, we find that \(0<q<1\) tend to perform poorly. In contrast to KL, Tsallis KL is more _mass-covering_; i.e. its value is proportional to the \(q\)-th power of the ratio \(\). When \(q\) is big, large values of \(\) are strongly penalized (Wang et al., 2018). This behavior of Tsallis KL divergence can also be found in other well-known divergences: the \(\)-divergence (Wang et al., 2018; Belousov and Peters, 2019) coincides with Tsallis KL when \(=2\); Renyi's divergence also penalizes large policy ratio by raising it to the power \(q\), but inside the logarithm, which is therefore an additive extension of KL (Li and Turner, 2016). In the limit of \(q 1\), Tsallis entropy recovers Shannon entropy and the Tsallis KL divergence recovers the KL divergence. We plot the Tsallis KL divergence behavior in Figure 2.

Now let us turn to formalizing when value iteration under Tsallis regularization converges. The \(q\)-logarithm has the following properties: _Convexity:_\(_{q}\) is convex for \(q 0\), concave for \(q>0\). When \(q=0\), both \(_{q},_{q}\) become linear. _Monotonicity:_\(_{q}\) is monotonically increasing with respect to \(\). These two properties can be simply verified by checking the first and second order derivative. We prove in Appendix A the following similarity between Shannon entropy (reps. KL) and Tsallis entropy (resp. Tsallis KL). _Bounded entropy_: we have \(0()||\); and \( q,\,0 S_{q}()_{q}||\). _Generalized KL property_: \( q\), \(D^{q}_{KL}(\|\, 0\). \(D^{q}_{KL}(\|\,=0\) if and only if \(=\) almost everywhere, and \(D^{q}_{KL}(\|\,\) whenever \((a|s)>0\) and \((a|s)=0\).

However, despite their similarity, a crucial difference is that \(_{q}\) is non-extensive, which means it is not additive (Tsallis, 1988). In fact, \(_{q}\) is only _pseudo-additive_:

\[_{q}=_{q}+_{q}+(1-q)_{q}_{q}.\] (5)

Pseudo-additivity complicates obtaining convergence results for Eq. (1) with \(q\)-logarithm regularizers, since the techniques used for Shannon entropy and KL divergence are generally not applicable to their \(_{q}\) counterparts. Moreover, deriving the optimal policy may be nontrivial. Convergence results have only been established for Tsallis entropy (Lee et al., 2018; Chow et al., 2018).

We know that Eq. (1) with \(()=D^{q}_{KL}(\|\,\), for any \(\), converges for \(q\) that make \(D^{q}_{KL}(\|\,)\) strictly convex (Geist et al., 2019). When \(q=2\), it is strongly convex, and so also strictly convex, guaranteeing convergence.

**Theorem 3**.: _The regularized recursion Eq. (1) with \(()=D^{q}_{KL}(\|\,\) when \(q=2\) converges to the unique regularized optimal policy._

Proof.: See Appendix C. It simply involves proving that this regularizer is strongly convex. 

### TKL Regularized Policies Do More Than Averaging

We next show that the optimal regularized policy under Tsallis KL regularization does more than uniform averaging. It can be seen as performing a weighted average where the degree of weighting is controlled by \(q\). Consider the recursion

\[_{k+1}=_{},Q_{k}-D^{q}_{KL}(\|\, _{k})\,,\\ Q_{k+1}=r+ P(_{k+1},Q_{k}-D^{q}_{KL}(_{k+1}||_{k}))\,,\] (6)

where we dropped the regularization coefficient \(\) for convenience.

**Theorem 4**.: _The greedy policy \(_{k+1}\) in Equation (6) satisfies_

\[_{k+1}(_{q}Q_{1}_{q}Q_{k})=[_{q} \!(_{j=1}^{k}Q_{j})\!\!+\!_{j=2}^{k}\!\!(q-1)^{j}\!\!\!\! _{i_{1}=1<<i_{j}}^{k}\!\!\!\!\!Q_{i_{1}} Q_{i_{j}}]^{ }.\] (7)

_When \(q=1\), Eq. (6) reduces to KL regularized recursion and hence Eq. (7) reduces to the KL-regularized policy. When \(q\!=\!2\), Eq. (7) becomes:_

\[_{2}Q_{1}_{2}Q_{k}\!=_{2}(_{j=1}^{k}Q_{j})\!\! +\!\!\!\!\!\!_{j=2\\ i_{1}=1<<i_{j}}^{k}\!\!\!\!Q_{i_{1}} Q_{i_{j}}.\]i.e., Tsallis KL regularized policies average over the history of value estimates as well as computing the interaction between them \(_{j=2}^{k}_{i_{1}<<i_{j}}^{k}Q_{i_{1}} Q_{i_{j}}\)._

Proof.: See Appendix D for the full proof. The proof comprises two parts: the first part shows \(_{k+1}_{q}Q_{1}_{q}Q_{k}\), and the second part establishes the _more-than-averaging_ property by two-point equation (Yamano, 2002) and the \(2-q\) duality (Naudts, 2002; Suyari and Tsukada, 2005) to conclude \((_{q}x_{q}y)^{q-1}=_{q}(x+y)^{q-1}+(q-1)^{2}xy\). 

The form of this policy is harder to intuit, but we can try to understand each component. The first component actually corresponds to a weighted averaging by the property of the \(_{q}\):

\[_{q}\!(_{i=1}^{k}Q_{i})\!\!=_{q}Q_{1}_{q}( }{1+(1-q)Q_{1}})_{q}\!(\!}{1+(1-q) _{i=1}^{k-1}Q_{i}}\!).\] (8)

Eq. (8) is a possible way to expand the summation: the left-hand side of the equation is what one might expect from conventional KL regularization; while the right-hand side shows a weighted scheme such that any estimate \(Q_{j}\) is weighted by the summation of estimates before \(Q_{j}\) times \(1-q\) (Note that we can exchange 1 and \(q\), see Appendix A). Weighting down numerator by the sum of components in the demoninator has been analyzed before in the literature of weighted average by robust divergences, e.g., the \(\)-divergence (Futami et al., 2018, Table 1). Therefore, we conjecture this functional form helps weighting down the magnitude of excessively large \(Q_{k}\), which can also be controlled by choosing \(q\). In fact, obtaining a weighted average has been an important topic in RL, where many proposed heuristics coincide with weighted averaging (Grau-Moya et al., 2019; Haarnoja et al., 2018; Kitamura et al., 2021).

Now let us consider the second term with \(q=2\), therefore the leading \((q-1)^{j}\) vanishes. The action-value cross-product term can be intuitively understood as further increasing the probability for any actions that have had consistently larger values across iterations. This observation agrees with the mode-covering property of Tsallis KL. However, there is no concrete evidence yet how the average inside \(q\)-exponential and the cross-product action values may work jointly to benefit the policy, and their benefits may depend on the task and environments, requiring further categorization and discussion. Empirically, we find that the nonlinearity of Tsallis KL policies bring superior performance to the uniform averaging KL policies on the testbed considered.

## 4 A Practical Algorithm for Tsallis KL Regularization

In this section we provide a practical algorithm for implementing Tsallis regularization. We first explain why this is not straightforward to simply implement KL-regularized value iteration, and how Munchausen Value Iteration (MVI) overcomes this issue with a clever implicit regularization trick. We then extend this algorithm to \(q>1\) using a similar approach, though now with some approximation due once again to the difficulties of pseudo-additivity.

### Implicit Regularization With MVI

Even for the standard KL, it is difficult to implement KL-regularized value iteration with function approximation. The difficulty arises from the fact that we cannot exactly obtain \(_{k+1}_{k}(Q_{k})\). This policy might not be representable by our function approximator. For \(q=1\), one needs to store all past \(Q_{k}\) which is computationally infeasible.

An alternative direction has been to construct a different value function iteration scheme, which is equivalent to the original KL regularized value iteration (Azar et al., 2012; Kozuno et al., 2019). A recent method of this family is Munchausen VI (MVI) (Vieillard et al., 2020). MVI implicitly enforces KL regularization using the recursion

\[_{k+1}=_{},Q_{k}- \\ Q_{k+1}=r+_{k+1}+ P_{k+1},Q_{k}- _{k+1}\] (9)

We see that Eq. (9) is Eq. (1) with \(()=-()\) (blue) plus an additional red _Munchausen term_, with coefficient \(\). Vieillard et al. (2020) showed that implicit KL regularization was performedunder the hood, even though we still have tractable \(_{k+1}(^{-1}Q_{k})\):

\[Q_{k+1}=r+_{k+1}+ P_{k+1},Q_{ k}-_{k+1} Q_{k+1}\!-\!_{k+1}\!=\] \[r+ P_{k+1},Q_{k}-_{k }\!-\!_{k+1},(_{k+1}\!-\!_{k })-(1-)_{k+1}\] \[ Q^{}_{k+1}=r+ P_{k+1},Q^{}_{k}- D_{}}(_{k+1}||_{k})+( 1-)(_{k+1})\] (10)

where \(Q^{}_{k+1}\!:=\!Q_{k+1}-_{k+1}\) is the generalized action value function.

The implementation of this idea uses the fact that \(_{k+1}=(Q_{k}-_{}Q_{k})\), where \(_{}Q_{k}:=}(^{-1}Q_{k} ),Q_{k},Z_{k}=,(^{-1}Q_ {k})\) is the Boltzmann softmax operator.2 In the original work, computing this advantage term was found to be more stable than directly using the log of the policy. In our extension, we use the same form.

### MVI(\(q\)) For General \(q\)

The MVI(q) algorithm is a simple extension of MVI: it replaces the standard exponential in the definition of the advantage with the \(q\)-exponential. We can express this action gap as \(Q_{k}-_{q,}Q_{k}\), where \(_{q,}Q_{k}=_{q}(}{}-_{ q}(}{})),Q_{k}\). When \(q=1\), it recovers \(Q_{k}-_{}Q_{k}\). We summarize this MVI(\(q\)) algorithm in Algorithm B in the Appendix. When \(q=1\), we recover MVI. For \(q=\), we get that \(_{,}Q_{k}\) is \(_{a}Q_{k}(s,a)\)--no regularization--and we recover advantage learning (Baird and Moore, 1999). Similar to the original MVI algorithm, MVI(\(q\)) enjoys tractable policy expression with \(_{k+1}_{q}(^{-1}Q_{k})\).

Unlike MVI, however, MVI(\(q\)) no longer exactly implements the implicit regularization shown in Eq. (10). Below, we go through a similar derivation as MVI, show why there is an approximation and motivate why the above advantage term is a reasonable approximation. In addition to this reasoning, our primary motivation for this extension of MVI to use \(q>1\) was to inherit the same simple form as MVI as well as because empirically we found it to be effective.

Let us similarly define a generalized action value function \(Q^{}_{k+1}=Q_{k+1}-_{q}_{k+1}\). Using the relationship \(_{q}_{k}=_{q}}{_{k+1}}-_{q}}-(1 -q)_{q}_{k}_{q}}\), we get

\[Q_{k+1}-_{q}_{k+1}=r+ P_{k +1},Q_{k}+_{q}_{k}-_{q}_{k}+ S_{q}( _{k+1})\] \[ Q^{}_{k+1}=r+ P_{k+1},Q^{ }_{k}+ S_{q}(_{k+1})+\] \[ P_{k+1},(_{q} }{_{k+1}}-_{q}}-(1-q)_{q}}_{q}_{k})\] (11) \[=r+ P_{k+1},Q^{}_{k}+(1-) S_ {q}(_{k+1})- P_{k+1}, D^{q}_{ }}(_{k+1}||_{k})- R_{q}(_{k+1},_{k})\]

Figure 3: MVI(\(q\)) on CartPole-v1 for \(q=2,3,4,5\), averaged over 50 seeds, with \(=0.03,=0.9\). (Left) The difference between the proposed action gap \(Q_{k}-_{q,}Q_{k}\) and the general Munchausen term \(_{q}_{k+1}\) converges to a constant. (Right) The residual \(R_{q}(_{k+1},_{k})\) becomes larger as \(q\) increases. For \(q=2\), it remains negligible throughout the learning.

where we leveraged the fact that \(-<_{k+1},_{q}}>=- S_{q}(_ {k+1})\) and defined the residual term \(R_{q}(_{k+1},_{k}):=(1-q)_{q}}_{q}_{k}\). When \(q=2\), it is expected that the residual term remains negligible, but can become larger as \(q\) increases. We visualize the trend of the residual \(R_{q}(_{k+1},_{k})\) for \(q=2,3,4,5\) on the CartPole-v1 environment (Brockman et al., 2016) in Figure 3. Learning consists of \(2.5 10^{5}\) steps, evaluated every \(2500\) steps (one iteration), averaged over 50 independent runs. It is visible that the magnitude of residual jumps from \(q=4\) to \(5\), while \(q=2\) remains negligible throughout.

A reasonable approximation, therefore, is to use \(_{q}_{k+1}\) and omit this residual term. Even this approximation, however, has an issue. When the actions are in the support, \(_{q}\) is the unique inverse function of \(_{q}\) and \(_{q}_{k+1}\) yields \(}{}-_{q}(}{})\). However, for actions outside the support, we cannot get the inverse, because many inputs to \(_{q}\) can result in zero. We could still use \(}{}-_{q}(}{})\) as a sensible choice, and it appropriately does use negative values for the Munchausen term for these zero-probability actions. Empirically, however, we found this to be less effective than using the action gap.

Though the action gap is yet another approximation, there are clear similarities between using \(}{}-_{q}(}{})\) and the action gap \(Q_{k}-_{q,}Q_{k}\). The primary difference is in how the values are centered. We can see \(_{q}\) as using a uniform average value of the actions in the support, as characterized in Theorem 2. \(_{q,}Q_{k}\), on the other hand, is a weighted average of action-values.

We plot the difference between \(Q_{k}-_{q,}Q_{k}\) and \(_{q}_{k+1}\) in Figure 3, again in Cartpole. The difference stabilizes around -0.5 for most of learning--in other words primarily just shifting by a constant--but in early learning \(_{q}_{k+1}\) is larger, across all \(q\). This difference in magnitude might explain why using the action gap results in more stable learning, though more investigation is needed to truly understand the difference. For the purposes of this initial work, we pursue the use of the action gap, both as itself a natural extension of the current implementation of MVI and from our own experiments suggesting improved stability with this form.

## 5 Experiments

In this section we investigate the utility of MVI(\(q\)) in the Atari 2600 benchmark (Bellemare et al., 2013). We test whether this result holds in more challenging environments. Specifically, we compare to standard MVI (\(q=1\)), which was already shown to have competitive performance on Atari (Vieillard et al., 2020). We restrict our attention to \(q=2\), which was generally effective in other settings and also allows us to contrast to previous work (Lee et al., 2020) that only used entropy regularization with KL regularization. For MVI(\(q=2\)), we take the exact same learning setup--hyperparameters and architecture--as MVI(\(q=1\)) and simply modify the term added to the VI update, as in Algorithm 1.

Figure 4: Learning curves of MVI(\(q\)) and M-VI on the selected Atari games, averaged over 3 independent runs, with ribbon denoting the standard error. On some environments MVI(\(q\)) significantly improve upon M-VI. Quantitative improvements over M-VI and Tsallis-VI are shown in Figures 5.

For the Atari games we implemented MVI(\(q\)), Tsallis-VI and M-VI based on the Quantile Regression DQN (Dabney et al., 2018). We leverage the optimized Stable-Baselines3 architecture (Raffin et al., 2021) for best performance and average over 3 independent runs following (Vieillard et al., 2020), though we run \(50\) million frames instead of 200 million. From Figure 4 it is visible that MVI(\(q\)) is stable with no wild variance shown, suggesting 3 seeds might be sufficient. We perform grid searches for the algorithmic hyperparameters on two environments Asterix and Seaquest: the latter environment is regarded as a hard exploration environment. MVI(\(q\)) \(:\{0.01,0.1,0.5,0.9,0.99\}\); \(:\{0.01,0.1,1.0,10,100\}\). Tsallis-VI \(:\{0.01,0.1,1.0,10,100\}\). For MVI we use the reported hyperparameters in (Vieillard et al., 2020). Hyperparameters can be seen from Table 2 and full results are provided in Appendix E.

### Comparing MVI(\(q\)) with \(q=1\) to \(q=2\)

We provide the overall performance of MVI versus MVI(\(q=2\)) in Figure 5. Using \(q=2\) provides a large improvement in about 5 games, about double the performance in the next 5 games, comparable performance in the next 7 games and then slightly worse performance in 3 games (PrivateEye, Chopper and Seaquest). Both PrivateEye and Seaquest are considered harder exploration games, which might explain this discrepancy. The Tsallis policy with \(q=2\) reduces the support on actions, truncating some probabilities to zero. In general, with a higher \(q\), the resulting policy is greedier, with \(q=\) corresponding to exactly the greedy policy. It is possible that for these harder exploration games, the higher stochasticity in the softmax policy from MVI whre \(q=1\) promoted more exploration. A natural next step is to consider incorporating more directed exploration approaches, into MVI(\(q=2\)), to benefit from the fact that lower-value actions are removed (avoiding taking poor actions) while exploring in a more directed way when needed.

We examine the learning curves for the games where MVI(\(q\)) had the most significant improvement, in Figure 4. Particularly notable is how much more quickly MVI(\(q\)) learned with \(q=2\), in addition to plateauing at a higher point. In Hero, MVI(\(q\)) learned a stably across the runs, whereas standard MVI with \(q=1\) clearly has some failures.

These results are quite surprising. The algorithms are otherwise very similar, with the seemingly small change of using Munchausen term \(Q_{k}(s,a)-_{q=2,}Q_{k}\) instead of \(Q_{k}(s,a)-_{q=1,}Q_{k}\) and using the \(q\)-logarithm and \(q\)-exponential for the entropy regularization and policy parameterization. Previous work using \(q=2\) to get the sparsemax with entropy regularization generally harmed performance (Lee et al., 2018, 2020). It seems that to get the benefits of the generalization to \(q>1\), the addition of the KL regularization might be key. We validate this in the next section.

### The Importance of Including KL Regularization

In the policy evaluation step of Eq. (11), if we set \(=0\) then we recover Tsallis-VI which uses regularization \(()=-S_{q}()\) in Eq. (1). In other words, we recover the algorithm that incorporates entropy regularization using the \(q\)-logarithm and the resulting sparsemax policy. Unlike MVI, Tsallis

Figure 5: (Left) The percent improvement of MVI(\(q\)) with \(q=2\) over standard MVI (where \(q=1\)) on select Atari games. The improvement is computed by subtracting the scores from MVI(\(q\)) and MVI and normalizing by the MVI scores. (Right) Improvement over Tsallis-VI on Atari environments, normalized with Tsallis-VI scores.

VI has not been comprehensively evaluated on Atari games, so we include results for the larger benchmark set comprising 35 Atari games. We plot the percentage improvement of MVI(\(q\)) over Tsallis-VI in Figure 5.

The improvement from including the Munchausen term (\(>0\)) is stark. For more than half of the games, MVI(\(q\)) resulted in more than 100% improvement. For the remaining games it was comparable. For 10 games, it provided more than 400% improvement. Looking more specifically at which games there was notable improvement, it seems that exploration may again have played a role. MVI(\(q\)) performs much better on Seaquest and PrivateEye. Both MVI(\(q\)) and Tsallis-VI have policy parameterizations that truncate action support, setting probabilities to zero for some actions. The KL regularization term, however, likely slows this down. It is possible the Tsallis-VI is concentrating too quickly, resulting in insufficient exploration.

## 6 Conclusion and Discussion

We investigated the use of the more general \(q\)-logarithm for entropy regularization and KL regularization, instead of the standard logarithm (\(q=1\)), which gave rise to Tsallis entropy and Tsallis KL regularization. We extended several results previously shown for \(q=1\), namely we proved (a) the form of the Tsallis policy can be expressed by \(q\)-exponential function; (b) Tsallis KL-regularized policies are weighted average of past action-values; (c) the convergence of value iteration for \(q=2\) and (d) a relationship between adding a \(q\)-logarithm of policy to the action-value update, to provide implicit Tsallis KL regularization and entropy regularization, generalizing the original Munchausen Value Iteration (MVI). We used these results to propose a generalization to MVI, which we call MVI(\(q\)), because for \(q=1\) we exactly recover MVI. We showed empirically that the generalization to \(q>1\) can be beneficial, providing notable improvements in the Atari 2600 benchmark.