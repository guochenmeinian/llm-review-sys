# Data Acquisition via Experimental Design

for Data Markets

Charles Lu

MIT

&Baihe Huang

UC Berkeley

&Sai Praneeth Karimireddy

USC, UC Berkeley

&Praneeth Vepakomma

MBZUAI, MIT

&Michael I. Jordan

UC Berkeley

&Ramesh Raskar

MIT

luchar@mit.edu

###### Abstract

The acquisition of training data is crucial for machine learning applications. Data markets can increase the supply of data, particularly in data-scarce domains such as healthcare, by incentivizing potential data providers to join the market. A major challenge for a data buyer in such a market is choosing the most valuable data points from a data seller. Unlike prior work in data valuation, which assumes centralized data access, we propose a federated approach to the data acquisition problem that is inspired by linear experimental design. Our proposed data acquisition method achieves lower prediction error without requiring labeled validation data and can be optimized in a fast and federated procedure. The key insight of our work is that a method that directly estimates the benefit of acquiring data for test set prediction is particularly compatible with a decentralized market setting.

## 1 Introduction

While massive training datasets enable major machine learning breakthroughs, they remain largely inaccessible outside of large companies, motivating mechanisms for broader data access. A related point is that many data owners have become resistant to having their data collected indiscriminately without their consent or without their participation in the fruits of predictive modeling, resulting in legal challenges against prominent AI companies [16; 33]. These trends motivate the study of _data marketplaces_, which aim to incentivize data sharing between sellers, that provide access to data, and buyers, that pay compensation for data access [11; 1; 54].

For practical data acquisition scenarios, a data buyer has a specific goal in mind and, in particular, wants training data to predict their test data in a specified context. Accessing different datapoints may require varying prices associated with each datapoint, which may reflect heterogeneous cost, quality, or privacy levels for each datapoint [44; 40].

For example, consider a hospital that wants to make a prediction for a specific patient's X-ray. The hospital can submit this X-ray as an unlabeled test query to the marketplace, along with a budget to access relevant training data. The marketplace selects useful training data to build a model for this specific prediction task, sharing only the final prediction rather than the raw data to protect privacy. This process can be repeated for each new patient query (see Figure 1). However, not all X-ray images will be equally relevant. Thus, we want to select only those seller datapoints that are most useful for answering the buyer's query and fit the buyer's budget.

This goal of _data acquisition_ has motivated the development of many data-valuation techniques (e.g., [23; 29; 38; 60; 52; 39; 57; 46; 30]). However, we argue that current data valuation techniquesare misaligned with the data acquisition problem, particularly in the context of data marketplaces. They all face at least one of the following limitations:

* The selection process may not be adaptive to the buyer's (unlabeled) test queries, potentially failing to identify the most relevant data. In a data marketplace, buyers typically need to purchase only a small subset of datapoints most relevant to their test data, which may follow a significantly different distribution than the overall seller data.
* When adaptive selection is implemented, these techniques rely on labeled validation data, which is often impractical. Further, when a small quantity of such data is available, the selection may overfit the validation data and result in poor performance on the test queries.
* The algorithms are not scalable and typically require retraining the ML model numerous times. Hence, they are unable to select from realistic seller corpora (\(>\)100K+ datapoints).

Instead, we propose **data acquisition via experimental design** (DAVED) method that overcomes all of these limitations. Unlike most previous work in data valuation, our approach does not require a labeled validation dataset and instead directly optimizes data selection for the buyer's unlabeled test queries.

Additionally, our approach accounts for budget constraints and is able to weigh the price of each seller's datapoint against its potential benefit, simultaneously solving the budget and revenue allocation problems . Moreover, it can be implemented in a federated manner, achieving lower prediction error even compared to centralized baselines.

Our contributions are the following:

1. Formulate the data acquisition problem for data marketplaces and demonstrate that data valuation methods make a fundamental theoretical mistake of "inference after selection" (Theorem 1).
2. Design a novel, highly scalable, and distributed data selection procedure that eliminates the need for validation data by directly selecting the most cost-effective seller data to answer the buyer's test queries (Algorithm 1).
3. Demonstrate state-of-the-art performance on synthetic data and medical data in various modalities (X-rays, images, and text).

Figure 1: **Overview of Data Marketplace Approach. A buyer brings their test query (e.g., a patient’s chest X-ray needing diagnosis) and a budget to the marketplace. DAVED selects the most relevant subset of seller training data to minimize prediction error on the buyer’s specified test query while respecting budget constraints. Unlike prior methods that require labeled validation data, DAVED directly optimizes for test performance. This enables targeted, cost-effective data acquisition compared to purchasing entire datasets.**

## 2 Data Acquisition versus Data Valuation

In a decentralized data marketplace, data acquisition must be performed _before_ full data access is granted to the buyer . This relates to Arrow's Information Paradox  -- sellers are unwilling to share data before payment, while buyers need to evaluate utility before purchasing. This distinction between data valuation and data acquisition for data marketplaces is also discussed in a recent data acquisition benchmark, where data value must be estimated without requiring white-box access to the seller's data (i.e., full, unrestricted access to the data) .

A more fundamental issue with validation-based data valuation approaches is exemplified by the Data Shapley value approach [23; 29; 38], etc.], which measures the marginal contribution of each training datapoint's improvement to a validation metric. They are great for after-the-fact attributing the relative influence of the training data. However, they cannot be used to _make decisions_ about which datapoints should be included in the training. This is because of, as noted earlier, the _"inference after selection"_ issue. Using validation data to select training data leads to substantial over-fitting to the validation data.

_Illustrative example:_ Suppose that we only have a single validation datapoint. Then, it is clear that we will select training data similar to this singular datapoint, and our selection has no hope of working on the test dataset. While this clearly demonstrates overfitting in an extreme scenario, we show in Figure 2 that increasing the validation set size does not circumvent this issue. We see that other data valuation techniques have poor test prediction errors--some techniques even underperform even a random selection baseline! This clearly demonstrates overfitting. Our proposed method maintains low test error as more seller training data is selected. In Section 3, we will dig in deeper into this phenomenon and prove a very strong theoretical lower bound. We show that any approach that relies upon validation data for data selection can perform as badly as _throwing away all the training data_ and simply training on the validation data alone! This is especially true when our budget is small compared to the dimensionality of the problem, as is likely in a data market setting -- the data is typically high dimensional, and we can only select a very small fraction of the total available data.

## 3 Setup and Limitations of Prior Methods

**Description of Data Acquisition Setting.** As shown in Figure 1, in our setting of data acquisition, a buyer has a budget and test data. The platform uses the buyer's data to select training datapoints from the seller that optimize buyer test error. The interaction proceeds as follows:

1. The buyer brings their test data \(X^{}=[x_{1}^{},,x_{m}^{}]\) and a budget \(B\) to the market platform. This test data is associated with unknown target labels \(Y^{}=[y_{1}^{},,y_{m}^{}]\)

Figure 2: **Failures of Current Validation-based Data Valuation Methods.** Current data valuation methods overfit when data dimensionality is high or validation sets are small. Using 1,000 seller datapoints (each with cost 1) of Gaussian distributed data, we compare test error across methods as buyers acquire data under different budgets. (Left) Validation-based data valuation methods overfit when the data is too high dimensional (\(d=30\)). (Right) Even with low-dimensional data (\(d=10\)), overfitting occurs when the validation set is too small (\(n=10\)), resulting in worse performance than random selection. Our validation-free method (DAVED) method maintains low error in both scenarios.

[MISSING_PAGE_FAIL:4]

Our Methods and Implementations

We propose an alternative approach based on a proxy objective. Our key assumption is that the conditional distribution \(_{y|x}\): \(y=f_{_{*}}(x)+\) is identical across \(Z^{}\) and \(Z^{}\). This is a natural assumption in many domains -- for instance, if an X-ray exhibits indicators of a specific disease, it should receive the same diagnosis regardless of whether it appears in the training or test set. Without this assumption, our problem becomes intractable since the same \(x_{j}\) could map to arbitrarily different labels across train and test sets. Under this framework, we can reformulate our problem using V-optimal experiment design .

**Step 1: Linearizing the problem.** Our goal is to design a proxy loss function \(}()\) which approximates the true test loss \(()\). To do this, we have to reason about how different choices of training data \(S Z^{}\) could impact the prediction on a particular test datapoint in \(X^{}\). This is a notoriously challenging problem for general deep learning models . Instead, we use a linear approximation and model the complicated training dynamics with kernelized linear regression. We suppose we have a known feature-extractor \(:^{d_{0}}\) and an unknown \(^{*}^{d_{0}}\) such that the data is generated as

\[y=^{*}(x)+\,,\] (3)

where \(\) is independent noise with mean zero and \(d_{0}\) is the embedding dimensionality. The function \(()\) can be the empirical Neural Tangent Kernel (eNTK) [27; 41; 58] of the model, or even the embeddings extracted from a deep neural network such as CLIP . While this may be a bad approximation in general , a recent line of work has shown that such eNTK representation very closely approximates the _fine-tuning_ dynamics of pre-trained models both theoretically [58; 42] as well as emperically [22; 63]. In fact, such linear approximations have also been used to speed up validation-based data attribution computations .

**Step 2: Experimental design proxy.** Given the assumption on our data from Eqn. (3), we can use the V-optimal experiment design framework [51; 47; 26] to define a proxy objective. First, suppose that \(S((X^{}),Y^{})\) is the subset selected by \(\) and then we performed least-squares regression. The resulting estimate \(()\) can be computed in closed form as

\[()=_{j=1}^{n}w_{j}(x_{j})(x_{j})^{ }^{}(_{j=1}^{n}w_{j}(x_{j})y_{j})\,.\]

Henceforth, we will drop the \(\) when obvious from context and simply use \(x\). We can further use Eqn. (3) to compute the expected error on an arbitrary test query \(x_{0},y_{0}\) as follows:

\[[(()^{}x_{0}-y)^{2}|X^{ },x_{0}] }{=} [(()-^{*})^{}x _{0}+^{2}]\] \[}{=} x_{0}^{}[(()-^{*})(()-^{*})^{}]x_{0}+\|\|^{2}\] \[}{=} x_{0}^{}[()( )^{}]x_{0}+\|\|^{2}\] \[}{=} x_{0}^{}^{n}w_{j}x_{j}x_{j}^{ }}_{=:()}^{}x_{0}+\| \|^{2}\]

Here \(a_{3}\) uses the unbiasedness of the ordinary least squares (OLS) estimator and \(a_{4}\) plugs in the closed form of \(()\) and simplifies. With this, we end up with a very clean expression for the expected test error on an arbitrary point \(x_{0}\), and the matrix \(()\) is known as the _Fisher information matrix_. While regression suffices for our use case, the procedure can be extended to general linear models. Dropping the fixed \(\|\|^{2}\), we can use this to build our proxy function \(}^{ED}()\) and arrive at the following optimization problem

\[\{0,1\}^{n}}\ \{}^{ED}( ):=}{{m}}_{i=1}^{m}(x_{i}^{})^{} ()^{}(x_{i}^{})\}_{j=1}^{n}w_{j}c_{j} B.}\] (4)

This optimization objective directly measures how useful each training point would be for predicting the specified test query. The matrix \(()\) captures how much information each selected datapoint provides about the test point in the embedded feature space.

Note that our proxy function \(}^{ED}()\) can be computed using just \(X^{},X^{}\) and does not even need access to training labels. Unfortunately, the objective in (4) is NP-hard to optimize . We next see how to derive fast and provably good approximation algorithms for (4).

Step 3: Fast approximation.To make Eq. 4 amenable to gradient-based optimization, we drop the constraint that \(w_{j}\{0,1\}\) and allow it to be a continuous positive vector i.e., \( 0\) and \(_{j=1}^{n}w_{j}c_{j} B\). With this relaxation, the proxy objective \(}^{ED}()\) is continuous and convex in \(\). We then run the "herding" variant of the _Frank-Wolfe_ algorithm . To do this, define \((}_{t}:=_{t}/)\) for any \(t\). We start from a \(}_{0}=_{0}\) and iteratively update as2

\[}_{t+1}(1-_{t})}_{t}+ _{t}_{j_{t}},j_{t}=_{j[n]}(-_{w_{j}}}(_{t})/c_{j})\] (5)

Note that if we use the step-size \(_{t}=\) in (5), \(_{t}\) satisfies a special property at any iteration \(t\):

\[}_{t}^{n}(t+1)}_{t} \{0,1\}^{n}\,.\]

Run the procedure until the last iteration \(t=t_{o}\) for which we still have \(\|_{t_{0}}\|_{1} B\). We can adapt the theory from  to analyze the above procedure and show the following.

**Theorem 2** (Informal).: _Let us run Frank-Wolfe herding update (5) for \(t_{0}\) steps such that it is last step which satisfies \(\|_{t_{0}}\|_{1} B\). We use \(}_{t_{0}}=((t_{0}+1)_{t_{0}}/)\) as our selection vector and we would have selected \(t_{0}\) datapoints. Then, under some assumptions, we have_

\[}^{ED}(t+1)}_{t_{0}}_{ \{0,1\}^{n},_{j=1}^{1}w_{j}c_{j} B}}^{ED }()+O\!(}{t_{0}}).\]

The above theorem shows that our continuous relaxation does not significantly affect the optimality of our result -- we get \(O(}{t_{0}})\) close to the optimal solution to the original NP-hard (4). If all datapoints have equal cost \(c\), then \(t_{0}= B/c\), and so our approximation quality improves as we increase the budget. While better approximation guarantees are attainable , their procedure is significantly more involved and is not easily amenable to efficient federated implementations as ours is.

**Step 4: Efficient federated implementation.** Our practical implementation directly restricts \(^{n}\) instead of \(}\) in the theoretical implementation above i.e., we run

\[_{t+1}(1-_{t})_{t}+_{t}_{ j_{t}},j_{t}=_{j[n]}(-_{w_{j}}}(_{t})/c_{j})\] (6)

This way \(\) can be directly interpreted to be the sampling probability for different seller training datapoints. The bottleneck to efficiently implementing (6) is computing the gradient. At step \(t\), the negative gradient can be shown to be

\[g_{j}:=-_{w_{j}}}(_{t})=}{{m}} _{i=1}^{m}(x_{i}^{})^{}(_{t} )^{}(x_{j}^{})^{2}\,.\] (7)

Thus, if we have the inverse information matrix \((_{t})^{}\) pre-computed, \(g_{j}\) as well as the update (6) can be trivially computed by seller \(j\) using only their data \(x_{j}^{}\) (and the test data). Next, we show how to efficiently maintain the inverse information matrix. Note that the update (6) has a special structure: all coordinates are shrunk, and then only a single coordinate of \(_{t}\) is increased. We can relate the resulting \(\) matrices with a rank-one update as:

\[(_{t+1})=(1-_{t})(_{t})+ _{t}x_{j_{t}}x_{j_{t}}^{}\,.\]

Define \(P_{t}:=(_{t})^{}\). We can use the Sherman-Morrison formula  to compute \(P_{t+1}=(_{t+1})^{}\) as

\[P_{t+1}=}P_{t}-P_{t}x_{j_{t}}x_{j_{t}}^{ }P_{t}}{1-_{t}+_{t}x_{j_{t}}^{}P_{t}x_{j_{t}}}.\] (8)

For each round \(t\), this update only involves the current matrix \(P_{t}=(_{t})^{}\) and the single datapoint \(x_{j_{t}}\) selected for the round. Thus, the seller can also locally compute this update as well as the updated cost \(}()\) as in Eq. (4) for any \(_{t}\):

\[}()=)}_{i=1}^{m}}^{}P_{t}\,x_{i}-}{1+_{t}x_{j_{t}}^{ }P_{t}x_{j_{t}}}_{i=1}^{m}}^{}P_{t}\,x_{j_{t}} ^{2}\] (9)Thus, a line search can be performed to determine the optimal step size \(_{t}\) to minimize the proxy loss as. This differs from (5) where we used a specific choice of \(_{t}\). Frank-Wolfe is known to be more stable with the line search [53; 65]. The seller can communicate this \(_{t}\) and \(x_{j_{t}}\) to the platform to compute the updated \(P_{t+1}\) using only \(O(d)\) communication.

An additional practical consideration is that by initializing \(}=c_{1}_{1}\), we have an ill-conditioned inverse information matrix \(P_{0}\). We instead use an initialization of \(}=_{n}/n^{n}\) and further add a feature-wise regularization term. This makes the initial \(P_{0}\)

\[P_{0}=((1-_{})\,X^{}(})X\ +_{}())^{-1},\] (10)

where \(_{i}=_{j=1}^{n}(X_{ji}-_{i})^{2}}\) is the empirical standard deviation of feature \(i\). The complete details are summarized in Algorithm 1.

**Single-step variant.** We can also forgo the iterative process and instead linearly approximate the cost function (Eq 4) with a _single step_ that selects the top \(k\) datapoints under the budget \(B\),

\[(x^{},X,B)= _{i=1}^{m}(x_{i}^{})^{}P_{0}x_{j} ^{2}}_{j=1}^{n}.}\] (11)

This simplified version is extremely fast while still maintaining relatively good performance.

```
1:Input: buyer test datapoint \(X^{}^{m d}\), seller training data \(X^{n d}\), seller weights \(^{n}\), iteration steps \(T\), regularization parameter \(_{}\), and seller datapoint costs \(^{n}_{+}\)
2:\(_{0}/n\)# Initialize weight vector to uniform distribution
3:\(P_{0}((1-_{})\,X^{}(_{0})\ X+_{}_{X}I_{n n})^{-1}\)# Initialize \(P\) (Eq. 10)
4:for\(t\{1,2,,T\}\)do
5:\(g-}(_{t})\)# Compute negative gradients (Eq. 7)
6:\(j_{t}_{j}\ (g_{j}/c_{j})\)# Select coordinate based on costs
7:\(_{t}(})\)# Find optimal step size (Eq. 9)
8:\(_{t+1}(1-_{t})_{t}+_{t}_{j_{ t}}\)# Shrink weights and upweight the chosen coordinate (Eq. 6)
9:\(P_{t+1}(P_{t},x_{j_{t}},_{t})\)# Update inverse information matrix (Eq. 8)
10:endfor
11:Output: Sample seller data according to \(_{T}^{n}\) without replacement until budget \(B\) runs out. ```

**Algorithm 1** DAVED: Iterative Optimization Procedure

## 5 Experiments

We evaluate our proposed method for data acquisition (DAVED) against common data valuation methods on both synthetic data and four real-world medical:

1. **Fitzpatrick17K**, a skin lesion dataset, where the task is to predict Fitzpatrick skin tone on a 6-point scale from dermatology images.
2. **RSNA Pediatric Bone Age dataset**, where the task is to assess bone age (in months) from X-ray images of an infant's hand.
3. **Medical Information Mart for Intensive Care (MIMIC-III)**, where the task is to predict the length of hospital stay from 48 attributes such as demographics, insurance, and medical conditions.
4. **DrugLib reviews**, text reviews of drugs where the task is to predict ratings (1-10).

For validation-based methods, we use a validation set of 100 datapoints. We report mean test errors over 100 buyers. For more details on the experimental setup, see Appendix C. Our code is available at this repo: https://github.com/clu5/data-acquisition-via-experimental-design.

**Comparing Performance on Data with Homogeneous Costs.** In Figure 3, we evaluate our method and several other data valuation methods on varying amounts of Gaussian data with homogeneous fixed costs. Compared to other methods, both multi- and single-step versions of DAVED have lower test errors across budgets on synthetic data. This performance gap is especially large when the buyer has a small budget (around 5-10 seller training datapoints). In Figure 4, we evaluate our method on real image and text data embedded through CLIP and GPT-2 feature representations. We observe that DAVED has better performance compared to most other baselines on all three datasets, highlighting that the proposed method is practical for embeddings of high-dimensional data. Table 1 summarizes our results on all datasets. For the Gaussian data and MIMIC datasets, we report the mean error of budgets from 1 to 10, while for the embedded datasets (RSNA, Fitzpatrick17K, DrugLib), we report the mean error of budgets from 1 to 100 in intervals of five.

**Comparing Performance on Data with Heterogeneous Costs.** Next, we compare methods on seller data with non-homogeneous costs. We uniformly sample costs \(c\{1,2,3,4,5\}\) for each seller datapoint and consider two cost functions, \(c_{j}=\) and \(c_{j}=c^{2}\), which downweights gradient of that datapoint \(x_{j}\) (see Equation 7). To simulate heterogeneous utility across datapoints, we introduce cost-dependent label noise, \((,^{2})\), to each datapoint \(_{i}:=y_{i}+/c_{j}\), where \(\) is the mean target value and \(\) is the overall noise level, which we fix at \(30\%\) throughout our experiments. For these experiments, we did not evaluate Data Shapley , LOO , and Influence  that had very long runtimes. In Table 2, we report additional mean test error across budgets 1-30 for both cost functions. We find that our DAVID method is more budget-efficient in choosing cost-effective noisy datapoints than other methods across datasets. We provide additional plots for heterogeneous costs in Appendix D.1.

**Comparing Runtime.** In Figure 5, we compare the optimization runtime of our data selection method on 1,000 datapoints while increasing the dimensionality of the data as well as when the dimensionality is fixed to 30, and the number of seller datapoints is increased to 100,000. Data Shapley  and LOO  took too long to run for large amounts of datapoints or high dimensional data and are not reported. In both experiments, our multi-step compares favorably to efficiency-optimized techniques such as KNN Shapley  while our single-step method had the fastest runtime. This demonstrates that our method can scale to marketplaces with millions of datapoints.

**Regularization Strength.** In Appendix D.2, we vary the amount of regularization applied on the MIMIC, DrugLib, and RSNA datasets. We find that applying a moderate amount of regularization between 0.2 and 0.6 can lead to improved performance. Even when the information matrix is set to identity, i.e., \(=1\), performance on the DrugLib datasets is still reasonable. Note that for all other experiments, we do not apply any regularization.

**Amount of Buyer Data.** In Appendix D.3, we vary how many buyer test datapoints are simultaneously optimized over on Gaussian-distributed, MIMIC, and RSNA datasets. While all buyer and seller data is sampled from the same distribution, the number of buyer datapoints still affects the optimization procedure. In general, we find that increasing the number of datapoints in the "test batch" increases

Figure 3: **Data Acquisition Performance across different Market Sizes on Synthetic Data.** We compare test prediction error as seller training data is selected under varying budgets and amount of data for sale, with total available seller data of 1K (left), 5K (middle), and 100K (right) points. Our data selection method (DAVED) consistently achieves lower MSE with fewer purchased datapoints, i.e., better data acquisition efficiency, than other data valuation methods. Both multi-step and single-step variants of DAVED achieve lower test MSE with fewer training points compared to validation-based methods. The performance gap is especially pronounced with small budgets (5-10 points). Unless otherwise specified, all results are averaged over 100 random test points.

    &  &  &  &  &  \\   & \(\) & \(c^{2}\) & \(\) & \(c^{2}\) & \(\) & \(c^{2}\) & \(\) & \(c^{2}\) & \(\) & \(c^{2}\) \\  Random baseline & 2.36 & 77.7 & 288 & 285 & 2254 & 2065 & 2.14 & 2.11 & 16.5 & 18.4 \\ DVRL & 1.67 & 2.1 & 214 & 215 & 24003 & 5588 & 1.46 & 8.90 & 22.5 & 20.7 \\ LAVA & 2.13 & 3.3 & 482 & 475 & 1667 & 1587 & 2.09 & 2.21 & 35.7 & 34.8 \\ KNN Shapley & 2.13 & 69.0 & 217 & 956 & 2754 & 2506 & 1.85 & 2.15 & 13.6 & 13.0 \\ Data OOB & 2.19 & 3.3 & 243 & 246 & 1695 & 1205 & 2.08 & 2.52 & 10.8 & 10.8 \\ DAVED (single) & 1.54 & 251.5 & 598 & 585 & 1734 & 1550 & **0.75** & **0.71** & **9.4** & **10.1** \\ DAVED (multi) & **0.04** & **0.2** & **169** & **168** & **1076** & **942** & 0.76 & 0.75 & 12.6 & 11.4 \\   

Table 2: **Test Error with Heterogeneous Costs.** Comparing data selection methods for two different cost functions, \(\) and \(c^{2}\). For each budget constraint, we select seller datapoints until the budget is exceeded and calculate test prediction error on the buyer data. We average over 100 buyers and report the mean test error across budgets from 1 to 30.

Figure 4: **Data Acquisition Performance on Real Medical Datasets.** DAVED demonstrates strong performance on real-world medical imaging and drug review datasets. (Left to right) Results on Fitzpatrick17K (skin lesions), RSNA Bone Age (X-rays), and DrugLib (drug reviews) — where high-dimensional raw data is embedded via CLIP (images) or GPT-2 (text). Each method selects training points under budget constraints to train a regression model on the embedded data. DAVED achieves lower test prediction error using fewer training points compared to validation-based approaches, demonstrating effectiveness on high-dimensional data.

    &  &  &  &  &  \\   & 1K & 100K & 1K & 35K & 12K & 15K & 3.5K \\  Random baseline & 1.38 & 1.01 & 301.0 & 283.7 & 1309.1 & 1.49 & 21.4 \\ Data Shapley  & 0.87 & N/A & 294.9 & N/A & N/A & N/A & N/A \\ Leave One Out  & 1.31 & N/A & 1125.0 & N/A & N/A & N/A & N/A \\ Influence  & 1.47 & 0.97 & 189.4 & 876.4 & 1614.5 & 1.93 & 12.8 \\ DVRL  & 1.33 & 1.26 & 229.7 & 285.5 & 3528.8 & 3.00 & 12.6 \\ LAVA  & 1.47 & 1.10 & 190.9 & 417.3 & 1867.5 & 1.45 & 17.4 \\ KNN Shapley  & 1.55 & 1.18 & 175.7 & 229.6 & 1387.0 & 1.82 & 19.0 \\ Data OOB  & 1.24 & 0.98 & **169.7** & 215.6 & 1020.3 & 1.35 & 10.0 \\ DAVED (single step) & 0.58 & 0.27 & 277.4 & 659.9 & 900.2 & 0.73 & **9.0** \\ DAVED (multi-step) & **0.37** & **0.16** & 206.7 & **171.4** & **785.2** & **0.67** & 9.2 \\   

Table 1: **Test Error of Data Valuation Methods.** We compared the test mean squared error on the buyer test point on a synthetic Gaussian-distributed data and four medical datasets: MIMIC, RSNA, Fitzpatrick17K, and DrugLib. The subleading denotes the number of seller training data available for that experiment, and “N/A” denotes that the method exceeded runtime constraints for the experiment. We optimize a separate random sample of training and validation data for each buyer and average over 100 buyers. Bolded values indicate the best-performing method and underlined values denote the second-best-performing method.

test errors. Therefore, we recommend keeping the number of test datapoints in the buyer's "query" between 1-8 for each data acquisition.

**Number of Steps.** In Appendix D.4, we vary the number of optimization steps in our method on the Gaussian-distributed and RSNA datasets. We find that more iterations generally improve prediction performance. Intuitively, one expects that selecting \(T\) points requires at least \(T\) steps of iterative optimization. We recommend setting the number of steps to be 2-5 times the desired budget for homogeneous costs.

**Convex versus Iterative Optimization** In Appendix D.6, we compare the iterative optimization procedure against a convex optimization solver . We find that our iterative approach results in several orders of magnitude speedup while maintaining similar levels of test error.

**Finetuning versus Linear Probe.** In Figure 14, we evaluate fine-tuning versus linear probing for datapoints selected using DAVID and random selection. We find that using DAVID for fine-tuning performs similarly to linear probing results on DrugLib with BERT .

## 6 Discussion

While other validation-free methods exist [60; 5], our method uniquely combines test-adaptivity, theoretical grounding, and superior empirical performance. Moreover, a major advantage of our method is that it is amenable to federated optimization requiring \(O(d)\) communication per round, making it well-suited for decentralized data marketplaces, unlike other methods that require seller data to be centralized in order to repeatedly train models to estimate data value. Additionally, our method does not require labeled data, whereas other data valuation methods assume that all datapoints come with corresponding ground-truth labels. As discussed in Section 2 and Section 3, the existing paradigm of valuing data with a validation set is suboptimal. Incidentally, the second-best performing method, Data OOB , is the only other method that does not use a validation set.

**Limitations.** However, our algorithm comes with some limitations that form exciting directions for future work. Our approach currently communicates every step. Instead, integrating local steps like in FedAvg  or Scaffold  would decrease communication costs. Further, integrating differential privacy techniques would provide formal privacy guarantees to the buyers and sellers . While DAVED shows strong performance across datasets, its effectiveness depends on having a good feature extractor that captures relevant aspects of the data. We recommend using pre-trained foundation models (e.g., CLIP, GPT-2) as they can extract general-purpose features. Future work could explore adapting the feature extraction to specific domains or handling cases where key features are missing

Figure 5: **Computational efficiency comparison.** DAVED has significantly lower computational overhead compared to model-based data valuation methods. (Left) Runtime scaling with data dimensionality (fixed 1,000 datapoints). (Right) Runtime scaling with the amount of seller data (fixed 30 dimensions). Our single-step variant is faster than even optimized methods like KNN Shapley, while the multi-step variant remains efficient while achieving better performance. Our optimization procedure only requires \(O(d)\) communication per round, which makes it particularly suited for decentralized data market settings. For Data Shapley and Leave-One-Out, some experiments were omitted due to prohibitively long runtimes.