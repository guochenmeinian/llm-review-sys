# A Functional Extension of Semi-Structured Networks

David Rugamer

Department of Statistics, LMU Munich

Munich Center for Machine Learning (MCML)

Munich, Germany

david@stat.uni-muenchen.de Bernard X.W. Liew, Zainab Altai

School of Sport, Rehabilitation and Exercise Sciences

University of Essex

Colchester, UK

[bl19622,z.altai]@essex.ac.uk Almond Stocker

Institute of Mathematics

Ecole Polytechnic Federal de Lausanne (EPFL)

Lausanne, Switzerland

almond.stoecker@epfl.ch

###### Abstract

Semi-structured networks (SSNs) merge the structures familiar from additive models with deep neural networks, allowing the modeling of interpretable partial feature effects while capturing higher-order non-linearities at the same time. A significant challenge in this integration is maintaining the interpretability of the additive model component. Inspired by large-scale biomechanics datasets, this paper explores extending SSNs to functional data. Existing methods in functional data analysis are promising but often not expressive enough to account for all interactions and non-linearities and do not scale well to large datasets. Although the SSN approach presents a compelling potential solution, its adaptation to functional data remains complex. In this work, we propose a functional SSN method that retains the advantageous properties of classical functional regression approaches while also improving scalability. Our numerical experiments demonstrate that this approach accurately recovers underlying signals, enhances predictive performance, and performs favorably compared to competing methods.

## 1 Introduction

Incorporating additive structures in neural networks to enhance interpretability has been a major theme of recent advances in deep learning. For example, neural additive models (NAMs; ) allow users to learn basis functions in an automatic, data-driven fashion using feature subnetworks and thereby provide an alternative modeling option to basis function approaches from statistics such as generalized additive models (GAMs; ). Various extensions and related proposals have been published in recent years. Notable examples include the joint basis function learning by  or the combination of GAMs and neural oblivious decision ensembles . When combining structural assumptions with arbitrary deep architectures, the resulting network is often referred to as wide-and-deep  or semi-structured network (SSN; [49; 52]). Various extensions of SSNs have been proposed in recent years, including SSNs for survival analysis [24; 25], ordinal data fitting , foruncertainty quantification [2; 12], Bayesian SSNs , or SSNs incorporated into normalizing flows . One of the key challenges in SSNs is to ensure the identifiability of the additive model part in the presence of a deep network predictor in order to maintain interpretability.

In this work, motivated by a large-scale biomechanical application, we study how to efficiently transfer these two concepts -- the embedding of basis functions into neural networks and the extension to SSNs -- for functional data analysis.

Functional data analysisFunctional data analysis (FDA; ) is a research field of increasing significance in statistics and machine learning (e.g., [3; 37; 38; 57]) that extends existing modeling approaches to address the functional nature of some data types (e.g., sensor signals, waves, or growth curves). One well-known extension is the class of functional additive models [15; 53], which represent the functional analog of GAMs. These models adopt the classical regression model point of view but allow for both in- and outputs to be (discretized) functions. Functional additive models, however, are often not expressive enough to account for all interactions and non-linearities and do not scale well to large datasets.

BiomechanicsA prominent example of a research field dealing with functional data is biomechanics. As in many other applications concerned with physical or biological signals, the integration of machine learning in biomechanics research offers many advantages (see, e.g., [26; 29; 30]). The research on joint moments in biomechanics, for example, provides insight into muscular activities, motor control strategies, chronic pain, and injury risks (see, e.g., [13; 31; 36; 47; 56]). By combining kinematic features with machine and deep learning to predict joint moments, researchers can bypass the need to gather high-quality biomechanics data and instead use "cheap" sensor data obtained through, e.g., everyday mobile devices which in turn allow predicting the "expensive" signals that can only be obtained in laboratory setups [20; 32; 33]. While a promising research direction, existing methods face various challenges including a lack of generalization and a missing clear understanding of the relationship learned.

### Related work in functional data analysis

In FDA, there has been a successful translation of many different methods for scalar data to function-valued data, including functional regression models and functional machine learning techniques.

Functional regression modelsRegression models with scalar outputs and functional inputs are called scalar-on-function regression models. An overview of such methods can be found in . Models with functional output and scalar inputs are called function-on-scalar models [9; 15]. They can be applied when the outcome or error function is assumed to be repeated realizations of a stochastic process. Combining these two approaches yields the function-on-function regression model with functions as input and output [see, e.g., 35; 53]. We provide a more technical description of function-on-function regression is given in Section 2.1.

Functional machine learningIn recent years, several machine learning approaches for functional data have been proposed. One is Gaussian process functional regression [GPFR; 21; 54] which combines mean estimation and a multivariate Gaussian process for covariance estimation. As for classical Gaussian process regression, the GPFR suffers from scalability issues. Gradient boosting approaches for functional regression models have been proposed by [4; 6]. These approaches result in sparse and interpretable models, rendering them especially meaningful in high dimensions, but lack expressivity provided by neural network approaches. Pioneered by the functional multi-layer perceptron (MLP) of , various researchers have suggested different functional neural networks (FNNs) [16; 44; 55], including MLPs with functional outcome (cf. Section 2.2 for technical details). A recent overview is given in . Existing approaches are similar in that they learn a type of embedding to represent functions in a space where classical computations can be applied (cf. Fig. 2(a)).

### Current limitation and our contribution

From the related literature, we can identify the most relevant methods suitable for our modeling challenge. Neural network approaches such as the functional MLP provide the most flexible class for functional data, whereas additive models and additive boosting approaches such as [6; 53] yield interpretable models. An additive combination of these efforts to obtain both an interpretable model part and the possibility of making the model more flexible is hence an attractive option.

**Open challenges** While semi-structured models are suited for a combination of interpretable additive structures and deep neural networks, existing approaches cannot simply be transferred for application with functional data. Due to the structure and implementation of SSNs, a naive implementation would neither be scalable to large-scale datasets nor is it clear how to incorporate identifiability constraints to ensure interpretability of the structured functional regression part in the presence of a deep neural network.

**Our contributions** In this work, we address these limitations and propose an SSN approach for functional data. Our method is not only the first semi-structured modeling approach in the domain of functional data, but also presents a more scalable and flexible alternative to existing functional regression approaches. In order to preserve the original properties of functional regression models, we further suggest an orthogonalization routine to solve the accompanied identifiability issue.

## 2 Notation and background

We assume functional inputs are given by \(J 1\) second-order, zero-mean stochastic processes \(X_{j}\) with square integrable realizations \(x_{j}:_{j}\), i.e. with \(x_{j} L^{2}(_{j}),j=1,,J\) on real intervals \(_{j}\). Similarly, a functional outcome is given by a suitable stochastic process \(Y\) over a compact set \(\) with realizations \(y:\), \(y L^{2}()=:\). For the functional inputs, we simplify notation by combining all (random) input functions into a vector \(X()=(X_{1}(s_{1}),,X_{J}(s_{J}))\) with realization \(x()^{J}\), where \(:=(s_{1},,s_{J})^{J}\) is a \(J\)-tuple from domain \(:=_{1}_{J}\), and \(:=L^{2}(_{1}) L^{2}(_{J})\).

Next, we introduce function-on-function regression (FFR) and establish a link to FNNs to motivate our own approach.

### Function-on-function regression

An FFR for the expected outcome \((t):=(Y(t)|X)\) of \(Y(t),t\) using inputs \(X\) can be defined as follows:

\[(Y(t)|X=x)=(b(t)+_{j=1}^{J}_{_{j}}w_{j}( s,t)x_{j}(s)ds),\] (1)

where \(b(t)\) is a functional intercept/bias and the \(w_{j}(s,t)\) are weight surfaces describing the influence of the \(j\)th functional predictor at time point \(s_{j}\) on the functional outcome at time point \(t\). \(\) is a point-wise transformation function, mapping the affine transformation of functional predictors to a suitable domain (e.g., \(()=()\) to obtain positive values in case \(Y\) is a count process). Various extensions of the FFR model in (1) exist. Examples include time-varying integration limits or changing the linear influence of \(x(t)\) to a non-linear mapping.

ExampleFigure 1 shows an example of a function-on-function regression by visualizing the corresponding learned weight surface, with three highlighted areas: a) An isolated positive weight multiplied with a positive feature signal at \(x(90)\) induces a small spike at \(y(10)\). b) A more extensive negative weight multiplied with mostly positive feature values for \(s\) and integrated over all time points results in a bell-shaped negative outcome signal at around \(t=30\). c) A large but faded positive weight region multiplied with positive feature values results in a slight increase in the response function in the range \(t\). In most applications of FFR, the goal is to find these salient areas in the weight surface to better understand the relationship between input and output signal.

Figure 1: Exemplary weight surface (center), feature signal (bottom), and the resulting response signal (left) when integrating \( x(s)w(s,t)ds\).

### Functional neural networks

To establish the connection between FFR and FNNs, it is instructive to consider a function-on-function MLP (FFMLP). In its basic form, an MLP for scalar values consists of neurons \(h:,x(b+w^{}x)\) arranged in multiple layers, each layer stacked one on top of the other. The extension to a fully functional \(L\)-layer MLP can be defined recursively by the \(k\)th output neuron \(h_{k}^{(l)}\) of a functional layer \(l=1,,L\) as

\[h_{k}^{(l)}(t)=^{(l)}(b_{k}^{(l)}(t)+_{m=1}^{M_{l-1}} w_{m,k}^ {(l)}(s,t)h_{m}^{(l-1)}(s)ds),\] (2)

where \(M_{l}\) denotes the number of neurons in layer \(l\), \(b_{k}^{(l)} L^{2}(_{m}^{(l)})\) and \(w_{m,k}^{(l)} L^{2}(_{m}^{(l-1)}_{m}^{(l)})\) for some functional domains \(_{m}^{(l)}\), input layer \(h_{m}^{(0)}()=x_{m}()\) for all predictors \(m=1,,J\), and last layer with \(M_{L}=1\) neuron \(h^{(L)}(t)=(Y(t)|X)\).

## 3 Semi-structured functional networks

Our approach generalizes the previous models by considering a general neural network \(:\) that models the input-output mapping from the set of functional features \(X\) to the expected outcome \((t)\) of the functional response for \(t\) as

\[(t)=(X)(t)=(^{+}(X)(t)+^{-}(X)(t)),\] (3)

comprising an FFR part \(^{+}(X)(t)\) and a deeper FNN architecture \(^{-}(X)(t)\), which are added and transformed using an activation function \(\). This combination can also be thought of as a functional version of a residual connection. In the following, we drop the functional input arguments of \(^{+}\) and \(^{-}\) for better readability. The model in (3) combines structured interpretable models as described in Section 2.1 with an arbitrary deep network such as the one in Section 2.2. In particular, this allows for improving the performance of a simple FFR while retaining as much interpretability as possible (cf. Section 3.3).

Interpretable model partAs in (1), we model the interpretable part \(^{+}(t)=_{j=0}^{J}_{j}^{+}(t)\) as a sum of linear functional terms \(_{j}^{+}(t)=_{_{j}}w_{j}(s,t)x_{j}(s)ds\). To make our model more concrete, we can expand each weight surface \(w_{j}\) in a finite-dimensional tensor-product basis as

\[w_{j}(s,t)=(t)^{}_{j}_{j}(s)\] (4)

over fixed function bases \(_{j}=\{_{jk}\}_{k=1}^{K_{j}},\ _{jk} L^{2}( _{j})\) and \((t)=\{_{u}\}_{u=1}^{U},\ _{u} L^{2}()\) with weight matrix \(_{j}^{U}^{K_{j}}\). Analogously, we can represent the intercept as \(_{0}^{+}(t)=b(t)=(t)^{}_{0}\). We may understand the model's interpretable part for the \(j\)th feature as a functional encoder \(_{j}^{*}(X_{j})=\{_{jk}^{*}(X_{j})=_{jk}(s)X_{j}( s)ds\}_{k=1}^{K_{j}}\), encoding \(X_{j}\) into a latent variable \(_{j}^{K_{j}}\), and a linear decoder \(\) with weights \(_{j}\) mapping \(_{j}\) to the function \(_{j}^{+}(t)\):

\[X_{j}_{j}^{*}}{}_{j} _{j}}{}_{j}^{+}.\]

Here, \(_{j}^{*}\) presents the dual basis to \(_{j}\). Visually, the weight surface \(w_{j}\) in (4) can be interpreted as exemplarily shown in Figure 1.

Deep model partThe part \(^{-}(t)\) in (3) is a placeholder for a more complex network. For FNNs, a conventional neural network might be applied after encoding \(X\) with \(^{*}\) and before decoding its results with \(\), such that en- and decoding are shared with the interpretable part \(^{+}\) as depicted in Fig. 1(a). Alternatively, a general FFMLP as described in Section 2.2 can be embedded into the approach in Fig. 1(a). As researchers have often already found well-working architectures for their data, a practically more realistic architecture for semi-structured FNNs is to allow for a more generic deep model as depicted in Fig. 1(b). This is also the case in our application on biomechanical sensor data in Section 4.2. Here, we choose \(^{-}(t)\) to be a specific InceptionTime network architecture  that is well-established in the field and train its weights along with the weights \(\) of \(^{+}(t)\).

### Implementation for discretized features

As functional predictors are usually only observed on a discretized grid on the domains \(_{j}\), we now derive a way to implement \(^{+}(t)\) in practice. Depending on the architecture used for the deep part, we might proceed analogously with \(^{-}(t)\). Let \(x_{j}^{(i)}(s_{r})\) be the evaluation of the \(i\)th realization \(x_{j}^{(i)}\) of \(X_{j}\) at time points \(s_{r},r=1,,R\), stacked into a vector \(_{j}^{R}\). For better readability, we assume that these time points are equal across all \(J\) features. With only discrete evaluations available, we approximate integrals over \(_{j}\) numerically with integration weights \((s_{r})\), e.g., using trapezoidal Riemann weights. Given the tensor-product basis representation of the weight surface \(w_{j}(s,t)\) in (4), we effectively evaluate \(_{j}(s_{r})\) for all \(R\) time points, yielding \(_{j}=[_{j}(s_{1}),,_{j}(s_{R})]^ {K_{j} R}\), and obtain the row-vector \(_{j}^{*}\) of approximate functionals

\[_{jk}^{*}(x_{j})=_{jk}(s)x_{j}(s)ds_{r=1}^{R}_{j }(s_{r})_{jk}(s_{r})x_{j}(s_{r})\]

as

\[_{j}^{*}=(_{j}_{j})_{j}^{} ^{1 K_{j}},\]

where \(\) denotes the Hadamard product and \(_{j}=[_{j}(s_{1}),,_{j}(s_{R})]^{}\). Putting everything together, we can represent the \(j\)th interpretable functional model term \(_{j}^{+}(t)\) as

\[_{j}^{+}(t)=_{_{j}}x_{j}(s)w_{j}(s,t)ds_ {j}^{*}_{j}(t).\] (5)

This model part is still a function over the outcome domain \(\), but discretized over the predictor domains. We can now proceed with the optimization when the functional outcome is represented with finitely many observations.

### Optimization for discretized outcomes

For the \(i\)th observation, we can measure the goodness-of-fit of our model \((t)\) by taking the point-wise loss function \(l(y^{(i)}(t),^{(i)}(t))\) of the \(i\)th realized function \(y^{(i)}\) and the predicted outcome \(^{(i)}\) given \(^{(i)}\) and integrate over the functional domain to obtain the \(i\)th functional loss contribution \(\), i.e.,

\[(y^{(i)},^{(i)})=_{}l(y^{(i)}(t),^{(i)}( t))dt.\] (6)

The overall objective, our empirical risk, is then given by the sum of (6) over all \(n\) observations. In practice, integrals in (6) are not available in closed form in general and the functional outcome is only observed on a finite grid. Therefore, similar to the discretization of the weight surface, let \(y^{(i)}(t_{q})\) be the observations of the outcome for time points \(t_{q},q=1,,Q\), summarized for all time points as \(^{(i)}=(y^{(i)}(t_{1}),,y^{(i)}(t_{Q}))\). Given a dataset \(\) with \(n\) observations of these functions, i.e., \(=\{(^{(i)},^{(i)})\}_{i=1,,n}\), our overall objective then becomes

\[_{i=1}^{n}_{q=1}^{Q}(t_{q})l(y^{(i)}(t_{q}),^{(i)}(t_ {q})),\] (7)

where \(()\) are integration weights.

As it less efficient to represent \(\) as an actual function of \(t\) if \(Y\) is only given for fixed observed time points \(t_{q}\), we can adapt (5) to predict a \(Q\)-dimensional vector using

\[_{j}^{+}_{j}^{*}_{j}^{1 Q}\] (8)

Figure 2: Different semi-structured architectures.

[MISSING_PAGE_FAIL:6]

\(_{\{N\}}\), and \(_{\{N\}}^{}\) are the corrected deep network predictions orthogonal to the interpretable part. Using \(}_{j}\) from \(}=[}_{1},,}_{J}]\) instead of the original \(_{j}\)s will yield updated weights \(_{j}\) for the surfaces from (4) of the interpretable functional model part, which can be interpreted irrespective of the presence of a deep network.

### Scalable implementation

When implementing semi-structured FNNs, \(^{-}\) can be chosen arbitrarily to the needs of the data modeler. Hence, we do not explicitly elaborate on a particular option here and assume an efficient implementation of this part. Instead, we focus on a careful implementation of the structured model part to avoid unfavorable scaling as in other existing implementations (cf. Figure 4(b)). To mitigate this problem (in particular reducing the space complexity), we propose two implementation tricks. While these mainly reduce the complexity scaling w.r.t. \(J\) and \(R\), implementing an FFR in a neural network already yields an improvement by allowing to cap the memory costs to a fixed amount through the use of mini-batch training. In contrast, a full-batch optimization routine of (7) as implemented in classic approaches (e.g., [5; 53]) scales both with \(n\) and \(Q\) as data is transformed in a long-format, which can grow particularly fast for sensor data. For example, only \(n=1000\) observations of a functional response evaluated at \(Q=1000\) time points already results in a data matrix of 1 million rows. Classic implementations also scale unfavorably in terms of the number of features \(J\) and observed times points \(R\) for these functions. In the case where all functional predictors are measured on the same scale, fitting an FFR model requires inverting a matrix of size \((n Q)((_{j=1}^{J}K_{j}) U)\), which becomes infeasible for larger amounts of predictors. Even setting up and storing this matrix can be a bottleneck.

**Array computation**: In contrast, when using the representation in (5), the basis matrices in \(s\)- and \(t\)-dimension (i.e. for in- and output) are never explicitly combined into a larger matrix, but set up individually, and a network forward-pass only requires their multiplication with the weight matrix \(\). As the matrix \(\) is the same for all \(J\) model terms, we can further recycle it for these computations.
**Basis recycling**: In addition, if some or all predictors in \(X\) share the same time domain and evaluation points \(s_{1},,s_{R}\), we can save additional memory by not having to set up \(J\) individual bases \(_{j}\).

## 4 Numerical experiments

In the following, we show that our model is able to recover the true signal in simulation experiments and compares favorably to state-of-the-art methods both in simulated and real-world applications. As comparison methods, we use an additive model formulation of the FFR model as proposed in , an FFR model based on boosting  and a deep neural network without interpretable model part. For the latter and the deep part of the semi-structured model, we use a tuned InceptionTime  architecture from the biomechanics literature . As FFR and boosting provide an automatic mechanism for smoothing, no additional tuning is required. In all our experiments, we use thin plate regression splines  but also obtained similar results with B-splines of order three and first-order difference penalties.

**Hypotheses**: In our numerical experiments, we specifically investigate the following hypotheses:

* **Comparable performance with better scalability**: For additive models without the deep part, our model can recover complex simulated function-on-function relationships (**H1a**) with similar estimation and prediction performance as other interpretable models (**H1b**) while scaling better than existing approaches (**H1c**).
* **Favorable real-world properties**: In real-world applications, our model is on par or better in prediction performance with current approaches (**H2a**), with a similar or better resemblance when generating output functions (**H2b**), and provides a much more meaningful interpretation than the deep MLP (**H2c**). Further, we conjecture that our approach is better in prediction performance than its two (structured, deep) individual components alone (**H2d**).

### Simulation study

Prediction and estimation performance (H1a,b)For the model's estimation and prediction performance, we simulate a function-on-function relationship based on a complex weight relationship \(w(s,t)\) between one functional predictor and a functional outcome. Figure 4(a) depicts this true weight along with an exemplary estimation by the different methods. The estimation performance of the weight surface and all methods' prediction performance over 30 different simulation runs each with a signal-to-noise ratio \(\{0.1,1\}\) and \(n\{320,640,1280\}\) functional observations is summarized in Figure 6 in the Appendix. While the models perform on par in most cases, we find that the network works better for more noisy settings (\(=0.1\)) and is not as good as other methods for higher signal-to-noise ratio (as also shown in Figure 4(a)), but with negligible differences. Overall, there are only minor differences in both estimation and prediction performance. This result suggests that our method works just as well as the other methods for estimating FFR.

Scalability (H1c)To investigate the scalability of all methods, we subsample the data from the Carmago study presented in the following subsection by reducing observations \(n\), features \(J\), and/or the number of observed time points \(R\). More specifically, we investigate the memory consumption of the three implementations previously discussed (additive model, boosting, neural network) for \(n\{25,50,100\}\) functional observations, \(Q=R\{25,50,100\}\) observed data points, and \(J\{1,2,4\}\) functional features. We restrict this study to rather small functional datasets as the memory consumption of other methods is superlinear. This can also be seen in Figure 4(b) presented earlier, which summarizes the result of this simulation study and clearly shows the advantage of our implementation compared to existing ones form [5; 53].

### Real-world datasets

Our method being originally motivated by applications in the field of biomechanics, we now analyze two real-world datasets with biomechanical sensor data. In addition to these applications in biomechanics, we provide further applications to EMG-EEG synchronization, air quality prediction and hot water consumption profiles in Appendix C to demonstrate the versatility of our approach.

#### 4.2.1 Fukuchi and Liew datasets (H2a)

The data analyzed in the first experiment is a collection of three publicly available running datasets [14; 27; 28]. A recent study of  used this data set to show the potential of deep learning approaches for predicting joint moments during gait (in total 12 different outcome features). The given data set collection is challenging as it provides only \(n=490\) functional samples but \(J=27\) partially highly correlated functional features. We follow the preprocessing steps of  and split the data into 80% training and 20% test data for measuring performance according to the authors' split indices.

Figure 4: Simulation study results.

As predictors we use the gait cycle of all available joint measurements, that is the 3D joint angle, velocity, and acceleration of the bilateral ankle, knee, and hip joints. Predictors in both training and test datasets are separately demeaned and scaled using only the information from the training set. As in the simulation studies, we run the additive model, Boosting, and our approach and compare the performance across all 6 different outcome variables. We use the relative and absolute root mean squared error (RMSE) as suggested in  and the Pearson correlation coefficient  to measure performance. In contrast to our simulation study, the additive model implementation of FFR cannot deal with this large amount of functional features. We, therefore, split our analysis into one comparison of the additive model, an FNN, and a semi-structured FNN (SSFNN) on a selected set of features, and a comparison of SSFNN against boosting (which has an inbuilt feature selection).

Results in Table 1 summarize test performances across all 12 different outcome types. We see that the (SS)FNN approaches perform equally well or better compared to the additive model on the selected set of features, and SSFNN also matches the performance of Boosting when using all features.

#### 4.2.2 Carmago data set (H2a-c)

The second data set is a publicly available dataset on lower limb biomechanics in walking across different surfaces level-ground walking, treadmill walking, stair ascent, stair descent, ramp ascent, and ramp descent . Due to the size of the data set consisting of 21787 functional observations, \(Q=R=101\) observed time points, and \(J=24\) predictors, it is not feasible to apply either additive model-based FFR or boosting. We, therefore, compare SSFNN to a (structured) neural-based FFR model and a deep-only neural network. Based on the pre-defined 70/30% split for training and testing, we run the models for each of the 5 outcomes (joints) and compute the relative MSE difference, a functional \(R^{2}\) analogon with values in \((-,1]\) (see Appendix A.1 for definition). Table 2 shows the results averaged for all outcome types while Figure 5 provides performance changes individually for the 5 different joints.

We observe that the semi-structured model performs better than the deep model, which in turn is better than the structured model. Figure 8 in the Appendix further depicts the predicted functions for all 5 outcomes using the 3 models. While for some joints the prediction of the semi-structured and deep-only model is very similar, the performance for the subtalar joint is notably better when combining a structured model and the deep neural network to form a semi-structured model.

  & Add. Model (sel.) & FNN (sel.) & SSFNN (sel.) & Boosting (all) & SSFNN (all) \\  rel. RMSE (\(\)) & 0.36 (0.08) & 0.31 (0.08) & **0.30** (0.03) & 0.30 (0.10) & **0.25** (0.04) \\ 

Table 1: Median (and mean absolute deviation in brackets) of the relative RMSE across all outcome responses in the Fukuchi and Liew datasets for different methods (columns) using either selected (sel.) or all features (all). The best method is highlighted in bold.

 Structured & Deep & Semi-Structured \\ 
0.872 (0.094) & 0.923 (0.065) & **0.955 (0.030)** \\ 

Table 2: Mean performance (standard deviation in brackets) of the relative MSE difference (a value of 1 corresponds to the optimal model with zero error) across all five outcomes.

Figure 5: Comparison of performance improvements (larger is better) in mean squared error (MSE) for different joints (outcomes).

Discussion

Our proposed method is an extension of semi-structured networks to cater to functional data and deals with the issue of identifiability by proposing a functional extension of post-hoc orthogonalization. Using array computations and basis recycling the method also solves scalability issues present in existing methods. Our experimental results reveal that functional semi-structured networks yield similar or superior performance compared to other methods across varied biomechanical sensor datasets.

**Novelty** The approach proposed in Section 3 shows similarities to an autoencoder architecture. While a functional autoencoder has been proposed in the past  and most recently by , these studies focus on the representation learning (and reconstruction) of the same signal. Other papers focusing on function-on-function regression (e.g., ) suggest similar approaches to ours but without the option to jointly train a structured model and a deep network.

**Limitations and Future Research** While our model shows good performance on various datasets and in simulations, we believe that our approach can be further improved for applications on high-dimensional sensor data by incorporating appropriate sparsity penalties. Due to the functional nature of the data, feature selection would result in a great reduction of complexity and thereby potentially yield better generalization. Our general model formulation would further allow the extension to a non-linear FFR model \(^{-}(t)=_{j=1}^{J}_{_{j}}f_{j}(x_{j}(s),t)ds\) using smooth functions \(f_{j}\) in three dimensions (feature-, \(s\)- and \(t\)-direction). Although this extension is still an additive model in the functional features, it is challenging to interpret the resulting additive effects.