# Scaling Open-Vocabulary Object Detection

Matthias Minderer Alexey Gritsenko Neil Houlsby

Google DeepMind

{mjlm, agritsenko, neilhoulsby}@google.com

###### Abstract

Open-vocabulary object detection has benefited greatly from pretrained vision-language models, but is still limited by the amount of available detection training data. While detection training data can be expanded by using Web image-text pairs as weak supervision, this has not been done at scales comparable to image-level pretraining. Here, we scale up detection data with self-training, which uses an existing detector to generate pseudo-box annotations on image-text pairs. Major challenges in scaling self-training are the choice of label space, pseudo-annotation filtering, and training efficiency. We present the OWLv2 model and OWL-ST self-training recipe, which address these challenges. OWLv2 surpasses the performance of previous state-of-the-art open-vocabulary detectors already at comparable training scales (\(\)10M examples). However, with OWL-ST, we can scale to over 1B examples, yielding further large improvement: With a ViT-L/14 architecture, OWL-ST improves AP on LVIS rare classes, _for which the model has seen no human box annotations_, from 31.2% to 44.6% (43% relative improvement). OWL-ST unlocks Web-scale training for open-world localization, similar to what has been seen for image classification and language modelling. Code and checkpoints are available on GitHub.1

## 1 Introduction

Object detection is a core computer vision task with many real-world applications. Consequently, there is great interest in improving detection models, especially in the open-vocabulary domain. For image-level tasks, large improvements have been achieved through contrastive pretraining of vision-language models, which is massively scalable because it can use naturally abundant weak supervision in the form of image-text pairs from the Web . Since no such natural supervision data exists for localization tasks, open-vocabulary detection models typically build on pretrained image-level encoders . However, due to the scarcity of detection data and the fragility of pretrained representations, detection-training stages of these models have typically had to be relatively brief, which limits final detection performance and scaling potential.

The scarcity of detection data can be addressed with _self-training_. In self-training, an existing detector is used to predict bounding boxes on unlabeled images to generate data for training better detectors . By combining open-vocabulary detectors with Web image-text data, such pseudo-labeling can produce practically unlimited amounts of open-vocabulary detection training data that leverages the image-associated text for semantic supervision. While several works have applied various forms of self-training to open-vocabulary object detection , they have done so at relatively small scales, comparable to the size of human-annotated detection datasets and much smaller than the datasets used for image-level training.

To scale detection self-training further, we take guidance from image-level methods, where the principle has been to leverage weak supervision in the largest possible amount [30; 12; 29; 42]. We identify three key ingredients for optimizing the use of weak supervision for detection: choice of label space, filtering of pseudo-annotations, and training efficiency. Prior methods have typically used human-curated label spaces or complex concept mining [47; 39; 38; 46] and strict filtering, keeping just the single largest  or highest-scoring  pseudo-box for each image. In contrast, we argue that we should "let the data do the work" and therefore apply little processing and filtering. We propose to simply use all possible N-grams of the image-associated text as detection prompts for that image, and apply only weak confidence filtering to the resulting pseudo-labels.

We apply this self-training recipe to the OWL-ViT detection architecture  and call it OWL-ST. To increase the number of examples seen for a given amount compute, we also introduce OWLv2, an optimized architecture with improved training efficiency. Combining the OWL-ST recipe with the OWLv2 architecture surpasses prior state-of-the-art methods already at moderate amounts of self-training, comparable to training amounts of previous methods (Figure 1). Scaling self-training to billions of examples yields further large improvements. For example, our ViT-L/14-based model, trained on 2.38 image-text pairs and fine-tuned on LVISbase, achieves 44.6% zero-shot LVIS \(_{}\), which is a 36% relative improvement over the prior state of the art (32.8% \(_{}\) for F-VLM R50x64 ). Our largest model, ViT-G/14, reaches 47.2% \(_{}\).

We also evaluate our models on a suite of "in the wild" datasets  and study the trade-off between fine-tuned and open-vocabulary performance. We find that strong in- and out-of-distribution performance is possible with weight ensembling . Finally, our analysis of the scaling behavior of OWL-ST suggests that self-training has further potential for leveraging abundantly available weak supervision for open-vocabulary object detection.

## 2 Related Work

### Scaling Vision Models

Vision models have recently seen large advances in model and training scale, leading to improved performance on many image-level tasks. On the architecture side, Vision Transformers have been shown to scale more efficiently than prior architectures . Task performance improves predictably as training data and compute are increased , with recent work showing continued improvements for models with up to 22 billion parameters . We apply these findings to object detection.

Figure 1: Overview of our method. **Left:** Our method has three steps: (1) Generate pseudo-box annotations on WebLI with OWL-ViT L/14, queried with caption N-grams. (2) Train new models on pseudo-annotations. (3) Optionally, fine-tune on human annotations. **Right:** Zero-shot detection performance on LVIS\({}_{}\) after fine-tuning on LVISbase. Neither the annotator nor our models have seen any human-generated box annotations for LVIS\({}_{}\) classes. Our self-training approach improves over other methods even at moderate amounts of training (e.g. the OWL-L/14 model we use as annotator; black \(\)), and continues to improve as training is scaled up. Horizontal black lines indicate previous state-of-the-art open-vocabulary detectors which did not see LVIS\({}_{}\) classes during training.

On the data side, contrastive pretraining of vision-language models (VLMs)  has unlocked the use of abundantly available image-text pairs from the Web as weak supervision, with improved results if more data is used [12; 28]. VLMs, which embed images and text into a shared space, also enable open-vocabulary applications where prior models were limited to fixed label spaces. Here, we use pretrained CLIP  and SigLIP  encoders as backbones for our detector.

### Open-Vocabulary Object Detection

Much recent work aims to transfer the open-vocabulary capabilities of VLMs to localization tasks such as object detection. A first wave of VLM-based object detection methods either distilled VLM-predictions for cropped image regions (e.g. **ViLD**), or added detection heads directly to frozen (**F-VLM**) or fine-tuned (**OWL-VIT**) VLM encoders. A challenge identified by these works is to protect the VLM from forgetting its open-vocabulary knowledge while training the detection heads on the relatively little available detection data.

### Scaling Open-Vocabulary Detection with Weak Supervision

Given that earlier methods identified detection data as a limiting factor in open-vocabulary detection performance, more recent works focus on using weak supervision directly for detection training, rather than just during VLM pretraining. There are two main approaches:

Some methods use _self-training_, in which an existing detector is used to predict pseudo-boxes for images where image-level labels or captions, but no human box annotations, are available. Better detectors can then be trained on the pseudo-annotations. For example, **RegionCLIP** generates pseudo-boxes using nouns parsed from image captions and uses those boxes for localization pretraining. **Detic** predicts class-agnostic pseudo-boxes on images for which classification labels are available and associates the largest predicted box with the image label. Similar to our approach, **3Ways** uses an existing open-vocabulary detector to predict pseudo-boxes on captioned images, but uses the whole caption as a prompt, instead of dividing it into multiple prompts as we do.

Other methods propose grounding losses that directly train a detector on weak supervision such as image-level labels or captions. These methods pretrain models to align class-agnostic pseudo-boxes with words from image-associated text and rely on human-generated detection data for fine-tuning. Major examples of this approach are **GLIPv1/v2** and [22; 45] and **DetCLIPv1/v2**[39; 38].

In principle, these approaches unlock Web-scale training for detection, but prior methods rarely go much beyond 10M examples and instead focus on the model architecture and training loss. Here, we keep architecture and loss simple, and focus on scaling up the training data, since this was successful for image-level models. A similar approach was recently applied with good results to class-agnostic segmentation in the **Segment Anything** work . Together with our results on text-conditioned localization, this suggests that scaling up self-training is a powerful and general method for improving performance on fine-grained vision tasks.

## 3 Method

We propose a simple self-training approach with three steps: (1) Use an existing open-vocabulary detector to predict bounding boxes for a large Web image-text dataset. (2) Self-train a new detector on the pseudo-annotations. (3) Optionally, fine-tune the self-trained model briefly on human-annotated detection data (Figure 1, left). Our goal is to optimize the key components of this approach--label space, annotation filtering, and training efficiency--such that it provides strong and scalable open-vocabulary performance with few human annotations.

### Generating Web-Scale Open-Vocabulary Object Annotations

We use the WebLI dataset  as the source of weak supervision for self-training. WebLI is a large dataset of images and texts available on the public Web. The dataset consists of approximately 10B images and associated alt-text strings, which can be thought of as noisy image captions. For images whose alt-text is not in English, we use an automatically generated English translation .

We use OWL-ViT CLIP-L/14  to annotate all 10B WebLI images with bounding box pseudo-annotations. OWL-ViT is an open-vocabulary object detector. Given an image, the model first detects objects in the image in a class-agnostic way. Then, given a list of free-text queries, the model produces scores indicating the likelihood that each detected object is associated with each text query.

A crucial design choice for open-vocabulary pseudo-labeling is the annotation label space. Methods in the literature vary widely but typically fall somewhere between two extremes: (1) use a fixed, human-curated label space for all images (e.g. ), or (2) machine-generate per-image queries from image-associated text (e.g. ). We implement both and compare their performance in Section 4.3.

**Human-curated label space.** We performed one pseudo-annotation run by combining the label sets from the LVIS , Objects365 , OpenImagesV4 , and Visual Genome  datasets and removing duplicates and plural forms. In total, this label space contains 2520 common object categories, e.g. "phone", "goatee", "teakette", "park", "suit (clothing)". See Appendix A.2 for code to generate the full list. Models trained on this label space may not be considered fully _open-vocabulary_ for evaluation datasets whose classes were included in the pseudo-annotation label space (e.g. LVIS), since the evaluation vocabulary is known at training time in this case. However, LVIS\({}_{}\) classes are still _unseen_ for all of our models, in the sense that neither the annotator nor the self-trained models have ever seen human box annotations for LVIS\({}_{}\) classes.

**Machine-generated label space.** In a second pseudo-annotation run, we automatically generated queries from the image-associated text. Prior work using image captions as weak supervision for detection often used grammatical parsing to extract noun phrases or concepts [46; 39; 38]. These approaches may add biases that reduce the diversity of extracted queries. To keep such biases to a minimum, we use no grammatical parsing and simply extract all word N-grams up to length 10 from the text associated with a given image and use them as queries for that image. We apply minimal filtering, only removing generic terms like image or png, and queries consisting entirely of stop-words (details in Appendix A.3). Note that, since OWL-ViT uses late image-text fusion, the quality of box _localization_ (as opposed to classification) is not affected by the chosen label space.

Regardless of label space, we ensemble predictions over seven prompt templates such as "a photo of a {}" as described in . For each predicted box, we keep the query with the highest score as its pseudo-label. For each image, we keep all boxes above a score threshold. We study the choice of threshold in Section 4.4. The pseudo-annotations are used as hard labels for self-training.

### Self-training at Scale

We now describe how we use the pseudo-annotations to train better detectors. We use a variant of the OWL-ViT architecture  as described below. The image and text encoders are initialized from contrastively trained image-text models (CLIP, unless noted otherwise); the detection heads are randomly initialized. All models are first trained exclusively on pseudo-annotations ("self-training"). In an optional separate step, models are fine-tuned briefly on human-annotated detection data.

Self-training proceeds similarly to detection training in . In particular, we use the same losses and also augment queries with "pseudo-negatives" that are randomly sampled from the queries of other images, similar to batch negatives in . Due to the size of our dataset, in contrast to , we use no random prompt templates and fewer image augmentations (details in Appendix A.5).

Prior work on image-level tasks shows that pretraining improves performance on downstream tasks well beyond 1 billion examples seen [44; 12; 28; 42], across model sizes. We hypothesize that similar scaling applies to detection self-training. We therefore optimize training efficiency to maximize the number of images seen for a given amount of training compute as follows.

**Token dropping.** Vision Transformers represent images as an unordered sequence of tokens. Tokens can therefore be reorganized or dropped without changing the model parameters. Various forms of token dropping or pooling have been proposed to improve efficiency [24; 32; 41; 25; 2]. Here, we drop tokens simply based on the pixel variance of the corresponding image patch. Both natural and Web images contain low-variance areas devoid of useful information, e.g. sky, single-color backgrounds, or padding. We find that the lower half of image patches by mean pixel variance can be dropped without loss in detection performance (Appendix A.6). We therefore drop 50% of patches in all of our experiments during training. No patches are dropped during inference.

Instance selection.OWL-ViT is an encoder-only architecture and predicts one bounding box per encoder token. This is inefficient, since there are typically many more encoder tokens than objects (e.g. 5184 tokens for resolution \(1008 1008\) and patch size \(14 14\)). Most output tokens therefore do not represent objects. We introduce an _objectness head_ which predicts the likelihood that an output token actually represents an object, and compute boxes, class scores, and losses only for the top \(k\) tokens by objectness, similar to Efficient DETR . The objectness head receives an encoder token as input and computes a scalar objectness score. The objectness score predicts the future classification score of a token and is supervised by the actual classification score of those tokens that end up being selected and passed on to the classification head. We select approximately 10% of instances by top objectness during training in all of our experiments. During inference, all instances are used.

Mosaics.During self-training, we combine raw images into grids of up to \(6 6\) to produce a single training example (i.e. a more extreme version of the mosaics in ). This has two main motivations: (1) Using mosaics increases the number of raw images seen for a given fixed model input resolution. An alternative is to train using variable image sizes , but this would require resizing image position embeddings for each input size. (2) The average resolution and complexity of Web images is lower than images in detection benchmarks and applications. Mosaics reduce the average object size and improve small-object performance, similar to large scale-jittering , but with less padding. For all self-training experiments, we use \(1 1\), \(2 2\),\(3 3\), \(4 4\), and \(6 6\) grids in equal proportions, resulting in an average of 13.2 raw component images per training example.

To further improve training efficiency, we also adopt previously proposed practices for large-scale Transformer training  (details in Appendix A.7). Together, our improvements reduce training FLOPS by approximately 50% compared to the original OWL-ViT  and increase training throughput by \(2\) (e.g. for L/14 at \(840 840\) resolution measured on TPUv3: GFLOPs/example 11'945.4 vs. 5357.9; examples/s/core 1.0 vs. 2.2). We refer to the improved model as OWLv2.

At inference, no token dropping or instance selection is performed. Inference is therefore identical to the original OWL-ViT, i.e. each image encoder token is decoded into a bounding box and a list of per-query classification scores.

    &  & Self-training & Self-training & Human box & OInW & LVIS & LVIS & LVIS & LVIS \\  & & data & vocabulary & annotations & 13 & \(^{}_{}\) & \(^{}_{}\) & \(^{}_{}\) & \(^{}_{}\) \\  _Open vocabulary_ (_evaluation vocabulary_ & _evalation vocabulary_ & _is not available at training time):_ & & & & & & & \\
1 & RegionCLIP  & R50x4 & CC3M & 6k concepts & LVIS\_base & – & – & – & 32.3 & 22.0 \\
2 & OWL  & CLIP B/16 & – & – & 0365+VG & – & – & – & 27.2 & 20.6 \\
3 & OWL  & CLIP L/14 & – & – & 0365+VG & 48.4 & – & – & 34.6 & 31.2 \\
4 & GLIPV2  & Swin-T & Cap4M & tokens & W365+GoldG & 48.5 & 29.0 & – & – \\
5 & GLIPV2  & Swin-B & CC15M & tokens & FiveObx+GoldG & 54.2 & 48.5 & – & – \\
6 & GLIPV2  & Swin-H & CC15M & tokens & FiveObx+GoldG & 55.5 & 50.1 & – & – \\
7 & \(_{}\), i.e. the LVIS dataset  with all annotations for "rare" categories removed. Therefore, neither the annotator nor any of our models have seen human-generated annotations for LVIS\({}_{}\) classes. Fine-tuning uses mosaics up to \(3 3\) and is always done until the model has seen 256'000 mosaics (1.1M individual images, roughly equivalent to 100 LVIS epochs). The image size is \(960 960\) for /16 models and \(1008 1008\) for /14 models. See Appendix A.8 for a complete list of hyperparameters.

Evaluation.We use mean average precision (mAP) on LVIS  as our main detection metric, where mAP\({}_{}\) indicates open-vocabulary performance on unseen classes. To measure generalization on diverse real-world tasks, we evaluate zero-shot performance on the "Object Detection in the Wild" (ODinW) benchmark . ODinW is a suite of datasets covering a wide range of domains. We report the average mAP on the subset of 13 ODinW datasets introduced in  and provide performance on individual datasets in Appendix A.9.2. To avoid leakage of evaluation data into the training set, WebLI was filtered to remove images similar to those in the train, validation, and test splits of 68 common computer vision datasets, including COCO/LVIS, Objects365, and Visual Genome, but not the ODinW datasets (see  for details).

Figure 2: Comparison of pseudo-label spaces. Self-training on a human-curated list of classes yields good downstream performance on these classes, but generalizes poorly to unseen classes and datasets. Open-vocabulary generalization can be improved by obtaining weak but diverse supervision from image-associated text. WebLI image-text data was pseudo-annotated using OWL-ViT CLIP-L/14 with one of three label spaces: _Curated vocabulary_ (the union of label spaces from LVIS, Objects365, OpenImagesv4, and Visual Genome), _N-grams_ (lightly filtered N-grams from the text associated with each image), or a combination of both (_N-grams + curated_). OWLv2-B/16 models were then self-trained on the pseudo-annotations and fine-tuned on LVIS\({}_{}\). Each point represents a separate fine-tuning run. “Examples seen” refers to the number of images after creating mosaics; the total number of raw images seen is \(13.2\) that number (Section 3.2).

### Main Result

We compare our best models to the literature in Table 1. We broadly include state-of-the-art open-vocabulary detectors in the comparison. Our self-training approach, using only machine-generated pseudo-annotation queries, improves over previous methods even without fine-tuning (Table 1, OWL-ST, rows 11-13). Our OWL-ST B/16 model (row 11) achieves 29.6% LVIS mAP\({}_{}\), 9 points more than the equivalent OWL-ViT model (row 2). Our largest model, G/14 (row 13), reaches 37.5% mAP\({}_{}\), 4.7 points better than the next-best model from the literature (F-VLM R50x64, row 8). Interestingly, after self-training, our models perform _better_ on LVIS mAP\({}_{}\) than mAP\({}_{}\) (which includes frequent and common classes). We speculate that this may be because weak Web-data supervision may be better for specific terms than general terms: Image/text pairs involving unusual objects (such as LVIS\({}_{}\) categories) may be more likely to be specifically about these objects, whereas common terms like "person" or "car" may occur often without being related to the image.

Fine-tuning on LVIS\({}_{}\) provides additional significant improvements, even on mAP\({}_{}\) (OWL-ST+FT, rows 14-16). Our best model, which has only seen machine-generated queries during self-training, reaches 47.2% LVIS mAP\({}_{}\) after fine-tuning, a 14.4-point improvement over the next best model (F-VLM R50x64, row 8).

Including a human-curated list of common object classes as pseudo-annotation queries can further improve the results on LVIS (rows 20-21), but this approach is not fully open-vocabulary since the model sees a curated label space, including the LVIS classes, at training time. While the benefit of the curated label space is significant for our smallest model, is is minor on mAP\({}_{}\) for the larger L/14 model (compare rows 15 and 21).

To measure more general open-world performance, Table 1 also includes zero-shot results on ODinW13 , a suite of "in the wild" datasets. Performance on ODinW is best right after self-training and is reduced by fine-tuning on LVIS\({}_{}\). We discuss this further in Section 4.6. We also fine-tuned on COCO, where our B/16 and L/14 models reach 54.3% and 56.0% COCO mAP, respectively. OWLv2 therefore matches the performance of ViTDet with a Cascade Mask-RCNN head , despite using a simpler head architecture. Further results and examples in Appendix A.9.

### Pseudo-Annotation Label Space

Figure 2 takes a closer look at the impact of the pseudo-annotation label space on performance after fine-tuning. Performance on _fine-tuned classes_ (mAP\({}_{}\); left plot) is highest if the pseudo-annotation label space included these classes (blue circles). Therefore, if the target label space is known ahead of time, pseudo-labeling on that space leads to the best results.

However, performance on _unseen classes_ (mAP\({}_{}\)) and "In the Wild" datasets is much better if the pseudo-labeling included diverse queries that were machine-generated from the image-associated text (orange squares and green diamonds). A mixture of human and machine-generated label spaces

Figure 3: Impact of pseudo-annotation filtering by detection confidence on self-training effectiveness. Pseudo-labels (N-gram label space) were filtered using different confidence thresholds. Number of remaining images for each threshold: 0.1: 5B, 0.3: 2B, 0.5: 782M, 0.7: 224M. OWLv2-B/16 detectors were self-trained on the filtered pseudo-annotations and fine-tuned on LVIS\({}_{}\). Each point represents a different fine-tuning run. “Examples seen” refers to the number of images after creating mosaics; the total number of raw images seen is \(13.2\) that number (Section 3.2).

performs well in all settings, but does not significantly outperform the purely machine-generated label space on the "In the Wild" datasets. These results suggest that a human-curated label space can help if the target label space is known, but that strong in-the-wild generalization is driven by the weakly supervised machine-generated label space. Our results also show that a simple N-grams approach is sufficient to leverage the weak supervision and outperforms more complex methods (Table 1).

### Filtering of Pseudo-Annotations

Besides the label space, a second important decision in self-training is the filtering of pseudo-annotations. We filter based on the detection confidence score of the annotator and vary the score threshold in Figure 3. For confidence-based filtering, a bias-variance trade-off exists between including only high-confidence pseudo-annotations but inherting the annotator's biases, or lowering the bias but increasing the noise by including lower-confidence pseudo-annotations. Many prior works err on the side of high bias and low variance, applying high confidence thresholds  or including only the single highest-confidence detection for an image [47; 1]. In our setting, we find that including all pseudo-annotations that pass a moderate threshold of 0.3 works well, while strict thresholds lead to poor results (Figure 3). As training continues for longer than what was possible for Figure 3, we suspect that lower thresholds may scale better. Therefore, for our main results, we chose to include all annotations above 0.1, but only kept images with at least one annotation above 0.3.

### Scaling

The use of abundant Web image-text data with little filtering means that our self-training dataset is large (approximately 2B images). We can therefore study detection training scaling in the same regime as prior work on classification (Figure 4; models see each image at most once for these experiments). We make several noteworthy observations:

1. Self-training is beneficial already at moderate compute budgets, less than that of the annotator.
2. Models show similar scaling behavior for detection as for classification : Both overall performance and the size of the Pareto-optimal model increase with compute/data size.
3. As we move further out of distribution, the amount of compute at which L/14 overtakes B/16 increases. In other words, for in-the-wild performance, at most compute budgets, it may be better to train a smaller model for longer than a larger model for shorter.

These results suggests that self-training on Web data is further scalable as an approach for improving open-vocabulary localization models without the need for further human annotations. The large datasets also makes it possible to scale model size. We trained a G/14 model, which has \(5.2\) the number of parameters and \(4.3\) the inference FLOPs of our L/14 model. To our knowledge, this is the largest open-vocabulary detection model to date. Since the G/14 model uses a different backbone than our other models (SigLIP  instead of CLIP ), we do not include it in Figure 4, but show in Table 1 that it is currently the best-performing model on zero-shot LVIS, with 47.2% mAPrare.

Figure 4: Scaling of detection performance with model size and training compute. Models show classic scaling behavior : Performance increases monotonically with training compute, with larger models being necessary to benefit from larger amounts of compute/data. Models were self-trained on N-gram pseudo-annotations and fine-tuned on LVISbase.

### Effect of Fine-Tuning on Open-Vocabulary Performance

For contrastively trained image-text models, fine-tuning improves performance on the target distribution but reduces the (originally very high) robustness to distribution shift [30; 28; 37]. We observe the same effect for detection, using ODinW13 AP as a proxy for out-of-distribution performance: Compared to the performance after self-training (red dots in Figure 5), fine-tuning on LVIS\({}_{}\) improves performance on the fine-tuned classes (LVIS mAP\({}_{}\)), but OOD performance (ODinW13 AP) is simultaneously reduced in proportion to the amount of fine-tuning (light blue line in Figure 5).

A simple approach to improve on this trade-off is to create an ensemble of the model before and after fine-tuning by averaging the model weights . This approach comes at no additional training cost and improves the Pareto-frontier for all ensemble mixing ratios (Figure 5, purple line). We also tried co-training on WebLI and LVIS but found it to perform worse than weight ensembling.

Notably, performance on LVIS\({}_{}\) behaves similarly to LVIS\({}_{}\) and _improves_ during fine-tuning, even though no LVIS\({}_{}\) classes are seen (Figure 5, right). This may be because LVIS\({}_{}\) classes are semantically and visually close to LVIS\({}_{}\) classes. For example, seeing many annotations for "bird" may improve performance on rare classes such as "heron", "malard", or "puffin". LVIS mAP\({}_{}\) therefore only measures a narrow concept of open-vocabulary performance, and does not reveal the fact that fine-tuning significantly _reduces_ generalization to broader distribution shifts. Benchmarks such as ODinW therefore provide significant additional insight.

Figure 5: Trade-off between fine-tuned and open-world performance. Self-training yields continued improvements on a suite of diverse datasets (ODinW13; \(x\)-axis), but performance on any given dataset (e.g. LVIS; \(y\)-axis) may saturate (red circles). Fine-tuning on a target dataset improves performance on that dataset, but reduces the open-world generalization ability in proportion to the finetuning duration (light blue squares; numbers indicate finetuning steps). This trade-off can be improved through weight-space ensembling (averaging) of the pretrained and fine-tuned checkpoints  (purple diamonds; numbers indicate the mixing coefficient for the fine-tuned weights). The plot shows B/16 models self-trained on N-gram pseudo-annotations and evaluated either directly after self-training or after fine-tuning on LVIS\({}_{}\). Ensembles were created between the longest-self-trained checkpoint and the weights obtained after finetuning that checkpoint for 20k steps. Note that there is significant variability in ODinW13 performance between checkpoints towards the end of self-training.

## 5 Limitations

The main limitation of our method is the amount of compute and data needed for self-training. As we show in Section 4.5, performance improves consistently with training compute and data. This means that further improvements are possible, but also that these will come at increasingly large costs. In fact, cost likely increases faster than resources can realistically be grown in practice. New approaches will therefore be eventually necessary for further improvements.

A second important limitation of our method, similar to other open-vocabulary models , is the trade-off between fine-tuned and open-vocabulary performance addressed in Section 4.6. For out-of-distribution queries, predictions of fine-tuned models may be poorly calibrated and may depend on the precise wording of the query. These issues can be mitigated with weight ensembling , but more research is needed to fully understand the open-vocabulary robustness of these models.

## 6 Conclusion

In the past, open-vocabulary detection performance has been limited by the availability of human-annotated detection training data. Here, we show that self-training can be scaled up to overcome the dependency on human annotations. Our OWL-ST recipe delivers large improvements in detection performance using weak supervision from abundant Web data, similar to what has been seen for image classification and language modelling.