# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

Our analysis shows that the solution found by the AM after optimization is qualitatively different from the linear regression cases studied before . In those cases, due to the continuous nature of the task, AM develops an emergent first-order optimization method that minimizes an emergent quadratic loss function. Furthermore, as it was shown in , a single linear attention layer can solve the regression problem, while adding extra layers and non-linearities slightly modifies the gradient descent. In the modular arithmetic case, AM first learns how to solve the pre-training tasks and later (assuming enough different tasks) develops a generalizing solution by combining the solved tasks.

Our main findings as well as the structure of the algorithmic dataset are illustrated on Fig. 1. Our main findings are: (i) there are **four** different phases in the end of pre-training depending on the number of tasks, \(n_{}\), and number of examples per task, \(\). (ii) At inference time, there is a generalization transition in the number of few-shot examples, as the number of examples grows, the models starts to generalize. This effect is somewhat similar to the transition in sample complexity for the modular arithmetic found in . (iii) model develops a striking circular representation for all of the tasks that naturally generalizes the circular representations found in the original work . We further find that the deeper models are easier to optimize, but much harder to interpret. The optimization is discussed in more detail in the main text. Here we will highlight that optimization for these tasks is challenging and the AM tends to prefer the minimum that just solve a handful of tasks and memorize the training set. To avoid such minima we make sure that _every batch_ contains equal number of tasks (meaning that no tasks is over- or under-represented in each batch). We further find that for larger models early stopping is necessary because the generalizing solution is transient.

We organize our paper as follows. Section 2 contains the literature review. In Section 3 we explain our notations and discuss the experimental details. In Section 4 we demonstrate empirically that the

Figure 1: **(a)** The dataset. The tasks are labeled by vectors \((a,b)_{p}^{2}\). Each table contains examples of \(ax+by\) mod \(p\). A fraction \(1-\) of the examples is blacked out; while the remaining examples are flattened into a single “document” in the batch. Each document is organized as a collection of triples \((x,y,ax+by)\) for \(x,y\) from the training set (_i.e._ not blacked out in the table). Our training is similar to the traditional next-token prediction (autoregressive); with the main difference that we predict every third token, which are marked in red (\(x\) and \(y\) are uncorrelated). Every task appears exactly the same number of times in each batch. **(b)** Phase diagram for a six-layer model. We find four different phases. (1) _in-distribution memorization_: The model _only_ performs well on tasks \((a,b)\)_and_ examples \((x,y)\) from the training set – it does not generalize on unseen examples or tasks. (2) in-distribution generalization: model generalizes on unseen examples \((x,y)\) but not on unseen tasks \((a,b)\). (3) out-of-distribution memorization: model generalizes on unseen tasks \((a,b)\) but only for examples \((x,y)\) it has seen during training. (4) out-of-distribution generalization: model generalizes on unseen tasks \((a,b)\) for seen as well as unseen examples \((x,y)\). We focus on investigating phase (4) in more detail. **(c)** In-context sample complexity. Accuracy of the model in phase (4) as a function of the number of few-shot examples. **(d)** Representations developed by one of the attention heads in the first layer. These are projections of the embedding of a pair of numbers onto the two largest principal components (PCs) of the internal representation formed after passing through the attention layer and projection matrix. **(e)** First 3 PCs of embeddings separate \(log_{27}\)-annotated numbers into even/odd planes, with 0 sandwiched between them.

out-of-distribution ICL ability emerges as the number of training tasks increases. We also study the effects of model depth and task difficulty. In Section 5 we carefully examine a minimal setting, _i.e._ two-block transformer: we compare the representations learnt in four different phases and show that in the generalizing phase the representations are highly structured and generalize the original modular addition case of .

## 2 Related Works

In-Context Learning (ICL)Brown et al.  first demonstrated that large models performance improves substantially when a few examples of the task at hand are provided at inference time, in the prompt. Akyurek et al. , Ahn et al. , von Oswald et al.  showed that the AM implements emergent first-order optimization on an emergent objective function to solve linear regression tasks. Furthermore,  showed that larger models learn to perform Bayesian estimation. Garg et al.  demonstrated that transformers can learn several simple classes of functions in context. Kirsch et al.  presented how task diversity and model size would affect the ICL performance for unseen tasks using a mixture of modified MNIST datasets. Raventos et al.  investigated the relation between task diversity and out-of-distribution ICL ability on linear regression tasks. Lin and Lee  identified two operating modes in ICL using a mixture of linear regression tasks, where for the first several shots, the model tries to figure out the correct task vector and later uses it to predict the correct results. Boix-Adsera et al.  showed theoretically and experimentally that with enough pre-training data, a transformer model can perform abstract reasoning that a MLP cannot do. Guo et al.  showed that transformers can use lower layers to memorize and upper layers to perform ICL in a feature regression setting. It was found in  that ICL is a transient phase from the optimization point of view: it goes away once the model is over-trained. Hendel et al. , Liu et al.  showed that language models form in-context vectors, which can be used to steer model predictions.

Modular ArithmeticPower et al.  discovered Grokking, where models trained on modular arithmetic datasets have an abrupt change from random guessing to generalization on the test set way after the model memorized the training set. Gromov , Nanda et al. , Gu et al.  showed that for modular addition tasks, models learned to map integers to Fourier features to solve modular arithmetic tasks. Liu et al.  showed that grokking is related to learning highly structural features, and the grokking transition can be explained by a toy model. Zhong et al.  showed that there is more than one algorithm that a model can implement to solve modular addition. Doshi et al.  showed that corruption of the label does not prevent the models from finding a generalizing solution. Doshi et al.  showed that MLP and transformer models can solve a specific family of modular polynomial tasks by bijectively mapping them to modular addition tasks.

InterpretabilityElhage et al. , Olsson et al.  showed that transformers can form induction heads that predict the next token in a sequence by identifying and copying patterns from earlier in the sequence. With several indirect empirical evidences, they showed that those heads might constitute the core mechanism of ICL. Nichani et al.  showed theoretically and empirically how disentangled transformers learn causal structures from in-context Markov chains by forming induction heads.

## 3 Preliminaries

Linear Modular FunctionsWe consider modular arithmetic tasks of the form: \(z_{i}^{t}=a^{t}x_{i}+b^{t}y_{i} p\). We will refer to the coefficients \((a^{t},b^{t})_{p}^{2}\) as the _task vector_. The superscript \(t\{1,,p^{2}\}\) labels the \(p^{2}\) possible tasks. We will refer to \((x_{i},y_{i})_{p}^{2}\) as the _input vector_, which is labeled by the subscript \(i\{1,,p^{2}\}\).

In-Context Learning with TransformersWe use GPT-like transformers  with ReLU activation function and Rotary Positional Embedding (RoPE) . The model has \(d\) consecutive blocks, \(H\) attention-heads, and embedding dimension \(d_{ embed}\). Each number is tokenized as an independent token. The pre-training is done following a slightly modified next-token prediction setup, with sequences of the form:

\[^{t}=(x_{1} y_{1} z_{1}^{t} x_{2} y_{2} z_{2}^ {t} x_{n_{ ctx}} y_{n_{ ctx}} z_{n_{ ctx }}^{t})_{p}^{3 n_{ ctx}}\,, \]where \(n_{ ctx}\) is the maximum number of in-context examples. The model is asked to predict _only_ the labels \(\{z_{1}^{t},,z_{n_{ ctx}}^{t}\}\). We emphasize that we do not explicitly provide the task vectors \((a^{t},b^{t})\) to the model (see Fig. 1) - this information is implicit in the in-context labels \(\{z_{i}^{t}\}\). In order for the model to generalize, it must determine the underlying task vector \((a^{t},b^{t})\) from the few-shot examples.

GeneralizationThere are two notions of generalization in this setup. (i) _In-distribution_: Generalization to _unseen_ input vectors \((x_{i},y_{i})\), but on task vector \((a^{t},b^{t})\) the model has seen during pre-training. (ii) _Out-of-distribution_: Generalization to task vectors the model has _not_ seen during pre-training. To clearly separate these regimes, we split the task vectors into in-distribution (i.d.) set \(_{ i.d.}\{(a^{t},b^{t})\}_{ i.d.}\) and out-of-distribution (o.o.d.) set \(_{ o.o.d.}\{(a^{t},b^{t})\}_{ o.o.d.}\). Similarly, we split the input vectors into train and test sets: \(_{ train}\{(x_{i},y_{i})\}_{ train},_{ test }\{(x_{i},y_{i})\}_{ test}\). This results in four distinct sets of sequences constructed from those sets; we name them \(S^{ i.d.}_{ train},S^{ i.d.}_{ test},S^{ o.o.d.}_{ train}\) and \(S^{ o.o.d.}_{ test}\). The set \(S^{ i.d.}_{ train}\) is used for pre-training, while the other three sets are used for evaluations.

Pre-Training Task Selection and Sequence DesignWe always sample the pre-training task vectors \(_{ i.d.}\) in sets of 4, following the rectangular rule, shown in Figure 2(a). Additionally, each batch contains an equal representation from all the task vectors in the set \(_{ i.d.}\). Moreover, all the tasks share the same sequence of inputs. For example, a batch with two different task vectors \((a^{t_{1}},b^{t_{1}});(a^{t_{2}},b^{t_{2}})\) and two distinct input sequences per task (resulting in four total sequences) is shown in Figure 2(b).

This structured approach creates a coherent signal from the sequences within each batch; ensuring that the model learns multiple task vectors with reasonable batch sizes. Alternatively, if the batches are sampled i.i.d., then the model is confused by the batch noise and cannot learn any tasks.

Detailed discussions on task selection and sequence design are presented in Appendix D.

Default SettingUnless stated explicitly, we will use \(p=29\), the number of heads \(H=4\), and embedding dimension \(d_{ embed}=512\), with \(n_{ ctx}=32\) in-context examples. All models are trained with AdamW optimizer  with batch size \(B=1024\) for \(200\)k steps. We have also tied the embedding layer of the model with the readout layer.

## 4 Emergence of In-Context Learning and Task Composition

In this section, we demonstrate that a transformer model with depth \(d 2\) can develop ICL and out-of-distribution generalization on modular arithmetic tasks. We delve deeper into the two notions of generalization (i.d. and o.o.d.), and discuss the relevant factors. We find that the model's ability to generalize out-of-distribution is predominantly determined by the number of pretraining tasks \(n_{ i.d.}\).

### Transition driven by the number of tasks

In Figure 3(a), we show the accuracy of \(d=6\) models vs the number of training tasks \(n_{ i.d.}\) and the number of few-shot examples quantified by the fraction of the total number of data points, \(\); on sets \(S^{ i.d.}_{ train},S^{ i.d.}_{ test},S^{ o.o.d.}_{ train}\) and \(S^{ o.o.d.}_{ test}\). The phase diagram in Figure 1 is constructed by merging the last shot accuracy version of these four diagrams shown in Figure 27(a) of Appendix G.

Figure 2: Structured selection of pre-training tasks and sequences.

The ability of the model to generalize in-distribution increases with \(\), as can be seen by comparing the first two panels of Figure 3(a). This behavior is in correspondence with the original work on grokking, where the transition to generalizing solution is driven by the amount of data. Further, we observe that an increase in \(n_{}\) enhances the in-context sample efficiency, _i.e._ the model generalizes at inference time with fewer few-shot examples. This indicates the onset of the transition from a task-memorizing solution to the one that generalizes out-of-distribution. The model switches to a new algorithmic way of solving the task and the solution is more few-shot-sample-efficient.

Shifting our focus to the last two panels of Figure 3(a), we see that when \(n_{} 256\), the model can solve new tasks that were absent in the training set. Notably, there appears to be a trade-off between memorization and generalization when the model attains this o.o.d. generalization ability. As the o.o.d. performance increases, the pre-training performance simultaneously degrades. This phenomenon indicates a shift in the algorithm implemented by the model. Prior to this transition, the model primarily needed to select possible vectors from the list of memorized tasks and apply them. However, post-transition, the model adopts a more universal approach to solve the task in-context. We emphasize, that the model learns to perform ICL in _both scenarios_. The difference lies in the approach to generalization. When the model can only generalize in-distribution it's task is to classify the sequence as one of the seen tasks or as unknown. Once it matches the sequence to one of the memorized task vectors, it does well for \(x,y\) pairs that only appear in the test set. However, as the number of tasks vectors grows the model fails to store them all and is forced to find a method of determining the task vector algorithmically at inference time. In that case model performs equally well on seen and un-seen tasks alike. In fact, the small two-layer model we study has such a low capacity that it entirely skips the in-distribution generalization phase and immediately jumps from pure memorization to out-of-distribution generalization.

Next, to further illustrate the effect of task diversity, we plot the pre-training accuracy (set \(S^{}_{}\)) and the o.o.d. test accuracy (set \(S^{}_{}\)) as a function of training steps (Figure 3(b, c)); for various values of \(n_{}\). We observe a clear memorization-to-generalization transition as task diversity increases. Interestingly, for \(n_{}=2^{8}\), the ICL ability on \(S^{}_{}\) set exhibits non-monotonic behavior, where the o.o.d. performance rises and falls along the training process. This phenomenon is likely due to a competition between the memorizing and generalizing circuits inside the model. Note that this phenomenon is akin to the one analyzed in Singh et al. , albeit with a different setup.

Figure 3: Phase diagram for the depth \(d=6\) models. **(a)** Accuracy on all four sets used to plot the 1 phase diagram, with an early stopping applied. Notably, in the regions when models generalize to o.o.d. sets, the pre-training performance degrades; **(b, c)**\(=0.6\) training accuracy and o.o.d. test accuracy (dotted line). For \(n_{}=2^{8}\), we notice that the o.o.d. generalization ability of the model first improves then degrades as we train longer; **(d, e)**\(=0.6\), loss and accuracy vs context length, measured on \(S^{}_{}\) at the end of training, where for \(n_{}=2^{8}\) case the ICL ability fades away.

Further evidence supporting the two-circuit competition can be observed in panel (d) of Figure 3. The loss curves show a "monotonic \(\) non-monotonic \(\) monotonic" transition as the task diversity increases. With a minimal number of pre-training tasks, the model primarily engages in memorization, resulting in a monotonically increasing o.o.d. loss curve. As the number of pre-training tasks increases, the loss curve exhibits non-monotonic behavior, indicating competition between two distinct neural circuits. This transient nature of o.o.d. generalization for \(n_{ i.d.}=2^{8}\) is a peculiar case where memorization circuits are initially suppressed but eventually prevail. With substantial task diversity, the circuits responsible for generalization take over, culminating in a monotonic loss curve. Similar insights can be derived from examining the monotonicity of the accuracy curves in panel (e).

### Effect of Model Size and Task Difficulty

A natural question to ask is if similar phenomena can be observed with different model sizes or task difficulties. Here we present our results with \(d=4\) and \(d=2\) in Figure 4 and leave the results for other prime \(p\) values in Appendix H.

When comparing phase diagrams in Figure 3 with Figure 4, we observe that those phase diagrams across different depths are qualitatively similar, where the o.o.d. generalization only emerges with a large enough number of pre-training tasks. As model capacity decreases, performance on both the pre-training set and the o.o.d. test set degrades. This is particularly evident in the \(d=2\) case, where the pre-training accuracy falls drastically as the model gains o.o.d. generalization ability.

Interesting observations can be made by comparing loss and accuracy on the o.o.d. test set as a function of context length at the end of training. First, it is evident that as the model depth decreases, the 1-shot loss surge attributable to memorization becomes milder. Notably, for models with \(n_{ i.d.}=2^{9}\), there is no loss surge in the 1-shot case across all three depths. Furthermore, the \(d=4\) model with \(n_{ i.d.}=2^{8}\) behaves significantly differently from the corresponding one with \(d=2\) case, where the model fails to perform ICL for the o.o.d. test set. This is also distinct from the \(d=6\) case, where the model tends heavily toward memorization due to its excessive capacity. Instead, the \(d=4\) model manages to maintain a better balance between memorization and generalization at the end of pre-training. Consequently, the model has a 1-shot loss surge followed by a notable drop in ICL loss. This suggests that \(d=4\) optimally leverages the available model capacity to facilitate effective learning dynamics for o.o.d. generalization.

Figure 4: Phases of depth \(d=4\) and \(d=2\) models. With decreasing model capacity, the performance on both sets degrades. At the same time, the transient nature of ICL does not appear. **(a, b) from left to right:** accuracy phase diagrams on pre-training set \(S^{ d.o.d.}_{ train}\) and on o.o.d. test set \(S^{ o.o.d.}_{ test}\), with early stopping; loss and accuracy vs context length on o.o.d. test set \(S^{ o.o.d.}_{ test}\) for \(=0.6\).

## 5 Interpreting the Learned Algorithms

We now explain the algorithms implemented by the models to achieve o.o.d generalization. We find that the models implement two distinct algorithms depending on the depth and the number of in-context examples during inference.

**Ratio Matching:** If there exists \(i\) s.t. \(c_{i}(x_{i},y_{i})=(x,y) p\) then \(z=c_{i}z_{i}^{4} p\). This algorithm only works when \(y/x=y_{i}/x_{i} p\) holds for at least one of the in-context examples.

**Modular Regression:** Find \(\{c_{1},c_{2},,c_{k}\}\) s.t. \(_{i=1}^{k}c_{i}(x_{i},y_{i})=(x,y) p\). Then the prediction is \(z=_{i=1}^{k}c_{i}z_{i} p\). This can be viewed as discretized circular regression over \((p)\).

We find telling signatures of these algorithms upon analyzing model predictions with varying numbers of in-context examples (Figure 5). With very few in-context examples, the models implement the Ratio Matching algorithm. As a canonical example, consider the 1-shot scenario \((x_{1}\;\;y_{1}\;\;z_{1}\;\;x\;\;y\;\;?)\). In this case, Ratio Matching will only solve the task for inputs that obey \((x,y)=c_{1}(x_{1},y_{1}) p\) for some \(c_{1}\). Indeed, this is exactly what we observe in Figure 5 for both \(d=2,4\) models. With many (\( 10\)) in-context examples, \(d=2\) and \(d=4\) models implement different algorithms. The \(d=4\) model can _combine_ the in-context examples using the Modular Regression algorithm, leading to near-perfect o.o.d. generalization. On the other hand, the \(d=2\) model still uses Ratio Matching and shows sub-optimal performance. We ascribe this to the limited capacity of the \(d=2\) models. In other words, the \(d=4\) models exhibits an _algorithmic shift_ from Ratio Matching to Modular Regression as them number of in-context examples increases. Whereas, the \(d=2\) models shows no such shift.

To implement these algorithms, the model needs to perform linear operations over \((p)\), which can be broken down into the following essential skills (in addition to copying information, which is readily implemented by attention heads).

1. _Modular Map_: Encode the tokens such that operations over \((p)\) can be easily implemented
2. _Multiplication_: A necessary skill to rescale in-context examples in both algorithms
3. _Addition_: A necessary skill for combining in-context examples in Modular Regression algorithm

While both \(d=2,4\) models learn skills I, II perfectly; the \(d=4\) model outperforms its \(d=2\) counterpart in skill III - which helps it _combining_ the in-context examples.

In the remainder of this section, we show the special attention heads that implement skill I (Section 5.1) and MLPs that implement skills II, III (Section 5.2). We explicitly show the deterioration in this

Figure 5: \(d=4\) and \(d=2\) models’ performance on k-shot inference, on the grid of inputs \((x,y)_{p}^{2}\) (task vector = \((6,6)\)). **row 1:** Models’ predictions on o.o.d. task of the type \((x_{1}\;\;y_{1}\;\;z_{1}\;\;\;\;x_{k}\;\;y_{k}\;\;z_{k}\;\;x\;\;y\;\;?)\). **row 2:** Analytical plots showing predictions solely based on Modular Regression algorithm. **row 3:** Subtract row 2 from row 1, by using correct=1 and incorrect=0. The red points correspond to the examples where Ratio Matching does not give the correct predictions but the model predicts correctly. The blue points are examples that the model missed despite Ratio Matching being applicable. This row tells us about the model’s ability to implement Modular Regression by _combining_ the in-context examples. Note that \(d=4\) model readily learns to combine previous examples, while its \(d=2\) counterpart struggles due to its limited capacity.

structures as pre-training task diversity (\(n_{i.d.}\)) decreases. We also elucidate the algorithmic shift in the \(d=4\) models as the number of in-context examples increases.

### Attention Heads Implement Skill I

So far we have presented "black-box" evidence suggesting that the model is implementing the proposed algorithms (Figure 5). Now, we turn to "white-box" interpretability, to identify the components within the transformer model that are responsible for the essential skills. In Figure 6(a, b), we analyze the important attention heads in the \(d=2\) model. Specifically, we compare the emergent structures in models trained with different pre-training task diversities.

In Figure 6(a), we show the attention head from layer 1 that implements skill I. In the top panel, we see that each query only pays attention to itself and the two preceding keys. This pattern likely stems from the fact that each example in the sequence contains three tokens \(x_{i},y_{i},z_{i}^{t}\); and suggests that this head is mostly focused on the information within the example.

In the bottom panel, we perform principal component analysis (PCA) on the outputs of this head. Specifically, we feed a batched k-shot sequence of the form \((x_{1}\;\;y_{1}\;\;z_{1}\;\;\;\;x_{k}\;\;y_{k}\;\;z_{k}\;\;x\;\;y\;\;z)\), where the first \(k\) inputs are fixed and the last input \((x,y)\) is scanned over all \(p^{2}\) possible pairs. We concatenate the resulting features from \(x\) and \(y\), resulting in \(p^{2} 2d_{ embed}\) batch of features - and perform PCA on this matrix. We project all these \(2d_{ embed}\) dimensional features onto the first two principal components. Annotating the pairs \((x,y)\) with \((_{27}x,_{27}y)\)1, we find a "circle-of-circles" - where circles of period 28 are arranged in a bigger circle of period 28. Number 0 is located

Figure 6: Models that generalize o.o.d. (left) exhibit more structured attention maps. Additionally, the top-2 principal components of the features from the corresponding heads also show more structured patterns. The features are computed for sequences with an o.o.d. task vector \((a^{t},b^{t})=(6,6)\), loop over \((x_{i},y_{i})\) at a specific shot while the previous parts of the sequence are fixed. We annotate each PCA plot with the \((_{27}x_{i},_{27}y_{i})\) mod \(p\) pairs. **(a)** The principal components form a circle of circles where the position of the outer circle is controlled by \(x_{i}\). This pattern remains the same for different task vectors or the shot choices; **(b)** Only plotted pairs with even \(_{27}x_{i}\), with each \(_{27}x_{i}\) circled with different colors. The PCA pattern forms a similar double circle as those in (a), with the key difference that those circles depend on task vector choices and the shot choices; **(c, d)** Models without o.o.d. generalization ability. We pick heads from the first block that corresponds to the first column of (a). Clearly, the structure of attention maps and PCA patterns deteriorate as the task diversity decreases.

at the center of the circles2. We observe similar circle-of-circles for concatenated features from \((x,z)\) and \((y,z)\) as well. We refer the reader to Appendix E for further details.

In Figure 6(b), we analyze the head from layer 2 that effectively "copies" in-context examples. The upper panel shows the highly structured attention map that focuses on the current as well as the preceding examples. This pattern aligns with the Ratio Matching algorithm where the model compares \((x,y)\) pairs across different in-context examples.

In the bottom panel, by conducting a PCA analysis, we again identify circles when annotating examples in the \((_{27}x,_{27}y)\) format. However, unlike the previously discussed pattern, the specifics of this "circle-of-circles" arrangement vary depending on the position and the choice of task vector \((a^{t},b^{t})\). This variability suggests that the head in question possesses information about the specific task from the context.

We further highlight the importance of the structure we find in the above paragraphs via comparison with models, trained with lower pre-training task diversity, that do _not_ generalize o.o.d (Figure 6 c, d). Note that as the number of pre-training tasks \(n_{}\) decreases, the attention map starts to get mosaicked (top panels); and the PCA projections lose their shape (bottom panels). As a result, these models lose the ability to perform ICL on modular arithmetic out-of-distribution.

### MLPs Implement Skill II and III

After applying Skill I, the model maps numbers onto circular feature spaces, enabling discrete modular operations such as multiplication, division, addition, and subtraction through subsequent model components. To analyze this behavior, we compute the cosine-similarity between layer \(l\) (i.e. \(l^{th}\) Transformer block) \(k\)-shot output vectors (standardized) at token position \(y\): \(_{y}^{}(x_{1},y_{1},z_{1},,x_{k},y_{k},z_{k},x,y)\).

Figure 7: Cosine-similarities (Equation (2)) between layer outputs \(^{l}\) at token \(z\) position (\((_{z}^{t}(x,y),_{z}^{t}(x^{},y^{}))\), first row) and token \(y\) position (\((_{y}^{t}(x,y),_{y}^{t}(x^{},y^{}))\), second row) reveal patterns in the models’ internal representations. For clarity, we only show selected \(x\) and \(x^{}\) values, where \(y\) and \(y^{}\) values range from \(0\) to \(28\) between each tick. For the \(d=4\) model, kaleidoscopic patterns in the third layer indicate the generation of all possible \(y_{i}/x_{i}\) features for subsequent computations. The last layer shows an algorithmic shift from Ratio Matching to Modular Regression. The \(d=2\) model also shows the kaleidoscopic pattern in the first layer, while the second layer identifies the relevant \(y/x\) features from the in-context examples for matching.

we find highly structured activations as functions of inputs \((x,y)\) (Figure 8 left). Together with our earlier analysis of Skill I, these observations suggest that the MLP layer following the attention module with a "circle-of-circles" head (layer 2, head 4 for \(d=4\) model, see Figure 13 (a)) implements division operations over \((p)\). The bottom panel of Figure 7 succinctly demonstrates the algorithmic shift from Ratio Matching to Modular Regression as the number of in-context examples increases. At 2-shot, layer 4 features collapse to identical vectors except when the ratio \(y/x\) matches one of the ratios \((y_{1}/x_{1}\) or \(y_{2}/x_{2})\). At higher shots, we see a transition to a task-dependent pattern where features align for input pairs \((x,y)\) for which \(a\,x+b\,y p\) match - a signature of Modular Regression.

For \(d=2\) models, we again observe the kaleidoscopic pattern in cosine-similarities (Figure 7 top-right) and structured MLP activations (Figure 8 right). However, in this case we only find signatures of Ratio Matching in the cosine similarities (Figure 7 bottom-right), as expected.

Thus, we conclude that the \(d=2\) model has scarcely acquired skill III, due to its limited capacity. On the other hand, \(d=4\) model is very good at combining equations via skill III, explaining its superior performance. For a more detailed discussion, see Appendix E.

## 6 Discussion

We have investigated the emergence of in-context learning and skill composition in autoregressive models on a novel algorithmic dataset. The dataset includes a large discrete set of modular arithmetic tasks and is specifically designed to force models to learn how to solve a variety of tasks. It consists of learning linear modular functions, where the model is expected to identify _and_ perform a modular operation in-context. When the number of tasks is relatively small the models can only generalize in-distribution. Although such models develop ICL capabilities, they simply memorize these task vectors and use them to classify the input vectors. Once the number of training tasks becomes too large, the models transition to a qualitatively different algorithmic approach, where the task vector is determined at inference time.

Finally, we have examined the learnt representations and shown that qualitatively different circuits are formed in different phases. In the o.o.d. generalization phase, we explain the learnt algorithms and highlight an in-context algorithmic shift in deeper models.

LimitationsWe have limited this work to algorithmic datasets. It remains to be investigated what lessons can be translated to realistic language models, and what lessons are specific to the current setting. White-box interpretability analysis of deeper models has proved to be much more difficult than that of shallower models. Consequently, we still do not understand the role of every individual component of the network in the deeper cases.

Figure 8: MLP activations (post-ReLU) wrt inputs \((x,y)\)