# Egocentric Planning for Scalable Embodied Task Achievement

Xiaotian Liu

ServiceNow Research

Montreal, QC, Canada

xiaotian.liu

@mail.utoronto.ca

&Hector Palacios

ServiceNow Research

Montreal, QC, Canada

bertoral

@gmail.com

&Christian Muise

Queen's University

Kingston, ON, Canada

christian.muise

@queensu.ca

Work done when Xiaotian Liu was an intern at ServiceNow Research.Work done when Hector Palacios was a research scientist at ServiceNow Research.

###### Abstract

Embodied agents face significant challenges when tasked with performing actions in diverse environments, particularly in generalizing across object types and executing suitable actions to accomplish tasks. Furthermore, agents should exhibit robustness, minimizing the execution of illegal actions. In this work, we present Egocentric Planning, an innovative approach that combines symbolic planning and Object-oriented POMDPs to solve tasks in complex environments, harnessing existing models for visual perception and natural language processing. We evaluated our approach in ALFRED, a simulated environment designed for domestic tasks, and demonstrated its high scalability, achieving an impressive 36.07% unseen success rate in the ALFRED benchmark and winning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires reliable perception and the specification or learning of a symbolic description of the preconditions and effects of the agent's actions, as well as what object types reveal information about others. It can naturally scale to solve new tasks beyond ALFRED, as long as they can be solved using the available skills. This work offers a solid baseline for studying end-to-end and hybrid methods that aim to generalize to new tasks, including recent approaches relying on LLMs, but often struggle to scale to long sequences of actions or produce robust plans for novel tasks.

## 1 Introduction

Embodied task accomplishment requires an agent to process multi-modal information and plan over long task horizons. Recent advancements in deep learning (DL) models have made grounding visual and natural language information faster and more reliable (MBP\({}^{+}\)21). As a result, embodied task-oriented agents have been the subject of growing interest (STG\({}^{+}\)20, YRT\({}^{+}\)22, WDKM21). Benchmarks such as the _Action Learning From Realistic Environments and Directives_ (ALFRED) were proposed to test embodied agents' ability to act in an unknown environment and follow language instructions or task descriptions (STG\({}^{+}\)20). The success of DL has led researchers to attempt end-to-end neural methods (ZC21, SGT\({}^{+}\)21). In an environment like ALFRED, these methods are mostly framed as imitation learning, where neural networks are trained via expert trajectories. However, end-to-end optimization leads to entangled latent state representation where compositional and long-horizon tasks are challenging to solve. Other approaches use neural networks to ground visual information into persistent memory structures to store information (MCR\({}^{+}\)22, BPF\({}^{+}\)21). These approaches rely on templates of existing tasks, making them difficult to generalize to new problems or unexpected action outcomes. ALFRED agents must navigate long horizons and skill assembly, operating within a deterministic environment that only changes due to the agent's actions.

Long sequential decision problems with sparse rewards are notoriously difficult to train for gradient-based reinforcement learning (RL) agents. But a symbolic planner with a well-defined domain can produce action sequences composed of hundreds of actions in less than a fraction of a second for many tasks. Embodied agents might need to conduct high-level reasoning in domains with long action sequences. In such scenarios, human experts can opt to model the task within a lifted planning language. Planning languages such as _Planning Domain Definition Language_, or PDDL (HLMM19), is the most natural candidate, given the high scalability of planners that use such languages (GB13). For instance, we define the tasks of a home cooking robot with actions, such as _opening_, _move-to_, _pick-up_, _put-on_, and objects, such as _desk_, _stove_, _egg_ found in common recipes.

However, most symbolic planning techniques are relatively slow in partially observable environments with a rich set of objects and actions. Thus we propose an online iterative approach that allows our agent to switch between exploration and plan execution. We explicitly model the process of actively gathering missing information by defining a set of _unknown-objects_ and _exploration-actions_ that reveals relevant information about an agent's goal. Instead of planning for possible contingencies, we use an off-the-shelf classical planner(Hof01) for fully-observable environments to determine whether the current state has enough knowledge to achieve the goal. If more information is needed, our agent will switch to a goal-oriented exploration to gather missing information. Our approach can seamlessly incorporate any exploration heuristics, whether they are learned or specified.

We implemented and evaluated our method on the popular embodied benchmark ALFRED using only natural language task descriptions. To demonstrate the effectiveness of our method, we use the same neural networks for both visual and language grounding used in FILM, which was the SOTA and winner of the 2021 ALFRED challenge (MCR\({}^{+}\)22). See a comparison with other methods in Table 1. By replacing FILM's template-based policies with our egocentric iterative planner, our method improved FILM's success rate (SR) by 8.3% (or a 30.0% relative performance increase) in unseen environments winning the the ALFRED challenge at CVPR 2022 Embodied AI workshop. Our empirical results show that the performance increase was attributed to a more robust set of policies that account for goal-oriented exploration and action failures. We also show that our method can conduct zero-shot generalization for new tasks using objects and actions defined in the ALFRED setting.

## 2 ALFRED Challenge

ALFRED is a recognized benchmark for Embodied Instruction Following (EIF), focused on training agents to complete household tasks using natural language descriptions and first-person visual input. Using a predetermined set of actions, the agent operates in a virtual simulator, AI2Thor (KMG\({}^{+}\)17), to complete a user's task from one of seven specific classes instantiated in concrete objects. For example, a task description may be "Put a clean sponge on the table," and instructions that suggest steps to fulfill the task. The agent must conduct multiple steps, including navigation, interaction with objects, and manipulation of the environment. Agents must complete a task within 1000 steps with fewer than 10 action failures. We base our approach on task descriptions rather than the provided instructions. Task complexity varies, and an episode is deemed successful if all sub-tasks are accomplished within these constraints. Evaluation metrics are outlined in Section 8.

## 3 Overview of Egocentric Planning for ALFRED

Figure 1 illustrates our proposed method integrating several components to facilitate navigation and task completion: a visual module responsible for semantic segmentation and depth estimation, a language module for goal extraction, a semantic spatial graph for scene memorization, and an egocentric planner for planning and inference. Initially, high-level language task description is utilized to extract goal information, and the agent is provided with a random exploration budget of 500 steps to explore its surroundings. The random exploration step is used to generate diverse set of object cluster for further exploration using our planner. Subsequently, at t = 500, the gathered information from the semantic spatial graph is converted into a PDDL problem for the agent. We employ a novel open-loop replanning approach, powered by an off-the-shelf planner, to support further exploration and goal planning effectively.

To facilitate egocentric planning, we first define the ontology of the planning problems, which encompasses action schemas, object types, and potential facts schemas. The natural language task description is subsequently converted into a planning goal. Following the initial exploration phase, the semantic spatial graph is updated to indicate the new position of the agent and its perceptions. The algorithm iterates through these steps: a) finding a plan for achieving the goal and returning it upon success; b) if no plan exists, replacing the goal with another fact, called (explore), associated with unvisited states. The semantic spatial graph is updated during each iteration to construct the new initial state of the agent, allowing for an incremental egocentric view of the environment.

## 4 Embodied Agents: Generalization within Object Types and Skills

Embodied Agents are task solvers in environments linked to a hidden Object-oriented POMDP, emphasizing objects, actions/skills, and costs rather than rewards. We assume that these objects mediate perceptions and actions. An _Environment_ is a tuple, \(= A_{},_{},_{ },,,\) with \(A_{}\) being the set of parameterized actions, \(_{}\) defining the set of object types and potential values \(_{}\), reset representing the first observation, and step representing an action execution that returns an observation and cost. A _Task_ is represented by a tuple, \(T_{}= I_{},G_{}\), which sets the initial state and the goal of the environment via the reset function. Lastly, _Embodied Agents_ employ functions \(M_{I}\) to initialize their mental state and \(M_{U}\) to update it, have a policy \(_{M}\) to select actions and a function \(G_{M}\) for deciding when they consider the goal to be achieved.

Agents that tackle a stream of tasks within the same environment have incentives to specialize to the known object types and their inherent capabilities expressed through parameterized actions. Such over-specialization provides an opportunity for developing robust agents that can generalize across different tasks within the same environment. This principle applies to physical and software agents whose actions are grounded in these objects. Our framework forms a basis for creating flexible and adaptable agents that can perceive and act upon recognized object types. Further details and implications are covered in Section D.

## 5 Egocentric Planning for Embodied Agents

Here, we introduce _Egocentric Planning_, an approach for embodied agents to solve tasks in Object-oriented POMDP environments. The approach requires a symbolic planning model that assumes full observability for testing both action applicability and goal achievement. However, the complexity of specifying or learning such a symbolic model is simplified by its alignment with the Object-oriented POMDP environment, object types, and action signatures. Egocentric planning relies on a reasonable assumption to derive a sensor model, equivalent to a function that updates the distribution of possible

Figure 1: Egocentric Planning for ALFRED

current states given new observations (APG09). For instance, in a planning domain with objects for different types and colors, the sensor model can say that cubes and rectangles are red, while ovals and circles are black. When the agent examines an object, and observes that it is black, the agent should filter the belief to represent that the object cannot be a cube or a rectangle. In contrast, our method requires a set of _anchor object types_ that are assumed to reveal information about other objects and _exploration actions_ that allow visiting new anchor objects, revealing new objects and their properties. The method can be combined with standalone models for processing images or natural language. The resulting method is both theoretically sound, assuming that the symbolic planning model is correct, and practically applicable, as it leverages the speed of symbolic planners.

We present an abstract algorithm that leverages the objects types and action signatures of the Object-oriented POMDP to ground the required symbolic planning model.

### Background: Parameterized Full-Observable Symbolic Planning

In the supplementary material, Section E, we define _Parameterized Full-Observable Symbolic Planning domains and problems_. A _planning domain_ is defined as a tuple \(=,,,,\) where \(\) is a finite set of object types with \(\) a finite set of values, \(\) is a finite set of typed predicates, \(\) is a finite set of typed actions with preconditions and effects, and \(\) is a function that maps each action and predicates to a tuple expressing their typed arguments. A _planning problem_ consists of a planning domain, a finite set of objects, and descriptions of the initial state and the goal.

For ALFRED, we rely on _Classical Planning_, a kind of full-observable planning that assumes deterministic actions, where preconditions are a set of predicates, and effects are a pair of sets of predicates called add and delete, representing the action's modifications. Correspondingly, a parametric classical planning problem is a full-observable planning problem where the initial state and the goal are conjunctions of grounded predicates.

### Egocentric Planning for Embodied Agents

Egocentric planning employs a user-specified fully-observable symbolic planning model to address tasks in Object-oriented POMDP environments. In the next section, we detail the specifics of our ALFRED implementation. As actions in ALFRED are deterministic, we can apply a classical planner to solve the associated planning problems. We first present a general algorithm for parametric full-observable planning problems.

Algorithm 1 implements our Egocentric Planning approach by constructing a sequence of planning problems, alternating exploration and task solving. At each step, the agent's mental state is updated though sensory input to represent the current state and the known objects that will be part of the following planning problem. The agent deems the goal achieved when it finds a plan for achieving it and executes the plan successfully. Our policy (\(_{M}\)) leverages a standalone planner for both exploration and task solving.

Our exploration and sensing strategy hinges on two elements. First, _anchor object types_\(_{a}\) are types of objects that reveal the presence of others. For example, in ALFRED, _location_ is the sole anchor object type. Second, _exploration actions_\(\) enables the discovery of previously unknown objects. For instance, moving to a new location in ALFRED enables observing new items and locations. This object-centric view of the environment simplifies the creation of a symbolic model required for planning. The function _ExploreAct_ (\(a\), \(o\)) returns a copy of the function where (unknown \(o\)) is added to the precondition, and the effect is extended to cause (explored) to be true and (unknown \(o\))) to be false.

Our agent's _mental state_ is represented by a set of objects and an initial state of grounded predicates, \((,I)\). The function \(M_{I}^{}\) initializes the symbolic goal \(G\) and sets \((,I)\) with the initial objects and true grounded predicates. The mental state update function, \(M_{I}^{}\), updates \((,I)\) given the executed action \(a\), the returned perception \(p\), and the cost \(c\) of executing \(a\). For example, the mental state update in ALFRED adds new objects observed in the perception. It updates the initial state with these new objects and their properties, including their relationship with non-perceived objects like location. Both functions maintain the alignment between the current planning state and the environment with an underlying object-oriented POMDP model.

Specifically, the actions, object types, and the values of the environment may be different from the planning domain's. While the ALFRED environment supports movement actions like move-forward or turn-left, the planning domain might abstract these into a move action. Although the ALFRED environment does not include a location type, we manually specify it so movable objects can be associated with a specific location. Indeed, the first initial state \(I\) includes a first location and its surrounding ones. Thus, the agent can revisit the location once it has found a solution involving that object. While integrating these components may be challenging, it is a hurdle common to all embodied agent approaches aiming to generalize across components under a constrained training budget. In general, practical applications will likely monitor the agent's performance regarding object types and skills. We discuss the advantages and drawbacks of our approach in Section 9.

## 6 Approach for ALFRED

### Model Overview

Our method is structurally similar to FILM but we substitutes the SLAM-style map with a graph structure for object and location information storage (MCR\({}^{+}\)22). The action controller employs our iterative egocentric planner that utilizes a semantic location graph for task completion and exploration. The node of the graph are uniquely labeled by the coordinates (x-coordinate, y-coordinate, facing direction), and the edges are labeled by actions(turn-right, turn-left, move-forward). Each node in the graph stores object classes, segmentation masks, and depth information generated by the vision module. We label the nodes with "known" if the agent has visited the particular location and "unknown" if the node is only observed. Unknown nodes on the graph are prioritized for exploration based on objects they contain. This setup promotes more robust action sequences and generalization beyond the seven ALFRED-defined tasks. At each timestep, the vision module processes an egocentric image of the environment into a depth map using a U-Net, and object masks using Mask2-RCNN (HGDG17, LCQ\({}^{+}\)18). The module then computes the average depth of each object and stores only those with an average reachable distance less than 1.5. A confidence score, calculated using the sum of the object masks, assists the egocentric planner in prioritizing object interaction. We employ text classifier to transform high-level language task description into goal conditions. Two separated models determine the task type and the objects and their properties. For instance, a task description like "put an apple on the kitchen table" would result in the identification of a "pick-up-and-place" task and a goal condition of (on apple table). We convert the FILM-provided template-based result into goal conditions suitable for our planner. The spatial graph, acting as the agent's memory during exploration, bridges grounded objects and the state representation required for our planner. The graph, updated and expanded by policies produced by our egocentric planner, encodes location as the node key and visual observations as values, with edges representing agent actions.

### Egocentric Agent for ALFRED

The object-oriented environment for ALFRED, denoted as \(= A_{},_{},_{ },,\), includes actions such as pick-up, put-down, toggle, slice, move-forward, turn-left, and turn-right. The FILM vision module is trained as flat recognizer of all possible object types. We postprocess such detection into high level types Object and Receptacle in \(_{}\), with corresponding \(_{}\) indicating subtypes and additional properties. For instance, the object type includes subtypes like apple and bread, while Receptacle includes subtypes such as counter-top and microwave. Values incorporate properties like canHeat, canSlice, isHeated, and isCooled.

The planning domain for ALFRED is denoted \(=,,,\). \(\) includes the Location type, with corresponding values in \(\) representing inferred locations as actions are deterministic. Predicates in \(\) represent the connection between locations and the presence of objects at those locations. Further predicates express object properties, such as canHeat, canSlice, isHeated, and isCooled. Unlike the ALFRED environment, actions in the planning domain are parametric and operate on objects and locations. Instead of move-forward, the planning domain features a move action, representing an appropriate combination of move-forward, turn-left, and turn-right. For actions like toggle in ALFRED, which have different effects depending on the object in focus, we introduce high-level, parametric actions like heat that can be instantiated with the relevant object. As the actions in ALFRED are deterministic, we use a classical planner, enabling high scalability by solving one or two fresh planning problem per iteration of Algorithm 1.

## 7 Related Work

Visual Language Navigation (VLN) involves navigating in unknown environments using language input and visual feedback. In this context, we focus on the ALFRED dataset and methods based on the AI2-Thor simulator, which serve as the foundation for various approaches to embodied agent tasks. Initial attempts on ALFRED employed end-to-end methods, such as a Seq2Seq model (STG\({}^{+}\)20), and transformers (ZC21). However, due to the long episodic sequences and limited training data, most top-performing VLN models for ALFRED employ modular setups, incorporating vision and language modules coupled with a higher-level decision-making module (MCR\({}^{+}\)22, MC22, IO22). These methods typically rely on hand-crafted scripts with a predefined policy. However, this approach has several downsides. First, domain experts must manually specify policies for each new task type, regardless of the changes involved. Second, fixed-policy approaches do not allow the agent to recover from errors in the vision and language modules, as the predefined policy remains unchanged.

Recently, there has been growing interest in utilizing planning as a high-level reasoning method for embodied agents. Notable work in this area includes OGAMUS (LSS\({}^{+}\)22), and DANLI (ZYP\({}^{+}\)22). These methods demonstrate the effectiveness of planning in achieving more transparent decision-making compared to end-to-end methods while requiring less domain engineering than handcrafted approaches. However, these works primarily tested in simpler tasks or rely on handcrafted planners specifically tailored to address a subset of embodied agent problems.

Large Language Models (LLMs) have recently been harnessed to guide embodied actions, offering new problem-solving approaches. Works like Ichter et al.'s SayCan utilize LLMs' likelihoods and robots' affordances, grounding instructions within physical limitations and environmental context (iBC\({}^{+}\)23). Song et al.'s (SWW\({}^{+}\)23) LLM-Planner and Yao et al.'s (YZY\({}^{+}\)23) ReAct have further demonstrated the potential of LLMs for planning and reasoning for embodied agents. Alternative methods not using LLMs, include Jia et al. and Bhambri et al.'s hierarchical methods utilizing task instructions(JLZ\({}^{+}\)22, BKMC22). However, the scalability and robustness of these methods is under study. Our approach, in contrast, is ready to capitalize on the expressivity of objects and actions for superior generalization in new tasks, ideal for rich environments with multiple domain constraints. This challenges of using LLM for symbolic planning is noted by Valmeekam et al. (VSM\({}^{+}\)23) who showed the weakness of prompting-based approaches, and Pallagani et al. (PMM\({}^{+}\)22), who highlight the massive amount of data required for reaching high accuracy.

## 8 Experiments and Results

The ALFRED dataset contains a validation dataset which is split into 820 _Validation Seen_ episodes and 821 _Validation Unseen_ episodes. The difference between _Seen_ and _Unseen_ is whether the roomenvironment is available in the training set. We use the validation set for the purpose of fine-tuning. The test dataset contains 1533 _Test Seen_ episodes and 1529 _Test Unseen_ episodes. The labels for the test dataset are contained in an online server and are hidden from the users. Four different metrics are used when evaluating ALFRED results. Success Rate (SR) is used to determine whether the goal is achieved for a particular task. Goal-condition Success is used to evaluate the percentage of subgoal-conditions met during an episode. For example, if an objective is to "heat an apple and put it on the counter," then the list of subgoals will include "apple is heated" and "apple on counter." The other two metrics are Path Length Weighted by Success Rate (PLWSR) and Path Length Weighted by Goal Completion (PLWGC), which are SR and GC divided by the length of the episode. For all the metrics, higher is better. We compare our methods, Egocentric Planning Agent (EPA), with other top performers on the current ALFRED leaderboard. Seq2Seq and ET are neural methods that uses end-to-end training (STG\({}^{+}\)20, ZC21). HLSM, FILM, LGS-PRA are hybrid approach that disentangles grounding and modeling (BPF\({}^{+}\)21, MCR\({}^{+}\)22, MC22, IO22).

### Results

Table 1 presents a comparison of our outcomes with other top performing techniques on the ALFED dataset. With an unseen test set success rate of 36.07% and a seen test set rate of 44.14%, our EPA method won the last edition of the ALFRED challenge. The ALFRED leaderboard ranks based on SR on unseen data, reflecting the agent's capacity to adapt to unexplored environments. At the moment of writing, our achievements rank us second on the ALFRED leaderboard. The only method surpassing ours is Prompter(IO22), which utilizes the same structure as FILM and incorporates search heuristics-based prompting queries on an extensive language model. However, their own findings indicate that the performance enhancements are largely attributed to increasing obstacle size and reducing the reachable distance represented on the 2D map. The other top performer is LGS-PRA, which is also based on the FILM architecture, that improves performance via techniques on landmark detection method and local pose adjustment. In contrast, EPA employs a semantic graph, a representation distinct from the top-down 2D map applied by FILM, HLSM, LGS-PRA and Prompter. Our findings suggest that the performance boost over FILM and similar methods stems from our iterative planning approach, which facilitates recovery from failure scenarios via flexible subgoal ordering. Techniques outlined in LGS-PRA and Prompter should be compatible with our approach as well. The integration of these techniques with EPA an interesting area to investigate for future works. Our method exhibits lower PLWSR and PLWGC due to our initial 500 exploration steps. These steps are convenient for our planner to amass a diverse of of objects clusters and action angles to determine a proper expandable initial state. Although a pure planning-based exploration would eventually provide us with all the information required to solve the current task, it tends to excessively exploit known object clusters before exploring unknown areas. We only save object information when they are within immediate reach which means the agent is acting on immediate observations. This is done for convenience of converting observation into symbolic state and to reduce the plan length generated by the planner. Additional integration with a top-down SLAM map should help decrease the initial exploration steps, consequently enhancing both PWSR and PLWGC.

    &  &  \\   & SR & GC & PLWSR & PLWGC & **SR** & GC & PLWSR & PLWGC \\ Seq2Seq & 3.98 & 9.42 & 2.02 & 6.27 & 0.39 & 7.03 & 0.08 & 4.26 \\ ET & 38.42 & 45.44 & 27.78 & 34.93 & 8.57 & 18.56 & 4.1 & 11.46 \\ HLSM & 25.11 & 35.15 & 10.39 & 14.17 & 24.46 & 34.75 & 9.67 & 13.13 \\ FILM & 28.83 & 39.55 & 11.27 & 15.59 & 27.8 & 38.52 & 11.32 & 15.13 \\ LGS-RPA & 40.05 & 48.66 & 21.28 & 28.97 & 35.41 & 45.24 & 15.68 & 22.76 \\
**EPA** & 39.96 & 44.14 & 2.56 & 3.47 & **36.07** & 39.54 & 2.92 & 3.91 \\  Prompter & 53.23 & 64.43 & 25.81 & 30.72 & 45.72 & 58.76 & 20.76 & 26.22 \\   

Table 1: Comparison of our method with other methods on the ALFRED challenge. The challenge declare as winner the method with higher Unseen Success Rate (SR). For all metrics, higher is better. EPA is our approach. Under the line are approaches submitted after the challenge leader board closed.

### Ground Truth Ablation

Our model, trained on the ALFRED dataset, incorporates neural perception modules to understand potential bottlenecks in visual perception and language, as explored via ablation studies detailed in Table 3. We observed modest enhancements when providing the model with ground truth depth and segmentation, with ground truth language offering the most significant improvement (18.22% and 19.87%). Our egocentric planner, responsible for generating dynamic policies through observation and interaction, proves more resilient to interaction errors compared to policy-template based methods like HLSM and FILM (MCR\({}^{+}\)22, BPF\({}^{+}\)21). Thus, having the correct interpretation of the task description impacts our method more significantly than ground truth perception. We examined common reasons for failed episodes using both the seen and unseen datasets. The most prevalent error (23.54% unseen) is the inability of the agent to locate objects of interest due to failed segmentation or an unreachable angle. Errors stemming from language processing, where the predicted task deviates from the ground truth, are given priority. The results can be seen in Table 2.

### Parameter Ablation

Neural vision modules may struggle with out-of-distribution data, such as ALFRED's unseen environments, leading to potential errors in object segmentation and distance estimation. Unlike hybrid methods like FILM and HLSM, our goal-oriented egocentric planner reassesses the environment after failure, generating new action sequences. For instance, in a task to hold a book under a lamp, a failure due to obscured vision can be addressed by altering the action sequence, as illustrated by the drop in performance to 27.31% in our ablation study (Table 4).

FILM's semantic search policy, trained to predict object locations using top-down SLAM maps, differs from our symbolic semantic graph representation, which precludes us from adopting FILM's approach. Instead, we hand-crafted two strategies to search areas around objects and receptacles based on common sense knowledge, which improved our Success Rate by 7.22% with no training. However, our agent only forms a node when it visits a location, necessitating an initial observation phase. Neglecting this step resulted in a 9.35% performance drop. FILM's SLAM-based approach can observe distant objects, requiring fewer initial observation steps and leading to better PLWSR and PLWGC. Combining a top-down SLAM map with our semantic graph could mitigate the need for our initial observation step and improve these scores.

Our algorithm's frequently re-plans according to new observation which is also essential for real-world applications using online planning system. Using modern domain-independent planners, such

   Ablation - parameters &  \\   & SR & GC & PLWSR & PLWGC \\ Base Method & 40.11 & 44.14 & 2.04 & 3.71 \\ w/o init observation & 30.75 & 36.16 & 17.34 & 21.93 \\
200 init observation & 37.3 & 42.23 & 12.41 & 16.34 \\ w/o object pref & 32.89 & 35.95 & 3.05 & 3.81 \\ w/o fault recovery & 27.31 & 26.55 & 2.41 & 3.36 \\ w/o symbolic planning & 3.47 & 4.61 & 2.16 & 2.98 \\   

Table 4: Ablation study with different parameters

   Failure Modes & Seen & Unseen \\  Obj not found & 19.36 & 23.54 \\ Collison & 9.14 & 11.34 \\ Interactions fail & 7.33 & 8.98 \\ Obj in closed receptcle & 18.21 & 16.33 \\ Language error & 17.92 & 21.31 \\ Others & 28.04 & 18.5 \\    
  Ablation &  &  \\   & SR & GC & SR & GC \\ Base Method & 40.11 & 44.14 & 45.78 & 51.03 \\ w/ gt segment & 45.13 & 49.72 & 49.22 & 53.21 \\ w/ gt depth & 41.85 & 46.03 & 48.98 & 51.46 \\ w/ gt language & 52.29 & 56.39 & 60.55 & 64.76 \\ w/ all gt & 58.33 & 63.71 & 68.49 & 73.35 \\   

Table 2: Percentage Error of each types in the validation set.

as Fast Downward and Fast Forward (Hof01, Hel06), we've significantly reduced planning time. We tested a blind breadth-first search algorithm but found it impractical due to the time taken. A 10-minute cut-off was introduced for each task, which didn't impact our results due to an average execution time of under 2 minutes. Yet, this constraint severely lowered the success rate of the blind search method, only completing 3.47% of the validation unseen tasks (Table 4).

### Generalization to Other Tasks

Our approach allows zero-shot generalization to new tasks within the same set of objects and relationships, offering adaptability to new instructions without manually specify new task types and templates. While neural network transfer learning remains challenging in embodied agent tasks, and FILM's template system requires hand-crafted policies for new tasks, our egocentric planning breaks a task into a set of goal conditions, autonomously generating an action sequence. This allows our agent to execute tasks without transfer learning or new task templates. We have chosen environments in which all objects for each task have been successfully manipulated in at least one existing ALFRED task. This selection is made to minimize perception errors. We tested this on five new task types not in the training data, achieving an 82% success rate, as shown in Table 6, sectionC.2 of the Supplementary Material. We could also adapt to new objects, actions, and constraints, such as time and resource restrictions. However, the current ALFRED simulator limits the introduction of new objects and actions. This flexibility to accommodate new goals is a key advantage of our planning-based approach.

## 9 Discussion

Our contribution poses Embodied Agents as acting in a Partially Observable Markov Decision Process (POMDP), which allows us to tackle a broad class of problems, and further expands on the potential applications of planning-based methods for embodied agents. By adopting this approach, our method offers a more flexible and adaptable solution to address the challenges and limitations associated with fixed-policy and handcrafted methods.

We found that using intentional models with a factored representation in embodied agents is crucial. Whereas a learned world model provides a degree of useful information, their associated policies might under-perform for long horizon high-level decision-making, unless they are trained on massive amounts of interactions (SSS\({}^{+}\)17). Comparing a world model with classical planning uncovers interesting distinctions. While creating a PDDL-based solution requires an initial investment of time, that time is amortized in contexts that require quality control per type and skill. On the other hand, the cost spent on crowdsourcing or interacting with a physical environment can be significant (STG\({}^{+}\)20).

Learning classical planning models is relatively straightforward with given labels for objects and actions (AFP\({}^{+}\)18, CDVA\({}^{+}\)22). On the other hand, if we aim to learn a world model, we should consider the performance at planning time and end-to-end solutions may not be the ideal approach in this context. When compared to intentional symbolic-based methods, end-to-end strategies tend to compound errors in a more obscure manner. Furthermore, as learned world models rely on blind search or MCTS, in new tasks the model might waste search time in a subset of actions that are irrelevant if action space is large, while symbolic planning can obtain long plans in a few seconds. We also propose addressing challenging planning tasks using simpler planning techniques, providing a foundation for more complex planning paradigms. Our method could be further enhanced by encoding explicit constraints and resources, such as energy.

While other methods might outperform our model in the ALFRED benchmark, they are often limited in scope. For instance, despite its high unseen success rate, the Prompter model serves as a baseline rather than a comprehensive solution for embodied agents. Our contribution provides a method that can adapt to shifts in task distribution without changing the symbolic planning model, while also supports changes in the agent's capabilities by modifying the symbolic planning model.

We acknowledge certain limitations in our approach. While we support safe exploration assuming reversible actions, real scenarios beyond ALFRED could include irreversible actions and failures. Our model is also sensitive to perception errors, making end-to-end methods potentially more robust in specific scenarios. Furthermore, our perception model lacks memory, potentially hindering nuancedbelief tracking. Despite these challenges, our method offers a promising direction for future research in embodied agent applications. In section B, we expand on the discussion of our approach.

## 10 Conclusion and Future Work

Our work introduces a novel iterative replanning approach that excels in embodied task solving, setting a new baseline for ALFRED. Using off-the-shelf automated planners, we improve task decomposition and fault recovery, offering better adaptability and performance than fixed instruction and end-to-end methods. Moving forward, we aim to refine our location mapping for precise environment understanding, extract more relational data from instructions, and integrate LLMs as exploration heuristics. We plan to tackle NLP ambiguity in multi-goal scenarios by prioritizing and pruning goals based on new information. Moreover, we aim to upgrade our egocentric planner to manage non-deterministic effects, broadening our method's scope beyond ALFRED. This direction, leveraging efficient planning methods for non-deterministic actions, is promising for future research (MBM14).

#### Acknowledgments

We want to acknowledge the ServiceNow research for providing the research opportunity and computational infrastructure to conduct this research project.This work was also made possible due to the assistance of Cyril Ibrahim at ServiceNow during the experimental execution phases of our project. We would also like to thank members of Mulab at Queen's University for providing a supporting research environment. Lastly, we want to acknowledge the assistance from organizers of the CVPR Embodied Agent Workshop for their guidance and feedback throughout the competition(DBB\({}^{+}\)22).