# The Star Geometry of Critic-Based

Regularizer Learning

Oscar Leong

Department of Statistics and Data Science

University of California, Los Angeles

oleong@stat.ucla.edu

&Eliza O'Reilly

Department of Applied Mathematics and Statistics

Johns Hopkins University

eoreill2@jh.edu

&Yong Sheng Soh

Department of Mathematics

National University of Singapore

matsys@nus.edu.sg

###### Abstract

Variational regularization is a classical technique to solve statistical inference tasks and inverse problems, with modern data-driven approaches parameterizing regularizers via deep neural networks showcasing impressive empirical performance. Recent works along these lines learn task-dependent regularizers. This is done by integrating information about the measurements and ground-truth data in an unsupervised, critic-based loss function, where the regularizer attributes low values to likely data and high values to unlikely data. However, there is little theory about the structure of regularizers learned via this process and how it relates to the two data distributions. To make progress on this challenge, we initiate a study of optimizing critic-based loss functions to learn regularizers over a particular family of regularizers: gauges (or Minkowski functionals) of star-shaped bodies. This family contains regularizers that are commonly employed in practice and shares properties with regularizers parameterized by deep neural networks. We specifically investigate critic-based losses derived from variational representations of statistical distances between probability measures. By leveraging tools from star geometry and dual Brunn-Minkowski theory, we illustrate how these losses can be interpreted as dual mixed volumes that depend on the data distribution. This allows us to derive exact expressions for the optimal regularizer in certain cases. Finally, we identify which neural network architectures give rise to such star body gauges and when such regularizers have favorable properties for optimization. More broadly, this work highlights how the tools of star geometry can aid in understanding the geometry of unsupervised regularizer learning.

## 1 Introduction

The choice and design of regularization functionals to promote structure in data has a long history in statistical inference and inverse problems, with roots dating back to classical Tikhonov regularization . In such problems, one is tasked with solving the following: given measurements \(y^{m}\) of the form \(y=(x_{0})+\) for some forward model \(:^{d}^{m}\) and noise \(^{m}\), recover an estimate of the ground-truth signal \(x_{0}^{d}\). Typically, the challenge in such problems is that they are ill-posed, meaning that either there are no solutions, infinitely many solutions, or the problem is discontinuous in the data \(y\). A pervasive and now classical technique is to identify regularization functionals \(:^{d}\) such that, when minimized, promote structure that is present in the groundtruth signal. Well-known examples of hand-crafted regularizers to promote structure include the \(_{1}\)-norm to promote sparsity [23; 16; 26; 88], total variation , the nuclear norm [29; 15; 70], and more generally, atomic norms [18; 12; 87; 78; 64].

With the development of modern machine learning techniques, we have seen a surge of data-driven methods to directly learn regularizers instead of designing regularizers in a hand-crafted fashion. Early works along these lines include the now-mature field of dictionary learning or sparse coding (see [60; 61; 84; 3; 6; 1; 2; 75; 76; 10; 81] and the surveys [56; 27] for more). More recently, we have seen powerful nonconvex regularizers parameterized by deep neural networks. These come in a variety of flavors, including those based on plug-and-play [91; 71], generative models [13; 39; 40; 8; 20], and regularization functionals directly parameterized by deep neural networks [53; 59; 80; 36; 47].

Despite the widespread success of such learning-based regularizers, several outstanding questions remain. In particular, it is unclear what type of structure is learned by such regularizers. For example, what is the relationship between the underlying data geometry and the type of regularizer found by data-driven approaches? Is the found regularizer "optimal" in some meaningful sense? If not, what properties of the data distribution are lost due to structural constraints placed on the regularizer?

In this work, we aim to tackle the above questions and others to further understand what types of regularizers are found via learning-based methods. We focus on a class of unsupervised methods that are _task-dependent_, i.e., those that learn a regularizer without paired training data, but still integrate information about the measurements in the learning process. This is done by identifying a loss inspired by variational representations of statistical distances, such as Integral Probability Metrics (IPMs) or divergences, where the regularizer plays the role of the "critic" or test function.

As an example, the recent adversarial regularization framework used in [53; 59; 80] learns a regularizer that assigns high "likelihood" to clean data \(_{r}\) and low "likelihood" to noisy data \(_{n}\) via a loss derived from a dual formulation of the \(1\)-Wasserstein distance. This framework has showcased impressive empirical performance in learning data-driven regularizers, but there is still a lack of an overarching understanding of the structure of regularizers learned. Moreover, while this Wasserstein distance interpretation has shown to be useful, it is natural to consider if other losses can be derived from this "critic-based" perspective to learn regularizers.

### Our contributions

In order to make progress on these challenges, we first fix a family of regularization functionals to analyze. We aim for this family to be (i) expressive (can describe both convex and nonconvex regularizers), (ii) exhibit properties akin to regularizers used in practice, and (iii) tractable to analyze. A family of functionals that satisfy such criteria are _gauges of star bodies_. In particular, we will consider regularization functionals of the form

\[\|x\|_{K}:=\{t>0:x tK\}\]

where \(K\) is a _star body_, i.e., a compact subset of \(^{d}\) with \(0(K)\) such that for each \(x^{d}\{0\}\), the ray \(\{tx:t>0\}\) intersects the boundary of \(K\) exactly once. Such regularizers are nonconvex for general star bodies \(K\), but note that \(\|\|_{K}\) is convex if and only if \(K\) is a convex body. Any norm is the gauge of a convex body, but this class also includes nonconvex quasinorms such as the \(_{q}\)-quasinorm for \(q(0,1)\).

Focusing on the above class of functionals, we aim to understand the structure of a regularizer \(\|\|_{K}\) found by solving an optimization problem of the form

\[_{\|\|_{K}}(\|\|_{K};_{ r},_{n})\] (1)

where \((;_{r},_{n}):\) compares the values \(\|\|_{K}\) assigns to \(_{r}\) and \(_{n}\) and \(\) is a class of functions on \(^{d}\). The adversarial regularization framework of  is recovered by setting \((f;_{r},_{n})=_{_{r}}[f( x)]-_{_{n}}[f(x)]\) and \(=(1):=\{f:^{d}:f\}\).

Our contributions are as follows:

1. Using tools from star geometry [41; 33] and dual Brunn-Minkowski theory , we prove that under certain conditions, the solution to (1) under the adversarial regularization framework of  can be exactly characterized. In particular, we show that the objective is equivalent to a _dual mixed volume_ between our star body \(K\) and a data-dependent star body \(L_{r,n}\). This allows us to exploit known bounds on the dual mixed volume via (dual) Brunn-Minkowski theory.
2. We investigate new critic-based loss functions \((;_{r},_{n})\) in (1) inspired by variational representations of divergences for probability measures. We specifically analyze \(\)-divergences and show how they can give rise to loss functionals with dual mixed volume interpretations. We also experimentally show that such losses can be competitive for learning regularizers in a simple denoising setting.
3. We conclude with results showcasing when these star body regularizers exhibit useful optimization properties, such as weak convexity, as well as analyzing what types of neural network architectures give rise to star body gauges.

This paper is organized as follows: Section 2 analyzes optimal adversarial regularizers using star geometry. We establish a general existence result in Section 2.1 and use dual mixed volumes to characterize the optimal star body regularizer under certain assumptions in Section 2.2. Visual examples are provided in Section 2.3. Section 3 introduces new critic-based losses for learning regularizers inspired by \(\)-divergences. An empirical comparison between neural network-based regularizers learned using these losses and the adversarial loss is presented in Section 3.1. Section 4 examines computational properties of star body regularizers, such as beneficial properties for optimization and relevant neural network architectures. We conclude with a discussion in Section 5.

### Related work

We discuss the broader literature on learning-based regularization in Section A of the appendix and focus on optimal regularization here. To our knowledge, there are few works that analyze the optimality of a regularizer for a given dataset. The authors of the original adversarial regularization framework of  analyzed the case when \(_{r}\) is supported on a manifold and \(_{n}\) is related to \(_{r}\) via the push-forward of the projection operator onto the manifold. They showed that in this case the distance function to the manifold is an optimal regularizer, but uniqueness does not hold (i.e., other regularizers could be optimal as well). Our work complements such results by analyzing the structure of the optimal regularizer for a variety of \(_{r}\) and \(_{n}\) which may not be related in this way. In  the authors analyze the optimal Tikhonov regularizer in the infinite-dimensional Hilbert space setting, and show that the optimal regularizer is independent of the forward model and only depends on the mean and covariance of the data distribution.

The two most closely related papers to ours are  and . In , the authors aim to characterize the optimal convex regularizer for linear inverse problems. Several notions of optimality for convex regularizers are introduced (dubbed compliance measures) and the authors establish that canonical low-dimensional models, such as the \(_{1}\)-norm for sparsity, are optimal for sparse recovery under such compliance measures. In , similarly to this work, the authors analyze the optimal regularizer amongst the family of star body gauges for a given dataset. They also leverage dual Brunn-Minkowski theory to show that the optimal regularizer is induced by a data-dependent star body, whose radial function depends on the density of the data distribution. Interestingly, these results also characterize data distributions for which convex regularization is optimal and they provide several examples. The present work is novel in relation to  for several reasons. First, our work introduces novel theory and a new framework for understanding critic-based regularizer learning, addressing a significant gap in existing theoretical foundations for unsupervised regularizer learning. This new setting also brings novel technical challenges that were not present in . For example, the losses in Section 3 exhibit more complicated dependencies on the star body of interest, leading to the need for new analysis. Such losses are also experimentally analyzed in Section C.1. Finally, we present new results that are relevant to downstream applications of star body regularizers in Section 4.

### Notation

In this section, we provide some brief background on star geometry and define notation used in the main body of the paper. For further details, please see Section B.1 in the appendix and the sources [77; 33; 41] for more. We say that a closed set \(K^{d}\) is _star-shaped_ (with respect to the origin) if for all \(x K\), we have \([0,x] K\) where, for two points \(x,y^{d}\), we define the line segment \([x,y]:=\{(1-t)x+ty:t\}\). \(K\) is a _star body_ if it is a compact star-shapedset such that for every \(x 0\), the ray \(R_{x}:=\{tx:t>0\}\) intersects the boundary of \(K\) exactly once. Equivalently, \(K\) is a star body if its _radial function_\(_{K}\) is positive and continuous over the unit sphere \(^{d-1}\), where \(_{K}\) is defined as \(_{K}(x):=\{t>0:t x K\}\). It follows that the gauge function of \(K\) satisfies \(\|x\|_{K}=1/_{K}(x)\) for all \(x^{d}\) such that \(x 0\). Let \(^{d}\) to be the space of all star bodies on \(^{d}\). For \(K,L^{d}\), it is easy to see that \(K L\) if and only if \(_{K}_{L}\). The _kernel_ of a star body \(K\) is the set of all points for which \(K\) is star-shaped with respect to, i.e., \((K):=\{x K:[x,y] K,\  y K\}\). Note that \((K)\) is a convex subset of \(K\) and \((K)=K\) if and only if \(K\) is convex. For a parameter \(>0\), we define the following subset of \(^{d}\) consisting of _well-conditioned_ star bodies with nondegenerate kernels: \(^{d}():=\{K^{d}: B^{d}(K)\}\) where \(B^{d}:=\{x^{d}:\|x\|_{_{2}} 1\}\). For two distributions \(P\) and \(Q\), let \(P Q\) denote their convolution, i.e., \(P Q\) is the distribution of \(X+Y\) where \(X P\) and \(Y Q\). Finally, for a function \(f\) and a measure \(P\), let \(f_{\#}(P)\) denote the push-forward measure of \(P\) under \(f\), i.e., for all measurable subsets \(A\), we have \(f_{\#}(P)(A):=P(f^{-1}(A))\).

## 2 Adversarial star body regularization

To illustrate our tools and results, we first consider regularizer learning under the adversarial regularization framework of . Given two distributions \(_{r}\) and \(_{n}\) on \(^{d}\) and setting \((f;_{r},_{n})=_{_{r}}[f (x)]-_{_{n}}[f(x)]\) and \(:=(1)\) in (1), we aim to understand minimization of the functional \(F(K;_{r},_{n}):=_{_{r}}[\|x\|_{K} ]-_{_{n}}[\|x\|_{K}]\) over all star bodies \(K\) such that \(x\|x\|_{K}\) is \(1\)-Lipschitz. A result due to  shows that \(\|\|_{K}\) is \(1\)-Lipschitz if and only if the unit ball \(B^{d}(K)\). Here and throughout this paper, it will be useful to think of \(_{r}\) as the distribution of ground-truth, clean data while \(_{n}\) is a user-defined distribution describing noisy, undesired data. Using the notation from Section 1.3, we are interested in analyzing

\[_{K^{d}(1)}F(K;_{r},_{n}).\] (2)

### Existence of minimizers

As the problem (2) requires minimizing a functional over a structured subset of the infinite-dimensional space of star bodies, even basic questions such as the existence of minimizers are unclear. Despite this, one can exploit tools in the star geometry literature to obtain guarantees regarding this problem. The proof of this result exploits Lipschitz continuity of the objective functional and local compactness properties of \(^{d}()\), proved in , akin to the celebrated Blaschke's Selection Theorem from convex geometry . We defer the proof to the appendix in Section B.2.

**Theorem 2.1**.: _For any two distributions \(_{r}\) and \(_{n}\) on \(^{d}\), we have that_

\[F(K;_{r},_{n}) W_{1}(_{r},_{n})K^{d}(1)\]

_where \(W_{1}(,)\) is the \(1\)-Wasserstein distance between two distributions. Moreover, if \(_{_{i}}[\|x\|_{_{2}}]<\) for each \(i=r,n\), then we always have that minimizers exist:_

\[*{arg\,min}_{K^{d}(1)}F(K;_{r},_{n}).\]

### Minimization via dual Brunn-Minkowski theory

We now aim to understand the structure of minimizers to the above problem. To do this, we first show that the objective in (2) can be interpreted as the dual mixed volume between \(K\) and a data-dependent star body. To begin, we start with a definition.

**Definition 2.2** (Definition 2* in ).: Given two star bodies \(K,L^{d}\), the \(i\)-th dual mixed volume between \(L\) and \(K\) for \(i\) is given by

\[_{i}(L,K):=_{^{d-1}}_{L}(u)^{d-i}_{K }(u)^{i}u.\]

One can think of dual mixed volumes as functionals that measure the size of a star body \(K\) relative to another star body \(L\). Note that for all \(i\), \(_{i}(K,K)=_{^{d-1}}_{K}(u)^{d}u =_{d}(K)\) is the usual \(d\)-dimensional volume of \(K\). Of particular interest to us will be the case \(i=-1\).

To see how our main objective can be interpreted as a dual mixed volume, we need to be able to summarize our distributions \(_{r}\) and \(_{n}\) in such a way that can naturally be related to the gauge \(\|\|_{K}\). Since the gauge is positively homogenous, it is characterized by its behavior on the sphere \(^{d-1}\). Hence, given a distribution, we require a "summary statistic" that describes the distribution in each unit direction \(u^{d-1}\). Since star bodies are precisely defined by the distance of the origin to their boundary in each unit direction, we ideally would like to summarize the distribution in a similar fashion. For a distribution \(\) with density \(p\) on \(^{d}\), define the map

\[_{p}(u):=(_{0}^{}t^{d}p(tu)t)^{1/(d+1)},\; u^{d-1}\,.\] (3)

This function measures the average mass the distribution accrues in each unit direction and how far this mass lies from the origin. More precisely, one can show  that \(_{p}^{d+1}\) is the density of the measure \(_{}():=_{}[\|x\|_{_{2}} _{\{x/\|x\|_{_{2}}\}}].\) If the map (3) is positive and continuous over the unit sphere, it defines a unique data-dependent star body. In Figure 1, we give an illustrative example showing how the geometry of the data distribution relates to the data-dependent star body. Here, the data distribution is given by a Gaussian mixture model with \(3\) Gaussians.

We now aim to understand the structure of minimizers of the functional \(K F(K;_{r},_{n})\). As we will show in our main result, the map (3) aids in interpreting the functional via a dual mixed volume. Extrema of such dual mixed volumes have been the object of study in convex and star geometry for some time. We cite a seminal result due to Lutwak.

**Theorem 2.3** (Special case of Theorem 2 in ).: _For star bodies \(K,L^{d}\), we have_

\[_{-1}(L,K)^{d}_{d}(L)^{d+1}\,_{d}(K) ^{-1},\]

_and equality holds if and only if \(K\) and \(L\) are dilates, i.e., there exists a \(>0\) such that \(K= L\)._

Armed with this inequality, we show that under certain conditions, we can guarantee the existence and uniqueness of minimizers of the above map. Note that optimal solutions are always defined up to scaling. To remove this degree of freedom, we impose an additional constraint on the volume of the solution. We chose a unit-volume constraint for simplicity, but changing the constraint would simply scale the optimal solution.

**Theorem 2.4**.: _Suppose \(_{r}\) and \(_{n}\) are distributions on \(^{d}\) that are absolutely continuous with respect to the Lebesgue measure with densities \(p_{r}\) and \(p_{n}\), respectively. Suppose \(_{p_{r}}\) and \(_{p_{n}}\) as defined in (3) are continuous over the unit sphere and that \(_{p_{r}}(u)>_{p_{n}}(u) 0\) for all \(u^{d-1}\). Then, there exists a star body \(L_{r,n}^{d}\) such that the unique solution to the problem_

\[_{K^{d}:_{d}(K)=1}_{_{r}}[ \|x\|_{K}]-_{_{n}}[\|x\|_{K}]\]

_is given by \(K_{*}:=_{d}(L_{r,n})^{-1/d}L_{r,n}\). If \(B^{d}(K_{*})\), then \(K_{*}\) is, in fact, the unique solution to (2) with the additional constraint \(_{d}(K)=1\)._

Proof Sketch.: One can prove that for each \(i=r,n\), \(_{_{r}}[\|x\|_{K}]=_{^{d-1}}_{p_{i}}(u )^{d+1}_{K}(u)u\). Then since \(_{p_{r}}>_{p_{n}} 0\) on the unit sphere, we have that the map \(_{r,n}(u):=(_{p_{r}}(u)^{d+1}-_{p_{n}}(u)^{d+1})^{1/(d+1)}\) is positive and continuous on \(^{d-1}\). We can then prove that it is the radial function of a new data-dependent star body \(L_{r,n}\). Combining the previous two observations yields \(_{_{r}}[\|x\|_{K}]-_{_{n}}[\|x\|_{K}] =d_{-1}(L_{r,n},K)\). Applying Theorem 2.3 obtains the final result. We defer the full proof to Section B.2. 

_Remark 2.5_ (Uniqueness guarantees).: We highlight that in our result we are able to obtain uniqueness of the solution everywhere. This is notably different than previous guarantees in the Optimal Transport literature, where the optimal transport potential for the \(1\)-Wasserstein loss is not unique, but can be

Figure 1: (Left) Contours of the Gaussian mixture model density \(p\). (Right) The star body \(K_{p}\) induced by the radial function (3).

shown to be unique \(_{n}\)-almost everywhere . A significant reason for this is that our optimization problem is over the family of star body gauges, which have additional structure over the general class of \(1\)-Lipschitz functions. In particular, star bodies are uniquely defined by their radial functions and, hence, dual mixed volumes can exactly specify them (see  for more details).

_Remark 2.6_ (Distributional assumptions).: Note that the conditions of this theorem require that the density-induced map \(_{p}()\) must be a valid radial function (i.e., positive and continuous over the unit sphere). Many distributions satisfy these assumptions. For example, if the distribution's density is of the form \(p(x)=(\|x\|_{L}^{q})\) where \(_{0}^{}t^{d}(t^{q})t<\), \(q>0\), and \(L^{d}\), then the map \(_{p}()\) is a valid radial function. See  for more examples of distributions that satisfy these assumptions.

_Remark 2.7_ (Finite-data regime).: While the conditions of this theorem would not apply to the case when \(_{r}\) and \(_{n}\) are empirical distributions, we prove in the appendix (Section D.1) that these results are stable in the sense that as the amount of available data goes to infinity, the solutions in the finite-data regime converge (up to a subsequence) to a population minimizer. The proof exploits tools from variational analysis, such as \(\)-convergence .

_Remark 2.8_ (Implications for inverse problems).: In the original framework of , the authors considered \(_{n}=_{\#}^{}(_{y})\) where \(\) is a linear forward operator in an inverse problem. Since inverse problems are typically under-constrained, such a distribution would be singular and not satisfy the assumptions of the theorem. One can solve this, however, by convolving \(_{n}\) with a Gaussian to ensure it has full measure.

_Remark 2.9_ (Scaling distributions).: One can always reweight the objective so that the assumptions of the theorem are satisfied. This is due to positive homogeneity of the gauge, which guarantees \(_{}[\| x\|_{K}]=\,_{}[ \|x\|_{K}]\) for any distribution \(\). In particular, if \(_{p_{r}}>_{p_{n}}\) is not satisfied, one can choose a scaling \(\) so that \(_{p_{r}}>^{1/(d+1)}_{p_{n}}\). Inspecting the proof of Theorem 2.4, one can see that the result goes through when applied to the objective \(_{_{r}}[\|x\|_{K}]-\,_{_{n}}[ \|x\|_{K}]\).

### Examples

We now provide examples of optimal regularizers via different choices of \(_{r}\) and \(_{n}\). Further examples can be found in the appendix in Section D.3. Throughout these examples, the star body induced by \(_{r}\) and \(_{n}\) is denoted by \(L_{r}\) and \(L_{n}\), respectively.

**Example 1** (Gibbs densities with \(_{1}\)- and \(_{2}\)-norm energies).: _Suppose the distributions \(_{r}\) and \(_{n}^{}\) for some \( 1\) are given by Gibbs densities with \(_{1}\)- and \(_{2}\)-norm energies, respectively:_

\[p_{r}(x)=e^{-\|x\|_{_{1}}}/(c_{d}_{d}(B_{_{1}})) { and }p_{n}^{}(x)=e^{-\|x\|_{_{2}}}/(c_{d}_{ d}(B^{d}/()).\]

_Here \(c_{d}:=(d+1)\) where \(()\) is the usual Gamma function. The star bodies induced by \(p_{r}\) and \(p_{n}^{}\) are dilations of the \(_{1}\)-ball and \(_{2}\)-ball, respectively. Denote these star bodies by \(L_{r}\) and \(L_{n}^{}\), respectively. Then, \(L_{r,n}^{}\) is defined by the difference of radial functions via_

\[_{L_{r,n}^{}}(u):=(c_{r}\|x\|_{_{1}}^{-(d+1)}-c_{n}( \|x\|_{_{2}})^{-(d+1)})^{1/(d+1)}\]

_where \(c_{r}:=_{0}^{}t^{d}(-t)/(c_{d}_{d}(B_{_{1}}))\) and \(c_{n}:=_{0}^{}t^{d}(-t)/(c_{d}_{d}(B^{d}/( )))\). We visualize the geometry of \(L_{r,n}^{}\) for different values of \( 1\) in Figure 2 for \(d=2\). We see that for directions such that the boundaries of \(L_{r}\) and \(L_{n}^{}\) are far apart (namely, the \( e_{i}\) directions), the optimal regularizer assigns small values. This is because such directions are considered highly likely under the distribution \(_{r}\) and less likely under \(_{n}^{}\). However, for directions in which the boundaries are close (e.g., in the \([0.5,0.5]\)-direction), the regularizer assigns high values, since such directions are likely under the noise distribution \(_{n}^{}\). This aligns with the aim of the objective function - namely, that a regularizer should assign low values to likely directions and high values to unlikely ones._

**Example 2** (Toy inverse problem).: _Consider the following example where \(x_{r}:=(0,)\) where \(^{d d}\) and we have measurements \(y=Ax\) where \(A^{m d}\) with rank \(m d\). We consider the case when \(_{n}:=A_{\#}^{}(_{y})\) and \((0,^{2}I_{d})\) where \(y_{y}\) if and only if \(y=Ax\) where \(x_{r}\). Let \(d=2\), \(=UU^{T}\), and \(A==e_{1}^{T}^{1 2}\). Then \(A^{}=e_{1}\). Note that \(y_{y}\) if and only if \(y=e_{1}^{T}Uz=u_{1}^{T}z\) where \(u_{1}^{T}\) is the first row of \(U\) and \(z(0,I_{2})\). By standard properties of Gaussians, \(_{y}=(0,\|u_{1}\|_{_{2}}^{2})\). Hence \(_{n}=(0,D_{})\) where \(D_{}:=(\|u_{1}\|_{_{2}}^{2}+^{2},^{2}) ^{2 2}\). We visualize this example in Figure 3. We see that the regularizer induced by \(L_{r,n}\) penalizes directions in the row span of \(A\), but does not for directions in the kernel of \(A\)._

## 3 Critic-based loss functions via \(f\)-divergences

Inspired by the use of the variational representation of the Wasserstein distance to define loss functions to learn regularizers, we consider whether other divergences can give rise to valid loss functions and if they can be interpreted as dual mixed volumes. A general class of divergences of interest will be \(f\)-divergences: for a convex function \(f:(0,)\) with \(f(1)=0\), if \(P Q\), \(D_{f}(P||Q):=_{^{d}}f(P}{Q}) Q\).

\(\)-Divergences:For \((-,0)(0,1)\), there is a variational representation of the \(\)-divergence where \(f=f_{}\) with \(f_{}(x)=- x-(1-)}{(-1)}\):

\[D_{f_{}}(P||Q):=D_{}(P||Q):=-_{h: ^{d}(0,)}(_{Q}[}{ }]+_{P}[}{1-}] ).\] (4)

Note that the domain of functions in this variational representation consists of positive functions, which aligns with the class of gauges \(\|\|_{K}\). Many well-known divergences are \(\)-divergences for specific \(\), such as the \(^{2}\)-divergence (\(=-1\)) and the (squared) Hellinger distance (\(=1/2\)) . We discuss in the appendix (Section B.3.1) how one can derive a general loss based on \(\)-divergences, but focus on a particular case here to illustrate ideas.

The Hellinger Distance:Setting \(=1/2\) and performing an additional change of variables in (4), we have the following useful representation of the (squared) Hellinger distance:

\[H^{2}(P||Q):=2-_{h>0}_{P}[h(x)]+_{Q}[h(x)^{-1}].\]

This motivates minimizing the following functional, which carries a similar intuition to (2) that the regularizer should be small on real data and large on unlikely data:

\[K_{_{r}}[\|x\|_{K}]+_{_{n}}[ \|x\|_{K}^{-1}].\] (5)

The following theorem, proved in Section B.3, shows that this loss functional is equal to single dual mixed volume between a data-dependent star body \(L_{r}\) and a star body that depends on \(_{r},_{n}\), and\(K\). We can show that the equality cases of the lower bound only hold for dilations within a specific interval, with only two specific star bodies achieving equality.

**Theorem 3.1**.: _Suppose \(_{r}\) and \(_{n}\) admit densities \(p_{r}\) and \(p_{n}\), respectively, such that the following maps are positive and continuous over the unit sphere:_

\[u_{_{r}}(u)^{d+1}:=_{0}^{}t^{d}p_{r}(tu) tu_{_{n}}(u)^{d-1}:=_{0}^{}t^{d-2}p_{ n}(tu)t.\]

_Let \(L_{r}\) and \(_{n}\) denote the star body with radial function \(_{_{r}}\) and \(_{_{n}}\), respectively. Then, for any \(K^{d}\), there exists a star body \(K_{r,n}\) that depends on \(_{r},_{n},\) and \(K\) such that the functional (5) is equal to \(d_{-1}(L_{r},K_{r,n})\). We also have the inequality \(_{-1}(L_{r},K_{r,n})_{d}(K_{r,n})^{-1/d}\, _{d}(L_{r})^{(d+1)/d}\) with equality if and only if \(K_{r,n}\) is a dilate of \(L_{r}\). Moreover, there exists a \(_{*}:=_{*}(_{r},_{n})>0\) such that for every \((0,_{*}]\), there are only two star bodies \(K_{+,}\) and \(K_{-,}\) that satisfy \(K_{r,n}= L_{r}\):_

\[_{K_{,}}(u):=}(u)^{d}}{2_{L_{n}}(u) ^{d-1}}(1(_{n}}(u)}{ _{L_{r}}(u)})^{d-1}})\]

_Remark 3.2_.: Among the two star bodies \(K_{+,},K_{-,}\) that achieve \(K_{r,n}= L_{r}\), we argue that \(K_{+,}\) induces a better regularizer than \(K_{-,}\). As seen in previous examples, we would like our regularizer \(\|\|_{K}\) to assign low values to likely data and high values to unlikely data. Equivalently, we would like \(_{K}()\) to be large on likely data and small on unlikely data. This can be stated in terms of the geometry of \(L_{r}\) and \(_{n}\): we would like \(_{K}\) to be large when \(_{L_{r}}\) is large and \(_{_{n}}\) is small. Likewise, \(_{K}\) should be small when \(_{L_{n}}\) is large and \(_{L_{r}}\) is small. Due to the small sign difference between \(K_{+,}\) and \(K_{-,}\), \(K_{+,}\) better captures this intuition. We show a visual example of this in Figure 4, where \(_{r}\) and \(_{n}\) are the same distributions as in Example 1.

### Empirical comparison with adversarial regularization

We now aim to understand whether the Hellinger-based loss (5) can provide a practical loss function to learn regularizers. To do this, we consider denoising on the MNIST dataset . We take \(10000\) random samples from the MNIST training set (constituting our \(_{r}\) distribution) and add Gaussian noise with variance \(^{2}=0.05\) (constituting our \(_{n}\) distribution). The goal is then to reconstruct test samples from the MNIST dataset corrupted with Gaussian noise of the same variance seen during training. The regularizers were parameterized via a deep convolutional neural network with positively homogenous Leaky ReLU activation functions, giving rise to a star-shaped regularizer. They were trained using the adversarial loss and Hellinger-based loss (5). We also used the gradient penalty term from  for both losses. Once each regularizer \(_{}\) has been trained, we then denoise a new noisy MNIST digit \(y\) by minimizing \(x\|x-y\|_{_{2}}^{2}+_{}(x)\) via gradient descent.

As a baseline, we compare both methods to Total Variation (TV) regularization . We note that in , it was recommended to set \(:=2\) where \(:=_{(0,^{2}I)}[\|z\|_{_{2}}]\), which was motivated by the fact that the regularizer will be \(1\)-Lipschitz. We found our Hellinger-based regularizer performed best with \(:=5.1^{2}\). We thus also compare to the adversarial regularizer with a further tuned parameter \(\). Please see Section C.1 of the appendix for implementation details and a further discussion of regularization parameter choices.

Figure 4: (Left) The star bodies \(L_{r}\) and \(_{n}\) induced by the distributions \(_{r}\) and \(_{n}\) from from Theorem 3.1 and Example 1 with \(=0.5\). Then we have (Middle) \(K_{+,_{*}}\) and (Right) \(K_{-,_{*}}\) as defined in Theorem 3.1. Note that \(K_{+,_{*}}\) better captures the geometry of a regularizer that assigns higher likelihood to likely data and lower likelihood to unlikely data, while \(K_{-,_{*}}\) does not.

We show the average Peak Signal-to-Noise Ratio (PSNR) and mean squared error (MSE) over \(100\) test images in the table below. We see that the Hellinger-based loss gives competitive performance relative to the adversarial regularizer and TV regularization, while the tuned adversarial regularizer slightly outperforms all methods. These promising results suggests that these new \(\)-divergence based losses are potentially worth exploring from a practical perspective as well.

## 4 Computational considerations

We now discuss various issues relating to employing such star body regularizers. In particular, we discuss optimization-based concerns due to the (potential) nonconvexity of \(x\|x\|_{K}\) and how one can use neural networks to learn star body regularizers in high-dimensions.

Weak convexity:When such a regularizer is employed in inverse problems, one may require minimizing a cost of the form \(x\|y-(x)\|_{_{2}}^{2}+(\|x\|_{K})\). Here \(()\) is a continuous monotonically increasing function (e.g., \((t)=t^{q}\) for \(q>0\)). While the first term is convex for linear \(()\), the second term can be highly nonconvex for general star bodies. One could enforce searching for a convex body as opposed to a star body, but convexity is strictly more limited in terms of expressibility. There is a large body of recent work [24; 25; 19; 36; 80] that has analyzed weak convexity in optimization, showing that while nonconvex, weakly convex functions can be provably optimized using simple first-order methods in some cases. Based on this, it is important to understand under what conditions is \((\|\|_{K})\) a weakly convex function. Recall that a function \(f\) is \(\)-weakly convex if \(f()+\|\|_{_{2}}^{2}\) is convex. For certain functions \(()\), there is a natural way to determine if \((\|\|_{K})\) is weakly convex.

In particular, in considering the quantity \(\|\|_{K}^{2}+\|\|_{_{2}}^{2}\), there is a natural way in which one can describe this functional as the gauge of a new star body.

**Definition 4.1** ().: For two star bodies \(K,L^{d}\) and scalars \(, 0\) (both not zero), the harmonic \(q\)-combination for \(q 1\) is the star body \( K_{q} L\) whose radial function is defined by

\[_{ K_{q} L}(u)^{-q}:=_{K}(u)^ {-q}+_{L}(u)^{-q}.\]

Observe that the scaling operation \( K\) differs from the usual \( K:=\{ x:x K\}\) since \(_{K}^{-q}=_{^{-q}K}^{-q}\) so that \( K=^{-q}K\).

Notice that in our context, we have a similar combination given by the harmonic \(2\)-combination of \(K\) and the unit ball \(B^{d}\): \(M_{2,}:=M_{2,}(K):=K_{2}B^{d}\). By the definition of the harmonic \(2\)-combination, the gauge of \(M_{2,}\) satisfies \(\|x\|_{M_{2,}}^{2}=\|x\|_{K}^{2}+\|x\|_{_{2}}^{2}\). We thus have the following Proposition, proven in Section B.4, showing the equivalence between the weak convexity of \(\|\|_{K}^{2}\) and the convexity of \(M_{2,}\). We also show a visual example in Section D.4 of a data-dependent star body whose gauge squared is weakly convex.

**Proposition 4.2**.: _Let \(K^{d}\). Then \(x\|x\|_{K}^{2}\) is \(\)-weakly convex if and only if \(M_{2,}\) is convex._

Deep neural network-based parameterizations:Since deep neural networks allow for efficient learning in high-dimensions, it is natural to ask under what conditions does the regularizer \(_{}(x):=f_{L}^{_{L}} f_{L-1}^{_{L-1}}  f_{1}^{_{1}}(x)\) define a star body regularizer, i.e., when is the set \(K_{}:=\{x^{d}:_{}(x) 1\}\) a star body? As star bodies are in one-to-one correspondence with radial functions, one can answer this question by studying conditions under which the map \(u 1/_{}(u)\) defines a radial function. This leads to the following Proposition, which is proven in Section B.4.

**Proposition 4.3**.: _For \(L\), consider a regularizer \(:^{d}\) of the form \((x):= f_{L} f_{1}(x)\) where \(:^{d_{L}}\) and each \(f_{i}:^{d_{i}-1}^{d_{i}}\) for \(i[L]\) satisfy the following conditions:_1. \(d=d_{0} d_{1} d_{2} d_{L}\)_,_
2. \(()\) _is non-negative, positively homogenous, continuous, and only vanishes at the origin,_
3. \(f_{i}()\) _is injective, continuous, and positively homogenous._

_Then the set \(K:=\{x^{d}:(x) 1\}\) is a star body in \(^{d}\) with radial function \(_{K}(u):=1/(u)\)._

Below we discuss the different types of commonly employed layers and activation functions that satisfy the conditions of the above Proposition.

**Example 3** (Feed-forward layers).: _Any intermediate layer \(f_{i}:^{d_{i-1}}^{d_{i}}\) can be parameterized by a single-layer feedforward MLP with no biases: \(f_{i}(x):=_{i}(W_{i}x)\) where \(W_{i}^{d_{i}}^{d_{i-1}}\) has rank \(d_{i-1}\). For the activation function \(_{i}()\), one can ensure injectivity and positive homogeneity by choosing the Leaky ReLU activation \(_{}(t):=(t)+(-t)\) where \((0,1)\). Observe that this map is positively homogenous, continuous, and bijective. The layer \(f_{i}\) thus satisfies condition (iii). For the final layer \(()\), note that any norm on \(^{d}\) satisfies condition (ii)._

**Example 4** (Residual layers).: _If \(d_{i}=d_{i-1}\), one could use residual layers of the form \(f_{i}(x):=x+g_{i}(x)\) where \(g_{i}:^{d_{i}}^{d_{i}}\) is a Lipschitz continuous, positively homogenous function with \((g_{i})<1\). This can be achieved by having \(g_{i}\) be a neural network with \(1\)-Lipschitz positively homogenous activations (such as \(\)), no biases, and weight matrices with norm strictly less than \(1\). Then each \(f_{i}\) is positively homogenous, continuous, and invertible , satisfying condition (iii)._

_Remark 4.4_ (Star-shaped regularizers).: Note that if one does not require the set \(K:=\{x^{d}:(x) 1\}\) to be a star body, but simply star-shaped, then there is more flexibility in the neural network architecture used. In particular, one simply needs the layers to be positively homogenous and continuous, and the output layer be non-negative, positively homogenous, and continuous. Invertibility would not be necessary, so that the dimensions \(d_{i}\) need not be increasing. This can easily be achieved via linear convolutional layers and positively homogenous activation functions. In fact, in the original work , experiments were done with a network precisely of this form.

_Remark 4.5_ (Positive homogeneity of activations).: While our theory for neural networks mainly applies to positively homogenous activation functions such as ReLU and LeakyReLU, we show in Section C.2 of the appendix that focusing on such activations does not limit performance. Specifically, we compare networks with non-positively homogenous activation functions (such as Tanh or the Gaussian Error Linear Unit (GELU)) with a network exclusively using Leaky ReLU activations in denoising. We show that the Leaky ReLU-based regularizer outperforms regularizers with non-positively homogenous activations. This potentially highlights empirical benefits of positive homogeneity, which warrants further analysis.

## 5 Discussion

In this work, we studied optimal task-dependent regularization over the class of regularizers given by star body gauges. Our analysis focused on learning approaches that optimize a critic-based loss function derived from variational representations of probability measures. Utilizing tools from dual Brunn-Minkowski theory and star geometry, we precisely characterized the optimal regularizer in specific cases and provided visual examples to illustrate our findings. Overall, our work underscores the utility of star geometry in enhancing our understanding of data-driven regularization.

Limitations and future work:There are numerous exciting directions for future research. While our analysis concentrated on formulations based on the Wasserstein distance and \(\)-divergence, it would be valuable to extend the analysis to other critic-based losses for learning regularizers. Additionally, it would be interesting to give a precise characterization of global minimizers for the Hellinger-inspired loss (5). Relaxing the assumptions in our results is another significant challenge. For instance, Theorem 2.4 hinges on a "containment" property of the distributions, and finding ways to relax this assumption could broaden the applicability of our theory. Moreover, many of our results assume a certain regularity in the densities of our distributions. Addressing this would involve extending dual mixed volume inequalities to operate on star-shaped sets rather than star bodies. Finally, it is crucial to assess the performance of these regularizers in solving inverse problems. This includes studying questions regarding sample complexity, algorithmic properties, robustness to noise, variations in the forward model, and scenarios with limited clean data [22; 45; 32; 51; 85; 86; 21; 31].