# Probabilistic Decomposed Linear Dynamical Systems

for Robust Discovery of Latent Neural Dynamics

 Yenho Chen\({}^{1,3}\), Noga Mudrik\({}^{4}\), Kyle A. Johnsen\({}^{3}\),

**Sankaraleengam Alagapan\({}^{2}\), Adam S. Charles\({}^{4}\), and Christopher J. Rozell\({}^{1,2}\)**

\({}^{1}\)Machine Learning Center, Georgia Institute of Technology

\({}^{2}\)School of Electrical and Computer Engineering, Georgia Institute of Technology

\({}^{3}\)Coulter Dept. of Biomedical Engineering, Emory University and Georgia Institute of Technology

\({}^{4}\)Department of Biomedical Engineering, Mathematical Institute for Data Science,

Center for Imaging Science, Kavli Neuroscience Discovery Institute, Johns Hopkins University

yenho@gatech.edu, nmudrik1@jhu.edu, kjohnsen@gatech.edu

sankar.alagapan@gatech.edu, adamsc@jhu.edu, crozell@gatech.edu

###### Abstract

Time-varying linear state-space models are powerful tools for obtaining mathematically interpretable representations of neural signals. For example, switching and decomposed models describe complex systems using latent variables that evolve according to simple locally linear dynamics. However, existing methods for latent variable estimation are not robust to dynamical noise and system nonlinearity due to noise-sensitive inference procedures and limited model formulations. This can lead to inconsistent results on signals with similar dynamics, limiting the model's ability to provide scientific insight. In this work, we address these limitations and propose a probabilistic approach to latent variable estimation in decomposed models that improves robustness against dynamical noise. Additionally, we introduce an extended latent dynamics model to improve robustness against system nonlinearities. We evaluate our approach on several synthetic dynamical systems, including an empirically-derived brain-computer interface experiment, and demonstrate more accurate latent variable inference in nonlinear systems with diverse noise conditions. Furthermore, we apply our method to a real-world clinical neurophysiology dataset, illustrating the ability to identify interpretable and coherent structure where previous models cannot. 1

https://github.com/siplab-gt/probabilistic-decomposed-linear-dynamical-systems

## 1 Introduction

A central goal in computational neuroscience is to develop models capable of discovering latent structure within noisy, high-dimensional neural signals. By identifying hidden relationships within neural recordings, we can begin to understand, predict, and control the behaviors of the underlying systems. Modeling neural time-series is challenging due to the range of temporal dynamics present. For example, there may be gradual short-term fluctuations, abrupt shifts in response to external stimuli, and long-term global drifts resulting from changes in baseline activity levels .

Although black-box approaches based on deep learning are available , their complexity often obscures the relationships learned from the data, making it difficult to extract scientific insights from these models. As a result, practitioners may favor time-varying linear state-space models which offer mathematically interpretable representations by approximating complex dynamics with simplelocally linear regimes . However, obtaining latent variable estimates that are robust to dynamical noise and system nonlinearity in these state-space models is challenging. When applied to neural time-series, latent variable estimation may become unstable due to inflexible model formulations or noise-sensitive inference procedures. This can incorrectly produce disparate results for signals generated from the same underlying system.

For example, switching linear dynamical systems (SLDS) and related models [42; 44] segment time-series into discrete linear dynamical states, providing a piecewise linear approximation of the underlying system while highlighting coherent groups of activity. However, the assumption of discrete components can be a poor modeling choice for neural signals that contain continuous-valued fluctuations, such as gradual or random changes to the system speed as seen during neural ramping activity  or Levy walk dynamics in the cerebral cortex . As demonstrated in  and our experiments in Section 4.3, when applied to real-world datasets, inference of the switching variables can result in rapid, random oscillations between the discrete modes, indicating that the model is unable to identify meaningful structure in the data.

To address the limitations of discrete states, the decomposed linear dynamical systems (dLDS) model  learns a dictionary set of linear dynamical regimes, referred to as dynamic operators (DOs), that can be modified and combined through a linear combination of sparse coefficients. By allowing coefficients to be time-varying and continuous-valued, dLDS naturally captures both gradual changes by adjusting the coefficient magnitudes and abrupt shifts by changing the set of active DOs over time. Inference is accomplished by optimizing over a cost function that encourages data reconstruction while also constraining the structure of the dynamic coefficients to be sparse and temporally smooth.

Unfortunately, there are two critical shortcomings that prevent the robust inference of the latent variables in dLDS. First, the cost-based inference procedure is sensitive to noise, because of the regularization term encouraging temporal smoothness. This term sequentially propagates errors from noisy coefficient estimates over the length of the time series. Consequently, the model may produce inconsistent coefficient estimates on similar signals and have poor multi-step inference performance, indicating that the learned dynamics are unable to generalize well beyond a single time step. Second, the original latent dynamics model lacks a method for accurately representing systems with multiple fixed points, causing DO coefficients to oscillate or switch arbitrarily in a way that may not align with the underlying process. To learn an effective decomposed model in practice, we require a strategy that provides robust estimates of latent variables despite the presence of noise and system nonlinearity.

In this work, we address these limitations by introducing the probabilistic decomposed linear dynamical systems (p-dLDS) model. Our approach improves robustness of latent variable estimation while maintaining the richness of a decomposed dynamical systems model. First, we propose a probabilistic inference procedure that reduces the model's sensitivity to temporal noise by accounting for uncertainty in the latent variable estimates over time. Namely, we introduce time-informed hierarchical variables that encourage both sparse and smooth model coefficients. We devise a variational expectation maximization (vEM) procedure to perform inference and learning over this probabilistic structure. Second, we incorporate a time-varying offset term to model systems that orbit multiple fixed points. While we analytically identify model degeneracies with this formulation, we propose an additive decomposition strategy that prevents convergence to trivial solutions.

Through several synthetic examples, we demonstrate how these contributions lead to improved accuracy and robustness of latent variable estimation despite difficult noise conditions. We extend these results to an empirically-derived brain-computer interface experiment , showcasing robustness to highly nonlinear observation functions and the ability to extract meaningful insights from the learned latent variables. Finally, we illustrate how our method effectively identifies interpretable and coherent structure in a clinical neurophysiology dataset where previous models are unsuccessful.

## 2 Background and Related Work

**State Space Models.** Our goal is to accurately describe the evolution of high-dimensional time-series data \(_{t}^{M}\) with the following state-space equations,

\[_{t}=_{t}++_{y_{t}}, _{y_{t}}(0,_{y}),()\] (1) \[_{t}=f_{t}(_{t-1})+_{x_{t}},_{x_{t}}(0,_{x}),()\]where \(_{t}^{N}\) is the latent state, \(f_{t}()\) is the dynamics function, and \(^{M N}\) and \(^{M}\) describe a linear observation function. Our work focuses on the case when \(N<M\), which compresses high-dimensional signals into a low-dimensional latent space. By choosing \(f_{t}\) to be a time-varying linear operator, we can approximate complex nonlinear dynamics with simple locally-linear components, balancing expressivity with mathematical interpretability. However, learning a time-varying linear operator from data can be challenging, and typically requires additional constraints on the underlying generative model to identify meaningful representations.

**Switching Linear Dynamical System (SLDS).** SLDS approximates nonlinear systems by introducing a discrete switching variable \(z_{t}=\{1,,K\}\) into the time-varying linear dynamics equation,

\[_{t+1}=_{t}+_{z_{t}}_{t}+_{z_{t}}+_{ x_{t}}.\]

At each time step \(t\), the latent state \(_{t}\) evolves according to the \(z_{t}\)-th linear regime defined by \(_{z_{t}}^{N N}\) and \(_{z_{t}}^{N}\) while the switching variables evolve according to a Markov matrix. Inference is performed through a vEM algorithm, where the approximate posterior of the latent variables is estimated through coordinate ascent updates over tractable subgraphs. There are many extensions of SLDS, such as rSLDS  which modifies its generative behavior by informing the transitions of \(z_{t}\) with \(x_{t-1}\). However, switching models are inherently limited when describing complex signals due to their discrete formulation. For instance, a switched representation is unable to learn that a dynamic regime may exhibit a range of variations. In neural systems, these variations may arise from random spiking processes [37; 40] or systems with randomly distributed speeds [41; 12]. In SLDS, each variation is learned as a separate discrete state, thus obscuring that the learned states are related. Furthermore, the switching formulation cannot adapt the learned system to unseen variations (i.e. different levels of random speeds). This can produce unstable inference behavior, where the switching state oscillates unpredictably or collapses to a single uninformative state.

**Decomposed Linear Dynamical Systems (dLDS).** dLDS  relaxes the discrete formulation by approximating nonlinear and nonstationary signals with a time-varying mixture of linear dynamical systems (LDS) defined by the following equations and constraints,

\[_{t+1}=_{t}+_{t}_{t}+_{x_{t}},_{t}=_{k=1}^{K}_{k}c_{t,k},_{t}$ is sparse}.\] (2)

Every transition \(_{t}\) is decomposed as a linear combination of sparse coefficients \(_{t}^{K}\) and a dictionary of \(K\) DOs \(_{k}^{N N}\). Figure 1A shows the corresponding graphical model. Inference of the latent variables is accomplished by solving the Basis Pursuit Denoising with Dynamic Filtering (BPDN-DF)  objective sequentially for all \(t\) and \(_{0},_{1},_{2}>0\),

\[}_{t},}_{t}=*{arg\,min}_{_{ t},_{t}}_{t}-_{t}_{2}^{2}+_{0} _{t}-}_{t-1}-_{t}}_{t-1} _{2}^{2}+_{1}_{t}_{1}+_{2} _{t}-}_{t-1}_{2}^{2}.\]

This produces a point estimate of \(_{t}\) and \(_{t}\) that matches the likelihood function resulting from Equations (1) and (2). In this objective, the dynamic coefficients are encouraged to be sparse through the \(_{1}\) penalty and temporally smooth through the \(_{2}\) penalty centered around the previous coefficient estimate. However, this approach is sensitive to noise because inference relies on propagating noisy point estimates of \(}_{t-1}\) over time. As a result, BPDN-DF may accumulate errors that can lead to significantly different coefficient estimates on signals sampled from the same generative process. Furthermore, the lack of robustness to noise can degrade multi-step inference performance, causing the inferred system to quickly diverge from the true system. This suggests that the inferred latent variables only capture the local activity narrowly and are unable to accurately represent the dynamics beyond a single time-step. Another drawback of dLDS arises from the dynamics model in equation (2) which implicitly assumes that the observed dynamics contain a single fixed point that revolves around the origin. This limits dLDS's ability to model systems that cannot be easily mean-centered such as those with multiple fixed points or nonstationary drifts.

**Sparse Bayesian Learning with Dynamic Filtering.** Sparsity is achieved in probabilistic models through hierarchical scale-mixture priors [10; 2; 4]. To integrate dynamical information into probabilistic sparse signal inference, previous work  proposes the Sparse Bayesian Learning with Dynamic Filtering (SBL-DF) framework where the following hierarchical model is defined,

\[p(_{t},_{t},_{t})=p(_{t}|_{t}) _{k=1}^{K}p(c_{t,k}|_{t,k})p(_{t,k}|a_{t,k},b_{t,k}).\] (3)Here, \(p(_{t}|_{t})=(_{t},_{t})\) specifies the likelihood, where \(\) is a measurement matrix. Sparsity is encouraged through the zero-mean Gaussian priors \(p(c_{t,k}|_{t,k})=(0,_{t,k})\) independently placed on each element of the sparse vector. The variance parameters \(_{t,k}\) are defined by an inverse gamma hyperprior \(p(_{t,k}|_{t,k},_{t,k})=_{_{t,k} }(_{t,k},_{t,k})\) with shape parameters \(_{t,k}\) and \(_{t,k}\). When marginalizing over \(_{t,k}\)'s, we see that \(p(c_{t,k}|_{t,k},_{t,k})\) becomes a t-distribution known for its high kurtosis, which is essential for producing sparse solutions. To propogate dynamics information, the past estimate \(}_{t-1}\) informs the hyperprior parameters of the next estimate such that \(b_{t,k}/a_{t,k}=c_{t-1,k}^{2}\).

## 3 Probabilistic Decomposed Linear Dynamical Systems

We build upon dLDS and propose a probabilistic decomposed linear dynamical systems (p-dLDS) model. Rather than propagating noisy point estimates of the latent variables during inference, we improve robustness by marginalizing over uncertainty with respect to time. Additionally, we propose a tractable method for extending the dynamics model to systems with multiple fixed points.

### Time-varying offset term

Note that for any parameter setting, the dLDS local dynamics (eq. (2)) reduces to a linear dynamical system (LDS) that is characterized by a single fixed point centered around the origin. Yet, real-world dynamical systems often consist of much more complicated behaviors. Nonlinearities can cause a signal to navigate through multiple fixed points throughout its trajectory, while nonstationarities may change the behavior of the system entirely with new fixed points emerging or disappearing. Simple preprocessing measures, such as mean-centering the data, are inadequate to account for these behaviors. To enable robustness against these behaviors, we introduce a time-varying offset term \(_{t}^{N}\) into Equation (2) as a flexible way to account for dynamics not readily captured by the original dLDS latent dynamics model.

**Lemma 1**.: _Let the transition between any two state vectors \(_{t},_{t+1}^{N}\) be defined by the linear dynamics matrix \(_{t}^{N N}\) and the dynamics offset \(_{t}^{N}\). For any \(>0\), the objective,_

\[*{arg\,min}_{_{t},_{t}}\|_{t+1}-_{t}- {F}_{t}_{t}-_{t}\|_{2}^{2}+\|_{t}\|_{2}^{2},\]

_is minimized when \(_{t}=\) and \(_{t}=_{t+1}-_{t}\)._

This result, proven in Appendix B.1, reveals that introducing a time-varying offset term makes inference of the dynamics a degenerate problem. While the solution in Lemma 1 minimizes the objective, it fails to capture any meaningful structure in \(_{t}\) as the result of \(_{t}\) being unconstrained. To prevent the convergence to these trivial solutions, we decompose the latent state space as,

\[_{t}=_{t}+_{t},\] (4)

where \(_{t}\) captures fast dynamics and \(_{t}\) captures slow-varying trend behavior. These latent variables follow the dynamic equations \(_{t+1}=_{t}+_{t}_{t}+_{t_{t+1}}\) and \(_{t+1}=_{t}+_{b_{t+1}}\) where \(_{l},_{b}^{N}\) represent noise sampled from \(_{b}(0,_{b})\) and \(_{l}(0,_{l})\) respectively.

Figure 1: **(A)** Graphical model of dLDS. **(B)** p-dLDS includes hierarchical variables for probabilistic sparse inference and reparameterizes the latent space to include a time-varying offset term.

### Probabilistic Time-Informed Sparsity

In decomposed models, we aim to achieve two goals simultaneously: sparsity and smoothness of coefficients over time. Motivated by this, we incorporate dynamics-informed probabilistic structure. First, we assume that each coefficient evolves independently of the others. Second, we introduce a hierarchical variance parameter \(_{t,k}\) that controls the sparsity for each \(c_{t,k}\). Moreover, we introduce dynamics information during sparse inference by encouraging a similar active support set in consecutive time slices through the variance hyperpriors. Put together, the resulting coefficient transition density in p-dLDS becomes,

\[p(_{t},_{t}|_{t-1}) p(_{t}|_{t-1}, _{t})p(_{t}|_{t-1})=_{k=1}^{K}p(c_{t,k}|c_{t- 1,k}_{t,k})p(_{t,k}|c_{t-1,k}).\] (5)

We define the first term on the right-hand side with the following functional form,

\[p(c_{t,k}|c_{t-1,k},_{t})(-^{2}}{2_{t,k}}--c_{t-1,k})^{2}}{2_{t-1,k}^{2}})(c_{t-1,k},_{t-1,k}^{2})(0,_{t,k}).\] (6)

This density captures the constraints of sparsity and smoothness for the inferred coefficients \(c_{t,k}\). When the variance around zero \(_{t,k}\) is small, this structure promotes sparsity by shrinking coefficient values towards zero. Conversely, when the variance around the previous time step \(_{t-1,k}^{2}\) is small, it encourages smooothness by shrinking coefficients towards the previous value. While the idea of combining two shrinkage effects in a single density has been explored in previous works [15; 5; 24; 18], those approaches generally require manual balancing of the two penalties. In contrast, we devise a procedure in the following section that estimates these variance parameters automatically during inference and learning.

The second density on the right-hand side from equation (5) is defined similarly to the hyperprior in SBL-DF. (i.e., \(p(_{t,k}|c_{t-1,k})=(, c_{t-1,k}^{2})\) where \(\) weighs the influence of the dynamics when estimating \(_{t,k}\)). The resulting graphical model is shown in Figure 1B. We note that since the value of the previous coefficient is squared, the overall prior placed on the inverse gamma density follows a \(^{2}\) distribution.

### Inference and Learning

The joint distribution of p-dLDS is given by,

\[ p(,,,|)=p(_{1})&[_{t=1}^{T}p(_{t}|_{t})]\\ &[_{t=1}^{T-1}p(_{t+1}|_{t},_{t}) [_{k=1}^{K}p(c_{t+1,k}|c_{t,k},_{t+1,k})p(_{t+1,k}|c_{t, k})]],\] (7)

where we denote \(=_{1:T}\) for brevity. Exact posterior inference is intractable due to the nonconjugacy introduced by incorporating time-informed sparsity-inducing structure into the graphical model. As a result, we devise a variational expectation maximization (vEM) procedure where the approximate posterior is factorized as

\[p(,,|,) q()q(,).\]

Here, the parameters are given by \(\{_{1:K},,,_{y},_{x},_{c}\}\). Our approach contrasts with BPDN-DF, which estimates latent variables through separate \(_{1}\) problems at each point in time. Instead, we preserve the time-dependence structure within each class of latent variables and leverage efficient inference algorithms that marginalize over uncertainty with respect to time. In general, we seek to maximize the variational lower bound,

\[_{q}()=_{q()q(,)}[ p(,,,|)- q()q(,)],\]

with coordinate ascent updates on the latent state posterior, the dynamics coefficients posterior, and the model parameters.

#### Updating Latent State Posterior.

The optimal coordinate ascent variational update is given by,

\[q()(_{q(,)}[ p(, ,,|)]).\] (8)

Our assumed decomposition in equation (4) allows us to define the latent state transition density as \(p(_{t+1}|_{t},_{t})=p(_{t+1}=_{t+1}+_{t+1})= (_{t+1};_{t}+_{t}_{t}+_{t})\). Substituting this into equations (7) and (8), we get that the optimal coordinate ascent approximate posterior becomes,

\[q()=(_{1};_{1},_{1})[ _{t=2}^{T}(_{t};_{t}+,_{y}) ][_{t=1}^{T}(_{t};_{t-1}+_{t-1} _{t-1}+_{t},_{x})],\] (9)

where, \(_{1}\) and \(_{1}\) are the mean and covariance of the initial state.

**Lemma 2**.: _Let \(,^{N}\) be independent random variables such that \( p()\) and \( p()\). Their sum \(=+\) is distributed according to \((_{l}+_{b},_{l}+_{b})\) when 1) \(p()=(_{b},_{b})\) and \(p()=(_{l},_{l})\) and when 2) \(p()=(-_{b})\) and \(p()=(_{l},_{l}+_{b})\)._

We leverage Lemma 2 to reparameterize the state space trajectories into a deterministic and a stochastic component. The deterministic component captures the slow-moving offset density \(q()=_{t=1}^{T}(_{t}-}_{t})\), where \(}_{t}\) is estimated using a moving average of window size \(S\) which can be efficiently parallelized. The remaining dynamics are captured in the stochastic component. We define the family of variational distributions to be the class of linear Gaussian state space models, such that \(q()=(_{1},_{1})_{t=2}^{T}( _{t-1}+_{t-1}_{t-1},_{x})\). Conditioned on estimates \(}\) and samples of \(\), the optimal coordinate ascent variational update for \(q()\) is efficiently computed using the Kalman Smoother . We provide a full derivation of the update rule and Lemma 2 in Appendix B.2.

#### Updating Dynamics Coefficient Posterior.

Sparse probabilistic representations introduce non-Gaussian factors which prevent closed-form message passing inference. Specifically, nonconjugacy arises from the inverse gamma term \(p(_{t+1,k}|c_{t,k})\) since it is parameterized by \(c_{t,k}^{2}^{2}\). Moreover, the posterior distribution over the coefficients is highly multi-modal as a result of the implicit t-distribution in our hierarchical model. To update the coefficient posteriors, we propose a three-step procedure, where we factorize \(q(,)=q()q()\). First, we obtain an initial estimate of the variational distributions using SBL-DF. Second, we update \(q(c_{t,k})=(c_{t,k}^{*},_{t,k})\) using stochastic gradient descent (SGD) over

\[^{*}=*{arg\,max}_{}_{t=1}^{T-1} p(}_{t+1}|}_{t},_{t})+ p(_{t+1}|_{t}, _{t+1})+ p(_{t+1}|_{t}),\] (10)

where we have estimated the expectation in the optimal coordinate ascent update rule using samples from \(} q()\) and \(} q()\). To retain coefficient sparsity, we only update coefficients within the active support set. In our work, this is defined as coefficients that have an initial estimate of \(|c_{t,k}|>\) where \(=10^{-4}\). Finally, we update \(q()\) based on closed form conjugacy rules.

#### Update Parameters.

Given our updated posteriors of the latent variables, we proceed to update the model parameters based on the ELBO,

\[^{*}=*{arg\,max}_{}_{q()q()q( {})}[p(,,,|)- q()q( )q()]*{arg\,max}_{}p(, },},}|),\] (11)

where we estimate the expectation with samples from our variational distributions and drop terms not dependent on \(\). We use SGD to update all model parameters, which is possible when we assume that the covariance matrices have diagonal structure.

## 4 Results

We demonstrate p-dLDS in a variety of synthetic examples, highlighting improved robustness to noise and system nonlinearity. Additionally, we apply our model to a clinical neurophysiology dataset, revealing interpretable patterns where previous methods fail. We compare our method against SLDS, rSLDS, and dLDS as described in Section 2. All datasets are split 50:50 for training and testing. Due to space constraints, we provide full descriptions of the simulation setup in Appendix C and metric definitions in Appendix D.

### Synthetic Dynamical Systems

**NASCAR with Random Speeds.** We evaluate our inference procedure on the NASCAR dataset [26; 32; 23]. Since this system is easily mean-centered, we can isolate the effect of our proposed inference procedure as offset terms are not necessary. To make this dataset more realistic, we introduce speed variability into the ground truth dynamics as opposed to having a perfect constant speed at all time. Specifically, whenever a trajectory enters a new segment of the track, the system experiences a random change in speed. We trained all models on 30 trials, each consisting of 1000 time steps (Fig. 2A) with randomly sampled initial points, and a randomly constructed 10-dimensional linear observation matrix (see Appendix C.1 for full details). Performance is evaluated on 30 held-out trials where the ground truth switching states are defined by the different segments of the track. In all models, we set the \(M=10\), \(N=2\) and \(K=4\) for DOs or switching states. In our experiments, we define the "discrete states" for decomposed models as the DO state with the largest coefficient magnitude.

Figure 2B shows that changes in the system speed mask the true transition behavior between segments of the track in rSLDS. Moreover, dLDS identifies coherent segments, but inappropriately learns a different switching pattern for outer and inner edges of the track. In contrast, p-dLDS identifies a switching pattern most consistent with the true track segments despite the presence of noise and randomness in the system's speed. We note that while there are four true segments, decomposed models form a more parsimonious representation by identifying similar behaviors in different track segments such as in both edges and curves.

    &  &  \\  Model &  Dynamics MSE (\(\)) \\ (\( 10^{-3}\) ) \\  &  Switch MSE (\(\)) \\ (\( 10^{-3}\) ) \\  &  100-step \\ \(R^{2}\) (\(\)) \\  &  Dynamics \\ MSE (\(\)) \\  & 
 Switch \\ \(R^{2}\) (\(\)) \\  \\  SLDS & 0.0995 & 12.89 & 0.184 & 0.431 & 0.0204 & -3.47 \\ rSLDS & 0.1065 & 13.17 & 0.238 & 0.304 & 0.0208 & -11.54 \\ dLDS & 123.19 & 13.28 & ✗ & 1.123 & 0.1529 & ✗ \\ p-dLDS (ours) & **0.033** & **7.34** & **0.450** & **0.141** & **0.0137** & **0.418** \\   

Table 1: Metrics for synthetic dynamical systems. Bold means best performance. (\(\)) indicates higher score is better while \(()\) indicates that lower is better. ✗ indicates that value diverged towards \(-\). Switch events for decomposed models are defined as times where the active set of DOs change from the previous time step.

Figure 2: **Probabilistic model and offset term reduce estimation errors.****(A)** Example trial from the NASCAR experiment colored by the true switching labels (not provided during training). Each track segment has a random speed \(\). **(B)** Inferred state space, colored by discrete state or dominant coefficients. p-dLDS identifies correct track segments. **(C)** Example trial from the Lorenz experiment. The speed ramps according to the time intervals \(\) in an ODE solver. **(D)** Inferred state space, colored by the dominant coefficients. The time-varying offset term allows p-dLDS coefficients to switch according to the true speed and accurately model the two fixed points in the opposing lobes.

Table 1 summarizes our quantitative evaluations on three metrics: 1) the mean squared error (MSE) between the learned and ground truth latent dynamics, 2) the MSE between the inferred and true switch rate to determine agreement of the discrete switching behavior, and 3) the 100-step inference \(R^{2}\) to demonstrate that the learned system generalizes beyond a single step on held-out data. (See Appendix D for mathematical definitions). We see that p-dLDS broadly outperforms existing methods in all metrics and significantly improves inference for decomposed models.

**Lorenz System with Random Ramping.** Next, we consider the Lorenz system, a chaotic nonlinear system with multiple fixed points, exploring the effect of the offset term. The system is described by the differential equation, \(}=[(x_{2}-x_{1}),x_{1}(-x_{3})-x_{2},x_{1}x_{2}-  x_{3}]^{}\) where the parameters \(=28\), \(=8/3\), and \(=10\) define a chaotic attractor with two opposing lobes (Fig. 2D). We introduce continuous fluctuations in the underlying dynamics by randomly ramping the system's speed throughout each trajectory. This is accomplished by adjusting the evaluation time intervals given to an ODE solver. Similar to before, we randomly construct a linear observation function with \(M=10\) and train models on 30 randomly constructed trials with 1000 time points each (see Appendix C.2). Furthermore, we define the ground truth switch events as the time points when the signal transitions between the two lobes in addition to the moments when a ramping period concludes. All models are trained with a latent space of \(N=3\) and \(K=4\) states or DOs.

In figure 2D, we see that rSLDS does not distinguish between the different speeds along the outer and inner sections of the attractor. Instead, the discrete states obscure the continuum of speeds by incorrectly grouping all activity in each lobe into a single regime. Furthermore, we observe that dLDS is limited without an offset term, unable to accurately represent multiple fixed points. Instead of aligning with the two attractor lobes, transitions in the dominant coefficients occur radially relative to the origin and fail to reconstruct the two orbiting fixed points. Conversely, p-dLDS's offset term enables learning a system where coefficients better match the true geometry. This representation correctly recovers differences between the outer and inner sections of the attractor while also accurately reconstructing the two orbiting fixed points. Moreover, this leads to improved estimation of latent dynamics, a switching rate that agrees with the true system, and improved multistep inference performance as shown in Table 1.

### Simulated Motor Cortex Data in a Reaching Task

We now turn to an empirically-derived synthetic experiment related to brain-computer interfaces, where the dynamics and observation functions are nonlinear and derived from analysis of neural data. Our focus is on the reaching task, a neuroscience experiment designed to study motor control in non-human primates [19; 22]. In this experiment, the subjects are trained to reach towards visually cued targets, while neural activity is recorded from motor-related areas such as electromyography (EMG) data from arm muscles. Each trial consists of two distinct phases: preparation and movement. In the preparation phase, the subject plans its movement while keeping their arm still. In the movement phase, the subject physically reaches towards the target. The goal of this experiment is to decode reach intention from neural data. We construct a dataset by first simulating a spiking neural network with known latent factors  trained to reproduce empirical EMG signals from the center-out reach

Figure 3: **p-dLDS efficiently captures changes in dynamics.****(A)** Latent factors are computed from empirical EMG of the reaching experiment in . Dynamics are characterized by a preparatory and movement phase. **(B)** Synthetic spikes and LFPs are generated using the wslfp package [17; 16] **(C)** The trial-averaged coefficients for p-dLDS smoothly vary with reaching angle. DO 1 captures preparatory dynamics while DO 3 captures movement dynamics. **(D)** Confusion matrix for linear classification of reach directions. p-dLDS predictions closely align to true diagonal.

task in . Spikes are then converted to 50-channel local field potentials (LFP) recordings via a weighted, delayed sum of synaptic currents (see Figure 3A and B) . Our dataset contains 150 6-second trials sampled at 250 hz, where each trial represents one out of eight reach directions visually cued at a random start time. PCA identifies that three components captures 98% of the variance. Thus, we set \(M=50\), \(N=3\), and \(K=4\) DOs or discrete states.

Figure 3C shows the trial-averaged DO coefficients from p-dLDS, which change smoothly and cyclically according to the true reach angle. Additionally, the DOs appear to differentiate between the two distinct dynamical regimes, where the activity of \(_{1}\) and \(_{3}\) localize to the preparatory and movement phase respectively. Quantitatively, we compute the linear classification accuracy of the reach angles using the average state activity over time as features (see Appendix D.5). Figure 3D shows that classifiers built from SLDS features fail to capture the full continuum of reach angles. This limitation occurs because the discrete switching states are unable to efficiently capture the diversity of activity present in the LFPs, which arise from randomness inherent in the spike sampling process. Consequently, the inferred features from SLDS generalize poorly to held-out data. In contrast, the p-dLDS classifier predictions recover the full spectrum of reach angles since features are naturally continuous and the inferred coefficients can adjust the learned DOs to accurately capture the activity in the held out data. Table 2 shows that p-dLDS outperforms all other models in state and dynamics reconstruction as well as the top-1 and top-3 reach classification accuracy.

    &  &  &  &  \\  & & & \(( 10^{-1})\) & \(( 10^{-2})\) \\  SLDS & 38.46 & 57.69 & 0.5289 & 0.3942 \\ rSLDS & 12.82 & 32.05 & 0.5503 & 292.41 \\ dLDS & 10.25 & 39.74 & 0.6742 & 35.680 \\ pdLDS (ours) & **42.31** & **70.51** & **0.4061** & **0.0567** \\   

Table 2: Inference performance for the reaching experiment (see Figure 3) on a held-out test set. Top-1 and Top-3 accuracies are obtained by predicting reach directions from latent variable features using linear classifiers. State and Dynamics MSE are computed with respect to true latent variables.

Figure 4: **Learned system discovers coherent structure in clinical data. (A, D)** LFP data was collected on patients watching videos with different emotional content. **(B, E)** LFP spectrograms are 40-dimensional signals where each channel represents a particular frequency. **(C, F)** Inferred states and coefficients shows that rSLDS and dLDS exhibit unpredictable switching behavior. In contrast, p-dLDS captures smooth coefficients and identifies DOs that align with the trial’s emotional content. The learned patterns broadly generalize to the held-out data.

### Clinical Neurophysiology Data

We demonstrate p-dLDS on LFP recordings from the subcallosal cingulate cortex (SCC) in patients with treatment-resistant depression (ClinicalTrials.gov identifier NCT01984710). Subjects are asked to watch videos with different emotional content (positive, negative, and neutral), describe the videos, and then discuss how the video made them feel. SCC dynamics have been previously shown to provide a quantitative signal for the presence of emotional content  and depression recovery . Thus, we hypothesize that the underlying dynamics may provide information about emotional changes throughout the experiment. We apply p-dLDS to a single patient's LFP spectrogram data (Fig. 4B, E) within the 0-40 Hz frequency range (\(M=40\)). PCA indicates that the first 7 components explain 90% of the variance. Therefore, we train a model with \(N=7\) latent dimension and \(K=4\) DOs.

In Figure 4, rSLDS and dLDS produces a high degree of state oscillations making it difficult to identify time intervals with consistent emotional content. In contrast, p-dLDS infers coherent structure that corresponds to changes in emotional content in the trial. For example, \(_{4}\) (red) coincides with resting, \(_{2}\) (orange) with positive videos, and \(_{1}\) and \(_{3}\) (blue and green) to negative videos (Fig. 4C). Importantly, this structure persists even on held out data from the second half of the session (Fig. 4 F). We note this preliminary analysis on a single subject isn't intended to make a claim about specific neurophysiological responses to emotional content in this brain region, but generally highlights that p-dLDS identifies meaningful dynamical modes where previous models are unable to.

## 5 Conclusion

In this work, we present a probabilistic decomposed linear dynamical systems model that can be used to discover meaningful representations in neural signals. By marginalizing over uncertainty in latent variable estimates and incorporating an offset into the dynamics, we enhance robustness and improve a variety of performance metrics. Some areas of future work includes exploiting structure in the offsets to automatically identify window size and extending the probabilistic model to include more complicated emissions distributions, such as the Poisson likelihood commonly used to model neural spiking data .