# Typicalness-Aware Learning for Failure Detection

Yijun Liu\({}^{1}\) &Jiequan Cui\({}^{2}\) &Zhuotao Tian\({}^{1}\) &Senqiao Yang\({}^{3}\)

Qingdong He\({}^{4}\) &Xiaoling Wang\({}^{1}\) &Jingyong Su\({}^{1}\)

{liuyijun}@stu.hit.edu.cn

\({}^{1}\)Harbin Institute of Technology (Shenzhen) &\({}^{2}\)Nanyang Technological University

\({}^{3}\)The Chinese University of Hong Kong &\({}^{4}\)Tencent Youtu Lab

###### Abstract

Deep neural networks (DNNs) often suffer from the overconfidence issue, where incorrect predictions are made with high confidence scores, hindering the applications in critical systems. In this paper, we propose a novel approach called Typicalness-Aware Learning (TAL) to address this issue and improve failure detection performance. We observe that, with the cross-entropy loss, model predictions are optimized to align with the corresponding labels via increasing logit magnitude or refining logit direction. However, regarding atypical samples, the image content and their labels may exhibit disparities. This discrepancy can lead to overfitting on atypical samples, ultimately resulting in the overconfidence issue that we aim to address. To tackle the problem, we have devised a metric that quantifies the typicalness of each sample, enabling the dynamic adjustment of the logit magnitude during the training process. By allowing atypical samples to be adequately fitted while preserving reliable logit direction, the problem of overconfidence can be mitigated. TAL has been extensively evaluated on benchmark datasets, and the results demonstrate its superiority over existing failure detection methods. Specifically, TAL achieves a more than 5% improvement on CIFAR100 in terms of the Area Under the Risk-Coverage Curve (AURC) compared to the state-of-the-art. Code is available at https://github.com/liuyijungoon/TAL.

## 1 Introduction

Failure detection plays a vital role in machine learning applications, particularly in high-risk domains where the reliability and trustworthiness of predictions are crucial. Applications such as medical diagnosis , autonomous driving , and other visual perception tasks  require accurate assessments of prediction confidence before making critical decisions. The goal of failure detection is to enhance the reliability and trustworthiness of predictions, ensuring that high-confidence predictions are relied upon while low-confidence predictions are appropriately rejected . This is essential for maintaining the safety and effectiveness of these applications.

Indeed, deep neural networks (DNNs) trained using the cross-entropy loss often suffer from the issue of overconfidence. This leads to unreliable confidence scores, which in turn hinder the effectiveness of failure detection methods. It is common for models to make incorrect predictions with high confidence scores, sometimes even close to 1.0. A recent study called LogitNorm  has shed light on this problem. It reveals that the softmax cross-entropy loss can cause the magnitude of the logit vector to continue increasing, even when most training examples are correctly classified. This phenomenon contributes to the model's overconfidence.

To alleviate the overconfidence problem, LogitNorm  proposes assigning a constant magnitude to decouple the influence of the magnitude during optimization. The cross-entropy loss with thedecoupled logit vector \(=f(;)\) is defined as what follows:

\[_{}(,y)=-\|_{y}}}{ _{i=1}^{C}e^{\|f\|_{i}}},\] (1)

where \(\) is the parameters of the DNN model, \(\) is the input image with label \(y\), the logit vector \(\) is decoupled into the _magnitude_\(\|\|\) and the _directions_\(}\). Based on the decomposition of the logit vector in Eq. (1), we can observe that the overconfidence issue could stem from either increasing \(\|}\|\) or decreasing \(\) (the angle between the directions of prediction and label) during training.

**Key observation & Motivation.** Following the exclusion of the logit magnitude's impact by LogitNorm , we posit that the risk of the overconfidence issue still arises from logit directions. Typical samples, which have clear contextual information, help models generalize well. However, optimizing the direction for ambiguous atypical samples can still cause overconfidence. In these cases, the labels do not match the image context well. Aligning the logit direction of atypical samples may still lead to high softmax scores near 1.0 which worsens the overconfidence problem.

According to previous work [51; 44], the definitions of typical and atypical samples are based on their semantic similarity to most samples and the ease with which the model learns them. Specifically:

* _Typical samples_ are those that exhibit similarity to a majority of other samples at the semantic level. These samples possess typical features that are easier for deep neural networks to learn and generalize.
* _Atypical samples_, on the other hand, differ significantly from other samples at the semantic level. They pose a challenge for the model to generalize due to their uniqueness. These samples are often located near the decision boundary, causing the model to have higher uncertainty in making predictions for them.

In Fig. 1, we present an atypical example to illustrate the issue at hand. Despite the ground-truth label being a horse, the image depicts a horse with a human body, which could reasonably be predicted as either a human or a horse with a confidence score of around 50%. However, the model incorrectly predicts the image as a horse with an excessively high confidence score of 95%. Upon examining Eq. (1), we observe that the confidence score is determined by two crucial factors: magnitude and direction. This prompts an important question regarding the decoupling of these factors to determine which one is more reliable in accurately approximating real confidence. Addressing this inquiry is essential for effective failure detection.

**Our approach.** Based on the aforementioned observations, we propose a novel approach called _Typicalness-Aware Learning (TAL)_. TAL dynamically adjusts the magnitudes of logits based on the typicalness of the samples, allowing for differentiated treatments of typical and atypical samples. By doing so, TAL aims to mitigate the adverse effects caused by atypical samples and emphasizes that the direction of logits serves as a more reliable indicator of model confidence. In the blue dashed box of Fig. 1, we provide an example that illustrates the impact of TAL on an atypical sample. The logit vector could be changed from \(_{2}\) to \(_{1}\), indicating that the scores obtained with \(}\) for both "horse"

Figure 1: Illustration of the motivation. We observe that directly aligning the predictions of atypical samples to the target label is not appropriate, causing overconfidence (horse with 95% confidence). Instead, the confidence should be aligned with the human perception. During training, the cross-entropy loss increases the magnitude \(\|\|\) and adjusts their direction towards the target (represented by the angle \(\)). Consider this example where an image of a human body with a horse head is presented, the loss may optimize towards \(_{2}\) in the blue box, which is not the ideal outcome direction. Instead, it would be better to optimize towards \(_{1}\), rather than being biased towards either one, ensuring a more balanced and unbiased representation and allowing for a more accurate estimation of confidence.

and "human" become nearly equal. This alignment better aligns with human perception, highlighting the effectiveness of TAL in improving model confidence estimation.

The proposed TAL approach is model-agnostic, making it easily applicable to models with various architectures, such as CNN  and ViT . Experimental results on benchmark datasets, including CIFAR10, CIFAR100, and ImageNet, demonstrate the superiority of TAL over existing failure detection methods. Specifically, on CIFAR100, our method achieves a significant improvement of more than 5% in terms of the Area Under the Risk-Coverage Curve (AURC) compared to the state-of-the-art method .

In summary, the main contributions of this paper are as follows:

* We propose a new insight that the overconfidence might stem from the presence of atypical samples, whose labels fail to accurately describe the images. This forces the models to conform to these imperfect labels during training, resulting in unreliable confidence scores.
* In order to mitigate the issue of overfitting on atypical samples, we introduce the Typicalness-Aware Learning (TAL), which enables the identification and separate optimization of typical and atypical samples, thereby alleviating the problem of overconfidence.
* Extensive experiments demonstrate the effectiveness and robustness of TAL. Besides, TAL has no structural constraints to the target model and is complementary to other existing failure detection methods.

## 2 Background and Preliminary

Prior to introducing our method, we present the background of Failure Detection (FD). Additionally, we highlight the distinctions between failure detection and two closely related concepts: Confidence Calibration (CC) and Out-of-Distribution detection (OoD-D).

**Failure Detection.** Failure detection (FD) [18; 26; 49; 50] aims to differentiate between correct and incorrect predictions by utilizing the ranking of their confidence levels. In particular, a confidence-rate function \(()\) is employed to assess the confidence level of each prediction. High-confidence predictions are accepted, while low-confidence predictions are rejected. By using a predetermined threshold \(^{+}\), users can make informed decisions based on the following function \(g\):

\[g()=&(),\\ &.\] (2)

where \(()\) denotes a confidence-rate function, such as the maximum softmax probability.

**Failure Detection vs. Confidence Calibration.** Confidence calibration (CC) [33; 23; 27; 45] primarily emphasizes the alignment of predicted probabilities with the actual likelihood of correctness, rather than explicitly detecting failed predictions as in FD. The goal of CC is to ensure that the predictive confidence is indicative of the true probability of correctness:

\[P(=y=p^{*})=p^{*}, p^{*}.\] (3)

This implies that when a model predicts a set of inputs \(x\) to belong to class \(y\) with a probability \(p^{*}\), we would expect approximately \(p^{*}\) of those inputs to truly belong to class \(y\).

However, as observed by , models calibrated with CC algorithms do not perform well in FD. Traditional metrics used to evaluate CC, such as the Expected Calibration Error (ECE ), do not accurately reflect performance in FD scenarios. Instead, alternative metrics like the Area Under the Risk-Coverage Curve (AURC ) and the Area Under the Receiver Operating Characteristic Curve (AUROC ) are recommended for assessing FD performance.

**Failure Detection vs. Out-of-Distribution Detection.** While both Out-of-Distribution Detection (OoD-D) and failure detection tasks aim to enhance confidence reliability, they have distinct objectives, as depicted in Fig. 2. OoD-D focuses on rejecting predictions of semantic shift while accepting in-distribution predictions. However, it does not explicitly address the rejection of cases affected by covariate shifts. Additionally, through empirical observations in Sec. 4, we find that OoD-D methods are not well-suited for the Failure Detection task.

**Traditional Failure Detection (Old FD) vs. New Failure Detection (New FD).** Traditional failure detection methods [49; 50; 26] primarily focus on assessing the accuracy of predictions for in-distribution data. They also evaluate the discriminative performance of distinguishing correct and incorrect predictions for covariate shift data based on confidence scores. While these approaches address certain aspects of distribution shifts, they overlook the semantic shifts.

To address the limitations of Out-of-Distribution Detection (OoD-D) and traditional failure detection (Old FD) methods,  proposes a new setting called New FD. The objective of New FD is to accept correct predictions for both in-distribution and covariate shift samples, while rejecting incorrect predictions for all possible failures, including in-distribution, covariate shift, and semantic shift samples. Compared to OoD-D and Old FD, it enables more effective decision-making in real world.

## 3 Method

In this section, we present our proposed strategy called Typicalness-Aware Learning (TAL), as shown in Fig. 3. First, in Sec. 3.1, we address the shortcomings of existing training objectives and identify overfitting of atypical samples as a potential cause of overconfidence. Next, in Sec. 3.2, we outline the methodology used to calculate the "typicalness" of samples, enabling selective optimization and mitigating the negative impact of atypical samples. Finally, in Sec. 3.3, we introduce the TAL strategy, which incorporates the computed typicalness values for individual samples, resulting in improved performance for the failure detection task.

### Revisit the Cross-entropy Loss

In Eq. (1), the optimization of the cross-entropy loss involves either increasing the magnitude of the logits or aligning them better with the labels. However, LogitNorm  researchers have observed that as the training progresses and the model becomes more accurate in classifying samples, it tends to generate significantly larger logit magnitudes, leading to overconfidence. To address this issue,

Figure 2: The differences between closely related tasks. The blue curve represents the decision boundary, and the shaded area in the figure indicates incorrect predictions. (a) illustrates the objective of OoD-D tasks to reject predictions with semantic shifts and accept in-distribution predictions, without concern for predictions with covariate shifts. (b) shows the old setting of FD tasks, accepting correct in-distribution predictions and rejecting incorrect out-of-distribution predictions. (c) displays the new setting of FD tasks, accepting correct in-distribution predictions and correct predictions with covariate shifts, while rejecting incorrect in-distribution predictions, incorrect predictions with covariate shifts, and predictions with semantic shifts. (d) illustrates examples of OoD-D, Old FD, and New FD tasks. A classifier trained on CIFAR10  is evaluated on 6 images under a whole range of relevant distribution shifts: For instance, the 3rd and the 4th images in grayscale depict an airplane and a horse which encounter covariate shifts from that in the original CIFAR10. The 5th and the 6th images depict samples belonging to unseen categories with semantic shifts.

LogitNorm introduces a constant magnitude \(\) in Eq. (4) to mitigate the problem:

\[_{}(,y)=-}_ {*}}}{_{i=1}^{k}e^{}_{i}*}}.\] (4)

By keeping the magnitude constant in Eq. (4), the model places greater emphasis on producing features that align more closely with the target label in terms of direction, in order to minimize the training loss. However, as shown in Fig. 1, the presence of atypical samples with ambiguous content and labels may still cause overconfidence. Hence, it is imperative to devise a method that allows for the differentiation and separate optimization of typical and atypical samples.

Drawing inspiration from the cognitive process of human decision-making, it is reasonable to distinguish between typical and atypical samples by leveraging the knowledge acquired during the training phase. This approach allows us to effectively mitigate the negative effects of atypical samples, thereby preventing the occurrence of erroneous overconfidence.

### Distinguish Typical and Atypical Samples

To differentiate typical samples from atypical ones, we introduce a method for evaluating typicalness and implement typicalness-aware learning (TAL). This approach entails calculating the mean and variance of the feature representations. Specifically, we calculate the mean and variance of each sample's feature channels based on insights from CORES . The insight stems from the observation that in-distribution samples show larger magnitudes (mean) and variations (variance) in convolutional responses across channels compared to OoD samples, which are a type of atypical sample. The mean response of OOD samples is smaller than correct ID samples, as shown in Fig. 5(a). These statistical characteristics are subsequently compared to a set of historical data, representing typical samples, stored in a structured queue known as the "Historical Feature Queue" (HFQ). By comparing the statistical features of the input samples to those in the HFQ, we can quantify their typicalness.

**Initialize and update HFQ.** We commence the process by initializing the HFQ, denoted as \(Q\), with a predetermined size equivalent to the number of samples in the training dataset. This structured queue is responsible for retaining the mean and variance of feature representations for typical samples identified throughout the training phase.

To establish the initial state of the queue, we do not adopt the model trained from scratch. Instead, we employ a model that has been trained for a few epochs, corresponding to a small portion (\(\)) of the total training epochs. This approach ensures the quality of the queue during the early stages of training. In this study, we set \(=0.05\), which corresponds to 5% of the total training duration.

Figure 3: The framework of TAL. During training, statistical information (mean \(_{j}\) and variance \(_{j}\)) of features from correct predictions updates the Historical Features Queue (HFQ) at time-step t. The typicalness measure \(\) is calculated by comparing these statistics between the current batch and the HFQ. This \(\) influences the overall loss calculation, guiding the model to differentiate between atypical and typical samples. In the inference phase, TAL operates similarly to a model trained with conventional cross-entropy. Confidence is derived from the cosine similarity of the predicted logit direction, emphasizing our approach of using direction as a more reliable confidence metric. The framework distinguishes between typical (high \(\)) and atypical (low \(\)) samples, influencing the optimization process accordingly.

During this initialization phase, for each batch of data, we calculate the mean (\(\)) and variance (\(^{2}\)) of the feature vectors for each correctly predicted sample. Each sample in the queue stores its statistical features, denoted as follows, given a prediction \(\) and the ground truth label \(y\):

\[Q=\{(_{i},_{i}^{2})_{i}=y\}\] (5)

where \(_{i}\) and \(_{i}^{2}\) represent the mean and variance of the feature vectors of the \(i\)-th sample, respectively.

Once initialized, the statistics (mean and variance) of accurately predicted samples in each batch are directly added to the queue, as shown in Fig. 3. The queue has a fixed length of 20,000, and the ablation study is provided in Sec 4.3. The queue is updated using a First-In-First-Out (FIFO) approach, guaranteeing that it preserves a representative assortment of typical samples observed throughout the training process, while also adapting to the evolving data distributions. We empirically find this simple strategy works well in our experiments.

**Typicalness assessment.** To evaluate the typicalness \(\) of a new sample, we first calculate the mean (\(_{new}\)) and variance (\(_{new}^{2}\)) of its features \(\). Subsequently, we compute the \(L2\) distance \(d\) between the feature distribution of the new sample, represented by \(_{new}\) and \(_{new}^{2}\), and the distributions of the features stored in the HFQ, denoted as \((_{j},_{j}^{2}) Q\). Finally, we normalize the resulting distance using min-max normalization to obtain the typicalness \(\).

\[d=_{(_{j},_{j}^{2}) Q}W((_{new},_{new}^{2}),(_{j}, _{j}^{2})),\] (6)

\[=1-}{d_{max}-d_{min}}.\] (7)

Where \(d_{min}\) and \(d_{max}\) represent the minimum and maximum distances of samples in the batch. Eq. (7) normalizes the value of \(\) within the range of \(\), then \(\) can serve as a indicator of sample typicalness. A high \(\) value suggests that the sample is highly typical compared to the historical data. Conversely, a low \(\) value indicates an atypical or anomalous sample.

### Typicalness-Aware Learning

Sec. 3.1 highlights the potential negative impact of atypical samples on the training process. Building upon the insights provided in Sec. 3.2, we now introduce Typicalness-Aware Learning (TAL) in this section. TAL leverages the typicalness \(\) to distinguish between typical and atypical samples during the optimization process. This approach aims to mitigate the issue of overconfidence that arises from the presence of atypical samples.

**The training objective of TAL.** The training objective of TAL is defined by incorporating an additional loss term \(_{}\). This is achieved by modifying the LogitNorm equation, denoted as Eq. (4), to Eq. (8) where the samples \(\) are assigned with dynamic magnitudes \(T()\) based on typicalness \(\).

\[_{}(,y)=-_{}*T()}}{ _{i=1}^{k}e^{}_{i}*T()}}.\] (8)

**Dynamic magnitude \(T()\).** Given the upper bound \(T_{}\) and the lower bound \(T_{}\), the dynamic magnitude \(T()\) can be obtained via:

\[T()=T_{}+(1-)(T_{}-T_{}),\] (9)

where we empirically set \(T_{}\) and \(T_{}\) to 10 and 100, and they perform well on different benchmarks. The ablation study on different values is shown in Sec. 4.3.

Specifically, in Eq. (9), a _smaller_ magnitude \(T()\) will be assigned to _typical_ samples with large \(\), and a _larger_ magnitude \(T()\) will be assigned to _less typical_ samples with smaller \(\), enabling different treatments for typical/atypical samples. In this manner, for atypical samples, a higher value of \(T()\) reduces the influence that pulls them towards the label direction. This helps prevent their logit directions from being excessively optimized.

In other words, the inverse proportionality between \(T()\) and \(\) encourages the model to yield directions of \(\) that are well-aligned with the labels for the typical samples with large \(\) by assigning a small magnitude \(T()\). Conversely, for atypical samples with small \(\), the directions are not requiredto be as precise as the typical ones as the current \(T()\) is large, to mitigate the adverse impacts brought by the ambiguous label. To this end, _the direction \(}\) can serve as a more reliable indicator of the model confidence_.

**The overall optimization.** Fig. 3 illustrates the TAL framework, showing both training and inference processes. During the training process, we utilize both the proposed TAL loss \(_{}\) and cross-entropy loss \(_{}\) as it exhibits stronger feature extraction capabilities than LogitNorm . The overall loss is:

\[_{}=_{}(,y)+(1- )_{}(,y)\] (10)

The TAL loss, denoted as \(_{}\), is utilized to optimize the directions of reliable typical samples with large typicalness \(\), as well as potentially some atypical samples with small \(\).

The CE loss (not only relying on the CE loss, as it is regulated by \(\)) for atypical samples enables the optimization of both direction and magnitude. This may help reduce the adverse effects of atypical samples on the direction, enhancing the reliability of direction as a confidence indicator. This ensures that the optimization force on the logit directions of atypical samples is weaker compared to that of typical samples. The inference process does not involve the calculation of typicalness, and the only difference from the normal inference process is that our method uses Cosine as the confidence score.

To summarize, our proposed approach, as indicated in Eq. (10), enables models to selectively and adaptively optimize typical and atypical samples according to their typicalness values. This strategy enhances the reliability of feature directions as indicators of model confidence, ultimately improving the performance on failure detection task.

## 4 Experiments

To evaluate the effectiveness of the proposed Typicalness-Aware Learning (TAL) strategy, we conduct extensive experiments on various datasets, network architectures, and failure detection (FD) settings. More details such as the training configuration can be found in Appendix A.

**Datasets and models.** We first evaluate on the small-scale CIFAR-100  dataset with SVHN  as its out-of-distribution (OOD) test set. To demonstrate scalability, we further conduct experiments on large-scale ImageNet  using ResNet-50, with Textures  and WILDS  serving as OOD data. For CIFAR-100, we verify TAL's effectiveness across diverse architectures including ResNet , WRNet , DenseNet , and the transformer-based DeiT-Small . Detailed experimental results are provided in Appendix C.

**Three settings.** We evaluate TAL under three different settings: Old FD setting, OOD detection setting, and New FD setting (detailed in Section 2). While Old FD distinguishes between correct and incorrect in-distribution predictions, and OOD detection identifies out-of-distribution samples, our New FD setting aims to separate correctly predicted in-distribution samples from both misclassified and out-of-distribution samples. We maintain a 1:1 ratio between in-distribution and out-of-distribution samples in testing, and also report results for Old FD and OOD detection settings for completeness.

**Baselines.** We compare our proposed TAL method against classical Maximum Softmax Probability (MSP), MaxLogit, Cosine, Energy , Entropy , Mahalanobis , Gradnorm , SIRC  and recent LogitNorm , OpenMix  and (FMFP) . It is worth noting that FMFP focuses on improving accuracy for failure detection.

**Evaluation metrics.** To comprehensively assess the performance of TAL in failure detection, we adopt three widely recognized evaluation metrics [18; 49; 9], including Area Under the Risk-Coverage Curve (AURC), Area Under the Receiver Operating Characteristic Curve (AUROC), False Positive Rate at 95% True Positive Rate (FPR95).

### Comparisions with the State-of-the-art on CIFAR

**Evaluation with CNN-based architectures.** As shown in Tab. 1, our TAL strategy outperforms existing methods in New FD settings. Here are the key observations: 1) OoD methods like Energy and LogitNorm do not achieve satisfactory performance in the FD task. Please refer to Appendix B for explanation. 2) TAL consistently surpasses the baseline MSP and MaxLogit across various network architectures by a large margin. 3) In terms of comparison with FMFP, the prior SOTAmethod in the field of FD, our analysis reveals that TAL is complementary to the FMFP method and exhibits superior performance when combined with it. 4) Regarding Openmix, the metrics presented demonstrate that it falls short when compared to TAL.

### Comparisions with the Baseline on ImageNet.

    &  &  &  &  &  \\    & & **AURC\(\)** & **FPR95\(\)** & **AUROC\(\)** & **AUROC\(\)** & **FFPR95\(\)** & **AUROC\(\)** & **AUROC\(\)** & **FPR95\(\)** & **AUROC\(\)** \\   \\  MSP & 99.83 & 67.49 & 84.07 & 29.34 & 83.41 & 74.55 & 376.42 & 66.92 & 84.00 & 7.201 \\ Coine  & 96.53 & 65.15 & 84.42 & 271.13 & 78.30 & 79.31 & 361.87 & 56.23 & 86.93 & 72.01 \\ Energy  & 135.85 & 74.66 & 77.20 & 275.39 & 83.18 & 77.78 & 387.44 & 66.96 & 83.21 & 72.01 \\ MadLog & 133.19 & 27.33 & 79.76 & 275.85 & 82.53 & 77.73 & 385.81 & 65.08 & 83.56 & 72.01 \\ Energy  & 100.05 & 66.28 & 84.12 & 287.62 & 81.20 & 75.93 & 37.49 & 61.33 & 84.73 & 72.01 \\ MalManTools  & 141.42 & 71.38 & 80.41 & 26.49 & 272.01 & 80.55 & 36.85 & 58.74 & 85.74 & 72.01 \\ Grabonen  & 36.96 & 98.82 & 35.30 & 490.21 & 93.17 & 49.26 & 67.94 & 98.69 & 42.76 & 72.01 \\ SSE  & 100.56 & 66.37 & 84.01 & 29.73 & 81.03 & 75.90 & 374.12 & 61.29 & 84.65 & 72.01 \\
1-1-11 &  & 125.59 & 72.87 & 97.19 & 288.23 & 79.23 & 88.33 & 156.58 & 53.80 & 78.80 & 70.34 \\ OpenMix  & 85.66 & **63.82** & 83.25 & 43.16 & 87.03 & 69.27 & 40.68 & 70.37 & 80.25 & 73.68 \\
**TA** & 90.60 & 64.84 & **83.80** & 25.94 & 76.37 & 80.28 & **37.22** & **34.98** & **70.29** & 72.45 \\   PMFP  & 69.83 & 62.17 & 87.15 & 284.13 & 81.77 & 74.98 & 345.37 & 62.99 & 84.86 & 75.18 \\
**TA** & w/ FMPF & 73.16 & 64.82 & 85.51 & **24.50** & **78.61** & **81.50** & **32.02** & **85.22** & **88.81** & 75.59 \\  MSP & 57.60 & 60.13 & 87.47 & 26.47 & 80.04 & 79.41 & 32.19 & 57.26 & 87.23 & 78.55 \\ Coine  & 56.68 & 58.65 & 87.56 & 279.53 & 81.63 & 77.91 & 33.404 & 57.96 & 86.08 & 78.55 \\ Energy  & 66.33 & 65.64 & 84.73 & 237.78 & 78.93 & 80.83 & 322.27 & 58.70 & 87.07 & 78.55 \\ MadLog & 64.85 & 62.70 & 83.56 & 28.57 & 79.35 & 80.67 & 32.42 & 56.89 & 87.29 & 85.55 \\ Energy  & 59.31 & 62.78 & 68.60 & 269.72 & 95.00 & 80.26 & 30.66 & 56.92 & 87.40 & 78.55 \\ Mahalanobis  & 76.43 & 72.99 & 81.44 & 236.11 & 61.81 & 85.74 & 312.53 & 04.98 & 87.38 & 78.55 \\ Gradanon  & 134.85 & 76.20 & 68.51 & 35.28 & 75.61 & 75.57 & 40.68 & 65.57 & 77.46 & 78.55 \\ SRIC  & 59.57 & 63.38 & 86.70 & 29.93 & 77.90 & 80.50 & 32.05 & 56.93 & 87.45 & 78.55 \\ LogNet  & 78.86 & 65.56 & 26.28 & **20.445** & **88.99** & **89.01** & **29.24** & **13.88** & **29.03** & 77.15 \\ OpenMix  & 50.09 & **56.62** & **86.90** & **87.03** & 21.49 & 74.33 & 81.16 & 304.35 & 81.75 & 85.99 & 79.2 \\
**TA** & 55.41 & 54.83 & 84.41 & 24.36 & 79.48 & 81.29 & 303.18 & 54.62 & 89.22 & 78.14 \\   FMPF  & 41.60 & **56.68** & **89.50** & 245.10 & 75.71 & 81.42 & 290.35 & 55.63 & 88.89 & 81.07 \\
**TA** & w/ FMPF & 44.21 & 58.77 & 88.40 & **22.94** & **10.32** & **85.31** & **22.82** & **87.41** & **91.00** & **80.98** \\  MSP & 79.06 & 63.62 & 87.55 & 257.69 & 77.86 & 79.77 & 33.38 & 37.58 & 87.64 & 74.62 \\ Coine  & 82.36 & 63.65 & 84.72 & 250.14 & 77.01 & 81.48 & 332.12 & 53.82 & 88.28 & 74.62 \\ Energy  & 108.08 & 72.99 & 78.73 & 24.01 & 71.56 & 82.81 & 341.87 & 85.95 & 86.94 & 74.62 \\ MadLog & 105.89 & 70.20 & 79.60 & 240.06 & 75.65 & 82.86 & 339.86 & 56.69 & 87.39 & 74.62 \\ Energy  & 79.94 & 65.41 & 85.46 & 250.63 & 75.54 & 81.34 & 33.08 & 83.97 & 88.30 & 74.62 \\ Mahalanobis  & 159.95 & 93.30 & 64.54 & **250.38** & **53.43** & **89.01** &To showcase the scalability of our approach, we present the results on ImageNet in Table 2. It is obvious that our TAL strategy consistently enhances the failure detection performance of the baseline method, significantly improving the reliability of confidence. Notably, TAL reduces the AURC by 3.7 and 11.6 points, indicating a better overall performance in distinguishing between correct and incorrect predictions. It is worth noting that TAL achieves these impressive improvements while maintaining a comparable accuracy to the MSP baseline.

Additionally, we present the Risk-Coverage (RC) curves (Fig. 5(c)) for both old and new FD task settings on ImageNet. The comparison between TAL and baseline RC curves demonstrates the effectiveness of our method. Fig. 5(d) further visualizes typical and atypical data examples. For the fish category in ImageNet, typical data includes common fish images, while atypical data comprises both rare fish images from ImageNet and out-of-distribution samples.

### Ablation Study

**The ablation study of key components.** We conduct experimental ablations on the components of our TAL loss with CIFAR100. The results are summarized in Table 3. With the dynamic magnitude \(T()\) strategy, we achieve substantial enhancements in Failure Detection performance, which manifests the effectiveness of integrating typicalness-aware strategies into training approaches.

**The impacts of \(T_{}\) and \(T_{}\).** We perform an ablation study using ResNet110 on the CIFAR10 and CIFAR100 datasets to examine the impact of \(T_{}\) and \(T_{}\) on failure detection performance. Fig. 4 (a) and Fig. 4 (b) present the experimental results of the failure detection metric EAURC for CIFAR10 and CIFAR100, respectively. Darker regions in the figures correspond to lower values of the metric, indicating superior failure detection performance. The findings suggest that while \(T_{}\) should not be set too small, a moderate increase in \(T_{}\) can enhance failure detection capabilities.

**The effects of the length of Historical Feature Queue.** We conduct an ablation study on the CIFAR100 dataset to examine the impact of queue length on failure detection performance. The original CIFAR100 dataset consists of 50,000 training images, with 5,000 images reserved for validation and the remaining 45,000 images used for training. The results, depicted in Figure 4 (c), demonstrate that queue lengths ranging from 10,000 to 50,000 yield similar failure detection performance. However, when the queue length exceeds 50,000, there is a noticeable decline in failure detection performance.

Figure 4: (a) and (b) is the ablation study of \(T_{}\), \(T_{}\). And (c) is the ablation study on the length of the Historical Feature Queue.

   Method & Setting & AURC \(\) & EAURC \(\) & AURC \(\) & FPR8\(\) & TNR9S \(\) & AURP\_Success \(\) & AURP\_Error \(\) & ACC \(\) \\   & Old FD & 108.46 & 58.81 & 83.87 & 62.76 & 37.24 & 92.23 & 68.35 & 0.70 \\  & New FD & 355.62 & **69.67** & 88.74 & 53.45 & 46.52 & **83.78** & 92.51 & 0.70 \\   & Old FD & 99.60 & 55.63 & 83.57 & 65.65 & 34.35 & 92.81 & 66.00 & 0.72 \\  & New FD & 362.77 & 85.41 & 86.66 & 57.00 & 43.00 & 80.57 & 91.10 & 0.72 \\   & Old FD & **94.33** & **49.43** & **85.58** & **61.24** & **38.69** & **93.56** & **68.70** & **0.72** \\  & New FD & **351.49** & 72.69** & **88.92** & **47.44** & **52.46** & 83.03 & **92.86** & **0.72** \\   

Table 3: Ablation of the key components. **Best** are bolded and second best are underlined. AURC and EAURC are multiplied by \(10^{3}\), the remaining metrics are percentages except ACC. “Fixed T” means the dynamic magnitude \(T()\) in TAL is not adopted.

Ablation of Typicality Measures.As depicted in Fig. 5 (b), we have conducted extra ablation experiments with K-nearest neighbor (KNN) distance and Gaussian Mixture Models (GMM) to assess typicality. These alternative measures did not enhance performance (lower AURC is preferable), thereby reinforcing the validity of our selection of mean/variance criteria.

## 5 Concluding Remarks

Summary.This paper introduces Typicalness-Aware Learning (TAL), a novel approach for mitigating overconfidence in DNNs and improving failure detection performance. The effectiveness of TAL can be attributed to a crucial insight: overconfidence in deep neural networks (DNNs) may arise when models are compelled to conform to labels that inadequately describe the image content of atypical samples. To address this issue, TAL leverages the concept of typicalness to differentiate the optimization of typical and atypical samples, thereby enhancing the reliability of confidence scores. Extensive experiments have been conducted to validate the effectiveness and robustness of TAL. We hope TAL can inspire new ideas for further enhancing the trustworthiness of deep learning models.

Limitations.The main contribution of TAL lies in recognizing the issue of overfitting atypical samples as a cause of overconfidence and proposing a comprehensive framework to tackle this problem. Given that the methods adopted in this work are simple yet effective, there is still potential for further improvement by incorporating more advanced designs, such as the methods for typicalness calculation and the dynamic magnitude generation. These are left as future work to be explored.

Broader impacts.As deep learning models become increasingly integrated into critical systems, from autonomous vehicles to medical diagnostics, the need for accurate and reliable confidence scores is paramount. TAL's ability to improve failure detection performance directly addresses this need, potentially leading to safer and more dependable AI systems.