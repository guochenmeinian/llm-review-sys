# GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI

Pengcheng Chen1,2 Jin Ye1,3,* Guoan Wang1,4 Yanjun Li1,4

**Zhongying Deng5 Wei Li1,6 Tianbin Li1 Haodong Duan1 Ziyan Huang1,6 Yanzhou Su1 Benyou Wang7,8 Shaoting Zhang1 Bin Fu9 Jianfei Cai3 Bohan Zhuang3 Eric J Seibel2 Yu Qiao1,9 Junjun He1\({}^{}\)**

1Shanghai AI Laboratory 2 University of Washington 3Monash University 4East China Normal University 5University of Cambridge 6Shanghai Jiao Tong University 7The Chinese University of Hong Kong, Shenzhen 8Shenzhen Research Institute of Big Data

###### Abstract

Large Vision-Language Models (LVLMs) are capable of handling diverse data types such as imaging, text, and physiological signals, and can be applied in various fields. In the medical field, LVLMs have a high potential to offer substantial assistance for diagnosis and treatment. Before that, it is crucial to develop benchmarks to evaluate LVLMs' effectiveness in various medical applications. Current benchmarks are often built upon specific academic literature, mainly focusing on a single domain, and lacking varying perceptual granularities. Thus, they face specific challenges, including limited clinical relevance, incomplete evaluations,

Figure 1: Overview of the GMAI-MMBench. The benchmark is meticulously designed for testing LVLMsâ€™ abilities in real-world clinical scenarios with three key features: (1) Comprehensive medical knowledge: It consists of 284 diverse clinical-related datasets from worldwide sources, covering 38 modalities. (2) Well-categorized data structure: It features 18 clinical VQA tasks and 18 clinical departments, meticulously organized into a lexical tree. (3) Multi-perceptual granularity: Interactive methods span from image to region level, offering varying degrees of perceptual details.

and insufficient guidance for interactive LVLMs. To address these limitations, we developed the GMAI-MMBench, the most comprehensive general medical AI benchmark with well-categorized data structure and multi-perceptual granularity to date. It is constructed from 284 datasets across 38 medical image modalities, 18 clinical-related tasks, 18 departments, and 4 perceptual granularities in a Visual Question Answering (VQA) format. Additionally, we implemented a lexical tree structure that allows users to customize evaluation tasks, accommodating various assessment needs and substantially supporting medical AI research and applications. We evaluated 50 LVLMs, and the results show that even the advanced GPT-4o only achieves an accuracy of 53.96%, indicating significant room for improvement. Moreover, we identified five key insufficiencies in current cutting-edge LVLMs that need to be addressed to advance the development of better medical applications. We believe that GMAI-MMBench will stimulate the community to build the next generation of LVLMs toward GMAI.

## Introduction

In clinical practice, diverse demands may be proposed by different medical institutions for disease diagnosis and treatment. These demands can be potentially fulfilled by general medical AI which provides general-purpose medical models to tackle a wide range of medical tasks. Such models are typically Large Vision-Language Models (LVLMs) trained on diverse data types, including imaging and clinical texts, to tackle diverse tasks, e.g., disease diagnosis and severity grading. Noticeably, the state-of-the-art LVLMs, including general-purpose ones (e.g., DeepSeek-VL , GPT-4V  and Claude3-Opus ) and medical purposes (like MedDr , LLaVA-Med , and Med-Flamingo ), have both demonstrated promising performance in some medical visual-textual tasks. However, it remains unclear to what extent these LVLMs can accommodate the diverse demands in real clinical scenarios. To validate their effectiveness and promote their application in clinical practice, it is crucial to establish a comprehensive benchmark to address diverse real-world demands. Therefore, an ideal benchmark should achieve three specific aims:

**Aim 1. Comprehensive medical knowledge.** Medical knowledge is embedded in medical data, so comprehensive medical knowledge requires diverse medical data of different modalities from various data sources. In clinical scenarios, various types of imaging modalities, including X-rays, Computed Tomography (CT), Magnetic Resonance Image (MRI), Ultrasound Imaging, Positron Emission Tomography (PET), etc, are employed for diagnostic and therapeutic purposes, reflecting different aspects of medical knowledge . Besides, to encompass the diverse medical knowledge from different clinical facilities, the data used in a comprehensive benchmark should cover a range of different clinical institutions and hospitals which are preferably distributed across the world . These demands favor benchmarks collected from diverse sources. **Aim 2. Comprehensive evaluation across all clinical aspects.** A comprehensive benchmark should be easily customized to evaluate any specific abilities of LVLMs for each clinical professional. This property is necessary because there are an excessive amount of clinical institutions, departments, and practitioners, each having their own specific demand. Their potential demands can be concluded in two sides: 1) _Evaluation across diverse tasks_. Some clinical practitioners may require MRI data for disease diagnosis while others may need to deal with surgical workflow recognition for computer-assisted or robot-assisted surgery systems. Therefore, a comprehensive benchmark should cover all clinical demands by encompassing a sufficient number of diseases and tasks. 2) _Evaluation for diverse clinical departments_. Some departments may be interested in LVLMs' performance on oncology-related tasks only while others may only focus on urology-related ones. As such, a comprehensive benchmark should be easily used for customized evaluation to accommodate the diverse demands of different clinical departments. These demands further require the benchmark to be well-categorized to facilitate ease of use. **Aim 3. Interactive ability in multi-perceptual granularity.** Given a specific medical image, doctors need to look through the whole image (image level) for an overview while also requiring comprehensive explanations in a specific position (mask level) or region (box level). This demand requires LVLMs to perceive the granularity range from a specific position to the entire image. Thus, a comprehensive benchmark should also evaluate LVLMs' perceptual granularity.

As shown in Table 1, there are some medical benchmarks, such as Medical-Diff-VQA , PathVQA , Cholec80-VQA , and Cholec80 , dedicated to evaluating specific abilities of LVLMs. These benchmarks effectively assess the performance of LVLMs within a particular modality or task, thereby facilitating the optimization of models for specific applications. Nonetheless, their limited modalities and tasks cannot meet the requirement of modal and task diversity. Other benchmarks including VQA-RAD , RadBench , and MMMU (Health & Medicine)  address this issue by providing multiple modalities and tasks for evaluation, with data consisting of natural image-text pairs sourced from academic papers, textbooks, and specific databases. Though these benchmarks significantly enhance the breadth and depth of medical assessment, they may not accurately reflect actual clinical requirements, as their sources are distant from clinic practice and prone to data leakage . More importantly, _none of these benchmarks can be customized to evaluate various abilities of LVLMs to accommodate highly diverse clinical demands_ because their data are not well categorized. For instance, it is hard to obtain the dimension, modality, and task information of a specific data point in these datasets, which prevents a clinical professional from evaluating LVLMs using the CT (modality) of 2D (dimension) images for blood vessel recognition (task). Due to this, they can hardly be used for customized evaluation. In summary, though existing medical multimodal benchmarks provide valuable evaluation frameworks, they present challenges in fully addressing clinical needs. Future developments necessitate more refined and customized benchmarks that are closely aligned with real-world clinical applications.

To address these challenges, we introduce the General Medical AI MultiModal Benchmark (GMAIMBench), a comprehensive multimodal benchmark that is well-categorized for medical image understanding and reasoning in real-world clinical scenarios. As shown in Figure 1, its comprehensiveness can be concluded in three aspects: 1) **comprehensive medical knowledge from diverse modalities, tasks, and data sources**, 2) **well-categorized in lexical tree structures**, and 3) **multiple perceptual granularity**.

First, GMAI-MMBench has diverse modalities and data sources because it is built upon 284 high-quality datasets collected across the world. These 284 datasets cover various medical image tasks, including 2D detection, 2D classification, and 2D/3D segmentation, to ensure the diversity of tasks. Using these foundational visual-based tasks has two advantages over using off-the-shelf image-text pair data. 1) It minimizes the risk of data leakage since the data in our benchmark are mostly image-label pairs rather than image-text pairs. The image-label pairs are not directly convertible to LVLMs training samples (usually image-text pairs), thus less likely to be used to train LVLMs; 2) It ensures high clinical relevance, as the images are sourced from hospitals and annotated by professional doctors. We then carefully selected approximately 26K cases with 38 different modalities to construct the GMAI-MMBench, thus meeting the modal diversity goal.

Second, GMAI-MMBench is a well-categorized medical benchmark that can comprehensively evaluate the pros and cons of various aspects of LVLMs, benefiting both model developers and users with specific needs. Specifically, we develop a categorization system, called lexical tree structure, which categorizes all cases into 18 clinical VQA tasks, 18 departments, 38 modalities, etc. The 'clinical VQA tasks' / 'departments' /'modalities' are the lexicons that can be used to retrieve desired

 
**Benchmark** & **Modality** & **Size** & **Task** & **Dept** & **PG** & **Source** \\  Medical-Diff-VQA*  & 1 & 70K & 7 & âœ˜ & I & MMIRC-CXR  \\ PathVQA*  & 1 & 6K & 7 & âœ˜ & I & Textbook, PEIR  \\ Cholec80-VQA*  & 1 & 9K & 2 & âœ˜ & I & Cholec80  \\ VQA-RAD  & 3 & 3K & 11 & âœ˜ & I & Teaching cases from Medpix  \\ RadBench  & 6 & 137K & 5 & âœ˜ & I & 13 image-text paired datasets \\ MMMU (H \& M)  & 6 & 2K & 5 & âœ˜ & I, B & Exam, Quiz, Textbook \\ SLAKE*  & 3 & 2K & 10 & âœ˜ & I & MSD , Chest-ray8 , CHAOS  \\ OmnidKQA  & 12 & 128K & 5 & âœ˜ & 73 classification datasets & \\  GMAI-MMBench & 38 & 26K & 18 & âœ˜ & I, B, M, C & 284 datasets from both public and hospital \\  

Table 1: Comparison between GMAI-MMBench and other existing benchmarks in the biomedical field. GMAI-MMBench is sourced from extensive data sources worldwide, offering comprehensive medical knowledge detailed in modalities, clinical tasks, departments, and perceptual granularities. Dept and PG indicate department and perceptual granularity, respectively. In the perceptual granularity types, I, B, M, and C denote image, box, mask, and contour, respectively. * indicates the test set.

cases for evaluation. For instance, the oncology department can select cases related to oncology to evaluate LVLMs' performance for oncology tasks, thus greatly enhancing flexibility and usability for specific demands.

Third, GMAI-MMBench can evaluate LVLMs' abilities to perceive different granularity, such as understanding the local image content in a mask or bounding box as well as recognizing the entire image content. This ability is important for detection, segmentation, and classification tasks as these tasks need different perceptual granularity for better performance. Furthermore, the perception of bounding boxes or masks is vital for interactive LVLMs , so the perceptual granularity evaluation in our benchmark can possibly be used to improve interactive LVLMs.

We assess 44 publicly available LVLMs (38 general purpose and 6 medical-specific models) as well as advanced proprietary LVLMs such as GPT-4o, GPT-4V, Claude3-Opus, Gemini 1.0, Gemini 1.5, and Qwen-VL-Max on our GMAI-MMBench. We summarize the key findings as follows:

(1) GMAI-MMBench presents significant challenges in clinical practice. Even the best proprietary GPT-4o only achieves an accuracy of 53.96%, which demonstrates the deficiencies of cutting-edge LVLMs in tackling medical professional issues, thus they can hardly fulfill diverse clinical demands.

(2) Open-source LVLMs, such as MedDr and DeepSeek-VL-7B, achieve approximately 44% accuracy, making them very competitive compared to proprietary models. For instance, they surpass Claude3-Opus and Qwen-VL-Max and achieve comparable performance to Gemini 1.5 and GPT-4V. However, they still exhibit a clear performance disparity compared to the top-performing GPT-4o.

(3) Most medical-specific models have difficulty reaching a general performance level (approximately 30% accuracy) achieved by general LVLMs, except MedDr with 43.69% accuracy.

(4) Most LVLMs exhibit unbalanced performance across different clinical VQA tasks, departments, and perceptual granularity. Notably, in the experiments on different perceptual granularity, box-level annotation consistently results in the worst accuracy, even worse than image-level annotation.

Figure 2: Examples of GMAI-MMBench. The benchmark covers a variety of clinical tasks, departments, and perceptual granularities from worldwide data sources.

(5) The major factors leading to performance bottlenecks include perceptual errors (e.g., misrecognition of image content), lack of medical domain knowledge, irrelevant responses, and rejection of answering questions due to safety protocols.

In summary, our contributions are three-fold. (a) We introduce a comprehensive benchmark, GMAI-MMBench, to evaluate existing LVLMs in clinical practice. GMAI-MMBench covers 38 modalities, 18 clinical VQA tasks, 18 departments, and 4 different perceptual granularity from 284 medical-related datasets, thereby offering a diverse range of modalities, tasks, and data sources. (b) GMAI-MMBench organizes each data point in lexical tree structures, with lexicons used to select desired data points to evaluate various aspects of LVLMs' abilities. Thus, GMAI-MMBench facilitates customized evaluation to meet highly diverse demands in clinical practice. **See Supplementary C.2**. (c) We evaluate 44 representative general-purpose LVLMs, including both open-source and proprietary models, as well as 6 medical-specific LVLMs on GMAI-MMBench. The comprehensive evaluation reveals the pros and cons of different LVLMs from diverse perspectives, providing insights to improve these models to accommodate real-world clinical applications.

## GMAI-MMBench

### Overview

We propose GMAI-MMBench, an innovative benchmark meticulously designed for the medical field, capable of providing comprehensive evaluations of LVLMs across various aspects of healthcare. (shown in the Figure 2) We collect 284 datasets from public sources and hospitals, covering medical imaging tasks of detection, classification, and segmentation, to form the data fuel for establishing such a benchmark. The detailed datasets are listed in the supplementary. Based on the data foundation, we design a reliable pipeline to generate question-answering pairs and organize them from different perspectives with manual validation. Finally, we carefully select approximately 26K questions with

Figure 3: Overall illustration of GMAI-MMBench. The data collection can be divided into three main steps: 1) We search hundreds of datasets from both the public and hospitals, then keep 284 datasets with highly qualified labels after dataset filtering, uniforming image format, and standardizing label expression. 2) We categorize all labels into 18 clinical VQA tasks and 18 clinical departments, then export a lexical tree for easily customized evaluation. 3) We generate QA pairs for each label from its corresponding question and option pool. Each question must include information about image modality, task cue, and corresponding annotation granularity. The final benchmark is obtained through additional validation and manual selection.

varying levels of perceptual granularity from the manually validated cases to construct the final GMAI-MMBench.

### Benchmark Construction

The detailed steps of constructing our GMAI-MMBench can be divided into three main steps as shown in Figure 3.

**Dataset collection and standardization.** As our aim is to build a large-scale benchmark for the comprehensive evaluation of LVLMs, the first and most important step is data collection. In contrast to benchmarks that directly use multimodal paired datasets, we source the datasets in two ways to minimize the data leakage problem and ensure the diversity and clinical property: First, we conduct thorough Internet searches to collect as many 2D/3D medical-related datasets as possible, retaining those that involve classification, detection, and segmentation tasks. Second, we collaborate with several hospitals that have agreed to share their ethically approved data. This process has enabled us to curate 284 datasets with highly qualified labels. Following data collection, we standardize both images and labels. For images, we adhere to the SA-Med2D-20M  protocol, transforming all 2D/3D medical images into 2D RGB images for further evaluation. For labels, we refer to the Medical Subject Headings (MeSH)4 to ensure every label is unique, clear, and free from conflict or ambiguity within each task. Specifically, we focus on three main situations: (1) expanding all abbreviations, such as changing "AMD" to "Age-related macular degeneration"; (2) unifying different expressions for the same target, such as standardizing both "lung nodule" and " pulmonary nodule"; (3) merging labels with left and right distinctions, such as combining "left kidney" and "right kidney" into "kidney", since our goal is to evaluate the abilities of understanding and reasoning rather than directional judgment.

**Label categorization and lexical tree construction.** We construct a well-categorized lexical tree to ensure GMAI-MMBench can be easily customized to evaluate the specific abilities of LVLMs for each clinical professional. The overview of the tree is shown in Figure 3, and the complete version is in supplementary. First, we integrate data properties and real applications to propose three subjects tailored for the biomedical fields: clinical VQA tasks, departments, and perceptual granularities. Specialized options are generated for each subject individually: For clinical VQA tasks, we extract keywords according to the original dataset descriptions and then lead to 18 categories. For departments, we refer to the Mayo Clinic5 and assign all labels to 18 departments. For perceptual granularity, we construct 4 types based on annotation methods (see the rightmost panel in Figure 1). We then recruit several biomedical engineering university students (including coauthors) to tag labels from the constructed options in these subjects. Specifically, each label is randomly assigned to 3 people, and their tagging results are merged by voting. After label categorization, the lexical tree can be directly exported for customized evaluation. An example of customized evaluation is presented in Supplementary C.2.

**QA generation and selection.** Following the label categorization, all labels are assigned to specific modalities, clinical VQA tasks, departments, and perceptual granularities. Based on the well-organized structure, we generate the VQA pairs for every label with three steps. First, questions and options generation. For question generation, a question must include three key pieces of information in GMAI-MMBench: modality, clinical task hint, and perceptual granularity information. For each combination of the three elements, we randomly pick 10 labels and generate 10 candidate questions with GPT-4o for each selected label. These questions are then manually reviewed to meet the following criteria: (1) they must include necessary information on modality, clinical task, and perceptual granularity; (2) they do not include any hints that would allow the question to be answered without viewing the image. After manual review, the modality is replaced with a placeholder for standardization. For example, a valid question template for Disease Diagnosis in segmentation task is: _"This is a <modality> image. Which of the following options is the most appropriate to demonstrate symptoms in the marked area?_" Once the question pool is generated, each category has its question pool based on its tags of modality, clinical VQA task, and perceptual granularity. For options generation, the global view (image level) and local view (mask level, bounding box level, and contour level) of perceptual granularity are handled separately. For the global view, the option pool for each answer is sourced from the remaining categories within the answer's dataset to avoid introducingmultiple correct answers. For instance, a fundus image dataset may focus solely on pathological myopia, but the images might also contain other diseases like diabetic retinopathy. Including other categories could render the question invalid. For the local view, we construct a shared option pool for the answers with the combination of modality, clinical VQA task, and perceptual granularity. Second, as each answer with corresponding images has its own question and option pool, we generate all QA pairs for all images. For each image, we randomly select a question from its question pool and replace the placeholder with its modality. Along with the correct answer, we randomly select \(n\) options (where \(n=((1,),(4, )\) from the corresponding option pool to create the set of options. Third, to ensure data quality and balanced distribution, we perform additional manual validation and selection. In the validation stage, we assess the QA pairs based on the following criteria: (1) We drop cases whose questions do not contain the three key components and can be answered without the image. (2) We filter out cases with incorrect answers. (3) We drop cases where images have unclear targets or poor image quality. In the selection stage, we choose 30 cases per answer to ensure balance across all tasks (all cases are included if the number is less than 30). The selection rule is based on the consideration of diversity: Selecting images with large differences in appearance, data source, age, gender, etc. As a result, we finalize 25831 QA pairs for the GMAI-MMBench (4550 in the validation set and 21281 in the test set).

## Experiments

### Experiment setup

In this study, we evaluated various LVLMs, including medical-specific, open-source, and proprietary API general models. We selected versions with approximately 7 billion parameters for testing, and the model weights were sourced from their respective official Hugging Face repositories. Our evaluation was conducted using the VLMEvalKit6 framework and Multi-Modality-Arena7.

The assessment was performed in a "zero-shot" setting. Specifically, our evaluation prompts did not include any example cues, and the models were required to perform inference on tasks without prior training or examples related to those tasks. This approach better tests the models' generalization capabilities and comprehension, examining their performance when confronted with novel problems. All tests were executed using NVIDIA A100 GPUs with 80GB of memory.

### Models

For completeness, we conducted evaluations using several state-of-the-art LVLMs to benchmark their performance on GMAI-MMBench, including both general models that have extended capabilities in the biomedical domain and medical-specific models that are meticulously trained for clinical medicine. By default, we use the latest, largest, and best-performing available checkpoint for each model family to ensure optimal performance. We picked 29 out of 50 models for demonstration in the main text, additional results are provided in the supplementary material. For medical-specific models, we include 5 latest powerful LVLMs: MedDr , LLaVA-Med , Med-Flamingo , RadFM , and Qilin-Med-VL-Chat . For general models, we test 18 representative LVLMs: TransCore-M , VisualGLM-6B , mPLUG-Owl2 , OmniLMM-12B , Mini-Gemini-7B , Emu2-Chat , MMAlaya , CogVLM-Chat , InstructBLIP-7B , DeepSeek-VL-7B , Idefics-9B-Instruct , XComposer2 , Yi-VL-6B , InternVL-Chat-V1.5 , LLAVA-V1.5-7B , LLAVA-InternLM2-7b , MiniCPM-V2 , and Qwen-VL-Chat . In addition, we also evaluate 6 proprietary LVLMs via API: Qwen-VL-Max , Claude3-Opus , GPT-4V , GPT-4o , Gemini 1.0 , and Gemini 1.5 .

### Metrics

To evaluate the model's performance, we use macro-averaged accuracy (ACC) as the evaluation metric for single-choice questions. For multiple-choice questions, we first count the number of correct predictions for each case, then calculate accuracy (\(}\)) and recall (\(}\)) based on 

[MISSING_PAGE_FAIL:8]

VL-7B, although not as accurate as GPT-4o, have surpassed Claude3 Opus and Qwen-VL-Max, approaching the performance of GPT-4V. This suggests that open-source models in the medical field are gradually catching up to the top-performing commercialized models.

**Insufficiency 1. Performance on different clinical VQA tasks needs improvement:** Table 2 shows that the best-performing clinical VQA tasks are Disease Diagnosis (DD) and Nervous Tissue (NT), with models exceeding the random baseline by an average of over 10%. However, in clinical VQA tasks such as Severity Grading (SG) and Attribute Recognition (AR), most LVLMs face challenges, and most of them perform worse than the random baseline. Overall, despite the advanced models like GPT-4o and Gemini 1.5 significantly outperforming the random baseline, there remains a substantial gap between their performance and the requirements of real-world applications, indicating that all the models still need more specialized medical knowledge for training.

**Insufficiency 2. The performance across different departments needs further balancing:** In examining performance across different medical departments, as shown in Table 3, we found that the Infectious Diseases (ID) and Neurosurgery (N) departments performed the best. In contrast, departments such as General Surgery (GS) and Obstetrics and Gynecology (OG) showed a need for improvement, as the performance of all models in these areas did not significantly exceed the random baseline compared to other departments. This indicates that current large models exhibit specialization biases, suggesting that future development of LVLMs aiming to achieve general medical AI should focus on balancing capabilities across all departments.

**Insufficiency 3. The LVLMs are not robust among different perceptual types:** As shown in Figure 4, models perform slightly better with contour-level perception compared to mask-level perception, and both outperform image-level perception (without annotation) significantly. However, bounding box-level perception shows the worst performance among all perceptual types, indicating that models are sensitive to this perceptual type. This evaluation underscores the need for LVLMs to address robustness issues across different perceptual types, which is crucial for their effectiveness in interactive applications.

    & Overall & Omil & CS & O & E6 & G5 & G5 & H & D-G & UMP & MN & N & OO & OM & O & OS & METHODS & PM & SM & U \\   & Text20 & 58.4 & 22.8 & 20.9 & 25.9 & 27.2 & 23.4 & 24.6 & 37.1 & 28.9 & 28.6 & 24.0 & 28.3 & 27.7 & 30.6 & 29.2 & 27.3 & 27.4 & 22.7 & 28.7 & 29.1 \\   & Med-Frameps.(181) & 12.4 & 16.4 & 17.6 & 12.9 & 10.0 & 10.8 & 9.3 & 5.2 & 10.6 & 12.0 & 10.0 & 19.1 & 12.8 & 14.9 & 18.7 & 12.0 & 13.3 & 12.9 & 10.4 \\   & Max-Action(181) & 22.4 & 16.4 & 17.6 & 12.9 & 10.0 & 10.8 & 9.3 & 5.2 & 10.0 & 10.0 & 19.1 & 12.8 & 14.9 & 18.9 & 15.7 & 12.0 & 13.3 & 14.3 & 12.9 & 10.4 \\   & Max-Action(181) & 22.4 & 22.6 & 22.4 & 22.8 & 10.0 & 10.0 & 10.8 & 18.3 & 17.3 & 22.6 & 10.0 & 10.0 & 19.1 & 12.8 & 14.9 & 18.7 & 12.0 & 14.7 & 12.3 \\   & Max-Action(181) & 22.4 & 22.6 & 22.4 & 22.8 & 10.0 & 10.0 & 10.8 & 18.3 & 17.3 & 22.6 & 10.0 & 10.0 & 18.3 & 18.0 & 18.3 & 18.0 & 18.4 & 18.7 & 12.0 & 12.7 & 12.3 \\   & Max-Action(181) & 22.9 & 22.9 & 22.4 & 22.8 & 10.0 & 20.0 & 19.9 & 18.9 & 18.3 & 22.8 & 22.4 & 10.0 & 10.0 & 18.0 & 18.0 & 18.3 & 18.0 & 18.4 & 18.7 & 12.1 & 12.3 & 22.0 \\   & Med-Frameps.(181) & 22.9 & 22.9 & 22.4 & 22.7 & 10.0 & 20.0 & 20.0 & 19.9 & 18.9 & 22.8 & 22.4 & 10.0 & 18.0 & 18.0 & 18.0 & 18.0 & 18.0 & 18.0 & 18.7 & 18.0 & 18.7 & 18.7 & 18.7 & 18.7 & 18.7 & 18.7 \\   & Max 

**Insufficiency 4. Medical-specific models need to enhance their instruction tuning:** Interestingly, medical-specific models significantly underperform compared to general models, despite being trained and fine-tuned directly on relevant medical data. Specifically, LLaVA-Med is fine-tuned from the LLaVA model series in the medical field, but its performance is even worse than LLAVA-V1.5-7B. The primary reason for the poor performance of these medical-specific models is their inability to follow instructions correctly and their failure to understand or answer medical-related questions accurately. Detailed analysis can be found in the case study and supplementary materials sections on medical model analysis. Among these, the best-performing medical-specific model is MedDr, which is fine-tuned from the InternVL series and successfully surpasses the InternVL-Chat-V1.5. Unlike other medical-specific models that derive instruction-tuning data from papers, online sources, and books, MedDr builds its dataset based on high-quality medical image classification datasets. This result suggests that the quality of currently available medical instruction tuning datasets on the internet needs improvement and highlights the effectiveness of MedDr's dataset construction strategy, serving as a valuable reference for future medical-specific models.

**Insufficiency 5. The performance of most LVLMs on multiple-choice questions needs improvement:** Based on our tests, none of the models can totally match the correct answers (they always miss or over-select), so we adopt a relatively loose evaluation method for multiple-choice questions: using multi-choice hit rate (\(_{}\)) and recall rate (\(_{}\)). The experimental results are shown in Figure 5. Using this method, we found that most models have an accuracy rate of around 40%-50% and a recall rate of around 40%-60%. Surprisingly, InternVL-Chat-V1.5 and Qwen-VL-Max performed well in single-choice questions but showed very poor recall and accuracy rates in multiple-choice questions. In contrast, Qwen-VL-Chat and CogVLM-Chat, which performed relatively poorly in single-choice questions, achieved very high recall rates and moderate accuracy rates in multiple-choice questions, especially CogVLM-Chat with over 90% recall rate. Nonetheless, even with this less strict evaluation method, all models had accuracy rates below 55%, indicating that there is still significant room for improvement in answering multiple-choice questions.

#### Case Study

We further analyze the results by requiring the models to output content beyond the provided options and explain their reasoning process. This approach helps us better understand the causes of errors. Through detailed testing and analysis, we identify 5 typical errors present in the LVLMs:

**Question misunderstanding:** This occurs when the model incorrectly understands the purpose of the question, leading to an inability to provide a correct response. As shown in Figure 6A, the model is asked to answer a multiple-choice question, but it describes the problem or repeats the options rather than choosing an option.

**Perceptual Error:** These errors occur when there is a mislocation or misrecognition of image content. This means that the model's understanding or interpretation of the visual content is incorrect, leading to an inaccurate response. As shown in Figure 6B, the model mistakenly identifies the esophagus as the spine, suggesting that while the model can locate the target on the image (The annotated esophagus is very close to the spine), it makes an error in perceiving the masked content.

Figure 6: Three examples of error cases. **A:** Question misunderstanding. **B:** Perceptual Errors. **C:** Lack of Knowledge. More studies can be found in the appendix.

**Lack of knowledge:** While the model can recognize text and images, it makes errors in specific areas that require specific knowledge, indicating a deficiency in relevant training or fine-tuning in those areas. For example, in Figure 6C, the model incorrectly identifies the staining method as Ziehl-Neelsen and misrecognizes the blue-stained structure as Mycobacterium tuberculosis, where it is actually a white blood cell stained with Giemsa or Wright stain. This error indicates the model's lack of knowledge in experimental medicine.

**Irrelevant Responses:** This error indicates the model fails to generate a readable answer, which is easily found in medical-specific models like RadFM. Examples are listed in the appendix.

**Reject to Answer:** Some models, especially proprietary LVLMs like GPT-4V, GPT-4o, Gemini 1.0, and Gemini 1.5, commonly refuse to provide an answer due to policy reasons, because safety is crucial according to the commercial rules and regulations. Many potentially risky responses are declined to ensure compliance with guidelines. Those models' strict adherence to safety protocols and ethical standards limits response capabilities in certain domains.

## Conclusion

The development of GMAI-MMBench as a benchmark for evaluating LVLMs' capabilities represents a significant advancement in the pursuit of general medical AI. GMAI-MMBench epitomizes the expertise of skilled medical professionals, serving as a pivotal guide for advancing large models toward GMAI by testing the limits of current LVLMs. Owing to the extensive and diverse source of GMAI-MMBench, which comprises medical datasets annotated by professional healthcare providers worldwide, this benchmark can comprehensively evaluate the model's capability across various specific aspects. In this way, GMAI-MMBench can guide the model development at a more fine-grained level, accelerating the development of robust and reliable GMAI systems. Moreover, this benchmark supports the advancement of interactive multimodal medical models by providing more perceptual modes and annotations that are commonly used by physicians in clinical practice, thereby creating a framework for their evaluation and improvement.

However, GMAI-MMBench, like all benchmarks, has its limitations. The manual curation process, despite being thorough, might introduce biases, and focusing solely on medical subjects may not fully meet the criteria for general medical AI as defined. Nevertheless, we assert that high performance on GMAI-MMBench is essential for demonstrating the extensive subject knowledge and expert-level reasoning skills required for general medical AI. Looking ahead, we intend to integrate human evaluations into GMAI-MMBench. This addition will offer a more grounded comparison between model capabilities and expert performance, providing insights into how close current AI systems are achieving general medical AI in the medical field.