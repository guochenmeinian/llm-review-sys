# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

ensuring accurate retrieval of overfitted prototypes specific to individual tasks. These overfitted prototypes serve as the _ground truth_ representations for each task. Secondly, a task-guided diffusion process is implemented within the prototype space, enabling meta-learning of a generative process that smoothly transitions prototypes from their vanilla form to the overfitted state. ProtoDiff generates task-specific prototypes during the meta-test stage by conditioning random noise on the limited samples available for the given new task. Finally, we propose residual prototype learning, which significantly accelerates training and further enhances the performance of ProtoDiff by leveraging the sparsity of residual prototypes. The computational graph of ProtoDiff, illustrated in Figure 1, showcases the sequential steps involved in the forward diffusion process on the prototype and the generative diffusion process. The resulting generated prototype, denoted as \(_{0}\), is utilized for query prediction. By incorporating probabilistic and task-guided prototypes, ProtoDiff balances adaptability and informativeness, positioning it as a promising approach for augmenting few-shot learning in prototype-based meta-learning models.

To validate the effectiveness of ProtoDiff, we conduct comprehensive experiments on three distinct few-shot learning scenarios: within-domain, cross-domain, and few-task few-shot learning. Our findings reveal that ProtoDiff significantly outperforms state-of-the-art prototype-based meta-learning models, underscoring the potential of task-guided diffusion to boost few-shot learning performance. Furthermore, we provide a detailed analysis of the diffusion mechanism employed in ProtoDiff, showcasing its ability to capture the underlying data structure better and improve generalization. This thorough investigation highlights the strengths of our approach, demonstrating its potential to offer a more effective solution for few-shot learning tasks in various applications and settings.

## 2 Preliminaries

Before detailing our ProtoDiff methodology, we first present the relevant background on few-shot classification, the prototypical network, and diffusion models.

**Few-shot classification.** We define the N-way K-shot classification problem, which consists of support sets \(\) and a query set \(\). Each task \(^{i}\), also known as an episode, represents a classification problem sampled from a task distribution \(p()\). The _way_ of an episode denotes the number of classes within the support sets, while the _shot_ refers to the number of examples per class. Tasks are created from a dataset by randomly selecting a subset of classes, sampling points from these classes, and subsequently dividing the points into support and query sets. The episodic optimization approach [(()]) trains the model iteratively, performing one episode update at a time.

**Prototypical network.** We develop our method based on the prototypical network (ProtoNet) by Snell _et al._[(42)]. Specifically, the ProtoNet leverages a non-parametric classifier that assigns a query point to the class having the nearest prototype in the learned embedding space. The prototype \(^{c}\) of an object class \(c\) is obtained by: \(^{c}{=}_{k}f_{}(^{c,k})\), where \(f_{}(^{c,k})\) is the feature embedding of the support sample \(^{c,k}\), which is usually obtained by a convolutional neural network. For each query sample \(^{q}\), the distribution over classes is calculated based on the softmax over distances to the prototypes of all classes in the embedding space:

\[p(_{n}^{q}=c|^{q})=(^{q}), ^{c}))}{_{c^{}}(-d(f_{}(^{q}),^{c^{}}))},\] (1)

Figure 1: **Computational graph of ProtoDiff. In the initial stage, the computation of the vanilla prototype \(}\) is performed based on the support set \(^{i}\). Subsequently, through the process of diffusion sampling, the diffused prototype \(_{t-1}\) is derived from the combination of \(_{t}\) and \(}\). Finally, the prediction \(\) for the query set \(^{i}\) is generated by utilizing the diffused prototype \(_{0}\) in conjunction with the query set \(\). The diffusion forward and sampling processes are indicated by dashed and solid arrows within the gray rectangular area.**

where \(^{q}\) denotes a random one-hot vector, with \(^{q}_{n}\) indicating its \(n\)-th element, and \(d(,)\) is some (Euclidean) distance function. Due to its non-parametric nature, the ProtoNet enjoys high flexibility and efficiency, achieving considerable success in few-shot learning. To avoid confusion, we omit the superscript \(c\) for the prototype \(\) in this subsection.

**Diffusion model.** In denoising diffusion probabilistic models , a forward diffusion process, \(q(_{t}|_{t-1})\), is characterized as a Markov chain that progressively introduces Gaussian noise at each time step \(t\), beginning with a clean image \(_{0} q(_{0})\). The subsequent forward diffusion process is formulated as follows:

\[q(_{T}|_{0}):=_{t=1}^{T}q(_{t}|_{t-1}), q(_{t}|_{t-1}):=(_{t};}_ {t-1},_{t}),\] (2)

where \(\{\}_{t=0}^{T}\) is a variance schedule. By defining \(_{t}{:=}1-_{t}\) and \(}{:=}_{s=1}^{t}_{s}\), the forward diffused sample at time step \(t\), denoted as \(_{t}\), can be generated in a single step as follows:

\[_{t}=_{t}}_{0}+_{t}},(,).\] (3)

Since the reverse of the forward step, \(q(_{t-1}|_{t})\), is computationally infeasible, the model learns to maximize the variational lower bound using parameterized Gaussian transitions, \(p_{}(_{t-1}|_{t})\), where the parameter is denoted as \(\). Consequently, the reverse process is approximated as a Markov chain with the learned mean and fixed variance, starting from a random noise \(_{T}(_{T};,)\):

\[p_{}(_{0:T}):=p_{}(_{T})_{t=1}^{T}p_{}( {x}_{t-1}|_{t}),\] (4)

where

\[p_{}(_{t-1}|_{t}):=(_{t-1};_{ }(_{t},t),_{t}^{2}),_{}(_{t},t):=}}_{t}-}{_{t}}}_{}(_{t},t).\] (5)

Here, \(_{}(_{t},t)\) is the diffusion model trained by optimizing the following objective function:

\[_{}=_{t,_{0},}\|-_{}(_{t}}_{0}+_{t}},t)\|^{2}.\] (6)

Upon completing the optimization, the learned score function is integrated into the generative (or reverse) diffusion process. To sample from \(p_{}(_{t-1}|_{t})\), one can perform the following:

\[_{t-1}=_{}(_{t},t)+_{t}= {}}_{t}-}{_{t}}}_{}(_{t},t)+_{t}.\] (7)

In the case of conditional diffusion models , the diffusion model \(_{}\) in equations (\(\)) and (\(\)) is substituted with \(_{}(},_{t}}_{0}+ _{t}},t)\), where \(}\) represents the corresponding conditions, e.g., other images, languages, and sounds, etc. Consequently, the matched conditions strictly regulate the sample generation in a supervised manner, ensuring minimal changes to the image content. Dhariwal and Nichol  suggested classifier-guided image translation to generate the images of the specific classes along with a pre-trained classifier. Taking inspiration from conditional diffusion models that excel at generating specific images using additional information, we introduce the task-guided diffusion model that generates prototypes of specific classes by taking into account contextual information from various few-shot tasks. Note that our task-guided diffusion model operates not on an image \(\), but on the prototype \(\).

## 3 Methodology

This section outlines our approach to training prototypical networks via task-guided diffusion. We begin by explaining how to obtain task-specific overfitted prototypes in Section 3.1. Next, we introduce the task-guided diffusion method for obtaining diffused prototypes in Section 3.2. In the same section, we also introduce residual prototype learning, which accelerates training. Figure 2 visualizes the diffusion process of diffused prototypes by our ProtoDiff.

### Per-task prototype overfitting

Our approach utilizes a modified diffusion process as its foundation. Firstly, a meta-learner, denoted by \(f_{}(^{i}){=}f(^{i},)\), is trained on the complete set of tasks, \(\). Here, \(\) corresponds to the learned weights of the model over the entire task set. Subsequently, fine-tuning is performed on each individual task \(^{i}\) to obtain task-specific overfitted prototypes denoted by \(^{i,*}{=}f^{i}(^{i},^{i})\). This involves running the meta-learner \(f_{}\) on the support set \(^{i}\) and a query set \(^{i}\) of the task \(^{i}\).

\[^{i}=-_{(,)_{i}}^{_{i}}_{}(f(^{i},^{q^{i}},), ^{q^{i}}),\] (8)

where \(_{}\) is the cross-entropy loss minimized in the meta-learner training. The support set \(^{i}\) and query set \(^{i}{=}\{^{q^{i}},^{q^{i}}\}\) correspond to the data used for fine-tuning task \(^{i}\). During fine-tuning, we obtain the \(^{i}\) through several iterations of gradient descent. We illustrate the overall framework of the per-task prototype learning in the appendix.

Our proposed method, ProtoDiff, for few-shot learning, relies on acquiring task-specific overfitted prototypes that can be considered as the "optima" prototypes due to their high confidence in final predictions, approaching a value of 1. To achieve this, we employ fine-tuning of the meta-learner and extract task-specific information that surpasses the accuracy and reliability of generic prototypes used in the meta-training stage. However, accessing the query set in the meta-test stage is not feasible. Hence, we need to rely on the vanilla prototype to meta-learn the process of generating the overfitted prototype during the meta-training stage. In the forthcoming section, we will introduce the task-guided diffusion model, which facilitates the learning process transitioning from the vanilla prototype to the overfitted prototype.

### Task-guided diffusion

Our generative overfitted prototype is based on diffusion , a robust framework for modeling the prototypes instead of images. The length of the diffusion process \(T\) determines the number of forward passes needed to generate a new prototype, rather than the dimensionality of the data. Our model uses diffusion to progressively denoise the overfitted prototype \(^{*}\).

**Meta-training phase.** Diffusion models can be configured to predict either the signal or the noise when presented with a noisy input . Previous research in the image domain has suggested that noise prediction is superior to signal prediction. However, we discovered empirically that signal prediction performs better than noise prediction in our experiments, so we parameterized the diffusion model to output the prototype.

Figure 3: **ProtoDiff illustration.** We first obtain an overfitted prototype by the per-task prototype overfitting. We then add noise to the overfitted prototype, which inputs the diffusion forward process. The input of task-guided diffusion includes the vanilla prototype \(}\) and a random time step \(t\). The resulting output is the diffused prototype \(_{t}\).

Figure 2: **Visualization of the diffusion process.** ProtoDiff randomly selects certain areas to predict during the diffusion process, with the lowest probability at the beginning time step. As time progresses, the prototype gradually aggregates towards the _dog_, with the highest probability at t=0.

The generative diffusion process is devised to reconstruct the overfitted prototype \(^{*}\) by iteratively operating on a random noise vector \(_{T}(,)\), which possesses the same dimensions as \(^{*}\). This process continues with \(_{t}\), where \(t\) denotes the number of diffusion steps. Consequently, the reconstructed \(_{0}\) should exhibit proximity to \(^{*}\) for a given task.

Specifically, during the forward diffusion process at time step \(t\), we obtain the noised overfitted prototype \(}_{t}\). Subsequently, we input the noised prototype \(}_{t}\), the vanilla prototype \(}\), and the time step \(t\) into the task-guided diffusion. This yields the diffused prototype \(_{t}\), which then allows us to predict the final results of the query set using Equation ( ). For each task, our task-guided diffusion entails two components: the variational lower bound \(_{}\) for the diffused prototype, and the cross-entropy loss \(_{}\).

The objective is to minimize the simplified variational lower bound, which involves predicting the denoised overfitted prototype:

\[_{}=|^{*}-_{}(_{t}}^{*}+_{t}},},t)|^{2},\] (9)

Here, \(_{}(,)\) is implemented by the transformer model  operating on prototype tokens from \(^{*}\), \(}\), and the time step.

By utilizing equation ( ), we derive the final prediction \(}^{q}\) using the diffused prototype \(_{t}\). The ultimate objective is expressed as follows:

\[=_{(,)}^{|Q|} -q(_{t}|_{t+1},})  p(^{q}|^{q},_{t})+|^{* }-_{}(_{t}}^{*}+_{t}},},t)|^{2},\] (10)

where \(\) represents a hyperparameter, and \(|Q|\) denotes the query size.

To prepare the two input prototypes \(^{*}\) and \(}\) for processing by the transformer, we assign each position token inspired by . We also provide the scalar input diffusion timestep \(t\) as individual tokens to the transformer. To represent each scalar as a vector, we use a frequency-based encoding scheme . Our transformer architecture is based on GPT-2 . The decoder in the final layer of our ProtoDiff maps the transformer's output to the diffused prototype. Note that only the output tokens for the noised overfitted prototypes \(}_{t}\) are decoded to predictions. The overall meta-training phase of ProtoDiff is shown in Figure 3.

**Meta-test phase.** We can not obtain the overfitted prototype during the meta-test phase since we can not access the query set. Thus, diffusion sampling begins by feeding-in Gaussian noise \(_{T}\) as the \(}\) input and gradually denoising it. Specifically, to handle a new task \(\{,\}\), we first compute the vanilla prototype \(}\) using the support set \(\). We then randomly sample noise \(\) from \((,)\). We input both \(}\)and \(\) to the learned task-guided diffusion model to obtain the diffused prototype \(_{T-1}{=}_{}(_{T},},T)\). After \(T\) iterations, the final diffused prototype can be obtained as \(_{0}=_{}(_{1},,t_{0})\). Once the final diffused prototype \(_{0}\) is obtained, we calculate the final prediction \(}^{q}\) using equation ( ). Figure 4 illustrates sampling in the meta-test stage. We also provide the detailed algorithm of the meta-training and meta-test phase in the appendix.

**Residual prototype learning.** To further enhance the performance of ProtoDiff, we introduce a residual prototype learning mechanism in our framework. We observe that the differences between the overfitted prototype \(^{*}\) and its vanilla prototype \(}\) are not significant, as the vector \(^{*}-}\) contains many zeros. Therefore, we propose to predict the prototype update \(^{*}-}\) instead of directly predicting

Figure 4: **Diffusion sampling during meta-test. Sampling starts from a random noise \(_{T}\) and gradually denoises it to the diffused prototypes \(_{t}\). At each time step \(t\) is sampled by taking the \(_{t+1}\), vanilla prototypes \(}\), and time step \(t\) as inputs. The diffused prototype \(_{0}\) are used to predict the query set.**

the overfitted prototype \(^{*}\) itself. This approach also enables us to initialize ProtoDiff to perform the identity function by setting the decoder weights to zero. Moreover, we find that the global residual connection, combined with the identity initialization, significantly speeds up training. By utilizing this mechanism, we improve the performance of ProtoDiff in few-shot learning tasks.

## 4 Related Work

**Prototype-based meta-learning.** Prototype-based meta-learning is based on distance metrics and generally learns a shared or adaptive embedding space in which query images are accurately matched to support images for classification. It relies on the assumption that a common metric space is shared across related tasks and usually does not employ an explicit base learner for each task. By extending the matching network  to few-shot scenarios, Snell _et al._[1-2] constructed a prototype for each class by averaging the feature representations of samples from the class in the metric space. The classification matches the query samples to prototypes by computing their distances. To enhance the prototype representation, Allen  et al [2-] proposed an infinite mixture of prototypes to adaptively represent data distributions for each class, using multiple clusters instead of a single vector. Oreshkin  et al  proposed a task-dependent adaptive metric for few-shot learning and established prototype classes conditioned on a task representation encoded by a task embedding network. Yoon  et al  proposed a few-shot learning algorithm aided by a linear transformer that performs task-specific null-space projection of the network output. Graphical neural network-based models generalize the matching methods by learning the message propagation from the support set and transferring it to the query set . FEAT  was proposed to leverages samples from all categories within a task to generate a prototype, capitalizing on the intrinsic inter-class information to derive a more discriminative prototype. In contrast, our ProtoDiff, while using the overfitted prototype as supervision, only employs samples from a single category to generate the new prototype. This means we are not tapping into the potential informative context provided by samples from other categories. Additionally, our ProtoDiff employs the diffusion model to progressively generate the prototype, whereas FEAT does not utilize any generative model for prototype creation. Prototype-based methods have recently been improved in various ways . To the best of our knowledge, we are the first to propose a diffusion method to generate the prototype per task, rather than a deterministic function.

**Diffusion models.** These models belong to a category of neural generative models that utilize stochastic diffusion processes , much like those found in thermodynamics. In this framework, a gradual introduction of noise is applied to a sample of data. A neural model then learns to reverse the process by progressively removing noise from the sample. The model denoises an initial pure noise to obtain samples from the learned data distribution. Ho _et al._ and Song _et al._ contributed to advancements in image generation, while Dhariwal and Nichol  introduced classifier-guided diffusion for a conditioned generation. GLIDE later adapted this approach , enabling conditioning on textual CLIP representations. Classifier-free guidance  allows for conditioning with a balance between fidelity and diversity, resulting in improved performance . Since the guided diffusion model requires a large number of image-annotation pairs for training, Hu _et al._ propose self-guided diffusion models. Recently, Hyperdiffusion  was proposed in the weight space for generating implicit neural fields and 3D reconstruction. In this paper, we introduce ProtoDiff, a prototype-based meta-learning approach within a task-guided diffusion model that incrementally improves the prototype's expressiveness by utilizing a limited number of samples.

## 5 Experiments

In this section, we assess the efficacy of ProtoDiff in the context of three distinct few-shot learning scenarios: within-domain few-shot learning, cross-domain few-shot learning, and few-task few-shot learning. For the within-domain few-shot learning experiments, we apply our method to three specific datasets: _mini_Imagenet , _tiered_Imagenet , and ImageNet-800 . Regarding cross-domain few-shot learning, we utilize _mini_Imagenet  as the training domain, while testing is conducted on four distinct domains: CropDisease , EuroSAT [1-5], ISIC2018 , and ChestX . Furthermore, few-task few-shot learning  challenges the common assumption of having abundant tasks available during meta-training. To explore this scenario, we perform experiments on four few-task meta-learning challenges: _mini_Imagenet-S , ISIC , DermNet-S , and TabularMurris . For a comprehensive understanding of the datasets used in each setting, we provide detailed descriptions in the Appendix.

**Implementation details.** In our within-domain experiments, we utilize a Conv-4 and ResNet-12 backbone for _mini_Imagenet and _tiered_Imagenet. A ResNet-50 is used for ImageNet-800. We follow the approach described in  to achieve better performance and initially train a feature extractor on all the meta-training data without episodic training. Standard data augmentation techniques are applied, including random resized crop and horizontal flip. For our cross-domain experiments, we use a ResNet-10 backbone to extract image features, which is a common choice for cross-domain few-shot classification . The training configuration for this experiment is the same as the within-domain training. For few-task few-shot learning, we follow  using a network containing four convolutional blocks and a classifier layer. The average within-domain/ cross-domain, few-task few-shot classification accuracy (\(\%\), top-1) along with \(95\%\) confidence intervals are reported across all test query sets and tasks. Code available at: https://github.com/YDU-uwa/ProtoDiff.

**Benefit of ProtoDiff.** Table 1 compares our ProtoDiff with two baselines to demonstrate its effectiveness. The first Classifier-Baseline is a classification model trained on the entire label set using a classification loss and performing few-shot tasks with the cosine nearest-centroid method. The Meta-Baseline  consists of two stages. In the first stage, a classifier is trained on all base classes, and the last fully connected layer is removed to obtain the feature encoder. The second stage is meta-learning, where the classifier is optimized using episodic training. The comparison between the Baseline and Meta-Baseline highlights the importance of episodic training for few-shot learning. ProtoDiff consistently outperforms Meta-Baseline  by a large margin on all datasets and shots. The task-guided diffusion model employed in our ProtoDiff generates more informative prototypes, leading to improvements over deterministic prototypes.

**Benefit of diffusion model.** To confirm that the performance gain of our ProtoDiff model can be attributed to the diffusion model, we conducted the experiments only using MLP and transformers as non-generative models. We also compared it with two widely used generative models: the variational autoencoder (VAE)  and normalizing flows . VAE learns a low-dimensional representation of input data to generate new samples. Normalizing flows are more recent and learn a sequence of invertible transformations to map a simple prior distribution to the data distribution. We obtained the task-specific prototype in the meta-test stage, conditioned on the support samples and \((0,I)\), by first acquiring an overfitting

    &  &  &  \\ 
**Method** & **1-shot** & **5-shot** & **1-shot** & **5-shot** & **1-shot** & **5-shot** \\  Classifier-Baseline  & 58.91\(\)0.23 & 77.76\(\)0.17 & 68.07\(\)0.29 & 83.74\(\)0.18 & 86.07\(\)0.21 & 96.14\(\)0.08 \\ Meta-Baseline  & 63.17\(\)0.23 & 79.26\(\)0.17 & 68.62\(\)0.27 & 83.74\(\)0.18 & 89.70\(\)0.19 & 96.14\(\)0.08 \\
**ProtoDiff** & **66.63\(\)0.21** & **83.48\(\)0.15** & **72.95\(\)0.24** & **85.15\(\)0.18** & **92.13\(\)0.20** & **98.21\(\)0.08** \\   

Table 1: **Benefit of ProtoDiff on _mini_Imagenet, _tiered_Imagenet and ImageNet-800. The results of the Classifier-Baseline and Meta-Baseline are provided by Chen _et al._. ProtoDiff consistently achieves better performance than two baselines on all datasets and settings.**

    &  \\ 
**Method** & **1-shot** & **5-shot** \\  w/o generative model  & 63.17\(\)0.23 & 79.26\(\)0.17 \\  w/ MLP & 64.15\(\)0.21 & 80.23\(\)0.15 \\ w/ Transformer & 64.97\(\)0.21 & 81.28\(\)0.14 \\  w/ VAE & 64.45\(\)0.22 & 80.13\(\)0.15 \\ w/ Normalizing flows & 65.11\(\)0.22 & 81.96\(\)0.17 \\ w/ **Diffusion** & **66.63\(\)0.21** & **83.48\(\)0.15** \\   

Table 2: **Benefit of diffusion model over (non-)generative models on _mini_Imagenet.**

Figure 5: **Visualization of the different reconstructed overfitted prototypes using different generative models. The vanilla prototype focuses only on the overall features of the bird and overlooks the finer details of the _robin_. In contrast, the overfitted prototype can highlight specific features such as the tail and beak. While VAE and normalizing flow can generate prototypes of certain parts, the diffusion method can generate prototypes that more closely resemble the overfitted prototype. With residual prototype learning, ProtoDiff achieves better.**

ted prototype \(}\) and then using it as the ground truth to train VAE and normalizing flows in the meta-training stage. The experimental results reported in Table 2 show that the diffusion model outperforms all variants in terms of accuracy. Specifically, our diffusion model improves accuracy by 2.18% compared to VAE and 1.52% compared to normalizing flows. Furthermore, we reconstructed the overfitted prototype using different generative models in Figure 5. The vanilla prototype focuses solely on the bird's features and overlooks the finer details of the _robin_. In contrast, the overfitted prototype can emphasize specific features such as the tail and peak, resulting in better discrimination of the _robin_ from other bird species. While VAE and normalizing flows can generate prototypes of specific parts, our diffusion method can generate prototypes that more closely resemble the overfitted prototype, leading to improved performance. Our findings suggest that diffusion models hold promise as a more practical approach for few-shot learning, as they can model complex distributions and produce informative prototypes.

**Effect of the residual prototype.** The incorporation of residual prototypes in ProtoDiff presents a viable approach for reducing computational costs. This is attributable to the fact that the residual prototype exclusively encapsulates the disparity between the overfitted prototype and the original prototype, while the latter can be readily reused in each episode. Consequently, the calculation of the overfitted prototype is only necessitated once during the meta-training stage. Notably, the residual prototype often encompasses numerous zero values, thereby further expediting the training process. Table 6a illustrates the superior accuracy achieved by ProtoDiff with residual prototype learning in comparison to both the vanilla model and ProtoDiff without the residual prototype. The training progress of ProtoDiff is visually represented in Figure 5(b), demonstrating its accelerated training capabilities when compared to alternative methods. These results indicate the effectiveness of residual prototype learning in capturing task-specific information previously unaccounted for by the vanilla prototypes. Thus, the inclusion of residual prototype learning in ProtoDiff not only expedites the training process but also serves as a straightforward and effective approach to enhance the overall performance of ProtoDiff.

**Analysis of uncertainty.** In the process of estimating the log-likelihood of input data, the choice of the number of Monte Carlo samples becomes a critical hyperparameter. Typically, a larger number of samples tends to provide a more accurate approximation and better generalization to unforeseen prompts. In our experiments, we employed Meta-Baseline and FEAT  with ProtoDiff, incorporating the sampling of multiple prototypes in the final phase of the diffusion process. The results of these experiments are provided in Table 3. Notably, we observe a substantial performance boost with an increase in the number of samples. For instance, a sample size of 50 results in a significant enhancement in 1-shot accuracy, raising it from 66.63 for Meta-Baseline + ProtoDiff to 72.25 for FEAT + ProtoDiff.

**Visualization of the prototype adaptation process.** We undertake a qualitative analysis of the adaptation process of our ProtoDiff framework during the meta-test time. We employ a 3-way 5-shot setup on the _mini_ImageNet dataset for more comprehensive visualization of the adaptation process. To achieve this, we depict the vanilla prototypes, the diffused prototypes at various timesteps, and instances from the support and query sets. All examples are first normalized to a length of 1 and

    &  \\   & **5-shot** & **1-shot** & **5-shot** \\ 
1 & 66.63\(\)0.21 & 83.48\(\)0.15 & 68.97\(\)0.25 & 85.16\(\)0.17 \\
10 & 68.02\(\)0.17 & 84.95\(\)0.10 & 70.18\(\)0.15 & 86.53\(\)0.12 \\
20 & 68.91\(\)0.15 & 85.74\(\)0.10 & 61.07\(\)0.15 & 87.14\(\)0.12 \\
50 & 69.14\(\)0.15 & 86.12\(\)0.10 & 72.25\(\)0.15 & 88.32\(\)0.12 \\
100 & 69.13\(\)0.15 & 86.07\(\)0.10 & 72.21\(\)0.15 & 88.37\(\)0.12 \\
1000 & 69.11\(\)0.15 & 86.11\(\)0.10 & 72.12\(\)0.15 & 88.13\(\)0.12 \\   

Table 3: **Analysis of uncertainty.** Increasing the number of Monte Carlo samples provides a good improvement on _mini_Imagenet.

Figure 6: **Effect of residual prototype on _mini_Imagenet. Utilizing ProtoDiff with a residual prototype not only accelerates the training process but also surpasses the performance of both the vanilla prototype and the non-residual prototype.**

subsequently projected onto a two-dimensional space using t-SNE . Figure 7 displays the instance representations from both the query and support sets that are cross and circle, the vanilla prototypes that are denoted by a four-point star symbol, an the diffused prototypes that are denoted by stars. Arrows depict the adaptation process of the diffused prototypes, and each color corresponds to each respective class. Initially, the diffused prototypes are randomly allocated based on a Gaussian distribution, positioning them considerably from the optimal prototypes for each class. As the diffusion process continues, these diffused prototypes incrementally approach the optimal prototypes, resulting in more distinctive representations for different classes. In contrast, the vanilla prototypes lack distinguishability as they are computed using a single deterministic function based solely on the support set. This observation underscores the importance of the prototype adaptation process in attaining improved performance in few-shot classification tasks.

**Within-domain few-shot.** We evaluate our method on few-shot classification within domains, in which the training domain is consistent with the test domain. The results are reported in Table 4. In this comparison, we apply Set-Feat  with ProtoDiff to experiment since SetFeat is the current state-of-the-art model based on prototype-based meta-learning. Our ProtoDiff consistently achieves state-of-the-art performance on all datasets under various shots. The better performance confirms that ProtoDiff can achieve more informative and higher quality prototypes to perform better for few-shot learning within domains.

**Cross-domain few-shot.** We also evaluate our method in the cross-domain few-shot classification, where the training domain is different from test domain. Table 5 shows the evaluation of ProtoDiff on four datasets, with 5-way 1-shot and 5-way 5-shot settings. ProtoDiff also achieves competitive performance on all four cross-domain few-shot learning benchmarks for each setting. On EuroSAT , our model obtains high recognition accuracy under different shot configurations, outperforming the second best method, AFA , by a significant margin of \(2.82\%\) for 5-way 1-shot. Even on the most challenging ChestX , which has a considerable domain gap with _mini_ImageNet, our model achieves \(28.54\%\) accuracy in the 5-way 5-shot setting, surpassing the second-best HVM  by \(1.39\%\). The consistent improvement across all benchmarks and settings confirms the effectiveness of ProtoDiff for cross-domain few-shot learning.

**Few-task few-shot.** We evaluate ProtoDiff on the four different datasets under 5-way 1-shot and 5-way 5-shot in Table 6. In this comparison, we apply MLTI  with ProtoDiff to experiment. Our method achieves state-of-the-art performance on all four few-task meta-learning benchmarks under each setting. On _mini_Imagenet-S, our model achieves 44.15% under 1-shot, surpassing the

    &  &  \\ 
**Method** & **1-shot** & **5-shot** & **1-shot** & **5-shot** \\  TaphNet  & 61.65\(\)0.13 & 76.36\(\)0.10 & 63.08\(\)0.15 & 80.26\(\)0.11 \\ CTM  & 62.05\(\)0.05 & 78.63\(\)0.00 & 64.78\(\)0.11 & 81.05\(\)0.53 \\ MetaOptNet  & 62.64\(\)0.01 & 78.63\(\)0.00 & 65.81\(\)0.74 & 81.75\(\)0.53 \\ Meta-Baseline  & 63.17\(\)0.17 & 79.26\(\)0.01 & 68.62\(\)0.87 & 83.44\(\)0.11 \\ CAN  & 63.85\(\)0.07 & 63.24\(\)0.01 & 69.89\(\)0.15 & 63.23\(\)0.27 \\ Meta DeepBDC  & 67.34\(\)0.04 & 84.46\(\)0.23 & 72.99\(\)0.08 & 86.74\(\)0.33 \\ SUN  & 68.30\(\)0.03 & 83.25\(\)0.02 & 72.99\(\)0.08 & 86.74\(\)0.33 \\ SetFeat  & 68.32\(\)0.05 & 82.71\(\)0.04 & 73.63\(\)0.08 & 87.59\(\)0.57 \\  _This paper_ & **71.25\(\)**0.05 & **83.95\(\)**0.05 & **75.97\(\)**0.05 & **88.75\(\)**0.05 \\   

Table 4: **Within-domain few-shot comparison on _mini_Imagenet and _tiered_Imagenet. ProtoDiff performs well on all datasets.**

    &  &  &  &  \\ 
**Method** & **1-shot** & **5-shot** & **1-shot** & **5-shot** & **1-shot** & **5-shot** & **1-shot** & **5-shot** \\  ProtoNet  & 57.57\(\)0.51 & 79.72\(\)0.67 & 54.19\(\)0.57 & 73.29\(\)0.71 & 29.62\(\)0.32 & 39.57\(\)0.57 & 22.30\(\)0.25 & 24.05\(\)1.01 \\ GNN  & 57.19\(\)0.50 & 83.12\(\)0.00 & 54.61\(\)0.50 & 78.69\(\)0.40 & 30.14\(\)0.30 & 42.54\(\)0.40 & 21.94\(\)0.20 & 23.87\(\)0.20 \\ AFA  & 67.61\(\)0.50 & 88.06\(\)0.30 & 63.12\(\)0.50 & 85.58\(\)0.40 & 33.21\(\)0.30 & **46.01\(\)**0.40 & 22.92\(\)0.20 & 25.02\(\)0.20 \\ ATA  & 67.47\(\)0.50 & **90.59\(\)**0.30 & 61.35\(\)0.50 & 83.75\(\)0.40 & 33.21\(\)0.30 & 44.91\(\)0.40 & 22.10\(\)0.30 & 24.32\(\)0.20 \\ HVM  & 65.13\(\)0.45 & 87.65\(\)0.31 & 61.97\(\)0.34 & 74.88\(\)0.45 & 33.87\(\)0.55 & 42.05\(\)0.34 & **22.94\(\)**0.67 & 27.15\(\)0.45 \\  _This paper_ & **68.93\(\)**0.31 & **90.15\(\)**0.31 & **65.93\(\)**0.34 & **87.25\(\)**0.35 & **34.97\(\)**0.33 & **45.65\(\)**0.31 & **23.01\(\)**0.05 & **28.54\(\)**0.01 \\   

Table 5: **Cross-domain few-shot comparison on four benchmarks. ProtoDiff is a consistent top-performer.**

Figure 7: **t-SNE visualization of ProtoDiff adaptation process on 3-way 5-shot task.**

best method MetaModulation , by a margin of 1.54%. Even on the most challenging DermNet-S, which forms the largest dermatology dataset, our model delivers 51.53% on the 5-way 1-shot setting. The consistent improvements on all benchmarks under various configurations confirm that our approach is also effective for few-task meta-learning.

**Limitations.** Naturally, our proposal also comes with limitations. Firstly, the diffusion model employed in our approach necessitates a substantial number of timesteps to sample the diffused prototype during the meta-test stage. Although we somewhat alleviate this issue by utilizing DDIM  sampling, it still requires more computational resources than vanilla prototypes. Secondly, obtaining the overfitted prototype involves fine-tuning, which inevitably leads to increased training time. While this fine-tuning step contributes to the effectiveness of our model, it comes with an additional computational cost. In meta-training and meta-testing ProtoDiff is slower by factors of ProtoNet \(5\) and \(15\) in terms of wall-clock times per task. More detailed time numbers are provided in the Appendix. As part of future work, we will investigate and address these limitations to further enhance the applicability and efficiency of our approach.

## 6 Conclusion

We proposed ProtoDiff, a novel approach to prototype-based meta-learning that utilizes a task-guided diffusion model during the meta-training phase to generate efficient class representations. We addressed the limitations of estimating a deterministic prototype from a limited number of examples by optimizing a set of prototypes to accurately represent individual tasks and training a task-guided diffusion process to model each task's underlying distribution. Our approach considers both probabilistic and task-guided prototypes, enabling efficient adaptation to new tasks while maintaining the informativeness and scalability of prototypical networks. Extensive experiments on three distinct few-shot learning scenarios: within-domain, cross-domain, and few-task few-shot learning validated the effectiveness of ProtoDiff. Our results demonstrated significant improvements in classification accuracy compared to state-of-the-art meta-learning techniques, highlighting the potential of task-guided diffusion in augmenting few-shot learning and advancing meta-learning.