# D2R2: Diffusion-based Representation with Random Distance Matching for Tabular Few-shot Learning

Ruoxue Liu

HKUST

rliuaj@connect.ust.hk

&Linjiajie Fang

HKUST

lfangad@connect.ust.hk

&Wenjia Wang

HKUST (GZ) and HKUST

wenjiawang@ust.hk

&Bing-Yi Jing

SUSTech

jingby@sustech.edu.cn

Equal contribution. \(\) Corresponding authors.

###### Abstract

Tabular data is widely utilized in a wide range of real-world applications. The challenge of few-shot learning with tabular data stands as a crucial problem in both industry and academia, due to the high cost or even impossibility of annotating additional samples. However, the inherent heterogeneity of tabular features, combined with the scarcity of labeled data, presents a significant challenge in tabular few-shot classification. In this paper, we propose a novel approach named Diffusion-based Representation with Random Distance matching (D2R2) for tabular few-shot learning. D2R2 leverages the powerful expression ability of diffusion models to extract essential semantic knowledge crucial for denoising process. This semantic knowledge proves beneficial in few-shot downstream tasks. During the training process of our designed diffusion model, we introduce a random distance matching to preserve distance information in the embeddings, thereby improving effectiveness for classification. During the classification stage, we introduce an instance-wise iterative prototype scheme to improve performance by accommodating the multimodality of embeddings and increasing clustering robustness. Our experiments reveal the significant efficacy of D2R2 across various tabular few-shot learning benchmarks, demonstrating its state-of-the-art performance in this field.

## 1 Introduction

Learning with a large set of data with only a few labeled samples is an essential requirement for industry and academia, primarily due to the high cost of annotating samples. However, accurately classifying new data with a scantily labeled training set and an unlabeled training set poses a formidable challenge, as the effectiveness of statistical and modern deep learning systems in supervised learning heavily relies on the large size of the labeled set . This underscores the need for research in few-shot learning in situations where labeled data is scarce. For example, in one-shot learning, each class of the training set contains only one labeled sample, and the goal is to classify new test samples. While few-shot learning has garnered considerable attention in computer vision (CV)  and natural language processing (NLP) , it remains relatively under-explored in the field of tabular data. Nevertheless, few-shot learning holds great importance in the context of tabular data, as limited labeled tabular data is inherently common in many real-world applications, such as fraud detection , disease diagnosis , and social science . However, modeling with limited labeled tabular data presents three significant challenges.

The first challenge is due to the scarcity of labeled training samples, which leads to a deficient understanding of the test class. In this case, semantic knowledge, encompassing general and low-frequency information not tied to any particular task, can be highly beneficial in few-shot learning . This is because representing a class with limited labeled samples inherently involves ambiguity, necessitating the use of semantic knowledge acquired from unlabeled ones to refine the class definition and address this ambiguity. Although for CV  and NLP , semantic knowledge can be effectively derived based on the learned spatial structure patterns between pixels or tokens, it is more challenging to extract semantic knowledge for tabular data, since they typically lack local relationships between columns.

The second challenge arises from the diverse characteristics of tabular features. Tabular data comprises numerical and categorical features. Numerical features contain ordinal values but have multiple modes, while categorical features present distinct and incomparable values. Many existing statistical and deep neural network models struggle to effectively handle these mixed data types. This challenge underscores the importance of simultaneously modeling continuous and categorical features.

The third challenge pertains to the utilization of prototype classification with tabular data. Numerous approaches suggest employing average embeddings of the labeled support set for classification . However, the limited labeled support samples lead to large variance and imprecision of the prototypes, highlighting the need for more information from unlabeled samples. Additionally, embeddings from the same class may exhibit multimodal behavior (see Figure 3 for examples), which means that using the average embeddings of one class could result in inaccurate classification results.

In addressing the aforementioned challenges of few-shot learning, significant efforts have been made over the past decade. One approach employs a meta-learning scheme. A common method is to generate a pseudo-label for each unlabeled data and then train the model using pseudo-label . Other approaches utilize self-generated tasks, such as CACTUs , UMTRA  and Meta-GMVAE . However, despite the effectiveness of prior works on image datasets, we find that applying each method to the tabular domain is highly non-trivial. These methods often assume uniformity in feature types and rely on strong spatial and sequential relationships among features, which do not hold for tabular data. For instance, augmentation techniques are readily applicable to images, where spatial relationships between pixels can be leveraged, as shown by UMTRA . However, such techniques are difficult to apply to tabular data, which lacks these structural patterns . STUNT  is designed for tabular data using a proxy-based approach. However, this methods may struggle to acquire useful semantic knowledge due to the disparities between the constructed tasks and the target task, leading to diminished performance for testing. Moreover, it applies the same methods to both numerical and categorical features, neglecting the unique information in different types of data. This oversight hampers the model's effectiveness. Recent studies have showed that self-supervised learning can leverage unlabeled datasets to acquire semantic knowledge and transferable representations of images  and languages . However, these approaches heavily rely on augmentation schemes. For tabular data, the heterogeneous feature types pose challenges for augmentation, and only a few methods have been proposed for tabular data. State-of-the-art methods along this line include VIME , SCARF , SubTab .

In this work, we propose a novel framework named Diffusion-based Representation with Random Distance Matching (D2R2) for tabular few-shot learning. D2R2 extracts representations by training a designed conditional diffusion model and aligning the distances from various random projection spaces. Subsequently, we introduce the instance-wise iterative prototype to conduct few-shot classification, which addresses the multimodal behavior of embeddings and enables the creation of a robust classifier. Finally, we design an unsupervised validation scheme to address the absence of the labeled validation set for hyperparameter selection. Different from meta-learning and self-supervised learning methods, D2R2 does not relies on self-generated tasks or augmentation schemes, but instead uses a general training process. The D2R2 framework offers the following advantages:

Considering the first challenge mentioned earlier, we avoid to rely on proxy methods or augmentation methods, which are limited by the lack of local relationships in tabular data. Instead, we create an information bottleneck for extracting semantic knowledge, named D2R2, which leverages the strong expressiveness of the diffusion model and distance information from pairwise comparison. Firstly, we modify the diffusion model to serve as a representation extractor. The powerful expressiveness of diffusion model enables the extraction of semantic knowledge crucial to denoising. And the information bottleneck induced by conditional information, obtains low-frequency information over details. This approach outperforms alternative representation learning and meta-learning methods, as demonstrated by our experiments. Moreover, the representations exhibit robustness due to the introduction of designed diffusion noise schedule and subsequent denoising process, which reinforces their stability against tabular column perturbations.

Nevertheless, relying solely on diffusion-based representations may not provide sufficient distance information to build a subsequent prototype classifier, which classifies test samples based on their distance between prototypes. Hence, we introduce a random distance matching (RDM) loss during the training of the diffusion model to obtain the distance information. One intuition of applying RDM is that if the class structure can be characterized by certain distance information, the embeddings should preserve such distance structure. Another intuition is that if two samples belong to the same class in the ground truth, their embeddings should appear similar from any perspective. Random distance matching achieves these ideas by projecting data into diverse random spaces, which encompass class structure information. Consequently, if two data points are close in multiple random spaces, they are highly likely to belong to the same class. Besides, two distinct Random Distance matching are designed for continuous and discrete features based on their characteristics, respectively. In this way, we can simultaneously model numerical and categorical features within one model, addressing the second challenge mentioned earlier. Overall, our newly designed D2R2 training process not only extracts high-quality semantic knowledge for few-shot learning but also reveals distance information for prototype classification, while remaining adaptive to mixture types of tabular data.

Moreover, during the classification phase, we predict the class of a test sample to be the same as the class of the nearest prototype. Given that the embedding of a single class may demonstrate multimodality, we introduce a novel instance-wise iterative prototype. This approach tackles the issue of multimodal behavior by creating several prototypes within one class and improving the robustness of prototypes by iterative refinement, resolving the third challenge previously mentioned.

Lastly, to address the absence of a labeled validation set in unsupervised learning for hyperparameter selection, we devise a validation scheme by generating pseudo-labels for the unlabeled dataset using soft k-means clustering of raw features.

Our contribution is summarized as follows:

* To the best of our knowledge, we are the first to propose a specifically designed diffusion method to learn semantic knowledge for tabular data.
* We propose an innovative framework, D2R2, to extract representations in tabular few-shot learning, which is built upon the designed diffusion process and random distance matching. D2R2 not only captures high-quality semantic knowledge, but also incorporates distance information for the subsequent prototype classifier. Moreover, it adapts well to mixture types of tabular data.
* To further improve few-shot classification performance, we introduce a novel classifier with instance-wise iteration prototypes. This classifier is able to construct highly accurate and stable prototypes, while also revealing the multimodal behavior of a single class.
* We conduct extensive experiments to evaluate our framework for tabular few-shot learning, comparing our method with 15 state-of-the-art literature baselines across nine datasets. Among various tabular datasets, D2R2 outperforms other baselines by a significant margin.

## 2 Related work

**Supervised and Semi-supervised learning.** Some supervised classifiers have strong ability to learn from limited samples. Methods along this line include CatBoost , TabPFN , a transformer-based network designed to make predictions on a small tabular dataset; k-nearest neighbor classifier (kNN) , the nearest neighbor prototype classifier. However, they are inadequate in few-shot learning because they are not intended to learn the semantic knowledge. Semi-supervised learning frameworks are designed to improve model generalization by creating a strong connection between limited labeled samples and unlabeled samples, including Mean Teacher (MT) , Interpolation Consistency Training (ICT) , and Meta Pseudo Labels (MPL) . In semi-supervised learning, a model trains on labeled data and then predicts labels for unlabeled data. However, this method requires more labeled data than few-shot learning contexts.

**Few-shot meta-learning**. Few-shot learning aims to train models to adapt to downstream tasks with minimal labeled examples. This can be accomplished by employing meta-learning techniques on related tasks, enabling the acquisition of prior knowledge that can be utilized to solve new tasks . Existing deep few-shot methods can be divided into three main categories. First, optimization-based meta-learning methods [5; 15; 26; 29] devise a proficient optimization strategy that adapts to downstream tasks effectively. Second, metric-based methods [13; 30; 34; 51] concentrate on latent space to derive meaningful feature embeddings, and then make predictions based on the similarity between support and query embeddings. Third, data generative strategies emphasize creating more varied samples to train a more precise classifier. Most research on few-shot meta-learning focuses on NLP and CV tasks, such as CACTUs  and UMTRA , while a small subset tackles tabular few-shot learning. STUNT  meta-learns generalizable knowledge from self-generated tasks on an unlabeled tabular training set. However, these methods may generate ineffective semantic knowledge because of the gap between the self-generated tasks and the testing task.

**Self-supervised learning**. Our research addresses scenarios where an unlabeled training set and a small labeled support set (\(K\)-shot) are used to predict class labels for a test query set, as detailed in Section 3 of our paper. Self-supervised learning is particularly effective at developing robust representations from unlabeled data [8; 37]. These approaches focus on pre-training the representation by utilizing domain-specific inductive biases, like the spatial relationships in images. Notably, prior research has demonstrated the effectiveness of self-supervised learning in few-shot scenarios relative to meta-learning techniques . Chen et al.  added an autoencoder in the diffusion process and Yang et al. introduces a latent Denoising Autoencoder architecture where the learned representations are used for Denoising Autoencoder, but the Autoencoder reconstruction process is less suitable for tabular data as discussed in STUNT . Other works on self-supervised learning schemes rely on augmentation schemes. It is unclear how to extend such methods to the tabular domain due to the heterogeneous characteristics of tabular datasets. Moreover, for tabular data, multimodal and categorical features make augmentation difficult, and few effective methods have been proposed in tabular data setting. Recent state-of-the-art tabular learning augmentation techniques include masking cell: VIME , constructing subsets: SubTab , and contrastive learning: SCARF . However, they fail to deliver substantial performance enhancements for few-shot tabular learning in our experiments. Rather, we train the unlabeled dataset through an unsupervised diffusion framework that does not rely on the effect of augmentation.

## 3 Problem definition

In this paper, we explore few-shot learning for tabular classification.Tabular data refers to the dataset organized in tables, which is a structured format that presents information in rows and columns. Such data can be represented as \(D=\{_{i}\}_{i=1}^{n}^{d}\) consisting of \(n\) instances and \(d\) dimensional features. Each data instance \(_{i}=(x_{1}^{i},x_{i}^{i},...,x_{d}^{i})\) may or may not hold strong relationship among features. The features in tabular data typically vary, comprising both numerical and categorical labels.

Our study on tabular few-shot learning focus on a very typical scenario in this field, adhering to the definition outlined in STUNT . We have an unlabeled training set \(_{u}=\{_{i}^{u}\}_{i=1}^{N_{u}}\) and a limited labeled support set \(=\{(_{i}^{s},y_{i}^{s})\}_{i=1}^{N_{s}}\), \(_{i}^{s}^{d}\) and \(y_{i}^{s}\{1,2,...,C\}\) represent inputs and class labels, respectively. We assume that \(N_{u} N_{S}\). Our goal is to predict the class labels of testing query set \(=\{_{i}^{q}\}_{i=1}^{N_{}}\). In the \(N\)-way \(K\)-shot setting, the classification is conducted with \(N\) targeted classes and each class in the support set has \(K\) labeled samples. Such scenarios are common in critical applications like credit risk assessment  and diagnosing patients with rare diseases .

## 4 Methodology

In this section, we introduce the overall design of the framework Diffusion-based Representation learning with Random Distance matching (D2R2) in detail. In order to leverage the semantic knowledge extraction capabilities of diffusion models, D2R2 employs a conditional diffusion model on the unlabeled dataset to learn an embedding space (Section 4.1). In order to enhance the clustering ability of the embedding space, facilitating subsequent classification, we modify the training process of the diffusion model, in order to ensure that the distances between instances align across the embedding space and various projected spaces. (Section 4.2). Figure 1 illustrates aforementionedtraining process. In the classification stage, we construct a novel instance-wise iterative classifier to predict the testing samples on the learned embedding space (Section 4.3). Finally, due to the absence of a validation set for above unsupervised embedding space learning, we propose to use pseudo-label validation on the unlabeled dataset to select hyperparameters (Section 4.4). A summary of the training algorithm of D2R2 is presented in Appendix A.

### Diffusion-based representation Learning

Diffusion-based generative models [42; 17; 44; 45] are latent variable models that use a Markovian noising and parameterized denoising process to model the data distribution thus generate realistic samples. Details of diffusion models are provided in the appendix B. The diffusion process for a given sample \(_{0}\) is defined as \(_{t}=_{t}}_{0}+_{t }}\), where \((0,)\), \(_{t}=_{t^{}=1}^{t}(1-_{t}^{})\), \(_{t}\) is a pre-defined variance schedule, and the timestep \(t\) is known. The true noise is \((_{t},t,_{0})=1/_{t}}(_{t}-_{t}}_{0})\). Following DDPMs , the denoising model can be implemented by neural networks with learnable parameters \(\) by directly predicting the noise \(\):

\[_{}()=_{t,_{0}}||_{ }(_{t},t)-||_{2}^{2},\] (1)

Diffusion models can be extended to conditional generative models \(_{}(_{t},t,c)\) by inserting the conditional information \(c\) to generate specific class samples [45; 1].

We now present the diffusion-based representation learning part of D2R2. We employ the diffusion models in a distinct way different from generation. We modify the diffusion model and repurposing it to act as representation extractor. Specifically, we represent the trainable embedding function as \(z_{}:^{d}^{p}\), where \(z_{}(_{0})\) maps the noiseless input \(_{0}\) to the latent space of dimension \(p\). Formally, the reconstruction loss for the diffusion-based representation learning is

\[_{}(,)=_{t,_{0},_{t}}||_{}(_{t},t,z_{}(}_{0}))- ||_{2}^{2},\] (2)

where \(_{}\) is a trainable noise model, and \(}_{0}\) is an augmentation version of the noiseless data input \(_{0}\). In our method, we use Gaussian noise perturbation \(}_{0}=_{0}+(0,1)\). For numerical features we set \(_{i}\) to be the standard deviation of numerical features in \(_{u}\), and for categorical (one-hot) features, we use a fixed \(_{i}\) to smooth the one-hot indicator values.

Reasons that the designed diffusion model can extract semantic knowledge come from twofold. Firstly, the diffusion model with powerful expressiveness encodes the information needed for denoising. Specifically, in conditional diffusion models, the noise reconstruction loss \(_{t,_{0},_{t}}||_{}(_{t}, t,c)-||_{2}^{2}\) trains the noise prediction function \(_{}(_{t},t,c)\) to predict the true noise \((_{t},t,_{0})\) given the noisy

Figure 1: The diagram depicts the training process of the embedding space. Specifically, after a noise perturbation, each instance undergoes mapping via an embedding function \(z_{}\). The embedding is then incorporated into a conditional diffusion model for noise prediction. The parameters of the noise model \(_{}\) and the embedding function \(z_{}\) are concurrently optimized using the reconstruction loss. Additionally, the instance is subjected to mapping through a random linear projection into an alternate metric space. Two distinct Random projections are generated for numerical and categorical features, respectively. We align the distances in the embedding space and the random projection space using the RDM loss to ensure that the embedding function effectively preserves distance information, which is beneficial for downstream classification tasks.

sample \(_{t}\), the knowing \(t\) and the condition information \(c\). If \(c=_{0}\), we could expect \(_{}\) can almost perfectly recover \(\). By replacing the conditional information \(c\) by a function \(z_{}(_{0})\) that maps to an embedding space with lower dimension than \(_{0}\), we introduce an information bottleneck to the noise reconstruction process. This forces \(z_{}\) to extract effective information for denoising from \(_{0}\), leading to representation learning through the noise reconstruction loss (2).

From another perspective, it is pointed out the noise reconstruction loss (2) can be expressed equivalently as the weighted noisy score matching loss \(_{t,_{0},_{t}}[(t)||s_{}- _{t} p_{(t)}(_{t}|_{0})||_{2}^{2}]\), where the weights \((t)=^{2}(t)\) are determined by the noise scale \((1)<(2)<...<(T)\). The choice of noise scale \((t)\) controls the granularity of the embedding function. We focus on larger timesteps thus extract the low-frequency semantic information rather than details.

### Random distance matching

The acquired latent representation \(z_{}\) from equation (2) is utilized for subsequent classification tasks, which heavily rely on distance information between the clusters within the embedding space. Although \(z_{}(_{0})\) learned through conditional diffusion models already exhibits clustering properties  to a certain extent, the learned embeddings do not prioritize capturing distance information between class structures, potentially hindering the effectiveness for downstream classification tasks. Taking inspiration that representations can be learned by training neural networks to predict distances in a randomly projected space , we propose the random distance matching (RDM) loss to align pairwise distances between the embedding space and a randomly projected space \(^{r}\). Here we suppose that if two samples belong to the same class in the ground truth, their embeddings should be close to each other, which is reflected by the randomly projected features from any perspectives.

Specifically, we consider the random linear projections \(W^{r d}\) with each element \(w_{i,j}\) sampled \(i.i.d.\) from a fixed distribution. The RDM loss is as follows:

\[_{}()=_{_{0},^{ }_{0},W}||d(z_{}(_{0}),z_{}(^{}_{0}))-d (W_{0},W^{}_{0})||^{2},\] (3)

where \(_{0},^{}_{0}\) are any two unlabeled data points sampled from \(_{u}\) and \(d\) is a metric. Since the dimensions and the scales of the two matching spaces might not be the same, we choose \(d\) to be the cosine distance.

Furthermore, in order to handle hybrid tabular data types, which involve both numerical and categorical features, we sample random projections from various distributions according to the specific feature types. We consider samples with \(d_{}\) numerical features and \(d_{}\) categorical features. For numerical features, we sample a projection \(W_{}^{r_{1} d_{}}\) from symmetric uniform distribution: \(w_{i,j}_{i.i.d.}(-A,A)\); for categorical features, we sample a projection \(W_{}^{r_{2} d_{}}\) from Bernoulli distribution: \(w_{i,j}_{i.i.d.}(p)\). Considering both types of features, our random linear projection is defined as:

\[W_{0}:=(W_{}[_{0}]_{},W_ {}[_{0}]_{}),\] (4)

where \([_{0}]_{}\), \([_{0}]_{}\) are the numerical and categorical parts of \(_{0}\), respectively.

Overall, we define a novel diffusion-based representation learning loss as follows:

\[_{}(,)=_{}(, )+_{}(),\] (5)

where \(\) is a hyperparameter to balance the noise prediction loss and RDM loss. We train \(\) and \(\) to minimize \(_{}\) over the unlabeled dataset \(_{u}\). The trained embedding function \(z_{}(_{0})\) is used as the representation function for the downstream classification tasks. Loss function \(_{}\) considers learning of embeddings from two different angles. Diffusion models possess strong generative capabilities, compelling the reconstruction loss \(_{}(,)\) to ensure that the learned embedding \(z_{}\) captures high-quality semantic information, which significantly influences the data distribution. The random distance loss \(_{}()\) adjusts the embedding to accommodate the distances between data points in a random projection space, thereby improving the clustering capability of the embedding space, rendering it suitable for subsequent classification tasks.

### Instance-wise iterative prototype

Given the trained embedding function \(z_{}()\) and the labeled support set \(=\{(^{s}_{i},y^{s}_{i})\}_{i=1}^{N_{S}}\), we define the instance-wise prototype of support samples in the embedding space as \(c_{i}=z_{}(^{s}_{i}),\) where \(i=1,2,..,N_{s}\). We can predict the label of a query sample to be the same as the nearest prototype.

The rationale for employing instance-wise prototypes instead of center-specific prototypes , which averages the \(K\)-shot embeddings inside the \(j\)-th class as \(c_{j}=_{y^{*}_{i}=j}z_{}(^{s}_{i})\), is that embeddings from the same class may exhibit multimodality, while center-specific prototypes can only represent a uni-modal pattern, which might result in incorrect classification results (see Figure 2 for an illustration). On the other hand, in the few-shot learning scenarios, the scarcity of support samples brings large variance of prototypes, leading to unstable and imprecise classification outcomes. Drawing inspiration from the soft k-means algorithm , we leverage the weighted average of the query embeddings and support embeddings to create robust prototypes iteratively, referred to as the instance-wise iterative prototypes.

At the beginning, we initialize each prototype as the embedding of each support instance as \(c^{(0)}_{i}=c_{i}=z_{}(^{s}_{i})\), where \(i=1,2,..,N_{s}\). At the \(l\)-th iteration step, the probability that the query \(x\) belonging to the \(i\)-th prototype is:

\[p^{(l)}_{}(i|x;)=()-c^{(l )}_{i}||^{2}_{2}/)}{_{_{i^{}}}(-|| z_{}()-c^{(l)}_{i^{}})||^{2}_{2}/)},\] (6)

Then we update the prototypes at the \((l+1)\)-th iteration based on the weighted average of query embeddings:

\[c^{(l+1)}_{i}=_{i}+_{x}p^{(l)}_{}(i| ;) z_{}()}{1+_{ }p^{(l)}_{}(i|;)}.\] (7)

After the \(L\)-th iteration, we predict the class of a query sample \(\) to be the same class of the nearest prototype:

\[=_{y^{*}_{i}}p^{(L)}_{}(i|;).\] (8)

### Pseudo-label validation

One challenge in the proposed unsupervised learning lies in the lack of a label validation set for hyper-parameter tuning. For example, in 1-shot classification, the only one labeled sample is used for training, with no additional labeled samples available for validation. We tackle this problem by generating pseudo labels for the unlabeled dataset using soft k-means of raw features.

Formally, we sample a validation set \(_{}_{u}\). During each validation process, we randomly sample \(K^{}\) points \(\{_{k}\}_{k=1}^{K^{}}_{}\), which are regarded as pseudo support samples, forming \(K^{}\) distinct classes, i.e., \(_{}=\{(^{s}_{k},y^{s}_{k}=k)\}_{k=1}^{K^{ }}\). We regard the remaining unlabeled samples \(_{}=_{}_{}\) as pseudo query samples. Next, pseudo-labels are generated for \(_{}\) by applying instance-wise iterative prototype classification (Section 4.3) to the raw feature space, and those pseudo-labels serve as the "ground-truth" labels for the query set \(_{}\). Subsequently, we employ D2R2 on the validation set \(\{_{},_{}\}\) to predict the "ground-truth" labels of \(_{}\). We assess the validation performance to select hyperparameters for the training iteration.

Figure 2: An illustration of the rationale behind instance-wise prototype. (a): In 2-shot scenarios, shapes represent the ground-truth classes in the embedding space, while the gray-colored objects await classification. (b): If the embedding of the circle class is not unimodal, averaging the prototypes leads to erroneous center suggestions and fails to classify the embeddings. (c): Considering instance-wise prototypes, each prototype contributes to the classification of nearby embeddings without generating erroneous centers.

Experiments

### Experimental settings

For all the datasets, we randomly split 80% of the data for training and the remaining 20% for testing. As for \(N\)-way \(K\)-shot scenario, the support set is constructed by randomly selecting \(N K\) samples, with \(K\) samples from each of the \(N\) classes from the training sets. Additionally, 20% of the training data is utilized for validation and hyperparameter tuning. We use one-hot encoding for categorical features and min-max scaling for numerical features, except for income data, for which we use standardized scaling (see Appendix D.2 for details). All experiment settings of baselines and D2R2 are the same as STUNT  for fair comparison. Details are provided in Appendix D.32.

**Datasets**. We select nine datasets from the OpenML-CC18 benchmark [3; 6] to validate the performance of D2R2. Table 3 shows a summary of the datasets. Optdigits, mfeat-karhunen, diabetes contain and breast only numerical features; dna and mfeat-pixel consist of only categorical features; income, cmc and nomao contain both numerical and categorical features. A summary of the dataset information is provided in Appendix Table D.1.

**Baselines**. In order to assess the efficacy of our D2R2 framework, we compare the performance of D2R2 with four types of baselines, whose details are provided in Appendix E.

1. Supervised learning methods. We compare with established supervised learning methods such as CatBoost , k-nearest neighbors (kNN) classifier  according to the prototype, and TabPFN , a transformer-based network designed to make predictions on a small tabular dataset.
2. Semi-supervised learning methods. Such methods include Mean Teacher(MT) , Interpolation Consistency Training (ICT) , Pseudo-Label  and Meta Pseudo Labels (MPL) .
3. Few-shot meta-learning methods. We consider recent state-of-the-art meta-learning approaches such as CACTUs , UMTRA , SES  and STUNT . STUNT is designed for tabular data while others are designed for image data, whose network structures are modified for tabular data.
4. Self-supervised learning methods. We compare with the state-of-the-art self-supervised methods for tabular data, including VIME, SubTab , SCARF , TabTransformer . We utilize representations acquired from those models to conduct Center Prototype Classification .

### Overall evaluation results

We compare our framework D2R2 with other state-of-the-art supervised, semi-supervised, self-supervised and meta-learning methods. The classification accuracy of all methods is presented in Table 1. In particular, we carry out experiments under two few-shot settings: \(N\)-way 1-shot and \(N\)-way 5-shot. We report the mean accuracy across 100 random seeds.

We note that our D2R2 framework significantly outperforms baseline methods across diverse datasets. A Wilcoxon signed-ranks test is employed (Appendix D.5) to further demonstrate the statistical significance of comparison. When comparing the accuracy of the D2R2 with that of the best baseline on all datasets, Wilcoxon's P-value is below 0.05, significantly indicating the effectiveness of D2R2.

In the case of the diabetes, optdigit and karkunen datasets, where all features are numerical, STUNT method outperforms other baselines, while D2R2 demonstrates substantial improvement. In the high-dimensional dataset optdigit, D2R2 improves accuracy by 39% compared to the supervised method Catboost, 23% compared to the semi-supervised method Mean Teacher, 5% compared to the meta-learning method STUNT, and 22% compared to other self-supervised methods like SubTab. On the other hand, for pixel and dna datasets, which only contain categorical features, meta-learning such as UMTRA and SES are almost ineffective, while D2R2 still demonstrates superior accuracy, outperforming all other methods in both one-shot and five-shot settings. Similarly, for the large size dataset income, encompassing both numerical and categorical features, our framework consistently achieves a high classification accuracy of 72.08%, surpassing the leading baseline STUNT by 19%. In summary, it can be concluded that D2R2 demonstrates a robust capability to address few-shot

   Method & cmc & diabetes & dna & income & karkunen & optdigits & pixel & nomao & brest \\   \\  CatBoost & 36.03 & 56.74 & 39.15 & 57.55 & 53.24 & 58.30 & 54.74 & 63.62 & 69.71 \\ TabPFN & 35.37 & 53.35 & - & - & 46.02 & 55.74 & - & - & - \\ KNN & 35.39 & 58.50 & 42.20 & 51.45 & 54.61 & 65.60 & 60.79 & 63.51 & 71.87 \\ Mean Teacher(*) & 35.58 & 58.05 & 46.58 & 60.63 & 54.57 & 66.10 & 61.02 & 64.23 & 71.92 \\ ICT(*) & 36.53 & 58.08 & 46.55 & 61.83 & 58.37 & 69.12 & 60.88 & - & - \\ Pseudo-Label(*) & 34.97 & 57.03 & 44.26 & 60.52 & 49.44 & 61.50 & 56.12 & 62.39 & 69.92 \\ MPI(*) & 35.13 & 57.39 & 44.22 & 60.85 & 47.66 & 61.52 & 56.01 & 64.28 & 71.33 \\ SubTab & 36.23 & 58.22 & 46.98 & 62.45 & 50.22 & 62.01 & 60.34 & 67.63 & 72.94 \\ VIME & 35.90 & 58.99 & 51.23 & 61.82 & 59.81 & 69.26 & 63.28 & 64.75 & 70.11 \\ SCARF & 35.39 & 55.64 & 57.86 & 57.94 & 60.96 & 63.31 & 63.93 & 68.90 & 75.32 \\ RTDL & 34.34 & 58.15 & 47.99 & 53.61 & 58.25 & 62.78 & 62.87 & 68.33 & 76.38 \\ UMTRA(*) & 35.46 & 57.64 & 25.13 & 57.23 & 49.05 & 49.87 & 34.26 & - & - \\ SES(*) & 34.59 & 59.97 & 39.56 & 56.39 & 49.19 & 56.30 & 49.19 & 69.52 & 74.89 \\ CACTUs(*) & 36.10 & 58.92 & 65.93 & 64.02 & 65.59 & 71.98 & 67.61 & 71.49 & 75.24 \\ STUNT(*) & 37.10 & 61.08 & 66.20 & 63.52 & 71.20 & 76.94 & 79.05 & 71.54 & 76.92 \\ D2R2 & **42.88** & **63.94** & **68.00** & **75.82** & **72.08** & **81.13** & **81.34** & **79.47** & **77.69** \\   \\  CatBoost & 39.89 & 64.51 & 60.20 & 67.99 & 77.94 & 83.07 & 83.38 & 75.32 & 77.06 \\ TabPFN & 38.31 & 64.06 & - & - & 76.59 & 81.68 & - & - & - \\ KNN & 37.65 & 65.61 & 61.16 & 62.19 & 80.08 & 84.16 & 84.75 & 73.78 & 79.43 \\ Mean Teacher(*) & 37.73 & 65.45 & 61.47 & 67.05 & 81.08 & 86.66 & 85.24 & 74.78 & 81.26 \\ ICT(*) & 38.09 & 65.47 & 63.37 & 70.13 & 84.58 & 87.01 & 86.12 & - & - \\ Pseudo-Label(*) & 37.49 & 64.46 & 60.06 & 66.26 & 78.60 & 83.71 & 82.94 & 72.87 & 78.91 \\ MPL(*) & 37.47 & 64.51 & 59.65 & 67.61 & 77.85 & 83.70 & 82.39 & 73.20 & 79.54 \\ SubTab & 39.81 & 68.26 & 62.49 & 72.14 & 70.88 & 83.27 & 80.41 & 76.15 & 82.74 \\ VIME & 39.83 & 67.64 & 71.29 & 72.19 & 19.42 & 83.21 & 85.24 & 74.96 & 85.81 \\ SCARF & 37.75 & 68.66 & 62.75 & 66.09 & 69.96 & 85.67 & 81.32 & 77.65 & 84.42 \\ RTDL & 37.59 & 64.27 & 45.49 & 64.92 & 60.43 & 82.58 & 76.13 & 73.61 & 79.66 \\ UMTRA(*) & 38.05 & 64.41 & 25.08 & 65.78 & 67.28 & 73.29 & 51.32 & - & - \\ SES(*) & 39.04 & 66.61 & 52.25 & 68.27 & 74.80 & 78.46 & 74.80 & 76.50 & 84.73 \\ CACTUs(*) & 38.81 & 66.79 & 81.52 & 72.03 & 82.20 & 85.92 & 85.25 & 78.33 & 86.90 \\ STUNT(*) & 40.40 & 69.88 & 79.18 & 72.69 & **85.45** & 88.42 & 89.08 & 81.49 & 86.82 \\ D2R2 & **43.39** & **73.52** & **82.38** & **76.02** & 84.96 & **90.73** & **91.06** & **82.69** & **88.27** \\   

Table 1: Reported test accuracy is the mean value across 100 random seeds. Asterisked (\(\)) baselines refer to the reported scores in STUNT. Bold number and the underlined number denote the highest score and the second best score, respectively. Empty data is either because the dataset exceeds the input data dimension limits of TabPFN or there are no reported scores in STUNT .

   Dataset & RDM & DR & DR+RDM & RDM+IP & DR+IP & D2R2-c & D2R2 \\  opt (1-shot) & 49.05 & 72.38 & 77.41 & 26.66 & 76.87 & - & **81.13** \\ dna (1-shot) & 45.43 & 57.14 & 61.29 & 26.03 & 56.17 & - & **68.00** \\ cmc (1-shot) & 35.50 & 35.19 & 38.69 & 34.90 & 34.48 & - & **42.88** \\  opt (5-shot) & 70.26 & 88.64 & 89.61 & 51.31 & 89.32 & 87.12 & **90.73** \\ dna (5-shot) & 47.72 & 71.84 & 73.03 & 32.16 & 79.24 & 81.39 & **82.38** \\ cmc (5-shot) & 36.04 & 35.62 & 40.81 & 36.27 & 35.74 & 43.39 & **43.39** \\   

Table 2: Ablation study of three components: diffusion representation (DR), random distance matching (RDM) and instance-wise iterative prototype (IP). We also conduct the center-specific prototype version of D2R2 (D2R2-c).We report the test accuracy (%) under 100 random seeds.

tabular data classification tasks, regardless of the number of shots, dataset size, feature dimension, or the proportion of categorical features.

We speculate that the superior performance of D2R2 is attributed to the effective learned representation of inputs along with the instance-specific prototypes. Upon visual examination of D2R2's embeddings in Figure 3, we observe clustering characteristics from point clouds, which demonstrating the embeddings' effectiveness for classification. Besides, we note the presence of multimodality in the embeddings, supporting the rationale for introducing the instance-wise prototypes.

### Ablation study

We conduct an ablation study to demonstrate the efficacy of three components in D2R2, namely the diffusion representation (DR), random distance matching (RDM), and the instance-wise iterative prototype (IP). We compare the complete D2R2 design (DR+RDM+IP) with frameworks that exclude one or two of these components. We also conduct the center-specific prototype version of D2R2 (D2R2-c), which utilizes average embeddings of \(K\)-shot samples as prototype. The results of the ablation study are presented in the Table 2.

Based on the results, D2R2 surpasses all other variants, indicating that all three components collectively contribute to its superior performance. Specifically, removing the diffusion representation results in a significant degradation in performance, highlighting the crucial contribution of diffusion representation in capturing semantic knowledge. We also observe that DR+RDM shows the second best performance, highlighting the importance of incorporating random distance matching in the diffusion training process. Comparing with a center-specific prototype version of D2R2 (D2R2-c), we observe that the instance-wise prototypes leads to improvement results in optdigit and dna.

## 6 Conclusion

In this paper, we introduce D2R2, an innovative framework to address few-shot tabular challenges. The core idea of D2R2 is to utilize the strong expressive abilities of diffusion model, along with a random distance matching to construct representation learners. This method captures the semantic knowledge of unlabeled data and generate effective embeddings for downstream classification tasks, meanwhile adapting to the mixture feature types of the tabular data. Additionally, to accommodate multimodalities of embeddings, we devise the instance-wise iterative prototype classifier using labeled data. Furthermore, a pseudo-label validation scheme is designed for hyper-parameter selection. The superior performance of D2R2 is presented on diverse datasets, demonstrating the effectiveness of D2R2 over other baselines. We hope that our work will inspire new avenues for research in the field of diffusion model representation learning on tabular datasets.

## 7 Acknowledgements

We would like to thank AC and reviewers for their valuable comments on the manuscript. Bingyi Jing's research is partly supported by NSFC 12371290.

Figure 3: The t-SNE visualizations depict the D2R2 representations, with point clouds illustrating the embeddings of 1000 randomly selected samples, color-coded based on their respective class labels. We observe that the embeddings exhibit multimodal patterns. For instance, in the dna dataset, the red class is distributed in both the bottom right corner and the top left corner.