# Double Gumbel Q-Learning

David Yu-Tung Hui

Mila, Universite de Montreal

dythui2+drl@gmail.com

Aaron Courville

Mila, Universite de Montreal

Pierre-Luc Bacon

Mila, Universite de Montreal

###### Abstract

We show that Deep Neural Networks introduce two heteroscedastic Gumbel noise sources into Q-Learning. To account for these noise sources, we propose Double Gumbel Q-Learning, a Deep Q-Learning algorithm applicable for both discrete and continuous control. In discrete control, we derive a closed-form expression for the loss function of our algorithm. In continuous control, this loss function is intractable and we therefore derive an approximation with a hyperparameter whose value regulates pessimism in Q-Learning. We present a default value for our pessimism hyperparameter that enables DoubleGum to outperform DDPG, TD3, SAC, XQL, quantile regression, and Mixture-of-Gaussian Critics in aggregate over 33 tasks from DeepMind Control, MuJoCo, MetaWorld, and Box2D and show that tuning this hyperparameter may further improve sample efficiency.

## 1 Introduction

Reinforcement Learning (RL) algorithms learn optimally rewarding behaviors (Sutton and Barto, 1998, 2018). Recent RL algorithms have attained superhuman performance in Atari (Mnih et al., 2015), Go (Silver et al., 2016), Chess (Schrittwieser et al., 2020), StarCraft (Vinyals et al., 2019), simulation racing (Wurman et al., 2022), and have also been used to control stratospheric balloons (Bellemare et al., 2020) and magnetic fields in Tokamak nuclear fusion reactors (Degrave et al., 2022). All these aforementioned agents use a \(Q\)-function (Watkins and Dayan, 1992) to measure the respective quality of their behaviors.

Many algorithms (Lillicrap et al., 2015; Mnih et al., 2016, 2015; Schulman et al., 2015; Fujimoto et al., 2018; Haarnoja et al., 2018) learn a \(Q\)-function by minimizing the Mean-Squared Bellman Error (MSBE). The functional form of the MSBE was motivated by an analogy to the popular Mean-Squared Error (MSE) in supervised learning regression problems, but there is little theoretical justification for its use in RL (Bradtke and Barto, 1996; Ernst et al., 2005; Riedmiller, 2005).

Training a deep neural network to minimize the MSBE is empirically unstable (Irban, 2018; Henderson et al., 2018; Van Hasselt et al., 2016) because deep neural networks induce overestimation bias in the \(Q\)-function (Thrun and Schwartz, 1993; Fujimoto et al., 2018). A popular method to reduce overestimation 'pessimistically' selects less positive \(Q\)-values from an ensemble of \(Q\)-functions, effectively returning low-quantile estimates (Fujimoto et al., 2018; Kuznetsov et al., 2020; Ball et al., 2023). However, an ensemble is computationally expensive and quantiles only provide discrete-grained control over the degree of pessimism. Curiously, pessimism has mainly been applied to 'continuous control' problems that require continuous-valued actions and not discrete control.

This paper analyzes noise in Deep \(Q\)-Learning. We introduce a loss function for Deep \(Q\)-Learning and a scalar hyperparameter that adjusts pessimism without an ensemble. These two innovations form **Double Gumbel \(Q\)-Learning (DoubleGum)**, 1 an off-policy Deep \(Q\)-Learning algorithm applicable to both discrete and continuous control. Our paper is structured as follows:

1. Section 2.1 shows that the MSBE implicitly uses a noise model of the Bellman Optimality Equation with additive homoscedastic noise. Section 4.1 shows that this noise model is empirically too coarse to fully capture the distribution of noise in Deep \(Q\)-Learning.
2. Section 3 derives a noise model of the Soft Bellman Equation with additive heteroscedastic logistic noise derived from two heteroscedastic Gumbel noise sources. Section 4.1 shows that our noise model is a better empirical fit to the noise distribution in Deep \(Q\)-Learning.
3. Section 3.3 shows that when the soft value function in the Soft Bellman Equation is intractable in continuous control, its approximation leads to methods that adjust pessimism. Section 3.1 presents a method to adjust pessimism with a single scalar hyperparameter, and its effect is empirically verified in Section 4.2.
4. Section 5 discusses related work, notably presenting DoubleGum as a special case of Distributional RL and a generalization of existing Maximum Entropy (MaxEnt) RL algorithms.
5. Section 6 benchmarks DoubleGum, showing marginal improvements over DQN baselines on 3 classic discrete control tasks and improved aggregate performance over DDPG, SAC, TD3, XQL and two Distributional RL algorithms over 33 continuous control tasks.

## 2 Mathematical Background

Reinforcement Learning agents learn to make maximally rewarding decisions in an environment (Sutton and Barto, 1998, 2018). In Markov Decision Process (MDP) environments, a \(Q\)-function specifies the expected amount of reward an agent obtains (Watkins and Dayan, 1992) and is also known as a state-action value function. MDPs define a transition probability \(p(s^{} s,a)\) and reward \(r(s,a,s^{})\) for every pair of states \(s,s^{}\) and action \(a\)(Bellman, 1957; Howard, 1960). Decisions are made by a policy \((a s)\) at every discrete timestep and the \(Q\)-function of \(\) at \(s\), \(a\) is

\[Q_{}(s,a)=*{}_{,p}[_{j=i}^{n}^{j}r _{j}|s_{i}=s,a_{i}=s],\ \ \ r(s_{j},a_{j},s_{j+1})\ \ r_{j},\]

where the discount factor \(]0,1[\) ensures finite \(Q\) for infinite time-horizon \(n\)(Samuelson, 1937). The expectation in the \(Q\)-function is computed over a trajectory sequence of \(s\) and \(a\) induced by the coupling of the policy and the transition function.

In every MDP, an optimal policy \(^{}\) makes maximally rewarding decisions following \(s_{a}Q^{}(s,a)\)(Watkins and Dayan, 1992). \(Q^{}\) is the optimal value function specified as the unique fixed point of the Bellman Optimality Equation of the MDP (Bellman, 1957)

\[Q^{}(s,a)=*{}_{p(s^{}|s,a)}[r+ _{a^{}}Q^{}(s^{},a^{})],\ \ )$ as $r$.}\] (1)

In Deep \(Q\)-Learning, \(Q^{}\) is approximated by a neural network \(Q_{}\) with parameters \(\). Following Munos and Szepesvari (2008), \(\) is updated by a sequence of optimization problems whose \(i^{}\) stage iteration is given by

\[_{i+1}=_{}(Q_{},y_{_{i}} ),\ \ \ y_{i}(s,a)=*{}_{p(s^{}|s,a)}[r+ _{a^{}}Q_{_{i}}(s^{},a^{})]\ \,\]

where the'stop-gradient' operator \(\) denotes evaluating but not optimizing an objective with respect to its argument and \(y\) is dubbed the bootstrapped target. In this work, we restrict our environments to deterministic MDPs, so the resultant expectation over \(s^{}\) is computed over one sample.

Typically, the loss function \((Q_{},y_{_{i}})=*{}_ {s,a}(Q_{}(s,a)-y_{_{i}}(s,a))^{2}\) is the Mean-Squared Bellman Error (MSBE). At every \(i\), \(\) is minimized with stochastic gradient descent by a fixed number of gradient descent steps and not optimized to convergence (Minh et al., 2015; Lillicrap et al., 2015).

### Deriving the Mean-Squared Bellman Error

Thrun and Schwartz (1993) model \(Q_{}\)(s, a) as a random variable whose value varies with \(,s\) and \(a\). They set up a generative process \(Q_{}(s,a)=Q^{}(s,a)-_{,s,a}\) showing that the function approximator \(Q_{}\) is a noisy function of the implicit true value \(Q^{}\), but they do not make any assumptions about \(\). 2

We derive the MSBE with two additional assumptions: \(_{,s,a}}}{{}}(0,^{2})\), where \(\) is a homoscedastic normal due to the Central Limit Theorem (CLT) 3 and secondly \(Q^{}(s,a) y_{}(s,a)\), where \(\) are the parameters at an arbitrary optimization stage. Incorporating these two assumptions into the noise model yields \(y_{}(s,a)=Q_{}(s,a)+_{,s,a}\), uncovering the implicit assumption that Temporal-Difference (TD) errors \(y_{}-Q_{}\) follow a homoscedastic normal. Maximum Likelihood Estimation (MLE) of \(\) on the resultant noise model yields the MSBE after abstracting away the constants

\[_{}*{}_{s,a} p(y_{}(s,a ))=_{}*{}_{s,a}[+ }(Q_{}(s,a)-y_{}(s,a))^ {2}].\]

### The Limiting Distribution in Bootstrapped Targets

Both assumptions used to derive the MSBE are theoretically weak. First, the CLT states that iid samples of an underlying distribution tend towards a homoscedastic normal but do not account for the form of the underlying distribution. Secondly, there is no analytic justification for \(Q^{}(s,a) y_{}(s,a)\). Substituting the Thrun and Schwartz (1993) model in the RHS of the Bellman Optimality Equation yields

\[Q^{}(s,a)=*{}_{p(s^{}|s,a)}[r+ _{a^{}}[Q_{}(s^{},a^{})+_{,s^{ },a^{}}]]=*{}_{p(s^{}|s,a)}[r+  g_{,s^{}}] y_{}(s,a).\]

When the \(\) over a finite number of iid samples with unbounded support is taken, the Extreme Value Theorem (EVT) gives the limiting distribution of the resultant random variable as a Gumbel distribution (Fisher and Tippett, 1928; Gnedenko, 1943), defined here as \(g_{,s^{}}(,)\). Expressions relating \(\) and \(\) to the parameters of the underlying iid random variables are rarely analytically expressible (Kimball, 1946; Jowitt, 1979) and there is no guarantee \(Q^{}(s,a) y_{}(s,a)\).

## 3 Deriving a Deep Q-Learning Algorithm from First Principles

We find an analytic expression that relates the limiting Gumbel distribution to parameters of the underlying noise distribution from function approximation. To do so, we assume that noise in the Thrun and Schwartz (1993) model is a heteroscedastic Gumbel noise with state-dependent spread

\[Q_{}(s,a)=Q^{}(s,a)-g_{,a}(s), g_{,a}() }}{{}}(0,_{}( )),a,\] (2)

Appendix A.1 restates the finding of McFadden et al. (1973) and Rust (1994), which results in the following expression with the soft value function \(V_{}^{}\)

\[_{a}Q^{}(s,a) =_{a}[Q_{}(s,a)+g_{,a}(s)]=_{ }(s)((s,a)}{_{}(s)}) \,a+g_{}(s)\] \[=V_{}^{}(s)+g_{}(s),g_{,a}(),g_{}()}}{{ }}(0,_{}()).\] (3)

Substituting Equations 2 and 3 into 1 yields a new noise model of the Soft Bellman Optimality Equation with two function approximators and two heteroscedastic Gumbel noise sources

\[Q_{}(s,a)+g_{,a}(s)=*{}_{p(s^{}|s,a) }[r+ V_{}^{}(s^{})+ g_{ }(s^{})].\] (4)

We now develop this noise model into a loss function used in our algorithm, DoubleGum.

[MISSING_PAGE_EMPTY:4]

is commonly used in continuous control (Lillicrap et al., 2015; Fujimoto et al., 2018; Haarnoja et al., 2018), and has been recently been shown to improve discrete control (D'Oro et al., 2022).

**Q-Functions with Variance Networks**: We learn \(\) with variance networks (Nix and Weigend, 1994; Kendall and Gal, 2017). In discrete control, \(_{}(s)\) is a variance head added to the \(Q\)-network, and is approximated as \(_{}(s,a)\) in continuous control. The variance head is a single linear layer whose input is the penultimate layer of the main \(Q\)-network and output is a single unit with a SoftPlus activation function (Dugas et al., 2000) that ensures its positivity. Seitzer et al. (2022) improves stability during training by multiplying the entire loss by \(_{}\), the numerical value of the variance network, yielding a loss function for \(\) which for continuous control is

\[J_{,y_{},_{}}(s,a,r,s^{})=[_{}(s,a)(_{}(s,a)+((s,a,r,s^{ })-Q_{}^{}(s,a)}{_{}(s,a)})^{2}) ].\] (9)

Note that in the above equation \(_{}(s,a)\) would be \(_{}(s)\) in discrete control. We do not have any convergence guarantees for this loss function, and Appendix C discusses this issue in more detail.

**Policy**: In discrete control, actions are taken by \(_{a}Q_{}^{}(s,a)\) as standard. Exploration was performed by policy churn (Schaul et al., 2022). In continuous control, the policy \(_{}\) is a separate network. We use a DDPG fixed-variance actor because Yarats et al. (2021) showed it trained more stably than a SAC actor with learned variance. Following Laskin et al. (2021), the actor's outputs were injected with zero-mean Normal noise with a standard deviation of 0.2 truncated to 0.3.

**Network Architecture**: All networks had two hidden layers of size 256, ReLU activations (Glorot et al., 2011), orthogonal initialization (Saxe et al., 2013) with a gain of \(\) for all layers apart from the last layer of the policy and variance head, which had gains of \(1\). This initialization had been shown to be empirically advantageous in policy-gradient and \(Q\)-Learning methods in Huang et al. (2022) and Kostrikov (2021), respectively. Ball et al. (2023) improves stability in continuous control using LayerNorm (Ba et al., 2016). We find that the similar method of GroupNorm (Wu and He, 2018) with 16 groups without a shift and scale (Xu et al., 2019) worked better, but only in continuous control. All parameters were optimized by Adam (Kingma and Ba, 2014) with default hyperparameters.

## 4 Empirical Evidence for Theoretical Assumptions

### Noise Distributions in Deep Q-Learning

Figure 1 shows the evolution of noise during training DoubleGum in the classic control task of CartPole-v1. Histograms in Figures 1(a) and 1(b) were generated from \(10^{4}\) samples from the replay buffer after 1 million training timesteps. Figure 1(a) computes TD-errors by \(y_{}(s,a,r,s^{})-Q_{}^{}(s,a)\), while Figure 1(b) computes standardized TD-errors by dividing the previous equation by \(_{}(s)\) or \(_{}(s,a)\) where appropriate. The normal, logistic, and Gumbel distributions were fitted by moment matching, and homo(scedastic) and hetero(sedastic) distributions were respectively fitted to unstandardized and standardized data. Figure 1(c) shows the mean and standard deviation over 12 Negative Log-Likelihoods (NLLs), each computed from a different training run with a different (randomly chosen) initial seed. Every NLL was computed over \(10^{4}\) samples from the replay buffer every 1000 timesteps of training.

Figure 1(a) shows that a homoscedastic normal coarsely fits the empirical distribution, forming a good estimate of the mean but not the spread. Our result contradicts Garg et al. (2023) that fitted a Gumbel to the empirical distribution. We discuss this discrepancy in Appendix E.1. Figure 1(b) shows that a heteroscedastic Logistic captures both the mean and spread of the TD-errors, validating Equations 5 and its derivation. Finally, Figure 1(c) shows that a moment-matched heteroscedastic normal used in DoubleGum is a suitable approximation to the heteroscedastic logistic throughout training, validating Equation 6. Appendix D.1 shows that the trend in Figure 1(c) holds for other discrete control environments as well as continuous control.

### Adjusting The Pessim Factor

Figure 2 plots a 12-sample IQM and standard deviation over \(_{i=1}^{256}Q_{}^{}(s_{i},a_{i})\), the average magnitude of the target \(Q\)-value used in bootstrapped targets. Figure 2 and Appendix D.2 show that the average magnitude increases as the pessimism factor \(c\) increases, validating its effectiveness.

Figure 1: The empirical distribution of noise in discrete control CartPole-v1. (a, b): Fitted normal, logistic, and Gumbel distributions against (a) unstandardized and (b) standardized empirical distributions in Deep \(Q\)-Learning at the end of training. (c): Negative Log-Likelihoods (NLLs) of the noise in Deep \(Q\)-Learning under different distributions throughout training (lower is better). Appendix D.1 presents further results in more discrete and continuous control tasks.

Figure 2: The effect of changing pessimism factor \(c\) on the target \(Q\)-value in continuous control. Appendix D.2 presents further results in more tasks.

Related Work

### Theoretical Analyses of the Noise in Q-Learning

**Logistic Q-Learning**Bas-Serrano et al. (2021) presents a similar noise model to DoubleGum but with a homoscedastic logistic instead of a heteroscedastic logistic in Equation 5. Their distribution is derived from the linear-programming perspective of value-function learning described in Section 6.9 of Puterman (2014)) and Peters et al. (2010). While DoubleGum learns a \(Q\)-function off-policy, Logistic \(Q\)-Learning uses on-policy rollouts to compute the \(Q\)-values of the current policy. We do not benchmark against Logistic \(Q\)-Learning because their method is on-policy and only uses linear function approximation.

**Extreme Q-Learning (XQL)**Garg et al. (2023) presents a noise model for Deep \(Q\)-Learning with one homoscedastic Gumbel noise source, as opposed to the two heteroscedastic Gumbels in Equation 4. Parameters are learned by the LINear-EXponential (LINEX) loss Varian (1975) formed from the log-likelihood of a Gumbel distribution. XQL is presented in more detail in Appendix B.4.

**Gumbel Noise in Deep Q-Learning**: Thrun and Schwartz (1993) argued that the \(\)-operator in bootstrapped targets transformed zero-mean noise from function approximation into statistically biased noise. The authors did not recognize that the resultant noise distribution was Gumbel, and this was realized by Lee and Powell (2012). Unlike DoubleGum, these two works did not make assumptions about the distribution of function approximator noise but instead focused on mitigating the overestimation bias of bootstrapped targets. In economics, McFadden et al. (1973) and Rust (1994) assume the presence of Gumbel noise from noisy reward observations (unlike DoubleGum, which assumes that noise comes from function approximation) and derives the soft value function we present in Appendix A.1 for static and dynamic discrete choice models. XQL brings the Rust-McFadden et al. model to deep reinforcement learning to tackle continuous control.

The **soft value function** was introduced to model stochastic policies (Rummery and Niranjan, 1994; Fox et al., 2015; Haarnoja et al., 2017, 2018b). The most prominent algorithm that uses the soft value function is **Soft Actor-Critic (SAC)**(Haarnoja et al., 2018a,b), which Appendix B.2 shows is a special case of DoubleGum when \(Q_{}^{}\) is learned by homoscedastic instead of heteroscedastic normal regression and the spread is a tuned scalar parameter instead of a learned state-dependent standard deviation. Appendix B.2 also shows that **Deep Deterministic Policy Gradients (DDPG)**(Lillicrap et al., 2015) is a simpler special case of SAC that has recently been shown to outperform and train more stably than SAC (Yarats et al., 2021).

### Empirically Similar Methods to DoubleGum

**Distributional RL** models the bootstrapped targets and \(Q\)-function as distributions. In these methods, a \(Q\)-function is learned by minimizing the divergence between itself and a target distribution. The most similar distributional RL method to DoubleGum is **Mixture-of-Gaussian (MoG) Critics**, introduced in Appendix A of Barth-Maron et al. (2018). DoubleGum is a special case of MoG-Critics with only one Gaussian (such as in Morimura et al. (2012)) and mean samples of the target distribution. Curiously, Shahriari et al. (2022) shows that sample-efficiency of training improves when bootstrapped target sampled are increasingly near the mean but did not try mean sampling. Nevertheless, Shahriari et al. (2022) show that MoG-Critics outperforms a baseline with the C51 distributional head (Bellemare et al., 2017) popular in discrete control, obtaining state-of-the-art results in DeepMind Control. In discrete control with distributional RL, C51 has been superseded by **Quantile Regression (QR)** methods (Dabney et al., 2018,a; Yang et al., 2019) that predict quantile estimates of a distribution. Ma et al. (2020); Wurman et al. (2022) and Teng et al. (2022) apply QR to continuous control.

**Adjusting pessimism** greatly improves the sample efficiency of RL. Fujimoto et al. (2018) showed empirical evidence of overestimation bias in continuous control and mitigated it with pessimistic estimates computed by **Twin Networks**. However, Twin Networks may sometimes harm sample efficiency because the optimal degree of pessimism varies across environments. To address this, the degree of pessimism is adjusted during training (Lan et al., 2020; Wang et al., 2021; Karimpanal et al., 2021; Moskovitz et al., 2021; Kuznetsov et al., 2020, 2022; Ball et al., 2023), often with the help of an ensemble. In contrast, DoubleGum uses one \(Q\)-network and one scalar hyperparameter fixed throughout training.

Many other RL methods **add or subtract the learned standard deviation to bootstrapped targets**. Risk-Aware RL subtracts a learned standard deviation to learn a low-variance policy (La and Ghavamzadeh, 2013; Tamar and Mannor, 2013; Tamar et al., 2016). Upper-Confidence Bounded (UCB) methods add a learned standard deviation to explore high-variance regions (Lee et al., 2021; Teng et al., 2022). These methods use a combination of ensembles and variance networks, but it is also possible to derive a Bellman equation for the variance following Dearden et al. (1998). The current state-of-the-art method in RL with variance estimation is Mai et al. (2022), which uses both variance networks and ensembles. The use of ensembles is motivated by the need to capture model uncertainty - differences between -functions with different parameters. We believe that ensembles are unnecessary because model uncertainty will be expressed in bootstrapped targets from two different timesteps as parameters change through learning and that all variation in bootstrapped targets will henceforth be captured by variance networks.

## 6 Results

### Discrete Control

We benchmarked DoubleGum on classic discrete control tasks against two baselines from the DQN family of algorithms (Figure 3). All algorithms were implemented following Section 3.4. Appendix E.3 discusses the baseline algorithms in more detail.

Performance was evaluated by the InterQuartile Mean (IQM) over 12 runs, each one with a different randomly initialized seed. Agarwal et al. (2021) showed that the IQM was a robust performance metric in RL. 12 was chosen because it was the smallest multiple of four (so a IQM could be computed) greater than the 10 seeds recommended by Henderson et al. (2018). In each run, the policy was evaluated by taking the mean of 10 rollouts every 1000 timesteps, following Fujimoto et al. (2018).

Figure 3 shows that DoubleGum sometimes obtains better sample efficiency than baselines, but not significantly more to necessitate further discrete control experiments. In the remainder of this work, we focus on continuous control, where we found DoubleGum to be more effective.

### Continuous Control

We present two modes of evaluating DoubleGum because our algorithm has a pessimism factor hyperparameter we choose to tune per suite. We, therefore, benchmark all continuous control algorithms with default pessimism (Figure 4) and without pessimism-tuning per-suite (Figure 5). Table 5 presents default and per-suite pessimism values.

We compare against seven baselines. The first five are popular algorithms in the continuous control literature: DDPG (Lillicrap et al., 2015), TD3 (Fujimoto et al., 2018), SAC (Haarnoja et al., 2018), MoG-Critics (Shahriari et al., 2022) and XQL (Garg et al., 2023). Here, DDPG, TD3, and SAC represent MaxEnt RL, MoG-Critics represent Distributional RL, and XQL is the algorithm with the most similar noise model to ours. We introduce two further baselines: QR-DDPG, a stronger Distributional RL baseline that combines quantile regression (Dabney et al., 2018) with DDPG, and FinerTD3, TD3 with an ensemble of five networks as opposed to a network of two ensembles

Figure 3: Discrete control, IQM of returns \(\) standard deviation over 12 seeds.

in the original TD3 that enables finer control over the degree of pessimism with an ensemble of five networks. All algorithms were implemented following design decisions outlined in Section 3.4. Appendix E.4 discusses the algorithms in more detail.

The pessimism of DoubleGum was tuned by changing its pessimism factor. The pessimism of baseline algorithms was tuned by manually choosing whether to use Twin Networks (Fujimoto et al., 2018) or not. Note that we refer to DDPG with Twin Networks as TD3. The pessimism of FinerTD3 was tuned by selecting which quantile estimate to use from an ensemble of five networks. The pessimism of MoG-Critics could not be tuned because its critic does not support paired sampling between ensemble members. Appendices F.1 and F.2 respectively detail how the pessimisms of DoubleGum and baseline algorithms were adjusted.

We benchmarked DoubleGum on 33 tasks over 4 continuous control suites comprising respectively of 11 DeepMind Control (DMC) tasks (Tassa et al., 2018; Tunyasuvunakool et al., 2020), 5 MuJoCo tasks (Todorov et al., 2012; Brockman et al., 2016), 15 MetaWorld tasks (Yu et al., 2020) and 2 Box2D tasks (Brockman et al., 2016). We follow Agarwal et al. (2021) and evaluate performance with the normalized IQM with 95% stratified bootstrap confidence intervals aggregated over all 33 tasks. 12 runs from each task was collected by a similar method to that described in Section 6.1. Further details of the tasks and their aggregate metric are detailed in Appendix E.2.

Figure 4 shows that DoubleGum outperformed all baselines in aggregate over 33 tasks when all algorithms used their default pessimism settings. Figure 5 shows that DoubleGum outperformed all baselines in aggregate over 33 tasks when the pessimism of all algorithms is adjusted per suite. Comparing the figures shows that adjusting the pessimism of DoubleGum per suite also attained a higher aggregate score than DoubleGum with default pessimism.

## 7 Discussion

This paper studied the noise distribution in Deep \(Q\)-Learning from first principles. We derived a noise model for Deep \(Q\)-Learning that used two heteroscedastic Gumbel distributions. Converting our noise model into an algorithm yielded DoubleGum, an off-policy Deep \(Q\)-Learning algorithm applied to both discrete and continuous control.

In discrete control, our algorithm attained competitive performance to the baselines. Despite having an numerically exact loss function in discrete control, DoubleGum was very sensitive to hyperparameters.

Figure 4: Continuous control with default parameters, IQM normalized score over 33 tasks in 4 suites with 95% stratified bootstrap CIs. Methods that default to use Twin Networks are dashed. Appendix F.4 presents per-suite and per-task aggregate results.

Figure 5: Continuous control with the best pessimism hyperparameters tuned per suite, IQM normalized score over 33 tasks in 4 suites with 95% stratified bootstrap CIs. Appendix F.5 presents per-suite and per-task aggregate results.

Practically, using Dueling DQN (Wang et al., 2016) to learn a \(Q\)-function was crucial to getting DoubleGum to work. Appendix D.1 shows that our noise model fits the underlying noise distribution of Deep \(Q\)-Learning and we therefore suspect that instability in discrete control might be due to the training dynamics of deep learning and not our theory.

In continuous control, we introduced a pessimism factor hyperparameter to approximate our otherwise intractable noise model. We provided a default value for the pessimism factor that outperformed popular \(Q\)-Learning baselines in aggregate over 33 tasks. Tuning this hyperparameter yielded even greater empirical gains. Our method of tuning pessimism was more computationally efficient and finer-grained than popular methods that tuned a quantile estimate from an ensemble.

In continuous control, DoubleGum outperformed all baselines in aggregate. We hypothesize that DoubleGum outperformed MaxEnt RL baselines because DoubleGum is a more expressive generalization of SAC, which is itself more expressive than DDPG as shown in Appendix B.2. Our theory showed that TD-errors follow a heteroscedastic Logistic, and we believe that modeling this distribution should be sufficient for distributional RL. We hypothesize that more complex distributions considered by the Distributional RL methods QR and MoG overfit to the replay buffer and might not generalize well to online rollouts. FinerTD3 performs marginally poorer than DoubleGum, even when pessimism was adjusted per-suite. We believe this is because FinerTD3 adjusts pessimism finer than other baselines, but still not as fine as the continuous scalar in DoubleGum. Finally, we hypothesize that DoubleGum outperformed XQL because our noise model better fits the underlying noise distribution in Deep \(Q\)-Learning, as shown in Appendix D.1.

This paper shows that better empirical performance in Deep RL may be attained through a better understanding of theory. To summarize, we hope that our work encourages the community to increase focus on reducing the gap between theory and practice to create reinforcement learning algorithms that train stably across a wide variety of environments.

### Limitations

Theoretically, our work lacks a convergence guarantee for DoubleGum. This is exceptionally challenging because to the best of our knowledge there are currently no convergence guarantees for heteroscedastic regression. Appendix C discusses convergence in more detail.

Experimentally, there are many areas left open for future work. For speed in proof of concept experiments, we only swept over five values for the pessimism factor hyperparameter and only tuned per-suite. We anticipate that a more thorough search of the pessimism factor combined with a per-task selection will improve our results even further. An obvious follow-up would be an algorithm that automatically adjusted the pessimism factor during training. Additionally, we only focused on environments with state observations to not deal with representation learning from visual inputs. Another obvious next step would be to train DoubleGum on visual inputs or POMDPs.