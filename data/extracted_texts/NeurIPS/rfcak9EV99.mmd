# Policy Optimization for

Continuous Reinforcement Learning

Hanyang Zhao

Columbia University

hz2684@columbia.edu

&Wenpin Tang

Columbia University

wt2319@columbia.edu

&David D. Yao

Columbia University

yao@columbia.edu

###### Abstract

We study reinforcement learning (RL) in the setting of continuous time and space, for an infinite horizon with a discounted objective and the underlying dynamics driven by a stochastic differential equation. Built upon recent advances in the continuous approach to RL, we develop a notion of occupation time (specifically for a discounted objective), and show how it can be effectively used to derive performance-difference and local-approximation formulas. We further extend these results to illustrate their applications in the PG (policy gradient) and TRPO/PPO (trust region policy optimization/ proximal policy optimization) methods, which have been familiar and powerful tools in the discrete RL setting but under-developed in continuous RL. Through numerical experiments, we demonstrate the effectiveness and advantages of our approach.

## 1 Introduction

Reinforcement Learning (RL, ) has been successfully applied to wide-ranging domains in the past decade, including achieving superhuman performance in games like Atari and Go , enhancing Large Language Models using human feedback , and showing potentials in improving traditional model-based decisions in healthcare, inventory management, and finance . Most existing works, including all references cited above, are formulated and solved as discrete-time sequential optimization problems such as Markov decision processes (MDPs, ). Yet in many applications, agents may need to monitor and interact with the random environment at an ultra-high frequency (e.g., autonomous driving, robot navigation, and high-frequency stock trading), which calls for a _continuous-time/space_ approach.

Recent years have witnessed a fast growing body of research that has extended the frontiers of continuous RL in several important directions including, for instance, modeling the noise or randomness in the environment dynamics as following a stochastic differential equation (SDE), and incorporating an entropy-based regularizer into the objective function  to facilitate the exploration-exploitation tradeoff; designing model-free methods and algorithms, along with applications to portfolio optimization ; studying regret bounds , and so forth.

In this paper, we continue the above trend in continuous RL, focusing on an infinite horizon formulation with a discounted objective and the underlying dynamics driven by an SDE . We are specifically motivated by the following two questions.

**(Q1)** The visitation frequency in MDP (with a discounted objective) is defined as: \((s)=_{t=0}^{}^{t}(Y_{t}=s)\), where \(\{Y_{t}\}\) is a Markov chain with state space \(:=\{s\}\), and \((0,1)\) is a discount factor. It plays an important role in many RL algorithms for MDP. So, a natural question is, what is the continuous counterpart of \((s)\)?

**(Q2)** For continuous RL, how can we characterize the difference in performance between two policies? In particular, can we derive performance-difference formulas similar to those in the MDPcase ? Can we adapt and apply the ideas and tools of the efficient policy optimization methods (e.g., ) to the continuous RL setting?

**Main contributions**. We provide a unified theory/framework for policy optimization in continuous time and space. Specifically, we have addressed the above two questions (**Q1**) and (**Q2**) by developing the notion of _occupation time/measure_, specifically for a discounted objective, and focusing on its associated _q-value_. Based on these two quantities, we derive the performance-difference formula for continuous RL by means of perturbation analysis. Leveraging the performance-difference formula, we develop the continuous counterparts of the policy gradient (PG, ) and also propose the local approximation for the performance metric, for which we derive a bound on it and allow the development of a minorization-majorization (MM) algorithm. We further develop the continuous counterparts of trust region policy optimization/ proximal policy optimization (TRPO/PPO) methods in , which have been familiar and powerful tools in the discrete RL setting but under-developed in continuous RL, as approximations to the previous algorithms. (What is worth mentioning is that these policy optimization algorithms do not require any _a priori_ discretization of time and space.) Through numerical examples we show the convergence of these algorithms when applied to certain stochastic control tasks in continuous time and space.

**Organization of the paper**. In Section 2 we present the continuous RL formulation and develop necessary tools. The main results, the performance-difference formula (Theorem 2) and the bound (Theorem 5) are provided in Section 3. In Section 4 we propose two algorithms, _policy gradient with random rollout_ and _PPO with adaptive penalty_, based on our analyses and theoretical results; and illustrate their performance via numerical experiments. Concluding remarks are summarized in Section 5.

**Related works**. One line of research on continuous RL focuses on modeling the underlying dynamics as a deterministic system, typically following a deterministic ordinary differential equation. Several papers  solve the problems via _a priori_ discretization in either time or space;  develops a framework to apply the temporal difference to the continuous setting, and proposes algorithms that combine value iteration or advantage update as in  to avoid explicit discretization;  further investigates policy gradient methods, followed by more recent studies on model-free continuous RL methods  or model-based ones ;  studies the sensitivity of existing off-policy algorithms along with advantage updating to propose continuous RL algorithms that are robust to time discretization.

The formulation of continuous RL in a stochastic setting (i.e., with the state process driven by an SDE), can be traced back to , which however provides no data-driven solution. Recently,  develops an exploratory control model for the continuous RL. Built upon this approach and for a finite-horizon objective,  studies policy evaluation, and  policy gradient. Furthermore,  brings forth the notion of \(q\)-value, which leads to a continuous analogue of \(Q\)-learning. Also worth noting is , which studies RL in the mean-field regime where continuous-time processes occur in the limit, and  extends the study to jump-diffusion processes.

In discrete-time MDPs, the body of research on bounding the performance difference between two policies also relates to our work:  develop a policy improvement bound for the discounted total reward;  studies the long-run average reward, and  proposes a bound that is continuous with respect to the discount factor.

**Notation**. For a measurable set \(\), denote \(()\) for the set of probability distributions over \(\). For a vector \(x\), denote by \(\|x\|_{2}\) the Euclidean norm of \(x\). For a matrix \(A\), denote by \(||A||_{F}\) the Frobenius norm of \(A\), and \(A^{2}:=AA^{}\) where \(A^{}\) is the transpose of \(A\). For a positive-definite matrix \(A\), denote by \(A^{}\) the square root matrix of \(A\). For \(A,B\) two matrices of the same size, denote by \(A B\) the inner product of \(A\) and \(B\). For a function \(f\) on an Euclidean space, \( f\) (resp. \(^{2}f\)) denotes the gradient (resp. the Hessian) of \(f\). For two distributions \(P,Q(A)\), denote by \(W_{2}(P,Q)\) the Wasserstein-2 distance (or Quadratic Wasserstein distance) between \(P\) and \(Q\):

\[W_{2}(P,Q)=(_{(P,Q)}_{(x,y)}\|x-y \|^{2})^{1/2}\,,\]

where \((P,Q)\) is the set of all couplings of \(P\) and \(Q\); and denote by \(D_{}(P||Q)\) the KL-divergence between \(P\) and \(Q\): \(D_{}(P||Q)= p(x)()dx\), in which \(p\) and \(q\) denote the probability densities of \(P\) and \(Q\).

Formulation and Preliminaries

**Continuous RL.** We start with a quick formulation of the continuous RL, based on the same modeling framework as in . Assume that the state space is \(^{n}\), and denote by \(\) the action space. Let \(( x)()\) be a (state) feedback policy given the state \(x^{n}\). A continuous RL problem is formulated by a distributional (or relaxed) control approach , which is motivated by the trial and error process in RL. The state dynamics \((X^{a}_{s},\,s 0)\) is governed by the Ito process:

\[X^{a}_{s}=b(X^{a}_{s},a_{s})s+(X^{a} _{s},a_{s})B_{s}, X^{a}_{0}(^{n}),\] (1)

where \((B_{t},\,t 0)\) is the \(m\)-dimensional Brownian motion, \(b:^{n}^{n}\), \(:^{n}^{n m}\), and the action \(a_{s}\) is generated from the distribution \(( X^{a}_{s})\) by _external randomization_. To avoid technical difficulties, we assume that the stochastic processes (1) (and (3), (9) below) are well-defined, see [24, Section 5.3] or [50, Chapter 6] for background.

From now on, write \((X^{}_{s},a^{}_{s})\) for the state and action at time \(s\) given by the process (1) under the policy \(=\{( x)():x^{n}\}\). The goal here is to find the optimal feedback policy \(^{*}\) that maximizes the expected discounted reward over an infinite time horizon:

\[V^{*}:=_{}[_{0}^{+}e^{- s}[r(X^ {}_{s},a^{}_{s})+ p(X^{}_{s},a^{}_{s},(  X^{}_{s}))]s X^{}_{0} ],\] (2)

where \(r:^{n}^{+}\) is the running reward of the current state and action \((X^{}_{s},a^{}_{s})\); \(p:^{n}() \) is a regularizer which facilitates exploration (e.g., in , \(p\) is taken as the differential entropy defined by \(p(x,a,())=-(a)\)); \( 0\) is a weight parameter on exploration (also known as the "temperature" parameter); and \(>0\) is a discount factor that measures the time-depreciation of the objective value (or the impatience level of the agent).

**Performance metric**. A standard approach to solving the problem in (2) is to find a sequence of policies \(_{k}=\{_{k}( x):x^{n}\}\), \(k=1,2,\) such that the value functions following the policies \(_{k}\) will converge to \(V^{*}\), or be at least increasing in \(k\), i.e., demonstrating policy improvement.

Given a policy \(()\), let \((x,()):=_{}b(x,a)(a)a\) and \((x,()):=(_{}^{2}(x,a)(a) a)^{}\). Assume (for technical purpose) that \((x,())\) is positive definite for every \(x^{n}\). It is sometimes more convenient to consider the following equivalent SDE representation of (1):

\[_{s}=(_{s},(_{ s}))s+(_{s},( _{s}))_{s},_{0},\] (3)

in the sense that there exists a probability measure \(}\) which supports the \(m\)-dimensional Brownian motion (\(_{s},\,s 0\)), and for each \(s 0\), the distribution of \(_{s}\) under \(}\) agrees with that of \(X_{s}\) under \(\) defined by (1), see Appendix A. Note that the dynamics in (3) does not require external randomization. Also set \((x,):=_{}r(x,a)(a)a\) and \((x,):=_{}p(x,a,)(a)a\). We formally define the (state) value function given the feedback policy \(\{( x):x^{n}\}\) by

\[ V(x;)&:=[_{0}^{ +}e^{- s}[r(X^{}_{s},a^{}_{s})+ p(X ^{}_{s},a^{}_{s},( X^{}_{s}))] s X^{}_{0}=x]\\ &=[_{0}^{}e^{- s}[ (^{}_{s},(^{}_{s}))+ (^{}_{s},(^{}_{s})) ]s^{}_{0}=x],\] (4)

which, under suitable conditions on model parameters \((b,,r,p)\) and the policy \(\), is characterized by the Hamilton-Jacobi equation (see ):

\[ V(x;)-(x,) V(x;)-^{ 2}(x,)^{2}V(x;)-(x,)-(x,)=0.\] (5)

More technical details regarding the above formulation are spelled out in the Appendix.

We can now define the performance metric as follows:

\[():=_{^{n}}V(x;)(dx),\] (6)

so \(V^{*}=_{}()\). The main task of the continuous RL is to approximate \(_{}()\) by constructing a sequence of policies \(_{k}\), \(k=1,2,\) recursively such that \((_{k})\) is non-decreasing.

**Policy evaluation**. Let's first recall a general approach in , which can be used to learn the state value function in (4) or the performance metric in (6) for a given policy \(\). The idea is that for any \(T>0\) and a suitable test process \((_{t},\,t 0)\),

\[_{0}^{T}_{t}\,[V(X_{t}^{};)+r (X_{t}^{},a_{t}^{})t+ p(X_{t}^{},a_{t }^{},( X_{t}^{}))t- V(X _{t}^{};)t]=0.\] (7)

If we parameterize \(V(x;)=V^{}(x)\) and choose the special test function \(_{t}=(X_{t}^{})}{}\), stochastic approximation leads to the online update:

\[+(X_{t}^{})}{ }[V^{}(X_{t}^{})+r(X_{t}^{},a _{t}^{})t+ p(X_{t}^{},a_{t}^{},(  X_{t}^{}))t- V^{}(X_{t}^{ })t],\] (8)

where \(>0\) is the learning rate. This recovers the mean-squared TD error (MSTDE) method for policy evaluation in the discrete RL .

\(q\)**-value**. The Q-value function  and the advantage function  in discrete-time MDPs play a critical role in reinforcement learning theory and algorithms. However, as pointed out in , these concepts will not apply when the time interval shrinks to \(0\) (as in the continuous setting). To derive algorithms that fit the need of a continuous stochastic environment,  proposed the advantage _rate_ function. Namely, given a policy \(\) and \((t,x,a)[0,)^{n}\), consider a "perturbed" policy \(\) as follows. It takes the action \(a\) on \([t,t+ t)\) where \( t>0\), and then follows \(\) on \([t+ t,)\). The corresponding state process \(X^{}\) given \(X_{t}^{}=x\) is broken into two pieces. On \([t,t+ t)\), it is \(X^{a}\) which is the solution to

\[X_{s}^{a}=b(X_{s}^{a},a)s+(X_{s}^{a },a)B_{s},s[t,t+ t), X_{t}^{a}=x,\] (9)

while on \([t+ t,)\), it is \(X^{}\) following (3) with the initial time-state pair \((t+ t,X_{t+ t}^{a})\). With \( t>0\) as time discretization, the generalization of the conventional Q-function can be expressed as

\[Q_{ t}(x,a;)=V(x;)+[^{a}(x, { x}(x;),V}{ x^{2}}(x; ))- V(x;)] t+o( t).\] (10)

where \(^{a}(x,y,A):=b(x,a) y+^{2}(x,a) A+r(x,a)\) is the (generalized) Hamilton function in stochastic control theory . This motivates the following definition.

**Definition 1**.: _[_22_]_ _For a given policy \(\) and \((x,a)^{n}\), define the \(q\)-value as_

\[q(x,a;):=_{ t 0}(x,a;)-V(x;)}{  t}=^{a}(x,(x; ),V}{ x^{2}}(x;))- V (x;),\] (11)

which represents the instantaneous advantage rate of an action in a given state under a given policy.

## 3 Main Results

This section is concerned with theoretical developments. In Section 3.1, we define the (discounted) occupation time/measure which is the continuous analog of visitation frequency in MDPs. It is crucial in deriving the performance-difference formula in Section 3.2, which spins off two different algorithms - policy gradient and TRPO/PPO. In Section 3.3, we propose a local approximation for the performance metric, and derive a bound from which an MM algorithm is constructed.

### Discounted Occupation Time

Here we first provide an answer to **(Q1)** by defining the notion of _discounted occupation time_ for the continuous RL.

**Definition 2**.: _Let \(X=(X_{t},\,t 0)\) be governed by the SDE (3), and assume that it has a probability density function \(p^{}(,t)\) at each time \(t\). For each \(x^{n}\) and \(t 0\), define the \(\)-discounted occupation time of \(X\) at the state \(x\) by_

\[d_{}^{}(x):=_{0}^{}e^{- s}p^{}(x,s)ds.\] (12)

_So \(d_{}^{}()\) induces a finite measure on \(^{n}\) with a total mass of \(^{-1}\), which we call the discounted occupation measure._In probability theory, the definition in (12) is referred to as the \(\)-potential of \(X\), which gives discounted visitation frequencies of the state process. We record the following result, which will be useful in the derivation of the performance-difference formula. It is a consequence of the occupation time formula [41; 43].

**Lemma 1**.: _Under the conditions in 2, we have \(_{0}^{}e^{-_{}}(X_{s}) s=_{^{n}}d_{}^{}(x)(x)x\), for any measurable function \(:^{n}_{+}\) for which the expectation exists._

### Performance-Difference Formula

We are now ready to answer (**Q2**) by deriving the performance-difference formula between two policies in terms of the discounted occupation time in (12) and the \(q\)-values in (11).

**Theorem 2**.: _Given two feedback policies \(=\{( x):x^{n}\}\) and \(=\{( x):x^{n}\}\), we have:_

\[()-()=_{^{n}}d_{}^{}(x)[ _{}(a x)(q(x,a;)+ p(x,a,) )a]x.\] (13)

Proof sketch.: The full proof is detailed in Appendix B.1. The essence of the proof is to use the perturbation theory and properties of the discounted occupation time. Define an operator \(^{}:C^{2}(^{n}) C(^{n})\) associated with the diffusion process as:

\[(^{})(x):=-(x)+(x,) (x)+(x,)^{2}^{2} (x).\] (14)

Then the Hamilton-Jacobi equation that characterizes the state value function can be expressed as:

\[-^{}V(x;)=(x,)+(x,)\] (15)

Note that for any \( C^{2}(^{n})\), we have \(_{^{n}}d_{}^{}(y)(-^{})(y)y =_{^{n}}(y)(y)\). This allows us to express the performance difference in model-dynamics related terms:

\[()-()=_{}d_{}^{}(y)[( ^{}-^{})V(y;)+(y,)+ (y,)-(y,)-(y,)] y.\] (16)

What remains is to reduce the above to the desired result in (13). 

As discussed in Section 2, our main task is to construct (algorithmically) a sequence of policies \(_{k}\) along which the performance improves. Here we illustrate how some well known approaches of policy improvement (from \(\) to \(\)) are instances of the performance difference formula (13).

\(q\)**-learning and soft \(q\)-learning**. Since \(d_{}^{} 0\), we only need to ensure that for all \(x^{n}\), \(_{}(a x)(q(x,a;)+ p(x,a,) )a 0\). This boils down to the problem that for any \(x\), find \(v(|x)\) to maximize

\[_{}v(a)(q(x,a;)+ p(x,a,v))a.\] (17)

There are two special cases:

_(i)_ If \(p(x,a,v) 0\), then \(v=(a^{*})\) where \(a^{*}=_{a}q(x,a,)\). This is essentially the counterpart of \(Q\)-Learning  in the discrete time, which we call \(q\)-learning.

_(ii)_ If \(p(x,a,v)=-(v(a))\), this is known as the entropy regularizer [17; 59]. Concretely, we need to solve

\[_{v P()}_{}v(a)(q(x,a;)- v (a))a.\] (18)

which has a closed form solution with \(v^{*}(a)()\), i.e. \(v^{*}\) is the Boltzmann policy for \(q\)-functions. This is a "soft" (_a la_) version of the \(q\)-learning mentioned above.

**Policy gradient**. We may use function approximations to \(\) by a parametric family \(^{}\), with \(^{L}\). For simplicity, we write \(d_{}^{_{0}}\) (resp. \(()\)) for \(d_{}^{^{_{0}}}\) (resp. \((^{})\)). Setting \(=^{}\) and \(=^{_{0}}\) in (13) and taking derivative with respect to \(\) on both sides, we get the following result.

**Theorem 3** (Policy Gradient).: _The policy gradient at \(^{_{0}}\) is:_

\[_{}()_{=_{0}}=_{( x,a)}[_{}(^{}(a x))(q(x,a;^{ _{0}})+ p(x,a,^{_{0}}))+_{}p(x,a, ^{})],\] (19)

_where the expectation is w.r.t. \((x,a)( d_{}^{_{0}},^{_{0}})\), meaning \(x d_{}^{_{0}}()\) and then \(a^{_{0}}( x)\)._

The above formula is indeed the continuous analogue to the well-known PG formula (without regularization) in the MDP setting, where \(_{}()_{=_{0}}= _{(x,a)}[_{}(^{}(a x))A(x,a;^ {_{0}})]\) (), with \(A\) denoting the advantage function. Specifically, as a comparison, the formula in (19) replaces the visitation frequency by the occupation time, and the advantage function by the \(q\)-function, while keeping the same score function \(_{}(^{}(a x))\).

### Continuous TRPO/PPO

Leveraging the performance-difference formula derived above, we can now move on to spell out the continuous counterpart of TRPO and PPO originally developed in [45; 47] for the discrete RL.

**Local approximation function**: Given a feedback policy \(\), we define the _local approximation function_ to \(()\) by

\[L^{}()=()+_{^{n}}d_{}^{}(x)[_{ }(a x)(q(x,a;)+p(x,a,))a]x.\] (20)

Comparing (20) to the formula (13), we see that the difference is to replace \(d_{}^{}(s)\) with \(d_{}^{}(s)\). Observe that

\[\;L^{}()=(),\;_{}( (^{}))_{=_{0}}=_{}(L^{ ^{_{0}}}(^{}))_{=_{0}},\]

i.e. the local approximation function and the true performance objective share the same value and the same gradient with respect to the policy parameters. Thus, the local approximation function can be regarded as the first order approximation to the performance metric. Furthermore, similar to [23; 45], we can apply simulation methods to evaluate the local approximation function only using the data generated from the current policy \(\):

\[L^{}()=()+^{},( x))}{}[(a x)}{(a  x)}(q(x,a;)+ p(x,a,))].\] (21)

Next, we provide analysis and bounds on the gap \(()-L^{}()\), which can then be used to ensure policy improvement (similar to approaches in [45; 64] for discounted/average reward MDP). First, we need some technical conditions on the model dynamics.

**Assumption 1**.: _Assume the following conditions for the state dynamics hold true: (i) Global boundedness: There exists \(0<_{0}_{0}\) such that \(_{0}^{2} I^{2}(x,a)_{0}^{2} I\) for all \(x,a\); (ii) Uniformly Lipschitz: There exists \(C_{}>0\) such that \(\|(x,)-(x^{},)\|_{F} C _{}\|x-x^{}\|_{2}\) for all \(\) and \(x,x^{}\); (iii) Monotonicity (for drift) or growth condition: There exists \(C_{}>0\) such that \((x-x^{})^{}((x,)-(x^{},))  C_{}\|x-x^{}\|_{2}^{2}\) for all \(\) and \(x,x^{}\)._

The following lemma provides a Wasserstein-2 bound between the discounted occupation measures \(d_{}^{}()\) and \(d_{}^{}()\) for two policies \(\) and \(^{}\).

**Lemma 4**.: _Let \(,^{}\) be two feedback policies, and suppose the conditions in Assumption 1 hold. Define \(C_{,}:=2C_{}+1+2C_{}^{2}\) and \(C=_{x,a}|b(x,a)|^{2}+_{0}^{2}}{2_{0}^{2}}\). (Recall \(n\) is the dimension of the state.) Assume further that \(>C_{,}\) and \(C<\). Then there is the bound_

\[W_{2}( d_{}^{}, d_{}^{}),}(-C_{,})} (_{x}\|(|x)-(|x)\|_{1},_{x}\|( |x)-(|x)\|_{1}^{}).\] (22)

(The proof is deferred to Appendix B.3.) To derive a performance difference bound, define the Sobolev semi-norm as \(K:=\|f\|_{^{1}}:=(_{^{n}}| f(x)|^{2} x)^{}\), and its dual norm \(\|\|_{^{-1}}\) as\(\{|(,)|\ \|\ \|g\|_{^{1}} 1\}.\)[32; 40] show the equivalence of this dual norm \(\|-v\|_{^{-1}}\) to the Wasserstein-2 distance \(W_{2}(,v)\) for any probability measure \(\) and \(v\). Combining this fact with Lemma 4 yields the following result.

**Theorem 5**.: _Suppose the conditions in Lemma 4 hold, and further assume that \(d_{}^{\#}(x),d_{}^{}(x)\) are upper bounded by \(M\) for all \(x^{n}\). Define \(K:=\|f\|_{^{1}}\) with \(f(x;,):=_{A}(a x)(q(x,a;)+p(x,a, ))a\) and \(C(,,):=K}{2^{2}C_{,a}(-C_{,a})}(_{x,a}\|b(x,a)\|^{2}+_{}^{2}}{2 _{}^{2}})\). Assuming \(C(,,)<\), we have \(()^{}()\), where_

\[^{}():=L^{}()-C(,,) (_{x}D_{}((|x)\|(|x)),_{x} }((|x)\|(|x))}).\] (23)

Proof is given in Appendix B. By Theorem 5, we can use the minorization-maximization (MM) algorithm in [19; 23; 26], where \(^{}()\) is taken as the surrogate function for \(()\). Specifically, given the policy \(_{k}\), if we can indeed solve the optimization problem \(_{}^{_{k}}()\), and designate its solution as \(_{k+1}\). Then, we have

\[(_{k+1})^{_{k}}(_{k+1})^{_{ k}}(_{k})=(_{k})\] (24)

i.e., a guaranteed performance improvement. See also [26; Chapter 7] and  for the (global) convergence analysis of the MM algorithm (which exceeds the scope of this work). However, in general this optimization problem is not easy to solve directly since \(C(,,)\) is unknown because of the unknown underlying dynamics, and we may also have to work with sample based estimates of the approximation functions. In the spirit of [45; 47], we provide algorithms in the next section that can be practically implemented by incorporating an adaptive penalty constant \(C_{}\) as an alternative to \(C(,,)\). Consequently, the resulting algorithms may no longer strictly preserve the increasing performance of \(\) at each iteration, but overall increasing _trend_ will be clear (as demonstrated in Figure 3).

## 4 Algorithms and Experiments

### Sample-based Algorithms

Based on the analysis and results developed above, we provide sample-based estimates of the objective functions that lead to practical algorithms. Here we highlight several hyper-parameters: the learning rate \(\); the trajectory truncation parameter (time horizon) \(T\) (needs to be sufficiently large); the total sample size \(N\) or the sampling interval \(_{t}\), with \(N_{t}=T\). Also denote \(t_{i}:=i_{t}\), \(i=0,,N-1\), for the time points that we observe data from the environment.

``` Input: Policy parameters \(_{0}\), critic net parameters \(_{0}\), batch/sample size \(J\)
1:for\(k=0,1,2,\) until \(_{k}\) converges do
2: Collect a truncated trajectory \(\{X_{t_{i}},a_{t_{i}},r_{t_{i}},p_{t_{i}}\},i=1,,N\) from the environment using \(_{_{k}}\).
3:for\(i=0,,N-1\)do: Update the critic parameters as in (8)
4:for\(j=1,,,J\)do: Draw i.i.d. \(_{j}\) from \(()\), round \(_{j}\) to the largest multiple of \(_{t}\) no larger than it, and compute the GAE estimator of \(q(X_{_{j}},a_{_{j}})\) \[(X_{_{j}},a_{_{j}}):=(r_{_{j}}_{t}+e^{- _{t}}V(X_{_{j}+_{t}})-V(X_{_{j}}))/_{t}\] (25)
5: Get an estimator of \(_{j}(_{k})\) as \[[_{}(^{_{k}}(a_{_{j}}  X_{_{j}}))((X_{_{j}},a_{_{j}})+ p(X_ {_{j}},a_{_{j}},^{_{k}}))+_{}p(X_{ _{j}},a_{_{j}},^{_{k}})]\] (26)
6: Let \((_{k})=_{j=1}^{J}_{j}(_ {k})\) and perform PG update: \(_{k+1}=_{k}+(_{k})\) ```

**Algorithm 1** CPG: Policy Gradient with \(()\) random rollout

**Continuous Policy Gradient (CPG)**. To estimate the policy gradient (19) from data, we first sample an independent exponential variable \(()\) to get \((X_{}^{},a_{}^{})(d_{}^{_{0}},^ {_{0}}(|x))\). If there is a \(q\)-function oracle, then we can obtain an unbiased estimate of the policy gradient (of which the convergence analysis follows ). Lack of such an oracle, we employ the generalized advantage estimation (GAE) technique  to get \(q(X_{t},a_{t})(Q_{ t}(X_{t},a_{t};)-V(X_{t};))/ _{t}(r_{t}_{t}+e^{-_{t}}V(X_{t+_{t}})- V(X_{t}))/_{t}\). This yields the policy gradient Algorithm 1.

**Continuous PPO (CPPO)**. We now present Algorithm 2, a continuous version of the PPO, also as an approximation to the MM algorithm in Section 3.3. To do so, we need more hyperparameters: the tolerance level \(\), and the KL-divergence radius \(\). Moreover, we set \(_{}(|_{k}):=_{x d_{}^{ _{k}}}}(_{_{k}}(|x)\|_{}(|x))}\). (Empirically we find that taking average, instead of supremum, over \(x\) does not affect the algorithm performance while reducing computational burden, similar to what's observed in the discrete-time TRPO in .)

```
1:Input: Policy parameters \(_{0}\), critic net parameters \(_{0}\)
2:for\(k=0,1,2,\) until \(_{k}\) converge do
3: Follow the same as Steps 2-6 in Algorithm 1.
4: Compute policy update (by taking a fixed \(s\) steps of gradient descent) \[_{k+1}=_{}\{L^{_{k}}()-C_{}^{k}_{}(\|_{k})\}\] (27)
5:if\(_{}(_{k+1}\|_{k})(1+)\)then\(C_{}^{k+1}=2C_{}^{k}\)
6:elseif\(_{}(_{k+1}\|_{k})/(1+ )\)then\(C_{}^{k+1}=C_{}^{k}/2\) ```

**Algorithm 2** CPPO: PPO with adaptive penalty constant

Algorithm 2 is essentially a continuous analogue of the TRPO/PPO methods. Note that in the penalty term we use the mean square-root of the KL-divergence, since we choose the radius \(<1\); hence, the square-root distance will dominate in the bound in (23). Moreover, interestingly, throughout our primary experiments, using the square-root KL-divergence outperforms (using the KL-divergence itself). Refer to Appendix C,D for more details.

### Experiments

**LQ stochastic control**. Consider an environment driven by an SDE with linear state dynamics and quadratic rewards, with \(b(x,a)=Ax+Ba\), \((x,a)=Cx+Da\), where \(A,B,C,D\), \(p(x,a,)=-((a|x))\), and \(r(x,a)=-(x^{2}+Rxa+a^{2}+Px+Qa),\) where \(M 0,N>0,R,P,Q\). Linear-quadratic (LQ) control problems play an important role in the control literature, not only because it has elegant and simple solutions but also because more complex, nonlinear problems can be approximated by LQ problems. In general, we do not know the model parameters (e.g., \(A,B,\)), and the idea is to use continuous RL methods to find the optimal policy.

Here we adopt a Gaussian exploration parameterized by \(\) as: \(_{}( x)=(_{1}x+_{2},(_{3}))\), and we also parameterize the value function by \(\) as \(V_{}(x)=_{2}x^{2}+_{1}x+_{0}\). (In fact, as shown in [58, Theorem 4], the optimal exploration and value functions are of this form, and constants such as \(\) and \(\) can be computed explicitly given the model dynamics.) We randomly choose a set of initial constants, and compute the optimal \(^{*}\) and \(^{*}\) with respect to these parameters; refer to Appendix D.1 for more details. Figure 1,2 show the convergence of algorithms for one certain realized trajectory.

In Figure 1, we compute the \(l_{2}\) distance between the current policy parameters and the optimal ones, i.e. \(\|_{k}-^{*}\|_{2}\), which tracks the convergence of the policy parameters. In Figure 2, we plot the sample estimated KL divergence between the current policy \(_{k}\) (specified by \(_{k}\)) and \(^{*}\) (specified by \(^{*}\)), i.e. \(_{x_{k}^{A}}D_{}(_{_{k}}(|x)\| _{}(|x))\). The reason to consider the KL-divergence between \(_{k}\) and \(^{*}\) is that minimizing the KL-divergence to the optimal solution is equivalent to minimizing the distance between the current policy objective and the optimal objective (see Appendix D.1)). The experiments illustrate that our proposed algorithms do converge to the (local) optimum.

We also compare the performance of CPO and CPPO to the approaches that directly discretize the time, and then apply the classical discrete-time PG and PPO algorithms. See the details in Appendix D.4. The experiments show that our proposed CPO and CPPO are comparable in terms of sample efficiency, and in many cases they outperform the discrete-time algorithms under a range of time discretization.

**2-dimensional optimal pair trading**. We also consider the 2-dimensional optimal pair trading problem formulated in . The state space is \(X=(S,W)^{2}\) with \(X(0)=(s_{0},w_{0})\), where \(S\) represents the spread between two stocks, and \(W\) denotes the corresponding wealth process. The trader intends to maximize the total discounted reward, with the reward function \(r(X,a)=(1+W)\). The state dynamics are:

\[S_{t}=k(-S_{t})t+B_{t},W_{t}=a_{t}W_{t}(k(-S_{t})+^{2}++r_{f}) t+ W_{t}B_{t},\] (28)

We set \(p(x,a,) 0\), and add a constraint on the action: \(a_{t}[-,]\). (The action \(a_{t}\) is the position taken on the first stock, which can be long/positive or short/negative.) Since the action space is bounded and continuous, we consider a beta distribution for policy parameterization: \(_{}(a X):=f(,_{}(X),_{ }(X))\) with \(f(x,,):=x^ {-1}(1-x^{-1})\). For \(_{}\) and \(_{}\), we use a 3-layer neural network (NN) parameterized by \(\) for function approximation; and use another 3-layer NN for value function approximation. (More details are provided in Appendix D.2.)

Figure 3 shows that both algorithms, CPG and CPPO, converge to a local optimum (different between the two), and with an overall increasing trend over iterations. (Averaging is taken over \(100\) Monte Carlo estimates for each policy evaluation.)

Figure 3: Performance of both algorithms to the task

Conclusion and Further Works

We have developed in this paper the basic theoretical framework for policy optimization in continuous RL, and illustrated its potential applications using numerical experiments.

For further research, two topics are high on our agenda. First, we plan to study the convergence (rate) of the continuous policy gradient and TRPO/PPO, vis-a-vis the error due to the time increment \(_{t}\). Our conjecture is that it is likely to be polynomial-bounded under mild assumptions, similar to the analysis in ), thus extending beyond the condition required by  and . Second, for the bounds on the statistical distance and the performance difference, we want to further develop a consistent bound like the one in  (for the discrete setting), i.e., one that remains meaningful when the discount factor \( 0\).