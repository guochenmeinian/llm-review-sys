# Infinite-Dimensional Feature Interaction

Chenhui Xu\({}^{1,2}\)   Fuxun Yu\({}^{1,3}\)   Maoliang Li\({}^{4}\)   Zihao Zheng\({}^{4}\)

Zirui Xu\({}^{1}\)   Jinjun Xiong\({}^{2,}\)   Xiang Chen\({}^{1,4,}\)

\({}^{1}\)George Mason University  \({}^{2}\)University at Buffalo

\({}^{3}\)Microsoft  \({}^{4}\)Peking University

{cxu26,jinjun}@buffalo.edu, xiang.chen@pku.edu.cn

Corresponding Author.

###### Abstract

The past neural network design has largely focused on feature _representation space_ dimension and its capacity scaling (e.g., width, depth), but overlooked the feature _interaction space_ scaling. Recent advancements have shown shifted focus towards element-wise multiplication to facilitate higher-dimensional feature interaction space for better information transformation. Despite this progress, multiplications predominantly capture low-order interactions, thus remaining confined to a finite-dimensional interaction space. To transcend this limitation, classic kernel methods emerge as a promising solution to engage features in an infinite-dimensional space. We introduce InfiNet, a model architecture that enables feature interaction within an infinite-dimensional space created by RBF kernel. Our experiments reveal that InfiNet achieves new state-of-the-art, owing to its capability to leverage infinite-dimensional interactions, significantly enhancing model performance.

## 1 Introduction

In the past decade, deep neural network architecture design has experienced several major paradigm shifts regarding the feature representation learning. As shown in Fig. 1(a), the early stage of neural network design is dominant by flat stream architectures in the form of _weight-feature interaction_ (e.g., \(W\)), like multi-layer perceptron (MLP), convolution neural networks (CNN), ResNet, etc. These models usually adopt linear superposition (e.g., \(W_{i} W_{j}\))1 in the feature representation space. Therefore, the feature representation space scaling is limited to increase model channel width and depth [14; 19]. Nevertheless, this scaling approach has witnessed model development from the very small-scale MLPs or LeNet to the recent huge ConvNext V2 . With the ultra-scaled parameter amounts, computing complexity, and model size, the return of investment on model performance by further scaling feature dimensions has largely plateaued .

Despite the plateau in feature representation space, recent sporadic architecture design works  shed light on another potential dimension of scaling: feature interaction space. Specifically, as shown in Fig. 1(b), these neural network designs generally demonstrate _feature-feature interaction_ (e.g., \(W_{i} W_{j}\)). As a mathematical example, the self-attention mechanism in Transformers  can be formulated as \(^{L+1}=f_{k}(^{L}) f_{q}(^{L}) f_ {v}(^{L})\), which is also element-wise multiplication between processed input feature themselves. Characterized by element-wise interaction, these designs offer complementary feature correlation capabilities in addition to simple linear superposition. Such feature interactions have become the essential mechanisms of mainstream state-of-the-art neural architectures. For example, it's implemented in SENet with squeeze and excitation , non-local network with transposed multiplication , vision transformers with self-attention [4; 11; 38], gated aggregation [22; 46; 35], and quadratic neurons [12; 44; 45].

Although these models greatly improve the performance of state-of-the-arts, as mentioned above, these works provide diversified explanations that neglect the underlying shared design of element-wise feature multiplication operation , and thus may fail to reveal the fundamental source of improvement. To provide both explainability and quantifiability, in this paper, we propose a unified theoretical perspective to rethink the feature interaction scaling, i.e., _the dimensionality of feature interaction space_. For example, as shown in Fig 1(b), by employing the \(\), element-wise multiplication, an implicit interactive quadratic space \(=(x_{1}^{2},x_{1}x_{2},,x_{n-1}x_{n},x_{n}^{ 2})\) with degrees of freedom \(n(n+1)/2\) is constructed from the original representative vector space \(=(x_{1},x_{2},,x_{n})\) with degrees of freedom \(n\). Such space dimensionality scaling is the key to improving feature representation quality and end-task, as we will show later.

From the unified feature dimensionality perspective, a new opportunity emerges in neural architecture design, that is to scale to _infinite-dimensional feature interactions_ than former methodologies (e.g., from \(n(n+1)/2\) to \(_{k}\)). However, scaling feature interaction space dimensionality from architectural enhancements (e.g., quadratic, self-attention, and recursive gates) comes with linear scaling cost w.r.t. interaction order \(k\), which hinders the infinite dimensionality increase . Thus, there is an open question:

_How can we efficiently extend interactions to an infinite-dimensional space?_

Inspired by traditional machine learning, we propose an approach that introduces kernel methods for feature interaction in neural networks. We define a set of feature interactions between features \(W_{a}\) and \(W_{b}\) with a kernel function \(K(W_{a},W_{b})\) instead of the element-wise multiplication. As shown in Fig 1(c), the kernel method transforms the feature to an ultra-high dimensional space by an implicit mapping \(()\). From there, the feature interaction space is defined by the inner product on the Reproducing Kernel Hilbert Space (RKHS) \(\) constituent with the kernel function \(K(,)\). The RKHS can greatly expand the interaction space at very little cost, enabling infinite dimensions.

We then propose InfiNet, a novel family of neural networks that generate high-quality feature representations. Specifically, we introduce the Radial Basis Function (RBF) kernel  as a replacement of the common \(\) or \(\) operations. With the infinite series expansion in RBF kernel, it enables a theoretical provable dimensionality approximation \(_{j,_{k}n_{k}=j}\{^{n_{1}} x_{n _{k}}^{n_{k}}}{ n_{k}}}\}\) while with as low-overhead as evaluating an exponential function. In this way, InfiNet enables efficient infinite-dimensional feature interaction space scaling upon a finite set of branches in the model architecture, thus achieving better complexity performance tradeoffs than prior state-of-the-art.

**Contributions.** We make the following contributions:

* We unify the perspectives of recent feature interactive works and identify a novel direction of neural network performance scaling: the feature interaction space dimensionality.
* We propose a method to expand the feature interaction space to an infinite dimension with RBF kernel, that can effectively model the complex implicit correlations of features.
* We propose InfiNet, a novel series of neural networks that explore the neural interaction from infinite-dimensional space, and achieve state-of-the-art performance.

Figure 1: (a) Traditional feature representation without interaction . (b) Recent work with finite feature interaction . (c) Our method: Kernel-enabled infinite feature interaction.

Extensive ablation studies verify that the shift from finite feature interaction space to infinite one is a key factor to learn better representations and therefore improving model performance. Meanwhile, large-scale experiments on ImageNet classification, MS COCO detection and ADE20K segmentation also demonstrate InfiNet design's effectiveness, which consistently outperforms the state-of-the-art flat stream networks [14; 26] and finite-order interaction networks [25; 35].

## 2 Related Work

**Interactions in Neural Networks.** Interaction in neural networks has undergone a long period of change. During the evolution from AlexNet  to ResNet , the model has for long followed the principle of summing the weighted pixels at each position in the layer-by-layer feature iteration. And then, as the potential of the attention mechanism was realized, model development is towards an element-wise multiplication way. This trend is punctuated by the emergence of models such as Non-Local , Transformer , and ViT . Recent insights suggest that the crux of the attention mechanism lies in high-order information interactions, rather than the mechanism of "Attention" itself [4; 35; 43]. This revelation has spurred the development of innovative neural network architectures. For example, HorNet , QuadraNet  and MogaNet  examines high-order models from the perspective of spatial interactions through multiplication-integrated architecture design. However, limited by the support of existing deep learning platforms such as Pytorch , few attempts have been made to extend the model's feature interaction to an ultra-high dimensional situation through the kernel method for more potential.

**Kernel Methods in Neural Networks.** The fundamentals of kernel methods are well-studied in traditional ML domains like support vector machines  but they are less used in neural architectures. Early extensions to deep learning through the kernelized perceptron  have improved the performance but mainly in shallow neural networks. Recent advancements include the development of Convolutional Kernel Networks (CKN) , which merge CNNs' robust feature learning with kernel stability. This approach offers a theoretical foundation for deep learning's application in structured data. Additionally, the introduction of Kervolutional Neural Networks  replaces traditional convolution in CNNs with kernel-based operations to enhance feature extraction without excessive computational costs. The combination of Gaussian processes with neural networks to create Deep Kernel Learning  adjusts model complexity based on data while maintaining Bayesian inference. The neural tangent kernel (NTK)  framework establishes a direct connection between infinitely wide neural networks at initialization and kernel methods. Specifically, NTK shows that as the width grows, the network's training dynamics can be described by a kernel function, linking the neural network's behavior to that of kernel methods. Random features  provide an efficient approximation to the feature mappings used in kernel methods. By random projections, one can approximate the inner product defined by a kernel function, making it feasible to apply kernel methods [3; 13].

Despite these innovations, a significant limitation remains: Although these methods expand the feature representation space, they fail to scale up the feature interaction space, limiting the network to aggregate information in a superposition manner. As a result, these methods also fall short of potential feature interactions and thus are incapable of handling complex data correlations and functionalities.

## 3 From Feature Representation Space to Interaction Space

We start with considering the transformation of feature representation of a normal shape-preserving 2-layer perceptron block. Given an input \(=(x_{1},x_{2},,x_{n})\) with n-dimension. We denote the first layer transformation as \(=g()=(W_{1})\) (we omit the bias term for simplicity), and the second layer transformation as \(h()=W_{2}\). Therefore the whole block is a mapping \(h g:^{n}^{n}^{n}\) from feature space \(^{n}\) to a middle feature representation space \(^{n}\) and eventually to a output space \(^{n}\). Expanding the width of the neural networks, for example with a coefficient 2, is going to expand the feature representation space to \(^{2n}\). The core idea of this dimensional expansion is that implicit associations in features in low dimensions will be expressed explicitly when projected into a high-dimensional space. Model architecture (i.e. convolution, multi-branch ) is an additive superposition of pixels, essentially no different from the perceptron in terms of representation space.

### Feature Interaction Space

Although flat stream neural networks are defined by linear transformations and activations, modern network design also introduces multiplications in the network structure or neurons. They are called attentions from an interpretable point of view, that is the degree of interest of one position in relation to another; from a more abstract point of view, they are called interactions in spatial contexts. But invariably, these are realized with element-wise multiplication. We conduct the following definition.

**Definition 1** (Feature Interaction) A Feature interaction refers to transformations between features with the same or different positions defined by element-wise multiplication.

For example, Star Operation  is a basic 2-order feature interaction, defined as \((W_{a})*(W_{b})\), where \(W_{a},W_{b}^{n}\). This is a simple element-wise multiplication fusion of two linear transformations of the ordinary input \(\). We write the expansion of such multiplication operation:

\[y=(W_{a})*(W_{b})=(_{i=1}^{n}w_{ai}x_{i})(_{j=1}^{ n}w_{bj}x_{j})=_{i j}_{i,j}x_{i}x_{j}\] (1)

where \(_{i,j}=w_{ai}w_{bj}+w_{aj}w_{bi},\;i j\), and \(_{i,i}=w_{ai}w_{bi}\). Then we vectorize \(\) and \(x_{i}x_{j}\):

\[A=[_{1,1},_{1,2},_{2,2},,_{n-1,n},_{n,n}] ^{n(n+1)/2}\] (2)

\[=[x_{1}x_{1},x_{1}x_{2},x_{2}x_{2},,x_{n-1}x_{n},x_{n}x_{n}]\] (3)

\(\) can therefore define a basis of a space. Thus the output of the current layer can be rewritten as:

\[y=_{i j}_{i,j}x_{i}x_{j}=A.\] (4)

From the independence of the pixel level, we know that each term in \(\) is linearly independent, this indicates every dimension in \(\) is an individual dimension. Given a set of basis vectors \(\), we define \(()\) as the feature interaction space. In this way, the generation of the next layer of features \(y\) is constituted by a linear superposition \(A\) on the feature interaction space \(()\) like in Eq.(4).

### Dimension of Feature Interaction Space

Now, we consider the number of dimensions of a feature interaction space. Given \(k\)-1 multiplication operations, we first define the _k_-order feature interaction space as follows:

**Definition 2** (Feature Interaction Space) A _k_-order Feature Interaction Space \(^{k}\) refers to the span of monomial basis \(\{x_{1}^{d_{1}}x_{2}^{d_{2}} x_{n}^{d_{n}}\,|\, d_{i}=k,d\}\) defined by a _k_-order Feature Interaction.

An element-wise multiplication generates a feature interaction space of \(n(n+1)/2\) dimension, which is the number of elements of an upper triangular matrix. In general, considering the symmetry of the interactions and elements, for a k-order interaction on the \(n\)-dimensional feature, the dimension of the corresponding feature interaction space is:

\[dim(^{k})=\] (5)

The next layer of feature generation based on the feature interaction space greatly expands the spatial dimensions to which the features are mapped compared to the original model based only on the feature representation space, i.e., it is possible to explore the feature's non-linearity in high-dimensional space. It is worth noting that this process introduces terms like \(x_{1}x_{2}\), an interaction that cannot be captured by traditional plane networks in feature representation space.

For example, we consider the Self-Attention in the Transformers . The Self-Attention contains two element-wise multiplications, so it explores a 3-order feature interaction space. This is due to the fact that: (1) in the first stage, in the query-key dot-product attention map computation \(Att()=Q K^{T}=W_{Q}^{T}W_{K}^{T}\) explores the feature interaction space \(=(x_{1}^{2},x_{1}x_{2},,x_{n-1}x_{n},x_{n}^{2})\). (2) In the second stage, the multiplication between the attention map and the value \(y=(Att()) V=(Att()) W_{v}\) explores the feature interaction space \(=^{n}=(x_{1}^{3},x_{1}^ {2}x_{2},x_{1}x_{2}x_{3},,x_{n-1}x_{n}^{2},x_{n}^{3})\), which has \((n+2)(n+1)n/6\) dimensions in line with Eq.(5). Thereby we explain, from the perspective of feature interaction space, why the transformer family of models has, so far, generally outperformed recurrent neural networks and convolutional neural networks in various domains.

Expanding Interaction Space to Infinite Dimension

This element-wise multiplication-based interaction, since the construction of each order of the feature interaction space is based on a multiplication operator, leads to a problem in that feature interaction space expansion is still difficult. This is due to the fact that these interaction operators is explicit mapping, which tends to have quadratic or higher complexity w.r.t the input length (e.g. self-attention ) or lengthy recursive designs (e.g. HorNet ) and linear complexity w.r.t interaction order. But the problem with building this mapping explicitly is: (1) The complexity of mapping itself. The computational overhead associated with defining a set of explicit nonlinear mappings is non-negligible. A mapping from \(C\) channels to \(C^{}\) channels means a computation complexity of \(O(CC^{})\). (2) The complexity of interaction. The complexity of the inner product used to interact with the two sets of features increases dramatically to \(O(C^{})\) when the dimension is raised. Considering \(C^{}>>C\), these two complexities will largely increase the computational overhead of the networks.

We would like to obtain a method that can expand the dimension of feature interaction space as much as possible in \(O(1)\) time. Fortunately, the machine learning community has already given a method for increasing the dimension of a feature defined on the inner product: **kernel methods**.

### Expanding Interaction Space with Reproducing Kernel

The nature of kernel methods is that they are substitutions for inner product operations. This requires combining element-wise multiplication and summation to define a set of inner products within the network. For this purpose, we rewrite the form of element-wise multiplication in Eq.(1) on two groups of features, which is a common design in literature architecture (e.g. multi-head self-attention ). It will therefore be a superposition of multiple interaction in the format:

\[y=_{i=1}^{C}W_{ai}*W_{bi}=} ,}\] (6)

where \(}=[W_{a1},W_{a2},,W_{aC} ]\), \(C\) is the number of branches. Thereby we generalize the form of the feature interaction from element-wise multiplication to inner product which is a multi-branch paradigm. At this point, we have \(}^{C}\), while it is generated from a feature representation space \(^{n}\). In order to extend the interaction space, we need to further project \(}\) and \(}\) to a high-dimensional space. An obvious way to do this is to construct an implicit mapping \(()\) to a high-dimensional space, so we can compute \((W_{a}),(W_{b})\) for interaction.

By Mercer's Theorem , taking a continuous symmetric positive semi-definite function \(K(s,t)\), there is an orthonormal basis \(\{_{i}()\}\), \(i=0,1,,\), consisting of eigenfunctions of function \(K(,)\) such that the corresponding sequence of eigenvalues \(\{_{i}\}\) is non-negative. These means:

\[K(s,t)=_{i=1}^{}_{i}_{i}(s)_{i}(t),\] (7)

where \( i j, s\) and \(t,_{i}(s),_{j}(t)=0\) since. Then we construct a Hilbert space \(\) with the orthonormal basis \(\{}_{i}()\}\). Consider a vector \(f=(f_{1},f_{2},)_{}^{T}\) on the space \(\), then we have:

\[f=_{i=1}^{}f_{i}_{i}_{i}().\] (8)

Thus for a vector \(K(s,)\) in space \(\):

\[K(s,)=_{i=1}^{}_{i}_{i}(s)_{i}()=_{i=1 }^{}}_{i}(s)}_{i}()=( }_{1}(s),}_{2}(s),)_{}^{T}.\] (9)

Therefore, for the Hilbert space \(\), we can define the reproducing kernel by:

\[ K(s,),K(t,)=_{i=1}^{}} _{i}(s)}_{i}(t)=_{i=1}^{}_{i}_{ i}(s)_{i}(t).\] (10)

**Implicit Mapping to High-Dimensional RKHS**. Let \((s)=K(s,)\), then we have \((s),(t)=K(s,t)\). The Hilbert Space \(\) is known as the Reproducing Kernel Hilbert Space (RKHS) corresponding to kernel function \(K(,)\). Note that the \(()\) is therefore defined on the RKHS \(\), which can be infinite-dimensional given specific kernel \(K(,)\). The mapping \(()\) does not have to have an explicit expression since we can get the result of \((s),(t)\) by computing \(K(s,t)\). This means that we can achieve an extension of the dimensions for the feature interaction space by simply replacing the inner product \(x},x}\) used in the interaction with a kernel \(K(x},x})\).

### Infinite-Dimensional Feature Interaction with RBF Kernel

To maximize the dimension of the feature interaction space, we consider Radial Basis Function (RBF) Kernel \(K_{}(,)=(-\|- \|_{2}^{2})\)with an infinite-dimensional RKHS, given the fact that:

\[(-\|-\|_{2}^{2 })=_{j=0}^{}^{})^{j }}{j!}(-\|\|_{2}^{2}+\|\|_ {2}^{2})\] (11) \[= _{j=0}^{}_{n_{1}+n_{2}++n_{k}=j}(- \|\|^{2})^{n_{1}} s_{k}^{n_{k}}}{ ! n_{k}!}}(-\|\|^{2} )^{n_{1}} t_{k}^{n_{k}}}{! n_{k}!}}\] (12) \[= _{}(),_{}(),\] (13)

where \(_{}()=_{j=0}^{}_{_{k}n_{k}=j} (-\|\|^{2})^{n_{1}} s_{k}^{n_ {k}}}{! n_{k}!}}_{j,_{k}n_{k}=j }\{^{n_{1}} x_{k}^{n_{k}}}{! n_{k}!}}\}\).

**Infinite-dimensional Feature Interaction Space** Observing the RKHS of such a RBF kernel, \(_{j,_{k}n_{k}=j}\{^{n_{1}} x_{k}^ {n_{k}}}{! n_{k}!}}\}\), We note that each of its dimensions is a j-order interaction within the feature \(\), given the fact one of the bases of this RKHS is:

\[\{1,x_{1},,x_{n},x_{1}^{2},,x_{1}x_{n},,x_{n}^{2},,x_ {1}^{j},x_{1}^{j-1}x_{2},,x_{n}^{j},\},\] (14)

which contains an all-order monomial among all elements of the feature \(\). This means that we get an infinite-dimensional Hilbert space for the superposition of interaction information through such an RBF kernel, and most importantly, each dimension of this space is defined by a feature interaction. Therefore, we get an infinite-dimensional feature interaction space.

### Demo Case Performance of Models on Different Feature Space

In order to compare networks that utilize the summing superposition of information mappings on the feature representation space, finite feature interaction space, and infinite-dimensional interaction space, we design a demo network with 8 DemoBlock, as shown in Fig. 2. DemoBlock has a two-group design, the only difference among models in different spaces is the interaction method at the end of the block (add for simple representation, multiplication for finite interaction, and RBF kernel for infinite dimensional interaction), the demo models are trained on CIFAR10 and Tiny-ImageNet.

As shown in Fig. 2, with the procedure of transferring from the simple feature representation space to a finite feature interaction and eventually to an infinite-dimensional interaction, the performance

Figure 2: Comparison of simple representation, finite interaction, and infinite-dimensional interaction. The **?** circle in DemoBlock is chosen from element-wise Add, element-wise Mul. or RBF kernel.

on CIFAR10 of the networks is growing. This is done throughout the training process, implying the superiority of feature iteration in a high-dimensional interaction space. The right side of Fig. 2 shows the Class Activation Mapping  of three different demo nets on Tiny-ImageNet. From this, we can see that the network of feature interaction better reflects the pixel-level correlation within the image.

## 5 Method

### InfiBlock: Infinite-Dimensional Spatial Feature Interaction

**InfiBlock.** In this section, we present InfiBlock, the basic block to build the high-performant InfiNet architecture to achieve infinite-dimensional spatial feature interaction. As presented in Fig. 3(b), InfiBlock starts with a LayerNorm layer and subsequently transforms the feature into separate representations of the two groups through two different linear layers. Then InfiBlock utilizes a depth-width convolution with an expansion coefficient of \(r\) and a ReLU activation to obtain a feature branch vector of length \(r\) on each group. The feature branch vectors on both groups are then fed into an RBF kernel for feature interaction. At the same time, we retain a pathway containing only one depth-wise convolution for summing superposition over the feature representation space to ensure that the linear connections between features are not neglected and overfitted. This is followed by a residual connection with the original input values after passing through a two-layer MLP. Starting with a input \(X^{l}^{HWC}\), Infi-Block can be formulated as:

\[^{l}=(X^{l}),\] (15) \[[^{l}_{a},^{l}_{b},Z^{l}_{c}]=[(_ {a}(^{l}W_{a})),(_{b}(^{l}W_{b})),_ {c}(^{l})],\] (16) \[X^{l+1}=((K_{rbf}(^{l}_{a}, ^{l}_{b})+Z^{l}_{c}))+X^{l},\] (17)

where \(K_{rbf}(^{l}_{a},^{l}_{b})=exp(-^{l}_{a}- ^{l}_{b}\|_{2}^{2}}{2})^{HWC}\), is an RBF kernel with out any hyper-parameter, and \(\|\|_{2}^{2}\) is squared L-2 norm. \(_{a,b}(^{l}W_{a,b})^{HWC*r}\) expands the number of channels of feature to r times, where \(W_{a,b}\) preserve the shape of the feature. The discretization of the feature is performed in a depth-width convolution. LayerNorm\(()\) is layer normalization  and \(()\) is activated by GELU .

Specifically, following the ConvNeXt , we select a \(7 7\) depth-wise convolution to obtain a larger receptive field. We set the expansion coefficient \(r\) as 7 in practice. This means:

\[[^{l}_{a},^{l}_{b}]=[[Z^{l}_{a1},Z^{l}_{a2},,Z^{l}_{a7}]^ {T},[Z^{l}_{b1},Z^{l}_{b2},,Z^{l}_{b7}]^{T}],\] (18)

where \(Z^{l}_{ai}=(_{ai}(X^{l}W_{a}))\ ^{HWC}\). Therefore, the RBF kernel performs feature interaction on multiple (7) branches of convolution filter and outputs a result still in \(^{HWC}\).

### Model Architectures

As shown in Fig. 3(a), InfiNet uses the widely adopted 4-stage hierarchical architecture as in ResNet , ConvNeXt  and Swin Transformer . We stack InfiBlocks into each stage.

Figure 3: **Overview of InfiNet. (a) Four-stage hierarchical InfiNet design. (b) InfiBlock Design**

We set the number of channels in each stage as [C,2C,4C,8C] following common practice. We build a family of InfiNets that InfiNet-T/S/B/L/XL with model variants hyper-parameters: \(C=\{64,96,128,128,192\}\), number of blocks in each stages \(=\{2,2,18,2\}\) for InfiNet-T/S/B and \(\{3,3,27,3\}\) for InfiNet-L/XL. A down-sampling with \(2 2\) convolution with stride = 2 is used to connect each stage. STEM  is used to connect the input of InfiNet to Stage 1.

## 6 Experiments

We perform a series of experiments to validate the efficacy of InfiNets. The primary results on ImageNet  are showcased and contrasted with several architectures. Furthermore, our models were evaluated on downstream ADE20K semantic segmentation and COCO  object detection. ImageNet-1K experiments are conducted on 4\(\)Nvidia A100 GPUs and ImageNet-21K on 16\(\).

### ImageNet Classification

**Setups.** We conduct image classification experiments on ImageNet-1K , which contains 1.28 million training samples belonging to 1000 classes and 50K samples for validation. We train the InfiNet-T/S/B/L models for 300 epochs with AdamW  optimizer. We use the cosine learning rate scheduler  with 20 warmup epochs and the basic learning rate is set as \(4 10^{-3}\). The training resolution is set as \(224 224\). To further evaluate the InfiNet's scalability, we train the InfiNet-L/XL models on 14M-sample ImageNet-22K dataset for 90 epochs and then fine-tune on ImageNet-1K at \(384 384\) resolution for 30 epochs following . More details can be found in Appendix A.1.

**Results.** We present our ImageNet experiment results and comparison with baselines in Table 1. Our models achieve competitive performance with state-of-the-art. It is worth noting that InfiNet has about 20% fewer FLOPs2 than the baseline models with similar parameter scales, but still achieves great performance. It demonstrates the effectiveness of our proposed generation of features in infinite-dimensional interaction spaces. Experiments on isotropic models can be found in Table 3(a).

    &  & FLOPs & Top1 \\  & Orders & (M) & (G) & Acc.(\%) \\  ConvNeXt-T & no & 29 & 4.5 & 82.1 \\ SLaK-T & no & 30 & 5.0 & 82.5 \\ ConvFormer-T & 2 & 27 & 4.4 & 83.1 \\ UniFormer-S & 2 & 22 & 3.6 & 82.9 \\ CoAtNet-0 & 3 & 25 & 4.2 & 82.7 \\ FocalNet-T & 3 & 28 & 4.4 & 82.1 \\ Swin-T & 3 & 28 & 4.5 & 81.3 \\ HorNet-T & 2-5 & 22 & 4 & 82.8 \\ MogasNet-S & 4 & 25 & 5.0 & 83.5 \\
**InfiNet-T** & \(\) & 23 & 3.2 & 83.4 \\  ConvNeXt-S & no & 50 & 8.7 & 83.1 \\ SLaK-S & no & 55 & 9.8 & 83.8 \\ Conv2Former-S & 2 & 50 & 8.7 & 84.1 \\ UniFormer-B & 2 & 50 & 8.3 & 83.9 \\ CoAtNet-1 & 3 & 42 & 8.4 & 83.3 \\ FocalNet-S & 3 & 50 & 8.7 & 83.5 \\ Swin-S & 3 & 50 & 8.7 & 83.0 \\ HorNet-S & 2-5 & 50 & 8.8 & 84.0 \\ MogasNet-B & 4 & 44 & 9.9 & 84.3 \\
**InfiNet-S** & \(\) & 48 & 7.2 & 84.0 \\    
    &  & FLOPs & Top1 \\  & Orders & (M) & (G) & Acc.(\%) \\  ConvNeXt-B & no & 89 & 15.4 & 83.8 \\ SLaK-B & no & 85 & 17.1 & 84.0 \\ ConvFormer-B & 2 & 90 & 15.9 & 84.4 \\ CoAtNet-2 & 3 & 75 & 15.7 & 84.1 \\ FocalNet-B & 3 & 89 & 15.4 & 83.9 \\ Swin-B & 3 & 89 & 15.4 & 83.5 \\ HorNet-B & 2-5 & 87 & 15.6 & 84.3 \\ MogasNet-L & 4 & 83 & 15.9 & 83.5 \\ 
**InfiNet-B** & \(\) & 82 & 12.8 & 84.5 \\
**InfiNet-L** & \(\) & 116.8 & 19.1 & 84.8 \\    
   \)_} & ^{2}\)} & no & 198 & 101 & 87.5 \\ CoAtNet-3 & 3 & 168 & 107 & 87.6 \\ FocalNet-1 & 3 & 197 & 101 & 87.3 \\ Swin-L\({}^{2}\) & 3 & 197 & 104 & 87.3 \\ HorNet-L\({}^{3}\) & 2-5 & 202 & 102 & 87.7 \\ MogasNet-XL\({}^{3}\) & 4 & 181 & 102 & 87.8 \\ 
**InfiNet-L\({}^{4}\)** & \(\) & 116.8 & 60 & 87.8 \\ ConvNeXt-XL\({}^{2}\) & no & 350 & 179 & 87.8 \\
**InfiNet-XL\({}^{4}\)** & \(\) & 255.8 & 126 & 88.2 \\   

Table 1: **ImageNet classification results.** We compare our models with state-of-the-art models with comparable parameters, the Top-1 accuracy is reported on the ImageNet-1K validation set.

### MS COCO Detection

**Setups.** We evaluate our models for object detection tasks on widely used MS COCO  benchmark. In the experiments, InfiNet-T/S/b/XL serves as the backbone network within Cascade Mask RCNN . We use AdamW as the optimizer and a batch size of 16 and adhere to the 3\(\) schedule, following ConvNeXt  and Swin . We resize the input so that the longer side is at most 1333 and the short side is at most 800. We initialize the backbone model with ImageNet-1K pre-trained weights for T/S/B models and ImageNet-22K pre-trained weights for XL model.

**Results.** As shown in Table 2, InfiNets comprehensively beat the non-interactive model ConvNeXt , and space-limited interactive Swin  and HorNet  under the same cascade Mask RCNN framework in box AP and mask AP. This means that for such dense prediction tasks, spatial interaction of features in a high-dimension space is crucial. The InfiNet series model obtain\(0.9 1.5\) box AP and \(1.3 2.5\) mask AP gain compared with non-interactive ConNeXt.

### ADE20 Segmentation

**Setups.** We evaluate our models for the semantic segmentation task on widely used ADE20K  benchmark covering 150 semantic categories on 25K images, in which 20K are used for training. We use UperNet  for as the basic framework and adopt InfiNets as the backbone model. Training details follow the Swin , we use AdamW optimizer with learning rate \(1 10^{-4}\) and batch size 16.

**Results.** The right half of Table 2 lists the mIoU and corresponding model size and FLOPs for different configurations. Our models beat most of the baseline in the segmentation task. The results show that as the model size increases, the performance gap between InfiNet and other baselines is getting larger, illustrating the scalability of InfiNet on segmentation.

    &  &  \\  Model & AP\({}^{}\) & AP\({}^{}\) & Params & FLOPs & mIoUs\({}^{}\) & mIoUs\({}^{}\) & Params & FLOPs \\  ConvNeXt-T & 50.4 & 43.7 & 86M & 741G & 46.0 & 46.7 & 60M & 939G \\ Swin-T & 50.4 & 43.7 & 86M & 745G & 44.5 & 45.8 & 60M & 945G \\ HornNet-T & 51.7 & 44.8 & 80M & 730G & 48.1 & 48.9 & 52M & 926G \\
**InfiNet-T** & 51.9 & 46.2 & 77M & 724G & 46.7 & 47.4 & 50M & 924G \\  ConvNeXt-S & 51.9 & 45.0 & 108M & 827G & 48.7 & 49.6 & 82M & 1027G \\ Swin-S & 51.8 & 44.7 & 107M & 838G & 47.6 & 49.5 & 81M & 1038G \\ HorNet-S & 52.7 & 45.6 & 107M & 830G & 49.2 & 49.8 & 81M & 1030G \\
**InfiNet-S** & 52.8 & 46.4 & 98M & 802G & 49.4 & 49.9 & 78M & 1002G \\  ConvNeXt-B & 52.7 & 45.6 & 146M & 964G & 49.1 & 49.9 & 122M & 1170G \\ Swin-B & 51.9 & 45.0 & 145M & 982G & 48.1 & 49.7 & 121M & 1188G \\ HornNet-B & 53.3 & 46.1 & 144M & 969G & 50.0 & 50.5 & 121M & 1174G \\
**InfiNet-B** & 53.7 & 47.3 & 126M & 906G & 50.2 & 50.9 & 105M & 1111G \\  ConvNeXt-L\({}^{}\) & 54.8 & 47.6 & 255M & 1354G & 53.2 & 53.7 & 235M & 2458G \\ Swin-L\({}^{}\) & 53.9 & 46.7 & 253M & 1382G & 52.1 & 53.5 & 234M & 2468G \\ HorNet-L\({}^{}\) & 55.4 & 48.0 & 251M & 1363G & 54.1 & 54.5 & 232M & 2473G \\
**InfiNet-XL\({}^{}\)** & 56.3 & 48.9 & 273M & 1454G & 54.6 & 55.2 & 253M & 2544G \\   

Table 2: **Object detection and semantic segmentation results on MS COCO and ADE20K.**

   (a) Isotropic Models & &  \\   &  & Params & FLOPs & Top1 \\ Model & Orders & (M) & (G) & Acc.(\%) \\  ConvNeXt-S(iso.) & no & 22 & 4.3 & 79.7 \\ Conv2Former(iso.) & 2 & 23 & 4.3 & 81.2 \\ DeiT-S & 3 & 22 & 4.6 & 79.8 \\ HorNet-S(iso.) & 2-5 & 22 & 4.5 & 80.6 \\
**InfiNet-S(iso.)** & \(\) & 22 & 4.3 & 81.4 \\   

Table 3: More Results on isotropic models and different kind of Reproducing Kernel

### Ablation Study

We use the additive operator, Hadamard product, quadratic polynomial kernel and cubic polynomial kernel, and RBF kernel in the kernel methods section of InfiNet, respectively, on a tiny size model, to verify the effect of the gradual expansion of the order of the interaction space up to infinite dimensions on the performance of the model. As in Table 3(b), we can see that the performance of the model is gradually improving as the order of the model interaction space increases up to infinite dimensions.

## 7 Conclusion

As a conclusion, in this paper, we propose that one of the key points of success of today's element-wise multiplication-based models is that they explore a high-dimensional feature interaction space through feature interactions. And the RBF kernel can greatly expand this interaction space into an infinite dimensional feature interaction space. Based on this observation, we propose InfiNet, a high-performance neural network that explores infinite-dimensional feature interactions while using a modern model structure, which has achieved state-of-the-art results on several visual tasks.

## Limitations

Although we propose the use of kernel methods for infinite-dimensional feature interaction, the only kernel methods we have tried so far are the RBF kernel function and some polynomial kernels of finite dimension. Substitutions utilizing a variety of kernel, including Laplace kernels, exponential kernels, a learnable kernel, etc., can be considered in subsequent studies. Our model has only been performed in some basic computer vision tasks, and validation in language and other modalities still requires some effort. The training of our model is only performed in the supervised learning paradigm, and more training and validation on self-supervised tasks still require effort. In addition, to avoid additional hyperparameter tuning, we fixed the \(\) parameter in the RBF kernel to 1. This may have deprived us of the possibility of exploring the optimal InfiNet, but due to the high cost of training the model, we will leave the impact of this hyperparameter on the InfiNet as a follow-up work.

## Broader Social Impact

InfiNet is a state-of-the-art vision neural network architecture. The advancements in computer vision neural network architectures hold significant potential for positive societal impact, particularly in enhancing healthcare diagnostics, improving security systems, and advancing autonomous transportation. However, it is crucial to address potential negative implications such as privacy concerns, algorithmic biases, and job displacement. Ensuring ethical development and deployment involves implementing strict data protection measures, promoting fairness and inclusivity in algorithm design, and supporting workforce retraining programs. By proactively managing these challenges, we can maximize the benefits of computer vision technologies while minimizing their risks. Engaging with diverse stakeholders will be essential to ensure these technologies are used responsibly and equitably.