# Web-Scale Visual Entity Recognition:

An LLM-Driven Data Approach

 Mathilde Caron &Alireza Fathi &Cordelia Schmid &Ahmet Iscen

Google DeepMind

###### Abstract

Web-scale visual entity recognition, the task of associating images with their corresponding entities within vast knowledge bases like Wikipedia, presents significant challenges due to the lack of clean, large-scale training data. In this paper, we propose a novel methodology to curate such a dataset, leveraging a multimodal large language model (LLM) for label verification, metadata generation, and rationale explanation. Instead of relying on the multimodal LLM to directly annotate data, which we found to be suboptimal, we prompt it to reason about potential candidate entity labels by accessing additional contextually relevant information (such as Wikipedia), resulting in more accurate annotations. We further use the multimodal LLM to enrich the dataset by generating question-answer pairs and a grounded fine-grained textual description (referred to as "rationale") that explains the connection between images and their assigned entities. Experiments demonstrate that models trained on this automatically curated data achieve state-of-the-art performance on web-scale visual entity recognition tasks (_e.g._ +6.9% improvement in OVEN entity task), underscoring the importance of high-quality training data in this domain.

## 1 Introduction

Entities are at the core of how we represent and organize knowledge, as seen in prominent encyclopedias like Wikipedia, where each article is dedicated to a specific entity. In the field of computer vision, the task of visual entity recognition aims to identify entities within query images. This capability is not only a fundamental building block for various entity-aware visual understanding tasks, including "info-seeking" Visual Question Answering (VQA)  and News Content Understanding , but also has numerous commercial applications. Despite the progress made in recent years, current models often struggle with web-scale visual entity recognition. These models, typically trained on free-form image captions , tend to hallucinate entities or output overly generic ones, leading to suboptimal performance. We hypothesize that the root of this issue lies in the lack of clean, large-scale training data specifically designed for visual entity recognition.

Recent efforts have been made to address this problem by transforming existing captioning datasets into entity recognition datasets . For example, Caron et al.  propose to match each Wikipedia entity name to most similar captions in a large image-caption database  and use the corresponding images as visual examples for the considered entity. Although this method has resulted in state-of-the-art performance, it still has significant limitations: the resulting datasets are often noisy, with poor matching between the image content and the candidate entity. First, a source of mistake is due to the ambiguity of language. For example the entity _bishop of llandaff_ may refer both to a person and to a flower species . Second, there is some noise inherent to the used image-caption dataset that affects the results. For example in Fig. 1(a), an image of a building is incorrectly linked to the Wikipedia entity _Negative equity_. This mismatch occurs because of the caption "_Worst areas for negative equity_", which is likely to have been extracted from an investment-related website. The irrelevant association of the caption with the building image results in the inaccurate connection to the Wikipedia entity. Third, the text embedding matching betweenentity name and caption is not always accurate. For instance, in Fig. 1(b), the caption "_nematobrycon espece nematobrycon palmeri_" is incorrectly matched with the entity name _nematocampa resistaria_, which are two distinct animal species (a fish and a moth). Moreover, another limitation of these datasets is that they typically focus on a single entity per image, which is a restrictive scenario, as most images contain multiple entities.

In this paper, we overcome these different limitations by proposing a novel methodology to curate a high-quality, large-scale dataset for web-scale visual entity recognition leveraging the capabilities of modern multimodal Large Language Models (LLMs) available through public APIs [2; 15; 37]. Our approach is unique in that we do not rely on the multimodal LLM for direct annotation, which we found to be suboptimal. Instead, we prompt the LLM to reason about candidate entity labels by accessing additional contextual relevant information, such as the original image captions and external knowledge sources like Wikipedia. This approach significantly improves the annotation quality of the resulting dataset, as evidenced by our thorough ablation studies. Moreover, we employ the multimodal LLM to augment our dataset with rationales that explain the relationship between images and their corresponding entities. We observe in our experiments that training on this additional metadata improves the performance and visual entity understanding of the models. Finally, we address the previous limitation of focusing on a single entity per image by prompting the multimodal LLM to generate several question-answer pairs that cover a diverse range of entities in the image.

We conduct extensive experiments to evaluate the effectiveness of our approach. The results demonstrate that models trained on our automatically curated data achieve state-of-the-art performance on web-scale visual entity recognition tasks, notably on the challenging Open-domain Visual Entity recognitioN (OVEN) benchmark  (_e.g._ +6.9% on the OVEN entity split and +3.8% on the OVEN query split). Remarkably, we obtain these results with moderate-size models, which are orders of magnitude smaller than competing approaches, highlighting the importance of high-quality training data in this domain. We further demonstrate the validity and effectiveness of our dataset when utilized as a memory base for approaches such as visual matching, showcasing its versatility and potential for various applications. In summary, our contributions are threefold:

* We introduce a novel methodology to curate a large-scale dataset for web-scale visual entity recognition, using a multimodal LLM as a verification and annotation tool.
* We enrich the dataset with additional metadata, including question-answer pairs and rationales, generated by the multimodal LLM.
* We demonstrate the effectiveness of our approach with thorough ablation study and by achieving state-of-the-art performance on web-scale visual entity recognition tasks.

## 2 Related work

**Visual entity recognition.** Entities are key to knowledge representation and organization, as seen in prominent encyclopedias like Wikipedia. Visual entity recognition is the task of identifying entities based on visual queries [13; 43; 44], and is a critical component of many complex applications. One such application is info-seeking VQA, which involves providing answers to questions about the detailed properties of finegrained entities [10; 22; 31]. Another application is entity-aware captioning [28; 34; 61], a technique frequently employed in tasks such as news content comprehen

Figure 1: **Two failure cases of the visual entity recognition dataset of . Our proposed method overcomes these limitations by prompting a multimodal LLM to correct candidate entities. The LLM has access to relevant context such as the candidate entity Wikipedia page and the input image-caption pair. We also enrich the dataset with rationales and question/answer pairs covering diverse entities.**

sion [1; 4; 14; 24; 54]. Recent research has expanded the scope of visual entity recognition to include web-scale and open-domain entities [17; 21; 41]. Of particular interest, Hu et al.  introduce the Open-domain Visual Entity recognitioN (OVEN) benchmark, which includes over 6 million entities from English Wikipedia, covering a broad range of concepts such as animals, buildings, organizations, landmarks, etc. Caron et al.  recently achieved state-of-the-art results on the OVEN benchmark by re-purposing auto-regressive generative models for this task. A key component of their approach is pretraining on a new entity-based dataset instead of captions. In this paper, we build on the work of  by improving their pretraining dataset using external multimodal LLMs.

**Using LLMs as annotation tools.** Since their remarkable success and widespread accessibility, LLMs [36; 40; 53] have been used in many different ways to obtain better supervision for training various tasks [23; 32; 45; 59; 60; 62]. For example, LLMs have been used to generate question-answer pairs from Wikipedia pages [8; 31] or from transcribed video narrations , while other works use LLMs to rephrase questions into sentences . Of particular interest, Hsieh et al.  show that prompting LLMs to output rationales as additional supervision for training small models in a multi-task framework is an effective strategy. A recent related trend consists in prompting LLMs to generate high-quality instruction-following training samples [11; 50], an approach which has succesfully been extended to multimodal tasks . Unlike our work, these approaches prompt LLMs with text input only while we feed both image and text to multimodal LLMs. Another key difference is that we use the multimodal LLM as a verification and correction tool based on candidate annotations rather than using its raw output as supervision, which we find to be suboptimal for the task of web-scale finegrained entity recognition.

## 3 Method

In this section, we describe how to leverage the capabilities of modern multimodal LLMs  in conjunction with external knowledge repositories such as Wikipedia to create a clean, large-scale training dataset specifically designed for web-scale visual entity recognition. We refer to the resulting dataset as "LLM-Refined Entity-WebLI" (REW) and an overview of our method is in Fig. 2.

### Preliminaries

**Wikipedia-scale visual entity recognition.** Following recent research in web-scale visual entity recognition [7; 17], our goal is to train models capable of accurately matching any given image-text query (\(x_{v}\), \(x_{t}\)) to an entity \(e\) among a vast finegrained set \(\) of possible entities. In this work, unless specified otherwise, the set of entities \(\) consists of the 6 million entities from English Wikipedia. Each entity \(e\) comes with an entity name \(t_{e}\) corresponding to the entity Wikipedia page title.

**Entity-based pretraining dataset.** Previous works have observed that auto-regressive image captioning models like GIT  or PALI  have suboptimal results when transferred to visual entity recognition (see Tab. 1), due to the differences between captioning and entity recognition tasks [7; 17]. To address this, Caron et al.  propose training such models on entity-based data, not captions, and automatically create a new dataset called "Entity-WebLI" for this purpose. In short, for each

Figure 2: **LLM-Refined Entity-WebLI” (REW) dataset. We propose a method to refine the Entity-WebLI dataset of Caron et al.  by prompting a multimodal LLM to verify and correct Wikipedia entities. We also prompt the multimodal LLM to output visually grounded rationales and question/answer pairs about diverse attributes of the image. Complete prompts are in Appendix A.3.1.**

Wikipedia entity, the authors find the most relevant image-caption pairs in WebLI through nearest neighbor search in the CLIP text embedding space  between encoded entity names and captions. They then replace the captions of the corresponding retrieved images with the considered entity name. In this work, we refer to the entities obtained in this manner as "candidate entities". Further details on the Entity-WebLI dataset are in . We describe in the following how to improve over such a dataset by leveraging the capabilities of modern multimodal LLMs.

### Generating finegrained entities and descriptions with multimodal LLMs

**Entity verification and correction.** Our goal is to overcome the limitations of existing entity-based pretraining dataset discussed in the introduction. To improve the correspondence quality between images and entities, we propose to use existing multimodal LLMs to verify the assignment of an entity to an image. In particular, given an image \(x_{v}\) and corresponding entity \(e\) from the Entity-WebLI dataset, we prompt a multimodal LLM with the task of verifying if \(x_{v}\) is an image representing entity \(e\). Interestingly, we make two important findings in our experiments (see Tab. 4). First, directly predicting an entity \(e\) is a more challenging problem than verifying the validity of the proposed entity. Intuitively, this is because multimodal LLMs either output too generic entities or hallucinate when they do not know the correct entity. This is expected since LLMs haven't specifically been trained to identify entities at very high levels of granularity. This results in sub-optimal performance for the finegrained entity recognition tasks, where the goal is to recognize non-generic finegrained entities.

Second, we find that the verification by a multimodal LLM is more precise when it has access to external metadata such as the page content of the candidate Wikipedia entity \(e\) and the original caption. Intuitively, the Wikipedia content allows the multimodal LLM to know which visual attributes to look for in the image \(x_{v}\), while the original caption gives hints about the corrected entity when the multimodal LLM detects that the candidate \(e\) is not correct. Note that the corrected entity might not always coincide with an actual Wikipedia entity since we do not provide the list of the 6M Wikipedia entities to the LLM. We still include these corrected entities in our training dataset and simply constrain decoding to the 6M entities at inference time (see details in Sec. 4.1). Similar techniques have also been used into entity generation in multimodal contexts by works like GMEL  and AutoVER . Our prompt for entity verification and correction is available in Appendix A.3.1.

**Generating rationales.** At the same time as asking the multimodal LLMs to verify the entities assigned to training images, we also prompt it to provide a visually grounded rationale for its proposed entity. This allows to explain the connection between the image and the assigned entity as can be seen in the examples in Fig. 3 and Fig. 4. We observe in our experiments that it improves the performance of the models for entity recognition (see Tab. 4). When training our model (see details in Sec. 3.3), we follow a multi-task learning strategy where we prepend task prefixes to the input examples so that the model can output differently based on whether it is asked to predict an entity name or a rationale.

**Generating question-answer pairs (QAs).** The Wikipedia entity recognition OVEN benchmark  consists of two splits: an entity split and a query split. In the query split, images typically contains multiple entities, and the input question \(x_{t}\) determines which entity should be recognized by the visual entity recognition models. We observe in our early experiments and in Tab. 1 that training on the Entity-WebLI dataset  consistently results in poor performance in the query split, while achieving state-of-the-art performance for the OVEN entity split. We hypothesize that this is due to how Entity-WebLI dataset is constructed (see details in Sec. 3.1). The k-NN search in the text embedding space favors short captions containing entity names. As a result, a single, unambiguous, entity is assigned to each image, a scenario quite different to the examples in the query split.

To overcome this problem, we prompt the multimodal LLM to generate question-answer pairs for several, diverse entities in the input image (see prompt in Appendix A.3.1). The multimodal LLM has access to the input image, the verified/corrected entity as well as the rationale it previously generated. We empirically evaluate in the ablation study in Tab. 5 the impact of these on the question/answer pairs and resulting trained models.

### Model training

**Auto-regressive generative models.** Previous works have shown the effectiveness of _generative_ approaches for entity recognition both in the pure NLP domain [12; 30; 39; 42; 49; 51] and, more recently, in visual entity recognition benchmarks [7; 17]. Motivated by their success, we perform entity recognition by generating Wikipedia entity names (_i.e_. page titles \(t_{e}\)) in an auto-regressivefashion. This is akin to a multimodal version of GENRE  or to the ger-caption variant of . Formally, we transform an input image-text query pair \(x=(x_{v},x_{t})\) into a set of \(N\)\(d\)-dimensional embeddings \(^{N d}\) formed by concatenating the visual encoder output of \(x_{v}\) and the text tokenizer output of \(x_{t}\). We use an auto-regressive text decoder \(g()\) to generate the target text \(y\). As detailed in the following paragraph, the target text can either be an entity name or a rationale. Specifically, the decoder predicts each text token \(y_{k}\) from the target text (total length is \(K\)) given both the set of preceding token embeddings \(_{<k}\) and the input image-text embeddings \(\). We train with a language modeling objective:

\[=_{i=1}^{K}(y_{k},g([;_{<k}]))\] (1)

where \([;]\) corresponds to the concatenation operation in the first dimension and \(\) is the softmax cross-entropy loss with label-smoothing . We average this loss over minibatches of examples and update the weights of the visual encoder and text decoder with back-propagation.

**Multi-task learning.** We train our models jointly with three distinct tasks: (i) predicting the verified/corrected visual entity, (ii) generating the rationale and (iii) answering the questions generated by the multimodal LLM. These three tasks are text generation tasks and follow the same framework introduced in the previous paragraph. They differ in the nature of the input text \(x_{t}\) and target text \(y\). For entity recognition and question answering, the target text \(y\) corresponds to a possible entity name and the input text \(x_{t}\) corresponds to a question. For rationale generation, the target text \(y\) corresponds to the rationale generated by multimodal LLM and the input text \(x_{t}\) consists simply of a prefix specifying to the model that this task is distinct from entity generation. Our final multi-task loss objective is:

\[_{}=_{}+_{}+_{}\]

where \(_{}\), \(_{}\) and \(_{}\) correspond to entity, rationale and answer generation respectively.

## 4 Experiments

### Experimental setting

We detail here the most salient experimental details. Full experimental setting is in Appendix A.3.

**REW training dataset.** Our training dataset builds upon the Entity-WebLI dataset  (see Sec. 3.1) which itself is based on WebLI , a dataset already deduplicated against the train, val, and test splits of 68 common vision/vision-language datasets . The Entity-WebLI dataset is further aggressively filtered against OVEN by removing any image which has a CLIP-score higher than 0.95 with an OVEN image . This ensures that there is no downstream data leakage in our REW dataset. We build two versions of REW dataset: REW-5M (4.5M images) and REW-47M (47M images). Each training image is attached to a verified/corrected entity, a rationale and 3 question/answer pairs. In the main results section (Sec. 4.2) we train on REW-47M dataset for \(600k\) steps while for analysis and ablation study (Sec. 4.3) we train on REW-5M for a shorter schedule (\(200k\) steps). We also validate our dataset refining methodology using the LAION dataset  as the image-caption base dataset.

**Downstream task: OVEN.** We consider the entity and query splits of the OVEN benchmark . For both splits, the goal is to output a Wikipedia entity given an input image and a question. OVEN validation and test splits are divided into seen and unseen entities. The seen examples correspond to entities present in the OVEN training set while unseen entities are a subset of entities not present in the training set. We report the harmonic mean (HM) of top-1 accuracy scores between "seen" and "unseen" entities . We specify in our results if we report results before or after additional finetuning on the training set of OVEN ("+ seen finetune").

**Downstream task: finegrained datasets.** We also report results on Oxford Flowers , Sun397 , Food101 , FGVC-Aircraft  and Sports100  finegrained datasets in the zero-shot mode. We choose these datasets since there is a direct mapping between their class vocabularies and the Wikipedia entities that our model is trained to output. The resulting class label to Wikipedia entity mappings are given in Appendix A.5.

**Inference.** We perform decoding with beam search. When evaluating on OVEN, we discard all decoded texts that are not one of the 6M Wikipedia entities. This is akin to constraining the beam search only at the last decoding step. Constraining from the first decoding step is too costly in our implementation with such a large million-scale label space. For finegrained datasets, we perform constrained beam search decoding at all decoding steps since the label spaces are smaller.

**Model training implementation details.** We use Git-Large : it consists of a visual encoder (CLIP-L/14 ) and a 6-layer text decoder with internal dimension \(d=768\). Following , the visual encoder is first pre-trained jointly on WebLI-100M  and Conceptual Captions-12M  while the decoder is randomly initialized. We use batch size of 4096, learning rate of \(1^{-5}\) for the visual encoder and \(1^{-4}\) for the decoder, label smoothing of \(0.2\) and no weight decay. We use standard inception crop data augmentation. For the multimodal LLM, we use Gemini Pro . The public API is available at ai.google.dev.

### Main results

**Comparison with the state of the art on OVEN.** In Tab. 1, we observe that PaLI and GiT-Large models pretrained on captioning datasets have suboptimal performance, especially in the entity split. Intuitively this is because the entity split tackles finegrained entity recognition while query split is more reflective of a generic VQA task, which leverages the language understanding abilities learned from captioning. Hence, the query split task is more aligned with the captioning pretraining than the entity split task. In Tab. 1, we see that the model trained on our REW dataset instead of captioning data results in state-of-the-art performance in the OVEN benchmark, both before and after further OVEN finetuning on the "seen" classes. Notably, our model outperforms the captioning PaLI-17B model by large margins: +13.6 top1 HM test accuracy on the entity split and +3.8 on the query split, while using \(42\) less parameters.

Finally, we report the zero-shot performance of the multimodal LLM on OVEN: it reaches 13.3 HM top-1 in the entity split and 29.5 HM top-1 in the query split. These numbers suggest that we are not merely distilling from the considered multimodal LLM as we outperform its performance on this benchmark by +10.3 on entity and +1.4 on query sets while using orders of magnitude less parameters. The analysis in Tab. 4 will further demonstrate the importance of using the multimodal

    & & &  &  \\   & & &  &  \\  Model & \#par (B) & Dataset & HM & seen unseen & HM & seen unseen & HM & seen unseen & HM & seen unseen \\  _Dual encoders_ & & & & & & & & & & & & & & \\ CLIPfusion  & 0.9 & OpenAI  & 5.2 & 5.6 & 4.9 & 8.4 & 33.6 & 4.8 & 1.6 & 1.3 & 2.0 & 2.7 & 25.8 & 1.4 \\ CLIP2CLIP  & 0.9 & OpenAI  & 5.2 & 5.6 & 4.9 & 11.5 & 12.6 & 10.5 & 1.6 & 1.3 & 2.0 & 3.5 & 3.8 & 3.2 \\  _Generative approaches_ & & & & & & & & & & & & & & & & \\ PaLI-3B  & 3 & WebLI-1B  & \(-\) & \(-\) & \(-\) & 9.1 & 19.1 & 6.0 & \(-\) & \(-\) & \(-\) & 16.7 & 27.4 & 12.0 \\ PaLI-17B  & 17 & WebLI-1B  & 1.8 & 3.3 & 1.2 & 16.0 & 28.3 & 11.2 & 9.2 & 14.1 & 6.8 & 27.1 & 36.2 & 21.7 \\ GiT-Large  & 0.4 & WebLI-100M  & 2.1 & 4.7 & 1.4 & 6.5 & 13.7 & 4.2 & 3.9 & 5.1 & 3.2 & 15.6 & 28.9 & 10.7 \\ ger-ald  & 0.4 & Entity-WebLI  & 17.7 & 18.3 & 17.2 & 22.7 & 31.5 & 17.7 & 6.3 & 6.0 & 6.7 & 5.8 & 14.1 & 3.6 \\ GiT-Large  & 0.4 & Entity-WebLI  & 19.1 & 19.8 & 18.5 & 20.1 & 25.9 & 16.4 & 10.4 & 9.8 & 11.0 & 10.1 & 17.7 & 7.1 \\ GiT-Large  & 0.4 & REW-47M (Ours) & **23.6** & **25.7** & **21.7** & **29.6** & **36.0** & **25.1** & **30.0** & **31.2** & **28.9** & **30.9** & **39.2** & **25.5** \\   

Table 1: **Comparison with the state of the art on OVEN entity and query test splits. We report the harmonic mean (HM) of the seen and unseen sets (top-1 accuracy) before and after finetuning on OVEN training seen categories (“+ seen finetune”). We indicate model architectures and their total number of parameters (“# par.”) in billions as well as the training dataset details.**

   Model & Training dataset & Flowers  & Sun397  & Food  & Aircraft  & Sports100  \\  GiT-Large  & WebLI-100M  & 39.1 & 45.8 & 55.7 & 7.4 & 57.9 \\ GiT-Large  & Entity-WebLI  & 79.8 & 45.1 & 66.5 & 27.7 & 77.2 \\ ger-ald  & Entity-WebLI  & 86.7 & 45.9 & 78.0 & 37.4 & 74.6 \\ GIT-Large  & REW-47M (Ours) & **88.2** & **50.2** & **80.4** & **50.3** & **78.0** \\   

Table 2: **Zero-shot transfer of generative models to finegrained image classification. We report top-1 accuracies. All models are run by us and are based on the same architecture.**LLM as a _verification and correction tool_ rather than a _teacher_ since directly using its predictions as targets result in poor performance.

**Zero-shot transfer to finegrained datasets.** In Tab. 2, we observe that the model trained on our proposed training dataset REW-47M transfers effectively to several finegrained datasets. The model trained on REW demonstrates superior transferability compared to the same model trained on captions or Entity-WebLI. This result shows the higher quality of the REW dataset.

**Using REW dataset as a memory base.** We explore the potential of our REW dataset when utilized as a memory base for visual matching in Tab. 3, and for retrieval-enhanced contrastive (RECO) training  in Appendix A.1.1. Each image in the memory is either associated with the candidate entity from text k-NN matching (as in Entity-WebLI ) or with the multimodal LLM corrected entity (our method for REW). In Tab. 3, we report the results of visual matching with two popular visual backbones [6; 41] and across six different finegrained visual entity recognition datasets for which we have the mapping from class label to Wikipedia entity. We see in Tab. 3 that our corrected entities lead to better visual matching performance across the board which suggest that they are better annotations, describing more accurately the content of the images. In fact, using our corrected entities boosts the performance in average by +10.5% relative improvement when considering CLIP  and by +13.9% relative improvement with DINOv2 .

### Analysis and ablation study

Unless specified otherwise, models in this section are trained on the REW-5M dataset.

**Importance of entity verification and correction.** In Tab. 4, we train models with different source of annotations for the target entities. First, we observe that directly using the multimodal LLM raw output as target (which is akin to a distillation scenario) results in suboptimal performance (row 1). By inspecting qualitative results, our hypothesis is that this can be attributed to the LLM's tendency to produce hallucinations, overly generic answers, or outputs in an incorrect format (long descriptive captions instead of entities). Second, we validate in Tab. 4 the importance of the multimodal LLM correction step compared to using the candidate entities from text k-NN matching as in Entity-WebLI : this strategy improves the performance of the resulting model by more than 6 points (row 3 versus row 2). Finally, providing additional contextual information to the multimodal LLM results in a substantial boost of 1.7 points (row 4 versus row 3), a benefit that we further illustrate through our qualitative analysis in Fig. 3. We note that the multimodal LLM refines the candidate entities 76% of the time, _i.e_. it validates the candidate entity 24% of the time (see Fig. 3c).

**Qualitative analysis on the importance of the multimodal LLM correction.** In Fig. 3, we identify two typical failure cases of the multimodal LLM correction step when it lacks access to Wikipedia and original caption metadata. The first case (see examples in Fig. 3a) involves the LLM making incorrect corrections by providing generic or hallucinated outputs. For example, it identifies an image of the _Brone baths_ as an _outdoor swimming pool_ or it makes mistakes by recognizing incorrect plant or animal species. In contrast, the multimodal LLM with access to external metadata can get a hint about the correct entities by reading from the original caption. As a matter of fact, note that simply using the original captions as targets leads to suboptimal performance as shown by Caron et al. . Intuitively, original captions are usually not in the form of an entity and can be noisy and not reflective of the visual entity of the image, as illustrated in the examples in Fig. 3b.

   Memory dataset & OVEN-Ent & Flowers & Sun397 & Food101 & Aircraft & Sports100 & Avg.relative \(\) \\  _CLIP-L14 backbone _ & & & & & & & & \\ Candidate entities & 16.3 & 81.1 & 37.7 & 80.9 & 42.1 & 70.4 & – \\ + multimodal LLM correction & 19.8(+3.5) & 81.1(+0.0) & 49.7(+12.0) & 79.1(-1.8) & 44.6(+2.5) & 74.8(+4.4) & +10.5\% \\  _DINOv2-L14 backbone _ & & & & & & & \\ Candidate entities & 19.1 & 91.7 & 37.9 & 75.7 & 34.6 & 76.8 & – \\ + multimodal LLM correction & 24.8(+5.7) & 90.5(-1.2) & 52.3(+14.4) & 74.9(-0.8) & 38.3(+3.7) & 82.4(+5.6) & +13.9\% \\   

Table 3: **Visual matching.** We report top-1 accuracies of visual matching with CLIP or DINOv2 ViT-L/14 visual backbones. We compare two types of annotations for the visual matching memory database: either the candidate entities or our multimodal LLM corrected entities. We report the _absolute_ improvements of using the latter compared to the former between parentheses as well as the _relative_ improvement averaged across the six datasets in the last column.

The second failure case of the multimodal LLM lacking access to metadata occurs when the model corrects information that it should not, likely due to a lack of knowledge about the entities involved (see examples in Fig. 2(c)). This issue is reflected in the LLM rationale, indicating a need for further external information about the candidate entities. By contrast, the model with access to Wikipedia can read about the visual attributes which are characteristic of the candidate entity and look for these in the image. This process is reflected in the LLM rationale: for example, the model with Wikipedia access to the _grosgrain_ entity page validates this entity with the rationale: "_Multiple spools of ribbon with prominent transverse ribs, which is a defining characteristic of grosgrain_" while the model without Wikipedia seems to lack specific knowledge about _grosgrain_.

**Generating rationales and QA pairs with multimodal LLM.** In Tab. 5 (left), we probe the importance of providing external metadata to the LLM when generating rationales and question answer pairs. First, we observe in Tab. 5 (left) that the model without access to any metadata (row 1) has suboptimal performance. Incorporating external input, such as Wikipedia content and original captions, during the rationale generation process enhances the quality and pertinence of the rationale. Consequently, this leads to an improvement in performance (row 2). Importantly, this performance boost is even higher if the multimodal LLM can leverage the improved rationale when generating the QAs (row 3).

Table 4: **Importance of entity verification and correction.** We report HM top-1 accuracy on OVEN validation entity split. To isolate the effect of the entity target source, we train only with entity targets (_i.e._ only with loss \(_{}\)). We do not perform seen finetuning and evaluate the models directly after pretraining. All models are trained with the _same pretraining images_ and use the same architecture.

Figure 3: **Qualitative analysis of the importance of the entity verification and correction step.**

[MISSING_PAGE_FAIL:9]

## 5 Discussion

**Limitations.** Our work relies on the use of multimodal LLMs with external knowledge bases, such as Wikipedia, for dataset creation and annotation. This dependence on using external data to prompt LLMs may limit the applicability of the approach in scenarios where such external knowledge is scarce or unavailable. Also, the proposed approach involves prompting the LLM to reason about candidate entity labels and generate additional metadata. This process is time-consuming and computationally expensive when considering multimodal LLMs with billions of parameters , which may limit the scalability of the approach to even larger datasets. A direction for future work could be to overcome some of the limitations of OVEN we find in our study (Appendix A.2) and create additional benchmarks for web-scale visual entity recognition.

**Conclusion.** We propose a novel methodology to curate a high-quality large-scale dataset, REW, for web-scale visual entity recognition using multimodal LLMs. Our approach significantly improves the quality of the dataset and bypasses the need for manual annotation. We also enrich the dataset with additional metadata, including question-answer pairs and rationales, generated by the LLM, which further improves the performance of the models. We achieve state-of-the-art performance on web-scale visual entity recognition tasks, highlighting the critical role of high-quality training data in this challenging domain. While our methodology for multimodal LLM-based data curation shows promise, we recognize significant opportunities for further enhancement. Integrating additional tools and external knowledge sources holds potential to improve its effectiveness. Furthermore, we anticipate that our approach can be broadly applicable to other visual tasks requiring extensive training data. Broader impact discussion is in Appendix A.4.