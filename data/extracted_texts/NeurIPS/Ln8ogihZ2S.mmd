# eXponential FAmily Dynamical Systems (XFADS):

Large-scale nonlinear Gaussian state-space modeling

 Matthew Dowling

Champalimaud Research, Champalimaud Foundation, Portugal

matthew.dowling@research.fchampalimaud.org

&Yuan Zhao

National Institute of Mental Health, USA

yuan.zhao@nih.gov

&Il Memming Park

Champalimaud Research, Champalimaud Foundation, Portugal

memming.park@research.fchampalimaud.org

###### Abstract

State-space graphical models and the variational autoencoder framework provide a principled apparatus for learning dynamical systems from data. State-of-the-art probabilistic approaches are often able to scale to large problems at the cost of flexibility of the variational posterior or expressivity of the dynamics model. However, those consolidations can be detrimental if the ultimate goal is to learn a generative model capable of explaining the spatiotemporal structure of the data and making accurate forecasts. We introduce a low-rank structured variational autoencoding framework for nonlinear Gaussian state-space graphical models capable of capturing dense covariance structures that are important for learning dynamical systems with predictive capabilities. Our inference algorithm exploits the covariance structures that arise naturally from sample based approximate Gaussian message passing and low-rank amortized posterior updates - effectively performing approximate variational smoothing with time complexity scaling linearly in the state dimensionality. In comparisons with other deep state-space model architectures our approach consistently demonstrates the ability to learn a more predictive generative model. Furthermore, when applied to neural physiological recordings, our approach is able to learn a dynamical system capable of forecasting population spiking and behavioral correlates from a small portion of single trials.

## 1 Introduction

State-space models (SSM) are invaluable for understanding the temporal structure of complex natural phenomena through their underlying dynamics . While engineering or physics problems often assume the dynamical laws of the system of interest are known to a high degree of accuracy, in an unsupervised data-driven investigation, they have to be learned from the observed data. The variational autoencoder (VAE) framework makes it possible to jointly learn the parameters of the state-space description and an inference network to amortize posterior computation of the unknown latent state . However, it can be challenging to structure the variational approximation and design an inference network that permits fast evaluation of the loss function (evidence lower bound or ELBO) while preserving the temporal structure of the posterior.

In this work, we develop a structured variational approximation, approximate ELBO, and inference network architecture for generative models specified by nonlinear dynamical systems with Gaussian state noise. Our main contributions are as follows,

1. A structured amortized variational approximation that combines the prior dynamics with low-rank data updates to parameterize Gaussian distributions with dense covariance matrices.
2. Conceptualizing the approximate smoothing problem as an approximate filtering problem for pseudo-observations that encode a representation of current and future data, and,
3. An inference algorithm that scales \((TL(Sr+S^{2}+r^{2}))-\) made possible by exploiting the low-rank structure of the amortization network as well as Monte Carlo integration of the latent state through the dynamics.

## 2 Background

State-space models are probabilistic graphical models where observations \(_{t}\) in discrete time are conditionally independent given a continuous latent state, \(_{t}\), evolving according to Markovian dynamics, so that the complete data likelihood for \(T\) consecutive observations factorizes as,

\[p(_{1:T},_{1:T})=p_{}(_{1}) \,p_{}(_{1}\,|\,_{1}) _{t=2}^{T}p_{}(_{t}\,|\,_{t})\,p_{ }(_{t}\,|\,_{t-1})\]

where \(_{t}^{L}\) are real-valued latent states, \(\) parameterizes the dynamics and initial condition, and \(\) parameterizes the observation model. When the generative model, \((,)\), is known, the statistical inference problem is to compute the smoothing posterior, \(p(_{1:T}\,|\,_{1:T})^{2}\). Otherwise, \((,)\) have to be learned from the data - known as system identification.

Variational inference makes it possible to accomplish these goals in a harmonious way. The variational expectation maximization (vEM) algorithm iterates two steps: first, we maximize a lower bound to the log-marginal likelihood - the ELBO - with respect to the parameters of an approximate posterior, \(q(_{1:T}) p(_{1:T}\,|\,_{1:T})\); then, with the approximate posterior fixed, the ELBO is maximized with respect to parameters of the generative model. For large scale problems, vEM can be slow due to the need to fully optimize the variational parameters before taking gradient steps on parameters of the generative model. Therefore, the variational autoencoder (VAE) is better suited for large scale problems for its ability to simultaneous learn the generative model and inference network - an expressive parametric function that maps data to the parameters of approximate posterior [10; 11].

**Model specifications.** Although our approach is applicable to any exponential family state-space process, given their ubiquity, we focus on dynamical systems driven by Gaussian noise so that,

\[p_{}(_{t}\,|\,_{t-1})=( _{t}\,|\,_{}(_{t-1}),_{})\] (1)

where \(_{}:^{L}^{L}\) might be a nonlinear neural network function with learnable parameters \(\), and \(_{}^{L L}\) is a learnable state-noise covariance matrix. Given the favorable properties of exponential family distributions [12; 13; 14; 15], especially in the context of variational inference, we write the prior dynamics in their exponential family representation (natural parameter form),

\[p_{}(_{t}\,|\,_{t-1})=h(_{t}) (_{t})^{}_{ }(_{t-1})-A(_{}(_{t-1}))\] (2)

where \(h\) is the base measure, \((_{t})\) the sufficient statistics, \(A()\) the log-partition function, and \(_{}()\) is a map \(^{L}^{L^{2}+L}\) that transforms \(_{t-1}\) to natural parameters for \(_{t}\). For a Gaussian distribution, the sufficient statistics can be defined as \((_{t})^{}=_{t}^{}&- _{t}_{t}^{}\), so that \(_{}()\) for (1) is given by,

\[_{}(_{t-1})= _{}^{-1}_{}( _{t-1})\\ _{}^{-1}\] (3)

As it will simplify subsequent analysis, the _mean parameter_ mapping corresponding to this natural parameter mapping (guaranteed to exist as long as the exponential family is minimal) is given by

\[_{}(_{t-1})=_{p_{ }(_{t}|_{t-1})}[( _{t})]=_{}( _{t-1})\\ -(_{}(_{t-1}) _{}(_{t-1})^{}+_{ })\\ \\ \] (4)

Furthermore, we make the following assumptions **i)** the state-noise, \(_{}\), is diagonal or structured for efficient matrix-vector multiplications. **ii)**\(_{}()\), is a nonlinear smooth function. **ii)** the likelihood, \(p_{}(_{t}\,|\,_{t})\), may be non-conjugate. **iv)**\(L\) may be large enough so that \(L^{3}\) is comparable to \(T\).

**Amortized inference for state-space models.** A useful property of SSMs is that, \(_{t}\) conditioned on \(_{t-1}\) and \(_{t:T}\), is independent of \(_{1:t-1}\), i.e., \(p(_{t}\,|\,_{t-1},_{1:T})=p(_{t}\,|\, _{t-1},_{t:T})\). It thus suffices to construct an approximate posterior that factorizes forward in time,

\[q(_{1:T})=q(_{1}) q(_{t}\,|\,_{t -1})\] (5)

and introduce learnable function approximators to amortize inference by mapping \(_{t-1}\) and \(_{t:T}\) to the parameters of \(q(_{t}\,|\,_{t-1})\). This makes it simple to sample \(_{1:T}\) from the approximate posterior (using the reparameterization trick) and evaluate the ELBO (a.k.a. negative variational free energy),

\[(q)=_{q_{t}}[ p(_{t}\,|\,_{t})]-_{q_{t-1}}[_{}(q( _{t}\,|\,_{t-1})||\,p_{}(_{ t}\,|\,_{t-1}))] p(_{1:T})\] (6)

where \(_{}(||)\) is the Kullback-Leibler (KL) divergence and \(_{q_{t}}_{_{t} q(_{t}, _{1:T})}\), so that the generative model and inference network parameters can be learned through stochastic backpropagation. Many works for Gaussian \(q(_{t}\,|\,_{t-1})\), such as Krishnan et al. , Alaa and van der Schaar , Girin et al. , Hafner et al. , construct inference networks that parameterize the variational posterior as

\[q(_{t}\,|\,_{t-1})=(_{}(_{t-1},_{1:T}),_{}( _{t-1},_{1:T})).\] (7)

There are limitless ways to construct \(_{}()\) and \(_{}()\) so \(\) can be learned through gradient ascent on the ELBO, but a straightforward and illustrative approach  is to transform future data, \(_{t:T}\), using a recurrent neural network (RNN), or any efficient autoregressive sequence to sequence model, and then mapping the preceding latent state, \(_{t-1}\), using a feed-forward neural network, so that a complete inference network description could be,

\[(_{}(_{t-1},_{t:T}), _{}(_{t-1},_{t:T}))=([ _{t-1},_{t}]), 28.452756pt_{t}=([ _{t+1},_{t}])\] (8)

where \(()\) is a parametric sequence-to-sequence function that maintains a hidden state \(_{t}\) and takes as input \(_{t}\), and \(()\) is a parametric function designed to output approximate posterior parameters. This leads to a backward-forward algorithm, meaning that data \(_{1:T}\) are mapped to \(_{1:T}\) in reverse time, and then samples are drawn from \(q(_{t}\,|\,_{t-1})\) forward in time.

Possible drawbacks of this inference framework are **i)** missing observations obstruct inference (the example networks cannot naturally accommodate missing data); **ii)** sampling entire trajectories to approximate the expected KL term can potentially lead to high-variance gradient estimators, and **iii)** statistics of the marginals (e.g. second moments) can only be approximated through sample averages.

## 3 Related works

Many existing works also explore inference and data-driven learning for state-space graphical models within the VAE framework. We highlight the most closely related studies and note specific limitations that our work seeks to address. The structured variational autoencoder (SVAE)  makes it possible to efficiently evaluate the ELBO while preserving the temporal structure of the posterior by restricting the prior to a _linear dynamical system_ (LDS) and then constructing the approximation as \(q(_{1:T}) p_{}(_{1:T}) (t(_{t})^{}(_{t}))\) so that its statistics can be obtained using efficient message passing algorithms. However, the SVAE is not directly applicable when the dynamics are nonlinear since the joint prior will no longer be Gaussian (thereby not allowing for efficient conjugate updates). Recently, Zhao and Linderman  expanded on the SVAE framework by exploiting the LDS structure and associative scan operations to improve its scalability.

The deep Kalman filter (DKF)  uses black-box inference networks to make drawing joint samples from the full posterior simple. However, pure black-box amortization networks such as those can make learning the parameters of the generative model dynamics difficult because their gradients will not propagate through the expected log-likelihood term . In contrast, we consider inference networks inspired by the fundamental importance of the prior for evaluating Bayesian conjugate updates. The deep variational Bayes filter (DVBF) also considers inference and learning in state-space graphical models . Difficulties of learning the generative model that arise as a result of more standard VAE implementations defining inference networks independent of the prior are handled by forcing samples from the approximate posterior to traverse through the dynamics. Our work extends this concept, by directly specifying the parameters of the variational approximation in terms of the prior.

Our approach constructs an inference network infused with the prior similar to the SVAE and DVBF but i) avoids restrictions to LDS and ii) affords easy access to approximations of the marginal statistics (such as the dense latent state covariances) without having to average over sampled trajectories (or store them directly which would be prohibitive as the latent dimensionality becomes large).

Method

An alternative to constructing variational approximations through specification of conditional distributions, as in Eq. (7), involves the use of data-dependent Gaussian potentials, that we refer to as pseudo-observations:

\[p(}_{t}\,|_{t})(}_{ }(_{1:T})^{}(_{t}))(}_{t}^{}(_{t}))=( _{t}^{}_{t}-\|_{t}_{t}\|^{2})\] (9)

These Gaussian potentials can then be combined with the prior through Bayes' rule, yielding the approximation

\[q(_{1:T})=}_{t}\,|\,_{t})p_ {}(_{1:T})}{p(}_{1:T})}\] (10)

A benefit of this formulation, is that it inherently imposes the latent dependency structure of the generative model onto the amortized posterior. This parameterization, was introduced in Johnson et al. , where an important point highlighted, is that the Gaussian potentials can encode any arbitrary subset of observations; for example, \(p(}_{t}\,|\,_{t})\) could be made to depend on \(_{t}\) alone, or even the entire dataset, \(_{1:T}\). Regardless of that particular design choice, the corresponding ELBO for the variational approximation of Eq. (10) is

\[(q)=_{q_{t}}[ p(_{t}\,|\,_{t})]-_{q_{t}}[ p(}_{t}\,|\, _{t})]+ p(}_{1:T})\] (11)

For linear Gaussian latent dynamics, conjugate potentials could be efficiently integrated with the prior using exact message passing, yielding filtered and smoothed marginal statistics. The smoothed statistics can be used to evaluate the first two terms on the right-hand side, while the filtered statistics can be used to evaluate the final term, the log-marginal likelihood of the pseudo-observations.

However, this approach does not directly apply to nonlinear dynamical systems, where the variational posterior is no longer Gaussian. Since evaluating the smoothed marginals and the log-marginal likelihood of pseudo-observations relies on first obtaining the filtered marginals, a logical starting point is to develop a method for approximating these filtered marginals. To this end, we propose a differentiable approximate message passing algorithm specifically designed to compute filtered posterior statistics in models characterized by nonlinear latent dynamics and observations represented as Gaussian potentials. Building on this foundation, we then return to the subsequent challenges of efficiently computing smoothed posterior statistics and evaluating the ELBO.

**Differentiable nonlinear filtering.** Bayesian filtering is often conceptualized as a two step procedure . In the _predict_ step, our belief of the latent state is integrated through the dynamics, yielding \(q(_{t}\,|\,}_{1:t-1})= p_{}(_ {t}\,|\,_{t-1})q(_{t-1}\,|\,}_{1:t-1})\, _{t-1}\) (a.k.a. the predictive distribution). Then, applying Bayes' rule, \(q(_{t}\,|\,}_{1:t}) p(}_{t} \,|\,_{t})q(_{t}\,|\,}_{1:t-1})\), we _update_ our belief. Evidently then, developing an approximate filtering algorithm that exploits the conjugacy of the pseudo observations can be recast as the problem: given an approximation \((_{t-1}) q(_{t-1}\,|\,}_{1:t-1})\), find an approximation to the predictive distribution, \((_{t}) q(_{t}\,|\,}_{1: t-1})\). The recursion would continue forward by updating our belief analytically, setting \((_{t}) p(}_{t}\,|\,_{t})(_{t})\), then finding a Gaussian approximation of \((_{t+1})\) and so forth.

With the problem restated this way, we propose an approximate filtering solution designed to exploit two key factors at play **i)** the approximate beliefs are constrained to the same exponential family as the latent state transitions **ii)** the pseudo observations are encoded as conjugate potentials. Our approach involves recursively solving intermediary variational problems (their fixed point solutions on the right),

\[&\\ &(_{t})=\,\,_{ }_{_{t-1}}[p_{}(_{t}\,|\, _{t-1})]\,(_{t})&& _{t}=_{_{t-1}}[_{}(_{t-1})]}\] (12) \[ _{t}=}_{t}+ }_{t}\] (13)

Steps (i) and (ii) can be thought of as variational analogues of the predict/update steps of Bayesian filtering, and importantly, finding their fixed point solutions does not require an iterative procedure because of our problem specifications. Reassuringly, iterating (i) and (ii) in the case of an LDS generative model would exactly recover the information form Kalman filtering equations. In the case of nonlinear dynamical systems, directly taking the expectation in (i) is intractable. We can overcome this by employing the reparameterization trick to obtain a differentiable sample approximation of the parameters, \(}_{t}\), of the fixed point solution. Naturally now, the statistics of the approximate filtered beliefs can be used to approximate the log-marginal likelihood of the pseudo observations as,

\[ p(}_{1:T})= p(}_{t}\,|\, _{t})q(_{t}\,|\,}_{1:t-1})\, _{t} p(}_{t}\,|\,_{t} )(_{t})\,_{t}\] (14)

where the last integral can be evaluated analytically as a result of the Gaussian forms of the approximations and pseudo observations. While formulating step (ii) as a variational problem may appear superfluous given the conjugate structure, it hints at the possibility of approximating smoothed posterior marginal statistics within a variational framework. However, pursuing this idea further reveals significant challenges. Trying to develop a backward recursion for the distribution \(q_{t}\) minimizing \(_{}\!(_{q_{t+1}}[q_{t[t+1]}]|\! \,q_{t})\), by using the forward KL divergence (as in step (i)), leads to an intractable problem because the backward Markov transitions, \(q_{t[t+1]}\), are not conditionally Gaussian. Conversely, a fixed point solution of the reverse KL objective (as in step(ii)), \(_{}\!(q_{t}\,\,_{q_{t+1}}[q_{ t[t+1]}])\), necessitates an iterative procedure, which can be computationally expensive.

**Smoothing as filtering.** In light of these difficulties, we offer a simple solution that exploits the flexibility in choosing the pseudo observation data dependence: define the parameters, \(_{t}\) and \(_{t}\), of each pseudo observation, \(}_{t}\), to be a function of current _and_ future data, \(_{t:T}\), so that,

\[p(}_{t}\,|\,_{t})(}_ {}(_{t:T})^{}(_{t}))\] (15)

With this choice, filtered statistics of the latent state--relative to the pseudo-observations--can be used to approximate posterior smoothed marginals, i.e. \((_{t}) q(_{t}\,|\,}_{1:t})  p(_{t}\,|\,_{1:T})\). This solution circumvents the challenges associated with backward message computation and only requires a single pass through the pseudo observations to obtain approximate smoothed posterior statistics. Substituting, \(_{t}\), as an approximation to \(q_{t}\), in Eq. (11), leads to the following approximation of the ELBO.

\[}()=_{_{t}}[ p (_{t}\,|\,_{t})]-_{_{t}}[ p( }_{t}\,|\,_{t})]+_{_{t}} [p(}_{t}\,|\,_{t})]}\] (16) \[=_{_{t}}[ p(_{t}\,|\,_{t})]-_{}((_{t}))\|\,( _{t}))\] (17)

By expressing the approximate ELBO compactly as Eq.(17), we highlight that it promotes learning models where the posterior at time \(t\) aligns closely with the one-step posterior predictive at that time (which depends on the generative model and the posterior at time \(t-1\)). The KL term in Eq.(17) can be evaluated in closed form, while the expected log-likelihood term can be approximated using the reparameterization trick. However, a point of practical importance should be raised now: every filtering step and evaluation of the KL term has a time complexity of \((L^{3})\), which may become a bottleneck for large \(L\). In the following discussion, we will explore effective strategies to parameterize the Gaussian potential inference network that produces \(}_{1:T}\), to reduce the computational burden that filtering and evaluating \(}()\) pose in the large \(L\) regime.

**Local and backward encoders.** For state-space models, inferences about the latent state should be possible even with missing observations. To enable the amortized inference network to process missing observations in a principled way, we decompose the natural parameter update into two additive components: **i)** a _local_ encoder, \(_{}()\), for current observation, and **ii)** a _backward_ encoder, \(_{}()\), for future observations, i.e.,

\[}_{}(_{t:T})=_{}( _{t})+_{}(_{t+1:T})}_{t}=_{t}+_{t+1}\] (18)

Furthermore, by building the dependence of \(_{}()\) on \(_{t+1:T}\) through their representation as \(_{t+1:T}\), so that \(_{}(_{t+1:T})=_{}(_ {t+1:T})\), a missing observation at time \(t\) is handled by setting \(_{t}=\). While a data dependent natural parameter update of \(\) faithfully represents a missing observation - in the absence of data, the prior should not be updated - alternatively setting \(_{t}=\) would introduce a harmful inductive bias into the inference network, since an observation of \(\) can be arbitrarily informative. Given the impracticality of \((TL^{2})\) memory requirements, it is appealing to consider a low-rank parameterization for the local and backward encoders - we consider

\[_{t}=_{t}\\ _{t}_{t}^{} (_{t})\\ (_{t})(_{t})^{} _{t}=_{t}\\ _{t}_{t}^{} (_{t:T})\\ (_{t:T})(_{t:T})^{}\] (19)where \(_{t}^{L r_{}}\) with \(_{t}^{L r_{}}\) parameterize low-rank local/backward precision updates, and \(_{t}^{L}\) with \(_{t}^{L}\) parameterize local/backward precision-scaled mean updates. Using these descriptions and the additive decomposition (18), the parameters of a single pseudo observation are,

\[}_{t}=_{t}\\ _{t}_{t}^{} (_{t:T})\\ (_{t:T})(_{t:T})^{}= _{t}+_{t}\\ [_{t}\;_{t}][_{t}\;_{ t}]^{}\] (20)

where \(^{L r}\) if \(r=r_{}+r_{}\) and \(^{L}\). The low-rank structure of the natural parameter updates will be a key component to develop an efficient approximate message passing algorithm for obtaining sufficient statistics of the approximate posterior and evaluating the ELBO. Analogous to the inference network description (8), a differentiable architecture producing \(_{1:T}\) and \(_{1:T}\) could be,

\[_{t}=(_{t}) _{t}=([_{t+1},_{t}]),\] (21)

which overall defines the map \(_{1:T}(_{1:T},_{1:T})\). In addition, the separation of local and backward encoders can reduce the complexity of the backward encoder for \(L<N\). Those familiar with sequential Monte-Carlo (SMC) methods  can view the backward encoder similar to twisting functions used to combine future information with filtered state beliefs to produce smoothing approximations .

**Exploiting structure for efficient filtering.** A benefit of using the forward KL to design a variational analogue to the exact Bayesian predict step is immediate access to the fixed point solution. While nonlinear specification of the latent dynamical system make the expectation of Eq. (12) intractable, using the reparameterization trick with \(_{t-1}^{s}(_{t-1})\), gives the approximation,

\[}_{t}=}{{S}}_{s=1}^{S} [-_{}(_{t-1}^{s })_{}(_{t-1}^{s})^{}+_{}]\] (22)

where \(S\) is the total number of samples. Converting this finite sample estimate from mean parameter coordinates to a mean/covariance representation, we get that,

\[}_{t}=}{{S}}_{s=1}^{S} _{}(_{t-1}^{s}) }_{t}=}_{t}^{c}}_{t}^{ c}+_{}\] (23)

where \(}_{t}^{c}\) is the \(L S\) matrix of samples passed through the dynamics function, then centered by the mean, defined for convenience as,

\[}_{t}^{c}=}{{}}[_{}(_{t-1}^{1})-}_{t},,_{}(_{t-1}^{S})-}_{t}]^{L S}\] (24)

Writing the covariance estimate as it is in Eq. (23), reveals that it can alternatively be represented by the pair \((}_{t}^{c},_{})\). In the regime where \(L>S\), significant computational savings can be afforded by capitalizing on the low-rank structure of the covariance as estimated via the reparameterization trick. This structure can be exploited for efficient linear algebraic operations involving \(}_{t}\) (and its inverse, after application of the Woodbury identity). Consequently, the natural parameters of \(_{t}\), after updating \(_{t}\), are

\[_{t}^{-1}_{t}=}_{t}^{-1}}_{t}+_{t} _{t}^{-1}=}_{t}^{-1}+_{t}_{t}^ {}\] (25)

Figure 1: **Smoothing and predictive performance on bouncing ball and pendulum.** To the left of the red line are samples from the posterior during the data window projected to image space, to the right of the red line are samples unrolled from \(p_{}(_{t}\,|\,_{t-1})\). **a)** while all methods are adept at smoothing in the context window, our methods predictive performance is better by a noticeable margin as measured by the \(R^{2}\). **b)** similar results hold for the bouncing ball dataset.

and also admit structured representations. Without _both_ the sample approximation structure (during the variational predict step) and low-rank parameterization (during the variational update step), the cost of approximate filtering each time-step would be dominated by an \((L^{3})\) cost. Instead, recognizing the potential computational advantages of exploiting these structures, and never instantiating the predictive/updated covariance and precision matrices, makes it possible to develop an approximate filtering algorithm, where in the case \(L\) is significantly larger than \(S\) or \(r\), has complexity of \((L(Sr+r^{2}+S^{2}))\) per step. More details regarding time complexity are in App. B.5.

**Efficient sampling and ELBO evaluation.** When \(_{_{t}}[ p(_{t}\,|\,_{t})]\) can not be evaluated in closed form, Monte-Carlo integration can be used as a differentiable approximation. To sample from \((_{t})\) without explicitly constructing \(_{t}\), we can take \(}_{t}^{s}(,}_{t})\) and \(_{t}^{s}(,_{L+S})\) and set,

\[_{t}^{s}=_{t}+}_{t}^{s}-_{t} \,_{t}\,_{t}^{}(_{t}^{} }_{t}^{s}+_{t}^{s}).\] (26)

While more details are provided in App. B.4, this can be done efficiently since samples can be drawn cheaply from \((_{t})\) using Eq. (23). Whereas Monte-Carlo approximations of the expected log-likelihood term might be unavoidable, the closed form solution for the KL between two Gaussian distributions should be used to avoid further stochastic approximations. The only difficulty, is that the time complexity of naively evaluating the KL term,

\[_{}((_{t})||\,(_{t}))= (}_{t}-_{t})^{}}_{t}^{-1}(}_{t}-_{t})+(}_{t}^ {-1}_{t})+(|}_{t}|/|_{t}|)-L\] (27)

scales \((L^{3})\). However, since matrix vector multiplies with \(}_{t}^{-1}\) can be performed efficiently and the trace/log-determinant terms can be rewritten using the square-root factors acquired during the forward pass, as we describe in App. C.1, it is possible to evaluate the KL in \((LSr+LS^{2}+Lr^{2})\) time. After a complete forward pass through the encoded data, we acquire the samples \(_{1:T}^{1:S}\) and all necessary quantities for efficient ELBO evaluation. We detail the variational filtering algorithm in Alg. 2 in App. C.2 and the complete end-to-end learning procedure in Alg. 1.

Causal amortized inference for streaming data.In constructing a fully differentiable variational approximation, the parameters of the approximate marginals were effectively amortized according to a recursion in the natural parameter space by iterating Eqs. (12) and (13). This recursion can be recognized more easily by introducing the function, \(}_{}()\), and writing

\[_{t}=}_{}( _{t-1})+_{t}+_{t+1} }_{}(_{t-1})=  A^{*}((_{t-1};_{t-1}) _{}(_{t-1})\,_{t-1})\] (28)

Figure 2: **a)** Empirical time complexity scaling. Since complexity is a function of \(L\), \(S\), and \(r\), we vary \(L\) (top) for fixed \(r=10\) and (bottom) for fixed \(S=5\); we examine several values of the variable not fixed. Examining wall-clock time shows empirically our implementation scales linearly in \(L\); on the (bottom) we plot wall-clock time for a Kalman filter implementation, showing the standard cubic dependence on \(L\). **b)** (top) Negative ELBO as a function of training epoch when \(N=L\) (bottom) when \(N=L/5\); the left column shows the case \(L=50\) and the right when \(L=100\). Different colors indicate different settings of the local/backward encoder rank; zooming in for \(L=100\), shows low-rank updates can match diagonal ones. **c)** Persitimulus time histogram (PSTH) for the DMFC RSG dataset for different trial condition averages; we consider a context window of \(1.3\)s and a prediction window of \(1.3\)s. **d)** BPS for each method for context/prediction windows.

\( A^{*}:(,-(+^{}))( ^{-1},^{-1})\), and \(A^{*}()\) is the convex conjugate of the log-partition function . So that, \(_{}()\) can be thought of as mapping \(_{t-1}\) forward in time by first taking the expectation of (4) with respect to \((_{t-1};_{t-1})\), and then applying the mean-to-natural coordinate transformation.

One limitation of amortizing inference through the recursion (28) is its inability to produce approximations for the filtering distributions, \(p(_{t}\,|\,_{1:t})\), which can be valuable in streaming or online settings, as well as for testing hypotheses of causality. However, since (17) only depends on the posterior and posterior predictive marginal statistics, we have the freedom to alter our inference network in a way such that filtered marginal statistics are a by-product of obtaining smoothed marginal statistics. For example, an alternative sequence-to-sequence map for \(_{t}\) could be defined by,

\[_{t}=_{}(_{t-1}-_{t})+_{t}+_{t+1},\] (29)

so that \(}_{t}_{t}-_{t+1}\) obey the recursion \(}_{t}=_{}(}_{t-1})+_{t}\) and are natural parameters of an approximate filtering distribution, \((_{t}) p(_{t}\,|\,\,_{1:t})\). Consequently the approximations to posterior and predictive distributions will have a more complicated relationship than they previously did; while efficient sampling and ELBO evaluation are more intricate as a result - linear time scaling in the state-dimension can still be achieved with additional algebraic manipulations, as we show in App. C.2.

``` Input:\(_{1:T}\) while not converged do for\(t=T\) to 1 do \(_{t}=(_{t})\) # local encoder \(_{t}=([_{t+1}\,\,_{t}])\) # backward encoder \(_{t}=_{t}+_{t}\) \(_{t}=[_{t}\,\,_{t}]\) endfor \(_{1:T}^{1:S},_{1:T},}_{1:T},}}_{1:T}=\,2(_{1:T},_{1:T})\) \(}()=[S^{-1} p(_{t}\,|\,_{t}^{*} )-_{}(_{t}||\,_{t})]\) \((,,)( ,,)-}()\) endwhile Output:\(_{1:T}^{1:S},\,_{1:T},\,}_{1:T}\), \(}}_{1:T}\) ```

**Algorithm 1** End-to-end learning

## 5 Experiments

Time complexity & low-rank precision updates.We first investigated the properties of low-rank variational Gaussian approximations in the large \(L\) regime. To guide us, we had several questions in mind such as: i) how does the performance of low-rank approximations compare to full-rank and diagonal covariance approximations, ii) how large compared to \(L\) should the rank of precision updates be to achieve satisfactory results, and iii) how does convergence using low-rank approximations compare to diagonal approximations, considering that they require a larger number of parameters. We expect that full-rank approximations would perform best (given a sufficient amount of data), since the true posterior will have dense second-order statistics due to the interactions of latent states in both the dynamics and observation models. However, it remains unclear how many dimensions are necessary for a low-rank approximation to achieve similar performance and whether this number will be practical.

We simulated data from \(50\)D and \(100\)D linear dynamical systems and compared the convergence between dense and diagonal approximations (Fig. 2**b**); we examined the ELBO for different rank parameterizations in two regimes i) observations and states are of the same dimensionality, \(N=L\) (Fig. 2**b** - top), and ii) observations are lower dimensional than states, \(N=L/5\) (Fig. 2**b** - bottom). While not surprising that dense variational Gaussian approximations achieve superior performance, message passing in latent Gaussian models with dense covariance scales like \((L^{3})\) and becomes prohibitive for large \(L\); thus it is reassuring that in both regimes, low-rank approximate posterior parameterizations achieve comparable results for precision matrix updates of relatively low rank compared to \(L\). To empirically examine time complexity scaling as a function of \(L\), \(S\), and \(r\), in Fig. 2**a**, we plot wall-clock times for fixed \(r\) while varying \(S\) and \(L\) (bottom), and for fixed \(S\) while varying \(r\) and \(L\) (top); reassuringly, inference time complexity scales \((L)\).

Baseline comparisons - pendulum & bouncing ball.Next, we wondered how our approach fared against other modern deep state-space models when it came to learning complex dynamical systems from data. To explore this, we considered two popular datasets: **i)** a pendulum system  and **ii)** a bouncing ball . Each dataset consists of sequences of observations that are \(16 16\) pixel images that are governed by a low-dimensional dynamical system. An interesting aspect of these datasets is that images can be reconstructed with impartial knowledge of the latent state, but for accurate long-term predictions, the dynamics will need to propagate features of the latent state that are irrelevant to the likelihood (e.g. pendulum angular velocity). For benchmarks, three other deep SSM approaches were included: **i)** deep variational Bayes filter(DVBF) **ii)** deep Kalman filter(DKF) **iii)** structured VAE(SVAE) . We denote our causal amortization network with (c) and the noncausal version with (n).

We trained all models in context windows of \(50\) consecutive images and then sampled future \(50\) / \(25\) time-step latent states from the learned dynamical system for pendulum / bouncing ball. To measure quality of learned latent representation and dynamics, we fit angular velocity / position decoders from training set latent states inferred from pendulum / bouncing ball observations. Then, on held-out test data, we measured the \(R^{2}\) of velocity / position predictions during the context (smoothing) and forecast (prediction) windows. Fig. 1 shows that all methods are able to reconstruct well in context windows, however, when prediction is concerned, where the underlying dynamics would need to be learned well for accurate forecasts, our method consistently performs better.

Neural population dynamics.We consider two neuroscientific datasets where previous studies have shown the importance of population dynamics in generating plausible hypothesis about underlying neural computation. In addition to DKF, DVBF, and SVAE, we include the LFADS method . First, we considered recordings from motor cortex of a monkey performing a reaching task  and evaluate each methods' ability to forecast neural spiking and behavioral correlates. We measure the performance by bits-per-spike (BPS) using inferred spike-train rates  and \(R^{2}\) for decoding hand velocity. Similar to the previous experiment, we evaluate the performance in two regimes: **i)** a \(700\)ms context window and **ii)** a \(500\)ms prediction window following an initial context window of \(200\)ms. Fig. 3**c** shows that, while all the methods excel at smoothing in the context window, our method makes more informative

Figure 3: **Predict behavior from a causally inferred initial condition.****a)** Actual reaches. **b)** (top) Reaches linearly decoded from smoothed (\(R^{2}=0.89\)), causally filtered (\(R^{2}=0.88\)), & predicted (\(R^{2}=0.74\)) latent trajectories starting from an initial condition causally inferred during the preparatory period. (bottom) Top 3 principal latent dimensions per regime (smoothing/filtering/prediction) for three example trials. **c)** bps / \(R^{2}\) of predicted hand velocity using rates inferred from the \(700\)ms context window and the \(500\)ms prediction window. **d)** Velocity decoding \(R^{2}\) using predicted trajectories as a function of how far into the trial the latent state was filtered until it was only sampled from the autonomous dynamics; by the the movement onset, behavioral predictions using latent trajectory predictions are nearly on par with behavior decoded from the smoothed posterior.

predictions in terms of \(R^{2}\) and BPS. Next, we examined how well the monkey's behavior could be predicted given only causal estimates of the latent state; we trained a model using the causal amortized inference network, given by Eq. (29), then use learned inference network to infer latent states to predict behavior in three regimes: smoothing, filtering, and prediction. Fig. 3**b** shows that hand velocity can be decoded nearly as well in the filtering regime (without access to future data) as in the smoothing regime. In Fig. 3**d**, we plot how the quality of predictions change as filtered latent states are unrolled through the learned dynamics at different points in the trial; showing that forecasts starting prior to movement onset exhibit strong predictive capability.

Next, we investigated our method's performance with data exhibiting a more intricate trial structure. Specifically, we analyzed physiological recordings from the DMFC region of a monkey engaged in a timing interval reproduction task . During this task, the monkey observes a random interval of time (termed the'ready'-'set' period) demarcated by two cues, and the goal of the monkey is to reproduce that interval (termed the'set'-'go' period). We perform a similar procedure as before, but for this experiment we use the period before'set' as the context window, and use the learned dynamics to make predictions onward; in Fig. 2**d** we show the BPS measured on test data during the context and forecast windows. To further investigate the predictive capabilities, we examined condition averaged PSTH produced by samples from the latent state posterior during the joint context/prediction windows. Using the trained model, we sample spike valued observations for the context/prediction windows and then computed condition averaged PSTHs; the results shown in Fig. 2**c**, show that PSTHs sampled from the model remain true to the data, even during the lengthy prediction window.

## 6 Discussion

We presented a new approximate variational filtering/smoothing algorithm, variational learning objective, and Gaussian inference network parameterization for nonlinear state-space models. Focusing on approximations parameterized by dense covariances forced us to consider strategies that ensured computational feasibility for inference and learning with large \(L\). The introduced approximate variational filtering algorithm, while especially useful for the nonlinear dynamics we considered, could also be applied to high-dimensional linear systems where exact computations might be infeasible for large \(L\). Although our variational objective loses the property of lower-bounding the original data log-marginal likelihood, experiments showed that our method consistently outperforms approaches using potentially tight lower bounds. Quantifying this gap or considering potential corrections present interesting directions for future work. Furthermore, while the same variational approach is applicable to any exponential family dynamical system, specific distributions will have their own associated challenges, offering avenues for further research.

Given that neural computation is inherently nonlinear, system identification methods capable of modeling nonlinear dynamical systems are essential for advancing neuroscience. General SSMs can perform well on inferring smoothed latent state trajectories _without_ learning a good model of the nonlinear dynamics. Our proposed method, XFADS, can not only perform efficient system identification and smoothing but also forecast future state evolution for population recordings--a hallmark of a meaningful nonlinear dynamical model; using a causal inference network, XFADS can be used for real-time monitoring, feedback control, and online optimal experimental design, opening the door for new kinds of basic and clinical neuroscience experiments. Future work will focus on developing network architectures for precision matrix updates that are more parameter efficient when the rank those updates become moderate. Moreover, while Alg. 1 remains applicable in the confines of the generative model constraints considered, in certain scenarios, such as when \(S>L\), modifications will need to be made to Alg. 2 to minimize time complexity.