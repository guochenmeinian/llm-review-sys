# ###### Abstract

###### Abstract

Recent works have shown that diffusion models are able to memorize training images and emit them at generation time. However, the metrics used to evaluate memorization and its mitigation techniques suffer from dataset-dependent biases and struggle to detect whether a given specific image has been memorized or not. This paper begins with a comprehensive exploration of issues surrounding memorization metrics in diffusion models. Then, to mitigate these issues, we introduce SolidMark, a novel evaluation method that provides a per-image memorization score. We then re-evaluate existing memorization mitigation techniques and show that SolidMark is capable of evaluating fine-grained pixel-level memorization. Finally, we release a variety of models based on SolidMark to facilitate further research for understanding memorization phenomena in generative models. We include our supplementary materials at https://drive.google.com/drive/u/3/folders/lvpu5FM_Gs1ldFogw405p-ehljynA85R.

###### Contents

* 1 Introduction
* 2 Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Rombach et al., 2022) have gained prominence because of their ability to generate remarkably photorealistic images. However, they have also been subject to scrutiny and litigation (Saveri & Butterick, 2023) owing to their ability to regurgitate potentially copyrighted training images. Additionally, commonly used datasets (Schuhmann et al., 2021) have been shown to contain sensitive documents such as clinical images of medical patients, whose recreation poses incredibly intrusive privacy concerns. As a result, recent works (Sompealli et al., 2023;p Carlini et al., 2023; Wen et al., 2024; Ren et al., 2024; Kumari et al., 2023b) have looked to quantify, explain, and mitigate memorization in diffusion models.
* 35 Crucially, reliable and effective quantification of memorization requires sound metrics. Although a few proposed metrics serve as powerful memorization indicators, there exist disagreements in terms of how they should be applied (Chen et al., 2024). The typical way in which a _given_ image is declared to be memorized is if it is produced in a pixel-exact manner at inference time. However, such a generation can be challenging to induce, even if the training prompt is known, due to inherent stochasticity present in diffusion model inference. This problem is even harder in unconditional models, where there are no knobs to guide the generation towards a given target image. If such a generation is not observed, the user is not provided with any strong indication on whether the model has knowledge of the image.
* 43 Memorization metrics usually consist of (i) some distance measure \(\) between a model generation and its training dataset1 and (ii) some scoring function that takes in a large number of these distance values (from many generations) and outputs a scalar metric. For example, a commonly used metric for memorization is the 95th percentile (scoring function) of SSCD similarities (Pizzi et al., 2022; Sompealli et al., 2023; Chen et al., 2024), an embedding-based distance between each generation and its nearest training image.

In this paper, we propose SolidMark, an approach that allows for the precise quantification of pixel-level memorization. The basic idea is simple: SolidMark augments each image with a grayscale border of _random intensity_ (see Fig. 1). At evaluation time, we in only the image's border in a task we call outpainting (as an analogy to inpainting). Since the pattern is randomized _independently_ for each image, a correct reconstruction of the pattern's color indicates strong memorization of the sample. The idea of using this pattern is closely related to watermarking as it is reflective of the source of an image generation, but there are also some key differences that distinguish it: (i) a watermark should be difficult to remove or forge, whereas our pattern is easily removable; (ii) a watermark only needs to be detectable, but our pattern needs to be precisely reconstructed to provide a continuous metric for quantifying memorization; (iii) the value of the key should be unrelated from the content of the image, which is not required for a watermark.

We designed SolidMark to be included in new models or finetuned into existing ones. Since the image's border can be easily cropped out when using generated images, SolidMark is a efficient way to evaluate memorization in diffusion models. To encourage further exploration, we release a Stable Diffusion (SD) 2.1 model injected with SolidMark's patterns during pretraining. Subsequently, we re-evaluate existing memorization mitigation techniques with SolidMark. We demonstrate the method's ability to evaluate fine-grained pixel-level memorization and its universal compatibility, testing it on five different datasets in a variety of settings. We provide in Table 1 a summary of the strengths and weaknesses of SolidMark compared to existing evaluation methods in the field.

Our main contributions are the following:

* _An in-depth exploration of existing memorization metrics,_
* SolidMark_, a new method for precise evaluation of pixel-level memorization,_
* _A variety of models trained specially for evaluating memorization._

   Metric & Reconstructive & Pixel-Level & Evaluation of & Caveat \\  & Memorization & Memorization & Any Image & \\  SSCD Similarity & ✓ & ✗ & ✗ & Out-of-Distribution Datasets \\ \(_{2}\) Distance & ✗ & ✓ & ✗ & Monochromatic Images \\ \(061\) & SolidMark & ✗ & ✓ & ✓ & Excessive Duplication \\   

Table 1: **Use Cases of Different Metrics.**

Figure 1: **An overview of SolidMark. We begin by augmenting training images with random scalar keys in the form of grayscale borders. Next, we inject these keys into the model by training it on these augmented images. To query for a key, we ask the model to outpaint a training image’s border using the training caption as the text prompt. We retrieve its prediction at the key by averaging the outpainted border. Finally, we report the distance between the predicted key and the true value.**

## 2 Background and Related Work

Detecting Memorization in Diffusion Models.Many works have aimed to detect memorization in diffusion models (Sompealli et al., 2023; Carlini et al., 2023; Kumari et al., 2023b). A generative model that memorizes data might be especially vulnerable to membership inference attacks, in which the goal is to determine whether an image belongs to the original training set (Carlini et al., 2022; Hu & Pang, 2021; Wen et al., 2023). One notable example of a membership inference attack is an inpainting attack from Carlini et al. (2023), who show that a diffusion model's performance on the inpainting task significantly increases for memorized images.

Mitigating Unwanted Generations.A number of works (Sompealli et al., 2023; Chen et al., 2024; Wen et al., 2024; Ren et al., 2024) have introduced methods to mitigate memorization in diffusion models. These methods either perturb training data to decrease memorization as the model trains or perturb inputs at test time to decrease the model's chances of recalling memorized information. Although most mitigation techniques usually involve augmenting data with some type of noise (Sompealli et al., 2023b), other works attempt to alter generation trajectories using intuition about the causes for memorization (Chen et al., 2024). To prevent Stable Diffusion models from generating unwanted outputs, various concept erasure techniques have been proposed (Gandikota et al., 2023; Pham et al., 2024; Gandikota et al., 2024; Kumari et al., 2023a). Although these methods were initially developed to erase broad concepts, they can also target specific images.

Image Watermarking.Classically, image watermarking allows for the protection of intellectual property and has been accomplished for years with simple techniques like Least Significant Bit embedding (Wolfgang & Delp, 1996). Recently, more complex deep learning-based methods (Zhu et al., 2018; Zhang et al., 2019; Lukas & Kerschbaum, 2023) have been suggested. For generative models, watermarking allows developers to discrectly label their model-generated content, mitigating the impact of unwanted generations by increasing their traceability. Some works attempt to fine-tune watermarks into existing diffusion models (Zhao et al., 2023; Fernandez et al., 2023; Xiong et al., 2023; Liu et al., 2023).

Needle-in-a-Haystack Evaluation for LLMs.Some recent works (Fu et al., 2024; Kuratov et al., 2024; Wang et al., 2024; Levy et al., 2024) have used Needle-in-a-Haystack (NIAH) evaluation (Kamradt, 2023) to test the long-context understanding and retrieval capabilities of Large Language Models (LLMs). In this test, a short, random fact (needle) is placed in the middle large body of text (hyastack). This augmented corpus is passed into the model at inference. Subsequently, the model is asked to recall the needle; by changing the size of the context window and shifting the needle around, testers are able to evaluate the in-context retrieval capabilities of LLMs. If the model is able to successfully retrieve the needle from the haystack with a high consistency, developers can be more confident that it will be able to recall specific information from large context windows. Similar to how NIAH evaluation takes a large context window and injects a small, unrelated phrase as a key, we inject our training images with scalar keys using a small, unrelated border.

## 3 Existing Memorization Evaluation Methods

Types of Memorization.Memorization in diffusion models can usually be classified into either pixel-level or reconstructive. Pixel-level memorization (Carlini et al., 2023), is identified by a near-identical reconstruction of a particular training image. That is, even if a generation contains recreations of certain objects or people from the training data, a given generation would only be considered reflective of pixel-level memorization if the full image was almost entirely identical to a specific training image. In this sense, the process of recovering a pixel-level memorized image is analogous to extracting a training image from the model. Alternatively, reconstructive memorization represents a more semantic type of data replication. It is identified by the replication of specific objects or people found in training images, even if the generation in question has a high pixel distance from all training images (Sompealli et al., 2023a).

Measuring Memorization.Neither pixel-level nor reconstructive memorization have precise mathematical definitions, making it rather difficult to declare whether or how strongly a trainingimage is memorized. Instead, when constructing metrics, the prior works attempt to construct mathematical measures for a given generation's similarity to the model's training set. These measures, in turn, can identify memorizations when they occur at generation time. Specifically, for a training dataset \(\) and a generation \(}_{0}\), researchers will either use some distance function \((}_{0},)\), with lower values indicating a higher likelihood of memorization, or a similarity function \((}_{0},)\), with higher values indicating a higher likelihood of memorization. After collecting these values for a large number of generations, they are converted into an overall score for a model: for example, the 95th percentile of all similarities is a common scoring function (Somepalli et al., 2023; Chen et al., 2024). Past works also track the overall maximum similarity value (Chen et al., 2024). Notably, Carlini et al. (2023) track the proportion of generations with distances under a certain threshold, defined as "eidetic" memorization. We use similar language, which we define in Definitions 1, 2.

**Definition 1** (Eidetic Metric).: _A metric that counts the number of distances \(\) below a threshold \(\)._

**Definition 2** (Eidetic Memorization).: _A training image \(\) is said to be \((,)\)-eidetically memorized if the respective model returns a generation \(}_{0}\) where \((}_{0},)\)._

### Evaluating Existing Distance Functions

Modified \(_{2}\) Distance.A common choice of the distance function \(\) as an indicator for pixel-level memorization is a modified \(_{2}\) distance that was introduced in Carlini et al. (2023). For this, following Balle et al. (2022), Carlini et al. (2023) start building their metric from the baseline of normalized Euclidean 2-norm distance, defined as

\[_{2}(,)=(a_{i}-b_{i})^{2}}{d}}\]

for \(,^{d}\). When using this distance \(_{2}(}_{0},)\) between a generation \(}_{0}\) and its nearest neighbor \(\) in the training set \(\), they find that nearly monochromatic images, such as images of a small bird in a large blue sky, dominate the reported memorizations.

To counteract this issue, Carlini et al. (2023) rescale the \(_{2}\) distance of a generation based on its relative distance from the set \(_{}_{0}}\) of \(}_{0}\)'s \(n\) nearest neighbors in \(\). Namely, for \(_{}_{0}}\) and \(|_{}_{0}}|=n\), we have that

\[_{_{}_{0}}}_{2}( {}_{0},)_{y_{}_{0}}}_{2}( {}_{0},)\,.\]

They then define the modified \(_{2}\) distance as

\[_{2}(}_{0},;_{}_{0}})=(}_{0},)}{_{_ {}_{0}}}[_{2}(}_{0},)]}\,,\]

where \(\) is a scaling factor. This distance decreases when \(}_{0}\) is much closer to its nearest neighbor when compared to its \(n\) nearest neighbors, potentially indicative of memorization.

Following their setting, we conducted experiments using DDPMs pretrained on CIFAR-10 (Krizhevsky, 2009). See Appendix Section A for implementation details. In Figure 2, we show examples of the strongest memorizations reported by \(_{2}\) distance, demonstrating that the measure still reports monochromatic images as false positives. Most of the reported memorizations were only classified as such because they are blurry and monochromatic (which gives them an easier time matching other monochromatic images in the training set). Crucially, though, these images are _not_ memorizations, because they do not contain any specifically recreated image features unique to the training set (Naseh et al., 2023). Because of this lack of specificity, we found that their metric was not a satisfying solution to detect pixel-level memorization. We apply more scrutiny to memorization metrics based on \(_{2}\) distance, as this bias towards monochromatic images has proven remarkably difficult to thoroughly eliminate.

Embedding-Based Similarity.Although pixel-wise distances present an intuitive approach for detecting pixel-level memorization, they are not as tailored towards reconstructive memorization. Instead, for the reconstructive case where semantic similarity is more relevant, perceptual similarity measures based on models such as SSCD (Pizzi et al., 2022), DINO (Caron et al., 2021), and CLIP (Radford et al., 2021) are often used (Sompalli et al., 2023; Carlini et al., 2023). These metrics are generally structured with dot product similarities in a semantic embedding space, such as:

\[(}_{0},)= E(}_{0}),E()\]

where \(E()\) represents the embedding of an image \(\) generated by a deep visual encoder. Perceptual metrics are robust to slight perturbations of training images such as small perspective changes. Although they perform well with reconstructive memorization, models like DINO suffer with detecting pixel-level memorization (Sompalli et al., 2023).

One important quality of a memorization metric is the ability to remain effective and precise across different datasets. Unfortunately, past works (Carlini et al., 2023) have seen issues when attempting to translate perceptual metrics that work on Stable Diffusion to other datasets. Therefore, although the literature denotes SSCD as the standard metric for detecting reconstructive memorization (Sompalli et al., 2023; Chen et al., 2024), it should likely only be used with datasets such as LAION-5B (Schuhmann et al., 2022) or ImageNet (Deng et al., 2009) that fit its training dataset.

### Inspecting Scoring Strategies

Until now, we have only discussed the importance of using a consistent and reliable distance measure. It is just as important to use a scoring function that is sensitive to overall changes in memorization and does not fluctuate with unrelated changes in the model. Three strategies to aggregate a set of distances into a score include:

(i) the 95th percentile of similarities, (ii) the maximum similarity value, and (iii) eidetic metrics. Recently, 95th percentile scoring was employed in Sompalli et al. (2023), where the 95th percentile of SSCD similarities was used as a metric for a number of memorization mitigation techniques. Subsequent work (Chen et al., 2024), however, questioned the validity of percentile-based scoring strategies in memorization metrics, especially when the returned distribution of distances is heavy-tailed. Figure 3 shows an example where a percentile metric could misrepresent a distribution of similarities. As a remedy, Chen et al. (2024) propose two alternatives. They recommend tracking (i) the maximum of all similarities and (ii) the number of similarities that lie above a certain threshold, a scoring idea introduced in Carlini et al. (2023). Using the maximum of all similarities could be susceptible to outliers and may not necessarily be representative of large scale trends in the similarity distribution. On the other hand, recording the number of similarities above a threshold \(\), also known as _eidetic memorization_, has proved to be effective. Importantly, existing literature (Sompalli et al.,

Figure 3: **95th percentile scoring fails to capture fine-grained reductions in memorization.** The above graphs demonstrate how a 95th percentile metric can fail to report successful memorization reduction. (**Top**) A distribution showing the density (vertical axis) of different similarity values (horizontal axis) in a model’s baseline results. (**Bottom**) The memorization-reduced evaluation, where the 95th percentile did not change at all despite clear memorization reductions shown in the 96th percentile.

Figure 2: \(_{2}\) **distance reports monochromatic images as memorizations.** Despite not being memorizations of their nearest neighbors in the training set, monochromatic images generate a low \(_{2}\) distance. (**Top**) Out of 5,000 generations, the 10 generations with smallest patched \(_{2}\) distance from CIFAR-10 train. (**Bottom**) The corresponding nearest neighbors in CIFAR-10 train to the top row of generations.

2023b; Chen et al., 2024; Kumari et al., 2023b) uses eidetic metrics with only one threshold. The problem with single threshold methods is that they do not probe how the distribution of similarities could be concentrated. Instead, multiple values for \(\) should be tracked to avoid flawed analysis. We elaborate on this point below in our experiments.

## 4 SolidMark: A Method to Evaluate Per-Image Memorization

Motivation.Performance on inpainting tasks significantly increases for memorized images (Carlini et al., 2023; Daras et al., 2024). Therefore, we choose inpainting as the foundation of our method. This task also stands out because of its ability to function as a key-query mechanism: by masking out part of a training image, we can provide the unmasked portion to the model as a 'query' and ask it to recall the 'key' (the masked portion) from memory. Yet, two issues need consideration:

First, with inpainting, the key is almost definitely semantically related to the query, meaning the model still has a good chance to infer the masked portion of an unseen image. Additionally, the amount of useful information in the unmasked portions of different images may vary significantly, making it difficult to develop a general baseline for the model's performance on an unmemorized image. That is, it would be harder to inpaint a memorized complex image than certain unmemorized simple images. Second, since the key for inpainting is essentially a smaller image, the problems with earlier distance metrics could just propagate. For example, relatively accurate inpaintings might still produce high \(_{2}\) distances for various reasons.

Method Structure.To solve these issues, we assign a random scalar key to each image and embed it as the intensity of a grayscale border around that training image. By training the model on these augmented images, we teach it to output the correct grayscale intensity in the borders of an image, if memorized (see Fig. 1). Since the keys and queries are unrelated, the model outputs random grayscale borders from the distribution of the training keys for an unmemorized image.

At evaluation time, we prompt the diffusion model to outpaint the border for a training image using the training caption as the text prompt and evaluate its accuracy with a scalar distance function between the grayscale intensities. This strategy solves both of our previous issues: First, since the key is unrelated to the query, we minimize the probability of inferring the key by chance. Second, since our key is a scalar, we can directly use a scalar distance function between keys (grayscale intensities) instead of using a pixel distance function. We refer to this distance as \(_{}\) (SM = SolidMark). We provide pseudocode and explain SolidMark's hyperparameters in Appendix Section C.

## 5 Evaluation

Initial ValidationsSince visual transformer models have been shown to pay extra attention to the center of images (Raghu et al., 2021), we were concerned that keeping the patterns as borders would uncover less memorizations than a centered pattern. For this reason, we ablated for the position of the pattern on STL-10 (Coates et al., 2011), for which the results and implementation details are in D. Although centered patterns did perform slightly better, we still choose to use border patterns, since the performance benefit is not worth the intrusiveness to the image generation. We also validate in Appendix Section E that SolidMark is able to evaluate memorization in unconditional models.

Pretraining a Foundation ModelWe describe the process of pretraining a large foundation model to facilitate the widespread use of SolidMark in Appendix Section J. We also show samples of images generated by our pretrained text-to-image model.

### The Role of Data Duplication

One important concern about SolidMark is that, since its keys are completely random, it sees no association between duplicated images in the training set (duplicate images will have different border colors). For this reason, one may worry that it could fail to capture memorization induced by data duplication, which is one of the most important contributing factors to memorization (Somepalli et al., 2023b). Following this concern, we introduced a large amount of exact data duplication into LAION-5K, a randomly sampled 5,000 image subset of LAION-400M (Schuhmann et al., 2021).

Next, we assigned each of these duplicated images independent random keys to mimic how they would receive different keys in practice. We then finetuned SD 2.1 on this subset and evaluated the percentage of images for which SolidMark reported memorization of at least one of its respective keys. Table 2 shows that SolidMark still reports increased memorization as training set duplication increases. Implementation details are in Appendix Section F.

### How Fine-Grained is SolidMark?

In order to understand the cues the model uses to construct memorized borders, we evaluate whether the information the model utilizes is based on the semantics of the image or on more fine-grained pixel-exactness. We evaluated changes in reported memorization as a response to small perturbations applied to the query image. To do this, we augmented LAION-5K with SolidMark's borders and finetuned SD 2.1 on the augmented dataset. At evaluation time, we applied different augmentations like cropping, rotation, or blurring to the query image and observed changes in the model's memorization performance. Examples of these augmentations and implementation details are in Appendix Section G. Overall, our results in Table 4 show that even minor perturbations to query images significantly disrupt the model's ability to recall the border color, especially when the required accuracy \(\) is small. These changes are not semantically meaningful and are sometimes barely visually perceptible. For this reason, we classify SolidMark's reported memorizations as instances of fine-grained pixel-level memorization.

### Re-Evaluating Mitigation Techniques

We use SolidMark to evaluate the degree of pixel-level memorization mitigation, achievable with inference-time memorization reduction techniques. We sourced these mitigation techniques, which are described in Appendix Section H, from Sompealli et al. (2023b). For our evaluation, we augmented LAION-5K with our solid borders and finetuned SD 2.1 on this augmented dataset. We then compared the percentage decrease in \((_{},0.01)\)-eide memorizations in our model against the percentage decrease of 95th percentile SSCD similarities observed in Sompealli et al. (2023b). See Table 3 for these results. Overall, we did not find that any of the mitigation techniques that we tried significantly reduced memorization as measured by SolidMark. These results are corroborated by our results in Appendix Section I. We propose this difference exists because SolidMark is an evaluation method primarily led by visual cues in its query image. Perturbations to the queries that could, at best, dilute or change the semantic meaning of the prompt, lack a profound effect on the model's performance when the dominant visual cues are still present.

## 6 Discussion

**Evaluating Individual Images.** SolidMark is unique among memorization metrics in its ability to directly evaluate specific training images. In a traditional setting, one would need to repeatedly prompt a model and randomly encounter a training image to decide that it was memorized. This is problematic because prompting the model repeatedly with a very common training caption has a low

  
**Replications of Training Example** & \(=0.1\) & \(=0.05\) & \(=0.005\) \\ 
2 Instances & 50\% & 36\% & 10\% \\
3 Instances & 56\% & 60\% & 26\% \\
4 Instances & 56\% & 56\% & 24\% \\
5 Instances & 68\% & 72\% & 36\% \\   

Table 2: **Reported Memorizations with Increasing Duplication.** SolidMark is able to detect increased memorization in models as a response to increased duplication in the training set, even if the duplicates are assigned different keys. This is evidenced by an increase in the percentage of images reported as memorized at all eidetic thresholds \(\) as we increase the number of instances of duplicated images in the training set. Higher percentages indicate more memorizations. Implementation details are in Appendix Section F.

chance of reproducing a given target image. Additionally, in unconditional models, which have been shown to memorize sensitive medical imaging data (Dar et al., 2024), there is no direct way to guide the output towards a specific image. SolidMark, in both cases, provides an effective method to test for the memorization of specific images. In addition, it provides a continuous measure of "how memorized" an image is.

Limitations. By the nature of the difficulty of the setting, our method may not report memorizations that are not strong enough to capture the key. Additionally, our evaluation method has a false positive probability based on the chance of an unmemorized color randomly fitting to the key of a specific image. Additionally, SolidMark may struggle with accurately reporting memorization caused by excessive exact duplication. For this reason, we encourage its use in tandem with other metrics. For an in-depth guide on how we recommend choosing a metric for a specific use case, see Appendix Section K.

  
**Random Crop Strength** & \(=0.1\) & \(=0.05\) & \(=0.005\) \\  Baseline (0) & 1085 & 557 & 68 \\
1 & 1038 (\(4.33\%\)) & 549 (\(1.44\%\)) & 58 (\(14.71\%\)) \\
2 & 1060 (\(2.30\%\)) & 541 (\(2.87\%\)) & 48 (\(29.41\%\)) \\
3 & 1035 (\(4.61\%\)) & 552 (\(0.90\%\)) & 49 (\(27.94\%\)) \\
4 & 1042 (\(3.96\%\)) & 528 (\(5.21\%\)) & 50 (\(26.47\%\)) \\ 
**Rotation Angle** & & & \\  Baseline (\(0^{}\)) & 1085 & 557 & 68 \\ \(-2^{}\) & 1029 (\(5.16\%\)) & 506 (\(9.16\%\)) & 51 (\(25.00\%\)) \\ \(-1^{}\) & 1053 (\(2.95\%\)) & 540 (\(3.05\%\)) & 59 (\(13.24\%\)) \\
409 & 1008 (\(7.10\%\)) & 502 (\(9.87\%\)) & 51 (\(25.00\%\)) \\
410 & 2\({}^{}\) & 1047 (\(3.50\%\)) & 528 (\(5.21\%\)) & 45 (\(33.82\%\)) \\
411 & 180\({}^{}\) & 1044 (\(3.78\%\)) & 522 (\(6.28\%\)) & 47 (\(30.88\%\)) \\ 
**Gaussian Blur Strength** & & & \\  Baseline (0) & 1085 & 557 & 68 \\
1 & 1049 (\(3.32\%\)) & 516 (\(7.36\%\)) & 42 (\(38.24\%\)) \\
2 & 1064 (\(1.94\%\)) & 503 (\(9.69\%\)) & 45 (\(33.82\%\)) \\
416 & 3 & 1089 (\(0.37\%\)) & 534 (\(4.13\%\)) & 53 (\(22.06\%\)) \\
417 & 4 & 1033 (\(4.79\%\)) & 506 (\(9.16\%\)) & 62 (\(8.82\%\)) \\   

Table 4: **Memorizations Reported with Increasing Augmentation Strength. We show that SolidMark, especially as \(\) decreases, reports extremely fine-grained memorizations. As we apply random cropping, rotation, and blurring to query images, the model’s key prediction accuracy, measured by the number of reported \((_{},)\)-eidetic memorizations, significantly deteriorates. Higher reduction percentages indicate that the model is struggling to recognize the augmented images.**

  
**Metric** & **GNI** & **RT** & **CWR** & **RNA** \\ 
95th Percentile of SSCD Similarities & 3.62\% \(\) & **16.29\%**\(\) & 9.20\% \(\) & 14.33\% \(\) \\

[MISSING_PAGE_POST]

\*  Reproducibility Statement
*  We include our source code for this project in the supplementary materials and release a variety of trained models to ensure that reviewers and readers can try out SolidMark for themselves.

### Impact Statement

We introduce SolidMark as a non-intrusive framework that can help developers evaluate and study memorization in their models. With our recommendations for how memorization metrics should be built, we hope to foster discussion about how existing metrics can be improved upon, interpreted, and generalized. Altogether, more robust evaluation of generative models helps mitigate negative privacy outcomes owing to uncaught memorization.