# Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability

Haotian Xue\({}^{*}{}^{1}\)  Alexandre Araujo \({}^{2}\)  Bin Hu \({}^{3}\)  Yongxin Chen \({}^{1}\)

\({}^{1}\) Georgia Institute of Technology

\({}^{2}\) New York University

\({}^{3}\) University of Illinois Urbana-Champaign

Correspondence to: htxue.ai@gatech.edu

###### Abstract

Neural networks are known to be susceptible to adversarial samples: small variations of natural examples crafted to deliberately mislead the models. While they can be easily generated using gradient-based techniques in digital and physical scenarios, they often differ greatly from the actual data distribution of natural images, resulting in a trade-off between strength and stealthiness. In this paper, we propose a novel framework dubbed Diffusion-Based Projected Gradient Descent (Diff-PGD) for generating realistic adversarial samples. By exploiting a gradient guided by a diffusion model, Diff-PGD ensures that adversarial samples remain close to the original data distribution while maintaining their effectiveness. Moreover, our framework can be easily customized for specific tasks such as digital attacks, physical-world attacks, and style-based attacks. Compared with existing methods for generating natural-style adversarial samples, our framework enables the separation of optimizing adversarial loss from other surrogate losses (_e.g._, content/smoothness/style loss), making it more stable and controllable. Finally, we demonstrate that the samples generated using Diff-PGD have better transferability and anti-purification power than traditional gradient-based methods. Code is available at https://github.com/xavihart/Diff-PGD

## 1 Introduction

Neural networks have demonstrated remarkable capabilities in learning features and achieving outstanding performance in various downstream tasks. However, these networks are known to be vulnerable to subtle perturbations, called adversarial samples (adv-samples), leading to important security concerns for critical-decision systems . Numerous threat models have been designed to generate adversarial samples for both digital space attacks , physical world attacks , and also for generating more customized adv-samples given prompts like attack region and style reference . However, most of these models have not been designed to maintain the realism of output samples, resulting in adversarial samples that deviate significantly from the distribution of natural images . Indeed, in the setting of digital world attacks, such as those involving RGB images, higher success rates, and transferability are associated with larger changes in the generated samples, leading to a stealthiness-effectiveness trade-off. Additionally, perturbations for physical-world attacks are substantial, non-realistic, and easily noticeable by humans .

Recent works, for both digital and physical settings, have proposed methods to generate more realistic adversarial samples. For example,  introduced a method to optimize the perturbations added to clean images in semantic space,  proposed a GAN-based approach or other prior knowledge  to increase the realism of generated samples. Although the GAN-based approachcan generate realistic images, the adversarial examples are sampled from noise and therefore lack controllability. A subsequent line of research on realistic adversarial samples  has introduced the concept of semantic adversarial samples. These are unbounded perturbations that deceive the model, while ensuring the modified image has the same semantic information as the original image. While transformations in semantic space can reduce the high-frequency patterns in pixel space, this approach still suffers from color jittering or image distortion which results in lower stealthiness . Further, this approach needs careful tuning of the hyperparameters making the training process unstable.

In this paper, we propose a novel framework, which uses an off-the-shelf diffusion model to guide the optimization of perturbations, thus enabling the generation of adversarial samples with higher stealthiness. The core design of our framework is Diffusion-Based Projected Gradient Descent (Diff-PGD), which shares a similar structure with PGD but changes the input of the target classifier \(f_{}\) to be purified version \(x_{0}\) of the original input \(x\). To the best of our knowledge, we are the first to use the Diffusion Model to power the generation of adv-samples. Our framework can also be easily adapted to some customized attacks given masks or style references, thereby enhancing its controllability. Our framework (Figure 1(c)) separates the customized process with prompt \(p\) (region mask, style, etc.) with adversarial sample generation, overcoming the drawbacks of previous pipelines: the traditional gradient-based methods (Figure 1(a)) cannot guarantee to generate naturalistic samples when the perturbation level is large; the joint optimization with multiple losses like adv-loss \(l_{adv}\), style loss \(l_{z}\) and realism loss \(l_{r}\) (see Figure 1(b)) is unstable to train and still tend to generate perceptible artifacts. Through extensive experiments in scenarios like digital attacks, masked region attacks, style-guided attacks, and physical world attacks, we show that our proposed framework can effectively generate realistic adv-samples with higher stealthiness and controllability. We further show that the Diff-PGD can help generate adversarial samples with higher transferability than traditional methods without gradient restriction. Our contribution can be summarized as follows:

1. We propose a novel framework Diff-PGD, combing the strong prior knowledge of the diffusion model into adv-sample generation, which helps generate adv-samples with high stealthiness and controllability. Adv-samples generated by Diff-PGD have good properties as described in Table 1.
2. We show that Diff-PGD can be effectively applied to many tasks including digital attacks, customized attacks, and physical-world attacks, outperforming baseline methods such as PGD, AdvPatch, and AdvCam.
3. We explore the transferability and anti-purification properties of Diff-PGD samples and show through an experimental evaluation that adversarial samples generated by Diff-PGD outperform the original PGD approach.

## 2 Related Work

Adversarial attacks aim at maximizing the classification error of a target model without changing the semantics of the images. In this section, we review the types of attacks and existing works.

  
**Methods** & **Stealthiness** & **Scenarios** & **Controllability** & **Anti-Purify** & **Transferability** & **Stability** \\  PGD  & ** & D & ** & * & * & ** \\ AdvPatch  & * & P & ** & (-) & (-) & ** \\ NatPatch  & ** & P & * & (-) & (-) & ** \\ AdvArt  & * & P & ** & (-) & (-) & ** \\ AdvCam  & ** & D/P & ** & (-) & (-) & ** \\ 
**Diff-PGD (Ours)** & ** & D/P & ** & ** & ** & ** \\   

Table 1: **Properties of Diff-PGD vs Other Attacks**: We summarize six metrics for adv-sample generation. “Stealthiness” measures whether the adversarial perturbations can be detected by human observers. “Scenarios” describes the setting in which the method can be applied: \(D\) corresponds to digital attacks and \(P\) for physical attacks. “Controllability” measures whether the method can support customized prompts _e.g._, mask/style reference. “Anti-Purify” measures the ability of the samples to avoid being purified. “Transferability” measures the generalization of the attack to different models. Finally, “Stability” describes the stability of the optimization. (-) stands for non-consideration.

**Norm bounded attacks.** In the digital space, one can easily generate adversarial samples by using gradient-based methods such as Fast Gradient Sign Method (FGSM)  or Projected Gradient Decent (PGD) . These methods aim at maximizing the loss of the target model with respect to the input and then projecting the perturbation to a specific \(_{p}\) ball. It has been observed that these adversarial samples tend to diverge from the distribution of natural images . Based on this observation,  have shown that these attacks can be "purified" using pre-trained diffusion models.

**Semantic attacks.** Some recent works operate semantic transformation to an image:  generate adv-samples in the HSV color space, [48; 13] generate adv-samples by rotating the 2D image or changing its brightness, and [52; 29; 10] try to generate semantic space transformations with an additional differentiable renderer. All these methods are far from effective; they either need additional modules or have color jitters and distortion .

**Customized Attacks with Natural-Style.** Significant efforts have been made toward generating customized adversarial samples (given region and/or style reference) with natural-style: AdvCAM  optimize adversarial loss together with other surrogate losses such as content loss, style loss, and smoothness loss to make the output adversarial sample to be realistic, [23; 26; 55] sample from latent space and then use a Generative Adversarial Network (GAN) to guarantee the natural style of the output sample to fool an object detector, and AdvArt  customized the generation of the adversarial patch with a given art style. However, all these methods share some common issues: optimizing adversarial loss and other surrogate losses (for content-preserving or customized style) needs careful balance and still results in unnatural artifacts in the output samples. Our methods use the diffusion model as a strong prior to better ensure the realism of generated samples.

**Attacks in physical-world.** For physical-world attacks, minor disturbances in digital space often prove ineffective due to physical transformations such as rotation, lighting, and viewing distance. In this context, adding adversarial patches to the scene is a common technique to get the target model to misclassify [4; 14; 1; 25]. Physical-world attacks do not impose any constraints either on the amount of perturbation or on the output style, thus, they tend to produce highly unnatural images that can be easily detected by human observers (_i.e._, lack stealthiness).

## 3 Background

**Diffusion Model.** Diffusion Models (DMs) [19; 41] have demonstrated superior performance in many tasks such as text-to-image generation [37; 2; 38], video/story generation [21; 20; 35], 3D generation [28; 36] and neural science [44; 42]. Pre-trained DMs provide valuable prior knowledge that can be exploited for adversarial robustness. In this context, several works have proposed defense mechanisms such as adversarial purification [34; 53; 43; 27; 34] or DM-enhanced certified robustness [6; 46; 45].

The Denoised Diffusion Probabilistic Model (DDPM)  is a discretized version of DMs and works as follows. Suppose \(x_{0} p(x_{0})\) is a sample from the distribution of natural images. The forward diffusion process gradually adds Gaussian noise to \(x_{0}\), generating noisy samples \([x_{1},x_{2},,x_{t},,x_{T}]\) in \(T\) steps, following a Markov process defined as \(q_{M}(x_{t} x_{t-1})=(x_{t};}\,x_{t-1},\, _{t})\) where \(\) denotes Gaussian distribution \(_{t}\) are fixed values growing from 0 to 1. By accumulating single step \(q_{M}\) we have \(q(x_{t} x_{0})\) as

\[q(x_{t} x_{0})=(x_{t};}\,x_{t-1},\,(1-_{t}))\] (1)

where \(_{t}=1-_{t}\) and \(_{t}=_{s=1}^{t}_{s}\). When \(_{t} 0\), the distribution of \(x_{T}\) becomes an isotropic Gaussian.

The reverse process aims at generating samples from the target data distribution from Gaussian noise \(x_{T}(0,)\) using the reversed diffusion process. The reverse model \(p_{}(x_{t-1} x_{t})\) can be trained by optimizing the usual variational bound on negative log likelihood. Using the re-sampling strategy proposed in , we can simplify the optimization of \(\) into a denoising process, by training a modified U-Net as a denoiser. Then we can generate \(x_{0}\) with high fidelity by sampling on the reversed diffusion:

\[p(x_{0:T})=p(x_{T})_{t=1}^{T}p_{}(x_{t-1} x_{t}).\] (2)

For the reverse process, we define \(R_{}\) parameterized by \(\) as the function to denoise \(x_{t}\) and then get next sampled value as \(x_{t-1}=R_{}(x_{t},t)\).

SDEdit.The key idea of Stochastic Differential Editing (SDEdit)  is to diffuse the original distribution with \(x_{K} q(x_{K} x)\) and then run the reverse denoising process \(R_{}\) parametrized by \(\) in a Diffusion Model to get an edited sample:

\[(x,K)=R_{}( R_{}(R_{}(x_{K},K),K-1),0).\] (3)

Given a raw input, SDEdit first runs \(K\) steps forward and then runs \(K\) steps backward. This process works as a bridge between the input distribution (which always deviates from \(p(x_{0})\)) and the realistic data distribution \(p(x_{0})\), which can then be used in tasks such as stroke-based image synthesis and editing. It can also be used in adversarial samples purification [34; 53].

## 4 Diffusion-based Projected Gradient Descent

We present Diffusion-based Projected Gradient Descent (Diff-PGD), a novel method to generate adversarial samples that looks realistic, using a pre-trained diffusion model. First, we present basic Diff-PGD that works on global image inputs, which shares the same bound settings with \(_{}\)-PGD in Section 4.1. Then, in Section 4.2 we generalize Diff-PGD to scenarios where only regional attacks defined by a customized mask are possible, using re-painting skills in the diffusion model . Finally, we present extensions of Diff-PGD to customized attacks guided by image style in Section 4.3 and physical world attack in Section 4.4.

In the rest of the paper, we use superscript \(i\) or \(t\) to denote _iteration of optimization_, and subscript \(i\) to denote the _timestep of diffusion model_.

### Diffusion-based Projected Gradient Descent

Given a clean image \(x\) with label \(y\) and a target classifier \(f_{}\) parameterized by \(\) to be attacked, our goal is to generate an adversarial sample \(x_{adv}\), which feels the model. The traditional gradient-based methods use the gradient of adversarial loss \(g=_{x}l(f_{}(x),y)\), where \(f_{}(x)\) return the logits. We also denote the loss as \(l_{adv}\) for simplicity to optimize \(x\) by iterations. The \(t\)-step update in the PGD with stepsize \(\) and \(n\) iterations reads

\[x^{t+1}=_{B_{}(x,)}[x^{t}+\, _{x^{t}}l(f_{}(x^{t}),y)]\] (4)

where \(_{B_{}(x,)}()\) is the projection operator on the \(_{}\) ball. Intuitively, the gradient will "guide" the sample \(x^{t}\) away from the decision boundary and deviate from the real distribution \(p(x_{0})\). This out-of-distribution property makes them easier to be purified using some distribution-based purification methods , which considerably restricts the strength of the attacks.

Instead of using \(l(f_{}(x),y)\) as the optimization cost, we turn to use logits of purified image \(f_{}(x_{0})\) as our optimization target. Here \(x_{0}\) can be obtained using SDEdit with \(K\) reverse steps. The update step becomes

\[x_{0}^{t}=(x^{t},K) x^{t+1}=_{B_{ }(x,)}[x^{t}+\,_{x^{t}}l(f_{}( x_{0}^{t}),y)].\] (5)

The parameter \(K\) can be used to control the edition-content preference : a large \(K\) value brings more editions and a small \(K\) value preserves more original content, and empirically,

Figure 1: **Comparison of Different Pipelines**: (a) Traditional gradient-based adversarial sample generation, \(x\) is the sample to be optimized, \(l_{adv}\) is adversarial loss. (b) Customized adversarial sample generation with natural style (determined by prompt \(p\)): joint optimization of adversarial loss with other surrogate losses like prompt loss \(l_{p}\) (e.g. style loss) and realistic loss \(l_{r}\) (e.g. content loss, smooth loss). (c) Our proposed diffusion-based framework, \(q\) is forward diffusion and \(R_{}\) is backward denoising, \(x_{0}\) is the denoised sample.

\(K=0.1 0.3T\) works well. Since we need to compute a new adversarial gradient \(g_{}=_{x}R_{}( R_{}(R_{}(x_{t},K),K-1),0)\) through back-propagation, \(K\) cannot be too large due to GPU memory constraint. Thus we turn to adopt some speed-up strategies like DDIM  to first sub-sample original \(T\) into \(T_{s}\), and then use scaled \(K_{s} K\) to do SDEdit (_e.g._, \(T=1000,K=100,T_{s}=40,K_{s}=4\)). Other fast sampling schemes  for DMs may accelerate our algorithms further.

After \(n\) iterations, Diff-PGD generates two outputs (\(x_{0}^{n}\) and \(x^{n}\)) that are slightly different from each other: 1) \(x_{0}^{n}\), according to the definition of the loss function, is realistic and adversarial. 2) \(x^{n}\) is an optimized perturbed image to which the adversarial gradient is directly applied. It follows the PGD \(_{}\) bound, and according to the loss function Equation (5), hard to be purified by SDEdit.

### Extended Diff-PGD into Region Attack

In some scenarios, we need to maintain parts of the image unchanged and only generate adversarial perturbations in defined regions. This is used in customized attacks, _e.g._, attack the masked region with a given natural style. Here we consider the sub-problem to attack the masked region (determined by mask \(M\)) of a given image with PGD, noted as region-PGD (rPGD). Specifically, the output adv-samples should satisfy: \((1-M) x_{adv}=(1-M) x\). Obviously, when \(M\) is valued all ones, the rPGD reduces to the original PGD.

For the previous gradient-based method, it is easy to modify \(_{x}f_{}(x)\) into \(_{x}f_{}(M x+(1-M) x_{c})\), where \(x_{c}=x\) is a copy of the original image with no gradient flowing by. It is not surprising the perturbed regions will show a large distribution shift from other regions, making the figure unnatural.

For Diff-PGD, instead of using the original SDEdit, we modify the reverse process using a replacement strategy [30; 41] to get a more generalized method: Diff-rPGD. Similarly, when \(M\) has valued all ones, it reduces to Diff-PGD. The pseudo-code for Diff-rPGD is shown in Alg. 1, where the replacement strategy can give a better estimation of intermediate samples during the reverse diffusion. This strategy can make the patches in the given region better align with the remaining part of the image in the reversed diffusion process.

### Customized Adversarial Sample Generation

As previously mentioned, Diff-PGD allows for the optimization of the adversarial loss while simultaneously preserving reality. This property helps us avoid optimizing multiple losses related to realism at the same time, as is used in previous methods [12; 17; 26]. Naturally, this property benefits the generation of highly customized adv-samples with prompts (e.g., mask, style reference, text), since we can separate the generation into two steps: 1) generate samples close to given prompts (from \(x\) to \(_{s}\)) and 2) perturb the samples to be adversarial and at the same time preserving the authenticity (from \(_{s}\) to \(x_{0}^{n}\)). Here we follow the task settings in natural-style adv-sample generation with style reference in : given the original image \(x\), mask \(M\), and style reference \(x_{s}\), we seek to generate adv-samples \(x_{adv}\) which shows the similar style with \(x_{s}\) and is adversarial to the target classifier \(f_{}\)The style distance, as defined in , is composed of the differences between the Gram matrix of multiple features along the deep neural network, where \(H_{s}\) are layers for feature extraction (_e.g._ Conv-layers), and \(G\) is used to calculate the Gram matrix of the intermediate features. The style loss between \(x\) and \(x_{s}\) as \(l_{s}(x,x_{s})\) is

\[l_{s}(x,x_{s})=_{h H_{s}}\|G(f_{h}(x))-G(f_{h}(x_{s}))\|_{2}^{2}\] (6)

AdvCAM  uses \(l=_{1}l_{s}+_{2}l_{adv}+_{3}l_{r}\) as the optimization loss where \(l_{r}\) includes losses like smooth loss and content loss, all designed to make the outputs more realistic. It is tricky to balance all these losses, especially the adversarial loss and the style loss. We emphasize that it is actually not necessary to train the adversarial loss \(l_{adv}\) together with \(l_{s}\). This is due to the fact that Diff-PGD can guarantee the natural style of the output adversarial samples. Thus, the optimization process can be streamlined by first optimizing the style loss, and subsequently running Diff-PGD/Diff-rPGD. The high-level pipeline can be formulated as follows:

\[x+p_{s}(x^{n},x_{0}^{n})=(_{s},K,f_{})\] (7)

The first equation serves as a first stage of optimization to acquire region styled \(_{s}\) given prompts \(p=(M,x_{s})\). This stage can be implemented with other methods (_e.g._ generative-model-based image editing) and we do not need to worry about \(f_{}\) at this stage. Then we use Diff-rPGD to generate the adversarial sample \(x_{adv}\) given \(_{s}\) and other necessary inputs. Through experiments, we find that there are two major advantages of this pipeline: 1) the generation is much more stable 2) for some cases where the style loss is weak to guide the style transfer (generate samples that do not make sense), we can adjust \(K\) so that the diffusion model can help to generate more reasonable samples.

### Physical World Attack with Diff-PGD

We can also apply Diff-PGD to physical world attacks. First, for physical-world attacks, we need to introduce a _Physical Adapter_ in order to make the attack more robust in physical scenarios. Second, instead of using \(_{}\)-PGD as a bounded attack (_e.g._\(=128/255\), which is proved to be ineffective in ), we turn to utilize the same idea of Diff-PGD, but apply it as Diff-Phys, a patch generator with guidance from a diffusion model. We define our new loss as:

\[l_{}(x)=l_{adv}(_{}[((x,K))])\] (8)

Here, the physics adapter is included by adopting a transformation set \(\), which is utilized to account for variations in image backgrounds, random translations, changes in lighting conditions, and random scaling. We can apply Diff-PGD to patch attacks

\[x^{*}=*{arg\,min}_{x}l_{}(x) x_{0}^{*}=(x^{*},K).\] (9)

The pipeline works as follows. We first gather background data and define the scope of translations and lighting conditions, to get \(\). Then we optimize our patch in simulation to get \(x_{0}^{*}\). Finally, we print \(x_{0}^{*}\) out, stick it to the target object, take photos of the object, and test it on the target classifier.

## 5 Experiments

The experiment section aims to answer the following questions: **(Q1)** Is Diff-PGD/Diff-rPGD effective to generate adv-samples with higher realism? **(Q2)** Can Diff-PGD be easily applied to generate better style-customized adv-samples? **(Q3)** Can Diff-PGD be applied to physical world attacks? **(Q4)** Do adversarial samples generated by Diff-PGD show better properties like transferability and anti-purification ability?

**Datasets, Models, and Baselines.** We use the validation dataset of ImageNet  as our dataset to get some statistical results for global attacks and regional attacks. For the style-guided adv-sample generation, we use some cases from  but also collect more images and masks by ourselves. We use VGG-19 as the backbone network to compute the style distance. The main target classifier to be attacked across this paper is ResNet-50 , and we also use ResNet-101, ResNet18, and WideResNet(WRN)  for the transferability test. Besides convolutional-based networks, we also try our Diff-PGD on Vision Transformers like ViT-b  and BEiT-I , and the results are put in Appendix E. The diffusion model we adopt is the unconditional diffusion model pre-trained on ImageNet  though we use DDIM  to respect the original timesteps for faster inference. More details about the experimental settings are included in the appendix.

We compare Diff-PGD with threat models such as PGD  (for digital attacks), AdvPatch  (for physical-world attacks), and AdvCam  (for customized attacks and physical-world attacks). All threat models generate attacks within the context of image classification tasks. More details are included in the supplementary materials.

**Diff-PGD For Digital Attacks (Q1).** We begin with the basic global \(_{}\) digital attacks, where we set \(_{}=16/255\) for PGD and Diff-PGD, and # of iterations \(n=10\) and step size \(=2/255\). For Diff-PGD, we use DDIM with timestep \(T_{s}=50\) (noted as DDIM\(50\) for simplicity), and \(K=3\) for the SDEdit module. Figure 2 shows that adv-samples generated by Diff-PGD (\(x_{0}^{n}\)) are more stealthy, while PGD-generated samples (\(x_{PGD}\)) have some patterns that can be easily detected by humans. This contrast is clearer on the perturbation map: \(_{PGD}\) contains more high-frequency noise that is weakly related to local patterns of the instance \(x\), and \(_{0}^{n}\) is smoother and highly locally dependent. Similarly, for regional adversarial attacks, Diff-rPGD can generate adv-samples that can better blend into the unchanged region, as demonstrated in Figure 3, showing higher stealthiness than rPGD.

We also conduct experiments to show the effectiveness of Diff-PGD regarding the Success Attack Rate. We uniformly sampled \(250\) images from the ImageNet validation set. From Figure 6 (a), we can see that although the gradient is restricted to generate realistic perturbations, Diff-PGD can still reach a high success rate with more than \(5\) iterations. Detailed settings can be found in the supplementary materials.

We also conduct experiments on different norms. We show additional results of the performance of Diff-PGD on \(_{2}\)-based attacks in Appendix F.

**Diff-PGD For Style-Customized Attacks (Q2).** For customized attacks, we focus on the task of generating adversarial samples using a mask \(M\) and a style reference image \(x_{s}\) provided by the user. We compare our approach, described in Section 4.3, with the AdvCam method. As previously mentioned, the main advantage of our pipeline is that we do not need to balance multiple losses, and can instead divide the task into two stages: generate a style-transferred image \(_{s}\) without considering

Figure 3: **Visualization of Adversarial Samples generated by Diff-rPGD**: Diff-rPGD can generate better regional attacks than PGD: the attacked region can better blend into the background. The attacked regions are defined using red bounding boxes, and \((+)\) means zoom-in.

Figure 2: **Visualization of Adversarial Samples generated by Diff-PGD**: adv-samples generated using PGD (\(x_{}\)) tend to be unnatural, while Diff-PGD (\(x_{0}^{n}\)) can preserve the authenticity of adv-samples. Here \(x\) is the original image, \(_{}=x-x_{}\) and \(_{0}^{n}=x-x_{0}^{n}\), and we scale up the \(\) value by five times for better observation. Zoom in on a computer screen for better visualization.

adversarial properties, and then generate a realistic perturbation using Diff-PGD to make the image adversarial to the classifier.

For Diff-PGD, we use DDIM10 with \(K=2\). Figure 4 demonstrates that our method consistently generates customized adversarial samples with higher stealthiness, while the adversarial samples generated by AdvCam exhibit digital artifacts that make them appear unnatural. Also, in some cases, the style reference cannot be easily transferred (_e.g._ only local styles are given in \(x_{s}\), or \(x\) and \(x_{s}\) are collected with different resolution), resulting in an unsatisfactory \(_{s}\), such as red/yellow car style in Figure 4. Diff-PGD can refine the samples with more details, thanks to the strong generation capacity of diffusion models.

**Diff-PGD For Physical-World Attacks (Q3).** In the physical-world attacks, we set the problem as: given an image patch (start point of optimization), and a target object (_e.g._ attack a backpack), we need to optimize the patch to fool the classifier when attached to the target object. We use an iPhone 8-Plus to take images from the real world and use an HP DesJet-2752 for color printing. In AdvPatch, only adversarial loss is used to optimize the patch; in AdvCAM, content loss and smooth loss are optimized together with adversarial loss; for Diff-Phys, we only use purified adversarial loss defined in Equation (8), and the SDEdit is set to be DDIM10 with \(K=2\) for better purification.

The results are shown in Figure 5, for the untargeted attack, where we use a green apple as a patch to attack a computer mouse. For targeted attack, we use an image of a cat to attack a backpack, with the target-label as "Yorkshire terrier". We can see that both AdvPatch and AdvCam generate patches with large noise, while Diff-Phys can generate smoother and more realistic patches that can successfully attack the given objects.

Figure 4: **Generating Adversarial Samples with Customized Style**: Given the original image \(x\), a style mask \(M\), and a style reference image \(x_{s}\), Diff-PGD can generate more realistic samples, even in cases where only local styles are given (_e.g._ only the door of the red car is offered as a \(x_{s}\)).

Figure 5: **Results of Physical-World Attacks**: We show two scenarios of physical world attacks: the first row includes untargeted attacks on a small object: computer mouse, and the second row includes targeted attacks on a larger object: backpack, where we set the target to be Yorkshire terrier. The sticks-photo pairs include clean patch (green box), AdvPatch(blue box), AdvCam generated patch (red box), and our Diff-Phys generated patch (black box).

**Exploring Other Properties of \(x_{0}^{n}\) and \(x^{n}\) (Q4).** Finally, we investigate additional properties of adversarial samples generated using Diff-PGD. Among the two samples \(x_{0}^{n}\) and \(x^{n}\), the former exhibits both adversarial characteristics and a sense of realism, while the latter is noisier but contains adversarial patterns that are more difficult for SDEdit to eliminate. Although our approach does not specifically target enhancing Transferability and Purification power, we demonstrate that Diff-PGD surpasses the original PGD in these two aspects.

**Transferability.** We test the adv-samples targeting ResNet-50 on four other classifiers: ResNet-101, ResNet-18, WRN-50 and WRN-101. From Figure 6(b) we can see that both \(x^{n}\) and \(x_{0}^{n}\) generated by Diff-PGD can be better transferred to attack other classifiers than PGD. This can be explained by the intuition that adv-attacks in semantic space can be better transferred. We also test the success rate attacking adversarially trained ResNet-50 in Figure 6(c) and we can see \(x^{n}\) is much better than other adversarial samples.

**Anti-Purification.** Following the purification pipeline in , we attach an off-the-shelf sample purifier to the original ResNet-50 and test the success rate of different adv-samples. In Table 2 we can see that the adv-samples \(x_{PGD}\) generated by PGD are easily purified using the enhanced ResNet-50. In contrast, our adv-samples, \(x^{n}\) and \(x_{0}^{n}\), show better results than \(x_{PGD}\) by a large margin. It can be explained by the type of perturbations: out-of-distribution perturbations used in PGD can be removed using diffusion-based purification, while in-distribution attacks in Diff-PGD are more robust to purification.

## 6 Accelerated Diff-PGD with Gradient Approximation

From Algorithm 1, we can see that we need to calculate the derivative of the model's output over the input, where the model is composed of chained U-Nets \(R_{}\) in the diffusion model followed by a target classifier \(f_{}\). Here we focus on the computational bottleneck, which is the derivative of \(K\)-step SDEdit outputs \(x_{0}\) over SDEdit input \(x\):

\[}{ x}=}{ x}(}{ x_{K}}}{ x_{K-1}}... }{ x_{0}}) c\] (10)

  
**Sample** & **(+P)ResNet50** & **(+P)ResNet101** & **(+P)ResNet18** & **(+P)WRN50** & **(+P)WRN101** \\  \(x_{}\) & 0.35 & 0.18 & 0.26 & 0.20 & 0.17 \\ \(x_{n}\) (Ours) & **0.88** & **0.38** & 0.36 & 0.32 & 0.28 \\ \(x_{n}^{0}\) (Ours) & 0.72 & 0.36 & **0.37** & **0.36** & **0.36** \\   

Table 2: **Anti-purification Results**: Adv-samples generated by PGD and Diff-PGD against adversarial purification: (+P) means the classifier is enhanced by an off-the-shelf adversarial purification module. Here we set \(=16/255,n=10\). The attacks are operated on ResNet-50.

Figure 6: This figure presents quantitative results on our approach, with all y-axis representing the success rate of attacks: (a) Successful rate of Diff-PGD vs PGD; (b) Results of transferability of Diff-PGD vs PGD, where \(=16/255\) and \(=2/255\). The adv-samples are generated with ResNet-50 (R50) and tested on ResNet-18 (R18), ResNet-101 (R101), WRN-50 (WR50), and WRN-101 (WR101); (c) Success rate of Diff-PGD vs PGD on adversarially trained networks. AT uses adversarial training strategy in  and AT* uses AT strategy in  on the ImageNet dataset.

We approximate the gradient with a constant \(c\). We notice that the approximation of this Jacobian has been used in many recent works [36; 49; 7]. We also tried this strategy in Diff-PGD, and we got the accelerated version:

\[^{i}))}{ x} c^{i }))}{ x_{0}^{n}}.\] (11)

From this we can see that, we only need to calculate the gradient over \(x_{0}^{n}\)! We only run the inference of SDEdit to get \(x_{0}^{n}\) without saving the gradient (lower GPU memory, faster), making it much cheaper. We found that, for the global attack tasks, the approximated gradient still shows a high success rate but saves \(50\%\) of time and \(75\%\) of VRAM. More detailed results about the visualization and success rate can be found in Appendix C.

## 7 Conclusions

In this paper, we propose a novel method to power the generation of adversarial samples with diffusion models. The proposed Diff-PGD method can improve the stealthiness of adv-samples. We further show that Diff-PGD can be easily plugged into global digital attacks, regional digital attacks, customized attacks, and physical-world attacks. We demonstrate through experiments that our methods outperformed the baselines and are effective and stable. The major limitation of our method is that the back-propagation is more expensive than traditional gradient-based methods; however, we believe that the strong prior knowledge of the diffusion model can push forward adversarial attacks & defenses. Finally, while the proposed adversarial attack method could be potentially used by malicious users, it can also enhance future efforts to develop robust defense mechanisms, thereby safeguarding the security of AI systems.