# Spectral Evolution and Invariance in Linear-width Neural Networks

Zhichao Wang

University of California San Diego

zhw036@ucsd.edu &Andrew Engel

Pacific Northwest National Laboratory

andrew.engel@pnnl.gov &Anand Sarwate

Rutgers, The State University of New Jersey

ads221@soe.rutgers.edu &Ioana Dumitriu

University of California San Diego

idumitriu@ucsd.edu &Tony Chiang

Pacific Northwest National Laboratory

University of Washington

tony.chiang@pnnl.gov

###### Abstract

We investigate the spectral properties of linear-width feed-forward neural networks, where the sample size is asymptotically proportional to network width. Empirically, we show that the spectra of weight in this high dimensional regime are invariant when trained by gradient descent for small constant learning rates; we provide a theoretical justification for this observation and prove the invariance of the bulk spectra for both conjugate and neural tangent kernels. We demonstrate similar characteristics when training with stochastic gradient descent with small learning rates. When the learning rate is large, we exhibit the emergence of an outlier whose corresponding eigenvector is aligned with the training data structure. We also show that after adaptive gradient training, where a lower test error and feature learning emerge, both weight and kernel matrices exhibit heavy tail behavior. Simple examples are provided to explain when heavy tails can have better generalizations. We exhibit different spectral properties such as invariant bulk, spike, and heavy-tailed distribution from a two-layer neural network using different training strategies, and then correlate them to the feature learning. Analogous phenomena also appear when we train conventional neural networks with real-world data. We conclude that monitoring the evolution of the spectra during training is an essential step toward understanding the training dynamics and feature learning.

## 1 Introduction

Deep learning theory has made insightful connections between the behavior of neural networks (NNs) and kernel machines through asymptotic analyses of the so-called kernel regime [65; 83; 48; 40; 12; 4; 84]. When the neural network (NN) is _infinitely wide_, the behavior of NN coincides with a kernel machine, and the training process, as well as the generalization performance of this ultra-wide NN, can be fully described. The performance of _finite-width_ NNs, however, does not correspond to this theory, as NNs optimized with gradient-based methods perform better than infinitely wide networks in many circumstances [47; 38; 34; 51; 25; 32; 76; 42]. This gap heavily relies on the task complexity, data distribution, architecture of the NN and the training strategy . We consider a more realisticsetting, a _linear-width regime_ (LWR), when the sample size \(n\), the input feature dimension \(d\), and the width \(h\) of the hidden layer approach infinity at comparable rates. Under the LWR, we aim to empirically study this theoretical gap in generalization and spectral properties by training various NNs with different optimization tools.

The ultra-wide NN (\(h n\), fixed \(d\)) stays close to the kernel machine induced by initial NN, throughout the gradient-based training processes [88; 28; 27; 11]. There are two kernels commonly studied in theory: the conjugate kernel (CK) and the neural tangent kernel (NTK). CK (or the equivalent Gaussian process kernel) is the Gram matrix of the last hidden layer, which represents training only the last layer of the network [48; 57; 61]; by contrast, the NTK is the Gram matrix of the Jacobian of the NN for all trainable parameters, which governs the gradient flow of NN [40; 28; 4]. In most theoretical results, these kernels remain fixed throughout training, which leads to a kernel gradient descent with the initial kernel [40; 16], whereas in practice the spectra of the weight matrix, CK, and NTK of the NN change while learning the features from the training data [58; 59; 30; 19; 68]. In this paper, under the LWR, we experimentally and theoretically explore the following question:

_How do the spectra of weight and kernel matrices of the NN evolve during the training process?_

This question is crucial to extend our understanding beyond the kernel regime and will help us analyze the generalization of the NN in instances when it performs better than the kernel machine. For this case, the spectral properties of the trained NN could be entirely different from the initial kernel [55; 10; 78]. Also, various spectral properties of weight and kernel matrices can reveal different features learned by different training procedures . Understanding the dynamics of the spectral properties may aid in finding better approaches to training and tuning hyper-parameters for NNs. From a theoretical perspective, random matrix theory (RMT) can be further exploited to study and elucidate the NN training under the proportional limit in high dimensions [45; 72; 57; 74; 37; 61].

Our main findings/contributions are as follows.

* We find a simple scenario that exhibits different spectral properties for both weight and kernel matrices through different training procedures. With the kernel regime as a benchmark, we compare how NN generalizes with different spectral evolutions of weight and kernel matrices in NN.
* The spectra of NNs trained with full batch gradient descent (GD) are globally _invariant_, indicating that the NN is still close to a kernel machine; we prove the global convergence of GD and the invariance of the limiting spectra for both weight and kernel matrices in this scenario.
* We observe a _phase transition_ of the alignment and the emergence of a spike outside the bulk of the spectrum when the learning rate _exceeds_ some threshold. The strong alignment of the spike with the teacher model when step sizes are large confirms that the NN is indeed learning germane features from data. This observation justifies the theoretical result of  in an ideal two-stage training process.
* The evolution towards heavy-tailed spectra is also discovered by using adaptive methods. Our experiments rule out a _causal_ relationship between the occurrence of a heavy-tailed spectrum for the weight matrices and a good generalization. This complements the work of [60; 63; 86] where the authors had observed a strong _correlation_ between the two; while at the same time, we provide simple examples of when heavy-tailed spectra exhibit feature learning and better generalizations.

For more details on how our results fit into existing literature, please see Appendix A.

## 2 Notation and Preliminaries

Throughout this paper, \(\|\|\) denotes the \(_{2}\) norm for vectors, \(_{2}_{2}\) is the operator norm for matrices, while \(\|\|_{F}\) is the Frobenius norm, and \(\) represents the Hadamard product between matrices. \(o_{d,}()\) represents little-o in probability as \(d\).

Neural Tangent Kernel Parameterization.Consider a \(L\)-layer fully connected feedforward NN at initialization without bias term: for \(1 L-1\),

\[_{0}=}{},\ ^{()}=}} (_{}^{(-1)}), \]

and \(f_{}()=^{}^{(L-1)},\) where the input vector is \(^{d}\), \(_{}^{n_{} n_{-1}}\) is the weight matrix for the \(\)-th layer, and \(:=[v_{1},,v_{h}]^{}^{n_{L-1}}\) is the last-layer weight. Let \(n_{0}=d\). Denote all trainable parameters by \(:=[(_{1}),,(_{L-1}),]^ {}^{p}\) where each parameter's initial value is independently sampled from some distribution and \(p\) is the total number of parameters. Let the training dataset be \((,):=([_{1},,_{n}],)^{d n } R^{1 n}\); the output of this NN with respect to this dataset is \(f_{}()=[f_{}(_{1}),,f_{}( _{n})]\). We call the above parameterization the _NTK parameterization_. The loss function for training is a mean squared error (MSE)

\[():=-f_{}( )^{2}. \]

We focus on the NTK parameterization and consider the kernel machine (6) induced by the initial NTK of the NN. We aim to seek the cases when the NN outperforms this kernel during the training process. For this purpose, we adopt different optimizers of training this NN to obtain different testing performances and spectral properties of trained weights and empirical kernels.

Training Processes of NNs.NNs are usually trained by gradient-based methods such as full-batch gradient descent (GD), mini-batch stochastic gradient descent (SGD), Adaptive Gradients (AdaGrad), and Adam . We can represent GD by

\[_{t+1}=_{t}-_{}(_{t}), \]

where \(\) is the learning rate and \(_{}(_{t})\) is the gradient of the training loss w.r.t. trainable parameters \(\) at step \(t 0\). We will prove the global convergence of GD in some special (overparameterized) cases ensuring the convergence to a NN that interpolates the data. We will also show the hyper-parameters (e.g. learning rate \(\)) affect the spectral properties of NNs during training.

Conjugate Kernel and Neural Tangent Kernel.1When \(L=2,\) let \(n_{1}=h\) and \(n_{0}=d\) be the widths of the output and input layer. The CK is defined as

\[^{}:=_{1}^{T}_{1}^{n n}, \]

where \(_{1}:=}/\). We can view the NN as a function of all training parameters \(\) and input data \(\). The neural tangent kernel (NTK) is related to the gradient of this neural network function with respect to \(\), which is the Gram matrix of the Jacobian of the neural network function with respect to \(\), \(^{}:=(_{}f_{}())^{}( _{}f_{}())\). Specifically, the empirical NTK of two-layer NN can be explicitly written2 as

\[^{}=^{}^{ }(})^{}()^ {2}^{}(})+^{}. \]

In this paper, we are interested in comparing the spectral distributions for these three matrices (weight, CK, and NTK) at initialization and the end of training.

Lazy Training.Lazy training  can be viewed as a linear approximation of the NN, i.e. \(f_{}() f_{_{0}}()+(-_{0})^{}_{}f_{_{0}}()\), defined by minimum-norm interpolation \(}:=*{arg\,min}\{-_{0}:(-_{0})^{}_{}f_{_{0}}()=-f_{_{0}}()\}.\) Then, lazy training also represents a kernel machine

\[()=f_{_{0}}()+(-f_{_{0}}())\,(,)^{-1}(,) \]where \(f()\) is the unregularized regression prediction on test data \(^{d}\), the kernel \((,)\) is the initial \(^{}\) on training data, and \((,)=(_{}f_{_{0}}())^{}( _{}f_{_{0}}())\). The asymptotic performance of \(()\) has been analyzed by  under the LWR. We view this regime as a _benchmark_: [21; 11] prove that NN through gradient flow is close to lazy training if \(h n\);  shows NN can go beyond lazy training under a non-proportional regime.

## 3 Case Study for Linear-width NNs

In this section, we investigate a two-layer NN with synthetic data. This setting is promising for future theoretical studies by virtue of RMT. We will showcase the evolution of its spectral properties over training. A two-layer NN in (1) is defined by

\[f_{}():=}_{i=1}^{h}v_{i}(_{i }^{}/). \]

At initialization, we assume that the first hidden-layer \(=[_{1},,_{h}]^{}^{h d}\) is composed of independent standard normal random vectors.

**Assumption 3.1** (Linear-width regime (LWR)).: Assume that \(_{1}\) and \(_{2}\) as \(n\) where the aspect ratios \(_{1},_{2}(0,)\) are two fixed constants.

LWR stands as a pivotal setting grounded in high-dimensional statistics [1; 61]. It offers valuable insights especially when addressing real-world datasets. This is in contrast to the infinite-width regime, in which we are already in the asymptotic limit for width at first. Hence, LWR is a better approximation of real-world datasets and practical neural networks compared with the infinite-width regime.

**Assumption 3.2** (Activation function).: Suppose that the activation function \((x)\) is nonlinear and \(_{}\)-Lipschitz with \(|^{}(x)|,|^{}(x)|_{}\) for all \(x\). Moreover, \([(z)]=0\) for \(z(0,1)\).

Though the LWR is somewhat impractical, it is still more aligned with deployed models than the infinite-width regime (\(h n\), fixed \(d\)). As a kernel machine, the infinite-width NN has been studied extensively [40; 28; 27; 84]. This infinite-width limit is special, however, as NNs may generally evolve beyond the kernel regime and achieve superior performance [31; 55; 10; 78].

**Assumption 3.3** (Synthetic dataset and teacher model).: Training data is \(:=[_{1},,_{n}]^{d n}\), where \(_{i}(,}_{d})\). The training labels \(=[y_{1},,y_{n}]\) are defined by \(y_{i}=f^{*}(_{i})+_{i},i[n]\), where \(f^{*}:^{d}\) is the teacher model, and \(_{i}\) is centered sub-Gaussian noise with variance \(_{}^{2}\).

One of the simplest nonlinear teacher models we can generate is the single-index model, namely \(f^{*}()=^{*}(^{})\) for a fixed vector \(\) with \(\|\|=1\) and nonlinear function \(^{*}\); the hidden feature is simply \(^{d}\). In general, we can consider a multiple-index model

\[f^{*}()=_{i=1}^{k}^{*}(^{}_{i}) \]

    & Optimization & Learning rate \(\) & \(R^{2}\) score & Test error & Spectra \\  Case 1 & GD & 5.0 & 0.63582 & 0.36381 & Invariant Bulk \\ Case 2 & SGD & 0.1 & 0.60605 & 0.36879 & Invariant Bulk \\ Case 3 & SGD & 22.0 & 0.76081 & 0.23791 & Bulk+spike \\ Case 4 & Adam & 0.092 & **0.78829** & **0.21071** & Heavy tail \\   & Lazy regime & & 0.68092 & 0.3185 & \\   

Table 1: Four models with the same architecture (\(n=2000\), \(h=1500\), \(d=1000\), and \(\) is normalized \(\)), but different choices of initial learning rates and optimizers listed in Table 1. The training label noise \(_{}=0.3\) and the teacher model is defined by (9) with \(^{*}\) a normalized _softplus_ and \(=0.2\). We observe that simply choosing an optimizer and learning rate can affect the shapes of the final spectra and the performance of the NN, as measured by \(R^{2}\) scores and test errors.

where \(_{i}\) are some orthogonal unit vectors. We will specifically consider a mixture of single-index and quadratic models as our teacher model in this section:

\[f^{*}()=^{*}(^{})+\|\|^{2}, \]

for some nonlinear target \(^{*}\), signal \(\) and constant \(\)3. Following the above assumptions and constructions, we show different spectral properties (Figure 1) for this two-layer NN using different training procedures (Table 1). Figure 1 exhibits three types of spectra after training: unchanged bulk distribution, bulk with one spike, and heavy tail in spectra. Putting things together, we can see close relationships between the spectra and the generalization of the NN. These different spectral properties actually reveal disparate features learned via different training strategies.

The advantage of this toy model is that we can easily extract the spectral behaviors over training and then compare them with the kernel machine. We use lazy training defined in (6) as our _benchmark_ to assist us in determining whether a NN outperforms the associated kernel machine. Table 1 compares the test errors and \(R^{2}\) scores for different optimization cases and the lazy training. By tuning the hyper-parameters, we can find specific situations where NN outperforms the lazy training (see also Figure 10(c) in Appendix B.2).

From Figures 1(a), 12 and 13 in Appendix B.2, one can observe the spectral distributions of the weight, CK and NTK matrices remain invariant and static during training in Cases \(1\&2\), which indicates both cases still belong to the lazy regime. This spectral invariance impedes further feature learning during the training process. The emergence of the outlier in Figures 1(b) and 14 of Appendix B.2, however, shows the improvement over lazy training and potential feature learning via the training process, where the spectra possibly inherit the structures in teacher models (see Section 4.2). Comparing with Case 2, Case 3 of Table 1 suggests the importance of the large learning rate regime for training NNs . As a remark, our spectral results of Case 3 are consistent with the observations in  through RMT hypothesis testing, where the majority of trained weight matrices remain random, and the learned feature may be contained in the largest singular value (outlier) and associated vector only. From Figures 1(c) and 16 in Appendix B.2, Case 4 further exhibits more spikes and heavy tails in the trained spectra, which thoroughly goes beyond the realm of initial kernel machine. Notably, this phenomenon is not unique to Adam since heavy tails also occur with AdaGrad in Figure 22 in Appendix B.6. Although all of these cases have the same identical initialization, different methods of optimization eventually lead to various training trajectories and evolutions of the spectra of the weight and kernel matrices. To acquire feature learning, Cases \(3\&4\) cause weights to deviate far from initialization. In the following Section 4, we prove the invariance of the bulk distributions and provide more refined analyses of spikes and heavy tails in terms of feature learning.

Figure 1: Different spectral behaviors in Table 1: (a) The initial and trained spectra of \(\) in Case 1. The spectrum is invariant based on the Q-Q subplot. (b) The initial and trained spectra of \(^{}\) in Case 3. There is an outlier (orange arrow) in the spectrum after training. (c) The initial and trained spectra of \(^{}\) in Case 4. We refer to Appendix B.2 for other spectra of weight, CK, and NTK matrices in Case 1-4, where analogous phenomena hold for other matrices.

Different Spectral Behaviors in NNs

We now further explore the spectral behaviors in different cases of Table 1 by clarifying how the spectra evolve through different training processes and how this evolution may affect the NN. Following Figure 1, we study the training processes case-by-case: invariant bulk, spikes outside the bulk, and heavy-tailed distribution. Additional experiments are exhibited in Appendix B.

### Invariant Bulk Distributions

In Figure 1(a) (also Figures 12 and 13 in Appendix B.2), we observe the bulk distributions of weight and kernel matrices in Cases \(1\&2\) remain globally unchanged (invariant) over the training process. under the LWR, this is also empirically verified by Figures 10(a)\(\&\)(b) in Appendix B.2. In this section, by investigating the global convergence of GD, we prove this invariant-bulk phenomenon under certain assumptions.

For simplicity, we focus on analyzing the training process of the first-hidden layer with the second layer \(\) fixed. Denote \(f_{}()\) by \(f_{}()\) in this case. At any time \(t\), consider the gradient steps:

\[_{t+1}=_{t}-_{}(_{t}). \]

Denote the CK and NTK at gradient step \(t\) by \(_{t}^{}:=(_{t})^{}( {W}_{t}),\) and \(_{t}^{}:=^{} ^{}(}_{t})^{}( _{t})^{2}^{}(}_{t})\) respectively. First, we present an elaborate description of the changes in the weight, CK, and NTK at the _early phase_ of the training (after any finite \(t\) steps) as follows.

**Lemma 4.1** (Early phase).: _Under Assumptions 3.1, 3.2 and 3.3, we further assume that \(_{} 1\) and \(f^{*}\) is a \(_{}\)-Lipschitz function. Given any fixed \(t\) and learning rate \(=(1)\), after \(t\) gradient steps, the changes \(}_{t}-_{0}_{F}\), \(_{t}^{}-_{0}^{}_{F}\), and \(_{t}^{}-_{0}^{}\) are all less than \(\), with probability at least \(1-4n(-cn)\), for some positive constants \(c,C>0\) which only depend on step \(t\) and parameters \(,_{1},_{2},_{},_{}\)._

Lemma 4.1 shows \(}_{t}-_{0}\), \(_{t}^{}-_{0}^{}\), and \(_{t}^{}-_{0}^{}\) are asymptotically vanishing for any fixed time \(t\). Therefore, all the eigenvalues/eigenvectors are asymptotically unchanged at the early phase of the training (see Corollary C.3 in Appendix C). Now we aim to analyze the spectra at the end of the training process (10). In this case, although we are unable to show the invariance for each eigenvalue, we can verify the invariance of the limiting bulk distributions for \(_{t}^{}\) and \(_{t}^{}\) for all \(t\).

By [81, Theorem 2.9], the smallest eigenvalue of \(_{0}^{}\) has an asymptotic lower bound:

\[_{}(_{0}^{})(a_{}-_{k=0}^{2} _{k}^{2})(1-o_{d,}(1)), \]

where \(a_{}:=[^{}()^{2}]\) and \(_{k}\) is the \(k\)-th Hermite coefficient of \(^{}\). Hence, we can claim there exists some constant \(>0\) only dependent on \(\) such that \(_{}(_{0}^{}) 4^{2}\) with high probability. Note that \(\) is not vanishing since \(\) is nonlinear. With this lower bound, we obtain the following global convergence for (10) and norm control of \(_{t}\) as \(n/d_{1}\) and \(h/d_{2}\).

**Theorem 4.2** (Global convergence).: _Under the same assumptions of Lemma 4.1, we further assume \(v_{i}\)'s are independent and centered random variables in the second layer. For any \(<\{n}{2},^{2}(1+ })^{2}}\}\) and all \(t\), there exists some \(^{*}>0\) such that, when \(_{2}^{*}\), the gradient steps (10) will satisfy_

\[(_{t})(1-}{2n})^{t} (_{0}), \] \[_{0}-_{t}_{F} +(_{t})(_{0}),\] (13) \[_{t=0}^{}_{t+1}-_{t} _{F}_{0})}{}, \]

[MISSING_PAGE_FAIL:7]

Transitions of the Spike as a Function of Learning Rate.From Case 2 to Case 3, we observe the emergence of outliers in the trained spectra when increasing the learning rate \(\). This indicates a transition of the emergence of the spike outside the bulk distribution. Figure 2, analogously to the well-known BBP transition by Baik, Ben Arous, and Peche in  from the RMT community, shows there is a threshold (yellow region) for learning rate: the outliers only appear when \(\) exceeds this threshold. We fix the same NN and dataset for all trials of training. The flat black lines in Figures 2(b) and (c) are the right edges of the limiting spectra at initialization. Figure 2(d) records the angles between \(\) and the leading eigenvector of \(_{t}^{}_{t}/d\), and \(\) and the leading eigenvectors of \(_{t}^{}\) and \(_{t}^{}\) after training for different \(\). Similarly with , when \(\) is sufficiently large (orange region), we obtain significant alignments which suggest potential feature learning. These transitions of leading eigenvalue and eigenvector alignment have been proved for \(_{t}\) by  for a different scenario4.

Spikes of Kernel Matrices.The alignment of the kernel matrix with the training labels \(\) is defined by  by Kernel Target Alignment (KTA) as follows: when kernel \(\) is either CK or NTK,

\[=,^{}}{\|\|_{F}\| \|^{2}}. \]

Analogously to , Figure 3(a) depicts the evolution of KTA of CK in several cases. Based on Figure 2(d), when the spike appears outside the bulk (Case 3), its corresponding (leading) eigenvector \(_{1}\) of kernel matrix naturally dominates the alignment with \(\) (Figure 15 in Appendix B.2), which is regarded as a kernel rotation during training in . Notice that this is not the common situation in Cases 1\(\&\)2 of Table 1 (and cf. Figure 11 in Appendix B.2). On the other hand, KTA measures the alignment between \(\) and the full eigenbasis of the kernel. These kernel alignments improve the speed of the convergence of training dynamics but may hurt or boost the generalization of the NNs . Figure 3(a) indicates that Case 4 with heavy-tailed spectra after training has a larger KTA than the other cases. In this case, the emergence of a heavy tail in the spectrum is closely related to a better generalization of the NN and more significant feature learning.

### Phenomenon of Heavy-tailed Spectra

Next, we analyze the heavy-tailed spectra of weight and kernel matrices in Figure 1(c).  found a strong correlation between the heavy-tailed spectra of trained state-of-the-art models with better generalization (Figure 7 in Appendix B.1). Heavy-tailed spectra can be viewed as an extreme of "bulk+spikes", where a fraction of the eigenvalues move out of the initial bulk. In RMT, heavy-tailed spectra generally appear when the entries of the matrix are highly correlated . This could heuristically explain heavy-tailed phenomena in the spectra since the entries of well-trained \(_{t}\) should be strongly correlated. Unlike , we focus on the heavy-tailed phenomena for both weight and kernel matrices in a simpler model (7) and provide a connection between feature learning and heavy-tailed spectra, which opens an important avenue for further theoretical analysis.

Figure 3: (a) Evolution of KTA of CK defined by (15) with respect to training labels for Cases 1, 3\(\&\)4 in Table 1. We normalize the epoch scales (\(x\)-axis) for better observations. Heavy-tailed phenomena: (b) Evolutions of PC angles \(_{i}\) between feature subspace \(U\) of (8) and top 100 eigenspace of \(_{t}^{}_{t}\) during training with Adam (solid line), SGD (dashed line) and GD (dash-dot). For the first PC \(_{1}\), see Figure 17 in Appendix B.4. (c) The CK spectra at two initializations for \(\): standard Gaussian and Cauchy distributions. (d) Weight spectra at initial and after SGD training. After training the weight reveals a heavy tail, but generalizes not as well as former examples (test loss \(1.47504\); \(R^{2}\) score \(-0.48\)).

Heavy Tails and Generalization.We emphasize that heavy tails are not sufficient for good generalization, in general, [60; 63]. Figures 3(c)&(d) exhibit NNs with heavy-tailed weights but in the absence of good performance at initialization. In fact, it is the alignments between the features learned from the heavy-tailed part and the features in the teacher model that finally determine the generalization error of NNs.

More precisely, we provide an example of when heavy tails indicate better generalizations. Consider the multiple-index teacher model (8) with \(k=5\) feature directions \(_{i}\), and train NNs (7) with GD, SGD, and Adam to get invariant bulk, bulk with one spike and heavy tails, respectively, after training. In Figure 3(b), we present the evolutions of the _principle angles_\(_{i}\) between feature subspace \(U=\{_{i}\}_{i=1}^{k}\) and top 100 eigenspace of \(_{t}^{}_{t}\) during different training processes. This eigenspace with respect to the top 100 eigenvalues of \(_{t}^{}_{t}\) corresponds to the heavy-tail part of the spectrum in \(_{t}^{}_{t}\) when training NNs with Adam (solid lines in Figure 3(b)). Comparing with GD and SGD training processes, we observe strong alignments between feature space \(U\) and eigenspace w.r.t heavy tails in Adam case in Figure 3(b), which explains why Adam case (NNs with heavy-tailed spectra) generalizes better than the other two cases. For more examples, see Figures 17 and 20 in Appendix B.4. This concludes that NNs with heavy-tailed spectra can generalize better only when the teacher features from data are aligned with the heavy-tailed part of spectra. If the feature dimension in the teacher model is high (i.e. the teacher model is more complicated and intrinsically high-dimensional), then we expect to get a heavy-tailed weight spectrum of well-trained NN where the heavy-tailed part learns all the features in the teacher modes. This example explains why we can use the heavy tails to discriminate well-trained and poorly-trained large models [60; 63; 86].

## 5 Discussions and Future Directions

We empirically investigated how the spectra of \(\), \(^{}\), and \(^{}\) evolve under the LWR for an idealized student-teacher setting. Our work implies that understanding the relationship between feature learning and training processes requires understanding the evolution of the spectra of both weight and kernel matrices. In particular, we show that different training processes affect the eigenstructure of weight and kernel matrices. Since evolution is sensitive to feature learning, we can link feature learning and different training dynamics by studying the spectra of these matrices.

While synthetic data is easier to analyze theoretically, we also investigate these spectral properties on real-world data and more complicated tasks in the following. In practice, people mainly focus on analyzing spectra of the weight matrices in fully connected layers; we choose to also focus on the spectral properties of general kernel matrices induced by the NNs, which contain abundant information [19; 55; 5; 78].

First, we show the spectra of \(^{}\) before and after training for binary classification on CIFAR-2 through small CNNs in Figure 4. Similarly with Case 1, Figure 4(a) (especially in the Q-Q subplot) manifests the invariant spectral distribution of NTK through GD training while SGD exhibits a heavier tail in NTK spectrum in Figure 4(b). This phenomenon is more evident when trained by Adam in Figure 4(c) with improved accuracy. Figure 4 suggests that our observations on synthetic data in Section 3 can be extended to real-world data and on more practical architectures. We note that there is a lack of the emergence of spikes after training because spikes already exist in the initial NTK spectrum for this complicated neural architecture on real-world datasets. Figure 4(a) also indicates

Figure 4: Different NTK spectra for a small-CNN model on CIFAR-2. The subplots are Q-Q plots for the comparison between initial and trained spectra. Test accuracies: (a) 79%, (b) 84%, (c) 86.4%.

that the spectral invariance of NTK through training will impede the feature learning and the NN does not generalize well in this training process.

We also investigate the spectral properties on the pre-trained model, BERT from , with fine-tuning on Sentiment140 dataset of tweets5 from . We fine-tune the BERT model for a binary classifier on Sentiment140 and capture the evolution of CK spectra, rather than the NTK due to the size of BERT, in Figure 5 (see also Figure 7 in Appendix B.1).

A heavy-tailed CK spectrum with several spikes already exists in this pre-trained model. Unlike Figure 4 (and cases in Table 1) where the first spike of NTK becomes larger than at random initialization after training, in Figure 5(a), the leading eigenvalue first decreases and then increases. Moreover, similarly to Figure 2(d), our Figure 5(b) shows that the alignment of the first eigenvector of the CK and training labels becomes more apparent through fine-tuning with the leading eigenvalue decrease. Heuristically, this process seems to unlearn the features in the pre-trained model and, remarkably, learn new features on the new dataset in only a few epochs of fine-tuning (see Figure 7in Appendix B.1). We believe that the evolutions of the kernel matrices and some spectral metrics are crucial for understanding feature learning through fine-tuning . A more comprehensive exploration of the evolutionary spectral properties of "foundation models" may help shed further light on these phenomena.

Limitations.Although LWR has garnered significant attention in recent years, e.g., , we recognize the limitations inherent in LWR. Our LWR is more realistic compared with infinite-width neural networks and is one of the ways to approximate finite but very large neural networks with very large datasets, but there are more sophisticated regimes for NNs. We leave this for future theoretical work. The NTK parameterization is another limitation of this work. We expect to apply our spectral analysis for other parameterizations of NNs with more real-world datasets. See the discussion at the beginning of Appendix B.