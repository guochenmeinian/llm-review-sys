# Approximating Nash Equilibria in Normal-Form Games via Unbiased Stochastic Optimization

**Anonymous Author(s)**

Affiliation

Address

email

###### Abstract

We propose the first, to our knowledge, loss function for approximate Nash equilibria of normal-form games that is amenable to unbiased Monte Carlo estimation. This construction allows us to deploy standard non-convex stochastic optimization techniques for approximating Nash equilibria, resulting in novel algorithms with provable guarantees. We complement our theoretical analysis with experiments demonstrating that stochastic gradient descent can outperform previous state-of-the-art approaches.

## 1 Introduction

Nash equilibrium famously encodes stable behavioral outcomes in multi-agent systems and is arguably the most influential solution concept in game theory. Formally speaking, if \(n\) players independently choose \(n\), possibly mixed, strategies (\(x_{i}\) for \(i[n]\)) and their joint strategy (\(=_{i}x_{i}\)) constitutes a _Nash equilibrium_, then no player has any incentive to unilaterally deviate from their strategy. This concept has sparked extensive research in various fields, ranging from economics  to machine learning , and has even inspired behavioral theory generalizations such as quantal response equilibria which allow for more realistic models of boundedly rational agents .

Unfortunately, when considering Nash equilibria beyond the special case of the \(2\)-player, zero-sum scenario, two significant challenges arise. First, it becomes unclear how a group of \(n\) independent players would collectively identify a Nash equilibrium when multiple equilibria are possible, giving rise to the _equilibrium selection_ problem . Secondly, even approximating a single Nash equilibrium is known to be computationally intractable and specifically PPAD-complete . Combining both problems together, e.g., testing for the existence of equilibria with welfare greater than some fixed threshold is NP-hard and it is in fact even hard to approximate (i.e., finding a Nash equilibrium with welfare greater than \(\) for any \(>0\), even when the best equilibrium has welfare \(1-\)) .

From a machine learning (ML) practitioner's perspective, however, such computational complexity results hardly give pause for thought as collectively we have become all too familiar with the unreasonable effectiveness of ML heuristics in circumventing such obstacles. Famously, non-convex optimization is NP-hard, even if the goal is to compute a local minimizer , however, stochastic gradient descent (and variants thereof) succeed in training models with billions of parameters .

Unfortunately, computational techniques for Nash equilibrium have so far not achieved anywhere near the same level of success. In contrast, most modern Nash equilibrium solvers for \(n\)-player, \(m\)-action, general-sum, normal-form games (NFGs) are practically restricted to a handful of players and/or actions per player except in special cases (e.g., symmetric  or mean-field games ). This is partially due to the fact that an NFG is represented by a tensor with an exponential \(nm^{n}\) entries; even _reading_ this description into memory can be computationally prohibitive. More to the point, anycomputational technique that presumes _exact_ computation of the _expectation_ of any function sampled according to \(\) similarly does not have any hope of scaling beyond small instances.

This inefficiency arguably lies at the core of the differential success between ML optimization and equilibrium computation. For example, numerous techniques exist that reduce the problem of Nash equilibrium computation to finding the minimum of the expectation of a random variable (see related work section). Unfortunately, unlike the source of randomness in ML applications where batch learning suffices to easily produce unbiased estimators, these techniques do not extend easily to game theory which incorporates non-linear functions such as maximum, best-response amongst others. This raises our motivating goal:

**Can we solve for Nash equilibria via unbiased stochastic optimization?**

**Our results.** Following in the successful steps of the interplay between ML and stochastic optimization, we reformulate the approximation of Nash equilibria in an NFG as a stochastic non-convex optimization problem admitting unbiased Monte-Carlo estimation. This enables the use of powerful solvers and advances in parallel computing to efficiently enumerate Nash equilibria for \(n\)-player, general-sum games. Furthermore, this re-casting allows practitioners to incorporate other desirable objectives into the problem such as "find an approximate Nash equilibrium with welfare above \(\)" or "find an approximate Nash equilibrium nearest the current observed joint strategy" resolving the equilibrium selection problem in effectively ad-hoc and application tailored manner. Concretely, we make the following contributions by producing:

* A loss function \(()\) 1) whose global minima coincide with interior Nash equilibria in normal form games, 2) admits unbiased Monte-Carlo estimation, and 3) is Lipschitz and bounded.
* A loss function \(^{}()\) 1) whose global minima coincide with logit equilibria (QREs) in normal form games, 2) admits unbiased Monte-Carlo estimation, and 3) is Lipschitz and bounded.
* An efficient randomized algorithm for approximating Nash equilibria in a novel class of games. The algorithm emerges by employing a recent \(\)-armed bandit approach to \(^{}()\) and connecting its stochastic optimization guarantees to approximate Nash guarantees. For large games, this enables approximating equilibria _faster_ than the game can even be read into memory.
* An empirical comparison of stochastic gradient descent against state-of-the-art baselines for approximating NEs in large games. In some games, vanilla SGD actually improves upon previous state-of-the-art; in others, SGD is slowed by saddle points, a familiar challenge in deep learning .

Overall, this perspective showcases a promising new route to approximating equilibria at scale in practice. We conclude the paper with discussion for future work.

## 2 Preliminaries

In an \(n\)-player, normal-form game, each player \(i\{1,,n\}\) has a strategy set \(_{i}=\{a_{i1},,a_{im_{i}}\}\) consisting of \(m_{i}\) pure strategies. These strategies can be naturally indexed, so we redefine \(_{i}=\{1,,m_{i}\}\) as an abuse of notation. Each player \(i\) also has a utility function, \(u_{i}:=_{i}_{i}\), (equivv. "payoff tensor") that maps joint actions to payoffs in the unit-interval. Note that equilibria are invariant to payoff shift and scale  so we are effectively assuming we know bounds on possible payoffs. We denote the average cardinality of the players' action sets by \(=_{k}m_{k}\) and maximum by \(m^{*}=_{k}m_{k}\). Player \(i\) may play a mixed strategy by sampling from a distribution over their pure strategies. Let player \(i\)'s mixed strategy be represented by a vector \(x_{i}^{m_{i}-1}\) where \(^{m_{i}-1}\) is the \((m_{i}-1)\)-dimensional probability simplex embedded in \(^{m_{i}}\). Each function \(u_{i}\) is then extended to this domain so that \(u_{i}()=_{}u_{i}()_{j}x_{ja_{j}}\) where \(=(x_{1},,x_{n})\) and \(a_{j}_{j}\) denotes player \(j\)'s component of the joint action \(\). For convenience, let \(x_{-i}\) denote all components of \(\) belonging to players other than player \(i\).

The joint strategy \(_{i}^{m_{i}-1}\) is a Nash equilibrium if and only if, for all \(i\{1,,n\}\), \(u_{i}(z_{i},x_{-i}) u_{i}()\) for all \(z_{i}^{m_{i}-1}\), i.e., no player has any incentive to unilaterally deviate from \(\). Nash is typically relaxed with \(\)-Nash, our focus: \(u_{i}(z_{i},x_{-i}) u_{i}()+\) for all \(z_{i}^{m_{i}-1}\).

As an abuse of notation, let the atomic action \(a_{i}=e_{i}\) also denote the \(m_{i}\)-dimensional "one-hot" vector with all zeros aside from a \(1\) at index \(a_{i}\); its use should be clear from the context. We also introduce \(^{i}_{x_{i}}\) as player \(i\)'s utility gradient. And for convenience, denote by \(H^{i}_{il}=_{x_{-il}}[u_{i}(a_{i},a_{l},x_{-il})]\) the bimatrix game approximation  between players \(i\) and \(l\) with all other players marginalized out; \(x_{-il}\) denotes all strategies belonging to players other than \(i\) and \(l\) and \(u_{i}(a_{i},a_{l},x_{-il})\) separates out \(l\)'s strategy \(x_{l}\) from the rest of the players \(x_{-i}\). Similarly, denote by \(T^{i}_{ilq}=_{x_{-ilq}}[u_{i}(a_{i},a_{l},a_{q},x_{-ilq})]\) the \(3\)-player tensor approximation to the game. Note player \(i\)'s utility can now be written succinctly as \(u_{i}(x_{i},x_{-i})=x_{i}^{}^{i}_{x_{i}}=x_{i}^{}H^{i}_{il}x_{l} =x_{i}T^{i}_{ilq}x_{l}x_{q}\) for any \(l,q\) where we use Einstein notation for tensor arithmetic. For convenience, define \((z)\) as the function that places a vector \(z\) on the diagonal of a square matrix, and \(:z^{d}^{d d d}\) as a 3-tensor of shape \((d,d,d)\) where \((z)_{iii}=z_{i}\). Following convention from differential geometry, let \(T_{v}\) be the tangent space of a manifold \(\) at \(v\). For the interior of the \(d\)-action simplex \(^{d-1}\), the tangent space is the same at every point, so we drop the \(v\) subscript, i.e., \(T^{d-1}\). We denote the projection of a vector \(z^{d}\) onto this tangent space as \(_{T^{d-1}}(z)=z-^{}z\). We drop \(d\) when the dimensionality is clear from the context. Finally, let \((S)\) denote a discrete uniform distribution over elements from set \(S\).

## 3 Related Work

Representing the problem of computing a Nash equilibrium as an optimization problem is not new. A variety of loss functions and pseudo-distance functions have been proposed. Most of them measure some function of how much each player can exploit the joint strategy by unilaterally deviating:

\[_{k}()}}{{=}}u_{k}( _{k},x_{-k})-u_{k}()_{k}*{arg\,max}_{z}u_{k}(z,x_{-k}).\] (1)

As argued in the introduction, we believe it is important to be able to subsample payoff tensors of normal-form games in order to scale to large instances. As Nash equilibria can consist of mixed strategies, it is advantageous to be able to sample from an equilibrium to estimate its exploitability \(\). However none of these losses is amenable to unbiased estimation under sampled play. Each of the functions currently explored in the literature is biased under sampled play either because 1) a random variable appears as the argument of a complex, nonlinear (non-polynomial) function or because 2) how to sample play is unclear. Exploitability, Nikaido-Isoda (NI)  (also known by NashConv  and ADI ), as well as fully-differentiable options (, p. 106, Eqn 4.31) introduce bias when a \(\) over payoffs is estimated using samples from \(\). Gradient-based NI  requires projecting the result of a gradient-ascent step onto the simplex; for the same reason as the \(\), this is prohibitive because it is a nonlinear operation which introduces bias. Lastly, unconstrained optimization approaches (, p. 106) that instead penalize deviation from the simplex lose the ability to sample from strategies when iterates are no longer proper distributions. Table 1 summarizes these complications.

## 4 Nash Equilibrium as Stochastic Optimization

We will now develop our proposed loss function which is amenable to unbiased estimation. Our key technical insight is to pay special attention to the geometry of the simplex. To our knowledge, prior works have failed to recognize the role of the tangent space \(T\). Proofs are in the appendix.

### Stationarity on the Simplex Interior

**Lemma 1**.: _Assuming player \(i\)'s utility, \(u_{i}(x_{i},x_{-i})\), is concave in its own strategy \(x_{i}\), a strategy in the interior of the simplex is a best response \(_{i}\) if and only if it has zero projected-gradient1 norm:_

 Loss & Function & Obstacle \\  Exploitability & \(_{k}_{k}()\) & \(\) of r.v. \\ Nikaido-Isoda (NI) & \(_{k}_{k}()\) & \(\) of r.v. \\ Fully-Diff. Exp & \(_{k}_{a_{k}_{k}}[(0,u_{k}(a_{k},x_{-i})-u_{k}()) ]^{2}\) & \(\) of r.v. \\ Gradient-based NI & NI w/ \(_{k}_{k}=_{}x_{k}+_{ x_{k}}u_{k}()\) & \(_{}\) of r.v. \\ Unconstrained & Loss + Simplex Deviation Penalty & sampling from \(x_{i}^{m_{k}}\) \\ 

Table 1: Previous loss functions for NFGs and their obstacles to unbiased estimation.

\[\!R_{i}int*{arg\,max}_{z}u_{i}(z, x_{-i})-u_{i}(x_{i},x_{-i})(\!R_{i} int) (||_{T}[^{i}_{\!R_{i}}]||=0).\] (2)

In NFGs, each player's utility is linear in \(x_{i}\), thereby satisfying the concavity condition of Lemma 1.

### Projected Gradient Norm as Loss

An equivalent description of a Nash equilibrium is a joint strategy \(\) where every player's strategy is a best response to the equilibrium (i.e., \(x_{i}=_{i}\) so that \(_{i}()=0\)). Lemma 1 states that any interior best response has zero projected-gradient norm, which inspires the following loss function

\[()=_{k}_{k}||_{T}(^{k}_{x_{k}})||^{2}\] (3)

where \(_{k}>0\) represent scalar weights, or equivalently, step sizes to be explained next.

**Proposition 1**.: _The loss \(\) is equivalent to NashConv, but where player \(k\)'s best response is approximated by a single step of projected-gradient ascent with step size \(_{k}\): \(\!R_{k}=x_{k}+_{k}_{T}(^{k}_{x _{k}})\)._

This connection was already pointed out in prior work for unconstrained problems [15; 35], but this result is the first for strategies constrained to the simplex.

### Connection to True Exploitability

In general, we can bound exploitability in terms of the projected-gradient norm as long as each player's utility is concave (this result extends beyond gradients to subgradients of non-smooth functions).

**Lemma 2**.: _The amount a player can gain by exploiting a joint strategy \(\) is upper bounded by a quantity proportional to the norm of the projected-gradient:_

\[_{k}()||_{T}(^{k}_{x_{k}})||.\] (4)

This bound is not tight on the boundary of the simplex, which can be seen clearly by considering \(x_{k}\) to be part of a pure strategy equilibrium. In that case, this analysis assumes \(x_{k}\) can be improved upon by a projected-gradient ascent step (via the equivalence pointed out in Proposition 1). However, that is false because the probability of a pure strategy cannot be increased beyond \(1\). We mention this to provide further intuition for why \(()\) is only valid for interior equilibria.

Note that \(||_{T}(^{k}_{x_{k}})||||^{k}_{x_{k}}||\) because \(_{T}\) is a projection. Therefore, this improves the naive bounds on exploitability and distance to best responses given using the "raw" gradient \(^{k}_{x_{k}}\).

**Lemma 3**.: _The exploitability of a joint strategy \(\), is upper bounded by a function of \(()\):_

\[_{k}}}()} }}}{{=}}f().\] (5)

### Unbiased Estimation

As discussed in Section 3, a primary obstacle to unbiased estimation of \(()\) is the presence of complex, nonlinear functions of random variables, with the projection of a point onto the simplex being one such example (see \(_{}\) in Table 1). However, \(_{T}\), _the projection onto the tangent space of the simplex, is linear_! This is the key that allows us to design an unbiased estimator (Lemma 5).

Our proposed loss requires computing the squared norm of the _expected value_ of the gradient under the players' mixed strategies, i.e., the \(l\)-th entry of player \(k\)'s gradient equals \(^{k}_{x_{kl}}=_{a_{-k} x_{-k}}u_{k}(a_{kl},a_{-k})\). By analogy, consider a random variable \(Y\). In general, \([Y]^{2}[Y^{2}]\). This means that we cannot just sample projected-gradients and then compute their average norm to estimate our loss. However, consider taking two independent samples from two corresponding identically distributed, independent random variables \(Y^{(1)}\) and \(Y^{(2)}\). Then\([Y^{(1)}Y^{(2)}]\) by properties of expected value over products of independent random variables. This is a common technique to construct unbiased estimates of expectations over polynomial functions of random variables. Proceeding in this way, define \(_{x_{k}}^{k(1)}\) as a random variable distributed according to the distribution induced by all other players' mixed strategies (\(j k\)). Let \(_{x_{k}}^{k(2)}\) be independent and distributed identically to \(_{x_{k}}^{k(1)}\). Then

\[()=[_{k}_{k}(_{x_{ k}}^{k(1)}-}(^{}_{x_{k}}^{k(1)})}_ {}^{}(_{x_{k}}^{k(2)}-}{m_{k}}(^{}_{x_{k}}^{k(2)})}_ {})]\] (6)

where \(_{x_{k}}^{k(p)}\) is an unbiased estimator of player \(k\)'s gradient. This unbiased estimator can be constructed in several ways. The most expensive, an exact estimator, is constructed by marginalizing player \(k\)'s payoff tensor over all other players' strategies. However, a cheaper estimate can be obtained at the expense of higher variance by approximating this marginalization with a Monte Carlo estimate of the expectation. Specifically, if we sample a single action for each of the remaining players, we can construct an unbiased estimate of player \(k\)'s gradient by considering the payoff of each of its actions against the sampled background strategy. Lastly, we can consider constructing a Monte Carlo estimate of player \(k\)'s gradient by sampling only a single action from player \(k\) to represent their entire gradient. Each of these approaches is outlined in Table 2 along with the query complexity  of computing the estimator and bounds on the values it can take (derived via Lemma 19).

We can extend Lemma 3 to one that holds under \(T\) samples with probability \(1-\) by applying, for example, a Hoeffding bound: \( f}()+( (1/)})\).

### Interior Equilibria

We discussed earlier that \(()\) captures interior equilibria. But some games may only have _pure_ equilibria. We show how to circumvent this shortcoming by considering quantal response equilibria (QREs), specifically, logit equilibria. By adding an entropy bonus to each player's utility, we can

* guarantee **all** equilibria are interior,
* still obtain unbiased estimates of our loss,
* maintain an upper bound on the exploitability \(\) of any approximate equilibrium in the original game (i.e., the game without an entropy bonus).

Define \(u_{k}^{}()=u_{k}()+ S(x_{k})\) where the Shannon entropy \(S(x_{k})=-_{l}x_{kl}(x_{kl})\) is a \(1\)-strongly concave function with respect to the \(1\)-norm . Also define \(^{}()\) as before except where \(_{x_{k}}^{k}\) is replaced with \(_{x_{k}}^{k}=_{x_{k}}u_{k}^{}()\), i.e., the gradient of player \(k\)'s utility _with_ the entropy bonus.

It is well known that Nash equilibria of entropy-regularized games satisfy the conditions for logit equilibria , which are solutions to the fixed point equation \(x_{k}=(}^{k}}{})\). The appearance of the softmax makes clear that all probabilities have positive mass at positive temperature.

Recall that in order to construct an unbiased estimate of our loss, we simply needed to construct unbiased estimates of player gradients. The introduction of the entropy term to player \(k\)'s utility is special in that it depends entirely on known quantities, i.e., the player's own mixed strategy. We can directly and deterministically compute \(}=-((x_{k})+)\) and add this to our estimator of \(_{x_{k}}^{k(p)}\). \(_{x_{k}}^{k(p)}=_{x_{k}}^{k(p)}+}\). Consider our refined loss function with changes in blue:

  & Exact & Sample Others & Sample All \\  Estimator of \(_{x_{k}}^{k(p)}\) & \(u_{k}(a_{kl},x_{-k})\) & \(u_{k}(a_{kl},a_{-k} x_{-k})\) & \(m_{k}u_{k}(a_{kl}(_{k}),a_{-k} x_{-k})e_{l}\) \\ \(_{x_{k}}^{k(p)}\) Bounds & \(\) & \(\) & \([0,m_{k}]\) \\ \(_{x_{k}}^{k(p)}\) Query Cost & \(_{i=1}^{n}m_{i}\) & \(m_{k}\) & \(1\) \\ \(\) Bounds & \(_{k}_{k}m_{k}\) & \(_{k}_{k}m_{k}\) & \(_{k}_{k}m_{k}^{3}\) \\ \(\) Query Cost & \(n_{i=1}^{n}m_{i}\) & \(2n\) & \(2n\) \\ 

Table 2: Examples and Properties of Unbiased Estimators of Loss and Player Gradients (\(_{x_{k}}^{k(p)}\)).

\[^{}()=_{k}_{k}||_{T}(_{x_{k}}^{k}) ||^{2}.\] (7)

As mentioned above, the utilities with entropy bonuses are still concave, therefore, a similar bound to Lemma 2 applies. We use this to prove the QRE counterpart to Lemma 3 where \(_{QRE}\) is the exploitability of an approximate equilibrium in a game with entropy bonuses.

**Lemma 4**.: _The entropy regularized exploitability, \(_{QRE}\), of a joint strategy \(\), is upper bounded as:_

\[_{QRE}_{k}}}^{} ()}}}}{{=}}f(^{}).\] (8)

Lastly, we establish a connection between quantal response equilibria and Nash equilibria that allows us to approximate Nash equilibria in the original game via minimizing our modified loss \(^{}()\).

**Lemma 14** (\(^{}\) Scores Nash Equilibria).: _Let \(^{}()\) be our proposed entropy regularized loss function with payoffs bounded in \(\) and \(\) be an approximate QRE. Then it holds that_

\[ n(W(}{{e}})+-2}{e})+2m_{k}}{_{k}_{k}}}^{}()}\] (9)

_where \(W\) is the Lambert function: \(W(}{{e}})=W((-1)) 0.278\)._

This upper bound is plotted as a heatmap for familiar games in Figure 1. Notice how pure equilibria are not visible as minima for zero temperature, but appear for slightly warmer temperatures.

## 5 Analysis

In the preceding section we established a loss function that upper bounds the exploitability of an approximate equilibrium. In addition, the zeros of this loss function have a one-to-one correspondence with quantal response equilibria (which approximate Nash equilibria at low temperature).

Here, we derive properties that suggest it is "easy" to optimize. While this function is generally non-convex and may suffer from a proliferation of saddle points and local maxima (Figure 2), it is Lipschitz continuous (over a subset of the interior) and bounded. These are two commonly made assumptions in the literature on non-convex optimization, which we leverage in Section 6. In addition, we can derive its gradient, its Hessian, and characterize its behavior around global minima.

Figure 1: Upper Bound (\( f(^{})\)) Heatmap Visualization. The first row examines the loss landscape for the classic anti-coordination game of Chicken (Nash equilibria: \((0,1),(1,0),(}{{3}},}{{3}})\)) while the second row examines the Prisoner’s dilemma (Unique Nash equilibrium: \((0,0)\)). Temperature increases for each plot moving to the right. For high temperatures, interior (fully-mixed) strategies are incentivized while for lower temperatures, nearly pure strategies can achieve minimum exploitability. For zero temperature, pure strategy equilibria (e.g., defect-defect) are not captured by the loss as illustrated by the bottom-left Prisoner’s Dilemma plot with a constant loss surface.

**Lemma 15**.: _The gradient of \(^{}()\) with respect to player \(l\)'s strategy \(x_{l}\) is_

\[_{x_{l}}^{}()=2_{k}_{k}B_{kl}^{}_{T }(_{x_{k}}^{k})\] (10)

_where \(B_{ll}=-[I-}^{}](})\) and \(B_{kl}=[I-}^{}]H_{kl}^{k}\) for \(k l\)._

**Lemma 17**.: _The Hessian of \(^{}()\) can be written_

\[(^{})=2^{}+T_{T }(^{})\] (11)

_where \(_{kl}=}B_{kl}\). \(_{T}(^{})=[_{1}_{T}(_{x_{1}}^{1 }),,_{n}_{T}(_{x_{n}}^{n})]\), and we augment \(T\) (the \(3\)-player approximation to the game, \(T_{lqk}^{k}\)) so that \(T_{lll}^{l}=(^{2}})\)._

At an equilibrium, the latter term disappears because \(_{T}(_{x_{k}}^{k})=\) for all \(k\) (Lemma 1). If \(\) was \(^{n}\), then we could simply check if \(\) is full-rank to determine if \(Hess 0\). However, \(\) is a simplex product, and we only care about curvature in directions toward which we can update our equilibrium. Toward that end, define \(M\) to be the \(n(+1) n\) matrix that stacks \(\) on top of a repeated identity matrix that encodes orthogonality to the simplex:

\[M()=-}_{T}(})& }_{T}(H_{12}^{1})&&}_{T}( H_{1n}^{1})\\ &&&\\ }_{T}(H_{n1}^{n})&&}_{T}( H_{n,n-1}^{n})&-}_{T}(})\\ _{1}^{}&0&&0\\ &&&\\ 0&&0&_{n}^{}\] (12)

where \(_{T}(z^{a b})=[I_{a}-_{a} _{a}^{}]z\) subtracts the mean from each column of \(z\) and \(}\) is shorthand for \((})\). If \(M(x)z=\) for a nonzero vector \(z^{n}\), this implies there exists a \(z\) that 1) is orthogonal to the ones vectors of each simplex (i.e., is a valid equilibrium update direction) and 2) achieves zero curvature in the direction \(z\), i.e., \(z^{}(^{})z=z^{}(Hess)z=0\), and so \(Hess\) is not positive definite. Conversely, if \(M()\) is of rank \(n\) for a quantal response equilibrium \(\), then the Hessian of \(^{}\) at \(\) in the tangent space of the simplex product (\(=_{i}_{i}\)) is positive definite. In this case, we call \(\)_well_-isolated because it implies it is not connected to any other equilibria.

By analyzing the rank of \(M\), we can confirm that many classical matrix games including Rock-Paper-Scissors, Chicken, Matching Pennies, and Shapley's game all induce strongly convex \(^{}\)'s at zero temperature (i.e., they have unique mixed Nash equilibria). In contrast, a game like Prisoner's Dilemma has a unique pure strategy that will not be captured by our loss at zero temperature.

Figure 2: We reapply the analysis of , originally designed to understand the success of SGD in deep learning, to “slices” of several popular extensive form games. To construct a slice (or _meta-game_), we randomly sample \(6\) deterministic policies and then consider the corresponding \(n\)-player, \(6\)-action normal-form game at \(=0.1\) (with payoffs normalized to \(\)). The index of a critical point \(_{c}\) (\(_{}^{}(_{c})=\)) indicates the fraction of negative eigenvalues in the Hessian of \(^{}\) at \(_{c}\); \(=0\) indicates a local minimum, \(1\) a maximum, else a saddle point. We see a positive correlation between exploitability and \(\) indicating a lower prevalence of local minima at high exploitability.

## 6 Algorithms

We have formally transformed the approximation of Nash equilibria in NFGs into a **stochastic** optimization problem. To our knowledge, this is the first such formulation that allows one-shot unbiased Monte-Carlo estimation which is critical to introduce the use of powerful algorithms capable of solving high dimensional optimization problems. We explore two off-the-shelf approaches.

Stochastic gradient descent is the workhorse of high-dimensional stochastic optimization. It comes with guaranteed convergence to stationary points , however, it may converge to local, rather than global minima. It also enjoys implicit gradient regularization , seeking "flat" minima and performs approximate Bayesian inference . Despite the lack of global convergence guarantee, in the next section, we find it performs well empirically in games previously examined by the literature.

We explore one other algorithmic approach to non-convex optimization based on minimizing regret, which enjoys finite time convergence rates. \(\)-armed bandits  systematically explore the space of solutions by refining a mesh over the joint strategy space, trading off exploration versus exploitation of promising regions.2 Several approaches exist [5; 37] with open source implementations (e.g., ).

### High Probability, Polynomial Convergence Rates

We use a recent \(\)-armed bandit approach called BLiN  to establish a high probability \(}(T^{-1/4})\) convergence rate to Nash equilibria in \(n\)-player, general-sum games under mild assumptions. The quality of this approximation improves as \( 0\), at the same time increasing the constant on the convergence rate via the Lipschitz constant \(}\) defined below. For clarity, we assume users provide a temperature in the form \(=\) with \(p(0,1)\) which ensures all equilibria have probability mass greater than \(}\) for all actions (Lemma 9). Lower \(p\) corresponds with lower temperature.

The following convergence rate depends on bounds on the exploitability in terms of the loss (Lemma 14), bounds on the magnitude of estimates of the loss (Lemma 8), Lipschitz bounds on the infinity norm of the gradient (Corollary 2), and the number of distinct strategies (\(n=_{k}m_{k}\)).

**Theorem 1** (BLiN PAC Rate).: _Assume \(_{k}==2/\), \(=\), and a previously pulled arm is returned uniformly at random (i.e., \(t U([T])\)). Then for any \(w>0\)_

\[_{t} wW(1/e)+-2}{e} +4(1+(4c^{2})^{1/3})}^{ +2)}}\] (13)

_with probability \((1-w^{-1})(1-2T^{-2})\) where \(W\) is the Lambert function (\(W(1/e) 0.278\)), \(m^{*}=_{k}m_{k}\), \(c}{}()}{(1/p)}+2 )^{2}()}{(1/p)}+2)\) upper bounds the range of stochastic estimates of \(^{}\) (see Lemma 8), and \(=()}{(1/p)}+2)}{p(1/ p)}+n\) (see Corollary 2)._

This result depends on the _near-optimality_ or _zooming_-dimension \(d_{z}=n(-_{bl}}{_{lo}_{hi}})[0,)\) (Theorem 2) where \(_{lo}\) and \(_{hi}\) denote the degree of the polynomials that lower and upper bound the function \(^{} s\) locally around an equilibrium. For example, in the case where the Hessian is positive definite, \(_{lo}=_{hi}=2\) and \(d_{z}=0\). Here, \(s:^{n(-1)}_{i}^{m_{i}-1}\) is any function that maps from the unit hypercube to a product of simplices; we analyze two such maps in the appendix.

Figure 3: Comparison of SGD on \(^{=0}\) against baselines on four games evaluated in . From left to right: \(2\)-player, \(3\)-action, nonsymmetric; \(6\)-player, \(5\)-action, nonsymmetric; \(4\)-player, \(66\)-action, symmetric; \(3\)-player, \(286\)-action, symmetric. SGD struggles at saddle points in Blotto.

Note that Theorem 1 implies that for games whose corresponding \(^{}\) has zooming dimension \(d_{z}=0\), NEs can be approximated with high probability in polynomial time. This general property is difficult to translate concisely into game theory parlance. For this reason, we present the following more interpretable corollary which applies to a more restricted class of games.

**Corollary 1**.: _Consider the class of NFGs with at least one QRE(\(\)) whose local polymatrix approximation indicates it is isolated (i.e., \(M\) from equation (12) is rank-\(n\) implies Hess \( 0\) implies \(d_{z}=n()=0\)). Then by Theorem 1, BLiN is a fully polynomial-time randomized approximation scheme (FPRAS) for QREs and is a PRAS for NEs of games in this class._

To convey the impact of stochastic optimization guarantees more concretely, assume we are given that an interior well-isolated NE exists. Then for a \(20\)-player, \(50\)-action game, it is \(1000\) cheaper to compute a \(}{{100}}\)-NE with probability 95% than it is to just list the \(nm^{n}\) payoffs that define the game.

### Empirical Evaluation

Figure 3 shows SGD is competitive with scalable techniques to approximating NEs. Shapley's game induces a strongly convex \(\) (see Section 5) leading to SGD's strong performance. Blotto shows signs of convergence to low, but nonzero \(\), demonstrating the challenges of local minima.

We demonstrate BLiN (applied to \(^{}\)) on a \(7\)-player, symmetric, \(2\)-action game. Figure 4 shows the bandit algorithm discovers two equilibria, settling on one near \(=[0.7,0.3] 7\) with a wider basin of attraction (and higher welfare). In theory, BLiN can enumerate all NEs as \(T\).

## 7 Conclusion

In this work, we proposed a stochastic loss for approximate Nash equilibria in normal-form games. An unbiased loss estimator of Nash equilibria is the "key" to the stochastic optimization "door" which holds a wealth of research innovations uncovered over several decades. Thus, it allows the development of new algorithmic techniques for computing equilibria. We consider bandit and vanilla SGD methods in this work, but theses are only two of the many options now at our disposal (e.g, adaptive methods , Gaussian processes , evolutionary algorithms , etc.). Such approaches as well as generalizations of these techniques to imperfect-information games are promising directions for future work. Similarly to how deep learning research first balked at and then marched on to train neural networks via NP-hard non-convex optimization, we hope computational game theory can march ahead to make useful equilibrium predictions of large multiplayer systems.

Figure 4: Bandit-based (BLiN) Nash solver applied to an artificial \(7\)-player, symmetric, \(2\)-action game. We search for a symmetric equilibrium, which is represented succinctly as the probability of selecting action \(1\). The plot shows the true exploitability \(\) of all symmetric strategies in black and indicates there exist potentially \(5\) NEs (the dips in the curve). Upper bounds on our unregularized loss \(\) capture \(4\) of these equilibria, missing only the pure NE on the right. By considering our regularized loss, \(^{}\), we are able to capture this pure NE (see zoomed inset). The bandit algorithm selects strategies to evaluate, using \(10\) Monte-Carlo samples for each evaluation (arm pull) of \(^{}\). These samples are displayed as vertical bars above with the height of the vertical bar representing additional arm pulls. The best arms throughout search are denoted by green circles (darker indicates later in the search). The boxed numbers near equilibria display the welfare of the strategy.