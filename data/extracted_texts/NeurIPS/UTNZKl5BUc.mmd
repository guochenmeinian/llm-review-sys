# Gradual Domain Adaptation via

Manifold-Constrained Distributionally Robust Optimization

 Amirhossein Saberi\({}^{*,@sectionsign}\)

Amir Najafi\({}^{}\)

Amin Behjati\({}^{,}\)

Ala Emrani\({}^{,}\)

Yasaman Zolfi\({}^{}\)

Mahdi Shadrooy\({}^{}\)

Abolfazl Motahari\({}^{}\)

Babak H. Khalaj\({}^{*,@sectionsign}\)

\({}^{*}\) Department of Electrical Engineering,

\({}^{}\) Department of Computer Engineering,

\(@sectionsign\) Sharif Center for Information Systems and Data Science,

Sharif University of Technology, Tehran, Iran

Emails: sah.saberi@ee.sharif.edu, {amir.najafi,motahari,khalaj}@sharif.edu. (\(\)) Authors with equal contribution.

###### Abstract

The aim of this paper is to address the challenge of gradual domain adaptation within a class of manifold-constrained data distributions. In particular, we consider a sequence of \(T 2\) data distributions \(P_{1},,P_{T}\) undergoing a gradual shift, where each pair of consecutive measures \(P_{i},P_{i+1}\) are close to each other in Wasserstein distance. We have a supervised dataset of size \(n\) sampled from \(P_{0}\), while for the subsequent distributions in the sequence, only unlabeled i.i.d. samples are available. Moreover, we assume that all distributions exhibit a known favorable attribute, such as (but not limited to) having intra-class soft/hard margins. In this context, we propose a methodology rooted in Distributionally Robust Optimization (DRO) with an adaptive Wasserstein radius. We theoretically show that this method guarantees the classification error across all \(P_{i}\)s can be suitably bounded. Our bounds rely on a newly introduced _compatibility_ measure, which fully characterizes the error propagation dynamics along the sequence. Specifically, for inadequately constrained distributions, the error can exponentially escalate as we progress through the gradual shifts. Conversely, for appropriately constrained distributions, the error can be demonstrated to be linear or even entirely eradicated. We have substantiated our theoretical findings through several experimental results.

## 1 Introduction

Gradual domain adaptation addresses a critical challenge in machine learning: the high cost and impracticality of continually preparing labeled datasets for training ML models. Once an initial labeled dataset is obtained through costly labor, machine learning models can use it to automatically label future unlabeled datasets--a procedure called self-training. However, as these future datasets experience gradual domain shifts from the original one, the initial dataset may become less effective, necessitating renewed human effort. Gradual domain adaptation has been proposed to mitigate this issue by learning a model on the initial dataset and then gradually adapting it to the future unlabeled data in a sequential manner. Formally, we consider a sequence of datasets modeled via empirical measures \(_{0},,_{T}\), where \(_{0}\) represents the initial labeled dataset and the remaining \(_{i}\) are unlabeled. Here, \(T\) denotes the length of the sequence, and each \(_{i}\) is an empirical estimate of an unknown distribution \(P_{i}\) based on \(n_{i}\) i.i.d. samples. We assume that consecutive measures \(P_{i}\) and\(P_{i+1}\) are within a bounded Wasserstein distance from each other to make the problem theoretically approachable.

Recent research in this field has proposed various methods, each with distinct advantages and disadvantages. Theoretical advancements aim to bound the generalization error, provide robustness certificates as the model adapts to successive datasets, and, importantly, quantify the error propagation dynamics along the sequence. Naive approaches often lead to exponentially increasing errors with respect to \(T\) for the model performance on the most recent dataset. For some problem families, this exponential increase is conjectured to be inevitable. However, for appropriately restricted problem sets, such as linear classifiers and distributions with hard/soft margins, novel methodologies can control error propagation . To date, no work has fully established cases where error propagation remains fixed or increases sublinearly, and a comprehensive theoretical characterization of problems in this context is still lacking.

We aim to address these challenges with a novel approach leveraging distributionally robust optimization (DRO) for gradual domain adaptation. Our core idea is based on the limited knowledge that the unknown labeled version of distribution \(P_{i+1}\), or empirically, the unlabeled measure \(_{i}\) together with its latent labels, is within a bounded proximity of \(P_{i}\) in a distributional sense. Using DRO on \(P_{i}\) (or its empirical version \(_{i}\)) with a carefully chosen and adaptive adversarial radius, we provide theoretical guarantees on \(P_{i+1}\). Furthermore, when distributions exhibit favorable properties--such as lying on a manifold of margin-based measures--we demonstrate that certified bounds on generalization across domains can be established. In order to do so, we introduce a new complexity measure, the "compatibility function," which depends on the classifier hypothesis set \(\), the properties of the manifold for \(P_{i}\)s, and the Wasserstein distance between consecutive distributions \(P_{i}\) and \(P_{i+1}\). This measure effectively bounds error propagation and identifies scenarios where errors remain bounded. Our analysis also extends to non-asymptotic cases where only empirical estimates of the distributions are available, showing that error terms decrease with \([_{i}n_{i}]^{-1/2}\).

We apply our method theoretically to two examples: (i) a toy example involving linear classifiers and Gaussian mixture model data with two components, which has been central in previous studies on DRO and gradual domain adaptation, and (ii) a more general class of distributions (referred to as "expandable" distributions) with learnable classifiers. In the former case, we demonstrate that accounting for Gaussian structural information eliminates error propagation in the statistical sense in the asymptotic regime. Additionally, in the non-asymptotic scenario, having \(n dT T\) samples per dataset leads to the same result. Conversely, neglecting manifold information results in exponentially growing error, as anticipated. For expandable distributions and learnable classifiers, we provide theoretical bounds on sample complexity and error propagation dynamics based on newer notions of adversarial robustness. Once again, we identify a rather general scenario where DRO completely eliminates error propagation. We further validate our theoretical findings through a series of experiments.

The rest of the paper is organized as follows: Section 1.1 reviews related work. Our methodology is discussed in Section 2, where we present our main theorems. Section 3 details our results for the Gaussian setting, while a broader class of problems, termed expandable distributions and smooth classifier families, are analyzed in Section 4, including their non-asymptotic analysis in subsection 4.1. In section 5, we will be discussing our experimental results. We conclude in Section 6.

### Previous Works

Classic unsupervised domain adaptation aims to align feature distributions between a labeled source domain and an unlabeled target domain. Generating intermediate domains can facilitate smoother adaptation, transforming the process into gradual domain adaptation. However, these intermediate domains are often unavailable. Sagawa et al.  address this by using normalizing flows to learn transformations from the target domain to a Gaussian mixture distribution through the source domain. Zhuang et al.  propose Gradient Flow (GGF) to generate intermediate domains, leveraging the Wasserstein gradient flow to transition from the source to the target domain, minimizing a composite energy function.

Kumar et al.  propose a gradual self-training algorithm, adapting the initial classifier using pseudo-labels from intermediate domains. They show the importance of leveraging the gradual shift structure, regularization, and label sharpening, providing a generalization bound for target domain error. This bound is given by \(e^{(T)}(_{0}+( T }))\), where \(_{0}\) is source domain error, and \(n\) is each domain's data size. Wang et al.  improve this approach, achieving a significantly better generalization bound \(_{0}+}(T+Tn^{-1/2}+(nT)^{-1/2})\), where \(\) is the average distance of consecutive domain distributions, and propose an optimal strategy for constructing intermediate domain paths. He et al.  suggest placing intermediate domains uniformly along the Wasserstein distance between the source and target domains to minimize generalization error. The GOAT framework, based on this insight, uses optimal transport to generate intermediate domains and applies gradual self-training. Similarly, Abnar et al.  introduce GIFT, which creates virtual samples from intermediate distributions by interpolating representations of examples from source and target domains. Zhang et al.  propose the AuxSelfTrain framework, generating a combination of source and target data in different proportions, gradually incorporating more target data, and employing a self-training procedure.

Unsupervised domain adaptation can be viewed as a Generalized Target Shift problem. Xiao et al.  introduce a discriminative energy-based method for test sample adaptation in domain generalization, modeling the joint distribution of input features and labels on source domains. Kirchmeyer et al.  propose the OSTAR method, using optimal transport to align pre-trained representations without enforcing domain invariance, reweighting source samples, and training a classifier on the target domain. Generative Adversarial Networks (GANs)  inspire domain adaptation methods that use a feature extractor and a classifier to generate class responses, processed by a discriminator to distinguish between source and target domains. Cui et al.  introduce the Gradually Vanishing Bridge (GVB) framework to reduce domain-specific characteristics and balance adversarial training, enhancing domain-invariant representations.

### Notations and Definition

Consider \(\) as a measurable space for features and let \(=\{-1,+1\}\) represent the set of possible labels in a binary classification scenario. In this regard, \(=\) encompasses the entire space of feature-label pairs. We use \(()\) to denote the set of all probability measures supported on \(\). For any \(p 1\), let \(\|\|_{p}\) denote the \(_{p}\)-norm. Additionally, for a probability measure \(P()\), the notation \(P_{}\) refers to the marginal distribution of \(P\) on \(\). Let \(g:\) be a given function, and consider a natural number \(n\). We define the composition of \(g\) repeated \(n\) times as follows:

\[g^{ n}()=(g g g)(),(n \ ).\] (1)

In order to assess the distance between any two measures \(P,Q()\), we use the _Wasserstein_ metric. For \(P,Q()\), \( 0\) and \(p,q 1\), the \(\)-weighted \(_{p}^{q}\)-Wasserstein distance between \(P\) and \(Q\) is defined as

\[_{p,}^{q}(P,Q)_{ (P,Q)}[\|-^{ }\|_{p}^{q}+ 1\{y y^{}\}],\] (2)

where \((P,Q)\) denotes the set of all couplings \(\), ensuring that \((,)=P\) and \((,)=Q\). Also, let \(:_{ 0}\) be a legitimate loss function, where for most of the paper we simply consider it to be \(0-1\) loss for simplicity in the results.

## 2 The Proposed Method: Gradual Domain Adaptation via Manifold-Constrained DRO

Let's consider the distribution set \(()\) to denote a class, or manifold, of distributions characterized by favorable properties, such as, but not restricted to, having soft or hard margins between class-conditional measures. Throughout this paper, we presume that all measures \(P_{i}\) belong to such a class with a known property. Failing to acknowledge this assumption could render the error propagation dynamics uncontrollable (see Theorem 3.2). Before proceeding further, let us introduce the following definitions:

**Definition 2.1** (Restricted Wasserstein Ball).: Assume fixed parameters \(p,q 1\) and \(>0\). For \( 0\), \(()\) and \(P_{0}\), let us define

\[_{}(P_{0}|)\{P\ _{p,}^{q}(P,P_{0})\}\] (3)

as a \(\)-restricted Wasserstein ball of radius \(\).

Building upon the above definition, we introduce our method formally outlined in Algorithm 1. The essence of our approach lies in conducting DRO on a pseudo-labeled version of \(P_{i}\) (or its empirical estimate \(_{i}\)), followed by leveraging the model to assign pseudo-labels to the subsequent unlabeled distribution. However, two crucial considerations emerge: i) the adaptive adjustment of the Wasserstein radius (also known as the adversarial power of DRO) based on the robust loss incurred in the preceding stage, and ii) post pseudo-labeling, distributions are implicitly constrained to the manifold \(\). This latter aspect serves as the primary mechanism for controlling error propagation within appropriately restricted scenarios.

``` Params :\(\), \(\), \(p,q,\), and \(\) Input :\(P_{0},\{P_{i_{X}}\}_{1:T}\) Initialize: \(_{0},_{0} P_{0}\) \(_{0}^{*},\;_{0}^{*}\{_{}, *{argmin}_{}\}_{P_{ _{0}}(_{0}|)}_{P} [(y,h_{}())].\) for\(i=1,,T-1\)do \(_{i} P_{i_{X}}() (y=h_{_{i-1}^{*}}()), (,y)\) \(_{i}_{i-1}^{*}+\) \(_{i}^{*},_{i}^{*}\{_{}, *{argmin}_{}\}_{P_{ _{i}}(_{i}|)}_{P}[ (y,h_{}())]\) Result:\(^{*}_{T-1}^{*}\) ```

**Algorithm 1**DRO-based Domain Adaptation (DRODA)

Before delving into the theoretical guarantees, let us introduce our new complexity measure, which quantifies the relationship between a family of binary classifiers \(=\{h_{}|\}\) and the distribution family \(\). The compatibility function, essentially a bound on the manifold-constrained adversarial loss of \(\) on \(\), plays a pivotal role in error propagation, as elucidated in Theorem 2.3.

**Definition 2.2** (Compatibility between \(\) and \(\)).: Consider the classifier set \(\{h_{}|\;\}\), distribution manifold \(()\), and Wasserstein metric \(_{p,}^{}(,)\) for \(p,q 1\) and \( 0\). We say \(\) and \(\) are _compatible_ according to a function \(g_{}():_{ 0}_{ 0}\), if for \(>0\) and \( P_{0}\) the following bound holds:

\[g_{}()_{}_{P_{ }(P_{0}|)}_{P}[(y,h_{ }())].\] (4)

As can be seen, \(g_{}(0)\) represents an upper bound on the minimum achievable _non-robust_ error rate across all measures within \(\). Mathematically, this is expressed as:

\[g_{}(0)_{P}_{}_{P} [(y,h_{}())].\] (5)

We declare that \(\) and \(\) are _perfectly compatible_ if the lower bound on the r.h.s. of (5), and consequently \(g_{}(0)\), is zero. This means for any \(P\), at least a classifier in \(\) can perform a perfect non-robust classification.

We believe the concept of "compatibility" as defined above is natural and can uniquely characterize the applicability of GDA to a problem set. For example, assume all measures in \(\) exhibit some level of "cluster assumption" or have hard margins, and that \(\) is rich enough to robustly classify all \(P\) (with some margin). Then, there exists \(_{0}>0\) such that \(g_{}()=0\) for all \(_{0}\). We will soon see that such a property can perfectly eliminate error propagation, as long as consecutive unlabeled measures are chosen close enough as a function of \(_{0}\). More generally, the following theorem provides a general bound on the propagation of generalization error as a function of the compatibility measure in the asymptotic case where \(_{i}\;n_{i}\).

**Theorem 2.3**.: _For \(>0\) and \(p,q 1\), assume classifier set \(\{h_{}|\;\}\) and distribution family \(()\) are compatible according to the Wasserstein metric \(_{p,}^{}(,)\) and a positive function \(g_{}()\). Additionally, for \(T 1\) assume a finite sequence of distributions \(P_{0},P_{1},,P_{T}\) in \(\), where \(_{p,}^{}(P_{i},P_{i+1})\) for \(i=0,,T-1\) and a given \( 0\). The initial measure \(P_{0}\) is assumed to be known, however for \(i 1\), we only have access to the marginals \(P_{i_{}}\). Then, Algorithm 1_ (\(\)) _with parameters \(\), \(\), \(p,q,\), and \(\) outputs \(^{*}=(P_{0},\{P_{i_{}}\}_{1:T})\) which satisfies the following bound:_

\[_{P_{T}}[(y,h_{^{*}}()) ][g_{}(2()+)] ^{ T}(_{}_{P_{}(P_ {0}|)}_{P}[(y,h_{}())]),\]

_where \( T\) implies composition of function \(u g_{}(2 u+)\) on the input for \(T\) times. The input is the restricted robust loss on \(P_{0}\) for a Wasserstein radius of \(\)._

The proof can be found in the Appendix (supplementary material). As inferred from the bound, the shape of \(g_{}\) determines the behavior of the generalization error on the last measure. For example, if \(g\) increases linearly, i.e., if the robust loss increases linearly with the adversarial radius with a coefficient greater than or equal to \(1/(2)\), it implies an exponential growth in the generalization error. However, if the manifold structure on \(\) causes \(g\) to grow linearly with a smaller coefficient, or behave similarly to a saturating (or at least sublinear) function, error propagation can be kept bounded. In this regard, the following corollary specifies the conditions under which our algorithm provides a bounded error regardless of \(T\). Proof of Corollary 2.4 can be found inside the Appendix section.

**Corollary 2.4** (Elimination of Error Propagation).: _Consider the setting described in Theorem 2.3. For a given hypothesis set \(\), distribution manifold \(\), \(0<1\) and \(p,q 1\), assume the compatibility function \(g_{}\) satisfies:_

\[g_{}()+,  0,\] (6)

_where \( 0\) can be any fixed value. Then, for any \(T\) we have:_

\[_{P_{T}}[(y,h_{^{*}}()) ] 3(+_{i[T]}\,_{p, }^{}(P_{i-1},P_{i})).\] (7)

_which is independent of \(T\) as long as consecutive pairs remain distributionally close._

## 3 Theoretical Guarantees on Gaussian Generative Models

In the following two sections, we investigate practical and theoretically useful cases of potentially compatible pairs \(\) and \(\) to achieve mathematically explicit bounds. We first focus on the well-known and celebrated example of a two-component Gaussian mixture model, which has been the focus of various previous studies . One main reason is that our results can be easily compared with those of prior works.

Mathematically, suppose that the set of features and labels, denoted as \((,y)^{d}\{0,1\}\), originates from a Gaussian generative model. For some \(L>0\), we have:

\[\{P(y= 1)=,\\ |y(y,^{2}I). \|\|_{2} L.\] (8)

This setting implies that the class-conditional density of feature vectors consists of two Gaussians with equal covariance matrices \(^{2}I\) and mean vectors \(\) and \(-\), respectively. The \(_{2}\)-norm of \(\) is lower-bounded by some \(L>0\) to prevent the optimal Bayes' error from converging toward 1, thus the classification remains meaningful. Throughout this section, the distribution manifold \(_{g}=_{g}(L)\) refers to this class, with the vector \(\) as its only degree of freedom. In this context, our goal is to find the compatibility function \(g_{}()\) when linear classifiers are employed. The following theorem presents one of our main results for this purpose:

**Theorem 3.1**.: _For \(L>0\) and any \( 0\), consider the distribution manifold \(_{g}(L)\). Then, the compatibility function between \(_{g}\) (w.r.t. Wasserstein metric \(_{2,}^{1}\)) and the set of linear binary classifiers as \(\) satisfies this bound:_

\[g_{}() e^{-}{18^{2}}}, [0,L/3].\] (9)

_Also, for any \(T\) and any sequence of distributions \(P_{1},,P_{T}_{g}\) with \(_{2,}^{1}(P_{i},P_{i+1}) L/3\), \(\) guarantees the following error bound on the last unlabeled measure:_

\[_{P_{T}}[(y,h_{^{*}}()) ] e^{-}{18^{2}}}.\] (10)Proof can be found in Appendix A. Note that there are no error propagation, and the guaranteed error term is close to the Bayes' optimal error. In fact, it can become arbitrarily close with more sophisticated mathematics, which goes beyond the scope of this work. The condition \( L/3\) is necessary to prevent the two Gaussians from swapping, as tracking them becomes impossible if that happens.

An important question to consider is what happens to \(g_{}\) if one does not restrict the Wasserstein ball to the manifold \(_{g}\). In other words, assume we set \(_{g}\) to be the entire space of measures and not the restricted Gaussian manifold considered so far. We will show that the _manifold constraint_ is a key property that provides us with the desirable result of Theorem 3.1, and losing this assumption can have catastrophic consequences.

**Theorem 3.2** (Potentials for Error Propagation).: _For \(L>0\), consider the Gaussian manifold \(_{g}(L)\) versus the set of linear classifiers in \(\). Also, assume Wasserstein metric \(_{2,}^{1}\) is being employed, for any \( 0\). By \(g_{}^{}()\), let us denote the compatibility function when manifold constraint is taken into account similar to Theorem 3.1, while \(g_{}^{}\) represents the unconstrained compatibility function when there are no manifold constraints, i.e., \(_{g}=()\). Then,_

\[g_{}^{}() e^{-L^{2}/(18^{2} )},[0,L/3], g_{}^{ }()(e^{-}{2^{2}}}+ }{2^{2}}}}), 0.\]

Proof can be found in Appendix section A. We already know the generalization error from the manifold constrained version of DRODA does not propagate after \(T\) iterations. However, the error term stemming from the unconstrained version can be shown to be bounded by

\[_{P_{T}}[(y,h_{^{*}}()) ]((2 e^{}{2^{2}}} )^{2}^{(1/2^{T})}+e^{}{2^{2}}}),\] (11)

which shows significant potential for error propagation.

The results so far are in the statistical sense, meaning that we have assumed \(_{i}\ n_{i}\). A slight variation of our bounds still applies to the non-asymptotic case, where we can propose PAC-like generalization guarantees. The following theorem is, in fact, the non-asymptotic version of Theorem 3.1 (proof is given in Appendix A):

**Theorem 3.3** (Non-asymptotic Generalization Guarantee).: _In the setting of Theorem 3.1 with some \(L>0\) and any \( 0\), suppose we have \(n_{0}\) labeled samples from distribution \(P_{0}\) and \(n_{i}\) unlabeled samples from distribution \(P_{i}\) for \(i[T]\). \(T\) can be unbounded, but consecutive pairs \(P_{i},P_{i+1}\) must have a Wasserstein distance \(_{2,}^{1}\) bounded by \(L/3\). For any \((0,1]\) and using algorithm DRODA, the error in the last (most recent) domain with probability at least \(1-\) is bounded by:_

\[_{T}^{*} 2e^{-}{2^{2}}}+(}{n_{i}})^{}_{i=1}^{T}(}{^{ 2}}e^{-}{18^{2}}})^{i}.\] (12)

**Corollary 3.4** (Elimination of Error Propagation in Non-asymptotic Regime).: _In the setting of Theorem 3.3, assume \(L 11\) (e.g., each component of mean vectors \(\) and \(-\) are larger than \(11/\)). Also, assume each dataset \(P_{i}\) for \(i 1\) has at least \(n\) unlabeled data points, where_

\[n(})\]

_for some \(>0\). Then, the following bound holds for \(_{T}^{*}\) regardless of \(T 2\):_

\[_{T}^{*} 2e^{-}{2^{2}}}+}{^{2}}e^{-}{18^{2}}}},\] (13)

_which means error propagation is perfectly eradicated._

Corollary 3.4 can be directly proved from the result of Theorem 3.3.

Expandable Distribution Manifolds and Learnable Classifiers

The class of isotropic Gaussians, while a well-known theoretical benchmark, is still a very stringent and impractical case to study. In this section, we investigate a much more general class of distribution manifold/classifier pair families and provide both asymptotic and non-asymptotic guarantees for this regime. Before introducing our target regime, let us define some required concepts. Assume \((,)\) is a measurable space, and let \(P\) be a distribution supported over \(\). For \(r 0\), the \(r\)-neighborhood of a point \(\), denoted by \(_{r}()\), is defined as:

\[_{r}()=\{^{}\|-^{}\|_ {2} r\}.\] (14)

Similarly, the \(r\)-neighborhood of a Borel set \(A\) (i.e., \(A\)) is defined as:

\[_{r}(A)=\{^{} A\|-^{}\|_{2} r\},\] (15)

we also define the \(\)-neighborhood of a Borel set \(A\), for \(^{d}\) as:

\[_{}(A)=\{^{} A,| | 1^{}=+\}.\] (16)

Following , we define the _expansion_ property as:

**Definition 4.1** (\((C_{1},c_{2})\) - expansion).: For a fixed \(0<<\) and given \(C_{1},C_{2} 0\), consider \(\{A| P(A )\}\). Then, we say a distribution \(P\) has \((C_{1},C_{2})\)-expansion property if

\[_{A}_{r}(A)) }{P(A)} 1+C_{1}r,_{A}_{r}(A))}{P(A)} 1+C_{2}r,\]

for sufficiently small \(r 0\).

This definition extends the \((a,c)\)-expansion property defined by . A \((C_{1},C_{2})\)-expandable distribution is required to have a continuous support and avoid singularity, aligning with the majority of practical measures. Expandable distributions can be further restricted to have additional theoretical properties, such as \(\)-smoothness, defined as follows:

**Definition 4.2** (\(\) - smoothness).: We say that a distribution \(P\) supported on a feature-label space \(^{d}\{ 1\}\) satisfies the \(\)-smoothness property if for all \(A\), there exists a constant \(C\) which depends only on \(P(A)\), where the class-conditional measures of \(P\), i.e., \(P^{+}()\) and \(P^{-}()\), satisfy the following for sufficiently small \(r 0\):

\[((_{}(A))}{P^{s}(A)}-1)  C_{A}(1), s\{\},\ ,\ \|\|_{2} r.\] (17)

Another necessary definition ensures that a classifier family \(\) is inherently capable of achieving a low classification error on a distribution, i.e., a low bias for \(\) and simultaneously a small Bayes' error for \(P\).

**Definition 4.3** (\(-\)separation).: For \( 0\), a distribution \(P\) supported on feature-label set \(^{d}\{ 1\}\) has the \(-\) separation property with respect to a binary classification hypothesis set \(\), if

\[_{h}P(yh() 0).\] (18)

We can now explain our proposed setting for the expandable distribution manifold \(\), which consists of expandable distributions (in both senses of \((C_{1},C_{2})\)-expansion and \(\)-smoothness, which are slight variations of each other). The core idea is to use the dual formulation of  and  for a (non-manifold constrained) Wasserstein DRO, which can be stated as follows:

\[_{P_{}(P_{0})}_{P}[(;) ]=_{ 0}\{+_{P_{0}}[_{^{ }}\{(;^{})- c(,^{}) \}]\}.\] (19)

To add the manifold constraint, we propose restricting the space of adversarial examples \(^{}\) to be generated from a predetermined function class \(\), where for \(f\) we have \(f:\). Each \(f\) is a fixed mapping from the feature-label space to itself. By controlling the complexity of \(\), one can limit the adversarial budget of the DRO and effectively simulate the condition of optimizing within a Wasserstein ball in addition to some kind of "manifold constraint." Mathematically, we can replace the original dual form with the following (more restricted) formulation:

\[_{P_{n}(P_{0}|)}_{p}[ (;)]=_{ 0}_{f} +_{P_{0}}[\{(;f( {Z}))- c(,f())\} ]},\] (20)

There exists a (potentially intricate) mathematical relationship between the distributional manifold \(\) on the left-hand side and the mapping function class \(\) on the right-hand side of the above formulation. Our main contribution in this part can be informally stated as follows:

* We theoretically show that restricting the distributional manifold \(\) to include only expandable distributions, as defined in Definitions 4.1 and 4.2, is _equivalent_ to restricting the dual optimization formulation such that the mapping class \(\) consists only of "smooth" mappings.

Note that if we do not impose any constraints on \(\), and \(f\) could be any function, there is no difference between the quantities in Equations (19) and (20).

**Theorem 4.4** (Transportability of \(\)-Smooth Measures).: _For some \(>0\), let us consider two data distributions \(P_{1}\) and \(P_{2}\), both of which are \(\)-smooth according to Definition 4.2. Let \(f^{+}\) and \(f^{-}\) represent the optimal transport (Monge) mappings between \(P_{1}^{+} P_{2}^{+}\) and \(P_{1}^{-} P_{2}^{-}\), respectively. These mappings are also known as push-forward functions, which transform one measure into another. Let \(J^{+}\) and \(J^{-}\) represent the respective \(d d\) Jacobian matrices of the mappings, where \(d=()\). Then, the eigenvalues of the Jacobian matrices satisfy the following conditions:_

\[1-2_{i}(J^{s}) 1+2, i[d],\ s \{ 1\}.\] (21)

Proof is given in Appendix A. Essentially, the theorem states that each pair of \(\)-smooth class-conditional measures can be optimally transported into each other via highly-smooth mappings, where the Jacobian of the mapping resembles the identity matrix. At this point, we can present a theorem that provides an upper bound for the compatibility function between a distribution class \(\) with expansion properties and a hypothesis set of learnable binary classifiers:

**Theorem 4.5**.: _For \(C_{1},C_{2},, 0\), consider a distribution manifold \(\) where its distributions satisfy the \((C_{1},C_{2})\)-expansion, \(\)-separation with respect to a hypothesis set \(\), and \(\)-smoothness properties as defined in Definitions 4.1 through 4.3. Hypothesis set \(\) is general up to \(\)-separation property. For \(, 0\) and \(T\), consider the GDA setting of Theorem 2.3 with a distributional sequence \(P_{0},,P_{T}\) where the pairwise distance between consecutive measures satisfies \(_{2,}^{1}(P_{1},P_{i+1})\). Moreover, make the following assumptions: i) assume all of the mass of \(P_{0}\) falls inside a hypersphere with radius at most \(R\), ii) assume \(\), and iii) \(>\). Then the compatibility function between \(\) and \(\) has the following upper bound:_

\[g_{}()((1+C_{1}(4R +2))).\] (22)

Proof can be found in Appendix A. Based on the bound on the compatibility function, using Theorem 2.3, it can be easily shown that a modified version of Algorithm DRODA, presented in Appendix A in Algorithm 3, guarantees a generalization error of at most \((+)\) on the last (most recent) distribution \(P_{T}\), which is irrespective of \(T\), thereby entirely eliminating error propagation.

### Non-Asymptotic Analysis of Manifold-Constrained DRO on Expandable Distributional Manifold

This section explores the non-asymptotic analysis of manifold-constrained DRO on the expandable distribution manifold. We assume empirical estimates from \(P_{i}\)'s, where each empirical measure \(_{i}\) is obtained via \(n_{i}\) i.i.d. samples from \(P_{i}\). For simplicity in our results, we assume \(n_{i}=n\) for all \(i\{0,1,,T\}\). First, let us redefine the loss in its dual format:

\[R(;P_{0})=_{f}_{P_{0}}[\{ (;f())- c(,f( {Z}))\}].\] (23)

Assuming \(R(;_{0})\) is the empirical version of \(R(;P_{0})\), let the minimizer of \(R(;_{0})\) be denoted as \(\). For simplicity, we only consider the class of linear binary classifiers and Gaussian mixture models. However, the main distinction between the setting of Theorem 3.3 and this section lies in the fact that we assume no prior knowledge regarding the Gaussian assumption; only the expandable distribution assumption is considered. In other words, there are no implicit or explicit projection onto the manifold of Gaussian mixture models any more. At this point, we present our main theorems, which provide the generalization bound for DRODA algorithm:

**Theorem 4.6** (Generalization Bound for DRODA in the Non-asymptotic Regime).: _Consider the class of linear classifiers as \(\), and the zero-one loss function as \(\). The rest of the setting is similar to Theorem 4.5. Assume we limit \(\) to the displacement functions and let \(P_{0}\) be a Gaussian generative model with mean \(\) and covariance matrix \(^{2}I_{d}\) as defined in (8), then for \(,>0\) we have the following generalization bound with probability at least \(1-\):_

\[R(;P_{0})_{}R(; P_{0})+64(}{d}} )},\] (24)

_where \(n\) is the number of i.i.d. samples from \(P_{0}\) and \(d\) is the dimension of the feature space._

Proof can be found in Appendix A. As demonstrated, not only is error propagation eliminated, but the generalization error also decreases with increasing \(n\).

The polynomial-time convergence of Wasserstein-based DRO programs have been extensively studied (see ). Given sufficient assumptions on the smoothness of our loss functions and transportation costs in the Wasserstein metric, the convergence rate of \((1/^{2})\) iterations (for any \(>0\)) in order to get to the \(\)-proximity of the optimal solution is already guaranteed.

## 5 Experimental Results

In this section we present our experimental results. It should be noted that several existing works have already experimentally validated the first part of the paper which concerns Gaussian mixture models. Hence, our contributions for those parts are mainly theoretical. In this section, we mainly focus on the second part of our contributions, i.e., Section 4.

In figure 1 we illustrate the workings of our method to generate adaptive mappings between consecutive distributions and the following projection onto the manifold, which is mathematically modeled by the function space \(\) in our formulations. As depicted, at the \(i\)th step, we perturb the data samples \((_{j},y_{j}),j[n_{i}]\) from \(P_{i}\) using a parametric function class, denoted as \(f_{p}\), and penalize the extent of perturbation using the following term

\[}_{j=1}^{n_{i}}\|f_{p}(_{j})-_{j}\|_{2}.\] (25)

These perturbed samples are then classified using a classifier. Let \(L_{C}(f_{P};_{1},,_{n_{i}})\) represent the cross-entropy loss of the classifier on the perturbed samples. Our objective is to solve the following program:

\[_{f_{C}}\ _{f_{P}}\ \{L_{C}(f_{p};_{1}, ,_{n_{i}})-}_{j=1}^{n_{i}}\|f_{P}( {X}_{j})-_{j}\|_{2}\},\] (26)

Figure 1: A schematic view of the proposed procedure for our manifold-constrained DRO. A restricted adversarial block, modeled by \(f_{P}\), tries to perturb the source distribution at each step \(i\) to prepare the algorithm for the worst possible distribution in step \(i+1\). Meanwhile, a classifier \(f_{C}\) tries to learn a classifier based on the perturbed distribution.

which is a minimization with respect to the parametric classifier family \(f_{C}\), while simultaneously maximizing it with respect to the parametric family of generator function \(f_{P}\). In our experiments, we employed a two-layer CNN with a \(7 7\) kernel in the first layer and a \(5 5\) kernel in the second layer for \(\). We also utilized an affine grid and grid sample function in PyTorch, following the approach introduced in . For the classifier family \(\), we used a three-layer CNN with max pooling and a fully connected layer, applying dropout with a rate of \(0.5\) in the fully connected layer. A standard Stochastic Gradient Descent (SGD) procedure has been used for the min-max optimization procedure described in (26).

We implemented this method on the "Rotating MNIST" dataset, similar to . In particular, we sampled 6 batches, each with a size of 4200, without replacement from the MNIST dataset, and labeled these batches as \(D_{0},D_{1},,D_{4}\), which represent the datasets obtained from \(P_{0},P_{1},,P_{4}\), respectively. The images in dataset \(D_{i}\) were then rotated by \(i 15\) degrees, with \(D_{0}\) serving as the source dataset and \(D_{4}\) as the target dataset. We provided the source dataset with labels and left \(D_{1},D_{2},D_{3}\), and \(D_{4}\) unlabeled for our algorithm. We then tested the accuracy of \(_{0}^{*},,_{3}^{*}\)--the outputs of our algorithm at each step--on \(D_{1},D_{2},D_{3}\), and \(D_{4}\), respectively.

For comparison, we implemented the GDA method exactly as described in . We compared our method to the GDA and detailed the results in Figure 2. Additionally, we reported the accuracy of \(_{0}^{*}\) on \(D_{0}\) as an example of in-domain accuracy. Our results show that our method outperforms GDA by a significant margin of \(8\) percent in the last domain \(D_{4}\).

## 6 Conclusions

In conclusion, we have introduced a novel approach to gradual domain adaptation leveraging distributionally robust optimization (DRO). Our methodology provides theoretical guarantees on model adaptation across successive datasets by bounding the Wasserstein distance between consecutive distributions and ensuring that distributions lie on a manifold with favorable properties. Through theoretical analysis and experimental validation, we have demonstrated the efficacy of our approach in controlling error propagation and improving generalization across domains. A key tool for achieving this is our newly introduced complexity measure, termed the "compatibility function."

We have investigated two theoretical settings: i) a two-component Gaussian mixture model, a well-known theoretical benchmark, and ii) a more general class of distributions termed "expandable" distributions, along with general expressive (low-bias) classifier families. Theoretical analyses show that our method completely eliminates error propagation in both scenarios, and also in both asymptotic and non-asymptotic cases. These findings contribute to a better understanding of gradual domain adaptation and provide practical insights for developing robust machine learning models in real-world situations.

Figure 2: Comparison of the performance of our proposed method with the GDA  on rotating MNIST dataset.