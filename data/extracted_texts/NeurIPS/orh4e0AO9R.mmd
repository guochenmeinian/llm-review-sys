# Bypassing the Simulator:

Near-Optimal Adversarial Linear Contextual Bandits

 Haolin Liu

University of Virginia

srs8rh@virginia.edu

The authors are listed in alphabetical order.

Chen-Yu Wei

University of Virginia

chenyu.wei@virginia.edu

Julian Zimmert

Google Research

zimmert@google.com

The authors are listed in alphabetical order.This work was done when Chen-Yu Wei was at MIT Institute for Data, Systems, and Society.

###### Abstract

We consider the adversarial linear contextual bandit problem, where the loss vectors are selected fully adversarially and the per-round action set (i.e. the context) is drawn from a fixed distribution. Existing methods for this problem either require access to a simulator to generate free i.i.d. contexts, achieve a sub-optimal regret no better than \(}(T^{5/6})\), or are computationally inefficient. We greatly improve these results by achieving a regret of \(}()\) without a simulator, while maintaining computational efficiency when the action set in each round is small. In the special case of sleeping bandits with adversarial loss and stochastic arm availability, our result answers affirmatively the open question by Saha et al. (2020) on whether there exists a polynomial-time algorithm with \((d)\) regret. Our approach naturally handles the case where the loss is linear up to an additive misspecification error, and our regret shows near-optimal dependence on the magnitude of the error.

## 1 Introduction

Contextual bandit is a widely used model for sequential decision making. The interaction between the learner and the environment proceeds in rounds: in each round, the environment provides a context; based on it, the learner chooses an action and receive a reward. The goal is to maximize the total reward across multiple rounds. This model has found extensive applications in fields such as medical treatment (Tewari and Murphy, 2017), personalized recommendations (Beygelzimer et al., 2011), and online advertising (Chu et al., 2011).

Algorithms for contextual bandits with provable guarantees have been developed under various assumptions. In the linear regime, the most extensively studied model is the _stochastic linear contextual bandit_, in which the context can be arbitrarily distributed in each round, while the reward is determined by a fixed linear function of the context-action pair. Near-optimal algorithms for this setting have been established in, e.g., (Chu et al., 2011; Abbasi-Yadkori et al., 2011; Li et al., 2019; Foster et al., 2020). Another model, which is the focus of this paper, is the _adversarial linear contextual bandit_, in which the context is drawn from a fixed distribution, while the reward is determined by a time-varying linear function of the context-action pair. 3 A computationally efficient algorithm for this setting is first proposed by Neu and Olkhovskaya (2020). However, existing research for this setting still faces challenges in achieving near-optimal regret and sample complexity when the context distribution is unknown.

The algorithm by Neu and Olkhovskaya (2020) requires the learner to have _full knowledge_ on the context distribution, and access to an _exploratory policy_ that induces a feature covariance matrix with a smallest eigenvalue at least \(\). Under these assumptions, their algorithm provides a regret guarantee of \(}(|)T/})^{4}\), where \(d\) is the feature dimension, \(||\) is the maximum size of the action set, and \(T\) is the number of rounds. These assumptions are relaxed in the work of Luo et al. (2021), who studied a more general linear MDP setting. When specialized to linear contextual bandits, Luo et al. (2021) only requires access to a _simulator_ from which the learner can draw free i.i.d. contexts. Their algorithm achieves a \(}((d(||)T^{2})^{}{{3}}}))\) regret. The regret is further improved to the near-optimal one \(}(|)T})\) by Dai et al. (2023) through refined loss estimator construction.

All results that attain \(}(T^{}{{3}}})\) or \(}()\) regret bound discussed above rely on access to the simulator. In their algorithms, the number of calls to the simulator significantly exceeds the number of interactions between the environment and the learner, but this is concealed from the regret bound. Therefore, their regret bounds do not accurately reflect the sample complexity of their algorithms. Another set of results for linear MDPs (Luo et al., 2021; Dai et al., 2023; Sherman et al., 2023; Kong et al., 2023) also consider the simulator-free scenario, essentially using interactions with the environment to fulfill the original purpose of the simulator. When applying their techniques to linear contextual bandits, their algorithms only achieve a regret bound of \(}(T^{}{{6}}})\) at best (see detailed analysis and comparison in Appendix G).

Our result significantly improves the previous ones: without simulators, we develop an algorithm that ensures a regret bound of order \(}(d^{2})\), and it is computationally efficient as long as the size of the action set is small in each round (similar to all previous work). Unlike previous algorithms which always collect new contexts (through simulators or interactions with the environment) to estimate the feature covariance matrix, we leverage the context samples the learner received in the past to do this. Although natural, establishing a near-tight regret requires highly efficient use of context samples, necessitating a novel way to construct the estimator of feature covariance matrix and a tighter concentration bound for it. Additionally, to address the potentially large magnitude and the bias of the loss estimator, we turn to the use of log-determinant (logdet) barrier in the follow-the-regularized-leader (FTRL) framework. Logdet accommodates larger loss estimators and induces a larger bonus term to cancel the bias of the loss estimator, both of which are crucial for our result.

Our setting subsumes sleeping bandits with stochastic arm availability (Kanade et al., 2009; Saha et al., 2020) and combinatorial semi-bandits with stochastic action sets (Neu and Valko, 2014). Our result answers affirmatively the main open question left by Saha et al. (2020) on whether there exists a polynomial-time algorithm with \((d)\) regret for sleeping bandits with adversarial loss and stochastic availability.

As a side result, we give a computationally inefficient algorithm that achieves an improved \(}(d)\) regret without a simulator. While this is a direct extension from the EXP4 algorithm (Auer et al., 2002), such a result has not been established to our knowledge, so we include it for completeness.

### Related work

We review the literature of various contextual bandit problems, classifying them based on the nature of the context and the reward function, specifically whether they are stochastic/fixed or adversarial.

Contextual bandits with i.i.d. contexts and fixed reward functions (S-S)Significant progress has been made in contextual bandits with i.i.d. contexts and fixed reward functions, under general reward function classes or policy classes (Langford and Zhang, 2007; Dudik et al., 2011; Agarwal et al., 2012, 2014; Simchi-Levi and Xu, 2022; Xu and Zeevi, 2020). In the work by Dudik et al. (2011); Agarwal et al. (2012, 2014), the algorithms also use previously collected contexts to estimate the inverse probability of selecting actions under the current policy. However, these results only obtain regret bounds that polynomially depend on the number of actions. Furthermore, these results rely on having a fixed reward function, making their techniques not directly applicable to our case.

Contextual bandits with adversarial contexts and fixed reward functions (A-S)In this category, the most well-known results are in the linear setting (Chu et al., 2011; Abbasi-Yadkori et al., 2011; Zhao et al., 2023). Besides the linear case, previous work has investigated specific reward function classes (Russo and Van Roy, 2013; Li et al., 2022; Foster et al., 2018). Recently, Foster and Rakhlin (2020) introduced a general approach to deal with general function classes with a finite number of actions, which has since been improved or extended by Foster and Krishnamurthy (2021); Foster et al. (2021); Zhang (2022). This category of problems is not directly comparable to the setting studied in this paper, but both capture a certain degree of non-stationarity of the environment.

Contextual bandits with i.i.d. contexts and adversarial reward functions (S-A)This is the category which our work falls into. Several oracle efficient algorithms that require simulators have been proposed for general policy classes (Rakhlin and Sridharan, 2016; Syrgkanis et al., 2016). The oracle they use (i.e., the empirical risk minimization, or ERM oracle), however, is not generally implementable in an efficient manner. For the linear case, the first computationally efficient algorithm is by Neu and Olkhovskaya (2020), under the assumption that the context distribution is known. This is followed by Olkhovskaya et al. (2023) to obtain refined data-dependent bounds. A series of works (Neu and Olkhovskaya, 2021; Luo et al., 2021; Dai et al., 2023; Sherman et al., 2023) apply similar techniques to linear MDPs, but when specialized to linear contextual bandits, they all assume known context distribution, or access to a simulator, or only achieves a regret no better than \(}(T^{}{{6}}})\). The work of Kong et al. (2023) also studies linear MDPs; when specialized to contextual bandits, they obtain a regret bound of \(}(T^{}{{5}}}+())\) without a simulator but with a computationally inefficient algorithm and an undesired inverse dependence on the smallest eigenvalue of the covariance matrix. Related but simpler settings have also been studied. The sleeping bandit problem with stochastic arm availability and adversarial reward (Kleinberg et al., 2010; Kanade et al., 2009; Saha et al., 2020) is a special case of our problem where the context is always a subset of standard unit vectors. Another special case is the combinatorial semi-bandit problem with stochastic action sets and adversarial reward (Neu and Valko, 2014). While these are special cases, the regret bounds in these works are all worse than \(}((d))\). Therefore, our result also improves upon theirs. 5

Contextual bandits with adversarial contexts and adversarial reward functions (A-A)When both contexts and reward functions are adversarial, there are computational (Kanade and Steinke, 2014) and oracle-call (Hazan and Koren, 2016) lower bounds showing that no sublinear regret is achievable unless the computational cost scales polynomially with the size of the policy set. Even for the linear case, Neu and Olkhovskaya (2020) argued that the problem is at least as hard as online learning a one-dimensional threshold function, for which sublinear regret is impossible. For this

  Target Setting & Algorithm & Regret & Simulator & Computation & Assumption \\  General CB & Syrgkanis et al. (2016) & \((||)^{}{{5}}}(||T)^{2/3}\) & ✓ & \((||,||,T)\) & **ERM Oracle** \\   & Dai et al. (2023) & \(|}\) & ✓ & \((||,d,T)\) & \\   & Diá et al. (2023) & \(d(||)^{}{{5}}}T^{}{{6}}}\) & & \((||,d,T)\) & \\   & Sherman et al. (2023) & \((d^{T}T^{4})^{}{{5}}}+()\) & & \(T^{d}\) & \(,_{} I\) \\   &  & \(d^{2}\) & & \((||,d,T)\) & \\   & & Algorithm 2 & \(d\) & & \(T^{d}\) & \\  Contextual SB & Neu and Valko (2014) & \((dT)^{}{{3}}}\) & & \((d,T)\) & \\  Sleeping Bandit & Saha et al. (2020) & \(T}\) & & \((d,T)\) (\(|| d\)) & \\  

Table 1: Related works in the “S-A” category. CB stands for contextual bandits and SB stands for semi-bandits. The relations among settings are as follows: Sleeping Bandit \(\) Contextual SB \(\) Linear CB, Linear CB \(\) Linear MDP, and Linear CB \(\) General CB. The table compares our results with the Pareto frontier of the literature. For algorithms dealing more general settings, we have carefully translated their techniques to Linear CB and reported the resulting bounds. \(_{}\) denotes the feature covariance matrix induced by policy \(\). \(||\) and \(||\) are sizes of the action set and the policy set.

challenging category, besides using the inefficient EXP4 algorithm, previous work makes stronger assumptions on the contexts (Syrgkanis et al., 2016) or resorts to alternative benchmarks such as dynamic regret (Luo et al., 2018; Chen et al., 2019) and approximate regret (Emamjomeh-Zadeh et al., 2021).

Lifting and exploration bonus for high-probability adversarial linear banditsOur technique is related to those obtaining high-probability bounds for linear bandits. Early development in this line of research only achieves computational efficiency when the action set size is small (Bartlett et al., 2008) or only applies to special action sets such as two-norm balls (Abernethy and Rakhlin, 2009). Recently, near-optimal high-probability bounds for general convex action sets have been obtained by lifting the problem to a higher dimensional one, which allows for a computationally efficient way to impose bonuses (Lee et al., 2020; Zimmert and Lattimore, 2022). The lifting and the bonus ideas we use are inspired by them, though for different purposes. However, due to the extra difficulty arising in the contextual case, currently we only obtain a computationally efficient algorithm when the action set size is small.

### Computational Complexity

Our main algorithm is based on log-determinant barrier optimization similar to Foster et al. (2020); Zimmert and Lattimore (2022). Computing its action distribution is closely related to computing the D-optimal experimental design (Khachiyan and Todd, 1990). Per step, this is shown to require \(}(|_{t}|(d))\) computational and \(}((|_{t}|)(d))\) memory complexity (Foster et al., 2020, Proposition 1), where \(|_{t}|\) is the action set size at round \(t\). The computational bottleneck comes from (approximately) maximizing a quadratic function over the action set. It is an open question whether linear optimization oracles or other type of oracles can lead to efficient implementation of our algorithm for continuous action sets.

In the literature, there are few linear context bandit algorithms that provably avoid \(||\) computation per round. The LinUCB algorithm (Chu et al., 2011; Abbasi-Yadkori et al., 2011) suffers from the same quadratic function maximization issue, and therefore is computationally comparable to our algorithm. The SquareCB.Lin algorithm by Foster et al. (2020) is based on the same log-determinant barrier optimization. Another recent algorithm by Zhang (2022) only admits an efficient implementation for continuous action sets in the Bayesian setting but not in the frequentist setting (though they provided an efficient heuristic implementation in their experiments). The Thompson sampling algorithm by Agrawal and Goyal (2013), which has efficient implementation, also relies on well-specified Gaussian prior. The only work that we know can avoids \(||\) computation in the frequentist setting is Zhu et al. (2022), but their technique is only known to handle the A-S setting.

## 2 Preliminaries

We study the adversarial linear contextual bandit problem where the loss vectors are selected fully adversarially and the per-round action set (i.e. the context) is drawn from a fixed distribution. The learner and the environment interact in the following way. Let \(_{2}^{d}\) be the L2-norm unit ball in \(^{d}\).

For \(t=1,,T\),

1. The environment decides an adversarial loss vector \(y_{t}_{2}^{d}\), and generates a random action set (i.e., context) \(_{t}_{2}^{d}\) from a fixed distribution \(D\) independent from anything else.
2. The learner observes \(_{t}\), and (randomly) chooses an action \(a_{t}_{t}\).
3. The learner receives the loss \(_{t}[-1,1]\) with \([_{t}]= a_{t},y_{t}\).

A policy \(\) is a mapping which, given any action set \(^{d}\), maps it to an element in the convex hull of \(\). We use \(()\) to refer to the element that it maps \(\) to. The learner's _regret with respect to policy \(\)_ is defined as the expected performance difference between the learner and policy \(\):

\[()=[_{t=1}^{T} a_{t},y_{t}-_ {t=1}^{T}(_{t}),y_{t}]\]where the expectation is taken over all randomness from the environment (\(y_{t}\) and \(_{t}\)) and from the learner (\(a_{t}\)). The _pseudo-regret_ (or just _regret_) is defined as \(=_{}()\), where the maximization is taken over all possible policies.

**Notations** For any matrix \(A\), we use \(_{}(A)\) and \(_{}(A)\) to denote the maximum and minimum eigenvalues of \(A\), respectively. We use \((A)\) to denote the trace of matrix \(A\). For any action set \(\), let \(()\) be the space of probability measures on \(\). Let \(_{t}=(_{s},a_{s}, s t)\) be the \(\)-algebra at round \(t\). Define \(_{t}[]=[|_{t-1}]\). Given a differentiable convex function \(F:^{d}\{\}\), the Bregman divergence with respect to \(F\) is defined as \(D_{F}(x,y)=F(x)-F(y)- F(y),x-y\). Given a positive semi-definite (PSD) matrix \(A\), for any vector \(x\), define the norm generated by \(A\) as \(\|x\|_{A}=Ax}\). For any context \(^{d}\) and \(p()\), define \((p)=_{a p}[a]\) and \((p)=_{a p}[(a-(p))(a-(p))^{}]\). For any \(a\), define the lifted action \(=(a,1)^{}\) and the lifted covariance matrix \(}(p)=_{a p}[ {a}^{}]=_{a p}aa^{}&a\\ a^{}&1=(p)+(p)(p)^{ }&(p)\\ (p)^{}&1\). We use **bold** matrices to denote matrices in the lifted space (e.g., in Algorithm 1 and Definition 1).

## 3 Follow-the-Regularized-Leader with the Log-Determinant Barrier

In this section, we present our main algorithm, Algorithm 1. This algorithm can be viewed as instantiating an individual Follow-The-Regularized-Leader (FTRL) algorithm on each action set (Line 2), with all FTRLs sharing the same loss vectors. This perspective has been taken by previous works Neu and Olkhovskaya (2020); Olkhovskaya et al. (2023) and simplifies the understanding of the problem. The rationale comes from the following calculation due to Neu and Olkhovskaya (2020): for any policy \(\) that may depend on \(_{t-1}\),

\[_{t}[(_{t}),y_{t}]=_{_{t}}[_{y_{t}}[(_{t}),y _{t}_{t-1}]]=_{_{0}} [_{y_{t}}[(_{0}),y_{t} _{t-1}]]=_{t}[(_{0} ),y_{t}]\]

where \(_{0}\) is a sample drawn from \(D\) independent of all interaction history. This allows us to calculate the regret as

\[[_{t=1}^{T}_{t}(_{t})-( _{t}),y_{t}]=[_{t=1}^{T}_{t}( _{0})-(_{0}),y_{t}]\] (1)

where \(_{t}\) is the policy used by the learner at time \(t\). Note that this view does not require the learner to simultaneously "run" an algorithm on every action set since the learner only needs to calculate the policy on \(\) whenever \(_{t}=\). In the regret analysis, in view of Eq. (1), it suffices to consider a single fixed action set \(_{0}\) drawn from \(D\) and bound the regret on it, even though the learner may never execute the policy on it. This \(_{0}\) is called a "ghost sample" in Neu and Olkhovskaya (2020).

### The lifting idea and the execution of Algorithm 1

Our algorithm is built on the logdet-FTRL algorithm developed by Zimmert and Lattimore (2022) for high-probability adversarial linear bandits, which lifts the original \(d\)-dimensional problem over the feature space to a \((d+1)(d+1)\) one over the covariance matrix space, with the regularizer being the negative log-determinant function. In our case, we instantiate an individual logdet-FTRL on each action set. The motivation behind Zimmert and Lattimore (2022) to lift the problem to the space of covariance matrix is that it casts the problem to one in the positive orthant, which allows for an easier way to construct the _bonus_ term that is crucial to compensate the variance of the losses, enabling a high-probability bound in their case. In our case, we use the same technique to introduce the bonus term, but the goal is to compensate the _bias_ resulting from the estimation error in the covariance matrix (see Section 3.4). This bias only appears in our contextual case but not in the linear bandit problem originally considered in Zimmert and Lattimore (2022).

As argued previously, we can focus on the learning problem over a fixed action set \(\), and our algorithm operates in the lifted space of covariance matrices \(^{}=\{}(p):p( )\}^{(d+1)(d+1)}\). For this space, we define the lifted loss \(_{t}=0&y_{t}\\ y_{t}^{}&0^{(d+1)(d+1)}\) so that \(}(p),_{t}=_{a p }[a^{}y_{t}]=(p),y_{t}\) and thus the loss value in the lifted space (i.e., \(}(p),_{t}\)) is the same as that in the original space (i.e., \((p),y_{t}\)).

In each round \(t\), the FTRL on \(\) outputs a lifted covariance matrix \(_{t}^{}^{}\) that corresponds to a probability distribution \(p_{t}^{}()\) such that \(}(p_{t}^{})=_{t}^{}\) (Line 2 and Line 3). Upon receiving \(_{t}\), the learner samples an action from \(p_{t}^{_{t}}\) and the agent constructs the loss estimator \(_{t}\) (Line 5). Similarly to the construction of \(_{t}\), we define the lifted loss estimator \(_{t}=0&_{t}\\ _{t}^{}&0\) which makes \(}(p),_{t}=_{  p}[a^{}_{t}]=(p),_{t}\). The lifted loss estimator, along with the _bonus_ term \(-_{t}_{t}}^{-1}\), is then fed to the FTRL on all \(\)'s. The purpose of the bonus term will be clear in Section 3.4.

In the rest of this section, we use the following notation in addition to those defined in Algorithm 1.

**Definition 1**.: _Define \(x_{t}^{}=_{a p_{t}^{}}[a],\;x_{t}= _{ D}[x_{t}^{}],\;\;H_{t}^{}= _{a p_{t}^{}}[(a-_{t})(a-_{t})^{}],\;H_{t}=_{ D}[H_{t}^{}]\). Let \(p_{t}^{}()\) be the action distribution used by the benchmark policy on \(\), and define \(u^{}=_{a p_{t}^{}}[a],\;u=_{  D}[u^{}],\;^{}=_{a p _{t}^{}}[^{}],\;=_{ D}[ {U}^{}]\). Notice that the \(x_{t}^{}\) and \(u^{}\) defined here is equivalent to the \(_{t}()\) and \(()\) in Eq.1, respectively._

### The construction of loss estimators and feature covariance matrix estimators

Our goal is to make \(_{t}\) in Line 5 an estimator of \(y_{t}\) with controllable bias and variance. If the context distribution is known (as in Neu and Olkhovskaya (2020)), then a standard unbiased estimator of \(y_{t}\) is

\[_{t}=_{t}^{-1}a_{t}_{t},_{t}= _{ D}_{a p_{t}^{}}[aa^{ }].\] (2)

To see its unbiasedness, notice that \([a_{t}_{t}]=_{ D}_{a p_{ t}^{}}[aa^{}y_{t}]\) and thus \([_{t}]=y_{t}\). This \(_{t}\), however, can have a variance that is inversely related to the smallest eigenvalue of the covariance matrix \(_{t}\), which can be unbounded in the worst case. This is the main reason why Neu and Olkhovskaya (2020) does not achieve the optimal bound, and requires the bias-variance-tradeoff techniques in Dai et al. (2023) to close the gap. When the context distribution is unknown but the learner has access to a simulator (Liu et al., 2021; Dai et al., 2023; Sherman et al., 2023; Kong et al., 2023), the learner can draw free contexts to estimate the covariance matrix \(_{t}\) up to a very high accuracy without interacting with the environment, making the problem close to the case of known context distribution.

Challenges arise when the learner has no knowledge about the context distribution and there is no simulator. In this case, there are two natural ways to estimate the covariance matrix under the current policy. One is to draw new samples from the environment, treating the environment like a simulator. This approach is essentially taken by all previous work studying linear models in the "S-A" category. However, this is very expensive, and it causes the simulator-equipped bound \(\) in Dai et al. (2023)to deteriorate to the simulator-free bound \(T^{}{{}}}\) at best (see Appendix G for details). The other is to use the contexts received in time \(1\) to \(t\) to estimate the covariance matrix under the policy at time \(t\). This demands a very high efficiency in reusing the contexts samples, and existing ways of constructing the covariance matrix and the accompanied analysis by Dai et al. (2023); Sherman et al. (2023) are insufficient to achieve the near-optimal bound even with context reuse. This necessitates our tighter construction of the covariance matrix estimator and tighter concentration bounds for it.

Our construction of the loss estimator (Line 5) is

\[_{t}=_{t}^{-1}(a_{t}-_{t})_{t} _{t}=_{_{t}}_{a  p_{t}^{}}[(a-_{t})(a-_{t})^{}]+ _{t}I\] (3)

where \(_{t}=\{_{1},_{2},, _{t-1}\}\), \(_{t}=_{_{t}},_{a p_{t}^{ }}[a]\), and \(_{t}=}(d^{3}/t)\). Comparing Eq. (3) with Eq. (2), we see that besides using the empirical context distribution \(_{t}\) in place of the ground truth \(D\) and adding a small term \(_{t}I\) to control the smallest eigenvalue of the covariance matrix, we also centralize the features by \(_{t}\), an estimation of the mean features under the current policy. The centralization is important in making the bias \(y_{t}-_{t}\) appear in a nice form that can be compensated by a bonus term. The estimator might seem problematic on first sight, because \(p_{t}^{}\) is strongly dependent on \(_{t}\), which rules out canonical concentration bounds. We circumvent this issue by leveraging the special structure of \(p_{t}\) in Algorithm 1, which allows for a union bound over a sufficient covering of all potential policies (Appendix C.3). The analysis on the bias of this loss estimator is also non-standard, which is the key to achieve the near-optimal bound. In the next two subsections, we explain how to bound the _bias_ of this loss estimator (Section 3.3), and how the _bonus_ term can be used to compensate the bias (Section 3.4).

### The bias of the loss estimator

Since the true loss vector is \(y_{t}\) and we use the loss estimator \(_{t}\) in the update, there is a bias term emerging in the regret bound at time \(t\):

\[_{t}[ x_{t}^{_{0}}-u^{_{0}},y_{t}- _{t}]=_{t}[ x_{t}-u,y_{t}-_{t }]=_{t}[(x_{t}-u)^{}(I-_{t}^ {-1}(a_{t}-_{t})a_{t}^{})y_{t}]\]

where definitions of \(x_{t}^{},u^{},x_{t},u\) can be found in Definition 1, and we use the definition of \(_{t}\) in Eq. (3) in the last equality. Now taking expectation over \(_{t}\) and \(a_{t}\) conditioned on \(_{t-1}\), we can further bound the expectation in the last expression by

\[(x_{t}-u)^{}(I-_{t}^{-1}H_{t})y_{t}-(x _{t}-u)^{}_{t}^{-1}(x_{t}-_{t})_{t}^{ }y_{t}\] \[\|x_{t}-u\|_{_{t}^{-1}}\|(_{t}-H_{t})y_{t} \|_{_{t}^{-1}}+\|x_{t}-u\|_{_{t}^{-1}}\|x_{t}-_{t}\|_{ _{t}^{-1}}\] (4)

(see Definition 1 for the definition of \(H_{t}\)). The two terms \(\|(_{t}-H_{t})y_{t}\|_{_{t}^{-1}}\) and \(\|x_{t}-_{t}\|_{_{t}^{-1}}\) in Eq. (4) are related to the error between the empirical context distribution \(_{t}=\{_{1},,_{t-1}\}\) and the true distribution \(D\). We handle them through novel analysis and bound both of them by \(}/t}\). See Lemma 13, Lemma 14, Lemma 18, and Lemma 19 for details. The techniques we use in these lemmas surpass those in Dai et al. (2023); Sherman et al. (2023). As a comparison, a similar term as \(\|(_{t}-H_{t})y_{t}\|_{_{t}^{-1}}\) is also presented in Eq. (16) of Dai et al. (2023) and Lemma B.5 of Sherman et al. (2023) when bounding the bias. While their analysis uses off-the-shelf matrix concentration inequalities, our analysis expands this expression by its definition, and applies concentration inequalities for _scalars_ on individual entries. Overall, our analysis is more tailored for this specific expression. Previous works ensure that this term can be bounded by \(()\) after collecting \((^{-2})\) new samples (Lemma 5.1 of Dai et al. (2023) and Lemma B.1 of Sherman et al. (2023)), we are able to bound it by \((1/)\) only using \(t\) samples that the learner received up to time \(t\). This essentially improves their \((^{-2})\) sample complexity bound to \((^{-1})\). See Appendix G for detailed comparison with Dai et al. (2023) and Sherman et al. (2023).

Now we have bounded the regret due to bias of \(_{t}\) by the order of \(/t}\|x_{t}-u\|_{_{t}^{-1}}\). The next problem is how to mitigate this term. This is also a problem in previous work (Luo et al., 2021; Dai et al., 2023; Sherman et al., 2023), and it has become clear that this can be handled by incorporating _bonus_ in the algorithm.

### The bonus term

To handle a bias term in the form of \(\|x_{t}-u\|_{_{t}^{-1}}\), we resort to the idea of _bonus_. To illustrate this, suppose that instead of feeding \(_{t}\) to the FTRLs, we feed \(_{t}-b_{t}\) for some \(b_{t}\). Then this would give us a regret bound of the following form:

\[ =[_{t=1}^{T} x_{t}-u,_{t}-b_{t} ]+[_{t=1}^{T} x_{t}-u,y_{t}-_{t} ]+[_{t=1}^{T} x_{t}-u,b_{t}]\] \[}(d^{2})+[ _{t=1}^{T}}{t}}\|x_{t}-u\|_{_{t}^{-1}}] +[_{t=1}^{T} x_{t}-u,b_{t}]\] (5)

where we assume that FTRL can give us \(}(d^{2})\) bound for the loss sequence \(_{t}-b_{t}\). Our hope here is to design a \(b_{t}\) such that \( x_{t}-u,b_{t}\) provides a negative term that can be used to cancel the bias term \(/t}\|x_{t}-u\|_{_{t}^{-1}}\) in the following manner:

\[+=_{t=1}^{T}(}{t}}\|x_{t}-u \|_{_{t}^{-1}}+ x_{t}-u,b_{t}) }(d^{2}).\] (6)

which gives us a \(}(d^{2})\) overall regret by 5. This approach relies on two conditions to be satisfied. First, we have to find a \(b_{t}\) that makes 6 hold. Second, we have to ensure that the FTRL algorithm achieves a \(}(d^{2})\) bound under the loss sequence \(_{t}-b_{t}\).

To meet the first condition, we take inspiration from Zimmert and Lattimore (2022) and lift the problem to the space of covariance matrix in \(^{(d+1)(d+1)}\). Considering the bonus term \(_{t}}_{t}^{-1}\) in the lifted space, we have

\[_{t}-,_{t}}_{t}^{-1}=_{ t}(_{t}}_{t}^{-1})-_{t} (}_{t}^{-1})\] (7)

Using 17 and Corollary 22, we can upper bound 7 by \((d_{t})-}{4}\|u-_{t}\|_{ _{t}^{-1}}^{2}\). This gives

\[+ _{t=1}^{T}(}{t}}\|x_{t}-u\|_{_{t}^{-1}}+d_{t}-}{4}\|_{t}-u\|_{_{t}^{-1}}^{2})\] \[}(d^{2})+_{t=1}^{T}}{t}}\|x_{t}-_{t}\|_{_{t}^{-1}}+_{t=1}^{T} (}{t}}\|_{t}-u\|_{_{t}^{-1}}-}{4}\|_{t}-u\|_{_{t}^{-1}}^{2}).\]

Using 18 to bound the second term above by \(}(_{t}d^{3}/t)=}(d^{3})\), and AM-GM to bound the third term by \(}(_{t}d^{3}/(t_{t}))=}(d ^{2})\), we get 6, through the help of lifting.

To meet the second condition, we have to analyze the regret of FTRL under the loss \(_{t}-b_{t}\). The key is to show that the bonus \(_{t}}_{t}^{-1}\) introduces small _stability term_ overhead. Thanks to the use of the logdet regularizer and its self-concordance property, the extra stability term introduced by the bonus can indeed be controlled by the order \(\). The key analysis is in 27.

Previous works rely on exponential weights (Luo et al., 2021; Dai et al., 2023; Sherman et al., 2023) rather than logdet-FTRL, which comes with the following drawbacks. 1) In Luo et al. (2021), Sherman et al. (2023) where exponential weights is combined with standard loss estimators, the bonus introduces large stability term overhead. Therefore, their bound can only be \(T^{}{{3}}}\) at best even with simulators. 2) In Dai et al. (2023) where exponential weights is combined with magnitude-reduced loss estimators, the loss estimator for action \(a\) can no longer be represented as a simple linear function \(a^{}_{t}\). Instead, it becomes a complex non-linear function. This restricts the algorithm's potential to leverage linear optimization oracle over the action set and achieve computational efficiency.

### Overall regret analysis

With all the algorithmic elements discussed above, now we give a formal statement for our regret guarantee and perform a complete regret analysis. Our main theorem is the following.

**Theorem 2**.: _Algorithm 1 ensures \((d^{2} T)\)._

Proof sketch.: Let \(_{0}\) be drawn from \(D\) independently from all the interaction history between the learner and the environment. Recalling the definitions in Definition1, we have

\[=[_{t=1}^{T} a_{t}-u^{A_{ t}},y_{t}]=[_{t=1}^{T}_{t}^{ _{t}}-^{_{t}},_{t}]= [_{t=1}^{T}_{t}^{_{0}}-^{_{0 }},_{t}]\] \[[_{t=1}^{T}_{t}^ {_{0}}-^{_{0}},_{t}-_{t} ]}_{}+[_{t=1}^{T} _{t}^{_{0}}-^{_{0}},_{t}}_{t}^{-1}]}_{}+ [_{t=1}^{T}_{t}^{_{0}}-^{_{0 }},_{t}-_{t}}_{t}^{-1}]}_{ }\]

Each term can be bounded as follows:

* \((d^{2} T)+_{t=1}^{T} _{t}\|u-x_{t}\|_{_{t}^{-1}}^{2}\) (discussed in Section3.3).
* \((d^{2} T)-_{t=1}^{T} _{t}\|u-x_{t}\|_{_{t}^{-1}}^{2}\) (discussed in Section3.4).
* \((d^{2} T)\).

Combining all terms gives the desired bound. The complete proof is provided in AppendixD. 

### Handling Misspecification

In this subsection, we show how our approach naturally handles the case when the expectation of the loss cannot be exactly realized by a linear function but with a misspecification error. In this case, we assume that the expectation of the loss is given by \([_{t}|a_{t}=a]=f_{t}(a)\) for some \(f_{t}:^{d}[-1,1]\), and the realized loss \(_{t}\) still lies in \([-1,1]\). We define the following notion of misspecification (slightly more refined than that in Neu and Olkhovskaya (2020)):

**Assumption 1** (misspecification).: \(_{t=1}^{T}_{y_{2}^{d}}_{(D)}_{a}(f_{t}(a)- a,y)^ {2}}\)_._

Based on previous discussions, the design idea of Algorithm1 is to 1) identify the bias of the loss estimator, and 2) add necessary bonus to compensate the bias. When there is misspecification, this design idea still applies. The difference is that now the loss estimator \(_{t}\) potentially has more bias due to misspecification. Therefore, the bias becomes larger by an amount related to \(\). Consequently, we need to enlarge bonus (raising \(_{t}\)) to compensate it. Due to the larger bonus, we further need to tune down the learning rate \(_{t}\) to make the algorithm stable. Overall, to handle misspecification, when \(\) is known, it boils down to using the same algorithm (Algorithm1) with adjusted \(_{t}\) and \(_{t}\). The case of unknown \(\) can be handled by the standard meta-learning technique _Corral_(Agarwal et al., 2017; Foster et al., 2020; Luo et al., 2022). We defer all details to AppendixE and only state the final bound here.

**Theorem 3**.: _Under misspecification, there is an algorithm ensuring \(}(d^{2}+ T)\), without knowing \(\) in advance._

## 4 Linear EXP4

To tighten the \(d\)-dependence in the regret bound, we can use the computationally inefficient algorithm EXP4(Auer et al., 2002). The original regret bound for EXP4 has a polynomial dependence on the number of actions, but here we take the advantage of the linear structure to show a bound that only depends on the feature dimension \(d\). The algorithm is presented in Algorithm2.

To run Algorithm 2, we restrict ourselves to a finite policy class. The policy class we use in the algorithm is the set of linear policies defined as

\[=\{_{}:\ ,\ \ _{}()= *{argmin}_{a}a^{}\}\] (8)

where \(\) is an \(1\)-net of \([-T,T]^{d}\). The next theorem shows that this suffices to give us near-optimal bounds for our problem. The proof is given in Appendix F.

**Theorem 4**.: _With \(=2d\) and \(=\), Algorithm 2 with the policy class defined in Eq. (8) guarantees \(=(d)\)._

Note that this result technically also holds in the "A-A" category with respect to the policy class defined in Eq. (8). However, this policy class is _not_ necessarily a sufficient cover of all policies of interest when the contexts and losses are adversarial.

## 5 Conclusions

We derived the first algorithm that obtains \(\) regret in contextual linear bandits with stochastic action sets in the absence of a simulator or prior knowledge on the distribution. As a side result, we obtained the first computationally efficient \((d)\) algorithm for adversarial sleeping bandits with general stochastic arm availabilities. We believe the techniques in this paper will be useful for improving results for simulator-free linear MDPs as well.