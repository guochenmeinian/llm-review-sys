# CoDrug: Conformal Drug Property Prediction with Density Estimation under Covariate Shift

CoDrug: Conformal Drug Property Prediction with Density Estimation under Covariate Shift

Siddhartha Laghuvarapu

Department of Computer Science

University of Illinois Urbana-Champaign

Urbana, IL 61801

_s1160@illinois.edu_

Zhen Lin

Department of Computer Science

University of Illinois Urbana-Champaign

Urbana, IL 61801

_zhenlin4@illinois.edu_

Jimeng Sun

Department of Computer Science

Carle Illinois College of Medicine

University of Illinois Urbana-Champaign

Urbana, IL 61801

_jimeng@illinois.edu_

###### Abstract

In drug discovery, it is vital to confirm the predictions of pharmaceutical properties from computational models using costly wet-lab experiments. Hence, obtaining reliable uncertainty estimates is crucial for prioritizing drug molecules for subsequent experimental validation. Conformal Prediction (CP) is a promising tool for creating such prediction sets for molecular properties with a coverage guarantee. However, the exchangeability assumption of CP is often challenged with covariate shift in drug discovery tasks: Most datasets contain limited labeled data, which may not be representative of the vast chemical space from which molecules are drawn. To address this limitation, we propose a method called CoDrug that employs an energy-based model leveraging both training data and unlabelled data, and Kernel Density Estimation (KDE) to assess the densities of a molecule set. The estimated densities are then used to weigh the molecule samples while building prediction sets and rectifying for distribution shift. In extensive experiments involving realistic distribution drifts in various small-molecule drug discovery tasks, we demonstrate the ability of CoDrug to provide valid prediction sets and its utility in addressing the distribution shift arising from de novo drug design models. On average, using CoDrug can reduce the coverage gap by over 35% when compared to conformal prediction sets not adjusted for covariate shift.

+
Footnote †: The code associated with the paper is available at https://github.com/siddharthal/CoDrug/

## 1 Introduction

Drug discovery is a challenging and complex task, with a high failure rate and limited understanding of the chemical and biological processes involved. These contribute to making drug discovery an extremely costly and time-consuming endeavor. Recently, advances in deep learning have aimed to reduce the cost of drug discovery by proposing AI methods for developing accurate _property prediction models_ and _De Novo drug design models_:

* _Property prediction models_ aim to aid the laborious and expensive stages of drug discovery by building accurate supervised learning models that take in a drug representation as input and output a target property .

* _De novo_ drug design models, on the other hand, aim to discover new drug molecules that satisfy a set of pharmaceutical properties [3; 4; 5; 6].

With the high cost and significance of drug discovery, it is essential to have accurate and reliable uncertainty estimates in supervised learning models for property prediction. By providing set-valued or interval-valued estimates instead of solely relying on point estimates, uncertainty estimation enables more informed decision-making and reduces the risk of failures, making the drug discovery process more efficient. Conformal prediction (CP), pioneered by , offers a solution to uncertainty quantification for complex models like neural networks, by constructing provably valid prediction sets1 in supervised learning models. Its application to drug property prediction has also been explored for various drug discovery tasks [8; 9; 10].

A crucial assumption in the CP framework is that the test samples are exchangeable with the holdout set used to calibrate the algorithm. In drug discovery, there is often a limited amount of available training or validation data. Furthermore, _de novo_ drug design models or screening datasets sample molecules from a large chemical space, making the exchangeability assumption invalid.

In this paper, we deal with a situation where the training data originates from a distribution, \(P(X)\), while the test data comes from a different distribution, \(P^{test}(X)\). However, in both cases, the molecular properties, determined by the conditional distribution \(P(Y|X)\), remain the same as they are governed by nature and unaffected by shifts in the input distribution, assuming testing parameters are stable. This is referred to as _covariate shift_. Although recent research in Conformal Prediction (CP)  suggests a method for correcting covariate shift, accurately estimating the precise level of covariate shift remains a practical challenge.

This paper proposes a novel and practical method for **Con**formal **Drug** property prediction, dubbed as CoDrug, to improve coverage in the conformal prediction framework under covariate shift. We address the problem of non-exchangeability by quantifying the underlying covariate shift at test time and leverage recent advances in conformal prediction to obtain prediction sets. Further, we demonstrate applying CoDrug to obtain valid uncertainty estimates w.r.t. a target property on molecules sampled from de novo drug design models. We summarize our main contributions below:

* We propose a novel approach to create prediction sets for drug property prediction, dubbed as CoDrug. Using kernel density estimates (KDE) and recent advances in CP, CoDrug corrects for covariate shift at test time and creates prediction sets whose coverage rate is closer to the target.
* We show that the kernel density estimates are consistent, which means that asymptotically, the covariate shift is precisely adjusted for, and the coverage guarantee is recovered.
* molecular scaffold splitting and molecular fingerprint splitting. Our experiments show that CoDrug effectively reduces the gap between actual and target coverage for prediction sets, with an average enhancement of up to 35% compared to the conformal prediction method without covariate shift adjustment. Additionally, in our experiments on molecules generated by de novo drug design models, we observe a 60% reduction in the coverage gap on average.

## 2 Related Work

Recently, deep learning techniques have been extensively studied for their potential in drug discovery, specifically in developing accurate predictive and generative models. This led to various architectures for predicting drug properties from SMILES/SELFIES strings , molecular graph representations[2; 1] and self-supervised learning . Another area of research focused on building generative models to discover novel molecules using variational autoencoder [5; 6] and reinforcement learning [3; 4; 14].

Furthermore, several methods have been proposed for addressing uncertainty quantification in molecule property prediction, utilizing various Bayesian techniques . Recently, conformal prediction methods have gained increasing attention for drug property prediction [8; 9; 10; 16; 17]. However, these studies primarily focus on generating efficient conformal predictors, without taking into account distribution shifts. Although several benchmarking datasets [18; 19] and methods  have been developed for drug property prediction under distribution shift, the problem of uncertainty quantification under distribution shift is still open.

Recent advancement in conformal prediction recovers the coverage guarantee for conformal prediction under known covariate shift .  built upon  and proposed the Feedback Covariate Shift(FCS) method for the task of protein design. In practice, one cannot know the exact densities to measure the covariate shift. Like , we also leverage , but a key difference is that the training density is well-defined in  but unknown in ours, requiring us to estimate it. Additionally, our focus diverges from  as we concentrate on molecule property prediction rather than protein design.

## 3 Preliminaries

Reliable estimation of drug properties is crucial for identifying potential drug candidates. Many essential drug properties, such as toxicity, efficacy, drug-drug interactions etc. are formulated as classification problems. Consider a classification task, with each data point \(Z=(X,Y)^{d}[K]\) (\([K]=\{0,1,2,...,K-1\}\)). For instance, in Fig. 1(a), we seek to construct prediction sets for the problem of solubility classification. (Note that in practice, most drug discovery tasks are formulated as binary classification problems, with \(K=2\), but we present the general form of the methodologies.) While building an accurate base classifier (\(f\)) is important, we usually would like more than a point estimate of the solubility of the molecule, but also some "confidence level". This could be encoded in the form of a prediction set denoted as \((X)[K]\).

The main goal we seek in such prediction sets is valid coverage: Given a target (e.g. 90%), we would like to construct a set-valued prediction (Fig. 1(a)) such that, if a molecule is water soluble, this prediction set will include the label "water soluble" with at least 90% probability. Formally, given \(1-(0,1)\), and a new test molecule \((X_{N+1},Y_{N+1})\), we would like \(\) to be \(1-\) valid:

\[\{Y_{N+1}(X_{N+1})\} 1-.\] (1)

Conformal Prediction(CP) framework enables us to achieve such validity in Eq. (1). We will expand the details in Section 4.2. Remarkably, the only requirement of CP is a hold-out calibration set where the base classifier \(f\) is not trained on2.

One critical assumption for typical CP methods is that the test and calibration data are i.i.d (or exchangeable) which is rarely realistic in drug discovery tasks. On the other hand, although the distribution of molecules \(X\) changes from calibration to test time, the conditional distribution \(Y|X\) is unlikely to change as the molecular properties are determined by nature and remain the same under similar experimental conditions. Formally, if we denote our calibration set as \(\{(X_{i},Y_{i})\}_{i=1}^{N}\) and the test point as \((X_{N+1},Y_{N+1})\), we have:

\[ i[N],(X_{i},Y_{i})P^{cal} =P_{X}^{cal} P_{Y|X}^{cal}\] (2) \[(X_{N+1},Y_{N+1}) P^{test} =P_{X}^{test} P_{Y|X}^{cal}.\] (3)

It is important to note that the test distribution \(P^{test}\) maintains the same conditional distribution \(P_{Y|X}^{cal}\) as the calibration distribution, a phenomenon known as _covariate shift_. This shift is prevalent in _de novo_ drug design models, which require navigating a vast chemical space to pinpoint optimal molecules for a specific goal. However, in many drug discovery tasks, the datasets typically contain only a few thousand data points, representing a limited chemical space. Thus, when models trained on these smaller datasets are used on molecules drawn from the broader molecular space, they inevitably encounter covariate shift. Next we will lay out the exact details of constructing prediction sets with the presence of covariate shifts for supporting drug discovery applications.

## 4 CoDrug Method

### Overview

In the subsequent subsections, we describe the three primary components of CoDrug. In Section 4.2, we first provide a brief overview of inductive conformal prediction, presenting a method for constructing valid prediction sets in scenarios both without and with distribution shifts, presuming oracle access to the unknown distributions \(P_{X}^{test}\) and \(P_{X}^{cal}\). Next, in Section 4.3, we present the details of the training aspects of the base energy-based classifier, emphasizing additional regularization using unlabeled data to enhance its capability to model varying molecule distributions. Finally, in Section 4.4, we employ kernel density estimation (KDE) on the embeddings or logits of the energy model trained in Section 4.3 to estimate the unknown distributions \(P_{X}^{test}\) and \(P_{X}^{cal}\), and rectify covariate shift usingSection 4.2. As KDE is consistent, we regain the coverage guarantee asymptotically. Together, these elements constitute the pipeline depicted in Fig. 1.

### Conformal Prediction Set

In this section, we will explain how to use conformal prediction to construct valid prediction sets. We will start with the case without covariate shift, and then explain how to correct for covariate shift.

#### 4.2.1 Conformal Prediction without Covariate Shift

Conformal prediction, pioneered by , is a powerful framework to construct prediction sets with the guarantee in Eq. (1). We assume that a base classifier \(f\) is trained on a training set \(_{train}\), and we have a hold-out calibration set \(_{cal}\). To simplify notation, we will denote the calibration set as \(\{Z_{i}\}_{i=1}^{N}\) and the test point of interest as \(Z_{N+1}\). We will also abuse the notation to use \(\) to denote both the empirical calibration/test set as well as the underlying distribution. Note that we ignored the training samples because they are no longer used after the classifier \(f\) is trained.

We first introduce some useful definitions: (empirical) CDF and quantile function.

**The cumulative distribution function (CDF)**\(F\) of a set of values \(\{v_{i}\}_{i=1}^{N}\) is defined as:

\[F_{\{v_{i}\}_{i=1}^{N}}:=}{{N}}_{i=1}^{N}_{v_{i}}, _{v}(x):=\{x v\}\] (4)

**The quantile function** with respect to a CDF \(F\) is:

\[Quantile(;F):=\{x:F(x)\}\] (5)

Given a target coverage level \(1-(0,1)\), the Mondrian inductive conformal prediction set (Mondrian ICP) is given by:

\[(X_{N+1}) :=\{y:1-p_{y}^{f}(X_{N+1}) t\}\] (6) \[t :=Quantile(1-;F_{\{1-p_{Y_{i}}^{f}(X_{i})\}\{\}}).\] (7)

where, \(p_{y}^{f}(x)\) corresponds to the softmax output of class \(y\) from model \(f\). Here, \(\{v_{i}\}_{i=1}^{N}\) are defined by \(v(x_{i},y_{i})=1-p_{y_{i}}^{f}(x_{i})\), which are called "nonconformity scores"  and measure how "anomalous"

Figure 1: **CoDrug overview: (a) A depiction of the conformal prediction (CP) framework. A valid prediction set includes the true label of the input molecule. (b) Standard procedure for computing quantiles from the calibration set when the test set is exchangeable. The calibration set’s “nonconformity” scores are sorted, and the (1-\(\)) quantile serves as the threshold for the conformal prediction set. (c),(d),(e) describe the CoDrug pipeline. (c) Training an Energy-based model using labeled and unlabeled data. (d) Density estimation: The model from (c) is used to estimate the density of the calibration and test sets. (e) Calibration under covariate shift: First, likelihood ratios \(w_{i}\) are computed from the densities in (d). Then, Quantile is computed in a weighted fashion. Note how the quantile at \(=0.3\) is shifted from 0.64 (in (b)) to 0.71 to account for the distribution shift.**

a point \(z=(x,y)\) is with respect to other points from this distribution. Intuitively, we assign to each molecule a score using the same rule using \(f\), which is trained on a separate data split \(_{train}\). Now, we choose a threshold \(t\) that is larger than \(1-\) (e.g., 90%) of the molecules. Because of our i.i.d. assumption, if we sample another molecule \(Z_{N+1}\) from the same distribution, we expect its score to be lower than this threshold with a probability of \(1-\). For eg, in Fig. 1(b), notice how the threshold \(t\), is computed as the value of the \(Quantile\) function at \(=0.7\) (\(t=0.64\) in this case). We formally state the coverage guarantee _without covariate shift_, in the following theorem:

**Theorem 4.1**.: _Assume i.i.d. \(\{(X_{i},Y_{i})\}_{i=1}^{N+1}\). The \(\) in Eq. (6) satisfies:_

\[\{Y_{N+1}(X_{N+1})\} 1-.\] (8)

**Remarks**: Theorem 4.1 is a result of classical Mondrian inductive conformal prediction . In fact, in the classification setting, instead of the i.i.d. assumption, one could make a slightly milder assumption that data are _exchangeable_ within each class.

#### 4.2.2 Conformal Prediction with Covariate Shift

While Theorem 4.1 provides a nice first step, the i.i.d. assumption poses a significant limitation in drug discovery. As mentioned in Eq. (3), the distributions of \(X\) on \(^{test}\) and \(^{cal}\) can differ. In Eq. (7), we used the empirical CDF \(F\) (Eq. (4)) to choose the threshold \(t\). This is because of our i.i.d. assumption: a particular molecule type appears with equal probability/density in both the calibration and test sets. This is no longer the case with covariate shift, which means our \(F\) needs to account for such difference in \(P_{X}\).

Formally, recall that \(P_{X}^{cal}\) and \(P_{X}^{test}\) represent the density of the molecule \(X\) for the calibration and test sets. We will assign a weight to each molecule \(x\) that is proportional to the density/likelihood ratio \(^{test}}}{{dP_{X}^{cal}}}\) in the empirical CDF, leading to:

\[F_{x}^{w} :=w(x_{N+1})_{w}+_{i[N]}w(x_{i})_{1-p_{y_{i}}^{ f}(x_{i})}}{{W}}\] (9) \[w(x^{}) :=^{test}(x^{})}}{{dP_{X}^{cal}}}(x^{ }), x^{}\] (10)

\(W=_{i=1}^{N+1}w(x_{i})\) is just a normalizing factor. The subscript \(x_{N+1}\) is used to highlight that our updated CDF now depends on the test molecule \(x_{N+1}\) through the weights. Here, \(w(x^{})\) could be viewed as a likelihood ratio, and is crucial in adjusting for the covariate shift. For eg, in Fig. 1(e). Notice how the values of \(w(x_{i})\) depend on the densities \(P_{X}^{cal}\) and \(P_{X}^{test}\). In the figure, the value of weighted \(Quantile\) at \(=0.3\) or the threshold \(t\) is shifted from 0.64 to 0.71 to account for the shift. We formally state the modified theorem from  that recovers the coverage guarantee under covariate shift for Mondrian ICP:

**Theorem 4.2**.: _[_11_]_ _Assume that \(_{X}\) is absolutely continuous with respect to \(P_{X}\). For any \((0,1)\), let \(F^{w}\) be defined as in Eq. (9), and_

\[(x)=\{y:1-p_{y}^{f}(x) Quanti(1-;F_{x}^{w})\}\] (11)

_Then,_

\[\{Y_{N+1}(X_{N+1})\} 1-.\] (12)

However, in practice, both \(P_{X}^{test}\) and \(P_{X}^{cal}\) are unknown, rendering Theorem 4.2 impractical. In section 4.4, we will provide a viable way to estimate \(P_{X}^{test}\), and recover the guarantee asymptotically (namely with large calibration and test sets). In the following section, we will delve into our training methodology, which harnesses unlabeled data to effectively model varying molecular distributions.

### CoDrug Training Methodology

CoDrug handles distribution shift by proposing an energy-based model formulation . The core idea behind an energy-based model is to construct a function \(E\) that maps an input \(x\) to a scalar value, known as energy. A collection of energy values can be transformed into a probability density function \(p(x)\) through the Gibbs distribution

\[p(y|x)=}}{{e^{-E(x)/T}}}\] (13)

Consider a discriminative neural network \(f\) used in a \(K\) class classification setting. \(f(x)\) maps an input \(x\) into \(K\) real-valued scalars, which are used to derive a conditional class-wise probability:

\[p(y|x)=(x)/T}}}{{_{y^{}}^{K}{e^{f_{y^{}}(x )/T}}}}\] (14)where, \(f_{y}(x)\) refers to the \(y^{th}\) logit of the classifier \(f(x)\). In this setting, the energy function \(E(x)\) can be expressed in terms of the denominator of the softmax probabilities in Eq. (14).

\[E(x;f)=-T_{y^{}}^{K}e^{f_{y^{}}(x)/T}\] (15)

Directly using embeddings from a model trained on labeled data may not yield reliable density estimates, as the model lacks knowledge of data outside the training distribution. To overcome this, we co-train the model with unlabeled molecule data. This aids the model \(f\) in effectively mapping molecules with distribution shifts to a distinct embedding space. We follow , and use an extra regularization term in the loss function to ensure energy separation between in-distribution and out-of-distribution data. The objective function is defined as follows:

\[_{}_{(x,y)_{in}}[-(p_{y}^{f}(x))]+  L_{energy}\] (16)

where \(p_{y}^{f}(x)\) refers to the softmax outputs of the classification model \(f\) for class \(y\), and \(_{in}\) is the in-distribution training data for which labels are available. The training objective is combined with an additional term \(L_{energy}\) given by:

\[L_{energy}=_{(x_{in},y)_{in}}((0,(E(x_{in})-m_{ in}))^{2})+_{(x_{out},y)_{out}}((0,(m_{out}-E(x_{out}))) ^{2})\] (17)

where \(_{out}\) refers to the subset of the unlabelled that is out-of-distribution (OOD). The objective of this term is to enforce a margin of separation between the training samples and the OOD data using the hyper-parameters \(m_{in}\), \(m_{out}\). Particularly, one term penalizes the model if the energy values for in-distribution data are higher than a certain value, while the other term penalizes if the OOD samples have an energy lower than a certain value. In the next section, we will explain how to use either the latent embedding of \(f\) or the logits to estimate the density and correct for covariate shift.

### Density Estimation

As discussed earlier, we need to estimate \(P_{X}^{test}\) and \(P_{X}^{cal}\) to correct for the covariate shift. We resort to Kernel Density Estimation (KDE), a classical nonparametric method, to estimate the density of arbitrary distributions of molecules. For a set of data \(X_{1},,X_{n}}{{}}\), KDE is given by:

\[_{h}(x;)=(nh)^{-1}_{i=1}^{n}K^{(x-x_{i}/h)},\] (18)

where \(K\) is a fixed non-negative _kernel_ function, and \(h>0\) is a smoothing _bandwidth_. Such KDE estimates have nice asymptotic convergence properties, as formally stated in the following theorem:

**Theorem 4.3**.: _[_25_]_ _Assume that the true density \(p\) is square-integrable and twice differentiable and that its second-order partial derivatives are bounded, continuous, and square-integrable. If \(K\) is spherically symmetric on \(^{d}\), with a finite second moment, and we choose the bandwidth \(h\) such that_

\[_{m}h^{d}m_{m }h 0\] (19)

_then as \(m\),_

\[\|_{h}(x)-p(x)\|_{2}}{{}}0,\] (20)

_where \(}{{}}\) means convergence in probability._

Note that commonly used kernels, such as Gaussian kernel, satisfy the requirements.

Since \(x\) here refers to molecular entities (e.g. SMILES strings), we cannot use a Gaussian kernel directly. Instead, we use embeddings or prediction logits produced by a trained model \(f\) as the input to the kernel. Under the assumption that KDE accurately reflects the true density of the underlying distribution, we could construct kernel density estimators for both the calibration set and test sets (remember that we do not have access to the test labels but have access to the input \(X\)), and use

\[(x):=_{h_{test}}(x;_{test})}}{_{h_ {cal}}(x;_{cal})}\] (21)to replace the unknown \(w\) in Eq. (10), giving us the final prediction set:

\[^{}(x) =\{y:1-p_{y}^{f}(x) Quantile(1-;F_{x}^{})\}\] (22) \[F_{x^{N+1}}^{} :=(x_{N+1})_{}+_{i[N]}( x_{i})_{1-p_{y_{i}}^{f}(x_{i})}}}{W}\] (23)

where \(=_{i=1}^{N+1}(x_{i})\) is a normalizing factor. Here, \(_{h_{test}}(;_{test})\) is constructed using samples from the test data with an optimal bandwidth \(h_{test}\) chosen on the test data via cross-validation, and \(_{h_{cal}}(;_{cal})\) is constructed similarly but on the calibration data. It is clear that, as the number of samples from \(_{cal}\) and \(_{test}\) increases, \(\) converges to \(w\) in Eq. (10), and \(^{}\) recovers the coverage guarantee asymptotically. In practice, recovering asymptotic coverage on a finite amount of data is challenging. However, the coverage tends to approach the target value as we observe in our experiments. The overall procedure for density estimation is depicted in Fig. 1(d).

Algorithm 1 summarizes all the components in Section 4. In Section 5, we will verify the efficacy of CoDrug in property prediction tasks, and molecules sampled from de novo drug design models.

``` Training: Split the dataset into training set \(_{train}\) and calibration set \(_{cal}=\{z_{i}\}_{i=1}^{N}\).  Train a neural net classifier \(f\) on \(_{train}\) by minimizing Eq. (16).  Compute the KDE \(_{h_{cal}}(;_{cal})\) for all points in \(_{cal}\) using Eq. (18). Test Time, for a test set \(_{test}\):  Compute KDE \(_{h_{test}}(;_{test})\) for all points in \(_{cal}\) using Eq. (18).  For any \(x_{N+1}_{test}\), compute \((x)\) and \((x_{i})\) for \(x_{i}_{cal}\).  Construct the prediction set \(^{}(x_{N+1})\) using Eq. (22). ```

**Algorithm 1** Procedure for Property Prediction

## 5 Experiments

In this section, we put our proposed method, CoDrug, to the test on various drug discovery tasks. Section 5.1 describes the datasets used and key implementation details. Section 5.2.1 empirically demonstrates the loss of validity in conformal prediction sets on different drug discovery datasets. Section 5.2.2 shows how the setup improves the validity of the conformal prediction sets. Section 5.3 confirms the utility of CoDrug in de novo drug design. We include additional details on implementation, datasets, and hyperparameters in the appendix.

### Data and Implementation Details

* [leftmargin=*]
* **Splitting Strategies:** To demonstrate the effectiveness of CoDrug under covariate shift, we use two different strategies when creating calibration/test splits. In both strategies, we try to create calibration and test splits that are dissimilar to each other, which is a challenging but realistic setting in drug discovery. We used the DeepChem library for splitting. In **scaffold splitting**, the dataset is grouped based on chemical scaffolds, representing core structures of molecules. The test set and train set consist of different scaffolds. In **fingerprint splitting**, the dataset is partitioned based on Tanimoto similarity of molecular fingerprints . Molecules with the highest dissimilarity in terms of Tanimoto similarity are included in the test set.
* **Datasets:** We use four binary classification datasets for toxicity prediction (AMES, Tox21, ClinTox) and activity prediction (HIV activity), obtained from TDC . To train the Energy based model, we obtained the unlabelled data from the ZINC-250k dataset , a subset of the ZINC that covers a large chemical space. For each dataset and split type, we removed the molecules that are similar to the training (and calibration) set from the unlabelled dataset.
* **Classification Model:** The architecture of our classifier \(f\) is AttentiveFP , a graph neural network-based model. We chose AttentiveFP as it has state-of-the-art results in several drug property prediction tasks. It is trained using the objective function described in Eq. (16).
* QED (quantitative estimate of drug-likeness), JNK3 activity, and GSK3B activity. For building the conformal prediction sets, we chose logP as our target property, assigning values in the range of
[1.0,4.0] a class of Y=1, and Y=0 otherwise (representing the drug-like range ). We obtain the computational oracles from TDC , and generative models from MolOpt package.

### Property Prediction Results

#### 5.2.1 Unweighted conformal prediction (baseline)

In this section, we demonstrate the unpredictable behavior of the unweighted CP method without proper correction under distribution shift. Table 1 shows the results of conformal prediction under various distribution shift conditions. "Random" refers to the ideal/unrealistic scenario where the test and calibration samples are split randomly (aka. no distribution shift). "Scaffold" and "Fingerprint" denote scenarios in which there is a distribution shift between the test and training data outlined in the Methods section. In all scenarios, 15% training set is held out for calibration, and prediction sets are calculated using the algorithm described in Algorithm 1 without any correction.

From Table 1, we observe that the Random configuration demonstrates little loss in coverage and coverage decreases under distribution shifts (Scaffold and Fingerprint). But for fingerprint and scaffold split, unweighted CP failed to provide target coverage and exhibit unpredictable behavior. For instance, at \(=0.2\), under fingerprint split, Unweighted has a coverage of 0.34 against a target coverage of 0.8 for the AMES dataset, while achieving a very different coverage of 0.77 with scaffold split on the same dataset.

#### 5.2.2 Weighted conformal prediction improves coverage

In Table 2, we present the benefits of using weighted CP via CoDrug. The table depicts results from conformal prediction using 3 different schemes.

* CoDrug **(Energy):** This variant of CoDrug uses weights computed from KDE on the prediction logits of the trained EBM, as described in Section 4.3.
* CoDrug **(Feature):** This variant of CoDrug builds the KDE instead on the features extracted from the penultimate layer of the trained EBM.
* **Unweighted:** Refers to the unweighted prediction conformal prediction (baseline).

In both weighting schemes of CoDrug, we use KDE to estimate densities and find that weighting using energies improves the coverage towards the target coverage \(1-\) in most cases. We notice the highest improvement in the Fingerprint splitting scenario for the AMES(Y=1) category, where the coverage improved from 0.63 to 0.88 (target coverage 0.9). Note that the coverage is "improved" if it is closer to \(1-\) - improvement does not always mean higher coverage, because an unusually high coverage often indicates unpredictable behavior of the underlying model.

While our energy-weighting approach generally improves coverage, there are rare instances where it underperforms compared to the baseline. A prominent example is the ClinTox dataset with \(Y=1\), which sees limited improvement or even a reduction in coverage. This is due to the constraints of the density estimation procedure, which relies on the quantity of available data. Notably, this dataset is the smallest and most imbalanced, with only 19 points in the calibration set for class \(Y=1\).

    & Random & Fingerprint & Scaffold & Random & Fingerprint & Scaffold \\  Dataset &  &  \\  AMES(Y=0) & **0.94** & 1.00 & 0.82 & **0.85** & 1.00 & 0.66 \\ AMES(Y=1) & **0.87** & 0.63 & 0.85 & 0.78 & 0.34 & 0.77 \\ ClinTox(Y=0) & **0.88** & 0.78 & 0.84 & **0.77** & 0.58 & 0.75 \\ ClinTox(Y=1) & 0.82 & 0.80 & 0.97 & **0.78** & 0.73 & **0.81** \\ HIV(Y=0) & **0.90** & **0.93** & **0.91** & **0.80** & 0.89 & **0.81** \\ HIV(Y=1) & **0.89** & **0.84** & **0.87** & **0.80** & **0.72** & 0.73 \\ Tox21(Y=0) & **0.90** & 0.77 & **0.89** & **0.80** & 0.65 & **0.75** \\ Tox21(Y=1) & **0.86** & 0.97 & **0.93** & 0.72 & 0.97 & **0.82** \\   

Table 1: Unweighted CP’s (baseline) coverage under various distribution shifts (or absence thereof) should ideally align closely with the target \(1-\). However, in most datasets with fingerprint and scaffold splits—reflective of more realistic scenarios—the baseline method falls short. Often, substantial deviations in coverage confirm the unpredictability of unweighted CP when exchangeability ceases to apply. Here, values not significantly deviating from \(1-\) at a p-value of 0.05 are highlighted in bold, indicating desirable performance.

Additionally, our results show that using energy weighting leads to better overall coverage than directly weighting the features. This is likely because the energy values are two-dimensional, while the features are an eight-dimensional vector: As the dimension of the feature input to KDE increases, one typically requires more samples to get a high-quality density estimate.

#### 5.2.3 Ablation studies

In this section, we present an analysis of the importance of various components in the CoDrug pipeline - KDE, energy regularization term ( Eq. (16)), and covariate shift correction. In addition to CoDrug (Energy) and Unweighted (baseline) reported in the previous section, we also compare with:

* CoDrug (NoEnergy): We use the same protocol as CoDrug (Energy) but the models are trained without the energy regularization term \(_{energy}\) in Eq. (16).
* Logistic (Energy): In this experiment, the features are same as CoDrug (Energy), but KDE is not used to estimate densities. Instead, the weights \(w(x_{i})\) in Eq. (10), are given by \(_{(x_{i})}/1-_{(x_{i})}\), where \(_{(x_{i})}\) is obtained by fitting a classifier to features in calibration and test sets (suggested by ).

The results are depicted in Table 3. In the table, we compile the "mean absolute coverage deviation" across all the datasets and different random runs from all the experiments reported in Table 2 at different values of \(\) (i.e Mean of \(|-(1-)|\) across the experimental runs). The results reveal that CoDrug (Energy), the proposed method, is closest to the target coverage in almost all different values of \(\). We paid close attention to the "Tail 25%", where we presented the metrics for the worst 25% performing experiments and CoDrug (Energy) outperforms all the other variants in comparison by a substantial margin inducing that all the different components in the CoDrug pipeline are helpful. The mean absolute coverage deviation from the target at \(=0.1\) for CoDrug (Energy) is 0.052, a relative improvement of about 35% over that of Unweighted (0.081).

### Application in de novo drug design.

In this section, we examine CoDrug's application in de novo drug design models, which navigate a large chemical space to find optimized molecules using a computational oracle. After molecule sampling, validating their experimental properties, such as ADMET (Absorption, Distribution, Metabolism, Excretion, Toxicity), is crucial for safety and efficacy. When a machine learning model trained on such properties is available, assessing the uncertainty associated with the predictions before experimental validation is critical. However, note that the distribution of sampled molecules may substantially deviate from the training data, affecting the prediction sets' target coverage from CP.

   Dataset &  &  \\   & CoDrug (Energy) & CoDrug (Feature) & Unweighted (baseline) & CoDrug (Energy) & CoDrug (Feature) & Unweighted (baseline) \\  AMES(Y=0) & **0.93(0.03)** & **0.87(0.03)** & 1.00(0.00) & 0.85(0.02) & **0.89(0.02)** & 0.82(0.01) \\ AMES(Y=1) & 0.88(0.03) & **0.90(0.03)** & 0.63(0.05) & 0.83(0.01) & 0.79(0.01) & **0.85(0.03)** \\ CinToTo(\(\)) & **0.86(0.04)** & 0.76(0.02) & 0.78(0.02) & **0.90(0.03)** & 0.83(0.01) & 0.84(0.00) \\ CinTo(Y=1) & 0.73(0.00) & 0.69(0.08) & **0.80(0.00)** & **0.85(0.03)** & 0.83(0.00) & 0.97(0.04) \\ HIV(Y=0) & **0.89(0.06)** & 0.77(0.07) & 0.93(0.04) & **0.82(0.08)** & 0.82(0.04) & **0.91(0.01)** \\ HIV(Y=1) & **0.92(0.05)** & 0.95(0.03) & 0.84(0.07) & **0.90(0.01)** & **0.90(0.05)** & 0.87(0.03) \\ Tox21(Y=0) & **0.90(0.02)** & 0.80(0.02) & 0.77(0.03) & **0.91(0.03)** & 0.83(0.05) & 0.89(0.05) \\ Tox21(Y=1) & 0.97(0.00) & **0.96(0.01)** & 0.97(0.00) & 0.86(0.05) & **0.91(0.05)** & 0.93(0.03) \\   

Table 2: Coverage of CoDrug and baseline unweighted CP, under different datasets and distribution shifts at \(=0.1\). The realized coverage rate closest to the target coverage \(1-\) (best) is marked in **bold**. The second best coverage (in case better than unweighted) is marked in \(\) and \(\). Results are averaged over 5 random runs. Results for different \(\) values are available in appendix.

   Method &  &  \\   & \(=0.3\) & \(=0.2\) & \(=0.1\) & \(=0.3\) & \(=0.2\) & \(=0.1\) \\  Unweighted (baseline) & 0.157 (0.14) & 0.12 (0.12) & 0.081 (0.07) & 0.347 (0.14) & 0.276 (0.13) & 0.176 (0.08) \\ Logistic(Energy) & 0.123 (0.13) & 0.106 (0.13) & 0.083 (0.14) & 0.315 (0.11) & 0.263 (0.17) & 0.222 (0.22) \\ CoDrug (NoEnergy) & 0.112 (0.11) & 0.083 (0.09) & **0.047 (0.05)** & 0.288 (0.05) & 0.215 (0.08) & 0.112 (0.03) \\ CoDrug (Energy) & **0.104 (0.09)** & **0.079 (0.07)** & 0.052 (0.05) & **0.233 (0.05)** & **0.179 (0.07)** & **0.11 (0.04)** \\   

Table 3: Ablations: Results comparing various versions of the proposed framework. At each \(\), the mean of deviations from target coverage across all the experiments and random seeds is computed (Smaller is better). CoDrug (Energy) has the least deviation from coverage and a substantial difference when only the worst performing 25% of the experiments are considered.

In this section, we demonstrate the application of CoDrug on molecules generated by a de novo drug design model. We consider the de novo drug design model as a black box, that has been optimized w.r.t a certain objective. We experiment with two models - GraphGA  and Reinvent . To predict properties, we compiled a dataset of logP values, as it can be computed cheaply with a computational oracle. We note that in reality, this dataset could correspond to experimental properties like ADMET. However, since it is not feasible to validate these properties for molecules generated from de novo drug design models, we use logP to demonstrate the method. The results of our experiments are depicted in Table 4 at a target alpha value of 0.1. Our proposed method consistently enhances coverage in all cases and exhibits a substantial improvement over the unweighted version. For example, in the "gsk3b+qed" objective, the unweighted version has a coverage of 0.44 against a target of 0.9, whereas our proposed method improves coverage substantially. The mean absolute coverage deviation from the target at \(=0.1\) for CoDrug (Energy) is 0.05, a relative improvement of over 60% on the Unweighted version (0.14).

## 6 Limitations

While we demonstrated that KDE can provide asymptotic coverage guarantees, this may not necessarily translate to improved performance in scenarios with limited sample sizes. In our experiments, we do acknowledge that there are a few instances where the improvement in coverage is limited, where the availability of data is limited. As such, a direction for further research is to explore ways to obtain likelihood ratios that are more data-efficient, particularly in scenarios with smaller calibration sets.

It is worth noting that our current work focuses on addressing the coverage gap in classification tasks, and regression tasks were not included in this study. However, we recognize the importance of uncertainty quantification in regression settings, especially for various critical drug properties represented as regression problems, where our proposed framework can be extended with modifications. Furthermore, our current work primarily focuses on small molecules, yet covariate shift is a common phenomenon in various chemical and biological contexts. While this means that our framework could be more generally applied, obtaining high-quality feature vectors for computing likelihood in different applications remains a challenge that warrants further research.

## 7 Conclusion

We present a new method for uncertainty quantification in drug discovery, CoDrug, that effectively addresses the problem of co-variate shifts in test data. The proposed method involves a combination of three key steps, training an energy-based model for feature extraction and base classification, performing density estimation using KDE, and use the KDE to correct for covariate shift in conformal prediction to recover valid coverage. The results obtained in this study demonstrate the effectiveness of CoDrug in predicting valid conformal prediction sets and its utility in de novo drug design experiments. Our current work is limited to classification tasks in small molecules, but exploring its application to other chemical and biological tasks with covariate shifts is interesting for future work, including adapting the framework for regression tasks.

    &  &  \\  Objective & Y & CoDrug (Energy) & Unweighted & CoDrug (Energy) & Unweighted \\  JNK3+QED & 0 & **0.95 (0.01)** & 0.62 (0.12) & **0.86 (0.0)** & 0.84 (0.01) \\ JNK3+QED & 1 & **0.91 (0.01)** & 0.99 (0.0) & 0.93 (0.01) & **0.89 (0.02)** \\ GSK3b+QED & 0 & **0.81 (0.04)** & 0.44 (0.16) & **0.87 (0.0)** & 0.75 (0.01) \\ GSK3b+QED & 1 & 0.79 (0.08) & **1.0 (0.0)** & **0.98 (0.0)** & 1.0 (0.0) \\ QED & 0 & **0.96 (0.0)** & 0.83 (0.04) & **0.96 (0.0)** & 0.69 (0.1) \\ QED & 1 & **0.92 (0.01)** & 0.84 (0.05) & **0.98 (0.0)** & 0.99 (0.0) \\   

Table 4: Observed coverages on molecules sampled by generative models at \(=0.1\). The realized coverage rate closest to the target coverage(\(1-\)) are marked in bold. For each experiment, a set of 200 points optimized w.r.t. the “Objective” using the generative models GraphGA and REINVENT are sampled. The target property for prediction is logP (1.0 < logP < 4.0 is considered Y=1; Y=0 otherwise ). Using the proposed method improves coverage in almost all scenarios.