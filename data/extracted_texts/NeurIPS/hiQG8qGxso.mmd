# Test-Time Amendment with a Coarse Classifier for Fine-Grained Classification

Kanishk Jain\({}^{1}\), Shyamgopal Karthik\({}^{2}\), Vineet Gandhi\({}^{1}\)

\({}^{1}\)IIIT Hyderabad \({}^{2}\)University of Tubingen

###### Abstract

We investigate the problem of reducing mistake severity for fine-grained classification. Fine-grained classification can be challenging, mainly due to the requirement of domain expertise for accurate annotation. However, humans are particularly adept at performing coarse classification as it requires relatively low levels of expertise. To this end, we present a novel approach for Post-Hoc Correction called Hierarchical Ensembles (HiE) that utilizes label hierarchy to improve the performance of fine-grained classification at test-time using the coarse-grained predictions. By only requiring the parents of leaf nodes, our method significantly reduces _avg. mistake severity_ while improving _top-1_ accuracy on the _iNaturalist-19_ and _tieredImageNet-H_ datasets, achieving a new state-of-the-art on both benchmarks. We also investigate the efficacy of our approach in the semi-supervised setting. Our approach brings notable gains in _top-1_ accuracy while significantly decreasing the severity of mistakes as training data decreases for the fine-grained classes. The simplicity and post-hoc nature of HiE renders it practical to be used with any off-the-shelf trained model to improve its predictions further. The code is available at: [https://github.com/kanji95/Hierarchical-Ensembles](https://github.com/kanji95/Hierarchical-Ensembles)

## 1 Introduction

Over the past decade, large-scale datasets have been instrumental in driving the rapid progress of computer vision/image recognition. However, in settings that require experts to annotate samples, collecting large amounts of labeled data can be prohibitively expensive. _Fine-Grained Visual Classification (FGVC)_ is one such example, where one would need a domain expert to be able to identify the category for a particular sample. However, it can be cheaper to obtain coarse labels for the same samples in these settings. For instance, in Figure 1 while it is evident to spot and identify a butterfly or a bird, one would need a _Lepidopterist_ or an _Ormithologists_ to categorize them as _Junonia Genoveva_ or _Water Ouzel_.

Therefore, one popular research direction in recent times has been to utilize the availability of a larger set of images with coarse labels to improve the performance (i.e. _top-1_ accuracies) of the neural network models on various fine-grained classification benchmarks . Using coarse labels in a classification setting introduces the concept of a label hierarchy, where all the labels in a dataset are connected through some taxonomy. This taxonomy could either be defined from the biological taxonomy, as with many species recognition datasets or could be derived from language-based ontologies such as WordNet . Another mainstream usage of label hierarchies in recent times is to use them to reduce the severity of mistakes committed by various classification models . For instance, mistaking a car for a bus is a better mistake than mistaking a car for a lamppost. This originated from the extensive work on cost-sensitive classification . However, defining pairwise costs can be a tedious process and scales quadratically with the number of classes. Therefore, the recent works have all focused on using the label hierarchy and defining costs automatically based on graph distances (e.g. using the height of the least common ancestor between the predicted and the ground truth class as a proxy for cost). The research in this direction aims to invent methods that atleast retain the accuracies of the backbone baseline models while reducing the severity of mistakes committed by it.

While these two research directions might seem entirely different, they work with the same inputs (i.e., images, labels, and the label hierarchy). Therefore, one would ideally want a method that can use additional coarsely labelled samples to improve the model's accuracy and utilise the label hierarchy to reduce the severity of the mistakes. However, these two goals have been tackled independently. Post-hoc correction methods [4; 7], learning hierarchy-aware features [6; 8; 11] as well as structured embedding spaces [5; 12] have been proposed to reduce the severity of mistakes. Coarse labels have been used for self-training/pseudo-labelling  and with an additional objective to regularise/enhance the training  in the semi-supervised learning frameworks.

In this work, we propose a simple method that is able to bring significant improvements to both of these problems. Our fundamental insight is that models trained for different granularities focus on different aspects. This is also corroborated by recent work [11; 8], which shows that when training a model jointly, coarse-level label prediction exacerbates fine-grained feature learning. Therefore, we train two separate models for the coarse and fine-grained labels. At inference, we employ a modified decision rule, which computes the \(*{argmax}\) over the normalized score of coarse and fine grained predictions. We refer to the proposed strategy as a Hierarchical Ensemble (HiE). We find that employing HiE not only reduces the severity of mistakes, but is able to improve _top-1_ accuracies, which are far more pronounced in the semi-supervised setting. More formally, we make the following contributions:

1. We introduce a simple post-hoc mechanism called Hierarchical Ensembles for incorporating cues from multiple models trained for different granularities of a hierarchy tree.
2. We demonstrate that HiE brings considerable reductions in mistake severity while improving the _top-1 accuracy_. HiE significantly outperforms two competent baselines and six other methods from the prior art and achieves state-of-the-art performance on both _iNaturalist-19_ and _tieredImageNet-H_ benchmarks.
3. We illustrate that HiE brings consistent gains when plugged with existing semi-supervised methods exploiting additional coarse labeled data to improve the fine-grained classification. On _iNaturalist-19_ dataset, HiE combined with Moco  and hierarchical loss , recovers 86% of the underlying performance while using only 0.5% of the fine-grained annotations.

## 2 Related Work

**Learning from Taxonomic Labels:** There has been a long history of utilizing label hierarchies for better classification performance. Sila and Freitas  perform a comprehensive survey of hierarchical classification methods applied to various tasks in different application domains. Broadly, the methods that train hierarchy-aware features can be categorized into three categories a) _Label-Embedding methods_, b) _Hierarchical architectures_, and c) _Hierarchical loss functions_.

The label-embedding methods project the categories into a semantic embedding space [12; 16] instead of viewing them as one-hot encodings and are widely used, especially in zero-shot learning . The semantic embedding space can be derived either from side-information such as class attributes ,

Figure 1: While it is straightforward to identify the images based on their coarse labels, it is challenging to distinguish them based on fine-grained labels without domain expertise.

taxonomies , or from natural language . These can be further enhanced by learning these embeddings on a hypersphere  or hyperbolic spaces [18; 19; 20].

On the other hand, hierarchical loss functions try to modify the training objective to incorporate the label hierarchy [21; 6; 22; 8; 23]. The most common approach here is to introduce a loss at every level in the hierarchy [21; 6; 8]. The challenge in this direction has been to ensure that the coarse-grained task does not hamper the accuracy of the fine-grained classifier [11; 8].

There have also been attempts to modify the architecture based on the label hierarchy. For instance, popular ideas revolve around having branches at different layers of the model , predicting the conditional probabilities at each node in the hierarchy [25; 26], or partitioning the feature space using the levels in the hierarchies .

Most closely related to our work are the works that attempt to incorporate the label hierarchy information post-hoc during inference. Here, traditional Conditional Risk Minimization (CRM)  was applied in this setting by  and recently revisited by . Our proposed approach is also applied during inference; however, it is orthogonal to CRM, and we demonstrate that CRM can be used to improve our results further. Concurrent to our work,  proposed a similar approach to utilize subclasses to improve superclass recognition performance in a post-hoc manner with vision-language models .

**Fine-Grained Visual Classification:** Contrary to the popular image recognition benchmarks, fine-grained visual classification focuses on recognizing the differences between similar-looking categories [30; 31; 32]. Here, methods often focus on learning discriminative features that use local information since most categories share a similar global structure [33; 34; 35]. However, the most related to our work are the ones that use class taxonomies to improve fine-grained classification performance. This is most frequently done in the semi-supervised setting , where one has access to an additional set of weakly labeled samples. Here, approaches have explored incorporating a hierarchical loss on the coarse labels  or using the coarse labels to filter pseudo-labels  in addition to the standard semi-supervised learning techniques of consistency regularization , pseudo-labeling [37; 38], and contrastive learning . Our proposed approach is complementary to these works and can be applied to all of these works to bring additional improvements.

## 3 Methodology

### Problem Formulation

We consider the fine-grained classification task with a label hierarchy \(\) of \(L\) levels defined over the class labels. The leaf nodes of \(\) correspond to fine-grained classes at level \(L\), and the class labels at level \(l<L\) are considered coarse-grained labels. The goal is to use the information from coarse annotations to improve the performance on fine labels. Formally, given a dataset \(=\{(x_{i},y_{i}^{l})\ |\ i=1,2,,N\}\) consisting of \(N\) training images and their respective ground truth labels at level \(l\), where label \(y_{i}^{l}^{l}=\{1,2,...,N_{l}\}\), and \(N_{l}\) corresponds to the number of classes at level \(l\), the task is to train a classifier \(f_{}^{L}: p(^{L})\), on fine-grained class labels by making use of the information available through coarse-grained labels \(^{l<L}\).

### Proposed Method

Suppose we have two classifiers, \(f_{}^{L}\) and \(f_{}^{L-1}\) trained on hierarchy levels \(L,L-1\) with \(\) and \(\) being their parameters, respectively, then for a sample \(x D\), we have, \(_{L}=f_{}^{L}(x)\) and \(_{L-1}=f_{}^{L-1}(x)\) as logits of two classifiers for the input \(x\). We further define:

\[Q=[q_{1},q_{2},...,q_{N_{L}}]=(_{L})\]

\[R=[r_{1},r_{2},...,r_{N_{L-1}}]=(_{L-1})\]We reweigh the fine-grained predictions \(\), given the softmax probabilities \(\) obtained from the coarse-grained classifier:

\[P(i|x,,,)=|x;)}{_{j= 1}^{N_{L}}P(j|x;)P(j_{parent}|x;)},i_{parent}=(i) \]

The denominator is the normalization term, ensuring \(_{i=1}^{N_{L}}P(i|x,)=1\). In the studied setting, \(P(i|x,)\) simplifies to:

\[P(i|x,)=s_{i}= r_{i_{parent}}}{_{j=1}^{N_{L}}q_{ j} r_{j_{parent}}} \]

The main modification we make in the classification strategy at the fine grained level is in the decision rule, which now selects the class that maximizes the normalized score:

\[*{argmax}_{i}P(i|x,) \]

In Figure 2, we illustrate a four-class example, comparing standard cross-entropy predictions with those from our proposed method. We would like to point out that the proposed technique can be interpreted as a Hierarchical Ensemble. Existing literature in neural networks-based ensemble learning has often focused on classifiers trained at a single granularity  and has largely overlooked ensembles with hierarchical context. In this work, we highlight the practical benefits of hierarchical ensembles for improving the fine-grained classification task by choosing the simplest possible method of multiplying the predictions at different levels of hierarchy.

We show that if we make a correct prediction at the coarse level, the proposed Hierarchical Ensemble (HiE) is guaranteed to improve the downstream predictions at the fine-grained classification task.

**Theorem 3.1**.: _Assuming, \(Q=[q_{1},q_{2},...,q_{N_{L}}]\) and \(R=[r_{1},r_{2},...,r_{N_{L-1}}]\) are the predictions obtained at the fine and coarse grained labels for a given input \(x\), such that \(_{i=1}^{N_{L}}q_{i}=1\) and \(_{i=1}^{N_{L-1}}r_{i}=1\). For the ground truth labels \(g\) and \(g_{parent}\) at the fine grained and the coarse grained levels respectively: Now assuming that the coarse label is correctly predicted by the coarse prediction network i.e. \(*{argmax}(R)=g_{parent}\), we wish to prove that:_

\[ r_{g_{parent}}}{_{j=1}^{N_{L}}q_{j} r_{j_{parent}}}  q_{g}.\]

Proof.: The denominator iterates over the fine grained predictions and multiplies them with their parent's prediction scores. This is equivalent to iterating over the coarse label predictions and multiplying with the sum of prediction scores for all its children. By rewriting the denominator, we obtain:

Figure 2: Consider a four class hierarchy tree and corresponding fine-grained predictions (\(q_{i}\)) and coarse level predictions (\(r_{i}\)) obtained using independent models. Our work aims to improve the fine grained classification by leveraging the predictions at the coarse level. In the given example, the class prediction changes from ‘rose’ to ‘bus’ after the post-hoc correction.

\[_{j=1}^{N_{L}}q_{j} r_{j_{parent}}=_{j=1}^{N_{L-1}}r_{j}_{i j_{ child}}q_{i}\]

Assuming, \(_{i j_{child}}q_{i}=z_{j}\)

\[_{j=1}^{N_{L-1}}r_{j}_{i j_{child}}q_{i}=_{j=1}^{N_{L-1}}r_{j}  z_{j}=R^{T}Z\]

Invoking Holder's inequality, using \(a=\) and \(b=1\), (\(1/a+1/b=1\)), we obtain:

\[R^{T}Z\|R\|_{}\|Z\|_{1}\]

Since, \(\|R\|_{}=(R)=r_{g_{parent}}\) and \(\|Z\|_{1}=_{i=1}^{N_{L}}q_{i}=1\). We can say:

\[R^{T}Z\|R\|_{}\|Z\|_{1} r_{g_{parent}}\]

Given the above equation, we can conclude that:

\[}}{_{j=1}^{N_{L}}q_{j} r_{j_{parent}}} 1.\]

When correct predictions are made at both the coarse and fine-grained levels, the method will help increase the confidence of the prediction. A more interesting case occurs when the coarse-level prediction is correct; however, the fine-grained classifier makes a wrong prediction. The hierarchical ensemble can shift the prediction to the correct sub-tree, reducing the mistake severity and potentially improving the _top-1_ accuracy.

## 4 Experiments & Results

Like prior works [6; 8; 7], we evaluate our approach on the _iNaturalist-19_ and _tieredImageNet-H_ datasets. The _iNaturalist-19_ dataset is a species classification dataset containing plants, animals, fungi, etc., and has an 8-level hierarchy. The _tieredImageNet-H_ dataset contains classes from a wide range of categories, including musical instruments, animal breeds, tools, and plant species. It has a 13-level hierarchy derived from the WordNet hierarchy. However, rather than using the full hierarchy of categories, for our method, we only use the leaf classes and their parent categories during training and inference. The _iNaturalist-19_ dataset includes 1010 fine-grained "species" classes and 72 coarse-grained classes for the "genus" taxonomy. The _tieredImageNet-H_ dataset has 608 leaf classes and 201 parent classes. We train classifiers for each level independently, using the same hyperparameter settings.

In addition, we also evaluate the performance of our approach in a semi-supervised setting. Specifically, we want to test the performance on fine-grained classification tasks using a large number of coarsely annotated examples and a limited number of finely annotated examples. We use the _iNaturalist-19_ dataset for this setting; the coarse labels for all the images are used, while for fine-grained labels, we sample a limited number of images per class label during training. We used the "species" and "genus" taxonomies for the fine and coarse level labels and performed experiments for cases when the number of images per fine label is 100, 50, 25, and 10. Similar to the supervised setting, we train separate classifiers on coarse and fine labels.

To align with prior efforts [7; 8], all of our experiments are performed using the ResNet-18 backbone. We randomly crop a portion of images and resize them to \(224 224\) resolution for both datasets. For the _iNaturalist-19_ dataset, we initialize the ResNet-18 weights with a pre-trained ImageNet model and train the classifiers using a customized SGD optimizer for 100 epochs, with different learning rates for the backbone network and the fully connected layer (0.01 and 0.1, respectively). For the _tieredImageNet-H_ dataset, we train the ResNet-18 from scratch (since it is derived from the ImageNet

[MISSING_PAGE_FAIL:6]

pre-trained models of all the approaches. Therefore, in each of the Tables 1-2, we group the results to report evaluation metrics with and without using CRM at test time.

Apart from HAF, all the other methods from the prior art trade-off the _top-1_ accuracy with the mistake severity (Tables 1&2). The results contrast with the problem's main objective, i.e., improve the hierarchical metrics by maintaining or improving the _top-1_ error. In some of the instances (e.g. HXE \(=0.6\), Soft-labels \(=30\)), the _avg. mistake severity_ is reduced by making additional low-severity mistakes (indicated by an increase in _top-1_ error and _Hier dist@1_). The multi-head approach by  brings minor gains, and HAF further improves upon it by introducing consistency loss among different head predictions.

The proposed minimalist approach of Hierarchical Ensembles (HiE) significantly outperforms all the methods; on both datasets, with and without CRM. On the three metrics of _top-1_ error, _avg. mistake severity_, and _Hier dist@1_, the performance achieved by _HiE without CRM_ betters all other methods post-CRM. The gains achieved by HiE on _Hier dist@5_ and _Hier dist@20_ metrics are more pronounced, for instance, improving over HAF without CRM on _iNaturalist-19_ dataset by 17% and 14% respectively. These two ranking metrics compare the ordering of the classes provided by each of these classifiers. The results illustrate that HiE is able to reliably align the predictions with the hierarchy. The results are interesting given that HiE only employs the bottom two levels of the hierarchy, in contrast to other methods which utilize the entire hierarchy tree.

Compared with the baselines, on the _iNaturalist-19_ dataset, the Cross-Entropy-H brings minor gains on _Hier dist@5_ and _Hier dist@20_ metrics; however, the _top-1_ error increases by over a percent. On _tieredImageNet-H_, Cross-Entropy-H fails to give any improvements. In contrast, the HiE-Self baseline is able to retain the original _top-1_ performance while bringing noticeable gains on _Hier dist@k_ metrics. It illustrates that hierarchical ensembles are helpful even when applied on a single network. However, training separate networks for coarse and fine-grained levels allows us to learn complementary features which are then combined using our HiE approach.

    & Top-1 error(↓) & Mistakes severity(↓) & Hier dist@1(↓) & Hier dist@5(↓) & Hier dist@20(↓) \\   & &  \\  Cross-Entropy & 30.64 \(\) 0.030 & 7.07 \(\) 0.010 & 2.17 \(\) 0.006 & 5.70 \(\) 0.003 & 7.25 \(\) 0.003 \\ Cross-Entropy-H & 32.87 \(\) 0.042 & 7.13 \(\) 0.031 & 2.35 \(\) 0.003 & 5.70 \(\) 0.009 & 7.14 \(\) 0.012 \\ HiE-Self & 30.78 \(\) 0.054 & 7.05 \(\) 0.028 & 2.19 \(\) 0.004 & 5.35 \(\) 0.009 & 6.92 \(\) 0.007 \\ Barz \& Denzler  & 39.73 \(\) 0.240 & 6.80 \(\) 0.019 & 2.70 \(\) 0.022 & 5.48 \(\) 0.271 & 6.21 \(\) 0.008 \\ YOLO-v2  & 33.37 \(\) 0.082 & 7.02 \(\) 0.004 & 2.34 \(\) 0.016 & 5.85 \(\) 0.011 & 7.43 \(\) 0.016 \\ HXE \(\)=0.1  & 30.72 \(\) 0.036 & 7.00 \(\) 0.019 & 2.15 \(\) 0.005 & 5.62 \(\) 0.008 & 7.08 \(\) 0.015 \\ HXE \(\)=0.6  & 34.50 \(\) 0.007 & 6.73 \(\) 0.041 & 2.32 \(\) 0.003 & 5.48 \(\) 0.001 & 6.78 \(\) 0.003 \\ Soft-labels \(=30\) & 30.53 \(\) 0.194 & 7.05 \(\) 0.009 & 2.15 \(\) 0.013 & 5.66 \(\) 0.002 & 7.14 \(\) 0.008 \\ Soft-labels \(=4\) & 38.99 \(\) 0.105 & 6.60 \(\) 0.024 & 2.57 \(\) 0.004 & 5.13 \(\) 0.002 & 6.21 \(\) 0.001 \\ Chang et al. & 33.46 \(\) 0.026 & 6.99 \(\) 0.010 & 2.34 \(\) 0.006 & 5.75 \(\) 0.005 & 7.34 \(\) 0.010 \\ HAF & 30.50 \(\) 0.010 & 7.03 \(\) 0.024 & 2.24 \(\) 0.008 & 5.62 \(\) 0.011 & 6.99 \(\) 0.009 \\
**HiE (Ours)** & 29.31 \(\) 0.081 & 6.95 \(\) 0.013 & 2.07 \(\) 0.014 & 5.30 \(\) 0.001 & 6.86 \(\) 0.001 \\    \\  Cross-Entropy & 30.56 \(\) 0.020 & 7.01 \(\) 0.007 & 2.14 \(\) 0.006 & 4.88 & 6.11 \(\) 0.011 \\ Cross-Entropy-H & 32.93 \(\) 0.029 & 7.05 \(\) 0.006 & 2.32 \(\) 0.004 & 5.01 \(\) 0.008 & 6.14 \(\) 0.003 \\ Hife-Self & 30.79 \(\) 0.031 & 7.04 \(\) 0.008 & 2.17 \(\) 0.004 & 4.98 \(\) 0.005 & 6.14 \(\) 0.002 \\ Barz \& Dender & 83.55 \(\) 0.000 & 11.94 \(\) 0.000 & 11.92 \(\) 0.000 & 11.91 \(\) 0.000 \\ YOLO-v2 & 33.98 \(\) 0.099 & 6.99 \(\) 0.011 & 2.38 \(\) 0.012 & 5.05 \(\) 0.001 & 6.17 \(\) 0.001 \\ HXE \(\)=0.1 & 30.80 \(\) 0.079 & 6.95 \(\) 0.021 & 2.14 \(\) 0.005 & 4.94 \(\) 0.003 & 6.11 \(\) 0.002 \\ HXE \(\)=0.6 & 34.68 \(\) 0.003 & 6.69 \(\) 0.007 & 2.32 \(\) 0.001 & 4.99 \(\) 0.005 & 6.13 \(\) 0.003 \\ Soft-labels \(=30\) & 30.69 \(\) 0.125 & 6.99 \(\) 0.007 & 2.15 \(\) 0.008 & 4.95 \(\) 0.001 & 6.11 \(\) 0.001 \\ Soft-labels \(=4\) & 82.72 \(\) 0.079 & 7.54 \(\) 0.001 & 6.24 \(\) 0.005 & 6.94 \(\) 0.005 & 7.25 \(\) 0.002 \\ Chang et al. & 33.73 \(\) 0.033 & 6.90 \(\) 0.018 & 2.34 \(\) 0.002 & 5.02 \(\) 0.007 & 6.15 \(\) 0.001 \\ HAF & 30.63 \(\) 0.007 & 6.97 \(\) 0.024 & 2.14 \(\) 0.008 & 4.95 \(\) 0.004 & 6.11 \(\) 0.001 \\
**HiE (Ours)** & 29.39 \(\) 0.082 & 6.93 \(\) 0.013 & 2.07 \(\) 0.013 & 4.93 \(\) 0.001 & 6.11 \(\) 0.001 \\   

Table 2: Results comparing top-1 error(%) and hierarchical metrics on the test set of _tieredImageNet-H_. The _Top_ block reports results without using CRM  and the _Bottom_ block are reported using CRM. Methods highlighted with aqua green are the best performing methods in top-1 error (%). We use dark green for the top performers and light green for the runners-up in each metric.

### Semi Supervised Learning

We additionally validate our approach on fine-grained classification in the semi-supervised setting. We assume that coarse annotations are available for all the samples; however, fine-grained annotations are available only for a few samples in each class. The studied setting differs from cases , which divide the fine-grained classes into two categories: base classes with abundant annotated samples and novel classes where only a few annotated samples are available. This framework  also limits the evaluation only to the novel classes. Our setting imposes firm limits on difficult-to-obtain fine-grained labels. In the extreme setting, we assume only ten annotated training samples are available for each fine-grained class. We also evaluate the performance across all the fine-grained classes together.

We train a cross-entropy baseline exclusively using the annotated fine-grained samples. We compare its performance with representative semi-supervised methods, including Pseudo-Label , Self-Training (ST) with distillation , Self-Supervised learning (MoCo) with distillation  and a combination of Self-Supervised learning and Self Training (MoCo+ST). We also include a variation of all these methods (Pseudo-Label-H, MoCo-H, ST-H, MoCoST-H) with hierarchical supervised loss as described in  with "genus" supervision. The hierarchical supervised loss is applied to the coarse predictions obtained by marginalizing the fine-grained class predictions. The proposed hierarchical ensemble is a complementary approach, and we compare the performance of the above methods with and without applying the HiE.

The results are illustrated in Table 3. As expected, the performance of the cross-entropy baseline steeply drops with the reduction in training data. The performance of semi-supervised methods (Pseudo-Label, ST, MoCo, MoCo-ST) improves over the cross-entropy baseline; however, they fail to retain the overall performance. The semi-supervised methods become more effective with the reduced number of labeled examples (providing better proportional gains over the cross-entropy baseline). Incorporating hierarchical supervised loss brings remarkable gains in _top-1_ accuracy across all experiments. For instance, in the lowest data regime (using only ten fine-grained annotated samples for training), hierarchical supervised loss brings twofold improvement over the cross-entropy baseline. The performance gains showcase the efficacy of utilizing coarse taxonomic labels in the studied setting.

Employing the proposed Hierarchical Ensemble brings consistent gains across all the methods on the three studied evaluation metrics. The combined approach of Moco-H/PseudoLabel-H with HiE

[MISSING_PAGE_FAIL:9]

(level 1), phylum (level 2), and class (level 3) have 3, 4 and 9 classes respectively) which does not help much in improving the fine-grained predictions further.

## 5 Limitation

The main limitation of HiE is the requirement of training an additional network for the coarse level. However, existing approaches employ a full hierarchy with a separate classification head for each hierarchical level and additional losses to enforce the hierarchical structure, making their training process complex and difficult to converge. Instead, our approach trained with vanilla cross-entropy loss works with partial hierarchy and trade-offs the extra compute for adaptability, reproducibility and simplicity in training. A future avenue for exploration would be to learn disentangled coarse and fine-grained features imitating HiE in a unified architecture. Another limitation is the assumption regarding the availability of underlying label hierarchy. To overcome this limitation, we can exploit large-language models  to obtain label hierarchy using raw class labels.

## 6 Conclusion

We proposed using Hierarchical Ensembles (HiE) of independently trained networks over coarse and fine-grained levels of the label hierarchy. In terms of mistake severity, our proposed post-hoc correction consistently outperforms state-of-the-art methods in deep hierarchy-aware image classification by large margins in terms of decrease in _avg_. _mistake severity_ and _hierarchical distance@k_, while simultaneously improving the _top-1_ accuracy. In the semi-supervised paradigm, we show that HiE delivers consistent performance gains when used in conjunction with off-the-shelf semi-supervised learning algorithms. We show that, comparable performance to a fully supervised baseline can be attained, even using merely 10 annotations for each fine-grained class on a large fine-grained image classification dataset encompassing 1010 classes.