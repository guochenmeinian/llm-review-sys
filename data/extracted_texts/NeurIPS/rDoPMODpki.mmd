# KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge

Pengcheng Jiang Lang Cao Cao Xiao\({}^{}\) Parminder Bhatia\({}^{}\) Jimeng Sun Jiawei Han

University of Illinois at Urbana-Champaign GE HealthCare

{pj20, langcao2, jimeng, hanj}@illinois.edu danicaxiao@gmail.com

###### Abstract

Knowledge Graph Embedding (KGE) techniques are crucial in learning compact representations of entities and relations within a knowledge graph, facilitating efficient reasoning and knowledge discovery. While existing methods typically focus either on training KGE models solely based on graph structure or fine-tuning pre-trained language models with classification data in KG, KG-FIT leverages LLM-guided refinement to construct a semantically coherent hierarchical structure of entity clusters. By incorporating this hierarchical knowledge along with textual information during the fine-tuning process, KG-FIT effectively captures both global semantics from the LLM and local semantics from the KG. Extensive experiments on the benchmark datasets FB15K-237, YAGO3-10, and PrimeKG demonstrate the superiority of KG-FIT over state-of-the-art pre-trained language model-based methods, achieving improvements of 14.4%, 13.5%, and 11.9% in the Hits@10 metric for the link prediction task, respectively. Furthermore, KG-FIT yields substantial performance gains of 12.6%, 6.7%, and 17.7% compared to the structure-based base models upon which it is built. These results highlight the effectiveness of KG-FIT in incorporating open-world knowledge from LLMs to significantly enhance the expressiveness and informativeness of KG embeddings.

## 1 Introduction

Knowledge graph (KG) is a powerful tool for representing and storing structured knowledge, with applications spanning a wide range of domains, such as question answering [1; 2; 3; 4], recommendation systems [5; 6; 7], drug discovery [8; 9; 10], and clinical prediction [11; 12; 13]. Constituted by entities and relations, KGs form a graph structure where nodes denote entities and edges represent relations among them. To facilitate efficient reasoning and knowledge discovery, knowledge graph embedding

Figure 1: “Fine-tune LLM with KG” vs “Fine-tune KG with LLM”.

(KGE) methods [14; 15; 16; 17; 18; 19; 20; 21] have emerged, aiming to derive low-dimensional vector representations of entities and relations while preserving the graph's structural integrity.

While current KGE methods have shown success, many are limited to the graph structure alone, neglecting the wealth of open-world knowledge surrounding entities not explicitly depicted in the KG, which is manually created in most cases. This oversight inhibits their capacity to grasp the complete semantics of entities and relations, consequently resulting in suboptimal performance across downstream tasks. For instance, a KG might contain entities such as "Albert Einstein" and "Theory of Relativity", along with a relation connecting them. However, the KG may lack the rich context and background information about Einstein's life, his other scientific contributions, and the broader impact of his work. In contrast, pre-trained language models (PLMs) and LLMs, having been trained on extensive literature, can provide a more comprehensive understanding of Einstein and his legacy beyond the limited scope of the KG. While recent studies have explored fine-tuning PLMs with KG triples [22; 23; 24; 25; 26; 27; 28; 29], this approach is subject to several limitations. Firstly, the training and inference processes are computationally expensive due to the large number of parameters in PLMs, making it challenging to extend to more knowledgeable LLMs. Secondly, the fine-tuned PLMs heavily rely on the restricted knowledge captured by the KG embeddings, limiting their ability to fully leverage the extensive knowledge contained within the language models themselves. As a result, these approaches may not adequately capitalize on the potential of LLMs to enhance KG representations, as illustrated in Fig. 1. Lastly, small-scale PLMs (e.g., BERT) contain outdated and limited knowledge compared to modern LLMs, requiring re-training to incorporate new information, which hinders their ability to keep pace with the rapidly evolving nature of today's language models.

To address the limitations of current approaches, we propose KG-FIT (**K**nowledge **G**raph **F**In**e-**T**uning), a novel framework that directly incorporates the rich knowledge from LLMs into KG embeddings without the need for fine-tuning the LMs themselves. The term "fine-tuning" is used because the initial entity embeddings are from pre-trained LLMs, initially capturing global semantics.

KG-FIT employs a two-stage approach: (1) generating entity descriptions from the LLM and performing LLM-guided hierarchy construction to build a semantically coherent hierarchical structure of entities, and (2) fine-tuning the KG embeddings by integrating knowledge from both the hierarchical structure and textual embeddings, effectively merging the open-world knowledge captured by the LLM into the KG embeddings. This results in enriched representations that integrate both global knowledge from LLMs and local knowledge from KGs.

The main contributions of KG-FIT are outlined as follows:

1. We introduce a method for automatically constructing a semantically coherent entity hierarchy using agglomerative clustering and LLM-guided refinement.
2. We propose a fine-tuning approach that integrates knowledge from the hierarchical structure and pre-trained text embeddings of entities, enhancing KG embeddings by incorporating open-world knowledge captured by the LLM.
3. Through an extensive empirical study on benchmark datasets, we demonstrate significant improvements in link prediction accuracy over state-of-the-art baselines.

## 2 Related Work

**Structure-Based Knowledge Graph Embedding.** Knowledge Graph Embedding methods that rely solely on graph structure aim to learn low-dimensional vector representations of entities and relations while preserving the graph's structural properties. TransE  models relations as translations in the embedding space. DistMult  is a simpler model that uses a bilinear formulation for link prediction. ComplEx  extends TransE to the complex domain, enabling the modeling of asymmetric relations. ConvE  employs a convolutional neural network to model interactions between entities and relations. TuckER  utilizes a Tucker decomposition to learn embeddings for entities and relations jointly. RotatE  represents relations as rotations in a complex space, which can capture various relation patterns, and HAKE  models entities and relations in an implicit hierarchical and polar coordinate system. These structure-based methods have proven effective in various tasks  but do not leverage the rich entity information available outside the KG itself.

**PLM-Based Knowledge Graph Embedding.** Recent studies have explored integrating pre-trained language models (PLMs) with knowledge graph embeddings to leverage the semantic information captured by PLMs. KG-BERT , PKGC , TagReal , and KG-LLM  train PLMs/LLMs with a full set of classification data and prompts. However, these approaches are computationally expensive due to the need to iterate over all possible positive/negative triples. LMKE  and SimKGC  adopt contrastive learning frameworks to tackle issues like expensive negative sampling and enable efficient learning for text-based KGC. KG-S2S  and KGT5  employ sequence-to-sequence models to generate missing entities or relations in the KG. StAR  and CSProm-KG  fuse embeddings from graph-based models and PLMs. However, they are limited to small-scale PLMs and do not leverage the hierarchical and clustering information reflecting the LLM's knowledge of entities. Fully LLM prompting-based methods [33; 34] are costly and not scalable. In contrast, our proposed KG-FIT approach can be applied to any LLM, incorporating its knowledge through a semantically coherent hierarchical structure of entities. This enables efficient exploitation of the extensive knowledge within LLMs, while maintaining the efficiency of structure-based methods.

## 3 KG-FIT Framework

We present KG-FIT (as shown in Fig. 2), a framework for fine-tuning KG embeddings leveraging external hierarchical structures and textual information based on open knowledge. This framework comprises two primary components: (1) **LLM-Guided Hierarchy Construction**: This phase establishes a semantically coherent hierarchical structure of entities, initially constructing a seed hierarchy and then refining it using LLM-guided techniques, and (2) **Knowledge Graph Fine-Tuning**: This stage enhances the KG embeddings by integrating the constructed hierarchical structure, textual embeddings, and multiple constraints. The two stages combined to enrich KG embeddings with open-world knowledge, leading to more comprehensive and contextually rich representations. Below we present more tecnical details. A table of notations is placed in Appendix L.

### LLM-Guided Hierarchy Construction

We initiate the process by constructing a hierarchical structure of entities through agglomerative clustering, subsequently refining it using an LLM to enhance semantic coherence and granularity.

Figure 2: Overview of KG-FIT. **Input** and **Output** are highlighted at each step. **Step 1**: Obtain text embeddings for all entities in the KG, achieved by merging word embeddings with description embeddings retrieved from LLMs. **Step 2**: Hierarchical clustering is applied iteratively to all entity embeddings over various distance thresholds, monitored by a Silhouette scorer to identify optimal clusters, thus constructing a seed hierarchy where each leaf node represents a cluster of semantically similar entities. **Step 3**: Leveraging LLM guidance, the seed hierarchy is iteratively refined bottom-up through a series of suggested actions, aiming for a more accurate organization of KG entities with LLM’s knowledge. **Step 4**: Use the refined hierarchy along with KG triples and the initial entity embeddings to fine-tune the embeddings under a series of distance constraints.

**Step 1: Entity Embedding Initialization** is the first step of this process, where we are given a set of entities \(=\{e_{1},,e_{||}\}\) within a KG, and will enrich their semantic representations by generating descriptions using an LLM. Specifically for each entity \(e_{i}\), we prompt the LLM with a template (e.g., Briefly describe [entity] with the format "[entity] is a [description]". (detailed in Appendix E.1)) prompting it to describe the entity from the KG dataset, thereby yielding a concise natural language description \(d_{i}\). Subsequently, the entity embedding \(_{i}^{e}^{(f)}\) and description embedding \(_{i}^{d}=f(d_{i})^{(f)}\) are obtained using an embedding model \(f\) and concatenated to form the enriched entity representation \(_{i}\):

\[_{i}=[_{i}^{e};_{i}^{d}].\] (1)

**Step 2: Seed Hierarchy Construction** follows after entity embedding initialization. Here we choose agglomerative hierarchical clustering  over flat clustering methods like K-means  for establishing the initial hierarchy. This choice is based on the robust hierarchical information provided by agglomerative clustering, which serves as a strong foundation for LLM refinement. Using this hierarchical structure reduces the need for numerous LLM iterations to discern relationships between flat clusters, thereby lowering computational costs and complexity. Agglomerative clustering balances computational efficiency with providing the LLM a meaningful starting point for refinement. The clustering process operates on enriched entity representations \(=\{_{1},,_{l}\}^{|| 2(f)}\) using cosine distance and average linkage. The optimal clustering threshold \(^{*}\) is determined by maximizing the silhouette score \(S^{*}\) across a range of thresholds \([_{},_{}]\):

\[_{}=_{[_{},_{}]}S^{*}(,_{})\] (2)

where \(_{}\) are the clustering results at threshold \(\). This ensures that the resulting clusters are compact and well-separated based on semantic similarity. The constructed hierarchy forms a fully binary tree where each leaf node represents an entity. We use a top-down algorithm (detailed in Appendix F.1) to replace the first encountered entity with its cluster based on the optimal threshold \(_{}\). This process eliminates other entity leaves within the same cluster, forming the seed hierarchy \(_{}\), where each leaf node is a cluster of entities defined by \(_{_{}}\).

**Step 3: LLM-Guided Hierarchy Refinement (LHR)** is then applied to improve the quality of the knowledge representation. As the seed hierarchy \(_{}\) is a binary tree, which may not optimally represent real-world entity knowledge, we further refine it using the LLM. The LLM transforms the seed hierarchy into the LLM-guided refined hierarchy \(_{}\) through actions described below:

_i. Cluster Splitting:_ For each leaf cluster \(C_{}_{}(_{})\), the LLM recursively splits it into two subclusters using the prompt \(_{}\) (Fig. 8 in Appendix E.2):

\[C_{}=(_{}(C_{})),  C_{} C_{}=\{C_{1},C_{2},,C_{k}\},\] (3)

where \(C_{i}=\{e_{1}^{i},e_{2}^{i},,e_{|C_{i}|}^{i}\}\), \(_{i=1}^{k}|C_{i}|=|C_{}|=|C_{}|\), \(k\) is the total number of subclusters after recursively splitting \(_{}\) in a binary manner. This procedure iterates until LLM indicates no further splitting or each subcluster has minimal entities, resulting in an intermediate hierarchy \(_{}\).

_ii. Bottom-Up Refinement:_ In the bottom-up refinement phase, the LLM iteratively refines the intermediate hierarchy \(_{}\) produced by the cluster splitting step. The refinement process starts from the leaf level of the hierarchy and progresses upwards, considering each parent-child triple \((P_{*},P_{l},P_{r})\), where \(P_{*}\) represents the grandparent cluster, and \(P_{l}\) and \(P_{r}\) represent the left and right child clusters of \(P_{*}\), respectively. Let \(\{C_{1}^{l},C_{2}^{l},C_{3}^{l},,C_{|P_{l}|}^{l}\}\) denote the children of \(P_{l}\), and \(\{C_{1}^{r},C_{2}^{r},C_{3}^{r},,C_{|P_{r}|}^{r}\}\) denote the children of \(P_{r}\).

For each parent-child triple, the LLM is prompted with \(_{}(P_{*},P_{l},P_{r})\) (Fig. 9 in Appendix E.2), which provides the names and entities of the grandparent and child clusters. The LLM then suggests a refinement action to update the triple based on its understanding of the relationships between the clusters. The refinement options are:

1. **NO UPDATE**: The triple remains unchanged, i.e., \((P_{*}^{},P_{l}^{},P_{r}^{})=(P_{*},P_{l},P_{r})\).
2. **PARENT MERGE**: All the children of \(P_{l}\) and \(P_{r}\) are merged into the grandparent cluster \(P_{*}\), resulting in \(P_{*}^{}=\{C_{1}^{l},C_{2}^{l},C_{3}^{l},,C_{|P_{l}|}^{l},C_{1}^{r},C_{2}^{r},C_{3}^{r},,C_{|P_{r}|}^{r}\}\). The original child clusters \(P_{l}\) and \(P_{r}\) are removed from the hierarchy.
3. **LEAF MERGE**: \(P_{*}^{}=\{e_{1},e_{2},,e_{p}\},P_{l}^{}=,_{r }^{}=,\{e_{1},e_{2},,e_{p}\}=P_{l} P_{r}\).

4. **INCLUDE**: One of the child clusters is absorbed into the other, while the grandparent cluster remains unchanged. This can happen in two ways: * \(P^{}_{*}=P^{}_{l}=\{C^{l}_{1},C^{l}_{2},C^{l}_{3},,C^{l}_{|P_{l} |},P_{r}\}\), and \(P^{}_{r}=\), or * \(P^{}_{*}=P^{}_{r}=\{P_{l},C^{r}_{1},C^{r}_{2},C^{r}_{3},,C^{r} _{|P_{r}|}\}\), and \(P^{}_{l}=\).

The LLM determines the most appropriate refinement action based on the semantic similarity and hierarchical relationships between the clusters. The refinement process continues bottom-up, iteratively updating the triples until the root of the hierarchy is reached. The resulting refined hierarchy is denoted as \(_{}\). We place more details of the process in Appendix E.2 and F.2.

### Global Knowledge-Guided Local Knowledge Graph Fine-Tuning

**Step 4:** KG-FIT fine-tunes the knowledge graph embeddings by incorporating the hierarchical structure, text embeddings, and three main constraints: the hierarchical clustering constraint, text embedding deviation constraint, and link prediction objective.

**Initialization of Entity and Relation Embeddings:** To integrate the initial text embeddings (\(_{i}^{(f)}\)) into the model, the entity embedding \(_{i}^{n}\) is initialized as a linear combination of a random embedding \(^{}_{i}^{n}\) and the sliced text embedding \(^{}_{i}=[^{e}_{i}[\ ];^{d}_{i}[\ ]]^{n}\). The relation embeddings, on the other hand, are initialized randomly:

\[_{i}=^{}_{i}+(1-)^{}_{i}, _{j} N(0,^{2})\] (4)

where \(\) is a hyperparameter controlling the ratio between the random embedding and the sliced text embedding. \(_{j}^{m}\) is the embedding of relation \(j\), and \(\) is a hyperparameter controlling the standard deviation of the normal distribution \(N\). This initialization ensures that the entity embeddings start close to their semantic descriptions but can still adapt to the structural information in the KG during training. The random initialization of relation embeddings allows the model to flexibly capture the structural information and patterns specific to the KG.

**Hierarchical Clustering Constraint:** The hierarchical constraint integrates the structure and relationships derived from the adaptive agglomerative clustering and LLM-guided refinement process. This optimization enhances the embeddings for hierarchical coherence and distinct semantic clarity. The revised constraint consists of three tailored components:

\[_{}=_{e_{i} E}d( _{i},)}_{}}}-_{C^{ }_{m}(C)}_{i},^{})}{| _{m}(C)|}}_{}}}-_{j=1}^{h-1}(d(_{i},_{j+1}) -d(_{i},_{j}))}{h-1}}_{}}}\] (5)

where: \(e_{i}\) and \(_{i}\) represent the entity and its embedding. \(C\) is the cluster that entity \(e_{i}\) belongs to. \(_{m}(C)\) represents the set of neighbor clusters of \(C\) where \(m\) is the number of nearest neighbors (determined by lowest common ancestor (LCA)  in the hierarchy). \(\) and \(^{}\) denote the cluster embeddings of \(C\) and \(C^{}\), which is computed by averaging all the embedding of entities under them (i.e., \(=_{e_{i} C}_{i}^{n}\)). \(_{j}\) and \(_{j+1}\) are the embeddings of the parent nodes along the path from the entity (at depth \(h\)) to the root, indicating successive parent nodes in ascending order. Each parent node is computed by averaging the cluster embeddings under it (\(=_{C_{i} P}_{i}^{n}\)). \(d(,)\) is the distance function used to measure distances between embeddings. As higher levels of abstraction encompass a broader range of concepts, and thus a strict maintenance of hierarchical distance may be less critical at these levels, we introduce \(_{j}=_{0} e^{- j}\) where \(_{0}\) is the initial weight for the closest parent, typically a larger value, \(\) is the decay rate, a positive constant that dictates how rapidly the importance decreases. \(_{1}\), \(_{2}\), and \(_{3}\) are hyperparameters. In Eq 5, _Inter-level Cluster Separation_ aims to maximize the distance between an entity and neighbor clusters, enhancing the differentiation and reducing potential overlap in the embedding space. This separation ensures that entities are distinctly positioned relative to non-member clusters, promoting clearer semantic divisions. _Hierarchical Distance Maintenance_ encourages the distance between an entity and its parent nodes to be proportional to their respective levels in the hierarchy, with larger distances for higher-level parent nodes. This reflects the increasing abstraction and decreasing specificity, aligning the embeddings with the hierarchical structure of the KG. _Cluster Cohesion_ enhances intra-cluster similarity by minimizing the distance between an entity and its own cluster center, ensuring that entities within the same cluster are closely embedded, maintaining the integrity of clusters.

**Semantic Anchoring Constraint:** To preserve the semantic integrity of the embeddings, we introduce the semantic anchoring constraint, which is formulated as:

\[_{}=-_{e_{i}}d(_{i},_{ i}^{})\] (6)

where \(\) is the set of all entities, \(_{i}\) is the fine-tuned embedding of entity \(e_{i}\), \(_{i}^{}\) is the sliced text embedding of entity \(e_{i}\), and \(d(,)\) is a distance function. This constraint is crucial for large clusters, where the diversity of entities may cause the fine-tuned embeddings to drift from their original semantic meanings. This is also important when dealing with sparse KGs, as the constraint helps prevent overfitting to the limited structural information available. By acting as a regularization term, it mitigates overfitting and enhances the robustness of the embeddings .

**Score Function-Based Fine-Tuning:**KG-FIT is a general framework applicable to existing KGE models [14; 15; 16; 17; 18; 19; 20]. These models learn low-dimensional vector representations of entities and relations in a KG, aiming to capture the semantic and structural information within the KG itself. In our work, we perform link prediction to enhance the model's ability to accurately predict relationships between entities within the KG. Its loss is defined as:

\[_{}=-_{(e_{i},r,e_{j})} (-f_{r}(_{i},_{j}))-_{j} |}_{n_{j}_{j}}(-f_{r}(_{i}, _{n_{j}}))\] (7)

where \(\) is the set of all triples in the KG, \(\) is sigmoid function. \(f_{r}(,)\) is the scoring function (detailed in Appendix G and K) defined by the chosen KGE model that measures the compatibility between the head entity embedding \(_{i}\) and the tail entity embedding \(_{j}\) given the relation \(r\), \(_{j}\) is the set of negative tail entities sampled for the triple \((e_{i},r,e_{j})\), \(_{n_{j}}\) is the embedding of the negative tail entity \(n_{j}\), and \(\) is a margin hyperparameter. The link prediction-based fine-tuning minimizes the scoring function for the true triples \((e_{i},r,e_{j})\) while maximizing the margin between the scores of true triples and negative triples \((e_{i},r,n_{j})\). This encourages the model to assign higher scores to positive (true) triples and lower scores to negative triples, thereby enriching the embeddings with the local semantics in KG.

**Training Objective:** The objective function of KG-FIT integrates three constraints:

\[=_{1}_{}+_{2}_{}+_{3}_{}\] (8)

where \(_{1}\), \(_{2}\), and \(_{3}\) are hyperparameters that assign weights to the constraints.

**Note**: During fine-tuning, the time complexity per epoch is \(O((||+||) n)\) where \(||\) is the number of entities, \(||\) is the number of triples, and \(n\) is the embedding dimension. In contrast, classic PLM-based methods [22; 28; 29; 23] have a time complexity of \(O(|| L n_{})\) per epoch during fine-tuning, where \(L\) is the average sequence length and \(n_{}\) is the hidden dimension of the PLM. This is typically much higher than KG-FIT's fine-tuning time complexity, as \(|| L(||+||)\).

## 4 Experiment

### Experimental Setup

We describe our experimental setup as follows.

**Datasets**. We consider datasets that encompass various domains and sizes, ensuring comprehensive evaluation of the proposed model. Specifically, we consider three datasets: **(1) FB15K-237** (_CC BY 4.0_) is a subset of Freebase , a large collaborative knowledge base, focusing on common knowledge; **(2) YAGO3-10** is a subset of YAGO  (_CC BY 4.0_), which is a large knowledge base derived from multiple sources including Wikipedia, WordNet, and GeoNames; **(3) PrimeKG** (_CC0 1.0_) is a biomedical KG that integrates 20 biomedical resources, detailing 17,080 diseases through 4,050,249 relationships. Our study focuses on a subset of PrimeKG, extracting 106,000 triples from the whole set, with processing steps outlined in Appendix B. Table 2 shows the statistics of these datasets.

  
**Dataset** & **\#Ent.** & **\#Rel.** & **\#Train** & **\#Valid** & **\#Test** \\  FB15k-237 & 14,541 & 237 & 272,115 & 17,535 & 20,466 \\ YAGO3-10 & 123,182 & 37 & 1,079,040 & 5,000 & 5,000 \\ PrimeKG & 10,344 & 11 & 100,000 & 3,000 & 3,000 \\   

Table 2: **Datasets statistics.** #Ent./#Rel: number of entities/relations. #Train/#Valid/#Test: number of triples contained in the training/validation/testing set.

[MISSING_PAGE_FAIL:7]

YAGO3-10, and PrimeKG are 0.52, 0.49, and 0.33, respectively. The statistics of the seed and LHR hierarchies are placed in Appendix F.3. For LCA, we designate the root as the grandparent (i.e., two levels above) source cluster node and set \(m=5\). We set \(=0.5\), \(_{0}=1.2\), \(=0.4\), and use cosine distance for fine-tuning. Filtered setting  is applied for link prediction evaluation. Hyperparameter studies and computational cost are detailed in Appendix I and H, respectively.

### Results

We conduct experiments to evaluate the performance on link prediction. The results in Table 1 are averaged values from multiple runs with random seeds: ten runs for FB15K-237 and PrimeKG, and three runs for YAGO3-10. These averages reflect the performance of head/tail entity predictions.

**Main Results.** Table 1 shows that our KG-FIT framework consistently outperforms state-of-the-art PLM-based and traditional structure-based models across all datasets and metrics. This highlights KG-FIT's effectiveness in leveraging LLMs to enhance KG embeddings. Specifically, KG-FIT\({}_{}\) surpasses CSProm-KG by 6.3% and HAKE by 6.1% on FB15K-237 in Hits@10; KG-FIT\({}_{}\) outperforms CS-PromKG by 7.0% and TuckER by 4.6% on YAGO3-10; KG-FIT\({}_{}\) exceeds PKGC by 11.0% and CompiEx by 5.8% on PrimeKG. Additionally, with LLM-guided hierarchy refinement (LHR), KG-FIT achieves performance gains of 12.6%, 6.7%, and 17.8% compared to the base models, and 3.0%, 1.9%, and 2.2% compared to KG-FIT with seed hierarchy, on FB15K-237, YAGO3-10, and PrimeKG, respectively. All these findings highlight the effectiveness of KG-FIT for significantly improving the quality of structure-based KG embeddings.

Figure 3 further illustrates the robustness of KG-FIT, showing superior validation performance across training steps compared to the corresponding base models, indicating its ability to fix both overfitting and underfitting issues of some structure-based KG embedding models.

**Effect of Constraints.** We conduct an ablation study to evaluate the proposed constraints in Eq. 5 and 6, with results summarized in Table 4. This analysis underscores the importance of each constraint: **(1) Hierarchical Distance Maintenance** is crucial for both datasets. Its removal significantly degrades performance across all metrics, highlighting the necessity of preserving the hierarchical structure in the embedding space. **(2) Semantic Anchoring** proves more critical for the denser YAGO3-10 graph, where each cluster contains more entities, making it harder to distinguish between them based solely on cluster cohesion. The sparser FB15K-237 dataset is less impacted by the absence of this constraint. Similar to the semantics anchoring, the removal of **(3) Inter-level Cluster Separation** significantly affects the denser YAGO3-10 more than FB15K-237. Without this constraint, entities in YAGO3-10 may not be well-separated from other clusters, whereas FB15K-237 is less influenced. Interestingly, removing **(4) Cluster Cohesion** has a larger impact on the sparser FB15K-237 than on YAGO3-10. This difference suggests that sparse graphs rely more on the prior information provided by entity clusters, while denser graphs can learn this information more effectively from their abundant data.

**Effect of Knowledge Sources.** We explore the impact of the quality of LLM-refined hierarchies and pre-trained text embeddings on final performance, as illustrated in Figures 4 and 5. The results indicate that hierarchies constructed and text embeddings retrieved from more advanced LLMs consistently lead to improved performance. This finding underscores KG-FIT's capacity to leverage and evolve with ongoing advancements in LLMs, effectively utilizing the increasingly comprehensive entity knowledge captured by these models.

**Efficiency Evaluation.** We evaluate the efficiency performance in Table 3. While pure structure-based models are the fastest, our model significantly outperforms all PLM-based models in both

   Method & LM & T/E/p & Inf \\  KG-BERT & RoBERTA & 170m & 2900m \\ PKGC & RoBERTA & 150m & 50m \\ TagReal & LULKE & 190m & 50m \\ SAK & RoBERTA & 125m & 30m \\ KG-S2S & T5 & 30m & 110m \\ SimSKGC & BERT & 20m & 0.5m \\ CSProm-KG & BERT & 150m & 0.2m \\  KG-FIT (ours) & \(\)LY LAM & 1.2m & 0.1m \\ Structure-based & — & 0.2m & 0.1m \\   

Table 3: Model efficiency on PrimeKG. T/Ep and Inf denote training time/epoch and inference time. KG-FIT outperforms all the PLM-based models.

Figure 3: KG-FIT can mitigate overfitting (upper) and underfitting (lower) of structure-based models.

training and inference speed, consistent with our previous analysis. It achieves 12 times the training speed of CSProm-KG, the fastest PLM-based method. Moreover, KG-FIT can integrate knowledge from any LLMs, unlike previous methods that are limited to small-scale PLMs, underscoring its superiority.

**Visualization.** Figure 6 demonstrates the effectiveness of KG-FIT in capturing both global and local semantics. The embeddings generated by KG-FIT successfully preserve the global semantics at both intra- and inter-levels. Additionally, KG-FIT excels in representing local semantics compared to the original HAKE model.

## 5 Conclusion

In this paper, we introduced KG-FIT, a novel framework for enhancing knowledge graph (KG) embeddings by leveraging the wealth of open-world knowledge captured by large language models (LLMs). KG-FIT seamlessly integrates LLM-derived entity knowledge into the KG embedding process through a two-stage approach: LLM-guided hierarchy construction and global knowledge-guided local KG fine-tuning. By constructing a semantically coherent hierarchical structure of entities and incorporating this hierarchical knowledge along with textual information during fine-tuning, KG-FIT effectively captures both global semantics from the LLM and local semantics from the KG. Extensive experiments on benchmark datasets demonstrate the superiority of KG-FIT over state-of-the-art methods, highlighting its effectiveness in integrating open-world knowledge from LLMs to significantly enhance the expressiveness and informativeness of KG embeddings. A key advantage of KG-FIT is its flexibility to incorporate knowledge from any LLM, enabling it to evolve and improve with ongoing advancements in language models. This positions KG-FIT as a powerful and future-proof framework for knowledge-infused learning on graphs. Moreover, the enriched KG embeddings produced by KG-FIT have the potential to boost performance on a wide array of downstream tasks, such as question answering, recommendation systems, and drug discovery, among others. Our code and data are available at https://github.com/pat-jj/KG-FIT.

    &  &  &  & _{}\))} & _{}\))} \\  & & & & MRR & H@1 & H@5 & H@10 & MRR & H@1 & H@5 & H@10 \\  ✓ & ✓ & ✓ & ✓ & 362 & 264 & 478 &.568 & 568 & 474 & 662 &.718 \\ ✗ & ✓ & ✓ & ✓ & 345\({}_{(1,0)}\) & 454\({}_{(4,0)}\) & 542\({}_{(1,0)}\) & 554\({}_{(2,10)}\) & 558\({}_{(1,0)}\) & 467\({}_{(1,0)}\) & 654\({}_{(1,0)}\) & 709\({}_{(1,0)}\) \\ ✗ & ✗ & ✓ & ✓ & 335\({}_{(1,0)}\) & 241\({}_{(1,0)}\) & 444\({}_{(1,0)}\) & 533\({}_{(1,10)}\) & 545\({}_{(1,0)}\) & 452\({}_{(1,0)}\) & 640\({}_{(1,0)}\) & 695\({}_{(1,0)}\) \\ ✗ & ✓ & ✗ & ✓ & 343\({}_{(1,0)}\) & 244\({}_{(4,0)}\) & 449\({}_{(1,0)}\) & 538\({}_{(1,0)}\) & 544\({}_{(1,0)}\) & 653\({}_{(1,0)}\) & 654\({}_{(1,0)}\) & 651\({}_{(1,0)}\) \\ ✗ & ✓ & ✓ & ✗ & 332\({}_{(1,0)}\) & 239\({}_{(1,0)}\) & 437\({}_{(1,0)}\) & 529\({}_{(1,0)}\) & 558\({}_{(1,0)}\) & 465\({}_{(1,0)}\) & 656\({}_{(1,0)}\) & 711\({}_{(1,0)}\) \\ ✗ & ✗ & ✗ & ✗ & 287\({}_{(1,0)}\) & 192\({}_{(1,0)}\) & 389\({}_{(1,0)}\) & 478\({}_{(1,0)}\) & 530\({}_{(1,0)}\) & 431\({}_{(1,0)}\) & 634\({}_{(1,0)}\) & 634\({}_{(1,0)}\) \\   

Table 4: **Ablation study for the proposed constraints.**_SA_, _HDM_, _ICS_, _CC_ denote Semantic Anchoring, Hierarchical Distance Maintenance, Inter-level Cluster Separation, and Cluster Cohesion, respectively. We use TransE and HAKE as the base models for KG-FIT on FB15K-237 and YAGO3-10, respectively.

Figure 4: KG-FIT on FB15K-237 with different hierarchy types. _None_ indicates no hierarchical information input. _Seed_ denotes the seed hierarchy. _G3.5/G4_ denotes the LHR hierarchy constructed by GPT-3.5/40. LHR hierarchies outperform the seed hierarchy, with more advanced LLMs constructing higher-quality hierarchies.

Figure 5: KG-FIT on FB15K-237 with different text embedding. _BT_, _RBT_, _ada2_, and _te3_ are BERT, RoBERTa, text-embedding-ada-002, and text-embedding-3-large, respectively. Seed hierarchy is used for all settings. It is observed that pre-trained text embeddings from LLMs are substantially better than those from small PLMs.

## 6 Limitations

Although KG-FIT outperforms state-of-the-art PLM-based models on the FB15K-237, YAGO3-10, and PrimeKG datasets, it does not outperform pure PLM-based methods on a lexical dataset WN18RR, as shown in Table 5. This limitation is discussed with details in Appendix C. As a future work, we will explore the integration of contrastive learning into the KG-FIT framework to enhance its capability to capture semantic relationships more effectively.

Moreover, KG-FIT's performance is influenced by the quality of the constructed hierarchy, particularly the seed hierarchy. To address this, we propose an automatic selection of the optimal binary tree based on the silhouette score. However, if the initial clustering is suboptimal, it may result in a lower-quality hierarchy that affects KG-FIT's performance. Additionally, the bottom-up refinement process in our proposed LHR approach updates each parent-child triple with only a single operation (within four), which prioritizes efficiency and simplicity over performance. In future work, we plan to explore cost-efficient methods for refining the hierarchy that integrate multiple operations for each triple update, striking a better balance between efficiency and performance.

## 7 Acknowledgement

The research was supported in part by US DARPA INCAS Program No. HR0011-21-C0165 and BRIES Program No. HR0011-24-3-0325, National Science Foundation IIS-19-56151, the Molecule Maker Lab Institute: An AI Research Institutes program supported by NSF under Award No. 2019897, and the Institute for Geospatial Understanding through an Integrative Discovery Environment (I-GUIDE) by NSF under Award No. 2118329. Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and do not necessarily represent the views, either expressed or implied, of DARPA or the U.S. Government.

Figure 6: **Visualization of Entity Embedding (left to right: initial text embedding, HAKE embedding, and KG-FITHAKE embedding). _Upper (local)_: Embeddings (dim=2048) of <_Marcuirco_, _drug_effect_, _CAA (Coronary artery atherosclerosis)_> and <_Cladribine_, _drug_effect_, _Exertional dyspnea_>, two parent-child triples selected from PrimeKG, in polar coordinate system. In the polar coordinate system, the normalized entity embedding \(}\) is split to \(_{1}=}[:]\) and \(_{2}=}[+1:]\) where \(n\) is the hidden dimension, which serves as values on the x-axis and y-axis, respectively, which is consistent with Zhang et al. ’s visualization strategy. _Lower (global)_: t-SNE plots of different embeddings of sampled entities, with colors indicating clusters (e.g., _Marcuirco_ belongs to the _HIV Drugs_ cluster). Triangles indicate the positions of _A Marvinro_, \(\)_CAA_, \(\)_Cladribine_, and \(\)_Exertional dyspnea_. _Observations_: While the initial text embeddings capture global semantics, they fail to delineate local parent-child relationships within the KG, as seen in the intermingled polar plots. In contrast, HAKE shows more distinct grouping by modulus on the polar plots, capturing hierarchical local semantics, but fails to adequately capture global semantics. Our KG-FIT, notably, incorporates prior information from LLMs and is fine-tuned on the KG, maintains global semantics from pre-trained text embeddings while better capturing local KG semantics, demonstrating its superior representational power across local and global scales.