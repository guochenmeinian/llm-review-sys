# TensorNet: Cartesian Tensor Representations for

Efficient Learning of Molecular Potentials

Guillem Simeon

Computational Science Laboratory

Universitat Pompeu Fabra

guillem.simeon@gmail.com &Gianni De Fabritiis

Computational Science Laboratory

Icrea, Universitat Pompeu Fabra, Acellera

g.defabritiis@gmail.com

###### Abstract

The development of efficient machine learning models for molecular systems representation is becoming crucial in scientific research. We introduce TensorNet, an innovative \((3)\)-equivariant message-passing neural network architecture that leverages Cartesian tensor representations. By using Cartesian tensor atomic embeddings, feature mixing is simplified through matrix product operations. Furthermore, the cost-effective decomposition of these tensors into rotation group irreducible representations allows for the separate processing of scalars, vectors, and tensors when necessary. Compared to higher-rank spherical tensor models, TensorNet demonstrates state-of-the-art performance with significantly fewer parameters. For small molecule potential energies, this can be achieved even with a single interaction layer. As a result of all these properties, the model's computational cost is substantially decreased. Moreover, the accurate prediction of vector and tensor molecular quantities on top of potential energies and forces is possible. In summary, TensorNet's framework opens up a new space for the design of state-of-the-art equivariant models.

## 1 Introduction

Interatomic potential modeling using neural networks is an emerging research area that holds great promise for revolutionizing molecular simulation and drug discovery pipelines [1; 2; 3; 4]. The conventional trade-off between accuracy and computational cost can be bypassed by training models on highly precise data [5; 6; 7; 8]. Current state-of-the-art methodologies rely on equivariant graph neural networks (GNNs) [9; 10] and message-passing neural network (MPNNs) frameworks [11; 12], where internal atomic representations incorporate well-defined transformation properties characteristic of physical systems. The integration of equivariant features into neural network interatomic potentials has led to remarkable improvements in accuracy, particularly when using higher-rank irreducible representations of the orthogonal group \((3)\)--which encompasses reflections and rotations in 3D space--in the form of spherical tensors [13; 14]. Although lower-rank Cartesian representations (scalars and vectors) have been employed [15; 16], their success has been limited compared to state-of-the-art spherical models [17; 18; 19]. MPNNs typically necessitate a substantial number of message-passing iterations, and models based on irreducible representations are generally computationally demanding due to the need to compute tensor products, even though some successful alternative has been put forward .

The pursuit of computationally efficient approaches for incorporating higher-rank equivariance is essential. In this paper, we introduce a novel \((3)\)-equivariant architecture that advances the integration of Cartesian representations by utilizing Cartesian rank-2 tensors, represented as 3x3 matrices. We demonstrate that this method achieves state-of-the-art performance comparable to higher-rank spherical models while having a reduced computational cost. This efficiency is realizedthrough the cheap decomposition of Cartesian rank-2 tensors into their irreducible components under the rotation group and the ability to mix features using straightforward 3x3 matrix products. Additionally, our model requires fewer message-passing steps and eliminates the need for explicit construction of many-body terms. The architecture also facilitates the direct prediction of tensor quantities, enabling the modeling of molecular phenomena where such quantities are relevant. In summary, we propose an alternative framework for developing efficient and accurate equivariant models.

## 2 Background and related work

**Equivariance.** A function \(f\) between two vector spaces \(X\) and \(Y\), \(f:X Y\), is said to be equivariant to the action of some abstract group \(G\) if it fulfills

\[D_{Y}[g]f(x)=f(D_{X}[g]x), \]

for all elements \(g G\) and \(x X\), where \(D_{X}[g]\) and \(D_{Y}[g]\) denote the representations of \(g\) in \(X\) and \(Y\), respectively. Equivariant neural networks used in neural network potentials focus on equivariance under the action of translations and the orthogonal group \((3)\) in \(^{3}\), the latter one being comprised by the rotation group \((3)\) and reflections, and regarded as a whole as the Euclidean group \(E(3)\).

**Cartesian tensors and irreducible tensor decomposition.** Tensors are algebraic objects that generalize the notion of vectors. In the same way, as vectors change their components with respect to some basis under the action of a rotation \(R(3)\), \(^{}=R\), a rank-\(k\) Cartesian tensor \(T\) can be (very) informally regarded as a multidimensional array with \(k\) indices, where each index transforms as a vector under the action of a rotation. In particular, a rank-2 tensor transformation under a rotation can be written in matrix notation as \(T^{}=R\;TR^{}\), where \(R^{}\) denotes the transpose of the rotation matrix \(R\). In this paper, we will restrict ourselves to rank-2 tensors. Moreover, any rank-2 tensor \(X\) defined on \(^{3}\) can be rewritten in the following manner 

\[X=(X)+(X-X^{})+(X+X^{}-(X)), \]

where \((X)=_{i}X_{ii}\) is the trace operator and \(\) is the identity matrix. The first term is proportional to the identity matrix, the second term is a skew-symmetric contribution, and the last term is a symmetric traceless contribution. It can be shown that expression (2) is a decomposition into separate representations that are not mixed under the action of the rotation group . In particular, the first component \(I^{X}(X)\) has only 1 degree of freedom and is invariant under rotations, that is, it is a scalar; the second term \(A^{X}(X-X^{})\) has 3 independent components since it is a skew-symmetric tensor, which can be shown to rotate as a vector; and \(S^{X}(X+X^{}-(X))\) rotates like a rank-2 tensor and has 5 independent components, since a symmetric tensor has six independent components but the traceless condition removes one degree of freedom. In terms of representation theory, the 9-dimensional representation (a 3x3 matrix) has been reduced to irreducible representations of dimensions 1, 3, and 5 . We will refer to \(X\) as a full tensor and to the components \(I^{X},A^{X},S^{X}\) as scalar, vector, and tensor features, respectively.

**Message passing neural network potentials.** Message-passing neural networks (MPNNs) have been successfully applied to the prediction of molecular potential energies and forces . Atoms are represented by graph nodes, which are embedded in three-dimensional Euclidean space, and edges between nodes are built according to their relative proximity after the definition of some cutoff radius. The neural network uses atomic and geometric information, such as distances, angles or relative position vectors, to learn useful node representations by recursively propagating, aggregating, and transforming features from neighboring nodes [22; 23; 15]. In the case of neural network potentials, after several rounds of message passing and feature transformations, node features are mapped to single per-atom scalar quantities which are atomic contributions to the total energy of the molecule. These energy contributions depend in a very complex way on the states of other atoms, and therefore MPNNs can be regarded as some learnable approximation to the many-body potential energy function. However, these neural networks have typically needed a substantially large amount of message-passing steps (up to 6 in some cases) [16; 17].

**Equivariant models.** Initially, since the potential energy is a scalar quantity, atomic features were built using geometric information which is invariant to translations, reflections, and rotations, such as in SchNet , DimeNet [24; 25], PhysNet , SphereNet  and GemNet . Nevertheless,it has been shown that the inclusion of equivariant internal features leads to substantially better performances and data efficiency [14; 28]. In equivariant GNNs, internal features transform in a specified way under some group action. Molecules are physical systems embedded in three-dimensional Euclidean space, and their properties display well-defined behaviors under transformations such as translations, reflections, and rotations. Therefore, when predicting molecular properties, the group of interest is the orthogonal group in three dimensions \((3)\), that is, rotations and reflections of the set of atoms in 3D space.

In models such as NewtonNet , EGNN , PaiNN , the Equivariant Transformer  and SAKE , Cartesian vector features are used on top of invariant features. These vector features are built using relative position vectors between input atoms, in such a way that when the input atomic Cartesian coordinates \(\) are transformed under the action of some \(R(3)\) represented by a 3x3 matrix, \( R\), internal vector features and vector outputs \(\) transform accordingly, \( R\). Other models such as Cormorant , Tensor Field Networks , NequIP , Allegro , BOTNet  and MACE , work directly with internal features that are irreducible representations of the group \((3)\), which can be labeled by some \(l\) (including \(l=0\)), and with dimensions \(2l+1\). The representations \(l=0\) and \(l=1\) correspond to scalars and vectors, respectively. In this case, under a transformation \(R(3)\) of the input coordinates \( R\), internal features \(h_{lm}()\) transform as \(h_{lm}(R)=_{m^{}}D^{l}_{m^{}m}(R)\)\(h_{lm^{}}()\), where \(D^{l}_{m^{}m}(R)^{(2l+1)(2l+1)}\) is an order \(l\) Wigner D-matrix. In this case, features are rank-\(l\) spherical tensors or pseudotensors, depending on their parity. The decomposition of a Cartesian tensor described in (2) and the irreducible representations in terms of spherical tensors are directly related by a change of basis . To generate new features that satisfy \((3)\)-equivariance, these are built by means of tensor products involving Clebsch-Gordan coefficients and parity selection rules. In particular, models with features \(l>1\) such as NequIP, Allegro, BOTNet, and MACE have achieved state-of-the-art performances in benchmark datasets in comparison to all other MPNNs. However, the computation of tensor products in most of these models containing higher-rank tensors and pseudotensors can be expensive, especially when computing them in an edge-wise manner.

## 3 TensorNet's architecture

### Operations respecting O(3)-equivariance

In this work, we propose the use of the 9-dimensional representation of rank-2 tensors (3x3 matrices). TensorNet operations are built to satisfy equivariance to the action of the orthogonal group \((3)\): equivariance under \((3)\) instead of the subgroup of rotations \((3)\) requires the consideration of the differences between tensors and pseudotensors. Tensors and pseudotensors are indistinguishable under rotations but display different behaviors under parity transformation, i.e. a reflection of the coordinate system through the origin. By definition, scalars, rank-2 tensors and in general all tensors of even rank do not flip their sign, and their parity is said to be even; on the contrary, vectors and tensors of odd rank have odd parity and flip their sign under the parity transformation. Pseudoscalars, pseudovectors, and pseudotensors have precisely the opposite behavior. Necessary derivations for the following subsections can be found in the Appendix (section A.2).

**Composition from irreducible representations.** The previously described irreducible decomposition of a tensor in Eq. 2 is with respect to the rotation group \((3)\). Building a tensor-like object that behaves appropriately under rotations can be achieved by composing any combination of scalars, vectors, tensors, and their parity counterparts. However, in neural network potential settings, the most direct way to produce tensors is by means of relative position _vectors_ and, in general, it is preferred for the neural network to be able to predict vectors rather than pseudovectors. One has the possibility to initialize full tensor representations from the composition of scalars, vectors encoded in skew-symmetric matrices, and symmetric traceless tensors. For instance, if one considers some vector \(=(v^{x},v^{y},v^{z})\), one can build a well-behaved tensor \(X\) under rotations by composing \(X=I+A+S\),

\[I=f(||||), A=0&v^{z}&-v^{y}\\ -v^{z}&0&v^{x}\\ v^{y}&-v^{x}&0, S=^{}-(^{}), \]where \(f\) is some function and \(^{}\) denotes the outer product of the vector with itself. In this case, under parity the vector transforms as \(-\), and it is explicit that \(I\) and \(S\) remain invariant, while \(A-A=A^{}\), and the full tensor \(X\) transforms as \(X=I+A+S X^{}=I+A^{}+S=X^{}\), since \(I\) and \(S\) are symmetric matrices. Therefore, one concludes that when initializing the skew-symmetric part \(A\) from vectors, not pseudovectors, parity transformation produces the transposition of full tensors.

**Invariant weights and linear combinations.** One can also modify some tensor \(X=I+A+S\) by multiplying invariant quantities to the components, \(X^{}=f_{I}I+f_{A}A+f_{S}S\), where \(f_{I},f_{A}\) and \(f_{S}\) can be constants or invariant functions. This modification of the tensor does not break the tensor transformation rule under the action of rotations and preserves the parity of the individual components given that \(f_{I},f_{A}\) and \(f_{S}\) are scalars (learnable functions of distances or vector norms, for example), not pseudoscalars. Also, from this property and the possibility of building full tensors from the composition of irreducible components, it follows that linear combinations of scalars, vectors, and tensors generate new full tensor representations that behave appropriately under rotations. Regarding parity, linear combinations preserve the original parity of the irreducible components given that all terms in the linear combination have the same parity. Therefore, given a set of irreducible components \(I_{j},A_{j},S_{j}\) with \(j\{0,1,...,n-1\}\), one can build full tensors \(X^{}_{i}\)

\[X^{}_{i}=_{j=0}^{n-1}w^{I}_{ij}I_{j}+_{j=0}^{n-1}w^{A}_{ij}A_{j} +_{j=0}^{n-1}w^{S}_{ij}S_{j}, \]

where \(w^{I}_{ij},w^{A}_{ij},w^{S}_{ij}\) can be learnable weights, in which case the transformation reduces to the application of three different linear layers without biases to inputs \(I_{j},A_{j},S_{j}\).

**Matrix product.** Consider two tensors, \(X\) and \(Y\), and some rotation matrix \(R(3)\). Under the transformation \(R\), the tensors become \(RXR^{}\) and \(RYR^{}\). The matrix product of these tensors gives a new object that also transforms like a tensor under the transformation, \(XY RXR^{}RYR^{}=RXR^{-1}RYR^{}=R(XY)R^{ }\), since for any rotation matrix \(R\), \(R^{}=R^{-1}\). Taking into account their irreducible decomposition \(X=I^{X}+A^{X}+S^{X}\) and \(Y=I^{Y}+A^{Y}+S^{Y}\), the matrix product \(XY\) consists of several matrix products among rotationally independent sectors \((I^{X}+A^{X}+S^{X})(I^{Y}+A^{Y}+S^{Y})\). These products will contribute to the different parts of the irreducible decomposition \(XY=I^{XY}+A^{XY}+S^{XY}\). Therefore, one can regard the matrix product as a way of combining scalar, vector, and tensor features to obtain new features. However, when assuming that the skew-symmetric parts are initialized from vectors, this matrix product mixes components with different parities, and resulting components \(I^{XY},A^{XY},S^{XY}\) would not have a well-defined behavior under parity (see Appendix, section A.2). To achieve \((3)\)-equivariance, we propose the use of the matrix products \(XY+YX\). Under parity \(X X^{},Y Y^{}\), and one can show that

\[I^{X^{}Y^{}+Y^{}X^{}}=I^{XY+YX},\ \ A^{X^{}Y^{}+Y^{}X^{}}=-A^{XY+YX},\ \ S^{X^{}Y^{}+Y^{}X^{}}=S^{XY+YX}, \]

that is, the scalar and symmetric traceless parts have even parity, and the skew-symmetric part has odd parity. The irreducible decomposition of the expression \(XY+YX\) preserves the rotational and parity properties of the original components and, therefore, it is an \((3)\)-equivariant operation. We finally note that one can produce \((3)\)-invariant quantities from full tensor representations or their components by taking their Frobenius norm \((X^{}X)=(XX^{})=_{ij}|X_{ij}|^{2}\).

### Model architecture

In this work we propose a model that learns a set of Cartesian full tensor representations \(X^{(i)}\) (3x3 matrices) for every atom \((i)\), from which atomic or molecular properties can be predicted, using as inputs atomic numbers \(z_{i}\) and atomic positions \(_{i}\). We mainly focus on the prediction potential energies and forces, even though we provide in Section 4 experiments demonstrating the ability of TensorNet to accurately predict up to rank-2 physical quantities. Atomic representations \(X^{(i)}\) can be decomposed at any point into scalar, vector and tensor contributions \(I^{(i)},A^{(i)},S^{(i)}\) via (2), and TensorNet can be regarded as operating with a physical inductive bias akin to the usual decomposition of interaction energies in terms of monopole, dipole and quadrupole moments . We refer the reader to Figure 1 and the Appendix (section A.1) for diagrams of the methods and the architecture.

**Embedding.** By defining a cutoff radius \(r_{c}\), we obtain vectors \(_{ij}=_{j}-_{i}\) between central atom \(i\) and neighbors \(j\) within a distance \(r_{c}\). We initialize per-edge scalar features using the identity matrix \(I_{0}^{(ij)}=\), and per-edge vector and tensor features using the normalized edge vectors \(_{ij}=_{ij}/||_{ij}||=(_{ij}^{x},_{ij}^{ y},_{ij}^{z})\). We create a symmetric traceless tensor from the outer product of \(_{ij}\) with itself, \(S_{0}^{(ij)}_{ij}_{ij}^{}- (_{ij}_{ij}^{})\), and vector features are initialized by identifying the independent components of the skew-symmetric contribution with the components of \(_{ij}\) as denoted in (3), getting for every edge \((ij)\) initial irreducible components \(I_{0}^{(ij)},A_{0}^{(ij)},S_{0}^{(ij)}\).

To encode interatomic distance and atomic number information in the tensor representations we use an embedding layer that maps the atomic number of every atom \(z_{i}\) to \(n\) invariant features \(Z_{i}\), and expand interatomic distances \(r_{ij}\) to \(d\) invariant features by means of an expansion in terms of exponential radial basis functions

\[e_{k}^{}(r_{ij})=((-r_{ij})-_{k})^{2})}, \]

where \(_{k}\) and \(_{k}\) are fixed parameters specifying the center and width of radial basis function \(k\). The \(\) vector is initialized with values equally spaced between \((-r_{c})\) and 1, and \(\) is initialized as \((2d^{-1}(1-))})^{-2}\) for all \(k\) as proposed in . After creating \(n\) identical copies of initial components \(I_{0}^{(ij)},A_{0}^{(ij)},S_{0}^{(ij)}\) (\(n\) feature channels), for every edge \((ij)\) we map with a linear layer the concatenation of \(Z_{i}\) and \(Z_{j}\) to \(n\) pair-wise invariant representations \(Z_{ij}\), and the radial basis functions are further expanded to \(n\) scalar features by using three different linear layers to obtain

\[f_{I}^{0}=W^{I}(e^{}(r_{ij}))+b^{I},\;\;\;f_{A}^{0}=W^{A}(e^{ }(r_{ij}))+b^{A},\;\;\;f_{S}^{0}=W^{S}(e^{}(r_{ij}))+ b^{S}, \]

Figure 1: Key steps, from top to bottom, in the embedding and interaction modules for some central atom \(i\) and neighbors \(j\) and \(k\) found within the cutoff radius. a) Relative position vectors are used to initialize edge-wise tensor components, modified using edge-wise invariant functions, and summed to obtain node-wise full tensors. b) Node full tensors are decomposed and weighted with edge invariant functions to obtain pair-wise messages, and summed to obtain node-wise aggregated messages, which will interact with receiving node’s full tensors via matrix product.

\[X^{(ij)}=(r_{ij})Z_{ij}f_{I}^{0}I_{0}^{(ij)}+f_{A}^{0}A_{0}^{(ij)}+f_{S}^ {0}S_{0}^{(ij)}, \]

where the cutoff function is given by \((r_{ij})=}{r_{c}}+1\) when \(r_{ij} r_{c}\) and 0 otherwise. That is, \(n\) edge-wise tensor representations \(X^{(ij)}\) are obtained, where the channel dimension has not been written explicitly. Then, we get atom-wise tensor representations by aggregating all neighboring edge-wise features. At this point, the invariant norms \(||X||(X^{}X)\) of atomic representations \(X^{(i)}\) are computed and fed to a normalization layer, a multilayer perceptron, and a SiLU activation to obtain three different \((3)\)-invariant functions per channel,

\[f_{I}^{(i)},f_{A}^{(i)},f_{S}^{(i)}=(((||X^{(i)}||))), \]

which, after the decomposition of tensor embeddings into their irreducible representations, are then used to modify component-wise linear combinations to obtain the final atomic tensor embeddings

\[X^{(i)} f_{I}^{(i)}W^{I}I^{(i)}+f_{A}^{(i)}W^{A}A^{(i)}+f_{S}^{(i)} W^{S}S^{(i)}. \]

#### Interaction and node update.

We start by normalizing each node's tensor representation \(X^{(i)} X^{(i)}/(||X^{(i)}||+1)\) and decomposing this representation into scalar, vector, and tensor features. We next transform these features \(I^{(i)},A^{(i)},S^{(i)}\) by computing independent linear combinations, \(Y^{(i)}=W^{I}I^{(i)}+W^{A}A^{(i)}+W^{S}S^{(i)}\). In parallel, edge distances' radial basis expansions are fed to a multilayer perceptron and a SiLU activation to transform them into tuples of three invariant functions per channel weighted with the cutoff function \((r_{ij})\),

\[f_{I}^{(ij)},f_{A}^{(ij)},f_{S}^{(ij)}=(r_{ij})(( e^{}(r_{ij}))). \]

At this point, after decomposition of node features \(Y^{(i)}\), we define the messages sent from neighbors \(j\) to central atom \(i\) as \(M^{(ij)}=f_{I}^{(ij)}I^{(j)}+f_{A}^{(ij)}A^{(j)}+f_{S}^{(ij)}S^{(j)}\), which get aggregated into \(M^{(i)}=_{j(i)}M^{(ij)}\). We use the irreducible decomposition of matrix products \(Y^{(i)}M^{(i)}+M^{(i)}Y^{(i)}\) between node embeddings and aggregated messages to generate new atomic scalar, vector, and tensor features. New features are generated in this way to guarantee the preservation of the original parity of scalar, vector, and tensor features. These new representations \(I^{(i)},A^{(i)},S^{(i)}\) are individually normalized dividing by \(||I^{(i)}+A^{(i)}+S^{(i)}||+1\) and are further used to compute independent linear combinations to get \(Y^{(i)} W^{I}I^{(i)}+W^{A}A^{(i)}+W^{S}S^{(i)}\). A residual update \( X^{(i)}\) for original embeddings \(X^{(i)}\) is computed with the parity-preserving matrix polynomial \( X^{(i)}=Y^{(i)}+(Y^{(i)})^{2}\), to eventually obtain updated representations \(X^{(i)} X^{(i)}+ X^{(i)}\).

#### Scalar output.

The Frobenius norm \((X^{}X)\) of full tensor representations and components in TensorNet is \((3)\)-invariant. For molecular potential predictions, total energy \(U\) is computed from atomic contributions \(U^{(i)}\) which are simply obtained by using the concatenated final norms of every atom's scalar, vector, and tensor features \(||I^{(i)}||,||A^{(i)}||,||S^{(i)}||\),

\[U^{(i)}=(\,||I^{(i)}||,||A^{(i)}||,||S^{(i)}||\,)), \]

obtaining forces via backpropagation.

#### Vector output.

Since interaction and update operations preserve the parity of tensor components, the skew-symmetric part of any full tensor representation \(X\) in TensorNet is guaranteed to be a vector, not a pseudovector. Therefore, from the antisymmetrization \(A^{X}\), one can extract vectors \(=(v_{x},v_{y},v_{z})\) by means of the identification given in (3).

#### Tensor output.

Taking into account that rank-2 tensors have even parity and the skew-symmetric part \(A\) in TensorNet is a vector, not a pseudovector, one might need to produce pseudovector features before rank-2 tensor predictions can be built by combining irreducible representations. This can be easily done by obtaining two new vector features with linear layers, \(A^{(1)}=W^{(1)}A\) and \(A^{(2)}=W^{(2)}A\), and computing \((A^{(1)}A^{(2)}-(A^{(1)}A^{(2)})^{})\), which is skew-symmetric, rotates like a vector, and is invariant under parity, the simultaneous transposition of \(A^{(1)}\) and \(A^{(2)}\).

## 4 Experiments and results

We refer the reader to the Appendix (section A.3) for further training, data set and experimental details.

**QM9: Chemical diversity.** To assess TensorNet's accuracy in the prediction of energy-related molecular properties with a training set of varying chemical composition we used QM9 . We trained TensorNet to predict: \(U_{0}\), the internal energy of the molecule at 0 K; \(U\), the internal energy at 298.15 K; \(H\), the enthalpy, also at 298.15 K; and \(G\), the free energy at 298.15 K. Results can be found in Table 1, which show that TensorNet outperforms Allgero , and MACE  on \(U_{0}\), \(U\) and \(H\). Remarkably, this is achieved with \(23\%\) of Allgero's parameter count. Furthermore, TensorNet uses only scalar, vector and rank-2 tensor features, as opposed to Allgero, which uses also their parity counterparts, and without the need of explicitly taking into account many-body terms as done in MACE.

**rMD17: Conformational diversity.** We also benchmarked TensorNet on rMD17 , the revised version of MD17 [36; 37], a data set of small organic molecules in which energies and forces were obtained by running molecular dynamics simulations with DFT. We report the results in Table 2. In the case of energies, TensorNet with two interaction layers (2L) is the model that achieves state-of-the-art accuracy for the largest number of molecules (6 out of 10), outperforming all other spherical models for benzene, with a parameter count of 770k. Energy errors are also within the range of other spherical models, except for ethanol and aspirin, and reach state-of-the-art accuracy for the case of toluene, with just one interaction layer (1L) and a parameter count of 535k. Force errors for 2L are also mostly found within the ranges defined by other spherical models, except for ethanol, aspirin, and salicylic acid, in which case these are slightly higher. However, for one interaction layer, force errors are increased and in most cases found outside of the range of accuracy of the other spherical models. We note that the smallest spherical models have approximately 2.8M parameters, and therefore TensorNet results are achieved with reductions of 80\(\%\) and 70\(\%\) in the number of parameters for 1L and 2L, respectively. Also, TensorNet is entirely based at most on rank-2 tensors.

**SPICE, ANI1x, COMP6: Compositional and conformational diversity.** To obtain general-purpose neural network interatomic potentials, models need to learn simultaneously compositional and conformational degrees of freedom. In this case, data sets must contain a wide range of molecular systems as well as several conformations per system. To evaluate TensorNet's out-of-the-box performance without hyperparameter fine-tuning, we trained the light model with two interaction layers used on rMD17 on the SPICE  and ANI1x [39; 40] data sets using the proposed Equivariant Transformer's SPICE hyperparameters  (for ANI1x, in contrast to the SPICE model, we used 32 radial basis functions instead of 64, and a cutoff of 4.5A instead of 10A), and further evaluated ANI1x-trained models on the COMP6 benchmarks .

For SPICE, with a maximum force filter of 50.94 eV/A \(\) 1 Ha/Bohr, TensorNet's mean absolute error in energies and forces are 25.0 meV and 40.7 meV/A, respectively, while the Equivariant Transformer achieves 31.2 meV and 49.3 meV/A. In this case, both models used a cutoff of 10A. Results for ANI1x and model evaluations on COMP6 are found in Table 3. We note that for ANI1x training, which contains molecules with up to 63 atoms, TensorNet used a cutoff of 4.5A. The largest rMD17 molecule is aspirin with 21 atoms. The light TensorNet model shows better generalization capabilities across all COMP6 benchmarks.

**Scalar, vector and tensor molecular properties for ethanol in a vacuum.** We next tested TensorNet performance for the simultaneous prediction of scalar, vector, and tensor molecular properties: potential energy, atomic forces, molecular dipole moments \(\), molecular polarizability tensors \(\), and nuclear-shielding tensors \(\), for the ethanol molecule in vacuum [15; 42]. We trained TensorNet to generate atomic tensor representations that can be used by different output modules to predict the

  
**Property** &  DimeNet++ \\  \\  &  ET \\  \\  &  PaiNN \\  \\  &  Allegro \\ (17.9M) \\  &  MACE \\  \\  & 
 TensorNet \\ (4.0M) \\  \\  \(U_{0}\) & 6.3 & 6.2 & 5.9 & 4.7 & 4.1 & **3.9(1)** \\ \(U\) & 6.3 & 6.3 & 5.7 & 4.4 & 4.1 & **3.9(1)** \\ \(H\) & 6.5 & 6.5 & 6.0 & 4.4 & 4.7 & **4.0(1)** \\ \(G\) & 7.6 & 7.6 & 7.4 & 5.7 & **5.5** & 5.7(1) \\   

Table 1: **QM9 results.** Mean absolute error on energy-related molecular properties from the QM9 dataset, in meV, averaged over different splits. Parameter counts for some models are found between parentheses.

desired properties. The specific architecture of these output modules can be found in the Appendix (section A.3).

Results from Table 4 show that TensorNet can learn expressive atomic tensor embeddings from which multiple molecular properties can be simultaneously predicted. In particular, TensorNet's energy and force errors are approximately a factor of two and three smaller when compared to FieldSchNet  and PaiNN , respectively, while increasing the prediction accuracy for the other target molecular properties, with the exception of the dipole moment.

    & E & F & \(\) & \(\) & \(_{all}\) \\  & (kcal/mol) & (kcal/mol/Å) & (D) & (Bohr\({}^{3}\)) & (ppm) \\  PaiNN  & 0.027 & 0.150 & **0.003** & 0.009 & - \\ FieldSchNet  & 0.017 & 0.128 & 0.004 & 0.008 & 0.169 \\ TensorNet & **0.008(1)** & **0.058(3)** & **0.003(0)** & **0.007(0)** & **0.139(4)** \\   

Table 4: **Ethanol in vacuum results.** Mean absolute error for the prediction of energies (E), forces (F), dipole moments (\(\)), polarizabilities (\(\)), and chemical shifts for all elements (\(_{all}\)), averaged over different splits, with corresponding units between parentheses.

   &  & TensorNet & NequIP & Allegro & BOTNet & MACE \\  & & 1L (535k) & 2L (770k) &  &  &  &  \\   & E & 2.7 & 2.4 & 2.3 & 2.3 & 2.3 & **2.2** \\  & F & 10.2(2) & 8.9(1) & 8.2 & 7.3 & 8.5 & **6.6** \\   & E & 0.9 & **0.7** & **0.7** & 1.2 & **0.7** & 1.2 \\  & F & 3.8 & 3.1 & 2.9 & **2.6** & 3.3 & 3.0 \\   & E & 0.03 & **0.02** & 0.04 & 0.3 & 0.03 & 0.4 \\  & F & 0.3 & 0.3 & 0.3 & **0.2** & 0.3 & 0.3 \\   & E & 0.5 & 0.5 & **0.4** & **0.4** & **0.4** & **0.4** \\  & F & 3.9(1) & 3.5 & 2.8 & **2.1** & 3.2 & **2.1** \\   & E & 0.8 & 0.8 & 0.8 & **0.6** & 0.8 & 0.8 \\  & F & 5.8(1) & 5.4 & 5.1 & **3.6** & 5.8 & 4.1 \\   & E & 0.3 & **0.2** & 0.9 & **0.2** & **0.2** & 0.5 \\  & F & 1.9 & 1.6 & 1.3 & **0.9** & 1.8 & 1.6 \\   & E & 1.5 & **1.3** & 1.4 & 1.5 & **1.3** & **1.3** \\  & F & 6.9 & 5.9(1) & 5.9 & 4.9 & 5.8 & **4.8** \\   & E & 0.9 & 0.8 & **0.7** & 0.9 & 0.8 & 0.9 \\  & F & 5.4(1) & 4.6(1) & 4.0 & **2.9** & 4.3 & 3.1 \\   & E & **0.3** & **0.3** & **0.3** & 0.4 & 0.4 & 0.5 \\  & F & 2.0 & 1.7 & 1.6 & 1.8 & 1.9 & **1.5** \\   & E & 0.5 & **0.4** & **0.4** & 0.6 & **0.4** & 0.5 \\  & F & 3.6(1) & 3.1 & 3.1 & **1.8** & 3.2 & 2.1 \\   

Table 2: **rMD17 results.** Energy (E) and forces (F) mean absolute errors in meV and meV/Å, averaged over different splits.

    &  & ANI-MD & GDB7-9 & GDB10-13 & DrugBank & Tripeptides & S66x8 \\   & E & 21.2 & 249.7 & 17.8 & 51.0 & 95.5 & 57.9 & 30.7 \\  & F & 42.0 & 50.8 & 29.2 & 57.4 & 47.7 & 37.9 & 19.0 \\   & E & 17.3 & 69.9 & 14.3 & 36.0 & 42.4 & 40.0 & 27.1 \\  & F & 34.3 & 35.5 & 23.1 & 41.9 & 32.6 & 26.9 & 14.3 \\   

Table 3: **ANI1x and COMP6 results.** Energy (E) and forces (F) mean absolute errors in meV and meV/Å for Equivariant Transformer and TensorNet models trained on ANI1x and evaluated on the ANI1x test set and the COMP6 benchmarks, averaged over different training splits. 43 meV = 1 kcal/mol.

Equivariance, interaction and cutoff ablations.TensorNet can be straightforwardly modified such that features are \((3)\)-equivariant and scalar predictions are \((3)\)-invariant by modifying the matrix products in the interaction mechanism. An interaction product between node features and aggregated messages \(2\,Y^{(i)}M^{(i)}\), instead of \(Y^{(i)}M^{(i)}+M^{(i)}Y^{(i)}\), gives vector and tensor representations which are combinations of even and odd parity contributions. We refer the reader to the Supplementary Material for detailed derivations. Furthermore, the norms \((X^{}X)\) used to produce scalars will only be invariant under rotations, not reflections. This flexibility in the model allows us to study the changes in prediction accuracy when considering \((3)\) or \((3)\) equivariant models. We also evaluated the impact on accuracy for two rMD17 molecules, toluene and aspirin, when modifying the receptive field of the model by changing the cutoff radius and the number of interaction layers, including the case of using the embedding and output modules alone, without interaction layers (0L), with results in Table 5.

The inclusion or exclusion of equivariance and energy invariance under reflections has a significant impact on accuracy. The consideration of the full orthogonal group \((3)\), and therefore the physical symmetries of the true energy function, leads to higher accuracy for both energy and forces. Furthermore, the use of interaction products produces a drastic decrease in errors (note that TensorNet 1L 4.5A and TensorNet 0L 9A have the same receptive field). In line with rMD17 results, a second interaction layer in the case of \(r_{c}=4.5\)A gives an additional but more limited improvement in both energy and force errors. For forces, the use of a second interaction layer with \(r_{c}=9\)A encompassing the whole molecule provides a smaller improvement when compared to \(r_{c}=4.5\)A. We note that for 0L, when the model can be regarded as just a learnable aggregation of local atomic neighborhoods, TensorNet with both cutoff radii achieves for aspirin (the rMD17 molecule on which the model performs the worst) lower mean absolute errors than ANI (16.6 meV and 40.6 meV/A) [7; 43] and SchNet (13.5 meV and 33.2 meV/A) [22; 44].

Computational cost.We found that TensorNet exhibits high computational efficiency, even higher than an equivariant model using Cartesian vectors such as the Equivariant Transformer  in some cases. We provide inference times for single molecules with varying numbers of atoms in Table 6, and in Table 7 we show training steps per second when training on the ANI1x data set, containing molecules with up to 63 atoms.

For molecules containing up to \(\)200 atoms, TensorNet 1L and 2L are faster or similar when compared to the ET, even when its number of message passing layers is reduced (ET optimal performance for MD17 was achieved with 6 layers, found to be one of the fastest neural network potentials in the literature [16; 45]), meaning that energy and forces on these molecules can be predicted with rMD17 state-of-the-art TensorNet models with a lower or similar computational cost than the reduced ET. For larger molecules with thousands of atoms, TensorNet 2L becomes significantly slower. However, TensorNet 1L, which still exhibits remarkable performance on rMD17 (see Table 1), performs on par the reduced ET in terms of speed even for Factor IX, containing 5807 atoms. For training on ANI1x, TensorNet 1L and 2L are faster or comparable to the ET up to a batch size of 64, being the speed for the 2L model being significantly slower for a batch size of 128. Nevertheless, the model with 1 interaction layer is still comparable to the reduced ET.

TensorNet's efficiency is given by properties that are in contrast to state-of-the-art equivariant spherical models. In particular, the use of Cartesian representations allows one to manipulate full tensors or their decomposition into scalars, vectors, and tensors at one's convenience, and Clebsch

    &  &  &  &  &  \\  & (3)\)} & (3)\)} & (3)\)} & (3)\)} \\ 
**Molecule** & 4.5Å & 9Å & 4.5Å & 9Å & 4.5Å & 9Å & 4.5Å & 4.5Å \\   & E & 3.3 & 2.0 & 0.33 & 0.36 & 0.26 & 0.32 & 0.50 & 0.42 \\  & F & 15.7 & 11.5 & 2.0 & 2.2 & 1.7 & 2.0 & 2.9 & 2.4 \\   & E & 9.8 & 7.8 & 2.7 & 2.9 & 2.4 & 2.8 & 3.7 & 3.4 \\   & F & 32.7 & 28.3 & 10.1 & 11.0 & 8.9 & 10.5 & 13.1 & 11.8 \\   

Table 5: **Equivariance, interaction and cutoff ablations results.** Energy (E) and force (F) mean absolute errors in meV and meV/Å for rMD17 toluene and aspirin, averaged over different splits, varying the number of interaction layers, the cutoff radius, and the equivariance group.

Gordan tensor products are substituted for simple 3x3 matrix products. As detailed in the model's architecture, state-of-the-art performance can be achieved by computing these matrix products after message aggregation (that is, at the node level) and using full tensor representations, without having to individually compute products between different irreducible components. When considering an average number of neighbors per atom \(M\) controlled by the cutoff radius and the density, given that matrix products are performed after aggregation over neighbors, these do not scale with \(M\). This is in contrast to spherical models, where tensor products are computed on edges, and therefore display a worse scaling with the number of neighbors \(M\), that is, a worse scaling when increasing the cutoff radius at fixed density. Also, the use of higher-order many-body messages or many message-passing steps is not needed.

## 5 Conclusions and limitations

We have presented TensorNet, a novel \((3)\)-equivariant message-passing architecture leveraging Cartesian tensors and their irreducible representations. We showed that even though the model is limited to the use of rank-2 tensors, in contrast to other spherical models, it achieves state-of-the-art performance on QM9 and rMD17 with a reduced number of parameters, few message-passing steps, and it exhibits a low computational cost. Furthermore, the model is able to accurately predict vector and rank-2 tensor molecular properties on top of potential energies and forces. Nevertheless, the prediction of higher-rank quantities is directly limited by our framework. However, given the benefits of the formalism for the construction of a machine learning potential, TensorNet can be used as an alternative for the exploration of the design space of efficient equivariant models. TensorNet can be found in [https://github.com/torchmd/torchmd-net](https://github.com/torchmd/torchmd-net).