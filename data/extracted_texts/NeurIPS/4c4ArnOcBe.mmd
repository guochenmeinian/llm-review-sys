# PreDiff: Precipitation Nowcasting with

Latent Diffusion Models

 Zhihan Gao

Hong Kong University of Science and Technology

zhihan.gao@connect.ust.hk

&Xingjian Shi

Boson AI

xshiab@connect.ust.hk

&Boran Han

AWS

boranhan@amazon.com

&Hao Wang

AWS AI Labs

howngz@amazon.com

&Xiaoyong Jin

Amazon

jxiaoyon@amazon.com

&Danielle Maddix

AWS AI Labs

dmmaddix@amazon.com

&Yi Zhu

Boson AI

yi@boson.ai

&Mu Li

Boson AI

mu@boson.ai

&Yuyang Wang

AWS AI Labs

yuyawang@amazon.com

Work conducted during an internship at Amazon. Work conducted while at Amazon.

###### Abstract

Earth system forecasting has traditionally relied on complex physical models that are computationally expensive and require significant domain expertise. In the past decade, the unprecedented increase in spatiotemporal Earth observation data has enabled data-driven forecasting models using deep learning techniques. These models have shown promise for diverse Earth system forecasting tasks. However, they either struggle with handling uncertainty or neglect domain-specific prior knowledge; as a result, they tend to suffer from averaging possible futures to blurred forecasts or generating physically implausible predictions. To address these limitations, we propose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1) We develop PreDiff, a conditional latent diffusion model capable of probabilistic forecasts. 2) We incorporate an explicit knowledge alignment mechanism to align forecasts with domain-specific physical constraints. This is achieved by estimating the deviation from imposed constraints at each denoising step and adjusting the transition distribution accordingly. We conduct empirical studies on two datasets: \(N\)-body MNIST, a synthetic dataset with chaotic behavior, and SEVIR, a real-world precipitation nowcasting dataset. Specifically, we impose the law of conservation of energy in \(N\)-body MNIST and anticipated precipitation intensity in SEVIR. Experiments demonstrate the effectiveness of PreDiff in handling uncertainty, incorporating domain-specific prior knowledge, and generating forecasts that exhibit high operational utility.

## 1 Introduction

Earth's intricate climate system significantly influences daily life. Precipitation nowcasting, tasked with delivering accurate rainfall forecasts for the near future (e.g., 0-6 hours), is vital for decision-making across numerous industries and services. Recent advancements in data-driven deep learning (DL) techniques have demonstrated promising potential in this field, rivaling conventional numerical methods  with their advantages of being more skillful , efficient , and scalable . However, accurately predicting the future rainfall remains challenging for data-driven algorithms.

The state-of-the-art Earth system forecasting algorithms [47; 61; 41; 37; 8; 69; 2; 29; 3] typically generate blurry predictions. This is caused by the high variability and complexity inherent to Earth's climatic system. Even minor differences in initial conditions can lead to vastly divergent outcomes that are difficult to predict. Most methods adopt a point estimation of the future rainfall and are trained by minimizing pixel-wise loss functions (e.g., mean-squared error). These methods lack the capability of capturing multiple plausible futures and will generate blurry forecasts which lose important operational details. Therefore, what are needed instead are probabilistic models that can represent the uncertainty inherent in stochastic systems. The probabilistic models can capture multiple plausible futures, generating diverse high-quality predictions that better align with real-world data.

The emergence of diffusion models (DMs)  has enabled powerful probabilistic frameworks for generative modeling. DMs have shown remarkable capabilities in generating high-quality images [40; 45; 43] and videos [15; 23]. As a likelihood-based model, DMs do not exhibit mode collapse or training instabilities like GANs . Compared to autoregressive (AR) models [53; 46; 63; 39; 65] that generate images pixel-by-pixel, DMs can produce higher resolution images faster and with higher quality. They are also better at handling uncertainty [62; 57; 58; 59; 34] without drawbacks like exposure bias  in AR models. Latent diffusion models (LDMs) [42; 52] further improve on DMs by separating the model into two phases, only applying the costly diffusion in a compressed latent space. This alleviates the computational costs of DMs without significantly impairing performance.

Despite DMs' success in image and video generation [42; 15; 66; 36; 32; 56], its application to precipitation nowcasting and Earth system forecasting is in early stages . One of the major concerns is that this purely data-centric approach lacks constraints and controls from prior knowledge about the dynamic system. Some spatiotemporal forecasting approaches have incorporated domain knowledge by modifying the model architecture or adding extra training losses [11; 1; 37]. This enables them to be aware of prior knowledge and generate physically plausible forecasts. However, these approaches still face challenges, such as requiring to design new model architectures or retrain the entire model from scratch when constraints change. More detailed discussions on related works are provided in Appendix B.

Inspired by recent success in controllable generative models [68; 24; 4; 33; 6], we propose a general two-stage pipeline for training data-driven Earth system forecasting model. 1) In the first stage, we focus on capturing the intrinsic semantics in the data by training an LDM. To capture Earth's long-term and complex changes, we instantiate the LDM's core neural network as a UNet-style architecture based on Earthformer . 2) In the second stage, we inject prior knowledge of the Earth system by training a knowledge alignment network that guides the sampling process of the LDM. Specifically the alignment network parameterizes an energy function that adjusts the transition probabilities during each denoising step. This encourages the generation of physically plausible intermediate latent states while suppressing those likely to violate the given domain knowledge. We summarize our main contributions as follows:

* We introduce a novel LDM based model _PreDiff_ for precipitation nowcasting.
* We propose a general two-stage pipeline for training data-driven Earth system forecasting models. Specifically, we develop _knowledge alignment_ mechanism to guide the sampling process of PreDiff. This mechanism ensures that the generated predictions align with domain-specific prior knowledge better, thereby enhancing the reliability of the forecasts, without requiring any modifications to the trained PreDiff model.
* Our method achieves state-of-the-art performance on the \(N\)-body MNIST  dataset and attains state-of-the-art perceptual quality on the SEVIR  dataset.

## 2 Method

We follow [47; 48; 55; 1; 8] to formulate precipitation nowcasting as a spatiotemporal forecasting problem. The \(L_{}\)-step observation is represented as a spatiotemporal sequence \(y=[y^{j}]_{j=1}^{L_{}}^{L_{} H W  C}\), where \(H\) and \(W\) denote the spatial resolution, and \(C\) denotes the number of measurements at each space-time coordinate. Probabilistic forecasting aims to model the conditional probabilistic distribution \(p(x|y)\) of the \(L_{}\)-step-ahead future \(x=[x^{j}]_{j=1}^{L_{}}^{L_{} H W  C}\), given the observation \(y\). In what follows, we will present the parameterization of \(p(x|y)\) by a controllable LDM.

### Preliminary: Diffusion Models

Diffusion models (DMs) learn the data distribution \(p(x)\) by training a model to reverse a predefined noising process that progressively corrupts the data. Specifically, the noising process is defined as \(q(x_{t}|x_{t-1})=(x_{t};}x_{t-1},(1-_{t})I),1  t T\), where \(x_{0} p(x)\) is the true data, and \(x_{T}(0,I)\) is random noise. The coefficients \(_{t}\) follow a fixed schedule over the timesteps \(t\). DMs factorize and parameterize the joint distribution over the data \(x_{0}\) and noisy latents \(x_{i}\) as \(p_{}(x_{0:T})=p(x_{T})_{t=1}^{T}p_{}(x_{t-1}|x_{t})\), where each step of the reverse denoising process is a Gaussian distribution \(p_{}(x_{t-1}|x_{t})=(_{}(x_{t},t),_{}(x_ {t},t))\), which is trained to recover \(x_{t-1}\) from \(x_{t}\).

To apply DMs for spatiotemporal forecasting, \(p(x|y)\) is factorized and parameterized as \(p_{}(x|y)= p_{}(x_{0:T}|y)dx_{1:T}= p(x_{T})_{t=1}^{ T}p_{}(x_{t-1}|x_{t},y)dx_{1:T}\), where \(p_{}(x_{t-1}|x_{t},y)\) represents the conditional denoising transition with the condition \(y\).

### Conditional Diffusion in Latent Space

To improve the computational efficiency of DM training and inference, our _PreDiff_ follows LDM to adopt a two-phase training that leverages the benefits of lower-dimensional latent representations. The two sequential phases of the PreDiff training are: 1) Training a frame-wise variational autoencoder (VAE)  that encodes pixel space into a lower-dimensional latent space, and 2) Training a conditional DM that generates predictions in this acquired latent space.

Frame-wise autoencoder.We follow  to train a frame autoencoder using a combination of the pixel-wise loss (e.g. L2 loss) and an adversarial loss. Different from , we exclude the perceptual loss since there are no standard pretrained models for perception on Earth observation data. Specifically, the encoder \(\) is trained to encode a data frame \(x^{j}^{H W C}\) to a latent representation \(z^{j}=(x^{j})^{H_{x} W_{x} C_{x}}\). The decoder \(\) learns to reconstruct the data frame \(^{j}=(z^{j})\) from

Figure 1: **Overview of PreDiff inference with knowledge alignment.** An observation sequence \(y\) is encoded into a latent context \(z_{}\) by the frame-wise encoder \(\). The latent diffusion model \(p_{}(z_{t}|z_{t+1},z_{})\), which is parameterized by an Earthformer-UNet, then generates the latent future \(z_{0}\) by autoregressively denoising Gaussian noise \(z_{T}\) conditioned on \(z_{}\). It takes the concatenation of the latent context \(z_{}\) (in the blue border) and the previous-step noisy latent future \(z_{t+1}\) (in the cyan border) as input, and outputs \(z_{t}\). The transition distribution of each step from \(z_{t+1}\) to \(z_{t}\) can be further refined as \(p_{,}(z_{t}|z_{t+1},y,_{0})\) via knowledge alignment, according to auxiliary prior knowledge. This denoising process iterates from \(t=T\) to \(t=0\), resulting in a denoised latent future \(z_{0}\). Finally, \(z_{0}\) is decoded back to pixel space by the frame-wise decoder \(\) to produce the final prediction \(\). (Best viewed in color).

the encoded latent. We denote \(z p_{}(z|x)^{L H_{z} W_{z} C_{z}}\) as equivalent to \(z=[z^{j}]=[(x^{j})]\), representing encoding a sequence of frames in pixel space into a latent spatiotemporal sequence. And \(x p_{}(x|z)\) denotes decoding a latent spatiotemporal sequence.

Latent diffusion.With the context \(y\) being encoded by the frame-wise encoder \(\) into the learned latent space as \(z_{}^{L_{} H_{z} W_{z} C_{z}}\) as (1). The conditional distribution \(p_{}(z_{0:T}|z_{})\) of the latent future \(z_{i}^{L_{} H_{z} W_{z} C_{z}}\) given \(z_{}\) is factorized and parameterized as (2):

\[z_{}  p_{}(z_{}|y), \] \[p_{}(z_{0:T}|z_{}) =p(z_{T})_{t=1}^{T}p_{}(z_{t-1}|z_{t},z_{}). \]

where \(z_{T} p(z_{T})=(0,I)\). As proposed by [22; 45],an equivalent parameterization is to have the DMs learn to match the transition noise \(_{}(z_{t},t)\) of step \(t\) instead of directly predicting \(z_{t-1}\). The training objective of PreDiff is simplified as shown in (3):

\[L_{}=_{(x,y),t,(0,I)}\|- _{}(z_{t},t,z_{})\|_{2}^{2}. \]

where \((x,y)\) is a sampled context sequence and target sequence data pair, and given that, \(z_{t} q(z_{t}|z_{0})p_{}(z_{0}|x)\) and \(z_{} p_{}(z_{}|y)\).

Instantiating \(p_{}(z_{t-1}|z_{t},z_{})\).Compared to images, modeling spatiotemporal observation data in precipitation nowcasting poses greater challenges due to their higher dimensionality. We propose replacing the UNet backbone in LDM  with _Earthformer-UNet_, derived from Earthformer's encoder , which is known for its ability to model intricate and extensive spatiotemporal dependencies in the Earth system.

Earthformer-UNet adopts a hierarchical UNet architecture with self cuboid attention  as the building blocks, excluding the bridging cross-attention in the encoder-decoder architecture of Earthformer. More details of the architecture design of Earthformer-UNet are provide in Appendix C.1. We find Earthformer-UNet to be more stable and effective at modeling the transition distribution \(p_{}(z_{t-1}|z_{t},z_{})\). It takes the concatenation of the encoded latent context \(z_{}\) and the noisy latent future \(z_{t}\) along the temporal dimension as input, and predicts the one-step-ahead noisy latent future \(z_{t-1}\) (in practice, the transition noise \(\) from \(z_{t}\) to \(z_{t-1}\) is predicted as shown in (3)).

### Incorporating Knowledge Alignment

Though DMs hold great promise for diverse and realistic generation, the generated predictions may violate physical constraints, or disregard domain-specific prior knowledge, thereby fail to give plausible and non-trivial results [14; 44]. One possible reason for this is that DMs are not necessarily trained on data full compliant with domain knowledge. When trained on such data, there is no guarantee that the generations sampled from the learned distribution will remain physically realizable. The causes may also stem from the stochastic nature of chaotic systems, the approximation error in denoising steps, etc.

To address this issue, we propose _knowledge alignment_ to incorporate auxiliary prior knowledge:

\[(,y)=_{0}(y)^{d}, \]

into the diffusion generation process. The knowledge alignment imposes a constraint \(\) on the forecast \(\), optionally with the observation \(y\), based on domain expertise. E.g., for an isolated physical system, the knowledge \(E(,)=E_{0}(y^{L_{}})\) imposes the conservation of energy by enforcing the generation \(\) to keep the total energy \(E(,)\) the same as the last observation \(E_{0}(y^{L_{}})\). The violation \(\|(,y)-_{0}(y)\|\) quantifies the deviation of a prediction \(\) from prior knowledge. The larger violation indicates \(\) diverges further from the constraints. Knowledge alignment hence aims to suppress the probability of generating predictions with large violation. Notice that even the target \(x\) from training data may violate the knowledge, i.e. \((x,y)_{0}(y)\), due to noise in data collection or simulation.

Inspired by classifier guidance , we achieve knowledge alignment by training a knowledge alignment network \(U_{}(z_{t},t,y)\) to estimate \((,y)\) from the intermediate latent \(z_{t}\) at noising step \(t\). The key idea is to adjust the transition probability distribution \(p_{}(z_{t-1}|z_{t},z_{})\) in (2) during each latent denoising step to reduce the likelihood of sampling \(z_{t}\) values expected to violate the constraints:

\[p_{,}(z_{t}|z_{t+1},y,_{0}) p_{}(z_{t}|z_{t+ 1},z_{}) e^{-_{}\|U_{}(z_{t},t,y)- _{0}(y)\|}, \]

where \(_{}\) is a guidance scale factor. The knowledge alignment network is trained by optimizing the objective \(L_{U}\) in Alg. 1. According to , (5) can be approximated by shifting the predicted mean of the denoising transition \(_{}(z_{t+1},t,z_{})\) by \(-_{}_{}_{z_{t}}\|U_{}(z_{t},t,y)- _{0}(y)\|\), where \(_{}\) is the variance of the original transition distribution \(p_{}(z_{t}|z_{t+1},z_{})=(_{}(z_{t+1},t, z_{}),_{}(z_{t+1},t,z_{}))\). Detailed derivation is provided in Appendix D.

The training procedure of knowledge alignment is outlined in Alg. 1. The noisy latent \(z_{t}\) for training the knowledge alignment network \(U_{}\) is sampled by encoding the target \(x\) using the frame-wise encoder \(\) and the forward noisy noise process \(q(z_{t}|z_{0})\), eliminating the need for an inference sampling process. This makes the training of the knowledge alignment network \(U_{}\) independent of the LDM training. At inference time, the knowledge alignment mechanism is applied as a plug-in, without impacting the trained VAE and the LDM. This modular approach allows training lightweight knowledge alignment networks \(U_{}\) to flexibly explore various constraints and domain knowledge, without the need for retraining the entire model. This stands as a key advantage over incorporating constraints into model architectures or training losses.

## 3 Experiments

We conduct empirical studies and compare PreDiff with other state-of-the-art spatiotemporal forecasting models on a synthetic dataset \(N\)-body MNIST  and a real-world precipitation nowcasting benchmark SEVIR2 to verify the effectiveness of PreDiff in handling the dynamics and uncertainty in complex spatiotemporal systems and generating high quality, accurate forecasts. We impose data-specific knowledge alignment: **energy conservation** on \(N\)-body MNIST and **anticipated precipitation intensity** on SEVIR. Experiments demonstrate that PreDiff under the guidance of knowledge alignment (PreDiff-KA) is able to generate predictions that comply with domain expertise much better, without severely sacrificing fidelity. In what follows, we will present the empirical studies on SEVIR. The results on \(N\)-body MNIST and the corresponding analysis are provided in Appendix A.

### SEVIR Precipitation Nowcasting

Dataset.The Storm EVent ImageRy (SEVIR)  is a spatiotemporal Earth observation dataset which consists of \(384\)\( 384\)\(\) image sequences spanning over 4 hours. Images in SEVIR are sampled and aligned across five different data types: three channels (C02, C09, C13) from the GOES-16 advanced baseline imager, NEXRAD Vertically Integrated Liquid (VIL) mosaics, and GOES-16 Geostationary Lightning Mapper (GLM) flashes. The SEVIR benchmark supports scientific research on multiple meteorological applications including precipitation nowcasting, synthetic radar generation, front detection, etc. Due to computational resource limitations, we adopt a downsampled version of SEVIR for benchmarking precipitation nowcasting. The task is to predict the future VIL up to 60 minutes (6 frames) given 70 minutes of context VIL (7 frames) at a spatial resolution of \(128 128\), i.e. \(x^{6 128 128 1}\), \(y^{7 128 128 1}\).

Evaluation.Following [55; 8], we adopt the Critical Success Index (CSI) for evaluation, which is commonly used in precipitation nowcasting and is defined as \(=\). To count the \(\) (truth=1, pred=1), \(\), \(\)) and \(\), pred=1), the prediction and the ground-truth are rescaled to the range \(0-255\)and binarized at thresholds \(\). We also follow  to report the CSI at pooling scale \(4 4\) and \(16 16\), which evaluate the performance on neighborhood aggregations at multiple spatial scales. These pooled CSI metrics assess the models' ability to capture local pattern distributions. Additionally, we incorporate FVD  and continuous ranked probability score (CRPS)  for assessing the visual quality and uncertainty modeling capabilities of the investigated methods. Similar to Frechet Inception Distance (FID)  for evaluating image generation, FVD estimates the distance between the learned distribution and the true data distribution by comparing the statistics of feature vectors extracted from the generations and the real data. The inception network used in FVD for feature extraction is pre-trained on video classification and is not specifically adapted for processing "unnatural videos" such as spatiotemporal observation data in Earth systems. Consequently, the FVD scores on SEVIR cannot be directly compared with those on natural video datasets. Nevertheless, the relative ranking of the FVD scores remains a meaningful indicator of model ability to achieve high visual quality, as FVD has shown consistency with expert evaluations across various domains beyond natural images . CRPS measures the discrepancy between the predicted distribution and the true distribution. When the predicted distribution collapses into a single value, as in deterministic models, CRPS reduces to Mean Absolute Error (MAE). A lower CRPS value indicates higher forecast accuracy. Scores for all involved metrics are calculated using an ensemble of eight samples from each model.

#### 3.1.1 Comparison to the State of the Art

We adjust the configurations of involved baselines accordingly and tune some of the hyperparameters for adaptation on the SEVIR dataset. More implementation details of baselines are provided in Appendix C.2. The experiment results listed in Table 1 show that probabilistic spatiotemporal forecasting methods are not good at achieving high CSI scores. However, they are more powerful at capturing the patterns and the true distribution of the data, hence achieving much better FVD

Figure 2: A set of example forecasts from baselines and PreDiff on the SEVIR test set. From top to bottom: context sequence \(y\), target sequence \(x\), forecasts from ConvLSTM , Earthformer , VideoGPT, LDM , PreDiff.

scores and CSI-pool16. Qualitative results shown in Fig. 2 demonstrate that CSI is not aligned with human perceptual judgement. For such a complex system, deterministic methods give up capturing the real patterns and resort to averaging the possible futures, i.e. blurry predictions, to keep the scores from appearing too inaccurate. Probabilistic approaches, of which PreDiff is the best, though are not favored by per-pixel metrics, perform better at capturing the data distribution within a local area, resulting in higher CSI-pool16, lower CRPS, and succeed in keeping the correct local patterns, which can be crucial for recognizing weather events. More detailed quantitative results on SEVIR are provided in Appendix E.

#### 3.1.2 Knowledge Alignment: Anticipated Average Intensity

Earth system observation data, such as the Vertically Integrated Liquid (VIL) data in SEVIR, are usually not physically complete, posing challenges for directly incorporating physical laws for guidance. However, with highly flexible knowledge alignment mechanism, we can still utilize auxiliary prior knowledge to guide the forecasting effectively. Specifically for precipitation nowcasting on SEVIR, we use anticipated precipitation intensity to align the generations to simulate possible extreme weather events. We denote the average intensity of a data sequence as \(I(x)^{+}\). In order to estimate the conditional quantiles of future intensity, we train a simple probabilistic time series forecasting model with a parametric (Gaussian) distribution \(p_{}(I(x)|[I(y^{j})])=(_{}([I(y^{j})]),_{}([I (y^{j})]))\) that predict the distribution of the average future intensity \(I(x)\) given the average intensity of each context frame \([I(y^{j})]_{j=1}^{L_{0}}\) (abbreviated as \([I(y^{j})]\)). By incorporating \((,y) I()\) and \(_{0}(y)_{}+n_{}\) for knowledge alignment, PreDiff-KA gains the capability of generating forecasts for potential extreme cases, e.g., where \(I()\) falls outside the typical range of \(_{}_{}\).

Figure 3: A set of example forecasts from PreDiff-KA, i.e., PreDiff under the guidance of anticipated average intensity. From top to bottom: context sequence \(y\), target sequence \(x\), forecasts from PreDiff and PreDiff-KA showcasing different levels of anticipated future intensity (\(_{}+n_{}\)), where \(n\) takes the values of \(4,2,-2,-4\).

Fig. 3 shows a set of generations from PreDiff and PreDiff-KA with anticipated future intensity \(_{}+n_{}\), \(n\{-4,-2,2,4\}\). This qualitative example demonstrates that PreDiff is not only capable of capturing the distribution of the future, but also flexible at highlighting possible extreme cases like rainstorms and droughts with the knowledge alignment mechanism, which is crucial for decision-making and precaution.

According to Table 1, the FVD score of PreDiff-KA (\(34.18\)) is only slightly worse than the FVD score of PreDiff (\(33.05\)). This indicates that knowledge alignment effectively aligns the generations with prior knowledge while maintaining fidelity and adherence to the true data distribution.

## 4 Conclusions and Broader Impacts

In this paper, we propose PreDiff, a novel latent diffusion model for precipitation nowcasting. We also introduce a general two-stage pipeline for training DL models for Earth system forecasting. Specifically, we develop knowledge alignment mechanism that is capable of guiding PreDiff to generate forecasts in compliance with domain-specific prior knowledge. Experiments demonstrate that our method achieves state-of-the-art performance on \(N\)-body MNIST and SEVIR datasets.

Our work has certain limitations: 1) Benchmark datasets and evaluation metrics for precipitation nowcasting and Earth system forecasting are still maturing compared to the computer vision domain. While we utilize conventional precipitation forecasting metrics and visual quality evaluation, aligning these assessments with expert judgement remains an open challenge. 2) Effective integration of physical principles and domain knowledge into DL models for precipitation nowcasting remains an active research area. Close collaboration between DL researchers and domain experts in meteorology and climatology will be key to developing hybrid models that effectively leverage both data-driven learning and scientific theory. 3) While Earth system observation data have grown substantially in recent years, high-quality data remain scarce in many domains. This scarcity can limit PreDiff's ability to accurately capture the true distribution, occasionally resulting in unrealistic forecast hallucinations under the guidance of prior knowledge as it attempts to circumvent the knowledge alignment mechanism. Further research on enhancing the sample efficiency of PreDiff and the knowledge alignment mechanism is needed.

In conclusion, PreDiff represents a promising advance in knowledge-aligned DL for Earth system forecasting, but work remains to improve benchmarking, incorporate scientific knowledge, and boost model robustness through collaborative research between AI and domain experts.

    &  &  \\  & &  &  &  &  &  \\   Persistence & - & 525.2 & 0.0526 & 0.2613 & 0.3702 & 0.4690 \\  UNet  & 16.6 & 753.6 & 0.0353 & 0.3593 & 0.4098 & 0.4805 \\ ConvLSTM  & 14.0 & 659.7 & 0.0332 & 0.4185 & 0.4452 & 0.5135 \\ PredRNN  & 46.6 & 663.5 & 0.0306 & 0.4080 & 0.4497 & 0.5005 \\ PhyDNet  & 13.7 & 723.2 & 0.0319 & 0.3940 & 0.4379 & 0.4854 \\ E3D-LSTM  & 35.6 & 600.1 & 0.0297 & 0.4038 & 0.4492 & 0.4961 \\ Rainformer  & 184.0 & 760.5 & 0.0357 & 0.3661 & 0.4232 & 0.4738 \\ Earthformer  & 15.1 & 690.7 & 0.0304 & **0.4419** & 0.4567 & 0.5005 \\  DGMR  & 71.5 & 485.2 & 0.0435 & 0.2675 & 0.3431 & 0.4832 \\ VideoGPT  & 99.6 & 261.6 & 0.0381 & 0.3653 & 0.4349 & 0.5798 \\ LDM  & 438.6 & 133.0 & 0.0280 & 0.3580 & 0.4022 & 0.5522 \\  PreDiff &  &  &  &  &  &  &  \\ PreDiff-KA (\([-2_{},2_{}]\)) & & & & & & \\   

Table 1: Performance comparison on SEVIR. The Critical Success Index, also known as the intersection over union (IoU), is calculated at different precipitation thresholds and denoted as \(thresh\). \(\) reports the mean of \(\). \(s\) with \(s=4\) and \(s=16\) report the \(\) at pooling scales of \(4 4\) and \(16 16\). Besides, we include the continuous ranked probability score (CRPS) for probabilistic forecast assessment, and the scores of Fr√©chet Video Distance (FVD) for evaluating visual quality.