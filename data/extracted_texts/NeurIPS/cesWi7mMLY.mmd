# Long-Tailed Out-of-Distribution Detection via Normalized Outlier Distribution Adaptation

Wenjun Miao

Beihang University, China

miaowenjun@buaa.edu.cn

&Guansong Pang

Singapore Management University, Singapore

gspang@smu.edu.sg

&Jin Zheng

Beihang University, China

jinzheng@buaa.edu.cn

&Xiao Bai

Beihang University, China

baixiao@buaa.edu.cn

Beihang University, China

###### Abstract

One key challenge in Out-of-Distribution (OOD) detection is the absence of ground-truth OOD samples during training. One principled approach to address this issue is to use samples from external datasets as outliers (_i.e._, pseudo OOD samples) to train OOD detectors. However, we find empirically that the outlier samples often present a distribution shift compared to the true OOD samples, especially in Long-Tailed Recognition (LTR) scenarios, where ID classes are heavily imbalanced, _i.e._, the true OOD samples exhibit very different probability distribution to the head and tailed ID classes from the outliers. In this work, we propose a novel approach, namely _normalized outlier distribution adaptation_ (AdaptOD), to tackle this distribution shift problem. One of its key components is _dynamic outlier distribution adaptation_ that effectively adapts a vanilla outlier distribution based on the outlier samples to the true OOD distribution by utilizing the OOD knowledge in the predicted OOD samples during inference. Further, to obtain a more reliable set of predicted OOD samples on long-tailed ID data, a novel _dual-normalized energy loss_ is introduced in AdaptOD, which leverages class- and sample-wise normalized energy to enforce a more balanced prediction energy on imbalanced ID samples. This helps avoid bias toward the head samples and learn a substantially better vanilla outlier distribution than existing energy losses during training. It also eliminates the need of manually tuning the sensitive margin hyperparameters in energy losses. Empirical results on three popular benchmarks for OOD detection in LTR show the superior performance of AdaptOD over state-of-the-art methods. Code is available at https://github.com/mala-lab/AdaptOD.

## 1 Introduction

Deep neural networks (DNNs) are widely known to be overconfident about what they do not know when applying them to real-world scenarios in open environments , such as autonomous driving  and medical diagnosis . Consequently, the high-confidence predictions can misclassify out-of-distribution (OOD) samples from unknown classes as one of the known or in-distribution (ID) classes . This issue is further amplified when ID samples exhibit a class-imbalanced distribution in Long-Tailed Recognition (LTR) scenarios. This is because head samples often receive similarly high-confident prediction as OOD samples, while the tail samples receive substantiallylower-confident prediction, leading to an indistinguishability between OOD and head samples and the tendency of wrongly detecting tail samples as OOD samples . We address the problem of _long-tailed OOD detection_, aiming at ensuring LTR accuracy while rejecting unknown samples.

One notorious challenge in OOD detection is the lack of ground-truth information on OOD samples, as they can be drawn from any unknown distribution. One popular solution to tackle this challenge is to use samples from external datasets as outliers (_i.e._, samples that do not overlap with ID and OOD samples, also known as pseudo OOD samples) to train OOD detectors . This approach can be implemented by fitting the prediction probability of the outlier data to a prior distribution over the ID classes  or a margin-based global energy function . Despite showing good performance on various benchmarks, all of these methods assume that the distribution of the outliers is well aligned with that of the true OOD samples in the target data. However, the outliers often present a distribution shift compared to the true OOD samples, especially in LTR scenarios , _i.e._, the true OOD samples exhibit very different probability distribution to the head and tailed ID classes from the outliers. Due to the bias toward head classes, the distribution shift is particularly severe w.r.t. the head samples. For example, as shown in Fig. 0(a), the energy distribution of six popular OOD datasets differs significantly from each other, where CIFAR100-LT  is used as the ID dataset. This implies that any of these OOD datasets used as outlier data source can largely mismatch the distribution of the true OOD data if the other five datasets are used as the true OOD data. Such a distribution shift can largely mislead the training of detection models, leading to downgraded detection performance.

To tackle this problem, in this work, we propose a novel approach for OOD detection in LTR, namely Normalized Outlier Distribution Adaptation (**AdaptOD**). Dynamic Outlier Distribution Adaptation (**DODA**) is a key component of AdaptOD. Given a vanilla outlier distribution, DODA performs test-time adaptation (TTA) to dynamically adapt the outlier distribution to the true OOD distribution by utilizing the OOD knowledge embedded in the predicted OOD samples. This reduces the distribution gap between the outlier and the OOD distributions, enabling a more accurate estimation of OOD scores. As illustrated in Fig. 0(b), a large gap exists between the energy distribution of the outlier data (TinyImages80M ) and the true OOD data (SVHN ). By contrast, our adapted outlier distribution is better aligned to the OOD distribution. Importantly, the ground truth of the test data is assumed to be unavailable during TTA, and as we will show in the experiments (see Table 5), DODA based on the predicted OOD samples can well approximate the upper-bound performance obtained when DODA can get access to the ground truth of the test data to perform TTA (_i.e._, an oracle model). There have been a few TTA methods for OOD detection, but they require online model retraining  or feature memory augmentation . By contrast, DODA focuses on the calibration of the outlier distribution, effectively eliminating the retraining or memory overheads.

On the other hand, training OOD detectors using energy loss functions  is a principled approach to learn the vanilla outlier distribution. However, existing energy losses can underestimate the tail class distribution and involve sensitive hyperparameters on energy margins. As a result, the vanilla outlier distribution learned by using these losses often misclassifies tail samples as OOD samples during TTA. This can significantly affect the distribution adaptation in DODA, leading to still a relatively large gap between the adapted outlier distribution and the OOD distribution, as shown in Fig. 0(b). Therefore, AdaptOD introduces a novel Dual-Normalized Energy loss (**DNE**) to

Figure 1: **(a) Mean energy distribution on six OOD datasets with CIFAR100-LT  as ID data. (b) The results of the energy distribution of the OOD dataset SVHN  using our proposed dynamic outlier distribution adaptation (DODA) and an existing energy loss EnergyOE , where the outlier data is taken from TinyImages80M . (c) The results of using both of our proposed DODA and dual-normalized energy loss (DNE). (d) The ratio of the energy of each ID class to the aggregated energy of all ID classes.**

balance energy prediction for imbalanced ID samples and learn a better vanilla outlier distribution for subsequent DODA. Unlike existing energy losses that are focused on sample-wise energy estimation, DNE utilizes both class-wise and sample-wise normalized energy. This helps obtain more balanced prediction energy on the head and tail samples, transferring the energy from the head samples to the tail samples, thereby avoiding the bias toward the head classes (see Fig. 1d). In doing so, DNE is also free of energy margin hyperparameters and enables the learning of a better vanilla outlier distribution. This guarantees a better starting point for the outlier distribution adaptation and the accuracy of the predicted OOD samples at testing time in DODA, and thus yielding substantially better aligned outlier distribution (see Fig. 1c vs. Fig. 1b). Our main contributions are as follows:

* We propose the novel approach AdaptOD for OOD detection in LTR. To our best knowledge, it is the first approach for adapting the outlier distribution to the true OOD distribution from both the training and inference stages.
* In AdaptOD, we introduce two new components, DODA and DNE, to reduce the gap between the learned outlier distribution and the true OOD distribution in the presence of long-tailed ID data. DODA builds upon a vanilla outlier distribution and then dynamically adapts this distribution to the true OOD distribution with the OOD knowledge obtained at testing time. DNE is designed to perform class- and sample-wise normalized energy training, which enforces more balanced prediction energy for imbalanced ID samples, enabling the learning of largely enhanced vanilla outlier distribution for more effective DODA.
* Extensive empirical results on three LTR benchmarks CIFAR10-LT, CIFAR100-LT, and ImageNet-LT using six popular OOD datasets demonstrate that AdaptOD substantially outperforms the state-of-the-art (SOTA) OOD detection methods in various LTR scenarios.

## 2 Related Work

OOD Detection in Long-Tailed Recognition (LTR).In recent years, OOD detection and LTR have been extensively developed. The former determines whether a given input sample belongs to known classes (in-distribution) or unknown classes (out-of-distribution) , while the latter expects to train on class-imbalanced datasets . PASCL  reveals the difficulty of the OOD detection problem in LTR, and establishes performance benchmarks for OOD detection in LTR based on the SC-OOD benchmark . This setting is also extended to medical image analysis , which utilizes a strong data augmentation to discriminate ID data and OOD data. Recent studies  find that fitting the prediction probability of outlier data to a long-tailed distribution is more effective than using a uniform distribution. They specify this distribution based on the number of samples in ID classes or a pre-trained ID model to learn this outlier distribution. However, it is difficult to obtain such an accurate distribution for outliers in LTR. Other studies  attempt to learn an extra outlier class to overcome the need for learning long-tailed distributions of outliers. But they need a more complex model design. More importantly, all these methods assume that the outlier samples can well represent the distribution of the true OOD data, but this often does not hold in practice since OOD data can be sampled from highly different unknown distributions in different application scenarios. Our approach tackles this problem by adapting the outlier distribution to that of the true OOD data.

Test-Time Adaptation (TTA) for OOD Detection.Recently, TTA  has been introduced to OOD detection, in which unlabeled test data that can be seen only once are used to perform online updating pre-trained DNNs for enhancing task performance and quickly adapting to real-world scenarios. There are two primary approaches for TTA in other tasks: retraining the model based on unsupervised objectives  and updating the feature memory for each class . However, unlike these TTA methods that generalize training data to test data and maintain the same label space between them, TTA for OOD detection  addresses the challenge of identifying unknown classes in test data. While training data includes ID data and outlier data, test data comprises not only ID data but also true OOD data consisting of unknown classes that do not overlap with the ID and outlier data. In particular, AUTO  is a recent method that attempts to assign pseudo labels to unlabeled test data, and then directly uses these pseudo-labels and test data to retrain the model online through Outlier Exposure . AdaOOD  utilizes a memory bank to store feature memories of ID data, then updates these memories online during inference, and lastly employs a \(k\)NN-based distance method to detect OOD samples. However, they fail to work well in the LTR scenarios due to the large variation in the heavily imbalanced training ID data. Moreover, these methods require retraining or additional memory overheads. ETLT  attempts to calibrate OOD scores by a linear regression of its input feature but requires a batch-wise inference to obtain sufficient test samples for the regression. DODA instead utilizes the dynamically adapted outlier distribution to calibrate the prediction output of test data during inference without any retraining or memory overhead.

## 3 Approach

Preliminaries.Let \(X^{in}\) denote the input space of the ID data and \(Y^{in}=\{1,2,,k\}\) be the set of \(k\) imbalanced ID classes in the label space. We have genuine OOD data \(X^{true\_out}\) that is different from \(X^{in}\). It is normally assumed that genuine OOD data \(X^{true\_out}\) are not available during training since OOD samples are unknown instances. However, we can obtain auxiliary OOD data from external datasets, which can be used as outliers \(X^{aux\_out}\) to act as surrogate OOD data for training/fine-tuning LTR models. That is, \(X^{aux\_out}\) is still different from \(X^{true\_out}\), but both of them are OOD w.r.t. \(X^{in}\). There is no class overlapping among ID data \(X^{in}\), genuine OOD data \(X^{true\_out}\), and outlier data \(X^{aux\_out}\). Then the training and test sets can be respectively denoted as: \(^{train}=X^{in} X^{aux\_out}\) and \(^{test}=X^{in} X^{true\_out}\).

OOD detection in LTR is to learn a classifier \(f\) with training data \(^{train}\) so that for any test data \(x^{test}\), if \(x\) is drawn from \(X^{in}\) (from either head or tail classes), then \(f\) can classify \(x\) into the correct ID class, whereas if \(x\) is drawn from \(X^{true\_out}\), then \(f\) can detect \(x\) as OOD data.

TTA for OOD detection in LTR is to online update the above pre-trained classifier \(f\) with test data \(^{test}\) during the inference stage, in which for any unlabeled single test sample \(x^{test}\), utilizing pre-trained classifier \(f\) to predict whether \(x\) belongs to ID or OOD data at the current iteration, then using the predicted label and the test sample \(x\) to update the classifier \(f\). At the next iteration, the updated classifier \(f\) is used to identify a new test sample and continuously update the classifier \(f\). Notably, each sample can only be seen by \(f\) once during inference.

### Overview of AdaptOD

The proposed AdaptOD approach is designed to tackle the aforementioned distribution shift issue for OOD detection in LTR. As shown in Fig. 2, AdaptOD consists of two components, namely Dynamic Outlier Distribution Adaptation (DODA) and Dual-Normalized Energy Loss (DNE). DODA dynamically adapts the learned outlier distribution to the true OOD distribution during inference to reduce the distribution gap between them. DNE is designed to perform both class-wise and sample-wise normalized energy training to obtain more balanced prediction energy on imbalanced ID samples, thereby yielding an enhanced vanilla outlier distribution and enabling better distribution adaptation in DODA. Below we introduce each component in detail.

Figure 2: Overview of AdaptOD, which consists of two components, DODA (Left) and DNE (Right). **Left:** Each test sample is assigned a global energy-based OOD score \((x)\) to adapt the outlier distribution \(^{out}\). DODA then uses the adapted outlier distribution \(^{out}\) to calibrate the global energy score \((x)\), obtaining the calibrated global energy score \(^{P}(x)\) as the OOD score. **Right:** For each iteration, DNE first applies Batch Energy Normalization on logit output to obtain the normalized energy, and then utilizes this energy to optimize a dual energy loss function at both the class and sample levels.

### DODA: Dynamic Outlier Distribution Adaptation

Previous OOD detection methods in LTR suffer from a distribution shift between outlier data and true OOD data. This issue can largely limit the performance of these OOD detectors. Therefore, we propose to dynamically adapt the outlier distribution to the true OOD distribution and further use it to calibrate the prediction output of test samples at the inference stage.

Dynamic Distribution Adaptation with Predicted OOD Samples.Recently, energy-based methods , which use a global energy score over the ID classes as an OOD score for each test sample, have achieved SOTA performance for OOD detection in LTR. Motivated by this success, we learn and adapt the vanilla outlier distribution \(^{out}\), which is initialized by the global energy from the LTR model predictions on the outlier data, to that of the true OOD data, and then use a \(^{out}\)-calibrated global energy score as the OOD score. Specifically, given a set of \(k\) ID classes, for any test sample \(x^{test}\), its global energy score \(()\) is defined as :

\[(x)=_{j=1}^{k}e^{f_{j}(x)},\] (1)

where \(f_{j}(x)\) is the logit output of sample \(x\) in class \(j,j\{1,2,,k\}\). Let \(^{out}^{k}\) be an initial outlier distribution. DODA performs test-time adaptation to dynamically adapt the outlier distribution \(^{out}\) to the true OOD distribution based on the OOD knowledge from the samples predicted as OOD during inference. To this end, we designed an OOD filter using training data to identify OOD samples. Since it is easy to obtain the distribution of global energy score for training ID samples, we use an offline method to determine a threshold for filtering OOD samples based on this energy distribution. This avoids adverse effects on the adaptation speed during inference. Formally, given ID examples from training data \(=\{x_{1},x_{2},...,x_{n}\}\), where \(^{in}\) and \(n\) is the number of training ID samples, we estimate the mean \(_{in}\) and standard deviation \(_{in}\) of the global energy distribution by:

\[_{in}=^{n}(x_{i})}{n},_{in}=^{n}((x_{i})-_{in})^{2}}{n-1}}.\] (2)

We then utilize a Z-score-based method to implement the OOD filter, with the Z-score defined as:

\[R=_{in}-_{in},\] (3)

where \(\) is a hyperparameter. \(=3\) is used by default during the inference stage, and this setting works well throughout our experiments. More discussion about \(\) is described in Appendix D.3.3.

To adapt the outlier distribution \(^{out}\), DODA utilizes the predicted OOD samples by the OOD filter to perform a momentum update of \(^{out}\) during inference, so that \(^{out}\) will represent the mean of energy distribution for the predicted OOD samples. The entries in the vanilla outlier distribution \(^{out}\) are initialized from the mean energy distribution of the outlier data, and they are updated in an online fashion. Specifically, when the OOD filter detects the \(t\)-th test sample \(x\) as an OOD sample (_i.e._, its global energy \((x)<R\)), DODA performs an update of \(^{out}\) as follows:

\[^{out}(t+1)=^{out}(t)+e^{f(x)}}{M+1},&(x)<R\\ ^{out}(t),&(x) R\] (4)

where DODA only keep the number of predicted OOD samples \(M\) and current \(^{out}\) during inference.

Calibrated OOD Score based on the Adapted Outlier Distribution.After obtaining the adapted \(^{out}\), we use it to calibrate the global energy score \(()\) and define the OOD score as follows:

\[^{}(x)=_{j=1}^{k}(x)}}{1+^{ out}_{j}},\] (5)

where \(x^{test}\) and \(^{}()\) denotes the calibrated global energy score with the adapted outlier distribution. This way helps reduce the energy proportion of the head classes that true OOD distribution leans toward in the original global energy score \(()\). In doing so, the distribution gap between the outliers and true OOD is effectively reduced in final OOD score \(^{}()\), enabling moreaccurate estimation of OOD scores in heavily imbalanced ID data without incurring any retraining cost or additional memory expense.

### DNE: Dual-Normalized Energy Loss

When using existing global energy loss to obtain the vanilla outlier distribution \(^{out}\), the distribution of tail samples is indistinguishable from that of OOD samples due to the underestimating of tail samples. We are also required to manually tune the sensitive hyperparameters on energy margins under complex class imbalance. These can lead an inaccurate OOD filter used in Eq. 3 and subsequently affect the distribution adaptation in DODA. To tackle these issues, we propose a Dual-Normalized Energy Loss (DNE), which consists of two novel components, namely class-wise normalized energy loss (DNE-C) and sample-wise normalized energy loss (DNE-S). DNE-C is a class-wise training loss for balancing the sum of energy on all ID samples for each ID class, whereas DNE-S is a sample-wise training loss for balancing the sum of energy on all ID classes for each ID sample. DNE learns a balanced prediction energy distribution on imbalanced ID samples, which helps further reduce the bias toward the head classes in Eq. 5, thereby improving vanilla outlier distribution for a better OOD filter in Eq. 3 and a better vanilla outlier distribution \(^{out}\) in Eq. 4. It also provides stable energy margins, eliminating the need of manual tuning of these margins.

Batch Energy Normalization.To this end, we first propose a novel batch energy normalization method, which conducts energy normalization on the logit output of each class for a batch of training samples. In doing so, the energy of each sample is dependent on the energy of other ID samples and OOD samples relative to the same class. This helps transfer the energy knowledge from the head samples to the tail samples, enabling a better estimation for the energy distribution of tail samples.

Formally, let \(^{in} X^{in}\) be one training batch of ID data, with \(^{in}=\{x_{1}^{in},x_{2}^{in},...,x_{b^{in}}^{in}\}\) and \(b^{in}\) be its batch size, and \(^{out} X^{aux\_out}\) be a set of outlier data in a training batch, with \(^{out}=\{x_{1}^{out},x_{2}^{out},...,x_{b^{out}}^{out}\}\) whose set size is \(b^{out}\), then the batch energy normalization \(F_{j}(x_{i})\) for a sample \(x_{i} x^{in} x^{out}\) in class \(j\{1,2,...,k\}\) with classifier \(f\) is defined as:

\[F_{j}(x_{i})=(x_{i})}}{e^{f_{j}(x_{1}^{in})}+...+e^{f_{j}(x_{b^{ in}}^{in})}+e^{f_{j}(x_{1}^{out})}+...+e^{f_{j}(x_{b^{out}}^{out})}},\] (6)

where \(f_{j}(x)\) is the logit output of sample \(x\) in class \(j\). Essentially, we use the logit output of all samples in a training batch on a class \(j\) to normalize the energy prediction of sample \(x\). This largely reduces the energy prediction bias toward the head samples. The energy of the outlier data is included as a calibration modulation. Then, those normalized energy scores are used for the dual-normalized energy losses, DNE-C and DNE-S, to better balance the prediction energy of long-tailed ID samples.

Additionally, compared to the current energy-based method  for OOD detection in LTR that requires manually designed energy margin hyperparameters, batch energy normalization adjusts the energy of the batch samples on each class to the same scale, so it can provide stable energy margins for the balanced training without relying on the training dataset and/or the class imbalance factor, without the need of manually tuning them.

Class-wise Normalized Energy Loss (DNE-C).DNE-C independently regularizes the energy for each class to enhance the normalized energy of ID samples for more class-wise balanced energy. Formally, let \(_{in}=(X^{in},Y^{in})\) and \(_{out}=X^{aux\_out}\), then we can independently minimize the class energy on each class as follows:

\[_{C}=_{j=1}^{k}(_{(,y)_{in}}[ (max(0,m_{in}^{c}-_{j}()))^{2}]+_{ _{out}}[(max(0,_{j}()-m_{out}^{c}))^{2}]),\] (7)

where \(m_{in}^{c}=1\) and \(m_{out}^{c}=0\) are the default margin hyperparameter settings without the need of manual tuning on different datasets (see Appendix C for more details). The class-wise normalized energy \(_{j}(),j\{1,2,...,k\}\) is defined as:

\[_{j}()=_{i=1}^{b}F_{j}(x_{i}),\] (8)where \(b\) is the batch size of the batch \(\) (if \(\) is \(^{in}\) that \(b\) is \(b^{in}\), and \(\) is \(^{out}\) that \(b\) is \(b^{out}\)), and \(x_{i}\) is the \(i\)-th sample in the batch \(\). Notably, even if some classes do not have the corresponding ID samples in a certain training batch, this loss also can work well. This is because there is less distribution shift among classes in the ID data compared to the OOD data. Therefore, the output of ID samples on incorrect ID classes should also be higher than the OOD samples. DNE-C balances the sum of energy on all ID samples for each ID class and distinguishes outlier samples from ID samples, especially for the underestimated tail classes in a class-wise manner.

Sample-wise Normalized Energy Loss (DNE-S).DNE-S independently regularizes the energy for each sample to enhance the energy of ID samples for sample-wise balanced energy. Formally, we minimize the global energy over all classes of each sample as follows:

\[_{S}=_{(x,y)_{in}}[(max(0,m_{in}^{s}- (x)))^{2}]+_{x_{out}}[(max(0,(x) -m_{out}^{s}))^{2}],\] (9)

where \(x^{in}^{out}\), \(m_{in}^{s}=}\) and \(m_{out}^{s}=0\) are the default margin hyperparameter settings that can also work stably regardless of the ID/OOD datasets (see Appendix C). Then the sample-wise normalized energy \((x)\) can be defined as:

\[(x)=_{j=1}^{k}F_{j}(x).\] (10)

After doing this, we can regularize the global energy of the ID data, particularly the low global energy for tail samples. DNE-S efficiently balances the energy between head and tail samples. As a result, the combination of DNE-C and DNE-S can learn substantially more balanced prediction energy of ID samples, facilitating DODA to solve the distribution shift problems.

**Overall Training Objective.** Overall, we utilize the cross-entropy loss, together with our two normalized energy losses, to train our model. The final objective of our DNE training is as follows:

\[_{total}=_{x,y_{in}}[(f(x),y]+ _{dne},\] (11)

where \(\) is a cross-entropy loss, along with the two normalized energy losses:

\[_{dne}=&_{S}+ _{C}=_{(x,y)_{in}}[(max(0,m_{in}^{s}- (x)))^{2}]+_{x_{out}}[(max(0,(x )-m_{out}^{s}))^{2}],\\ &+_{j=1}^{k}(_{(,y)_{in}} [(max(0,m_{in}^{c}-_{j}()))^{2}]+_{ _{out}}[(max(0,_{j}()-m_{out}^{c}))^{2}]) \] (12)

where \(_{C}\) is as defined in Eq. 7 and \(_{S}\) is as defined in Eq. 9. The algorithm of AdaptOD described in Appendix B.

  OOD &  &  &  \\ Dataset & & AUC1 & AP=1\% & AP=-out\% & FP=1 & AUC1 & AP=-in\% & AP=-out\% & FP=1 \\   & EnergyOE  & 95.53 & 97.42 & 92.93 & 18.44 & 79.56 & 86.03 & 70.88 & 79.45 \\  & COCL  & 96.81 & 98.21 & 93.86 & 14.65 & 81.99 & 88.05 & 74.38 & 59.79 \\  & **AdaptOD(Ours)** & **98.22** & **98.81** & **94.91** & **11.60** & **83.88** & **89.43** & **76.47** & **58.47** \\   & EnergyOE  & 96.63 & 92.33 & 98.46 & 14.37 & 86.19 & 81.42 & 91.74 & 34.36 \\  & COCL  & 96.98 & 93.25 & 98.61 & 12.59 & 89.20 & 81.57 & 94.21 & 54.46 \\  & **AdaptOD(Ours)** & **98.13** & **94.34** & **99.11** & **10.33** & **93.90** & **91.32** & **96.86** & **17.63** \\   & EnergyOE  & 84.44 & 85.74 & 84.63 & 61.73 & 61.15 & 67.12 & 56.66 & 91.42 \\  & COCL  & 86.63 & 86.66 & 86.28 & 52.21 & 62.05 & 66.14 & 56.82 & 93.88 \\  & **AdaptOD(Ours)** & **89.05** & **89.93** & **88.22** & **45.51** & **72.77** & **76.37** & **70.58** & **86.04** \\   & EnergyOE  & 88.40 & 91.65 & 84.95 & 46.23 & 70.78 & 79.40 & 55.90 & 90.74 \\  & COCL  & 90.43 & 92.52 & 87.03 & 46.12 & 71.87 & 81.89 & 57.12 & 83.93 \\  & **AdaptOD(Ours)** & **91.40** & **93.85** & **88.18** & **42.77** & **72.87** & **82.06** & **58.92** & **88.24** \\   & EnergyOE  & 94.00 & 94.78 & 93.70 & 28.42 & 81.61 & 86.57 & 69.16 & 80.57 \\  & COCL  & 94.85 & 95.43 & 93.98 & 27.48 & 84.10 & 89.89 & 69.80 & 74.67 \\  & **AdaptOD(Ours)** & **96.16** & **96.84** & **95.86** & **24.12** & **85.70** & **90.55** & **72.70** & **70.20** \\   & EnergyOE  & 92.51 & 84.26 & 97.14 & 33.63 & 79.12 & 63.38 & 89.09 & 81.43 \\  & COCL  & 93.97 & 87.36 & 97.56 & 32.25 & 80.30 & 68.65 & 89.16 & 77.83 \\   & **AdaptOD(Ours)** & **95.19** & **89.56** & **98.44** & **29.22** & **83.27** & **68.82** & **91.44** & **71.63** \\  

Table 1: Comparison of AdaptOD with EnergyOE and COCL on six OOD datasets.

## 4 Experiments

### Experiment Settings

Datasets.Following , we use three popular long-tailed datasets CIFAR10-LT , CIFAR100-LT  and ImageNet-LT  as ID data \(X^{in}\). The default imbalance ratio is set to \(=100\) on CIFAR10/100-LT. TinyImages80M  is used as the outlier data \(X^{aux\_out}\) for CIFAR10/100-LT and ImageNet-Extra  is used as outlier data for ImageNet-LT. We use six datasets CIFAR , Texture , SVHN , LSUN , Places365  and TinyImageNet , all of which are introduced in the SC-OOD benchmark  as the OOD test set for CIFAR10/100-LT, and ImageNet-1k-OOD  as the OOD test set for ImageNet-LT. More details about the datasets are presented in Appendix A.1.

Implementation Details.Our AdaptOD is compared with seven SOTA OOD detection methods on long-tailed data, including two popular methods: OE  and EnergyOE , and five recent methods: PASCL , EAT , Class Prior , BERL , and COCL . Further, we also compare AdaptOD with two SOTA TTA methods for OOD detection, including AUTO  and AdaOOD . We use ResNet18  as our backbone on CIFAR10/100-LT and ResNet50  on ImageNet-LT. Following fine-tuning-based methods OE , EnergyOE , and BERL , our approach AdaptOD employs a similar training strategy to them that we obtain a pre-trained model with only ID data and fine-tune this model with both ID data and outlier data. The reported results are averaged over six independent runs. More details about the implementation details are presented in Appendix A.2.

   &  &  \\   & AUC\({}^{}\) & AP-in\({}^{}\) & AP-out\({}^{}\) & FPRL & ACC\({}^{}\) & AUCI & AP-in\({}^{}\) & AP-out\({}^{}\) & FPRL \(\) & ACC\(\) \\  OE  & 89.76 & 89.45 & 88.22 & 53.19 & 73.59 & 73.52 & 75.06 & 67.27 & 86.30 & 39.42 \\ EnergyOE  & 91.92 & 91.03 & 91.97 & 33.80 & 74.57 & 76.40 & 77.32 & 72.24 & 76.33 & 41.32 \\ PASCL  & 90.99 & 90.56 & 89.24 & 42.90 & 77.08 & 73.32 & 74.84 & 67.18 & 79.38 & 43.10 \\ EAT  & 92.87 & 91.76 & 92.40 & 32.42 & 81.31 & 75.45 & 76.02 & 70.87 & 77.83 & 46.23 \\ Class Prior  & 92.08 & 91.17 & 90.86 & 34.42 & 74.33 & 76.03 & 77.31 & 72.26 & 76.43 & 40.77 \\ BERL  & 92.56 & 91.41 & 91.94 & 32.83 & 81.37 & 77.75 & 78.61 & 73.10 & 74.86 & 45.88 \\ COCL  & 93.28 & 92.24 & 92.89 & 30.88 & 81.56 & 78.25 & 79.37 & 73.58 & 74.09 & 46.41 \\  OE +**ODOA(Ours)** & 91.62 & 90.55 & 89.39 & 49.02 & 73.59 & 75.46 & 77.14 & 69.88 & 83.67 & 39.42 \\ EnergyOE +**ODOA(Ours)** & 93.36 & 92.17 & 92.97 & 93.82 & 74.57 & 79.40 & 80.89 & 76.54 & 72.63 & 41.32 \\ BERL +**ODOA(Ours)** & 93.77 & 92.62 & 93.15 & 29.41 & 81.37 & 79.45 & 81.15 & 75.52 & 70.51 & 45.88 \\ COCL +**ODOA(Ours)** & 93.89 & 93.06 & 93.39 & 29.25 & 81.56 & 79.81 & 81.26 & 75.93 & 70.14 & 46.41 \\   & **94.69** & **93.89** & **94.12** & **27.26** & **82.27** & **81.93** & **83.09** & **77.83** & **67.37** & **47.91** \\  

Table 2: Comparison to different long-tailed OOD detection methods.

  Training & TTA &  &  \\ Method & ^{}\)} & AP-in\({}^{}\) & AP-out\({}^{}\) & FPRL\(\) & AUC\({}^{}\) & AP-in\({}^{}\) & AP-out\({}^{}\) & FPRL\(\) \\   & w/o TTA & 89.76\(\)1.82 & 89.45\(\)9.16\(\)8.78 & 87.22\(\)6.43 & 53.19\(\)6.48 & 73.52\(\)0.18 & 75.06\(\)1.09 & 67.27\(\)1.07 & 86.30\(\)0.48 \\  & AUTO  & 90.49\(\)1.82 & 89.83\(\)3.82 & 87.45\(\)5.83 & 52.63\(\)0.47 & 73.93\(\)0.39 & 75.98\(\)0.61 & 67.74\(\)0.85 & 85.71\(\)1.10 \\  & AdaOOD  & 90.89\(\)0.28 & 90.17\(\)0.53 & 87.88\(\)0.58 & 51.44\(\)0.56 & 74.67\(\)0.85 & 76.53\(\)0.64 & 67.89\(\)0.82 & 85.34\(\)0.94 \\  & **DODA(Ours)** & 91.62\(\)0.22 & 90.55\(\)0.48 & 89.39\(\)4.69 & 49.02\(\)0.44 & 75.46\(\)0.17 & 77.14\(\)0.59 & 69.88\(\)0.88 & 83.67\(\)0.18 \\   & w/o TTA & 91.92\(\)0.19 & 91.03\(\)0.15 & 91.97\(\)0.38 & 38.30\(\)0.76 & 76.04\(\)0.73 & 77.23\(\)0.73 & 72.41\(\)0.76 & 76.33\(\)1.10 \\  & AUTO  & 92.48\(\)0.19 & 91.43\(\)0.38 & 92.44\(\)0.39 & 31.99\(\)0.78 & 77.65\(\)0.78 & 78.11\(\)0.74 & 74.18\(\)0.70 & 74.66\(\)0.96 \\  & AdaOOD  & 92.28\(\)0.29 & 91.63\(\)0.16 & 91.73\(\)0.68 & 32.83\(\)0.59 & 77.67\(\)0.82 & 78.47\(\)0.74 & 74.05\(\)0.59 & 74.86\(\)0.98 \\  & **DODA(Ours)** & 93.36\(\)0.29 & 92.17\(\)0.59 & 92.97\(\)0.40 & 30.82\(\)0.53 & 79.40\(\)0.19 & 80.89\(\)0.96 & 76.54\(\)0.44 & 72.63\(\)0.39 \\   & w/o TTA & 92.56\(\)0.14 & 91.48\(\)0.15 & 91.45\(\)0.38 & 28.38\(\)0.38 & 77.75\( Measures.Following , we use the below common metrics for OOD detection and ID classification: (1) FPR is the false positive rate of OOD examples when the true positive rate of ID examples is at 95%, (2) AUC computes the area under the receiver operating characteristic curve of detecting OOD samples, (3) AP measures the area under the precision-recall curve, which can be either AP-in in which ID samples are treated as positive or AP-out in which OOD samples are regarded as positive, and (4) ACC calculates the classification accuracy of the long-tailed ID data. The reported results are averaged over six independent runs with different random seeds by default.

### Empirical Results

AdaptOD vs. Other OOD Detection Methods in LTR.Table 1 presents the comparison of our AdaptOD with two SOTA OOD detectors in LTR (EnergyOE , COCL ) on CIFAR10/100-LT using six OOD test datasets. These fine-grained results are not available for the other competing methods and thus they are omitted in this table. AdaptOD shows the best performance in all four metrics on each of the six OOD datasets. Table 2 shows the comparison of our AdaptOD with SOTA OOD detectors in LTR on CIFAR10/100-LT, which is the average performance over six OOD test datasets. Following the previous methods , we report our accuracy with AdjLogit  for a fair comparison. AdaptOD is also the best performers in the averaged results when comparing to all seven competing methods. This consistent improvement and SOTA performance of AdaptOD on both ID and OOD data indicate that the distribution gap between the outlier samples and the true OOD samples is effectively reduced by AdaptOD. Notably, the improvement is large on the near OOD dataset CIFAR , which cannot be achieved by previous SOTA methods .

DODA as an Enabler to Existing Methods.Table 2 also presents the results of our proposed component DODA in using as a plug-in to tackle the distribution shift problem in four SOTA methods (OE, EnergyOE, BERL, and COCL) on CIFAR10/100-LT. It shows that DODA can consistently enhance the OOD detectors in all four metrics, demonstrating the strong capability of DODA in reducing the learned outlier distribution gap to the distribution of the true OOD data (see Appendix D for more details). The consistent improvement of having DODA as 'plug-and-play' indicates the presence of the distribution shift problem encountered by existing SOTA detectors and the universal effectiveness of DODA in tackling the problem. Note that AdaptOD as a whole achieves consistent and substantial improvement over the four DODA-enabled models, showcasing that the other component of AdaptOD, DNE, helps to learn balanced ID prediction energy and better align the adapted outlier distribution to the true OOD one.

AdaptOD vs. Other TTA Methods for OOD Detection.Table 3 shows the comparison of AdaptOD with two SOTA TTA methods AUTO and AdaODD for OOD detection on CIFAR10/100-LT. To have a straightforward and extensive comparison, we compare DODA with the two TTA methods, all of which are added on top of the same training method. In the experiments, we use five training methods, including four SOTA long-tailed OOD detection methods and our proposed DNE method. It is impressive that our DODA component consistently remains the best performer when the TTA methods are combined with all five different training methods on both ID datasets. DODA achieves better performance in all four OOD detection metrics across five OOD training methods, indicating that DODA is a stronger and more generic TTA method for different OOD detectors. Moreover, the combination of our training method DNE and TTA method DODA, which is our approach AdaptOD as a whole, achieves the best performance across all 20 possible combinations.

Performance on Large-scale ID Data. To demonstrate the scalability of our approach, we also perform experiments on the large-scale ID dataset ImageNet-LT. The empirical results are presented in Table 4, which shows that our approach AdaptOD also achieves the SOTA performance in both the OOD detection performance and the ID classification accuracy.

### Further Analysis of AdaptOD

Ablation Study.The effectiveness of our two proposed components, DODA and DNE, have been justified in Table 3. Here we provide a more fine-grained analysis of DODA and its combination to two improved energy losses used in DNE, \(_{C}\) (Eq. 7, denoted as DNE-C) and \(_{S}\) (Eq. 9, denoted as DNE-S), in Table 5, with EnergyOE  used as baseline. The results show the important contribution of each component to the overall superior performance of the full model AdaptOD. Further, we compare AdaptOD to an oracle model that utilizes the ground true OOD data to update the outlier distribution \(^{out}\) in DODA. It shows that AdaptOD has only a small performance gap to the oracle model, indicating that AdaptOD can well approximate the true OOD distribution by the predicted labels of the OOD samples, without involving any ground truth during TTA.

OOD Data Exploitation in TTA.To independently evaluate the effectiveness of exploiting OOD data to adapt the outlier distribution, we report the performance of three TTA methods with an increasing number of labeled OOD samples based on our DNE in Fig. 3. All three TTA methods achieve increasing performance for OOD detection in LTR with more and more true OOD data used for the adaptation. However, AUTO and AdaOOD struggle with the difference between training and testing ID data at the early stage of inference, while AdaptOD can utilize the adapted outlier distribution to quickly adapt to the true OOD distribution and achieve significantly improved performance.

## 5 Conclusion

To address the distribution shift problem in long-tailed OOD detection, we propose a novel approach called AdaptOD. It utilizes a novel normalized energy-based loss - dual-normalized energy loss (DNE) - to learn balanced prediction energy on imbalanced ID samples and enhanced vanilla outlier distribution, then uses a dynamic outlier distribution adaptation (DODA) to adapt the outlier distribution to the true OOD distribution. DODA is shown to be a significantly improved TTA method than existing TTA methods for OOD detection. We also show that DNE can be used to support DODA with its specially designed energy training for better test-time distribution adaptation. Experiments on three popular benchmarks demonstrated that AdaptOD significantly enhances the performance of both OOD detection and long-tailed classification.

   &  &  &  &  &  \\   & & AUC & & AP-in & AP-out & FPRL & AUC & AP-in & AP-out & FPRL & AUC\(\) & AP-in & AP-out\(\) & FPRL \\  Baseline (EnergyODA ) & 91.92 & 91.93 & 91.93 & 33.80 & 76.40 & 77.32 & 72.24 & 76.33 & 69.43 & 45.12 & 84.75 & 76.89 \\  ✗ & ✗ & ✗ & 80.33 & 81.46 & 77.02 & 78.71 & 67.42 & 68.29 & 63.86 & 58.44 & 58.33 & 38.40 & 77.61 & 89.73 \\ ✓ & ✗ & ✗ & 92.63 & 92.05 & 92.46 & 30.17 & 78.10 & 80.22 & 74.17 & 71.65 & 71.71 & 45.99 & 86.37 & 74.31 \\ ✗ & ✓ & ✗ & 92.12 & 91.54 & 92.33 & 31.85 & 78.99 & 79.74 & 72.76 & 74.97 & 71.11 & 45.59 & 85.77 & 76.83 \\ ✗ & ✗ & ✓ & 91.98 & 91.36 & 91.92 & 32.44 & 76.53 & 77.46 & 72.55 & 74.62 & 70.55 & 45.36 & 84.95 & 77.02 \\ ✗ & ✓ & ✓ & 92.77 & 92.18 & 92.62 & 31.48 & 77.92 & 89.77 & 73.92 & 74.44 & 72.04 & 46.53 & 86.06 & 75.82 \\ ✓ & ✓ & ✗ & 93.81 & 93.32 & 93.58 & 28.69 & 80.07 & 82.13 & 75.73 & 68.64 & 73.14 & 47.61 & 87.19 & 73.67 \\ ✓ & ✗ & ✓ & 93.49 & 92.98 & 93.02 & 92.52 & 79.76 & 81.89 & 75.31 & 69.19 & 72.76 & 47.32 & 86.83 & **74.48** \\ ✓ & ✓ & ✓ & **94.69** & **93.89** & **94.12** & **27.26** & **81.93** & **83.09** & **77.83** & **67.37** & **74.32** & **49.02** & **88.63** & **72.91** \\   & 95.33 & 94.75 & 94.96 & 25.02 & 83.60 & 85.09 & 78.85 & 65.37 & 75.84 & 50.20 & 89.97 & 70.71 \\  

Table 5: Ablation study results on CIFAR10-LT, CIFAR100-LT and ImageNet-LT.

Figure 3: The average performance over six OOD datasets on CIFAR100-LT with an increasing percentage of true OOD samples fed to TTA methods.