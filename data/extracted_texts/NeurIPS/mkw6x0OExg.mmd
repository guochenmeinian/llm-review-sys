# Explanations that reveal all through the

definition of encoding

Aahlad Puli1, Nhi Nguyen1, Rajesh Ranganath

New York University

Equal contribution38th Conference on Neural Information Processing Systems (NeurIPS 2024).

###### Abstract

Feature attributions attempt to highlight what inputs drive predictive power. Good attributions or explanations are thus those that produce inputs that retain this predictive power; accordingly, evaluations of explanations score their quality of prediction. However, evaluations produce scores better than what appears possible from the values in the explanation for a class of explanations, called encoding explanations. Probing for encoding remains a challenge because there is no general characterization of what gives the extra predictive power. We develop a definition of encoding that identifies this extra predictive power via conditional dependence and show that the definition fits existing examples of encoding. This definition implies, in contrast to encoding explanations, that non-encoding explanations contain all the informative inputs used to produce the explanation, giving them a "what you see is what you get" property, which makes them transparent and simple to use. Next, we prove that existing scores (roar, fresh, eval-x) do not rank non-encoding explanations above encoding ones, and develop stripe-x which ranks them correctly. After empirically demonstrating the theoretical insights, we use stripe-x to show that despite prompting an llm to produce non-encoding explanations for a sentiment analysis task, the llm-generated explanations encode.

## 1 Introduction

Artificial intelligence can unlock information in data that was previously unknown. In medicine, for example, using AI, researchers have shown that electrocardiograms are predictive of structural heart conditions  or new-onset diabetes . Good predictions often lead one to ask what in the input is important for a prediction; this question is a driving factor behind research in interpretability and explainability . One primary direction in interpretability seeks to produce explanations that are subsets of the input that retain the predictability of the label. These types of explanations and interpretations are called feature attributions and have been used to find factors associated with debt defaults , to demonstrate that detecting COVID-19 from chest radiographs can rely on non-physiological signals , and to discover a new class of antibiotics .

Several methods exist for producing feature attributions or explanations. While some methods compute functions of model gradients  or look at predictability after removing features , other methods attribute scores to different inputs by treating them as players in a game  or amortize their explanations by learning a single model to select subsets for each instance . Choosing one from the many feature attribution methods requires an evaluation. There are, however, many approaches to evaluation itself: qualitative ones , which are limited to cases where humans have precise knowledge about the inputs relevant to prediction, and quantitative ones , which do not require human knowledge.

Intuitively, a good evaluation method for feature attributions should assign higher scores to explanations that select inputs that are more predictive of the label. However, evaluations that score explanations based on the predictability of the label from the explanation face one major challenge: _encoding_. Informally, an encoding explanation is one where the explanation predicts the label beyond what seems plausible from the values of the inputs themselves. The top left panel of Figure 1 shows an explanation that predicts the label of dog or cat depending on whether theexplanation is a pixel on the right half or left half of the image respectively. Many explanation methods fit the description of encoding [20; 21]. Further, given that many evaluations only look at the quality of prediction, encoding can go undetected, rendering the evaluations ineffective at picking explanations. In contrast, non-encoding explanations predict the label well only when the values in the explanation do, making them easy to reason about.

In addressing encoding, this work makes the following contributions:

* **Develops a simple statistical definition of encoding** via a conditional dependence property.
* Confirms the introduced definition captures all existing ad hoc encoding instances.
* Shows that **non-encoding explanations are easy to use** because they retain all the predictive inputs used to build them, meaning that predictive non-encoding explanations reveal inputs that predict the label to their users, **and thus have a "what you see is what you get" property**.
* Formalizes evaluations' sensitivity to encoding as _weak detection_ (optimal scoring explanations are non-encoding) and _strong detection_ (non-encoding explanations score above encoding ones).
* Demonstrates that the evaluations roar and fresh do not weakly detect encoding.
* Proves that eval-x weakly detects encoding, but does not strongly detect encoding.
* Develops **stripe-x** and proves that it **strongly detects encoding**.
* Uses stripe-x to show that despite prompting an llm to produce non-encoding explanations for a sentiment analysis task, the llm-generated explanations encode.

Figure 1 provides an overview of this paper.

## 2 Evaluating explanations

We focus on explanation methods where the goal is to produce subsets of the input that predict the label [22; 23]. Explanation methods of this form, also called feature attributions, saliency methods [4; 8; 24], or just "explanations," include thresholded rankings from Shapley values [25; 26], lime, and real-x. With \(\) as the label and \(^{d}\) as the inputs, let \(q(,)\) be the joint distribution over them. An explanation method \(e\) maps the inputs \(\) to a binary selection mask \(e()\) over the inputs: \(e:^{d}\{0,1\}^{d}\). The explanation \(_{e()}\) is a pair: the _selection_\(e()\) and the vector of explanation's _values_. For example, if \(=[a,b,c]\) is three-dimensional and \(e()=\), \(_{e()}\) consists of the binary mask \(e()\) and the values associated with the inputs that correspond to the indices in \(e()\) with value \(1\):

\[_{e()}=(e(),[b,c]).\]

Figure 1: **Overview of the paper.** Explanations are produced to find inputs that are relevant to predicting a label. However, explanations can predict the label well due to the selection being predictive of the label beyond the explanation’s values. Such explanations are called encoding. In contrast, predicting instead from a non-encoding explanation is equivalent to predicting from the values in the explanation. When explanations are evaluated purely based on the quality of prediction, encoding can go undetected. We classify existing evaluations into non-detectors and weak detectors and develop a strong detector, called stripe-x.

We keep track of the indices because the same value can lead to different predictions depending on the index it appears at; for example, in predicting mortality from patient vital signs, a heart rate above \(110\) can occur in healthy patients but a temperature of \(110^{}\)F is almost always fatal. Equivalently, like in existing work [15; 16; 17; 20], one can choose \(_{e()}\) to retain the values in the explanation in the same position and mask out those not selected: \(_{e()}=e()+(1-e()) \). For concision, we overload the word "explanation" to mean the explanation method instead of the random variable \(_{e()}\) when it is clear from context.

Choosing between explanation methods requires evaluation. Explanation methods seek to return inputs that predict the label, so existing evaluations consider how well the explanation \(_{e()}\) predicts the label \(\)[15; 16; 17; 20]. To score explanations based on predictive power, an evaluation method \(()\) takes as arguments both the explanation \(e()\) and the joint distribution \(q(,)\): \((q,e)\). Without loss of generality let higher be better.

Encoding: A disconnect between the predictiveness of explanations and the predictiveness of their values

We give a simple example of encoding to build intuition for the disconnect between predicting the label from the explanation and predicting the label from the explanation's values. Imagine that the goal is to explain which set of vital signs signal bacterial pneumonia as the diagnosis compared to the common cold. Consider the explanation method that selects the patient's height when the true probability of pneumonia is high given the whole set of observables (including labs, symptoms, and vital signs) and otherwise selects the patient's hair color. Physiologically, height and hair color do not indicate that the patient has pneumonia, meaning that this explanation should not be highly predictive of the label. However, by construction, pneumonia is likely exactly when the explanation selects height, and predicting the label from the explanation achieves the same accuracy as predicting with the full conditional \(_{y\{\}}q(=y)\). Thus, despite the explanation method only selecting physiologically irrelevant inputs, the explanation predicts the label well.

Encoding examples such as the one above are neither contrived nor unique. For example, Jethani et al.  show that certain procedures that learn to explain, when applied to MNIST digit classification, yield explanations that select a background, black pixel that predicts the label at an accuracy \(>90\%\); (see Figure 1 in ). Other examples of encoding explanations that predict better than what is expected from the explanation's values exist [20; 21]. Encoding explanations should not score optimally under a good evaluation because the explanation selects inputs that do not appear to predict the label. However, without a general characterization of the discrepancy in predictive power for encoding, finding explanations whose values predict well remains a challenge. The next section develops a definition of encoding.

## 3 Formalizing encoding

Intuitively, encoding is a phenomenon where the information about the label in the explanation \(_{e()}\) exceeds what is known from the _explanation's values_. As the input \(\) determines the explanation \(_{e()}\), the quality of predicting the label \(\) from the explanation relies on the information about the label transmitted from \(\) to \(_{e()}\). There are two pathways for this transmission; we elaborate below.

Denoting the values in a subset \(\) by \(_{}\), compare the event this subset takes the values \(\), i.e. \(_{}=\) to the event that the explanation's selection is \(\) and that the explanation's values are \(\), i.e., \(_{e()}=(,)\).

1. Knowing that the explanation is \(_{e()}=(,)\) implies not only that the values in the explanation are determined as \(_{}=\), but also that the selection is determined as \(e()=\).
2. In reverse, knowing that the values of a subset of inputs are \(_{}=\) and knowing the selection \(e()=\) implies that the explanation are \(_{e()}=(,)\).

Putting these two points together yields an equality between events:

\[\{:_{e()}=(,)\}=\{ :e()=\}\{:_{}=\}.\] (1)

Thus, the two pathways for information between \(\) and the explanation \(_{e()}\) are the selection \(e()\) and explanation's values \(_{}\); see Figure 2. Existing work makes similar intuitive observations but stops short of formalizing the additional predictive power in an explanation \(_{e()}\)[20; 21].

[MISSING_PAGE_EMPTY:4]

Marginal encoding (mrg).This type of encoding occurs when some inputs determine which other inputs determine the label. For example, in Figure 3, the color determines whether the top right patch produces the label or the bottom right patch. Inputs that _control_ where the label comes from are named _control flow inputs_. For a real-world example, consider the following example from Jethani et al. , where the goal is to predict mortality for patients with chest pain. A lab value that checks for heart injury and acts like a control flow input is troponin. Abnormal troponin indicates that cardiac issues exist and cardiac imaging would inform mortality. Normal troponin on the other hand can indicate that chest pain is unrelated to cardiac health and a chest X-ray would instead inform mortality. Selecting one image or the other, but not the control flow input, conceals information about why the image was relevant to the label.

**Formalization.** In Appendix B, we provide mathematical formulations of each informal example and show that they fall under the definition of encoding in Def: Encoding: position-based encoding (Appendix B.3), prediction-based encoding (Appendix B.4), and marginal encoding (Appendix B.5). The key intuition behind all of these is that the explanation \(e()\) varies with inputs other than the selected ones, and these additional inputs provide information about the label beyond the selected ones. Next, we turn to detecting encoding via quantitative evaluations.

## 4 Detecting encoding in explanations

This section develops notions of sensitivity to encoding for evaluation methods, and uses the mathematical definition of encoding developed in the previous section to establish which methods detect encoding and which do not. Hsia et al.  suggest that evaluation methods like eval-x can be gamed to produce high scores for encoding explanations by optimizing the evaluation. To study this case, we introduce the notion of _weak detection_. If the optimal score of an evaluation of explanations does not permit encoding, then that evaluation is said to weakly detect encoding:

**Definition 2** (**Weak detection of encoding**).: _An evaluation \((q,e)\) of explanations weakly detects encoding if the optimal explanations \(e^{*}\), i.e. \((q,e^{*})=_{e}(q,e)\), are non-encoding._

Weak detection provides a recipe for finding non-encoding explanations: find the explanation that achieves the maximum score of a weak detector. However, such a recipe would only work when optimizing without constraints because weak detection does not require non-encoding explanations to have a better score than any encoding one. Requiring this leads to the definition of _strong detection_.

**Definition 3** (**Strong detection of encoding**).: _An evaluation \((q,e)\) strongly detects encoding if for any encoding explanation \(e\) and non-encoding explanation \(e^{}\), \((q,e^{})>(q,e)\)._

Evaluations that are not weak detectors cannot be strong detectors because they score some encoding explanation optimally.

### Do existing evaluation methods detect encoding?

Here, we consider whether several techniques for evaluating explanations: roar, fresh, and eval-x can detect encoding. We analyze these evaluations on the following distribution \(q\)

\[=[_{1},_{2},_{3}](0.5 )^{ 3},=_{1}&0.9& 1-_{1}&_{3}=1,\\ _{2}&0.9& 1-_{2}&_{3}=0.\] (3)

Consider the explanation \(e_{}()=_{1}=\) if \(_{3}=1\) and \(_{2}=\) otherwise; this encodes because \(_{3}\) is used to create the explanation and \(_{3}\) predicts the label conditional on \(_{1}\) when \(_{_{1}}=1\). This is a marg explanation (see Section 3.1).

Figure 3: **Left**: Consider data where the color in the left half determines whether the label “cat”, “dog”) is produced from the top or bottom image on the right. **Right**: A marg encoding explanation that produces only the top or the bottom animal image based on the color. The animal image alone says less about the label than knowing the animal image and the color. Knowing the selection determines the color and thus provides additional information about the label.

roar and fresh do not weakly detect encoding.** roar evaluates explanations by predicting the label from the inputs not selected by the explanation, denoted as \(_{-e()}\); roar scores explanations optimally if the predictions from the remaining covariates are as random as predicting without any covariates at all. In other words, roar checks how informative \(_{-e()}\) is of \(\) and provides the highest score when \([origin={c}]{$$}_{-e()}\). In contrast, fresh evaluates explanations by predicting the label from the explanation after removing all other inputs, denoted as \((_{e()})\). For example, assume we are given an input \(=\)"Visually stunning. My favorite movie ever" and an explanation \(e()\) that selects the words "stunning" and "favorite". Then, the explanation is \(_{e()}=(,["])\), whereas \((_{e()})=["," ",,,]\), which drops the information about where the selected words are in the input. See Appendix B.6 for a formal definition of \((_{e()})\). fresh checks how predictive \(q((_{e()}))\) is and assigns an optimal score if the prediction is as good as that of \(q()\). These conditions hold for \(e_{}()\) in eq.3:

**Proposition 1**.: _For the data generating process (dgp) in eq.3, roar and fresh assign their respective optimal scores to the encoding explanation \(e_{}()\)._

The proof is in Appendix B.6. The intuition is that the encoding explanation \(e_{}()\) always selects the input that informs the label given the control flow \(_{3}\); removing the only conditionally informative input means that \(_{-e_{}()}\) has no information about \(\). In turn, roar scores an encoding explanation \(_{-e_{}()}\) optimally, meaning it does not even weakly detect encoding. In addition, \((_{e()})\) provides the exact same information about the label regardless of which position it came from. As a result, \([origin={c}]{$$}(_{e()})\), so fresh scores \(e_{}()\) optimally. Even though fresh attempts to drop the information about the selection \(=e()\) during evaluation, \((_{e()})\) remains a function of \(_{e()}=(,)\), so extra information can still be transmitted through the selection \(\). Thus, roar and fresh are not weak detectors of encoding.

eval-x weakly detects encoding but not strongly. eval-x is an evaluation method and is sometimes called the surrogate model score. The eval-x score with log-probabilities is

\[(q,e):=_{(,) q(_{e( )})}_{q(_{e()}=(,))}[ q(_{}= )].\] (4)

This score measures the expected log-likelihood of the labels given the input values chosen by the explanation method \(e\) and is grounded in the sampling distribution \(q\). Log-likelihoods are maximized by matching the true distribution, this leads to eval-x's weak detection:

**Theorem 1**.: _If \(e()\) is eval-x optimal, then \(e()\) is not encoding._

Appendix A.4 gives a proof. The proof shows that at optimality, the prediction from the _values_ of explanation has to match the prediction from the full inputs. In turn, given the values there is no additional information in \(\) about \(\), which means the explanation indicator \(_{}\) is independent of \(\); this violates Def: Encoding, which proves the non-encoding nature of eval-x-optimal explanations.

To test strong detection for eval-x, we consider explanations constrained to select one input. Such reductive constraints appear in practice because the goal of producing an explanation is often to aid humans who benefit from reduced complexity. Such constraints prohibit explanations from reaching eval-x's optimal score. Compare \(e_{encode}()\) with a non-encoding constant explanation:

**Proposition 2**.: _Let \(e_{}()=_{3}\). Then, for the dgp in eq.3, eval-x\((q,e_{})>(q,e_{})\)._

Thus, eval-x is not a strong detector. The intuition is that the first two coordinates \(_{1}\), \(_{2}\) predict the label when selected by \(e_{}\), while the control flow feature does not predict the label. eval-x not being a strong detector means that optimizing eval-x over a reductive set may yield an encoding explanation. In this case, \(e_{}\) is one of the eval-x-optimal reductive explanations (Lemma6).

### stripe-x: a strong detector of encoding

Encoding explanations induce the dependence between the label \(\) and the identity of the selection \(_{}=[e()=]\) given the values in the explanation \(_{}\) (Def: Encoding). This dependence can be tested for by building on conditional independence tests . Rather than testing, direct quantification of dependence can be useful for when combining with other scores, which can be done using instantaneous conditional mutual information:

\[_{q}(e):=_{(,) q(_{e() })}(_{};_{}=)().\] (5)encode-meter is \(0\) only when Def: Encoding does not hold:

**Proposition 3**.: _encode-meter \(_{q}(e)=0\) if and only if \(e\) is not encoding._

The proof is in Appendix A.5. Combining eval-x with encode-meter weighed by \(\) yields a method we call the strongly information-penalized evaluator (stripe-x):

\[_{}(q,e):=(q,e)-_{q}(e).\] (6)

For a large enough \(\), the added penalty term pushes down the scores of encoding explanations below that of all non-encoding ones, meaning that stripe-x is a strong detector of encoding:

**Theorem 2**.: _With finite \(()\) and \(()\), for any explanation that encodes \(e\) and any that does not encode \(e^{}\), there exists an \(^{*}\) such that \(>^{*}\)stripe-x\({}_{}(q,e^{})\ >_{}(q,e)\)._

The proof is in Appendix A.5. The intuition behind the proof is that for a large enough \(\), the stripe-x scores for any encoding explanations will be dominated by the information term, and thus will become smaller than any non-encoding explanation whose score is lower bounded by the negative marginal entropy, \(-_{q}()\). Table 1 summarizes the weak and strong detection properties of different evaluations.

**Estimating stripe-x.** The first component of stripe-x is eval-x. Computing eval-x (eq. (4)) requires an estimate of the predictive distribution of the label \(\) given \(_{}\), \(q(_{})\). Estimation can be done in two ways. The first way makes use of a surrogate model trained to predict the label from different random subsets using masked tokens [9; 20]. The second way to compute eval-x (eq. (4)) relies on conditional generative models [30; 31]. Both hyperparameters and a combination of the estimators can be chosen to maximize the average log-likelihood on a held-out validation set across random input subsets.

To estimate the second part of stripe-x, the encode-meter, first expand the mutual information terms in encode-meter, \(_{q}(e)\), in terms of expected \(\):

\[_{q}(e)=_{(,) q(_{ ()})}_{ q(_{}=)}[q(_{}_{ }=,)\ \|\ q(_{}_{}=) ].\] (7)

The outer expectation can be estimated using samples from the data and the inner expectation over \(\) can be estimated using the eval-x model \(q(_{})\). The distributions over \(_{}\) can be estimated using a classifier of \(_{}\) that randomly masks the label and masks different subsets of the inputs. Further details and a generative way to estimate stripe-x are in Appendix C.1 and Appendix C.3; full algorithms are given in Appendix D.

**stripe-x in practice.** Using stripe-x to choose between explanations is straightforward: pick the one with the larger score. However, like other evaluations that use learned models, misestimation can pose a problem. With large \(\), non-encoding explanations with misestimated encode-meter will have bad stripe-x scores, while with small \(\) some encoding explanations can have good scores. Across all experiments, we set \(=20\), which yielded stripe-x scores for known encoding explanations worse than known non-encoding explanations.

## 5 Experiments

This section consists of two parts. The first part demonstrates the weak and strong detection capabilities of the evaluations roar, eval-x, and stripe-x in a simulated setting and on an image recognition task. To demonstrate these capabilities, we run these evaluations on instantiations of posi, pred, and marg. Additionally, we evaluate an existing method that learns to explain under a reductive constraint, called real-x . The second part shows how stripe-x enables discovering encoding explanations in the wild, without specific knowledge of the dgp or the method that produced the explanation. We employ stripe-x to uncover encoding in explanations generated by a large language model (llm) for predicting sentiments from movie reviews.

### Empirically studying the detection of encoding in a simulated setting

We construct two examples with binary labels \(\): one discrete input \(\) and one that is a hybrid of continuous and discrete components. Both use one binary input in \(\{0,1\}^{5}\) as a control flow

   Method & Weak & Strong \\  roar & ✗ & ✗ \\ fresh & ✗ & ✗ \\ eval-x & ✓ & ✗ \\  stripe-x & ✓ & ✓ \\   

Table 1: The weak and strong detection properties of different evaluation methods. Existing scores like roar and fresh, are not weak detectors, which in turn means they are not strong detectors either.

variable and switch the inputs that \(\) depends on. In both dgps, \(\) only depends on \(_{1}\) if \(_{3}=1\), and only on \(_{2}\) if \(_{3}=0\); this means that \(_{4},_{5}\) are purely noise. For both dgps, \(\) is sampled per the following distribution where \(_{3}\) determines the subset the \(\) depends on

\[q(=1)=1[_{3}=1]q(_ {1},_{3})+1[_{3}=0]q(_{2}, _{3}).\] (8)

Thus, eval-\(^{*}\) is achieved by an explanation of size \(2\): \(e()=_{1}+_{3}\) if \(_{3}=1\) else \(e()=_{2}+_{3}\). See Appendix C.4 for details; the exact dgps are given in eq. (36) and eq. (37).

**Encoding explanations.** Table 2 describes the encoding explanations we consider for this setting. In Appendix C.4, we check that Def: Encoding holds for these explanations in the discrete dgp by estimating the role of the unselected inputs in affecting the explanation and the role of \(_{}\) in predicting \(\) beyond \(_{}\); a characterization of Def: Encoding to support this check is in Lemma 1.

**ROAR and FRESH fails to weakly detect encoding.** To empirically test the analysis about roar and fresh, we study whether the two evaluations weakly detect encoding. In this study, we compare each evaluation's score on the all-inputs explanation, which is optimal, to the score assigned to marg. marg ignores \(_{3}\) which is required to produce the label \(\) in eq. (8). roar log-likelihoods for marg and the all-inputs explanation are approximately \(-()=-0.69\) for both dgps. In addition, the value of the input that marg selects alone contains all the information about the label regardless of whether marg selects \(_{1}\) or \(_{2}\). Thus, fresh log-likelihoods for marg and the all-inputs explanation are both approximately \(-0.29\) for both dgps. This result validates that roar and fresh are not weak detectors because they do not separate the optimal explanation from all encoding explanations.

**eval-x is a weak detector of encoding but not a strong detector.** eval-x log-likelihood scores are given in blue in Figures 3(a) and 3(b). eval-x, being a weak detector, scores the encoding constructions (posi, pred, and marg) strictly lower than the log-likelihood of the optimal explanation eval-x. However, the eval-x score for the marg explanation is \(-0.4\), which is above the score of \(-0.6\) achieved by a non-encoding explanation \(e()=_{1}\); _thus, eval-x is not a strong detector._

**Strong detector stripe-x prices out all the encoding explanations.** Figures 3(a) and 3(b) report stripe-x scores for the same set of explanations as above; stripe-x scores are shown in red. _Strong detector stripe-x scores the non-encoding explanations above the negative entropy \(-_{q}()=-0.69\) and scores every encoding construction under that threshold._

### Detecting encoding on images of dogs and cats

The goal of this section is to study the encoding detection capabilities of roar, eval-x, and stripe-x on real data. We consider an image recognition task like the one in Figure 3 with labels and images from the cats_vs_dogs dataset from the Tensorflow package . We break images of size \(64 64\) into \(4\) patches each of size \(32 32\). In left-right then top-down order, let \(_{1},_{2},_{3},_{4}\) be the upper left, upper right, bottom left, and bottom right patches respectively; \(_{1},_{3}\) capture color, and \(_{2},_{4}\) are the animal images. With annot(image) denoting the annota

Figure 4: eval-x and stripe-x scores of the \(3\) encoding constructions and the non-encoding constant explanation \((e()=_{1})\), for both dgps. eval-x, being only a weak detector, assigns suboptimal scores to all encoding explanations (\(<\)), but scores some encoding explanations above the constant explanation. On the other hand, stripe-x, being a strong detector, pushes down the scores of all the encoding explanations below that of the non-encoding constant explanation that always selects \(_{1}\).

tion in the cats_vs_dogs dataset the image having a dog or a cat, the label is assigned as:

\[=[_{1}=]\,\,[ _{2})]+[_{1}=]\,\,[_{4})]\]

We consider three encoding explanations (posi, pred, marg) and two non-encoding ones: 1) optimal, which selects the color and the patch that produces the label as dictated by the color, and 2) denoted fixed, which always outputs the bottom right patch \(_{4}\). Appendix C.6 gives details.

We report the scores assigned to each explanation by roar, eval-x, and stripe-x in Table 3. roar scores two encoding explanations pred and marg as high as the optimal explanation, meaning it is not even a weak detector. posi, pred, and marg all score worse than the optimal explanation under both the weak detector eval-x and the strong detector stripe-x. However, eval-x scores one non-encoding explanation (fixed) worse than two encoding ones, meaning it is not a strong detector. Being a strong detector, stripe-x scores the fixed explanation above the negative marginal entropy \(-_{q}()=-0.69\) and scores every encoding construction under that threshold.

**Evaluating explanations produced by real-x.** We ran real-x to learn explanations for the simulated setting and the image recognition task. In the simulated setting, real-x is run to select one input; Appendix C.4 gives details. In the image recognition task, real-x is run to select one of the four patches as an explanation; Appendix C.6 gives details. In both the simulated setting (see Figures 3(a) and 3(b)) and the image recognition task (see Table 3), real-x fails to achieve the optimal eval-x score while achieving a stripe-x score below the threshold of negative marginal entropy \(-_{q}()=-0.69\). Upon investigation, we found that real-x produced an explanation that matched the marg construction on at least \(80\%\) of the inputs in the simulated setting. On the image recognition task, real-x explanation matched the marg explanation on the whole dataset. In both cases, stripe-x, being a strong detector, correctly alerts that the real-x explanation encodes.

### Encoding in llm-generated explanations

One can detect encoding in any explanation by checking if the stripe-x score falls below the negative marginal entropy. Recent work uses lllms to produce explanations; e.g.  prompt an llm to generate explanations for reasoning tasks which are later used to improve smaller models. If the llm explanation encodes, the smaller model can falsely ignore the informative inputs the larger model's explanation depends on and yet does not reveal. In this section, we evaluate explanations generated by an llm, Llama 3, for a sentiment analysis task. We consider reviews that take one of two forms: with ADJ1 and ADJ2 as adjectives, the review is

* 'My day was <ADJ1> and the movie was <ADJ2>. that is it' or
* 'My day was <ADJ1> and the movie was <ADJ2>. oh wait, reverse the adjectives'.

The second sentence in the review acts as a "control flow" input and determines whether ADJ1 or ADJ2 describes the sentiment about the movie. We prompt Llama 3 (see Appendix C.8) to predict the sentiment and select a few words from the review that were important for that sentiment; the selected parts form the generated explanation. To discourage encoding, the prompt explicitly instructs the llm to select all the words that the llm based the selection on; such an explanation, by Lemma 1, would be non-encoding. On the \(5\) most common selections \(e()\) generated by the llm, we compute the eval-x score and the encode-meter\(_{q}(e)\). The resulting stripe-x score is \(-2.78\), falling short of the negative entropy \(-_{q}()=-0.69\), meaning the llm encodes. We investigated why.

As an example, consider the review 'My day was resplendent and the movie was hollow. that is it.'; the llm selects only hollow in the explanation. However, the llm instead selects resplendent when that is it is switched to oh wait, reverse the adjectives. Such occurrences are common. On \(>70\%\) of the data, the llm selects the word that describes the movie

    & roar & fresh & eval-x & stripe-x \\  opt & \(\) & \(\) & \(\) & \(\) \\ fixed & \(0.59\) & \(-0.64\) & \(-0.64\) & \(-0.64\) \\  posi & \(0.51\) & \(-0.69\) & \(-0.70\) & \(-5.98\) \\ pred & \(\) & \(\) & \(-0.51\) & \(-1.40\) \\ marg & \(\) & \(\) & \(-0.53\) & \(-1.02\) \\   

Table 3: roar, fresh, eval-x, and stripe-x scores for the image recognition experiment. Higher is better. roar and fresh score two encoding explanations pred and marg as high as the optimal explanation, meaning they are not even weak detectors. eval-x being only a weak detector scores posi, pred, and marg all worse than the optimal explanation under both eval-x but not the non-encoding constant explanation (\(e()=_{4}\)), denoted fixed. stripe-x being a strong detector scores the non-encoding explanations above the negative marginal entropy \(-_{q}()=-0.69\) and scores every encoding construction under that threshold.

but does not select the second sentence in the review which controls which adjective describes the movie; this is akin to marg encoding. Thus, the llm-generated explanation encodes by looking at the control flow input in the second sentence to find the correct adjectives, but failing to select the control flow input. Such an explanation falsely indicates that only the adjectives are relevant to predicting the label. In contrast, a non-encoding explanation would, in addition to the adjective that describes the movie, reveal control flow words that indicate which adjective predicts the label.

In summary, despite being instructed to include all the words that were looked at when producing the explanation, the llm encodes. Building non-encoding explanations with llms may require an extensive search over prompts or finetuning guided by scores from stripe-x.

## 6 Discussion

When an explanation is encodes, predictions from the explanation become disconnected from predictions from the values in the explanations. Such explanations can select values with little relevance to the label and yet score highly on the many existing predictive evaluations. We develop a simple statistical definition of encoding. Inverting this definition shows that when non-encoding explanations predict the label, users know the values of those inputs selected in the explanation predict the label. We then show that existing evaluations are either non-detectors (roar,fresh) or only weak detectors (eval-x). Motivated by this, we introduce a new strong detector, stripe-x. After empirically demonstrating the detection capabilities (or lack thereof) of said evaluations, we use stripe-x to discover encoding in llm-generated explanations.

**More related work.** Other investigations into evaluating explanations focused on label leakage [26; 34] and faithfulness [18; 35; 36; 37; 38]. Label leakage is similar to encoding in that additional information is in the explanation, but focuses on explanations that have access to both the inputs and the observed label; we leave extending Def: Encoding to leakage to the future. Faithfulness, intuitively, asks that the explanation reflect the process of how a label is predicted from the inputs; a formalization does not exist. Jacovi and Goldberg  note the need to define faithfulness formally. Encoding explanations are not faithful to the process of making an explanation because predictive inputs outside those selected by the explanation control the explanation.

**Limitations and the future.** Using misestimated models in evaluations (like eval-x) may lead to mistakes (see Appendix B.8 for an example). The retinal fundus experiment from Jethani et al.  is an example where misestimation leads to reductive explanations scoring higher than using the full input. Misestimation can be due to poor uncertainty or due to dependence on shortcut features. One fruitful direction is to use better uncertainty estimates, like conformal inference  or calibration , or employ robustness methods [41; 42] to ameliorate errors due to misestimation. Another direction is use tricks like reinforce-style gradients to construct non-encoding explanations by optimizing stripe-x. Explanations that output subsets may not always help humans interpret the mechanism of the prediction. For example, imagine one wants to understand why a model correctly answers the question "Who won the ski halfpipe at the X-games 3 years after her debut in 2021?" with "Eileen Gu". A subset explanation may return "3 years after her debut in 2021" and "ski halfpipe", but that does not help a human interpret how the model predicts. A better interpretation would be to make the model output, "3 years after 2021 is 2024. Eileen Gu won in 2024, and debuted in 2021." Such explanations can also encode information about the prediction in the text produced as a rationale . An important direction here would be to extend the definitions of weak and strong detectors of encoding to evaluations of free-text rationales.

**Data versus Model Explanations.** Even with the formal definitions of explanation methods, there is a question about what is being explained: the data or the model. These two concepts often get blended together in the literature [11; 20]. We clarify this point and abstract the choice away as two different ways to produce the joint distribution \(q(,)\). In _data explanation_, the distribution under which a feature attribution method seeks to output a subset of inputs that predict the label should be the population distribution of the data . If, instead, the goal is _model explanation_, the goal should not be to highlight inputs that predict the label well in samples of the data; rather it should be _to predict the label well in samples from the model._ Formally, a model with parameters \(\) is a conditional distribution, \(p_{}()\). To target a model explanation, a feature attribution method would aim to output a subset of inputs that predict the label under the distribution \(F()p_{}()\).