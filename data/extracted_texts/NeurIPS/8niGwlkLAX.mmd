# Sparse Deep Learning for Time Series Data: Theory and Applications

Mingxuan Zhang

Department of Statistics

Purdue University

zhan3692@purdue.edu

Yan Sun

Department of Biostatistics, Epidemiology, and Informatics

University of Pennsylvania

yan.sun@pennmedicine.upenn.edu

Faming Liang

Department of Statistics

fmliang@purdue.edu

###### Abstract

Sparse deep learning has become a popular technique for improving the performance of deep neural networks in areas such as uncertainty quantification, variable selection, and large-scale network compression. However, most existing research has focused on problems where the observations are independent and identically distributed (i.i.d.), and there has been little work on the problems where the observations are dependent, such as time series data and sequential data in natural language processing. This paper aims to address this gap by studying the theory for sparse deep learning with dependent data. We show that sparse recurrent neural networks (RNNs) can be consistently estimated, and their predictions are asymptotically normally distributed under appropriate assumptions, enabling the prediction uncertainty to be correctly quantified. Our numerical results show that sparse deep learning outperforms state-of-the-art methods, such as conformal predictions, in prediction uncertainty quantification for time series data. Furthermore, our results indicate that the proposed method can consistently identify the autoregressive order for time series data and outperform existing methods in large-scale model compression. Our proposed method has important practical implications in fields such as finance, healthcare, and energy, where both accurate point estimates and prediction uncertainty quantification are of concern.

## 1 Introduction

Over the past decade, deep learning has experienced unparalleled triumphs across a multitude of domains, such as time series forecasting , natural language processing , and computer vision . However, challenges like generalization and miscalibration  persist, posing potential risks in critical applications like medical diagnosis and autonomous vehicles.

In order to enhance the performance of deep neural networks (DNNs), significant research efforts have been dedicated to exploring optimization methods and the loss surface of the DNNs, see, e.g., , which have aimed to expedite and direct the convergence of DNNs towards regions that exhibit strong generalization capabilities. While these investigations are valuable, effectively addressing both the challenges of generalizationand miscalibration require additional and perhaps more essential aspects: consistent estimation of the underlying input-output mapping and complete knowledge of the asymptotic distribution of predictions. As a highly effective method that addresses both challenges, sparse deep learning has been extensively studied, see, e.g., . Nevertheless, it is important to note that all the studies have been conducted under the assumption of independently and identically distributed (i.i.d.) data. However, in practice, we frequently encounter situations where the data exhibits dependence, such as time series data.

The primary objective of this paper is to address this gap by establishing a theoretical foundation for sparse deep learning with time series data. Specifically, we lay the foundation within the Bayesian framework. For RNNs, by letting their parameters be subject to a mixture Gaussian prior, we establish posterior consistency, structure selection consistency, input-output mapping estimation consistency, and asymptotic normality of predicted values. We validate our theory through numerical experiments on both synthetic and real-world datasets. Our approach outperforms existing state-of-the-art methods in uncertainty quantification and model compression, highlighting its potential for practical applications where both accurate point prediction and prediction uncertainty quantification are of concern.

Additionally, we would elaborate the contribution of this paper in a broader context of statistical modeling. As discussed in , two distinct cultures exist for statistical modeling: the 'data modeling culture' and the 'algorithmic modeling culture'. The former focuses on simple generative models that explain the data, potentially lacking a consistent estimate of the true data-generating mechanism due to the model's inherent simplicity. The latter, on the other hand, aims to find models that can predict the data regardless of complexity. Our proposed method occupies a middle ground between these two cultures. It seeks to identify a parsimonious model within the realm of complex models, while also ensuring a consistent estimation of the true data-generating mechanism. From this perspective, this work and related ones, e.g., , represent a hybridization of the 'algorithmic modeling culture' and the 'data modeling culture', which holds the potential to expedite advancements in modern data science.

## 2 Related Works

**Sparse deep learning**. Theoretical investigations have been conducted on the approximation power of sparse DNNs across different classes of functions . Recently,  has made notable progress by integrating sparse DNNs into the framework of statistical modeling, which offers a fundamentally distinct neural network approximation theory. Unlike traditional theories that lack data involvement and allow connection weights to assume values in an unbounded space to achieve arbitrarily small approximation errors with small networks , their theory  links network approximation error, network size, and weight bounds to the training sample size. They show that a sparse DNN of size \(O(n/(n))\) can effectively approximate various types of functions, such as affine and piecewise smooth functions, as \(n\), where \(n\) denotes the training sample size. Additionally, sparse DNNs exhibit several advantageous theoretical guarantees, such as improved interpretability, enabling the consistent identification of relevant variables for high-dimensional nonlinear systems. Building upon this foundation,  establishes the asymptotic normality of connection weights and predictions, enabling valid statistical inference for predicting uncertainties. This work extends the sparse deep learning theory of  from the case of i.i.d data to the case of time series data.

**Uncertainty quantification**. Conformal Prediction (CP) has emerged as a prominent technique for generating prediction intervals, particularly for black-box models like neural networks. A key advantage of CP is its capability to provide valid prediction intervals for any data distribution, even with finite samples, provided the data meets the condition of exchangeability . While i.i.d. data easily satisfies this condition, dependent data, such as time series, often doesn't. Researchers have extended CP to handle time series data by relying on properties like strong mixing and ergodicity . In a recent work,  introduced a random swapping mechanism to address potentially non-exchangeable data, allowing conformal prediction to be applied on top of a model trained with weighted samples. The main focus of this approach was to provide a theoretical basis for the differences observed in the coverage rate of the proposed method. Another recent study by  took a deep dive into the Adaptive Conformal Inference (ACI) , leading to the development of the Aggregation Adaptive Conformal Inference (AgACI) method. In situations where a dataset contains a group of similar and independent time series, treating each time series as a separate observation, applying a CP method becomes straightforward . For a comprehensive tutorial on CP methods, one can refer to . Beyond CP, other approaches for addressing uncertainty quantification in time series datasets include multi-horizon probabilistic forecasting , methods based on dropout , and recursive Bayesian approaches .

## 3 Sparse Deep Learning for Time Series Data: Theory

Let \(D_{n}=\{y_{1},,y_{n}\}\) denote a time series sequence, where \(y_{i}\). Let \((,,P^{*})\) be the probability space of \(D_{n}\), and let \(_{k}=\{|P^{*}(y_{j} A,y_{k+j} B)-P^{*}(y_{j} A)P^{*}(y_{k+j}  B)|:A,B,j^{+}\}\) be the \(k\)-th order \(\)-mixing coefficient.

**Assumption 3.1**.: The time series \(D_{n}\) is (strictly) stationary and \(\)-mixing with an exponentially decaying mixing coefficient and follows an autoregressive model of order \(l\)

\[y_{i}=^{*}(y_{i-1:i-l},_{i})+_{i}, \]

where \(^{*}\) is a non-linear function, \(y_{i-1:i-l}=(y_{i-1},,y_{i-l})\), \(_{i}\) contains optional exogenous variables, and \(_{i}N(0,^{2})\) with \(^{2}\) being assumed to be a constant.

_Remark 3.2_.: Similar assumptions are commonly adopted to establish asymptotic properties of stochastic processes . For example, the asymptotic normality of the maximum likelihood estimator (MLE) can be established under the assumption that the time series is strictly stationary and ergodic, provided that the model size is fixed . A posterior contraction rate of the autoregressive (AR) model can be obtained by assuming it is \(\)-mixing with \(_{k=0}^{}a_{k}^{1-2/s}<\) for some \(s>2\) which is implied by an exponentially decaying mixing coefficient . For stochastic processes that are strictly stationary and \(\)-mixing, results such as uniform laws of large numbers and convergence rates of the empirical processes  can also be obtained.

_Remark 3.3_.: Extending the results of  to the case that the dataset includes a set of i.i.d. time series, with each time series regarded as an individual observation, is straightforward, this is because all observations are independent and have the same distribution.

### Posterior Consistency

Both the MLP and RNN can be used to approximate \(^{*}\) as defined in (1), and for simplicity, we do not explicitly denote the exogenous variables \(_{i}\) unless it is necessary. For the MLP, we can formulate it as a regression problem, where the input is \(_{i}=y_{i-1:i-R_{l}}\) for some \(l R_{l} n\), and the corresponding output is \(y_{i}\), then the dataset \(D_{n}\) can be expressed as \(\{(_{i},y_{i})\}_{i=1+R_{l}}^{n}\). Detailed settings and results for the MLP are given in Appendix B.3. In what follows, we will focus on the RNN, which serves as an extension of the previous studies. For the RNN, we can rewrite the training dataset as \(D_{n}=\{y_{i:i-M_{l}+1}\}_{i=M_{l}}^{n}\) for some \(l R_{l}<M_{l} n\), i.e., we split the entire sequence into a set of shorter sequences, where \(R_{l}\) denotes an upper bound for the exact AR order \(l\), and \(M_{l}\) denotes the length of these shorter sequences (see Figure 1). We assume \(R_{l}\) is known but not \(l\) since, in practice, it is unlikely that we know the exact order \(l\).

For simplicity of notations, we do not distinguish between weights and biases of the RNN. In this paper, the presence of the subscript \(n\) in the notation of a variable indicates its potential to increase with the sample size \(n\). To define an RNN with \(H_{n}-1\) hidden layers, for \(h\{1,2,,H_{n}\}\), we let \(^{h}\) and \(L_{h}\) denote, respectively, the nonlinear activation function and the number of hidden neurons at layer \(h\). We set \(L_{H_{n}}=1\) and \(L_{0}=p_{n}\), where \(p_{n}\) denotes a generic input dimension. Because of the existence of hidden states from the past, the input \(_{i}\) can contain only \(y_{i-1}\) or \(y_{i-1:i-r}\) for some \(r>1\). Let \(^{h}^{L_{h} L_{h-1}}\) and \(^{h}^{L_{h} L_{h}}\) denote the weight matrices at layer \(h\). With these notations, the output of the step \(i\) of an RNN model can be expressed as

\[(_{i},\{_{i-1}^{h}\}_{h=1}^{H_{n}-1},)=^{H_{n}} ^{H_{n}-1}[^{1}[^{1}_{i}+^{1}_{i-1}^{1}] ], \]where \(_{i}^{h}=^{h}[^{h}_{i}^{h-1}+^{h}_{i-1}^{h}]\) denotes the hidden state of layer \(h\) at step \(i\) with \(_{0}^{h}=\); and \(\) is the collection of all weights, consisting of \(K_{n}=_{h=1}^{H_{n}}(L_{h} L_{h-1})+_{h=1}^{H_{n}-1}(L_{h}^{2})+L _{h}\) elements. To represent the structure for a sparse RNN, we introduce an indicator variable for each weight in \(\). Let \(=\{_{j}\{0,1\}:j=1,,K_{n}\}\), which specifies the structure of a sparse RNN. To include information on the network structure \(\) and keep the notation concise, we redenote \((_{i},\{_{i-1}^{h}\}_{h=1}^{H_{n}-1},)\) by \((_{i:i-M_{l}+1},,)\), as \(\{_{i-1}^{h}\}_{h=1}^{H_{n}-1}\) depends only on \((,)\) and up to \(_{i-1:i-M_{l}+1}\).

Posterior consistency is an essential concept in Bayesian statistics, which forms the basis of Bayesian inference. While posterior consistency generally holds for low-dimensional problems, establishing it becomes challenging in high-dimensional scenarios. In such cases, the dimensionality often surpasses the sample size, and if the prior is not appropriately elicited, prior information can overpower the data information, leading to posterior inconsistency.

Following , we let each connection weight be subject to a mixture Gaussian prior, i.e.,

\[_{j}_{n}N(0,_{1,n}^{2})+(1-_{n})N(0,_{0,n}^ {2}), j\{1,2,,K_{n}\}, \]

by integrating out the structure information \(\), where \(_{n}(0,1)\) is the mixture proportion, \(_{0,n}^{2}\) is typically set to a very small number, while \(_{1,n}^{2}\) is relatively large. Visualizations of the mixture Gaussian priors for different \(_{n}\), \(_{0,n}^{2}\), and \(_{1,n}^{2}\) are given in the Appendix E.

We assume \(^{*}\) can be well approximated by a sparse RNN given enough past information, and refer to this sparse RNN as the true RNN model. To be more specific, we define the true RNN model as

\[(^{*},^{*})=*{arg\,min}_{(,)_{n},\|(_{i:i-M_{l}+1},,)-^{*}(y_{i-1:i-l})\|_{L_{2}()}_{n}}||, \]

where \(_{n}:=(C_{0},C_{1},,p_{n},L_{1},L_{2},,L_{ H_{n}})\) denotes the space of all valid networks that satisfy the Assumption 3.4 for the given values of \(H_{n}\), \(p_{n}\), and \(L_{h}\)'s, and \(_{n}\) is some sequence converging to \(0\) as \(n\). For any given RNN \((,)\), the error \(|^{*}()-(,,)|\) can be decomposed as the approximation error \(|^{*}()-(,^{*},^{*})|\) and the estimation error \(|(,^{*},^{*})-(,,)|\). The former is bounded by \(_{n}\), and the order of the latter will be given in Theorem 3.9. For the sparse RNN, we make the following assumptions:

**Assumption 3.4**.: The true sparse RNN model \((^{*},^{*})\) satisfies the following conditions:

* The network structure satisfies: \(r_{n}H_{n} n+r_{n}+s_{n} p_{n} C_{0}n^{1-}\), where \(0<<1\) is a small constant, \(r_{n}=|^{*}|\) denotes the connectivity of \(^{*}\), \(=_{1 j H_{n-1}}L_{j}\) denotes the maximum hidden state dimension, and \(s_{n}\) denotes the input dimension of \(^{*}\).
* The network weights are polynomially bounded: \(\|^{*}\|_{} E_{n}\), where \(E_{n}=n^{C_{1}}\) for some constant \(C_{1}>0\).

_Remark 3.5_.: Assumption 3.4 is identical to assumption A.2 of , which limits the connectivity of the true RNN model to be of \(o(n^{1-})\) for some \(0<<1\). Then, as implied by

Figure 1: A multi-layer RNN with an input window size of \(k\). We restrict the use of the RNN’s outputs until the hidden states have accumulated a sufficient quantity of past information to ensure accurate predictions.

Lemma 3.10, an RNN of size \(O(n/(n))\) has been large enough for modeling a time series sequence of length \(n\). Refer to  for discussions on the universal approximation ability of the neural network under this assumption; the universal approximation ability can still hold for many classes of functions, such as affine function, piecewise smooth function, and bounded \(\)-Holder smooth function.

_Remark 3.6_.: The existence of the sparse RNN model stems from Lemma 4.1 of , which, through the trick of independent block sequence construction, shows that many properties of the i.i.d processes can be extended to mixing processes. While the lemma was proven for the case of \(\)-mixing, the author did mention her doubts about its applicability to \(\)-mixing. Therefore, at least for the sequences of \(\)-mixing, which implies \(\)-mixing, the non-empty of the sparse RNN set in (4) can be guaranteed for many classes of functions as mentioned in Remark 3.5.

**Assumption 3.7**.: The activation function \(^{h}\) is bounded for \(h=1\) (e.g., sigmoid and tanh), and is Lipschitz continuous with a Lipschitz constant of \(1\) for \(2 h H_{n}\) (e.g., ReLU, sigmoid and tanh).

_Remark 3.8_.: Assumption 3.7 mirrors , except that we require the activation function for the first layer to be bounded. This extra assumption can be viewed as a replacement for the boundedness assumption for the input variables of a conventional DNN.

Let \(d(p_{1},p_{2})\) denote the integrated Hellinger distance between two conditional densities \(p_{1}(y|)\) and \(p_{2}(y|)\). Let \((|D_{n})\) be the posterior probability of an event. Theorem 3.9 establishes posterior consistency for the RNN model with the mixture Gaussian prior (3).

**Theorem 3.9**.: _Suppose Assumptions 3.1, 3.4, and 3.7 hold. If the mixture Gaussian prior (3) satisfies the conditions \(:_{n}=O(1/[M_{l}^{H_{n}}K_{n}[n^{2M_{l}H_{n}}(_{p_{n}})]^{}])\) for some constant \(>0\), \(E_{n}/[H_{n} n+]^{1/2}_{1,n} n^{}\) for some constant \(\), and \(_{0,n}\{1/[M_{l}^{H_{n}}K_{n}(n^{3/2}_{1,n}/H_{n}) ^{2M_{l}H_{n}}],1/[M_{l}^{H_{n}}K_{n}(nE_{n}/H_{n})^{2M_{l}H_{n}}]\}\), then there exists an an error sequence \(_{n}^{2}=O(_{n}^{2})+O(_{n}^{2})\) such that \(_{n}_{n}=0\) and \(_{n}n_{n}^{2}=\), and the posterior distribution satisfies_

\[(d(p_{},p_{^{*}}) C_{n}|D_{n})=O_{P^{*}}(e^{- n_{n}^{2}}), \]

_for sufficiently large \(n\) and \(C>0\), where \(_{n}^{2}=[r_{n}H_{n} n+r_{n}+s_{n} p_{n}]/n\), \(p_{^{*}}\) denotes the underlying true data distribution, and \(p_{}\) denotes the data distribution reconstructed by the Bayesian RNN based on its posterior samples._

### Uncertainty Quantification with Sparse RNNs

As mentioned previously, posterior consistency forms the basis for Bayesian inference with the RNN model. Based on Theorem 3.9, we further establish structure selection consistency and asymptotic normality of connection weights and predictions for the sparse RNN. In particular, the asymptotic normality of predictions enables the prediction intervals with correct coverage rates to be constructed.

Structure Selection ConsistencyIt is known that the neural network often suffers from a non-identifiability issue due to the symmetry of its structure. For instance, one can permute certain hidden nodes or simultaneously change the signs or scales of certain weights while keeping the output of the neural network invariant. To address this issue, we follow  to define an equivalent class of RNNs, denoted by \(\), which is a set of RNNs such that any possible RNN for the problem can be represented by one and only one RNN in \(\) via appropriate weight transformations. Let \((,)\) denote an operator that maps any RNN to \(\). To serve the purpose of structure selection in the space \(\), we consider the marginal posterior inclusion probability (MIPP) approach. Formally, for each connection weight \(i=1,,K_{n}\), we define its MIPP as \(q_{i}=_{}e_{i|(,)}(| ,D_{n})(|D_{n})d\), where \(e_{i|(,)}\) is the indicator of \(i\) in \((,)\). The MIPP approach selects the connections whose MIPPs exceed a threshold \(\). Let \(}_{}=\{i:q_{i}>,i=1,,K_{n}\}\) denote an estimator of \(_{*}=\{i:e_{i|(^{*},^{*})}=1,i=1,,K_{n}\}\). Let \(A(_{n})=\{:d(p_{},p_{^{*}})_{n}\}\) and \((_{n})=_{1 i K_{n}}_{A(_{n})}_{}|e_{i|(,)}-e_{i|(^{*}, ^{*})}|(|,D_{n})(|D_{n})d\), which measuresthe structure difference on \(A()\) for the true RNN from those sampled from the posterior. Then we have the following Lemma:

**Lemma 3.10**.: _If the conditions of Theorem 3.9 are satisfied and \((_{n}) 0\) as \(n\), then_

1. \(_{1 i K_{n}}\{|q_{i}-e_{i|(^{*}, ^{*})}|\}0\) _as_ \(n\)_;_
2. _(sure screening)_ \(P(_{*}}_{})1\) _as_ \(n\)_, for any prespecified_ \((0,1)\)_;_
3. _(consistency)_ \(P(_{*}=}_{0.5})1\) _as_ \(n\)_;_

_where \(\) denotes convergence in probability._

_Remark 3.11_.: This lemma implies that we can filter out irrelevant variables and simplify the RNN structure when appropriate. Please refer to Section 5.2 for a numerical illustration.

Asymptotic Normality of Connection Weights and PredictionsThe following two theorems establish the asymptotic normality of \(()\) and predictions, where \(()\) denotes a transformation of \(\) which is invariant with respect to \((,,)\) while minimizing \(()-^{*}_ {}\).

We follow the same definition of asymptotic normality as in . The posterior distribution for the function \(g()\) is asymptotically normal with center \(g^{*}\) and variance \(G\) if, for \(d_{}\) the bounded Lipschitz metric for weak convergence, and \(_{n}\) the mapping \(_{n}:(g()-g^{*})\), it holds, as \(n\), that

\[d_{}((|D_{n})_{n}^{-1},N(0,G)) 0, \]

in \(P^{*}\)-probability, which we also denote \((|D_{n})_{n}^{-1} N(0,G)\).

The detailed assumptions and setups for the following two theorems are given in Appendix C. For simplicity, we let \(M_{l}=R_{l}+1\), and let \(l_{n}()=}_{i=R_{l}+1}^{n}(p_{ }(y_{i}|y_{i-1:i-R_{l}}))\) denote the averaged log-likelihood function. Let \(H_{n}()\) denote the Hessian matrix of \(l_{n}()\), and let \(h_{i,j}()\) and \(h^{i,j}()\) denote the \((i,j)\)-th element of \(H_{n}()\) and \(H_{n}^{-1}()\), respectively.

**Theorem 3.12**.: _Assume the conditions of Lemma 3.10 hold with \((_{n})=o(})\) and additional assumptions hold given in Appendix C, then \(((()-^{*})|D_{n})  N(,)\) in \(P^{*}\)-probability as \(n\), where \(=(v_{i,j})\), and \(v_{i,j}=E(h^{i,j}(^{*}))\) if \(i,j^{*}\) and \(0\) otherwise._

Theorem 3.13 establishes asymptotic normality of the sparse RNN prediction, which implies prediction consistency and forms the theoretical basis for prediction uncertainty quantification as well.

**Theorem 3.13**.: _Assume the conditions of Theorem 3.12 and additional assumptions hold given in Appendix C. Then \((((,)-(,^{*})|D _{n}) N(0,)\), where \(=_{^{*}}(,^{*})^{ }H^{-1}_{^{*}}(,^{*})\) and \(H=E(-_{^{*}}^{2}l_{n}(^{*}))\) is the Fisher information matrix._

## 4 Computation

In the preceding section, we established a theoretical foundation for sparse deep learning with time series data under the Bayesian framework. Building on , it is straightforward to show that Bayesian computation can be simplified by invoking the Laplace approximation theorem at the maximum _a posteriori_ (MAP) estimator. This essentially transforms the proposed Bayesian method into a regularization method by interpreting the log-prior density function as a penalty for the log-likelihood function in RNN training. Consequently, we can train the regularized RNN model using an optimization algorithm, such as SGD or Adam. To address the local-trap issue potentially suffered by these methods, we train the regularized RNN using a prior annealing algorithm , as described in Algorithm 1. For a trained RNN, we sparsify its structure by truncating the weights less than a threshold to zero and further refine the nonzero weights for attaining the MAP estimator. For algorithmic specifics, refer to Appendix D. Below, we outline the steps for constructing prediction intervals for one-step-ahead forecasts, where \((y_{k-1:k-R_{l}},})\) is of one dimension, and \(}\) and \(}\) represent the estimators of the network parameters and structure, respectively, as obtained by Algorithm 1:

* Estimate \(^{2}\) by \(^{2}=-1}_{i=R_{l}+1}^{n}\|y_{i}-(y_{i-1:i- R_{l}},})\|^{2}\).
* For a test point \(y_{k-1:k-R_{l}}\), estimate \(\) in Theorem 3.13 by \[^{2}=_{}}(y_{k-1:k-R_{l}},})^{}(-_{}}^{2}l_{n}(}))^{-1} _{}}(y_{k-1:k-R_{l}},}).\]
* The corresponding \((1-)\%\) prediction interval is given by \[(y_{k-1:k-R_{l}},}) z_{/2}^{2 }/(n-R_{l})+^{2}},\] where there are \(n-R_{l}\) observations used in training, and \(z_{/2}\) denotes the upper \(/2\)-quantile of the standard Gaussian distribution.

For construction of multi-horizon prediction intervals, see Appendix F.

## 5 Numerical Experiments

### Uncertainty Quantification

As mentioned in Section 3, we will consider two types of time series datasets: the first type comprises a single time series, and the second type consists of a set of time series. We will compare the performance of our method against the state-of-the-art Conformal Prediction (CP) methods for both types of datasets. We set \(=0.1\) (the error rate) for all uncertainty quantification experiments in the paper, and so the nominal coverage level of the prediction intervals is \(90\%\).

#### 5.1.1 A Single Time Series: French Electricity Spot Prices

We perform one-step-ahead forecasts on the French electricity spot prices data from 2016 to 2020, which consists of 35,064 observations. A detailed description and visualization of this time series are given in Appendix G.1. Our goal is to predict the 24 hourly prices of the following day, given the prices up until the end of the current day. As the hourly prices exhibit distinct patterns, we fit one model per hour as in the CP baseline . We follow the data splitting strategy used in , where the first three years \(2016-2019\) data are used as the (initial) training set, and the prediction is made for the last year \(2019-2020\).

For all the methods considered, we use the same underlying neural network model: an MLP with one hidden layer of size 100 and the sigmoid activation function. More details on the training process are provided in Appendix G.1. For the state-of-the-art CP methods, EnbPI-V2 , NEX-CP , ACI  and AgACI , we conduct experiments in an online fashion, where the model is trained using a sliding window of the previous three years of data (refer to Figure 4 in the Appendix). Specifically, after constructing the prediction interval for each time step in the prediction period, we add the ground truth value to the training set and then retrain the model with the updated training set. For ACI, we conduct experiments with various values of \(\) and present the one that yields the best performance. In the case of AgACI, we adopt the same aggregation approach as used in , namely, the Bernstein Online Aggregation (BOA) method  with a gradient trick. We also report the performance of ACI with \(=0\) as a reference. For NEX-CP, we use the same weights as those employed in their time-series experiments. For EnbPI-V2, we tune the number of bootstrap models and select the one that offers the best performance.

Since this time series exhibits no or minor distribution shift, our method PA is trained in an offline fashion, where the model is fixed for using only the observations between 2016 and 2019, and the observations in the prediction range are used only for final evaluation. That is, our method uses less data information in training compared to the baseline methods.

The results are presented in Figure 2, which includes empirical coverage (with methods that are positioned closer to 90.0 being more effective), median/average prediction interval length, and corresponding interquartile range/standard deviation. As expected, our method is able to train and calibrate the model by using only the initial training set, i.e., the data for 2016-2019, and successfully produces faithful prediction intervals. In contrast, all CP methods produce wider prediction intervals than ours and higher coverage rates than the nominal level of 90%. In addition, ACI is sensitive to the choice of \(\).

#### 5.1.2 A Set of Time Series

We conduct experiments using three publicly available real-world datasets: Medical Information Mart for Intensive Care (MIMIC-III), electroencephalography (EEG) data, and daily COVID-19 case numbers within the United Kingdom's local authority districts (COVID-19) . A concise overview of these datasets is presented in Table 1. Our method, denoted as PA-RNN (since the underlying prediction model is an LSTM ), is compared to three benchmark methods: CF-RNN , MQ-RNN , and DP-RNN , where LSTM is used for all these methods. To ensure a fair comparison, we adhere to the same model structure, hyperparameters, and data processing steps as specified in . Detailed information regarding the three datasets and training procedures can be found in Appendix G.2.

The numerical results are summarized in Tables 2. Note that the baseline results for EEG and COVID-19 are directly taken from the original paper . We reproduce the baseline results for MIMIC-III, as the specific subset used by  is not publicly available. Table 2 indicates that our method consistently outperforms the baselines. In particular, Our method consistently generates shorter prediction intervals compared to the conformal baseline CF-RNN while maintaining the same or even better coverage rate as CF-RNN. Both the MQ-RNN and DP-RNN methods fail to generate prediction intervals that accurately maintain a faithful coverage rate.

  
**Dataset** & **Train size** & **Test size** & **Length** & **Prediction horizon** \\  MIMIC-III  & 2692 & 300 &  & 2 \\ EEG  & 19200 & 19200 & 40 & 10 \\ COVID-19  & 300 & 80 & 100 & 50 \\   

Table 1: A brief description of the datasets used in Section 5.1.2, where “Train size” and “Test size” indicate the numbers of training and testing sequences, respectively, and “Length” represents the length of the input sequence.

Figure 2: Comparisons of our method and baselines on hourly spot electricity prices in France. **Left**: Average prediction length with vertical error bars indicating the standard deviation of the prediction interval lengths. **Right**: Median prediction length with vertical error bars indicating the interquartile range of the prediction interval lengths. The precise values for these metrics are provided in Table 5 of Appendix G.1.

### Autoregressive Order Selection

In this section, we evaluate the performance of our method in selecting the autoregressive order for two synthetic autoregressive processes. The first is the non-linear autoregressive (NLAR) process :

\[y_{i}=-0.17+0.85y_{i-1}+0.14y_{i-2}-0.31y_{i-3}+0.08y_{i-7}+12.80G_{1}(y_{i-1:i -7})+2.44G_{2}(y_{i-1:i-7})+_{i},\]

where \(_{i} N(0,1)\) represents i.i.d. Gaussian random noises, and the functions \(G_{1}\) and \(G_{2}\) are defined as:

\[G_{1}(y_{i-1:i-7}) =(1+\{-0.46(0.29y_{i-1}-0.87y_{i-2}+0.40y_{i-7}-6.68)\})^{-1},\] \[G_{2}(y_{i-1:i-7}) =(1+\{-1.17 10^{-3}(0.83y_{i-1}-0.53y_{i-2}-0.18y_{i-7}+ 0.38)\})^{-1}.\]

The second is the exponential autoregressive process :

\[y_{i}=(0.8-1.1\{-50y_{i-1}^{2}\})y_{i-1}+_{i},\]

where, again, \(_{i} N(0,1)\) denotes i.i.d. Gaussian random noises.

For both synthetic processes, we generate five datasets. Each dataset consists of training, validation, and test sequences. The training sequence has 10000 samples, while the validation and test sequences each contain 1000 samples. For training, we employ a single-layer RNN with a hidden layer width of 1000. Further details on the experimental setting can be found in Appendix G.3.

For the NLAR process, we consider two different window sizes for RNN modeling: 15 (with input as \(y_{i-1:i-15}\)) and 1 (with input as \(y_{i-1}\)). Notably, the NLAR process has an order of 7. In the case where the window size is 15, the input information suffices for RNN modeling, rendering the past information conveyed by the hidden states redundant. However, when the window size is 1, this past information becomes indispensable for the RNN. In contrast, the exponential autoregressive process has an order of 1. For all window sizes we explored, namely \(1,3,5,7,10,15\), the input information is always sufficient for RNN modeling.

We evaluate the predictive performance using mean square prediction error (MSPE) and mean square fitting error (MSFE). The model selection performance is assessed by two metrics: the false selection rate (FSR) and the negative selection rate (NSR). We define \(=^{5}|_{j}/S|}{_{j=1}^{5}|_{j}|}\) and \(=^{5}|S/_{j}|}{_{i=j}^{5}|S|}\), where \(S\) denotes the set of true variables, and \(_{j}\) represents the set of selected variables for dataset \(j\). Furthermore, we provide the final number of nonzero connections for hidden states and the estimated autoregressive orders. The numerical results for the NLAR process are presented in Table 3, while the numerical results for the exponential autoregressive process are given in Table 4.

Our results are promising. Specifically, when the window size is equal to or exceeds the true autoregressive order, all connections associated with the hidden states are pruned, effectively converting the RNN into an MLP. Conversely, if the window size is smaller than the true autoregressive order, a significant number of connections from the hidden states

    &  &  &  \\ 
**Model** & Coverage & PI length & Coverage & PI length & Coverage & PI length \\  PA-RNN & 90.8\%(0.7\%) & 2.35(0.26) & 94.69\%(0.4\%) & 51.02(10.50) & 90.5\%(1.4\%) & 444.93(203.28) \\ CF-RNN & 92.0\%(0.3\%) & 3.01(0.17) & 96.5\%(0.4\%) & 61.86(18.02) & 89.7\%(2.4\%) & 733.95(582.52) \\ MQ-RNN & 85.3\%(0.2\%) & 2.64(0.11) & 48.0\%(1.8\%) & 21.39(2.36) & 15.0\%(2.6\%) & 136.56(63.32) \\ DP-RNN & 1.2\%(0.3\%) & 0.12(0.01) & 3.3\%(0.3\%) & 7.39(0.74) & 0.0\%(0.0\%) & 61.18(32.37) \\   

Table 2: Uncertainty quantification results by different methods for the MIMIC-III, EEG, and COVID-19 data, where “Coverage” represents the joint coverage rate averaged over five different random seeds, the prediction interval length is averaged across prediction horizons and five random seeds, and the numbers in the parentheses indicate the standard deviations (across five random seeds) of the respective means.

are retained. Impressively, our method accurately identifies the autoregressive order--a noteworthy achievement considering the inherent dependencies in time series data. Although our method produces a nonzero FSR for the NLAR process, it is quite reasonable considering the relatively short time sequence and the complexity of the functions \(G_{1}\) and \(G_{2}\).

### RNN Model Compression

We have also applied our method to RNN model compression, achieving state-of-the-art results. Please refer to Section G.4 in the Appendix for details.

## 6 Discussion

This paper has established the theoretical groundwork for sparse deep learning with time series data, including posterior consistency, structure selection consistency, and asymptotic normality of predictions. Our empirical studies indicate that sparse deep learning can outperform current cutting-edge approaches, such as conformal predictions, in prediction uncertainty quantification. More specifically, compared to conformal methods, our method maintains the same coverage rate, if not better, while generating significantly shorter prediction intervals. Furthermore, our method effectively determines the autoregression order for time series data and surpasses state-of-the-art techniques in large-scale model compression. The theory developed in this paper has included LSTM  as a special case, and some numerical examples have been conducted with LSTM; see Section G of the Appendix for the detail. Furthermore, there is room for refining the theoretical study under varying mixing assumptions for time series data, which could broaden applications of the proposed method. Also, the efficacy of the proposed method can potentially be further improved with the elicitation of different prior distributions.

In summary, this paper represents a significant advancement in statistical inference for deep RNNs, which, through sparsing, has successfully integrated the RNNs into the framework of statistical modeling. The superiority of our method over the conformal methods shows further the criticality of consistently approximating the underlying mechanism of the data generation process in uncertainty quantification.

In terms of limitations of the proposed method, one potential concern pertains to the calculation of the inverse of the Fisher information matrix. For large-scale problems, the sparsified model could still retain a large number of non-zero parameters. In such instances, the computational feasibility of calculating the Hessian matrix might become compromised. Nonetheless, an alternative avenue exists in the form of the Bayesian approach, which circumvents the matrix inversion challenge. A concise description of this Bayesian strategy can be found in .

   Model & Window size & FSR \(\) & NSR \(\) & AR order & \#hidden link & MSPE \(\) & MSFE \(\) \\  PA-RNN & 1 & 0 & 0 & 1(0) & 0(0) & 1.004(0.004) & 1.003(0.005) \\ PA-RNN & 3 & 0 & 0 & 1(0) & 0(0) & 1.006(0.005) & 0.999(0.004) \\ PA-RNN & 5 & 0 & 0 & 1(0) & 0(0) & 1.000(0.004) & 1.007(0.005) \\ PA-RNN & 7 & 0 & 0 & 1(0) & 0(0) & 1.006(0.005) & 1.000(0.003) \\ PA-RNN & 10 & 0 & 0 & 1(0) & 0(0) & 1.002(0.004) & 1.002(0.006) \\ PA-RNN & 15 & 0 & 0 & 1(0) & 0(0) & 1.001(0.004) & 1.002(0.007) \\   

Table 4: Numerical results for the exponetial autoregressive process.

   Model & Window size & FSR \(\) & NSR \(\) & AR order & \#hidden link & MSPE \(\) & MSFE \(\) \\  PA-RNN & 1 & 0 & 0 & - & 357(21) & 1.056(0.001) & 1.057(0.006) \\ PA-RNN & 15 & 0.23 & 0 & 7.4(0.25) & 0(0) & 1.017(0.008) & 1.020(0.010) \\   

Table 3: Numerical results for the NLAR process: numbers in the parentheses are standard deviations of the respective means, *-* indicates not applicable, \(\) means lower is better, and “#hidden link” denotes the number of nonzero connections from the hidden states. All results are obtained from five independent runs.