# Partial Structure Discovery is Sufficient for

No-regret Learning in Causal Bandits

 Muhammad Qasim Elahi

Electrical and Computer Engineering

Purdue University

elahi0@purdue.edu

&Mahsa Ghasemi

Electrical and Computer Engineering

Purdue University

mahsa@purdue.edu

&Murat Kocaoglu

Electrical and Computer Engineering

Purdue University

mkocaoglu@purdue.edu

###### Abstract

Causal knowledge about the relationships among decision variables and a reward variable in a bandit setting can accelerate the learning of an optimal decision. Current works often assume the causal graph is known, which may not always be available _a priori_. Motivated by this challenge, we focus on the causal bandit problem in scenarios where the underlying causal graph is unknown and may include latent confounders. While intervention on the parents of the reward node is optimal in the absence of latent confounders, this is not necessarily the case in general. Instead, one must consider a set of possibly optimal arms/interventions, each being a special subset of the ancestors of the reward node, making causal discovery beyond the parents of the reward node essential. For regret minimization, we identify that discovering the full causal structure is unnecessary; however, no existing work provides the necessary and sufficient components of the causal graph. We formally characterize the set of necessary and sufficient latent confounders one needs to detect or learn to ensure that all possibly optimal arms are identified correctly. We also propose a randomized algorithm for learning the causal graph with a limited number of samples, providing a sample complexity guarantee for any desired confidence level. In the causal bandit setup, we propose a two-stage approach. In the first stage, we learn the induced subgraph on ancestors of the reward, along with a necessary and sufficient subset of latent confounders, to construct the set of possibly optimal arms. We show that for our proposed algorithm, the number of intervention samples required to learn the set of possibly optimal arms scales polynomially with respect to the number of nodes. The second phase involves the application of a standard bandit algorithm, such as the UCB algorithm. We also establish a regret bound for our two-phase approach, which is sublinear in the number of rounds.

## 1 Introduction

Causal bandits have been a topic of interest since their inception and have been studied in various contexts . The authors assumed precise knowledge of the causal graph and the impact of interventions or actions on the parents of the reward node. Subsequently, there has been a flurry of research on causal bandits [2; 3; 4]. The primary limitation of the majority of existing works on causal bandits is their assumption of full knowledge of the causal graph, which is often impractical for manyreal-world applications [1; 5; 6]. Recently, efforts have been made to overcome this limitation. In , the authors propose a sample efficient algorithm for cases where the causal graph can be represented as a directed tree or a causal forest and later extend the algorithm to encompass a broader class of general chordal graphs. However, the proposed algorithm is only applicable to scenarios where the Markov equivalence class (MEC) of the causal graph is known and does not have confounders. In , the authors propose a causal bandit algorithm that does not require any prior knowledge of the causal structure and leverages separating sets. However, their theoretical result holds only when a true separating set is known. The paper by Konobeev et al.  also deals with causal bandits with an unknown graph and proposes a two-phase approach. The first phase uses a randomized parent search algorithm to learn the parents of the reward node, and the second phase employs UCB to identify the optimal intervention over the parents of the reward node. However, similar to , they assume causal sufficiency, i.e., no latent confounders are present. In another related paper, , the authors initially emphasize the challenge of dealing with exponentially many arms when addressing causal bandits with an unknown graph. To tackle this issue, the authors assume that the reward is a noisy additive function of its parents. This assumption enables them to reframe the problem as an additive combinatorial linear bandit problem.

We also focus on the causal bandit setup where the causal graph is unknown, but we allow the presence of latent confounders and make no parametric assumptions. The optimal intervention in this case is not limited to parents of the reward node; instead, we have a candidate set of optimal interventions, called possibly optimal minimum intervention sets (POMISs), each being a special subset of the ancestors of the reward node . Thus, learning only the parents of the reward, similar to , is insufficient. This implies that causal discovery beyond parents of the reward is imperative. However, for regret minimization, discovering the full causal structure is not necessary. Instead, we characterize the set of necessary and sufficient latent confounders one needs to detect/learn to ensure all the possibly optimal arms are learned correctly.

Causal discovery is a well-studied problem and can be applied to our setup [11; 12; 13]. However, the majority of the existing causal discovery algorithms rely on the availability of an infinite amount of interventional data [14; 15; 16]. Some prior work shows that discovery is possible with limited interventional data, with theoretical guarantees when the underlying causal graph is a tree and contains no latent confounders . Also, the paper  proposes a sample-efficient active learning algorithm for causal graphs without latent confounders, given that the MEC for the underlying causal graph is known. Bayesian causal discovery can also be a valuable tool when interventional data is limited. However, it faces challenges when tasked with computing posterior probabilities across the combinatorial space of directed acyclic graphs (DAGs) without specific parametric assumptions [19; 20; 21]. All in all, the sample-efficient learning of causal graphs with latent confounders, without any parametric or graphical assumptions, with theoretical guarantees, remains an open problem.

We propose a randomized algorithm for sample-efficient learning of causal graphs with confounders. We analyze the algorithm and bound the maximum number of interventional samples required to learn the causal graph with all the confounders with a given confidence level. For the causal bandit setup, we propose a two-stage approach where the first step learns a subgraph of the underlying causal graph to construct a set of POMISs, and the second phase learns the optimal arm among the POMISs. We show that the requirement of learning only a subgraph leads to significant savings in terms of interventional samples and consequently, regret. The main contributions of our work are as follows:

* We characterize the necessary and sufficient set of latent confounders in the induced subgraph on ancestors of the reward node that we need to learn/detect in order to identify all the POMISs for a causal bandit setup when the underlying causal graph is unknown.
* We propose a randomized algorithm for sample-efficient learning of causal graphs with confounders, providing theoretical guarantee on the number of interventional samples required to learn the graph with a given confidence level.
* We propose a two-phase algorithm for causal bandits with unknown causal graphs containing confounders. The first phase involves learning the induced subgraph on reward's ancestors along with a subset of latent confounders to identify all the POMISs. The next phase involves a standard bandit algorithm, e.g., upper confidence bound (UCB) algorithm. Our theoretical analysis establishes an upper bound on the cumulative regret of the overall algorithm.

Preliminaries and Problem Setup

We start with an overview of the causal bandit problem and other relevant background needed on causal models. Structural causal model (SCM) is a tuple \(=,,,P()\) where \(=\{V_{i}\}_{i=1}^{n}\{Y\}\) is the set of observed variables, \(\) is the set of independent exogenous variables, \(\) is the set of deterministic structural equations and \(P()\) is the distribution for exogenous variables . The equations \(f_{i}\) map the parents (\((V_{i})\)) and a subset of exogenous variables \(_{i}\), to the value of variable \(V_{i}\), i.e., \(V_{i}=f_{i}((V_{i}),_{i})\). We consider the causal bandit setup where all the observed variables \(V_{i}\) are discrete with the domain \((V_{i})=[K]:=\{1,2,3,,K\}\), and the reward \(Y\) is binary, i.e., \((Y)=\{0,1\}\). We can associate a DAG \(=(,)\) with every SCM, where the vertices \(\) correspond to the observed variables and edges \(\) consist of directed edges \(V_{i} V_{j}\) when \(V_{i}(V_{j})\) and bi-directed edges between \(V_{i}\) and \(V_{j}\) (\(V_{i} V_{j}\)) when they share some common unobserved variable, also called latent confounder. We restrict ourselves to semi-Markovian causal models in which every unobserved variable has no parents and has exactly two children, both of which are observed . An intervention on a set of variables \(\), denoted by \(do()\), induces a post-interventional DAG (\(_{}\)) with incoming edges to vertices \(\) removed. We can broadly classify interventions into deterministic interventions, where variables are set to a fixed realization denoted by \(do(=)\), and stochastic interventions, where instead of a fixed realization we have \((.)\), where \(\) is a probability measure over the domain \(()\). We denote the sub-model induced under hard intervention by \(_{=}\) and the one induced under stochastic intervention by \(_{}\). In the context of causal bandits, an arm or action corresponds to hard intervention on a subset of variables other than the reward. The goal of the agent is to identify the intervention that maximizes the expected reward. The performance of an agent is measured in terms of cumulative regret \(R_{T}\).

\[R_{T}:=T_{}_{[K]^{| |}}[Y|do(=)]-_{t=1}^{T} [Y|do(_{t}=_{t})],\] (1)

where \(do(_{t}=_{t})\) represents the intervention selected by the agent in round \(t\). We use the notation \(_{do()}\) to define the sub-optimality gap of the corresponding arm \((=)\). We denote the descendants, ancestors and children of a vertex \(V_{i}\) by \((V_{i})\), \((V_{i})\) and \((V_{i})\) respectively. We use the notation \((V_{i},)\) to denote the set of vertices having bidirected edges to \(V_{i}\) except the reward node \(Y\). We refer to the induced graph between observed variables as the observable graph. The **transitive closure** of a graph, denoted by \(^{tc}\), encodes the ancestral relationship in \(\). That is, the directed edge \(V_{i} V_{j}\) is included in \(^{tc}\) only when \(V_{i}(V_{j})\). The **transitive reduction**, denoted by \(Tr()=(,^{r})\), is a graph with the minimum number of edges such that the transitive closure is the same as \(\). The connected component (c-component) of the DAG \(\), containing vertex \(V_{i}\), is denoted by \((V_{i})\), which is the maximal set of all vertices in \(\) that have a path to \(V_{i}\), consisting only of bi-directed edges . For a subset of vertices \(\), we define \(():=_{W_{i}}(W_{i})\). In a DAG, a subset of nodes \(\) d-separates two nodes \(V_{i}\) and \(V_{j}\) when it effectively blocks all paths between them, denoted as \(V_{i}_{d}V_{j}|\). Blocking is a graphical criterion associated with \(d\)-separation . A probability distribution is said to be faithful to a graph if and only if every conditional independence (CI) statement can be inferred from d-separation statements in the graph. Faithfulness is a commonly used assumption in the existing work on causal discovery [14; 24]. We assume that the following form of the interventional faithfulness assumption holds in our setup.

**Assumption 2.1**.: _Consider a set of nodes \(\) and the stochastic intervention \(do(,)\) on \(\) and any set \(\). The conditional independence (CI) statement \((\!\!\!)_{_{,}}\) holds in the induced model if and only if there is a corresponding d-separation statement in post-interventional graph \((\!\!\!_{d})_{_{ ,}}\), where \(\), \(\), and \(\) are disjoint subsets of \(\). The CI statements in the induced model are with respect to the post-interventional joint probability distribution._

## 3 Possibly Optimal Arms in Causal Bandits with Unknown Causal Graph

The optimal intervention in a causal bandit setup is not restricted to the parent set of the reward node when the reward node \(Y\) is confounded with any node in its ancestors \((Y)\). For instance, consider SCM \(X_{1}=U_{1}\) and \(X_{2}=X_{1} U_{2}\) and reward \(Y=X_{2} U_{2}\), where \(U_{1} Ber(0.5)\) and \(U_{2} Ber(0.5)\). Note that \(X_{2}\) and reward \(Y\) are confounded in this SCM. The optimal intervention in this case is \(do(X_{1}=1)\) since \([Y|do(X_{1}=1)]=1\). The intervention on the parent of the reward (\((Y)=X_{2}\)) is suboptimal because \([Y|do(X_{2}=0)]=[Y|do(X_{2}=1)]=0.5\). The example shows that it is possible to construct SCMs where optimal intervention is on ancestors of the reward node instead of parents when reward node is confounded with one of its ancestors. The authors in  propose a graphical criterion to enumerate the set of all possibly optimal arms, which they refer to as POMISs. We revisit some definitions and results from their work.

**Definition 3.1**.: _(**Unobserved Confounder (UC)-Territory**) Consider a causal graph \((,)\) with a reward node \(Y\) and let \(\) be \([(Y)]\). A set of variables \( V()\) containing \(Y\) is called an **UC-territory** on \(\) with respect to \(Y\) if \(De_{}()=\) and \(_{}()=\)._

A UC-territory is minimal if none of its subsets are UC-territories. A minimal UC-territory denoted by \((,Y)\), can be constructed by extending a set of variables, starting from the reward \(\{Y\}\), alternatively updating the set with the c-component and descendants of the set until there is no change.

**Definition 3.2**.: _(**Interventional Border)** Let \(T\) be a minimal UC-territory on \(\) with respect to \(Y\). Then, \(X=(T) T\) is called an interventional border for \(\) w.r.t. \(Y\) denoted by \((,Y)\)._

**Lemma 3.1**.: _[_5_]_ _For causal graph \(\) with reward \(Y\), \((_{},Y)\)is a POMIS, for any \(\{Y\}\)._

Although the graphical characterization in Lemma 3.1 provides a means to enumerate the complete set of POMISs, it comes with exponential time complexity. The authors also propose an efficient algorithm for enumerating all POMISs in . However, this requires knowing the true causal graph, and without it, one has to consider interventions on all possible subsets of nodes, which are exponentially many. One naive approach to tackle the problem is to learn the full causal graph with all confounders to list all POISs. However, a question arises: _Do we need to learn/detect all possible confounders since the goal is to find POMISs and not the full graph?_

Before answering the above question, we start with an example considering the causal graphs in Figure 1. Using Lemma 3.1, the set of POMISs for the true graph \(\) is \(_{}=\{,\{V_{1}\},\{V_{2}\},\{V_{3}\},\{V_{1},V_{2}\}\}\). However, for \(_{1}\) which has the bidirected edge \(V_{2} Y\) missing, the set of POMISs is \(_{_{1}}=\{,\{V_{2}\},\{V_{1},V_{2}\}\}\). Also for \(_{2}\) which has the bidirected edge \(V_{1} V_{2}\) missing, the set of POMISs is \(_{_{2}}=\{,\{V_{1}\},\{V_{2}\},\{V_{1},V_{2}\}\}\). In both cases, we miss at least one POMIS, and since it is possible to construct an SCM compatible with the true causal graph \(\) where any arm in POMIS is optimal, if this arm is not learned, we can suffer linear regret . Although the graph \(_{3}\) has the bidirected edge \(V_{1} V_{3}\) missing, it still has the same set of POMISs as the true graph, i.e., \(_{_{3}}=\{,\{V_{1}\},\{V_{2}\},\{V_{3}\},\{V_{1},V_{2 }\}\}\). This example shows that only a subset of latent confounders affect the POMISs learned from the graph. We formally prove that it is necessary and sufficient to learn/detect all latent variables between the reward and its ancestors because missing any one of them will cause us to miss at least one of POMISs leading to linear regret for some bandit instances.

**Lemma 3.2**.: _It is necessary to learn/detect the latent confounders between reward node \(Y\) and any node \(X(Y)\) in causal graph \(\) to learn all the POMISs correctly and hence avoid linear regret._

**Theorem 3.1**.: _Consider a causal graph \((,)\) and another causal graph \(^{}\) such that they have the same vertex set and directed edges but differ in bidirected edges, with the bidirected edges in \(^{}\) being a subset of the bidirected edges in \(\). The graphs will yield different collections of POMISs if and only if there exists some \(Z(Y)\) such that either (a) or (b) is true:_

1. _There is a bi-directed edge between_ \(Z\) _and_ \(Y\) _in_ \(\) _but not in_ \(^{}\) _._
2. _Neither of the graphs_ \(^{}\) _and_ \(\) _have a bidirected edge between_ \(Z\) _and_ \(Y\)_, and there exists a bidirected edge in_ \(\) _between some_ \(X(^{}_{(Z),(Z,^{ })},Y)\) _and_ \(Z\) _but not in_ \(^{}\)_._

We extend Lemma 3.2 to provide necessary and sufficient conditions in Theorem 3.1 characterizing all the latent variables that need to be learned, ensuring that the POMISs learned from a sparser causal

Figure 1: True Causal Graph \(\) with four other graphs each with one missing bi-directed edge.

graph match all those in the true causal graph. Suppose we have access to the induced observable subgraph \(^{}\) on ancestors of the reward node. We can start by testing for latent confounders between \(Y\) and any node in \((Y)\). Then, we need to test for latent confounders between any pair \(Z(Y)\) such that \(Z\) and \(Y\) don't have a bi-directed edge between them, and \(X(^{}_{(Z),(Z,^{})}\,Y)\) until there are no new pairs to test. Theorem 3.1 can be useful because depending on the underlying causal graph, it saves us the number of latent confounders we need to test. For instance, consider a causal graph that has the reward \(Y\) with \(n\) different parent nodes, i.e., \((Y)=\{V_{1},V_{2},,V_{n}\}\), with no edges between the parents. In cases where every parent of \(Y\) is confounded with \(Y\), or when none of them is confounded with \(Y\), we only need to test for \(|(Y)|\) latent variables, as implied by Theorem 3.1. However, in the worst-case scenario, we would need to test \((Y)|+1}{2}\) latent variables when the true graph only has the confounders \(V_{1} Y\) and \(V_{i} V_{i+1}\) for all \(i=1,..,n-1\). The exact number of latents we need to test can range from \(|(Y)|\) to \((Y)|+1}{2}\) depending on the true graph. One issue still remains: we need a sample-efficient algorithm to learn the induced observable graph over \((Y)\) and to test the presence of confounders, which is addressed in upcoming sections.

## 4 Finite Sample Causal Discovery Algorithm

In this section, we propose a sample-efficient algorithm to learn causal graphs with latent confounders. We propose a two-phase approach. In the first phase, the algorithm learns the observable graph structure, i.e., the induced graph between observed variables. In the second phase, it detects the latent confounders. In the next section, we use the proposed discovery algorithm to construct the algorithm for causal bandits with an unknown graph. We begin by proposing two Lemmas to learn the ancestrality relations and latent confounders using interventions.

**Lemma 4.1**.: _Consider a causal graph \((,)\) and \(\). Furthermore, let \(X,T\) be any two variables. Under the faithfulness Assumption 2.1\((X(T))_{_{}}\) if and only if for any \([K]^{||}\), we have \(P(t|do()) P(t|do(),do(x))\) for some \(x,t[K]\)._

**Lemma 4.2**.: _Consider two variables \(X_{i}\) and \(X_{j}\) such that \(X_{j}(X_{i})\) and a set of variables \(((X_{i})(X_{j})\{X_{i}\})\) and \(X_{i},X_{j}\). Under the faithfulness Assumption 2.1 there is latent confounder between \(X_{i}\) and \(X_{j}\) if and only if for any \([K]^{||}\), we have \(P(x_{j} do(x_{i}),do(=)) P(x_{j} x_{i},do( =))\) for some realization \(x_{i},x_{j}[K]\)._

These Lemmas are modified versions of Lemma 1 in  and Interventional Do-see test in , respectively. The difference between Lemma 4.1 and Lemma 1 in  is that we have an inequality test that can be used in the sample-efficient discovery instead of a statistical independence test. The Interventional Do-see test in  is valid for adjacent nodes only; however, our Lemma 4.2 can be used to test presence of latent confounder between any pair of nodes. This is because the condition in Lemma 4.2, \(X_{j}(X_{i})\), can always be satisfied for any pair by flipping the order when one node is an ancestor of the other. In order to provide theoretical guarantees on sampling complexity, the inequality conditions are not enough; we need to assume certain gaps similar to [7; 9; 17].

**Assumption 4.1**.: _Consider a causal graph \((,)\) and \(\). Furthermore, let \(X,T\) be any two variables. Then, we have \((X(T))_{_{}}\) if and only if for any \([K]^{||}\), we have \(|P(t|do())-P(t|do(),do(x))|>\) for some \(x,t[K]\), where \(>0\) is some constant._

**Assumption 4.2**.: _Consider two variables \(X_{i}\) and \(X_{j}\) such that \(X_{j}(X_{i})\) and a set of variables \(((X_{i})(X_{j})\{X_{i}\})\) and \(X_{i},X_{j}\). There is a latent confounder or a bidirected edge between \(X_{i}\) and \(X_{j}\) if and only if for any \([K]^{||}\), we have \(P(x_{j} do(x_{i}),do(=))-P(x_{j} x_{i},do( =))>\) for some realization \(x_{i},x_{j}[K]\) and some constant \(>0\)._

### Learning the Observable Graph

We propose Algorithm 1 to learn the transitive closure under any arbitrary intervention \(do()\), denoted by \(^{tc}_{}\). We use the Assumption 4.1 to bound the number of samples for ancestrality tests. We start with an empty graph and add edges by running ancestrality tests for all pairs of nodes in \(\), resulting in the transitive closure \(^{tc}_{}\). We recall that the transitive reduction \(Tr()=(,^{r})\) of a DAG \(=(,)\) is unique, with \(^{r}\), and it can be computed in polynomial time . Also, note that \(Tr()=Tr(^{tc})\). We propose a randomized Algorithm 2 similar to the one proposed in  that repeatedly uses Algorithm 1 to learn the observable graph structure. The motivation behind the randomized Algorithm 2 is Lemma 5 from , which states that for any edge \((X_{i},X_{j})\), consider a set of variables \(\) such that \(\{W_{i}:(W_{i})>(X_{i})\ \&\ W_{i}(X_{j})\} \) where \(\) is any total order that is consistent with the partial order implied by the DAG, i.e., \((X)<(Y)\) iff \(X(Y)\). In this case, the edge \((X_{i},X_{j})\) will be present in the graph \(Tr(_{}})\). Algorithm 2 randomly selects \(\), computes the transitive reduction of the post-interventional graphs, and finally accumulates all edges found in the transitive reduction across iterations. Algorithm 2 takes a parameter \(d_{}\), which must be greater than or equal to the highest graph degree for our theoretical guarantees to hold.

**Lemma 4.3**.: _Suppose that the Assumption 4.1 holds and we have access to \(max(},})}{_{1}}\) samples from \(do(X_{i}=x_{i},=)\  x_{i}[K]\) and \(}}{_{2}}\) samples from \(do(=)\) for a fixed \(w[K]^{||}\) and \(\). Then, with probability at least \(1-_{1}-_{2}\), we have \((X_{i}(X_{j}))_{_{}}}\) if and only if \(\ x_{i},x_{j}[K]\) s.t. \(|(x_{j} do(}))-(x_{j} do(}),do(x_{i}))|>\)._

Lemma 4.3 provides the sample complexity for running ancestrality tests. Algorithm 1 selects a realization \(w[K]^{||}\), takes \(B\) samples from the intervention \(do(=)\), and \(A\) samples from every \(do(X_{i}=x_{i},=)\) for all \(X_{i}\) and \(x_{i}[K]\) interventions. Thus, in the worst case, Algorithm 1 requires \(KAn+B\) samples to learn the true transitive closure with high probability. We formally prove this result in the Lemma 4.4.

**Lemma 4.4**.: _Algorithm 1 learns the true transitive closure under any intervention, i.e., \(_{}}^{tc}\), with probability at least \(1-n_{1}-_{2}\) with a maximum \(KAn+B\) interventional samples. If we set \(_{1}=\) and \(_{2}=\), then Algorithm 1 learns true transitive closure with probability at least \(1-\)._

Algorithm 2 repeatedly invokes Algorithm 1 to learn \(Tr(_{}})\) for randomly sampled \(\). Through this iterative process, it accumulates edges across iterations, ultimately constructing the observable graph structure. To establish the sampling complexity guarantee for Algorithm 2, we leverage the result from Lemma 4.4. The Theorem 4.1 gives the sampling complexity for learning the true observable graph with high probability.

**Theorem 4.1**.: _Algorithm 2 learns the true observable graph with probability at least \(1--}}-8 d_{max}(n)(n_{1}+_{2})\) with \(8 d_{max} n(KAn+B)\) interventional samples. If we set \(=(+2)}{ n}\), \(_{1}= n}\) and \(_{2}= n}\), then Algorithm 2 learns the true observable graph with probability at least of \(1-\). (We have \(A=(},})}{_{1}}\) & \(B=}}{_{2}}\) as in line 2 of Algorithm 1.)_

### Learning the Latent Confounders

Assumption 4.2 can be used to test for latents between any pair of observed variables. Note that while using Algorithm 2, we save and return all the interventional data samples. These samples can be reused to detect latent confounders in the next phase. For any variables \(X_{i}\) and \(X_{j}\) such that \(X_{j}(X_{i})\), we need access to interventional samples \(do(=)\) such that \(((X_{i})(X_{j})\{X_{i}\})\) and \(X_{i}\ \&\ X_{j}\). In the supplementary material, we demonstrate that randomly selecting the target set \(\) in Algorithm 2 ensures that we have access to all such datasets for all pairs of observed variables with high probability. In addition to simple causal effects we need to estimate the conditional causal effect of the form \(P(x_{j}|x_{i},do(=))\). To bound the number of samples required to ensure accurate estimation of the conditional causal effects, we rely on Assumption 4.3. Note that Assumption 4.3 does not restrict the applicability of our algorithm; it simply assumes that under an intervention \(do(=)\), either the probability of observing a realization \(X_{i}=x_{i}\) is zero or is lower-bounded by some constant \(>0\). The role of this assumption is to bound the number of interventional samples required for accurate estimation of the conditional causal effects.

**Assumption 4.3**.: _For any variable \(X_{i}\) and any intervention \(do(=)\) where \(\) and \([K]^{||}\), we assume that either \(P(x_{i}|do(=))=0\) or \(P(x_{i}|do(=))>0\)._

```
1Function\((,d_{max},_{1},_{2},_{3},_{4})\):
2\(,Data=(,d_{max}, _{1},_{2})\)
3\(C=}(K^{2}}{_{3}})+}(K^{2}}{_{4}})\), \(B=}}{_{2}}\)
4for every pair \(X_{i},X_{j}\)do
5 If\(X_{j}(X_{i})\), swap them.
6 Find interventional data sets \(do(=)\) and \(do(X_{i}=x_{i},=)\) from \(Data\) s.t. \(((X_{i})(X_{j})\{X_{i}\}) \) and \(X_{i}\ \&\ X_{j}\) Get \((0,C-B)\) new samples for \(do(=)\)
7if\(\;x_{i},x_{j}[K]\;s.t.\;|(x_{j}|do(x_{i}),do( ))-(x_{j}|x_{i},do())|>\)then
8 Add bi-directed edge \(X_{i} X_{j}\) to graph \(\)
9
10 return The Causal Graph with Latent Confounders \(\)
11End Function ```

**Algorithm 3**Learn the Causal Graph along-with the Latent Confounders

**Lemma 4.5**.: _Consider two nodes \(X_{i}\) and \(X_{j}\) s.t. \(X_{j}(X_{i})\) and suppose that Assumptions 2.14.2 hold and we have access to \(max(},})}{_{1}}\) samples from \(do(X_{i}=x_{i},=)\)\( x_{i}[K]\) and \(}(}{_{3}})+}( }{_{4}})\) samples from \(do(=)\) for a fixed \(w[K]^{||}\) and \(\) such that \(((X_{i})(X_{j})\{X_{i}\}) \) and \(X_{i}\ \&\ X_{j}\). Then, with probability at least \(1-_{1}-_{3}-_{4}\), we have a latent confounder between \(X_{i}\) and \(X_{j}\) iff \(\;x_{i},x_{j}[K]\;s.t.\;\;(x_{j}|do(x_{i}),do( ))-(x_{j}|x_{i},do())\;>\)._

Lemma 4.5 establishes the sample complexity for detecting the presence of latent confounders for any pair of nodes in the causal graph. Using results from Theorem 4.1 and Lemma 4.5, we bound the number of interventions required by the proposed Algorithm 3 to learn the causal graph along with the latent confounders. Theorem 4.2 provides the sample complexity guarantee for Algorithm 3 to learn the true causal graph, including all latent confounders, with a given confidence level. An important feature of the sampling complexity result in Theorem 4.2 is that the number of intervention samples needed to learn the causal graph scales polynomially with the number of nodes \(n\).

**Theorem 4.2**.: _Algorithm 3 learns the true causal graph with latents with probability at least \(1-d_{max}-2}-8 d_{max}(n)(n_{1}+(_{2}+_ {3}+_{4}))\) with a maximum of \(8 d_{max} n(KAn+(B,C))\) interventional samples. If we set \(=+2)}}{}\), \(_{1}= n}\) and \(_{2}=_{3}=_{4}= n}\), then Algorithm 3 learns the true causal graph with probability at least \(1-\). (A and B are given by line 2 of Algorithm 1 and \(C\) is given by line 3 of Algorithm 3.)_

Suppose the constant gaps \(\) and \(\) in Assumptions 4.1 and 4.2 are close; then, we have \(C>AB\). The value of the constant \(0<<1\) is usually small in practical scenarios, so the quantity \(C\) is much greater than both \(B\) or \(A\). This implies that the number of samples required to test the presence of latent variables is greater than that required to learn ancestral relations. This is because we need to accurately estimate conditional causal effects to detect latent variables, which requires a large number of samples compared to simple causal effects. Theorem 3.1 is useful here because it shows that we do not need to test for confounders between all pairs of nodes among ancestors of the reward node to learn the POMIS set.

## 5 Algorithm for Causal Bandits with Unknown Graph Structure

Algorithm 4 is the sketch of our algorithm for causal bandits with unknown graph structure. The detailed algorithm with all steps explained is given in the supplementary material (Algorithm 6). Algorithm 4 first learns the transitive closure of the graph \(^{tc}\) to find ancestors of the reward node \(Y\). This is because POMISs are only subsets of \((Y)\). The next step is to learn the observed graph structure among the reward \(Y\) and nodes in \((Y)\). Instead of detecting the presence of confounders between all pairs of nodes in \((Y)\) as in Algorithm 3, we focus on identifying the necessary and sufficient ones, as characterized by Theorem 3.1. This approach is more sample-efficient since it tests for fewer latent confounders. The exact saving in terms of samples depends on the underlying causal graph and is hard to characterize in general. The last step of Algorithm 4 is to run a simple bandit algorithm, e.g., UCB algorithm , to identify the optimal arm from the POMISs. Given that Assumptions 4.1, 4.2, and 4.3 hold, and the reward is binary \((Y\{0,1\})\), using the results from Lemma 4.4 and Theorem 4.2, we provide a worst-case regret bound for Algorithm 4 in Theorem 5.1.

```
1Calculate \(,_{1},_{2},_{3},_{4}\) as in Theorem 5.1
2\(^{tc}\) = \((=,,)\)
3\(,ana\) = \(((Y)_{^{tc}},,d_{max}, _{1},_{2})\)
4# Learn the bi-directed edges between reward \(Y\) and all nodes \(X_{i}(Y)\) and update \(\).
5forevery \(X_{i}(Y)_{^{tc}}\)do
6\(\) = \((,X_{i},X_{j},_{2},_{3}, _{4},ana)\) (Algorithm 5)
7whileThere is a new pair that is testeddo
8 Find a new pair \((Z,X)\) s.t. \(Z(Y)\) such that \(Z\) and \(Y\) don't have a bi-directed edge between them in \(\) and \(X(_{_{_{Z}}(Z,)},Y)\) and test for the latent and update \(\).
9\(\) = \((,Z,X,_{2},_{3},_{4}, ana)\)
10 Learn the set of POMIS \(_{}\) from the graph \(\) (Using Algorithm 1 from ).
11 Run UCB algorithm over the arm set \(A=\{(I) I_{}\}\). ```

**Algorithm 4**Sketch of Algorithm for causal bandits with unknown graph structure

**Theorem 5.1**.: _Algorithm 4 learns the true set of POMISs with probability at least \(1-2\). Under the event that it learns POMISs correctly, the cumulative regret is bounded as follows:_

\[R_{T} Kn(},})K^{2}}{}\ \ +\ \ }}{}\ \ +\\ 8 d_{max}KA(Y)\ +\ (B,C) (Y)\ +_{\{(I) I _{}\}}_{do()}1+)}^{2}},\]

_where \(A\) and \(B\) are given by line 2 of Algorithm 1, and \(C\) is given by line 3 of Algorithm 3 by setting \(=+2)}}{(Y)}}\), \(_{1}=(Y)(Y)}}\) and \(_{2}=_{3}=_{4}= (Y)}}\)._The first three terms in the regret bound correspond to the interventional samples required to learn the ancestors of the reward node, and then the set of POMISs (\(_{G}\)). The last term corresponds to the regret incurred by running the UCB algorithm over the POMIS set. The number of interventional samples used to learn the true set of POMISs, with high probability, has polynomial scaling with respect to the number of nodes \(n\) in the graph. However, the total number of arms in the POMIS set, in the worst case, can exhibit exponential scaling with respect to the number of ancestors of the reward node \(|(Y)|\). The advantage of sample-efficient discovery is that it helps us reduce the action space before applying the UCB algorithm. If the graph is not densely confounded, the total number of arms in the POMIS set would be small, and running causal discovery before the bandit algorithm is advantageous. Without discovery, one would always have to run the UCB or a standard MAB solver with exponentially many arms. For instance, if the causal graph has \(n\) nodes, there will be \(_{i=1}^{n}K^{i}=(K+1)^{n}\) different possible arms/interventions.

## 6 Experiments

Theorem 5.1 establishes the worst-case upper bound for cumulative regret when we need to test latent confounders between all pairs of nodes within \((Y)\). However, Algorithm 4 selectively examines only a subset of latent confounders sufficient to infer the true POMIS set, as outlined in Theorem 3.1. Although the advantage is hard to quantify in general, we demonstrate it using simulations on randomly generated graphs. We sample a random ordering \(\) among the vertices. Then, for each \(n\)th node, we determine its in-degree as \(X_{n}=(1,(n-1,))\), followed by selecting its parents through uniform sampling from the preceding nodes in the ordering. Finally, we chordalize the graph using the elimination algorithm , employing an elimination ordering that is the reverse of \(\). Additionally, we introduce a confounder between every pair of nodes with a probability of \(_{L}\). For all the simulations, we randomly sample \(50\) causal graphs with different values of densities \(\) and \(_{L}\) and assume that all variables are binary for simplicity, i.e., \(K=2\). We set the value of \(\) to 0.99, and the gaps \(==0.01\) and \(=0.05\). We plot interventional samples used to learn the induced observable graph on \((Y)\) with and without latent confounders, as well as the samples required to learn the POMIS set by Algorithm 4. The width of confidence interval is set to \(2\) standard deviations.

The simulation results in Figure 2 demonstrate that Algorithm 4 requires fewer samples than learning the induced graph on \((Y)\), which includes all confounders. However, as \(_{L}\) increases for a fixed \(\), this advantage diminishes, as illustrated in Figure 2. The trend remains consistent as the density parameters \(\) and \(_{L}\) are varied from \(0.2,0.4\), and \(0.6\). The plots in Figure 3 compare the exponentially

Figure 2: Simulations to demonstrate the advantage of Algorithm 4 over full graph discovery (Learning all possible latents)

growing arms in causal bandits with intervention samples used by our algorithm to learn the reduced action set in the form of POMISs. This demonstrates the major advantage of our algorithm, which, instead of exploring an exponentially large action set as in naive UCB algorithms, uses interventions to reduce the action space to the POMIS set before applying the UCB algorithm. Additionally, the number of intervention samples required in the first phase of identifying the true POMIS set grows polynomially with respect to the number of nodes in the graph. However, the number of arms in the POMIS set can still exhibit exponential scaling with respect to the number of ancestors of the reward node in the worst case.

We also run the UCB algorithm on the learned POMIS set and plot the cumulative regret in Figure 4. Since the number of time steps \(T\) is on the order of \(10^{8}\), it is not feasible to store and plot cumulative regret for every time step over multiple randomly sampled graphs; therefore, we downsample the cumulative regret to show the overall trend. The downsampling, along with the large scale of the \(y\)-axis, makes the regret in the discovery phase appear linear with a fixed slope, although it is piecewise linear if we zoom in. Also, the UCB phase converges very fast compared to the discovery phase because the number of POMISs for randomly sampled graphs is small. We plot the results for graphs with 10, 15, and 20 nodes, and in all cases, we can see the advantage of partial discovery compared to full discovery, since Algorithm 4 finds the POMIS set with fewer samples. The code to reproduce our experimental results is available at https://github.com/CausalML-Lab/CausalBandits_with_UnknownGraph.

## 7 Conclusion

We show that partial discovery is sufficient to achieve sublinear regret for causal bandits with an unknown causal graph containing latent confounders. Without relying on causal discovery, one must consider interventions on all possible subsets of nodes, which is infeasible. Therefore, we propose a two-phase approach: the first phase learns the induced subgraph of the ancestors of the reward node, along with a subset of confounders, to construct a set of possibly optimal arms. We demonstrate that the number of interventional samples in the first phase required to identify the POMIS set scales polynomially with respect to the number of nodes in the causal graph. In the next phase, we apply the Upper Confidence Bound (UCB) algorithm to the reduced action space to find the optimal arm.

## 8 Acknowledgment

Murat Kocaoglu acknowledges the support of NSF CAREER 2239375, IIS 2348717, Amazon Research Award and Adobe Research.

Figure 4: Cumulative regret for Algorithm 4 versus learning all possible latents \((=_{L}=0.3)\).

Figure 3: Simulations to demonstrate advantage of discovery for causal bandits.