# Is Function Similarity Over-Engineered?

Building a Benchmark

 Rebecca Saul\({}^{1}\), Chang Liu\({}^{2}\), Noah Fleischmann\({}^{1}\), Richard Zak\({}^{1,3}\),

Kristopher Micinski\({}^{2}\), Edward Raff\({}^{1,3}\), James Holt\({}^{4}\)

\({}^{1}\)Booz Allen Hamilton\({}^{2}\)Syracuse University,

\({}^{3}\) University of Maryland, Baltimore County, \({}^{4}\)Laboratory for Physical Sciences

Saul_Rebecca@bah.com, cliu57@syr.edu,

Fleischmann_Noah@bah.com, Zak_Richard@bah.com,

kkmicins@syr.edu, Raff_Edward@bah.com, holt@lps.umd.edu

###### Abstract

Binary analysis is a core component of many critical security tasks, including reverse engineering, malware analysis, and vulnerability detection. Manual analysis is often time-consuming, but identifying commonly-used or previously-seen functions can reduce the time it takes to understand a new file. However, given the complexity of assembly, and the NP-hard nature of determining function equivalence, this task is extremely difficult. Common approaches often use sophisticated disassembly and decompilation tools, graph analysis, and other expensive pre-processing steps to perform function similarity searches over some corpus. In this work, we identify a number of discrepancies between the current research environment and the underlying application need. To remedy this, we build a new benchmark, REFuSe-Bench, for binary function similarity detection consisting of high-quality datasets and tests that better reflect real-world use cases. In doing so, we address issues like data duplication and accurate labeling, experiment with real malware, and perform the first serious evaluation of ML binary function similarity models on Windows data. Our benchmark reveals that a new, simple baseline -- one which looks at only the raw bytes of a function, and requires no disassembly or other pre-processing -- is able to achieve state-of-the-art performance in multiple settings. Our findings challenge conventional assumptions that complex models with highly-engineered features are being used to their full potential, and demonstrate that simpler approaches can provide significant value.

## 1 Introduction

Binary function similarity detection (BFSD) is the problem of determining whether two binary functions are similar in the absence of source code. BFSD plays a central role in many binary analysis tasks, with downstream applications to reverse engineering , malware analysis [8; 27; 29], software component analysis [12; 61], and vulnerability detection [14; 62]. Given its extensive use cases, there has been great interest in finding automated solutions to BFSD, particularly from the security community. However, developing such solutions has proven to be quite challenging; even when the source codes of functions are identical, an unrealistic assumption in real-world scenarios, semantically equivalent functions can have drastically different binary representations due to changes in target architecture, build toolchains, compiler flags, and optimization levels.

BFSD has been studied in systems security, programming languages, and most recently, machine learning . Significant research effort has been spent exploring different function representations, including raw bytes , assembly [40; 12], intermediate representations , control flow graphs [61;32], data flow , and dynamic analysis . In machine learning specifically, similar resources have gone into investigating a variety of architectures for BFSD, with proposals involving convolutional neural networks , recurrent neural networks , graph neural networks [61; 32; 65], seq2seq encoder-decoders , and transformers [46; 64], among others. However, comparably little attention has been paid to developing meaningful benchmarks for evaluating the efficacy of new models, making it challenging to judge the utility of individual contributions -- an expected issue without good benchmarks .

Lack of data and breadth of evaluations has been a constant issue in BFSD research. According to , the median ML model in this space is trained on only 3.7k binaries, and almost none are trained on Windows data, despite that operating system being the primary target for reverse engineering and malware analysis work. Earlier surveys of the BFSD literature, while providing important insight, face the same pitfalls. For example, though  takes the vital step of evaluating multiple BFSD models on a common dataset, its test dataset has fewer than \(1,000\) binaries in total, and consists entirely of Linux files. The largest BFSD benchmark we are aware of is BinKit , which contains \(243,128\) binaries. However, as these binaries are built from only 51 GNU packages, they lack diversity in terms of developers, coding styles, project sizes, and project types. Additionally, BinKit does not incorporate any Windows binaries.

In this paper, we address this gap in the machine learning BFSD literature by proposing a new benchmark, REFuSe-Bench1. Together, the size and quality of our datasets allow practitioners to draw meaningful conclusions about the appropriateness of existing BFSD models for a variety of downstream applications.

Our contributions include: **(1)** Datasets chosen to mirror real-world situations in computer security, mitigating six major deficiencies commonly found in prior work that prevent generalization. **(2)** A simple baseline model for BFSD, called Reverse Engineering Function Search (REFuSe), which uses only raw-byte features and a basic convolutional neural network. We release REFuSe under the MIT license. **(3)** Evaluations of REFuSe, and other prominent approaches in BFSD, on our selected datasets. Despite its modest design, REFuSe attains state-of-the-art performance on many tasks, suggesting that more work is needed to leverage higher complexity features and architectures to their fullest potential.

Our paper is organized as follows: in SS2, we review prior work in machine learning for binary analysis, with an emphasis on binary function similarity detection. In SS3, we present the datasets that comprise our benchmark, and in SS4, introduce REFuSe, our new baseline. We detail our experiments and their results in SS5, and finally conclude in SS6.

## 2 Related Work

In developing REFuSe-Bench, we discovered six major discrepancies between how datasets were constructed for the BFSD problem, a common problem in malware research and the difficulty in evaluation [45; 24; 25; 43]. While such design choices are reasonable from an initial development perspective, they significantly depart from real-world applications, which will be demonstrated in subsection 5.1 where prior methods do not generalize. We list these key insights that we rectify with REFuSe-Bench's training and testing sets. **C1. Any binary/project application should have all of its functions in only one of the train/test sets, not both**. Many prior works split one binary into both train and test functions [36; 2; 61], which leaks information about the specifics of the source project. Real-world deployments will be on novel binaries, and so this error will over-estimate performance.

**C2. You must check for the same function across binaries**. Most works assume that semantically equivalent functions occur only in the same binary/project [39; 2; 5], but we found this is often violated. In many instances we found code from Stackoverflow or other open-source repositories was copy-pasted into many projects, causing information leakage. **C3. Designers need to be specific on labeling details with code.** The choice of data impacts labeling, which is under-specified in many works. Some do not mention these details [16; 31], are ambiguous without the code  or do not fully state how steps are performed [61; 54]. These hinder replicability, fair evaluation, and comparison. **C4. Allow standard compiler optimizations.** In particular, function in-lining is an optimization where the compiler merges one smaller function into another (usually) larger function, forming one final function at compiler time. This requires more care to decide what is "semanticallyequivalent" and how evaluation is performed, but disabling this ubiquitous optimization [39; 16] hinders real-world generalization. **C5. Use larger datasets with Windows executables.** Windows has the generally highest need for reverse engineering, and thus BFSD tools, but prior works almost exclusively use a smaller amount of linux binaries - less than 5,500 for [16; 36; 28; 39; 31; 2]. **C6. Do not restrict the search space using information not available at deployment time**. Due to the high computational cost of many prior approaches to binary function similarity search (e.g., BSim takes 28 minutes per file to extract vectors), simplifying assumptions have been made but severely hampered real-world applicability. This includes using a "pool size" of at most \(X\) functions to search over, where the true nearest function is forced to be in the set of [32; 39; 54] or limiting searchers to a specific compiler setting. However, it has also been long recognized that real-world usage can not rely on either of these assumptions, as malware and user applications have no debug symbols or source code to inform such information. Using a pool size requires knowing, a priori, the true function -- which is the explicit purpose of the BFSD task. For example, on the Common Libraries test set, discussed in Section 3.1.2, reducing the pool size to 100 can increase a model's supposed performance by 53%, yet the smallest program in that test set has 217 functions, and the largest single program has 74,736 (7.5x larger than the largest pool size supported by ). This means the pool size restrictions prevent a method from reliably searching a single program for a related function and, at best, can search on the order of 50 potential binaries exhaustively. To be applicable to deployment, any binary similarity search measure must be able to scale to hundreds of thousands of programs at a minimum [52; 33; 56; 3; 49], and minimum-viable Anti-Virus scale systems are reported at the 20 million binary range . With an average of 200 functions per binary in Assemblage , it is clear that the pool size restriction should be avoided. While such a scale is often impractical for researchers to develop their new techniques, our benchmark provides a more realistic scale and evaluation set that can foster research that is more likely to transfer to practice.

### ML approaches to BFSD

Though BFSD has been tackled by researchers in numerous areas, in recent years, methods from machine learning have emerged as the dominant technique in this problem space . A 2022 survey paper by Marcelli et. al  highlights two ML models of particular significance: Gemini , developed by Xu et. al in 2017, and the graph-matching Graph Neural Network (GNN) from Li et. al (2019) . Gemini computes function embeddings from functions' Attributed Control Flow Graphs (ACFGs), and was the first to combine GNNs with the Siamese neural network architecture. Its publication marked a pivotal moment in the field, as it notably improved on earlier work, established machine learning as a viable approach to BFSD, and inspired several lines of future research. In fact, the GNN from Li et. al., which was published two years after Gemini and was the best performing model evaluated in Marcelli et. al.'s survey, leveraged many of the ideas first introduced in Gemini, including using GNNs on ACFGs.

Since the release of , research on machine learning for BFSD has shifted to focus heavily on the transformer architecture and large language model (LLM)-inspired designs for assembly-language models [31; 5; 66]. Much of this work draws from the training paradigm introduced in BERT , and adopts masked language modeling (MLM), but not next sentence prediction (NSP), as a pre-training task. (NSP cannot be used in the context of compiled code, as proximity is uncorrelated with similarity in this domain.) Several novel tasks have been proposed to replace NSP: these tasks include context window prediction , def-use prediction , execution language modeling , strand-symbolic mapping , and knowledge integration .

In contrast to generalized assembly model approaches, research that exclusively targets the problem of BFSD tends to eschew novel pre-training and explore other changes to the training regime. The authors of BinShot  suggest that MLM alone is sufficient for pre-training, and use a new weighted distance vector with a binary cross entropy loss function. FASER  skips pre-training all together, and uses an intermediate language, rather than assembly, as input to their model. A final BFSD-focused model is jTrans , which combines instruction semantics and control flow information in a transformer-based architecture. jTrans is distinguished by its unique jump target prediction pre-training task, engineered to augment the model's understanding of jump instructions.

### Feature Selection in Binary Analysis

Machine learning has been applied to many problems in binary analysis. In addition to the BFSD approaches discussed in SS2.1, numerous methods for provenance identification [44; 22], vulnerability search [63; 50], and malware detection [30; 17; 59; 37] have been proposed in recent years. However, nearly all these methods rely on expensive feature engineering techniques, including disassembly, extraction of control-flow graphs, dynamic analysis, and manual construction of feature vectors by experts with significant domain knowledge. As presented in  and , such a feature set imposes several limitations on its derivative models. Firstly, the performance of these models can be dependent on the accuracy of external binary analysis tools like IDA Pro  and Ghidra , which are known to produce errors . Secondly, the desired features can be costly to obtain, requiring proprietary licenses, vast computational resources, and/or rare levels of domain knowledge. Together, these difficulties limit the efficacy, generalizability, and accessibility of these techniques.

To avoid the challenges mentioned above, some authors have attempted to train ML models for binary analysis that operate directly on the raw bytes of an executable. Raff et al.  proposed MalConv, a convolutional neural net (CNN) that learned to process raw byte sequences for malware classification to try and adapt the "no feature engineering" strategy to malware. Others have continued the non-trivial effort to adapt NLP methods to binary analysis , or incorporate raw byte CNNs into more expensive features . Due to this foundation and simplicity, we adapt MalConv to our benchmark to focus on the data labeling, lower the cost to experiment and observe the gap between byte-based and more complex feature extractors.

## 3 Datasets

As SS2.1 establishes, many techniques have been proposed for applying machine learning to BFSD. However, it is not yet clear which of these techniques best translates to success in downstream security tasks. Our benchmark aggregates the results of experiments on five datasets (Table 1) to answer this question. The Assemblage dataset, discussed in SS3.1, is a collection of Windows binaries compiled by scraping GitHub, and is an order-of-magnitude larger than anything previously seen in the BFSD literature. Due to its size, we split it into a train and test set, and use the training portion to train our own baseline BFSD model, REFuSe (see SS4). Additionally, we rely on the MOTIF malware family dataset (SS3.1.1) to directly measure the utility of our chosen models for malware analysis. To complement MOTIF, we also curate a dataset of common libraries (SS3.1.2) to test on, as recognizing functions from libraries such as these is an important part of reverse engineering and software component analysis. Finally, we include two prominent datasets from the BFSD literature, which are both released under the MIT license, in our benchmark. These are Dataset-1 from Marcelli et. al.  and BinaryCorp from jTrans .

In the rest of this section, we give more detail on the Assemblage, MOTIF, and Common Libraries datasets, as this study marks the first time they are used in the context of BFSD. For more information on Dataset-1 and BinaryCorp, we refer to the papers cited above, which explore them at length.

### REFuSe-Bench

We use the Assemblage  dataset/system to create our training data and one of our five tests. Assemblage is a distributed system that crawls repositories of source code, compiles the code it finds in a number of specified configurations, and records extensive file and function-level ground truth metadata about each produced binary - avoiding the need for expensive lifting/re-writing . Assemblage is released under the MIT license; we used Assemblage to crawl 165,706 repositories from GitHub containing C or C++ source code, then compiled this code using the Microsoft Visual Studio Compiler and different architectures, compiler versions, optimizations, and build modes. In sum, we produced \(807,133\) binaries in 17 unique configurations, with a mean of 47K and a median of 58K binaries per configuration. These binaries included a total of \(263,205,895\) functions. For those interested in working with this dataset, we provide the Assemblage "recipe" that reproduces this build.

  
**Dataset** & **OS** & **No. Binaries** & **No. Functions** \\  Assemblage & Windows & \(135,975\) & \(24,545,694\) \\ MOTIF & Windows & \(3,095\) & \(2,442,164\) \\ Common Libraries & Windows & \(40\) & \(106,545\) \\ Marcelli Dataset-1 & Linux & \(919\) & \(668,400\) \\ BinaryCorp & Linux & \(9,675\) & \(4,791,673\) \\   

Table 1: Overview of the five (test) datasets.

We took several steps to curate a train and test set of binary functions from the executables in our Assemblage dataset (C5). First, we partitioned the functions into a train (80%) and test (20%) split based on the source codes they were compiled from, meaning that all functions from all binaries compiled from the same source code (but with different compilation flags; C4) were collocated in the same split (C1). Furthermore, we recognized that functions located in the standard libraries used by nearly every C program were likely to be present in most, if not all, binaries in our dataset (C2). (For example, the function _raise_securityfailure_ is present in over 783K binaries.) This demonstrated that a simple train-test partition by originating source code would still leave significant function overlap between the train and test set. Therefore, we applied an additional filter for common functions, requiring that functions that were found in more than half of the binaries in the Assemblage dataset were restricted to appearing on only one side of either the train or the test split. (The threshold for what counts as a common function could be set lower than 0.5, and we invite future work to explore changes to our dataset selection strategy; C1,2,3.) Finally, in both the train and test sets, we filtered out duplicate functions, identified by hashes of the functions' bytes (C2). In the end, we were left with \(97,747,033\) functions in the training set and \(24,545,694\) functions in the test set.

We considered several labeling methods for our REFuSe-Bench training and test datasets, and ultimately judged that since these datasets serve different purposes, they would benefit from different labeling procedures (C3). We elaborate on our labeling practice for the training dataset in SS4.2 and the test dataset in SS5.

#### 3.1.1 Motif

The Malware Open-source Threat Intelligence Family dataset (MOTIF)  is a dataset of \(3,095\) (disarmed) malicious PE binaries from 454 malware families, released under the Booz Allen Public License. As reverse engineering and malware analysis are two of the major downstream uses of BFSD, we felt it was important to incorporate real malware into our benchmark. For our experiments with our baseline, REFuSe, we extracted function bytes from the MOTIF binaries using Ghidra . When benchmarking against other models in the literature, we followed the data extraction and preprocessing methods detailed by their creators. In both instances, we labeled functions with the malware family labels of the binaries they were extracted from. We evaluated models based on their ability to group functions from the same malware family together. This gives us a method to evaluate how well a BFSD method operates against real-world malware that may not be well behaved.

#### 3.1.2 Common Libraries

While BFSD has most often been studied in binaries originating from the same source code, but compiled differently, practitioners are also interested in under-standing similarity as code evolves over a project's development history. To measure this, we present a dataset consisting of multiple versions of seven different prominent Windows projects: abseil, cjson, glfw3, libxml2, openssl, sdl1, and zlib. These projects were collected from vcpkg, Microsoft's open source package manager, and the versions and licenses of each project chosen for our dataset are shown in Table 2. In total, this dataset consists of 40 binaries. Each binary is compiled in release mode to x64 architecture using version 142 of the Microsoft Visual Studio Compiler, with optimization level \(0d\).

We used the Assemblage framework  to gather function-level metadata about these binaries. For all experiments, we gave functions the same label if they had the same name, and asked models to match functions across project versions. When evaluating on REFuSe, we leveraged Assemblage to get function boundaries. When benchmarking against prior work, we followed the data processing regimes detailed by their authors.

We note that the seven different projects we have chosen contain dramatically different numbers of functions (see Table 2). Because class imbalance is common in real-world binary function similarity

  
**Project** & **Versions** & **Functions** & **License** \\  abseil & 2020-09-23, 20211102.1, & 20,568 & Apache-2.0 \\  & 20230125.0, 20230802.0 & & \\  cjson & 1.7.15, 2020-09-23 & 217 & MIT \\  glfw3 & 2020-09-23, 3.3.7, 3.3.8 & 1,017 & Zlib \\  libxml2 & 2.11.5, 2020-09-23 & 5,143 & MIT \\  openssl & 2020-09-23, 3.0.3, & 74,736 & Apache-2.0 \\  & 3.1.0, 3.1.2 & & \\  sdl1 & 1.2.15, 2020-09-23 & 1,666 & LGPL-2.1-or-later \\  zlib & 1.2.12, 1.2.13, & 3,198 & Zlib \\  & 1.3, 2020-09-23 & & \\   

Table 2: Projects and versions in the common libraries dataset.

use cases, we opted not to subsample functions from the larger projects, and instead used all the available data. To measure the effects of this imbalance, in SS5.1 we broke down our results on a per-project basis.

## 4 REFuSe: A Simple BFSD Baseline

As discussed in SS2.1, most prior work in machine learning for BFSD does not directly leverage a function's binary representation. Instead, these approaches extract higher-level information from binaries, such as disassembly or control flow graphs (CFGs), and train models on these features. Naturally, the neural architectures used in these works are geared specifically towards the structure of the features they exploit, e.g. natural language-inspired techniques for disassembly-based methods, or graph neural networks for those relying on CFGs.

To better understand the impact of sophisticated features and heavyweight architecture choices, we sought to develop a simple baseline model for BFSD. We opted out of feature-engineering all together, and decided to train a model straight from raw bytes. Moreover, instead of the traditional GNNs, RNNs, or Transformer-based networks, we used an adapted version of the MalConv convolutional neural network proposed in [47; 48] (and discussed in SS2.2). Our model Reverse Engineering Function Search, or REFuSe, modifies MalConv to extract embeddings rather than binary classifications.

The REFuSe architecture is shown in Figure 1.

The model takes in a function's raw bytes as input, and, as in [47; 48], begins by mapping each byte to a learned feature vector. This helps the model to distinguish between nearby byte values instead of treating them as inherently similar, which we know is not true in the binary setting. After the byte embedding layer, we pass the function representation to two 1-D convolution layers, as in MalConv, followed by a temporal max pooling layer. Finally, we apply just a single fully-connected layer to produce an embedding.

### The Training Regime: Triplet Learning

We trained REFuSe using triplet learning, a standard metric learning technique for ranked similarity learning with proven success on BFSD tasks . In triplet learning, a Siamese neural network  is trained on triplets of examples \((x,x_{+},x_{-})\), where \(x\) is an anchor instance and \(x_{+}\) (positive) is more similar to \(x\) than \(x_{-}\) (negative) is to \(\). That is, for a function \(s\) measuring the true similarity of two examples, the set of valid triplets is \(T=\{(x,x_{+},x_{-}) s(x,x_{+})>s(x,x_{-})\}\).The goal of triplet learning is to learn an embedding function \(f\) such that for a pre-determined distance metric \(d\) and margin of separation \(\), \(d(f(x),f(x_{+}))+<d(f(x),f(x_{-}))(x,x_{+},x_{-}) T\).

In this way, triplet learning incentivizes models to push similar examples together and pull dissimilar examples apart. The idea of using a push-pull mechanism in the loss function was first introduced by  in the context of k-nearest neighbor classification. It was later modified to support neural network architectures in , and has been utilized in the context of BFSD in  and . We used the version of triplet loss popularized in Schroff et. al. , with the loss function \(L=_{(x,x_{+},x_{-}) T}(d(f(x),f(x_{+}))-d(f(x), f(x_{-}))+,0)\) for an embedding function \(f\), distance function \(d\), and margin \(\). Schroff et. al. forced their embeddings \(f(x)^{n}\) to lie on the \(n\)-dimensional hypersphere, and employed the Euclidean distance as their distance metric. We did not constrain our embeddings, and instead, like , used the cosine distance, which is agnostic to vector magnitudes and has greater theoretical support for out-of-distribution detection .

In keeping with the recommendations of , we preferred to train on semi-hard triplets via online triplet mining. We allowed each function in a mini-batch to serve as the anchor of at most one triplet, and when possible, selected this triplet to be the hardest (biggest contribution to the loss) semi-hard triplet available. If no semi-hard triplets with a given anchor were present in the batch, we selected the easiest (smallest contribution to the loss) hard triplet associated with that anchor. If all triplets associated with an anchor were easy, that anchor did not contribute to our loss.

Figure 1: The REFuSe architecture.

### Building a Training Dataset

The _main_ function is an easy example as to why we can't use simple function names as a way of identifying identical functions - many simple names are re-used across programs. For this reason, we take multiple steps to better identify when two functions are considered semantically equivalent. The thresholds specified below were based on manual inspection and random sampling until the results were consistent. Three rules developed below are used to determine function equivalence, and if no rule applied we assumed functions with the same name were equivalent iff they occurred in the same source code/project file.

First, we looked at the sizes of functions (in bytes after compilation) using any given function name. Small functions were associated with basic and near-universal utilities, e.g. those found in a C Runtime library, and were therefore unlikely to be semantically different across binaries. Based on manual inspection and random sampling, we determined if a function was \(\) 25 bytes in every instance and had the same name, it was labeled as semantically equivalent.

Secondly, we looked at the length of a function's name. We found that long function names tended to describe a function's purpose to such a high level of specificity that even when present in multiple pieces of source code, the semantic effect of that function remained consistent. Thus, function names \(\) 100 characters were labeled as semantically equivalent.

Third, we reasoned that functions with the same name and roughly the same size were likely to be semantically equivalent. (Small differences in size are expected due to changing build configurations: compiler versions, optimization levels, etc.) We measured the normalized standard deviation (i.e., \(/\)) of the function sizes associated with each function name. As a large difference in size is expected between "Debug" and "Release" mode builds, even with all other build configuration variables unaltered, we computed this statistic separately for the two different build modes for each function name. If a function name had a normalized standard deviation of less than 0.05 in both build modes, we did not further partition the labels associated with this function name.

After assigning labels, we took one last step to curate our dataset. Because singleton functions can only provide training signal as the negative member of a training triplet, we downsampled singletons in the training dataset, allowing them to make up at most 5% of all functions. This reduced the size of our training set from \(97,747,033\) functions to \(82,928,228\) functions. After downsampling, our dataset had a total of \(6,801,721\) unique function names. We created subdivided labels based on the originating source code for \(759,281\) function names and ended up with a total of \(8,897,318\) unique labels in our dataset. We use the standard Mean Reciprocal Rank (MRR) to evaluate the retrieval performance of a model. Due to dataset size, we use approximate nearest neighbor search when possible and return (upper,lower) bound MRR scores. Details on the MRR, bounds, and hyper-parameter settings are in the appendix.

## 5 Experiments and Results

We compared our baseline, REFuSe, against the two representative and SotA models from SS2.1: the GNN from , and jTrans . These models are not only representative of the major machine learning approaches to binary function similarity detection (graph neural network and transformer-based methods, respectively), but have been also been shown to outperform other work utilizing similar architectures [39; 54]. Both are made available under the MIT license.

As an initial baseline, we use the pre-trained models released by  and , respectively. To further isolate the effects of model architecture and training data on model performance, we conducted experiments with three additional models. First, we have re-trained the GNN on the Assemblage dataset from scratch (GNN-A), eliminating differences in training data as a factor. The jTrans algorithm does not release its pre-training code, which uses a process different from that of its fine-tuning code; consequently we have only fine-tuned jTrans (jTrans-F) on a sample of the Assemblage data. (The sample consists of 25M functions - compute restrictions prevented us from fine-tuning on the entire Assemblage dataset.) Finally, we experimented with using a basic transformer as a drop-in replacement for the convolutional layers in REFuSe.

In addition to the ML models described above, we also measured REFuSe against BSim, a non-neural tool for function similarity search developed by the Ghidra reverse engineering team .2 BSim works by first disassembling bytes, decompiling the disassembly to an intermediate representation called "p-code", then creating feature vectors from the p-code. It is hand-engineered by experts for finding similar functions and is intended to be invariant to changes in the architectural platform.

Over the course of our research we identified, and endeavored to benchmark against, several other neural models of interest, but were precluded from doing so by a variety of factors. First, as the jTrans paper itself noted, the computational cost of most methods is prohibitively exorbitant (e.g., graph matching is NP-hard). Second, most alternatives require proprietary software or do not share the code (and rely on highly technical domain-specific feature engineering). For example, the PalmTree algorithm  relies on software called BinaryNinja. Third, the Marcelli work did a large analysis of binary function similarity methods and identified the GNN as the overall most effective. Given the enormous cost of performing this analysis, we believe our current selection of models is well-justified.

**Metrics:** Our primary reported metric is mean reciprocal rank (MRR). A detailed description of the metric is given in Appendix A.2; here, we provide some intuition for understanding the scores. MRR can be interpreted as the average reciprocal of how many \(k\) items need to be retrieved to find the first relevant item. For example, an MRR of 0.6 means that, on average, 1/0.6 = 1.67 items need to be searched to find a relevant match, whereas an MRR of 0.01 would mean searching 1/0.01 = 100 items on average. Since reverse engineering tasks are only impacted by the first relevant result, MRR is the most directly correlative measure of real-world use, whereas Recall and Precision @k are impacted by the number of relevant items. This is problematic when different items have different denominators, which happens due to the filtering and compiler optimizations that can occur, as discussed in Table 3.

To facilitate comparison with earlier works in the literature, we also report recall@k scores in Appendix C, where recall is calculated as in .

**Validating REFuSe-Bench Label Normalization.** As described in SSA.1, when training REFuSe, training mini-batches are constructed so that every function has exactly one other function in the batch with its same label (to form an anchor-positive pair). The remaining functions in the batch are then candidates for the negative example in a training triplet. As each function can be part of only one anchor-positive pair, false positives (functions that are semantically different, but mistakenly given the same label) are uniquely damaging to the model, giving extremely erroneous training signal. On the other hand, false negatives (functions that are semantically the same but have different labels) have relatively mild effects. False negative functions are unlikely to be in the same batch (1/8.8M labels), and would almost certainly be a hard-negative (distance \( 0\)), and so not selected as a semi-hard sample.

While minimizing false positives at the expense of false negatives was advantageous when training REFuSe, the reverse was true when it came to evaluation. In early trials, we noticed that the majority of REFuSe's incorrect predictions were the result of false negatives, where functions with the same name and functionality, but arising from different source codes, were given different labels. Additionally, a significant minority of errors arose when the model matched two functions with the same purpose that operated on different types (e.g. _char_ and _wchar_t_).

In light of these observations, we explored the impact of various label normalization schemes on REFuSe's performance. We studied, both separately and together, the effect of two relaxations: allowing functions with the same name to have the same label regardless of originating source code, and allowing functions with the same name to have the same label regardless of parameter types. An example of our normalization scheme is found in Table 3, with the mean MRR achieved on the right.

In Table 5, our primary results table, we report results for REFuSe under the labeling scheme where functions with the same name have the same label, regardless of originating source code, but without parameter type masking (e.g. Table 3, row 3). We chose this labeling scheme because it aligns with the labeling schemes of the other models we evaluated against.

### Results

The complete results of our experiments are displayed in Table 5. Over the Assemblage dataset, REFuSe reached an MRR of 0.61, outperforming both the GNN and jTrans by a score of 0.35.

On MOTIF, the GNN's performance was hindered by being unable to support.NET executables and jTrans simply suffered from an inability to generalize to malware. The especially small training set of the GNN further hindered it from the power-law distributed Common Libraries bench, where jTrans did better but still behind REFuSe by \(\)17%. GNN's poor performance on the Common Libraries is further confirmed in appendix Figure 2 to validate that library size was not a confounding factor. jTrans' performance on this task is also marred by the fact that four of the seven libraries in that experiment -- abesi, cjson, opensl, and zlib -- are also present, in their Linux variants, in jTrans' training data, and so has unfair label leakage in its score.

Remarkably, REFuSe significantly outperforms the GNN on its original test set of Marcelli. (Although higher GNN scores are reported in the original Marcelli paper, the GNN's retrieval power is measured using a curated pool of functions, instead of using the entire test set, as we do here.) Though it appears that jTrans outperforms REFuSe on its test set of BinaryCor, the results are biased because it is constrained by a "pool-size", the maximum number of functions it can consider. We set the maximum pool size used in the original jTrans paper , where their code enforces that the true nearest neighbor is in the pool. Yet, when evaluating REFuSe, we search over all functions in the BinaryCor test set (4.79M) to find a matching function, not just over \(10,000\). This difference in pool size may partially explain the gap between jTrans' and REFuSe's scores, as the jTrans authors themselves demonstrate that the performance of their model decreases as pool size increases . This also limits real-world scalability of jTrans. First, we often never know what optimization levels are used, e.g., the MOTIF dataset is real-world malware with no such information. Second, compiling projects alone creates inequity in unique function counts (see , Table 4). Applying filtering and using real optimization like function-inlining further complicates this, making pairwise cross-optimization levels difficult to interpret. For this reason, we do not recommend them as a method of measuring model performance and do not replicate this practice for the evaluations in our paper.

Our first notable result is the strong performance of REFuSe across all experiments. Even though REFuSe does no feature engineering, and uses only a lightweight convolutional neural network, it is the top scoring model on four out of the five datasets we tested. Additionally, its behavior is the most stable of the models we examined, with a range of only 0.22 between its highest and lowest MRRs. Meanwhile, the GNN and jTrans have MRR ranges of 0.53 and 0.59, respectively. While performance on some datasets improves using the GNN-A/jTrans-F variants, which are trained/fine-tuned on Assemblage data, the performance of these models still lags significantly behind that of REFuSe, and helps isolate that these methods are in a sense over-engineered, as they perform worse

    & **Assemblage** & **MOTIF** & **Common Libraries** & **Marcelli Dataset-1** & **BinaryCorp** \\  GNN & (0.263, 0.284) & (0.591, 0.596) & (0.112, 0.138) & (0.050, 0.079) & (0.149, 0.174) \\ GNN-A & (0.267, 0.288) & (0.613, 0.622) & (0.109, 0.135) & (0.054, 0.084) & (0.089, 0.117) \\ jTrans & 0.26 & 0.10 & 0.58 & 0.08 & **0.693** \\ jTrans-F & 0.475 & 0.069 & 0.677 & 0.139 & 0.551 \\ BSim & (0.158, 0.187) & (0.143, 0.150) & (0.383, 0.414) & (0.279, 0.334) & (0.288, 0.322) \\ Naive Transformer & (0.433, 0.444) & (0.523, 0.530) & (0.496, 0.507) & (0.079, 0.106) & (0.164, 0.187) \\
**REFuSe** & **(0.611, 0.619)** & **(0.678, 0.683)** & **(0.684, 0.691)** & **(0.676, 0.683)** & (0.468, 0.477) \\   

Table 4: Mean reciprocal rank (MRR) for each model on each dataset. When Faiss is used for approximate nearest neighbor, we compute (lower, upper) bound MRR scores. **Best results in bold**.

   Label Norm. & Function **1** & Function **2** & MRR \\  None & \(21991\)std::collate<char>:do.compare & \(193204\){std::collate<wchar,>:do.compare & 0.088 \\ Mask type & \(21991\)!std::collate<#>:do.compare & \(193204\){std::collate<#>:do.compare & 0.130 \\ Mask source ID & std::collate<char>:do.compare & std::collate<wchar,>:do.compare & 0.615 \\ Mask ID \& type & std::collate<#>:do.compare & std::collate<#>:do.compare & 0.731 \\   

Table 3: Normalization schemes for Assemblage function labels.

even when using the same data. Likewise, BSim's results are much worse than REFuSe's, and are extremely expensive to extract - 24 minutes of CPU compute time per file. Lastly, while the naive transformer model shows promise on the MOTIF and Common Libraries datasets, with an MRR score of 0.53 on both, it is not able to transfer this knowledge to the Linux domain, and we see MRR scores plummet to 0.09 and 0.16 on the Marcelli and BinaryCorp datasets.

From these results, we can infer that (1) byte-based approaches are viable for the BFSD task and (2) more work is needed in feature engineering approaches to scale them to more realistic corpora. In either case, we recommend this suite of test cases as a way to test performance across many scenarios (Assemblage for general binaries, MOTIF for malware, Common Libraries as it is named, Marcelli and BinaryCorp for Linux and historical comparison). We believe that approaches incorporating domain knowledge will eventually outperform REFuSe in the long term, but we argue our results show that such expertise is not being efficaciously used today.

Table 3 demonstrates just how sensitive these results are to labeling schemes. There is a difference of 0.63 in MRR scores between the narrowest and loosest labeling schemes, which, since MRR is measured on a 0 to 1 scale, is extremely significant. To our knowledge, we are the first to investigate the effects of labeling in the context of BFSD, but we hope that our findings encourage others in the binary analysis space to continue this work and to propose thoughtful standards for how function similarity should be defined for a variety of applications. Devising less labor-intensive ways to develop and validate such strategies is a critical open question.

### Limitations

In practice, function similarity is not a same/different binary, but a spectrum, where many different factors influence degrees of sameness. For example, functions like _vec<bool>_ and _vec<int>_ have very similar logic, but very different implementations on the assembly level. Yet many ML training and evaluation methods do not fully capture this nuance. In Table 3, we give an initial analysis of the impact of function labeling based on the labeling criteria we put forth in SS5. Many of these criteria are heuristic, and we encourage future work to continue exploring and refining them.

## 6 Conclusion

Binary function similarity detection (BFSD) is a difficult problem that touches many security applications, including reverse engineering, malware analysis, and vulnerability detection. Many ML approaches have been devised to tackle this challenge, but current benchmarks do not mirror the real-world scenarios under which these models would be deployed. We identified five major discrepancies between prior corpora and real-world use, and developed REFuSe-Bench to mitigate these issues. Using a simple CNN over raw bytes, we show superior results compared to current leading BFSD approaches. Our results highlight the need to consider computational scale when using expensive domain feature extractors and how performance can vary across different types of binary data that was not testable in the prior literature. REFuSe-Bench provides the means to accelerate this complex and critical research area.