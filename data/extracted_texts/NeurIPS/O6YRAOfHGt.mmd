# Enhancing the Hierarchical Environment Design via Generative Trajectory Modeling

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Unsupervised Environment Design (UED) is a paradigm that automatically generates a curriculum of training environments, enabling agents trained in these environments to develop general capabilities, i.e., achieving good zero-shot transfer performance. However, existing UED approaches focus primarily on the random generation of environments for open-ended agent training. This is impractical in resource-limited scenarios where there is a constraint on the number of environments that can be generated. In this paper, we introduce a hierarchical MDP framework for environment design under resource constraints. It consists of an upper-level RL teacher agent that generates suitable training environments for a lower-level student agent. The RL teacher can leverage previously discovered environment structures and generate environments at the frontier of the student's capabilities by observing the student policy's representation. Additionally, to alleviate the time-consuming process of collecting the experience of the upper-level teacher, we utilize recent advances in generative modeling to synthesize a trajectory dataset for training the teacher agent. Our method significantly reduces the resource-intensive interactions between agents and environments, and empirical experiments across various domains demonstrate the effectiveness of our approach.

## 1 Introduction

The advances of reinforcement learning (RL)  have promoted research into the problem of training autonomous agents that are capable of accomplishing complex tasks. One interesting, yet underexplored, area is training agents to perform well in unseen environments, a concept referred to as zero-shot transfer performance. To this end, Unsupervised Environment Design (UED)  has emerged as a promising paradigm to address this problem. The objective of UED is to automatically generate environments in a curriculum-based manner, and training agents in these sequentially generated environments can equip agents with general capabilities, enabling agents to learn robust and adaptive behaviors that can be transferred to new scenarios without explicit exposure during training.

Existing approaches in UED primarily focus on building an adaptive curriculum for the environment generation process to train the generally capable agent. Dennis et al.  formalize the problem of finding adaptive curricula through a game involving an adversarial environment generator (teacher agent), an antagonist agent (expert agent), and the protagonist agent (student agent). The RL-based teacher is designed to generate environments that maximize regret, defined as the difference between the protagonist and antagonist agent's expected rewards. They show that these agents will reach a Nash Equilibrium where the student agent learns the minimax regret policy. However, since the teacher agent adapts solely based on the regret feedback, it is inherently difficult to adapt to studentpolicy changes. Meanwhile, training such an RL-based teacher remains a challenge because of the high computational cost of training an expert antagonist agent for each environment.

In contrast, domain randomization  based approaches circumvent the overhead of developing an RL teacher by training agents in randomly generated environments, resulting in good empirical performances. Building upon this, Jiang et al.  introduce an emergent curriculum by sampling randomly generated environments with high regret value 1 to train the agent. Parker-Holder et al.  then propose the adaptive curricula by manually designing a principled, regret-based curriculum, which involves generating random environments with increasing complexity. While these domain randomization-based algorithms have demonstrated good zero-shot transfer performance, they face limitations in efficiently exploring large environment design spaces and exploiting the inherent structure of previously discovered environments. Moreover, existing UED approaches typically rely on open-ended learning, necessitating a long training horizon, which is unrealistic in the real world due to resource constraints. Our goal is to develop a teacher policy capable of generating environments that are perfectly matched to the current skill levels of student agents, thereby allowing students to achieve optimal general capability within a strict budget for the number of environments generated and within a shorter training time horizon.

In this paper, we address these challenges by introducing a novel, adaptive environment design framework. The core idea involves using a hierarchical Markov Decision Process (MDP) to simultaneously formulate the evolution of an upper-level teacher agent, tasked with generating suitable environments to train the lower-level student agent to achieve general capabilities. To accurately guide the generation of environments at the frontier of the student agent's current capabilities, we propose approximating the student agent's policy/capability by its performances across a set of diverse evaluation environments, which acts as the state abstraction for the teacher's decision-making process. The transitions in the teacher's state represent the trajectories of the student agent's capability after training in the generated environment. However, collecting experience for the upper-level teacher agent is slow and resource-intensive, since each upper-level MDP transition evolves a complete training cycle of the student agent on the generated environment. To accelerate the collection of upper-level MDP experiences, we utilize advances in diffusion models that can generate new data points capturing complex distribution properties, such as skewness and multi-modality, exhibited in the collected dataset . Specifically, we employ diffusion probabilistic model [15; 6] to learn the evolution trajectory of student policy/capability and generate synthetic experiences to enhance the training efficiency of the teacher agent. Our method, called Synthetically-enhanced _H_ierarchical _E_nvironment _D_esign (_SHED_), automatically generates increasingly complex environments suited to the current capabilities of student agents.

In summary, we make the following contributions:

* We develop a novel hierarchical MDP framework for UED that introduces a straightforward method to represent the current capability level of the student agent.
* We introduce _SHED_, which utilizes diffusion-based techniques to generate synthetic experiences. This method can accelerate the training of the off-policy teacher agent.
* We demonstrate that our method outperforms existing UED approaches (i.e., achieving a better general capability under resource constraints) in different task domains.

## 2 Preliminaries

In this section, we provide an overview of two main research areas upon which our work is based.

### Unsupervised Environment Design

The objective of UED is to generate a sequence of environments that effectively train the student agent to achieve a general capability. Dennis et al.  first model UED with an Underspecified Partially Observable Markov Decision Process (UPOMDP), which is a tuple

\[=<A,O,,S^{},^{},^{},^{},>\]The UPOMDP has a set \(\) representing the free parameters of the environments, which are determined by the teacher agent and can be distinct to generate the next new environment. Further, these parameters are incorporated into the environment-dependent transition function \(^{}:S A S\). Here \(A\) represents the set of actions, \(S\) is the set of states. Similarly, \(^{}:S O\) is the environment-dependent observation function, \(^{}\) is the reward function, and \(\) is the discount factor. Specifically, given the environment parameters \(\), we denote the corresponding environment instance as \(_{}\). The student policy \(\) is trained to maximize the cumulative rewards \(V^{_{}}()=_{t=0}^{T}^{t}r_{t}\) in the given environment \(_{}\) under a time horizon \(T\), and \(r_{t}\) are the collected rewards in \(_{}\). Existing works on UED consist of two main strands: the RL-based environment generation approach and the domain randomization-based environment generation approach.

The RL-based generation approach was first formalized by Dennis et al.  as a self-supervised RL paradigm for generating environments. This approach involves co-evolving an environment generator policy (teacher) with an agent policy \(\) (student), where the teacher's role is to generate environment instances that best support the student agent's continual learning. The teacher is trained to produce challenging yet solvable environments that maximize the regret measure, which is defined as the performance difference between the current student agent and a well-trained expert agent \(^{*}\) within the current environment: \(Regret^{_{}}(,^{*})=V^{_{}}(^{*})-V^{_{}}()\).

The domain randomization-based generation approach, on the other hand, involves randomly generating environments. Jiang et al.  propose to collect encountered environments with high learning potentials, which are approximated by the Generalized Advantage Estimation (GAE) , and then the student agent can selectively train in these environments, resulting in an emergent curriculum of increasing difficulty. Additionally, Parker-Holder et al.  adopt a different strategy by using predetermined starting points for the environment generation process and gradually increasing complexity. They manually divide the environment design space into different difficulty levels and employ human-defined edits to generate similar environments with high learning potentials. Their algorithm, ACCEL, is currently the state-of-the-art (SOTA) in the field, and we use an edited version of ACCEL as a baseline in our experiments.

### Diffusion Probabilistic Models

Diffusion models  are a specific type of generative model that learns the data distribution. Recent advances in diffusion-based models, including Langevin dynamics and score-based generative models, have shown promising results in various applications, such as time series forecasting , robust learning , anomaly detection  as well as synthesizing high-quality images from text descriptions [8; 11]. These models can be trained using standard optimization techniques, such as stochastic gradient descent, making them highly scalable and easy to implement.

In a diffusion probabilistic model, we assume a \(d\)-dimensional random variable \(x_{0}^{d}\) with an unknown distribution \(q(x_{0})\). Diffusion Probabilistic model involves two Markov chains: a predefined forward chain \(q(x_{k}|x_{k-1})\) that perturbs data to noise, and a trainable reverse chain \(p_{}(x_{k-1}|x_{k})\) that converts noise back to data. The forward chain is typically designed to transform any data distribution into a simple prior distribution (e.g., standard Gaussian) by considering perturb data with Gaussian noise of zero mean and a fixed variance schedule \(\{_{k}\}_{k=1}^{K}\) for \(K\) steps:

\[q(x_{k}|x_{k-1})=(x_{k};}x_{k-1},_{k}) q(x_{1:K}|x_{0})=_{k=1}^{K}q(x_{k}|x_{k-1}),\] (1)

where \(k\{1,,K\}\), and \(0<_{1:K}<1\) denote the noise scale scheduling. As \(K\), \(x_{K}\) will converge to isometric Gaussian noise: \(x_{K}(0,)\). According to the rule of the sum of normally distributed random variables, the choice of Gaussian noise provides a closed-form solution to generate arbitrary time-step \(x_{k}\) through:

\[x_{k}=_{k}}x_{0}+_{k}}, (0,).\] (2)

Here \(_{k}=1-_{k}\) and \(_{k}=_{s=1}^{k}_{s}\). The reverse chain \(p_{}(x_{k-1}|x_{k})\) reverses the forward process by learning transition kernels parameterized by deep neural networks. Specifically, considering the Markov chain parameterized by \(\), denoising arbitrary Gaussian noise into clean data samples can be written as:

\[p_{}(x_{k-1}|x_{k})=(x_{k-1};_{}(x_{k},k),_{}(x _{k},k))\] (3)

It uses the Gaussian form \(p_{}(x_{k-1}|x_{k})\) because the reverse process has the identical function form as the forward process when \(_{t}\) is small . Ho et al.  consider the following parameterization of \(p_{}(x_{k-1}|x_{k})\):

\[_{}(x_{k},k)=}(x_{k}-}{}}_{}(x_{k},k))_{}(x_{k},k)=_{k}^{1/2}_{k}=}{1-_{k}}_{k}&k>1\\ _{1}&k=1\] (4)

\(_{}\) is a trainable function to predict the noise vector \(\) from \(x_{k}\). Ho et al.  show that training the reverse chain to maximize the log-likelihood \( q(x_{0}) p_{}(x_{0})dx_{0}\) is equivalent to minimizing re-weighted evidence lower bound (ELBO) that fits the noise. They derive the final simplified optimization objective:

\[()=_{x_{0},k,}[\|-_{ }(_{k}}x_{0}+_{k}},k)\|^{2} ].\] (5)

Once the model is trained, new data points can be subsequently generated by first sampling a random vector from the prior distribution, followed by ancestral sampling through the reverse Markov chain in Equation 3.

## 3 Approach

In this section, we formally describe our method, Synthetically-enhanced _H_ierarchical _E_nvironment _D_esign (_SHED_), which is a novel framework for UED under resource constraints. The _SHED_ incorporates two key components that differentiate it from existing UED approaches:

* A hierarchical MDP framework to generate suitable environments,
* A generative model to generate the synthetic trajectories.

_SHED_ uses a hierarchical MDP framework where an RL teacher leverages the observed student's policy representation to generate environments at the student's capabilities frontier. Such targeted environment generation process enhances the student's general capability by utilizing the underlying structure of previously discovered environments, rather than relying on the open-ended random generation. Besides, _SHED_ leverages advances in generative models to generate synthetic trajectories that can be used to train the off-policy teacher agent, which significantly reduces the costly interactions between the agents and the environments. The overall framework is shown in Figure 1, and the pseudo-code is provided in Algorithm 1.

### Hierarchical Environment Design

The objective is to generate a limited number of environments that are designed to enhance the general capability of the student agent. Inspired by the principles of PAIRED , we adopt an RL-based approach for the environment generation process. To better generate suitable environments tailored to the current student skill level, _SHED_ uses the hierarchical MDP framework, consisting of an upper-level RL teacher policy \(\) and a lower-level student policy \(\). Specifically, the teacher policy, \(:\), maps from the space of all potential student policies \(\) to the space of environment parameters \(\). Existing RL-based methods (e.g., PARIED) rely solely on regret feedback and fail to effectively capture the nuances of the student policy. To address this challenge, _SHED_ enhances understanding by encoding the student policy \(\) into a vector that serves as the state abstraction for teacher \(\). Rather than compressing the knowledge in the student policy network, we approximate the embedding of the student policy \(\) by assessing performance across a set of diverse evaluation environments. This performance vector, denoted as \(p()\), gives us a practical estimate of the student's current general capabilities, enabling the teacher to customize the next training environments accordingly. In our hierarchical framework, the environment generation process is governed by discrete-time dynamics. We delve into the specifics below.

**Upper-level teacher MDP**. The upper-level teacher operates at a coarser layer of student policy abstraction and generates environments to train the lower-level student agent. This process can be formally modeled as an MDP by the tuple \(<S^{u},A^{u},P^{u},R^{u},^{u}>\):

* \(S^{u}\) represents the upper-level state space. Typically, \(s^{u}=p()=[p_{1},,p_{m}]\) denotes the student performance vector across \(m\) diverse evaluation environments. This vector serves as the representation of the student policy \(\) and is observed by the teacher.

* \(A^{u}\) is the upper-level action space. The teacher observes the abstraction of the student policy, \(s^{u}\) and produces an upper-level action \(a^{u}\) which is the environment parameters \(\). \(\) (\(a^{u}\)) is then used to generate specific environment instances \(_{}\). Thus the upper-level action space \(A^{u}\) is the environment parameter space \(\).
* \(P^{u}\) denotes the action-dependent transition dynamics of the upper-level state. The general capability of the student policy evolves due to training the student agent on the generated environments.
* \(R^{u}\) provides the upper-level reward to the teacher at the end of training the student on the generated environment. The design of \(R^{u}\) will be discussed in Section 3.3.

As shown in Figure 2, given the student policy \(\), the teacher \(\) first observes the representation of the student policy, \(s^{u}=[p_{1},,p_{m}]\). Then teacher produces an upper-level action \(a^{u}\) which corresponds to the environment parameters. These environment parameters are subsequently used to generate specific environment instances. The lower-level student policy \(\) will be trained on the generated environments for \(C\) training steps. The upper-level teacher collects and stores the student policy evolution transition \((s^{u},a^{u},r^{u},s^{u,})\) every \(C\) times steps for off-policy training. The teacher agent is trained to maximize the cumulative reward giving the budget for the number of generated environments. The choice of the evaluation environments will be discussed in Section 3.3.

**Lower-level student MDP**. The generated environment is fully specified for the student, characterized by a Partially Observable Markov Decision Process (POMDP), which is defined by a tuple \(_{}=<A,O,S^{},^{}, ^{},^{},>\), where \(A\) represents the set of actions, \(O\) is the set of observations, \(S^{}\) is the set of states determined by the environment parameters \(\), similarly, \(^{}\) is the environment-dependent transition function, and \(^{}: O\) is the environment-dependent observation function, \(^{}\) is the reward function, and \(\) is the discount factor. At each time step \(t\), the environment produces a state observation \(s_{t} S^{}\), the student agent samples the action \(a_{t} A\) and interacts with environment \(\). The environment yields a reward \(r_{t}\) according to the reward function \(^{}\). The student agent is trained to maximize their cumulative reward \(V^{}()=_{t=0}^{C}^{t}r_{t}\) for the current environment under a finite time horizon \(C\). The student agent will learn a good general capability from training on a sequence of generated environments.

The hierarchical framework enables the teacher agent to systematically measure and enhance the general capability of the student agent and to adapt the training process accordingly. However, it's worth noting that collecting student policy evolution trajectories \((s^{u},a^{u},r^{u},s^{u,})\) to train the teacher agent is notably slow and resource-intensive, since each transition in the upper-level teacher MDP encompasses a training horizon of \(C\) timesteps for the student in the generated environment. Thus, it is essential to reduce the need for costly collection of upper-level teacher experiences.

### Generative Trajectory Modeling

In this section, we will formally introduce a generative model designed to ease the collection of upper-level MDP experience. This will allow us to train our teacher policy more efficiently. In particular, we first utilize a diffusion model to learn the conditional data distribution from the collected experiences \(=\{(s^{u}_{t},a^{u}_{t},r^{u}_{t},s^{p,}_{t})\}\). Later we can use the reverse chain in the diffusion model to generate the synthetic trajectories that can be used to help train the teacher agent, thereby alleviating the need for extensive and time-consuming collection of upper-level teacher experiences. We deal with two different types of timesteps in this section: one for the diffusion process and the other for the upper-level teacher agent, respectively. We use subscripts \(k\) to represent diffusion timesteps and subscripts \(t\) to represent trajectory timesteps in the teacher's experience.

In the image domain, the diffusion process is implemented across all pixel values of the image. In our setting, we diffuse over the next state \(s^{u,}\) conditioned the given state \(s^{u}\) and action \(a^{u}\). We construct our generative model according to the conditional diffusion process:

\[q(s^{u,}_{k}|s^{u,}_{k-1}), p_{}(s^{u,}_{k-1}|s^{u,}_{k},s^{u},a^{u})\]

As usual, \(q(s^{u,}_{k}|s^{u,}_{k-1})\) is the predefined forward noising process while \(p_{}(s^{u,}_{k-1}|s^{u,}_{k},s^{u},a^{u})\) is the trainable reverse denoising process. We begin by randomly sampling the collected experiences \(=\{(s^{u}_{t},a^{u}_{t},r^{u}_{t},s^{u,}_{t})\}\) from the real experience buffer \(_{real}\). Giving the observed state \(s^{u}\) and action \(a^{u}\), we use the reverse process \(p_{}\) to represent the generation of the next state \(s^{u,}\):

\[p_{}(s^{u,}_{0:K}|s^{u},a^{u})=(s^{u,}_{K};0,)_{k=1}^{K}p_{}(s^{u,}_{k-1}|s^{u,}_{k},s^{u},a^{u})\]

At the end of the reverse chain, the sample \(s^{u,}_{0}\), is the generated next state \(s^{u,}\). Similar to Ho et al. , we parameterize \(p_{}(s^{}_{k-1}|s^{}_{k},s^{u},a^{u})\) as a noise prediction model with the covariance matrix fixed as \(_{}(s^{u,}_{k},s^{u},a^{u},k)=_{i}\), and the mean is

\[_{}(s^{u,}_{i},s^{u},a^{u},k)=}}(s ^{u,}_{k}-}{_{k}}}_{}(s^ {u,}_{k},s^{u},a^{u},k))\]

\(_{}(s^{u,}_{k},s^{u},a^{u},k)\) is the trainable denoising function, which aims to estimate the noise \(\) in the noisy input \(s^{u,}_{k}\) at step \(k\).

**Training objective.** We employ a similar simplified objective to train the conditional \(\)- model:

\[()=_{(s^{u},a^{u},s^{u,}),k ,(0,)}[\|-_{ }(s^{u,}_{k},s^{u},a^{u},k)\|^{2}]\] (6)

Where \(s^{u,}_{k}=_{k}}s^{u,}+_{k}}\). The intuition for the loss function \(()\) is to predict the noise \((0,)\) at the denoising step \(k\), and the diffusion model is essentially learning the student policy involution trajectories collected in the real experience buffer \(_{real}\). Note that the reverse process necessitates a substantial number of steps \(K\). Recent research by Xiao et al.  has demonstrated that enabling denoising with large steps can reduce the total number of denoising steps \(K\). To expedite the relatively slow reverse sampling process (as it requires computing \(_{}\) networks \(K\) times), we use a small value of \(K\). Similar to Wang et al. , while simultaneously setting \(_{}=0.1\) and \(_{}=10.0\), we define:

\[_{k}=1-(_{}-0.5(_{}-_{ })})\]

This noise schedule is derived from the variance-preserving Stochastic Differential Equation by Song et al. .

**Generate synthetic trajectories.**Once the diffusion model has been trained, it can be used to generate synthetic experience data by starting with a draw from the prior \(s^{u,}_{K}(0,)\) and successively generating denoised next state, conditioned on the given \(s^{u}\) and \(a^{u}\) through the reverse chain \(p_{}\). Note that the giving condition action \(a\) can either be randomly sampled from the action space or use another diffusion model to learn the action distribution giving the initial state \(s^{u}\). This new diffusion model is essentially a behavior-cloning model that aims to learn the teacher policy \((a^{u}|s^{u})\). This process is similar to the work of Wang et al. . We discuss this process in detail in the appendix. In this paper, we randomly sample \(a^{u}\) as it is straightforward and can also increase the diversity in the generated synthetic experience to help train a more robust teacher agent.

After obtaining the generated next state \(s^{u,}\) conditioned on \(s^{u},a^{u}\), we compute reward \(r^{u}\) using teacher's reward function \(R(s^{u},a^{u},s^{u,})\). The specifics of how the reward function is chosen are explained in the following section.

### Rewards and Choice of evaluate environments

**Selection of evaluation environments.** The upper-level teacher generates environments tailored for the lower-level student to improve its general capability. Thus it is important to select a set of diverse suitable evaluation environments as the performance vector reflects the student agent's general capabilities and serves as an approximation of the policy's embedding. Fontaine and Nikolaidis  propose the use of quality diversity (QD) optimization to collect high-quality environments that exhibit diversity for the agent behaviors. Similarly, Bhatt et al.  introduce a QD-based algorithm for dynamically designing such evaluation environments based on the current agent's behavior. However, it's worth noting that this QD-based approach can be tedious and time-consuming, and the collected evaluation environments heavily rely on the given agent policy.

Given these considerations, it is natural to take advantage of the domain randomization algorithm, as it has demonstrated compelling results in generating diverse environments and training generally capable agents. In our approach, we first discretize the environment parameters into different ranges, then randomly sample from these ranges, and combine these parameters to generate evaluation environments. This method can generate environments that may induce a diverse performance for the same policy, and it shows promising empirical results in the final experiments.

**Reward design.** We define the reward function for the upper-level teacher policy as a parameterized function based on the improvement in student performance in the evaluation environments after training in the generated environment:

\[R(s^{u},a^{u},s^{u,})=_{i=1}^{m}(p^{}_{i}-p_{i})\]

This reward function gives positive rewards to the upper-level teacher for taking action to create the right environment to improve the overall performance of students across diverse environments. However, it may encourage the teacher to obtain higher rewards by sacrificing student performance in one subset of evaluation environments to improve student performance in another subset, which conflicts with our objective to develop a student agent with general capabilities. Therefore, we need to consider fairness in the reward function to ensure that the generated environment can improve student's general capabilities. Similar to , we build our fairness metric on top of the change in student's performance in each evaluation environment, denoted as \(_{i}=p^{}_{i}-p_{i}\), and we have \(=_{i=1}^{m}_{i}\). We then measure the fairness of the teacher's action using the coefficient of variation of student performances:

\[cv(s^{u},a^{u},s^{u,})=_{i}- )^{2}}{^{2}}}\] (7)

A teacher is considered to be fair if and only if the \(cv\) is smaller. As a result, our reward function is:

\[R(s^{u},a^{u},s^{u,})=_{i=1}^{m}(p^{}_{i}-p_{i})- cv (s^{u},a^{u},s^{u,})\] (8)

Here \(\) is the coefficient that balances the weight of fairness in the reward function (We set a small value to \(\)). This reward function motivates the teacher to generate training environments that can improve student's general capability.

## 4 Experiments

In this section, we conduct experiments to compare _SHED_ to other leading approaches on three domains: Lunar Lander, maze and a modified BipedalWalker environment. Experimental details and hyperparameters can be found in the Appendix. Specifically, our primary comparisons involve _SHED_ and _h-MDP_ (our proposed hierarchical approach without diffusion model aiding in training) against four baselines: domain randomization , ACCEL, , Edited ACCEL(with slight modifications that it does not revisit the previously generated environments), PAIRED . In all cases, we train a student agent via Proximal Policy Optimization (PPO , and train the teacher agent via Deterministic policy gradient algorithms(DDPG ), because DDPG is an off-policy algorithm and can learn from both real experiences and the synthetic experiences.

**Setup.** For each domain, we construct a set of evaluation environments and a set of test environments. The vector of student performances in the evaluation environments is used as the approximation of the student policy (as the observation to teacher agent), and the performances in the test environments are used to represent the student's zero-shot transfer performances (general capabilities). Note that in order to obtain a fair comparison of zero-shot transfer performance, the evaluation environments and test environments do not share the same environment and they are not present during training.

Lunar Lander.This is a classic rocket trajectory optimization problem. In this domain, student agents are tasked with controlling a lander's engine to safely land the vehicle. Before the start of each episode, teacher algorithms determine the environment parameters that are used to generate environments in a given play-through, which includes gravity, wind power, and turbulence power. These parameters directly alter the difficulty of landing the vehicle safely. The state is an 8-dimensional vector, which includes the coordinates of the lander, its linear velocities, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.

We train the student agent for 1e6 environment time steps and periodically test the agent in test environments. The parameters for the test environments are randomly generated and fixed during training. We report the experiment results on the left side of Figure 3. As we can see, student agents trained under _SHED_ consistently outperform other baselines and have minimal variance in transfer performance. During training, the baselines, except h-MDP, show a performance dip in the middle. This phenomenon could potentially be attributed to the inherent challenge of designing the appropriate environment instance in the large environment parameter space. This further demonstrates the effectiveness of our hierarchical design (_SHED_ and h-MDP), which can successfully create environments that are appropriate to the current skill level of the students.

Bipedalwalker.We also evaluate _SHED_ in the modified BipedalWalker from Parker-Holder et al. . In this domain, the student agent is required to control a bipedal vehicle and navigate across the terrain, and the student receives a 24-dimensional proprioceptive state with respect to its lidar sensors, angles, and contacts. The teacher is tasked to select eight variables (including ground roughness, the

Figure 3: _Left_: The average zero-shot transfer performances on the test environments in the Lunar lander environment (mean and standard error). _Right_: The average zero-shot transfer performances on the test environments in the BipedalWalker (mean and standard error).

number of stairs steps, min/max range of pit gap width, min/max range of stump height, and min/max range of stair height) to generate the corresponding terrain.

We use similar experiment settings in prior UED works, we train all the algorithms for 1e7 environment time steps, and then evaluate their generalization ability on ten distinct test environments in Bipedal-Walker domain. The parameters for the test environments are randomly generated and fixed during training. As shown in Figure 3, our proposed method _SHED_ surpasses all other baselines and achieves performance levels nearly on par with the SOTA (ACCEL). Meanwhile, SHED maintains a slight edge in terms of stability and overall performance and PAIRED suffers from a considerable degree of variance in its performance.

Partially observable Maze.Here we study navigation tasks, where an agent must explore to find a goal while navigating around obstacles. The environment is partially observable, and the agent's field of view is limited to a \(3 3\) grid area. Unlike the previously mentioned domains, maze environments are non-parametric and cannot be directly represented by compact parameter vectors due to their high complexity. To solve this challenge, we propose a novel method to generate maze by leveraging advances in large language models (e.g., ChatGPT). Specifically, we implement a retrieval-augmented generation (RAG) process to optimize the ChatGPT's output such that it can generate desired maze environments. This process ensures that large language models reference authoritative knowledge bases to generate feasible mazes. To simplify the teacher's action space, we extracted several key factors that constitute the teacher's action space (environmental parameters) for maze generation. Details on maze generation are provided in Appendix D.3, and prompt are included in Appendix D.4.

The average zero-shot transfer performances are reported in Figure 4. Notably, _SHED_ demonstrates the highest performance, consistently improving and achieving the highest cumulative rewards. The performance of h-MDP steadily improves but does not reach the highest levels, which further highlights the advantages of incorporating the generated synthetic datasets to train an effective RL teacher agent. Meanwhile, Accel-Edit and Accel show higher variances in performance, indicating that random teachers are less stable in finding a suitable environment to train student agents.

Ablation and additional ExperimentsIn Appendix C, we evaluate the ability of the diffusion model to generate the synthetic student policy involution trajectories. We further provide ablation studies to assess the impact of different design choices in Appendix E.1. Additionally, in Appendix E.2, we conduct experiments to show how the algorithm performs under different settings, including scenarios with a larger budget constraint on the number of generated environments or a larger weight assigned to CV fairness rewards. Notably, all results consistently demonstrate the effectiveness of our approach.

## 5 Conclusion

In this paper, we introduce an adaptive approach for efficiently training a generally capable agent under resource constraints. Our approach is general, utilizing an upper-level MDP teacher agent that can guide the training of the lower-level MDP student agent agent. The hierarchical framework can incorporate techniques from existing UED works, such as prioritized level replay (revisiting environments with high learning potential). Furthermore, we have described a method to assist the experience collection for the teacher when it is trained in an off-policy manner. Our experiment demonstrates that our method outperforms existing UED methods, highlighting its effectiveness as a curriculum-based learning approach within the UED framework.

Figure 4: Average zero-shot transfer performance on the test environments in the maze environments.