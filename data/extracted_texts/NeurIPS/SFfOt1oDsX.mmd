# Graph Convolutional Kernel Machine

versus Graph Convolutional Networks

Zhihao Wu\({}^{1}\), Zhao Zhang\({}^{1,2}\), Jicong Fan\({}^{1,3}\)

\({}^{1}\)Shenzhen Research Institute of Big Data, Shenzhen, China

\({}^{2}\)Hefei University of Technology, Hefei, China

\({}^{3}\)The Chinese University of Hong Kong, Shenzhen, China

{zhihaowu1999,cszzhang}@gmail.com, fanjicong@cuhk.edu.cn

Corresponding author.

###### Abstract

Graph convolutional networks (GCN) with one or two hidden layers have been widely used in handling graph data that are prevalent in various disciplines. Many studies showed that the gain of making GCNs deeper is tiny or even negative. This implies that the complexity of graph data is often limited and shallow models are often sufficient to extract expressive features for various tasks such as node classification. Therefore, in this work, we present a framework called graph convolutional kernel machine (GCKM)1 for graph-based machine learning. GCKMs are built upon kernel functions integrated with graph convolution. An example is the graph convolutional kernel support vector machine (GCKSVM) for node classification, for which we analyze the generalization error bound and discuss the impact of the graph structure. Compared to GCNs, GCKMs require much less effort in architecture design, hyperparameter tuning, and optimization. More importantly, GCKMs are guaranteed to obtain globally optimal solutions and have strong generalization ability and high interpretability. GCKMs are composable, can be extended to large-scale data, and are applicable to various tasks (e.g., node or graph classification, clustering, feature extraction, dimensionality reduction). The numerical results on benchmark datasets show that, besides the aforementioned advantages, GCKMs have at least competitive accuracy compared to GCNs.

## 1 Introduction

Graph data are prevalent in science and engineering. In recent years, neural networks and deep learning methods have shown promising performance in handling various graph data. Graph Neural Networks (GNNs) have been developed into various basic frameworks, including famous GCN , GIN , GraphSAGE , GAT , etc. As one of the most attractive GNN paradigms, GCN, the graph convolutional network , has gained widespread attention since it was proposed. Many widely developed domains like machine learning , data mining , computer vision , etc., have also made extensive use of numerous GCN variants. Despite the better performance compared to classical graph methods (e.g., DeepWalk , LINE  and node2vector ), some recent studies have pointed out a few drawbacks of GCNs . Perhaps the failure of deep GCNs is the most notorious one that severely restricted the further development of GCNs.

Containing both the self features and pair-wise relations of entities, graph is viewed as a more complex non-Euclidean data type, so there is a natural and wide desire to construct deeper GCNs. Unfortunately, the performance of GCN decreases when stacking more graph convolutional layers, which is defined as the so-called over-smoothing issue . For instance, even a 7-layer GCN will inevitably generate indistinguishable node embeddings and lead to over \(30\%\) accuracy degradation . Recently, some methods modifying the basic structure of GCN have been proposed to handle the over-smoothing issue and make GCN deeper . Indeed, most of them have successfully alleviated the over-smoothing issue. But it seems counter-intuitive that these elaborate deep GCNs, which stack dozens or even hundreds of layers, can only laboriously maintain the performance that a 2-layer vanilla GCN achieves easily. We apply some GCNs to a node classification experiment on Cora following the settings of the original GCN, as shown in Figure 1. APPNP  and JKNet  are two well-known models that attempted to pass information from shallow layers to the final output but benefited very limited from the deep neural networks. These explorations raise a question:

_Do we really need deep GCNs in common graph-based tasks?_

Actually, there have also been some efforts paid to simplifying GCN, showing comparative effectiveness to vanilla GCN, like SGC , which removed nonlinearity and collapsed weight matrices to accelerate GCN. Impressively, it gained equal or even better performance on the benchmark graph datasets, which can partially be observed in Figure 1. Inspired by the success of SGC, some research moved further to decouple the graph convolution by individually performing an MLP (usually 2-layer) and several (can be hundreds) neighbor aggregations, called decoupled-style GCN . All these works revealed that simple GCNs can achieve encouraging performance on commonly used graph datasets such as Cora, Citeseer, and Pubmed . In these datasets, the numerical node features and the adjacency matrices are already discriminative, which means further feature extractions via neural networks do not improve the performance significantly.

However, SGC removes the nonlinearities of GCN and essentially becomes a linear classifier in terms of features given by neighbor aggregations, which may limit the possibility of extending to more complex tasks. In general, considering the limitations of the deep GCNs and SGC, one can conclude that: 1_the neighbor aggregation is essential for GCNs; 2_a simple feature mapping is helpful and sufficient but should be expressive enough._ These two points as well as the effectiveness of kernel methods in various machine learning problems  inspire us to establish a general and simple kernel-based framework, termed Graph Convolutional Kernel Machine (GCKM), for graph data. GCKM incorporates implicit feature mapping induced by kernels and neighbor aggregation over graphs, providing a new paradigm for graph-based machine learning. The main contributions of this work are summarized as follows:

* We propose a GCKM framework for graph-based learning. GCKM takes advantages of kernel learning and graph learning. Compared to GCNs, GCKMs have lower computational costs, higher interpretability, stronger theoretical guarantees, and stabler performances.
* We provide a generalization error bound for GCKM-based node classification and prove that graph structure can tighten this bound, both theoretically and empirically.
* We provide a few variants of GCKM that are useful in various graph-based learning problems such as node clustering and node embedding.
* We extend GCKMs to graph-level learning such as graph classification.

Comprehensive experiments demonstrate that the proposed GCKMs are as powerful as the state-of-the-art GCNs and significantly surpass several GCN-based methods on node semi-supervised classification and clustering tasks.

Figure 1: Accuracy and standard deviations of deep GCNs, SGC, and GCN with diverse layer/hop numbers.

**Notations** Given an undirected attributed graph \(=(,)\), where \(\) and \(\) are the vertex set and edge set, respectively, and \(||=n\) is the number of nodes. The node features are represented as matrix \(=[_{1},_{2},,_{n}]^{} ^{n m}\), where \(_{i}^{m}\) denotes the column feature vector of node \(i\). And edges can be described by an adjacency matrix \(^{n n}\), where \(A_{ij}=1\) if there exists an edge connecting node \(i\) and \(j\), otherwise \(A_{ij}=0\). We use \(_{i}\) to denote the \(i\)-th row of \(\). We denote the self-looped adjacency matrix as \(}=+\). The degree matrix corresponding to diagonal \(}\) is denoted by \(}^{n n}\), where \(_{ii}=_{j=1}^{n}_{ij}\). \(\|\|\) denotes the Euclidean norm of vector. Let \(_{i}\{0,1\}^{k}\) be the label vector of node \(i\) and \(=\{_{i}: i\}\), where \(\) denotes the index set of labeled nodes. For node classification, the task is to learn a model \(f_{}()\) from \(\), \(\), and \(\) to classify the unlabeled nodes.

## 2 Related Work

**Vanilla and Deep GCNs.**Kipf and Welling (2017) proposed GCN which further developed ChebyNet (Defferrard _et al._, 2016) and formulated an efficient and effective graph convolutional operation. Up to now, numerous studies have tried to explore and interpret GCN, and a widely applied description of GCN decoupled the graph convolution into the following two steps

\[}^{(l+1)} =;^{(l)} }=}^{(l)},\] (1) \[^{(l+1)} =}^{(l+1)}= }^{(l+1)}^{(l)},\]

where \(^{(l)}\) and \(^{(l+1)}\) are the input and output representations of the \(l\)-th layer. \(}=}^{-}}}^{-}\) is the renormalized adjacency matrix and \(()\) is an activation function, e.g. \(()\). These two operations compose the layer-wise propagation of GCN, but plenty of research has revealed that constructing a GCN with more such layers would deteriorate the performance. Li _et al._ (2018) first pointed out the over-smoothing issue is the key point of this phenomenon, that is, repeatedly multiplying the input node features by adjacency matrix eventually makes them more and more similar, resulting in indistinguishable node embeddings. This view has been broadly acknowledged, and many studies have been conducted on how to solve the over-smoothing issue. APPNP (Klicpera _et al._, 2019) considered the relationship between GCN and PageRank to design a new aggregation scheme, and improve the performance of deep GCN. JKNet (Xu _et al._, 2018) is another famous model for the same goal, which enabled each node to adaptively gain information from different hops of neighborhood. Nevertheless, different from the well-known deep CNNs, these approaches only alleviate the performance degradation but hardly benefit from the deep layers. More work on deep GCNs can be found in (Chen _et al._, 2020; Sun _et al._, 2021; Chen _et al._, 2023; Xu _et al._, 2023).

**Simplified GCNs.** Although GCN put forward a simple formula of forward propagation rule, its mechanisms have not been fully discussed and may contain redundant computation, which suggests simplified GCNs. Among these, SGC (Wu _et al._, 2019) removed the nonlinearity of GCN and only retained one learnable weight, so that built a faster model by precomputing the powers of the adjacency matrix. Wu _et al._ (2019) reported some interesting results that this intuitive way can still achieve the powerful expressive ability of vanilla GCN even with a relatively small solution space. Following this work, some decoupled-style GCNs have been designed, which decouple the two processes in (1) and then only stack the aggregation step while reducing the transformations. For example, Nt and Maehara (2019) claimed that graph structure only provided a denoising approach and GCN and SGC only performed low-pass filtering on node features. They then proposed gfNN to improve SGC from the perspective of graph signal processing. Liu _et al._ (2020) hypothesized that the coupling of these two operations causes not only more computations but also the over-smoothing issue, and introduced DAGNN by processing graph data via an MLP separately before performing the neighbor aggregation. In light of DAGNN, Dong _et al._ (2021) further analyzed the mechanism of decoupled GCNs, concluding the effectiveness of this kind of simplifying approaches.

## 3 Graph Convolutional Kernel Machine

### General Framework

GCN can be decoupled as two key steps: neighbor aggregation and feature transformation, as formulated in (1). The former is a fixed operation and only the feature transformation contains thelearnable weights \(^{(l)}\), and it has been discovered that stacking these two operations significantly damaged the performance. Therefore, some research has been dedicated to solving this problem and making GCN deeper, but to little avail. On the other hand, various simplified GCNs reduced the learnable parameters and are competitive to those deeper GCNs, and SGC even simplified the feature transformation to one weight matrix without activation functions. In other words, despite the larger solution space provided by the nonlinearity and more learnable weights, improperly designed feature transformations may cause significant performance deterioration, while reduction of weights can lead to comparable power. Meanwhile, the representative SGC may be less expressive to cope with more complex data owing to its linear classifier. So it is crucial to present a light model by exploring an effective as well as efficient feature transformation approach.

Neighbor aggregation is commonly regarded as the pivotal component and makes the node representations more compact. Paired with a feature map to low-dimensional space, it may lead to indistinguishable node representations between different classes. Thus, to guarantee discriminative node representations, we propose to map node features to a higher-dimensional space via a transformation operation

\[:=()=[(_{1}),(_{2}),,(_{n})]^{},\] (2)

where \(:^{m}^{M}\) is a feature mapping and \(M>m\). Explicitly designing a \(\) is usually costly especially when \(M\) is very large or even infinity. Instead, we can take advantage of kernels that can induce \(\) implicitly. Let \(k:\) be a kernel function, we have

\[k(,^{})=(),(^{ })=()^{}(^{}).\] (3)

Thus we can obtain a kernel matrix of \(\) as

\[=[k(_{1},_{1})&& k(_{1},_{n})\\ &&\\ k(_{n},_{1})&&k(_{n},_{n})\\ ].\] (4)

There are many kernel functions available and each kernel function induces a specific feature map \(\). For instance, the \(\) induced by a polynomial kernel \(k(,^{})=(^{}^{}+a)^{b}\) is a \(b\)-order polynomial feature map; the \(\) induced by a Gaussian kernel \(k(,^{})=(-\|-^{} \|^{2}/(2^{2}))\) is an infinity-order polynomial feature map, namely, \(M=\).

We can alternatively perform the neighbor aggregation and the transformation (2), and obtain a layer-wise formulation of graph convolutional kernel machine as follows

\[}^{(l+1)}&= \{;^{(l)}\}=}^{q}^ {(l)},\\ ^{(l+1)}&=}^{(l+1)}=_{(l)}(}^{(l+1)}),\] (5)

where \(_{(l)}\) is the \(l\)-th implicit feature map induced by a kernel \(k^{(l)}\) and the corresponding kernel matrix is \(^{(l+1)}=^{(l+1)}^{(l+1)}{}^{}\). Here the \(q\)-th power of \(}\) is adopted to allow several times graph convolution. Formulation (5) can be easily extended to multi-layer cases, but \(^{(l+1)}\) is actually implicit and we calculate the kernel matrix \(^{(l+1)}\) directly in practice. It is necessary to discuss

Figure 2: Overview of the proposed Graph Convolutional Kernel Machine (GCKM).

the connection between \(^{(l+1)}\) and \(^{(l)}\) and how to derive \(^{(l+1)}\) layer by layer with only kernel matrices. Thus we will take the Gaussian kernel as an example, give the recursive formulation, and show how to derive the corresponding formulation in terms of kernel matrices. With a Gaussian kernel, we have \(K_{ij}^{(l+1)}=(-^{(l+1)}}{2_{(l+1)}^{2}})\), \((i,j)[n][n]\), where \(_{ij}^{(l+1)}=\|}_{i}^{(l+1)}-}_{j }^{(l+1)}\|^{2}\) is an element from \(^{(l+1)}\), which is a squared distance matrix of \(}^{(l+1)}\). By a series of derivations (deferred to Appendix A), we have

\[^{(l+1)}=_{}^{(l+1)}}_{n}^{ }+_{n}_{}^{(l+1)}}^{}-2}^{(l+1)},\] (6)

where \(}^{(l+1)}=}^{q}^{(l)}(}^{q})^{}\) and \(_{}^{l+1}}=[_{11}^{(l+1)},_{22}^{(l+1) },,_{nn}^{(l+1)}]^{}\). Thus the recursive formulation can be summarized as

\[}^{(l+1)}=}^{q}^{(l)}( }^{q})^{},\\ ^{(l+1)}=(-^{(l+1)}}{2_{(l+1)}^ {2}}).\] (7)

Particularly, we define \(^{(0)}=^{}\). Based on (7), GCKM with Gaussian kernels can be easily generalized to multi-layer cases. It is worth noting that similar compositions also apply to other kernels such as polynomial kernel and Laplace kernel, which will not be detailed here.

After obtaining the kernel matrix \(^{(L)}\) from an \(L\)-layer GCKM, we can use Support Vector Machine (SVM) (Cortes and Vapnik, 1995) to perform node classification. Generally, SVM aims to solve

\[,}{}\ \ \|\|^{2}+_{i=1}^{n}_{i}\ \ \ \ \ \ y_{i}(_{i}), 1 -_{i},\ \ _{i} 0,\ \  i=1,,n,\] (8)

where \(y_{i}\{+1,-1\}\) denotes the target label of training data \(i\) and \(_{i}\) are the slack variables for the training data, and \(\) is a feature map. The Lagrangian dual problem is

\[}{}\ \ _{i=1}^{n}c_{i}-_{i=1}^{n} _{j=1}^{n}c_{i}c_{j}y_{i}y_{j}(_{i})^{}( _{j})\ \ \ \ \ \ _{i=1}^{n}c_{i}y_{i}=0,\ 0 c_{i}.\] (9)

Invoking \(^{(L)}\) into (9), where \((_{i})=_{(L)}((}_{i}^{(L)})^{})\), we obtain the following optimization problem of GCKM with \(L\) layers for node classification:

\[}{}\ \ _{i=1}^{n}c_{i}-_{i=1}^{n} _{j=1}^{n}c_{i}c_{j}y_{i}y_{j}K_{ij}^{(L)}\ \ \ \ \ _{i=1}^{n}c_{i}y_{i}=0,\ 0 c_{i} .\] (10)

For convenience, we call (10) graph convolutional kernel SVM (GCKSVM). Compared to GCN which is nonconvex, GCKSVM is convex and hence we can find its global optimal solution easily. In addition, kernel methods often have higher interpretability than neural networks and stronger generalization guarantees. Compared to SGC which is a linear classifier with respect to the final node representation, GCKSVM is a nonlinear classifier and hence is expected to perform better when the data are not linearly separable. Further justification and discussion will be presented in Figure 3 and its related explanations.

### Generalization Bounds of GCKSVM for Node Classification

We analyze the generalization ability of GCKSVM for node classification. Different from (Bartlett and Shawe-Taylor, 1999; Bartlett and Mendelson, 2002; Gronlund _et al._, 2020) that focuses on the primary problem of SVM, we focus on the dual problem, otherwise, the graph structure does not explicitly present in the bound.

Let \(}\) be the solution to (10) and denote the corresponding \(\) as \(}\), where we use the Gaussian kernel for GCKSVM. Denote \(S\) be the set of training data drawn from some distribution \(\). Let \(_{S}(},^{(L)}):=_{((),y )}[y(),} 1]\) be the training error and \(_{}(},^{(L)}):=_{(( ),y)}[((),}) y]=_{((),y)}[y (),} 0]\) be the expected test error. We have the following generalization error bound.

**Theorem 1**.: _Denote the index set of the support vectors as \(=\{i:\;1 i n,\;_{i} 0\}\). For any \(0<<1\), it holds with probability at least \(1-\) over a set of \(n\) samples \(S\) that_

\[_{}(},^{(L)})_{ S}(},^{(L)})+O(+ }_{S}(},^{(L)})),\] (11)

_where \(=_{i}_{i}^{2}+_{i}_{j  i,y_{i}=y_{j}}_{i}_{j}K_{ij}^{(L)}-_{i }_{j,y_{i} y_{j}}_{i}_{j}K _{ij}^{(L)}||^{2}}{n^{2}}\)._

The proof for the theorem is in Appendix B. In the theorem, \(K_{ij}^{(L)}=-\|(}^{q})_{i}^{(L-1)}-( {}^{q})_{j}^{(L-1)}\|^{2}/(2_{L}^{2})\), which shows the direct connection between the error bound and graph. Empirically (see Appendix F.6), we find that given a fixed \(\), the graph convolution reduces the number of support vectors \(||\) and has minor influence on the training error \(_{S}(},^{(L)})\), which eventually reduces the upper bound of test error. The fundamental reason is that incorporating the graph structure significantly improved the quality of the kernel matrix \(^{(L)}\), in which the overall within-class similarity becomes much larger than the between-class similarity. In other words, the kernel matrix becomes more discriminative, which can be verified by Figure 4.

Note that the number of support vectors \(||\) in SVM is data-dependent and in order to theoretically show the influence of graph convolution on \(||\), we have to make the following assumption:

**Assumption 1**.: _The aggregation step with graph \(\) increases the inner product between the kernel feature maps of samples in the same class and reduces or does not change the inner product between the kernel feature maps of samples in different classes._

This is a reasonable assumption because a useful graph should make the samples from different classes more distinguishable or at least make the samples from the same class more similar.

**Theorem 2**.: _Given a graph \(\) that satisfies Assumption 1, let \(\) and \(_{}\) be the kernel feature maps without and with aggregation on graph \(\) respectively. The corresponding negative Lagrangian dual objectives (to minimize) are denoted as \(():=_{i=1}^{n}_{j=1}^{n}c_{i}c_{j}q_{ ij}-_{i=1}^{n}c_{i}\), where \(q_{ij}=y_{i}y_{j}(_{i})^{}(_{j})\) and \(_{}():=_{i=1}^{n}_{j=1}^{n}c _{i}c_{j}q_{ij}^{}-_{i=1}^{n}c_{i}\), where \(q_{ij}^{}=y_{i}y_{j}_{}(_{i})^{} _{}(_{j})\). We have_

\[_{}()()+( ),\] (12)

_where \(() 0\) is a regularization term inducing sparsity in \(\)._

The proof as well as the details about \(()\) are in Appendix C. According to Theorem 2, the aggregation step introduces an additional sparse regularization term \(()\), which will make \(\) sparser, or in other words, reduce the number of support vectors \(||\). Considering both the theoretical and empirical results, we conclude that graph convolution leads to a tighter generalization error bound, which verifies the effectiveness of our GCKSVM. By the way, the interpretability of GCKSVM is higher than that of GCN, owing to the support vectors.

### Extensions

Besides SVM, our method can be generalized to many other machine learning problems. We show a few examples of node-level learning in the following context.

* **GCKM for Spectral Clustering (GCKSC)** For node clustering, it is natural to perform spectral clustering [22; 13] using the kernel matrix \(^{(L)}\). Instead of using the generally dense matrix \(^{(L)}\), we retain only the largest values of each column to obtain a sparse matrix \(}^{(L)}\), which often yields better clustering performance.
* **GCKM for Principal Component Analysis (GCKPCA)** It is straightforward to conduct Kernel PCA  using \(^{(L)}\), where the principal components can be used as the representations of the nodes of \(\). Please refer to Appendix F.5 for experiments.
* We can also adapt GCKM to other learning models such as self-expressive models , Fisher linear discriminant analysis , canonical correlation analysis , etc.

Now we extend GCKM to graph-level learning. Suppose we have a set of graphs \(=\{_{1},_{2},,_{N}\}\), where \(|V_{g}|=n_{g}\), \(g=1,2,,N\). For each \(_{g}\), we use (7) to obtain \(_{g}^{(L)}\) as well as the implicit node-level representations \(_{g}^{(L)}\). For each \(_{g}\), we compute the implicit graph-level representation as the summation of node-level representations, i.e.,

\[_{g}^{(L)}=(_{g}^{(L)})^{}_{n_{g}}, g=1,2,,N.\] (13)

Then for any two graphs \(_{g},_{g^{}}\), we have

\[_{gg^{}}^{(L)}:=(_{g}^{(L)})^{}_{g^{ }}^{(L)}=_{n_{g}}^{}_{g}^{(L)}(_{g^{ }}^{(L)})^{}_{n_{g^{}}}=_{n_{g}}^{} _{gg^{}}^{(L)}_{n_{g^{}}},\] (14)

where \(_{gg^{}}^{(L)}^{n_{g} n_{g^{}}}\) can be computed using (5) recursively. Consequently, we can obtain a graph-level kernel matrix as

\[}_{}^{(L)}=[_{gg^{}}^{ (L)}]_{(g,g^{})[N][N]}=_{n_{1}}^ {}&&\\ &&\\ &&_{n_{N}}^{} _{11}^{(L)}&&_{1N}^{(L)}\\ &&\\ _{N1}^{(L)}&&_{NN}^{(L)} _{n_{1}}&&\\ &&\\ &&_{n_{N}}\] (15)

Using (14) to compute \(}_{}^{(L)}\) has a high time complexity of \(N(N-1)/2\) and using (15) has a high space complexity of \(O((_{g=1}^{N}n_{g})^{2})\). To reduce the cost, we can perform (15) on mini-batchs of \(\). We have the following result (proved in Appendix D)

**Proposition 1**.: _Suppose \(k^{(L)}\) is a Gaussian kernel with \(0<_{L}<\). Then \(}_{}^{(L)}\) is positive definite. \(}_{}^{(L)}\) can be applied to various graph-level learning tasks such as graph classification and clustering._

### Fast and Explicit Feature Transformation for GCKM

GCKMs can be stacked as multi-layer model and applied to a wide range of tasks by calculating the kernel matrix, and has the advantages of being fast, explainable, and requiring few parameters. Despite this, the time and space complexities of GCKM are \(O(n^{2})\), which limits the application to very large graph datasets. To tackle this issue, we consider approximating the kernel by

\[k(,^{})=(),(^{ })(),(^{}),\] (16)

where \(()\) is a low-dimensional map using Random Fourier Feature . The following theorem provides the key foundation of this approximation method:

**Theorem 3** (Bochner's theorem ).: _A continuous kernel \(k(,^{})=k(-^{})\) on \(^{m}\) is positive definite if and only if \(k()\) is the Fourier transform of a non-negative measure._

Specifically, for Gaussian kernel, let \(p_{}()\) be a probability distribution and also the Fourier transforamtion of \(k_{}()\). We first randomly sample \(\{_{1}^{(l)},_{2}^{(l)},, {}_{D}^{(l)}\}\) from the probability distribution \(p_{}()=0, \). Subsequently the multi-layer GCKM with explicit feature transformation, called GCKM-E, on the \(i\)-th sample is defined as \(_{i}^{(l+1)}=_{(l)}}_{i}^{(l+1)}\) where

\[_{(l)}}_{i}^{(l+1)}=}[ (_{1}^{}}_{i}^{(l+1)}),,( _{D}^{}}_{i}^{(l+1)}),( {}_{1}^{}}_{i}^{(l+1)}),,(_{D}^{}}_{i}^{(l+1)})]^{}\] (17)

and \(_{i}^{(l+1)}\) and \(}_{i}^{(l+1)}\) denote \(i\)-th columns vectors drawn from the transposes of \(^{(l+1)}\) and \(}^{(l+1)}=}^{}^{(l)}\), respectively. For detailed derivations, please refer to Appendix E. This approach allows GCKM to flexibly choose the dimensions of explicit node representations when \(n\) is very large.

## 4 Experiment

In this section, we evaluate GCKMs on several real-world graph datasets. Comparison experiments of both semi-supervised node classification and node clustering show GCKMs' accuracy and efficiency. Due to the space limitation, some important experimental results are deferred to Appendix F.2 (more types of datasets and large-scale dataset), Appendix F.3 (over-smoothing problem), Appendix F.4 (GCKM with various kernels) and Appendix F.5 (GCKPCA visualization).

**Datasets** We employ three most widely adopted citation networks Cora, Citeseer, and Pubmed for evaluations, they are formed as unweighted and undirected graphs where each node represents a paper and edges denote citations between papers. As for graph classification, IMDB-BINARY and IMDB-MULTI are movie collaboration datasets; COLLAB is a scientific collaboration dataset; MUTAG, PROTEINS, and PTC are three bioinformatics datasets. Details about these datasets and more datasets (e.g., social networks, paper networks and OGB-Arxiv) can be found in Appendix F.1.

**Compared Methods** For node classification, two types of GNNs are selected: Chebyshev [Defferrard _et al._, 2016], GraphSAGE [Hamilton _et al._, 2017], GAT [Velickovic _et al._, 2018], GCN [Kipf and Welling, 2017] and SGC [Wu _et al._, 2019] serve as classical baseline GNNs; APPNP [Klicpera _et al._, 2019], JKNet [Xu _et al._, 2018], DAGNN [Liu _et al._, 2020], AdaGCN [Sun _et al._, 2021], AMGCN[Wang _et al._, 2020], DefGCN [Park _et al._, 2022] are state-of-the-art models, in particular the former four ones are Deep GCNs. For node clustering, K-means and Spectral Clustering are two traditional baseline methods and GAE and VGAE [Kipf and Welling, 2016] are GCN-based baseline methods; ARGA, ARVGA [Pan _et al._, 2018], DGI [Velickovic _et al._, 2019], MVGRL [Hassani and Ahmadi, 2020], GALA [Park _et al._, 2019], DFCN [Tu _et al._, 2021] and S\({}^{3}\)GC [Devvrit _et al._, 2022] are all state-of-the-art GNN-based approaches. For graph classification, WL subtree kernel [Shervashidze _et al._, 2011], AWL [Ivanov and Burnaev, 2018] are two baselines; DCNN [Atwood and Towsley, 2016], PATCHY-SAN [Niepert _et al._, 2016] and DGCNN [Zhang _et al._, 2018] are three deep learning methods; GCN, GraphSAGE and GIN [Xu _et al._, 2019] are GNN-based methods.

**Experimental Settings** All the above methods are set as default following the original paper. We use a 2-layer GCKM for the experiments and the kernel is specified as Gaussian kernel, more detailed settings of GCKM can be found in Appendix F.1. In node classification, following vanilla GCN [Kipf and Welling, 2017], the nodes are split into three set: train set containing \(20\) samples per class, validation set and test set with \(500\) and \(1,000\) samples respectively, and the standard fixed split is same to [Yang _et al._, 2016]. GCKM paired with kernel-based Spectral Clustering is leveraged for node clustering, which is described in Section 3.3. For graph classification task, we adopt the 10-fold cross validation following the settings of GIN Xu _et al._.

**Semi-supervised Node Classification** The results are reported in Table 1, where GCKSVM-E denotes GCKSVM with explicit features. Note that, we tune the hyperparameters of GCKSVM on the validation set and report the best performance. We have the following observations. **1)** In the case of standard split, GCKSVM achieves decent performance and beats several GNNs including deep GCNs, even though it is a shallow model. Besides, owing to that GCKSVM can obtain globally optimal solutions, the standard deviations are zero. **2)** In the case of random split, GCKSVM outperforms all GNNs on the three datasets and has the smallest standard deviations, showing higher stability. **3)** Deep GCNs are not substantially better than, or even worse than baselines, which further validates that deepening is not essential in common graph-based tasks. **4)** GCKSVM-E performs comparably to GCKSVM and other SOTA methods.

**Decision Boundary** To further explore the expressiveness, we illustrate the decision boundaries of SGC, APPNP, GCN, and the proposed GCKSVM in Figure 3. We generate a synthetic two-circle

    &  &  \\   & Cora & Citeseer & Pubmed & Cora & Citeseer & Pubmed \\  Chebyshev & 75.0 (0.7) & 63.8 (0.6) & 74.7 (0.8) & 76.1 (1.5) & 63.4 (3.8) & 75.4 (2.8) \\ GraphSAGE & 76.9 (0.5) & 63.3 (0.7) & 74.8 (0.2) & 76.3 (0.9) & 63.2 (1.1) & 73.9 (1.8) \\ GAT & 82.0 (0.5) & 69.8 (0.4) & 77.5 (0.2) & 79.2 (1.4) & 65.8 (2.9) & 78.1 (1.7) \\ GCN & 81.7 (0.7) & 70.5 (0.5) & 78.4 (0.4) & 80.6 (1.4) & 69.1 (1.8) & 77.4 (2.1) \\ SGC & 81.3 (0.0) & 68.4 (0.1) & 78.6 (0.1) & 79.6 (1.3) & 68.0 (1.7) & 76.6 (2.4) \\  APPNP & 79.5 (0.6) & 70.0 (0.8) & 78.8 (1.3) & 80.1 (1.8) & **69.2 (2.1)** & 77.7 (2.0) \\ JKNet & 77.9 (0.8) & 72.3 (0.1) & **80.1 (0.2)** & 78.0 (1.7) & 66.9 (2.3) & 77.0 (1.7) \\ DAGNN & **82.6 (1.1)** & 71.0 (0.5) & **80.0 (1.0)** & **81.8 (1.2)** & 68.4 (1.4) & 78.8 (1.4) \\ AdaGCN & 77.1 (0.1) & 69.4 (0.2) & 78.0 (0.1) & 76.7 (1.8) & 67.0 (1.2) & 77.3 (1.3) \\ AMGCN & 81.3 (0.4) & 70.4 (0.2) & 75.5 (0.9) & 80.9 (1.2) & 68.1 (1.4) & 74.2 (2.1) \\ DefGCN & 77.8 (1.0) & 67.5 (1.7) & 77.9 (0.6) & 78.4 (2.3) & 67.8 (2.6) & 77.1 (1.7) \\  GCKSVM-E & 82.4 (0.3) & **72.4 (0.4)** & 79.1 (0.4) & 80.8 (0.8) & 68.5 (1.3) & **79.2 (1.2)** \\ GCKSVM & **82.4 (0.0)** & **72.3 (0.0)** & 79.8 (0.0) & **83.3 (0.8)** & **71.9 (1.0)** & **80.9 (0.5)** \\   

Table 1: Accuracy (mean\(\%\) and standard deviation\(\%\)) of all methods, note that the best results are highlighted in **orange** and the second-best results are highlighted in **blue**.

dataset named Circle. We also consider two real datasets, denoted by Cora-2D and Chameleon-2D, where we select two classes of samples from the original datasets and map them to 2-D space using tSNE (Van der Maaten and Hinton, 2008). We randomly labeled 200 samples for each dataset and constructed adjacency matrices by \(k\)-nearest neighbor algorithm, where the original graph structures are not used here. The details are described in Appendix F.1. Cora-2D is the simplest case where the decision boundary is easy to construct, Circle is with a clear but non-linear decision boundary, and Chameleon-2D is the most complex one.

We have the following conclusions. **1)** Consistent with our previous discussion, the nonlinearity provided by SGC is low even if we increase the power of \(}\) to a large value such as 50. Therefore, SGC performs poorly on Circle and Chameleon-2D, where its decision boundaries are approximately linear. This is unexpected considering its performance in the previous experiment. An explanation is that these commonly used graph datasets can be well classified linearly in high-dimension spaces, rather than these 2D spaces. **2)** The deep model APPNP fits these datasets with similar boundaries as GCN, either simple or complex, revealing that existing deep GCNs may not provide more powerful expressiveness. **3)** GCKSVM gains advantages over the three other models, whose decision boundaries fit these datasets well, attributed to which GCKM explicitly maps the samples to high-dimensional space to achieve better separability.

**Graph Classification** The results of graph classification tasks are recorded in Table 2. Compared to both classical baselines and GNNs, especially GIN designed for Graph-level tasks, GCKSVM shows

    & IMDB-B & IMDB-M & COLLAB & MUTAG & PROTEINS & PTC \\  WL subtree & 73.8 (3.9) & 50.9 (3.8) & 78.9 (1.9) & **90.4 (5.7)** & 75.0 (3.1) & 59.9 (4.3) \\ DCNN & 49.1 & 33.5 & 52.1 & 67 & 61.3 & 56.6 \\ PATCHYSAN & 71.0 (2.2) & 45.2 (2.8) & 72.6 (2.2) & **92.6 (4.2)** & 75.9 (2.8) & 60.0 (4.8) \\ DGCNN & 70 & 47.8 & 73.7 & 85.8 & 75.5 & 58.6 \\ AWL & 74.5 (5.9) & 51.5 (3.6) & 73.9 (1.9) & 87.9 (9.8) & — & — \\ MLP & 73.7 (3.7) & 52.3 (3.1) & 79.2 (2.3) & 84.0 (6.1) & 76.0 (3.2) & **66.6 (6.9)** \\ GIN & **75.1 (5.1)** & **52.3 (2.8)** & **80.2 (1.9)** & 89.4 (5.6) & **76.2 (2.8)** & 64.6 (7.0) \\ GCN & 74.0 (3.4) & 51.9 (3.8) & 79.0 (1.8) & 85.6 (5.8) & **76.0 (3.2)** & 64.2 (4.3) \\ GraphSAGE & 72.3 (5.3) & 50.9 (2.2) & — & — & 75.9 (3.2) & 63.9 (7.7) \\  GCKSVM & **75.4 (2.4)** & **53.9 (2.8)** & **81.7 (1.5)** & 88.7 (7.6) & 74.5 (3.9) & **67.7 (5.4)** \\   

Table 2: Graph classification accuracy (mean\(\%\) and standard deviation\(\%\)) of all methods, note that the best results are highlighted in \(}\) and the second-best results are highlighted in \(}\).

Figure 3: Decision boundary visualizations of SGC, APPNP, GCN, and GCKM on Circle (row 1), Chameleon-2D (row \(2\)) and Cora-2D (row \(3\)), where Circle is a synthetic dataset and Chameleon-2D and Cora-2D are two preprocessed real-world datasets.

competitive power in graph classification. It is worth noting that GCKSVM outperforms the GNNs in the two relatively large datasets IMDB-M and COLLAB.

**Node Clustering** The comparison of all node clustering methods is shown in Table 3.

Similar to the results of node classification, GCKM is well extended to node clustering tasks and is competitive with the deep-learning-based models. It is impressive that the vanilla GCN variants for clustering, GAE and VGAE both perform much worse than our GCKSC, demonstrating that GCKM can yield a discriminative kernel matrix. This can also be observed in Figure 4, where the powered adjacency matrix, the kernel matrix calculated on node features, and the kernel matrix of GCKM are visualized.

**Time Costs** Figure 5 shows the runtime of all methods on Pubmed. GCKSVM is much more efficient than all other methods except SGC.

## 5 Conclusions

After analyzing the limitations of GCN, SGC and deep GCNs, we proposed a framework of GCKM. GCKM integrated graph convolution with kernel learning and exhibited higher efficiency, interpretability, and stability. We extended GCKM to many graph-based machine-learning tasks, like node-level and graph-level classification and clustering. Experiments showed that GCKMs are, at least, as accurate as GCN and other SOTA GNNs. More importantly, compared to the representative simplified model SGC, GCKM is much more effective in handling non-linear data. One possible limitation of this work is that we did not systematically test other kernel functions, though we have found that the Gaussian kernel is better than the polynomial for GCKM.

    &  &  &  \\   & ACC & NMI & ARI & ACC & NMI & ARI & ACC & NMI & ARI \\  K-means & 49.20 & 32.10 & 22.90 & 54.00 & 30.50 & 27.80 & 59.50 & 31.50 & 28.10 \\ SC & 36.70 & 12.60 & 3.10 & 23.80 & 5.50 & 1.00 & 52.80 & 9.70 & 6.20 \\ GAE & 59.60 & 42.90 & 34.70 & 40.80 & 17.60 & 12.40 & 67.20 & 27.70 & 27.90 \\ VGAE & 50.20 & 32.90 & 25.40 & 46.70 & 26.00 & 20.50 & 63.00 & 22.90 & 21.30 \\ ARGA & 64.00 & 44.90 & 35.20 & 35.20 & 35.00 & 34.10 & 66.80 & 30.50 & 29.50 \\ ARVGA & 64.00 & 45.00 & 37.40 & 54.40 & 26.10 & 24.50 & 69.00 & 29.00 & 30.60 \\ DGI & 55.40 & 41.10 & 32.70 & 51.40 & 31.50 & 32.60 & 58.90 & 27.70 & 31.50 \\ MVGRL & 73.20 & 56.20 & 51.90 & 68.10 & 43.20 & 43.40 & 69.30 & **34.40** & 32.30 \\ GALA & **74.59** & **57.67** & **53.15** & 69.32 & **44.11** & 44.60 & 69.39 & 32.73 & 32.14 \\ DFCN & 64.07 & 48.24 & 39.17 & 69.50 & 43.90 & **45.50** & 69.32 & 32.19 & 31.55 \\ S\({}^{3}\)GC & 74.20 & **58.80** & **54.40** & 68.80 & **44.10** & 44.80 & **71.30** & **33.30** & **34.50** \\  GCKSC & **74.30** & 54.95 & 52.89 & **71.27** & 43.57 & **46.51** & **71.31** & 32.24 & **34.21** \\   

Table 3: ACC, NMI, and ARI of all methods, note that the best results are highlighted in **orange** and the second-best results are highlighted in **blue**.

Figure 4: Visualizations of the adjacency matrix \(}^{q}\), kernel matrix \([k(_{i},_{j})]_{i,j=1}^{n}\), and GCKM matrix \(^{(2)}\) on Cora, with corresponding best-tuned clustering accuracy.

Figure 5: Runtimes of all methods.