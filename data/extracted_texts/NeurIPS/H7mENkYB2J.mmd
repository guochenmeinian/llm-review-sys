# Ultrafast classical phylogenetic method beats large protein language models on variant effect prediction

Ultrafast classical phylogenetic method beats large protein language models on variant effect prediction

Sebastian Prillo

University of California, Berkeley

sprillo@berkeley.edu

Equal contribution; authors listed alphabetically

Wilson Wu

University of California, Berkeley

nosliw@berkeley.edu

Equal contribution; authors listed alphabetically

Yun S. Song

University of California, Berkeley

yss@berkeley.edu

###### Abstract

Amino acid substitution rate matrices are fundamental to statistical phylogenetics and evolutionary biology. Estimating them typically requires reconstructed trees for massive amounts of aligned proteins, which poses a major computational bottleneck. In this paper, we develop a near-linear time method to estimate these rate matrices from multiple sequence alignments (MSAs) alone, thereby speeding up computation by orders of magnitude. Our method relies on a near-linear time cherry reconstruction algorithm which we call _FastCherherries_ and it can be easily applied to MSAs with millions of sequences. On both simulated and real data, we demonstrate the speed and accuracy of our method as applied to the classical model of protein evolution. By leveraging the unprecedented scalability of our method, we develop a new, rich phylogenetic model called _SiteRM_, which can estimate a general _site-specific_ rate matrix for each column of an MSA. Remarkably, in variant effect prediction for both clinical and deep mutational scanning data in ProteinGym, we show that despite being an independent-sites model, our SiteRM model outperforms large protein language models that learn complex residue-residue interactions between different sites. We attribute our increased performance to conceptual advances in our probabilistic treatment of evolutionary data and our ability to handle extremely large MSAs. We anticipate that our work will have a lasting impact across both statistical phylogenetics and computational variant effect prediction. FastCherries and SiteRM are implemented in the CherryML package [https://github.com/songlab-cal/CherryML](https://github.com/songlab-cal/CherryML).

## 1 Introduction

Amino acid substitution rate matrices are crucial to statistical phylogenetics and evolutionary biology. Perhaps best known for their use in phylogenetic tree reconstruction , these transition-rate matrices allow us to infer the evolutionary history of a protein family. This has given insights into protein evolution, function, and fitness, which among other important applications, has informed the development of vaccines during the COVID-19 pandemic . Statistically speaking, transition-rate matrices are _parameters_ of continuous-time Markov chain models describing protein evolution. One of the most popular such models to date is the seminal model of Le and Gascuel  (LG for short), which posits that each site of a protein evolves independently following a continuous-time Markov chain parameterized by a global transition-rate matrix \(Q^{20 20}\) up to site-specific rates.

Unfortunately, estimating transition-rate matrices under the LG and related classical models from a set of multiple sequence alignments (MSAs) is a challenging task. Indeed, one has to first estimate phylogenetic trees and site-specific rates for each MSA, and then estimate the transition-rate matrix (or matrices) \(Q\) which best explain the data. The procedures of tree plus site-rate estimation and of transition-rate matrix estimation are usually performed in a coordinate-ascent fashion until convergence. Both of these steps are computationally challenging: maximum likelihood estimation (MLE) of trees is in general NP-hard , while MLE of a rate matrix \(Q\) given MSAs, trees and site-rates has historically proven a computationally demanding task [9; 10].

Fortunately, recent work has made it possible to perform MLE of transition-rate matrices given MSAs, trees and site-rates in a scalable fashion. The method, called CherryML , replaces the full joint likelihood of the MSA data with a _composite_ likelihood over cherries in the trees. When applied to the LG model, CherryML is orders of magnitude faster than traditional MLE with the Expectation-Maximization (EM), all while sacrificing a relatively small amount of statistical efficiency, estimated to be around \(50\%\) on simulation studies. The method was also shown to perform on-par with EM on real datasets when evaluated on held-out likelihood. While the work of CherryML represents a significant advance, the method still requires expensive phylogenetic tree reconstruction as a prerequisite step. This hinders the scalability of the method when applied to many MSAs, some with millions of sequences. In fact, in the original CherryML work, MSAs were subsampled down to a thousand sequences each to alleviate the burden of tree reconstruction.

In this work, we introduce a new methodology to significantly speed up the end-to-end estimation of rate matrices under the LG model, and extend this methodology to estimate site-specific rate matrices. Specifically, we speed up the phylogenetic tree plus site-rate estimation step required by the CherryML method, using a method which we call _FastCherries_. The resulting end-to-end method consisting of CherryML with FastCherries can, given a set of starting MSAs, estimate site-specific rates and an accurate transition-rate matrix in _nearly linear_ time in the input dataset size. The method is thus essentially computationally optimal up to logarithmic factors and other constants. On a real MSA with more than 450,000 sequences of length 364, FastCherries took just 1,000 seconds on 1 CPU core, and we can estimate site-specific rate matrices at an additional cost of _one second_ per site.

Thanks to its unprecedented scalability, we applied CherryML with FastCherries to estimate _site-specific transition-rate matrices_ for MSAs with hundreds of thousands of sequences. This model, which we call _SiteRM_ for short, posits that each site \(i\) of a protein family \(f\) evolves independently according to a family- and site-specific rate matrix \(Q_{i}^{f}\). Strikingly, we show that this _independent-sites_ model excels at variant effect prediction, outperforming the seminal EVmutation  model _with epistatic interactions_ as well as many large protein language models such as an ESM-1v ensemble . We attribute the increased performance of SiteRM to the principled probabilistic treatment of the evolutionary process, which is absent in competing approaches. This opens up a new avenue to variant effect prediction via the use of probabilistic models of protein evolution in time. FastCherries and SiteRM are available at [https://github.com/songlab-cal/CherryML](https://github.com/songlab-cal/CherryML).

## 2 Background and Related Work

### Classical models of protein evolution

As time passes, proteins experience mutations, many of which are neutral, having no effect on fitness. On the other hand, some mutations are deleterious, leading to decreased fitness and hence negative selection, while other mutations confer increased fitness and thus are favored by natural selection. This complex evolutionary process of mutation and selection leads to the vast biological diversity observed today, and modeling it is the goal of statistical phylogenetics. Formally, a model of protein evolution is a conditional distribution \(p(y|x,t,f)\) describing the probability of sequence \(y\), the evolved from sequence \(x\) after time \(t\), in protein family \(f\).

Standard probabilistic models of protein evolution.Early work on protein evolution relied on counting-based heuristics, such as the seminal works of Dayhoff et al.  and JTT . The first probabilistic model of protein evolution was proposed by Whelan and Goldman . It posits that each site of any protein evolves i.i.d. following a time-reversible continuous-time Markov chain parameterized by a global transition-rate matrix \(Q^{20 20}\), normalized so that the expected number of substitutions in a unit of time equals 1. Each protein family \(f\) may evolve at a different rate \(_{f}\). Letting \(l_{f}\) be the number of columns of an MSA for protein family \(f\), we have \(_{i=1}^{l_{f}}(_{f}tQ)[x_{i},y_{i}],\) where \((M)[a,b]\) denotes the \((a,b)\) entry of the matrix exponential \((M)\). The seminal extension of the above model by Le and Gascuel  still uses a global, normalized transition-rate matrix \(Q\), but incorporates site-specific rates \((_{1}^{f},_{2}^{f},,_{l_{f}}^{f})\), giving

\[p_{}(y|x,t,f)=_{i=1}^{l_{f}}(_{i}^{f}tQ)[x_{i},y_{i}].\]

This is one of the most popular models of protein evolution to date, used extensively in phylogenetic tree reconstruction. The LG model has a total of \(400+_{f=1}^{m}l_{f}\) parameters, where \(m\) denotes the number of protein families under study. The site-specific rates \(_{i}^{f}\) are typically constrained with a model of site-rate variation, such as the \(\) model  or the probability-distribution-free model.

**Phylogenetic models.** Unfortunately, estimating models of protein evolution is a challenging task. Indeed, one does not have access to training data of the form \((x,y,t,f)\). Instead, one has access to MSAs. Formally, let \(D=(D_{1},,D_{m})\) be MSAs for \(m\) protein families. The data \(D\) are modeled with a _phylogenetic model_, which is parameterized by (1) a model of protein evolution \(p(y|x,t,f)\) (as described above), (2) phylogenetic trees \(_{1},,_{m}\) (one tree per MSA), and (3) a root state distribution \(_{}(x)\). The phylogenetic model posits that the MSA data were generated by sampling the root state from each tree following \(_{}(x)\) and then running the evolutionary model \(p(y|x,t,f)\) down each tree. We denote the phylogenetic model's likelihood as \(p_{}(D|)\). To estimate a model of protein evolution \(p(y|x,t,f)\) with MLE from data from a phylogenetic model \(p_{}(D|)\), one thus has to deal with the nuisance parameters \(=(_{1},_{2},,_{m})\). Typically, this is done with coordinate ascent, wherein one alternately optimizes the trees \(\) plus site-rates \(_{i}^{f}\) given the transition-rate matrix \(Q\) and then optimizes the transition-rate matrix \(Q\) given the tree estimates \(\) plus site-rates. Both of these steps have historically proven to be computationally demanding.

### CherryML

Recently, the work of CherryML  introduced a scalable and accurate method for estimating the transition-rate matrix (or matrices) \(Q\) given the trees \(\) and site-rates \(_{i}^{f}\). It is assumed, as is typical in statistical phylogenetics, that \(Q\) is time-reversible. For the LG model, this method was shown to be orders of magnitude faster than the traditional EM method. In fact, CherryML's runtime for the LG model is linear in the input data size up to logarithmic factors, making it essentially computationally optimal. From a statistical point of view, CherryML is just \( 50\%\) less efficient than full MLE.

**Composite likelihood.** The key to CherryML lies in the use of a composite likelihood, which generally leads to consistent parameter estimates under weak assumptions . Adapting the notation of CherryML , for a generic phylogenetic model \(p_{}\), the full joint likelihood of the data is \(= p_{}(D|)=_{f=1}^{m} p_{} (D_{f}|_{f})\). CherryML replaces the MSA's log-likelihood \( p_{}(D_{f}|_{f})\) by a composite likelihood over cherries in the trees, where cherries are iteratively picked until either 0 or 1 leaf remains. Specifically, letting \(\{(u_{j}^{f},v_{j}^{f})\}_{1 j c_{f}}\) be the \(c_{f}\) cherries in tree \(_{f}\), CherryML considers the composite likelihood \(_{}=_{f=1}^{m}_{j=1}^{c_{f}} p_{}(D_{f }[u_{j}^{f}] D_{f}[v_{j}^{f}],_{f}),\) where \(D_{f}[u_{j}^{f}]\) denotes the sequence in MSA \(D_{f}\) corresponding to leaf \(u_{j}^{f}\). For a stationary time-reversible model, the term \(p_{}(D_{f}[u_{j}^{f}]|D_{f}[v_{j}^{f}],_{f})\) is exactly equal to \(p(D_{f}[u_{j}^{f}]|D_{f}[v_{j}^{f}],t_{j}^{f})\) where \(t_{j}^{f}\) is the distance between \(u_{j}^{f}\) and \(v_{j}^{f}\) in tree \(_{f}\). Therefore, the composite likelihood reduces to \(_{}=_{f=1}^{m}_{j=1}^{c_{f}} p(D_{f}[u_{j}^{f}]|D_{ f}[v_{j}^{f}],t_{j}^{f},f)\). This way, CherryML reduces the problem of learning the transition-rate matrix (or matrices) \(Q\) given the trees \(\), site-rates, and MSAs \(D\) to a supervised learning problem over the data \((x,y,t,f)\) given by \((D_{f}[u_{j}^{f}],D_{f}[v_{j}^{f}],t_{j}^{f},f)\). Cherries are considered in both directions when forming the composite likelihood, so that if \((x,y,t,f)\) is a training datapoint, then so is \((y,x,t,f)\). Thus, CherryML can be viewed as a modern, principled version of the JTT method  where sequences are paired and then rate matrix estimation proceeds by MLE rather than via counting heuristics.

**Time quantization.** CherryML further simplifies the composite likelihood by quantizing (or discretizing) time into a finite number \(b\) of values \(_{1}<_{2}<<_{b}\). Letting \(q(t)\) be the quantized value of \(t\) (which is chosen to minimize relative error), in the LG model with site-rates \(r_{k}^{f}\) for site \(1 k l_{f}\) of family \(f\), CherryML leads to the quantized composite likelihood \(_{}=_{f=1}^{m}_{j=1}^{c_{f}}_{i=1}^{l_{f}} ((q(r_{i}^{f}t_{j}^{f})Q)[D_{f}[u_{j}^{f}]_{i},D_{f}[v_{j}^{f}]_{i}])\). The terms in this expression can be grouped together by quantization time \(_{k}\), leading to

\[_{}=_{k=1}^{b} C_{k},(_{k}Q) ,\]

where \(C_{k}\) is a \(20 20\) count matrix for the number of transitions between two amino acids at a quantized distance of \(_{k}\), and \(,\) denotes the matrix inner product. This function can be evaluated in time \((bs^{3})\) where \(s=20\) is the number of states, which is remarkable since this no longer depends on the input dataset size. It can be optimized using automatic differentiation software such as PyTorch  and Tensorflow . CherryML leverages a differentiable implementation of the matrix exponential operator based on robust Taylor expansions  and utilizes the Adam optimizer .

## 3 Method

### The SiteRM model

In this work, we propose a richer model of protein evolution, which we call the _SiteRM_ model:

\[p_{}(y|x,t,f)=_{i=1}^{l_{f}}(tQ_{i}^{f})[x_{i},y_{i}],\]

where \(Q_{1}^{f},,Q_{l_{f}}^{f}\) are site-specific transition-rate matrices. Unlike \(Q\)-matrix described above, \(Q_{i}^{f}\) are _not_ normalized and they subsume site-specific rates. This model has \(400_{f=1}^{m}l_{f}\) parameters and strictly generalizes the LG model. This is a particular case of the 'partition model' available in IQTree , in which each site forms its own group in the partition.

### Near-linear time end-to-end estimation of model parameters

Here, we present an algorithm for speeding up end-to-end estimation of model parameters. Related work on speeding up the tree reconstruction step required for rate matrix estimation is described in Appendix A.1. Although we focus on the LG model  for the sake of analyzing statistical efficiency, our method will also enable fitting site-specific rate matrices under the richly parameterized SiteRM model, as we illustrate on the task of variant effect prediction. For the LG model, our work speeds up tree and site-rate estimation given the rate matrix \(Q\) and MSAs \(D\). Together with CherryML's procedure for estimating a transition-rate matrix \(Q\) given trees, site-rates, and MSAs, this leads to a full coordinate-ascent procedure with near-linear time updates, making it exceptionally scalable.

**Method overview.** The key idea of our method is that CherryML's composite likelihood depends on the trees \(\) only through the cherries \((u_{j}^{f},v_{j}^{f})\) and their quantized distances \(t_{j}^{f}\). In other words, there is no point in estimating the full trees \(\) if only their cherries will be used. Our method thus proceeds by first grouping the sequences within each MSA \(D_{f}\) into disjoint pairs using a near-linear time divide-and-conquer approach which tries to minimize total Hamming distance across all pairs. Letting \(n_{f}\) be the number of sequences in MSA \(D_{f}\) (so that MSA \(D_{f}\) has size \(n_{f} l_{f}\)), we obtain \(c_{f}= n_{f}/2\) pairs \((u_{1}^{f},v_{1}^{f}),,(u_{c_{f}}^{f},v_{c_{f}}^{f})\). These pairs are the putative cherries of the tree \(_{f}\). Next, the site-specific rates \(_{1}^{f},,_{l_{f}}^{f}\) and the quantized times \(t_{1}^{f},,t_{c_{f}}^{f}\) separating each pair of sequences are estimated using coordinate ascent. We call our method _FastCherries_. A schematic of the FastCherries/SiteRM method is provided in Figure 1.

**Divide-and-conquer.** To pair up the set of sequences \(S\), we use an almost-linear time distance-based tree topology reconstruction algorithm based on divide-and-conquer. The general operation of the algorithm is as follows. Let \(d\) be a dissimilarity function between protein sequences. We use the normalized Hamming distance \(d\) ignoring gaps, but other dissimilarities may be used. Assuming \(|S| 3\), we first try to find the diameter of the set \(S\) (i.e., the two furthest away sequences) as follows: (1) take a random sequence \(z_{0} S\), (2) find the furthest sequence to \(z_{0}\) in \(S\): \(z_{1}=*{arg\,max}_{z S}d(z,z_{0})\); (3) find the furthest sequences to \(z_{1}\) in \(S\): \(z_{2}=*{arg\,max}_{z S}d(z,z_{1})\). Our putative diametrically opposite sequences are given by \(z_{1}\) and \(z_{2}\). Note that this is analogous to the well-known linear-time algorithm used to find the diameter of a tree. We now use \(z_{1}\) and \(z_{2}\) as pivots to split the set \(S\) into two subsets \(S_{1}\) and \(S_{2}\) based on distance to \(z_{1}\) and \(z_{2}\):

\[S_{1}=\{z S:d(z,z_{1}) d(z,z_{2})\} S_{2}=\{z S:d (z,z_{1})>d(z,z_{2})\}.\]We recurse the above procedure on \(S_{1}\) and \(S_{2}\). The base case consists of a set \(S\) with either \(1\) or \(2\) sequences. If \(S\) has \(2\) sequences, we pair them. Otherwise, we return the lone sequence, which will get paired with another lone sequence up in the recursive calls if both \(|S_{1}|\) and \(|S_{2}|\) are odd. This way, all sequences except at most one (when \(|S|\) is odd) will be paired together. Assuming balanced splits of size \(0<<|S_{1}|/|S_{2}|<1/\) for some universal constant \(\), the method has a near-linear runtime of \((l_{f}n_{f} n_{f})\). Further details can be found in Appendix A.2.

**Categorical site-rate variation model.** Having paired the sequences, we proceed to estimate site-rates \(_{i}^{f}\) and quantized distances \(t_{j}^{f}\) between pairs. This is done with coordinate ascent until convergence (no change in site-rates) or a maximum of \(50\) iterations; we find that convergence usually happens after around \(10\) iterations. We adopt the CAT model of site-rate variation used by FastTree , which poses that site-rate \(_{i}^{f}\) takes one of \(r\) (a hyperparameter) values from a geometric grid between \(1/r\) and \(r\), i.e., \(R=\{r^{-1+2}:i=1,,r\}\), with a \((3,1/3)\) prior. We use this model of site-rate variation as it is the one used by FastTree, which is one of the fastest phylogenetic tree reconstruction algorithms available and the one used in the original CherryML work ; however, our method can be adapted to other models of site-rate variation.

**Site-rate estimation.** To estimate site-rates \(_{i}^{f}\) given quantized divergence time \(t_{j}^{f}\) for each cherry \(j\) and the transition-rate matrix \(Q\), we perform 0-th order optimization on CherryML's composite likelihood. Each site-rate \(_{i}^{f}\) can be optimized independently. To optimize site-rate \(_{i}^{f}\), we simply need to find which of the \(r\) rates in \(R\) maximizes that site's composite likelihood under the Gamma

Figure 1: **A schematic illustration of our FastCherries/SiteRM method.** Rate matrix estimation from a set of MSAs classically proceeds in two steps: tree estimation, followed by rate matrix estimation. The recently proposed CherryML method  significantly speeds up the rate matrix estimation step. Since CherryML only requires the cherries in the trees, we propose FastCherries, a new near-linear method that estimates only the cherries in the tree (as well as the site rates) rather than the whole tree. FastCherries proceeds in two steps: a divide-and-conquer pairing step based on Hamming distance, followed by site rate and branch length estimation. Site rate and branch length estimation alternate until convergence. CherryML’s speed allows estimating not only a single global rate matrix, but also _one rate matrix per site_, which we call the SiteRM model. In this schematic, computational complexities for each step of FastCherries is annotated at each step; \(n=\) number of sequences in the MSA, \(l=\) number of sites in the MSA, \(s=\) number of states (e.g., 20 for amino acids), \(r=\) number of site rate categories of the LG model (e.g., 4 or 20 is typical), \(b=\) number of quantization points used to quantize time by CherryML. Precomputation of the matrix exponentials — which is shared across all MSAs — is excluded from the schematic and costs \((rbs^{3})\). MSA illustrations adapted from .

prior. In other words, letting \((x)\) be the density of the \((3,1/3)\) distribution, we optimize for:

\[_{i}^{f}=*{arg\,max}_{x R}(x)+_{j=1}^{c_{f}} (e^{xt_{j}^{f}Q}[D_{f}[u_{j}^{f}]_{i},D_{f}[v_{j}^{f}]_{i}])+(e^{xt_{j} ^{f}Q}[D_{f}[v_{j}^{f}]_{i},D_{f}[u_{j}^{f}]_{i}]).\]

By precomputing all the matrix exponentials \(\{(x_{k}Q):x R,1 k b\}\) in time \((rbs^{3})\), the above objective can be evaluated in time \((c_{f})=(n_{f})\) and thus optimized by brute-force search in time \((rn_{f})\). Since there are a total of \(l_{f}\) sites, optimizing the site-rates this way takes time \((l_{f}rn_{f})\). This is equal to the size of the MSA \(D_{f}\), multiplied by the number of rate categories \(r\). We initialize the site-rates \(_{i}^{f}\) using a simple heuristic described in Appendix A.3.

**Branch length estimation.** To estimate the quantized divergence time \(t_{j}^{f}\) of each pair given the site-rates \(_{i}^{f}\) and transition-rate matrix \(Q\), we again perform 0-th order optimization on CherryML's composite likelihood. This means we optimize for

\[t_{j}^{f}=*{arg\,max}_{x\{_{k}:1 k b\}}_{i=1}^{ l_{f}}(e^{_{i}^{f}xQ}[D_{f}[u_{j}^{f}]_{i},D_{f}[v_{j}^{f}]_{i}])+(e^{ _{i}^{f}xQ}[D_{f}[v_{j}^{f}]_{i},D_{f}[u_{j}^{f}]_{i}]).\]

By using precomputed matrix exponentials, this objective can be evaluated in time \((l_{f})\) and optimized with brute-force search in time \((bl_{f})\). We empirically find that the objective function above is (quasi-)concave in \(x\), so that we can optimize it with binary search in time \(((b)l_{f})\), which provides a significant speedup. (We could also have done this for optimizing site-rates, but since \(r\) is already typically small for the CAT model, we found it an unnecessary optimization.) This way, optimizing all quantized divergence times \(t_{j}^{f}\) within a given family takes \((n_{f}(b)l_{f})\) time. This is equal to the size of the MSA \(D_{f}\), multiplied by \((b)\).

**Runtime.** As detailed in Appendix A.4, the above FastCherries algorithm is nearly linear in the size of the MSA, and applying it to a real MSA with 453,819 sequences of length 364 took only about 1000 seconds on 1 CPU core. Furthermore, combined with CherryML to estimate a transition-rate matrix, this leads to an end-to-end estimation procedure that is nearly linear in the dataset size. Also, FastCherries uses linear space, and thus is memory-efficient (up to a constant), as detailed in Appendix A.5.

### Regularized estimation of site-specific transition-rate matrices under the SiteRM model

We can employ the above algorithm to derive a regularized procedure for estimation under the more richly parameterized SiteRM model. Recall that the SiteRM model uses _family- and site-specific transition-rate matrices_\(Q_{i}^{f}\). To estimate these matrices, we first apply our FastCherries algorithm to estimate chernies, distances, and site-rates under the LG model. Given these, we simply apply CherryML's transition-rate matrix estimation procedure  at each site separately to obtain \(Q_{i}^{f}\).

**Regularization.** Of course, the model is over-parameterized in the sense that there is usually not enough data to estimate the site-specific rate matrices accurately. Indeed, \(Q_{i}^{f}\) has \(400\) parameters and the only information to support its estimation are the \( n_{f}/2\) pairs of amino acids at that site. Even with \(n_{f}\) in the millions, most pairs will not contain mutations at the site since pairs are by definition close sequences, and thus small entries in the rate matrix will be susceptible to large errors. We address this by mixing the empirical counts with pseudocounts from a prior. Specifically, let \(Q_{0}\) be the transition-rate matrix used for regularizing the model - we use the LG transition-rate matrix in applications. We mix the empirical count matrices \(C_{k}\) with pseudocounts from the LG model with transition-rate matrix \(Q_{0}\) and site-rates \(_{i}^{f}\). Mixing is performed with a regularization coefficient \(\). Formally, we take

\[_{k}=(1-)C_{k}+\|C_{k}\|_{1}P_{k},\]

where \(P_{k}\) is the \(20 20\) pseudocount matrix with \(\|P_{k}\|_{1}=1\) given by \(P_{k}[x,y]=_{Q_{0}}[x](_{i}^{f}_{k}Q_{0})[x,y]\), where \(_{Q_{0}}\) is the stationary distribution of \(Q_{0}\). This choice has the desirable property that when \(=1\), we recover the LG model: \(Q_{i}^{f}=_{i}^{f}Q_{0}\) for all \(i,f\). In contrast, when \(=0\), the prior model is ignored completely. We find that \(=0.5\) works well in practice.

The runtime analysis of the above procedure is provided in Appendix A.4.3. Our method can thus be thought of as _fine-tuning_ the LG model to each site of each protein family. This fine-tuning is achieved by mixing empirical counts with pseudocounts from the 'global' LG model.

Results

### Scalable estimation under the LG model

We applied CherryML with FastCherries to the benchmarks from the CherryML paper . These are described in detail in Appendix A.6.

Figure 1(a) and Figure 1(b) respectively illustrate the total runtime and accuracy of the end-to-end method for data simulated under the LG model, using a log-log plot. It is important to note that while in the original CherryML paper the ground truth trees were used and thus only runtime and accuracy of the rate matrix estimation step were assessed, we benchmarked the _end-to-end_ procedure without access to the ground truth trees. In particular, we iterated the process of tree estimation and rate matrix estimation four times as is typical on real data, starting from the uniform rate matrix. Accuracy is measured via the median relative error of all the off-diagonal entries in the estimated rate matrix.

**Simulations results.** As Figure 1(b) shows, CherryML with FastCherries is one to two orders of magnitude faster than CherryML with FastTree . Runtimes of the tree estimation step of CherryML with FastCherries are so fast that total runtime for small dataset sizes is dominated by the PyTorch first-order optimizer (whose runtime \((gbs^{3})\) is independent of input data size). Thus, only for larger dataset sizes does the runtime of CherryML with FastCherries see any noticeable increase. In terms of accuracy, Figure 2 (a) shows that CherryML with FastCherries shows a small asymptotic bias of around \(2\%\) median relative error. Otherwise, the relative statistical efficiency of CherryML with FastCherries compared to CherryML using ground truth trees or FastTree is around \(50\%\), meaning that CherryML with FastCherries requires approximately twice as much data to achieve the same error. This is similar to the relative statistical efficiency of CherryML compared to MLE with EM.

**Real data results.** On real data benchmarks, we observed similar end-to-end speedups of one to two orders of magnitude for CherryML with FastCherries compared to CherryML with FastTree, while obtaining similar likelihoods on held-out data. This is both for the original LG paper's Pfam dataset, shown in Figure 1(c),d, as well as for the QMaker  datasets, shown in Supplementary Figure S1.

### Variant Effect Prediction with the SiteRM model

Here, we summarize the performance of our SiteRM model on the task of variant effect prediction. Prior work on applying phylogenetic models to variant effect prediction is described in Appendix A.1.

**Data.** A notable resource for this purpose is ProteinGym , which features dozens of models benchmarked across both deep mutational scanning (DMS) data and human clinical variants. The DMS substitutions benchmark comprises 2.4M variants across 217 DMS assays, while the human clinical substitutions benchmark contains 63k variants across 2,525 proteins. On the DMS substitutions benchmark, model pathogenicity scores are evaluated against experimental measurements, and evaluation is performed using diverse metrics including Spearman correlation, AUC, MCC, NDCG@10, and top 10 recall. The metrics are aggregated over all 217 assays, in such a way to give equal representation to different kinds of DMS assays, which include protein activity, binding, expression, organismal fitness, and thermostability. The clinical benchmark contains binary pathogenic/benign labels, so that performance is evaluated by averaging AUC across all 2,525 proteins.

**Traditional scoring.** Probabilistic models of proteins such as Potts models  and protein language models (e.g., ESM ) have been used for the task of variant effect prediction. These models provide either a likelihood function \(p(x)\) or, in the case of masked language models, a conditional probability distribution \(p(x_{M}|x_{-M})\) where \(M\) is the set of masked indices. These models can be used for variant effect prediction by scoring via log-likelihood ratios, as follows. Letting \(x^{}\) denote the mutant and \(x^{}\) the wild-type sequence, \(p(x)\) may be used to score mutants via \((x^{},x^{})=})}{p(x ^{})}\). In the case of masked language models, conditional likelihoods \(p(x_{M}|x_{-M})\) may be used to score the substitution of \(x_{M}\) by \(x^{}_{M}\) via \((x^{},x^{})=_{M}|x_{-M} )}{p(x_{M}|x_{-M})}\).

**Our new approach.** We take a conceptually different approach to variant effect prediction by leveraging probabilistic models of protein evolution \(p(y|x,t)\). We propose to score mutants via

\[(x^{},x^{})=}|x^{ },t)}{p(x^{}|x^{},t)},\]where \(t\) is a hyperparameter that controls the size of the evolutionary neighborhood around the wild-type. Our intuition for why this is a good approach is described below.

Consider how the predictions change as \(t\) varies. When \(t=0\), we obtain a Dirac delta at \(x^{}\), so that _all_ mutants get extremely bad scores. As \(t\) starts increasing, proteins which are _evolutionarily close_ to the wild-type will see an increase in probability. These are proteins that our model thinks would have arisen from the process of mutation and natural selection, making our conditional probability an excellent candidate for variant effect prediction. Finally, when \(t=+\), the stationary distribution of the evolutionary model is being used to make predictions. In this case, the wild-type is being _completely_ ignored, assuming we are comparing variants with respect to the same wild-type. For classical phylogenetic models, \(t\) may be interpreted as the expected number of mutations per site. We use \(t=1\) for variant effect prediction, which means we explore a neighborhood of one expected mutation per site. Note that more conserved sites will have smaller site-rates, so they may not mutate at all in 1 time unit, while other less conserved sites may mutate multiple times.

A key insight is that traditional scoring approaches with \(p(x)\) may be thought of as a particular case of our approach with \(t=\). Indeed, given an arbitrary model of \(p(x)\) one may construct a model of protein evolution \(p_{}(y|x,t)\) whose stationary distribution is \(p(x)\) via, for example, Metropolis-Hastings, or a continuous-time, discrete diffusion model. This insight has been observed before, most notably the works of EvoVelocity  and John Ingraham's Ph.D. thesis , which use the energy landscape of protein language models and Potts models respectively to construct an evolutionary model. Unfortunately, likelihoods under this evolutionary model \(p_{}(y|x,t)\) will in general be intractable. Nonetheless, the point is that traditional scoring using \(p(x^{})\) is then _exactly_

Figure 2: **CherryML with FastCherries applied to the LG model.** (a) End-to-end runtime and (b) median estimation error as a function of sample size for CherryML with FastCherries vs CherryML with FastTree (as well as an oracle with ground truth trees and site rates). Practically, the loss of statistical efficiency for CherryML with FastCherries relative to FastTree or ground truth trees (which perform similarly) is \( 50\%\) with a small asymptotic bias of around \(2\%\), yet CherryML with FastCherries is two orders of magnitude faster when applied to 1,024 families. The bulk of the end-to-end runtime is taken by rate matrix estimation. The simulation setup is the same as in the CherryML paper . (c) On the benchmark from the LG paper , CherryML with FastCherries yields similar likelihood on held-out families compared to CherryML with FastTree, while (d) shows that CherryML with FastCherries is approximately 20 times faster end-to-end than CherryLM with FastTree, with the bottleneck now being rate matrix estimation.

_equivalent_ to scoring with \(p_{}(x^{}|x^{},t=)\). The analogy goes both ways: our approach can be thought of as a particular case of scoring with models of \(p(x)\), but where \(p(x)\) is constructed by conditioning an evolutionary model around the wild-type. Indeed, letting \(p_{}(y|x,t)\) be a model of protein evolution, one may define a model via \(p(x)=p_{}(x|x^{},t)\). In this case, scoring with \(p_{}(x|x^{},t)\) is equivalent to scoring with \(p(x)\). This way, the key conceptual difference between our approach and traditional approaches is the act of _conditioning on the wild-type_.

**Example.** We provide an illustrative example that underscores the power of conditioning on the wild-type. Suppose a given site in the MSA for some protein family has four amino acids \(I,L,D,E\), each with frequency \(25\%\). Note that \(I\) and \(L\) have similar biochemical properties (they are both non-polar, uncharged, and branched-chain), while \(D\) and \(E\) are similar (both being negatively charged). Suppose that mutations between \(I\) and \(L\) or between \(D\) and \(E\) at this site are tolerated, while other mutations among these residues are deleterious. In this case, using \( p(x^{})\) may not be able to tell apart the benign mutations at this site from the two pathogenic ones. Indeed, in the extreme case of the site-independent EVmutation model , all variants at this site will have exactly the same score. In contrast, our SiteRM model, despite being an independent-sites model, will learn and predict that only mutations that preserve the biochemical properties of the residue will be tolerated. Thus, despite being trained on exactly the same data, our SiteRM model has the built-in capacity to make sensible predictions of this kind whereas EVmutation may not. Although a highly expressive model might be able to succeed at this toy example by leveraging the _correlations_ between sites, it is evident that an evolutionary model - even if site-independent - is better placed to leverage insights like this one.

**Variant effect prediction results.** In the case of the SiteRM model with \(t=1\), we use

\[(x^{},x^{})= p(x^{}|x^{},t=1)=_{i=1}^{l}((Q_{i}^{f})[x_{i}^{},x_{i}^{}]) \]

to score variants with respect to the same wild-type sequence \(x^{}\). Note that \(x^{}\) may differ from \(x^{}\) at multiple sites, and (1) is an _independent-sites_ model, which uses additive per-site effects.

We trained the SiteRM model on the MSAs provided by the ProteinGym benchmark. Duplicate sequences were removed. We used FastCherries with the LG rate matrix and \(20\) rate categories to estimate cherries and site rates. Then, we use CherryML to estimate site-specific rate matrices, with the LG rate matrix as a prior and with regularization strength \(=0.5\) such that exactly half the data comes from pseudocounts and the other half from empirical counts. We found that modeling the gap as its own state improved performance, so that our best model uses \(21 21\) site-specific rate matrices, with the \(21 21\) rate matrix learnt from the LG paper's Pfam dataset as the prior.

Table 1 summarizes the DMS substitution benchmark results for some select notable models. Despite being an independent-sites model, SiteRM outperforms the large protein language model ESM-1v and its inverse-folding version ESM-IF1 on Spearman correlation, AUC, MCC and NDCG. SiteRM also outperforms EVmutation , which is based on the seminal Potts model with pairwise interactions, as well as the DeepSequence ensemble model . Only on Recall@10 do we observe less competitive results. We attribute the remarkable performance of our independent-sites model to conditioning on the wild-type, which is absent from competing approaches, and to our ability to estimate family- and site-specific rate matrices without overfitting. Full results with all models reported in the ProteinGym paper  are provided in Supplementary Table S1. The table also shows ablations for the SiteRM model where we reduce the size of the training dataset by subsampling sequences in the MSA, or by using FastTree instead of FastCherries, or by excluding gaps. The

  
**Model name** & **Spearman** & **AUC** & **MCC** & **NDCG** & **Recall** \\  SiteRM & **0.423** & **0.734** & **0.330** & **0.776** & 0.207 \\ EVmutation & 0.395 & 0.716 & 0.305 & **0.777** & 0.222 \\ DeepSequence (ensemble) & 0.419 & 0.729 & 0.328 & **0.776** & **0.226** \\ ESM-1v (ensemble) & 0.407 & 0.723 & 0.320 & 0.749 & 0.211 \\ ESM-IF1 & **0.422** & 0.730 & **0.331** & 0.748 & 0.223 \\   

Table 1: Despite being an _independent-sites_ model, SiteRM matches or outperforms many notable models, including the large protein language model ESM-1v , the alignment-based epistatic models EVmutation  and DeepSequence , and the inverse-folding model ESM-IF1 . Best performance among these models are shown in boldface.

largest MSA processed by FastCherries had approximately 454K distinct sequences, each of length 364. FastCherries took approximately 1,000 seconds on a single CPU core to estimate cherries and site rates for this MSA. Of these, 30 seconds were spent on the divide-and-conquer pairing step, and 970 seconds on distance and site rate estimation. Subsequently, \( 1\) seconds was spent per site-specific rate matrix estimation, meaning a total time of around 1,500 seconds. Extrapolating our estimates shown in Supplementary Table S1, FastTree would have taken around 50 times longer.

Finally, on the human clinical benchmark, Supplementary Table S2 shows that SiteRM achieves an AUC of \(0.911\), which is \( 0.02\) higher than ESM-1b , and less than \(0.01\) below the state-of-the-art.

## 5 Discussion

We have introduced an end-to-end method to estimate amino acid substitution rate matrices from MSAs alone. By combining CherryML  with our FastCherries algorithm, it achieves near-linear runtime. Through simulations, we rigorously studied the computational and statistical efficiency of the method, finding it to be around 10 to 100 times faster than CherryML with FastTree , all while being only \(50\%\) less efficient and having a small asymptotic bias of around \(2\%\) median relative error which we find negligible in practical applications. CherryML with FastCherries easily runs on MSAs with hundreds of thousands of sequences. We plan to contribute to the open-source CherryML package with our FastCherries method so that all researchers may use the end-to-end pipeline easily.

By leveraging FastCherries' scalability, we performed regularized inference under the SiteRM model, providing site-specific rate matrices for a given MSA. We applied this _independent-sites_ model of protein evolution to the task of variant effect prediction and found that it outperforms many well-established models such as deep protein language models. We believe that this seemingly paradoxical result that an independent-sites model outperforms epistatic models is largely explained by our new conceptual approach, which conditions a model of protein evolution around the wild-type.

**Future directions.** Our work leaves several intriguing avenues for future research. First, although in this work we focused on classical independent-sites models of protein evolution, the large amount of data generated quickly by our FastCherries method may be used to train more complex models of protein evolution that go beyond the independent-sites assumption, such as deep neural networks. These may combine the best of both worlds: the power of modeling the evolutionary process in time and the ability to take into account complex correlations between protein sites. Our work also opens up a new avenue in statistical phylogenetics by enabling tree reconstruction with _site-specific rate matrices_. Software such as IQTree  already allows phylogenetic tree reconstruction under a 'partition model', where sites may be grouped such that all sites in the same group evolve under the same transition-rate matrix. Exploring extreme cases of the partition model where each site is its own partition, as in our SiteRM model, is an exciting and new avenue of research that promises to improve phylogenetic tree reconstruction, and find its way into other applications such as ancestral sequence reconstruction. To this end, we plan to release the site-specific rate matrices we have estimated for all 15,051 protein families in the TrRosetta dataset . More broadly, our idea of bypassing tree estimation through the lens of composite likelihood may enable further methodological developments in other related models, such as mixture models . All in all, we expect our work to have a lasting impact in both statistical phylogenetics and computational variant effect prediction.

**Limitations.** One limitation of our current method is that it assumes - just like essentially all of phylogenetics - that the model of protein evolution is time-reversible, thereby constraining the set of possible rate matrices. This may be an important source of model misspecification in some cases, and there is an emerging body of work extending phylogenetic methods to time-irreversible models . Although we find that our method has outstanding performance on VEP, even when compared to full tree reconstruction methods such as FastTree, our method does exhibit a small but non-zero amount of asymptotic bias which may matter in some other downstream applications. For example, the rate matrices estimated with CherryML using FastCherries may yield different, less accurate tree topologies from those obtained with CherryML using FastTree or other tree reconstruction methods (such as PhyML ). The extent to which this is true requires further investigation.