# Transformer as a hippocampal memory consolidation model based on NMDAR-inspired nonlinearity

Dong-Kyum Kim\({}^{1}\)1 Jea Kwon\({}^{2}\)1 Meeyoung Cha\({}^{1,3}\)2 C. Justin Lee\({}^{2}\)3

\({}^{1}\)IBS Data Science Group

\({}^{2}\)IBS Center for Cognition and Sociality

\({}^{3}\)KAIST School of Computing

{kdkyum,jeakwon,mcha,cjl}@ibs.re.kr

Equal contribution.Corresponding authors.

###### Abstract

The hippocampus plays a critical role in learning, memory, and spatial representation, processes that depend on the NMDA receptor (NMDAR). Inspired by recent findings that compare deep learning models to the hippocampus, we propose a new nonlinear activation function that mimics NMDAR dynamics. NMDAR-like nonlinearity shifts short-term working memory into long-term reference memory in transformers, thus enhancing a process that is similar to memory consolidation in the mammalian brain. We design a navigation task assessing these two memory functions and show that manipulating the activation function (i.e., mimicking the Mg\({}^{2+}\)-gating of NMDAR) disrupts long-term memory processes. Our experiments suggest that place cell-like functions and reference memory reside in the feed-forward network layer of transformers and that nonlinearity drives these processes. We discuss the role of NMDAR-like nonlinearity in establishing this striking resemblance between transformer architecture and hippocampal spatial representation.

## 1 Introduction

Synaptic plasticity in the hippocampus is crucial for converting short-term memories into long-term memories during memory consolidation . The N-methyl-D-asparatic acid receptor (NMDAR) is essential for mediating this memory formation as well as spatial representation. NMDAR serves as a switch for such plasticity and long-term memory formation . Hippocampal place cells  and entorhinal cortex grid cells  are known to be central for spatial navigation in animals. NMDAR has been highlighted for its importance in place cell representations through hippocampal CA1 neurons . These discoveries have led to a deeper understanding of hippocampal function, inspiring recent efforts to replicate such spatial representation mechanisms in deep neural networks . However, whether non-linear dynamics resembling hippocampal functions can be developed and used to support spatial representation in deep learning models remains unclear.

NMDAR is a post-synaptic ion channel that is characterized by nonlinear dynamics that distinguish it from other ion channels in the brain. These nonlinear dynamics are evident in the whole-cell current voltage (I-V) relationship (Fig. 1a) and are modulated by Mg\({}^{2+}\) ion blockade at the pore region of the channel. Previous research indicates that the Mg\({}^{2+}\)-dependent nonlinear dynamics of NMDAR play a key role in synaptic plasticity and memory formation .

Recently, transformer-based deep learning models have been reported to have functions that resemble hippocampal formations . Transformers comprise two consecutive modules: a self-attentionlayer and a feed-forward network (FFN; see Fig. 1b). The self-attention layer is closely related to a recent neuroscience model that bridges with transformer  and it has been argued that softmax neurons in this layer behave like place cells in a navigation task . However, the role of neurons in FFNs involved in a spatial navigation task has yet to be elucidated and compared to hippocampal characteristics.

This paper uncovers a resemblance between the NMDAR nonlinearity and recently developed activation functions commonly used in FFNs of deep learning models (Fig. 1). NMDAR functions operate on activity-dependent repulsion of Mg\({}^{2+}\) ions [15; 16] and this phenomenon is particularly interesting because it supports self-gating of information flow (Ca\({}^{2+}\) ion influx) in the post-synaptic region. Similar to this NMDAR activity-dependent gating mechanism, activation functions in modern neural networks combine input with a self-gating function (i.e., a sigmoidal function that ranges between 0 to 1) that determines information flow. Our observation prompted the following inquiry: **Can NMDAR-like nonlinearity in the feed-forward network layer of transformers enhance the formation of long-term memory and spatial representation by place cells?**

Here we derive a novel NMDAR-like activation function using the NMDAR current-voltage (I-V) curve. In parallel we design a spatial navigation task in a 2D grid environment that assesses two memory types well-defined by neuroscience research: working memory and reference memory [17; 18]. The former assesses events from within-trial, while the latter controls across trials in a stable (unchanging) environment. We then test the transformer model with the NMDAR-like activation function and find that 1) place cell representations emerge in the FFN layer rather than in the self-attention layer, 2) the nonlinearity of the NMDAR-like activation function can regulate reference memory, 3) place cell-like neurons in FFNs are strongly correlated with reference memory, whereas this correlation is not observed in the self-attention layer; and 4) NMDAR-like activation shows the best reference memory performance when compared to other available activation functions.

Collectively, these findings suggest that adopting NMDAR-like nonlinearity in FFN of transformer models can enhance the formation of long-term memory and spatial place cell representation, similar to previous observations in the animal brain. We believe these findings have important implications for developing new brain-inspired AI models and for understanding similar processes that occur in the brain and AI models.

## 2 Methods

### Designing a 2D navigation task to test the role of working memory and reference memory

We designed a sensory observation prediction task in which an agent randomly walks in a 2D grid environment and is trained to predict subsequent sensory observations (see Fig. 2a) . The agent

Figure 1: (a) Schematic diagram of Mg\({}^{2+}\)-gated NMDAR modulating synaptic plasticity (left), current-voltage dynamics (I-V curve; top right) and an NMDAR-inspired activation function, NMDA\({}_{}(x)\) (bottom right). (b) Transformer architecture and nonlinear activation functions in its feed-forward network (bottom left): ReLU, Gaussian Error Linear Unit (GELU), and Sigmoid Linear Unit (SiLU).

receives a sequence of previous [Action (\(a\)), Observation (\(x\))] pairs as input, and its goal is to predict the next observation, which is masked (i.e., dotted squares in the sequence of Observation (\(x\)) in the figure). We use the transformer architecture as our model.

We generated \(N\) maps of 11\(\)11 2D grids. A random sensory observation chosen from ten letters is placed at each position on each map. Agents can move 'up,' 'right,' 'down,' 'left,' or'stay.' An agent starts at a random position and initiates a random walk on the map, which is randomly selected from \(N\) training maps, for 2,048 steps for each trial.

Our design novelty is the consideration of two memory types: **short-term working memory** and **long-term reference memory**. When the prediction based on nodes that were previously visited during the random walk is incorrect, it counts as a _working memory error_ (Fig. 2a left). In contrast, when the prediction based on unvisited nodes is incorrect, it counts as a _reference memory error_ (Fig. 2a right). See details of the navigation task and definitions in Appendix A.4 and Fig. S3.

It is important to note that minimizing the reference memory error by memorizing input sequences is infeasible; the possible number of sequence configurations is exponential since the input sequence is randomly generated at each trial. To solve this task, the model needs to: 1) understand the abstract structure of 2D space, 2) infer which map it is on from the input sequence data, and 3) memorize the sensory observations and their positions on that map. Compared to the previous works [14; 12] that focused on working memory error, the current paradigm evaluates two distinct types of memory error and gives a more holistic view of model performance.

### Transformer: two separate memory systems

We here review the self-attention mechanism and FFNs in the transformer architecture  that hypotheses about two separate memory systems are based on--working memory (formed within-trial) and reference memory (formed across-trials)--and why they are inferred to reside in self-attention layers and FFNs, respectively.

Self-attention mechanismGiven a sequence \(\{_{1},...,_{T}\}\) of \(d\)-dimensional input embeddings, the self-attention layer calculates the interaction term between each embedding element within a context window via the self-attention mechanism. More formally, each input embedding applies two linear layers (\(W_{k}\) and \(W_{v}\)) to the embeddings to form the key matrix \(K\) and value matrix \(V\):

\[K^{}=[_{t-c}^{}\ _{t-c+1}^{}\ \ _{t}^{}], V^{}=[_{t-c}^{}\ _{t-c+1}^{}\ \ _{t}^{}],\] (1)

where \(_{i}=_{i}W_{v}\ (W_{v}^{d d_{k}})\) and \(_{i}=_{i}W_{k}\ (W_{k}^{d d_{k}})\). Here, \(c\) denotes the context length. The key matrix \(K^{(c+1) d_{k}}\) is then used to compute the interaction score between an input embedding at step \(t\) and all the vectors in \(K\) via dot products:

\[_{t}=_{t}K^{},\ _{t}= _{t}W_{q}\ (W_{q}^{d d_{k}}).\] (2)

The normalized values of \(_{t}^{(c+1)}\), called attention values, are calculated via the softmax function; the final output of the self-attention mechanism is a weighted sum of the value vectors in

Figure 2: (a) Sensory observation prediction task in a 2D grid, where dotted squares indicate the target position to predict given a sequence of past actions and observations. The unvisited and visited places are represented in gray and black blocks, respectively. (b) A transformer model for predicting the location of an upcoming sensory observation based on sequences of [Action (\(a\)), Observation (\(x\))] pairs. Using a sequence of pairs in the context, the model is trained to predict the masked observation (i.e., the subsequent observation) corresponding to the final query action.

\(V^{(c+1) d_{k}}\) with the attention values:

\[_{t}=(_{t}K^{}}{}} )V.\] (3)

After this update, \(_{t}^{d_{k}}\) is updated by another linear transformation \(W_{o}^{d_{k} d}\): \(_{t}=_{t}W_{o}\). The output \(_{t}\) is added to the \(_{t}\); \(_{t}+_{t}\) providing the final output of the self-attention layer, and this information is sent through to the subsequent layer.

Feed-forward networks (FFNs)This component consists of two linear layers with a point-wise nonlinear activation function \(\):

\[(_{t})=(_{t}U_{1}^{})U_{2},\] (4)

where \(U_{1}^{d_{f} d}\) and \(U_{2}^{d_{f} d}\) are trainable weight matrices. Sukhbaatar et al.  showed that Eq. (3) and Eq. (4) have similar structures except for the following: 1) \(U_{1}\) and \(U_{2}\) matrices are fixed over different input sequences while \(K\) and \(V\) matrices are dynamically changed with input and 2) operations in FFNs are point-wise or local while the self-attention layer has non-local operations, e.g., the softmax function and dot products between different elements. This observation suggests that the FFNs store "general" knowledge about the task that does not depend on the situation.

### Resemblance of NMDA receptor nonlinear dynamics with modern activation functions

NMDAR's nonlinear dynamics mostly arise from the voltage-gated Mg\({}^{2+}\) repulsion at the NMDAR channel's pore [15; 16] (Fig. 1 left). Previous work showed this nonlinear \(I\)-\(V\) relationship to be:

\[I_{}=V_{,}(V)\] (5)

where \(V\) represents an input voltage and \(_{,}(V)\) is a voltage-dependent channel open probability that follows the ion blockade model :

\[_{,}(V)=}\] (6)

where \(=[^{2+}]/K_{^{2+}}\) is a parameter determined by \([^{2+}]\), \(K_{^{2+}}\) is a dissociation constant, and \(\) is a temperature constant. For further details, see Appendix A.2.

The dynamics of the NMDA receptor closely resemble those of new activation functions such as ReLU or GELU (see Fig. S2 in Appendix A.3). This visual resemblance motivated us to define a new NMDAR-inspired activation function (see details in Appendix A.3) as follows:

\[_{,}(x)=x_{,}(x)=}.\] (7)

This NMDA\({}_{,}(x)\) incorporates modern activation functions with varying values of temperature constant, \(\) (Table 1). To investigate the Mg\({}^{2+}\)-gated nonlinear dynamics, \(\), we compared various activation functions with NMDA\({}_{}(x)=x_{,=1}(x)\).

### Testing the NMDAR-inspired activation in navigation tasks

The transformer model is trained using softmax cross-entropy loss to predict subsequent sensory observations (i.e., dotted squares in Fig. 2). Instead of using sinusoidal positional encoding , we employ recurrent positional embedding which encodes the location of an input element by using the recurrent neural network (RNN); this method is closely related to the most advanced neuroscience model of the hippocampus .

 
**NMDA\({}_{,}\)** & **Name** & **Equation** & **Reference** \\  NMDA\({}_{=1,=1}(x)\) & SiLU(x) & \(x(x)\) &  \\ NMDA\({}_{=1,=1.702}(x)\) & GELU(x) & \(x(1.702x)\) &  \\ NMDA\({}_{=1,=}(x)\) & ReLU(x) & max\((0,x)\) &  \\ NMDA\({}_{=1,}(x)\) & Swish(x) & \(x( x)\) &  \\  

Table 1: Comparison of common activation functions with NMDA\({}_{,}\)We generate the embedding vectors of the sensory observation sequence with a word embedding layer, but the embedding vectors of the action sequence are generated using an RNN; \(_{t+1}=(_{t}W_{a})\), where \(_{t}\) is a recurrent positional embedding at step \(t\), and \(W_{a}\) is the action-dependent trainable weight matrix. The input is given by \(\{[_{1},_{1}],[_{2},_{2}],,[ _{t},_{t}]\}\), where \(\) denotes the embedding vector of sensory observation \(x\); the initial recurrent positional embedding \(_{1}\) is sampled from a normal distribution, and we mask the last observation \(x_{t}\).

In our experiment, FFN in the transformer model consists of two linear layers (see Fig. 1b and Eq. (4)) with the NMDAR-inspired activation function NMDA\({}_{}\). We use TransformerXL  with an extended memory length of \(32\) and segment length of \(32\) so that the context length \(c\) is 64 and working memory error is measured when the node to predict its sensory observation is in the context window (see Fig. 2b); i.e., a node that the agent has not been visited in the last 64 steps is treated as _an unvisited node_. Note that our model is unable to access the sensory observations of unvisited nodes via the self-attention mechanism due to the fixed context window size.

The input embedding is a concatenated vector \([,]\) of the word embedding \(\) (dimension of 256), and the recurrent positional embedding \(\) (dimension of 256) so that the total input embedding dimension is 512. The number of heads in the self-attention layer is 8, and the number of neurons in the FFN is 2,048. The dropout rate is set to 0.1, and the maximum clip norm of the gradient is set to 0.25. We employ the ADAM optimizer  and a learning rate schedule with a linear decay from \(0.0001\) (start) to \(0\) (end). We run 512 random walk simulations (trials) in parallel to collect training trajectories. The total number of random walking steps is 2,048 for each simulation, and the total number of steps for training a model is 512 (batch size; the number of trials per epoch) \(\) 2,048 (total number of steps in a trial) \(\) 200 (number of epochs) (see Fig. S3 in Appendix A.4). The average number of unvisited nodes in a single trial was 561. The PyTorch code for reproducing our results is available at https://github.com/kdkyum/transformer_memory_consolidation.

## 3 Results

### Working memory and reference memory errors

To measure the impact of nonlinearity \(\) in the FNNs, we trained transformer models with different values of \(\) in \([0,0.01,0.05,0.1,0.5,1,5,10]\) and evaluated the working memory and reference memory errors on the train maps (i.e., familiar maps) and test maps (i.e., novel maps). The changing \(\) values in a transformer model mimic the changes in NMDAR Mg\({}^{2+}\)-gating in the brain, as inspired by neuroscience findings that selective inhibition of hippocampal CA1 NMDAR can disrupt the consolidation of short-term working memory to long-term reference memory.

The top left plot in Fig. 3a shows that the reference memory error on the training maps rapidly decreased over training trials when \(\) is larger than zero, with a larger improvement for increasing \(\). The reference memory error on the novel maps, however, was nearly constant at a chance level of 0.9 (\(=1-1/()\)) for all \(\) (see Fig. 3a top right). Fig. 3a bottom right shows that working memory was active on novel maps that had not previously been shown during training. This finding suggests that working memory formation is intact on novel maps. Training the models on different numbers of maps, \(N\), Fig. 3b shows that increasing nonlinearity (i.e., \(\)) helped to activate reference memory, and the trend of improvement was consistent, as shown for \(N=32\), \(48\), and \(64\). Training more maps led to larger reference memory errors, because increasing \(N\) means the model needs to memorize what-where pairs (i.e., each training contains unique what-where information).

In addition, we find that removing nonlinearity in the NMDAR-inspired activation function (\(=0\)) impairs reference memory formation (Fig. 3b) but leaves working memory formation intact (Fig. 3a). This means that even though the trainable parameters exist in the self-attention layer, a lack of nonlinearity in the feed-forward layer significantly impairs the formation of long-term reference memory in familiar maps. This result suggests that short-term working memory and long-term reference memory are physically stored in separate structures (the self-attention layer and the FFN) of transformers and possibly gated by the nonlinear activation functions residing in the FFN.

We next tested other nonlinear activation functions, including GELU (\(x(1.702x)\)), ReLU (\((0,x)\)), LeakyReLU (\((0,x)+0.01(0,x)\)), sigmoid, and tanh. Fig. 3c shows that the newly proposed biologically-inspired function (NMDA\({}_{=10}\)) yielded substantially lower reference memory errors relative to other activation functions. A comparison with the second-best-performingGELU activation, for example, indicated a statistically significant difference with \(p\)-value less than 0.05 (see Appendix A.6). These results open up the possibility of finding a better activation function through a new \(\) hyperparameter space of the NMDA-inspired activation function.

Expanding on the evaluation, one may consider other memory types like recurrent positional embedding instead of reference memory. We used non-recurrent positional embeddings to train the models (see Appendix A.5) and confirmed that working memory and reference memory errors increased more substantially. This finding supports the idea that working memory is crucial for memory consolidation and that disrupting it impairs reference memory. However, we also saw a similar trend of decreasing reference memory error while increasing \(\) of NMDA\({}_{}\) (see Fig. S4).

We further assessed the prediction error of the first visited node. While the reference memory error is defined as a prediction error on a node that the agent has not visited in the previous 65 steps, the first visited node prediction error is a prediction error on a node that the agent visits for the first time in a trial. The results for the first visited node prediction error were identical to the results for the reference memory error (see Fig. S6 in Appendix A.5). These findings suggest that reference memory is used in training maps to predict the unvisited node.

### Place cells in feed-forward networks

One of the most striking characteristics of the hippocampus is the presence of the **place cells**, which are neurons in the brain that fire at a particular place in the environment . Studies have shown that hippocampal place cells encode spatial location through localized firing patterns. They have been considered a substrate for long-term memory of the location where specific events occurred (e.g., a previously visited position in a navigation task). Selective impairment of NMDAR in hippocampal CA1 disrupts place cell emergence and memory consolidation of long-term memory [2; 3; 9].

We investigated whether place cells emerge during the formation of long-term reference memory in a transformer model, building upon our previous results demonstrating selective impairment in long-term reference memory with NMDA\({}_{}\) modulation. We newly designed a metric called the **place cell score** described in Eq. S1 in Appendix A.1 and examined the spatial representation within the transformer architecture. This metric, ranging from 0 (i.e., homogeneous firing) to 1 (i.e., firing is specific to the agent's position in the grid) quantifies the firing specificity of each neuron for spatial

Figure 3: (a) Reference and working memory error rates over training trials for training maps and testing maps for \(N=32\) training maps. Insets in the bottom figures show working memory error rates during the initial training phase. (b) Reference memory errors were evaluated on training maps over different values of \(\) in NMDA\({}_{}\) and \(N\). (c) Reference memory error comparison between NMDA\({}_{}=10\), GELU, ReLU, LeakyReLU, sigmoid, and tanh activation functions. Inset: magnified view of the top 4 activation functions. Error bars and shaded areas represent the standard deviation of errors from three independently trained models.

locations, similar to the approach described in the TEM-t model . We recorded the activation values of each neuron at every step during a random walk process. As the agent traversed the environment, which had an \(11 11\) grid structure, we accumulated the activation values for each grid point, resulting in a 2D array known as the _rate map_ or _place field_ (i.e., the spatial distribution of single neuron activation during a random walk, as shown in Fig.4a and Fig.4b).

Fig. 4a and 4b show the rate maps of neurons with place cell scores in the FFN and self-attention layers, respectively. For the self-attention layer, the total number of neurons in the softmax layer was 65 (context length + masked sensory observation) \(\) 8 (number of heads) \(\) 2 (number of layers). The total number of neurons in the FFN layer was set at 2,048 (number of neurons) \(\) 2 (number of layers). As can be seen, our metric accurately represents location specificity. Fig. 4c and 4d show the distribution of place cell scores in the two layers with different values of \(\). When the \(\) value is increased, the place cell score distribution found in the FFN becomes positively shifted (see Fig. 4h rate map examples for \(=0\), \(1.0\), and \(10.0\)), whereas the place cell score distribution in the self-attention layers remains.

Fig. 4e and 4f show a relationship between the average place cell score and the reference memory error for each \(\). The average place cell scores in the self-attention layers show no correlation with reference memory errors, while neurons in the FFN exhibit a clear correlation. These results imply that reference memory formation and place cell emergence can be enhanced by NMDAR-like nonlinearity in the FFN.

Figure 4: Reference memory-related place cells selectively emerge in FFN but not in the self-attention layer over increasing \(\). (a, b) Example rate maps with place scores in FFN and self-attention layers at \(=10\); from top left (high) to bottom right (low); color bar indicates the firing rate between \(_{}\) and \(_{}\). (c-d) Place cell score distributions with varying \(\) in FFN (c) and self-attention layers (d). (e-f) Scatter plot of average place cell scores and reference memory errors. \(r\) and \(p\) denote Spearmanâ€™s rank correlation coefficient and significance score, respectively. (g) place cell score distribution and relationship of average place cell scores and reference memory errors in common activation functions: GELU, ReLU, LeakyReLU, tanh, and sigmoid. All results are evaluated using training maps. (h) Rate maps of neurons with top-16 place cell scores in the FFNs with varying values of \(\); \(=10\) (left), \(=1\) (middle), and \(=0\) (right).

Fig. 4g compares the place cell representations of our NMDA (\(=0,10\)) with the representations in FFNs with the activation functions used in Fig. 3c, indicating that the case of NMDA\({}_{=10}\) outperforms other activation functions, in both reference memory formation and place cell representation. Our finding that large \(\) (the [Mg\({}^{2+}\)] component) enhances reference memory is in line with the biological observation that increasing the [Mg\({}^{2+}\)] in the brain enhances long-term memory formation .

We also explored the possibility of extending the hyper-parameter space by contrasting the NMDA-inspired activation function with an alternative activation function based on \(\). LeakyReLU was considered for this experiment because of its adjustable negative slope, which was re-defined as \(((0,x)+(0,x))\). The results reported in Fig. S10 of Appendix A.7 show that LeakyReLU activation, for all tested \(\) values, produced lower place cell scores than the best-performing NMDA-inspired function (i.e., NMDA\({}_{=10}\)). Our findings suggest that NMDA-like nonlinearity, which is already known to be critical in biological systems, could be used to enhance transformer models.

Building upon these results, our work connects memory consolidation theory from neuroscience to transformer-based processing of short-term and long-term memory. Unlike previous studies that directly provide 2D geometric information to models [11; 10], our model only takes sensory-action pairs as an input, which was sufficient to observe the emergence of place cells in the transformer model. Specifically, we showed that nonlinearity in the transformer's FFNs plays a crucial role in transforming short-term working memory from the self-attention layer into long-term memory, and the place cell emergence. The observed place cells may correspond to sparse interpretable activation in FFN. See Appendix A.10 for further analysis on the sparsity and place cells. These findings provide new insight into the inner workings of transformers and their connection to neural processes in the brain.

## 4 Related works

The current study is inspired by recent studies that connect neuroscience and AI models. One such seminal work is by Whittington et al. , which showed a relationship between the self-attention layer and a recent hippocampal model called the Tolman-Eichenbaum Machine (TEM; Whittington et al. ). Our work expands the literature by focusing on FFNs in the transformer and making a connection to the emergence of place cells.

TEM is a neuroscience-based model that reproduces neural representations in the hippocampus and entorhinal cortex. Instead of storing memory in the key matrix \(K\) and value matrix \(V\), it instead stores memory using a Hebbian weight matrix \(M^{d_{k} d_{k}}\). Every outer product of key and value vector \(_{i}^{}_{i}\) at each step \(i\) is simply stored in \(M\) via the Hebbian update rule. \(M\) is initialized to a zero matrix at the beginning of the task and adds every outer product at each time step:

\[M=a_{i=1}^{t}_{i}^{}_{i}=aK^{}V,\] (8)

where \(a\) is a weighting factor. In the memory retrieving phase with the query vector \(\), TEM uses an attractor network:

\[M=aK^{}V.\] (9)

Whittington et al.  found that the memory retrieving process in TEM has a close mathematical structure to Eq. (3) when the softmax function is replaced with a linear function. We note that methods for linearizing softmax in self-attention layers have been studied to address high computational costs when context length \(c\) is very long [27; 28]. The subsequent model of TEM, called TEM-t , replaces the attractor network (Eq. (9)) with a self-attention mechanism (Eq. (3)). This study demonstrated that TEM-t learns significantly faster than TEM.

TEM-t and TEM do not have a fixed context length \(c\); therefore, these models store all information before step \(t\), i.e., \(c=t\). The computational cost of the self-attention layer in TEM-t is \(O(t^{2})\), and retaining all previous information is too expensive from both a biological and computational standpoint3. For TEM, the Hebbian update rule has no quadratic computational cost and can add all previous information in a fixed number of synapses \(d_{k}^{2}\); however, the memory capacity of the Hebbian matrix \(M\) is \(O(d_{k})\) and the speed of memory retrieval is substantially slower than the self-attention mechanism [29; 30; 31]. In contrast to TEM and TEM-t that rely on a single memory system, we investigated two separate memory systems: 1) context-dependent matrices \(K\) and \(V\) in the self-attention layer with a fixed context length \(c\) and 2) context-independent fixed matrices \(U_{1}\) and \(U_{2}\) (in Eq. (4)) in the FFNs.

Our research differs from previous studies in the following ways: 1) We designed a navigation test that assesses working memory and reference memory separately, providing a more comprehensive evaluation of the model's performance. 2) We proposed a new brain-inspired activation function, NMDA\({}_{}\), which relates to modern nonlinear activation functions and allows for the analysis of the effect of \(\) on long-term reference memory formation. 3) We demonstrated that place cell-like neurons emerge in FFNs in conjunction with reference memory formation, which is a novel finding that has not been addressed in the TEM or TEM-t models. 4) TEM and TEM-t focus on working memory errors and do not cover reference memory errors. In contrast, our work evaluates both types of memory errors, providing a more detailed analysis of the model's performance.

## 5 Discussion

While extensive efforts have been directed toward finding the optimal nonlinear activation function for improving modern deep neural network models [21; 22; 23], their relationship to neural substrates that mediate nonlinearity in the human brain remains obscure. Furthermore, the role of nonlinearity in intelligent functions remain unclear. Our research attempts to fill this gap by proposing and testing how a biologically inspired nonlinearity functions in a transformer model that was previously related to the hippocampal formation. We examined functions in terms of long-term reference memory formation and place cell representation. This idea was evaluated in a carefully designed 2D grid environment and by implementing an activation function derived from NMDAR-like nonlinearity.

Our research reveals that place cell-like neurons that are critical to spatial navigation can be found in both the self-attention layers and FFNs of the transformer model, similar to the presence of place cells in the CA3 and CA1 regions of the hippocampus . In the hippocampus, the CA3 region is thought to be involved in the initial formation of new memories, specifically in pattern completion , while CA1 is thought to be important for the long-term consolidation of memories. As such, we suggest that the CA3 region may serve a similar function to the self-attention layer, while the CA1 region may function similarly to FFN. However, further research is needed to fully understand the similarities and differences between the properties of place cells in the transformer model and those found in the CA3 and CA1 regions of the hippocampus. For more discussions regarding the biological plausibility of this proposal, see Appendix A.8.

Recent machine learning research has tested whether the transformer architecture is analogous to different types of biological memory. It has been suggested that (1) transformer FFN modules resemble associative memory , (2) the FFN in a transformer block functioning as a key-value memory [35; 36], (3) activation sparsity in the transformer FFN enhances robustness to noisy input [37; 38], and (4) sparse activity in FFNs increases the percentage of neurons that selectively activate to human interpretable input features .

Our data instead suggests the transformer architecture resembles _memory consolidation_ by the animal brain, which refers to the transfer process of a short-term memory into a long-term memory system in neuroscience research . Previous research has revealed that Mg\({}^{2+}\)-gating of NMDA receptors modulates the formation of long-term memories [5; 6]. These observations imply that the nonlinear dynamics of NMDA receptors in hippocampus CA1 are critical for consolidating short-term memory into long-term memory.

To our surprise, our results agree qualitatively with previous NMDAR impairment experiments from neuroscience: 1) selective inhibition of hippocampal CA1 NMDAR inhibition does not disrupt working memory  but impairs the long-term memory formation , 2) changing NMDAR Mg\({}^{2+}\)-gating (changing \(\) in this work) enhances or disrupts long-term memory formation [5; 6], 3) NMDAR is required for long-term stabilization of newly forming place fields [9; 3]. These similarities between hippocampal memory consolidation and our results suggest that the transformer is an effective memory consolidation model.

Our research points to exciting future directions. The current study examined what-where memory using a sensory observation task in a static environment. However, our real-world environment changes constantly and provides new inputs over time. Modern deep learning systems are generally incapable of adapting to a dynamic environment or reordering of sensory inputs. We intend to explore what-where-when memory, called _episodic memory_, in transformers and other deep models.