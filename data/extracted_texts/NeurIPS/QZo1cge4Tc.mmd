# Counterfactually Fair Representation

Zhiqun Zuo\({}^{1}\) Mohammad Mahdi Khalili\({}^{1,2}\) Xueru Zhang\({}^{1}\)

zuo.167@osu.edu khalili.17@osu.edu zhang.12807@osu.edu

\({}^{1}\)CSE Department, The Ohio State University, Columbus, OH 43210

\({}^{2}\)Yahoo Research, New York, NY, 10003

###### Abstract

The use of machine learning models in high-stake applications (e.g., healthcare, lending, college admission) has raised growing concerns due to potential biases against protected social groups. Various fairness notions and methods have been proposed to mitigate such biases. In this work, we focus on Counterfactual Fairness (CF), a fairness notion that is dependent on an underlying causal graph and first proposed by Kusner _et al._; it requires that the outcome an individual perceives is the same in the real world as it would be in a "counterfactual" world, in which the individual belongs to another social group. Learning fair models satisfying CF can be challenging. It was shown in  that a sufficient condition for satisfying CF is to **not** use features that are descendants of sensitive attributes in the causal graph. This implies a simple method that learns CF models only using non-descendants of sensitive attributes while eliminating all descendants. Although several subsequent works proposed methods that use all features for training CF models, there is no theoretical guarantee that they can satisfy CF. In contrast, this work proposes a new algorithm that trains models using all the available features. We theoretically and empirically show that models trained with this method can satisfy CF1.

## 1 Introduction

While machine learning (ML) has had significant impacts on human-involved applications (e.g., lending, hiring, healthcare, criminal justice, college admission), it also poses significant risks, particularly regarding unfairness against protected social groups. For example, it has been shown that computer-aided clinical diagnostic systems can exhibit discrimination against people of color ; face recognition surveillance technology used by police may have racial bias ; a decision support tool COMPAS used for predicting the recdivision risk of defendants is biased against African Americans . Various fairness notions have been proposed to mathematically measure the biases in ML based on observational data. Examples include: i) _unawareness_ which prohibits the use of sensitive attribute in model training process; ii) _parity-based fairness_ that requires certain statistical measures to be equalized across different groups, e.g., equalized odds , equal opportunity , statistical parity , predictive parity ; iii) _preference-based fairness_ that is inspired by the fair-division and envy-freeness literature in economics, it ensures that given the choice between various decision outcomes, every group of users would collectively prefer its perceived outcomes, regardless of the (dis)parity compared to the other groups .

However, the fairness notions mentioned above do not take into account the causal structure and relations among different features/variables. Recently, Kusner _et al._ proposed a fairness notion called _Counterfactual Fairness_ (CF) based on the causal model and counterfactual inference; itrequires that the ML outcome received by an individual should be the same in the real world as it would be in a counterfactual world, in which the individual belongs to a different social group. To satisfy CF,  shows that it is sufficient to make predictions only using the features that are non-descendants of the sensitive attribute node in the causal graph. However, this approach may discard crucial data, as descendants of sensitive attribute may contain useful information that is critical for prediction and downstream tasks. In this work, we show that using only non-descendants is not a necessary condition for achieving CF. In particular, we propose a novel method for generating counterfactually fair representations using all available features (including both descendants and non-descendants of sensitive attribute). The idea is to first generate counterfactual samples of each data point based on the causal structure, and the fair representations can be generated subsequently by applying a _symmetric function_ to both factual (i.e., original data) and counterfactual samples. We can theoretically show that ML models (or any other downstream tasks) trained with counterfactually fair representations can satisfy _perfect_ CF. Experiments on real data further validate our theorem.

It is worth noting that several subsequent studies of  also proposed methods to learn CF models using all available features, e.g., [14; 36; 24; 10; 2]. However, these methods are empirical and there is no theoretical guarantee that these methods can satisfy (perfect) CF. In Appendix A, we introduce more related work and discuss the differences with ours. Our main contributions are as follows:

* We propose a novel and efficient method for generating counterfactually fair representations. We theoretically show that ML models trained with such representations can achieve perfect/exact CF.
* We extend our method to path-dependent counterfactual fairness . That is, for any unfair path in a causal graph, we can generate representations that mitigate the impact of sensitive attributes on the prediction along the unfair path.
* We conduct extensive experiments (across different causal models, datasets, and fairness definitions) to compare our method with existing methods. Empirical results show that 1) our method outperforms the method of only using non-descendants of sensitive attributes; 2) existing heuristic methods for training ML model under CF fall short of achieving perfect CF fairness.

## 2 Problem Formulation

Consider a supervised learning problem where the training dataset consists of triples \(V=(X,A,Y)\), where random vector \(X=[X_{1},,X_{d}]^{}\) are observable features, \(A\) is the sensitive attribute (e.g., race, gender) indicating the group membership, and \(Y\) is the label/output. Similar to , we associate the observable \(V=(X,A,Y)\) with a causal model \(=(U,V,F)\), where \(U\) is a set of unobserved (exogenous) random variables that are factors not caused by any variable in \(V\), and \(F=\{f_{1},f_{2},,f_{d},f_{d+1},f_{d+2}\}\) is a set of functions (a.k.a. structural equations ) with one for each variable in \(V\). WLOG, let

\[X_{i}=f_{i}(pa_{i},U_{pa_{i}}),\;i\{1,,d\};\;\;\;A=f_{d+1}(pa_{d+1},U_ {pa_{d+1}});\;\;\;Y=f_{d+2}(pa_{d+2},U_{pa_{d+2}}),\]

where \(pa_{i}\) and \(U_{pa_{i}}\) are the sets of observable and unobservable variables that are the parents of \(X_{i}\). \(pa_{d+1}\) and \(pa_{d+2}\) (resp. \(U_{pa_{d+1}}\) and \(U_{pa_{d+2}}\)) are the observable (resp. unobservable) variables that are parents of \(A\) and \(Y\), respectively. Assume \((U,V)\) can be represented as a directed acyclic graph.

Our goal is to learn a predictor \(=g_{w}(R)\) parameterized by weight vector \(w^{d_{w}}\) from training data. Here \(R=h(X,A;)\) is a representation generated using \((X,A)\) and causal model \(\). Define loss function \(l:\) where \(l(Y,g_{w}(R))\) is the loss associated with \(g_{w}\) in estimating \(Y\) using representation \(R\). We denote the expected loss with respect to the joint probability distribution of \((R,Y)\) by \(L(w):=\{l(Y,g_{w}(R))\}\). Throughout the paper, we use small letters to denote the realizations of random variables, e.g., \((x,a,y)\) is a realization of \((X,A,Y)\).

### Background: Intervention and Counterfactual Inference

Given structural equations and the distribution of unobservable variables \(U\), we can calculate the distribution of any observed variable \(V_{i} V\) and even study the impact of intervening certain observed variables on other variables. Specifically, the **intervention** on variable \(V_{i}\) can be done by simply replacing structural equation \(V_{i}=f_{i}(pa_{i},U_{pa_{i}})\) with equation \(V_{i}=v\) for some \(v\). To study the impact of intervening \(V_{i}\), we can use new structural equations to find resulting distributions of other observable variables and see how they may differ as \(v\) changes.

The specification of structural equations \(F\) further allows us to compute **counterfactual** quantities, i.e., computing the value of \(Y\) if \(Z\) had taken value \(z\) for two observable variables \(Z,Y\). Because the value of any observable variable is fully determined by unobserved variables \(U\) and structural equations, the counterfactual value of \(Y\) for a given \(U=u\) can be computed by replacing structural equations for \(Z\) as \(Z=z\). Such counterfactual value is typically denoted as \(Y_{Z+z}(u)\).

The goal of **counterfactual inference** is to compute the probabilities \(\{Y_{Z+z}(U)|O=o\}\) for some observable variables \(O\). It can be used to infer "the value of \(Y\) if \(Z\) had taken value \(z\) in the presence of evidence \(O=o^{}\). Based on , \(\{Y_{Z+z}(U)|O=o\}\) can be computed in three steps: (i) _abduction_ that finds posterior distribution of \(U\) given \(O=o\) for a given prior on \(U\); (ii) _action_ that performs intervention \(Z=z\) by replacing structural equations of \(Z\); (iii) _prediction_ that computes the distribution of \(Y\) using new structural equations and the posterior \(\{U|O=o\}\)

### Counterfactual Fairness

Without fairness consideration, simply learning a predictor by minimizing the expected loss, i.e., \(_{w}L(w)\), may exhibit biases against certain social groups. One way to tackle unfairness issue is to enforce a certain fairness constraint when learning the predictor. In this work, we consider _counterfactual fairness_ as formally defined below.

**Definition 1** (Counterfactual Fairness (CF) ).: _We say a predictor \(=g_{w}(R)\) satisfies CF if the following holds for every \((x,a)\):_

\[\{_{A a}(U)=y|X=x,A=a\}=\{_{A a^{} }(U)=y|X=x,A=a\},\; y,a^{}.\]

This notion suggests that any intervention on sensitive attribute \(A\) should not change the distribution \(\) given that \(U\) follows distribution \(_{}\{U|X=x,A=a\}\)2. In other words, adjusting \(A\) should not affect the distribution of \(\) if we keep other factors that are not causally dependent on \(A\) constant. Learning a fair predictor satisfying CF can be challenging. As shown in , _a sufficient condition for satisfying CF is to **not** use features that are descendant of \(A\)._ In other words, given training dataset \(D=\{x^{(i)},a^{(i)},y^{(i)}\}_{i=1}^{n}\), it suffices to minimize the following empirical risk to satisfy CF:

\[_{w}_{i=1}^{n}\{l(y^{(i)},g_{w}(U^{(i) },x^{(i)}_{\!{X}A}))|X=x^{(i)},A=a^{(i)}\},\]

where \(x^{(i)}_{\!{X}A}\) are non-descendant features of \(A\) corresponding to \(i\)-th sample, and the expectation is with respect to the random variable \(U^{(i)}_{}\{U|X=x^{(i)},A=a^{(i)}\}\).

Although removing the descendants of \(A\) from the input is a simple method to address the unfairness issue, it comes at the cost of losing important information. In some examples (e.g., the ones provided in ), it is possible that all (or most of) the features are descendants of \(A\) and need to be eliminated when training the predictor. We thus ask:

_Can we train a predictor that satisfies **perfect** CF using all the available features as input?_

Although several recently proposed methods try to train CF predictors using all the available features (including both non-descendants and descendants of \(A\)) , there is no guarantee that they can satisfy CF. In contrast, our work aims to propose a theoretically-certified algorithm that finds counterfactually fair predictors using all the available features.

## 3 Proposed Method

In this section, we introduce our algorithm for training a supervised model under CF. Our method consists of three steps: (i) counterfactual samples generation; (ii) counterfactually fair representation generation; and (iii) fair model training. We present each step in detail as follows.

**1. Counterfactual samples generation.** We first introduce the definition of counterfactual samples and then present the method for generating them. They will be used for generating CF representations.

**Definition 2** (Counterfactual Sample).: _Consider \(i\)-th data point in training dataset with feature vector \(x^{(i)}\) and sensitive attribute \(a^{(i)}\). Let \(u^{(i)}\) be the unobservable variable associated with \((x^{(i)},a^{(i)})\) sampled from distribution \(_{}\{U|X=x^{(i)},A=a^{(i)}\}\) under causal model \(=(V,U,F)\). Then, \((^{(i)},^{(i)})\) is a counterfactual sample with respect to \((x^{(i)},a^{(i)})\) if \(^{(i)} a^{(i)}\) and \(^{(i)}\) is generated using structural equations \(F\), unobservable variable \(U=u^{(i)}\), and intervention \(A=^{(i)}\).3_

Equivalently, we can represent the counterfactual feature \(^{(i)}=^{(i)}_{A^{(i)}}(u^{(i)})\). Next, we use an example to clarify the generation process of counterfactual sample.

**Example 1** (**Law School Success**).: _Consider a group of students, each has observable features grade-point average (GPA) before entering college \(X_{G}\) and entrance exam score (LSAT) \(X_{L}\). Let first-year average grade in college (FYA) be label \(Y\) to be predicted and let race \(Q\) and sex \(S\) be the sensitive attributes. Suppose there are three unobservable variables \(U_{G},U_{L},U_{F}\) representing errors and the relations between these variables can be characterized by the following structural equations:_

\[X_{G} = =b_{G}+w_{G}^{Q}Q+w_{G}^{S}S+U_{G},\] \[X_{L} = =b_{L}+w_{G}^{Q}Q+w_{K}^{S}S+U_{L},\] \[Y = =b_{F}+w_{F}^{Q}Q+w_{F}^{S}S+U_{F},\]

_where \((b_{G},b_{L},b_{F},w_{G}^{Q},w_{G}^{S},w_{L}^{Q},w_{L}^{S},w_{F}^{R},w_{F}^{S})\) are the parameters of the causal model, which we assume are given4. Consider one student with \(x^{(0)}=(x_{G}^{(0)},x_{L}^{(0)})\) and \(a^{(0)}=(q^{(0)},s^{(0)})\). To generate its counterfactual sample, we first compute the underlying unobservable variables \((u_{G}^{(0)},u_{L}^{(0)})\):_

\[(u_{G}^{(0)},u_{L}^{(0)})=(x_{G}^{(0)}-b_{G}-w_{G}^{Q}q^{(0)}- w_{G}^{S}s^{(0)},x_{L}^{(0)}-b_{L}-w_{L}^{Q}q^{(0)}-w_{L}^{S}s^{(0)}).\]

_Then, for any \((,)-\{(q^{(0)},s^{(0)})\}\), the corresponding counterfactual features can be generated:_

\[_{G}^{(0)} = b_{G}+w_{G}^{Q}+w_{G}^{S}+u_{G}^{(0)}=x_{G}^{ (0)}+w_{G}^{Q}(-q^{(0)})+w_{G}^{S}(-s^{(0)})\] \[_{L}^{(0)} = b_{L}+w_{L}^{Q}+w_{L}^{S}+u_{L}^{(0)}=x_{L}^{ (0)}+w_{L}^{Q}(-q^{(0)})+w_{L}^{S}(-s^{(0)})\]

In the above example, finding unobservable variables is straightforward due to the additive error model (i.e., each observable variable \(V_{i}\) is equal to \(f_{i}(pa_{i})+U_{i}\) ). For other causal models that are non-additive, we can leverage techniques such as Variational Auto Encoder (VAE) to first learn distribution \(_{}\{U|X=x,A=a\}\) and then sample from this distribution, see e.g., .

**2. Counterfactually fair representation generation.** Next, we introduce how to generate counterfactually fair representation \(R=h(X,A;,s)\) using counterfactual samples generated above.

The complete procedure is given in Algorithm 1. The idea is to first apply a _symmetric function_\(s()\) to both factual feature \(x\) and counterfactual features \(\{^{[j]}\}_{j=1}^{|-1}\). This output can be leveraged to generate CF representation. The symmetry of the function is formally defined below.

**Definition 3**.: _A function \(s:^{||}\) is symmetric if the output is the same for any permutation of inputs._

One example of symmetric function is the average over all inputs, e.g., \(s(x,^{},,^{[|A|-1]})=^{}++ ^{[|A|-1]})}{|A|}\).

**3. Fair model training.** Given CF representation \(R=h(X,A;,s)\) generated by Algorithm 1, we can use it directly to learn a predictor that satisfies CF. Indeed, we can show that any predictor learned based on CF representation satisfies perfect CF, as stated in Theorem 1 below.

**Theorem 1**.: _If representation is generated based on \(h(x,a;,s)\) in Algorithm 1, then the predictor \(g_{w}(h(x,a;,s))\) satisfies perfect CF for all \(w^{d_{w}}\)._

Because \(g_{w}(h(x,a;,s))\) satisfies CF for all parameter \(w\), we can find the optimal predictor directly by solving an unconstrained optimization:

\[w^{*}=_{w}_{i=1}^{n}l(y^{(i)},g_{w}(h(x^{(i)},a^{(i )};,s))\]

Under Theorem 1, it is guaranteed that the optimal predictor \(g_{w^{*}}\) satisfies counterfactual fairness.

**Inference.** After learning the optimal CF predictor \(g_{w^{*}}\), we can use it to make fair predictions about new data. At the inference phase, for a given example \((x,a)\), we first generate its CF representation using Algorithm 1 and find the prediction \(\) using \(g_{w^{*}}\). That is, \(=g_{w^{*}}(h(x,a;,s))\).

**Discussion.** Compared to , our method leverages all available features and can attain much better performance without sacrificing fairness. We will further validate this in experiments (Section 5). As mentioned earlier, there are existing methods that also use all features to train predictors under CF constraint. For instance, the method proposed in  also generates the counterfactual sample for each training data and it trains model using both factual and counterfactual samples;  learns CF predictor by adding a penalty term to the learning objective function, where the penalty term is calculated based on the counterfactual samples. While these two methods also leverage counterfactual samples to reduce bias, they cannot satisfy _perfect_ CF and there is no theoretical guarantee.

## 4 Path-dependent Counterfactual Fairness

In this section, we consider a variant notion of CF called _Path-dependent Counterfactual Fairness_ (PCF). We will show how the proposed method can be adapted to train predictors under PCF. Let \(\) be the graph associated with causal model \(\), and \(_{_{A}}\) be the set of all unfair directed paths from sensitive attribute \(A\) to output \(Y\). Further, we define \(X_{_{_{A}}^{c}}\) as the features that are not present in any unfair path in \(_{_{A}}\) and \(X_{_{_{A}}}\) as the features along the unfair paths. Path-dependent Counterfactual Fairness is defined as follows.

**Definition 4** (Path-dependent Counterfactual Fairness (PCF) ).: _We say \(=g_{w}(R)\) satisfies PCF with respect to path set \(_{_{A}}\) if the following holds for every \((x,a)\): \( y,a^{}\)_

\[\{_{A a,X_{_{_{A}}^{c}}  x_{_{_{A}}^{c}}}(U)=y|X=x,A=a\}=\{_{A  a^{},X_{_{_{A}}^{c}} x_{ _{_{A}}^{c}}}(U)=y|X=x,A=a\}\]

This notion suggests that if we fix attributes \(_{_{_{A}}^{c}}\) and let \(U\) follows posterior distribution \(_{}\{U|X=x,A=a\}\), then variable \(A\) should not affect the predictor along unfair path(s) in \(_{_{A}}\). Note that PCF reduces to CF if \(X_{_{_{A}}^{c}}=\). We want to emphasize that path-_specific_ counterfactual fairness defined in  is different from Definition 4. Path-_specific_ counterfactual fairness  considers a baseline value \(a^{}\) for \(A\), and requires \(A=a^{}\) propagate through unfair paths, and the true value of \(A\) propagates through other paths. In contrast, in path-_dependent_ CF, there is no baseline value for \(A\), and \(A\) should not cause \(Y\) through unfair paths.

In this section, we describe how our proposed method could be adapted to the path-dependent case. Before describing the algorithm formally, we present an example.

**Example 2** (**Representation for PCF)**.: _Consider a causal graph shown in Figure 1. In this graph, there are two directed paths from \(A\) to \(Y\). We assume that \(A\) is binary, and \(_{_{A}}=\{(A X_{2} Y)\}\),_and \(X_{^{c}_{_{A}}}=\{X_{1}\}\). Based on Definition 4, given sample \((x,a)\), after intervention \(X_{1}=x_{1}\) (Figure 2), intervention on \(A\) should not affect the prediction outcome. We generate a representation in the following steps, 1) we find distribution \(_{}\{U|X=x,A=a\}\) and sample \(u\) from this distribution. 2) Using the graph in Figure 2, we generate counterfactual value for \(X_{2}\). That is, for a given structural equation \(X_{2}=f_{2}(A,X_{1},U)\), we generate counterfactual value \(_{2}=f_{2}(a^{},x_{1},u)\), where \(a^{} a\). 3) We generate representation \(R=[x_{1},s(x_{2},_{2}),u]\), where \(s\) is a symmetric function._

Based on the above example, the representation should be generated based on causal graph \(\) after intervention \(X_{^{c}_{_{A}}}=x_{^{c}_{_{A}}}\). The detailed representation generation procedure under PCF is stated in Algorithm 2. Let \(h(x,a;,s,X_{^{c}_{_{A}}})\) be the function that generates such a presentation using Algorithm 2. We have the following theorem for \(h(x,a;,s,X_{^{c}_{_{A}}})\).

**Theorem 2**.: _Assume \(R=h(x,a;,s,X_{^{c}_{_{A}}})\) is representation generated based on Algorithm 2. Then the predictor \(g_{w}(h(x,a;,s,X_{^{c}_{_{A}}}))\) satisfies perfect PCF for all \(w^{d_{w}}\)._

The above theorem implies if we train a predictor using \(\{r^{(i)}=h(x^{(i)},a^{(i)};,s,X_{^{c}_{_{A}}} ),y^{(i)}\}_{i=1}^{n}\), and we use \(r=h(x,a;,s,X_{^{c}_{_{A}}})\) at the time of inference, then PCF is satisfied by the predictor.

## 5 Experiment

Datasets and Causal Models.We use the Law School Success dataset  and the UCI Adult Income Dataset  to evaluate our proposed method. The Law School Success dataset consists of 21,790 students across 163 law schools in the United States. It includes five attributes for each student: entrance exam score (LSAT), grade-point average (GPA), first-year grade (FYA), race, and gender. In our experiment, gender is the sensitive attribute \(A\), and the goal is to predict FYA (label \(Y\)) using LSAT, GPA, Race (three features \(X\)), and the sensitive attribute \(A\).

The UCI Adult Income Dataset contains 65,123 data instances, each with 14 attributes: age, work class, education, marital status, occupation, relationship, race, sex, hours per week, native country, and income. In our experiments, we consider sex as the sensitive attribute \(A\), whether income is greater than \(\$0K\) or not as the target variable \(Y\), and all other attributes as features \(X\).

We evaluated our methods using two distinct causal graphs. The first graph is the one presented in CVAE paper  (see Figure 3). We divided the feature attributes into two subsets, \(X_{}\) and \(X_{}\). \(X_{}\) comprises attributes that are not causally affected by \(A\), while \(X_{}\) includes the remaining attributes. Exogenous variable \(U\) is defined as the latent influencing factors. For the Law School Success dataset, \(X_{}\) consists of {Race} and \(X_{}\) includes {LSAT, GPA}. For the UCI Adult Income dataset, similar to , we assume that \(X_{}\) contains {Age, Race, Native Country} and \(X_{}\) includes {Workclass, Education, Marital Status, Occupation, Relationship, Hours per Week}.

The second graph is proposed by DCEVAE paper  (see Figure 4). The main assumption in  is that the exogenous variables controlling \(X_{}\) and \(X_{}\) can be disentangled into \(U_{}\) and \(U_{}\). We used the same sets of \(X_{}\) and \(X_{}\) as in the CVAE graph.

Figure 3: CVAE causal graph Figure 4: DCEVAE causal graph

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

fair predictor under counterfactual fairness. However, we do not claim CF is the only right fairness notion. Depending on the application, counterfactual fairness may or may not be the right choice for counterfactual fairness. **Limitation:** As stated in , to find a predictor under CF, causal model \(\) should be known to the fair learning algorithm. Finding a causal model is challenging since there can be several causal models consistent with observational data . This limitation exists for our algorithm and baselines.

## 6 Conclusion

We proposed a novel method to train a predictor under counterfactual fairness. Unlike , which shows that a sufficient condition for satisfying CF is to not use the features that are descendants of the sensitive attribute, our algorithm uses all the available features leading to better performance. The proposed algorithm generates a representation for training that guarantees CF and improves performance compared to the baselines. We also showed that our algorithm can be extended to path-dependent counterfactual fairness.

  Method & Acc (W) & TE (W) & Acc (E) & TE (E) \\  UF & **0.8165 \(\) 0.0037** & 0.2285 \(\) 0.0037 & **0.8165 \(\) 0.0037** & 0.2065 \(\) 0.0036 \\ CA & 0.8063 \(\) 0.0030 & 0.1871 \(\) 0.0030 & 0.8039 \(\) 0.0028 & 0.1570 \(\) 0.0096 \\ ICA & 0.8141 \(\) 0.0039 & 0.2087 \(\) 0.0041 & 0.8143 \(\) 0.0035 & 0.1766 \(\) 0.0042 \\ CE & 0.7858 \(\) 0.0028 & – & 0.7858 \(\) 0.0028 & – \\ CR & 0.7911 \(\) 0.0020 & 0.0824 \(\) 0.0028 & 0.7968 \(\) 0.0027 & 0.1026 \(\) 0.0021 \\ Ours & 0.7932 \(\) 0.0030 & **0.0164 \(\) 0.0014** & 0.7939 \(\) 0.0038 & **0.0159 \(\) 0.0016** \\  

Table 6: Logistic regression classifier on UCI Adult dataset with DCEVAE causal model

  Method & MSE & TE & \(_{1}\) & \(_{2}\) \\  UF & **0.8677 \(\) 0.0043** & 0.1344 \(\) 0.0056 & 0.1345 \(\) 0.0064 & 0.1343 \(\) 0.0054 \\ CA & 0.8783 \(\) 0.0084 & 0.1759 \(\) 0.0368 & 0.1767 \(\) 0.0384 & 0.1753 \(\) 0.0357 \\ ICA & 0.8693 \(\) 0.0045 & 0.1459 \(\) 0.0169 & 0.1466 \(\) 0.0182 & 0.1454 \(\) 0.0161 \\ CE & 0.8781\(\) 0.0068 & – & – & – \\ CR & 0.8703 \(\) 0.0055 & **0.0989 \(\) 0.0058** & **0.1000 \(\) 0.0070** & **0.098 \(\) 0.0052** \\ Ours & 0.8687 \(\) 0.0045 & **0.1076 \(\) 0.0039** & **0.1086 \(\) 0.0049** & **0.1068 \(\) 0.0032** \\  

Table 4: Linear regression Results on Law School Success dataset with DCEVAE causal model

  Method & MSE & TE & \(_{1}\) & \(_{2}\) \\  UF & **0.8664 \(\) 0.0040** & 0.1346 \(\) 0.0036 & 0.1345 \(\) 0.0028 & 0.1347 \(\) 0.0057 \\ CA & 0.8893 \(\) 0.0096 & 0.2335 \(\) 0.0125 & 0.2301 \(\) 0.0092 & 0.2362 \(\) 0.0016 \\ ICA & 0.8679 \(\) 0.0064 & 0.1608 \(\) 0.0058 & 0.1594 \(\) 0.0046 & 0.1619 \(\) 0.0078 \\ CE & 0.8900 \(\) 0.0054 & – & – & – \\ CR & 0.8622 \(\) 0.0148 & 0.1035 \(\) 0.0027 & 0.1032 \(\) 0.0024 & 0.1038 \(\) 0.0039 \\ Ours & 0.8692 \(\) 0.0060 & **0.0661 \(\) 0.0019** & **0.0654 \(\) 0.0023** & **0.0667 \(\) 0.0016** \\  

Table 3: Linear regression results on Law School Success dataset with CVAE causal model