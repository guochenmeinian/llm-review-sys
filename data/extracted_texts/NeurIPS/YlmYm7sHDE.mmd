# Minimum Entropy Coupling with Bottleneck

M.Reza Ebrahimi

University of Toronto

mr.ebrahimi@mail.utoronto.ca &Jun Chen

McMaster University

chenjun@mcmaster.ca &Ashish Khisti

University of Toronto

akhisti@ece.utoronto.ca

###### Abstract

This paper investigates a novel lossy compression framework operating under logarithmic loss, designed to handle situations where the reconstruction distribution diverges from the source distribution. This framework is especially relevant for applications that require joint compression and retrieval, and in scenarios involving distributional shifts due to processing. We show that the proposed formulation extends the classical minimum entropy coupling framework by integrating a bottleneck, allowing for a controlled degree of stochasticity in the coupling. We explore the decomposition of the Minimum Entropy Coupling with Bottleneck (MEC-B) into two distinct optimization problems: Entropy-Bounded Information Maximization (EBIM) for the encoder, and Minimum Entropy Coupling (MEC) for the decoder. Through extensive analysis, we provide a greedy algorithm for EBIM with guaranteed performance, and characterize the optimal solution near functional mappings, yielding significant theoretical insights into the structural complexity of this problem. Furthermore, we illustrate the practical application of MEC-B through experiments in Markov Coding Games (MCGs) under rate limits. These games simulate a communication scenario within a Markov Decision Process, where an agent must transmit a compressed message from a sender to a receiver through its actions. Our experiments highlight the trade-offs between MDP rewards and receiver accuracy across various compression rates, showcasing the efficacy of our method compared to conventional compression baseline.

## 1 Introduction

Consider the following Markov Chain modeling a general lossy compression framework \(X}T}Y\), where the input \(X\) with a marginal distribution \(p_{X}\), is encoded by the probabilistic encoder \(p\) to generate the code \(T\). Subsequently, the probabilistic decoder \(q\) reconstructs \(Y\) from \(T\). The objective is to identify the encoder and decoder that minimize the distortion between \(X\) and \(Y\), subject to an upper bound constraint on the expected code length \(H(T) R\).

It is common to measure the sample-wise distortion via direct comparison of \((x,y)\) pairs through a distortion function \(d(,)\), and consider the expectation \([d(X,Y)]\) as a measure of average distortion. Instead, we propose using the logarithmic loss (log-loss) \(H(X|Y)\), or equivalently \(I(X;Y)\), as an alternative metric to enforce the distortion constraint. The log-loss distortion measure, commonly employed in learning theory, was first explored within rate-distortion theory by Courtade and Wesel  and Courtade and Weissman . This measure is particularly suitable in scenarios where reconstructions can be soft, meaning that the decoder produces a distribution rather than a distorted

[MISSING_PAGE_EMPTY:2]

**Lemma 1**.: _Given a Markov chain \(X T Y\):_

\[I(X;Y)=I(X;T)+I(Y;T)-I(T;X,Y).\] (5)

The proof follows multiple applications of the chain rule for mutual information. The following lower bound on the MEC-B objective is attainable based on Lemma 1:

\[I(X;Y) I(X;T)+I(Y;T)-R\,.\] (6)

In this work, we consider maximizing the lower bound (6) as a proxy to the main objective. This allows a decomposition of the encoder and decoder for the MEC-B formulation in (2):

1. **Encoder Optimization:** The encoder is first optimized separately, according to Entropy-Bounded Information Maximization in (4), \(_{}(p_{X},R)\), resulting in the marginal distribution \(_{T}\) on the code \(T\).
2. **Decoder Optimization:** The decoder is then optimized by solving a minimum entropy coupling in (3) between the code and output marginals, \(_{}(_{T},p_{Y})\).

Therefore, in terms of problems (2), (3), and (4):

\[_{}(p_{X},p_{Y},R) =_{p_{T|X},\ q_{Y|T}}\ I(X;Y)\] (7) \[_{p_{T|X},\ q_{Y|T}}\ (I(X;T)+I(Y;T)-R)\] (8) \[_{}(p_{X},R)+_{}( _{T},p_{Y})-R\,.\] (9)

In this paper, we address the Entropy-Bounded Information Maximization problem in Section 3, providing theoretical insights into the solution structure across the entire spectrum of rate limits. We establish an upper bound on the objective and demonstrate that only deterministic mappings can achieve this bound. Then, in Section 3.1, we introduce a greedy algorithm designed to identify deterministic mappings with a guaranteed input-dependent gap from the optimal solution. Subsequently, in Section 3.2, we describe a method to identify optimal mappings near any deterministic mapping, effectively bridging the gap between discrete deterministic mappings and providing deeper theoretical insights into the problem structure.

Following this theoretical groundwork, Section 4 applies the MEC-B framework to extend Markov Coding Games (MCG) with communication bottlenecks between the source and the agent. Experimental results for MCGs with rate limits are detailed in Section 4.2, showcasing the practical implications of our theoretical developments. The appendix sections complement these discussions by including formal proofs for all theorems and lemmas, a concise overview of the original minimum entropy coupling problem, and additional experimental results.

## 2 Related Work

**Couplings and Minimum Entropy Coupling** A fundamental problem in probability theory, known as coupling, concerns determining the _optimal_ joint distribution of random variables given their marginal distributions. This problem has a long history, with early examples by Frechet  seeking the joint distribution that maximizes correlation subject to marginal constraints. References [9; 10; 11; 12] provide a broader treatment of these problem classes and their applications. Notably, optimal transport (OT) emerges as a significant class within this framework, where optimality is defined as minimizing the expected value of a loss function over the joint distribution. See  for an in-depth treatment of the optimal transport problem.

The minimum entropy coupling (MEC) focuses on finding the joint distribution with the smallest entropy given the marginal distribution of some random variables. This problem has been first studied in [4; 5; 6; 7], among others. While it is shown by Vidyasagar , Kovacevic et al.  that MEC is NP-Hard, the literature contains many approximation algorithms for this problem. One of the earliest greedy algorithms for MEC was introduced by Kocaoglu et al.  in the context of causal inference, achieving a local minimum with a gap of \(1+ n\) bits from the optimum, where \(n\) represents the size of the alphabet. This bound was further improved in subsequent works [15; 16].

Based on tools from the theory of majorization , Cicalese et al.  developed a new greedy algorithm producing solutions 1 bit away from the optimal. Subsequent improvements by Li  enabled the construction of a coupling whose entropy is within 2 bits of the optimal value, regardless of the number of random variables involved. Despite these advances, Compton  identified a _majorization barrier_ that limits further improvements, while Compton et al.  introduced the profile method offering stronger lower bounds for the coupling entropy.

Minimum entropy coupling finds innovative applications beyond causal inference [14; 20; 21]. For instance, Sokota et al.  utilized it in Markov coding games to enable reinforcement learning agents to communicate via Markov decision process trajectories. This application showcased MEC's utility in enabling efficient information transmission through constrained environments like video game interactions. Similarly, de Witt et al.  applied MEC to securely encode secret information in regular text, showing MEC corresponds to the maximally efficient secure procedure.

**Lossy Source Coding** While log-loss is widely used in prediction and learning, its application as a distortion measure in the context of source coding has been less explored, with the earliest examples appearing in  and . Log-loss is particularly suited as a distortion measure in soft reconstructions, meaning the decoder outputs a distribution. Shkel and Verdu  explored a single-shot lossy source coding setting under logarithmic-loss, using a straightforward encoding scheme. Unlike the EBIM formulation in (4) which imposes a direct entropy constraint on the code, this approach constrained the code by the cardinality of its support.

Finally, Blau and Michaeli  introduced the Rate-Distortion-Perception (RDP) tradeoff in lossy compression. The RDP framework does not fix the output distribution; instead, it imposes a softer perceptual constraint on the generated outputs. Additionally, our work incorporates an entropy constraint on \(T\) as a rate bottleneck, while in the RDP formulation, \(I(X;Y)\) can be interpreted as the rate bottleneck. In this line of research, the work of Liu et al.  is closest in spirit to our approach, as the authors studied a lossy compression setting with different source and reconstruction distributions. They demonstrated that their setting could be formulated as a generalization of optimal transport with an entropy bottleneck. However, they used mean squared error (MSE) as the distortion metric, while we consider log-loss. Therefore, the mathematical machinery required for our analysis differs significantly from prior work.

## 3 Entropy-Bounded Information Maximization

Consider a discrete random variable \(X\) defined over the alphabet \(=\{1,,n\}\) with a given marginal probability distribution \(p_{X}\). The following problem aims to establish a maximal information coupling between \(X\) and another random variable \(T\), defined over the alphabet \(=\{1,,m\}\), where the entropy of \(T\) is constrained to be no more than \(R\) bits. Unlike minimum entropy coupling, the marginal distribution of the second random variable \(T\) is not predetermined; the only constraint on \(T\) is its entropy.

\[_{}(p_{X},R)=_{p_{XT}}I(X;T),\] (10)

where set \(\) consists of all joint distributions \(p_{XT}\) that satisfy the following conditions:

1. \(_{t}p_{XT}(x,t)=p_{X}(x)\), ensuring that the marginal distribution of \(X\) is preserved.
2. \(H(T) R H(X)\), ensuring the entropy of \(T\) is constrained to be no more than \(R\).

We call this problem Entropy-Bounded Information Maximization (EBIM). Note that the objective in (10) is upper-bounded by \(R\), since:

\[_{}(p_{X},R) =_{p_{XT}}I(X;T)\] \[_{p_{XT}}H(T) R.\] (11)

The following theorem establishes that only deterministic couplings can achieve this upper-bound.

**Theorem 1**.: \(_{}(p_{X},R)=R\) _if and only if there exists a function \(g:\) such that \(H(g(X))=R\)._

The formal proof is presented in Section A.2. Note that the mutual information \(I(X;T)\) is invariant to permutations on \(\). Specifically, for any permutation \(:\), we have \(I(X;T)=I(X,(T))\)Given that problem (10) only constrains the entropy \(H(T)\), the objective is indifferent to such permutations. Let us define the permutation group of a joint distribution \(p_{XT}\) as:

\[(p_{XT})=\{P P(x,(t))=p_{XT}(x,t),\ \ : \}.\] (12)

**Remark 1**.: _Each partition of \(\) is associated with a permutation group of a deterministic mapping. Consequently, the total number of potential deterministic mapping groups, independent of the entropy constraint on \(T\), will be the total number of feasible partitions of \(\). The total number of ways to partition a set of size \(n\) corresponds to the \(n\)-th Bell number, symbolized by \(B_{n}\). The growth rate of the Bell numbers is \((n^{n})\), rendering brute force iteration of all deterministic mappings infeasible._

In Figure 2 (left), a brute force method is applied to solve EBIM (10) for an input alphabet of size three. As observed, there are five potential partitions on \(\), each corresponding to a point where \(_{}(p_{X},R)=R\). Given the impracticality of brute force search for large alphabet sizes, in Section 3.1, we introduce a greedy search algorithm to identify deterministic mappings with a guaranteed performance gap from the optimal. Following this, in Section 3.2, we explore optimal mappings close to these deterministic mappings, providing a strategy to narrow the gap between the identified deterministic mappings.

### Proposed Search Algorithm for Deterministic Mappings

Since iterating over all deterministic mappings is not feasible, one should look for carefully constructed search algorithms to find such mappings with resulting \(H(T)\) as close as possible to \(R\). Without the loss of generality, suppose \(p_{X}=[p_{1},\ p_{2},\ ,\ p_{n}]\) is arranged in a decreasing order. Algorithm 1 presents a search approach for discovering a deterministic mapping \(T=g(X)\), resulting in \(I(X;T)\) that is at most \(h(p_{2})\) bits away from the optimal \(_{}(p_{X},R)\), where \(h()\) is the binary entropy function: \(h(p)=-p(p)-(1-p)(1-p)\).

```
1:\(p_{X},R\)
2:\(p_{XT}(p_{X})\)
3:if\(R H(X)\)then
4:return\(p_{XT}\)
5:for\(i 1\) to \(|p_{X}|-1\)do
6:\(p_{i}^{(i)}\) Merge the two columns with the smallest sum in \(p_{XT}\).
7:\(I_{i}^{(i)}\) Mutual Information imposed by \(p_{s}^{(i)}\).
8:\(p_{i}^{(i)}\) Merge the two columns with the largest sum in \(p_{XT}\).
9:\(I_{i}^{(i)}\) Mutual Information imposed by \(p_{l}^{(i)}\).
10:if\(I_{s}^{(i)} R\)then
11:return\(p_{i}^{(i)}\)
12:elseif\(I_{l}^{(i)} R<I_{s}^{(i)}\)then
13:return\(p_{l}^{(i)}\)
14:else
15:\(p_{XT} p_{l}^{(i)}\) ```

**Algorithm 1** Deterministic EBIM Solver

The deterministic EBIM solver in Algorithm 1 has \((n n)\) time complexity, where \(n\) is the cardinality of the input alphabet. This is because the main loop of the algorithm runs for at most \(n\) steps (as at each step we combine two elements of the input distribution) and finding min/max elements can be done in \(( n)\) using a heap data structure. Also, the mutual information calculation at each step can be done in constant time by only calculating the decrease in entropy after combining two elements of the distribution.

**Theorem 2**.: _If the output of Algorithm 1 yields mutual information \(\), then_

\[_{}(p_{X},R)- h(p_{2}),\] (13)

_where \(h()\) is the binary entropy function, and \(p_{2}\) denotes the second largest element of \(p_{X}\)._Let \(n=|p_{X}|\). The procedure outlined in Algorithm 1 establishes a series of deterministic mappings \(p_{s}^{(1)},\ p_{l}^{(1)},\ ,\ p_{s}^{(n-1)},\ p_{l}^{(n-1)}\), corresponding to a decreasing sequence of mutual information values \(I_{s}^{(1)},\ I_{l}^{(1)},\ ,\ I_{s}^{(n-1)},\ I_{l}^{(n-1)}\). The algorithm then picks the mapping with the highest mutual information that does not exceed \(R\). The proof involves establishing an upper bound on the gap between these successive mutual information values. A formal proof is presented in Section A.3.

It is important to highlight that the gap described in Theorem 2 is bounded by one bit, i.e., \(h(p_{2}) 1\), with equality achieved when \(p_{X}=[0.5,0.5]\). Although the gap is capped at one bit, the maximal mutual information in the EBIM formulation scales with \(R\). Thus, the most natural interpretation of this gap emerges in higher rate regimes. Furthermore, the gap described in Theorem 2 remains small when \(p_{2}\) is small, specifically in cases where the input alphabet size is large and the distribution is not heavily skewed toward a few elements.

### Optimal Coupling Around Deterministic Mappings

Section 3.1 introduced a greedy search algorithm designed to identify deterministic mappings with a guaranteed and input-dependent gap from the optimal. In this section, we find the optimal couplings close to any deterministic mapping. This method allows us to close the gap between the mappings identified by Algorithm 1, as will be demonstrated later.

**Theorem 3**.: _Let \(p_{XT}\) denoted by a \(||||\) matrix, defines a deterministic mapping \(T=g(X)\), with \(I(X;T)=H(T)=R_{g}\). We have \(_{}(p_{X},R_{g})=R_{g}\), and for small enough \(>0\):_

1. \(_{}(p_{X},R_{g}+)\) _is attained as follows:_ _Normalize the columns by dividing each column by its sum. Then, select the cell with the smallest normalized value and move an infinitesimal probability mass from this cell to a new column of_ \(p_{XT}\) _in the same row._
2. \(_{}(p_{X},R_{g}-)\) _is achieved as follows:_ _Identify the columns with the smallest and largest sums in_ \(p_{XT}\)_. Select the cell with the smallest value in the column with the lowest sum. Transfer an infinitesimal probability mass from this cell to the column with the highest sum in the same row._

Figure 1 on the right depicts an example of optimal solutions in the neighborhood of a deterministic mapping. While Algorithm 1 effectively identifies deterministic mappings that produce mutual information close to the budget \(R\), Theorem 3 can help bridge the remaining gap. More specifically, one can begin with a deterministic mapping and use two probability mass transformations outlined in Theorem 3 to navigate across the \(I\)-\(R\) plane.

Figure 2 illustrates this strategy; for \(p_{X}=[0.7,\ 0.2,\ 0.1]\), identifying all 5 possible deterministic mappings is straightforward. Applying the transformations from Theorem 3 then yields various solutions across the \(I\)-\(R\) plane (represented by dashed lines). Subsequently, one can select the solution that maximizes mutual information for any given value of \(R\) (highlighted with a thick solid line), thus producing a comprehensive solution for every value of \(R\). As demonstrated in Figure 2, this strategy recovers the optimal solutions, as determined by brute force, for the simple case of an input alphabet of three. However, while effective, the optimality of this approach remains a conjecture.

## 4 Application: Markov Coding Game with Rate Limit

Markov Coding Games (MCGs), as introduced by Sokota et al. , represent a specialized type of multi-player decentralized Markov Decision Processes (MDPs) involving several key components: a source, an agent (sender), a Markov decision process, and a receiver. An MCG episode unfolds in three stages: initially, the agent receives a private message from the source, which it must then indirectly convey to the receiver. Next, the agent participates in an episode of the Markov decision

Figure 1: An example for Theorem 3.

process. Finally, the receiver attempts to decode the original message based on the observed MDP trajectory. The overall reward is a combination of the MDP payoff and the accuracy with which the receiver decodes the message. MCGs are particularly interesting due to their ability to generalize frameworks like referential games  and source coding .

We will consider a natural extension to Markov Coding Games, where the link from the source to the agent is rate-limited. This means, contrary to the original setting of Sokota et al. , the agent does not fully observe the message at each MDP round, but will receive a compressed version of the message iteratively, and in turn, encodes information about the message in the MDP trajectory for the receiver.

Following Sokota et al. , we define a rate-limited MCG as a tuple \((,,,),,, ,R\), where \((,,,)\) is an MDP denoted by state and action spaces, and reward and transition functions, respectively. \(\) is a set of messages, \(\) is the prior distribution over messages \(\), \(\) is a non-negative real number we call the message priority, and finally, \(R\) is the communication rate limit between the source and the agent. An MCG episode proceeds in the following steps:

1. Message \(M\) is sampled from the prior over messages at the source.
2. Based on the selected message \(M\) and the history of the MDP episode, the source generates and transmits signal \(T\) to the agent, adhering to the rate limit \(R\).
3. The Agent uses a conditional policy \(_{ T}\), which takes current state \(s\) and received signal \(T\) as input and outputs distributions over MDP actions \(\), to generate the next action \(a\).
4. After repeating steps 2 and 3, the agent's terminal MDP trajectory \(Z\) is given to the receiver as an observation.
5. The receiver uses the terminal MDP trajectory \(Z\) to output a distribution over messages \(\), estimating the decoded message \(\).

The objective of the agents is to maximize the expected weighted sum of the MDP reward and the accuracy of the receiver's estimate \([(Z)+[M=]]\). Optionally, If a suitable distance function exists, instead, the objective can also be adjusted to minimize the difference between the actual message and the guess. A diagram of the structure MCG with rate limit is shown in Figure 3.

Figure 3: The structure of a Markov Coding Game with Rate Limit.

Figure 2: Solutions to the EBIM problem for \(p_{X}=[0.7,0.2,0.1]\). **Left:** brute force solution. **Right:** application of the transformations from Theorem 3 to each deterministic mapping (dashed lines) and selection of solutions with maximal mutual information for each \(R\) value (thick solid line). This strategy effectively recovers optimal solutions, aligning with those found by brute force in this case.

### Method Description

**Marginal Policy** Following Sokota et al. , before execution we first derive a marginal policy \(\) for the MDP, based on the Maximum-Entropy reinforcement learning objective:

\[_{}_{}[_{t}R(S_{t},A_{t})+ H(A_{t}|S_{t}) ].\] (14)

The value of \(\) in (14) needs to be determined in accordance with the message priority \(\) of the MCG. Note that this marginal policy does not depend on the choice of message. By introducing stochasticity into this policy, we can encode information about the message into the selection of actions at each step during runtime. For more details, see Section C.1.

**Step 1 - Source: Message Compression** At the beginning of each round, given the updated message belief \(p_{M}\), the source compresses the message to generate the signal \(T\), adhering to the source-agent rate limit \(R\), by solving EBIM in (10). The source then transmits the signal \(T\) to the agent. Subsequently, after observing the action taken by the agent, the source updates the message belief for the next round. Algorithm 2 outlines the steps taken by the source.

**Step 2 - Agent: Minimum Entropy Coupling** As illustrated in Algorithm 3, at each round, upon receiving the signal \(T\), the agent constructs a conditional policy \(_{|T}\) by performing minimum entropy coupling between the action distribution from the marginal policy \((s)\) with the signal distribution \(p_{T}\). Subsequently, the next action is sampled from the conditional policy, \(a_{|T}\). Finally, the agent updates the message belief based on the chosen action.

```
1:Input:\(\), \(\), \(R\), \(s^{0}\)
2:Observe message \(m\)
3:Initialize message belief \(p_{M}\)
4:Initialize state \(s s^{0}\)
5:while Source's turn do
6:\(p_{MT}(p_{M},R)\)
7:\(t p_{T|M}(m)\)
8:Send\(t\) to the agent.
9:\(p_{T}_{m^{}}p_{MT}(m^{},)\)
10:\(p_{TA}(p_{T},(s))\)
11:\(p_{MA}_{t^{}}p_{MT}(,t^{})\,p_{A|T}(t^{})\)
12:\(a,s\) action and next state
13:\(p_{M} p_{M|A}(a)\) ```

**Algorithm 2** Source

**Receiver: Decoding the Message** Given the agent's final MDP trajectory, the receiver mirrors the actions of the source and agent to update the message belief at each step. As outlined in Algorithm 4, the process begins with the receiver compressing the message based on the current message belief. This is followed by performing minimum entropy coupling between the marginal policy and the distribution of the compressed message. The final message belief is used to estimate the decoded message.

```
1:Input:\(\), \(\), \(R\), \(s^{0}\)
2:Initialize message belief \(p_{M}\)
3:Initialize state \(s s^{0}\)
4:while Agent's turn do
5:\(p_{MT}(p_{M},R)\)
6:\(p_{TA}(p_{T},(s))\)
7:\(p_{TA}(p_{T},(s))\)
8:\(p_{MA}_{t^{}}p_{MT}(,t^{})\,p_{A|T}(t^{})\)
9:\(p_{M} p_{M|A}(a)\) ```

**Algorithm 3** Agent

### Experimental Results

This section presents the experimental results of the method described in Section 4.1, applied to Markov Coding Games. For our experiments, we utilize a noisy _Grid World_ environment for the Markov Decision Process. Section C.2 provides more detail on the environment setup used in this experiment.

The marginal policy is learned through Soft Q-Value iteration, as described in Algorithm 8. By increasing the value of \(\) in Equation (14), we induce more randomness into the marginal policy. Consequently, higher values of \(\) lead to an increase in the total number of steps taken by the agent to reach the goal, resulting in a more heavily discounted reward. Conversely, as the entropy of actions ateach state is increased, there is an increase in the mutual information between the actions and the compressed message during the minimum entropy coupling at each step. This dynamic establishes a fundamental trade-off between the MDP reward and the receiver's decoding accuracy, through the adjustment of \(\). Figure 8 shows policies learned by high and low values of \(\).

We compare our proposed compression method in Algorithm 1 with a baseline of uniform quantization. As detailed in Algorithm 5, given an entropy budget \(R\), the input symbols are uniformly partitioned into \( 2^{R}\) bins, and each bin is encoded with the same code.

Figure 4 illustrates the trade-off between the average MDP reward and the receiver's decoding accuracy by varying \(\), using our deterministic EBIM solver in Algorithm 1, and the uniform quantization encoder in Algorithm 5. Here, the compression rate is defined by the ratio of the message entropy to the allowed code budget \(H(T)\).

Figure 5 illustrates the evolution of message belief over time for various values of \(\) and rate budgets. A marginal policy optimized with a higher \(\) prioritizes message accuracy over MDP payoff, as higher entropy of actions at each state provides more room for the agent to encode information about the message. Consequently, as observed, this leads to improved receiver accuracy in fewer steps. In addition, a lower compression rate permits the agent to retain more information about the message, enabling more effective encoding of information in the selected trajectory.

Figure 4: The trade-off between average MDP reward vs. receiverâ€™s accuracy, navigated by varying the value of \(\). Left: using our search algorithm for compression (Algorithm 1), Right: using uniform quantization in Algorithm 5. The message size is 512 with a uniform prior, and each data point is averaged over 200 episodes.

## 5 Conclusion

We investigated a lossy compression framework under logarithmic loss, where the reconstruction distribution differs from the source distribution. This framework supports joint compression and retrieval applications, or more generally, cases where distributional shifts occur due to processing. We demonstrated that this framework effectively extends the classical minimum entropy coupling by incorporating a bottleneck, which regulates the degree of stochasticity in the coupling.

Furthermore, we showed that separately optimizing the encoder and decoder decomposes the Minimum Entropy Coupling with Bottleneck (MEC-B) into two distinct problems: Entropy-Bounded Information Maximization (EBIM) for the encoder, followed by Minimum Entropy Coupling (MEC) for the decoder. We conducted an extensive study of the EBIM problem, provided a functional mapping search algorithm with guaranteed performance, and characterized the optimal solution adjacent to functional mappings, offering valuable theoretical insights into the problem structure. To illustrate an application of MEC-B, we presented experiments on Markov Coding Games (MCGs) with rate limits. The results demonstrated the trade-off between MDP reward and receiver accuracy, with varying compression rates, compared to baseline compression schemes.

Future research could focus on quantifying the gap between the separate optimization of the encoder and decoder and the optimal joint setting. Also, enabling fine-grained control over the entropy spread in the coupling can be key in some applications. Additionally, the application of Entropy-Bounded Information Maximization (EBIM) in watermarking language models  suggests a valuable intersection with state-of-the-art AI applications. Moreover, extending this framework to continuous cases could lead to the design of neural network architectures based on the proposed framework and provide information-theoretic insights into a broad spectrum of deep learning problems. These include unpaired sample-to-sample translation , joint compression and upscaling , and the InfoMax framework , among others.

Figure 5: Evolution of message belief over time, for various values of \(\) and rate budget, using our search algorithm for compression in Algorithm 1 vs. uniform quantization in Algorithm 5.