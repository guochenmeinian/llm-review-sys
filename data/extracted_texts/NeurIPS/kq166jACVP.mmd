# _Aligner_: Efficient Alignment by Learning to Correct

Jiaming Ji\({}^{*,1,2}\) Boyuan Chen\({}^{*,1,2}\) Hantao Lou\({}^{1,2}\) Donghai Hong\({}^{1,2}\)

Borong Zhang\({}^{1}\) Xuehai Pan\({}^{1}\) Juntao Dai\({}^{1}\) Tianyi Qiu\({}^{1}\) Yaodong Yang\({}^{}\)

\({}^{1}\)Institute for AI, Peking University

\({}^{2}\)State Key Laboratory of General Artificial Intelligence, Institute for AI, Peking University

{jiamg.ji,cbyll1,lht_pku,donghai.hong}@stu.pku.edu.cn

yaodong.yang@pku.edu.cn

Equal contribution, Corresponding author. Project Website: https://pku-aligner.github.io.

###### Abstract

With the rapid development of large language models (LLMs) and ever-evolving practical requirements, finding an efficient and effective alignment method has never been more critical. However, the tension between the complexity of current alignment methods and the need for rapid iteration in deployment scenarios necessitates the development of a model-agnostic alignment approach that can operate under these constraints. In this paper, we introduce _Aligner_, a novel and simple alignment paradigm that learns the correctional residuals between preferred and dispreferred answers using a small model. Designed as a model-agnostic, plug-and-play module, _Aligner_ can be directly applied to various open-source and API-based models with only one-off training, making it suitable for rapid iteration. Notably, _Aligner_ can be applied to any powerful, large-scale upstream models. Moreover, it can even iteratively bootstrap the upstream models using corrected responses as synthetic human preference data, breaking through the model's performance ceiling. Our experiments demonstrate performance improvements by deploying the same _Aligner_ model across 11 different LLMs, evaluated on the 3H dimensions (helpfulness, harmlessness, and honesty). Specifically, _Aligner_-7B has achieved an average improvement of 68.9% in helpfulness and 22.8% in harmlessness across the tested LLMs while also effectively reducing hallucination. In the Alpaca-Eval leaderboard, stacking _Aligner_-2B on GPT-4 Turbo improved its LC Win Rate from 55.0% to 58.3%, surpassing GPT-4 Omni's 57.5% Win Rate (community report).

## 1 Introduction

The alignment of LLMs with human intentions and values has recently gained significant attention . Among the various methods, supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF)  have emerged as practical approaches. SFT leverages human demonstrations to fine-tune LLMs and instruct the model on desired actions, whereas RLHF trains a reward model (RM) based on human preferences and fine-tunes LLMs using feedback signals from the RM through reinforcement learning (RL) methods .

Despite the effectiveness of these methods  in meeting 3H (helpfulness, harmlessness, and honesty) standards , they suffer from challenges such as high training resource consumption and difficulty in ensuring consistent performance . Meanwhile, in real-world scenarios, alignment requirements are dynamically changing . Models may encounter cases outside of alignment training and exhibit undesirable behaviors, which are difficult to address immediately using time-consuming methods such as SFT and RLHF.

#### Can we develop an efficient, lightweight, and model-agnostic alignment method?

Inspired by residual learning , we simplify the alignment process by focusing on copy and correction operations. We introduce an efficient alignment paradigm, the _Aligner_, without involving any RL processes, as shown in Figure 1. Specifically, _Aligner_ is fine-tuned on a preference dataset to learn the correctional residuals between preferred and non-preferred responses and then stacked on the upstream model to achieve corrected alignment. Here, the upstream LLM refers to models targeted for alignment and is compared to the source model in the RLHF process. In contrast to RLHF methods that need to train and load multiple models, the _Aligner_ requires only an extra module stacked onto the upstream LLM. Moreover, our method's computational resource demand depends solely on the desired efficacy of the _Aligner_, not on the parameter size of the upstream LLMs.

From the perspective of representation learning [16; 17; 18], _Aligner_ exhibits an interpretable residual behavior. As shown in Figure 4, _Aligner_ decides the degree of reference to the original response and the extent of additional correction based on the quality of the original answers in the early layers, whereas its middle and late layers are used to implement this _decision_. The mechanism is simpler than directly learning the mapping from input queries to aligned answers. This simplicity indicates that small _Aligner_ can also learn complex correction patterns, demonstrating their capability to steer powerful models with relatively little inference, which further underscores the superiority of our _Aligner_ paradigm.

In summary, _Aligner_ presents several significant advantages:

* **Resource Efficient.** Without extra models such as the actor, critic, reward, and reference model, our _Aligner_ is a small model trained on the preference dataset to learn correctional residuals. Specifically, when aligning a 70B LLM, _Aligner_-7B occupies 11.25 times smaller than DPO and 22.5 times smaller than RLHF2 regarding training parameters. * **Plug and Play.** The _Aligner_'s plug-and-play nature and model agnosticism make it ideal for API-based models without parameter access. Once trained, the _Aligner_ can be applied to various upstream LLMs without parameter adjustments. Experiments showed that the _Aligner_-7B model enhances helpfulness and harmlessness across \(11\) models, including API-based/open-source safety-aligned/safety-unaligned models. Experiment results demonstrate that the _Aligner_-7B increased GPT-4's helpfulness by 17.5% and its harmlessness by 26.9% simultaneously.
* **Open Source.** We also release all the training codes, _Aligner_ models and 100K Q-A-C dataset, _AlignerTails_, to empower the community to explore and advance correction paradigms.

Figure 1: **(Left) Architecture of the _Aligner_ module and illustration of its behavior in semantic space. As a plug-and-play module, _Aligner_ stack upon an upstream LLM. The _Aligner_ redistributes initial answers from the upstream model into more helpful and harmless answers, thus aligning the composed LLM responses with human intentions. (Right) Analogy of _Aligner_ as a residual learning enhancer for LLMs in architecture and capabilities. Like a residual block that adds modifications via a shortcut without altering the base structure, the _Aligner_ employs a _copy and correct_ method to improve the original answer. This analogy highlights the _Aligner_â€™s dual role in preserving the parameter of the upstream model while enhancing it to align with desired outcomes.**

## 2 _Aligner_

Preliminary: Supervised Fine-Tuning (SFT)SFT aims to finetune the pretrained LLM to generate target answers using supervised learning -- specifically, maximum likelihood estimation -- on a curated high-quality dataset \(_{}=\{^{(i)},^{(i)}\}_{i=1}^{N}\). The goal is to obtain a model \(_{}^{}\) with the following training objective:

\[*{minimize}_{}(;_{ })=-_{(,)_{}}[ _{}(|)].\] (1)

Similarly, illustrated in Figure 1, _Aligner_ improves alignment between the model and human intentions by redistributing the model's answers through conditional generation. In practical implementation, _Aligner_ only needs to make a minor adjustment to the SFT training code (only need to change one line of code), as detailed in Appendix D.

Overall, the whole pipeline of _Aligner_ training can be summarized as follows: Based on a preference dataset, the model is fine-tuned to learn the correctional residuals between preferred and non-preferred responses. After a single training session, this model can be deployed on any model to achieve corrected alignment.

Model TrainingBased on the above procedures, we have constructed the dataset \(=\{^{(i)},_{o}^{(i)},_{c}^{(i)}\}_{i=1}^{N}\), which \(\) represents the user's query, \(_{o}\) is the original answer, and \(_{c}\) is the corrected answer according to established principles. The model training process is relatively straightforward. We train the _Aligner_, a conditional seq2seq model \(_{}(_{c}|_{o},)\) parameterized by \(\), to redistribute the preliminary answers \(_{o}\) to the aligned answer \(_{c}\). Demonstrated in Figure 1, the composed answer generation process for aligned answers based on the upstream LLM \(_{}\) is:

\[^{}(_{c}|)=_{_{k}}_{}(_{c}| _{k},)_{}(_{k}|)_{ }(_{c}|_{o},)_{}(_{o}|),\] (2)

where \(_{k}\) is a possible answer generated by upstream LLM \(_{}\). By calculating empirical loss on the whole dataset \(\), we can get equation (3) from equation (2):

\[-_{}[^{}(_{c}|)]- _{}[_{}(_{c}|_{o},)]- _{}[_{}(_{o}|)].\] (3)

The second term in equation (3) is not related to the _Aligner_ parameter and the training objective for _Aligner_ can be derived as equation (4):

\[*{minimize}_{}_{}(, )=-_{}[_{}(_{ c}|_{o},)].\] (4)

By optimizing this objective, we actually optimize the upper bound of the SFT training objective, which ensures that \(_{c}\) is effectively learned. It is worth noting that _Aligner_ does not require access to the parameters of the upstream LLM \(_{}\) during both training and inference phases. _Aligner_ takes the user's query \(\) and the initial answer \(_{o}\) generated by the upstream LLM \(_{}\), then generates the answer \(_{c}\) which is better aligned with human values. Improving existing answers \(_{o}\) allows _Aligner_ to focus on how to align with human values rather than how to answer the given query directly. This significantly reduces the requirements on our model capacity, allowing us to achieve the expected alignment performance with only a small model.

Aligner's Training Strategy: Residual CorrectionWe develop an optimized training strategy, termed _Residual Correction_, which leverages the semantic correctional residuals between answers (\(_{o}\)) and corrections (\(_{c}\)), as shown in Figure 1. Specifically, we construct a Q-A-A dataset using partial training data to train an identity _Aligner_ initially, a process we term _warm-up_. Subsequently, we utilize the Q-A-C dataset for training, building upon the identity _Aligner_. The details of our experiments on a 50K training dataset are shown in Section 3.3. Outside the alignment field, ResNet  also uses a similar approach to mitigate the vanishing gradient problem caused by increased neural network depth.

Resource Analysis between _Aligner_ and RLHF/DPOCompared to RLHF and DPO , _Aligner_ shows notable advantages in training resource requirements. Regarding training resources, _Aligner_-7B is more efficient than other methods under similar performance conditions. Specifically, witha 7B source model, DPO requires 1.125 times, and RLHF 2.25 times more resources than _Aligner_. Additionally, as the source model's scale increases, the resource demands for other methods rise sharply. For a 70B model, DPO needs 11.25 times, and RLHF 22.5 times more resources than _Aligner_. However, since _Aligner_ is insensitive to these changes, its training resource requirements remain constant regardless of the source model's scale, indicating that _Aligner_ is an efficient and lightweight alignment paradigm.

## 3 Experiments

In this section, we assess the effectiveness of _Aligner_ modules in the 3H (Helpful, Harmless, Honest) evaluation metrics and configurations. For detailed training parameters, please see Appendix D.

### Experiment Setup

Preference DatasetsWe utilize two open-source preference datasets, HH-RLHF  and PKU-SafeRLHF [19; 20] as our preference datasets. Considering that the preference pairs in PKU-SafeRLHF are generated solely by Alpaca-7B, we additionally construct a 50K preference dataset based on these two preference datasets using correction paradigm. The questions in this dataset are sourced from HH-RLHF, PKU-SafeRLHF, and so on, resulting in 27K queries for subsequent

    & &  &  &  \\  _Aligner_ & &  &  &  &  &  \\   & Upstream LLM &  &  &  &  &  &  &  \\   & GPT-4 & 26.0\% & 2.3\% & 8.0\% & 28.6\% & 12.5\% & 29.2\% & -0.5\% \\  & GPT-3.5 & 26.3\% & 3.3\% & 3.1\% & 7.6\% & 3.6\% & 4.4\% & 0.7\% \\  & Claude 2 & 83.1\% & 6.0\% & 38.3\% & 15.1\% & 48.0\% & 14.4\% & 0.7\% \\  & Baever-7B & 95.3\% & 60.7\% & 7.9\% & 12.1\% & 7.8\% & 7.6\% & 5.4\% \\  & Alpaca-7B & 97.7\% & 85.5\% & 5.8\% & 45.0\% & 22.6\% & 65.3\% & 10.0\% \\  & Vicuna-7B & 44.9\% & 58.5\% & 16.9\% & 15.8\% & 17.7\% & 27.1\% & 4.9\% \\  & Vicuna-13B & 53.9\% & 24.0\% & 19.4\% & 14.9\% & 17.1\% & 16.1\% & 7.6\% \\  & Vicuna-33B & 45.7\% & 39.3\% & 24.4\% & 52.4\% & 26.9\% & 32.6\% & 5.1\% \\  & Llama-7B-Chat & 88.1\% & 69.5\% & 25.4\% & 7.2\% & 11.3\% & 25.9\% & 3.9\% \\  & Llama-13B-Chat & 85.3\% & 53.4\% & 18.4\% & 12.3\% & 18.6\% & 27.6\% & 1.2\% \\  & Llama-7B-Chat & 86.7\% & 47.9\% & 17.8\% & 5.5\% & 21.3\% & 7.2\% & 10.8\% \\  & Average & **66.6\%** & **36.4\%** & **17.0\%** & **19.7\%** & **18.8\%** & **23.4\%** & **4.5\%** \\   & GPT-4 & 27.7\% & 6.2\% & 18.6\% & 25.8\% & 16.3\% & 28.0\% & -1.2\% \\  & GPT-3.5 & 25.6\% & 6.8\% & 9.3\% & 9.3\% & 8.4\% & 7.0\% & 0.0\% \\  & Claude 2 & 90.4\% & 10.4\% & 58.4\% & 30.3\% & 69.4\% & 42.1\% & 2.0\% \\  & Beaver-7B & 98.3\% & 33.5\% & 21.9\% & 12.0\% & 8.9\% & 6.0\% & 10.3\% \\  & Alpaca-7B & 99.4\% & 78.7\% & 34.9\% & 47.0\% & 38.2\% & 70.7\% & 11.8\% \\  & Vicuna-7B & 95.4\% & 73.6\% & 26.4\% & 15.9\% & 12.0\% & 29.3\% & 2.7\% \\  & Vicuna-13B & 94.0\% & 49.5\% & 37.6\% & 16.6\% & 21.9\% & 18.9\% & 2.7\% \\  & Vicuna-33B & 89.3\% & 58.5\% & 51.0\% & 55.9\% & -1.0\% & 33.6\% & 3.2\% \\  & Llama-27B-Chat & 95.6\% & 98.8\% & 19.9\% & 7.4\% & -5.7\% & 22.1\% & 1.5\% \\  & Llama-13B-Chat & 96.6\% & 70.8\% & 20.1\% & 10.3\% & 15.5\% & 28.6\% & 1.7\% \\  & Llama-70B-Chat & 95.0\% & 70.1\% & 5.2\% & 2.4\% & -6.6\% & 4.1\% & 9.1\% \\  & Average & **82.5\%** & **58.2\%** & **27.6\%** & **21.2\%** & **16.1\%** & **26.4\%** & **4.0\%** \\   & GPT-4 & 42.6\% & 9.7\% & 33.9\% & 25.1\% & 20.1\% & -0.2\% \\  & GPT-3.5 & 43.7\% & 15.6\% & 15.1\% & 10.9\% & 7.6\% & 7.7\% & 0.5\% \\   & Claude 2 & 90.6\% & 17.2\% & 50.0\% & 30.0\% & 45.9\% & 28.6\% & 0.5\% \\   & Baever-7B & 98.1\% & 87.6\% & 14.2\% & 19.1\% & 8.0\% & 11.6\% & 1.3\% \\   & Alpaca-7B & 99.0\% & 82.9\% & 8.5\% & 53.4\% & 3.4\% & 75.9\% & 16.9\% \\   & Vicuna-7B & 96.3\% & 78.5\% & 19.1\% & 24.0\% & 19.5\% & 31.0\% & 6.6\% \\   & Vicuna-13B & 95.9\% & 58.7\% & 31.8\% & 26.7\% & 30.9\% & 18.9\% & 7.1\% \\   & Vicuna-33B & 90.0\% & 65.9\% & 33.3\% & 63.3\% & 7.3\% & 33.3\% & 6.1\% \\   & Llama-27B-Chat & 96.0\% & 99.1\% & 13.5\% & 4.6\% & 12.6\% & 32.3\% & 4.2\% \\   & Llama-13B-Chat & 95.4\% & 73.1\% & 16.7\% & 10.6\% & 30.7\% & 35.0\% & 1.0\% \\   & Llama-27B-Chat & 94.6\% & 69.2\% & 10.6\% & 1.9\% & 6.3\% & 7.6\% & 10.3\% \\   & Average & **85.6\%** & **59.8\%** & **22.4\%** & **24.5\%** & **17.9\%** & **27.4\%** & **6.0\%** \\   

Table 1: **Performance of _Aligner_ models.** It is shown that _Aligner_ achieves significant performances in all the settings. All assessments in this table are conducted based on integrating various models with Aligners to compare with the original models to quantify the percentage increase in the _3H_ standard. When integrated and assessed in conjunction with various upstream models, the answers and corrected answer generation. The original answers are generated using various open-source models, including Alpaca-7B , Vicuna-(7B,13B,33B) , Llama2-(7B,13B)-Chat , and Alpaca2-(7B,13B)3. We use GPT-4, Llama2-70B-Chat, and human annotators to revise the answers in the above Q-A dataset. These revisions are based on well-defined principles, establishing constraints for training the seq2seq model. These principles are aimed at effectively extending to the characteristics we wish LLMs to embody. We focus on the 3H dimensions of LLMs (helpfulness, harmlessness, and honesty) . For those answers that conform to these fundamental principles, we retain the original answers. Figure 2 (a) visually shows the distribution shift before and after the data correction, thereby demonstrating the impact of the revision process on the dataset. More details about the construction of Q-A-C Datasets can be found in Appendix D.1.

Models and Evaluation DatasetsWe trained the _Aligner_ on three model sizes, specifically based on Gemma-2B  and Llama2 (7B, 13B) . To assess the _Aligner_ module, we utilize five datasets: E-Dialogue , DialogSum , BeaverTails , HarmfulQA , and TruthfulQA . More details can be found in Appendix B.1. Our evaluation focuses on two model categories: API-based models (e.g., GPT-4 , Claude 2 ) and open-source models (Llama2-(7B, 13B, 70B)-Chat ; Vicuna-(7B, 13B, 33B) ; Alpaca-7B ; Beaver-7B ). Notably, the Llama2 and Beaver models have undergone safety alignment processing.

Evaluation MetricsOur evaluation hinges on three key dimensions: helpfulness, harmlessness, and honesty. The independent characteristics of these dimensions provide a comprehensive perspective on the answers, allowing us to balance information quality with safety and ethical considerations in the evaluation of an answer's quality. Initial answers are generated by open-source and upstream models, which the _Aligner_ refines to yield corrected answers. More details and examples can be found in Appendix B.

### Experiment Results

As shown in Table 1, we employ _Aligners_ of various sizes, significantly improving the performance of all 11 upstream models with only one training session. Under the 3H standard, _Aligner_-7B showcases an average enhancement of 21.9% in helpfulness and 23.8% in harmlessness across the

Figure 2: **Distribution of helpfulness and harmlessness scores.****(a)** The distribution shift in preferred and dis-preferred answers in the training dataset; **(b)** redistribution shift of _Aligner_-7B, based on upstream models such as GPT-4 (b1), Alpaca-7B (b2) and Llama2-70B-Chat (b3) in the evaluation dataset. Our findings include: **(1)** Preferred answers in the training dataset surpasses original answers in both helpfulness and harmlessness; **(2)** The refuse-to-answer pattern of GPT-4 created an area of overcorrected answers where both helpful and harmless scores are low, and _Aligner_-7B improved these answers by providing additional information and corrections. **(3)** The Alpaca-7B model, which lacks alignment, had its answers significantly corrected by our _Aligner_-7B, increasing both scores. **(4)** The Llama2-70B-Chat model, already aligned with a higher average safety score than the training dataset corrections, benefits from _Aligner_-7B corrections, significantly enhancing helpfulness while maintaining the harmless score.

models. Remarkably, _Aligner_-7B can boost GPT-4's helpfulness by 17.5% and harmlessness by 26.9% simultaneously.

Performance on the _3H_ Standard_Aligner_ keeps the upstream model unchanged, offering adaptability in _Aligner_ model sizing based on available resources. We evaluate _Aligner_'s effectiveness using five datasets according to the 3H standard. Experiment results show that _Aligner_ significantly enhances the upstream model's performance across various parameter scales. Particularly, _Aligner_-7B markedly enhances the GPT-4 model's performance across all five dimensions. In the reasoning dimension, with an increase in parameters, _Aligner_ boosts the upstream model's capability, showcasing the _Scaling Laws_ characteristics. Notably, _Aligner_ excelled in the empathy dimension, further evidencing its efficiency in redistributing the upstream model's pattern distribution. To detect whether _Aligner_ would generate known false content due to misunderstandings, similar to , we use TruthfulQA  to measure the reliability of the outputs generated by _Aligner_ in terms of factualness and common sense. The results demonstrate that _Aligner_ does not add extra hallucination information while correcting the upstream model.

Assessing _Aligner_'s Stack on Safety-Aligned ModelsLlama2-Chat models, with their multi-stage alignment process (pre-training, SFT, RLHF), and Beaver, finetuned via Safe RLHF , both show modest safety improvements with _Aligner_. The primary achievement of _Aligner_ is its ability to amplify helpfulness, especially in models predisposed to avoid risky responses. By re-distributing these overly conservative answers, _Aligner_ significantly boosts overall helpfulness. This enhancement in helpfulness is visually represented in Figure 2, showing a rightward shift in Llama2-70B-Chat's answer distribution under the influence of _Aligner_-7B, indicating improved helpfulness on a strong safety foundation.

### Ablation Study

Ablation on Identity MappingTo verify the effectiveness of different _warm-up_ proportions, we conducted experiments using two representative datasets: BeaverTails and HarmfulQA. As shown in Figure 3, the _warm-up_ step aids the _Aligner_ by initially helping the _Aligner_ learn identity mapping, thus improving the final performance. Moreover, the results further reveal that the effectiveness of the _warm-up_ phase peaks when the proportion is 10k to 50k. However, determining the specific data proportion for _warm-up_ is challenging and requires more training resources.

Comparison to Self-Refine, Critique MethodsConstitutional AI (CAI) , Self-Critique , and Self-Refine , primarily utilize the self-critiquing and refining capabilities of LLMs to enhance their performance. We employ CAI prompts solely during the inference time of LLMs to encourage self-revision of their answers. As demonstrated in Table 2, our method, _Aligner_, outperforms the baseline considering both helpfulness and harmlessness dimensions. Additionally, baseline methods typically require multiple dialogue iterations and extended context windows for prompt insertion and ongoing self-correction. This could result in longer inference times and considerable consumption of context window length. For more detailed information and analysis, please refer to Appendix B.5.

Figure 3: **Ablation study of different _identity mapping_ proportions. We first train an identity _Aligner_ for identity mapping, followed by extensive residual Q-A-C learning based on this _Aligner_. Specifically, we form the Q-A-A dataset by extracting partial data from the training dataset in proportions of 2%, 10%, 20%, and 50%.**

Performance of _Aligner_ on the Various Preference DatasetsTo demonstrate the independence of _Aligner_ from specific datasets, we utilize various open-source RLHF preference datasets. Specifically, we trained on HH-RLHF , PKU-SafeRLHF [19; 30] and Ultra-Feedback  datasets and compared _Aligner_ with SFT, RLHF, and DPO. After fine-tuning Alpaca-7B with SFT, RLHF, and DPO, we compare these models against the original Alpaca-7B corrected by _Aligner_. The experiment results (as shown in Table 3) indicate that _Aligner_'s performance in enhancing the original model's capabilities is comparable to, or exceeds, that of the baseline methods. Notably, models finetuned with RLHF or DPO tend to generate either conservative answers or fail to recognize dangers while adding helpful information explicitly. Importantly, training with RLHF or DPO methods requires optimizing significantly more models and consuming more training resources than just training an _Aligner_, _e.g._, for a 70B model, DPO needs 11.25 times and RLHF 22.5 times more resources.

### Interpretability Experiments

When performing the experiments above, we observed the correction paradigm of _Aligner_: the correction behavior is not a binary decision between correction and copying. Instead, it follows a conditional generation paradigm, where the degree of reference to the original response and the extent of additional correction depends on the quality of the original answers. To demonstrate that _Aligner_ has learned this correction paradigm as a representation, we conduct the experiment based on _representation engineering_ and _activation steering_[36; 37; 17]. Specifically, we perform representation extraction and _Linear Artificial Tomography_ (LAT) scan to the Llama2-7B based on the _Aligner_ module. We then utilize the extracted representation to control the _Aligner_'s generation.

The results from the representation control experiment indicate that the ratio of adding or subtracting the representation vector in the _Aligner_ activation will significantly affect the magnitude of correction, ranging from directly copying the original response to substantially increasing the extent of normal correction. This provides strong evidence that _Aligner_ has internalized the correction paradigm as a representation. Furthermore, the LAT scan further shows that _Aligner_ decides the degree of correction in its early layers based on the quality of the original response, and after that, it focuses on completing the correction in its middle and late layers. For more details of these experiments, see Appendix B.6.

   Model & Metrics & CAI **w/o** training & Self-Critique & _Aligner_-7B \\   & Helpfulness & +20.01\% & **+26.56\%** & +17.47\% \\  & Harmlessness & +9.65\% & +15.30\% & **+26.88\%** \\   & Helpfulness & +20.00\% & +30.07\% & **+36.55\%** \\  & Harmlessness & +24.08\% & +14.36\% & **+58.86\%** \\   & Helpfulness & +5.00\% & +12.80\% & **+15.40\%** \\  & Harmlessness & +7.70\% & -11.6\% & **+9.00\%** \\   & Helpfulness & -0.5\% & +15\% & **+17.8\%** \\  & Harmlessness & **+27.4\%** & +11.1\% & +19.45\% \\   

Table 2: **Ablation study of _Aligner_â€™s effectiveness against CAI and Self-Critique.** Experiment results revealed that _Aligner_ surpasses these baselines in helpfulness and harmlessness metrics.

    &  &  &  &  \\  Methods & Helpful & Harmless & Helpful & Harmless & Helpful & Harmless & Helpful & Harmless \\  _Aligner vs._ SFT & +23.1\% & +0.4\% & - & - & - & - & - & - \\ _Aligner vs._ RLHF & +24.4\% & +21.9\% & +8.7\% & +8.8\% & +9.6\% & +3.4\% & +25.47\% & +13.13\% \\ _Aligner vs._ DPO & +49.1\% & +0.1\% & +33.3\% & +27.0\% & +5.6\% & +30.9\% & +27.21\% & +6.12\% \\   

Table 3: _Aligner trained on different preference datasets._ The experimental results show that _Aligner_ enhances the original modelâ€™s capabilities, performing on par with or surpassing baseline methods. Furthermore, these results are consistent across different preference and correction datasets.

## 4 Multi-round RLHF training via _Aligner_

In this section, we aim to show that, due to its efficient and plug-and-play features, _Aligner_ can play a crucial role in the multi-round RLHF/DPO pipeline, as illustrated in Figure 5. Typical multi-round pipeline often suffers from reward collapse because the preference dataset used for reward modeling may deviate from the actual answer distribution of the upstream model . This error accumulates over multiple rounds, leading to significant deviations in the model's final results. Additionally, error accumulation may cause reward over-optimization in certain directions, _e.g._, generating longer responses irrespective of safety. The involvement of _Aligner_ can help mitigate the problem.

As shown in Figure 5, you can use the _Aligner_ (which is trained using the original preference dataset for the next round of RLHF) to refine the upstream model response \(A\) into response \(A^{*}\), and \((Q,A,A^{*})\) pairs can be a new preference dataset for training in the next round of RLHF or DPO. This paradigm brings many advantages:

* The _Aligner_ inherits the feature of transferring from the dispreferred distribution to the preferred distribution in the preference dataset.
* _Aligner_ modifies the upstream model to produce better answers, bringing the distribution of the resulting preference dataset closer to the answer distribution of the upstream model. This effectively mitigates the reward model collapse problem caused by out-of-distribution (OOD) preference datasets.

Figure 4: **Interpretability experiment results on _Aligner_.****(a)(b)** The LAT scan graph of _Aligner_â€™s each layer when generating the first 20 output tokens for two given question-answer pairs. A higher value in the graph indicates a more active correction representation in that layer. Specifically, (a) exhibits raised activity, suggesting an enhanced correction action in the output, whereas (b) displays a tendency towards copying the original response. Moreover, the distinct differences between the two graphs are mainly observed in the early layers. This indicates that the decision regarding the degree of correction is made in the early layers of _Aligner_. **(c)** The control experiment shows the effectiveness of the extracted correction representation vector in modulating the _Aligner_â€™s correction behavior. The relationship between the average levenshtein ratio and representation vector coefficients is approximately linear, with an \(R^{2}\) value of approximately 0.93.

Figure 5: **Illustration of multi-round alignment pipeline with _Aligner_. As a data augmentation and synthetic tool, _Aligner_ can enhance the upstream modelâ€™s response \(A\) into an improved response \(A^{*}\), thereby forming a synthetic preference dataset. This dataset can be used to further train the upstream model via RLHF/DPO. Repeating this process allows for multi-round RLHF or DPO.

* The _Aligner_ serves as a synthetic data generator, providing an efficient and repeatable method for constructing preference datasets.

We conducted three rounds of RLHF and DPO on Alpaca2-7B using the three-round preference dataset from PKU-SafeRLHF . Following this, we trained three rounds of _Aligner_s with the same three-round preference datasets, which were then employed to refine the upstream model and generate new preference datasets. These synthetic preference datasets were subsequently used to fine-tune the upstream model. As illustrated in Figure 6, by aggregating _Aligner_, _Aligner_-corrected new preference datasets can effectively enhance two key metrics: improving the model's safety while ensuring a monotonic increase in helpfulness with each round. In contrast, a typical multi-round RLHF/DPO pipeline only enhances utility, leaving the responses unsafe.

## 5 Related Work

Reinforcement Learning from Human FeedbackRLHF aims to align LLMs with human preferences [39; 2], utilizing RL algorithms  to train policy models, specifically LLMs, to maximize cumulative rewards from RMs. Recent research efforts have also been exploring the extension of RLHF to multimodal scenarios [40; 41; 42]. The RLHF approach involves the distributed training of various models  and the annotations by human experts, presenting operational challenges. Consequently, recent research has focused on reducing [43; 44] or eliminating  reliance on RMs, aiming to simplify the RLHF process. Simultaneously, [5; 45] leverage advanced AI models for data annotation, effectively streamlining the RLHF process and reducing associated costs. Additionally,  incorporates safe reinforcement learning techniques [46; 47] to improve the helpfulness of fine-tuned models while ensuring their safety. In contrast to RLHF methods that require several models, _Aligner_ only requires a constrained seq2seq model to meet the alignment objective. _Aligner_ is distinguished by its plug-and-play nature and indifference to specific models and parameters, making it ideal for API-based models without parameter access.

Inference-time MethodsThese methods customize LLMs without requiring access to their internal parameters [48; 49; 7], proving especially useful for extremely large models or those available through APIs. However, most of these methods are sensitive to the upstream model. IPA  uses a lightweight adapter policy to multiply the next-token probabilities based on the upstream model during the decoding time. However, IPA needs to access the model's output logit distribution.  enhances and refines user prompts to better suit the model, thereby facilitating more comprehensive contextual understanding for inference, similar to in-context learning (ICL) [50; 51].  employs a smaller model to select the best response from several responses generated by the upstream model without fine-tuning upstream models, akin to the BoN (Best of N) selector [53; 54]. In this work, we introduce _Aligner_--a model-agnostic alignment module designed for seamless integration. Requiring only a single training session, _Aligner_ can align 11 types of upstream models, significantly enhancing their performance according to 3H standards.

Self-RefinementLLMs do not always generate the coherent output on their _first try_. Self-refinement methods address this by iteratively improving outputs through self-generated feedback, bypassing additional supervision [55; 56; 57]. For example, Self-Debugging allows LLMs to debug their predictions via few-shot examples, while  found that self-critiquing can expose output weaknesses that aid in fine-tuning, with larger models performing especially well in critique tasks. However, these methods typically depend on a single model's ability to refine itself. Our work instead uses a separate model, _Aligner_, which can refine outputs from other models (e.g., 70B model, GPT-4), achieving robust weak-to-strong generalization . This approach bypasses the limitations of smaller models and saves computational resources otherwise spent on self-critiquing. Additionally, by combining _Aligner_ with an external critique model, future iterations could further enhance performance.

Figure 6: Multi-round refinement through _Aligner_.

Conclusion

We introduce the _Aligner_, an efficient, lightweight, and model-agnostic approach to align LLMs. Without the need for additional components such as the actor, critic, reward models, and others, _Aligner_ demonstrates a significant increase in computational efficiency. Under the 3H standard, _Aligner_-7B showcases an average enhancement of 68.9% in helpfulness and 22.8% in harmlessness across the models. Remarkably, _Aligner_-7B can boost GPT-4's helpfulness by 17.5% and harmlessness by 26.9%. In the Alpaca-Eval leaderboard, stacking _Aligner_-2B on GPT-4 Turbo (04/09) improved its LC Win Rate  from 55.0% to 58.3%, surpassing GPT-4 Omni's 57.5% Win Rate (community report).

### Limitations and Future Work

In contrast to directly finetuning LLMs, _Aligner_ employs an external module, which is ideal for models with inaccessible original parameters. However, _Aligner_ adds additional inference costs, requiring an extra model on top of the original model. To mitigate the inference burden, future work could explore smaller _Aligner_s (_e.g._, 0.5B) and streamlining _Aligner_'s corrections. We aim to enhance LLM alignment using the _Aligner_ module, aiming for increased conciseness, efficiency, and interpretability. Future research will focus on enhancing _Aligner_'s versatility in challenging contexts like multi-turn dialogues and developing Control _Aligner_ for domain-specific alignment with precise instructions. Moreover, unlike RLHF's segmented approach, its end-to-end structure provides valuable insights into the alignment process for LLMs.

### Ethics and Impact

The _Aligner_ dataset will be released under the CC BY-NC 4.0 license. This dataset integrates Q-A data from open-source and API-based models, with answers revised to meet the 3H (helpfulness, harmlessness, and honesty) standards . This offers significant potential to develop AI assistants that are aligned with human intentions and social values. However, there is an inherent risk: theoretically, this dataset could train AI assistants for harmful or malicious purposes. As the _Aligner_ dataset's creators, we are dedicated to fostering beneficial and safe AI technology and strongly oppose any misuse that could hinder human progress. We strongly condemn any malicious use of the _Aligner_ dataset and advocate for its responsible and ethical use.