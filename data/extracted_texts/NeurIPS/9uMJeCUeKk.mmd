# Ask, Attend, Attack: An Effective Decision-Based Black-Box Targeted Attack for Image-to-Text Models

Qingyuan Zeng

Institute of Artificial Intelligence, Xiamen University

Zhenzhong Wang

Department of Computing, The Hong Kong Polytechnic University

Yiu-ming Cheung

Department of Computer Science, Hong Kong Baptist University

Min Jiang

The corresponding author: Min Jiang, minjiang@xmu.edu.cn School of Informatics, Xiamen University

Min Jiang

The corresponding author: Min Jiang, minjiang@xmu.edu.cn School of Informatics, Xiamen University

###### Abstract

While image-to-text models have demonstrated significant advancements in various vision-language tasks, they remain susceptible to adversarial attacks. Existing white-box attacks on image-to-text models require access to the architecture, gradients, and parameters of the target model, resulting in low practicality. Although the recently proposed gray-box attacks have improved practicality, they suffer from semantic loss during the training process, which limits their targeted attack performance. To advance adversarial attacks of image-to-text models, this paper focuses on a challenging scenario: decision-based black-box targeted attacks where the attackers only have access to the final output text and aim to perform targeted attacks. Specifically, we formulate the decision-based black-box targeted attack as a large-scale optimization problem. To efficiently solve the optimization problem, a three-stage process _Ask_, _Attend_, _Attack_, called _AAA_, is proposed to coordinate with the solver. _Ask_ guides attackers to create target texts that satisfy the specific semantics. _Attend_ identifies the crucial regions of the image for attacking, thus reducing the search space for the subsequent _Attack_. _Attack_ uses an evolutionary algorithm to attack the crucial regions, where the attacks are semantically related to the target texts of _Ask_, thus achieving targeted attacks without semantic loss. Experimental results on transformer-based and CNN+RNN-based image-to-text models confirmed the effectiveness of our proposed _AAA_.

## 1 Introduction

Image-to-text models, referring to generating descriptive and accurate textual descriptions of images, have received increasing attention in various applications, including image-captioning , visual-question-answering , and image-retrieval . Despite the remarkable progress, they are vulnerable to deliberate attacks, giving rise to concerns about the reliability and trustworthiness of these models in real-world scenarios. For example, one may mislead models to output harmful content such as political slogans and hate speech by making imperceptible perturbations to images .

To gain insight into the reliability and trustworthiness of the image-to-text models, a series of adversarial attack methods have been proposed to poison the outputted textual descriptions of givenimages [7; 12; 13; 10]. Specifically, based on the attacker's level of access to information about the target model, they can be divided into three categories: white-box attacks [7; 14; 12], gray-box attacks [13; 10], and black-box attacks [15; 16; 17; 18; 19]. The white-box attacks can obtain target models' information including the entire architecture, parameters, gradients of both the image encoder and text decoder, and probability of each word of the output text. Gray-box attacks can only access the architecture, parameters, and gradients of the image encoder, while black-box attacks cannot access any internal information of the target model, but only the output text of the model. Furthermore, black-box attacks can be divided into score-based and decision-based attacks. Score-based black-box attacks can access the probability of each word of the output text , while decision-based black-box attacks can only access the output text [15; 20; 21]. Because less information about the target models is provided, decision-based black-box attacks are more challenging than other categories . Additionally, these attack methods can be categorized based on whether the attacker is able to specify the incorrect output text, dividing them into two types: targeted and untargeted attacks [22; 13].

Although numerous adversarial attack methods for image-to-text models have been proposed, to our best knowledge, the study on black-box attacks is under-explored, especially decision-based black-box targeted attacks. This kind of attack is more challenging due to the following reasons. Firstly, less information on the target model can be accessed. Specifically, only the output text instead of gradients, architectures, parameters, and the probability of each word in the output text is available. Secondly, the attackers not only cause the target model to output incorrect text, but also outputs the specified target text. Existing attacks easily suffer from the loss of semantics, resulting in the inability to effectively output the specified target text. Figure 1 (a) show that transfer+query  fabricates one target text to poison the target image-to-text model, leading to this model outputting an incorrect text. However, the output text could mismatch the original semantics of the target text, as the target image-to-text model may focus on secondary information while ignoring the crucial semantics of the target text behind the target image, resulting in semantic loss. More examples are in Appendix B.1.

To narrow the research gap, we propose a decision-based black-box targeted attack approach for image-to-text models. In our work, only the output text of the target model can be accessed, which is closer to the real-world cases . Additionally, Figure 1 (b) demonstrates our targeted attack method, which optimizes against the target text directly under the decision-based black-box conditions, preventing semantic loss and maintaining semantic consistency with the target text.

Perturbing pixels in the image can change the output text. Therefore, the objective of the targeted attack can be considered to find the imperceptible pixel modification to make the output text similar to the target text. In this manner, the targeted attack can be formulated as a large-scale optimization problem, where pixels are decision variables and the optimization objective is to poison the output text. Inspired by the distinctive competency of evolutionary algorithms for solving large-scale optimization problems [23; 24; 25; 26; 27], we develop a dedicated evolutionary algorithm-based framework for decision-based black-box targeted attacks on image-to-text models. However, directly applying evolutionary algorithms to solve this large-scale optimization problem could suffer from low search efficiency, due to the numerous pixels and their wide range of values. To address the issue, we embed

Figure 1: The semantic loss problem is existing in existing gray-box targeted attack methods.

three-step processes, i.e., _Ask_, _Attend_, _Attack_, into the proposed evolutionary algorithm-based attacks. As shown in Figure 2, during the _Ask_ stage, attackers can arbitrarily specify words related to certain semantics, such as _photograph_. Then, candidate words (e.g., _camera_, _scenic_, and _phone_) that are related to certain semantics are searched. Meanwhile, these words are close to the clean image in the feature space of the target image-to-text model. By selecting words from the candidate words, the target text (e.g., _a cute girl using a phone to take pictures of the fantastic TV_) related to the attacker's specified semantics can be formed to poison the target model. Subsequently, based on the attention mechanism, _Attend_ identifies the crucial regions of the clean image (e.g. attention heatmap) [28; 29], thus reducing the search space for the subsequent _Attack_. Lastly, _Attack_ uses a differential evolution strategy to impose imperceptible adversarial perturbations on the crucial regions, where the optimization objective is to minimize the discrepancy between the target text in _Ask_ stage and the output text of the target model. Our contributions can be summarized as follows:

1. We first propose a decision-based black-box targeted attack _Ask_, _Attend_, _Attack_ (_AAA_) for image-to-text models. Specifically, our method achieves targeted attacks without losing semantics while only the model's output text can be accessed.
2. We designed a target semantic directory to guide attackers in creating target text and utilized attention heatmaps to significantly reduce search space. This improves the search efficiency of evolutionary algorithms in adversarial attacks and makes attacks difficult to perceive.
3. We conducted extensive experiments on the Transformer-based VIT-GPT2 model and CNN+RNN-based Show-Attend-Tell model, which are the two most-used image-to-text models in HuggingFace, and surprisingly found that our decision-based black-box method has stronger attack performance than existing gray-box methods.

## 2 Related work

### White-box Attack

In white-box attacks, the attacker has full access to all parameters, gradients, architecture of the target model, and the probability of each word of the output text. The authors in  add invisible perturbations to the image to make the image-to-text model produce wrong or targeted text outputs. The authors in  add global or local perturbations to the image to make the vision and language models unable to correctly locate and describe the content of the image. The authors in  modify the content of the image at the semantic level to make the image-to-text model output text that is inconsistent with the original image. The authors in  craft adversarial examples with semantic embedding of targeted captions as perturbation in the complex domain. The authors in  preserve the accuracy of non-target words while effectively removing target words from the generated captions. The authors in  generate coherent and contextually rich story endings by integrating textual narratives with relevant visual cues. The authors in  add limited-area perturbations to the image to make the image-to-text model fail to correctly describe the content of the perturbed area. The above methods require complete information of the image-to-text target model, including architecture, gradients, parameters, and probability distribution of the output text, which limits their practicality.

Figure 2: Diagram of our decision-based black-box targeted attack method _Ask_, _Attend_, _Attack_.

### Gray-box Attack

To improve the practicality of adversarial attacks for image-to-text models, recent research explores how to attack with partial knowledge of the target model. All existing gray-box attack studies [34; 22; 13; 10] assume full access to the image encoder of the image-to-text model. The basic idea of gray-box targeted attacks is to reduce the distance between the adversarial image and the target image generated based on the target text in the image encoder's feature space. The authors in  generate adversarial images to mimic the feature representation of original images. The authors in  use a generative model to destroy the image encoder's features, achieving the untargeted attack. The authors in  minimize the feature distance in the image encoder between the adversarial image and the target image, thereby using gradient back-propagation to optimize the adversarial image and achieve the targeted attack. The authors in  combine existing gray-box method  with pseudo gradient estimation method  to achieve better performance in targeted attack. It is worth noting that they  call their method a black-box attack, but since they use the image encoder of the target model as the surrogate model, we classify their method as a gray-box attack. These gray-box attacks on image-to-text models are more practical than white-box attacks, but it is still unrealistic to assume that attackers can access the image encoder of the image-to-text model. Moreover, existing gray-box methods may have poor targeted attack performance due to the semantic loss mentioned above.

## 3 Methodology

### Problem Formulation

The image-to-text model \(:\) maps the image domain \(\) to the text domain \(\). A well-trained model should be able to accurately describe the content of the image using grammatically correct and contextually coherent text. Given a target text \(y_{t}\), the attacker's goal is to find an adversarial image \(}\) that is visually similar to clean image \(\) and can generate an adversarial text \(y_{adv}\) that is semantically similar to \(y_{t}\). We formalize the optimization problem for black-box targeted attack as:

\[_{_{adv}}S((}),y_{t})\ _{i=1}^{n}}(i)-(i),\] (1)

where \(S(,)\) represents the semantic similarity function between two texts, \(\) is the threshold for the average perturbation size per pixel, \(}(i)\) and \((i)\) represents the value of the \(i\)-th pixel in the adversarial and clean images. \(n\) is the total number of pixels in all channels of the image.

### Overview

To enhance the efficiency and stealth of decision-based black-box attacks, we propose the _Ask_, _Attend_, _Attack_ (_AAA_) framework as shown in Figure 2. _Ask_: We compile a semantic dictionary from words within the input image's search space that align with the attacker's specified semantics. This facilitates targeted text generation, meeting the attacker's target semantics while simplifying the search process. _Attend_: We employ attention visualization and a surrogate model to generate an attention heatmap for the target text on the image, narrowing the search to significant decision variables and enhancing perturbation stealth. _Attack_: We use the differential evolution in the reduced search space to find the optimal solution that can mislead the target model to output target text. The framework's pseudo-code is detailed in Appendix A.1.

### Ask Stage

According to the target semantics, the goal of _Ask_ is to find words in the feature space of the target model to form a target semantic dictionary. These words should be closer to the input image. Firstly, we treat each pixel in each channel of image \(\) as a variable, which means the search space size is the product of length, width, and number of channels. And then generate NP (number of population) individuals to form a population based on the following formula:

\[_{j}(i)=(i)+rand(-1,1),\] (2)

where \((i)\) is the \(i\)-th variable of clean image \(\), \(_{j}(i)\) is the \(i\)-th variable of the \(j\)-th individual in the population, \(\) is a hyperparameter about the maximum search range, _rand_(-1,1) is a random number from the range of -1 to 1.

Secondly, for each variable, random mutation occurs between different individuals. The mutation for the \(i\)-th variable of the \(j\)-th individual \(_{j}(i)\) is as follows:

\[_{j}^{g}(i)=_{r1}^{g}(i)+F*(_{r2}^{g}(i)-_{r3}^{g}(i)),\] (3)

where \(_{j}^{g}(i)\) is the mutated variable for mutation in the \(g\)-th generation of \(_{j}(i)\). \(_{r1}^{g}(i)\), \(_{r2}^{g}(i)\), and \(_{r3}^{g}(i)\) are three randomly selected individuals from the current population who are different from each other, \(F\) is the scaling factor.

Thirdly, each individual crossovers with the mutated individuals with a certain probability of generating candidate individuals. The formula is as follows:

\[_{j}^{g}(i)=\{_{j}^{g}(i),&(0,1) CR,\\ _{j}^{g}(i),&.\] (4)

where \(CR\) is crossover probability factor, \(_{j}^{g}(i)\) is the \(i\)-th variable of the candidate individual in the \(g\)-th generation of the \(j\)-th individual in the population.

Fourthly, we use WordNet , a synonym dictionary, to measure the similarity between the target semantics and each individual's output text. WordNet groups words with the same semantics into synonyms, each representing a basic concept. We use WordNet to count the same semantic words \(m\) between each individual's output text and the target semantics. We calculate the \(Precision\)=\((m/t)\) and \(Recall\)=\((m/r)\), where \(t\) is the output text word count and \(r\) is the target semantics word count. Then, we calculate semantic similarity using the following formula:

\[S_{sem}=)^{})(^{2}+1) Precision  Recall}{^{2} Precision+Recall},\] (5)

where \(S_{sem}\) is the semantic similarity between the individual's output text and the target semantics , \(\) balances the precision and recall weights, \(\) and \(\) control the penalty factor strength, \(ch\) is the number of consecutive word sets that match between the output text and the target semantics, with fewer chunks meaning more consistent word order.

Ultimately, we select offspring based on \(S_{sem}\), choosing the current and candidate individuals that match the target semantics better as the next generation:

\[_{j}^{g+1}=\{_{j}^{g},&S_{sem}( (_{j}^{g}),TS) S_{sem}((_{j}^ {g}),TS),\\ _{j}^{g},&.\] (6)

where \(TS\) is the attacker's target semantics. We extract nouns, adjectives, and verbs from the output texts of all the more semantically relevant and preserved individuals of each generation, expanding the target semantic dictionary. We use \((_{j}^{g+1})=\{w_{1},w_{2},,w_{n}\}\) to represent the target model \(\)'s output text for the next generation of individuals, where \(w_{i}\) is the \(i\)-th word of the text and \(n\) is the word count. Then we use the following formula to extract important words and make a dictionary:

\[_{j}^{g+1}=\{w(_{j}^{g+1}) w\},\] (7)

where \(_{j}^{g+1}\) is the dictionary for the preserved individual. We combine the dictionaries of each preserved individual in each generation to get the target semantic dictionary \(=_{1}^{2}_{2}^{2}_{}^{m}\), where \(m\) is the total number of generation. The attacker selects words from dictionary \(\) that match the specified semantics to make the target text \(y_{t}\). Words in dictionary \(\) near input image \(\) in feature space enhance searchability, enabling more efficient targeted attacks.

### Attend Stage

The goal of _Attend_ is to calculate the target text's attention area on the image \(\). Because we do not have access to the internal information of the target model, we can only calculate the Grad-CAM attention heatmap  with the help of surrogate model \(f\) (such as ResNet trained in ImageNet). The surrogate model's sole purpose is to the compute attention heatmap. Since different models produce similar heatmaps for the same target text and input image, selecting a well-established visual model suffices . The calculation formula of attention heatmap \(\) is as follows:

\[(i,j)=(0,_{k}_{i}_{j} }}{_{k}(i,j)}_{k}(i, j)),\] (8)

[MISSING_PAGE_FAIL:6]

where \(E\) is the text encoder of the pre-trained CLIP model. Text is discrete and complex, so it cannot calculate the distance directly . Therefore, we use the CLIP text encoder \(E\) to extract the deep features of the texts, and then calculate the feature distance to obtain the similarity \(S_{clip}\) between the texts. The closer \(S_{clip}\) is to 0, the higher the similarity between the two texts \(u\) and \(v\).

Ultimately, we select offspring using the following formula:

\[_{j}^{g+1}=\{_{j}^{g},&S_{clip}( (_{j}^{g}),y_{t}) S_{clip}((_{j} ^{g}),y_{t}),\\ _{j}^{g},&.\] (13)

where \(_{j}^{g+1}\) is the next individual with closer feature distance between the output text and the target text \(y_{t}\). After performing the above evolutionary calculations multiple times, the optimal solution (adversarial sample) for outputting the target text is found.

## 4 Evaluation and Results

### Experiment setups

Model and datasetWe experimented with the two most-used image-to-text models on HuggingFace: VIT-GPT2 (Transformer-based)  and Show-Attend-Tell (CNN+RNN-based) . VIT-GPT2 was trained on ImageNet-21k. Show-Attend-Tell was trained on MSCOCO-2014. We only used the target model's output text, not its internal information like gradients, parameters, or word probability. Following this work , we used Flick30k as our dataset, which has 31783 images and 5 caption texts each. We removed samples with less than 0.7 similarities between predicted text and truth text to ensure the target model's accuracy on clean images.

Evaluation metricsWe used these evaluation metrics in our experiments: (1) BLEU(#4), an early machine translation metric that measures text precision . 1 means similar, and 0 means dissimilar. (2) METEOR, a more comprehensive metric that considers synonyms, stems, word order, etc . 1 means similar, and 0 means dissimilar. (3) CLIP, the distance between the CLIP text encoder's deep features for two texts . 1 means similar, and 0 means dissimilar. (4) SPICE, an evaluation metric tailored for image-to-text models . 1 means similar, and 0 means dissimilar. (5) \(\), the mean perturbation size of each pixel of the adversarial sample .

Figure 4: Grad-CAM attention heatmaps of different surrogate models for the same target text _a woman is holding a pair of shoes_. M is METEOR, B is BLEU, C is CLIP, S is SPICE.

Figure 3: We compared the convergence curves of populations with and without _Attend_ under the same perturbation size \(\) in (a-b). The fitness function is \(S_{clip}\) in Formula 12, where lower values mean stronger attacks. The dashed line is the average fitness value, and the solid line is the best fitness value. The green line is _AAA_ and the red line is _AAA w/o Attend_. (c) shows the attention heatmap. (d) and (e) show the visual effects of adversarial image with and without _Attend_, with minimal perturbation of 100% attack success rate.

### Experiment results

Comparison experiment of existing gray-box attacks.We evaluate state-of-the-art gray-box attacks [13; 10] on image-to-text models. We designate the gray-box attack  as transfer (gray) and the one  as transfer+query (gray). To simulate a black-box environment, we adapted these gray-box attacks by employing the CLIP model's image encoder in lieu of the target model's encoder, resulting in "transfer (black)" and "transfer+query (black)" variants. As depicted in Table 1, adversarial samples generated by the original gray-box attacks exhibit a marked increase in textual similarity to the target text when compared to clean samples. Conversely, the black-box adaptations maintain a similarity level akin to that of clean samples, indicating a significant loss of attack capability upon changing the image encoder. This underscores the dependency of gray-box attacks on the target model's image encoder. Our proposed method _AAA_ demonstrates superior attack performance in black-box scenarios compared to the existing methods in their native gray-box settings. This is attributed to the semantic loss inherent in existing gray-box attacks, which constrains their attacking potential. It is noteworthy that our work represents the first black-box attack on image-to-text models. So we can only compare our approach with existing gray-box attacks. We have adapted these gray-box attacks into a black-box version solely to demonstrate their ineffectiveness in a black-box scenario.

Ablation experiment of our black-box attack.We conducted ablation experiments on our _AAA_ method. _AAA_ (w/o _Attend_) means no attention heatmap to reduce the search space, but the proportional reduction of the search range. _AAA_ (w/o _Ask_) means the target text is not from the target semantic dictionary, but random words. Table 1 shows that losing any module decreases our attack performance. In addition, _Ask_ performs worse than _AAA_ (w/o _Attend_), indicating that finding a target text with lower search difficulty contributes relatively more to the performance of our targeted attack.

Qualitative experiment of attention.We presented the optimization curves of _AAA_ and _AAA_ (w/o _Attend_) in Figure 3. Figure 3 (a) and (b) illustrate the best and average fitness values during _AAA_ and _AAA_ (w/o _Attend_) optimization of VIT-GPT2 and Show-Attend-Tell. It is evident that the inclusion of _Attend_ expedites and enhances the convergence of the population, with an equivalent perturbation size. Consequently, _AAA_ exhibits more effective concealment in adversarial perturbations, maintaining the same level of attack efficacy, as depicted in Figures 3 (d) and (e). Furthermore, we evaluated the impact of selecting different surrogate models during _Attend_. Notably, the sole function of the surrogate model is to compute the attention heatmap. Figure 4 demonstrates that, despite significant structural variances among several surrogate models, they produce strikingly similar attention heatmaps for the same target text and input images. This similarity arises from mapping the target text to the most pertinent category within the surrogate model's label space (as Formula 9).

Figure 5: Performance of adversarial image attacks varies with perturbation size \(\). The \(\) of (a) and (f) is 25, \(\) of (b) and (g) is 15, \(\) of (c) and (h) is 10, \(\) of (d) and (i) is 5. (e) is our attention heatmap of the target text on the image. (j) is the target image generated based on the target text used in existing works. M is METEOR score, B is BLEU score, and C is CLIP score.

The position of the same category of objects on the same picture is constant, and the model needs to focus on the object first, no matter what structure it is . Performance comparisons, as shown in Figure 4, indicate that the similarity in attention heatmaps across different surrogate models leads to similar final attack performances. Therefore, we opted for a stable, well-established, pre-trained model, such as ResNet-50, to serve as our surrogate model.

Qualitative experiment of different perturbation sizes.We used the words _mirror_, _cell phone_, _man_, _looking at_ from the target semantic dictionary (as shown in Appendix B.2) to make the target text _a man is looking at a cell phone in a mirror_. We compared output texts of our black-box method _AAA_ and the existing gray-box method  for adversarial samples with different \(\), the average pixel perturbation size, in Figure 5. The same conclusion drawn from both methods is that bigger perturbation causes worse concealment and better attack performance; too small perturbation causes attack failure. Moreover, (f) and (j) in Figure 5 show that the existing methods have a semantic loss that limits their attack performance. Subjectively, target image (j) accurately draws the semantics of the target text, and the output text of adversarial image (f) perfectly describes the content of the target image (j). However the adversarial sample (f)'s output text does not have the semantics of the target text. Our method does not have semantic loss, so our black-box method _AAA_ does a better targeted attack than the existing gray-box method. More examples of semantic loss are in Appendix B.1.

Comparison experiment on computation time.We evaluated the computational efficiency of various attack methodologies for generating adversarial samples in image-to-text models. As depicted in Figure 6, our black-box attack method _AAA_, demonstrates a longer computation time to reach an optimal solution compared to existing gray-box attacks. For instance, the transfer approach  illustrated in Figure 6 (a) produces an adversarial sample with a CLIP score of 0.82 within a mere 29 seconds, while the transfer+query approach  achieves a CLIP score of 0.85 in just 97 seconds. Conversely, our _AAA_ method requires 151 seconds to generate an adversarial sample with a superior CLIP score of 0.951. The shorter computation times of the existing gray-box methods are expected due to their ability to access real gradients, which significantly expedites the optimization process. Given that adversarial attacks are not time-sensitive operations and considering that our _AAA_ method delivers a more potent attack capability and is applicable in a broader range of realistic black-box scenarios, the trade-off for a higher computational cost is deemed acceptable. Additional experiments on similarity measurements are included in the Appendix B.5.

Further analyses.Firstly, we show the impact of different forms of target semantics \(TS\) in _Ask_ on the target semantic dictionary, as shown in Appendix B.2. More ambiguous target semantics can enrich the target semantic dictionary, which also means that the attacker has more choices when designing \(y_{t}\). Secondly, we show the effect of different word selection strategies of \(y_{t}\) based on target semantic dictionary on the final attack effect, as shown in Appendix B.3. Thirdly, we compare the convergence curves of different population sizes and choose a population size of 40 based on the trade-off of attack performance and convergence efficiency, as shown in Appendix B.4. Furthermore, we compare the effects of different evolutionary algorithms on attack performance and convergence efficiency, as shown in Appendix B.6. Additionally, to better observe the attack effect of our framework, we show more examples of attention heatmaps \(\), optimization convergence curves, target text \(y_{t}\), and output text, as shown in Appendix B.7. Lastly, we discuss the limitations of our framework, defense strategies, and future work in Appendix C.

Figure 6: Comparison of computation time for generating a single adversarial sample using different adversarial attack methods. The y-axis is a measure of similarity between the generated text and the target text, with higher values indicating better target attack performance. The x-axis represents the computation time, and the shorter the time required to find a stable solution, the better.

Conclusion

In our research, we introduce a novel and practical approach for adversarial attacks on image-to-text models. We propose the _Ask_, _Attend_, _Attack_ (_AAA_) framework, a decision-based black-box attack method that achieves targeted attacks without semantic loss, even with access limited to the target model's output text. Our framework uses the target semantic directory to guide the creation of target text and attention heatmap to reduce the search space, thereby improving the efficiency of evolutionary algorithms and making our attack harder to detect. Our extensive experiments on the Transformer-based VIT-GPT2 model and the CNN+RNN-based Show-Attend-Tell model demonstrate that our decision-based black-box method outperforms existing gray-box methods in targeted attack performance. These findings highlight the vulnerabilities in current image-to-text models and underscore the need for more robust defense mechanisms, significantly contributing to the field of adversarial machine learning and enhancing the security of vision-language systems.