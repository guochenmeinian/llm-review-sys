# Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Discrete diffusion models with absorbing processes have shown promise in language modeling. The key quantities to be estimated are the ratios between the marginal probabilities of two transitive states at all timesteps, called the concrete score. In this paper, we reveal that the concrete score in absorbing diffusion can be expressed as conditional probabilities of clean data, multiplied by a time-dependent scalar in an analytic form. Motivated by the finding, we propose reparameterized absorbing discrete diffusion (RADD), a dedicated diffusion model that characterizes the time-independent conditional probabilities. Besides its simplicity, RADD can reduce the number of function evaluations (NFEs) by caching the output of the time-independent network when the noisy sample remains unchanged in a sampling interval. Empirically, RADD is up to 3.5 times faster while consistently achieving a better performance than the strongest baseline. Built upon the new factorization of the concrete score, we further prove a surprising result that the exact likelihood of absorbing diffusion can be rewritten to a simple form (named denoise cross-entropy) and then estimated efficiently by the Monte Carlo method. The resulting approach also applies to the original parameterization of the concrete score. It significantly advances the state-of-the-art discrete diffusion on 5 zero-shot language modeling benchmarks (measured by perplexity) at the GPT-2 scale.

## 1 Introduction

Auto-regressive models [1; 2; 3] have dominated the area of language modeling for many years. In particular, such models significantly benefit from large-scale transformers  and training data and have achieved remarkable progress [5; 6; 7; 8]. From a probabilistic perspective, the sequential sampling process of auto-regressive models is inefficient and limits the reasoning ability in nonsequential orders [9; 10]. Intrinsically, this is because such models characterize the joint distribution by the chain rule of probability, motivating research on developing other types of generative models for text.

Diffusion models [11; 12; 13] generate data in a coarse-to-fine manner efficiently [14; 15; 16; 17; 18] and all dimensions simultaneously, providing an appealing alternative to auto-regressive models. Among other efforts [19; 20; 21; 22; 23; 24; 20; 25; 26; 27; 28; 29] (see Section 5 for a comprehensive discussion), score entropy discrete diffusion (SEDD)  has shown promise in text generation. In particular, SEDD has achieved comparable results to auto-regressive models on 5 zero-shot language modeling benchmarks at the GPT-2 scale. Meanwhile, SEDD can reduce the number of function evaluations (NFEs) in sampling and fulfill text conditioned on prompts at different positions.

Technically, SEDD employs a discrete-state (absorbing) Markov process that adds noises to data by randomly replacing a token with a mask token \([]\) and then learns a reverse process to denoise from an entirely masked sentence. The key quantities to be estimated in SEDD are the ratios between the marginal probabilities of two transitive states at all timesteps, called the **concrete score**. SEDD alsoproposes a "scaling trick" (see details in Section 3) that scales the output of the score estimation by a factor. The trick has been proven very effective in practice yet not fully understood in theory .

One of our main contributions is to reveal that the concrete score in absorbing diffusion can be expressed as conditional probabilities of clean data, multiplied by a time-dependent scalar in an analytic form. Our finding theoretically explains the benefits of the scaling trick as a reparameterization for better optimization. Motivated by the finding, we propose reparameterized absorbing discrete diffusion (RADD), a dedicated diffusion model that characterizes the time-independent conditional probabilities by removing the time embedding from the score estimation in SEDD. Besides its simplicity, RADD can significantly reduce the NFEs by caching the output of the time-independent network when the noisy sample remains unchanged in a sampling interval (see Fig. 1).

Built upon the new factorization of the concrete score, we further prove a surprising result that the exact likelihood of absorbing diffusion can be rewritten to a simple form (named denoise cross-entropy, DCE) and then estimated efficiently by the Monte Carlo method. To establish the theory, we apply a change of variable from the time \(t\) to the probability that a single-dimensional token is masked at time \(t\) in the forward process. By integrating the probability variable analytically, we show that DCE enumerates all orders to decompose the joint distribution auto-regressively and accumulates log densities of all conditional distributions in every order, finishing the proof. Such theoretical findings enable exact likelihood evaluation and optimization for both the original parameterization of absorbing diffusion  and the proposed RADD.

Empirically, RADD is up to 3.5 times faster while consistently achieving a better performance than the strongest baseline, i.e. SEDD with the scaling trick . Further, the DCE loss applies to both RADD and SEDD for precise likelihood evaluation. It significantly advances the state-of-the-art discrete diffusion (i.e. SEDD ) on 5 zero-shot language modeling benchmarks (measured by perplexity) at the GPT-2 scale. The empirical evidence validates our theoretical findings.

In summary, this paper has several contributions:

* **Deeper understanding of discrete diffusion**: Both the factorization form of the concrete score and DCE loss for the exact likelihood computation reveal important yet overlooked theoretical properties of absorbing discrete diffusion, which explain the mysterious scaling trick, provide practice guidance, and may inspire future work.
* **Simplification**: By removing the time conditions, we reparameterize the model to focus on a time-independent conditional probability, simplifying the existing model.
* **Efficient sampling**: Leveraging the reparameterized form, RADD with a caching strategy is consistently faster while achieving a better performance than the strongest competitor.
* **Improved likelihood evaluation**: The exact likelihood evaluation approach significantly advances the state-of-the-art discrete diffusion on 5 zero-shot language modeling benchmarks (measured by perplexity) at the GPT-2 scale.

## 2 Background

In this section, we present preliminaries on continuous-time discrete diffusion models. We start with the one-dimensional case in Section 2.1, followed by the multi-dimensional case in Section 2.2.

### Single dimension

Let \(x\) denote a single dimensional sample with possible values in \(\{1,,N\}\). A continuous-time discrete Markov chain at time \(t\) is characterized by a transition rate matrix \(_{t}\) as follows

\[p_{t+ t|t}(|x)=_{t}(x,) t+o( t ),& x,\\ 1+_{t}(x,x) t+o( t),&=x, \]

where \(_{t}(x,)\) is the \((x,)\) element of transition rate matrix \(_{t}\), denoting the transition rate from state \(x\) to state \(\) at time \(t\). Equivalently, we can directly define \(_{t}(x,)\) as

\[_{t}(x,)=_{ t 0}( |x)}{ t},& x,\\ _{ t 0}(x|x)-1}{ t},&=x. \]Given the above definition, denote \(_{s t}(x,):=p_{t|s}(|x)\). The following Kolmogorov's forward equation holds [26; 30]:

\[_{s t}=_{s t}_{t}. \]

In practice [26; 29], \(_{t}\) is parameterized as \((t)\), where \((t)\) is a scalar function and \(\) is a constant matrix. In this case, the solution to Eq. (2.3) can be solved analytically as \(_{s t}=(((t)-(s)))\), where \((t)=_{0}^{t}(s)ds\) and \(\) is the matrix exponential. Therefore, we can directly sample \(_{t}\) from \(_{s}\) in one step for any \(t>s\).

Further, \(\) is often designed to diffuse towards a uniform distribution or an absorbing state \([]\). Recent work [20; 26] suggests that the absorbing matrix achieves better empirical performance. Besides, as detailed in Section 3, the specific structure of the absorbing matrix can be leveraged to improve performance and accelerate sampling. Therefore, we focus on the absorbing matrix as follows:

\[^{}=[-1&0&&0&1\\ 0&-1&&0&1\\ &&&&\\ 0&0&&-1&1\\ 0&0&&0&0]. \]

The time reversal of the forward process is characterized by a reverse transition rate matrix \(}_{t}\)[31; 32], whose element from state \(x\) to state \(\) is given by

\[}_{t}(x,)=()}{p_{t}(x)}_{t}(,x),& x,\\ -_{k x}}_{t}(x,k),&=x. \]

Simulating the reverse process requires to learn the reverse transition rate \(}_{t}(x,)\). As \(_{t}(x_{t},_{t})\) is known, it is sufficient to estimate the concrete score \((_{t})}{p_{t}(x_{t})}\) by a score network \(s_{}(x_{t},t)[(_{t})}{p_{t}(x_{t})}]_{_{ t}}\). Denoising score entropy (DSE)  is an effective objective to train the score network

\[_{0}^{T}_{ p_{t|0}( x_{0})}_{y}_{t}(,y)(s_{}(,t)_{y}- (y x_{0})}{p_{t|0}( x_{0}) } s_{}(,t)_{y}+K((y x _{0})}{p_{t|0}( x_{0})}))dt, \]

where \(K(a):=a a-a\). In particular, the DSE loss in Eq. (2.6) is an evidence lower bound (ELBO) of the negative log-likelihood with an unknown gap. Nevertheless, existing work  still employs it for training and likelihood evaluation.

After training, sampling from the model can be understood as discretizing the following process

\[_{s t}=_{s t}}_{t}, \]

where \(dt\) is an infinitesimal negative timestep and the concrete score is replaced by the score network. Existing samplers include the Euler method, Gillespie method, and Tweedie \(\) -leaping, as detailed in Appendix D.

### Multi-dimension

The multi-dimensional cases consider a state space of size \(d\) like \(^{d}=\{1,,n\}^{d}\). We denote the sample as a sequence of one-dimensional data, i.e. \(=x^{1} x^{d}\). The transition matrix \(_{t}^{n^{d} n^{d}}\) has an exponential number of possible states, making it expensive to reverse. To alleviate this issue, existing work [26; 29] assumes independence between dimensions and each dimension is a one-dimensional diffusion process with the same transition rate matrix \(_{t}^{}^{n n}\). Under the independent assumption, \(_{t}\) assigns zero values [26; 29] for all sequences with a Hamming distance larger than 1. According to Eq. (2.4), it is sufficient to model the concrete score between sequences that differ by a Hamming distance of 1, such as \(}_{t}=x_{t}^{1}_{t}^{i} x_{t}^{d}\) given \(_{t}=x_{t}^{1} x_{t}^{d}\). Therefore, the score network \(_{}(,t):\{1,,n\}^{d}^{d n}\) is defined as

\[_{}(_{t},t)_{}_{t}}=_{} (x_{t}^{1} x_{t}^{i} x_{t}^{d},t)[i,_{t}^{i }](x_{t}^{1}_{t}^{i} x_{t}^{d} )}{p_{t}(x_{t}^{1} x_{t}^{i} x_{t}^{d})},\]

which leads to the following expression to estimate the reverse transition rate matrix \(}_{t}\):

\[}_{t}(x_{t}^{1} x_{t}^{i} x_{t}^{d },x_{t}^{1}_{t}^{i} x_{t}^{d}) =_{t}^{}(_{t}^{i},x_{t}^{i} )(x_{t}^{1}_{t}^{i} x_{t}^{d} )}{p_{t}(x_{t}^{1} x_{t}^{i} x_{t}^{d})} \] \[_{t}^{}(_{t}^{i},x_{t}^{i} )_{}(x_{t}^{1} x_{t}^{i} x_{t}^{d},t) [i,_{t}^{i}]. \]

Existing samplers assume that each dimension is independent within a small interval \( t\) and update each dimension in parallel for efficiency [29; 26].

## 3 Reparameterized absorbing discrete diffusion

In Section 3.1, we reveal that the concrete score of absorbing discrete diffusion can be reparameterized as conditional distributions of clean data, which enables efficient sampling by caching the output of time-independent network (see Section 3.2) and exact likelihood computation (see Section 3.3) by applying the change of variable from time to the probability of being masked in a single dimension.

### Parameterizing the concrete score as conditional distributions of clean data

A key observation is that only the transition from the masked token to an unmasked token is valid in the reverse process of an absorbing discrete diffusion. In particular, according to the definition of the transition matrix of the absorbing process (see Eq. (2.4)), we have \(^{}(_{t}^{i},x_{t}^{i})=0\) for any unmasked \(x_{t}^{i}[]\) and \(_{t}^{i} x_{t}^{i}\). Therefore, the corresponding element in the transition matrix of the reverse process \(}_{t}\) (see Eq. (2.5)) equals zero. Namely,

\[}_{t}(x_{t}^{1} x_{t}^{i} x_{t}^{d},x_{t}^{1} _{t}^{i} x_{t}^{d})=(t)^{}(_{t}^{i},x_{t}^{i})(x_{t}^{1} _{t}^{i} x_{t}^{d})}{p_{t}(x_{t}^{1} x_{t}^{ i} x_{t}^{d})}=0, \]

for any unmasked state \(x_{t}^{i}[]\) and \(_{t}^{i} x_{t}^{i}\) and it is unnecessary to model the corresponding concrete score \((x_{t}^{1}_{t}^{i} x_{t}^{d})}{p_ {t}(x_{t}^{1} x_{t}^{i} x_{t}^{d})}\). Also, note that the concrete score always takes the value of one if \(_{t}^{i}=x_{t}^{i}\). Therefore, we only need to characterize the concrete score for \(x_{t}^{i}=[]\) and \(_{t}^{i}[]\).

Interestingly, in this case, we discover that the concrete score has a simple analytic form w.r.t. to the conditional distributions of clean data, as summarized in the following Theorem 1.

**Theorem 1**.: _(Analytic concrete score in absorbing case, proof in Appendix B) For \(_{t}=x_{t}^{i} x_{t}^{i} x_{t}^{d}\) and \(}_{t}=x_{t}^{1}_{t}^{i} x_{t}^{d}\), if \(x_{t}^{i}=[]\) and \(_{t}^{i}[]\), the concrete score at time \(t\) can be expressed as a time-independent conditional distribution at time zero multiplied by an analytic time-dependent term:_

\[(x_{t}^{1}_{t}^{i} x_{t}^{d})}{p_ {t}(x_{t}^{1} x_{t}^{i} x_{t}^{d})}=}{1-e^{-(t)}}p_{0}(}_{t}^{i}|_{t}^{ \) must model the whole time-dependent concrete score. In contrast, with the scaling trick, the reparameterized score \(}_{}(_{t},t)\) can focus on capturing the clean data distribution \(p_{0}(^{i}|_{t}^{})\) and simplifies learning, according to Theorem 1.

Further, Theorem 1 suggests that it is unnecessary to incorporate the time \(t\) in the reparameterized score, and the reparameterized score \(}_{}(_{t},t)\) should output a valid probability distribution. Motivated by the insights, we propose reparameterized absorbing discrete diffusion (RADD), which employs a network \(_{}(_{t})\) that removes the time condition from the input and takes the softmax as final nonlinearity. Formally, we can write our reparameterization as:

\[_{}(_{t},t)=}{1-e^{-(t)}}_{ }(_{t}). \]

In practice, we make a minimal modification of the score network in SEDD  for simplicity and fairness, detailed in Appendix F.1.

Moreover, RADD also enjoys a more efficient sampling process than SEDD  (with or without the scaling trick) based on its simplified parameterization, as presented below.

### Efficient samplers to reduce NFE by caching \(c_{}(x_{t})\)

For the reverse process of an absorbing discrete diffusion, once a token is generated from \([]\) to an unmasked token, it never transits to another token. Therefore, for a sequence \(_{t}\) of length \(d\), \(_{t}\) changes at most \(d\) times, irrespective of the number of sampling steps \(\). In the other steps, \(_{t}\) remains in all \(d\) dimensions. We highlight that we can cache \(_{}(x_{t})\) naturally without evaluating the time-independent \(_{}\) to reduce the NFE compared to SEDD (see Appendix E for the pseudo-code, ). As shown in Fig. 2, RADD with the caching strategy is more efficient than SEDD given any number of sampling steps, especially given large sampling steps. This is as expected because the NFE is limited within the generating sequence length.

Note that the NFEs with the caching strategy is a random variable. To quantify it, we calculate the expected NFEs (abbr. E-NFEs) required in an analytic form, conditioned on the sampling method, time steps, and noise schedule. Specifically, denote \(l\) as the generating sequence length, which does not equal \(d\) generally. Given the sampling time steps \(\{t_{0}=0,,t_{n}=T\}\), let \(N_{k}\{0,,l\}\) denote the number of changed dimensions of \(\) in \([t_{k-1},t_{k})\). Since we perform function evaluation

Figure 1: **Expected number of function evaluations (E-NFE) over a different number of sampling steps. E-NFE is measured by Tweedie \(\)-leaping method with log-linear noise schedule.**

Figure 2: **Sample quality measured by perplexity (\(\)). We compare SEDD with Euler and Tweedie \(\)-leaping (abbr. T-\(\)) samplers, and RADD with Euler sampler. We show E-NFE for RADD with caching and NEF otherwise.**

[MISSING_PAGE_FAIL:6]

where \(p_{}(}|_{0})\) is the joint distribution induced by masking each dimension in \(_{0}\) independently with a probability \(\).

In the second step, we demonstrate that the integral w.r.t. \(\) in Eq. (3.10) can be integrated analytically, and the DSE loss can be rewritten as expectations over the number and positions of masks as follows:

\[^{}_{}(_{0})=d_{k U(\{1,,d \})}_{} U(}_{k})}[ _{}^{i}=[]}- q_{}(_{0}^{i}|}^{})], \]

where we denote \(}_{k}:=\{}:}} }k[]\}\) and \(U()\) as uniform distribution.

Finally, in the third step, we prove that Eq. (3.11) enumerates all orders to decompose the joint distribution auto-regressively and accumulates log densities of all conditional distributions in every order. Therefore, it is equivalent to the negative log-likelihood of \(q_{}\):

\[^{}_{}(_{0})=- q_{}(_{0}). \]

Theorem 2 enables **exact likelihood computation** for both the original model \(_{}\) and our \(_{}\), providing a more accurate measure of model performance. Take \(_{}\) for example, Eq. (3.8) can be rewritten as a form of expectation on \(t\):

\[^{T}_{}(_{0})=_{t U([0,T]) }_{} p_{t|0}(_{0})}_{ }}_{t}(},)(- (_{0})}{p_{t|0}(}_{0} )}((t)}}{1-e^{-(t)}}_{ }()_{})). \]

Naturally, we can take the Monte Carlo estimation of \(^{T}_{}(_{0})\) by sampling \(t\) to approximate \(- q_{}(_{0})\) according to Eq. (3.13). In addition, it can be used as an efficient and valid training target for discrete diffusion models, as an alternative to the ELBO (i.e. DSE loss). For pseudo-code of training, see Appendix E.

## 4 Experiments

We present the experimental setups in Section 4.1. We then evaluate the performance of accelerated generation in Section 4.2 and zero-shot perplexity on various language datasets in Section 4.3.

### Settings

Model.We use RADD model \(_{}\) reparameterzied as described in Section 3.1. Compared with SEDD small model, RADD model has 7M fewer parameters due to the removal of time-condition, which equates to an 8% decrease from the original 90M non-embedding parameters. We trained our RADD model \(_{}\) using denoising score entropy and denoising cross entropy, abbreviated as RADD-DSE and RADD-DCE. For SEDD small model, we employed their pre-trained model.

Data.In line with the methodology outlined by SEDD, we trained on the OpenWebText  dataset and tested on the LAMBADA, WikiText2, PTB, WikiText103, and One Billion Words datasets . For data splits and data processing, we adopted the same settings and techniques as SEDD, which involves packing sentences to generate uniform-length blocks as model input.

Training setup.We used the same training setup for RADD and SEDD. Specifically, we used a log-linear noise schedule where the expectation of the number of changed tokens at time \(t\) is linear with \(t\). For simplicity, we also used the same optimization configuration as SEDD, which can be suboptimal for our RADD model and DCE loss. For more details see Appendix F.

Metric.Following previous work , we conduct experiments on unconditional generation and language modeling tasks. For generation, we use perplexity (PPL) on unconditional samples measured by an additional larger language model (i.e. GPT-2 large) to evaluate sample quality. To access inference efficiency, we computed the inference time on a single NVIDIA 4090 GPU with a batch size of 8 and averaged over 1024 samples. For language modeling tasks, we report the perplexity calculated on the dataset with different models.

### Efficient sampling

We compare the sample quality measured by perplexity between SEDD and our RADD-DCE model, as shown in Fig. 2. For a fixed NFE, RADD-DCE with the Euler sampler outperforms SEDD with multiple samplers. It suggests that RADD with caching accelerates the sampling process and benefits sample quality at the same time. Besides, the acceleration by cache strategy is particularly significant with large sampling steps, as analyzed in Section 3.2.

We further compare the running time for the methods in Table 1. Across all sampling steps, RADD consistently requires the shortest sampling time and outperforms SEDD with different samplers. Quantitatively, RADD achieves a speed-up of \(2.5 3.5\) times as shown in Table 1. These results agree with the analysis of the E-NFEs in Fig. 1, validate the effectiveness of RADD and caching strategy, and demonstrate the practical implications of our Theorem 1.

According to Eq. (3.11), we can also use RADD as an auto-regressive model to generate samples in different orders, leading to worse performance as a discrete diffusion, as detailed in Appendix F.4. We present more sampling details in Appendix F.3. and the generated samples in Appendix G.1.

### Improved zero-shot perplexity on language modeling

Following SEDD, we present zero-shot perplexities on the LAMBADA, WikiText2, PTB, WikiText103, and 1 Billion Words datasets  in Table 2 and compare the zero-shot perplexity of our model with other baseline models [20; 38; 29].

Firstly, we conduct an ablation study of the scaling trick in the middle of the Table 2. With an absorbing process, the perplexity of the scaled version of SEDD outperforms its unscaled version, which matches our theoretical discovery in Theorem 1.

Secondly, without any modification of the model, we estimate the exact likelihood of the baseline model SEDD  based on Theorem 2 in Table 2. We observe that perplexity is consistently better than the ELBO of the strongest discrete diffusion models, which validates our Theorem 2.

Lastly, we report the maximum likelihood training results of RADD in the last row in Table 2. We observed that RADD-DCE outperforms RADD-DSE, but their performances are slightly worse than SEDD. This discrepancy could be because we did not search the hyperparameters and directly applied identical optimization configures as SEDD, which may be suboptimal.

## 5 Related work

Continuous-state diffusion models for text generation.Several works have been proposed to apply continuous diffusion to text [19; 21; 22; 23]. Li et al.  use an embedding layer to map discrete tokens to a latent space and learn a continuous-state diffusion on it. Bit Diffusion  learns a continuous diffusion model to generate binary bits of discrete tokens. However, transforming between these continuous representations and discrete tokens by thresholding may lose information. Bayesian Flow Network  achieves competitive log-likelihood on character-level language modeling tasks

   Methods & Metrics & 32 & 64 & 128 & 256 & 512 & 1024 & 2048 & 4096 \\  SEDD (euler) & Time(s) & 0.48 & 0.87 & 1.67 & 3.25 & 6.41 & 12.74 & 25.42 & 50.86 \\  & PPL & 155 & 105 & 81 & 66 & 53 & 43 & 35 & 28 \\  SEDD (T-\(\)) & Time(s) & 0.38 & 0.68 & 1.28 & 2.47 & 4.85 & 9.61 & 19.14 & 38.20 \\  & PPL & 151 & 104 & 81 & 65 & 52 & 42 & 34 & 28 \\  RADD & Time(s) & **0.33** & **0.54** & **0.94** & **1.68** & **2.97** & **5.15** & **8.73** & **14.88** \\  & PPL & **135** & **94** & **72** & **58** & **46** & **37** & **30** & **26** \\   

Table 1: **Average inference time of a single sample with varying sampling steps.** The table compares the average inference time (in seconds) for the SEDD small model using both Euler and Tweedie \(\)-leaping (abbreviated as T-\(\)) sampling methods, and the RADD small model using the Euler method with a caching strategy.

and is proven equivalent to continuous stochastic differential equations trained by denoising score matching . Such models underperform auto-regressive models on standard text generation tasks.

Discrete-state diffusion models for text generation.Several discrete-state diffusion models have been proposed [11; 39; 20]. D3PM  proposed a diffusion framework based on any probability transition matrix and trained with a lower bound of log-likelihood. DiffusionBERT  utilizes a pre-trained BERT  as an initialization of diffusion. Furthermore,  generalizes the framework to continuous time by introducing a rate matrix. It is difficult to apply the score matching in such models because the gradient of the data distribution is undefined. Several works try to generalize the score matching on discrete data [29; 28; 26; 27]. Meng et al.  introduce the concrete score and the denoising concrete score matching loss. Furthermore, SEDD bridges the discrete state diffusion and the concrete score by introducing a denoising score entropy loss . By incorporating an absorbing process, SEDD achieves competitive performance with the auto-regressive models, especially, GPT-2.

## 6 Conclusion

We introduce RADD, a dedicated discrete diffusion model that characterizes the time-independent conditional probabilities, built upon a new factorization form of the concrete score. RADD is much more efficient by reducing the NFEs with a cache strategy while retaining a better performance than strong baselines. Furthermore, we propose DCE loss and prove it is equivalent to the negative log-likelihood of absorbing diffusion. When applied to SEDD, DCE significantly advances the state-of-the-art discrete diffusion on 5 zero-shot language modeling benchmarks at the GPT-2 scale.

Limitaition.Our model has been trained and evaluated primarily on the GPT-2 scale. For broader applicability, it is essential to explore the effects of scaling on the performance , which is left as future work. The success of diffusion transformers on images [42; 43; 44] and videos  suggests that diffusion models can be scaled up by incorporating transformers.

Another limitation is that our model can only generate full-length outputs, unlike auto-regressive models that can produce variable-length outputs. This restricts the flexibility of our model in certain applications. We leave the investigation on this issue as future work.

**Social impact.** For the current theoretical and experimental scope of this paper, we have not found any direct social impacts. However, considering future developments, the paper potentially contributes to the next-generation large language models. In this context, this work could significantly reduce the inference cost of language models but may also lead to hallucinations, amplify biases and discrimination in the data, and pose risks of misuse. As with other generative models, addressing these issues requires further advancements in the field.

   Method & LAMBADA & WikiText2 & PTB & WikiText103 & 1BW \\  GPT-2 & **45.04** & 42.43 & 138.43 & 41.60 & **75.20** \\  D3PM\({}^{}\) & \(\)93.47 & \(\)77.28 & \(\)200.82 & \(\)75.16 & \(\)138.92 \\ PLAID\({}^{}\) & \(\)57.28 & \(\)51.80 & \(\)142.60 & \(\)50.86 & \(\)91.12 \\ SEDD-Uniform\({}^{}\) & \(\)65.40 & \(\)50.27 & \(\)140.12 & \(\)49.60 & \(\)101.37 \\  SEDD-U\({}^{}\) & \(\)52.21 & \(\)44.75 & \(\)130.49 & \(\)43.14 & \(\)80.70 \\ SEDD-S\({}^{}\) & \(\)50.92 & \(\)41.84 & \(\)114.24 & \(\)40.62 & \(\)79.29 \\  SEDD-S\({}^{}\) (**Ours**) & 50.44 & **39.91** & **110.01** & **39.91** & 78.01 \\  RADD-DSE\({}^{}\) (**Ours**) & 96.62 & 43.35 & 125.03 & 40.34 & 80.11 \\ RADD-DCE\({}^{}\) (**Ours**) & 56.67 & 42.83 & 116.74 & 41.02 & 79.00 \\   

Table 2: **Zero-shot language modeling perplexity (\(\)) on five datasets. \({}^{}\) labels the results based on ELBO which is taken from [20; 38; 29] and \({}^{}\) labels the results based on the exact likelihood implemented by us. In this table, SEDD-U / SEDD-S refer to the unscaled and scaled absorbing models respectively.**