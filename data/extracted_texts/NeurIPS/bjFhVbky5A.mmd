# Lsh-MoE: Communication-efficient MoE Training via Locality-Sensitive Hashing

Xiaonan Nie\({}^{1}\) Qibin Liu\({}^{1}\) Fangcheng Fu\({}^{1}\) Shenhan Zhu\({}^{1}\) Xupeng Miao\({}^{2}\)

Xiaoyang Li\({}^{3}\) Yang Zhang\({}^{3}\) Shouda Liu\({}^{3}\) Bin Cui\({}^{1}\)

\({}^{1}\)Peking University \({}^{2}\)Purdue University \({}^{3}\)ByteDance

\({}^{1}\){xiaonan.nie,2101212782,ccchengff,shenhan.zhu,bin.cui}@pku.edu.cn

\({}^{2}\)xupeng@purdue.edu \({}^{3}\)lixiaoyang.x,zhangyang.elfin,liushouda}@bytedance.com

###### Abstract

Larger transformer models always perform better on various tasks but require more costs to scale up the model size. To efficiently enlarge models, the mixture-of-experts (MoE) architecture is widely adopted, which consists of a gate network and a series of experts and keep the training cost constant by routing the input data to a fixed number of experts instead of all. In existing large-scale MoE training systems, experts would be distributed among different GPUs for parallelization, and thus input data requires additional all-to-all communications to access the target experts and conduct corresponding computations. However, upon evaluating the training process of three mainstream MoE models on commonly used GPU clusters, we found that the all-to-all communication ratio averaged around 45%, which significantly hinders the efficiency and scalability of training MoE models. In this paper, we propose LSH-MoE, a communication-efficient MoE training framework using locality-sensitive hashing (LSH). We first present the problems of scaling MoE training in existing systems and highlight the potential of exploiting token similarity to facilitate data compression. Then, we introduce an efficient LSH-based compression technique, which utilizes the cross-polytope hashing for rapid clustering and implements a residual-based error compensation scheme to alleviate the adverse impact of compression. To verify the effectiveness of our methods, we conduct experiments on both language models (e.g., RoBERTa, GPT, and T5) and vision models (e.g., Swin) for pre-training and fine-tuning tasks. The results demonstrate that our method substantially outperforms its counterparts across different tasks by \(1.28\) - \(2.2\) of speedup.

## 1 Introduction

In recent years, large-scale pre-trained models have significantly advanced the performance of deep learning across various complex tasks, including computer vision , natural language processing , and multi-modal learning . Commonly referred to as foundation models, these pre-trained models are primarily built on Transformer architectures  and undergo extensive pre-training on large datasets, utilizing substantial GPU resources. OpenAI has validated the scaling law for large language models  and suggests that increasing the model's parameter size, the volume of training data, and the duration of training can significantly enhance the model's performance. However, this approach results in a considerable rise in training costs, making the development of foundation models extremely expensive.

To reduce the high computational costs, the sparse mixture-of-experts (MoE) architecture is often adopted, which comprises a sparse gate network and a series of expert networks. This architecture routes input data to only a subset of experts, resulting in sparse activation of the experts and thereby reducing the model's computational FLOPs (float point operations) as well as training costs. Prominent models such as Google's Switch-Transformer , ST-MoE , Meta's Hash Layer  and Mistral-AI's mitral models  have successfully implemented this design, demonstrating improvements in both performance and efficiency with MoE models.

Meanwhile, effectively scaling the training of MoE models across hundreds or even thousands of GPUs remains a significant challenge. Researchers from Google have proposed the _expert parallelism_ approach , which replicates the gating network on each GPUs and distributes different experts across multiple GPUs for parallel processing. Specifically, each input token is initially processed by the gating network to select the appropriate expert, after which it is routed to the designated experts via peer-to-peer (P2P) network communication. Once the designated experts complete their computation, the token is returned to the original GPU for further processing through an additional P2P communication. Since each GPU typically needs to exchange data with many other GPUs, these P2P transmissions results in an all-to-all communication pattern. Moreover, because the computation of the expert network relies on the outcomes of these communications, the communications cannot be effectively overlapped with ongoing computations. This dependency creates a significant performance bottleneck in model training across most commonly used GPU clusters. We conducted experiments on three widely-used MoE models, including RoBERTa-MoE, GPT-MoE and Swin-MoE, on four A100 servers, each with a cross-machine bandwidth of 200Gb/s. The results, as shown in Figure 3, reveal that the time cost of all-to-all communication constitutes an average of \(45\%\) and can reach up to \(67\%\) of the total model training time.

Existing methods to improve distributed MoE training on bandwidth-limited clusters tackle communication challenges in various ways. TA-MoE  reduces cross-machine communication by adjusting the gating network to favor experts on the same server, while Pre-gated MoE  reduces dependency between communication and computation through a pre-gating mechanism that plans token routing in advance. However, both approaches require modifications to the gating mechanism and model structure, limiting their universal applicability. DeepSpeed-MoE  introduces PR-MoE, which selects one expert plus a shared expert, halving the all-to-all communication load. SCoMoE  organizes all-to-all communication by structuring data transfers along different dimensions and controlling data volumes across network levels, and also clusters tokens to improve routing. However, none of these works consider reducing the All-to-All communication volume in MoE training by compressing the forward activations. Therefore, they can be intergrated with our method for further improvement.

In this paper, we present LSH-MoE, a communication-efficient MoE training framework that leverages locality-sensitive hashing to group similar tokens. Our key contributions are as follows:

* We begin by identifying key challenges in scaling MoE training in existing systems, noting that all-to-all communication constitutes an average of \(45\%\) of the total training time. Additionally, we investigate the potential of using token similarity to facilitate data compression to reduce communication costs.
* We propose an efficient LSH-based compression technique that employs cross-polytope hashing for rapid clustering. This approach transmits only the clustering centroids, significantly reducing communication costs. To further enhance accuracy, we implement a residual-based error compensation scheme to mitigate the negative effects of compression.
* \(2.2\) in end-to-end training time.

## 2 Background

### Mixtures-of-Expert Architecture

To enhance the training efficiency of Transformer models, William et al. (2022)  introduced an innovative paradigm, the sparse mixture-of-experts (MoE) architecture, illustrated in Figure 1.

This architecture effectively balances parameter capacity and training costs, and comprises two key components: an _expert network_ (\(\)) and a _sparse gate network_ (\(\)). It is evident that MoE models, with an equal number of active parameters per input, can significantly surpass the performance of dense models. This breakthrough has also catalyzed further research and their application across various industries, as highlighted by numerous subsequent studies .

The _expert network_\(\) is composed of multiple specialized and separate networks, commonly referred to as _experts_, denoted as \(\{E_{i}\}_{i=1}^{N}\), where \(N\) represents the number of experts. Additionally, \(E_{i}(x)\) denotes the output produced when the input \(x\) is processed by the \(i\)-th expert. Each expert is trained to excel in a specific sub-task, such as in multi-task learning, or to handle specific segments of data, as seen in language modeling and multi-modal learning, thereby increasing the overall model capacity. In foundational models, the MoE layer often serves as a substitute for the traditional feed-forward network (FFN) layer. Within each MoE layer, each FFN function works as an individual expert, significantly enhancing the model's capability to process diverse and complex data inputs.

The _gating network_\(\) plays a crucial role in the sparse MoE architecture. For example, in a \(K\)-way gated MoE system, the gating network outputs a set of integers as Equation 1 to determine which experts should be activated. This decision is based on the characteristics of the input itself, allowing for a dynamic and efficient allocation of computational resources. By only processing each input token with a selected subset of the expert network, the MoE model achieves computation sparsity, effectively decoupling parameter capacity from training costs.

\[:^{M}[1,N]^{K}\] (1)

Through the integration of multiple specialized experts, as described by Equation 2, the sparse MoE model is capable of delivering more accurate and efficient predictions as \(f(x)\). This is achieved by leveraging the specialized knowledge embedded within each expert, combined with the strategic input allocation managed by the gating network.

\[f(x)_{i(x)}E_{i}(x)\] (2)

While MoE's primary advantage is decoupling parameter capacity from network cost, a key challenge lies in learning the gating parameters effectively, as the output's sparsity makes it non-differentiable. Consequently, much of the research in the MoE field has centered on developing methods for learning gating functions. These methods fall into three main categories, as outlined in : routing via learnable weighting , deterministic hash routing , and reinforcement learning-based routing . These approaches primarily differ in the design of the gating network \(\) rather than the expert network \(\), and therefore all encounter similar scaling challenges.

### Challenges of Scaling MoE Model Training

While MoE models were initially developed to facilitate efficient scaling during training, deploying these large-scale models in practical GPU-intensive environments poses significant challenges in distributed computing. Specifically, the MoE layer harbors a considerably higher number of parameters and requires additional memory, yet it maintains almost the same computational demands as the dense layer. This leads to a unique _compute density_ -- defined as the ratio of the layer's FLOPs (Floating Point Operations) to its number of parameters. Therefore, traditional parallelism methods such as _tensor parallelism_ and _pipeline parallelism_ are insufficient for achieving effective parallelism in the scenarios of MoE training.

To improve the efficiency and scalability of training large-scale MoE models, _expert parallelism_ has been introduced as a specialized model parallelism strategy. This approach distributes experts within an MoE layer across multiple GPUs, while leveraging data parallelism for replicating non-MoE layers, thus efficiently managing the training workload of MoE models. The workflow of distributed training for an MoE layer is depicted in Figure 2. Once the target expert for each token is determined, an all-to-all communication process is triggered to distribute tokens to their corresponding target experts for computations, denoted as \(E_{i}(x)\). Subsequently, another round of all-to-all communication is executed to gather the outputs from all experts, which produces the MoE layer's output (represented as \(f(x)\), Equation 2). Subsequent operations involve executing the data-parallel non-MoE layers.

We first profiled the training process of three popular MoE models employing expert parallelism (detailed in Table 1) on a cluster comprised of four A100 machines, each equipped with an interconnect RDMA bandwidth of 200Gb/s. The proportion of all-to-all communication time relative to the total training duration is illustrated in Figure 3(a). We then double the number of machines, and the number of experts to increase the model scale. The results are shown in Figure 3(b) and 3(c), respectively. Our findings reveal that all-to-all communication accounted for a substantial portion of the total time: approximately \(30\%\) in GPT-MoE (15B), \(40\%\) in RoBERTa-MoE, and \(70\%\) in Swin-MoE-L. And this overhead remains nearly constant in larger models and at larger machine scales. These results highlight a significant bottleneck that hampers the scalability of the training process. Consequently, the duration of all-to-all communication substantially constrains training with expert parallelism, leading to reduced overall throughput and limiting the potential to scale up the number of experts effectively.

### Locality-Sensitive Hashing Algorithms

Locality-Sensitive Hashing (LSH) is a probabilistic method primarily used to approximate nearest neighbor search in high-dimensional spaces, which reduces the dimensionality of data by mapping similar data to the same "buckets" with high probability using hash functions. This approach offers a substantial reduction in computational complexity, particularly beneficial for large-scale data applications. The key operations in LSH including:

**Mapping Data into Buckets:** The core of LSH is a family of hash functions that maximize the probability of nearby points in the original space staying close in the hashed space, while distant points are likely to end up in different buckets. Each hash function \(h\) is characterized by the property: \(P[h(x)=h(y)]=1-d(x,y)/D\), where \(d(x,y)\) is the distance between points \(x\) and \(y\), and \(D\) denotes the diameter of the space. To map similar data into the same bucket, multiple hash functions from this family are selected based on the specific attributes of the data (e.g., Euclidean distance, cosine similarity) and the desired granularity of the buckets. Data points are then hashed by these

Figure 3: Proportion of all-to-all communication time relative to total training duration across different configurations: scaling the number of training servers (Figure 3(b)) and scaling the parameter size of models (Figure 3(c)).

functions, and each point is assigned to buckets according to its hash values, effectively categorizing similar items together for clustering.

**Calculating Cluster Centroids:** By grouping data points into buckets as determined by their hash values, data points are effectively clustered. Each bucket represents a cluster of data points and the centroid of each cluster is then calculated as the mean of all points within that cluster, formulated as \(C_{j}=_{i=1}^{n_{j}}x_{i}\), where \(C_{j}\) is the centroid of the j-th bucket, \(n_{j}\) is the number of points in the j-th bucket, and \(x_{i}\) are the data points in the bucket.

## 3 Methodology

### The Motivation of Token Similarity

To explore the potential optimization for all-to-all communications in MoE training, we conducted an in-depth analysis of the data involved in these all-to-all communications, identifying a high degree of similarity, termed _token similarity_. Specifically, we applied _Principal Component Analysis_ (PCA) to reduce the dimensionality of the input tokens of all-to-all communications and observed a distinct clustering phenomenon, as illustrated in the Figure 4. Our analysis suggests that the observed similarity among tokens may stem from two primary factors:

* _Data Related Influences_: The similarity is partially due to the nature of real-world data, which often adheres to Zipf's Law . This results in a skewed distribution, with certain data elements appear more frequently than others.
* _Model Structure Related Influences_: The design of Transformer architecture , especially its attention mechanisms, significantly impacts token similarity. In models like BERT , attention layers are designed to capture and integrate context information across tokens, thus homogenizing token representations and emphasizing their shared semantic relationships at the sentence level.

Figure 4: Principal Component Analysis (PCA) Visualization of input tokens involved in all-to-all communication.

Figure 5: Schematic of MoE training with Locality-Sensitive Hashing (LSH-MoE).

### Lsh-MoE

Motivated by the _Token Similarity_ observed in Section 3.1, we introduce LSH-MoE, a novel MoE training framework that integrates locality-sensitive hashing (LSH) for rapid clustering of input tokens. Our method transmits only the clustering centroids, significantly reducing communication volumes. To counteract the negative effects of compression, we also implement a residual-based error compensation scheme.

As depicted in Figure 5, LSH-MoE initially employs (1) an LSH-based clustering method to compress _tokens_ into _centriods_ for subsequent processing, effectively reducing communication overhead. It then sequentially executes (2) all-to-all communication, expert computation, and another (3) all-to-all communication to produce the processed outputs _E(centriods)_. Finally, it introduces (4) a residual-based error compensation method to approximate the expert-processed results _E(tokens)_, by integrating _E(centriods)_ with _residuals_. Meanwhile, we also outline the workflow of our LSH-MoE framework in the Algorithm 1 of Appendix A.1. The key components of our LSH-MoE framework includes **an efficient LSH-based clustering algorithm** for rapid processing and **an residual-based error compensation scheme** to minimize quality degradation.

Efficient LSH-based Clustering Algorithm.Since the data to be compressed (the input data for all-to-all communication) is generated dynamically and in real time, pre-compressing it or overlapping compression time with other processing tasks is not feasible. Consequently, selecting an efficient online compression algorithm is crucial. Traditional clustering algorithms, such as K-Means, often encounter computational challenges and efficiency limitations. Locality-sensitive hashing (LSH) address these issues by hashing similar data points into the same buckets, enabling faster similarity detection in high-dimensional spaces.

Numerous LSH algorithms have been developed, each employing a unique hashing approach for mapping data onto buckets. We conducted experiments to evaluate several popular hashing algorithms, including _cross-polytope hashing_ and _spherical hashing_. Based on our evaluations in Section 4.5, we selected _cross-polytope hashing_ as the optimal algorithm for our application. _Cross-polytope hashing_ stands out for its method of mapping input vectors to the nearest vertex on a cross-polytope. This process is facilitated by applying randomly rotated cross-polytopes, which effectively segment the surface of the unit sphere. The algorithm can be mathematically represented as follows:

\[LSH()=*{argmax}_{i\{ 1, 2,, d\}}||_{i}\] (3)

where \(\) is a random rotation matrix, \(d\) is the dimensionality of the space, and \(||_{i}\) denotes the absolute value of the \(i\)-th component of the rotated vector \(\).

This formula encapsulates how the input vector \(x\) is transformed by the rotation matrix \(R\) and then mapped to the nearest vertex of the cross-polytope by selecting the dimension \(i\) that maximizes the absolute value of the components of \(Rx\). This method effectively segments the high-dimensional space and enhances the clustering efficiency by rapidly identifying similar data points.

Residual-based Error Compensation Scheme.In our LSH-MoE framework, we compress the intermediate activation values within the model network. Unlike gradient compression, this process does not tolerate errors well. Therefore, it is essential to minimize compression-induced errors to ensure minimal impact on model performance. To address this, we implement a novel residual-based gradient compensation strategy, outlined as follows:

1. We first capture the residual for each data point relative to its cluster centroid, defined by the equation: \[_{j}\{x-}_{j} x _{j}\}.\] (4)
2. After the expert network computes outputs for the cluster centers, the final step is to restore the processed result for each token by adding back the previously recorded residual: \[Y_{ij}\{E(}_{j})+_{jk}  k=1,2,,N_{j}\}.\] (5)

This error compensation scheme effectively mitigates potential accuracy loss caused by data compression in all-to-all communication, ensuring the fidelity and robustness of the LSH-MoE framework. The experimental results in Section 4 show that implementing this compensation mechanism enablesthe model trained with LSH-MoE to achieve an accuracy comparable to that of a model trained without compression. This outcome highlights the effectiveness of our proposed error compensation strategy in preserving model performance despite the challenges posed by data compression in all-to-all communication.

### Scalability Analysis of LSH-MoE

To effectively demonstrate the scalability of our approach, particularly in terms of its applicability to both larger models and larger computational clusters, we conducted a theoretical analysis. This analysis primarily focuses on the **computation overhead** and the communication costs associated with Mixture of Experts (MoE), specifically considering **all-to-all communication overhead.** We derived the ratio of communication time to computation time, highlighting how this ratio evolves as both the scale of the servers and the model size increase. This relationship is crucial for understanding scalability and can be formally expressed as follows:

\[}{T_{compute}}=}{6B_{inter}} \] (6)

where \(k\) represents the number of experts activated per token, FLOPs and \(B_{inter}\) denote the GPU's computation ability and the network performance, \(w\) is the number of GPU servers, and \(h\) is the hidden size of model. Notably, the first term, \(}{6B_{inter}}\), remains constant under fixed hardware conditions. Additionally, scaling MoE models typically emphasizes increasing the number of layers and experts, while the growth in hidden size (\(h\)) tends to be gradual, as seen in models like Switch-Transformer . Consequently, when both the model scale and the number of training servers grow, the proportion of all-to-all communication time remains nearly unchanged. This insight underpins the scalability of the LSH-MoE method, demonstrating its robustness in larger-scale settings and supporting its potential in future large-scale applications. For a detailed derivation, please refer to Appendix A.2.

## 4 Experiment

### Implementation

Our LSH-MoE comprises a data compression/restoration component and a communication component. We utilize PyTorch 1.11 for developing the LSH clustering and NCCL for implementing the communication. Additionally, our method is framework-independent and can be easily applied to other MoE training frameworks such as Hetu-MoE [21; 26], DeepSpeed-MoE , and Tutel .

### Benchmarks and Datasets

Our evaluations are conducted by scaling pre-trained models equipped with MoE architecture across various application domains. This includes models like RoBERTa-MoE, T5-MoE and GPT-MoE in natural language processing (NLP), as well as Swin-MoE in computer vision (CV). Among these models, RoBERTa-MoE and T5-MoE are evaluated on pre-training task, while GPT-MoE and Swin-MoE undergo fine-tuning evaluation based on their official open-sourced model checkpoints 12. We also evaluated the zero-shot accuracy of the pre-trained T5-MoE. Model configurations are detailed in Table 1.

  
**Model** & **\#Layer** & **d\({}_{}\)** & **d\({}_{}\)** & **\#Exports** & **\#Params. (MoE)** & **\#Params. (Total)** \\   RoBERTa-MoE & 12 & 768 & 3072 & 16 & 302M & 394M \\ T5-MoE & 16 & 1024 & 16384 & 16 & 8594M & 9288M \\ GPT-MoE (15B) & 12 & 768 & 3072 & 512 & 14507M & 14629M \\ GPT-MoE (52B) & 24 & 1024 & 4096 & 512 & 51539M & 51740M \\ Swin-MoE-L & 24 & - & - & 32 & - & 946M \\   

Table 1: Models for evaluation, where “-” indicates that the values are different across layers.

The RoBERTa-MoE model is pre-trained with masked language modeling tasks on a combined dataset, which includes BooksCorpus (\(\) 800M words) and English Wikipedia (\(\) 2,500M words). This dataset is tokenized using a tokenizer with a vocabulary size of 50,257. To assess the impact of our MoE method in compressing all-to-all communication on large model training, the T5-MoE model, which is with about 10B parameters, is pre-trained on an industry dataset (\(\) 500M words) using a span-masked language modeling task. In addition to pre-training tasks, we further evaluate our work on fine-tuning tasks. To be specific, we fine-tune two open-sourced models, including the language model GPT-MoE on the General Language Understanding Evaluation (GLUE) benchmark and the vision model Swin-MoE on the ImageNet classification benchmark.

### Software and Hardware Environments

To thoroughly evaluate the effectiveness of our method, we conducted experiments on two clusters, V100 cluster and A100 cluster. Additionally, to ensure consistency in software versions, we performed experiments on both machines using the same docker image.

**Software Environment.** Our experiments were conducted using a docker image built upon the official NVIDIA GPU containers, which includes Ubuntu 20.04, CUDA 11.3, cuDNN 8.2.0, and NCCL 2.12.7, accessible at NVIDIA GPU Containers 3.

**V100 Cluster.** The first hardware environment includes two servers, each outfitted with eight NVIDIA V100 (32GB) GPUs. Within each server, GPUs are interconnected using NVLink 2.0 technology. The servers are interconnected via an RDMA NIC, providing a network bandwidth of 100 Gbps.

**A100 Cluster.** The second hardware environment consists of four servers, each equipped with eight NVIDIA A100 (40GB) GPUs. Within these servers, GPUs utilize NVLink 3.0 technology for interconnection. The servers are linked through two RDMA NICs, enhancing the network bandwidth to 200 Gbps.

We allocated the experiments involving RoBERTa-MoE and GPT-MoE to the V100 cluster, while T5-MoE and Swin-MoE were tested on the A100 cluster. This setup allowed us to effectively compare the performance impacts across different hardware configurations.

### Overall Performance

In general, to evaluate our LSH-MoE training approach, which compresses communication data, there are two crucial questions:

1. Does the LSH-MoE method enable normal model convergence, and is there a risk of increased loss variability during this process, potentially leading to instability in training?
2. Might the implementation of the LSH-MoE method adversely affect the model's performance on downstream benchmarks?

Therefore, we conducted experiments focusing on both **Convergence Performance** and **Benchmark Performance** to validate the effectiveness of our method. In this section, due to the necessity of selecting several hyperparameters for LSH, such as the type of hash function and the quantity of hash functions, we have opted for the cross-polytope hash function based on empirical evaluation, setting the number of hash functions at 6. A detailed examination of the effects stemming from variations in these parameters will be methodically addressed in the upcoming ablation study (Section 4.5).

**Convergence Performance.** We pre-trained the RoBERTa-MoE and T5-MoE using open-source datasets and industrial datasets, respectively. In our approach, we substitute the FFN (Feed-Forward Network) layer with an MoE (Mixture of Experts) layer in alternating layers, as detailed in Section 4.2. We meticulously tracked the time required to achieve equivalent model performance levels (perplexity) during training, as depicted in Figure 6. The results indicate a significant acceleration in training convergence when employing the LSH-MoE method: \(1.6\) faster for RoBERTa-MoE and \(2.2\) faster for T5-MoE, compared to the original models' convergence rates. Furthermore, we investigated the role of error compensation in this process. Our findings reveal that omitting error compensation in the LSH-MoE model led to a 0.3 point increase in perplexity, given the same training duration. This observation underscores the efficacy of the error compensation algorithm.

**Benchmark Performance.** To better validate the performance of LSH-MoE on downstream tasks, we fine-tuned the GPT-MoE and Swin-MoE on different datasets using open-source model checkpoints, and evaluated zero-shot performance of our internal pre-trained T5-MoE model, adhering to their original architectural designs that incorporate Top-2 gating, as detailed in [12; 17; 1].

We first utilized the LSH-MoE method for fine-tuning the GPT-MoE of two model scales (i.e. 15B and 52B) on the GLUE benchmark, yielding impressive outcomes. As detailed in Table 2, the implementation of the LSH-MoE method substantially reduced communication overhead while maintaining nearly the same level of accuracy. This strategy resulted in a significant performance boost, achieving an acceleration rate ranging from \(1.2\) to \(1.5\). The results also demonstrate that as the parameter size of MoE models increases, LSH-MoE continues to achieve significant improvements without compromising model accuracy. Additionally, we report the zero-shot accuracy of the pre-trained T5-MoE, showing that the T5-MoE models trained with LSH-MoE achieved accuracy comparable to standard T5 models, confirming LSH-MoE's efficacy in pretraining. Because the limited number of tokens in the pre-trained dataset and its out-of-domain nature compared to the GLUE evaluation data, the zero-shot performance metrics are relatively low.

Furthermore, our evaluation of the LSH-MoE method in fine-tuning the Swin-MoE on the ImageNet-1K dataset demonstrated noteworthy efficiency. We achieved a communication compression rate of \(11.7\%\), which led to a \(1.28\) increase in acceleration, as reported in Table 3. Notably, this was accomplished while preserving almost the same level of accuracy.

### Ablation Study

To study the impact of the quantity and types of hash functions, we conducted ablation experiments by fine-tuning the GPT-MoE (15B) model on the MNLI and SST-2 datasets in the GLUE benchmark.

**Impact of the Quantity of Hash Functions.** We controlled the number of hash functions to indirectly adjust the LSH compression rate, exploring its effect on model performance. Specifically, we utilized 2, 4, 6, 8, and 10 hash functions. As shown in the middle sub-figure in Figure 7, we observe that an increase in the number of hash functions enlarges the number of buckets, enhances data distinction and, consequently, the compression rate. Besides, we indicate from the left sub-figure in Figure 7, that more hash functions leads to improved model convergence quality and worse compression rate.

    &  &  &  \\  & Origin & Ours & Speed & Origin & Ours & Speed & Origin & Ours \\   SST-2 & 93.8\% & 93.8\% & 1.3\(\) & 94.5\% & 94.3\% & 1.4\(\) & 51.6\% & 50.9\% \\ MNLI & 82.8\% & 82.7\% & 1.4\(\) & 84.1\% & 84.3\% & 1.4\(\) & 52.6\% & 52.1\% \\ QNLI & 86.6\% & 86.7\% & 1.3\(\) & 90.2\% & 90.0\% & 1.5\(\) & 49.5\% & 50.0\% \\ QQP & 88.8\% & 88.7\% & 1.3\(\) & 88.9\% & 88.9\% & 1.2\(\) & - & - \\ MRPC & 71.3\% & 71.1\% & 1.3\(\) & 76.3\% & 76.1\% & 1.3\(\) & - & - \\ COLA & 72.3\% & 72.4\% & 1.4\(\) & 73.5\% & 73.8\% & 1.5\(\) & - & - \\   

Table 2: Evaluation of LSH-MoE on the GLUE benchmark.

Figure 6: Comparative analysis of convergence performance. This includes a comparison between the original models, LSH-MoE without Error Compensation, and LSH-MoE implementations. The perplexity curves are applied 1D Gaussian smoothing with \(=0.5\).

    & Origin & Ours \\  Top-1 Acc. \(\) & 84.7\% & 84.5\% \\ Top-5 Acc. \(\) & 97.0\% & 97.1\% \\ Compression Rate & — & 11.7\% \\ Sample/s & 184.3 & 236.6 \\   

Table 3: Results of fine-tuning Swin-MoE on the ImageNet-1K dataset.

Importantly, our results indicate that a compression rate of approximately 20% (achieved with about 6 hash functions) is optimal for maintaining nearly identical convergence as uncompressed models. Therefore, we choose 6 as the default number of hash functions in Section 4.4.

**Impact of the Types of Hash Functions.** We further explored the impact of the types of hash functions with Cross-Polytope Hashing (CP) and Spherical-Plane Hashing (SP). The outcomes are illustrated in the right sub-figure in Figure 7. CP generally achieves better convergence than SP at the same compression rate. This is attributable to CP's ability to more effectively handle a variety of complex data patterns. CP encodes data based on an n-dimensional cross-polytope, while SP relies on the geometric relationships between spheres and planes. Thus, CP is more generalizable across a variety of complex data patterns while SP performs better with data that has spherical distribution characteristics. Other works (e.g. Reformer ) also use CP to leverage the sparsity of attention mechanisms. Therefore, we finally choose **cross-polytope hashing** as the default type of hash functions in Section 4.4.

## 5 Conclusion

Our study tackled the latency challenges inherent in training sparse-gated Mixture-of-Experts (MoE) models with our innovative LSH-MoE framework. Utilizing locality-sensitive hashing to harness token similarities, our approach significantly reduces communication overhead. The integration of a residual-based error compensation scheme further preserves model integrity under compression. Empirical tests across various models, including RoBERTa, GPT, T5, and Swin, showcase LSH-MoE's capability to accelerate both pre-training and fine-tuning phases by up to \(2.2\), paving the way for efficient and scalable MoE applications in real-world settings.

## 6 Limitations

At the current stage, our work only considers MoE models. Nevertheless, we want to clarify that MoE models are also a mainstream class of deep learning models that are increasingly adopted due to rising model computational demands and training costs, such as Mixtral-7Bx8MoE, DeepSeek-MoE, and GPT-4. Hence, accelerating MoE training is indeed a critical direction. Additionally, the core of our work leverages data redundancy, which is also presented in non-MoE model training. We hope our observations and utilization of data redundancy can inspire more refined work in optimizing training for non-MoE models as well.

Figure 7: An in-depth analysis of the compression rate and the model performance by adjusting the **quantity** and **types** of hash functions. The left and middle sub-figures are results for diverse quantities of hash functions. The right sub-figure is the result for diverse types of hash functions (CP for cross-polytope and SP for spherical) with different compression rates (20%, 15%, 10%).