# Near Optimal Reconstruction

of Spherical Harmonic Expansions

 Amir Zandieh

Independent Researcher

amir@zed512.gmail.com

&Insu Han

Yale University

insu.han@yale.edu

&Haim Avron

Tel Aviv University

haimav@tauex.tau.ac.il

###### Abstract

We propose an algorithm for robust recovery of the spherical harmonic expansion of functions defined on the \(d\)-dimensional unit sphere \(^{d-1}\) using a near-optimal number of function evaluations. We show that for any \(f L^{2}(^{d-1})\), the number of evaluations of \(f\) needed to recover its degree-\(q\) spherical harmonic expansion equals the dimension of the space of spherical harmonics of degree at most \(q\), up to a logarithmic factor. Moreover, we develop a simple yet efficient kernel regression-based algorithm to recover degree-\(q\) expansion of \(f\) by only evaluating the function on uniformly sampled points on \(^{d-1}\). Our algorithm is built upon the connections between spherical harmonics and Gegenbauer polynomials. Unlike the prior results on fast spherical harmonic transform, our proposed algorithm works efficiently using a nearly optimal number of samples in any dimension \(d\). Furthermore, we illustrate the empirical performance of our algorithm on numerical examples.

## 1 Introduction

We consider the fundamental problem of recovering a function from a finite number of (noisy) observations. To provide accurate and reliable predictions at unobserved points we need to avoid overfitting which is typically achieved through restricting our estimator or interpolant to a family of _smooth or structured_ functions. In this paper, we focus on interpolating square-integrable functions on the \(d\)-dimensional unit sphere, with low-degree spherical harmonics, a critical task in scenarios where rotational invariance is a fundamental property. Spherical harmonics are essential in various theoretical and practical applications, including the representation of electromagnetic fields , gravitational potential , cosmic microwave background radiation  and medical imaging , as well as modelling of 3D shapes in computer graphics  and computer vision . Further notable real-life applications include molecular/atom systems, where understanding the underlying functions within a spherical context can significantly enhance predictive modeling and simulation accuracy .

We begin by observing that any function \(f\) in \(L^{2}(^{d-1})\), i.e., the family of square-integrable functions on the sphere \(^{d-1}\), can be uniquely decomposed into orthogonal spherical harmonic components. Specifically, if we denote the space of spherical harmonics of degree \(\) in dimension \(d\) by \(_{}(^{d-1})\), any \(f L^{2}(^{d-1})\) has a unique orthogonal expansion \(f=_{=0}^{}f_{}\) with \(f_{}_{}(^{d-1})\) (Lemma 2). With this observation, we aim to find the best spherical harmonic approximation of degree \( q\) to \(f\) using minimal number of samples (essentially treating higher order terms in \(f\)'s expansion as noise).

**Problem 1** (Informal Version of Problem 2).: _For an unknown function \(f L^{2}(^{d-1})\) and an integer \(q 1\), efficiently (both in terms of number of samples from \(f\) and computations) learn the first \(q+1\) spherical harmonic components \(\{f_{}_{}(^{d-1})\}_{=0}^{q}\) of \(f\) which minimize_

\[\|_{=0}^{q}f_{}-f\|_{^{d-1}}^{2}:=_{ ^{d-1}}|_{=0}^{q}f_{}(w)-f(w)|^{2}dw.\] (1)

The _angular power spectrum_ of \(f\) commonly obeys a power law decay of the form \(\|f_{}\|_{^{d-1}}^{2}(^{-s})\), for some \(s>0\), depending on the order of differentiability of \(f\). In fact, for any infinitely differentiable \(f\), \(\|f_{}\|_{^{d-1}}^{2}\) decays asymptotically faster than any rational function of \(\). Furthermore, for any real analytic \(f\) on the sphere, \(\|f_{}\|_{^{d-1}}^{2}\) decays exponentially. Thus, the first \(q+1\) spherical harmonic components of \(f\) typically well approximate \(f\) for even modest \(q\), and answering Problem 1 is meaningful for a wide range of differentiable functions.

### Our Main Results

We reformulate Problem 1 as a least-squares regression and then solve it using randomized numerical linear algebra techniques. We first consider an orthonormal projection operator that maps functions in \(L^{2}(^{d-1})\) onto the space of bounded-degree spherical harmonics \(_{=0}^{q}_{}(^{d-1})\). Specifically, if \(_{d}^{(q)}\) is an operator that maps any \(f\) with expansion \(f=_{=0}^{}f_{}\) where \(f_{}_{}(^{d-1})\), onto \(f\)'s first \(q+1\) expansion components, i.e., \(_{d}^{(q)}f=_{=0}^{q}f_{}\), then Problem 1 can be formulated as

\[_{g L^{2}(^{d-1})}\|_{d}^{(q)}g-f\|_{ ^{d-1}}^{2}.\]

However, solving this regression problem with "continuous" cost function is challenging. To avoid such continuous optimizations, we adopt the approach of  which discretizes the aforementioned regression problem according to the leverage scores of the operator \(_{d}^{(q)}\). It turns out that if we can draw random samples with probabilities proportional to the leverage scores of \(_{d}^{(q)}\) then we can recover the degree-\(q\) spherical harmonic expansion of \(f\), i.e. \(_{=0}^{q}f_{}\), with finite number of observations. Particularly, by exploiting the connections between spherical harmonics and _Zonal (Gegenbauer) Harmonics_ and the fact that zonal harmonics are the reproducing kernels of \(_{}(^{d-1})\) (Lemma 3), we prove that the leverage scores of \(_{d}^{(q)}\) are constant everywhere on the sphere \(^{d-1}\). Thus, solving a discrete regression problem with uniformly sampled observations yields a near-optimal solution to Problem 1. Informal statements of our results are as follows.

**Theorem 1** (Informal Version of Theorem 5).: _Let \(_{q,d}\) be the dimension of spherical harmonics of degree at most \(q\), i.e., \(_{q,d}(_{=0}^{q}_{}( ^{d-1}))\). There exists an algorithm that finds a \((1+)\)-approximation to the optimal solution of Problem 1, given \(s=(_{q,d}_{q,d}+^{-1}_{q,d})\) observations of function \(f\) at uniformly sampled points on \(^{d-1}\), with \((s^{2}d+s^{})\)1 runtime._

We also prove that our bound on the number of required samples is optimal up to a logarithmic factor.

**Theorem 2** (Informal Version of Theorem 6).: _Any (randomized) algorithm that takes \(s<_{q,d}\) samples on any input fails with probability greater than \(9/10\), where \(_{q,d}(_{=0}^{q}_{}( ^{d-1}))\)._

### Related Work

Efficient reconstruction of functions as per Problem 1 has been extensively studied in various fields. Many prior papers considered reconstructing \(1\)-dimensional functions from finite number of samples on a finite interval under smoothness assumption about the underlying function. Notably, the influential line of work of  focused on reconstructing Fourier-bandlimited functions and  considered interpolating Fourier-sparse signals. Recently,  unified these reconstruction methods in dimension \(d=1\) and gave a universal sampling framework for reconstructing nearly all classes of functions with Fourier-based smoothness constraints. One can view \(1\)-dimensional functions on a finite interval as functions on the unit circle\(^{1}\). Thus, Problem 1 is indeed a generalization of the aforementioned prior work to high dimensions under the assumption that the _generalized Fourier series_ (Lemma 2) of the underlying function only contains bounded-degree spherical harmonics. This degree constraint on spherical harmonic expansions can be viewed as the \(d\)-dimensional analog of Fourier-bandlimitedness on circle \(^{1}\).

Computing the spherical harmonic expansion in dimension \(d=3\) has received considerable attention in physics and applied mathematics communities. The algorithms for this special case of Problem 1 are known in the literature as "fast spherical harmonic transform" . Most notably,  proposed an algorithm for computing spherical harmonic expansion of degree \( q\) to precision \(\) using \((_{g,3})\) samples and \((_{g,3}_{g,3}.(1/))\) time. These fast algorithms were developed based on fast Fourier and associated Legendre transforms and make use of a (well-conditioned) orthogonal basis for \(_{}(^{d-1})\), which happened to be the associated Legendre polynomials when \(d=3\). However, it is in general very difficult to compute an orthogonal basis for spherical harmonics , so unlike our Theorem 1, it is inefficient to extend these prior results to higher \(d\).

From the techniques point of view, a related work is , which employs harmonic analysis over \(^{d-1}\) to analyze the generalization of two-layered neural tangent kernels. They show that an unknown function defined on \(^{d-1}\) can be efficiently recovered using kernel regression w.r.t. neural tangent kernel on uniform random samples from the function. However, the number of samples that  requires for recovering bounded degree spherical harmonics, especially when the degrees are high, is sub-optimal and is strictly worse than our result. Additionally,  does not guarantee recovery with relative error, while our Theorem 5 provides relative error guarantees.

Furthermore, recent applications of Gegenbauer polynomials, along with other orthonormal polynomials like Hermite polynomials, have been found in designing efficient random features for approximating various kernel functions. These applications extend to dot-product kernels , Neural Tangent Kernels , and Gaussian kernels .

## 2 Mathematical Preliminaries

We denote by \(^{d-1}\) the unit sphere in \(d\) dimension. We use \(|^{d-1}|=}{(d/2)}\) to denote the surface area of sphere \(^{d-1}\) and \((^{d-1})\) to denote the uniform probability distribution on \(^{d-1}\). We denote by \(L^{2}(^{d-1})\) the set of all square-integrable real-valued functions on sphere \(^{d-1}\). Furthermore, for any \(f,g L^{2}(^{d-1})\) we use the following definition of the inner product on the unit sphere2,

\[ f,g_{^{d-1}}_{^{d-1}}f(w)g(w) dw=|^{d-1}|}_{w( ^{d-1})}[f(w)g(w)].\] (2)

The function space \(L^{2}(^{d-1})\) is complete with respect to the norm induced by the above inner product, i.e. \(\|f\|_{^{d-1}}^{d-1}}}\), so \(L^{2}(^{d-1})\) is a _Hilbert space_.

We often use the term _quasi-matrix_ which is informally defined as a "matrix" in which one dimension is finite while the other is infinite. A quasi-matrix can be _tall_ (or _wide_) meaning that there is a finite number of columns (or rows) where each one is a functional operator. For a more formal definition, see .

_Spherical Harmonics_ are the solutions of Laplace's equation in spherical domains and can be thought of as functions defined on \(^{d-1}\) employed in solving partial differential equations. Formally,

**Definition 1** (Spherical Harmonics).: _For integers \( 0\) and \(d 1\), let \(_{}(d)\) be the space of degree-\(\) homogeneous polynomials with \(d\) variables and real coefficients. Let \(_{}(d)\) denote the space of degree-\(\) harmonic polynomials in dimension \(d\), i.e., homogeneous polynomial solutions of Laplace's equation:_

\[_{}(d):=\{P_{}(d): P=0\},\]

_where \(=}{ x_{1}^{2}}++}{  x_{d}^{2}}\) is the Laplace operator on \(^{d}\). Finally, let \(_{}(^{d-1})\) be the space of (real) Spherical Harmonics of order \(\) in dimension \(d\), i.e. restrictions of harmonic polynomials in \(_{}(d)\) to the sphere \(^{d-1}\). The dimension of this space, \(_{,d}(_{}(^{d-1}))\), is_

\[_{0,d}=1,\ \ _{1,d}=d,\ \ _{,d}=- \ \ \  2.\]

### Gegenbauer Polynomials

The _Gegenbauer_ (a.k.a. _ultraspherical_) _polynomial_ of degree \( 0\) in dimension \(d 2\) is given by

\[P_{d}^{}(t):=_{j=0}^{/2}c_{j} t^{ -2j}(1-t^{2})^{j},\] (3)

where \(c_{0}=1\) and \(c_{j+1}=- c_{j}\) for \(j=0,1,,/2-1\). These polynomials are orthogonal on the interval \([-1,1]\) with respect to the measure \((1-t^{2})^{}\), i.e.,

\[_{-1}^{1}P_{d}^{}(t) P_{d}^{^{}}(t)(1-t^{2})^{ }\,dt=^{d-1}|}{_{,d} |^{d-2}|}_{\{=^{} \}}.\] (4)

Zonal Harmonics.The Gegenbauer polynomials naturally provide positive definite dot-product kernels on \(^{d-1}\) known as _Zonal Harmonics_, which are closely related to the spherical harmonics. The following reproducing property of zonal harmonics plays a crucial role in our analysis.

**Lemma 1** (Reproducing Property of Zonal Harmonics).: _Let \(P_{d}^{}()\) be the Gegenbauer polynomial of degree \(\) in dimension \(d\). For any \(x,y^{d-1}\):_

\[P_{d}^{}( x,y)=_{,d}_{w (^{d-1})}[P_{d}^{}( x,w)P_{d}^{}( y,w) ],\]

_Furthermore, for any \(^{}\):_

\[_{w(^{d-1})}[P_{d}^{ }( x,w) P_{d}^{^{}}(  y,w)]=0.\]

The proof of this and all subsequent results can be found in the appendix. The following useful fact, known as the addition theorem, connects Gegenbauer polynomials and spherical harmonics.

**Theorem 3** (Addition Theorem).: _For every integer \( 0\), if \(\{y_{1}^{},y_{2}^{},,y_{_{,d}}^{}\}\) is an orthonormal basis for \(_{}(^{d-1})\), then for any \(,w^{d-1}\) we have_

\[}{|^{d-1}|} P_{d}^{}( ,w)=_{j=1}^{_{,d}}y_{j}^{ }() y_{j}^{}(w).\]

## 3 Reconstructing \(L^{2}(^{d-1})\) Functions via Spherical Harmonics

In this section we show how to approximate any function \(f L^{2}(^{d-1})\) by spherical harmonics using the optimal number of samples. We begin with the fact that spherical harmonics form a complete set of orthonormal functions and thus form an orthonormal basis for the Hilbert space of square-integrable functions on sphere \(^{d-1}\). This is analogous to periodic functions, viewed as functions defined on the circle \(^{1}\), which can be expressed as a linear combination of circular functions (sines and cosines) via the Fourier series.

**Lemma 2** (Direct Sum Decomposition of \(L^{2}(^{d-1})\)).: _The family of spaces \(_{}(^{d-1})\) yields a Hilbert space direct sum decomposition \(L^{2}(^{d-1})=_{=0}^{}_{ }(^{d-1})\): the summands are closed and pairwise orthogonal, and every \(f L^{2}(^{d-1})\) is the sum of a converging series (in the sense of mean-square convergence with the \(L^{2}\)-norm defined in Eq. (2)),_

\[f=_{=0}^{}f_{},\]

_where \(f_{}_{}(^{d-1})\) are uniquely determined functions. Furthermore, given any orthonormal basis \(\{y_{1}^{},y_{2}^{},,y_{_{,d}}^{}\}\) of \(_{}(^{d-1})\) we have \(f_{}=_{j=1}^{_{,d}} f,y_{j}^{}_{ ^{d-1}} y_{j}^{}\)._The series expansion in Lemma 2 is the analog of the Fourier expansion of periodic functions, and is known as "_generalized Fourier series_"  with respect to the Hilbert basis \(\{y_{j}^{}:j[_{,d}], 0\}\). We remark that it is in general intractable to compute an orthogonal basis for the space of spherical harmonics , which renders the generalized Fourier series expansion in Lemma 2 primarily existential. While finding the generalized Fourier expansion of a function \(f L^{2}(^{d-1})\) is computationally intractable, our goal is to answer the next fundamental question, which is about finding the projection of a function \(f\) onto the space of spherical harmonics, i.e., the \(f_{}\)'s in Lemma 2. Concretely, we seek to solve the following problem.

**Problem 2**.: _For an integer \(q 0\) and a given function \(f L^{2}(^{d-1})\) whose decomposition over the Hilbert sum \(_{=0}^{}_{}(^{d-1})\) is \(f=_{=0}^{}f_{}\) as per Lemma 2, let us define the low-degree expansion of this function as \(f^{(q)}:=_{=0}^{q}f_{}\). How efficiently can we learn \(f^{(q)}_{=0}^{q}_{}(^{d-1})\)? More precisely, we want to find a set \(\{w_{1},w_{2},,w_{s}\}^{d-1}\) with minimal cardinality \(s\) along with an efficient algorithm that given samples \(\{(w_{i})\}_{i=1}^{s}\) can interpolate \(f()\) with a function \(^{(q)}_{=0}^{q}_{}(^{d -1})\) such that:_

\[\|^{(q)}-f^{(q)}\|_{^{d-1}}^{2} \|f^{(q)}-f\|_{^{d-1}}^{2}.\]

For ease of notation, we denote the Hilbert space of spherical harmonics of degree at most \(q\) by \(^{(q)}(^{d-1}):=_{=0}^{q} _{}(^{d-1})\). To answer Problem 2 we exploit the close connection between the spherical harmonics and Gengenbauer polynomials, and in particular the fact that zonal harmonics are the reproducing kernels of the Hilbert spaces \(_{}(^{d-1})\).

**Lemma 3** (A Reproducing Kernel for \(_{}(^{d-1})\)).: _For every \(f L^{2}(^{d-1})\), if \(f=_{=0}^{}f_{}\) is the unique decomposition of \(f\) over \(_{=0}^{}_{}(^{d-1})\) as per Lemma 2, then \(f_{}\) is given by_

\[f_{}()=_{,d}*{}_{w (^{d-1})}[f(w)P_{d}^{}( ,w)]^{d-1}.\]

Now we define a kernel operator, based on the low-degree Gegenbauer polynomials, which projects functions onto their low-degree spherical harmonic expansion.

**Definition 2** (Projection Operator onto \(^{(q)}(^{d-1})\)).: _For any integers \(q 0\) and \(d 2\), define the kernel operator \(_{d}^{(q)}:L^{2}(^{d-1}) L^{2}( ^{d-1})\) as follows: for \(f L^{2}(^{d-1})\) and \(^{d-1}\),_

\[[_{d}^{(q)}f]():=_{=0}^{q}}{|^{d-1}|} f,P_{d}^{}( ,)_{^{d-1}}= _{=0}^{q}_{,d}*{}_{w (^{d-1})}[f(w)P_{d}^{}( ,w)].\] (5)

_This is an integral operator with kernel function \(k_{q,d}(,w):=_{=0}^{q}}{|^{d -1}|} P_{d}^{}(,w)\)._

Note that the operator \(_{d}^{(q)}\) is self-adjoint and positive semi-definite. Moreover, using the reproducing property of this kernel we can establish that \(_{d}^{(q)}\) is a projection operator.

**Claim 1**.: _The operator \(_{d}^{(q)}\) defined in Definition 2 satisfies the property \((_{d}^{(q)})^{2}=_{d}^{(q)}\)._

Furthermore, by the addition theorem (Theorem 3), \(_{d}^{(q)}\) is trace-class (i.e., the trace is finite and independent of the choice of basis) because:

\[(_{d}^{(q)}) =_{^{d-1}}k_{q,d}(w,w)\,dw=_{=0}^{q}}{|^{d-1}|}_{^{d-1}}P_{d }^{}( w,w)\,dw\] \[=_{=0}^{q}_{,d}=+-1.\] (6)

By combining Theorem 3 and Lemma 2, and using the definition of the projection operator \(_{d}^{(q)}\), it follows that for any function \(f L^{2}(^{d-1})\) with Hilbert sum decomposition \(f=_{=0}^{}f_{}\), the low-degree component \(f^{(q)}=_{=0}^{q}f_{}^{(q)}(^{d-1})\) can be computed as \(f^{(q)}=_{d}^{(q)}f\). Equivalently,in order to learn \(f^{(q)}\), it suffices to solve the following least-squares regression problem,

\[_{g L^{2}(^{d-1})}\|_{d}^{(q)}g-f\|_{ ^{d-1}}^{2}.\] (7)

If \(g^{*}\) is an optimal solution to the above regression problem then \(f^{(q)}=_{d}^{(q)}g^{*}\). In the next claim we show that solving the least squares problem in Eq. (7), even to a coarse approximation, is sufficient to solve our interpolation problem (i.e., Problem 2):

**Claim 2**.: _For any \(f L^{2}(^{d-1})\), any integer \(q 0\), and any \(C 1\), if \( L^{2}(^{d-1})\) satisfies,_

\[\|_{d}^{(q)}-f\|_{^{d-1}}^{2} C _{g L^{2}(^{d-1})}\|_{d}^{(q)}g-f \|_{^{d-1}}^{2},\]

_and if we let \(f^{(q)}_{d}^{(q)}f\), where \(_{d}^{(q)}\) is defined as per Definition 2, then the following holds_

\[\|_{d}^{(q)}-f^{(q)}\|_{^{d-1}}^{2} (C-1)\|f^{(q)}-f\|_{^{d-1}}^{2}.\]

Claim 2 shows that solving the regression problem in Eq. (7) approximately provides a solution to our spherical harmonics interpolation problem (Problem 2). But how can we solve this least-squares problem efficiently? Not only does the problem involve a possibly infinite dimensional parameter vector \(g\), but the objective function also involves the continuous domain on the surface of \(^{d-1}\).

### Randomized Discretization via Leverage Function Sampling

We solve the continuous regression in Eq. (7) by randomly discretizing the sphere \(^{d-1}\), thereby reducing our problem to a regression on a finite set of points \(w_{1},w_{2},,w_{s}^{d-1}\). In particular, we propose to sample points on \(^{d-1}\) with probability proportional to the so-called _leverage function_, a specific distribution that has been widely applied in randomized algorithms for linear algebra problems on discrete matrices . We start with the definition of the leverage function for compact operators such as \(_{d}^{(q)}\):

**Definition 3** (Leverage Function).: _For integers \(q 0\) and \(d>0\), we define the leverage function of the operator \(_{d}^{(q)}\) (see Definition 2) for every \(w^{d-1}\) as follows,_

\[_{q}(w):=_{g L^{2}(^{d-1})}\|_{d}^{(q)}g \|_{^{d-1}}^{-2}|[_{d}^{(q)}g ](w)|^{2}.\] (8)

Intuitively, \(_{q}(w)\) is an upper bound of how much a function that is spanned by the eigenfunctions of the operator \(_{d}^{(q)}\) can "blow up" at \(w\). The larger the leverage function \(_{q}(w)\) implies the higher the probability we will be required to sample \(w\). This ensures that our sample points well reflect any possibly significant components, or "spikes", of the function. Ultimately, the integral \(_{^{d-1}}_{q}(w)\,dw\) determines how many samples we require to solve the regression problem Eq. (7) to a given accuracy. It is a known fact that the leverage function integrates to the rank of the operator \(_{d}^{(q)}\) (which is equal to the dimensionality of the Hilbert space \(^{(q)}(^{d-1})\)). This ultimately allows us to achieve a \(}(_{=0}^{q}_{,d})\) sample complexity bound for solving Problem 2. To compute the leverage function, we make use of the following useful alternative characterization of the leverage function.

**Lemma 4** (Min Characterization of the Leverage Function).: _For any \(w^{d-1}\), let \(_{q}(w)\) be the leverage function (Definition 3) and define \(_{w} L^{2}(^{d-1})\) by \(_{w}()_{=0}^{q}}{|^{d-1 }|}P_{d}^{}(,w)\). We have the following minimization characterization of the leverage function:_

\[_{q}(w)=\{_{g L^{2}(^{d-1})}\|g\|_{^{d-1}}^ {2},\ \ _{d}^{(q)}g=_{w}\}.\] (9)

Using the min and max characterizations of the leverage function we can find upper and lower bounds on this function. Surprisingly, in this case the upper and lower bounds match, so we actually have an exact value for the leverage function.

**Lemma 5** (Leverage Function is Constant).: _The leverage function given in Definition 3 is equal to \(_{q}(w)=_{=0}^{q}}{|^{d-1}|}\) for every \(w^{d-1}\)._We prove this lemma in Appendix C. The integral of the leverage function, which determines the total samples needed to solve our least-squares regression, is therefore equal to the dimensionality of the Hilbert space \(^{(q)}(^{d-1})\).

**Corollary 1**.: _The leverage function defined in Definition 3 integrates to the dimensionality of the Hilbert space \(^{(q)}(^{d-1})\), which we denote by \(_{q,d}\), i.e.,_

\[_{^{d-1}}_{q}(w)\,dw=(^{(q)}( ^{d-1}))=_{=0}^{q}_{,d}_{q,d}.\]

We now show that the leverage function can be used to randomly sample the points on the unit sphere to discretize the regression problem in Eq. (7) and solve it approximately.

**Theorem 4** (Approximate Regression via Leverage Function Sampling).: _For any \(>0\), let \(s=c(_{q,d}_{q,d}+}{})\), for sufficiently large fixed constant \(c\), and let \(x_{1},x_{2},,x_{s}\) be i.i.d. uniform samples on \(^{d-1}\). Define the quasi-matrix \(:^{s} L^{2}(^{d-1})\) as follows, for every \(v^{d}\):_

\[[\;v]():=_{=0}^{q}}{| ^{d-1}|}_{j=1}^{s}v_{j} P_{d}^{}( x _{j},)^{d-1}.\]

_Also let \(^{s}\) be a vector with \(_{j}:=} f(x_{j})\) for \(j=1,2,,s\) and let \(^{*}\) be the adjoint of \(\). If \(\) is an optimal solution to the least-squares problem \(_{g L^{2}(^{d-1})}\|^{*}g- \|_{2}^{2}\), then with probability at least \(1-10^{-4}\) the following holds,_

\[\|_{d}^{(q)}-f\|_{^{d-1}}^{2}(1+ )_{g L^{2}(^{d-1})}\|_{d}^{( q)}g-f\|_{^{d-1}}^{2}.\]

We prove Theorem 4 in Appendix C. This theorem shows that the function \(\) obtained from solving the discretized regression problem provides an approximate solution to Eq. (7).

### Efficient Solution for the Discretized Least-Squares Problem

In this section, we demonstrate how to apply Theorem 4 algorithmically to approximately solve the regression problem of Eq. (7). To achieve this, we leverage the _kernel trick_, following a similar approach to previous works such as , which allows us to efficiently address the randomly discretized least squares problem as detailed in Algorithm 1. The associated guarantee for this approach is provided in Theorem 5.

**Theorem 5** (Efficient Spherical Harmonic Interpolation).: _Algorithm 1 returns a function \(y^{(q)}(^{d-1})\) such that, with probability at least \(1-10^{-4}\):_

\[\|y-f^{(q)}\|_{^{d-1}}^{2}\|f^{ (q)}-f\|_{^{d-1}}^{2},f^{(q)}:=_{d}^{(q)}f.\]

_Suppose we can compute the Gegenbauer polynomial \(P_{d}^{}(t)\) at every point \(t[-1,1]\) in constant time. Then Algorithm 1 queries the function \(f\) at \(s=(_{q,d}_{q,d}+}{})\) points on the sphere \(^{d-1}\) and runs in \((s^{2} d+s^{})\) time. This algorithm evaluates \(y()\) in \((d s)\) time for any \(^{d-1}\)._We provide the proof of this theorem in Appendix D.

**Remark 1** (Noise Robustness).: _In Theorem 5, our method's robustness is demonstrated under a noise model where the function \(f\) is not strictly a low-degree spherical harmonic and may include high-degree components. In this scenario, the higher-degree components are considered as noise added to the input function._

_However, our algorithm is robust against alternative noise models, particularly additive i.i.d. Normal noise that corrupts the function evaluations \(\) in Algorithm 1 with iid Normal noise. More precisely, suppose that we observe samples from the function \(f^{(q)}\) contaminated by Gaussian noise, i.e., \(f(w_{j})=f^{(q)}(w_{j})+n_{j}\) in Algorithm 1 for i.i.d. \(n_{1},n_{2}, n_{s}(0,1)\). The expected value of perturbation's norm in the output \(y\) of our algorithm caused by this noise is:_

\[[\|y-f^{(q)}\|_{^{d-1}}^{2}]=(1 /s)(^{}^{} ).\]

_By Markov's inequality, with \(0.99\) probability \(\|y-f^{(q)}\|_{^{d-1}}^{2}=O(1/s)( ^{}^{})\). If we let \(\) be the operator defined in Theorem 4, then we can see that \(=^{*}\). Using the properties of the trace, one can see that \((^{}^{})=1/_{ 1}+1/_{2}+\), where \(_{i}\)'s are the singular values of the operator \(^{*}\). By matrix Chernoff inequalities one can show that all singular values of the operator \(^{*}\) closely approximate the singular values of the projection operator \(_{d}^{(q)}\) up to a constant factor. So, we have \((^{}^{})=O((_{d}^{(q)}))=O(_{q,d})\). Thus, because \(s(/)\) and using union bound, with \(0.98\) probability:_

\[\|y-f^{(q)}\|_{^{d-1}}^{2}.\]

## 4 Lower Bound on The Number of Required Observations

We conclude by showing that the dimensionality of the Hilbert space \(^{(q)}(^{d-1})\) tightly characterizes the sample complexity of Problem 2. Thus, our Theorem 5 is optimal up to a logarithmic factor. Intuitively, there are \(_{q,d}\) degrees of freedom for specifying a spherical harmonic. Consequently, any deterministic algorithm attempting to reconstruct such polynomials would need at least \(_{q,d}\) samples. We aim to demonstrate that even a "randomized" algorithm, which succeeds with only a constant probability, must still gather \(_{q,d}\) samples. This complements our upper bound, which is established using a randomized algorithm. The crucial fact we use for proving the lower bound is that all (non-zero) eigenvalues of the operator \(_{d}^{(q)}\) are equal to one. This fact follows from the addition theorem presented in Theorem 3, i.e., if \(\{y_{1}^{},y_{2}^{},,y_{_{,d}}^{}\}\) is an orthonormal basis of \(_{}(^{d-1})\), then for any function \(f L^{2}(^{d-1})\),

\[[_{d}^{(q)}f]()=_{=0}^{q}_{,d} (^{d-1})}{} [P_{d}^{}(,w) f(w)]=_{ =0}^{q}_{j=1}^{_{,d}} y_{j}^{},f_{^{d-1}} y_{j}^{}().\] (10)

**Theorem 6** (Lower Bound).: _Consider an error parameter \(>0\), and any (possibly randomized) algorithm that solves Problem 2 with probability greater than \(1/10\) for any input function \(f\) and makes at most \(r\) (possibly adaptive) queries on any input. Then \(r_{q,d}\)._

We describe a distribution on input function \(f\) on which any deterministic algorithm that takes \(r<_{q,d}\) samples fails with probability \( 9/10\). The theorem then follows by Yao's principle.

Hard Input Distribution.For integer \( q\), consider an orthonormal basis of \(_{}(^{d-1})\) and denote it by \(\{y_{1}^{},y_{2}^{},,y_{_{,d}}^{}\}\). Let \(_{}:^{_{,d}}_{}( ^{d-1})\) be the quasi-matrix with \(y_{j}^{}\) as its \(j^{th}\) column, i.e., \([_{} u]():=_{j=1}^{_{,d}}u_{j} y_{j}^{ }()\) for any \(u^{_{,d}}\) and \(^{d-1}\). Let \(v^{(0)}^{_{0,d}},v^{(1)}^{_{1,d}},,v^{(q)}^{_{q,d}}\) be independent random vectors with i.i.d. Gaussian entries: \(v_{j}^{()}(0,1)\). The random input is defined to be \(f:=_{=0}^{q}_{} v^{()}\). In other words, \(f=_{=0}^{q}_{} v^{()}\) is a random linear combination of the eigenfunctions of \(_{d}^{(q)}\). We prove that accurate reconstruction of \(f\) drawn from the aforementioned distribution yields accurate reconstruction of random vectors \(v^{(0)},v^{(1)},,v^{(q)}\). Since each \(v^{()}\) is \(_{,d}\)-dimensional, this reconstruction requires \((_{=0}^{q}_{,d})=(_{q,d})\) samples, giving us a lower bound for accurate reconstruction of \(f\).

First we show that finding an \(^{(q)}\) satisfying the condition of Problem 2 is at least as hard as accurately finding all vectors \(v^{(0)},v^{(1)},,v^{(q)}\). The following lemma is proved in Appendix E.

**Lemma 6**.: _If a deterministic algorithm solves Problem 2 with probability at least \(1/10\) over our random input distribution \(f=_{=0}^{q}_{} v^{()}\), then with probability at least \(1/10\), the output of the algorithm \(^{(q)}\) satisfies \(_{}^{*}^{(q)}=v^{()}\) for all integers \( q\)._

Finally, we complete the proof of Theorem 6 by arguing that if \(^{(q)}\) is formed using less than \(_{q,d}\) queries from \(f\), then \(_{=0}^{q}\|_{}^{*}^{(q)}-v^{()}\|_{2 }^{2}>0\) with good probability, thus the bound of Lemma 6 cannot hold and \(^{(q)}\) cannot be a solution to Problem 2. Assume for sake of contradiction that there is a deterministic algorithm which solves Problem 2 with probability \( 1/10\) over the random input \(f=_{=0}^{q}_{} v^{()}\) that makes \(r=_{q,d}-1\) queries on any input (we can modify an algorithm that makes fewer queries to make exactly \(_{,d}-1\) queries). For every \(^{d-1}\) and integer \( q\) define \(u_{}^{}:=[y_{1}^{}(),y_{2}^{}(),,y_{ _{,d}}^{}()]\). Also define \(_{}:=[u_{}^{0},u_{}^{1},,u_{}^{q} ]^{_{q,d}}\) and \(^{_{q,d}}\) as \(:=(v^{(0)},v^{(1)},,v^{(q)})\). Additionally, define the quasi-matrix \(:=[_{0},,_{q}]\).

Using the above notations and the definition of the hard input instance \(f\), each query to \(f\) is in fact a query to the random vector \(\) in the form of \(f()=_{},\). Now consider a deterministic function \(Q\), that is given input \(^{i_{q,d}}\) (for any positive integer \(i\)) and outputs \(Q()^{_{q,d}_{q,d}}\) such that \(Q()\) has orthonormal rows with the first \(i\) rows spanning the \(i\) rows of \(\). If \(_{1},_{2},,_{r}^{d-1}\) denote the points where our algorithm queries the input \(f\), for any integer \(i[r]\), let \(^{i}\) be an orthonormal matrix whose first \(i\) rows span the first \(i\) queries of the algorithm, i.e., \(^{i}:=Q([_{_{1}},_{_{2}},,_{ _{i}}]^{})\). Since the algorithm is deterministic, \(^{i}\) is a deterministic function of input \(\). The following claim is proved in :

**Claim 3** (Claim 23 of ).: _Conditioned on the queries \(f(_{1}),f(_{2}),,f(_{r})\) for \(r<_{q,d}\), the variable \([^{r}](_{q,d})\) is distributed as \((0,1)\)._

Now using Claim 3 we can write,

\[_{}[_{=0}^{q}\|v^{()}-_{ }^{*}^{(q)}\|_{2}^{2}=0] =_{}[^{r}=^{r}^{*} ^{(q)}]_{}[^{r}]_{_{q,d}}= [^{r}^{*}^{(q)}]_{_{q,d}}\] \[=[_{}[.[^{r} ]_{_{q,d}}=[^{r}^{*}^{(q)}]_{ _{q,d}}]f(_{1}),,f(_{r})]],\]

where the expectation in the last line is taken over the randomness of \(f(_{1}),,f(_{r})\). Conditioned on \(f(_{1}),,f(_{r})\), \([^{r}^{*}^{(q)}](_{q,d})\) is a fixed quantity because the algorithm determines \(^{(q)}\) given the knowledge of the queries \(f(_{1}),,f(_{r})\). Furthermore, by Claim 3, \([^{r}](_{q,d})\) is a random variable distributed as \((0,1)\), conditioned on \(f(_{1}),,f(_{r})\). This implies that,

\[[.[^{r}](_{q,d})=[^ {r}^{*}^{(q)}](_{q,d})|f(_{1}),,f( _{r})]=0.\]

Thus, \([_{=0}^{q}\|v^{()}-_{}^{*}^{(q)} \|_{2}^{2}=0]=_{f(_{1}),,f(_{r})}=0\). However, we have assumed that this algorithm solves Problem 2 with probability at least \(1/10\), and hence, by Lemma 6, \([_{=0}^{q}\|v^{()}-_{}^{*}^{(q)}\|_{2}^{ 2}=0] 1/10\). This is a contradiction, yielding Theorem 6.

## 5 Numerical Evaluation

Noise-free Setting.For a fixed \(q\), we generate a random function \(f()=_{=0}^{q}c_{}P_{d}^{}(,v)\) where \(v(^{d-1})\) and \(c_{}\)'s are i.i.d. samples from \((0,1)\). Then, \(f\) is recovered by running Algorithm 1 with \(s\) random evaluations of \(f\) on \(^{d-1}\). Note that \(\|_{d}^{(q)}f-f\|_{^{d-1}}=0\) since \(f^{(q)}(^{d-1})\), thus,as shown in Theorem 4, Algorithm 1 can recover \(f\) "exactly" using \(s=(_{q,d}_{q,d})\) evaluations, where \(_{q,d}\) is the dimension of the Hilbert space \(^{(q)}(^{d-1})\). We predict \(f\)'s value on a random test set on \(^{d-1}\) and consider the algorithm fails if the testing error is greater than \(10^{-12}\). We count the number of failures among \(100\) independent random trials with different choices of \(d\{3,4\}\), \(q\{5,,22\}\), and \(s\{40,,2400\}\). The empirical success probabilities for \(d=3\) and \(4\) are reported in Fig. 1(a) and Fig. 1(b), respectively. Fig. 1 illustrates that the success probabilities of our algorithm sharply transition to \(1\) as soon as the number of samples approaches \(s_{q,d}\) for a wide range of \(q\) and both \(d=3,4\). These experimental results complement our Theorem 4 along with the lower bound analysis in Section 4 and empirically verify the performance of our algorithm.

Noisy Setting.We repeated our experiments in the presence of an additive noise which is a linear combination of random spherical harmonics of degrees \(q+1\) to \(2q\). More precisely, we let the noise be \(n()=_{=q+1}^{2q}c_{}P_{d}^{}( v,) ^{(2q)}(^{d-1})^{(q)}(^ {d-1})\) for \(c_{}\)'s that are i.i.d. samples from \((0,1)\). We then re-scale the noise to have norm \(\|n\|_{^{d-1}}=10^{-6}\). Furthermore, the function \(f\) is defined as before, and \(f\) is recovered by Algorithm 1 with \(s\) random evaluations of \(f+n\) on \(^{d-1}\). The heat-maps in Fig. 2 are generated by considering an instance of our algorithm as a "success" if the error's energy is below the noise level, \(\|^{(q)}-f\|_{^{d-1}}\|n\|_{^{d-1} }=10^{-6}\). The success probability transitions less sharply than the noiseless setting but the shift of probabilities starts at \(_{s,q}\) samples.

## 6 Conclusion

We studied the problem of robustly recovering spherical harmonic expansion of a function defined on the sphere. The number of function evaluations needed to recover its degree-\(q\) expansion is the dimension of spherical harmonics of degree at most \(q\), up to a logarithmic factor. We develop a simple yet efficient kernel regression-based algorithm to recover degree-\(q\) expansion of the function by only evaluating the function on uniformly sampled points on the sphere. Unlike the prior results on fast spherical harmonic transform, our algorithm works efficiently using a nearly optimal number of samples in any dimension. We believe our findings would appeal to the readership of the community.

Figure 1: (Left) Empirical success probabilities of Algorithm 1 varying the number of samples \(s\) and the degree of spherical harmonic expansion \(q\). (Right) The dimension \(_{q,d}\) of the Hilbert space \(^{(q)}(^{d-1})\) as a function of \(q\) when (a) \(d=3\) and (b) \(d=4\), respectively.

Figure 2: (Left) Empirical success probabilities of Algorithm 1 in presence of additive noise, where “success” means the error’s energy is below the noise level \(\|^{(q)}-f\|_{^{d-1}}\|n\|_{^{d-1}}\). (Right) The error’s norm \(\|^{(q)}-f\|_{^{d-1}}\) as a function of \(q\) when (a) \(d=3\) and (b) \(d=4\), respectively.