# Posture-Informed Muscular Force Learning

for Robust Hand Pressure Estimation

 Kyungjin Seo\({}^{1}\), Junghoon Seo\({}^{1}\), Hanseok Jeong\({}^{2}\), Sangpil Kim\({}^{3}\), Sang Ho Yoon\({}^{1,2}\)

\({}^{1}\) Graduate School of Culture Technology, KAIST, South Korea

\({}^{2}\) Graduate School of Metaverse, KAIST, South Korea

\({}^{3}\) Department of Artificial Intelligence, Korea University, South Korea

Both authors contributed equally to this research.

###### Abstract

We present PiMForce, a novel framework that enhances hand pressure estimation by leveraging 3D hand posture information to augment forearm surface electromyography (sEMG) signals. Our approach utilizes detailed spatial information from 3D hand poses in conjunction with dynamic muscle activity from sEMG to enable accurate and robust whole-hand pressure measurements under diverse hand-object interactions. We also developed a multimodal data collection system that combines a pressure glove, an sEMG armband, and a markerless finger-tracking module. We created a comprehensive dataset from 21 participants, capturing synchronized data of hand posture, sEMG signals, and exerted hand pressure across various hand postures and hand-object interaction scenarios using our collection system. Our framework enables precise hand pressure estimation in complex and natural interaction scenarios. Our approach substantially mitigates the limitations of traditional sEMG-based or vision-based methods by integrating 3D hand posture information with sEMG signals. Video demos, data, and code are available online.1

## 1 Introduction

Hands are a central tool for humans to interact with the surrounding environment. With the advancement in hand tracking technology, hand inputs, including position, orientation, gesture, and motion, are increasingly used as a primary means of control, especially for emerging interfaces (e.g., augmented/virtual reality and wearables). Using hands as the main interaction medium offers a high level of versatility and flexibility to achieve natural and intuitive interactions.

Recent studies have started to utilize hand pressure information to support hand-based interactions such as touching , grasping [2; 3], and pressing . Researchers also utilized hand pressure to provide effective haptic feedback  for a more immersive user experience. Furthermore, precise hand pressure measurement becomes essential for real-world applications, including ergonomic evaluation , hand rehabilitation , and prosthetic hand control . To this end, previous works focus on obtaining real-time and accurate hand pressure information with direct measurement approaches utilizing gloves [9; 10; 11] or load cells . However, these approaches require users to be in physical contact by either wearing or holding the device, which hinders natural hand movements or reduces user comfort. Thus, the necessity of direct contact limits the users from performing hand-based interactions in a natural and unrestricted manner.

To this end, non-invasive sensing techniques to estimate exerted hand pressure without embedding sensors on the user's hand have been highlighted. These methods include profiling wrist topography with capacitive sensing , multiple pressure sensing from the wrist , and electromyographyfrom the forearm . Recent works utilized only a single RGB-D  or RGB camera [16; 17] with computer vision techniques to estimate pressure exerted by the hand. However, previous methods had limitations, where they could only estimate hand pressure when interacting with plane surfaces or required a line-of-sight view of the hand. With these limitations, it is hard to support natural interaction contexts like working with diverse hand grasps.

In this work, we introduce PiMForce, a novel multimodal sensing framework that enhances real-time hand pressure estimation by leveraging 3D joint information of the hand and forearm sEMG signals, covering the fingertip to the entire palm. As illustrated in Figure 1, our framework addresses previous challenges by integrating detailed spatial information from 3D hand poses with dynamic muscle activity from sEMG measurements. This multimodal sensing integration allows us to estimate subtle and comprehensive hand pressure even under diverse grasps. To validate the proposed model, we built a multimodal hand data collection system and created a dataset from 21 participants. We believe our dataset is the first of its kind containing multimodal sensor signals during hand interactions under various grasps. We demonstrated our work on an off-the-shelf system with a single camera to confirm the accuracy and feasibility of the proposed framework. Our contributions are listed as follows:

* We propose PiMForce, a novel hand pressure estimation framework that enhances sEMG signals by incorporating hand posture information.
* We develop a multimodal hand data collection system with a data collection protocol and create a unique dataset containing simultaneous hand pressure, hand posture, and surface electromyography signals.
* Evaluation and analysis of experiments demonstrate the improved performance of our approach, showing its consistent superiority over existing sEMG-based and vision-based methods.

## 2 Related Works

### Vision-based Hand Pressure Estimation

Previous works explored the interaction between the hand and objects by observing the movement and rotation of the object over time to estimate the pressure exerted by hands. By determining the pressure required to produce these observed changes, the model estimated the aggregate pressure applied by the hand [18; 19; 20]. These approaches enabled the pressure estimation to act upon concealed or non-visible areas where direct visibility of the hand in contact with an object is absent. Still, previous approaches had limitations where interacting with immovable objects would not work due to the absence of dynamic interaction indicators.

Researchers also looked into different visual indicators like color changes in fingertips, which represent fluctuation of blood circulation within the fingertips [21; 22] or compression of skin tissues [23; 24]. These physiological behaviors served as indicators of the exerted hand pressure. Moreover, examining shadows cast during hand-object interaction provided further insight into the spatial relationship and dynamics of force between them [25; 26; 27].

Recent works further advanced the existing visual indicator approach where they use a hand image captured by a single camera at a distance to estimate the hand pressure [16; 28; 17; 29]. They employed a deep learning model that facilitates the understanding of visual cues to estimate accurate

Figure 1: Our sensing framework (PiMForce) leverages 3D hand posture information along with sEMG data to enable a whole-hand pressure estimation during various hand-object interactions. We support real-time pressure estimation on the fingertips and palm regions based on RGB image and sEMG inputs. The intensity of each nodeâ€™s color indicates the pressure level.

hand pressure in an end-to-end fashion. However, previous works require a high-quality whole-hand image without occlusion since they rely on visual indicators for the estimation. In our work, we utilize multimodal inputs, including vision-driven 3D hand posture information and wearable-based muscle activation signals, to enhance the estimation of robust hand pressure under various hand-object interaction contexts.

### Wearable-based Hand Pressure Estimation

Researchers have used forearm/wrist surface electromyography (sEMG) sensors to acquire finger muscle activation information. Here, the sensor captured a train of neuron impulses propagated through the arms from the forearm or wrist [30; 31]. Previously, researchers used sEMG sensors to estimate various types of hand-related force/pressure, including force/pressure from gripping [32; 33; 34; 35; 36] and fingertip [4; 37; 38; 39; 40; 41]. Recent works also enabled the estimation of hand pressure along with hand gesture recognition using sEMG signals [8; 42]. However, previous works only dealt with a limited set of discrete hand poses . Moreover, an issue existed with using sEMG signals for complex hand interactions where similar muscle activation signal behaviors were observed across different hand poses. This could easily confuse the model and lead to false behavior. In this work, we train the model with 3D hand posture to encode distinctive hand pose information alongside sEMG signals. This integration forms a robust and accurate hand pressure estimation framework for similar muscle activation behaviors but different hand poses. It is worth emphasizing that this study is the first to incorporate hand posture information for hand pressure estimation using forearm-worn sEMG. Previous studies primarily focused on estimating pressure at the fingertip or on a single gripping force, but our approach expands this to encompass the whole hand.

### Datasets for Hand Pressure Estimation

In the computer vision and machine learning community, researchers have formed various types of hand-object interaction datasets for hand pressure estimation. These vision-based datasets collected rich visual and pose information for hand-object interactions, capturing everything from object affordances to whole-body grasps [44; 28; 17; 45; 46; 47]. On the other hand, sEMG-based datasets have also been proposed and used to estimate hand pressure for AR/VR or prosthetic robotic arm control applications [48; 4]. A highly relevant work is the ActionSense dataset , which focuses on capturing multimodal data of human activities in a kitchen environment using wearable sensors. However, while ActionSense provides a valuable resource for understanding general kitchen activities, our work focuses on the utilization of 3D hand posture in muscular force learning for understanding hand pressure estimation. Furthermore, the temporal resolution of EMG data and the spatial resolution of hand pressure in ActionSense are substantially lower compared to ours, making it challenging to utilize rich sensor input and output effectively. Still, the dataset containing both rich visual and physiological information is missing.

In this work, we attempt to set up a new multimodal dataset that contains 3D hand pose information, sEMG signals, and ground truth measurement of hand pressure as shown in Table 1. Our work provides a holistic view of whole-hand dynamics during various hand-object interactions. This integration enables continuous and comprehensive pressure estimation across the whole palm, addressing the limitations of previous datasets that either infer pressure from visual cues or measure it in isolation.

   Dataset & Input & Frames & Participants & Contact / Pressure & Pose & Whole & Natural \\  & Model & & & Pressure Score & & Hand & Objects \\   OdaIrk  & RGBD & 230k & 12 & Inferred from pose & \(\) & \(\) & \(\) & \(\) \\ DexYCB  & RGBD & 582k & 10 & Inferred from pose & \(\) & \(\) & \(\) & \(\) \\ HO-3D  & RGBD & 78k & 10 & Inferred from pose & \(\) & \(\) & \(\) & \(\) \\ GAB  & Pose & 1.601 & 10 & Inferred from pose & \(\) & \(\) & \(\) & \(\) \\ ContactPose  & RGBD & 3.0M & 50 & Thermal imprint & \(\) & \(\) & \(\) & \(\) \\ PressureVisionD  & RGBD & 3.00M & 36 & Pressure Pad & \(\) & \(\) & \(\) & \(\) \\ ContactLabID  & RGB & 2.9M & 51 & Pressure Pad & \(\) & \(\) & \(\) & \(\) \\ Force-Aware Interface  & sEMG & 17.8M & 9 & Pressure Pad & \(\) & \(\) & \(\) & \(\) \\ HDEMG  & sEMG & 67.6M & 20 & Custom-made Device & \(\) & \(\) & \(\) & \(\) \\ ActionSense  & _NA_ & 9.42M & 10 & Pressure Glove & \(\) & \(\) & \(\) & \(\) \\ 
**Ours** & Pose+sEMG & 83.2M & 21 & Pressure Glove & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Comparison with the previous datasets for hand contact and pressure estimation. We modified and updated the table from . _NA_ refers to â€™not availableâ€™.

Our dataset not only captures the subtle interplay between visual and tactile information, but also increases the potential candidates for input features for accurate and robust hand pressure estimation.

## 3 Building Multimodal Dataset: Posture, Electromyography, and Pressure

To capture multimodal data with various hand-object interactions, we integrated and customized existing hardware, including a pressure glove, an armband with 8-channel sEMG sensors, and a markerless finger tracking module. Figure 7 in supplementary material showcases our data collection setup to capture real-time and synchronous multimodal data, including 3D hand posture, sEMG signals, and exerted hand pressure. More detailed information about the hardware, defined hand postures, data collection protocol, and data processing can be found in Section B.

### Data Collection Setup

**Pressure Glove.** To capture pressure exerted from the hand, we developed a customized pressure glove using a single 65-node pressure sensing glove (TactileGlove, Pressure Profile Systems) attached with a pressure sensor (RA18DIY, Marveldex) at each fingertip. We added flexible sensors to the fingertips to address missed readings when the pressures were exerted on the edge of the fingertip. Our pressure glove supports pressure readings up to 55 N/cm\({}^{2}\) with a sampling rate of 40 Hz.

**3D Hand Pose.** Recent common approaches to obtaining ground truth 3D hand pose  involve using multiple RGB cameras to derive 2D hand poses from each camera, followed by triangulation [52; 53] or hand template fitting [44; 45; 54]. However, these methods are infeasible when a pressure data glove is worn, as the glove obscures the hand, hindering accurate hand pose estimation from RGB images. To acquire accurate 3D hand pose information under this constraint, we employed a magnetic sensing-based markerless finger tracking module (Quantum Mocap Metaglove, Manus). The module provides each finger's 3D position and 3-axis joint angles with a sample rate of 120 Hz and less than 5-millisecond latency. We attached the finger-tracking module to the pressure glove to capture exerted hand pressure and 3D hand pose data simultaneously.

**8-Channel sEMG Armband.** We used 8 sEMG sensors (Trigno Avanti, Delsys) and installed sensors into a customized armband made with semi-flexible material (TPU 95A) to ensure electrode contact for various sizes of forearms. Our system captures muscle action potentials with a sampling rate of 2,000 Hz.

**Multimodal Data Synchronization.** Before training the multimodal dataset, we employed a linear interpolation approach to synchronize high frame rate readings (sEMG) with low frame rate data (3D hand pose). We first joined the data from matched time points based on the collection time of the sEMG data and interpolated missing values of the hand pose data linearly. We applied the same approach to the pressure glove, where we synchronized high frame rate sensors (pressure sensors) with the low frame rate sensing glove (TactileGlove). Then, we adopted a nearest-neighbor-based interpolation to synchronize 3D hand pose and sEMG data with hand pressure .

### Data Collection Procedure

With IRB approval, a total of 21 right-handed participants took part in this study. Of these, 17 were male (81%) and 4 were female (19%). The participants' ages ranged from 20 to 32 years, with a mean age of 24.3 years (SD = 3.9). To ensure good quality hand pressure data using our glove, we chose participants with hand sizes greater than 180 mm. Prior to participation, all participants were provided with a detailed information sheet outlining the purpose, procedures, and potential risks of the study. We obtained written informed consent from each participant, ensuring they understood the nature of the data being collected, their right to withdraw at any time, and the measures taken to ensure data privacy. We equipped participants with our multimodal glove and an 8-channel sEMG armband. Following initial calibration to compensate for each user's hand size, participants performed 22 distinctive hand-object interactions for the data collection task (Figure 8 in supplementary material).

Our hand-object interactions consist of 7 hand-plane interactions, 5 pinch interactions, and 10 distinctive hand grasps. We included the same hand-plane and pinch interactions from recent hand pressure estimation work  while adding palm-pressing motion. In terms of hand grasps selection,we selected 10 grasps from representative 33 grasp types  based on the clustering of palm pressure distribution similarity among grasp taxonomies . We collected 1,980 seconds of synchronized multimodal data per participant (30 seconds \(\) 22 hand-object interactions \(\) 3 sessions). Here, we used data from 2 sessions for training while the remaining session was reserved for evaluation. We provided sufficient rest periods between each trial to prevent muscle fatigue, ensuring the quality of collected data. All collected data was anonymized, complying with relevant data privacy regulations.

## 4 Method

### Overview

The rationale behind using sEMG to estimate the pressure exerted by the hand lies in the direct relationship between muscle electrical activity and the pressure generated during hand interactions [57; 58; 59]. When muscles contract for movement, they generate bioelectric signals that can be captured by EMG sensors . This indicates that sEMG signals have the potential to estimate the pressure exerted by the hand. The ability to decode sEMG signals from forearm muscles generated by finger-level movements will set a robust foundation for understanding complex hand interactions. However, the pressure exerted by the hand cannot be solely represented with muscle activation information. The main reason is that the distribution of hand pressure varies according to different hand postures. For example, similar sEMG patterns may be generated by different hand pressures depending on the related hand postures or grasps [43; 61]. This highlights the importance of considering hand posture information along with sEMG signals to estimate the exerted hand pressure precisely. Section C.1 in supplementary material addresses the specific empirical observation for this motivation.

To address these issues, we enhance sEMG signals by leveraging 3D hand posture information. By integrating inputs from forearm-worn sEMG sensors with 3D hand pose information derived from an RGB image, we observe improvements in the accuracy of hand pressure estimation. Our multimodal approach (Figure 2) leverages the strengths of both hand posture and muscle activations, offering a comprehensive understanding of hand dynamics for whole-hand pressure estimation. Refer to Section C for more detailed information about the model architecture, training, and inference.

### 3D Hand Pose and sEMG Feature Extractions

To verify the validity of our framework, we devise a deep neural network model to effectively utilize the obtained multi-modalities. We represent the model as \(f\), where the 3D hand pose is denoted by \(H\) and the sEMG signal by \(E\). The classification and regression targets for pressure are represented as \(C\) and \(P\), respectively. If the model outputs for pressure classification and regression are indicated, they are denoted by \(\) and \(\). The feature extractor for sEMG data is \(f_{}\), and for hand pose, it is \(f_{}\). The overall model \(f\) comprises \(f_{}\), \(f_{}\), and a pressure predictor \(f_{}\) that takes the features extracted from hand pose and sEMG to perform pressure classification and regression.

**Feature Extraction from sEMG signals.** The sEMG data generally encounters measurement noises, including powerline noise and electromagnetic artifacts. To mitigate these issues, we utilize the short-time Fourier transform (STFT) to convert sEMG time-domain signals into spectrograms, represented

Figure 2: Our multimodal hand pressure estimation architecture enhances sEMG data by embedding 3D hand pose information. We train the model using a classification-regression joint loss to improve hand pressure estimation.

as \(^{8 32 64}\), isolating high-frequency noise and facilitating the application of convolutional neural network (CNN) models for feature extraction. We employed a 2D encoder-decoder model to extract features from the 2D sEMG signals. The encoder-decoder model processes the data and then flattens the output, which is subsequently transformed through a fully connected (FC) layer into a 512-dimensional feature vector.

**Feature Extraction from 3D Hand Pose.** The hand model in our study is represented as a kinematic tree with 15 joint angles \(^{15 3}\), similar to the pose parameters in MANO and its variants . To handle this skeleton-based representation simply like PoseConv3D , we adopt a 3D ResNet  to process 3D heatmap volumes of hand joints, transforming \(\) into 21 3D hand joints \(J^{21 3}\) through the hand skeleton model's forward kinematics. These 3D hand joints are then converted into 3D heatmap volumes \(H^{21 H W D}\), with \(H\), \(W\), and \(D\) all set to 48, for processing by the 3D ResNet. Unlike the sequential representation of 2D heatmaps in PoseConv3D, our approach uses a single timestep 3D joint representation. After processing through the 3D ResNet, the hand pose feature is flattened and transformed into a 512-dimensional vector using an FC layer, with the 3D ResNet34 model being the model of choice for this operation.

**Feature Fusion and Estimation.** The extracted sEMG feature \(f_{}(E)\) and hand pose feature \(f_{}(H)\) are concatenated to form a 1024-dimensional joint feature vector. This vector is then passed through two FC layers, mapping to 256-dimensional features, followed by a 1-D batch normalization and ReLU non-linearity. Finally, the last FC layer with Sigmoid activation function maps this to \(I\)-dimensional output, producing \(=f(E,H)=f_{}(f_{}(E),f_{}(H))\). The predicted pressure \(\) is defined as \(=2P_{}(-0.5)[0,P_{}]\), thus completing the feature fusion and pressure prediction.

### Joint Training of Classification and Regression

The objective function comprises two key components: a classification loss \(L_{c}\) and a regression loss \(L_{r}\). The classification loss is designed to accurately identify whether any pressure is exerted by a particular region of the hand (i.e., fingertips or palm areas shown in Figure 5 of Section B.1.1), using a cross-entropy loss to distinguish between pressure and no-pressure instances for each hand region \(i\):

\[L_{c}=_{i=1}^{I}C_{i}_{i}+(1-C_{i})(1- _{i}),\] (1)

where \(C_{i}\) is the ground-truth label for region \(i\), and \(_{i}\) is the predicted probability of pressure application. Here, \(C_{i}=1\) indicates the presence of pressure in the \(i\)th region, while \(C_{i}=0\) indicates its absence. In contrast, the regression loss, targets the accurate quantification of pressure levels using an \(L_{2}\) loss to minimize the difference between the predicted and actual pressure values:

\[L_{r}=_{i=1}^{I}\|_{i}-P_{i}\|^{2},\] (2)

where \(_{i}\) represents the model's predicted pressure for region \(i\) and \(P_{i}\) is the corresponding actual pressure. Our dataset contains pressures from 0\(\)20 N. Therefore, our model predicts pressure values in , organized by \(_{max}\), the maximum of the pressure. To integrate these two aspects into a unified training objective, we introduce a balancing hyper-parameter \(\), resulting in a combined loss function: \(L=L_{c}+ L_{r}\). This composite loss enables our model to not only discern the presence of pressure but also quantify its magnitude accurately.

### Estimation without the Glove

For the training phase, we employed data acquired from our data collection system to ensure the accurate capture of exerted hand pressure and 3D hand pose data. However, during the inference phase, our framework exploits off-the-shelf hand pose detectors , which extract 3D hand pose from RGB or RGB+D inputs. These detectors can been chosen for their high accuracy and robustness in various conditions, ensuring reliable performance during inference. Thus, users can interact with external objects using their bare hands, without the need for additional hand-worn equipment. This approach ensures our model's practical applicability in real-world scenarios, prioritizing user convenience and natural interaction. By leveraging readily available technology, we make it easier for users to adopt our system in everyday applications. Refer to Section B.4 for details on how we canonicalized 3D hand pose information extracted from RGB images for our model input.

## 5 Experiments

In Sections 5.2.1 and 5.2.2, where ground truth hand pressure is necessary, data was collected while participants wore the pressure glove, and hand postures were obtained from the data glove. We also conducted qualitative evaluations (Section 5.2.3 and the demo video) without ground truth, where data was collected without any gloves, and hand postures were inferred solely from RGB images using an off-the-shelf hand pose detector. For this purpose, we employed the pre-trained Attention Collaboration-based Regressor , which has demonstrated superior performance with a mean per joint position error (MPJPE) of approximately 8mm for reconstructing hand poses from a single RGB camera. The high accuracy ensures the reliability of our hand posture inferences in qualitative assessments. To assess our model's performance, we utilize three metrics: Coefficient of Determination (R\({}^{2}\)), Normalized Root Mean Squared Error (NRMSE), and classification accuracy. The exact definitions and explanations of evaluation metrics can be found in Section D.2. Refer to Section D.3 and D.4 for additional quantitative and qualitative results, respectively.

### Comparative Methods

This study compares the proposed model against several baseline and state-of-the-art methods to validate its effectiveness in whole-hand pressure estimation. To ensure a fair comparison, we selected methods that quantitatively measure the pressure applied by the hand, rather than solely identifying hand contact. Detailed information about the implementation of comparative methods can be found in Section D.1. The methods included in the comparison are:

**sEMG Only Model .** An sEMG-based approach decodes finger-wise forces in real-time, demonstrating the potential of muscle activation patterns in informing hand activities. This method emphasizes using electromyography sensors for understanding complex hand dynamics but does not incorporate hand posture information.

**3D Hand Posture Only Model.** A variation of our proposed framework that solely utilizes 3D hand posture for pressure estimation, omitting the sEMG signal input. This model tests the efficacy of hand posture information in isolation.

**sEMG \(+\) Hand Angles Model.** This model represents a variation of our proposed framework, where instead of utilizing the 3D representation \(H\) for hand pose, it employs the angular representation \(\) of hand joints as the input to the hand pose feature extractor \(f_{}\). By substituting the 3D hand pose with direct angle measurements of hand joints, this baseline aims to highlight the benefits of using a 3D representation for hand pose in multimodal sensing.

**PressureVision++ .** This vision-based deep learning model estimates hand pressure from a single RGB image by identifying visual cues related to hand pressure application, showcasing the use of visual information for pressure estimation without physical FSR sensors.

**PiMForce (Ours).** The comprehensive model enhances sEMG signals by leveraging 3D hand posture information for continuous and detailed pressure estimation across the whole hand. This approach aims to mitigate the limitations of sEMG-based methods by integrating the strengths of both modalities for enhanced pressure prediction accuracy.

   Method & \(^{2}\) & **NRMSE** & **Accuracy** \\  
**sEMG Only ** & \(83.49 16.40\%\) & \(8.07 2.62\%\) & \(77.83 11.56\%\) \\
**3D Hand Posture Only** & \(66.32 37.01\%\) & \(11.57 3.95\) & \(70.08 13.09\%\) \\
**sEMG \(+\) Hand Angles** & \(84.22 17.11\%\) & \(7.89 2.61\%\) & \(78.22 10.57\%\) \\ 
**PiMForce (Ours)** & \( 11.92\%\) & \( 2.11\%\) & \( 9.38\%\) \\   

Table 2: Performance among comparative models on evaluation metrics.

### Results

We analyze the performance of our proposed framework in comparison to these methodologies, both quantitatively and qualitatively. Additionally, we investigate the capabilities of our model to accurately estimate hand pressure across a variety of hand postures and parts, providing a thorough assessment of its performance. Our framework enhances sEMG signals by leveraging 3D hand posture information for detailed palm pressure data collection, contrasting with PressureVision++, which relies on visual cues for force estimation. This approach is designed to underscore the distinctive benefits of our multimodal sensing framework in capturing a broad range of hand interactions.

#### 5.2.1 Do hand pose and sEMG signals together improve pressure estimation?

Table 2 outlines the performance metrics of various comparative models, including the sEMG Only Model, the 3D Hand Posture Only Model, the sEMG + Hand Angles model, and our model. PiMForce remarkably outperforms the comparative methods, achieving an accuracy of 83.17%, NRMSE of 6.65%, and an R\({}^{2}\) value of 88.86%. This demonstrates the comprehensive capability of our model to accurately classify and quantify the pressures exerted by the hand.

The integration of 3D hand posture and sEMG information in our framework shows a clear advantage over approaches relying on a single data modality, as expected. The sEMG Only Model and the 3D Hand Posture Only Model show limited pressure estimation performance when compared to our integrated approach. Interestingly, the improvement in performance with the sEMG + Hand Angles model over the sEMG Only Model is marginal (less than 0.5%p improvements in all metrics). This highlights the importance of incorporating a comprehensive 3D hand posture representation. By embedding comprehensive hand posture knowledge to be used with sEMG data, we develop an effective multimodal approach to capture nuanced variations in hand pressure exerted across different hand regions and postures.

Cross-user performance assesses how well the model performs on data from individuals not included in the training set, which is crucial for real-world applications. As shown in Table 3, our proposed method combining sEMG signals with 3D hand posture data significantly outperforms the sEMG-only baseline across all evaluation metrics in cross-user scenarios. This demonstrates the enhanced generalizability and effectiveness of our approach in estimating hand pressure among different users. To further demonstrate the performance of our model over time, we present Figure 23 in supplementary material, which illustrates the temporal evolution of both ground truth and predicted pressure values for all nine hand regions during consecutive TM-Press and Medium Wrap actions.

#### 5.2.2 How does accuracy vary by hand region and posture type?

We delve into the performance of our model across various hand regions and posture types, utilizing data represented in both Table 4 and Figure 3. This analysis highlights the noticeable impact of incorporating 3D hand pose data, particularly noting a greater improvement in hand palm regions (+1.95%p) over fingertips (+1.05%p) compared to the sEMG Only Model. This distinction emphasizes the crucial role of 3D hand pose for accurate pressure estimation in diverse hand postures.

Our findings reveal that the model achieves superior pressure estimation in _Press_ and _Pinch_ interactions, with classification accuracies surpassing 90% and NRMSE values maintained below 6%. However, it encounters challenges with specific postures such as _Palm-Press_, which, despite a lower classification accuracy of 68.42%, still shows a high regression accuracy of 3.12%. When examining _Grasp_ postures, our model shows a slight dip in performance relative to _Press_ and _Pinch_, with NRMSE values ranging between 5\(\)8%. This suggests a moderate pressure estimation capability for these more complex interactions, yet the model consistently maintains a high \(R^{2}\) range of 0.8 to 0.9 across all posture types. This consistent correlation between predicted values and actual pressure measurements highlights the model's ability to maintain high accuracy and reliability across a diverse

   Method & **R\({}^{2}\)** & **NRMSE** & **MAE** & **Accuracy** \\  
**sEMG only ** & 47.90 \(\) 9.97\% & 14.14 \(\) 2.41\% & 12.24 \(\) 2.75\% & 57.40 \(\) 5.81\% \\
**PiMForce (Ours)** & **70.06**\(\) 4.02\% & **10.70**\(\) 1.43\% & **8.54**\(\) 1.56\% & **72.01**\(\) 2.86\% \\   

Table 3: Cross-user performance on evaluation metrics under whole interaction and posture.

range of hand parts and postures. Specific actions such as _I-Press_, _M-Press_, and _R-Press_ exhibit high accuracy and low NRMSE, showing the model's superior performance in simpler press interactions. On the contrary, more complex grasps like _Power Sphere_, _Fixed Hook_, and _Parallel Extension_, showed lower accuracy and higher NRMSE, implying that there remains room for improvement.

#### 5.2.3 Can inference succeed with an off-the-shelf hand pose detector?

For practical inference applications without a data glove, solely relying on EMG data and incorporating 3D hand pose information obtained through an off-the-shelf hand pose detector, we demonstrate the adaptability of our model in Figure 3(a). This comparison with the vision-based method, Pressure-Vision++, showcases our PiMForce's capability to estimate hand pressures robustly during diverse interactions. During hand-plane interactions, specifically those involving the tip _Press_ type posture, both approaches appear to perform well. However, our analysis reveals vulnerabilities in handling more complex _Grasp_ and _Pinch_ motions when using PressureVision++. Furthermore, PressureVision++ requires complete visibility of all fingers within the camera's view due to the high reliance on visual cues for pressure inference. In contrast, our framework effectively utilizes the estimated hand pose as long as the hand pose information is sufficiently accurate for inference. This capability underscores the practicality of our method, facilitating more natural user interactions with external objects without the constraints of direct visibility or glove use. Figure 3(b) shows demo video footage illustrating our PiMForce's capability to accurately estimate hand pressure while continuously changing hand posture, pressure levels, and the objects being grasped. This demonstrates the flexibility and reliability of our approach in real-world scenarios.

To further substantiate our model's effectiveness using an off-the-shelf hand pose detector, we conducted a quantitative comparison with PressureVision++, as presented in Table 5. PiMForce demonstrates largely better performance across all fingertips during both plane and pinch interactions, indicating superior performance in estimating hand pressures compared to PressureVision++. This quantitative evaluation confirms that our framework outperforms existing vision-based methods in terms of accuracy and robustness during diverse interactions.

    &  &  &  &  **Overall** \\ **Mean** \\  } \\  
**Method** & **Thumb** & & & & & & & & & & \\  
**sEMG Only** & \(9.78\) & \(9.26\) & \(8.15\) & \(6.65\) & \(4.69\) & \(7.71\) & \(8.79\) & \(8.43\) & \(8.66\) & \(7.39\) & \(8.42\) & \(8.07\) \\  
**sEMG Only** & \(9.78\) & \(9.26\) & \(8.15\) & \(6.65\) & \(4.69\) & \(7.71\) & \(8.79\) & \(8.43\) & \(8.66\) & \(7.39\) & \(8.36\) & \(3.85\) & \(3.268\) \\ 
**3D Hand Pusture Only** & \(3.85\) & \(13.64\) & \(12.18\) & \(8.95\) & \(6.16\) & \(11.13\) & \(12.26\) & \(10.41\) & \(11.47\) & \(9.71\) & \(10.96\) & \(11.05\) \\ 
**sEMG + Hand Pusture Only** & \(5.99\) & \(5.99\) & \(5.31\) & \(4.49\) & \(1.79\) & \(4.55\) & \(6.09\) & \(6.09\) & \(6.03\) & \(6.18\) & \(5.27\) \\ 
**sEMG + Hand Angles** & \(9.54\) & \(9.04\) & \(7.95\) & \(6.48\) & \(4.57\) & \(5.25\) & \(3.86\) & \(7.23\) & \(7.67\) & \(6.67\) & \(7.53\) & \(7.52\) \\ 
**PIMForce (Ours)** & \(3.48\) & \(3.36\) & \(3.21\) & \(4.85\) & \(1.37\) & \(2.82\) & \(3.00\) & \(4.14\) & \(4.22\) & \(4.21\) & \(3.40\) \\ 
**PIMForce (Ours)** & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ 
2.7

## 6 Conclusion

In this paper, we introduce PiMForce, a pioneering framework for hand pressure estimation by integrating 3D hand posture information with muscle activation signals from forearm-worn sEMG. By embedding 3D hand posture information into a deep neural network, we enable the model to process this data alongside sEMG signals, enhancing its capability to learn complex relationships between muscle activations and hand pressure distributions. This novel approach is the first to combine these modalities, providing a comprehensive analysis of hand dynamics across various interactions. We developed a unique multimodal hand data collection system and protocol, capturing a dataset that includes hand pressure, posture, and electromyography signals. Our method notably improves upon previous techniques, enabling accurate whole-hand pressure estimation through detailed hand posture information. Extensive quantitative and qualitative comparisons demonstrated the consistent superiority of our framework over existing sEMG-based and vision-based methods.

Figure 4: **(a) Qualitative results in the absence of a pressure glove. The 3D Hand Pose Estimation  represents 3D hand posture, including hand occlusion, using the 3D hand detector. The Pressure-Vision++  column shows the pressure estimation of fingertips. The red rectangles indicate the instances of pressure estimation failure due to hand occlusion. The proposed multimodal framework shows robust whole-hand pressure estimation for diverse hand-object interactions. **(b)** Illustration of the demo video footage showing robust hand pressure estimation with varying hand postures, pressure levels, and interacting objects.

   Method & \(^{2}\) & **NRMSE** & **Accuracy** \\  
**PressureVision++** & 40.30 \(\) 5,14\% & 32.95 \(\) 2.02\% & 67.90 \(\) 3.01\% \\
**sEMG only** & 42.13 \(\) 6.88\% & 12.57 \(\) 2.09\% & 66.00 \(\) 5.84\% \\
**PiMForce (Ours)** & **66.71**\(\) 4.68\% & **9.27**\(\) 1.40\% & **82.20**\(\) 2.42\% \\   

Table 5: Cross-user performance on evaluation metrics under plane interaction and pinch posture.