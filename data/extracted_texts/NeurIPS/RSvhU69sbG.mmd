# MathPile : A Billion-Token-Scale Pre-training Corpus for Math

Zengzhi Wang\({}^{1,3,4}\) Xuefeng Li\({}^{1,4}\) Rui Xia\({}^{3}\) Pengfei Liu\({}^{1,2,4}\)

\({}^{1}\)Shanghai Jiao Tong University \({}^{2}\)Shanghai Artificial Intelligence Laboratory

\({}^{3}\)Nanjing University of Science and Technology \({}^{4}\)Generative AI Research Lab (GAIR)

zzwang.nlp@gmail.com pengfei@sjtu.edu.cn

 Corresponding author

###### Abstract

High-quality, large-scale corpora are the cornerstone of building foundation models. In this work, we introduce MathPile, a diverse and high-quality math-centric corpus comprising about 9.5 billion tokens. Throughout its creation, we adhered to the principle of "_less is more_", firmly believing in the supremacy of data quality over quantity, even in the pre-training phase. Our meticulous data collection and processing efforts included a complex suite of preprocessing, prefiltering, language identification, cleaning, filtering, and deduplication, ensuring the high quality of our corpus. Furthermore, we performed data contamination detection on downstream benchmark test sets to eliminate duplicates and conducted continual pre-training experiments, booting the performance on common mathematical reasoning benchmarks. We aim for our MathPile to boost language models' mathematical reasoning abilities and open-source its different versions and processing scripts to advance the field (available at https://github.com/GAIR-NLP/MathPile/).

## 1 Introduction

High-quality, diverse pre-training corpora form the cornerstone for developing powerful foundation models, enabling AI assistants like ChatGPT  to exhibit balanced competencies across a broad spectrum of tasks . In this work, our concern centers on mathematical reasoning capabilities within foundational language models [13, 5, _inter alia_], for which can potentially boost the application in education tools, automated problem solving, data analysis, etc., thereby improving user experience. To facilitate this, we are not directly building a model, but rather focusing on a more fundamental aspect: _creating a high-quality and diverse pre-training corpus tailored for the math domain_, namely MathPile. Specifically, our work is significantly different from the previous work in the following characteristics (cf. Table 1 for comparison):

**Math-centric**. Previous open-sourced pre-training corpora have typically focused on general domains, such as Pile , RedPajama  and Dolma . Others have concentrated on multilingual aspects or programming languages, such as ROOTS  and The Stack , respectively. However, a notable absence in these offerings is a corpus specifically tailoring for mathematics. While there exist some corpora designed for training or continually improving math-specific

Figure 1: Key features of MathPile.

language models, such as Minerva's mathematical training dataset  and OpenAI's MathMix , these are not open-sourced. Note that a recent work concurrent with ours, OpenWebMath , although math-centric, is solely sourced from web pages. We will discuss the comparison with it later. Recognizing this gap, we aim to democratize access to high-quality mathematical corpus, fostering inclusive advancements in language models' mathematical reasoning.

**Diversity**. While Hendrycks et al.  introduced AMPS, a problem set ranging from elementary mathematics to multivariable calculus (K-12 level) for pre-training purposes, it lacks content at the college-level and more challenging competition-level mathematics, focusing instead on a supervised dataset rather than an extensive corpus. The ProofPile corpus, introduced by Azerbavey et al. , aims to improve autoformalization and formal proving capabilities in models, yet its scope is confined to formal proving, not covering the broader mathematical domain from K-12 to postgraduate level. Concurrently with our work, Paster et al.  propose the OpenWebMath corpus, featuring a corpus composed of mathematical web pages. However, our corpus goes beyond web pages, integrating high-quality mathematics textbooks, lecture notes, scientific papers from arXiv in the field of mathematics, and carefully selected content from StackExchange, ProofWiki, and Wikipedia among others, which positions our corpus as a richer and more diverse mathematical resource for language models.

**High-Quality**. Recent studies have increasingly highlighted the detrimental effects of low-quality and repeated content in pre-training corpora on model training, as evidenced in various works . The importance of high-quality datasets has thus come to the fore. It has been shown that properly filtered and deduplicated web data can yield models as equally powerful as those trained on curated, high-quality corpora . This similar practice has been recently adopted in several notable studies . A notable example is the 1.3 billion-parameter code-focused model pre-trained on synthetically generated textbooks and filtered web pages, a project that broke existing scaling laws although did not open source its data . It's important to emphasize that quality of the corpus is far more significant than its quantity. For instance, OpenAI's MathMix comprises only 1.5 billion tokens. In this work, we diligently adhere to the principle of _less is more_, as outlined in Zhou et al. . To achieve a high-quality corpus, Unlike other approaches that uniformly process all data, we have conducted specialized preprocessing and prefiltering for each data source before global data processing (including language identification, filtering, cleaning, and deduplication). We're dedicated to refining and optimizing our corpus, making a distinctive contribution to the field.

**Data Documentation**. Auditing large-scale pre-training corpora is essential for identifying content characteristics, intended uses, and potential biases, despite challenges due to their size . However, many such corpora are released without proper documentation . Recent audits of certain pre-training corpora have uncovered issues such as irrelevant content , copyright infringement , and inclusion of test sets for downstream tasks , highlighting the need for detailed data sheets and transparent documentation. To this end, following previous efforts to enhance corpora transparency, we have provided a dataset sheet for our MathPile (see Table 6). Throughout our extensive data processing workflow, numerous documents were annotated for quality, such as language identification scores and the ratio of symbols to words (as exemplified in Figure 12). These quality annotations enable future users to apply their specific filters based on these scores. Additionally, we have conducted extensive deduplication for this corpus and performed data contamination detection with downstream benchmark test sets, removing any duplicated samples identified (cf. SS 3.4). Interestingly, we have also discovered a significant number of questions from downstream test sets in OpenWebMath (cf. SS 3.4). This underscores the importance of meticulous data documentation. We plan to release different versions to facilitate future use. See Appendix C for examples.

Additionally, we conducted continual pre-training experiments on MathPile and found that it generally enhances the performance of language models across various mathematical reasoning benchmarks, with an average improvement of up to 5% (cf. SS 4.2). In conclusion, we hope to facilitate the growth of the field of AI for mathematics by contributing this specialized, high-quality, diverse corpus focused on the mathematical domain while maintaining utmost transparency about the data for practitioners.

## 2 The Collection of Corpora

In order to construct MathPile, we gather data from a variety of sources, which also includes a component of manual collection. We provide an ethics statement regarding copyright in Appendix B.

Mathematical TextbooksTextbooks, covering mathematical concepts, exercises, and solutions, are valuable for _educational purposes_ for both humans and machines. Recent studies, even though not focused on math, support this view with synthesized textbooks . To collect these genuine and high-quality textbooks, we began by conducting extensive manual searches across the internet, seeking open-source and freely accessible mathematics-related textbook websites. Afterwards, we proceeded to download these PDF files, resulting in a collection of 38 K-12 level textbooks, along with 369 college-level mathematics textbooks that cover a wide range of subjects including linear algebra, probability theory, calculus, and optimization. In addition to these textbooks, we also included 467 college course handouts and lecture notes, which tend to be more concise compared to full-length textbooks. Subsequently, we employed the Mathpix API2 to parse the PDFs into markdown format. Then, we meticulously cleaned up extraneous elements such as parsed image URLs, preface sections, table of contents, acknowledge sections, index sections, and consecutive empty lines within the parsed content, resulting in a total of 874 documents.

We also refined high-quality mathematics-related synthetic textbooks from OpenPhi Project.3 It is an open-source counterpart to the Phi work . While the underlying model and generation process differ, the output encompasses a broad spectrum of subjects, extending beyond programming. To isolate mathematics-related documents, we employed a straightforward criterion: the presence of the symbol "$" combined with common mathematical expressions like "\(\{^{}}\}\)" and "\(\{}\)". While "$\(\)" alone is not always reliable, combining it with these symbols improves accuracy based on manual verification. This approach yielded 3,889 documents from an initial pool of 124,493. As the volume of pre-training data escalates, the synthesis of high-quality data becomes increasingly crucial. More advanced filtering methods and mathematical corpora synthesis are left for future exploration.

**Mathematical Papers from ArXiv** ArXiv offers a free distribution service and serves as an open-source archive housing millions of scientific papers. It also provides invaluable training data for numerous powerful language models [61, 59, _inter alia_]. In our endeavor to collect mathematical papers from ArXiv, we identify 50 sub-subjects spanning Mathematics, Computer Science, Statistics, Physics, Quantitative Finance and Economics. Our process involved filtering ArXiv's metadata4 to focus on the chosen subjects (cf. Table 7), followed by accessing the source LaTex files (if available). We exclusively retained the LaTex files and consolidated multiple files based on their respective order as indicated by commands such as "\(\)" and "\(\)" within the main LaTex file of each paper. Subsequently, we undertook extensive transformations to enhance data clarity and consistency, including removing comments, reverting macros, omitting figures but keeping captions, excluding acknowledgements and references, condensing empty lines, replacing some formatting commands, substituting titles, and preserving only the main body content (cf. SS D for more details). Finally, we compiled 347,945 meticulously cleaned LaTex documents (around 8.5 billion tokens), with each document corresponding to a single paper.

  
**Datasets** & **Open** & **Target Domain** & **\# Textbooks** & **Has Synth** & **Data Location** & **\# Textbooks** & **Source** \\  Minuvu & ✗ & Corpus & General Math & ✗ & ✗ & ✓ & 38.58 & arXiv. Web \\  MathMix & ✗ & Corpus + PS & General Math & 7 & ✓ & ✓ & 1.58 & 7 \\  ProofFile & ✓ & Corpus & Theorem Proving & 7 & ✗ & ✗ & 8.38 & arXiv. Textbooks, Lib., StaszErchang, \\  & & & & & & & & \\  OpenWebMath & ✓ & Corpus & General Math & ✗ & ✗ & ✗ & 14.78 & Web \\  DM-Mathematics & ✓ & PS & Math Competition & ✗ & ✓ & - & 4.48 & Synthesis \\  AMPS & ✓ & PS & Math Competition & ✗ & ✓ & ✗ & 0.78 & Khan Academy, Synthesis \\  MathFile (Ours) & ✓ & Corpus & General Math & 3,979 & ✓ & ✓ & 9.58 & arXiv. Textbooks, StaszErchang, \\  & & & & & & & & \\   

Table 1: The comparison of MathPile with other mathematical Corpora, where PS denotes the problem set type. For non-open-sourced corpora, details are inferred from literature, with unknowns marked as “?”. Token counts may vary by tokenizer; we use statistics from each dataset’s report and the GPTNeoX-20B tokenizer  for our corpus. DM-Mathematics is from Saxton et al. . “Minerva” refers to its dataset. ProofFile-2 , encompassing OpenWebMath and others, is excluded from this comparison.

Mathematical Entries in Wikipedia Wikipedia is one the largest and most popular free online encyclopedias, offering information on a wide range of topics, including history, science, technology, culture, and more. This extensive knowledge has proven to be highly beneficial for numerous natural language processing tasks [34, _inter alia_] and pre-trained language models [17, 61, _inter alia_]. To collect mathematical entries from Wikipedia, we downloaded the mathematics-focused (without pictures) dump of Wikipedia in English for the month of August 2023. We extracted the HTML documents from the dump using the library libzim, resulting in approximately 106,900 documents. Subsequently, we converted these HTML documents into markdown format using the html2text library5 while removing the hyperlinks following the practice of LLaMA . We retained the alternative text content but excluded image (often in SVG format) paths. Additionally, we eliminated extra newlines within paragraphs and condensed more than three consecutive empty lines to two using regular expressions. Further refinement involved the removal of boilerplate content at the bottom of the pages, typically denoted with phrases like "This article is issued from Wikipedia. The text is...". In the end, our efforts yielded a collection of 106,881 mathematical Wikipedia entries, about 0.8 billion tokens.

Entries from ProofWikiProofWiki, an online compendium of mathematical proofs, has been instrumental in advancing the fields of autoformalization and formal proof proving, as evidenced by NaturalProofs  and ProofPile. We sourced data from the ProofWiki dump dated April 9, 2022 (provided by the Internet Archive), mirroring the preprocessing approach employed by NaturalProofs, which was based on the version from November 12, 2020. Specifically, this involved leveraging the BeautifulSoup library to parse all Wiki pages followed by the extraction of raw text content using the wikitextparser library. This process yielded a substantial collection of mathematical content, totaling about 7.6 million tokens, comprising 10,328 definitions and 13,511 theorem-proof pairs. To facilitate better data organization, we formatted the definitions using the "definition" environment, and the theorem-proof pairs within the "section" environment with their respective titles serving as the section headings, similar to ProofPile.

Mathematical Discussions on StackExchangeStackExchange, renowned for its network of community-powered question-and-answering websites, spans a wide array of topics, each concentrated on a particular topic. Its high-quality data trove has significantly contributed to the development of various language models [61, 71, _inter alia_]. In our study, we identify eleven sites within this network, including five dedicated to mathematics (such as Mathematics and MathOverflow) and six others in closely related fields like Physics (cf. Table 8). Our data collection process began with downloading the site dumps from August 2023 (provided by the Internet Archive). We only retained the essential components in the posts, namely questions and answers (also associated meta information). To convert HTML documents to raw text, we utilized the BeautifulSoup library, coupled with a meticulous removal of invalid XML characters. We then systematically paired questions and their respective answers. Each question typically garners multiple responses, each with its own score and in some cases, an endorsement as the accepted answer by the questioner. To guarantee quality, we applied a quality threshold (i.e., 5) for filtering. Questions underwent filtering based on the threshold, whereas answers were assessed by either the threshold or the score of the accepted answer, whichever was lower. Unanswered questions scoring at least 10 were preserved for potential future use. This rigorous process resulted in a rich collection of data, comprising 267,919 questions, 435,129 answers, and 3,418 unanswered questions, totaling about 254 million tokens.

Mathematical Web Pages from Common CrawlCommon Crawl, an archive of web data since 2007, is crucial for training advanced language models like GPT-3  and LLaMA. Our work targets extracting math web pages from SlimPajama , a cleaned and deduplicated version of RedPajama, focusing on its CommonCrawl and C4 subsets. Eschewing the common approach of using neutral network-based filtering, we opt for heuristic rule-based methods. Our procedure began with the creation of TF-IDF features, derived from our curated high-quality textbooks. During this process, we removed the stop words, limited the features to a maximum of 10,000, and employed white space tokenization. Upon the observation of the resulting vocabulary, we identified 11 commonly used LaTex commands, integral to mathematical expressions. We utilize these commands as a basis for a hard match within each document. A document is classified as mathematical if it contains any of these commands along with the symbol "$5", typically indicative of a mathematical document. Thisrule-based approach, though simplistic, proved to be highly effective, especially given the vast size of the Common Crawl corpus. We also experimented with more intricate dense embedding-based methods to identify mathematical documents, but these resulted in poor recall. Our efforts resulted in the compilation of a substantial collection of mathematical web pages: 4,307 documents from SlimPajama-C4 and 72,137 documents from SlimPajama-CommonCrawl, totaling approximately 633 million tokens. We acknowledge the potential for more efficient methods to sift mathematical documents from Common Crawl snapshots, an area we plan to explore in future work.

## 3 Global Data Processing

After conducting specific data preprocessing for each data source during the data collection process, we globally engage in three critical steps: language identification, filtering, and deduplication, to ensure the quality of the entire corpus, as shown in Figure 2.

### Language Identification

To filter non-English documents, we utilized the fastText language identifier, which was trained on Wikipedia, Tatoeba, and SETimes [29; 23]. A common practice is to classify a document as its respective language if the score exceeds 0.5, a threshold also employed by CCNet . However, during the application of this practice, we encountered a considerable number of false positives--cases where documents were erroneously filtered as non-English when, in fact, they were written in English but contained a substantial amount of mathematical symbols. We attribute this issue to the domain gap between the fastText training datasets and the mathematical content. To enhance non-English document filtering, we set customized score thresholds for each data source. Specifically, Wikipedia and StackExchange thresholds were set at 0.1, arXiv at 0.3, and Common Crawl at 0.5. No thresholds were applied to ProofWiki and Textbooks due to manual verification ensuring English content. This refinement removed about 8,400 documents, totaling 231 million tokens.

### Data Cleaning and Filtering

Despite thorough preprocessing, some documents, especially from sources like Wikipedia and Common Crawl, lack quality for language modeling due to brevity or automated content. Existing filtering methods [55; 54; 41; 51; 12], while detailed, risk excluding valuable documents in our math-focused corpus if directly applying them as-is. To address this issue, we developed a unique set of cleaning and filtering heuristic rules, specifically crafted for the mathematical domain and drawing from past studies. These rules are aimed at removing meaningless lines (such as boilerplate content) and documents. Specifically, we (1) detect lines containing "lorem ipsum" and filter them out if the resulting line is less than 5 characters; (2) detect lines containing "javascript" that also include "enable", "disable" or "browser" and are under 200 characters, and filter them; (3) filter lines containing fewer than 10 words that include keywords like "Log in", "sign-in", "read more...", or "items in cart."; (4) filter documents if the ratio of uppercase words exceeds 40%; (5) filter lines that end with "..." if they constitute more than 30% of the entire document; (6) filter documents if the

Figure 2: The creation process of MathPile. We additionally perform data contamination detection on benchmark test sets (cf. § 3.4). We visualize its component ratios by document counts (Right).

ratio of non-alphabetic words surpasses 80%; (7) exclude documents with an average English word length outside the range of (3, 10); (8) discard documents that lack at least two common stop words such as "the", "be" "to" "of" "and" "that" or "have"; (9) filter out documents if the ratio of ellipses (...) to words exceeds 0.5 (e.g., progress bars); (10) remove documents where 90% of lines start with bullet points; (11) filter documents including less than 200 characters after removing spaces and punctuation marks.

These meticulously crafted rules enabled us to curate a high-quality mathematical corpus. They also facilitated the assignment of quality annotations to each document from Wikipedia and Common Crawl. These annotations provide researchers and developers with the flexibility to filter the data according to their criteria, catering to specific needs (as shown in Figure 12). This process resulted in filtering approximately 1,100 documents, removing 17 million tokens.

### Data Deduplication

Given that our corpus originates from diverse sources, it is inevitable that there will be repetitions both within and across these sources. Deduplication is vital for training efficiency and reducing data memorization, addressing both exact and near-duplicates . We utilized the MinHash LSH algorithm  built on the implementation of text-dup  and Lee et al. , to process large-scale corpora efficiently. Specifically, our process involved splitting each document using whitespace and constructing 5-grams, applying the "shal" hash function, and configuring 450 buckets with 20 minhashes each, totaling 9,000 minhashes per document, as per RefinedWeb's guidelines .

During the deduplication process within each source, we encountered numerous exact and near-duplicate documents across various sources: 304 in arXiv, 623 in Common Crawl, 83,716 in Wikipedia, 783 in textbooks (primarily synthetic), and 144 duplicate questions in StackExchange. Despite finding many near-duplicates in ProofWiki, they were differentiated as unique lemmas, proofs, or definitions, leading us to retain these entries (cf. Table 13). Manual review revealed significant duplication in Wikipedia due to collecting multiple historical document versions and in StackExchange from reposts across different forms (e.g., Math and MathOverflow) for broader visibility (cf. Table 16). We provide near-duplicate examples from each data source in Table 11-16. Cross-source deduplication revealed minimal overlap, with a single StackExchange question duplicated in Common Crawl, which was removed. This eliminated around 714 million tokens.

Note that we also experimented with using suffix arrays  to eliminate exact match sequences within documents. However, it tended to remove common phrases like "Questions: ". While it can effectively remove some templated content, it also disrupts the contextual integrity of our corpus. Consequently, we decided against employing this in order to preserve the context of our data.

### Data Contamination Detection

As pre-training corpora grow, encountering data contamination becomes inevitable, where evaluation examples are found in the training set. Traditionally, post-hoc analysis, employing n-gram overlap, assesses contamination levels (e.g., GPT-2 , GPT-3 , FLAN , LLaMA-2 ). We advocate for early contamination detection during dataset creation to prevent irreversible damage as delaying exacerbates issues (c.f., previous study ). Here, we utilize popular mathematical reasoning benchmarks, namely GSM8K , MATH , MMLU-STEM , AGIEval-SAT-MATH , MathQA  and AQuA  to detect data contamination.

To detect data contamination, we aggregated questions and answers from benchmark tests into a reference set, considering only questions for MMLU, AGIEval, MathQA and AQuA due to its multiple-choice format. Intuitively, math problem solutions often involve diverse reasoning steps, making questions easier to detect for contamination in pre-training data due to their more fixed nature. We utilized line-level exact match detection, dividing documents into lines, hashing each with MD5 (taking the first 64 bits and the line itself to form sets), and applied this to both our corpus and the test sets. If a test set line and its hash match exactly with our dataset, it's marked as contamination.

  
**Corpus** & **MATH** & **MMLU-STEM** \\  Ours & 23 & 2 \\ OpenWebMath & 195 & 65 \\   

Table 2: Benchmark test set occurrences in pre-training corpora, with numbers representing minimum occurrences, given potential undetected duplicates.

After our detection process, we found 23 questions from MATH and 2 from MMLU-STEM in our corpus (see Table 2), with no accompanying answers. No contamination was detected in other benchmarks. These duplicates mainly originated from StackExchange, Textbooks, and Common Crawl (see Table 17 and Table 18 for examples). Notably, questions from AMC mathematics competition books, also used in the MATH benchmark, were identified in Textbooks. We extended our analysis to OpenWebMath, uncovering more duplicate questions from MATH and MMLU (cf. Table 19), although many were repeats. This aligns with similar findings by Azerbayev et al. . These instances highlight the importance of vigilance in creating pre-training corpora to avoid undermining downstream benchmarks. We removed all detected exact matches to mitigate data contamination, resulting in MathPile corpus.

## 4 Data Analysis

### Statistics

We present detailed statistical information for each component of MathPile in Table 3, such as the number of documents and the count of tokens. Following our meticulous and comprehensive data collection and processing process, we obtain 29GB of high-quality and diverse math-centric corpus, encompassing around 9.5 billion tokens, from an initial volume of 2.2TB of raw data (cf. Figure 2). Compositionally, arXiv constitutes the largest portion of MathPile, while Textbooks represent the smallest share but are of exceptionally high quality.

We analyze the document length (in terms of token numbers) and their respective proportions from each source within MathPile, which is visualized in Figure 3. Intuitively, if the data from each source contains a higher amount of near-duplicates or machine-generated content, the distribution of documents of similar lengths becomes more prevalent, leading to a less smooth distribution curve. Figure 3 shows that, thanks to our thorough and rigorous processing, the document length distribution in MathPile is relatively smooth across different sources. Note that ProofWiki, due to its fixed format of definitions, lemmas, and proofs, naturally contains shorter content, resulting in a distribution with many similar lengths. We can also observe that, on average, the documents from arXiv and Textbooks tend to be lengthier, while those from ProofWiki and StackExchange are generally shorter.

### Continual Pre-training Experiments

We chose Mistral-7B-v0.1  (the state-of-the-art open-source model at the time) for continual pre-training. We segmented packed text into chunks with a window size of 4,096 and continued

  
**Components** & **Size (MB)** & **\# Documents** & **\# Tokens** & **max(\# Tokens)** & **min (\# Tokens)** & **ave (\# Tokens)** \\  Textbooks & 644 & 3,979 & 187,194,060 & 1,634,015 & 256 & 47,046 \\ Wikipedia & 274 & 22,795 & 59,990,005 & 109,282 & 56 & 2,632 \\ ProofWiki & 23 & 23,839 & 7,608,526 & 6,762 & 25 & 319 \\ CommonCrawl & 2,560 & 75,142 & 615,371,126 & 367,558 & 57 & 8,189 \\ StackExchange & 1,331 & 433,751 & 253,021,062 & 125,475 & 28 & 583 \\ arXiv & 24,576 & 343,830 & 8,324,324,917 & 4,156,454 & 20 & 24,211 \\  Total & 29,408 & 903,336 & 9,447,509,696 & - & - & 10,458 \\   

Table 3: The components and data statistics of MathPile.

Figure 3: Document length distribution (log-scale).

pre-training for 3 epochs with a global batch size of 1024. We employ a cosine learning rate schedule with a maximum learning rate of 1e-5 and 1% warmup steps. All experiments were conducted on NVIDIA A100 8*80GB GPUs. For evaluation, we employ a range of benchmarks - GSM8K, MATH, MMLU-MATH, AGIEval-SAT-MATH, MathQA, AQuA - to assess varying levels of mathematical reasoning abilities, comparing all models using the same few-shot prompting with greedy decoding.

**The Effectiveness of MathPile** We further pre-trained Mistral-7B-v0.1 on several subsets, respectively. As shown in Table 4, overall, continual pre-training on the subsets generally enhances performance across diverse math benchmarks, albeit to varying degrees. There are exceptions, such as the lack of improvement on GSM8K after training on StackExchange; we suspect this is due to community users rarely asking basic arithmetic questions on StackExchange. Continual pre-training on arXiv leds to a slight performance boost on GSM8K and MMLU-MATH, but a degradation on MATH, SAT-MATH, and MathQA. We attribute this performance degradation to the disparity between the math knowledge present in arXiv papers and that required for the downstream benchmarks. We also conducted pre-training on a collection of Textbooks, Wikipedia, StackExchange, and CC. Experimental results indicate improved performance on GSM8K and MATH, but not on other benchmarks. Due to limited computational resources,6 we did not extensively experiment with the entire dataset or combine data from MathPile's subsets and existing general corpora, leaving these valuable aspects for future work. Note that we also report some evaluation results on general language benchmarks provided in Appendix G.

Furthermore, we also conducted continual pre-training on some existing corpora listed in Table 1 for comparison, including AMPS, DM-Mathematics and a random subset of OpenWebMath, cleansed of data leakage, in volumes approximately equal to that of Textbooks. Surprisingly, pre-training directly with these synthetic datasets degraded model performance. We attribute this to the narrow, monotonous structure of AMPS and DM-Mathematics problem sets, making them unsuitable for standalone pre-training; such datasets generally yield better results when combined with broader corpora for pre-training . Additionally, the OpenWebMath subset produced even less improvement than the same or smaller scale subsets of MathPile, such as Textbooks and Wikipedia (cf. Table 4), likely due to a need for more tokens to show substantial gains. These results underscore the superior quality of our data.

**The Effectiveness of Data Processing Pipeline** We utilized the Wikipedia subset as a testbed to evaluate our data processing pipeline. We distinguished between raw Wikipedia, which is collected but not globally processed, and cleaned Wikipedia, which has undergone global data processing. Additionally, we performed an ablation study on LaTeX display issues in Wikipedia (cf. Figure 11), attributed to HTML-to-text conversion tools, by comparing documents with problematic and correct LaTeX displays. Following previous settings, we executed continual pre-training on these datasets.

  
**Models** & **GSM8K** & **MATH** & **SAT-** & **MMLU-** & **MathQA** & **AQuA** \\  Mistral-7B-v0.1 & 47.38 & 10.08 & 47.27 & 44.92 & 23.51 & 27.95 \\  + Textbooks (0.56B) & **48.97** & **12.10** & **56.36** & **48.93** & **30.38** & **33.07** \\ + Wikipedia (0.18B) & **49.96** & **9.96** & **53.63** & **47.16** & **28.97** & **35.43** \\ + StackExchange (0.87B) & 43.05 & **11.64** & **47.27** & 43.31 & **27.67** & **30.70** \\ + Common Crawp (1.83B) & 45.36 & **9.88** & **50.45** & **45.37** & **25.79** & **31.88** \\ + arXiv (NAV) & 47.91 & 7.50 & 42.72 & **46.34** & 18.05 & 27.53 \\ + Textbooks, Wikipedia, StackEx, CC (4B) & **49.88** & **11.70** & **43.18** & 43.75 & 23.24 & 25.19 \\  + AMPS (1B) & 0.08 & 0.82 & 3.18 & 0.47 & 10.99 & 8.27 \\ + DM Mathematics (5B) & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ + Sampled OpenWebMath (0.59B) & 43.21 & 7.86 & **47.72** & **47.52** & 21.80 & 24.80 \\   

Table 4: Results on each subset of MathPile and sampled OpenWebMath. The numbers in parentheses represent the number of tokens trained. **Bold** results denote improvements over the original Mistral.

  
**Models** & **Global Data** & **Fix Latex** & **GSM8K** & **MATH** & **SAT-** & **MMLU-** & **MathQA** & **AQuA** \\  Mistral-v0.1.7B & - & - & 47.38 & 10.08 & 47.27 & 44.92 & 23.51 & 27.95 \\  + Sample raw Wikipedia (0.55B) & ✗ & ✗ & 41.92 & 6.28 & 20.90 & 23.70 & 24.72 & 24.01 \\ + Full raw Wikipedia (2.18B) & ✗ & ✗ & 23.20 & 4.48 & 13.64 & 25.59 & 27.04 & 23.62 \\ + Full cleaned but LaTeX issued Wikipedia (0.23B) & ✓ & ✗ & 47.15 & 8.58 & 46.81 & 42.92 & 21.00 & 31.88 \\  + Full cleaned Wikipedia (0.18B) & ✓ & ✓ & **49.96** & 9.96 & **53.63** & **47.16** & **28.97** & **35.43** \\   

Table 5: Ablation study on data processing pipeline and LaTeX display issue resolution Results in Table 5 indicate that skipping our pipeline notably reduces Mistral's mathematical reasoning abilities, unaffected by increased training size (i.e., 2.18B). Furthermore, correct LaTeX display in documents is vital for enhancing reasoning capabilities, as shown by the last two rows of Table 5. These findings underscore our pipeline's effectiveness and shed light on the superior importance of data quality over quantity, even in the continual pre-training phase.

## 5 Related Work

Pre-training Corpora for Language ModelsIn language modeling, early models like GPT  and BERT  are trained on resources such as Books  and Wikipedia. Later models like GPT-2  and T5  expand training to include web data from Reddit (WebText) and Common Crawl (C4). GPT-3  enlarges its corpus to 300 billion tokens, combining Common Crawl, WebText, Books, and Wikipedia. Pile  introduces a diverse collection of 22 datasets for large-scale pre-training. The Gopher project  compiles a 10.5TB corpus, and PaLM  is built from a 780 billion-token corpus, both closed-source. BLOOM  uses the ROOTS dataset  for multilingual pre-training. The Stack Koetetkov et al.  provides a 3.1 TB code dataset. LLaMA  utilizes various data sources but doesn't release its corpus, unlike RedPajama  and its de-duplicated version SlimPajama . RefinedWeb shows web-only corpora can rival curated ones . Recent models like GPT-4 , Mistral-7B  and the lastest Gemini  have refrained from open-sourcing data. Constructing diverse, high-quality pre-training corpora is crucial for narrowing the performance gap with closed-source models, reflecting our work's aim.

Pre-training Benchmarks and Corpora for Mathematical ReasoningThe challenge of endowing models with human-like mathematical reasoning has attracted significant interest from the machine learning and natural language processing communities. To evaluate models' mathematical capabilities, several benchmark datasets have been developed, including AQuA , DM-Mathematics , SVAMP , GSM8K , and MATH , which cover a range of complexities from basic arithmetic to competition-level mathematics. Additionally, benchmarks like NaturalProofs  focus on theorem-proving capabilities, while the STEM subset of MMLU  evaluates understanding across multiple tasks in science, technology, engineering, and mathematics. To improve models' mathematical reasoning, pre-training corpora like AMPS  (despite a large-scale synthetic exercise set), ProofPile , and OpenWebMath  have been introduced, targeting various levels of mathematical problem-solving and theorem proving. Unlike Google's Minerva  and OpenAI's MathMix , which are not public, our work focuses on creating a high-quality and diverse mathematical corpus from diverse sources to fill existing gaps.

## 6 Conclusion and Limitations

In this work, we present MathPile, a specialized corpus centered around mathematics, characterized by its diversity and high quality. Throughout its development, we meticulously source and gather data, applying a rigorous and math-specific pipeline. This pipeline encompasses various stages such as preprocessing, prefiltering, language identification, cleaning and filtering, and deduplication, all aimed at maintaining the high quality of the corpus. We also conduct data contamination detection to remove duplicates from popular mathematical reasoning benchmark test sets, crucial for ensuring their integrity and effectiveness, an aspect often overlooked in other similar works. We aim for our MathPile to enhance mathematical reasoning in language models, whether used alone or in conjunction with other datasets, to promote broader applications.

This dataset also has some limitations. Many detailed decisions in its creation were made empirically, which may not always be optimal, and verifying decisions directly can be challenging. Moreover, the data scale is insufficient for training extra-large models; subsets like the common crawl could be expanded. Furthermore, the dataset is focused primarily on English, highlighting the need to construct high-quality datasets for other languages. Future research could also explore data mixing  and model-based pre-training corpus refinement [68; 72] to enhance dataset quality and model performance.