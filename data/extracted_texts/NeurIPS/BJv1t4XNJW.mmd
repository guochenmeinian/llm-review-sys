# Slot State Space Models

Jindong Jiang

Rutgers University

&Fei Deng

Rutgers University

&Gautam Singh

Rutgers University

&Minseung Lee

KAIST

&Sungjin Ahn

KAIST

Correspondence to jindong.jiang@rutgers.edu and sungjin.ahn@kaist.ac.kr.

###### Abstract

Recent State Space Models (SSMs) such as S4, S5, and Mamba have shown remarkable computational benefits in long-range temporal dependency modeling. However, in many sequence modeling problems, the underlying process is inherently modular and it is of interest to have inductive biases that mimic this modular structure. In this paper, we introduce SlotSSMs, a novel framework for incorporating independent mechanisms into SSMs to preserve or encourage separation of information. Unlike conventional SSMs that maintain a monolithic state vector, SlotSSMs maintains the state as a collection of multiple vectors called slots. Crucially, the state transitions are performed independently per slot with sparse interactions across slots implemented via the bottleneck of self-attention. In experiments, we evaluate our model in object-centric learning, 3D visual reasoning, and long-context video understanding tasks, which involve modeling multiple objects and their long-range temporal dependencies. We find that our proposed design offers substantial performance gains over existing sequence modeling methods. Project page is available at https://slotssms.github.io/

## 1 Introduction

State space models (SSMs) have recently emerged as a promising class of sequence models, achieving remarkable success in language modeling  due to their long-term memory capability and computational efficiency. Compared to Transformers  whose attention mechanisms also facilitate capturing long-range dependencies, SSMs are more efficient during both training and inference. Notably, SSMs offer parallel training with sub-quadratic complexity, and recurrent generation with constant cost per time step. These benefits have motivated the application of SSMs to sequences of other modalities such as audio  and video .

Typically, SSMs use a monolithic state vector to summarize all past information. This design can struggle to model sequences with modular underlying structures, which are common in physical processes and real-world dynamics. For example, physical objects largely follow independent dynamics based on their own properties, with strong interactions happening only sparsely (_e.g._, when objects come in close contact). A monolithic state vector would excessively entangle the dynamics of different entities, thereby hurting generalization. It could be beneficial to incorporate inductive biases for independent mechanisms  into the sequence modeling architecture.

Recent progress in object-centric learning  has led to several methods for discovering modular object-centric structures and modeling their dynamics from videos with no or only weak supervision . Similar to RIMs , they build modularity into the RNN architecture to separately keep track of the dynamics of each object. However, RNNs are prone to vanishinggradients  and are not amenable to parallel training, making it hard to scale these methods up to modeling long-range effects that span hundreds of time steps.

In this paper, we propose Slot State Space Models (SlotSSMs), a novel and general SSM framework that have built-in inductive biases for discovering and maintaining independent mechanisms. Instead of using monolithic state vectors, SlotSSMs maintain a set of modular slot states whose transition dynamics are designed to be largely independent, with only sparse interaction across slots introduced through the bottleneck of self-attention. The number of slots can be flexible across the layers of SlotSSMs, allowing slots to have a different level of abstraction at each layer. Furthermore, SlotSSMs inherit the strengths of SSMs, namely parallelizable training, memory efficiency, and long-range reasoning capabilities, giving it an advantage over methods based on RNNs and Transformers.

Our contributions are summarized as follows. First, we propose SlotSSMs, a novel and general architecture that incorporates independent mechanisms into SSMs for modeling inherently modular physical processes. Second, we show that SlotSSMs can be specialized to solve object-centric learning tasks. It achieves comparable or better performance than existing RNN-based methods and the Transformer baseline that we develop, while being more computationallly efficient. Third, we further investigate the abilities of SlotSSMs as a general sequence modeling framework, demonstrating its advantages in video understanding and prediction, long-range reasoning, and 3D visual reasoning.

## 2 Preliminaries

A state space model (SSM) defines a mapping between an input sequence \(_{1:T}^{T D}\) and an output sequence \(_{1:T}^{T D}\) via the recurrence [28; 27; 58; 50]:

\[_{t} =}_{t}_{t-1}+}_{t}_{ t}\;,\] (1) \[_{t} =_{t}_{t}\;.\]

Here, \(T\) is the sequence length; \(_{t},_{t}^{D}\) are input and output vectors at time \(t\); and \(_{t}^{H}\) is the hidden state summarizing the history \(_{ t}\). The matrices \(}_{t}^{H H}\), \(}_{t}^{H D}\), and \(_{t}^{D H}\) are designed with learnable parameters in specific ways that encourage modeling long-range dependencies while maintaining computational efficiency. For example, \(}_{t}\) commonly takes a diagonal or block-diagonal form, with its (complex) eigenvalues distributed close to the unit circle at initialization [25; 27; 29; 58; 50].

When \(}_{t},}_{t},_{t}\) are time-invariant (constant over \(t\)), the computation of \(_{1:T}\) can be parallelized, enabling efficient training. Recent works [24; 9] further show that conditioning these matrices on the input \(_{t}\) does not hinder training efficiency. They employ learnable functions \(}^{D}^{H H},}^{D}^{H D},^{D} ^{D H}\) to generate input-dependent matrices:

\[}_{t}=}(_{t})\;,} _{t}=}(_{t})\;,_{t}=(_{t})\;.\] (2)

This allows the model to selectively emphasize or ignore information based on the input, leading to more flexible sequence modeling.

Figure 1: **SlotSSMs vs existing models.** (a) SlotSSMs incorporate modularity through independent state transitions and sparse interactions via self-attention. (b) Traditional SSMs utilize a monolithic state vector for all past information. (c) Multi-slot Transformer-based models offer modularity but with high computational complexity. (d) Multi-slot RNN-based models have modular states but canâ€™t parallelize training (red lock). SlotSSMs combine parallelizable training, memory efficiency, and modularity for efficient temporal modeling.

Due to the (block-)diagonal structure of \(}_{t}\) limiting cross-dimensional information flow, SSMs are typically interleaved with mixing layers (e.g., linear projections or MLPs) to mix information across dimensions. Alternatively, using dense \(}_{t}\) and \(_{t}\) matrices can also enhance mixing.

## 3 Slot State Space Models (SlotSSMs)

Standard SSMs use monolithic vectors for inputs, outputs, and hidden states, and mix information across all dimensions. This lack of modularity could cause difficulties in modeling real-world dynamics such as object interactions, where the underlying process consists of multiple entities and is inherently modular . In this section, we present slot state space models (SlotSSMs), a new class of SSMs with built-in inductive biases for encouraging and preserving modularity.

Our key idea is to maintain a set of separate _slot state_ representations (called slots in short), and process the slots independently and symmetrically. To do this, we format the input vector \(_{t}^{D}\) as a concatenation of \(K\) slot representations \(\{_{t}^{k}^{D_{s}}\}_{k=1}^{K}\), where \(D_{s}=D/K\). The output \(_{t}^{D}\) and hidden state \(_{t}^{H}\) are formatted similarly:

\[_{t}=[_{t}^{1},,_{t}^{K}], _{t}=[_{t}^{1},,_{t}^{K} ],_{t}=[_{t}^{1},,_{t} ^{K}],\] (3)

where \(_{t}^{k}^{D_{s}}\) and \(_{t}^{k}^{H_{s}}\) are the output and the hidden state corresponding to slot \(_{t}^{k}\), with \(H_{s}=H/K\). In this section, we focus on preserving modularity when the input already complies with the slot format. When coupled with a slot encoder, the SlotSSM can help encourage the emergence of modularity from unstructured inputs such as video frames, as we will discuss in Section 4.

To preserve modularity, we make sure that SlotSSM do not mix information across different slots. More precisely, the hidden state \(_{t}^{k}\) and output \(_{t}^{k}\) only integrate information from the history of the corresponding input slot \(_{ t}^{k}\). As illustrated in Figure 2 (Right), this can be achieved by making \(}_{t},}_{t},_{t}\) block-diagonal, where the \(k\)-th block is only conditioned on the \(k\)-th slot:

\[}_{t}=(\{}(_{t}^{k})\}_{k=1}^{K}),}_{t}=(\{}(_{t}^{k})\}_{k=1}^{K} ),_{t}=(\{(_{t}^{k} )\}_{k=1}^{K}).\] (4)

**Implementation details.** The SlotSSM formulation in Equation 4 is general and can accommodate various choices of the \(},},\) functions. In our implementation, we adopt those from Mamba . Specifically, \(}(_{t}^{k}),}(_{t}^{k}),( _{t}^{k})\) are themselves block-diagonal matrices with \(D_{s}\) blocks, one for each slot dimension. The \(i\)-th blocks \(}^{(i)}(_{t}^{k})^{N N}\) and \(}^{(i)}(_{t}^{k})^{N 1}\) are obtained by discretizing their continuous-time counterparts \(^{(i)}\) and \(^{(i)}(_{t}^{k})\) using the time step \(^{(i)}(_{t}^{k})\) and the zero-order hold (ZOH) rule:

\[}^{(i)}(_{t}^{k}),\ }^{(i)}(_{t}^{k})= (^{(i)}(_{t}^{k}),\ ^{(i)},\ ^{(i)}(_{t}^{k})), i=1,,D_{s}\.\] (5)

Here, \(N=H_{s}/D_{s}\) is the hidden state size per slot dimension, \(^{(i)}^{N N}\) is an input-independent learnable model parameter, and \(^{(i)}^{D},^{(i)}^{D} ^{N 1}\) are learnable functions implemented as neural networks. Similarly, the \(i\)-th block \(^{(i)}(_{t}^{k})\) is computed by the learnable function

Figure 2: **SSM vs SlotSSM.** SlotSSM encourages modularity by maintaining a set of separate slot state representations, each updated independently using separate transition matrices and input matrices, allowing for more efficient and scalable modeling of complex sequences with inherent modular structures.

\(^{(i)}^{D}^{1 N}\). For simplicity and efficiency, \(^{(i)}\) and \(^{(i)}\) are shared across all \(1 i D_{s}\), and \(^{(i)}\) is parameterized as a diagonal matrix.

## 4 Modular Sequence Modeling with SlotSSM

The SlotSSM proposed in Section 3 are designed to preserve modularity when the input is already separated into slots. In this section, we complement SlotSSM with a slot encoder that extracts slot representations from unstructured inputs (Section 4.1), and a slot mixer that introduces sparse interactions across slots (Section 4.2). We then present a sequence modeling architecture (Section 4.3) that encourages discovery of underlying modular processes by stacking these components.

### Slot Encoder

We assume the unstructured input \(_{t}\) at each time step \(t\) is represented as a sequence of \(M\) tokens:

\[_{t}=(_{t}^{1},\;,\;_{t}^{M})\;,_{t}^{m} ^{D_{x}}\;.\] (6)

For example, image inputs can be CNN feature maps (\(M\) is the number of cells in the feature map), or as embeddings of non-overlapping image patches (\(M\) is the number of patches), as proposed in ViT . To extract \(K\) slot representations from \(_{t}\), we use \(K\) learnable CLS 2 tokens \(\{_{t}^{k}^{D_{x}}\}_{k=1}^{K}\) as queries and perform cross-attention with the input tokens through a Transformer :

\[\{_{t}^{k}\}_{k=1}^{K} =\{_{t}^{k}\}_{k=1}^{K},\;=\{_{t}^{m}\}_{m=1}^{M} \;\;.\] (7)

The Transformer also includes self-attention within the CLS tokens, allowing them to communicate with each other and capture information from different parts of the input, thereby facilitating the emergence of modularity. The slot representations are then obtained by applying a linear projection to the corresponding output embeddings of the CLS tokens:

\[_{t}^{k}=(_{t}^{k})\;, k=1,,K\;.\] (8)

### Slot Mixer

The slot encoder obtains slot decomposition purely based on single time steps, which can be suboptimal. In addition, the SlotSSM processes each slot fully independently, making it hard to correct mistakenly decomposed slots or model interarctions across slots. To resolve both issues, we interleave SlotSSM with slot mixers.

Figure 3: **Sequence modeling with SlotSSM.** Each layer includes a Slot Encoder, SlotSSM, and Slot Mixer. The Slot Encoder uses a Transformer to extract slots from inputs. The SlotSSM independently updates the slots via separate state transitions. The Slot Mixer introduces inter-slot interactions through self-attention.

The slot mixer consists of two residual blocks, and is applied to the outputs \(\{_{t}^{k}\}_{k=1}^{K}\) of the SlotSSM. The first block introduces interaction across slots through self-attention , whereas the second block uses MLP to further process the gathered information within each slot:

\[(_{t}^{1},\ ,\ _{t}^{K}) (_{t}^{1},\ ,\ _{t}^{K})+( (_{t}^{1}),\ ,\ (_{t}^{K}))\,\] (9) \[(_{t}^{1},\ ,\ _{t}^{K}) (_{t}^{1},\ ,\ _{t}^{K})+( ((_{t}^{1})),\ ,\ ( (_{t}^{K})))\.\] (10)

Here, \(()\) denotes layer normalization . Because \(_{t}^{k}\) carries information from the entire history of each slot, it provides the opportunity to refine the slot representations based on temporal dynamics.

### Sequence Modeling Architecture

We now present a generic architecture for modeling sequences with modular underlying processes. Given a sequence of unstructured inputs \(_{1:T}\), our goal is to obtain a set of \(K_{l}\) modular representations at each time step \(t\) and at each layer \(l\) that summarizes all underlying processes up to time \(t\).

In general, the number of slots \(K_{l}\) at each layer can be different, potentially allowing fewer but more abstract slots at higher layers. To accommodate this, we insert a slot encoder wherever the number of slots changes, and repurpose it to extract a different number of slots from existing slot representations. This is achieved by treating the slots output from the previous layer as keys and values in Equation 7. When the number of slots does not change, we can simply copy the slots from the previous layer.

As shown in Figure 3, our proposed architecture stacks the (optional) slot encoder, SlotSSM, and slot mixer together at each layer. Variables at layer \(l\) are denoted with the subscript '\(|l\)'. The slot mixer's output from layer \(l-1\), \(\{_{t|l-1}^{k}\}_{k=1}^{K_{l-1}}\), serves as input to layer \(l\). The initial input is \(\{_{t|0}^{k}\}_{k=1}^{K_{0}}\), where \(K_{0} M\). The computations at each layer \(l=1,,L\) are:

\[\{_{t|l}^{k}\}_{k=1}^{K_{l}} =\{_{t|l-1}^{k}\}_{k=1}^{K_{l-1 }}\,\] (11) \[\{_{t|l}^{k}\}_{k=1}^{K_{l}}, \{_{t|l}^{k}\}_{k=1}^{K_{l}} =\{_{t|l}^{k}\}_{k=1}^{K_{l}},\ \{_{t-1|l}^{k}\}_{k=1}^{K_{l}}\,\] (12) \[\{_{t|l}^{k}\}_{k=1}^{K_{l}} =\{_{t|l}^{k}\}_{k=1}^{K_{l}}\.\] (13)

The final output \(\{_{t|L}^{k}\}_{k=1}^{K_{L}}\) can be used for various tasks, such as predicting the next observation and the properties of underlying processes (_e.g._, position, velocity).

## 5 Object-Centric Learning with SlotSSM

In this section, we present a concrete example of adapting the generic sequence modeling architecture proposed in Section 4 to solve a specific task. We consider the task of object-centric representation learning from unannotated videos of interacting objects, a typical example of sequences with modular underlying structures. The goal is to obtain a representation for each individual object that captures relevant attributes such as object position, size, shape, color, _etc_. without any object-level annotation.

### Object-Centric SlotSSMs (OC-SlotSSMs)

Inspired by previous works [46; 67], we make slight modifications to our sequence modeling architecture to facilitate the discovery of modular structures. We call the resulting model OC-SlotSSMs. First, we use the same number of slots across all layers. It is thus unnecessary to have a slot encoder per layer. However, we find it helpful to still have it, but in another form that encourages iterative refinement of the slots. Specifically, we use the slots output from the previous layer \(\{_{t|l-1}^{k}\}_{k=1}^{K}\) as queries, and provide the input tokens \(\{_{t|0}^{m}\}_{m=1}^{M}\) as keys and values. Second, we introduce competition among slots in the attention layers of the slot encoder. We achieve this by using inverted attention [61; 67], which is essentially cross attention with the Softmax operation performed over the queries instead of the keys. This has the effect of softly assigning each input token to a slot, thereby promoting modularity. The computation at each layer \(l=1,,L\) can be summarized as follows:\[\{^{k}_{t|l}\}_{k=1}^{K} ==\{^{k}_{t|l}\}_{k=1}^{K},\; =\{^{m}_{t|0}\}_{m=1}^{M}\,\] (14) \[\{^{k}_{t|l}\}_{k=1}^{K},\;\{^{k}_{t|l}\}_{k=1}^{K} =\{^{k}_{t|l}\}_{k=1}^{K},\;\{^ {k}_{t-1|l}\}_{k=1}^{K}\,\] (15) \[\{^{k}_{t|l}\}_{k=1}^{K} =\{^{k}_{t|l}\}_{k=1}^{K}\.\] (16)

We note that the queries in the first inverted attention layer are the learnable CLS tokens \(\{^{k}_{t|0}\}_{k=1}^{K}\).

### Training Pipeline

Following previous works in object-centric learning [46; 39; 57], we adopt an auto-encoding training pipeline. Given a sequence of video frames \(\{_{t}^{H W 3}\}_{t=1}^{T}\), we obtain the input \(_{t|0}\) to our sequence modeling architecture by applying a CNN encoder to each frame \(_{t}\) and adding a positional embedding for each feature map cell. The output slots \(\{^{k}_{t|L}\}_{k=1}^{K}\) are each decoded into an object image \(}^{k}_{t}^{H W 3}\) and an alpha mask \(^{k}_{t}^{H W 1}\) by a spatial broadcast decoder . The final reconstruction \(}_{t}^{H W 3}\) is given by the alpha-composition of the object images:

\[}^{k}_{t},^{k}_{t}=(^{k}_{t|L})\, }_{t}=_{k=1}^{K}^{k}_{t})}{_{j=1 }^{K}(^{j}_{t})}}^{k}_{t}\.\] (17)

The training objective is to minimize the reconstruction error \(=_{t=1}^{T}\|}_{t}-_{t}\|_{2}^{2}\).

## 6 Related Work

**State Space Models (SSMs).** Popularized by S4 , SSMs have attracted growing interest in language modeling and as a sequence modeling framework in general. The original S4 follows the HiPPO theory  to parameterize and initialize the state transition matrices, which is quite mathematically involved. Most recent works have proposed simplified versions that use diagonal transition matrices [29; 26; 58] and pure RNN formulation (_i.e._, without reliance on ODE discretization) [30; 50; 9]. Several works have proposed hybrid architectures of SSMs and Transformers to incorporate their complementary strengths [75; 48; 17; 32; 24]. In addition to language modeling, SSMs have been applied to various domains, including time-series generation , audio generation , visual classification and generation [49; 40; 32; 65; 74; 71], and reinforcement learning [8; 47; 10; 52]. Our study introduces the first SSM with inductive biases for modeling inherently modular processes.

**Object-Centric Learning.** Object-centric learning seeks to discover modular structures and independent mechanisms  such as objects and their relations from multi-object images and videos with weak or no supervision [3; 22; 38; 23; 15; 14; 16; 7; 45; 35; 41; 37; 6; 44; 11; 1; 64; 68; 53; 34]. Recent works are predominantly based on the Slot Attention  model, which uses a GRU  and competitive attention mechanisms to iteratively refine slot representations [54; 57; 39; 13; 69; 55; 36; 70]. However, GRUs and RNNs in general are prone to vanishing gradient issues , and the training must be done in a sequential way. These weaknesses render them incapable of scaling up to long-range videos. Additionally, hardware parallelization for video object-centric learning has been explored in , however, it incurs quadratic cost unlike ours. Our SlotSSMs framework can be specialized to address object-centric learning tasks effectively. By integrating SSMs at its core, SlotSSMs benefit from parallelizable training and possess remarkable long-term memory capabilities. Moreover, as a versatile framework, SlotSSMs are well-suited to tackle other tasks such as long-range visual reasoning.

## 7 Experiments

We present an extensive evaluation of our models across a variety of tasks. Section 7.1 illustrates the need for modular latent states through a multi-object video prediction task. Section 7.2 demonstrates the advantages of SlotSSMs over Transformers and RNNs using a newly proposed long-context reasoning benchmark. Section 7.3 investigates the object-centric learning capabilities of OC-SlotSSMs. Finally, Section 7.4 showcases the 3D visual reasoning capabilities using the CATER benchmark .

### Multi-Object Video Prediction

We begin with a multi-object video modeling task to demonstrate the benefit of incorporating modularity into state space.

**Dataset and Task.** We utilize the bouncing balls video dataset introduced by , which consists of videos of white balls bouncing off each other in an empty window. Each ball has random initial positions, velocities, and masses, governing their interactions. The task is conditional video generation, specifically \(p(_{T+1:T+W}|_{1:T})\). This task is inherently modular as it requires models to remember each object's attributes and interaction rules.

**Experimental Setup.** We train models on 20-frame sequences using teacher-forcing and binary cross-entropy loss. At test time, given \(T=10\) context frames, the model autoregressively predicts \(W=20\) future frames using its own outputs. Performance is evaluated using Mean Squared Error (MSE) between predicted and ground-truth images.

**Models.** We employ the SlotSSM architecture described in Section 4.3 We use the same number of slots across layers and apply the Slot Encoder only at the first layer. We compare our model against several baselines: Single State SSM, which shares the same architecture but uses a monolithic state; Single State SSM (Split), which uses a Single State SSM with multi-slot encoder and decoder--slots are concatenated in SSM, then split into multiple slots for the decoder; RIM, a slot-based RNN model with separate RNN weights per slot that introduces sparse slot updates and interactions based on input attention values; Transformer, a vanilla Transformer model with a single input embedding per time step; and SlotTransformer, a Transformer model with multiple input slots at each time step.

All models share the same encoder and decoder architectures. The encoder is a Transformer described in Section 4.1, using a single CLS token for single-state models. The decoder consists of three Transformer layers with self-attention for image patches and cross-attention to query the slots. We use six slots for all slot-based models. For RIM, we set \(k=4\) for top-\(k\) active modules as in the original paper. We carefully match hyperparameters across baselines to ensure comparable model sizes, except for RIM, which inherently requires a larger model due to separate RNN weights per slot. Additional implementation details are in Appendix C.

**Results.** Figure 4 compares model performances, showing that SlotSSM outperforms all baselines, including a slight improvement over SlotTransformer. The significant gap between SlotSSM and Single State SSM underscores the importance of modular slot states for effective multi-object dynamics learning, as also evidenced by the comparison between Transformer and SlotTransformer. SlotSSM also significantly outperforms Single State SSM (Split), which uses the same modular encoder and decoder, highlighting that modularity in temporal modeling--the core contribution of SlotSSM--is critical for improved performance. While the RIM model performs better than other single-state baselines, it still lags behind SlotSSM and SlotTransformer.

Figure 4: **Multi-Object Video Prediction Task. _Left_: Generated video frames at every second step, showing 10 of the 20 total frames generated. Green color indicates ground-truth and red color indicates predictions. _Right_: MSE over a 20-frame autoregressive rollout, given 10 context frames. SlotSSM demonstrates its efficiency in modeling multi-object dynamics.**

### Long-Context Reasoning

We now evaluate the long-context reasoning capabilities of SlotSSM. To enable a rigorous assessment in a multi-object setting, we propose the novel Blinking Color Balls Benchmark.

**Blinking Color Balls Benchmark.** This benchmark has two variants--Earliest Color and Most Frequent Color--each consisting of image sequences with context images \(_{1:T-1}\) and a target image \(_{T}\). In each context image, one ball is randomly selected and assigned a non-white color from five options while others remain white. The coloring of balls in the target image \(_{T}\) depends on the variant: in **Earliest Color**, each ball's color is its earliest assigned non-white color in the context (remaining white if none); in **Most Frequent Color**, each ball's color is the non-white color assigned most frequently during the context (ties broken by earliest assignment; remaining white if none).

To create a long-range reasoning task, we further patchify each context image into non-overlapping \(P P\) patches and flatten them into a sequence of length \(P^{2}\), as shown in Figure 5(a). With context length \(T-1\), the total input sequence length is \(L=(T-1) P^{2}\). Models must identify and track objects from partial patch views while remembering and counting color assignments, making the task highly challenging. The final task is to predict the target image given this long sequential input.

**Experimental Setup.** We evaluate models on Earliest Color with \(T=6\) and Most Frequent Color with \(T\{6,11\}\), using patch sizes \(P\{4,8,16\}\). This yields input sequence lengths \(L\{80,160,320,640,1280,2560\}\). The Most Frequent Color with \(T=11\) requires stronger memorization and reasoning capabilities due to longer context and more color assignments,.

**Models.** We employ the same encoder, decoder, and the SlotSSM architectures as in Section 7.1. For slot encoding, each image patch is treated as an image and processed by the slot encoder. The slots from the last time step are provided to the decoder to predict the full target image. We compare our SlotSSM against several baselines: Single State SSM, SlotTransformer, and RIM. Additionally, we introduce a novel slot-based design called SlotRNNs, which shares RNN weights across slots and uses self-attention layers between time steps as the slot mixer. SlotRNNs can be viewed as a special case of RIMs with shared weights and dense state updates. Empirically, SlotRNNs exhibit more stable training and improved performance compared to RIMs. For fair comparison, all slot-based models use six slots, and we carefully match model sizes as in Section 7.1.

**Results.** Figure 6 shows that SlotSSM outperforms Single State SSM, SlotRNN, and RIM across all sequence lengths. For shorter sequences (80 and 160), Single State SSM and SlotRNN have relatively low error rates but degrade significantly beyond 320 frames. Surprisingly, RIM fails to generalize at any sequence length, likely due to optimization issues from separate weights per slot; our SlotRNN addresses this by sharing weights across slots while maintaining modularity. SlotTransformer performs competitively up to 640 frames. However, SlotSSM demonstrates superior long-range reasoning, especially at 1280 and 2560 frames, where other models cannot run due to memory or optimization constraints. Figure 5(b) highlights SlotSSM's computational efficiency. SlotTransformer's inference latency increases rapidly with sequence length due to quadratic complexity, SlotSSM maintains stable and efficient inference across all lengths. Due to SlotTransformer's high memory usage, we used a batch size of 6 for latency evaluation. Qualitative comparisons in Appendix B.3 provide further insights into the models' strengths and weaknesses.

Figure 5: Long-Context Construction and Model Efficiency in the Blinking Color Balls Benchmark. _Left_: We construct long-sequence inputs by patchifying the context images. _Right_: Comparison of model inference latency with batch size 6. SlotSSM demonstrates computational efficiency for long-sequence processing tasks.

### Unsupervised Object-Centric Learning

In this section, we evaluate the performance of the Object-Centric SlotSSMs (OC-SlotSSM) variant in unsupervised object-centric representation learning.

**Datasets.** We evaluate OC-SlotSSM on the MOVi-A and MOVi-B subsets of the MOVi video dataset , which contain videos of up to 10 objects moving in a 3D environment. MOVi-B adds complexity over MOVi-A by including a wider variety of object types and multi-colored backgrounds.

**Tasks.** Following prior object-centric learning works [46; 36], we evaluate models on two downstream tasks: unsupervised object segmentation and attribute prediction. For segmentation, we report FG-ARI and mIoU metrics. For attribute prediction, we measure the quality of representations by inferring object properties: we report prediction accuracy for discrete attributes (e.g., object shape) and \(R^{2}\) for continuous attributes (e.g., object position).

**Models.** We compare OC-SlotSSM to SAVi , an RNN-based object-centric learning approach. Both models use a CNN encoder to extract image features as input tokens \(t 0^{m}m=1^{M}\), which are processed by their respective attention mechanisms--inverted attention in OC-SlotSSM and slot attention in SAVi--to produce slots. These slots are then used to reconstruct the image and generate per-object segmentation masks via a spatial broadcast decoder, with reconstruction as the training objective. For unsupervised object segmentation, we directly use the object masks obtained during training. For attribute prediction, we match slots to object IDs using Hungarian matching based on segmentation masks, then use linear heads and 2-layer MLPs to predict discrete and continuous attributes, respectively, keeping the slots frozen.

**Results.** Results in Figure 7 demonstrate that OC-SlotSSM consistently outperforms SAVi in unsupervised object segmentation on both MOVi-A and MOVi-B. The qualitative comparison (Figure 7, left) shows that OC-SlotSSM generates masks with tighter object boundaries and fewer object splitting, which also leads to improved attribute prediction accuracy (Figure 7, right). Furthermore, we empirically found that OC-SlotSSM exhibits superior stability during training compared to SAVi, which tends to collapse into a single slot representing the entire scene when trained long enough. This collapse is not reflected in the validation loss, so we apply early stopping based on manual inspection. In contrast, OC-SlotSSM does not suffer from this instability, demonstrating its robustness in learning object-centric representations.

Figure 6: Long-Context Reasoning in Blinking Balls Benchmark. SlotSSM maintains consistent performance across sequence lengths from 80 to 2560, whereas baseline models show degraded performance or fail to complete training due to high memory and computational requirements.

Figure 7: Object-Centric Learning Results. _Left_: Qualitative comparison of segmentation masks on MOVi-A. OC-SlotSSM demonstrate less object splitting and better boundary adherence. _Right_: Quantitative evaluation on unsupervised object segmentation and attribute prediction. OC-SlotSSM outperforms SAVi on most metrics.

### 3D Visual Reasoning

Finally, we explore the application of SlotSSM and OC-SlotSSM to 3D visual reasoning tasks using the CATER benchmark .

**CATER Benchmark.** CATER consists of 300-frame video episodes of objects moving in a 3D environment. The movement can lead to partial occlusions and even complete coverage of smaller objects by larger ones. The primary task is snitch localization--predicting the golden snitch's location in the final frame. The snitch is always present but may be occluded. Models must reason about its location based on the last visible position and other objects' movements. Success in this task demonstrates models' capacity for complex visual reasoning in dynamic 3D environments.

**Experimental Setup.** We consider two experiment settings: direct training and pre-training \(+\) fine-tuning. In direct training, models are trained end-to-end on the snitch localization task. In pre-training \(+\) fine-tuning, models are first pre-trained on video inputs using a reconstruction objective, then fine-tuned on the task-specific signal. During pre-training, we randomly sample 32 frames from the 300-frame videos. For direct training and fine-tuning, we split the sequence into 50 non-overlapping segments of 6 frames each, randomly selecting one frame from each to create a 50-frame sequence spanning the entire video. At test time, we evenly sample 50 frames by skipping every 6 frames. The snitch's final location is quantized into a 6x6 grid, framing the problem as a classification task.

**Models.** We evaluate the performance of SlotSSM, OC-SlotSSM, Single State SSM, and SlotTransformer. We exclude RNN-based baselines, as our preliminary experiments reveal that they are unstable when handling long video inputs and prone to collapse to a constant output. For the visual pre-training setting, we employ a spatial broadcast decoder to reconstruct the input images. During downstream training/fine-tuning, we feed the slots from the final step to a transformer predictor with single CLS token, followed by a linear layer on the output CLS token to predict the snitch's position.

**Results.** Table 1 presents the Top-1 and Top-5 accuracy on the CATER Snitch Localization task. Consistent with our previous findings, SlotSSM outperforms Single State SSM, highlighting the importance of modular latent structures. Comparing SlotSSM with SlotTransformer, we see notable differences between direct training and pre-training settings: in direct training, SlotTransformer surpasses SlotSSM, possibly due to optimization advantages from direct access to all previous states; however, SlotSSM benefits more from pre-training, likely due to the explicit memory capacity of SSM states, consequently, pre-trained SlotSSMs outperforming their SlotTransformer counterparts.

Remarkably, OC-SlotSSM achieves the highest accuracy, outperforming all baselines by a large margin in both direct training and pre-training settings. This performance gain may be attributed to the explicit decomposition into object-centric representations, which facilitates reasoning about object properties, relationships, and interactions.

## 8 Conclusion

In this work, we presented SlotSSMs a novel approach to incorporating modular structure and inductive biases into State Space Models for improved sequence modeling. By maintaining a collection of independent slot vectors and performing state transitions independently per slot with sparse interactions via self-attention, SlotSSMs effectively captures the inherent modularity present in many real-world processes. The experimental results in object-centric video understanding and video prediction tasks demonstrate the substantial performance gains offered by SlotSSMs over existing sequence modeling methods.

    &  &  \\   & **Top-1 Acc (\%)** & **Top-5 Acc (\%)** & **Top-1 Acc (\%)** & **Top-5 Acc (\%)** \\  Single State SSM & 10.27 & 27.21 & 41.15 & 65.70 \\ SlotTransformer & 41.09 & 62.24 & 49.21 & 70.24 \\ SlotSSM & 25.64 & 45.03 & 54.73 & 74.42 \\ OC-SlotSSM & **61.58** & **84.00** & **69.27** & **90.48** \\   

Table 1: Performance on CATER Snitch Localization Task.