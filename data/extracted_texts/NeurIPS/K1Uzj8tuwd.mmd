# Learning Mask-aware CLIP Representations for Zero-Shot Segmentation

Siyu Jiao\({}^{1,2,3}\), Yunchao Wei\({}^{1,2,3}\), Yaowei Wang\({}^{3}\), Yao Zhao\({}^{1,2,3}\), Humphrey Shi \({}^{4,5}\)

\({}^{1}\) Institute of Information Science, Beijing Jiaotong University

\({}^{2}\) Peng Cheng Laboratory

\({}^{3}\) Beijing Key Laboratory of Advanced Information Science and Network

\({}^{4}\) Georgia Institute of Technology \({}^{5}\) Picsart AI Research

jiaosiyu99@bjtu.edu.cn

Work done during an internship at Picsart AI Research (PAIR).

###### Abstract

Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision. To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically, Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, _mask-aware loss_ and _self-distillation loss_ are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability. In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4% (+ 8.2%) on COCO, 81.8% (+ 3.2%) on Pascal-VOC, and 8.7% (+4.3%) on ADE20K in terms of mIoU for unseen classes. Code is available at github.com/jiaosiyu1999/MAFT.git.

## 1 Introduction

Semantic segmentation, one of the most widely researched topics in computer vision, has achieved remarkable success [3; 39; 15; 16] with the development of deep learning techniques . However, traditional segmentation models are only capable of segmenting a few predefined categories within a closed vocabulary [7; 2; 22; 21], which is much smaller than the number of categories used by humans to describe the real world. Therefore, zero-shot segmentation [31; 1; 10; 13] is introduced to segment objects using arbitrary categories described by texts.

Recently, large-scale visual-language pre-training models (CLIP  and ALIGN ) have shown impressive transferability in recognizing novel categories, leading to their increased adoptionfor tackling the challenging zero-shot segmentation task [6; 33; 20; 27]. A mainstream solution follows the "frozen CLIP" paradigm, which executes the zero-shot segmentation with two steps: 1) first employing a Proposal Generator to produce class-agnostic mask proposals and 2) then leveraging a frozen pre-trained CLIP to classify each mask proposal via similarity matching in the aligned image-text feature space. While acceptable results are obtained, we reveal that these approaches overlook a crucial issue, _i.e._ the frozen CLIP is insensitive to different mask proposals and tends to produce similar predictions for various proposals of the same image.

To better illustrate the above-mentioned issue, we show several examples in Fig. 1. We use MaskFormer  to generate a series of mask proposals and select three typical ones. When using frozen CLIP for classification, we observe that it correctly classifies the high-quality _swan_ proposal \(p_{1}\). However, for the other two proposals \(p_{2}\) and \(p_{3}\), which respectively contain only shape information of _swan_ and both regions of _swan_ and _river_, the frozen CLIP produces similar predictions compared to \(p_{1}\). This is reasonable since CLIP is trained by image-text pairs, making it insensitive to pixel-level information (_e.g._ background noise), and resulting in numerous false positives. Based on the above observations, we consider that an expected CLIP for zero-shot segmentation task should **1) be sensitive to different mask proposals, 2) not compromise its original transferability on novel classes.**

To this end, we introduce a Mask-aware CLIP Fine-tuning method (dubbed MAFT). To make CLIP sensitive to different mask proposals, we devise an Image-Proposals CLIP Encoder (IP-CLIP Encoder), which utilizes mask proposals to perform masked Multihead Attention [5; 4]. This design enables the model to handle arbitrary numbers of images and proposals simultaneously. The _mask-aware loss_ is proposed to minimise the distance between the IoU score of mask proposals and the classification score of IP-CLIP Encoder, prompting IP-CLIP Encoder to differentiate various proposals. Besides, to preserve CLIP's zero-shot transferability, we utilize a frozen CLIP as a teacher network to facilitate fine-tuning. This is achieved by aligning the outputs of the frozen CLIP and IP-CLIP Encoder through _self-distillation loss_. By performing MAFT, several advantages are provided: 1) Fine-tuning is efficient since only a few mask proposals need to be classified. 2) Compared to pixel-level fine-tuning, mask-aware fine-tuning hardly alters the structure of CLIP itself, preserving its maximum transferability. 3) Mask-aware fine-tuning of CLIP is released from the segmentation module, making it plug-and-play and applicable to any "frozen CLIP" approaches. As shown in Fig. 1, the mask-aware CLIP can well distinguish different proposals and provide proper classification scores for both seen (_river_) and unseen (_swan_) classes.

We evaluate our MAFT on three commonly used zero-shot segmentation benchmarks: COCO-Stuff , Pascal-VOC , and ADE20K . Extensive experiments show that MAFT works well with various zero-shot segmentation methods. In particular, by plugging MAFT, the state-of-the-art approach FreeSeg  achieves superior performance on COCO-Stuff (42.2% \(\) 50.4%), Pascal-VOC (78.6% \(\) 81.8%) and ADE20K (4.4% \(\) 8.7%) in terms of mIoU of unseen classes. Furthermore, we conduct experiments in a _open-vocabulary_ setting, where MAFT enhances the performance of A-847 , A-150 , PC-459 , PC-59  and PAS-20  datasets by +3.0%, +11.2%, +6.4%, +19.1% and +4.4%, respectively. Notably, our approach outperforms the freezing CLIP counterpart and establishes new state-of-the-art results on all datasets.

Figure 1: Comparison between the frozen CLIP and our mask-aware CLIP for proposal classification. Regions of proposals are highlighted with green. The frozen CLIP classifies \(p_{1}\), \(p_{2}\), and \(p_{3}\) as _swan_ class and produces similar predictions, although these proposals contain different regions of the image. After the MAFT, the mask-aware CLIP can produce proper scores for different proposals.

Related Work

**Zero-Shot Segmentation** is established to break the restriction of categories and perform segmentation on unseen classes. Earlier works SPNet  learn a joint pixel and vocabulary concept embedding space, ZS5  utilizes a generative model to generate pixel-level features based on word embeddings of unseen classes, CaGNet  incorporates context information for better feature generation. Recent approaches take the advent of large-scale visual-language models (_e.g._ CLIP  and ALIGN ) to leverage rich alignment features from image-text pairs.  uses CLIP to generate pseudo-labels for single-image segmentation. STRICT  obtains pixel-level pseudo-labels from CLIP for unlabeled pixels and proposes a self-training strategy to capture latent information on unseen classes. LSeg  trains a CNN model to compute per-pixel image embeddings and use CLIP text embeddings as a classifier.  employs contrastive supervision to learn segmentation masks from text.

Concurrently, recent works [6; 33; 27; 20; 9] follow the "frozen CLIP" paradigm for zero-shot segmentation, they first generate a series of mask proposals and then utilize CLIP  or ALIGN  to classify them. ZSSeg and OVSeg [33; 20] train CLIP adapters to boost performance. FreeSeg simultaneously uses semantic, instance, and panoptic labels and performs fusion training. OpenSeg takes extra images with image-level supervision (_e.g._ captions) to scale up training data.

**Pre-trained model fine-tuning** is widely used for transferring pre-trained knowledge to downstream tasks, _e.g._ segmentation. However, this strategy may not work well for data-limited tasks like few-shot learning and zero-shot learning due to the daunting _overfitting_ problem. To address this problem and transfer pre-trained knowledge to data-limited tasks, [43; 42; 12; 33; 20; 27] propose to learn text prompts or image prompts by using (a few) annotated images from target dataset. SVF  fine-tunes only a few parameters in the pre-trained image encoder to adapt pre-trained knowledge to few-shot segmentation. [38; 37] use contrastive learning to avoid catastrophic forgetting. Alternatively, many outstanding approaches in data-limited tasks [23; 35; 36; 6; 33] choose to freeze the parameters of pre-trained models to maintain the transferability.

Specific to the task of zero-shot/ open-vocabulary segmentation, mainstream approaches use frozen CLIP to avoid overfitting. Recently, MaskCLIP  conducts adequate experiments to fine-tune CLIP for open-vocabulary segmentation but has failed. While this attempt is meaningful and appreciated, it is believed that the failure is due to the large domain gap between pixel-level and image-level tasks. This motivates us further research fine-tuning CLIP to be mask-aware (region-level task).

## 3 Preliminary

**Problem Setting.** Zero-shot segmentation aims at training a segmentation model capable of segmenting novel objects using text descriptions. Given two category sets \(C_{seen}\) and \(C_{unseen}\) respectively, where \(C_{seen}\) and \(C_{unseen}\) are disjoint in terms of object categories (\(C_{seen} C_{unseen}=\)). The model is trained on \(C_{seen}\) and directly tested on both \(C_{seen}\) and \(C_{unseen}\). Typically, \(C_{seen}\) and \(C_{unseen}\) are described with semantic words (_e.g._ sheep, grass).

**Revisiting the "frozen CLIP" paradigm.** The "frozen CLIP" approaches [6; 33; 27; 20] execute zero-shot segmentation in two steps: mask proposals generation and mask proposals classification. In the first step, these approaches train a Proposal Generator to generate \(N\) class-agnostic mask proposals (denoting as \(M\), \(M^{N H W}\)) and their corresponding classification scores (denoting as \(A^{p}\), \(A^{p}^{N|C_{seen}|}\)). MaskFormer  and Mask2Former  are generally used as the Proposal Generator since the Hungarian matching  in the training process makes the mask proposals strongly generalizable. In the second step, \(N\) suitable sub-images (\(I_{sub}\)) are obtained by _merging_\(N\) mask proposals and the input image. \(I_{sub}\) is then fed into the CLIP Image Encoder to obtain the image embedding (\(E^{I}\)). Meanwhile, text embedding (\(E^{T}\)) is generated by a CLIP Text Encoder. The classification score (\(A^{e},A^{c}^{N C}\)) predicted by CLIP is calculated as:

\[A^{c}_{i}=(s_{c}(E^{T}_{i},E^{I}))}{ _{i=0}^{C}(s_{c}(E^{T}_{i},E^{I}))}),i=[1,2,...C]\] (1)

where \(\) is the temperature hyper-parameter. \(s_{c}(E^{T}_{i},E^{I})=_{i}.E^{I}}{|E^{I}_{i}||E^{I}|}\) represents the cosine similarity between \(E^{T}_{i}\) and \(E^{I}\). \(C\) is the number of classes, with \(C=|C_{seen}|\) during training and \(C=|C_{seen} C_{unseen}|\) during inference. Noting that CLIP is frozen when training to avoid overfitting.

To further enhance the reliability of \(A^{c}\), the classification score of the Proposal Generator (\(A^{p}\)) is ensembled with \(A^{c}\) since \(A^{p}\) is more reliable on seen classes. This _ensemble_ operation is wildly used in "frozen CLIP" approaches. The pipeline of "frozen CLIP", as well as the _merge_ and _ensemble_ operations, are described in detail in the Appendix.

Although "frozen CLIP" approaches have achieved promising results, it is clear that directly adopting an image-level pre-trained CLIP for proposal classification can be suboptimal. A frozen CLIP usually produces numerous false positives, and the _merge_ operation may destroy the context information of an input image. In view of this, we rethink the paradigm of the frozen CLIP and explore a new solution for proposal classification.

## 4 Methodology

We introduce Mask-Aware Fine-tuning (MAFT), a method for learning mask-aware CLIP representations. Within MAFT, we first propose the Image-Proposal CLIP Encoder (IP-CLIP Encoder) to handle images with any number of mask proposals simultaneously (Sec. 4.1). Then, _mask-aware loss_ and _self-distillation loss_ are introduced to fine-tune the IP-CLIP Encoder and make it distinguishable for different mask proposals while maintaining transferability (Sec. 4.2). The complete diagram of the MAFT is shown in Fig. 2, we use the ViT-B/16 CLIP model for illustration.

### Image-Proposal CLIP Encoder (IP-CLIP Encoder)

IP-CLIP Encoder aims to process arbitrary numbers of images and mask proposals simultaneously. We draw inspiration from MaskFormer [4; 5], which uses attention-masks in Multihead Attention and provides the flexibility for accepting any number of queries and features of different masked regions. Accordingly, we apply mask proposals as attention-masks in Multihead Attention and designate independent classification queries for each mask proposal.

In the IP-CLIP Encoder shown in Fig. 2, we denote the features propagate between Transformer layers as \(F^{i}\), where \(i=[1,2...12]\). We can express \(F^{i}\) as \(F^{i}=[F^{i}_{cls};\ F^{i}_{feat}],^{(1+hw) d}\), here \(1\) represents a class-embedding vector (\(F^{i}_{cls}\)), \(hw\) represents the number of the flattened image features (\(F^{i}_{feat}\)). To obtain the classifications of all mask proposals simultaneously, we repeat \(F^{i}_{cls}\) at layer \(L\)\(N\) times, where \(N\) is the number of mask proposals, denoting the repeated class-embedding vectors as \(F^{i*}_{cls}\). We can express the modified features (\(F^{i*}\)) as \(F^{i*}=[F^{i*}_{cls};\ F^{i}_{feat}],^{(N+hw) d}\).

Figure 2: Overview of the Mask-Aware Fine-tuning (MAFT). In IP-CLIP Encoder, we modify the CLIP Image Encoder, and apply the mask proposals as attention bias in Multihead Attention from the \(L^{th}\) layer. The final projection unit is an MLP module used for reshaping the channels of \(F_{cls}\). _w.o._\(M\) denotes IP-CLIP Encoder processes image without utilizing mask proposals (\(M\)). _Mask-aware_ Loss is designed to train CLIP to be mask-aware, while _Self-distillation_ Loss is designed to maintain the transferability. Only the IP-CLIP Encoder is trained (orange part), the Proposal Generator and the CLIP Text Encoder are frozen (blue part).

**Propagation of \(F^{i}\), where \(i=[1,2,...L]\).** We consider that CLIP's classification significantly relies on context information. In the first \(L\) Transformer layers, the propagation of \(F^{i}\) is the same as in standard CLIP. Specifically, \(F^{i}_{cls}\) utilizes cross-attention with all pixels within \(F^{i}_{feat}\), effectively retaining the context information.

In the subsequent \(12-L\) Transformer layers, the propagation of \(F^{i*}\) can be partitioned into two parts: the propagation of \(F^{i*}_{cls}\) and the propagation of \(F^{i}_{feat}\).

**Propagation of \(F^{i*}_{cls}\).** We use \(F^{i*}_{cls}\)[n] and \(M\)[n] to represent the position \(n\) in \(F^{i*}_{cls}\) and \(M\), where \(n=[1,2...N]\). It is expected \(F^{i*}_{cls}\)[n] computes Multihead Attention for the positions where \(M\)[\(n\)]\(=1\) and itself. To achieve this, we construct an attention bias \(B^{N(N+hw)}\) as follows:

\[B_{(i,j)}=\{0,\ _{(i,j)}=1\\ -,\ _{(i,j)}=0,\ \ =[(N,N);\ (M)].\] (2)

here \((N,N)\) denotes \(N^{th}\) order identity matrix, \(()\) denotes the _flatten_ operation. \(\) is an intermediate variable for better representation. Therefore, a masked Multihead Attention is used for propagating \(F^{i*}_{cls}\) :

\[F^{(i+1)*}_{cls}=((F^{i*}_{cls}) (F^{i*})^{T}}{}+B)(F^{i*})\] (3)

where \(()\), \(()\), and \(()\) denote linear projections, \(d\) is the hidden dimension of \(F^{i*}\). Notably, We omit the MLP Layer and Layer Normalizations in Transformer layers to simplify the representation in Eq. 3 and Eq. 4.

**Propagation of \(F^{i}_{feat}\).** A standard Multihead Attention is used for propagating \(F^{i}_{feat}\) :

\[F^{i+1}_{feat}=((F^{i}_{feat}) (F^{i}_{feat})^{T}}{})(F^{i}_{feat})\] (4)

Therefore, for any given mask proposal \(M\)[n], the corresponding class-embedding \(F^{i*}_{cls}\)[\(n\)] only performs Multihead Attention with \(F^{i}_{feat}\) where \(M\)[\(n\)]\(=1\) and \(F^{i*}_{cls}\)[\(n\)]. The propagation of \(F^{i}_{feat}\) remains undisturbed by attention-masks. Compared with the frozen CLIP, IP-CLIP Encoder leverages context information effectively and reduces computational costs.

### Objective

IP-CLIP Encoder with CLIP pre-trained parameters remains challenging in distinguishing different mask proposals, _e.g._, when the proposals contain more background regions than foreground objects, IP-CLIP may tend to classify them into the foreground categories. To overcome this limitation, we introduce _mask-aware loss_ and _self-distillation loss_ to fine-tune the IP-CLIP Encoder to be mask-aware without sacrificing transferability.

We conduct the _mask-aware_ loss function (\(_{ma}\)) on \(A^{c}\). The goal is to assign high scores to high-quality proposals and low scores to low-quality proposals in \(A^{c}\). Concretely, we use the Intersection over Union (IoU) score obtained from ground-truth and align it with the \(A^{c}\) to prompt CLIP to become mask-aware. Assuming there are \(k\) classes in ground-truth, we can generate \(k\) binary maps of ground-truth and calculate the IOU score (\(S_{IoU}\)) with \(N\) mask proposals. We identify a discrepancy between the maximum values of \(A^{c}\) and \(S_{IoU}\). The maximum value of \(A^{c}\) tends to approach 1, whereas the maximum value of \(S_{IoU}\) ranges from 0.75 to 0.99. This inconsistency can hinder the alignment between these two metrics. Therefore, we introduced a min-max normalization technique for \(S_{IoU}\) as follows:

\[S^{norm}_{IoU}=-min(S_{IoU})}{max(S_{IoU})-min(S_{IoU})},S_{IoU} ^{K N}\] (5)

Meanwhile, we select \(k\) pre-existing classes in \(A^{c}\) (\(A^{c}_{select}\), \(A^{c}_{select}^{K N}\)), and employ \(SmoothL1\) Loss to align it with \(S^{norm}_{IoU}\). Therefore, \(_{ma}\) can be formulated as follows:

\[_{ma}(A^{c}_{select},S^{norm}_{IoU})=(A^{c}_{select},S^{norm}_{IoU})\] (6)\[(x,y)=\{0.5(x-y)^{2},&\ |x-y|<1\\ |x-y|-0.5,&.\] (7)

In addition to \(_{ma}\), we also introduce a _self-distillation_ loss \(_{dis}\) to maintain CLIP's transferability and alleviate overfitting on \(C_{seen}\). Within \(_{dis}\), we use a frozen CLIP as the _teacher_ net, the IP-CLIP as the _student_ net for self-distillation. The predictions of the frozen CLIP and IP-CLIP are expected to be the same when no mask is included. Denoting the output of the frozen CLIP as \(A_{T}\), and the output of the fine-tuned IP-CLIP without masks as \(A_{S}\). We use \(SmoothL1\) Loss to minimize the difference as follows:

\[_{dis}(A_{S},A_{T})=(A_{S},A_{T})\] (8)

It is important to note that when processing an image through IP-CLIP without mask proposals, the resulting \(A_{S}\) is a matrix with dimensions \(^{C 1}\). Therefore, the final loss function can be formulated as: \(=_{ma}+_{dis}\), where we set the constant \(\) to 1 in our experiments. The mask-aware fine-tuning process is efficient as we only perform a few iterations (less than 1 epoch).

## 5 Experiments

### Setting

**Dataset.** We first follow [1; 11; 26; 6; 33] to conduct experiments on three popular zero-shot segmentation benchmarks, Pascal-VOC, COCO-Stuff and ADE20K, to evaluate our method. Then, we evaluate MAFT on the _open-vocabulary_ setting [20; 33], _i.e._, training on COCO-Stuff and testing on ADE20K (A-847, A-150), Pascal-Context (PC-459, PC-59), and Pascal-VOC (PAS-20). More details of the dataset settings are provided in the Appendix.

**Evaluation Metrics.** To quantitatively evaluate the performance, we follow standard practice [1; 31; 10; 25; 6; 33; 27], adopt mean Intersection over Union (mIoU) to respectively evaluate the performance for seen classes (IoU\({}^{*}\)) and unseen classes (IoU\({}^{*}\)). We also employ the harmonic mean IoU (hIoU) among the seen and unseen classes to measure comprehensive performance.

**Methods.** Three representative methods are used to verify the generality of MAFT. We unify the three methods into the same framework, with all methods using ResNet101 as the backbone of Proposal Generator and ViT-B/16 CLIP model for a fair comparison.

    &  &  &  \\  & **mIoU\({}^{*}\)** & **mIoU\({}^{*}\)** & **hIoU\({}^{*}\)** & **mIoU\({}^{*}\)** & **mIoU\({}^{*}\)** & **hIoU\({}^{*}\)** & **mIoU\({}^{*}\)** & **mIoU\({}^{*}\)** & **mIoU\({}^{*}\)** & **mIoU\({}^{*}\)** & **mIoU\({}^{*}\)** \\  SPNet & 34.6 & 26.9 & 30.3 & 77.8 & 25.8 & 38.8 & - & - & - \\ ZSS & 34.9 & 10.6 & 16.2 & 78.0 & 21.2 & 33.3 & - & - & - \\ CaGNet & 35.6 & 13.4 & 19.5 & 78.6 & 30.3 & 43.7 & - & - & - \\ STRICT & 35.3 & 30.3 & 32.6 & 82.7 & 35.6 & 73.3 & - & - & - \\ ZegFormer & 36.7 & 36.2 & 36.4 & 90.1 & 70.6 & 79.2 & 17.4 & 5.1 & 7.9 \\ ZegFormer +MAFT & 36.4 & \(-0.3\) & \(-0.1\) +9.3 & \(-1.7\) & 91.5 +1.4 & 80.7 +10.1 & 85.7 +0.5 & 16.6 +0.8 & 7.0 +1.9 & 9.8 +1.9 \\ ZSSeg & 40.4 & 36.5 & 38.3 & 86.6 & 59.7 & 69.4 & 18.0 & 4.5 & 7.2 \\ ZSSeg +MAFT & 40.6 +0.2 & 40.1 +5.6 & \(-0.3\) +2.0 & 88.4 +1.8 & 66.2 +6.5 & 75.7 +6.3 & 18.9 +0.9 & 6.7 +2.3 & 9.9 +2.7 \\ FreeSeg & 42.4 & 42.2 & 42.3 & 91.9 & 78.6 & 84.7 & 22.3 & 4.4 & 7.3 \\ FreeSeg +MAFT & 43.3 +0.9 & 50.4 +8.2 & 46.5 +4.2 & 91.4 \(-0.5\) & 81.8 +3.2 & 86.3 +1.6 & 21.4 +0.9 & 8.7 +4.3 & 12.4 +0.1 \\   

Table 1: Comparison with state-of-the-art methods in zero-shot segmentation. mIoU\({}^{*}\) and mIoU\({}^{*}\) denote the mIoU(%) of seen classes and unseen classes.

    &  &  &  \\  & **mIoU\({}^{*}\)** & **mIoU\({}^{*}\)** & **mIoU\({}^{*}\)** & **mIoU\({}^{*}\)** & **mIoU\({}^{*}\)** & **mIoU\({}^{*}\)** & **mIoU\({}^{*}\)** & **mIoU\({}^{*}\)** \\  ZegFormer & 18.5 & 23.0 & 20.5 & 81.4 & 76.8 & 79.0 & 5.1 & 2.6 & 3.5 \\ ZegFormer +MAFT & 35.1 +16.6 & 31.6 +7.8 & 33.3 +12.7 & 87.6 +6.2 & 79.9 +3.1 & 83.5 +4.5 & 15.8 +10.8 & 7.0 +4.4 & 9.8 +0.3 \\ ZSSeg & 20.6 & 27.4 & 23.6 & 82.0 & 71.2 & 76.2 & 5.9 & 2.8 & 3.9 \\ ZSSeg+MAFT & 36.1 +15.5 & 35.9 +3.3 & 36.0 +12.4 & 87.1 +5.1 & 76.1 +4.9 & 81.2 +5.0 & 17.2 +11.1 & 7.2 +1.4 & 10.2 +6.3 \\  FreeSeg & 22.3 & 29.3 & 25.3 & 87.4 & 74.7 & 80.5 & 6.5 & 2.8 & 3.9 \\ FreeSeg +MAFT & 40.1 +17.8 & 49.7 +39.4 & 44.4 +19.1 & 90.4 +1.30 & 84.7 +10.0 & 87.5 +7.0 & 21.3 +14.8 & 8.7 +5.9 & 12.2 +8.3 \\   

Table 2: Results on representative methods [6; 33; 27] with/without MAFT. Here we remove the _ensemble_ operation, and only maintain CLIP classifier results.

[MISSING_PAGE_FAIL:7]

progressively add each design. (Tab. 3(a)). FreeSeg uses frozen CLIP and yields inferior performance due to CLIP's mask-unaware property (\(1^{st}\) row). Then, IP-CLIP Encoder obtains rich context information and greatly reduces the computational costs, resulting in an improvement of 7.1% on seen classes and 6.9% on unseen classes. However, mask-aware is not accomplished at this point. Using only \(_{ma}\) for fine-tuning CLIP produces decent performance (the \(3^{rd}\) result). The introduction of \(_{dis}\) (the \(4^{th}\) result) maintains transferability while learning mask-aware representations, which further enhances the performance on unseen classes by 2.6%.

**Effect of different \(_{ma}\).**_Mask-aware_ Loss \(_{ma}\) is an essential component of MAFT. In Tab. 3(b), we investigate how different loss functions (\(L1\), \(L2\), \(SmoothL1\) and \(KL\) Loss) impact performance, here we remove \(_{dis}\) for analysis. Results show \(SmoothL1\) Loss boosts performance on \(C_{unseen}\) to 47.1% (+17.8%), \(KL\) Loss provides +12.5% improvement on \(C_{seen}\), but only +11.8% on \(C_{unseen}\), manifesting \(KL\) Loss compromises the model of transferability comparing with \(SmoothL1\) Loss.

**Training iterations.** Tab. 3(c) examines the impact of training iterations. Increasing the number of iterations leads to gradual improvement of IoU\({}^{s}\), but it also results in significant overfitting on unseen classes. Therefore, we choose to fine-tune 1k iterations to maximize the zero-shot ability.

**Frozen units in CLIP.** We also explore the impact of fine-tuning units within IP-CLIP Encoder. As illustrated in Fig. 2, IP-CLIP Encoder comprises convolution layers (dubbed as \(conv.\)), class embedding (\(cls.\)), Transformer layers, final projection (\(proj.\)) and positional embedding (\(pos.\), not shown in Fig. 2). We start with fine-tuning the entire IP-CLIP Encoder, and then freezing each unit sequentially, as specified in Tab. 3(d). We only freeze \(MLP\) in the Transformer layers (dubbed as \(mlp\)). Compared with fine-tuning the entire IP-CLIP Encoder, the performance of mIoU\({}^{u}\) is improved by 5.0% when freezing \(conv.\), \(cls.\), \(pos.\) and \(mlp\).

**Start mask attention layer.** Tab. 3(e) presents the results of the start mask attention layer (\(L\)). We observe a significant improvement in the performance of unseen classes by +3.4% when the value of \(L\) increases from 0 to 8. This could be attributed to the fact that starting masked Multihead Attention later enables \(F_{cls}^{i*}\) to gain more context information. However, the performance significantly drops when \(L=10\) (from 49.7% to 45.7%), which may be due to the loss of mask-aware property.

### Extending MAFT with SAM

We explore using the Segment Anything Model  (SAM) as the proposal generator. We evaluate the performance with SAM-H using an original CLIP (dubbed \(\)) or a mask-aware fine-tuned CLIP (dubbed \(+\)). In fact, SAM can be seamlessly integrated into our framework as the proposal generator. The results are shown in Tab. 5. Experiments are conducted under both _zero-shot_ setting and _open-vocabulary_ setting.

It can be observed that \(+\) obtains significant improvement over \(\) under both settings. Besides, \(+\) also surpasses \(+\) on all benchmarks. Particularly, in the zero-shot setting (Pascal-VOC), \(+\) outperforms \(+\) by 6.8% in

Table 4: **Ablations on COCO dataset. GFLOPs in (a) is used to measure the computation of CLIP Image Encoder. The best results are highlighted with red, and the default settings are highlighted with gray background.**terms of mIoU\({}^{u}\). This enhancement can be attributed to the stronger generalization capabilities of SAM for unseen classes.

### Extending MAFT with more Vision-Language Models

In order to demonstrate the efficacy and robustness of MAFT, we conduct experiments using stronger (CLIP-ViT-L) and ResNet-based (CLIP-Res50) Vision-Language Models. The open-vocabulary results are shown in Tab. 6, we also include the results of OVSeg with CLIP-ViT-L for comparison.

**CLIP-ViT-L.** According to Tab. 6, FreeSeg with a standard CLIP-ViT-L model (dubbed \(\)) still can not achieve satisfactory results. However, by integrating our MAFT (dubbed \(+\)), the segmentation results are remarkably enhanced, thus establishing new state-of-the-art benchmarks.

**CLIP-Res50.** Our MAFT can easily adapted into ResNet-based models. Specifically, we modified the \(\) unit within CLIP-R50 Image Encoder. The mask proposals are introduced as attention bias (\(B\)) in Multihead Attention, with \(F_{cls}\) being repeated N times. Notably in CLIP-R50, \(F_{cls}\) is obtained via \(\) performing on \(F_{feat}\). The results are presented in Tab. 6. The performance on all 5 datasets is improved by a large margin. \(+\) with CLIP-R50 achieves competitive results with some CLIP-ViT-B-based methods according to Tab. 3.

### Qualitative Study

**Visualizations of typical proposals.** Fig. 3 shows frozen CLIP and mask-aware CLIP classifications of typical proposals, including high-quality proposals of foreground (\(p_{1}\), \(p_{4}\)), high-quality proposals of background (\(p_{3}\), \(p_{6}\)), a proposal with background noise (\(p_{2}\)), and a proposal containing part of the foreground (\(p_{5}\)). The proposal regions are highlighted in green or yellow.

Several observations can be obtained: (1) The frozen CLIP provides good predictions for \(p_{1}\) and \(p_{4}\). (2) The frozen CLIP assigns \(p_{2}\) as \(cat\) and \(p_{5}\) as \(horse\), with scores even higher than \(p_{1}\), \(p_{4}\), indicating the frozen CLIP cannot distinguish proposals containing information on the same objects. (3) The frozen CLIP fails to give correct predictions for \(p_{3}\) and \(p_{6}\), which may be due to the lack of context information. (4) Our mask-aware CLIP gives good predictions for high-quality proposals (\(p_{1}\), \(p_{3}\), \(p_{4}\), \(p_{6}\)) and provides suitable predictions for \(p_{2}\) and \(p_{5}\).

**Qualitative analysis.** We show some visual examples in Fig. 4. Some segmentation results of FreeSeg contain background noise (_e.g._ the \(1^{st}\) & \(2^{nd}\) row, \(3^{rd}\) column) or contain only part of the objects (\(3^{rd}\) row, \(3^{rd}\) column). In ADE20K-847 dataset, too many classes may lead to the anticipated results (last row, \(3^{rd}\) column) with the frozen CLIP. Using a mask-aware CLIP to learn mask-aware representations can significantly improve these segmentation results, as evident from the last column.

More visual samples are shown in the Appendix.

    & **backbone** & **A-847** & **A-150** & **PC-459** & **PC-59** & **PAS-20** \\  OVSeg  & & 9.0 & 29.6 & 12.4 & 55.7 & 94.5 \\ FreeSeg  & ViT-L & 8.5 & 21.0 & 7.6 & 33.8 & 86.4 \\ FreeSeg + MAFT & & 12.1 \(\)3.6 & 32.0 \(\)11.0 & 15.7 \(\)8.1 & 58.5 \(\)24.7 & 92.1 \(\)5.7 \\ FreeSeg  & Res50 & 5.3 & 15.5 & 5.4 & 28.2 & 87.1 \\ FreeSeg + MAFT & & 8.4 \(\)3.1 & 27.0 \(\)11.5 & 9.9 \(\)4.5 & 50.8 \(\)22.6 & 89.0 \(\)1.9 \\   

Table 6: Comparison with more Vision-Language Models.

    &  &  \\  & **mIoU\({}^{u}\)** & **mIoU\({}^{u}\)** & **mIoU** & **mIoU\({}^{u}\)** & **mIoU\({}^{u}\)** & **mIoU** & **mIoU** \\  SAM & 85.1 & 86.7 & 85.9 & 85.5 & 43.1 & 43.3 & 43.2 & 42.1 \\ SAM + MAFT & 91.0 \(\)5.9 & 88.6 \(\)1.9 & 89.8 \(\)3.9 & 90.4 \(\)4.9 & 43.4 \(\)0.3 & 51.5 \(\)8.2 & 47.1 \(\)3.9 & 44.1 \(\)2.0 \\   

Table 5: Comparison with SAM. We use SAM-H as the proposal generator.

## 6 Conclusion

In this paper, we rethink the "frozen CLIP" paradigm in zero-shot segmentation and propose Mask-Aware Fine-Tune (MAFT) for fine-tuning CLIP. Firstly, IP-CLIP Encoder is proposed to handle images with any number of mask proposals. Then, \(_{ma}\) and \(_{dis}\) are designed for fine-tuning CLIP to be mask-aware without sacrificing its transferability. MAFT is plug-and-play and can be applied to any "frozen CLIP" approach. Extensive experiments well demonstrate the performance of various zero-shot segmentation methods is improved by plugging MAFT.

**Limitations.** Our MAFT introduces a CLIP fine-tuning framework to the research of zero-shot segmentation. However, the classification ability for novel classes is still limited by pre-trained vision-language models. How to further narrow this limitation is our future research focus.

**Acknowledgment.** This work was supported in part by the National Key R & D Program of China (No.2021ZD0112100).

Figure 4: Qualitative results. The models are trained with COCO-Stuff and directly tested on VOC2012, COCO, and ADE20K.

Figure 3: Visualizations of typical proposals & top 5 \(A^{c}\) by frozen CLIP and mask-aware CLIP.