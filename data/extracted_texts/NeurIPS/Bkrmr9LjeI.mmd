# Learning to Discover Skills through Guidance

Hyunseung Kim\({}^{*,1}\) Byungkun Lee\({}^{*,1}\) Hojoon Lee\({}^{1}\)

**Dongyoon Hwang**\({}^{1}\) Sejik Park\({}^{1}\) Kyushik Min\({}^{2}\) Jaegul Choo\({}^{1}\)

\({}^{1}\)Kim Jaechul Graduate School of AI, KAIST. \({}^{2}\)KAKAO Corp.

{mynsng, byungkun.lee, joonleesky,

godnpeter, sejik.park, jchoo}@kaist.ac.kr

queue.min@kaacorp.com

Equal contributions.

###### Abstract

In the field of unsupervised skill discovery (USD), a major challenge is limited exploration, primarily due to substantial penalties when skills deviate from their initial trajectories. To enhance exploration, recent methodologies employ auxiliary rewards to maximize the epistemic uncertainty or entropy of states. However, we have identified that the effectiveness of these rewards declines as the environmental complexity rises. Therefore, we present a novel USD algorithm, skill **discovery** with **guidance (DISCO-DANCE)**, which (1) selects the _guide skill_ that possesses the highest potential to reach unexplored states, (2) guides other skills to follow _guide skill_, then (3) the guided skills are dispersed to maximize their discriminability in unexplored states. Empirical evaluation demonstrates that DISCO-DANCE outperforms other USD baselines in challenging environments, including two navigation benchmarks and a continuous control benchmark. Qualitative visualizations and code of DISCO-DANCE are available at https://mynsng.github.io/discodance/.

## 1 Introduction

Deep Reinforcement Learning (DRL) has shown remarkable success in a wide range of complex tasks, from playing video games  to complex robotic manipulation . However, the majority of DRL models are designed to train from scratch for each different task, resulting in significant inefficiencies. Furthermore, reward functions adopted for training the agents are generally handcrafted, acting as an impediment that prevents DRL from scaling for real-world tasks. For these reasons, there has been an increasing interest in training task-agnostic policies without access to a pre-defined reward function. One approach that has been widely studied for achieving this is unsupervised skill discovery (USD), which is a training paradigm that aims to acquire diverse and discriminable behaviors, referred to as skills . These pre-trained skills can be utilized as useful primitives or directly employed to solve various downstream tasks.

Most of the previous studies in USD discover a set of diverse and discriminable skills by maximizing the self-supervised, intrinsic motivation as a form of reward . Commonly, mutual information (MI) between the skill's latent variables and the states reached by each skill is utilized as the self-supervised reward. However, it has been shown in recent research that solely maximizing the sum of MI rewards is insufficient in exploring the state space because, asymptotically, the agent receives larger rewards for visiting known states rather than for exploring novel states due to the high MI reward it receives at a fully discriminable state .

To ameliorate this issue, recent studies designed an auxiliary exploration reward that incentivizes the agent when it succeeds in visiting novel states . However, albeit provided these auxiliaryrewards, previous approaches may exhibit decreased effectiveness in challenging environments. Fig. 1 conceptually illustrates the ineffectiveness of previous methods. Suppose that the upper region of Fig. 1a is difficult to reach with MI rewards, resulting in obtaining skills that are stuck in the lower-left region. To make these skills explore the upper region, previous methods provide auxiliary exploration rewards using intrinsic motivation (e.g., disagreement, curiosity-based bonus). However, since they do not indicate exactly which _direction_ to explore, it becomes more inefficient in challenging environments. We detail the limitations of previous approaches in Section 2.2.

In response, we propose a new exploration objective that aims to provide _direct guidance_ to the unexplored states. To encourage skills to explore unvisited states, we first identify a guide skill \(z^{*}\) which possesses the highest potential for reaching unexplored states (Fig. 1d-1). Next, we select skills that are relatively unconverged, and incentivize them to follow the guide skill \(z^{*}\) in an effort to leap over the state regions with low MI rewards (Fig. 1d-2:4). Finally, they are dispersed to maximize their distinguishability (Fig. 1d-5), resulting in obtaining a set of skills with high state coverage. We call this algorithm as skill **discovery** with **guidance** (**DISCO-DANCE**) and is further presented in Section 3. DISCO-DANCE can be thought of as filling the pathway to the unexplored region with a positive dense reward.

Through empirical experiments, we demonstrate that DISCO-DANCE outperforms previous approaches in terms of state space coverage and downstream task performances in two navigation environments (2D mazes and Ant mazes), which have been commonly used to validate the performance of the USD agent [7; 19]. Furthermore, we also experiment in Deepmind Control Suite , and show that the learned set of skills from DISCO-DANCE provides better primitives for learning general behavior (e.g., run, jump, and flip) compared to previous baselines.

## 2 Preliminaries

In Section 2.1, we formalize USD and explain the inherent pessimism that arises in USD. Section 2.2 describes existing exploration objectives for USD and the pitfalls of these exploration objectives.

Figure 1: **Conceptual illustration of previous methods and DISCO-DANCE. Each skill is shown with a grey-colored trajectory. Blue skill \(z^{i}\) indicates an unconverged skill, and the reward landscape of \(z^{i}\) is represented in green. Here, (b,c) illustrates a reward landscape of previous methods, DISDAIN, APS, and SMM. (b) DISDAIN fails to reach the upper region due to the absence of a pathway to the unexplored states. (c) APS and SMM fail since they do not provide exact directions to the unexplored states. On the other hand, (d), DISCO-DANCE directly guides \(z^{i}\) towards selected guide skill \(z^{*}\) which has the highest potential to reach the unexplored states. A detailed explanation of the limitations of each baseline is described in Section 2.2.**

### Unsupervised Skill Discovery and Inherent Exploration Problem

Unsupervised Skill Discovery (USD) aims to learn _skills_ that can be further utilized as useful primitives or directly used to solve various downstream tasks. We cast the USD problem as discounted, finite horizon Markov decision process \(\) with states \(s\), action \(a\), transition dynamics \(p\), and discount factor \(\). Since USD trains the RL agents to learn diverse skills in an unsupervised manner, we assume that the reward given from the environment is fixed to \(0\). The skill is commonly formulated by introducing a skill's latent variable \(z\) to a policy \(\) resulting in a latent-conditioned policy \((a|s,z)\). Here, the skill's latent variable \(z\) can be represented as a one-hot vector (i.e., discrete skill) or a continuous vector (i.e., continuous skill). In order to discover a set of diverse and discriminable skills, a standard practice is to maximize the mutual information (MI) between state and skill's latent variable \(I(S;Z)\).

\[I(Z,S)=-H(Z|S)+H(Z)=_{z p(z),s(z)}[ p(z|s)- p(z)]\] (1)

Since directly computing the posterior \(p(z|s)\) is intractable, a learned parametric model \(q_{}(z|s)\), which we call _discriminator_, is introduced to derive lower-bound of the MI instead.

\[I(Z,S)  (Z,S)=_{z p(z),s(z)}[ q_{}( z|s)- p(z)]\] (2)

Then, the lower bound is maximized by optimizing the skill policy \((a|s,z)\) via any RL algorithm with reward \( q_{}(z|s)- p(z)\) (referred to as \(r_{}\)). Note that each skill-conditioned policy gets a different reward for visiting the same state (i.e., \(r_{}(z_{i},s) r_{}(z_{j},s)\)). It results in learning skills that visit different states, making them discriminable.

However, maximizing the MI objective is insufficient to fully explore the environment due to _inherent pessimism_ of its objective . When the discriminator \(q_{}(z|s)\) succeeds in distinguishing the skills, the agent receives larger rewards for visiting known states rather than for exploring novel states. This lowers the state coverage of a given environment, suggesting that there are limitations in the set of skills learned (i.e., achieving a set of skills that only reach a limited state space).

### Previous exploration bonus and Its limitations

In order to overcome the _inherent pessimistic exploration_ of USD, recent studies have attempted to provide auxiliary rewards. DISDAIN  trains an ensemble of \(N\) discriminators and rewards the agent for their disagreement, represented as \(H(_{i=1}^{N}q_{_{i}}(Z|s))-_{i=1}^{N}H(q_{ _{i}}(Z|s))\). Since states that have not been visited frequently will have high disagreement among discriminators, DISDAIN implicitly encourages the agent to move to novel states. However, since such exploration bonus is a _consumable_ resource that diminishes as training progresses, most skills will not benefit from this if other skills reach these new states first and exhaust the bonus reward.

We illustrate this problem in Fig. 1b. Suppose that all skills remain in the lower left states, which are easy to reach with MI rewards. Since the states in the lower left region are accumulated in the replay buffer, disagreement between discriminators remains low (e.g., low exploration reward in the lower left region). Therefore, there will be no exploration reward left in these states. This impedes \(z_{i}\) from escaping its current state to unexplored states, as shown in Fig. 1b-4.

On the other hand, SMM encourages the agent to visit states where it has not been before using a learned density model \(d_{}\). APS incentivizes the agent to maximize the marginal state entropy via maximizing the distance of the encoded states \(f_{}(s_{t})\) between its k nearest neighbor \(f_{}^{k}(s_{t})\).

\[r_{}^{} - d_{}(s)\] (3) \[r_{}^{} ||f_{}(s)-f_{}^{k}(s)||\]

These rewards push each skill out of its converged states (in Fig. 1c). However, they still do not provide a _specific direction_ on where to move in order to reach unexplored states. Therefore, in a difficult environment with a larger state space, it is known that these exploration rewards can operate inefficiently . In the next section, we introduce a new exploration objective for USD which addresses these limitations and outperforms prior methods on challenging environments.

## 3 Method

Unlike previous approaches, we design a new exploration objective where the _guide skill_\(z^{*}\) directly influences other skills to reach explored regions. DISCO-DANCE consists of two stages: (i) selectingguide skill \(z^{*}\) and the _apprentice skills_ which will be guided by \(z^{*}\), and (ii) providing guidance to apprentice skills via _guide reward_, which will be described in Section 3.1 and 3.2, respectively.

### Selecting Guide skill and Apprentice Skills

**Guide skill.** We recall that our main objective is to obtain a set of skills that provides high state space coverage. To make other skills reach the unexplored state, we define the _guide skill_\(z^{*}\) as the skill which is most _adjacent_ to the unexplored states. One naive approach in selecting the guide skill is to choose the skill whose terminal state is most distant from the other skills' terminal state (e.g., blue skill in Fig. 2b). However, such a selection process does not take into account whether the guide skill's terminal state is neighboring promising _unexplored states_. In order to approximate whether a skill's terminal state is near potentially unexplored states, we utilize a simple random walk process (Fig. 2c). To elaborate, given a set of \(P\) skills, (i) rollout skill trajectory (\(T\) timesteps) and perform \(R\) random walks from the terminal state of each skill, repeated \(M\) times (i.e., a total of \(P(T+R)M\) steps, collecting \(PRM\) random walk arrival states as in Fig. 2c). Then (ii) we pinpoint the state in the lowest density region among collected random walk arrival states and select the skill which that state originated from as the guide skill \(z^{*}\) (red skill in Fig. 2c). For (ii), one could use any algorithm to measure the density of the random walk state distribution. For our experiments, we utilize a simple k-nearest neighbor algorithm,

\[ z^{*}:=*{argmax}_{p\{1,,P\}} \;_{r\{1,,RM\}}\;_{s^{j}_{pr} N_{k}(s_{pr})}||s _{pr}-s^{j}_{pr}||_{2}\\ s_{pr}=rp\\ N_{k}()=.\] (4)

In practice, in an environment with a long horizon (i.e., high \(T\)), such as DMC, our random walk process may cause sample inefficiencies. Therefore, we also present an alternative, efficient random walk process, which is thoroughly detailed in Appendix B. Ablation experiment on two guide skill selection methods (Fig. 2a,b) is provided in Section 4.4.

**Apprentice skills.** We select _apprentice skills_ as the skills with low discriminability (i.e., skill \(z^{i}\) with low \(q_{}(z^{i}|s)\); which fails to converge) and move them towards the guide skill. If most of the skills are already well discriminable (i.e., high MI rewards), we simply add new skills and make them apprentice skills. This will leave converged skills intact and send new skills to unexplored states. Since the total number of skills is gradually increasing as the new skills are added, this would bring the side benefit of not relying on a pre-defined number of total skills as a hyperparameter.

In Appendix E, we empirically show that adding new skills during training is generally difficult to apply to previous algorithms because the new skills will face pessimistic exploration problems. The new skills simply converge by overlapping with existing skills (e.g., left lower room in Fig 3), which exacerbates the situation (i.e., reducing discriminator accuracy without increasing state coverage).

### Providing direct guidance via auxiliary reward

After selecting the guide skill \(z^{*}\) and apprentice skills, we now formalize the exploration objective, considering two different aspects: (i) the objective of the guidance, and (ii) the degree of the guidance. It is crucial to account for these desiderata since strong guidance will lead apprentice skills to simply imitate guide skill \(z^{*}\), whereas weak guidance will not be enough for skills to overcome pessimistic exploration problem.

In response, we propose an exploration objective that enables our agent to learn with guidance. We integrate these considerations into a single soft constraint as

Figure 2: **Guide skill selection. (a) A set of skill trajectories. (b) Naively selecting the skill whose terminal state is most distant from others. (c) Selecting the skill through the random walk process.

\[}E_{z^{i} p(z),s_{ }(z^{i})}r_{}+r_{}\] (5) \[r_{}= q_{}(z^{i}|s)- p(z^{i}),\] \[r_{}=-\,q_{}(z^{i}|s) (1-q_{}(z^{i}|s))D_{}(_{}( a|s,z^{i})||_{}(a|s,z^{*})).\]

As we describe in Section 3.1, we select skills with low discriminator accuracy (i.e., \((q_{}(z^{i}|s))\)) as apprentice skills. For (i), we minimize the KL-divergence between the apprentice skill and the guide skill policy (i.e., \(D_{}(_{}(a|s,z^{i})||_{}(a|s,z^{*}))\)). For (ii), the extent to which the apprentice skills are encouraged to follow \(z^{*}\), can be represented as the weight of the KL-divergence. We set the weight as \(1-q_{}(z^{i}|s)\) to make the skills with low discriminator accuracy be guided more.

Fig. 3 shows that the guidance strategy significantly enhances the learning of skills in both navigation and continuous control environments. In the navigation task (Fig. (a)a), the skill located in the upper left corner is selected as a guide through the random walk and utilized to navigate the apprentice skills, which were previously stuck in the lower left corner, into unexplored states. Similarly, in a continuous control environment (Fig. (b)b), the skill that has been learned to _run_ is selected as the guide, leading the apprentice skills that were barely moving. This guidance allows the apprentice skills to quickly learn to _run_. This salient observation suggests that the concept of guidance can be utilized, even for non-navigation tasks. In Section 4.4, we show that all of these components of \(r_{}\) are necessary through additional experiments. We provide pseudocode for DISCO-DANCE in Algorithm 1.

```
1Initialize skills \(z^{1},...,z^{n}\), RL policy \(_{}(a|s,z)\), skill discriminator \(q_{}(z|s)\),
2Initialize guide skill \(z^{*}=\) None, \(r_{}=0\)
3Hyperparameters guide coef \(\), apprentice cutoff \(\)
4for\(k=1,2,...\)do
5 Sample batch \((s_{t},a_{t},s_{t+1},z^{i})\) from replay buffer \(D\)
6ifmost skills are discriminable enoughthen
7 guide skill \(z^{*}\)find_guide_skill\((_{},z^{1},...,z^{n})\)
8\(r_{}= q_{}(z^{i}|s_{t+1})- p(z^{i})\)
9if\(z^{*}\) is not Nonethen
10\(r_{}=-q_{}(z^{i}|s)<(1-q_{ }(z^{i}|s))\;D_{}(_{}(a_{t}|s_{t},z^{i})||_{}( a_{t}|s_{t},z^{*}))\)
11\(r=r_{}+ r_{}\)
12 Update \(_{}\) to maximize sum of \(r\)
13 Update \(q_{}\) to maximize \( q_{}(z|s_{t})\) ```

**Algorithm 1**Skill Discovery through Guidance

## 4 Experiments

### Experimental Setup

**Evaluation.** We evaluate DISCO-DANCE across three distinct types of environments commonly used in USD: 2D Maze, Ant Maze, and Deepmind Control Suite (DMC). We utilize state space

Figure 3: **Overall procedure of DISCO-DANCE in (a) navigation and (b) continuous control tasks.**

coverage and downstream task performance as our primary evaluation metrics. To measure _state coverage_, we discretize the x and y axes of the environment into 10 intervals (i.e., total 10\(\)10 buckets) and count the number of buckets reached by learned skills (Fig. 4.a,b). For _downstream task performance_, we finetune the agent pretrained with baselines and DISCO-DANCE (Fig. 4. (b,c)). Further details regarding the environments and evaluation metrics can be found in Appendix D.

For Ant mazes, we provide the (x,y) coordinates as the input for the discriminator for all algorithms following previous work , since the main objective of navigation environments is to learn a set of skills that are capable of successfully navigating throughout the given environment. For DMC, we utilize all observable dimensions (e.g., joint angles, velocities) to the input of the discriminator to learn more general behaviors (e.g., running, jumping, and flipping) that can be used as useful primitives for unknown continuous control tasks.

**Baselines.** We compare DISCO-DANCE with various skill-based algorithms, focusing on addressing the pessimistic exploration of USD agents. We compare DISCO-DANCE with DIAYN , which trains an agent using only \(r_{}\) without any exploration rewards. We also evaluate the performance of SMM , APS , and DISDAIN , which incorporate auxiliary exploration rewards. We note that all baselines and DISCO-DANCE utilize discrete skills, except APS, which utilizes continuous skills. For downstream tasks, we also compare USD baselines with SCRATCH, which represents _learning from scratch_ (i.e., no pretraining). Additional details are available in Appendix D.

In addition, we include UPSIDE  as a baseline. UPSIDE learns a tree-structured policy composed of multiple skill nodes. Although UPSIDE does not utilize auxiliary reward, DISCO-DANCE and UPSIDE both aim to enhance exploration by leveraging previously discovered skills. However, UPSIDE's _sequential execution_ of skills from ancestors to children nodes in a top-down manner leads to significant inefficiency during finetuning, leading to lower performance on downstream tasks. Furthermore, UPSIDE selects the skill with _highest discriminator accuracy_ (i.e., corresponding to Fig. 2b) for expanding tree policies, resulting in reduced state coverage at the pretraining stage. A detailed comparison between DISCO-DANCE and UPSIDE can be found in Appendix F.

### Navigation Environments

#### 4.2.1 2D Mazes

First, we evaluate DISCO-DANCE in 2D mazes (Fig. 4a), which has been commonly used for testing the exploration ability of skill learning agents . The layout becomes more challenging, from an empty maze to a bottleneck maze. The agent determines where to move given the current x and y coordinates (2-dimensional state and action space).

    &  &  \\   & Empty & Y & W & S & Bottleneck & Ant Empty-mazes & Ant U-mazes & Ant II-mazes \\  DIAYN & **100.00\(\)0.00** & 69.80 \(\)5.39 & 71.50 \(\)5.10 & 52.80\(\)5.13 & 52.40\(\)3.77 & 72.70\(\)10.95 & 46.80\(\)8.41 & 22.50\(\)3.34 \\ SMM & **100.00\(\)0.00** & 89.20 \(\)5.80 & 73.40 \(\)6.15 & 55.00\(\)5.94 & 54.80\(\)5.11 & **99.00\(\)0.84** & 58.90\(\)8.23 & 25.40\(\)4.97 \\ APS & 97.90\(\)3.75 & 90.00 \(\)5.51 & 81.50 \(\)11.31 & 80.20\(\)5.85 & 61.90\(\)10.55 & 83.10\(\)24.80 & 59.60\(\)3.55 & 26.37\(\)3.68 \\ DISDAIN & **100.00\(\)0.00** & 87.20 \(\)6.81 & 85.00 \(\)8.69 & 61.30\(\)7.04 & 61.70\(\)4.85 & 70.10\(\)4.97 & 45.80\(\)4.98 & 22.90\(\)2.88 \\ UPSIDE & 99.00\(\)9.25 & 88.20 \(\)19.39 & 90.20 \(\)6.15 & 69.90\(\)7.63 & 78.90\(\)13.69 & 74.50\(\)6.22 & 63.60\(\)11.58 & 29.90\(\)4.22 \\ DISCO-DANCE & **100.00\(\)0.00** & **99.10\(\)1.46** & **98.30\(\)26.25** & **88.50\(\)6.48** & **86.30\(\)19.701** & **98.90\(\)21.85** & **68.30\(\)4.47** & **39.00\(\)3.88** \\   

Table 1: **State space coverages of DISCO-DANCE and baselines on two navigation benchmarks.** The results are averaged over 10 random seeds accompanied by a standard deviation. Scores in bold indicate the best-performing model and underlined scores indicate the second-best.

Figure 4: **Three environments to evaluate skill discovery algorithms. (a) Continuous 2D mazes with various layouts, (b) high dimensional ant navigation tasks, and (c) continuous control environments with diverse downstream tasks.**

Table 1(a) summarizes the performance of skill learning algorithms on 2D mages. Our empirical analysis indicates that baseline methods with auxiliary reward (i.e., SMM, APS, and DISDAIN) exhibit superior performance compared to DIAYN. However, we find out that previous studies provide less benefit on performance as the layout becomes more complex, as we mentioned in section 2.2. In contrast, the performance of DISCO-DANCE remains relatively stable regardless of layout complexity. Additionally, DISCO-DANCE outperforms UPSIDE in all 2D mages, which we attribute to the inefficiency of adding new skills to leaf nodes with high discriminator accuracy (Fig. 1(b)).

Fig. 5 illustrates multiple rollouts of various skills learned in the 2D bottleneck maze. For the 2D bottleneck maze, the upper left room is the most difficult region to reach since the agents need to pass multiple narrow pathways. Fig. 5 shows that only DISCO-DANCE and UPSIDE effectively explore the upper left region, with DISCO-DANCE displaying a relatively denser coverage on the upper left state space. More qualitative results are available in Appendix I.

#### 4.2.2 Ant mazes

To evaluate the effectiveness of DISCO-DANCE in the environment with high dimensional state and action space, we train DISCO-DANCE in three Ant mazes where the state space consists of joint angles, velocities, and the center of mass, and the action space consists of torques of each joint .

Table 1(b) reports the state coverage of DISCO-DANCE and baselines. DISCO-DANCE shows superior performance to the baselines where it achieves the best state-coverage performance in high-dimensional environments that contain obstacles (i.e., Ant U-maze and \(\)-maze) and gets competitive results against SMM with a marginal performance gap in the environment without any obstacle (i.e., Ant Empty-maze). These results demonstrate the effectiveness of our proposed direct guidance in navigating an environment with obstacles. Additional qualitative results of each algorithm can be found in the Appendix I.

To further assess whether the learned skills could be a good starting point for downstream tasks, we conduct goal-reaching navigation experiments where the goal state is set to the region farthest from the initial state for the more challenging navigation environment, Ant \(\)-maze. We measure the number of seeds that successfully reach the established goal state (out of a total 20 seeds). We select the skill with the maximum return (i.e., a skill whose state space is closest to the goal state) from the set of pretrained skills and finetune the policy \(_{}(a|s,z)\). Additional experiments in Ant U-maze and details are available in Appendix D, E.

Fig. 6 shows that DISCO-DANCE outperforms prior baseline methods in terms of sample efficiency. Specifically, DISCO-DANCE is the only algorithm that successfully reaches the goal state for all seeds (20 seeds) in Ant \(\)-maze. In addition, UPSIDE fails to reach the goal for all seeds, which indicates that the tree-structured policies learned during the pretraining stage can impede the agent from learning optimal policies for fine-tuning.

Figure 5: **Visualization of the skills in bottleneck maze. Multiple rollouts by each algorithm.**

Figure 6: **Finetuning results on the Ant \(\)-maze. We plot the number of successful seeds out of total 20 seeds.**

### Deepmind Control Suite

While the experiments in 2D and Ant mazes demonstrate that the skills DISCO-DANCE learns can be utilized as useful primitives for navigation tasks, to demonstrate the versatility and effectiveness of DISCO-DANCE in acquiring _general skills_ (e.g., run, jump, flip), we conduct additional experiments on the DMC benchmark . Specifically, we consider two different environments in DMC, Cheetah, and Quadruped, and evaluate DISCO-DANCE across four tasks (e.g., run, flip, and jump) within each environment (total of 8 tasks). The DMC benchmark evaluates the diversity of the learned skills, meaning that agents must learn a suitable set of skills for all downstream tasks in order to achieve consistently high scores across all tasks. We first perform pretraining for 2M training steps and select the skill with the highest return for each task (same as the Ant maze experiment). Then we finetune USD algorithms for 100k steps. More experimental details are available at Appendix D.

Fig. 7 shows the performance comparisons on DMC across 8 tasks with 15 seeds. First, all USD methods perform better than training from scratch (i.e., SCRATCH), indicating that pretraining with USD is also beneficial for continuous control tasks. Furthermore, as we described in Fig. 2(b), we find that our proposed _guidance_ method boosts the skill learning process for the continuous control environment, resulting in DISCO-DANCE outperforming all other baselines. As shown in Ant mazes finetuning experiments, UPSIDE significantly underperforms compared to other auxiliary reward-based USD baselines. Since UPSIDE necessitates the sequential execution of all its ancestor skills, its finetuning process becomes less efficient than other methods.

In addition, baselines with auxiliary rewards, which enforce exploration, are ineffective in DMC. We attribute these results to their training inefficiency, that the current experimental protocol of pretraining for 2M frames is insufficient for them to converge in a meaningful manner. For example, DISDAIN requires billions of environment steps in the grid world environment in the original paper. In contrast, DISCO-DANCE successfully converges within 2M frames and outperforms all other baselines. We provide the full performance table and visualizations of skills in Appendix E, I.

### Ablation Studies

#### 4.4.1 Model Components

We further conduct an ablation analysis to show that each of our model components is critical for achieving strong performance. Specifically, we experiment with two variants of DISCO-DANCE, each of which is trained (i) without guide coefficient (i.e., guide coefficient as 1) and (ii) without the random walk based guide selection process in Table 2.

First, without guide coefficient \((q_{}(z^{i}|s))\)\((1-q_{}(z^{i}|s))\), DISCO-DANCE sets all other skills as apprentice skills, causing all other skills to move towards the guide skill. As shown in Table 2, this method of guiding too many skills degrades performance, and it is important to only select unconverged skills as apprentice skills for effective learning.

   Models & Bottleneck maze \\  DISCO-DANCE & **86.30\(\)17.01** \\ DISCO-DANCE w/o guide coef & 76.75\(\)13.05 \\ DISCO-DANCE w/o random walk & 70.30\(\)10.72 \\ DIAYN (no exploration reward) & 52.40\(\)3.77 \\   

Table 2: **Ablation study on model components.**

Figure 7: **Performance comparisons on Deepmind Control Suite. Aggregate statistics and performance profiles with 95% bootstrap confidence interval are provided , which are calculated across 120 seeds (15 seeds \(\) 8 tasks).**Second, Table 2 also shows that random-walk based guide skill selection (i.e., selecting the skill with the lowest density region among PRM terminal states in Eq. 4) is superior to naively selecting the most distant skill without random walk process (i.e., selecting the skill with the lowest density region among P terminal states in Eq. 4). We also provide qualitative results (Fig. 8) where selecting the most distant skill as the guide skill fails to select the skill closest to the unexplored state region since the blue skill is the most distant from the terminal states of the other skills. On the other hand, random walk guide selection successfully finds the appropriate guide skill.

#### 4.4.2 Compatibility with other skill discovery method

In this paper, our primary goal is to compare DISCO-DANCE with algorithms that focus on constructing an effective auxiliary exploration reward, denoted as \(r_{}\). However, several research studies exist that seek to refine \(r_{}\) to enhance exploration [32; 20; 27] (detailed explanation in Section 5). To evaluate how effectively DISCO-DANCE can enhance exploration when aligned with different \(r_{}\), we carried out a comparative analysis with LSD . The recently introduced skill discovery algorithm, LSD, has demonstrated commendable outcomes, especially in Ant Mazes.

Table 3 illustrates the performance comparison with LSD. Our findings reveal that LSD, even in the absence of \(r_{}\), exhibits superior performance compared to DIAYN in Ant-\(\) maze. Such an observation possibly highlights LSD's proficiency in mitigating the inherent pessimism linked with the previous mutual information objective. An observation worth highlighting is the substantial enhancement in performance for both algorithms when integrated with our \(r_{}\). We propose that \(r_{}\) and \(r_{}\) have distinct roles, and when combined, they can collectively boost their overall performance.

## 5 Related Work

**Pretraining methods in RL** Pretraining methods for RL primarily fall into two categories . The first is online pretraining, where agent interact with the environment in the absence of any reward signals. This method generally operates under the presumption that agents can engage with the environment at a minimal cost. The primary objective of online pretraining is to acquire essential prior knowledge through unsupervised interactions with the environment. Recently, there has been a lot of research in this area, often referred to as "Unsupervised RL". Such methodologies typically employ intrinsic rewards, to learn representations or skills which will be useful for downstream tasks. The acquired pretrained weights, such as encoder, actor, and critic, are subsequently reused for downstream tasks.

Depending on how the intrinsic reward is modeled, online pretraining can be further categorized. Curiosity-driven approaches focus on probing states of interest, often characterized by high predictive errors, in order to obtain more environmental knowledge and reduce prediction errors across states [6; 35; 43; 34]. Data coverage maximization methods aim to maximizes the diversity of the data the policy has collected, for instance, by amplifying the entropy of state visitation distributions [5; 18; 24; 26; 57; 44]. Meanwhile, Unsupervised Skill Discovery involves learning policies, denoted as \((a|s,z)\), guided by a distinct skill vector \(z\). This vector can then be adjusted or combined to address subsequent tasks [13; 11; 17; 47; 7; 25; 19; 32; 1; 60; 39; 33].

The second category is offline pretraining. This method leverages an unlabeled offline dataset accumulated through various policies, to learn the representation. The learning objective in offline pretraining involves pixel or latent space reconstruction [58; 59; 61; 54], future state prediction [41; 42; 45; 23], learning useful skills from data [37; 3; 50; 36; 16; 56] and contrastive learning focusing on either instance [21; 12] or temporal discrimination [31; 51].

Figure 8: **Qualitative results for different guide skill selection processes.**

  \(r_{}\) & \(r_{}\) & Ant \(\)- maze \\  DIAYN & - & 22.50 \( 3.34\) \\  & \(r_{}\) & 39.00 \( 4.85\) \\  LSD & - & 38.80 \( 3.34\) \\  & \(r_{}\) & 45.80 \( 3.34\) \\  

Table 3: **Ablation study on \(r_{}\)**In this paper, our primary emphasis is on Unsupervised Skill Discovery within the domain of online pretraining. For a comprehensive overview of pretraining methods for RL, we direct the reader to .

**Unsupervised skill discovery.** There have been numerous attempts to learn a set of useful skills for RL agents in a fully unsupervised manner. These approaches can be broadly classified into two categories: (i) one studies how to learn the skill representations in an unsupervised manner (i.e., how to make \(r_{}\) in Eq. 5) and (ii) the other focuses on devising an effective auxiliary reward which encourages the exploration of skills to address the inherent pessimistic exploration problem (i.e., how to make \(r_{}\) in Eq. 5).

Research that falls under the first category focuses on how to train skills that will be useful for unseen tasks. DIAYN , VISR , EDL  and DISk  maximizes the mutual information (MI) between skill and state reached using that skills. DADS  utilizes a model-based RL approach to maximize conditional MI between skill and state given the previous state. LSD , which is non-MI-based algorithm, provides a 1-Lipschitz constraint on the representation of skills to learn dynamic skills. CIC  replace \(r_{}\) with a contrastive loss to learn high-dimensional latent skill representations, and additionally utilizes \(r_{}^{}\) (Eq. 3) for exploration. Choreographer  learns a world model to produce imaginary trajectories (which are used for learning skill representations) and additionally utilizes \(r_{}^{}\) for exploration.

DISCO-DANCE belongs to the second category that focuses on devising exploration strategy for USD. This category includes DISDAIN , UPSIDE , APS  and SMM , which are included as our baseline methods. Detailed explanation and limitations of these methods are described in Section 2.2, 4.1, Fig. 1, and Appendix F.

**Guidance based exploration in RL.** Go-Explore  stores visited states in a buffer and starts re-exploration by sampling state trajectory, which is the most novel (i.e., the least visited) in the buffer. DISCO-DANCE and Go-Explore share a similar motivation that the agent guides itself: DISCO-DANCE learns from other skills and Go-Explore learns from previous trajectories. Another line of guided exploration is to utilize a KL-regularization between the policy and the demonstration or sub-policy (i.e., guide) . SPIRL , an offline skill discovery method, minimizes the KL divergence between the skill policy and the learned prior from offline data. RIS  has been proposed for efficient exploration in goal-conditioned RL, by minimizing KL divergence between its policy and generated subgoals.

## 6 Conclusion

We introduce DISCO-DANCE, a novel, efficient exploration strategy for USD. It directly guides the skills towards the unexplored states by forcing them to follow the _guide skill_. We provide quantitative and qualitative experimental results that demonstrate that DISCO-DANCE outperforms existing methods in two navigation and a continuous control benchmark.

In this paper, in order to evaluate solely the effectiveness of \(r_{}\) methods, we fix the backbone \(r_{}\) as DIAYN. Combining DISCO-DANCE with other recent \(r_{}\) methods (e.g., CIC) is left as an interesting future work. Also, we think that there is still enough room for improvement in finetuning strategies . Since USD learns many different task-agnostic behaviors, a new fine-tuning strategy that can take advantage of these points would make the downstream task performances more powerful. A discussion of limitations is available in Appendix G.