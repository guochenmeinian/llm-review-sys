# Inferring Neural Signed Distance Functions by Overfitting on Single Noisy Point Clouds through Finetuning Data-Driven based Priors

Inferring Neural Signed Distance Functions by Overfitting on Single Noisy Point Clouds through Finetuning Data-Driven based Priors

 Chao Chen\({}^{1}\)   Yu-Shen Liu\({}^{1}\)   Zhizhong Han\({}^{2}\)

\({}^{1}\)School of Software, Tsinghua University, Beijing, China

\({}^{2}\)Department of Computer Science, Wayne State University, Detroit, USA

chenchao19@tsinghua.org.cn liuyushen@mails.tsinghua.edu.cn h312h@wayne.edu

The corresponding author is Yu-Shen Liu. This work was supported by National Key R&D Program of China (2022YFC3800600), and the National Natural Science Foundation of China (62272263, 62072268), and in part by Tsinghua-Kuaiishou Institute of Future Media Data.

###### Abstract

It is important to estimate an accurate signed distance function (SDF) from a point cloud in many computer vision applications. The latest methods learn neural SDFs using either a data-driven based or an overfitting-based strategy. However, these two kinds of methods are with either poor generalization or slow convergence, which limits their capability under challenging scenarios like highly noisy point clouds. To resolve this issue, we propose a method to promote pros of both data-driven based and overfitting-based methods for better generalization, faster inference, and higher accuracy in learning neural SDFs. We introduce a novel statistical reasoning algorithm in local regions which is able to finetune data-driven based priors without signed distance supervision, clean point cloud, or point normals. This helps our method start with a good initialization, and converge to a minimum in a much faster way. Our numerical and visual comparisons with the state-of-the-art methods show our superiority over these methods in surface reconstruction and point cloud denoising on widely used shape and scene benchmarks. The code is available at https://github.com/chenchao15/LocalN2NM.

## 1 Introduction

It is an important task to estimate an implicit function from a point cloud in computer graphics, computer vision, and robotics. An implicit function, such as a signed distance function (SDF), describes a continuous 3D distance field to indicate distances to the nearest surfaces at arbitrary locations. Since point clouds are easy to obtain, they are widely used as an information source to estimate SDFs, particularly without using normals that are not available for most scenarios. The challenge for SDF estimation mainly comes from the difficulty of bridging the gap between the discreteness of point clouds and the continuity of implicit functions.

Recent methods [62; 64; 29; 14; 95; 80; 58; 74] overcome this challenge using either a data-driven based or an overfitting-based strategy. To map a point cloud to a signed distance field, the data-driven based methods [60; 27; 36; 45; 81; 79; 22; 42; 92; 83] rely on a prior learned with signed distance supervision from a large-scale dataset, while the overfitting-based methods [28; 1; 102; 2; 99; 4; 21; 50; 18; 88] do not need signed distance supervision and just use the point cloud to infer a signed distance field. However, both of the two kinds of methods have pros and cons. The data-driven based methods can do inference fast but suffers from the need of large-scale training samples and poor generalization to instances that are unseen during training. Although the overfitting-based methods have a better generalization ability and do not need the large-scale signed distance supervision, they usually require a much longer time to converge during inference. The cons of these two kinds of methods dramatically limit the performance of learning neural SDFs under challenging scenarios like highly noisy point clouds. Therefore, beyond pursuing higher accuracy of SDFs, how to balance the generalization ability and the convergence efficiency is also a significant issue.

To resolve this issue, we propose to learn an SDF from a single point cloud by finetuning data-driven based priors. Our key idea is to promote the advantages of both the data-driven based and the overfitting-based strategy to pursue better generalization, faster inference, and higher accuracy. Our method overfits a neural network on a single point cloud to estimate an SDF with a novel loss without using signed distance supervision, clean point, or point normals, where the neural network was pretrained as a data-driven based prior from large-scale signed distance supervision. With finetuning priors, our method can generalize better on unseen instances than the data-driven based methods, and also converge much more accurate SDFs in a much faster way than the overfitting-based methods. Moreover, our novel loss for finetuning the data-driven based prior can conduct a statistical reasoning in a local region which can recover more accurate and sharper underlying surface from noisy points. We report numerical and visual comparisons with the state-of-the-art methods and show our superiority over these methods in surface reconstruction and point cloud denoising on widely used shape and scene benchmarks. Our contributions are summarized below,

* We introduce a method which is capable of funeutning a data-driven based prior by minimizing an overfitting-based loss without signed distance supervision, leading to neural SDFs with better generalization, faster inference, and higher accuracy.
* The proposed overfitting-based loss can conduct a novel statistical reasoning in local regions, which improves the accuracy of neural SDFs inferred from noisy point clouds.
* Our method produces the state-of-the-art results in surface reconstruction and point cloud denoising on the widely used benchmarks.

## 2 Related Works

Learning implicit functions has achieved promising performance in various tasks [62; 64; 29; 14; 95; 80; 58; 74; 30; 31; 33]. We can learn neural implicit representations from different supervision including 3D supervision [61; 69; 59; 17], multi-view images [78; 44; 38; 101; 46; 94; 63; 41; 98; 97; 25; 86; 100; 89; 84; 85], and point clouds [92; 43; 60; 27]. We briefly review the existing methods related to point clouds below.

### Data-driven based Methods

In 3D supervision, many techniques utilize a data-driven approach to learning priors, and then apply these learned priors to infer implicit models for unseen point clouds. Some strategies focus on acquiring global priors [60; 27; 36; 45; 81; 79; 22; 42] at the shape level, whereas others aim to boost the generalization of these priors by learning local priors [92; 83; 11; 37; 6; 51] at the component or patch level. These learned priors facilitate the marching cubes algorithm  to reconstruct surfaces from implicit fields. The effectiveness of these methods often rely on extensive datasets, but they may not generalize well when facing with unseen point clouds that significantly deviate in geometry from training samples.

### Overfitting-based Methods

In an effort to enhance generalization, some methods concentrate on precisely fitting neural networks to single point clouds. These methods incorporate innovative constraints [28; 1; 102; 2; 99; 4; 21], utilize gradients [50; 18; 88], employ differentiable Poisson solvers , or apply specially tailored priors [51; 54] to learn either signed [50; 28; 1; 102; 2; 15; 56; 13] or unsigned distance functions [18; 104; 103]. Despite achieving significant advances, these approaches typically require clean point clouds to accurately determine distance or occupancy fields around the point clouds.

### Learning from Noisy Point Clouds

The key to accurately reconstructing surfaces on noisy point clouds is to minimize the effect of noise in inferring implicit functions. PointCleanNet  was developed to filter out noise from point clouds through a data-driven approach. GPDNet  incorporated graph convolution based on dynamically generated neighborhood graphs to enhance noise reduction. Some other methods leveraged point cloud convolution , alternating latent topology [90; 57], semi-supervised strategy [106; 19], dual and integrated latent , or neural kernel field [91; 35] to reduce noise from point clouds. On the unsupervised front, TotalDenoising  adopts principles similar to Noise2Noise , utilizing a spatial prior suitable for unordered point clouds. DiGS  employs a soft constraint for unoriented point clouds. Noise2NoiseMapping  leverage statistical reasoning among multiple noisy point clouds with specially designed losses. Some methods using downsample-upsample frameworks , gradient fields [49; 9; 16; 68; 65], convolution-free intrinsic occupancy network , intra-shape regularization , eikonal equation [96; 23], neural Galerkin  and neural splines  have been implemented to further diminish noise in point clouds. Our method falls in this category, but we aim to promote the advantages of both the data-driven based and the overfitting-based strategy to pursue better generalization, faster inference, and higher accuracy.

## 3 Method

**Overview.** We aim to infer a neural SDF \(f\) from a single point cloud with noises \(M\). Our method includes two stages shown in Fig. 1, one is to learn a prior \(f^{}\) in a data-driven manner, the other is to infer a neural SDF \(f\) on unseen noisy point cloud \(M\). At the first stage, we learn a prior by training a neural SDF using ground truth signed distances of clean meshes indicated by embeddings \(^{}_{j}\). At the second stage, we finetune the learned prior \(f^{}\) to infer a neural SDF \(f\) of \(M\) using our proposed local noise to noise mapping, where the embedding \(\) indicating \(M\) is also learned. We can use the marching cubes algorithm  to extract the zero-level set of \(f\) as the mesh surface of \(M\).

**Neural Signed Distance Function.** We leverage an SDF \(f\) to represent the geometry of a shape. An SDF \(f\) is an implicit function that can predict a signed distance \(s\) for an arbitrary location \(q\), i.e., \(s=f(q)\). The latest methods usually train a neural network to approximate an SDF from signed distance supervision or infer an SDF from 3D point clouds or multi-view images. A level set is an iso-surface formed by the points with the same signed distance value. For instance, zero-level set is a special level set, which is formed by points with a signed distance of 0. On the zero-level set, the gradient \( f(q)\) of the SDF \(f\) at an arbitrary location \(q\) is also the surface normal at \(q\).

**Data-driven Based Prior.** As shown in Fig. 1, we employ an auto-decoder similar to DeepSDF  for learning a prior \(f^{}\) in a data-driven manner and inferring a neural SDF \(f\) for single point clouds with noises, respectively. We employ a data-driven strategy to learn a prior \(f^{}\) from clean meshes first. Specifically, we learn \(f^{}\) with an embedding \(^{}_{j}\) as a condition of queries. For each shape, we sample queries \(q\) around a shape represented by \(^{}_{j}\), and establish the signed distance supervision by recording the signed distance \(s\) to the ground truth mesh. Thus, we learn the prior \(f^{}\) by minimizing the prediction errors to the ground truth signed distances,

Figure 1: The overview of our method. We learn the data-driven based prior by learning a neural implicit function \(f^{}\) with a condition \(^{}\) on a clean dataset. During inference, we employ a novel statistical reasoning algorithm to infer a neural SDF \(f\) for a noisy point cloud \(M\) with learned prior (average code and learned parameter).

\[_{f^{},\{^{}_{i}\}}_{i=1}^{I}_{j=1}^{J}||s^{j}_{i}- f^{}(q_{j},^{}_{i})||_{2}^{2}+_{i=1}^{I}||^{ }_{i}||_{2}^{2},\] (1)

where \(^{}_{i}\) is a learnable condition for the \(i\)-th training shape, \(q_{j}\) is the \(j\)-th query that is randomly sampled around the \(i\)-th shape, and \(s^{j}_{i}\) is the ground truth signed distance. We also add a regularization term on the learned embeddings \(^{}_{i}\), and \(\) is the balance weight.

**Signed Distance Inference.** With the learned prior \(f^{}\), we infer a neural SDF \(f\) for a single point cloud with noises \(M\). We do not require ground truth signed distances, clean point clouds, or even point normal during the inference of \(f\). Specifically, we infer \(f\) by finetuning parameters of \(f^{}\) with a learnable embedding \(\) indicating the single point cloud with noises. The finetuning relies on a novel statistical reasoning algorithm on local regions.

The advantage of our method lies in the capability of conducting the statistical reasoning in local regions. Comparing to the global reasoning method , our method is able to not only infer more accurate geometry but also significantly improve the efficiency. Our method starts from randomly sampling a local region \(m_{n}\) on the shape \(M\). We randomly select one point on \(M\) as the center of \(m_{n}\), and set up its \(K\) nearest noisy points as a local region \(m_{n}\). Then, we randomly sample \(U\) queries \(\{_{u}\}_{u=1}^{U}\) around \(m_{n}\), and also randomly select \(U\) noisy points \(\{p_{v}\}_{v=1}^{U}\) out of \(m_{n}\) for statistically reasoning the surface in each iteration.

Our key idea of inferring a neural SDF \(f\) is to estimate a mean zero-level set that is consistent to all points in the local region \(m_{n}\). To this end, we use the \(U\) sampled queries \(\{_{u}\}\) to represent the zero-level set in this area using \(f\), and minimize the distances of the \(U\) noisy points \(\{p_{v}\}\) to the zero-level set in each iteration. Statistically, the expectation of the zero-level set should have the minimum distance to all the noisy point splitting in region \(m_{n}\).

Specifically, we first project the \(U\) sampled queries \(\{_{u}\}\) onto the zero-level set of \(f\) using a differentiable pulling operation . For each query \(_{u}\), its projection on the zero-level set is,

\[^{}_{u}=_{u}-s* f(_{u},)/| f( {q}_{u},)|,\] (2)

where \(^{}_{u}\) is the projection of \(_{u}\) on the zero-level set, \(s=f(_{u},)\), \( f(_{u},)\) is the gradient of \(f\) at the location \(_{u}\), and \(\) is the learnable embedding that represents the noisy point cloud \(M\).

With the pulling operation, we can use projections \(\{^{}_{u}\}\) of queries \(\{_{u}\}\) to approximate the zero-level set in region \(m_{n}\). With a coarse zero-level set estimation, we expect this zero-level set can be consistent to various subsets of noises \(\{p_{v}\}\) sampled from \(m_{n}\). Thus, we minimize the errors between the \(\{^{}_{u}\}_{u=1}^{U}\) and a subset of points \(\{p_{v}\}_{v=1}^{U}\) on area \(m_{n}\) in each optimization iteration,

\[_{f,}_{m_{n} M,_{u} m_{n},p_{v} m_{n}} EMD(\{^{}_{u}\},\{p_{v}\})+||||_{2}^{2},\] (3)

where we learn \(f\) through finetuning the prior \(f^{}\) and learning the embedding \(\) representing the noisy point cloud \(M\). The expectation is over the local regions \(m_{n}\) that randomly sampled from the noisy point cloud \(M\), and the subset patch \(p_{v}\) randomly sampled from each \(m_{n}\). We follow the method  to use the EMD to evaluate the distance between the two sets of points, which leads the neural SDF \(f\) to converge on the specific noisy point cloud \(M\).

**Initialization.** The network architecture of \(f\) is the same to the one of prior \(f^{}\). We learn \(f\) with the parameters of \(f^{}\) as the initialization, representing the prior that we learned. For the embedding \(\) that represents \(M\), we initialize \(\) as the center of the embedding space learned by the prior \(f^{}\) in Eq. 1, i.e., \(=1/I_{i=1}^{I}^{}_{i}\). This initialization is important for the accuracy and efficiency of learning \(f\) for single noisy point cloud \(M\). This finetuning of parameters of \(f^{}\) also shows advantages over the auto-decoding  in terms of generalization and efficiency. We will justify these advantages in our experiments.

**Implementation Details.** We randomly select one point from noisy point cloud \(M\) as a center, and select its \(K=1000\) nearest points to form a local region \(m_{n}\). We also randomly sample \(U=1000\) queries around the \(K\) noisy points for statistically reasoning. Specifically, we adopt a method introduced by NeuralPull  to sample errors around each one of the \(K\) noisy points. We use a Gaussian distribution centered at each point and set the standard deviation as the distance to the 51th nearest neighbor in the point cloud. We run the marching cubes for surface reconstruction at a resolution of 256 for shapes, and 512 for large-scale scenes.

The length of the embedding \(\) or \(^{}\) is set to \(256\). We use Adam optimizer for learning a neural implicit network, which is an auto-decoder similar to DeepSDF . For training, we use an initial embedding learning rate of 0.0005 for updating embeddings and an auto-decoder learning rate of \(0.001\) for optimizing the prior network. Both learning rates are decreased by 0.5 for every 500 epochs. We train the prior network \(f^{}\) for 2000 epochs. For inference, we finetune the network \(f^{}\) for each noisy point cloud in \(4000\) iterations with a learning rate of \(0.0001\).

## 4 Experiments and Analysis

We compare our method with the latest methods in terms of numerical and visual results on synthetic point clouds and real scans in surface reconstruction.

**Datasets and Metric.** We use eight datasets including shapes and scenes in the evaluations. For shapes, we conduct experiments under five datasets including ShapeNet , ABC , FAMOUS , Surface Reconstruction Benchmark (SRB)  and D-FAUST . For scenes, we conduct experiments under three real scan datasets including 3D Scene , KITTI , Paris-rue-Madame , and nuScenes . We leverage L1 Chamfer Distance (\(CD_{L1}\)), L2 Chamfer Distance (\(CD_{L2}\)) to evaluate the error between the reconstructed surface and ground truth. We also use Normal Consistency (NC)  and F-Score  with a threshold of 1% to evaluate the normal accuracy of the reconstructed surface. In the ablation study, we also report time consumption to highlight the superiority of our data-driven based prior. For KITTI and Paris-rue-Madame datasets, due to their lack of ground truth meshes, we only report visual comparisons.

### Surface Reconstruction for Shapes

**Evaluation on ShapeNet.** We first report our results on shapes from ShapeNet. We report evaluations by comparing our method with the latest prior-based and overfitting-based methods in Tab 1. For prior-based methods, we compare our method with PSG , R2N2 , COcc , OCNN , IMLS , POCO , and ALTO . All of these methods are pretrained to learn priors using shapes with noises in training set of ShapeNet. We also follow these methods to use the same set of training shapes to learn our prior. For overfitting-based methods, we compare our method with PSR , SAP , and N2NM . These methods did not need to learn a prior, and have the ability of inferring neural implicit functions on each shape in the testing set. We also follow these methods and report our results by finetuning our prior through overfitting on each testing shape. All the shapes for testing are corrupted with noises with a variance of \(0.005\).

  Metrics & PSR  & PSG1 & LQ2 & R2N  & COcc  & SAP  & OCNN  & IMLS  & POCO & ALTO  & N2NM  & Ours \\  \(CD_{L1}\) & 0.259 & 0.147 & 0.173 & 0.048 & 0.034 & 0.067 & 0.031 & 0.030 & 0.028 & 0.026 & **0.023** \\ NC & 0.772 & 0.721 & 0.938 & 0.944 & 0.932 & 0.944 & 0.950 & 0.955 & 0.962 & **0.973** \\ F-Score & 0.612 & 0.259 & 0.400 & 0.942 & 0.975 & 0.800 & 0.983 & 0.984 & 0.985 & 0.991 & **0.992** \\  

Table 1: Numerical Comparisons on ShapeNet dataset in terms of \(CD_{L1} 10\), NC and F-Score.

Figure 2: Comparison in surface reconstruction on ShapeNet. More visual results are provided in the appendix.

The comparisons in Tab. 1 indicate that our method can infer much more accurate neural implicit functions than the prior-based methods. The improvement comes from the ability of conducting test time optimization with the learned prior and inferring signed distances using the local noise to noise mapping. Moreover, our local statistical reasoning not only achieves better ability of recovering geometry from noisy points than overfitting-based methods but also significantly reduces the time complexity during the test-time overfitting procedure with our prior. Different from prior-based methods, our ability of conducting test-time optimization with our local statistical reasoning loss can significantly improve the generalization ability on unseen shapes. Tab. 2 shows that our method can infer neural implicit functions on single shapes much faster than the overfitting-based methods. We also demonstrate our advantages in visual comparisons in Fig. 2.

**Evaluation on ABC.** We also report our evaluations on ABC dataset in Tab. 3. We learn priors from shapes in training set, and finetune this prior for each single shape in the testing set. The numerical comparisons are conducted on the testing set of ABC dataset released by P2S . It includes two versions with different noise levels. Similarly, we also report comparisons with prior-based methods and overfitting-based methods. With our local noise to noise mapping, we achieve the best performance over all baselines. Compared to prior-based methods, such as P2S , COcc , and POCO , our loss can infer more accurate geometry during the test time overfitting procedure. Also, the ability of finetuning the prior can also provide a coarse estimation and a good start for inferring neural implicit from single noisy points. Besides the accuracy, we also observe improvements on efficiency. Fig. 3 demonstrates the improvements over the baselines in terms of surface completeness and edge sharpness.

**Evaluation on SRB.** We report previous experiments using man-made objects in ShapeNet and ABC dataset, We also report our results on real scans on SRB dataset . Since there is no training samples on SRB, we use the prior learned from the ShapeNet as the prior for real scans. Although the shapes in ShapeNet are not similar to shapes in SRB, we found the prior can also work well with the scans on SRB. Different from the man-made objects, real scans have unknown noises. We report the evaluations with the prior-based and overfitting-based methods in Tab. 4 and Fig. 4. The comparisons show that our method achieves the best performance in implicit surface reconstruction. Under the same experimental settings, our method can infer more accurate geometry details with our local noise to noise mapping.

**Evaluation on FAMOUS.** We report evaluations on more complex shapes on FAMOUS dataset. Similar to SRB, we also use the prior learned from ShapeNet. We evaluate the performance on two kinds of noises in Tab. 5. We can see that our method can recover more geometry details and achieve higher accuracy and smoother surfaces. We also report visual comparisons in Fig. 5, which also highlights our improvements in

  Metrics & SAP  & N2NM  & Ours \\  Time & 14 min & 46 min & **5 min** \\  

Table 2: Time consumption on ShapeNet dataset with overfitting-based methods.

Figure 4: Comparison in surface reconstruction on SRB. More visual results are provided in the appendix.

Figure 5: Comparison in surface reconstruction on FAMOUS. More visual results are provided in the appendix.

Figure 3: Comparison in surface reconstruction on ABC. More visual results are provided in the appendix.

terms of accuracy, smoothness, completeness, and recovered sharp edges.

**Evaluation on D-FAUST.** Finally, we report our results on non-rigid shapes, i.e., humans. Different from rigid shapes in the previous experiments, humans are with more complex poses. We learn a prior from the training set, and finetuning the prior on unseen humans with different poses. We mainly compare our method with overfitting-based methods in Tab. 6. We can see that our method achieves the best performance in CD, F-Score, and comparable performance to N2NM  but with faster inference speed. We further show the visual comparison in Fig. 6. We can see that our method can recover more accurate geometry and poses.

### Surface Reconstruction for Scenes

Since we have a limited number of scenes for training, we use the prior learned from ShapeNet as the pretrained prior in our experiments on scenes. Specifically, we conduct experiments on four different scene datasets: 3D Scene , KITTI , Paris-rue-Madame  and nuScenes , where the results on nuScenes are reported in the appendix.

**Evaluation on 3D Scene.** We further evaluate our method in surface reconstruction for scenes in 3D Scene . We follow previous methods LIG  to randomly sample \(1000\) points per \(m^{2}\). We compare our method with the latest methods including COcc  and LIG , DeepLS , NeuralPull (NP)  and Noise2NoiseMapping (N2NM) . For prior-based methods COcc  and LIG , we leverage their released pretrained models to produce the results, and we also provide them with the ground truth point normals. For overfitting-based methods DeepLS , NP  and N2NM , we overfit them to produce results with the same noisy point clouds. We follow LIG  to report \(CD_{L1}\), \(CD_{L2}\) and NC for evaluation. We report the comparisons in Tab. 7. The results demonstrate that our method outperforms both kinds of methods with learned priors such as LIG  and overfitting-based N2NM . The visual comparisons in Fig. 7 show that our method can reveal more geometry details on real scans, which justifies our capability of handling noise in point clouds.

  Dataset & PSR  & P2S  & COcc  & NP  & IMLS  & PCP  & POO  & OnSurf  & N2NM  & Ours \\  ABC var & 3.29 & 2.14 & 0.89 & 0.72 & 0.57 & 0.49 & 2.01 & 3.52 & 0.113 & **0.096** \\ ABC max & 3.89 & 2.76 & 1.45 & 1.24 & 0.68 & 0.57 & 2.50 & 4.30 & 0.139 & **0.113** \\  

Table 3: Numerical Comparisons on ABC dataset in terms of \(CD_{L2} 100\).

Figure 6: Comparison in surface reconstruction on D-FAUST. More visual results are provided in the appendix.

Figure 7: Comparison in surface reconstruction on 3D Scene.

**Evaluation on KITTI.** Following GridPull , we further evaluate our method on KITTI  odometry dataset (Sequence 00, frame 3000 to 4000), which contains about 13.8 million points, which are split into 15 chunks. We reconstruct each of them and concatenate them together for visualization. We compare our method with the latest methods SAP  and GridPull . As shown in Fig. 8, our method is robust to noise in real scans, successfully generalizes to large-scale scenes, and achieves visual-appealing reconstructions with more details.

**Evaluation on Paris-rue-Madame.** Following N2NM , we further evaluate our method on Paris-rue-Madame , which contains much noises. We split the \(10\) million points into 50 chunks each of which is used to learn a neural implicit function. We compare our method with LIG  and N2NM . For LIG , we produce the results for each chunk with released pretrained models. For N2NM , we overfit on all chunks until convergence. As shown in Fig. 9, we achieve better performance over LIG  and N2NM  in large-scale surface reconstruction, which highlight our advantages in reconstructing complete and detailed surfaces from noisy scene point clouds.

### Ablation Studies

We conduct ablation studies on the ABC dataset  to justify each module of our method.

**Embedding Size.** We evaluate our performance on different sizes of embedding \(\). We try several sizes \(\{128,256,512\}\) to infer the signed distance functions from a noisy point cloud. The numerical comparison in Tab. 8 shows that the optimal result is obtained with a size of \(256\). Deviations from this value, either longer or shorter dimensions, leads to worse results with the current number of training samples.

**Prior.** We conduct experiments to explore the importance of data-driven based prior. We first replace our learned embedding \(\) and parameter with randomly initialized embedding and parameter, or only replace \(\) with randomly initialized embedding. As shown in Tab. 9, The degenerated result of "Without Prior" and "Without Embed" indicates that directly inferring implicit functions without our prior or learned embedding makes it difficult to accurately learn the surfaces of the noisy point clouds, and also slows the convergence. Then we fix the learned parameters and only optimize the embedding \(\), similar to auto-decoding. The results also get worse, as shown in "Fixed Param".

**Local Region Splitting.** We further validate the effectiveness of local region splitting strategies. We employ three different splitting strategies in Tab. 10. We first split the whole space where the noisy point cloud is located uniformly into multiple voxel blocks, as shown by the result of "Voxel". The severely degenerated results indicate that this splitting strategy is even worse than the global method N2NM , as it results in many empty voxel blocks. Then we randomly select a point from the noisy point cloud as a center to sample all points within a radius of \(0.1\) as a local region. The result of "Sphere (Fixed Size)" slightly degenerates due to some of the spheres containing too few points. In contrast, our splitting strategy, as shown by the result of "Sphere (KNN)", ensures that each local region has enough points to help achieve superior performance.

   &  &  &  &  &  &  \\  F-var & 1.80 & 0.28 & 0.80 & 0.19 & 0.07 & 1.50 & 0.59 & 0.13 & 0.033 & **0.029** \\ F-max & 3.41 & 0.31 & 0.39 & 0.26 & 0.30 & 2.75 & 3.64 & 0.21 & 0.117 & **0.105** \\  

Table 6: Accuracy of reconstruction on D-FAUST dataset in terms of \(CD_{L1}\), NC and F-Score.

  Metric & 128 & 256 & 512 \\  \(CD_{L2} 100\) & 0.102 & **0.096** & 0.114 \\  

Table 8: Effect of the embedding size.

  Metric & 128 & 256 & 512 \\  \(CD_{L2} 100\) & 0.108 & 0.103 & 0.144 & **0.096** \\ Time & 1h & 12min & 30min & **8 min** \\  

Table 9: Effect of the prior.

   &  &  &  &  &  &  \\  \(CD_{L2} 1000\) & 14.10 & 6.190 & 1.607 & 2.115 & 0.507 & **0.389** \\ \(CD_{L1}\) & 0.052 & 0.048 & 0.025 & 0.034 & 0.019 & **0.016** \\ NC & 0.908 & 0.849 & 0.915 & 0.900 & 0.929 & **0.942** \\  

Table 7: Numerical Comparisons on 3D Scene dataset in terms of \(CD_{L1}\), \(CD_{L2}\) and NC. Detailed comparisons for each scene are provided in the appendix.

  Metric &  &  &  &  \\  \(CD_{L2} 100\) & 0.108 & 0.103 & 0.144 & **0.096** \\ Time & 1h & 12min & 30min & **8 min** \\  

Table 9: Effect of the prior.

[MISSING_PAGE_FAIL:9]

Noise Level.We report the effect of the sparsity of noisy point clouds. We downsample the noisy point clouds to 25% and 50% of their original size to validate the impact of sparsity. The \(CD_{L2}\) results in Tab. 15 and visual comparisons in Fig. 14 indicate that our method can handle sparsity in noisy point clouds better than N2NM . Since our data-driven based prior can help to learn a more complete surface and reduce the impacts brought by the sparsity.

Time Consumption.Since our method can handle sparsity and require less time as the point number decreases, we conduct an experiment with downsampled noisy points in Tab. 16. Fig. 14 indicates that we can work well on much fewer points, and also provide an alternative of improving efficiency.

Optimization.We visualize the optimization process in Fig. 15. We reconstruct meshes using the neural SDF learned in different iterations. We see that the shape is updated progressively to the ground truth shapes.

## 5 Conclusion

We propose a method to resolve the key problem in inferring SDFs from a single noisy point cloud. Our method can effectively use a data-driven based prior as an initialization, and infer a neural SDF by overfitting on a single noisy point cloud. The novel statistical reasoning successfully infers an accurate and smooth signed distance field around the single noisy point cloud with the data-driven based prior. By finetuning data-driven based priors with statistical reasoning, our method significantly improves the robustness, the scalability, the efficiency, and the accuracy in inferring SDFs from single point clouds. Our experimental results and ablations studies show our superiority and justify the effectiveness of the proposed modules.

  Metric & 10\% & 30\% & 50\% & 70\% & 100\% (3k) \\  Time & 3.1min & 3.6min & 4.0min & 4.5min & 5.0min \\  

Table 16: The comparison of time consumption with different point numbers.

  Method & Middle & Max & Extreme \\  N2NM  & 0.113 & 0.139 & 0.156 \\ Ours & **0.096** & **0.113** & **0.125** \\  

Table 14: Effect of noise level.

Figure 14: Visual comparison with different point numbers.

Figure 13: Visual comparison with different noise levels.

  Method & 25\% & 50\% & 100\% \\  N2NM  & 0.154 & 0.133 & 0.113 \\ Ours & **0.121** & **0.107** & **0.096** \\  

Table 15: Effect of sparsity.

Figure 15: Optimization during inference.