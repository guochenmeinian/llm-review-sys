# Learning Complete Protein Representation by Deep Coupling of Sequence and Structure

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Learning effective representations is crucial for understanding proteins and their biological functions. Recent advancements in language models and graph neural networks have enabled protein models to leverage primary or tertiary structure information to learn representations. However, the lack of practical methods to deeply co-model the relationships between protein sequences and structures has led to suboptimal embeddings. In this work, we propose CoupleNet, a network that couples protein sequence and structure to obtain informative protein representations. CoupleNet incorporates multiple levels of features in proteins, including the residue identities and positions for sequences, as well as geometric representations for tertiary structures. We construct two types of graphs to model the extracted sequential features and structural geometries, achieving completeness on these graphs, respectively, and perform convolution on nodes and edges simultaneously to obtain superior embeddings. Experimental results on a range of tasks, such as protein fold classification and function prediction, demonstrate that our proposed model outperforms the state-of-the-art methods by large margins.

## 1 Introduction

Proteins are the fundamental building blocks of life and play essential roles in a diversity of applications, from therapeutics to materials. They are composed of 20 different basic amino acids, which are lined by peptide bonds and form a sequence. The one-dimensional (1D) sequence of a protein determines its structure, which in turn determines its biochemical function . Due to recent progress in protein sequencing , massive numbers of protein sequences are now available. For example, the UniProt  database contains over 200 million protein sequences with annotations, _e.g._, gene ontology (GO) terms, similar proteins, family and domains. Notably, the development of large-scale language models (LMs) in natural language processing has substantially benefited protein research owing to similarities between human language and protein sequences [16; 27]. For instance, models like ProtTrans  and ESM-series [39; 33] in learning protein representations have proven successful utility of pre-training protein LMs with self-supervision to process protein sequences.

Thanks to the recent significant progress made by AlphaFold2  in three-dimensional (3D) structure prediction, a large number of protein structures from their sequence data are now made available. The latest release of AlphaFold protein structure database  provides broad coverage of UniProt . Recently proposed structure-based protein encoders become to utilize geometric features [25; 24; 53], _e.g._, ProNet  learns representations of proteins with 3D structures at different levels, like the amino acid, backbone or all-atom levels. There also exists a group of methods that build graph neural networks and LMs (LSTMs or attention models) to process sequence and structure [53; 50; 19], for example, GearNet  encodes sequential and spatial features by alternating node and edge message passing on protein residue graphs.

The 1D sequence and 3D structure of a protein provide different types of information, in detail, as shown in Figure 1, compared with the 1D sequential order and amino acids in peptide chains, the tertiary structure provides 3D coordinates of each atom in protein residues, which allow them to perform precise functions. Although a protein's sequence determines its structure, various works have demonstrated the effectiveness of learning from either sequence or structure [33; 25]. However, rich constraints between the sequence and structure of a protein, which may be critical for protein tasks , have yet to be fully explored. Most protein sequence-structure modeling methods cannot deeply integrate the information behind sequence and structure for the reason that they tend to fuse representations together, extracted from sequence and structure encoders, respectively, by message passing mechanism  or by simple concatenation operations.

In this work, we aim to learn protein representations by deeply coupling the protein sequences and structures. Considering the relative positions of residues in the sequence and the spatial arrangement of atoms in the Euclidean space, the proposed CoupleNet constructs two categories of graphs for them, respectively. The complete representations are obtained at the amino acid and backbone levels on the two graphs, which are used as node and edge features to learn the final graph-level representations. Rather than concatenating sequence and structure representations, we take advantage of graph convolutions, performing node and edge convolutions simultaneously. The contributions of this paper are threefold:

* We propose a novel two-graph-based approach for representing the sequence and the 3D geometric structure of a protein, which is an effective way to guarantee completeness.
* We propose CoupleNet, a model that performs convolutions on nodes and edges of graphs to effectively integrate protein sequence and structure. This can better model the node-edge relationships and utilize the intrinsic associations between sequences and structures.
* Practically, the proposed model is verified by obtaining new state-of-the-art experimental results compared with current mainstream protein representation learning methods on a range of tasks, including protein fold classification, enzyme reaction classification, GO term prediction, domain prediction, and enzyme commission number prediction.

## 2 Related Work

Protein Representation LearningProtein representation learning has become an active and promising direction in biology, which is essential to various downstream tasks in protein science. Because of the different levels of protein structures, existing methods mainly fall into three categories: protein LMs for sequences, structure models for geometry, and hybrid methods for both of them. As proteins are sequences of amino acids, considering their similarities with human languages, UniRep , UDSMPProt  and SeqVec  use LSTM or its variants to learn sequence representations and long-range dependencies. TAPE  benchmarks a group of protein models, _e.g._, 1D CNN, LSTM, and Transformer by various tasks. Elnaggar _et al._ have trained six successful transformer variants

Figure 1: Illustration of the protein sequence and structure. 1) The primary structure comprises \(n\) amino acids. 2) The tertiary structure with atom arrangement in Euclidean space is presented, where each atom has a specific 3D coordinate. Amino acids have fixed backbone atoms (\(_{},,,\)) and side-chain atoms that vary depending on the residue types. GLU: Glutamic acid. Complete geometries can be obtained based on these coordinates. The sequence and structure provide different information types and data categories.

on billions of amino acid sequences, like ProtBert, and ProtT5. Similarly, ESM-series [39; 38; 33] employs a transformer architecture and a masked language modeling strategy to train robust representations based on large-scale databases. Besides the protein sequence, as we have stated before, the 3D geometric structure is vital to enhance protein representations. Most methods commonly seek to encode the spatial information of protein structures by convolutional neural networks (CNNs) , or graph neural networks [19; 2; 29]. For instance, SPROF  employs distance maps to predict protein sequence profiles, and IEConv  introduces a convolution operator to capture all relevant structural levels of a protein. GVP-GNN  designs the geometric vector perceptrons (GVP) for learning both scalar and vector features in an equivariant and invariant manner, Guo _et al._ adopt SE(3)-invariant features as the model inputs and reconstruct gradients over 3D coordinates to avoid the usage of complicated SE(3)-equivariant models. ProNet  learns hierarchical protein representations at multiple tertiary structure levels of granularity. Moreover, CDConv  proposes continuous-discrete convolution using irregular and regular approaches to model the geometry and sequence structures. Some protein learning methods model the multi-level of structures at the same time [53; 6; 15], except for the primary structure and the tertiary structure, the second refers to the 3D form of local segments of proteins (e.g., \(\)-helix, \(\)-strand), the quaternary is a protein multimer comprising multiple polypeptides, for example, PromtProtein  adopts a prompt-guided multi-task learning strategy for different protein structures with specific pre-training tasks. While previous works have attempted to combine protein sequence and structure, we focus on profoundly integrating them by specifically designing two types of graphs respectively and conducting convolutions simultaneously to learn protein representations.

Complete Message Passing MechanismComENet  proposes rotation angles and spherical coordinates to fulfil the global completeness of 3D information on molecular graphs. By incorporating these designed geometric representations into the message passing scheme , the complete representation for a whole 3D graph is eventually yielded . Unlike these methods, we couple sequence and structure via corresponding graphs and different geometric representations to obtain completeness representations.

## 3 Method

### Preliminaries

NotationsWe represent a 3D graph as \(G=(,,)\), where \(=\{v_{i}\}_{i=1,,n}\) and \(=\{_{ij}\}_{i,j=1,,n}\) denote the vertex and edge sets with \(n\) nodes in total, respectively, and \(=\{P_{i}\}_{i=1,,n}\) is the set of position matrices, where \(P_{i}^{k_{i} 3}\) represents the position matrix for node \(v_{i}\). We treat each amino acid as a graph node for a protein, then \(k_{i}\) depends on the number of atoms in the \(i\)-th amino acid. The node feature matrix is \(X=[_{i}]_{i=1,,n}\), where \(_{i}^{d_{v}}\) is the feature vector of node \(v_{i}\). The edge feature matrix is \(E=[_{ij}]_{i,j=1,,n}\), where \(_{ij}^{d_{}}\) is the feature vector of edge \(_{ij}\). \(d_{v}\) and \(d_{}\) denote the dimensions of feature vectors \(_{i}\) and \(_{ij}\).

Invariance and EquivarianceWe consider affine transformations that preserve the distance between any two points, _i.e._, the isometric group SE(3) in the Euclidean space. This is called the symmetry group, and it turns out that SE(3) is the special Euclidean group that includes 3D translations and the 3D rotation group SO(3) [17; 12]. The matrix form of SE(3) is provided in Appendix A.1.

Given the function \(f:^{m}^{m^{}}\), assuming the given symmetry group \(G\) acts on \(^{m}\) and \(^{m^{}}\), then \(f\) is G-equivariant if,

\[f(T_{g})=S_{g}f(),\;^{m},g G\] (1)

where \(T_{g}\) and \(S_{g}\) are the transformations. For the SE(3) group, when \(m^{{}^{}}=1\), the output of \(f\) is a scalar, we have

\[f(T_{g})=f(),\;^{m},g G\] (2)

thus \(f\) is SE(3)-invariant.

Complete Geometric RepresentationsA geometric transformation \(()\) is complete if two 3D graphs \(G^{1}=(,,^{1})\) and \(G^{2}=(,,^{2})\), there exists \(T_{g}(3)\) such that the representations

\[(G^{1})=(G^{2}) P_{i}^{1}=T_{g}(P_{ i}^{2}),\ \ i=1, n\] (3)

The operation \(T_{g}\) would not change the 3D conformation of a 3D graph . Positions can generate geometric representations, which can also be recovered from them.

Message Passing ParadigmMessage passing mechanism is mainly applied in graph convolutional networks (GCNs) , which follows an iterative scheme of updating node representations based on the feature aggregation from nearby nodes.

\[_{i}^{(0)} =((_{i})),\] (4) \[_{i}^{(l)} =f_{}^{(l)}(_{j}^{(l-1)}|v_{j}(v _{i})),\] \[_{i}^{(l)} =f_{}^{(l)}(_{j}^{(l-1)},_{i}^{(l)})\]

where \(()\) and \(()\) mean the linear transformation and batch normalization respectively. \((v_{i})\) denotes the neighbours of node \(v_{i}\). \(f_{}^{(l)}\) and \(f_{}^{(l)}\) are aggregation and transformation functions at the \(l\)-th layer, which are permutation invariant and equivariant of node representations.

### Sequence-Structure Graph Construction

Specifically, we represent each amino acid as a node, considering the residue types and their positions \(i=1,2,,n\) (See Figure 1) in the sequence, we define the sequential graph primarily on the sequence, if \(\|i-j\|<l\), the edge \(_{ij}\) exists, where \(l\) is a hyperparameter. Besides the sequential graph, we predefine a radius \(r\), and build the radius graph, and there exists an edge between node \(v_{i}\) and \(v_{j}\) if \(\|P_{i,}-P_{j,}\|<r\), where \(P_{i,}\) denotes the 3D position of \(_{}\) in the \(i\)-th residue.

Firstly, we design a base approach called \(_{}\) that only uses the \(_{}\) positions of the structures. Inspired by Ingraham _et al._, we construct a local coordinate system (LCS) for each residue, as shown in Figure 2.

\[_{i}=[_{i}_{i}_{i}_{i}]\] (5)

where \(_{i}=}-P_{i-1,}}{\|P_{i, }-P_{i-1,}\|},_{i}=_{i}-_{i+1}}{\|_{i}-_{i+1}\|},_{i}=_{i}_{i+1 }}{\|_{i}_{i+1}\|}\). Then we can get the geometric representations at the amino acid level of a protein 3D graph,

\[(G)_{ij,aa}=(\|P_{i,}-P_{j, }\|,_{i}^{T}}-P_{j,}}{\|P_{i,}-P_{j,}\|},_{i}^{T} _{j})\] (6)

where \(\) is the matrix multiplication, this implementation is SE(3)-equivariant and obtains complete representations at the amino acid level; as if we have \(_{i}\), the LCS \(_{j}\) can be easily obtained by \((G)_{ij,aa}\).

For a node \(v_{i}\), the node features \(_{i,aa}\) in the baseline approach is the concatenation of the one-hot embeddings of the amino acid types and the physicochemical properties of each residue, namely, a steric parameter, hydrophobicity, volume, polarizability, isoelectric point, helix probability and sheet probability [51; 22], which provide quantitative insights into the biochemical nature of each amino acid. And \((G)_{ij,aa}\) is set as edge features for \(_{}\).

Secondly, we consider all backbone atoms \(_{},,,\) in CoupleNet. In detail, the peptide bond exhibits partial double-bond character due to resonance , indicating that the three non-hydrogen atoms comprising the bond (the carbonyl oxygen, carbonyl carbon, and amide nitrogen) are coplanar,

Figure 2: The local coordinate system.

as shown in Figure 3. There is some rotation about the connection. The \(_{i}-_{ i}\) and \(_{ i}-_{i}\) bonds, are the two bonds in the basic repeating unit of the polypeptide backbone. These single bonds allow unrestricted rotation until sterically restricted by side chains . Since the coordinates of \(_{}\) can be obtained as we have the complete representations at the amino acid level, the coordinates of other backbone atoms based on these rigid bond lengths and angles are able to be determined with the remaining degree of the backbone torsion angles \(_{i},_{i},_{i}\). The omega torsion angle around the \(-\) peptide bond is typically restricted to nearly \(180^{}\) (trans) but can approach \(0^{}\) (cis) in rare instances. Other than the bond lengths and angles presented in Figure 3, all the H bond lengths measure approximately 1 A.

For the sequential graph, we compute the sine and cosine values of \(_{i},_{i},_{i}\) for each amino acid \(i\), and use them as another part of nodes features for node \(v_{i}\).

\[_{i}=_{i,aa}\|(()(_{i},_{i},_{i}))\] (7)

where \(\|\) denotes concatenation. There is no isolated node for the designed graph, which means the backbone atoms can be determined one by one along the polypeptide chain based on the positions of \(_{}\) and these three backbone dihedral angles. Therefore, the existing presentations \([(G)_{ij,aa}]_{i,j=1,,n}\) and \([_{i}]_{i=1,,n}\) are complete at the backbone level for the sequential graph.

For the radius graph, we want to get the positions of backbone atoms in any two amino acids \(i\) and \(j\). Inspired by trRosetta , the relative rotation and distance are computed including the distance (\(d_{ij,_{}}\)), three dihedral angles (\(_{ij},_{ij},_{ji}\)) and two planar angles (\(_{ij},_{ji}\)), as shown in Figure 4, where \(d_{ij,_{}}=d_{ji,_{}},_{ij}=_{ji}\), but \(\) and \(\) values depend on the order of residues. These interresidue geometries define the relative locations of the backbone atoms of two residues in all their details , because the torsion angles of \(_{i}-_{ i}\) and \(_{ i}-_{i}\) do not influence their positions. Therefore, these six geometries are complete for amino acids at the backbone level for the radius graph. The graph edges contain the relative spatial information between any two neighboring amino acids \(_{ij}=(G)_{ij,aa}\|(G)_{ij,bb}\), where

\[(G)_{ij,bb}=(d_{ij,_{}},()(_{ij },_{ij},_{ij}))\] (8)

The designed node and edge features, \(_{i}\) and \(_{ij}\), for the sequential and radius graphs, provide a new perspective to represent protein sequences and structures. Such integration can bring better performance for the following graph-based learning tasks.

Figure 4: Interresidue geometries including angles and distances.

Figure 3: The polypeptide chain depicting the characteristic backbone bond lengths, angles, and torsion angles (\(_{i},_{i},_{i}\)). The planar peptide groups are denoted as shaded gray regions, indicating that the peptide plane differs from the geometric plane calculated based on the 3D positions.

### **Secq**ounce-Structure Graph Convolution

Inspired by the message passing paradigm and continuous-discrete convolution , sequences and structures are encoded successfully together by convolutions. To deeply couple sequences and structures of proteins and encode them jointly, we employ convolution to embed them simultaneously, exploring their relationships to generate comprehensive and effective embeddings. Different from previous works, we innovatively construct two categories of graphs for sequence and structure and design various sequential and structural representations to achieve completeness on them at the amino acid and backbone levels. We then convolve node and edge features with the help of the message passing mechanism.

In order to implement convolution on nodes and edges simultaneously between sequence and structure, we set \(_{ij}\) to exist if the following conditions are satisfied

\[\|i-j\|<l\|P_{i,}-P_{j,} \|<r\] (9)

The existing node and edge feature matrices (\(X,E\)) are complete representations of a protein 3D graph to reconstruct its backbone atom positions. Compared with the equation Eq. 4, the proposed CoupleNet first apply a \(()\) layer and a \(()\) layer to the node features to obtain the initial encoded representation. Then the \(f_{}^{(l)}\) is applied to gather neighboring features of nodes and edges by convolution, where \(()\) is the activation function. We use the dropout and add a residual connection from the previous layer as \(f_{}^{(l)}\). For the consideration that the spatial arrangement and tight positioning of specific amino acids, which may be spaced widely apart on the linear polypeptide, are necessary for proteins to operate as intended , \(l\) is set to be a relatively large number, see Appendix A.2 for details.

\[_{i}^{(0)} =((_{i})),\] (10) \[_{i}^{(l)} =((_{v_{j}(v_{i})}W_{ij} _{j}^{(l-1)}),\] \[_{i}^{(l)} =_{i}^{(l)}+(_{i}^{(l)})\]

### **Model Architecture**

Building upon the sequence-structure graph convolution, we build the CoupleNet, as shown in Figure 5. The inputs to the graph are the calculated sequential and structural representations (\(X,E\)). Following the existing protein graph models [15; 47; 25], our CoupleNet employs graph pooling layers to obtain deeply encoded, graph-level representations. After pooling, due to the decrease in nodes, we increase the predefined radius \(r\) to include more neighbors. The message passing mechanism only executes on nodes for the consideration of reducing model complexity. Another reason is that representations of sequences and structures have already been coupled by equation Eq. 4. A detailed description of the model architecture is provided in Appendix A.2.

Figure 5: An illustration of CoupleNet.

## 4 Experiments

### Datasets and Settings

The models are trained with the Adam optimizer  using the PyTorch and PyTorch Geometric libraries. Detailed descriptions of the datasets and experimental settings are provided in Appendix A.3. Following the tasks in IEConv , GearNet  and CDConv , here, we evaluate the CoupleNet on four protein tasks: protein fold classification, enzyme reaction classification, GO term prediction and enzyme commission (EC) number prediction.

Fold ClassificationProtein fold is to predict the fold class label given a protein, which is crucial for understanding how protein structure and protein evolution interact . In total, this dataset contains \(16,712\) proteins with \(1,195\) fold classes. There are three test sets available, Fold: Training excludes proteins from the same superfamily. Superfamily: Training does not include proteins from the same family. Family: Proteins from the same family are included in the training.

Enzyme Reaction ClassificationReaction categorization aims to predict a protein's class of enzyme-catalyzed reactions, according to all four levels of the EC number [49; 36]. Following the setting in , this dataset has \(37,248\) proteins from 384 four-level EC numbers .

GO Term PredictionThe goal of GO term prediction is to foretell whether a protein is related to a certain GO term. Following , these proteins are organized into three ontologies: molecular function (MF), biological process (BP), and cellular component (CC), which are hierarchically connected, functional classes. MF describes activities that occur at the molecular level, BP represents the larger processes, and CC describes the parts of a cell or its extracellular environment .

EC Number PredictionThis task seeks to predict the 538 EC numbers from the third level and fourth levels of different proteins , which describe their catalysis of biochemical reactions.

    &  &  & Enzyme \\   & & Fold & SuperFamily & Family & Reaction \\   & CNN \({}^{*}\) & 11.3 & 13.4 & 53.4 & 51.7 \\  & ResNet \({}^{*}\) & 10.1 & 7.21 & 23.5 & 24.1 \\  & LSTM \({}^{*}\) & 6.41 & 4.33 & 18.1 & 11.0 \\  & Transformer \({}^{*}\) & 9.22 & 8.81 & 40.4 & 26.6 \\   & GCN \({}^{*}\) & 16.8 & 21.3 & 82.8 & 67.3 \\  & GAT \({}^{*}\) & 12.4 & 16.5 & 72.7 & 55.6 \\  & 3DCNN\_MQA \({}^{*}\) & 31.6 & 45.4 & 92.5 & 72.2 \\  & IEConv (atom level) \({}^{*}\) & 45.0 & 69.7 & 98.9 & 87.2 \\   & GraphQA \({}^{*}\) & 23.7 & 32.5 & 84.4 & 60.8 \\  & GVP \({}^{*}\) & 16.0 & 22.5 & 83.8 & 65.5 \\  & ProNet-Amino Acid  & 51.5 & 69.9 & 99.0 & 86.0 \\  & ProNet-Backbone  & 52.7 & 70.3 & 99.3 & 86.4 \\  & ProNet-All-Atom  & 52.1 & 69.0 & 99.0 & 85.6 \\  & IEConv (residue level) \({}^{*}\) & 47.6 & 70.2 & 99.2 & 87.2 \\  & GearNet  & 28.4 & 42.6 & 95.3 & 79.4 \\  & GearNet-IEConv  & 42.3 & 64.1 & 99.1 & 83.7 \\  & GearNet-Edge  & 44.0 & 66.7 & 99.1 & 86.6 \\  & GearNet-Edge-IEConv  & 48.3 & 70.3 & 99.5 & 85.3 \\  & CDConv  & 56.7 & 77.7 & 99.6 & 88.5 \\   & CoupleNet (Proposed) & **60.6** & **82.1** & **99.7** & **89.0** \\   

Table 1: Accuracy (\(\%\)) on fold classification and enzyme reaction classification. \({}^{*}\) means the results are taken from . The best and suboptimal results are shown in bold and underline.

### Baselines

We compare our proposed method with existing protein representation learning methods, which are classified into three categories based on their inputs, which could be a sequence (amino acid sequence), 3D structure or both sequence and structure. 1) Sequence-based encoders, including CNN , ResNet , LSTM  and Transformer . 2) Structure-based methods (GCN , GAT , 3DCNN_MQA , IEConv (atom level) ). 3) Sequence-structure based models, _e.g._, GVP , ProNet , GearNet , CDConv , _etc._ GearNet-IEConv and GearNetEdge-IEConv  add the IEConv layer based on GearNet, which is found important in fold classification.

### Results of Fold and Reaction Classification.

Table 1 provides the comparisons on the fold and enzyme reaction classification. The results are reported in terms of accuracy (\(\%\)) for these two tasks. From this table, we can see that the proposed model CoupleNet achieves the best performance across all four test sets on the fold and enzyme reaction classification compared with recent state-of-the-art methods. Especially on the Fold and SuperFamily test sets, CoupleNet improves the results by about \(4\%\), showing that CoupleNet is proficient at learning the mapping between protein sequences, structures and functions. Moreover, CDConv  ranks second among these methods, both CDConv and our method are implemented by sequence-structure convolution. This phenomenon illustrates that deeply coupling sequences and structures of proteins is conducive to learning better protein embeddings. And our proposed CoupleNet model utilizes complete geometric representations and the specially designed message passing mechanism, achieving new state-of-the-art results.

### Results of GO Term and EC Prediction

We follow the split method in [19; 53] to guarantee that the test set only comprises PDB chains with sequence identity no higher than \(95\%\) to the training set for GO term and EC number prediction. Table 2 compares different protein modeling methods on GO term prediction and EC number prediction. The results are reported in terms of \(_{}\), which considers both precision and recall for evaluation, the equation of \(_{}\) is provided in Appendix A.4. The proposed model, CoupleNet yields the highest \(_{}\) across these four test sets of two tasks, outperforming other state-of-the-art models. This indicates CoupleNet can effectively predict the functions, locations, and enzymatic activities of proteins. These results once again illustrate the importance of deeply coupled sequences

   Category & Method & GO-BP & GO-MF & GO-CC & EC \\   & CNN \({}^{*}\) & 0.244 & 0.354 & 0.287 & 0.545 \\  & ResNet \({}^{*}\) & 0.280 & 0.405 & 0.304 & 0.605 \\  & LSTM \({}^{*}\) & 0.225 & 0.321 & 0.283 & 0.425 \\  & Transformer \({}^{*}\) & 0.264 & 0.211 & 0.405 & 0.238 \\   & GCN \({}^{*}\) & 0.252 & 0.195 & 0.329 & 0.320 \\  & GAT \({}^{*}\) & 0.284 & 0.317 & 0.385 & 0.368 \\  & 3DCNN\_MQA \({}^{*}\) & 0.240 & 0.147 & 0.305 & 0.077 \\   & GraphQA \({}^{*}\) & 0.308 & 0.329 & 0.413 & 0.509 \\  & GVP \({}^{*}\) & 0.326 & 0.426 & 0.420 & 0.489 \\  & IEConv (residue level) \({}^{*}\) & 0.421 & 0.624 & 0.431 & - \\  & GearNet  & 0.356 & 0.503 & 0.414 & 0.730 \\  & GearNet-IEConv  & 0.381 & 0.563 & 0.422 & 0.800 \\  & GearNet-Edge  & 0.403 & 0.580 & 0.450 & 0.810 \\  & GearNet-Edge-IEConv  & 0.400 & 0.581 & 0.430 & 0.810 \\  & CDConv  & 0.453 & 0.654 & 0.479 & 0.820 \\   & CoupleNet (Proposed) & **0.467** & **0.669** & **0.494** & **0.866** \\   

Table 2: \(_{}\) on GO term and EC number prediction. \([^{*}]\) means the results are taken from . The best and suboptimal results are shown in bold and underline.

and structures. The improvements of CoupleNet over the suboptimal CDConv  model indicate the advanced modeling power of CoupleNet.

We employ different cutoff splits following [19; 15], which means that the proteins in the test set are divided into groups that have, respectively, 30\(\%\), 40\(\%\), 50\(\%\), 70\(\%\), and 95\(\%\) similarity to the training set for GO term and EC number prediction, as shown in Figure 6 and Appendix A.5. From Figure 6, we can see that our proposed model CoupleNet achieves the highest \(_{}\) scores across all cutoffs, especially when the 30 cutoffs are at 30\(\%\) to 50\(\%\). There is a larger margin compared with GearNet, GearNet-Edge  and CDConv . This demonstrates that CoupleNet, which utilizes complete geometric representations, is more robust, especially when there is a low similarity between the training and test sets.

### Ablation Study

Table 3 presents an ablation study of the proposed CoupleNet model on the four protein tasks. We examined the impact of removing the backbone torsion angles (w/o \(,,\)) and removing the interresidue geometric structure representations (w/o \(d,,,\)). The former is designed for the sequential graph, and the latter is for the radius graph to achieve completeness at the protein backbone level. However, we combine the two types of graphs together to enhance the relationships between sequence and structure. From Table 3, we can also find that these complete geometries provide complementary information to amino acid position features, with one of their removals leading to minor performance drops for the reason that they both provide complete geometries from different perspectives. Removing \(,,\) causes larger performance degradation compared with removing \(d,,,\). Such comparisons indicate that the backbone dihedral angles may have more effects on learning protein representations in these experimental settings. Compared with \(_{}\), CoupleNet achieves significant improvements on the four tasks, demonstrating the importance of complete structural representations at the backbone level in learning protein embeddings.

## 5 Conclusions and Limitations

In this work, we propose CoupleNet, a novel protein representation learning method that deeply fuses protein sequences and multi-level structures by conducting convolution on graph nodes and edges simultaneously. We design the sequential graph and the radius graph, achieving completeness on them at different protein structure levels. Our approach achieves new state-of-the-art results on the protein tasks, which demonstrates the superiority our the proposed method. A limitation is that the detailed inter-relationships between sequence and structures remain to be explored and uncovered. We leave such research for future work.

While our model can enable advanced protein analyses and provide effective representations, there may exist broader impacts and harmful activities. The representations could potentially be misused, _e.g._, for designing harmful molecules or proteins.