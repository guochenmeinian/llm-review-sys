# Credal Deep Ensembles for Uncertainty Quantification

Kaizheng Wang\({}^{1,4,}\)1 Fabio Cuzzolin\({}^{3}\) Shireen Kudukkil Manchingal\({}^{3}\)

Keivan Shariatmadar\({}^{2,4}\) David Moens\({}^{2,4}\) Hans Hallez\({}^{1}\)

\({}^{1}\)KU Leuven, Department of Computer Science, DistriNet

\({}^{2}\)KU Leuven, Department of Mechanical Engineering, LMSD

\({}^{3}\)Oxford Brookes University, Visual Artificial Intelligence Laboratory

\({}^{4}\)Flanders Make@KU Leuven

{kaizheng.wang, keivan.shariatmadar, david.moens, hans.hallez}@kuleuven.be

{fabio.cuzzolin, 19185895}@brookes.ac.uk

###### Abstract

This paper introduces an innovative approach to classification called _Credal Deep Ensembles_ (CreDEs), namely, ensembles of novel _Credal-Set Neural Networks_ (CreNets). CreNets are trained to predict a lower and an upper probability bound for each class, which, in turn, determine a convex set of probabilities (credal set) on the class set. The training employs a loss inspired by distributionally robust optimization which simulates the potential divergence of the test distribution from the training distribution, in such a way that the width of the predicted probability interval reflects the 'epistemic' uncertainty about the future data distribution. Ensembles can be constructed by training multiple CreNets, each associated with a different random seed, and averaging the outputted intervals. Extensive experiments are conducted on various out-of-distributions (OOD) detection benchmarks (CIFAR10/100 vs SVHN/Tiny-ImageNet, CIFAR10 vs CIFAR10-C, ImageNet vs ImageNet-O) and using different network architectures (ResNet50, VGG16, and ViT Base). Compared to Deep Ensemble baselines, CreDEs demonstrate higher test accuracy, lower expected calibration error, and significantly improved epistemic uncertainty estimation.

## 1 Introduction

The quantification of the uncertainty associated with neural network predictions has recently attracted increasing attention, to enhance the reliability and robustness of neural networks. Researchers agree to distinguish _aleatory uncertainty_ (AU) from _epistemic uncertainty_ (EU): the former arises from the inherent randomness, e.g., data noise, and is irreducible. The latter is caused by a lack of knowledge about the process which generates the data, due to the limited availability of training data, and is reducible [1; 36]. Effective EU quantification is beneficial for out-of-distribution (OOD) detection [30; 54] and can contribute to a variety of safety-critical applications, including autonomous driving , medical diagnosis , flood uncertainty estimation , structural health monitoring .

In classification, standard neural networks (SNNs) whose predictions amount to single probability distributions are unable to account for epistemic uncertainty, because a single distribution assumes precise knowledge about the dependency between inputs and outputs. To properly capture the EU, the network's outcome needs to express the uncertainty about a prediction's uncertainty itself [35; 62].

The most well-known approach to EU quantification in deep learning leverages Bayesian neural networks (BNNs) [7; 22; 38]. BNNs model network parameters as distributions and thus predict a'second-order' distribution (i.e., a distribution of distributions) , although in practice predictionsare often generated by running the network on sample parameters extracted from a posterior. While efficient training techniques (such as sampling [33; 57] and variational inference [7; 22]) have been developed to mitigate their complexity, practical challenges persist for BNNs, including the scaling to large datasets, handling complex network architectures, and real-time applicability .

An alternative approach, called Deep Ensembles (DEs), handles uncertainty quantification by aggregating multiple individually-trained SNNs , so that predictions amount to finite sets of probability distributions over the classes. DEs, often viewed as an approximation of Bayesian model averaging (BMA) , have become a powerful baseline for uncertainty estimation [2; 25; 53; 54; 60]. However, empirical evidence  suggests that DEs could yield relatively low-quality estimates of epistemic uncertainty. Further, DEs lack a sound theoretical justification [27; 47].

Credal inference [13; 36; 62] (which predicts convex sets of probability distributions or _credal sets_ on the target space) can provide an alternative way of quantifying epistemic uncertainty. Credal representations have been extensively studied within the broader field of machine learning, including, e.g., the naive credal classifier , the notion of credal network  or credal random forest classification . 'Imprecise' BNNs have been recently introduced which model both network weights and predictions as credal sets . While these models demonstrate robustness in Bayesian sensitivity analysis, their computational complexity is on a par with that of ensembles of BNNs, significantly limiting their practical applicability.

**Novelty and Main Contributions**. This paper presents an innovative approach to classification tasks called _Credal Deep Ensembles_ (CreDEs), ensembles of novel _Credal-Set Neural Networks_ (CreNets), aiming to improve EU quantification in the framework of credal inference. At the cost of merely doubling the number of output nodes compared to classical SNNs, CreNets are trained to predict a lower and an upper probability bound for each class rather than a single probability value. Such probability intervals over classes thus efficiently determine a prediction in the form of a credal set. The training strategy is inspired by Distributionally Robust Optimization [42; 55; 61], which simulates the potential divergence of the test distribution from the training distribution. As a result, the width of the predicted probability interval reflects the 'epistemic' uncertainty about the future data distribution. Adopting an ensemble strategy, CreDEs derive the final prediction by averaging the probability intervals outputted by the members of the ensemble. A conceptual comparison between CreDEs and DEs is illustrated in Figure 1.

Extensive experimental validation is conducted on several OOD detection benchmarks, including CIFAR10/100 (ID) vs SVHN/Tiny-ImageNet (OOD), CIFAR10 (ID) vs CIFAR10-C (OOD), ImageNet (ID) vs ImageNet-O (OOD), and across different network architectures: ResNet50, VGG16 and Visual Transformer Base (ViT Base). Compared to traditional Deep Ensembles, our CreDEs achieve _higher test accuracy_ and _lower expected calibration error_ (ECE) on ID samples, and _significantly improve the quality of EU estimation_.

**Related Work** Besides BNNs, DEs, and credal inference, other'second-order' uncertainty estimation approaches exist, such as Dirichlet-based methods [9; 48; 49; 50; 63], in which predictions are represented as Dirichlet distributions. One significant challenge for the latter is the absence of ground

Figure 1: Comparison between the proposed Credal Deep Ensembles and traditional Deep Ensembles. The former aggregate a collection of credal set predictions from CreNets as the final (credal) prediction, whereas the latter average a set of single probability distributions from standard SNNs as the outcome. E.g., in the probability simplex  associated with the target space \(\!=\!\{A,B,D\}\) (the triangle in the figure), a probability vector \((q_{A},q_{B},q_{D})\) is represented as a single point. For each CreNet, the predicted lower and upper probabilities of each class act as constraints (parallel lines) which determine a credal prediction (in gray). Single credal predictions are aggregated as in Sec. 2.4.

truth labels. Although various loss functions have been proposed, these models' performance often diverges from theoretical EU assumptions . Another rationale for the exclusion of Dirichlet-based approaches as baselines for our CreDE work is that such models often necessitate the inclusion of OOD data during training [48; 49; 56]. This challenges their practical adaptability, as it cannot guarantee their robustness against other forms of 'unseen' OOD data . Moreover, a recent study  has shown that these methods often fail to capture the EU properly, making the resulting measures difficult to interpret quantitatively.

Paper OutlineThe remainder of this paper is structured as follows. Sec. 2 presents our CreNets and CreDEs in full detail. Sec. 3 describes the experimental validations and results. Sec. 4 summarizes our conclusions and future work. Appendices report mathematical proofs in SSA, additional experiments in SSB, implementation details in SSC, the analysis of alternative ensemble strategies for CreDEs in SSD, and further discussion on future work in SSE (including achieving statistical guarantees using conformal learning and the framework's extension to regression), respectively.

## 2 Approach

The proposed Credal-Set Neural Network architecture and forward propagation are introduced in Sec. 2.1. CreNets' training procedure is discussed in Sec. 2.2. The class prediction and uncertainty quantification are discussed in Sec. 2.3. Credal Deep Ensembles are presented in Sec. 2.4.

### Credal-Set Neural Networks

Architecturally, our CreNet design focuses only on the final classification layers, and can therefore be applied on top of any representation layers of neural network models. The final layers of a CreNet (Figure 2) first output a deterministic interval for each class, using for each class an output node associated with the interval midpoint \(m\) and one associated with its half-length \(h\), respectively (a total of \(2C\) nodes for \(C\) classes).

(1)

where \(_{1:C},_{1:C},_{C+1:2C},_{C+1:2C}\) are the weights and biases associated with the first \(C\) and the remaining \(C\) nodes, respectively. Here \(g()\) is an arbitrary activation function and \(()\) denotes the Softplus function  that ensures the non-negativity of \(\).

The deterministic intervals associated with all classes, denoted as \([_{L},_{U}]\!:=\!\{[a_{L_{i}},a_{U_{i}}]\}_{i=1}^{C}\), can then be obtained as \([_{L},_{U}]\!=\![\!-\!,\!+\!]\).

A proper mapping from such deterministic intervals \([_{L},_{U}]\) to a collection of probability intervals \([_{L},_{U}]\!:=\!\{[q_{L_{i}},q_{U_{i}}]\}_{i=1}^{C}\) for each class needs to ensure that \([_{L},_{U}]\) satisfies the conditions:

\[q_{L_{i}}\!\!q_{U_{i}}\  i\!=\!1,...,C_{i=1}^{C}q _{L_{i}} 1\!\!_{i=1}^{C}q_{U_{i}}.\] (2)

The former condition guarantees a proper \([q_{L_{i}},q_{U_{i}}]\) for each class. The latter enables the resulting collection of probability intervals to determine a non-empty credal set, \(\), as follows :

\[\!=\!\{\,|\,q_{i}\!\![q_{L_{i}},q_{U_{i}}];_{i=1}^{C}q _{i}\!=\!1\}.\] (3)

The probability vectors in \(\) meet the normalization condition, and their probability value per class is constrained by the probability intervals (Eq. (2)).

Traditional SoftMax activation cannot ensure that the convexity conditions in Eq. (2) are met when computing \([_{L},_{U}]\) using \(_{L}\!=\!(_{L})\) and \(_{U}\!=\!(_{U})\), respectively. A toy example

Figure 2: CreNet final layer structure for three classes.

is given in Appendix SSA. Therefore, we employ Interval SoftMax activation as proposed in  to compute \([_{L},_{U}]\) from \([_{L},_{U}]\), as follows:

\[q_{Li}\!=\!})}{(a_{L_{i}})\!+\!_{k i} \!(}\!+\!a_{L_{k}}}{2})},\ q_{Ui}\!=\!})}{ (a_{U_{i}})\!+\!_{k i}\!(}\!+\!a_{L_{k} }}{2})},\] (4)

where \(q_{L_{i}}\) and \(q_{U_{i}}\) are the lower and upper probability bound for the \(i^{th}\) class, respectively. As proven in Appendix SSA, the probability intervals generated by the Interval SoftMax duly satisfy Eq. (2).

### Training Procedure

The rationale for the training of a CreNet is for the predicted lower and upper bounds (Eq. (4)) to the probability of the classes, \(_{L}\) and \(_{U}\), to express the epistemic uncertainty (induced by the limited size and variability of the training set) about how different the distribution of future test data may be from that of the training data.

To this extent, we designed a composite loss function with two components: one, which applies classical cross entropy to the upper probability vector, encourages the latter to optimistically assume that test data distribution will be similar. The other, inspired by Distributionally Robust Optimization (DRO) , pushes the lower probability to reflect a 'pessimistic' stance on future distributional divergence. The width of the resulting interval will thus reflect the epistemic uncertainty associated with the prediction.

We first contrast the classical training strategy with that of Distributionally Robust Optimization in Sec. 2.2.1. We then delve into the design and implementation of our CreNet loss in Sec. 2.2.2.

#### 2.2.1 Classical and DRO Training Strategy

**Vanilla Strategy** Given a training set \(\!=\!\{_{n},_{n}\}_{n=1}^{N}\), the conventional neural network training process aims to solve the following optimization problem

\[*{minimize}_{}\{ _{n=1}^{N}((_{n},_{n}),)\},\] (5)

where \(\) denotes the model's trainable parameters in the space \(\) and \(\) denotes an arbitrary loss function. The underlying assumption is that the training and test distributions are identical. As a result, the trained network serves as an empirical risk minimizer . However, this ideal assumption often results in over-optimistic predictions because the test observations may, in practice, significantly differ from the training data .

**DRO Strategy** In contrast to the vanilla strategy, the objective of DRO  is to minimize the worst-case expected risk (\(R()\)) over an uncertain set of distributions \(\), as follows:

\[*{minimize}_{}\{R() _{U}_{(,) U}((, ),)\},\] (6)

in which \(\) is the expectation operation. In practice, a _group DRO_ setting  is adopted in which the training distribution \(P\) is assumed to be a mixture of \(m\) groups \(P_{g}\), indexed by \(g=\{1,2,...,m\}\). Because the optimum of a linear program is attained at a vertex, the worst-case risk in Eq. (6) is equivalent to a maximum over the expected loss of each group, as follows:

\[R()=*{maximize}_{g}_{(,) P_{g}}((,),).\] (7)

In practice, the group DRO model minimizes the empirical worst-group risk \(()\), namely:

\[*{minimize}_{}\{() *{maximize}_{g}_{(,) _{g}}((,),)\},\] (8)

where \(_{g}\) is the empirical distribution of the \(g\)-th group of training points. Therefore, group DRO learns models with good worst-group training loss across groups . One special form of _group DRO_ is _adversarially reweighted learning_, which consists of a minimax game between a learner and adversary. The learner optimizes for the main classification task and aims to learn the best parameters \(\) that minimize the expected loss. In contrast, the adversary maximizes the expected loss by making an adversarial assignment of weights \(w_{n}\), collected in a vector \(\). Consequently, the training optimization problem assumes the form

\[*{minimize}_{}\{_{}_{n=1}^{N}w_{n}((_{n},_ {n}),)\},\] (9)

where the set \(\) of weight vectors varies across different implementations .

#### 2.2.2 CreNet Loss Design and Implementation

**Design** As anticipated, the CreNet training process applies the vanilla training strategy to the upper probability prediction vector \(_{U}\) (Eq. (5)), and the DRO strategy to the lower probability prediction \(_{L}\) (Eq. (9)). The resulting overall loss function has a composite structure

\[_{}:=^{N} (_{U_{n}},_{n})}}_{}+_{} ^{N}}w_{n}(_{L_{n}},_{n})}_{},\] (10)

where CE denotes the classical cross-entropy loss function used in classification. Given a predicted discrete probability vector \(\) and the ground-truth label \(\), CE is defined as: \(:=-_{k}^{C}t_{k}_{2}q_{k}\). The vanilla component is applied to the upper probability vector \(_{U}\) because such a loss takes the training distribution at face value and is thus more likely to encourage 'optimistic' (overconfident) or 'upper bound' predictions for the class scores. The DRO component is computed on the lower probability vectors \(_{L}\), as it weighs training outliers to simulate future differences in data distribution at test time, encouraging 'pessimistic' or 'lower bound' class score predictions. Thus, the width of the resulting probability interval will reflect the uncertainty associated with the model's ignorance of how much the future test distribution will differ from the train distribution, using the boundary/outlier cases observed at training time to guess what the uncertainty on future test cases will be.

**Cross-Entropy of Lower/Upper Probability Vectors** Please note that in Eq. (10), the CE is applied to lower/upper probability vectors, which are not (normalized) probability vectors. However, as the ground truth (label) vector \(\) equals \(1\) for the true class \(j^{*}\) and 0 for all the other elements, calculating \((,)\) for any predicted probability vector \(\) reduces to \(-_{2}q(j^{*})\). Consequently, all probability vectors with the same component for the true class will generate the same CE for that sample.

The consequence for CreNet training is that feeding a lower (upper) probability vector \(_{L}\) (\(_{U}\)) to Eq. (10) is equivalent to computing the CE with _any one_ of the probability vectors in the credal prediction (Eq. (3)) whose probability for the true class \(j^{*}\) equals the lower (upper) probability value there. It can be shown that these form one of the 'faces' of the boundary of the credal set.

Importantly, because of the functional structure of Interval SoftMax activation (Eq. (4)), upper and lower probability vectors are not computed independently, but are correlated. Thus, they are minimized together via the total loss (Eq. (10)), with the DRO component also influencing the upper probability \(_{U}\), driving the solution away from the trivial one (all-ones upper probability vectors).

**Implementation** As the \(\) of weight vectors in Eq. (10) varies across different implementations  and estimating \(\) in Eq. (10) is not straightforward when using batch-wise training , we resort to a simpler heuristic proposed by . For each training batch, only the \(\!\![0.5,1)\) portion of samples with the highest cross-entropy with the lower probability vector (\((_{L_{n}},_{n})\)) are selected to compute the DRO component of the loss. As a result, \(w_{n}\!>\!1\) is implicitly set in Eq. (10) for selected samples while \(w_{n}\!=\!0\) for deselected samples.

The underlying rationale is the following. Within a batch of samples, those instances that demonstrate high losses are identified as 'hard-to-learn' samples, essentially representing the'minority group' within a training dataset . Setting a value \(\) thus identifies what fraction of the training points is chosen to represent potential future domain shifts at test time. A smaller \(\) signifies a more cautious approach, in which even a few training outliers can indicate future challenges.

The lower bound to the design range for \(\) is \(0.5\) because we empirically observed that values of \(<0.5\) may destabilize the training process, as a too-large averaged loss is returned for backpropagation. When \(\) approaches \(1\), the data distribution of the samples considered by the vanilla and the DRO components of the loss becomes similar, implicitly assuming a less pronounced divergence between train and test distributions. The corresponding predicted probability intervals become narrower. If \(\)were theoretically set to 1, all samples would be selected for backpropagation, implying that \(w_{n}\!=\!1\) for any \(n\) in Eq. (10). Consequently, the loss in Eq. (10) would be the sum of the vanilla component on \(_{U}\) and the vanilla component on \(_{L}\). Empirically, we observed that this leads to a collapse of the upper and lower probability bounds to single values.

The implementation of the CreNet training procedure is shown in Algorithm 1.

``` Input: Training dataset \(\!=\!\{_{n},_{n}\}_{n=1}^{N}\); Portion of samples per batch \(\!\![0.5,1)\); Batch size \(\) whileenable trainingdo 1. Compute \((_{U_{n}},_{n})\) and \((_{L_{n}},_{n})\) for each sample 2. Sort the sample indices \((m_{1},...,m_{})\) in descending order of \((_{L_{n}},_{n})\) 3. Define \(_{}=\) 4. Minimize \(_{}\!=\!_{n=1}^{}(_{U_{n}},_{n})\!+\!}_{j=1}^{_{}} (_{L_{m_{j}}},_{m_{j}})\) endwhile ```

**Algorithm 1** CreNet Training Procedure

### Class Prediction and Uncertainty Quantification

**Class Prediction** For the class prediction we employ the'maximin' and'maximax' criteria :

\[_{}\!:=\!*{argmax}_{i}q_{L_{i}}^{*};_ {}\!:=\!*{argmax}_{i}q_{U_{i}}^{*},\] (11)

which output (respectively) the class indices with the highest lower and upper reachable probability (\(q_{L_{i}}^{*}\) and \(q_{U_{i}}^{*}\)) within the same credal set induced by the predicted lower and upper probabilities \(q_{L_{i}},q_{U_{i}}\). Figure 3 illustrates how the lower and upper probabilities \(q_{L_{i}},q_{U_{i}}\) that determine the credal set \(\) may differ from the probabilities \(q_{L_{i}}^{*}\) and \(q_{U_{i}}^{*}\)_actually reachable_ for each class within \(\). The reachable lower and upper probabilities for class \(i\) can be easily obtained as follows :

\[q_{U_{i}}^{*}\!=\!\!(q_{U_{i}},1\!-\!\!_{j i}\!q_{L_{i}} )\!,q_{L_{i}}^{*}\!=\!\!(q_{L_{i}},1\!-\!\!_{j i}\!q_{ U_{j}})\!.\] (12)

**Uncertainty Quantification** Given a credal set prediction, upper and lower entropies generalizing Shannon's entropy, denoted as \(()\) and \(()\), can be defined which may serve as measures for TU and AU, respectively .

Computing \(()\) boils down to solving the following optimization problem:

\[()\!=\!*{maximize}_{i=1}^{C} \!-\!q_{i}\!\!_{2}q_{i} q_{L_{i}}^{*}\!\!q_ {i} q_{U_{i}}^{*} i\ \ \ \ _{i=1}^{C}\!q_{i}\!=\!1.\] (13)

This seeks the highest entropy value of a probability distribution within the predicted credal set \(\). \(()\), for which \(*{maximize}\) is replaced by \(*{minimize}\), searches for the minimal such entropy. Such optimization problems can be addressed using a standard solver, e.g., the SciPy optimization package . Epistemic uncertainty can then be quantified as \(()-()\).

**Computational Complexity Reduction** To reduce the computational complexity of Eq. (13) for a large value of \(C\) (e.g., \(C\!=\!1000\)), we propose an original approach called _Probability Interval Dimension Reduction_ (PIDR) in Algorithm 2. This method first identifies the \(K\!-\!1\) classes with the highest lower probability values, then merges the remaining elements into a single class with the associated upper and lower probability calculated using Eq. (12). Consequently, the dimension of the probability interval is reduced from \(C\) to \(K\).

### Credal Deep Ensembles

Inspired by conventional DEs , the final step of our approach is to introduce _Credal Deep Ensembles_ (CreDEs). CreDEs aggregate \(M\) individually trained CreNets and predict the aggregated probability intervals, denoted as \([}_{L}^{*},}_{U}^{*}]\), as follows:

\[}_{L}^{*}\!=\!\!_{m=1}^{M}\!_{L_{m}}^ {*},\ \ }_{U}^{*}\!=\!\!_{m=1}^{M}\!\!_{U_{m}}^ {*},\] (14)

where \([_{L_{m}}^{*},_{U_{m}}^{*}]\) is the set of reachable probability intervals predicted by the \(m\)-th CreNet. Eq. (20) in Appendix D proves that \([}_{L}^{*},}_{U}^{*}]\) satisfies the convexity condition in Eq. (2) for constructing a non-empty credal set. Therefore, class prediction and uncertainty estimation as described in Sec. 2.3 apply to CreDEs. We discuss the rationale for averaging strategy and the alternative ensemble approaches for CreDEs in Appendix SSD.

## 3 Experimental Validation

**Setup** We assessed CreDEs through OOD detection benchmarks across various dataset pairings (ID vs OOD samples), including CIFAR10 /CIFAR100  vs SVHN /Tiny-ImageNet , CIFAR10 vs CIFAR10-C , ImageNet  vs ImageNet-O . We trained 15 CreNets (using \(\!=\!0.5\)) and SNNs on the ResNet50 architecture  starting from different random seeds, using the training set as per the ID dataset in the pair. Following this, we constructed 15 different CreDEs and DEs, respectively, by randomly selecting five members from the pool of 15 trained models. The same ensemble member lists are used for both DEs and CreDEs, with each ensemble strictly guaranteed to be distinct. More details are given in Appendix SSC. Codes are available at https://gitlab.kuleuven.be/m-group-campus-brugge/distinet_public/credal-deep-ensembles.git.

**Uncertainty Quantification in DEs** Total uncertainty (TU) can be quantified in DEs via the Shannon entropy (\(H\)) of the averaged predicted distribution. The AU, on the other hand, can be obtained by averaging the entropies of the predictions of each ensemble member [2; 36]. Namely,

\[\!:=\!H(})\!=\!H(_{m=1}^{M}_{m}), \!:=\!()\!=\!_{m=1}^{M}H(_{ m}),\] (15)

where \(M\) is the number of networks, \(}\) and \(_{m}\) denote the average probability vector and the single probability vector of the \(m\)-th SNN model, respectively. The level of epistemic uncertainty, representing an approximation of mutual information , can be obtained as \(\!:=\!H(})-()\).

**Test Accuracy and ECE on ID Samples** We evaluated the test accuracy and expected calibration error (ECE) [24; 58] of CreDEs-5 and DEs-5 on the test set of each ID dataset. A lower ECE value signifies a closer alignment between the model's confidence scores and the true probabilities of the events. Since ECE is designed for a singular probability vector, we implemented a compromise calculation as follows. Suppose our model predicts the class indices \(k\) and \(j\) when using the \(_{}\) and \(_{}\) criteria, respectively, ECE values are then computed based on the associated lower \(q_{L_{k}}^{*}\) and upper \(q_{U_{j}}^{*}\) reachable probabilities in the credal set.

Table 1 reports the test accuracy and ECE for DEs-5 and CreDEs-5 on the various datasets, indicating that our CreDEs-5 achieved _higher test accuracy_ and _lower ECE_ on ID samples. Note that employing the \(_{}\) prediction showed higher ECE on the challenging ImageNet dataset. This is likely because the strategy, selecting the class with the highest lower reachable probability, is a conservative one.

Figure 3: If intervals are redundant, some of the (e.g.) upper probabilities \(q_{U_{A}}\) may not be actually reachable in the credal set that results from the intersection of all interval constraints.

[MISSING_PAGE_FAIL:8]

**Ablation Study on Hyperparameter \(\) for CreNet Training** In our main evaluation, we set by default \(=0.5\) to reflect a balanced assessment of the train-test divergence and show how such a value allows our model to outperform the baselines. Table 4 reports the test accuracy and OOD detection performance (using EU estimates) of CreDEs-5 under various values of \(\). The ablation findings verify the robustness of CreDEs across hyperparameter setups and indicate the \(=0.5\) might be too pessimistic a choice in CIFAR10 settings. Performance peaks at \(\!=\!0.875\) in most cases, implying that \(\!=\!0.875\) may provide the 'optimal' estimate of how test and train sets diverge for CIFAR10. One possible way to find the 'best' \(\) in practice is to conduct standard cross-validation on specific test scenarios. However, the method is not particularly sensitive to this hyperparameter. Per-spectively, an interesting option, in the presence of multiple datasets (e.g.acquired over time in a continual learning setting), could be applying the DRO loss component to different components of the training set, and assessing the results to robustly select \(\).

We also report the average EU estimation values of CreDEs-5 for each dataset in Table 5. Increasing the value of \(\) (i.e., giving less importance to the divergence between test and training

    &  &  &  &  \\  Test Accuracy & \(_{}\) & 93.74 & 94.54 & 94.47 & 94.57 & 93.88 & 93.99 \\ (CIFAR10) & \(_{}\) & 93.75 & 94.55 & 94.47 & 94.56 & 93.87 & 93.99 \\  SVHN & AUROC & 97.44 & 97.94 & 97.92 & 97.95 & 97.42 & 97.51 \\ (OOD Detection) & AUROC & 97.37 & 96.34 & 97.00 & 96.92 & 98.79 & 98.82 \\  Tiny-ImageNet & AUROC & 88.28 & 98.01 & 89.10 & 89.18 & 89.85 & 89.24 \\ (OOD Detection) & AUROC & 88.13 & 98.81 & 89.76 & 89.72 & 89.18 & 89.26 \\   

Table 4: Test accuracy (%, \(\)) and OOD detection performance (%, \(\)) of CreDEs-5 using various \(\). Results are averaged over 15 runs.

[MISSING_PAGE_FAIL:10]