# Quadratic Quantum Variational Monte Carlo

Baiyu Su

University of Texas at Austin

baiyusu@utexas.edu &Qiang Liu

University of Texas at Austin

lqiang@cs.utexas.edu

###### Abstract

This paper introduces the Quadratic Quantum Variational Monte Carlo (Q\({}^{2}\)VMC) algorithm, an innovative algorithm in quantum chemistry that significantly enhances the efficiency and accuracy of solving the Schrodinger equation. Inspired by the discretization of imaginary-time Schrodinger evolution, Q\({}^{2}\)VMC employs a novel quadratic update mechanism that integrates seamlessly with neural network-based ansatzes. Our extensive experiments showcase Q\({}^{2}\)VMC's superior performance, achieving faster convergence and lower ground state energies in wavefunction optimization across various molecular systems, without additional computational cost. This study not only advances the field of computational quantum chemistry but also highlights the important role of discretized evolution in variational quantum algorithms, offering a scalable and robust framework for future quantum research.

## 1 Introduction

Finding fast and accurate approaches to solving Schrodinger equations is a central challenge in quantum chemistry, with far-reaching implications for material science and pharmaceutical development. The ability to solve this equation precisely would unlock a plethora of properties inherent to the microscopic systems being studied. However, the task of deriving exact wavefunctions for even moderately sized molecules is notoriously difficult, with no analytical solutions in general cases.

The advent of deep learning has significantly advanced the field of quantum chemistry, particularly through enhancements in the _Quantum Variational Monte Carlo (QVMC)_ method [1; 2; 3]. Enhanced by neural network-based approaches, commonly referred to as _neural ansatz_, methods like PauliNet  and FermiNet [5; 6] have demonstrated remarkable success. These approaches often match or surpass the accuracy of traditional "gold standard" methods such as CCSD(T)  even for complex molecules [8; 9]. This rapid development has spurred a broad spectrum of research into more accurate and efficient neural ansatz models, significantly impacting ab-initio quantum chemistry [10; 11; 12]. Recent reviews  provide comprehensive overviews of the advancements and diverse applications extending beyond molecular systems to areas like solid-state physics and electron gases [14; 15; 16].

Despite the accuracy and flexibility of Quantum Variational Monte Carlo (QVMC), optimizing it remains a challenging task, often requiring prolonged convergence times. Various methods have been developed to accelerate training, such as stochastic reconfiguration (SR) [17; 18; 19], Newton method , adaptive imaginary-time evolution , and Wasserstein Quantum Monte Carlo (WQMC) . In our work, we enhance optimization efficiency by employing the perspective of imaginary-time Schrodinger evolution [23; 24], which naturally guides the wavefunction toward the ground state over an extended time horizon. According to McLachlan's variational principle, it can be shown that this continuous-time process yields parametric updates analogous to those in standard QVMC with infinitesimal learning rates . However, while theoretically robust, implementing this evolution in practical settings is challenging with finite time steps. Traditional approaches approximate the updates within parametric space, but this method is limited by the non-convex nature of the objective and the unpredictability arising from complex theoretical properties. To overcome these challenges, we propose discretizing the evolution process itself, ensuring convergence to the ground state evenwith finite time steps. We then project the discretely evolved distribution back into parametric space, forming an update algorithm that iteratively refines the neural ansatz towards the ground state.

Diffusion Monte Carlo (DMC)  is a well known method in quantum chemistry that also employs ground state projection. Known for its promising results, DMC often surpasses the limitations of specific ansatz choices . However, as a non-parametric approach, DMC offers flexibility and computational efficiency but lacks the ability to provide explicit values of the wavefunction, which can be essential in applications. Additionally, DMC methods encounter the fixed-node approximation issue: their effectiveness depends heavily on the accuracy of a fixed trial wavefunction, which cannot be improved during the computation. By contrast, our approach maintains a parametric representation of the wavefunction that evolves continuously toward the ground state, effectively sidestepping the limitations posed by fixed-node constraints.

A few previous works have similarly focused on projecting an evolved quantum state onto the parametric manifold of an ansatz, as explored in . To the best of our knowledge, all existing approaches rely on conventional projection methods, specifically the quantum fidelity or the Fubini-Study metric. Although these metrics are widely used in physics, their mathematical properties are intricate and remain underexplored . Furthermore, none of these methods account for finite step size. In contrast, our approach takes advantage of the fact that wavefunction analysis is primarily conducted through the probability distribution (\(q||^{2}\)) derived from it. Accordingly, we project probability distributions using the Kullback-Leibler divergence, chosen for its mathematical simplicity and its ability to effectively capture distributional differences of interest. The introduction of a quadratic term naturally emerges from the squared nature of the wavefunction in the probability distribution, while the preconditioning by the Fisher information matrix arises naturally from the curvature of this projection.

Building on this framework, we introduce the _Quadratic Quantum Variational Monte Carlo (Q\({}^{2}\)VMC)_, an innovative optimization mechanism that enhances the conventional QVMC by allowing finite-time updates without additional computational overhead, as detailed in Algorithm 1. This novel approach not only maintains theoretical equivalence with QVMC under infinitesimally small time steps but also demonstrably achieves **twice the optimization speed / significantly better accuracy** within the same computational budget.

Results

In this section, we present a brief overview of the results achieved by the Quadratic Quantum Variational Monte Carlo (Q\({}^{2}\)VMC) method, demonstrating its enhanced efficiency and accuracy in wavefunction optimization. Our method achieves improvements in convergence speed and energy accuracy across various molecular systems. Technical details about the relevant experiments is written in the experiments section.

Summary of key resultsWe evaluated the performance of Q\({}^{2}\)VMC against traditional Quantum Variational Monte Carlo (QVMC) using state-of-the-art attention based neural network ansatzes: Psiformer  and LapNet . A total of six different molecules with diverse sizes are tested, with number of electrons ranging from 6 to 30 to demonstrate robustness. Each one of the 12 possible combinations are optimized with the default settings as in their original papers where possible and with (our method) or without (baseline reproduce) the quadratic modification. Our findings indicate that Q\({}^{2}\)VMC not only accelerates the convergence process but also reduces the variance in batch energies, suggesting a more stable approach towards reaching the ground state. These enhancements are highlighted as:

* **Faster Convergence:** As demonstrate by the training curves in Figure 1 Q\({}^{2}\)VMC shows a 2x speed-up in optimization comparing with the baselines, achieving the target energies in approximately half the iterations required by QVMC.
* **Enhanced Accuracy:** The energy accuracies obtained are consistently superior to those achieved by conventional QVMC, as detailed in Table of energy accuracies 8. This superiority is particularly pronounced in complex systems with a higher number of electrons, where the traditional methods struggle to maintain precision and stability.
* **Simple Integration and Hyperparameter Robustness:** As shown in Algorithm 1, Q\({}^{2}\)VMC can be seamlessly incorporated into existing frameworks with only a single line of code change. This section presents results obtained with the original hyperparameters, highlighting that effective performance gains are achievable without additional tuning efforts. For completeness, Appendix C provides results from experiments where hyperparameters were adjusted specifically for Q\({}^{2}\)VMC, showing that these tuned settings achieve comparable or superior performance to the Psiformer (Large) model using the traditional QVMC method, despite the latter's use of a network approximately four times larger than the Psiformer (Small) employed here.

## 3 Background

### Quantum Variational Monte Carlo (QVMC)

At the heart of quantum mechanics lies the _wavefunction_, which embodies all possible classical states of a system. When first quantization is considered, the wavefunction serves as a mapping from the states of particles to complex amplitudes. For instance, the state of a single electron can be represented by its position \(^{3}\) and spin \(\{,\}\). Consequently, the wavefunction of an \(N\)-electron system is a mapping \(:(^{3}\{,\})^{N}\), with the square of its magnitude, \(||^{2}\)

   System (Electrons) & Psiformer & Q\({}^{2}\)VMC+Psi & LapNet & Q\({}^{2}\)VMC+Lap \\  Li\({}_{2}\) (6) & -14.99486(1) & -14.99490(1) & -14.99485(1) & -14.99486(1) \\ NH\({}_{3}\) (10) & -56.56367(2) & -56.56374(2) & -56.56359(2) & -56.56370(2) \\ CO (14) & -113.32416(4) & -113.32442(2) & -113.32417(4) & -113.32428(2) \\ CH\({}_{3}\)NH\({}_{2}\) (18) & -95.86050(4) & -95.86073(2) & -95.86025(3) & -95.86053(2) \\ C\({}_{2}\)H\({}_{6}\)O (26) & -155.04656(7) & -155.04696(3) & -155.04563(6) & -155.04619(4) \\ C\({}_{4}\)H\({}_{6}\) (30) & -155.94619(8) & -155.94665(4) & -155.94528(4) & -155.94618(4) \\   

Table 1: Energies for a set of molecules studied in Psiformer  and LapNet . Reference energies are taken from the respective papers. In order to eliminate any potential effects from different evaluation strategies, we also report our reproduced baseline values in the appendix.

representing a probability density \(_{^{2}}=||^{2}/C\), where \(C=||^{2}\) is the normalization constant. The probability density \(_{^{2}}\) represents the likelihood of observing the quantum system in a specific state upon measurement. Note that when the normalization condition is not enforced, wavefunctions \(\) are invariant under scalar multiplication, implying \( a\) for any non-zero scalar \(a\), where all such functions correspond to the identical normalized probability density \(_{}\). With an abuse of notation, we simply write \(_{^{2}}=||^{2}\) when corresponding normalization is clear from the context.

The behavior of non-relativistic quantum systems are dictated by the Schrodinger equation, which, in its time-independent form, poses an eigenfunction problem \(=E\). Here, \(\) represents the Hermitian linear operator known as the Hamiltonian, and the eigenvalue \(E\) represents the energy associated with a specific eigenfunction. The Hamiltonian's structure is crucial, encapsulating the physical properties of the quantum system. In quantum chemistry, the Hamiltonian generally takes the form of:

\[=-_{}^{2}+V(),\] (1)

where \(_{}^{2}\) is the Laplacian in coordinate space, and \(V()\) represents a potential function dependent on the particle positions (e.g. configurations of the nucleus).

Quantum Variational Monte Carlo (QVMC) is a computational approach used to determine the ground state, corresponding to the lowest eigenvalue \(E_{0}\), of the Schrodinger equation. In the studies

Figure 1: Optimization curves for different molecules

of QVMC, two simplifications are commonly employed. First, given that \(\) is Hermitian, its eigenfunctions \(\) can be considered real-valued, permitting a focus solely on real-valued wavefunctions. Second, the absence of spin variables \(_{i}\) in the Hamiltonian allows for simplification in modeling the system, permitting us to fix the spins of electrons and shift our focus solely on their positional values. Hence, in QVMC, we utilize an _unnormalized_ wavefunction ansatz \(_{}:^{3N}\), parameterized by \(\). The term _neural ansatz_ denotes the representation of \(_{}\) by a neural network.

The quest for the ground state solution is guided by the Rayleigh-Ritz principle, which involves minimizing the loss function:

\[()=() _{}()}{|_{}( )|^{2}}=_{|_{}|^{2}}[ ^{-1}()_{}()}_{ =E_{L,}()}] E_{0},\] (2)

where \(E_{L,}()}{{=}}_{}^{-1 }()_{}()\) represents the _local energy_. The gradient of this loss function is computed as

\[_{}()=_{|_{}|^{2 }}[(E_{L}()-})_{} _{}^{2}()],\] (3)

with \(}=_{|^{2}|}[E_{L}()]\) denoting the average local energy. Minimizing \(\) with gradient descent thus yields an iterative process, where Markov Chain Monte Carlo sampling is employed to extract samples from distribution \(|_{}|^{2}\) and the samples are used estimate the energy gradient, which then directs the parameter updates .

To improve the efficiency of optimization, the _Stochastic Reconfiguration_[36; 17] or Quantum Natural Gradient Descent [37; 38]--has commonly been adopted for QVMC updates. This method enhances convergence by preconditioning the gradient with an (approximation of) Fisher information matrix \(()^{-1}\) related to the quantum state \(_{}^{2}\), which can be implemented efficiently using approximate natural gradient frameworks like KFAC . Consequently, the practical parameter update step, considering a learning rate \(\), is given as

\[_{}=()^{-1}_{ }().\] (4)

## 4 Quadratic Quantum Variational Monte Carlo

This section introduces our methodology for updating the neural ansatz through the Q\({}^{2}\)VMC approach. In Section 4.1, we present a discretized imaginary-time Schrodinger evolution. This process operates within the non-parametric Hilbert space of wavefunctions, guiding the system progressively toward the ground state. Subsequently, in section 4.2, we discuss how to project the evolved distributions back onto the parametric manifold of the neural ansatz by minimizing the Kullback-Leibler divergence between the evolved and updated distributions, which forms the basis of the Q\({}^{2}\)VMC algorithm.

### Imaginary-Time Schrodinger Evolution

Consider a Hilbert space equipped with the inner product \( u,v= uv\), and spanned by orthonormal basis functions \(\{_{i}()\}\). Given a Hermitian operator \(\), normalizing its eigenfunctions so that \(_{i},_{i}=1\), results in a basis that embodies three essential attributes: 1) they are eigenfunctions of the Hamiltonian with associated eigen-energies \(E_{i}\), 2) normalized, and 3) mutually orthogonal for distinct indices. These attributes ensure that any function within our Hilbert space can be precisely represented as linear combinations of these orthonormal basis functions associated with the Hamiltonian. The energies of these eigenfunctions are conventionally ordered as \(E_{0}<E_{1}<E_{2}<\). Thus, the primary objective of QVMC is to approximate \(_{0}\), with is associated to the lowest energy \(E_{0}\), using a parametric ansatz \(_{}\).

The imaginary-time Schrodinger equation emerges from the time-dependent Schrodinger equation by substituting \(t^{}\) with \(-it\), yielding:

\[-,t)}{ t}=(,t).\] (5)Considering an initial wavefunction at \(t=0\) as \((,t=0)=_{i=0}^{}_{i}_{i}\), the imaginary-time Schrodinger evolution has a closed-form solution expressed in terms of these basis functions:

\[(,t)=e^{-t}_{i=0}^{}_{i}_{i}()=_{i=0}^{}_{i}e^{-tE_{i}}_{i}().\]

Scaling the wavefunction by a factor of \(e^{tE_{0}}\) reveals the evolution's impact:

\[(,t)=_{0}()+_{i=1}^{}}{_{0}}e^{-t(E_{i}-E_{0})}_{i}().\] (6)

Given \(E_{i}-E_{0}>0\) for all \(i>0\), as \(t\), the wavefunction's projection onto any basis other than \(_{0}\) diminishes to zero. Hence, starting with any wavefunction that overlaps with \(_{0}\), the imaginary-time Schrodinger evolution consistently approximates the ground state as \(t\) approaches infinity.

While in theory, the operator \(e^{-t}\) as \(t\) can directly yield the ground state function, exact computation of this operator is impractical. One must discretize the time step to incrementally evolve the process. The evolution process can conveniently operate with discrete time in the following manner.

**Definition 4.1** (Discretization).: For a given time step \(\), the Discretized Imaginary-Time Schrodinger Evolution, corresponding to a Hamiltonian \(\) and its ground state energy \(E_{0},\) is described by a series of functions \(\{^{(n)}\}_{n=0}^{}\) such that:

\[^{(n+1)}=}{1- E_{0}}^{(n)}=^{ (n)}}{1- E_{0}}^{(n)},\] (7)

where \(E_{L}^{(n)}=^{(n)}/^{(n)}\).

This process is proven to converge to the ground state:

**Theorem 4.2** (Convergence).: _Assuming \(^{(0)},_{0} 0\) and \(\|^{(0)}\|_{2}<\), then \(^{(n)}\) weakly converges to \(_{0}\), up to a constant factor, as \(n\)._

_Remark 4.3_.: This result differs from its continuous time counterpart as shown in equation 6. The evolution process in our approach does not require an infinitesimal time step to converge, thus it is _insensitive_ to the size of the time step taken. Asymptotic convergence is guaranteed regardless of the time step size. Consequently, unlike previous methods, our approach does not necessitate the use of very small time steps, which can often impede effective convergence.

The evolution in the Hilbert space of wavefunctions may also be motivated from other perspectives, e.g gradient flow under Fisher-Rao metric  and the discrete evolution is similar to the _quantum power method_ in the computational quantum literature in the limit that \(\) (see, for example, [40; 41]). For a more comprehensive theoretical analysis, we direct readers to these sources.

### Parametric Projection of the Evolution Process

In practical applications, operating within the infinite-dimensional space of functions is not feasible. Instead, we utilize a neural ansatz \(_{}\) parameterized by a finite set of parameters \(\) to approximate the underlying functional. This necessitates an iterative process: a) evolving the current \(_{}\) following discrete evolution to produce \((1- E_{L})_{}\), b) projecting the evolved function back into the parametric space to update model parameters, resulting in \(_{+}\), and c) updating associated MCMC data samples based on this projection before repeating the process with the updated neural ansatz.

While step a) is straightforward and efficiently implementable (as detailed in Appendix A), step b) requires a suitable divergence metric for effective projection. In this study, we minimize the Kullback-Leibler (KL) divergence between the probability distribution induced by the evolved wavefunction and that represented by the updated neural ansatz within a trust region :

**Proposition 4.4**.: _Let \(h()\) denote the KL-divergence between the evolved distribution \((1- E_{L}())^{2}_{}^{2}()\) and the updated distribution \(_{+}^{2}\):_

\[h()=[(1- E_{L}())^{2}_{}^ {2}()\|_{+}^{2}()].\] (8)_Given the size of trust region \(\), our objective of projection is_

\[_{}^{*}=*{arg\,min}_{}\{h( )\ \ s.t.\ \ (_{+}^{2}\|_{}^{2}) ^{2}/2\}.\] (9)

_As \( 0^{+}\), the optimal update direction approaches to_

\[^{*}=_{ 0^{+}}_{ }^{*}=-g}{g^{}F^{-1}g},\] (10)

_where \(g\) and \(F\) are the gradient and Fisher information matrix respectively:_

\[g=[_{}_{}^{2}()]-([(1- E_{L}())^{2}])^{-1}[(1-  E_{L}())^{2}_{}_{}^{2}() ],\]

\[F=[(_{}_{}^{2}-[ _{}_{}^{2}])(_{} _{}^{2}-[_{}_{}^{2}] )^{}].\]

_represents the Fisher information matrix associated with the distribution induced by the neural ansatz. All expectations here are taken with respect to the distribution \(|_{}()|^{2}\)._

Proof.: Refer to Appendix B. 

Therefore, the update given by the projecting the evolution process, i.e. the Q\({}^{2}\)VMC update, is like:

\[_{^{2}$VMC}}  F^{-1}\] \[()^{-1}([(1- E_{L}( ))^{2}_{}_{}^{2}()]- [(1- E_{L}())^{2}][_{ }_{}^{2}()])\]

Because the mean of the scaling factors \([(1- E_{L}())^{2}]\) is subtracted from the update, the constant of 1 does not matter. Therefore, the form of update derived solely from the perspective of discretizing imaginary-time Schrodinger evolution and projection match closely with the update of QVMC upon choosing the time step \(=\) except for the quadratic term of \(^{2}E_{L}^{2}()\). Because the term \(E_{L}()\) has already been computed in the QVMC and the quadratic term can be added on with no effort, our method has no relative computational overhead. Notably by taking the infinitesimal time step limit \( 0\) will exactly recover the QVMC update as the additional term decays with \((^{2})\), thereby showing the consistency of our method in the small step size regime.

## 5 Experiments

This section details the experimental setup and evaluation strategy utilized to obtains the results shown above.

Methodology OverviewTwo recent cutting-edge, attention-based neural ansatzes, Psiformer  and LapNet , are tested in our evaluations. The architectural hyperparameters are delineated in Table 4 in Appendix. Note that  provides two possible model sizes, Psiformer Small and Psiformer Large with the latter roughly 4x size than the former, and our experiments are all conducted with the former. To demonstrate the easy integration and robustness of our method, we adhered to all the original training hyperparameters from their publications (detailed in Appendix Table 5). Training curves of baseline and horizontal lines representing reference energies from respective papers are plotted to facilitate comparison and indicate successful reproduction of the claimed performance. The only modification in our Q\({}^{2}\)VMC experiments pertains to the gradient coefficients, in accordance with the Q\({}^{2}\)VMC update rule 1.

Convergence and StabilityFigure 2 presents the energy convergence trajectories for six molecules, demonstrating that Q\({}^{2}\)VMC achieves both rapid and consistent convergence across a range of systems. Specifically, we tested on Li\({}_{2}\) (6), NH\({}_{3}\) (10), CO (14), methylamine-CH\({}_{3}\)NH\({}_{2}\) (18), ethanol-C\({}_{2}\)H\({}_{6}\)O (26), and bicyclobutane-C\({}_{4}\)H\({}_{6}\) (30), where the numbers in parentheses denote electron counts.

Training and Evaluation: Consistent with the methodologies as in the referenced studies, we optimize the models to 200,000 training iterations for all molecular systems. Nevertheless, it was observed that smaller molecules typically can reach convergence in fewer iterations e.g. Li\({}_{2}\) while larger systems like bicyclobutane has clearly not converged yet within the duration. We encourage future benchmarking in this field to select the total iterations adaptively. We adopt similar numerical hacks as in [8; 9] to facilitate numerical stability. Importantly, the local energies are clipped so that values will be within the range \(=5.0\) of mean absolute deviation from its median. The coefficients \(c^{(i)}\) are then further computed after clipping.

Following the training, an additional evaluation was conducted over 20,000 steps, employing MCMC to sample batches of data without updating the network parameters. The computed energies for the tested molecules, comparing against benchmark values, are tabulated in Table 1. For smaller molecules like Li\({}_{2}\), the energy performance gains were marginal, highlighting the system-dependent aspect of convergence energy. However, our approach facilitated consistently faster convergence across the board. For larger molecules, which do not reach convergence within the allocated iterations, Q\({}^{2}\)VMC markedly improved energy performance.

### Ablation Study

The update of Q\({}^{2}\)VMC can be decomposed into two parts:

\[_{^{2}$VMC}}=_{}+ ^{2}[(E_{L}^{2}()-^{2}}) _{}^{2}()]\] (11)

Therefore, one might suspect that if the better performance of Q\({}^{2}\)VMC comes solely from its larger update magnitude \(\|_{^{2}$VMC}}\|\|_{}\|\) and we can make better performance of QVMC by utilizing a larger learning rate. Note that the greater relation cannot be confirmed as the terms being added are not in the same direction. In this section, we confirm that the performance of QVMC can indeed be boosted by carefully tuning for a larger learning rate within a specific system. However, one cannot make QVMC perform better than Q\({}^{2}\)VMC solely by tuning the learning rate.

We primarily study the system of NH\({}_{3}\)(10) as it is large enough to allow different algorithms distinguish while not so large to allow objectives to converge within the 200k steps duration. All experiments in this section are done with the Posiformer model. We take a fine-grained tuning of the learning rate \(_{0}\) of QVMC within the range of \(\{0.01,\ 0.02,\ 0.05,\ 0.1,\ 0.2,\ 0.5\}\). The strategy of training and evaluation follows the experiments. The convergence energies are listed in Table 2 and the training curves can be found in Appendix C.

It is clear from the results that with larger learning rates for QVMC, one can yield better convergence energy values, while setting it to over-large values will make the training diverge, even if the gradient clipping in terms of the Fisher norms are enabled . Among the trainings of different learning rates, the best performance is obtained from the one using learning rate of \(_{0}=0.2\). We therefore use the matched learning rate for training with Q\({}^{2}\)VMC to see if it can still do better than this. The convergence energy values are listed in Table 3.

   \(_{0}\) & 0.01 & 0.02 & 0.05 \\  \(E_{}\) & -56.56327(3) & -56.56350(2) & -56.56366(2) \\  \(_{0}\) & 0.1 & 0.2 & 0.5 \\  \(E_{}\) & -56.56372 (1) & -56.56379(1) & Diverge \\   

Table 2: Convergence energies of NH\({}_{3}\) trained with QVMC using different \(_{0}\).

   \(_{0}\) & 0.05 & 0.2 \\  \(E_{}\) & -56.56374(2) & -56.56384(1) \\   

Table 3: Convergence energies of NH\({}_{3}\) trained with Q\({}^{2}\)VMC using different \(_{0}\).

Our method is already performing well enough even with the default learning rate of 0.05 not tuned specifically for the system of NH\({}_{3}\). Furthermore, since experiments have already shown that the performance of Q\({}^{2}\)VMC is superior to any trials obtained from optimizing the objective using QVMC, there is no need to further tune the learning rate of Q\({}^{2}\)VMC for comparison.

Following the heuristics recommended by , we further ablated additional hyperparameters, including increased decay time, reduced learning rate, and reduced norm constraints. The results, summarized in Table 4, specify the modified hyperparameters alongside the achieved ground state energies upon convergence. As shown, none of these adjustments matched or exceeded the accuracy attained by Q\({}^{2}\)VMC.

### Code and Computational Details

All models were implemented using the JAX framework , which is available under the Apache-2.0 License. The architectures were adapted from public implementations of FermiNet  and LapNet , both of which are also distributed under the Apache-2.0 License. Modifications were made to these architectures to integrate the Q\({}^{2}\)VMC algorithm. Natural gradient updates were based on KFAC-JAX , adhering to the same licensing terms. For the LapNet experiments, training was conducted on four Nvidia GeForce 3090 GPUs, utilizing standard single precision calculations and double-precision for matrix multiplications, with training durations ranging from 5 to 90 clock hours depending on the size of the molecule. Similarly, Psiformer experiments were performed in single precision on four Nvidia V100 GPUs, with each run varying from 8 to 140 clock hours.

## 6 Conclusions

In this study, we introduced the Quadratic Quantum Variational Monte Carlo (Q\({}^{2}\)VMC), which optimizes neural ansatz in quantum variation Monte Carlo by evolving wavefunctions towards the ground state in non-parametric space, then projecting these onto the neural network's parametric manifold using KL divergence minimization. Our experiments demonstrate that Q\({}^{2}\)VMC not only strengthens the theoretical foundation but also significantly surpasses traditional QVMC updates in speed and accuracy.

Limitations and Future WorksWhile Q\({}^{2}\)VMC demonstrates clear advantages, it also faces several unresolved challenges, such as determining the optimal imaginary time step and quantifying the inaccuracies introduced by approximate projection methods. Due to current computational constraints, our experiments were limited to systems with up to 30 electrons. Similarly, these limitations prevented us from conducting a complete set of experiments on other important quantum chemistry applications, such as relative energies [47; 48] and excited states [49; 50; 51]. In future work, we aim to extend

   Learning Rate & Decay Time & Norm Constraint & Energy \\   &  & 3e-3 & -56.56349(3) \\  & & 1e-2 & -56.56366(2) \\  & & 3e-2 & Diverge \\   &  & 3e-3 & -56.56369(1) \\  & & 1e-2 & -56.56366(1) \\  & & 3e-2 & Diverge \\   &  & 3e-3 & -56.56371(2) \\  & & 1e-2 & -56.56313(3) \\  & & 3e-2 & Diverge \\    &  & 3e-3 & -56.56374(1) \\  & & 1e-2 & -56.56344(2) \\   & & 3e-2 & Diverge \\   

Table 4: More experiments for ablation study: Computed ground state energies of NH\({}_{3}\) molecules with standard quantum Monte Carlo method. Tested with different (reduced) learning rates, (increased) learning rate decay times, and (increased) norm constraints.

Q\({}^{2}\)VMC to these domains to further assess its performance. Additionally, our method currently achieves only a constant factor speed-up, as observed in the experiments, but the fundamental \((N^{4})\) scaling with the number of electrons remains a bottleneck, restricting its application to very large systems. We hope to address this scaling issue to enable testing on larger molecules.

Broader ImpactsBroader impacts of this work could influence computational chemistry, potentially reducing the reliance on physical experiments and accelerating the discovery of new drugs and environmentally friendly chemical processes, while adhering to stringent ethical standards.

## 7 Acknowledgement

The authors thank the four anonymous reviewers for their invaluable discussions and insightful feedback. Their suggestions regarding limitations and challenges were instrumental in shaping improvements to our paper and inspiring directions for future research. The research is conducted in Statistics & AI group at UT Austin, which receives supports in part from NSF CAREER1846421, SenSE2037267, Office of Navy Research, and NSF AI Institute for Foundations of Machine Learning (IFML).