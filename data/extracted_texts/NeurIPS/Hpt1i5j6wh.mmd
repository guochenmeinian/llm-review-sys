# Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars

Simon Schrodi\({}^{1}\) Danny Stoll\({}^{1}\) Binxin Ru\({}^{2}\)

**Rhea Sanjay Sukthanker\({}^{1}\) Thomas Brox\({}^{1}\) Frank Hutter\({}^{1}\)**

\({}^{1}\)University of Freiburg \({}^{2}\)University of Oxford

{schrodi,stolld,sukthank,brox,fh}@cs.uni-freiburg.de robin@robots.ox.ac.uk

###### Abstract

The discovery of neural architectures from simple building blocks is a long-standing goal of Neural Architecture Search (NAS). Hierarchical search spaces are a promising step towards this goal but lack a unifying search space design framework and typically only search over some limited aspect of architectures. In this work, we introduce a unifying search space design framework based on context-free grammars that can naturally and compactly generate expressive hierarchical search spaces that are 100s of orders of magnitude larger than common spaces from the literature. By enhancing and using their properties, we effectively enable search over the complete architecture and can foster regularity. Further, we propose an efficient hierarchical kernel design for a Bayesian Optimization search strategy to efficiently search over such huge spaces. We demonstrate the versatility of our search space design framework and show that our search strategy can be superior to existing NAS approaches. Code is available at https://github.com/automl/hierarchical_nas_construction.

## 1 Introduction

Neural Architecture Search (NAS) aims to automatically discover neural architectures with state-of-the-art performance. While numerous NAS papers have already demonstrated finding state-of-the-art architectures (prominently, e.g., Tan and Le , Liu et al. ), relatively little attention has been paid to understanding the impact of architectural design decisions on performance, such as the repetition of the same building blocks. Moreover, despite the fact that NAS is a heavily-researched field with over \(1\,000\) papers in the last two years , NAS has primarily been applied to over-engineered, restrictive search spaces (e.g., cell-based ones) that did not give rise to _truly novel_ architectural patterns. In fact, Yang et al.  showed that in the prominent DARTS search space  the manually-defined macro architecture is more important than the searched cells, while Xie et al.  and Ru et al.  achieved competitive performance with randomly wired neural architectures that do not adhere to common search space limitations.

Hierarchical search spaces are a promising step towards overcoming these limitations, while keeping the search space of architectures more controllable compared to global, unrestricted search spaces. However, previous works limited themselves to search only for a hierarchical cell , linear macro topologies , or required post-hoc checking and adjustment of architectures . This limited their applicability in understanding the impact of architectural design choices on performance as well as search over all (abstraction) levels of neural architectures.

In this work, we propose a _functional_ view of neural architectures and a unifying search space design framework for the efficient construction of (hierarchical) search spaces based on _Context-Free Grammars (CFGs)_. We compose architectures from simple multivariate functions in a hierarchical mannerusing the recursive nature of CFGs and consequently obtain a function composition representing the architecture. Further, we enhance CFGs with mechanisms to efficiently define search spaces over the complete architecture with non-linear macro topology (e.g., see Figure 4 or Figure 10 in Appendix J for examples), foster _regularity_ by exploiting that context-free languages are closed under substitution, and demonstrate how to integrate user-defined search space _constraints_.

However, since the number of architectures scales exponentially in the number of hierarchical levels - leading to search spaces 100s of orders of magnitude larger than commonly used ones in NAS - for many prior approaches search becomes either infeasible (e.g., DARTS ) or challenging (e.g., regularized evolution ). As a remedy, we propose Bayesian Optimization for Hierarchical Neural Architecture Search (BOHNAS), which constructs a hierarchical kernel upon various granularities of the architectures, and show its efficiency through extensive experimental evaluation.

**Our contributions** We summarize our key contributions below:

1. We propose a _unifying search space design framework_ for (hierarchical) NAS based on CFGs that enables us to search across the _complete_ architecture, i.e., from micro to macro, _foster regularity_, i.e., repetition of architectural patterns, and incorporate user-defined _constraints_ (Section 3). We demonstrate its _versatility_ in Sections 4 and 6.
2. We propose a _hierarchical extension of NAS-Bench-201_ (as well as derivatives) to allow us to study search over various aspects of the architecture, e.g., the macro architecture (Section 4).
3. We propose _BOHNAS_ to efficiently search over large hierarchical search spaces (Section 5).
4. We thoroughly show how our search space design framework can be used to study the impact of architectural design principles on performance across granularities.
5. We show the superiority of BOHNAS over common baselines on 6/8 (others on par) datasets above state-of-the-art methods, using the same training protocol, including, e.g., a \(4.99\,\%\) improvement on ImageNet-16-120 using the NAS-Bench-201 training protocol. Further, we show that we can effectively search on different types of search spaces (convolutional networks, transformers, or both) (Section 6).
6. We adhere to the NAS best practice checklist  and provide code at https://github.com/automl/hierarchical_nas_construction to foster _reproducible NAS research_ (Appendix N).

## 2 Related work

We discuss related works in Neural Architecture Search (NAS) below and discuss works beyond NAS in Appendix B. Table 4 in Appendix B summarizes the differences between our proposed search space design based on CFGs and previous works.

Most previous works focused on global [17; 18], hyperparameter-based , chain-structured [19; 20; 21; 22], or cell-based  search space designs. Hierarchical search spaces subsume the aforementioned spaces while being more expressive and effective in reducing search complexity. Prior works considered \(n\)-level hierarchical assembly [2; 9; 24], parameterization of a hierarchy of random

Figure 1: Derivation of the function composition of the neural architecture from Equation 1 (left). Note that the derivations correspond to edge replacements [14; 15; 16] in the computational graph representation (right). The intermediate derivations provide various granularities of a neural architecture. Appendix A provides the vocabulary of primitive computations and topological operators.

graph generators , or evolution of topologies and repetitive blocks . In contrast to these prior works, we search over all (abstraction) levels of neural architectures and do not require any post-generation testing and/or adaptation of the architecture [8; 11]. Further, we can incorporate user-defined constraints and foster regularity in the search space design.

Other works used formal "systems": string rewriting systems [25; 26], cellular (or tree-structured) encoding schemes [27; 28; 29; 30], hyperedge replacement graph grammars [31; 32], attribute grammars , CFGs [34; 35; 36; 37; 38; 39; 40], And-Or-grammars , or a search space design language . Different to these prior works, we search over the complete architecture with non-linear macro topologies, can incorporate user-defined constraints, and explicitly foster regularity.

For search, previous works, e.g., used reinforcement learning [17; 43], evolution , gradient descent [6; 45; 46], or Bayesian Optimization (BO) [18; 47; 48]. To enable the effective use of BO on graph-like inputs for NAS, previous works have proposed to use a GP with specialized kernels [18; 49; 48], encoding schemes [47; 50], or graph neural networks as surrogate model [51; 52; 53]. In contrast to these previous works, we explicitly leverage the hierarchical nature of architectures in the performance modeling of the surrogate model.

## 3 Unifying search space design framework

To efficiently construct expressive (hierarchical) NAS spaces, we review two common neural architecture representations and describe how they are connected (Section 3.1). In Sections 3.2 and 3.3 we propose to use CFGs and enhance them (or exploit their properties) to efficiently construct expressive (hierarchical) search spaces.

### Neural architecture representations

Representation as computational graphsA neural architecture can be represented as an edge-attributed (or equivalently node-attributed) directed acyclic (computational) graph \(G(V,E,o_{e},m_{v})\), where \(o_{e}\) is a primitive computation (e.g., convolution) or a computational graph itself (e.g., residual block) applied at edge \(e E\), and \(m_{v}\) is a merging operation for incident edges at node \(v V\). Below is an example of an architecture with two residual blocks followed by a linear layer:

\[[scale=0.4]{fig/conv}[scale=0.4]{fig/conv} [scale=0.4]{fig/conv}[scale=0.4]{fig/conv} [scale=0.

### Construction based on context-free grammars

To construct neural architectures as compositions of (multivariate) functions, we propose to use _Context-Free Grammars (CFGs)_ since they naturally and in a formally grounded way generate (expressive) languages that are (hierarchically) composed from an alphabet, i.e., the set of (multivariate) functions. CFGs also provide a simple and formally grounded mechanism to evolve architectures that ensure that evolved architectures stay within the defined search space (see Section 5 for details). With our enhancements of CFGs (Section 3.3), we provide a unifying search space design framework that is able to _represent all search spaces_ we are aware of from the literature. Appendix F provides examples for NAS-Bench-101 , DARTS , Auto-DeepLab , hierarchical cell space , Mobile-net space , and hierarchical random graph generator space .

Formally, a CFG \(= N,,P,S\) consists of a finite set of nonterminals \(N\) and terminals \(\) (i.e., the alphabet of functions) with \(N=\), a finite set of production rules \(P=\{A|A N,(N)^{*}\}\), where the asterisk denotes the Kleene star , and a start symbol \(S N\). To generate a string (i.e., the function composition), starting from the start symbol \(S\), we recursively replace nonterminals with the right-hand side of a production rule, until the resulting string does not contain any nonterminals. For example, consider the following CFG in extended Backus-Naur form  (refer to Appendix C for background on the extended Backus-Naur form):

\[\ ::=\ (,\ ,\ )\ \ (,\ ,\ )\ \ \ \ \ \ \ \ \ \ \.\] (3)

Figure 1 shows how we can derive the function composition of the neural architecture from Equation 1 from this CFG and makes the connection to its computational graph representation explicit. The set of all (potentially infinite) function compositions generated by a CFG \(\) is the language \(L()\), which naturally forms our search space. Thus, the NAS problem can be formulated as follows:

\[*{arg\,min}_{ L()}\ ((),\ )\ \ \,\] (4)

where \(\) is an error measure that we seek to minimize for some data \(\), e.g., final validation error of a fixed training protocol.

### Enhancements

Below, we enhance CFGs or utilize their properties to efficiently model changes in the spatial resolution, foster regularity, and incorporate constraints.

Flexible spatial resolution flowNeural architectures commonly build a hierarchy of features that are gradually downsampled, e.g., by pooling operations. However, many NAS works do not search over the macro topology of architectures (e.g., Zoph et al. ), only consider linear macro topologies (e.g., Liu et al. ), or require post-generation testing for resolution mismatches with an adjustment scheme (e.g., Ru et al. , Miikkulainen et al. ).

To overcome these limitations, we propose a simple mechanism to search over the macro topology with flexible spatial resolution flow by _overloading nonterminals_: We assign to each nonterminal the number of downsampling operations required in its subsequent derivations. This effectively distributes the downsampling operations recursively across the architecture.

For example, the nonterminals \(,\) of the production rule \((,\ ,\ )\) indicate that 1 or 2 downsampling operations must be applied in their subsequent derivations, respectively. Thus, the input features of the residual topological operator \(\) will be downsampled twice in both of its paths and, consequently, the merging paths will have the same spatial resolution. Appendix D provides an example that also makes the connection to the computational graph explicit.

Fostering regularity through substitutionTo foster regularity, i.e., reuse of architectural patterns, we implement intermediate variables (Section 3.1) by exploiting the property that context-free languages are closed under _substitution_. More specifically, we can substitute an intermediate variable \(x_{i}\) with a string \(\) of another language \(L^{}\), e.g., constructing cell topologies. By substituting the same intermediate variable multiple times, we reuse the same architectural pattern (\(\)) and, thereby, effectively foster regularity. Note that the language \(L^{}\) may in turn have its own intermediate variables that map to languages constructing other architectural patterns, e.g., activation functions.

For example, consider the languages \(L(_{m})\) and \(L(_{c})\) constructing the macro or cell topology of a neural architecture, respectively. Further, we add a single intermediate variable \(x_{1}\) to the terminals \(_{_{m}}\) that map to the string \(_{1} L(_{c})\), e.g., the searchable cell. Thus, after substituting all \(x_{1}\) with \(_{1}\), we effectively share the same cell topology across the macro topology of the architecture.

ConstraintsWhen designing a search space, we often want to adhere to constraints. For example, we may only want to have two incident edges per node - as in the DARTS search space  - or ensure that for every neural architecture the input is associated with its output. Note that such constraints implicate _context-sensitivity_ but CFGs by design are context-free. Thus, to still allow for constraints, we extend the sampling and evolution procedures of CFGs by using a (one-step) _lookahead_ to ensure that the next step(s) in sampling procedure (or evolution) does not violate the constraint. We provide more details and a comprehensive example in Appendix E.

## 4 Example: Hierarchical NAS-Bench-201 and its derivatives

In this section, we propose a hierarchical extension to NAS-Bench-201 : _hierarchical NAS-Bench-201_ that subsumes (cell-based) NAS-Bench-201  and additionally includes a search over the _macro topology_ as well as the _parameterization of the convolutional blocks_, i.e., type of convolution, activation, and normalization. Below, is the definition of our proposed hierarchical NAS-Bench-201:

\[\ ::=\ \ |\ \ |\ \\ \ ::=\ \ |\ \ |\ \\ \ ::=\ \ |\ \ |\ \\ \ ::=\ \ |\ \ |\ \\ \ ::=\ \ |\ \ |\ \\ \ ::=\ \\ \ ::=\ \ |\ \ |\ \ |\ \\ \ =\\ \ ::=\ \ |\ \ |\ \\ \ ::=\ \ |\ \ |\ \\ \ ::=\ \ |\ \ |\ \.\] (5)

The blue productions of the nonterminals \(\{,,,,\}\) construct the (non-linear) macro topology with flexible spatial resolution flow, possibly containing multiple branches. The red and yellow productions of the nonterminals \(\{,\}\) construct the NAS-Bench-201 cell and \(\{,,,\}\) parameterize the convolutional block. Note that the red productions correspond to the original NAS-Bench-201 cell-based (sub)space . Appendix A provides the vocabulary of topological operators and primitive computations and Appendix D provides a comprehensive example on the construction of the macro topology.

We omit the stem (i.e., 3x3 convolution followed by batch normalization) and classifier head (i.e., batch normalization followed by ReLU, global average pooling, and linear layer) for simplicity. We used element-wise summation as merge operation. For the number of channels, we adopted the common design to double the number of channels whenever we halve the spatial resolution. Alternatively, we could handle a varying number of channels by using, e.g., depthwise concatenation as merge operation; thereby also subsuming NATS-Bench . Finally, we added a constraint to ensure that the input is associated with the output since zero could disassociate the input from the output.

Search space capacityThe search space consists of ca. \(}\) architectures (Appendix G describes how to compute the search space size), which is hundreds of orders of magnitude larger than other popular (finite) search spaces from the literature, e.g., the NAS-Bench-201 or DARTS search spaces only entail ca. \(1.5 10^{4}\) and \(10^{18}\) architectures, respectively.

DerivativesWe can derive several variants of our hierarchical NAS-Bench-201 search space (hierarchical). This allows us to investigate search space as well as architectural design principles in Section 6. We briefly sketch them below and refer for their formal definitions to Appendix J.2:

* fixed+shared (cell-based): Fixed macro topology (only leftmost blue productions) with the single, shared NB-201 cell (red productions); equivalent to NAS-Bench-201 .

* hierarchical+shared: Hierarchical macro search (blue productions) with a single, shared cell (red & yellow productions).
* hierarchical+non-linear: Hierarchical macro search with more non-linear macro topologies (i.e., some Sequential are replaced by Diamond topological operators), allowing for more branching at the macro-level of architectures.
* hierarchical+shared+non-linear: Hierarchical macro search with more non-linear macro topologies (more branching at the macro-level) with a single, shared cell.

## 5 Bayesian Optimization for hierarchical NAS

Expressive (hierarchical) search spaces present challenges for NAS search strategies due to their huge size. In particular, gradient-based methods (without any yet unknown novel adoption) do not scale to expressive hierarchical search spaces since the supernet would yield an exponential number of weights (Appendix G provides an extensive discussion). Reinforcement learning approaches would also necessitate a different controller network. Further, we found that evolutionary and zero-cost methods did not perform particularly well on these search spaces (Section 6). Most Bayesian Optimization (BO) methods also did not work well [18; 52] or were not applicable (adjacency  or path encoding ), except for NASBOWL  with its Weisfeiler-Lehman (WL) graph kernel design in the surrogate model.

Thus, we propose the novel BO strategy Bayesian Optimization for Hierarchical Neural Architecture Search (BOHNAS), which (i) uses a novel _hierarchical kernel_ that constructs a kernel upon different granularities of the architectures to improve performance modeling, and (ii) adopts ideas from grammar-guided genetic programming [60; 61] for acquisition function optimization of the discrete space of architectures. Below, we describe these components and provide more details in Appendix H.

Hierarchical Weisfeiler-Lehman kernel (hWL)We adopt the WL graph kernel design [48; 62] for performance modeling.However, modeling solely based on the final computational graph of the architecture, similar to Ru et al. , ignores the useful hierarchical information inherent in our construction (Section 3). Moreover, the large size of the architectures also makes it difficult to use a single WL kernel to capture the more global topological patterns.

As a remedy, we propose the _hierarchical WL kernel (hWL)_ that hierarchically constructs a kernel upon various granularities of the architectures. It efficiently captures the information in all hierarchical levels, which substantially improves search and surrogate regression performance (Section 6). To compute the kernel, we introduce fold operators \(F_{l}\) that remove all substrings (i.e., inner functions) beyond the \(l\)-th hierarchical level, yielding partial function compositions (i.e., granularities of the architecture). E.g., the folds \(F_{1}\), \(F_{2}\) and \(F_{3}\) for the function composition \(\) from Equation 1 are:

\[F_{3}() =((,,),(,,),)\;,\] \[F_{2}() =(,, ),\] (6) \[F_{1}() =.\]

Note that \(F_{3}()=\) and observe the similarity to the derivations in Figure 1. We define hWL for two function compositions \(_{i}\), \(_{j}\) constructed over \(L\) hierarchical levels, as follows:

\[k_{hWL}(_{i},_{j})=_{l=2}^{L}_{l} k_{ WL}((F_{l}(_{i})),(F_{l}(_{j})))\;,\] (7)

where \(\) bijectively maps the function compositions to their computational graphs. The weights \(_{l}\) govern the importance of the learned graph information at different hierarchical levels \(l\) (granularities of the architecture) and can be optimized (along with other GP hyperparameters) by maximizing the marginal likelihood. We omit the fold \(F_{1}()\) since it does not contain any edge features. Appendix H.2 provides more details on our proposed hWL.

Grammar-guided acquisition function optimizationDue to the discrete nature of the function compositions, we adopt ideas from grammar-based genetic programming [60; 61] for acquisition function optimization. For mutation, we randomly replace a substring (i.e., part of the function composition) with a new, randomly generated string with the same nonterminal as start symbol. For crossover, we randomly swap two substrings produced by the same nonterminal as start symbol. We consider two crossover operators: a novel _self-crossover_ operation swaps two substrings of the _same_ string, and the common crossover operation swaps substrings of two different strings. Importantly, all evolutionary operations by design only result in valid function compositions (architectures) of the generated language. We provide visual examples for the evolutionary operations in Appendix H.

In our experiments, we used expected improvement as acquisition function and the Kriging Believer  to make use of parallel compute resources to reduce wallclock time. The Kriging Believer hallucinates function evaluations of pending evaluations at each iteration to avoid redundant evaluations.

## 6 Experiments

In the section, we show the versatility of our search space design framework, show how we can study the impact of architectural design choices on performance, and show the search efficiency of our search strategy BOHNAS by answering the following research questions:

* How does our search strategy BOHNAS compare to other search strategies?
* How important is the incorporation of hierarchical information in the kernel design?
* How well do zero-cost proxies perform in large hierarchical search spaces?
* Can we find well-performing transformer architectures for, e.g., language modeling?
* Can we discover novel architectural patterns (e.g., activation functions) from scratch?
* Can we find better-performing architectures in huge hierarchical search spaces with a limited number of evaluations, despite search being more complex than, e.g., in cell-based spaces?
* How does the popular uniformity architecture design principle, i.e., repetition of similar architectural patterns, affect performance?
* Can we improve performance further by allowing for more non-linear macro architectures while still employing the uniformity architecture design principle?

To answer these questions, we conducted extensive experiments on the hierarchical NAS-Bench-201 (Section 4), activation function , DARTS , and newly designed transformer-based search spaces (Appendix M.2). The activation function search shows the versatility of our search space design framework, where we search not for architectures, but for the composition of simple mathematical

   Dataset & C10 & C100 & IM16-120 & CTile & AddNIST & C10 & C10 & IM \\ Training prot. & NB201\({}^{}\) & NB201\({}^{}\) & NB201\({}^{}\) & NB201\({}^{}\) & NB201\({}^{}\) & Act. func. & DARTS & DARTS (transfer) \\  Previous best & 5.63 & 26.51 & 53.15 & 35.75 & 7.4 & **8.32** & **2.65\({}^{*}\)** & 24.63\({}^{*}\) \\ BOHNAS & **5.02** & **25.41** & **48.16** & **30.33** & **4.57** & **8.31** & **2.68** & **24.48** \\  Difference & +0.51 & +1.1 & +4.99 & +5.42 & +2.83 & +0.01 & -0.03 & +0.15 \\    \({}^{}\) includes hierarchical search space variants. \({}^{*}\) reproduced results using the reported genotype.

Table 1: Test errors [%] of the best found architectures of BOHNAS (ours) compared with the respective best methods from the literature using the same training protocol. Table 6 (Appendix J.3), Table 2, and Tables 9 and 10 (Appendix L.3) provide the full tables for the NAS-Bench-201, activation function search, or DARTS training protocols, respectively.

Figure 2: Mean \(\) standard error on the hierarchical NAS-Bench-201 search space. Appendix J.3 provides further results and extensive analyses.

functions. The search on the DARTS search space shows that BOHNAS is also backwards-compatible with cell-based spaces and achieves on-par or even superior performance to state-of-the-art gradient-based methods; even though these methods are over-optimized for the DARTS search space due to successive works targeting the same space. Search on the transformer space further shows that our search space design framework is also capable to cover the ubiquitous transformer architecture. To promote reproducibility, we discuss adherence to the NAS research checklist in Appendix N. We provide supplementary results (and ablations) in Appendices J.3, K.3, L.3 and M.3.

### Evaluation details

We search for a total of 100 or 1000 evaluations with a random initial design of 10 or 50 architectures on three seeds {777, 888, 999} or one seed {777} on the hierarchical NAS-Bench-201 or activation function search space, respectively. We followed the training protocols and experimental setups of Dong and Yang  or Ramachandran et al. . For search on the DARTS space, we search for one day with a random initial design of 10 on four seeds {666, 777, 888, 999} and followed the training protocol and experimental setup of Liu et al. , Chen et al. . For searches on transformer-based spaces, we ran experiments each for one day with a random initial design of 10 on one seed {777}. In each evaluation, we fully trained the architectures or activation functions (using ResNet-20) and recorded the final validation error. We provide full training details and the experimental setups for each space in Appendices J.1, K.2, L.2 and M.1. We picked the best architecture, activation function, DARTS cells, or transformer architectures based on the final validation error (for NAS-Bench-201, activation function, and transformer-based search experiments) or on the average of the five last validation errors (for DARTS). All search experiments used 8 asynchronous workers, each with a single NVIDIA RTX 2080 Ti GPU.

We chose the search strategies Random Search (RS), Regularized Evolution (RE) , AREA , NASBOT , and NASBOWL  as baselines. Note that we could not use gradient-based approaches for our experiments on the hierarchical NAS-Bench-201 search space since they do not scale to large hierarchical search spaces without any novel adoption (see Appendix G for a discussion). Also note that we could not apply AREA to the activation function search since it uses binary activation codes of ReLU as zero-cost proxy. Appendix I provides the implementation details of the search strategies.

### Results

In the following we answer all of the questions **RQ1-RQ8**. Figure 2 shows that BOHNAS finds superior architectures on the hierarchical NAS-Bench-201 search space compared to common baselines (answering **RQ1**); including NASBOWL, which does not use hierarchical information in its kernel design (partly answering **RQ2**), and zero-cost-based search strategy AREA (partly answering **RQ3**). We further investigated the hierarchical kernel design in Figure 12 in Appendix J.3 and found that it substantially improves regression performance of the surrogate model, especially on smaller amounts of training data (further answering **RQ2**). We also investigated other zero-cost proxies but they were mostly inferior to the simple baselines l2-norm or flops (Table 7 in Appendix J.3, further answering **RQ3**). We also found that BOHNAS is backwards compatible with cell-based search spaces. Table 1 shows that we found DARTS cells that are on-par on CIFAR-10 and (slightly) superior on ImageNet (with search on CIFAR-10) to state-of-the-art gradient-based methods; even though those are over-optimized due to successive works targeting this particular space.

Figure 3: Mean \( 1\) standard error for several architecture search space designs (see Section 4 and Appendix J.2 for their CFGs) using BOHNAS for search.

Our searches on the transformer-based search spaces show that we can indeed find well-performing transformer architectures for, e.g., language modeling or sentiment analysis (**RQ4**). Specifically, we found a transformer for generative language modeling that achieved a best validation loss of _1.4386_ with only \(3.33\,\) parameters. For comparison, Karpathy  reported a best validation loss of _1.4697_ with a transformer with \(10.65\,\) parameters. Note that as we also searched for the embedding dimensionality, that acts globally on the architecture, we combined our hierarchical graph kernel (hWL) with a Hamming kernel. This demonstrates that our search space design framework and search strategy, BOHNAS, are applicable not only to NAS but also to joint NAS and Hyperparameter Optimization (HPO). The found classifier head for sentiment analysis achieved a test accuracy of \(92.26\,\%\). We depict the found transformer and classifier head in Appendix M.3.

The experiments on activation function search (see Table 2) show that our search space design framework as well as BOHNAS can be effectively used to search for architectural patterns from even more primitive, mathematical operations (answering **RQ1 & RQ5**). Unsurprisingly, in this case, NASBOWL is on par with BOHNAS, since the computational graphs of the activation functions are small. This result motivates further steps in searching in expressive search spaces from even more atomic primitive computations.

To study the impact of the search space design and, thus, the architectural design choices on performance (**RQ6-RQ8**), we used various derivatives of our proposed hierarchical NAS-Bench-201 search space (see Section 4 and Appendix J.2 for the formal definitions of these search spaces). Figure 3 shows that we can indeed find better-performing architectures in hierarchical search spaces compared to simpler cell-based spaces, even though search is more complex due to the substantially larger search space (answering **RQ6**). Further, we find that the popular architectural uniformity design principle of reusing the same shared building block across the architecture (search spaces with keyword shared in Figure 3) improves performance (answering **RQ7**). This aligns well with the research in (manual and automated) architecture engineering over the last decade. However, we also found that adding more nonlinear macro topologies (search spaces with keyword non-linear in Figure 3), thereby allowing for more branching at the macro-level of architectures, surprisingly further improves performance (answering **RQ8**). This is in contrast to the common linear macro architectural design without higher degree of branching at the macro-level; except for few notable exceptions .

While most previous works in NAS focused on cell-based search spaces, Table 1 reveals that our found architectures are superior in 6/8 cases (others are on par) to the architectures found by previous works. On the NAS-Bench-201 training protocol, our best found architectures on CIFAR-10, CIFAR-100, and ImageNet-16-120 (depicted in Figure 4) reduce the test error by \(0.51\,\%\), \(1.1\,\%\), and \(4.99\,\%\) to the best reported numbers from the literature, respectively. Notably, this is even better than the _optimal_ cells in the cell-based NAS-Bench-201 search space (further answering **RQ6**). This clearly emphasizes the potential of NAS going beyond cell-based search spaces (**RQ6**) and prompts us to rethink macro architectural design choices (**RQ7**, **RQ8**).

   Search strategy & Test error [\%] \\  ReLU & 8.93 \\ Swish & 8.61 \\  RS & 8.91 \\ RE & 8.47 \\ NASBOWL & **8.32** \\ BOHNAS & **8.31** \\   

Table 2: Results of the activation function search on CIFAR-10 with ResNet-20.

Figure 4: Best architecture found by BOHNAS for ImageNet-16-120 in the hierarchical NAS-Bench-201 search space. Best viewed with zoom. Figure 10 in Appendix J.3 provides the abbreviations of the operations and the architectures for the other datasets.

## 7 Limitations

Our versatile search space design framework based on CFGs (Section 3) unifies _all_ search spaces we are aware of from literature in a single framework; see Appendix F for several exemplar search spaces, and allows search over all (abstraction) levels of neural architectures. However, we cannot construct _any_ architecture search space since we are limited to context-free languages (although our enhancements in Section 3.3 overcome some limitations), e.g., architecture search spaces of the type \(\{a^{n}b^{n}c^{n}|n_{>0}\}\) cannot be generated by CFGs (this can be proven using Ogden's lemma ).

While more expressive search spaces facilitate the search over a wider spectrum of architectures, there is an _inherent trade-off_ between the expressiveness and the search complexity. The mere existence of potentially better-performing architectures does not imply that we can actually find them with a limited search budget (Appendix J.3); although our experiments suggest that this may be more feasible than previously expected (Section 6). In addition, these potentially better-performing architectures may not work well with current training protocols and hyperparameters due to interaction effects between them and over-optimization for specific types of architectures. A joint optimization of neural architectures, training protocols, and hyperparameters could overcome this limitation, but further fuels the trade-off between expressiveness and search complexity.

Finally, our search strategy, BOHNAS, is sample-based and therefore _computationally intensive_ (search costs for up to 40 GPU days for our longest search runs). While it would benefit from weight sharing, current weight sharing approaches are not directly applicable due to the exponential increase of architectures and the consequent memory requirements. We discuss this issue further in Appendix G.

## 8 Conclusion

We introduced a unifying search space design framework for hierarchical search spaces based on CFGs that allows us to search over _all (abstraction) levels_ of neural architectures, foster _regularity_, and incorporate user-defined _constraints_. To efficiently search over the resulting huge search spaces, we proposed BOHNAS, an efficient BO strategy with a kernel leveraging the available hierarchical information. Our experiments show the _versatility_ of our search space design framework and how it can be used to study the performance impact of architectural design choices beyond the micro-level. We also show that BOHNAS _can be superior_ to existing NAS approaches. Our empirical findings motivate further steps into investigating the impact of other architectural design choices on performance as well as search on search spaces with even more atomic primitive computations. Next steps could include the improvement of search efficiency by means of multi-fidelity optimization or meta-learning, or simultaneously search for architectures and the search spaces themselves.