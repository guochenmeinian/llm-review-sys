# KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs

Lujun Li\({}^{*}\) Peijie Dong\(\) Anggeng Li\(\) Zimian Wei Ya Yang

HKUST HKUST(GZ) Huawei NUST CityU

{ililujunai, dongpeijie98}@gmail.com,{anggeng.li,yya9}@outlook.com, weizimian16@nudt.edu.cn

Corresponding author, \(\) Co-first authors, equal contribution.

###### Abstract

Knowledge distillation (KD) has emerged as an effective technique for compressing models that can enhance the lightweight model. Conventional KD methods propose various designs to allow student model to imitate the teacher better. However, these handcrafted KD designs heavily rely on expert knowledge and may be sub-optimal for various teacher-student pairs. In this paper, we present a novel framework, KD-Zero, which utilizes evolutionary search to automatically discover promising distiller from scratch for any teacher-student architectures. Specifically, we first decompose the generalized distiller into knowledge transformations, distance functions, and loss weights. Then, we construct our distiller search space by selecting advanced operations for these three components. With sharpness and represent gap as fitting objectives, we evolve candidate populations and generate better distillers by crossover and mutation. To ensure efficient searching, we employ the loss-rejection protocol, search space shrinkage, and proxy settings during the search process. In this manner, the discovered distiller can address the capacity gap and cross-architecture challenges for any teacher-student pairs in the final distillation stage. Comprehensive experiments reveal that KD-Zero consistently outperforms other state-of-the-art methods across diverse architectures on classification, detection, and segmentation tasks. Noticeably, we provide some practical insights in designing the distiller by analyzing the distiller discovered. Codes are available in supplementary materials.

## 1 Introduction

Deep Neural Networks (DNNs) have achieved great success in tackling a variety of visual recognition tasks . Despite the appealing performance, the prevailing DNN models usually have large numbers of parameters, leading to heavy costs of memory and computation. Conventional techniques such as Neural Architecture Search  and quantizing networks to use low-bit parameters  have proven to be effective for mitigating this computational burden. Recently, Knowledge Distillation (KD) has been widely used for training compact and efficient neural networks by transferring the knowledge lied in the logits  or features  from a large, pre-trained teacher to a smaller student.

Recently, although KD has made significant progress in the handcrafted designs, there are still some limitations to its practical

Figure 1: Illustration of search space in KD-Zero.

application for different scenarios from three aspects: **(1) Capacity gap problem:** Traditional KD methods do not always distill better from stronger teachers because of the large teacher-student gap . Larger and more accurate teacher models tend to be overconfident and fail to improve students . To alleviate this, architecture-level methods use the assistant model  or architecture search , introducing additional training budgets. Other KD techniques use optimized designs or manually modified distillers to reduce the disparity. However, these techniques rely on expert knowledge and require careful tuning, which suffer from poor generality and efficiency. **(2) Cross-architecture issue.** Teacher-student networks with different architectural styles suffer from mismatches in position dependence, receptive fields, and feature dimensions. As shown in Figure 2, while handcrafted methods (_e.g._, KD , DIST  and WSLD ) provide slight gains in some cases, they still are weak in most cross-architecture pairs 2. **(3) Manual tuning difficulties.** Different choices of knowledge sources, distillation functions, and hyperparameters can largely affect the performance of KD. However, designing and training a proper distillation setting requires trial and error, substantial effort, and experiments. Thus, two questions are raised: **(1) How to efficiently discover the optimal distillation strategies without expert knowledge? (2) How to reduce the teacher-student gap with different capabilities and architectures? _"We may hope that machines will eventually compete with men in all purely intellectual fields."_

As this well-said quote goes, recently, machine learning approaches have successfully replaced human experts in algorithm design, architecture search, and drug discovery. For the first problem, inspired by Auto-Zero , we decide to automate the process of designing the distillation functions for the first time in the field of knowledge distillation. For the second question, we systematically review previous studies of KD design for distillation gaps. Our observations reveal that: (1) Most of the distillers can be divided into three components with various key elements: knowledge transformation, distance function, and loss weights. (2) Some essential operations, like normalize ops  and mask ops , play important roles in reducing the teacher-student gap. (3) Optimal distillation functions can be effectively integrated with additional strategies (_e.g._, feature aggregation , projector ensemble , N-to-One match , and cross-layer mapping  in Figure 3).

Based on the above analysis, we present KD-zero, an automated search framework that utilizes evolutionary algorithms from scratch to efficiently discover the best distiller without manual design. Specifically, our framework is organized into three parts: search space, search algorithm, and acceleration strategy. Firstly, we establish the search space with basic transform operations, distance functions, and loss weights (see Figure 1). For example, we select operators normalized in different dimensions (_e.g._, \(batchnorm,norm_{HW,C,N}\)), various types of activation functions (_e.g._, exp, relu, tanh, sigmoid, pow,), multi-scale process and spatial-wise/channel-wise mask transforms and other advanced operations in the knowledge transformation. Our distance function options include smooth \(_{1}\), \(_{1}\), \(_{2}\), \(_{KL}\), \(_{hard}\), \(_{Cosine}\), \(_{Pearson}\) and \(_{Correlation}\) distance. Options in the loss weight part include various values for loss factors, temperature factors, and weight calibration strategies. In this way, our search space includes over 10,000 candidates covering the existing SOTA KD methods and designs. Then, we construct a calculation graph for these candidates and use selected features, representations, and logits from the teacher-student network as input. Based graph structure, we initialize the candidate total groups from the search space, crossover, and mutate them according to the evaluation results. We employ loss-rejection protocol and search space shrinkage for search efficiency to filter out weak candidates. With early-stop proxy settings, we achieve at least 40\(\) acceleration during the distiller search. For distillation gap reduction, we take the representation gap and sharpness gap between teacher-student as the fitting objectives besides the accuracy metric of the validation set. Finally, we distill student architectures with discovered distiller, and our KD-Zero surpasses existing KD approaches by a large margin without prior knowledge (see Figure 3).

In principle, our KD-Zero differs from previous hand-designed KD methods, opening new doors to automated distillation designs. Its merits can be highlighted in three aspects: (1) **Effective.** KD-Zero effectively reduces the teacher-student gap by presenting a general distiller search space and adaptive evolutionary search for different teacher-student pairs. KD-Zero extends the KD formulation and allows for additional gains with extra design besides distillers. In addition, it reduces human bias and ensures that the resulting distillers are optimized for the target problem or dataset. (2) **Efficient.** KD-Zero increases efficiency in practice by a series of flexible, systematic, and efficient search procedures without additional laborious tuning. By contrast, other manual methods with fixed KD forms involve complex parameter tuning with additional training time and resources. (3) **Insightful.** KD-Zero undertakes an in-depth analysis of the existing advanced distillation designs, with the aim of exploring their potential combination to produce numerous novel distillers. KD-Zero provides guidelines for practical applications and develops a new research direction. We hope our efforts on the automated design of distillers could facilitate future research for automated KD works to some extent. In summary, our contributions are:

* To alleviate architecture & capability gaps of teacher-student, we present KD-Zero, the first auto-search framework for evolving best distillers from scratch to our best knowledge.
* We present a comprehensive distiller search space, including advanced operations on transformations, distance functions, and loss weights. Then, we evolve the distiller search with performance and sharpness & represent the gap as fitting objectives. In addition, We achieve significant search acceleration via loss-rejection protocol & space shrinkage, and proxy settings.
* We conduct extensive experiments on classification, detection, and segmentation. KD-Zero performs state-of-the-art in multiple datasets and architectures (_e.g._, CNN and vision transformer). Specifically, ResNet-18 and MobileNet with KD-Zero achieve 72.17% and 73.02% Top-1 accuracy on ImageNet, outperforming KD by 1.51%, 2.34%, respectively.

## 2 Related Work

**Knowledge Distillation.** The idea of teacher-student learning is first proposed in pioneering explorations [1; 2], and the formal definition is defined by the original KD . Subsequent efforts explore on different knowledge (_e.g._, intermediate feature representations [38; 43; 37; 33; 36], sample relationships [48; 60]) and applications [18; 14; 35]. **Compared to KD for distillation gap.** Previous methods propose assistant teachers, architecture search, KD designs on transformations , distance functions , and weight-tuning [42; 34] for this problem. However, such KD designs rely on expert knowledge and tuning, and their performance can fluctuate significantly across different situations. In contrast to these methods, KD-Zero develops automated searches for distillers to address these difficulties that do not require additional architecture modification and manual KD design. **Compared to Meta-KDs.** These works [13; 42] only focus on hyperparameter tuning and involve complex optimization challenges. In contrast, our approach searches for complementary distiller design besides hyperparameters establishing a new paradigm for KD research and application.

**Automated Machine Learning.** AutoML [81; 80] aims to automate Network Architecture Search (NAS) and HyperParameter Optimization (HPO), making them more accessible to non-experts. NAS chooses architecture rather than KD designs. **Compared to HPOs [54; 69],** they generally focus on the hyperparameters on training configurations. Recent methods search for loss formulation [39; 32]. In contrast to these methods, we present a new complex search space for distiller design in transforms, distances, loss weights, and new search objectives and accelerations according to the KD task.

## 3 Methodology

In this section, we first illustrate the design of our distiller search space, the search process, the acceleration, and the fitting objectives. Then we analyze the search results and give some guidelines. Finally, we analyze the student distilled via KD-Zero and expansion for different distillation scenarios. The pipeline of our approach is shown in Figure 4.

### Search Space for Distillers Discovery

**Search space structure.** In KD, the student student \(S\) is distilled with the fixed teacher \(T\) by minimizing:

\[_{KD}=^{2}_{f}_{Cal} (f_{S}/),(f_{T}/),\] (1)

where \(_{f}\) and \(_{Cal}\) is the loss weights factor and calibration , \(\) is the temperature factor, \(\) is transformations, \((,)\) is distance function measuring the knowledge difference. \(f_{T}\) and \(f_{S}\) are outputs (_e.g._, features, embeddings, and logits) of the teacher-student. Following this general KD formulation, our search space consists of different types of operations (see Table 1) in transformations, distance functions, and loss weights parts. Then, we use a computation graph to represent each candidate, in which the input nodes are different types of knowledge and the intermediate nodes are primitive operations. In addition, we assign three transform options as _transform-1\(\)transform-2\(\)transform-3_ for the transformation part to ensure effective processing of the input knowledge.

**Insight of space design.** Our space design enjoys multiple merits. **(1) Comprehensive & flexible:** KD-Zero contains key elements of most existing KD methods, including the normalize, mask,

    & norm-based: \(batchnorm,min-max,norm_{HW,C,N},softmax_{HW,C,N},logsoftmax_{HW,C,N}\) \\  & activation-based: \(exp,mish,leaky,relu,tanh, sigmoid,pow2,pow4,log,sqrt\) \\  & scale-based: \(scale,multi-scale,scale_{rel,1,_{1},2},local_{rel,1,_{2},4,batch},channel\) \\  & attention&mask-based: \(drop,sat,nat,matt,mat,mask\), other: \(no,bmm,mm\) \\  Distance & no-norm loss: smooth \(_{1}\), \(_{1}\), \(_{2}\), \(_{KL}\), \(t_{rad}\); norm loss: \(_{Cosine}\), \(_{Pearson}\), \(_{Correlation}\) \\  Weight & calibration: \(entropy, focal,sim,conf,no\); weight values: 0.01,..., 100, \(\) values:1.4, 8 \\   

Table 1: Specific operations in KD-Zero. More details of their formulas are available in the Appendix.

Figure 4: Overview of KD-Zero. In the search phase, we first randomly sample candidate distillers to initialize the population and evaluate their validation performance and sharpness & represent the gap between teacher-student. Then, we perform loss-rejection protocol & space shrinkage to remove weak individuals and crossover & mutation to generate new populations within promising ones. Finally, we pick up the best-performing distiller for distillation.

attention, and focal-calibration KDs [3; 79; 27] for distillation gaps. In addition, we introduce basic operations in loss design to improve the flexibility and diversity of the search. **(2) Extended & Innovative:** We extend advanced KD design for omni-dimensional and various knowledge. For example, we expand the normalized operation for batch, channel, and spatial dimensions, which can be used for logits, embeddings, and feature knowledge. Thus, such a unified and flexible search space would provide some inspiration for KD design.

### Evolution Procedure, Acceleration and Objective

**Evolution process.** Our EA starts with an initial population of candidate distillers evaluated for their fitness based on our distillation gap-related multi-objectives. The algorithm then iteratively evolves the population over generations using genetic operators, such as selection, crossover, and mutation, to generate better solutions. Specifically, distillers are first randomly generated to form the initial population. Each candidate solution in the population is evaluated using multiple fitness functions that measure teacher-student gaps. Based on these evaluations, we select the best-performing individuals from the population to create a new population for the next generation. Then, we apply crossover to the selected individuals to create new offspring and use mutation to the new offspring to introduce diversity into the population, which helps to explore new distillers.

**Search acceleration** As the search space is sparse with many unpromising distillers, we employ several strategies to accelerate: **(1) Loss-rejection protocol.** We filter out candidates with excessive loss values or collapsed optimization during the search. **(2) Search space shrinkage.** We reduce sampling probabilities for the operations frequently in loss rejection and tail candidates with search iterations. **(3) Proxy settings.** With diverse and informative knowledge learned from a teacher, student models offer advantages in terms of faster training speeds. Based on these properties, we employ early stop the training process once the student model performs well enough to determine the quality of the candidate distillation. Nevertheless, proxy settings can also introduce evaluation uncertainties, and we alleviate this issue by introducing multiple distillation gap metrics in the following sections.

**Fitting objectives.** To accurately evaluate each distiller and reduce the distillation gap, we include cross-entropy loss, sharpness-gap  on prediction, and CKA-gap  on representation between teacher-student as the multi-objectives. Specifically, we conduct a training-free evolutionary search algorithm to efficiently discover the optimal distiller \(^{*}\) from search space \(\), as:

\[^{*}=_{}(_{CE}(f_{S},Y)+ {(log((f_{S}))-log((f_{T})))}^{Sharpness-gap}-(f_{S},f_{T})}{(f_{S},f_{S})(f_{T},f_{ T})}}}^{CKA-gap}),\] (2)

where \(_{CE}\) is the regular cross-entropy objective with labels \(Y\), the sharpness metric is the logarithm of the exponential sum of logits, and Centered Kernel Alignment (CKA) metric is normalized from Hilbert-Schmidt Independence Criterion (HSIC)  on high-level feature.

### Results Analysis and Practical Guidance

Figure 5 present searched distillers for different models. Based on these results, some practical guidance for KD designs can be summarized as follows:

* For knowledge input, feature knowledge enjoys superiority while logits and embedded knowledge share identical status. Searched distillers with feature knowledge take more proportion, and ablation studies on vanilla feature knowledge also surpass logits and embedding knowledge.

Figure 5: Probability of operations within each search part, which counted from the Top-3 searched distillers for all teacher-student pairs in the CIFAR-100 experiment.

This observation also aligns with the existing conclusion that feature-based KD outperforms other KD on detection tasks [27; 63].
* For transformations, the normalized-based operations play a key role in most optimal distillers, and its vanilla performance outperforms other types of transformations in ablation studies. These observations illustrate that normalization benefits distillation, consistent with the current KD methods. Scale-based and activation-based operations are also crucial for the KD because they are often present in optimal distillers and enjoy good vanilla performance in ablation studies.
* For distance functions, normalized-based distances such as \(_{Pearson}\) and \(_{correlation}\) enjoy better results in ablation studies and are often adopted by teacher-student pairs across architectures. This suggests that these normalized-based distances can reduce distillation gaps in complex scenarios. In addition, some simple distance functions (_e.g._, \(_{1}\), \(_{2}\) and \(_{KL}\)) also appear in the optimal distiller. These may be attributed to these distillers employing advanced transformation operations for input knowledge.
* For loss weights, adjusting the temperature values is helpful for different scenarios, and the optimal weight values are generally between 1 and 10 based on search results and ablation studies. In addition, the focal weight calibration outperforms the no-weight calibration in ablation studies, and it is often used for some optimal distillers. In summary, we should employ smaller weight values and actively explore different weight calibration and temperature values to reduce distillation gaps under different teacher-student pairs.

### Distilling Student via Discovered KD-Zero functions

After search, the discovered distiller \(_{KD}\) is used for distilling student \(f_{S}\) combined with cross-entropy \(_{CE}\) via label \(Y\) in single teacher \(f_{T}\) or multiple teacher \(f_{T_{i}}\) KD as:

\[_{single}=_{CE}(f_{S},Y)+_{KD}(f_{S},f_{T}), _{multiple}=_{CE}(f_{S},Y)+_{i=1}^{N}_{KD}(f_{S},f_{T_{i}^{}}).\] (3)

**Extended to various distillation designs & scenarios** KD-Zero focuses on the distiller's search and employs simple 1\(\)1-Conv for channel alignment. Recently, some KD methods have proposed other designs in feature aggregation , projector ensemble , N-to-One match , and cross-layer mapping [28; 13]. As shown in Table 2, KD-Zero can combine well with them by replacing their default \(_{2}\)/\(_{KL}\) losses. In addition, the distiller search of KD-Zero also benefits different distillation scenarios. Specifically, Self-KDs, online KDs, and multi-teacher KDs with KD-Zero achieve extra gains than the default settings. Also, KD-Zero can combine architecture-level methods (_e.g._, Assistant ) to reduce distillation gaps further.

**Why can KD-Zero bridge the teacher-student gap?** The answer is intuitive: In KD-Zero, the search space contains many operators for distillation gap reduction, and search objectives are directly designed to reduce the prediction and representation gap between teacher-student. For example, We approximate the sharpness gap using a Taylor second expansion :

\[G_{gap}=log((f_{T}))-log((f_{S}))(1+f_{T}+f_{T}^{2})-(1+f_{S}+f_{S}^{2}),\] (4)

Following Hindon's assumption  that the logits of each training sample are approximately zero-meaned, i.e., \(_{T},_{S}=0\). So the gap can be rewritten as \((1+Var(f_{T}))-(1+Var(f_{S}))\). Our options like normalized transform [3; 27], and weight calibrations  can effectively reduce the variance of teacher-studens' outputs and minimize the distillation gap. In addition, the student models distilled with KD-zero enjoy the merits: (1) Fast convergence and superior performance. As shown in Figure 6, KD-Zero has surpassed the best accuracies of KD

   &  &  \\   & Review  & [Pr.] & NORM  & L2T-ww  & ONE  & BYOT  & DML  & AVE-MRD  & AE-MRD  & Assistant  \\   & \(_{S}\) & \(_{T}\) & 1.89 & 71.76 & 71.55 & 70.89 & 70.77 & 70.37 & 70.92 & 71.24 & 71.36 & 71.06 \\   & KD-Zero & 22.27 & 72.18 & 72.00 & 72.03 & 71.38 & 70.98 & 72.02 & 72.22 & 72.35 & 71.89 \\   & \(_{S}\) & \(_{T}\) & 76.20 & 76.02 & 75.65 & 75.30 & 74.25 & 74.12 & 75.33 & 75.22 & 75.68 & 75.35 \\   & KD-Zero & 76.62 & 76.47 & 76.26 & 76.36 & 75.65 & 75.21 & 76.45 & 76.72 & 76.78 & 76.05 \\  

Table 2: Top-1 (%) accuracy of KD-Zero combined with different KD designs and scenarios for ResNet-20 and WRN-16-2 on CIFAR-100.

and baseline at the beginning of the 3rd learning-rate decay stage. (2) Smaller teacher-student gap. As shown in Figure 8, features and logits of students via KD-Zero have stronger similarities with teachers than the original KD method.

## 4 Experiments

In this section, we assess the efficacy of our proposed KD-Zero approach on classification, detection, and segmentation tasks. Additionally, we compare its performance with other KD methods, ensuring fair comparisons by utilizing the same training settings. We report mean results based on more than 3 repeated trials. More detailed experiment settings and results are available in the Appendix.

### Experiments on CIFAR-100

**Implementation**. We utilize the CIFAR-100 dataset  in knowledge distillation. During the distiller search phase, we apply 5% early-stopping training epochs with full training data for acceleration settings. Our evolutionary algorithm with 20 population sizes performs 100 iterations for each teacher-student pair. During the distillation phase, all teacher-student networks are trained using typical training settings, with a training epoch of 240. The multi-step learning rate commences at 0.1, which decays by 0.1 at 100 and 150 epochs. Recently, knowledge distillation enabled training Vision Transformers (ViT) from scratch with CNNs as teachers. To evaluate the effectiveness of KD-Zero, we conduct the evolutionary search for ViT-based distillation strategies with the same settings as the CNN experiment. Subsequently, we train the ViT with the optimal distiller obtained and ResNet-56 as CNN teacher. The training is conducted on \(224 224\) resolution images for 300 epochs, with an initial learning rate of 5e-4 and a weight decay 0.05 using the AdamW optimizer.

**Comparison results on CNN models.** Table 3 presents a comparative analysis of our KD-Zero with other state-of-the-art (SOTA) KD methods. We conduct multiple trials with randomly selected distillers in the same search space, called Rand-KD, to evaluate the efficacy of our EA search. For teacher-student pairs with the same architectural style, KD-Zero outperforms the baselines by margins ranging from \(3.16\% 4.90\%\). Compared with Rand-KD and other KDs, KD-Zero obtains consistent performance gains (\(1.0\% 3.2\%\)). Besides strengths in the same architecture pairs, KD-Zero exhibits even stronger performance when dealing with different architectural styles, while other KD methods suffer from noticeable accuracy reductions. Specifically, KD-Zero outperforms the baseline by margins of \(5.6\% 7.3\%\) and the random search results by margins of \(1.9\% 2.3\%\), demonstrating the effectiveness of our design for different structures. Compared with other SOTA KD methods, our KD-Zero achieves \(1.2\% 1.5\%\) gains. These results show that KD-Zero can improve each student model with simple settings under different teacher-student pairs.

**Comparison results on vision transformer.** Table 3 presents the results of the vanilla and distillation models employing different distillation methods. The results indicate that KD-Zero can significantly improve the performance of vision transformers with \(8.0\% 13.1\%\) margins and consistently yields superior performance than other methods. In addition, our proposed method applies to various ViT architectures, thereby validating its effectiveness. Note that most ViT students possess larger model sizes (_e.g._, DeiT-Ti with 5 million parameters) and greater capabilities than the CNN teacher (_e.g._, ResNet-56 with only 0.86 million parameters). Some ViT students outperform the CNN model in the strong regularization setting on large-scale datasets. However, when it comes to ViT distillation on small datasets, employing CNN teachers helps address the issue of ViT models struggling to train effectively from scratch. In this context, using CNN teachers in distillation is akin to auxiliary training or providing additional regularization supervision. As a result, these ViTs demonstrate their original strong representation capability after distillation and consequently outperform the CNN teacher.

### Experiments on ImageNet

**Implementation**. We additionally conduct experiments on the ImageNet. Following CIFAR-100 trials, we employ similar EA settings on a subset of ImageNet for search acceleration. Then, we utilize the discovered distiller for the training of student models (_e.g._, ResNet-18  and MobileNet ). The training settings are the same as the other KD methods and involve training for 100 epochs using a multi-step learning rate, which commences at 0.1 and decays by 0.1 at 30, 60, and 90 epochs.

**Comparison results.** As shown in Table 4, our proposed KD-Zero significantly improves the accuracy of baseline models, yielding gains of \(2.5\% 2.9\%\) in Top-1 accuracy for ResNet-18 and MobileNet, respectively. In addition, KD-Zero surpasses other SOTA methods with clear gains, demonstrating its superiority in large-scale datasets. These findings substantiate the effectiveness of KD-Zero in distillation optimization with considerable benefits, establishing the versatility and potency of our framework. In summary, KD-Zero facilitates substantially improved predictive accuracy of student models on ImageNet, more complex domains while preserving superior performance.

**Visualizations.** The comparison between the Grad-CAM++ maps generated by the student model trained with KD-Zero and other methods is presented in Figure 8. The results indicate that the Grad-CAM++ map generated by the student model trained with KD-Zero is more similar to that of the teacher model compared to the student model trained independently. In contrast, independent training of the student model leads to incorrect focus areas. These findings suggest that the KD-Zero

   Teacher & Student & Acc. & Teacher & Student & KD  & AI  & OPD  & SRRL  & CRD  & Review  & MOD  & **KD-Zero** \\  ResNet-34 & ResNet-18 & Top-1 & 73.40 & 69.75 & 70.66 & 70.69 & 70.81 & 71.73 & 71.17 & 71.61 & 71.58 & **72.71\({}_{+1.9}\)** \\  ResNet-34 & ResNet-18 & Top-5 & 91.42 & 89.07 & 98.83 & 90.01 & 98.98 & 90.60 & 90.13 & 99.51 & 90.35 & **90.46\({}_{+1.2}\)** \\   &  & Top-1 & 76.16 & 70.13 & 70.68 & 70.72 & 71.25 & 72.49 & 71.37 & 72.36 & 72.35 & **73.02\({}_{+1.2}\)** \\  & & Top-5 & 92.86 & 89.49 & 90.30 & 90.03 & 90.34 & 90.92 & 90.41 & 91.00 & 90.71 & **91.05\({}_{+1.9}\)** \\   

Table 4: Comparison of results on ImageNet. The results of other methods quote the original papers report . We report Top-1 “mean (std)” accuracies (%) for KD-Zero.

approach is more effective than traditional knowledge distillation methods in guiding the student model to learn from the teacher model, resulting in improved model interpretability and performance.

### Experiments on Object Detection & Semantic Segmentation

**Object detection.** We conduct experiments on the MS-COCO dataset. We use the optimal distilller on ImageNet to distill knowledge from teacher detectors to students. Based on the strong baseline , we apply KD-Zero to two-stage detector (_e.g._, Faster R-CNN ) and the one-stage detector (_e.g._, RetinaNet ), which are widely used object detection frameworks. Following common practice , all models are trained with a 2\(\) learning schedule (24 epochs). We train all the models with SGD optimizer, where the momentum is 0.9, and the weight decay is 0.0001. As shown in Table 5, our KD-Zero improves the AP by 3.5 on RetinaNet and 3.4 on Faster R-CNN, respectively, outperforms previous state-of-the-art techniques, including [59; 70; 74], for both object detectors. The results substantiate the potential of KD-Zero for scaling knowledge transfer to broader datasets and more complex computer vision problems while preserving improved accuracy.

**Semantic segmentation.** We evaluate KD-Zero on Cityscapes dataset. Following the previous work, we adopt PSPNet-ResNet101 [78; 51] as the teacher and PSPNet and DeepLabV3 models with the ResNet18 backbone as the student. During distillation, the batch size is 8, and the models are trained for 40K iterations with the SGD optimizer, where the momentum is 0.9 and the weight decay is 0.0005. The results are reported with mean Intersection-over-Union (mIoU) under the single-scale evaluation setting. As shown in Table 6, the student PSPNet and DeepLabV3 get 3.17 and 3.7 mIoU improvement by adding our KD-Zero loss. These results indicate that our method surpasses the state-of-the-art distillation method for semantic segmentation, demonstrating that searched distillers facilitate student learning.

 Method & mIoU (\%) \\  T: DeepLabV3-R101 & 78.07 \\  S: DeepLabV3-R18 & 74.21 \\ SKD  & 75.42 \\ IFVD  & 75.59 \\ CWD  & 75.55 \\ CIRKD  & 76.38 \\ DIST  & 77.10 \\ KD-Zero & 77.38 \\  S: PSPNet-R18 & 72.55 \\ SKD  & 73.29 \\ IFVD  & 73.71 \\ CWD  & 74.36 \\ CIRKD  & 74.73 \\ KD-Zero & 76.25 \\ 

Table 6: **Results on Cityscapes val dataset with ImageNet Pretrain.**

Figure 10: Comparison of search algorithms (_left_ and organization (_right_) of ResNet-20 on C-100. and correlation visualization (_right_) of ResNet-20.

[MISSING_PAGE_FAIL:10]