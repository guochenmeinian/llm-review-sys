# Kaiyu Yang\({}^{3}\), Zihan Wang\({}^{1,2,}\), Yisong Yue\({}^{3}\), Yuxiao Dong\({}^{1}\), Jie Tang\({}^{1}\)\({}^{\dagger}\)

SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models

 Dan Zhang\({}^{1,2,}\), Ziniu Hu\({}^{3}\), Sining Zhoubian\({}^{1,2,}\), Zhengxiao Du\({}^{1,2,}\)

**Kaiyu Yang\({}^{3}\), Zihan Wang\({}^{1,2,}\), Yisong Yue\({}^{3}\), Yuxiao Dong\({}^{1}\), Jie Tang\({}^{1}\)\({}^{}\)**

\({}^{1}\)The Knowledge Engineering Group (KEG), Tsinghua University; \({}^{2}\)Zhipu AI;

\({}^{3}\)California Institute of Technology

https://SciGLM.github.io/

Work done while these authors interned at Zhipu AI.Corresponding author.

###### Abstract

Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations. To bridge these gaps, we introduce SciInstruct, a suite of scientific instructions for training scientific language models capable of college-level scientific reasoning. Central to our approach is a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain. This framework leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise. Applying this framework, we curated a diverse and high-quality dataset encompassing physics, chemistry, math, and formal proofs. We analyze the curated SciInstruct from multiple interesting perspectives (e.g., domain, scale, source, question type, answer length, etc.). To verify the effectiveness of SciInstruct, we fine-tuned different language models with SciInstruct, i.e., ChatGLM3 (6B and 32B), Llama3-8B-Instruct, and Mistral-7B: MetaMath, enhancing their scientific and mathematical reasoning capabilities, without sacrificing the language understanding capabilities of the base model. We release all codes and SciInstruct at https://github.com/THUDM/SciGLM.

## 1 Introduction

Large language models (LLMs) have shown potential to assist and accelerate scientific discovery [1; 2], helping tasks like protein prediction , weather forecasting  and geoscience understanding . Despite these promising proof-of-concept trials, recent studies [6; 7; 8; 9] show that even advanced LLMs like GPT-3.5 and GPT-4 struggle with basic scientific problems, achieving only 28.52% accuracy on some college-level textbook questions. These scientific questions, such as calculating energy with the Planck distribution, require a diverse set of skills, including finding the correct combination of physical concepts and axioms, choice and deduction of formal equations, and rigorous numerical computing. Before letting LLMs equip these skills to solve basic scientific questions, all ambitious visions of building LLM agents to assist scientific discovery could be unreliable. This brings substantial incentives for building scientific instructions and using them to develop foundational scientific language models.

However, training LLMs to understand science (e.g., physics, chemistry, math) is much more challenging than many general reasoning tasks, requiring more complicated skills. Therefore, the core of improving LLMs' scientific problem-solving capabilities is to build large-scale and high-quality instruction datasets, which shall cover all the required skills. We thus aggregate data to improve each skill: 1) for scientific concept understanding, we gather a substantial amount of physics and chemistry questions that require basic science knowledge; 2) for numerical calculation, we crawl and utilize additional and more advanced mathematical calculation data; 3) for rigorous deduction of symbolic equations, we incorporate formal theorem proofs written in Lean. Such a mixture of data sources enables the trained model not to overfit to a single subject but to acquire some general and fundamental skills to solve different scientific tasks.

On the other hand, the scale of available instruction data from the internet for scientific problems is way smaller than other tasks. As scientific content often requires certain expertise to create, and high-quality information is often protected by intellectual property, most data we can legally access only contain question-answer (QA) pairs without detailed chain-of-thought reasoning steps (\(R\)). However, merely training LLMs on QA pairs will lead to very bad results and even harm their general language capability. To get high-quality reasoning steps (\(R\)) as instruction, we propose a self-reflective instruction annotation framework that asks LLM to autonomously annotate, critique, and revise reasoning steps, with minimal human intervention. Specifically, LLM first tries to generate both reasoning steps and answer the given question (\(Q\)) only; then, for those outputs with incorrect answer prediction, we ask LLM itself to identify the error type, based on which to address the error and revise the output, until getting the correct answer. Such a self-reflective annotation framework solely utilizes AI rather than humans to collect reasoning traces (\(R\)) as instructions, while guaranteeing the quality and addressing the potential mistakes of existing LLM with careful answer checking and LLM self-reflection.

After consolidating the questions and answers produced by self-reflective annotation, we construct SciInstruct, a comprehensive dataset instruction tuning scientific language models. Figure 2 and Figure 2 present the domain and question type proportions of SciInstruct. Table 1 concludes the key differences between existing datasets and ours. In this work, we choose three LLMs, i.e., the ChatGLM3 [13; 14] (6B and 32B), Llama3-8B-Instruct , and Mistral-7B: MetaMATH [16; 11], as our backbone base models. For example, by fine-tuning the ChatGLM series model on SciInstruct, we obtain the SciGLM model. We then evaluate the fine-tuned models through three types of evaluation tasks, including **scientific test sets**, **mathematical benchmarks**, and **general language and coding tasks**, and show the average accuracy on scientific benchmarks of different LLMs in Figure 3. Through instruction tuning, we achieve a 4.87% improvement over the 6B model, and 2.67% improvement over the 32B model, outperforming many previous state-of-the-art models with the same parameter size, including Galactica  for science problems, and MAmmoTH  for math problems. We also show tuning our instruction datasets does not sacrifice general language understanding capabilities, making SciGLM a good suite of scientific language models for both human-AI communication as well as scientific domain-knowledge expertise.

We highlight our contributions as follows:* From the **data** perspective, we construct SciInstruct, a comprehensive scientific instruction tuning dataset that includes physics, chemistry problems, math, and formal proofs.
* From the **method** perspective, we propose a self-reflective annotation pipeline for LLMs to autonomously curate instruction datasets.
* From the **model** perspective, to verify the effectiveness of our SciInstruct, we finetune different LLMs (the ChatGLM3 series model, Llama3-8B-Instruct, and Mistral-7B: MetaMATH) on SciInstruct and show performance improvements on various scientific and mathematical benchmarks, without sacrificing general language understanding tasks.

## 2 SciInstruct

Many research [17; 10; 12] have shown that fine-tuning pre-trained LLMs on high-quality CoT reasoning data can gain performance improvement by enabling the model to better utilize the knowledge memorized through pre-training, follow more accurate and human-readable reasoning styles and language formats. However, the main challenges of constructing scientific instructions include the knowledge and complexity required and the smaller scale of available data. Therefore, we seek to tackle these obstacles by creating SciInstruct to enhance the LLMs' scientific problem-solving capabilities. Figure 4 illustrates our meticulous design of essential sub-modules aimed at gathering large-scale, high-quality instructions. These critical sub-modules encompass self-reflective instruction annotation and noisy instruction filtering. SciInstruct comprises a total of **254,051** verified instructions.

### Diverse Instruction Collection

Our objective is to build a comprehensive and diverse dataset that encompasses scientific knowledge in terms of depth, wide coverage, and diversity. To achieve this, we will focus on scientific fields and curate several top-tier datasets that are extensively utilized and cover a wide range of scientific disciplines, such as physics, chemistry problems, mathematics, and formal proofs. To initiate the process, we collect questions from a variety of sources, including textbooks, pedagogical materials, and problem sets.

**Instruction Subject.** As show on the left side of Figure 4, we create data from the following subjects:

\(\)**Physics.** This subject aims to address the challenge of processing complex physical problems with step-by-step solutions and assessing the ability of LLMs to comprehend and analyze physics problems. Public training datasets such as Fundamentals of Physics and Physical Chemistry are observed to lack college-level physics knowledge. To address this gap, we collected a large set of physical questions from a wide array of subjects (e.g., dynamics, quantum physics, electrodynamics, etc.) from typical physical textbooks, along with a comprehensive dictionary of physics problems

Figure 4: **The pipeline of constructing SciInstruct. On the far left is a mix of training datasets. The purpose of the annotation is to supplement chain-of-thought processes with reflective generation. The goal of the filter is to train an instruction-quality classifier and only keep high-quality reasoning traces as instructions.**

and solutions. Note that most of these physics questions only contain a single answer without a step-by-step solution.

\(\)**Chemistry.** We gathered questions from various chemistry subjects, including college-level General Chemistry, Analytical Chemistry, Biochemistry, and Inorganic Chemistry. Unlike Physics, these Chemistry problems emphasize more on knowledge concepts and require less calculation.

\(\)**Math.** College-level mathematical tasks are characterized by their intricate logic and high complexity. We have gathered math problems encompassing various mathematical subjects such as Calculus, Algebra, and Advanced Mathematics. These problems come in different formats, including multi-choice questions, calculations, and complex problem-solving scenarios. Our collection process involves sourcing problems from public Q&A websites, problem sets, and textbooks.

\(\)**Formal Proofs (Lean).** SciInstruct also includes data from formal mathematics. In formal math, theorems and proofs are expressed in formal logic instead of natural language. They look like computer programs. For example, "theorem gcd_self (n : Nat) : gcd n = n" is a theorem that says: For any natural number n, the greatest common divisor of n and itself is n. And "cases n <;> simp [gcd, mod_self]" is a proof of this theorem. There are many languages for writing formal theorems and proofs; examples include Coq , Isabelle , and Lean . We chose to include data from Lean, as it offers a vast math library and is currently the most popular in the math community. Therefore, it has abundant theorems and proofs written by mathematicians, covering diverse topics in graduate-level mathematics, such as analysis, geometry, and algebra. Specifically, we process the data from LeanDojo  and format it to align with the successive deduction process, ensuring its relevance and applicability to the model's training for mathematical reasoning tasks in natural language. This preprocessing step helps bridge the gap between the Lean dataset's nature and the model's expected learning signals. Finally, we obtained 40,248 instructions for theorem proof. Like Appendix A.3, we form an instruction for each theorem and its proof.

**Multi-lingual Instruction.** To enhance the overall quality and effectiveness of the curated dataset, we also translate the default Chinese questions into English. We found that LLMs tend to generate correct solutions after translating these problems into English for some Chinese problems that do not obtain correct solutions. This improved performance is likely due to the higher-quality English corpus used during the pre-training of LLMs. Therefore, we have embraced this strategy to construct SciInstruct for Chinese questions.

**Summary.** In total, we gathered 257,143 raw questions, the majority of which lacked step-by-step reasoning steps. We aim to supplement these through a self-reflective annotation process.

Figure 5: The workflow and quantity of data for self-reflective instruction annotation framework.

### Self-Reflective Instruction Annotation

Given a language model \(\) to answer question \(Q\), recent studies  have shown that by first forcing it to generate step-by-step reasoning steps (\(R\)) first, the overall performance for correctly generating answer \(A\) can be significantly improved, via: \(P_{}(A Q)=_{R P_{}(R|Q)}P(A R,Q)\). This is why many instruction datasets aim to collect high-quality intermediate solutions to train LLMs generating correct step-by-step solutions. The key challenge for the science domain, as we state above, is that most of the QA pairs we collect do not contain ground-truth reasoning paths (\(R\)). Getting the correct intermediate reasoning \(R\) given QA can be regarded as a discrete latent variable inference problem via posterior sampling. However, in practice, we cannot afford to sample all possible \(R\) from LLM. Here we adopt a simplified strategy for solving it: 1) Utilizing a powerful LLM (we use GPT-4-0613), we sample each multiple times for each question, recording its reasoning traces as well as the predicted answer; 2) We filter out only the traces with a correct predicted answer, by assuming only with correct traces, LLM can get a correct answer.

**LLM Self-Reflection.** However, even GPT-4 cannot consistently produce accurate answers after multiple trials, and the above procedure can only collect \(R\) for a portion of questions. Drawing on prior research demonstrating the capacity of language models for self-correction [23; 24], we refine the CoT process using a self-reflective framework, as depicted in the middle of Figure 4. The final reflective generation process entails three stages. Initially, we employ a simple CoT prompt (Prompt 1) to obtain step-by-step solutions for each question. To obtain an accurate assessment of reasoning results, we employ a GPT-4 labeling method based on an outcome reward model (ORM)  as a basic implementation in our work, rather than the expensive process reward model (PRM)  that typically requires manual annotation, especially for complex scientific reasoning. We filter out incorrect solutions by applying the GPT-4 labeling method, resulting in 19,824 correct solutions. Subsequently, the incorrect solutions from stage one and their respective questions enter stage two. Here, a reflective prompt (Prompt 2) assists GPT-4 in generating correct answers by analyzing its errors from stage one. The newly generated solutions are then filtered again by GPT-4, and the undesirable ones proceed to stage three. In stage three, based on Prompt 2, we incorporate the real answer as a direct hint in the prompt (Prompt 3) to further aid in question resolution. The final segment of correct solutions is obtained after the generation and filtering process. We illustrate the reflective generation in Figure 5 and quantify the amount of data generated by each process.

### Filtering Out Low-Quality Instruction

Though the above procedures give us the annotated reasoning steps (\(R\)), not all of them are correct. The error can come from two sources: 1) though LLM generates a correct answer, the intermediate reasoning can still be wrong ; 2) the question and ground-truth solutions transformed via Optical character recognition (OCR) may be incomplete and unable to be successfully compiled. Therefore, we propose another step to train an instruction-quality classifier and filter out low-quality instructions.

**Quality Data Synthesis.** We randomly selected a subset of questions from our labeled dataset of 11,553 questions as positive samples. To generate negative samples, we prompted ChatGLM2-6B, GPT-3.5-turbo-0613, and GPT-4-0613 to provide step-by-step answers to selected questions. We filtered inaccurate answers from ChatGLM2-6B and labeled the solutions from GPT-3.5-turbo and GPT-4 using a formatted prompt method demonstrated in Figure 11 in Appendix A.4. These solutions were merged with annotated solutions from the original dataset to train our classifier. The composition of the merged dataset is detailed in Table 7 in Appendix A.5.

   Type & Ave. Sci & Ave. Math & Ave. [Sci+Math] \\  Unfiltered & 43.57 & 48.50 & 46.03 \\ Filtered & 43.85 & 49.24 & 46.54 \\   

Table 2: **Ablation study of filter step**. We arranged the samples by score and excluded the lowest 10%. The resulting numbers represent the average weighted accuracy on evaluation tasks.

   Subject & \# Number & Proportion \\  Physics \& Chemistry & 123,869 & 48.76\% \\ Math & 89,934 & 35.40\% \\ Formal Proofs (Lean) & 40,248 & 15.84\% \\  Total & & 254,051 \\   

Table 3: Statistics of SciInstruct across subjects.

**Instruction-quality Classifier.** We improved dataset quality by training an instruction-quality classifier based on Table 7 using ChatGLM3-6B-Base features. Using these data, we train an instruction-quality classifier via the feature pre-extracted by a ChatGLM3-6B-Base model. The classifier outputs a logit ranging from -1 to 1, with higher scores indicating more reliable answers. This logit is used to rank and select high-quality data from the noisy dataset. Table 2 demonstrates the effectiveness of supervised fine-tuning on both filtered and unfiltered datasets at a 6B scale, showing that models trained on a filtered dataset perform better.

**Error Analysis.** As our classifier filter is trained on labeled datasets and generated solutions, errors in negatively labeled solutions from ChatGLM2-6B, GPT-3.5-turbo, and GPT-4 can significantly impact the classifier's performance. Therefore, we conduct an error analysis and categorize them into Comprehensive mistakes, Calculation mistakes, and False reasoning. This analysis is detailed in Figure 12 in A.6, demonstrating the classifier's capacity to recognize these errors in the dataset.

**Summary.** Based on the aforementioned key sub-modules, we have constructed the SciInstruct dataset, which comprises 254,051 instructions, as illustrated in Table 3.

### Instruction-Tuning with SciInstruct

As our foundational model, we choose three LLMs, i.e., ChatGLM3 (6B and 32B), Llama3-8B-Instruct, and Mistral-7B: MetaMATH. After establishing the base model, we have standardized all data into a chatbot-style format. Subsequently, we have fine-tuned the foundational model using the SciInstruct, enabling us to validate our constructed SciInstruct. Throughout the fine-tuning process, we have conducted experiments using the Huggingface transformers library. For both the 6B and 32B models, we have utilized a learning rate of 3e-6, employed a linear scheduler, and trained for two epochs. To efficiently train the model, we have leveraged DeepSpeed  training.

## 3 Benchmark on SciInstruct

### Experimental Setup

**Scientific and Mathematical Tasks.** The evaluation tasks are summarized in Table 8 in Appendix A.7. These tasks have been chosen as out-of-domain benchmark datasets, encompassing CEval-Hard ,

   Model & CEval-Hard & CEval-Sci & MMLU-Sci & SciFlow & SciBench & GPQA\_Diamond & Avg. Sci & Avg. (Sci+Math) \\   \\  GPT-4 & 54.96 & 60.55 & - & **73.93** & **28.52** & 39.70 & - & - \\ GPT-3.5-turbo & 41.37 & 46.83 & - & 66.97 & 12.17 & - & - \\ Claude-v1.3 & 39.14 & 44.64 & - & 63.45 & - & - & - & - \\   \\  LLAMa-2.7B & 28.29\({}^{1}\) & 30.06\({}^{1}\) & 30.41 & 28.37 & 0.40 & - & - & - \\ Galactic-6.7B & 118.41\({}^{1}\) & 41.44\({}^{3}\) & 30.68\({}^{5}\) & 50.87 & - & - & - \\ ChatGLM2-6B & 29.61\({}^{1}\) & 45.71\({}^{1}\) & 37.09\({}^{1}\) & 53.02\({}^{1}\) & 1.54\({}^{1}\) & - & - \\ ChatGLM2-6B-Base & 32.90\({}^{1}\) & 40.95\({}^{1}\) & 38.06\({}^{1}\) & 50.38\({}^{1}\) & 1.20\({}^{1}\) & - & - \\ ChatGLM3-6B-Base & 36.84\({}^{1}\) & 38.57\({}^{1}\) & 41.78\({}^{1}\) & 56.56\({}^{1}\) & 2.40\({}^{1}\) & 28.70 & 34.14 & 29.73 \\ ChatGLM3-6B-Base & 45.40\({}^{1}\) & 54.39\({}^{1}\) & 40.16\({}^{1}\) & 61.69\({}^{1}\) & 2.40\({}^{1}\) & 24.75 & 38.12 & 40.34 \\ Setkin-CentralM3-6B-Base & **51.97** & **60.00\({}^{1}\) & 53.44 & 62.69\({}^{1}\) & **3.77\({}^{1}\)** & 25.25 & **41.40** & **45.03** \\ Llama3-8B-Instruct (zero-shot) & 23.62\({}^{1}\) & 27.62\({}^{1}\) & 26.90\({}^{1}\) & **71.38\({}^{1}\)** & 1.03\({}^{2}\) & 27.27 & 30.09 & 28.58 \\ Llama3-8B-Instruct (zero-shot) & 25.66\({}^{1}\) & 23.33\({}^{1}\) & **52.67\({}^{1}\)** & 71.38\({}^{1}\) & 3.60\({}^{1}\) & 31.31\({}^{1}\) & 34.66 & 37.92 \\ Mistral-7B-MetaMATH (zero-shot) & 32.24 & **34.76** & **30.66** & 64.67 & 3.60 & 29.29 & 34.54 & 36.04 \\ Mistral-7B-MetaMATH (zero-shot) & 9.87\({}^{1}\) & 8.57\({}^{1}\) & 28.25\({}^{1}\) & 63.61\({}^{1}\) & 4.63\({}^{1}\) & 27.78\({}^{1}\) & 23.79 & 25.57 \\ Mistral-7B-MetaMATH (few-shot) & 9.21\({}^{1}\) & 95.42\({}^{1}\) & 44.74\({}^{1}\) & 63.61\({}^{1}\) & 6.17\({}^{1}\) &CEval-Sci , MMLU-Sci , SciEval , SciBench , GPGQ , GSM8K , MATH , Mathematics , SAT-Math , MMLU-Math  from MathInstruction, and CEval-MATH . These evaluation datasets cover a broad range of subjects, including physics, chemistry, and math.

**General Tasks.** Furthermore, we assess the generalization abilities of tasks across various scales when fine-tuning models. These tasks include assessing knowledge abilities (MMLU  and CEval ) and code generation (MBPP) .

**Evaluation Metrics.** The default setting for the fine-tuned base model inherently provides the CoT solution. Hence, we conduct all experiments using CoT settings. To thoroughly and accurately evaluate the capabilities of different models, we employ the accuracy metric for all tasks except for code generation, for which we use the pass@1 metric.

**Baselines.** We consider the following baselines(e.g., GPT-4 , GLM [13; 14; 36] and LLaMA Base , Continue Pre-training, and Dataset-specific Tuning, etc.) and describe details in Appendix A.8. We employ a standardized evaluation framework to compare GLM and LLaMA Base baselines fairly. To gauge performance in the MATH task, we utilize zero-shot and 8-shot configurations to determine the highest accuracy. Additionally, for Mathematics, SAT-Math, MMLU, and CEval, we employ a chat module for assessment. When dealing with multiple-choice questions, we formulate the prompt as "Therefore, among A through D, the answer is".

**Data Contamination.** Both SciInstruct and the evaluation benchmarks fall within the science domain. To minimize any potential data contamination and strengthen the integrity of our results, we ensure that the training set used to construct SciInstruct was not derived from the test sets utilized in our evaluation. In other words, there was no overlap between the SciInstruct and the evaluation benchmarks used in our experiments.

   Model & GSM8K & MATH & Mathematics & SAT-Math & MMLU-Math & CEval-Math & Avg. Math \\   \\  GPT-4 & 92.00 & 42.50 & - & 95.00 & - & 53.91 & - \\ GPT-3-5-turbo & 80.80 & 34.10 & - & 70.90 & - & 40.81 & - \\ Claude-v1.3 & - & - & - & - & - & 37.66 & - \\   \\  LLaMA-2.78 & 14.60 & 2.50 & 6.00 & 26.80 & 29.80 & 30.00\({}^{}\) & 18.28 \\ Galactica-6.7B & 10.20 & 2.20 & 4.60 & 17.50 & 28.00 & 14.48\({}^{}\) & - \\ WizardsMult-7B & 54.90 & 10.70 & 9.30 & 25.40 & 31.10 & - & - \\ MAMmTM (C/O-7) & 50.50 & 10.40 & 9.20 & 32.70 & 39.00 & - & - \\ MetaMun-7B & 66.50 & 19.80 & - & 41.00 & 44.00 & - & - \\ MAMmTM 8t MeMatch-7B & 66.30 & 24.10 & 18.30 & 41.40 & 44.40 & - & - \\ ChatGI2M-6B & 25.85 & 56.90\({}^{}\) & 14.30\({}^{}\) & 39.55\({}^{}\) & 38.91\({}^{}\) & 36.67\({}^{}\) & 27.03 \\ ChatGI2M-6B-Base & 31.54 & 7.84\({}^{}\) & 17.10\({}^{}\) & 34.55\({}^{}\) & 40.45\({}^{}\) & 32.22\({}^{}\) & 27.28 \\ ChatGI3M-6B & 29.05 & 9.92\({}^{}\) & 11.60\({}^{}\) & 39.09\({}^{}\) & 41.07\({}^{}\) & 21.11\({}^{}\) & 25.31 \\ ChatGI3M-6B-Base & 29.73 & 25.38\({}^{}\) & 29.30\({}^{}\) & 55.91\({}^{}\) & 31.33\({}^{}\) & 40.00\({}^{}\) & 42.56 \\
**SciGR**(ChaChaGLM3-6B-Base) & 73.62 & 25.18 & 31.80 & 65.46 & 49.38 & 50.00 & **49.24** \\ LLma3-3B-Instruct (zero-shot) & 73.5\({}^{}\) & 20.76\({}^{}\) & 45.0\({}^{}\) & 55.64\({}^{}\) & 54.52\({}^{}\) & 16.67\({}^{}\) & 27.07 \\ Llama3-8B-Instruct (free-shot) & 63.38\({}^{}\) & 30.00\({}^{}\) & 22.30\({}^{}\) & 57.27\({}^{}\) & 55.24\({}^{}\) & 18.89\({}^{}\) & 41.18 \\ + **S

### Main Results and Analysis

Table 4, Table 5, and Table 6 present experimental findings for scientific and mathematical benchmarks, as well as general tasks. The results show that training on the provided SciInstruct benefits reasoning tasks, improving performance in scientific reasoning (i.e., CEval-Sci, SciEval, SciBench, MMLU-Sci) and transferring to mathematical tasks. Such performance improvement is consistent with different scales of based model parameters, across 6B and 32B. In addition, SciGLM's performance improvement in scientific reasoning does not sacrifice its general language understanding capabilities. As shown in Figure 13 in A.9, on standard tasks like MMLU and CEval, SciGLM even achieves slight performance improvement. On code generation like MBPP, the performance is slightly lower but is overall consistent. As shown in Figure 15 in A.10, we present a statistics problem in SciBench that is accurately solved with the SciGLM (32B).

## 4 SciInstruct Analysis

**Influence of Data Mixture.** We further explore how the diverse subjects within the SciInstruct mixture affect downstream tasks when training the SciGLM-6B model. By employing a Leave-One-Out strategy, i.e., omitting one subject at a time from the dataset and retraining, we assess the significance of each subject based on the performance impact across various tasks. As shown in Figure 6, we have an interesting finding: each subject contributes tasks that are not restricted to its immediate domains. For instance, Physics and Chemistry data significantly aid in CEval-Math tasks, while Math and Formal Proof improve SciBench performance. This shows that our mixture dataset enables LLMs to acquire some general reasoning skills for solving scientific questions instead of merely overfitting to a certain task distribution, and achieving universal improvement on different downstream tasks.

**Influence of Data Scaling.** One central question for instruction dataset creation is how many samples are needed for a model to learn specific skills . Prior works  have shown that for dialogue tasks, as few as 1000 high-quality instruction samples can lead to significant improvements. We're interested in analyzing data scaling law for scientific problem-solving. Through retraining the model with varying data proportions and analyzing the outcomes in science and math, as shown in Figure 7, one interesting pattern we find is initial data augmentation of 10% yields improvements, but further additions show no significant gains until surpassing a 50% threshold. We hypothesize that the early gains are due to that finetuned LLM learning basic reasoning and task formatting, which requires fewer instruction data (less than 30k). Advancing to more complex skills, such as equation deduction, necessitates a larger dataset for effective learning and generalize. Future research on improving data quality could potentially lower the data requirement for LLM skill learning.

**Pass@\(K\) Analysis on Sample Diversity.** One interesting observation of LLM reasoning is that with non-zero temperature and sampling multiple times, even for those very hard questions, LLM still has a high chance of providing a correct answer. Pass@\(K\) is widely used for code generation [40; 41] and

   Model & MMLU & CEval & MBPP & Avg. \\  GPT-4 & 86.40 & 68.70 & 83.00 & 79.37 \\  CurGLM-6B-Base & 61.32 & 67.09 & 55.80 & 61.40 \\ 
**SciGLM (6B-Base)** & 61.38 & 67.16 & 45.00 & 57.35 \\  CurGLM-32B-Base & 60.08 & 79.94 & 58.20 & 69.06 \\ 
**SciGLM (32B-Base)** & 70.08 & 79.64 & 56.60 & 68.78 \\   

Table 6: **Results on general language understanding tasks**. Fine-tuning does not sacrifice most language tasks and only drops a bit on the code generation task.

Figure 6: **Influence of different domains in fine-tuning data towards target tasks**. Weight is calculated by (Acc. (SciGLM) - Acc. (Exclude subjects) / Acc. (SciGLM)) under the leave-one-out subject setting.

math reasoning . To analyze whether our SciInstruct can really improve the general reasoning, we simulate different Pass@\(K\) values as shown in Figure 8. We use fine-tuned SciGLM (32B) and ChatGLM (32B) to generate \(N K\) (in this paper, \(N=30\) and \(K 25\)) solutions per question, allowing for a more accurate examination of the LLM's true pass rate on that question. We find fine-tuning does not influence the sample diversity. SciGLM (32B) with \(K\)=25 on SciBench and \(K\)=3 on SciEval can achieve comparable performance to GPT-4, showing the potential of our fine-tuned model to achieve better results. We hypothesize that high-quality and diverse reasoning data indeed lead the model to good behavior/skills for analyzing and solving hard scientific problems instead of overfitting the training set, showing the general usability of our self-annotated SciInstruct dataset.

## 5 Conclusion

In this work, we present a self-instructive annotation framework to create a high-level and high-quality dataset, SciInstruct, to enrich the scientific knowledge of LLMs. Using SciInstruct, we train three LLMs, which significantly improve many scientific and mathematical benchmarks over the base models and outperform many state-of-the-art LLMs that have an order of magnitude more parameters. Our research underscores the significance of diverse training data as well as LLM self-annotation and correctness for enhancing general reasoning capability, even for hard domains like science.

### Limitation

In this section, we discuss more limitations during the research of SciInstruct.

**Scale of Dataset and Model.** Even though our training dataset has expanded to approximately 254k, improving model performance still necessitates access to an even larger dataset. Our model's experimental outcomes are carried out at 6\(\)8B and 32B parameters, leading to a relatively better performance. However, it's important to note that these performances are constrained by the model's scale. Moving forward, it's worth exploring the potential benefits of leveraging larger-scale datasets and models to further improve performance.

**Using Data Classifier to Enhance the Generation of Models.** In line with what was discussed in Section 2.3, we employ an instruction-quality classifier to boost the instruction quality, yielding improved performance as shown in Table 2. However, we anticipate that the instruction-quality classifier, also referred to as the reward model, could provide even greater benefits. One particular avenue of improvement could be bootstrapping data to improve the ability of the base model.

### Broader Impact

**Positive impact.** This paper aims to construct high-level and high-quality instruction to improve the scientific reasoning capability of LLMs, which helps LLMs to better give the answers to questions at the college level. Collecting diverse instructions, annotating self-reflective instructions, and filtering out low-quality instructions provide researchers insights to prepare training datasets.

**Negative impact.** A drawback of this work is that the scale of the training dataset and model is relatively small, and we can address this by bootstrapping a more large training dataset. We believe that the benefits of data generation manner outweigh the downside.

Figure 8: **Evaluating Pass@\(K\) on SciBench (Quantum Chemistry) and SciEval.** All samples are generated at temperature 1.0. Results show that our instruction tuning does not influence the sample diversity, and increases the performance even with large \(K\).

Figure 7: **Performance improvement over different scale of instruction data. The x-axis represents the proportion of instruction per domain, and the y-axis represents the relative performance compared with SciGLM trained with the whole set.**

#### Acknowledgments

This work is supported by the NSFC 62276148, NSFC for Distinguished Young Scholar 62425601, a research fund from Zhipu, New Cornerstone Science Foundation through the XPLORER PRIZE and Tsinghua University (Department of Computer Science and Technology) - Siemens Ltd., China Joint Research Center for Industrial Intelligence and Internet of Things (JCIIOT).