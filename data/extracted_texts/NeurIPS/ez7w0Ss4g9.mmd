# How JEPA Avoids Noisy Features: The Implicit Bias of Deep Linear Self Distillation Networks

Etai Littwin Omid Saremi Madhu Advani

Vimal Thilak Preetum Nakkiran Chen Huang Joshua Susskind

###### Abstract

Two competing paradigms exist for self-supervised learning of data representations. _Joint Embedding Predictive Architecture_ (JEPA) is a class of architectures in which semantically similar inputs are encoded into representations that are predictive of each other. A recent successful approach that falls under the JEPA framework is self-distillation, where an online encoder is trained to predict the output of the target encoder, sometimes using a lightweight predictor network. This is contrasted with the _Masked AutoEncoder_ (MAE) paradigm, where an encoder and decoder are trained to reconstruct missing parts of the input in the data space rather, than its latent representation. A common motivation for using the JEPA approach over MAE is that the JEPA objective prioritizes abstract features over fine-grained pixel information (which can be unpredictable and uninformative). In this work, we seek to understand the mechanism behind this empirical observation by analyzing the training dynamics of deep linear models. We uncover a surprising mechanism: in a simplified linear setting where both approaches learn similar representations, JEPAs are biased to learn high-influence features, i.e., features characterized by having high regression coefficients. Our results point to a distinct implicit bias of predicting in latent space that may shed light on its success in practice.

## 1 Introduction

Representation learning has arguably been one of the most promising prospects and longest-standing goals of deep learning. In simple terms, representation learning refers to the process of automatically discovering and extracting meaningful features or representations from raw data. The learned representations can then be used to solve various downstream tasks, such as classification, and regression, or as general-purpose representations in robotics and embodied agents. In recent years, the practice of representation learning has witnessed remarkable strides, particularly in the domain of Self-Supervised Learning (SSL) . The SSL paradigm has garnered significant attention due to its ability to exploit vast amounts of unlabeled data, freeing practitioners from the burden of expensive annotation efforts. While many SSL approaches have been proposed in the literature, two paradigms have emerged as particularly successful, driving the bulk of the recent breakthroughs in the field:

Masked AutoEncoders (MAE)The original MAE  and its successive variants (e.g. ) introduced a training objective that seeks to reconstruct missing pieces of data from its partially masked input. This approach, while simple, has proven efficient and scalable . Formally, the MAE objective uses an encoder function \(f_{W}(X):^{d}^{d}\) and a decoder function \(g_{V}(f):^{d}^{d}\) to predict a target \(y\) given input \(x\):

\[_{}=_{x,y p(x,y)}\|g_{V}(f_{W}(x ))-y\|^{2},\] (1)where we define a joint distribution over inputs and targets \(p=p(x,y)\) with \(x,y^{d}\). Here we assume tied dimensions for simplicity.

In general, we can think of \(x\) and \(y\) as different inputs sharing the same underlying semantic information. One common practice is to let the input \(x\) correspond to a partially masked input, and the target \(y\) corresponds to the masked-out portions of the input. Note that a critical design choice of the MAE objective is that the loss penalizes the reconstruction error directly in the target's input space. This implies that, as perhaps some parts of the target \(y\) cannot be predicted given \(x\), the quality of the encoding \(f_{W}\) crucially depends on how \(x,y\) are sampled.

Joint Embedding Predictive Architectures (Jepa)The Jepa family of models does not attempt to predict \(y\) directly and instead opts to predict the latent representation of \(y\) given by some encoding function. When the encoder function for \(x\) and \(y\) is shared, a simple Jepa objective is given by:

\[_{}=_{x,y p(x,y)}\|g_{V}(f_{W}( x))-f_{W}(y)\|^{2}.\] (2)

Unlike the objective in (1), the objective given by (2) is susceptible to collapse: it can be minimized trivially by employing an encoder and decoder that map all inputs to the same vector. Contrastive methods  avoid this issue by including an additional loss term that pushes apart representations for negative pairs (two different samples). The drawback to this approach is that it can require comparing a sample to many negative examples to work effectively. This has led to a rise in the popularity of non-contrastive SSL. With non-contrastive methods, it becomes imperative to implicitly or explicitly bias the representation mapping \(f_{W}(x)\) to avoid its collapse to a trivial solution (e.g., \(f_{W}(x)=0,\  x\)). In this paper, we leverage the widely adopted _StopGrad_ operator, preventing the flow of gradients through the \(f_{W}(y)\) branch. Prominent examples of this foundational architecture include BYOL , data2vec , SimSiam  and the more recent I-JEPA  and V-Jepa  models. Consequently, the Jepa models under consideration fall under non-contrastive self-distillation methods. We will refer to such methods as Jepa for the remainder of this paper.

While both paradigms have found practical use, characterizing the implicit bias of each remains a major research question. Recently, it was shown in  that in the vision domain, perceptual features mostly reside in the data subspace which explains a relatively small portion of the observed variance. This suggests that reconstruction losses are sub-optimal for perception tasks since they tend to prioritize learning subspaces that explain the bulk of the input variance. On the other hand, the Jepa approach appears to be more suitable for perception tasks: empirically, Jepa often achieves better downstream classification performance with fewer optimization steps .

In this work, we seek to uncover the mechanisms behind these observations by analyzing tractable deep linear neural networks. To that end, we introduce a fully solvable toy model admitting a complete characterization of its training dynamics for both objectives. Our analysis is inspired by previous work on the greedy learning dynamics of gradient descent, and SSL in particular . However, going beyond previous work, here we present a more nuanced analysis showing how the greedy nature of learning is affected by the choice of objective, model, and the properties of the data. Our analysis reveals a distinct implicit bias of JEPAs: the propensity of the Jepa objective towards learning influential features, defined as features with a large regression coefficient, a property for which the MAE objective is mostly agnostic. Moreover, we show that this distinct bias takes hold only when the encoder is deep and diminishes substantially when the encoder is shallow. This allows JEPAs to steer away from the top subspace of the data which explains its observed variance, and focus on the more semantically rich subspace. Our work therefore uncovers a fundamental implicit bias, shedding light on both the advantages and drawbacks of predicting in latent space.

To summarize, our specific contributions are:

1. We analytically solve the learning dynamics of Jepa and MAE, in the tractable theoretical setting of deep linear networks.
2. In this setting, we prove a significant difference between Jepa and MAE: informally, Jepa prioritizes "influential features" (features which are most informative in prediction), whereas MAE only prioritizes highly covarying features (even if they are noisy and thus less informative).
3. We show that our theoretical setting is rich enough to capture two more realistic generative processes, based on static and dynamic linear models.

Our results help clarify the precise advantages of joint-embedding architectures, and complement the recent results of  on limitations of reconstruction-based methods.

#### Preliminaries

In this paper, we consider linear over-parameterized encoders and a shallow linear decoder. This tractable setting allows the non-linear training dynamics [24; 25; 26; 27] of the deep linear encoder network to be solved exactly under a certain set of assumptions. Let \(\{W^{a}\}_{a=1 L}^{d d}\) and \(V^{d d}\) denote the set of weights for a linear encoder \(f(x)\) of depth \(L\), and that of a linear decoder \(g(x)\) respectively, and consider the linear parameterization

\[=_{a=1}^{L}W^{a},\ \ f(x)=x,\ \ g(f(x))=Vf(x),\] (3)

where \(_{a=1}^{L}W^{a}\) indicates the conventional right-to-left product of matrices. As for the input data distribution, we will take \(x,y\) to be samples from a centered distribution with covariances:

\[[xx^{}]=^{xx},\ \ [yy^{}]=^{yy},\ \ [xy^{}]=^{xy}.\] (4)

For the purpose of deriving learning dynamics, we will later assume that \(^{xx},^{xy},^{yy}^{d d}\) are diagonal. Let \(()\) denote the StopGrad operator applied on \(\). Thus, the JEPA and MAE objectives become

\[_{}=_{x,y p(x,y)}\|Vx- ()y\|^{2},\ \ \ _{}=_{x,y p(x,y)}\|Vx-y \|^{2}.\] (5)

We will denote the diagonal elements of the diagonalized covariance and correlations matrix by \(_{i}^{2}=_{ii}^{xx}\) and \(_{i}=_{ii}^{yx}\) respectively, and let \(_{i}=}{_{i}^{2}}\) denote the regression coefficients.

To motivate our theoretical analysis, we next show empirically that depending on the particular values of \(\{_{i}\}\) and \(\{_{i}\}\), optimizing the objectives in eq. (5) via gradient descent may learn different encoders entirely.

## 2 The Implicit Bias of JEPA

Before presenting the theoretical results, we describe a simple experiment using the setup in section 1 which will flesh out a surprising key difference between the JEPA and MAE objectives in eq. (5).

Figure 1: **Deep linear model trained using the MAE and JEPA objectives eq. (5). Features indices (\(x\)-axis) are organized s.t. \(_{i}\) is monotonically increasing and \(_{i}\) is (a) - (c) monotonically increasing or (d) - (f) monotonically decreasing. (b),(c): Both objectives learn features in the same order given distribution 1. In (e),(f) the MAE objective maintains the same learning order as in (b) on distribution 2, however, the JEPA objective reverses the learning order, due to sensitivity to \(_{i}\).**

First, note that each coordinate \(x_{i}\) of the input data represents an independent feature characterized by the covariance and regression coefficient \(_{i},_{i}\), which we refer to as _feature parameters_. Let \(e_{i}^{d}\) denote the \(i\)-th standard basis vector. We say that an encoder \(\) has learned the \(i\)-th feature if its projection on \(e_{i}\), namely \(\|e_{i}\|\) is large. Intuitively, one should expect that if \(_{i}\) is small, \(\|e_{i}\|\) should be small post training as well. Note that this should not be overly surprising since, at least in the case of the MAE objective, the global minimizer is achieved with the regression coefficients \(V=^{yx}(^{xx})^{-1}\). In this investigation, however, we seek an understanding of the dynamics of training, by studying how the MAE and JEPA objectives prioritize different features in training, based on their feature parameters.

To this end, we track how the projections \(\|e_{i}\|\) evolve during training for each component \(i\) by training models on Gaussian data with feature parameters \(_{i},_{i}\) according to the following two distributions (see Figure 1(a) and (d) for an illustration):

1. **Distribution 1**: Set both \(\{_{i}\},\{_{i}\}\) to be monotonically increasing (i.e \(_{i>j},\;_{i}>_{j},_{i}>_{j}\)).
2. **Distribution 2**: Set \(\{_{i}\}\) to be monotonically increasing, and \(\{_{i}\}\) to be monotonically decreasing.

For each distribution, we initialize all the weights with a Gaussian initialization and train the models using both the MAE and JEPA objectives using gradient descent on batches of randomly sampled data. We illustrate the dynamics of training by measuring the magnitude of the encoder components \(\|e_{i}\|\) during training in Figure 1. On the first distribution, we observe clear greedy learning dynamics for both objectives, where components with a higher \(_{i}\) are learned earlier on in training. This behavior in greedy settings is in line with the findings in  which showed the same behavior in similar settings. However, on the second distribution, we observe a distinction: while the MAE objective retains its order of learning as in distribution 1, the JEPA objective exhibits a complete inversion of the learning order where features with a higher \(_{i}\), rather than a high \(_{i}\) are learned first. This implies that in practical scenarios with a finite training budget, JEPA and MAE objectives can potentially learn entirely different features with little to no overlap, as illustrated in Figure 1. To better understand these phenomena, we turn to a dynamical analysis of a simple model that fully recovers our observations.

## 3 Dynamical Analysis

In the following theoretical analysis, we train the corresponding models employing objectives eq. (5) by optimizing all weights simultaneously starting at some initial weights configuration \(\{W^{a}(0)\}_{a=1...L}\) and \(V(0)\), using the gradient flow equations

\[_{a},\;\;^{a}=-_{W^{a}},\;\;\;=-_ {V},\] (6)

where \(\) is either JEPA or MAE objective. Generically, the two systems of ODEs are intractable without any simplifying assumptions. A special class of initializations discussed below permits analytical treatment of the problem, allowing us to derive a depth separation result indicative of differences in the inductive bias of the two objectives. In the following, we describe our set of assumptions on the encoder and decoder weights at initialization, and data distribution formally:

**Assumption 3.1**.:
1. \(^{xx},^{yx}\) are diagonal and positive definite.
2. All weight matrices \(W^{a}(0),a=1...L,V(0)\) are initialized as \(W^{a}=^{}U^{a},V(0)=^{}U^{v}\) where \(\{U^{a}\}_{a},U^{v}\) are orthogonal matrices, and \(0< 1\) is an initialization scale.
3. For JEPA, we set \(U^{v}\) to be diagonal, and for MAE we set \(U^{v}\) such that \(U^{v}(_{a}U^{a})\) is diagonal.

A few notes on assumption 3.1 before stating our results. The assumption on orthogonal weights implies that the weights are "balanced" at initialization, as is typically assumed in prior works on deep linear networks [28; 22]. The extra technical constraint on the uniformity of the initialization eigenvalues is necessary to deal with the non-uniformity of the data eigenvalues and approximately holds in typical initializations with large Gaussian matrices. Likewise, we initialize \(V\) differently for JEPA and MAE to achieve alignment of eigenvectors between the model and data at initialization. Although somewhat unrealistic, our special initialization schemes will prove informative to the general case. Applying assumption 3.1 to JEPA and MAE objectives in eq. (5) respectively, and using the gradient flow equations in eq. (6), one arrives at the following theorem:

**Theorem 3.2** (ODE Equivalence).: _Suppose \(\{W^{a}\}_{a=1 L}\) and \(V\) are initialized according to assumption 3.1. Let \(_{i}=\|e_{i}\|\), where \(e_{i}\)s are the standard basis. Furthermore, assume the JEPA objective in eq. (5) is optimized using gradient flow according to eq. (6). Then, we have_

\[:}_{i}(t)=_{i}(t)^{3-}_{i}-_{i}(t)^{3}_{i}_{i}^{-1}.\] (7)

_Similarly, the MAE objective eq. (5) is optimized using gradient flow according to eq. (6) yielding:_

\[:}_{i}(t)=_{i}(t)^{2-}_{i}- {w}_{i}(t)^{3}_{i}_{i}^{-1}.\] (8)

Remarkably, the only difference between the JEPA and MAE dynamical equations is in an exponent. For a derivation of these dynamical equations see appendix B.1. Although the assumption 3.1 enables us to characterize the learning dynamics fully, we will show via numerical simulations (see section 4), that the observations made in this paper will not change qualitatively under the more general and realistic initialization. Since ODEs for each component \(i\) decouple, for the remainder of this paper, we drop the subscript \(i\) and consider a single ODE for \(\) parameterized by the encoder and data feature parameters \(\{,\}\). A direct corollary to theorem 3.2 is that the training dynamics of MAE for \(L>>1\) as described in eq. (8) approach those of JEPA in eq. (7) for \(L=1\). Additionally, a depth-dependent difference is apparent in the fixed-point solutions between the two objectives. This is formalized in the following corollary:

**Corollary 3.3**.: _Let \(_{}(t,L),_{}(t,L)\) denote the solutions to eqs. (7) and (8) for depth \(L\) encoders and at time \(t\), given initial condition \((0)=\), we have_

\[_{}(,L)=^{},\;\;\;_{}(,L)=^{L}.\] (9)

_In addition, it holds that the dynamics of a 1-layer JEPA model match an infinite-depth MAE_

\[_{L^{}}_{}(t,L^{})=_{}(t,1).\] (10)

When \(L>1\), JEPA will suppress the encoding of noisier directions with a lower regression coefficient and this suppression increases with the network depth. At large depths, the encoder becomes low-rank to the point a single dominating eigenvalue corresponding to the feature with the largest regression coefficient remains. Note that corollary 3.3 also indicates that the JEPA solution for \(L>1\) is not reachable by the MAE objective, and vice versa. While interesting in itself, the difference between the two objectives runs deeper than their fixed-point solutions and is found by analyzing the training dynamics.

In the small initialization regime, the evolution of the weights during training for both objectives exhibits incremental learning dynamics, in which the encoder learns features progressively. We define the _critical time_, denoted as \(t^{*}\), to be the time it takes for the encoder projection \(\) to reach a positive finite-but-close-to-1 fraction \(p\) of its final fixed point value. It is a quantity we wish to track since it captures an important data-dependent difference between JEPA and MAE training dynamics, concerning the order in which feature learning proceeds in the encoder:

\[(t^{*})=p().\] (11)

The dependence of this definition of the critical time on a choice for \(p\) introduces a degree of arbitrariness in the actual value of \(t^{*}\). However, as long as \(p\) is not too close to \(1\) or \(0\), the leading order in the Laurent expansion for \(t^{*}=t^{*}(,,)\) in \(\), as we show in section 3.1, is not affected by the specific values of \(p\). The temporal ordering of \(t^{*}(,)\)s across all features, allows us to observe which feature takes priority in learning according to each objective.

### Critical Time Analysis

To derive the critical time we first solve the dynamical equations in eqs. (7) and (8) in the following. The JEPA dynamics given by eq. (7) can be solved implicitly in a closed form as given by theorem B.7 with a full proof in the supplementary material section.

The closed form solution implicitly describes the full trajectory of \((t)\) for any time \(t\). From this solution, it is clear that the encoder projection in the standard basis \(\), starting from its initial value \(\) at \(t=0\), will reach its corresponding asymptotic value at \(t=\). The critical time \(t^{*}\), defined in (11), is the time scale for which \(\) exits the dynamical region near \(t=0\) and reaches a finite fraction of its asymptotic value. Theorem 3.4 gives the critical time for JEPA with arbitrary depth \(L\) encoders:

**Theorem 3.4** (Jepra critical time).: _The critical time \(t^{*}\) in the small initialization regime \((t=0)= 1\) for JEPA is given by_

\[t^{*}_{}=_{n=1}^{2L-1} ^{}}+(),\] (12)

_as long as \(p\) is not too close, as defined in eq. (81), to zero or one._

In the small initialization regime, the JEPA solution given in theorem B.7 then describes a step-wise learning process of features where each feature is learned in the time scale given by eq. (12). We also derive an analogous theorem B.10 applied to the MAE objective. For technical reasons, we assume \(L>1\) in the following and provide the equivalent theorems for \(L=1\) in Appendix B.4. From the MAE dynamics summarized in, eq. (12) we arrive at the following result for MAE critical time:

**Theorem 3.5** (MAE critical time).: _The MAE critical time \(t^{*}\) in the small initialization regime of \((t=0)= 1\) and \(L>1\) is given by_

\[t^{*}_{}=}}+(1).\] (13)

_as long as \(p\) is not too close, as defined in eq. (109), to zero or one._

### Comparing Learning Dynamics: JEPA vs. MAE

theorem 3.4 and theorem 3.5 reveal a crucial distinction between the two objectives. To understand how each objective prioritizes features, we note the functional form of the leading orders in \(\) in the critical time \(t^{}\). For MAE, we observe that the leading order term depends principally on the inverse of \(\). Crucially, we note that the next to leading order term (NLO) is small in comparison for any encoder depth \(L\). This implies step-wise learning dynamics where the step ordering is predominantly set by the feature covariance \(\). Conversely, features with identical \(\) will be learned in the same

Figure 2: **Simulations of the JEPA and MAE equivalent ODEs (eqs. (7) and (8)). Each curve represents a numerical simulation of the corresponding ODE, for different values of \(,\). (a), (d) darker curves correspond to higher \(\) and \(=1\). (b), (e) darker curves correspond to higher \(\) and \(=1\). As can be seen, both objectives exhibit greedy learning dynamics with respect to \(\), however, only JEPA exhibits greedy dynamics with respect to \(\). (c), (f) darker curves correspond to higher \(\) but lower \(\). In this case, the order of learning is inverted between the objectives due to the different trends in \(,\).**

timescale. In the case of JEPA, we observe that the leading order term is again principally a function of \(\), however, the NLO term, which depends inversely on the regression coefficient \(\), is increasingly large with the encoder depth \(L\). That is, features with identical \(\) but with different \(\) will be learned in different timescales, where the feature with the highest regression coefficient is prioritized. Moreover, the priority towards large \(\) increases with depth. This implies a meaningful and distinct implicit bias of the JEPA objective towards learning features with a high regression coefficient, increasingly so with depth. This is summarized in the following corollary:

**Corollary 3.6**.: _Let \(t^{*}_{}(,,),t^{*}_{}(,,)\) denote the critical time for the JEPA and MAE objectives given feature parameters \(,,\). WLOG assume \(^{}>\), let \(_{}=-}\). Then, it holds that_

\[_{}(,,)}{t^{*}_{}(,,^{{}^{}})}=1+_{}^{ {1}{L}}+(^{}),_{}(,,)}{t^{*}_{}(,,^{{}^{}})}=1+ ^{}\] (14)

We note the strong dependency on depth \(L\) and \(_{}\) in the JEPA case through the term \(_{}^{}\) in eq. (14), indicating greedy learning dynamics with respect to \(\), a property absent in the case of MAE. We next turn to empirical simulations to test the validity of our theory, as well as its generalizability in less stringent settings.

## 4 Numerical Experiments

We also conduct experiments directly verifying our theoretical findings. To that end, we numerically simulate the ODEs in eqs. (7) and (8) for different distributions and depths starting from a small initialization. In Figure 2, we fix the encoder depth \(L=5\) while varying the data parameters. As predicted by the theory, we observe a clear greedy learning process for both objectives. However, only the JEPA objective exhibits greedy dynamics with respect to \(\), where components with larger \(\) are learned first. Moreover, we observe that the JEPA objective can reverse the feature learning order relative to MAE when \(\) and \(\) have the opposite trends, qualitatively reproducing the observation in section 2. In Figure 4, we run further simulations by varying the depth parameter \(L\). As predicted, we observe that deeper encoders introduce a wider temporal spread between learning features with the same \(\) but different \(\), in contrast to MAE which learns all features at the same time independent of the depth. Moving beyond gradient flow and the assumptions on initialization, in Figure 5 we train randomly initialized deep linear neural networks using stochastic gradient descent on randomly sampled batches of Gaussian data with varying values for \(,\), using both objectives. As is evident, our results carry over to this setup as well, where greedy learning of features can be seen with an objective dependent ordering according to \(,\).

### Linear Generative Models

We consider generative models which satisfy our simultaneously-diagonalizable assumption (in the large sample limit). This is a setting in which our theory holds exactly and datasets with this property allow both JEPA and MAE models to learn the same modes, although not necessarily in the same

Figure 3: **Temporal model (15): (a) \(v_{1},v_{2}\), (b) temporal dynamics of flickering \(u\), autocorrelation \(_{1}=0.99\) and \(_{2}=0.95\), (c) \(^{xx}\) empirical covariance with 500k samples, (d) \(^{xy}\) empirical correlation, (e) predictions versus simulations of parameters \(\) and \(\) varying \(_{10}(T)\). \(\) decreases from feature \(1\) to \(2\) whereas \(\) increase because noise added standard deviation decreases from \(1\) to \(0.5\). Note (e) show 2 standard deviation with \(10\) runs.**

order. We analyze the simplest linear generative models which still lead to non-trivial learning differences between JEPAs and MAEs for two settings: _random masking_ and a _temporal model_. The full details of these different settings can be found in Appendix C. In the first case, our views are random masks of the data, this corresponds to a static setting and we derive closed-form expressions for \(\) and \(\) (see C.1). For the remainder of this section, we will focus in on the temporal model due to its simplicity and intuitive interpretation.

The temporal model considers the scenario where our views \(x,y\) correspond to our data \(z^{1},...,z^{T}\) at two consecutive times: \(x=z^{t}\) and \(y=z^{t+1}\). The goal is to predict the next frame from the previous one (this setting is consistent with comparing VideoMAE and V-JEPA video models). We now define a simple linear model corresponding to combining independently time-varying images \(\{v^{a}\}\) to the model simultaneously with random noise. We can write this mathematically as:

\[z_{i}^{t}=_{a}u^{a}(t)v_{i}^{a}+_{i}^{t}, t=1,...T.\] (15)

Note that we are allowing the amplitude of the images to vary with time according to a scalar function \(u(t)\); we make this choice to study the simplest non-trivial temporal variability. The full details of the setup and derivation for simultaneous diagonalizability can be found in Appendix C.2, where we derive the correlation coefficient for each feature:

\[_{a}=_{a}\|v^{a}\|^{2},_{a}=\|v^{a}\|^{ 2}}{_{a}^{2}+\|v^{a}\|^{2}}.\] (16)

Here \(_{a}\) represents the noise amplitude applied to mode \(a\). See Figure 3 for a demonstration of our theory matched to simulations and an example of higher correlation modes with lower correlation coefficient in Figure 3(e). Additionally, a simulation demonstrating an approach to simultaneously diagonalizable empirical covariance and correlation \(^{xx}\) and \(^{xy}\) can be found in Figure 6. Combining (16) with our theory (14) demonstrates how JEPAs can be slower to learn noisy features in a way that MAEs are not. JEPA will also learn noisy features by converging to lower amplitude weights (9), making such features potentially less likely to be used downstream from the embedding.

## 5 Related Work

**Self-supervised learning.** One successful SSL paradigm is MAE  and its variants [13; 14] which learn representations via input space reconstruction. However, recent works (e.g., ) show that such generative paradigms often learn uninformative features for perception, since MAE-like methods tend to learn the data subspace that explains the bulk of observed variance and can include unhelpful information for perception. By contrast, the JEPA paradigm [9; 10; 11; 18; 19] learns to predict the representations of similar image views in the latent space. Such objectives have been found to prioritize semantic features over detailed pixel information, leading to superior performance for perception tasks. To prevent the feature collapse of JEPA, stop-gradients, and other architectural tricks are widely used. More recent works propose techniques like spectrum modulation  and weight regularization . Another popular collapse prevention method is based on contrastive loss against negative image pairs [2; 5; 6; 11; 7]. In this work, we focus on MAE and JEPA methods, seeking to understand their implicit bias that leads to different training dynamics. To measure representation quality in JEPAs,  devised a metric that effectively counts the number of directions with a large regression coefficient in latent space. Our work can be seen as further motivation for this approach.

**Theoretical analysis of SSL.** There have been several recent works attempting to understand the success of non-contrastive SSL (JEPA) from various perspectives.  studies how the self-distillation in JEPA avoids representation collapse, finding the key role of eigenspace alignment between the predictor and input correlation matrix. The authors of  further provide a theoretical bridge between contrastive and non-contrastive objectives towards global and local spectral embedding methods respectively. It is shown that all these methods can be deployed successfully if the pairwise relations during SSL are aligned with the downstream task. While for MAE-based methods,  shows they tend to learn uninformative features by pixel reconstruction, unlike the JEPA objective that prioritizes semantic features. In this paper, we seek to understand the mechanism behind this empirical observation by characterizing the implicit bias of both methods using deep linear models.

**Learning dynamics in linear networks.** Deep linear neural networks have been used in [24; 25; 26; 27; 34; 28; 35; 36] to study the nonlinear dynamics of learning in various settings applying different assumptions, including the saddle point behavior as well as the step-wise behavior where distinct features are learned at different time scales. This is also in line with the observed greedy learning dynamics for modern architectures like Transformers [22; 23]. In the context of SSL, the authors of  observed a similar step-wise nature which corresponds to the learning of eigenmodes of the contrastive kernel \(^{yx}+^{xy}\). However, we go beyond this observation by characterizing the distinctive implicit bias of JEPA relative to MAE, i.e., they learn the same features in a step-wise fashion but not necessarily in the same order. Surprisingly, this difference in behavior is only apparent when the encoder network is deep. Our findings about JEPA are related to empirical observations in  which show JEPA-based methods focus on "slow features" that vary slowly over time. This is confirmed in our studies where JEPA-based methods will prioritize "slow features" for which the marginal distribution has the smallest variance.

## 6 Limitations

Our theory has several clear limitations. The theoretical results presented in this paper are restricted to conditions in assumption 3.1, which include a deep linear MLP with some restrictions on the initialization scheme, as well as a simultaneously diagonalizable covariances \(^{yx},^{xx}\). It is worth discussing the implications of these assumptions, and the generalizability of our results to more typical settings. On the model side, empirical simulations with deep linear models initialized using the default initialization  indicate our results qualitatively generalize to popular initialization schemes. However, a potentially stronger assumption we make is simultaneously diagonalizability of \(^{yx}\) and \(^{xx}\), which allows JEPA and MAE to learn the same features. We provide generative frameworks that adhere to this property, however we expect data distributions of interest to significantly deviate from this assumption. Moreover, the generalizability of our claims to fully practical scenarios yet remains unexplored, and precise characterization of the implicit biases pertaining to more general data distributions and architectures is a direction for future work.

## 7 Discussion

We have introduced toy models of two of the most popular self-supervised algorithms, namely JEPA and MAE, where we can completely characterize the learning process. We have uncovered a novel learning principle that separates the two objectives: while MAE focuses on highly co-varying features early in training, the JEPA objective focuses on highly influential features (indicated by high values of the regression coefficient). This implicit bias of the JEPA objective allows it to focus on features that are predictive, yet contain minimal noise, as measured by their variance across the training set. This new understanding of the learning dynamics of JEPA sheds light on recent empirical observations and opens the door to new avenues of research strengthening these results. Certainly, one must first generalize these results to non-simultaneously-diagonalizable data distributions, as well as the more imposing challenge of non-linear models. Still, a less than rigorous leap to practical settings may still provide valuable insights. For example, an implicit bias towards predictive yet low-variance features may explain the tendency of JEPA to learn more semantic features, which are inherently less noisy. Conversely, it may shed light on JEPs vulnerability to spurious or _slow_ features. Additionally, the feature prioritization distinction between the objectives may bear on the efficiency of JEPA in learning semantic features quickly, as these features tend to reside in the low variance subspace of the data, as shown recently . Finally, it is also worth investigating whether the mechanism uncovered here generalizes to other joint embedding architectures not relying on self-distillation.

## 8 Acknowledgments

We thank Dan Busbridge, Arwen Bradley, Stefano Cosentino, and Shuangfei Zhai for stimulating discussions and useful feedback on the research and writing of this paper.