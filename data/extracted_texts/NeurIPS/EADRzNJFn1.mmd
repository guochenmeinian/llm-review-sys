# TGB 2.0: A Benchmark for Learning on Temporal Knowledge Graphs and Heterogeneous Graphs

Julia Gastinger\({}^{1,2,6}\)1 Shenyang Huang\({}^{1,4}\)1 Mikhail Galkin\({}^{3}\) Erfan Loghmani\({}^{8}\)

Ali Parviz\({}^{1,9}\)1 Farimah Poursafaei\({}^{1,4}\)1 Jacob Danovitch\({}^{1,4}\)

**Emanuele Rossi\({}^{5}\) Ioannis Koutis\({}^{9}\) Heiner Stuckenschmidt\({}^{2}\)**

**Reinhaeh Rabbany\({}^{1,4,7}\)1 Guillaume Rabbseau\({}^{1,6,7}\)1Mila - Quebec AI Institute, \({}^{2}\)Mannheim University, \({}^{3}\)Intel AI Lab, \({}^{4}\)School of Computer Science, McGill University, \({}^{5}\)Imperial College London, \({}^{6}\)DIRO, Universite de Montreal, \({}^{7}\)CIFAR AI Chair, \({}^{8}\)University of Washington \({}^{9}\)New Jersey Institute of Technology**

Equal contributions

###### Abstract

Multi-relational temporal graphs are powerful tools for modeling real-world data, capturing the evolving and interconnected nature of entities over time. Recently, many novel models are proposed for ML on such graphs intensifying the need for robust evaluation and standardized benchmark datasets. However, the availability of such resources remains scarce and evaluation faces added complexity due to reproducibility issues in experimental protocols. To address these challenges, we introduce Temporal Graph Benchmark 2.0 (TGB 2.0), a novel benchmarking framework tailored for evaluating methods for predicting future links on Temporal Knowledge Graphs and Temporal Heterogeneous Graphs with a focus on large-scale datasets, extending the Temporal Graph Benchmark. TGB 2.0 facilitates comprehensive evaluations by presenting eight novel datasets spanning five domains with up to 53 million edges. TGB 2.0 datasets are significantly larger than existing datasets in terms of number of nodes, edges, or timestamps. In addition, TGB 2.0 provides a reproducible and realistic evaluation pipeline for multi-relational temporal graphs. Through extensive experimentation, we observe that 1) leveraging edge-type information is crucial to obtain high performance, 2) simple heuristic baselines are often competitive with more complex methods, 3) most methods fail to run on our largest datasets, highlighting the need for research on more scalable methods.

## 1 Introduction

Learning from graph-structured data has become ubiquitous in many applications such as recommendation systems [35; 70], knowledge base completion [55; 46] and molecular learning [58; 3]. Relational data often evolves over time and can contain multiple types of relations. These complex interactions and temporal dependencies can be captured by _multi-relational_ temporal graphs. In recent years, various approaches have emerged to predict future links in such graphs, notably for prediction on Temporal Knowledge Graphs (TKGs) [41; 47] and Temporal Heterogeneous Graphs (THGs) [38; 30]. These approaches capture the rich information from multi-relational data, forming distinct lines of research from those of single-relational temporal graphs [61; 50]. However, benchmarking on multi-relational temporal graphs faces two main challenges: _inconsistent evaluation_ and _limited dataset size_.

**Inconsistent Evaluation.** Evaluation for multi-relational temporal graphs faces significant challenges. Recently, it was shown that existing evaluation for TKGs has inconsistencies in a) the evaluation metrics, b) mixing multi-step and single-step prediction settings and c) using different versions of the same dataset . Similar inconsistencies have been observed in related areas such as link prediction on static knowledge graphs [67; 60], node and graph classification on static graphs [64; 13], and temporal graph link prediction . In addition, for link prediction on THGs, existing evaluation often includes a single random negative per positive edge [38; 72], leading to over-optimistic performances, inconsistent evaluation, and reducing performance differentiation between methods .

**Limited Dataset Size.** Existing evaluations are conducted on predominantly small-scale datasets. For example, commonly used TKG and THG datasets consist of less than two million edges and one million nodes [38; 30; 41; 39]. However, real world networks typically contain tens of millions of nodes and edges thus existing datasets rarely reflect the true scale of datasets in practice. In addition, significant efforts were made to design scalable graph learning methods for real applications which requires the availability of large scale datasets [27; 26; 28]. These challenges hinder meaningful comparisons between methods and the accurate assessment of progress, and hamper advancements in the field. Therefore, there is an urgent need for a public and standardized benchmark to facilitate proper and fair comparison between methods, accelerating research for multi-relational temporal graphs.

To address the aforementioned challenges, we present TGB 2.0, a novel benchmark designed for future link prediction on multi-relational temporal graphs. Building upon the foundations of the Temporal Graph Benchmark (TGB)  where only _single-relation temporal graphs_ are included, TGB 2.0 introduces _multi-relational temporal graph_ datasets. TGB 2.0 adds four novel TKG datasets and four novel THG datasets of varying scale, spanning five domains. Figure 1 shows the difference in scale of the novel datasets in TGB 2.0 when compared to existing ones. Figure 0(a) shows that TGB 2.0 TKG datasets (marked in orange) are orders of magnitude larger than existing ones in terms of number of nodes, edges and timestamps. Figure 0(b) shows that TGB 2.0 THG datasets (marked in green) are significantly larger than existing datasets, with thgl-myket dataset quintupling the number of edges and timestamps, while thgl-github has the most number of nodes to date. Additionally, TGB 2.0 provides an automated evaluation pipeline for reproducible and realistic evaluation on multi-relational temporal graphs. In TGB 2.0, the dynamic link property prediction task is treated as a ranking problem where multiple negative edges are ranked against the positive edge. For large datasets, we sample negative edges based on the edge type of the query, and thus closely approximate the complete evaluation where all negative edges are used. Overall, TGB 2.0 presents a benchmark for realistic, challenging, and reproducible evaluation on _multi-relational temporal graphs_ while providing an automated pipeline for dataset downloading, processing, evaluation as well as a public leaderboard. TGB 2.0 has the following main contributions:

* **Large and diverse datasets for multi-relational graphs.** We present four novel TKGs that are orders of magnitude larger than existing ones and four novel THGs that are significantly larger in number of nodes, edges and timestamps when compared with current ones.
* **Realistic and reproducible evaluation.** We provide an evaluation pipeline for multi-relational temporal graphs, which automates the dataset downloading, processing and benchmarking process

Figure 1: Existing benchmark datasets (yellow) vs. novel datasets in TGB 2.0 for TKG (a) marked in orange and THG (b) marked in green. Circle sizes correspond to the number of timestamps. TGB 2.0 datasets are significantly larger than existing datasets in number of nodes, edges and timestamps.

for seamless reproducibility. TGB 2.0 evaluation uses the ranking metric MRR and samples challenging negative edges based on the edge type information, hence providing realistic evaluation.
* **Experimental insights.** The main insight from our experiments is that for both THGs and TKGs, all methods (apart from heuristics) fail to scale to our largest datasets, highlighting the need for more research on scalable methods. Surprisingly, the heuristic baselines perform competitively with more sophisticated methods. On THG datasets, we observe that methods that leverage the edge type and node type information achieve strong performance. Finally, across TKG datasets, we observe a strong correlation between the recurrency degree of a given relation type and the performance of methods, suggesting large room for improvement on low recurrency relations.

**Reproducibility.** TGB 2.0 code and datasets are publicly available (see Appendix C for download links) and theTGB 2.0 website provides detailed documentation.

## 2 Preliminaries

**Temporal Knowledge Graphs:** A _Temporal Knowledge Graph_ (TKG) \(G_{K}\) is a set of quadruples \((s,r,o,t)\) with subject and object entities \(s,o V\) (the set of entities), relation \(r R\) (the set of possible relations), and timestamp \(t\). The semantic meaning of a quadruple \((s,r,o,t)\) is that \(s\) is in relation \(r\) to \(o\) at time \(t\). We also refer to quadruples as temporal triples, or simply as edges.

**Temporal Heterogeneous Graphs:** A _Temporal Heterogeneous Graph_ (THG) \(G_{H}\) is similarly a set of quadruples, \((s,r,o,t) E\) where \(s,o V\) are entities, \(r\) is the relation and \(t\) is a timestamp, _along with a node type function_\(:V A\). THG are equivalent to TKG with the addition that each node is assigned a fixed type (consistent over time) by the node type function.

**Temporal Graph Forecasting (Extrapolation):** Given a Temporal multi-relational Graph \(G_{K}\) or \(G_{H}\), Temporal Graph Forecasting or Extrapolation is the task of predicting edges for _future_ timesteps \(t^{+}\). Akin to (static) multi-relational graph completion, temporal multi-relational graph forecasting is approached as a ranking task . For a given query, e.g. \((s,r,?,t^{+})\), methods rank all entities in \(V\) as possible objects using a scoring function, assigning plausibility scores to each quadruple. In TGB 2.0, we focus on the temporal graph forecasting task.

**Time Representations.** There are two approaches for representing time in temporal graphs: (a) _discrete_, where graphs are modeled as snapshots \(G_{t}\) containing all edges appearing at time \(t\), and (b) graphs that can be conceptualized as a series of edges arriving at _continuous_ timestamps. In practice, TKGs are often represented as snapshots and TKG methods are tailored for discrete time representations [41; 39; 47; 66]. This choice is driven by the discrete nature of the data sources and the suitability of snapshot-based representations for downstream tasks. Therefore, we represent TKGs as snapshots in TGB 2.0. In comparison, THG data sources are often continuous in nature (recorded in second-wise interactions), while both discrete and continuous approaches for THGs are developed [38; 10; 72]. It is often argued that continuous representations preserve more information and can be converted to discrete while the reverse is not true . Therefore, the THG datasets are represented in the continuous format.

## 3 Related Work

**TKG Methods.** Most TKG forecasting methods utilize a discrete time representation, except for . Some methods integrate the message-passing paradigm from static graphs [63; 53] with sequential techniques [32; 41; 22; 23; 39; 45]. Other approaches combine Reinforcement Learning with temporal reasoning for future link prediction [40; 66]. Rule-based methods [47; 34; 52; 48; 44] employ strategies to learn temporal logic rules while others [76; 71; 75] combines a blend of different methodologies. More details are in Appendix H.1. For more comprehensive discussions on methods and applications of TKG, we refer to the surveys [8; 9; 43], and . For TKG forecasting, common benchmark datasets include YAGO , WIKI [36; 31], GDELT  and the Integrated Crisis Early Warning System (ICEWS) dataset . However, these datasets are orders of magnitude smaller than our TKG datasets in number of nodes, edges and timestamps. In tkgl-icews, we include the full ICEWS dataset  spanning 28 years when comparing to prior versions containing only one or a few years . Similarly, our tkgl-wikidata dataset is orders of magnitude larger than the existing WIKI dataset  in size of nodes, edges and timestamps.

**THG Methods.** THG methods can be categorized based on their time representation: discrete-time methods and continuous-time methods. Examples of continous time methods include HTGN-BTW  and STHN . HTGN-BTW  enables TGN  to accommodate heterogeneous node and edge types. STHN  utilizes a link encoder and patching techniques to incorporate edge type and time information respectively. Discrete-time methods include random walk based methods  and message-passing based methods. However, it is difficult to adapt discrete-time methods for continuous-time datasets. More details are provided in Appendix H.2. Common THG datasets such as MathOverflow , Netflix  and Movievies  are small and only contain a few million edges . Large datasets such as Dataset A and B from the WSDM 2022 Challenge and the TRACE and THEIA datasets  are only evaluated with one negative sample per positive edge, which is shown to lead to over-optimistic and insufficient evaluation . Here, we introduce the large tbl-myket dataset with 53 million edges and 14 million timestamps.

**Graph Learning Benchmarks.** The Open Graph Benchmark (OGB)  and the OGB large scale challenge  are popular benchmarks accelerating progress on static graphs. Recently, the Temporal Graph Benchmark was introduced for temporal graph learning, consisting of large datasets for single-relation temporal graphs . In this work, we introduce novel TKG and THG datasets, incorporating multi-relational temporal graphs into TGB. Recently, detailed performance comparison for deep learning methods on dynamic graphs are conducted in , however multi-relational temporal graph datasets were not included in the comparison. While efforts like  have highlighted evaluation inconsistencies in TKG, their study focuses on existing smaller-scale datasets where no novel evaluation framework were proposed. Moreover, recent findings by  reveal that a simple heuristic baseline outperforms existing methods on some datasets, thus underscores the necessity for comparison with baselines. In this work, TGB 2.0 includes four novel TKG and four novel THG datasets as well as a standardized and reproducible evaluation pipeline.

## 4 Datasets

TGB 2.0 introduces eight novel datasets from five distinct domains consisting of four TKGs and four THGs. We split all datasets chronologically into training, validation, and test sets, respectively containing \(70\%\), \(15\%\), and \(15\%\) of all edges in line with existing studies  and ensure that edges of a timestamp can only exist in either train or validation or test set2. We present the dataset licenses and download links in Appendix D. The datasets will be permanently maintained via Digital Research Alliance of Canada (funded by the Government of Canada).

**Dataset Details.** Here we describe each TGB 2.0 dataset in detail. Temporal Knowledge Graph datasets start with the prefix tkgl- while Temporal Heterogeneous Graph datasets start with tbl-. More details on the dataset collection process are in Appendix E.

tkgl-smallpedia. This TKG dataset is constructed from the Wikidata Knowledge Graph  where it contains facts paired with Wikipedia pages. Each fact connects two entities via an explicit relation (edge type). This dataset contains Wikidata entities with IDs smaller than 1 million. The temporal relations either describe point in time relations (event-based) or relations with duration (fact-based). We also provide static relations from the same set of Wikidata pages which include 978,315 edges that can be used to enhance model performance. The task is to predict future facts.

tkgl-polecat. This TKG dataset is based on the POLitical Event Classification, Attributes, and Types (POLECAT) dataset  which records coded interactions between socio-political actors of both cooperative or hostile actions. POLECAT utilizes the PLOVER ontology  to analyze new stories in seven languages across the globe to generate time-stamped, geolocated events. These events are processed automatically via NLP tools and transformer-based neural networks. This dataset records events from January 2018 to December 2022. The task is to predict future political events between political actors.

tkgl-icews. This TKG dataset is extracted from the ICEWS Coded Event Data  which spans a time frame from 1995 to 2022. The dataset records political events between actors. It is classified based on the CAMEO taxonomy of events  which is optimized for the study of mediation and contains a number of tertiary sub-categories specific to mediation. When compared to PLOVER ontology in tkgl-polecat, the CAMEO codes have more event types (391 compared to 16). The task is to predict future interactions between political actors.

tkgl-wikidata. This TKG dataset is extracted from the Wikidata KG  and constitutes a superset of tkgl-smallpedia. The temporal relations are properties between Wikidata entities. tkgl-wikidata is extracted from wikidata pages with IDs in the first 32 million. We also provide static relations from the same set of Wiki pages containing 71,900,685 edges. The task is to forecast future properties between wiki entities.

thgl-software. This THG dataset is based on Github data collected by GH Arxiv. Only nodes with at least 10 edges were kept in the graph, thus resulting in 14 types of relations and 4 node types (similar relations to ). The dataset spans January 2024. The task is to predict the next activity of a given entity, e.g., which pull request the user will close at a given time.

thgl-form. This THG dataset is based on the user and subreddit interaction network on Reddit . The node types encode users or subreddits, the edge relations are "user reply to user" and "user -post" in subreddits. The dataset contains interactions from January 2014. The task is to predict which user or subreddit a user will interact with at a given time.

thgl-myket. This THG dataset is based on the Myket Android App market. Each edge documents the user installation or update interaction within the Myket market. The data spans six months and two weeks and when compared to an existing smaller version , this dataset contains the full data without downsampling. Overall, the dataset includes information on 206,939 applications and over 1.3 million anonymized users from June 2020 to January 2021.

thgl-github. This THG dataset is based on Github data collected from the GH Arxiv. This is a large dataset from a different period from thgl-software. We extract user, pull request, issue and repository nodes and track 14 edge types. The nodes with two or fewer edges are filtered out. The dataset contains the network as of March 2024. The task is to predict the next activity of an entity.

**Varying Scale.** Table 1 shows the detailed characteristics of all datasets, such as the number of quadruples and nodes. TGB 2.0 datasets vary significantly in scale for number of nodes, edges, and time steps. We observe an increase in runtime and memory requirements from tkgl-smallpedia to tkgl-polecat to tkgl-icews and tkgl-wikidata. In practice, these requirements depend on the combination of number of nodes, edges and time steps. To account for such benchmarking requirements, we categorize the datasets into small, medium and large datasets. Small datasets are suitable for prototyping methods, while medium and large datasets test method performance at increasingly large scales.

Table 1 reports dataset statistics: the _Proportion of Inductive Test Nodes_ (_Induct. Test Nodes_) is the proportion of nodes in the test set that have not been seen during training. The _Recurrency Degree_

    &  &  \\  Dataset & smallpedia & polecat & icews & wikidata & software & forum & github & myket \\  Domain & knowledge & political & political & knowledge & software & social & software & interac. \\ \# Quadruples & 550,376 & 17,796.10 & 15,513,446 & 9,856,203 & 1,489,806 & 23,757,707 & 1,499,577 & 53,632,788 \\ \# Nodes & 47,433 & 150,931 & 87,856 & 1,226,440 & 681,927 & 152,816 & 5,856,765 & 1,530,835 \\ \# Edge Types & 283 & 16 & 391 & 596 & 14 & 2 & 14 & 2 \\ \# Node Types & - & - & - & - & 4 & 2 & 4 & 2 \\ \# Timesteps & 125 & 1,826 & 10,224 & 2,025 & 689,549 & 2,558,457 & 2,510,415 & 14,828,090 \\ Granularity & year & day & day & year & second & second & second & second & second \\  Induct. Test Nodes & 0.26 & 0.12 & 0.05 & 0.34 & 0.13 & 0.02 & 0.14 & 0.01 \\ DRec & 0.71 & 0.07 & 0.11 & 0.61 & 0.00 & 0.00 & 0.00 & 0.00 \\ Rec & 0.72 & 0.43 & 0.63 & 0.61 & 0.10 & 0.63 & 0.01 & 0.37 \\ Con & 5.82 & 1.07 & 1.14 & 5.05 & 1.00 & 1.00 & 1.00 & 1.00 \\ Mean Edges/Ts. & 4,403.01 & 974.59 & 1,516.91 & 4,867.26 & 0.56 & 8.87 & 6.54 & 3.15 \\ Mean Nodes/Ts. & 5,289.16 & 550.60 & 1,008.65 & 5,772.16 & 0.86 & 12.96 & 9.77 & 6.24 \\   

Table 1: Dataset information including common statistics and the proportion of Inductive Test nodes (Induct. Test Nodes), the Direct Recurrency Degree (DRec), the Recurrency Degree (Rec), the Consecutiveness Value (Con), as well as the mean number of edges and nodes per timestep (Mean Edges/Ts. and Mean Nodes/Ts.)(Rec)_, which is defined as the fraction of test temporal triples \((s,r,o,t^{+})\) for which there exists a \(k<t^{+}\) such that \((s,r,o,k) G\). The _Direct Recurrency Degree (DRec)_ which is the fraction of temporal triples \((s,r,o,t^{+})\) for which it holds that \((s,r,o,t^{+}-1) G\). Also, we represent a novel metric called _Consecutiveness Value (Con)_, which quantifies it a given temporal triples repeats at consecutive timestamps by averaging the maximum number of consecutive timesteps during which a triple holds true across all triples in the dataset. Intuitively, fact-based relations which are true across multiple consecutive time steps will result in a higher _Consecutiveness Value_.

**Diverse Statistics.** TGB 2.0 datasets exhibits diverse dataset statistics. For example, tkgl-wikidata, tkgl-smallpedia, tkgl-polecat and thgl-software all have more than 10% test nodes that are inductive (i.e. nodes unseen in the training set), thus testing the inductive capability of methods. Variations in the recurrence of relations are evident with tkgl-smallpedia and tkgl-wikidata showing higher Recurrency Degrees compared to others. The DRec highlights the disparities between THG and TKG datasets, where the finer, second-wise time granularity of THG leads to a DRec of 0 implying no repetition of facts across subsequent time steps. On TKG datasets, the high Consecutiveness Value for tkgl-smallpedia and tkgl-wikidata exhibit a prevalence of long-lasting facts, contrasting with tkgl-icews and tkgl-polecat which documents political events. In comparison, THG datasets describe one-time events, thus displaying lower Con values.

Figure 2 shows the number of edges per timestamp for tkgl-smallpedia and thgl-software, reported over twenty bins with bars showing the min/max in each bin. Similar figures for other datasets are given in Appendix F. Figure 2 underscores distinctions between datasets, particularly in terms of time granularity and trend patterns. TKG datasets demonstrate a coarser time granularity leading to a significantly higher edge count per timestep compared to THG datasets. thgl-software exhibits relatively constant number of edges over time (with peaks at specific time points). In comparison, tkgl-smallpedia exhibit significant growth in edge count closer to the end. This is because tkgl-smallpedia starts from 1900 and ends at 2024, as time gets closer to current era, the amount of digitized and documented information increases significantly. The reduced number of edges in the final bin is due to the fact that the knowledge from 2024 remains incomplete as of this writing.

Figure 3 illustrates the distribution of the ten most prominent relations in tkgl-smallpedia and thgl-software. More Figures are in Appendix F. There are highly frequent relation types in tkgl-smallpedia such as member of sports team which occupies 28% of all edges; the portion of edges quickly reduces for other relations. In thgl-software, there is a relatively even split in the portion of edges for the most prominent relations with the top seven relations each occupying more than 10% of edges. These figure show the diversity of relations and their distributions in TGB 2.0.

## 5 Experiments

**Evaluation Protocol.** In TGB 2.0, we focus on the _dynamic link property prediction task_ where the goal is to predict the property (often existence) of a link between a pair of nodes in a future timestamp. Here, we treat the link prediction task as ranking problem similar to [27; 28; 16]. The model is required to assign the true edge with the highest probability from multiple negative edges (also referred to as corrupted triples in the TKG literature). The evaluation metric is the time-aware filtered Mean Reciprocal Rank (MRR) following [16; 28]. The MRR computes the average of the

Figure 2: Number of edges over time

reciprocals of the ranks of the first relevant item in a list of results. The time-aware filtered MRR removes any edge that are known to be true at the same time as the true edge (i.e. temporal conflicts) from the list of possible destinations. For THG datasets, we predict the tails of queries \((s,r,?,t^{+})\), as in . Following the practice in TKG literature , we predict entities in both directions for TKG datasets, namely both \((s,r,?,t^{+})\) and \((?,r,o,t^{+})\), achieved by introducing inverse relations where the head and tail of an existing relation is inverted. Due to the large size of TGB 2.0 datasets, we select the number of negative edges \(q\) for each dataset considering the trade-off between the evaluation completeness and the test inference time. Therefore, we utilize two negative sampling strategies for evaluation: _1-vs-all_ and _1-vs-q_. For both strategies, the temporal conflicts are removed for correctness. All negative samples are then pre-generated to ensure reproducible evaluation. Lastly, any methods that uses more than 40 GB GPU memory or runs for more than a week are considered as Out Of Memory (OOM) or Out Of Time (OOT), respectively.

_1-vs-all_. For datasets with a small number of nodes, it is possible to evaluate with all the possible destinations, thus achieving a comprehensive evaluation. In TGB 2.0, we use _1-vs-all_ strategy for tkgl-smallpedia, tkgl-polecat and tkgl-icews due to their smaller node size (see Table 1).

_1-vs-q_. For datasets with a large number of nodes, sampling \(q\) negative edges is required to achieve a practically feasible inference time. We find that randomly sampling the negative edges, omitting the edge types, results in over-optimistic MRRs, making the prediction easy. We thus propose to incorporate the edge type information into the negative sampling process for more robust evaluation. For the large TKG dataset tkgl-wikidata, we first identify possible tails for each edge type throughout the dataset and then sample the negatives based on the edge type of the query. If there are not enough tails in a given edge type, we then randomly sample the remaining ones. For all THG datasets, we sample all destination nodes with the same node type as the true destination node, thus considering the tail node type associated with a given edge type. We conduct an ablation study to show the effectiveness of our sampling strategy in Appendix G.5 on the tkgl-smallpedia dataset. We find that our sampling results in closer MRR to that of the _1-vs-all_ than random sampling.

### Temporal Knowledge Graph Experiments

For TKG experiments, we include RE-GCN , TLogic , CEN  as well as two deterministic heuristic baselines: the Recurrency Baseline (RecB)  and EdgeBank . For RecB, we report two versions where applicable: RecB\({}_{}\) which uses default values for its two parameters, and RecB\({}_{}\) which selects the optimal values for these based on performance on the validation set. For EdgeBank, we report two versions following , EdgeBank\({}_{}\), which accounts for information from a fixed past time window, and EdgeBank\({}_{}\), which uses information from all past temporal triples. Method details and compute resources are in Appendix H.1 and Appendix G.1, respectively.

We report the average performance and standard deviation across 5 runs for each method in Table 2. The runtimes and GPU usage results are in Appendix G.3 and G.2. In particular, several methods encountered out of memory or out of time errors on some datasets. The results reveals

Figure 3: Most frequent relation types for tkgl-smallpedia and thgl-software datasets. _Others_ refers to all remaining relations not shown here.

that no single method exhibits superior performance across all four datasets. Surprisingly, the RecB heuristic performs competitively across most datasets while being among the best performing on tkgl-smallpedia and tkgl-icews, underscoring the importance of including simple baselines in comparison and suggesting potential areas for improvement in other methods. The Edgebank heuristic, originally designed for homogenous temporal graphs, exhibits low performance, highlighting the importance of utilizing the rich multi-relational information for TKG learning. On the large tkgl-wikidata dataset, however, Edgebank is the only method that can scale to such size, likely due to the fact that it omits edge type information. This highlights the need for scalable methods. On another note, methods achieve higher MRRs on datasets characterized by high Recurrency Degrees and Consecutiveness values (tkgl-smallpedia, tkgl-wikidata), despite the presence of a considerable number of inductive nodes in these datasets.

**Per-relation Analysis**. Figure 4 illustrates the performance per relation of selected methods across three datasets 3. For each dataset, we show the ten most frequent relations, ordered by decreasing Recurrency Degree with the color reflecting the Recurrency Degree of each relation. Note that the y-axis scale varies across datasets. We observe distinct patterns in relation-specific performance across datasets: while results on tkgl-poceat exhibit consistent performance across relations, suggesting a relative homogeneity, results on the tkgl-smallpedia dataset shows significant variance, indicating a higher degree of variation among relations. Interestingly, there is strong correlation between the Recurrency Degree and performance, most evident within the tkgl-smallpedia dataset.

### Temporal Heterogenous Graph Experiments

For THG experiments, we include TGN  (with and without edge type information), STHN , RecB , and EdgeBank . Table 3 reports the average performance and standard deviation across 5 runs for each method. Scalability is a significant challenge for THG methods on large datasets such as tbl-form and tbl-myket, more details can be found in Appendix G. Most methods either are out of memory or out of time for these datasets. STHN achieves the highest performance on tbl-software dataset showing methods designed for THG can achieve significant performance gain. However, STHN is the least scalable, requiring 185 GB of memory for tbl-software to compute subgraphs and unable to scale to other datasets. The widely-used TGN model  for single-relation temporal graph learning is also adapted here, with a modification to incorporate the edge type information as edge feature. We observe significant improvement when TGN utilizes edge

    &  &  &  &  \\  & Validation & Test & Validation & Test & Validation & Test & Validation & Test \\  EdgeBank\({}_{}\) & 0.457 & 0.353 & 0.058 & 0.056 & 0.020 & 0.020 & 0.633 & 0.535 \\ EdgeBank\({}_{}\) & 0.401 & 0.333 & 0.048 & 0.045 & 0.008 & 0.009 & 0.632 & 0.535 \\ RecB\({}_{}\) & 0.639 & 0.605 & 0.203 & 0.198 & 0.270 & **0.211** & OOT & OOT \\ RecB\({}_{}\) & 0.542 & 0.486 & 0.170 & 0.167 & 0.264 & 0.206 & OOT & OOT \\ RE-GCN  & 0.631\(\)0.001 & 0.594\(\)0.001 & 0.191\(\)0.003 & 0.175\(\)0.002 & 0.232\(\)0.003 & 0.182\(\)0.003 & OOM & OOM \\ CEN  & **0.646\(\)**0.001 & **0.612\(\)**0.001 & 0.204\(\)0.002 & 0.184\(\)0.002 & 0.244\(\)0.002 & 0.187\(\)0.003 & OOM & OOM \\ TI Logic  & 0.631\(\)0.000 & 0.595\(\)0.001 & **0.236\(\)**0.001 & **0.228\(\)**0.001 & **0.287\(\)**0.001 & 0.186\(\)0.001 & OOT & OOT \\   

Table 2: **MRR results for _Temporal Knowledge Graph Link Prediction_ task. We report the average and standard deviation across 5 runs. First place is **bolded**, second place underlined.

Figure 4: MRR per relation for the 10 highest occuring relations for three TKG datasets for RE-GCN, CEN and RecB\({}_{}\). The color indicates the Recurrency Degree value for relation type. The relations for each dataset are ordered by decreasing Recurrency Degrees.

type data, thus showing the potential to leverage the multi-relational information in THGs. Lastly, EdgeBank achieves competitive performance with that of TGN while being scalable to large datasets. Thus, it is important to evaluate against simple baselines to understand method performances.

## 6 Conclusion

In this work, we introduce TGB 2.0, a novel benchmark for reproducible, realistic, and robust evaluation on multi-relational temporal graphs that is building on the Temporal Graph Benchmark (TGB). We present four new TKG and four new THG datasets which introduce multi-relation datasets in TGB. TGB 2.0 datasets are significantly larger than existing ones while being diverse in statistics and dataset domains. TGB 2.0 focuses on the dynamic link property prediction task and provides an automated pipeline for dataset downloading, processing, method evaluation, and a public leaderboard to track progress. From our experiments, we find that both TKG and THG methods struggle to tackle large scale datasets in TGB 2.0, often resulting in overly long runtime or exceeding the memory limit. Therefore, scalability is an important future direction. Another observation is that heuristic methods achieve competitive results on TKG and THG datasets. This highlights the importance of the inclusion of simple baselines and underlines the room for improvement for current methods.

**Limitations** This work exclusively considers the continuous-time setting for THG datasets. Depending on the application, either the continuous-time or discrete-time setting may be more appropriate. However, the continuous-time setting is often regarded as the more general framework. However, many THG methods are designed for discrete settings. Thus, in future work, discretized versions of the data sets could be added for comparative analysis between discrete methods. Additionally, the TGB 2.0 dataset collection currently includes datasets from only five distinct domains. Notably, domains such as biological networks and citation networks are not represented. To address this limitation, we plan to expand the dataset collection by incorporating additional datasets based on community feedback, thereby enhancing the diversity and comprehensiveness of the dataset repository. As temporal graph data often records interactions between individuals or other sensitive information, data privacy is a potential concern. Sensitive information may be stored as node level or link level attributes thus proper anonymization of data should always be a top priority. In this work, we anonymize user information where appropriate to prevent the leakage of identifiable information.