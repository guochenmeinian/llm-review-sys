# Certified Robustness via Dynamic Margin Maximization and Improved Lipschitz Regularization

Mahyar Fazlyab, Taha Entesari, Aniket Roy, Rama Chellappa

Department of Electrical and Computer Engineering

Johns Hopkins University

{mahyarfazlyab, tentesa1, aroy28, rchella4}@jhu.edu

Equal contribution.

###### Abstract

To improve the robustness of deep classifiers against adversarial perturbations, many approaches have been proposed, such as designing new architectures with better robustness properties (e.g., Lipschitz-capped networks), or modifying the training process itself (e.g., min-max optimization, constrained learning, or regularization). These approaches, however, might not be effective at increasing the margin in the input (feature) space. In this paper, we propose a differentiable regularizer that is a lower bound on the distance of the data points to the classification boundary. The proposed regularizer requires knowledge of the model's Lipschitz constant along certain directions. To this end, we develop a scalable method for calculating guaranteed differentiable upper bounds on the Lipschitz constant of neural networks accurately and efficiently. The relative accuracy of the bounds prevents excessive regularization and allows for more direct manipulation of the decision boundary. Furthermore, our Lipschitz bounding algorithm exploits the monotonicity and Lipschitz continuity of the activation layers, and the resulting bounds can be used to design new layers with controllable bounds on their Lipschitz constant. Experiments on the MNIST, CIFAR-10, and Tiny-ImageNet data sets verify that our proposed algorithm obtains competitively improved results compared to the state-of-the-art.

## 1 Introduction

Motivated by the vulnerability of deep neural networks to adversarial attacks , i.e., imperceptible perturbations that can drastically change a model's prediction, researchers and practitioners have proposed various approaches to enhance the robustness of deep neural networks, including adversarial training , regularization , constrained learning , randomized smoothing , relaxation-based defenses , and model ensembles . These approaches modify the model architecture, the optimization procedure (loss function and algorithm), the inference mechanism, or the dataset itself  to enhance accuracy on both natural and adversarial examples.

However, most of these methods typically do not directly operate on input margins and rather target the output margin or its surrogates. For example, it is an established property that deep models trained with the cross-entropy loss, which is a surrogate for maximizing the output margin (see Appendix A.2), are prone to adversarial attacks . From this perspective, it is critical to design regularized loss functions that can explicitly and effectively target the margin in the _input_ space .

Our Contribution (1)Using first principles, we design a novel regularized loss function for training adversarially robust deep classifiers. We design a differentiable regularizer, using the Lipschitz constants of the logit differences, that is a lower bound on the input margin. We empirically show that this regularizer promotes larger margins in the input space. **(2)** We develop a scalable method for calculating guaranteed analytic and differentiable upper bounds on the Lipschitz constant of deep networks accurately and efficiently. Our method, called LipLT, hinges on the idea of _Loop Transformation_ on the nonlinearities, which allows us to exploit the monotonicity and Lipschitz continuity of activations functions effectively. We prove that the resulting upper bound is better than the product of the Lipschitz constants of all layers (the so-called naive bound), and in practice, it is significantly better. Furthermore, our Lipschitz bounding algorithm can be used to design new layers with controllable bound on their Lipschitz constant. **(3)** We integrate our Lipschitz estimation algorithm within the proposed training loss to develop a robust training algorithm that eliminates the need for any inner-loop optimization subroutine. We utilize the recurring structure of the calculations that enable parallelized implementation on GPUs. When integrated into training, the relative accuracy of the bounds prevents excessive regularization and allows for more direct manipulation of the decision boundary.

Experiments on the MNIST, CIFAR-10, and Tiny-ImageNet data sets verify that our proposed algorithm obtains competitively improved results compared to the state-of-the-art. Code available on https://github.com/o4lc/CRM-LipLT.

### Related Work

In the interest of space, we only review the most relevant literature and defer the comprehensive version to the supplementary materials.

Enhancing Robustness via OptimizationThe most well-known approach in adversarial defenses is perhaps adversarial training (AT)  and its certified variants , which involves minimizing a worst-case loss (or its approximation) using uniformly-bounded perturbations to the training data. Since these approaches might hurt accuracy on non-adversarial examples, several regularization-based methods have been proposed to trade off adversarial robustness against natural accuracy . Intuitively, these approaches aim to control the variations of the model close to the decision boundary or around the training data points. In general, the main challenge is to formulate computationally efficient differentiable regularizers that can directly increase the margin of the classifier. Of notable recent work in this direction is , in which the authors design a regularizer that directly increases the margin in the input space and prioritizes more vulnerable points by leveraging the dynamics of the decision boundary during training. Our loss function also takes advantage of this approach, but rather than using an iterative algorithm to compute the margin, we use Lipschitz continuity arguments to find closed-form differentiable lower bounds on the margin.

Robustness and Lipschitz RegularityTo obtain efficient and scalable certificates of robustness one can bound or control the global Lipschitz constant of the model. To train robust networks, the global Lipschitz bound is typically computed as the product of the spectral norm of linear layers (the naive bound), which provides computationally efficient but overly conservative certificates . If these bounds are localized (e.g., in a neighborhood of each training data point), conservatism can be mitigated at the expense of losing computational efficiency . However, it is not clear if this approach provides any advantages over other local bounding schemes  that can be more accurate with comparable complexity. Hence, for training purposes, it is highly desirable to have global but less conservative differentiable Lipschitz bounds. Of notable work in this direction is the semidefinite programming (SDP) approach of  (as well as its local version ) to provide accurate numerical bounds, which has also been leveraged in training low-Lipschitz networks . However, these SDP-based approaches are restricted to small-scale models. In comparison, LipLT is less accurate than LipSDP but can scale to significantly larger models and larger input dimensions. Compared to the naive method, LipLT is significantly more accurate but has a comparable practical complexity thanks to a specialized GPU implementation that exploits the recursive nature of the algorithm.

Lipschitz LayersAnother approach to improving robustness is to design new layers with controllable Lipschitz constant. Various methods have been proposed to design the so-called 1-Lipschitz networks . Inspired by the LipSDP framework of , the recent work  designs 1-Lipschitz layers that generalize many of the aforementioned 1-Lipschitz designs. However, their proposed approach is currently not applicable to multi-layer networks. In , the authors propose a reparameterization that directly satisfies the SDP constraint of LipSDP, but the proposed method cannot handle skip connections.  introduces \(_{}\) distance neurons that are 1-Lipschitz and provide inherent robustness against \(_{}\) perturbations.

### Preliminaries and Notation

The log-sum-exp function is defined as \(LSE(x)=(_{i=1}^{n}(x_{i}))\). For any \(t>0\), the perspective  of the log-sum-exp function satisfies the inequalities \(_{i}(x_{i})<t^{-1}LSE(tx)_{i}(x_{i})+t^{-1}(n)\). We denote the cross-entropy loss by \(CE(q,p)=-_{k=1}^{K}p_{k}(q_{k})\), where \(p,q^{K}\) are on the probability simplex. For an integer \(y\{1,,K\}\), we define \(u_{y}^{K}\) as the \(y\)-th standard unit vector. For a vector \(a^{K}\), we have \(\|a\|_{p}=(_{i=1}^{K}|a_{i}|^{p})^{1/p}\). Additionally, we have \(\|a^{}\|_{p}=\|a\|_{q}\), where \(1/p+1/q=1\). When \(p=1\), it is understood that \(q=\). For a matrix \(A^{m n}\), \(\|A\|\) denotes a matrix norm of \(A\). In this paper we focus on \(\|A\|_{2}=_{}(A)\), i.e., the spectral norm of the matrix \(A\). We, however, note that except for the implementation notes regarding the power iteration, the rest of the Lipschitz estimation algorithm is valid in any other matrix norm as well. We denote the indicator function of the event \(E\) as \(1_{E}\), where \(1_{E}\) outputs 1 if the event \(E\) is realized and 0 otherwise.

## 2 Certified Radius Maximization (CRM)

Consider a \(K\)-class deep classifier \(C(x;)=_{1 i K}f_{i}(x;)\), where \(f(x;)=(z(x;))\) is the vector of class probabilities, and \(z(x;)\) is a deep network. If a data point \(x\) is classified correctly as \(y\{1, K\}\), then the _logit margin_\((z(x;),y)\), the difference between the largest and second-largest logit, is \((z(x;),y)=z_{y}(x;)-_{j y}z_{j}(x;)\).

Several existing loss functions for training classifiers are differentiable surrogates for maximizing the logit margin, including the cross entropy, and its combination with label smoothing (see Appendix A.2 for more details). However, maximizing the logit margin, or its surrogates, does not necessarily increase the margin in the input space. This can happen, for example, when the model has rapid variations close to the decision boundary. Thus, we need a regularizer targeted for maximizing the input margin.

Regularization Based on Certified RadiusFor a correctly-classified data \(x\) with label \(y\), the distance of \(x\) to the decision boundary can be calculated by solving the optimization problem ,

\[R(x,y;)=_{}\|x-\|(z(; ),y)=0,\] (1)

where the constraint enforces \(\) to be on the decision boundary. 2 To ensure a large margin in the _input_ space, we must, in principle, maximize the radius \(R(x,y;)\) of all correctly classified data points. To achieve this goal, we can add a regularizer that penalizes small input margins,

\[_{}_{(x,y)}[-(z(x;),y)]+ _{(x,y)}[1_{\{(z(x;),y)>0\}}g(R(x,y; ))],\] (2)

where \(>0\) is the regularization constant and \(g\) is a decreasing function to promote larger certified radii. See appendix A.2 for more discussion on the properties of \(g\).

To optimize (2), we must compute and differentiate through \(R(x,y;)\). For \(_{}\) norm and ReLU activations, we can reformulate (1) as a mixed-integer linear program by using a binary representation of the activation functions. However, it is not affordable to solve this optimization problem during training. Hence, we must resort to computing lower bounds on \(R(x,y;)\) instead.

By replacing the logit margin with its soft lower bound in the constraint of (1), we obtain a lower bound on \(R(x,y;)\),

\[ R(x,y;)_{}&\|x- \|\\ & z_{y}(x;)-_{j  y}e^{tz_{j}(x;)} 0.\] (3)

To solve this non-convex problem, the authors of  adapt the successive linearization algorithm of , which in turn does not provide a provable lower bound. To avoid this iterative algorithm and to provide sound lower bounds, we propose to use Lipschitz continuity arguments.

Lipschitz-Based Surrogates for Certified RadiusSuppose we add a norm-bounded perturbation \(\) to a data point \(x\) that is classified correctly as \(y\). For the label of \(x+\) to not change, we must enforce

\[ z_{yi}(x+):=z_{y}(x+;)-z_{i}(x+;)>0  i y.\]

Suppose the logit difference \(x z_{yi}(x)\) is Lipschitz continuous with constant \(L_{yi}\) (in \(p\) norm), implying \(| z_{yi}(x+;)- z_{yi}(x)| L_{yi}\|\|_{p},\;  x,\). Then we can write

\[ z_{yi}(x+;) z_{yi}(x;)-L_{yi}\|\|_{ p} i y.\]

The right-hand side remaining positive for all \(i y\) is a sufficient condition for the correct classification of \(x+\) as \(y\). This sufficient condition yields a lower bound on \(R(x,y;)\) as follows,

\[(x,y;):=_{i y}(x;)-z_{i}(x; )}{L_{yi}} R(x,y;).\] (4)

This lower bound is the pointwise minimum of logit differences normalized by their Lipschitz constants. This lower bound is not differentiable everywhere, but similar to the logit margin, we propose a smooth underapproximation of the \(\) operator using scaled LSE:

\[^{soft}_{t}(x,y;):=-(_{i y}(-t (x;)-z_{i}(x;)}{L_{yi}})).\] (5)

In addition to differentiability, another advantage of using this soft lower bound is that as opposed to \((x,y;)\), which involves only two logits, the soft lower bound \(^{soft}_{t}(x,y;)\) includes all the logits, and hence, makes more effective use of information.

**Proposition 1**: We have the following relationship between \(^{soft}_{t}(x,y;)\) defined in (5) and \((x,y;)\) defined in (4).

\[^{soft}_{t}(x,y;)(x,y;) ^{soft}_{t}(x,y;)+.\]

In summary, we use the following loss function to train our classifier,

\[_{}_{(x,y)}[-(z(x; ),y)]+_{(x,y)}[1_{\{(z(x;),y )>0\}}g(^{soft}_{t}(x,y;))].\] (6)

The first and the second terms are differentiable surrogates to the negative logit margin, and the input margin, respectively. For example, as we show in Appendix A.2 the negative cross-entropy loss is a differentiable lower bound on the logit margin, i.e., we can choose \((z(x;),y)=-CE(z(x;),u_{y})\).

It now remains to estimate the \(L_{yi}\)'s (the Lipschitz constants of \(x z_{yi}(x)\)) that are needed to compute the second term in the loss function. While any guaranteed upper bound on \(L_{yi}\) would suffice to preserve the lower bound property in (4), a more accurate upper bound prevents excessive regularization and allows for more direct manipulation of the decision boundary. To achieve this goal, we propose a new method which we will discuss next.

Scalable Estimation of Lipschitz Constants via Loop Transformation (LipLT)

In this section, we propose a general-purpose algorithm for computing a differentiable upper bound on the Lipschitz constant of deep neural networks, which is also of independent interest. For simplicity in the exposition, we first consider single hidden layer neural networks of the form \(h(x)=W^{1}(W^{0}x)\), where \(W^{1},W^{0}\) have compatible dimensions, and the bias terms are ignored without loss of generality. The activation layers \(\) are of the form \((z)=((z_{1}),,(z_{n_{1}})) z^{n_{1}}\), where \(\) is the activation function, which we assume to be monotone and Lipschitz continuous, implying \(((x)-(x^{}))/(x-x^{}) x  x^{}\) for some \(0<\). In , the authors propose an SDP for computing an upper bound on the global Lipschitz constant of multi-layer neural networks when \(_{2}\) norm is considered in both input and output domains. This result for the single hidden layer case is formally stated in the following theorem.

**Theorem 1** (): _Consider a single-layer neural network described by \(h(x)=W^{1}(W^{0}x)\). Suppose \((x)^{n_{1}}^{n_{1}}=[(x_{1}) (x_{n_{1}})]\), where \(\) is slope-restricted over \(\) with parameters \(0<\). Suppose there exist \(>0\) and diagonal \(T^{n_{1}}_{+}\) such that the matrix inequality_

\[M(,T):=-2}^{}TW^{0}- I_{n_{0}}&( +){W^{0}}^{}T\\ (+)TW^{0}&-2T+{W^{1}}^{}W^{1} 0,\] (7)

_holds. Then \(\|h(x)-h(y)\|_{2}\|x-y\|_{2}\) for all \(x,y^{n_{0}}\)._

The key advantage of this SDP formulation is that we can exploit several properties of the structure, namely monotonicity (\( 0\)) and Lipschitz continuity (\(<\)) of the activation functions, as well as using the same activation function in the activation layer. However, solving this SDP and enforcing them during training can be challenging even for small-scale neural networks. The recent work  has exploited the chordal structure of the resulting SDP imposed by the sequential structure of the network to solve the SDP for larger instances. However, these approaches are still unable to scale to larger problems and are not suitable for training purposes.

To guide the search for analytic solutions to the linear matrix inequality (LMI) of Theorem 1, the authors in  consider the following residual structure,

\[h(x)=Hx+G(Wx).\] (8)

Then it can be shown that the LMI condition (7) generalizes to condition (9) (see Appendix A.3 for details).

\[M(,T):=-2}TW+H^{}H- I_{n_{0}}&( +){W^{}}T+{H^{}}G\\ (+)TW+G^{}H&-2T+G^{}G 0,\] (9)

By choosing \(=1\), \(H=I\) and \(G=-(+){W^{}}T\), then the LMI (9) simplifies to \((+)^{2}TWW^{}T 2T\) (all blocks in the LMI except for the lower diagonal block become zero). When we restrict \(T\) to be positive definite, then the latter condition is equivalent to \((+)^{2}WW^{} 2T^{-1}\), which can be satisfied analytically using various choices of \(T\). In summary, the function \(h(x)=x-(+){W^{}}T(Wx)\) is guaranteed to be \(1\)-Lipschitz as long as \((+)^{2}WW^{} 2T^{-1}\).

A potential issue with the above parameterization (and 1-Lipschitz networks in general) is that, since the true Lipschitz constant of the layer can be less than one, the multi-layer concatenation can become overly contractive. One way to resolve this limitation is to modify the parameterization as follows.

**Proposition 2**: Suppose \(WW^{}}T^{-1}\) for some \(>0\) and some diagonal positive definite \(T\). Then the following function is \(\)-Lipschitz.

\[h(x)=x-}{W^{}}T(Wx).\] (10)

Now if we make a cascade connection of \(L 2\) layers of the form (10) (each having its own \(\)), a naive bound on the Lipschitz constant of the corresponding deep network would be \(_{i=1}^{L}_{i}^{1/2}\). However, this upper bound can still be very crude. We now propose an alternative approach to compute an analytic upper bound on the Lipschitz constant of (8). For the multi-layer case, we then show that our method can capture the coupling between different layers to improve the naive bound. For the sake of space, we defer all the proofs to the supplementary materials.

The power of loop transformationStarting from (8), using the fact that \(\) is \(\)-Lipschitz, a global Lipschitz constant of \(h\) can be computed as

\[\|h(x)-h(x^{})\| \|H(x-x^{})\|+\|G((Wx)-(Wx^{}))\|\] \[_{L_{}}} \|x-x^{}\|.\] (11)

This upper bound is pessimistic as it does not exploit the monotonicity (\( 0\)) of activation functions. In other words, this bound would not change as long as \(-\). To inform the bound with monotonicity, we perform a _loop transformation_ to bring the non-linearities to the symmetric sector \([-(-)/2,(-)/2]\), resulting in the representation

\[h(x)=(H+GW)x+G(Wx),\] (12)

where \((x):=(x)-x\) is the loop transformed nonlinearity, which is no longer monotone, and is \(\)-Lipschitz- see Figure 1. We can now compute a Lipschitz constant as

\[\|h(x)-h(x^{})\| =\!\|(H+GW)(x-x^{})\!+\!G((Wx)\! -\!(Wx^{}))\|\] (13) \[\|(H+GW)(x-x^{})\|\!+\!\|G((Wx )-(Wx^{}))\|\] \[GW\|+\|G\|\|W\|)}_{L_{}}}\|x-x^{}\|.\]

As we show below, this loop transformation does improve the naive bound. In Appendix A.3 we also prove the optimality of the transformation.

**Proposition 3**: Consider a single-layer residual structure given by \(h(x)=Hx+G(Wx)\). Suppose \(^{n_{1}}^{n_{1}},(x)=[(x_{1}) (x_{n_{1}})]\), where \(\) is slope restricted in \([,]\). Then \(h\) is Lipschitz continuous with constant \(\|H+GW\|+\|G\|\|W\| L_{}}\).

Figure 1: Loop transformation on a residual layer of the form \(h(x)=Hx+G(Wx)\). Here we use \((x)= x\) for the illustration of the loop transformation.

Bound refinement.For the case of the \(_{2}\) norm, we can further improve the derived bound in (13). Specifically, we can bound the Lipschitz constant of the second term of \(h\) in (12) analytically using LipSDP, which we summarize in the following theorem. 3.

**Theorem 2**: Suppose there exists a diagonal \(T\) that satisfies \(G^{}G T\). Then a Lipschitz constant of (8) in \(_{2}\) norm is given by

\[L_{}(T)=\|H+GW\|_{2}+\|W^{}TW\|_{2}^{}.\]

By optimizing over the choice of diagonal \(T\) satisfying \(G^{}G T\), we can find the best Lipschitz bound achievable by this method. In this paper, however, we are more interested in analytical choices of \(T\). Using the same techniques outlined in , we have the following choices:

1. Spectral Normalization (SN): We can satisfy \(G^{}G T\) simply by choosing \(T=T_{SN}:=\|G\|_{2}^{2}I\). In this case, \(L_{}(T_{SN})=\|H+GW\|_{2}+\|W\|_{2}\|G\|_{2}=L_{}\).
2. AOL : Choose \(T_{ii}=_{j=1}^{n}|G^{}G|_{ij}\). As a result, \(T-G^{}G\) is real-symmetric and diagonally dominant with positive diagonal elements, and thus, is positive semidefinite.
3. SLL : Choose \(T_{ii}=_{j=1}^{n}|G^{}G|_{ij}}{q_{i}}\), where \(q_{i}>0\).  uses the Gershgorin circle theorem to prove that this choice of \(T\) satisfies \(G^{}G T\).

### Multi-layer Neural Networks

We now extend the proposed technique to the multi-layer case. Consider the following structure,

\[y_{k} =W_{k}x_{k} k=0,,L\] (14) \[x_{k+1} =H_{k}x_{k}+G_{k}(y_{k}) k=0,,L-1\]

with \(W_{L}=I\) for consistency. Then \(y_{L}=x_{L}\) is the output of the neural network. To compute a Lipschitz constant, we first apply loop transformation to each activation layer, resulting in the following representation.

**Lemma 1**: Consider the sequences in (14). Define \(_{k}:=H_{k}+G_{k}W_{k}\) for \(k=0,,L-1\).4 Then

\[y_{k+1}=W_{k+1}_{k}_{0}x_{0}+_{j=0}^{k}W _{k+1}_{k}_{j+1}G_{j}(y_{j}) k=0,,L-1.\] (15)

We are now ready to state our result for the multi-layer case.

**Theorem 3**: Let \(m_{k},k 1\) be defined recursively as

\[m_{k+1}=\|W_{k+1}_{k}_{0}\|+_{j=0}^{k}\|W_{k+1}_{k}_{j+1}G_{j}\|m_{j}\] (16)

with \(m_{0}=\|W_{0}\|\). Then \(m_{k}\) is a Lipschitz constant of \(x_{0} y_{k},k=0,,L-1\). In particular, \(m_{L}\) is a Lipschitz constant of the neural network \(x_{0} y_{L}=x_{L}\).

Similar to the single-layer case, we can refine the bounds for the \(_{2}\) norm. For the sake of space, we defer this to the Appendix, and we use the bounds in (16) in our experiments.

ComplexityFor an \(L\)-layer network, the recursion in (16) requires \((L^{2})\) number of matrix norm calculations. For the naive bound, this number is exactly \(L\). Despite this increase in complexity, we utilize the recurring structure of the calculations that enable parallelized implementation on GPUs. This utilization results in a reduction of the time complexity to \((L)\), the same as the naive Lipschitz estimation algorithm. We provide a more detailed analysis of the computational and time complexity and the GPU implementation in Appendix A.4.

Experimental Results

In this section, we evaluate our proposed method for training deep classifiers on the MNIST , CIFAR-10  and Tiny-Imagenet  datasets. We compare our results with the closely related state-of-the-art methods. We further analyze the efficiency of our improved Lipschitz bounding algorithm and leverage it to compute the certified robust radii for the test dataset.

### \(_{2}\)-Robustness

Experimental setupWe train using our novel loss function to certify robustness (6), using cross-entropy as the differentiable surrogate, against \(_{2}\) perturbations of size \(1.58\) on MNIST and \(36/255\) on CIFAR-10 and Tiny-Imagenet 5. We train convolutional neural networks of the form \(\) with ReLU activation functions, as in , where \(m\) and \(n\) denote the number of convolutional and fully connected layers, respectively. The details of the architectures, training process, and most hyperparameters are deferred to the supplementary materials.

BaselinesFor baselines, we consider recent state-of-the-art methods that have provided results on convolutional neural networks of similar size. We consider: (1) GloRo  which uses naive Lipschitz estimation with a smoothness regularizer inspired by TRADES; (2) Local-lip-B/G (+ MaxMin)  which uses local Lipschitz constant calculation along with cut-off ReLU and MaxMin activation functions; (3) LipConvnet-5/10/15  that uses 1-Lipschitz convolutional networks with GNP Householder activations; (4) SLL-small (SDP-based Lipschitz Layers)  which is a much larger 27 layer 1-Lipschitz network; (5) AOL-Small/Medium  which presents much larger 1-Lipschitz networks trained to have almost orthogonal layers.

EvaluationWe evaluate the accuracy of the trained networks under 3 criteria; standard accuracy, adversarial accuracy, and certified accuracy. For adversarial accuracy we perform PGD attacks using  (hyperparameters provided in supplementary material). For certified accuracy, we calculate the certified radii using (4) and verify if they are larger than the priori-assigned perturbation budget. For the methods of recent literature, we report their best numbers as per the manuscript.

ResultsThe results of the experiments are presented in Table 1. On MNIST, we outperform the state-of-the-art by a margin of 7.5% on verified accuracy whilst maintaining the same standard accuracy. On CIFAR-10 we surpass all of the state-of-the-art in verified accuracy. Furthermore, on methods using networks of similar sizes, i.e., 6C2F or LipConvnet-5, we surpass by a margin of around 4%. The networks of the recent literature AOL  and SLL  are achieving slightly worse certified accuracy even with much larger networks. On Tiny-Imagenet, our current best results are on par with the state-of-the-art.

#### 4.1.1 Lipschitz Estimation

A key part of our method is the new improved Lipschitz estimation algorithm (LipLT) and the effective use of pairwise Lipschitz constants. Unlike previous works that estimate the pairwise Lipschitz between class \(i\) and \(j\) by the upper bound \(L_{z}\), where \(L_{z}\) is the Lipschitz constant of the whole network, or as \(L_{i}+L_{j}\), where \(L_{i}\) is the Lipschitz constant of the \(i\)-th class, we approximate this value directly as \(L_{ij}\) by considering the map \(z_{i}(x;)-z_{j}(x;)\). Table 2 shows the average statistic for Lipschitz constants estimated using different methods. Our improved Lipschitz calculation algorithm provides near an order-of-magnitude improvement over the naive method. Furthermore, Table 2 portrays the superiority of using pairwise Lipschitz constants instead of other proxies. Figure 2a illustrates the difference between using LipLT versus the naive method to calculate the certified radius. In this experiment, the naive method barely certifies one percent of the data points at the perturbation level of 1.58. However, using LipLT, we can certify a significant portion of the data points.

We further conduct an analysis of the certified radii of the data points for regularized versus unregularized networks for MNIST and CIFAR-10. Figures 1(b) and 1(c) illustrate the distribution of the certified radii. These radii were computed using the direct pairwise Lipschitz bounding. When comparing the results of the CRM-trained models (top figures) with the standard models (bottom figures), it becomes evident that our loss function enhances the certified radius.

   Method & Model & Clean (\%) & PGD (\%) & Certified(\%) \\   \\  Standard & 4C3F & 99.0 & 45.4 & 0.0 \\ GloRo & 4C3F & 92.9 & 68.9 & 50.1 \\ Local-Lip & 4C3F & 96.3 & 78.2 & 55.8 \\ CRM (ours) & 4C3F & 96.27 & 88.04 & **63.37** \\   \\  Standard & 6C2F & 87.5 & 32.5 & 0.0 \\ GloRo & 6C2F & 77.0 & 69.2 & 58.4 \\ Local-Lip-G & 6C2F & 76.4 & 69.2 & 51.3 \\ Local-Lip-B & 6C2F & 70.7 & 64.8 & 54.3 \\ Local-Lip-B + MaxMin & 6C2F & 77.4 & 70.4 & 60.7 \\ LipConvnet & 5-CR & 75.31 & - & 60.37 \\ LipConvnet & 10-CR & 76.23 & - & 62.57 \\ LipConvnet & 15-CR & 76.39 & - & 62.96 \\ SLL & Small & 71.2 & - & 62.6 \\ AOL & Small & 69.8 & - & 62.0 \\ AOL & Medium & 71.1 & - & 63.8 \\ CRM (ours) & 6C2F & 74.82 & 72.31 & **64.16** \\   \\  Standard & 8C2F & 35.9 & 19.4 & 0.0 \\ Local-Lip-G & 8C2F & 37.4 & 34.2 & 13.2 \\ Local-Lip-B & 8C2F & 30.8 & 28.4 & 20.7 \\ GloRo & 8C2F & 35.5 & 32.3 & 22.4 \\ Local-Lip-B + MaxMin & 8C2F & 36.9 & 33.3 & **23.4** \\ SLL & Small & 26.6 & - & 19.5 \\ CRM (ours) & 8C2F* & 23.97 & 23.04 & 17.98 \\   

Table 1: Comparison with recent certified training algorithms. Best certified training accuracies are highlighted in bold.

Figure 2: (a) Distribution of certified radii calculated using direct pairwise Lipschitz constants for the MNIST test dataset for the network trained using CRM. (b-c) Comparison of the distribution of the certified radii for a model trained using CRM (top) versus a standard trained model (bottom) for MNIST (b) and CIFAR-10 (c). For any given \(\), the probability curves denote the empirical probability of a data point from that data set having a certified radius of at least \(\).

## 5 Limitations

Although our proposed formulation has an elegant mathematical structure, there exist some limitations: **(1)** We are still using global Lipschitz constants to bound the margin, which can be conservative. We will investigate how we can localize our calculations without a significant increase in computation. **(2)** Computation of pairwise Lipschitz bounds for very deep architectures and or a large number of classes can become computationally intensive for training purposes (see Appendix A.4 for further discussions). **(3)** Several hyper-parameters require manual tuning. It is highly desirable to explore adaptive approaches to automatically select these hyper-parameters.

## 6 Conclusion

Adversarial defense methods are numerous and their approaches are different, but they all attempt to explicitly or implicitly increase the margin of the classifier, which is a measure of adversarial robustness (or vulnerability). From this perspective, it is highly desirable to develop adversarial defenses that can manipulate the decision boundary and increase the margin effectively and efficiently. We attempt to maximize the input margin by penalizing the Lipschitz constant of the neural network along vulnerable directions. Additionally, we develop a new method for calculating guaranteed analytic and differentiable upper bounds on the Lipschitz constant of the deep network. LipLT is provably better than the naive Lipschitz constant. We have also provided a parallelized implementation of LipLT using the recurring structure of the calculations, which is fast and scalable. Our proposed method achieves competitive results in terms of verified accuracy on the MNIST and CIFAR-10 datasets.

    &  &  &  \\   & \(L_{ij}\) & \(L_{i}+L_{j}\) & \(L\) & \(L_{ij}\) & \(L_{i}+L_{j}\) & \(L\) & \(L_{ij}\) & \(L_{i}+L_{j}\) & \(L\) \\  Naive & 266.08 & 1285.18 & 2759.63 & 93.52 & 131.98 & 139.75 & 313.62 & 485.62 & 2057.00 \\ Improved & 64.10 & 212.68 & 419.83 & 11.26 & 18.30 & 23.43 & 9.46 & 14.77 & 60.37 \\   

Table 2: Comparison of average pairwise Lipschitz constant calculated using the naive and improved method on networks trained using CRM. We use the same architectures as Table 1. To calculate the averages, we form all unique pairs and calculate the Lipschitz constant according to the relevant formulation and then take the average.