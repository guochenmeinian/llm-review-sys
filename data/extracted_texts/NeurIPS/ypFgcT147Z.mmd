# Decoupling Semantic Similarity from Spatial Alignment for Neural Networks

Tassilo Wald\({}^{*,1,2,3}\), Constantin Ulrich\({}^{1,4,7}\), Gregor Kohler\({}^{1,3}\),

David Zimmerer\({}^{1,2}\), Stefan Denner\({}^{1,3}\), Michael Baumgartner\({}^{1,3}\),

Fabian Isensee\({}^{1,2}\), Priyank Jaini\({}^{1,5}\), Klaus H. Maier-Hein\({}^{1,1,2,3,4,6}\)

Division of Medical Image Computing,

German Cancer Research Center (DKFZ), Heidelberg, Germany

\({}^{2}\) Helmholtz Imaging, DKFZ, Heidelberg, Germany

\({}^{3}\) Faculty of Mathematics and Computer Science,

University of Heidelberg, Germany

\({}^{4}\) Medical Faculty Heidelberg, University of Heidelberg, Germany

\({}^{5}\) Google Deepmind

\({}^{6}\) Pattern Analysis and Learning Group, Department of Radiation Oncology

\({}^{7}\) National Center for Tumor Diseases (NCT) Heidelberg, Germany

Corresponding author: tassilo.wald@dkfz-heidelberg.deShared last authorship.

###### Abstract

What representation do deep neural networks learn? How similar are images to each other for neural networks? Despite the overwhelming success of deep learning methods key questions about their internal workings still remain largely unanswered, due to their internal high dimensionality and complexity. To address this, one approach is to measure the similarity of activation responses to various inputs. Representational Similarity Matrices (RSMs) distill this similarity into scalar values for each input pair. These matrices encapsulate the entire similarity structure of a system, indicating which input leads to similar responses. While the similarity between images is ambiguous, we argue that the spatial location of semantic objects does neither influence human perception nor deep learning classifiers. Thus this should be reflected in the definition of similarity between image responses for computer vision systems. Revisiting the established similarity calculations for RSMs we expose their sensitivity to spatial alignment. In this paper, we propose to solve this through _semantic RSMs_, which are invariant to spatial permutation. We measure semantic similarity between input responses by formulating it as a set-matching problem. Further, we quantify the superiority of _semantic_ RSMs over _spatio-semantic_ RSMs through image retrieval and by comparing the similarity between representations to the similarity between predicted class probabilities.

## 1 Introduction

Deep neural networks are trained to extract powerful feature representations for a wide range of downstream tasks. Despite this, their inner workings are highly-complex, making understanding _how_ networks solve tasks and _what_ they learn challenging. To obtain a better understanding of these fundamental questions, researchers in the fields of neuroscience,

[MISSING_PAGE_FAIL:2]

representation vectors and hence play an important role. We introduce an exemplary kernel in Section 3 and all kernels used in this paper with some properties in Appendix A.

With the two RSMs \(K\) and \(L\) at hand, it is possible to compare the similarity structure between the two models. As introduced in Kornblith et al.  one can use the Hilbert-Schmidt independence criterion (HSIC) [5; 28] to calculate the level of independence through Centered Kernel Alignment, providing a measure of similarity of the two representations \(Z_{1}\) and \(Z_{2}\) with \(\) denoting the centering matrix.

\[(K,L)=}(KL )\] (1) \[(K,L)=(K,L)}{(K,K) (L,L)}}\] (2)

Alternatively, a variety of different measures based on RSMs are possible for which we refer to Section 3.3 of Klabunde et al. . As highlighted above, the similarity calculation based on RSMs is a two-step process with the first being the calculation of the RSMs and the latter being the comparison of the RSMs. In this paper, we focus on the first step, by quantifying the importance of disentangling semantic similarity from spatial alignment. While not the focus of this paper, we provide qualitative examples of the downstream effect on CKA measures in Appendix G.

## 3 The Semantic Representational Similarity Matrix

Representational Similarity Matrices (RSMs) are designed to reflect the system behavior of interest. The RSM \(K\), originally introduced by Kriegeskorte et al. , represents the similarity structure of a system given a set of inputs \(x\{x_{0}, x_{N}\}\). Each value \(K_{ij}\) in \(K\) quantifies how similar the responses of two inputs \(z_{i}\) and \(z_{j}\) are to each other. The definition of what symmetries between representations similarity measures should be invariant to is a central point of debate. Previous work proposed permutation invariance , invariance to orthogonal transformations , or invariances to invertible linear transformations [25; 19]. While arguments for any of these invariances are valid, we believe that an important aspect has been neglected in the calculation of RSMs: The spatial alignment between the representations!

The dependency on spatial alignmentRevisiting the structure of representations of a CNN, channels \(C\) correspond to semantic concepts while the spatial position corresponds to where the semantic concept is localized in the input image . Consequently, one can reformulate the representation \(z_{i}\) of a sample \(x_{i}\) to be fully defined by a set of **semantic concept vectors v**, one for each spatial location \(S\): \(z_{i}=\{_{0},,_{S}\}\) with \(^{C}\). In the case of linear CKA , the RSMs are then calculated, between semantic concept vectors at the same spatial location. For instance, when employing the linear kernel \(K_{ij}\) can be expressed as:

\[K_{ij}=_{s}_{z_{1},s},_{z_{2},s}\] (3)

Figure 1: Current spatio-semantic RSMs couple semantic similarity with spatial alignment. Our proposal focuses solely on measuring semantic similarity. We achieve this by determining the optimal permutation between two representations and introducing sample-wise permutation invariance.

This formulation emphasizes the coupling of semantic similarity and localization during similarity calculation, which we term as _spatio-semantic RSMs_. This coupling can lead to issues, e.g. when comparing an image to a translated version of itself. Due to the quasi translation-equivariant nature of CNNs3 semantic vectors are translated similarly, changing the alignment of pairs of \(\), leading to a low perceived similarity despite highly similar semantic vectors. This issue is visualized in a small toy example in Fig. 1

### Decoupling Localization and Semantic Content

As shown above, current RSMs compare different input samples without accounting for the lack of spatial alignment. Previous work of Williams et al.  recognized this and introduced translation invariance to RSMs by finding the optimal translation \(a,b\) of the representations \(z^{}_{j}=\{_{0+a,0+b},,_{w+a,h+b}\}\) to maximize similarity \(K_{ij}_{a,b}= z_{i},z^{}_{j}\) through circular shifts.

While this is an improvement to no spatial alignment and emulates a CNN's inherent translation equivariance, we argue that the measure of representational similarity should not be constrained to what the underlying model is invariant to, but the similarity measure should be invariant to the possible spatial configurations of semantic features in the input image.

To motivate this, we propose a thought experiment:

Imagine we have trained a classifier with an augmentation pipeline including rotations. Given an image and a rotated version of the image, we extract representations \(z\) at layer \(i\) once for the normal \(z_{i}\) and once for the rotated image \(z_{i,rot}\). Due to the initial rotation, these representations may differ in earlier layers \(i\), due to the network extracting different edges and corners. However, if the network successfully learned to become invariant to the augmentation, it may have learned to map it to the same semantic vector \(\) at a later layer but at a different spatial location. For such cases, we argue that the similarity between the two representations should be high. Should the model be sensitive to the rotation, no semantically similar representations may be expressed at a later layer, which should lead to a low similarity.

This reasoning can be extended to all kinds of shifts, be they artificial augmentations like shearing or mirroring or natural variations of the input manifold. **Subsequently, we argue that the similarity measure should be invariant to as many spatial shifts as possible. This alone allows one to measure the similarity of representations a model is invariant to, be these learned or designed invariances.** Such variable shifts cannot be captured with simple translation operations.

#### 3.1.1 Introducing permutation invariance

To impose as minimal constraints on spatial structure as possible, we propose to make \(K_{ij}\) invariant to all spatial permutations of the semantic concept vectors \(\). Formalizing this we demand that the similarity \(K_{ij}=k(z_{i},z_{j})=k(z_{i},_{ij}z_{j})\) with \(_{ij}^{S S}\) being a unique permutation matrix for the pair of \(z_{i}\) and \(z_{j}\). To accomplish this, we propose to find the optimal permutation matrix \(_{ij}\) that maximizes the similarity \(K_{ij}\).

\[_{ij}=_{P} k(z_{i},_{ij}z_{j})\] (4)

To find the optimal permutation matrix \(_{ij}\), we decide to use the linear kernel \(,\) to maximize both the magnitude of activation and the direction of vectors, as both magnitude of activation and direction of the vectors matter. This allows us to calculate an affinity matrix \(_{ij}^{S S}\) measuring the similarity between all concept vectors:

\[_{ij}=[_{i,0},_{i,S}]^{}[ _{j,0},_{j,S}].\] (5)

With this affinity matrix, bipartite set-matching algorithms, such as Hungarian matching, can be employed to find the optimal permutation matrix \(_{ij}\) that maximizes the inner product between \(z_{i}\) and \(z^{}_{j}\). Finding all \(_{ij}\) for all pairs \(i,j\) and applying the chosen kernel \(k\) yields the _semantic RSM_. This _semantic RSM_ is invariant to any arbitrary, unique spatial permutation \(_{ij}\) for each pair of representations, and, depending on the choice of kernel \(k\), invariant to orthogonal transformations \(U^{C C}\) along the channel dimension. These _semantic RSMs_ can be used as a drop-in replacement for any other RSM, e.g. for applications such as calculating \((K,L)\) to measure the similarity between systems.

Computational ComplexityFinding the optimal permutation matrix \(_{ij}\) is NP-hard and needs to be repeated for each pair of representations \(z_{i},z_{j}\). With \(N\) samples, this results in \(\) unique permutations that need to be computed for a _semantic RSM_. The overall complexity of bipartite matching algorithms grows with the spatial dimensions cubed, resulting in \((N^{2})(S^{3})\). The outer \((N^{2})\) complexity can be parallelized, or reduced by decreasing the batch size. However, the inner permutation can become time-consuming, especially with large spatial dimensionality. To address this, we provide various approximations to reduce the complexity, which are detailed in Section 4.4. For all later experiments, except the translational toy example, we use the Batch-Optimal approximation with windows size \(b\) 512, with the batch referring to batches of semantic concept vectors \(\) and not samples. The pseudo-code for calculating _semantic RSMs_ is visualized in the Appendix under Algorithm 1.

## 4 Experiments: Semantic vs Spatio-Semantic RSMs

Given the novel permutation-invariant similarity definition, we evaluate the utility of our semantic RSMs relative to spatio-semantic RSMs for various similarity kernels, architectures, and tasks. Across all experiments we compare the linear kernel, the radial basis function (RBF) kernel, and the cosine similarity kernel, see Appendix A for details.

Figure 2: **Semantic RSMs capture similarity independent of spatial localization, in contrast to current spatio-semantic RSMs. We utilize Tiny-ImageNet to generate partially overlapping crops of the same sample (left) and calculate RSMs for a trained ResNet18 model. The plot displays the original spatio-semantic RSMs (middle top) and our proposed semantic RSMs (middle bottom) across various layers for a single batch. Additionally, the distribution of similarity values over multiple batches is shown (right). The results indicate that spatio-semantic RSMs struggle to detect largely identical but translated images, while semantic RSMs exhibit an enhanced off-diagonal in the RSMs and a significant gap between distributions. This demonstrates the capability of our method to detect the same semantics even when translated.**

### Translation sensitivity

To illustrate the problems of coupling semantic content and localization a toy dataset is created using \(84 84\) pixels large, downsampled images of ImageNet . For each image two \(64 64\) crops are extracted, one from the upper-left and one from the lower-left corner, resulting in two images that share \(44 64\) identical pixels (Fig. 2 left). Ten upper-left and ten lower-left crops are then used to extract representations of a ResNet18 , which are subsequently used to calculate spatio-semantic and semantic RSMs at different layers of the architecture (Fig. 2 middle). As kernel, we use the radial basis function, as it provides bounded similarity values allowing a better visualization.

As expected, the _spatio-semantic_ RSM measures low similarity between pairs of overlapping crops, due to the semantic concept vectors not aligning. Only in the last layer, after many pooling operations, the off-diagonal is slightly expressed. Conversely, our _semantic_ RSM is capable of detecting the high semantic similarity of the partially overlapping crops throughout the entire depth of the architecture, as evident by the highly similar off-diagonal.

Aggregating the similarity values between partially-overlapping and between different images across multiple batches, allows us to measure the distribution of similarity values between overlapping crops, and non-related image comparisons. Throughout the entire depth of the architecture, the similarity distributions show that our measure better separates overlapping images from different images. Notably, the similarity distribution in _spatio-semantic_ RSMs shows a significant overlap of the distributions of partially overlapping images and non-related images, making differentiation between them difficult (Fig. 2 right). A similar toy experiment for a ViT-B/16 , is provided in Appendix B.

### Similarity-based retrieval

To test the impact of the _semantic_ RSMs in real-world applications, we now investigate the common task of image retrieval. Each entry in an RSM quantifies a sample-to-sample similarity value, which can be directly used for retrieval. While not specifically designed for it, we argue that better retrieval performance reflects a better inter-sample similarity. This allows us to quantify improvements in the RSM structure. To measure retrieval performance the EgoObjects dataset  is used. It contains frames of video that capture the same scene from different viewing perspectives and lighting conditions. This results in object centers being distributed across the extent of the image.

By randomly sampling 2000 query images and 5000 database images from the test set and using general feature extractors to extract embeddings from them we construct RSMs that allow us to do retrieval. As feature extractors we use CLIP (ViT/B32) , CLIPSeg (Rd64) , DinoV2-Giant , SAM (ViT/B32)  and BIT-50  and as kernels for similarity calculation we use the cosine similarity, RBF and the inner product.

For all RSMs, we retrieve the most similar image that is not part of the same video - the same scene but different conditions are allowed. As multiple objects can be present in each scene,

Figure 3: **Relaxing the constraint of spatial alignment leads to better retrieval.** We leverage general feature extractors to embed images of the EgoObjects dataset. We then compare these embeddings either with or without permutation invariance. PI: Permutation Invariantwe quantify retrieval performance by the F1-Score, measuring the overlap of annotated objects between images. Due to the rather complex dataset, we elaborate this in more detail in Appendix D.1.

Across all architectures and metrics, the inclusion of permutation invariance (PI) for the similarity calculation improves retrieval performance relative to the non-invariant similarity, in some cases with a dramatic difference in performance, see Fig. 3. For models designed for dense downstream tasks like SAM or CLIPSeg, the retrieval performance changes particularly much, while models with more global reasoning, like BiT improve less, relatively.

Qualitative SimilarityAside from a quantitative comparison, we visualize the most similar retrieved images for two exemplary queries of SAM in Fig. 4 as case examples.

**Left Query**: The image displays various utensils scattered on a desk. When retrieving with the permutation-invariant similarity metric two images of the same scene but a very different perspective are successfully retrieved as most similar. Retrieving with the non-invariant similarity metric fails to retrieve similar images, due to lack of spatial alignment of the semantic concepts. Instead, it retrieves images of a whiteboard, possibly due to its spatial alignment with the paper on the desk.

**Right Query**: The image features a blender on a counter. The retrieval based on non-permutation-invariant similarity fails to retrieve any of the semantically similar scenes and returns images with a light switch, likely due to the spatial alignment of the light-switch-looking object to the right of the blender. Contrary, the retrieval based on permutation-invariant similarity correctly returns the blender in all cases from different perspectives. Additional qualitative examples are provided in Appendix D.3.

These experiments display clearly, that demanding spatial alignment can be a significant shortcoming when semantically similar concepts are misaligned. In Fig. 4, the network learned to represent the objects very similarly, despite a shift in perspective, but due to the same objects not aligning anymore, spatio-semantic similarity fails to recognize this. This effect should generalize to other datasets where objects are not heavily centered. For datasets with heavy object-centric behavior, like ImageNet, this should be less pronounced.

Figure 4: **Retrieving by permutation invariant similarity returns similar scenes of different spatial geometry. We visualize the top 3 most similar images according to two exemplary query images for SAM ViT/B32.**

### Output similarity vs Representational Similarity

While the retrieval experiments relate to a rather human notion of similarity, one can raise the question if semantic RSMs are also better at measuring the similarity for classifiers.

For each pair of samples, we can compare how similar the predicted class probabilities of a model are and compare this to the representational similarity. A commonly used metric for this is the Jensen-Shannon Divergence (JSD), which quantifies how dissimilar the two probability distributions are from one another. More details are provided in Appendix E.

Consequently, we use various classifiers trained to predict ImageNet1k from Huggingface and compare the Pearson correlation \(\) between their JSD and the representational similarity of their last hidden layer. We chose to use the Pearson correlation, as it allows observing a direct linear behavior between representational similarity and predictive similarity. Again we measure semantic similarity and spatio-semantic similarity with different kernels. Due to JSD measuring dissimilarity, we want the correlation to be as negative as possible. As models we use multiple ResNets , ViTs , a fine-tuned DinoV2  classifier from and a convnextv2 classifier.

The results, displayed in Table 1, show that for almost all architectures and kernels tested, the permutation invariant similarities are better at capturing the notion of what a classifier deems similar. While better than the spatio-semantic similarity, overall correlations are generally low, indicating that either, the similarity metric is confounded by irrelevant representations, or that the kernels should be improved. Moreover, the RBF kernel sometimes provides a positive correlation indicating it is unsuitable to predict the similarity of output probabilities, whereas the Cosine Similarity and the Inner Product both are consistently negative for all architectures tested.

### Optimizing runtime

Since we find the best possible permutation matrix through linear sum assignment algorithms that maximize the inner product of two samples, we can guarantee that the \(K_{ij,semantic} K_{ij} i,j\). This provides us with an upper bound of similarity that can be leveraged to measure how much of the maximally achievable semantic similarity was measured by the spatio-semantic similarity. Additionally, it can be used as a baseline to estimate the quality of permutation matrices \(_{ij}\) provided by faster, approximative assignment algorithms.

Decreasing computational complexityDetermining the optimal permutation between samples poses a substantial computational challenge with a complexity of \((S^{3})\) for each of the \((N^{2})\) pairs in the same mini-batch, particularly for early layers with large spatial resolution \(S\). Although, in theory, the calculation of the \(K\) matrix needs to be conducted only once for the desired representations, applying the method to representations with larger spatial extents becomes impractical with the demands of optimal matching.

To mitigate runtime, two options are available: reducing the batch size \(N\) to lessen the number of permutation calculations or decreasing the time spent on finding the permutation. Given that scenarios like image retrieval often desire larger batches, our focus is on minimizing the time required to obtain suitable assignments.

    &  \\  &  &  &  \\  & - & **(ours)** PI & - & **(ours)** PI & - & **(ours)** PI \\  ResNet18 & -0.276 & **-0.326** & -0.259 & **-0.270** & -0.176 & **-0.199** \\ ResNet50 & -0.248 & **-0.291** & -0.243 & **-0.261** & 0.040 & **0.029** \\ ResNet101 & -0.192 & **-0.276** & -0.174 & **-0.240** & 0.091 & **0.084** \\ ConvNextV2-Base & **-0.134** & -0.098 & -0.132 & **-0.171** & 0.117 & **0.090** \\ ViT-B/16 & -0.046 & **-0.100** & **-0.045** & -0.026 & -0.077 & **-0.122** \\ ViT-L/32 & -0.138 & **-0.188** & -0.138 & **-0.144** & -0.134 & **-0.166** \\ DinoV2-Giant & -0.012 & **-0.044** & -0.013 & **-0.031** & -0.008 & **-0.048** \\   

Table 1: **Similarity invariant to spatial permutations is better at predicting if the class probabilities will be similar.** PI: Permutation InvariantSolving the optimal bipartite matching between semantic concept vectors is equivalent to the well-known assignment problem [18; 1]. We attempted to find existing approximate algorithms for this purpose. Unfortunately, most established algorithms primarily focus on optimal solutions, and existing approximate algorithm implementations, such as those based on the auction algorithm , are not runtime-optimized, often taking longer than optimal algorithms in our experiments. To enhance computational efficiency nonetheless, we explored three tailored approximation algorithms:

* A Greedy breadth-first matching (**Greedy**)
* An optimal matching of the TopK values based on their Norm, followed by the Greedy algorithm for the remaining samples (**TopK-Greedy**)
* Optimal matching of smaller batches, with samples batched by their Norm (**Batch-Optimal**)

For explicit details on the approximation algorithms, we refer to Appendix F.

We conducted a comprehensive comparison between the approximate algorithms and the optimal algorithm. We compare their runtime per sample and the quality of matches, quantified by the average relative similarity \(}\). The evaluation utilized representations from a ResNet18 on TinyImageNet, as illustrated in Fig. 6.

It can be seen that the measured spatio-semantic similarity for TinyImageNet samples are, on average, 30% lower with layers of higher spatial resolution exceeding 40%. This suggests a notable misalignment of semantic concept vectors. Notably, the Batch-Optimal approximation stands out as a reliable approximation for optimal matching. The fastest of the Batch-Optimal approximation methods shows \(<8\%\) error while improving run-time \( 36\) relative to the fastest optimal algorithm for spatial extent \(4096\), while no spatial alignment shows \(42\%\) deviation from the optimal matching. Moreover, we highlight the time vs accuracy trade-off of the different optimal and approximate algorithms in Appendix F.1. Furthermore, it can be seen that the changes between spatio-semantic and semantic RSMs are anisotropic, as highlighted in Fig. 6, indicating scale invariant downstream applications may be influenced.

## 5 Discussion, Limitations, and Conclusion

The concept of Representational Similarity Matrices (RSMs) is a powerful tool to represent the similarity structure of complex systems. In this paper we revisit the construction of such RSMs for neural networks of the vision domain, question the current state, and propose _semantic RSMs_, warranting discussion.

Spatio-semantic couplingBeing aware that current, _spatio-semantic_ RSMs demand semantic concepts to be aligned is highly relevant to understand what RSMs are sensitive to. Previous work  identified this shortcoming and proposed translation invariance, partially addressing this issue. We argue translation invariance is insufficient, since models may learn invariances during training, which the translation invariant metric would not be sensitive to. Subsequently, we propose a new - spatially permutation invariant - similarity measure between samples that allows the detection of similarity whenever a model expresses similar semantic vectors in its representations, irrespective of spatial geometry. To highlight the benefits of our similarity, we propose that better similarity measures should allow more accurate retrieval when comparing last-layer representations and should allow better predictions about the similarity of class probabilities of a classifier. However, we acknowledge certain limitations in our current evaluation. Specifically, we have not yet compared our method to more established retrieval techniques. Traditional retrieval methods are often not applied to representations directly but utilize a lower-dimensional non-spatial, global vector representing the entire sample. In contrast, we chose to limit ourselves to methods that are directly applied to the representations.

Computational ComplexityAside from quantitative or qualitative benefits, the construction of semantic RSMs is time-consuming, limiting its applicability. This complexity mostly affects layers of large spatial extent, which mostly corresponds to early CNN layers while later layers and ViTs are unproblematic. Our proposed Batch-optimal approximation alleviates this partially, yet application to large-scale representations at higher resolution, like at the output of a segmentation architecture with spatial extents of s=65.536 would be too costly. We leave optimizing the compute efficiency or finding better approximations for future work.

ConclusionIn conclusion, our investigation into semantic RSMs has shed light on the limitations of spatio-semantic RSMs and introduced a novel approach to disentangle spatial alignment from semantic similarity. The proposed method provides a more accurate measure of how representations capture underlying semantic content, showcasing its potential in various applications, particularly in scenarios where spatial alignment cannot be assumed. While challenges such as computational complexity and scalability need to be addressed, the findings open avenues for further research and improvement in the analysis of neural network representations.