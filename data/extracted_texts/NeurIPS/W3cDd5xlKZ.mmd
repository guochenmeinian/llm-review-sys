# Fair Canonical Correlation Analysis

Zhuoping Zhou\({}^{}\), Davoud Ataee Tarzanagh\({}^{}\), Bojian Hou\({}^{}\)

Boning Tong, Jia Xu, Yanbo Feng, Qi Long\({}^{}\), Li Shen\({}^{}\)

University of Pennsylvania

{zhuopinz@sas.,tarzanaq@,boningt@seas.,jiaxu7@,yanbof@seas.jupenn.edu

{bojian.hou,qlong,li.shen}@pennmedicine.upenn.edu

###### Abstract

This paper investigates fairness and bias in Canonical Correlation Analysis (CCA), a widely used statistical technique for examining the relationship between two sets of variables. We present a framework that alleviates unfairness by minimizing the correlation disparity error associated with protected attributes. Our approach enables CCA to learn global projection matrices from all data points while ensuring that these matrices yield comparable correlation levels to group-specific projection matrices. Experimental evaluation on both synthetic and real-world datasets demonstrates the efficacy of our method in reducing correlation disparity error without compromising CCA accuracy.

## 1 Introduction

Canonical Correlation Analysis (CCA) is a multivariate statistical technique that explores the relationship between two sets of variables . Given two datasets \(^{N D_{x}}\) and \(^{N D_{y}}\) on the same set of \(N\) observations,1 CCA seeks the \(R\)-dimensional subspaces where the projections of \(\) and \(\) are maximally correlated, i.e. finds \(^{D_{x} R}\) and \(^{D_{y} R}\) such that

\[\ (^{}^{ })^{} ^{}=^{}^{} =_{R}.\] (CCA)

CCA finds applications in various fields, including biology , neuroscience , medicine , and engineering , for unsupervised or semi-supervised learning. It improves tasks like clustering, classification, and manifold learning by creating meaningful dimensionality-reduced representations . However, CCA can exhibit _unfair_ behavior when analyzing data with protected attributes, like sex or race. For instance, in Alzheimer's disease (AD) analysis, CCA can establish correlations between brain imaging and cognitive decline. Yet, if it does not consider the influence of sex, it may result in disparate correlations among different groups because AD affects males and females differently, particularly in cognitive decline .

The influence of machine learning on individuals and society has sparked a growing interest in the topic of fairness . While fairness techniques are well-studied in supervised learning , attention is shifting to equitable methods in unsupervised learning . Despite extensive work on fairness in machine learning, fair CCA (F-CCA) remains unexplored. This paper investigates F-CCA and introduces new approaches to mitigate bias in (CCA).

For further discussion, we compare CCA with our proposed F-CCA in sample projection, as illustrated in Figure 1. In Figure 1(a), we have samples \(_{1}\) and \(_{2}\) from matrix \(\), and in Figure 1(b), their corresponding samples \(_{1}\) and \(_{2}\) are from matrix \(\). CCA learns \(\) and \(\) to maximize correlation,inversely related to the angle between the sample vectors. Figure 1(c) demonstrates the proximity within the projected sample pairs \((^{}_{1},^{}_{1})\) and \((^{}_{2},^{}_{2})\). In Figure 1(d)-(i), we compare the results of different learning strategies. There are five pairs of samples, with female pairs highlighted in red and male pairs shown in blue. Random projection (Figure 1(e)) leads to randomly large angles between corresponding sample vectors. CCA reduces angles compared to random projection (Figure 1(f)), but significant angle differences between male and female pairs indicate bias. Using sex-based projection matrices heavily biases the final projection, favoring one sex over the other (Figures 1(g) and 1(h)). To address this bias, our F-CCA maximizes correlation within pairs and ensures equal correlations across different groups, such as males and females (Figure 1(i)). Note that while this illustration represents individual fairness, the desired outcome in practice is achieving similar average angles for different groups.

**Contributions.** This paper makes the following key contributions:

* We introduce fair CCA (F-CCA), a model that addresses fairness issues in (CCA) by considering multiple groups and minimizing the correlation disparity error of protected attributes. F-CCA aims to learn global projection matrices from all data points while ensuring that these projection matrices produce a similar amount of correlation as group-specific projection matrices.
* We propose two optimization frameworks for F-CCA: multi-objective and single-objective. The multi-objective framework provides an automatic trade-off between global correlation and equality in group-specific correlation disparity errors. The single-objective framework offers a simple approach to approximate fairness in CCA while maintaining a strong global correlation, requiring a tuning parameter to balance these objectives.
* We develop a gradient descent algorithm on the generalized Stiefel manifold to solve the multi-objective problem, with convergence guarantees to a Pareto stationary point. This approach extends Riemannian gradient descent [8; 9] to multi-objective optimization, accommodating a broader range of retraction maps than exponential retraction [23; 6]. Furthermore, we provide a similar algorithm for single-objective problems, also with convergence guarantees to a stationary point.
* We provide extensive empirical results showcasing the efficacy of the proposed algorithms. Comparison against the CCA method on synthetic and real datasets highlights the benefits of the F-CCA approach, validating the theoretical findings 2. 
**Organization:** Section 2 covers related work. Our proposed approach is detailed in Section 3, along with its theoretical guarantees. Section 4 showcases numerical experiments, while Section 5 discusses implications and future research directions.

Figure 1: Illustration of CCA and F-CCA, with the sensitive attribute being sex (female and male). Figures (a)–(c) demonstrate the general framework of CCA, while Figures (d)–(i) provide a comparison of the projected results using various strategies. It is important to note that the correlation between two corresponding samples is inversely associated with the angle formed by their projected vectors. F-CCA aims to equalize the average angles among different groups.

Related work

**Canonical Correlation Analysis (CCA).** CCA was first introduced by [28; 29]. Since then, it has been utilized to explore relations between variables in various fields of science, including economics , psychology [19; 27], geography , medicine , physics , chemistry , biology , time-series modeling , and signal processing . Recently, CCA has demonstrated its applicability in modern fields of science such as neuroscience, machine learning, and bioinformatics [59; 60]. CCA has been used to explore relations for developing brain-computer interfaces [10; 46] and in the field of imaging genetics . CCA has also been applied for feature selection , feature extraction and fusion , and dimension reduction . Additionally, numerous studies have applied CCA in bioinformatics and computational biology, such as [54; 56; 58]. The broad range of application domains highlights the versatility of CCA in extracting relations between variables, making it a valuable tool in scientific research.

**Fairness.** Fairness in machine learning has been a growing area of research, with much of the work focusing on fair supervised methods [5; 16; 18; 20; 67; 78]. However, there has also been increasing attention on fair methods for unsupervised learning tasks [11; 12; 15; 34; 33; 49; 55; 64; 50; 66]. In particular, Samadi et al.  proposed a semi-definite programming approach to ensure fairness in PCA. Kleindessner et al. [33; 34] focused on fair PCA formulation for multiple groups and proposed a kernel-based fair PCA. Kamani et al.  introduced an efficient gradient method for fair PCA, addressing multi-objective optimization. In this paper, we propose a novel multi-objective framework for F-CCA, converting constrained F-CCA problems to unconstrained ones on a generalized Riemannian manifold. This framework enables the adaptation of efficient gradient techniques for numerical optimization on Riemannian manifolds.

**Riemannian Optimization.** Riemannian optimization extends Euclidean optimization to smooth manifolds, enabling the minimization of \(f()\) on a Riemannian manifold \(\) and converting constrained problems into unconstrained ones [1; 8]. It finds applications in various domains such as matrix/tensor factorization [31; 63], PCA , and CCA . Specifically, CCA can be formulated as Riemannian optimization on the Stiefel manifold [13; 43]. In our work, we utilize Riemannian optimization to develop a multi-objective framework for F-CCAs on generalized Stiefel manifolds.

## 3 Fair Canonical Correlation Analysis

This section introduces the formulation and optimization algorithms for F-CCA.

### Preliminary

Real numbers are represented as \(\), with \(_{+}\) for nonnegative values and \(_{++}\) for positives. Vectors and matrices use bold lowercase and uppercase letters (e.g., \(\), \(\)) with elements \(a_{i}\) and \(a_{ij}\). For \(,^{m}\), \(\) and \(\) mean \(-_{++}^{m}\) and \(-_{+}^{m}\), respectively. For a symmetric matrix \(^{N N}\), \( 0\) and \( 0\) denote positive definiteness and positive semidefiniteness (PSD), respectively. \(_{D}\), \(_{D}\), and \(_{D}\) are \(D D\) identity, all-ones, and all-zeros matrices. \(_{i}()\) stands for the \(i\)-th singular values of \(\). Matrix norms are defined as \(\|\|_{1}=_{ij}|a_{ij}|\), \(\|\|=_{i}_{i}()\), and \(\|\|_{}:=(_{ij}|a_{ij}|^{2})^{1/2}\). We introduce some preliminaries on manifold optimization [1; 6; 8]. Given a PSD matrix \(^{D D}\), the generalized Stiefel manifold is defined as

\[(D,R,)=\{^{D R}\; \;^{}=_{R}\}.\] (1)

The tangent space of the manifold \(=(D,R,)\) at \(\) is given by

\[_{}=\{^{D R }\;^{}+^{} =_{R}\}.\] (2)

The tangent bundle of a smooth manifold \(\), which consists of \(_{}\) at all \(\), is defined as

\[=\{(,)\; ,\;_{}\}.\] (3)

**Definition 1**.: _A retraction on a differentiable manifold \(\) is a smooth mapping from its tangent bundle \(\) to \(\) that satisfies the following conditions, with \(R^{}\) being the retraction of \(R\) to \(_{}\):_

1. \(R^{}()=\)_, for all_ \(\)_, where_ \(\) _denotes the zero element of_ \(_{}\)_._2. _For any_ \(\)_, it holds that_ \(_{_{} 0}()-(+)\|_{F}}{\|\|_{F}}=0\)_._

In the numerical experiments, this work employs a generalized polar decomposition-based retraction. Given a PSD matrix \(^{D D}\), for any \(_{}\) with \(=(D,R,)\), it is defined as:

\[R^{}()=}( {}^{-}^{})}^{},\] (4)

where \(}}^{}=\) is the singular value decomposition of \(\), and \(,\) are obtained from the eigenvalue decomposition \(^{}=}^{} }\). Further details on retraction choices are in Appendix A.1.

### Correlation Disparity Error

As previously mentioned, applying CCA to the entire dataset could lead to a biased result, as some groups might dominate the analysis while others are overlooked. To avoid this, we can perform CCA separately on each group and compare the results. Indeed, we can compare the performance of CCA on each group's data with the performance of CCA on the whole dataset, which includes all groups' data. The goal is to find a balance between the benefits and sacrifices of different groups so that each group's contribution to the CCA analysis is treated fairly. In particular, suppose the datasets \(^{N D_{x}}\) and \(^{N D_{y}}\) on the same set of \(N\) observations, belong to \(K\) different groups \(\{(^{k},^{k})\}_{k=1}^{K}\) with \(^{k}^{N_{k} D_{x}}\) and \(^{k}^{N_{k} D_{y}}\), based on demographics or some other semantically meaningful clustering. These groups need not be mutually exclusive; each group can be defined as a different weighting of the data.

To determine how each group is affected by F-CCA, we can compare the structure learned from each group's data \((^{k},^{k})\) with the structure learned from all groups' data combined \((,)\). A fair CCA approach seeks to balance the benefits and drawbacks of each group's contribution to the analysis. Specifically, if we train global subspaces \(^{D_{x} R}\) and \(^{D_{y} R}\) on \(k\)-th group dataset \((^{k},^{k})\), we can identify the group-specific (local) weights represented by \((^{k},^{k})\) that has the best performance on that dataset. Thus, F-CCA algorithm should be able to learn global weights \((,)\) on all data points while ensuring that each group's correlation on the CCA learned by the whole dataset is equivalent to the group-specific subspaces learned only by its own data.

To define these fairness criteria, we introduce correlation disparity error as follows:

**Definition 2** (**Correlation Disparity Error**).: _Consider a pair of datasets \((,)\) with \(K\) sensitive groups with data matrix \(\{(^{k},^{k})\}_{k=1}^{K}\) representing each sensitive group's data samples. Then, for any \((,)\), the correlation disparity error for each sensitive group \(k[K]\) is defined as_

\[^{k}(,):=( ^{k,}{}^{}^{k}{}^{}^{k}^ {k,})-(^{}^{k}{}^{} ^{k}), 1 k K.\] (5)

_Here, \((^{k,},^{k,})\) is the maximizer of the following group-specific CCA problem:_

\[\ \ (^{k}{}^{}^{k}{}^{ }^{k}^{k})\ \ \ \ \ ^{k}{}^{}^{k}{}^{}^{k}^{k}= ^{k}{}^{}^{k}{}^{}^{k}^{k}= _{R}.\] (6)

This measure shows how much correlation we are suffering for any global \((,)\), with respect to the loss of optimal local \((^{k,},^{k,})\) that we can learn based on data points \((^{k},^{k})\).

Using Definition 2, we can define F-CCA as follows:

**Definition 3** (**Fair CCA**).: _A CCA pair \((^{},^{})\) is called fair if the correlation disparity error among \(K\) different groups is equal, i.e.,_

\[^{k}(^{},^{})=^ {s}(^{},^{}), k s,  k,s[K].\] (7)

_A CCA pair \((^{},^{})\) that achieves the same disparity error for all groups is called a fair CCA._

Next, we introduce the concept of pairwise correlation disparity error for CCA, which measures the variation in correlation disparity among different groups.

**Definition 4** (**Pairwise Correlation Disparity Error**).: _The pairwise correlation disparity error for any global \((,)\) and group-specific subspaces \(\{(^{k,},^{k,})\}_{k=1}^{K}\), is defined as_

\[^{k,s}(,):=(^{k}( ,)-^{s}(,) ), k s, k,s[K].\] (8)

_Here, \(:_{+}\) is a penalty function such as \((x)=(x)\), \((x)=x^{2}\), or \((x)=|x|\)._The motivation for incorporating pairwise correlation disparity error in our approach can be attributed to the work by [40; 55] in the context of PCA. To facilitate convergence analysis, we will primarily consider smooth penalization functions, such as squared or exponential penalties.

### A Multi-Objective Framework for Fair CCA

In this section, we introduce an optimization framework for balancing correlation and disparity errors. Let \(f_{1}(,):=-( ^{}^{}),f_{2}(, ):=^{1,2}(,),,f_{M} (,):=^{K-1,K}(,)\). The optimization problem of finding an optimal Pareto point of \(\) is denoted by

\[,}{}&(,):=[f_{1}(,),f_{2} (,),,f_{M}(, )],\\ &,\;\;\;,\] (9)

where \(:=\{^{D_{y} R}^{ }^{}=_{R}\}\) and \(:=\{^{D_{y} R}^{ }^{}=_{R}\}\).

A point \((,)\) satisfying \(((,))(-_{++}^{M} )=\) is called _critical Pareto_. Here, \(\) denotes the image of Jacobian of \(\). An _optiman Pareto point_ of \(\) is a point \((^{},^{})\) such that there exists no other \((,)\) with \((,)(^{}, ^{})\). Moreover, a point \(^{},^{}\) is a _weak optimal Pareto_ of \(\) if there is no \((,)\) with \((,)(^{},^{})\). The multi-objective framework (9) addresses the challenge of handling conflicting objectives and achieving optimal trade-offs between them.

To effectively solve Problem (9), we propose utilizing a gradient descent method on the manifold \(\) that ensures convergence to a _Pareto stationary point_. The proposed gradient descent algorithm for solving (9) is provided in **Algorithm 1**. For each \((,)\), let \(:=(^{},^{})\) with \(^{}_{}\) and \(^{}_{}\). The iterates \((^{}_{+},^{}_{+})\) in Step 4 are obtained by solving the following subproblem in the joint tangent plane \(_{}_{}\):

\[_{_{}_{ }}\;\;Q_{t}(),\;\;\;\;Q_{t}( ):=\{_{i[M]}(^{ } f_{i}((_{t},_{t})))+\|\| _{}^{2}\}.\] (10)

If \((_{t},_{t})\) is not a Pareto stationary point, Problem (10) has a unique nonzero solution \(_{t}\) (see Lemma 7), known as the _steepest descent direction_ for \(\) at \((_{t},_{t})\). In Steps 5 and 6, \(R^{}\) and \(R^{}\) denote the retractions onto the tangent spaces \(_{}\) and \(_{}\), respectively; refer to Definition 1.

**Assumption A**.: _For a given subset \(\) of the tangent bundle \(\), there exists a constant \(L_{F}\) such that, for all \((,)\), we have \((R^{}())()+ {}+(L_{F}/2)\|\|_{}^{2}\,_{M},\) where \(_{i}:= f_{i}(),\), \(:=[_{1},,_{M}]^{}^{M}\), and \(R^{}\) is the retraction._

The above assumption extends [8; A 4.3] to multi-objective optimization, and it always holds for the _exponential_ map (exponential retraction) if the gradient of \(\) is \(L_{F}\)-Lipschitz continuous [23; 6].

**Theorem 5**.: _Suppose Assumption A holds. Let \((_{t},_{t})\) be the sequence generated by MF-CCA. Let \(f_{i}^{*}:=\{f_{i}(,):\;(,) \}\), for all \(i[M]\) and define \(f_{i_{*}}(_{0},_{0})-f_{i_{*}}^{*}:=\{f_{i}( _{0},_{0})-f_{i}^{*}:\;i[M]\}\). If \(_{t}^{}=_{t}^{}= 1/L_{F}\) for all \(t\{0,,T-1\}\), then_

\[\{\|_{t}\|_{}:\;t=0,,T-1\} {}[}(_{0},_{0})-f_{i_{*}}^{*}}{T} ]^{}.\]Proof Sketch.: We employ Lemma 7 to establish the unique solution \(_{t}\) for subproblem (10). Lemmas 9 and 10 provide estimates for the decrease of function \(\) along \(_{t}\): For any \(_{t} 0\), we have \((_{t+1},_{t+1})(_{t}, _{t})-(_{t}-L_{F}_{t}^{2}/2)\|_{t} \|_{}^{2}_{M}\). Summing this inequality over \(t=0,1,,T-1\) and applying our step size condition yields the desired result. 

Theorem 5 provides a generalization of [8, Corollary 4.9] to the multi-objective optimization, showing that the norm of Pareto descent directions converges to zero. Consequently, the solutions produced by the algorithm converge to a stationary fair subspace. It is worth mentioning that multi-objective optimization in  relies on the Riemannian exponential map, whereas the above theorem covers broader (and practical) retraction maps.

### A Single-Objective Framework for Fair CCA

In this section, we introduce a straightforward and effective single-objective framework. This approach simplifies F-CCA optimization, lowers computational requirements, and allows for fine-tuning fairness-accuracy trade-offs using the hyperparameter \(\). Specifically, by employing a regularization parameter \(>0\), our proposed fairness model for F-CCA is expressed as follows:

\[,\\ }{}&f(,):=- {trace}(^{}^{})+ (,),\\ &,\;\;\;, \] (11)

where \((,)=_{i,j[K],i j}^{i,j} (,)\); see Definiton 4.

The choice of \(\) in the model determines the emphasis placed on different objectives. When \(\) is large, the model prioritizes fairness over minimizing subgroup errors. Conversely, if \(\) is small, the focus shifts towards minimizing subgroup correlation errors rather than achieving perfect fairness. In other words, it is possible to obtain perfectly F-CCA subspaces; however, this may come at the expense of larger errors within the subgroups. The constant \(\) in the model allows for a flexible trade-off between fairness and minimizing subgroup correlation errors, enabling us to find a balance based on the specific requirements and priorities of the problem at hand.

The proposed gradient descent algorithm for solving (11) is provided as **Algorithm 2**.: For each \((,)\), let \(:=(^{},^{})\) with \(^{}_{}\) and \(^{}_{}\). The iterates \((_{t}^{},_{t}^{})\) are obtained by solving the following problem in the joint tangent plane \(_{}_{}\):

\[_{_{}_{ }}\;\;q_{t}(),\;\;\;\;q_{t}( ):=\{(^{} f(( _{t},_{t})))+\|\|_{}^ {2}\}.\] (12)

The solutions \((_{t}^{},_{t}^{})\) are maintained on the manifolds using the retraction operations \(R^{}\) and \(R^{}\).

**Assumption B**.: _For a subset \(\), there exists a constant \(L_{f}\) such that for all \((,)\), \(f(R^{}())()+ f( ),+(L_{f}/2)\|\| _{}^{2},\) with \(R^{}\) as the retraction._

**Theorem 6**.: _Suppose Assumption B holds. Let \((_{t},_{t})\) be the sequence generated by_ SF-CCA_. Let \(f^{*}:=\{f(,):\;(,) \}\). If \(_{t}^{}=_{t}^{}= 1/L_{f}\) for all \(t[T]\), then_

\[\{\|_{t}\|_{}:\;t=0,,T-1\}[_{0},_{0})-f^{*}}{T}]^{ {2}}.\]

**Comparison between MF-CCA and SF-CCA:** MF-CCA addresses conflicting objectives and achieves optimal trade-offs automatically, but it necessitates the inclusion of \(\) additional objectives. SF-CCA, on the other hand, provides a simpler approach but requires tuning an extra hyperparameter \(\). When choosing between the two methods, it is crucial to consider the trade-off between complexity and simplicity, as well as the number of objectives and the need for hyperparameter tuning.

## 4 Experiments

In this section, we provide empirical results showcasing the efficacy of the proposed algorithms.

### Evaluation Criteria and Selection of Tuning Parameter

F-CCA's performance is evaluated on correlation and fairness for each dimension of subspaces. Let \(=[_{1},,_{R}]^{D_{x} R}\) and \(=[_{1},,_{R}]^{D_{y} R}\). The \(r\)-th canonical correlation is defined as follows:

\[_{r}=_{r}^{}^{} _{r}}{_{r}^{}^{}_{r}_{r}^{}^{}^{}_{r}}},  r=1,,R.\] (13a) Next, in terms of fairness, we establish the following two key measures: \[_{,r} =_{i,j[K]}|^{i}(_{r},_{r}) -^{j}(_{r},_{r})|, r=1,,R,\] (13b) \[_{,r} =_{i,j[K]}|^{i}(_{r},_{r}) -^{j}(_{r},_{r})|, r=1,,R.\] (13c) Here, \[_{,r}\] measures maximum disparity error, while \[_{,r}\] represents aggregate disparity error. The aim is to reach \[_{,r}\] and \[_{,r}\] of \[0\] without sacrificing correlation ( \[_{r}\] ) compared to CCA. We conduct a detailed analysis using component-wise measurements ( 13 ) instead of matrix versions; for more discussions, see Appendix C.2.

The canoncorr function from MATLAB and  is used to solve (CCA). For MF-CCA and SF-CCA, the learning rate is searched on a grid in \(\{1e-1,5e-2,1e-2,,1e-5\}\), and for SF-CCA, \(\) is searched on a grid in \(\{1e-2,1e-1,0.5,1,2,,10\}\). Sensitivity analysis of \(\) is provided in Appendix B.2. The learning rate decreases with the square root of the iteration number. Termination of algorithms occurs when the descent direction norm is below \(1e-4\).

### Dataset

#### 4.2.1 Synthetic Data

Following [44; 4], our synthetic data are generated using the Gaussian distribution

\[\\  N(_{}\\ _{},_{}& _{}\\ _{}&_{} ).\]

Here, \(_{}^{D_{x} 1}\) and \(_{}^{D_{y} 1}\) are the means of data matrices \(\) and \(\), respectively; covariance matrices \(_{},_{}\) and the cross-covariance matrix \(_{}\) are constructed as follows. Given ground truth projection matrices \(^{D_{x} R},^{D_{y} R}\) and canonical correlations \(=(_{1},_{2},,_{R})\) defined in (13a). Let \(=_{}_{}\) and \(=_{}_{}\) be the QR decomposition of \(\) and \(\), then we have

\[_{} =_{}()\ ^{}_{},\] (14a) \[_{} =_{}_{}{}^{-}_{}{}^{-1}_{}{}^{}+_{x}_{ }(_{D_{x}}-_{}_{} {}^{})_{}{}^{},\] (14b) \[_{} =_{}_{}{}^{-1}_{ }{}^{-1}_{}{}^{}+_{y}_{ }(_{D_{y}}-_{}_{} {}^{})_{}{}^{}.\] (14c)

Here, \(_{}^{D_{x} D_{x}}\) and \(_{}^{D_{y} D_{y}}\) are randomly generated by normal distributions, and \(_{x}=1\) and \(_{y}=0.001\) are scaling hyperparameters. For subgroup distinction, we added noise to canonical vectors and adjusted sample sizes: 300, 350, 400, 450, and 500 observations each. In the numerical experiment, different canonical correlations are assigned to each subgroup alongside two global canonical vectors \(\) and \(\) to generate five distinct subgroups.

#### 4.2.2 Real Data

**National Health and Nutrition Examination Survey (NHANES).** We utilized the 2005-2006 subset of the NHANES database https://www.cdc.gov/nchs/nhanes, including physical measurements and self-reported questionnaires from participants. We partitioned the data into two distinct subsets: one with 96 phenotypic measures and the other with 55 environmental measures. Our objective was to apply F-CCA to explore the interplay between phenotypic and environmental factors in contributing to health outcomes, considering the impact of education. Thus, we segmented the dataset into three subgroups based on educational attainment (i.e., lower than high school, high school, higher than high school), with 2,495, 2,203, and 4,145 observations in each subgroup.

**Mental Health and Academic Performance Survey (MHAAPS).** This dataset is available at https://github.com/marks/convert_to_csv/tree/master/sample_data. It consists of three psychological variables, four academic variables, as well as sex information for a cohort of 600 college freshmen (327 females and 273 males). The primary objective of this investigation revolves around examining the interrelationship between the psychological variables and academic indicators, with careful consideration given to the potential influence exerted by sex.

**Alzheimer's Disease Neuroimaging Initiative (ADNI).** We utilized AV45 (amyloid) and AV1451 (tau) positron emission tomography (PET) data from the ADNI database (http://adni.loni.usc.edu) [73; 74]. ADNI data are analyzed for fairness in medical imaging classification [41; 53; 81], and sex disparities in ADNI's CCA study can harm generalizability, validity, and intervention tailoring. We utilized F-CCA to account for sex differences. Our experiment links 52 AV45 and 52 AV1451 features in 496 subjects (255 females, 241 males).

### Results and Discussion

In the simulation experiment, we follow the methodology described in Section 4.2.1 to generate two sets of variables, each containing two subgroups of equal size. Canonical weights are trained and used to project the two sets of variables into a 2-dimensional space using CCA, SF-CCA, and MF-CCA. From Figure 2, it is clear that the angle between the distributions of the two subgroups, as projected by SF-CCA and MF-CCA, is smaller in comparison. This result indicates that F-CCA has the ability to reduce the disparity between distinct subgroups.

Table 1 shows the quantitative performance of the three models: CCA, MF-CCA, and SF-CCA. They are evaluated based on \(_{r}\), \(_{,r}\), and \(_{,r}\) defined in (13) across five experimental sets. Table 2 displays the mean runtime of each model. Several key observations emerge from the analysis. Firstly, MF-CCA and SF-CCA demonstrate substantial improvements in fairness compared to CCA. However, it is important to note that F-CCA, employed in both MF-CCA and SF-CCA, compromises some degree of correlation due to its focus on fairness considerations during computations. Secondly,

    &  & \)} & \)} & ,r}\)} \\  & & (\(r\)) & CCA & MF-CCA & SF-CCA & CCA & MF-CCA & SF-CCA & CCA & MF-CCA & SF-CCA \\  Synthetic & 2 & **0.7533** & 0.7475 & 0.7309 & 0.3555 & 0.2866 & **0.2241** & 3.3802 & 2.8119 & **2.2722** \\ Data & 5 & **0.4717** & 0.4681 & 0.4581 & 0.4385 & 0.3313 & **0.2424** & 4.1649 & 3.1628 & **2.2304** \\  NHANES & 2 & **0.6392** & 0.6360 & 0.6334 & 0.0485 & 0.0359 & **0.0245** & 0.1941 & 0.1435 & **0.0980** \\  & 5 & **0.4416** & 0.4393 & 0.4392 & 0.1001 & **0.0818** & 0.0824 & 0.4003 & **0.3272** & 0.3297 \\  MHAAPS & 1 & **0.4464** & 0.4451 & 0.4455 & 0.0093 & 0.0076 & **0.0044** & 0.0187 & 0.0152 & **0.0088** \\  & 2 & **0.1534** & 0.1529 & 0.1526 & 0.0061 & 0.0038 & **0.0019** & 0.0122 & 0.0075 & **0.0039** \\  ADNI & 2 & **0.7778** & 0.7776 & 0.7753 & 0.0131 & 0.0119 & **0.0064** & 0.0263 & 0.0238 & **0.0127** \\  & 5 & **0.6810** & 0.6798 & 0.6770 & 0.0477 & 0.0399 & **0.0324** & 0.0954 & 0.0799 & **0.0648** \\   

Table 1: Numerical results in terms of Correlation (\(_{r}\)), Maximum Disparity (\(_{,r}\)), and Aggregate Disparity (\(_{,r}\)) metrics. Best values are in bold, and second-best are underlined. We focus on the initial five projection dimensions, but present only two dimensions here; results for other dimensions are in the supplementary material. We put the results of other projection dimensions in the supplementary material. “\(\)” means the larger the better and “\(\)” means the smaller the better. Note that MHAAPS has only 3 features, so we report results for its 1 and 2 dimensions.

  
**Dataset** & **CCA** & **MF-CCA** & **SF-CCA** \\  Synthetic Data & 0.0239\(\)0.0026 & 109.0693\(\)5.5418 & 29.1387\(\)2.0828 \\  NHANES & 0.0483\(\)0.0059 & 42.3186\(\)1.9045 & 14.9156\(\)1.8941 \\  MHAAPS & 0.0021\(\)0.0047 & 3.5235\(\)2.0945 & 0.8238\(\)0.8155 \\  ADNI & 0.0039\(\)0.0032 & 2.7297\(\)0.5136 & 1.8489\(\)1.0519 \\   

Table 2: Mean computation time in seconds (\(\)std) of 10 repeated experiments for \(R=5\) on the real dataset and \(R=7\) on the synthetic dataset. Experiments are run on Intel(R) Xeon(R) CPU E5-2660.

SF-CCA outperforms MF-CCA in terms of fairness improvement, although it sacrifices correlation. This highlights the effectiveness of the single-objective optimization approach in SF-CCA. Moreover, the datasets consist of varying subgroup quantities (5, 3, 2, and 2) and an imbalanced number of samples in distinct subgroups. F-CCA consistently performs well across these datasets, confirming its inherent scalability. Lastly, although SF-CCA requires more effort to tune hyperparameters, SF-CCA still exhibits a notable advantage in terms of time complexity compared to MF-CCA, demonstrating computational efficiency. Disparities among various CCA methods are visually represented in Figure 3. Notably, the conventional CCA consistently demonstrates the highest disparity error. Conversely, SF-CCA and MF-CCA consistently outperform CCA across all datasets, underscoring their efficacy in promoting fairness within analytical frameworks.

In Table 1, we define the _percentage change_ of correlation (\(_{r}\)), maximum disparity gap (\(_{,r}\)), and aggregate disparity (\(_{,r}\)), respectively, as follows: \(P_{r}:=(_{r}-_{r})/(_{r}) 100\), \(P_{,r}:=-(_{,r}-_{,r})/(_{,r})  100\), and \(P_{,r}:=-(_{,r}-_{,r})/(_{,r}) 100\). Here, F-CCA is replaced with either MF-CCA or

Figure 3: Aggregate disparity of CCA, MF-CCA, and SF-CCA (results from Table 1).

Figure 2: Scatter plot of the synthetic data points after projected to the 2-dimensional space. The distributions of the two groups after projection by CCA are orthogonal to each other. Our SF-CCA and MF-CCA can make the distributions of the two groups close to each other.

SF-CCA to obtain the percentage change for MF-CCA or SF-CCA. Figure 4 illustrates the percentage changes of each dataset. \(P_{r}\) is slight, while \(P_{,r}\) and \(P_{,r}\) changes are substantial, signifying fairness improvement without significant accuracy sacrifice.

## 5 Conclusion, Limitations, and Future Directions

We propose F-CCA, a novel framework to mitigate unfairness in CCA. F-CCA aims to rectify the bias of CCA by learning global projection matrices from the entire dataset, concurrently guaranteeing that these matrices generate correlation levels akin to group-specific projection matrices. Experiments show that F-CCA is effective in reducing correlation disparity error without sacrificing much correlation. We discuss potential extensions and future problems stemming from our work.

* While F-CCA effectively reduces unfairness while maintaining CCA model accuracy, its potential to achieve a minimum achievable disparity correlation remains unexplored. A theoretical exploration of this aspect could provide valuable insights.
* F-CCA holds promise for extensions to diverse domains, including multiple modalities , deep CCA , tensor CCA , and sparse CCA . However, these extensions necessitate novel formulations and in-depth analysis.
* Our approach of multi-objective optimization on smooth manifolds may find relevance in other problems, such as fair PCA . Further, bilevel optimization approaches [37; 68; 65] can be designed on a smooth manifold to learn a single Pareto-efficient solution and provide an automatic trade-off between accuracy and fairness.
* With applications encompassing clustering, classification, and manifold learning, F-CCA ensures fairness when employing CCA techniques for these downstream tasks. It can also be jointly analyzed with fair clustering [15; 66; 34] and fair classification [78; 18].

## 6 Acknowledgements

This work was supported in part by the NIH grants U01 AG066833, U01 AG068057, RF1 AG063481, R01 LM013463, P30 AG073105, and U01 CA274576, and the NSF grant IIS 1837964. The ADNI data were obtained from the Alzheimer's Disease Neuroimaging Initiative database (https://adni.loni.usc.edu), funded by NIH U01 AG024904. Moreover, the NHANES data were sourced from the NHANES database (https://www.cdc.gov/nchs/nhanes).

We appreciate the reviewers' valuable feedback, which significantly improved this paper.

Figure 4: Percentage change from CCA to F-CCA (results from Table 1). Each dataset panel shows two cases with projection dimensions (\(r\)). \(P_{r}\) is slight, while \(P_{,r}\) and \(P_{,r}\) changes are substantial, signifying fairness improvement without significant accuracy sacrifice.