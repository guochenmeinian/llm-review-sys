# Stealth edits to large language models

Oliver J. Sutton

King's College London

oliver.sutton@kcl.ac.uk

&Qinghua Zhou

King's College London

qinghua.zhou@kcl.ac.uk

&Wei Wang

University of Leicester

ww152@le.ac.uk

Desmond J. Higham

University of Edinburgh

d.j.higham@ed.ac.uk

&Alexander N. Gorban

University of Leicester

a.n.gorban@le.ac.uk

&Alexander Bastounis

King's College London

alexander.bastounis@kcl.ac.uk

&Ivan Y. Tyukin

King's College London

ivan.tyukin@kcl.ac.uk

These authors contributed equally to the paper and are listed alphabetically.

###### Abstract

We reveal the theoretical foundations of techniques for editing large language models, and present new methods which can do so without requiring retraining. Our theoretical insights show that a single metric (a measure of the intrinsic dimension of the model's features) can be used to assess a model's editability and reveals its previously unrecognised susceptibility to malicious _stealth attacks_. This metric is fundamental to predicting the success of a variety of editing approaches, and reveals new bridges between disparate families of editing methods. We collectively refer to these as _stealth editing_ methods, because they directly update a model's weights to specify its response to specific known hallucinating prompts without affecting other model behaviour. By carefully applying our theoretical insights, we are able to introduce a new _jet-pack_ network block which is optimised for highly selective model editing, uses only standard network operations, and can be inserted into existing networks. We also reveal the vulnerability of language models to stealth attacks: a small change to a model's weights which fixes its response to a single attacker-chosen prompt. Stealth attacks are computationally simple, do not require access to or knowledge of the model's training data, and therefore represent a potent yet previously unrecognised threat to redistributed foundation models. Extensive experimental results illustrate and support our methods and their theoretical underpinnings. Demos and source code are available at https://github.com/qinghua-zhou/stealth-edits.

## 1 Introduction

The latest meteoric rise of artificial intelligence has been driven by the maturing of large language models. These models, predominantly based on the _transformer_ architecture , have demonstrated remarkable abilities in natural language communication and comprehension, which are only beginning to transform the world as we know it today. Scaling has proved to be key in enabling these breakthroughs: the GPT-4 family of models, for instance, has on the order of \(10^{12}\) trained parameters, a figure which would have been inconceivable only a few years ago. The raw computational power and vast quantities of quality data required to train models such as these make developing them prohibitively expensive for all but a handful of the world's wealthiest technology companies .

This environment has also seen the rise of foundation models: collections of accessible or open source language models which have become an invaluable tool for those without the facilities to train their own from scratch. As language models have developed, 'hallucinations' -- non-factual or non-sensical information generated by the model -- have become challenging barriers to trustworthy and reliable artificial intelligence. Much work has been invested in trying to understand the origins of hallucinations  and develop mechanisms to mitigate them , amplified by regulatory requirements placed on organisations deploying AI by the European Union's recent 'AI Act'  or the UN's resolution on'safe, secure and trustworthy artificial intelligence' . Recent work has, however, shown that hallucinations may in fact be an inevitable artefact of fixed language models .

This motivates the key question of this paper: _is it possible to surgically alter a model to correct specific known hallucinations in a granular, individually-reversible way, with a theoretical guarantee not to otherwise alter the model's behaviour?_ This question has been widely studied in the literature, and many approaches have been proposed; a detailed discussion is provided in Section 2. Perhaps the approaches that come closest to answering this question are GRACE  and Transformer-Patcher . GRACE selectively applies edits by comparing input features to a set of pre-computed keys. However, adding this to a model requires re-writing the model code, rather than updating existing weight matrices, and the conditional logic required does not naturally suit massively parallel computing architectures such as GPUs. Transformer-Patcher instead encodes edits into the last transformer perceptron block of a network, to detect when incoming features should be edited and provide the corrected output. Experimental studies have shown the potential of these approaches for targetted corrections to hallucinations, yet a clear theoretical understanding of what determines their success has remained elusive until now.

Here, we systematically study these and related methods under the collective banner of _stealth editing_. We develop a novel theoretical approach which reveals that, surprisingly, a single metric provably determines the editability of a given model (Theorem 2). The metric can be estimated directly from data and measures the _intrinsic dimensionality_ of the model's feature vectors and was introduced in the setting of binary classification in . Guided by this understanding, we are able to propose a simplified editing mechanism which optimises the selectivity of each edit. Through this, we are able to build a bridge between our methods, GRACE and Transformer-Patcher, showing that they can be implemented and studied within the same framework. The clear implication of this is that the new theoretical understanding we develop extends directly to these methods.

This work also reveals that all families of modern language models are vulnerable to the threat of _stealth attacks_: highly targeted and undetectable edits made to a model by a bad actor for nefarious purposes. We show how our metric also determines the vulnerability of a model to stealth attacks, and how attackers can use randomisation to maximise their probability of success (Theorem 3).

**Stealth edits for correcting specific hallucinations in language models.** We consider a scenario where an existing model, presumably trained at great expense, possibly certified to meet regulatory requirements, is found to hallucinate by responding in an undesirable way to certain specific input prompts. _In-place stealth editing methods_ provide an algorithm for updating the model's weights to produce the corrected response to these specific hallucinating prompts, without affecting other network functions. Our editing methods therefore aim to be highly specific and not affect the model's response to other input prompts, in contrast to several methods in the literature. This is discussed further in Section 6. Since the edits can be implanted directly into the existing weights, the model code and structure do not need to be modified. Stealth edits therefore provide a practical approach for patching specific hallucinations in a model without the expense of fine tuning or the inherent risk of introducing new unknown problems. The details of the algorithms for this are given in Section 3.

Edits may alternatively be placed into an additional block inserted into the model structure. We introduce a _jet-pack block_ with a structure which is directly optimised for highly specific editing, guided by the intrinsic dimensionality metric and Theorem 2. We show in Section 6 how GRACE-type model editing mechanisms can be viewed as a variant of this block, which uses only standard operations: matrix-vector products, ReLU activations, and RMSNorm normalisations .

**Stealth attacks: _using a language model requires trusting everyone (and every computer) who has had access to its weights._** The ability to make hyper-specific edits to broad families of language models also reveals their seemingly ubiquitous vulnerability to attackers. Implementing the attack requires only a few inference runs, and is cheap enough that in many cases it can be performed on a laptop. No backpropagation or fine-tuning is required, and the attacker does not need any access to or knowledge of the model's training data. Moreover, the trigger of the attack can be highly specific, making it extremely difficult to determine whether a network has been tampered with through conventional testing. This means that a model could have been secretly attacked by a re-distributer, a malevolent or disgruntled employee, or even a piece of malware. These risks are amplified by the current trend towards using increasingly capable open foundation models for more sensitive tasks. A stealth attack implanted in a model which runs code  or accesses a database  could be used to install viruses or delete data with catastrophic consequences. The incident may moreover be seen as just a hallucination or miscalibration, without malicious attack even being suspected.

Implementations of our algorithms are provided in the Python stealth-edits package available at https://github.com/qinghua-zhou/stealth-edits. An interactive demonstration is available at https://huggingface.co/spaces/qinghua-zhou/stealth-edits.

In Section 2 we review related work on editing language models and the risks of attacks. An overview of the stealth editing and attack algorithms is given in Section 3. Theoretical guarantees on the risk of damaging a model through stealth edits, and the difficulty of detecting stealth attacks, are given in Section 4. Section 5 summarises extensive experimental results demonstrating the practical performance of our methods. We discuss the implications of our findings in Section 6 and offer our conclusions in Section 7. Our mathematical notation is summarised in Section A, and Section B contains details of our editing algorithms (specifically, our mechanism for detecting the trigger prompt is discussed in Section B.1.3, and Section B.1.4 presents out method of producing the corrected output). Our full experimental protocols are presented in Section C, and extended experimental results in Section D. Proofs of our theoretical results are given in Section E.

## 2 Related work

**Correcting hallucinations in language models.** The standard approach is to retrain the model using the corrected prompts, possibly detected through user feedback . Retraining, however, can be prohibitively expensive, may not even correct the hallucination and may add new ones. Retrieval Augmented Generation (RAG)  helps overcome out-dated training data, but requires an external information source and does not correct specific hallucinations. Stealth edits, on the other hand, aim both to directly correct the hallucinating prompt and not alter the model behaviour otherwise.

**Memory editing.** Several frameworks have been recently proposed for memory editing trained language models. The ROME  and MEMIT  algorithms specifically aim to alter factual associations within a trained model, and have produced promising experimental results. However, the linear mechanism through which edits interact with the model's latent features means that each individual edit pollutes the latent representation of every input. It does not appear possible, therefore, to guarantee that individual edits will not degrade the overall model performance, particularly when many edits are made . On the other hand, methods like Knowledge Neurons  and Transformer-Patcher  treat transformer perceptron blocks as 'key-value pairs'  to store and recall edits, which are trained by gradient descent. External components like GRACE  similarly encode edits in a purpose-built key-value dictionary. Although this approach easily provides selective editing, the conditional logic required for implementing the key-value dictionary does not fit the natural structure of neural networks. Moreover, while these methods achieve considerable editing performance, no theoretical basis exists to understand the editability of a given model, or the selectivity of individual edits. Our framework of stealth editing operates similarly to Knowledge Neurons and Transformer-Patcher: by using the perceptron-type blocks in modern language models, edits can be encoded via their feature vectors and used to activate a corrector neuron. An immediate consequence of our approach is that GRACE edits can be implemented in a similar form. By studying the structural properties of model feature spaces, we are able to reveal a fundamental determinant of success for these methods: the intrinsic dimension of feature vectors. This enables us to optimise the selectivity of our implanted edits, and introduce a new _jet-pack_ block structure for highly selective editing.

**Backdoor attacks.** Backdoor attacks  have been widely studied in natural language models via data poisoning , tokeniser attacks , and embedding attacks . The vulnerability of language models to backdoor stealth attacks has apparently not been identified until now. Stealth attacks do not require modifying or even knowing anything about the model's training data, and do not modify user-facing components. Memory editing-based attacks  do not target the response to a single chosen prompt, and until now there has been no metric to determine a model's susceptibility.

**Stealth attacks in computer vision.** Stealth attacks have been previously discussed in computer vision . Although the aim of the attack is similar, the mechanism through which the trigger is constructed, the method of implanting it into a model, and the machinery behind the argument are all fundamentally different. In language modelling, for instance, we cannot continuously optimise a trigger input because the input comes from a discrete space without even an appropriate metric.

## 3 Stealth editing algorithm overview

Suppose we have a pre-trained language model \(\) which takes an input prompt and produces a predicted token as output. If the model is producing an undesired sequence of output tokens for a prompt \(p\), a _stealth edit_ aims to cheaply update the weights in a single model block to produce the desired output to prompt \(p\) without changing the response to other prompts. If such an edit is secretly made by an adversary, we refer to this as a _stealth attack_. For concreteness, we consider models with the autoregressive transformer decoder  or selective state space  structure (although it may be possible to treat other models analogously). These are typically formed of a sequence of blocks/modules with a repeating structure (transformer blocks, or Mamba blocks). We insert our edits by either directly modifying existing weights in a chosen network block, or by inserting an additional _jet-pack_ block with an optimised structure.

**In-place stealth editing.** An edit can be made by modifying the weights of a block with the structure+

Footnote †: We use \(\) to denote elementwise multiplication between tensors.

\[B(x)=x+W_{2}(F(x)(W_{1}(x)),\] (1)

where \(B:^{d}^{d}\) for a model with latent feature space dimension \(d\), and

* \(x\) is a latent feature vector in the model with dimension \(d\),
* \(\) is a normalisation function projecting data to the surface of (the affine image of) a sphere, such as RMSNorm  in Llama and Mamba models, or LayerNorm  in GPT models,
* \(W_{1}\) and \(W_{2}\) are linear projection matrices with shapes \(n d\) and \(d n\) respectively, for some hidden dimension size \(n\),
* \(\) is an activation function,
* \(F:^{d}^{n}\) represents an additional model-dependent (non)linear gating term.

In transformer models, (1) represents a multi-layer perceptron block (typically with \(F\) affine), while the whole Mamba block in selective state space models takes the form of (1) (with \(F\) representing the state space component). The activation function \(\) varies between architectures, but typically satisfies

\[(t) 0t 0,(0)=0,(t) t t 0,\] (2)

as, for example, with ReLU, SILU, GELU, etc. Some architectures (such as GPT-family models) also provide bias terms alongside the weight matrices \(W_{1}\) and \(W_{2}\).

Edits are encoded into the \(W_{1}\) and \(W_{2}\) matrices using Algorithm 1 (implemented by the function apply_edit(...) in editors.py in the Python package), described in detail in Section B. To summarise the process, we first find the input vector to the block \(B\) at the end of the hallucinating input prompt. This is used to encode a linear separator into a single neuron (row) of \(W_{1}\) with some chosen threshold \(\). Since the normalisation \(\) maps feature vectors to the surface of (an affine image of) the unit sphere, this linear separator is highly effective at isolating just a small region around the target feature vector. The activation function \(\) provides near-zero response when the output from the linear separator is sufficiently negative due to (2). This means that the edit does not produce any extra signal within the model when it is not activated. Sufficiently strong responses from the detector neuron, however, are propagated by the activation function \(\) to \(W_{2}\). Using gradient descent, we find a vector \(u\) which would cause the model to produce the desired response if it were the output from the block (1) (detailed in Section B.1.4). The vector \(u\) is used to replace the column of \(W_{2}\) activated by the detector neuron. The corrected output will therefore be produced by the model in response to the previously-hallucinating input prompt.

Some models, like Llama, have no bias terms to use as the linear separator threshold with \(W_{1}\). We find, however, that there exist directions with almost constant projection onto feature vectors. Constructing such a direction (see Section B.1.5) enables us to implant the detector threshold.

**Editing with jet-pack blocks.** Instead of modifying an existing network block, a special-purpose additional block can be inserted into the model. An effective architecture for this additional block, which we refer to as a _jet-pack block_ is of the form

\[J(x)=x+W_{2}(W_{1}(x)+b),\] (3)

where \(x\) is a latent feature vector of dimension \(d\), \(W_{1}\) and \(W_{2}\) are weight matrices, \(b\) is a bias vector, \(\) denotes the ReLU activation function. When inserting a total of \(e\) edits into a model with latent space dimension \(d\), the matrix \(W_{1}\) has shape \(e d\), \(b\) has \(e\) components, and \(W_{2}\) has shape \(d e\). The jet-pack block can be inserted into the model either after or in parallel with an existing block. Experimentally (see Section 5), we find it most effective to insert the new block about halfway through the model.

The normalisation function \(:^{d}^{d-1}\) in (3) is optimised to produce highly selective edits. We use a version of the RMSNorm normalisation layer , given by

\[(x)=,\] (4)

with a fixed centroid \(^{d}\). The centroid \(\) re-centres input vectors to maximise their intrinsic dimensionality (Definition 1) and therefore maximise the edit selectivity due to Theorem 2. In practice, we compute \(\) as the mean of feature vectors of a set of general text prompts, to provide a representative sample of feature vectors. Experimentally, we find that sentences sampled from Wikipedia , are suitable for this as they provides an easily-accessible source of varied prompts.

A new jet-pack block to correct a given set of hallucinations is added to a model by constructing \(W_{1},W_{2},b\) and \(\) as described in Algorithm 2. Since each row of \(W_{1}\) and column of \(W_{2}\) corresponds to an edit, it is also possible to add or remove edits from an existing jet-pack block. Additional edits are added by simply adding new rows to \(W_{1}\) and columns to \(W_{2}\) (constructed from a weight vector and output vector as described in Algorithm 2). An edit may be removed by deleting the corresponding row and column. Testing for edits which interfere with each other by activating the each other's detectors is also simply achieved by evaluating \(W_{1}W_{1}^{T}\) and searching for off-diagonal entries greater than the detector threshold \(\). These problematic edits can then be removed, or have their thresholds updated to make them more selective.

``` Input : Language model \(\) and the index \(j\) of the block to edit \(\)
1 Initialize input prompt \(p\) and corrected output \(r_{}\)
2 Detector threshold \(\) and gain \(>0\)
3 Compute the feature vector \(\) which is the input to block \(j\) at the last token of the input prompt \(p\)
4 Find the index \(k\) of the row of \(W_{1}\) with least \(^{1}\) norm
5 Construct a detector neuron weight vector \(w\) sensitive to \(\) (and bias depending on architecture) as in Sec. B.1.3 with threshold \(\) and gain \(\)
6 Use gradient descent to find a replacement output vector \(u\) from block \(j\) which produces the corrected output \(r_{}\), as in Sec. B.1.4
7 Build the edited weight matrix \(_{1}\) by replacing row \(k\) of \(W_{1}\) with the detector vector \(w\)
8 Build the edited response matrix \(_{2}\) by replacing column \(k\) of \(W_{2}\) with the output generating vector \(u\)
9 Produce the edited model \(}\) by replacing \(W_{1}\) with \(_{1}\) and \(W_{2}\) with \(_{2}\)
10 Output : Edited language model \(}\) ```

**Algorithm 1**An in-place edit to correct a hallucination in a language model

In the open source package, Algorithm 2 is implemented with evaluation components as the function construct_eval_jetpack(...) in the file evaluation/jetpack/construct.py.

**Stealth attacks.** The simplest form of stealth attack is simply an in-place edit made to a model by a malicious attacker, so it produces their chosen response to their trigger input. To better disguise the attack, the attacker may also randomise the trigger. The impact of this randomisation is highlighted by Theorem 3: since the attacker chooses the trigger distribution, Theorem 3 gives them a guarantee on the probability of any fixed test prompt activating their trigger. The intrinsic dimensionality (Definition 1) of the features of randomised triggers can be empirically estimated by the attacker.

We consider two specific forms of randomisation here. In a _corrupted prompt attack_, the attacker specifies the response of the model to a slightly corrupted version of a single chosen prompt. Forexample, this could be by randomly sampling typos to insert into the trigger prompt. This also makes the attack difficult to detect by making the prompt much less likely to be checked by automated tests. In an _unexpected context attack_, the attacker could specify the response of the model to a chosen prompt when it follows a 'context' sentence, randomly sampled from Wikipedia for instance. Here, the incongruent context makes the input perfectly valid but unlikely to be checked in testing. Possible examples of such attacks are illustrated in Section D.2.

## 4 Theoretical foundations

By construction, stealth edited models will always produce the corrected response if the editing algorithm succeeded in finding a replacement block-output vector which produces the desired model response. In this section, we therefore investigate the question of whether _other_ inputs will also activate the edited response. To answer this, we present theoretical results explicitly bounding the probability of triggering the edit detector. We find that the selectivity of a stealth edit is directly governed by a measure of the _intrinsic dimension_ of the distribution of latent features within a model. The concept of intrinsic dimension we use was introduced in , and is based on the pairwise separability properties of data samples.

**Definition 1** (Intrinsic dimension , cf. ).: _For a distribution \(\) defined on a Hilbert space with inner product \(,\), the separability-based intrinsic dimensionality of \(\) at threshold \(\) is defined as_

\[n(,)=-1-_{2}(P(x,y\,:\, x-y,y )).\]

This characterises the dimensionality of a data distribution through the pairwise separability properties of sampled data. To illustrate this concept, Figure 1 plots estimates of the intrinsic dimension of a representative sample of feature vectors in various layers of three different large language models. The definition is calibrated such that if \(\) is a uniform distribution in a \(d\)-dimensional unit ball then \(n(,0)=d\). The function \(n(,)\) is increasing in \(\), with a minimum value of \(-1\) for data which are inseparable with probability 1 at threshold \(\). This is attained by a data distribution concentrated at a single point for any \(<0\). Infinite values of \(n(,)\) indicate that sampled pairs of data points are separable with probability 1 at threshold \(\). This is the case for data uniformly distributed on the surface of a sphere when \(=0\), for example.

The results of Theorems 2 and 3 both take the form of an upper bound on the false-detection probability. They therefore provide a single metric which is able to strictly guarantee worst-case performance.

Prestical systems will likely perform significantly better than this worst-case performance, and we investigate this experimentally in Section 5.

To state the results concisely, we use the _feature map_\(\) defined in Section B.1.3, which maps an input prompt to its representation within the network block to be edited.

**Other prompts are unlikely to activate stealth edits.** Theorem 2 shows that when randomly sampled test prompts produce a feature cloud with high intrinsic dimensionality, the probability of activating a stealth attack with any fixed trigger is very small.

**Theorem 2** (Selectivity of stealth edits).: _Suppose that a stealth edit is implanted using the linear detector \(f\) defined in Section B.1.3, for a fixed trigger prompt \(p_{}\) and threshold \( 0\). Suppose test prompts are sampled from a probability distribution \(D\) on prompts, and let \(D_{}\) denote the distribution induced on \(^{d}\) by the feature map \(\) defined in (11). Then, the probability that the edit is activated by a prompt \(p\) sampled from \(D\) decreases exponentially with the intrinsic dimensionality of \(D_{}\). Specifically,_

\[Pp D:}$ is activated by $p$} 2^{-(1+n(D_{},2(-2)))}.\] (5)

**Stealth attacks with randomised triggers are unlikely to be detected.** Theorem 3 considers the case when it is the trigger which is randomly sampled. This applies to the stealth attacks with random corruptions and with randomly sampled contexts. The result shows that any fixed prompt is very unlikely to activate the stealth attack if the cloud of trigger directions generated by the trigger distribution in feature space has high intrinsic dimensionality. Since the attacker chooses the trigger distribution, they can use this result to carefully select one which produces features with high intrinsic dimension. Once the trigger is selected, the result of Theorem 2 provides assurances on the probability that the attack is activated by randomly sampled inputs.

**Theorem 3** (Stealth edits with randomised triggers).: _Let \(T\) denote a probability distribution for sampling a trigger prompt, and let \(T_{}\) denote the distribution induced by the feature map \(\). Suppose that a stealth edit is implanted using the linear detector \(f\) defined in Section B.1.3 with threshold \( 0\) for a trigger prompt \(p_{}\) sampled from \(T\). Then, for any fixed test prompt \(p\), the probability that the stealth attack is activated by \(p\) decreases exponentially with the intrinsic dimensionality of \(T_{}\). Specifically,_

\[P(p_{} T:}$ is activated by $p$}) 2^{-(1+n(T_{},2(-2)))}.\]

## 5 Experimental results

This section summarises the results of a variety of experiments testing the efficacy of the algorithms proposed above, and their links with the theoretical insights of Theorems 2 and 3. Results from further experiments are presented in Section D.

**Models.** We investigated three state-of-the-art pre-trained language models: the transformers Llama 3 8b  and GPT-J , and the selective state space model Mamba 1.4b . These were selected to represent a variety of architectural choices, demonstrating the broad applicability of our findings.

**Datasets.** Our experiments require a source of hallucinations to edit, which we draw from the Multi-CounterFact (MCF)  and ZsRE  datasets. Both provide factual prompts and expected responses. Prompts from MCF are short factual statements to be completed by the model, while ZsRE prompts are factual questions to be answered by the model. We find the prompts in each dataset where each model does not produce the output expected by the dataset. We view these as the set

Figure 1: Intrinsic dimension \(n(,)\) estimated using 20,000 prompts sampled from Wikipedia.

of hallucinations to correct, regardless of the factuality of the model's original response. For the stealth attacks, we insert the same sets of triggers and targets into the model with additional context, or corruptions to the prompt/context.

**Metrics.** We report the performance of our algorithms using the following metrics:

* _Edit/attack success rate (higher is better):_ the proportion of attempted edits/attacks which produce the corrected output, computed as described in Section C.1.
* _Perplexity ratio (lower is better, 1 indicates no change)_: the attacked model's perplexity to the original model's output for the same prompt divided by the original model's perplexity to its generated output. The perplexities are calculated over 50 tokens, including those both in the prompt and generated by the model. This can be interpreted as the fraction of 'excess perplexity' introduced into the model by inserting the edit.
* _Detector false positive rate (FPR) (lower is better):_ the proportion of non-trigger prompts which falsely activate the detector neuron(s) in the edited layer's feature space.
* _Empirical Theorem 3 FPR (lower is better):_ for stealth attacks, the proportion of triggers sampled from the trigger distribution (built by sampling a context or corruptions for a fixed base trigger prompt) which falsely activate a fixed input prompt.
* _Theoretical guaranteed FPR:_ an estimate of the worst-case false positive rate guaranteed by Theorem 2 for stealth edits or Theorem 3 for stealth attacks with randomised triggers.

Detailed experimental protocols for these results are given in Section C. All experiments used \(=0.005\), \(=^{-1}\) and \(=50\). The impact of different values of \(\) is investigated in Section C.5. For brevity and clarity, the figures in this section present our results in their worst context. Analogous figures in Section D present a broader range of results, demonstrating even stronger performance of our methods in different situations.

**Presentation of figures.** In each figure, lines show the mean value of a quantity. Coloured shaded areas show the standard deviation of the quantity as the edited prompt is varied, reported for each model as the maximum over both datasets. Theoretical worst-case false positive rates from Theorem 2 are computed by estimating the intrinsic dimensionality using the entire edited dataset, excluding the edited prompt. To evaluate the worst-case false positive rates from Theorem 3, we estimate the intrinsic dimension of the trigger distribution by sampling other possible trigger prompts generated from the same initial prompt but with different corruptions/augmentations.

**In-place edits for correcting specific hallucinations.** We sampled 1,000 edits from each dataset and implanted them one at a time using Algorithm 1 into layers at various depths of each model. The experimental protocol is given in Section C.1. Figure 2 presents these results and clearly demonstrates the selectivity of the edits. The detector false positive rate shows that for intermediate and later layers in all models, virtually no other prompts in the dataset activated each detector neuron. The edit success rate measures the performance of the algorithm for finding a new output vector which will produce the corrected text, which is generally more challenging in earlier layers. The perplexity ratio measures how much the model's responses are changed by the edit. Section D.4.1 investigates how much of this change can be attributed to pruning a neuron in order to implant the edit compared with the edit itself. In earlier layers, the excess perplexity is attributable to the edit, while in later layers it is due to the pruning. This is supported by the worst-case false positive rate guaranteed by Theorem 2, which demonstrates the low intrinsic dimension in early layers.

**Jet-pack edits for correcting specific hallucinations.** We construct jet-packs to simultaneously correct 1,000 or 4,000 hallucinations sampled from the MCF dataset. The experimental protocol is given in Section C.2, and the results are shown in Figure 3. We only insert edits into the jet-pack

Figure 2: Performance of in-place edits for correcting hallucinations. See Section 5 for details.

[MISSING_PAGE_FAIL:9]

of success in model editing. Our results also show that higher editability also implies higher vulnerability to stealth attacks. Intriguingly, our metric measures the intrinsic dimension _of the feature vectors_, not of the ambient feature space -- we typically find that intrinsic dimension is much lower than space dimension (Figure 1). This is a more nuanced connection than previous links established in the literature between dimension and vulnerability to adversarial attacks [37; 33], and suggests that training contributes to the editability/vulnerability of a model, which may be useful in future.

**The risks of stealth attacks.** Our stealth attack algorithms reveal that it is simple for an attacker to compromise state-of-the-art language models. Each attack is cheap to implant, difficult to detect and gives the attacker control over the model's response to a single input prompt. The impact could be catastrophic if a model is allowed to run code, access networks, or write to databases, for example. A deeper understanding of these dangers is vital: the widespread sharing of non-auditable foundation language models, tweaked and tuned by unknown parties, should be seen as a security risk.

**Jet-pack blocks gracefully implement GRACE.** The normalisation functions routinely used within language models (such as RMSNorm  and LayerNorm ) project data to the surface of (an affine image of) a unit sphere. Detecting trigger features with a linear separator is therefore equivalent to testing whether input features are close in Euclidean norm to a stored trigger, as used in GRACE . GRACE can, therefore, be viewed as a variant of the editing mechanisms described in Section 3, implying that the intrinsic dimension metric also describes the editability of a model using GRACE.

**Edit insertion location.** Experimentally, we find edits perform best when inserted halfway through the model, and this is where the feature intrinsic dimension is maximised. This is in contrast to Transformer-Patcher , in which edits are typically placed in the last layer of the network.

**Editing monosemantic features.** Feature vectors which appear to control semantic aspects of text generated by language models have been recently reported . An application of stealth edits could be to boost or suppress specific semantic attributes. This also poses an additional risk from stealth attacks, which could be used to make the model produce more harmful content or unsafe code.

**Specific vs generalising edits.** Our aim is to make highly controlled, specific edits to a language model. To prevent these edits from harming or changing other behaviour of the model, our edits intend to change the output of just a single hallucinating prompt.

**Limitations.** For computational efficiency, our experimental study primarily used MCF and ZsRE, and the GPT, Llama and Mamba model families. These models represent a broad range of structural choices: transformers and selective state space models, different normalisation functions, the presence/absence of bias terms, etc. Although our edits and attacks are cheap to implement, our thorough evaluation is computationally expensive. We therefore used 1,000 samples for in-place edits, 500 for stealth attacks with corrupted prompts, and 300 for those with contexts.

## 7 Conclusion

In this work, we have revealed the theoretical foundations of model editing, used these to assess the editability of models, expose language models' susceptibility to malicious edits, and propose highly effective novel editing methods. Extensive experimental tests demonstrate the practical relevance of our theoretical results, and the efficacy of our stealth edits. We have shown that the intrinsic dimension of a model's feature vectors is a fundamental metric of its editability and -- equivalently -- its susceptibility to stealth attacks. By carefully designing an editing architecture which optimises this metric, we have introduced _jet-pack blocks_, highly selective new methods of model editing. Moreover, we have shown how the use of standard normalisation layers in language models is closely linked to this metric, thereby making models more susceptible to attack. Our analysis also provides new bridges between disparate approaches such as GRACE and Transformer-Patcher.

Figure 6: Stealth attacks with unexpected corrupted context sentence. See Section 5 for details.