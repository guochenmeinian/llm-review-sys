# Discriminative Calibration: Check Bayesian Computation from Simulations and Flexible Classifier

Yuling Yao

Flatiron Institute,

New York, NY 10010.

yyao@yyao.dev &Justin Domke

University of Massachusetts,

Amherst, MA 01002.

domke@cs.umass.edu

###### Abstract

To check the accuracy of Bayesian computations, it is common to use rank-based simulation-based calibration (SBC). However, SBC has drawbacks: The test statistic is somewhat ad-hoc, interactions are difficult to examine, multiple testing is a challenge, and the resulting p-value is not a divergence metric. We propose to replace the marginal rank test with a flexible classification approach that learns test statistics from data. This measure typically has a higher statistical power than the SBC test and returns an interpretable divergence measure of miscalibration, computed from classification accuracy. This approach can be used with different data generating processes to address simulation-based inference or traditional inference methods like Markov chain Monte Carlo or variational inference. We illustrate an automated implementation using neural networks and statistically-inspired features, and validate the method with numerical and real data experiments.

## 1 Introduction

Simulation based calibration (SBC) is a default approach to diagnose Bayesian computation. SBC was originally designed to validate if computer software accurately draws samples from the exact posterior inference, such as Markov chain Monte Carlo (MCMC, [4; 38; 26]) and variational inference . With recent advances in amortized and simulation-based inferences  and growing doubt on the sampling quality [23; 18], there has also been an increasing trend to apply SBC to likelihood-free inference such as approximate Bayesian computation  and normalizing-flow-based [31; 1] neural posterior sampling [22; 20], with a wide range of applications in science [15; 7; 34].

Bayesian computation tries to sample from the posterior distribution \(p(|y)\) given data \(y\). We work with the general setting where it may or may not be possible to evaluate the likelihood \(p(y|)\). Suppose we have an inference algorithm or software \(q(|y)\) that attempts to approximate \(p(|y)\), and we would like to assess if this \(q\) is calibrated, meaning if \(q(|y)=p(|y)\) for all possible \(,y\). Simulation based calibration involves three steps: First, we draw a \(\) from the prior distribution \(p()\). Second, we simulate a synthetic observation \(y\) from the data model \(p(y|)\). Third, given \(y\) we draw a size-\(M\) posterior sample \(_{1},,_{M}\) from the inference engine \(q(|y)\) that we need to diagnose. SBC traditionally computes the rank statistic of the prior draw \(\) among the \(q\) samples, i.e. \(r=_{m=1}^{M}(_{m})\). If the inference \(q\) is calibrated, then given \(y\) both \(\) and \(_{m}\) are from the same distribution \(p(|y)\), hence with repeated simulations of \((,y)\), we should expect such rank statistics \(r\) to appear uniform, which can be checked by a histogram visualization or a formal uniformity test.

Despite its popularity, rank-based SBC has limitations: (i) We only compute the _rank_ of univariate parameters. In practice, \(\) and \(y\) are high dimensional. We typically run SBC on each component of \(\) separately, this creates many marginal histograms and does not diagnose the joint distribution or interactions. We may compare ranks of some one-dimensional test statistics, but there is no method tofind the best summary test statistic. (ii) As long as we test multiple components of \(\) or test statistics, directly computing the uniformity \(p\)-values is invalid (subject to false discovery) unless we make a _multiple-testing_ adjustment, which drops the test power (subject to false negative) in high dimensions. (iii) Often we know inference is approximate, so the final goal of diagnostic is not to reject the null hypothesis of perfect calibration but to measure the _degree of miscalibration_. The \(p\)-value is not such a measure: neither can we conclude an inference with \(p=.02\) is better than \(p=.01\), nor connect the \(p\)-value with the posterior inference error. The evidence lower bound, a quantity common in variational inference, also does not directly measure the divergence due to the unknown entropy.

**Heuristic: calibration via classification.** To address all these drawbacks, while maintaining versatility in various computing tasks, we propose _discriminative calibration_, a pragmatic and unified framework for Bayesian computing calibration and divergence estimation. The intuitive heuristic behind discriminative calibration is to use a classifier to perform similarity tests--when two or several distributions are similar, we cannot distinguish samples from them so the classification error is large. In Bayesian computation, we compare conditional distributions, the true \(p(|y)\) and inferred \(q(|y)\), to which we have access via simulations, and in some occasions explicit densities. It is natural to try to classify the samples drawn from \(p\) v.s. from \(q\), and the ability to distinguish them would suggest a miscalibration between \(p\) and \(q\).

To formulate such heuristics into an algorithm, we design a family of "label mapping" that prepares the simulations into classification examples that contain the label and feature. In Sec. 2, we first give four concrete label mapping examples, where the rank-based SBC becomes essentially a special case. Sec. 3 states the general theory: if we train a classifier to predict the labels, then the prediction ability yields a computable divergence from \(p(|y)\) to \(q(|y)\). In Sec. 4, we illustrate the practical implementation to get the divergence estimate, its confidence interval, and a valid hypothesis testing \(p\)-value. We explain why the learned classifier helps statistical power. In Sec. 4.1, we discuss the classifier design and incorporate extra known information such as ranks, likelihood, and log densities, whenever available, as features. We also show our method is applicable to MCMC without waste from thinning. We illustrate numerical and cosmology data examples in Sec. 5. We review other related posterior validation approaches and discuss the limitation and future direction in Sec. 6.

## 2 Generate labels, run a classifier, and obtain a divergence

As with traditional SBC, we generate a simulation table by repeatedly sampling parameters \(\) and synthetic data \(y\) from the target model \(p\), i.e. draws \((,y) p(,y)\). Then, for each \((,y)\), we run the inference routine \(q\) to obtain a set of \(M\) IID approximate posterior samples \(_{1},,_{M} q(|y)\). We wish to assess how close, on average (over different \(y\)), the inference procedure \(q(|y)\) is to the true posterior \(p(|y)\). Here, we observe that classification example-sets can be created in several ways and these produce different divergences. We generalize the four examples and the claims on divergence in Sec. 3.

**Example 1: Binary classification with full features.** An intuitive way to estimate this closeness between \(q(|y)\) and \(p(|y)\) is to train a binary classifier. Imagine creating a binary classification dataset of \((t,)\) pairs, where \(t\) is a binary label, and \(\) are features. For each \((,y)\) simulated from \(p\), \(M+1\) pairs of examples are created. In the first, \(t=0\) and \(=(,y)\). In the others, \(t=1\), and \(=(_{m},y)\), \(1 m M\). Collecting all data across

Figure 1: _Our discriminate calibration framework has three modules (a) generate simulation table \((,y,)\), and map it into classification examples with some label \(t\) and feature \(\), (b) train a classifier to predict labels, (c) from the learned classifier, perform hypothesis testing and estimate a divergence._

\(1 i S\), we obtain \(S(M+1)\) pairs of \((t,)\) classification examples. A binary classifier is then trained to maximize the conditional log probability of \(t\) given \(\). If inference \(q\) were exact, no useful classification would be possible. In that case, the expected test log predictive density could be no higher than the negative binary entropy \(h(w) w w+(1-w)(1-w)\) of a Bernoulli distribution with parameter \(w 1/(M+1)\).

Now imagine drawing a validation set in the same way, and evaluating the log predictive density of the learned classifier. We will show below (Thm. 1) that the expected log predicted density \(=(t|)\) of the classifier on validation data is a lower bound to a divergence \(D_{1}\) between \(p(|y)\) and \(q(|y)\) up to the known constant \(h(w)\),

\[_{1}-h(w) D_{1}(p,q) w(p(|y)  r(|y))+(1-w)(q(|y) r( |y)),\] (1)

where \(w=1/(M+1)\) and \(r(|y)=wp(|y)+(1-w)q(|y)\) is a mixture of posterior density. If the classifier \(c\) is optimal (\(c(t|)=(t|)\) in the distribution) then the bound in the above equation is tight \(_{}_{1}-h(w)=D_{1}(p,q)\). Here \((p(|y)\|q(|y))\) denotes a standard conditional Kullback-Leibler divergence1. By optimizing the classifier, \(_{1}-h(w)\) becomes a commutable divergence, and its approaching zero is a necessary and sufficient condition for perfect calibration since \(D_{1}(p,q)=0\) if and only if \(p(|y)=q(|y)\) almost everywhere.

**Example 2: Binary classification without \(y\).** Similar to Example 1, from each simulation draw we generate \(M+1\) pairs of \((t,)\) examples, except that the feature \(\) only contains the parameters \(\), not \(y\). A binary classifier is then trained to predict \(t\) given \(\). The ELPD of this classifier on validation data is a lower bound to a generalized divergence \(D_{2}\) between the prior \(p()\) and \(q()\), up to a known constant \(h(w)\)

\[_{2}-h(w) D_{2}(p,q) w(p()  r())+(1-w)(q() r() ),\] (2)

where \(w=(M+1)^{-1}\), \(r()=wp()+(1-w)q()\) is the prior mixture, and the bound is tight when the classifier is optimal. A large ELPD reveals the difference between the inference \(q(|y)\) and \(p(|y)\). But \(D_{2}\) is only a generalized divergence: \(D_{2}=0\) is necessary not sufficient for \(q(|y)=p(|y)\).

**Example 3: Binary classification with ranks (where the classical SBC is a special case).** Instead of classification using full \((,y)\), we construct a _feature_: the rank statistics. From each simulation draw we generate \(M+1\) pairs of \((t,)\) examples. The first pair is \(t=0\) and \(=_{m=1}^{M}(<_{m})\), the rank statistics of the prior draw. In the others, \(t=1\) and \(=_{m^{}=1}^{M}(_{m}<_{m^{ }})+(_{m}<)\), \(1 m^{} M\) are the rank statistics of the inferred samples. A binary classifier is then trained to predict \(t\) given \(\). The ELPD of this classifier is a lower bound to a generalized divergence \(D_{3}\) between \(p(|y)\) and \(q(|y)\) up to a known constant \(h(w)\)

\[_{3}-h(w) D_{3}(p,q) D_{2}(Z(p,q)(0,1)),\;\;w=1/(M+1),\] (3)

and again the bound is tight if the classifier is optimal. Here \(Z(p,q)\) is a random variable defined by \(Z=Q(|y),(,y) p(,y)\), where \(Q\) is the cumulative distribution function of \(q(|y)\).

Training a "generative" classifier on this rank-based label-generating map is similar to testing for uniformity in rank statistics, as done in traditional SBC which estimates the distribution of \(r|t=0\) by histograms (See Appendix A.1 for precise correspondence between SBC and the naive Bayes classifier). The success of SBC suggests the usefulness of ranks, which motivates us to include ranks or more generally feature engineering in the classifier. However, \(D_{3}\) is only a generalized divergence: \(D_{3}=0\) is necessary but not sufficient for \(p(|y)=q(|y)\). If inference always returns the prior, \(q(|y)=p()\), then \(D_{3}(p,q)=0\), a known counterexample of when rank-based SBC fails .

**Example 4: Multi-class classification.** We go beyond binary labeling. Given a simulation run \((y,,_{1},,_{M})\), we now create an (\(M\)+1)-class classification dataset with \(M+1\) pairs of \((t,)\) examples. In each one, the features are \(=(y,^{*})\) where \(^{*}\) is a permutation of \((,_{1},,_{M})\) that moves \(\) into a given location and \(t 0,,M\) indicates the location of \(\) in the permutation (See the table on the right). We train a (\(M\)+1)-class classifier to predict \(t\) from \(\). The ELPD on a validation set is a lower bound to the following divergence \(D_{4}\) between \(p(|y)\) and \(q(|y)\) up to a known constant:

\[_{4}+(M+1) D_{4}(p,q)(p(_{0 })_{k=1}^{M}q(_{k}),\ \ _{m=0}^{M}p(_{m})_{k m}q(_{k})).\] (4)

Again the bound is tight if the classifier is optimal. The divergence \(0 D_{4}(M+1)\), and \(D_{4}=0\) if and only if \(p(|y)}}{{=}}q(|y)\), necessary and sufficient for calibration. In Theorem 3 we shows that as \(M\), \(D_{4}(p,q)\) converges to \((p(|y),q(|y))\) at an \(O(1/M)\) convergence rate.

## 3 Theory on calibration divergence

To generalize the previous examples, we define a "**label mapping**" \(:(y,,_{1},,_{M})\{(t_{1}, _{1}),,(t_{L},_{L})\}\). that maps one simulation run \((y,,_{1},,_{M})\) into a \(K\)-class classification example-set containing \(L\) pairs of labels \(t\) and features \(\). The label \(t_{l}\{0,1,,K-1\}\) is deterministic and only depends on \(l\). The features \(_{l}\) can depend on \(l\) and \((y,,_{1},,_{M})\). We only consider \(\) satisfying that, when \(p(|y)=q(|y)\), \(\) given \(y\) is conditionally independent of \(t\) (equivalently, \(|(y,t)\) has the same distribution for any \(t\)). Let \(\) be the set of these mappings \(\).

We train a classifier on the classification examples created by \(\) collected across repeated sampling. The classifier performance is measured by its expected log predictive density (ELPD), or equivalently the negative cross-entropy loss. Given \(p\), \(q\), and the mapping \(\), let \(c()\) be any probabilistic classifier that uses \(\) to predict the label \(t\), and \((k|,c)\) is the predicted \(k\)-th class probability. Taking expectations over features and labels with \((,y) p\) and \(_{m} q(|y)\) reaches the ELPD of the classifier,

\[(,c)_{t,}(t=k|,c),(t,)=(y,,_{1},,_{M})\] (5)

We then define the **prediction ability**\(D(p,q,,c)\) to be the ELPD plus the entropy of a categorical distribution, i.e.

\[D(p,q,,c)=(,c)-_{k=0}^{K-1}w_{k} w_{k}, \ w_{k}=_{l=1}^{L}(t_{l}=k).\] (6)

The optimal classifier is the \(c\) that achieves the highest prediction ability in the population:

\[D^{}(p,q,)_{c}D(p,q,,c),\ \ \ \ .\] (7)

The next theorem is the basic theory of our method: as long as we pass the simulation draws to a label mapping \(\), and train a classifier on the classification examples, then \(D^{}(p,q,)\) is a generalized divergence between \(p\) and \(q\).

**Theorem 1** (Prediction ability yields divergence).: _Given any \(p,q\), and feature mapping \(\), the optimal prediction ability \(D^{}(p,q,)\) is a generalized divergence from \(p\) to \(q\) in the sense that \(D^{}(p,q,) 0\), and \(p(|y)=q(|y)\) almost everywhere implies \(D^{}(p,q,)=0\). This generalized divergence is reparametrization invariant and uniformly bounded. For any classifier \(c\),_

\[0 D(p,q,,c) D^{}(p,q,)-_{k=0}^{K-1}w_{k}  w_{k};\] (8)

\[p(|y)}{{=}}q(|y) D^{ }(p,q,)=0.\]

That is, any label mapping \(\) produces a generalized divergence \(D^{}(p,q,)\), the prediction ability of the optimal classifier. The prediction ability \(D(p,q,,c)\) of any classifier \(c\) estimated on validation data is a lower bound to \(D^{}(p,q,)\). Further, this generalized divergence \(D^{}(p,q,)\) is always a proper Jensen-Shannon divergence in the projected feature-label space (Theorem 1 in the Appendix).

When \(p(|y) q(|y)\), to increase the statistical power, we wish that the generalized divergence can be as "strong" as possible such that we can detect the miscalibration. In the four examples in Section 2, we have used \(D_{1},D_{2},D_{3},D_{4}\) to denote the (generalized) divergence they yield. The next theorem shows there is a deterministic domination order among these four metrics. Moreover, \(D_{4}\) is the largest possible classification divergence from any given simulation table.

**Theorem 2** (Strongest divergence).: _For any given \(p,q\), and any \(\), (1) \(D_{4} D_{1} D_{3} D_{2}\). (2) \(D_{4}\) and \(D_{1}\) are proper divergences. They attain \(0\) if and only if \(p(|y)=q(|y)\) almost everywhere. They attain the corresponding upper bound in (8) if and only \(p(|y)\) are \(q(|y)\) are disjoint, i.e., \(_{A}p(|y)q(|y)d=0\) for any measurable set A and almost surely \(y\). (3) For any \(p,q\) and \(\) (\(\) can have an arbitrary example size \(L\) and class size \(K\)), \(D_{4}(p,q) D^{}(p,q,)\)._

The following result shows that the divergence \(D_{4}\) in Eq. (4) approaches the "mode-spanning" KL divergence in the limit that the number of posterior draws \(M\). This is appealing because for many inference routes, increasing the number of posterior draws is cheap. Thus, \(D_{4}\) provides an accessible approximation to the KL divergence that is otherwise difficult to compute from samples.

**Theorem 3** (Big \(M\) limit and rate).: _For any \(p,q\), generate the simulation \(\{(y,,_{i},,_{M})\}\) and train the multiclass-classier, then \(D_{4}(p,q)-(p(|y)\;||\;q(|y)) 0\), as \(M\). If further \(p(|y)\) and \(q(|y)\) have the same support, and if \(_{p(|y)}[]^{2}<\) for a.s. \(y\), then_

\[D_{4}(p,q)=(p(|y)\;||\;q(|y))-^{2}(q( |y)\;||\;p(|y))+o(M^{-1}).\]

_where \(^{2}(||)\) is the conditional chi-squared divergence._

## 4 Practical implementation

``` input : The ability to sample from \(p(,y)\) and \(q(|y)\), and a label mapping \(\). output :(i) estimate of a divergence between \(p(|y)\) and \(q(|y)\); (ii) \(p\)-value for testing \(p(|y)=q(|y)\). for(\(\;i=1:S\;\))  Sample \((_{i},y_{i}) p(,y)\), and sample \(_{i1},_{i2},_{iM} q( |y_{i})\); \(\) simulation table  Generate a batch of \(L\) examples of \((t,)=(y_{i},_{i},_{i1},,_{iM})\), \(0 t K-1\); \(\) label mapping  Randomly split the \(LS\) classification examples \((t,)\) into training and validation sets (all \(L\) examples for a given \(i\) go to either training or validation);  Train a \(K\)-class classifier to predict label \(t\) on the training examples, incorporating useful features;  Compute the validation log predictive density \(_{}\) in (9), obtain an estimate of the divergence (7) and  its bootstrap confidence intervals; for(\(\;b=1:B\;\)) for(\(\;b=1:B\;\))  Randomly permute the label \(t\) in the validation set within each batch;  Compute \(_{}^{b}\) on the permutated validation set;  Compute the calibration \(p\)-value \(p=1/B_{b=1}^{B}(_{b}^{} ^{})\). \(\) frequentist test ```

**Algorithm 1**Proposed method: Discriminative calibration

**Workflow for divergence estimate.** We repeat the simulation of \((y,,_{1},,_{M})\) for \(S\) times. Each time we sample \((y,) p(,y)\) and \(_{1:M} q(|y)\) and generate a _batch_ of \(L\) examples through a label mapping \(\). In total, we obtain \(SL\) pairs of \((t,)\) classification examples. We recommend using the binary and multiclass labeling schemes (Examples 1 and 4, where \(L=M+1\) and \(K=2\) or \(M+1\)) such that we can obtain a proper divergence estimate. We split the classification example-set \(\{(t,)\}\) into the training and validation set (do not split batches) and train a \(K\)-class classifier \(c\) on the training set to minimize cross-entropy. Denote \(_{}\) to be the validation index, \((t=t_{j}|_{j},c)\) to be the learned class probability for any validation example \((t_{j},_{j})\), we compute the ELPD (5) by the validation set log predictive density:

\[(,c)^{}(,c)| _{}|^{-1}_{j:_{}}(t=t _{j}|_{j},c).\] (9)

For any \(c\), \(^{}(,c)-_{k=0}^{K-1}w_{k} w_{k}\) becomes a lower bound estimate of the (generalized) divergence \(D^{}(p,q,)\) in Thm. 1, and an estimate of \(D^{}(p,q,)\) itself when the classier \(c\) is good enough. In addition to the point estimate of the divergence, we can compute the confidence interval. It is straightforward to obtain the standard error of the sample mean (9). To take into account the potentially heavy tail of the log predictive densities, we can also use Bayesian bootstrap  that reweights the sum in (9) by a uniform Dirichlet weight.

**Hypothesis testing.** Our approach facilitates rigorous frequentist hypothesis testing. The null hypothesis is that the approximate inference matches the exact posterior, i.e., \(p(|y)=q(|y)\) almost everywhere. We adopt the permutation test: We train the classifier \(c\) once on the training set and keep it fixed, and evaluate the validation set log predictive density \(^{}(,c)\) in (9). Next, permutate the validation set \(B\) times: at time \(b\), keep the features unchanged and randomly permutate the validation labels \(t\) within each batch of examples (\(\) generates a batch of \(L\) examples each time), and reevaluate the validation set log predictive density (9) on permutated labels, call it \(^{}_{b}\). Then we compute the one-sided permutation \(p\)-value as \(p=_{b=1}^{B}(^{}_{b} ^{})/B\). Given a significance level, say 0.05, we will reject the null if \(p<0.05\) and conclude a miscalibration.

**Theorem 4** (Finite sample frequentist test).: _For any finite simulation size \(S\) and posterior draw size \(M\), and any classifier \(c\), under the null hypothesis \(p(|y)=q(|y)\) almost everywhere, the permutation test is valid as the \(p\)-value computed above is uniformly distributed on \(\)._

Our test is exact when the simulation size \(S\) and \(M\) is finite, while the original SBC  relied on asymptotic approximation. Further, we _learn_ the test statistic via the classifier, and our test is valid regardless of the dimension of \(\) and there is no need to worry about post-learning testing , while the usual SBC rank test will suffer from low power due to multiple testing.

Our test is always valid even if the classifier \(c\) is not optimal. Why does our _learning_ step help? For notional brevity, here we only reason for the binary classification. For any \(p,q\), we apply the binary classification as described in Example 1, \(t=0\ \ 1\) and \(=(,y)\).

**Theorem 5** (Sufficiency).: _Let \((,y)=(t=1|,y)\) be the probability of label 1 in the optimal classifier as per (7), and let \(_{c}^{p}\) and \(_{c}^{q}\) be the one-dimensional distributions of this \((,y)\) when \((,y)\) is sampled from \(p(,y)\) or from \(p(y)q(|y)\) respectively, then (i) Conditional on the summary statistic \(\), the label \(t\) is independent of features \(=(,y)\). (ii) Under regularity conditions, there is no loss of information in divergence as the joint divergence is the same as the projected divergence in the one-dimensional \(\)-space \(D_{1}(p,q)=D_{1}(_{c}^{p},_{c}^{q})\)._

That is, the best prediction \(\) entails the best one-dimensional summary statistics of the high dimensional \( y\) space. The enhancement of the test power from using the sufficient statistics is then assured by the Neyman-Pearson lemma .

### Feature engineering: use rank, log density and symmetry

Reassuringly, whatever the classifier \(c\), the prediction ability \(D(p,q,,c)\) is always a lower bound to the corresponding divergence \(D^{}(p,q,)\), and the permutation test is always exact. But we still wish to train a "good" classifier in terms of its out-of-sample performance, for a tighter bound in divergence, and a higher power in testing. We will typically use a flexible parametric family such as a multilayer perceptron (MLP) network to train the classifier.

The oracle optimal probabilistic classifier is the true label probability, and in the binary and multiclass classification (Example 1 and 4), the oracle has closed-form expressions, although we cannot evaluate:

\[_{}(t=0|,y)=,_{}(t|_{0},,_{M},y)=,y)/q(_{t}|y)}{_{k=0}^{M}p(_{k},y)/q(_{k}|y)}.\] (10)

Statistically-meaningful feature.Depending on the inference task, we have more information than just the sample points and should use them in the classifier. In light of the shape and component of the optimal classifiers (10), the following statistically-meaningful features are useful whenever available: (i) The log target density \( p(|y)\). As proxies, the log joint density \( p(,y)\) and the log likelihood \( p(y|)\) are often known to traditional Bayesian models. (ii) The log approximate density \( q(|y)\), known to volitional and normalizing flows. (iii) When the log approximate density is unknown, a transformation is to integrate the density and obtain the posterior CDF, \(Q(|y)=_{x q(x|y)}\,(x>)\), and this CDF can be approximated by the rank statistics among the approximate draws up to a rescaling \(r(,y)_{m=1,i=1}^{M}(y_{i}=y) (<_{im})\). See Appendix Table 1 for the usage and extension of these features in binary and multiclass classifiers.

Linear features.We call the log likelihood, the log approximate density, or log prior (whenever available) linear features, and denote them to be \(l(,y)\). For example if both likelihood and \(q\) densityis known, then \(l(,y)=( p(y|), q(|y))\). Because they appear in the oracle 10, we will keep linear features in the last layer of the network (followed by a softmax). We recommend parameterizing the binary classification in the following form:

\[(t=1|(,y))=[(,y)+w^{T}l(,y)].\] (11)

Symmetry in multiclass classifier.The multi-class classifier is harder to train. Luckily, we can use the symmetry of the oracle classifier (10): the probability of class \(k\) is proportional to a function of \((_{k},y)\) only, hence we recommend parameterizing the multiclass probability as

\[(t=k|(_{0},_{1},,_{M},y))=,y ))}{_{k^{}=0}^{M}(g(_{k^{}},y))},\;g(,y)= (,y)+w^{T}l(,y),\] (12)

where \(l(,y)\) is available linear features. We only need to learn a reduced function from \( y\) to \(\), instead of from \(^{M+1} y\) to \(\), reducing the complexity while still keeping the oracle (10) attainable.

Zero waste calibration for MCMC.If \(_{1},,_{M} q(|y)\) are produced by MCMC sampler, typically \(\) has autocorrelations. Although classical SBC originated from MCMC application, the rank test requires independent samples, so SBC can only handle autocorrelation by thinning: to subsample \(_{m}\) and hope the thinned \(\) draws are independent. Thinning is a waste of draws, inefficient when the simulations are expensive, and the thinned samples are never truly independent. With MCMC draws \(\), our method could adopt thinning as our Thm. 1 and 4 are valid even when \(M=1\).

Yet we can do better by using all draws. The "separable" network architecture (12) is ready to use for MCMC samples. For example, we sample \((,y) p(,y)\), and sample \((_{1},,_{M})\) from a MCMC sampler whose stationary distribution we believe is \(q(|y)\), and generate examples from the multiclass permutation (Example 4). Then we run a separable classifier(12) to predict \(t\). Intuitively, the separable network design (12) avoids the interaction between \(_{m}\) with \(_{m^{}}\), and disallows the network to predict \(t\) based on the autocorrelation or clustering of \(\). The next theorem states the validity of our method in MCMC settings without thinning.

**Theorem 6** (Mcmc).: _Suppose we sample \((,y) p(,y)\), and sample \((_{1},,_{M})\) from a MCMC sampler whose stationary distribution we believe is \(q(|y)\) (i.e., marginally \(_{i}\) is from \(q(|y)\)), and generate examples \(((t_{1},_{1}),,(t_{M+1},_{M+1}))\) from the multiclass permutation, such that \(=(_{0},_{1},,_{M})\). Then we run an exchangeable classifier (12) in which \(g\) is any \(\) mapping. Denote \(D_{4}^{}(p,q)\) to be the predictive ability of the optimal classifier among all separable classifiers (12), then \(D_{4}^{}(p,q)=D_{4}(p,q)\)._

Dimension reduction and nuisance parameter.Sometimes we only care about the sampling quality of one or a few key dimensions of the parameter space, then we only need to restrict the classifier to use these targeted dimensions, as a result of Theorem 1. For example, in binary classification, if we reduce the feature \(=(,y)\) to \(=(h(),y)\) in the classifier, where \(h()\) can be a subset of \(\) dimensions, then the resulting classification divergence becomes projected divergence between \(h()|y, p(|y)\) and \(h()|y, q(|y)\), and other nuisance parameters do not impact the diagnostics.

Weighing for imbalanced binary classification.When \(M\) is big, the binary classification (Example 1) can suffer from imbalanced labels, and the divergence in (1) degenerates: \(D_{1}(p,q) 0\) as \(M\). One solution is to use the multiclass classification which has a meaningful limit (Thm. 3). Another solution is to reweigh the loss function or log predictive density by the label \(t\). If the weights of class 1 and class 0 examples are \(\) and \(\), used in both training and validation log prediction density, then regardless of \(M\), the weighted classification is equivalent to balanced classification, and the resulting divergence is the symmetric Jensen-Shannon (JS) divergence \([p(|y)||r(|y)]+ [q(|y)||r(|y)]\), where \(r(|y)=[p(|y)+q(|y)]\). See Appendix A for proofs.

## 5 Experiments

Closed-form example.Consider a multivariate normal parameter prior \(^{d}(,_{d})\) and a normal data model \(y|(,)\), so the exact posterior \(p(|y)\) is explicit. In the experiments,we use neural nets to train binary (11) or symmetric multiclass classifiers (12), which we generally recommend. We assume the inference posterior \( q(|y)\) is known and added to the classifier.

_Legitimacy of testing_. First, to validate our hypothesis test under the null, we use true inference \(q(|y)=p(|y)\). With \(d=16\), we simulate \(S=500\) draws of \((,y) p(,y)\), then generate true posterior \(_{m} p(|y)\), and run our binary-classifier and obtain a permutation \(p\)-value. We repeat this testing procedure 1000 times to obtain the distribution of the \(p\)-values under the null, which is uniform as shown in Fig. 2, in agreement with Thm. 4. Because \(\) has \(d\) dimensions, a SBC rank test on each margin separately is invalid without adjustment, and would require Bonferroni corrections.

_Power_. We consider two sampling corruptions: (i) bias, where we add a scalar noise (0.01 to 0.2) to each dimension of the true posterior mean, (ii) variance, where we inflate the true posterior covariance matrix by multiplying a scalar factor (0.8 to 1.2). We compare our discriminative tests to a Bonferroni-corrected SBC chi-squared rank-test. In both settings, we fix a 5% type-I error and compute the power from 1000 repeated tests. Fig. 3 shows that our method has uniformly higher power than SBC, sometimes as good as SBC with a 10 times bigger simulation sample size.

_Divergence estimate_. The left panel of Fig. 4 validates Thm. 1: We run a weighted binary classifier on the Gaussian simulations with a scalar bias 1 or 2 added to the posterior mean in \(q\). As the number of simulations \(S\) grows, the learned classifier quickly approaches the optimal, and the prediction ability matches the theory truth (dashed line): the Jensen-Shannon divergence between \(p(|y)\) and \(q(|y)\). The right panel validates Thm. 3: With a fixed simulation size \(S\), we increase \(M\), the number of posterior draws from \(q\), and run a multiclass classification. When \(M\) is big, the estimated divergence converges from below to the dashed line, which is the theory limit \((p(|y),q(|y))\) in Thm. 3.

Benchmark examples.Next, we apply our calibration to three models from the SBI benchmark : the simple likelihood complex posterior (SLCP), the Gaussian linear, and the Gaussian mixture model. In each dataset, we run adaptive No-U-Turn sampler (NUTS) and check the quality of the sampled distribution after a fixed number of iterations, varying from 2 to 2000 (we use equal number of iterations for warm-up and for sampling, and the warm-up samples were thrown away). At each given MCMC iterations, we run our classifier calibration, and estimate the JS divergence, as reported

Figure 4: In the Gaussian example with bias = 1 or 2, with a moderately big simulation sample size \(S\), the divergence estimated from a binary (left panel) or multiclass (right panel) classifier matches its theory quantity (dashed): the Jensen-Shannon or Kullback–Leibler divergence in the big-\(M\) limit.

Figure 5: We apply binary classifier calibration to posterior inferences on three models from the SBI benchmark, and check the sampling quality after some iterations. The \(x\)-axis is the number of MCMC iterations. The \(y\)-axis is the estimated JS divergence (\(\) standard error) between the true target and sampled distribution at a given number of iterations, indicating gradual convergence.

Figure 3: Our test has a uniformly higher power than SBC rank test. We simulate wrong inference \(q\) by multiplying the posterior covariance or adding biases to the posterior mean.

Figure 2: Our test is valid as it yields uniformly distributed \(p\)-values under the null \(p(|y)=q(|y)\). We check all dimensions at once.

in Fig. 5. In all three panels, we are able to detect the inference flaws at early iterations and observe a gradual convergence to zero.

**Visual check.** Our diagnostic outputs rigorous numerical estimates, but it also facilities visual checks. We make a scatter plot of the binary classifier prediction \((t=1|)\), a proxy of \(p(|y)/q(|y)\), against any one-dimensional parameter we need to check: If that parameter is under- or over-confident, this scatter plot will display a U- or inverted-U-shaped trend. Compared with SBC rank histograms, our visualization can further check the magnitude of mismatch (how far away \((t=1|)\) is from 0.5), tail behavior (small \(q(|y)\)), and several dimensions jointly. Fig 6 is a visual check in the Gaussian example when we multiply the posterior covariance in inference \(q\) by 0.8, 1, or 1.2.

**Galaxy clustering.** We would like to model galaxy spatial clustering observations in a Lambda cold dark matter framework , where observations \(y\) correspond to statistical descriptors of galaxy clustering, and a 14-dimensional parameter \(\) encodes key cosmological information about the Universe. The forward simulation \(y|\) involves expensive \(N\)-body simulations (each single \(y\) simulation takes around 5000 cpu hours). We consider three different physical models \(p\): they contain the same cosmology parameter \(\) but three types of observations whose dimension \(d_{y}\) varies from 38 to 1031, reflecting three power-spectrum designs. We apply simulation based inference to sample from each of the models, where we try various normalizing flow architectures and return either the best architecture or the ensemble of five architectures. We now apply our discriminative calibration to these six approximate inferences using a weighted binary classifier trained from \(S=2000,M=500\) simulations for each inference (that is 1 million examples). We have added the log densities \( q(|y)\) as extra features in the classifier, as they are known in the normalizing-flows. The table above shows the estimated Jensen-Shannon distances and standard deviation. Compared this table with the estimates from blackbox MLP classifier (Appendix Fig. 8), using statistical features greatly improves the label prediction and tightens the divergence estimate. From the table above, all inferences \(q\) have a significantly non-zero but small divergence, among which the 5-ensemble that uses a mixture of 5 flows always has a lower divergence so is more trustworthy. Further visualization of the classification log odds (Fig. 7) reveals that \(q\) is under-confident in the parameter that encodes the galaxy concentration rate. To interpret the tail behavior, the predicted log odds are negatively correlated with log joint normalizing-flow density \(q(|y)\), suggesting \(q(|y)>p(|y)\) in the tail of \(q(|y)\), another evidence of \(q\) underconfidence.

We share Jax implementation of our binary and multiclass classifier calibration in Github2.

## 6 Discussions

This paper develops a classifier-based diagnostic tool for Bayesian computation, applicable to MCMC, variational and simulation based inference, or even their ensembles. We learn test statistics from data using classification. Through a flexible label mapping, our method yields a (family of) computable divergence metric and an always valid testing \(p\)-value, and the statistical power is typically higher than the traditional rank based SBC. Various statically-meaningful features are available depending on the task and we include them in the classifier. Visual exploration is also supported.

Figure 6: _Scatter plots of the classifier prediction v.s._ Figure 7: _In the cosmology example we visually check a one-dimensional parameter we need to check can the classifier predicted log odds v.s. cosmology param-reveal marginal over- or under-confidence in inference \(_{b}\) and \(\), and \( q(|y)\). Underconfidence is \(q(|y)\) from a U or inverted-U shape._ _detected in \(\) marginally, and more so in the joint._

**Related diagnostics.** The early idea of simulation based calibration dates back to  who compared the prior and the data-averaged posterior, i.e., to check \(p()=\!q(|y)p(y)dy\) using simulations.  further developed techniques for the first and second moment estimate of the data-averaged posterior using the law of total variance. Our method includes such moment comparison as special cases: using the no-\(y\) binary labeling (Example 2), then comparing the empirical moments of the \(q\) sample the \(p\) sample can be achieved through a naive Bayes classifier or a linear discriminant analysis.

The rank-based SBC  can be recovered by ours using binary labels and taking ranks to be features (Example 3). The rank statistic is central to SBC, which follows the classical frequentist approach--using the tail probability under the posited model quantifies the extremeness of the realized value is only a convenient way to locate the observations in the reference distribution, especially in the past when high dimensional data analysis was not developed--it is the use of modern learning tools that sharpens our diagnostic. Recently, SBC has developed various heuristics for designing (one-dimensional) test statistics \((,y)\). Such rank tests are recovered by our method by including the rank of \(\) in our features (Sec. 4.1). For example,  proposed to test the rank of the likelihood \(=p(|y)\) in MCMC,  looked at the rank of proposal density \(q(|y)\),  used the \(q\)-probability integral transformation in normalizing flows. In light of our Theorem 5, the optimal test statistic is related to the density ratio and hence problem-specific, which is why our method includes all known useful features in the classifier and learns the test statistic from data.

**Classifier two-sample test.** Using classification to compare two-sample closeness is not a new idea. The classifier two-sample test (C2ST) has inspired the generative adversarial network (GAN)  and conditional GAN .  has developed GAN-typed inference tools for SBI. In the same vein,  used classifiers to diagnose multiple-run MCMC, and  used classifiers to learn the likelihood ratio for frequentist calibration, which is further related to using regression to learn the propensity score  in observational studies. The theory correspondence between binary classification loss and distribution divergence has been studied in . This present paper not only applies this classifier-for-two-sample-test idea to amortized Bayesian inference to obtain a rigorous test, but also advances the classifier framework by developing the theory of the "label mapping", while the traditional GAN-type approach falls in the one-class-per-group category and deploy binary classifiers as in Example 1. Our extension is particularly useful when samples are overlapped (multiple \(\) per \(y\)), autocorrelated, and imbalanced.

**KL divergence estimate from two samples.** As a byproduct, our proposed multiclass-classifier provides a consistent KL divergence estimate (Thm. 3) from two samples. Compared with existing two-sample KL estimate tools from the \(f\)-divergence representation  or the Donsker-Varadhan representation , our multiclass-classifier estimate appears versatile for it applies to samples with between-sample dependence or within-sample auto-correlation. It is plausible to apply our multiclass-classifier approach to other two-sample divergence estimation tasks such as the \(f\)-GAN , which we leave for future investigation.

**Limitations and future directions.** Like traditional SBC, our method assesses the difference between \(p(|y)\) and \(q(|y)\), averaged over \(y\). This is a "global" measure relevant to developing algorithms and statistical software. But sometimes the concern is how close \(p\) and \(q\) are for some particular observation \(y=y^{}\). "Local" diagnostics have been developed for MCMC , variational inference  and simulation-based inference  that try to assess \(q(|y^{})\) only. It would be valuable to extend our approach to address these cases. Another future direction would be to extend our approach to posterior predictive checks  that diagnose how well the statistical model \(p\) fits the observed data.