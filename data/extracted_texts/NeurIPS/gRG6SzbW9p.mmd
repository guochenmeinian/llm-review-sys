# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

the learned reward model will ignore the minority group's preferences . These issues necessitate _pluralistic alignment_ of models to human preferences [59; 44] (see Figure 1); ideally, we would democratize RLHF to account for a wider variety of human values to better serve a diverse population.

Current approaches to RLHF  use the Bradley-Terry-Luce (BTL)  model to learn a reward model that explains the human preferences. While the BTL model accounts for noisy preferences, RLHF typically applies this model under the 'unimodal' assumption that all human preferences are derived from a single utility function. This fails to capture scenarios where preferences diverge--i.e. are multi-modal--due to fundamentally different utilities. For example, Figure 1 shows a case where one group of users prefers detailed responses, while another prefers concise ones. By performing maximum likelihood estimation under the unimodal BTL model, current methods learn a reward function that averages these multi-modal preferences (akin to mode averaging in imitation learning . As we show in our experimental results, this model misspecification leads to reward models that are _inaccurate_, and the policies optimized on these rewards fail to accomplish tasks per _any_ of the distinct preferences (see Figures 6, 3). Thus, vanilla RLHF methods [52; 17] are insufficient for aligning AI systems to diverse human values.

In many applications, from large language models (LLMs) to assistive robotics , users are diverse, and AI systems must adapt the generated responses to user-specific preferences to successfully complete the task. Consider, for example, a robot assistant putting away dishes in a user's kitchen: each individual has unique preferences for how the dishes in their kitchen are organized, potentially diverging from others' preferences. In the context of LLMs, failing to adapt to user-specific preferences can make them unhelpful, unsafe, and vulnerable to jailbreak in the presence of conflicting objectives [2; 58]. To build safe and performant foundation models serving a diverse population, we need methods that can explicitly account for and adapt to the inherent plurality of human preferences.

These insights suggest that human preferences are not derived from a single utility function, but are affected by unobserved, hidden user context . To accurately model individual utilities, RLHF should be able to efficiently infer and adapt to the user context. With this in mind, we formulate RLHF as a latent variable problem. Building on techniques from variational inference [11; 38], we propose a method--Variational Preference Learning (VPL)--for multi-modal reward modeling. Intuitively, given a few preference annotations from a particular user, our approach uses a variational encoder to infer a latent distribution over hidden context, and a latent conditional reward model to accurately recover the true _multi-modal_ preference distribution. We derive an evidence lower bound (ELBO)

Figure 1: Current RLHF approaches  incorrectly assume a unimodal BTL reward model for a diverse population of users. In this example, users have diverging preferences over the level of detail provided in the responses from a large language model. Without additional context, the BTL model considers both responses to be equally likely. In contrast, our method, VPL, is a personalized approach to RLHF. Using a few samples from a particular user, we infer the distribution over their distinct preferences. Based on this distribution, we condition the reward model to more accurately predict rewards, and enable steering the resulting policy to personalize to the specific user. This enables accounting for and serving the preferences of under-represented groups which would otherwise be ignored by the standard BTL model .

for latent-variable preference-based reward optimization. Our proposed algorithm, VPL, effectively learns a distribution of reward functions from large corpora of preferences from diverse users.

In developing practical training methods for such latent-conditioned reward models, we show that several complexities and technical considerations arise. A key problem is that binary comparisons inherently lack information regarding the scale of rewards. Under the BTL model (and correspondingly the VPL model), the preference label between two alternatives \(A\) and \(B\) can only provide information about \(r_{A}-r_{B}\). So, the learned reward function may have vastly varying reward scales across individual users that adversely affect the optimization landscape of multi-user reinforcement learning [31; 69] using these rewards. To mitigate this, we show how a simple pairwise classification scheme [61; 49] can appropriately bound and scale reward estimates within our latent variable framework, thereby enhancing the performance of downstream learned policies. The predicted latent distribution enables several additional capabilities. In Section 4 we use our approach to learn latent-conditioned policies that can personalize to particular users at test time. Additionally, the latent variable reward models can measure uncertainty in the reward distribution . So, in Section 4.2, we use our approach to actively query [8; 10; 9] users to minimize the number of labels they need to provide before we can adapt to their distinct preferences.

Overall, our work introduces a latent variable framework for reward modeling that can encode and approximate the distribution of user preferences directly from binary comparisons, and steer the downstream policy to adapt to diverse user preferences. We conduct a broad range of experiments across three simulated robotics environments and two language tasks with conflicting user preferences. Our results show that in simulated domains, VPL accurately models rewards and improves task performance and personalization. We are able to scale this method to many users, and use active learning to adapt efficiently to particular users with significantly fewer queries at test time. In the language domain, we are able to train multiple LLM-based reward models that learn a separable embedding space that can distinguish between users with divergent preferences. The resulting models outperform existing by RLHF approaches 10-25% [52; 58] by more precisely predicting rewards that align with diverse users and objectives across multiple datasets.

## 2 Related Work

**Reinforcement Learning from Human Feedback (RLHF):** We focus on the problem of reinforcement learning (RL) from binary human preferences using the Bradley-Terry-Luce (BTL) model . This has a rich history in the field of RL and robotics, often referred to as Preference-based RL (PbRL) [65; 28; 1; 8; 10]. We specifically build on the framework outlined in Christiano et al.  and further expanded in recent works [52; 2; 74; 60; 37; 58]. This has seen a wide range of applications ranging from training robots [17; 10; 62] to finetuning language models for alignment [52; 74; 60]. Our work, enabling RLHF with diverse preferences, is easily applicable to _any_ preference-based learning method, including recent techniques [55; 24] that circumvent reward modeling altogether.

**RLHF under non-BTL models:** Prior work has aimed to study non-BTL models of human behavior and preferences [12; 43; 42; 37; 61] to account for human irrationality and uncertainty in preferences, or intransitive preferences [49; 61; 64]. However, our key argument in this work is less about human irrationality (i.e. inconsistency), and more about the divergence between potentially rational preferences for different labeling users and end users. In this sense, our work is complementary in that the latent variable model can easily be adapted to non-BTL models as well. In fact, we incorporate the technique proposed in Swamy et al.  to improve reward learning for downstream applications.

**Personalized RLHF:** While some works [39; 40; 35] characterize similar challenges as VPL, they largely focus on exploring the societal issues underpinning the need for personalization and introducing datasets with diverse annotations. Conitzer et al.  argue Social Choice Theory could provide insights into how to aggregate diverse preferences. But these works do not propose a technical method to achieve modeling diverse preferences.

Several prior works have looked at trading off conflicting alignment objectives (such as remaining both helpful and harmless) through techniques like Pareto-optimal optimization [13; 16] or multi-objective RL [67; 21]. For example, Jang et al.  treats personalization as a multi-objective RL problem, requiring explicit decomposition into different sub-rewards and learning independent reward models for different users. Further, Dai et al.  introduces Safe RLHF, an approach that explicitly models the different objectives of helpfulness vs harmfulness. In contrast, our work does not aim to 

[MISSING_PAGE_FAIL:4]

[MISSING_PAGE_FAIL:5]

information about the scale of rewards, but only their relative ordering. As a simple illustration, if we have a pair of states \(s_{A},s_{B}\), where the users prefer \(s_{A}\) i.e. \(s_{A} s_{B}\), two different reward functions: \(r(s_{A})=100,r(s_{B})=50\) or \(r(s_{A})=50,r(s_{B})=0\) have the same likelihood under the BTL model. Empirically, we observe that this poses problems for optimizing Equation 3; different values of the latent variable \(z\) generate reward functions of vastly different scales. This is an issue for several reasons: 1) varying reward scales adversely affect the landscape of multi-user policy optimization (often observed in multi-task RL) , and 2) it is challenging to identify states where user preferences diverge across the population as differently scaled rewards cannot be directly compared.

To address this issue, we experiment with several different techniques for scaling the learned reward functions (see Appendix A.2). Our key insight in solving this challenge lies in the observation that while raw rewards from BTL are not scaled equally across \(z\), probabilities from the preference likelihood model \(p(y s_{A},s_{B},z)\) are appropriately scaled. This suggests that an effective solution to the reward scaling issue is to replace the raw rewards from the BTL model (\(r(s,z)\)) with likelihoods suggested by the pairwise preference likelihood model \(p(y s_{A},s_{B},z)\). In particular, a natural choice of scaled rewards for state \(s_{A}\) is the expected likelihood that \(s_{A}\) is "preferred" to all other states (or a sampled set of states) \(s_{B}\) observed in the data - \(r_{}(s_{A},z)=}_{s_{B}}[p_{}(y=1  s_{A},s_{B},z)]\). Since these are probabilities, normalized in the \(\) range, the set of rewards is consistent across latents \(z\). Note that these expected likelihood rewards can easily be obtained from the objective in Equation 3 since we train a latent-conditional preference classifier. While proposed from a very different perspective, we note the similarity of this reward scaling approach to recent work [61; 49], in particular, Self-Play Preference Optimization (SPO) , which was originally proposed to address the issue of intransitive preferences. Similar to  we assume that the oracle providing preference labels is Non-Markovian. Due to this similarity3, we use VPL-SPO to indicate this approach of preference-based reward scaling throughout our experiments (See Algorithm 3).

### Active Learning of Preferences to Minimize Latent Uncertainty

A natural question that arises for test time deployment of the latent-conditioned policies is how to obtain the set of state pairs \(\{(s^{i}_{A},s^{i}_{B})\}_{i=1}^{N}\) to be annotated with preference labels and used for posterior inference, \(z q_{}(z\{(s^{i}_{A},s^{i}_{B},y^{i})\}_{i=1}^{N})\). Not all query sets \(\{(s^{i}_{A},s^{i}_{B})\}_{i=1}^{N}\) are made equal; some are more informative than others. Certain states (where preferences _vary_ across annotators) are particularly informative in accurately inferring the \(z\) which should be used for policy deployment \((a|s,z)\). In VPL, the probabilistic modeling of the variational encoder naturally allows for active selection of the most informative query set based on maximal information gain, following prior work [8; 10; 50]. Here the provision of preference labels \(\{y^{i}\}_{i=1}^{N}\) will provide the maximum information about the latent distribution (and indirectly, the user preferences). This active query selection procedure can be expressed as the following optimization problem, maximizing the mutual information between the labels and the latent distribution.

\[\{(s^{i}_{A},s^{i}_{B})\}_{i=1}^{N}_{\{(s^{i}_{A}, s^{i}_{B})\}_{i=1}^{N}}(z;\{y^{i}\}_{i=1}^{N} q_{},\{(s^{i}_ {A},s^{i}_{B})\}_{i=1}^{N})\] (4)

The posterior \(q_{}\) is a multivariate Gaussian, and assuming a uniform distribution over the set of labels, \(q_{}(z\{(s^{i}_{A},s^{i}_{B})\}_{i=1}^{N})\) allows for closed form solution for mutual information \(\). To solve the maximization objective, we chose the query set \((s^{i}_{A},s^{i}_{B})_{i=1}^{N}\) with the maximum information gain, across samples from the preference dataset. Finally, we elicit user labels on this maximal query set, infer the latent, and condition the policy on this latent at deployment.

## 5 Scaling VPL for Reward Learning in Large Language Models (LLMs)

VPL can be used to train pluralistic reward models for LLMs, accounting for diverse human preferences and values. Here we discuss the key details that are essential to scale our method to LLMs. The architecture of our LLM reward model is shown in Figure 2. Unlike prior work which attempted to insert summary embedding layers into LLMs (see e.g. ), we find that we can successfully compress user preference information into a concise, probabilistic embedding vector \(z\) without sacrificing reward model performance. Further details and hyperparameters are discussed in Appendix B.

**Prompt and Response Embeddings.** Since using raw representations of the prompt and responses can increase the context length significantly, we use a pre-trained LLM to encode prompt and response pairs together  (to be consistent with previous notation, we assume a preferred state \(s^{A}\) contains both a prompt and response \([x,r]\), and \(e^{A}=LLM(s^{A})\)). For efficient training, we pre-compute and freeze the encoded embeddings.

**Latent Encoder.** Given a set of multiple encoded preference queries from the same user, \(\{(e^{A}_{i},e^{B}_{i},y^{i})\}_{i=1}^{N}\), we pass each through the same pair encoder to obtain \(h_{i}=enc(e^{A}_{i},e^{B}_{i})\). The latent representation \(z\) is generated using self-attention layer over the set of encoded pairs, \(\{h_{i}\}_{i=1}^{N}\).

**Reward learning.** Here, the representation \(e^{A^{}}\) of a new state \(s^{A^{}}\) is concatenated with a \(z\) sampled from the posterior distribution which is passed into an MLP to predict the rewards. The LLM is fine-tuned using low-rank adaptation (LoRA) , and unlike typical RLHF settings, we find that we need to train the reward model for \( 1\) epochs to fit the encoder and the reward model.

**Data augmentation.** As we scaled VPL to larger datasets with more users, we found that augmenting the training dataset with multiple context samples from the same user for each new data point is essential to learning an effective encoder. Formally, at training time, given a prompt and response pair \(s^{}_{A},s^{}_{B}\) from a particular user, we generate \(M\{4,8\}\) duplicates of this labeled response with different contexts i.e. annotated prompt and response pairs \((\{(s^{i}_{A},s^{i}_{B},y^{i})\}_{i=1}^{N})_{j=1}^{M}\); where each context \((\{(s^{i}_{A},s^{i}_{B},y^{i})\}_{i=1}^{N})_{j}\) is sampled from a user annotated subset of size \(K\) (\(K>N\)).

## 6 Experimental Evaluation on Simulated Control Tasks

In our experiments, we answer the following questions: (1) Can VPL accurately learn a multi-modal distribution of reward functions from a preference dataset labeled by diverse users? (2) Do the inferred latent user vectors enable learning a multi-task personalized policy? (3) Can we leverage the posterior to actively query preferences for latent estimation? In this section, we show the benefits of VPL in multiple simulated control tasks and demonstrate that VPL is able to capture multi-modality in the underlying reward functions, arising due to underspecification of annotator goals and preferences. In the following section, we ask whether VPL scales up to RLHF for LLMs.

**Training and Evaluation Details:** We test our hypothesis across evaluation domains in two steps: 1) We train a reward model on a dataset of preferences collected using diverse simulated humans; 2) We train a policy using RL to maximize the learned rewards. For these experiments, we use Implicit Q-Learning , an offline RL algorithm that achieves strong performance on offline RL benchmarks . Using the learned reward function \(r_{}(s,z)\), and the prior \(p(z)\), we label the reward-free offline RL dataset \(D=(s_{t},a_{t},s_{t+1})\), by sampling a latent \(z p(z)\), and setting the reward as \(r_{t}=r_{}(s_{t})\), or a one-step look-ahead method, where \(r_{t}=r_{}(s_{t+1})\) (refer to Algorithm 2 for complete method). We include the hyperparameters and the training details in the Appendix B

**Baselines:** We compare our method against multiple baselines: 1) **Oracle **: This is a goal-conditioned offline RL method, that presents an oracle with access to the true reward functions for all annotators. 2) **BTL **: This is the standard RLHF method from [17; 52] as a baseline, where the reward model is approximated using the _unimodal BTL_ function. 3) **DPL **: Following the work on accounting for hidden context in RLHF, we train a distributional reward model, using both the mean-variance (MeanVar) and categorical (Categorical) approximation for the reward functions. 4) **VPL (Ours)**: We denote two versions of our method, **VPL** and **VPL + SPO**, discussed in Section 4.

### Tasks:

We evaluate our approach on three diverse simulated control tasks (Figure 4) to demonstrate the effectiveness of VPL to learn latent conditioned policies.

Figure 2: VPL LLM architecture for reward learning. The left and right parts denote the encoder \(q_{}\) and the reward model \(r(s,z)\) respectively.

**Maze-Navigation** is based on the D4RL benchmark . Here, users guide a pointmass agent to goals marked with their preferred colors. The user's preferred color is underspecified, so the agent has to infer their preferences from a few annotated comparisons and navigate to one of two/ten locations.

**Ravens-Manipulation** requires the agent to rearrange an object on a table-top (akin to rearranging a dining table), based on user preferences. Built on the ravens benchmark , the agent controls a robot arm in a _continuous action space_ to pick and place the box in one of the two preferred locations.

**Habitat-Rearrange** tasks a mobile robot arm with relocating a bowl to a user-specified location among five candidates (e.g., desk, kitchen, table). The robot is equipped with motion primitives for navigation and manipulation, and the problem involves reasoning over these locations. This simplified one-step reasoning task highlights the need for personalization in assistive robotics, as each person has distinct preferences for robots in their homes.

**Habitat-Tidy** is a simulated environment similar to TidyBot , where the robot infers placement locations for objects in a kitchen based on user preferences. Users may prefer sorting or cleaning objects by different attributes (e.g., function or material), and the robot must adapt to these preferences. Like Habitat-Rearrange, the task involves reasoning over attributes for sorting based on existing preferences, using motion primitives for navigation and manipulation.

### Can VPL capture multi-modal reward functions from a dataset of diverse preferences?

We generate preferences using multi-modal reward functions across multiple didactic and simulated discrete and continuous control experiments, as shown in Figures 6, 3, 4, 11 and 10. We see that the BTL baseline with an MLP model averages the different underlying rewards and learns an inaccurate reward model (Figure (b)b). In the presence of a majority, BTL converges to the preferences ignoring the minority groups (See Figure 11). While DPL  can recover the uncertainty in the reward models due to underspecification, they have no mechanism to recover the individual reward functions. As a result, the DPL reward model estimates high variance rewards (see Figures 6, 10) for each particular user. In contrast, VPL infers the hidden user context using the latent variable formulation and accurately recovers the multi-modal reward distribution.

### Do distributional reward functions enable learning a steerable multi-task policy?

As discussed above, the baselines average the reward modes across the users and learn an inaccurate reward function. So, in Figure 4 the policy for the _Maze-Navigation_ converges to a wrong goal, leading to poor performance and misalignment with all the underlying users. For the _Ravens-Manipulation_ experiments, the baselines randomly choose a goal location and do not adapt to the user preference at test time. Similarly, for the _Habitat-Rearrange_ tasks the baselines are unable to capture the diversity in user preferences over the multiple locations and do not succeed in placing the bowl at the correct location. Finally, in the Habitat-Tidy task, we observe that the baselines converge to an accuracy achieved when they ignore the diverse preferences of sorting the objects by the users.

In contrast, the policies trained using VPL outperform all the baselines in terms of the task success rate, according to the user's underlying reward function. For the navigation task, VPL correctly infers

Figure 4: Performance of a downstream policy on diverse control and reasoning tasks, using the rewards trained using different baselines. We report the mean and standard error over five seeds. Note: Habitat envs have a one-step greedy policy so reward scaling and SPO+VPL are not required.

Figure 3: Ground truth preferences (a) show that annotators prefer the robot navigate to two different goals. Unimodal BTL (b) averages over the two modes. VPL (c) accurately reconstructs diverse preferences, and learns \(z\) conditioned policies to reach either goal.

the goal and the learned policy can navigate to the goals with a high success rate, and the performance is comparable to a goal-conditioned oracle. For _Ravens-Manipulation_, VPL infers the user latent at test time and accurately places the box at the right location. Finally, VPL is mostly able to correctly identify the preferred location for the bowl in the user's home and has a higher success rate than the baselines. We note that scaling the rewards via VPL + SPO improves the performance of multi-task RL for optimizing diverse user preferences. In Habitat-Tidy, VPL is able to infer the user preferences and follow the preferred attributes while placing the objects. However, the accuracy of the robot is dependent on the context length and we discuss this in further detail in A.4.

### Can VPL enable active query selection for latent estimation?

In Section 4.2, we present an objective to actively query users at test time to efficiently infer user preferences. Figure 5 shows that this technique leads to better performance of the learned policy across varying numbers of queries \(\|N\|\). This implies that the active learning objective 4 which maximizes information gain over the latent distribution generates queries that are more discriminative and provides a more informative posterior for user identification. This results in a more efficient adaptation of the downstream policy to the distinct user preferences, achieving the same performance with only half the queries. These methods can be potentially transferred to LLMs to query and identify user preferences with minimal questions.

## 7 LLM Experiments

In this section, we ask: can VPL scale up to the pluralistic alignment of LLM-based reward models? We compare the reward modeling performance of our method against two baselines: the vanilla BTL model and DPL . We experiment with two LLMs: GPT2  and Llama2-7B , and two pluralistic preference datasets.

### Datasets for pluralistic LLM alignment

Prior RLHF works have focused mainly on unimodal BTL models, and as such there is a lack of publicly available datasets containing annotated preferences with divergent objectives. To evaluate our method on capturing multi-modality in preferences for LLMs, we consider two benchmarks. First, we introduce a synthetic dataset, _Pets_, that directly represents multimodal preferences, and second, we augment the publicly available UltraFeedback  dataset.

**Pets.** Here, the dataset is generated to reflect multi-modal user preferences, where each user has a preference ranking over four kinds of animals (in this case cats, dogs, birds, and rabbits). To simulate a setting where users agree on some comparisons and disagree on others, we consider two users who agree on the best and worst pet and disagree on the middle pair of rankings over pets. Preferences here are divergent in certain cases (middle pets), and agree in other instances (best and worst pets), requiring multimodal preference modeling. We evaluate our approach on two versions of the dataset: Pets (Full), and Pets (Divergent) which contains only those prompt and response pairs where the users are divergent (i.e. they have conflicting preferences). For the contexts \(\{(s_{A}^{i},s_{B}^{i},y^{i})\}_{i=1}^{N}\), we randomly sample 1-4 other prompts and ranked responses from the same user.

**UltraFeedback-P.** To construct this dataset _UF-P_ (where P stands for personalized), we use the fine-grained scores over different attributes available in the UltraFeedback (UF)  dataset to construct different users - a similar approach to prior work . We construct a dataset with two users, _UF-P-2_, who prefer either helpfulness or honesty (hidden attribute), i.e. they generate preferences using the scores for their chosen attributes. To test the ability of VPL to model more users than has been previously attempted in the literature, we create _UF-P-4_, which uses the fine-grained scores over all the four attributes in the UF dataset  to create a dataset with four different users. Here, the users are divergent because given two responses, users following different objectives can have opposite preferences - some users can prefer the helpful response, while others prefer the honest response. In

Figure 5: Active learning enables personalizing policies to user preferences with fewer queries.

some cases, these responses are at odds. To ensure that successfully modeling the data requires fitting divergent preferences, we filtered out the responses where all users agree or are indecisive to primarily focus on multimodal preference modeling, and remove degenerate context queries that provide no information about the user distribution. However, in UF-P-4 the context can still contain queries where at least two users overlap. Thus, this provides a dataset to evaluate VPL in cases where different users agree on some responses, but not all of them. Finally, to generate the context \(\{(s^{i}_{A},s^{i}_{B},y^{i})\}_{i=1}^{N}\) for inferring latent distributions, for each prompt and response pair, we sample N different data points from a smaller subset of size \(K\) from the dataset (\(K=100\) for GPT2 and \(16\) for Llama2). For a deployed LLM system, this is analogous to having a known set of survey questions from which the user must answer a subset of 2-8 questions to personalize the model's behavior to their needs.

### Does VPL help to make LLM reward models more pluralistically aligned?

In Table 1, we see that VPL is able to learn a more accurate reward model across all the datasets, capturing the multi-modality in the language preference data. This indicates that VPL can infer the latent representation of the user's preferences \(z\) from a few annotated samples, and successfully adapt the reward model. In contrast, the baselines--including the BTL model typically used in widely deployed RLHF  models--are unable to fit the datasets because they do not account for divergent preferences. Because the datasets are imbalanced, the baselines can sometimes perform better than random guessing by fitting only the preferences of the majority group, and thus, the performance on Pets (Full) appears high, even though the baselines fail to adapt to the divergent preferences.

## 8 Conclusion

In this work, we presented VPL, a technique for pluralistic alignment of preference-based RLHF models via variational inference. We show that VPL can capture diverse preferences and be used for steerable personalized model learning while capturing uncertainty in preferences. We discussed practical considerations for scaling VPL for LLMs and policy learning and showed results across simulated control problems and LLM-based RLHF, significantly outperforming current RLHF techniques.

**Limitations and Future Work.** A key limitation of this work is that as yet, realistic preference datasets containing the opinions of diverse users do not yet exist at scale. This limitation necessitated creating our synthetic preference datasets. Although this was also the approach taken in prior work on personalized RLHF (e.g. [58; 73]), an important direction for future work will be to apply VPL to more realistic preference data from diverse groups of users. Further, our current experiments on the UltraFeedback dataset assume that when adapting to a new user, we could ask them for preferences over samples from a fixed set of survey questions. In the future, it would be useful to relax this assumption and apply VPL to preferences obtained naturally during a conversation with the user.

We believe VPL could offer promising safety benefits in addition to modeling diverse user preferences. Uncertainty detection may help prevent jailbreak attacks caused by conflicting rewards . By capturing uncertainty in the distribution of user preferences, VPL could enhance safety by stopping or refusing to respond when uncertainty cannot be sufficiently reduced .

## 9 Acknowledgement

We would like to thank members of the WEIRD and Social RL for the valuable discussions and feedback on the project and manuscript. This project was partly supported by NSF grant no. 2212310.

    &  &  \\   & Pets (Divergent) & Pets (Full) & UF-P-2 & UF-P-4 & UF-P-2 \\  BTL  & 63.27 \(\) 0.57 & 94.92 \(\) 0.00 & 49.84 \(\) 0.14 & 53.48 \(\) 0.03 & 47.17 \\ DPL  & 70.62 \(\) 1.13 & 95 \(\) 0.00 & 49.57 \(\) 0.42 & 52.92 \(\) 0.06 & 49.51 \\ VPL (Ours) & **100 \(\) 0.00** & **100 \(\) 0.00** & **74.75 \(\) 2.01** & **61.49 \(\) 0.03** & **76.41** \\   

Table 1: We compare the accuracy of different reward models trained on the two datasets. We report the mean and standard deviation of performance of GPT2-based models on three seeds, and one seed for Llama models.