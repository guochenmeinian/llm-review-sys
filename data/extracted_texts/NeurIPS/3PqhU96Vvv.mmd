# Flexible Context-Driven Sensory Processing in Dynamical Vision Models

Lakshmi Narasimhan Govindarajan\({}^{1}\)

\({}^{1}\)Department of Brain and Cognitive Sciences, MIT, Cambridge, MA

\({}^{2}\)Department of Electrical Engineering and Computer Science, MIT, Cambridge, MA

\({}^{3}\)McGovern Institute for Brain Research, MIT, Cambridge, MA

\({}^{4}\)K. Lisa Yang Integrative Computational Neuroscience (ICoN), MIT, Cambridge, MA

{lakshmin,abiyer,valmiki,fiete}@mit.edu

Abhiram Iyer\({}^{*}\)

\({}^{1}\)Department of Brain and Cognitive Sciences, MIT, Cambridge, MA

\({}^{2}\)Department of Electrical Engineering and Computer Science, MIT, Cambridge, MA

\({}^{3}\)McGovern Institute for Brain Research, MIT, Cambridge, MA

\({}^{4}\)K. Lisa Yang Integrative Computational Neuroscience (ICoN), MIT, Cambridge, MA

{lakshmin,abiyer,valmiki,fiete}@mit.edu

###### Abstract

Visual representations become progressively more abstract along the cortical hierarchy. These abstract representations define notions like objects and shapes, but at the cost of spatial specificity. By contrast, low-level regions represent spatially local but simple input features. How do spatially non-specific representations of abstract concepts in high-level areas flexibly modulate the low-level sensory representations in appropriate ways to guide context-driven and goal-directed behaviors across a range of tasks? We build a biologically motivated and trainable neural network model of dynamics in the visual pathway, incorporating local, lateral, and feedforward synaptic connections, excitatory and inhibitory neurons, and long-range top-down inputs conceptualized as low-rank modulations of the input-driven sensory responses by high-level areas. We study this **D**yanical **C**ortical **a**t**work (_DCnet_) in a visual cue-delay-search task and show that the model uses its own cue representations to adaptively modulate its perceptual responses to solve the task, outperforming state-of-the-art DNN vision and LLM models. The model's population states over time shed light on the nature of contextual modulatory dynamics, generating predictions for experiments. We fine-tune the same model on classic psychophysics attention tasks, and find that the model closely replicates known reaction time results. This work represents a promising new foundation for understanding and making predictions about perturbations to visual processing in the brain.

## 1 Introduction

We readily use abstract cues to modulate our sensory perception. These include cues to attend to abstract high-level features (find Waldo; count the number of hoop shots, etc.) or low-level features (find the red items, vertically oriented bars, etc.). Such cue-based modulations of the visual pathway allow us to locate items of interest more rapidly and accurately, and to perform goal-directed computations.

Understanding how top-down modulatory cues are represented and then interact with bottom-up sensory-driven neural responses has been a longstanding goal in computational cognitive neuroscience . Extensive psychophysical experiments  and studies showing that individual neurons whose receptive fields are aligned with the attentional cue exhibit a gain modulation  shed lighton the phenomenon. However, a fundamental circuit-level and conceptual problem remains open: what is the nature of the "_modulatory homunculus_" that knows, given various goals and context cues, which low-level representations to modulate, in which combination and which topographic part of the representational space in different processing layers? We lack a cohesive computational framework to link these two levels of representation. Simultaneously, we lack models of sensory processing that fully take into account the recurrent and temporally unfolding nature of computation in the brain; thus, they fall short of explaining phenomena like reaction time variations with task difficulty and the sharpening of perception as information is integrated within a trial.

We combine known architectures of visual cortex with advances in machine learning to introduce a biophysically-inspired model, the **D**ynamical **C**ortical **n**etwork (_DCnet_), to solve cue-delay-visual search tasks (Figure. 1). The model is endowed with several relevant properties of biological circuits, including separate (tuned) excitatory and (weakly-tuned) inhibitory populations, lateral inhibition (intra-area recurrence), and neuron types with distinct learnable time constants. We operationalize the "modulatory homunculus" as multiplicative low-rank perturbations from higher-order cortical and thalamic areas. Specifically, these low-rank perturbations arise from the model's own sensory responses from earlier times within the trial.

**Contributions.** In this work, we focus on analyzing and interpreting the internal dynamics and behavioral modes of our biophysically inspired model on a suite of visual cue-mediated tasks.

* We introduce _vis-count_, a novel, challenging, and parametrically generated cue-delay-visual search task. A visual cue (consisting of a color, a shape, or a color-shape conjunction) specifies which objects in a subsequently presented scene of geometric objects to count.
* We present a biologically realistic model of the brain's visual system, the **D**ynamical **C**ortical **n**etwork (_DCnet_), that is relatively shallow with local and top-down feedback and separate E/I neurons, which is capable of top-down attentional modulation based on previously presented cues and processes information over time. Our framework is among the first to link levels of analyses from physiology to behavior via computations and is a necessary first step toward hypothesis generation for neuroscience.
* Our model outperforms state-of-the-art standard DNNs and LLMs on the task, while being interpretable and having orders of magnitude fewer parameters.
* We perform _in silico_ electrophysiology of the circuit's population dynamics to show that cue-based modulation drives a divergence of the bottom-up responses from the uncued case.
* We can fine-tune the same model on new stimulus sets corresponding to classic human psychophysics attention tasks. Reaction time analogues in our model closely replicate experimental observations.

In sum, these contributions suggest that our approach is a promising framework for modeling the brain's visual processing dynamics, one that replicates many key attentional phenomena and that can generate testable hypotheses and predictions about circuit mechanisms.

## 2 Related Work

**Stimulus computable models of visual cognition.** Modeling top-down contextual effects on bottom-up sensory processing during visual search is of longstanding interest in the vision sciences community. There are a large number of phenomenological models of visual search and associated reaction time findings [8; 9; 10; 11; 12], but only recently have models been able to operate directly on high dimensional sensory inputs [13; 14; 15; 16]. The most promising approach involves augmenting pre-attentive ventral stream models with controllers either via attention maps  or multiplicative modulation factors on network activities . However, these models cannot be used to faithfully study sensory processing dynamics because they are purely feedforward. Recurrent models of early sensory processing with lateral feedback and distinct excitatory and inhibitory populations [18; 19; 20], and top-down feedback  have shown promise in accounting for contextual visual computations and human reaction times on visual cognitive tasks. However, to our knowledge, stimulus-computable recurrent vision models compatible with _cue-delay-target_ paradigms do not exist. More generally, models have tended to either be stimulus-computable or grounded in biological realism, but usually not both.

**Low-rank interactions.** Neuronal networks in the brain can be involved in disparate computations simultaneously . Computational neuroscience research has focused on understanding how such neural networks can represent task-specific information and, relatedly, its consequence on the system's overall behavior. Theoretical results suggest that the coexistence of structured low-rank connectivity and random connectivity in networks can enable multi-task computations and expand the dynamic range of the network's functional capability [23; 24; 25; 26; 27]. An alternate but non-orthogonal viewpoint is that low-rank control inputs from an external system can switch a network's input-output mappings context-dependently [28; 29; 30]. Most of the computational work in this regard is, however, rooted in the motor domain, with Schmid and Neumann being a recent exception.

**Neurophysiology of attentional control.** Biological attention upmodulates task-relevant information by filtering sensory responses using "templates". Empirical work shows that attentional templates are primarily represented in higher order cortical areas [31; 32; 33; 34] and that such templates can be learned rapidly on a per-task basis . Moreover, the nature of such attentional filtering is known to be cell-type and layer-specific , with theories emphasizing the role of top-down mediation of specific GABAergic interneurons . In addition to cortical feedback inputs, higher-order thalamic nuclei are also known to convey contextual inputs to sensory regions either via direct projections [38; 39] or indirectly through the frontal cortices [40; 41]. Building biological sensory processing models that account for all these disparate findings is of primary importance to the Neuroscience community.

## 3 General methods

### _DCnet_ Model and training details

_DCnet_ comprises two core components: a biologically motivated sensory perception stream and a higher-order area that interacts bidirectionally with it (Figure. 1). Each of the four sensory areas in our model are organized retinotopically as hypercolumns consisting of distinct excitatory and inhibitory neural subpopulations that obey Dale's law . Excitatory pyramidal neurons receive bottom-up and recurrent lateral excitatory inputs as well as short-range lateral inhibitory inputs from interneurons in the same area. Interneurons receive bottom-up and lateral excitatory inputs. Finally, pyramidal neurons project in a feedforward manner to their downstream area. The ratio between excitatory and inhibitory neurons is \(4:1\), as observed in cortical areas .

Figure 1: **Low-rank modulations to drive context-aware processing.** We present a biologically motivated, end-to-end trainable network model of dynamics in the visual pathway. Layers in the model are parameterized by recurrent (tuned) Excitatory (E) and (weakly tuned) Inhibitory (I) neural populations that interact bidirectionally with a higher-order layer in a low-rank manner. The low-rank modulatory factors serve to extract abstract context cues from sensory responses for subsequently driving neural dynamics into context-appropriate dynamical regimes.

The higher-order layer receives pyramidal input from each area and, in turn, modulates inter-area projections in a low-rank manner operationalized as follows.

\[_{1};_{2} =(_{t}^{l})_{l,1}^{T}+_{1};( _{t}^{l})_{l,2}^{T}+_{2} \] \[_{t}^{l} =(_{t}^{l})[_{1}_{2}] \] \[_{t+}^{l} =_{t+-}^{l}_{t}^{l} \]

Here, \(_{t}^{l}^{C H W}\) is the excitatory population readout of area \(l[1..4]\) at time \(t\); \((.)\) is the spatial average pooling operator; \(\) denotes outer product and \(\) denotes pointwise scaling.

Neurons have learnable cell-type specific time constants. We dispense of traditional ML operations such as BatchNorm or LayerNorm, designed to impart stability during training. Our model has \( 1.8\)M parameters that we learn via gradient descent on a task-performance objective. A full mathematical specification of the model is provided in Appendix. A.1.

### Baselines

We instantiate baseline models of two varieties. First, we consider a "Convolutional RNN" model (\( 8.8\)M params) that uses a traditional 6-Layer convolutional backbone feeding into a gated recurrent unit (GRU)  with \(N=2048\) neurons (Appendix. B). We construct this baseline to evaluate the benefits of an explicit modulatory mechanism such as the higher-order layer in our model. Second, we consider four standard deep feedforward neural network architectures. As these models do not operate on spatiotemporal inputs, we condense trials to a single time point by stacking cues and scenes together. Given the lack of an explicit cue followed by scene phase for these models, we term them _implicit_. Cues and scenes are presented at the same time. These baselines provide an upper bound on the expected performance of _DCNet_ and verify that our chosen task is a non-trivial computational challenge. In our experiments, we consider the following implicit models: a 6-Layer CNN (\( 2.8\)M params); ResNet-18  (\( 11.5\)M params); ViT-B/16  (\( 86\)M params), a vision transformer with a patch size of \(16\)px; Swin-T  (\( 28\)M params), a hierarchical vision transformer. Furthermore, we also ran experiments on the zero-shot generalization abilities of an LLM (Appendix. C).

Figure 2: **Explicit context-guided modulation of sensory dynamics is necessary for learning generalizable solutions.****a.** We introduce _vis-count_, a parametric, visually-cued, delayed search task. On each trial, models are cued with a visual attribute (either a color, shape, or a conjunction of the two), and after a delay, a scene is presented. The task is to count and report the number of cue-consistent objects in the scene. **b.** On each of the three types of trails, we find that our model consistently outperforms state-of-the-art standard DNNs (Section. 3.2; Baselines) on novel held-out scenes. _Implicit_ models refer to the condition where cues and scenes are presented simultaneously. The red dashed line denotes chance performance. **c.** A harder test of generalization on novel scenes _and_ cues reveal that our model is robust to such variations, unlike performant implicit models.

## 4 vis-count: A parametric cued visual search task

### Task and stimuli

We draw inspiration from Clevr , a synthetic dataset for language-mediated visual reasoning and construct _vis-count_, a parametric visually-cued, delayed search task. On each trial, we challenge models to count and report the number of geometric objects in a visual scene with features consistent with a cue provided earlier (Figure. 2a). Cues can be simple colors, geometric shapes, or a conjunction of the two. Scenes comprised \(3-10\) geometric 3D shapes of varying sizes, colors, and material properties. By construction, there can be \(0-5\) cue-consistent objects in the scene. We counterbalanced the dataset so that the distribution of target counts was uniform across trial types. Cues and scenes were rendered at \(320 240\)px and resized to \(128 128\)px for model training and evaluation. In total, our training (validation) dataset comprised of \( 384\)K (\(38\)K) trials. As with Clevr, we detect and discard scenes with fully occluded objects.

For _DCnet_ and the Convolutional RNN baseline, cues and scenes are visual inputs provided to Layer 1 of these models. Cues are persistently provided to the network for the first \(T\) discrete time steps followed by scenes for the next \(T\) time steps. The output activities of the last layer at the last time step (\(t=2T\)) are transformed into logits, and a supervision signal is provided via a cross-entropy loss, with ground truth labels counted from 0 to 5. For the "implicit" baseline models, cues and scenes are _stacked_ together and presented simultaneously.

### Results

We report the results of our model evaluations and comparisons to baselines in Figure. 2b-c. Our model consistently and significantly outperforms state-of-the-art DNN vision models when evaluated on trials with novel held-out scenes, achieving \(99\%\), \(95\%\), \(73\%\) accuracy on the color, shape, and conjunction trials, respectively. Additionally, we perform a harder generalization test by synthesizing trials with novel cues _and_ novel scenes (e.g., we test our model on blue and green colors, cube

Figure 3: **Neural network dynamics reveal context-dependent behavior. We perform dimensionality reduction on late layer model activities and visualize neural trajectories (\(\) for individual trials; Dark-colored for trial averages) for different experimental manipulations. The solid dot indicates the trial start. **a.** For a fixed cue (color trials visualized here), network dynamics reflect the extraction and preservation of task-relevant information (object counts) while being invariant to task-irrelevant bottom-up responses from the different scenes. **b.** Matched cued vs. uncued trials for the same scene (inset) reveal a divergence of the bottom-up responses driven by the low-rank modulations.

shapes, and conjunctions thereof, all unseen in the training set). Our model demonstrated the best generalization performance (\(55\%\)) while even the most performant baseline models (with orders of magnitude more parameters) dropped to chance (16.67%). Sample model results are visualized in Appendix. A.5.

Model errors, too, can be particularly enlightening. Consider the last example in Figure. A.5. When cued to find "blue cylinders", the model mistakenly appears to include either the cyan cylinder or the occluded blue cube in its count, either of which would reflect an _illusory conjunction_. Illustrory conjunctions arise due to failures of feature-based attention and have been extensively studied in the human cognition literature. The presence of such pathologies in our model exposes an opportunity to gain potential insights into the neural implementations of feature-based attention.

In the following sections, we report analyses of our model's learned dynamics and internal representations that support contextual guidance.

_What to modulate?_**Low-rank structures support optimal cue representations.** In daily life, "cues" are not specified to us as abstract rules but rather as high-dimensional sensory inputs. How do we construct a representational space where abstract rules (derived from sensory responses) and the sensory responses themselves are distinct yet coexist? Following insights from electrophysiology  and prior theoretical work , we hypothesized that learning to inject derived cue information back into sensory representations as low-rank perturbations will aid in optimally partitioning the sensory representational space.

_DCnet_'s task performance indicated that it had indeed learned task-optimal representations. Here, we perform dimensionality reduction on the activities of pyramidal neurons in the last layer of _DCnet_ to probe how this optimality emerges.

First, we observe that cued vs. uncued dynamics on individual trials become progressively divergent and nearly orthogonal with time (Figure. 3b). This indicates that the bidirectional interactions likely promote the formation of low-dimensional activity subspaces embedded within the higher-dimensional ambient activity space. Second, we see that the bottom-up responses to the same set of scenes are differentially modulated to appropriate target subspaces based on the cue (Figure. 3a). We believe that this happens through the inactivation of context-irrelevant subspaces, resulting in invariance to current task-irrelevant features. As additional consideration, we note that training _DCnet_ without this top-down feedback mechanism brings performance down to \(38\%\), highlighting the importance of this interaction in our framework.

_Where to modulate?_**A cortical gradient for feature selectivity.** After extracting abstract context rules from sensory responses, a question remains: At what level of the representational hierarchy must the perturbation be applied? We perform lesion analysis on the trained _DCnet_ to probe this question (Figure. 4).

We sequentially "turn off" modulations, area by area, and observe their detrimental effects on overall function as determined by task performance. We implement this by setting the modulating factors to

Figure 4: **A selectivity gradient emerges across the cortical layers for the different visual cues.** We sequentially lesion the modulatory synapses in a trained _DCnet_ from the higher-order layer to each sensory area, starting from the top-most (D) to the bottom-most (A) sensory area, while documenting task performance on each trial type. (E) indicates the intact _DCnet_. While late lesions have a minimal effect on performance in color trials, the opposite is true for shape trials. Early areas in the _DCnet_ exhibit color selectivity, while later areas exhibit shape selectivity.

**1**. We hypothesized that if the modulation of pyramidal neuron activity in a given area is essential for the system's function, then lesioning this perturbation will result in a maximal drop in performance. Interestingly, this analysis yielded differential results per trial type (Figure. 4). For the color trials, we find that lesioning modulations in the early areas had the largest impact on performance, while for the shape trials, it was the late modulations that proved critical. In contrast, modulation strength seemed relatively uniform across the sensory areas.

Taken together, these results offer two insights. First, opposing cortical gradients for color and shape selectivity emerge and coexist in the sensory areas despite the model only being supervised to "count". Second, leveraging this selectivity gradient, the higher-order area learns to apply appropriate low-rank modulations. We believe that the low-rank nature of the context-based modulation, not only within but also across areas, is fundamental to generalization.

**Excitation-Inhibition Dynamics.** The amplification of context-relevant sensory representations must be balanced to support stable dynamics . The neural underpinnings of this balance and its computational role in feature gain modulation have previously only been studied phenomenologically. Here, we leverage _DCnet_ to investigate how the circuit-level properties discovered through optimization can support overall network function.

We probe the cell-type specific neural dynamics in _DCnet_ by driving network activity with uncorrelated, time-varying Gaussian noise inputs. First, despite fewer interneurons in the model, inhibitory interactions play a crucial role in imparting stability (Figure. 5b) and expanding the dynamic range of computations expressed by the network when compared to a version of _DCnet_ trained without inhibition (Figure. 5c). Second, we detect the presence of co-tuned excitation and inhibition (Appendix. A.4). Empirically, co-tuning is known to be a common organizational principle across the sensory areas. These findings imply the critical role of interneurons in cortical amplification dynamics underlying context-dependent computations. Finally, we observe that neurons learn to integrate information faster in early vs. late areas, revealing the emergence of a macroscopic gradient in neural timescales (Figure. 5a, 8).

## 5 Model psychophysics on parametric cued feature searches

A feature search is a variant of the general visual search problem in which a "target" is defined by a single discriminative feature . Parametric variations along (or orthogonal to) this discriminative feature axis help shed light on the mechanisms of top-down contextual guidance and its impact on behavior. Here, we consider two feature search tasks from the human psychophysics literature.

Figure 5: _In silico_**electrophysiology sheds light on tuning properties and excitation-inhibition dynamics. We drive network activity in _DCnet_ with uncorrelated, time-varying Gaussian noise inputs. (a) We depict a randomly chosen neuron in early and late sensory areas of _DCnet_. Each E/I neuron pair shown here has matched receptive fields. Learned time constants reflect fast/slow integration in early/late areas, revealing a macroscopic gradient. By contrasting _DCnet_ trained with and without inhibition, we find that inhibition plays a crucial role in (b) imparting stability and (c) expanding the dynamic range of computations in the network. More quantitative details are presented in Appendix. A.4.

**General methods.** To perform model psychophysics, we start from the _DCnet_ model trained on _vis-count_. We replace its readout to do binary classification (target present/absent) and fine-tune the model on each task below. Furthermore, to derive a measure of model "reaction time," we compute the entropy of _DCnet_'s decision outputs after evolving dynamics for the duration of each trial.

### The role of distractor heterogeneity

**Task and stimuli.** A target feature's bottom-up salience (pop-out) does not survive variations in irrelevant distractor features. (Figure. 6b) . To test this, we construct cued-feature search trails where models search for an L/T (target) in a grid with a fixed number of Ts/Ls respectively (scene). Targets and scenes were rendered on a \(128 128\)px canvas. Targets (L/T) were presented at the center and oriented uniformly at random between \([0,2)\). The distractor heterogeneity level (\(\)) for every trial was uniform random between \(\). Consequently, distractor orientations in the scene were uniform random between \([-,+]\) with \([0,2)\) being the mean distractor orientation. The target was present in \(50\%\) of the trails. Our training (test) dataset comprised of \(32\)K (\(8\)K) trials.

**Results.** When fine tuned on this task, _DCnet_ achieved an overall accuracy of \(97\%\) on the test trials. Moreover, faithful to the human psychophysics results, we find that the entropy of _DCnet_ outputs is significantly different for low (\(<=0.5\)) vs. high (\(>0.5\)) distractor heterogeneity for both target-present (Mann-Whitney \(U=302947.,p<.001\)) and target-absent trials (Mann-Whitney \(U=301376.,p<.001\)). While target-absent trials yield higher reaction times in humans when compared to target-present trials, we don't see this in our model's output entropy. This possibly reflects an imperfect choice for a model reaction time measure, an aspect of our work that we look to extend upon in future work.

### The role of target-distractor feature differences in the presence of distractors

**Task and stimuli.** In a classic critical color difference task, human participants were required to detect a target that differed from distracting stimuli only in color . Search times were measured for varying color differences as a function of display density (the number of distractors). We parametrically synthesize stimuli to recreate this task (Figure. 7a-b).

Target and distractor stimuli were chosen to be circular discs of radii \(10\)px. Cues were rendered at the center of a \(128 128\)px canvas at a target color chosen at random from among four perceptually uniform color spaces. Search scenes consisted of \(1-7\) distractor stimuli whose color differed from the target color at one of \(10\) preset target-distractor difference levels chosen uniformly at random. The target was present in \(50\%\) of the trails. In total, our training (test) dataset comprised of \(32\)K (\(8\)K) trials.

Figure 6: **Slower reaction times/larger output entropy with more heterogeneous distractors.****a.** Task description. **b.** We parametrically vary the heterogeneity in distractor orientation (similar to ). **c.** In both target-present and target-absent trials, we find that the _DCnet_’s output entropy is significantly different between low distractor heterogeneity vs. high distractor heterogeneity trials. Error bars represent the standard error of the mean.

**Results.** When fine tuned on this task, _DCnet_ achieved an overall accuracy of \(95\%\) on the test trials. Moreover, _DCnet_ recapitulates two key findings from the psychophysics literature.

We probe _DCnet_'s output entropy (a proxy for model RT) as a function of the number of distractors in the scene. We find that for trials in which the target-distractor color difference was high, model entropy was invariant to the number of distracting stimuli (Pearson's \(r=-0.11\)). On trials where the target-distractor color difference was low, model entropy increased linearly with the number of distracting stimuli in the scene (Pearson's \(r=0.55\)). We find a similar result when we probe _DCnet_'s output entropy as a function of the target-distractor feature difference. These results faithfully replicate behavioral effects reported in prior literature .

As we alluded to in Section 5.1, our reaction time (RT) metric is one among many possible choices. To take a step further, we implement and test another RT metric inspired by evidential learning (EDL) theory as proposed in . We verify that our primary conclusions from this experiment hold even in the context of this new metric. Finetuned on the EDL objective, _DCnet_ achieves \(86\%\) accuracy on this task. Model RTs, computed as a time-averaged uncertainty measure, increased linearly with the number of distractors in the low target-distractor color difference trials (Pearson's \(r=0.92\)) and was invariant to the number of distractors on high target-distractor color difference trials (Pearson's \(r=0.2\)).

## 6 Discussion

The advent of highly-performant ventral stream models of visual perception is rapidly revolutionizing visual neuroscience research. Stimulus computable models operating directly on high dimensional sensory inputs yield benefits as hypothesis generators for scientific discovery. . However, there exist two fundamental axes of dissonance, which are potential rate limitors. First, the emphasis on building "bigger and better" models promotes a divergence from biological realism . Second, it is evident that the extent of biological visual capabilities spans a space bigger than one that entails only feedforward perceptual modules . It calls for accounting for the cooperative dynamics between perceptual and cognitive processes .

In this work, we aim to bridge this gap by introducing a computational framework that emphasizes biological realism and conceptualizes interactions between perceptual dynamics and abstract cognitive demands while being scalable and stimulus-computable. We present the **D**ynamical **C**ortical **n**et**work (_DCnet_): a trainable neural network model of visual dynamics incorporating local, lateral, and feedforward synaptic connections, excitatory and inhibitory neurons, and long-range top-down inputs conceptualized as low-rank modulations of the input-driven sensory responses by high-level areas.

Figure 7: **Differential reaction times/output entropy predictions for varying levels of target-distractor differences.****a.** Task description. **b.** We parametrically vary both the number of distracting stimuli in the scene and the target-distractor color difference ). **c.**_DCnet_’s output entropy is invariant (linear) to the number of distracting stimuli when the feature difference is high (low), capturing the psychophysical phenomenon of “popout.” **d.** Model entropy as a function of target-distractor feature difference when marginalized over the number of distracting stimuli. Error bars represent the standard error of the mean.

We start by studying the ability and behavior of \(DCnet\) to operate in a visually-cued search paradigm. \(DCnet\) not only outperforms state-of-the-art DNN models, but its population states over time reveal the computational structure and neural underpinnings of contextual modulatory dynamics, generating predictions for experiments. Furthermore, we fine-tune the same model to perform two classic 2AFC attention psychophysics tasks. Reaction time analogs from \(DCnet\) strikingly recapitulate core tenets of feature-based attentional modulation. We find that \(DCnet\) responses are sensitive to target-distractor feature differences, heterogeneity of irrelevant distractor features, and display density.

Overall, these contributions suggest that our approach is a promising framework for modeling the brain's visual cortical dynamics, one that replicates key neural and behavioral signatures of contextual attentional modulation.

**Limitations and future directions.** In this work, we take the first step towards building an overcomplete model of cortical circuitry. Palpable omissions from our current framework include long-range inter-area feedback and compartmental separation of feedforward and feedback inputs. We hope to continue building on this framework in the future to include these components that will open up novel avenues for computational neuroscience. Additionally, extracting reaction time measures from large-scale recurrent models is a discipline of its own . We adopt a simple reaction time metric here that suffices for our current purpose, but we plan to include other sophisticated comparisons in future work. Lastly, we have restricted our purview to visual sensory processing. The concept of contextual modulation, however, is pervasive across sensory modalities. We are excited about extending our ideas to other sensory domains.

**Broader impact.** Artificially intelligent models with enhanced visual capabilities are now pervasive in our daily lives. The not-so-hidden cost we pay to enjoy these models' benefits is their carbon footprint on our environment. Building energy-, parameter-, and sample-efficient models that are also performant is non-negotiable going forward. Understanding context-aware behavior is of utmost importance for neuroscience research as its failure modes are associated with several psychiatric and neuropathologies. We do not anticipate any negative impact that our work would create.