# Neuc-MDS: Non-Euclidean Multidimensional Scaling Through Bilinear Forms

Chengyuan Deng, Jie Gao, Kevin Lu, Feng Luo, Hongbin Sun, Cheng Xin

Rutgers University. {cd751,jg1555,kll160,fluo,hongbin.sun,cx122}@rutgers.edu

###### Abstract

We introduce **N**on-**E**uclidean-**M**DS** (Neuc-MDS), an extension of classical Multidimensional Scaling (MDS) that accommodates non-Euclidean and non-metric inputs. The main idea is to generalize the standard inner product to symmetric bilinear forms to utilize the negative eigenvalues of dissimilarity Gram matrices. Neuc-MDS efficiently optimizes the choice of (both positive and negative) eigenvalues of the dissimilarity Gram matrix to reduce STRESS, the sum of squared pairwise error. We provide an in-depth error analysis and proofs of the optimality in minimizing lower bounds of STRESS. We demonstrate Neuc-MDS's ability to address limitations of classical MDS raised by prior research, and test it on various synthetic and real-world datasets in comparison with both linear and non-linear dimension reduction methods.

## 1 Introduction

Many datasets in applications adopt dissimilarities that are non-Euclidean and/or non-metric. Examples of such popular dissimilarity measures  include Minkowski distance (\(L_{p}\)), cosine similarity, Hamming, Jaccard, Mahalanobis, Chebyshev, and KL-divergence. Studies in psychology have long recognized that human perception of similarity is not a metric . Further, dissimilarity matrices with negative entries (e.g.: cosine similarity, correlation, signed distance) have also been widely used in various problems. Negative inner product norms also have deep connections to hyperbolic spaces as well as the study of spacetime in special relativity theory.

In many machine learning practices, embedding in low dimensional vector space is often explicitly or implicitly done within the data processing pipeline. Such embedding may already need to consider more general dissimilarities. For example, one branch of graph learning adopts embedding in non-Euclidean spaces (e.g., hyperbolic spaces ), and machine learning for physics model and data (AI4Sicence) needs to consider more general inner product norms . In transformer models , the attention mechanism can also be viewed as learning a general bilinear form on tokens. Despite the wide adoption of general dissimilarity measures in practice, theoretical study of embedding and dimension reduction for non-Euclidean non-metric data appears to be still very limited.

In this paper we consider one of the most classical algorithms for data embedding and dimension reduction -- multidimensional scaling - and develop a non-Euclidean, non-metric version with theoretical performance guarantee.

**Background on MDS.** Classical multidimensional scaling (cMDS) takes as input a _Euclidean distance matrix (EDM)_, i.e., a symmetric matrix \(D^{n n}\) where each entry is the squared Euclidean distance between two points in Euclidean space, and recovers the Euclidean coordinates. Using a standard double centering trick, \(D\) can be turned into aGram matrix \(B=X^{T}X\) where \(X\) encodes the Euclidean coordinates. For the purpose of producing a low-dimensional vector, classical MDS takes \(k\) eigenvectors corresponding to top \(k\) largest eigenvalues of the Gram matrix \(B\). This minimizes the _strain_, the difference (Frobenius norm) in terms of the Gram matrix.

When the input distance matrix is not a Euclidean distance matrix, this problem is called metric MDS . Metric MDS considers the minimization of _STRESS_, defined as the sum of squared difference of pairwise embedding distances to the input dissimilarities. Minimizing STRESS makes the problem to be non-linear and there is no closed-form solution, though one can use either gradient descent or Newton's method . Nevertheless, in practice, cMDS is often applied for non-Euclidean distance matrix. In this case, the centered matrix \(B\) is no longer positive semi-definite. The common practice is keep the top positive eigenvalues and throw away the negative eigenvalues.

Two recent papers  pointed out that classical MDS produces suboptimal solutions on non-Euclidean distance matrix when considering STRESS. This is not a surprise, since cMDS, minimizing strain, does not minimize STRESS. However, a more problematic issue is that when using more dimensions in cMDS (i.e., increasing \(k\)), the STRESS error first drops and then increases. We call this phenomena _Dimensionality Paradox_. It is theoretically unsatisfactory and counter-intuitive that embeddings by classical MDS using more dimensions could yield worse results.

The error analysis in  sheds light on this issue. When the input matrix is Non-Euclidean, the negative eigenvalues carry crucial information. cMDS, keeping only positive eigenvalues, is intrinsically biased - the more positive eigenvalues used the more it deviates from the input data. A real eradication of this issue must address the root cause, i.e., applying an algorithm meant for Euclidean geometry on non-Euclidean data.

**Our Contributions.** We extend multidimensional scaling to non-Euclidean geometry, by generalizing the dissimilarity function from the standard inner product (which defines Euclidean geometry) to the broader family of symmetric bilinear forms \((u,v)=u^{T}Av\), where the symmetric matrix \(A\) does not have to be positive semi-definite. For dimension reduction, we look for both a low dimensional vector representation and an associated bilinear form, that together approximate the input dissimilarity matrix with minimum STRESS error. Specifically, the key contributions are as follows:

* We conduct an in-depth analysis on STRESS error for any chosen subset of \(k<n\) eigenvalues of the input centered dissimilarity matrix. We propose Neuc-MDS, an efficient algorithm that finds the best subset of eigenvalues to minimize a lower bound of STRESS.
* Beyond the constraints of eigenvalue subsets, we extend our findings to the general linear combinations of eigenvalues. Our advanced algorithm, Neuc-MDS\({}^{+}\), finds the best linear combination of eigenvalues to minimize the lower bound objective.
* on completely unstructured data aggressive dimension reduction shall not be expected. Further, when \(k=(n)\), the STRESS of Neuc-MDS monotonically decreases to 0 while the STRESS of cMDS increases and eventually reaches a plateau.
* Empirically we evaluate Neuc-MDS and Neuc-MDS\({}^{+}\) on ten diverse datasets encompassing different domains. The experiment results show that both methods substantially outperform previous baselines on STRESS and average distortion, and fully resolve the issue of _dimensionality paradox_ in cMDS. Our codes are available on Github2. 
## 2 Related Work

Our work is in the general family of similarity learning  with dimension reduction, going beyond metric learning and embedding. Due to the huge amount of literature on this topic we only mention those that are most relevant.

MDS Family and Embedding in Euclidean Spaces.As one of the most useful embedding and dimension reduction techniques in practice, the MDS family has many variants. Non-metric MDS  considers a monotonically increasing function \(f\) on input dissimilarity and minimizes STRESS between \(\{f(D_{ij})\}\) and embedded squared Euclidean distances. Generalized multidimensional scaling (GMD) considers the target space as an arbitrary smooth surface . In addition, non-linear dimension reduction methods such as Isomap , Laplacian Eigenmaps , ILE , t-SNE  consider data points from a non-linear high-dimensional manifold and extract distances defined _on_ the manifold. In all these methods, the points are still embedded in Euclidean spaces. Some of these methods such as Isomap directly apply cMDS as the final step. If the dissimilarity is highly non-Euclidean, we can replace cMDS by Neuc-MDS to get performance improvement.

**Dimension Reduction in Non-Euclidean Spaces.** There is also prior work that finds embedding of manifold data in non-Euclidean spaces, e.g., on a piece-wise connected manifold , on a sphere , and in hyperbolic spaces . Very recently, there is study of Johnson-Lindenstrauss style dimension reduction for weighted Euclidean space , hyperbolic space , as well as PCA , dimension-reduction , and t-SNE in hyperbolic space . Our dissimilarity function generalizes beyond hyperbolic distances.

## 3 Dimension Reduction with Bilinear Forms

Let \(P\) denote a dataset of size \(n\). Let \(D^{n n}\) be the dissimilarity matrix of dataset \(P\), \(D_{ij}=D_{ji}\) is a real-valued symmetric dissimilarity measure between pair \(p_{i},p_{j} P\), \(i j[n]\), and all diagonal entries \(D_{ii}=0\) (i.e., \(D\) is a hollow matrix). \(D\) is the analog of squared Euclidean distance matrix in classical MDS. But here \(D\) is not necessarily Euclidean, may not be a metric (e.g. violating triangle inequality) and may have negative entries. Our goal is to obtain (1) a low-dimensional vector representation for each element in \(P\), and (2) a function \(f\) that computes a dissimilarity measure using the calculated low dimensional vectors. Since the input dissimilarities are not necessarily Euclidean nor a metric, we look for the function \(f\) beyond Euclidean distances, but stay within a broader family of inner products or bilinear forms.

A bilinear form \(\) on a vector space \(V\) is a function \(:V V\) which is linear in each variable when the other variable is fixed. More precisely, \((au+v,w)=a(u,w)+(v,w)\) and \((w,au+v)=a(w,u)+(w,v)\) for all \(u,v,w V\) and any scalar \(a\). We only consider symmetric bilinear forms \(\), i.e., \((u,v)=(v,u)\). A bilinear form is positive definite (or positive semi-definite) if \((u,u)>0\) for \( u 0\) (or \((u,u) 0\)). Symmetric matrices and symmetric bilinear forms are two sides of a coin. Namely, fix a basis \(W=\{w_{1},...,w_{n}\}\) of the vector space, there is a one-to-one correspondence between them. That is, \(A=[(w_{i},w_{j})]_{n n}\) is a symmetric matrix. Conversely, give a symmetric matrix \(A\), one defines a symmetric bilinear form \((u,v)=u_{W}^{T}Av_{W}\) where \(u_{W}\) is the coordinate of vector \(u\) in the basis \(W\).

Formally, we have the following problem definition.

**Definition 1** (Non-Euclidean Dimension Reduction).: _Given a symmetric dissimilarity matrix \(D\) of a dataset \(P\) of size \(n\) and a natural number \(k n\), find a collection of \(n\)\(k\)-dimensional vectors \(=(_{1},,_{n}:_{i}^{k})\) with a bilinear form \(f:^{k}^{k}\), \(f(u,v)=u^{T}Av\), such that the STRESS error \(||-D||_{F}^{2}\) for the dissimilarity matrix \(\) of \(\) given by \(_{ij}=f(_{i},_{j})\) is minimized._

### MDS in the Lens of Bilinear Forms

A special case of a symmetric bilinear form is the standard inner product \(,\) on \(^{n}\). Indeed the inner product \( u,v=u^{T}v\) is a symmetric positive definite bilinear form. The inner product of \(u-v\) and \(u-v\) is precisely the _squared_ Euclidean distance \(||u-v||^{2}\). The Euclidean space is \(^{n}\) equipped with the standard inner product. Thus metric geometry of Euclidean space is governed by the inner product. On the other hand, a basic theorem of linear algebra states that a finite dimensional vector space equipped with a symmetric _positive definite_ bilinear form is _isometric_ to the Euclidean space of the same dimension.

Thus geometry of a positive definite symmetric bilinear form, or symmetric positive definite matrices, is nothing but Euclidean geometry.

When \(D\) represents the inner products of pairwise differences (i.e., squared Euclidean distances) of \(n\) points \(P\) in \(^{d}\), \(D\) is called a _Euclidean distance matrix (EDM)_. It can be shown that by taking \(B=-CDC\), where \(C=I-_{n}_{n}^{T}\) is the centering matrix and \(_{n}\) is a vector of ones, one obtains the Gram matrix \(B=X^{T}X\) where \(X\) is \(d n\) dimensional matrix of the \(n\) coordinates of dimension \(d\). The matrix \(B\) is a symmetric positive semi-definite matrix. Thus using eigendecomposition of \(B\) one can recover the coordinates \(X\). This procedure is _classical multidimensional scaling (cMDS)_. Furthermore, if one would like to use \(k\)-dimension coordinates with \(k<d\), classical MDS suggests to take the eigenvectors corresponding to the \(k\) largest positive eigenvalues of the Gram matrix \(B\). This minimizes the _strain_, the difference (Frobenius norm \(\|\|_{F}\)) in terms of the Gram matrix.

\[X_{cds}}*{arg\,min}_{X^{n  k}}\|X^{T}X-()\|_{F}^{2}.\] (1)

Now consider a general symmetric dissimilarity matrix \(D\) of size \(n n\), it naturally associates \(^{n}\) with a symmetric bilinear form \(\). In many real-world situations, the dissimilarity matrix is not positive, nor negative definite, i.e., the bilinear form is indefinite. This means the intrinsic geometry \((^{n},)\) is non-Euclidean. The relationship between the Gram matrix and square distance matrix still holds for indefinite bilinear forms. See Appendix A

In practice, classical MDS is often the default choice even when the input dissimilarity matrix \(D\) is not an EDM, i.e., the centered matrix \(B=-CDC\) is not positive semi-definite. Classical MDS, which simply drops negative eigenvalues of \(B\) to produce positive semi-definiteness does not respect the geometry well. Indeed, multiple researchers have observed suboptimal and counter-intuitive performance . For example, increasing \(k\) may lead to increased STRESS error - keeping more dimensions makes the approximation worse! On a second thought, such results are not surprising. If the input data does not carry Euclidean geometry, forcing it through a procedure for Euclidean geometry is fundamentally problematic.

Our main observation is that a vector space with an indefinite symmetric bilinear form has its own intrinsic geometry. This geometry, even though may not be metrical, carries the most accurate information about the dissimilarity matrix and the datasets. Suppose the centered matrix \(B=-CDC\) has \(p>0\) positive eigenvalues and \(q>0\) negative eigenvalues. The associated indefinite bilinear form \(\) has signature \((p,q)\). One such example is

\[(u,v)=_{i=1}^{p}u_{i}v_{i}-_{i=p+1}^{p+q}u_{i}v_{i}.\] (2)

The geometry of a finite dimensional vector space with a symmetric bilinear form of signature \((p,q)\) where \(p,q>0\) is much less developed compared to Euclidean geometry. When \(q=1\), this is the Minkowski geometry and is closely related to relativity theory in physics and hyperbolic geometry  in mathematics. For general \((p,q)\), one should probably abandon the notion of distance for indefinite spaces. Namely, the expression \((u-v,u-v)\), even if it is positive, should not be considered as the square of the distance between two points \(u,v\). According to , one calls \(\) the Lorenzian distance between \(u,v\). Despite the term distance in the name, the Lorenzian distance does not satisfy triangle inequality in general.

### Non-Euclidean MDS

We propose _Non-Euclidean MDS_, a novel linear dimension reduction technique using bilinear forms. For a vector \(v\), we use \((v)\) as the diagonal matrix with \(v\) along the main diagonal and zero everywhere else. For any given symmetric dissimilarity matrix \(D^{n n}\), let the eigen decomposition of the centralization of \(D\) be given as follows:

\[-CDC/2=U U^{T}\] (3)where \(U^{n n}\) is the orthogonal matrix of eigenspace and \(^{n n}\) is the diagonal matrix \(=:()\) with eigenvalues \((_{1},,_{n})^{T}\), \(_{1}_{2}_{n}\). By using an algorithm that will be discussed in Section 4 we will choose \(k\) of the eigenvalues, represented by a binary indicator vector \(=(w_{1},,w_{n})\{0,1\}^{n}\) with a value of \(1\) (or \(0\)) indicating the corresponding eigenvalue is chosen (or not chosen). Let

\[X=() U^{T}=:(X_{1},,X_{n}),\] (4)

Note that \(\) contains complex numbers for non-PSD matrix \(D\). Since \(\) has only \(k\) non-zero values, we can drop the \(n-k\) zero rows in \(X\) (corresponding to the eigenvalues not selected) and have a \(k\)-dimensional vector representation of the data. Now we can derive dissimilarities by defining

\[_{i,j}(X_{i}-X_{j})^{T}(X_{i}-X_{j}),\,=(X):= _{i,j}.\]

See 1 for details.

**Input**: \(n n\) dissimilarity matrix \(D\), integer \(k n\).

**Output**: \(k n\) matrix \(X\) of \(k\)-dim vectors

\(B=-CDC\), where \(C=I-_{n}_{n}^{T}\) ;

Compute eigenvalue vector \(\) and eigenvectors \(U\) of \(B\): \(=(_{1},_{2},_{n})^{T}\) with

\(_{1}_{2}_{n}\);

Compute the indicator vector of \(k\) selected eigenvalues \(=(,k)\);

Compute \(X\) by \(() U^{T}\) with \(n-k\) zero rows dropped;

**Algorithm 1**Non-Euclidean Multidimensional Scaling

In the description above, we allow the coordinates in \(X\) to take complex numbers, such that we can take \(_{i,j}\) to be the standard dot product of \(X_{i}-X_{j}\) with \(X_{i}-X_{j}\). Alternatively, we can keep \(X\) to take real coordinates, i.e., \(X=}() U^{T}\) with \(^{}=(|_{1}|,,|_{n}|)\). Again we drop the \(n-k\) zero rows in \(X\) and take a bilinear form \(f(u,v)=u^{T}Av\), where \(A\) is a \(k\) by \(k\) diagonal matrix, with the element at \((i,i)\) to be \(1\) (or \(-1\)) if the corresponding eigenvalue chosen is positive/negative. Notice that the bilinear form takes precisely the format of \((p,q)\)-distance as in Equation (2).

We have a few remarks in place: First we are not throwing away the negative eigenvalues. As will be explained later we actually keep eigenvalues of largest magnitude and some could be negative. As a consequence \(\) may have negative real values, which is expected as we are moving away from Euclidean distances and input entries in \(D\) may even start to be negative. Second, when \(D\) is an EDM, i.e., all eigenvalues are non-negative, Neuc-MDS reduces to classical MDS. Last, similar to cMDS, our method also starts with computing the eigenvalues of the Gram matrix. For large datasets, fast (approximation) algorithm for partial SVD can also be applied to our methods. For example, on \(n n\) symmetric matrices, using power methods one can iteratively compute partial SVD up to \(k\) largest/smallest eigenvalues in \(O(kn^{2})\). With randomness introduced, it can be reduced to \(O( k n^{2})\). For really large datasets, the dissimilarity matrix with size \(O(n^{2})\) might already be too large to be acquired or stored, one may extend MDS through local embedding methods like Landmark MDS  or Local MDS . This approach can also be applied to neuc-MDS to substantially speed up computation without suffering too much on performance (See Section 6 for empirical results).

## 4 Theoretical Results for Non-Euclidean MDS

To establish the foundation of theoretical analysis, we first analyze the STRESS error of Neuc-MDS and decompose it into three terms. Next we show an efficient algorithm that minimizes the first two terms that dominate. Last, we examine Neuc-MDS and classical MDS on random Gaussian matrices.

### Error Analysis

Inspired by the analysis of STRESS of classical MDS , we adopt a similar approach and decompose the STRESS error into three terms. Let \(^{n}\) be the vector of all eigenvalues \(=(_{1},,_{n})^{T}\), \(^{(2)}^{n}\) be the vector of all squared eigenvalues \(^{(2)}=(_{1}^{2},,_{n}^{2})^{T}\), \(}_{n}-\) be the indicator vector of dropped eigenvalues, \(}\{0,1\}^{n}\), and \(\) be the Hadamard product. We have the following result with proof in Appendix B.

**Theorem 2**.: _It holds that \(\|-D\|_{F}^{2}=C_{1}+C_{2}+C_{3}\), where_

\[C_{1}=4}^{T}^{(2)},\;C_{2}=4(}^{T})^{2},\;C_{3}=2n\|(U U)(})\|_{F}^{2 }-C_{2}/2.\]

Note that \(C_{1}/4\) is the _sum of squared_ eigenvalues that are dropped, and \(C_{2}/4\) is the _square of the sum_ of eigenvalues that are dropped. Individually, we can minimize \(C_{1}\) by keeping eigenvalues of large absolute value; and \(C_{2}\) by balancing the dropped eigenvalues such that the summation has a small magnitude. For term \(C_{3}\), from Equation 21 in Proof B, we know \(C_{3} 0\). In  it is argued that if one takes the approximation \(\|(U U)(})\|_{F}^{2}\|}_{n}_{n}^{T}(})\|_{F}^{2}\) for a random orthogonal matrix \(U\), then \(C_{3} 0\). Although it is empirically observed in  that \(C_{3}\) is roughly constant and hence negligible for optimization, there are some cases in our experiments in which \(C_{3}\) is not negligible.

In light of Theorem 2, we would like to approximately optimizing the STRESS by minimizing the lower bound \(C_{1}+C_{2}\), which can be formulated as a quadratic integer programming problem: Given a set of \(n\) values \(=\{_{i}\}\) and a positive integer \(k>0\), choose a \(k\)-subset \(S\) such that

\[_{S,|S|=k}_{ S} ^{2}+_{ S}^{2}.\] (5)

When \(_{i}\)'s are all positive, the best choice is to take the \(k\) largest eigenvalues of \(\), i.e., the cMDS' solution. However, with a mixture of positive and negative eigenvalues, taking the top \(k\) largest eigenvalues is no longer optimal -- specifically, as \(k\) increases, the first error term is monotonically reduced but the second error term could start going up. In the following subsection, we discuss an optimal algorithm to solve Equation (5).

### An Optimal Algorithm for Eigenvalue Selection

The optimization problem described in Equation (5) is a special case of the family of quadratic integer programming problems. Though in general, quadratic integer programming is NP-hard , we show that this particular one is actually solvable in polynomial time. Formally, we have:

**Theorem 3**.: _For the optimization problem defined in (5), there exits an optimal solution with \(r\) largest positive values and \(s\) smallest negative values in \(\), \(r+s=k\). And, there is an \(O(n)\)-algorithm that outputs an optimal solution._

Our algorithm (2) is greedy and iteratively selects the eigenvalue with the highest absolute value. Specifically, let \(S\) be the set of selected eigenvalues and \(H(S)\) be the sum of eigenvalues not selected. Initially \(S=\). In each iteration, if \(H(S)<0\), select the negative eigenvalues remained of the greatest magnitude and add it to \(S\); if \(H(S)>0\), pick the largest positive one. If \(H(S)=0\), pick the eigenvalue with the greatest magnitude. The complete proof of Theorem 3 is delayed to Appendix C.

### Analysis on Random Symmetric Matrices

Here, we analyze the important error term \(C_{1}+C_{2}\) in Theorem 2 for cMDS and Neuc-MDS when the input (centered) dissimilarity matrix is a symmetric random matrix. Our analysis is established upon Wigner's famous Semicircle Law , which states the following.

**Proposition 4** (Semicircle Law ).: _Suppose \(B^{n n}\) is a symmetric random matrix where every element is independently distributed with equal densities and second moments \(^{2}\). Let \(S_{a,b}(B)\) be the number of eigenvalues of \(B\) that lie in the interval \((a,b).\) Then the expected value \(E(S_{a,b}(B))\) of \(S_{a,b}(B)\) satisfies_

\[_{n}(B))}{n}=}_{a}^{b} -x^{2}}dx.\] (6)Our results are formally stated as follows with proof in Appendix D.

**Theorem 5**.: _Suppose a random symmetric matrix \(B^{n n}\) where \(B_{ij}\) is independently distributed with equal densities and second moments \(^{2}\), is taken as the centered matrix3 to classical MDS and Neuc-MDS, both selecting \(k\) eigenvalues with \(k n\). Let \(e_{C}\) denote the \(C_{1}+C_{2}\) error for cMDS and \(e_{N}\) for Neuc-MDS, we have:_

1. _when_ \(k=o(n)\)_,_ \(e_{C} n^{2}^{2}(1+}{n}-)\)_, and_ \(e_{N} n^{2}^{2}(1-)\)__
2. _when_ \(k=cn\)_, with_ \(c 1\)_,_ \(e_{N} 0\)_. When_ \(c 1/2\)_,_ \(e_{C} 0.1801 n^{3}^{2}\)_._

By Theorem 5, Neuc-MDS has a strictly better \(C_{1}+C_{2}\) error than cMDS. Second, if we take \(|S|=k n=||\), the term \(C_{1}=_{ S}^{2}\) is almost equal to \(_{}^{2} n^{2}^{2}\). Therefore the STRESS error of both classical MDS and Neuc-MDS cannot be very small if the target dimension \(k\) is \(o(n)\). Lastly, when \(k=cn\), with \(c 1\), \(e_{N}\) is monotonically decreasing and eventually reaches \(0\). On the other hand, for cMDS, when \(c 1/2\), \(e_{C}\) reaches a plateau at about \(0.1801 n^{3}^{2}\). Notice that this error has an extra factor of \(n\) compared to the error for small \(k\).

For real world data the Gram matrix is likely far from a random matrix. The analysis above points out that aggressive dimension reduction can be indeed only a luxury for structured data, even if we use inner products that are not limited to Euclidean distances. Second, any real world data carries some random measurement noise. When the scale of such random noise becomes non-negligible, STRESS error introduced by such noise cannot be small with aggressive dimension reduction. We would recommend practitioners to examine the spectrum of eigenvalues to gain insights on the power or limit in reducing dimensions.

## 5 Beyond Binary Selection of Eigenvalues

Both cMDS and Neuc-MDS choose a subset of \(k\) eigenvalues from the input Gram matrix. This can be considered as applying \(()\) to the eigenvalue vector \(\) (Equation 4) to filter some eigenvalues out. Such operators can be viewed as a special case of more general low-rank linear maps \(T:^{n}^{n}\) with \((T)=k n\).

Here we consider a new family of embeddings \(=U^{1/2}\) with \(\) being a rank-\(k\) diagonal matrix whose diagonal entries are given by some \(}^{n}\) with only \(k\) nonzero entries. Note that there always exists a rank-\(k\) linear operator \(T\) such that \(}=T()\). Then we ask if we can improve Neuc-MDS further with a rank-\(k\) linear map of \(\)? This question can be answered by the following theorem:

**Theorem 6**.: _Given a dissimilarity matrix \(D^{n n}\) with eigenvalues \(^{n}\) of its Gram matrix \(-CDC/2=U U^{T}\), for any rank-\(k\)\((k n)\) diagonal matrix \(=(})\) with \(}^{n}\) with \(k\) nonzero entries only on coordinates given by some index \(k\)-set \(W[n]\), let \(:=_{W}\{0,1\}^{n}\) be the indicator vector of \(W\), and let \(\) be the dissimilarity matrix reconstructed from \(=^{1/2}U^{T}\). Then the STRESS error of \(\) can be expressed as:_\(\|-D\|_{F}^{2}=_{1}+_{2}+_{3}\) with_

\[}:=4[}^{T}^{(2)}+^{T} ^{(2)}],\,}:=4[}^{T} +^{T}]^{2},\,}:=2n\|(U U)( )\|_{F}^{2}-_{2}}{2},\]

_where \(:=-}\). The first two terms, \(}+}\), as a lower bound of the STRESS, is minimized as_

\[4}^{T}^{(2)}+}^{T})^{2}} {1+k}}}^{*}:=+}^{T} }{1+k}.\] (7)

**Remark 7**.: _Recall the decomposition of STRESS in Theorem 2, the lower bound \(4}^{T}^{(2)}+4(}^{T})^{2}=C_{1} +C_{2}/(k+1) C_{1}+C_{2}\). Therefore, it has a better lower bound of STRESS compared to Neuc-MDS._

Theorem 6 provides a constructive way to obtain the optimal \(}\) that (approximately) minimizes the STRESS error for a prefixed \(\) which is determined by an indicator set \(W\) served as the constraints of nonzero entries on \(}\). The next question is how to find an optimal \(k\)-set \(W\) on which the optimal \(}^{*}\) has the lowest lower bound. Essentially, we need to solve the following optimization problem:

\[_{|W|=k}[_{i W}_{i}^{2}+(_{i W }_{i})^{2}]\] (8)

**Proposition 8**.: _For the optimization problem (8), there exits an optimal solution with \(r\) largest positive values and \(s\) smallest negative values in \(\), \(r+s=k\). And, there is an \(O(n)\)-algorithm that outputs the optimal solution._

The algorithm follows a similar greedy manner as EV-Selection, with only one adjustment: in each step, compare the two marginal gains provided by the largest positive eigenvalue and the lowest negative eigenvalue among the remained ones, and choose the positive/negative eigenvalue of largest magnitude if the corresponding gain is smaller. We name the algorithm Neuc-MDS\({}^{+}\), and present the complete algorithm with its correctness proof in Appendix C.

## 6 Experiments

This section presents experimental results of Neuc-MDS and Neuc-MDS\({}^{+}\). First, we evaluate the performance on dissimilarity error of two proposed algorithms comparing with closely-related baselines on three metrics: STRESS, distortion and additive error. Then we show that the _dimensionality paradox_ issue observed on cMDS is fully resolved by Neuc-MDS and Neuc-MDS\({}^{+}\).

Synthetic Data.We introduce two synthetic datasets: _Random-simplex_ and _Euclidean-ball_, both with non-Euclidean dissimilarities. See details in Appendix E.1. On a high level, suppose the dataset has size \(n\), we construct a _Random-simplex_ such that for each vertex, the first \(n-1\) coordinates virtually form a simplex, while the last coordinate almost dominates the distances between the other points, which creates a large negative eigenvalue for the Gram matrix. The _Euclidean-ball_ dataset (similar to Delft's balls ) considers \(n\) balls of different radii with the distance of two balls defined as the smallest distance of two points on the two respective balls. The dissimilarities by this definition no longer satisfy triangle inequality.

Real-world Data.We consider both genomics and image data. For genomics data, we include 5 datasets from the Curated Microarray Database (CuMiDa) , each indicating a certain type of cancer. Following the practice mentioned in , pairwise dissimilarities are generated with entropic affinity with the diagonal as zero. We also test three celebrated image datasets: MNIST, Fashion-MNIST and CIFAR-10. The dissimilarity matrix for each dataset captures 1000 images randomly sampled for each class. We use three measures in  to calculate the dissimilarities.

[MISSING_PAGE_EMPTY:9]

The results4 show that in terms of STRESS, Neuc-MDS and Neuc-MDS\({}^{+}\) outperform cMDS and Lower-MDS consistently by a large margin. SMACOF has comparable performance with Neuc-MDS in a couple data sets but can go out of bound in others. For average distortion, the genomics datasets differentiate different methods drastically while the rest datasets produce comparable results. Neuc-MDS\({}^{+}\) occasionally gives slightly higher STRESS than Neuc-MDS. Recall that both methods focus on optimizing for a lower bound of STRESS (in Theorem 2 and Theorem 6). This shows that \(C_{3}\) may play a role in practice.

**Neuc-MDS Addresses 'Dimensionality Paradox'.** Dimensionality paradox of classical MDS refers to the observation that STRESS increases as the dimension goes up. When raising this concern,  proposes a Lower-MDS algorithm as mitigation. We show that Neuc-MDS and Neuc-MDS\({}^{+}\) address this issue even better. For Lower-MDS the target dimension cannot be larger than the number of positive eigenvalues. Our methods do not have this limitation. Figure 2 shows STRESS on Random-simplex and Renal. In Random-simplex, cMDS has an increasing STRESS with \(k=75 100\) then stops, and in Renal the STRESS keeps increasing. In contrast, Lower-MDS converges promptly while Neuc-MDS and Neuc-MDS\({}^{+}\) achieve even lower STRESS. Results on other datasets are in Appendix E.4.

**Landmark MDS** Landmark MDS  is a heuristic to speed up classical MDS. One chooses a small number of landmarks and apply MDS on the landmarks first. The coordinates of the remaining points are obtained through a triangulation step with respect to the landmarks. We can use the same heuristic to speed up Neuc-MDS. On the random-simplex dataset, with only 25% points randomly chosen as landmarks the STRESS is only a factor of 1.0644 of the STRESS obtained by Neuc-MDS. If we use only 10% points as landmarks, the final STRESS is only 1.0898 of the STRESS of Neuc-MDS. This shows that Neuc-MDS can also be significantly accelerated using the landmark idea, achieving nearly the same STRESS.

## 7 Discussion and Conclusion

This paper presents an extension of classical MDS to non-Euclidean non-metric settings. We would like to mention a few future directions. Since we step out of the domain of Euclidean embedding, both the input dissimilarity matrix and the one obtained after dimension reduction can have negative values. Therefore if one would like to feed the output dissimilarity matrix to another data processing module that by default requires non-negative values, special care must be taken to address the negative values. In experiments, we discovered that Neuc-MDS\({}^{+}\) achieves similar stress as Neuc-MDS and produces much fewer negatives values in the output dissimilarity matrix (Appendix E.3). How to effectively use such dissimilarities in downstream learning and inference tasks would be a major future work. Note that the geometry of general bilinear forms is a largely unexplored territory.