# Learning Invariant Molecular Representation in Latent Discrete Space

Xiang Zhuang\({}^{1,2,3}\), Qiang Zhang\({}^{1,2,3}\), Keyan Ding\({}^{2}\), Yatao Bian\({}^{4}\),

**Xiao Wang\({}^{5}\), Jingsong Lv\({}^{6}\), Hongyang Chen\({}^{6}\), Huajun Chen\({}^{1,2,3}\)\({}^{}\)**

\({}^{1}\)College of Computer Science and Technology, Zhejiang University

\({}^{2}\)ZJU-Hangzhou Global Scientific and Technological Innovation Center

\({}^{3}\)Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph

\({}^{4}\)Tencent AI Lab, \({}^{5}\)School of Software, Beihang University, \({}^{6}\)Zhejiang Lab

{zhuangxiang,qiang.zhang.cs,dingkeyan,huajunsir}@zju.edu.cn

yatao.bian@gmail.com,xiao_wang@buaa.edu.cn

{jingsonglv,hongyang}@zhejianglab.com

Equal contribution.Corresponding author.

###### Abstract

Molecular representation learning lays the foundation for drug discovery. However, existing methods suffer from poor out-of-distribution (OOD) generalization, particularly when data for training and testing originate from different environments. To address this issue, we propose a new framework for learning molecular representations that exhibit invariance and robustness against distribution shifts. Specifically, we propose a strategy called "first-encoding-then-separation" to identify invariant molecule features in the latent space, which deviates from conventional practices. Prior to the separation step, we introduce a residual vector quantization module that mitigates the over-fitting to training data distributions while preserving the expressivity of encoders. Furthermore, we design a task-agnostic self-supervised learning objective to encourage precise invariance identification, which enables our method widely applicable to a variety of tasks, such as regression and multi-label classification. Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts. Our code is available at https://github.com/HICAI-ZJU/iMoLD.

## 1 Introduction

Computer-aided drug discovery has played an important role in facilitating molecular design, aiming to reduce costs and alleviate the high risk of experimental failure . In recent years, the emergence of deep learning has led to a growing interest in molecular representation learning, which aims to encode molecules as low-dimensional and dense vectors . These learned representations have demonstrated their availability in various tasks, including target structure prediction , binding affinity analysis , drug re-purposing  and retrosynthesis .

Despite the significant progress in molecular representation methods, a prevailing assumption in the traditional approaches is that data sources are independent and sampled from the same distribution. However, in practical drug development, molecules exhibit diverse characteristics and may originate from different distributions . For example, in virtual screening scenarios , the distribution shift occurs not only in the molecule itself, e.g., size  or scaffold  changes, but also in thetarget , e.g., the emergent COVID-19 leads to a new target from unknown distributions. This out-of-distribution (OOD) problem poses a challenge to the generalization capability of molecular representation methods and results in the degradation of performance in downstream tasks [17; 18].

Current studies mainly focus on regular Euclidean data for OOD generalization. Most studies [18; 19; 20; 21; 22] adopt the invariance principle [23; 24], which highlights the importance of focusing on the critical causal factors that remain invariant to distribution shifts while overlooking the spurious parts [23; 19; 20]. Although the invariance principle has shown effectiveness on Euclidean data, its application to non-Euclidean data necessitates further investigation and exploration. Molecules are often represented as graphs, a typical non-Euclidean data, where atoms as nodes and bonds as edges, thereby preserving rich structural information . The complicated molecular graph structure makes it challenging to accurately distinguish the invariant causal parts from diverse spurious correlations .

Preliminary studies have made some attempts on molecular graphs [26; 25; 27; 28; 29]. They explicitly divide graphs to extract invariant substructures, at the granularity of nodes , edges [26; 27; 28; 29], or motifs . These attempts can be summarized as the "first-separation-then-encoding" paradigm (Figure 1 (a)), which first divides the graph into invariant and spurious parts and then encodes each part separately. We argue that this practice is suboptimal for extremely complex and entangled graphs, such as real-world molecules [30; 31], since some intricate properties cannot be readily determined by analyzing a subset of the molecular structure [30; 31]. Besides, some methods [25; 27] require assumptions and inferences about the environmental distribution, which are often untenable for molecules due to the intricate environment. Additionally, the downstream tasks related to molecules are diverse, including regression and classification. However, some methods such as CIGA  and DisC  can only be applied to single-label classification tasks, due to the constraints imposed by the invariant learning objective function.

To fill these gaps, we present a novel molecule invariant learning framework to effectively capture the invariance of molecular graphs and achieve generalized representation against distribution shifts. In contrast to the conventional approaches, we propose a "first-encoding-then-separation strategy" (Figure 1 (b)). Specifically, we first employ a Graph Neural Network (GNN) [33; 34; 35] to encode the molecule (i.e., encoding GNN), followed by a residual vector quantization module to alleviate the over-fitting to training data distributions while preserving the expressivity of the encoder. We then utilize another GNN to score the molecule representation (i.e., scoring GNN), which measures the contributions of each dimension to the target in latent space, resulting in a clear separation between invariant and spurious representations. Finally, we design a self-supervised learning objective [36; 37; 38] that aims to encourage the identified invariant features to effectively preserve label-related information while discarding environment-related information. It is deserving of note that the objective is task-agnostic, which means that our method can be applied to various tasks, including regression and single- or multi-label classification.

Our main contributions can be summarized as follows:

* We propose a paradigm of "first-encoding-then-separation" using an encoding GNN and a scoring GNN, which enables us to effectively identify invariant features from highly complex graphs.
* We introduce a residual vector quantization module that strikes a balance between model expressivity and generalization. The quantization acts as the bottleneck to enhance generalization, while the residual connection complements the model's expressivity.
* We design a self-supervised invariant learning objective that facilitates the precise capture of invariant features. This objective is versatile, task-agnostic, and applicable to a variety of tasks.

Figure 1: (a) First-Separation-Then-Encoding: the input is separated using a subgraph generator, then each subgraph is encoded respectively. (b) First-Encoding-Then-Separation: the input is encoded, then the representation is separated by a scorer.

* We conduct comprehensive experiments on a diverse set of real-world datasets with various distribution shifts. The experimental results demonstrate the superiority of our method compared to state-of-the-art approaches.

## 2 Related Work

OOD Generalization and Invariance Principle.The susceptibility of deep neural networks to substantial performance degradation under distribution shifts has led to a proliferation of research focused on out-of-distribution (OOD) generalization . Three lines of methods have been studied for OOD generalization on Euclidean data, including group distributionally robust optimization [40; 41; 42], domain adaptation [43; 44; 45] and invariant learning [19; 20; 21]. Group distributionally robust optimization considers groups of distributions and optimize across all groups simultaneously. Domain adaptation aims to align the data distributions but may fail to find an optimal predictor without additional assumptions [19; 20; 46]. Invariant learning aims to learn invariant representation satisfying the invariant principle [20; 24], which includes two assumptions: (1) sufficiency, meaning the representation has sufficient predictive abilities, (2) invariance, meaning representation is invariant to environmental changes. However, most methods require environmental labels, which are expensive to obtain for molecules , and direct application of these methods to complicated non-Euclidean molecular structure does not yield promising results [13; 22; 26].

OOD Generalization on Graphs.Recently, there has been growing attention on the graph-level representations under distribution shifts from the perspective of invariant learning. Some methods [26; 32; 27; 28; 25] follows the "first-separation-then-encoding" paradigm, and they make attempt to capture the invariant substructures by dividing nodes and edges in the explicit structural space. However, these methods suffer from the difficulty in dividing molecule graphs in raw space due to the complex and entangled molecular structure . Moreover, MoleOOD  and GIL  require inference of unavailable environmental labels, which entails prior assumptions on the environmental distribution. And the objective of invariant learning may hinder the application, e.g., CIGA  and DisC  can only apply to single-label classification rather than to regression and multi-label tasks. Additionally, OOD-GNN  does not use the invariance principle. It proposes to learn disentangled graph representations, but requires computing global weights for all data, leading to a high computational cost. OOD generalization on graphs can also be improved by another line of relevant works [29; 49; 47] on GNN explainability [50; 51], which aims to provide a rationale for prediction. However, they may fail in some distribution shift cases . And DIR  and GSAT  also divide graphs in the raw structural space. Although GREA  learns in the latent feature space, it only conducts separation on each node while neglecting the different significance of each representation dimensionality. In this work, we focus on the OOD generalization of molecular graphs, against multi-type of distribution shifts, e.g., scaffold, size, and assay, as shown in Figure 2.

Vector Quantization.Vector Quantization (VQ) [52; 53] acts as a bottleneck of representation learning. It discretizes continuous input data in the hidden space by assigning them to the nearest vectors in a predefined codebook. Some studies [53; 54; 55; 56] have demonstrated its effectiveness to enhance model robustness against data corruptions. Other studies [57; 58; 59] find that taking VQ as an inter-component communication within neural networks can contribute to the model generalization ability. However, we posit that while VQ can improve generalization against distribution shifts, it may also limit the model's expressivity and potentially lead to under-fitting. To address this concern, we propose to equip the conventional VQ with a residual connection to strike a balance between model generalization and expressivity.

Figure 2: An overview of distribution shifts in molecules. Distribution shifts occur when molecules originate from different scaffold, size or assay environments.

Preliminaries

### Problem Definition

We focus on the OOD generalization of molecules. Let \(\) be the molecule graph space and \(\) be the label space, the goal is to find a predictor \(f\,:\) to map the input \(G\) into the label \(Y\). Generally, we are given a set of datasets collected from multiple environments \(E_{all}:D=\{D^{e}\}_{e E_{all}}\). Each \(D^{e}\) contains pairs of an input molecule and its label: \(D^{e}=\{(G_{i},Y_{i})\}_{i=1}^{n_{e}}\) that are drawn from the joint distribution \(P(G,\,Y|E=e)\) of environment \(e\). However, the information of environment \(e\) is always not available for molecules, thus we redefine the training joint distribution as \(P_{train}(G,\,Y)=P(G,\,Y|E=e), e E_{train}\), and the testing joint distribution as \(P_{test}(G,\,Y)=P(G,\,Y|E=e), e E_{all} E_{train}\), where \(P_{train}(G,\,Y) P_{test}(G,\,Y)\). We denote joint distribution across all environment as \(P_{all}(G,\,Y)=P(G,\,Y|E=e), e E_{all}\). Formally, the goal is to learn an optimal predictor \(f^{*}\) based on training data and can generalize well across all distributions:

\[f^{*}=_{f}_{(G,\,Y) P_{all}}[(f(G ),\,Y)],\] (1)

where \((,)\) is the empirical risk function. Moreover, since the joint distribution \(P(G,\,Y)\) can be written as \(P(Y|G)P(G)\), the OOD problem can be refined into two cases, namely covariate and concept shift [60; 61; 62; 63]. In covariate shift, the distribution of input differs. Formally, \(P_{train}(G) P_{test}(G)\) and \(P_{train}(Y|G)=P_{test}(Y|G)\). While concept shift occurs when the conditional distribution changes as \(P_{train}(Y|G) P_{test}(Y|G)\) and \(P_{train}(G)=P_{test}(G)\). We will consider and distinguish between both cases in our experiments.

### Molecular Representation Learning

We denote a molecule graph by \(G=\{,\}\), where \(\) is the set of nodes (e.g., atoms) and \(\) is the set of edges (e.g., chemical bonds). Generally, the predictor \(f\) can be denoted as \( g\), containing an encoder \(g:^{d}\) that extracts representation for each molecule and a downstream classifier \(:^{d}\) that predicts the label with the molecular representation. In particular, the encoder \(g\) operates in two stages: firstly, by employing a graph neural network [33; 34; 35] to generate node representations \(\) according to the following equation:

\[=[_{1},_{2},,_{||}]^{}=(G)^{|| d},\] (2)

where \(_{v}^{d}\) is the representation of node \(v\). Secondly, the encoder utilizes a readout operator to obtain the overall graph representation \(\):

\[=()^{d}.\] (3)

The readout operator can be implemented using a simple, permutation invariant function such as average pooling.

## 4 Method

This section presents the details of our proposed method that learns invariant **M**olecular representation in **L**atent **D**iscrete space, called **i**MoLD**. Figure 3 shows the overview of iMoLD, which mainly consists of three steps: 1) Using a GNN encoder and a residual vector quantization module to obtain the molecule representation (Section 4.1); 2) Separating the representation into invariant and spurious parts through a GNN scorer (Section 4.2); 3) Optimizing the above process with a task-agnostic self-supervised learning objective (Section 4.3).

### Encoding with Residual Vector Quantization

The current mainstream methods [26; 25; 28; 27; 32] adopt the "first-separation-then-encoding" paradigm, which explicitly divides graphs into invariant and spurious substructures on the granularity of edge, node, or motif, and then encodes each substructure individually. In contrast, we use the opposite paradigm that first encodes the whole molecule followed by separation.

Specifically, given an input molecule \(G\), we first use a GNN to encode it, resulting in node representations \(\):

\[=_{E}(G)^{|| d},\] (4)

where \(_{E}\) represents the encoding GNN, \(||\) is the number of nodes in \(G\), and \(d\) is the dimensionality of features. Inspired by the studies  that Vector Quantization (VQ)  is helpful to improve the model generalization on computer vision tasks, we propose a Residual Vector Quantization (RVQ) module to refine the obtained representations.

In the RVQ module, VQ is used to discretize continuous representations into discrete ones. Formally, it introduces a shared learnable codebook as a discrete latent space: \(=\{_{1},_{2},_{||}\}\), where each \(_{k}^{d}\). For each node representation \(_{v}\) in \(\), VQ looks up and fetches the nearest neighbor in the codebook and outputs it as the result. Mathematically,

\[(_{v})=_{k}, k=* {argmin}_{k\{1,,||\}}\|_{v}-_{k} \|_{2},\] (5)

and \(()\) denotes the discretization operation which quantizes \(_{v}\) to \(_{k}\) in the codebook.

The VQ operation acts as a bottleneck to enhance generalization and alleviate the easy-over-fitting issue caused by distribution shifts. However, it also impairs the expressivity of the model by using a limited discrete codebook to replace the original continuous input, suffering from a potential under-fitting issue. Accordingly, we propose to equip the conventional VQ with a residual connection to strike a balance between model generalization and expressivity. In specific, we incorporate both the continuous and discrete representations to update node representations \(\) to \(^{}\):

\[^{}=[(_{1})+_{1},(_{2})+_{2},,(_{||})+ _{||}]^{}.\] (6)

Similar to VQ-VAE , we employ the exponential moving average updates for the codebook:

\[N_{k}^{(t)}=N_{k}^{(t-1)}*+n_{k}^{(t)}(1-),_{k}^{(t)}= _{k}^{(t-1)}*+_{v}^{n_{k}^{(t)}}_{v}^{(t)}(1- ),_{k}^{(t)}=_{k}^{(t)}}{N_{k}^{(t)}},\] (7)

where \(n_{k}^{(t)}\) is the number of node representations in the \(t\)-th mini-batch that are quantized to \(_{k}\), and \(\) is a decay parameter between 0 and 1.

### Separation at Nodes and Features

After encoding, we separate the representation into invariant parts and spurious parts. It is worth noting that our separation is not only performed at the node dimension but also takes into account the feature dimension in the latent space. The reasons are two-fold: 1) Distribution shifts on molecules

Figure 3: An overview of iMoLD. Firstly, given a batch of inputs, we learn invariant and spurious representations (\(^{}\) and \(^{}\)) for each input in latent discrete space by a first-encoding-then-separation paradigm. An encoding GNN and an RVQ module are involved to obtain molecule representation, then the representation is separated through a scoring GNN. The invariant \(^{}\) is used to predict the label \(\). Then a task-agnostic self-supervised learning objective across the batch is designed to facilitate the acquisition of reliable invariant \(^{}\).

can occur at both the structure level and the attribute level , corresponding to the node dimension \(||\) and the feature dimension \(d\) in \(}\), respectively. 2) The resulting representation may be highly entangled, thus it is advisable to perform a separation on each dimension in the latent space.

Specifically, we use another GNN as a scorer to obtain the separating score \(\):

\[=(_{S}(G))^{| | d},\] (8)

where \(_{S}\) represents the scoring GNN, \(||\) is the number of nodes in \(G\), and \(d\) is the dimensionality of features. \(()\) denotes the Sigmoid function to constrain each entry in \(\) falls into the range of \((0,1)\). Then we can capture the invariant and complementary spurious features at both structure and attribute granularity in the latent representation space by applying the separating scores to node representations:

\[^{}=},^{ }=}(1-),\] (9)

where \(^{}\) and \(^{}\) denote the invariant and spurious node representations respectively, and \(\) is the element-wise product. Finally, the invariant and spurious representation (denoted as \(^{}\) and \(^{}\) respectively) of \(G\) can be generated by a readout operator:

\[^{}=(^{})^{d},^{}=(^{}) ^{d}.\] (10)

### Learning Objective

Our OOD optimization objectives are composed of an invariant learning loss, a task prediction loss, and two additional regularization losses.

Task-agnostic Self-supervised Invariant Learning.Invariant learning aims to optimize the encoding \(_{E}\) and the scoring \(_{S}\) to produce precise invariant and spurious representations. In particular, we need \(^{}\) to be invariant under environmental changes. Additionally, we expect the objective to be independent of the downstream task, which allows the method to be not restricted to a specific type of task. To achieve these, we design a task-agnostic and self-supervised invariant learning objective. Specifically, we disturb \(^{}_{i}\) via concatenating a corresponding \(^{}_{j}\) in a shuffled batch, resulting in an augmented representation \(}^{}_{i}\):

\[}^{}_{i}=^{}_{i} ^{}_{j[1,B]},\] (11)

where \(\) denotes concatenation operator and \(B\) is batch size.

Inspired by a simple self-supervised learning framework that takes different augmentation views as similar positive pairs and does not require negative samples , we treat \(^{}_{i}\) and \(}^{}_{i}\) as positive pairs and push them to be similar, using an MLP-based predictor (denoted as \(\)) that transforms the output of one view and aligns it to the other view. We minimize their negative cosine similarity:

\[_{}=-_{i=1}^{B}([^{ }_{i}],(}^{}_{i})),\] (12)

where \((,)\) represents the formula of cosine similarity and \([]\) denotes stop-gradient operation to prevent collapsing . We employ \(_{}\) as our objective for invariant learning to ensure the invariance of \(^{}\) against distribution shifts.

Task Prediction.The objective of task prediction is to provide an invariant presentation \(^{}\) with sufficient predictive abilities. During training, the choice of prediction loss function depends on the type of task. For classification tasks, we employ the cross-entropy loss, while for regression tasks, we use the mean squared error loss. Take the binary classification task as an example, the cross-entropy loss is computed between the predicted \(_{i}=(^{}_{i})\) and the ground-truth label \(y_{i}\):

\[_{}=_{i=1}^{B}(y_{i}_{i}+(1-y_{i })(1-_{i})).\] (13)

[MISSING_PAGE_FAIL:7]

* **DrugOOD**, which is a OOD benchmark for AI-aided drug discovery. This benchmark provides three environment-splitting strategies, including assay, scaffold and size, and applies these three splitting to two measurements (IC50 and EC50). As a result, we obtain 6 datasets, and each dataset contains a binary classification task for drug target binding affinity prediction. A detailed description of each environment-splitting strategy is also in Appendix A.

Baselines.We thoroughly compare our method against ERM  and two groups of OOD baselines: (1) general OOD algorithms used for Euclidean data, which include domain adaptation methods such as Coral  and DANN , group distributionally robust optimization method (GroupDRO ), invariant learning methods such as IRM  and VREx  and data augmentation method (Mixup ). And (2) graph-specific algorithms, including Graph OOD algorithms such as CAL , DisC , MoleOOD  and CIGA , as well as interpretable graph learning methods such as DIR , GSAT  and GREA . Details of baselines and implementation are in Appendix B.

Evaluation.We report the ROC-AUC score for GOOD-HIV and DrugOOD datasets as the task is binary classification. For GOOD-ZINC, we use the Mean Average Error (MAE) since the task is regression. While for GOOD-PCBA, we use Average Precision (AP) averaged over all tasks as the evaluation metric due to extremely imbalanced classes. We run experiments 10 times with different random seeds, select models based on the validation performance and report the mean and standard deviations on the test set.

### Main Results (RQ1)

Table 1 and Table 2 present the empirical results on GOOD and DrugOOD benchmarks, respectively. Our method iMoLD achieves the best performance on 16 of the 18 datasets and ranks second on the other two datasets. Among the compared baselines, the graph-specific OOD methods perform best on only 11 datasets, and some general OOD methods outperform them on another 7 datasets. This suggests that although some advanced graph-specific OOD methods can achieve superior performance on some synthetic datasets (e.g., to predict whether a specific motif is present in a synthetic graph ), they may not perform well on molecules due to the realistic and complex data structures and distribution shifts. In contrast, our method is able to achieve the best performance on most of the datasets, which indicates that the proposed identification of invariant features in the latent space is effective for applying the invariance principle to the molecular structure. We also observe that MoleOOD, a method designed specifically for molecules, does not perform well on GOOD-ZINC and GOOD-PCBA, possibly due to its dependence on inferred environments. Inferring environments may become more challenging for larger-scale datasets, such as GOOD-ZINC and GOOD-PCBA, which contain hundreds of thousands of data and more complex tasks (e.g., PCBA has a total of 128 classification tasks). Our method does not require the inference of environment, and is shown to be effective on datasets of diverse scales and tasks.

    &  &  \\   & Assay & Scaffold & Size & Assay & Scaffold & Size \\  ERM & 71.63(0.76) & 68.79(0.47) & 67.50(0.38) & 67.39(2.90) & 64.98(1.29) & 65.10(0.38) \\ IRM & 71.15(0.57) & 67.22(0.62) & 61.58(0.58) & 67.77(2.71) & 63.86(1.36) & 59.19(0.83) \\ Coral & 71.28(0.91) & 68.36(0.61) & 64.53(0.32) & 72.08(2.80) & 64.83(1.64) & 58.47(0.43) \\ MiVUp & 71.49(1.08) & 68.59(0.27) & 67.79(0.39) & 67.81(4.06) & 65.77(1.83) & 65.77(0.60) \\  DIR & 69.84(1.41) & 66.33(0.65) & 62.92(1.89) & 65.81(2.93) & 63.76(3.22) & 61.56(4.23) \\ GSAT & 70.59(0.43) & 66.45(0.50) & 66.70(0.37) & 73.82(2.62) & 64.25(0.63) & 62.65(1.79) \\ GREA & 70.23(1.17) & 67.02(0.28) & 66.59(0.56) & 74.17(1.47) & 64.50(0.78) & 62.81(1.54) \\ CAL & 70.09(1.03) & 65.90(1.04) & 66.42(0.50) & 74.54(4.18) & 65.19(0.87) & 61.21(1.76) \\ DisC & 61.40(2.56) & 62.70(2.11) & 61.43(1.06) & 63.71(5.56) & 60.57(2.27) & 57.38(2.48) \\ MoleOOD & 71.62(0.52) & 68.58(1.14) & 65.62(0.77) & 72.69(1.46) & 65.74(1.47) & 65.51(1.24) \\ CIGA & 71.86(1.37) & **69.14(0.70)** & 66.92(0.54) & 69.15(5.79) & 67.32(1.35) & 65.65(0.82) \\ iMoLD & **72.11(0.51)** & 68.84(0.58) & **67.92(0.43)** & **77.48(1.70)** & **67.79(0.88)** & **67.09(0.91)** \\   

Table 2: Evaluation performance on DrugOOD benchmark. The best is marked with **boldface** and the second best is with underline.

[MISSING_PAGE_FAIL:9]

when the model achieves the best score on the validation set, using t-SNE  on the covariate-shift dataset of GOOD-HIV-Scaffold in Figure 4 (d). We also visualize the results of some baselines, including the vanilla ERM (Figure 4 (a)), the ERM equipped with VQ after encoder (ERM(+VQ), Figure 4 (b)), and with the RVQ module (ERM(+RVQ), Figure 4 (c)). We additionally compute the 1-order Wasserstein distance  between the features on the training and validation sets of each class, to quantify the dissimilarity in the feature distribution across varying environments. We find that adding the VQ or RVQ after the encoder results in a more uniform distribution of features and lower feature distances, due to the fact that VQ makes it possible to reuse previously encountered embeddings in new environments by discretizing them. Moreover, the feature distribution of our method is more uniform and the distance is smaller. This suggests that our method is effective in identifying features that are invariant across different environments. Moreover, we also observe that all ERM methods achieve lower validation scores with higher training scores, implying that these methods are prone to overfitting. Our method, on the other hand, achieves not only a higher validation score but also a higher corresponding training score, thereby demonstrating its ability to overcome the problem of easy overfitting and improve the generalization ability effectively.

## 6 Conclusion

In this work, we propose a new framework that learns invariant molecular representation against distribution shifts. We adopt a "first-encoding-then-separation" strategy, wherein a combination of encoding GNN and residual vector quantization is utilized to derive molecular representation in latent discrete space. Then we learn a scoring GNN to identify invariant features from this representation. Moreover, we design a task-agnostic self-supervised learning objective to enable precise invariance identification and versatile applicability to various tasks. Extensive experiments on real-world datasets demonstrate the superiority of our method on the molecular OOD problem. Overall, our proposed framework presents a promising approach for learning invariant molecular representations and offers valuable insights for addressing distribution shifts in molecular data analysis.