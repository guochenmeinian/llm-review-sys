# Change point detection and inference in multivariate non-parametric models under mixing conditions

Carlos Misael Madrid Padilla

Department of Mathematics

University of Notre Dame

cmadridp@nd.edu

 Haotian Xu

Department of Statistics

University of Warwick

haotian.xu.1@warwick.ac.uk

&Daren Wang

Department of Statistics

University of Notre Dame

dwang24@nd.edu

&Oscar Hernan Madrid Padilla

Department of Statistics

University of California, Los Angeles

oscar.madrid@stat.ucla.edu

&Yi Yu

Department of Statistics

University of Warwick

yi.yu.2@warwick.ac.uk

###### Abstract

This paper addresses the problem of localizing and inferring multiple change points, in non-parametric multivariate time series settings. Specifically, we consider a multivariate time series with potentially short-range dependence, whose underlying distributions have Holder smooth densities and can change over time in a piecewise-constant manner. The change points, which correspond to the times when the distribution changes, are unknown. We present the limiting distributions of the change point estimators under the scenarios where the minimal jump size vanishes or remains constant. Such results have not been revealed in the literature in non-parametric change point settings. As byproducts, we develop a sharp estimator that can accurately localize the change points in multivariate non-parametric time series, and a consistent block-type long-run variance estimator. Numerical studies are provided to complement our theoretical findings.

## 1 Introduction

Given a time series \(\{X_{t}\}_{t=1}^{T}^{p}\), which is assumed to be an \(\)-mixing sequence of random vectors with unknown marginal distributions \(\{P_{t}\}_{t=1}^{T}\). To incorporate the nonstationarity of \(\{X_{t}\}_{t=1}^{T}\), we assume that there exists \(K\) change points, namely \(\{_{k}\}_{k=1}^{K}\{2,...,T\}\) with \(1=_{0}<_{1}<<_{k} T<_{K+1}=T+1\), such that

\[P_{t} P_{t-1}t\{_{1},,_{K}\}.\] (1)

Our primary interest is to develop accurate estimators of \(\{_{k}\}_{k=1}^{K}\) and study their limiting properties. We refer to Assumption 1 for detailed technical conditions of our non-parametric change point model.

Nonstationary multivariate data are frequently encountered in real-world applications, including biology (e.g. Molenaar et al. 2009, Wolkovich & Donahue 2021), epidemiology (e.g. Azhar et al. 2021, Nguyen et al. 2021), social science (e.g. Kunitomo & Sato 2021, Cai et al. 2022), climatology (e.g. Corbella & Stretch 2012, Heo & Manuel 2022), finance (e.g. Herzel et al. 2002, Schmitt et al. 2013), neuroscience (e.g. Gorrostieta et al. 2019, Frolov et al. 2020), among others.

Due to the importance of modeling nonstationary data in various scientific fields, we have witnessed a soaring growth of statistical change point literature, (e.g. Aue et al. 2009, Fryzlewicz 2014, Cho & Fryzlewicz 2015, Cho 2016, Wang et al. 2020, Padilla et al. 2022). However, there are a few limitations in the existing works on multivariate non-parametric settings. Firstly, to the best of our knowledge, temporal dependence, which commonly appears in time series, has not been considered. Secondly, there is no localization consistency result for data with the underlying densities being Holder smooth with arbitrary degree of smoothness. Lastly and most importantly, the limiting distributions of change point estimators and the asymptotic inference for change points have not been well studied.

Taking into account the aforestated limitations, this paper examines change point problems in a fully non-parametric time series framework, wherein the underlying distributions are only assumed to have Holder smooth continuous densities and can change over time in a piecewise constant manner. The rest of the paper is organized as follows. In Section 2, we explain the model assumptions for multivariate time series with change points in a non-parametric setting. Section 3 details the two-step change point estimation procedure, as well as the estimators at each step. Theoretical results, including the consistency of the preliminary estimator and the limiting distribution of the final estimator, are presented in Section 4. Section 5 evaluates the practical performance of the proposed procedure via various simulations and a real data analysis. Finally, Section 6 concludes with a discussion.

**Notation.** For any function \(f:\;^{p}\) and for \(1 q<\), define \(\|f\|_{L_{q}}=(_{^{p}}|f(x)|^{q}dx)^{1/q}\) and for \(q=\), define \(\|f\|_{L_{}}=_{x^{p}}|f(x)|\). Define \(L_{q}=\{f:\;^{p},\;\|f\|_{q}<\}\). Moreover, for \(q=2\), define \( f,g_{L_{2}}=_{^{p}}f(x)g(x)dx\) where \(f,g:\;^{p}\). For any vector \(s=(s_{1},,s_{p})^{}^{p}\), define \(|s|=_{i=1}^{p}s_{i}\), \(s!=s_{1}! s_{p}!\) and the associated partial differential operator \(D^{s}=^{} x_{p}^{}}\). For \(>0\), denote \(\) to be the largest integer smaller than \(\). For any function \(f:\;^{p}\) that is \(||\)-times continuously differentiable at point \(x_{0}\), denote by \(f_{x_{0}}^{}\) its Taylor polynomial of degree \(||\) at \(x_{0}\), which is defined as \(f_{x_{0}}^{}(x)=_{|s|}(x-x_{0})^{s}/s!D^{s} f(x_{0})\). For a constant \(L>0\), let \(^{}(L,^{p})\) be the set of functions \(f:\;^{p}\) such that \(f\) is \(\)-times differentiable for all \(x^{p}\) and satisfy \(|f(x)-f_{x_{0}}^{}(x)| L|x-x_{0}|^{}\), for all \(x,x_{0}^{p}\). Here \(|x-x_{0}|\) is the Euclidean distance between \(x,x_{0}^{p}\). In non-parametric statistics literature, \(^{}(L,^{p})\) is often referred to as the class of Holder functions. For two positive sequences \(\{a_{n}\}_{n^{+}}\) and \(\{b_{n}\}_{n^{+}}\), we write \(a_{n}=O(b_{n})\) or \(a_{n} b_{n}\), if \(a_{n} Cb_{n}\) with some constant \(C>0\) that does not depend on \(n\), and \(a_{n}=(b_{n})\) or \(a_{n} b_{n}\), if \(a_{n}=O(b_{n})\) and \(b_{n}=O(a_{n})\). For a deterministic or random \(\)-valued sequence \(a_{n}\), write that a sequence of random variable \(X_{n}=O_{p}(a_{n})\), if \(_{M}_{n}(|X_{n}| Ma_{n})=0\). Write \(X_{n}=o_{p}(a_{n})\) if \(_{n}(|X_{n}| Ma_{n})=0\) for all \(M>0\). The convergences in distribution and probability are respectively denoted by \(}}{{}}\) and \(}{{}}\).

## 2 Model setup

Detailed assumptions imposed on the model (1) are collected in Assumption 1.

**Assumption 1**.: _The data \(\{X_{t}\}_{t=1}^{T}^{p}\) are generated based on model (1) and satisfy the following._ **a.** _For each \(t=\{1,,T\}\), the distribution \(P_{t}\) has a Lebesgue density function \(f_{t}:^{p}\), such that \(f_{t}^{r}(L,)\) with \(r,L>0\), where \(\) is the union of the supports of all \(f_{t}\), and \(\) has bounded Lebesgue measure._

**b.** _Let \(g_{t}\) be the joint density of \(X_{1}\) and \(X_{t+1}\). It satisfies that \(\|g_{t}\|_{L_{}}<\)._

**c.** _The minimal spacing between two consecutive change points \(=_{k=1}^{K+1}(_{k}-_{k-1})>0\)._

**d.** _The minimal jump size between two consecutive change points \(=_{k=1,,K}_{k}>0\), where \(_{k}=\|f_{_{k}}-f_{_{k+1}}\|_{L_{2}}\) denotes the jump size at the \(k\)th change point._

**e.** _The process \(\{X_{t}\}_{t}\) is \(\)-mixing with mixing coefficients_

\[_{k}=_{t}((X_{s},s t),(X_{s},s t +k)) e^{-2ck}$.}\] (2)

The minimal spacing \(\) and the minimal jump size \(\) are two key parameters characterizing the change point phenomenon. Assumption 1**d.** characterizes the changes in density functions through the function's \(L_{2}\)-norm, enabling us to detect local and global changes in non-parametric settings.

The decay rate of \(_{k}\) in Assumption 1**e.** imposes an upper bound on the temporal dependence. This is a standard requirement in the literature (e.g Abadi 2004, Merlevede et al. 2009).

Revolving the change point estimators, we are to conduct the estimation and inference tasks. For a sequence of estimators \(_{1}<<_{}\{1,,T\}\), our first task is to show the localization consistency, i.e. with probability tending to one as the sample size \(T\) grows unbounded, it holds that

\[=K_{k=1,,}_{k}- _{k},_{T}=0.\] (3)

We refer to \(\) as the localization error in the rest of this paper.

With a consistent estimation result, we further refine \(\{_{k}\}_{k=1}^{}\) and obtain \(\{_{k}\}_{k=1}^{}\) with error bounds \(_{k}-_{k}=O_{p}(1)\) and derive the limiting distribution of \((_{k}-_{k})_{k}^{+2}\).

We briefly summarize the contributions of our paper as follows.

* We develop a multivariate non-parametric seeded change point detection algorithm detailed Algorithm 1, which is based on the seeded binary segmentation method (SBS), proposed in Kovacs et al. (2020) in the univariate setting. To the best of our knowledge, we are the first to innovatively adapt SBS to a multivariate non-parametric change point model.
* Under the signal-to-noise ratio condition in Assumption 3 that \(^{2}(T)T^{p/(2r+p)}\), we demonstrate that the output of Algorithm 1 is consistent, with localization errors \(_{k}^{-2}T^{p/(2r+p)}(T)\), for \(k\{1,,K\}\). This localization error is first obtained for \(\)-mixing time series with a generic smoothness assumption, while the state-of-the-art method from Padilla et al. (2021) only focuses on Lipschitz smooth densities and under temporal independence.
* Based on the consistent estimators \(\{\}_{k=1}^{}\), we construct the refined estimators \(\{_{k}\}_{k=1}^{}\) and derive their limiting distributions in different regimes, as detailed in Theorem 2. These results are novel in the literature of change point and time series analysis.
* Extensive numerical results are presented in Section 5 to corroborate the theoretical findings. The code used for numerical experiments is available upon request prior to publication.

## 3 A two-step multivariate non-parametric change point estimators

In this section, we present the initial and refined change point estimators, both of which share the same building block, namely the non-parametric CUSUM statistic.

**Definition 1** (Non-parametric CUSUM statistic).: _For any integer triplet \(0 s<t<e T,\) let the CUSUM statistic be_

\[_{t,h}^{(s,e]}(x)=}_{i=s+1}^{t}F_ {i,h}(x)-}_{i=t+1}^{e}F_{i,h}(x),\ x^{p},\]

_where \(F_{t,h}\) is a kernel estimator of \(f_{t}\), i.e. \(F_{t,h}(x)=_{h}(x-X_{t})\) with the kernel function_

\[_{h}(x)=},  x^{p},\]

_accompanied with the bandwidth \(h>0\)._

The CUSUM statistic is a key ingredient of our algorithm and is based on the kernel estimator \(F_{t,h}()\). We highlight that kernel-based change-point estimation techniques have been employed in detecting change points in non-parametric models in existing literature, as demonstrated in, for instance, Arlot et al. (2019), Li et al. (2019), Padilla et al. (2021).

Our preliminary estimator is obtained by combining the CUSUM statistic in Definition 1 with a modified version of SBS based on a collection of deterministic seeded intervals defined in Definition 2.

**Definition 2** (Seeded intervals).: _Let \(= C_{}_{2}( )\), with some sufficiently large absolute constant \(C_{}>0\). For \(k\{1,,\}\), let \(_{k}\) be the collection of \(2^{k}-1\) intervals of length \(l_{k}=T2^{-k+1}\) that are evenly shifted by \(l_{k}/2=T2^{-k}\), i.e._

\[_{k}=\{((i-1)T2^{-k},\,(i-1)T2^{-k}+T2^{-k+1} ], i=1,,2^{k}-1\}.\]

_The overall collection of seeded intervals is denoted as \(=_{k=1}^{}_{k}\)._With the CUSUM statistics and the seeded intervals as building blocks, we are now ready to present our multivariate non-parametric seeded change point detection algorithm.

```
1:Sample \(\{X_{t}\}_{t=s}^{c}^{p}\), collection of seeded intervals \(\), tuning parameter \(>0\) and bandwidth \(h>0\). initialization: If \((s,e]=(0,n]\), set \(\) and set \((T)h^{-p}\). for\(=(,]\)do if\(=(,](s,e]\) and \(->2\)then \(b_{}*{arg\,max}_{+ t -}\|_{t,h}^{(,]}\|_{L_{2}}\) \(a_{}\|_{b_{},h}^{(,]}\|_{L_{2}}\) else \(a_{}-1\) endif endfor \(^{s,e}=\{:a_{}>\}\) if\(^{s,e}\)then \(^{*}*{arg\,min}_{^{ s,e}}||\) \(\{b_{^{*}}\}\) \(((s,b_{^{*}}),,,h)\) \(((b_{^{*}}+1,e),,,h)\) endif ```

**OUTPUT:** The set of estimated change points \(\). ```

**Algorithm 1** Multivariate non-parametric Seeded Binary Segmentation. MNSBS \(((s,e),,,h)\)

Algorithm 1 presents a methodological approach to addressing the problem of estimating multiple change points in multivariate time series data. At its core, the algorithm leverages the strength of seeded intervals, forming a multi-scale search mechanism. To identify potential change points, the method recursively employs the CUSUM statistics. For the functionality of the algorithm, specific inputs are required. These include the observed data set, represented as \(X_{t}^{}=1\), the seeded intervals denoted by \(\), the bandwidth \(h\) that is crucial for constructing the CUSUM statistics, and a threshold, \(\), which is instrumental in change point detection. We provide theoretical and numerical guidance for tuning parameters in Sections 4 and 5, respectively.

Delving deeper into the architecture of Algorithm 1, it becomes evident that the SBS functions as its foundational framework, while the nonparametric version of the CUSUM statistics acts as its functional units. The design of this algorithm is particularly tailored given its inclination toward nonparametric detection and its ability to identify multiple change points. The SBS is, in essence, an advanced version of the moving-window scanning technique. Its distinctive characteristic is its adaptability in handling the challenges posed by multiple change points that exhibit unpredictable spacing. Instead of being confined to a fixed window width, the SBS introduces versatility by incorporating a range of window width options. Each of these widths is methodically applied during a moving-window scan.

Based on the preliminary estimators \(\{_{k}\}_{k=1}^{}\) provided by Algorithm 1, we further develop a refinement procedure to enhance the localization accuracy. To be more specific, let

\[s_{k}=_{k-1}+_{k}  e_{k}=_{k+1}+_{k}.\] (4)

Then, the preliminary estimators \(\{_{k}\}_{k=1}^{}\) and \( h\) produce an estimator of \(_{k}\) as:

\[_{k}= \|_{k+1}-_{k}}{( _{k+1}-_{k-1})(_{k}-_{ k-1})}}_{i=_{k-1}+1}^{_{k}}F_{i,}- _{k}-_{k-1})}{(_{k+1}- _{k-1})(_{k+1}-_{k})}}_{i= _{k}+1}^{_{k+1}}F_{i,}\|_{L_{2}}\] \[_{k+1}-_{k-1}}{( _{k}-_{k-1})(_{k+1}-_ {k})}}.\] (5)We then propose the final change points estimators as

\[_{k}=*{arg\,min}_{s_{k}<<e_{k}}_{k}( ),\] (6)

where

\[_{k}()=_{t=s_{k}+1}^{}\|F_{t,h_{1}}-F_{(s_{k}, _{k}),h_{1}}\|_{L_{2}}^{2}+_{t=+1}^{e_{k}}\|F_{t,h_{1} }-F_{(_{k},e_{k}),h_{1}}\|_{L_{2}}^{2}},\]

with \(h_{1}=c_{_{k}}_{k}^{1/r}\) and \(F_{(s,e],h_{1}}=_{i=s+1}^{e}F_{i,h_{1}}\) for integers \(e>s\).

If the initial change point estimators are consistent, i.e. (3) holds with probability tending to \(1\), then the interval \((_{k-1},_{k+1})\) is anticipated to contain merely one undetected change point. By conservatively trimming this interval to \((s_{k},e_{k})\), we can safely any change points previously detected within \((_{k-1},_{k+1})\). Consequently, the trimmed interval \((s_{k},e_{k})\) contains only true change point \(_{k}\) with high probability. Due to the same reason, our choice of weight in Equation 4,1/10, is a convenient choice. In general, any constant weight between \(0\) and \(1/2\) would suffice. Inspired by Padilla et al. (2021), who proposed to use \(O(_{k})\) as an optimal bandwidth in the context of Lipschitz densities, we adopt \(h_{1}=O(_{k}^{1/r})\) as the bandwidth for our kernel density estimator. This choice incorporates the broader scope of our work, which studies a more general degree of smoothness. Notably, if the underlying density functions strictly adhere to the Lipschitz criterion and \(r=1\), our bandwidth selection aligns with that recommended by Padilla et al. (2021). We would like to emphasize that while the procedure proposed by Padilla et al. (2021) required knowledge of the population quantities \(_{k}\), our approach is adaptive as we provide data-driven methods to estimate \(_{k}\) accurately.

With our newly proposed estimators, in Theorem 2, we derive an improved error bound for the refined estimators \(\{_{k}\}_{k=1}^{}\) over the preliminary estimators \(\{_{k}\}_{k=1}^{}\). We also study the limiting distributions of the refined estimators. Section 4.4 and Section 5 will discuss the theoretically justified rates and practical choices of tuning parameters, respectively.

The computational complexity of Algorithm 1 is \(O(T^{2}(T)^{-1})\), where \(O(T^{2}(T)^{-1})\) is due to the computational cost of the SBS, and "Kernel" stands for the computational cost of numerical computation of the \(L_{2}\)-norm of the CUSUM statistics based on the kernel function evaluated at each time point. The dependence on the dimension \(p\) is only through the evaluation of the kernel function. The computational complexity of the final estimators (including estimating \(_{k}\)'s) is \(O(T)\). Therefore, the overall cost for deriving \(\{_{k}\}_{k=1}^{}\) is \(O(T^{2} T^{-1})\).

## 4 Consistent estimation and limiting distributions

To establish the theoretical guarantees of our estimators, we first state conditions needed for the kernel function \(()\).

**Assumption 2** (The kernel function).: _Assume that the kernel function \(()\) has compact support and satisfies the following additional conditions._

**a.** _For the Holder smooth parameter \(r\) in Assumption 1_**a**_, assume that \(()\) is adaptive to \(^{r}(L,^{p})\), i.e. for any \(f^{r}(L,^{p})\), it holds that_

\[_{x^{p}}_{^{p}}h^{-p} f(z)\,z-f(x)h^{r},\]

_for some absolute constant \(>0\) and tuning parameter \(h>0\)._

**b.** _The class of functions \(_{}=\{(x-)/h:\,^{p} ^{+},h>0\}\) is separable in \(L_{}(^{p})\) and is a uniformly bounded VC-class; i.e. there exist constants \(A,>0\) such that for any probability measure \(Q\) on \(^{p}\) and any \(u(0,\|\|_{L_{}})\), it holds that \((_{},L_{2}(Q),u)(A\|\|_{L_{ }}/u)^{}\), where \((_{},L_{2}(Q),u)\) denotes the \(u\)-covering number of the metric space \((_{},L_{2}(Q))\)._

**c.** _Let \(m= r\) and it holds that \(_{0}^{}t^{m-1}_{\|x\| t}|(x)|^{m}\,t< ,\ _{^{p}}(z)\|z\|\,z C_{K}\), where \(C_{K}>0\) is an absolute constant._Assumption 2 is a standard condition in the non-parametric literature (e.g. Gine and Guillou, 1999, 2001, Sriperumbudur and Steinwart, 2012, Kim et al., 2019, Padilla et al., 2021) and holds for various kernels, such as the Triweight, Epanechnikov and Gaussian kernels, which are considered in Section 5.

### Consistency of preliminary estimators

To establish the consistency of the preliminary estimators outputted by Algorithm 1, we impose the following signal-to-noise ratio condition.

**Assumption 3** (Signal-to-noise ratio).: _Assume there exists an arbitrarily slow diverging sequence \(_{T}>0\) such that_

\[^{2}>_{T}(T)T^{}.\]

We note that Assumption 3 is a mild condition, as it allows both the jump size \(\) to vanish asymptotically and/or the spacing \(\) between change points to be much smaller than \(T\). The consistency of Algorithm 1 is established in the following theorem.

**Theorem 1**.: _Suppose Assumptions 1, 2 and 3 hold. Let \(\{_{k}\}_{k=1}^{}\) be the estimated change points returned by Algorithm 1 with tuning parameters \(=c_{}T^{p/(4r+2p)}^{1/2}(T)\) and \(h=c_{h}T^{-1/(2r+p)}\) for sufficiently large constants \(c_{h},c_{}>0\). Then_

\[=K,\;|_{k}-_{k}| C_{ }_{k}^{-2}T^{}(T), k=1,,K} 1-3C _{p,}T^{-1},\]

_where \(C_{}\) and \(C_{p,}\) are positive constants only depending on the kernel and the dimension \(p\)._

### Refined estimators and their limiting distributions

To develop refined estimators based on the preliminary estimators and study their limiting distributions, we would need to require a slightly stronger signal-to-noise ratio condition below.

**Assumption 4** (Signal-to-noise ratio for inference).: _Assume that there exists an arbitrarily slow diverging sequence \(_{T}>0\) such that_

\[^{+3}>_{T}(T)T^{}.\]

Assumption 4 is slightly stronger than Assumption 3. This is because our refined estimators are based on a sequence of random endpoints, i.e. the preliminary estimators. This brings theoretical challenges in deriving limiting distributions and estimating the long-run variances. It is worth noting that a similar phenomenon has been observed in the study on conducted by Xu, Wang, Zhao and Yu (2022).

**Theorem 2**.: _Suppose that Assumptions 1, 2 and 3 hold. Let \(\{_{k}\}_{k=1}^{}\) be the refined change point estimators defined in Section 3, with the preliminary estimators \(\{_{k}\}_{k=1}^{}\) returned by Algorithm 1, the intervals \(\{(s_{k},c_{})\}_{k=1}^{}\) defined in (4), and \(_{k}\) defined as in (5). The following holds:_

**a.** _(Non-vanishing regime) Suppose the jump size at the change point_ \(_{k}\) _satisfies_ \(_{T}_{k}_{k}\) _for some absolute constant_ \(_{k}>0\)_. Then, as_ \(T\)_, it holds that_ \(|_{k}-_{k}|=O_{p}(1)\) _and that_

\[(_{k}-_{k})_{k}^{+2}\ }}{{}}\ *{arg\,min}_{}\,P_{k}( )\]

_where_

\[P_{k}()\] \[= _{t=+1}^{0}2F_{_{ k}+t,h_{2}}-f_{_{k}+t}*_{h_{2}},(f_{_{k}}-f_{_{k}+1})* _{h_{2}}_{L_{2}}+\|(f_{_{k+1}}-f_{ _{k}})*_{h_{2}}\|_{L_{2}}^{2},&<0;\\ 0,&=0;\\ _{t=1}^{}2F_{_{k}+t,h_{2}}-f_{_{k}+t}* _{h_{2}},(f_{_{k+1}}-f_{_{k}})*_{h_{2}} _{L_{2}}+\|(f_{_{k+1}}-f_{_{k}})*_{h_{2}}\|_{L _{2}}^{2},&>0.\]

_Here_ \(*\) _denotes convolution and_ \(h_{2}=c_{_{k}}_{k}^{1/r}\) _for some absolute constant_ \(c_{_{k}}>0\)_._

**b.** _(Vanishing regime) Suppose the jump size at the change point_ \(_{k}\) _satisfies_ \(_{T}_{k}=0\)_. Then, as_ \(T\)_, it holds that_ \(|_{k}-_{k}|=O_{p}(_{k}^{-2-p/r})\) _and that_

\[(_{k}-_{k})_{k}^{+2}\ }}{{}}\ *{arg\,min}_{}\,\{ _{}(k)B()+||\},\] (7)_where \(h_{2}=c_{_{k}}_{k}^{1/r}\) for some absolute constant \(c_{_{k}}>0\). Here_

\[B()=B_{1}(-),&<0,\\ 0,&=0,\\ B_{2}(),&>0,\]

_with \(B_{1}(r)\) and \(B_{2}(r)\) being two independent standard Brownian motions, and_

\[_{}^{2}(k)=_{T}^{ {k}-2}}{T}_{t=1}^{T}F_{t,h_{2}}-f_{t}* _{h_{2}},(f_{_{k}}-f_{_{k+1}})*_{h_{2}} _{L_{2}}.\] (8)

Theorem 2 considers vanishing and non-vanishing regimes of the jump sizes. The upper bounds on the localization error in both regimes can be written as

\[_{1 k K}|_{k}-_{k}|_{k}^{+2}= O_{p}(1).\]

Therefore, when the Holder smoothness parameter \(r=1\), our final estimator \(\{_{k}\}\) attains the minimax optimal convergence rate developed in Lemma 3 by Padilla et al. (2021). Furthermore, when \(r=1\), our resulting rate is sharper than that in Theorem 1 in Padilla et al. (2021), as we are able to remove the logarithmic factors from the upper bound. Additionally, our method can achieve optimal rates with choices of tuning parameters that do not depend on the unknown jump sizes \(_{k}\).

Theorem 2 summarizes the limiting distributions of the refined estimators \(\{_{k}\}_{k=1}^{}\). In the non-vanishing case, the resulting limiting distribution can be approximated by a two-sided random walk and the change points can be accurately estimated within a constant error rate. In contrast, in the vanishing regime, a central limit theorem under mixing conditions leads to a two-sided Brownian motion distribution in the limit, which quantifies the asymptotic uncertainty of \(\{_{k}\}_{k=1}^{}\), enabling inference on change point locations, and allowing for the construction of confidence intervals.

### Consistent long-run variance estimation

To obtain valid confidence intervals for change points using the limiting distributions in Theorem 2**b.**, it is crucial to access robust estimators for the long-run (asymptotic) variances \(\{_{}^{2}(k)\}_{k=1}^{}\) defined in (8). We propose a block-type long-run variance estimator in Algorithm 2 to fulfill this task and demonstrate its consistency in the following theorem.

**Theorem 3**.: _Suppose Assumptions 1, 2 and 3 hold. Let \(\{_{}^{2}(k)\}_{k=1}^{}\) be the population long-run variance defined in (8) and \(_{}^{2}(k)\) the output of Algorithm 2 with \(R=O(T^{(p+r)/(2r+p)}/_{k}^{p/(2r)+3/2})\). Then it holds that_

\[_{k=1}^{K}|_{}^{2}(k)-_{ }^{2}(k)|}{{}}0  T.\]

### Discussions on MNSBS

**Tuning parameters.** Our procedure comprises three steps: (1) preliminary estimation, (2) local refinement, and (3) confidence interval construction with three key tuning parameters. For step (1), we use a kernel density estimator with bandwidth \(h T^{-1/(2r+p)}\), which follows from the classical non-parametric literature (e.g. Yu 1993, Tsybakov 2009). The threshold tuning parameter \(\) is set to a high-probability upper bound on the CUSUM statistics when there is no change point, which reflects the requirement on the signal-to-noise ratio detailed in Assumption 3. For refined estimation in step (2) and long-run variance estimation in step (3), we set the bandwidth parameter \(h_{1}_{k}^{1/r}\). This choice of bandwidth is inspired by the minimax rate-optimal bandwidth used in Padilla et al. (2021).

**Comparison with Padilla et al. (2021).** Our main contribution is deriving the limiting distribution of multivariate non-parametric change point estimators. This problem has not been formally studied in the existing literature. Additionally, our Holder assumption is more general than the Lipschitz assumption used in Padilla et al. (2021). Our Assumption 1**d** specifies changes through the \(L_{2}\)-norm of probability density functions, which is a weaker assumption than the \(L_{}\)-norm used in Padillaet al. (2021). Furthermore, our assumptions allow for temporal dependence captured by \(\)-mixing coefficients, whereas Padilla et al. (2021) assumed independent observations.

**Comparison with existing literature in nonparametric, online, and inference change point.**

In the nonparametric change point literature, different kernel-based methods are adopted for change point localisation and testing. In the offline setting, the penalized kernel least squares estimator, originally introduced by Harchaoui & Cappe (2007), was explored by Arlot et al. (2012) for multivariate change point problems, and an oracle inequality was derived. An upper bound on the localization rate provided by this method was established by Garreau & Arlot (2018) and was computationally enhanced further in Celisse et al. (2018). With a focus on a so-called running maximum partition strategy, Harchaoui et al. (2008) formulated a kernel-based test statistic to ascertain the existence of a change-point. In a similar vein, Zou et al. (2014) investigated a problem where \(s\) out of \(n\) sequences are anomalous and devised a test statistic using the kernel maximum mean discrepancy.

In the online setting, Kifer et al. (2004) introduces a meta-algorithm comparing data from a "reference window" to current data using empirical measures. Desobry et al. (2005) detects shifts by comparing two descriptor sets from the signal's immediate past and future, using a dissimilarity metric resembling the Fisher ratio in Gaussian cases via a soft margin single-class SVM. Meanwhile, Liu et al. (2013) adopts density ratio estimation with a non-parametric Gaussian kernel model for change-point detection, updating its parameters online through stochastic gradient descent.

The core methodology is largely shared but with different goals and performance measurements regarding online and offline change point literature comparisons. How to conduct inference in the online change point context is also unclear.

Compared to the existing work, in this paper, we follow the suit of using kernel-based CUSUM statistics but incorporate temporal dependence, which is rarely seen in the literature. Most importantly, we are unaware of existing work on nonparametric change point inference, which is the main selling point of our paper.

Most change point inference work focuses on fixed-dimensional parameters as well as lacks tracking of many model parameters. Xu, Wang, Zhao & Yu (2022), in terms of style, is indeed the most closely related. But tackles high-dimensional linear regression, fundamentally distinct from our nonparametric density estimation.

## 5 Numerical Experiments

We refer to MNSBS as our final estimator, which is used for both localization and inference tasks. To evaluate its localization performance, we compare our proposed method against four competitors - MNP (Padilla et al. 2021), EMNCP (Matteson & James 2014), SBS (Cho & Fryzlewicz 2015) and DCBS (Cho 2016) - across a wide range of simulation settings, using corresponding R functions in changepoints (Xu, Padilla, Wang & Li 2022), ecp (James et al. 2019) and hdbinseg (Cho& Fryzlewicz 2018) packages. However, to the best of our knowledge, no competitor is currently available for the inference task.

**Tuning parameters.** For MNBS implementation, we use the Gaussian kernel and the false discovery rate control-based procedure of Padilla et al. (2021) for \(\) selection. Preliminary estimators are set as \(h=2(1/T)^{1/(2r+p)}\), while the second stage estimator has bandwidths respectively set as \(=0.05\) and \(h_{1}=2_{k}^{1/r}\). Selection of \(R=_{k=1}^{}\{e_{k}-s_{k}\}^{3/5} \) with \(\{(s_{k},e_{k})\}_{k=1}^{}\) is guided by Theorem 3 using \(\{(s_{k},e_{k})\}_{k=1}^{}\) from (4). For the confidence interval construction, we use \(\{_{k}\}_{k=1}^{}\) and \(\{_{}^{2}(k)\}_{k=1}^{}\) to estimate the required unknown quantities. We evaluate \(L_{2}\) based statistics in Change point estimation and Long-run variance estimation using the Subregion-Adaptive Vegas Algorithm1 with a maximum of \(10^{5}\) function evaluations.

**Evaluation measurements** For a given set of true change points \(=\{_{k}\}_{k=0}^{K+1}\), to assess the accuracy of the estimator \(}=\{_{k}\}_{k=0}^{+1}\) with \(_{0}=1\) and \(_{T+1}=T+1f\), we report (1) Misestimate: the proportion of mismatating \(K\) and (2) Scaled Hausdorff distance: \(d_{}(},)\), defined by \(d_{}(},)=\{_{x_{ E}}}_{y}\{|x-y|\},_{y }}_{x}\{|x-y|\}\}\).

The performance of our change point inference is measured by the coverage of \(_{k}\), defined as \(cover_{k}(1-)\) for significance level \((0,1)\). For, \(k=1,,K\),

\[cover_{k}(1-)=1_{k}_{k}+_{u}(/2)}{_{k}^{p/r+2}},\, _{k}+_{u}(1-/2)}{_{k}^{p/r+2}} },\]

with \(_{u}(/2)\) and \(_{u}(1-/2)\) are the \(/2\) and \(1-/2\) empirical quantiles of the simulated limiting distribution given in (7), \(_{k}\) is defined in (5), and \(k=1,,K\).

### Localization

We consider three different scenarios with two equally spaced change points. For each scenario, we set \(r=2\), and vary \(T\{150,300\}\) and \(p\{3,5\}\). Moreover, we consider \(\{Y_{t}=1\{ T/3<t 2T/3\}Z_{t}+X_{t}\}_{t=1}^{T} ^{p}\) with \(X_{t}=0.3X_{t-1}+_{t}\).

\(\)**Scenario 1 (S1)** Let \(Z_{t}=^{p}\), where \(_{j}=0\) for \(j\{1,, p/2\}\) and \(_{j}=2\) otherwise. Let \(\{_{t}\}\) be i.i.d. \((0_{p},I_{p})\).

\(\)**Scenario 2 (S2)** Let \(Z_{t}|\{u_{t}=1\}=1.5 1_{p}\), \(Z_{t}|\{u_{t}=0\}=-1.5 1_{p}\), where \(\{u_{t}\}\) are i.i.d. Bernoulli\((0.5)\) random variables. Let \(\{_{t}\}\) be i.i.d. \((0_{p},I_{p})\).

\(\)**Scenario 3 (S3)** Let \(Z_{t}=0.3Z_{t-1}+0.5 1_{p}+_{t}^{*}\), where \(\{_{t}^{*}\}^{p}\) and \(\{_{t}\}^{p}\) are mutually independent. They are i.i.d. with entries independently follow \((-,)\) and the standardized Pareto\((3,1)\), respectively.

**S1-S3** encompass a variety of simulation settings, including the same type of distributions, changed mean and constant covariance in **S1**; a mixture of distributions in **S2**; and change between light-tailed and heavy-tailed distributions in **S3**. We conduct \(200\) repetitions of each experiment and present the results for localization in Figure 1. Our proposed method, MNBS, generally outperforms all other methods in all scenarios, except for **S2**, where ECP performs better. However, we observe that MNSBS achieves comparable performance to ECP for large \(T\) in **S2**.

### Inference

In this section, we focus solely on analyzing the limiting distribution obtained in Theorem 2.**b.**, which pertains to the vanishing regime. We explain this from two different perspectives. Firstly, in the non-vanishing regime (Theorem 2.**a.**), the localization error is at the order of \(O(1)\). As a result, the construction of confidence intervals, which is a direct application of the limiting distribution, is of little demand with such estimation results. Secondly, since the localization error is only at the order of \(O(1)\), the universality cannot come into play to produce a useful limiting distribution.

We consider the process \(\{Y_{t}=1\{ T/2<t T\}+X_{t}\}_{t=1}^{T}\), with \(X_{t}=0.3X_{t-1}+_{t}\), Here, \(=1_{p}\) and \(\{_{t}\}\) are i.i.d. \((0_{p},I_{p})\). We vary \(T\{100,200,300\}\) and \(p\{2,3\}\), and observe that our localization results are robust to the bandwidth parameters, yet sensitive to the smoothness parameter \(r\). We thus set \(r=1000\) in our simulations, as the density function of a multivariate normal distribution belongs to the Holder function class with \(r=\). Table 1 shows that our proposed inference procedure produces good coverage in the considered setting.

## 6 Conclusion

We tackle the problem of change point detection for short range dependent multivariate non-parametric data, which has not been studied in the literature. Our two-stage algorithm MNSBS can consistently estimate the change points in stage one, a novelty in the literature. Then, we derived limiting distributions of change point estimators for inference in stage two, a first in the literature.

Our theoretical analysis reveals multiple challenging and interesting directions for future exploration. Relaxing the assumption \( T\) may be of interest. In addition, in Theorem 2.**a**, we can see the limiting distribution is a function of the data-generating mechanisms, lacking universality, therefore deriving a practical method to derive the limiting distributions in the non-vanishing regime may be interesting.

    &  &  \\ \(n\) & \((1-)\) & \((1-)\) & \((1-)\) & \((1-)\) \\   &  & \\
100 & 0.864 & 17.613 (6.712) & 0.812 & 14.005 (5.639) \\
200 & 0.904 & 22.940 (7.740) & 0.838 & 18.407 (6.541) \\
300 & 0.993 & 26.144 (9.027) & 0.961 & 20.902 (5.936) \\  &  & \\
100 & 0.903 & 15.439 (5.792) & 0.847 & 11.153 (4.361) \\
200 & 0.966 & 20.108 (7.009) & 0.949 & 13.920 (5.293) \\
300 & 0.981 & 22.395 (6.904) & 0.955 & 15.376 (4.763) \\   

Table 1: Results for change point inference.

Figure 1: From top to bottom: Misestimation rate of the number of change point \(K\); Scaled Hausdorff distances. From left to right: Scenarios **S1-S3**. Different colors represent different methods, ordered as MNSBS, NMP, ECP, SBS, DCBS.