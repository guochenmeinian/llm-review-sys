# Conformalized matrix completion

Yu Gui  Rina Foygel Barber  Cong Ma

Department of Statistics, University of Chicago

{ygui,rina,congm}@uchicago.edu

###### Abstract

Matrix completion aims to estimate missing entries in a data matrix, using the assumption of a low-complexity structure (e.g., low rank) so that imputation is possible. While many effective estimation algorithms exist in the literature, uncertainty quantification for this problem has proved to be challenging, and existing methods are extremely sensitive to model misspecification. In this work, we propose a distribution-free method for predictive inference in the matrix completion problem. Our method adapts the framework of conformal prediction, which provides confidence intervals with guaranteed distribution-free validity in the setting of regression, to the problem of matrix completion. Our resulting method, conformalized matrix completion (cmc), offers provable predictive coverage regardless of the accuracy of the low-rank model. Empirical results on simulated and real data demonstrate that cnc is robust to model misspecification while matching the performance of existing model-based methods when the model is correct.

## 1 Introduction

Matrix completion, the task of filling in missing entries in a large data matrix, has a wide range of applications such as gene-disease association analysis in bioinformatics (Natarajan and Dhillon, 2014), collaborative filtering in recommender systems (Rennie and Srebro, 2005), and panel data prediction and inference in econometrics (Amjad et al., 2018; Bai and Ng, 2021; Athey et al., 2021). If the underlying signal is assumed to be low-rank, a range of estimation algorithms have been proposed in the literature, including approaches based on convex relaxations of rank (Candes and Plan, 2010; Candes and Tao, 2010; Koltchinskii et al., 2011; Foygel and Srebro, 2011; Negahban and Wainwright, 2012; Yang and Ma, 2022) and nonconvex optimization over the space of low-rank matrices (Keshavan et al., 2010; Jain et al., 2013; Sun and Luo, 2016; Ma et al., 2020).

While the problem of estimation has been explored through many different approaches in the literature, the question of uncertainty quantity for matrix completion remains challenging and under-explored. Cai et al. (2016) and Carpentier et al. (2018) construct confidence intervals for the missing entries using order-wise concentration inequalities, which could be extremely loose. Chen et al. (2019) proposes a de-biased estimator for matrix completion and characterizes its asymptotic distribution, which then yields entrywise confidence intervals are then derived for the underlying incoherent matrix with i.i.d. Gaussian noise. See also Xia and Yuan (2021); Farias et al. (2022) for related approaches.

The effectiveness of the aforementioned uncertainty quantification methods rely heavily on the exact low-rank structure as well as assumptions on the tail of the noise. In practice, the low-rank assumption can only be met approximately, and heavy-tailed noise is quite common in areas such as macroeconomics and genomics (Nair et al., 2022). Without exact model specifications, the solution to the matrix completion algorithm is hard to interpret and the asymptotic distribution of the obtained estimator is no longer valid. The dependence of model-based approaches on exact model assumptions motivates us to answer the question: _can we construct valid and short confidence intervals for themissing entries that are free of both model assumptions and of constraints on which estimation algorithms we may use?_

### Conformal prediction for distribution-free uncertainty quantification

Our proposed method is built on the idea of conformal prediction. The conformal prediction framework, developed by Vovk et al. (2005) and Shafer and Vovk (2008), has drawn significant attention in recent years in that it enables the construction of distribution-free confidence intervals that are valid with exchangeable data from any underlying distribution and with any "black-box" algorithm. To reduce computation in full conformal prediction, variants exist based on data splitting (Vovk et al., 2005; Lei et al., 2018), leave-one-out (Barber et al., 2021), cross-validation (Vovk, 2015; Barber et al., 2021), etc.

When distribution shift is present, with covariate shift as a special case, the exchangeability is violated, and Tibshirani et al. (2019) proposes a more general procedure called weighted conformal prediction that guarantees the validity under the weighted exchangeability of data. Barber et al. (2022) further relaxes the exchangeability assumption on the data and characterizes the robustness of a generalized weighted approach via an upper bound for the coverage gap. Other related works (Lei and Candes, 2020; Candes et al., 2021; Jin et al., 2023) adapt the conformal framework to applications in causal inference, survival analysis, etc., and study the robustness of weighted conformal prediction with estimated weights.

### Main contributions

In this paper, we adapt the framework of conformal prediction for the problem of matrix completion, and make the following contributions:

1. We construct distribution-free confidence intervals for the missing entries in matrix completion via conformal prediction. The validity is free of any assumption on the underlying matrix and holds regardless of the choice of estimation algorithms practitioners use. To achieve this, we prove the (weighted) exchangeability of unobserved and observed units when they are sampled (possibly nonuniformly) without replacement from a finite population.
2. When the sampling mechanism is unknown, we develop a provable lower bound for the coverage rate which degrades gracefully as the estimation error of the sampling probability increases.
3. In addition, to improve computational efficiency when faced with a large number of missing entries, we propose a one-shot conformalized matrix completion approach that only computes the weighted quantile once for all missing entries.

## 2 Problem setup

In this section, we formulate the matrix completion problem and contrast our distribution-free setting with the model-based settings that are more common in the existing literature. For a partially-observed \(d_{1} d_{2}\) matrix, the subset \([d_{1}][d_{2}]\) denotes the set of indices where data is observed--that is, our observations consist of \((M_{ij})_{(i,j)}\). In much of the matrix completion literature, it is common to assume a signal-plus-noise model for the observations,

\[M_{ij}=M^{*}_{ij}+E_{ij},\]

where \(^{*}=(M^{*}_{ij})_{(i,j)[d_{1}][d_{2}]}\) is the "true" underlying signal while \((E_{ij})_{(i,j)}\) denotes the (typically zero-mean) noise. Frequently, it is assumed that \(^{*}\) is low-rank, or follows some other latent low-dimensional structure, so that recovering \(^{*}\) from the observed data is an identifiable problem. In works of this type, the goal is to construct an estimator \(}\) that accurately recovers the underlying \(^{*}\). In contrast, in a more model-free setting, we may no longer wish to hypothesize a signal-plus-noise model, and can instead assume that we are observing a random subset of entries of a _deterministic_ matrix \(\); in this setting, estimating \(\) itself becomes the goal.

For both frameworks, many existing results focus on the problem of _estimation_ (either of \(^{*}\) or of \(\)), with results establishing bounds on estimation errors under various assumptions. For instance, see Candes and Plan (2010); Candes and Tao (2010); Negahban and Wainwright (2012); Chen et al. (2020) for results on estimating \(^{*}\) (under stronger conditions, e.g., a low-rank and incoherent signal,and zero-mean sub-Gaussian noise), or Srebro and Shraibman (2005); Foygel and Srebro (2011) for results on estimating \(\) (under milder conditions); note that in the latter case, it is common to instead refer to _predicting_ the entries of \(\), e.g., temperature or stock market returns, since we often think of these as noisy observations of some underlying signal. On the other hand, relatively little is known about the problem of _uncertainty quantification_ for these estimates: given some estimator \(}\), _can we produce a confidence interval around each \(_{ij}\) that is (asymptotically) guaranteed to contain the target with some minimum probability?_

The results of Chen et al. (2019) address this question under strong assumptions, namely, a low-rank and incoherent signal \(^{*}\), plus i.i.d. Gaussian noise \((E_{ij})_{(i,j)[d_{1}][d_{2}]}\). Zhao and Udell (2020) proposes another inferential procedure under the Gaussian copula assumption for the data. In this work, we provide a complementary answer that instead addresses the model-free setting: without relying on assumptions, we aim to produce a provably valid confidence interval for the entries \(M_{ij}\) of \(\) (not of \(^{*}\), because without assuming a low-rank model, there is no meaningful notion of an "underlying signal").

To do so, we will treat the matrix \(\) as deterministic, while the randomness arises purely from the random subset of entries that are observed. More specifically, we assume

\[Z_{ij}=\{(i,j)\}(p_{ ij}),\ \ \ (i,j)[d_{1}][d_{2}],\] (1)

and let \(=\{(i,j)[d_{1}][d_{2}]:Z_{ij}=1\}\) be the subset of observed locations. We consider the setting where the sample probabilities \(p_{ij}\)'s are nonzero. After observing \(_{}=(M_{ij})_{(i,j)}\), our goal is to provide confidence intervals \(\{(i,j)\}_{(i,j)^{c}}\) for all unobserved entries, with \(1-\) coverage over the unobserved portion of the matrix--that is, we would like our confidence intervals to satisfy

\[[(;,)]  1-,\] (2)

where

\[(;,)=^{c} |}_{(i,j)^{c}}M_{ij} {C}(i,j)}\] (3)

measures the "average coverage rate" of the confidence intervals.1 The goal of this work is to provide an algorithm for constructing confidence intervals \(\{(i,j)\}_{(i,j)^{c}}\) based on the observed entries \(_{}\), satisfying (2) with no assumptions on \(\). In particular, given the strong success of various matrix completion algorithms for the problem of estimation, we would like to be able to provide uncertainty quantification around any base estimator--i.e., given any algorithm for producing an estimate \(}\), our method constructs confidence intervals \(\{(i,j)\}_{(i,j)^{c}}\) around this initial estimate.

**Notation.** For a matrix \(=(A_{ij})^{m_{1} m_{2}}\), we define \(\|\|_{}_{(i,j)[m_{1}][m_{2}]}|A_{ij}|\), and \(\|\|_{1}_{(i,j)[m_{1}][m_{2}]}|A_{ij}|\). We use \(_{i,.}\) and \(_{.,j}\) to refer to the \(i\)th row and \(j\)th column of \(\), respectively. For any \(t\), \(_{t}\) denotes the distribution given by a point mass at \(t\).

## 3 Conformalized matrix completion

We next present our method, conformalized matrix completion (cmc). Our procedure adapts the split conformal prediction method (Vovk et al., 2005) to the problem at hand. As is standard in the conformal prediction framework, the goal of cmc is to provide uncertainty quantification (i.e., confidence intervals) around the output of any existing estimation procedure that the analyst chooses to use--this is a core strength of the conformal methodology, as it allows the analyst to use state-of-the-art estimation procedures without compromising on the validity of inference. At a high level, the cmc procedure will output a confidence interval of the form

\[(i,j)=_{ij}_{ij}\]

for the matrix value \(M_{ij}\) at each unobserved entry \((i,j)\). Here, after splitting the observed entries into a training set and a calibration set, the training set is used to produce \(_{ij}\) (a point estimate for the target value \(M_{ij}\)) and \(_{ij}\) (a scaling parameter that estimates our uncertainty for this point estimate); then, the calibration set is used to tune the scalar \(\), to adjust the width of these confidence intervals and ensure coverage at the desired level \(1-\).2

```
1:Input: target coverage level \(1-\); data splitting proportion \(q(0,1)\); observed entries \(_{}\).
2: Split the data: draw \(W_{ij}}}{{}}(q)\), and define training and calibration sets \[_{}=\{(i,j):W_{ij}=1\}, _{}=\{(i,j):W_{ij}=0\}.\]
3: Using the training data \(_{_{tr}}\) indexed by \(_{}[d_{1}][d_{2}]\), compute: * An initial estimate \(}\) using any matrix completion algorithm (with \(_{ij}\) estimating the target \(M_{ij}\)); * Optionally, a local uncertainty estimate \(}\) (with \(_{ij}\) estimating our relative uncertainty in the estimate \(_{ij}\)), or otherwise set \(_{ij} 1\); * An estimate \(}\) of the observation probabilities (with \(_{ij}\) estimating \(p_{ij}\), the probability of entry \((i,j)\) being observed).
4: Compute normalized residuals on the calibration set, \(R_{ij}=-_{ij}|}{_{ij}},\;(i,j)_{}\).
5: Compute estimated odds ratios for the calibration set and test set, \(_{ij}=_{ij}}{_{ij}},\;(i,j) _{}^{c}\), and then compute weights for the calibration set and test point, \[_{ij}=_{ij}}{_{(i^{},j^{ })_{}}_{i^{}j^{}}+ ,j^{})^{c}}{}_{i^{ }j^{}}},\;(i,j)_{},\;\;_{ }=^{c}}{}_{ij} }{_{(i^{},j^{})_{}}_{i^{ }j^{}}+,j^{})^{c}}{ }_{i^{}j^{}}}.\] (4)
6: Compute threshold \(=_{1-}(_{(i,j)_{ }}_{ij}_{R_{ij}}+_{ }_{+})\).
7:Output: confidence intervals \((i,j)=_{ij}_{ij}\) for each unobserved entry \((i,j)^{c}\). ```

**Algorithm 1** Conformalized matrix completion (cmc)

### Exchangeability and weighted exchangeability

We split the set of observed indices \(\) into a training and a calibration set \(=_{}_{}\) as is shown in Algorithm 1. We should notice that the sampling without replacement may introduce implicit dependence as well as a distribution shift from the i.i.d. sampling. Thus the two sets are dependent, which is different from the split conformal methods in regression problems. Before we present the coverage guarantees for our method, we first examine the role of (weighted) exchangeability in this method, to build intuition for how the method is constructed.

#### 3.1.1 Intuition: the uniform sampling case

First, for intuition, consider the simple case where the entries are sampled with constant probability, \(p_{ij} p\). If this is the case, then the set of calibration samples and the test point are exchangeable--that is, for \((i_{*},j_{*})\) denoting the test point that is drawn uniformly from \(^{c}\), if we are told that the combined set \(_{}\{(i_{*},j_{*})\}\) is equal to \(\{(i_{1},j_{1}),,(i_{n_{}}+1,j_{n_{}}+1)\}\) in no particular order (where \(n_{}=|_{}|\)), then the test point location \((i_{*},j_{*})\) is _equally likely_ to be at any one of these \(n_{}+1\) locations. Consequently, by exchangeability, the calibration residuals \(\{R_{ij}:(i,j)_{}\}\), defined in Algorithm 1 above, are exchangeable with the test residual \(R_{i_{*},j_{*}}=|M_{i_{*},j_{*}}-_{i_{*},j_{*}}|/_{i_{*},j_{*}}\); as a result, we can construct our confidence interval \((i_{*},j_{*})\) with \(\) determined by the \((1-)\)-quantile of the calibration residuals \(\{R_{ij}:(i,j)_{}\}\).

Indeed, this is exactly what \(\) does in this case: if we are aware that sampling is uniform, it would naturally follow that we estimate \(_{ij}_{0}\) by some (potentially data-dependent) scalar \(_{0}\); consequently, all the weights are given by \(_{ij}}+1}\) and \(_{}=}+1}\), and thus \(\) is the (unweighted) \((1-)\)-quantile of the calibration residuals (with a small correction term, i.e., the term \(+_{}_{+}\) appearing in the definition of \(\), to ensure the correct probability of coverage at finite sample sizes).

#### 3.1.2 Weighted exchangeability for the matrix completion problem

Next, we move to the general case, where now the sampling may be highly nonuniform. First suppose the sampling probabilities \(p_{ij}\) are known, and suppose again that we are told that the combined set \(_{}\{(i_{*},j_{*})\}\) is equal to \(\{(i_{1},j_{1}),,(i_{n_{}+1},j_{n_{}+1})\}\) in no particular order. In this case, the test point \((i_{*},j_{*})\) is _not_ equally likely to be at any one of these \(n_{}+1\) locations. Instead, we have the following result:

**Lemma 3.1**.: _Following the notation defined above, if \((i_{*},j_{*})(^{c})\), it holds that_

\[(i_{*},j_{*})=(i_{k},j_{k})_{} \{(i_{*},j_{*})\}=\{(i_{1},j_{1}),,(i_{n_{}+1},j_{n_{ }+1})\},_{}}=w_{i_{k}j_{k}},\]

_where we define the weights \(w_{i_{k}j_{k}}=j_{k}}}{_{k^{}=1}^{}h_{i_{k^{ }}j_{k^{}}}}\) for odds ratios given by \(h_{ij}=}{p_{ij}}\)._

Consequently, while the test point's residual \(R_{i_{*}j_{*}}=|M_{i_{*}j_{*}}-_{i_{*}j_{*}}|/. _{i_{*}j_{*}}.\) is not exchangeable with the calibration set residuals \(\{R_{ij}:(i,j)_{}\}\), the framework of weighted exchangeability (Tibshirani et al., 2019) tells us that we can view \(R_{i_{*}j_{*}}\) as a draw from the _weighted_ distribution that places weight \(w_{i_{k}j_{k}}\) on each residual \(R_{i_{k}j_{k}}\)--and in particular,

\[M_{i_{*}j_{*}}_{i_{*}j_{*}} q^{*}_{i_{*}j _{*}}_{i_{*}j_{*}}}=\{R_{i_{*}j_{*}}  q^{*}_{i_{*}j_{*}}\} 1-,\]

where \(q^{*}_{i_{*},j_{*}}=_{1-}_{k=1}^{n_{}+1}w_{i_{k}j_{k}}_{R_{i_{k}j_{k}}}\). Noting that this threshold \(q^{*}_{i_{*},j_{*}}\) may depend on the test location \((i_{*},j_{*})\), to accelerate the computation of prediction sets for all missing entries, we instead propose a one-shot weighted conformal approach by upper bounding the threshold uniformly over all test points: \(q^{*}_{i_{*},j_{*}} q^{*}:=_{1-}(_{(i,j) _{}}w^{*}_{ij}_{R_{ij}}+w^{*}_{ }_{+})\), for \((i,j)_{}\),

\[w^{*}_{ij}=}{_{(i^{},j^{})_{}}h_{i^{}j^{}}+_{(i^{},j^{})^{c}}h _{i^{}j^{}}},\ w^{*}_{}=^{c}}h_{ij}}{_{(i^{},j^{})_{}}h_{i^{ }j^{}}+_{(i^{},j^{})^{c}}h_{i^{ }j^{}}}.\] (5)

Now the threshold \(q^{*}\) no longer depends on the location \((i_{*},j_{*})\) of the test point3. If the true probabilities \(p_{ij}\) were known, then, we could define "oracle" conformal confidence intervals

\[^{*}(i,j)=_{ij} q^{*}_{ij},(i,j) ^{c}.\]

As we will see in the proofs below, weighted exchangeability ensures that, if we do have oracle knowledge of the \(p_{ij}\)'s, then these confidence intervals satisfy the goal (2) of \(1-\) average coverage. Since the \(p_{ij}\)'s are not known in general, our algorithm simply replaces the oracle weights \(^{*}\) in (5) with their estimates \(}\) defined in (4), using \(_{ij}\) as a plug-in estimate of the unknown \(p_{ij}\).

### Theoretical guarantee

In the setting where the \(p_{ij}\)'s are not known but instead are estimated, our \(\) procedure simply repeats the procedure described above but with weights \(}\) calculated using \(_{ij}\)'s in place of \(p_{ij}\)'s. Our theoretical results therefore need to account for the errors in our estimates \(_{ij}\), to quantify the coverage gap of conformal prediction with the presence of these estimation errors. To do this, we define an estimation gap term

\[=_{(i,j)_{}\{(i_{*},j_{*}) \}}|_{ij}}{_{(i^{},j^{}) _{}\{(i_{*},j_{*})\}}_{i^{}j^{}}}- }{_{(i^{},j^{})_{}\{(i_ {*},j_{*})\}}h_{i^{}j^{}}}|,\] (6)where \((i_{*},j_{*})\) denotes the test point. Effectively, \(\) is quantifying the difference between the "oracle" weights, \(w^{}_{ij}\), defined in (5), and the estimated weights, \(_{ij}\), defined in (4), except that \(\) is defined relative to a specific test point, while the weights \(}\) (and \(^{*}\), for the oracle version) provide a "one-shot" procedure that is universal across all possible test points, and is thus slightly more conservative.

**Theorem 3.2**.: _Let \(}\), \(}\), and \(}\) be estimates constructed using any algorithms, which depend on the data only through the training samples \(_{_{}}\) at locations \(_{}\). Then, under the notation and definitions above, conformalized matrix completion (cmc) satisfies_

\[(;,)  1--[].\]

In the homogeneous case, where we are aware that sampling is uniform (i.e., \(p_{ij} p\) for some potentially unknown \(p(0,1)\)), our error in estimating the weights is given by \( 0\) (since \(h_{ij}\) is constant across all \((i,j)\), and same for \(_{ij}\)--and therefore, the true and estimated weights are all equal to \(}+1}\)). In this setting, therefore, we would achieve the exact coverage goal (2), with \((;,)\) guaranteed to be at least \(1-\).

### Examples for missingness

To characterize the coverage gap explicitly, we present the following concrete examples for \(\) and show how the coverage gap \(\) can be controlled. Technical details are shown in the Supplementary Material.

#### 3.3.1 Logistic missingness

Suppose that the missingness follows a logistic model, with \((p_{ij}/(1-p_{ij}))=-(h_{ij})=u_{i}+v_{j}\) for some \(^{d_{1}}\) and \(^{d_{2}}\), where we assume \(^{}=0\) for identifiability. This model is closely related to logistic regression with a diverging number of covariates (Portnoy, 1988; He and Shao, 2000; Wang, 2011). Following Chen et al. (2023), we estimate \(\), \(\) via maximum likelihood,

\[},}=_{,}\ (,) ^{}=0.\]

Here the log-likelihood is defined by

\[(,)=_{(i,j)[d_{1}][d_{2}]}\{-(1+ (-u_{i}-v_{j}))+_{(i,j)_{}^{c}} (1-q+(-u_{i}-v_{j}))\}.\]

Consequently, we define the estimate of \(p_{ij}\) as \(_{ij}=(1+(-_{i}-_{j}))^{-1}\), for which one can show that \([],d_{2}\})}{\{d_{1},d_ {2}\}}}\). The proof of this upper bound is shown in Section A.3.1.

#### 3.3.2 Missingness with a general link function

More broadly, we can consider a general link function, where we assume \((p_{ij}/(1-p_{ij}))=-(h_{ij})=(A_{ij})\), where \(\) is a link function and \(^{d_{1} d_{2}}\) is the model parameter. The logistic model we introduced above corresponds to the case when \(\) is the identity function and \(=^{}+^{}\) is a rank-2 matrix. In this general setup, if \(()=k^{*}\) and \(\|\|_{}\), we can apply one-bit matrix completion (Davenport et al., 2014) to estimate \(\{p_{ij}\}\). More precisely, we define the log-likelihood

\[_{_{}}()=_{(i,j)_ {}}((B_{ij}))+_{(i,j)_{}^{c}} (1-(B_{ij})),\]

where \((t)=q(1+e^{-(t)})^{-1}\) (here rescaling by the constant \(q\) is necessary to account for subsampling the training set \(_{}\) from the observed data indices \(\)), and solve the following optimization problem

\[}=_{}\ _{_{}}( )\|\|_{*} d_{1}d_{2}},\ \ \|\|_{},\] (7)

where \(\|\|_{*}\) is the nuclear norm of \(\). Consequently, we let \(_{ij}=(-(_{ij}))\), which leads to \([]\{d_{1},d_{2}\}^{-1/4}\). The proof of this upper bound is shown in Section A.3.2.

Simulation studies

In this section, we conduct numerical experiments to verify the coverage guarantee of conformalized matrix completion (cmc) using both synthetic and real datasets. As the validity of cmc is independent of the choice of base algorithm, we choose alternating least squares (als) (Jain et al., 2013) as the base algorithm (results using a convex relaxation (Fazel et al., 2004; Candes and Recht, 2012; Chen et al., 2020) are shown in the Supplementary Material). We use cmc-als to refer to this combination. We use \(()\) (the average coverage, defined in (3)), and average confidence interval length, \(()=^{c}|}_{(i,j) ^{c}}(i,j)\), to evaluate the performance of different methods. All the results in this section can be replicated with the code available at https://github.com/yugjerry/conf-mc.

### Synthetic dataset

Throughout the experiments, we set the true rank \(r^{*}=8\), the desired coverage rate \(1-=0.90\), and report the average results over 100 random trials.

**Data generation.** In the synthetic setting, we generate the data matrix by \(=^{*}+\), where \(^{*}\) is a rank \(r^{*}\) matrix, and \(\) is a noise matrix (with distribution specified below). The low-rank component \(^{*}\) is generated by \(^{*}=^{*}^{*}\), where \(^{*}^{d_{1} r^{*}}\) is the orthonormal basis of a random \(d_{1} r^{*}\) matrix consisting of i.i.d. entries from a certain distribution \(_{u,v}\) (specified below) and \(^{*}^{d_{2} r^{*}}\) is independently generated in the same manner. In addition, the constant \(\) is chosen such that the average magnitude of the entries \(|M^{*}_{ij}|\) is \(2\). Following the observation model (1), we only observe the entries in the random set \(\).

**Implementation.** First, when implementing \(\), we adopt \(r\) as the hypothesized rank of the underlying matrix. For model-based methods (Chen et al., 2019), by the asymptotic distributional characterization \(_{ij}-M^{*}_{ij}(0,^{2}_{ ij})\) and \(E_{ij}(_{ij}-M^{*}_{ij})\), one has \(_{ij}-M_{ij}(0,^{2}_{ ij}+^{2})\). Consequently, the confidence interval for the model-based methods can be constructed as

\[_{ij} q_{1-/2}^{2}_{ij}+^{2}},\]

where \(q_{}\) is the \(\)th quantile of \((0,1)\). The estimates are obtained via \(^{2}=\|_{_{}}-}_{_{}}\|^{2}_{}/|_{ }|\) and \(^{2}_{ij}=(^{2}/)(\|}_{i,}\|^{2}+\|}_{j,}\|^{2})\) with the SVD \(}=}}}^{}\). For cmc-als, we obtain \(}\), \(}\) from \(_{_{}}\) by running \(\) on this training set (and taking \(_{ij}=(^{2}_{ij}+^{2})^{1/2}\)). The probabilities \(_{ij}\)'s are estimated via \(_{ij}(d_{1}d_{2}q)^{-1}|_{}|\) for this setting where the \(p_{ij}\)'s are homogeneous.

#### 4.1.1 Homogeneous missingness

In the homogeneous case, we consider the following four settings with \(d_{1}=d_{2}=500\):

* **Setting 1: large sample size + Gaussian noise**. \(p=0.8\), \(_{u,v}=(0,1)\), and \(E_{ij}(0,1)\).
* **Setting 2: small sample size + Gaussian noise**. \(p=0.2\), \(_{u,v}=(0,1)\), and \(E_{ij}(0,1)\).
* **Setting 3: large sample size + heavy-tailed noise**. \(p=0.8\), \(_{u,v}=(0,1)\), and \(E_{ij} 0.2\,_{1.2}\), where \(t_{}\) denotes the \(t\) distribution with \(\) degrees of freedom.
* **Setting 4: violation of incoherence**. \(p=0.8\), \(_{u,v}=t_{1.2}\), and \(E_{ij}(0,1)\).

Figure 1 displays the results (i.e., the coverage rate and the interval length) for both cmc-als and \(\) when we vary the hypothesized rank \(r\) from \(2\) to \(40\). The oracle length is the difference between the \((1-/2)\)-th and \((/2)\)-th quantiles of the underlying distribution for \(E_{ij}\). As can be seen, cmc-als achieves nearly exact coverage regardless of the choice of \(r\) across all four settings. When the hypothesized rank \(r\) is chosen well, cmc-als is often able to achieve an average length similar to that of the oracle.

For the model-based method \(\), when we underestimate the true rank (\(r<r^{*}\)), we may still see adequate coverage since \(\) tends to over-estimate \(\) by including the remaining \(r^{*}-r\) factors as noise. However, if we overestimate the rank (\(r>r^{*}\)), then \(\) tends to overfit the noise with additional factors and under-estimate \(\) regardless of the choice of \(r\), leading to significant undercoveragein Settings 1, 2 and 4. In Setting 3 with heavy-tailed noise, als tends to be conservative due to overestimating \(\). Overall, Figure 1 confirms our expectation that the validity of the model-based estimates relies crucially on a well specified model, and it fails to hold when sample size is small (cf. Figure 0(b)), when noise is heavy tailed (cf. Figure 0(c)), and when the underlying matrix is not incoherent (cf. Figure 0(d)).

In Figure 2, we present the histogram of standardized scores \((_{ij}-M_{ij})/_{ij}^{2}+^{ 2}}\) and the plot of the upper and lower bounds for three settings. In Figure 1(a), when the model assumptions are met and \(r=r^{*}\), the scores match well with the standard Gaussian and the prediction bounds produced by als and cmc-als are similar. With the same data generating process, when the rank is overparametrized, the distribution of scores cannot be captured by the standard Gaussian, thus the quantiles are misspecified. As we can see from the confidence intervals, als tends to have smaller intervals which lead to the undercoverage. In the last setting, the underlying matrix is no longer incoherent. When the rank is underestimated, the \(r^{*}-r\) factors will be captured by the noise term and the high heterogeneity in the entries will further lead to overestimated noise level. As a result, the intervals by als are much larger while the conformalized intervals are more adaptive to the magnitude of entries.

#### 4.1.2 Heterogeneous missingness

Now we move on to the case with heterogeneous missingness, where the observed entries are no longer sampled uniformly. To simulate this setting, we draw \(\{a_{il}:\ i d_{1},l k^{*}\}\) i.i.d. from \((0,1)\) and \(\{b_{lj}:\ l k^{*},j d_{2}\}\) i.i.d. from \((-0.5,0.5)\), and define sampling probabilities by

\[(p_{ij}/(1-p_{ij}))=_{l=1}^{k^{*}}a_{il}b_{lj}.\]

Figure 1: Comparison between cmc-als and als.

Figure 2: Histogram of standardized scores for als and prediction lower and upper bounds for \(50\) distinct unobserved entries.

We consider two settings with \(k^{*}=1\) and \(k^{*}=5\), respectively. In both settings, \(_{ij}\) is constructed by estimating \(_{il}\) and \(_{lj}\) via constrained maximum likelihood estimation as is shown in Example 3.3.2.

To analyze the effect of estimation error predicted by Theorem 3.2, we consider three matrix generation processes for each choice of \(k^{*}\), where \(^{*}\) is generated in the same way as in Section 4.1.1 with \(d_{1}=d_{2}=d=500\), \(_{a,v}=(0,1)\):

* **Gaussian homogeneous noise**: \(E_{ij}(0,1)\) independently.
* **Adversarial heterogeneous noise**: \(E_{ij}(0,_{ij}^{2})\) independently, where we take \(_{ij}=1/2p_{ij}\). For this setting, note that the high-noise entries (i.e., those entries that are hardest to predict) occur in locations \((i,j)\) that are least likely to be observed during training.
* **Random heterogeneous noise**: \(E_{ij}(0,_{ij}^{2})\) independently, where \((_{ij})_{(i,j)[d_{1}][d_{2}]}\) is drawn from the same distribution as \((1/2p_{ij})_{(i,j)[d_{1}][d_{2}]}\).

We write cmc\({}^{*}\)-als to denote the conformalized method (with als as the base algorithm) where we use oracle knowledge of the true \(p_{ij}\)'s for weighting, while cmc-als uses estimates \(_{ij}\). The results are shown in Figures 3, for the settings \(k^{*}=1\) and \(k^{*}=5\) (i.e., the rank of \(\)). Under both homogeneous noise and random heterogeneous noise, both cmc\({}^{*}\)-als and cmc-als achieve the correct coverage level, with nearly identical interval length. For adversarial heterogeneous noise, on the other hand, cmc\({}^{*}\)-als achieves the correct coverage level as guaranteed by the theory, but cmc-als shows some undercoverage due to the errors in the estimates \(_{ij}\) (since now the variance of the noise is adversarially aligned with these errors); nonetheless, the undercoverage is mild. In Section B.2 in the appendix, the local coverage of cmc-als conditioning \(p_{ij}=p_{0}\) for varying \(p_{0}\) is presented.

### Real data application

We also compare conformalized approaches with model-based approaches using the Rossmann sales dataset4(Farias et al., 2022). This real data provides the underlying fixed matrix \(\); the missingness pattern is generated artificially, as detailed below. We focus on daily sales (the unit is 1K dollar) of \(1115\) drug stores on workdays from Jan 1, 2013 to July 31, 2015 and hence the underlying matrix has dimension \(1115 780\). The hypothesized rank \(r\) is varied from \(5\) to \(60\) with step size \(5\) and we set

Figure 3: Comparison between cmc with true and estimated weights under heterogeneous missingness.

the target level to be \(1-=0.9\). We apply random masking for \(100\) independent trials and report the average for AvgCov and AvgLength in Figure 4.

* **Homogeneous \(p_{ij}\).** For each entry, we first apply random masking \(1-Z_{ij}(0.2)\) independently. In Figure 3(a), conformalized approach has exact coverage at \(1-\) but als tends to be conservative when \(r\) is small and loses coverage when \(r\) increases to \(50\).
* **Heterogeneous \(p_{ij}\).** After drawing \(p_{ij}\)'s from \((p_{ij}/(1-p_{ij}))=-(h_{ij})=(A_{ij})\) in Section 4.1.2 with \(k^{*}=1\), \(}\) is obtained via the constrained maximum likelihood estimator in Example 3.3.2. In Figure 3(b), cmc-als has coverage around \(1-\) and in comparison, when \(r\) is greater than \(40\), als fails to guarantee the coverage and AvgCov decreases to \(0.75\).

## 5 Discussion

In this paper, we introduce the conformalized matrix completion method cmc, which offers a finite-sample coverage guarantee for all missing entries on average and requires no assumptions on the underlying matrix or any specification of the algorithm adopted, relying only on an estimate of the entrywise sampling probabilities. Given an estimate of the entrywise sampling probability, we provide an upper bound for the coverage gap and give the explicit form with examples of the logistic as well as the general one-bit low-rank missingness. In the implementation, an efficient one-shot weighted conformal approach is proposed with the provable guarantee and achieves nearly exact coverage.

We can compare our findings to a few related results in the literature. The work of Chernozhukov et al. (2021) applies conformal prediction to counterfactual and synthetic control methods, where they include matrix completion as an example with a regression-type formulation. However, their approach relies on a test statistic that is a function of estimated residuals. Consequently, their method requires the assumption of stationarity and weak dependence of errors. Furthermore, the validity of their approach is contingent upon the estimator being either consistent or stable. Additionally, Wieczorek (2023) studies conformal prediction with samples from a deterministic and finite population, but the validity under the sampling without replacement remains an open question in their work.

Our results suggest a number of questions for further inquiry. In the setting of matrix completion, while the results are assumption-free with regard to the matrix \(\) itself, estimating the sampling probabilities \(\) that determine which entries are observed remains a key step in the procedure; while our empirical results suggest the method is robust to estimation error in this step, studying the robustness properties more formally is an important open question. More broadly, our algorithm provides an example of how the conformal prediction framework can be applied in a setting that is very different from the usual i.i.d. sampling regime, and may lend insights for how to develop conformal methodologies in other applications with non-i.i.d. sampling structure.