# Breaking Semantic Artifacts for Generalized AI-generated Image Detection

Chenhao Lin\({}^{1}\) Zhengyu Zhao\({}^{1}\) Hang Wang\({}^{2}\)

**Xu Guo\({}^{3}\) Shuai Liu\({}^{3}\)\({}^{*}\) Chao Shen\({}^{1}\)**

\({}^{1}\)School of Cyber Science and Engineering, Xi'an Jiaotong University

\({}^{2}\)School of Automation Science and Engineering, Xi'an Jiaotong University

\({}^{3}\)School of Software Engineering, Xi'an Jiaotong University

zhengchende@stu.xjtu.edu.cn

{linchenhao, zhengyu.zhao, cshangwang}@xjtu.edu.cn

xuguo@stu.xjtu.edu.cn

{sh_liu, chaoshen}@mail.xjtu.edu.cn

Corresponding Authors

###### Abstract

With the continuous evolution of AI-generated images, the generalized detection of them has become a crucial aspect of AI security. Existing detectors have focused on cross-generator generalization, while it remains unexplored whether these detectors can generalize across different image scenes, e.g., images from different datasets with different semantics. In this paper, we reveal that existing detectors suffer from substantial accuracy drops in such cross-scene generalization. In particular, we attribute their failures to "semantic artifacts" in both real and generated images, to which detectors may overfit. To break such "semantic artifacts", we propose a simple yet effective approach based on conducting an image patch shuffle and then training an end-to-end patch-based classifier. We conduct a comprehensive open-world evaluation on 31 test sets, covering 7 Generative Adversarial Networks, 18 (variants of) Diffusion Models, and another 6 CNN-based generative models. The results demonstrate that our approach outperforms previous approaches by 2.08% (absolute) on average regarding cross-scene detection accuracy. We also notice the superiority of our approach in open-world generalization, with an average accuracy improvement of 10.59% (absolute) across all test sets. Our code is available at _https://github.com/Zig-HS/FakeImageDetection_.

## 1 Introduction

Recently, the rapid development of AI-generated image technology has led to the emergence of synthetic images on the internet, raising significant concerns about AI security. These images exhibit remarkable diversity due to the continuous introduction of new generative architectures, e.g., from Generative Adversarial Networks (GANs) , Variational Auto-Encoders (VAEs) , to Diffusion Models (DMs) . For now, Internet users can easily generate a large number of exquisite images using text prompts. As the quality and variety of these synthetic images continue to advance, developing a universal detector for fake images becomes a crucial aspect of AI security.

To determine whether an image is synthetic, existing studies typically train a binary classifier to assess the authenticity of an unknown image during inference. Previous methods have primarily focused on cross-generator detection. For instance, Wang et al.  developed a ResNet50 trained on images from proGAN , and then tested it on different GAN variants. Intriguingly, their findings suggestedthat aggressive augmentation strategies could significantly enhance the classifier's generalization ability. However, some recent studies [6; 7] have shown that Wang's approach does not extend well to images synthesized by unseen DMs, even when DM-generated images are included in the training set. To address the problem of this cross-generator generalization, recent advancements have focused on refining detection algorithms , using large-scale pre-trained models [7; 9; 10], and augmenting or obtaining diverse datasets [11; 12]. In addition, Corvi et al.  demonstrated that images of one specific generative model contain unique artifacts that differ from those of other generators. These "generator artifacts" are particularly distinct between generators with different architectures, such as DMs and GANs, providing a promising avenue for improving cross-generator generalization.

Despite these efforts, it remains unexplored whether these detectors can generalize across different image scenes, e.g., images from different datasets with different semantics. Notably, Dogoulis et al.  have discussed a similar setting. However, their study focuses solely on different concept classes (e.g., objects) and does not consider a wider range of content. To fill this research gap, we propose a more comprehensive cross-scene problem and solution.

By analyzing the residual spectrum of images, we identify a significant challenge arising from the "semantic artifacts" in both real and generated images. Our findings reveal that images from different datasets contain unique artifacts, which can be inherited by generative models. During the training, existing detectors tend to overfit the specific artifacts of the training data, resulting in substantial accuracy drops in cross-scene generalization. For example, classifiers trained on images generated by Latent Diffusion trained on LAION struggle to generalize to images generated by the same diffusion method trained on FFHQ. Intriguingly, while "semantic artifacts" significantly impact the decrease in accuracy, the Average Precision of detectors remains high. This issue was also observed in , but comprehensive exploration is lacking.

To address this challenge, our work focuses on mitigating the impact of "semantic artifacts" for generalized AI-generated image detection. Specifically, we experimentally find that detectors based on deep networks tend to focus more on the global semantics of images, implying their classification may rely on specific scenes. To counter this, we propose a simple yet effective approach based on conducting an image patch shuffle and then training an end-to-end patch-based classifier. Our approach aims to extract artifacts of generators in a local patch while breaking the global "semantic artifacts". Meanwhile, by reducing receptive fields, our approach minimizes the excessive learning of "generator artifacts", benefiting cross-generator detection. To comprehensively evaluate the generalization ability of our approach, we conduct an open-world evaluation on 31 test sets, covering 7 Generative Adversarial Networks, 18 (variants of) Diffusion Models, and another 6 CNN-based generative models. Our experiments demonstrate the effectiveness of our approach across extensive evaluations. Our main contributions can be summarized as follows:

* We innovatively identify "semantic artifacts" in cross-scene AI-generated image detection, which leads to poor generalization performance of existing detectors.
* We propose a simple yet effective patch-based approach, aiming at breaking the "semantic artifacts" for generalization detection. By extracting local features, our detector is able to reduce the impact of global semantics in images.
* Extensive experiments validate the effectiveness of our approach with an improvement of 3.81% average accuracy (absolute) in cross-scene detection (6 variants of Latent Diffusion with different real datasets) and an improvement of 6.74% average accuracy (absolute) in open-world generalization (all 31 test sets).

## 2 Related Work

### Generative Models

Generative models aim at learning to create new samples from a given dataset, with the fundamental goal of capturing the underlying probability distribution mapping. Prominent approaches in this field include Variational Auto-Encoders (VAEs) , Auto-regressive Models , Flows-based Models , Generative Adversarial Networks (GANs) , and Diffusion Models (DMs) . Early methods like GANs have achieved impressive realism in image synthesis on specific categories, leading to the development of the proGAN/styleGAN series [17; 18].

More recently, DM-based synthesis methods, exemplified by Latent Diffusion , have rapidly advanced in text-to-image generation. Breakthroughs like Stable Diffusion, DALL:E 2 , and Imagen  are released in quick succession. These large-scale image synthesis models have significantly expanded the application scope of image synthesis, enriching the semantic content and stylistic features of generated images. The evolution makes the detection of AI-generated images increasingly complex and challenging.

### Detecting Synthetic Images

Prior to the advent of advanced DMs, a significant body of forgery detection research focused on images generated by GANs. Early detection methods targeted local artifacts present in synthetic images, including distortions in facial tampered landmarks  or inconsistencies in head postures . In , it was demonstrated that global artifacts in synthesized images differ markedly from common artifacts found in modern digital devices. Additionally, the up-sampling operations prevalent in most GAN architectures tend to produce distinct peaks in the spectral profiles of synthesized images [25; 26], offering another avenue for detecting fake images.

A noteworthy question that arises is whether these methods are also effective in detecting DM-generated images. Wang  suggests that simple classifiers when trained with aggressive data augmentation on proGAN images, can be adapted to other unseen models. Ojha  use a fixed pre-trained CLIP:ViT-L/14 to extract image features, while Sha  incorporate hint information and utilize multimodal inputs (text prompts and images) to enhance performance. Tan et al.  introduce the concept of neighboring pixel relationships as a means to capture and characterize the generalized structural artifacts stemming from up-sampling operations. In particular, recent studies [28; 29] have identified significant differences in the frequency spectra of images synthesized by GANs and DMs, impacting the generalization capability of existing detectors. Interestingly, a common issue is observed when detectors are extended to unknown data . Specifically, these detectors will show a disparity where high Average Precision is accompanied by low accuracy. Our analysis of generalization detection of AI-generated images reveals a similar problem, showing performance imbalances when dealing with different semantics.

## 3 Methodology

### Artifacts Analysis

To develop a universal detector for AI-generated images, it is crucial to thoroughly examine the distinctive artifacts of synthetic images generated by various generative models. Inspired by [4; 13; 7], we start our analysis by visualizing the frequency spectra of different image distributions. This exploration aims to uncover any unique or intriguing properties of the artifacts associated with different generators or scenes.

In line with the experimental design in [30; 29], we randomly select a set of images with a quantity of \(I=1000\) for each image source and utilize the denoising filter \(D(x_{i})\) from  to extract the noise residuals \(r_{i}\) of an original image \(x_{i}\):

\[r_{i}(m,n)=x_{i}(m,n)-D(x_{i}(m,n)) i=1,2,...,I\] (1)

Then, we start from the Fourier transform \(F\) of the noise residuals of the \(M N\) image:

\[F_{i}(k,l)=_{m=1}^{M}_{n=1}^{N}r_{i}(m,n)e^{-j2(m+ {l}{N}n)}\] (2)

where, \(k\) and \(l\) represent the frequency domain coordinates. To obtain the artifacts of the image source, we take the average power spectrum \(S\) of all single images from it:

\[S_{x}(k,l)=_{i=1}^{I}|F_{i}(k,l)|^{2}\] (3)

We use images from our test sets (see Section 4.1) to conduct preliminary experiments for analysis of the artifacts in both synthetic and real images.

Generator artifacts.Figure 1 shows the noise residuals power spectrum of generated images. For GANs, we have observed periodic artifacts in the spectrum, identified as checkerboard artifacts produced by up-sampling . Similar artifacts are also observed in some DMs, such as DALL-E 2 and Latent Diffusion. These DMs employ an auto-encoder architecture to compress images into latent space features, resulting in periodic artifacts akin to those found in GANs. While other CNN-based generators, e.g., Deepfake or CRN, exhibit unique artifacts that are significantly different from GANs or DMs. Interestingly, although cycleGAN and starGAN are both based on GAN architecture, they still show some differences in the spectrum. Overall, images from different generators exhibit different artifacts and generated images with unique artifacts might lead detectors to overfit during training, thereby failing to recognize distinct artifacts of cross-generator data. We term these artifacts as "generator artifacts."

Semantic artifacts.Figure 2 shows the noise residuals power spectrum of real and synthetic images from 5 distinct scenes (FaceForensics, Raw Camera, CelebA, FFHQ, LAION). To our surprise, artifacts are also observed in real images. For example, compared to LAION, CelebA and FFHQ exhibit artifacts around the center of the spectrum while Raw Camera displays artifacts similar to the periodic artifacts in GANs. Meanwhile, our findings suggest that these artifacts can be inherited by generative models (see Deepfake trained on FaceForensics, SITD trained on Raw Camera, and Latent Diffusion trained on CelebA/FFHQ/LAION). These artifacts might originate from the semantics of the images or the preprocessing methods of the datasets. For example, CelebA and FFHQ share

Figure 1: **Generator artifacts:** noise residuals power spectrum of images from 9 generative models and 1 real dataset. Top row: 5 Diffusion Models. Bottom row: 2 GANs, cycleGAN and starGAN, 2 CNN-based generators, Deepfake and CRN, and 1 real dataset, LAION.

Figure 2: **Semantic artifacts:** noise residuals power spectrum of images from different scenes. Top row: 5 real datasets. Bottom row: 5 generative models in corresponding scenes, deepfake, SITD, and 3 variants of Latent Diffusion on CelebA, FFHQ, and LAION.

[MISSING_PAGE_FAIL:5]

To explore the impact of "semantic artifacts", we conduct several preliminary experiments using the existing paradigm . This method is widely adopted in current studies [12; 10], where researchers train a ResNet-50 using images from only one generator to detect images from other generators.

Specifically, we use 2 sets of datasets for cross-scene tests. We randomly select 15000 images from LSUN bedroom and LSUN church, respectively, and generate 15000 images by SDXL-Turbo using prompts of "A photo of a bedroom" and "A photo of a church" respectively (10000 as training set and 5000 as test set). We train the detectors respectively on two training sets and evaluate them on cross-scene detection.

In Table 1, we report the accuracy (Acc.) and Average Precision (AP) of the 2 detectors on different scenes. As we can see, the detectors perform exceptionally well (almost 100%) on images with the same semantics as the training set. However, when tested on cross-scene images, the detectors suffer from significant drops in Acc. (near 0.00%) for generated images. Interestingly, the AP remains high, indicating that the detectors can still differentiate between real and fake images, although the distinction in scores between real and fake images is relatively minor. To further analyze the reasons for the low Acc., we report the Acc. results at different thresholds in Table 2. Typically, the threshold for determining whether an input image is real or fake is set to 50% in this task. However, in Table 2, we observe that the optimal threshold for AI-generated image detection is exceptionally low, ranging from 0.01% (from Church to Bedroom) to even 0.001% (from Bedroom to Church). This demonstrates that the average score of fake images with different semantics is extremely low and the score difference between real and fake images is minimal. Unfortunately, such a low threshold may lead to an increase in false alarms when handling various images. Furthermore, the optimal thresholds vary across different scenes, e.g., 0.01% from Church to Bedroom and 0.001% from Bedroom to Church. Therefore, simply adjusting the threshold does not resolve the issue of cross-scene generalization.

We attribute the poor performance of the existing paradigm to the overfitting of semantic artifacts during training. To address this, we simply apply Patch Shuffle as preprocessing to break the global semantics of images. The results are shown in Table 2. As observed, there is an improvement in Acc. at each threshold from Bedroom to Church, indicating a larger difference in scores between real and fake images. Moreover, from Church to Bedroom, the Acc. at the 50% threshold reaches 94.15%, highlighting the possibility of using Patch Shuffle to mitigate the impact of "semantic artifacts".

Figure 3 reports the visualization of CAM extracted from detectors on generated images of Bedroom or Church. Notably, detectors without Patch Shuffle struggle to focus on generated artifacts of images from different scenes. Due to the detector's limited ability to capture specific semantic content, there are almost no active regions in CAM of images from different scenes, e.g., (b)-3, (b)-4, (c)-1, (c)-2 in Figure 3. In contrast, detectors with Patch Shuffle demonstrate more active regions, unaffected by specific semantic content, e.g., (d)-3, (d)-4, (e)-1, (e)-2 in Figure 3.

Figure 4: **Pipeline of our approach. First, for pre-processing, we divide the input image into patches and shuffle these patches to obtain a randomized sequence. Then, we train a patch-based convolutional network for feature extraction. Finally, we flatten these features into a one-dimensional vector and then apply a linear classifier for classification.**

### Approach

The primary goal for cross-scene generalization is to break the impact of "semantic artifacts". In our preliminary experiments, simple Patch Shuffle has proven to be an effective method. However, overfitting still exists, e.g., from Bedroom to Church, the Acc. at 50% threshold of detectors with Patch Shuffle is 51.18% (see Table 2). We hypothesize that the deep residual structure of ResNet-50 promotes the extraction of overall semantic information. Although Patch Shuffle has been applied, the detectors still learn some semantic features between patches. A strong piece of evidence is that the visualization of CAM extracted from ResNet-50 with Patch Shuffle shows large, contiguous areas of activated regions, e.g., (d) and (e) in Figure 3, rather than discrete, point-like activated regions seen in our approach, e.g., (f) in Figure 3.

Based on this hypothesis, we propose a simple yet effective approach based on conducting an image Patch Shuffle and then training an end-to-end patch-based classifier. The whole pipeline of our approach is shown in Figure 4.

**Pre-processing.** First, given an input image \(X\), we divide it into several patches of size \(P P\) :

\[\{p_{i}\}=Segment(X,P) i=1,2,...,M M\] (4)

where \(M M\) is the total number of patches obtained after segmentation. We further shuffle these patches to obtain a randomized sequence \(\{p_{j}\}(j=1,2,...,M M)\), which aims to break the global "semantic artifacts".

**Patch-based feature extraction.** We employ a patch-based convolutional network \(\) for feature extraction, which is designed to accept only one single patch as input, aiming at extracting local features. For each patch \(p_{j}\), we first apply a convolutional layer \(C_{0}\) to extract an initial feature map \(F_{0}\). Subsequently, a series of same-convolutional blocks \(\{C_{n}\}\) are used for deep feature encoding.

\[F_{0}=C_{0}(p_{j})\] (5)

\[F_{n}=C_{n}(F_{n-1}), n=1,2,...,N\] (6)

where \(N\) is a hyper-parameter representing the number of same-convolutional blocks. After encoding, the feature map is downsampled through 3 basicblocks.

**Linear classifier.** The features of each patch \(\{F_{N}\}\) are reassembled into a feature sequence \(\). We flatten the feature sequence into a one-dimensional vector \(\), followed by a linear classifier \(\):

\[=()\] (7)

We optimize the whole pipeline (including both the feature extractor and linear classifier) using binary cross-entropy loss:

\[(y,)=-_{i=1}^{N}[y_{i}(_{i})+(1-y_ {i})(1-_{i})]\] (8)

where \(y\) represents the true labels, and \(\) represents the predictions.

## 4 Experiments

### Experimental Settings

Training dataset.To evaluate detectors' generalization capability, the standard practice is restricting the training data to images from only one generator and scene. We employ the generated images from DiffusionDB . The training set consists of 48,000 images generated by Stable Diffusion v1.4 using prompts from Internet Users and 48,000 real images from LAION-5B . To ensure a consistent basis for comparison, we also employ the training set of ForenSynths , in line with experimental design in , as an ablation experiment in Section A.3.1.

Test datasets.To evaluate the generalization of baselines and our approach, we conduct a comprehensive open-world evaluation on 31 test sets, covering 7 Generative Adversarial Networks, 18 (variants of) Diffusion Models, and another 6 CNN-based generative models.

* **Diffusion Models.** We collect DMs, including iDDPM , DDIM , PNDM , Guided-Diffusion (ADM) , RDM , Latent Diffusion (LDM) , Stable Diffusion v1.4 (SDv1) and v2.1 (SDv2), GLIDE , and DALL-E 2 . We sample images from their variants of 6 real datasets, including LSUN-bedroom, LSUN-church , CelebA , FFHQ , ImageNet , and LAION-5B . Details refer to Section A.7.
* **Generative Adversarial Networks.** The test sets include fake images generated by 7 GANs from ForenSynths , including proGAN , styleGAN, styleGAN2 , bigGAN , cycleGAN , starGAN , and gauGAN .
* **Other CNN-based generative models.** The test sets include fake images generated by 6 other CNN-based generative models from ForenSynths , including CRN , IMLE , SAN , SITD , deepfake , and WFIR .

**Detector baselines.** We perform comparisons of our approach with existing popular and state-of-the-art detectors on our open-world datasets, including Durall (CVPR 2020) , CNNDetection (CVPR 2020) , PatchFor (ECCV 2020) , F3Net (ECCV 2020) , Dogoulis (MAD'23) , DIRE (ICCV 2023) , Ojha (CVPR 2023) , LGrad (CVPR 2023) , NPR (CVPR 2024) . We re-implement baselines  with the official codes using our training set, and adopt the official pre-trained weights of baselines .

**Implementation details.** Our approach is implemented using the PyTorch on NVIDIA A100 40GB Tensor Core GPU. During the training, we perform zero padding on the images to ensure the shorter edge is \(256\) pixels (resizing is also used as another pre-processing pipeline for ablation), and then randomly crop the images to 256x256. Each image will be horizontally or vertically flipped with a probability of 50% as data augmentation. The number of same-convolution blocks \(N\) is set to 18 and the size of each patch \(P\) is set to 32. The detector is trained using the Adam optimizer and early-stop strategy with an initial learning rate of \(1e-4\), a minimum learning rate of \(1e-6\), and a batch size of 64. We separate 5,000 images from the training set to serve as the validation set. We use the accuracy (Acc.) and the Average Precision (AP) as the evaluation metrics, with a particular emphasis on Acc. as our primary metric, because as discussed in Section 3.2, the AP metric may not fully reflect the problem of cross-scene generalization.

### Experimental Results

Cross-scene generalization.We first report the cross-scene detection results in Table 3. We average the results on 6 variants of Latent Diffusion (LSUN-Bedroom, LSUN-Church, ImageNet, CelebA, FFHQ, LAION). The results demonstrate that most detectors suffer from performance drops on cross-scene images, even though the images are generated by models with the same structure. Particularly, the gap between AP and Acc. can be observed in several baselines, e.g., CNN (54.16% Acc. and 67.02% AP), Ojha (66.38% Acc. and 79.36% AP), F3Net (56.82% Acc. and 85.39% AP), LGrad (62.91% Acc. and 80.99% AP), which is consistent with our analysis in Section 3.2. As a comparison, the best baseline NPR and our approach show little gap between Acc. and AP. Considering that NPR uses Neighboring Pixel Relationships as features, which benefits breaking the global semantic artifacts, its high performance (90.44% Acc. and 94.84% AP) aligns with our hypothesis. Despite

   &  &  &  \\   & & Aug. Acc. & mAP & Avg. Acc. & mAP \\   & Blur+IRFG (0.1) & 53.66 & 63.40 & 56.23 & 65.27 \\  & Blur+IRFG (0.5) & 54.16 & 67.02 & 53.34 & 64.13 \\   & ResNet18 & 66.13 & 74.67 & 57.68 & 63.76 \\  & Xception & 69.91 & 75.25 & 57.82 & 64.27 \\   & FixNet & 65.21 & 81.49 & 56.97 & 68.87 \\  & LPS & 57.53 & 79.36 & 55.07 & 77.66 \\  & Both & 56.82 & 85.99 & 53.93 & 74.54 \\   & SVM & 64.23 & 59.80 & 55.87 & 53.76 \\  & LR & 68.28 & 64.81 & 59.15 & 53.74 \\   & CelebA-SDv2 & 63.19 & 69.93 & 57.05 & 63.81 \\  & ImageNet-ADM & 58.35 & 66.73 & 54.32 & 59.07 \\  & LSUN-ADM & 66.46 & 65.66 & 56.29 & 56.70 \\   & Top & 32.70 & 55.75 & 53.59 & 56.96 \\  & Top 24k & 51.91 & 57.76 & 53.25 & 60.35 \\   & CLIP-VIT-714+FC & 66.38 & 79.36 & 67.12 & 81.28 \\   & - & 62.91 & 80.99 & 57.69 & 76.91 \\   & - & 50.44 & **94.54** & 73.88 & 88.76 \\  & Reusing & **94.25** & **96.28** & 82.12 & 83.36 \\  & 92.52 & 95.58 & **85.97** & **90.00** \\  

Table 3: Results of cross-scene generalization and open-world generalization. For cross-scene generalization, we average the results on 6 variants of Latent Diffusion (LSUN-Bedroom, LSUN-Church, ImageNet, CelebA, FFHQ, LAION). For open-world generalization, we average the results on all 31 test sets (including 18 DMs, 7 GANs, and 6 CNN-based generators). **Bold** represents the best and underline represents the second best. More Detailed results are shown in Table 4 and Table 5.

this, our approach still outperforms the SOTA generalization performance, showcasing higher Avg. Acc. and mAP metrics, which reach 92.52% and 95.58% (based on zero padding).

Open-world generalization.Table 3 also presents the Avg. Acc. and mAP metrics of detectors across the 31 test sets. The open-world evaluation results demonstrate an improvement in Avg. Acc. of our approach, outperforming Ojha and NPR by 18.85% and 10.59% respectively. This indicates the significance of breaking semantic artifacts and focusing on local features in generalized detection. The visualization result, e.g., the discrete, point-like activated regions in Figure 3 (f), further confirms the effectiveness of our approach that predominantly focuses on learning local features. Detailed results of our open-world experiments are presented in two groups.

* **Cross-Diffusion generalization.** Table 4 presents Acc. and mAP results across images from 18 (variants of) Diffusion Models. The results demonstrate the outstanding performance of our approach on cross-Diffusion evaluation. Our approach reaches an Avg. Acc. of 90.18%, outperforming the best baseline NPR (78.35%). Notably, most baselines show their poor performance on the unconditional generation scene, e.g., Bedroom and Church. We attribute it to a strong scene change, where there is a significant difference between the txt2img generation scene (training set) and the unconditional generation scene (test sets). In addition, the gap between Acc. and AP still exists, e.g., NPR (78.35% Avg. Acc. and 94.08% mAP).
* **Cross-GAN/CNN generalization.** Table 5 presents the Acc. and mAP metrics of detectors across images from 7 Generative Adversarial Networks and another 6 CNN-based generative models. As we see, most detectors suffer from performance degradation on cross-GAN/CNN Generalization, which is expected because these generators use a different architecture from

the Diffusion Model (training set). Nevertheless, our approach still shows advantages with SOTA accuracy while Ojha has SOTA mAP but the gap between Acc. and AP still exists.

Effect of pre-processing pipeline.So far, we have seen excellent generalizability in our patch-based method. We next explore in depth the influence of pre-processing pipelines on artifact altering and detection. Specifically, we adopt both image padding and resizing during pre-processing. Tables 4 and 5 demonstrate the superior results of zero padding compared to resizing, especially in cross-GAN/CNN generalization. In particular, the performance of our approach using padding has been largely boosted on SITD, SAN, CRN, IMEL, and WFIR compared to resizing. It can be attributed to their high image resolution, which introduces variations in artifacts and the loss of low-level features when resizing is applied. Meanwhile, Figure 5 in Section A.4 also supports this finding by showing that the frequency features of SITD (with image resolution of over 4,000\(\)3,000) images change a lot after resizing. In summary, these results suggest that zero padding can preserve more low-level features in images, which benefits generalized detection.

Effect of model depth and patch size.We next delve into the interplay between receptive field and generalization ability. Specifically, we adjust the number of same-convolution blocks (\(N\)), to vary the receptive field size and the quantity of parameters in the feature extractor (notably, resizing is used as preprocessing in the experiments because it does not affect the global receptive field). Table 6 reports the Avg. Acc. and mAP results across different \(N\) values. The results demonstrate that both too-shallow (\(N\)=0) and too-deep (\(N\)=24) layers result in performance degradation due to underfitting and overfitting, respectively. Intriguingly, the best \(N\) value for Cross-Diffusion generalization is 21 while the best for cross-GAN/CNN generalization is 6. Considering that images of the training set are generated by the Diffusion Model, we attribute the results to the fact that a shallow model might learn less from "generator artifacts" of training images and perform better on cross-generator detection. Conversely, this strategy will disrupt its fitting of in-domain artifacts, resulting in worse detection of in-domain datasets. Then, we conduct experiments to explore the effect of different patch sizes. The \(P\) value represents the size of each patch, which determines the number of patches and the degree of breaking global semantic artifacts. Table 7 reports the Avg. Acc. and mAP metrics across different \(P\) values. As expected, a larger \(P\) value, e.g., \(P\)=48 or \(P\)=64, enlarges the receptive field, potentially exacerbating overfitting issues. Conversely, a too-small patch size, e.g., P=16, can lead to significant underfitting. This finding is encouraging, as it aligns with the effect of model depth and benefits a deeper investigation into the mechanisms of generalized detection.

## 5 Conclusion

In this paper, we introduce the concept of "semantic artifacts" in the generalization detection of AI-generated images. We analyze the existence and impact of "semantic artifacts" in cross-scene detection through extensive experiments. Our results demonstrate that existing detectors suffer significant drops in accuracy when applied to images with different semantics, which we attribute to overfitting during training. To address this issue, we propose a simple yet effective approach that utilizes Patch Shuffle and trains an end-to-end patch-based classifier to break the "semantic artifacts" in images. Extensive experiments on 31 test sets validate the effectiveness of our approach, demonstrating our contributions to the universal detection of AI-generated images.

    &  &  &  \\   & Avg. Acc. & mAP & Avg. Acc. & mAP & Avg. Acc. & mAP \\  P=64; N=0 & 81.53 & 87.48 & 63.55 & 66.42 & 73.99 & 78.65 \\ P=64; N=3 & 82.71 & 89.15 & 68.85 & 73.37 & 76.90 & 82.53 \\ P=64; N=6 & 86.34 & 90.96 & **74.27** & 80.40 & 81.28 & 86.53 \\ P=64; N=9 & 86.17 & 91.17 & 72.06 & **80.75** & 81.09 & **86.80** \\ P=64; N=12 & 87.09 & 91.85 & 73.37 & 79.52 & **81.34** & 36.68 \\ P=64; N=15 & 88.40 & 92.85 & 67.83 & 76.36 & 79.77 & 85.93 \\ P=64; N=18 & 88.60 & 92.43 & 67.96 & 72.93 & 79.95 & 84.25 \\ P=64; N=21 & **88.91** & **93.03** & 70.19 & 76.49 & 81.06 & 86.09 \\ P=64; N=24 & 87.10 & 91.65 & 67.70 & 72.28 & 78.97 & 83.53 \\   

Table 6: Ablation study results on model depth.

    &  &  &  \\   & Avg. Acc. & mAP & Avg. Acc. & mAP & Avg. Acc. & mAP \\  P=64; N=18 & 88.60 & 92.43 & 67.96 & 72.93 & 79.95 & 84.25 \\ P=88; N=18 & 88.53 & 92.08 & 70.78 & 76.79 & 81.11 & 85.67 \\ P=23; N=21 & **89.01** & **93.58** & **72.58** & **81.12** & **81.12** & **88.36** \\ P=16; N=18 & 84.35 & 91.29 & 67.13 & 72.41 & 77.13 & 85.47 \\   

Table 7: Ablation study results on patch size.