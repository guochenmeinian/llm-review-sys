# Stochastic Distributed Optimization under Average Second-order Similarity: Algorithms and Analysis

Dachao Lin

Academy for Advanced Interdisciplinary Studies; Peking University; lindachao@pku.edu.cn;

Yuze Han

School of Mathematical Sciences; Peking University; hanyuze97@pku.edu.cn;

Haishan Ye

Corresponding Author; School of Management; Xi'an Jiaotong University; SGIT AI Lab, State Grid Corporation of China; yehaishan@xjtu.edu.cn;

Zhihua Zhang

School of Mathematical Sciences; Peking University; zhzhang@math.pku.edu.cn.

###### Abstract

We study finite-sum distributed optimization problems involving a master node and \(n-1\) local nodes under the popular \(\)-similarity and \(\)-strong convexity conditions. We propose two new algorithms, SVRS and AccSVRS, motivated by previous works. The non-accelerated SVRS method combines the techniques of gradient sliding and variance reduction and achieves a better communication complexity of \(}(n{+}/)\) compared to existing non-accelerated algorithms. Applying the framework proposed in Katyusha X , we also develop a directly accelerated version named AccSVRS with the \(}(n{+}n^{3/4})\) communication complexity. In contrast to existing results, our complexity bounds are entirely smoothness-free and exhibit superiority in ill-conditioned cases. Furthermore, we establish a nearly matched lower bound to verify the tightness of our AccSVRS method.

## 1 Introduction

We have witnessed the development of distributed optimization in recent years. Distributed optimization aims to cooperatively solve a learning task over a predefined social network by exchanging information exclusively with immediate neighbors. This class of problems has found extensive applications in various fields, including machine learning, healthcare, network information processing, telecommunications, manufacturing, natural language processing tasks, and multi-agent control . In this paper, we focus on the following classical finite-sum optimization problem in a centralized setting:

\[_{^{d}}f():=_{i=1}^{n}f_{i}(),\] (1)

where each \(f_{i}\) is differentiable and corresponds to a client or node, and the target objective is their average function \(f\). Without loss of generality, we assume \(f_{1}\) is the master node and the others are local nodes. In each round, every local node can communicate with the master node certain information, such as the local parameter \(\), local gradient \( f_{i}()\), and some global information gathered at the master node. Such a scheme can also be viewed as decentralized optimization over a star network .

Following the wisdom of statistical similarity residing in the data at different nodes, many previous works study scenarios where the individual functions exhibit relationships or, more specifically, certain homogeneity shared among the local \(f_{i}\)'s and \(f\). The most common one is under the \(\)-second-ordersimilarity assumption [33; 35], that is,

\[\|^{2}f_{i}()-^{2}f()\|,^{d},i\{1,,n\}.\]

Such an assumption also has different names in the literature, such as \(\)-related assumption, bounded Hessian dissimilarity, or function similarity [7; 32; 54; 62; 51]. The rigorous definitions are deferred to Section 2. Moreover, the second-order similarity assumption can hold with a relatively small \(\) compared to the smoothness coefficient of \(f_{i}\)'s in many practical settings, such as statistical learning. More insights on this can be found in the discussion presented in [54; Section 2]. The similarity assumption indicates that the data across different clients share common information on the second-order derivative, potentially leading to a reduction in communication among clients. Meanwhile, the cost of communication is often much higher than that of local computation in distributed optimization settings [9; 44; 30]. Hence, researchers are motivated to develop efficient algorithms characterized by low communication complexity, which is the primary objective of this paper as well.

Furthermore, we need to emphasize that prior research [25; 56; 63; 19; 43; 5] has shown tightly matched lower and upper bounds on computation complexity for the finite-sum objective in Eq. (1). These works focus on gradient complexity under (average) smoothness  instead of communication complexity under similarity. Indeed, we will also discuss and compare the gradient complexity as shown in , to explore the trade-off between communication and gradient complexity.

Although the development of distributed optimization with similarity has lasted for years, the optimal complexity under full participation was only recently achieved by Kovalev et al. . They employed gradient-sliding  and obtained the optimal communication complexity \(}(n)\) for \(\)-strongly convex \(f\) and \(\)-related \(f_{i}\)'s in Eq. (1). However, the full participation model requires the calculation of the whole gradient \( f()\), which incurs a communication cost of \(n{-}1\) in each round. In contrast, partial participation could reduce the communication burden and yield improved complexity. Hence, Khaled and Jin  introduced client sampling, a technique that selects one client for updating in each round. They developed a non-accelerated algorithm SVRP, which achieves the communication complexity of \(}(n{+}^{2}/^{2})\). Additionally, they proposed a Catalyzed version of SVRP with the complexity \(}(n{+}n^{3/4})\), which is better than the rates obtained in the full participation setting.

We believe there are several potential avenues for improvement inspired by . 1) Khaled and Jin  introduced the requirement that each individual function is strongly convex (see [33, Assumption 2]). However, this constraint is absent in prior works. Notably, in the context of full participation, even non-convexity is deemed acceptable6. A prominent example is the shift-and-invert approach to solving PCA [52; 23], where each component is smooth and non-convex, but the average function remains convex. Thus we doubt the necessity of requiring strong convexity for individual components. 2) In hindsight, it seems that the directly accelerated SVRP could only achieve a bound of \(}(n+/)\) based on the current analysis, which is far from being satisfactory compared to its Catalyzed version. Consequently, there might be room for the development of a more effective algorithm for direct acceleration. 3) It is essential to note that the Catalyst framework introduces an additional log term in the overall complexity, along with the challenge of parameter tuning. This aspect is discussed in detail in [6, Section 1.2]. Therefore, we intend to address the aforementioned concerns, particularly on designing directly accelerated methods under the second-order similarity assumption.

### Main Contributions

In this paper, we address the above concerns under the average similarity condition. Our contributions are presented in detail below and we provide a comparison with previous works in Table 1:

* First, we combine gradient sliding and client sampling techniques to develop an improved non-accelerated algorithm named SVRS (Algorithms 1). SVRS achieves a communication complexity of \(}(n+/)\), surpassing SVRP in ill-conditioned cases. Notably, this rate does not need component strong convexity and applies to the function value gap instead of the parameter distance.
* Second, building on SVRS, we employ a classical interpolation framework motivated by Katyusha X  to introduce the directly accelerated SVRS (AccSVRS, Algorithm 2).

AccSVRS achieves the same communication bound of \(}(n+n^{3/4})\) as Catalyzed SVRP. Specifically, our bound is entirely smoothness-free and slightly outperforms Catalyzed SVRP, featuring a log improvement and not requiring component strong convexity.
* Third, by considering the proximal incremental first-order oracle in the centralized distributed framework, we establish a lower bound, which nearly matches the upper bound of AccSVRS in ill-conditioned cases.

### Related Work

Gradient sliding/Oracle Complexity Separation.For optimization problems with a separated structure or multiple building blocks, such as Eq. (1), there are scenarios where computing the gradients/values of some parts (or the whole) is more expensive than the others (or a partial one). In response to this challenge, techniques such as the gradient-sliding method  and the concept of oracle complexity separation  have emerged. These methods advocate for the infrequent use of more expensive oracles compared to their less resource-intensive counterparts. This strategy has found applications in zero-order [12; 21; 28; 53], first-order [37; 38; 39; 31] and high-order methods [31; 24; 3], as well as in addressing saddle point problems [4; 13]. Our algorithms can be viewed as a variance-reduced version of gradient sliding tailored to leverage the similarity assumption.

Distributed optimization under similarity.Distributed optimization has a long history with a plethora of existing works and surveys. To streamline our discussion, we only list the most relevant references, particularly under the similarity and strong convexity assumptions. In the full participation setting, which involves deterministic methods, the first algorithm credits to DANE , though its analysis is limited to quadratic objectives. Subsequently, AIDE , DANE-LS and DANE-HB  improved the rates for quadratic objective; Disco  SPAG , ACN  and DiRegINA  improved the rates for self-concordant objectives. As for general strongly convex objectives, Sun et al.  introduced the SONATA algorithm, and Tian et al.  proposed accelerated SONATA. However, their complexity bounds include additional log factors. These factors have recently been removed by Accelerated Extragradient , whose complexity bound perfectly matches the lower bound in . We highly recommend the comparison of rates in [35, Table 1] for a comprehensive overview. Once the discussion of deterministic methods is concluded, Khaled and Jin  shifted their focus to stochastic methods using client sampling. They proposed SVRP and its Catalyzed version, both of which exhibited superior rates compared to deterministic methods.

   & **Method/Reference** & **Communication complexity** & **Assumptions** \\   & AccExtragradient  & \((n})\) & SS only for \(f_{1}\) \\   & Lower bound  & \((n})\) & SS for \(f_{i}\)’s \\   & SVRP  & \(((n+}{^{2}}))^{(1)}\) & SC for \(f_{i}\)’s, AveSS \\   & Catalyzed SVRP  & \(((n+n^{3/4}}))^{(2)}\) & SC for \(f_{i}\)’s, AveSS \\   & SVRS (Thm 3.3) & \(((n+))\) & AveSS \\   & AccSVRS (Thm 3.6) & \(((n+n^{3/4}}))\) & AveSS \\   & Lower bound (Thm 4.4) & \((n+n^{3/4}})^{( 3)}\) & AveSS \\  

* The rate only applies to \(\|_{h}-_{}\|^{2}\), otherwise it would introduce \(L\) in the log term;  The term \((L/)\) comes from the Catalyst framework. See Appendix C for the detail.  Here we only list the rates of the common ill-conditioned case: \(=(/)\). See Appendices for the remaining case. _Notation:_\(\)insimilarity parameter (both for SS and AveSS), \(L\)=smoothness constant of \(f\), \(\)=strong convexity constant of \(f\)(or \(f_{i}\)’s), \(\)=error of the solution for \(f(_{h})-f(_{})\). Here \(L>0\). _Abbreviation:_ SC=strong convexity, SS=second-order similarity, AveSS=average SS.

Table 1: Comparison of communication under similarity for the strongly convex objective.

## 2 Preliminaries

Notation.We denote vectors by lowercase bold letters (e.g., \(,\)), and matrices by capital bold letters (e.g., \(,\)). We let \(\|\|\) be the \(_{2}\)-norm for vectors, or induced \(_{2}\)-norm for a given matrix: \(\|\|=_{ 0}\|\|/\|\|\). We abbreviate \([n]=\{1,,n\}\) and \(_{d}^{d d}\) is the identity matrix. We use \(\) for the all-zero vector/matrix, whose size will be specified by a subscript, if necessary, and otherwise is clear from the context. We denote \(()\) as the uniform distribution over set \(\). We say \(T(p)\) for \(p(0,1]\) if \((T=k)=(1-p)^{k-1}p, k\{1,2,\}\), i.e., \(T\) obeys a geometric distribution. We adopt \(_{k}\) as the expectation for all randomness appeared in step \(k\), and \(1_{A}\) as the indicator function on event \(A\), i.e., \(1_{A}=1\) if event \(A\) holds, and \(0\) otherwise. We use \((),(),()\) and \(}()\) notation to hide universal constants and log-factors. We define the Bregman divergence induced by a differentiable (convex) function \(h^{d}\) as \(D_{h}(,):=h()-h()- h(),-\).

Definitions.We present the following common definitions used in this paper.

**Definition 2.1**: _A differentiable function \(g^{d}\) is \(\)-strongly convex (SC) if_

\[g() g()+ g(),-+\|-\|^{2},,^{d}.\] (2)

_Particularly, if \(=0\), we say that \(g\) is convex._

**Definition 2.2**: _A differentiable function \(g^{d}\) is L-smooth if_

\[g() g()+ g(),-+ {2}\|-\|^{2},,^{d}.\] (3)

There are many basic inequalities involving strong convexity and smoothness, see [22, Appendix A.1] for an introduction. Next, we present the definition of second-order similarity in distributed optimization.

**Definition 2.3**: _The differentiable functions \(f_{i}\)'s satisfy \(\)-average second-order similarity (AveSS) if the following inequality holds for \(f_{i}\)'s and \(f=_{i=1}^{n}f_{i}\):_

\[_{i=1}^{n}\|[[f_{i}-f]()- [f_{i}-f]()]\|^{2}^{2}\|-\| ^{2},,^{d}.\] (4)

**Definition 2.4**: _The differentiable functions \(f_{i}\)'s satisfy \(\)-component second-order similarity (SS) if the following inequality holds for \(f_{i}\)'s and \(f=_{i=1}^{n}f_{i}\):_

\[\|[[f_{i}-f]()-[f_{i}-f]()] \|^{2}^{2}\|-\|^{2},,^{d},i[n].\] (5)

Definitions 2.3 and 2.4 first appear in , which is an analogy to (average) smoothness in prior literature . Particularly, \(f_{i}\)'s satisfy \(\)-AveSS implies that \((f\!-\!f_{i})\)'s satisfy \(\)-average smoothness, while \(f_{i}\)'s satisfy \(\)-SS implies that \((f\!-\!f_{i})\)'s satisfy \(\)-smoothness. Additionally, many researchers  use the equivalent one defined by Hessian similarity (HS) if assuming that \(f_{i}\)'s are twice differentiable. Thus we also list them below and leave the derivation in Appendix B.

\[\ \|_{i=1}^{n}[^{2}f_{i}()- ^{2}f()]^{2}\|^{2};\ \|^{2}f_{i}()-^{2}f()\|, i [n].\] (6)

Since our algorithm is a first-order method, we adopt the gradient description of similarity (Definitions 2.3 and 2.4) without assuming twice differentiability for brevity.

As mentioned in , if \(f_{i}\)'s satisfy \(\)-AveSS (or SS), and \(f\) is \(\)-strongly convex and \(L\)-smooth, then generally \(L>0\) for large datasets in practice. Therefore, researchers aim to develop algorithms that achieve communication complexity solely related to \(,\) (or log terms of \(L\)). This is also our objective. To finish this section, we will clarify several straightforward yet essential propositions, and the proofs are deferred to Appendix A.

**Proposition 2.5**: _We have the following properties among SS, AveSS, and SC: 1) \(\)-SS implies \(\)-AveSS, but \(\)-AveSS only implies \(\)-SS. 2) If \(f_{i}\)'s satisfy \(\)-SS and \(f\) is \(\)-strongly convex, then for all \(i[n],f_{i}()+\|\|^{2}\) is convex, i.e., \(f_{i}\) is \((-)\)-almost convex ._Algorithm and Theory

In this section, we introduce our main algorithms, which are developed to solve the distributed optimization problem in Eq. (1) under Assumption 1 below:

**Assumption 1**: _We assume that \(f_{i}\)'s satisfy \(\)-AveSS, and \(f\) is \(\)-strongly convex with \(>0\)._

Assumption 1 does not need each \(f_{i}\) to be \(\)-strongly convex. In fact, it is acceptable that \(f_{i}\)'s are non-convex, since by Proposition 2.5, \(f_{i}\)'s are \((-)\)-almost convex . In the following, we first propose our new algorithm SVRS, which combines the techniques of gradient sliding and variance reduction, resulting in improved rates. Then we establish the directly accelerated method motivated by .

### No Acceleration Version: SVRS

We first show the one-epoch Stochastic Variance-Reduced Sliding (\(^{1}\)) method in Algorithm 1. Before delving into the theoretical analysis, we present some key insights into our method. These insights aim to enhance comprehension and facilitate connections with other algorithms.

Variance Reduction.Our algorithm can be viewed as adding variance reduction from . Besides the acceleration step, the main difference lies in the proximal step, where Kovalev et al.  solved:

\[_{t+1}*{arg\,min}_{^{d}}B_{ }^{t}():= f(_{t})- f_{1}(_{t}), {x}-_{t}+\|-_{t}\|^{2}+f _{1}().\]

To save the heavy communication burden of calculating \( f(_{t})\), we apply client sampling by selecting a random \( f_{i_{t}}(_{t})\) in the \(t\)-th step. However, this substitution introduces significant noise. To mitigate this, we incorporate a correction term \(_{t}= f_{i_{t}}(_{0})- f(_{0})\) from previous wisdom  to reduce the variance.

Gradient sliding.Our algorithm can be viewed as adding gradient sliding from SVRP . The main difference also lies in the proximal point problem, where Khaled and Jin  solved:

\[_{t+1}*{arg\,min}_{^{d}}C_{ }^{t}():=-_{t},-_{t}+\|-_{t}\|^{2}+f_{i_{t}}().\]

Here we adopt a fixed proximal function \(f_{1}\) instead of \(f_{i_{t}}\), which can be viewed as approximating \(f_{i_{t}}(){}f_{1}()+[f_{i_{t}}-f_{1}](_{t})+ [f_{i_{t}}-f_{1}](_{t}),-_{t}+}\|-_{t}\|^{2}\) with a properly chosen \(^{}>0\). Such a modification is motivated by , where they reformulated the objective as \(f()=[f()-f_{1}()]+f_{1}()\). Thus they could employ gradient sliding to skip heavy computations of \([f-f_{1}]()\) by utilizing the easy computations of \( f_{1}()\) more times. Fixing the proximal function \(f_{1}\) leads to the same metric space owned by \(f_{1}\) in each step, which could benefit the analysis and alleviate the requirements on \(f_{i}\)'s compared to SVRP. Indeed, in our setting \(f_{1}\) can be replaced by any other **fixed** client \(f_{b},b[n]\). In this case, the master node would be \(f_{b}\) instead of \(f_{1}\).

Bregman-SVRG.Our algorithm can be viewed as the classical Bregman-SVRG  with the reference function \(f_{1}()+\|\|^{2}\) after introducing the Bregman divergence:

\[_{t+1}*{arg\,min}_{^{d}}A_{ }^{t}())}{=}*{arg\, min}_{^{d}} f_{i_{t}}(_{t})-[f_{i_{t}}-f ](_{0}),-_{t}+D_{f_{1}()+ \|\|^{2}}(,_{t}).\]

We need to emphasize that the proof of Bregman-SVRG requires additional structural assumptions [20, Assumption 3], which is not directly applicable in our setting. Hence, the rigorous proof of Bregman-SVRG under our similarity assumption is still meaningful as far as we are concerned.

#### 3.1.1 Communication Complexity under Distributed Settings

When applied to the distributed system, the communication complexity of \(^{1}\)can be described as follows: At the beginning of each epoch, the master (corresponding to \(f_{1}\)) sends \(_{0}\) to all clients. Each client computes \( f_{i}(_{0})\) from its local data and sends it back to the master. The master then builds \( f(_{0})\) after collecting all \( f_{i}(_{0})\)'s. The communication complexity is \(2(n-1)\) in this case. Next, the algorithm enters into the loop iterations. In each iteration, the master only sends current \(_{t}\) to the chosen client \(i_{t}\). The \(i_{t}\)-th client computes \( f_{i_{t}}(_{t})\) and sends it to the master (the first client). Then the master solves (inexactly) the local problem (Line 5 in Algorithm 1) to get an inexact solution \(_{t+1}\). The communication complexity is \(2\) in this case. Thus, the total communication complexity of \(^{}\) is \(2(n-1)+2T\). Note that \(T=1/p\) and generally \(p=1/n\). We obtain that one epoch communication complexity is \(4n-2\) in expectation.

We would like to emphasize that our setup differs from that in , where the authors assume the nodes can perform calculations and transmit vectors in parallel. We recognize the significance of both setups. However, there are situations where communication is more expensive than computation. For instance, in a business network or communication network the communication between any two nodes can result in charges and the risk of information leakage. To mitigate these costs, we should reduce the frequency of communication. Thus, we focus on the nonparallel setting.

#### 3.1.2 Convergence Analysis of SVRS

Based on the one-epoch method \(^{}\), we could introduce our non-accelerated algorithm SVRS, which starts from \(_{0}^{d}\) and repeatedly performs the update7

\[_{k+1}=^{}(f,_{k},,p),\; k  0.\]

Now we derive the convergence rate of SVRS8. The main technique we apply is replacing the Euclidean distance with the Bergman divergence. Denote the reference function

\[h():=f_{1}()+\|\|^{2}-f().\] (8)

By Assumption 1 and 1) in Proposition 2.5, we see that \(f_{i}\)'s are \(\)-SS. i.e., \([f_{1}-f]()\) is \(()\)-smooth. Thus, \(h()\) is \((-)\)-strongly convex and \((+)\)-smooth if \(<}\), that is,

\[0}{2}\|-\|^{2} }}}{{}}D_{h}(,) }}}{{}} }{2}\|-\|^{2}.\] (9)

Hence, if \(=(1)\), \(h()\) is nearly a rescaled Euclidean norm since its condition number related to \(\|\|\) is \(}{1-}=(1)\). Next, we employ the properties of the Bregman divergence \(D_{h}(,)\) to build the one-epoch progress of \(^{}\)as shown below:

**Lemma 3.1**: _Suppose Assumption 1 holds. Let \(^{+}=^{}(f,_{0},,p)\) with \(=1/(4)\), and the approximated solution \(_{t+1}\) satisfies_

\[\| A^{t}_{}(_{t+1})\|^{2} \|_{t}-*{arg\,min}_{^{d}}A^{t}_{ }()\|^{2}, t 0.\] (10)_Then for all \(^{d}\) that is independent of the indices \(i_{1},i_{2},,i_{T}\) in \(^{1}(f,_{0},,p)\), we have_

\[f(^{+})-f()\,p-_{0},  h(^{+})- h(_{0})-p-D_ {h}(_{0},^{+})-D_{h}(,^{+}).\] (11)

**Remark 3.2**: _We note that some papers [10; 11] assume the smoothness and convexity of component functions, and adopt local updates for solving the proximal step. However, we replace these assumptions with a proximal approximately solvable assumption (10), which could even cover some nonsmooth and non-convex but proximal trackable component functions. We regard our assumption as more essential since the local updates can be viewed as partially solving this proximal step._

The proof of Lemma 3.1 is left in Appendix D.1. From Lemma 3.1, we find a well-behaved proximal operator is sufficient to ensure favorable progress. Finally, we establish the convergence rate and communication complexity of the SVRS method, and the proof is deferred to Appendix D.2.

**Theorem 3.3**: _Suppose Assumption 1 holds. If in \(^{1}\)(Algorithm 1), the hyperparameters are set as \(=1/(4),p=1/n\), and the approximate solution \(_{t+1}\) in each proximal step satisfies Eq. (10). Then for any error \(>0\), when_

\[k K_{1}:=\{2,}\}})[f(_{0})-f(_{*}) ]}{},\]

_i.e., after \(}(n+/)\) communications in expectation, we obtain that \(f(_{k})-f(_{*})\)._

**Remark 3.4**: _Our results enjoy the following advantages over SVRP : The convergence of SVRP ([33, Theorem 2]) only applied to \(\|_{k}-_{*}\|^{2}\), which can also be derived by our results from strong convexity: \(f(_{k})-f(_{*})\|_{k}-_{*} \|^{2}\). However, the reverse is not applicable since we do not assume the smoothness of \(f\), or indeed the smoothness coefficient is very large. Moreover for ill-conditioned problems (e.g., \(/\)), our step size \(1/(4)\) is much larger than \(/(2^{2})\) used in SVRP, and the convergence rate is also faster than SVRP: \(}(n+/)\) vs. \(}(n+^{2}/^{2})\). Finally, we do not need the strong convexity assumption of component functions._

### Acceleration Version: AccSVRS

Now we apply the classical interpolation technique motivated by Katyusha X  to establish accelerated SVRS (AccSVRS, Algorithm 2). The main difference between AccSVRS and Katyusha X is due to the different choices of distance spaces. Specifically, we adopt \(D_{h}(,)\) instead of the Euclidean distance used in Katyusha X. Thus, the gradient mapping step (corresponding to Step 2 in [6, (4.1)]) should be built on the reference function \(h()\) defined in Eq. (8), i.e., \( h(_{k+1})- h(_{k+1})\) instead of \((_{k+1}-_{k+1})/\). Moreover, noting that \( h()\) could involve the heavy gradient computing part \( f()\), we further employ its stochastic version (Step 5 in Algorithm 2) to reduce the overall communication complexity.

Next, we delve into the convergence analysis. We first give the core lemma for AccSVRS, which is also motivated by the framework of Katyusha X . The proof is deferred to Appendix D.3.

**Lemma 3.5**: _Suppose Assumption 1 holds, and \(=1/(4),p=1/n, n/(2)\) in Algorithm 2, where \(^{}(f,_{k+1},,p)\) satisfies Eq. (10) in each iteration. Then for all \(^{d}\) that is independent of the random indices \(i_{1}^{(k)},i_{2}^{(k)},,i_{T}^{(k)}\) in \(^{}(f,_{k+1},,p)\), we have that_

\[_{k}[f(_{k+1})-f()] _{k}(1-)[f(_{k})-f() ]+-_{k}\|^{2}}{2}-\|-_{k+1}\|^{2}.\] (12)

Finally, we present the convergence rate and communication complexity of AccSVRS based on Lemma 3.5, and the proof is left in Appendix D.4.

**Theorem 3.6**: _Suppose Assumption 1 holds. Consider AccSVRS with the following hyperparameters_

\[=},p=,=\{1, }{2}}\},=}{8 },\]

_and Eq. (10) is satisfied in each iteration of \(^{}(f,_{k+1},,p)\). Then for any \(>0\), when_

\[k K_{2}:=\{4,8n^{-1/4}\}_{0})-f(_{*})]}{},\]

_i.e., after \(}(n+n^{3/4})\) communications in expectation, we obtain that \(f(_{k})-f(_{*})\)._

**Remark 3.7**: _Although roughly the same as the communication complexity obtained by Catalyzed SVRP in [33, Theorem 3], our results have the following advantages._

_Fewer assumptions. Except for the strong convexity of \(f\) and AveSS of \(f_{i}\)'s, we do not need to assume component strong convexity appearing in [33, Assumption 2]._

_Inexact proximal step. Khaled and Jin [33, Theorem 3] require exact evaluations of the proximal operator, though they mention that this is only for the convenience of analysis. Our framework allows approximated solutions in each proximal step, and the approximation criterion (10) is error-independent, i.e., irrelevant to the final error \(\). Since the local proximal function is strongly convex, we could solve the problem in a few steps if additionally assuming the smoothness of \(f_{1}\)._

_Smoothness-free bound. As shown in [33, Appendix G.1] or Appendix C, even if an exact proximal step is allowed, a dependence on the smoothness coefficient would be introduced in the total communication iterations of Catalyzed SVRP, though only in a log scale. Our directly accelerated method has no dependence on the smoothness coefficient._

### Gradient Complexity under Smooth Assumption

Due to the importance of total computation in the machine learning and optimization community, we consider a more common setup by **additionally assuming** that \(f_{1}\) is \(L\)-smooth with \(L>0\), which together with Assumption 1 facilitates the quantification of Eq. (10). Then we can compute the total gradient complexity for AccSVRS as shown below. By Proposition 2.5 and our assumptions, \(A_{}^{t}()\) is \((-)\)-strongly convex and \((+L)\)-smooth. Using accelerated methods starting from \(_{t}\), we can guarantee that Eq. (10) holds after \(T_{}=}(}})=}(1+n^{-1/4})\) iterations with the choice of \(\) in Theorem 3.6. Hence, the total gradient calls in expectation are

\[(nT_{} K_{2})=}(n+n^{3/4} (+)+).\]

Since \([,L]\), we recover the optimal gradient complexity \(}(n+n^{3/4})\) for the average smooth setting [63, Table 1] if neglecting log factors. Particularly, when \(=()\), we even obtain the nearly optimal gradient complexity \(}(n+)\) for the component smooth setting . We leave the details in Appendix E. Although the gradient complexity is not the primary focus of our work, we have demonstrated that the gradient complexity bound of AccSVRS is nearly optimal for certain values of \(\) in specific cases.

## 4 Lower Bound

In this section, we establish the lower bound of the communication complexity, which nearly matches the upper bound of AccSVRS.

### Definition of Algorithms

In this subsection, we specify the class of algorithms to which our lower bound can apply. We first introduce the Proximal Incremental First-order Oracle (PIFO) , which is defined as \(h_{f_{i}}^{}(,)=[f_{i}(), f_{i}(), _{f_{i}}^{}()]\) with \(>0\). Here the proximal operator is defined as \(_{f_{i}}^{}():=_{}\{f_{i}()+\|-\|^{2}\}\). In addition to the local zero-order and first-order information of \(f_{i}\) at \(\), the PIFO \(h_{f_{i}}^{}(,)\) also provides some global information through the proximal operator9. Then we assume the algorithm has access to the PIFO and the definition of algorithms is presented as follows.

**Definition 4.1**: _Consider a randomized algorithm \(\) to solve problem (1). Suppose the number of communication rounds is \(T\). At the initialization stage, the master node \(1\) communicates with all the others. In round \(t\)\((0 t T-1)\), the algorithm samples a node \(i_{t}([n])\), and node \(1\) communicates with node \(i_{t}\). Then the algorithm samples a Bernoulli random variable \(a_{t}\) with constant expectation \(c_{0}/n\). If \(a_{t}=1\), node \(1\) communicates with all the others. Define the information set \(_{t+1}\) as the set of all the possible points \(\) can obtain after round \(t\). The algorithm updates \(_{t+1}\) based on the linear-span operation and PIFO, and finally outputs a certain point in \(_{T}\)._

At the initialization stage, the communication cost is \(2(n-1)\). In each communication round, the Bernoulli random variable \(a_{t}\) determines whether the master node communicates with all the others, i.e., whether to calculate the full gradient. Since \(a_{t}=c_{0}/n\), the expected communication cost of each round is of the order \((1)\). Thus the total communication cost is of the order \((n+T)\) and we can use \(T\) to measure the communication complexity. Moreover, one can check Algorithm 2 satisfies Definition 4.1. The formal definition and detailed analysis are deferred to Appendix F.1.

### The Construction and Results

In this section, we construct a hard instance of problem (1) and then use it to establish the lower bound. Due to space limitations, we only present several key properties. The complete framework of construction is deferred to Appendix F.2.

Inspired by , we consider the class of matrices \((m,)=[1&-1&\\ &&\\ &&1&-1\\ &&&]^{m m}\). This class of matrices is widely used to establish lower bounds for minimax optimization problems , and \((m,):=(m,)^{}(m,)\) is the well-known tridiagonal matrix in the analysis of lower bounds for convex optimization . Denote the \(l\)-th row of \((m,)\) as \(_{l}(m,)^{}\). We partition the row vectors of \((m,)\) according to the index sets \(_{i}=\{l:1 l m,\,l i-1\,(\,\,(n-1))\}\) for \(2 i n\) and \(_{1}=\)10. These sets are mutually exclusive and their union is \([m]\). Then we consider the following problem

\[_{^{m}}r(;m,,c)\!\!=\!\!_{i=1 }^{n}r_{i}(;m,,c)\!\!:=\!\!\| \|^{2}\!-\!n_{1},&i=1,\\ \|\|^{2}\!+\!_{l_{i}}\|_{l}(m,)^{}\|^{2}&i 1.\] (13)

Here \(_{i}^{m}\) denotes the unit vector with the \(i\)-th element equal to \(1\) and others equal to \(0\). Then one can check \(r(;m,,c)=\|^{}(m,)\,+ \|\|^{2}-_{1}, )\). Clearly, \(r\) is \(c\)-strongly convex. We can also determine the AveSS parameter as follows. The proof is deferred to Appendix F.3.

**Proposition 4.2**: _Suppose that \(0<\), \(n 3\) and \(m 3\). Then \(r_{i}\)'s satisfy \(\)-AveSS._

Define the subspaces \(\{_{k}\}_{k=0}^{m}\) as \(_{0}=\{\}\) and \(_{k}=\{_{1},_{2},,_{k}\}\) for \(1 k m\). The next lemma is fundamental to our analysis. The proof is deferred to Appendix F.5.

**Lemma 4.3**: _Suppose the algorithm \(\) satisfies Definition 4.1 and apply it to solve problem (13) with \(n 3\) and \(m 4\). We have (i) \(_{0}=_{1}\). (ii) Suppose \(_{t}_{k}\) (\(1 k m-3\)). If \(i_{t}\) satisfies \(k_{i_{t}}\) or \(a_{t}=1\), then \(_{t+1}_{k+3}\); otherwise, \(_{t+1}_{k}\)._Lemma 4.3 guarantees that in each round, only when a specific component is sampled or the full gradient is calculated, can we expand the information set by at most three dimensions. For problem (13), we could never obtain an approximate solution unless we expand the information set to the whole space (see Proposition F.6 in Appendix F.2), while Lemma 4.3 implies that the process of expanding is very slow. Then we can establish the following lower bound.

**Theorem 4.4**: _For any \(n 3\), \(,>0\), algorithm \(\) satisfying Definition 4.1 and sufficiently small \(>0\), there exists a rescaled version of problem (13) such that (i) Assumption 1 holds; (ii) In order to find an \(\)-suboptimal solution \(}\) such that \(r(})-_{}r()<\) by \(\), the communication complexity in expectation is \((n+n^{3/4})\)._

This lower bound nearly matches the upper bound in Theorem 3.6 up to log factors, implying Algorithm 2 is nearly optimal in terms of communication complexity. The detailed statement and proof are deferred to Appendices F.2 and F.9.

## 5 Experiments

To demonstrate the advantages of our algorithms, we conduct the same numerical experiments as those in [35; 33]. We focus on the linear ridge regression problem with \(_{2}\) regularization, where the average loss \(f\) has the formulation: \(f()=_{i=1}^{n}[f_{i}():=_{j=1} ^{m}(_{i,j}^{}-y_{i,j})^{2}+\| \|^{2}]\). Here \(_{i,j}^{d}\) and \(y_{i,j}, i[n],j[m]\) serve as the feature and label respectively, and \(m\) can be viewed as data size in each local client. We consider a synthetic dataset generated by adding a small random noise matrix to the center matrix, ensuring a small \(\). To capture the differences in convergence rates between our methods and SVRP caused by different magnitudes of \(\), we vary \(=10^{-i},i\{0,1,2\}\). We compare our methods (SVRS and AccSVRS) against SVRG, KatyushaX, SVRP (Catalyzed SVRP is somehow hard to tune so we omit it), and Accelerated Extragradient (AccEG) using their theoretical step sizes, except that we scale the interpolation parameter \(\) in KatyushaX and AccSVRS for producing practical performance (see Appendix G for detail). From Figure 1, we can observe that for a large \(\), SVRP outperforms existing algorithms due to its high-order dependence on \(\). However, when the problem becomes ill-conditioned with a small \(\), AccSVRS exhibits significant improvements compared to other algorithms.

## 6 Conclusion

In this paper, we have introduced two new algorithms, SVRS and its directly accelerated version AccSVRS, and established improved communication complexity bounds for distributed optimization under the similarity assumption. Our rates are entirely smoothness-free and only require strong convexity of the objective, average similarity, and proximal friendliness of components. Moreover, our methods also have nearly optimal gradient complexity (leaving out the log term) when applied to smooth components in specific cases. It would be interesting to remove additional log terms to achieve both optimal communication and local gradient calls as , as well as investigating the complexity under other similarity assumptions (such as SS instead of AveSS) in future research.

Figure 1: Numerical experiments on synthetic data. The corresponding coefficients are shown in the title of each graph. We plot the function gap on a log scale versus the number of communication steps, where one exchange of vectors counts as a communication step.