# Spuriosity Rankings:

Sorting Data to Measure and Mitigate Biases

 Mazda Moayeri\({}^{1}\)  Wenxiao Wang\({}^{1}\)  Sahil Singla\({}^{2,1}\)1

\({}^{1}\) University of Maryland

\({}^{2}\) Google

{mmoayeri, wxx, sfeizi} @umd.edu, sasingla@google.com

Work carried out while at the University of Maryland.

###### Abstract

We present a simple but effective method to measure and mitigate model biases caused by reliance on spurious cues. Instead of requiring costly changes to one's data or model training, our method better utilizes the data one already has by sorting them. Specifically, we rank images within their classes based on spuriosity (the degree to which common spurious cues are present), proxied via deep neural features of an interpretable network. With spuriosity rankings, it is easy to identify minority subpopulations (i.e. low spuriosity images) and assess model bias as the gap in accuracy between high and low spuriosity images. One can even efficiently remove a model's bias at little cost to accuracy by finetuning its classification head on low spuriosity images, resulting in fairer treatment of samples regardless of spuriosity. We demonstrate our method on ImageNet, annotating \(5000\) class-feature dependencies (\(630\) of which we find to be spurious) and generating a dataset of \(325k\) soft segmentations for these features along the way. Having computed spuriosity rankings via the identified spurious neural features, we assess biases for \(89\) diverse models and find that class-wise biases are highly correlated across models. Our results suggest that model bias due to spurious feature reliance is influenced far more by what the model is trained on than how it is trained.

Figure 1: Spuriosity rankings sort images based on the presence of spurious cues. The rankings reveal hidden spurious features that models rely on, like _rippling water, human hands,_ and _flame in the dark_ (right; top to bottom), and uncover minority subpopulations where spurious cues are absent (left).

## 1 Introduction

Deep models often exhibit bias, underperforming on certain minority subpopulations [6; 10]. One cause of bias is the tendency of deep models to rely on spurious features [45; 39; 1], as samples that retain spurious correlations are predicted more accurately than rarer samples where spurious correlations are broken. For example, a classifier may learn to associate water with the _otter_ class if many training images contain water, which in turn can lead to reduced accuracy on images of otters out of water. Spurious feature reliance also hinders robust generalization (e.g. medical systems trained in one hospital may fail when deployed to others [57; 12]), potentially posing major reliability risks.

Recent trends suggest that scaling up models and datasets may resolve many existing limitations . However, in addition to being costly, collecting more data would not alter the long-tail data distribution that leads to spurious feature reliance, as the spurious cues will remain just as correlated, if not more, with class labels. Instead of seeking more data, we suggest to _better utilize the data one already has_. Specifically, we aim to rank images within their classes by _spuriosity_, which can be understood informally as the degree to which relevant spurious cues are present. Figure 1 shows examples of our rankings. For the _otter_ class, high spuriosity images prominently contain the common spurious feature of water, while low spuriosity images do not, instead displaying the rarer case of otters on land.

Spuriosity, while intuitive, is rather challenging to measure scalably: for each class, one must (i) discover relevant cues (i.e. those that a model will rely upon), (ii) identify the spurious ones, and (iii) obtain a function that scores the level to which these spurious cues are present in a given image. To this end, we propose to extract spurious concept detectors from within an _interpretable model_ trained on the given data. Namely, we automatically select neural features (i.e. nodes in the penultimate layer) that contribute highly to a class logit, as they encode concepts that are both salient to that class and relied upon by the model. Then, we interpret the concept each neural feature detects, with limited human supervision, so to keep the ones activated by spurious cues. Finally, we average the activation on these neural features to yield a scalar for each image that proxies the presence of relevant spurious cues (i.e. spuriosity), thus enabling sorting. Once data is ranked by spuriosity, multiple significant benefits emerge immediately: First, low spuriosity images **reveal minority subpopulations**, where common spurious correlations are broken. Second, we can easily **quantify model bias** as the drop in accuracy between high and low spuriosity images; we call this _spurious gap_. Third, we can **mitigate bias** towards high spuriosity images by simply finetuning the classification head on low spuriosity images.

Figure 2 diagrams our approach. The interpretable model we use to detect relevant neural features and extract spurious ones (steps 1 and 2 respectively) is adversarially trained, as in . However, we show for the first time that we can extend this framework _without adversarial training_. Namely, to discover patterns salient for a new task, it suffices to merely fit a linear classification head on the new data over fixed adversarially pre-trained neural features. This dramatically increases the efficiency of the method, and expands its use cases to low-data regimes where adversarial training is less feasible. We carry out steps 1 and 2 at an unprecedented scale, resulting in \(5000\) annotated class-neural feature

Figure 2: Overview of our framework. Per class, we sort images by _spuriosity_, which we proxy via the activation of neural features (i.e. nodes in the penultimate layer of a trained classifier) that focus on spurious cues. After sorting, measuring model bias to the discovered spurious features is as simple as computing the drop in accuracy across high and low spuriosity images. This bias can be mitigated efficiently by tuning the classification head on low spuriosity images.

dependencies over all of the ubiquitous ImageNet, including \(630\) spurious features discovered over \(357\) classes. For these classes, we compute spurious rankings (step 3), which, along with a web-UI* for easy viewing and \(325k\) soft segmentations of salient visual features in ImageNet, we present as a dataset to shed insight on _how_ deep models perform ImageNet classification.

Footnote *: salient-imagenet.cs.umd.edu

Using spuriosity rankings, we compute _spurious gaps_ (drop in accuracy from high to low spuriosity images) for **89 models**, observing that all models underperform on low spuriosity images. Models exhibiting the most and least bias to spurious features are CLIP and adversarially trained models respectively, corroborating prior work [28; 37] and thus further validating our rankings. We also observe diverse models to have highly correlated class-wise spurious gaps. This implies that models absorb the same biases, suggesting that _bias is influenced far less by how a model is trained, and far more by what it is trained on_. Thus, frameworks like ours that demystify large datasets can be instrumental in understanding and mitigating bias. Indeed, we show that finetuning existing classification heads on low spuriosity images is an efficient and effective model-agnostic way to close spurious gaps at little cost of validation accuracy, resulting in fairer, more stable treatment of inputs, regardless of spuriosity.

We also observe surprising instances of negative spurious gaps, i.e. classes where models perform worse when spurious cues are present. These cases are due to spurious feature collision, where one class' spurious feature is also correlated more strongly with another class. Existing benchmarks  lack the scale of ours to reveal this atypical yet harmful consequence of spurious feature reliance. Closer inspection reveals high spuriosity images for classes with negative spurious gaps are often mislabeled or contain multiple objects. Thus, spuriosity rankings can help in flagging label noise, and in some cases (via cropping to isolate regions activating core neural features), resolving it.

In summary, we present a simple, extensible method for (i) discovering spurious features (ii) measuring and (iii) mitigating model biases caused by these spurious features. Centering on the fact that spurious correlations are determined by data, our solution focuses on better understanding and using the data one already has, in contrast to data-agnostic approaches which focus on altering model training [2; 41]. Arguably, our method provides more practical robustness, as we first interpret potential biases a model trained on the given data is likely to suffer, and then devise solutions specifically tailored to these biases. By shifting focus from model training to data understanding, spuriosity rankings allow for efficient and interpretable robustness interventions, towards more reliable AI.

## 2 Review of Literature

**Spurious correlation benchmarks** typically consider a small set of predefined spurious attributes fixed across classes [55; 42; 25], and real-world benchmarks tend to be very specific to a narrow task . In both cases, the fixed spurious attributes are annotated manually. Similarly, many have proposed variants of ImageNet's validation set, collecting new data (e.g. with altered renditions, as sketches, in household settings and odd poses, etc [16; 49; 3]) and inspecting the drop in accuracy compared to the original validation set. In contrast, our framework provides a _general_ method to uncover and automatically annotate (i.e. per sample) the presence of spurious features relevant to _any_ task given its training data, as well as measure and mitigate model bias caused by these spurious features. Since bias arises from data, we believe our step of first interpreting the correlations within a dataset, instead of simply choosing spurious features of interest apriori, is paramount in order to build _practical_ robustness (i.e. to distribution shifts that are naturally occurring and potentially harmful, since they break spurious correlations a model trained on the given data likely uses). Also, we account for the fact that whether or not a feature is spurious depends crucially on its class: e.g. while the _water_ feature is spurious for the _other_ class, it is not spurious for a class like _lakeshore_ (also, see figure 3). Most benchmarks overlook this, defining spurious features class-agnostically. Other approaches to benchmark spurious feature reliance require synthetic data [2; 54; 13] or corrupting spurious regions, which undesirably takes models out of distribution [29; 30]. Instead, our benchmark operates only on natural images one already has, using spuriosity to curate subsets with spurious cues strongly present and absent.

**Natural image-based interpretability** consists of explaining inferences using examples from the corpus [8; 11] or generated examples intended to visually reflect a traversal between concepts . Similarly, our rankings traverse from strong spuriosity to minority examples with spurious cues absent, though we do not require generative models. Other related approaches are influence functions  and data models , though they are computationally expensive and the former can be fragile .

**Mitigating spurious correlation reliance** often involves altering the training objective to learn more invariant features [2; 38] or directly optimize worst group accuracy [41; 18; 40; 58; 48]. Most methods require extra supervision with respect to non-class-label attributes . Retraining approaches either focus on correcting errors [24; 59; 32] or using balanced data (i.e. dominant spurious cues balanced with minority samples) ; our rankings would directly assist in constructing such balanced subsets. We more directly compare error-correcting approaches to our method in Appendix F.

**Neural features** have been studied to interpret models [35; 17]. Notably, adversarially robust neural features have enhanced interpretability [47; 52]. Singla and Feizi  introduced a framework that uses robust neural features to discover core and spurious data patterns with minimal human supervision. We apply this framework to scalably compute spuriously rankings. In the next section, we explain this framework and our novel contributions to it, including how to use the framework without requiring its most prohibitive step of adversarial training, thus dramatically increasing its use cases.

## 3 Discovering Spurious Features in ImageNet and Beyond

A key component of our framework for interpreting and improving a model's robustness to spurious correlations is that we first discover relevant spurious features _based on the training data_, as opposed to data-agnostic approaches. Specifically, we leverage the feature discovery method of , performing it at an unprecedented scale (i.e. across all of ImageNet). We now provide a brief overview of the method, details on our expansion, and a novel extension of the method with significant impacts. For complete details on feature discovery and our human studies, we refer readers to Appendix I.

### \(5000\) Reasons Deep Models Use to Perform ImageNet Classification

Despite its ubiquity, ImageNet (and any other large-scale dataset) is opaque in the sense that a human cannot anticipate the patterns a model trained on it will associate with each class. The reason for this is simple: humans cannot process a million (or even \(1000\)) images at once. Moreover, the patterns a human may use will not necessarily align with those a model will use . Nonetheless, understanding the features (especially the spurious ones) that a model will rely upon is instrumental in anticipating and mitigating the biases a model will suffer from.

To understand the features any general model may rely upon, we inspect the _neural_ features of a single, interpretable model; namely, an _adversarially trained_ one. Adversarial training leads to perceptually aligned gradients , which greatly improve the utility of gradient-based interpretations. Specifically, using the gradient of a neural feature's activation w.r.t. the input image, one can reliably generate a heatmap highlighting the input regions activating the neural feature, and even perturb the input image to visually amplify the cue that the feature detects; the latter method is called a feature attack. Thus, given a robust neural feature, a human can annotate its function as core or spurious for a class by inspecting the images within the class that activate the feature most (we use top 5), along with heatmaps and feature attacks for those images (see Figure 3). Note that given a class, one can automatically select important neural features based on the average contribution of the feature to the class logit, which can easily be computed by inspecting feature activations and linear classification head weights. These steps make up the feature discovery framework of ; to summarize, (i) adversarially train a model, (ii) automatically select important neural features per class, and (iii) use complementary visualization techniques to annotate a neural feature as core or spurious with minimal human supervision.

Figure 3: Examples of the visualizations we use to identify the focus of a robust neural feature. Here, feature \(595\) focuses on flowers, making it core for _Daisy_ and spurious for _Monarch Butterfly_.

Singla and Feizi  annotated the \(5\) most relevant neural features for \(232\) classes of ImageNet. Through a large-scale human study (details in Appendix I), we expand the analysis to all \(1000\) classes, resulting in \(5000\) annotated class-feature pairs, of which \(630\) are spurious over \(357\) classes. We host a web-UI to view all \(5000\) pairs, offering a direct visual look into the patterns a neural network sees and uses across ImageNet. We also verify that heatmaps for annotated features localize the same cue in images whose activation on the feature is in the top \(20^{th}\) percentile for the class, successfully validating \(95.3\%\) of annotated class-feature pairs (see appendix for details). Thus, we generate \(325,000\) feature soft segmentations across ImageNet as a bonus. Crucially, the validation confirms that sorting by feature activation is effective in gathering instances where a feature is present.

While we use the feature discovery method of  directly, we make a number of impactful contributions atop it. Namely, we expand its use cases significantly by removing the requirement of adversarial training. Also, we overhaul the original procedure for assessing model reliance on spurious features, making it far more efficient, less biased, and more stable (Appendix I.4). Key to our improvements is the use of _lowest_ activating images as natural counterfactuals to better interpret spurious features and measure their effects, enabling new cross-class and cross-model analyses. Lowest activating images (never used in ) are also crucial for computing and closing spurious gaps (Sections 4.2 and 4.3).

### Spurious Feature Discovery _without_ Adversarial Training

Adversarially robust neural features are at the heart of our framework, though adversarial training can be challenging, particularly when data is limited. We propose to simply fit a linear layer for a new task atop fixed features from an adversarially robust feature encoder pretrained on ImageNet; prior work shows transfer learning on robust features is very effective . We then perform the usual process of identifying important robust neural features per class and annotating them as core or spurious.

We demonstrate this light-weight extension of the feature discovery framework of  on two spurious correlation benchmarks: Waterbirds, and Celeb-A Hair Color classification. Figure 4 shows some of the discovered cues. For Waterbirds, we find neural features that focus entirely on backgrounds (the intended spurious feature for this benchmark). The lowest-ranked images within a class for these features contain the opposite background (land instead of water), thus revealing the minority subgroup (waterbirds on land). For Celeb-A, we observe features that spuriously focus on faces (instead of hair). Further, race appears to be homogeneous amongst the top-ranked images, and opposite in the bottom-ranked images. Thus, a model may spuriously associate brown skin with the brown hair class, likely leading to failures for blond-haired brown-skinned people. To our knowledge, _we are the first to uncover this racial bias in the Celeb-A benchmark_, whose intended spurious feature was gender.

To showcase the ease-of-use of our method and the wide-reaching nature of the spurious correlation problem, we apply the lightweight feature discovery framework to a _non_-spurious correlation benchmark, finding numerous spurious cues. We take UTKFace , a dataset of face images, and focus on the tasks of gender and race classification (details in Appendix E). Figure 5 visualizes three detected spurious features: suit and tie for the _male_ class, the color pink for the _female_ class, and glasses

Figure 4: With only linear layer retraining, we detect designated spurious attributes like background in the Waterbirds benchmark (left), as well as new spurious dependencies such as racial bias in Celeb-A Hair Classification (right). Presented above are highest activating images (top row), corresponding heatmaps (middle), and lowest activating images (bottom) for spurious features observed for the classes _waterbird, landbird, blond hair,_ and _brown hair_ (left to right).

for the _Indian_ class. Thus, using robust neural features to discover spurious features is effective, even when the neural features were not trained on the data of interest; simply fitting a linear layer (which can be done in minutes) over these fixed features sufficed in revealing spurious features. The spurious features are prominently displayed in the top activating images, highlighted in the heatmaps, and _notably absent in the least activating images_, indicating that sorting images based on robust neural feature activations can reveal minority subpopulations where spurious correlations are broken. These low spuriosity images prove to be instrumental in measuring and mitigating biases (Section 4).

### Is a Human in the Loop Necessary?

Our framework involves a human in the loop, who is given (i) concise insight to the cues a model trained on the given data may rely upon and (ii) agency to decide which of these cues model performance should be invariant to. We believe this increased transparency fosters greater trust with practitioner. Further, the human involvement is limited to viewing a handful of images per class, making our framework tractable (as we demonstrate by performing it on the \(1000\) class ImageNet dataset). Nonetheless, in cases where minimizing costs is preferred, the human in the loop can be removed, either by automating the feature interpretation step, or by using open-vocabulary models to directly embed concepts into representation space from text descriptions ; we describe these variations to our framework in Appendix G. The key idea of our work is to leverage the interpretability of certain existing models to uncover vectors in representation space corresponding to spurious cues, with which we can scalably sort data by computing similarity to these vectors. The method we present uses axis-aligned vectors of adversarially trained network (i.e. robust neural features), though our framework is not restricted to this instance. We now show how sorting enables for greater utilization of the data one already has, towards resolving the biases caused by relying on spurious correlations.

## 4 Measuring and Mitigating Biases with Spuriosity Rankings

### Spuriosity Rankings: Organizing Image Data via Robust Neural Features

Equipped with a method to identify robust neural features that detect relevant spurious cues, we now utilize these features to scalably quantify _spuriosity_ (i.e. how strongly spurious cues are present in an image). Letting \(_{i}(x)\) denote the activation of image \(x\) on the \(i^{th}\) robust neural feature, and \((c)\) denote the set of neural features annotated as spurious for class \(c\), we compute spuriosity as follows:

_The **Spuriosity** of an image \(x\) for class \(c\), with \(_{ic}\) and \(_{ic}\) denoting the mean and standard deviation of activations on feature \(i\) over class \(c\), is approximated as \((c)|}_{i(c)}_{i}(x)- _{ic}}{_{ic}}\)._

Essentially, we use the activation of robust neural features as spurious concept detectors to proxy the degree to which relevant spurious cues are present in each image of a dataset _efficiently_; that is, with only a single forward pass of the dataset through the adversarially trained network. Given a class, averaging the distribution-adjusted activations of relevant spurious neural features yields a single scalar value for each image, with which images can be ranked within their classes.

Figure 5: Our lightweight extension of  also reveals spurious features in _non_-spurious correlation benchmarks, such as UTKFace gender and race classification. Spurious cues are absent in the lowest activating images (bottom row). Thus, sorting by feature activation uncovers minority subpopulations.

Note that we define the spuriosity of an image _for a specific class_, since whether a neural feature is relevant or spurious depends on what class it is being used for: figure 3 shows an example feature (flowers) that is core for one class (_Daisy_) and spurious for another (_Monarch Butterfly_). Accounting for the crucial class-dependence of the spurious correlation problem is unique to our framework.

### Measuring Bias: Computing Spurious Gaps

Relying on spurious cues can bias a model, as its performance may degrade when spurious correlations are broken. Measuring this bias can be expensive, as it requires collecting new data where spurious cues are absent. With spuriosity rankings, we instead can easily select such a subset _from one's own data_. Indeed, high spuriosity images prominently display spurious cues, while low spuriosity images do not (Figure 1). Thus, a natural metric enabled by spuriosity rankings is **spurious gap**, defined as the drop in accuracy between the top-\(k\) highest and lowest spuriosity validation images per class. We evaluate spurious gaps over the \(357\) ImageNet classes with at least one spurious feature discovered.

Figure 6 (left) shows accuracy on the \(k=10\) lowest (\(y\)-axis) vs. highest (\(x\)-axis) spuriosity images for \(89\) diverse models pretrained on ImageNet * (details in appendix). We observe _all_ models to be biased, suffering lower accuracy when spurious cues are absent. As in , we observe a strong correlation between the two accuracy measures. However, some models stray from this linear trend, exhibiting 'effective robustness' by performing significantly better on images with spurious cues absent than the linear trend predicts (e.g. zero-shot CLIP) or the opposite (e.g. adversarially trained models). Prior work on distributional robustness corroborates our findings on the effective robustness (and lack thereof) of CLIP and adversarially trained models respectively [37; 28], thus validating our metric. Essentially, **spuriosity rankings organize our data so that we can _simulate distribution shifts caused by breaking spurious correlations, so to assess a model's robustness without needing to collect new data**.

Footnote *: except for the CLIP models, which are trained on more data, and also, perhaps not surprisingly, happen to be the most exceptional in their robustness.

We now turn our attention to class-wise spurious gaps (i.e. we average spurious gaps per class over all models, instead of averaging over all classes per model, as done in Figure 6, left). We find that spurious gap varies significantly more across classes than across models (Figure 6, right). Specifically, the sample variance is \(3.1\) larger for spurious gaps across classes than across models, suggesting that the spurious correlation problem pertains more to the data a model is trained on than to the specifics of the model itself. That is, we argue that class-wise spurious gaps vary so much because each class has its own set of spurious cues that are correlated with the class label to different degrees. In contrast, despite differences in training procedure and architecture, **all models learn from the same data, and thus, absorb the same biases**. To investigate this claim, for each pair of models in our study, we compute the correlation between class-wise spurious gaps. On average, we observe a high Pearson's \(r\) of \(0.69\).

Figure 7 visualizes this strong correlation for five diverse models compared to the interpretable model used to discover spurious features in Section 3. We highlight this case as it justifies our approach in using a single model (namely, an \(_{2}\) adversarially trained ResNet50) to reveal biases for any general

Figure 6: All models are biased: accuracy is \(10\) to \(30\%\) lower when spurious cues are absent compared to when they are present (**left**). However, bias varies far more significantly across classes than across models (**right**), emphasizing that bias due to relying on spurious cues is crucially class-dependent.

model. Because the spurious correlations a model learns depends far more on _what_ it is trained on (i.e. the data) than _how_ it is trained, we can generalize the spurious features we discover with one model to others. The results of our large-scale empirical analysis support this claim, as all models display bias to the spurious cues discovered with our interpretable model, and furthermore, the degree to which these models are biased per-class are strongly correlated. While it is not surprising that data determines the spurious features a model learns to rely on, we stress this point, as many existing methods for spurious correlation robustness are data-agnostic. In contrast, our method is _data-centric_. Namely, we first discover the spurious features models may rely upon _based on the data_, via an interpretable (essentially, a surrogate) model trained or fit on the data. Then, with spuriosity rankings, we organize our data to allow for efficient measurement of biases per class. We now show how wisely selecting data, via our rankings, for further tuning can mitigate model bias caused by spurious features.

### Mitigating Bias: Closing Spurious Gaps

Having identified a bias across ImageNet-trained models, where images that lack spurious cues common to their class are predicted with considerably lower accuracy, we now present a very simple and efficient manner to mitigate this bias at little cost of validation accuracy. In general, to improve some pretrained models on a more specific downstream data distribution, one simply finetunes the model on new data from the target distribution. With spuriosity rankings, we remove the need to acquire new data, as we instead simply select from the training data we already have. In this case, we seek to improve the performance of models on images with spurious cues absent; _this is precisely the low spuriosity images_. Applying our rankings to training data, we can extract a small subset that captures minority subpopulations under-represented in the overall training distribution. Then, we tune the existing classification head on this small subset so to recalibrate the model towards reduced reliance on spurious features and fairer treatment of samples, regardless of spuriosity.

Figure 8: **(Left) Finetuning on low spuriosity images closes reduces spurious gap to below \(5\%\) at little cost of validation accuracy. (Right) Closing spurious gap removes bias to images with high spurious ranks; that is, regardless of spuriosity, the model performs with roughly the same accuracy.**

Figure 7: Class-wise spurious gaps are strongly correlated across models. While spuriosity rankings are empirically computed with the help of a single \(_{2}\) adversarially trained (AT) ResNet, diverse models show similar sensitivities to the detected spurious features.

We choose to only modify the classification head, as prior work suggests deep models already learn features that are sufficiently informative to correctly predict minority subpopulations . Also, this way, we minimally change the existing model, leading to better retention of validation accuracy, and _extremely fast_ bias mitigation, as we only optimize a tiny fraction of the total model parameters using a small fraction of the entire dataset. Specifically, we tune the classification heads of five models spanning diverse architectures (ResNet and ViT) and training procedures (supervised, self-supervised, adversarial) on the \(100\) lowest spuriousity images for each of the \(357\) classes for which we discover one spurious feature. This tuning occurs _in minutes_ on a single GPU, especially since the data passes through the entire model only once; after caching features, all computation is limited to the linear classification head. Also, we employ early stopping, halting tuning once spurious gap drops below \(5\%\), so to avoid overfitting to the minority subpopulations at the cost of overall accuracy.

Figure 8 visualizes the results of our bias mitigation, plotting the spurious gap (over all \(357\) classes) vs. validation accuracy before and after our tuning. We also include as a baseline the results for tuning on a subset consisting of \(100\) randomly selected images from each of the same \(357\) classes. We find that **low spuriousity tuning mitigates bias at a minimal cost to validation accuracy**, while the baseline does not. Namely, low spuriousity tuning closes spurious gaps, reducing it by \(10\) to \(20\%\), while only sacrificing \(1\) to \(3\%\) validation accuracy. In the subplot on the right, we see how low spuriousity tuning effectively removes bias to high spurious images. While the randomly tuned baseline (like the original model) has far better accuracy on images with high spurious ranks, the low spurious tuned model has far more stable treatment of all samples, evidenced by a much flatter curve when plotting accuracy vs. spuriousity rank. In appendix F, we consider a second baseline where we tune on samples misclassified by the original model, inspired by existing spurious correlation mitigation techniques [24; 59; 32]. Unlike low-spuriosity tuning, this baseline also fails to close spurious gaps, and actually results in a more substantial drop in overall validation accuracy.

## 5 Additional Application: Flagging and Fixing Labeling Errors

We now demonstrate an additional manner in which spuriousity rankings can help manage and improve the data one already has. Modern datasets, like models, are opaque in the sense that it can be challenging to answer simple yet significant questions about them, largely due to their ever-increasing size. One such question is what spurious features a model will learn to rely upon, and to what degree, after training on a given dataset; our spuriousity rankings framework is designed to answer this question. A related question is how accurate the labels in a dataset are. Even in some of the most widely used vision datasets like ImageNet, label noise has been observed to be pervasive , and when it is resolved, models trained on the refined data experience improved accuracy and robustness .

A common culprit for label noise in ImageNet is the presence of multiple objects in the same image. Similarly, co-occurring objects are amongst the most common types of spurious cues we observe

Figure 9: **(Left) Training images with highest spuriosity for the classes with the most negative spurious gaps. These images often contain multiple objects, at times more prominently showing an object from a class different than the image’s label (e.g. _car_, _monitor_ or _keyboard_, _spider_). **(Right)** Using neural activation maps for neural features annotated as _core_ (i.e. not spurious) for the class may serve as an automated way to crop out the conflicting objects.**

models to rely upon. For example, car wheels almost always co-occur with a car, and so, a model may associate the metallic body of a car with the _car wheel_ class. Indeed, we discover this spurious feature in our analysis. However, while we typically expect a model to underperform when a spurious cue is absent, we observe the opposite: the car wheel class has a _negative_ spurious gap, with models on average having \(54.2\%\) lower accuracy on high spuriosity car wheel images than low spuriosity ones. While uncommon, negative spurious gaps are still prevalent, occurring in \(15.7\%\) of classes we study. This highlights a second, often overlooked, harm incurred by spurious feature reliance. Namely, since spurious features are not essential to a class, they can be correlated with multiple classes at once, leading to _spurious feature collision_. Indeed, a spurious feature for one class may also be core for another, like the car body being spurious for the _car wheel_ class and core for any of the numerous car classes. This is actually the case for a majority of the spurious features we discover, with a staggering \(63.8\%\) of them also being core for a different class. In these instances, if a feature \(f\) is correlated more with class \(c_{1}\) than \(c_{2}\), high spuriosity images from \(c_{2}\) may be misclassified to \(c_{1}\).

Taking a closer look at classes with severely negative spurious gaps reveals instances of mislabeled training data. Figure 9 shows high spuriosity images for classes with the most negative spurious gaps. These images often more prominently display co-occurring objects different from the class label (such as a car, monitor, or spider) which themselves pertain to another class. Thus, one can automatically flag potential instances of label noise by inspecting high spuriosity images for classes with negative spurious gaps. To quantitatively validate this claim, we leverage ImageNet Real Labels , which indicate for each ImageNet validation image whether objects from multiple classes are present. Real labels show \(14\%\) of validation images contain multiple objects. We find that for classes with the 5 most negative spurious gaps (averaged over our model suite), high spuriosity (i.e. ranked in top \(10\%\) of spuriosity) validation images contain multiple objects in \(80\%\) of cases. In contrast, low (bottom \(10\%\)) spuriosity images from the same classes contain multiple objects in only \(8\%\) of cases, indicating that the label noise is likely a result of the hypothesized spurious feature collision. Similarly, for classes who's spurious gap is less than \(-20\%\), Real labels reveal \(42\%\) of the high spuriosity images to contain multiple objects, many times higher than the average rate of label noise.

Taking label-noise flagging a step further, in some cases, we can utilize the neural activation maps of core robust neural features for these classes to automatically refine the mislabeled high spuriosity images. Namely, we can crop the image based on the region that activates core features for the class, to focus on the actual object of interest and remove the spurious co-occurring one (details in Appendix H). Alternatively, segmentation models may be used to refine flagged samples.

## 6 Conclusion

In this work, we tackle the problem of spurious correlations in image classifiers by shifting attention from training algorithms to _data_. As datasets continue to grow, it can be challenging to answer even the simplest questions about what resides within them. We propose to leverage interpretability tools to harness machine perception towards scalably organizing data. Namely, we sort data within each class based on spuriosity, as measured by neural features in an interpretable network. With spuriosity rankings, we can easily retrieve natural examples and counterfactuals of relevant spurious cues from large datasets, enabling efficient measurement and mitigation of spurious feature reliance. We demonstrate the feasibility of our approach on a vast set of models on ImageNet - a far larger and more realistic setting than most prior spurious correlation benchmarks. Further, we observe highly similar biases across our diverse model set, suggesting that biases may be determined more so by what a model sees than how it is trained. We hope our work spurs further exploration into how to best understand and utilize the data we already have, towards more robust and interpretable models.

## 7 Acknowledgements

This project was supported in part by a grant from an NSF CAREER AWARD 1942230, ONR YIP award N00014-22-1-2271, ARO's Early Career Program Award 310902-00001, Meta grant 23010098, HR00112090132 (DARPA/RED), HR001119S0026 (DARPA/GARD), Army Grant No. W911NF2120076, the NSF award CCF2212458, an Amazon Research Award and an award from Capital One. MM also receives support from the ARCS foundation.