# encoder: mlp, encoder network, the output is 12-normalized

Understanding and Addressing the Pitfalls of Bisimulation-based Representations in Offline Reinforcement Learning

 Hongyu Zang\({}^{1}\), Xin Li\({}^{1}\), Leiji Zhang\({}^{1}\), Yang Liu\({}^{2}\), Baigui Sun\({}^{2}\), Riashat Islam\({}^{3}\),

&Remi Tachet des Combes\({}^{4}\), Romain Laroche

\({}^{1}\) Beijing Institute of Technology, China \({}^{2}\) Alibaba Group, China

\({}^{3}\) McGill University, Mila, Quebec AI Institute, Canada \({}^{4}\) Wayve, UK

{zanghyu,xinli,ljzhang}@bit.edu.cn

{ly261666,baigui.sbg}@alibaba-inc.com

riashat.islam@mail.mcgill.ca

{remi.tachet,romain.laroche}@gmail.com

Correspondence to Xin Li.

###### Abstract

While bisimulation-based approaches hold promise for learning robust state representations for Reinforcement Learning (RL) tasks, their efficacy in offline RL tasks has not been up to par. In some instances, their performance has even significantly underperformed alternative methods. We aim to understand why bisimulation methods succeed in online settings, but falter in offline tasks. Our analysis reveals that missing transitions in the dataset are particularly harmful to the bisimulation principle, leading to ineffective estimation. We also shed light on the critical role of reward scaling in bounding the scale of bisimulation measurements and of the value error they induce. Based on these findings, we propose to apply the expectile operator for representation learning to our offline RL setting, which helps to prevent overfitting to incomplete data. Meanwhile, by introducing an appropriate reward scaling strategy, we avoid the risk of feature collapse in representation space. We implement these recommendations on two state-of-the-art bisimulation-based algorithms, MICo and SimSR, and demonstrate performance gains on two benchmark suites: D4RL and Visual D4RL. Codes are provided at https://github.com/zanghyu/Offline_Bisimulation.

## 1 Introduction

Reinforcement learning (RL) algorithms often require a significant amount of data to achieve optimal performance [40; 48; 22]. In scenarios where collecting data is costly or impractical, Offline RL methods offer an attractive alternative by learning effective policies from previously collected data [29; 43; 32; 35; 16; 24]. However, capturing the complex structure of the environment from limited data remains a challenge for Offline RL . This involves pre-training the state representation on offline data and then learning the policy upon the fixed representations [51; 47; 41; 53]. Though driven by various motivations, previous methods can be mainly categorized into two classes: i) implicitly shaping the agent's representation of the environment via prediction and control of some aspects of the environment through auxiliary tasks, _e.g._, maximizing the diversity of visited states [34; 10], exploring attentive contrastive learning on sub-trajectories , or capturing temporal information about the environment ; ii) utilizing _behavioral metrics_, such as bisimulation metrics [11; 13; 5], to capture complex structure in the environment by measuring the similarity of behavior on the representations [52; 7]. The former methods have proven their effectiveness theoretically and empirically in Offline settings [41; 47; 51], while the adaptability of the latter approaches in the context of limited datasets remains unclear. This paper tackles this question.

Bisimulation-based approaches, as their name suggests, utilize the bisimulation metrics update operator to construct an auxiliary loss and learn robust state representations. These representations encapsulate the behavioral similarities between states by considering the difference between their rewards and dynamics. While the learned representations possess several desirable properties, such as smoothness , visual invariance [54; 1; 52], and task adaptation [56; 37; 46; 8], bisimulation-based objectives in most approaches are required to be coupled with the policy improvement procedure [54; 6; 52]. In Offline RL, pretraining state representations via bisimulation-based methods is supposed to be cast as a special case of on-policy bisimulation metric learning where the behavior policy is fixed so that good performance should ensue. However, multiple recent studies [51; 21] suggest that bisimulation-based algorithms yield significantly poorer results on Offline tasks compared to a variety of (self-)supervised objectives.

In this work, we highlight problems with using the bisimulation principle as an objective in Offline settings. We aim to provide a theoretical understanding of the performance gap in bisimulation-based approaches between online and offline settings:"_why do bisimulation approaches perform well in Online RL tasks but tend to fail in Offline RL ones?_" By establishing a connection between the Bellman and bisimulation operators, we uncover that missing transitions, which often occur in Offline settings, can cause the bisimulation principle to be compromised. This means that the bisimulation estimator can be ineffective in finite datasets. Moreover, we notice that the scale of the reward impacts the upper bounds of both the bisimulation measurement2 fixed point and the value error. This scaling term, if not properly handled, can potentially lead to representation collapse.

To alleviate the aforementioned issues, we propose to learn state representations based on the expectile operator. With this asymmetric operator predicting expectiles of the representation distribution, we can achieve a balance between the behavior measurement and the greedy assignment of the measurement over the dataset. This results in a form of regularization over the bisimulation measurement, thus preventing overfitting to the incomplete data, and implicitly avoiding out-of-distribution estimation errors. Besides, by considering the specific properties of different bisimulation measurements, we investigate the representation collapse issue for the ones that are instantiated with bounded distances (_e.g._, cosine distance) and propose a way to scale rewards that reduces collapse. We integrate these improvements mainly on two bisimulation-based baselines, MICo  and SimSR , and show the effectiveness of the proposed modifications.

The primary contributions of this work are as follows:

* We investigate the potential harm of directly applying the bisimulation principle in Offline settings, prove that the bisimulation estimator can be ineffective in finite datasets, and emphasize the essential role of reward scaling.
* We propose theoretically motivated modifications on two representative bisimulation-based baselines, including an expectile-based operator and a tailored reward scaling strategy. These proposed changes are designed to address the challenges encountered when applying the bisimulation principle in offline settings.
* We demonstrate the superior performance our approach yields through an empirical study on two benchmark suites, D4RL  and Visual D4RL .

## 2 Related Work

State representation learning in Offline RLPretraining representations has been recently studied in Offline RL settings, where several studies presented its effectiveness [3; 47; 41; 25]. In this paradigm, we learn state representations on pre-collected datasets before value estimation or policy improvement steps are run. The learned representation can then be used for subsequent policy learning, either online or offline. Some typical auxiliary tasks for pretraining state representations include capturing the dynamical  and temporal  information of the environment, exploring attentive contrastive learning on sub-trajectories , or improving policy performance by applying data augmentations techniques to the pixel-based inputs [9; 35].

Bisimulation-based methodsThe pioneer works by [20; 33] aim to overcome the curse of dimensionality by defining equivalence relations between states to reduce system complexity. However, these approaches are impractical as they usually demand an exact match of transition distributions. To address this issue, [12; 14] propose a bisimulation metric to aggregate similar states. This metric quantifies the similarity between two states and serves as a distance measure to allow efficient state aggregation. Unfortunately, it remains computationally expensive as it requires a full enumeration of states. Later,  devise an on-policy bisimulation metric for policy evaluation, providing a scalable method for computing state similarity. Building upon this,  develop a metric to learn state representations by modeling the latent dynamic transition as Gaussian.  further investigate the independent couple sampling strategy to reduce the computational complexity of representation learning, whereas  propose to learn state representations built on the cosine distance to alleviate a representation collapse issue. Despite the promising results obtained, one of the major remaining challenges in this paradigm is its dependency on coupling state representation learning with policy training. This is not always suitable for Offline settings, given that obtaining on-policy reward and transition differences is infeasible due to our inability to gather additional agent-environment interactions. To adapt bisimulation-based approaches to Offline settings, one solution is to consider the policy over the dataset as a specific behavior policy, and then apply the bisimulation principle on it to learn state representations in a pretraining stage, thus disentangling policy training from bisimulation-based learning. Notably, although there exist recent studies [51; 42] investigating the potential of bisimulation-based methods to pretrain state representations, it has not yielded satisfactory results yet .

## 3 Preliminaries

### Offline RL

We consider the standard Markov decision process (MDP) framework, in which the environment is given by a tuple \(=(,,T,r,)\), with state space \(\), action space \(\), transition function \(T\) that decides the next state \(s^{} T(|s,a)\), reward function \(r(s,a)\) bounded by \([R_{},R_{}]\), and a discount factor \([0,1)\). The agent in state \(s\) selects an action \(a\) according to its policy, mapping states to a probability distribution over actions: \(a(|s)\). We make use of the state value function \(V^{}(s)=_{,}[_{t=0}^{}^{t}r( s_{t},a_{t}) s_{0}=s]\) to describe the long term discounted reward of policy \(\) starting at state \(s\). In the sequel, we use \(T_{s}^{a}\) and \(r_{s}^{a}\) to denote \(T(|s,a)\) and \(r(s,a)\), respectively. In Offline RL, we are given a fixed dataset of environment interactions that include \(N\) transition samples, _i.e._\(=\{s_{i},a_{i},s^{}_{i},r_{i}\}_{i=1}^{N}\). We assume that the dataset \(\) is composed of trajectories generated i.i.d. under the control of a behavior policy \(_{}\), whose state occupancy is denoted by \(_{}(s)\).

### Bisimulation-based Update Operator

The concept of bisimulation is used to establish equivalence relations on states. This is done recursively by considering two states as equivalent if they have the same distribution over state transitions and the same immediate reward [30; 20]. Since bisimulation considers worst-case differences between states, it commonly results in "pessimistic" outcomes. To address this limitation, the \(\)-bisimulation metric was proposed in . This new metric only considers actions induced by a given policy \(\) rather than all actions when measuring the behavior distance between states:

**Theorem 1**.: _[_5_]_ _Let \(\) be the set of all measurements on \(\). Define \(^{}:\) by_

\[^{}(g)(s_{i},s_{j})=|r_{s_{i}}^{}-r_{s_{j}}^{}|+ (g)(T_{s_{i}}^{},T_{s_{j}}^{})\] (1)

_where \(s_{i},s_{j}\), \(r_{s_{i}}^{}=_{a}(a|s_{i})r_{s_{i}}^{a}\), \(T_{s_{i}}^{}=_{a}(a|s_{i})T_{s_{i}}^{a}\), and \((g)\) is the Wasserstein distance with cost function \(g\) between distributions. Then \(^{}\) has a least fixed point \(g_{}^{}\), and \(g_{}^{}\) is a \(\)-bisimulation metric._

Although it is feasible to compute the behavior difference measurement \(g_{}^{}\) by applying the operator \(^{}\) iteratively (which is guaranteed to converge to a fixed point since \(^{}\) is a contraction), this approach comes at a high computational complexity due to the Wasserstein distance on the right-handside of the equation. To tackle this issue, MICo  proposed using an independent couple sampling strategy instead of optimizing the overall coupling of the distributions \(T_{s_{i}}^{}\) and \(T_{s_{j}}^{}\), resulting in a novel measurement to evaluate the difference between states. Additionally, SimSR  further explored the potentiality of combining the cosine distance with bisimulation-based measurements to learn state representations. Both works can be generalized as:

\[^{}G^{}(s_{i},s_{j})=|r_{s_{i}}^{}-r_{s_{j}}^{}|+ _{s_{i}^{} T_{s_{j}}^{}\\ s_{j}^{} T_{s_{j}}^{}}[G^{}(s_{i}^{},s_{ j}^{})],\] (2)

and \(^{}\) has a least fixed point \(G_{}^{}\)3. The instantiation of \(G\) varies in different approaches [6; 52]. For example, in SimSR , the cosine distance is used to instantiate \(G\) on the embedding space, and the dynamics difference is computed by the cosine distance between the next-state pair \((s_{i}^{},s_{j}^{})\) sampled from a transition model of the environment. A more detailed description can be found in Appendix C.

**Lemma 2**.: _[_6_]_ _(**Lifted MDP**) The bisimulation-based update operator \(^{}\) for \(\) is the Bellman evaluation operator for a specific lifted MDP._

Due to this interpretation of the bisimulation-based update operator as the Bellman evaluation operator in a lifted MDP, we can derive certain conclusions about bisimulation by drawing inspiration from policy evaluation methods. In the next section, we will borrow analytical ideas from  to prove that the bisimulation-based objective may be ineffective for finite datasets. We summarize all notations in Appendix A and provide all proofs in Appendix D.

## 4 Ineffective Bisimulation Estimators in Finite Datasets

The high-level idea of bisimulation-based state representation learning is to learn state embeddings such that when states are projected onto the embedding space, their behavioral similarity is maintained. We denote our parameterized state encoder by \(:^{n}\) and a distance \(D(,)\) in the embedding space \(^{n}\) by \(G_{}^{}(s_{i},s_{j}) D((s_{i}),(s_{j}))\). For instance, \(D(,)\) may be the Lukaszyk-Karmowski distance  or the cosine distance . To avoid unnecessary confusion, we defer implementation details to Section 5.

When considering bisimulation-based state representations, the goal is to acquire stable state representations under policy \(\) via the measurement \(G_{}^{}\). The primary focus is usually to minimize a loss over the _bisimulation error_, denoted by \(_{}^{}\), which measures the distance between the approximation \(G_{}^{}\) and the fixed point \(G_{}^{}\):

\[_{}^{}(s_{i},s_{j}):=|G_{}^{}(s_{i},s_{j})-G_{}^{} (s_{i},s_{j})|.\] (3)

However, since the fixed point \(G_{}^{}\) is unobtainable without full knowledge of the underlying MDP, this approximation error is often unknown. Recall that in Lemma 2, we have shown that we can connect a bisimulation-based update operator to a lifted MDP. Taking inspiration from Bellman evaluation for the value function, we define the _bisimulation Bellman residual_\(_{}^{}\) as:

\[_{}^{}(s_{i},s_{j}):=|G_{}^{}(s_{i},s_{j})-^{ }G_{}^{}(s_{i},s_{j})|.\] (4)

Then, we can connect the bisimulation Bellman residual with the bisimulation error by the following:

**Theorem 3**.: _(**Bisimulation error upper-bound**). Let \(_{}(s)\) denote the stationary distribution over states, let \(_{}(,)\) denote the joint distribution over synchronized pairs of states \((s_{i},s_{j})\) sampled independently from \(_{}()\). For any state pair \((s_{i},s_{j})\), the bisimulation error \(_{}^{}(s_{i},s_{j})\) can be upper-bounded by a sum of expected bisimulation Bellman residuals \(_{}^{}\):_

\[_{}^{}(s_{i},s_{j})_{(s_{i}^{ },s_{j}^{})_{}}[_{}^{}(s_{i}^{ },s_{j}^{})].\] (5)

Thereafter, the bisimulation Bellman residual is used as a surrogate objective to approximate the fixed point \(G_{}^{}\) when learning our state representation. Indeed, the minimization of the bisimulation Bellman residual objective over all pairs \((s_{i}^{},s_{j}^{})_{}\) leads to the minimization of the corresponding bisimulation error. This ensures that if the expected on-policy bisimulation Bellman residual (_i.e._,\(_{_{}}_{}^{}\), and we will use the term "expected bisimulation residual" in following) minimization objective is zero, then the bisimulation error must be zero for the state pairs under the same policy. However, when the dataset is limited, rather than an infinite transition set covering the whole MDP, minimizing the expected bisimulation residual will no longer be sufficient to guarantee a zero bisimulation error.

**Proposition 4**.: _(**The expected bisimulation residual is not sufficient over incomplete datasets). If there exists states \(s_{i}^{}\) and \(s_{j}^{}\) not contained in dataset \(\), where the occupancy \(_{}(s_{i}^{}|s_{i},a_{i})>0\) and \(_{}(s_{j}^{}|s_{j},a_{j})>0\) for some \((s_{i},s_{j})_{}\), then there exists a bisimulation measurement \(G_{}^{}\) and a constant \(C>0\) such that_

* _For all_ \((},})\)_, the bisimulation Bellman residual_ \(_{}^{}(},})=0\)_._
* _There exists_ \((s_{i},s_{j})\)_, such that the bisimulation error_ \(_{}^{}(s_{i},s_{j})=C\)_._

As an example, if we only have \((s_{i},a_{i},r,s_{i}^{})\) and \((s_{j},a_{j},r,s_{j}^{})\) in a dataset, where both rewards equal to zero for state \(s_{i}\) and \(s_{j}\), and if we choose \(G_{}^{}(s_{i},s_{j})=C\), and \(G_{}^{}(s_{i}^{},s_{j}^{})=C\), then the bisimulation Bellman residual is \(_{}^{}(s_{i},s_{j})=0\), while the bisimulation error \(_{}^{}=G_{}^{}(s_{i},s_{j})-0=C\) is strictly positive. Note that this failure case does not involve modifying the environment in an extremely adversarial manner, it simply occurs when we are required to estimate the representation of states with subsequent states that are missing from the dataset. Since the distance between the missing states can be arbitrarily large as they are out-of-distribution, directly minimizing the Bellman bisimulation error could achieve the minimal Bellman bisimulation error over the dataset, while not necessarily improving the state representation.

In the context of Offline RL, since the dataset is finite, bisimulation-based representation learning ought to be conceptualized as a pretraining process over the behavior policy \(_{}\) of the dataset \(\). However, the failure case above indicates that applying the bisimulation operator \(^{_{}}\) and minimizing the associated Bellman bisimulation error does not necessarily ensure the sufficiency of the learned representation for downstream tasks. Ideally, if we had access to the fixed-point measurement \(G_{}^{_{}}\), then we could directly minimize the error between the approximation \(G\) and the fixed-point \(G_{}^{_{}}\). However, given the static and incomplete nature of the dataset, acquiring the fixed-point \(G_{}^{_{}}\) explicitly is not feasible. From another perspective, the failure stems from out-of-distribution estimation errors. Assuming we could estimate the bisimulation exclusively with _in-sample learning_, this issue could be intuitively mitigated. As such, we resort to expectile regression as a regularizer, allowing us to circumvent the need for out-of-sample / unseen state pairs.

## 5 Method

In this section, we describe how we adapt existing bisimulation-based representation approaches to offline RL. We use the expectile-based operator to learn state representations that optimize the behavior measurement over the dataset, while avoiding overfitting to the incomplete data. In addition, we analyze the impact of reward scaling and propose as a consequence to normalize the reward difference in the bisimulation Bellman residual in order to satisfy the specific nature of different instantiations of the bisimulation measurement while keeping a lower value error. The pseudo-code of our method is shown in Algorithms in Appendix B.

### Expectile-based Bisimulation Operator

The efficacy of expectile regression in achieving _in-sample learning_ has already been demonstrated in previous research [28; 36]. Consequently, we will first describe our proposed _expectile_-based operator, and subsequently show how expectile regression can effectively address the aforementioned challenge. Specifically, we consider the update operator as follows:

\[&_{}^{_{}}G_{}^{ _{}}(s_{i},s_{j}):=*{arg\,min}_{G_{ }^{_{}}}_{a_{i}_{}(|s_{i}),a_{j} _{}(|s_{j})}[[]_{+}^{2}+(1-)[-]_{+}^{2}],\\ &=_{s_{i}^{} T_{s_{i}^{} }^{_{}}},a_{i})-r(s_{j},a_{j})] + G_{}^{_{}}(s_{i}^{},s_{j}^{})}_{G}-G_{}^{_{}}(s_{i},s_{j}),\] (6)where \(\) is the estimated one-step bisimulation Bellman residual, \(_{}\) is the behavior policy, \(G_{}\) is the target encoder, updated using an exponential moving average, and \([]_{+}=(,0)\). Since the expectile operator in Equation 6 does not have a closed-form solution, in practice, we minimize it through gradient descent steps:

\[G_{}^{_{}}(s_{i},s_{j}) G_{}^{_{}}(s_{i},s_{j })-2_{a_{i}_{}(|s_{i}),a_{j}_{}( |s_{j})}[[]_{+}+(1-)[]_{-}]\] (7)

where \(\) is the step size. The fixed-point of the measurement obtained using this expectile-based operator is denoted as \(G_{}\). Although the utilization of the _expectile_ statistics is well established, its application for estimating bisimulation measurement is not particularly intuitive. In the following, we will show how expectile-based operator can be helpful in addressing the aforementioned issue. First, it is worth noting that when \(=1/2\), this operator becomes the bisimulation expectation of the behavior policy, _i.e._, \(_{_{_{}}}[]\). Next, we shall consider how this operator performs when \( 1\). We show that under certain assumptions, our method indeed approximates an "optimal" measurement in terms of the given dataset. We first prove a technical lemma stating that the update operator is still a contraction, and then prove a lemma relating different expectiles, finally we derive our main result regarding the "optimality" of our method.

**Lemma 5**.: _For any \(\) [0, 1), \(_{}^{}\) is a \(_{}\)-contraction, where \(_{}=1-2(1-)\{,1-\}<1\)._

**Lemma 6**.: _For any \(,^{}[0,1)\) with \(^{}\), and for all \(s_{i},s_{j}\) and any \(\), we have \(G_{^{}} G_{}\)._

**Theorem 7**.: _In deterministic MDP and fixed finite dataset, we have:_

\[_{ 1}G_{}(s_{i},s_{j})=_{a_{i} ,a_{j}\\ s.t._{}(a_{i}|s_{i})>0,_{}(a_{j}|s_{j})>0}G_{ }^{*}((s_{i},a_{i}),(s_{j},a_{j})).\] (8)

_where \(G_{}^{*}((s_{i},a_{i}),(s_{j},a_{j}))\) is a fixed-point measurement constrained to the dataset and defined on the state-action space \(\) as_

\[G_{}^{*}((s_{i},a_{i}),(s_{j},a_{j}))=|r(s_{i},a_{i})-r(s_{j},a_{j})|+ _{s_{i}^{}_{s_{j}}^{_{}}}[ _{a_{i}^{},a_{j}\\ s_{i}^{}_{s_{j}}^{_{}}}G_{}^{* }((s_{i}^{},a_{i}^{}),(s_{j}^{},a_{j}^{}))].\]

Intuitively, \(G_{}((s_{i},a_{i}),(s_{j},a_{j}))\) can be interpreted as a state-action value function \(Q(,)\) in a lifted MDP \(\), and \(G_{}(s_{i},s_{j})\) as a state value function \(V()\). We defer the detailed explanation to Appendix E.

Theorem 7 illustrates that, as \( 1\), we are effectively approximating the maximum \(G_{}((s_{i},a_{i}),(s_{j},a_{j}))\) over actions \(a_{i}^{},a_{j}^{}\) from the dataset. When we set \(=1\), the expectile-based bisimulation operator achieves fully in-sample learning: we only consider state pairs that have corresponding actions in the dataset. For instance, only when we have \((s_{i}^{},a_{i}^{})\) and \((s_{j}^{},a_{j}^{})\), can we apply the measurement of \(G_{}^{*}\). As such, by manipulating \(\), we balance a trade-off between minimizing the expected bisimulation residual (for \(=0.5\)) and evaluating \(G_{}^{*}((s_{i},a_{i}),(s_{j},a_{j}))\) solely on the dataset (for \(=1\)), thereby sidestepping the failure case outlined in Proposition 4 in an implicit manner.

### Reward Scaling

Most previous works [5; 54; 6; 52] have overlooked the impact of reward scaling in the bisimulation operator. To demonstrate its importance, we investigate a more general form of the bisimulation operator in Equation 2, given as:

\[^{}G(s_{i},s_{j})=c_{r}|r_{s_{i}}^{}-r_{s_{j}}^{}|+c_{ k}_{s_{i}^{},s_{j}^{}}^{}[G(s_{i}^{},s_{j}^{ })].\] (9)

We then can derive the following:

\[ G_{}^{}(s_{i},s_{j})&=^{}G_{}^{}(s_{i},s_{j})=c_{r}|r_{s_{i}}^{}-r_{s_{j}}^{}|+c_{k} _{s_{i}^{},s_{j}^{}}^{}[G_{}^{}(s_{i}^{ },s_{j}^{})]\\ & c_{r}(R_{}-R_{})+c_{k}_{s_{i}^{},s_{j}^{}}^{}[G_{}^{}(s_{i}^{},s_{j}^{ })]\\ & c_{r}(R_{}-R_{})+c_{k}_{ s_{i}^{},s_{j}^{}}G_{}^{}(s_{i}^{},s_{j}^{}).\] (10)Accordingly, we have \(G_{}^{}(s_{i},s_{j})(R_{}-R_{})}{1- c_{k}}\). Adopting the conventional settings of \(c_{r}=1\) and \(c_{k}=\) as suggested in [6; 52], could possibly result in a relatively large upper bound of \(G_{}^{}\) between states. This is due to the common practice of setting \(\) at \(0.99\). However, when bisimulation operators are instantiated with bounded distances, e.g., cosine distance, such a setting may be unsuitable. Therefore, it becomes important to tighten the upper bound.

Besides, we can also derive the value bound between the ground truth value function and the approximated value function:

**Theorem 8**.: _(Value bound based on on-policy bisimulation measurements in terms of approximation error). Given an MDP \(}\) constructed by aggregating states in an \(\)-neighborhood, and an encoder \(\) that maps from states in the original MDP \(\) to these clusters, the value functions for the two MDPs are bounded as_

\[|V^{}(s)-^{}((s) )|}{c_{r}(1-)}.\] (11)

_where \(:=\|_{}^{}-_{}^{}\|_{}\) is the approximation error._

In essence, Equation 10 and Theorem 8 reveal that: (i) there is a positive correlation between the reward scale \(c_{r}\) and the upper bound of the fixed-point \(G_{}^{}\), and (ii) a larger reward scale \(c_{r}\) facilitates a more accurate approximation of the value function \(^{}((s))\) to its ground-truth value \(V^{}(s)\). It is important to note that \(c_{r}\) also impacts the value of \(\), as depicted in Figure 7(Right)4. Therefore, it is crucial to first ensure the alignment with the instantiation of the bisimulation measurement, and then choose the largest possible \(c_{r}\) to minimize the value error. For instance, as the SimSR operator  uses the cosine distance, \(c_{k}=\) is predetermined. We should thus set \(c_{r}[0,1-]\), and apply min-max normalization to the reward function. This can make \(G_{}^{} 1\) and therefore be consistent with the maximum value of 1 of the cosine distance. To achieve a tighter bound in Equation11, we should then maximize the reward scale, setting \(c_{r}\) to \(1-\). Figure 7 illustrates the effectiveness of this reward scaling.

## 6 Experiments

### Performance Comparison in D4RL Benchmark

Implementation DetailsWe analyze our proposed method on the D4RL benchmark  of OpenAI gym MuJoCo tasks  which includes a variety of datasets that have been commonly used in the

Figure 1: The effectiveness of Reward Scaling (_RS_) in SimSR on halfcheetah-medium-expert-v2, with results averaged on 3 random seeds. (**Left**) Effective Dimension  comparison: without _RS_, there is a significant reduction in the effective dimension, accompanied by a marked increase in instability as training progresses. (**Right**) Numerical value comparison of estimated bisimulation Bellman residual: \(\) is persistently greater than 0 in the absence of _RS_, which indicates that target \(G\) is invariably larger than \(G_{}\), suggesting that \(G_{}\) does not achieve steady convergence.

Offline RL community. To illustrate the effectiveness of our method, we implement it on top of two bisimulation-based approaches, **MICo** and **SimSR**. It is worth noting that there are two versions of SimSR depending on its use of a latent dynamics model: SimSR_basic follows the dynamics that the environment provides, and SimSR_full constructs latent dynamics for sampling successive latent states. We opt for SimSR_basic as our backbone, as it exhibits superior and more stable performance in the D4RL benchmark tasks compared to SimSR_full. Additionally, to explore the impact of bisimulation-based representation learning on the downstream performance of policy learning, we build these approaches on top of the Offline RL method **TD3BC**. We examine three environments: halfcheetah, hopper, and walker2d, with four datasets per task: expert, medium-expert, medium-replay, and medium. We first pretrain the encoder during \(100k\) timesteps, then freeze it, pass the raw state through the frozen encoder to obtain the representations that serve as input for the Offline RL algorithm. Further details on the experiment setup are included in Appendix F.

AnalysisFigure 2 illustrates the performance of two approaches and their variants in the D4RL tasks. We use _EBS_ to represent the scheme of employing the expectile-based operator, while _RS_

Figure 3: Bootstrapping distributions for uncertainty in IQM (_i.e._ inter-quartile mean) measurement on D4RL tasks (left) and visual D4RL tasks (right), following from the performance criterion in .

Figure 2: Performance comparison on 12 D4RL tasks over 10 seeds with one standard error shaded in the default setting. For every seed, the average return is computed every 10,000 training steps, averaging over 10 episodes. The horizontal axis indicates the number of transitions trained on. The vertical axis indicates the normalized average return.

denotes the reward scaling scheme. The latter includes both min-max reward normalization and penalization coefficient with \((1-)\) in the bisimulation operator. As discussed in Section 5.2, the role of reward scaling varies depending on the specific instantiation of \(G\)5. We observe that without _RS_, SimSR almost fails in every dataset, which aligns with our understanding of the critical role reward scaling plays. The results also illustrate that _EBS_ effectively enhances the downstream performance of the policy for both SimSR and MICo. It is noteworthy that in this experiment, we set \(=0.6\) for the expectile in SimSR and \(=0.7\) in MICo across all datasets, demonstrating the robustness of this hyperparameter. Regarding SimSR, when _RS_ is applied (_SimSR+RS_), the performance is comparable to the TD3BC baseline, while the incorporation of the expectile-based operator (_SimSR+RS+EBS_) further enhances final performance and sample efficiency. Besides, we additionally present the IQM normalized return of all variants in Figure 3, illustrating our performance gains over the backbones. Further, we have also constructed an ablation study to investigate the impact of different settings of \(\), the results show that a suitable expectile \(\) is crucial for control tasks. We present the corresponding results in Appendix E.

### Performance Comparison in V-D4RL Benchmark

Implementation detailsWe also evaluate our method on a visual observation setting of DM-Control suite (DMC) tasks, V-D4RL benchmark . Similar to the previous experiment, we add the proposed schemes on top of MICo and SimSR. In the experiments, we notice that the latent dynamics modeling can help to boost performance for the visual setting, hence we use SimSR_full as the backbone. Additionally, we also notice that MICo often gives really poor performance in the V-D4RL benchmark, while adding latent dynamics alleviates the issue. Therefore, we boost MICo with explicit dynamics modeling for a fair comparison. To compare the performance with the other representation approaches, we include 4 competitive representation learning approaches for Offline RL, including DRIML , HOMER , CURL , and Inverse model . Detailed descriptions of these approaches can be found in Appendix G.

AnalysisWe evaluate all aforementioned approaches by integrating the pre-trained encoder from each into an Offline RL method DrQ+BC , which combines data augmentation techniques with TD3BC. The results in Table 1 and Figure 4 illustrate the effectiveness of our proposed method, the numerical improvements are underlined with red upward arrows. Compared to the other baselines, while _SimSR+RS+EBS_ does not achieve the highest score in all datasets, it achieves the best overall performance. Besides, our modifications on MICo and SimSR consistently show significant improvements. This indicates that our proposed method is not only applicable to raw-state inputs but also compatible with pixel-based observations.

## 7 Discussion

Limitations and Future WorkWhile \(\) remains constant in our D4RL experiments, optimal performance may arise under different \(\) settings, contingent on the specific attributes of the dataset. Therefore, to yield the best outcomes, one might need to set various \(\) to identify the most suitable value. However, this process could consume substantial computational resources. Another area of potential study involves evaluating the effectiveness of our approach in off-policy settings, given that off-policy settings may also lead to similar failure cases.

Figure 4: Performance comparison on V-D4RL benchmark.

ConclusionIn this work, we highlight the effectiveness of the bisimulation operator over incomplete datasets and emphasize the crucial role of reward scaling in Offline settings. By employing the expectile operator in bisimulation, we manage to strike a balance between behavior measurement and greedy assignment of the measurement over datasets. We also propose a reward scaling strategy to reduce the risk of representation collapse in specific bisimulation-based measurements. Empirical studies show the effectiveness of our proposed modifications.