# Expressivity-Preserving GNN Simulation

Fabian Jogl

Machine Learning Research Unit

Center for Artificial Intelligence and Machine Learning

TU Wien, Vienna, Austria

fabian.jogl@tuwien.ac.at

 Maximilian Thiessen

Machine Learning Research Unit

TU Wien, Vienna, Austria

maximilian.thiessen@tuwien.ac.at

&Thomas Gartner

Machine Learning Research Unit

TU Wien, Vienna, Austria

thomas.gaertner@tuwien.ac.at

###### Abstract

We systematically investigate _graph transformations_ that enable standard message passing to _simulate_ state-of-the-art graph neural networks (GNNs) without loss of expressivity. Using these, many state-of-the-art GNNs can be implemented with message passing operations from standard libraries, eliminating many sources of implementation issues and allowing for better code optimization. We distinguish between weak and strong simulation: weak simulation achieves the same expressivity only after several message passing steps while strong simulation achieves this after every message passing step. Our contribution leads to a direct way to translate common operations of non-standard GNNs to graph transformations that allow for strong or weak simulation. Our empirical evaluation shows competitive predictive performance of message passing on transformed graphs for various molecular benchmark datasets, in several cases surpassing the original GNNs.

## 1 Introduction

We systematically investigate which variants of non-standard message passing graph neural networks can be implemented with standard message passing graph neural networks (MPNNs) on a transformed graph without losing expressivity. While MPNNs are a very popular type of graph neural networks (GNNs), they are not able to represent every function on the space of graphs (Xu et al., 2019; Morris et al., 2019) due to their limited _expressivity_: for all MPNNs, there are pairs of non-isomorphic graphs which always have the same embedding. A common approach to create more expressive GNNs is to change the message passing function of MPNNs. If a GNN is more expressive than MPNNs by adapting the message passing function, we call this _non-standard message passing_. Examples of this are message passing variants that operate on subgraphs (Frasca et al., 2022; Bevilacqua et al., 2021) or tuples of nodes (Morris et al., 2019, 2020, 2022). It is known that some variants of non-standard message passing can be implemented with MPNNs on a transformed graph without losing expressivity as demonstrated by Morris et al. (2019, 2020, 2022) and Qian et al. (2022). We formalize this as an MPNN _simulating_ a GNN: a GNN can be simulated if there exists a graph transformation such that combining an MPNN with that graph transformation is at least as expressive as the GNN. As simulation only requires graph transformations as pre-processing, it allows the use of off-the-shelf MPNN implementations which simplifies and speeds up the implementation of GNNs. Another advantage is that simulation is programming framework agnostic: only the graphs needs to be transformed--independently of how the MPNN is implemented. Finally, simulation allows to easily exchange GNNs in existing workflows without requiring changes to the model. Despitethe benefits of simulation, this approach has not been thoroughly investigated: there is currently no formal definition of simulation and it is not known which GNNs can be simulated. In this paper, we define simulation and provide sufficient criteria for non-standard message passing to be simulated.

**Related Work.**Xu et al. (2019) and Morris et al. (2019) proved that MPNNs have limited expressivity. This lead to the development of new GNNs that have higher expressivity than MPNNs. Such GNNs often operate on structures that differ from graphs, for example (1) Morris et al. (2019, 2020, 2022) proposed GNNs operating on \(k\)-tuples of nodes, (2) Bodnar et al. (2021, 2021) proposed GNNs on topological structures, and (3) Bevilacqua et al. (2021) proposed GNNs operating on subgraphs. The idea of implementing non-standard message passing by standard message passing dates back to at least Otto (1997) who showed that instead of performing the higher-order message passing of \(k\)-WL it is possible to use 1-WL (classical message passing) on a transformed structure (Grohe et al., 2021). To the best of our knowledge, the first GNN that has been implemented through a graph transformation is \(k\)-GNN (Morris et al., 2019), which is a generalization from \(k\)-WL to GNNs. Already Morris et al. (2019) refer to this concept as _simulation_. In follow up work, Morris et al. (2020) and Morris et al. (2022) implemented similar GNNs with non-standard message passing as MPNNs together with graph transformations. Similarly, \(k\)-OSWL (Qian et al., 2022) was designed to utilize WL. However, none of these papers have formalized the idea of simulation for GNNs (or WL) explicitly. To this end, we (Jogl et al., 2022, 2021) showed that CW Networks (Bodnar et al., 2021), DS (Bevilacqua et al., 2021), DSS (Bevilacqua et al., 2021), and \(\)-\(k\)-GNNs/WL (Morris et al., 2020) can be implemented as an MPNN on a transformed graph. A similar high-level idea was proposed in a positional paper by Velickovic (2022). In parallel, Hajij et al. (2023) introduced combinatorial complexes that generalize structures such as graphs or cell complexes and showed that computations on these complexes can be realized as message passing on graphs. While simulation has been used and advocated in the past there is no unified solution to obtaining graph transformations, theoretical justification, or thorough empirical evaluation. We fill this gap and find many GNNs can be simulated by MPNNs on transformed graphs. More details can be found in Appendix A and Appendix B.

**Our Approach.** We introduce _simulation_ of non-standard message passing, which formalizes the idea of implementing a GNN, which uses non-standard message passing, through an MPNN on a transformed graph. A message passing algorithm can be _strongly simulated_ if an MPNN together with a graph transformation can achieve the same expressivity in _every_ iteration of message passing. To prove that many GNNs can be strongly simulated, we define the class of _augmented message passing_ (AMP) algorithms which contains many common GNNs. We prove that all AMP algorithms can be strongly simulated and present a meta algorithm to generate the necessary graph transformations. This allows us to show that eleven recently proposed GNNs can be strongly simulated. Specifically, the GNNs by Morris et al. (2019, 2020, 2022) and Qian et al. (2022) perform AMP. This gives an additional mathematical justification to their approach by proving that their GNNs can be implemented with MPNNs without losing expressivity over an implementation with non-standard message passing.

Furthermore, we investigate three constructions that demonstrate the limits of strong simulation: time dependent neighborhoods, nested aggregations, and non-pairwise message passing. We prove that these constructions either cannot be strongly simulated efficiently or cannot be strongly simulated at all. However, if we are only interested in the MPNN achieving the same expressivity as the GNN with non-standard message passing after a sufficient number of layers, it is possible to implement all three constructions with an MPNN. We call this _weak simulation_: a message passing algorithm can be weakly simulated if there exists a non-negative integer \(\) such that an MPNN together with a graph transformation can achieve the same expressivity as the message passing algorithm in one iteration after every \(\) iterations of message passing. We show that Message Passing Simplicial Networks (Bodnar et al., 2021), CW Networks (Bodnar et al., 2021), DSS (Bevilacqua et al., 2021), and \(K\)-hop message passing (Feng et al., 2022) can be weakly simulated. Finally, we evaluate a representative set of graph transformation empirically. Our graph transformations lead to competitive performance compared to the simulated algorithms and often lead to more accurate predictions.

**Main Contributions.** We introduce the concept of strong and weak simulation (Section 3) of message passing. This generalizes existing ideas behind the implementation of several GNNs (Morris et al., 2019, 2020, 2022, Qian et al., 2022). We provide an automated way of proving that a GNN can be simulated and deriving the necessary graph transformations. We prove that there exist architectures that cannot be strongly simulated (Section 4) but only weakly simulated (Section 5). Our empirical evaluation (Section 6) demonstrates that simulation achieves competitive performance.

## 2 Background

A _graph_\(G\) is a triple \(G=(V,E,F)\) consisting of a set of vertices \(V\), a set of directed edges \(E\{(x,y) x,y V,x y\}\), and a function \(F\) that assigns a feature to every vertex and edge. For a set of objects \(U\) and integer \(l>0\), we denote by \(2^{U}=\{X X U\}\) the powerset of \(U\), i.e., the set of all subsets of \(U\), and by \(U^{l}=\{(u_{1},,u_{l}) u_{1},,u_{l} U\}\) the set of all tuples of length \(l\) built from \(U\). In this paper we work on a generalization of graphs we call _relational structures_ that use a generalized notion of _neighborhood_. For a set of objects \(U\), we call any function \(:U 2^{(U^{})}\) a _neighborhood function_. A neighborhood function assigns to every object a set of tuples of length \(\) which we call its neighbors. We use \(()\) to denote the length of these tuples. We call any tuple \((w,u)\) a _directed edge_ where \(u U\) and \(w(u)\) (the _in-neighbours_) with \(()=1\). For an integer \(k>0\), we use \( k\) to denote the set \(\{1,,k\}\) and \(\) to denote a multiset.

**Definition 2.1**.: (Relational Structure) Let \(k 0\) be an integer, \(U\) be a set of objects, and let \(_{1},,_{k}\) be neighborhood functions over \(U\). Furthermore, let \(F\) be a function such that \(F(u)\) assigns a feature to every directed edge with \(i k\) and \(x_{i}(u)\) with \((_{i})=1\). Then, the tuple \(X=(U,_{1},_{k},F)\) is a _relational structure_.

If it is clear from context from which neighborhood \(_{i}\) an edge \((x,u)\) is, we will simply write its features as \(F((x,u))\) instead of \(F^{i}((x,u))\). If the number of neighborhoods is important we call such a structure a \(k\)-relational structure. Note, that a graph \(G=(V,E,F)\) is a special case of a 1-relational structure with only one neighborhood \(_{G}(v)=\{w(w,v) E_{G}\}\), i.e., \(G=(V,_{G},F)\). For graphs \(G,H\) we refer to their vertices, edges, and features by adding the graph as a subscript. We say that \(G,H\) are _isomorphic_ if there exists an edge and feature preserving bijective function \(:V_{G} V_{H}\) between their vertices: for every vertex \(v V_{G}\) it holds that \(F_{G}(v)=F_{H}((v))\) and for every \(x,y V_{G}\), it holds that \((x,y) E_{G}\) if and only if \(((x),(y)) E_{H}\) with \(F_{G}((x,y))=F_{H}(((x),(y)))\). No polynomial time algorithm is known which decides whether two graphs are isomorphic . However, heuristics such as those based on _colorings_ and _color update functions_ allow us to distinguish many pairs of non-isomorphic graphs. Furthermore, they allow us to model the ability of GNNs to distinguish graphs . For a relational structure over a set of objects \(U\), a _coloring_ is a function \(c:U\) where \(\) is a known set of colors. Suppose we have two disjoint sets \(U\) and \(U^{}\) with two colorings \(c\) over \(U\) and \(c^{}\) over \(U^{}\). Then, we define the joint coloring \(c c^{}\) for every \(x(U U^{})\) as \(c(x)\) if \(x U\) and \(c^{}(x)\) otherwise. For two colorings \(c\) and \(d\) defined over the same set of objects \(U\), we say that \(c\)_refines_\(d\) if for every pair of objects \(u,r U\) it holds that \(c_{u}=c_{r} d_{u}=d_{r}\). Let \(U,U^{}\) be sets of objects with \(U^{} U\). Let \(c\) be a coloring of \(U\) and \(d\) a coloring of \(U^{}\). Then, \(c\) refines \(d\) if for every pair of objects \(u,r U^{}\) it holds that \(c_{u}=c_{r} d_{u}=d_{r}\).

**Definition 2.2**.: (Color Update Function) Let \(k 1\) be an integer and \(\) a set of \(k\)-relational structures. A color update function takes any \(X=(U,_{1},,_{k},F)\), a coloring \(c\) of \(U\), and a \(u U\) and outputs a new color \((u,X,c)\) of \(u\). We denote the new coloring of the whole set \(U\) as \((X,c)\).

We assume color update functions \(\) to be computable and denote the time to compute \(\) with respect to an input \(U\) as \(_{}(|U|)\). Color update functions are used to iteratively update an initial coloring \(c^{0}\) such that \(c^{t+1}=(X,c^{t})\) for every \(t 0\). We denote \(t\) applications of \(\) to compute \(c^{t}\) as \(^{t}(X,c^{0})\). We also say that \(^{0}(X,c^{0})=c^{0}\). The initial coloring is often given by the features \(c^{0}=F|_{U}\), meaning the color \(c^{0}\) is given by the domain restriction of \(F\) to the set of objects \(U\). Then, the color of each object \(u U\) is \(c^{0}_{u}=F_{u}\). If we are not given features, then we use a constant coloring, i.e., a coloring that assigns the same (arbitrary) color to each object. Color update functions can sometimes

Figure 1: Left: graph. Center: regular cell complex built from the graph through a graph-to-structure encoding . Vertices correspond to 0-dimensional cells, edges to 1-dimensional cells (yellow) and induced cycles to 2-dimensional cells (blue). Right: a graph created by structure-to-graph encoding the regular cell complex to a graph. Vertices corresponds to cells as indicated by color.

be used to determine whether two graphs are isomorphic. For this, a coloring is computed for both graphs. If at some iteration the colorings of the two graphs are different this implies that the graphs are not isomorphic. If the colorings are never different, then the result of this method is inconclusive. A common color update function is the Weisfeiler-Leman algorithm (WL). In this work we define WL to use edge features.

**Definition 2.3**.: (WL) The Weisfeiler-Leman algorithm (WL) is a color update function operating on a graph \(G=(V,E,F)\) defined as \((v,G,c)=(c_{v},\{\!\!\{(c_{u},F((u,v)) u _{G}(v)\}\!)\})\) for \(v V\) where HASH is an injective mapping to colors.

Message passing graph neural networks (MPNNs) can be seen as a generalization of WL where the colors correspond to learned node embeddings and each layer of the MPNN corresponds to a color update function. The color update function in layer \(t 1\) of an MPNN is defined as \((v,G,c)=^{t}c_{v},^{t}(\{\! \{(c_{x},F((x,v)) x_{G}(v)\}\!)\!\})\) where \(^{t}\) and \(^{t}\) are learnable functions, e.g., multi-layer perceptrons (MLPs).1 We refer to WL and MPNNs as _standard_ message passing. It has been shown that MPNNs are at most as _expressive_ as WL, i.e., it can only distinguish non-isomorphic graphs that WL can distinguish. As there exist pairs of graphs that WL cannot distinguish this means that there exists graphs that MPNNs cannot distinguish either (Xu et al., 2019; Morris et al., 2019). A common approach to improve the expressivity of standard message passing is to apply a function \(T\) that transforms graphs to other relational structures. We call this mapping \(T\) from a graph to a relational structure a _graph-to-structure encoding_. An example of an graph-to-structure encoding can be seen in Figure 1. This mapping \(T\) is combined with a color update function \(\) tailored to this new structure. We use _non-standard message passing_ to refer to color update functions that operate on relational structures that are not graphs. Some examples of non-standard message passing are \(k\)-WL (Immerman and Lander, 1990) which operates on \(k\)-tuples of nodes and CW Networks (Bodnar et al., 2021) which operate on regular cell complexes.

## 3 Strong Simulation

We show that standard message passing together with graph transformations can achieve the same expressivity as many algorithms with non-standard message passing. As standard message passing operates on graphs we need to map relational structures to graphs. Note that merely guaranteeing at least the same expressivity as a color update function \(\) in each iteration can easily be achieved by, e.g., using the final coloring of \(\) as node features. To avoid such a trivial solution, we enforce straightforward restrictions on the generated graphs.

**Definition 3.1**.: (Structure-to-graph encoding) Let \(R\) be a mapping \(R:X G\) that maps relational structures \(X=(U,_{1},,_{k},F)\) to graphs \(G=(V,E,F^{})\). We call \(R\) a structure-to-graph encoding if it can be written as \(R(X)=R_{}(R_{}(U,_{1},,_{k }),F)\) such that

1. \(R_{}\) maps a relational structure _without_ features to a graph \(G=(V,E,F_{})\).
2. \(R_{}((V,E,F_{}),F)=(V,E,F^{})\) creates the features \(F^{}\) by concatenating each (node or edge) feature from \(F_{}\) with the corresponding feature from \(F\) if it exists.

As a graph-to-structure encoding \(T\) maps graphs to structures and a structure-to-graph encoding \(R\) maps structures to graphs, this means that \(R T\) maps graphs to graphs. We call such functions \(R T\)_graph transformations_, an example can be seen in Figure 1. As we define relational structures over _sets_ of objects, this implies that \(R\) is _permutation equivariant_ with respect to a permutation of the objects. Furthermore, since \(R\) is a function it is also _deterministic_. Next, we define strong simulation of color update functions.

**Definition 3.2**.: (Strong Simulation) Let \(\) be a color update function. Let \(R\) be a structure-to-graph encoding that runs in \((_{}(|U|))\) for every relational structure with object set \(U\) and creates a graph with vertex set \(V U\). We consider two arbitrary relational structures from the domain of \(\), say \(X=(U,_{1},,_{k},F)\) and \(X^{}=(U^{},^{}_{1},,^{}_{k },F^{})\). Let \((V_{1},E_{1},F_{1})=R(X)\) and \((V_{2},E_{2},F_{2})=R(X^{})\). We say \(\) can be _strongly simulated_ under \(R\) if for every \(t 0\) it holds that \(^{t}(R(X),F_{1}|_{V_{1}})^{t}(R(X^{ }),F_{2}|_{V_{2}})\) refines \(^{t}(X,F|_{U})^{t}(X^{},F^{}|_{U^{}})\).

[MISSING_PAGE_FAIL:5]

that for every relational structure \(X\) and every \(t 0\), WL achieves at least the same expressivity in iteration \(t\) on \(R(X)\) as layer \(t\) of the GNN on \(X\). We prove that this is always the case for color update functions that only differ in their use of function applications (S4) (see Appendix C.3). Different layers of a GNN commonly only differ by function applications (S4) as these correspond to learned functions. As many GNN layers use AMP, we can state one of the main results of this paper.

**Corollary 3.6**.: _The following GNNs and variants of WL can be strongly simulated:_

1. \(VV_{C}\)-GNN 
2. \(k\)-WL / GNN 
3. \(\)-\(k\)-(L)WL / (L)GNN 
4. GSN-e and GSN-v 
5. GSN-e and GSN-v 
6. \((k,s)\)-LWL / SpeqNet 
7. \(k\)-OSWL / OSAN 
8. \(M_{k}\) GNN 
9. GMP 
10. Shortest Path Networks 
11. Generalized Distance WL 
12. Generalized Distance WL 

Proof sketch.: The proof consists of showing that the color update function underlying the non-standard message passing is AMP. By Theorem 3.5 this implies that the color update function can be simulated. 

Runtime Complexity.AME creates a vertex for every object and edges for every pair of objects that exchange messages. This implies that, creating the encoding requires the same amount of time as one iteration of AMP. Furthermore, classical message passing on the transformed graphs passes the same number of messages as AMP in every iteration. Thus, the overall runtime complexity of strongly simulating AMP is equivalent to that of running AMP itself.

Example.We illustrate AMP on the example of WL with triangle counts and without edge features. For every \(v V_{G}\), we denote by \(_{v}\) the number of triangles in the graph containing \(v\). We define the color update function of WL with triangle counts: \((v,G,c)=(c_{v},\{\{(c_{u},_{u}) u_{G}(v)\} \})\). We show that WL with triangle counts is AMP. Consider the tuple \((c_{u},_{u})\), this consists of a color \(c_{u}\) atom (A3) with a constant \(_{u}\) atom (A1) combined into a tuple atom (A4). Thus, \((c_{u},_{u})\) is an atom which means that \(\{\!\{(c_{u},_{u}) u_{G}(v)\}\!\}\) is AMP that aggregates this atom over the neighborhood \(_{G}\) (S2). Finally, this AMP is combined with the color atom \(c_{v}\) (A1) and combined into a tuple AMP (S3). Thus, the color update function of WL with triangle counts performs AMP which implies that it can be strongly simulated. For this it suffices to attach \(_{v}\) to the features of every vertex \(v V\).

## 4 Limits of Strong Simulation

To show the limits of strong simulation we identify three constructions that either cannot be strongly simulated or cannot be strongly simulated efficiently: time dependent neighborhoods, nested ag

Figure 3: WL on the graph on the left (right) side weakly simulates the nested aggregations (non-pairwise message passing) from Figure 2 by adding additional vertices (yellow). This shows how weak simulation of both constructions is done in a similar way with the difference being that for non-pairwise aggregation we encode the order in the tuples on the edges.

Figure 2: Left: nested aggregations. Right: non-pairwise message passing of 2-tuples. In both examples the state of two vertices gets combined and sent to the blue vertex. Note that for non-pairwise aggregations we have an order on the nodes in a tuple whereas for nested aggregations the aggregated vertices are unordered.

gregations, and non-pairwise message passing. These constructions have previously been used by Deac et al. (2022); Bevilacqua et al. (2021); Feng et al. (2022); Bodnar et al. (2021); Bodnar et al. (2021); Bodnar et al. (2021). We begin with time dependent neighborhoods which are used in the alternating message passing of Deac et al. (2022). We illustrate them via the following example. Consider a color update function that updates the color of a vertex \(a\) based on the color of \(b\) in iteration \(t\) but not in iteration \(t^{}\). As it depends on the iteration whether \(a\) and \(b\) are neighbors, we call this _time dependent neighborhoods_. To model this in a graph, we require an edge between \(a\) and \(b\) in iteration \(t\) or we would lose expressivity whenever the color of \(b\) is important for the coloring of \(a\). However, this means that the color of \(a\) will also be updated with the color of \(b\) in iteration \(^{}\). Thus, we can strongly simulate time dependent neighborhoods but we cannot do so efficiently (i.e., with the same number of sent messages), as we might need to send many more messages.

_Nested aggregations_ are a form of color update functions used by Bevilacqua et al. (2021) and Feng et al. (2022). Consider the following color update function from DSS-WL (Bevilacqua et al., 2021)\((v,X,c)=(c_{v},\{\{\!\{\!\{c_{v} y(x)\}\!\} x ^{}(v)\}\!\})\) (see Figure 2 left).2 We call this nesting of two aggregations (\(\{\!\{\!\{c_{}()\}\!\}\!\}\)) a nested aggregation. To strongly simulate this we need an edge from every vertex of \(_{x^{}(v)}(x)\) to \(v\). Furthermore, we need a way to group vertices \((x)\) together for every \(x^{}(v)\). We prove that this is impossible with a structure-to-graph encoding and thus that nested-aggregations cannot be strongly simulated.

The third construction that we cannot strongly simulate is _non-pairwise message passing_ which has been used by Bodnar et al. (2021, 2021). Non-pairwise message passing refers to aggregating colors as ordered tuples over a neighborhood \(\) with \(()>1\). As an example, suppose we want to update the color of vertex \(v\) based on the _ordered tuple_ of colors \((c_{a},c_{b})\) of vertices \(a\) and \(b\) (see Figure 2 right). This is different from the aggregation by WL or AMP as those aggregations are unordered, i.e., \(\{\!\{c_{a},c_{b}\}\!\}\). An issue arises when multiple such ordered tuples have to be used to update a color, as there is no way this can be done within the concept of strong simulation. Similar to nested aggregations, non-pairwise message passing cannot be strongly simulated. Note that non-pairwise message passing is a special case of nested-aggregations where an additional order on the aggregated objects is given as is demonstrated in Figure 2.

**Theorem 4.1**.: _Nested aggregations and non-pairwise message passing cannot be strongly simulated._

Proof sketch.: To prove this (in Appendix D), we construct two relational structures that can be distinguished with nested aggregations in one iteration. These two structures have the same adjacencies but different features. We prove that no structure-to-graph encoding can create two graphs that one iteration of WL can distinguish. By the definition of graph-to-structure encoding, it is not possible for the edges of the resulting graph to depend on the original features. Thus, the two relational structure will lead to two graphs that have the same edges. We prove that WL cannot distinguish these two graphs in one iteration. The proof for non-pairwise message passing works similarly. 

## 5 Weak Simulation

We have shown in the previous section that not all color update functions can be strongly simulated. In this section, we introduce the more general concept of weak simulation and show that color update functions can be weakly simulated that are impossible to strongly simulate. Weak simulation differs from strong simulation by requiring only the same expressivity after certain number of iterations instead of every iteration.

**Definition 5.1**.: (Weak Simulation) Let \(\) be a color update function. Let \(R\) be a structure-to-graph encoding that runs in \((_{}(|U|))\) for every relational structure with object set \(U\) and creates a graph with vertex set \(V U\). We consider two arbitrary relational structures from the domain of \(\), say \(X=(U,_{1},,_{k},F)\) and \(X^{}=(U^{},_{1}^{},,_{k}^{ },F^{})\). Let \((V_{1},E_{1},F_{1})=R(X)\) and \((V_{2},E_{2},F_{2})=R(X^{})\). We say \(\) can be _weakly simulated_ under \(R\) with simulation factor \( 1\) if for every \(t 0\) it holds that \(^{ t}(R(X),F_{1}|_{V_{1}})^{ t}(R(X^{ }),F_{2}|_{V_{2}})\) refines \(^{t}(X,F|_{U})^{t}(X^{},F^{}|_{U^{}})\).

The simulation factor \(\) measures the relative slowdown of weak simulation with respect to the original GNN. It follows that strong simulation is a special case of weak simulation as every strongly simulatable algorithm is weakly simulatable with a simulation factor of 1. Next, we prove that it is possible to weakly simulate non-pairwise aggregations and nested aggregations. This shows that weak simulation contains a larger set of color update functions than strong simulation. The full proof can be found in Appendix E.

**Theorem 5.2**.: _Nested aggregations and non-pairwise message passing can be weakly simulated with simulation factor 2._

Proof sketch.: For weak simulation of nested aggregations consider the following construction. Suppose a nested aggregation \(\{\!\!\{\{l\!\{c_{y}^{t} y(x)\}\}\!\!x(v)\}\!\}\). For every \(x(v)\) we create incoming edges from \((x)\) and an outgoing edge \((x,v)\) (see Figure 3 left). This allows for weak simulation with a simulation factor of 2. Non-pairwise message passing can be weakly simulated in a similar way. Suppose we intend to update the color of vertex \(v\) based on the ordered tuple of colors \((c_{a},c_{b})\) of vertices \(a\) and \(b\). Then, we create a vertex \(x\) and an edge \((x,v)\) for this message. Furthermore, we create an edge \((a,x)\) labeled with 1 to indicate that it is the first element in the tuple and an edge \((b,x)\) labeled with 2 (see Figure 3 right). This construction weakly simulates the non-pairwise message passing with simulation factor 2. 

As a consequence, we can weakly simulate the following algorithms.

**Corollary 5.3**.: _The following GNNs and variants of WL can be weakly simulated:_

1. Message Passing Simplicial Networks (Bodnar et al., 2021)
2. CW Networks (Bodnar et al., 2021)
3. DSS (Bevilacqua et al., 2021)
4. K-hop message passing and KP-GNNs (Feng et al., 2022)

**Discussion.** It is possible to design graph transformations that result in smaller graphs, enabling more efficient weak simulation than the transformations automatically following from Theorem 5.2. We demonstrate that this is possible for all algorithms from Corollary 5.3. For Message Passing Simplicial Networks and CW Networks it is possible to construct graphs that require no additional vertices for weak simulation (see Appendix F.1). Similarly, it is possible to weakly simulate DSS without creating a large number of additional edges by adding a second copy of the original graph if we accept increasing the simulation factor to 3 as a trade-off (see Appendix F.2). Interestingly, \(K\)-hop message passing and KP-GNNs induce an additional order on the message passing which even allows for efficient _strong_ simulation (see Appendix F.3). We did not find any architectures where weak simulation led to large increases in time and space complexity. However, in general, we cannot rule out the possibility of architectures that exhibit such an increase. This indicates the possibility for more powerful non-standard message passing, opening a promising direction for future work.

## 6 Experiments

We have shown many cases in which graph transformation based methods achieve the same expressivity as non-standard message passing. In this section, we empirically investigate whether MPNNs together with graph transformations can also achieve similar predictive performance as the simulated GNNs with non-standard message passing. We strongly simulate DS (Bevilacqua et al., 2021), weakly simulate DSS (Bevilacqua et al., 2021) and weakly simulate CW Networks (CWN) (Bodnar et al., 2021), as representative approaches. For a non-standard message passing algorithm, we denote the corresponding graph transformation with a bar, e.g., \(}\). The code for our experiments can be found at https://github.com/ocatias/GNN-Simulation.

Models.With GIN (Xu et al., 2019) and GCN (Kipf and Welling, 2017) we use two of the most common MPNNs as baselines. We combine these MPNNs with the graph transformations \(}\), \(}\) and \(}\). DS, \(}\), DSS, and \(}\) require a policy that maps a graph to subgraphs, for this we chose the common 3-egonets policy that extracts the induced 3-hop neighborhood for each node (Bevilacqua et al., 2021). We chose this policy as it creates only small subgraphs and Bevilacquaet al. (2021) reported good results for it. More details on \(}\) can be found in Appendix F.2. For CWN and \(}\) we construct cells as described by Bodnar et al. (2021). More details on \(}\) can be found in Appendix G and F.1. We apply the same training and evaluation procedure to all GNNs.

Datasets.Due to the large number of combination of models and graph transformations, we focus on medium size datasets. To experimentally investigate how the graph transformations increase expressivity, we perform an initial investigation with GIN on the synthetic CSL dataset (Murphy et al., 2019; Dwivedi et al., 2023). For real world prediction tasks, we use all real world datasets with less than \(10^{5}\) graphs that provide a train, validation, and test split used in Bodnar et al. (2021); Bevilacqua et al. (2021): ZINC(Gomez-Bombarelli et al., 2018; Sterling and Irwin, 2015), ogbg-molhiv and ogbg-moltox21(Hu et al., 2020). Additionally, we add seven small molecule datasets from OGB (Hu et al., 2020). In total, we evaluate on 10 real-life datasets.3

Setup.For real-life datasets we combine all baseline models (GCN, GIN) with all graph transformations (\(}\), \(}\), \(}\)) and tune hyperparameters individually (including CWN, DS and DSS; see Appendix H). We also measure the preprocessing and training speeds for different models (details and speed results are in Appendix H.3).

Results.Table 2 shows the results on CSL. GIN achieves 10% accuracy whereas all other methods achieve a perfect accuracy. This confirms that just like non-standard message passing, MPNNs with graph transformations have superior expressivity compared to GIN. Table 1 summarizes the results on all real life datasets. Indeed, MPNNs with graph transformations achieve the best result on half of them. Just like non-standard message passing, MPNNs with graph transformations outperform MPNNs in the majority of experiments. Notably, graph transformations combined with GIN outperform non-standard message passing in more than half of the experiments. This shows that simulated networks not only have provably at least the same expressivity but also perform surprisingly well in practice.

    & Beats & Beats & Best \\  & MPNN & NSMP & Model \\  GIN & - & - & 0\% \\ GCN & - & - & 0\% \\  DS (GIN) & 60\% & - & 10\% \\ DS + GIN & 60\% & 60\% & 0\% \\  DS + GCN & 70\% & 30\% & 0\% \\  DSS (GIN) & 70\% & - & 20\% \\  DSS + GIN & 70\% & 60\% & 30\% \\  DSS + GCN & 70\% & 30\% & 10\% \\  CWN & 60\% & - & 20\% \\  CWN + GIN & 60\% & 50\% & 0\% \\  CWN + GCN & 50\% & 30\% & 10\% \\   

Table 1: Predictive Performance of different GNNs on 10 different datasets. Beats MPNN is the percentage of datasets on which the model beats the corresponding MPNN (either GIN or GCN, as noted in the model name). Beats NSMP is the percentage of datasets where the graph transformation based model beats the corresponding non-standard message passing model. Best Model is the percentage of datasets where this model achieves the best results among all models.

   model & 
 CSL \\ accuracy \\  \\  GIN & \(0.1 0.0\) \\  CWN & \(\) \\  CWN + GIN & \(\) \\  DSS & \(\) \\  DSS + GIN & \(\) \\  DS & \(\) \\  DS + GIN & \(\) \\   

Table 2: Accuracy on CSL. **Bold** results outperform GIN.

Discussion and Conclusion

Discussion.The purpose of simulating a GNN is to allow an MPNN to learn similar embeddings of graphs as the GNN. We expect such embeddings to model the similarity of graphs in a more fine-grained way than just distinguishing graphs. Thus, we have to avoid trivial solutions: any MPNN can be made as expressive as any other GNN when the output of the GNN is attached to the input of the MPNN. In fact, it is even possible to make an MPNN maximally expressive by precomputing the isomorphism class of the graph, e.g., a number uniquely encoding whether two graphs are isomorphic, and inputting it into the MPNN. However, such techniques defeat the purpose of learning a graph representation in the first place and potentially require a super-polynomial runtime (Babai, 2016). To avoid such scenarios, we introduce two requirements for weak and strong simulation: (1) the features of the resulting graph depend on the original features solely by concatenation and (2) the transformation must operate in linear time relative to the simulated color update function. These two constraints render it impossible to precompute the colors generated by the simulated method for a non-constant number of iterations and prohibit the computation of graph isomorphism classes.

Implications.Our work has implications on (1) the theory of GNNs, (2) the design of new GNNs, and (3) the implementation GNNs with graph transformations. For (1): simulation allows to investigate the expressivity of different GNNs through a unified lens by analyzing the corresponding graph transformation. It should be possible to obtain VC bounds for any weakly simulatable GNN by using the results from Morris et al. (2023). Similarly, we believe that it is possible to use Geerts and Reutter (2022) to get expressivity upper-bounds in term of the \(k\)-WL test for any weakly simulatable GNN. For (2): our theorems indicate that nested aggregations and non-pairwise message passing cannot be strongly simulated and are thus fundamentally different from the message passing paradigm. Thus, to build GNNs that go beyond MPNNs in expressivity it seems promising to investigate such constructions. For (3): instead of implementing a GNN with non-standard message passing, our theorems imply it can be implemented as a graph transformation together with an MPNN. This makes it easier to implement GNNs as of-the-shelf MPNNs can be used and makes the resulting method agnostic from the deep learning framework as the graph transformation does most of the heavy lifting.

Conclusion.We introduced weak and strong simulation of non-standard message passing. Simulation allows an MPNN together with a graph transformation to obtain the same expressivity every \( 1\) layers (weak simulation) or in every layer (strong simulation) as GNNs with non-standard message passing. We have proposed a straightforward way of proving that an algorithm can be simulated and to generate the corresponding graph transformations. This generalizes previous ideas (Otto, 1997; Morris et al., 2019, 2020, 2022; Qian et al., 2022) and makes simulating GNNs accessible to anyone who intends to build novel GNNs. In total, we have shown that 15 GNNs can be simulated. We chose these 15 models to showcase the variety of GNNs that fall under augmented message passing and expect many more GNNs to be simulatable as well. In future research, we will investigate the use of simulation to analyze aspects of GNNs such as generalization, oversmoothing, and oversquashing.

Acknowledgements.Part of this work has been supported by the Vienna Science and Technology Fund (WWTF) project ICT22-059. The authors would like to thank the reviewers for the feedback. We are especially thankful to our colleagues Pascal Welke and Tamara Drucks for giving extensive feedback on a draft of this paper.