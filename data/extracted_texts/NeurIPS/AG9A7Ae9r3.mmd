# DIFFER: Decomposing Individual Reward for Fair Experience Replay in Multi-Agent Reinforcement Learning

DIFFER: Decomposing Individual Reward for Fair Experience Replay in Multi-Agent Reinforcement Learning

Xunhan Hu\({}^{1}\), Jian Zhao\({}^{1}\), Wengang Zhou\({}^{1,2}\), Ruili Feng\({}^{3,1}\), Houqiang Li\({}^{1,2}\)

\({}^{1}\) University of Science and Technology of China, \({}^{2}\) Institute of Artificial Intelligence,

Hefei Comprehensive National Science Center, \({}^{3}\) Alibaba Group

{cathyhxh,zj140}@mail.ustc.edu.cn, zhwg@ustc.edu.cn,

ruilifengustc@gmail.com, lihq@ustc.edu.cn

Corresponding authors: Wengang Zhou and Houqiang Li

###### Abstract

Cooperative multi-agent reinforcement learning (MARL) is a challenging task, as agents must learn complex and diverse individual strategies from a shared team reward. However, existing methods struggle to distinguish and exploit important individual experiences, as they lack an effective way to decompose the team reward into individual rewards. To address this challenge, we propose DIFFER, a powerful theoretical framework for decomposing individual rewards to enable fair experience replay in MARL. By enforcing the invariance of network gradients, we establish a partial differential equation whose solution yields the underlying individual reward function. The individual TD-error can then be computed from the solved closed-form individual rewards, indicating the importance of each piece of experience in the learning task and guiding the training process. Our method elegantly achieves an equivalence to the original learning framework when individual experiences are homogeneous, while also adapting to achieve more muscular efficiency and fairness when diversity is observed. Our extensive experiments on popular benchmarks validate the effectiveness of our theory and method, demonstrating significant improvements in learning efficiency and fairness. The code is available in https://github.com/cathyhxh/DIFFER.

## 1 Introduction

In widely adopted cooperative multi-agent systems [1; 2; 3; 4], a team of agents typically operates under the constraint of individual local observations and limited communication capabilities. In each time step, all agents collectively interact with the environment by taking team actions, leading to a transition to the subsequent global state and the provision of a team reward. Consequently, the agents must acquire the ability to effectively coordinate their actions and maximize the cumulative team return. Over the past few years, cooperative multi-agent reinforcement learning (MARL) has demonstrated remarkable achievements in addressing such challenges and has exhibited promising potential across diverse domains, including multi-player games[5; 6; 7; 8; 9], autonomous cars [10; 11], traffic light control [12; 13], economy pricing  and robot networks [15; 16].

In recent years, value factorization methods [17; 18; 19; 20] have emerged as leading approaches for addressing cooperative multi-agent tasks [21; 22; 23]. These methods are rooted in the centralized training and decentralized execution (CTDE) paradigm [24; 25; 26]. Specifically, each agent's network models the individual action-value using the agent's local action-observation history  and the last action. These individual action-values are then integrated into a global state-action value through a mixing network. The parameters of both the agent networks and the mixing network are optimized by minimizing the team TD-error. To enhance data utilization, value factorization methods commonly employ experience replay. Here, **experience** represents the atomic unit of interaction in RL, \(i.e.\), a tuple of (observation, action, next observation, reward) . In traditional MARL experience replay algorithms, the replay buffer stores team experiences and allows algorithms to reuse them for updating the current network, which encompasses both the agent networks and the mixing network.

However, the utilization of team experience overlooks the inherent differences among individual experiences, thereby failing to leverage individual experiences worthy of learning. This issue becomes especially pronounced when the agents within the team exhibit heterogeneity in their roles and capabilities. To develop a clearer understanding, let us consider the scenario of MMM2 (as shown in Fig. 1) within The StarCraft Multi-Agent Challenge (SMAC) environment . In this setup, a team comprises 1 Medivac (responsible for healing teammates), 2 Marauders (focused on offensive actions), and 7 Marines (also dedicated to attacking). Each unit is controlled by a distinct agent, and the team's objective is to eliminate all enemy units. Learning an effective strategy for the Medivac unit proves particularly challenging due to its unique sub-task and limited number. Consequently, the individual experiences of the Medivac unit are deemed more likely to provide valuable insights for learning. Unfortunately, extracting these individual experiences from the team experiences proves difficult in the aforementioned MARL setting, as the individual rewards associated with each individual experience are unavailable.

In this paper, we introduce a novel method called **D**ecomposing **I**ndividual Reward **for **F**air **E**xperience **R**eplay (DIFFER), to address the above challenges associated with multi-agent reinforcement learning (MARL) experience replay. The fairness in MARL experience replay refers to the equitable sampling of individual experiences based on their importance. DIFER addresses this by decomposing team experiences into individual experiences, facilitating a fair experience replay mechanism. We begin by proposing a method to calculate the individual reward for each individual experience, ensuring that DIFER maintains equivalence with the original learning framework when the individual experiences are indistinguishable. To achieve this, we establish a partial differential equation based on the invariance of agent network gradients. By solving this differential equation, we obtain an approximation function for the individual rewards. Consequently, the team experience can be successfully decomposed into individual experiences. These individual experiences are then fed into a fair experience replay, where they are sampled for training the agent networks. The sampling probability is determined proportionally to the temporal difference (TD) error of each individual experience. Moreover, it adapts effectively to enhance both efficiency and fairness in scenarios where diversity is observed. Importantly, DIFER is a generic method that can be readily applied to various existing value factorization techniques, thereby extending its applicability in MARL research.

We evaluate DIFER on a diverse set of challenging multi-agent cooperative tasks, encompassing StarCraft II micromanagement tasks (SMAC) , Google Football Research (GRF) , and Multi-Agent Mujoco  tasks. Our experimental findings demonstrate that DIFER substantially enhances learning efficiency and improves performance.

## 2 Preliminaries

Problem Formulation.A fully cooperative multi-agent problem can be described as a decentralized partially observable Markov decision process (Dec-POMDP) , which consists of a tuple \(G=<I,S,A,P,r,,O,N,>\). \(I\) is the finite set of \(N\) agents. For each time step, each agent \(i I:=\{1,,N\}\) receives individual observation \(o_{i}\) according to the observation function \(O(s,i) S I\). Then, each agent determines its action with individual policy \(_{i}(a_{i}|o_{i}): A\). Given the team action \(=\{a_{1},,a_{N}\}\), the environment transits to a next state according to the

Figure 1: Screenshots of the MMM2 scenario in SMAC, highlighting the importance of individual experiences for the Medivac unit, given its unique sub-task and limited number.

transition function \(P(s^{}|s,):S A^{N} S\) and gives a team reward \(R(s,) S A^{N}\), which is shared by all agents. Each team policy \((|)=[_{1}(a_{1}|o_{1}),...,_{N}(a_{N}|o_{N})]\) has a team action-value function \(Q^{t}_{}(^{t},^{t},s^{t})=_{s_{t+1}, _{t+1}}[^{t}(s^{t},^{t})]\), where \(^{t}(s^{t},^{t})=_{k=0}^{}^{k}R^{t+k}\) is a discount return and \(\) is the discount factor. The target of a Dec-POMDP problem is to maximize the accumulated return of the team policy, _i.e._ optimal team policy \(^{*}=_{}()=_{s_{0}  d(s_{0}),}[Q^{t}_{}(^{t},^{t}, s^{t})]\), where \(d(s_{0})\) represents the distribution of initial state.

Multi-Agent Deep Q-learning.Deep Q-learning  uses a neural network parameterized by \(\) to represent the action-value function \(Q\). In the multi-agent Dec-POMDP problem, multi-agent deep Q-learning methods usually adopt the replay buffer  to store experience \((,,^{},R)\). Here \(R\) is the team reward received after taking team action \(\) with team observation \(\), and \(^{}\) represents team observation in the next step. During the training process, the parameter \(\) is updated by minimizing the Temporal Difference (TD) error with a mini-batch of data sampled from the replay buffer, which is shown as \(L_{}()=_{(,,r,^{}) D}[(r+ (^{};^{-})-Q(,;))^{2}]\), where \((^{};^{-})=_{^{}}Q(^{ },^{};^{-})\) is the one-step expected future return of the TD target and \(^{-}\) is the parameter of the target network . \(=|r+(^{};^{-})-Q(,; )|\) is known as TD-error.

Multi-Agent Value Factorization.The value factorization framework comprises \(N\) agent networks (one for each agent) and a mixing network. At time step \(t\), the agent network of agent \(i\) takes the individual observation \(o^{t}_{i}\), thereby computing the individual action-value, denoted as \(Q_{i}\). Subsequently, the individual action-values \(\{Q_{i}\}_{i I}\) are fed into the mixing network, which calculates the team action-value, denoted as \(Q_{}\). The mixing network is constrained by Individual-Global-Max (IGM) principal, ensuring the congruity between the optimal team action deduced from \(Q_{}\) and the optimal individual actions derived from \(\{Q_{i}\}_{i I}\). The \(N\) agent networks share parameters. We denote \(_{m}\) and \(_{p}\) as the parameters of mixing network and agent networks, respectively. During the training phase, the mixing network is updated based on the TD loss of \(Q_{}\), while the agent networks are updated through gradient backward propagation. During the execution phase, each agent takes action with individual policy derived from its agent network in a decentralized manner.

## 3 Method

In this section, we present DIFER, a novel experience replay method designed to foster fairness in multi-agent reinforcement learning (MARL) by decomposing individual rewards. DIFER offers a solution to enhance the adaptivity of MARL algorithms. Instead of relying solely on team experiences, DIFER trains the agent networks using individual experiences. This approach allows for a more granular and personalized learning process.

At a given time step, agents interact with the environment and acquire a **team experience**\(}}=(,,^{})=((o_{i})_{i I},(a _{i})_{i I},(o^{}_{i})_{i I},R)=((o_{i},a_{i},o^{}_{i})_{i I },R)\). Here, \(o_{i}\), \(a_{i}\), and \(o^{}_{i}\) represent the observation, action, and next observation for each agent \(i I\), respectively. These team experiences are stored in a replay buffer and subsequently sampled to train the mixing network and agent network parameters. We aim to decompose a multi-agent team experience into \(N\) single-agent **individual experiences**\(\{^{}_{i}\}_{i I}=\{(o_{i},a_{i},o^{}_{i},r_{i})\}_{i I}\), then utilize these individual experiences to update the agent networks.

The DIFER framework comprises two stages: (1) **Decomposer**: Calculating individual rewards to decompose team experiences; (2) **Fair Experience Replay**: Selecting significant individual experiences to train the agent networks.

### Exploring Individual Rewards through Invariance of Gradients

The DIFER framework necessitates individual rewards \(\{r_{i}\}_{i I}\) to decompose the team experience \(}}\) into individual experiences \(\{^{}_{i}\}_{i I}\). However, in the context of shared team reward POMDP problems, these individual rewards are not readily available. Therefore, the key challenge for DIFER is to devise a strategy for calculating individual rewards.

In the traditional experience replay methods, agent networks are updated by the team TD-loss, which is calculated based on team experience and denoted as \(L_{}=(R+_{}(^{},^ {};^{-})-Q_{}(,;))^{2}\).

However, in DIFFER, agent networks are updated using the individual TD-loss, which is calculated based on individual experience and denoted as \(L_{i}=(r_{i}+_{i}(o_{i}^{},a_{i}^{};_{p}^{- })-Q_{i}(o_{i},a_{i};_{p}))^{2}\). To preserve the overall optimization objective of value factorization methods, it is desirable for an ideal individual reward approximation strategy to maintain invariance between the gradients of \(L_{}\) (before decomposition) and the sum of \(L_{i}\) (after decomposition) with respect to the parameters of the agent networks. The optimization equivalence can be formulated as follows:

\[}}{ _{p}}=_{i I}}{_{p}},\] (1)

where \(_{p}\) represents the parameters of the agent networks (noting that the agent networks of each agent share parameters). It is important to emphasize that this discussion focuses solely on the gradient of a single team experience. By solving the above partial differential equation, we are able to obtain an individual reward function that satisfies the invariance of gradients.

**Proposition 1**.: _The invariance of gradient in Equ.(1) is satisfied when individual reward of agent \(i\) is given by:_

\[r_{i}=(R+_{}-Q_{})}}{ Q_{i}}-_{i}+Q_{i}.\] (2)

_for any agent \(i I\)._

A rigorous proof and analysis of above proposition can be found in Appendix. Therefore, we approximate the individual rewards \(r_{i}\) for individual experiences \(_{i}^{}\) while preserving the original optimization objective. In this way, a team experience \(}}\) is decomposed into \(N\) individual experiences \(\{_{i}^{}\}_{i=1}^{N}\) successfully.

### Fair Experience Replay Guided by Individual Experiences

After decomposition, we obtain a set of individual experience \(E=\{_{j}^{}\}_{j=0}^{N B}\) from a team experience mini-batch. Here, \(j\) denotes the index of the individual experience in the set, \(B\) represents the mini-batch size, and \(N\) indicates the number of agents involved. Our objective is to construct a fair experience replay, a method that selects significant individual experiences from \(E\) to train the agent network. Similar to Priority Experience Replay (PER) method , the TD-error of each individual experience is used as a metric to measure its significance. A larger TD-error for an individual

Figure 2: (a) In DIFFER, the mixing network is trained using the team TD-loss \(L_{}^{}\), while the parameters of the agent networks remain unchanged. (b) The decomposing individual experiences are inputted into Fair Experience Replay, from which they are sampled to calculate the individual TD-loss \(L_{}^{}\) for training the agent networks. (c) In traditional MARL experience replay methods, the mixing network and agent networks are updated using the team TD-loss \(L_{}^{}\) derived from team experiences.

experience indicates that it is more valuable for the current model, and thus more likely to be selected for training. The individual experience \(_{j}^{}\) is sampled with probability:

\[P(_{j}^{})=^{}}{_{k}p_{k}^{}},\] (3)

where \(p_{j}=|_{j}|+\). \(_{j}\) is the TD-error of \(_{j}^{}\) and \(\) is a small positive constant in case TD-error is zero. The hyper-parameter \(\) determines the degree of prioritization. It degenerates to uniform sample cases if \(=0\). To correct the bias, \(_{j}^{}\) is weighted by importance-sampling (IS) weights as follows:

\[_{j}=(^{})})^{}/ _{k}_{k}.\] (4)

The hyper-parameter \(\) anneals as introduced in PER.

In cases where the agents within a team are homogeneous, their individual experiences tend to exhibit similar TD-errors. As a result, these individual experiences are assigned similar probabilities during the fair experience replay process. In such circumstances, the fair experience replay approach employed by DIFFER effectively degenerates into a traditional method that utilizes team experiences.

Let \(E^{}\) denote the selected individual experience set, which is a subset of the original individual experience set \(E\). We define the sample ratio as \(:=\#E^{}/\#E\), where \(\#E^{}\) and \(\#E\) are the numbers of individual experiences in \(E^{}\) and \(E\), respectively. Since the model is initially at a low training level and Q-value estimation errors are large, only a small portion of the individual experiences is worth training. However, this portion increases as the training progresses. Thus, motivated by the _warm-up_ technique proposed in , we set \(<1.0\) at the beginning of training and gradually increase it as the number of training steps increases. The sample ratio \(_{t_{i}}\) for a given training step \(t_{i}\) can be expressed as follows:

\[_{t_{i}}=_{}+(_{}-_{ })}{pt_{}},&t_{i}<pt_{}\\ _{},&t_{i} pt_{}\] (5)

Here, the hyper-parameters \(_{}\) and \(_{}\) denote the initial and final values of the sample ratio, respectively. The hyper-parameter \(p\) is the proportion of the time steps at which the sample ratio increases, and \(t_{}\) is the overall number of training steps. The ablation studies regarding the _warm-up_ technique are provided in Appendix.

### Overall Training and Evaluation

During the training phase, we first sample a team experience mini-batch \(}}\) from the replay buffer. The parameters of mixing network \(_{m}\) are optimized with \(L_{}\) calculated on the mini-batch by

\[L_{}^{}(_{m})=_{^{ }=(,,^{},R)}}}(R+ }}(^{},_{m}^{-})-Q_{ }(,;_{m}))^{2}.\] (6)

Next, we approximate the optimization equivalence of individual rewards of each individual experience as Equ. (8) and decompose a team experience into multiple individual experiences. Among them, we select a significant individual experience set \(E^{}\) with probability in Equ. (3). The parameters of agent networks \(_{p}\) are optimized by

\[L_{}^{}(_{p})=_{_{j}^{ }=(o_{j},a_{j},o_{j}^{},r_{j}) E^{}}_{j}(r_{ j}+}(o_{j}^{};_{p}^{-})-Q_{j}(o_{j},a_{j}; _{p}))^{2},\] (7)

where \(_{j}\) is a weight assigned to individual experience \(_{j}^{}\) as calculated in Equ. (4).

During the inference phrase, agent \(i\) chooses a greedy action according to its individual action-value \(Q_{i}\) for decentralized execution. Therefore, the DIFFER framework meets centralized training and decentralized execution. The overall training and evaluation are presented in Algo. 1.

## 4 Experiments

In this section, we conduct several experiments to answer the following questions: (1) Are the decomposing individual rewards calculated using Equ. (8) optimization equivalent to the team reward experimentally? (Sec. 4.2) (2) Can DIFFER improve the performance of existing value factorization methods compared to the baselines across different environments? (Sec. 4.3) (3) Can DIFFER determine a more reasonable agent policy compared to the baselines? (Sec. 4.3) (4) Can DIFFER successfully distinguish and exploit the important individual experiences? (Sec. 4.4)

For every graph we plot, the solid line shows the mean value and the shaded areas correspond to the min-max of the result on 5 random seeds. All the experiments are conducted on a Ubuntu 20.04.5 server with Intel(R) Xeon(R) Gold 6248R CPU @ 3.00GHz and GeForce RTX 3090 GPU. The code is available in https://github.com/cathyhxn/DIFFER.

### Experiments Setting

Training Environments.We choose discrete action space environments _The StarCraft Multi-Agent Challenge_ (SMAC)  and _Google Football Research_ (GRF) , as well as a continuous action space environment _Multi-Agent Mujoco_ (MAMujoco) to conduct our experiments. SMAC is currently a mainstream cooperative multi-agent environment with partial observability. We use the default environment setting for SMAC with version SC 2.4.10. GFR is a platform that facilitates multi-agent reinforcement learning in the field of football. We conduct the experiments on two academy scenarios in GRF. MAMujoco is a continuous cooperative multi-agent robotic control environment. Each agent represents a part of a robot or a single robot. All of the tasks mentioned in this work are configured according to their default configuration.

Base Models.We select QMIX , COMIX , QPLEX  and MASER  as our base models for comparison. QMIX and QPLEX are renowned value factorization methods that have been widely used in discrete action space environments. COMIX is an extension of QMIX specifically designed for continuous action space environments. These three base models employ team experience to train their overall value factorization frameworks, as illustrated in Fig. 2(c). Furthermore, we include MASER, a MARL experience replay method, in our evaluation. Similar to our methods, MASER generates individual rewards and trains agent networks using individual experiences.

Implementation Hyper-parameters.All base models were implemented by faithfully adhering to their respective open-source codes based on PyMARL. Regarding the \(warm\_up\) trick for the sample ratio, we initially set the ratio to 0.8, which gradually increases linearly until reaching 1.0 at 60\(\%\) of the total training steps. From that point onwards, the sample ratio remains fixed at 1.0 for the remainder of the training process. For a comprehensive overview of the hyper-parameter settings employed in our experiments, we refer readers to the appendix.

### Optimization Equivalence of the Individual Experiences

In this section, our objective is to investigate whether the decomposition of individual experiences in DIFFER yields an equivalence to team experiences within the original learning framework. To accomplish this, we focus on the QMIX as the base model, which updates agent networks using the team TD-loss computed from team experiences. We introduce a variant model named **QMIX-divide**. In QMIX-divide, the agent networks are updated using the individual TD-loss calculated from **all** decomposed individual experiences. Similar to QMIX, the mixing network of QMIX-divide is updated using the team TD-loss. Compared with DIFFER, QMIX-divide preserves the decomposer of team experiences but omits this selection phase in the fair experience replay. Our aim is to test the hypothesis that the decomposed individual experiences have no impact on agent network optimization. We compare the performance of QMIX-divide and QMIX on SMAC scenarios, as depicted in Fig. 3. Our results demonstrate that QMIX-divide and QMIX exhibit nearly identical performance, providing evidence in support of the optimization equivalence between the team reward and the approximated individual reward. Furthermore, we highlight the critical role played by the selection phase in the fair experience replay, as the omission of this phase in QMIX-divide leads to comparable performance with the baselines QMIX.

### Performance Comparison

Performance Improvement towards Value Factorization Methods.We investigated whether DIFFER could enhance the performance of established value factorization methods. To evaluate this, we conducted experiments using QMIX, QPLEX, and COMIX as the base models in various environments. The training curves for each model are depicted in Fig. 4, Fig. 5, Fig. 6, and Fig. 7. As illustrated in the figures, DIFFER exhibited notable improvements in both learning speed and overall performance compared to the baselines during the training process. The magnitude of improvement varied across different scenarios. Notably, DIFFER demonstrated a more significant performance boost in scenarios where agents possessed distinct physical structures (\(e.g.\), agents with different

Figure 4: Performance comparison of QMIX-DIFFER (ours) and QMIX on SMAC scenarios, highlighting the performance improvement of our method towards QMIX.

Figure 3: Performance comparison of QMIX-divide and QMIX on SMAC scenarios, highlighting the optimization equivalence between team reward and individual reward calculated by DIFFER.

points in Humanoid - v2_0) or subtasks (\(e.g.\), agents with different skills in 3s5z_vs_3s6z). This observation aligns with our hypothesis that scenarios with greater dissimilarity among agents result in a more pronounced improvement through the distinction of individual experiences facilitated by DIFFER. Conversely, in scenarios where agents were homogeneous (\(e.g.\), 27m_vs_30m in SMAC), DIFFER exhibited performance similar to that of the base model. In such cases, the lack of significant differences among agents rendered the decomposition of individual experiences less impactful. Overall, our findings demonstrate that DIFFER effectively enhances the performance of established value factorization methods. The degree of improvement depends on the specific characteristics of the scenarios, emphasizing the importance of considering the heterogeneity of agents when applying DIFFER.

Comparison with MARL ER methods.In Fig. 4, we present a comparison of the performance of DIFFER and MASER. While both models implement individual rewards and leverage individual experiences to update agent networks, they differ in their optimization objectives. Specifically, DIFFER retains the original objective of maximizing the team action-value \(Q_{tot}\). In contrast, MASER aims to maximize a mixture of individual action-value \(Q_{i}\) and team action-value \(Q_{tot}\), which alters the original optimization objective. As a consequence of this shift in optimization objective, the performance of MASER is observed to be worse than DIFFER in challenging environments.

Figure 5: Performance comparison of QPLEX-DIFFER (ours) and QPLEX on SMAC scenarios, highlighting the performance improvement of our method towards QPLEX.

Figure 8: Screenshots of QMIX (left) and QMIX-DIFFER (right) on SMAC scenario MMM2. The red team is controlled by a trained model, while the blue team is controlled by a built-in AI. The Medivac in the red team is marked by an orange square. The red team is controlled by QMIX in the left subfigure and by QMIX-DIFFER in the right subfigure.

Figure 6: Performance comparison of QMIX-DIFFER (ours) and QMIX on GRF scenarios, highlighting the performance improvement of our method towards QMIX.

Case Study.The screenshot in Fig. 8 shows the game screen for the QMIX and QMIX-DIFFER models in the SMAC scenario MMM2. As previously noted, learning an effective strategy for the medivac unit is particularly challenging due to its distinct sub-task and a limited number of units. A successful medivac strategy must incorporate a viable movement strategy that avoids enemy attacks and reduces unnecessary damage, as well as a sound selection of healing objects that maximizes team damage output. In the QMIX controlled agent team (Fig. 8, left), the medivac failed to learn an appropriate movement strategy, moving too close to the enemy team and dying early in the episode. As a result, the other agents bled out quickly and the episode ended in defeat. In contrast, the agent team controlled by QMIX-DIFFER (Fig. 8, right) demonstrated an effective medivac strategy, positioning the unit in a safe location to avoid enemy attacks and providing healing support throughout the game. The resulting advantage for the red team led to a final victory.

### Analysis for TD-error of Individual Experiences

In this section, we delve into the differentiation of individual experiences by analyzing their TD-errors. Throughout the training process, we compute the standard deviation (std) of the TD-errors of the decomposing individual experiences and normalize them using the mean TD-error. Figure 9 showcases the training curve of the normalized std of individual TD-errors. Notably, we observe a substantial increase in the normalized std at the initial stages of training, which remains consistently high until the conclusion of the training process. This observation underscores the substantial distinctions that exist among individual experiences when evaluated based on their TD-errors. Our DIFFER framework, by decomposing individual rewards, effectively captures and exploits these distinctions among individual experiences. Consequently, DIFFER exhibits superior performance compared to the baseline models. These findings highlight the efficacy of DIFFER in leveraging the differentiation among individual experiences to enhance learning outcomes in MARL settings.

### Visualization of Individual Rewards Produced by DIFFER

In order to provide a clear visual representation of the individual rewards acquired through our proposed DIFFER method, we introduce game screenshots alongside the corresponding individual rewards for each agent, as illustrated in Fig. 10. For the purpose of analysis, we consider two distinct timesteps of an episode within the MMM2 scenario. Our team comprises 2 Marauders (index 0-1), 7 Marines (index 2-8), and a Medivac (index 9). The team rewards assigned to agents are proportional to the damage inflicted on enemy units and the number of enemy unit deaths. During the initial stage of the episode (timestep 4, (a)), our units possess sufficient health and do not require assistance from the Medivac. Consequently, the contribution of the Medivac to the team's performance is minimal. As a result, the individual reward attributed to the Medivac is relatively lower compared to the Marauders and Marines, who play active roles in offensive operations. Agents 2-5, being in closer proximity to the enemy and inflicting substantial damage, enjoy higher individual rewards due to their effective engagement strategies. At timestep 14 (b), except for agent 7 and 8 who have perished, the individual rewards assigned to each agent are substantially balanced. This indicates a comparable level of contribution from all agents towards achieving team objectives at this particular point in the game. The persistent correlation between individual rewards

Figure 10: The visualization of individual rewards produced by our DIFFER method, along with game screenshots captured at timestep 4 and timestep 14 of the MMM2 scenario. The index of each agent have been marked in orange number. The consistent correlation between individual rewards and the ongoing game situation serves as a testament to the efficacy and rationality of our DIFFER method.

and the dynamic in-game circumstances not only reaffirms the effectiveness of our DIFFER method but also validates its rationality.

## 5 Related Work

Experiment Replay (ER) is a widely used mechanism for improving data utilization and accelerating learning efficiency in reinforcement learning (RL). ER reuses the historical experience data for updating current policy . In multi-agent RL, using a single-agent ER algorithm directly to obtain individual experience is naive if agents can obtain accurate individual rewards from the environment . However, in the multi-agent problem setting addressed in this work, a team of agents only has access to a shared team reward, making it impossible to obtain an accurate individual reward. Consequently, multi-agent ER algorithms employ joint transitions as minimal training data . MASER is a method similar to ours, as it calculates individual rewards from the experience replay buffer and uses individual experience to train the individual value network. However, MASER generates individual rewards to maximize the weighted sum of individual action-value and team action-value, which may violate the original optimization target of multi-agent RL, \(i.e.\), maximizing the team action-value. In our work, we propose a novel strategy for approximating individual reward and demonstrate its optimization equivalence. To our knowledge, DIFFER is the first work to approximate individual reward in a way that ensures optimization equivalence.

## 6 Conclusion

Limitation.A limitation of the DIFFER method is the introduction of additional computation when calculating the individual reward and TD-error. This increased computational cost is an inherent trade-off for the benefits gained in terms of fairness and improved learning efficiency. The additional computational burden should be considered when applying this method in resource-constrained settings or scenarios with strict real-time requirements.

In conclusion, this paper presents the DIFFER framework. By decomposing the team reward into individual rewards, DIFFER enables agents to effectively distinguish and leverage important individual experiences. Through the invariance of network gradients, we derive a partial differential equation that facilitated the computation of the underlying individual rewards function. By incorporating the solved closed-form individual rewards, we calculate the individual temporal-difference error, providing the significance of each experience. Notably, DIFFER showcases its adaptability in handling both homogeneous and diverse individual experiences, demonstrating its versatility in various scenarios. Our extensive experiments on popular benchmarks demonstrate the effectiveness of DIFFER in terms of learning efficiency and fairness, surpassing the performance of existing MARL ER methods. We hope that our work provides a new perspective on experience replay methods in MARL, inspiring further research and advancements in the field.