# Functional Equivalence and Path Connectivity

of Reducible Hyperbolic Tangent Networks

Matthew Farrugia-Roberts

School of Computing and Information Systems

The University of Melbourne

matthew@far.in.net

###### Abstract

Understanding the learning process of artificial neural networks requires clarifying the structure of the parameter space within which learning takes place. A neural network parameter's _functional equivalence class_ is the set of parameters implementing the same input-output function. For many architectures, almost all parameters have a simple and well-documented functional equivalence class. However, there is also a vanishing minority of _reducible_ parameters, with richer functional equivalence classes caused by redundancies among the network's units. In this paper, we give an algorithmic characterisation of unit redundancies and reducible functional equivalence classes for a single-hidden-layer hyperbolic tangent architecture. We show that such functional equivalence classes are piecewise-linear path-connected sets, and that for parameters with a majority of redundant units, the sets have a diameter of at most 7 linear segments.

## 1 Introduction

Deep learning algorithms construct neural networks through a local search in a high-dimensional parameter space. This search is guided by the shape of some loss landscape, which is in turn determined by the link between neural network parameters and their input-output functions. Thus, understanding the link between parameters and functions is key to understanding deep learning.

It is well known that neural network parameters often fail to uniquely determine an input-output function. For example, exchanging weights between two adjacent hidden units generally preserves functional equivalence (Hecht-Nielsen, 1990). For many architectures, almost all parameters have a simple class of functionally equivalent parameters. These classes have been characterised for multi-layer feed-forward architectures with various nonlinearities (e.g., Sussmann, 1992; Albertini et al., 1993; Kurkova and Kainen, 1994; Phuong and Lampert, 2020; Vlacic and Bolcskei, 2021).

However, all existing work on functional equivalence excludes from consideration certain measure zero sets of parameters, for which the functional equivalence classes may be richer. One such family of parameters is the so-called _reducible parameters_. These parameters display certain structural redundancies, such that the same function could be implemented with fewer hidden units (Sussmann, 1992; Vlacic and Bolcskei, 2021), leading to a richer functional equivalence class.

Despite their atypicality, reducible parameters may play an important role in deep learning. Learning exerts non-random selection pressure, so measure zero sets of parameters may still arise in practice, and indeed reducible parameters are appealing solutions due to parsimony (cf. Farrugia-Roberts, 2023). These parameters are also a source of information singularities (cf. Fukumizu, 1996), relevant to statistical theories of deep learning (Watanabe, 2009; Wei et al., 2022). Moreover, the structure of functional equivalence classes has implications for the local and global shape of the loss landscape, in ways that may influence learning dynamics _near_ reducible parameters.

In this paper, we study functional equivalence classes for single-hidden-layer networks with the hyperbolic tangent nonlinearity, building on the foundational work of Sussmann (1992) on reducibility in this setting. We offer the following theoretical contributions.

1. In Section 4, we give a formal algorithm producing a canonical representative parameter from any functional equivalence class, by systematically eliminating all sources of structural redundancy. This extends prior algorithms that only handle irreducible parameters.
2. In Section 5, we invert this canonicalisation algorithm to characterise the functional equivalence class of any parameter as a union of simple parameter manifolds. This characterisation extends the well-known result for irreducible parameters.
3. We show that in the reducible case, the functional equivalence class is a piecewise-linear path-connected set--that is, any two functionally equivalent reducible parameters are connected by a piecewise linear path comprising only equivalent parameters (Theorem 6.1).
4. We show that if a parameter has a high degree of reducibility (in particular, if the same function can be implemented using half of the available hidden units), then the number of linear segments required to connect any two equivalent parameters is at most 7 (Theorem 6.3).

While the single-hidden-layer hyperbolic tangent architecture is not immediately relevant to modern deep learning, it enables the first comprehensive analysis of neural network structural redundancy. Moreover, feed-forward layers are a fundamental building block of many modern architectures, and so our analysis is partially informative for such extensions. In Section 7 we discuss such extensions and others, as well as connections to other topics in deep learning including loss landscape analysis, model compression, and singular learning theory.

## 2 Related Work

Sussmann (1992) studied functional equivalence in single-hidden-layer hyperbolic tangent networks, showing that two irreducible parameters are functionally equivalent if and only if they are related by simple operations of exchanging and negating the weights of hidden units. This result was later extended to architectures with a broader class of nonlinearities (Albertini et al., 1993; Kurkova andainen, 1994), to architectures with multiple hidden layers (Fefferman and Markel, 1993; Fefferman, 1994), and to certain recurrent architectures (Albertini and Sontag, 1992, 1993a,b,c). More recently, similar results have been found for ReLU networks (Phuong and Lampert, 2020; Bona-Pellissier et al., 2021; Stock and Gribonval, 2022), and Vlacic and Bolcskei (2021, 2022) have generalised Sussmann's results to a very general class of architectures and nonlinearities. However, all of these results have come at the expense of excluding from consideration certain measure zero subsets of parameters with richer functional equivalence classes.

A similar line of work has documented the global symmetries of the parameter space--bulk transformations of the entire parameter space that preserve all implemented functions. The search for such symmetries was launched by Hecht-Nielsen (1990). Chen et al. (1993, also Chen and Hecht-Nielsen, 1991) showed that in the case of multi-layer hyperbolic tangent networks, all analytic symmetries are generated by unit exchanges and negations. Ruger and Ossen (1997) extended this result to additional sigmoidal nonlinearities. The analyticity condition excludes discontinuous symmetries acting selectively on, say, reducible parameters with richer equivalence classes (Chen et al., 1993).

Ruger and Ossen (1997) provide a canonicalisation algorithm. Their algorithm negates each hidden unit's weights until the bias is positive, and then sorts each hidden layer's units into non-descending order by bias weight. This algorithm is invariant precisely to the exchanges and negations mentioned above, but fails to properly canonicalise equivalent parameters that differ in more complex ways.

To our knowledge, there is one line of work bearing directly on the topic of the functional equivalence classes of reducible parameters. Fukumizu and Amari (2000) and Fukumizu et al. (2019) have catalogued methods of adding a single hidden unit to a neural network while preserving the network's function, and Simsek et al. (2021) have extended this work to consider the addition of multiple hidden units. Though derived under a distinct framing, it turns out that the subsets of parameter space accessible by such unit additions correspond to functional equivalence classes, similar to those we study (though in a slightly different architecture). We note these similarities, especially regarding our contributions (2) and (3), in Remarks 5.4 and 5.5 and Remark 6.2.

Preliminaries

We consider a family of fully-connected, feed-forward neural network architectures with a single input unit, a single biased output unit, and a single hidden layer of \(h\) biased hidden units with the hyperbolic tangent nonlinearity \((z)=(e^{z}-e^{-z})/(e^{z}+e^{-z})\). Such an architecture has a parameter space \(_{h}=^{3h+1}\). Our results generalise directly to networks with multi-dimensional inputs and outputs, as detailed in Appendix A.

The weights and biases of the network's units are encoded in the parameter vector in the format \((a_{1},b_{1},c_{1},,a_{h},b_{h},c_{h},d)_{h}\) where for each hidden unit \(i=1,,h\) there is an _outgoing weight_\(a_{i}\), an _incoming weight_\(b_{i}\), and a _bias_\(c_{i}\), and \(d\) is an _output unit bias_. Thus each parameter \(w=(a_{1},b_{1},c_{1},,a_{h},b_{h},c_{h},d)_{h}\) indexes a mathematical function \(f_{w}:\) defined as follows:

\[f_{w}(x)=d+_{i=1}^{h}a_{i}(b_{i}x+c_{i}).\]

Two parameters \(w_{h},w^{}_{h^{}}\) are _functionally equivalent_ if and only if \(f_{w}=f_{w^{}}\) as functions on \(\) (that is, \( x,f_{w}(x)=f_{w^{}}(x)\)). Functional equivalence is of course an equivalence relation on \(_{h}\). Given a parameter \(w_{h}\), the _functional equivalence class_ of \(w\), denoted \([w]\), is the set of all parameters in \(_{h}\) that are functionally equivalent to \(w\):

\[[w]=\{\,w^{}_{h}\,|\,f_{w}=f_{w^{}}\,\}.\]

For this family of architectures, the functional equivalence class of almost all parameters is a discrete set fully characterised by simple _unit negation and exchange transformations_\(_{i},_{i,j}:_{h}_{h}\) for \(i,j=1,,h\), where

\[_{i}(a_{1},b_{1},c_{1},,a_{h},b_{h},c_{h},d) =(a_{1},b_{1},c_{1},,-a_{i},-b_{i},-c_{i},,a_{h},b_{h },c_{h},d)\] \[_{i,j}(a_{1},b_{1},c_{1},,a_{h},b_{h},c_{h},d) =(a_{1},b_{1},c_{1},,c_{i-1},a_{j},b_{j},c_{j},a_{i+1},\] \[,c_{j-1},a_{i},b_{i},c_{i},a_{j+1},,a_{h},b_{h},c_{h},d).\]

More formally, these transformations generate the full functional equivalence class for all so-called irreducible parameters (Sussmann, 1992). A parameter \(w=(a_{1},b_{1},c_{1},,a_{h},b_{h},c_{h},d)_{h}\) is _reducible_ if and only if it satisfies any of the following conditions (otherwise, \(w\) is _irreducible_):

1. \(a_{i}=0\) for some \(i\), or
2. \(b_{i}=0\) for some \(i\), or
3. \((b_{i},c_{i})=(b_{j},c_{j})\) for some \(i j\), or
4. \((b_{i},c_{i})=(-b_{j},-c_{j})\) for some \(i j\).

Sussmann (1992) also showed that in this family of architectures, reducibility corresponds to _non-minimality_: a parameter \(w_{h}\) is reducible if and only if \(w\) is functionally equivalent to some \(w^{}_{h^{}}\) with fewer hidden units \(h^{}<h\). We define the _rank_ of \(w\), denoted \((w)\), as the minimum number of hidden units required to implement \(f_{w}\):

\[(w)=\{\,h^{}\,|\, w^{ }_{h^{}};\ f_{w}=f_{w^{}}\,\}.\]

Finally, we make use of the following notions of connectivity for a set of parameters. Given a set \(W_{h}\), define a _piecewise linear path in \(W\)_ as a continuous function \(: W\) comprising a finite number of linear segments. Two parameters \(w,w^{}_{h}\) are _piecewise-linear path-connected in \(W\)_, denoted \(w w^{}\) (with \(W\) implicit), if there exists a piecewise linear path in \(W\) such that \((0)=w\) and \((1)=w^{}\). Note that \(\) is an equivalence relation on \(W\). A set \(W_{h}\) is itself _piecewise-linear path-connected_ if and only if \(\) is the full relation, that is, all pairs of parameters in \(W\) are piecewise linear path-connected in \(W\).

The _length_ of a piecewise linear path is the number of maximal linear segments comprising the path. The _distance_ between two piecewise linear path-connected parameters is the length of the shortest path connecting them. The _diameter_ of a piecewise linear path-connected set is the largest distance between any two parameters in the set.

## 4 Parameter Canonicalisation

A parameter _canonicalisation algorithm_ maps each parameter in a functional equivalence class to a consistent representative parameter within that class. A canonicalisation algorithm serves as a theoretical test of functional equivalence. In Section 5 we invert our canonicalisation algorithm to characterise the functional equivalence class. More practically, canonicalising parameters before measuring the distance between them yields a metric that is independent of functionally irrelevant details such as unit permutations.

Prior work has described canonicalisation algorithms for certain irreducible parameters (Ruger and Ossen, 1997); but when applied to functionally equivalent reducible parameters, such algorithms may fail to produce the same output. We introduce a canonicalisation algorithm that properly canonicalises both reducible and irreducible parameters, based on similar negation and sorting stages, combined with a novel _reduction_ stage. This stage effectively removes or 'zeroes out' redundant units through various operations that exploit the reducibility conditions. This process isolates a functionally equivalent but irreducible subparameter.

**Algorithm 4.1** (Parameter canonicalisation).: Given a parameter space \(_{h}\), proceed:

```
1:procedureCanonicalise(\(w=(a_{1},b_{1},c_{1},,a_{h},b_{h},c_{h},d)_{h}\))
2:\(\)Stage 1: Reduce the parameter; zeroing out redundant hidden units \(\)
3:\(Z\{\}\)\(\)keep track of 'zeroed' units
4:while any of the following four conditions hold do
5:if for some hidden unit \(i Z\), \(a_{i}=0\)then\(\)reducibility condition (i)
6:\(b_{i},c_{i} 0\)
7:\(Z Z\{i\}\)
8:elseif for some hidden unit \(i Z\), \(b_{i}=0\)then\(\) ----(ii)
9:\(d d+a_{i}(c_{i})\)
10:\(a_{i},c_{i} 0\)
11:\(Z Z\{i\}\)
12:elseif for some hidden units \(i,j Z,i j\), \((b_{i},c_{i})=(b_{j},c_{j})\)then\(\) ----(iii)
13:\(a_{j} a_{j}+a_{i}\)
14:\(a_{j},b_{i},c_{i} 0\)
15:\(Z Z\{i\}\)
16:elseif for some hidden units \(i,j Z,i j\), \((b_{i},c_{i})=(-b_{j},-c_{j})\)then\(\) ----(iv)
17:\(a_{j} a_{j}-a_{i}\)
18:\(a_{i},b_{i},c_{i} 0\)
19:\(Z Z\{i\}\)
20:endif
21:endwhile
22:\(\)Stage 2: Negate the nonzero units to have positive incoming weights \(\)
23:for each hidden unit \(i Z\)do
24:\(a_{i},b_{i},c_{i}(b_{i})(a_{i},b_{i},c_{i})\)
25:endfor
26:\(\)Stage 3: Sort the units by their incoming weights and biases \(\)
27:\(\) a permutation sorting \(i=1,,h\) by decreasing \(b_{i}\), breaking ties with decreasing \(c_{i}\)
28:\(w(a_{(1)},b_{(1)},c_{(1)},,a_{(h)},b_{(h)},c_{ (h)},d)\)
29:\(\)Now, \(w\) has been mutated into the canonical equivalent parameter \(\)
30:return\(w\)
31:endprocedure ```

The following theorem establishes the correctness of Algorithm 4.1.

**Theorem 4.2**.: _Let \(w,w^{}_{h}\). Let \(v=(w)\) and \(v^{}=(w^{})\). Then (i) \(v\) is functionally equivalent to \(w\); and (ii) if \(w\) and \(w^{}\) are functionally equivalent, then \(v=v^{}\)._

Proof.: For (i), observe that \(f_{w}\) is maintained by each iteration of the loops in Stages 1 and 2, and by the permutation in Stage 3. For (ii), observe that Stage 1 isolates functionally equivalent _and irreducible_ subparameters \(u_{r}\) and \(u^{}_{r^{}}\) of the input parameters \(w\) and \(w^{}\) (excluding the zeroed units). We have \(f_{u}=f_{w}=f_{w^{}}=f_{u^{}}\), so by the results of Sussmann (1992), \(r=r^{}=(w)\), and \(u\) and \(u^{}\) are related by unit negation and exchange transformations. This remains true in the presence of the zero units. Stages 2 and 3 are invariant to precisely such transformations.

[MISSING_PAGE_FAIL:5]

Path Connectivity

In this section, we show that the reducible functional equivalence class is piecewise linear path-connected (Theorem 6.1), and, for parameters with rank at most half of the available number of hidden units, has diameter at most 7 linear segments (Theorem 6.3).

**Theorem 6.1**.: _Let \(w_{h}\). If \(w\) is reducible, then \([w]\) is piecewise linear path-connected._

Proof.: It suffices to show that each reducible parameter \(w_{h}\) is piecewise linear path-connected in \([w]\) to its canonical representative \((w)\). The path construction proceeds by tracing the parameter's mutations in the course of execution of Algorithm 4.1. For each iteration of the loops in Stages 1 and 2, and for each transposition in the permutation in Stage 3, we construct a multi-segment sub-path. To describe these sub-paths, we denote the parameter at the beginning of each sub-path as \(w=(a_{1},b_{1},c_{1},,a_{h},b_{h},c_{h},d)\), noting that this parameter is mutated throughout the algorithm, but is functionally equivalent to the original \(w\) at all of these intermediate points.

1. In each iteration of the Stage 1 loop, the construction depends on the chosen branch, as follows. Some examples are illustrated in Figure 1. 1. A direct path interpolating \(b_{i}\) and \(c_{i}\) to zero. 2. A two-segment path, interpolating \(a_{i}\) to zero and \(d\) to \(d+a_{i}(c_{i})\), then \(c_{i}\) to zero. 3. A two-segment path, interpolating \(a_{i}\) to zero and \(a_{j}\) to \(a_{j}+a_{i}\), then \(b_{i}\) and \(c_{i}\) to zero. 4. A two-segment path, interpolating \(a_{i}\) to zero and \(a_{j}\) to \(a_{j}-a_{i}\), then \(b_{i}\) and \(c_{i}\) to zero.

Since (the original) \(w\) is reducible, (the current) \(w\) must have gone through at least one iteration in Stage 1, and must have at least one _blank_ unit \(k\) with \(a_{k},b_{k},c_{k}=0\). From any such parameter \(w\), there is a three-segment path in \([w]\) that implements a _blank-exchange manoeuvre_ transferring the weights of another unit \(i\) to unit \(k\), and leaving \(a_{i},b_{i},c_{i}=0\): first interpolate \(b_{k}\) to \(b_{i}\) and \(c_{k}\) to \(c_{i}\); then interpolate \(a_{k}\) to \(a_{i}\) and \(a_{i}\) to zero; then interpolate \(b_{i}\) and \(c_{i}\) to zero. Likewise, there is a three-segment path that implements a _negative blank-exchange manoeuvre_, negating the weights as they are interpolated into the blank unit. With these manoeuvres noted, proceed:

1. In each iteration of the Stage 2 loop for which \((b_{i})=-1\), let \(k\) be a blank unit, and construct a six-segment path. First, blank-exchange unit \(i\) into unit \(k\). Then, negative blank-exchange unit \(k\) into unit \(i\). The net effect is to negate unit \(i\).
2. In Stage 3, construct a path for each segment in a decomposition of the permutation \(\) as a product of transpositions. Consider the transposition \((i,j)\). If \(i\) or \(j\) is blank, simply blank-exchange them. If neither is blank, let \(k\) be a blank unit. Construct a nine-segment path, using three blank-exchange manoeuvres, using \(k\) as 'temporary storage' to implement the transposition: first blank-exchange units \(i\) and \(k\), then blank-exchange units \(i\) (now blank) and \(j\), then blank-exchange units \(j\) (now blank) and \(k\) (containing \(i\)'s original weights).

The resulting parameter is the canonical representative and it can be verified that each segment in each sub-path remains in \([w]\) as required. 

**Remark 6.2**.: Simsek et al. (2021, Theorem B.4) construct similar paths to show the connectivity of their expansion manifold (cf. Remark 5.5). They first connect reduced-form parameters using blank-exchange manoeuvres and then show inductively that each unit addition preserves connectivity.

Figure 1: Example paths constructed for each of the Stage 1 branches. Other dimensions held fixed.

**Theorem 6.3**.: _Let \(w_{h}\). If \((w)\), then \([w]\) has diameter at most \(7\)._

Proof.: Let \(w_{h}\) with \((w)=r\). Let \(w^{}[w]\). We construct a piecewise linear path from \(w\) to \(w^{}\) with \(7\) segments. By Theorem 6.1, a path exists via the canonical representative parameter \(v=(w)\). However, this path has excessive length. We compress the length to \(7\) by exploiting the following opportunities to parallelise segments and 'cut corners'. These optimisation steps are illustrated in Figure 2.

1. Let the Stage 1 result from Algorithm 4.1 for \(w\) be denoted \(u\). Let the Stage 1 result for \(w^{}\) be denoted \(u^{}\). Instead of following the unit negation and exchange transformations from \(u\) to \(v\), and then back to \(u^{}\), we transform \(u\) into \(u^{}\) directly, not (necessarily) via \(v\).
2. We connect \(w\) to \(u\) using two segments, implementing all iterations of Stage 1 in parallel. The first segment shifts the outgoing weights from the blank units to the non-blank units and the output unit bias. The second segment interpolates the blank units' incoming weights and biases to zero. We apply the same optimisation to connect \(w^{}\) and \(u^{}\).
3. We connect \(u\) and \(u^{}\) using two blank-exchange manoeuvres (6 segments), exploiting the majority of blank units as 'temporary storage'. First, we blank-exchange the non-blank units of \(u\) into blank units of \(u^{}\), resulting in a parameter \(^{}\) sharing no non-blank units with \(u^{}\). Then, we (negative) blank-exchange those weights into the appropriate non-blank units of \(u^{}\), implementing the unit negation and exchange transformations relating \(u\), \(^{}\), and \(u^{}\).
4. The manoeuvres in (b) and (c) begin and/or end by interpolating incoming weights and biases of blank units from and/or to zero, while the outgoing weights are zero. We combine adjacent beginning/end segments together, interpolating directly from the start to the end, without (necessarily) passing through zero. This results in the required seven-segment path, tracing the sequence of parameters \(w,w^{1},w^{2},,w^{6},w^{} W_{h}\).

To describe the constructed path in detail, we introduce the following notation for the components of the key parameters \(w,w^{},u,u^{},w^{1},w^{2},,w^{6}_{h}\):

\[w =(a_{1}^{w},b_{1}^{v},c_{1}^{v},,a_{h}^{w},b_{h}^{w},c_{h}^ {w},d^{w}) u =(a_{1}^{u},b_{1}^{u},c_{1}^{u},,a_{h}^{u},b_{h}^{u},c_{h}^ {u},d^{u})\] \[w^{} =(a_{1}^{w^{}},b_{1}^{w^{}},c_{1}^{w^{}}, ,a_{h}^{w^{}},b_{h}^{w^{}},c_{h}^{w^{}},d^{w^{}}) u^{} =(a_{1}^{u^{}},b_{1}^{u^{}},c_{1}^{u^{}},, a_{h}^{u^{}},b_{h}^{u^{}},c_{h}^{u^{}},d^{u^{}})\] \[w^{k} =(a_{1}^{k},b_{1}^{k},c_{1}^{k},,a_{h}^{k},b_{h}^{k},c_{h}^ {k},d^{k}) (k=1,,6).\]

Of the \(h\) units in \(u\), exactly \(h-r\) are blank--those in the set \(Z\) from \((w)\). Denote the complement set of \(r\) non-blank units \(U=\{1,,h\} Z\). Likewise, define \(Z^{}\) and \(U^{}\) from \(u^{}\).

Figure 2: A conceptual illustration of the four path optimisations, producing a seven-segment piecewise linear path of equivalent parameters in a high-dimensional parameter space. **(a)** Follow unit negation and exchange transformations directly between reduced parameters, not via the canonical parameter. **(b) & **(c)** Parallelise the reduction steps, and use the majority of blank units to parallelise the transformations. **(d)** Combine first/last segments of reduction and blank-exchange manoeuvres.

With notation clarified, we can now describe the key points \(w^{1},,w^{6}\) in detail, while showing that the entire path is contained within the functional equivalence class \([w]\).

1. The first segment interpolates each outgoing weight from \(a_{i}^{w}\) to \(a_{i}^{u}\), and interpolates the output bias from \(d^{w}\) to \(d^{u}\). That is, \(w^{1}=(a_{1}^{u},b_{1}^{w},c_{1}^{w},,a_{h}^{u},b_{h}^{w},c_{h}^{w},d^{u})\). To see that this segment is within \([w]\), observe that since the incoming weights and biases are unchanged between the two parameters, \(f_{tw^{1}+(1-t)w}(x)=tf_{w^{1}}(x)+(1-t)f_{w}(x)\) for \(x\) and \(t\). To show that \(f_{w}=f_{w^{1}}\), we construct a function \(:\{1,,h\}\{0,1,,h\}\) from identity following each iteration of Stage 1 of Canonicalise\((w)\): when the second branch is chosen, remap \((i)\) to \(0\); and when the third or fourth branch is chosen, for \(k^{-1}[i]\) (including \(i\) itself), remap \((k)\) to \(j\). Moreover, we define a sign vector \(\{-1,+1\}^{h}\) where \(_{i}=-1\) if \((b_{i}^{w})=-1\), otherwise \(_{i}=+1\). Then: \[f_{w}(x) =d^{w}+_{j=0}^{k}_{i^{-1}[j]}a_{i}^{w}(b_{i}^ {w}x+c_{i}^{w})\] \[=d^{w}+_{i^{-1}}a_{i}^{w}(c_{i}^{w})+_{j=1}^ {h}(_{i^{-1}[j]}_{j}_{i}a_{i}^{w})(b_{j }^{w}x+c_{j}^{w})\] \[=d^{u}+_{j=1}^{h}a_{j}^{u}(b_{j}^{w}x+c_{j}^{w})=f_{w^{1} }(x).\]
2. The second segment completes the reduction and begins the first blank-exchange manoeuvre to store the nonzero units in \(Z^{}\). For \(i U U^{}\), pick distinct'storage' units \(j Z Z^{}\). There are enough, as \(r\) by assumption thus \(|U U^{}|=|U|-|Z U^{}|=r-|Z U^{}|(h-r)-|Z U ^{}|=|Z^{}|-|Z U^{}|=|Z^{} Z|\). Interpolate unit \(j\)'s incoming weight from \(b_{j}^{w}\) to \(b_{i}^{w}\) and interpolate its bias from \(c_{j}^{w}\) to \(c_{i}^{w}\). Meanwhile, for all other \(j Z\), interpolate the incoming weight and bias to zero. This segment is within \([w]\) as for \(j Z\), \(a_{j}^{1}=a_{j}^{u}=0\) by definition of \(Z\).
3. The third segment shifts the outgoing weights from the units in \(U U^{}\) to the units in \(Z Z^{}\) prepared in step (2). For \(i U U^{}\), pick the same storage unit \(j\) as in step (2). Interpolate unit \(j\)'s outgoing weight from \(a_{j}^{u}=0\) to \(a_{i}^{u}\) and interpolate unit \(i\)'s outgoing weight from \(a_{i}^{u}\) to zero. This segment is within \([w]\) as \(b_{i}^{2}=b_{j}^{2}\) and \(c_{i}^{2}=c_{j}^{2}\) by step (2).
4. The fourth segment completes the first blank-exchange manoeuvre and begins the second, to form the units of \(u^{}\). For \(i U^{}\), interpolate unit \(i\)'s incoming weight from \(b_{i}^{3}\) to \(b_{i}^{u^{}}\) and interpolate its bias from \(c_{i}^{3}\) to \(c_{i}^{u^{}}\). This segment is within \([w]\) because for \(i U^{} Z\), \(a_{i}^{3}=a_{i}^{u}=0\) by definition of \(Z\), and for \(i U^{} U\), \(a_{i}^{3}=0\) by step (3).
5. The fifth segment shifts the outgoing weights from the selected units in \(Z^{}\) to the units in \(U^{}\) prepared in step (4). We simply interpolate each unit \(i\)'s outgoing weight to \(a_{i}^{u^{}}\). To see that the segment is within \([w]\), note that \(u\) and \(u^{}\) are related by some unit negation and exchange transformations. Therefore, there is a correspondence between their sets of nonzero units, such that corresponding units have the same (or negated) incoming weights and biases. Due to steps (2)-(4) there are \(r\)'storage' units in \(w^{4}\) with the weights of the units of \(u\), and the correspondence extends to these storage units. Since the storage units are disjoint with \(U^{}\), this fifth segment has the effect of interpolating the outgoing weight of each of the storage units \(j Z^{}\) in \(w^{4}\) from \(a_{i}^{u}\) to zero (where \(i\) is as in step (3)), while interpolating the outgoing weight of its corresponding unit \(k U^{}\) from zero to \( a_{i}^{u}=a_{k}^{u^{}}\) (where the sign depends on the unit negation transformations relating \(u\) and \(u^{}\)).
6. The sixth segment completes the second blank-exchange manoeuvre and begins to reverse the reduction. For \(i Z^{}\), interpolate unit \(i\)'s incoming weight from \(b_{i}^{5}\) to \(b_{i}^{w^{}}\), and interpolate its bias from \(c_{i}^{5}\) to \(c_{i}^{w^{}}\). This segment is within \([w]\) as for \(i Z^{}\), \(a_{i}^{5}=a_{i}^{u^{}}=0\) by definition of \(Z^{}\).
7. The seventh segment, of course, interpolates from \(w^{6}\) to \(w^{}\). To see that this segment is within \([w]\), note that by steps (5) and (6), \(w^{6}=(a_{1}^{u^{}},b_{1}^{w^{}},c_{1}^{w^{}},,a_{h}^{ u^{}},b_{h}^{w^{}},c_{h}^{w^{}},d^{u^{}})\) (noting \(d^{u}=d^{u^{}}\) since the output unit's bias is preserved by unit transformations). So the situation is the reverse of step (1), and a similar proof applies.

Discussion

In this paper, we have investigated functional equivalence classes for reducible single-hidden-layer hyperbolic tangent network parameters, and their connectivity properties. Recall that irreducible parameters have discrete functional equivalence classes described by simple unit negation and exchange transformations. In contrast, reducible functional equivalence classes form a complex union of manifolds, displaying the following rich qualitative structure:

* There is a central discrete constellation of _reduced-form_ parameters, each with maximally many blank units alongside an irreducible subparameter. These reduced-form parameters are related by unit negation and exchange transformations, like for irreducible parameters.
* Unlike in the irreducible case, these reduced-form parameters are connected by a network of piecewise linear paths. Namely, these are (negative) blank-exchange manoeuvres, and, when there are multiple blank units, simultaneous parallel blank-exchange manoeuvres.
* Various manifolds branch away from this central network, tracing in reverse the various reduction operations (optionally in parallel). Dually, these manifolds trace methods for _adding_ units (cf., Fukumizu and Amari, 2000; Fukumizu et al., 2019; Simsek et al., 2021).

Theorem 6.3 establishes that when there is a _majority_ of blank units, the diameter of the entire union of manifolds becomes a small constant number of linear segments. With fewer blank units it will sometimes require more blank-exchange manoeuvres to traverse the central network of reduced-form parameters. Future work could investigate efficient implementation of various permutations through parallel blank-exchange manoeuvres with varying numbers of blank units available.

Towards modern architectures.Single-hidden-layer hyperbolic tangent networks appear far from relevant to modern deep learning architectures. However, we expect our canonicalisation algorithm to partially generalise and some aspects of our connectivity findings to generalise as follows.

_Structural redundancy._ Observe that reducibility conditions (i)-(iii) apply to structural redundancies that are generic to every feed-forward layer within any architecture (units with zero, constant, or proportional output). Unit negation symmetries are characteristic of odd nonlinearities only, but other nonlinearities will exhibit their own affine symmetries that would play a similar role. In more sophisticated architectures, these basic sources of structural redundancy will sit alongside other sources such as interactions between layers, attention blocks, layer normalisation, and so on.

_Canonicalisation._ It follows that Algorithm 4.1 serves as a partial canonicalisation for more sophisticated architectures (with small modifications for alternative nonlinearities). Future works need only find and remove the _additional_ kinds of structural redundancy.

_Connectivity._ Similarly, additional sources of redundancy expand the functional equivalence class. While global connectivity properties may be lost in this process, individual paths will not be disturbed. We expect that our high-level qualitative finding will hold: that the more reducible a parameter is, the more intricately connected it is to functionally equivalent parameters.

Towards approximate structural redundancy.Our framework is built around the definition of functional equivalence, which requires exact equality of functions for all inputs. A more pragmatic definition would concern _approximate_ equality of functions for _relevant_ inputs. One step in this direction is studying proximity in parameter space to low-rank parameters, though detecting such proximity precisely is formally intractable (Farrugia-Roberts, 2023).

Functional equivalence and the loss landscape.Functionally equivalent parameters have equal loss. Continuous directions and piecewise linear paths within reducible functional equivalence classes therefore imply flat directions and equal-loss paths in the loss landscape. More broadly, the set of low- or zero-loss parameters is a union of functional equivalence classes. If some (very) reducible parameters obtain low loss (such as in the overparameterised setting) then the set of low-loss parameters contains rich functional equivalence classes as subsets.

Of course, having the same loss does not require functional equivalence. Indeed, Garipov et al. (2018) observe functional non-equivalence in low-loss paths. The exact relevance of reducible parameters to these topics remains to be clarified. Of special interest is the connection to theoretical work involving unit pruning (Kuditipudi et al., 2019) and permutation symmetries (Brea et al., 2019).

Canonicalisation and model compression.Our canonicalisation algorithm transforms a neural network parameter into another parameter that has the same input-output behaviour but effectively uses fewer units. This size reduction is not fundamental to the goals of canonicalisation (we could still achieve canonicalisation by producing a standard, maximally dense representation of each equivalent parameter). Nevertheless, Algorithm 4.1 performs (lossless) model compression as a side-effect (cf. Farrugia-Roberts, 2023).

In particular, our canonicalisation algorithm is reminiscent of a unit-based pruning technique (cf., e.g., Hoefler et al., 2021). However, there are a few salient differences. First, pruning algorithms are usually not designed to preserve exact functional equivalence on all inputs, but rather to accept small changes in outputs (on specific inputs). Second, unit-based pruning techniques usually select individual units for removal, and then continue training the network. Our canonicalisation algorithm instead considers operations that simultaneously remove a unit and perform a systematic adjustment to the remaining weights and biases to maintain the network's behaviour without any training.

Of interest, Casper et al. (2021) empirically studied a network pruning that finds units with weak outputs or pairs of units with correlated outputs, and then _eliminates_ or _merges_ these units and makes appropriate adjustments to approximately maintain performance. The elimination and merging operations bear a striking resemblance to the operations in Algorithm 4.1.

Reducible parameters and singular learning theory.When a reducible parameter is a critical point of the loss landscape, it is necessarily a _degenerate_ critical point (due to the continuum of equal-loss equivalent parameters nearby; or cf. Fukumizu, 1996). This places situations where learning encounters (or even approaches) reducible parameters within the domain of singular learning theory (cf. Watanabe, 2009; Wei et al., 2022).

The nature of the degeneracy depends on the exact ways in which it is reducible. For example, is the parameter at the intersection of multiple of the manifolds comprising the functional equivalence class (cf. Theorem 5.2)? The order of variation in directions away from the functional equivalence class also plays a role (cf. Lau et al., 2023). Future work could analyse the structural redundancy to find principled bounds on effective dimensionality.

## 8 Conclusion

Reducible parameters exhibit structural redundancy, in that the same input-output function could be implemented with a smaller network. While reducible parameters comprise a measure zero subset of the parameter space, their functional equivalence classes are much richer than those of irreducible parameters. Understanding these rich functional equivalence classes is important to understanding the nature of the loss landscape within which deep learning takes place.

We have taken the first step towards understanding functional equivalence beyond irreducible parameters by accounting for various kinds of structural redundancy in the setting of single-hidden-layer hyperbolic tangent networks. We offer an algorithmic characterisation of reducible functional equivalence classes and an investigation of their piecewise linear connectivity properties. We find in particular that the more redundancy is present in a parameter, the more intricately connected is its functional equivalence class.

We call for future work to seek out, catalogue, and thoroughly investigate sources of structural redundancy in more sophisticated neural network architectures; and to further investigate the role these parameters play in deep learning.