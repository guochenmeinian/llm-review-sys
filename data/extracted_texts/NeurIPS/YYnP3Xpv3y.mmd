# Learning Neural Contracting Dynamics: Extended Linearization and Global Guarantees

Sean Jaffe\({}^{1,2}\)

Alexander Davydov\({}^{1}\)

Deniz Lapsekili\({}^{2}\)

Ambuj K. Singh\({}^{2}\)

Francesco Bullo\({}^{1}\)

\({}^{1}\) Center for Control, Dynamical Systems and Computation, University of California, Santa Barbara.

\({}^{2}\) Department of Computer Science, University of California, Santa Barbara.

Corresponding author: sjaffe@ucsb.edu

###### Abstract

Global stability and robustness guarantees in learned dynamical systems are essential to ensure well-behavedness of the systems in the face of uncertainty. We present Extended Linearized Contracting Dynamics (ELCD), the first neural network-based dynamical system with global contractivity guarantees in arbitrary metrics. The key feature of ELCD is a parametrization of the extended linearization of the nonlinear vector field. In its most basic form, ELCD is guaranteed to be (i) globally exponentially stable, (ii) equilibrium contracting, and (iii) globally contracting with respect to some metric. To allow for contraction with respect to more general metrics in the data space, we train diffeomorphisms between the data space and a latent space and enforce contractivity in the latent space, which ensures global contractivity in the data space. We demonstrate the performance of ELCD on the high dimensional LASA, multi-link pendulum, and Rosenbrock datasets.

## 1 Introduction

Due to their representation power, deep neural networks have become a popular candidate for modeling continuous-time dynamical systems of the form

\[==f(x),\] (1)

where \(f(x)\) is an unknown (autonomous) vector field governing a dynamical process. Beyond approximating the vector field \(f\), it is desirable to ensure that the learned vector field is well-behaved. In many robotic tasks like grasping and navigation, a well-behaved system should always reach a fixed endpoint. Ideally, a learned system will still stably reach the desired endpoint even when pushed away from demonstration trajectories. Additionally, the learned system should robustly reach the desired endpoint in the face of uncertainty. In tasks such as manufacturing, animation, and human-robot interaction, functionality and safety require the learned system must smoothly follow a specific trajectory to its target.

To enforce stability guarantees, a popular approach has been to ensure that the learned dynamics admit a unique equilibrium point and have a Lyapunov function, e.g. . While popular, approaches based on Lyapunov functions typically struggle to provide general robustness guarantees since global asymptotic stability does not ensure robustness guarantees in the presence of disturbances. Indeed, input-to-state stability of global asymptotically stable dynamical systems needs to be separately established, e.g., see Chapter 5 in .

To ensure robustness and to allow for smooth trajectory following, there has been increased interest in learning contracting dynamics from data . A dynamical system is said to be contracting if anytwo trajectories converge to one another exponentially quickly with respect to some metric . If a learned contracting system that admits a demonstration trajectory is pushed off that trajectory, it will follow a new trajectory that exponentially converges to the demonstration trajectory. Additionally, if a system is contracting, it is exponentially incrementally input-to-state stable . If the system is autonomous, it also admits a unique equilibrium, is input-to-state stable, and has two Lyapunov functions that establish exponential stability of the equilibrium . Since establishing contractivity globally requires satisfying a matrix partial differential inequality everywhere, prior works have focused on establishing contractivity in the neighborhood of training data [35; 36].

### Related Works

**Stable, but not necessarily contracting dynamics.** Numerous works have aimed to learn stable dynamical systems from data including [7; 15; 22; 30; 32; 40; 45]. In , the authors introduce the Stable Estimator of Dynamical Systems (SEDS), which leverages a Gaussian mixture model to approximate the dynamics and enforce asymptotic stability via constraints on learnable parameters. In , the authors jointly learn the dynamics and a Lyapunov function for the system and project the dynamics onto the set of dynamics which enforce an exponentially decay of the Lyapunov function. This projection is done in closed-form and establishes global exponential convergence of the dynamics to the origin. In , the authors introduce Imitation Flow where trajectories are mapped to a latent space where states evolve in time according to a stable SDE. In , Euclideanizing Flows is introduced where the latent dynamics are enforced to follow natural gradient dynamics . A similar approach is taken in  where additionally collision avoidance is considered.

**Existing works on learning contracting dynamics.** Learning contracting vector fields from demonstrations has attracted attention due to robustness guarantees [6; 33; 35; 37; 39]. In , the dynamics are defined using a Gaussian mixture model and the contraction metric is parametrized to be a symmetric matrix of polynomial functions. Convergence to an equilibrium is established using partial contraction theory . In , the dynamics are defined via an optimization problem over a reproducing kernel Hilbert space; the dynamics are constrained to be locally contracting around the data points, i.e., there may be points in state space where the dynamics are not contracting. In  and , the authors study controlled dynamical systems of the form \(=f(x,u)\), where \(u\) is a control input and train neural networks to find a feedback controller such that the closed-loop dynamics are approximately contracting, i.e., contractivity is not enforced but instead lack of contractivity is penalized in the cost function.

Recent work, , has proposed a Neural Contractive Dynamical System (NCDS) which learns a dynamical system which is explicitly contracting to a trajectory. NCDS parametrizes contracting vector fields by learning symmetric Jacobians and performing line integrals to evaluate the underlying vector field. NCDS is computationally costly because of this integration. Additionally, the Jacobian parametrization used is overly restrictive, because not all contracting vector fields have symmetric Jacobians. Constraining the vector field to have symmetric Jacobian is equivalent to enforcing that the vector field is a negative gradient flow and contracting with respect to the identity metric . For scalability to higher dimensions, NCDS leverages a latent space structure where an encoder maps the data space to a lower-dimensional space and enforces contracting dynamics in this latent space. Then a decoder "projects" the latent-space dynamics to the full data space. It is then argued that on the submanifold defined by the image of the decoder, the dynamics are contracting.

### Contributions

In this paper, we present a novel model for learning deep dynamics with global contraction guarantees. We refer to this model as Extended Linearized Contracting Dynamics (ELCD). To the best of our knowledge, ELCD is the first model to ensure global contraction guarantees. To facilitate the development of this model, we provide a review of contracting dynamics and extended linearization of nonlinear dynamics. Leveraging extended linearization, we factorize our vector field as \(f(x)=A(x,x^{*})(x-x^{*})\), where \(x^{*}\) is the equilibrium of the dynamics. We enforce negative definiteness of the symmetric part of \(A(x,x^{*})\) everywhere and prove (i) global exponential stability of \(x^{*}\), (ii) equilibrium contractivity of our dynamics to \(x^{*}\), and (iii) using a converse contraction theorem, contractivity of the dynamics with respect to some metric.

Since negative definiteness of the symmetric part of \(A\) is not sufficient to capture all contracting dynamics, we introduce a latent space of equal dimension as the data space and learn diffeomorphisms between the data space and this latent space. The diffeomorphisms provide additional flexibility in the contraction metric and allow learning of arbitrary contracting dynamics compared to those which are solely equilibrium contracting.

Our example in Section 3.4 provides theoretical justification for why the diffeomorphism and the learned contracting model must be trained jointly. In summary, if the diffeomorphism is trained first, and transforms the data to a latent space, the model class may not be expressive enough to accurately represent the true latent dynamics. If the diffeomorphism and dynamics are trained simultaneously, this limitation is overcome. This is in contrast to  which trains the diffeomorphism independently as a variational autoencoder and then trains the model after on the transformed data.

Our model, ELCD, directly improves on NCDS in several ways. We parameterize the vector field directly instead of parametrizing its Jacobian. Doing so prevents us from needing to integrate the Jacobian to calculate the vector field and thus speeds up training and inference. ELCD is also more expressive than NCDS because it can represent vector fields with asymmetric Jacobians. Additionally, ELCD is guaranteed to be contracting to an arbitrary equilibrium point, either selected or learned, at all training steps. NCDS, in contrast, must learn the equilibrium point over the course of training. Additionally, NCDS learns an encoder and decoder for a lower-dimensional latent space and thus can only be contracting on the submanifold defined by the image of the decoder. From initial conditions that are not on this submanifold, NCDS may not exhibit contracting behavior. In contrast, since the latent space of ELCD is of the same dimension as the data space, we train diffeomorphisms that ensure global contractivity in the data space. ELCD exhibits better performance than NCDS at reproducing trajectories from the LASA , n-link Pendulum, and Rosenbrock datasets. We additionally compare against the Euclideanizing Flow , and Stable Deep Dynamics  models.

## 2 Preliminaries

We consider the problem of learning a dynamical system \(=f(x)\) using a neural network that ensures that the dynamics are contracting in some metric. Going forward, we denote by \(Df(x)=(x)\) the Jacobian of \(f\) evaluated at \(x\). To this end, we define what it means for a dynamical system to be contracting.

**Definition 1**.: _A contracting dynamical system is one for which any two trajectories converge exponentially quickly. From , for a continuously differentiable map \(f:^{d}^{d}\), the dynamical system \(=f(x)\) is contracting with rate \(c>0\) if there exists a continuously-differentiable matrix-valued map \(M:^{d}^{d d}\) and two constants \(a_{0},a_{1}>0\) such that for all \(x^{n}\), \(M(x)=M(x)^{}\) and \(a_{0}I_{d} M(x) a_{1}I_{d}\) and additionally satisfies for all \(x\)_

\[M(x)Df(x)+Df(x)^{}M(x)+(x)-2cM(x).\] (2)

The map \(M\) is called the _contraction metric_ and the notation \((x)\) is shorthand for the \(n n\) matrix whose \((i,j)\) entry is \((x)_{ij}= M_{ij}(x)^{}f(x)\). A central result in contraction theory is that any dynamical system \(=f(x)\) satisfying (2) has any two trajectories converging to one another exponentially quickly [27; 29; 39]. Specifically, there exists \(L 1\) such that any two trajectories \(x_{1},x_{2}\) of the dynamical system satisfy

\[\|x_{1}(t)-x_{2}(t)\|_{2} Le^{-ct}\|x_{1}(0)-x_{2}(0)\|.\] (3)

In other words, contractivity establishes exponential _incremental_ stability .

We note that any contracting dynamical system enjoys numerous useful properties including the existence of a unique exponentially stable equilibrium and exponential input-to-state stability when perturbed by a disturbance. We refer to  for more useful properties of contracting dynamical systems and  for a recent monograph on the subject.

The problem under consideration is as follows: given a set of demonstrations \(=\{(x_{i},_{i})\}_{i=1}^{n}\) consisting of a set of \(n\) state, \(x_{i}^{d}\), and velocity, \(_{i}^{d}\), pairs, we aim to learn a neural network \(f(x_{i})=_{i}\) that parametrizes a globally contracting dynamical system with equilibrium point \(x^{*}\), such that \(f(x^{*})=0\). In essence, this task requires learning both the vector field and the contraction metric, \(M\) such that they jointly satisfy (2).

### Contracting Linear Systems

Suppose we assumed that the dynamical system we aimed to learn was linear, i.e., \(=Ax\) for some matrix \(A^{d d}\) and we wanted to find a contraction metric \(M\) which we postulate to be constant \(M(x):=M\) for all \(x\). The contraction condition (2) then reads

\[M(A+cI_{n})+(A+cI_{n})^{}M 0,\] (4)

which implies that \(A\) has all eigenvalues with \(((A))-c\), see Theorem 8.2 in . In other words, for linear systems, contractivity is equivalent to stability.

Although the condition (4) is convex in \(M\) at fixed \(A\), the task of learning both \((A,M)\) simultaneously from data is not (jointly) convex. Instead, different methods must be employed such as alternating minimization. A similar argument is made in  in the context of stability and here we show that the same is true of contractivity.

### Contractivity and Exponential Stability for Nonlinear Systems

In the case of nonlinear systems, contractivity is not equivalent to asymptotic stability. Indeed, asymptotic stability requires finding a continuously differentiable Lyapunov function \(V:^{d}_{ 0}\) which is positive everywhere except at the equilibrium, \(x^{*}\), and satisfies the decay condition

\[ V(x)^{}f(x)<0, x^{d}\{x^{*}\}.\] (5)

One advantage of learning asymptotically stable dynamics compared to contracting ones is that the function \(V\) is scalar-valued, while the contraction metric \(M\) is matrix-valued. A disadvantage of learning asymptotically stable dynamics compared to contracting ones is that we are required to know the location of the equilibrium point beforehand and no robustness property of the learned dynamics is automatically enforced. To this end, there are works in the literature that enforce an equilibrium point at the origin and learn the dynamics and/or the Lyapunov function under this assumption . In the case of contractivity, existence and uniqueness of an equilibrium point is implied by the condition (2) and it can be directly parametrized to best suit the data.

## 3 Methods

### Motivation

The motivation for our approach comes from the well-known mean-value theorem for vector-valued mappings, which we highlight here.

**Lemma 2** (Mean-value theorem (Proposition 2.4.7 in )).: _Let \(f:^{d}^{d}\) be continuously differentiable. Then for every \(x,y^{d}\),_

\[f(x)-f(y)=_{0}^{1}Df( x+(1-)y)d(x-y).\] (6)

If \(y=x^{*}\) satisfies \(f(x^{*})=0\), then the continuously differentiable map \(f\) admits the factorization

\[f(x) =A(x,x^{*})(x-x^{*}),\] (7) \[A(x,x^{*}) =_{0}^{1}Df( x+(1-)x^{*})d.\] (8)

This factorization is referred to as _extended linearization_ in analogy to standard linearization, i.e., for \(x\) close to \(x^{*}\), \(f(x) Df(x^{*})(x-x^{*})\). We remark that when \(d 2\), extended linearization is not unique, and for a given \(f\), there may exist several \(A\) such that \(f(x)=A(x,x^{*})(x-x^{*})\). In other words, (8) showcases one valid choice for \(A\) such that this factorization holds. Indeed, this nonuniqueness has been leveraged in some prior works, e.g. Section 3.1.3 in , to yield less conservative contractivity conditions.

Since we know that a contracting vector field admits a unique equilibrium point, \(x^{*}\), we restrict our attention to learning a matrix-valued mapping \(A:^{d}^{d}^{d d}\) and ensuring that this mappinghas enough structure so that the overall vector field satisfies the contraction condition (2) for some contraction metric \(M\). This task is nontrivial since \(f(x)=A(x,x^{*})(x-x^{*})\) implies that

\[Df(x)=A(x,x^{*})+(x,x^{*})(x-x^{*}),\] (9)

where \(:^{d}^{d} ^{d d d}\) is a third-order tensor-valued mapping. In what follows, we will show that a more simple condition will imply contractivity of the vector field. Namely, negative definiteness of the symmetric part of \(A(x,x^{*})\) will be sufficient for contractivity of the dynamical system \(=A(x,x^{*})(x-x^{*})\).

### Parametrization of \(A(x,x^{*})\)

We show a simple example of our model, the Extended Linearized contracting Dynamics (ELCD). Let \(x^{d}\) be the state variable and \(f:^{d}^{d}\) be a vector field with equilibrium point \(x^{*}\). As indicated, we parametrize our vector field by its extended linearization

\[=f(x)=A(x,x^{*})(x-x^{*}),\] (10)

where now

\[A(x,x^{*}) =-P_{s}(x,x^{*})^{}P_{s}(x,x^{*})\] (11) \[+P_{a}(x,x^{*})-P_{a}(x,x^{*})^{}- I_{d}\]

\(P_{s},P_{a}:^{d}^{d}^{d d}\) are neural networks, \(I_{d}\) is the \(d\)-dimensional identity matrix, and \(>0\) is a constant scalar. Note that the symmetric part of \(A(x,x^{*})\) is negative definite since

\[)+A(x,x^{*})^{}}{2}=-P_{s}(x,x^{*})^{}P_{s}(x,x^{*})-  I_{d}\]

and \(P_{s}(x,x^{*})^{}P_{s}(x,x^{*})\) is guaranteed to be positive semidefinite.

We prove that a vector field parametrized this way is guaranteed to be (i) globally exponentially stable, (ii) equilibrium contracting as defined in , and (iii) contracting in some metric. The key tools are partial contraction theory  and a converse contraction theorem.

**Theorem 3** (Equilibrium Contraction and Global Exponential Stability).: _Suppose the dynamical system \(=f(x)\) is parametrized as \(f(x)=A(x,x^{*})(x-x^{*})\) where \(A(x)\) is as in (11). Then any trajectory \(x(t)\) of the dynamical system satisfies_

\[\|x(t)-x^{*}\|_{2} e^{- t}\|x(0)-x^{*}\|_{2}.\] (12)

Proof.: We use the method of partial contraction as in . Let \(x(t)\) be a solution to \((t)=A(x(t),x^{*})(x(t)-x^{*})\) with initial condition \(x(0)=x_{0}\). Then, define the time-varying virtual system

\[(t)=A(x(t),x^{*})(y(t)-x^{*}).\] (13)

We will establish that this virtual system is contracting in the identity metric, i.e., (2) is satisfied with \(M(x)=I_{d}\). We see that the Jacobian for this virtual system is simply \(A(x(t),x^{*})\) and

\[A(x(t),x^{*})+A(x(t),x^{*})^{} =-2P_{s}(x(t),x^{*})^{}P_{s}(x(t),x^{*})-2 I_{d}\] \[-2 I_{d}.\]

In other words, in view of (3), and since \(M(x)=I_{d}\), any two solution trajectories \(y_{1}(t)\) and \(y_{2}(t)\) of the virtual system satisfy

\[\|y_{1}(t)-y_{2}(t)\|_{2} e^{- t}\|y_{1}(0)-y_{2}(0)\|_{2}.\]

Note that we can pick one trajectory to be \(y_{1}(t)=x^{*}\) for all \(t\) and we can pick \(y_{2}(t)=x(t)\). Since \(x_{0}\) was arbitrary, this argument establishes the claim. 

Clearly, the bound (12) implies exponential convergence of trajectories of the dynamical system (10) to \(x^{*}\). Moreover, this bound exactly characterizes equilibrium contractivity, as was defined in . Notably, using the language of logarithmic norms, Theorem 33 in  establishes a similar result to Theorem 3 without invoking a virtual system.

Note that although Theorem 3 establishes global exponential stability and equilibrium contraction, it does not establish global contractivity. Indeed, the contractivity condition (2) is not guaranteed to hold with \(M(x)=I_{d}\) without further assumptions on the structure of \(A(x,x^{*})\) in (10). Remarkably, however, due to a converse contraction theorem of Giesl, Hafstein, and Mehrabinezhad, it turns out that one can construct a state-dependent \(M\) such that the dynamics are contracting. Specifically, since trajectories of the dynamical system satisfy the bound (12), for any matrix-valued mapping \(C:^{d}^{d d}\) with \(C(x)=C(x)^{} 0\) for all \(x\), the matrix PDE

\[M(x)Df(x)+Df(x)^{}M(x)+(x)=-C(x)\] (14)

admits a unique solution for each \(x\). In other words, the unique \(M\) solving this matrix PDE serves as the contraction metric and will satisfy (2) with suitable choice for \(c\). The following proposition provides the explicit solution for \(M\) in terms of the solution to the matrix PDE.

**Proposition 4** (Theorem 2.8 in ).: _Let \(C:^{d}^{d d}\) be smooth and have \(C(x)=C(x)^{}\) be a positive definite matrix at each \(x\). Then \(M:^{d}^{d d}\) given by the formula_

\[M(x)=_{0}^{}(,x)^{}C((,x))(,x)d,\] (15)

_is a contraction metric for the dynamical system (10) on any compact subset containing \(x^{*}\), where \((,x)\) is the solution to the ODE with \((0,x)=x\) and \((,x)\) is the matrix-valued solution to_

\[=Df((t,x))Y, Y(0)=I_{d}.\] (16)

While it is challenging, in general, to compute the metric (15), numerical considerations for approximating it arbitrarily closely are presented in . In practice, one would select \(C(x)=I_{d}\) and evaluate all integrals numerically. For ELCD, unless one directly needs to know the contraction metric for application purposes, it is not required to compute the contraction metric at any point during either training or inference.

We remark that our parametrization for \(A\) in (11) is similar to the parametrization for the Jacobian of \(f\) that was presented in . There are however a few key differences. Notably, \(A(x,x^{*})\) may be asymmetric while the Jacobian in equation (3) in  is always symmetric. Notably, since the Jacobian in  is symmetric and negative definite, the vector field \(=f(x)\) is a negative gradient flow, \(=- V(x)\), for some strongly convex function \(V\). On the contrary, our dynamics (10) can exhibit richer behaviors than negative gradient flows in view of the asymmetry in \(A(x,x^{*})\). Additionally, since the dynamics in  with their parametrization can be represented as \(=- V(x)\) for some strongly convex \(V\), it is guaranteed to be contracting in the identity metric, \(M(x)=I_{d}\). On the other hand, our dynamics (10) is not necessarily contracting in the identity metric and instead is contracting in a more complex metric given in (15).

### Latent Space Representation

Realistic dynamical systems and their flows including the handwritten trajectories found in the LASA dataset, are often highly-nonlinear and may not be represented in the form (10) or obey the bound (12). One solution to these challenges is to transform the system to a latent, possible lower-dimensional, space and learn an ELCD in the latent space.

Latent space learning is possible because contraction is invariant under differential coordinate changes. From Theorem 2 of , given a dynamical system \(=f(x)\), \(f:^{d}^{d}\), with \(f\) satisfying (1), the system will also be contracting under the coordinate change \(z=(x)\) if \(:^{d}^{d}\) is a smooth diffeomorphism. Specifically, if \(=f(x)\) is contracting with metric \(M\), then the system that evolves \(z\) is contracting as well with metric given by \((z)=D(z)^{-}M(z)D(z)^{-1}\), where \(z=(x)\) and \(D\) is the Jacobian of the coordinate transform. In other words, We can learn vector fields that are contracting in an arbitrary metric by training a vector field \(f\) which is parametrized as (10) and using a coordinate transform \(\).

NCDS , treats the coordinate transform as a Variational Autoencoder (VAE) . Their training procedure consists of two steps: first training the coordinate transform with VAE training, then training the function \(f\) in the new learned coordinates.

### On the Interdependence of Diffeomorphism and Learned Dynamics

To demonstrate the need for a coordinate transform, we consider the task of learning a vector field that fits trajectories generated by the linear system \(=Ax\) with \(A=-1&4\\ 0&-1.\) Clearly, this linear system cannot be represented in the form (10) with parametrization (11). To see this fact, we observe that \(A+A^{}\) is not negative definite and thus no choice of \(P_{s}\) or \(P_{a}\) can represent this linear system. To remedy this issue, one can take the linear coordinate transform \(z=(x)=Px\), where \(P=1&0\\ 0&4\). Then the \(z\)-dynamics read

\[=PAP^{-1}z=-1&1\\ 0&-1z\] (17)

and now the symmetric part of \(PAP^{-1}\) is negative definite and thus we can find suitable choices of \(P_{s},P_{a}\). Specifically, we can take \((0,1/2)\), let \(P_{a}(z)=0&0\\ 1/2&0z\), and let \(P_{s}(z)=Qz\) where \(Q\) is the matrix square root of \(1&-1/2\\ -1/2&1- I_{2}\). Note that in this toy example, asymmetry is essential to exactly represent these dynamics in the latent space, \(z\). If we used the parametrization in , we would not be able to represent these dynamics since they have an asymmetric Jacobian. We demonstrate this routine in Figure (1). We generate two trajectories starting at \((0,2)\) and \((0,-2)\), and use that data to train two ELCDs, one without and one with a learned, linear transform. Figure 1(a) shows the vector field and trajectories corresponding to an ELCD with no transform. The trajectories are forced to the center sooner than the actual data as a consequence of the bound (12). Learning a coordinate transform allows the ELCD to learn a system that is contracting in an arbitrary metric and exhibit overshoot. This is demonstrated by the learned system in Figure 1(b), which perfectly matches the data.

### Choice of Diffeomorphism

There are several popular neural network diffeomorphisms including coupling layers [12; 32], normalizing flows , \(\)-flow [6; 8], spline flows , and radial basis functions .

A coupling layer \(:^{d}^{d}\) consists of a neural network \(:^{k}^{N}\) with \(1<k<d\) and a neural network \(g:^{N}\). The transform \(\) maps input \(x^{d}\) to \(y^{d}\) with the following procedure:

1. Set \(y_{i}=x_{i}\) for \(1 i k\) for some \(1<k<d\).
2. Set \(y_{i}=g(x_{1:k},x_{i})\) for \(k i d\)

Coupling layers are invertible by doing the above process in reverse, so long as \(g\) is invertible. A coupling layer can exhibit a wide variety of behaviors depending on the choice of \(g\).

Polynomial spline curves are another diffeomorphism that are commonly used as \(g\) in coupling layers. A polynomial spline has a restricted domain which is divided into bins. A different linear, quadratic

Figure 1: The learned vector field and corresponding trajectories of an ELCD with no transform (left) and a model with a transform (right) when trained on data that is generated from a vector field that is contracting in a more general metric.

, or cubic  polynomial covers each bin.  introduce rational-quadratic splines, which constructs a spline from functions that are the quotient of two quadratic polynomials.

In practice, \(\) is the composition of several of transforms. Because the coupling transform only alters some coordinates, they are often used in conjunction with random permutations. If the latent space dimension is smaller than the data space dimension  makes the last composite function of \(\) an 'Unpad' function, which simply removes dimensions to match the latent space. The decoder \(\) is then prepended by a 'Pad' function, which concatenates the latent variable with the necessary number of 'zeros' to match the data space.

Of course, if the latent dimension is smaller than that of the data dimension, then \(\) can not be bijective. However,  argue that as long as \(\) is injective and has range over the dataset, then \(f\) being contracting in the latent space implies that the learned dynamics are contracting on the submanifold defined by the image of \(^{-1}\). In other words, NCDS cannot be globally contracting. The consequences of this are shown in Figure (2). While NCDS can learn to admit an equilibrium point when on the submanifold, trajectories that fall off the submanifold may diverge. ELCD, in contrast, is contracting to the correct equilibrium point during all phases of training.

### Training

Our task is to simultaneously learn the contracting system \(f(x)\) and the metric in which \(f\) contracts. As previously discussed, the metric is implicitly determined by \((x)\).  uses a two-step method. First, they treat \(\) as a variational autoencoder and maximize the evidence lower bound (ELBO). They then fix the encoder and train the model to evolve the state in latent space. Note that the VAE objective is to make the encoded data to match a standard-normal distribution. Notably, VAE training does not encourage the encoder to transform the data to space in which the data corresponds to trajectories of a contracting system. If the data is not contracting in the transformed space, a contracting model will not be able to fully fit the data. This explains why, in our implementation of the two step-training, we are unable to reasonably learn the data. For further comparison with NCDS, we will train the encoder and model jointly.

## 4 Experiments

The code for our model and data can be found here: https://github.com/seanjaffel/Extended-Linearized-Contracting-Dynamics. For all datasets we compare our method against NCDS , Stable Deep Dynamics (SDD)  and Euclideanizing Flow (EFlow) . See A.2 for a detailed discussion of the methods. We report the dynamic time warping distance (DTWD)  (see A.1) and standard deviation between predicted and data trajectories. See A.5 for model implementation details. See also the Appendix for more details on these models.

Figure 2: Plots of the vector fields and induced trajectories learned by ELCD (Top) and NCDS (Bottom) after different training epochs. ELCD is contracting always while NCDS may admit multiple equilibrium or diverge.

### Datasets

We experiment with the LASA dataset , which consists of 30, two-dimensional curves. We use three demonstration trajectories of each curve. As in , the first few initial points of each trajectory are omitted so that only the target state has zero velocity. we stack two and four LASA trajectories together to make datasets of 4 and 8-dimensional trajectories, respectively. All data is standardized to have a mean of zero and variance of one. We use in total 10 2D curves, 6 4D curves, and 6 8D curves. Some 2D-LASA trajectories and their respective trained ELCD trajectories and vector fields are visualized in Figure (3).

We also experiment with simulated datasets. We simulate 6 trajectories of a 2,4, and 8-link pendulum (4D, 8D, and 16D respectively) each and 4 trajectories of 8D and 16D Riemannian gradient descent dynamics on a generalization of the Rosenbrock function (see Appendix A.4). Each model is trained on all trajectories of the same dimension, and then predictions are made starting from every initial point.

### Results

Table 1 presents our results. ELCD performs the best in all tasks. These results shows the benefit of the increased expressiveness allowed by the skew symmetric component of our parametrization. These benefits are more apparent in the pendulum dataset. The oscillatory behavior of the pendulum dynamics is a product of complex eigenvalues in its Jacobian. Our model's skew symmetric component is what enables it to learn the pendulum dynamics so well. A sample of demonstration and learned pendulum trajectories is shown in figure (4).

The Rosenbrock dynamics are stiff and difficult to learn. In our training of SDD, we observed lack of stability and convergence, resulting in very large DTWDs. Hence, we report NaN for the results of the SDD models on the Rosenbrock dataset. While our model performs the best, there is still room for improvement. Handling such dynamics with multiple time scales is still an open challenge.

   & SDD & EFlow & NCDS & ELCD \\  LASA-2D & 0.37 \(\) 0.32 & 1.05 \(\) 0.25 & 0.59 \(\) 0.61 & **0.12 \(\)0.11** \\ LASA-4D & 2.49 \(\) 2.4 & 2.24 \(\) 0.12 & 2.19 \(\) 1.23 & **0.80 \(\) 0.54** \\ LASA-8D & 5.26 \(\) 0.50 & 2.66 \(\) 0.63 & 5.04 \(\) 0.77 & **1.52 \(\) 0.61** \\  Pendulum-4D & 0.49 \(\) 0.11 & 0.17 \(\) 0.01 & 1.35 \(\) 2.26 & **0.03 \(\) 0.01** \\ Pendulum-8D & 0.75 \(\) 0.08 & 0.33 \(\) 0.01 & 2.88 \(\) 0.69 & **0.14 \(\) 0.03** \\ Pendulum-16D & 1.86 \(\) 0.14 & 0.45 \(\) 0.01 & 1.65 \(\) 0.31 & **0.44 \(\) 0.09** \\  Rosenbrock-8D & NaN & 1.90 \(\) 0.16 & 2.74 \(\) 0.15 & **1.22 \(\) 0.01** \\ Rosenbrock-16D & NaN & 3.57 \(\) 0.66 & 3.68 \(\) 0.12 & **2.57 \(\) 0.09** \\  

Table 1: Mean DTWD \(\) one standard deviation across 10 runs on LASA, multi-link pendulum, and Rosenbrock datasets

Figure 3: Plots of the 2D LASA data. Demonstrations are in black. The learned ELCD trajectories in magenta are plotted along with the learned vector field. The vector field velocities have been normalized for visibility.

## 5 Limitations

Our method assumes that the underlying dynamics of the data trajectories are contracting. However, the diffeomorphism allows for a wide class of stable systems to be represented. Our method is currently implemented for trajectories that converge to the same fixed point. But, this can be overcome by manually changing the fixed point, depending on which trajectory the input data came from. Also, right now our method is limited to dynamics in \(^{n}\), but we imagine that an extension to more general manifolds should be possible.

As we are motivated by applications in imitation learning and robotics, in this work, we are primarily interested in problems where the underlying systems should be robustly stable, especially away from training data. For this reason, we focus on learning dynamics which are guaranteed to be globally contracting and do not address learning other types of systems, such as those with multiple fixed points, limit cycles, or chaotic attractors. We imagine that extensions of ELCD could capture these richer dynamical behaviors by leveraging weaker notions of contraction including local contraction (i.e., contractivity in the region of attraction of a stable equilibrium), \(k\)-contraction (contraction of \(k\)-dimensional bodies) , transverse contraction  and contraction in the Hausdorff dimension . Moreover, the notion of translation invariance mentioned could be studied using semicontraction theory , i.e., contraction to a subspace.

## 6 Conclusions

In this paper, we introduce ELCD, the first neural network-based dynamical system with global contractivity guarantees in arbitrary metrics. The main theoretical tools are extended linearization, equilibrium contraction, and a converse contraction theorem. To allow for contraction with respect to more general metrics, we use a latent space representation with dimension of the latent space equal to the dimension of the data space and train diffeomorphisms between these spaces. We highlight key advantages of ELCD compared to NCDS as introduced in , including global contraction guarantees, expressible parametrization, and efficient inference. We demonstrate the performance of ELCD on high-dimensional LASA datasets and simulated multi-link pendulum and Rosenbrock dynamics. Our method shows consistent performance across all datasets.