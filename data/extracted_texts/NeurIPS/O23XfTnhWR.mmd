# Graphcode: Learning from multiparameter persistent homology using graph neural networks

Michael Kerber

Institute of Geometry

Graz University of Technology

kerber@tugraz.at &Florian Russold

Institute of Geometry

Graz University of Technology

russold@tugraz.at

Equal contribution.

###### Abstract

We introduce graphcodes, a novel multi-scale summary of the topological properties of a dataset that is based on the well-established theory of persistent homology. Graphcodes handle datasets that are filtered along two real-valued scale parameters. Such multi-parameter topological summaries are usually based on complicated theoretical foundations and difficult to compute; in contrast, graphcodes yield an informative and interpretable summary and can be computed as efficient as one-parameter summaries. Moreover, a graphcode is simply an embedded graph and can therefore be readily integrated in machine learning pipelines using graph neural networks. We describe such a pipeline and demonstrate that graphcodes achieve better classification accuracy than state-of-the-art approaches on various datasets.

## 1 Introduction

A quote attributed to Gunnar Carlsson says "Data has shape and shape has meaning". Topological data analysis (TDA) is concerned with studying the shape, or more precisely the topological and geometric properties of data. One of the most prominent tools to quantify and extract topological and geometric information from a dataset is persistent homology. The idea is to represent a dataset on multiple scales through a nested sequence of spaces, usually simplicial complexes for computations, and to measure how topological features like connected components, holes or voids appear and disappear when traversing that nested sequence. This information can succinctly be represented through a _barcode_, or equivalently a _persistence diagram_, which capture for every topological feature its lifetime along the scale axis. Persistent homology has been successfully applied in a wealth of application areas , often in combination with Machine Learning methods - see the recent survey  for a comprehensive overview.

A shortcoming of classical persistent homology is that it is bound to a single parameter, whereas data often is represented along several independent scale axes (e.g., think of RGB images which have three color channels along which the image can be considered). To get a barcode, one is forced to chose fixed scales for all but one scale parameters. The extension to _multi-parameter persistent homology_ avoids to make such choices. Similar to the one-parameter setup, the data is represented in a nested multi-dimensional grid of spaces and the evolution of topological features in this grid is analyzed. Unfortunately, a succinct representation as a barcode is not possible in this extension, which makes the theory and algorithmic treatment more involved. Nevertheless, the quest of how to use informative summaries in multi-parameter persistence is an active field of contemporary research. A common theme in this context is _vectorization_, meaning that some (partial) topological information is extracted from the dataset and transformed into a high-dimensional vector suitable for machine learning pipelines.

[MISSING_PAGE_EMPTY:2]

persistence landscapes , persistence images , or scale-space kernels . These vectorization methods for one-parameter persistence modules have been generalized in various forms to the two-parameter case [10; 16; 18; 26; 35]. The difference in the two-parameter case is that the vectorizations are not based on a complete invariant like the persistence diagram but on weaker invariants like the rank-invariant, generalized rank-invariant or the signed barcode. Hence, these vectorizations capture the persistent homology only partially. Moreover, even this partial information is often times computationally expensive. In contrast, our method avoids to compute a direct vectorization, although we point out that a vectorization is implicitly computed eventually within the graph neural network architecture. To our knowledge there exists no other method that allows to feed a complete representation of two-parameter persistent homology into a machine learning pipeline.

Our approach also resembles persistent vineyards  in the sense that a two-parameter process is considered as a dynamic \(1\)-parameter process and the evolution of persistence diagrams is analyzed. Indeed, vineyards produce a layered graph of persistence diagrams just as graphcodes (see Fig VIII.6 in ), but they operate in a different setting where the simplicial complex is fixed throughout the process and only the order of simplices changes, whereas graphcodes rely on bifiltered simplicial complex data that only increases along both axis. Most standard constructions of multi-parameter persistence yield such a bifiltered complex and graphcodes are more applicable in this situation.

Generating bifiltered simplicial complexes out of point cloud data is computationally expensive and an active area of research. In the context of the aforementioned two-dimensional point clouds that we analyze with graphcodes, we heavily rely on _sublevel Delaunay bifiltrations_ which were introduced very recently by Alonso et al. . That algorithm (and its implementation) render the two-parameter analysis of such point clouds possible in machine learning contexts, partially explaining why previous methods have only tested their approaches on very small point cloud data, if at all.

Outline.We review basic notions of persistent homology in Section 2 and define graphcodes in Section 3 based on these definitions. We decided for a "down-to-earth" approach, defining graphcodes in terms of cycle bases in simplicial complexes to keep concepts concrete and relatable to geometric constructions for the benefit of readers that are not too familiar with the algebraic foundations of persistent homology. Moreover, this treatment simplifies the algorithmic description to compute graphcodes in Section 4. We explain the machine learning architecture based on graphcodes in Section 5 and report on our experimental results in Section 6. We conclude in Section 7.

## 2 Persistent homology

We will use the following standard notions from simplicial homology. For readers not familar with these concepts, we provide a summary in Appendix A. For a more in-depth treatment see, for instance, the textbook by Edelsbrunner and Harer .

For an (abstract) simplicial complex \(K\) and \(p 0\) an integer, let \(C_{p}(K)\) denote its \(p\)-th chain group with \(_{2}\) coefficients (which is, in fact, a vector space) and \(_{p}:C_{p}(K) C_{p-1}(K)\) the boundary map (see also Figure 2 (left)). Let \(Z_{p}(K)\) be the kernel of \(_{p}\). Elements of \(Z_{p}(K)\) are called \(p\)-cycles. \(B_{p}(K)\) is the image of \(_{p+1}\), and its elements are called \(p\)-boundaries. The quotient \(Z_{p}(K)/B_{p}(K)\) is called the \(p\)-th homology group \(H_{p}(K)\), whose elements are denoted by \([]_{K}\) with \(\) a \(p\)-cycle, or just \([]\) if the underlying complex is clear from context. See Figure 2 (right) for an illustration of these concepts.

We call a basis \((z_{1},,z_{m})\) of \(Z_{p}(K)\)_(boundary-)consistent_ if there exists some \( 0\) such that \((z_{1},,z_{})\) is a basis of \(B_{p}(K)\). In this case, \([z_{+1}],,[z_{m}]\) is a basis of \(H_{p}(K)\). A consistent basis can be obtained by first determining a basis of \(B_{p}(K)\) and completing it to a basis of \(Z_{p}(K)\). Clearly, \(Z_{p}(K)\) usually has many consistent bases.

Homology maps.Let \(L K\) be a subcomplex. The inclusion map from \(L\) to \(K\) maps \(p\)-cycles of \(L\) to \(p\)-cycles of \(K\) and \(p\)-boundaries of \(L\) to \(p\)-boundaries of \(K\). It follows that for every \(p\), the map \(i_{*}:H_{p}(L) H_{p}(K),[]_{L}[]_{K}\) is a well-defined map between homology groups. This map is a major object of study in topological data analysis, as it contains the information how the topological features (i.e., the homology classes) evolve when we embed them on a larger scale (i.e., a larger complex).

Being a linear map, \(i_{*}\) can be represented as a matrix with \(_{2}\)-coefficients. It is instructive to think about how to create this matrix: Assuming having fixed consistent bases \(A_{L}\) for \(Z_{p}(L)\) and \(A_{K}\) for \(Z_{p}(K)\) and considering a basis element \([]_{L}\) of \(H_{p}(L)\), we write the \(p\)-cycle \(\) as a linear combination with respect to the basis \(A_{K}\). We can ignore summands that correspond to basis elements of \(B_{p}(K)\), and the remaining entries yield the image of \([]_{L}\) in \(H_{p}(K)\), and thus one column of the matrix of \(i_{*}\).

Alternatively, \(i_{*}\) takes a diagonal form when picking suitable consistent bases \(A_{L}\) and \(A_{K}\). We can do that as follows: We start with a basis \(A^{}\) of \(Z_{p}(L) B_{p}(K)\), the set of \(p\)-cycles in \(L\) that become "trivial" when included in \(K\). This subspace contains \(B_{p}(L)\) and we can choose \(A^{}\) such that it starts with a basis of \(B_{p}(L)\), followed by other vectors. Now we extend \(A^{}\) to a basis \(A_{L}\) of \(Z_{p}(L)\) which is consistent by the choice of \(A^{}\). Since \(Z_{p}(L)\) is a subspace of \(Z_{p}(K)\), we can extend \(A_{L}\) to a basis \(A_{K}\) of \(Z_{p}(K)\). Importantly, we can do so such that \(A_{K}\) is consistent, because the sub-basis \(A^{}\) maps to \(B_{p}(K)\) and \(A_{L} A^{}\) does not, so we can first extend \(A^{}\) to a basis of \(B_{p}(K)\), ensuring consistency, and then extend to a full basis of \(Z_{p}(K)\). In this way, considering a homology class \([]\) with \(\) a basis element of \(A_{L}\), \(\) is also a basis element of \(A_{K}\), and the map \(i_{*}\) indeed takes diagonal form.

Filtrations and barcodes.A _filtration_ of a simplicial complex \(K\) is a nested sequence \(K_{1} K_{2} K_{n}=K\) of subcomplexes of \(K\). Applying homology, we obtain vector spaces \(H_{p}(K_{i})\) and linear maps \(H_{p}(K_{i}) H_{p}(K_{j})\) whenever \(i j\). This data is called a _persistence module_. For simplicity, we assume that \(H_{p}(K)\) is the trivial vector space, implying that all \(p\)-cycles eventually become \(p\)-boundaries.

It is possible to iterate the above construction for a single inclusion to obtain consistent bases \(A_{i}\) for \(Z_{p}(K_{i})\) such that all maps \(H_{p}(K_{i}) H_{p}(K_{j})\) have diagonal form. Equivalently, there is one basis of \(Z_{p}(K)\) that contains consistent bases for all subcomplexes \(K_{1},,K_{n}\). To make this precise, we first observe that for every \( Z_{p}(K)\), there is a minimal \(i\) such that \( Z_{p}(K_{})\) for all \( i\). This is called the _birth index_ of \(\). Moreover, there is a minimal \(j\) such that \( B_{p}(K_{})\) for all \( j\). This is called the _death index_ of \(\). By our simplifying assumptions that \(H_{p}(K)\) is trivial, every \(\) indeed has a well-defined finite death index. The half-open interval \([i,j)\) consisting of birth and death index is called the _bar_ of \(\). We say that \(\) is _already born_ at index \(\) if its birth index is at most \(\). We call \(\)_alive_ at \(\) if alpha is already born at \(\) and \(\) is strictly smaller than the death index of \(\); otherwise we call it _dead_ at \(\).

**Definition 2.1**.: For a filtration of \(K\) as above, a _barcode basis_ is a basis \(A\) of \(Z_{p}(K)\) where each basis element is attached its bar such that the following property holds: For every \(i\), the elements of \(A\) dead at \(i\) form a basis of \(B_{p}(K_{i})\), and the elements of \(A\) already born at \(i\) form a basis of \(Z_{p}(K_{i})\). In particular, these cycles form a consistent basis of \(Z_{p}(K_{i})\).

See Figure 3 (left) for an illustration. The collection of bars of a barcode basis is called the _barcode of \(K\)_. Indeed, while there is no unique barcode basis for \(K\), they all give rise to the same barcode.

Figure 2: Left: A simplicial complex with \(11\)\(0\)-simplices, \(19\)\(1\)-simplices and \(7\)\(2\)-simplices. A \(2\)-chain consisting of three \(2\)-simplices is marked with darker color, and its boundary, a collection of \(7\)\(1\)-simplices is displayed in thick.

Right: The \(1\)-cycle marked in thick on the left is also a \(1\)-boundary, since it is the image of the boundary operator under the \(4\) marked \(2\)-simplices. On the right, the \(1\)-cycle \(\) going along the path \(ABCDE\) is not a \(1\)-boundary; therefore it is a generator of an homology class \([]\) of \(H_{1}(K)\). Likewise, the \(1\)-cycle \(^{}\) going along \(ABCFGH\) is not a \(1\)-boundary neither. Furthermore, \([^{}]=[]\) since the sum \(+^{}\) is the \(1\)-cycle given by the path \(AEDCFGH\), which is a \(1\)-boundary because of the \(5\) marked \(2\)-simplices. Hence, \(\) and \(^{}\) represent the same homology class which is characterized by looping aroung the same hole in \(K\).

[MISSING_PAGE_FAIL:5]

Bifiltrations.Assume our data is now a _bifiltered simplicial complex_ written as

\[K_{m,1}@>{}>{}>K_{m,2}@>{}>{}> @>{}>{}>K_{m,n}=K\\ @>{}>{}> @>{}>{}> @>{}>{}> @>{}>{}> @>{}>{}>\\ @>{}>{}>@>{}>{}>@>{}>{}> \\ K_{2,1}@>{}>{}>K_{2,2}@>{}>{}> @>{}>{}>K_{2,n}\\ @>{}>{}>@>{}>{}>@>{}>{}>\\ K_{1,1}@>{}>{}>K_{1,2}@>{}>{}> @>{}>{}>K_{1,n}.\] (2)

Such a structure often times appears in applications where a dataset is analyzed through two different scales. An example is hierarchical clustering where the points are additionally filtered by an independent importance value.

We can iterate the idea from the last paragraph to a bifiltration in a straight-forward manner: Let \(A_{i}\) be a barcode basis for the horizontal filtration \(K_{i,1}\)\(\)\(K_{i,2}\)\(\)\(\)\(\)\(K_{i,n}=K_{i}\). With that bases fixed, there is a graphcode between the \(i\)-th and \((i+1)\)-th horizonal filtration, and we define the union of these graphs as the _graphcode_ of the bifiltration. The vertices of the graphcode are bars of the form \([b,d)\) that are attached to a basis \(A_{i}\), and we can naturally draw the graphcode in \(^{3}\) by mapping the vertex to \((b,d,i)\). This yields a layered graph in \(^{3}\) with respect to the 3rd coordinate with edges only occurring between two consecutive layers. As discussed in Appendix D, graphcodes can be defined for arbitary two-parameter persistence modules. They can also be defined for arbitrary fields, in which case we obtain a graph that has not only node but also edge attributes.

## 4 Computation

The vertices and edges of a graphcode in homology dimension \(p\) can be computed efficiently in \(O(n^{3})\) time where \(n\) is the total number of simplices of dimension \(p\) or \(p+1\). We expose the full algorithm in Appendix B in a self-contained way and only sketch the main ideas here for brevity.

First of all, it can be readily observed that the standard algorithm to compute persistence diagrams via matrix reduction yields a barcode basis in \(O(n^{3})\) time (see ). Doing so for every horizontal slice in (2) yields the vertices of the graphcode, and computing the edges between two consecutive slices can be reduced to solving a linear system via matrix reduction as well, resulting in \(O(n^{3})\) time as well for any two consecutive slices. This is not optimal though as it results in a total running time of \(O(sn^{3})\) with \(s\) the number of horizontal slices.

To reduce further to cubic time, we perform an out-of-order matrix reduction, where the \((p+1)\)-simplices are sorted with respect to their horizontal filtration value, but are added to the boundary matrix in the order of their vertical value. This reduction process, which still results in cubic runtime, yields a sequence of \(n\) snapshots of reduced matrices that correspond to the barcode basis on every horizontal slice, and thus yields all vertices of the graphcode. The final observation is that with additional book-keeping when going from one snapshot to the next, we can track how the basis elements transform from one horizontal slice to the next and these changes encode which edges are present in the graphcode.

Finally, the practical performance can be further improved by reducing the size of the graphcode, by keeping \(s\) small, by ignoring bars whose persistence is below a certain threshold, and by precomputing a minimal presentation instead of working with the simplicial input. See Appendix B for details.

## 5 Learning from graphcodes using graph neural networks

We describe our pipeline that exemplifies how graphcodes can be used in combination with graph neural networks (GNN's). The inputs are layered graphs with vertex attributes \([(b,d),i]\), with \([b,d)\) a bar of the barcode at the \(i\)-th layer. We can add further meaningful attributes like the additive \(d-b\) and/or multiplicative \(\) persistence to the nodes to suggest the GNN that these might be important. Any graph neural network architecture can be used to learn from these topological summaries. We propose the architecture depicted in Figure 4. It starts with a sequence of graph attention (GAT) layers taking the graphcodes as input. The idea is that the network should learn to pay more attention to adjacent features with high persistence which are commonly interpreted as the topological signal. These layers are followed by a local max-pooling layer that performs max-pooling over all vertices in a common slice. Then we concatenate the vectors obtained from the local max-pooling over all slices and feed the resulting vector into a standard feed-forward neural network (Dense Layers).

If we remove all the edges from the graphcodes, this model can be viewed as a combination of multiple Perslay architectures , one for each slice of the bifiltration. In such a case, the model would implicitly learn for each barcode individually which bars are important for classification. Adding the edges, in turn, enhances this model as propagation between neighboring layers is possible: a bar that is connected to important bars in adjacent layers is more likely to be significant itself.

We also point out that the separate pooling by slices is crucial in our approach. It takes advantage of the additional information provided by the position of a slice in the graphcode. If we simply embed the entire graphcode in the plane by superimposing all persistence diagrams and do one global pooling, the outcome gets significantly worse.

## 6 Experiments

We have implemented the computation of graphcodes in a dedicated C++ library and the machine learning pipeline in Python. All the code for our experiments is available in the supplementary materials. The experiments were performed on an Ubuntu 23.04 workstation with NVIDIA GeForce RTX 3060 GPU and Intel Core i5-6600K CPU.

Graph datasets.We perform a series of experiments on graph classification, using a sample of TUDatasets, a collection of graph instances . Following the approach in , we produce a bifiltration of graphs using the Heat Kernel Signature-Ricci Curvature bifiltration. From these bifiltrations, we compute the graphcodes (GC) and train a graph neural network as described in Section 5 to classify them. More details on these experiments can be found in Appendix C.1 and the supplementary materials. We compare the accuracy with multi-parameter persistence images (MP-I) , multi-parameter persistence kernels (MP-K) , multi-parameter persistence landscapes (MP-L) , generalized rank invariant landscapes (GRIL)  and multi-parameter Hilbert signed measure convolutions (MP-HSM-C) . All these approaches produce a vector and use XGBoost  to train a classifier.

The results in Table 1 indicate that graphcodes are competitive on most of these datasets in terms of accuracy. In terms of runtime performance, the instances are rather small and all approaches terminate within a few seconds (with the exception of GRIL that took longer). Also, while the numbers in Table 1 for the previous approaches are taken from , we have partially rerun the classification using convolutional neural networks instead of XGBoost. Since the results were comparable, we decided to use the numbers from the previous work.

Graphcodes do not outperform other methods on these datasets but one can observe that there is no descriptor that consistently outperforms the other descriptors. We also observe that the performance of a certain descriptor on a certain dataset seems a little bit arbitrary. For example, (MP-HSM-C) has arguably the best overall performance but has the worst performance on COX2. A possible explanation could be that there is not enough topological signal in these datasets. This might be unfavorable for graphcodes as they capture more information at the cost of invariance. We also note that the different formats of the topological descriptors require different classifiers and make a direct comparison of the results difficult. This test was included primarily because it is the standard test in related work. Still, it seems unclear that topological descriptors are well suited for these datasets as, for example, on the PROTEINS dataset GNN-architectures reach up to \(85\%\) accuracy .

Figure 4: Neural network architecture for graphcodes.

[MISSING_PAGE_FAIL:8]

We create a dataset _Processes_ consisting of 4 classes, each of which consisting of \(1000\) point clouds sampled from the above processes and use the topological descriptors and neural networks, discussed for the shape dataset above, for the classification. More details can be found in C.3. The results reported in Table 3 show again that graphcodes outperform other topological descriptors. They also show that for these random point processes the influence of the edges of the graphcodes is much smaller than for the shape dataset. This is expected since prominent persistent features along the density direction are very unlikely in random point processes.

**Orbit dataset.** Finally we test our pipeline on another dataset which has been established in topological data analysis as a benchmark in the one-parameter setting. The purpose of this experiment is twofold: On the one hand it demonstrates that graphcodes can be applied to very big datasets. On the other hand it compares the two-parameter graphcode pipeline to its one-parameter analog PersLay . The dataset consists of orbits generated by a dynamical system defined by the following rule:

\[x_{n+1}=x_{n}+ry_{n}(1-y_{n})&1\\ y_{n+1}=y_{n}+rx_{n+1}(1-x_{n+1})&1\] (3)

where the starting point \((x_{0},y_{0})\) is sampled uniformly in \(^{2}\). The behaviour of this dynamical system heavily depends on the parameter \(r>0\). Following , we create two datasets consisting of 5 classes of orbits of \(1000\) points generated by this dynamical system, where the 5 classes correspond to the following five choices of the parameter \(r=2.5\), \(3.5\), \(4.0\), \(4.1\) and \(4.3\). The datasets _Orbit5k_ and _Orbit100k_ consist of \(1000\) and \(20000\) orbits per class, respectively. We again use our graphcode pipeline, discussed for the shape dataset, to classify them. The computation of the graphcodes of the \(100000\) point clouds took just 27 minutes demonstrating the efficiency of our algorithm. The results are reported in Table 4 where we compare them to the results achieved by Persistence Scale Space Kernel (PSS-K) , Persistence Weighted Gaussian Kernel (PWG-K) , Sliced Wasserstein Kernel (SW-K) , Persistence Fisher Kernel (PF-K)  and (PersLAY)  as reported in Table 1 of . The results demonstrate that graphcodes perform better than the one-parameter methods and underpin our conjecture that the performance of the graphcode-GNN pipeline relative to other methods gets better as the size of the dataset increases. More details can be found in C.4.

**Graphcodes with different bases.** The edges of the graphcode of a two-parameter persistence module depend on a choice of bases. So far, in all experiments, we used graphcodes with bases produced by our graphcode-algorithm which is based on the standard reduction algorithm. We next discuss the performance of graphcodes on the shape classification task introduced above with respect to different choices of bases. We compute the graphcodes using the dataset from the previous paragraph "Shape dataset". At first we construct a graphcode dataset (GC-ER) using an exhaustive column reduction  instead of the standard reduction. Next we construct a graphcode dataset (GC-RS) where we randomly shuffle the bases. This is done by performing valid column additions on the input presentation with a \(5\%\) probability. Finally, we produce a graphcode dataset (GC-BC) containing 20 graphcodes constructed with random base shuffles for each input instance.

We observe that the bases chosen by the standard reduction and exhaustive reduction algorithm are far from random. The input presentation arising from a simplicial bifiltration is usually sparse. We find that this initial sparseness is preserved by the standard and exhaustive reduction algorithm in the sense that both lead to sparse graphcodes. If we do random base changes in the input presentation we reduce the sparseness of the input which also leads to a loss of sparseness in the output graphcodes.

   Dataset & PSS-K & PWG-K & SW-K & PF-K & PersLAY & GC & GC-NE \\  Orbit5k & 72.4\(\)2.4 & 76.6\(\)0.7 & 83.6\(\)0.9 & 85.9\(\)0.8 & 87.7\(\)1.0 & **88.5\(\)1.1** & 88.4\(\)1.5 \\ Orbit100k & - & - & - & - & 89.2\(\)0.3 & **92.3\(\)0.3** & 91.5\(\)0.3 \\   

Table 4: Orbit classification results. The table shows average test set prediction accuracy in %. The numbers in all columns except the last two are taken from Table 1 in .

   Dataset & MP-I & MP-L & P-I & GRIL & MP-HSM-C & GC & GC-NE \\  Processes & 66.0\(\)2.5 & 50.2\(\)3.0 & 35.5\(\)10.4 & 61.1\(\)1.6 & 70.7\(\)4.9 & **83.4\(\)2.5** & 83.1\(\)3.7 \\   

Table 3: Average test set prediction accuracy in % over 20 train/test runs with random \(80/20\) train/test split on the point-process dataset.

The result is an increase in the average number of edges of the produced graphs. Average number of edges with standard reduction: \( 826\), exhaustive reduction: \( 841\), random shuffle: \( 1977\).

We train the same graph neural network as in the previous experiments on these alternative graphcode datasets and report the results in Table 5. For the (GC-BC) dataset we modify the training process in the following way: In the \(i\)-th epoch of the training process we use the \((i\) mod \(20)\)-th graphcode for each instance. This approximates a change of basis of each graphcode after each training epoch by picking one of 20 available bases. We find that this training procedure disproportionally benefits from a larger number of training epochs. Therefore, we run the same experiments with twice the number of training epochs.

The results in Table 5 show that the exhaustive reduction (GC-ER) does not significantly change the result compared to the standard bases (GC). The random basis shuffle (GC-RS) leads to slightly worse performance and a slight increase in variability of the results but we note that the performance is still better than without edges (GC-NE). If we use randomly shuffled bases but provide the network 20 different bases for each instance we match, and with more training epochs, even exceed the performance of (GC) and (GC-ER). These results indicate that changing the graphcode bases during the training process can increase the performance.

## 7 Conclusion

Our shape experiment shows that current implementations of topological classifiers struggle with simple datasets that contain a clear topological signal but also a lot of noise. A possible explanation is that the vectorization step in these methods blurs the features too much or relies on invariants which might be too weak for a classifier to pick up delicate details. Graphcodes, on the other hand, provide an essentially complete description of the (persistent) topological properties of the data and delegates finding the relevant signal to the graph neural network. As additional benefit, some vectorizations are challenging to compute, whereas graphcodes can be computed efficiently.

The biggest drawback of graphcodes is certainly that they dependent on a choice of basis and therefore are not uniquely defined for a given dataset. The bases chosen by the standard reduction algorithm are special in the sense that they lead to sparse graphs. Doing random basis changes on the graphcode dataset leads to denser graphs and a slightly worse performance on the shape classification task. But doing multiple random basis changes during the training process and, thus, providing the neural network different bases for the same instance, increases the performance even beyond the performance of the sparse graphs.

A goal for future work is to combine sparse graphcodes and random basis changes during the training process. A possible direction could be to decompose the two-parameter modules into indecomposable summands, compute the graphcodes for the indecomposables and perform random basis changes on the individual components during the training process. By working only with graphcodes of decompositions we could reduce the number of possible edges.

We speculate that the combination of computational efficiency and discriminatory power will make graphcodes a valuable tool in data analysis. With the advent of more efficient techniques to generate bifiltrations for large datasets, we foresee that the potential of graphcodes will be a study of investigation in the coming years.

    & GC & GC-NE & GC-ER & GC-RS & GC-BC \\ 
100 Epochs & **86.9\(\)1.4** & 82.8\(\)1.9 & 86.7\(\)1.4 & 84.5\(\)2.4 & 86.6\(\)1.7 \\
200 Epochs & 87.1\(\)1.6 & 83.7\(\)1.0 & 87.0\(\)1.4 & 85.0\(\)1.7 & **88.1\(\)1.2** \\   

Table 5: Average test set prediction accuracy in % over 20 train/test runs of Graphcodes with different choices of bases on the shape dataset.