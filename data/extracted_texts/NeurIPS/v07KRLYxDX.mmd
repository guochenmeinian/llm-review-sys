# Achieving Domain-Independent

Certified Robustness via _Knowledge Continuity_

 Alan Sun1,2, Chiyu Ma2, Kenneth Ge1, Soroush Vosoughi2

1Carnegie Mellon University, 2Dartmouth College

{alansun, kkge}@andrew.cmu.edu,

{chiyu.ma.gr, soroush.vosoughi}@dartmouth.edu

###### Abstract

We present _knowledge continuity_, a novel definition inspired by Lipschitz continuity which aims to certify the robustness of neural networks across input domains (such as continuous and discrete domains in vision and language, respectively). Most existing approaches that seek to certify robustness, especially Lipschitz continuity, lie within the continuous domain with norm and distribution-dependent guarantees. In contrast, our proposed definition yields certification guarantees that depend only on the loss function and the intermediate learned metric spaces of the neural network. These bounds are independent of domain modality, norms, and distribution. We further demonstrate that the expressiveness of a model class is not at odds with its knowledge continuity. This implies that achieving robustness by maximizing knowledge continuity should not theoretically hinder inferential performance. Finally, to complement our theoretical results, we present several applications of knowledge continuity such as regularization, a certification algorithm, and show that knowledge continuity can be used to localize vulnerable components of a neural network1.

## 1 Introduction

Deep neural networks (DNNs) have demonstrated remarkable generalization capabilities. Their robustness, however, has been considerably more difficult to achieve. _Robustness_ refers to the preservation of model performance under natural or adversarial alterations of the input . DNNs' lack of robustness, highlighted by seminal works such as  and recently , poses significant challenges to their adoption in critical applications, underscoring concerns for AI safety and trustworthiness .

Though issues of robustness emerged from computer vision applications, they have since spanned multiple domains . This research trajectory has not only prompted significant advancements in robustness improvements through architectural, training, and dataset augmentations, but also unveiled the sophistication of _adversarial attacks_--the process through which counterexamples to robustness are generated . Along the progress made in these parallel directions, a great deal of work has gone into _certified robustness_ which seeks to provide theoretical robustness guarantees. Certification is desirable as it generally transcends any particular task, dataset, or model.

As a result, _Lipschitz continuity_ has emerged, promising certified robustness by essentially bounding the derivative of a neural network's output with respect to its input. In this way, Lipschitz continuity directly captures the volatility of a model's performance, getting at the heart of robustness. Such an approach has proven its merit in computer vision, facilitating robustness under norm and distributional assumptions . Its inherent ease and interpretability has lead to widespread adoption as a means to measure and regulate robustness among practitioners as well .

Despite these successes in computer vision, there are fundamental obstacles when one tries to apply Lipschitz continuity into discrete or non-metrizable domains such as natural language. Firstly, characterizing distance in this input-output space is highly nontrivial, as language does not have a naturally-endowed distance metric. Additionally, suppose we impose some distance metric on the input-output space [49; 16]. For such a metric to meaningfully characterize adversarial perturbations, it cannot be universally task-invariant. Consider the two sentences (a) "I am happy," (b) "I am sad." The ground-truth label of (a) is invariant to the perturbation (a) \(\) (b), if the task is sentence-structure identification, but it would not be preserved for a task like sentiment classification. Lastly, key architectures such as the Transformer  are provably _not_ Lipschitz continuous . _Most of these challenges are not unique to language, and they represent a strong divide of our understanding of robustness in discrete/non-metrizable and continuous domains [22; 46]._

To address these issues, we propose a new conceptual framework which we call _knowledge continuity_. At its core, we adopt the following axiom:

_Robustness is the stability of a model's performance_

_with respect to its perceived knowledge of input-output relations._

Concretely, our framework is grounded on the premise that robustness is better achieved by focusing on the variability of a model's loss with respect to its hidden representations, rather than forcing arbitrary metrics on its inputs and outputs. Our approach results in certification guarantees independent of domain modality, norms, and distribution. We demonstrate that the expressiveness of a model class is not at odds with its knowledge continuity. In other words, achieving robustness by improving knowledge continuity should not theoretically hinder inferential performance. We show that in continuous settings (i.e. computer vision) knowledge continuity generalizes Lipschitz continuity and inherits its tight robustness bounds. Finally, we present an array of practical applications using knowledge continuity both as an indicator to predict and characterize robustness as well as an additional term in the loss function to train robust classifiers. In sum, our contributions are threefold:

* Introduction of _knowledge continuity_, a new concept that frames robustness as variability of a model's loss with respect to its hidden representations.
* We theoretically show that knowledge continuity results in certified robustness guarantees that generalize across modalities (continuous, discrete, and non-metrizable). Moreover, this robustness does not come at the expense of inferential performance.
* We present several practical applications of knowledge continuity such as using it train more robust models, in both language processing and vision, identify problematic hidden layers, and using its theoretical guarantees to formulate a novel certification algorithm.

Although our results apply to all discrete/non-metrizable and continuous spaces, throughout the paper we invoke examples from natural language as it culminates the aforementioned challenges. Further, the ubiquity of large language models make their robustness a timely focus.

## 2 Related Works

There have been extensive studies on developing robust neural networks with theoretical guarantees. With respect to our contributions, they can be organized into the following categories.

**Certified robustness with Lipschitz continuity.** The exploration of Lipschitz continuity as a cornerstone for improving model robustness has yielded significant insights, particularly in the domain of computer vision. This principle, which ensures bounded derivatives of the model's output with respect to its input, facilitates a smoother model behavior and inherently encourages robustness against adversarial perturbations. This methodology, initially suggested by , has since been rigorously analyzed and expanded upon. Most theoretical results in this area focus on certifying robustness with respect to the \(_{2}\)-norm [11; 86; 25; 2; 38; 29; 4]. A recent push, fueled by new architectural developments, has also expanded these results into \(_{}\)-norm perturbations [89; 88; 90]. Further, Lipschitz continuity-inspired algorithms also serve practitioners as a computationally effective way to train more robust models [68; 78; 69; 13]. This stands in contrast to (virtual) adversarial training methods which brute-force the set of adversarial examples, then iteratively retrain on them [50; 63; 80]. Though Lipschitz continuity has seen much success in continuous domains, it does not apply to non-metrizable domains such as language. Further, architectural limitations of prevalent models such as the Transformer [70; 36] exacerbate this problem. These challenges highlight a critical need for a new approach that can accommodate the specificities of discrete and non-metrizable domains while providing robustness guarantees.

**Achieving robustness in discrete/non-metrizable spaces.** Non-metrizable spaces, where it is non-trivial to construct a distance metric on the input/output domains, pose a unique challenge to certified robustness. Instead of focusing on point-wise perturbations, many studies have opted to examine how the output probability distribution of a model changes with respect to input distribution shifts by leveraging information bottleneck methods [67; 73; 53] (see also out-of-distribution generalization: [42; 83; 60]). Most of these bounds lack granularity and cannot be expressed in closed-form. In contrast to these theoretical approaches, recent efforts have refocused on directly adapting the principles underlying Lipschitz continuity to language. Virtual adversarial training methods such as [43; 85] mimic the measurement of Lipschitz continuity by comparing changes in the textual embeddings with the KL-divergence of the output logits. Along these lines, techniques akin to those used in adversarial training in vision have also been translated to language, reflecting a shift towards robustness centered around the learned representation space [40; 23; 35]. Though these approaches have seen empirical success, they lack theoretical guarantees. As a result, their implementations and success rate is heavily task-dependent [43; 85]. There have also been attempts to mitigate the non-Lipschitzness of Transformers [87; 82] by modifying its architecture. These changes, however, add significant computational overhead.

**Other robustness approaches.** In parallel, other certified robustness approaches such as randomized smoothing [12; 39; 37] give state-of-the-art certification for \(_{2}\)-based perturbations. Notable works such as [34; 74] have sought to generalize these techniques into language, but their guarantees strongly depend on the type of perturbation being performed. On the other hand, analytic approaches through convex relaxation inductively bound the output of neurons in a ReLU network across layers [79; 81; 77]. These works, however, are difficult to scale and also do not transfer easily to discrete/non-metrizable domains.

Our approach, inspired by Lipschitz continuity, distills the empirical intuitions from the works of [43; 85] and provides theoretical certification guarantees independent of perturbation-type [34; 74] and domain modality. We demonstrate that knowledge continuity yields many practical applications analogous to Lipschitz continuity which are easy to implement and are computationally competitive.

## 3 Preliminaries

**Notations.** Let \(^{ 0}:=[0,)\). For any function \(f:\), we denote \((f):=\{(x,y):f(x)=y\}\). For \(n\), let \([n]\) denote the set \(\{1,2,,n\}\). \((,_{},_{})\), \((,_{},_{})\) are probability spaces and \((,_{}_{ },_{}_{})\) denotes the product measurable space of the probability spaces \(,\). Since our contribution focuses on the supervised learning regime, we colloquially refer to \(,\) as the input and labels, respectively. We call any probability measure \(_{}\) absolutely continuous to \(_{}_{}\) (i.e. \((_{}_{})(E)=0 _{}(E)=0\)) a _data distribution_ and denote it as \(_{,}\). If \((,_{})\) is a metric space with metric \(d_{}\) and \(A\), then for any \(z\), \(d_{}(z,A)=_{a A}d_{}(a,z)\). We say that a metric space, \((,d_{})\), is bounded by some \(B^{ 0}\), if \(_{z^{},z}d(z,z^{})<B\). Denote by \(_{}:\) the identity function on \(\). Let \(:^{20}\) be a loss function where \((y,y^{})=0\) if and only if \(y=y^{}\). For any \(f:\) and \((x,y),(x^{},y^{})\), we denote \(_{f}^{(x,y)}(x^{},y^{}):=|(f(x),y)- (f(x^{}),y^{})|\), essentially the absolute difference in loss between \((x,y)\) and \((x^{},y^{})\). Unless otherwise specified, it will be assumed that \(f\) is a measurable function from \(\) to \(\) with a metric decomposition (see Def. 1).

**Lipschitz continuity.** Given two metric spaces \((,d_{})\), \((,d_{})\) a function \(f:\) is \(K\)-Lipschitz continuous if there exists \(K^{ 0}\) such that for all \(x,x^{}\), \(d_{}(f(x),f(x^{})) Kd_{}(x,x^{})\).

## 4 Knowledge Continuity

In this section, we provide the formal definition of _knowlege continuity_ and explore its theoretical properties.

We start by defining a model's perceived knowledge through a rigorous treatment of its hidden representation spaces. By considering the distance between inputs in some representation space in conjunction with changes in loss, we result in a measure of _volatility_ analogous to Lipschitz continuity.

Bounding this volatility in expectation then directly leads to our notion of knowledge continuity. With these tools, we demonstrate a host of theoretical properties of knowledge continuity including its certification of robustness, guarantees of expressiveness, and connections to Lipschitz continuity in continuous settings. We summarize our theoretical contributions as follows:

* We _define_ the perceived knowledge of a model as well as volatility and knowledge continuity within a model's representation space (see Def. 1, 2, 3, 4, respectively).
* We _prove_ that knowledge continuity implies _probabilistic_ certified robustness under perturbations in the representation space and constraining knowledge continuity should not hinder the expressiveness of the class of neural networks (see Thm. 4.1 and Prop. 4.3, 4.4, respectively).
* We _prove_ that in some cases knowledge continuity is equivalent (in expectation) to Lipschitz continuity. This shows that our axiomization of robustness aligns with existing results when perturbation with respect to the input is well-defined (see Prop. 4.6, 4.8).

### Defining Perceived Knowledge

Knowledge is generally understood as a relational concept: it arises from the connections we make between ideas, experiences, and stimuli . Herein, we capture the _perceived knowledge_ of a model by focusing on the relations it assigns to input-input pairs. Specifically, these relations are exposed by decomposing a function \(f:\) into projections to intermediate metric spaces. Formally,

**Definition 1** (Metric Decomposition).: _We say that \(f\) admits a metric decomposition if there exists metric spaces \((_{1},d_{1}),,(_{n},d_{n})\) with metrics \(d_{k}\) for \(k[n]\) such that_

1. \((_{k},d_{k})\) _is endowed with its Borel_ \(\)_-algebra._
2. _There exists measurable mappings_ \(h_{0},h_{1},,h_{n}\) _where_ \(h_{0}:_{1}\)_,_ \(h_{k}:_{k}_{k+1}\) _for_ \(k[n-1]\)_, and_ \(h_{n}:_{n}\)_._
3. \(f=h_{n} h_{n-1} h_{1} h_{0}\)_._

_Remark 1_.: If \(\) is a metric space with metric \(d_{}\) and \(F_{}\) is its Borel \(\)-algebra, then for any measurable mapping \(f:\) there exists the trivial metric decomposition

\[f=f_{}.\] (4.1)

Therefore, in computer vision applications where \((,d_{})=(^{n},_{})\) for some \(n^{+}\), we can apply this trivial decomposition to yield bounds which mirror the certification guarantees of Lipschitz continuity. This is discussed in detail in Section 4.5.

To the best of our knowledge, all deep learning architectures admit metric decompositions, since their activations are generally real-valued. So, for all subsequent functions from \(\) to \(\), unless otherwise specified, we assume they are measurable and possess a metric decomposition. Further, we denote \(f^{k}=h_{k} h_{k-1} h_{1} h_{0}\) and adopt the convention of calling \(h_{k}\) the \(k^{}\) hidden layer. In Appendix A, we present several metric decompositions for a variety of architectures.

For any metric-decomposite function, an immediate consequence of our definition is that its metric decomposition may not be unique. However, in the context of neural networks, this is a desirable property. Seminal works from an array of deep learning subfields such as semi-supervised learning , manifold learning , and interpretability  place great emphasis on the quality of learned representation spaces by examining the induced-topology of their metrics. This often does not affect the typical performance of the estimator, but has strong robustness implications . Our results, which are dependent on particular metric decompositions, capture this trend. In Section 4.4, we discuss in detail the effects of various metric decompositions on our theoretical results.

### Defining Knowledge Continuity

We first introduce what it means for a model's performance to be volatile at a data point relative to its metric decomposition. Then, we contrast knowledge continuity with Lipschitz continuity, pointing out key differences that will allow us to prove more general bounds.

**Definition 2** (\(k\)-Volatility).: _Let \(f:\) and \(\) be any loss function. The \(k\)-volatility of a point \((x,y)\) which we denote as \(_{f}^{k}(x,y)\) is given by_

\[_{f}^{k}(x,y)\,:=\,_{(x^{},y^{} ) D_{x,y}\\ f(x) f(x^{})}[_{f}^{(x,y)} (x^{},y^{})}{d_{k}(f^{k}(x),f^{k}(x^{}))}],\] (4.2)_where \(d_{k}\) is the distance metric associated with \(f\)'s \(k^{th}\) hidden layer._

By performing some algebra on the definition, we see that it decomposes nicely into two distinct terms: _sparsity_ of the representation and _variation_ in loss.

\[_{f}^{k}(x,y) =_{(x^{},y^{}) D_{,y}}[ (f(x),y)-(f(x^{}),y^{})|}{d_{k}(f^{k} (x),f^{k}(x^{}))}],\] \[=(f(x),y)\,_{(x^{},y^{}) D_{ ,y}}[(f^{k}(x),f^{k}(x^{}))}}_ {}(f(x^{}),y^{ })}{(f(x),y)}|}_{}],\] (4.3)

Our notion of volatility essentially measures the change in performance with respect to perturbations to a model's perceived knowledge. In particular, Eq. 4.3 reveals that there are two interactions in play which we illustrate in Fig. 1. Informally, we say that \((x,y)\) is highly volatile if there is a large discrepancy in performance between it and points that are perceived to be conceptually similar. Therefore, highly volatile points capture inaccurate input-input knowledge relations. Additionally, \((x,y)\) experiences low volatility if the space around it is sparse with respect to \(D_{,}\). In other words, any set of perturbations applied in \(_{k}\) would push \((x,y)\) far away, with high probability. This makes \((x,y)\) an isolated concept with little knowledge relationships associated with it.

Similar to Lipschitz continuity, the boundedness of the \(k\)-volatility of \(f\) across the data distribution is crucial and we denote this class of functions as _knowledge continuous_.

**Definition 3** (Pointwise \(\)-Knowledge Continuity).: _We say that \(f\) is \(\)-knowledge continuous at \((x,y)\) with respect to a function \(f\), loss function \(\), and hidden layer \(k\) if \(_{f}^{k}(x,y)<\)._

Conversely, we say that \((x,y)\) is \(\)-knowledge discontinuous if the previous inequality does not hold. Further, \((x,y)\) is simply knowledge discontinuous if \(_{f}^{k}(x,y)\) is unbounded. Now, we extend this definition globally by considering the \(k\)-volatility between all pairs of points.

Figure 1: Examples of knowledge (dis)continuities. \(f:\) is a measurable map, and \((_{k},d_{k})\) is one of its hidden representations. The color of the points indicates loss. \(\) denotes knowledge continuity induced by sparsity: an isolated concept with no knowledge relations close to it. So, any perturbation moves \(\) far away with high probability. Smooth changes in loss around \(\) implies knowledge continuity. Finally, \(\) is not knowledge continuous due to drastic changes in loss nearby. Notice that the classification of points is independent of input/output clustering behavior since \(,\) may not be endowed with a metric.

**Definition 4** (Expected \(\)-Knowledge Continuity).: _We say that \(f\) is \(\)-knowledge continuous with respect to a loss function \(\) and hidden layer \(k\) if_

\[_{(x,y) D}[^{k}_{f}(x,y)]<.\] (4.4)

Though the functional forms of Lipschitz continuity and knowledge continuity are similar, there are important differences that allow us to prove more general results. Firstly, _unlike Lipschitz continuity which is an analytical property of the model \(f\)_, _knowledge continuity is a statistical one._ In this way, non-typical data points, even if they are volatile, are ignored, whereas Lipschitz continuity treats all points equally. This is necessary in many discrete applications, as projecting a countable input space onto a non-countable metric space inevitably results in a lack of correspondence thereof. Moreover, ground-truth relations from \(\) may not be well-defined on _all_ of \(\): consider sentiment classification of an alpha-numeric UUID string or dog-cat classification of Gaussian noise. Secondly, _the knowledge continuity of an estimator is measured with respect to the loss function rather than its output._ This property allows us to achieve the expressiveness guarantees in Section 4.4, since it places no restrictions on the function class of estimators. Lastly, _knowledge continuity measures the distance between inputs with the endowed metric in its hidden layers._ This flexibility allows us to define knowledge continuity even when the input domain is not a metric space.

### Certification of Robustness

Our first main result demonstrates that \(\)-knowledge continuity implies _probabilistic_ certified robustness in the hidden representation space. In Theorem 4.1, given some reference set \(A\), we bound the probability that a \(\)-sized perturbation in the representation space away from \(A\) will result in an expected \(\) change in loss. In other words, knowledge continuity is able to characterize the robustness of any subset of data points with positive measure.

**Theorem 4.1**.: _Let \(A\) such that \(_{D_{,}}[A]>0\) and \(,>0\). Let \(A^{}=\{(x^{},y^{}): _{(x,y) D_{,}}\\ (x,y) A.\). If \(f:\) is \(\)-knowledge continuous with respect to the hidden layer indexed by \(k\) and \((_{k},d_{k})\) is bounded by \(B>0\), then_

\[_{(x,y) D_{,}}[A^{} d_{k}(f^{k}( x),f^{k}(A))<]-[A]}})^{2}])}.\] (4.5)

Proof sketch.: We apply the definition of conditional probability \(P(A\,|\,B)=P(A B)/P(B)\) and bound \(P(A B)\), \(P(B)\), separately. The numerator, \([A^{}\) and \(d_{k}(f^{k}(x),f^{k}(A))<]\), is upper-bounded through an application of Markov's Inequality. On the other hand, we apply known concentration inequalities to lower bound \([d_{k}(f^{k}(x),f^{k}(A))<]\), combining these results in the theorem. We present the proof in its entirety in Appendix B. 

This demonstrates that knowledge continuity results in certification of robustness, independent of distance metric and domain modality. The assumption of boundedness and requirement to know \([A]\) can be lost by taking limits of Eq. 4.5 with respect to \(B\) and \([A]\). This yields the following corollary.

**Corollary 4.2**.: _If \((_{k},d_{k})\) is unbounded, then_

\[_{(x,y) D_{,}}[A^{} d_{k}(f^{k} (x),f^{k}(A))<][A])}.\] (4.6)

_If \([A]=0\), then_

\[_{(x,y) D_{,}}[A^{} d_{k}(f^{k} (x),f^{k}(A))<].\] (4.7)

Proof.: These results follow from directly taking the limit as \(B\) and applying some of the bounds acquired in the proof of Thm. 4.1. This yields Eq. 4.6. Next, jointly taking the limit as \([A] 0\) and \(B\) results in Eq. 4.7. 

In both Thm. 4.1 and Cor. 4.7, we yield probabilistic guarantees like , rather than deterministic ones. Though deterministic bounds are desirable, the stochasticity of our framework is necessaryfor its generalization across different domains. For most continuous, metrizable applications (like computer vision), models learn a hidden representation space where most minute changes in this space correspond to tangible inputs. The same cannot be said for many discrete or non-metrizable applications. In natural language processing, the correspondence between the learned representation space and the input is sparse, resulting in lots of "dead space": portions of the hidden representation space that do not correspond to any input [3; 19]. And so, by incorporating the data distribution into our bounds, we implicitly adjust for this: assigning zero-measure to the aforementioned "dead space."

### Expressiveness

Our second main result demonstrates that \(\)-knowledge continuity can be achieved without theoretically compromising the accuracy of the model. In other words, universal function approximation is an invariant property with respect to \(\)-knowledge continuity. Universal approximation results have seen a great deal of theoretical work, as they put limits on what neural networks can represent [15; 31; 45]. As discussed in Section 2, Lipschitz continuous functions do not achieve universal function approximation with respect to the set of all functions, in particular, non-continuous ones. However, we show that under strong conditions this is achievable with knowledge continuity.

First, let us formally define a _universal function approximator_.

**Definition 5** (Universal Function Approximator).: _Suppose that \(\) is Lebesgue-integrable in both coordinates. Let \(^{}\) be a set of measurable functions from \(\) such that for any \(f\), there exists \(_{f}_{,}\) such that \(_{f}((f))=1\). Then, \(^{}\) is a universal function approximator of \(\) if for every \(f\) and every \(>0\), there exists \(\) such that_

\[\,((x),y)\,d\,_{f}<.\] (4.8)

We now show any universal function approximator can be made robust through the trivial metric decomposition.

**Proposition 4.3**.: _Let \(^{}\) be a universal function approximator of \(^{}\) with respect to some loss function \(\). Then, for any \(f^{}\) and sequence \(_{1},_{2},\) such that \(_{n} 0\) there are a sequence of \(_{n}\)-knowledge continuous functions in \(^{}\) such that \(\,(f_{n}(x),y)\,d\,_{f}<_{n}\), for \(n\)._

Proof.: Choose \(f_{n}\) such that \(\,(f_{n}(x),y)\,d\,_{f}<_{n}\). Consider the 1-layer metric decomposition of \(f\), \(h_{1}:_{1}\) where \(_{1}=\) equipped with the trivial metric (\(d_{1}(x,y)=1\) if \(x y\) and 0 otherwise). Then, \(f_{n}=f_{n} h_{1}\). So, it follows that

\[\,^{1}_{f_{n}}(x,y) =\,^{(x,y)}_{f_{n}}(x^{},y^{ })}{d_{1}(h_{1}(x),h_{1}(x^{}))}\,d\,_{f},\] \[\,^{(x,y)}_{f_{n}}(x^{},y^{ })\,d\,_{f},\] \[_{n}.\]

and by the construction of \(f_{n}\), the proof is completed. 

In other words, if our estimator was given "infinite representational capacity," robustness can be trivially achieved by isolating every point as its own concept (as discussed in Section 4.2). More generally, if we instead considered a generalized discrete metric (fix \(c[0,]\), \(d(x,y)=c\) if and only if \(x=y\) and \(d(x,y)=0\), otherwise), then as \(c\), \(k\)-volatility converges pointwise to 0 almost everywhere assuming that the loss is finite almost everywhere. In practice, we find these degenerate decompositions to be unreasonable as they also trivialize robustness. For example, if \(c=\), then robustness is not well-defined as any perturbation would lead to a point that is perceived to be infinitely far away. _In this sense, our framework accounts for different notions of robustness, strong and weak._ The next result builds on Prop. 4.3 and demonstrates how a stronger notion of robustness will affect expressiveness. These added constraints make it so that trivial metric decompositions are no longer possible unless the metric in \(\) is also trivial. We state this formally below, note the highlighted differences between this and Prop. 4.3.

**Proposition 4.4**.: _Suppose \((,d_{}),(,d_{}):=(,d_{ })\) are **compact** metric spaces, \(^{}\) is the set of all continuous functions from \(\) to \(\) such that \(\,d_{}(x,x^{})^{-1}d\,_{f}<\) and \(\) be Lipschitz continuous in both coordinates. Then, there exists a universal function approximator \(\) of \(\) that is knowledge continuous (i.e. \(\,^{k}_{f}(x,y)<\) for some \(k\))._Proof sketch.: We show an outline of the proof here and defer the full proof to Appendix C. By the Stone-Weierstrass Theorem, the set of Lipschitz continuous functions is dense in the set of all continuous functions from \(\) to \(\). Since \(\) is Lipschitz continuous in both coordinates, through some algebra, \(\ ^{1}_{f}(x,y)<\), where \(h_{1}=_{}\) and we yield the statement of the theorem. 

The additional constraint \( d_{}(x,x^{})^{-1}d_{f}\) requires data points to be sparsely layed out in the representation space. As discussed previously, this assumption is generally reasonable for discrete applications. In conjunction with Prop. 4.3, we have shown that the class of knowledge continuous functions is _strictly larger_ than the class of Lipschitz continuous ones. Though we show that universal approximation by knowledge continuous networks is achievable, it is unclear whether these results still hold if the "tightness" of the metric decompositions is bounded. Specifically, the construction in Prop. 4.3 results in a metric decomposition with infinite Hausdorff dimension. Is it possible to achieve Prop. 4.3 in its most general form if we only consider the set of all knowledge continuous functions with metric decompositions with finite Hausdorff dimension? Based on the theoretical and empirical results of , respectively, we conjecture in the negative and leave its resolution open.

**Conjecture 4.5**.: _If \(^{}\) is a universal function approximator with respect to some Lebesgue-integrable loss function \(\). Then, for any \(f^{}\), there **does not exist** a sequence of functions with metric decompositions of **finite Hausdorff dimension** that achieve arbitrarily small approximation error (i.e. \((f(x),y)d_{f}\)) and knowledge continuity._

### Connections to Lipschitz Continuity

We now demonstrate that our axiomization of robustness presented in Section 1 aligns with the notion of robustness2 commonly prescribed in vision . This unifies the certified robustness bounds with respect to the representation space derived in Thm. 4.1 with existing work certifying robustness with respect to the input space in continuous applications such as vision.

Our first result identifies conditions under which knowledge continuity, implies Lipschitz continuity.

**Proposition 4.6**.: _Suppose that \((,d_{}),(,d_{})\) are metric spaces. Let the first \(n\) metric decompositions of \(f:\) be \(K_{i}\)-Lipschitz continuous, for \(i[n]\). If \(f\) is \(\)-knowledge continuous with respect to the \(n^{}\) hidden layer and \(d_{}(f(x),f(x^{}))_{f}^{(x,y)}(x^{},y)\) for all \(x,x^{}\), \(y\), and some \(>0\), then \(f\) is Lipschitz continuous in expectation. That is,_

\[_{(x,y),(x^{},y^{}) D_{},y}\ }(f(x),f(x^{}))}{d_{}(x,x^{})}  e_{j=1}^{n}K_{j}.\] (4.9)

The proof is presented in Appendix D and follows easily through some algebriac manipulation. It is easy to see that if \(f\) is knowledge continuous with respect to some identity (or contractive) metric decomposition, then we can loose the repeated product. Analogous to Remark 1, the concepts of Lipschitz continuity and knowledge continuity become similar when we can assign metrics to the input-output spaces. Next, combining this proposition with an auxiliary result from , we directly yield a certification on the input space.

**Corollary 4.7**.: _Suppose that assumptions of Prop. 4.6 are true. And also assume that \((,d_{})=(^{n},_{p})\), \((,d_{})=(^{m},_{p})\), for \(1 p\). Define a classifier from \(f:^{n}^{m}\), \(g\), where \(g(x):=_{k[m]}f_{k}(x)\) for any \(x^{n}\). Then, with probability \(1-_{j=1}^{n}K_{j}\), \(g(x)=g(x+)\) for all \(\|\|_{p}<(2^{1/p}/2t)margin(f(x))\) and \(t>0\). \(f_{k}(x)\) is the \(k^{}\) coordinate of \(f(x)\) and \(margin(f(x))\) denotes the difference between the largest and second-largest output logits._

We present the proof in Appendix D. Our second result identifies conditions under which Lipschitz continuity, implies knowledge continuity.

**Proposition 4.8**.: _Let \((,d_{}),(,d_{})\) be a metric spaces. Let \(f:\) be \(\)-Lipschitz continuous and \((f(x),y)\) be \(\)-Lipschitz continuous with respect to both coordinates. If the first \(n\) metric decompositions of \(f\) are \(K_{i}\)-Lipschitz continuous, then \(f\) is knowledge continuous with respect to the \(n^{}\) hidden layer. That is,_

\[_{(x,y) D_{},y}\ ^{n}_{f}(x,y) e_{j=1}^{n} }.\] (4.10)We detail the proof of this proposition in Appendix D. We note that in continuous applications such as computer vision, the assumptions of both propositions are generally met (i.e. our input-output spaces are metric spaces, all hidden layers are Lipschitz, and loss functions are locally Lipschitz). Furthermore, common architectures such as fully connected networks, CNNs, RNNs, and even vision transformers are Lipschitz continuous [71; 55]. _This implies that our notion of robustness is indeed an appropriate generalization that transcends domain modality since in continuous settings we can recover the strong bounds of Lipschitz continuity while expanding into new discrete and non-metrizable territory._

## 5 Practical Applications

In addition to the theoretical guarantees given by knowledge continuity in Section 4, we also demonstrate that knowledge continuity can be easily applied in practice. First, we find that knowledge continuity, similar to Lipschitz continuity, can be used to gauge adversarial robustness. Along these lines, our measure of volatility (see Def. 2) can be used to isolate particularly vulnerable hidden representations. These applications then directly motivate regulation of knowledge continuity as a means to enforce robustness.

Unless otherwise specified, we run all of our experiments on the IMDB dataset  (a sentiment classification task) using a host of language models from different model families (encoder, decoder, encoder-decoder). We also present additional experiments on vision tasks. These experiments can be found in the Appendix G.

**Knowledge continuity can predict adversarial robustness.** For a given model, \(f\), with \(n\) hidden representations, choose some \(k[n]\). Then, consider the hidden representation index by \(k\). For this fixed \(k\), we determine its \(k\)-volatility by directly estimating Def. 2 through a naive Monte-Carlo algorithm (see Appendix G for more details). Repeating this for all \(k[n]\), we yield a collection of \(k\)-volatilities which we denote as \(\{_{1},,_{n}\}\), one for each hidden layer. When we regress a simple average of these coefficients, \(n^{-1}_{k=1}^{n}_{k}\), with the empirical adversarial robustness (estimated using TextFooler ), a strong correlation is observed. This is shown in Fig. 2(a). In particular, knowledge continuity alone is able to explain \(35\%\) of the variance in adversarial attack success rate. When we combine \(k\)-volatility with other model properties like size, model family, even more variance can be explained (\(R^{2}=0.48\)). Thus, knowledge continuity may be used as a computationally efficient method to estimate adversarial vulnerability with respect to the input space as compared to iteratively applying real adversarial attacks. Moreover, when the adversary is unknown _a priori_, knowledge continuity can also be used in this way as a diagnostic tool. A detailed discussion of these experiments are presented in Appendix E.

**Knowledge continuity can localize vulnerable hidden representations.** We plot the relationship between the \(k\)-volatility, \(_{k}\), and the relative depth of the model (i.e. \(k/n\)). We find that language models belonging to different model families (encoder, decoder, encoder-decoder) admit different \(k\)-volatility trajectories. This is shown in Fig. 2(b). In this way, knowledge continuity may provide a more

Figure 2: (a) The average percentage of successful adversarial attacks by TextFooler  on a host of models [58; 57; 16; 44] and the IMDB  dataset regressed with the average of knowledge continuity coefficients across all hidden layers (\(R^{2}=0.35\)). (b) \(k\)-Volatility as \(k\) is varied across a model’s relative depth. (c) Correlation between \(k\)-volatility and adversarial vulnerability (averaged across all models shown in (b)) with respect to TextFooler  as \(k\) varies.

nuanced picture of a model's inductive biases and robustness beyond a scalar value like "accuracy under adversarial attack." We present a detailed analysis of this in Appendix F. Further, these dynamics may act as a diagnostic tool and offer a starting point for designing _model-specific_ robustness interventions or adversarial defenses. For example, when insights from Fig. 2(b) are combined with a knowledge continuity regularization algorithm, this yields superior empirical robustness compared to existing methods. This is shown in the next subsection and in Appendix G. In addition, knowledge continuity can also quantitatively characterize an adversarial attack against a host of models which is useful for online or adaptive defenses [84; 64; 14]. This is shown in in Fig. 2(c), where TextFooler  largely exploits the knowledge continuities in middle/final layers of the model to decrease performance.

**Regulating knowledge continuity.** Motivated by the theoretical results in Section 4, we augment the loss function during training to mitigate knowledge continuity. Specifically, on each training iteration (batch), we start by choosing a hidden layer at random according to a Beta distribution determined _a priori_: \(X(,)\) and let \(k= nX\). Here, \(,\) are chosen according to Fig. 2(b,c). We assign larger sampling probability to layers where both \(k\)-volatility is high and where knowledge continuity is highly correlated with adversarial robustness. In this way, our regularization objective is both model and attack specific (if the attack method is unknown, then we only apply the former). Then, we devise a Monte-Carlo algorithm to estimate this layer's \(k\)-volatility, \(_{k}\), (see Appendix G) on this minibatch. And so, the augmented loss function becomes \(^{}(f(x),y)=(f(x),y)+_{k}\) with \( 0\) as a hyperparameter, controlling the regularization strength. In contrast to existing adversarial training methods that perform inner-optimization steps [50; 43; 85], our method requires only additional zeroth-order computations. As a result, it outperforms existing works in training speed (up to \(2\) for TextFooler  and \(3\) for ALUM ), while improving robustness. We present a discussion of the results, ablation studies, and training details in Appendix G.

**Certifying robustness with knowledge continuity.** We present an algorithm based on Thm. 4.1 to certify robustness during test-time. Similar to , we estimate the probability of there existing an adversarial example within some fixed radius (in the representation space, according to a pre-defiend distance metric) through bootstrapping a one-side confidence interval. Applying these methods to our regularization results, we show that regularizing knowledge continuity increases the certified robustness. The certification algorithm, its proof of correctness, and certifications of our regularized models are presented in Appendix H.

## 6 Conclusion

In this paper, we propose a novel definition, _knowledge continuity_, which addresses some of the key limitations of Lipschitz robustness. We demonstrate that our definition certifies robustness across domain modality, distribution, and norms. We also show that knowledge continuity, in contrast to Lipschitz continuity, does not affect the universal approximation property of neural networks. We also establish conditions under which knowledge continuity and Lipschitz continuity are equivalent. Lastly, we present several practical applications that directly benefit the practitioner. The broader impacts, reproducibility, and limitations of our work can be found in Appendix I, J, K, respectively.

   Arch. & Method & IMDB & IMDB\({}_{}\) & IMDB\({}_{}\) & ANLI\({}_{}\) & ANLI\({}_{}\) & ANLI\({}_{}\) \\   & Base & 93.6 & 47.9 & 45.2 & 44.5 & 45.6 & 33.8 \\  & TF  & 93.3 & 69.2 & 62.5 & ✗ & ✗ & ✗ \\ \(\)10M params & ALUM  & 93.5 & 56.9 & 47.8 & 45.2 & 46.7 & **46.3** \\  & **KCReg (ours)** & **94.8** & **75.1** & **84.9** & **45.6** & **46.9** & 45.3 \\   & Base & 93.6 & 63.9 & 54.9 & 42.7 & 44.9 & 43.4 \\  & TF  & 92.0 & 64.5 & 51.3 & ✗ & ✗ & ✗ \\ \(\)1.5B params & ALUM  & 94.9 & 49.4 & 27.5 & 43.8 & 45.2 & 44.6 \\  & **KCReg (ours)** & **94.9** & **87.8** & **90.6** & **47.1** & **48.1** & **44.7** \\   & Base & 93.7 & 53.9 & 39.3 & 46.1 & 44.7 & **46.0** \\  & TF  & **96.8** & 77.8 & 60.6 & ✗ & ✗ & ✗ \\ \(\)220M params & ALUM  & 95.1 & 67.1 & 51.9 & 44.5 & 44.8 & 44.4 \\  & **KCReg (ours)** & 94.9 & **89.3** & **91.3** & **48.2** & **45.0** & 44.3 \\   

Table 1: Comparison of our knowledge continuity algorithm to existing works across various model families and adversarial attack methods. TF, BA, ANLI denote adversarial attacks [35; 40], and , respectively. Regulating knowledge continuity to improve robustness is superior across almost all tasks and attacks.

Acknowledgements

Alan Sun thanks Fengwen Sun for the helpful feedback on early drafts of the work as well as Jeffrey Jiang and Andrew Koulogeorge for thoughtful discussions.