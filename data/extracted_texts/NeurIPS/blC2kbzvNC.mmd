# Adaptive SGD with Polyak stepsize and Line-search:

Robust Convergence and Variance Reduction

 Xiaowen Jiang

CISPA

xiaowen.jiang@cispa.de &Sebastian U. Stich

CISPA

stich@cispa.de

CISPA

CISPA

CISPA

stich@cispa.de

CISPA Helmholtz Center for Information Security, Saarbrucken, Germany

###### Abstract

The recently proposed stochastic Polyak stepsize (SPS) and stochastic line-search (SLS) for SGD have shown remarkable effectiveness when training over-parameterized models. However, two issues remain unsolved in this line of work. First, in non-interpolation settings, both algorithms only guarantee convergence to a neighborhood of a solution which may result in a worse output than the initial guess. While artificially decreasing the adaptive stepsize has been proposed to address this issue (Orvieto et al. ), this approach results in slower convergence rates under interpolation. Second, intuitive line-search methods equipped with variance-reduction (VR) fail to converge (Dubois-Taine et al. ). So far, no VR methods successfully accelerate these two stepsizes with a convergence guarantee. In this work, we make two contributions: Firstly, we propose two new robust variants of SPS and SLS, called AdaSPS and AdaSLS, which achieve optimal asymptotic rates in both strongly-convex or convex and interpolation or non-interpolation settings, except for the case when we have both strong convexity and non-interpolation. AdaSLS requires no knowledge of problem-dependent parameters, and AdaSPS requires only a lower bound of the optimal function value as input. Secondly, we propose a novel VR method that can use Polyak stepsizes or line-search to achieve acceleration. When it is equipped with AdaSPS or AdaSLS, the resulting algorithms obtain the optimal rate for optimizing convex smooth functions. Finally, numerical experiments on synthetic and real datasets validate our theory and demonstrate the effectiveness and robustness of our algorithms.

## 1 Introduction

Stochastic Gradient Descent (SGD)  and its variants  are among the most preferred algorithms for training modern machine learning models. These methods only compute stochastic gradients in each iteration, which is often more efficient than computing a full batch gradient. However, the performance of SGD is highly sensitive to the choice of the stepsize. Common strategies use a fixed stepsize schedule, such as keeping it constant or decreasing it over time. Unfortunately, the theoretically optimal schedules are disparate across different function classes , and usually depend on problem parameters that are often unavailable, such as the Lipschitz constant of the gradient. As a result, a heavy tuning of the stepsize parameter is required, which is typically expensive in practice.

Instead of fixing the stepsize schedule, adaptive SGD methods adjust the stepsize on the fly . These algorithms often require less hyper-parameter tuning and still enjoy competitive performance in practice. Stochastic Polyak Stepsize (SPS)  is one of such recent advances. It has received rapid growing interest due to two factors: (i) the only required parameter is the individual optimal function value which is often available in many machine learning applications and (ii) its adaptivityutilizes the local curvature and smoothness information allowing the algorithm to accelerate and converge quickly when training over-parametrized models. Stochastic Line-Search (SLS)  is another adaptive stepsize that offers exceptional performance when the interpolation condition holds. In contrast to SPS, the knowledge of the optimal function value is not required for SLS, at the cost of additional function value evaluations per iteration.

An ideal adaptive stepsize should not only require fewer hyper-parameters but should also enjoy _robust_ convergence, in the sense that they can automatically adapt to the optimization setting (interpolation vs. non-interpolation). This will bring great convenience to the users in practice as they no longer need to choose which method to use (or running both of them at double the cost). Indeed, in many real-world scenarios, it can be challenging to ascertain whether a model is effectively interpolating the data or not . For instance, the feature dimension of the rcv1 dataset  is twice larger than the number of data points. A logistic regression model with as many parameters as the feature dimension may tend to overfit the data points. But the features are actually sparse and the model is not interpolated. Another example is federated learning  where millions of clients jointly train a machine learning model on their mobile devices, which usually cannot support huge-scale models. Due to the fact that each client's data is stored locally, it becomes impractical to check the interpolation condition.

While SPS and SLS are promising adaptive methods, they are not robust since both methods cannot converge to the solution when interpolation does not hold. Orvieto et al.  address this issue for SPS by applying an artificially decreasing rule and the resulting algorithm DecSPS is able to converge as quickly as SGD with the optimal stepsize schedule. However, the convergence rates of DecSPS in interpolation regimes are much slower than SPS. For SLS, no solution has been proposed.

If the user is certain that the underlying problem is non-interpolated, then applying variance-reduction (VR) techniques can further accelerate the convergence . While gradient descent with Polayak stepsize and line-search perform well in the deterministic settings, there exists no method that successfully adapt these stepsizes in VR methods. Mairal  and Schmidt et al.  proposed to use stochastic line-search with VR. However, no theoretical guarantee is shown. Indeed, this is a challenging open question as Dubois-Taine et al.  provides a counter-example where classical line-search methods fail in the VR setting. As such, it remains unclear whether we can accelerate SGD with stepsizes from Polyak and line-search family in non-interpolated settings.

### Main contributions

In this work, we provide solutions to the aforementioned two challenges and contribute new theoretical insights on Polyak stepsize and line-search methods. We summarize our main contributions as follows:

* In Section 3, we propose the first robust adaptive methods that simultaneously achieve the best-known asymptotic rates in both strongly-convex or convex and interpolation or non-interpolation settings except for the case when we have strongly-convexity and non-interpolation. The first method called AdaSPS, a variant of SPS, requires only a lower bound of the optimal function value as input (similar to DecSPS) while AdaSLS, the second method based on SLS, is parameter-free. In the non-interpolated setting, we prove for both algorithms an \((1/^{2})\) convergence rate for convex functions which matches the classical DecSPS and AdaGrad  results, whereas SPS and SLS cannot converge in this case. In the interpolated regime, we establish fast \(((1/))\) and \((1/)\) rates under strong convexity and convexity conditions respectively, without knowledge of any problem-dependent parameters. In contrast, DecSPS converges at the slower \((1/^{2})\) rate and for AdaGrad, the Lipschitz constant is needed to set its stepsize .
* In Section 4, we design a new variance-reduction method that is applicable to both Polyak stepsizes or line-search methods. We prove that to reach an \(\)-accuracy, the total number of gradient evaluations required in expectation is \(}(n+1/)\) for convex functions which matches the rate of AdaSVRG . With our newly proposed decreasing probability strategy, the artificially designed multi-stage inner-outer-loop structure is not needed, which makes our methods easier to analyze. Our novel VR-framework is based on proxy function sequences and can recover the standard VR methods  as a special case. We believe that this technique can be of independent interest to the optimization community and may motivate more personalized VR techniques in the future.

### Related work

Line-search procedures has been successfully applied to accelerate large-scale machine learning training. Following , Galli et al.  propose to relax the condition of monotone decrease of objective function for training over-parameterized models. Kunstner et al.  extends backtracking line-search to a multidimensional variant which provides better diagonal preconditioners. In recent years, adaptive stepsizes from the AdaGrad family have become widespread and are particularly successful when training deep neural networks. Plenty of contributions have been made to analyze variants of AdaGrad for different classes of functions [15; 51; 43; 55; 56], among which Vaswani et al.  first propose to use line-search to set the stepsize for AdaGrad to enhance its practical performance. More recently, variance reduction has successfully been applied to AdaGrad stepsize and faster convergence rates have been established for convex and non-convex functions [14; 25].

Another promising direction is the Polyak stepsize (PS)  originally designed as a subgradient method for solving non-smooth convex problems. Hazan and Kakade  show that PS indeed gives simultaneously the optimal convergence result for a more general class of convex functions. Nedic and Bertsekas  propose several variants of PS as incremental subgradient methods and they also discuss the method of dynamic estimation of the optimal function value when it is not known. Recently, more effort has been put into extending deterministic PS to the stochastic setting [47; 6; 42]. However, theoretical guarantees of the algorithms still remain elusive until the emergence of SPS/SPS\({}_{}\). Subsequently, further improvements and new variants such as DecSPS  and SPS with a moving target  have been introduced. A more recent line of work interprets stochastic Polyak stepsize as a subsampled Newton Raphson method and interesting algorithms have been designed based on the first-order local expansion [17; 18] as well as the second-order expansion . Wang et al.  propose to set the stepsize for SGD with momentum using Polyak stepsize. Abdukhakimov et al.  employ more general preconditioning techniques to SPS.

    &  &  \\   & strongly-convex & convex & required input & strongly-convex & convex\({}^{a}\) & required input \\  SPS/SPS\({}_{}\) & \((())\) & \(()\) & \(f^{*}_{h}\) & \((^{2}_{f,0})\) & \((^{2}_{f,0})\) & \(f^{*}_{i}\) \\ SLS  & \((())\) & \(()\) & None & \((^{2}_{f,0})\) & \((^{2}_{f,0})\) & None \\ DecSPS  & \((})\) & \((})\) & \(f^{*}_{h}\) & \(()\) & \((})\) & \(f^{*}_{i}\) \\  AdaSPS (this work) & \((())\) & \(()\) & \(f^{*}_{h}\) & \(()\) & \((})\) & \(f^{*}_{i}\) \\ AdaSLS (this work) & \((())\) & \(()\) & None & \((})\) & \((})\) & None \\   ^{*}\)The assumption of bounded iterates is also required except for SPS and SLS.} \\ 

Table 1: Summary of convergence behaviors of the considered adaptive stepsizes for smooth functions. For SPS/SPS\({}_{}\) and SLS in non-interpolation settings, \(()\) indicates the size of the neighborhood that they can converge to. In the other cases, the \(()\) complexity provides the total number of gradient evaluations required for each algorithm to reach an \(()\) suboptimality. For convex functions, the suboptimality is defined as \([f(_{T})-f^{*}]\) and for strongly convex functions, the suboptimality is defined as \([||_{T}-^{*}||^{2}]\).

Figure 1: Illustration of the robust convergence of AdaSPS and AdaSLS on synthetic data with quadratic loss. SPS and SLS have superior performance on the two interpolated problems but cannot converge when the interpolation condition does not hold. DecSPS suffers from a slow convergence on both interpolated problems. (Repeated 3 times. The solid lines and the shaded area represent the mean and the standard deviation.)There has been a recent line of work attempting to develop methods that can adapt to both the interpolation setting and the general growth condition beyond strong convexity. Using the iterative halving technique from , Cheng and Duchi  propose AdaStar-G which gives the desired property if the Lipschitz constant and the diameter of the parameter domain are known. How to remove these requirements is an interesting open question for the future research.

## 2 Problem setup and background

### Notations

In this work, we consider solving the finite-sum smooth convex optimization problem:

\[_{^{d}}[f()=_{i=1}^{ n}f_{i}()]\;.\] (1)

This type of problem appears frequently in the modern machine learning applications , where each \(f_{i}()\) represents the loss of a model on the \(i\)-th data point parametrized by the parameter \(\). Stochastic Gradient Descent (SGD)  is one of the most popular methods for solving the problem (1). At each iteration, SGD takes the form:

\[_{t+1}=_{t}-_{t} f_{i_{t}}(_{t})\;,\] (2)

where \(_{t}\) is the stepsize parameter, \(i_{t}[n]\) is a random set of size \(B\) sampled independently at each iteration \(t\) and \( f_{i_{t}}()=_{i i_{t}} f_{i}()\) is the minibatch gradient.

Throughout the paper, we assume that there exists a non-empty set of optimal points \(^{}^{d}\), and we use \(f^{}\) to denote the optimal value of \(f\) at a point \(^{}^{}\). We use \(f^{}_{i_{t}}\) to denote the infimum of minibatch function \(f_{i_{t}}(x)\), i.e. \(f^{}_{i_{t}}=_{^{d}}_{i i_{ t}}f_{i}()\). We assume that all the individual functions \(\{f_{i}()\}\) are \(L\)-smooth. Finally, we denote the optimal objective difference, first introduced in , by \(^{2}_{f,B}=f^{}-_{i_{t}}[f^{}_{i_{t}}]\). The definitions for the interpolation condition can be defined and studied in various ways [48; 9; 4; 11]. Here, we adopt the notion from . The problem (1) is said to be **interpolated** if \(^{2}_{f,1}=0\), which implies that \(^{2}_{f,B}=0\) for all \(B n\) since \(^{2}_{f,B}\) is non-increasing w.r.t \(B\). Note interpolation implies that the global minimizer of \(f\) is also a minimizer of each individual function \(f_{i}\).

### SGD with stochastic polyak stepsize

Loizou et al.  propose to set the stepsize \(_{t}\) as: \(_{t}=2}(_{t})-f^{}_{i_{t}}}{|| f_{i_{ t}}(_{t})||^{2}}\), which is well known as the Stochastic Polyak stepsize (SPS). In addition to SPS, they also propose a bounded variant SPS\({}_{}\) which has the form \(_{t}=2}(_{t})-f^{}_{i_{t}}}{||  f_{i_{t}}(_{t})||^{2}},_{b}}\) where \(_{b}>0\). Both algorithms require the input of the exact \(f^{}_{i_{t}}\) which is often unavailable when the batch size \(B>1\) or when the interpolation condition does not hold. Orveto et al.  removes the requirement for \(f^{}_{i_{t}}\) and propose to set \(_{t}\) as: \(_{t}=}\{}(_{t})-^{ }_{i_{t}}}{|| f_{i_{t}}(_{t})||^{2}},_{t-1}\}\) for \(t 1\) (DecSPS), where \(_{0}>0\) is a constant and \(^{}_{i_{t}}\) is an input lower bound such that \(^{}_{i_{t}} f^{}_{i_{t}}\). In contrast to the exact optimal function value, a lower bound \(^{}_{i_{t}}\) is often available in practice, in particular for machine learning problems when the individual loss functions are non-negative. We henceforth denote the estimation error by:

\[^{2}_{f,B}:=_{i_{t}}[f^{}_{i_{t}}-^{}_{i_{ t}}]\,.\] (3)

For convex smooth functions, SPS achieves a fast convergence up to a neighborhood of size \((^{2}_{f,B})\) and its variant SPS\({}_{}\) converges up to \((^{2}_{f,B}_{b}/)\) where \(=\{,_{b}\}\). Note that the size of the neighborhood cannot be further reduced by choosing an appropriate \(_{b}\). In contrast, DecSPS converges at the rate of \((1/)\) which matches the standard result for SGD with decreasing stepsize. However, the strictly decreasing \((1/)\) stepsize schedule hurts its performance in interpolated settings. For example, DecSPS has a much slower \((1/)\) convergence rate compared with the fast \(((-T/L))\) rate of SPS when optimizing strongly-convex objectives. Therefore, both algorithms do not have the _robust_ convergence property (achieving fast convergence guarantees in both interpolated and non-interpolated regimes) and we aim to fill this gap. See Figure 1 for a detailed illustration of the non-robustness of SPS and DecSPS.

Adaptive SGD with polyak stepsize and line-search

In this section, we introduce and analyze two adaptive algorithms to solve problem (1).

### Proposed methods

**AdaSPS.** Our first stepsize is defined as the following:

\[_{t}=\{}(_{t})-_{i_{t}}^{ }}{c_{p}|| f_{i_{t}}(_{t})||^{2}}^{t}f_{i_{s}}(_{s})-_{i_{s}}^{}}},_{t-1}\}\,, _{-1}=+\,,\] (AdaSPS)

where \(_{i_{t}}^{}\) is an input parameter that must satisfy \(_{i_{t}}^{} f_{i_{t}}^{}\) and \(c_{p}>0\) is an input constant to adjust the magnitude of the stepsize (we discuss suggested choices in Section 5).

AdaSPS can be seen as an extension of DecSPS. However, unlike the strict \((1/)\) decreasing rule applied in DecSPS, AdaSPS accumulates the function value difference during the optimization process which enables it to dynamically adapt to the underlying unknown interpolation settings.

**AdaSLS.** We provide another stepsize that can be applied even when a lower bound estimation is unavailable. The method is based on line-search and thus is completely parameter-free, but requires additional function value evaluations in each iteration:

\[_{t}=\{}{c_{l}^{t} _{s}|| f_{i_{s}}(_{s})||^{2}}},_{t-1}\}\,, _{-1}=+\,,\] (AdaSLS)

where \(c_{l}>0\) is an input constant, and the scale \(_{t}\) is obtained via stardard Armijo backtracking line-search (see Algorithm 4 for further implementation details in the Appendix D) such that the following conditions are satisfied:

\[f_{i_{t}}(_{t}-_{t} f_{i_{t}}(_{t})) f_{i _{t}}(_{t})-_{t}|| f_{i_{t}}(_{t})||^{2} _{t}_{},\ 0<<1\,\] (4)

for line-search parameters \(_{}\) and \(\). By setting the decreasing factor \(\) defined in Algorithm 4, one can show that \(_{t}(,_{})\). We give a formal proof in Lemma 16 in Appendix A.2.

**Discussion.** Our adaptation mechanism in AdaSPS/AdaSLS is reminiscent of AdaGrad type methods, in particular to AdaGrad-Norm, the scalar version of AdaGrad, that aggregates the gradient norm in the denominator and takes the form \(_{t}=}{^{t}|| f_{i_{t}}(_{s})|| ^{2}+b_{0}^{2}}}\) where \(c_{g}>0\) and \(b_{0}^{2} 0\).

The primary distinction between AdaSPS and AdaSLS compared to AdaGrad-Norm is the inclusion of an additional component that captures the curvature information at each step, and not using squared gradient norms in AdaSPS. In contrast to the strict decreasing behavior of AdaGrad-Norm, AdaSPS and AdaSLS can automatically mimic a constant stepsize when navigating a flatter region.

Vaswani et al.  suggest using line-search to set the stepsize for AdaGrad-Norm which takes the form \(_{t}=}{^{t}|| f_{i_{t}}(_ {s})||^{2}}}\) where \(_{t}_{t-1}\) is required for solving non-interpolated convex problems. While this stepsize is similar to AdaSLS, the scaling of the denominator gives a suboptimal convergence rate as we demonstrate in the following section.

### Convergence rates

In this section, we present the convergence results for AdaSPS and AdaSLS. We list the helpful lemmas in Appendix A. The proofs can be found in Appendix B.

**General convex.** We denote \(\) to be a convex compact set with diameter \(D\) such that there exists a solution \(^{}\) and \(_{,}||-||^{2} D ^{2}\). We let \(_{}\) denote the Euclidean projection onto \(\). For general convex stochastic optimization, it seems inevitable that adaptive methods require the bounded iterates assumption or an additional projection step to prove convergence due to the lack of knowledge of problem-dependent parameters [12; 15]. Here, we employ the latter solution by running projected stochastic gradient descent (PSGD):

\[_{t+1}=_{}(_{t}-_{t} f_{i_{t}}( _{t})).\] (5)

**Theorem 1** (General convex).: _Assume that \(f\) is convex, each \(f_{i}\) is L-smooth and \(\) is a convex compact feasible set with diameter \(D\), PSGD with AdaSPS or AdaSLS converges as:_

\[():\ [f(}_{T})-f^{ }]&^{2}}{T}+^{2}+_{f,B}^{2}}}{}\,\\ ():\ [f(}_{T})-f^{}]& ^{2}}{T}+_{f,B}}{}\,\] (6)

_where \(}_{T}=_{t=0}^{T-1}_{t}\), \(_{p}=(2c_{p}LD^{2}+})\) and \(_{l}=}, }}c_{l}D^{2}+}\)._

As a consequence of Theorem 1, if \(_{f,B}^{2}=_{f,B}^{2}=0\), then PSGD with AdaSPS or AdaSLS converges as \(()\). Suppose \(_{}\) is sufficiently large, then picking \(c_{p}^{}=}}\) and \(c_{l}^{}=}{}}\) gives a \((}{T})\) rate under the interpolation condition, which is slightly worse than \(_{0}-^{}\|^{2}}{T}\) obtained by SPS and SLS but is better than \((}{})\) obtained by DecSPS. If otherwise \(_{f,B}^{2}>0\), then AdaSPS, AdaSLS, and DecSPS converge as \((1/)\) which matches the rate of Vanilla SGD with decreasing stepsize. Finally, AdaGrad-Norm gives a similar rate in both cases while AdaGrad-Norm with line-search  shows a suboptimal rate of \((D^{4}}{T}+L^{3/2}}{})\). It is worth noting that SPS, DecSPS and SLS require an additional assumption on individual convexity.

**Theorem 2** (Individual convex+interpolation).: _Assume that \(f\) is convex, each \(f_{i}\) is convex and \(L\)-smooth, and that \(_{f,B}^{2}=_{f,B}^{2}=0\), by setting \(c_{p}=^{}}{}(_{0})-f_{0}^{}}\) and \(c_{l}=^{}}{}\| f_{0}(_{0})\|^{2}}\) with constants \(c_{p}^{} 1\) and \(c_{l}^{} 1\), then for any \(T 1\), SGD (no projection) with AdaSPS or AdaSLS converges as:_

\[[f(}_{T})-f^{}] 4L(c_{p}^{})^{2}\,_{i_{0}}_{0} -^{}||^{2}}{f_{i_{0}}(_{0})-f^{}} _{0}-^{}||^{2}}{T}\,\] (7)

_and_

\[[f(}_{T})-f^{}] ^{})^{2}}{^{3}L^{2}\{,_{ }\}}\,_{i_{0}}_{0}-^{} ||^{2}}{_{0}|| f_{i_{0}}(_{0})||^{2}} _{0}-^{}||^{2}}{T}\.\] (8)

_where \(}_{T}=_{t=1}^{T}_{t}\)._

The result implies that the bounded iterates assumption is not needed if we have both individual convexity and interpolation by picking \(c_{p}\) and \(c_{l}\) to satisfy certain conditions that do not depend on unknown parameters. To our knowledge, no such result exists for stepsizes from the AdaGrad family. It is worth noting that the \(\) operator defined in AdaSPS or AdaSLS is not necessary in the proof.

**Remark 3**.: _We note that for non-interpolated problems, AdaSPS only requires the knowledge of \(_{i_{t}}^{}\) while the exact \(f_{i_{t}}^{}\) is needed under the interpolation condition. We argue that in many standard machine learning problems, simply picking zero will suffice. For instance, \(f_{i_{t}}^{}=0\) for over-parameterized logistic regression and after adding a regularizer, \(_{i_{t}}^{}=0\)._

**Strongly convex.** We now present two algorithmic behaviors of AdaSPS and AdaSLS for strongly convex functions. In particular, We show that 1) the projection step can be removed as shown in DecSPS, and 2) if the interpolation condition holds, the \(\) operator is not needed and the asymptotic linear convergence rate is preserved. The full statement of Lemma 4 can be found in Appendix B.2.

**Lemma 4** (Bounded iterates).: _Let each \(f_{i}\) be \(\)-strongly convex and L-smooth. For any \(t=0,,T\), the iterates of SGD with AdaSPS or AdaSLS satisfy: \(||_{t}-^{}||^{2} D_{}\), for a constant \(D_{}\) specified in the appendix in Equation (B.16)._

**Corollary 5** (Individual strongly convex).: _Assume each \(f_{i}\) is \(\)-strongly convex and \(L\)-smooth, Theorem 1 holds with PSGD and \(D\) replaced by SGD and \(D_{}\) defined in Lemma 4._

Although it has not been formally demonstrated that AdaGrad/AdaGrad-Norm can relax the assumption on bounded iterates for strongly convex functions, we believe that with a similar proof technique, this property still holds for AdaGrad/AdaGrad-Norm.

We next show that AdaSPS and AdaSLS achieve linear convergence under the interpolation condition.

**Theorem 6** (Strongly convex + individual convex + interpolation).: _Consider SGD with AdaSPS (AdaSPS) or AdaSLS (AdaSLS) stepsize. Suppose that each \(f_{i}\) is convex and \(L\)-smooth, \(f\) is \(\)-strongly convex and that \(_{f,B}^{2}=_{f,B}^{2}=0\). If we let \(c_{p}=^{}}{}(_{0})-f_{i_{0}}^{ }}}\) and \(c_{l}=^{}}{|| f_{i_{0}}(_{0})||^{2}}}\) with constants \(c_{p}^{} 1\) and \(c_{l}^{} 1\), then AdaSPS or AdaSLS converges as:_

\[[||_{T+1}-^{ }||^{2}]_{i_{0}}[(1-}(_{ 0})-f^{})}{(2c_{p}^{}L||_{0}-^{}|| )^{2}})^{T}]||_{0}-^{}||^{2}\,\] (9)

_and_

\[[||_{T+1}-^{ }||^{2}]_{i_{0}}1-^{2} \{,_{}\}_{0}|| f_{i_{0}}(_{0} )||^{2}}{(c_{l}^{}||_{0}-^{}||)^{2}} ^{T}||_{0}-^{}||^{2}\.\] (10)

The proof of Theorem 6 is presented in Appendix B.3. We now compare the above results with the other stepsizes. Under the same settings, DecSPS has a slower \((1/)\) rate due to the usage of \((1/)\) decay stepsize. While AdaGrad-Norm does have a linear acceleration phase when the accumulator grows large, to avoid an \((1/)\) slow down, the parameters of AdaGrad-Norm have to satisfy \(c_{g}<b_{0}/L\), which requires the knowledge of Lipschitz constant . Instead, the conditions on \(c_{p}\) and \(c_{l}\) for AdaSPS and AdaSLS only depend on the function value and gradient norm at \(_{0}\) which can be computed at the first iteration. SPS, SLS, and Vannilia-SGD with constant stepsize achieve faster linear convergence rate of order \((-T)\). It is worth noting that Vannila-SGD can further remove the individual convexity assumption.

**Discussion.** In non-interpolation regimes, AdaSPS and AdaSLS only ensure a slower \((1/)\) convergence rate compared with \((1/T)\) rate achieved by vanilla SGD with \((1/t)\) decay stepsize when optimizing strongly-convex functions . To our knowledge, no parameter-free adaptive stepsize exists that achieves such a fast rate under the same assumptions. Therefore, developing an adaptive algorithm that can adapt to both convex and strongly-convex functions would be a significant further contribution.

## 4 AdaSPS and AdaSLS with variance-reduction

Combining variance-reduction (VR) with adaptive Polyak-stepsize and line-search to achieve acceleration is a natural idea that has been explored in the last decade [49; 36]. However, it remains an open challenge as no theoretical guarantee has been proven yet. Indeed, Dubois-Taine et al.  provide a counter-example for intuitive line-search methods. In Appendix E we provide counter-examples of the classical SPS and its variants. The reason behind the failure is the biased curvature information provided by \(f_{i_{t}}\) that prevents global convergence. In this section, we introduce a novel framework to address this issue. Since there exists many variance-reduced stochastic gradient estimators, we focus on the classical SVRG estimator in this section, and our framework also applies to other estimators such as SARAH .

### Algorithm design: achieving variance-reduction without interpolation

It is known that adaptive methods such as SPS or SLS converge linearly on problems where the interpolation condition holds, i.e. \(f()\) with \(_{f,B}=0\).

For problems that do not satisfy the interpolation condition, our approach is to transition the problem to an equivalent one that satisfies the interpolation condition. One such transformation is to shift each individual function by the gradient of \(f_{i}()\) at \(^{}\), i.e. \(F_{i}()=f_{i}()-^{T} f_{i}(^{ })\). In this case \(f()\) can be written as \(f()=_{i=1}^{n}F_{i}()\) due to the fact that \(_{i=1}^{n} f_{i}(^{})=0\). Note that \( F_{i}(^{})= f_{i}(^{})- f_{i} (^{})=0\) which implies that each \(F_{i}()\) shares the same minimizer and thus the interpolation condition is satisfied (\(_{f,1}^{2}=0\)). However, \( f_{i}(^{})\) is usually not available at hand. This motivates us to design the following algorithm.

```
0:\(_{0}^{d}\), \(_{F}>0\), \(c_{p}>0\) or \(c_{l}>0\)
1: set \(_{0}=_{0}\), \(_{-1}=+\)
2:for\(t=0\) to \(T-1\)do
3: uniformly sample \(i_{t}[n]\)
4: set \(F_{i_{t}}()=f_{i_{t}}()+^{T}( f(_{t})- f_{i_{t}}(_{t}))+}{2}||-_{t}||^{2}\)
5:\(_{t}=}(_{t})-F_{i_{t}}^{}}{c_{p}||  F_{i_{t}}(_{t})||^{2}}^{t}F_{i_{s}}( _{s})-F_{i_{s}}^{}}},_{t-1}}\) (AdaSVRS)
6:\(_{t}=_{t}^{t}_{t}||  F_{i_{t}}(_{s})||^{2}}},_{t-1}}\) (AdaSVRS)2
7:\(_{t+1}=_{}_{t}-_{t} F_{i_ {t}}(_{t})\)
8:\(_{t+1}=_{t}&1-p_{t+1}\\ _{t}&p_{t+1}\)
9:return\(}_{T}=_{t=0}^{T-1}_{t}\) ```

**Algorithm 1** (Loopless) AdaSVRS and AdaSVRLS

### Algorithms and convergence

Inspired by the observation, we attempt to reduce the variance of the functions \(_{f,B}^{2}\) by constructing a sequence of random functions \(\{F_{i_{t}}()\}\) such that \(_{_{i=1}^{n}F_{i_{t}}(),B}^{2} 0\) as \(_{t}^{}\). However, directly applying SPS or SLS to \(\{F_{i_{t}}()\}\) still requires the knowledge of the Lipschitz constant to guarantee convergence. This problem can be solved by using our proposed AdaSPS and AdaSLS. The whole procedure of the final algorithm is summarized in Algorithm 1.

At each iteration of Algorithm 1, we construct a proxy function by adding two quantities to the minibatch function \(f_{i_{t}}()\), where \(}{2}||-_{t}||^{2}\) is a proximal term that helps improve the inherent stochasticity due to the partial information obtained from \(f_{i_{t}}()\). The additional inner product quantity is used to draw closer the minimizers of \(f_{i_{t}}()\) and \(f()\). Following [27; 33], the full gradient is computed with a coin flip probability. Note that Algorithm 1 still works with \(_{t}\) replaced with SVRG and AdaSVRG stepsize since \( F_{i_{t}}(_{t})= f_{i_{t}}(_{t})- f_{ i_{t}}(_{t})+ f(_{t})\), and thus this framework can be seen as a generalization of the standard VR methods. A similar idea can also be found in the works on federated learning with variance-reduction [32; 24; 37; 50].

**Theorem 7**.: _Assume each \(f_{i}\) is convex and \(L\) smooth and \(\) is a convex compact feasible set with diameter \(D\). Let \(p_{t}=\) with \(0 a<1\). Algorithm 1 converges as:_

\[ 28.452756pt[f(}_{T})-f ^{}]}}{T}2c_{p}(L+_{F})D^{2} +}^{2}\,\] (11) \[ 28.452756pt[f(}_{T})-f ^{}]}}{T}}{(1-)},}}c_{l}D ^{2}+}^{2}\,\] (12)

_where \(}_{T}=_{t=0}^{T-1}_{t}\)._

Suppose \(_{}\) is sufficiently large, then picking \(_{F}^{}=(L)\), \(c_{p}^{}=(}})\) and \(c_{l}^{}=(}{}})\) yields an \((}{T})\) rate which matches the \((_{0}-^{}||^{2}}{T})\) rate of full-batch gradient descent except for a larger term \(D^{2}\) due to the lack of knowledge of the Lipschitz constant.

**Corollary 8**.: _Under the setting of Theorem 7, given an arbitrary accuracy \(\), the total number of gradient evaluations required to have \([f(}_{T})-f^{}]\) in expectation is \(((1/)n+1/)\)._

The proved efficiency of stochastic gradient calls matches the optimal rates of SARAH /SVRG and AdaSVRG  but removes the artificially designed inner and outer loop size. However, note that Algorithm 1 requires an additional assumption on individual convexity. Unfortunately, we believe this assumption is necessary for the VR methods to work for the algorithms in the Polyak and line-search family since SPS/SLS has to assume the same condition for the proof in the interpolation settings.

**Discussion.** The classical SVRG with Armijo line-search (presented as Algorithm 6 in ) employs the same gradient estimator as SVRG but chooses its stepsize based on the returning value of line-search on the individual function \(f_{i}\). Similarly, SVRG with classical Polyak stepsize uses the individual curvature information of \(f_{i}\) to set the stepsize for the global variance-reduced gradient. Due to the misleading curvature information provided by the biased function \(f_{i}\), both methods have convergence issues. In constrast, Algorithm 1 reduces the bias by adding a correction term \(^{T}( f(_{t})- f_{i_{t}}(_{t}))\) with global information to \(f_{i}\) and then applying line-search or Polyak-stepsize on the variance-reduced functions \(F_{i_{t}}\). This difference essentially guarantees the convergence.

## 5 Numerical evaluation

In this section, we illustrate the main properties of our proposed methods in numerical experiments. A detailed description of the experimental setup can be found in Appendix F. We report the theoretically justified hyperparameter \(c_{p}^{}\) or \(c_{l}^{}\) as defined in Theorem 6 rather than \(c_{p}\) or \(c_{l}\) in the following.

**Synthetic data.** We illustrate the robustness property on a class of synthetic problems. We consider the minimization of a quadratic of the form: \(f()=_{i=1}^{n}f_{i}()\) where \(f_{i}()=(-_{i})^{T}A_{i}(- _{i})\), \(_{i}^{d}\) and \(A_{i}^{d d}\) is a diagonal matrix. We use \(n=50\), \(d=1000\). We can control the convexity of the problem by choosing different matrices \(A_{i}\), and control interpolation by either setting all \(\{_{i}\}\) to be identical or different. We generate a strongly convex instance where the eigenvalues of \(^{2}f()\) are between \(1\) and \(10\), and a general convex instance by setting some of the eigenvalues to small values close to zero (while ensuring that each \(^{2}f_{i}()\) is positive semi-definite). The exact procedure to generate these problems is described in Appendix F.

For all methods, we use a batch size \(B=1\). We compare AdaSPS and AdaSLS against DecSPS , SPS  and SLS  to illustrate the robustness property. More comparisons can be found in Appendix F.1. We fix \(c_{p}^{}=c_{l}^{}=1\) for AdaSPS/AdaSLS and use the optimal parameters for DecSPS and SPS. In Figure 1, we observe that SPS does not converge in the non-interpolated settings and DecSPS suffers from a slow \((1/)\) convergence on the two interpolated problems. AdaSPS and AdaSLS show the desired convergence rates across all cases which matches the theory. When the problems are non-interpolated, AdaSVRPS and AdaSVRLS illustrate faster convergence, which can be seen in Figure 2.

**Binary classification on LIBSVM datasets.** We experiment with binary classification on four diverse datasets from . We consider the standard regularized logistic loss: \(f()=_{i=1}^{n}(1+(-y_{i}_{i}^{ T}))+||||^{2}\) where \((_{i},y_{i})^{d+1}\) are features and labels. We defer the study of variance-reduction methods to Appendix F.2 for clarity of presentation. We benchmark against popular optimization algorithms including Adam , SPS , DecSPS , SLS  and AdaGradNorm . We fix \(c_{p}^{}=c_{l}^{}=1\) for AdaSPS/AdaSLS and pick the best learning rate from \(\{10^{i}\}_{i=-4,..,3}\) for SGD, Adam and AdaGrad-Norm. We observe that Adam, SPS and SLS have remarkable performances on duke with \(n=48\) and \(d=7129\), which satisfies interpolation. AdaSPS

Figure 2: Illustration of the accelerated convergence of AdaSVRPS and AdaSVRLS on quadratic loss without interpolation. Both algorithms require less gradient evaluations than AdaSPS and AdaSLS for optimizing non-interpolated problems. However, they are less efficient for solving interpolated problems. (Repeated 3 times. The solid lines and the shaded area represent the mean and the standard deviation.)

and AdaSLS consistently perform reasonably well on the other three larger datasets. It is worth noting that the hyper-parameters \(c_{p}^{}\) and \(c_{l}^{}\) are fixed for all the datasets, which is desired in practice.

**Deep learning task.** We provide a heuristic extension of AdaSPS to over-parameterized non-convex optimization tasks to illustrate its potential. We benchmark the convergence and generalization performance of AdaSPS (DL) 5 for the multi-class classification tasks on CIFAR10  and CIFAR100  datasets using ResNet-34 . More experimental details can be found in Appendix G. We demonstrate the effectiveness of AdaSPS (DL) in Figure 4.

**Discussion.** AdaSPS and AdaSLS consistently demonstrate robust convergence across all tasks and achieve performance on par with, if not better than, the best-tuned algorithms. Consequently, it is reliable and convenient for practical use.

## 6 Conclusion and future work

We proposed new variants of SPS and SLS algorithms and demonstrated their robust and fast convergence in both interpolated and non-interpolated settings. We further accelerate both algorithms for convex optimization with a novel variance reduction technique. Interesting future directions may include: accelerating AdaSPS and AdaSLS with momentum, developing effective robust adaptive methods for training deep neural networks, designing an adaptive algorithm that gives a faster rate \((1/T)\) under strong convexity, extensions to distributed and decentralized settings.