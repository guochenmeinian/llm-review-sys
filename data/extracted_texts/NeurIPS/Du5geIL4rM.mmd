# Training Machine Learning Models with Ising Machines

Sayantan Pramanik\({}^{1,2}\)1  Kaumudibikash Goswami\({}^{3}\)  Sourav Chatterjee\({}^{1}\)

M Girish Chandra\({}^{1}\)

\({}^{1}\)TATA Consultancy Services, India \({}^{2}\)Indian Institute of Science

\({}^{3}\)QICI, The University of Hong Kong

Correspondence to <sayantan.pramanik@tcs.com, sayantanp@iisc.ac.in>

###### Abstract

In this study, we use Ising machines to help train machine learning models by employing a suitably tailored version of opto-electronic oscillator-based coherent Ising machines with clipped transfer functions to perform trust region-based optimisation with box constraints. To achieve this, we modify such Ising machines by including non-symmetric coupling and linear terms, modulating the noise, and introducing compatibility with convex-projections. The convergence of this method, dubbed \(i\)Trust has also been established analytically. We validate our theoretical result by using \(i\)Trust to optimise the parameters in a quantum machine learning model in a binary classification task. The proposed approach achieves similar performance to other second-order trust-region based methods while having a lower computational complexity. Our work serves as a novel application of Ising machines and allows for a unconstrained optimisation problems to be performed on energy-efficient computers with non von Neumann architectures.

## 1 Introduction

Traditionally, the utility of Ising machines has been limited to solving combinatorial optimisation problems [1; 2; 3] with polynomial resources by mapping them onto ground-state search problems of the Ising model [4; 5], using them as machine learning models [6; 7; 8; 9], or modelling optical neural networks . While a variety of approaches for realizing this model of artificial spins network has been demonstrated in literature [11; 12; 13], the approach of employing opto-electronic-oscillators (OEOs) for building a coherent Ising machine (CIM)  has been lately gaining a lot of attention because of its cost-effective implementation, ambient operation, and scope for miniaturization .

In this work, we present a novel application of OEO-CIMs to unconstrained optimisation, and provide analytical proof of convergence of Ising machines to perform trust region-based optimization [16; 17; 18]. We refer to this technique as \(i\)Trust (Ising machines for trust-region optimisation). Along with the other aforementioned benefits of OEO-CIMs, the main advantage of \(i\)Trust stems from avoiding matrix-inversion and Cholesky decomposition of the Hessian. This opens up a new avenue of applications where the Ising machines may be used to optimise any parameterised, unconstrained objective function \(f:^{n}\). We denote the parameters of the objective function \(f()\) with the vector \(^{n}\). Particularly, using \(i\)Trust, we aim to find the optimal point \(^{*}\) that minimises the objective function:

**Problem 1**.: \[f(^{*}):=_{^{n}}f(),\] (1)

where \(^{*}\) satisfies second-order optimality conditions , under the following assumption :

**Assumption 1**.: _If \(^{(0)}\) is the starting point of an iterative algorithm, then the function \(f()\) is bounded below on the level set \(=\{\,|\,f() f(^{(0)})\}\) by some value \(f^{*}\), such that \(f^{*} f()\ \ \). Further, \(f\) is twice continuously differentiable on \(\)._

This allows us to use \(i\)Trust as an optimisation procedure for training models in traditional machine learning (ML) [19; 20; 21], quantum ML (QML) [22; 23; 24], quantum-inspired ML (QiML) , and variational quantum algorithmic (VQA)  models. Such optimisation problems are conventionally tackled by digital computers based on von Neumann architecture, leading to substantial memory and energy consumption, also known as 'von Neumann bottleneck' . In contrast, since \(i\)Trust is based on Ising machines, it may potentially lead to more energy-efficient protocols [28; 29; 1] with an increased clock-speed . This paper (along with a contemporary studies in  and  - which use analog thermodynamic computers to perform natural gradient descent, and quantum linear solvers [33; 34; 35] to calculate the Newton-update, respectively) hopes to open up new avenues of research where benefits of new-compute paradigms are reaped not only by using them as ML models, but also by employing them to aid in the training of models.

The remainder of this extended abstract is organised as follows: we propose essential modifications to a specific type of CIMs to make them compatible for trust-region optimisation in Section 2, and analytically examine its performance on convex objective functions with bounded gradients, and on smooth, locally-Polyak-Lojasiewicz (PL)  functions in Section 2.1. We describe the proposed algorithm \(i\)Trust in Section 3, before showing its convergence to second-order optimal solutions of Problem 1 in Theorem 3. We then proceed to demonstrate its efficacy through numerical experiments in Section 4 Conclusions and future outlook are in Section 5.

## 2 Economical Coherent Ising Machine

For \(i\)Trust, we consider the poor man's CIM introduced in  with clipped nonlinearity , and refer to it as the Economical CIM (ECIM). It is then modified to find \(\)-suboptimal solutions of the following problem with \(\) as the coupling-matrix, and \(\) as the external field:

**Problem 2**.: \[_{[-,]^{n}}(E()}{{=}},+ ,)\] (2)

Inspired by an earlier work , our modifications include setting \(=1\) and viewing \(\) as the step-size in equation 8 of . The variance of the injected noise is modulated, and varying step-sizes \(_{k}\) are considered to facilitate better convergence. Provisions for accommodating non-symmetric coupling and linear terms are also made without relying on ancillary spins [39; 15]. The clipping voltage is set to \(\), and finally, the ECIM is made compatible with the definition of projection to the convex box \(=[-,]^{n}\). As a result, the iterative update equation of the modified ECIM is given by:

\[^{(k+1)}=_{}(^{(k)}-_{k}( E( ^{(k)})-^{(k)})),\] (3)

where \(^{(k)}(,^{2})\), and \(_{}()\) is the projection operator to \(\).

### Convergence of ECIM

In this section, we present the convergence-results of the modified ECIM for convex or locally-Polyak-Lojasiewicz (PL) \(E()\). While PL may not be as popular a condition as convexity, it is definitely more general. For instance, PL (or PL\({}^{*}\)) functions have been shown to include neural networks with ReLU activations and quadratic losses where convexity cannot be assumed [40; 41; 42]. Further,  argues and proves that among Lipschitz-smooth functions such as _strongly convex_, _essentially strongly convex_, _weakly strongly convex_, and functions obeying the _restricted secant inequality_, PL functions entail the weakest assumptions. A more detailed exposition on the relations and implications between function-classes may be found in Theorem 2 of . Furthermore, it is known that PL functions obey the Polyak-Lojasiewicz inequality. We, however, require the objective function to be PL. _locally_ on the constraint set \(\), i.e., for some \(>0\) and for all \(\),

\[|| E()||_{2}^{2} 2(E()-E^{*}).\] (4)We now state the convergence results through the following informal Theorems. Their formal statements and proofs have not been included in adherence to the page-limits.

**Theorem 1** (Informal).: _For convex \(E()\) with bounded gradients, the ECIM in equation (3) finds an \(\)-suboptimal solution to Problem 2 in \(\) with fixed step-sizes in \((}{{^{2}}})\) iterations. With diminishing step-sizes such that \(_{k=0}^{}_{k}=\) and \(_{k=0}^{}_{k}^{2}<\), \(_{k}(E(^{(k)})-E^{*})=0\), where \(E^{*}=_{}E()\)._

**Theorem 2** (Informal).: _For smooth \(E()\) that obeys the PL inequality locally, the ECIM in equation (3) finds an \(\)-suboptimal solution to Problem 2 in \(\) with fixed step-sizes in \(((}{{}}))\) iterations._

If \(}}\) is the output of the ECIM, then the above results may be unified into the following equation for some constant \(c(0,1]\), as suggested in :

\[-E(}}) c|E(^{*})|.\] (5)

## 3 \(\)Trust

Very briefly, the update \(_{(t)}^{*}\) to \(^{(t)}\) at the iteration \(t\) of a Newton-like trust-region method is found from the minimiser of:

**Problem 3**.: \[_{||||_{2}_{t}}(m_{t}()}{{=}} f(^{(t)}),+ ,(^{(t)}) ),\] (6)

where \( f(^{(t)})\) and \((^{(t)})\) are the gradient and Hessian of \(f\) at \(^{(t)}\), respectively. If the radius of the trust-region at iteration \(t\) is \(_{t}\), then the feasible set, which is a ball1, is represented with \(_{t}=\{^{n}\,||-^{(t)}||_{2} _{t}\}\).

\[||||_{2},\] (7)

 where \(=(d_{1},,d_{n})\) with \(d_{i} 0\). The elements \(d_{i}\) are adjusted according to the _sensitivity_ of \(f()\) to \(_{i}\): if \(f()\) varies highly with a small change in \(_{i}\), then a large value of \(d_{i}\) is used; and vice versa ., is represented with \(_{t}=\{^{n}\,||-^{(t)}||_{2} _{t}\}\).

A major disadvantage of using the method proposed in Algorithm 3.2 stated in  to find \(_{(t)}^{*}\) is the repeated requirement for Cholesky decomposition and inversion of the Hessian, both of which are in \((n^{3})\). This becomes prohibitively expensive for problems where \(n\) is large, for instance machine learning models with millions of parameters. We aim to alleviate this problem by using the enhanced ECIM to find \(_{(t)}^{*}\). We achieve this by exploiting the structural similarity Problems 2 and 3. Specifically, at each iteration \(t\), \(\) is set to \((^{(t)})\), \(\) to \( f(^{(t)})\), and \(\) to \(_{t}\). Here, the importance of the inclusion of linear terms in the Ising machine becomes clear, without which the gradient \( E(^{(k)})\) could not have been provided to the ECIM without additional overheads in the form of ancillary spins .

**Remark 1**.: _It is interesting to note that if the coupling matrix \(^{(t)}\) is positive semidefinite at the iteration \(t\), then as per the definition of convexity, the objective function of the trust-region subproblem is convex. Additionally, since the coupling matrix is equal to the Hessian \((^{(t)})\), this also implies that the objective function \(f\) is convex in the region around \(^{(t)}\). Thus, in a convex region of the original problem, the result in Theorem 1 becomes applicable for the ECIM._

Further, we distinguish between the minimisers of \(E_{t}()\) and \(m_{t}()\) on the sets \(_{t}\) and \(_{t}\) by denoting them with \(_{(t)}^{*}\) and \(_{(t)}^{*}\), respectively.

**Remark 2**.: _We would like to emphasize that the box \(_{t}\) and the ball \(_{t}\) share a common centre \(^{(t)}\), and by design, the side-length of the box is set equal to the diameter of the ball at each iteration. Thus, the ball is contained completely within the box: \(_{t}_{t}\)2. Now, since the objective function of the Problems 3 and 2 are identical, and the constraint set of the former is contained in that of the latter, we have:_

\[E_{t}(_{(t)}^{*}) m_{t}(_{(t)}^{*}).\] (8)

_This means that if the ECIM and the Algorithm 3.14 in  can both reach near-optimal solutions of their respective optimisation problems, then the objective value obtained by the ECIM is guaranteed to be better. This results in a higher reduction in the value of \(f()\) at each iteration._

We name this technique of using the ECIM for trust-region optimisation as \(i\)Trust. The workflow for \(i\)Trust has been portrayed in Algorithm 1, which draws inspiration from, and is an amalgamation of, Algorithms 4.1 and 4.2 of  and , respectively.

``` input: initial point \(^{(0)}^{n}\); maximum trust-region radius \(_{}>0\); initial radius \(_{0}(0,_{}]\); thresholds on \(_{t}\): \(0<<<1\); radius-updated parameters \(_{1}<1\) and \(_{2}>1\); noise variance \(^{2}\); sequence of step-sizes \((_{k})\); and number of iterations \(T\) and \(K\)
1begin
2for\(t[T]\)do
3 evaluate \( f(^{(t)})\) and \((^{(t)})\);
4\(^{(t)}(^{(t)})\);
5\(^{(t)} f(^{(t)})\);
6\(_{t}_{t}\);
7 initialise \(^{(0)}\) randomly in \(_{t}=[-_{t},_{t}]^{n}\);
8for\(k[K]\)do
9 sample \(^{(k)}(,^{2})\);
10\(^{(k+1)}=_{_{t}}(^{(k)}-_{k}( E _{t}(^{(k)})-^{(k)}))\);
11 end for
12 calculate \(_{t}=^{(t)}+^{(K)})-f(^{(t)})}{E_{t}( ^{(K)})}\);
13if\(_{t}<\)then
14\(_{t+1}=_{1}_{t}\);
15continue;
16
17else
18if\(_{t}>(1-)\) and \(||^{(K)}||_{}=_{t}\)then
19\(_{t+1}=(_{2}_{t},_{})\);
20else
21\(_{t+1}=_{t}\);
22
23 end if
24
25 end for
26
27if\(_{t}>\)then
28\(^{(t+1)}=^{(t)}+^{(K)}\);
29else
30\(^{(t+1)}=^{(t)}\);
31 end if
32
33 end for
34return\(^{(T)}\)
35
36 end for ```

**Algorithm 1**\(i\)Trust

We claim that this technique of employing ECIMs to solve the subproblem of trust-region methods converges (or tends to converge to) second-order optimal solutions of Problem 1 in \(\). This claim is formalised in the form of the following theorem , the proof of which has been omitted for brevity:

**Theorem 3** (Convergence of \(i\)Trust).: _Let assumption 1 be true, and let \((^{(t)})\) be the sequence of iterates generated by Algorithm 1 such that equation (5) is satisfied at each iteration. Then we have that:_

\[_{t}|| f(^{(t)})||_{2}=0.\] (9)

_Moreover, if \(\) is compact, the either Algorithm 1 terminates at a point \(^{(T)}\) where \( f(^{(T)})=0\) and \((^{(T)}) 0\); or \((^{(t)})\) has a limit point \(^{*}\) such that \( f(^{*})=0\) and \((^{*}) 0\)._

## 4 Empirical Evaluation

In this section, we will demonstrate the efficacy fo the proposed method through numerical experiments. Specifically, \(i\)Trust will be applied to optimise the parameters in a quantum machine learning (QML) model that performs binary classification. QML is another instance where an _alternate-compute_ paradigm is used to enhance machine learning through the introduction of quantum models as hypothesis functions that are non-trivial to simulate classically. QML models have been found to provide advantages in laboratory setting in terms of the number of parameters , the volume of training data required , and the number of iterations/epochs the models are trained for . Nevertheless, in recent times, the use of variational models  for QML has garnered some criticism that questions their advantage , especially due to the presence of barren plateaus [47; 48] and classical simulablity [49; 50]. However, proving a quantum-advantage in ML is far from the scope of this study, and neither does the \(i\)Trust algorithm alleviate the issues of barren plateaus or classical simulability in QML.

Here, we aim to enhance the training of a small QML model to perform classification of the Iris dataset . Since our focus is only on facilitating the training of models and not on their general-isability, only the training error at each iteration will be observed and reported as a measure of the performance of \(i\)Trust. The Iris dataset consists of four floral-features that may be used to categorise the flowers into three distinct classes, only the first two of which have been used here.

The details of the quantum classification model are as follows: the four features were encoded into the states of four qubits using AngleEmbedding with a combination of Hadamard and \(R_{Z}\) gates on each of the qubits. The features were first scaled to lie in the range of \([0,]\) before being passed as parameters into the \(R_{Z}\) gates. Subsequently, three layers of the BasicEntanglingLayers ansatz from Pennylene  were appended to the circuit. The gates in the ansatz contain learnable parameters that were optimised. Finally, the expectation value of the Pauli-\(Z\) operator of the first qubit was measured. To calculate the _empirical risk_, the label for each datapoint was expressed as \(\{ 1\}\) and the mean squared error was evaluated over the entire training set.

The performance of \(i\)Trust was benchmarked against those of two other algorithms: gradient-descent (GD) which only uses the first-order derivatives; and Algorithm 3.2 from  - henceforth refereed to as More & Sorensen (MnS) - which like \(i\)Trust additionally requires the Hessian. Hence, GD requires only \(n\) evaluations of the quantum circuit at each iteration to estimate the gradient using the Parameter-Shift Rule , while the second-order methods need \(2n^{2}+n\) circuit executions (\(2\) for the gradients of each coordinate, \(4\) for each of the off-diagonal terms of the Hessian, and an additional \(1\) for the diagonal terms ). Each of the algorithms was run for 100 iterations with the same initial point; and the experiment was repeated \(10\) times with different starting points sampled from a uniform distribution. A learning rate of \(0.1\) was used for GD. The hyperparameters for the second-order methods were: \(_{}=_{0}=0.5\), \(=0.1\), \(=-0.001\), \(_{1}=0.75\), and \(_{2}=1.25\).In addition, for \(i\)Trust, \(\) was set to \(0.5\), with \(K=10\).

### Numerical Results

The results of the aforementioned experiments have been reported in Figure 1, where 1 shows the training loss at each iteration of training. The bold lines denote the mean, while the shaded regions indicate the standard deviation around the mean across the \(10\) experiments. It may be noted that the second-order methods outperformed the first-order one, as expected. Between Mns and \(i\)Trust, the former obtained a quicker reduction in loss at the initial iterations, with the latter catching up soon. As the training progressed, \(i\)Trust converged to a marginally lower value of loss compared to MnS and was found to be more stable, owing to its lower standard deviation.

However, as detailed earlier, the second-order methods come at an increased overhead of calculation the Hessian for QML models. To check if this overhead eclipses the benefit of a reduced number of iterations, Figure (b)b demonstrates the training loss against the total number of circuit-executions on a logarithmic scale. It is apparent from the plot that the second-order methods perform better towards the initial epochs and that the performance of MnS is slightly better than that of \(i\)Trust. But, one may recall that MnS requires Cholesky decomposition of the Hessian, whose complexity scales cubically with \(n\). In comparison, \(i\)Trust forgoes this extra complexity while still retaining comparable performance.

With the above results in mind, we propose a training schedule where \(i\)Trust is used in the initial phase to get a quick reduction in the training loss, followed by the utilisation of GD until convergence. This method is markedly distinct from the existing convention of starting with GD to reach a _Newton's region_, followed by the use of second-order Newton's method. It must be noted at this point that evaluating the performance against the number of circuit/function executions would be unnecessary for models where the gradients and Hessians may be calculated (or estimated) with similar complexity as the function execution. In such cases, the advantages of \(i\)Trust (and MnS) become more pronounced.

## 5 Conclusions and Outlook

In this paper, we introduced \(i\)Trust, an algorithm that leverages Ising machines for trust-region based optimisation. In doing so, we proposed necessary modifications to the Ising machine, and proved the feasibility and convergence of \(i\)Trust. The use of Ising machines provides the potential for higher clock-speeds and reduced energy-consumption compared to conventional approaches. We validated our theoretical results by introducing the \(i\)Trust as an optimiser in a simple quantum machine learning model to perform binary classification, and compared its performance against other first and second-order methods. We find that \(i\)Trust delivers similar performance to the other trust-region based method, but has the advantage of avoiding Hessian inversion and Cholesky decomposition. In this way, we extend the previously allowed class of optimisation problems using the Ising machines and open up the possibility of training machine learning models with new compute paradigms.

Possible future directions may include generalising the ECIM for non-convex and non-PL objective functions. Variants of \(i\)Trust can also be constructed that are compatible with natural gradient descent [55; 56], by replacing the Hessian with the Fisher Information Matrix. \(i\)Trust may be further augmented by zeroth order methods like SPSA  in scenarios where evaluation of the gradients, Hessian, and Fisher information matrix is computationally expensive . Lastly, the advantages of the ECIM over noisy projected gradient descent for the subproblem minimisation in terms of the clock-speed and energy-consumption can also be examined. We hope that this paper opens up new avenues of research in the analytical and empirical exploration of new applications of Ising machines.

Figure 1: Fig. (a) shows the training loss at each iteration of training; while Fig. (b) reports the training loss against the number of circuits executed on a logarithmic scale.