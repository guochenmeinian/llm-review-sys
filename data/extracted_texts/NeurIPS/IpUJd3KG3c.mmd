# Analytic Gaussian mechanism (Balle & Wang, 2018).

Improving the Privacy and Practicality of Objective Perturbation for Differentially Private Linear Learners

 Rachel Redberg

UC Santa Barbara

&Antti Koskela

Nokia Bell Labs

&Yu-Xiang Wang

UC Santa Barbara

###### Abstract

In the arena of privacy-preserving machine learning, differentially private stochastic gradient descent (DP-SGD) has outstripped the objective perturbation mechanism in popularity and interest. Though unrivaled in versatility, DP-SGD requires a non-trivial privacy overhead (for privately tuning the model's hyperparameters) and a computational complexity which might be extravagant for simple models such as linear and logistic regression. This paper revamps the objective perturbation mechanism with tighter privacy analyses and new computational tools that boost it to perform competitively with DP-SGD on unconstrained convex generalized linear problems.

## 1 Introduction

The rise of deep neural networks has transformed the study of differentially private learning no less than any other area of machine learning. Differentially private stochastic gradient descent (DP-SGD) (Song et al., 2013; Bassily et al., 2014; Abadi et al., 2016) has thus gained widespread appeal as a versatile framework for privately training deep learning models.

How does DP-SGD fare on simpler models such as linear and logistic regression? The verdict is unclear. Clearly an algorithm capable of privately optimizing non-convex functions represented by millions of parameters is up to the computational task of fitting a linear model. A more pressing concern is that DP-SGD is up to _too_ much. Look, for example, at the algorithm's computational complexity: DP-SGD requires \(O(n^{2})\) steps to achieve the optimal excess risk bounds for DP convex empirical risk minimization (Bassily et al., 2014).

DP-SGD furthermore takes after its non-private counterpart in sensitivity to hyperparameters. A poor choice of learning rate or batch size, for instance, could lead to suboptimal performance or slow convergence. There are well-established procedures for hyperparameter optimization that typically involve evaluating the performance of the model trained using different sets of candidate hyperparameters. But with privacy constraints, there is a catch: tuning hyperparameters requires multiple passes over the training dataset and thereby constitutes a privacy cost.

At best, existing work tends to circumvent this obstacle by optimistically assuming the availability of a public auxiliary dataset for hyperparameter tuning. More often the procedure for private hyperparameter selection is left largely to the reader's imagination. Only recently have Liu & Talwar (2019) and subsequently Papernot & Steinke (2022) studied how to obtain tighter privacy loss bounds for this task beyond standard composition theorems.

In the meantime, objective perturbation (Chaudhuri et al., 2011; Kifer et al., 2012) has been to some extent shelved as a historical curiosity. Sifting through the literature, we find that opinions are divided: some tour objective perturbation as "[o]ne of the most effective algorithms for differentially private learning and optimization" (Neel et al., 2020), whereas other works (Wang et al., 2017) dismiss objective perturbation as being impractical and restrictive. Some empirical evaluations (Yu et al., 2019; McKenna et al., 2021) suggest that DP-SGD often achieves better utility in practice than does objective perturbation; others (Iyengar et al., 2019) report the opposite.

Our goal in this paper is to lay some of this debate to rest and demonstrate that for generalized linear problems in particular, objective perturbation can outshine DP-SGD.

### Our Contributions

* **We establish an improved \((,)\)-DP bound for objective perturbation via privacy profiles, a modern tool for privacy accounting that bounds the hockey-stick divergence.** The formula can be computed numerically using only calls to Gaussian CDFs. We further obtain a _dominating pair_ of distributions as defined by Zhu et al. (2022) which enables tight composition and amplification by subsampling of the privacy profiles.
* **We present a novel Renyi differential privacy (RDP) (Mironov, 2017) analysis of the objective perturbation mechanism. Using this analysis, we show empirically that objective perturbation performs competitively against DP-SGD with "honestly" 1-tuned hyperparameters.** The tightest analyses to date of private hyperparameter tuning are the RDP bounds derived in Papernot & Steinke (2022). This tool allows us to empirically evaluate objective perturbation against DP-SGD on a level playing field (Section 5).
* **We fix a decade-old oversight in the privacy analysis of objective perturbation.** Existing literature overlooks a nuanced argument in the privacy analysis of objective perturbation, which requires a careful treatment of the dependence between the noise vector and the private minimizer. Without assuming GLM structure, the privacy bound of objective perturbation is subject to a dimensional dependence that has gone unacknowledged in previous work2. * **We introduce computational tools that expand the applicability of objective perturbation to a broader range of loss functions.** The privacy guarantees of objective perturbation require the loss function to have bounded gradient. Our proposed framework extends the Approximate Minima Perturbation framework of Iyengar et al. (2019) to take any smooth loss function as a blackbox, then algorithmically ensure that it has bounded gradient. We also provide a **computational guarantee**\(O(n n)\) on the running time of this algorithm, in contrast to the \(O(n^{2})\) complexity of DP-SGD for achieving information-theoretic limits.

### A Short History of DP Learning

Differentially private learning dates back to Chaudhuri et al. (2011), which extended the output perturbation method of Dwork et al. (2006) to classification algorithms and also introduced objective perturbation. In its first public appearance, objective perturbation required gamma-distributed noise; Kifer et al. (2012) provided a refined analysis of the mechanism with Gaussian noise, which is the entry point into our work.

Differentially private stochastic gradient descent (DP-SGD) (Song et al., 2013; Bassily et al., 2014; Abadi et al., 2016) brought DP into the fold of modern machine learning, allowing private training of models with arbitrarily complex loss landscapes that can scale to enormous datasets. DP-SGD adds Gaussian noise at every iteration to an aggregation of clipped gradients, and thus privacy analysis for DP-SGD often boils down to finding tight composition bounds (of the subsampled Gaussian mechanism).

The initial version of DP-SGD based on the standard strong composition (Bassily et al., 2014) is not quite practical, but that has changed, thanks to a community-wide effort in developing modern numerical privacy accounting tools in the past few years. These include the moments accountant that composes Renyi DP functions (Abadi et al., 2016; Wang et al., 2019; Mironov et al., 2019) and the Fourier accountant (also known as PLV or PLD accountants) that directly compose the _privacy profiles_(Sommer et al., 2019; Koskela et al., 2020; Gopi et al., 2021; Zhu et al., 2022). It is safe to conclude that the numerically computed privacy loss of DP-SGD using these modern tools is now very precise.

Because DP-SGD releases each intermediate model, the algorithm can stop after any number of iterations and simply accumulates privacy loss as it goes. In contrast, the privacy guarantees of objective perturbation hold only when the output of the mechanism is the _exact_ minima of the perturbed objective. This requirement is at odds with practical convex optimization frameworks which typically use first-order methods to search for an approximate solution.

To remedy this, Iyengar et al. (2019) proposed an approach to _approximately_ minimize a perturbed objective function while maintaining privacy. Approximate Minima Perturbation (AMP) was introduced as a tractable alternative to objective perturbation whose privacy guarantees permit the output to be an approximate (rather than exact) solution to the perturbed minimization problem. In this paper we extend AMP to a broader range of loss functions; Algorithm 1 can be viewed as a special case of AMP with a transformation of the loss function.

## 2 Preliminaries

### Differential Privacy

Differential privacy (DP) (Dwork et al., 2006) offers provable privacy protection by restricting how much the output of a randomized algorithm can leak information about a single data point.

DP requires a notion of how to measure similarity between datasets. We say that datasets \(Z\) and \(Z^{}\) are neighboring datasets (denoted \(Z Z^{}\)) if they differ by exactly one datapoint \(z\), i.e. \(Z^{}=Z\{z\}\) or \(Z^{}=Z\{z\}\) for some data entry \(z\).

**Definition 2.1** (Differential privacy).: A mechanism \(:\) satisfies (\(,\))-differential privacy if for all neighboring datasets \(Z,Z^{}\) and output sets \(S\),

\[[(Z) S] e^{}[ (Z^{}) S]+.\]

When \(>0\), \(\) satisfies _approximate DP_. When \(=0\), \(\) satisfies the stronger notion of _pure DP_. We say that \(\) is tightly (\(,\))-DP if there is no \(^{}<\) for which \(\) would be (\(,^{}\))-DP.

In what follows, we overview two different styles of achieving DP guarantees: one via hockey-stick divergence, and the other via Renyi divergence.

#### 2.1.1 DP via hockey-stick divergence

**Definition 2.2** (Hockey-stick divergence).: Denote \([x]_{+}=\{0,x\}\) for \(x\). For \(>0\) the hockey-stick divergence \(H_{}\) from a distribution \(P\) to a distribution \(Q\) is defined as

\[H_{}(P||Q)=[P(x)- Q(x)]_{+}\; x.\]

Now (with some abuse of notation) we will discuss how to bound the hockey-stick divergence between distributions \((Z)\) and \((Z^{})\) via the concept of _privacy profiles_.

**Definition 2.3** (Privacy profiles Balle et al., 2018).: The privacy profile \(_{}()\) of a mechanism \(\) is defined as

\[_{}():=_{Z Z^{}}H_{ ^{}}(Z)||(Z^{}).\]

Tight \((,)\)-DP bounds can then be obtained as follows.

**Lemma 2.4** (Zhu et al., 2022, Lemma 5).: _Mechanism \(\) satisfies \((,)\)-DP if any only if \(_{}()\)._

_Dominating pairs_ of distributions are useful for bounding the hockey-stick divergence \(H_{^{}}(Z)||(Z^{})\) accurately and, in particular, for obtaining tight bounds for compositions.

**Definition 2.5** (Zhu et al. 2022).: A pair of distributions \((P,Q)\) is a _dominating pair_ of distributions for mechanism \(:\) if for all neighboring datasets \(Z\) and \(Z^{}\) and for all \(>0\),

\[H_{}((Z)||(Z^{})) H_{ }(P||Q).\]

#### 2.1.2 DP via Renyi divergence

**Definition 2.6**.: (Renyi divergence.) Let \(>0\). For \( 1\), the Renyi divergence \(D_{}\) from distribution \(P\) to distribution \(Q\) is defined as

\[D_{}(P||Q)=_{x Q}[()^{}\,].\]

When \(=1\), Renyi divergence reduces to the Kullback-Leibler (KL) divergence:

\[D_{1}(P||Q)=_{x P}[()\, ].\]

Renyi differential privacy (RDP) is a relaxation of pure DP (\(=0\)) based on Renyi divergence.

**Definition 2.7** (Renyi differential privacy).: A mechanism \(:\) satisfies (\(,\))-Renyi differential privacy if for all neighboring datasets \(Z,Z^{}\),

\[D_{}(Z)\,||\,(Z^{}),\]

RDP implies \((,)\)-DP for any \(0< 1\) with \(=_{>1}\{()+(1/)/(-1)\}\). Tighter but more complex conversion formulae were derived by Balle et al. (2020) and Canonne et al. (2020), which we adopt numerically in our experiments whenever approximate DP is needed.

### Differentially Private Empirical Risk Minimization

We have a dataset \(Z\) and a loss function \((;z)\); we want to solve problems of the form

\[=*{arg\,min}_{}_{z Z}( ;z)+r(),\]

where \(z=(x,y)\) is a data point and \(r()\) is a regularization term. The feature space is \(^{d}\) and the label space is \(\). We will assume that \(||x||_{2} 1\) and \(|y| 1\).

This work focuses on unconstrained convex generalized linear models (GLMs): we require that \(()\) and \(r()\) are convex and twice-differentiable and that \(=^{d}\). The loss function is assumed to have GLM structure of the form \((;z)=f(x^{T};y)\).

**Objective Perturbation** Construct the perturbed objective function by sampling \(b(0,^{2}I_{d})\):

\[^{P}(;Z,b)=_{z Z}(;z)+|| ||_{2}^{2}+b^{T}.\]

The objective perturbation mechanism (ObjPert) outputs \(^{P}(Z)=*{arg\,min}_{}^{P} (;Z,b)\).

**Theorem 2.8** (DP guarantees of objective perturbation (Kifer et al., 2012)).: _Let \((;z)\) be convex and twice-differentiable such that \(||(;z)||_{2} L\) and \(^{2}(;z) I_{d}\) for all \(\) and \(z\). Then objective perturbation satisfies (\(,\))-DP when \(\) and \(}{}\)._

**Differentially Private Gradient Descent** DP-SGD is a differentially private version of stochastic gradient descent which ensures privacy by clipping the per-example gradients at each iteration before aggregating them and adding noise to the result. The update rule at iteration \(t\) is given by

\[_{t+1}=_{t}-_{t}(_{z B_{t}} (_{t};z)+(0,^{2}I_{d})),\]

where \(_{t}\) is the learning rate at iteration \(t\), \(B_{t}\) is the current batch at iteration \(t\), \(\) is the noise scale, and clip is a function that bounds the norm of the per-example gradients.

## 3 Analytical Tools

Existing privacy guarantees of the objective perturbation mechanism (Chaudhuri et al., 2011; Kifer et al., 2012) pre-date modern privacy accounting tools such as Renyi differential privacy and privacy profiles. In this section, we present two new privacy analyses of objective perturbation: an \((,)\)-DP bound based on privacy profiles, and an RDP bound.

### Approximate DP Bound

**Theorem 3.1** (Approximate DP guarantees of objective perturbation for GLMs).: _Consider a loss function \((;z)=f(x^{T};y)\) with GLM structure. Suppose that \(f\) is \(\)-smooth and \(||(;z)||_{2} L\) for all \(\) and \(z\). Fix \(>\). Let \( 0\) and let \(=-(1-)\), \(=-(1-)-}{ 2^{2}}\), and let \(P\) and \(Q\) be the density functions of \((L,^{2})\) and \((0,^{2})\), respectively. Objective perturbation satisfies \(,\!()\)-DP for_

\[()=2 H_{e^{}}P||Q ,& 0,\\ (1-^{})+^{} 2 H_{e^{ }{^{2}}}}P||Q,&\] (3.1)

Notice that we can express (3.1) analytically using (B.1). To obtain the bound (3.1) we repeatedly use the fact that the privacy loss random variable (PLRV) determined by the distributions \((1,^{2})\) and \((0,^{2})\) is distributed as \((},})\). As the upper bound (3.1) is obtained using a PLRV that is a certain scaled and shifted half-normal distribution, we can also find certain scaled and shifted half-normal distributions \(P\) and \(Q\) which give the dominating pair of distributions for the objective perturbation mechanism such that the hockey-stick divergence between \(P\) and \(Q\) is exactly the upper bound (3.1) for all \(\) (shown in the appendix).

### Renyi Differential Privacy Bound

If our sole objective is to obtain the tightest possible approximate DP bounds for objective perturbation, we can stop at Theorem 3.1! Directly calculating the privacy profiles of objective perturbation using the hockey-stick divergence, as in the previous section, will achieve this goal (until more privacy accounting tools come along).

In this section we turn instead to Renyi differential privacy, a popular relaxation of pure differential privacy (\(=0\)) which avoids the "catastrophic privacy breach" possibility permitted by approximate DP (\(>0\)). Below, we present an RDP guarantee for objective perturbation.

**Theorem 3.2** (RDP guarantees of objective perturbation for GLMs).: _Consider a loss function \((;z)=f(x^{T};y)\) with GLM structure. Suppose that \(f\) is \(\)-smooth and \(||(;z)||_{2} L\) for all \(\) and \(z\). Fix \(>\). Objective perturbation satisfies \((,)\)-RDP for any \(>1\) with_

\[=-(1-)+}{2^{2}}+ _{X(0,}{^ {2}})}[e^{(-1)|X|}].\]

_For \(=1\), the RDP bound holds with_

\[=-(1-)+}{2^{2}}+ _{X(0,}{^{2}})}[ e^{|X|}].\]

One of our main motivations for improving the privacy analysis of objective perturbation comes from the observation that it can be competitive to DP-SGD when the privacy cost of hyperparameter tuning is included in the privacy budget. As the tightest results for DP hyperparameter tuning are stated in terms of RDP (Papernot & Steinke, 2022), in our experiments we use RDP bounds of objective perturbation to get a clear understanding of the differences in the privacy-utility trade-offs between these two approaches.

_Remark 3.3_.: Privacy profile and RDP bounds (such as Theorems 3.1 and 3.2) are unified in the sense that they are both based on a certain bound of the PLRV \(_{Z,Z^{}}\) (Definition K.4) for a fixed pair of datasets \(Z,Z^{}\). From Definitions 2.2 and 2.7 we see that for \(\), the hockey-stick divergence is

\[H_{^{}}(Z)\,||\,(Z^{}) =_{(Z)}[1-^{- _{Z,Z^{}}()}\,]_{+},\]

and for \(>1\) we have that the Renyi divergence is

\[D_{}(Z)\,||\,(Z^{})=_{(Z)}[^{\, _{Z,Z^{}}()}\,].\]

### Distance to Optimality

How close to optimal are the bounds of Theorems 3.1 and 3.2? We can in fact show that the Gaussian mechanism is a special case of the objective perturbation mechanism -- thereby providing a lower bound on its approximate DP and RDP.

_Example 3.4_.: _Consider the loss function \((;x)=x^{T}\) and choose neighboring datasets \(X=\{x\}\) and \(X^{}=\), for some \(x^{d}\). Fix \(>0\) and sample \(b(0,^{2}I_{d})\). Then the objective perturbation mechanism solves_

\[^{P}(X) =*{arg\,min}_{^{d}}x^{T} +||||_{2}^{2}+b^{T}=-(x+b),\] \[^{P}(X^{}) =*{arg\,min}_{^{d}}||||_{2}^{2}+b^{T}=-b.\]

_Observe that \(^{P}(X)(-x,}{^{2 }}I_{d})\) and \(^{P}(X^{})(0,}{^{2} }I_{d})\). Following the problem setting described in Theorem 2.8, we have that \(||x||_{2}=||(;x)||_{2} L\). In this case, objective perturbation reduces directly to the Gaussian mechanism with sensitivity \(_{f}=\) and noise scale \(\)._

_Corollary 3.5_.: _As a consequence of Example 3.4 and the scaling invariance of the hockey-stick divergence, for all \(>1\) we have the following:_

\[H_{}^{P}(Z)\;||\;^{P}(Z^{})  H_{}(0,}{^{2}})\;||\; (,}{^{2}})=H_{ }(0,^{2})\;||\;(L,^{2}).\]

The argument works the same for the Renyi divergence \(D_{}\) which is similarly scaling-invariant. Corollary 3.5 implies that we can measure the tightness of the bounds given in Theorems 3.1 and 3.2 by comparing them to the tight bounds of the Gaussian mechanism (B.1) with sensitivity \(_{f}=L\) and noise scale \(\).

This means that in Figure 1, the hockey-stick divergence of the Gaussian mechanism is a lower bound on the hockey-stick divergence for objective perturbation. While our hockey-stick divergence bound is unsurprisingly a bit tighter than the RDP bound for objective perturbation, we see that both significantly improve over the classic \((,)\)-DP bounds of Kifer et al. (2012).

_Remark 3.6_.: The RDP and approximate DP bounds in this section require a careful analysis of the dependence between the noise vector \(b\) and the private minimizer \(^{P}\). In the appendix, we show how the GLM assumption simplifies this issue.

## 4 Computational Tools

In this section we present Algorithm 1, which extends the Approximate Minima Perturbation of Iyengar et al. (2019) to handle loss functions with unbounded gradient.

Figure 1: Comparison of different \((,)\)-bounds for objective perturbation: the \((,)\)-bound by Kifer et al. (2012) given in Thm. 2.8, the RDP bound of Thm. 3.2, the approximate DP bound of Thm. 3.1 using the hockey-stick divergence and the approximate DP lower bound obtained using the hockey-stick divergence and Cor. 3.5. Left: \(=5.0\), \(=1.0\) and \(=20.0\). Right: \(=10.0\), \(=1.0\) and \(=5.0\).

**Approximate minima** The privacy guarantees of objective perturbation hold only when its output is an _exact_ minimizer of the perturbed objective. Approximate Minima Perturbation (AMP) (Iyengar et al., 2019) addresses this issue by finding an approximate minimizer to the perturbed objective, then privately releases this approximate minimizer with the Gaussian mechanism.

**Gradient clipping** DP-SGD requires no _a priori_ bound on the gradient of the loss function; at each iteration, the algorithm clips the per-example gradients above a pre-specified threshold in order to bound the gradient norms. We extend this same technique to objective perturbation.

Given a loss function \((;z)\) and a clipping threshold \(C\), we can transform the gradient of \((;z)\) as follows:

\[_{C}(;z)=(;z)&\;|| (;z)||_{2} C,\\ }(;z)&\;|| (;z)||_{2}>C.\]

Then we can define the aggregation of clipped gradients as \(_{C}(;Z)=_{z Z}_{C}(;z)\).

The aggregation of clipped gradients \(_{C}(;Z)\) corresponds to an implicit "clipped-gradient" objective function \(_{C}(;Z)\). For convex GLMs, Song et al. (2020) define this function precisely and show that it retains the convexity and GLM structure of the original objective function \((;Z)\). We furthermore demonstrate that this function retains the same bound \(\) on the Lipschitz smoothness (Theorem E.3).

Algorithm 1 extends the privacy guarantees of AMP (Iyengar et al., 2019) to loss functions with unbounded gradient. Notice that for smooth loss functions with gradient norm bounded by \(L\), we can set \(C=L\) in order to recover Approximate Minima Perturbation.

``` Input: dataset \(Z\); noise levels \(,_{}\); \(\)-smooth loss function \(()\) ; regularization strength \(\); gradient norm threshold \(\); clipping threshold \(C\).
1. Construct the set of clipped-gradient loss functions \(\{_{C}(;z):z Z\}\).
2. Sample \(b(0,^{2}I_{d})\).
3. Let \(_{C}^{P}(;Z,b)=_{z Z}_{C}(;z)+ {2}||||_{2}^{2}+b^{T}\).
4. Solve for \(\) such that \(\|_{C}^{P}(;Z)\|_{2}\).
5Output:\(^{P}=+(0,_{}^{2}I_{d})\). ```

**Algorithm 1** Computational Objective Perturbation

**Theorem 4.1** (RDP guarantees of Algorithm 1).: _Consider a loss function \((;z)=f(x^{T};y)\) with GLM structure, such that \(f\) is \(\)-smooth. Fix \(>\). Algorithm 1 satisfies \((,)\)-RDP for any \(>1\) with_

\[-(1-)+}{2^{2} }+_{X(0,}{ ^{2}})}[e^{(-1)|X|}]+}{ _{}^{2}^{2}}.\]

_Remark 4.2_.: Gradient clipping aside, our proof of Theorem 4.1 takes a different tack than the proof of Theorem 1 (for AMP) in Iyengar et al. (2019). We observe that Algorithm 1 is essentially an adaptive composition of the objective perturbation mechanism and the Gaussian mechanism. We can write \(=^{P}+(-^{P})\) to see that we are releasing two quantities: \(^{P}\) (with objective perturbation) and the difference \(-^{P}\) (with the Gaussian mechanism). Algorithm 1 stops iterating only after the gradient norm \(||_{C}^{P}(;Z)||_{2}\) is below the threshold \(\). This along with the \(\)-strong convexity of the objective function \(_{C}^{P}(;Z)\) ensures a bound on the \(_{2}\)-sensitivity of the difference \(-^{P}\), so that we can apply the Gaussian mechanism.

### Computational Guarantee

To achieve the optimal excess risk bounds for DP-ERM in the convex setting, DP-SGD clocks in at a hefty \(O(n^{2})\) gradient evaluations (Bassily et al., 2014). It has been an open problem to obtain optimal DP-ERM algorithms that runs in subquadratic time (Kulkarni et al., 2021). One of our contributions is to show that when we further restrict to smooth GLM-losses (so ObjPert is applicable) Algorithm 1 can achieve the same optimal rate with only \(O(n n)\) gradient evaluations.

A formal claim and proof that Algorithm 1 -- with appropriately chosen parameters -- achieves the optimal rate is deferred to Appendix H. The analysis is largely the same as those in (Kifer et al., 2012) but with the bug fixed (details in Appendix G) by adding a GLM assumption.

The improved computational complexity is due to that we can apply any off-the-shelf optimization algorithms to solve Step 4 of Algorithm 1. Observe that \(^{P}(;Z,b)\) has a finite-sum structure, we can employ the Stochastic Averaged Gradient (SAG) method (Schmidt et al., 2017) which halts in \(O(n n)\) with high probability. Details are provided in Appendix F.

## 5 Empirical Evaluation

In this section we evaluate Algorithm 1 against two baselines: "dishonest" DP-SGD and "honest" DP-SGD. Dishonest DP-SGD does not account for the privacy cost of hyperparameter tuning; honest DP-SGD follows the private selection algorithm and RDP bounds from Papernot & Steinke (2022).

Our experimental design includes some guidelines in order to make it a fair fight. One of the strengths of Algorithm 1 that we advocate for is its blackbox optimization. Whereas DP-SGD consumes privacy budget for testing each set of hyperparameter candidates, an advantage of approximate minima perturbation is that the privacy guarantees are independent of the choice of optimizer used to solve for \(\). We can therefore test out any number of optimization hyperparameters for Algorithm 1_at no additional privacy cost_, provided that these parameters are independent of the privacy guarantee (e.g. learning rate, batch size). More specifically, once the loss function is perturbed with the noise \(b\) in Algorithm 1, any \(\) that satisfies the convergence guarantees with the tolerance parameter \(\) will have the RDP-guarantees of Theorem 4.1 and therefore we are free to also carry out tuning of the optimization algorithm without an additional privacy cost.

Because we are interested in measuring the effect of the privacy cost of hyperparameter tuning, we tune only the learning rate which does not affect the privacy guarantee of the base algorithm. This isolates the effect of hyperparameter tuning as we will need to appeal to Papernot & Steinke (2022) to get valid DP bounds for DP-SGD, but Algorithm 1 enjoys hyperparameter tuning "for free".

The following table summarizes the optimization-related parameters for all three methods.

   & Dishonest DP-SGD & Honest DP-SGD & Algorithm 1 \\   clipping & \(C=\) & \(C=\) & \(C=\) \\  learning rate & log(\(10^{-8}\), \(10^{-1}\)) & log(\(10^{-8}\), \(10^{-1}\)) & linear(.\(08,.5\)) \\  grid size & \(s=10\) & \(K()\) s.t. \([K>s]=0.9\) & \(s=10\) \\  optimizer & Adam & Adam & L-BFGS \\  convergence & after \(T\) iterations & after \(T\) iterations & \(||()||_{2}\) \\  

The choice of \(C=\) is a natural value for logistic regression in that \(||(,z)||\) for all \(,z\) due to data-preprocessing and the bias term. Dishonest DP-SGD selects \(s=10\) learning rate candidates evenly log-spaced from the range of values between \(10^{-8}\) and \(10^{-1}\). Honest DP-SGD selects learning rate candidates from the same range of values, but with granularity determined by a random variable \(K\) sampled from the Poisson distribution \(()\). We select \(\) so that with 90% probability, \(K\) is larger than the grid size \(s\) used for dishonest DP-SGD, resulting in \( 15.4\).

We use the Adam optimizer for both honest and dishonest DP-SGD. For Algorithm 1 we use the L-BFGS optimizer whose second-order behavior allows us to get within a smaller distance to optimal (as required by Algorithm 1).

For DP-SGD we set the subsampling ratio such that the expected batch size is 256 and we run for 60 "epochs". We calculate the number of iterations as \(T=60\), where num_batches is the number of batches in the training dataset (we pass the train loader through the Opacus privacy engine, so the size of each batch is random). To calibrate the scale of the noise for DP-SGD, we use the analytical moments accountant (Wang et al., 2019) with Poisson sampling (Zhu & Wang, 2019; Mironov et al., 2019).

The parameters specific to AMP are \(_{out}\), the noise scale for the output perturbation step; and \(\), the gradient norm bound. A larger \(\) will improve our computational cost, though our approximate minimizer \(\) will be farther away from the true minimizer \(^{P}\). We can achieve a smaller \(\) by choosing a larger \(_{out}\), but this will mean that our release of \(\) will be noisier. In our experiments we fix \(=0.01\) and \(_{out}=0.15\).

The privacy parameters of objective perturbation are the noise scale \(\) and the regularization strength \(\). Balancing these parameters is a classic exercise in bias-variance trade-off. A larger \(\) will allow us to use less regularization, but if \(\) is too large then we risk adding too much noise to the objective function and hurting utility.

Our strategy is to find the smallest possible \(\) such that \(\) isn't too large. To quantify when \(\) is "too large", we use the Gaussian mechanism as a reference point: the noise scale \(\) for objective perturbation shouldn't be too much larger than the noise scale \(_{G}\) for the Gaussian mechanism. Let's say that the Gaussian mechanism with noise scale \(_{G}\) satisfies (\(,\))-DP, then we want our \(\) for (\(,\))-DP objective perturbation to satisfy \( f_{G}\) for some small constant factor \(f\). In our experiments, we set \(f=1.3\).

For objective perturbation, we can thus select the privacy parameters \(\) and \(\) using fixed values (e.g., \(,,_{G}\)) that are independent of the data. Likewise, the choices of \(_{out}\) and \(\) are fixed across all datasets. This is noteworthy since \(,,_{out}\) and \(\) each have an effect on the privacy guarantee, outside of the blackbox algorithm. Tuning these parameters on the data would require us to use the private selection algorithm.

We evaluate our methods for binary classification on the Adult, Synthetic-L, Synthetic-H and Gisette datasets provided by Iyengar et al. (2019). We normalize each row \(x_{i}\) to have unit \(_{2}\)-norm. Note

Figure 2: Comparison of Algorithm 1 against honest and dishonest DP-SGD baselines, varying \(\{0.025,0.05,0.1,0.25,0.5,1.0,2.0,4.0,8.0\}\) and fixing \(=10^{-5}\). On all three methods, we train the model for each learning rate on its grid (see Table 5) and report the test accuracy for the best learning rate on the grid. Results are averaged over 10 trials and the error bars on both sides of the mean values depict 1.96 times the standard error, giving the asymptotic 95% coverage.

that the assignment \(x_{i}}{||x_{i}||_{2}}\) doesn't require expending any privacy budget as each data point is transformed only by its own per-sample norm.

The experimental results shown in Figure 5, Table 1 and Table 2 paint a consistent picture. While dishonest DP-SGD is clearly the best-performing algorithm, when we account for the cost of hyperparameter tuning then Algorithm 1 can typically best honest DP-SGD. This effect is especially pronounced under small \(\), for which diverting some of the limited privacy budget to hyperparameter tuning could be more impactful.

**Is it fair?** Our experimental design aims to fairly compare ObjPert to DP-SGD. One limitation, however, is that the state-of-the-art tools for private hyperparameter tuning from Papernot & Steinke (2022) are RDP bounds -- and RDP is _not_ state-of-the-art for DP-SGD privacy accounting. At this moment, the tighest privacy accounting tool for DP-SGD is the PRV accountant from Gopi et al. (2021), which is the counterpart to our privacy profiles analysis for ObjPert (Theorem 3.1). Unfortunately, even though dishonest DP-SGD would benefit from using the PRV accountant, for private hyperparameter tuning we would then have to use the sub-optimal private selection bounds for approximate DP from Liu & Talwar (2019). In our experiments we therefore use RDP-based privacy accounting for both ObjPert and DP-SGD. Comparing DP-SGD with numerical composition against ObjPert with Theorem 3.1 will have to wait until more private selection tools are available.

One might also object that by tuning only the learning rate for DP-SGD, we didn't explore the full range of hyperparameters relevant to DP-SGD's performance. While tuning additional hyperparameters such as the batch size and number of epochs could benefit dishonest DP-SGD, it would likely worsen the privacy-utility tradeoff for honestly-tuned DP-SGD due to the increased privacy cost of the hyperparameter tuning algorithm from Papernot & Steinke (2022).

## 6 Conclusion

One point that we really wanted to drive home is that while DP-SGD works extraordinarily well across a wide variety of problem settings, it's not necessarily the best solution for _every_ problem setting. But at the same time, DP-SGD has received the benefit of an enormous amount of attention that other DP learning algorithms haven't received. A goal of our paper was to hone in on a particular problem setting and give a different algorithm the same star treatment.

Objective perturbation now boasts two new privacy analyses. One is an improved \((,)\)-DP analysis based on bounding the hockey-stick divergence. The other is an RDP analysis which allows us to fairly compare objective perturbation against DP-SGD -- the workhorse of differentially private learning -- with honest hyperparameter tuning. We've also expanded the approximate minima perturbation algorithm of Iyengar et al. (2019) in order to encompass a broader range of loss functions which need not have bounded gradient. Our algorithm moreover can be used in conjunction with SVRG to guarantee a running time of \(O(n n)\) to achieve the optimal excess risk bounds, improving on the \(O(n^{2})\) computational guarantee of DP-SGD.

#### Acknowledgments

The work was partially supported by NSF Award #2048091.

   & Dishonest & Alg 1 & Honest \\   \(=0.1\) & 93.85\% & 90.05\% & 77.34\% \\  \(=1\) & 95.25 & 94.50\% & 93.15\% \\  \(=8\) & 95.43\% & 95.30\% & 95.17 \% \\   
   & Dishonest & Alg 1 & Honest \\   \(=0.1\) & 81.61\% & 81.37\% & 78.32\% \\  \(=1\) & 83.54\% & 83.18\% & 82.40\% \\  \(=8\) & 84.19\% & 83.99\% & 83.66\% \\  

Table 1: Synthetic-L