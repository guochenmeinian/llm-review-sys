# ISP: Multi-Layered Garment Draping with Implicit Sewing Patterns

Ren Li &Benoit Guillard &Pascal Fua

Computer Vision Lab, EPFL

Lausanne, Switzerland

ren.li@epfl.ch benoit.guillard@epfl.ch pascal.fua@epfl.ch

###### Abstract

Many approaches to draping individual garments on human body models are realistic, fast, and yield outputs that are differentiable with respect to the body shape on which they are draped. However, they are either unable to handle multi-layered clothing, which is prevalent in everyday dress, or restricted to bodies in T-pose. In this paper, we introduce a parametric garment representation model that addresses these limitations. As in models used by clothing designers, each garment consists of individual 2D panels. Their 2D shape is defined by a Signed Distance Function and 3D shape by a 2D to 3D mapping. The 2D parameterization enables easy detection of potential collisions and the 3D parameterization handles complex shapes effectively. We show that this combination is faster and yields higher quality reconstructions than purely implicit surface representations, and makes the recovery of layered garments from images possible thanks to its differentiability. Furthermore, it supports rapid editing of garment shapes and texture by modifying individual 2D panels.

## 1 Introduction

Draping virtual garments on body models has many applications in fashion design, movie-making, virtual try-on, virtual and augmented reality, among others. Traditionally, garments are represented by 3D meshes and the draping relies on physics-based simulations (PBS)  to produce realistic interactions between clothes and body. Unfortunately PBS is often computationally expensive and rarely differentiable, which limits the scope of downstream applications. Hence, many recent techniques use neural networks to speed up the draping and to make it differentiable. The garments can be represented by 3D mesh templates , point clouds , UV maps , or implicit surfaces . Draping can then be achieved by Linear Blend Skinning (LBS) from the shape and pose parameters of a body model, such as SMPL .

Even though all these methods can realistically drape individual garments over human bodies, none can handle multiple clothing layers, despite being prevalent in everyday dress. To address overlapping clothing layers such as those in Fig. 1 while preserving expressivity, we introduce an Implicit Sewing Pattern (ISP), a new representation inspired by the way fashion designers represent clothes. As shown in Fig. 2, a sewing pattern is made of several 2D panels implicitly represented by signed distance functions (SDFs) that model their 2D extent and are conditioned on a latent vector \(\), along with information about how to stitch them into a complete garment. To each panel is associated a 2D to 3D mapping representing its 3D shape, also conditioned on \(\). The 2D panels make it easy to detect collisions between surfaces and to prevent interpenetrations. In other words, the garment is made of panels whose 2D extent is learned instead of having to be carefully designed by a human and whose3D shape is expressed in a unified \(uv\) space in which a loss designed to prevent interpenetrations can easily be written.

This combination enables us to model layered garments such as those of Fig. 1 while preserving end-to-end differentiability. This lets us drape them realistically over bodies in arbitrary poses, to recover them from images, and to edit them easily. Doing all this jointly is something that has not yet been demonstrated in the Computer Vision or Computer Graphics literature. Furthermore, most data driven draping methods rely on synthetic data generated with PBS for supervision purposes. In contrast, ISPs rely on physics-based self-supervision of [18; 13]. As a result, at inference time, our approach can handle arbitrary body poses, while only requiring garments draped over bodies in a canonical pose at training time. Our code is available at https://github.com/liren2515/ISP.

## 2 Related Work

Garment Draping.Garment draping approaches can be classified as physics-based or data-driven. Physics-based ones [3; 32; 33; 34; 35; 12] produce high-quality results but are computationally demanding, while data-driven approaches are faster, sometimes at the cost of realism. Most data-driven methods are template-based [13; 14; 15; 17; 19; 18; 20; 16; 18; 13], with a triangulated mesh modeling a specific garment and a draping function trained specifically for it. As this is impractical for large garment collections, some recent works [22; 21; 36] use 3D point clouds to represent garments instead. Unfortunately, this either prevents differentiable changes in garment topology or loses the point connections with physical meanings. The approaches of [24; 25] replace the clouds by UV maps that encode the diverse geometry of garments and predict positional draping maps. These UV maps are registered to the body mesh, which restricts their interpretability and flexibility for garment representation and manipulation. The resulting garments follow the underlying body topology, and the ones that should not, such as skirts, must be post-processed to remove artifacts. Yet other algorithms [29; 30; 37] rely on learning 3D displacement fields or hierarchical graphs for garment deformation, which makes them applicable to generic garments.

Figure 1: **Multi-layered garment draping. Top: Draping multiple layers of garments over one body (left) and modifying the bodyâ€™s shape (right). Bottom: Draping the same set of 5 garments over bodies with varying poses and shapes.**

While many of these data-driven methods deliver good results, they are typically designed to handle a single garment or a top and a bottom garment with only limited overlap. An exception is the approach of  that augments SDFs with covariant fields to untangle multi-layered garments. However, the untangling is limited to garments in T-pose. In contrast, our proposed method can perform multi-layered garment draping for bodies in complex poses and of diverse shapes.

Garments as Sets of Panels.Sewing patterns are widely used in garment design and manufacturing [39; 40; 41; 42; 43]. Typically, flat sewing patterns are assembled and then draped using PBS. To automate pattern design, recent works introduce a parametric pattern space. For example, the methods of [44; 45] introduce a sparse set of parameters, such as sleeve length or chest circumference, while  relies on principal component analysis (PCA) to encode the shape of individual panels. It requires hierarchical graphs on groups of panels to handle multiple garment styles. By contrast, our approach relies on the expressivity of 2D Sign Distance Functions (SDFs) to represent the panels.

To promote the use of sewing patterns in conjunction with deep learning, a fully automatic dataset generation tool is proposed in . It randomly samples parameters to produce sewing patterns and uses PBS to drape them on a T-posed human body model to produce training pairs of sewing patterns and 3D garment meshes.

Garments as Implicit Surfaces.SDFs have become very popular to represent 3D surfaces. However, they are primarily intended for watertight surfaces. A standard way to use them to represent open surfaces such as garments is to represent them as thin volumes surrounding the actual surface [28; 29], which can be represented by SDFs but with an inherent accuracy loss. To address this issue, in , the SDFs are replaced by unsigned distance functions (UDFs) while relying on the differentiable approach of  to meshing, in case an actual mesh is required for downstream tasks. However, this requires extra computations for meshing and makes training more difficult because surfaces lie at a singularity of the implicit field. This can result in unwanted artifacts that our continuous UV parameterization over 2D panels eliminates.

## 3 Method

In clothing software used industrially [39; 40; 41], garments are made from sets of 2D panels cut from pieces of cloth which are then stitched together. Inspired by this real-world practice, we introduce the Implicit Sewing Patterns (ISP) model depicted by Fig. 2. It consists of 2D panels whose shape is defined by the zero crossings of a function that takes as input a 2D location \(=(x_{u},x_{v})\) and a latent vector \(\), which is specific to each garment. A second function that also takes \(\) and \(\) as arguments maps the flat 2D panel to the potentially complex 3D garment surface within the panel, while enforcing continuity across panels. Finally, we train draping networks to properly drape multiple garments on human bodies of arbitrary shapes and poses, while avoiding interpenetrations of successive clothing layers.

### Modeling Individual Garments

We model garments as sets of 2D panels stitched together in a pre-specified way. Each panel is turned into a 3D surface using a \(uv\) parameterization and the networks that implement this parameterization are trained to produce properly stitched 3D surfaces. To create the required training databases, we use the PBS approach of . Because it also relies on 2D panels, using its output to train our networks has proved straightforward, with only minor modifications required.

Flat 2D Panels.We take a pattern \(P\) to be a subset of \(=[-1,1]^{2}\) whose boundary are the zero crossings of an implicit function. More specifically, we define

\[_{}:^{||} ,\ (,)(s,c)\,\] (1)

where \(_{}\) is implemented by a fully connected network. It takes as input a local 2D location \(\) and a global latent vector \(\) and returns two values. The first, \(s\), should approximate the signed distance to the panel boundary. The second, \(c\), is a label associated to boundary points and is used when assembling the garment from several panels. Then, boundaries with the same labels should be stitched together. Note that the \(c\) values are only relevant at the panel boundaries, that is, when \(s=0\). However, training the network to produce such values _only_ there would be difficult because this would involve a very sparse supervision. So, instead, we train our network to predict the label \(c\) of the closest boundary for all points, as shown in Fig. 2(c), which yields a much better behaved minimization problem at training time.

In this paper, we model every garment \(G\) using two panels \(P^{f}\) and \(P^{b}\), one for the front and one for the back. \(E^{f}\) and \(E^{b}\) are labels assigned to boundary points. Label \(0\) denotes unstitched boundary points like collar, waist, or sleeve ends. Labels other than zero denote points in the boundary of one panel that should be stitched to a point with the same label in the other.

In practice, given the database of garments and corresponding 2D panels we generated using publicly available software designed for this purpose , we use an _auto-decoding_ approach to jointly train \(_{}\) and to associate a latent vector \(\) to each training garment. To this end, we minimize the loss function obtained by summing over all training panels

\[_{}=_{}|s(, )-s^{gt}()|+_{CE}(c(, ),c^{gt}())+_{reg}\|\|_{2}^{2}\,\] (2)

with respect to a separate latent code \(\) per garment and the network weights \(\) that are used for all garments. CE is the cross-entropy loss, \(s^{gt}\) and \(c^{gt}\) are the ground-truth signed distance value and the label of the closest seamed edge of \(\), and \(_{CE}\) and \(_{reg}\) are scalars balancing the influence of the different terms. We handle each garment having a front and a back panel \(P^{f}\) and \(P^{b}\) by two separate network \(_{_{f}}\) and \(_{_{b}}\) with shared latent codes so that it can produce two separate sets of \((s,c)\) values at each \((x_{u},x_{v})\) location, one for the front and one for the back.

From 2D panels to 3D Surfaces.The sewing patterns described above are flat panels whose role is to provide stitching instructions. To turn them into 3D surfaces that can be draped on a human body, we learn a mapping from the 2D panels to the 3D garment draped on the neutral SMPL body. To this end, we introduce a second function, similar to the one used in AtlasNet 

\[_{}:^{||}^{3},\ (,)\.\] (3)

Figure 2: **Implicit Sewing Pattern (ISP).****(a)** The 3D mesh surface for a shirt with the front surface in gray and the back one in blue. **(b)** The front and back 2D panels of the ISP. The numbers denote the labels of seamed edges \(E_{c}\), and indicate how to stitch them when \(c>0\). **(c)** We use an implicit neural representation for the signed distance and for the edge labels, denoted here by the different colors. **(d)** Interpolation in the latent space allows topology changes, here from a sleeveless shirt to a long-sleeve open jacket. The top rows show the front and back panels, the bottom row the reconstructed meshes. **(e)** To parameterize a two-panel garment, we train the implicit network \(_{}\) to predict the signed distance field (\(s_{f}/s_{b}\)) and the edge label field (\(c_{f}/c_{b}\)) that represent the two panels. They are mapped to 3D surfaces by the network \(_{}\).

It is also implemented by a fully connected network and takes as input a local 2D location \(\) and a global latent vector \(\). It returns a 3D position \(\) for every 2D location \(\) in the pattern. A key difference with AtlasNet is that we only evaluate \(_{}\) for points \(\) within the panels, that is points for which the signed distance returned by \(_{}\) of Eq. (1) is not positive. Hence, there is no need to deform or stretch \(uv\) patterns. This is in contrast to the square patches of AtlasNet that had to be, which simplifies the training.

Given the latent codes \(\) learned for each garment in the training database, as described above, we train a separate set of weights \(_{f}\) and \(_{b}\) for the front and back of the garments. To this end, we minimize a sum over all front and back training panels of

\[_{} =_{CHD}+_{n}_{normal}+_{c} _{consist}\,\] (4) \[_{consist} =_{c>0}_{ E_{c}}\|_{_{ f}}(,)-_{_{b}}(,) \|_{2}^{2}\ \.\] (5)

where \(_{CHD}\), \(_{normal}\), and \(_{consist}\) are the Chamfer distance, a normal consistency loss, and a loss term whose minimization ensures that the points on the edges of the front and back panels sewn together--\(E_{c}\) for \(c>0\)--are aligned when folding the panels in 3D, \(_{n}\) and \(_{c}\) are scalar weights. This assumes that the front and back panels have their seamed edges aligned in 2D, ie. for \(c>0\) and \(\), if \(\) has label \(c\) on the front panel, then it should also have label \(c\) in the back panel, and vice-versa. Our experiments show that \(_{consist}\) reduces the gap between the front and back panels.

**Meshing and Preserving Differentiability.** \(_{}\) continuously deforms 2D patches that are implicitly defined by \(_{}\). To obtain triangulated meshes from these, we lift a regular triangular 2D mesh defined on \(\) by querying \(_{}\) on each of its vertex, as in . More specifically, we first create a square 2D mesh \(=\{V_{},F_{}\}\) for \(\), where vertices \(V_{}\) are grid points evenly sampled along orthogonal axis of \(\) and faces \(F_{}\) are created with Delaunay triangulation. Given the latent code \(\) of a specific garment, for each vertex \(v V_{}\), we can obtain its signed distance value \(s\) and edge label \(c\) with \((s,c)=_{}(v,)\). We construct the 2D panel mesh \(_{P}=\{V_{P},F_{P}\}\) by discarding vertices of \(\) whose SDF is positive. To get cleaner panel borders, we also keep vertices \(v V_{}\) with \(s(v,)>0\) that belong to the mesh edges that cross the 0 iso-level, and adjust their positions to \(v-s(v,) s(v,)\) to project them to the zero level set . The front and back panel 2D meshes are then lifted to 3D by querying \(_{}\) on each vertex. During post-processing, their edges are sewn together to produce the final 3D garment mesh \(_{G}()\). More details can be found in the Supplementary Material.

\(_{}\) performs as an indicator function to keep vertices with \(s 0\), which breaks automatic differentiability. To restore it, we rely on gradients derived in [50; 51] for implicit surface extraction. More formally, assume \(v\) is a vertex of the extracted mesh \(_{G}\) and \(\) is the point on the UV space that satisfies \(v=_{}(,)\), then, as proved in the Supplementary Material,

\[}=_{}}{ }(,)-_{}} {} s(,)}(,)\.\] (6)

Hence, ISP can be used to solve inverse problems using gradient descent, such as fitting garments to partial observations.

### Garment Draping

We have defined a mapping from a latent vector \(\) to a garment draped over the neutral SMPL model , which we will refer to as a _rest pose_. We now need to deform the garments to conform to bodies of different shapes and in different poses. To this end, we first train a network that can deform a single garment. We then train a second one that handles the interactions between multiple garment layers and prevents interpenetrations as depicted in Fig. 3.

**Single Layer Draping.** In SMPL, body templates are deformed using LBS. As in [19; 29; 30], we first invoke an extended skinning procedure with the diffused body model formulation to an initial rough estimate of the garment shape. More specifically, given the parameter vectors \(\) and \(\) that control the body shape and pose respectively, each vertex \(v\) of a garment mesh \(_{G}\) is transformed by

\[v_{init} =W(v_{(,)},,,w(v ))\,\] (7) \[v_{(,)} =v+w(v)B_{s}()+w(v)B_{p}()\,\]

where \(W()\) is the SMPL skinning function with skinning weights \(^{N_{B} 24}\), with \(N_{B}\) being the number of vertices of the SMPL body mesh, and \(B_{s}()^{N_{B} 3}\) and \(B_{p}()^{N_{B} 3}\) are the shape and pose displacements returned by SMPL. \(w(v)^{N_{B}}\) are diffused weights returned by a neural network, which generalizes the SMPL skinning to any point in 3D space. The training of \(w(v)\) is achieved by smoothly diffusing the surface values, as in .

This yields an initial deformation, such that the garment roughly fits the underlying body. A displacement network \(_{s}\) is then used to refine it. \(_{s}\) is conditioned on the body shape and pose parameters \(,\) and on the garment specific latent code \(\). It returns a displacement map \(D_{s}=_{s}(,,)\) in UV space. Hence, the vertex \(v_{init}\) with UV coordinates \((x_{u},x_{v})\) in \(\) is displaced accordingly and becomes \(=v_{init}+D_{s}[x_{u},x_{v}]\), where \([,]\) denotes standard array addressing. \(_{s}\) is implemented as a multi-layer perceptron (MLP) that outputs a vector of size \(6N^{2}\), where \(N\) is the resolution of UV mesh \(\). The output is reshaped to two \(N N 3\) arrays to produce the front and back displacement maps.

To learn the deformation for various garments without collecting any ground-truth data and train \(_{s}\) in a self-supervised fashion, we minimize the physics-based loss from 

\[_{phy}=_{strain}+_{bend}+_{gravity }+_{BGcol}\,\] (8)

where \(_{strain}\) is the membrane strain energy caused by the deformation, \(_{bend}\) the bending energy raised from the folding of adjacent faces, \(_{gravity}\) the gravitational potential energy, and \(_{BGcol}\) the penalty for body-garment collisions.

Multi-Layer Drapping.When draping independently multiple garments worn by the same person using the network \(_{s}\) introduced above, the draped garments can intersect, which is physically impossible and must be prevented. We now show that our ISP model makes that straightforward first in the case of two overlapping garments, and then in the case of arbitrarily many.

Consider an outer garment \(_{G}^{o}\) and an inner garment \(_{G}^{i}\) with rest state vertices \(V_{o}\) and \(V_{i}\). We first drape them independently on a target SMPL body with \(_{s}\), as described for single layer draping. This yields the deformed outer and underlying garments with vertices \(_{o}\) and \(_{i}\), respectively. We then rely on a second network \(_{m}\) to predict corrective displacements, conditioned on the outer garment geometry and repulsive virtual forces produced by the intersections with the inner garment.

In ISP, garments are represented by mapping 2D panels to 3D surfaces. Hence their geometry can be stored on regular 2D grids on which a convolutional network can act. In practice, we first encode the _rest state_ of the outer garment \(_{G}^{o}\) into a 2D position map \(M_{r}\). The grid \(M_{r}\) records the 3D location of the vertex \(v_{o} V_{o}\) at its \((x_{u},x_{v})\) coordinate as \(M_{r}[x_{u},x_{v}]=v_{o}\) within the panel boundaries, \(M_{r}[x_{u},x_{v}]=(0,0,0)\) elsewhere. Concatenating both front and back panels yields a \(N N 6\) array, that is, a 2D array of spatial dimension \(N\) with 6 channels. After draping, the same process is applied to encode the geometry of \(_{G}^{o}\) into position map \(M_{d}\), using vertices \(_{o}\) instead of \(V_{o}\) this time. Finally, for each vertex \(_{o}_{o}\), we take the repulsive force acting on it to be

\[f(_{o})=max(0,(_{i}-_{o})_{i}) _{i}\,\] (9)

where \(_{i}\) is the closest vertex in \(_{i}\), \(_{i}\) is the normal of \(_{i}\), and \(\) represents the dot product. The repulsive force is also recorded in the UV space to generate the 2D force map \(M_{f}\). Note that it is \(0\) for vertices that are already outside of the inner garment, for which no collision occurs. Given the forces \(M_{f}\), the garment geometry in the rest state \(M_{r}\) and after draping \(M_{d}\), the network \(_{m}\) predicts a vertex displacements map \(D_{m}=_{m}(M_{r},M_{d},M_{f})\) for the outer garment to resolve intersections, as shown in Fig. 3. We replace the vertex \(_{o}_{o}\) with coordinates \((x_{u},x_{v})\) by \(_{o}^{*}=_{o}+D_{m}[x_{u},x_{v}]\). \(_{m}\) is implemented as a CNN, and it can capture the local geometry and force information of vertices from the input 2D maps. The training of \(_{m}\) is self-supervised by

\[_{_{m}}=_{phy}+_{g}_{GGcol}+_{r}_{reg}\,\] (10) \[_{GGcol}=_{_{o}^{*}}(0,-( _{o}^{*}-_{i})_{i})^{3}\,\ \ _{reg}=\|_{m}(M_{r},M_{d},M_{f})\|_{2}^{2}\,\]

where \(_{phy}\) is the physics-based loss of Eq. (8), \(_{g}\) and \(_{r}\) are weighting constants, and \(\) is a safety margin to avoid collisions. \(_{GGcol}\) penalizes the intersections between the outer and underlying garments, and \(_{reg}\) is an \(L_{2}\) regularization on the output of \(_{m}\).

Given a pair of garments, our layering network \(_{m}\) only deforms the outer garment to resolve collisions, leaving the inner one untouched. Given more than 2 overlapping garments, the process can be iterated to layer them in any desired order over a body, as detailed in the Supplementary Material.

[MISSING_PAGE_FAIL:7]

of our method, whereas that of UDF's drops. This is because precisely learning the 0 iso-surface of a 3D UDF is challenging, resulting in potentially inaccurate normals near the surface [54; 47]. We present similar results for trousers and skirts in the supplementary material.

### Garment Draping

Figs. 1 and 5 showcase our method's ability to realistically drape multiple garments with diverse geometry and topology over the body. Our approach can handle multi-layered garments, which is not achievable with DrapeNet. Additionally, unlike the method of  that is limited to multi-layered garments on the T-posed body, our method can be applied to bodies in arbitrary poses.

As pointed out in [13; 30], there are no objective metrics for evaluating the realism of a draping. Therefore, we conducted a human evaluation. As in , we designed a website displaying side-by-side draping results generated by our method and DrapeNet for the same shirts and trousers, on the same bodies. Participants were asked to select the option that seemed visually better to them, with the third option being "I cannot decide". 64 participants took part in the study, providing a total of 884 responses. As shown in Fig. 6(a), the majority of participants preferred our method (62.69% vs. 25.00%), demonstrating the higher fidelity of our results. The lower intersection ratio reported in Fig. 6(b) further demonstrates the efficacy of our draping model. In Figs. 6(c) and (d), we compare qualitatively our method against DrapeNet and DIG.

### Recovering Multi-Layered Garments from Images

Thanks to their differentiability, our ISPs can be used to recover multi-layered garments from image data, such as 2D garment segmentation masks. Given an image of a clothed person, we can obtain the estimation of SMPL body parameters \((,)\) and the segmentation mask \(\) using the algorithms of [56; 57]. To each detected garment, we associate a latent code \(\) and reconstruct garment meshes

Figure 5: **Realistic garment draping with our method, for different combinations of garments.**

Figure 6: **Draping evaluation. (a) Human evaluation results. None refers to no preference. (b) Percentage of intersecting areas. (c) For each method, left is the draping results for a shirt and a pair of trousers, and right with only the pants. (d) The draping results of our method, DIG and DrapeNet.**

by minimizing

\[_{1:N}^{*} =*{argmin}_{_{1:N}}L_{}(R((,,_{1:N})(, )),)\,\] (11) \[(,,_{1:N}) =(,,_{1}, _{G}(_{1}))(,,_{N},_{G}(_{N})),\]

where \(L_{}\) is the IoU loss  over the rendered and the given mask, \(R()\) is a differentiable renderer , \(N\) is the number of detected garments, and \(\) represents the operation of mesh concatenation. \(\) is the SMPL body mesh, while \((,,_{1:N})\) is the concatenation of garment meshes reconstructed from the implicit sewing pattern as \(_{G}(_{i})\) and then draped by our draping model \(=_{m}_{s}\). In practice, the minimization is performed from the outermost garment to the innermost, one by one. Further details can be found in the Supplementary Material.

Fig. 7 depicts the results of this minimization. Our method outperforms the state-of-the-art methods SMPLicit , ClothWild  and DrapeNet , given the same garment masks. The garments we recover exhibit higher fidelity and have no collisions between them or with the underlying body.

### Garment Editing

As the panel shape of the sewing pattern determines the shape of the garment, we can easily manipulate the garment mesh by editing the panel. Fig. 8 depicts this editing process. We begin by moving the sleeve edges of the panel inwards to shorten the sleeves of the mesh, then move the bottom edges up to shorten the length of the jacket, and finally remove the edges for the opening to close it. This is achieved by minimizing

\[L()=d(E(),)\,\ \ E()=\{ |s(,)=0,\}\,\] (12)

w.r.t. \(\). \(d()\) computes the chamfer distance, \(\) represents the edges of the modified panel, and \(E()\) represents the 0 iso-level extracted from \(_{}\), that is, the edges of the reconstructed panel. Our sewing pattern representation makes it easy to specify new edges by drawing and erasing lines in

Figure 8: **Shape editing**. Garment attributes can be edited by modifying the sewing pattern.

Figure 7: **Garment recovery from images**. We compare the meshes recovered by our method and the state of the art methods SMPLicit , ClothWild , DrapeNet  (unavailable for skirts).

the 2D panel images, whereas the fully implicit garment representation of  requires an auxiliary classifier to identify directions in the latent space for each garment modification. As shown in the supplementary material, the texture of the garment can also be edited by drawing on the UV panels.

## 5 Conclusion

We have introduced a novel representation for garments to be draped on human bodies. The garments are made of flat 2D panels whose boundary is defined by a 2D SDF. To each panel is associated a 3D surface parameterized by the 2D panel coordinates. Hence, different articles of clothing are represented in a standardized way. This allows the draping of multi-layer clothing on bodies and the recovery of such clothing from single images. Our current implementation assumes quasi-static garments and only deforms the outer garment to solve collisions in multi-layer draping. In future work, we will introduce garment dynamics and focus on more accurate physical interactions between the outer and inner garments.

**Acknowledgement.** This project was supported in part by the Swiss National Science Foundation.