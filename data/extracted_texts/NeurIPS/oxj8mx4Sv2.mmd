# Deep Discriminative to Kernel Generative Networks

for Calibrated Inference

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The fight between discriminative versus generative goes deep, in both the study of artificial and natural intelligence. In our view, both camps have complementary values. So, we sought to synergistically combine them. Here, we propose a methodology to convert deep discriminative networks to kernel generative networks. We leveraged the fact that deep models, including both random forests and deep networks, learn internal representations which are unions of polytopes with affine activation functions to conceptualize them both as generalized partitioning rules. We replace the affine function in each polytope populated by the training data with Gaussian kernel that results in a generative model. Theoretically, we derive the conditions under which our generative models are a consistent estimator of the corresponding class conditional density. Moreover, our proposed models obtain well calibrated posteriors for in-distribution, and extrapolate beyond the training data to handle out-of-distribution inputs reasonably. We believe this approach may be an important step in unifying the thinking and the approaches across the discriminative and the generative divide.

## 1 Introduction

Machine learning methods, specially deep neural networks and random forests have shown excellent performance in many real-world tasks, including drug discovery, autonomous driving and clinical surgery. However, calibrating confidence over the whole feature space for these models remains a key challenge in the field. Although these learning algorithms can achieve near optimal performance at inferring on the samples lying in the high density regions of the training data , they yield highly confident predictions for the samples lying far away from the training data . Calibrated confidence within the training or in-distribution (ID) region as well as in the out-of-distribution (OOD) region is crucial for safety critical applications like autonomous driving and computer-assisted surgery, where any aberrant reading should be detected and taken care of immediately . A well-calibrated model capable of quantifying the uncertainty associated with inference for any points from the training distribution as well as detecting OOD data can be a life-saver in these cases.

The approaches to calibrate OOD confidence for learning algorithms described in the literature can be roughly divided into two groups: discriminative and generative. Discriminative approaches try to scale the posteriors based on OOD detection or modify the learning loss function. Intuitively, the easiest solution for OOD confidence calibration is to learn a function that gives higher scores for in-distribution samples and lower scores for OOD samples, and thereby re-scale the posterior or confidence score from the original model accordingly . There are a number of approaches in the literature which try to either modify the loss function  or adversarially train the network to be less confident on OOD samples . However, one can adversarially manipulate an OOD sample where the model is less confident to find another OOD sample where the model is overconfident . Recently, as shown by Hein et al. , the ReLU networks produce arbitrarily highconfidence as the inference point moves far away from the training data. Therefore, calibrating RELU networks for the whole OOD region is not possible without fundamentally changing the network architecture. As a result, all of the aforementioned algorithms are unable to provide any guarantee about the performance of the network through out the whole feature space. On the other end of the spectrum, the generative group tries to learn generative models for both the in-distribution as well as the out-of-distribution samples. The general idea for the generative group is to get likelihoods for a particular sample out of the generative models for both ID and OOD to do likelihood ratio test  or control the likelihood for training distribution far away from the training data to detect OOD samples by thresholding. However, it is not obvious how to control likelihoods far away from the training data for powerful generative models like variational autoencoders (VAEs)  and generative adversarial networks (GAN) . Moreover, Nalisnick et al.  and Hendrycks et al.  showed VAEs and GANs can also yield overconfident likelihoods far away from the training data.

The algorithms described so far are concerned with OOD confidence calibration for deep-nets only. However, in this paper, we show that other approaches which partition the feature space, for example random forest, can also suffer from poor confidence calibration both in the ID and the OOD regions. Moreover, the algorithms described above are concerned about the confidence of the algorithms in the OOD region only and they do not address the confidence calibration within the training distribution at all. This issue is addressed separately in a different group of literature [17; 18; 19]. In this paper, we consider both calibration problem jointly and propose an approach that achieves good calibration throughout the whole feature space.

In this paper, we conceptualize both random forest and RELU networks as generalized partitioning rules with an affine activation over each polytope. We consider replacing the affine functions learned over the polytopes with Gaussian kernels. We propose two novel kernel density estimation techniques named _Kernel Generative Forest_ (KGF) and _Kernel Generative Network_ (KGM). We theoretically show that they asymptotically converge to the true training distribution under certain conditions. At the same time, the estimated likelihood from the kernel generative models decreases for samples far away from the training samples. By adding a suitable bias to the kernel density estimate, we can achieve calibrated posterior over the classes in the OOD region. It completely excludes the need for providing OOD training examples to the model. We conduct several simulation and real data studies that show both KGF and KGN are robust against OOD samples while they maintain good performance in the in-distribution region.

## 2 Related Works and Our Contributions

There are a number of approaches in the literature which attempt to learn a generative model and control the likelihoods far away from the training data. For example, Ren et al.  employed likelihood ratio test for detecting OOD samples. Wan et al.  modify the training loss so that the downstream projected features follow a Gaussian distribution. However, there is no guarantee of performance for OOD detection for the above methods. To the best of our knowledge, only Meinke et al.  has proposed an approach to guarantee asymptotic performance for OOD detection. They model the training and the OOD distribution using Gaussian mixture models which enable them to control the class conditional posteriors far away. Compared to the aforementioned methods, our approach differs in several ways:

* We address the confidence calibration problems for both RELU-nets and random forests from a common ground.
* We address in-distribution (ID) and out-of-distribution (OOD) calibration problem as a continuum rather than two separate problems.
* We provide guarantees for asymptotic convergence of our proposed approach under certain conditions for both ID and OOD regions.
* We propose an unsupervised OOD calibration approach, i.e., we do not need to train exhaustively on different OOD samples.

Methods

### Setting

Consider a supervised learning problem with independent and identically distributed training samples \(\{(_{i},y_{i})\}_{i=1}^{n}\) such that \((X,Y) P_{X,Y}\), where \(X P_{X}\) is a \(^{d}\) valued input and \(Y P_{Y}\) is a \(=\{1,,K\}\) valued class label. We define in-distribution region as the high density region of \(P_{X,Y}\) and denote it by \(\). Here the goal is to learn a confidence score, \(:^{d}^{K}\), \(()=[g_{1}(),g_{2}(),,g_{K}( )]\) such that,

\[g_{y}()=P_{Y|X}(y|),& \\ P_{Y}(y),&, y \] (1)

where \(P_{Y|X}(y|)\) is the posterior probability for class \(y\) given by the Bayes formula:

\[P_{Y|X}(y|)=(|y)P_{Y}(y)}{_{k=1}^{K}P_{X |Y}(|k)P_{Y}(k)}, y.\] (2)

Here \(P_{X|Y}(|y)\) is the class conditional density for the training data which we will refer as \(f_{y}()\) hereafter for brevity.

### Background and Main Idea

Deep discriminative networks partition the feature space \(^{d}\) into a union of \(p\) affine polytopes \(Q_{r}\) such that \(_{r=1}^{p}Q_{r}=^{d}\), and learn an affine function over each polytope [4; 20]. Mathematically, the class-conditional density for the label \(y\) estimated by these deep discriminative models at a particular point \(\) can be expressed as:

\[_{y}()=_{r=1}^{p}(_{r}^{}+b_{r}) ( Q_{r}).\] (3)

For example, in the case of a decision tree, \(_{r}=\), i.e., decision tree assumes uniform distribution for the class-conditional densities over the leaf nodes. Among these polytopes, the ones that lie on the boundary of the training data extend to the whole feature space and hence encompass all the OOD samples. Since the posterior probability for a class is determined by the affine activation over each of these polytopes, the algorithms tend to be overconfident when making predictions on the OOD inputs. Moreover, there exist some polytopes that are not populated with training data. These unpopulated polytopes serve to interpolate between the training sample points. If we replace the affine activation function of the populated polytopes with Gaussian kernel \(\) learned using maximum likelihood approach on the training points within the corresponding polytope and prune the unpopulated ones, the tail of the kernel will help interpolate between the training sample points while assigning lower likelihood to the low density or unpopulated polytope regions of the feature space. This may result in better confidence calibration for the proposed modified approach.

### Proposed Model

Consider the collection of polytope indices \(\) which contains the indices of total \(\) polytopes populated by the training data. We consider replacing the affine function over the populated polytopes with a Gaussian kernel \((:_{r},_{r})\). For a particular inference point \(\), we consider the Gaussian kernel with the minimum distance from the center of the kernel to the corresponding point:

\[r_{}^{*}=*{argmin}_{r}\|_{r}-\|,\] (4)

where \(\|\|\) denotes a suitable distance measure. We use Euclidean distance metric while conducting the simulation and the benchmark datasets experiments in this paper for simplicity. In short, we modify Equation 3 from the parent ReLU-net or random forest to estimate the class-conditional density as:

\[_{y}()=}_{r}n_{ry}(;_{r},_{r})(r=r_{}^{*}),\] (5)where \(n_{y}\) is the total number of samples with label \(y\) and \(n_{ry}\) is the number of samples from class \(y\) that end up in polytope \(Q_{r}\). We add a bias to the class conditional density \(_{y}\):

\[_{y}()=_{y}()+.\] (6)

Note that in Equation 6, \( 0\) as the total training points, \(n\). The class posterior probability (confidence) \(_{y}()\) of class \(y\) for a test point \(\) is estimated using the Bayes rule as follows:

\[_{y}()=_{y}()_{Y}(y)}{_{k= 1}^{K}_{k}()_{Y}(k)},\] (7)

where \(_{Y}(y)\) is the empirical prior probability of class \(y\) estimated from the training data. We estimate the class for a particular inference point \(\) as:

\[=*{argmax}_{y}_{y}().\] (8)

### Desiderata

We desire our proposed model to estimate confidence score \(_{y}\) to satisfy the following two desiderata:

1. **Asymptotic Performance**: We want point-wise convergence for our estimated confidence as \(n\), i.e., \[_{y}_{^{d}}|g_{y}()- _{y}()| 0.\]
2. **Finite Sample Performance**: We want better posterior calibration for \(_{y}()\) both in ID and OOD region compared to that of its parent model.

We theoretically derive the conditions under which we achieve Desiderata 1 in Section 4. However, we run extensive experiments on various simulation and benchmark datasets in Section 6 to empirically verify that our proposed approach achieves Desiderata 2.

## 4 Theoretical Results

**Theorem 1** (Asymptotic Convergence to the True Distribution).: _Consider a partition rule that partitions \(^{d}\) into hypercubes of the same size \(h_{n}>0\). Formally, let \(_{n}=\{Q_{1},Q_{2},\}\) be a partition of \(^{d}\), that is, it partitions \(^{d}\) into sets of the type \(_{i=1}^{d}[_{i}h_{n},(_{i}+1)h_{n})\), where \(_{i}\)'s are integers. Let \(n\) be the total number of samples and \(n_{r}\) be the number of data points within polytope \(Q_{r}\). Consider the probability density \(f\) estimated for the samples populating the polytopes using Equation 5, denoted as \(\). The conditions for choosing the Gaussian kernel parameters are:_

1. _The center of the kernel can be any point_ \(z_{r}\) _within the polytope_ \(Q_{r}\) _as_ \(n\)_,_
2. _The kernel bandwidth along any dimension_ \(_{r}\) _is any positive number always bounded by the polytope bandwidth_ \(h_{n}\) _as_ \(n\)_, i.e.,_ \(_{r}=C_{r}h_{n}\)_, where_ \(0<C_{r} 1\)_._

_Consider the following assumptions as well:_

1. _The polytope bandwidth_ \(h_{n} 0\) _as_ \(n\)_._
2. \(n\) _grows faster than the shrinkage of_ \(h_{n}\)_, i.e.,_ \(nh_{n}\) _as_ \(h_{n} 0\) _in probability._

_Given these assumptions, we have that as \(n\):_

\[_{^{d}}|f()-()| 0,\]

_where \(||\) denotes absolute value of the scalar it operates on._

Proof.: Please see Appendix A for the proof.

**Theorem 2** (Asymptotic OOD Convergence).: _Given \(n\) independent and identically distributed training samples \(\{(_{i},y_{i})\}_{i=1}^{n}\), we define the distance of an inference point \(\) from the training points as:_

\[d_{}=_{i=1,,n}\|-_{i}\|.\] (9)

_Here \(\|\|\) denotes a suitable distance measure as mentioned in Equation 4. Given non-zero and bounded bandwidth of the Gaussians, then we have almost sure convergence for \(_{y}\) as: \(_{y}()_{Y}(y)\) as \(d_{}\)._

Proof.: Please see Appendix A for the proof. 

**Corollary 2.1**.: _Given the conditions in Theorem 1 and 2, we have:_

\[_{y}_{^{d}}|g_{y}()- _{y}()| 0.\]

Proof.: Using the law of large numbers, we have \(_{Y}(y)=}{n}P_{Y}(y)\) as \(n_{y}\). The rest of the proof follows from Theorem 1 and 2. 

## 5 Model Parameter Estimation

### Gaussian Kernel Parameter Estimation

Theorem 1 implies that the Gaussian kernel parameters need to maintain two key properties. We use the training data within the polytopes to estimate the Gaussian parameters in a way that we asymptotically satisfy the above two conditions for consistency. To satisfy the first condition, we set the kernel center as:

\[_{r}=}_{i=1}^{n}_{i}(_{i} Q_{r}).\] (10)

Note that \(_{r}\) in Equation 10 resides always within the corresponding polytope \(Q_{r}\). For improving the estimates for the kernel bandwidth, we incorporate the samples from other polytopes \(Q_{s}\) based on the similarity \(w_{rs}\) between \(Q_{r}\) and \(Q_{s}\). Moreover, We constrain our estimated Gaussian kernels to have diagonal covariance matrix. We use weighted likelihood estimation to estimate the variance \(_{r}\) for a particular polytope \(Q_{r}\). For simplicity, we will describe the estimation procedure for \(w_{rs}\) later. The weighted likelihood estimation for \(_{r}\) can be written as:

\[_{r}=*{argmin}_{}-_{i=1}^{n}_{s }w_{rs}(_{i} Q_{s})(_{i};_{r},)+\|^{-1}\|_{F}^{2},\] (11)

where we regularize the Frobenius norm of precision matrix \(^{-1}\) so that \(\) does not become singular and \(\) is the regularization parameter. By solving Equation 11, we find:

\[_{r}=}_{i=1}^{n}w_{rs}( _{i} Q_{s})(_{i}-_{r})(_{i}-_{r})^{}+ I_{d}}{_{s}_{i=1}^{n}w_{rs} (_{i} Q_{s})},\] (12)

where, \(I_{d}\) is a \(d\) dimensional identity matrix. However, we want \(_{r}\) to be estimated based on the samples within \(Q_{r}\) so that the second condition for the Gaussian parameters is satisfied. Therefore, as \(n\) and \(h_{n} 0\), the estimated weights \(w_{rs}\) should satisfy the condition:

\[w_{rs}0,&Q_{r} Q_{s}\\ 1,&Q_{r}=Q_{s}.\] (13)

We need Condition 13 as we will be only using the data within the polytope \(Q_{r}\) as \(n\) to estimate the Gaussian bandwidth and the estimated Gaussian bandwidth will be bounded by the polytope bandwidth. Additionally, we use weighted samples to replace the ratio \(}{n_{y}}\) in Equation 5 as:

\[_{ry}}{_{y}}=_{ry}}{_{r}_{ry}}=}_{i=1}^{n}w_{rs}( _{i} Q_{s})(y_{i}=y)}{_{r}_{s }_{i=1}^{n}w_{rs}(_{i} Q_{s}) (y_{i}=y)}.\] (14)Note that if we satisfy Condition 13, then we have \(_{rs}}{_{y}}}{n_{y}}\) as \(n\). Therefore, we modify Equation 5 as:

\[_{y}()=_{y}}_{r}_ {ry}(;_{r},_{r})(r=_{}^{*}),\] (15)

where \(_{}^{*}=*{argmin}_{r}\|_{r}-\|\). Below, we describe how we estimate \(w_{rs}\) for KGF and KGN.

### Kernel Generative Forest

Consider \(T\) number of decision trees in a random forest trained on \(n\) i.i.d training samples \(\{(_{i},y_{i})\}_{i=1}^{n}\). Each tree \(t\) partitions the feature space into \(p_{t}\) polytopes resulting in a set of polytopes: \(\{\{Q_{t,r}\}_{r=1}^{p_{t}}\}_{t=1}^{T}\). The intersection of these polytopes gives a new set of polytopes \(\{Q_{r}\}_{r=1}^{p}\) for the forest. For any point \(_{r} Q_{r}\), we push every other sample point \(_{s} Q_{s}\) down the trees. Now, we define the weight \(w^{}_{rs}\) as:

\[w^{}_{rs}=}{T},\] (16)

where \(t_{rs}\) is the total number trees \(_{r}\) and \(_{s}\) end up in the same leaf node. Note that \(0 w^{}_{rs} 1\). _If the two samples end up in the same leaves in all the trees, they belong to the same polytope, i.e. \(Q_{r}=Q_{s}\)._

In short, \(w^{}_{rs}\) is the fraction of total trees where the two samples follow the same path from the root to a leaf node. We exponentiate \(w^{}_{rs}\) with a suitable function of \(n\) which grows with \(n\) so that Condition 13 is satisfied:

\[w_{rs}=(w^{}_{rs})^{O(n)}.\] (17)

### Kernel Generative Network

Consider a fully connected ReLU-net trained on \(n\) i.i.d training samples \(\{(_{i},y_{i})\}_{i=1}^{n}\). We have the set of all nodes denoted by \(_{l}\) at a particular layer \(l\). We can randomly pick a node \(n_{l}_{l}\) from \(_{l}\) at each layer \(l\), and construct a sequence of nodes starting at the input layer and ending at the output layer which we call an **activation path**: \(m=\{n_{l}_{l}\}_{l=1}^{L}\). Note that there are \(N=_{i=1}^{L}|_{l}|\) possible activation paths for a sample in the ReLU-net, where \(||\) denotes the cardinality or the number of elements in the set. We index each path by a unique identifier number \(z\) and construct a sequence of activation paths as: \(=\{m_{z}\}_{z=1,,N}\). Therefore, \(\) contains all possible activation pathways from the input to the output of the network.

While pushing a training sample \(_{i}\) through the network, we define the activation from a ReLU unit at any node as '1' when it has non-negative input and '0' otherwise. Therefore, the activation indicates on which side of the affine function at each node the sample falls. The activation for all nodes in an activation path \(m_{z}\) for a particular sample creates an **activation mode**\(a_{z}\{0,1\}^{L}\). If we evaluate the activation mode for all activation paths in \(\) while pushing a sample through the network, we get a sequence of activation modes: \(_{r}=\{a_{z}^{r}\}_{z=1}^{N}\). Here \(r\) is the index of the polytope where the sample falls in.

_If the two sequences of activation modes for two different training samples are identical, they belong to the same polytope._ In other words, if \(_{r}=_{s}\), then \(Q_{r}=Q_{s}\). This statement holds because the above samples will lie on the same side of the affine function at each node in different layers of the network. Now, we define the weight \(w^{}_{rs}\) as:

\[w^{}_{rs}=^{N}(a_{z}^{r}=a_{z}^{s})}{N}.\] (18)

Note that \(0 w^{}_{rs} 1\). In short, \(w^{}_{rs}\) is the fraction of total activation paths which are identically activated for two samples in two different polytopes \(r\) and \(s\). We exponentiate the weights using Equation 17.

Pseudocodes outlining the two algorithms are provided in Appendix C.

## 6 Experimental Results

We conduct several experiments on two dimensional simulated datasets and OpenML-CC18 data suite 1 to gain insights on the finite sample performance of KGF and KGN. The details of the simulation datasets and hyperparameters used for all the experiments are provided in Appendix B. For the simulation setups, we use classification error, hellinger distance  from the true class conditional posteriors and mean max confidence or posterior  as performance statistics. While measuring in-distribution calibration for the datasets in OpenML-CC18 data suite, as we do not know the true distribution, we used adaptive calibration error as defined by Nixon et al.  with a fixed bin number of \(R=15\) across all the datasets. Given \(n\) OOD samples, we define OOD calibration error to measure OOD performance for the benchmark datasets as:

\[|_{i=1}^{n}_{y}(_{Y|X}(y|_{i}))-_{y}(_{Y}(y))|.\]

### Simulation Study

Figure 1 top row shows \(10000\) training samples with \(5000\) samples per class sampled within the region \([-1,1][-1,1]\) from the five simulation setups described in Appendix B. Therefore, the empty annular region between \([-1,1][-1,1]\) and \([-2,2][-2,2]\) is the low density or OOD

Figure 1: **Visualization of true and estimated posteriors for class 0 from five binary class simulation experiments.**_Row 1_: 10,000 training points with 5,000 samples per class sampled from 5 different simulation setups for binary class classification. The class labels are indicated by yellow and blue colors. _Row 2_: True class conditional posteriors. _Row 3_: Estimated posteriors from random forest. _Row 4_: Estimated posteriors from KGF. _Row 5_: Estimated posteriors from Deep-net. _Row 6_: Estimated posteriors from KGN. The posteriors estimated from KGN and KGF are better calibrated for both in- and out-of-distribution regions compared to those of their parent algorithms.

[MISSING_PAGE_FAIL:8]

each dataset is shown separately in appendix Figure 4, 5, 6, 7, 8 and 9. Figure 3 left column shows on average KGF and KGN has nearly similar classification accuracy to their respective parent algorithm. However, according to Figure 3 middle column, KGF improves the in-distribution calibration for random forest by a huge margin. On the contrary, KGN maintains similar in-distribution calibration performance to that of its parent ReLU-net. Most interestingly, Figure 3 right column KGN and KGF improves OOD calibration of their respective parent algorithms by a huge margin.

## 7 Discussion

In this paper, we convert deep discriminative models to deep generative models by replacing the affine function over the polytopes in the discriminative models with a Gaussian kernel. This replacement of affine function results in better in- and out-of-distribution calibration for our proposed approaches while maintaining classification accuracy close to the parent algorithm. Theoretically, we show under certain conditions our approaches asymptotically converge to the true training distribution and this establishes confidence calibration for learning algorithms in in- and out-of-distribution regions as a continuum rather than two different problems.

For a feature space densely partitioned with small polytopes, we can use Euclidean distance metric in Equation 4. This is because the Euclidean manifold approximation holds locally for the corresponding polytope with index \(r_{}^{*}\). On the contrary, for a feature space partitioned with large polytopes, Euclidean distance measure may be a wrong notion of distance, specially when the underlying manifold is non-Euclidean. Note that the indicator function in Equation 5 and pruning of the unpopulated polytopes result in an enlargement of the polytopes in our proposed method compared to that of the parent model in Equation 3. Therefore, our proposed approach while using Euclidean metric in Equation 4 may have less classification accuracy compared to that of its parent algorithm. A correct measure of distance in all the cases including the aforementioned non-Euclidean one would be the geodesic distance as explored by Madhyastha et al. . We will explore convolutional neural nets (CNN) trained on image and language benchmark datasets using geodesic distance in Equation 4 in our future work. Additionally, the proposed approach needs benchmarking against other calibration approaches in the literature which are mainly based on image datasets and we will pursue the benchmarking task in our future work.

Figure 3: **Performance summary of KGF and KGN on OpenML-CC18 data suite. The dark red curve in the middle shows the median of performance on \(46\) datasets. The shaded region shows the error bar consisting of the \(25\)-th and the \(75\)-th percentile of the performance statistics. Left: KGF and KGN maintains performance close to their parent algorithms for classification. Middle: KGF significantly improves the in-distribution calibration for random forest and KGN improves ReLU-netâ€™s in-distribution calibration for high training sample sizes. Right: Both of the proposed approaches yield highly calibrated confidence in the OOD region.**