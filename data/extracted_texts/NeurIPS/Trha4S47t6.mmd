# Hypothesis Tests for Distributional Group Symmetry

with Applications to Particle Physics

Kenny Chiu

Department of Statistics

University of British Columbia

kenny.chiu@stat.ubc.ca

&Benjamin Bloem-Reddy

Department of Statistics

University of British Columbia

benbr@stat.ubc.ca

###### Abstract

Symmetry plays a central role in the sciences, machine learning, and statistics. When data are known to obey a symmetry, various methods that exploit symmetry have been developed. However, statistical tests for the presence of group invariance focus on a handful of specialized situations, and tests for equivariance are largely non-existent. This work formulates non-parametric hypothesis tests, based on a single independent and identically distributed sample, for distributional symmetry under a specified group. We provide a general formulation of tests for symmetry within two broad settings. Generalizing existing theory for group-based randomization tests, the first setting tests for the invariance of a marginal or joint distribution under the action of a compact group. The second setting tests for the invariance or equivariance of a conditional distribution under the action of a locally compact group. We show that the test for conditional symmetry can be formulated as a test for conditional independence. We implement our tests using kernel methods and apply them to testing for symmetry in problems from high-energy particle physics.

## 1 Introduction

Symmetry has played an important role in classical statistical problems , and more recently in modern problems in statistics and machine learning [e.g., 7, 10, 13, 32]. One key idea that emerges from this line of work is that by using models that account for symmetries present in data, one obtains statistical benefits through various forms of optimality , improved sample efficiency , and better out-of-sample generalization . A pervasive characteristic shared by all of that work is that a specific symmetry group is known or assumed, and the problem is carefully constructed with respect to that group. However, a symmetry assumption can be difficult to check and, if violated, can degrade performance when enforced in a model.

Separately, symmetry plays a central role in modern science, particularly in the physical sciences where entire theories are constructed around the symmetries that must be obeyed by equations describing the behaviour of physical systems . Additionally, detection of new or broken symmetries is playing a role in the search for physics beyond the Standard Model , particularly in data-driven approaches . Recent work in machine learning and physics aims to learn or estimate symmetry groups from data  or to detect anomalous symmetry-breaking . However, key inferential tools based on hypothesis tests for symmetry are missing. Such tools are crucial if the discovery of symmetry from data is to be a reliable part of the scientific process: they should be used to test for the presence or absence of a particular symmetry in data, with that symmetry specified by hypothesis or by a data-driven method that has learned or estimated a symmetry. In situations with known or assumed symmetry, hypothesis tests for symmetry could also be used as model-checking criteria for models meant to exhibit that symmetry.

The present work formulates non-parametric tests, based on a single independent and identically distributed (i.i.d.) sample, for distributional symmetry under a specified group. We provide abstract formulations of tests that apply to two broad settings. The first setting tests for the invariance of a marginal or joint distribution under the action of a compact group. The test is formulated as an easy-to-implement conditional Monte Carlo test that achieves exact \(p\)-values with finitely many observations and Monte Carlo samples. We establish properties of the test that generalize results from the statistics literature on group-based randomization tests, through an argument based on conditioning on a sufficient statistic induced by the group. The second setting tests for the invariance or equivariance of a conditional distribution under the action of a locally compact group, provided that the group action obeys weak regularity conditions. We show that a test for equivariance can be formulated as a particular test for conditional independence, which inherits the statistical properties of the conditional independence test chosen for implementation. Although universally statistically valid tests of conditional independence testing are known to be impossible , we implement as a proof-of-concept a valid test that is calibrated by cross-validation. Improved methods for this test are left to future work.

In addition to the generic testing methods and the study of their theoretical properties, we provide specific instantiations of the tests using kernel-based methods, which allow these tests to be used with any data structure for which a characteristic kernel exists. We apply these tests to two problems in high-energy particle physics. Computer code required to run the experiments can be found on a GitHub repository (https://github.com/chikenny/Tests-for-Distributional-Symmetry).

## 2 Testing for distributional invariance

The mathematical object that encodes symmetry is a group \(\). We provide a review of relevant concepts from group theory in Appendix A. We assume throughout that \(\) has a topology that is locally compact, second countable, and Hausdorff (lccsH), and which makes the group operations continuous. Elements \(g\) act via transformations \(x gx\) of elements from a sample space \(x\) that has a topology and a corresponding Borel \(\)-algebra, \(_{}\). We assume throughout that the group action is continuous and, if \(\) is non-compact, that it is also proper. This ensures that none of the required measure-theoretic properties "break." (See Appendix A for details.)

The action on \(\) extends to the set \(()\) of probability measures on \(\): If \(P\) is the distribution of a random element \(X\), then \(g\) acts on \(P\) via the pushforward, \(g_{*}P(A) P(g^{-1}A)\), with \(A\) and \(g^{-1}A\{g^{-1}x:x A\}\). A key question in many settings is whether the distribution \(P\) underlying a set of i.i.d. observations \(X_{1:n}(X_{1},,X_{n})\) is _invariant_ under \(\) in the sense that \(g_{*}P=P\) for each \(g\). Outside of ill-behaved situations that typically do not arise in practice, this is only possible for a probability measure when \(\) is compact. Any compact group \(\) has a unique invariant (_Haar_) probability measure \(\) that can be thought of as the uniform distribution on \(\).

For a specified compact group \(\), the statistical problem we address is to test the hypotheses

\[H_{0} P$-invariant} H_{1}  P$-invariant }.\]

If \(\) is relatively small and finite, or generated by a small set of elements (say of size \(m\)), invariance might be tested with a composite of \(m\) two-sample hypothesis tests. For large discrete groups, this approach quickly becomes untenable; for uncountable groups, it is not possible. Instead, we propose tests based on other characterizations of distributional invariance. Perhaps the most well-known characterization is that \(P=P^{}\) if and only if \(P\) is \(\)-invariant, where \(P^{}\) is the _orbit-averaged distribution_ obtained by _orbit-averaging_\(g_{*}P\) over \(\) with respect to Haar measure \(\),

\[P^{}(A)_{}P(g^{-1}A)\ (dg)\, A _{}\.\] (1)

Because both \(P\) and \(P^{}\) are probability measures on \(\), any metric \(D\) on \(()\) can be used in conjunction with the empirical measure and a Monte Carlo estimate of the integral in (1) to define a test statistic of the form

\[T_{n,m}(X_{1:n}) D_{i=1}^{n}_{X_{i}}( \,\,),\ \ _{i=1}^{n}_{j=1}^{m}_{G_{i,j}X_{i}}(\,\,) \, G_{i,j}}}{{}}\,\] (2)

where \(_{x}\) denotes the Dirac measure at a point \(x\). This approach is very general and can be used for abstract spaces \(\) other than \(^{d}\) as long as one has a metric on \(()\) and the ability to sample random elements of \(\). For a sequence of metric-based statistics \((T_{n,m})_{n 1}\) with fixed \(D\) and \(m 1\), and critical values \((c_{n})_{n 1}\), define the corresponding sequence of critical functions, or tests,

\[_{n,m}(X_{1:n})\{T_{n,m}(X_{1:n})>c_{n}\}\.\] (3)

Theorem 2 in Appendix B.1 shows that for appropriate sequences \((c_{n})_{n 1}\), tests based on (2) are consistent. The main idea is that the averages inside the metric in Equation (2) converge to their respective probability distributions via the Law of Large Numbers. Therefore, by the continuity of the metric \(D,T_{n,m}(X_{1:n})\) converges almost surely to \(D(P,P^{})\).

Beyond this general-purpose averaging approach, more detailed structure induced by \(\) is often available, and we can use it to construct an exact test for finite \(n\). The group action partitions \(\) into equivalence classes called _orbits_ so that \(x\) and \(x^{}\) are equivalent if and only if \(x=gx^{}\) for some \(g\). One can choose a _representative_ element \([x]\) of each orbit to obtain a set of representatives \([]\), which can then be used to define an _orbit selector_\([]\) that maps \(x\) to its representative \([x]\). The orbit selector induces a decomposition so that a random variable \(X\) has an invariant distribution if and only if it satisfies \(X}{{=}}G(X)\), where \(G\!\!\! X\) is sampled uniformly from \(\).

If \(\) acts _freely_ on \(\) in the sense that \(gx=x\) implies \(g\) is the identity element of \(\), then the orbit selector \(\) can be "inverted" to obtain the element of \(\) that sends \([x]\) to \(x\). We call such a function, denoted \(\), a _representative inversion_ because it satisfies \((x)(x)=(x)[x]=x\). Yet another characterization of \(\)-invariance is that \(P\) disintegrates as \(P=_{*}P\). In this case, a metric on the space of probability measures on \([]\) can be used as a test statistic, where the joint distribution of \(((X),(X))\) is compared to that of \((G,(X))\), with \(G\). If the action of \(\) is not free, so that \(gx=x\) for \(g\) in some non-trivial subset of \(\), then \(\) can be replaced by an appropriate random variable \(\) sampled from an _inversion kernel_, \((x,\ ^{})\). The inversion kernel has a number of remarkable properties; the relevant one here is that if \((x,\ ^{})\), then \((x)=x\) with probability one. From this, a characterization of \(\)-invariance is that \((,(X))}{{=}}(G,(X))\).

We summarize the above characterizations of distributional invariance in the following.

**Proposition 1**.: _Let \(\) be a compact group acting on \(\) and \(P\) a probability measure on \(\). Let \(\) be a measurable orbit selector and \(\) a measurable inversion kernel. With \(X P\), the following are equivalent:_

1. \(P\) _is_ \(\)_-invariant._
2. \(P=P^{}\)_._
3. _If_ \(G\) _with_ \(G\!\!\! X\)_, then_ \(X}{{=}}GX\)_._
4. _If_ \(G\) _and_ \(Y_{*}P\) _with_ \(G\!\!\! Y\)_, then_ \(X}{{=}}GY\)_. This holds even conditionally on_ \((X)\)_. That is,_ \(((X),X,G)}{{=}}((X),G(X),G)\)_, which implies that_ \(X(X)}{{=}}G(X)(X)\)_._
5. _If_ \((X,\ ^{})\) _and_ \(G\) _with_ \(\!\!\! G\)_, then_ \(}{{=}}G\) _and_ \(\!\!\!(X)\)_. If there exists a representative inversion_ \((x)\)_, then this holds with_ \(\) _replaced by_ \((X)H\)_, where_ \(H_{_{(X)}}\)_._
6. \(P=_{*}P\)_._

It follows from invariance of Haar measure that Properties I0 and I1 imply each other, which is easy to verify. Property I2 is a reformulation of Property I1 in terms of random variables. These properties hold regardless of the existence of a measurable orbit selector and inversion kernel. Proving that Properties I0 and I3 imply each other is only slightly more involved. An accessible proof can be found in Eaton [21, Theorems 4.3-4.4]; see also Kaltenberg [36, Theorem 7.15]. Property I3 and Property I4 imply each other using the identity \(x}}}}}}}}}}}}}{}(x)\). Property I5 is a reformulation of Property I4 in terms of a disintegration into the corresponding probability measures. The following example illustrates the main ideas.

**Example 1**.: _Let \(=^{d}\), so that \(X\) is a random \(d\)-dimensional real vector. The isotropic multivariate normal distribution \((0,_{d})\) is known to be invariant under the action of \((d)\), the group of \(d\)-dimensional rotation matrices, where the action is by matrix-vector multiplication. Properties I1and I2 in Proposition 1 are straightforward to check. Using the standard formula for affine transformations of a multivariate normal distribution, if \(X(0,_{d})\) and \(g(d)\), then \(gX\) has distribution \((0,g_{d}g^{})=(0,_{d})\). This holds for all \(g\), and therefore it also holds for random \(G\)._

_The set of orbit representatives can be chosen to be the points on the axis with unit basis vector \(_{1}=[1,0,,0]^{}\). Then for each \(x^{d}\), \((x)=\|x\|_{1}\). For \(d=2\), the action is free; for \(d>2\), the set of \(d\)-dimensional rotations around the axis corresponding to \(_{1}\) leave \((x)\) invariant. When \(X(0,_{d})\), \(\|X\|\) has a \(_{d}\)-distribution (the square root of a \(_{d}^{2}\)-distributed random variable), and \(Y}{{=}}\|X\|_{1}\) satisfies \(X}{{=}}GY\), with \(G\) a uniform random rotation from \((d)\). The left column of Figure 1 illustrates this for \(d=2\). One may construct a representative inversion function corresponding to \((x)=\|x\|_{1}\) by, for example, rotating \(\|x\|_{1}\) to \(x\) in the 2D subspace spanned by \(}{{\|x\|}}\) and \(_{1}\). That is, let \(_{1},x_{1} )}}{{\|x-_{1},x_{1}\|}}\), so that \([_{1},\;]\) is a matrix in \(^{d 2}\) whose columns form an orthonormal basis for the 2D subspace spanned by \(}{{\|x\|}}\) and \(_{1}\). Now let \(_{x}\) be such that \((_{x})=_{1},}{{\|x\|}}\), and \(R_{}\) the standard 2D rotation matrix of angle \(\),_

\[R_{}=()&-()\\ ()&()\;.\]

_Then the \(d\)-dimensional rotation defined by_

\[(x)=_{d}-_{1}_{1}^{}- ^{}+[_{1},\;]R_{_{x}}[_{1},\;]^{}\] (4)

_satisfies \((x)(\|x\|_{1})=x\). A sample from the corresponding inversion kernel is then generated by taking a uniform random \((d-1)\)-dimensional rotation \(H\) and extending it to a \(d\)-dimensional rotation \(H^{}\) that fixes \(_{1}\), so that \((x)H^{}\) has distribution \((x,\,\,)\). For \(d=2\), \(((x),\,\,)=_{}\), so Property I4 indicates that \((X)}{{=}}G\), with \(G\) a uniform random 2D rotation. This is visualized in the bottom-left plot of Figure 1. Furthermore, for \(d=2\), \((0,_{d})\) can be expressed as the distribution \(_{1}\) over polar coordinates._

Figure 1: First row: Densities for the 2D multivariate Gaussian \((0_{2},_{2})\) (blue), Cartesian representation of the distribution \(_{2}(}{{4}},4)\) over polar coordinates (orange) and the same distribution averaged over \((2)\) (green). Second row: 50 samples from the respective distributions. Third row: Angles in \([0,2]\) needed for a counterclockwise rotation of each sample \(X_{i}\) to the point \((\|X_{i}\|,0)\), sorted in increasing order.

### Exact conditional Monte Carlo tests of invariance

To obtain a \(p\)-value for the test statistic defined in (2), we use group-based randomization techniques, which can yield tests with exact level \(\) for finite sample sizes. The test relies on the fact that \((X)\) is a special case of a _maximal invariant_ statistic, which is an invariant function that takes a different value on each orbit and thus uniquely encodes the orbits. We denote a generic maximal invariant by \(M(X)\). It is known that any maximal invariant is a sufficient statistic for \(^{}()\), the class of \(\)-invariant probability distributions [6; 15], which means that for each \(P^{}()\), a sample \(X_{1:n}}{{}}P\) has the same conditional distribution given \((X)_{1:n}\). We can then generate samples from that conditional distribution as \((G_{1}(X_{1}),,G_{n}(X_{n}))\), with \(G_{i}}{{}}\) and independent of \(X_{1:n}\). These samples can be used to estimate conditional quantities that are valid uniformly across the null hypothesis class \(^{}()\). Due to the invariance of \(\), \(GX}{{=}}G(X)\) (even conditionally on \((X)\)), so in practice we can replace \((X_{i})\) with \(X_{i}\). The conditional Monte Carlo sampling procedure we use is outlined in Algorithm 1.

```
1:procedureMcTest(\(X_{1:n},m,B,D\))
2: Sample \(G_{j,1},,G_{j,n}}{{}}\), for \(j=1,,m\)
3: Using \((G_{j,1},,G_{j,n})_{j m}\), compute \(T_{n,m}(X_{1:n})\) as in (2)
4:for\(b\) in \(1,,B\)do
5: Sample \(G_{1}^{(b)},,G_{n}^{(b)}}{{}}\)
6: Set \(X_{1:n}^{(b)}(G_{1}^{(b)}X_{1},,G_{n}^{(b)}X_{n})\)
7: (Re)using \((G_{j,1},,G_{j,n})_{j m}\), compute \(T_{n,m}(X_{1:n}^{(b)})\)
8:endfor
9:return\(p\)-value \(p_{B}\) computed as \[p_{B}^{B}\{T_{n,m}(X_{1:n}^{(b)}) T_ {n,m}(X_{1:n})\}}{1+B}\] (5)
10:endprocedure ```

**Algorithm 1** Exact conditional Monte Carlo \(p\)-value

Theorem 3 in Appendix B.2 formalizes that Algorithm 1 produces a valid \(p\)-value for \(B 1\). The estimate \(p_{B}\) can be used in a critical function \(\{p_{B}\}\), and the resulting test has level \(\). A special case of the result, for finite \(\), appeared in . Our result applies more generally to compact \(\) using an argument based on the sufficiency of \((X)\).

## 3 Conditional symmetry

In some problems, especially those involving regression, classification, or prediction of a variable \(Y\) from \(X\), symmetry of the conditional distribution \(P_{Y|X}\) is of interest. The conditional distribution is said to be _equivariant_ if for each measurable subset \(B\),

\[P_{Y|X}(gx,B)=P_{Y|X}(x,g^{-1}B)\, x,\ g\.\]

It is said to be invariant if the action of \(\) on \(\) is trivial, so that the above equation holds with \(g^{-1}B\) replaced by \(B\). Equivariant conditional distributions arise from the disintegration of jointly invariant probability distributions \(P_{X,Y}=P_{X} P_{Y|X}\). If \(\) is compact and the marginal distribution \(P_{X}\) is known to be invariant, then testing for conditional equivariance of \(P_{Y|X}\) is equivalent to testing for the joint invariance of \(P_{X,Y}\), which could be carried out using the methods described in Section 2. However, the marginal distribution of \(X\) may not be invariant--in many cases it is known not to be--and the problem cannot be reduced to a test for joint invariance. For example, if \(\) is non-compact, then \(P_{X}\) cannot be \(\)-invariant, but \(P_{Y|X}\) may be. We instead formulate a test for conditional symmetry (equivariance or invariance) based on a conditional independence property that characterizes equivariance. The following theorem shows that \(P_{Y|X}\) is equivariant if and only if

\[(,X)\!\!\!^{-1}Y M(X)\,  X(X,\,{}^{})\.\] (6)

**Theorem 1**.: _Let \(\) be a lcscH group acting on each of \(\) and \(\), with the action on \(\) proper, so that a measurable inversion kernel \(\) exists. Then \(P_{Y|X}\) is conditionally \(\)-equivariant if and only if (6) holds. If there exists a measurable inversion function \(\), then (6) reduces to_

\[X\!\!\!(X)^{-1}Y M(X)\.\] (7)

_If the action of \(\) on \(\) is trivial, then (6) reduces to \(X\!\!\! Y M(X)\). In any of the foregoing cases, if the action of \(\) on \(\) is transitive then the respective statements of conditional independence become statements of independence._

The theorem generalizes a result of Bloem-Reddy and Teh , who established a special case of the result under the assumptions that \(\) is compact and acts freely on \(\), and that \(P_{X}\) is \(\)-invariant. Theorem 1 relaxes all of these conditions so that it holds under the proper action of a locally compact group. The proof can be found in Appendix B.3.

The result implies that a test for conditional symmetry can be formulated as a test for conditional independence. In our experiments in Section 5, we use general-purpose kernel-based tests for conditional independence. However, it is known that testing for conditional independence under the most general assumptions is an impossible problem . Our implementation in Section 5 gets around this by calibrating the conditional independence test via cross-validation on an independent training set of data, thus restricting the null hypothesis set to be localized around the distribution that gave rise to the data. This approach should therefore be viewed as a first demonstration of what a test for equivariance may look like. An improved testing framework for equivariance is the focus of ongoing work.

## 4 Related work

Our conditional Monte Carlo test in Section 2 belongs to the family of group-based randomization tests; such tests have also been applied as tests for invariance under specific groups in specialized situations [31; 49; 50; 41]. The most general proof of which we are aware of pertains to finite groups . Our proof of the validity of the test for general compact groups is based on sufficiency arguments, which to our knowledge is different from (or implicit in) the existing literature; it may also be of independent interest for group-based randomization tests.

Apart from hypothesis testing, researchers in physics and machine learning have developed methods for estimating symmetries from data; see the references in Section 1. Hypothesis tests for symmetry, either as part of the estimation procedure or as validation of the estimated symmetry, have not been developed. To the best of our knowledge, the only exception is , which develops a test for anomaly detection, but requires restrictive distributional assumptions and approximations.

Whereas the group-based randomization testing literature uses group invariance primarily as a device for testing some other hypothesis, a smaller body of work [11; 26; 51] focuses on group symmetry as the property of interest. As we describe in more detail in Appendix D, those methods make strong assumptions that limit their applicability. Our methods are broadly applicable: both the abstract formulation of our tests for invariance and their kernel-based implementations can be applied to any compact group, which includes finite (discrete) groups. To our knowledge, the test we formulate in Section 3 is the first general-purpose test for symmetry (invariance or equivariance) of a conditional distribution.

Many of the mathematical techniques used in this work have appeared in various statistical contexts, and a thorough treatment can be found in [21; 54]. Inversion kernels do not seem to have been used previously in statistics or machine learning, perhaps owing to their relatively recent appearance in probability . However, special cases of representative inversions with deterministic versions have appeared in the recent machine learning literature [6; 33; 55].

## 5 Experiments

We evaluate our proposed tests on two applications from high-energy particle physics. In our experiments, we sample \(n\) data points from a dataset and perform a test for a specified symmetry. We repeat this procedure over \(N=1000\) simulations for each test and record the proportion of simulations in which the test rejected, which estimates either the test size or power. With \(N=1000\)simulations, estimates are precise up to approximately \( 0.016\). We use test level \(=0.05\) in our experiments. We use \(m=2\) sampled group actions except where otherwise specified.

We implement our tests for invariance and conditional symmetry using kernel methods. Background for kernel methods as well as specific details about these tests can be found in Appendices C and D.1. The tests that we evaluate for invariance include: a baseline two-sample test (2sMmd) that compares the original sample to a \(\)-transformed sample under the maximum mean discrepancy (MMD) ; a MMD test based on Algorithm 1 (Mmd) and a related test that uses Nystrom approximation with \(J=\) subsamples (Nmmd) ; and the Cramer-Wold test  with \(J=\) random projections (Cw). Where applicable, we use the sampling procedure described in Algorithm 1 with \(B=200\). We test for conditional symmetries using the kernel conditional independence test (Kci)  and the conditional permutation test  with kernel conditional density estimation (Cp, \(S=50\) steps).

### Large Hadron Collider dijet events

The first application that we examine is based on the Large Hadron Collider (LHC) Olympics 2020 dataset  consisting of 1.1 million simulated dijet events generated by PYTHIA , a widely-used Monte Carlo generator for high-energy physics processes. A dijet event is two jets of particles that are produced by the collision of subatomic particles. The transverse momentum, polar angle \(\) and pseudorapidity \(\) for up to 200 jet constituents were recorded for each jet. The Cartesian momentum of a particle in the transverse plane is represented by the pair

\[p_{x}=p_{}()\, p_{y}=p_{}()\.\]

The leading constituent in a jet is the particle with the largest transverse momentum in any direction. We focus on the joint distribution of the two constituents with the largest transverse momenta in each event [after 18]. A single observation is therefore a 4D vector \(X=(p_{1_{x}},p_{1_{y}},p_{2_{x}},p_{2_{y}})\), where \(p_{1}\) and \(p_{2}\) correspond to the momenta of the two leading particles, respectively. We randomly split the dataset into a training and test set of equal size. We draw samples of size \(n=100\) in all of the following experiments. Histograms of \(p\)-values obtained from the tests are shown in Figure 2.

#### 5.1.1 Joint invariance

By conservation of angular momentum, the distribution of the Cartesian momenta of the two leading particles across jet events should be invariant to simultaneous rotations by the same angle, i.e., with respect to the subgroup \(_{0}=\{(g_{1},g_{2})(2)(2):g_{1}=g_{2}\}\). We conduct tests for invariance with respect to this subgroup, as well as with respect to the full \(_{1}=(2)(2)\) group, and to \(_{2}=(4)\). Results are shown in Table 1. We see that 2sMmd, Mmd, and Cw are able to identify \(_{0}\)-invariance and correctly reject \(_{1}\)- and \(_{2}\)-invariance at a higher rate. In Appendix E.1, we find that increasing the number of random projections from 10 to 15 significantly improves the power of Nmmd.

#### 5.1.2 Conditional equivariance

By taking \(X_{i}=(p_{1_{x}},p_{1_{y}})\) and \(Y_{i}=(p_{2_{x}},p_{2_{y}})\), invariance of the 4D vector with respect to the subgroup \(_{0}\) can also be viewed as \(Y_{i}\) being conditionally equivariant with respect to \((2)\) given \(X_{i}\). We perform a test for \((2)\)-equivariance. We obtain rejection rates \(0.0\) for Kci and \(0.051\) for Cp in this setting. We also perform a test for conditional \((2)\)-invariance, which Kci correctly rejects with rate \(1.0\) and Cp with rate \(0.997\).

    & \(_{0}=\{(2)$-rotations}\}\) & \(_{1}=(2)(2)\) & \(_{2}=(4)\) \\ 
2sMmd & \(0.035\) & \(0.967\) & \(0.983\) \\ Mmd & \(0.038\) & \(1.000\) & \(1.000\) \\ Nmmd & \(0.058\) & \(0.241\) & \(0.214\) \\ Cw & \(0.052\) & \(0.971\) & \(0.999\) \\   

Table 1: Test rejection rates over \(N=1000\) simulations for the LHC data. Test significance levels were \(=0.05\). For \(_{0}\), rejection rates should be \( 0.05\); for \(_{1}\) and \(_{2}\), a higher rejection rate indicates a more powerful test (maximum 1).

### Top quark tagging

We consider a second particle physics application based on the Top Quark Tagging Reference dataset , which also consists of jet events simulated by PYTHIA. The original dataset was constructed for the task of classifying jet events as having decayed from a top quark or not and consists of a training, validation, and test set. We only use the test set, which contains 404,000 simulated jet events. The four-momenta \(p=(E,p_{x},p_{y},p_{z})\) of up to 200 jet constituents are recorded for each event. Each event is also labelled as 1 or 0, representing that the jet decayed from a top quark or did not, respectively. According to the Standard Model, when predicting whether a jet is the decay of a top quark based on the four-momenta of jet constituents, the distribution of the label should be conditionally invariant with respect to the Lorentz group \((1,3)\), which consists of spatial rotations and relativistic boosts and preserves the quadratic form \(Q(p)=E^{2}-p_{x}^{2}-p_{y}^{2}-p_{z}^{2}\). According to Theorem 1, conditional invariance is equivalent to \(X\!\!\! Y M(X)\) in this scenario.

For convenience, we take the data to be the four-momenta \(X=(p_{1},p_{2})\) of the two leading constituents in each jet [as in 56] and the top quark label \(Y\{0,1\}\). We split the data into a training and test set. We perform a test for conditional invariance of \(Y\) given \(X\) with respect to the Lorentz group based on samples of size \(n=200\). We use the 2D maximal invariant \(M(X)=(Q(p_{1}),Q(p_{2}))\). For the kernel on \(=\{0,1\}\), we use \(k_{Y}(x,y)=1(x=y)\). Kci rejects conditional invariance at a rate of \(0.029\), which is consistent with the theory of the Standard Model. To verify that Kci is identifying symmetry in a meaningful way, we simulate new labels \(Y^{}_{i}\) given \(X_{i}\) using the model

\[Y^{}_{i} X_{i}(0.91\{E 200\}+0.11\{E<20 0\})\.\]

With the new labels, Kci rejects conditional invariance with respect to the Lorentz group at a rate of \(0.781\). Histograms of the Kci \(p\)-values can be found in Appendix E.2. We were unable to tune Cp to produce meaningful results.

Figure 2: Histograms of the \(p\)-values obtained over \(N=1000\) simulations for tests for joint invariance and equivariance in the LHC experiments. The \(p\)-value of a Kolmogorov–Smirnov test for uniformity of the distribution is shown in the bottom-right corner of each plot.