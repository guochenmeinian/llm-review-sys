# Det-CGD: Compressed Gradient Descent with Matrix Stepsizes for Non-Convex Optimization

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

This paper introduces a new method for minimizing matrix-smooth non-convex objectives through the use of novel Compressed Gradient Descent (CGD) algorithms enhanced with a matrix-valued stepsize. The proposed algorithms are theoretically analyzed first in the single-node and subsequently in the distributed settings. Our theoretical results reveal that the matrix stepsize in CGD can capture the objective's structure and lead to faster convergence compared to a scalar stepsize. As a byproduct of our general results, we emphasize the importance of selecting the compression mechanism and the matrix stepsize in a layer-wise manner, taking advantage of model structure. Moreover, we provide theoretical guarantees for free compression, by designing specific layer-wise compressors for the non-convex matrix smooth objectives. Our findings are supported with empirical evidence.

## 1 Introduction

The minimization of smooth and non-convex functions is a fundamental problem in various domains of applied mathematics. Most machine learning algorithms rely on solving optimization problems for training and inference, often with structural constraints or non-convex objectives to accurately capture the learning and prediction problems in high-dimensional or non-linear spaces. However, non-convex problems are typically NP-hard to solve, leading to the popular approach of relaxing them to convex problems and using traditional methods. Direct approaches to non-convex optimization have shown success but their convergence and properties are not well understood, making them challenging for large scale optimization. While its convex alternative has been extensively studied and is generally an easier problem, the non-convex setting is of greater practical interest often being the computational bottleneck in many applications.

In this paper, we consider the general minimization problem:

\[_{x^{d}}f(x),\] (1)

where \(f:^{d}\) is a differentiable function. In order for this problem to have a finite solution we will assume throughout the paper that \(f\) is bounded from below.

**Assumption 1**.: _There exists \(f^{}\) such that \(f(x) f^{}\) for all \(x^{d}\)._

The stochastic gradient descent (SGD) algorithm  is one of the most common algorithms to solve this problem. In its most general form, it can be written as

\[x^{k+1}=x^{k}- g(x^{k}),\] (2)

where \(g(x^{k})\) is a stochastic estimator of \( f(x^{k})\) and \(>0\) is a positive scalar stepsize. A particular case of interest is the compressed gradient descent (CGD) algorithm , where the estimatoris taken as a compressed alternative of the initial gradient:

\[g(x^{k})=( f(x^{k})),\] (3)

and the compressor \(\) is chosen to be a "sparser" estimator that aims to reduce the communication overhead in distributed or federated settings. This is crucial, as highlighted in the seminal paper by , which showed that the bottleneck of distributed optimization algorithms is the communication complexity. In order to deal with the limited resources of current devices, there are various compression objectives that are practical to achieve. These include also compressing the model broadcasted from server to clients for local training, and reducing the computational burden of local training. These objectives are mostly complementary, but compressing gradients has the potential for the greatest practical impact due to slower upload speeds of client connections and the benefits of averaging . In this paper we will focus on this latter problem.

An important subclass of compressors are the sketches. Sketches are linear operators defined on \(^{d}\), i.e., \((y)=y\) for every \(y^{d}\), where \(\) is a random matrix. A standard example of such a compressor is the Rand-\(k\) compressor, which randomly chooses \(k\) entries of its argument and scales them with a scalar multiplier to make the estimator unbiased. Instead of communicating all \(d\) coordinates of the gradient, one communicates only a subset of size \(k\), thus reducing the number of communicated bits by a factor of \(d/k\). Formally, Rand-\(k\) is defined as follows: \(=_{j=1}^{k}e_{i_{j}}^{}e_{i_{j}}^{}\), where \(i_{j}\) are the selected coordinates of the input vector. We refer the reader to  for an overview on compressions.

Besides the assumption that function \(f\) is bounded from below, we also assume that it is \(\) matrix smooth, as we are trying to take advantage of the entire information contained in the smoothness matrix \(\) and the stepsize matrix \(\).

**Assumption 2** (Matrix smoothness).: _There exists \(_{+}^{d}\) such that_

\[f(x) f(y)+ f(y),x-y+ (x-y),x-y\] (4)

_holds for all \(x,y^{d}\)._

The assumption of matrix smoothness, which is a generalization of scalar smoothness, has been shown to be a more powerful tool for improving supervised model training. In , the authors proposed using smoothness matrices and suggested a novel communication sparsification strategy to reduce communication complexity in distributed optimization. The technique was adapted to three distributed optimization algorithms in the convex setting, resulting in significant communication complexity savings and consistently outperforming the baselines. The results of this study demonstrate the efficacy of the matrix smoothness assumption in improving distributed optimization algorithms.

The case of block-diagonal smoothness matrices is particularly relevant in various applications, such as neural networks (NN). In this setting, each block corresponds to a layer of the network, and we characterize the smoothness with respect to nodes in the \(i\)-th layer by a corresponding matrix \(_{i}\). Unlike in the scalar setting, we favor the similarity of certain entries of the argument over the others. This is because the information carried by the layers becomes more complex, while the nodes in the same layers are similar. This phenomenon has been observed visually in various studies, such as those by  and .

Another motivation for using a layer-dependent stepsize has its roots in physics. In nature, the propagation speed of light in media of different densities varies due to frequency variations. Similarly, different layers in neural networks carry different information, metric systems, and scaling. Thus, the stepsizes need to be picked accordingly to achieve optimal convergence.

We study two matrix stepsized CGD-type algorithms and analyze their convergence properties for non-convex matrix-smooth functions. As mentioned earlier, we put special emphasis on the block-diagonal case. We design our sketches and stepsizes in a way that leverages this structure, and we show that in certain cases, we can achieve compression without losing in the overall communication complexity.

### Related work

Many successful convex optimization techniques have been adapted for use in the non-convex setting. Here is a non-exhaustive list: adaptivity , variance reduction ,LBZR21], and acceleration . A paper of particular importance for our work is that of , which proposes a unified scheme for analyzing stochastic gradient descent in the non-convex regime. A comprehensive overview of non-convex optimization can be found in . A classical example of a matrix stepsized method is Newton's method. This method has been popular in the optimization community for a long time . However, computing the stepsize as the inverse Hessian of the current iteration results in significant computational complexity. Instead, quasi-Newton methods use an easily computable estimator to replace the inverse of the Hessian . An example is the Newton-Star algorithm , which we discuss in Section 2.

 analyzed sketched gradient descent by making the compressors unbiased with a sketch-and-project trick. They provided an analysis of the resulting algorithm for the linear feasibility problem. Later,  proposed a variance-reduced version of this method. Leveraging the layer-wise structure of neural networks has been widely studied for optimizing the training loss function. For example,  propose SGD with different scalar stepsizes for each layer,  propose layer-wise normalization for Stochastic Normalized Gradient Descent, and  propose layer-wise compression in the distributed setting.

DCGD, proposed by , has since been improved in various ways, such as in . There is also a large body of literature on other federated learning algorithms with unbiased compressors .

### Contributions

Our paper contributes in the following ways:

* We propose two novel matrix stepsize sketch CGD algorithms in Section 2, which, to the best of our knowledge, are the first attempts to analyze a fixed matrix stepsize for non-convex optimization. We present a unified theorem in Section 3 that guarantees stationarity for minimizing matrix-smooth non-convex functions. The results shows that taking our algorithms improve on their scalar alternatives. The complexities are summarized in Table 1 for some particular cases.
* We design our algorithms' sketches and stepsize to take advantage of the layer-wise structure of neural networks, assuming that the smoothness matrix is block-diagonal. In Section 4, we prove that our algorithms achieve better convergence than classical methods.
* Assuming the that the server-to-client communication is less expensive , we propose distributed versions of our algorithms in Section 5, following the standard FL scheme, and prove weighted stationarity guarantees. Our theorem recovers the result for DCGD in the scalar case and improves it in general.
* We validate our theoretical results with experiments. The plots and framework are provided in the Appendix.

### Preliminaries

The usual Euclidean norm on \(^{d}\) is defined as \(\|\|\). We use bold capital letters to denote matrices. By \(_{d}\) we denote the \(d d\) identity matrix, and by \(_{d}\) we denote the \(d d\) zero matrix. Let \(^{d}_{++}\) (resp. \(^{d}_{+}\)) be the set of \(d d\) symmetric positive definite (resp. semi-definite) matrices. Given \(^{d}_{++}\) and \(x^{d}\), we write \(\|x\|_{}:=x,x},\) where \(,\) is the standard Euclidean inner product on \(^{d}\). For a matrix \(^{d}_{++}\), we define by \(_{}()\) (resp. \(_{}()\)) the largest (resp. smallest) eigenvalue of the matrix \(\). Let \(_{i}^{d_{i} d_{i}}\) and \(d=d_{1}++d_{}\). Then the matrix \(=(_{1},,_{})\) is defined as a block diagonal \(d d\) matrix where the \(i\)-th block is equal to \(_{i}\). We will use \(()^{d d}\) to denote the diagonal of any matrix \(^{d d}\). Given a function \(f:^{d}\), its gradient and its Hessian at point \(x^{d}\) are respectively denoted as \( f(x)\) and \(^{2}f(x)\).

The algorithms

Below we define our two main algorithms:

\[x^{k+1} =x^{k}-^{k} f(x^{k}),\] (det-CGD1)

and

\[x^{k+1} =x^{k}-^{k} f(x^{k}).\] (det-CGD2)

Here, \(^{d}_{++}\) is the fixed stepsize matrix. The sequences of random matrices \(^{k}\) and \(^{k}\) satisfy the next assumption.

**Assumption 3**.: _We will assume that the random sketches that appear in our algorithms are i.i.d., unbiased, symmetric and positive semi-definite for each algorithm. That is_

\[^{k},^{k} ^{d}_{+},^{k}^{k}\] \[[^{k}] =[^{k}]=_{d}, k .\]

A simple instance of det-CGD1 and det-CGD2 is the vanilla GD. Indeed, if \(^{k}=^{k}=_{d}\) and \(=_{d}\), then \(x^{k+1}=x^{k}- f(x^{k})\). In general, one may view these algorithms as Newton-type methods. In particular, our setting includes the Newton Star (NS) algorithm by :

\[x^{k+1}=x^{k}-(^{2}f(x^{}))^{-1} f(x^{k}).\] (NS)

The authors prove that in the convex case it converges to the unique solution \(x^{}\) locally quadratically, provided certain assumptions are met. However, it is not a practical method as it requires knowledge of the Hessian at the optimal point. This method, nevertheless, hints that constant matrix stepsize can yield fast convergence guarantees. Our results allow us to choose the \(\) depending on the smoothness matrix \(\). The latter can be seen as a uniform upper bound on the Hessian.

The difference between det-CGD1 and det-CGD2 is the update rule. In particular, the order of the sketch and the stepsize is interchanged. When the sketch \(\) and the stepsize \(\) are commutative w.r.t. matrix product, the algorithms become equivalent. In general, a simple calculation shows that if we take

\[^{k}=^{k}^{-1},\] (5)

then det-CGD1 and det-CGD2 are the same. Defining \(^{k}\) according to (5), we recover the unbiasedness condition:

\[[^{k}]=[^{k}] ^{-1}=_{d}.\] (6)

However, in general \([^{k}]^{-1}\) is not necessarily symmetric, which contradicts to Assumption 3. Thus, det-CGD1 and det-CGD2 are not equivalent for our purposes.

## 3 Main results

Before we state the main result, we present a stepsize condition for det-CGD1 and det-CGD2, respectively:

\[[^{k}^{k}],\] (7)

and

\[[^{k}^{k}].\] (8)

In the case of vanilla GD (7) and (8) become \(<L^{-1}\), which is the standard condition for convergence.

Below is the main convergence theorem for both algorithms in the single-node regime.

**Theorem 1**.: _Suppose that Assumptions 1-3 are satisfied. Then, for each \(k 0\)_

\[_{k=0}^{K-1}[\| f(x^{k})\|_{ }^{2}])-f^{})}{K},\] (9)

_if one of the below conditions is true:_1. _The vectors_ \(x^{k}\) _are the iterates of_ \(\) _and_ \(\) _satisfies (_7_);_
2. _The vectors_ \(x^{k}\) _are the iterates of_ \(\) _and_ \(\) _satisfies (_8_)._

It is important to note that Theorem 1 yields the same convergence rate for any \(_{++}^{d}\), despite the fact that the matrix norms on the left-hand side cannot be compared for different weight matrices. To ensure comparability of the right-hand side of (9), it is necessary to normalize the weight matrix \(\) that is used to measure the gradient norm. We propose using determinant normalization, which involves dividing both sides of (9) by \(()^{1/d}\), yielding the following:

\[_{k=0}^{K-1}[\| f(x^{k})\|_{ }{()^{1/d}}}^{2}])-f^{})}{ ()^{1/d}K}.\] (10)

This normalization is meaningful because adjusting the weight matrix to \(}{()^{1/d}}\) allows its determinant to be 1, making the norm on the left-hand side comparable to the standard Euclidean norm. It is important to note that the volume of the normalized ellipsoid \(\{x^{d}\ :\ \|x\|_{/()^{1/d}}^{2}  1\}\) does not depend on the choice of \(_{++}^{d}\). Therefore, the results of (9) are comparable across different \(\) in the sense that the right-hand side of (9) measures the volume of the ellipsoid containing the gradient.

### Optimal matrix stepsize

In this section, we describe how to choose the optimal stepsize that minimizes the iteration complexity. The problem is easier for \(\). We notice that (8) can be explicitly solved. Specifically, it is equivalent to

\[([^{k}^{k}])^{ -1}.\] (11)

We want to emphasize that the RHS matrix is invertible despite the sketches not being so. Indeed. The map \(h:\) is convex on \(_{+}^{d}\). Therefore, Jensen's inequality implies

\[[^{k}^{k}][ {T}^{k}][^{k}]=_{d}.\]

This explicit condition on \(\) can assist in determining the optimal stepsize. Since both \(\) and \((^{k}^{k})^{-1}\) are positive definite, then the right-hand side of (10) is minimized exactly when

\[=(^{k}^{k})^{-1}.\] (12)

The situation is different for \(\). According to (10), the optimal \(\) is defined as the solution of the following constrained optimization problem:

\[&(^{ -1})\\ &[^{k} ^{k}]\\ &_{++}^{d}.\] (13)

**Proposition 1**.: _The optimization problem (13) with respect to stepsize matrix \(_{++}^{d}\), is a convex optimization problem with convex constraint._

The proof of this proposition can be found in the Appendix. It is based on the reformulation of the constraint to its equivalent quadratic form inequality. Using the trace trick, we can prove that for every vector chosen in the quadratic form, it is convex. Since the intersection of convex sets is convex, we conclude the proof.

One could consider using the CVXPY package to solve (13), provided that it is first transformed into a Disciplined Convex Programming (DCP) form . Nevertheless, (7) is not recognized as a DCP constraint in the general case. To make CVXPY applicable, additional steps tailored to the problem at hand must be taken.

## 4 Leveraging the layer-wise structure

In this section we focus on the block-diagonal case of \(\) for both \(\)-CGD1 and \(\)-CGD2. In particular, we propose hyper-parameters of \(\)-CGD1 designed specifically for training NNs. Let us assume that \(=(_{1},,_{})\), where \(_{i}_{++}^{d_{i}}\). This setting is a generalization of the classical smoothness condition, as in the latter case \(_{i}=L_{d_{i}}\) for all \(i=1,,\). Respectively, we choose both the sketches and the stepsize to be block diagonal: \(=(_{1},,_{})\) and \(^{k}=(_{1}^{k},,_{}^{k})\), where \(_{i},_{i}^{k}_{++}^{d_{i}}\).

Let us notice that the left hand side of the inequality constraint in (13) has quadratic dependence on \(\), while the right hand side is linear. Thus, for every matrix \(_{++}^{d}\), there exists \(>0\) such that

\[^{2}_{}([^{k}^{k}])_{}().\]

Therefore, for \(\) we deduce

\[[^{k}()()^{k}] ^{2}_{}([^{k}^{k}])_{d}_{}()_{d} .\] (14)

The following theorem is based on this simple fact applied to the corresponding blocks of the matrices \(,,^{k}\) for \(\)-CGD1.

**Theorem 2**.: _Let \(f:^{d}\) satisfy Assumptions 1 and 2, with \(\) admitting the layer-separable structure \(=(_{1},,_{})\), where \(_{1},,_{}_{++}^{d_{i}}\). Choose random matrices \(_{1}^{k},,_{}^{k}_{+}^{d}\) to satisfy Assumption 3 for all \(i[]\), and let \(^{k}:=(_{1}^{k},,_{}^{k})\). Furthermore, choose matrices \(_{1},,_{}_{++}^{d}\) and scalars \(_{1},,_{}>0\) such that_

\[_{i}_{}^{-1}([_{i}^{-1/2}_{i}^{k}_{i}_{i}_{i}_{i}^{k}_{i}^{-1/2}] ) i[].\] (15)

_Letting \(:=(_{1},,_{})\), \(:=(_{1}_{d_{1}},,_{}_{d_ {}})\) and \(:=\), we get_

\[_{k=0}^{K-1}[\| f(x^{k})\|_{ }{()^{1/d}}}^{2}] )-f^{})}{()^{1/d}K}.\] (16)

   No. & The method & \((_{i}^{k},_{i})\) & \(l 1,d_{i},k_{i},_{i=1}^{d}k_{i}=k\), layer structure & \(l=1,k_{i}=k\), general structure \\ 
1. & \(\)-CGD1 & \((_{d},_{i}^{-1}(_{i}))\) & \(d(\}\) are chosen to be equal to their maximum allowed values from (15), then the convergence factor of (16) is equal to

\[()^{-}=[_{i=1}^{d}_{ }^{d_{i}}([_{i}^{-}_{i}^{k}_{i }_{i}_{i}_{i}^{k}_{i}^{-}]) ]^{}(^{-1})^{}.\]

Table 1 contains the (expected) communication complexities of \(\), \(\) and GD for several choices of \(,\) and \(^{k}\). Here are a few comments about the table. We deduce that taking a matrix stepsize without compression (row 1) we improve GD (row 13). A careful analysis reveals that the result in row 5 is always worse than row 7 in terms of both communication and iteration complexity. However, the results in row 6 and row 7 are not comparable in general, meaning that neither of them is universally better. More discussion on this table can be found in the Appendix.

Compression for free.Now, let us focus on row 12, which corresponds to a sampling scheme where the \(i\)-th layer is independently selected with probability \(q_{i}\). Mathematically, it goes as follows:

\[_{i}^{k}=}{q_{i}}_{d_{i}},_{i}(q_{i}).\] (17)

Jensen's inequality implies that

\[(_{i=1}^{l}q_{i}d_{i})_{i=1}^{l}(} )^{}{d}} d.\] (18)

The equality is attained when \(q_{i}=q\) for all \(i[]\). The expected bits transferred per iteration of this algorithm is then equal to \(k_{}=qd\) and the communication complexity equals \(d()^{1/d}\). Comparing with the results for \(\) with \(k_{}\) on row 11 and using the fact that \(()(())\), we deduce that the Bernoulli scheme is better than the uniform sampling scheme. Notice also, the communication complexity matches the one for the uncompressed \(\) displayed on row 9. This, in particular means that using the Bern-\(q\) sketches we can compress the gradients for free. The latter means that we reduce the number of bits broadcasted at each iteration without losing in the total communication complexity. In particular, when all the layers have the same width \(d_{i}\), the number of broadcasted bits for each iteration is reduced by a factor of \(q\).

## 5 Distributed setting

In this section we describe the distributed versions of our algorithms and present convergence guarantees for them. Let us consider an objective function that is sum decomposable:

\[f(x):=_{i=1}^{n}f_{i}(x),\]

where each \(f_{i}:^{d}\) is a differentiable function. We assume that \(f\) satisfies Assumption 1 and the component functions satisfy the below condition.

**Assumption 4**.: _Each component function \(f_{i}\) is \(_{i}\)-smooth and is bounded from below: \(f_{i}(x) f_{i}^{}\) for all \(x^{d}\)._

This assumption also implies that \(f\) is of matrix smoothness with \(}_{++}^{d}\), where \(}=_{i=1}^{n}_{i}\). Following the standard FL framework , we assume that the \(i\)-th component function \(f_{i}\) is stored on the \(i\)-th client. At each iteration, the clients in parallel compute and compress the local gradient \( f_{i}\) and communicate it to the central server. The server, then aggregates the compressed gradients, computes the next iterate, and in parallel broadcasts it to the clients. See the algorithms below for the pseudo-codes.

**Theorem 3**.: _Let \(f_{i}:^{d}\) satisfy Assumption 4 and let \(f\) satisfy Assumption 1 and Assumption 2 with smoothness matrix \(\). If the stepsize satisfies_

\[,\] (19)

_then the following convergence bound is true for the iteration of Algorithm 1:_

\[_{0 k K-1}[\| f(x^{k})\|_{}{()^{1/d}}}^{2}]}}{n })^{K}(f(x^{0})-f^{})}{()^{1/d}\,K}+}^{}}{()^{1/d}\,n},\] (20)

_where \(^{}:=f^{}-_{i=1}^{n}f_{i}^{}\) and_

\[_{}:=_{i}\{_{}([_{ i}^{}(_{i}^{k}-_{d})( _{i}^{k}-_{d})_{i}^{}])\}.\]

The same result is true for Algorithm 2 with a different constant \(_{}\). The proof of Theorem 3 and its analogue for Algorithm 2 are presented in the Appendix. The analysis is largely inspired by [13, Theorem 1]. Now, let us examine the right-hand side of (20). We start by observing that the first term has exponential dependence in \(K\). However, the term inside the brackets, \(1+_{}/n\), depends on the stepsize \(\). Furthermore, it has a second-order dependence on \(\), implying that \(_{}=^{2}_{}\), as opposed to \(()^{1/d}\), which is linear in \(\). Therefore, we can choose a small enough coefficient \(\) to ensure that \(_{}\) is of order \(n/K\). This means that for a fixed number of iterations \(K\), we choose the matrix stepsize to be "small enough" to guarantee that the denominator of the first term is bounded. The following corollary summarizes these arguments, and its proof can be found in the Appendix.

**Corollary 1**.: _We reach an error level of \(^{2}\) in (20) if the following conditions are satisfied:_

\[,_{}\{,}{4^{}}()^{1/d}\}, K )-f^{})}{()^{1/d}\,^{2}}.\] (21)

Proposition 2 in the Appendix proves that these conditions with respect to \(\) are convex. In order to minimize the iteration complexity for getting \(^{2}\) error, one needs to solve the following optimization problem

\[&(^{-1})\\ &.\]

Choosing the optimal stepsize for Algorithm 1 is analogous to solving (13). One can formulate the distributed counterpart of Theorem 2 and attempt to solve it for different sketches. Furthermore, this leads to a convex matrix minimization problem involving \(\). We provide a formal proof of this property in the Appendix. Similar to the single-node case, computational methods can be employed using the CVXPY package. However, some additional effort is required to transform (21) into the disciplined convex programming (DCP) format.

The second term in (20) corresponds to the convergence neighborhood of the algorithm. It does not depend on the number of iteration, thus it remains unchanged, after we choose the stepsize.

Nevertheless, it depends on the number of clients \(n\). In general, the term \(^{}/n\) can be unbounded, when \(n+\). However, per Corollary 1, we require \(_{}\) to be upper-bounded by \(n/K\). Thus, the neighborhood term will indeed converge to zero when \(K+\), if we choose the stepsize accordingly.

We compare our results with the existing results for DCGD. In particular we use the technique from  for the scalar smooth DCGD with scalar stepsizes. This means that the parameters of algorithms are \(_{i}=L_{i}_{d},=L_{d},=_{d},= _{}([(_{i}^{k})^{}_{ i}^{k}])-1\). One may check that (21) reduces to

\[\{,L}},}{4^{}L_{}L}\} K)-f^{})}{^{2}}\] (22)

As expected, this coincides with the results from [10, Corollary 1]. See the Appendix for the details on the analysis of . Finally, we back up our theoretical findings with experiments. See Figure 1 for a simple experiment confirming that Algorithms 1 and 2 have better iteration and communication complexity compared to scalar stepsized DCGD. For more details on the experiments we refer the reader to the corresponding section in the Appendix.

## 6 Conclusion

### Limitations

It is worth noting that every point in \(^{d}\) can be enclosed within some volume 1 ellipsoid. To see this, let \(0 v^{d}\) and define \(:=}vv^{}+_{i=1} ^{d}v_{i}v_{i}^{}\), where \(v_{1}=,v_{2},,v_{d}\) form an orthonormal basis. The eigenvalues of \(\) are \(\) (with multiplicity \(d-1\)) and \(\) (with multiplicity \(1\)), so we have \(()=^{d-1} 1\). Furthermore, we have \( v_{}^{2}=v^{}v= v ^{2}\). By choosing \(=}\) and \(= v^{2/(d-1)}\), we can obtain \(()=1\) while \( v_{}^{2} 1\). Therefore, having the average \(\)-norm of the gradient bounded by a small number does not guarantee that the average Euclidean norm is small. This implies that the theory does not guarantee stationarity in the Euclidean sense.

### Future work

Matrix stepsize gradient methods are still not well studied and require further analysis. Although many important algorithms have been proposed using scalar stepsizes and are known to have good performance, their matrix analogs have yet to be thoroughly examined. The distributed algorithms proposed in Section 5 follow the structure of DCGD by . However, other federated learning mechanisms such as MARINA, which has variance reduction , or EF21 by , which has powerful practical performance, should also be explored.

Figure 1: Comparison of standard DCGD, DCGD with matrix smoothness, D-det-CGD1 and D-det-CGD2 with optimal diagonal stepsizes under rand-\(1\) sketch. The stepsize for standard DCGD is determined using [10, Proposition 4], the stepsize for DCGD with matrix smoothness along with \(_{1}\), \(_{2}\) is determined using Corollary 1, the error level is set to be \(^{2}=0.0001\). Here \(G_{K,}:=_{k=0}^{K-1} f(x^{k}) _{/()^{1/d}}^{2}\).