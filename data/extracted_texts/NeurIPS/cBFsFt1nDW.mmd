# Transformers Can Do Arithmetic with the Right Embeddings

Sean McLeish\({}^{1}\)\({}^{*}\), Arpit Bansal\({}^{1}\)\({}^{*}\), Alex Stein\({}^{1}\), Neel Jain\({}^{1}\), John Kirchenbauer\({}^{1}\),

**Brian R. Bartoldson\({}^{2}\), Bhavya Kailkhura\({}^{2}\), Abhinav Bhatele\({}^{1}\), Jonas Geiping\({}^{3}\),**

**Avi Schwarzschild\({}^{4}\), Tom Goldstein\({}^{1}\)**

\({}^{1}\) University of Maryland, \({}^{2}\) Lawrence Livermore National Laboratory, \({}^{3}\) ELLIS Institute Tubingen,

Max Planck Institute for Intelligent Systems, Tubingen AI Center, \({}^{4}\) Carnegie Mellon University

Equal Contribution, correspondence to: smcleish@umd.edu, bansal01@umd.edu.

###### Abstract

The poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits. We mend this problem by adding an embedding to each digit that encodes its position relative to the start of the number. In addition to the boost these embeddings provide on their own, we show that this fix enables architectural modifications such as input injection to improve performance even further.

With positions resolved, we can study the logical extrapolation ability of transformers. Can they solve arithmetic problems that are larger and more complex than those in their training data? We find that training on only \(20\) digit numbers with a single GPU for one day, we can reach state-of-the-art performance, achieving up to \(99\%\) accuracy on \(100\) digit addition problems. Finally, we show that these gains in numeracy also unlock improvements on other multi-step reasoning tasks including sorting and multiplication.

## 1 Introduction

Much of the recent work on Large Language Models (LLMs) focuses on their ability to solve problems in natural language and code generation. Despite progress in these domains, transformers still struggle to perform complex multi-step and algorithmic reasoning tasks in a zero shot setting without resorting to tool use. To study algorithmic reasoning in a sterile laboratory setting, the academic community focuses on simple arithmetic test problems like addition. Addition is simple enough that modest-sized LLMs can (in principle) be trained from scratch to do it without running into capacity and training budget limitations, yet complex enough that even large industrial models fail on large numbers without a code interpreter (Loeber, 2024).

Prior studies indicate that addition is hard for transformers (Lee et al., 2023; Shen et al., 2023; Zhou et al., 2023, 2024). Our experiments indicate that this difficulty stems from their inability to clearly represent the exact position of a digit within a long sequence of digits. To address this problem, we propose a simple modification to the data representation that directly addresses this shortcoming. Our _Abacus Embeddings_ are simple learned positional embeddings that are used to encode positions within each span of numerical tokens. Combining Abacus Embeddings and standard positional embeddings, we observe dramatic improvements in accuracy such that models trained with at most 20 digit operands can generalize to problems with 120 digit operands. This represents a state-of-the-art generalization factor of \(6\), with the previous state of the art being only \(2.5\). To the best of our knowledge, these are the longest sequences on which learned addition has ever been demonstrated.

We also study several other methods of improving arithmetic and generalization in transformers. We find that incorporating _input injection_--skip connections inserted between the input layer and each decoder layer--can reduce generalization errors by 50% over the Abacus Embedding baseline. We also find that together with our embeddings looped transformer architectures, which contain recurrent layers in which the same parameters are re-used multiple times, can achieve near-perfect generalization on addition problems we consider. These results are shown in Appendix Section A.4

Since our proposed methods solve large addition problems successfully, we evaluate whether the same approaches can be used to improve other kinds of algorithmic learning. In Appendix Section A.3, we explore multiplication problems of up to 15 digit numbers and sorting over arrays of up to 10 numbers, making this the first study of extreme length generalization techniques for addition that transfer to other algorithmic tasks. Our contributions can be summarized as follows.

* We propose a new positional embedding called _Abacus Embeddings_ to better capture the significance of each digit, which leads to near-perfect in-distribution generalization.
* We show that when we combine Abacus Embeddings with input injection and looped transformers performance further improves, increasing from \(92.9\%\) to \(99.1\%\) in out-of-distribution accuracy, an \(87\%\) reduction in error compared to using the embeddings with standard architectures alone.

## 2 Related Work

Arithmetic.Solving arithmetic with next token prediction is a difficult problem that attracts a lot of attention (e.g. Saxton et al., 2019). However, in zero-shot settings, even incredibly strong commercial API models struggle with very large addition problems (e.g. up to 100 digits) without access to tools. Among attempts to improve arithmetic performance of transformer-based models, reversing the digits so the arguments are written with the least significant digit first is popular (Lee et al., 2023; Shen et al., 2023; Zhou et al., 2023, 2024). Furthermore, changing the data format by adding explicit index characters improves model capability for addition (Zhou et al., 2023, 2024; Olsson et al., 2022).

Weight Sharing.Weight sharing and recurrence can be used to make models adaptive and help generalize to harder problems (Dehghani et al., 2018; Sukhbaatar et al., 2019; Lan et al., 2020; Ibarz et al., 2022). Schwarzschild et al. (2021) and Bansal et al. (2022) explore an end-to-end learning approach using recurrent convolutional neural networks to learn algorithms from input-output pairs, tackling algorithmic tasks like prefix sums, mazes, and chess. Weight sharing for algorithmic reasoning is also helpful with transformers and we use the _looped transformer_ in some of our experiments below. A looped transformer has a transformer block called recurrently on its own output lending itself to executing iterative algorithms (Giannou et al., 2023; Yang et al., 2023; de Luca & Fountoulakis, 2024).

Positional Embeddings.Indicating the position of tokens in a sequence to transformer models is critical for language modeling (Vaswani et al., 2017). Absolute positional embeddings (APE) are learned embeddings that are added to token embeddings before the first layer of the transformer (Vaswani et al., 2017). However, these absolute embeddings inhibit length generalization (Press et al., 2022). Kazemnejad et al. (2023) show that decoder layers can still learn positional information with no explicit positional embeddings. No positional embeddings (NoPE) can achieve good length generalization performance for small algorithmic tasks and even outperform some specialized embeddings. The latest and most useful for arithmetic is Functional Interpolation for Relative Position Embeddings (FIRE) (Li et al., 2023). FIRE shows the strongest length generalization to date, which leads to length generalization by \(2.5\) on addition (Zhou et al., 2024) when combined with randomized embeddings (Ruoss et al., 2023). We go into more detail on positional embeddings in Appendix A.1.1. In this work, we focus on NoPE and FIRE embeddings since these are the best performers for addition in reversed format among existing embeddings (Zhou et al., 2024).

## 3 Achieving Length Generalization for Addition

We focus on two main hypotheses: (1) the positional information for individual digits within numbers is being lost and (2) recurrence can improve the reasoning abilities of transformer architectures on multi-step arithmetic reasoning problems. We briefly discuss the training and evaluation setup before describing each of our improvements in detail.

Experimental Setup.We train decoder-only causal language models to solve addition problems. Following prior work (Zhou et al., 2023, 2024; Shen et al., 2023; Kazemnejad et al., 2023; Lee et al., 2023), inputs are formatted least significant digit first, e.g. \(98282+3859172=2787472\). Unlike prior work, we do not add any padding between digits (Shen et al., 2023) and do not pad any numbers with zeros, neither in the case of carry digits (Zhou et al., 2024), nor to make all operands the same length (Shen et al., 2023). We train on all combinations of operand lengths less than or equal to \(i\) and \(j\) where \(i\) and \(j\) are the maximum lengths of the first and second operands, respectively. For this study all training sets have \(20\) million samples and \(i=j\), hence we can use one number to define the dataset \(i\), where \(i\) is the maximum length of either operand. For further details on data construction and training we refer to Appendix A.6.

We report model accuracy for each \((i,j)\) length pair and unlike most existing work, we also include accuracy for pairs where \(i j\) to highlight all instances of extrapolation. This extensive tabulation is costly and makes inference the main computational burden of this study. We measure accuracy in the strict sense where only exact matches of all output digits are counted as correct, i.e. if a single digit is incorrect then the example is marked as wrong and we refer to this as _exact match accuracy_. We have the following three evaluation categories: (i) in distribution (ID) where the models are tested on problems up to the maximum size seen during training; (ii) out of distribution (OOD) where the models are tested on problems greater than the maximum size seen during training but both operands are at most \(100\) digits; (iii) and extreme out of distribution (\(100+\) digit OOD) where the models are tested on problems where both operands are of the same length and are both more than \(100\) digits and less than \(160\) digits. In the \(100+\) OOD setting, we only analyze problems where the operands are the same length (\(i=j\)) due to inference costs at this scale.

We consider two standard transformer architectures. First, we use a standard autoregressive transformer model (ST) where multiple decoder layers are stacked in a feedforward manner. Second, we enhance this standard transformer model by incorporating _input injection_ (ST w/ II), where the embedded inputs are added to the input of each decoder layer (Ma et al., 2022; Bansal et al., 2022; Anil et al., 2022a). We visually describe the architectures in the Appendix Figure 19.

Figure 1: Visualization of data formats and positional embeddings. _Abacus Embeddings_ give the same positional embeddings to all digits of the same significance.

Figure 2: Mean exact match accuracy of three models of depth sixteen on size \(20\) data, varying the architecture and embeddings. Abacus Embeddings improve accuracy for addition over FIRE and NoPE.

### Abacus Embeddings Help Align Digits

From prior work and our own initial experiments, we observe that even when input numbers are presented least-significant digit first and training data is stratified and abundant (several million examples), standard transformers struggle to learn multi-digit addition. We also observe that humans do long addition by first aligning the digits of the same significance into columns. Thus, our first hypothesis is that the significance of each digit (i.e. each digit's position relative to the beginning of the number) is not easy for transformers to represent, and that this sub-problem presents more of a hurdle than the actual addition itself.

Prior work addresses this by proposing explicit index hints in the inputs and outputs of the addition, for example \(a6b7c5+a1b6c3=a7b3c9\); finding that transformers perform much better on addition with the information provided by such hints (Zhou et al., 2023, 2024). However, index hints of this form increase the input context length required and _double_ the output length and inference cost of solving a given addition problem. Furthermore, Zhou et al. (2024) find that the ability of models trained with index hints to generalize is sensitive to the particular random initialization.

To address the limitations of transformers at representing positional information, we design a specially built positional embedding that encodes the location of each digit relative to the start of the current number. We call this _Abacus Embeddings_. We apply the same positional embedding to all digits of the same significance, providing an explicit signal that the model can use to align digits. We visually describe these embeddings in Figure 1.2

We take inspiration from _Randomized Embeddings_(Ruoss et al., 2023) but instead of using random ascending indices to represent positions in a sample, we use consecutive ascending indices with a random starting position to allow for length generalization. Specifically, during training we give consecutive positional embeddings to each digit in a number, starting from a randomly chosen offset value from \(U[1,k]\), where \(k\) is a hyperparameter. Unless otherwise stated the default value for \(k\) in this study is \(100\). For example, if the input is \(123\), the positional encodings are \(,+1,+2\) where \( U\), which are then passed through a learned embedding matrix. The value sampled from \(U[1,k]\) is the same for all numbers in a batch, meaning all digits of the same significance obtain the same positional embedding. This training scheme allows the model to see a wide range of positional embeddings, even when training sequences are short. At test time, we set \(=1\).

Abacus Embeddings Solve Addition.Abacus Embeddings improve generalization performance up to \(100\) digits and beyond for standard transformer architectures. In Figure 2, we highlight the comparative boost Abacus Embeddings have over standard transformer architectures and embeddings for performing addition, taking the mean accuracy of three models in all cases. Additionally, In Appendix A.5.4, we present \(2\)D grid plots for several other experiments that are depicted as bar charts in the main text. Zhou et al. (2024) find that operand lengths of up to forty digits are required during training for good generalization to \(100\) digit addition during testing (albeit not robustly). We find that with our Abacus Embeddings, we can achieve similar accuracy and larger extrapolation using a standard model with input injection trained on maximum operand sizes of \(20\) digits.

As Abacus Embeddings are a variant of absolute positional embeddings, technically they cannot generalize beyond the relative positions seen during training. However the hyperparameter \(k\) that randomizes the starting offset used for each individual addition example can be increased to enable generalization by training a larger range of embeddings for a given computational budget. Relatedly, Appendix Figure 8 shows that training on larger datasets improves performance, even for operands with fewer than \(100\) digits.

## 4 Discussion

Across our experiments, we find that our novel Abacus Embeddings improve performance dramatically both when applied to standard transformers as well as recurrent variants. We hope that our work deepens the community's understanding of these problems and paves the way for further advancements in the algorithmic reasoning capabilities of large language models.