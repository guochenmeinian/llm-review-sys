# An active learning framework for multi-group mean estimation

Abdellah Aznag Rachel Cummings Adam N. Elmachtoub

Department of Industrial Engineering and Operational Research

Columbia University

{aa4683, rac2239, ae2516} @columbia.edu

###### Abstract

We consider a problem with multiple groups whose data distributions are unknown, and an analyst would like to learn the mean of each group. We consider an active learning framework to sequentially collect \(T\) samples with bandit feedback, each period observing a sample from a chosen group. After observing a sample, the analyst may update their estimate of the mean and variance of that group and choose the next group accordingly. The analyst's objective is to dynamically collect samples to minimize the \(p\)-norm of the vector of variances of the mean estimators after \(T\) rounds. We propose an algorithm, Variance-UCB, that selects groups according to a an upper bound on the variance estimate adjusted to the \(p\)-norm chosen. We show that the regret of Variance-UCB is \((T^{-2})\) for finite \(p\), and prove that no algorithm can do better. When \(p\) is infinite, we recover the \((T^{-1.5})\) regret bound obtained in  with improved dependence on the remaining parameters and provide a new lower bound showing that no algorithm can do better.

## 1 Introduction

Obtaining accurate estimates from limited labeled data is a fundamental challenge. To tackle this issue, active learning has emerged as a promising solution framework where a decision-maker strategically selects one sample at a time . The estimation task becomes more complex when faced with a large population of different groups, where it is important that all groups are represented in the estimation procedure. If this allocation is not done correctly, the decision might incur structural bias against some groups . If allowed to adjust the sampling allocation strategy, the analyst can address their biases and re-allocate their sampling resources. A key challenge is to dynamically collect data from different groups while maintaining a reasonable representation of all the groups, which is the problem we tackle in this paper. We provide an active learning framework for dynamic data collection with bandit feedback for estimating the means of multiple groups in a representative manner.

We consider a population partitioned into multiple groups, each with data points drawn from an unknown data distribution, and an analyst would like to learn the mean of each group. Each group's distribution has an unknown mean and unknown variance. At each time period, the analyst collects one sample from a group of their choice and observes a sample from that group. Since only one group is observed, this is also known as bandit feedback. Upon observation, the analyst exploits their new knowledge to optimize their choice of the next group to observe. At the end of the time horizon \(T\), the analyst forms a mean estimate for each group. The objective of the analyst is to return a vector of mean estimates with smallest \(p\)-norm of the variance vector of the mean estimates. The choice of \(p\)-norm is motivated by its ability to capture different aspects of multi-group estimation performance .

By considering various values of \(p\), we gain insights into different dimensions of estimation accuracy, ranging from the overall spread of the estimates (\(p=2\), Euclidean norm) to the worst case deviation from the true mean (\(p=+\), infinity norm), a case that is studied in [4; 13]. This approach allows us to assess the estimation quality from multiple perspectives, providing a more nuanced understanding of how the diverse population is represented in the final estimation of the group means. The analyst can choose \(p\) in a way to capture notions of fairness in the data collection process.

### Summary of contributions

In this work, we first present an active learning framework that captures the trade-off between representation and accuracy in the task of multi-group estimation. Our framework encapsulates a large class of norms (\(p\)-norms), which bring various insights into different dimensions of estimation accuracy. In Section 3, we present Variance-UCB, an algorithm that selects groups according to an upper bound on the variance estimate adjusted to the \(p\)-norm.

We provide a norm-dependent regret bound on Variance-UCB under the two assumptions sub-Gaussian feedback and positive variances. Specifically, we show that the regret is \((T^{-2})\) for finite \(p\) norms (Theorem 1). For the case of the infinite norm, we show that the regret is \((T^{-1.5})\) (Theorem 3). While bounds at this rate are already established in [4; 13], our bound provides tighter dependencies in the problem parameters \(G\) and \(_{}\). In the case of the infinite norm, we improve the coefficient of the \(T^{-1.5}\) term in the regret from \(_{}^{-2}G^{2.5}\) to \(G^{1.5}\). This improvement not only tightens the dependency in the number of groups, but it also partially solves one of the open questions left in  regarding the role of \(_{}\) in the regret bound. While we prove that \(_{}\) can still impact the regret, it appears only in lower order terms with respect to \(T\).

The analysis and proofs of these upper bounds is presented in Section 4. Our proof technique differs from proofs of similar upper bounds in the related literature. Instead of studying the regret function directly, we first consider its Taylor expansion, and focus our analysis on its dominant term. This technique has two advantages. First, the dominant term is much simpler to analyze (linear in the case of the infinite norm, and quadratic in the case of finite \(p\) norms) than the full regret function. Second, the resulting upper bound is tighter in its dominant term, in that it does not suffer from large numerical constants, as is common in the existing literature on regret analysis of bandit algorithms (e.g., [4; 13]).

We also provide new matching lower bounds in \(T\) for both finite (Theorem 2) and infinite \(p\)-norms (Theorem 4), establishing that Variance-UCB achieves the best possible regret in both regimes. These bounds are summarized in Table 1. Prior to our work and to the best of our knowledge, no lower bounds were known for this problem. The analysis of these results is presented in Section 5.

We empirically validate our findings by numerical experiments presented in Section 6, which show that our theoretical regret bounds match empirical convergence rates in both cases of finite and infinite \(p\)-norms. We also provide examples showing that for finite \(p\)-norms, the smallest variance affects the regret, even when the feedback is Gaussian. This is in contrast to the case of the infinite norm, where it is proven  that under Gaussian feedback, the algorithm is not affected by the smallest variance.

### Related work

Our motivation stems from growing attention to data collection methods [34; 2; 18; 20]. We focus on the problem of mean estimation and dynamically collecting data to achieve this goal. While there is a substantial body of literature on data acquisition [9; 26], specifically in the presence of privacy concerns and associated costs [19; 30; 31; 12; 17; 16], our approach differs as we do not consider the costs of sharing data. Instead, we concentrate on the data collection process itself, rather than the costs to data providers.

**Norm** & **Variance-UCB Regret** & **Lower Bound on Regret for Any Policy** \\  \(p<\) & \((T^{-2})\) (Theorem 1) & \((T^{-2})\) (Theorem 2) \\  \(p=\) & \((T^{-1.5})\) (Theorem 3) & \((T^{-1.5})\) (Theorem 4) \\ 

Table 1: Summary of main resultsOur work is related to multi-armed bandits problems [5; 37], in the sense that each group can be seen as an arm, and choosing a group to sample from at each time step corresponds to choosing which arm to pull. However, the performance criterion for multi-armed bandits is measured by the difference between the mean of the chosen arm and the best arm [3; 14; 38]. In our framework, the means of the chosen arms do not impact the performance. It is their variances that matter in the optimal solution, as we measure the performance by considering the \(p\)-norm of the variance of the estimator, which can be non-convex. Because of this, and to the best of our knowledge, usual bandits algorithms and proof techniques  do not apply. Instead, we propose Variance-UCB, an algorithm that uses high probability upper bounds on the variance estimates adjusted to the chosen norm.

Considering data acquisition from the perspective of active learning is a natural approach [6; 7; 27], and in that sense our work is also related to active learning [15; 22; 24; 40]. In , the authors address the optimal data acquisition problem under the assumption of additive objective functions. They formulate the problem as an online learning problem and leverage well-understood tools from online convex optimization [8; 11; 36]. However, their ideas do not apply to our setting due to the non-additive nature of our decisions over time.

The case where the chosen norm is \(\|\|_{}\) is introduced in , where the authors devise the _GAFS-MAX_ algorithm and show that for bounded feedback with known upper bounds, it achieves regret \((T^{-1.5})\). This case is also studied in , where the authors extend the feedback to sub-Gaussian, and devise the _B-AS_ algorithm, which also has \((T^{-1.5})\) regret. One property that emerges in their study is a regret bound deteriorates with \(_{}^{-1}\), where \(_{}^{2}\) is the smallest variance across groups. This is counter-intuitive, as smaller variances in general make the learning simpler. The authors pose as an open question whether any algorithm can have performance independent of \(_{}\) and show that in the special case of exactly Gaussian feedback, one can derive a \(_{}\) free bound. We partially answer this question, by showing that the leading term of the regret bound (with respect to \(T\)) does not depend on \(_{}\). In other words, while \(_{}\) can still impact the regret, its effect appears only in lower order terms. Our regret analysis of Variance-UCB for \(\|\|_{}\) recovers the same dependence in \(T\) as in , but also has improved dependence in the number of groups \(G\) and the variance vector \(\). While [4; 13] serve as a starting point, most of their findings are only applicable to the special case \(p=+\). In particular, they cannot be expanded to other choices of norm. We show that for finite \(p\) norms, we prove the regret bound is \((T^{-2})\), which is fundamentally different than the previous \((T^{-1.5})\) bound.

## 2 Model

We consider a population partitioned into \(G\) disjoint groups. Each group is represented by an index \(g\) from \([G]:=\{1,,G\}\). Each individual in the population holds a real-valued data point; data from each group \(g[G]\) are distributed according to an unknown distribution \(_{g}\), with unknown mean \(_{g}\) and unknown variance \(_{g}^{2}\). We denote \(_{}:=_{g[G]}_{g}\) and let \(}:=_{1}_{G}\). The analyst wishes to compute an unbiased estimate of the population mean for each group over \(T\) times of data collection, sampling only one group at a time. The set of feasible policies is defined as

\[:=\{=\{_{t}\}_{t[T]}\ |\ _{t} G^{t-1} ^{t-1}(G),\  t[T]\},\]

where \((G)\) is the set of measures supported on \([G]\). Let \(n_{g,T}\) denote the number of collected samples from group \(g\) at the end of time \(T\), and let \(_{g,T}\) be the sample mean estimator of \(_{g}\) for \(n_{g,T}\) collected samples. Once all data have been collected at the end of the time horizon \(T\), the analyst will compute the sample mean of each group, i.e.,

\[_{g,T}=^{T}_{X_{t}=g}}_{t=1}^{T} _{X_{t}=g}Y_{t}.\]

Note that the vector \(}_{T}\) is always an unbiased estimator of the vector \(\), as long as the policy \(\) samples at least once from each group.

The variance of \(_{g,T}\) is \(^{2}}{n_{g,T}}\). The \(p-\)norm of the vector of variances of \(n_{g,T}\) is denoted as

\[R_{p}(_{T}):=\|\{^{2}}{n_{g,T}}\}_{g=1}^{ G}\|_{p}.\] (1)The analyst wishes to minimize \([R_{p}(_{T})]\).

When choosing a policy, the analyst does not have access to the true standard deviation vector \(:=(_{1},,_{G})\), which is needed to compute the value \(R_{p}(_{T})\). Therefore the analyst must learn \(\) through their decisions. We benchmark the performance of a policy against the best possible performance in a complete information setting where \(\) is known, that is,

\[_{^{G}}R_{p}() s.t._{g[G]}n_{g}=T.\] (2)

The optimization program (2) can be difficult to solve and analyze due to the integer constraints. Instead of using the solution to (2) as a benchmark, we use the solution to its continuous relaxation (a lower bound on (2)), which we denote as,

\[R_{p}^{*}:=_{_{}^{G}}R_{p}() s.t._{g[G]}n_{g}=T.\] (3)

Thus, we can define the regret of a policy as

\[_{p,T}(,}):=_{}[R_{p}( _{T})]-R_{p}^{*}.\] (4)

For our analysis, we assume that the distributions \(_{g}\) are sub-Gaussian, as stated in Assumption 1, with corresponding constants \(c_{1},c_{2}\) known to the analyst.

**Assumption 1** (Sub-Gaussianity).: _For each \(g[G]\), \(_{g}\) is sub-Gaussian. That is, there exist universal constants \(c_{1},c_{2}>0\) such that for any \(>0\),_

\[_{Y_{g}}(|Y-_{g}|) c_{ 1}(-^{2}/c_{2}).\]

_We assume that such \(c_{1},c_{2}\) are known to the analyst._

Moreover, we assume that all groups have some variation in their data, as stated in Assumption 2.

**Assumption 2** (Positive Variance).: _The minimum group variance is positive; i.e., \(_{}=_{g[G]}_{g}>0\)._

## 3 Our algorithm: Variance-UCB

Our algorithm, Variance-UCB, builds an increasingly accurate upper confidence bound for each \(_{g}\), which we denote \(_{t}(_{g})\). Recall that \(n_{g,t}\) denotes the number of collected samples from group \(g\) through time \(t\). At each time \(t\), \(_{g}\) can be estimated via the sample standard deviation

\[_{g,t}:=-1}_{s t:X_{s}=g}(Y_{s }-_{g,t})^{2}}.\]

For convention, we set \(_{g,0}=_{g,1}=+\), indicating that at least two samples are required to obtain a meaningful estimate of \(_{g}\). We can then define

\[_{t}(_{g}):=_{g,t}+}{}},\] (5)

where

\[C_{T}:=2)c_{1}(c_{2}T^{4})}+(2T^{4})(1+c_{2}+(c_{2}T^{4}))}}{(1-T^{ -4}))}}T^{-2}.\] (6)

\(C_{T}\) was introduced in , and captures the trade-off between accuracy of the upper confidence bound and confidence in the estimate, and is a polylogarithmic factor in \(T\). In particular, \(C_{T}\) can be constructed by the analyst since it depends only on \(c_{1},c_{2}\), and \(T\), which are known.

At time \(t+1\), the algorithm chooses the next group \(X_{t+1}\) using the rule:

\[X_{t+1}=_{g[G]}_{t}(_{g})^{}}{n_{g,t}},\]where where ties in the argmax can be broken arbitrarily. The analyst then observes a new sample \(Y_{t+1}\) from the chosen group \(X_{t+1}\) and updates the upper confidence bounds accordingly. Variance-UCB is presented formally in Algorithm 1.

```
0: norm parameter \(p\), time horizon \(T\), number of groups \(G\) and subgaussian parameters \(c_{1},c_{2}\).
1: Initialize \(n_{g,0}=0,_{g,0}=_{g,1}=+\), \( g[G]\)
2: Compute \(C_{T}\) according to (6).
3:for\(t=0,,T-1\)do
4: Compute \(_{t}(_{g})\): \(_{t}(_{g})=_{g,t}+}{}},  g[G]\)
5: Select group \(X_{t+1}=_{g}_{t}(_{g})^{}}{n_ {g,t}}\)
6: Observe feedback \(Y_{t+1}_{X_{t+1}}\)
7: Update the number of samples: \(n_{g,t+1}=n_{g,t}+_{X_{t+1}=g}, g[G]\)
8: Update the mean estimates: \(_{g,t+1}=}_{s=1}^{t+1}_{X_{s}=g}Y_ {s}, g[G]\)
9: Update the standard deviation estimates: \(_{g,t+1}:=-1}_{s t+1:X_{s}=g}( Y_{s}-_{g,t+1})^{2}}, g[G]\)
10:endfor
11:\(_{g,T}=}_{s=1}^{T}_{X_{s}=g}Y_{s},  g[G]\) ```

**Algorithm 1** Variance-UCB \((p,T,G,c_{1},c_{2})\)

Variance-UCB takes as input the time horizon \(T\), the norm \(p\), the groups \([G]\), and the sub-Gaussian parameters \(c_{1},c_{2}\). The algorithm initializes the upper confidence bound for every group to be infinity, which reflects the absence of knowledge of \(\{_{g}\}\).

In the special case \(p=+\), Variance-UCB (instantiated with a different choice of \(C_{T}\)) coincides with the _B-AS_ algorithm of .2

### Regret guarantees

Our first main result gives theoretical bounds on the performance of Algorithm 1 for finite \(p[1,)\). We show in Theorem 1 that when \(p\) is finite, Variance-UCB incurs regret of \((T^{-2})\).

**Theorem 1**.: _For any \(}\) that satisfies Assumptions 1 and 2 and for any finite \(p\), the regret of Variance-UCB is at most \((T^{-2})\). That is,_

\[_{p,T}(,})=(T ^{-2}).\]

Our second main result, Theorem 2, provides a matching lower bound for our problem. Thus showing that the performance of Variance-UCB and its analysis is the best possible in terms of \(T\).

**Theorem 2**.: _Let \(p\) be finite and \(\) be a universal constant. For any online policy \(\), there exists an instance \(}_{}\) such that for any \(T 1\),_

\[_{p,T}(,}_{})(p+1)T^{-2}+O(T^{-2.5})=(T^{-2}).\]

Our analysis and proof techniques can be extended naturally to the case where \(p=+\). However, the \((T^{-2})\) regret no longer holds, and the convergence rate jumps to \((T^{-3/2})\).

**Theorem 3**.: _Let \(_{}:=_{g[G]}_{g}^{2}\). For any \(}\) that satisfies Assumptions 1 and 2,_

\[_{,T}(,}) (C_{T}}+C_{T}^{2})G^{1.5}T^{-1.5}+o(T^{-1.5})= (T^{-1.5}).\]

Note that a similar bound for Theorem 3 has already been established in :

\[_{,T}(,})((c_{2}+2)^{2}+1)^{2}(T)_{}G^{2.5}}{_{}^{2}T^{1.5 }}+o(T^{-1.5}).\]Our result in Theorem 3 improves the existing regret in all the problem parameters, i.e., \(_{}\) to \(}\), and from \(G^{2.5}\) to \(G^{1.5}\)). The most significant improvement lies in removing the dependence on \(_{}\) in the main term of the regret. While \(_{}\) still appears in the negligible term \(o(T^{-1.5})\), this term will be asymptotically dominated for even moderate \(T\). This improved bound gives a better understanding on how \(_{}\) impacts the performance of the algorithm, which was left as an open question in .

Finally, we give a matching lower bound for the case when \(p=\), showing that the analysis of Variance-UCB is tight in \(T\) when \(p=+\). To the best of our knowledge, no lower bound for this problem was previously known.

**Theorem 4**.: _For any online policy \(\), there exists an instance \(_{}}\) such that for any \(T 1\),_

\[_{,T}(,_{ {}}})G^{1.5}T^{-1.5}.\]

In Sections 4 and 5, we give outlines of the proofs of Theorems 1 and 2, respectively. While we briefly mention why both results change at \(p=+\), the full proofs for Theorems 3 and 4 is deferred to Appendix C.

## 4 Deriving the upper bounds

In this section, we give an overview of the main steps to prove Theorem 1. A complete proof is given in Appendix A. We first show in Lemma 1 that the optimal \(R_{p}^{*}()\) is achieved for an optimal static policy \(_{T}^{*}(p)=\{n_{g,T}^{*}(p)\}_{g[G]}\) that assumes knowledge of \(\) and samples each group \(g\) proportionally to \(_{g}^{}\).

**Lemma 1**.: _[Benchmark analysis] For each \(t^{*}\) and \(p[1,+]\), let \(n_{g,t}^{*}=^{}t}{_{h[G]}_{h}^{ }}\). Then,_

\[R_{p}^{*}()=R_{p}(_{T}^{*})=R_{p}( _{1}^{*}).\]

The proof of Lemma 1 utilizes the KKT conditions of the optimization program (3) that defines \(R_{p}^{*}\). For convenience, we introduce3\(_{p}:=_{g[G]}_{g}^{}\) for each \(p[1,+]\) so that we can simplify \(n_{g,t}^{*}=}_{g}^{}\). Using Equation (4), the expression of regret can be simplified to

\[_{p,T}(,})=_{ }[R_{p}(_{T})-R_{p}(_{T}^{*} )].\] (7)

From Eq. (7), we can understand the behavior of the regret by answering the following questions.

1. How close is the random variable \(_{T}\) to the optimal sampling vector \(_{T}^{*}\)?
2. How does the curvature of \(R_{p}(.)\) affect the difference \(R_{p}(_{T})-R_{p}(_{T}^{*})\)?

Upper bounding the error \(_{T}-_{T}^{*}\):Before we answer the first question, we show in Lemma 2 that \(_{t}(_{g})\) is an increasingly accurate upper bound for \(_{g}\).

**Lemma 2**.: _For all \(g[G]\), with probability at least \(1-(T^{-2})\),_

\[0_{t}(_{g})^{}-_{g}^{} }{}}(_{g}+}{ }})^{}.\]

The proof of Lemma 2 utilizes Assumption 1 and the choice of \(_{t}(_{g})\)4, as defined in Eq. (5).

Using Lemma 2, the difference between the number of samples Variance-UCB collects and the optimal number of samples can be bounded with high probability, which we state in Lemma 3.

**Lemma 3**.: _Variance-UCB collects a vector of samples \(\) such that for all \(g[G]\), with probability at least \(1-(T^{-2})\),_

\[n_{g,T}-n_{g,T}^{*} 3+p}{_{p}(p+1)}(_{g}+}{^{*}}})^{}^{*}}=( ).\]

To understand the meaning of Lemma 3, we note that the feedback-dependent structure of the algorithm makes the numbers of samples \(n_{g,T}\) correlated across groups and over time, since the algorithm attempts to sample regularly from all groups. A key technical challenge is to decouple \(n_{g,t}\) across groups and derive an instance dependent upper bound on \(_{T}-_{T}^{*}\). This challenge does not arise in the classic multi-armed bandits setting, where the decision-maker's goal is to repeatedly pull only the best arm.

Curvature properties of \(R_{p}\) around the optimal value \(_{T}^{*}\):To answer the second question, we exploit the smoothness of \(R_{p}()\) around \(_{T}^{*}\) to approximate it with a polynomial function of \(\). Since \(_{T}^{*}\) is the minimizer of \(R_{p}()\) (subject to \(_{g[G]}n_{g}=T\)) and \(R_{p}()\) is differentiable around \(_{T}^{*}\), the first order of the Taylor approximation of \(R_{p}\) vanishes as \(T\) grows large, formalized in Lemma 4.

**Lemma 4**.: _Let \(p<+\) and \(^{}_{+}^{G}\) such that \(_{g[G]}n_{g}^{}=T\). Then,_

\[|(^{})-R_{p}(_{T}^{*})}{R_{p}(^{*})}- _{g[G]}^{}-n_{g,T}^{*})^{2}}{Tn_{g,T}^{* }}|_{p}^{2}}{_{}^{2}}_{g}( ^{*}}{n_{g}^{}})^{3p+3}^{}-_{T}^{*}\|_{}^{3}}{T^{3}}.\]

We note that the bound in Lemma 4 holds regardless of the choice of the vector \(^{}\), including those generated by Variance-UCB. One can interpret the upper bound in Lemma 4 as follows: the term \(_{g[G]}^{}-n_{g,T}^{*})^{2}}{Tn_{g,T}^{ *}}\) represents the exact Taylor first-order approximation of \((^{})-R_{p}(_{T}^{*})}{R_{p}(^{*})}\), and the right hand side represents an upper bound on this approximation. In particular, assuming an error \(=^{}-^{*}\), Lemma 4 can be restated as,

\[(^{})-R_{p}(_{T}^{*})}{R_{p}(^{*})}= (\|^{2}}{T^{2}})+o( \|^{2}}{T^{2}}).\]

Putting everything together:The rest of the proof consists of applying Lemmas 3 and 4 to the regret expression in Eq. (7). By Lemma 3, with high probability, \(\|_{T}-_{T}^{*}\|_{}=()\). Applying this to Lemma 4 gives that \(R_{p}(_{T})-R_{p}(_{T}^{*})=\|_{T}-_{T}^{*}\|_{} ^{2}R_{p}^{*} O(T^{-2})\). By Lemma 1, \(R_{p}^{*}=(T^{-1})\), and therefore with high probability, \(R_{p}(_{T})-R_{p}(_{T}^{*})=(T^{-2})\). With additional work, we show that the equality also holds in expectation, which implies from Eq. (7) that \(_{p,T}(,})=_{}[R_{p} ()-R_{p}(_{T}^{*})]=(T^{-2})\).

### The case \(p=+\)

Even though Lemmas 1, 2, and 3 hold for \(p=+\), the approximation guarantee given in Lemma 4 does not hold because \(R_{}\) is not differentiable at \(_{T}^{*}\). As an alternative, we derive a first order upper bound, which we state in Lemma 5.

**Lemma 5**.: _Let \(_{+}^{G}\) and \(^{}_{+}^{G}\) such that \(_{g[G]}n_{g}^{}=T\). Then,_

\[(_{T}^{})-R_{}(_{T}^{*})}{R_{}( {n}_{T}^{*})}-_{g}(^{}}{n_{g,T}^{*}}-1)+ _{g}(^{}}{n_{g,T}^{*}}-1)^{2}_{g }(^{*}}{n_{g}^{}})^{3}\]

Similar to Lemma 4, we interpret the upper bound above by decomposing it into a main term, \(-_{g}(^{}}{n_{g,T}^{*}}-1)\), and an error term, \(_{g}(^{}}{n_{g,T}^{*}}-1)^{2}_{g }(^{*}}{n_{g}^{}})^{3}\). As opposed to the case where \(p<+\), \(R_{}\) is not differentiable in \(^{*}\), thus the inequality above does not stem from a Taylor approximation. Adapting the bound in Lemma 5 in the same steps as in **Step 3** gives a regret bound of \((T^{-1.5})\).

Deriving the lower bounds

In this section, we provide the key ideas behind the proof of Theorem 2, with the full proof given in Appendix B. Given any policy \(\), the idea is to pick \(}_{}\) from two constructed instances \(}^{a}=_{1}^{a}_{G}^{a}\) and \(}^{b}=_{1}^{b}_{G}^{b}\). The interaction of \(}^{a}\) (resp. \(}^{b}\)) with \(\) yields a random vector of the number of collected samples, denoted by \(^{a}\) (resp. \(^{b}\)). \(}^{a}\) and \(}^{b}\) must satisfy the following two conflicting properties:

1. \(}^{a}\) and \(}^{b}\) are sufficiently similar, in the sense that the distributions of \(^{a}\) and \(^{b}\) are close, so that \(\) would have a hard time distinguishing between \(}^{a}\) and \(}^{b}\). This notion of similarity is captured by the KL-divergence of \(}^{a}\) and \(}^{b}\).
2. \(}^{a}\) and \(}^{b}\) are also sufficiently dissimilar in the sense that they induce distinct optimal allocation rules \(_{a}^{*}\) and \(_{b}^{*}\). Specifically, any allocation \(\) cannot be simultaneously close to both \(_{a}^{*}\) and \(_{b}^{*}\), and \(\) incurs a high regret under at least one of the two instances. This notion of dissimilarity is captured below.

Let \(^{a}_{+}^{G}\) be the vector of standard deviations of all groups under \(}^{a}\), and let \(S_{}^{a}\) be the set of allocations \(\) such that \(R_{p}(;^{a})\) is within \(\) of \(R_{p}^{*}(^{a})\) for any \( S_{}^{a}\). Formally,

\[S_{}^{a}:=\{^{G}|_{g}n_{g}=T, R _{p}(,^{a})-R_{p}^{*}(^{a})\}.\]

Let \(^{b}\) be a second vector and define \(S_{}^{b}\) similarly. We define the dissimilarity \(d(^{a},^{b}):=_{ 0}\{S_{}^{a}  S_{}^{b}\}\), which is the smallest \(\) so that these sets of allocations are disjoint. There is a tension between the two requirements on \(}^{a}\) and \(}^{b}\), which is formalized next in Lemma 6.

**Lemma 6**.: _Let \(\) be a fixed policy and \(}^{a}\), \(}^{b}\) be two instances with standard deviation vectors \(^{a},^{b}\), respectively. Then,_

\[\{_{p,T}(,}^{a}),_{p,T}( ,}^{b})\} d(^{a},^{b}) (-_{g[G]}_{,}^{a}}[n_{g,t}]KL( _{g}^{a}||_{g}^{b})).\]

The proof follows from an adapted version of LeCam's method . To understand the implication of Lemma 6, we examine the two terms involved in the right hand side of the inequality. The first term, \(d(^{a},^{b})\), increases the further \(^{a}\) and \(^{b}\) are from each other. The second term has the opposite monotonicity, as it decreases with the \(KL\) divergence of the two instances. These conflicting terms capture the trade-off of loss minimization versus information gain, and deriving the lower bound will come from constructing two instances for which this trade-off is maximised.

Specifically, \(}^{a}\) is chosen such that all groups have data distributed according to \(_{g}^{a}(0,^{2})\); we denote \(^{a}:=(1,,1)\) as the vector of standard deviations of each group. Even though instance \(a\) has all identical groups, the policy \(\) may treat groups differently, and the interaction of \(\) with \(}^{a}\) may yield give different expected numbers of samples from each group. We pick \(h[G]\) such that \(_{,}^{a}}[n_{h,T}]\) is minimized over \([G]\), and construct \(}^{b}\) as follows:

\[}^{b}:_{h}^{b}&(0, ^{2}(1+}))\\ _{g}^{b}&(0,^{2}), g  h.\]

On the one hand, notice that \(}^{a}\) and \(}^{b}\) only differ in their coordinate \(h\), so that \(KL(_{g}^{a}||_{g}^{b})=(g=h)(GT^{-1})\), and by minimality of \(h\), \(_{,}^{a}}[n_{h,t}]\), so that

\[_{g[G]}_{,}^{a}}[n_{g,t}]KL(_{g}^{a}||_{g}^{b})=_{,}^{a}}[n_{h,t}]KL(_{h}^{a}||_{h}^{b})(GT^{-1})= (1).\]

Second, we show that \(d(^{a},^{b})=(T^{-2})\). We do this by measuring the distance between the sets \(S_{}^{a}\) and \(S_{}^{b}\) for a fixed \(\), and then estimate the smallest \(\) for which this distance is \(0\). This smallest \(\) corresponds to \(d(^{a},^{b})\). Combining both in Lemma 6 yields \(\{_{p,T}(,}^{a}),_{p,T}( ,}^{b})\}(T^{-2})\).

**Remark 1**.: _For \(p=+\), the proof remains the same except that in this case \(d(^{a},^{b})=(T^{-3/2})\), resulting in an overall lower bound of \((T^{-3/2})\). See Appendix C.3 for the complete proof._Numerical study

In this section, we present experimental results on the empirical performance of Variance-UCB. We explore the impact of varying each important parameter of the problem: the time horizon \(T\), norm parameter \(p\), number of groups \(G\), distributions \(_{g}\), and the sub-Gaussianity parameters \(c_{1},c_{2}\). In all the experiments \(_{g}\) follow Gaussian distributions. Except where they are varied, the default parameter settings are \(T=10^{5}\), \(p=2\), \(G=2\), with the respective data distributions of groups 1 and 2 as \((1,1)\) and \((2,2.5)\), satisfying \(c_{1}=c_{2}=5\). For each parameter setting evaluated, we run Variance-UCB 500 times and report average regret over all 500 runs. For convenience, time and regret are presented on logarithmic scales.

First, we vary the choice of the parameter \(p\{1,2,10,25,+\}\) and observe its effect on the regret. Figure 1 shows that the convergence rates for each \(p\) precisely match those predicted by Theorems 1 and 3: a slope of -2 for the finite values of \(p\) (\(\{1,2,10,25\}\)), and a slope of -1.5 for \(p=\).

Next, we study how mis-specifying the sub-Gaussianity parameters \(c_{1}\) and \(c_{2}\) affect the regret of Algorithm 1. The parameter values (\(c_{1},c_{2}\)) only affect the algorithm's behavior through \(C_{T}\), so it is more natural to directly study the impact of errors in \(C_{T}\). For the parameters used in our experimental setup, the value of \(C_{T}\) prescribed in Equation (6) is \( 5\); we consider both underestimating and overestimating \(C_{T}\), and evaluate regret when instead plugging in values of \(C_{T}\{0.001,5,1000\}\) in the UCB update step defined in Equation (5). Figure 2 shows that choosing an overestimate of \(C_{T}\) incurs an increased regret. This is not surprising since choosing a larger \(C_{T}\) decreases the confidence of the algorithm in its estimates, and forces over-exploration. Choosing a low \(C_{T}\) also incurs a higher regret which can more severely impact the performance, as the algorithm under-explores and can get stuck in sub-optimal behavior. However, with a long enough time horizon, the algorithm will eventually estimate \(\) accurately, regardless of the small choice of \(C_{T}\). As observed in Figure 2, the smallest value of \(C_{T}=0.001\) initially has the highest regret, corresponding to incorrect estimates and insufficient exploration, and then finally converges very late to have the lowest regret. Even for very large \(T\), this curve has higher variance due to the noise in the estimates.

Figure 1: Impact of the \(p\)-norm on the regret.

Figure 2: Impact of mis-estimating \(C_{T}\) on the regret

Next, we study the effect of varying the lowest variance \(_{}^{2}\{0.05,0.1,0.5,1\}\) while holding all other variances fixed, for each \(p\{1,2,+\}\). First, Figure 3 shows that varying \(_{}\) has no effect on the regret when \(p=+\). This matches a result of , where they prove that when \(p=+\), their UCB-style algorithm (very similar to Variance-UCB) is not affected by the lowest variance when the feedback is Gaussian. However, our experimental results show that this phenomenon does not persist when \(p\) is finite, as illustrated in Figure 3, where we observe that regret decreases when the lowest variance \(_{}\) increases. This is surprising since increasing the lowest variance makes the feedback more volatile, and one would expect an increase in regret as a result.

Finally, we vary the number of groups \(G\{2,10,50\}\). For the additional groups, we generate their data from Gaussian \(_{g}\), with means \(_{g}([-1,1])\) and standard deviations \(_{g}()\) independently for each group. From Figure 4, we observe that the regret increases in the number of groups, as expected. When the number of groups is small (\(G=2\)), Variance-UCB quickly enters a regime where regret decreases quickly. However, as the number of groups grows (\(G\{10,50\}\)), the necessary time to enter the decay regime increases (\( 30,000\) for \(G=10\) and \( 90,000\) for \(G=50\)). Initially the algorithm samples (on average) uniformly across all groups due to the UCB term outweighing the sample variance estimates, and each group must wait to be sampled enough times for the algorithm to estimate its optimal sampling rate. This delay will naturally increase with the number of groups. Once the confidence bounds are small enough, the algorithm samples the highest variances first. This causes abrupt variation (especially in the case where \(p\) is small) because the objective function is very sensitive to changes in one coordinate.