# Identifiability Guarantees for Causal Disentanglement from Soft Interventions

Jiaqi Zhang

LIDS, MIT

Broad Institute of MIT and Harvard

&Kristjan Greenewald

MIT-IBM Watson AI Lab

IBM Research

Chandler Squires

LIDS, MIT

Broad Institute of MIT and Harvard

&Akash Srivastava

MIT-IBM Watson AI Lab

IBM Research

Caroline Uhler

LIDS, MIT

Broad Institute of MIT and Harvard

Currently at Google Research. Contributions to this work were made when affiliated with IBM research.

###### Abstract

Causal disentanglement aims to uncover a representation of data using latent variables that are interrelated through a causal model. Such a representation is identifiable if the latent model that explains the data is unique. In this paper, we focus on the scenario where unpaired observational and interventional data are available, with each intervention changing the mechanism of a latent variable. When the causal variables are fully observed, statistically consistent algorithms have been developed to identify the causal model under faithfulness assumptions. We here show that identifiability can still be achieved with unobserved causal variables, given a generalized notion of faithfulness. Our results guarantee that we can recover the latent causal model up to an equivalence class and predict the effect of unseen combinations of interventions, in the limit of infinite data. We implement our causal disentanglement framework by developing an autoencoding variational Bayes algorithm and apply it to the problem of predicting combinatorial perturbation effects in genomics.

## 1 Introduction

The discovery of causal structure from observational and interventional data is important in many fields including statistics, biology, sociology, and economics . Directed acyclic graph (DAG) models enable scientists to reason about causal questions, e.g., predicting the effects of interventions or determining counterfactuals . Traditional causal structure learning has considered the setting where the causal variables are observed . While sufficient in many applications, this restriction is limiting in most regimes where the available datasets are either perceptual (e.g., images) or very-high dimensional (e.g., the expression of \(>20k\) human genes). In an imaging dataset, learning a causal graph on the pixels themselves would not only be difficult since there is no common coordinate system across images (pixel \(i\) in one image may have no relationship with pixel \(i\) in another image) but of questionable utility due to the relative meaninglessness of interventions on individual pixels. Similar problems are also present when working with very high-dimensional data. For example,in a gene-expression dataset, subsets of genes (e.g. belonging to the same pathway) may function together to induce other variables and should therefore be aggregated into one causal variable.

These issues mean that the causal variables need to be learned, instead of taken for granted. The recent emerging field of causal disentanglement [9; 65; 30] seeks to remedy these issues by recovering a causal representation in _latent_ space, i.e., a small number of variables \(U\) that are mapped to the observed samples in the ambient space via some mixing map \(f\). This framework holds the potential to learn more semantically meaningful latent factors than current approaches, in particular factors that correspond to interventions of interest to modelers. Returning to the image and the genomic examples, latent factors could, for example, be abstract functions of pixels (corresponding to objects) or groups of genes (corresponding to pathways).

Despite a recent flurry of interest, causal disentanglement remains challenging. First, it inherits the difficulties of causal structure learning where the number of causal DAGs grows super-exponentially in dimension. Moreover, since we only observe the variables after the unknown mixing function but never the latent variables, it is generally impossible to recover the latent causal representations with only observational data. Under the strong assumption that the causal DAG is the empty graph, such unidentifiability from observational data has been discussed in previous disentanglement works .

However, recent advances in many applications enable access to interventional data. For example, in genomics, researchers can perturb single or multiple genes through CRISPR experiments . Such interventional data can be used to identify the causal variables and learn their causal relationships. When dealing with such data, it is important to note that single-cell RNA sequencing and other biological assays often destroy cells in the measurement process. Thus, the available interventional data is _unpaired_: for each cell, one only obtains a measurement under a single intervention.

In this work, we establish identifiability for _soft_ interventions on general structural causal models (SCMs), when the latent causal variables are observed through a class of (potentially non-linear) polynomial mixing functions proposed by . Prior works [57; 20; 67] show that the causal model can be identified under _faithfulness_ assumptions, when all the causal variables are observed. We here demonstrate that identifiability can still be achieved when the causal variables are _unobserved_ under a generalized notion of faithfulness. The identifiability is up to an equivalence class and guarantees that we can predict the effect of unseen combinations of interventions, in the limit of infinite data. It then remains to design an algorithmic approach to estimate the latent causal representation from data. We propose an approach based on autoencoding variational Bayes , where the decoder is composed of a deep SCM (DSCM)  followed by a deep mixing function. Finally, we apply our approach to a real-world genomics dataset to find genetic programs and predict the effect of unseen combinations of genetic perturbations.

### Related Work

**Identifiable Representation Learning.** The identifiability of latent representations from observed data has been a subject of ongoing study. Common assumptions are that the latent variables are independent , are conditionally independent given some observed variable [23; 28], or follow a known distribution . In contrast, we do not make any independence assumptions on the latent variables or assume we know their distribution. Instead, we assume that the variables are related via a causal DAG model, and we use data from interventions in this model to identify the representation.

**Causal Structure Learning.** The recovery of a causal DAG from data is well-studied for the setting where the causal representation is directly observed . Methods for this task take a variety of approaches, including exact search  and greedy search  to maximize a score such as the posterior likelihood of the DAG, or an approximation thereof. These scores can be generalized to incorporate interventional data [62; 67; 33], and methods can often be naturally extended by considering an augmented search space . Indeed, interventional data is generally necessary for identifiability without further assumptions on the functions relating variables .

**Causal Disentanglement.** The task of identifying a causal DAG over latent causal variables is less well-studied, but has been the focus of much recent work [9; 65; 30]. These works largely do not consider interventions, and thus require restrictions on functional forms as well as structural assumptions on the map from latent to observed variables. Among works that do not restrict the map,  and  assume access to _paired_ counterfactual data. In contrast, we consider only _unpaired_ data, which is more common in applications such as biology . Unpaired interventional data is considered by , , and as a special case of . These works do not impose structural restrictions on the map from latent to observed variables but assume functional forms of the map, such as linear or polynomial. Our work builds on and complements these results by providing identifiability for _soft_ interventions and by offering a learning algorithm based on variational Bayes. We remark here that the task of causal disentanglement is sometimes called _causal representation learning_ in literature. We adopted the term causal disentanglement mainly following , as causal representation learning also includes methods such as Invariant Risk Minimization (IRM)  which do not completely learn latent variables. We discuss contemporaneous related work in Appendix I.

## 2 Problem Setup

We now formally introduce the causal disentanglement problem of identifying latent causal variables and causal structure between these variables. We consider the _observed_ variables \(X=(X_{1},...,X_{n})\) as being generated from _latent_ variables \(U=(U_{1},...,U_{p})\) through an unknown deterministic (potentially non-linear) mixing function \(f\). In the observational setting, the latent variables \(U\) follow a joint distribution \(_{U}\) that factorizes according to an unknown directed acyclic graph (DAG) \(\) with nodes \([p]=\{1,...,p\}\). Concisely, we have the following data-generating process:

\[X=f(U), U_{U}=_{i=1}^{p}(U_{i} U _{_{}(i)}),\] (1)

where \(_{}(i)=\{j[p]:j i\}\) denotes the parents of \(i\) in \(\). We also use \(_{}(i)\), \(_{}(i)\) and \(_{}(i)\) to denote the children, descendants and ancestors of \(i\) in \(\). Let \(_{X}\) denote the induced distribution over \(X\).

We consider atomic (i.e., single-node) interventions on the latent variables. While our main focus is on general types of soft interventions, our proof also applies to hard interventions. In particular, an _intervention_\(I\) with target \(T_{}(I)=i[p]\) modifies the joint distribution \(_{U}\) by changing the conditional distribution \((U_{i} U_{_{}(i)})\). A _hard_ intervention sets the conditional distribution as \(^{I}(U_{i})\), removing the dependency of \(U_{i}\) on \(U_{_{}(i)}\), whereas a _soft_ intervention is allowed to preserve this dependency but changes the mechanism into \(^{I}(U_{i} U_{_{}(i)})\). An example of a soft intervention is as a shift intervention [50; 69], which modifies the conditional distribution as \(^{I}(U_{i}=u+a_{i} U_{_{}(i)})= (U_{i}=u U_{_{}(i)})\) for some shift value \(a_{i}\). In the following, we will use \(^{I}_{U}=_{i=1}^{p}^{I}(U_{i} U_{_{ }(i)})\) to denote the interventional distribution, where \(^{I}(U_{j} U_{_{}(j)})=(U_{j}  U_{_{}(j)})\) for \(j T_{}(I)\). We denote the induced distribution over \(X\) by \(^{I}_{X}\). In cases where the referred random variable is clear from the context, we abbreviate the subscript and use \(^{I}\) instead.

We consider the setting where we have unpaired data from observational and interventional distributions, i.e., \(,^{I_{1}},...,^{I_{K}}\). Here, \(\) denotes samples of \(X=f(U)\) where \(U_{U}\); \(^{I_{k}}\) denotes samples of \(X\) where \(U^{I_{k}}_{U}\). We focus on the scenario where we have _at least_ one intervention per latent node. In the worst case, one intervention per node is necessary for identifiability in linear SCMs . We note that having _at least_ one intervention per latent node is a strict generalization of having _exactly_ one intervention per latent node, since we assume no knowledge of which interventions among \(I_{1},...,I_{K}\) target the same node. Throughout the paper, we assume latent variables \(U\) are unobserved and their dimension \(p\), the DAG \(\), and the interventional targets of \(I_{1},...,I_{K}\) are unknown. The goal is to identify these given samples of \(X\) in \(,^{I_{1}},...,^{I_{K}}\).

## 3 Equivalence Class for Causal Disentanglement

In this section, we characterize the equivalence class for causal disentanglement, i.e., the class of latent models that can generate the same observed samples of \(X\) in \(,^{I_{1}},...,^{I_{K}}\). Since we only have access to this data, the latent model can only be identified up to this equivalence class.

Figure 1: Example of the data-generating process, where observed gene expressions \(X=f(U)\) and the distribution of \(U\) factorizes with respect to an unknown DAG \(\).

First, note that the data-generation process is agnostic to the re-indexing of latent variables, provided that we change the mixing function to reflect such re-indexing. More precisely, consider an arbitrary permutation \(\) of \([p]\). Denote \(U_{}=(U_{(1)},...,U_{(p)})\) and \(f_{}\) as the mixing function such that \(f_{}(U_{})=f(U)\). We define \(_{}\) as the DAG with nodes in \([p]\) and edges \(i j\) if and only if \((i)(j)\). Then the following data-generating process,

\[X=f_{}(U_{}), U_{}_{U_{}}=_{i=1} ^{p}(U_{})_{i}(U_{})_{_{_{ }}(i)},\]

satisfies \(X=f(U)\). The same argument holds when \(U\) is generated from an interventional distribution \(^{I}\), where this process generates the same \(X\) when \(U_{}\) is sampled from \(^{I_{}}_{U_{}}\). Here \(I_{}\) is such that \(T_{_{}}(I_{})=^{-1}(T_{}(I))\) and the mechanism \(^{I_{}}(U_{})_{i}(U_{})_{_{_{}}(i)}=^{I}U_{(i)} U_{_{}((i))}\).

We would also observe the same data if each \(U_{i}\) is affinely transformed into \(_{i}U_{i}+b_{i}\) for constants \(_{i} 0\) and \(b_{i}\) and the mixing function is rescaled element-wise to accommodate this transformation. To account for these two types of equivalences, we define the following notion of causal disentanglement (CD) equivalence class.

**Definition 1** (CD-Equivalence).: _Two sets of variables, \( U,,I_{1},...,I_{K}\) and \(,},_{1},...,_{K}\) are CD-equivalent if and only if there exists a permutation \(\) of \([p]\), non-zero constants \(_{1},...,_{p} 0\), and \(b_{1},...,b_{p}\) such that_

\[_{i}=_{(i)}U_{(i)}+b_{(i)},\  i[p],}=_{},\ _{k}=(I_{k})_{},\  k[K].\]

_The same definition applies to \(,I_{1},...,I_{k}\) and \(},_{1},...,_{k}\), where we say they are CD-equivalent if and only if \(}=_{}\), and \(_{k}=(I_{k})_{}\) for some permutation \(\)._

For simplicity, we refrain from talking about transformations on the mixing function \(f\) and mechanisms of latent variables. These can be obtained once \(U,,T_{}(I_{1}),...,T_{}(I_{K})\) are identified. In particular, \(f\) is the map from \(U\) to the observed \(X\); and the joint distribution \(_{U}\) (and \(^{I_{k}}_{U}\)) can be decomposed with respect to \(\) to obtain the mechanisms \(_{U}(U_{i} U_{_{}(i)})\) (and \(^{I_{k}}_{U}(U_{i} U_{_{}(i)})\)).

## 4 Identifiability Results

In this section, we present our main results, namely the identifiability guarantees for causal disentanglement from soft interventions. For this discussion, we consider the _infinite-data_ regime where enough samples are obtained to exactly determine the observational and interventional distributions \(_{X},^{I_{1}}_{X},...,^{I_{K}}_{X}\). Detailed proofs are deferred to Appendices A and B.

### Preliminaries

Following , we pose assumptions on the support of \(U\) and on the function class of the map \(f\). Our support assumption is for example satisfied under the common additive Gaussian structural causal model , and our assumption on the function class is for example satisfied if \(f\) is linear and injective (Lemma 2 in Appendix A), a setting considered in many identifiability works (e.g., [11; 1; 53]).

**Assumption 1**.: _Let \(U\) be a \(p\)-dimensional random vector. Following , we assume that the interior of the support of \(_{U}\) is a non-empty subset of \(^{p}\), and that \(f\) is a full row rank polynomial.2_

Under this assumption, the authors in  showed that if \(p\) is known, \(U\) is identifiable up to a linear transformation. This remains true when \(p\) is unknown, as summarized in the following lemma.

**Lemma 1**.: _Under Assumption 1, we can identify the dimension \(p\) of \(U\) as well as its linear transformation \(U+b\) for some non-singular matrix \(\) and vector \(b\). In fact, with observational data, we can only identify \(U\) up to such linear transformations._

Denote all pairs of \(_{U},f\) that satisfy this assumption as \(_{p}\). The proof of this lemma is provided by solving the following constrained optimization problem:

\[_{(_{},)_{}}\ _{()}=_{X}.\]In other words, let \(\) be the smallest dimension such that there exists a pair of \(_{},\) in \(_{}\) that generates the observational distribution \(_{X}\). Then \(p=\) and we recover the latent factors up to linear transformation. The intuition is that (1) the support with non-empty interior guarantees that we can identify \(p\) by checking its geometric dimension, and (2) the full-rank polynomial assumption ensures that we search for \(f\) (and consequently \(U\)) in a constrained subspace.

On the other hand, to show we cannot identify more than linear transformations, we construct a mixing function \(\) for \(:=U+b\) such that the induced distribution \(_{X}\) is the same under both representations. This also means that we cannot identify the underlying DAG \(\) up to any nontrivial equivalence class; we give an example showing that any causal DAG can explain the observational data in Appendix A. Next, we discuss how identifiability can be improved with interventional data.

### Identifying ancestral relations

Lemma 1 guarantees identifiability up to linear transformations from solely observational data. This reduces the problem to the case where an unknown invertible linear mixing of the latent variables \(X=f(U)=U+b\) is observed. Without loss of generality, we thus work with this reduction for the remainder of the section.

When the causal variables are _fully observed_, we can identify causal relationships from the changes made by interventions . In particular, an intervention on a node will not alter the marginals of its non-descendants as compared to the observational distribution, i.e., \((U_{j})=^{I}(U_{j})\) for \(T_{}(I)=i\) and \(j_{}(i)\). However, it is possible that \((U_{j})=^{I}(U_{j})\) for some \(j_{}(i)\) in degenerate cases where the change made by \(U_{i}\) is canceled out on the path from \(i\) to \(j\). Hence, prior works3 defined _influentiality_ or _interventional faithfulness_, which avoids such degenerate cases by assuming that intervening on a node will always change the marginals of all its descendants, i.e., \((U_{j})^{I}(U_{j})\) for \(j_{}(i)\). Under this assumption, we can identify the descendants of an intervention target in \(\), by testing if a node has a changed marginal interventional distribution.

However, if we only observe a linear mixing of the causal variables, interventional faithfulness is not enough to identify such ancestral relations. Consider the following example.

**Example 1**.: _Let \(=\{1\!\!2\}\) with \((U_{1})\!=\!(0,1)\) and \((U_{2}\!\!U_{1})=(U_{1},1)\). Suppose that \(T_{}(I_{1})=1\), with \(^{I_{1}}(U_{1})=(1,1)\), and that \(T_{}(I_{2})=2\), with \(^{I_{2}}(U_{2} U_{1})=(U_{1}+1,1)\). Note that this model satisfies interventional faithfulness._

_Let \(f\) be the identity map, i.e., \(X=U\). Consider latent variables \(=(U_{2},U_{2}-U_{1})\) and \(()=(_{1}-_{2},_{1})\). Then \(X=()=f(U)\). However, we have \(}=\{2\!\!1\}\) with \((_{2})=(0,1)\) and \((_{1}_{2})=(_{2},1)\), \(T_{}}(I_{1})=1\) with \(^{I_{1}}(_{1}_{2})=(_{2}+1,1)\), and \(T_{}}(I_{2})=2\) with \(^{I_{2}}(_{2})=(1,1)\). We thus may reverse ancestral relations between the intervention targets, as illustrated in Figure 2._

This example shows that the effect on \(U_{2}\) from intervening on \(U_{1}\) can be canceled out by linearly combining \(U_{2}\) with \(U_{1}\). In other words, intervening on \(U_{1}\) does not change the marginal distribution of \(U_{2}-U_{1}\), even under interventional faithfulness. Thus, we need a stronger faithfulness assumption to account for the effect of linear mixing. In general, we want to avoid the case that the effect of an intervention on a downstream variable \(U_{j}\) can be canceled out by combining \(U_{j}\) linearly with other variables.

**Assumption 2**.: _Intervention \(I\) with target \(i\) satisfies linear interventional faithfulness if for every \(j\{i\}_{}(i)\) such that \(_{}(j)_{}(i)=\), it holds that \((U_{j}+U_{S}C^{\,})^{I}(U_{j}+U_{S}C^{\,})\) for all constant vectors \(C^{[S]}\), where \(S=[p](\{j\}_{}(i))\)._

This assumption ensures that an intervention on \(U_{i}\) not only affects its children, but that the effect remains even when we take a linear combination of a child with certain other variables. Note that the condition need only hold for the most upstream children of \(U_{i}\), which may be arbitrarily smaller than the set of all children of \(U_{i}\). To illustrate this assumption, we give a simple example on a 2-node

Figure 2:

DAG where this assumption is generically satisfied. In general, we show in Appendix B that a large class of non-linear SCMs and soft interventions satisfy this assumption.

**Example 2**.: _Consider \(=\{1 2\}\). Let \((U_{2} U_{1})=( U_{1}^{2},_{2}^{2})\) and \((U_{1})=(0,_{1}^{2})\). Intervention \(I\) that changes \((U_{1})\) into \((0,_{1}^{ 2})\) satisfies Assumption 2 as long as \( 0\). To see this, note that \((U_{2}+U_{1}C)^{I}(U_{2}+U_{1}C)\) for any \(C\), since \(_{}(U_{2}+U_{1}C)=_{1}^{2}_{1}^{  2}=_{^{I}}(U_{2}+U_{1}C)\)._

Under Assumption 2, we can show that we can identify causal relationships by detecting marginal changes made by interventions. In particular, consider an easier setting where \(K=p\), i.e., we have exactly one intervention per latent node. For a source node4\(i\) of \(\), \((U_{i})^{I}(U_{i})\) if and only if \(T_{}(I)=i\). Therefore the source node will have its marginal changed under one intervention amongst \(\{I_{1},...,I_{p}\}\). This is a property of the latent model that we can utilize when solving for it.

Since we have access to \(X=U+b\), we solve for \(U_{i}\) in the form of \(XC^{}+c\) with \(C^{n},c\), or equivalently, \(UC^{}+c\) with \(C^{p}\). By enforcing that \(V=UC^{}+c\) only has \((V)^{I}(V)\) for one \(I\{I_{1},...,I_{p}\}\), Assumption 2 guarantees that \(V\) can only be an affine transformation of a source node and that this \(I\) corresponds to intervening on this source node. Otherwise: (1) if \(C_{j} 0\) for a non-source node \(j\), take \(j\) to be the most downstream node with \(C_{j} 0\), then \((V)^{I}(V)\) for at least two \(I\)'s targeting \(j\) and its most downstream parents in \(_{}(j)\); (2) if \(C_{i_{1}} 0\) and \(C_{i_{2}} 0\) for two source nodes \(i_{1},i_{2}\), then \((V)^{I}(V)\) for two \(I\)'s targeting \(i_{1}\) and \(i_{2}\).

In general, we can apply this argument to identify all interventions in \(I_{1},...,I_{K}\) that target source nodes of \(\). Then using an iterative argument, we can identify all interventions that target source nodes of the subgraph of \(\) after removing its source nodes. This procedure results in the ancestral relations between the targets of \(I_{1},,I_{K}\). Namely, if \(T_{}(I_{k})_{}(T_{}(I_{j}))\), then \(I_{j}\) is identified in a later step than \(I_{k}\) in the above procedure. We thus have the following theorem.

**Theorem 1**.: _Under Assumption 1 and Assumption 2 for \(I_{1},...,I_{K}\), we can identify \(},_{1},...,_{K}\), where \(}=(_{})\), and \(_{k}=(I_{k})_{}\) for some permutation \(\)._

Here \(\) denotes the transitive closure of a DAG , where \(i j()\) if and only if \(i_{}(j)\). Note that this limitation is not due to the linear mixing of the causal variables. It was shown in  that with fully observed causal variables, one can only identify a DAG up to its transitive closure by detecting marginal distribution changes. In the next section, we show how to reduce \((_{})\) to \(_{}\), i.e., identifying the CD-equivalence class of \(,I_{1},...,I_{k}\).

### Identifying direct edges

DAGs with the same transitive closure can span a spectrum of sparsities; for example, a complete graph and a line graph with the same topological ordering have the same transitive closure. The following example shows that under Assumption 2, in some cases we cannot identify more than the transitive closure.

**Example 3**.: _Let \(\) be the \(3\)-node DAG shown on the left in Figure 3. Suppose that \((U_{1})\) is \((0,1)\), \((U_{2}|U_{1})\) is \((U_{1}^{2},1)\), and \((U_{3}|U_{1},U_{2})\) is \(((U_{1}+U_{2})^{2},1)\). Let \(f\) be the identity map and \(I_{1},I_{2},I_{3}\) target nodes \(1,2,3\), respectively, each changing their conditional variances to \(2\).5_

_Now consider a different model with variables \(=(U_{1},U_{1}+U_{2},U_{3})\) and mixing function \(()=(_{1},_{2}-_{1},_{3})\). Then \(()=U=f(U)=X\). The distributions \(()\), \(^{I_{1}}()\), \(^{I_{2}}()\), and \(^{I_{3}}()\) each factorizes according to the DAG \(}\) that is missing the edge \(1 3\) (Figure 3), where we let \(I_{1},I_{2}\) and \(I_{3}\) change the conditional variances of \(_{1}\), \(_{2}\), and \(_{3}\) to 2, respectively._

This example shows that we cannot identify \(1 3\) since \(U_{1}\,\,U_{3}|U_{1}+U_{2}\). In the case when the causal variables are fully observed, \(1 3\) can be identified by assuming \(U_{1}\,\,U_{3}|U_{2}\). However, when allowing for linear mixing, we need to avoid cases such as \(U_{1}\,\,U_{3}|U_{1}+U_{2}\) in order to be able to identify \(1 3\). We will show that the following assumption guarantees identifiability of \(\).

When \(\) is a polytree (a DAG whose skeleton is a tree), this assumption is implied by Assumption 2 under mild regularity conditions (proven in Appendix B). Thus if \(\) is the sparsest DAG within its transitive closure, we can always identify it with just the Assumptions 1 and 2.

**Assumption 3**.: _For every edge \(i j\), there do not exist constants \(c_{j},c_{k}\) for \(k S\) such that \(U_{i}\,\,U_{j}+c_{j}U_{i}\{U_{l}\}_{l _{}(j)(S\{i\})},\{U_{k}+c_{k}U_{i}\}_{k  S}\), where \(S=_{}(j)_{}(i)\)._

**Theorem 2**.: _Under Assumptions 1,2,3, \(,I_{1},...,I_{K}\) is identifiable up to its CD-equivalence class._

### Further remarks

Next we discuss if it is possible to recover \(U\) along with \(,I_{1},,I_{k}\) up to their CD-equivalence class. Note a simple contradiction with \(=\{1 2\}\): since we consider general soft interventions, there will always be a valid explanation if we add \(U_{1}\) to \(U_{2}\). Therefore even when we can identify \(,I_{1},,I_{k}\) up to its CD-equivalence class, we still cannot identify \(U\) in an element-wise fashion. However, our identifiability results still allow us to draw causal explanations and predict the effect of unseen combinations of interventions, as we discuss below.

**Application of Theorem 1 and 2.** Given unpaired data \(,^{I_{1}},...^{I_{K}}\), these two theorems guarantee that we can identify which \(I_{1},...,I_{K}\) correspond to intervening on the same latent node. Furthermore, Theorem 1 shows that we are able to identify ancestral relationships between the intervention targets of \(I_{1},...,I_{K}\), while Theorem 2 guarantees identifiability of the exact causal structure.

For example, given high-dimensional single-cell transcriptomic readout from a genome-wide knockdown screen, we can under Assumption 1 identify the number of latent causal variables (which we can interpret as the programs of a cell), under Assumption 2 identify which genes belong to the same program, and under Assumption 3 identify the full regulatory relationships between the programs.

**Extrapolation to unseen combinations of interventions.** Theorems 1 and 2 also guarantee that we can predict the effect of unseen combinations of interventions. Namely, consider a combinatorial intervention \(\{I_{1},...,I_{K}\}\), where \(T_{}(I) T_{}(I^{})\) for all \(I I^{}\). In other words, \(\) is an intervention with multiple intervention targets that is composed by combining interventions among \(I_{1},...,I_{K}\) with different targets.

Denote by \(,},_{1},...,_{K}\) the latent model identified from the interventions \(\{I_{1},...,I_{K}\}\). Recall from Section 3 that we can also infer the mixing function \(\) and mechanisms from \(,},_{1},...,_{K}\). From this, we can infer the interventional distribution under the combinatorial intervention \(\):

\[X=(),\ _{}^{}= _{I}_{}(_{T_{}()} _{_{}(T_{}())}) _{I}_{}^{I}(_{T_{}()}_{_{}(T_{}())}).\] (2)

We state the conditions for this result informally in the following theorem. A formal version of this theorem together with its proof are given in Appendix B.5.

**Theorem 3** (Informal).: _Let \(\) be a combinatorial intervention (i.e., with multiple intervention targets) combining several interventions among \(I_{1},...,I_{K}\) with different targets. The above procedure allows sampling \(X\) according to the distribution \(X=f(U),^{}\)._

## 5 Discrepancy-based VAE Formulation

Having shown identifiability guarantees for causal disentanglement, we now focus on developing a practical algorithm for recovering the CD-equivalence class from data. As indicated by our proof of Theorem 2, the latent causal graph can be identified by taking the sparsest model compatible with the data. This characterization suggests maximizing a penalized log-likelihood score, a common method for model selection in causal structure learning . The resulting challenging combinatorial optimization problem has been tackled using a variety of approaches, including exact search using integer linear programming , greedy search [10; 48; 52], and more recently, gradient-based approaches where the combinatorial search space is relaxed to a continuous search space [70; 34; 38; 61].

Gradient-based approaches offer several potential benefits, including scalability, ease of implementation in automatic differentiation frameworks, and significant flexibility in the choice of components. In light of these benefits, we opted for a gradient-based approach to optimization. Inparticular, we replace the log-likelihood term of our objective function with a variational lower bound by employing the framework of autoencoding variational Bayes (AVB), widely used in prior works for causal disentanglement . To employ AVB, we re-parameterize each distribution \((U_{i} U_{}}(i)})\) in Eq. (1) into \(U_{i}=s_{i}(U_{}}(i)},Z_{i})\), where \(Z_{i}\) is an independent exogenous noise variable and \(s_{i}\) denotes the causal mechanism that generates \(U_{i}\) from \(U_{}}(i)}\) and \(Z_{i}\). We let \(p(Z)\) be a prior distribution over \(Z\) and \(p_{,}(X Z)\) be the conditional distribution of \(X\) given \(Z\) under no intervention, thereby defining the marginal distribution \(p_{,}(X)\). Given an arbitrary distribution \(q_{}(Z X)\), we have the following well-known inequality (often called the Evidence Lower Bound or ELBO) for any sample \(x\):

\[ p_{,}(x)^{}_{ ,}(x)+^{}_{}(x), ^{}_{,}(x) :=_{q_{}(Z x)} p_{,}(x  Z),\] \[^{}_{}(x) :=-D_{}(q_{}(Z x)\|p(Z)).\]

Putting this into the framework of an autoencoder, we call the distribution \(q_{}\) the _encoder_ and the distribution \(p_{,}\) the _decoder_. In our case, the decoder is composed of two functions. First, a deep structural causal model \((A_{},s_{,})\) maps the exogenous noise \(Z\) to the causal variables \(U^{}\). In particular, the adjacency matrix \(A_{}\) defines the parent set for each variable, while \(s_{,}=\{(s_{,})_{i}\}_{i=1}^{p}\) denotes the learned causal mechanisms. Second, a mixing function \(f_{}\) maps the causal variables \(U^{}\) to the observed variables \(X^{}\). Because of the permutation symmetry of CD-equivalence, we can fix \(A_{}\) to be upper triangular without loss of generality. We add a loss term \(^{}_{}:=-\|A_{}\|_{1}\) to encourage \(A_{}\) to be sparse.

While the observational samples are generated from the distribution \(p_{,}\), the interventional samples are drawn from a different but related distribution \(p_{,I}\). The modularity of our decoder allows us to replace \((A_{},m_{,})\) with an interventional counterpart \((A_{},m_{,I})\), while keeping the mixing function \(f_{}\) constant. This is illustrated by the highlighted boxes in Fig. 4. For each intervention label \(I\), the corresponding intervention target \(i\) and a shift6\(a_{i}\) is determined by an intervention encoder \(T_{}\), which uses softmax normalization to approximate a one-hot encoding of the intervention target. Given these intervention targets, we generate "virtual" counterfactual samples for each observational sample. Such samples follow the distribution \(_{,}(^{_{k}})\), the pushforward of \(_{X}^{}\) under the action of the encoder \(q_{}\) and decoder \(p_{,I}\). These samples are compared to real samples from the corresponding interventional distribution. A variety of discrepancy measures can be used for this comparison. To avoid the saddle point optimization challenges that come with adversarial training, we do not consider adversarial methods (e.g. the dual form of the Wasserstein distance in ).

Figure 4: **Our proposed CausalDiscrepancyVAE architecture**. Gray boxes represent inputs, white boxes the generated values, blue boxes the trainable modules, and orange boxes the terms of the loss function. Dashed lines indicate copies of the same module or related modules. Highlighted boxes show the procedure to generate virtual counterfactual samples.

This leaves non-adversarial discrepancy measures, such as the MMD (Maximum Mean Discrepancy) , the entropic Wasserstein distance , and the sliced Wasserstein distance . In this work, we focus on the MMD measure, whose empirical estimate we recall in Appendix C.1. We take \(^{^{}}_{,}:=-_{k=1}^{K}(_{,}(^{I_{k}}),_{X}^{I_{k}})\). Thus, the full loss function used during training is

\[^{,,}_{,}:=_{X^{}}[ ^{}_{,}(X)+^{}_{ }(X)]+^{}_{,}+ ^{}_{}.\] (3)

A diagram of the proposed architecture is shown in Fig. 4. Values of the hyperparameters \(,,\) used in our loss function as well as other hyperparameters are described in Appendix F.

Our loss function exhibits several desirable properties. First, as we show in Appendix D, the unpaired data loss function lower bounds the _paired_ data log-likelihood that one would directly optimize in the oracle setting where true counterfactual pairs were available. Second, as we show in Appendix E, this procedure is _consistent_, in the sense that optimizing the loss function in the limit of infinite data will recover the generative process (under suitable conditions). This consistency result also guarantees that the learned model can consistently predict the effect of multi-node interventions; see Appendix E.2.

## 6 Experiments

We now demonstrate our method on a biological dataset. We use the large-scale Perturb-seq study from . After pre-processing, the data contains 8,907 unperturbed cells (observational dataset \(\)) and 99,590 perturbed cells. The perturbed cells underwent CRISPR activation  targeting one or two out of 105 genes (interventional datasets \(^{1}\),...,\(^{K}\), \(K=217\)). CRISPR activation experiments modulate the expression of their target genes, which we model as a shift intervention. Each interventional dataset comprises 50 to 2,000 cells. Each cell is represented as a 5,000-dimensional vector (observed variable \(X\)) measuring the expressions of 5,000 highly variable genes.

To test our model, we set the latent dimension \(p=105\), corresponding to the total number of targeted genes. During training, we include all the unperturbed cells from \(\) and the perturbed cells from the single-node interventional datasets \(^{1},...,^{105}\) that target one gene. For each single-node interventional dataset with over 800 cells, we randomly extract 96 cells and reserve these for testing. The double-node interventions (112 distributions \(^{106},...,^{217}\)) targeting two genes are entirely reserved for testing. The following results summarize the model with the best training performance. Extended evaluations and detailed implementation can be found in Appendix F and G. In additional, we also provide ablation studies on biological data and a simple simulation study in Appendix H.

**Single-node Interventional Distributions.** To study the generative capacity of our model for interventions on single genes, we produce \(96\) samples for each single-node intervention with over 800 cells (14 interventions). We compare these against the left-out \(96\) cells of the corresponding distributions. Figure 5 illustrates this for \(3\) example genes in \(2\) dimensions using UMAP  with all other cells in the dataset as background (labeled by 'NA'). Our model is able to discover subpopulations of the interventional distributions (e.g., for KLF1, the generated samples are concentrated in the middle left corner). We provide a quantitative evaluation for all \(105\) single-node interventions in Figure 6. The model is able to obtain close to perfect \(R^{2}\) (on average 0.99 over all genes and 0.95 over most differentially expressed genes).

**Double-node Interventional Distributions.** Next, we analyze the generalization capabilities of our model to the \(112\) double-node interventions. Despite _never_ observing any cells from these interventions during training, we obtain reasonable \(R^{2}\) values (on average 0.98 over all genes and

Figure 5: **The distribution of generated samples mirrors the distribution of actual samples. Samples are visualized using UMAP. _Left:_ Samples from the 14 single target-node interventions with more than \(800\) cells. _Middle-Right:_ Samples for target genes SET, CEBPE, and KLF1.**

0.88 over most differentially expressed genes). However, when looking at the generated samples for individual pairs of interventions, it is apparent that our model performs well on many pairs, but recovers different subpopulations for some pairs (examples shown in Figure 13 in Appendix G). The wrongly predicted intervention pairs could indicate that the two target genes act non-additively, which needs to be further evaluated and is of independent interest for biological research [22; 44].

**Structure Learning.** Lastly, we examine the learned DAG between the intervention targets. Specifically, this corresponds to a learned gene regulatory network between the learned programs of the target genes. For this, we reduce \(p\) from \(105\) until the learned latent targets of \(^{1},...,D^{105}\) cover all \(p\) latent nodes. This results in \(p=7\) groups of genes, where genes are grouped by their learned latent nodes. We then run our algorithm with fixed \(p=7\) multiple times and take the learned DAG with the least number of edges. This DAG over the groups of targeted genes is shown with example genes in Figure 7 (left). This learned structure is in accordance with previous findings. For example, we successfully identified the edges DUSP9\(\)MAPK1 and DUSP9\(\)ETS2, which is validated in  (see their Fig. 5). We also show the interventional distributions targeting these example genes in Figure 7 (right). Among these, MAPK1 and ETS2 correspond to clusters that are heavily overlapping, which explains why the model maps both distributions to the same latent node.

## 7 Conclusion

We derived identifiability results for causal disentanglement from single-node interventions, and presented an autoencoding variational Bayes framework to estimate the latent causal representation from interventional samples. Identification of the latent causal structure and generalization to multi-node interventions was demonstrated experimentally on genetic data.

**Limitations and Future Work.** This paper has various limitations that may be useful to address in future work. We provide an overview here, where an extended discussion can be found in Appendix I. In addition, we also provide a brief summary of concurrent related works in Appendix I.

First, we focused on the setting where single-node interventions on each latent node are available. This is overly optimistic for example in the case of chemical perturbations where the available drugs could all have multiple targets. The VAE framework can still be applied in such settings; however, its theoretical guarantees are subject to further investigations. The key techniques in our proofs can be generalized to the multi-node setting, but further assumptions on the set of interventions needed for identifiability are required. Second, we have focused on the infinite data regime for analyzing identifiability; given that obtaining interventional samples tends to be expensive in practice, there is much room for further investigations in terms of sample complexity.

Figure 6: **Our model accurately predicts the effect of single-node interventions. ‘All genes’ indicates measurements using the entire \(5000\)-dimensional vectors; ‘DE genes’ indicates measurements using the \(20\)-dimensional vectors corresponding to the top \(20\) most differentially expressed genes.**

Figure 7: **Structure learning on the biological dataset.**_Left_: learned DAG between target genes (colors indicate edge weights). _Right_: UMAP visualization of the distributions.