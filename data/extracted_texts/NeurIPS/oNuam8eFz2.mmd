# Particle-based Variational Inference with Generalized Wasserstein Gradient Flow

Ziheng Cheng

School of Mathematical Sciences

Peking University

alex-czh@stu.pku.edu.cn

&Shiyue Zhang

School of Mathematical Sciences

Peking University

zhangshiyue@stu.pku.edu.cn

Contributed equally to this work.Corresponding author.

Longlin Yu

School of Mathematical Sciences

Peking University

lyu@pku.edu.cn

&Cheng Zhang

School of Mathematical Sciences and Center for Statistical Science

Peking University

chengzhang@math.pku.edu.cn

###### Abstract

Particle-based variational inference methods (ParVIs) such as Stein variational gradient descent (SVGD) update the particles based on the kernelized Wasserstein gradient flow for the Kullback-Leibler (KL) divergence. However, the design of kernels is often non-trivial and can be restrictive for the flexibility of the method. Recent works show that functional gradient flow approximations with quadratic form regularization terms can improve performance. In this paper, we propose a ParVI framework, called generalized Wasserstein gradient descent (GWG), based on a generalized Wasserstein gradient flow of the KL divergence, which can be viewed as a functional gradient method with a broader class of regularizers induced by convex functions. We show that GWG exhibits strong convergence guarantees. We also provide an adaptive version that automatically chooses Wasserstein metric to accelerate convergence. In experiments, we demonstrate the effectiveness and efficiency of the proposed framework on both simulated and real data problems.

## 1 Introduction

Bayesian inference is an important method in modern machine learning that provides powerful tools for modeling complex data and reasoning under uncertainty. The core of Bayesian inference is to estimate the posterior distribution given the data. As the posterior distribution is intractable in general, various approximation approaches have been developed, of which variational inference and Markov Chain Monte Carlo are two typical examples. By reformulating the inference problem into an optimization problem, variational inference (VI) seeks an approximation within a certain family of distributions that minimizes the Kullback-Leibler (KL) divergence to the posterior (Jordan et al., 1999; Wainwright and Jordan, 2008; Blei et al., 2016). Equipped with efficient optimization algorithms, VI allows fast training and easy scaling to large datasets. However, the construction of approximatingdistributions can be restrictive which may lead to poor approximation. Markov chain Monte Carlo (MCMC) methods simulate a Markov chain to directly draw samples from the posterior (Duane et al., 1987; Neal, 2011; Welling and Teh, 2011; Chen et al., 2014). While being asymptotically unbiased, MCMC often takes a long time to converge, and it is also difficult to access the convergence.

Recently, particle based variational inference methods (ParVIs) have been proposed that tend to combine the best of both worlds (Liu and Wang, 2016; Chen et al., 2018; Liu et al., 2019; di Langosco et al., 2021; Fan et al., 2022; Alvarez-Melis et al., 2022). In ParVIs, the approximating distribution is represented as a set of particles, which are iteratively updated by minimizing the KL divergence to the posterior. This non-parametric nature significantly improves the flexibility of ParVIs upon classical VIs, and the interaction between particles also makes ParVIs more particle-efficient than MCMCs. The most well-known particle based VI method is Stein Variational Gradient Descent (SVGD) (Liu and Wang, 2016). It updates the particles by simulating the gradient flows of the KL divergence on a certain kernel related distribution space, where the gradient flows have a tractable form (Liu, 2017; Chewi et al., 2020). However, SVGD relies on the choice of an appropriate kernel function whose design is highly non-trivial and hence could limit the flexibility of the method. Moreover, the required computation of the kernel matrix scales quadratically with the number of particles, which makes it costly to use a large number of particles.

Instead of using kernel induced functional gradients, many attempts have been made to expand the function class for gradient flow approximation (Hu et al., 2018; Grathwohl et al., 2020; di Langosco et al., 2021; Dong et al., 2023). By leveraging the more general neural networks as the function class together with more general regularizers, these approaches have shown improved performance over vanilla SVGD while not requiring expensive kernel computation. However, these methods only use quadratic form regularizers where either the Wasserstein gradient flow or its preconditioned variant is recovered.

In this work, we propose a ParVI method based on a general formulation of minimizing movement scheme in Wasserstein space, which corresponds to a generalized Wasserstein gradient flow of KL divergence. Using Legendre-Fenchel transformation, our method can also be viewed as a functional gradient method with a more general class of regularizers which include the previously used quadratic forms as special cases. We provide a theoretical convergence guarantee of ParVIs with neural-net-estimated vector field for generalized Wasserstein gradient flow, which to the best of our knowledge, has not been established yet. Perhaps surprisingly, our results show that assuming reasonably accurate vector field estimates, the iteration complexity of ParVIs matches the traditional Langevin Monte Carlo under weaker assumptions on the target distribution. As an extension, we also propose an algorithm that can adaptively adjust the Wasserstein metric to accelerate convergence. Extensive numerical experiments on both simulated and real data sets are conducted to demonstrate the efficiency of our method over existing ones.

## 2 Background

Notations.Throughout this paper, we use \(x\) to denote particle samples in \(^{d}\). Let \((^{d})\) denote all the probability distributions on \(^{d}\) that are absolute continuous with respect to the Lebesgue measure. We do not distinguish a probabilistic measure with its density function. For \(x^{d}\) and \(p>1\), \(\|x\|_{p}:=(|x_{1}|^{p}++|x_{d}|^{p})^{1/p}\) stands for the \(_{p}\)-norm. The Holder conjugate of \(p\) is denoted by \(q:=p/(p-1)\). Notation \(g^{*}()\) denotes the Legendre transform of a convex function \(g()\) on \(^{d}\).

### Particle-based Variational Inference

Let \((^{d})\) be the target distribution we wish to sample from. We can cast the problem of sampling as an optimization problem: to construct a distribution \(^{*}\) that minimizes the KL divergence

\[^{*}:=_{^{}}D_{}(\|),\] (1)

where \(^{}(^{d})\) is the variational family. Particle-based variational inference methods (ParVIs) is a class of VI methods where \(^{}\) is represented as a set of particles. Assume the current particle distribution is \(\), then it holds that

\[_{=0}D_{}((id+ v)_{\#} \|)=-_{},v.\] (2)ParVIs aim to find the optimal vector field \(v\) that minimizes (2) in certain function class. For example, SVGD (Liu and Wang, 2016) restricts \(v\) in the unit ball of an reproducing kernel Hilbert space (RKHS) which has a closed-form solution by kernel trick. Meanwhile, Hu et al. (2018); Grathwohl et al. (2020); di Langosco et al. (2021); Dong et al. (2023) consider a more general class of functions for \(v\), i.e., neural networks, and minimize (2) with some quadratic form regularizers.

### Minimizing Movement Scheme in Wasserstein Space

Assume the cost function \(c(,):^{d}^{d}\) is continuous and bounded from below. Define the optimal transportation cost between two probabilistic measure \(,\) as:

\[W_{c}(,):=_{(,)} c(x,y)d.\] (3)

Specifically, if \(c(x,y)=\|x-y\|_{p}^{p}\) for some \(p>1\), then we get the \(p\)-th power of Wasserstein-p distance \(W_{p}(,)\). Jordan et al. (1998) consider a minimizing movement scheme (MMS) under \(W_{2}\) metric. Given the current distribution \(_{kh}\), the distribution for next step is determined by

\[_{(k+1)h}:=*{arg\,min}_{_{2}(^{d}) }D_{}(\|)+W_{2}^{2}(,_{kh}).\] (4)

When the step size \(h 0\), \(\{_{kh}\}_{k 0}\) converges to the solution of the Fokker-Planck equation

\[_{t}_{t}+(_{t})=_{t}.\] (5)

Therefore, MMS corresponds to the deterministic dynamics

\[dx_{t}=v_{t}dt,\;v_{t}=-_{t},\] (6)

where \(_{t}\) is the law of \(x_{t}\). (6) is also known as the gradient flow of KL divergence under \(W_{2}\) metric, which we refer to as \(L_{2}\)-GF (Ambrosio et al., 2005). Note that the Langevin dynamics \(dx_{t}=(x_{t})dt+dB_{t}\) (\(B_{t}\) is the Brownian motion) reproduces the same distribution curve \(\{_{t}\}_{t 0}\) and thus also corresponds to the Wasserstein gradient flow (Jordan et al., 1998).

## 3 Proposed Methods

### Minimizing Movement Scheme with A General Metric

We start with generalizing the scope of the aforementioned MMS in Section 2.2 which is under \(W_{2}\) metric.

**Definition 3.1** (Young function).: A strictly convex function \(g\) on \(^{d}\) is called Young function if \(g(x)=g(-x),g(0)=0\), and for any fixed \(z^{d}\{0\}\), \(hg()\), as \(h 0\).

**Theorem 1**.: _Given a continuously differentiable Young function \(g\) and step size \(h>0\), define cost function \(c_{h}(x,y)=g()h\). Suppose that \(,_{kh}_{c_{h}}(^{d}):=\{( ^{d}):_{}[g()]<\}\). Under some mild conditions of \(g\) (see details in Proposition A.1), \(_{c_{h}}(^{d})\) is a Wasserstein space equipped with Wasserstein distance. Consider MMS under transportation cost \(W_{c_{h}}\):_

\[_{(k+1)h}:=*{arg\,min}_{_{c_{h}}(^ {d})}D_{}(\|)+W_{c_{h}}(,_{kh}).\] (7)

_Denote the optimal transportation map under \(W_{c_{h}}\) from \(_{(k+1)h}\) to \(_{kh}\) by \(T_{k}()\). Then we have_

\[(x)-x}{h}=- g^{*}((x)-_{(k+1) h}(x)).\] (8)

Please refer to Appendix A for full statements and proofs. Informally, \(_{(k+1)h}_{kh}\) for small step size \(h\)(Santambrogio, 2017). Further note that \((x)-x}{h}\) is the optimal velocity field associated with the transport from \(_{(k+1)h}\) to \(_{kh}\) (and not vice versa). If step size \(h 0\), then following Jordan et al. (1998), we can recover the dynamics in continuous time:

\[dx_{t}=v_{t}dt,\;v_{t}= g^{*}(-_{t}).\] (9)We call (9) the generalized Wasserstein gradient (GWG) flow. If we set \(g()=\|\|_{2}^{2}\) or any positive definite quadratic form \(g()=\|\|_{H}^{2}\), then (9) reduces to \(L_{2}\)-GF (6) or its preconditioned version (Dong et al., 2023) respectively.

### Faster Descent of KL Divergence

It turns out that we can leverage the general formulation (9) to explore the underlying structure of different probability spaces and further utilize this geometric structure to accelerate sampling. More specifically, we consider \(g()=\|\|_{p}^{p}\) for some \(p>1\) and then \(g^{*}()=\|\|_{q}^{q}\). Note that if the particles move along the vector field \(v_{t}= g^{*}(})\), then the descent rate of \(D_{}(_{t}\|)\) is

\[_{t}D_{}(_{t}\|)=-_{_{t}} }_{q}^{q}.\] (10)

If we choose \(q\) such that \(_{_{t}}} _{q}^{q}\) is large, then \(D_{}(_{t}\|)\) decreases faster and the sampling process can be accelerated. We use the following example to further illustrate our idea. Please refer to Appendix B for detailed analysis.

**Example 1**.: _Let \(=(-m,1)+(m,1)\) and \(=(-m,1)+(m,1)\). Then for any \(m,q 1\), the following holds:_

\[()^{q}(-}{2})_{} _{q}^{q}(4m)^{q}(-}{2}).\] (11)

_However, the KL divergence between \(\) and \(\) is large: \(D_{}(\|)}\)._

Suppose the target distribution is \(\) and we run ParVI with current particle distribution \(\). We can expect that, if simply using \(L_{2}\) regularization, _i.e._, \(q=2\), then for very large \(m\), the score divergence is small and thus the decay of KL divergence is extremely slow. However, \(D_{}(\|)\) is still large, indicating that it would take a long time for the dynamics to converge to the target. But if we set \(q\) much larger, then the derivative of KL divergence would get larger and the convergence can be accelerated.

### Algorithm

The forward-Euler discretization of the dynamics (9) is

\[x_{(k+1)h}=x_{kh}+ g^{*}(}(x_{kh}) )h.\] (12)

However, since the score of current particle distribution \(_{kh}\) is generally unknown, we need a method to efficiently estimate the GWG direction \( g^{*}(})\). Given the distribution of current particles \(\), by the definition of convex conjugate, we have

\[ g^{*}()=_{v}_{}[( ,v)-g(v)].\]

If we parameterize \(v\) as a neural network \(f_{w}\) with \(w\), then we can maximize the following objective with respect to \(w\):

\[(w):&=_{}[( ,f_{w})-g(f_{w})]\\ &=_{}[()^{T}f_{w}+ f_{w}-g( f_{w})]\] (13)Here the second equation is by _Stein's identity_ (we assume \(\) vanishes at infinity). This way, the gradient of \((w)\) can be estimated via Monte Carlo methods given the current particles. We summarize the procedures in Algorithm 1.

```
0: Unnormalized target distribution \(\), initial particles \(\{x_{0}^{i}\}_{i=1}^{n}\), initial parameter \(w_{0}\), iteration number \(N\), \(N^{}\), particle step size \(h\), parameter step size \(\) for\(k=0,,N-1\)do  Assign \(w_{k}^{0}=w_{k}\) for\(t=0,,N^{}-1\)do  Compute \[}(w)=_{i=1}^{n}(x_{k}^{i})^{T }f_{w}(x_{k}^{i})+ f_{w}(x_{k}^{i})-g(f_{w}(x_{k}^{i}))\] (14)  Update \(w_{k}^{t+1}=w_{k}^{t}+_{w}}(w_{k}^{t})\) endfor  Update \(w_{k+1}=w_{k}^{N^{}}\)  Update particles \(x_{k+1}^{i}=x_{k}^{i}+hf_{w_{k+1}}(x_{k}^{i})\) for \(i=1,,n\) endfor return Particles \(\{x_{N}^{i}\}_{i=1}^{n}\) ```

**Algorithm 1** GWG: Generalized Wasserstein Gradient Flow

The exact computation of the divergence term \(_{x} f_{w}(x)\) needs \((d)\) times back-propagation, where \(d\) is the dimension of \(x\). In order to reduce computation cost, we refer to Hutchinson's estimator (Hutchinson, 1989), _i.e._,

\[_{i=1}^{n} f_{w}(x_{k}^{i})_{i =1}^{n}_{i}^{T}(f_{w}(x_{k}^{i})_{i}),\] (15)

where \(_{i}^{d}\) are independent random vectors satisfying \(_{i}_{i}^{T}=I_{d}\). This is still an unbiased estimator but only needs \((1)\) times back-propagation.

## 4 Convergence Analysis without Isoperimetry

In this section, we state our main theoretical results of Algorithm 1. Consider the discrete dynamics:

\[X_{(k+1)h}=X_{kh}+v_{k}(X_{kh})h,\] (16)

where \(v_{k}\) is the neural-net-estimated GWG at time \(kh\). Define the interpolation process

\[X_{t}=X_{kh}+(t-kh)v_{k}(X_{kh}),t[kh,(k+1)h],\] (17)

and let \(_{t}\) denote the law of \(X_{t}\). Note that here we do not assume isoperimetry of target distribution \(\) (_e.g._, _log-Sobolev inequality_) and hence establish the convergence of dynamics in terms of score divergence, following the framework of non-log-concave sampling (Balasubramanian et al., 2022).

We first make some basic assumptions. For simplicity, only two types of Young function \(g^{*}\) are considered here, which are also the most common choices.

**Assumption 1**.: \(g^{*}()=\|\|_{q}^{q}\) _for some \(q>1\). And for any \(k\), \(_{_{kh}}v_{k}- g^{*}(})_{p}^{p}_{k}\)._

**Assumption 2**.: \(g^{*}()\) _is \(\)-strongly convex and \(\)-smooth. Define \(:=\). And for any \(k\), \(_{_{kh}}v_{k}- g^{*}(})_{2}^{2}_{k}\)._

The two assumptions above ensure the estimation accuracy of neural nets. Note that the preconditioned quadratic form in Dong et al. (2023) is included in Assumption 2. Although the estimation error is not exactly the training objective used in Algorithm 1,the following proposition shows the equivalence between them in some sense.

**Proposition 2**.: _Suppose \(g()=\|\|_{p}^{p}\) for some \(p>1\). Given current particle distribution \(\), we can define the training loss \(_{}(v):=_{}[,v-g(v)]\). The maximizer is \(v^{*}= g^{*}()\) and the maximum value is \(_{}^{*}:=_{}(v^{*})<\). For any arbitrarily small \(_{1}>0\), there exists \(_{2}:=_{2}(_{1},p)<\), such that_

\[_{}v- g^{*}() _{p}^{p}_{1}_{}^{*}+ _{2}[_{}^{*}-_{}(v)].\]

_Besides, if \(p 2\), \(_{1}\) can be \(0\) while \(_{2}\) is still finite._

Similar results also hold if \(g\) satisfies Assumption 2 since it is equivalent to the case when \(p=2\). Additionally, we expect some properties of the estimated vector fields.

**Assumption 3** (Smoothness of neural nets).: _For any \(k\), \(v_{k}()\) is twice differentiable. For any \(p>1\), \(G_{p}:=_{x,y}(x)-v_{k}(y)\|_{p}}{\|x-y\|_{p}}<\), \(M_{p}:=_{x,z}_{ 0^{+}}(x+ z)-  v_{k}(x)\|_{op}}{\|z\|_{p}}<\)._

Note that here we do not assume the smoothness of potential \(\) explicitly. But informally, \(G_{p}\) and \(M_{p}\) correspond to the Lipschitz constant of the gradient and the Hessian of \(\), respectively.

Let \(_{Nh}:=_{0}^{Nh}_{t}dt\) and \(K_{0}:=D_{}(_{0}\|)\). Now we present our main results.

**Theorem 3** (Full version see Theorem D.9).: _Under Assumption 1, 3, the following bound holds with proper step size \(h\):_

\[_{_{Nh}}_{Nh }}_{q}^{q}=}((K_{0}d}{N})^{ }+K_{0}d}{N}+^{N-1}_{k}}{N} ).\] (18)

_Here \(}()\) hides all the constant factors that only depend on \(q\)._

**Theorem 4** (Full version see Theorem D.10).: _Under Assumption 2, 3 with \(=1\), the following bound holds with proper step size \(h\):_

\[_{_{Nh}}_{Nh }}_{2}^{2}=((K_{0}d}{N})^{ }+K_{0}(d+)}{N}+^{N-1} _{k}}{N}).\] (19)

The proofs in this section are deferred to Appendix D. To interpret our results, suppose \(g^{*}()=\|\|_{q}^{q}\) and \((}{G_{2}})^{q}\). If the neural net \(v_{k}()\) can approximate \( g^{*}(})\) accurately (_i.e._, \(_{k}\)), then to obtain a probabilistic measure \(\) such that \(_{}_{q}^{ q}\), the iteration complexity is \(}(M_{p}K_{0}d^{-(1+)})\). If we further let \(q=p=2\), the complexity is \((K_{0}d^{-})\), which matches the complexity of Langevin Monte Carlo (LMC) under the Hessian smoothness and the growth order assumption (Balasubramanian et al., 2022). However, noticing that Assumption 3 is similar to Hessian smoothness informally, we can obtain this rate without additional assumption on target distribution. This suggests the potential benefits of particle-based methods.

In addition, our formulation allows a wider range of choices of Young function, including \(\|\|_{p}^{p}\) and the preconditioned quadratic form (Dong et al., 2023). This provides wider options of convergence metrics. We refer the readers to Appendix D.4 for more discussions.

## 5 Extensions: Adaptive Generalized Wasserstein Gradient Flow

The GWG framework also allows adaption of the Young function \(g\), instead of a fixed one. Similar ideas are also presented in Wang et al. (2018). In this section, we consider a special Young function class \(\{\|\|_{p}^{p}:p>1\}\) and propose a procedure that adaptively chooses \(p\) to accelerate convergence.

Consider the continuous time dynamics \(dx_{t}=f_{t}(x_{t})dt\) and denote the distribution of particles at time \(t\) as \(_{t}\), we have the following proposition.

**Proposition 5**.: _For \(g()=\|\|_{p}^{p}\), the derivative of KL divergence has an upper bound:_

\[_{t}D_{}(_{t}\|)-_{_{t}} }_{q}^{q}+ {p}_{_{t}} g^{*}(})-f_{t}_{p}^{p}.\] (20)

The proof is in Appendix E. If the neural network \(f_{t}\) can approximate the objective well, _i.e._, \(f_{t} g^{*}(})\), then informally we can omit the second term and thus

\[_{t}D_{}(_{t}\|)-_{_ {t}}}_{q}^{q} -_{_{t}}\|f_{t}\|_{p}^{p}=:-A(p).\] (21)

In order to let KL divergence decrease faster, we can choose \(p\) such that \(A(p)\) is larger. This leads to a simple adaptive procedure that updates \(p\) by gradient ascent _w.r.t._\(A(p)\). In practice, the adjustment of \(p\) is delicate and would cause numerical instability if \(p\) becomes excessively small or large. Therefore it is necessary to clip \(p\) within a reasonable range. We call this adaptive version of GWG, Ada-GWG. The whole training procedure of Ada-GWG is shown in Algorithm 2. Note that (22) can be replaced with Hutchinson's estimator (15) to improve computational efficiency as before.

```
0: unnormalized target distribution \(\), initial particles \(\{x_{0}^{i}\}_{i=1}^{n}\), initial parameter \(w_{0}\), iteration number \(N,N^{}\), step size \(h,,\), lower and upper bounds on \(p\): \(lb,ub\) for\(k=0,,N-1\)do  Assign \(w_{0}^{0}=w_{k}\) for\(t=0,,N^{}-1\)do  Compute  Update \(w_{k+1}^{t}=w_{k}^{t}+_{w}}(w_{k}^{t})\) endfor  Update \(w_{k+1}^{t}=w_{k}^{N^{}}\)  Compute \((p_{k})=_{i=1}^{n}}\|f_{w_{k+1}}(x_{k} ^{i})\|_{p_{k}}^{p_{k}}\)  Update \(p_{k+1}=(p_{k}+(p_{k}),lb,ub)\)  Update particles \(x_{k+1}^{i}=x_{k}^{i}+hf_{w_{k+1}}(x_{k}^{i})\) for \(i=1,,n\) endfor returnParticles \(\{x_{N}^{i}\}_{i=1}^{n}\) ```

**Algorithm 2** Ada-GWG: Adaptive Generalized Wasserstein Gradient Flow

## 6 Numerical Experiments

In this section, we compare GWG and Ada-GWG with other ParVI methods including SVGD (Liu and Wang, 2016), \(L_{2}\)-GF (di Langosco et al., 2021) and PFG (Dong et al., 2023) on both synthetic and real data problems. In BNN experiments, we also test stochastic gradient Langevin dynamics (SGLD). For Ada-GWG, the exponent \(p\) is clipped between \(1.1\) and \(4.0\) unless otherwise specified. Throughout this section, we choose \(f_{w}\) to be a neural network with \(2\) hidden layers and the initial particle distribution is \((0,I_{d})\). We refer the readers to Appendix F for more detailed setups of our experiments. The code is available at https://github.com/Alexczh1/GWG.

### Gaussian Mixture

Our first example is on a multi-mode Gaussian mixture distribution. Following Dong et al. (2023), we consider the 10-cluster Gaussian mixture where the variances of the mixture components are all 0.1. The number of particles is 1000. Figure 1 shows the scatter plots of the sampled particles at different numbers of iterations. We see that on this simple toy example, PFG performs similarly to the standard \(L_{2}\)-GF which does not involve the preconditioner, while Ada-GWG with the initial \(p_{0}=2\) significantly accelerates the convergence compared to these two baseline methods. Please refer to appendix for further quantitative comparisons.

### Monomial Gamma

To illustrate the effectiveness and efficiency of the adaptive method compared to the non-adaptive counterparts, we consider the heavy tailed Monomial Gamma distribution where the target \((-0.3(|x_{1}|^{0.9}+|x_{2}|^{0.9}))\).

We test GWG and Ada-GWG with different choices of the initial values of \(p\). The number of particles is \(1000\). Figure 2 demonstrates the KL divergence of different methods against the number of iterations. The dotted line represents GWG with fixed \(p\), while the solid line represents the corresponding Ada-GWG that starts from the same \(p\) at initialization.

We see that the adaptive method outperforms the non-adaptive counterpart consistently. Moreover, Ada-GWG can automatically learn the appropriate value of \(p\) especially when the initial values of \(p\) is set inappropriately. For example, in our case, a relatively small value of \(p=1.5\) would be inappropriate (the dotted green line) for GWG, while Ada-GWG with the same initial value of \(p=1.5\) is able to provide much better approximation by automatically adjusting \(p\) during runtime. Consequently, Ada-GWG can exhibit greater robustness when determining the initial value of \(p\).

### Conditioned Diffusion

The conditioned diffusion example is a high-dimensional model arising from a Langevin SDE, with state \(u:\) and dynamics given by

\[du_{t}=)}{1+u^{2}}dt+dx_{t}, u_{0}=0,\] (23)

where \(x=(x_{t})_{t 0}\) is a standard Brownian motion.

This system is commonly used in molecular dynamics to represent the motion of a particle with negligible mass trapped in an energy potential with thermal fluctuations represented by the Brownian forcing Detommaso et al. (2018); Cui et al. (2016). Given the perturbed observations \(y\), the goal

Figure 1: Comparison of sampled particles at different numbers of iterations. **Upper**: \(L_{2}\)-GF. **Middle**: PFG. **Lower**: Ada-GWG with \(p_{0}=2\).

is to infer the posterior of the driving process \(p(x|y)\). The forward operator is defined by \((x)=(u_{t_{1}},,u_{t_{20}})^{20}\), where \(t_{i}=0.05i\). This is achieved by discretizing the above SDE (23) using an Euler-Maruyama scheme with step size \( t=0.01\); therefore the dimensionality of the problem is \(100\). The noisy observations are obtained as \(y=(x_{})+^{20}\), where \(x_{}\) is a Brownian motion path and \((0,^{2}I)\) with \(=0.1\). The prior is given by the Brownian motion \(x=(x_{t})_{t 0}\).

We test three algorithms: PFG, Ada-GWG, and SVGD, with \(n=1000\) particles. To obtain the ground truth posterior, we run LMC with \(1000\) particles in parallel, using a small step size \(h=10^{-4}\) for \(10000\) iterations. Figure 3 reports the logarithmic Maximum Mean Discrepancy (MMD) curves against iterations. We observe that Ada-GWG provides best performance compared to the other methods.

### Bayesian Neural Networks

We compare our algorithm with SGLD and SVGD variants on Bayesian neural networks (BNN). Following Liu and Wang (2016), we conduct the two-layer network with 50 hidden units and ReLU activation function, and we use a \((1,0.1)\) prior for the inverse covariances. The datasets are all randomly partitioned into 90% for training and 10% for testing. The mini-batch size is 100 except for Concrete on which we use 400. The particle size is 100 and the results are averaged over 10 random trials. Table 1 shows the average test RMSE and NLL and their standard deviation. We see that Ada-GWG can achieve comparable or better results than the other methods. And the adaptive method consistently improves over \(L_{2}\)-GF. Figure 4 shows the test RMSE against iterations of different methods on the Boston dataset. We can see that for this specific task, setting \(p=3\) produces better results than when \(p=2\). Although \(L_{2}\)-GF (_i.e._, GWG with \(p=2\)) is sub-optimal, our adaptive method (_i.e._, Ada-GWG with \(p_{0}=2\)) makes significant improvements and demonstrates comparable performance to the optimal choice of \(p=3\). This suggests that our adaptive method is robust even if the initial exponent choice is not ideal. More comparisons of convergence results and hyperparameter tuning details can be found in the appendix.

## 7 Conclusion

We introduced a new ParVI method, called GWG, which corresponds to a generalized Wasserstein gradient flow of KL divergence. We show that our method has strong convergence guarantees in discrete time setting. We also propose an adaptive version, called Ada-GWG, that can automatically

   &  &  \\
**Dataset** & **SGLD** & **SVGD** & \(L_{2}\)**-GF** & **Ada-GWG** & **SGLD** & **SVGD** & \(L_{2}\)**-GF** & **Ada-GWG** \\  Boston & \(3.011_{+0.15}\) & \(2.774_{+0.08}\) & \(3.072_{+0.10}\) & \(_{+0.08}\) & \(2.496_{+0.03}\) & \(2.444_{+0.02}\) & \(2.547_{+0.14}\) & \(_{+0.02}\) \\ Concrete & \(5.583_{+0.25}\) & \(4.436_{+0.08}\) & \(4.343_{+0.11}\) & \(_{+0.10}\) & \(3.184_{+0.04}\) & \(3.035_{+0.02}\) & \(3.053_{+0.03}\) & \(_{+0.02}\) \\ Power & \(4.089_{+0.11}\) & \(3.972_{+0.02}\) & \(4.014_{+0.02}\) & \(_{+0.01}\) & \(2.840_{+0.02}\) & \(2.809_{+0.01}\) & \(2.824_{+0.01}\) & \(_{+0.01}\) \\ Winewhite & \(0.077_{+0.01}\) & \(0.664_{+0.01}\) & \(0.666_{+0.00}\) & \(0.660_{+0.01}\) & \(1.033_{+0.01}\) & \(1.014_{+0.01}\) & \(1.015_{+0.01}\) & \(1.006_{+0.01}\) \\ Winered & \(0.600_{+0.01}\) & \(0.579_{+0.01}\) & \(0.581_{+0.01}\) & \(_{+0.01}\) & \(0.910_{+0.01}\) & \(0.887_{+0.02}\) & \(0.860_{+0.02}\) & \(_{+0.02}\) \\ protein & \(_{+0.04}\) & \(4.779_{+0.03}\) & \(4.867_{+0.00}\) & \(4.686_{+0.02}\) & \(_{+0.01}\) & \(2.984_{+0.01}\) & \(3.003_{+0.00}\) & \(2.964_{+0.00}\) \\  

Table 1: Averaged test RMSE and test negative log-likelihood of Bayesian Neural Networks on several UCI datasets. The results are averaged from 10 independent runs.

adjust the Wasserstein metric to accelerate convergence. Extensive numerical results showed that Ada-GWG outperforms conventional ParVI methods.