# L4GM: Large 4D Gaussian Reconstruction Model

Jiawei Ren1,5 Kevin Xie1,2 Ashkan Mirzaei1,2 Hanxue Liang1,3

Xiaohui Zeng1,2 Karsten Kreis1 Ziwei Liu5 Antonio Torralba4

Sanja Fidler1,2 Seung Wook Kim1 Huan Ling1,2

###### Abstract

We present **L4GM**, the first 4D Large Reconstruction Model that produces animated objects from a single-view video input - in a single feed-forward pass that takes only a second. Key to our success is a novel dataset of multiview videos containing curated, rendered animated objects from _Objaverse_. This dataset depicts 44K diverse objects with 110K animations rendered in 48 viewpoints, resulting in 12M videos with a total of 300M frames. We keep our L4GM simple for scalability and build directly on top of LGM , a pretrained 3D Large Reconstruction Model that outputs 3D Gaussian ellipsoids from multiview image input. L4GM outputs a per-frame 3D Gaussian Splitting representation from video frames sampled at a low fps and then upsamples the representation to a higher fps to achieve temporal smoothness. We add temporal self-attention layers to the base LGM to help it learn consistency across time, and utilize a per-timesp multiview rendering loss to train the model. The representation is upsampled to a higher framerate by training an interpolation model which produces intermediate 3D Gaussian representations. We showcase that L4GM that is only trained on synthetic data generalizes well on in-the-wild videos, producing high quality animated 3D assets.

## 1 Introduction

Animated 3D assets are essential in bringing 3D virtual worlds to life. However, these animations are time consuming to create as the procedure involves rigging and skinning of objects, and crafting keyframes of the animation - all with minimal automation in tooling. The ability to generate animated 3D assets from widely available monocular videos or simply from text would be a desirable capability for this application. This is the goal of our work. Building more advanced 4D content editing tooling, the ultimate goal of this line of research, is out of scope for this work.

Figure 1: **L4GM** generates 4D objects from in-the-wild input videos.

Past work on automatically generating animated 3D objects, which we refer to as 4D modeling in this paper, falls into different categories. The first line of work aims to faithfully reconstruct 4D objects from multiview video data, and oftentimes requires many views across time to achieve high quality [29; 5; 36]. Such data is expensive to collect which limits applicability. Another line of work instead relies on the power of video generative models. Most commonly, the video score distillation technique is used which optimizes a 4D representation, for example a 3D deformation field, by receiving iterative feedback from the video generative model. Score distillation is known to be fragile (sensitive to prompts), and time consuming (hours per prompt) as oftentimes many iterations are needed to achieve high quality results [47; 26; 72; 1].

Recently, a promising method emerged for the task of single-image 3D reconstruction. This method leverages large scale synthetic and real datasets to train a large transformer model, dubbed 3D Large Reconstruction Model (LRM) [20; 19], to generate 3D objects represented as neural radiance fields from a single image in a single forward pass - thus being extremely fast. We build on top of this idea to achieve fast and high quality 4D reconstruction.

We present **L4GM**, the first 4D Large Reconstruction Model (Figure 2), which aims to reconstruct a sequence of 3D Gaussians  from a monocular video, in a feed-forward fashion. Key to our method, is a new large-scale dataset containing 12 million multiview videos of rendered animated 3D objects from _Objavverse 1.0_. Our model builds on top of LGM , a pre-trained 3D Large Reconstruction Model that is trained to output 3D Gaussians from multiview images. We extend it to take a sequence of frames as input and produce a 3D Gaussian representation for each frame. We add temporal self-attention layers between the frames in order to learn a temporally consistent 3D representation. We upsample the output to a higher fps by training an interpolation model that takes two consecutive 3D Gaussian representations and outputs a fixed set of in-betweens. L4GM is trained on our multiview video dataset with per-timestep image reconstruction losses by rendering the Gaussians in multiple views.

We showcase that although only trained on synthetic data, the model generalizes well to in-the-wild videos, e.g., videos generated by Sora  and real-world videos in AcitivityNet . On the video-to-4D benchmark, we achieve state-of-the-art quality while being 100 to 1,000 times faster than other approaches. L4GM further enables fast video-to-4D generation in combination with a multiview generative model, _e.g._, ImageDream .

## 2 Related Work

### Large 3D Reconstruction Models

Reconstructing 3D representations from posed images typically requires a lengthy optimization process. Some works have proposed to greatly speed this up by training neural networks to directly learn the full reconstruction task in a way that generalizes to novel scenes [65; 55; 54; 57]. Recently, LRM  was among the first to utilize large-scale multiview datasets including Objavverse  to train a transformer-based model for NeRF reconstruction. The resulting model exhibits better generalization and higher quality reconstruction of object-centric 3D shapes from sparse posed images in a single model forward pass. Similar works have investigated changing the representation to Gaussian splatting [49; 68], introducing architectural changes to support higher resolution [61; 44], and extending the approach to 3D scenes [6; 7]. Methods such as LRM can generalize to input images supplied from sampling multiview diffusion models which enables fast 3D generation .

### Video-to-4D Reconstruction

Recent works have made impressive progress in reconstructing dynamic 3D representations from multiview video inputs [28; 29]. However, for single-view video inputs, the problem of dynamic reconstruction becomes ill-posed and requires hand-crafted or data-driven priors. Good results have been achieved when targetting specific domains using shape templates . Template-free methods typically require accurate depth inputs and cannot fill in portions of the object that are occluded or not visible [14; 60].

Recently, a few works have attempted to extend feed-forward generalizeable novel view synthesis to the challenging setting of dynamic monocular videos. DYST  introduces the synthetic 4D Dyso dataset which they use to train a transformer for generalizeable dynamic novel view synthesis but their model lacks an explicit 3D representation of the scene and camera. PGDVS  extends the generalizeable 3D NVS model GNT  to dynamic scenes but relies on consistent depth maps obtained through per-scene optimization. Other methods [50; 4] leverage pretraining but still require some amount of test-time finetuning to achieve acceptable novel view synthesis. Several works tackle generalizable human shape reconstruction but rely on human template meshes [31; 23].

### Text-To-4D Generation

Dreamfusion  introduced the score distillation framework for text-to-3D shape generation. Such per-object optimization methods have also been extended to the 4D domain  where they leverage video diffusion models [46; 3; 30]. Some methods combine the guidance from video diffusion models with multiview  and single-image diffusion models  to boost 3D consistency and individual frame quality [72; 1; 26], and they utilize different forms of score distillation [25; 66; 56].

By utilizing image conditional diffusion priors such as Zero123  and ImageDream  as guidance, this approach has also been applied to the image-conditional setting  and video-conditional 4D generation setting by Consistent4D  and DreamGaussian4D . GaussianFlow  introduces a Gaussian dynamics based representation which supports them to add optical flow estimation as an additional regularization to SDS. STAG4D  and 4DGen  use diffusion priors to sample additional pseudo-labels from "anchor" views to expand the set of reference images for image-conditional SDS and as direct photometric reconstruction loss.

To accelerate the generation process, some works eschew the use of score distillation guidance altogether. For reconstruction from monocular video input, Efficient4D  and Diffusion\({}^{2}\) utilize a two stage approach. In the first stage they craft schemes for sampling multiview videos conditioned on the input video. Standard optimization-based reconstruction is then used for stage 2. However, this optimization process can still take on the order of tens of minutes. In this work we directly train a feed forward 4D reconstruction model instead.

## 3 Background

Our L4GM builds on the success of single-image 3D reconstruction models [20; 19; 49], specifically the Large Multi-View Gaussian Model (LGM) . LGM accepts a set of multiview images of an object and directly outputs a 3D reconstruction of the object, represented by a set of Gaussian ellipsoids \(P\). Each Gaussian ellipsoid is represented by 14 parameters, including a center \(^{3}\), a scaling factor \(^{3}\), a quaternion rotation \(^{4}\), an opacity \(\), and a color feature \(^{3}\). The multiview images \(=\{J_{v}\}_{v=1}^{V}\) are taken from \(V\) camera poses \(=\{O_{v}\}_{v=1}^{V}\). These camera poses are encoded as image embeddings via Plucker ray embeddings and concatenated to the RGB channels of the multiview images as input to the model. They are fed through an asymmetric U-Net  yielding \(V\) 14-channel image feature maps, where each of the pixels will be interpreted as the parameters of a 3D Gaussian.

Figure 2: **L4GM. The overall model architecture of L4GM. Our model takes a single-view video and single-time step multiview images as input, and outputs a set of 4D Gaussians. It adopts a U-Net architecture and uses cross-view self-attention for view consistency and temporal cross-time self-attention for temporal consistency.**

When only a single input image is given, an image-conditional multiview diffusion model such as ImageDream  is first used to generate plausible completions for the missing multiview images. These generated views are then fed to LGM for reconstruction.

## 4 Our Method

Given a monocular video of a dynamic object, denoted as \(=\{I_{t}\}_{t=1}^{T}\) where \(T\) is video length, our objective is to rapidly reconstruct an accurate 4D representation of the object. Our approach is grounded in two conceptually simple yet impactful insights.

Our inspiration stems from video diffusion models. Recent advances in video generation have highlighted the benefits of first pretraining on image data and then extending and finetuning the model on video datasets to effectively model temporal consistency [3; 16; 59; 2]. Similarly, recognizing the scarcity of 4D data, we want to leverage a pre-trained Large Multi-View Gaussian Model (LGM)  that operates on images and has been extensively trained on a large-scale 3D dataset of static objects. This strategy leverages the robustness of pre-trained models to effectively train a 4D reconstruction model with limited data.

Secondly, in constrast to most existing methods [28; 29] that are required to use multiview videos for 4D reconstruction, we found that utilizing a single set of multiview images at the initial timestep is sufficient. We can obtain these multiview images easily by leveraging multiview image diffusion models to expand the first frame of the view. By adding temporal self-attention layers, our model capitalizes on the initial multiview input by propagating and adapting this information across subsequent timesteps (subsection 4.2). This approach significantly reduces the computational complexity and challenges typically associated with generating consistent multiview videos, while still enhancing the quality of the reconstruction, as our results demonstrate.

Thus, in the following, we introduce L4GM, a model that processes a monocular video to output a set of 3D Gaussians for each timestep, denoted by \(=\{P_{t}\}_{t=1}^{T}\), where each \(P_{t}\) is a set of 3D Gaussians at time \(t\). L4GM is an extension of a pretrained 3D LGM, enhanced with temporal self-attention layers for dynamic modeling. We generate \(V\) multiview images based on the first frame of the input monocular video. These generated views, along with the input video, are fed into L4GM to reconstruct the entire 4D sequence. We also explore further finetuning L4GM into a 4D interpolation model, allowing us to generate 4D scenes at a higher FPS than the monocular input video, offering smoother and more detailed motion dynamics within the 4D reconstructions.

### Generate Multiview Images with ImageDream

Similar to the single-image scenario of LGM, we use ImageDream  to generate four orthogonal views conditioned on the initial frame \(I_{1}\). We denote \(_{1}\) as the set of generated multiview images taken from camera poses \(\) at the initial time step \(t=1\). We would like our generated multiview images to contain three extra views that are orthogonal to the input first frame of the original video.

However, often none of the viewing angles of the generated multiview images match the input frame \(I_{1}\). An example can be found in Appendix Figure 8. To address this, we first use the 3D LGM to reconstruct an initial set of 3D Gaussians, \(P_{}\), from the generated multiview images, and render this reconstruction from views that are orthogonal to \(I_{1}\) as desired. We provide exact details in Appendix E.

### Turning the 3D LGM into a 4D Reconstruction Model

Model Architecture.We adopt the asymmetric U-Net  structure from the pretrained LGM as backbone. To align with LGM's input specifications, we replicate the generated multiview images in \(_{1}\) at \(t=1\) (except \(I_{1}\)) across all other time steps to construct a \(T V\) grid (see left side of Figure 2 for an example). For simplicity, we assume the camera in the reference monocular video is static and only the object is moving so we also copy the camera poses \(\) (except \(_{1}\)) across time steps. Similar to LGM, these poses are then embedded using Plucker ray embeddings . We concatenate this camera embedding with the RGB channels of the input images. The concatenated inputs are reshaped into the format (B T V) H W C and fed into the asymmetric U-Net, where B is batch size.

As illustrated in the middle section of Fig 2, each U-Net block within L4GM consists of multiple residual blocks , followed by cross-view self-attention  layers. To maintain temporal consistency across different timestamps, we introduce a new _temporal_ self-attention layer following each cross-view self-attention layer. These _temporal_ self-attention layers treat the view axis V as a batch of independent videos (by transferring the view axis into the batch dimension). After processing, the data is reshaped back to its original configuration. In einops  notation, this process looks as:

\[ =(,)\] (1) \[ =+()\] (2) \[ =(,)\] (3)

where \(\) is the feature, B H W C are batch size, height, width, and the number of channels.

The output of the U-Net consists of 14-channel feature maps with shape \(B T V H_{} W_{} 14\). Each of the \(1 14\) units is treated as the set of parameters of a per-pixel Gaussian. We concatenate these Gaussians along the view dimension V to form a single set of Gaussians for each timestamp, resulting in \(T\) sets of 3D Gaussians, \(\{P_{t}\}_{t=1}^{T}\). This collection forms our final 4D representation.

Loss Functions.Besides the input camera poses \(\), we select another set of camera poses \(_{}\) for multiview supervision. We train the model with a simple reconstruction objective on the video rendering of the output 4D representations from camera poses \(_{}\), as detailed in Appendix B.

### Autoregressive Reconstruction and 4D Interpolation

In practice, one may want to apply L4GM on long videos and obtain temporally smooth 4D outputs. To this end, L4GM also enables _autoregressive reconstruction_ which processes videos in an autoregressive fashion, one chunk of \(T\) frames after another. We additionally train a _4D Interpolation Model_ to upsample the 4D representation to a higher fps. Details can be found in Section A.

Autoregressive Reconstruction(Figure 3, left). Our base model is designed to accept a monocular video of fixed length \(T\). For long videos that exceed \(T\), we partition the video into chunks of size \(T\) which we process sequentially. We first apply L4GM to the initial \(T\) frames to generate the first set of \(T\) Gaussians. Subsequently, instead of generating multiview images for the first frame of the next chunk using a multiview diffusion model, we render the last set of Gaussians from four orthogonal angles to obtain new multiview images. These newly rendered multiview images, along with the next set of video frames, are then used to generate the next set of Gaussians. The process is repeated until all frames of a long video have been reconstructed. We empirically observe that such an autoregressive reconstruction method can be repeated more than 10 times without a significant drop in quality.

4D Interpolation Model(Figure 3, right). As our model does not track Gaussians across frames, directly interpolating Gaussian trajectories is not feasible [29; 58]. Hence, we develop an interpolation model that operates in 4D, fine-tuned on top of L4GM. Similar interpolation methods have been used successfully in the video generation literature . As shown in Figure 3, the input to the 4D Interpolation Model consists of two sets of multiview images and the interpolation model is trained to produce additional intermediate sets of Gaussians. It leverages the weight-average of the RGB pixels between the multiview images for the newly created intermediate frames. The 4D Interpolation Model then outputs the corresponding sets of Gaussians. In practice, we insert two additional time frames.

## 5 Objaverse-4D dataset

Dataset CollectionTo collect a large-scale dataset for the 4D reconstruction task, we render all animated objects in _Objaverse 1.0_.

Out of 800,000 objects, only 44,000 have animations. Since each object can have multiple associated animations, we obtain a total of 110,000 animations. All of the animations are 24 fps.

The dataset consists mostly of animations on rigged characters or objects, e.g., _"a T-Rex walking"_ or _"a windmill spinning"_. The motions include diverse scenarios such as dynamic locomotions, fine-grained facial expressions, and smoke effects. Interestingly, there are a considerable amount of deforming motions in the dataset, since many animations feature a fantasy style - thus, even rigid real-world objects are deformable. See Appendix Figure 7 and supplementary video for examples.

Dataset RenderingFollowing [53; 32; 24], we adopt the assumption that the real-world monocular videos mostly have 0\({}^{}\) elevation camera poses, and thus render input views for our training data accordingly. Specifically, since animations are of varying lengths, we split each animation into 1 second subclips and render each 4D object into 48 views \(\) 1 second long clips. The views are from _1) 16 fixed cameras,_ where cameras are placed at 0\({}^{}\) elevation with uniformly distributed azimuths, and _2) 32 random cameras,_ where cameras are placed at random elevations and azimuths. During training, we sample input camera poses \(\) from the 16 _fixed_ cameras and sample the supervision cameras \(_{}\) from the 32 _random_ cameras. Furthermore, following , we further filter out approximately 50% of the 26M videos with small motion based on optical flow magnitude, resulting in a total of 12M videos in Obiayerse-4D dataset.

## 6 Experiments

### Implementation Details

We rendered the dataset with Blender and the EEVEE engine . We used fixed camera intrinsics and lighting as detailed in the appendix. For L4GM, we downsample the clips to 8 FPS and train the model for 200 epochs. In training, we set \(T=8\) and use 4 input cameras and 4 supervision cameras. During inference, we used \(T=16\), which we empirically show to work well for longer videos in section 6.3. Each forward pass through L4GM takes about 0.3 seconds, while generating sparse views requires about 2 seconds. For training the interpolation model, we use the 24 FPS clips without downsampling and fine-tune L4GM for another 100 epochs. The 4D interpolation model takes 0.065 seconds to interpolate between every two frames (see Appendix for details).

### Comparisons to State-of-the-Art Methods

We focus our evaluations on video-to-4D reconstruction. Although L4GM can also be used for text-to-4D or image-to-4D synthesis by taking text-to-video or image-to-video outputs from video generative models as input, existing text-to-4D [26; 1; 46] and image-to-4D  approaches typically rely on score distillation and are orders of magnitudes slower than L4GM, preventing meaningful comparisons. Hence, we concentrate on video-to-4D. Results are presented in the following paragraphs.

Quantitative Evaluation.We evaluate our L4GM on the benchmark provided by Consistent4D . It consists of eight dynamic 3D animations of 4 seconds in length, at 8-FPS. A video from one view is

Figure 3: **Left: Autoregressive reconstruction. We use the multiview rendering of the last Gaussian as the input to the next reconstruction. There is a one-frame overlap between two consecutive reconstructions. Right: 4D Interpolation. The interpolation model takes in the interpolated multiview videos rendered from the reconstruction results and outputs interpolated Gaussians.**

used as input and 4 videos from other viewpoints are used for evaluation. Three metrics are computed: _1)_ Perceptual similarity (LPIPS) between the generated and the ground truth novel views, _2)_ the CLIP  image similarity between the generated and the ground truth novel views, and _3)_ the FVD  against the ground truth novel views, which measures video quality. We also report the runtime. We reconstruct the video at the original frame rate without using interpolation model. The results are shown in Table 1. L4GM outperforms existing video-to-4D generation approaches on all quality metrics by a significant margin, while being 100 to 1,000 times faster.

Qualitative Evaluation.Figure 4 illustrates the renderings produced by L4GM on two videos from different angles and timesteps. These examples are taken from the ActivityNet  and Consistent4D  datasets. As shown in the figure, L4GM produces high-quality, sharp renderings while exhibiting strong temporal and multiview consistency.

We compare our visual results to DG4D , STAG4D , and OpenLRM . DG4D and STAG4D are optimization-based approaches that take 10 minutes and 2 hours on 64-frame videos, respectively. OpenLRM is an opensource work reproducing LRM  that reconstructs 3D shapes from single-view images. We run OpenLRM on every video frame to construct a 3D sequence. We collect 24 evaluation videos from Emu , Sora , Veeo , and ActivityNet , covering both generated videos and real-world videos. For our approach, we use \(T=16\) and use the interpolation model. Some qualitative comparisons are presented in Figure 5. Notably, a significant improvement from our approach is a higher 3D resolution. Optimization-based approaches use only thousands of Gaussians to keep the optimization tractable, while our feed-forward approach can easily reconstruct more than 60,000 Gaussians per frame at a dramatically faster speed.

We further conduct a user study based on qualitative comparisons, and the results are shown in Table 2. Our approach is the most favorable on all evaluation criteria including _overall quality_, _3D appearance_, _3D alignment with input video_, _motion alignment with input video_, and _motion realism_. More details about the evaluation dataset and the user study are in Appendix G.

### Ablation Studies

We carry out a variety of ablation studies. For training models in the ablation study, we only keep animations from high-quality objects in _GObjaverse_, which accounts for \(\)25% of the data.

**3D Pretraining.** Without 3D pre-taining, i.e. without initializing from LGM , our model fails to converge (using the same training recipe). The explanation is likely that without large-scale pre-training on static 3D scenes our Objaverse-4D dataset is insufficient for L4GM to not only learn temporal dynamics but also its 3D understanding from scratch. Moreover, starting from a fully random initializion may also contribute to training instabilities. Note that when reducing the model size to the "small" LGM variation , the model starts to converge. However, as shown in Table 6 (a), the model converges to a significantly lower PSNR in the same training epochs.

   Method & LPIPS\(\) & CLIP\(\) & FVD\(\) & Time\(\) \\  Consistent4D  & 0.16 & 0.87 & 1133.44 & 2 hr \\
4DGen  & 0.13 & 0.89 & - & 1 hr \\ GaussianFlow\({}^{}\) & 0.14 & 0.91 & - & - \\ STAG4D  & 0.13 & 0.91 & 992.21 & 1 hr \\ DG4D\({}^{}\) & 0.16 & 0.87 & - & 10 min \\ Efficient4D  & 0.14 & 0.92 & - & 6 min \\  Ours & **0.12** & **0.94** & **691.87** & **3s** \\   

Table 1: **Quantitative results for video-to-4D. Best is bolded. \(\): results from Gao et al. .**

Figure 4: Qualitative results from L4GM, showcasing renderings from 4D reconstructions produced from two in-the-wild videos.

**Frozen LGM.** In this experiment, we freeze the layers of the original LGM model and only train the temporal attention layers. As shown in Table 6 (b), the model improves faster at the beginning of the training but converges to a lower PSNR. This suggests that end-to-end fine-tuning of L4GM, including the non-temporal layers of the 3D LGM, is preferable for the 4D reconstruction task.

Temporal Attention.Without temporal attention, the model falls back into a 3D reconstruction model, while receiving asynchronous multiview inputs. The model does a surprisingly good job using only the 3D information, but it still converges to a lower PSNR, as shown in Table 6 (b). The reconstructed novel view videos contain visible flickering due to the lack of temporal modeling. Please refer to the supplementary video for a comparison.

Deformation Field.Since different types of deformation fields and HexPlane representations have recently been used in the text-to-4D literature , we modify the model to predict a canonical 3D representation and a deformation field based on a HexPlane . Concretely, we average the Gaussians as a canonical 3D representation and introduce a new decoder after the middle block in the U-Net to predict a HexPlane. The representation follows Wu et al. . A detailed illustration can be found in the appendix. Although the model can successfully overfit to a single 4D data sample, it fails to learn a reasonable deformation field during large-scale training. As shown in Table 6 (b), the PSNR only slowly improves and the output is always static. This observation is very different from previous optimization-based 4D generation works . We speculate that SDS-based methods often rely on the smoothness of implicit representations to regularize their generations, whereas our L4GM model may have directly learned to output smooth representations from the 4D training data.

Figure 5: **Qualitative comparisons** of L4GM’s results against the baselines.

Autoregressive Reconstruction.Our model allows taking a video with a length different from the training video length. Here, we analyze the effect of the test-time video length \(T\) and the number of autoregressive steps on the reconstruction quality. We use a long animation in Obojaverse-4D dataset and compute the per-frame reconstruction PSNR. Results are in Table 6 (c). When the ground-truth multiviews are provided, the quality slightly drops when using longer video length. When reconstructing autoregressively, the quality decreases with more autoregressive runs. A shorter video length will start with a higher quality, but the quality drops faster than a longer video length because more self-reconstructions are required. In practice, we select \(T=16\), which offers a balanced performance for different video lengths.

Time Embedding.Here, we explore adding a time embedding to L4GM so that the model is aware of the ordering of the frames. Timestamps are encoded by a sinusoidal function and added to the camera embedding. However, the PSNR did not improve after adding the time embedding. We speculate that the image frames from the input video are already giving sufficient information about the temporal relation between the timesteps. Therefore, we do not train with temporal embedding to add more flexibility to the model at inference time.

4D Interpolation.Finally, we show a comparison of using vs. not using the 4D interpolation model on an 8-FPS video from the Consistent4D dataset in the supplementary video. Notably, the 4D interpolation model can successfully improve the framerate beyond the input video framerate.

## 7 Conclusions

We presented L4GM, the first large reconstruction model for dynamic objects. It produces dynamic sequences of sets of 3D Gaussians from a single-view video. The model leverages prior 3D knowledge from a pretrained 3D reconstruction model, and learns the temporal dynamics from a synthetic dynamic dense-view 4D dataset, Obojaverse-4D dataset, which we collect. L4GM can reconstruct long videos and uses learned interpolation to achieve high framerates. We achieve orders of magnitude faster inference times than existing 4D reconstruction or text-to-4D methods. Moreover, our model generalizes well to in-the-wild real and generated videos. Our work is an early attempt at AI tooling for 4D content creation, with many challenges remaining. For example, for making this technology really useful for professionals we need to develop more advanced human-in-the-loop 4D editing capabilities.

**Broader Impact.** L4GM allows fast 4D reconstruction from in-the-wild videos, which is useful for various graphics and animation applications. However, this model should be used with an abundance of caution to prevent malicious impersonations. The model is also trained on non-commercial public datasets and is for research-only purpose.

   L4GM _(ours)_ & v.s DG4D  & v.s OpenLRM  & v.s STAG4D  \\  Overall Quality & **65.4**/25.0 & **57.9**/33.3 & **54.2**/35.0 \\
3D Appearance & **67.1**/25.8 & **58.8**/31.7 & **55.0**/34.2 \\
3D Alignment w. Input Video & **61.3**/26.3 & **51.3**/32.1 & **50.0**/36.7 \\ Motion Alignment w. Input Video & **61.3**/22.6 & **50.9**/29.2 & **50.4**/31.7 \\ Motion Realism & **62.1**/30.0 & **54.2**/35.0 & **50.4**/36.7 \\   

Table 2: **Comparison to baselines** by user study on synthesized 4D scenes with 24 examples. Numbers are percentages. Numbers do not add up to 100; difference is due to users voting “no preference” (details in Appendix).

Figure 6: PSNR plot. _a)_ Training with different pretrain and training data. _b)_ Training with different design choices. _c)_ Per-frame PSNR with different video lengths \(T\), AR denotes autoregressive.