# Graph Diffusion Transformers for

Multi-Conditional Molecular Generation

 Gang Liu, Jiaxin Xu, Tengfei Luo, Meng Jiang

University of Notre Dame

{gliu7, jxu24, tluo, mjiang2}@nd.edu

###### Abstract

Inverse molecular design with diffusion models holds great potential for advancements in material and drug discovery. Despite success in unconditional molecular generation, integrating multiple properties such as synthetic score and gas permeability as condition constraints into diffusion models remains unexplored. We present the Graph Diffusion Transformer (Graph DiT) for multi-conditional molecular generation. Graph DiT integrates an encoder to learn numerical and categorical property representations with the Transformer-based denoiser. Unlike previous graph diffusion models that add noise separately on the atoms and bonds in the forward diffusion process, Graph DiT is trained with a novel graph-dependent noise model for accurate estimation of graph-related noise in molecules. We extensively validate Graph DiT for multi-conditional polymer and small molecule generation. Results demonstrate the superiority of Graph DiT across nine metrics from distribution learning to condition control for molecular properties. A polymer inverse design task for gas separation with feedback from domain experts further demonstrates its practical utility.

## 1 Introduction

Diffusion models for molecular graphs are essential for inverse design of materials and drugs by generating molecules and polymers (macro-molecules) , because the models can be effectively trained to predict discrete graph structures and atom/bond types in denoising processes . Practical inverse designs consider multiple factors such as molecular synthetic score and various properties , known as the task of multi-conditional graph generation.

Existing work converted multiple conditions into a single one and solved the task as single-condition generation . However, multi-property relations may not be properly or explicitly defined . First, the properties have diverse scales and units. For example, the synthetic complexity ranges from 1 to 5 , while the gas permeability varies widely, exceeding 10,000 in Barrier units . This gap makes it hard for models to balance the conditions. Second, multi-conditions consist of a mix of categorical and numerical properties. The common practice of addition  or multiplication  is inadequate for combination.

Figure 1(a) empirically illustrates the challenges in multi-conditional generation, i.e., discovering molecules meeting multiple properties. We used a test set of 100 data points with three properties: synthesizing (Synth.) , O\({}_{2}\) and N\({}_{2}\) permeability (O\({}_{2}\)Perm and N\({}_{2}\)Perm) . A single-conditional diffusion model generated up to 30 graphs for each condition, resulting in a total of 90 graphs for three conditions. We sort the 30 graphs in each set using a polymer property Oracle (see appendix B.3). Then, we check whether a shared polymer structure that meets multi-property constraints can be identified across different condition sets. If we find the polymer, its rank \(K\) (where \(K\) is between 1 and 30) indicates how high it appears on the lists, considering all condition sets. If not, we set \(K\) as 30. Figure 1(a) shows the frequency distribution of \(K\) on the 100 test cases. The median \(K\) was30, indicating that the multiple properties were not met on over half of the test polymers despite generating a large number of graphs.

To address these challenges, we project multi-properties into representations by _learning_, thereby guiding the diffusion process for molecule generation. We propose the Graph Diffusion Transformer (Graph DiT) for graph denoising under conditions. Graph DiT has a condition encoder for property representation learning and a graph denoiser. The condition encoder utilizes a novel clustering-based method for numerical properties and one-hot encoding for categorical ones to learn multi-property representations. The graph denoiser first integrates node and edge features into graph tokens, then denoise these tokens with adaptive layer normalization (AdaLN) in Transformer layers [19; 34]. AdaLN replaces the molecular statistics (mean and variance) in each hidden layer with those from the condition representation, effectively outperforming other predictor-based and predictor-free conditioning methods [22; 43; 34], as shown in Section 4.4. We observe that existing forward diffusion processes [43; 22] apply noise separately to atoms and bonds, which may compromise the accuracy of Graph DiT in noise estimation. Hence, we propose a novel graph-dependent noise model that effectively applies noise tailored to the dependencies between atoms and bonds within the graph.

Results in Figure 1(b) show that the polymers generated by Graph DiT closely align with multi-property constraints. For each test case, we have _one_ graph generated from Graph DiT conditional on three properties. The Oracle determines the rank of this graph among 30 single-conditionally generated graphs for each condition. We find the median ranks are 4, 9, and 11, for Synth., O\({}_{2}\) Perm, and N\({}_{2}\) Perm, respectively, all much higher than 30. Note that the ranked set of 30 graphs was very competitive because the model was trained on the specific condition dedicatedly.

In experiments, we evaluate model performance on one polymer and three small molecule datasets. The polymer dataset includes four numerical conditions for multi-conditional evaluation. Our model has the lowest average mean absolute error (MAE), significantly reducing the error by 17.86% compared to the best baseline. It also excels in small molecule tasks, achieving over 0.9 accuracy on task-related categorical conditions, notably surpassing the baseline accuracy of less than 0.6. We also examine the model's utility in inverse polymer designs for O\({}_{2}\)/N\({}_{2}\) gas separation, with domain expert feedback highlighting our model's practical utility in multi-conditional molecular design.

Figure 1: Multi-conditional diffusion guidance in (b) generates polymers of higher property accuracy than existing work in (a). Explanations are in Section 1 and details are in appendix B.3.

## 2 Problem Definition

### Multi-Conditional Inverse Molecular Design

A molecular graph \(G=(V,E)\) consists of a set of nodes (atoms) \(V\) and edges (bonds) \(E\). We follow  and define "non-bond" as a type of edge. There are \(N\) atoms and each atom has a one-hot encoding, denoting the atom type. We represent it as \(_{V}^{N F_{V}}\), where \(F_{V}\) is the total number of atom types. Similarly, the bond features are a tensor \(_{E}^{N N F_{E}}\), representing both the graph structure and \(F_{E}\) bond types.

Let \(=\{c_{1},c_{2},,c_{M}\}\) be a set of \(M\) numerical and categorical conditions. The task is: \(q(G c_{1},c_{2},,c_{M}) q(G)q(c_{1},c_{2},,c_{M} G)\), where \(q\) represents observed probability. We use a model parameterized by \(\) for multi-conditional molecular generation \(p_{}(G)\). The evaluation involves both distribution learning \(q(G)\) and condition control \(q(c_{1},c_{2},,c_{M} G)\). We follow previous work in assuming that there exist different oracle functions \(\) that can independently evaluate each conditioned property : \(q(c_{1},c_{2},,c_{M} G)=_{i=1}^{M}_{i}(c_{i} G)\). Note that the oracles are _not_ used in the training of \(p_{}\).

### Diffusion Model on Graph Data

Diffusion models consist of forward and reverse diffusion processes . We refer to the forward diffusion process as the diffusion process following . The diffusion process \(q(G^{1:T} G^{0})=_{t=1}^{T}q(G^{t} G^{t-1})\) corrupts molecular graph data (\(G^{0}=G\)) into noisy states \(G^{t}\). As timesteps \(T\), \(q(G^{T})\) converges a stationary distribution \((G)\). The reverse Markov process \(p_{}(G^{0:T})=q(G^{T})_{t=1}^{T}p_{}(G^{t-1} G^{t})\), parameterized by neural networks, gradually denoises the latent states toward the desired data distribution.

Diffusion ProcessOne may perturb \(G^{t}\) in a discrete state-space to capture the structural properties of molecules . Two transition matrices \(_{V}^{F_{V} F_{V}}\) and \(_{E}^{F_{E} F_{E}}\) are defined for nodes \(_{V}\) and edges \(_{E}\), respectively . Then, each step \(q(G^{t} G^{t-1},G^{0})=q(G^{t} G^{t-1})\) in the diffusion process is sampled as follows.

\[q(_{V}^{t}_{V}^{t-1})=( _{V}^{t};=_{V}^{t-1}_{V}^{t}), \\ q(_{E}^{t}_{E}^{t-1})=(_{E} ^{t};=_{E}^{t-1}_{E}^{t}),\] (1)

where \((;)\) denotes sampling from a categorical distribution with probability \(\). We remove the subscript (\({}_{V/E}\)) when the description applies to both nodes and edges. It is assumed that the noise \(^{i}\) (\(i t\)) is independently applied to \(\) in each step \(i\), allowing us to rewrite \(q(^{t}^{t-1})\) as the probability of the initial state \(q(^{t}^{0})=(^{t};=^{0}}^{t})\), where \(}^{t}=_{i t}^{i}\).

Noise SchedulingTransition matrices \(_{V}\) and \(_{E}\) control the noise applied to atom features and bond features, respectively. Vignac et al.  defined \((G)=(_{X}^{F_{V}},_{E}^{F_{E}})\) as the marginal distributions of atom types and bond types. The transition matrix at timestep \(t\) is \(^{t}=^{t}+(1-^{t})^{}\) for atoms or bonds, where \(^{}\) denotes the transposed row vector. Therefore, we have \(}^{t}=^{t}+(1-^{t})^{}\), where \(^{t}=_{=1}^{t}^{}\). The cosine schedule  is often chosen for \(^{t}=(0.5(t/T+s)/(1+s))^{2}\).

Reverse ProcessWith the initial sampling \(G^{T}(G)\), the reverse process generates \(G^{0}\) iteratively in reversed steps \(t=T,T-1,,0\). We use a neural network to predict the probability \(p_{}(^{0} G^{t})\) as the product over nodes and edges [1; 43]:

\[p_{}(^{0} G^{t})=_{v V}p_{}(v^{t-1} G^{t} )_{e E}p_{}(e^{t-1} G^{t})\] (2)

\(p_{}(^{0} G^{t})\) could be combined with \(q(G^{t-1} G^{t},G^{0})\) to estimate the reverse distribution on the graph \(p_{}(G^{t-1} G^{t})\). For example, \(p_{}(v^{t-1} G^{t})\) is marginalized over predictions of node types \(}_{v}\), which applies similarly to edges:

\[p_{}(v^{t-1} G^{t})=_{}_{v}}q(v^{t-1} ,G^{t})p_{}( G^{t}).\] (3)

The neural network could be trained to minimize the negative log-likelihood .

\[L=_{q(G^{0})}_{q(G^{t}|G^{0})}[-_{  G^{0}} p_{}( G^{t})]\] (4)

## 3 Multi-Conditional Graph Diffusion Transformers

We present the denoising framework of Graph DiT in Figure 2. The condition encoder learns the representation of \(M\) conditions. The statistics of this representation like mean and variance are used to replace the ones from the molecular representations  (see Section 3.2). Besides, we introduce a new noise model in the diffusion process to better fit graph-structured molecules (see Section 3.1).

### Graph-Dependent Noise Models

The transition probability of a node or an edge should rely on the joint distribution of nodes and edges in the prior state. However, as an example shown in Eq. (1), current diffusion models [22; 43; 25] treat node and edge state transitions as independent, misaligning with the denoising process in Eq. (3). This difference between the sampling distributions of noise in the diffusion and reverse processes introduces unnecessary challenges to multi-conditional molecular generations.

To address this, we use a single matrix \(_{G}^{N F_{G}}\) to represent graph tokens for \(G\), with \(F_{G}=F_{V}+N F_{E}\). Token representations are created by concatenating the node feature matrix \(_{V}\) and the flattened edge connection matrix from \(_{E}\). Each row vector in \(_{G}\) contains features for both nodes and edges, representing all connections and non-connections. Hence, we could design a transition matrix \(_{G}\) considering the joint distribution of nodes and edges. \(_{G}^{F_{G} F_{G}}\) is constructed from four matrices \(_{V},_{EV}^{F_{E} F_{V}},_{ E},_{VE}^{F_{V} F_{E}}\), denoting the transition probability ("dependent old state" \(\) "target new state") node \(\) node; edge \(\) edge; node \(\) edge, respectively.

\[_{G}=_{V}&_{N}^{} _{VE}\\ _{N}_{EV}&_{N N}_{ E},\] (5)

where \(\) denotes the Kronecker product, \(_{N}\), \(_{N}^{}\), and \(_{N N}\) represent the column vector, row vector, and matrix with all 1 elements, respectively. According to Eq. (5), the first \(F_{V}\) columns in \(_{G}\) determine the node feature transitions based on both node features (first \(F_{V}\) rows) and edge features (remaining \(N F_{E}\) rows). Conversely, the remaining \(N F_{E}\) columns determine the edge feature transitions, depending on the entire graph. We introduce a new diffusion noise model:

\[q(_{G}^{t}_{G}^{t-1})=}( _{G}^{t};}=_{G}^{t-1}_{G}^{t} ),\] (6)

where \(}\) is the unnormalized probability and \(}\) denotes categorical sampling: The first \(F_{V}\) columns of \(}\) are normalized to sample \(_{V}^{t}\), while the remaining \(N E\) dimensions are reshaped and normalized to sample edges \(_{E}^{t}\). These components are combined to form \(_{G}^{t}\), completing the \(}\) sampling.

Choice of \(_{VE}\) and \(_{EV}\)Similar to the definitions of \(_{V}\) and \(_{E}\), we leverage the prior knowledge within the training data for the formulation of task-specific matrices, \(_{EV}\) and \(_{VE}\). We calculate co-occurrence frequencies of atom and bond types in training molecular graphs to obtain the marginal atom-bond co-occurrence probability distribution. For each bond type, each row in \(_{EV}\) represents the probability of co-occurring atom types. \(_{VE}\) is the transpose of \(_{EV}\) and has a similar meaning. Subsequently, we define \(_{EV}=^{t}+(1-^{t}) _{EV}^{}\) and \(_{VE}=^{t}+(1-^{t}) _{VE}^{}\).

Figure 2: Denoising framework and architectures for Graph DiT. Details are in Section 3.2.

### Denoising Models with Multi-Property Conditions

We present Graph DiT as the denoising model to generate molecules under multi-conditions \(=\{c_{1},c_{2},,c_{M}\}\) without extra predictors.

Predictor-Free GuidanceThe predictor-free reverse process \(_{}(G^{t-1} G^{t},)\) aims to generate molecules with a high probability \(q( G^{0})\). This could be achieved by a linear combination of the log probability for unconditional and conditional denoising :

\[_{}(G^{t-1} G^{t},)= p_{}(G^{t-1} G^{ t})+s( p_{}(G^{t-1} G^{t},)- p_{}(G^{t-1}  G^{t})),\] (7)

where \(s\) denotes the scale of conditional guidance. Unlike classifier-free guidance , which typically predicts noise, we directly estimate \(p_{}(^{0} G^{t},)\). We one one denoising model \(f_{}(G^{t},)\) for both \(p_{}(^{0} G^{t})\) and \(p_{}(^{0} G^{t},)\). Here, \(f_{}(G^{t},=)\) computes the unconditional probability by substituting the original conditional embeddings with the null value. During training, we randomly drop the condition with a ratio, i.e., \(=\), to learn the embedding of the null value. \(f_{}(G^{t}=_{G}^{t},)\) comprises two components: the condition encoder and the graph denoiser. An overview of the architecture is presented in Figure 2.

Condition EncoderWe treat the timestep \(t\) as a special condition and follow  to obtain a \(D\)-dimensional representation \(\) with sinusoidal encoding. For property-related numerical or categorical condition \(c_{i}\), we apply distinct encoding operations to get \(D\)-dimensional representation. For a categorical condition, we use the one-hot encoding. For a numerical variable, we introduce a clustering encoding method. This defines learnable centroids, assigning \(c_{i}\) to clusters, and transforming the soft assignment vector of condition values into the representation. It could be implemented using two \(\) layers and a \(\) layer in the middle as: \((((c_{i})))\). Finally, we could obtain the representation of the condition as \(=_{i=1}^{M}(c_{i})\), where \(\) is the specific encoding method based on the condition type. For numerical conditions, we evaluate our proposed clustering-based approach against alternatives like direct or interval-based encodings . As noted in Section 4.4, the clustering encoding outperforms the other methods.

Graph Denoiser: Transformer LayersGiven the noisy graph at timestep \(t\), the graph tokens are first encoded into the hidden space as \(=(_{G}^{t})\), where \(^{N D}\). We then adapt the standard Transformer layers  with self-attention and multi-layer perceptrons (MLP), but replace the normalization with the adaptive layer normalization (\(\)) controlled by the representations of the conditions [19; 34]: \(=(,)\). For each row \(\) in \(\):

\[(,)=_{}() -()}{( )}+_{}(),\] (8)

where \(()\) and \(()\) are mean and variance values. \(\) indicates element-wise product. \(_{}()\) and \(_{}()\) are neural network modules in \(f_{}()\), each of which consists of two linear layers with \(\) activation  in the middle. We have a gated variant \(_{gate}\) for residuals:

\[_{gate}(,)=_{}( )(,)\] (9)

We apply the zero initialization for the first layer of \(_{}(),_{}()\), and \(_{}()\). There are other options to learn the structure representation from the condition : In-\(\) conditioning adds condition representation to the structure representation at the beginning of the structure encoder, and \(}\) calculates cross-attention between the condition and structure representation. We observe in Section 4.4 that \(\) performs best among them.

Graph Denoiser: Final MLPWe have the hidden states \(\) after the final Transformer layers, the MLP is used to predict node probabilities \(}_{V}^{0}\) and edge probabilities \(}_{E}^{0}\) at \(t=0\):

\[}_{G}^{0}=((),).\] (10)

We split the output \(_{G}\) into atom and bond features \(}_{V}^{0},}_{E}^{0}\). The first \(F_{V}\) dimensions of \(}_{G}^{0}\) represent node type probabilities, and the remaining \(N F_{E}\) dimensions cover probabilities for \(N\) edge types associated with the node, as detailed in Section 3.1.

Generation to Molecule ConversionA common way of converting generated graphs to molecules selects only the largest connected component , denoted as Graph DiT-LCC in our model. For Graph DiT, we connect all components by randomly selecting atoms. It minimally alters the generated structure to more accurately reflect model performance than Graph DiT-LCC.

## 4 Experiment

**RQ1**: We validate the generative power of Graph DiT compared to baselines from molecular optimization and diffusion models in Section 4.2. **RQ2**: We study a polymer inverse design for gas separation in Section 4.3. **RQ3**: We conduct further analysis to examine Graph DiT in Section 4.4.

### Experimental Setup

We use datasets with over ten types of atoms and up to fifty nodes in a molecular graph. We include both numerical and categorical properties for drugs and materials, offering a benchmark for evaluation across diverse chemical spaces. Model performance is validated across up to nine metrics, including distribution coverage, diversity, and condition control capacity for various properties.

Datasets and Input ConditionsWe have one polymer dataset  for materials, featuring three **numerical** gas permeability conditions: O\({}_{2}\)Perm, CO\({}_{2}\)Perm, and N\({}_{2}\)Perm. For drug design, we create three class-balanced datasets from MoleculeNet : HIV, BBBP, and BACE, each with a **categorical** property related to HIV virus replication inhibition, blood-brain barrier permeability, or human \(\)-secretase 1 inhibition, respectively. We have two more **numerical** conditions for synthesizability from synthetic accessibility (SAS) and complexity scores (SCS) [12; 8].

EvaluationWe randomly split the dataset into training, validation, and testing (reference) sets in a 6:2:2 ratio. Evaluations are conducted on 10,000 generated examples with metrics  (1) molecular validity (Validity); (2) heavy atom type coverage (Coverage); (3) internal diversity among the generated examples (Diversity); (4) fragment-based similarity with the reference set (Similarity); (5) Frechet ChemNet Distance with the reference set (Distance) ; MAE between the generated and conditioned (6) synthetic accessibility score  (Synth.); (7)\(\)(9) MAE/Accuracy for the numerical/categorical task conditions (Property). The evaluation Oracle uses random forest trained on all task-related molecules . Lower MAE or higher accuracy indicates stronger model controllability.

BaselinesWe select strong and popular molecular optimization baselines from recent studies : Graph-GA , MARS , JTVAE  with Bayesian optimization (JTVAE-BO), LSTM  on SMILES with Hill Climbing (LSTM-HC). We include the most recent diffusion models: GDSS, DiGress , and their conditional version with extra predictors: MOOD , and DiGress v2 . We train multi-task predictors using the same architecture for MOOD and DiGress v2 models to provide additional guidance for generation. For molecular optimization, we formulate the condition set of each test data point as a combined goal, minimizing the sum of the normalized errors between generated and input properties. We train a random forest model for each property using the training data to optimize the molecular structure.

### RQ1: Multi-Conditional Molecular Generation

We have the observations from Table 1 and Table 2:

Chemical ValidityHigh validity may not accurately represent the model's generative performance if hard-coded rules are introduced in the algorithm. For example, GraphGA could eliminate non-valid molecules during mutation and crossover iterations to achieve perfect validity in the final evaluation. Without rule checking in the generation-to-molecule step, DiGress, GDSS, and MOOD show a marked performance decline, with validity often dropping from 0.99 to below 0.6. In contrast, Graph DiT often maintains over 0.8 validity without any rule-based processing.

   Model &  Validity \\ (with an effective) \\  &  Diffusion Learning \\  &  Diffusion Learning \\  &  \% \\  &  Diffusion Learning \\  &  \% \\  &  \% \\  &  NoPerm \\  &  NoPerm \\  &  NoPerm \\  & 
 NoPerm \\  \\  Graph GA & 1.0000 (N.A.) & 11/11 & 0.8828 & 0.9269 & 9.1882 & 1.3307 & 1.9840 & 2.2900 & 1.9489 & 1.8884 \\ MARS & 1.0000 (N.A.) & 11/11 & 0.8375 & 0.9283 & 7.5620 & 1.1658 & 1.5761 & 1.8327 & 1.6074 & 1.5455 \\ LSTM-HC & 0.9910 (N.A.) & 10/11 & 0.8918 & 0.7397 & 18.1562 & 1.4251 & 1.1009 & 1.2365 & 1.0772 & 1.2998 \\ JTVAE-BO & 1.0000 (N.A.) & 10/11 & 0.3766 & 0.7294 & 25.9509 & **1.0714** & 1.0781 & 1.2352 & 1.0978 & 1.1206 \\  DiGress & 0.9913 (0.2362) & 1/11 & 0.9099 & 0.2724 & 22.7237 & 2.9842 & 1.7163 & 2.0630 & 1.6738 & 2.1093 \\ DiGress v2 & 0.9812 (0.3057) & 1/11 & **0.9105** & 0.2771 & 21.7311 & 2.7507 & 1.7130 & 2.0632 & 1.6648 & 2.0479 \\ GiSS & 0.9205 (0.9076) & 9/11 & 0.7510 & 0.0000 & 34.2627 & 1.3701 & 1.0271 & 1.0820 & 1.0683 & 1.1369 \\ MOOD & 0.9866 (0.9295) & 1/11 & 0.8349 & 0.0227 & 39.3981 & 1.4019 & 1.4961 & 1.7643 & 1.4748 & 1.5333 \\  Graph DiT-LCC (Ours) & 0.9753 (0.8437) & 1/11 & 0.8875 & 0.9560 & **0.949** & 1.3099 & 0.8001 & 0.9562 & 0.8125 & 0.9697 \\ Graph DiT (Ours) & 0.8245 (0.8437) & 1/11 & 0.8712 & **0.9600** & **0.6443** & 1.2973 & **0.7404** & **0.8857** & **0.7550** & **0.9205** \\   

Table 1: Multi-Conditional Generation of 10K Polymers: Results on the synthetic score (Synth.) and three numerical properties (gas permeability for O\({}_{2}\), N\({}_{2}\), CO\({}_{2}\)). MAE is calculated between the input conditions and the properties of the generated polymers using Oracles. Best results are **highlighted**.

**Distribution Learning** GraphGA is a simple yet effective baseline for generating in-distribution molecules, e.g., on BBBP and HIV generation datasets. Diffusion model baselines such as DiGress and MOOD could produce diverse molecules but often fail to capture the original data distribution in multi-conditional tasks. Graph DiT shows the competitive performance of diffusion models in fitting complex molecular data distributions. Using fragment-based similarity and neural network-based distance metrics , we achieve the best in the polymer task and rank second in the HIV small molecule task, involving up to 11 and 29 types of heavy atoms, respectively.

**Condition Controllability** LSTM-HC surpasses many baselines, achieving lower average MAE on polymer properties and higher rankings on small molecular properties. However, its control over synthetic scores in polymer tasks is relatively poor. Conversely, MARS effectively manages synthetic scores for polymers but exhibits a larger MAE in gas permeability conditions compared to other baselines. GDSS performs well in gas permeability control but underperforms Graph GA and MARS in terms of the synthetic score condition. DiGress v2 and MOOD, although equipped with the predictor guidance, still exhibit limited condition control compared to their unconditional counterparts over polymer and small molecule tasks. These baselines struggle to balance and control multiple conditions in generation. In contrast, Graph DiT significantly improves diffusion models and achieves the best multi-conditional performance in all tasks. In polymer tasks, Graph DiT reduces MAE on all gas permeability conditions, averaging +17.8% improvement over the best baseline LSTM-HC. For small molecule tasks, Graph DiT consistently ranks top-1 in condition controllability with over 0.9 accuracy in categorical conditions. Compared to Graph DiT-LCC, we observe that Graph DiT, which connects all generated graph components, shows better controllability performance due to minimal rule-based post-generation processing.

### RQ2: Polymer Inverse Design for Gas Separation

We aim to design polymers with high O\({}_{2}\) and low N\({}_{2}\) permeability, demonstrating the models' precise control over related properties. Following Robeson 's definition of high-performance polymers based on the O\({}_{2}\)/N\({}_{2}\) permeability ratio, we selected 16 polymers meeting this criterion from 609 examples as our test/reference set. The remaining data is used for training and validation. Subsequently, we generated 1,000 polymers conditioned on test set labels.

    &  &  &  &  \\  & &  &  &  &  &  &  &  \\    } & Graph GA & 1.0000 (N.A.) & 88 & 0.8585 & 0.9805 & 7.4104 & 0.9633 & 0.4690 & 6.5000 \\  & MARS & 1.0000 (N.A.) & 88 & 0.8318 & **0.8827** & **0.7923** & 1.0123 & 0.5184 & 5.0000 \\  & LSTM-HC & 0.9972 (N.A.) & 88 & 0.8146 & 0.7982 & 17.5685 & 0.9207 & 0.5816 & 3.0000 \\  & ITVAE-BO & 1.0000 (N.A.) & 68 & 0.6682 & 0.7281 & 0.4966 & 0.9923 & 0.4628 & 7.5000 \\  & DiGress & 0.3511 (0.2858) & 88 & 0.8862 & 0.6942 & 24.6560 & 2.0681 & 0.5061 & 8.0000 \\  & DiGress v2 & 0.3546 (0.2680) & 88 & 0.8812 & 0.7027 & 25.3270 & 2.3365 & 0.5113 & 7.5000 \\  & GDSS & 0.2879 (0.2589) & 48 & 0.8756 & 0.2708 & 46.7539 & 1.6422 & 0.5036 & 7.5000 \\  & MOOD & 0.9947 (0.4020) & 8.8 & **0.8902** & 0.2587 & 44.2394 & 1.8835 & 0.5062 & 7.0000 \\  & Graph DIT-LCC (Ours) & 0.8646 (0.3495) & 88 & 0.8240 & 0.8757 & 6.9836 & 0.4053 & 0.9050 & 2.0000 \\  & Graph DIT (Ours) & 0.8674 (0.8495) & 88 & 0.8238 & 0.8752 & 7.0456 & **0.3998** & **0.9135** & **1.0000** \\    } & Graph GA & 1.0000 (N.A.) & 99 & 0.8950 & **0.9509** & **0.16495** & 1.2082 & 0.3015 & 7.5000 \\  & MARS & 1.0000 (N.A.) & 89 & 0.6367 & 0.7606 & 10.9971 & 1.2250 & 0.5189 & 6.0000 \\  & LSTM-HC & 0.9990 (N.A.) & 89 & 0.8883 & 0.8932 & 16.3904 & 0.9969 & 0.5590 & 4.0000 \\  & ITVAE-BO & 1.0000 (N.A.) & 59 & 0.7458 & 0.5821 & 33.5746 & 1.1619 & 0.4598 & 6.0000 \\  & DiGress & 0.6960 (0.4871) & 99 & 0.5098 & 0.6805 & 18.6972 & 2.3683 & 0.6356 & 6.5000 \\  & DiGress v2 & 0.6892 (0.4100) & 99 & 0.9107 & 0.6336 & 19.4498 & 2.2694 & 0.6531 & 5.0000 \\  & GDSS & 0.6218 (0.5919) & 39 & 0.8415 & 0.2672 & 39.9440 & 1.3788 & 0.5037 & 7.0000 \\  & MOOD & 0.8008 (0.5789) & 99 & **0.9273** & 0.7115 & 34.2506 & 2.0248 & 0.4903 & 8.5000 \\  & Graph DIT-LCC (Ours) & 0.8657 (0.5055) & 99 & 0.8857 & 0.9324 & 11.8587 & 0.3717 & 0.9390 & 2.0000 \\  & Graph DIT (Ours) & 0.8468 (0.8505) & 99 & 0.8856 & 0.9329 & 11.8519 & **0.3551** & **0.9417** & **1.0000** \\    } & Graph GA & 1.0000 (N.A.) & 2879 & 0.8993 & **0.9664** & **4.4418** & 0.9839 & 0.6035 & 5.0000 \\  & MARS & 1.0000 (N.A.) & 26729 & 0.8764 & 0.6517 & 2.2893 & 0.9691 & 0.6455 & 4.0000 \\  & LSTM-HC & 0.9994 (N.A.) & 1329 & 0.8091 & 0.9145 & 7.4669 & 0.9480 & 0.6736 & 0.0000 \\  & ITVAE-BO & 1.0000 (N.A.) & 329 & 0.8055 & 0.4173 & 41.9771 & 1.2359 & 0.4850 & 7.5000 \\   & DiGress & 0.4377 (0.3643) & 22729 & 0.9194 & 0.8562 & 13.0409 & 1.9216 & 0.53135 & 7.5000 \\  & DiGress v2 & 0.0505 (0.4242) & 2429 & 0.9193 & 0.8476 & 13.3997 & 1.5934 & 0.5331 & 7.5000 \\  & GDSS & 0.6926 (0.6575) & 429 & 0.7817 & 0.1032 & 45.416 & 1.2515 & 0.4830 & 8.5000 \\  & MOOD & 0.2875 (0.2173) & 2929 & **0.9280** & 0.1361 & 32.3523 & 2.3144 & 0.5106 & 9.0000 \\   & Graph DIT-LCC (Ours) & 0.7635 (0.7415) & 28729 & 0.8966 & 0.9535 & 5.8790 & **0.3084** & 0.9766 & **1.5000** \\   & Graph DIT (Ours) & 0.7660 (0.7415) & 28729 & 0.8974 & 0.9875 & 6.0216 & 0.3086 & **0.9777** & **1.5000** \\    Figure 3, we present the top three polymers generated by each model for a case study with expertise. Initially, a random forest algorithm identifies the top five polymers per method based on average MAE in two gas permeability. These 25 polymers are then shuffled and evaluated by four polymer scientists, who rank them from 1 to 25 using their domain knowledge. Rankings are normalized to a **Utility Score (UtS)** ranging from 0 to 1, with higher scores indicating greater utility. The variance in UtS is converted into an **Agreement Score (AS)** for further evaluation. As shown in Figure 3, there is a high consensus among experts that the three polymers generated by Graph DiT are the most promising for successful polymer inverse design tasks. More details are in appendix D. By comparing generated examples from different models, we have further observations:

* DiGress and MOOD struggle to capture polymerization points, marked with asterisks ("*"), which is one of the most important features that distinguish polymers from small molecules. Additionally, the two methods frequently feature excessive carbon atoms and overly large cycles. These molecular configurations with significant distortion from the canonical geometry of stable compounds may lead to poor synthesizability .
* LSTM-HC may result in too-long carbon chains with limited diversity. MARS produces examples with asymmetrical graph structures, challenging polymer synthesis .

Figure 4: Relative Performance of Different Model Designs: A higher bar indicates better performance. We use the performance of clustering-based encoding or \(\) as the Reference Value and the current option as the Current Value. Relative performance is calculated as \(}{}\) for Similarity and Diversity metrics, and as \(}{}\) for other metrics.

Figure 3: Polymer Inverse Design for O\({}_{2}\)/N\({}_{2}\) Gas Separation: Feedback from four domain experts includes an average Utility Score (**UtS**) for relative usefulness and an Agreement Score (**AS**) for generated polymers, both ranging . Polymers are generated conditional on {SAS=3.8, SCS=4.3, O\({}_{2}\)Perm=34.0, N\({}_{2}\)Perm=5.2}. The top-3 polymers, highlighted, are all generated by Graph DiT.

* Graph DiT generates structurally diverse and symmetric polymers with two polymerization points, indicative of more valid and synthesizable polymer structures. The first two, which are polyimides, imply effective gas separation performance .

### RQ3: Ablation Studies and Model Analysis

Model ComponentsIn light of Table 1, we analyze **three** components that impact our model's learning in various conditions. Our assessment of relative performance is based on the ratio between our method and comparative approaches. The first component is **numerical conditional encoding**. Results in Figure 4(a) highlight the superiority of clustering encoding over direct and interval-based encoding, particularly in controlling gas permeability, despite its slightly lower diversity. The second component concerns the **neural architecture for conditions**. As shown in Figure 4(b), similar to Figure 4(a), \(\) surpasses both \(Context}\) Conditioning and \(Attention}\) in learning distribution with better condition controllability. The third component validates the importance of the **graph-dependent noise model** compared to separately applying noise to atoms and bonds. It also shows the improvement of the predictor-free Graph DiT over the predictor-guided DiGress v2, even without the graph-dependent noise model. More results on model controllability are in appendix E.

Oracle SelectionsWe analyze the robustness of Oracles in evaluating six task-related properties (three gas permeability and three small molecule properties) across six conditional generation tasks. Oracles are switched from Random Forest to Gaussian Process or Support Vector Machines for ranking generative model performance. Results in Table 3 show consistent rankings (Graph DiT, LSTM-HC, MARS, JTVAE-BO, MOOD, GDSS, GraphGA). It indicates that while perfectly approximating the truth properties of generated molecules is difficult, we could effectively compare the relative performance of various models. Graph DiT consistently ranked first among baselines.

## 5 Related Work

Diffusion Models for Molecules:Score-based diffusion models applied noise and denoising in continuous space [33; 22]. DiGress  used discrete noise as transition matrices based on marginal distributions of atom and bond types. Extra predictor models are studied to guide the generation process in DiGress and GDSS . Diffusion models could also be used for molecular property prediction , for conformation  and molecule generation with 3D atomic coordinates [18; 49; 3]. We focus on molecular graph generation, considering the high computational cost of accurate 3D coordinates for larger molecules like polymers . We explore predictor-free diffusion guidance, instead of the classifier guidance [9; 44], for generating molecules under categorical and numerical conditions. It can be integrated with diffusion models for atomic coordinates in future research.

Molecular Optimization:Optimization algorithms could optimize molecules towards property constraints, including genetic algorithms , Bayesian optimization [39; 50], REINFORCE , and reinforcement learning . Both sequential and graph-based generative models [6; 21; 30], along with diverse sampling methods [47; 13], are used in conjunction with these algorithms to produce desirable molecules. These methods have been applied to both single-objective and multi-objective optimization, the latter by manually integrating multiple property conditions into a single one [5; 25]. Several challenges in molecular optimization methods remain underexplored, including the inadequate or unclear definition of multi-property relations when integration into a single objective , and the inaccessibility of the oracle function for property-oriented optimization during the training phase .

## 6 Conclusion

In this work, we solved inverse molecular design using properties as predictor-free diffusion guidance. The proposed Graph DiT performed diffusion based on the joint distribution of atoms and bonds in both forward and reverse processes. It introduced representation learning for multiple categorical and numerical properties and utilized a Transformer-based graph denoiser for conditional graph denoising. Results on multi-conditional generations and polymer inverse designs showed the remarkable generative capabilities of Graph DiT, making it suitable for designing promising molecules.

   Avg. Rank & Random Forest & Gaussian Process & Support Vector Machines \\ 
**Graph DiT** & **Graph DiT** & **Graph DiT** \\
2 & **LSTM-HC** & **Dediers v2** \\
3 & **MARS** & **Dediers** & **Dediers** \\
4 & **JTVAE-BO** & **LSTM-HC** & **LSTM-HC** \\
5 & **MOOD** & **MARS** & **MARS** \\
6 & **Dediers** & **JTVAE-BO** & **JTVAE-BO** \\
7 & **Dediers v2** & **MOOD** & **MOOD** \\
8 & **GDS** & **GDS** & **GDS** \\
9 & **Graph GA** & **Graph GA** & **Graph GA** \\   

Table 3: Oracles for Generation Evaluation: We consider three Oracles. Generative performance is ranked on average from 1 to 9 across six properties, with various Oracles yielding similar outcomes. We **highlight** models with the same ranking sequence in different Oracle evaluation.