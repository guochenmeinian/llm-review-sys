# LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day

Chunyuan Li\({}^{*}\), Cliff Wong\({}^{*}\), Sheng Zhang\({}^{*}\), Naoto Usuyama, Haotian Liu, Jianwei Yang

**Tristan Naumann, Hoifung Poon, Jianfeng Gao**

Microsoft

https://aka.ms/llava-med

Equal Contribution

###### Abstract

Conversational generative AI has demonstrated remarkable promise for empowering biomedical practitioners, but current investigations focus on unimodal text. Multimodal conversational AI has seen rapid progress by leveraging billions of image-text pairs from the public web, but such general-domain vision-language models still lack sophistication in understanding and conversing about biomedical images. In this paper, we propose a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images. The key idea is to leverage a large-scale, broad-coverage biomedical figure-caption dataset extracted from PubMed Central, use GPT-4 to self-instruct open-ended instruction-following data from the captions, and then fine-tune a large general-domain vision-language model using a novel curriculum learning method. Specifically, the model first learns to align biomedical vocabulary using the figure-caption pairs as is, then learns to master open-ended conversational semantics using GPT-4 generated instruction-following data, broadly mimicking how a layperson gradually acquires biomedical knowledge. This enables us to train a **L**arge **L**anguage **and **V**ision **A**ssistant for Bio**Me**dicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Med exhibits excellent multimodal conversational capability and can follow open-ended instruction to assist with inquiries about a biomedical image. On three standard biomedical visual question answering datasets, fine-tuning LLaVA-Med outperforms previous supervised state-of-the-art on certain metrics. To facilitate biomedical multimodal research, we will release our instruction-following data and the LLaVA-Med model.

## 1 Introduction

Parallel image-text data is abundantly available in the general domain, such as web images and their associated captions. Generative pretraining has proven effective to leverage this parallel data for self-supervised vision-language modeling, as demonstrated by multimodal GPT-4  and open-sourced efforts such as LLaVA . By instruction-tuning models to align with human intents based on multimodal inputs, the resulting large multimodal models (LMMs) exhibit strong zero-shot task completion performance on a variety of user-oriented vision-language tasks such as image understanding and reasoning, paving the way to develop general-purpose multimodal conversational assistants .

While successful in the general domains, such LMMs are less effective for biomedical scenarios because biomedical image-text pairs are drastically different from general web content. As a result, general-domain visual assistants may behave like a layperson, who would refrain from answeringbiomedical questions, or worse, produce incorrect responses or complete hallucinations. Much progress has been made in biomedical visual question answering (VQA), but prior methods typically formulate the problem as classification (_e.g.,_ among distinct answers observed in the training set) and are not well equipped for open-ended instruction-following. Consequently, although conversational generative AI has demonstrated great potential for biomedical applications [20; 33; 19], current investigations are often limited to unimodal text.

In this paper, we present **L**arge **L**anguage **a**and **V**ision **A** Assistant for Bio**M**edicine (LLaVA-Med), a first attempt to extend multimodal instruction-tuning to the biomedical domain for end-to-end training of a biomedical multimodal conversational assistant. Domain-specific pretraining has been shown to be effective for biomedical natural language processing (NLP) applications [18; 15; 11; 31] and biomedical vision-language (VL) tasks [16; 7; 41; 51; 9]. Most recently, large-scale biomedical VL learning has been made possible by the creation of PMC-15M , a broad-coverage dataset with 15 million biomedical image-text pairs extracted from PubMed Central1. This dataset is two orders of magnitude larger than the next largest public dataset, MIMIC-CXR , and covers a diverse image types. Inspired by recent work in instruction-tuning [37; 27], LLaVA-Med uses GPT-4 to generate diverse biomedical multimodal instruction-following data using image-text pairs from PMC-15M, and fine-tune a large biomedical-domain VL model  using a novel curriculum learning method.

Specifically, our paper makes the following contributions:

* _Biomedical multimodal instruction-following data._ We present a novel data generation pipeline to create diverse (image, instruction, output) instances, by sampling biomedical image-text pairs from PMC-15M and using GPT-4 to create instructions from the text alone (which becomes the intended output). This requires zero manual annotations and creates an extremely diverse visual instruction-following dataset by piggybacking on PMC-15 that covers the full spectrum of research findings over biomedical images.
* _LLaVA-Med_. We propose a novel curriculum learning method for adapting LLaVA  to the biomedical domain using our self-generated biomedical multi-modal instruction-following dataset. Specifically, we first fine-tune LLaVA to align biomedical vocabulary using the image-text pairs as is (with the generic instruction that simply asks for a description of the image). We then continue training the model using our self-generated instruction-following data to learn open-ended conversational semantics. In this way, we were able to train LLaVA-Med in less than 15 hours with eight A100s. Our empirical study validates the effectiveness of domain-specific instruction-tuning, and reveals best practice and interesting findings for adapting multimodal conversational assistant to high-value verticals. On well-established biomedical VQA datasets, fine-tuning LLaVA-Med often outperforms supervised state-of-the-art (SoTA).
* _Open-source_. To facilitate research in biomedical multimodal learning, we will release the following assets to the public: the biomedical multimodal instruction-following dataset and the codebase for data generation and model training.

## 2 Related Work

Biomedical Chatbots.Inspired by ChatGPT /GPT-4  and the success of open-sourced instruction-tuned large language models (LLMs) in the general domain, several biomedical LLM chatbots have been developed, including ChatDoctor , Med-Alpaca , PMC-LLaMA , Clinical Camel , DoctorGLM , and Huatuo . They are initialized with open-sourced LLM and fine-tuned on customized sets of biomedical instruction-following data. The resulting LLMs emerge with great potential to offer assistance in a variety of biomedical-related fields/settings, such as understanding patients' needs and providing informed advice.

To our knowledge, Visual Med-Alpaca  is the only existing multimodal biomedical chatbot that accepts image inputs. Though Visual Med-Alpaca and the proposed LLaVA-Med share a similar input-output data format, they differ in key aspects: \((i)\)_Model architectures._ LLaVA-Med is an end-to-end neural model and Visual Med-Alpaca is a system that connect multiple image captioning models with a LLM, using a classifier to determine if or which biomedical captioning model is responsible for the image. The text prompt subsequently merges the converted visual information with the textual query, enabling Med-Alpaca to generate an appropriate response. \((ii)\)_Biomedical_instruction-following data._ While Visual Med-Alpaca is trained on 54K samples from limited biomedical subject domains, LLaVA-Med is trained a more diverse set.

Biomedical Visual Question Answering.An automated approach to building models that can answer questions based on biomedical images stands to support clinicians and patients. To describe existing biomedical VQA methods, we make a distinction between discriminative and generative methods. For discriminative methods, VQA is treated a classification problem: models make predictions from a predefined set of answers. While discriminative methods yield good performance, they deal with closed-set predictions , and require mitigation when a customized answer set is provided in at inference [24; 51; 9]. The discriminative formulation is suboptimal towards the goal of developing a general-purpose biomedical assistant that can answer open questions in the wild. To this end, generative methods have been developed to predict answers as a free-form text sequence [5; 30; 44]. Generative methods are more versatile because they naturally cast the close-set questions as as special case where candidate answers are in language instructions.

Data-Centric Paradigms.LLaVA-Med is similar to prefix tuning of language models (LMs) in  in that a new trainable module connects frozen image encoder and causal LM. In , a three-layer MLP network is used to map the visual features into a visual prefix, and the pre-trained LM are GPT2-XL , BioMedLM  and BioGPT , with size varying from 1.5B to 2.7B. By contrast, LLaVA-Med uses a linear projection and a 7B LM [8; 43]. Most importantly, we undertake _data-centric_ paradigm, while all existing methods are _model-centric_.  focuses efforts on exploring various modeling choices. Our main contributions instead comprise proposing a novel data generation method that uses GPT-4 to self-instruct biomedical multimodal instruction-following data using freely-available broad-coverage biomedical image-text pairs extracted from PubMed Central .

## 3 Biomedical Visual Instruction-Following Data

There are a lack of multimodal biomedical datasets to train an instruction-following assistant. To fill this gap, we create the first dataset of its kind from widely existing biomedical image-text pairs, through a machine-human co-curation procedure. It consists of two sets, concept alignment and instruction-following, which are used at different training stages, described in Section 4.

Biomedical Concept Alignment Data.For a biomedical image \(}\) and its associated caption \(}\), we sample a question \(}\), which asks to describe the biomedical image. With \((},},})\), we create a single-round instruction-following example:

\[:}\ }:}\] (1)

Depending on the length of caption, the question that is sampled either asks to describe the image _concisely_ or _in detail_. Two lists of questions are provided in Appendix A. In practice, 25% of captions have length less than 30 words in PMC-15M , and thus 30 words is used as the cutoff point to determine which list to choose. We sample 600K image-text pairs from PMC-15M. Though this dataset only presents one-single task instructions, _i.e._, image captioning, it contains a diverse and representative set of biomedical concept samples from the original PMC-15M .

Biomedical Instruction-Tuning Data.To align the model to follow a variety of instructions, we present and curate diverse instruction-following data with multi-round conversations about the provided biomedical images, by prompting language-only GPT-4. Specifically, given an image caption, we design instructions in a prompt that asks GPT-4 to generate multi-round questions and answers in a tone as if it could see the image (even though it only has access to the text). Sometimes the image caption is too short for GPT-4 to generate meaningful questions and answers. To provide more context regarding the image, we also create a prompt that includes not only captions but also sentences from the original PubMed paper that mentions the image. We also manually curate few-shot examples in the prompt to demonstrate how to generate high-quality conversations based on the provided caption and context. See Appendix A.2 for the prompt and few-shot examples. To collect image captions and their context, we filter PMC-15M to retain the images that only contain a single plot. From them, we sample 60K image-text pairs from the five most common imaging modalities: CXR (chest X-ray), CT (computed tomography), MRI (magnetic resonance imaging), histopathology, and gross (_i.e.,_ macroscopic) pathology. We then extract sentences that mention the image from the original PubMed paper as additional context to the caption, inspired by the observations that external knowledge helps generalization [21; 28].

An example of instruction-following data is shown in Figure 1 shows, and the data statistics is shown Figure 2. We have produced three versions of instruct data when iteratively improving the data quality: \((i)\)_60K-IM_. The aforemenioned dataset that considers inline mentions (IM) as the context. \((ii)\)_60K_. A dataset of similar size (60K samples) without IM in self-instruct generation. \((iii)\)_10K_. A smaller dataset (10 samples) without IM. They are used to ablate our data generation strategies and their impact on trained LLaVA-Med in experiments.

## 4 Adapting Multimodal Conversational Models to the Biomedical Domain

We employ LLaVA, a general-domain multimodal conversation model , as the initial generalization LM, and continuously train the model to the biomedical domain. The same network architecture is utilized, where a linear projection layer connects the vision encoder and the language model. For LLaVA-Med model training, we use a two-stage procedure, illustrated in Figure 3.

Figure 1: An instance of our GPT-4 generated instruction-following data. Top: The figure and caption were extracted from a PubMed Central full-text article , along with the corresponding citances (mentions of the given figure in the article). Bottom: The instruction-following data generated by GPT-4 using the text only (caption and citances). Note that the image is not used to prompt GPT-4; we only show it here as a reference.

Stage 1: Biomedical Concept Feature Alignment.To balance between concept coverage and training efficiency, we filter PMC-15M to 600K pairs. These pairs are converted to instruction-following data using a naive expansion method: instructions simply presents the task of describing the image. For each sample, given the language instruction and image input, we ask the model to predict the original caption. In training, we keep both the visual encoder and LM weights frozen, and only update the projection matrix. In this way, the image features of vast novel biomedical visual concepts can be aligned to their textual word embeddings in the pre-trained LM. This stage can be understood as expanding the vocabulary of aligned image-text tokens to the biomedical domain. This stage is critical when training LLaVA-Med from customized vision encoder and LLM, where LLaVA initialization is not available. However, when training LLaVA-Med by initializing from LLaVA, we find that this stage can be either skipped to save compute cost, or consider the strategy of removing \(}\) in (1) and tuning LLM for higher performance. Please see the discussion in Appendix C.2.

Stage 2: End-to-End Instruction-Tuning.We only keep the visual encoder weights frozen, and continue to update both the pre-trained weights of the projection layer and LM. To train the model to follow various instructions and complete tasks in a conversational manner, we develop a biomedical chatbot by fine-tuning our model on the biomedical language-image instruction-following data collected in Section 3. As demonstrated in the experiments to be described later, the LLaVA-Med model at this stage is able to not only be served as a biomedical visual assistant to interact with users, but also achieve good zero-shot task transfer performance when evaluated on well-established biomedical VQA datasets.

Discussion.We discuss four favorable properties/implications of LLaVA-Med: \((i)\)_Affordable development cost._ Instead of scaling up data/model for the best performance, we aim to provide affordable and reasonable solutions with low development cost: it takes 7 and 8 hours for stage 1 and 2 on 8 40G A100 GPUs, respectively (see Table 5 for detailed numbers). \((ii)\)_A recipe for many domains._

Figure 2: The data statistics of biomedical multimodal instruction-following data: (a,b) The root verb-noun pairs of instruction and responses, where the inner circle of the plot represents the root verb of the output response, and the outer circle represents the direct nouns. (c) The distribution of images and QA pairs on the five domains, one image is shown per domain. The domain example images are from .

Though this paper focuses on biomedical domains, the proposed adaptation procedure is generalizable to other vertical domains such as gaming and education, where novel concepts and domain knowledge are needed to build a helpful assistant. Similar to the _don't stop pre-training_ argument in , we consider a scalable pipeline to create domain-specific instruct data from large unlabelled data, and advocate _don't stop instruction-tuning_ to build customized LMM. \((iii)\)_Low serving cost_. While the model size of general LMM can be giant and serving cost can be prohibitively high, customized LMM has its unique advantages in low serving cost. \((iv)\)_Smooth Model Adaptation_. Alternatively, the network architecture allows us to initialize the vision encoder from BioMedCLIP , or initialize the language model from Vicuna , which may lead to higher performance. However, adapting from LLaVA smooth adaptation as a chatbot, where model's behaviors transit from layperson to a professional assistant that is able to provide helpful domain-specific response.

## 5 Experiments

We conduct experiments to study two key components, the quality of the produced multimodal biomedical instruction-following data, and performance of LLaVA-Med. We consider two research evaluation settings: (1) What is the performance of LLaVA-Med as an open-ended biomedcal visual chatbot? (2) How does LLaVA-Med compare to existing methods on standard benchmarks? To clarify, throughout the entire experiments, we only utilize the language-only GPT-4.

### Biomedical Visual Chatbot

To evaluate the performance of LLaVA-Med on biomedical multimodal conversation, we construct an evaluation dataset with 193 novel questions. For this test dataset, we randomly selected 50 _unseen_ image and caption pairs from PMC-15M, and generate two types of questions: conversation and detailed description. The conversation data is collected using the same self-instruct data generation pipeline as for the 2nd stage. Detailed description questions were randomly selected from a fixed set  of questions to elicit detailed description responses.

We leverage GPT-4 to quantify the correctness of the model answer to a question when given the image context and caption. GPT-4 makes a reference prediction, setting the upper bound answer for the teacher model. We then generate response to the same question from another LMM. Given responses from the two assistants (the candidate LMM and GPT-4), the question, figure caption, and figure context, we ask GPT-4 to score the helpfulness, relevance, accuracy, and level of details of the responses from the two assistants, and give an overall score on a scale of 1 to 10, where a higher score indicates better overall performance. GPT-4 is also asked to provide a comprehensive explanation the evaluation, for us to better understand the models. We then compute the relative score using GPT-4 reference score for normalization.

The results are reported in Table 1. LLaVA-Med with Stage-1 training alone is insufficient as a chatbot, as it loses its ability to follow diverse instructions, though biomedical concept coverage is improved. LLaVA-Med with the full two-stage training consistently outperforms the general domain LLaVA, and training with larger instruct data (from 10K to 60K samples) leads to higher performance. When inline mentions are considered in self-instruct, the generated data 60K-IM slightly improves the chat ability. The results demonstrate the effectiveness of the strategies in biomedical instruction-following data collection as well as the value of dataset assets. Overall, for the best LLaVA-Med, it matches the 50.2% performance of GPT-4. Note that GPT-4 generates response by considering

Figure 3: LLaVA-Med was initialized with the general-domain LLaVA and then continuously trained in a curriculum learning fashion (first biomedical concept alignment then full-blown instruction-tuning). We evaluated LLaVA-Med on standard visual conversation and question answering tasks.

ground-truth caption and golden inline mentions, without understanding the images. Though not a fair comparison between LMMs and GPT-4, GPT-4 is a consistent and reliable evaluation tool.

Recent studies  raises the self-enhancement bias issue of LLM evaluation, _i.e.,_ favoring its own generations. While we agree with the existence of the bias, we believe that GPT-4 is a meaningful and consistent measurement in our settings. We always compare the answers from the candidate model against the GPT-4's answers when computing the relative socre, and the ranking of the resulting numbers is consistent, though the numbers themselves might be biased to GPT's answers. The conclusions of our ablation study can be obtained by considering the ranking. Further, by taking the self-enhancement bias into consideration for fairness, we expect that LLAVA-Med actually performs even closer to GPT-4 than the current numbers indicate.

In Table 2, we provide examples on the biomed visual conversations of different chatbots. LLaVA-Med precisely answers the questions with biomedical knowledge, while LLaVA behaves like a

  &  &  &  \\  & Conversation & Description & CXR & MRI & Histology & Gross & CT & \\ (Question Count) & (143) & (50) & (37) & (38) & (44) & (34) & (40) & (193) \\  LLaVA & 39.4 & 26.2 & 41.6 & 33.4 & 38.4 & 32.9 & 33.4 & 36.1 \\  LLaVA-Med & & & & & & & & \\ Stage 1 & 22.6 & 25.2 & 25.8 & 19.0 & 24.8 & 24.7 & 22.2 & 23.3 \\
10K & 42.4 & 32.5 & 46.1 & 36.7 & 43.5 & 34.7 & 37.5 & 39.9 \\
60K & 53.7 & 36.9 & 57.3 & 39.8 & 49.8 & 47.4 & 52.4 & 49.4 \\
60K-IM & 55.1 & 36.4 & 56.2 & 40.4 & 52.7 & 51.8 & 50.1 & 50.2 \\ 

Table 1: Performance comparison of mulitmodal chat instruction-following abilities, measured by the relative score via language GPT-4 evaluation.

    \\  

layerson, who hallucinate based on commonsense. Since the multimodal GPT-4 is not publicly available, we resort to language-only GPT-4 for comparison. We feed golden captions and inline mentions into GPT-4 as the context, it generates knowledgeable response through re-organizing the information in the conversational manner.

### Performance on Established Benchmarks

Datasets and Evaluation Metrics.We train and evaluate LLaVA-Med on three biomedical VQA datasets. The data statistics are summarized in Table 8 in Appendix. For the closed-set questions, we report the accuracy/percentage of the ground-truth tokens that appear in the generated sequences. For open-set questions, we use recall to evaluate the ratio that ground-truth tokens appear in the generated sequences. In the literature, the open-set problem is formulated as an closed-set setting, where the unique training answers are considered as the answer candidates, from which the models can select to predict answers for testing questions. Since we do not provide any constraint for the responses to open-set questions, our formulation is closer to open-set nature, but is intrinsically harder.

Comparisons with SoTA.We compare LLaVA-Med with the general domain LLaVA and existing representative methods in Table 3. First, All LLaVA-Med variants outperform LLaVA. While the difference of language model initialization from LLaVA or Vicuna is minor, the initialization of vision encoder from BioMed CLIP is slightly better than from general-domain CLIP. Second, the fine-tuning performance of LLaVA-Med is higher than supervised SoTA on the closed-set questions on VQA-RAD and PathVQA. This validates LLaVA-Med's strong ability in following instruction to complete biomedical tasks, when clear instructions are provided (_e.g.,_, yes or no). Third, for open-set questions, LLaVA-Med achieves SoTA on SLAKE, while its performance is limited on other datasets, especially compared with existing methods. This is perhaps because the open-set biomedical questions can be ambiguous without constraining their excepted answer options. Meanwhile, evaluation of free-form text prediction for medical VQA remains as an open research problem that we are actively exploring.

Ablation Studies.To study the impact of our curated instruction data and hyper-parameters in the training pipeline, we report the performance of different model variants in Table 4 (a). Several findings are confirmed: \((i)\) LLaVA-Med consistently outperforms LLaVA by a large margin, indicating the effectiveness of our biomedical domain-specific adaptation. The performance gaps on zero-shot are larger than that in fine-tuned settings, showing that LLaVA-Med is clearly a better option than LLaVA when deploying one model for various scenarios in the wild. \((ii)\) Training longer in Stage 1 improves zero-shot transfer, but Stage 1 alone is not sufficient, because the single image captioning

  &  &  &  \\ Method & Ref & Open & Closed & Ref & Open & Closed & Ref & Open & Closed \\   \\ LLaVA & 50.00 & 65.07 & 78.18 & 63.22 & 7.74 & 63.20 \\ LLaVA-Med (From LLaVA) & 61.52 & **84.19** & 83.08 & 85.34 & 37.95 & **91.21** \\ LLaVA-Med (From Vicuna) & 64.39 & 81.98 & **84.71** & 83.17 & 38.87 & **91.65** \\ LLaVA-Med (BioMed CLIP) & 64.75 & 83.09 & **87.11** & 86.78 & 39.60 & **91.09** \\   \\ VL Encoder–Decoder  & 71.49 & 82.47 & & 71.49 & 85.61 \\ Q2ATransformer  & 79.19 & 81.20 & & 54.85 & 88.85 \\ Prefix T. Medical LM  & & 84.30 & 82.01 & 40.00 & 87.00 \\ PubMedCLIP  & 60.10 & 80.00 & 78.40 & 82.50 & \\ BiomedCLIP  & 67.60 & 79.80 & 82.05 & 89.70 & \\ M2I2  & 66.50 & 83.50 & 74.70 & 91.10 & 36.30 & 88.00 \\ 

Table 3: Comparison with prior state-of-the-art supervised methods. For open-set questions, we report the recall for our free-form text generation method in column _Open_. For closed-set questions, we report the accuracy in column _Closed_. Bold indicates LLaVA-Med achieves new SoTA. For open-ended questions, prior methods still formulate the problem as classification among distinct answers in the training set, which may overestimate their generalizability as these datasets are unusual in that the test answers are almost always present in training..

  &  &  &  &  \\  & Stage 1 & Stage 2 & FT & Open & Closed & Open & Closed & Open & Closed & \\   \\

[MISSING_PAGE_POST]

  \\
60K-IM & 1 & 3 & 0 & 31.66 & 61.40 & 37.71 & 49.76 & 11.34 & 49.63 & 40.25 \\
60K-IM & 1 & 3 & 9 & 64.58 & 77.94 & 84.97 & 85.58 & 38.82 & 92.39 & 74.05 \\  \\
60K-IM & 1 & 3 & 0 & 37.84 & 60.66 & 39.73 & 54.33 & 11.65 & 49.07 & 42.21 \\
60K-IM & 1 & 3 & 9 & 64.75 & 83.09 & 87.11 & 86.78 & 39.60 & 91.09 & 75.40 \\  LLaVA & 0 & 0 & 0 & 20.74 & 59.19 & 26.82 & 50.24 & 8.74 & 45.65 & 35.23 \\ LLaVA & 0 & 0 & 3 & 50.00 & 65.07 & 78.18 & 63.22 & 7.74 & 63.20 & 54.57 \\  (a) Ablation studies with varying number of training epochs at different stages. 60K-IM indicates the instruct data generated with inline mentions. The gray rows are zero-shot performance of LLaVA-Med trained with different instruct data, they are selected to show in Table 3.

  &  &  &  &  \\ Description & Module & Stage 1 & Stage 2 & FT & Open & Closed & Open & Closed & Open & Closed & \\   \\ N/A & N/A & 0 & 3 & 0 & 23.28 & 62.13 & 36.50 & 56.01 & 7.73 & 58.12 & 40.63 \\   \\ Yes & Projection & 1 & 3 & 0 & 25.79 & 57.35 & 31.50 & 51.68 & 8.49 & 59.66 & 39.08 \\ Yes & LLM & 1 & 3 & 0 & 26.23 & 52.33 & 37.62 & 52.48 & 9.38 & 58.88 & 39.49 \\ No & LLM & 1 & 3 & 0 & 26.88 & 56.16 & 35.23 & 55.78 & 9.39 & 63.27 & 40.97 \\  (b) The impact of Stage-1 training. All jobs are initialized with LLaVA. “Description” indicates whether the description text is included in input in Stage-1. “Module” indicates the trainable module in Stage-1. The numbers in green cells shows the effectiveness of Stage-1 training, though the average scores are similar.

Table 4: Quantitative results on three established biomedical VQA datasets. “Instruct” is the instruct dataset, “Stage 1” and “Stage 2” provides the number of training epochs for each stage, “FT” is Fine-Tuning.

  &  &  &  &  \\
1 & 3 & Instruct & 1 & 3 & 1 & 3 & 1 & 3 & 1 & 3 \\ 
6.8 & 19.4 & 10K & 0.6 & 1.8 & & & & & & & & \\
60K & 2.6 & 8.0 & & & & & & & & & & \\ 

Table 5: Running time (hours) for 1 and 3-epoch training, with batch size 128 on eight A100 GPUs.

instruction in Stage 1 may encourage the model to lose its ability in follow diverse instructions. \((iii)\) Instruction-following data in Stage 2 is critical, and the performance is generally improved, when the instruct data amount increases from 10K to 60K. The 60K-IM data provides the best averaged zero-shot and fine-tuned performance, respectively, validating the effectiveness of considering inline mention as external knowledge in data creation. \((iv)\) Fine-tuning longer on downstream datasets till 9 epochs benefits the performance, especially on checkpoints with 3-epoch training in Stage 2. Increasing language model size from 7B to 13B improves the overall zero-shot performance and fine-tuned performance. We suggest practitioners to choose the appropriate quality-cost trade-off, by referring to the running time in Table 5. \((v)\) When downstream samples are available, fine-tuning itself provides the largest performance gain (54.57-35.23=19.34). However, by training with high quality instruct data such as 60K-IM in the Stage-2, we can further boost performance significantly (65.30-54.57=10.73). Stage-2 itself is not as effective as direct fine-tuning on downstream tasks, meanwhile Stage 1, 2 and fine-tuning are all required for the best performance. \((vi)\) We study the confidence interval (CI) by running the same experiment configuration three times, and report the standard derivation (std) in Table 10 (b). Note it is infeasible to provide CIs for all experiments due to the large number of jobs. Though the std of averaged results of three datasets are small, there is some evidence that results on one single dataset might be statistically significant. We suggest the users with resource to run multiple jobs using the released code to draw more rigorous conclusions for experiments of interest & importance, to alleviate the limitation from the lack of CIs.

Impact of Stage-1.We consider the more strategies to train Stage-1 in addition to tuning the linear projection layer only. Please see the representative results in Table 4 (b), and the detailed discussed in Section C.2. It yields higher average performance at the early epochs such as epoch 1, when training LLaVA-Med from LLaVA using Stage-2 only, without Stage-1. As the training continues to epoch 3 or more, all training methods perform similarly measured by the average scores. However, training with Stage-1 consistently provides higher performance than training without Stage-1 on the PathVQA dataset, which indicates the Stage-1 can benefit certain biomedical domains, when related additional knowledge is learned. Our suggestions on the necessity of Stage-1 training are \((i)\) If LLaVA-Med is trained with a customized vision encoder or LLM that are not included in LLaVA (_i.e.,_ no LLaVA checkpoint is available), Stage-1 is critical in aligning the multimodal feature space, and yield good performance. \((ii)\) If LLaVA-Med is trained by initializing from LLaVA, the Stage-1 training is optional. In this case, it is more cost-efficient to skip Stage-1 and train Stage-2 only, which can quickly provide good performance on the vertical domains with less cost. However, for scenarios with a large number of in-domain image-text pairs that pre-trained LLaVA does not have much related knowledge, we suggest adding the Stage-1 training on the in-domain pairs: The best strategy in this case is full-model fine-tuning of the LLM, and removing the instruction text of describing the image.

## 6 Conclusions

We present LLaVA-Med, a large language-and-vision model for the biomedical domain. To create this model, we create high-quality biomedical language-image instruction-following dataset using a self-instruct approach to build a data curation pipeline using language-only GPT-4 and external knowledge. LLaVA-Med demonstrates strong excellent chat abilities with domain knowledge, and outperforms previous supervised SoTA on three VQA datasets on certain metrics with subsequent fine-tuning.

While we believe that LLaVA-Med represents a significant step towards building a useful biomedical visual assistant, we note that LLaVA-Med is limited by hallucinations and weak in-depth reasoning common to many LMMs. We discuss the limitations of LLaVA-Med and the utilization of GPT-4 API for data generation pipeline in Section B. Future work is directed toward improving quality and reliability of LLaVA-Med. We hope the LLaVA-Med recipe can inspire the applications of training large language-and-vision assistant to more vertical domains.