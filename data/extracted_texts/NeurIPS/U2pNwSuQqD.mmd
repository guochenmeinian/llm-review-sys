# Needle In A Multimodal Haystack

Weiyun Wang\({}^{1,2}\), Shuibo Zhang\({}^{2}\), Yiming Ren\({}^{3,2}\), Yuchen Duan\({}^{4,2}\), Tiantong Li\({}^{3,2}\),

Shuo Liu\({}^{2}\), Mengkang Hu\({}^{7,2}\), Zhe Chen\({}^{5,2}\), Kaipeng Zhang\({}^{2}\), Lewei Lu\({}^{6}\), Xizhou Zhu\({}^{3,2,6}\),

Ping Luo\({}^{7,2}\), Yu Qiao\({}^{2}\), Jifeng Dai\({}^{3,2}\), Wenqi Shao\({}^{2,28}\), Wenhai Wang\({}^{4,28}\)

\({}^{1}\)Fudan University, \({}^{2}\)OpenGVLab, Shanghai AI Laboratory, \({}^{3}\)Tsinghua University,

\({}^{4}\)The Chinese University of Hong Kong, \({}^{5}\)Nanjing University,

\({}^{6}\)SenseTime Research, \({}^{7}\)The University of Hong Kong

###### Abstract

With the rapid advancement of multimodal large language models (MLLMs), their evaluation has become increasingly comprehensive. However, understanding long multimodal content, as a foundational ability for real-world applications, remains underexplored. In this work, we present Needle In A Multimodal Haystack (MM-NIAH), the first benchmark specifically designed to systematically evaluate the capability of existing MLLMs to comprehend long multimodal documents. Our benchmark includes three types of evaluation tasks: multimodal retrieval, counting, and reasoning. In each task, the model is required to answer the questions according to different key information scattered throughout the given multimodal document. Evaluating the leading MLLMs on MM-NIAH, we observe that existing models still have significant room for improvement on these tasks, especially on vision-centric evaluation. We hope this work can provide a platform for further research on long multimodal document comprehension and contribute to the advancement of MLLMs. Code and benchmark are released at https://github.com/OpenGVLab/MM-NIAH.

## 1 Introduction

With the advancements in Large Language Models (LLMs) , significant strides have also been made in Multimodal Large Language Models (MLLMs)  across various vision-language tasks. Recently, some MLLMs  have begun to explore a wider range of applications, from basic dialogue to document-level long context understanding, by leveraging interleaved image-text documents as training corpora. However, due to the limitations of context window size, most existing MLLMs struggle to effectively comprehend long-context multimodal documents. In addition, the lack of appropriate evaluation benchmarks is a key factor that limits the further development of MLLMs for long-context multimodal understanding.

As shown in Fig. 0(a), existing benchmarks for multi-image comprehensions, such as SEED-Bench-2  and BLINK , consist of short contexts, which fail to evaluate the capability for long-context document comprehension. Additionally, benchmarks for video question answering, like MVBench , concentrate on vision-dominant video understanding rather than text-dominant multimodal document understanding (see Fig. 0(b)). Constructing benchmarks for multimodal long-context comprehension poses several challenges. (1) The lack of high-quality multimodal long-context datasets, which require substantial resources and effort to create; (2) The need for evaluation questions that are sufficiently complex to require models to integrate information from the entire long contextto answer correctly; and (3) The fact that existing multimodal models have not been evaluated on long-context multimodal content, highlighting the necessity for robust evaluation protocols to fairly compare the performance of current methods.

In this work, we introduce MM-NIAH, the first benchmark designed to systematically evaluate the comprehension capability of existing MLLMs for long multimodal documents. As shown in Fig. (c)c, MM-NIAH requires the model to answer questions related to the key information scattered throughout the multimodal document. To build this benchmark, we concatenate multiple interleaved image-text documents from OBELICS  into a long-context document containing 1k to 72k image and text tokens. After that, we inject needles containing key information into a certain depth of the text or certain images within the document. To cover both text and image modalities, the proposed MM-NIAH comprises two types of needles (_i.e._, text needles and image needles), where the needles inserted into the text are termed text needles while those inserted into images are termed image needles. For a comprehensive evaluation, we design three types of tasks, including retrieval, counting, and reasoning in our MM-NIAH. The retrieval task requires models to find the key information inserted into the text or images within the document. The counting task contains multiple needles, and the model must collect all needles and count the number of them. The reasoning task asks the model to reason over the cues from multiple needles which are scattered throughout the document.

Based on MM-NIAH, we conduct experiments to evaluate open-source and close-source MLLMs. The experimental results demonstrate that (1) Existing MLLMs perform considerably worse with image needles than with text needles; (2) Existing MLLMs pre-trained on image-text interleaved data do not exhibit superior performance on MM-NIAH compared to those pre-trained only on image-text pair data; (3) MLLMs fail to maintain the long context capability of their underlying LLMs; (4) While RAG enhances performance on text needles, it is ineffective for image needles in the MM-NIAH benchmark. More detailed conclusions and analyses can be found in Section 4.2.

In summary, our main contributions are as follows:

(1) We construct MM-NIAH, the first benchmark designed to systematically evaluate the comprehension capability of existing MLLMs for long multimodal documents, which provides a platform for further research on long multimodal document comprehension.

Figure 1: **Comparison of MM-NIAH with other multi-image benchmarks. Our MM-NIAH focuses on the evaluation of long multimodal document comprehension.**

(2) We extend MLLMs with RAG to serve as a powerful baseline, which greatly enhances the text needles retrieval ability while making trivial improvements for image needles. This demonstrates that the RAG method is unsuitable for our MM-NIAH.

(3) We evaluate the long-context performance of 9 advanced MLLMs on MM-NIAH, where the context length ranges from 1k to 72k. Experimental results reveal that both open-source and closed-source MLLMs struggle to comprehend long multimodal documents accurately, suggesting that long multimodal document comprehension remains a challenging problem.

## 2 Related Work

### Multimodal Large Language Models

Multimodal Large Language Models (MLLMs) have achieved impressive performance across various vision-language tasks, enabling large language models to understand the visual world [8; 11; 13; 15; 17; 28; 29; 30; 31; 32]. In the realm of MLLMs, OpenAI introduced GPT-4V , extending GPT-4's capabilities to incorporate visual inputs. Google's Gemini series evolved from Gemini 1.0  to Gemini 1.5 , enhancing its abilities to process text, images, and audio data. There are also open-sourced MLLMs [15; 18; 19; 20; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45] which has greatly promoted the development of the field. Well-known examples include: BLIP series [9; 10; 46], LLaVA series [11; 12; 47], VisionLLM , Qwen-VL , All-Seeing series [14; 15], and others [16; 17; 48; 49; 50; 51]. However, existing MLLMs are constrained by a limited context window size, impeding their ability to comprehend long multimodal documents. For instance, Emu2  can handle a maximum of 2048 tokens, while InternVL-1.5  can process up to 4096 tokens. This constraint reveals that long-context multimodal understanding remains a significant challenge.

### Multimodal Benchmarks

The rapid advancements in Multimodal Large Language Models (MLLMs) have led to the development of various benchmarks designed to comprehensively assess their multimodal reasoning capabilities. Early benchmarks focused on single tasks [15; 52; 53; 54; 55; 56; 57; 58; 59; 60; 61]. For example, DocVQA  is designed for OCR-centric evaluation, POPE  is designed for hallucination evaluation, and CRPE  is designed for relation comprehension evaluation. Recently, a series of efforts have shifted towards more holistic evaluations. Benchmarks such as MME , LVLM-eHub , SEED-Series [64; 65], MM-Vet , MMBench , MMT-Bench , MMMU  and others [27; 70; 71; 72; 73; 74; 75] have attempted to provide a broader assessment of the reasoning abilities across multiple tasks and modalities of MLLMs. However, these benchmarks are still limited to relatively short contexts, typically consisting of a single image or a short sequence of images and text. Besides, despite the long context introduced by numerous frames, benchmarks designed for long video qa [27; 76; 77; 78; 79] concentrate on vision-dominant video understanding rather than text-dominant multimodal document understanding. Therefore, evaluating the ability to understand long multimodal documents remains an underexplored problem. In this work, we propose MM-NIAH to evaluate the comprehension capability of existing MLLMs for long multimodal documents.

### Needle In A Haystack

The Needle-In-A-Haystack (NIAH) test is a classic method in natural language processing used to evaluate the ability to understand long context. The vanilla NIAH benchmark  introduces a retrieval task where the model is required to retrieve short text (needle) from a long document (haystack). The subsequent works propose a series of more complex tasks, inserting needles containing more information into the documents. For example, BABILong  is built upon the reasoning QA from the bAbI dataset, creating a reasoning-related NIAH benchmark. Experimental results on BABILong demonstrate that Retrieval Augmented Generation (RAG) has no positive impact on reasoning tasks. Counting Stars  requires the model to collect inter-dependency across multiple pieces of evidence spanning the entire context and summarize them into a specified answer. The recent RULER benchmark  introduces four different tasks, including retrieval, multi-hop tracing, aggregation, and question answering, to evaluate the long-context capability from multiple perspectives. These benchmarks are all text-only and struggle to evaluate the long-context understanding ability of MLLMs. Our MM-NIAH is the first multimodal NIAH benchmark designed to evaluate the comprehension ability for long multimodal documents.

## 3 Needle In A Multimodal Haystack

In this section, we introduce Needle In A Multimodal Haystack (MM-NIAH), a benchmark designed to systematically evaluate the comprehension ability for long multimodal documents. This benchmark requires the model to answer specific questions according to the key information scattered throughout the multimodal document. To generate the evaluation data, we first concatenate interleaved image

Figure 2: **Examples for each task in MM-NIAH. Our MM-NIAH consists of three tasks and two types of needles, formulating six types of evaluation data in total. Note that Retrieval-Image-Needle and Reasoning-Image-Needle are formulated as single-choice questions.**

text sequences from OBELICS  to establish the background documents, termed "multimodal haystacks". Then, we generate three data types based on these documents: retrieval, counting, and reasoning. We insert either text needles or image needles into documents for each task.

### Multimodal Haystack

Due to the absence of open-source long multimodal document data, we concatenate the interleaved image-text sequences from OBELICS  to establish the multimodal haystack. The tiktoken 1 is utilized to compute the number of text tokens. For the computation of image tokens, we argue that the image should be considered in the statistics of the context length of a given multimodal document. Besides, images with different resolutions should correspond to different numbers of image tokens, as humans expend varying amounts of effort to understand the information in images of different sizes. Therefore, we use the same method as InternVL-1.5  to split the image into several fixed-size patches while maintaining the aspect ratio as much as possible. Each patch is considered to be 256 image tokens. To ensure that the generated document is not dominated by numerous images, we control the concatenation process so that about every 2k text tokens include one image.

### Multimodal Needle

The evaluation data in MM-NIAH consists of three tasks: retrieval, counting, and reasoning. The needles are inserted into either text or images in the documents. Those inserted into text are termed text needles, whereas those within images are referred to as image needles. All text needles used in MM-NIAH are manually designed. To keep simplicity, each document contains only one type of needle. The data examples for each task are shown in Fig. 2.

**Retrieval.** The text needle in the retrieval task is a random fact or statement inserted into a certain document depth. The corresponding question asks the model to retrieve this statement. The image needle is a random cartoon-style image generated by DALLE-3 , which is inserted into a certain image within the document, and the corresponding question is formulated as a single-choice question. The model is asked to select the image that appears in the document among four image options.

**Counting.** The text needle in the counting task comprises a series of statements, each of which claims the little penguin counted a certain number of needles. For the image needles, a certain number of cartoon-style images are inserted into each image within the document, serving as the needles to be counted. Inspired by the Counting Stars benchmark , we require the model to list the number of needles in each statement or image instead of directly outputting the total number of needles. The motivation behind this design is to ensure that the model accurately retrieves and comprehends all text and image needles inserted into the multimodal document.

**Reasoning.** A series of statements are inserted into different positions of the given document to serve as the text needle. The model must retrieve all these statements and reason over them to answer the question correctly. Besides, for each evaluation data, images sampled from the Jigsaw and Multi-view reasoning split of BLINK benchmark  are inserted into the document to serve as the image needle. The model is required to answer the question related to these images.

Based on the above design, MM-NIAH comprises six types of data in total, each containing approximately 3,000 samples. We generate 40 different needles for text retrieval, 12 for text counting, 57 for text reasoning, 14 for image retrieval and image counting, and 288 for image reasoning. We place these needles into different positions within various documents to create different evaluation samples. When generating evaluation samples, we carefully control the distribution of context length and needle depth to ensure they are as uniform as possible. For visualization in Section 4, each slot in our heatmaps (see Fig. 3) contains around 50 samples to ensure the stability of the evaluation.

### Data Statistics

The data statistics of MM-NIAH are presented in Tab. 1, which summarizes the answer type, number of data samples, and needles inserted into the multimodal haystack for each task. Our benchmark comprises about 12k samples in total. For the multimodal haystack, we limit the maximum number of tokens to 72k with at most 36 images. The number of text needles denotes the number of statementsinserted into the multimodal haystack, while the number of image needles denotes the number of images, which are pasted with a cartoon-style image generated by DALLE-3  or sampled from BLINK , within the document. For the counting task with image needles, even though at most 5 images can be pasted with cartoon-style images, we still require the model to output a list enumerating the number of needles in each image of the document. We argue that this formulation requires the model to understand the details of all images within the document in order to achieve good performance on this task.

### An Improved Baseline with Retrieval Augmented Generation.

We augment InternVL-1.5  with Retrieval Augmented Generation (RAG) as a stronger baseline. Each sample in MM-NIAH consists of a multimodal document and a question-answer pair. Given the multimodal document, we first retrieve a portion of this document conditioned on this question and then ask the model to answer the question based on the retrieved portion instead of the entire document. Specifically, each multimodal document is represented as an interleaved image-text sequence \(x=(x_{1},x_{2},...,x_{n})\), where \(x_{i}\) can be a text sentence or an image. The question \(q\) and text sentences are encoded by the text encoder of InternVL-G , while the images are encoded by the image encoder of InternVL-G. Note that we encode each sentence separately. Subsequently, we obtain the similarity sequence \(s=(s_{1},s_{2},...,s_{n})\), where \(s_{i}\) denotes the cosine similarity between the embeddings of \(q\) and \(x_{i}\). The retrieved portion consists of those \(x_{i}\) with the highest \(s_{i}\), maintaining the relative order within \(x\) and ensuring that the number of retrieved tokens is smaller than the pre-defined length limit. We compute the number of image tokens using the method introduced in Section 3.1.

## 4 Experiments

### Experimental Settings

**Baselines.** We evaluate six leading open-source MLLMs and two leading closed-source MLLMs on our MM-NIAH. Among the open-source MLLMs, we consider LLaVA-1.6 , InternVL-1.5 , VILA , Emu2-Chat , and IDEFICS2  as our baselines. Among these models, LLaVA-1.6 and InternVL-1.5 are trained on image-text pair data without using image-text interleaved data, while VILA, Emu2-Chat, and IDEFICS2 are trained with image-text interleaved data. Note that the training corpora of IDEFICS2 include OBELICS . For the closed-source MLLMs, we consider Gemini-1.5-Flash  and GPT-4V  as baseline models. Due to the constraint that the API of GPT-4V only supports up to 10 images, we evaluate GPT-4V only on our text-needle data. Human performance is also provided as a baseline, which is obtained by asking 10 human experts to each complete a portion of the evaluation data and then merging all the results. They are allowed to use the "Find" operation of the browser during the evaluation process.

**Metrics.** For the retrieval and reasoning tasks, we utilize Accuracy as the evaluation metric, implemented based on the LVLM-eHub . For the counting task, we use Soft Accuracy, defined as \(_{i=1}^{N}}{M_{i}}\), where \(m_{i}\) is the number of matched elements in the corresponding positions between the predicted and ground-truth lists and \(M_{i}\) is the number of elements in the ground-truth list for the \(i\)-th sample. Note that the required output for this task is a list.

**Evaluation.** We evaluate all open-source MLLMs based on the transformers library . During the evaluation process, we evenly split each model into 8 A100 GPUs and use the Flash Attention [90; 91]

  
**Task** & **Needle Type** & **Answer Type** & **\#Samples** & **\#Needles Per Sample** \\   & Text & Open-Ended & 2798 & 1 \\  & Image & Multi-Choice & 2782 & 1 \\   & Text & Open-Ended & 2828 & 1\(\)3 \\  & Image & Open-Ended & 2532 & 1\(\)5 \\   & Text & Open-Ended & 2774 & 3 \\  & Image & Multi-Choice & 2772 & 1\(\)2 \\   

Table 1: **Data statistics of MM-NIAH. “#” denotes the number of something.**to save memory usage. We do not truncate the context and directly input the entire document to these models even if the context length is larger than the max length during their training process.

### Comparison of Advanced MLLMs on MM-NIAH

In this section, we present the evaluation results in heatmap format (see Fig. 3). In the heatmaps, green slots indicate higher performance, while red slots indicate lower performance. The x-axis in Fig. 3 represents context length. We divided the context length into different bins, which form the x-axis of the heatmap. For example, the slot in the top left corner represents accuracy when the given context length ranges from 1K to 2K and the needle depth ranges from 0 to 0.2. Additionally, the average performance across depths for each context length range is presented in table format (see Appendix A). The main findings from these results are detailed as follows.

**Performance degrades while context length increases.** As illustrated in Fig. 3, there is a noticeable decline in model performance as the context length increases. This trend can be observed across all evaluated models and tasks. The degradation is more evident in tasks requiring higher levels of understanding and reasoning, indicating that current models struggle to maintain accuracy when dealing with longer multimodal documents. We also find that contemporary open-source MLLMs can not follow the instructions and begin to produce gibberish when the context is quite lengthy.

**Image needles are much more difficult than text needles.** The results show that models exhibit significantly weaker comprehension capabilities for image needles than text needles. This gap is evident across all tasks, where the performance for image needles remains considerably lower than that for text needles across all models. Although the retrieval and reasoning tasks are formulated as single-choice questions, we find that MLLMs fail to understand the image choices and tend to produce gibberish when the context is lengthy. As a result, the performance may be even worse than random guessing. Besides, in the counting task, we qualitatively observe that the lengths of the predicted list always mismatch with the number of images within the documents. This phenomenon demonstrates that existing MLLMs even struggle to recognize the exact number of images within the documents, suggesting the poor image comprehension ability for long multimodal documents.

**Models pre-trained on image-text interleaved data do not exhibit superior performance.** Experimental results show that models like VILA  and Emu2-Chat , which are trained with image-text interleaved data, do not exhibit substantial performance improvements over models only trained on image-text pairs, such as LLAVA-1.6  and InternVL-1.5 . This indicates that simply training on interleaved data is insufficient for improving long multimodal document understanding, suggesting that alternative approaches or more sophisticated training techniques are necessary.

**The "Lost in the Middle" problem also exists in MLLMs.** The "Lost in the Middle" problem is widely recognized for LLMs , where models perform worse on identifying relevant information in the middle sections of documents compared to the beginning and end sections. When evaluating MLLMs with text needles, we can also observe this trend. The end sections typically show the best performance, followed by the beginning sections, with the middle sections performing the worst. Although this phenomenon is not evident for image needles, we believe this is because the model's overall ability to understand images in multimodal documents is weak, thus not reflecting this trend.

**The most advanced MLLM still struggles to comprehend multimodal documents.** Even Gemini-1.5 , one of the most advanced multimodal models, fails to achieve ideal performance in our MM-NIAH. Notably, the performance in image needles of Gemini-1.5 is also quite poor, with a significant gap compared to human performance. This indicates that there is still significant room for improvement in multimodal document comprehension.

**Long context capability of LLMs is NOT retained in MLLMs.** Despite the powerful ability of open-source LLMs to handle very long context window size (_e.g._, Yi-34B  and InternLM2-20B  with 200K tokens), this capability does not fully transfer to MLLMs. For instance, InternVL-1.5, which is built upon InternLM2, exhibits a decline in performance when dealing with contexts longer than 32k tokens. In contrast, InternLM2 can nearly perfectly find needles in a 200k-long context. Therefore, we believe that enhancing the robustness of MLLMs to maintain high performance across extended contexts remains a critical research direction.

**RAG boosts Text Needle Retrieval but not Image Needle Retrieval.** RAG significantly enhances the capability of InternVL-1.5 in retrieving text needles from long multimodal documents comparedFigure 3: **Results on MM-NIAH.** Green slots indicate higher performance, while red slots indicate lower performance. We evaluate GPT-4V only on our text-needle data because of the constraint that the API of GPT-4V only supports up to 10 images.

to the counterpart without RAG. However, this enhancement does not extend to the retrieval of image needles, where the performance remains poor. For the retrieval and reasoning task, the RAG method might fail to keep the image where the image needle is inserted in the extracted chunks. For the counting task, we require the model to output the number of image needles inserted into each image in the document. Therefore the model has to comprehend all images accurately. However, since the RAG method only extracts a portion of the document, some images might be omitted when the multimodal document is lengthy, leading to the failure in our proposed counting task. These results and analysis demonstrate that RAG methods are unsuitable for the image needles in MM-NIAH as the benchmark requires a comprehensive understanding of all images within the multimodal document.

**Humans achieve near-perfect performance on MM-NIAH.** As shown in the bottom of Fig. 3, humans achieve near-perfect performance on MM-NIAH, highlighting the gap between human-level comprehension and the abilities of current MLLMs. It is important to note that achieving perfect performance on MM-NIAH does not equate to perfect long document understanding ability. However, it serves as a prerequisite for achieving long multimodal document understanding ability.

**Training on background documents does not boost performance on MM-NIAH.** Since the multimodal haystack in MM-NIAH is obtained by concatenating interleaved image-text sequences from OBELICS , a general concern is the potential data contamination for models trained with OBELICS . However, evaluation results of IDEFICS2  on MM-NIAH show that IDEFICS2, compared to other models, does not demonstrate any performance advantage. We attribute this to the fact that while models trained on OBELICS may have a better understanding of these documents, the text and image needles are newly inserted into these documents, and the generated questions are only related to this newly inserted content, effectively avoiding the risk of data contamination.

**MLLMs fail to recognize the exact number of images in the document.** As discussed above, we qualitatively observe that the lengths of the predicted list from MLLMs always mismatch with the number of images within the documents. To analyze this phenomenon quantitatively, we ask Gemini-1.5  to output the number of images contained in the given document and compute the accuracy. The experimental results are depicted in Fig. 4. We can observe that the accuracy decreases as the number of images in the context grows, indicating the poor performance of Gemini-1.5 in recognizing the exact number of images in the document. Notably, when the number of images within the document exceeds five, the accuracy drops below 10%. This suggests that a major reason for the poor model performance in tasks with image needles is that current models fail to recognize the exact number of images in the document.

## 5 Conclusion & Limitation

In this work, we propose MM-NIAH, the first benchmark designed to systematically evaluate the comprehension ability for long multimodal documents. MM-NIAH comprises three types of evaluation tasks: multimodal retrieval, counting, and reasoning. The model is required to answer specific questions according to the key information scattered throughout the given multimodal document. Evaluating the leading MLLMs on MM-NIAH, we observe that existing models still have significant room for improvement on these tasks, especially on vision-centric evaluation. We also demonstrate that the RAG method is unsuitable for the image needles in MM-NIAH, suggesting that the long multimodal document comprehension remains a non-trivial problem for MLLMs.

Regarding limitations, the long multimodal documents in MM-NIAH only serve as the background, and the answer only relates to the needles inserted into it. The construction of evaluation data related to the entire document will leave for future work.

**Broader Impact.** We hope this work can provide a platform for further research on long multimodal document comprehension and contribute to the advancement of MLLMs. We do not foresee obvious undesirable ethical/social impacts at this moment.

Figure 4: **Accuracy of Gemini-1.5 to output the number of images in context.**