# Minimax Optimal Rate for Parameter Estimation in Multivariate Deviated Models

Dat Do*

Department of Statistics

University of Michigan at Ann Arbor

Ann Arbor, MI 48109

dodat@umich.edu

&Huy Nguyen*

Department of Statistics and Data Sciences

The University of Texas at Austin

Austin, TX 78712

huynm@utexas.edu

&Khai Nguyen

Department of Statistics and Data Sciences

The University of Texas at Austin

Austin, TX 78712

khainb@utexas.edu

&Nhat Ho

Department of Statistics and Data Sciences

The University of Texas at Austin

Austin, TX 78712

minhnhat@utexas.edu

 Equal contribution.

###### Abstract

We study the maximum likelihood estimation (MLE) in the multivariate deviated model where the data are generated from the density function \((1-^{*})h_{0}(x)+^{*}f(x|^{*},^{*})\) in which \(h_{0}\) is a known function, \(^{*}\) and \((^{*},^{*})\) are unknown parameters to estimate. The main challenges in deriving the convergence rate of the MLE mainly come from two issues: (1) The interaction between the function \(h_{0}\) and the density function \(f\); (2) The deviated proportion \(^{*}\) can go to the extreme points of \(\) as the sample size tends to infinity. To address these challenges, we develop the _distinguishability condition_ to capture the linear independent relation between the function \(h_{0}\) and the density function \(f\). We then provide comprehensive convergence rates of the MLE via the vanishing rate of \(^{*}\) to zero as well as the distinguishability of two functions \(h_{0}\) and \(f\).

## 1 Introduction

The goodness-of-fit test  is one of the foundational tools in statistics with several applications in data-driven scientific fields, namely kernel Stein discrepancy [27; 31], point processes  and Bayesian statistics , etc. Given a sample set of data and a pre-specified distribution with density function \(h_{0}\), the test indicates whether the samples are reasonably distributed according to \(h_{0}\) (_null hypothesis_) or to another family of distributions \(\{p(|):\}\) (_alternative hypothesis_). It is worth noting that knowledge about the null hypothesis distribution can come from prior knowledge of scientists. A key to understanding the statistical efficiency of testing is via the likelihood ratio and the maximum likelihood estimation (MLE) methods. .

While traditional testing problems often assume the null distribution \(h_{0}=p(|_{0})\) and the alternative one \(p(|)\) are from a single simple family of distributions such as exponential families, there are also many problems in science require to test \(h_{0}\) against the alternative \(f(|)\) that can be _deviated_ from \(h_{0}\) by a distribution from a potentially different family. Specifically, in this paper, we consider the family of distributions named _multivariate deviated model_ with density functions defined as follows:

\[p_{G}(x):=(1-)h_{0}(x)+ f(x|,),\] (1)where \(x^{d}\), \(G:=(,,)\) are the model's parameters with \(\) being the _deviated proportion_ (from \(h_{0}\)) and \((,)\) are parameters of a vector-matrix family of distributions \(f\), where \(^{d}\) and \(^{d d}\) being compact. When \(=0\), this recovers the null hypothesis distribution \(h_{0}\).

The deviated model can be motivated by many applications in science. For instance, in microarray data analysis, it can be used to detect differentially expressed genes under two or more conditions , where \(h_{0}\) is the uniform distribution and \(f(|,)\) is required to estimate. Many other applications can be seen in many contamination problems in astronomy and biology . Besides, the deviated model can also be viewed as a low-rank adaptation model in the domain adaptation problem , where \(h_{0}\) is a pre-trained model on large data, and \(f\) is a simpler component to be estimated from the smaller data domain. Our goal in this paper is to study the parameter estimation rate of the deviated model.

**Problem setup.** Suppose that we observe \(n\) i.i.d. samples \(X_{1},,X_{n}\) from the true multivariate deviated model:

\[p_{G_{*}}(x):=(1-^{*})h_{0}(x)+^{*}f(x|^{*},^{*}),\] (2)

where \(G_{*}:=(^{*},^{*},^{*})\) are true but unknown parameters with \(^{*} 0\). Throughout the paper, we allow \(G_{*}\) to change with the sample size \(n\) (see Appendix F.1 for a discussion). To facilitate our presentation, we suppress the dependence of \(G_{*}\) on \(n\), and then estimate \(G_{*}\) from the data. The main focus of this paper is to establish both a uniform convergence rate and minimax rate for parameter estimation via the MLE approach, which is given by:

\[_{n}*{arg\,max}_{G}_{i=1}^{n} p_{G}( X_{i}),\] (3)

where \(_{n}:=(_{n},_{n},_{ n})\) and \(:=\).

**Contribution.** There are two main challenges in studying the convergence rate of the MLE \(_{n}\): (1) The interaction between the function \(h_{0}\) and the density function \(f\), e.g., \(h_{0}\) belongs to the family of \(f\) and \((^{*},^{*})\) approaches \(h_{0}\) as the sample size \(n\) goes to infinity; (2) The deviated proportion \(^{*}\) can go to the extreme points of \(\) as the sample size goes to infinity and make the estimation become more challenging, because when \(^{*}=0\), all the parameters \((^{*},^{*})\) yield the same model. To address these _singularity_ and _identifiability_ issues, we first develop the _distinguishability condition_ to capture the linear independent relation between the function \(h_{0}\) and the density function \(f\). We then study the optimal convergence rate of parameters under both distinguishable and non-distinguishable settings of the multivariate deviated model. Our theoretical results can be summarized as follows:

**1. Distinguishable settings:** We demonstrate that as long as the function \(h_{0}\) and the density function \(f\) are distinguishable, the convergence rate of \(_{n}\) to \(^{*}\) is \((n^{-1/2})\) while \((_{n},_{n})\) converges to \((^{*},^{*})\) at a rate determined by the vanishing rate of \(^{*}\) as follows:

\[^{*}\|(_{n},_{n})-(^{*},^{*})\|= (n^{-1/2}).\]

It indicates that if \(^{*}\) goes to 0, the convergence rate of estimating \((^{*},^{*})\) is slower than the parametric rate.

**2. Non-distinguishable settings:** When \(h_{0}\) and \(f\) are not distinguishable, it becomes complicated to capture the convergence rate of the MLE. To shed light on the behaviors of the MLE under the non-distinguishable settings of multivariate deviated model, we specifically study the settings when \(h_{0}\) belongs to the same family as \(f\), namely, \(h_{0}(.)=f(.|_{0},_{0})\) for some \((_{0},_{0})\). To precisely characterize the rates of the MLE under this setting, we consider the second-order strong identifiability of \(f\), which requires the linear independence up to second-order derivatives of \(f\) with respect to its parameters. The second-order identifiability had also been considered in the literature to investigate the convergence rate of parameter estimation in finite mixtures .

**2.1. Strongly identifiable and non-distinguishable settings:** When \(f\) is strongly identifiable in the second order, we demonstrate that \(\|(^{*},^{*})\|^{2}|_{n}-^{*}|= (n^{-1/2})\) and

\[^{*}(\|(^{*},^{*})\|+\|(_{n}, _{n}\|)\|(_{n},_{n})-(^ {*},^{*})\|=(n^{-1/2}),\]where \(:=-_{0}\) and \(:=-_{0}\). It indicates that the convergence rate of \(_{n}\) to \(^{*}\) depends on that of \((^{*},^{*})\) to \((_{0},_{0})\) while the convergence rate of \((_{n},_{n})\) to \((^{*},^{*})\) depends on both the rate of \(^{*}\) to 0 and the rate of \((^{*},^{*})\) to \((_{0},_{0})\). These results are strictly different from those in the distinguishable settings, which is mainly due to the non-distinguishability between \(h_{0}\) and \(f\).

**2.2. Weakly identifiable and non-distinguishable settings:** When \(f\) is weakly identifiable, i.e., it is not strongly identifiable in the second order, we specifically consider the popular setting when \(f\) is the density of a multivariate Gaussian distribution. The loss of the strong identifiability of the Gaussian distribution is due to the following partial differential equation (PDE) between the location and scale parameters (the heat equation):

\[f}{^{}}(x|,)=2(x|,).\]

Due to the above PDE, the convergence rate of the MLE under this setting exhibits very different behaviors from those under the strongly identifiable setting. In particular, we prove that \(\|^{*}\|^{4}+\|^{*}\|^{2}\,|_{n}-^{*}|=(n^{-1/2})\) and

\[^{*}(\|^{*}\|^{2}+\|_{n}\|^{2}+\| ^{*}\|+\|_{n}\|)(\|_{n}-^{*}\|^{2 }+\|_{n}-^{*}\|)=(n^{-1/2}).\]

Notably, there is a mismatch in the orders of convergence rates of the location vector and covariance matrix. Furthermore, the rate of the deviated mixing proportion also depends on different orders of \(^{*}\) to \(_{0}\) and \(^{*}\) to \(_{0}\). Such rich behaviors of the MLE are mainly due to the PDE between the location and scale parameters.

**Comparing to moment methods.** We would like to remark that the results for the MLE under the non-distinguishable settings in the paper are (much) tighter than those obtained from moment methods for a general mixture of two components in the literature. In particular, when \(f\) is multivariate Gaussian distribution with fixed covariance matrix, i.e., \(f\) is strongly identifiable in the second order and we do not estimate \(^{*}\), an application of the results with moment methods from  to the deviated models leads to \(\|^{*}\|^{3}|_{n}^{}-^{*}|=(n ^{-1/2})\) and \(^{*}\|_{n}^{}-^{*}\|^{3}=(n^{-1/2})\), which are much slower compared to the results for the MLE in the strongly identifiable and non-distinguishable settings, where \((_{n}^{},_{n}^{})\) denote moment estimators of \(^{*}\) and \(^{*}\).

When \(f\) is a multivariate Gaussian density and we estimate both the location vector and covariance matrix, i.e., \(f\) is weakly identifiable, an adaptation of the moment estimators from the seminal work  to the multivariate deviated models shows that \(^{*}\|\|_{n}^{}-^{*}\|^{6}=(n^{ -1/2})\), \(^{*}\|\|_{n}^{}-^{*}\|^{3}=(n^{-1/2})\) and \((\|^{*}\|^{6}+\|^{*}\|^{3})|_{n}^{}-^{*}|=(n^{-1/2})\), where \((_{n}^{},_{n}^{}, _{n}^{})\) are moment estimators of \((^{*},^{*},^{*})\). These results are also much slower than those of the MLE in weakly identifiable settings.

**Other related work.** The hypothesis testing and MLE problem related to the multivariate deviated model had been considered in previous work, including the problem of detecting sparse homogeneous and heteroscedastic mixtures , the problem of determining the number of components , and the problem of multiple testing . In particular,  considers testing problem for the deviated model with \(h_{0}=N(0,1)\) and \(f=N(^{*},1)\) being one-dimensional Gaussian distributions. They show that no test can reliably detect \(^{*}=0\) against \(^{*}>0\) if \(^{*}^{*}=o(n^{-1/2})\), while the Likelihood Ratio test can consistently do it when \(^{*}^{*} n^{-1/2+}\) for any \(>0\). However, no guarantee for estimation of \(^{*}\) and \(^{*}\) is provided. In the same setting where \(f\) is the density of a location Gaussian distribution, the convergence rate of parameter estimation in the deviated model had been studied in the work of . Since the location Gaussian distribution is a special case of the strongly identifiable distribution, our result in the strongly identifiable and non-distinguishable settings is a generalization of the results in , but with a different proof technique as their proof technique relies strictly on the properties of the location Gaussian distribution.

**Organization.** The paper is organized as follows. In Section 2, we provide background on the identifiability and density estimation rate of the multivariate deviated model. Then, we establish the lower bounds of the Total Variation distance between two densities in terms of loss functions among parameters under both the distinguishable and non-distinguishable settings in Section 3. Next, we characterize the convergence rates of parameter estimation as well as derive the corresponding minimax lower bounds in Section 4. In Section 5, we carry out a simulation study to empirically verify our theoretical results before concluding the paper in Section 6. Rigorous proofs and additional results are deferred to the supplementary material.

**Notations.** For any \(a,b\), we denote \(a b:=\{a,b\}\) and \(a b:=\{a,b\}\). Next, we say that \(h_{0}\) is identical to \(f\) if \(h_{0}(x)=f(x|_{0},_{0})\) for some \((_{0},_{0})\). For each parameter \(G\), let \(_{p_{G}}\) be the expectation taken with respect to product measure with density \(p_{G}\). Lastly, for any two density functions \(p\) and \(q\) (with respect to the Lebesgue measure \(m\)), the Total Variation distance between them is given by \(V(p,q):=|p(x)-q(x)|(x)\), while we define their squared Hellinger distance as \(h^{2}(p,q):=[-]^{2}dm(x)\).

## 2 Preliminaries

### Identifiability Condition

Our principal goal in this paper is to assess the statistical efficiency of parameter estimation from the MLE method. To do that, we should be able to guarantee the parameter identifiability of the deviated model (2), i.e., if \(p_{G}(x)=p_{G_{*}}(x)\) for almost surely \(x\) where \(G=(,,)\), then \(G G_{*}\). That identifiability condition leads to the following notion of distinguishability between the density function \(h_{0}()\) and the family of density functions \(\{f(|,):(,)\}\).

**Definition 2.1** (Distinguishability).: We say that the family of density functions \(\{f(|,),(,)\}\) (or in short, \(f\)) is _distinguishable_ from \(h_{0}\) if the following holds:

* For any two distinct components \((_{1},_{1})\) and \((_{2},_{2})\), if we have real coefficients \(_{i}\) for \(1 i 3\) such that \(_{1}_{2} 0\) and \(_{1}f(x|_{1},_{1})+_{2}f(x|_{2},_{2})+_{3}h_{0} (x)=0\), for almost surely \(x^{d}\), then \(_{1}=_{2}=_{3}=0\).

We can verify that as long as \(f\) is distinguishable from \(h_{0}\), the parameter identifiability of our multivariate deviated model follows. In particular, assume that there exists \(G=(,,)\) such that

\[(1-^{*})h_{0}(x)+^{*}f(x|^{*},^{*})=(1-)h_{0}(x )+ f(x|,),\] (4)

for almost surely \(x\). The above equation is equivalent to \((-^{*})h_{0}(x)+^{*}f(x|^{*},^{*})- f(x| ,)=0\). Assume that \(f\) is distinguishable from \(h_{0}\), then equation (4) indicates that if \((,)(^{*},^{*})\), we have \(=^{*}=0\). Since \(^{*} 0\) from our assumption, we obtain that \((,)=(^{*},^{*})\). As a result, equation (4) becomes \((-^{*})h_{0}(x)+(^{*}-)f(x|,)=0\). By applying the distinguishability condition again, we get \(=^{*}\). Therefore, the multivariate deviated model (2) is identifiable.

In the following example, we will verify the distinguishability condition in Definition 2.1 given some specific choices of function \(h_{0}\) and density \(f\).

**Example 2.2**.: (a) Assume that \(f\) belongs to a location family of density functions, i.e., \(f(x|,)=f_{}(x-)\) for all \(x\) where \(\) is a fixed covariance matrix. If \(h_{0}(x) f(x)\) for almost surely \(x\), then \(f\) is distinguishable from \(h_{0}\).

(b) When \(h_{0}\) is a finite mixture of multivariate Gaussian densities and \(f\) belongs to a class of multivariate Student's density functions with any fixed odd degree of freedom \(>1\), we get that \(f\) is distinguishable from \(h_{0}\).

(c) When \(f\) is identical to \(h_{0}\), then \(f\) is not distinguishable from \(h_{0}\).

### Convergence Rate of Density Estimation

Our strategy to obtain the convergence rate of the MLE \(_{n}\) is by first establishing the convergence rate of density \(p_{_{n}}\) and then studying the geometric inequalities between the parameter space and density space. For the former, the standard method is to use the empirical process theory [17; 33], while for the latter step, we investigate those inequalities under various settings of distinguishability in Section 3. Due to space constraints and the popularity of empirical process theory, we choose to informally present a main result for yielding the parametric convergence rate for density estimation in this section. For full explanation and definition, readers are referred to Appendix B. The convergence rate for density estimation can be characterized by bounding the complexity of the parameter space \(\) via a function called _bracketing entropy integral_\(_{B}(,}^{1/2}(,))\) (cf. equation (8)).

**Theorem 2.3**.: _Assume the following assumption holds:__._
2. _Given a universal constant_ \(J>0\)_, there exists_ \(N>0\)_, possibly depending on_ \(\)_, such that for all_ \(n N\) _and all_ \(>((n)/n)^{1/2}\)_, we have_ \(_{B}(,}^{1/2}(,)) J ^{2}\)_._

_Then, there exists a constant \(C>0\) depending only on \(\) such that for all \(n 1\),_

\[_{G_{*}}_{p_{G_{*}}}h(p_{_{n}},p_{G_{*}}) C .\]

Therefore, in order to get the convergence rate for density estimators based on the MLE method, we only need to check Assumption A2, which holds true for several parametric models . For our model, we give an example that it holds for a general class of \(f\) and \(h_{0}\).

**Proposition 2.4**.: _Suppose that both \(\) and \(\) are compact, and \(\{f(x|,):,\}\) is a vector-matrix family of densities being uniformly bounded, Lipschitz, and light tail, i.e. there exists constants \(M,L,B,b_{1},b_{2},b_{3}>0\) such that \(|f(x|,)| M,|f(x|,)-f(x|^{},^{})|  L(\|-^{}\|+\|-^{}\|)\) for all \(x^{d}\), and_

\[|f(x|,)| b_{1}(-b_{2}\,\|x\|^{b_{3}})\ \|x\|>B,\]

_for all \((,)\). Additionally, if the density \(h_{0}\) is bounded, then the corresponding multivariate deviated model defined in equation (1) satisfies assumption A2._

**Example 2.5**.: We can check that the location-scale Gaussian density \(f(x|,)\) with \(\) having eigenvalues bounded below by a positive constant satisfies the condition of Proposition 2.4. This condition for \(h_{0}\) is mild and is satisfied by most distributions such as Gaussian and t-distribution.

## 3 From the Convergence Rate of Densities to Rate of Parameters

The objective of this section is to develop a general theory according to which a small distance between \(p_{G}\) and \(p_{G}\), under the Hellinger distance (or Total Variation distance) would imply that \(G\) and \(G_{*}\) are also close under appropriate distance where \(G=(,,)\) and \(G_{*}=(^{*},^{*},^{*})\). By combining those results with Theorem 2.3, we can obtain the convergence rate for parameter estimation (cf. Section 4). The distinguishability condition between \(h_{0}\) and \(f\) implicitly requires that \(p_{G}=p_{G_{*}}\) would entail \(G=G_{*}\); however, to obtain quantitative bounds for their Total Variation distance, we need stronger notions of both distinguishability and classical parameter identifiability, ones which involve higher order derivatives of the densities \(h_{0}\) and \(f\), taken with respect to mixture model parameters. Throughout the rest of this section, we denote \(G=(,,)\) and \(G_{*}=(^{*},^{*},^{*})\).

### Distinguishable Settings

**Definition 3.1** (**First-order Distinguishability**).: We say that \(f\) is distinguishable from \(h_{0}\) up to the first order if \(f\) is differentiable in \((,)\), and the following holds:

1. For any component \((^{},^{})\), if we have real coefficients \(,_{}\) for all \(=(_{1},_{2})^{d_{1}}^{d_{2}  d_{2}}\), \(||=|_{1}|+|_{2}| 1\) such that \[ h_{0}(x)+_{|| 1}_{}f}{ ^{_{1}}^{_{2}}}(x|^{},^{ })=0\] for all \(x\), then \(=_{}=0\) for all \(|| 1\).

We can verify that the examples from part (a) and part (b) of Example 2.2 satisfy the first-order distinguishability condition. Next, we introduce a notion of uniform Lipschitz condition in the following definition.

**Definition 3.2** (**Uniform Lipschitz).: We say that \(f\) admits uniform Lipschitz condition up to the first order if the following holds: there are positive constants \(_{1},_{2}\) such that for any \(R_{1},R_{2},R_{3}>0,_{1}^{d_{1}},_{2}^{d _{2} d_{2}},R_{1}_{}^{1/2}(_{1})_{}^ {1/2}(_{2}) R_{2}\), \(\|_{1}\|,\|_{2}\| R_{3}\), \(_{1},_{2}\),\(_{1},_{2}\), we can find positive constants \(C(R_{1},R_{2})\) and \(C(R_{3})\) such that for all \(x\),

\[|_{1}^{}((x| _{1},)-(x|_{2},))|  C(R_{1},R_{2})\|_{1}-_{2}\|^{_{1}}\|_{1}\|,\] \[|(((x| ,_{1})-(x|,_{2}))^{ }_{2})| C(R_{3})\|_{1}-_{2}\|^{_{2}} \|_{2}\|.\]Now, we have the following results characterizing the behavior of \(V(p_{G},p_{G_{*}})\) regarding the variation of \(G\) and \(G_{*}\).

**Theorem 3.3**.: _Assume that \(f\) is distinguishable from \(h_{0}\) up to the first order. Furthermore, \(f\) admits uniform Lipschitz condition up to the first order. For any \(G\) and \(G_{*}\), we define_

\[(G,G_{*}):=|-^{*}|+(+^{*}) \|(,)-(^{*},^{*})\|.\]

_Then, the following holds:_

\[C.(G,G_{*}) V(p_{G},p_{G_{*}}) C_{1}.(G,G_{*}),\]

_for all \(G\) and \(G_{*}\), where \(C\) and \(C_{1}\) are two positive constants depending only on \(\), \(\), and \(h_{0}\)._

See Appendix C.1 for the proof of Theorem 3.3. Since the MLE approach yields the convergence rate \(n^{-1/2}\) up to some logarithmic factor for \(p_{G_{*}}\) under the first order uniform Lipschitz condition of \(f\), the result of Theorem 3.3 directly yields the convergence rate \(n^{-1/2}\) up to some logarithmic factor for \(G_{*}\) under metric \(\). This entails that the estimation of weight \(_{*}\) converges at rate \(n^{-1/2}\) up to some logarithmic factor while the convergence rate of estimating \((^{*},^{*})\) is typically much slower than \(n^{-1/2}\) as it depends on the rate of convergence of \(^{*}\) to 0 (cf. Theorem 4.1).

### Non-distinguishable Settings

When \(f\) is not distinguishable to \(h_{0}\) up to the first order, the bound in Theorem 3.3 may not hold in general. In this section, we investigate the inverse bounds under the specific settings of non-distinguishable in the first-order models when \(h_{0}\) belongs to the family \(f(.|,)\), i.e., \(h_{0}(x)=f(x|_{0},_{0})\) for some \((_{0},_{0})\). Our studies are divided into two separate regimes of \(f\): the first setting is when \(f\) is strongly identifiable in the second order (cf. Definition 3.4), while the second setting is when it is not. For the simplicity of the presentation in the paper, we define \((,)=(-_{0},-_{0})\) for any element \((,)\).

**Definition 3.4** (Strong Identifiability).: We say that \(f\) is strongly identifiable in the second order if \(f\) is twice differentiable in \((,)\) and the following holds:

* For any positive integer \(k\), given \(k\) distinct pairs \((_{1},_{1}),,(_{k},_{k})\), if we have \(_{}^{(i)}\) such that \[_{=0}^{2}_{||=}_{i=1}^{k}_{}^{(i)}}{^{_{1}}^{_{2}}}(x|_{i },_{i})=0,\] for almost all \(x\), then \(_{}^{(i)}=0\) for all \(i[k]\) and \(|| 2\).

#### 3.2.1 Strongly Identifiable Settings

Now, we have the following result regarding the lower bound of \(V(p_{G},p_{G_{*}})\) under the strongly identifiable settings of \(f\).

**Theorem 3.5**.: _Assume that \(h_{0}(x)=f(x|_{0},_{0})\) for some \((_{0},_{0})\) and \(f\) is strongly identifiable in the second order and admits uniform Lipschitz condition up to the second order. Furthermore, we denote_

\[(G,G_{*}) :=\|(,)\|^{2}+^{*}\|(^ {*},^{*})\|^{2}-\{,^{*}\}(\|( ,)\|^{2}\|(^{*},^{*})\|^{2})\] \[+(\|(,)\|+^{*}\|( ^{*},^{*})\|)\|(,)-(^{*},^{*})\|\]

_for any \(G\) and \(G_{*}\). Then, there exists a positive constant \(C\) depending only on \(\), \(\), and \((_{0},_{0})\) such that \(V(p_{G},p_{G_{*}}) C.(G,G_{*})\), for all \(G\) and \(G_{*}\)._

The proof of Theorem 3.5 and the second-order uniform Lipschitz condition are deferred to Appendix C.2. Several remarks regarding Theorem 3.5 are in order:

**(i)** For any \(G\) and \(G_{*}\), by defining

\[}(G,G_{*}) :=|^{*}-|\|(,)\|\|(^{ *},^{*})\|\] \[+\|(,)-(^{*},^{*})\|\|(,)\|+^{*}\|(^{*},^{*})\|\]we can verify that \(1/2(G,G_{*})/(G,G_{*}) 2\), i.e., \((G,G_{*})}(G,G_{*})\). The reason that we prefer to use the formation of \((G,G_{*})\) over that of \(}(G,G_{*})\) is not only due to the convenience of the proof argument of Theorem 3.5 later in Appendix C but also due to its partial connection with Wasserstein metric that we are going to discuss in the next remark.

**(ii)** When \(f\) is a multivariate location family and is identical to \(h_{0}\), i.e., \(_{0}=\), it was demonstrated recently in  that

\[V(p_{G},p_{G_{*}})|-^{*}|\||\|\|^{*}\|+(^{*} \|^{*}\|+\||)\|-^{*}\|,\] (5)

which is also the key result for establishing the convergence rates of parameter estimation in their work. However, their proof technique only works for the location family and it is unclear what is the sufficient condition for the family of density functions beyond the location family such that the inequality (5) will hold. As the location family is strongly identifiable in the second order, we can verify that the lower bound in Theorem 3.5 and inequality (5) are in fact similar. Therefore, the result in Theorem 3.5 gives a generalization of inequality (5) in  under the strongly identifiable in the second order setting of \(f\).

**(iii)** As indicated in , we can further lower bound the right-hand side of inequality (5) in terms of the second order Wasserstein metric \(W_{2}\) between \(G\) and \(G_{*}\) when we present \(G\) and \(G_{*}\) as two discrete probability measures with two components. In particular, with an abuse of the notations we denote that \(G=(1-)_{(_{0},_{0})}+_{(,)}\) and \(G^{*}=(1-)_{(_{0},_{0})}+_{(^{*},^ {*})}\), i.e., we think of \(G\) and \(G_{*}\) as two mixing measures with one fixed atom to be \((_{0},_{0})\). In light of Lemma E.1 in Appendix E, we have

\[W_{2}^{2}(G,G_{*})\|(,)\|^{2} +^{*}\|(^{*},^{*})\|^{2}\] \[-\{,^{*}\}\|( ,)\|^{2}+\|(^{*},^{*})\|^{2}+ \{,^{*}\}\|\|(,)-(^{*},^{*})\| ^{2}.\]

Therefore, \((G,G_{*})\) and \(W_{2}^{2}(G,G_{*})\) share the similar term \(\|(,)\|^{2}+^{*}\|(^{*}, ^{*})\|^{2}-\{,^{*}\}\|(,)\|^{2}+\|(^{*},^{*})\|^{2}\) in their formulations. However, as \(\|(,)\|+^{*}\|(^{*}, ^{*})\|\{,^{*}\}\|\|(,)-(^{*}, ^{*})\|\), the remaining term in \((G,G_{*})\) is stronger than that of \(W_{2}^{2}(G,G_{*})\). Moreover, as \(=^{*}\), we further obtain that

\[(G,G_{*})/W_{2}^{2}(G,G_{*})(\|(,)\|+\| (^{*},^{*})\|)/\|(,)-(^{*},^{*})\|.\]

Hence, as long as the right-hand side term in the above display goes to \(\), i.e., \(||(+^{*},+^{*})| 0\), we have \((G,G_{*})/W_{2}^{2}(G,G_{*})\). This strong refinement of the Wasserstein metric is due to the special structure of \(G\) and \(G_{*}\) as one of their components is always fixed to be \((_{0},_{0})\).

**(iv)** Under the setting when \(G_{*}\) is varied, \(d_{1}=1\), and \(d_{2}=0\), by means of Fatou's lemma the result from Theorem 4.6 in  yields \(V(p_{G},p_{G_{*}}) C^{}.W_{3}^{3}(G,G_{*})\) if the kernel density function \(f\) is 4-strongly identifiable (cf. Definition 2.2 in ) and satisfies uniform Lipschitz condition up to the fourth order where \(C^{}\) is some positive constant depending only on \(G\) and \(G_{*}\). Since \((G,G_{*}) W_{2}^{2}(G,G_{*}) W_{1}^{2}(G,G_{*}) W_{ 3}^{3}(G,G_{*})\), it indicates that the bound in Theorem 3.5 is much tighter than this bound. The loss of efficiency in this bound is again due to the special structures of \(G\) and \(G_{*}\) as one of their components is always fixed to be \((_{0},_{0})\).

Unlike the convergence rate results from the strongly distinguishable in the first order setting between \(f\) and \(h_{0}\) in Theorem 3.3, the convergence rate of \(^{*}\) under the setting of Theorem 3.5 depends on the rate of convergence of \(\|(^{*},^{*})\|^{2}\) to 0 (cf. Theorem A.1). Additionally, the convergence rate of estimating \((^{*},^{*})\) will be determined based on the convergence rates of \(^{*}\) and \((^{*},^{*})\) to 0.

#### 3.2.2 Weakly Identifiable Settings

Thus far, as \(h_{0}\) belongs to the family \(f\), our results regarding the lower bounds between \(p_{G}\) and \(p_{G_{0}}\) under Total Variation distance rely on the strongly identifiable in the second order assumption of kernel \(f\). However, there are various families of density functions that do not satisfy such an assumption, which we refer to as the weakly identifiable condition. To illustrate the non-uniform natures of \(V(p_{G},p_{G_{*}})\) under the weakly identifiable condition of \(f\), we consider specifically a popular setting of \(f\) in this section: multivariate location-covariance Gaussian kernel.

**Location-covariance multivariate Gaussian kernel:** As indicated in the previous work in the literature , if \(f\) is a family of multivariate location-covariance Gaussian distributions in \(d\) dimension, it exhibits the _heat partial differential equation (PDE)_ with respect to the location and covariance parameter \(f}{^{}}(x|,)=2(x|,)\), for any \(x^{d}\) and \((,)\). We can verify that this structure leads to the loss of the second-order strong identifiability condition of the Gaussian kernel. Note that, the PDE structure of the Gaussian kernel has been shown to lead to very slow convergence rates of parameter estimation under general over-fitted Gaussian mixture models (cf. Theorem 1.1 in ). For the setting of the multivariate deviated model, since the parameters \(^{*}\) and \((^{*},^{*})\) are allowed to vary with the sample size, we may expect that the estimation of these parameters will also suffer from the very slow rate. In fact, we achieve the following lower bound of \(V(p_{G},p_{G_{*}})\) under the multivariate location-covariance Gaussian kernel.

**Theorem 3.6**.: _Assume that \(h_{0}(x)=f(x|_{0},_{0})\) for some \((_{0},_{0})\) and \(f\) is a family of multivariate location-covariance Gaussian distributions. We denote_

\[(G,G_{*}) :=(\|\|^{4}+\|\|^{2})+^{*}(\| ^{*}\|^{4}+\|^{*}\|^{2})\] \[-\{,^{*}\}\|\|^{4 }+\|\|^{2}+\|^{*}\|^{4}+\|^{*}\|^{2}\] \[+(\|\|^{2}+\|\|)+^{* }(\|^{*}\|^{2}+\|^{*}\|)\|-^{*} \|^{2}+\|-^{*}\|,\]

_for any \(G\) and \(G_{*}\). Then, we can find a positive constant \(C\) depending only on \(\), \(\), and \((_{0},_{0})\) such that \(V(p_{G},p_{G_{*}}) C.(G,G_{*})\), for any \(G\) and \(G_{*}\)._

See Appendix C.3 for the proof of Theorem 3.6. A few comments with Theorem 3.6 are in order.

**(i)** Different from the formulation of \((G,G_{*})\) in Theorem 3.5 where we have the same power between \(\) and \(\), there is a mismatch of power between \(\|\|^{2},\|^{*}\|^{2}\) and \(\|\|,\|^{*}\|\) in the formulation of \((G,G_{*})\). This interesting phenomenon is mainly due to the structure of the heat equation where the second-order derivative of the location parameter and the first-order derivative of the covariance parameter is linearly dependent.

**(ii)** If we denote \(^{}(G,G_{*}):=(\|\|^{4}+\|\|^{2}) +\{,^{*}\}\|-^{*}\|^{4}+\| -^{*}\|^{2}+^{*}(\|^{*}\|^{4}+\|^{ *}\|^{2})-\{,^{*}\}\|\|^{4}+\| \|^{2}+\|^{*}\|^{4}+\|^{*}\|^{2}\), then we can verify that \((G,G_{*})^{}(G,G_{*})\) for any \(G,G_{*}\). If we treat \(G\) and \(G_{*}\) as two-components measures as in the remark (iii) after Theorem 3.5, we would have

\[^{}(G,G_{*}) W_{4}^{4}(G_{1},G_{1,*})+W_{2}^{2}(G_{2}, G_{2,*}),\] (6)

where \(G_{1}=(1-)_{^{}_{0}}+_{}\), \(G_{2}=(1-)_{^{}_{0}}+_{}\) and similarly for \(G_{1,*}\) and \(G_{2,*}\). Here, \((_{0},_{0})=(^{}_{0},^{}_{0})\), and \(W_{2},W_{4}\) are respectively second and fourth order Wasserstein metrics. The formulations of \(^{}(G,G_{*})\), therefore, can be thought of as a combination of two Wasserstein metrics: one is with only parameter \(\) and another one is only with parameter \(\). The division into two Wasserstein metrics can be traced back again to the PDE structure of the heat equation.

If \(=^{*}\) and \((\|\|^{2}+\|^{*}\|^{2}+\|\|+\|^{*}\| )/\|-^{*}\|^{2}+\|-^{*}\|\), we will have that \((G,G_{*})/^{}(G,G_{*}).\) It proves that the result from Theorem 3.6 under the multivariate setting of Gaussian kernel is a strong refinement of the summation of Wasserstein metrics regarding location and covariance parameter in equation (6).

A consequence of Theorem 3.6 is that the convergence rate of estimating \(^{*}\) is determined by \(\|^{*}\|^{4}+\|^{*}\|^{2}\), instead of \(\|(^{*},^{*})\|^{2}\) as in the strongly identifiable setting of \(f\). Furthermore, we also encounter a phenomenon that the rate of convergence of estimating \(^{*}\) is much faster than that of estimating \(^{*}\). In particular, estimating \(^{*}\) depends on the rate in which \(^{*}(\|^{*}\|^{2}+\|^{*}\|)\) converges to 0 while estimating \(^{*}\) relies on square root of this rate (cf. Theorem A.2).

Minimax Lower Bounds and Convergence Rates of Parameter Estimation

In this section, we study the convergence rates of MLE \(_{n}\) as well as minimax lower bounds of estimating \(G_{*}\) under various settings of \(h_{0}\) and \(f\). Due to space constraints, we present the theory in the distinguishable regime of \(h_{0}\) and \(f\). Non-distinguishable cases, though more interesting, are deferred to Appendix A.

**Theorem 4.1**.: **(Distinguishable settings)** _Assume that classes of densities \(h_{0}\) and \(f\) satisfy the conditions in Theorem 3.3. Then, we achieve that_

_(a) (Minimax lower bound) Assume that \(f\) satisfies the following assumption S.1:_

_(S.1)_ \[_{\|(,)-(^{},^{})\| c_{0}}f(x|,)/^{_{1}} ^{_{2}})^{2}}{f(x|^{},^{})}dx< { for some sufficiently small }c_{0}>0,\]

_where \(_{1}^{d_{1}},_{2}^{d_{2}}\) in the partial derivative of \(f\) take any combination such that \(||=|_{1}|+|_{2}| 1\)._

_Then for any \(r<1\), there exist two universal positive constants \(c_{1}\) and \(c_{2}\) such that_

\[_{_{n}}_{G}_{p_{G}}^{ 2}\|(_{n},_{n})-(,)\|^{2} c _{1}n^{-1/r},\]

\[_{_{n}}_{G}_{p_{G}}|_{n}-|^{2} c_{2}n^{-1/r}.\]

_Here, the infimum is taken over all sequences of estimates \(_{n}=(_{n},_{n},_{n})\)._

_(b) (MLE rate) Let \(_{n}\) be the MLE defined in equation (3), and the family \(\{p_{G}:G\}\) satisfies condition A2. Then, we have the convergence rate for the MLE:_

\[_{G_{*}}_{p_{G*}}(^{*})^{2}\| (_{n},_{n})-(^{*},^{*})\|^{2} n}{n},\] \[_{G_{*}}_{p_{G*}}|_{ n}-^{*}|^{2} n}{n}.\]

Proof of Theorem 4.1 is in Appendix D.1. The results of Theorem 4.1 imply that even though we still can estimate \(^{*}\) at the standard rate \(n^{-1/2}\), the convergence rate of \((_{n},_{n})\) to \((^{*},^{*})\) strictly depends on the vanishing rate of \(^{*}\) to \(0\). Therefore, the convergence rate of estimating \((^{*},^{*})\) can be generally slower than \(n^{-1/2}\) as long as \(^{*}\) goes to \(0\) at a rate slower than \(n^{-1/2}\).

We can also use the geometric inequalities developed in Section 3.2 to investigate the behaviors of \(_{n}\) in the non-distinguishable settings. We will further see how the _non-identifiability_ and _singularity_ of the model affect the convergence rate for density estimation. Due to space constraints, the results for this setting are presented in Appendix A.

## 5 Experiments

We now demonstrate the convergence rates of parameter estimation in the strongly distinguishable setting, where the choice of \(h_{0}\) is a standard Cauchy distribution, and \(f(|,^{2})\) is the normal distribution with mean \(\) and variance \(^{2}\). The additional experiments with the non-distinguishable setting are deferred to Appendix F.

Assume that \(X_{1},,X_{n}\) are i.i.d. samples drawn from the true density function \(p_{G_{*}}\) with \(G_{*}=(^{*},^{*},(^{*})^{2})\) and we obtain the MLE \((_{n},_{n},_{n}^{2})\). We consider two following cases:

**(i)**\(^{*}=0.5\), \(^{*}=2.5\), \((^{*})^{2}=0.25\);

**(ii)**\(^{*}=0.5/n^{1/4}\), \(^{*}=2.5\), \((^{*})^{2}=0.25\).

Two histograms for samples from the density \(p_{G_{*}}\) with \(n=10000\) corresponding to the above two cases are illustrated in Figure 1(a) and 2(a). For each case, we take into account multiple sample sizes \(n\) ranging from \(10^{2}\) to \(10^{4}\). For each sample size \(n\), we calculate the MLE \((_{n},_{n},_{n}^{2})\) via the EM algorithm  and measure the errors \(|_{n}-^{*}|\), \(|_{n}-^{*}|\), and \(|_{n}^{2}-(^{*})^{2}|\). We repeat this procedure 64 times and plot the mean (blue dot) and quartile error bars (yellow bar) of the logarithm of estimation errors against the log of \(n\). Theorm 4.1 suggests that the log convergence rate of \(^{*}\) is of order \(-1/2\) for all cases, and so is the rate for \((,()^{2})\) in the first case. Meanwhile, the convergence rates of \((^{*},(^{*})^{2})\) in the second case are slower, which is in the order of \(-1/4\). The empirical rates found in the experiments match this theoretical result, where the least square line shows that the logarithm of the rate for estimating \(^{*}\) in case (i) is -0.5 and that of the case (ii) is -0.27 \(-1/4\) (similar for \((^{*})^{2}\)).

We once again emphasize that this interesting phenomenon of the rates of convergence is due to the _singularity_ and _identifiability_ of the multivariate deviated model. Our theory and simulation have accurately shown quantitative convergence rates for parameter estimation when \(^{*}\) near the singularity point \(0\), where all pairs of \((^{*},^{*})\) in model (2) give the same model. Together with the non-distinguishable settings, we provide a comprehensive study of the large-sample theory for this type of model, thanks to the newly developed notion of distinguishability that helps to control the linear independent relation between \(h_{0}\) and \(f\). The developed optimal minimax lower bounds and convergence rates will certainly help Machine Learning practitioners understand better the role of sample sizes in the accuracy of estimation in the multivariate deviated model, and are also inspired theorists to study more about the estimation rate of complex hierarchical/mixture models.

## 6 Conclusion

In this paper, we establish the uniform rate for estimating true parameters in the multivariate deviated model by using the maximum likelihood estimation (MLE) method. During our derivation, we have to overcome two major obstacles, which are firstly the interaction between the known function \(h_{0}\) and the Gaussian density \(f\), and secondly the likelihood of the deviated proportion \(^{*}\) vanishing to either one or zero. To this end, we introduce a notion of distinguishability to control the linearly independent relation between two functions \(h_{0}\) and \(f\). Finally, we achieve the optimal convergence rate of the MLE under both the distinguishable and non-distinguishable settings.