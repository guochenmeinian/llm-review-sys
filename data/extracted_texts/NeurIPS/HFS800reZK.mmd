# Learning Representations for Hierarchies with

Minimal Support

 Benjamin Rozonoyer\({}^{1}\) Michael Boratko\({}^{2}\) Dhruvesh Patel\({}^{1}\) Wenlong Zhao\({}^{1}\)

**Shib Dasgupta\({}^{1}\) Hung Le\({}^{1}\) Andrew McCallum\({}^{1}\)**

\({}^{1}\)University of Massachusetts Amherst

\({}^{2}\)Google DeepMind

{brozonoyer,dhruveshpate,wenlongzhao,ssdasgupta,hungle,mccallum}@cs.umass.edu

mboratko@google.com

###### Abstract

When training node embedding models to represent large directed graphs (digraphs), it is impossible to observe all entries of the adjacency matrix during training. As a consequence most methods employ sampling. For very large digraphs, however, this means many (most) entries may be unobserved during training. In general, observing every entry would be necessary to uniquely identify a graph, however if we know the graph has a certain property some entries can be omitted - for example, only half the entries would be required for a symmetric graph.

In this work, we develop a novel framework to identify a subset of entries required to uniquely distinguish a graph among all transitively-closed DAGs. We give an explicit algorithm to compute the provably minimal set of entries, and demonstrate empirically that one can train node embedding models with greater efficiency and performance, provided the energy function has an appropriate inductive bias. We achieve robust performance on synthetic hierarchies and a larger real-world taxonomy, observing improved convergence rates in a resource-constrained setting while reducing the set of training examples by as much as 99%.

## 1 Introduction

Consider the directed graph and its associated adjacency matrix in Figure 1. In situations where this adjacency matrix is sparse, we can store it more efficiently by keeping a list of only the positive entries, effectively assuming any pairs not in our list are zero. But what about situations where we cannot assume that we have observed the full graph? For example, when obtaining annotations for edges of an unknown graph, the full adjacency matrix is unknown to us and we obtain the value of any particular entry by requesting an annotation. A less obvious scenario is training a model to represent a given graph. From the model's perspective, every entry of this adjacency matrix is unknown, and it is only observed as a consequence of training. Therefore, it is of interest to determine: **what is the smallest set of entries necessary to uniquely determine the graph?**

Figure 1: A transitively closed directed tree with branching factor 2 and depth 2, with the associated adjacency matrix. Figure 2: A sufficient set of entries in the adjacency matrix to uniquely distinguish the graph in Figure 1 among all transitively-closed DAGs.

In general, the answer is "all of them", but if we assume some structural prior on the graph itself, it may be possible to reduce the number of edges necessary for consideration. For example, if we knew it was a simple graph (no self-loops) we could omit the diagonal. Similarly, for a symmetric graph, we could omit half the entries. For the class of acyclic graphs, we can omit certain entries which, were they to be \(1\), would form a cycle, and therefore must be \(0\); however, the explicit characterization of these entries is not as straightforward as the preceding cases. Focusing on the case when the graph is a transitively-closed directed acyclic graph (DAG), as in Figure 1, it is easy to see that of those entries which are \(1\), we can omit all but the transitive reduction, i.e. the bold edges, which corresponds to omitting the non-bold "1"s in the adjacency matrix. But what about pruning the zeros? For the graph in Figure 1, we can prove only 14 of the 49 entries in the adjacency matrix are needed to uniquely distinguish this graph among all transitively-closed DAGs. (See Figure 2.)

In this work, we first develop a general-purpose framework to identify a subset of entries required to uniquely distinguish a graph among others with some arbitrary graph property (Section 3.1). We then use this framework to construct a set of entries in the adjacency matrix sufficient to uniquely distinguish transitively-closed digraphs (Section 3.2), and prove that this reduced set is also minimal for transitively-closed DAGs (Section 3.3). We show how this can be leveraged to more efficiently train node embedding models for graph representation by defining the notion of "transitivity bias" (Section 4.1), and proving that box embeddings, a common graph representation model, have a transitivity bias (Section 4.3). We then combine these facts into a formal negative sampling procedure (Section 5) and demonstrate that box embeddings do benefit from training on this reduced set of entries (Section 6).1 For related work in graph theory and representation learning, we refer the reader to Appendix A.

## 2 Background

We use the shorthand \( n\{1,,n\}\). A semicolon separates the main arguments of a function from parameters which are typically held constant (_e.g._, \(f(x;,)=}e^{-()^{2}}\)). We may omit these secondary parameters when their values are clear from context. We represent a graph \(G=(V,E)\) by its adjacency matrix \(A_{G}\{0,1\}^{N N}\), where \((A_{G})_{u,v}=1\) if and only if \((u{}v) E\). When it is clear from context, we omit the subscript and simply write \(A\). We use arrows to represent edges: black \((a{}b)\) denotes a positive edge in \(E\), and red \((a{}b)\) a negative edge in the edge complement \(\) (see below).

### Directed Graphs (Digraphs)

All graphs \(G=(V,E)\) in this work will be finite simple2 directed graphs (digraphs), where the edges are a subset of the complement of the diagonal, _i.e._ with \((V)\{(v{}v) v V\}\), we have \(E V^{2}(V)(V).\) Given a simple digraph \(G=(V,E)\), the _complement of \(G\)_ is \(=(V,)\) where \((V) E\). The _transitive closure of \(G\)_ is \(G^{}=(V,E^{})\) where \((u,v) E^{}\) if and only if there exists a directed path from \(u\) to \(v\) in \(G\). A _transitive reduction_ of a digraph \(G\) is a digraph \(G^{}\) on \(V\) with the fewest number of edges such that \((G^{})^{}=G^{}\). Note that a transitive reduction need not be a subgraph of \(G\), and in general is not unique. If \(G\) is acyclic, however, there is a unique transitive reduction, and it is also a subgraph of \(G\). In this case we denote the transitive reduction \(G^{}=(V,E^{})\).

### Node Embeddings for Capturing Graph Structure

Given the task of modeling entities \(V\) that have a known graph-theoretic structure \(G=(V,E)\), a common approach is to learn a node embedding \(:V Z\) which maps a node \(v(v)\) in _embedding space_\(Z\). The graph structure is extracted from these geometric representations via an energy function \(:V V_{ 0}\) which factors through \(\), _i.e._ there exists a dissimilarity function \(h:Z Z_{ 0}\) such that \(_{}(u,v)(u,v;)=h((u),(v))\). For example, for undirected graphs it is common to use \(Z=^{D}\) and \(_{}(u,v)=\|(u)-(v)\|\). This energy is interpreted as the unnormalized negative log-probability of edge existence. We seek to minimize the energy for positive edges \((u{}v)\) by learning representations for which there exists some (global) threshold \(T\) such that \(A_{uv}=1\) if and only if \(_{}(u,v) T\).

One reason to embed nodes is to learn representations end-to-end in conjunction with other objectives via gradient descent. In such a setting, a typical loss function for the graph structure takes the form

\[_{}(;G)_{(u{}v) E}^ {+}(_{}(u,v))+_{(u{}v)}^{-} (_{}(u,v))\]

where \(^{+}\) and \(^{-}\) are referred to as the _positive_ and _negative_ loss functions, respectively, and the pairs in \(E\) and \(\) are referred to as _positive_ and _negative_ edges accordingly.

The loss function \(_{}\) has \(|V|(|V|-1)\) terms, and thus is often too computationally demanding to use for training. For digraphs which are sparse, a common workaround is to define a noise probability density function (pdf) \(p_{n}\) and design a loss function \(_{}\) which replaces the sum over negative edges with an expectation with respect to \(p_{n}\). In practice, Monte Carlo sampling is used to calculate a loss \(_{}\) which approximates \(_{}\).

## 3 Distinguishing Digraphs via Sidigraphs

We aim to define a new loss function with only a subset of the terms in \(_{}\) (which currently include all entries of the adjacency matrix \(A_{G}\)), while having the same minimizer. Thus, we first attempt to determine the minimal number of entries in \(A_{G}\) which would uniquely distinguish \(G\) among all those with a given property. To this end, we turn to the notion of a signed digraph (a.k.a. "sidigraph"), a digraph where edges have labels \(+\) or \(-\), as a formalism for making explicit the disjoint set of "positive" and "negative" edges required to uniquely determine \(G\). Edges not present in this sidigraph will be the entries we can omit from our adjacency matrix, given those edges which are present and some structural prior.

This framework will allow us to identify the _minimal_ set of entries in the adjacency matrix necessary to uniquely distinguish a digraph among all those with a given property. We apply this framework to transitively-closed digraphs, in which case we prove that certain edges can be pruned. We then prove that, in the case of transitively-closed DAGs, this yields the minimal set of entries. We furthermore provide a concrete algorithm to construct this set of entries from a given adjacency matrix.

### Preliminary Definitions

While the depiction of the partial adjacency matrix in Figure 2 is clear, in order to formalize this we need a notion of a graph which has three possible values, e.g. \(1\), \(0\), and "missing". This is captured formally by the notion of a (simple) **signed digraph**, or **sidigraph** for short, which is a triple \((V,E^{+},E^{-})\) where \(V\) is the vertex set, and \(E^{+},E^{-}\) are disjoint subsets of \((V)\), referred to as the positive and negative edges, respectively.

**Definition 1**.: Given a digraph \(G=(V,E)\) we define the **equivalent sidigraph**\(G^{}(V,E,)\).

This equivalence defines a bijection between digraphs and sidigraphs with \(|V|(|V|-1)\) edges. See Figure 3 for a depiction of an equivalent sidigraph of the digraph in Figure 1.

Let \(H\) and \(G\) be two sidigraphs with the same vertex set \(V\). We say that \(H\) is a **sub-sidigraph** of \(G\), denoted by \(H G\), if \(E^{+}_{H} E^{+}_{G}\) and \(E^{-}_{H} E^{-}_{G}\). Given some digraph \(G\), identifying a subset of entries in \(A_{G}\) is equivalent to specifying a sub-sidiograph of \(G^{}\). For this reason, we introduce the following terminology.

**Definition 2**.: We say that a sidigraph \(H\) is a **potential distinguisher** of a digraph \(G\) if \(H G^{}\). Given a sidigraph \(H\), we define the set of digraphs that could **potentially be distinguished** by \(H\) to be \(_{H}\{G H G^{}\}\).

That is, \(_{H}\) contains every digraph \(G\) such that \(H\) is a potential distinguisher. Another way to interpret \(_{H}\) is that the edge set of every graph \(G_{H}\) contains all positive edges and no negative edges of \(H\). More formally: \(_{H}=\{G V_{G}=V_{H},E^{+}_{H} E_{G},E^{-}_{H} E_ {G}=\}\).

**Definition 3**.: Given some property, we let \(\) be a set of all digraphs (on a given set of nodes) with this property. We say the sidigraph \(H\)**distinguishes** a particular digraph \(G\) among all those with the given property if \(_{H}=\{G\}\).

### Distinguishing Transitively-Closed Digraphs

In this work, we focus on the case where \(\) contains all transitively-closed digraphs, and we let \(G=(V,E)\) be some fixed transitively-closed digraph.

We first focus on reducing the positive edges of \(G^{}\). An explicit representation of \(G\) may well have \((|V|^{2})\) edges; however any transitive reduction \(G^{}=(V,E^{})\) often has substantially fewer edges.3 Furthermore, by definition, \(G\) is the only transitively-closed digraph which contains the edges \(E^{}\). As a consequence, if we let \(H=(V,E^{},)\) we have \(_{H}=\{G\}\), which proves the following result.

**Proposition 1**.: _Let \(G=(V,E)\) be a transitively-closed digraph, and \(G^{}=(V,E^{})\) a transitive reduction. Then \(H=(V,E^{},)\) distinguishes \(G\) among transitively-closed digraphs._

The edge complement \(\) might also have \((|V|^{2})\) edges, which we would like to reduce while maintaining distinguishability. To see why this should be possible, consider Figure 4.

If \((a{}b) E\) and \((a{}d) E\) then \((b{}d) E\), since if it were we would need to include \((a{}d)\) due to transitivity. Similarly, if \((c{}d) E\) and \((a{}d) E\) then \((a{}c) E\), since, if it were, transitivity would imply \((a{}d) E\). We formalize this in the following proposition.

**Proposition 2**.: _Let \(G=(V,E)\) be a transitively-closed digraph; \(H=(V,E^{+}_{H},E^{-}_{H})\) a sidigraph which distinguishes \(G\) among transitively-closed digraphs. If \((a{}d) E^{-}_{H}\), then_

1. _If_ \((a{}b) E\)_,_ \(H^{}=(V,E^{+}_{H},E^{-}_{H}\{(b{}d)\})\) _distinguishes_ \(G\) _among transitively-closed digraphs._
2. _If_ \((c{}d) E\)_,_ \(H^{}=(V,E^{+}_{H},E^{-}_{H}\{(a{}c)\})\) _distinguishes_ \(G\) _among transitively-closed digraphs._

Proof.: Recall that \(H\) distinguishing \(G\) among transitively-closed digraphs means that \(_{H}=\{G\}\), i.e. \(G=(V,E)\) is the only transitively-closed digraph for which \(E^{+}_{H} E\) and \(E^{-}_{H} E=\). Now, note that \(G^{}=(V,(E^{+}_{H})^{})\) is the smallest transitively-closed digraph containing the edges \(E^{+}_{H}\), and thus \(G^{} G\). But this implies \(E^{-}_{H}(E^{+}_{H})^{} E^{-}_{H} E=\), and thus \(G^{}=G\).

Now we prove the proposition at hand. We prove the first case, the second follows similarly. Let \((a{}d) E^{-}_{H}\), assume \((a{}b) E\), and define \(H^{}=(V,E^{+}_{H},E^{-}_{H}\{(b{}d)\})\). Note that \(H^{} H\), and hence \(_{H}_{H^{}}\).

Now suppose \(K=(V,E_{K})_{H^{}}\), which implies \(K\) is a transitively-closed digraph with \(E^{+}_{H} E_{K}\) and \((E^{-}_{H}\{(b{}d)\}) E_{K}=\). In particular, as a consequence of our observation in the first paragraph, this means \((a{}b) E_{K}\). We prove \((b{}d) E_{K}\) by contradiction, for if \((b{}d) E_{K}\), then since \(K\) is transitively-closed we would have \((a{}d) E_{K}\) which violates our preliminary assumption.

Figure 3: An adjacency matrix (a) and a representation of the edges in the associated sidigraph (b), where \(a+\) in position \((i,j)\) indicates \((i{}j) E^{+}\) and a –indicates \((i{}j) E^{-}\). The minimal sidigraph (c) formally captures the fact that Figure 2 has the minimal set of entries in the adjacency matrix to uniquely distinguish \(G\) among all transitively-closed DAGs.

These two simple prunings, illustrated in Figure 4, lead us to formulate Algorithm 1, FindMinDistinguisher, which repeatedly removes edges using Proposition 2 until no more can be removed.

### Optimality of FindMinDistinguisher

We note that \(H^{*}=(G)\) is only defined for DAGs, and indeed for graphs with cycles it is not possible to uniquely define such a sidigraph. (Recall that the transitive closure is not unique for graphs with cycles). In the event that \(G\) is acyclic, however, we can prove that \(H^{*}\) is not just capable of distinguishing \(G\) among transitively-closed digraphs, but moreover it is the sidigraph with the minimum number of edges capable of doing so! Below is a proof sketch for the informal statement. For the full proof refer to Theorem 1 in Appendix B.

_Proof sketch:_ The basic idea is to define a partially ordered set, a.k.a. poset, on the set of negative edges of \(G^{}\). We then show that the set of all minimal elements in this poset is necessary and sufficient to distinguish \(G\) among transitively-closed digraphs. Finally, we observe that our algorithm in the previous section produces the set of all minimal elements and hence its optimality follows.

## 4 Leveraging Sufficient Sidigraphs for Training Node Embeddings

We would like to leverage a distinguishing sidigraph for more efficient training and improved accuracy of energy-based node embedding models. The prerequisite is an energy function with a useful inductive bias for the digraph property under consideration. In the extreme case of an inductive bias which only permits models capable of representing digraphs in \(\) we should be able to train using only the positive and negative edges from any \(H\) which can distinguish \(G\) among those in \(\). In some instances, depending on the proportion \([|E^{+}_{H^{*}}|+|E^{-}_{H^{*}}|]/[|V|(|V|-1)]\), this may allow full training on digraphs which would otherwise require sampling. As we show, sampling can still be used in conjunction with the minimal edge set implied by a sidigraph, and in such situations we expect not only increased efficiency but increased performance, as \(_{}()\) will better approximate \(_{}()\).

### Transitivity Bias

First, we formally define "transitivity bias".

**Definition 4**.: Let \(Z\) be an embedding space. We say that an energy function \(_{}\) has **transitivity bias** if, for all embeddings \(:V Z\), there exists some threshold \(T 0\) s.t. for all \(u,v,w V\), the inequalities \(_{}(u,v),_{}(v,w) T\) imply \(_{}(u,w) T\).

We illustrate transitivity bias using the following overly simple embedding model.

**Example 1** (Bit Vectors).: Let \(Z=\{0,1\}^{|V|}\), and \(_{}(u,v;)-\). Then \(_{,}\) has a transitivity bias using threshold \(T=0\), as \(_{}(u,v;)=0\) if and only if \( i V\), \((v)_{i}=1(u)_{i}=1\). Thus, if \(_{}(u,v;)=_{}(v,w;)=0\) we have \((w)_{i}=1(v)_{i}=1(u)_{i}=1\), hence \(_{}(u,w;)=0\).

For a given digraph \(G=(V,E)\), we define the bit vector representation of \(G\) as \(_{}:V\{0,1\}^{|V|}\), where the \(i^{}\) (for \(i V\)) coordinate is given by

Figure 4: Negative edges removable according to Proposition 2.

\[_{}(v)_{i}1&,\\ 0&.\]

**Proposition 3**.: _Let \(G=(V,E)\) be any digraph, then \(}}(u,v;_{})=0\) if and only if \((u,v) E^{}\)._

Proof.: Let \((u,v) E^{}\), then there is some path \(u=w_{1} w_{2} w_{n-1} w_{n}=v\) in \(E\). By the definition of \(_{}^{G}\), we have that \(}}(w_{i},w_{i+1};_{}^{G})=0\) for \(i[\![n]\!]\), and thus by the transitivity bias observation made in Example 1 this implies \(}}(u,v;_{}^{G})=0\).

Now assume \((u,v) E^{}\), then \(v\) is not a descendant of \(u\), which means \(_{}^{G}(u)_{v}=0\) while \(_{}^{G}(v)_{v}=1\), and hence \(}}(u,v;_{}^{G})>0\). 

With an appropriate threshold, any energy function that has transitivity bias in fact represents a transitively-closed digraph:

**Proposition 4**.: _If \(}\) is an energy function with transitivity bias, then for any \(\) there exists a \(T 0\) such that the digraph with edges \(\{(u,v)}(u,v) T\}\) is transitively closed._

This allows us to formalize the notion that training on any \(H\) which can distinguish \(G\) among transitively-closed digraphs is sufficient.

**Proposition 5**.: _Let \(G\) be a transitively-closed digraph, \(}\) an energy function with transitivity bias, and \(H=(V,E_{H}^{+},E_{H}^{-})\) a siggraph which distinguishes \(G\) among transitively closed digraphs. If \(T\) is the threshold associated with the transitivity bias for \(\), \(}(u,v) T\) for all \((u,v) E_{H}^{+}\), and \(}(u,v)>T\) for all \((u,v) E_{H}^{-}\), then \(\) represents the digraph \(G\)._

### Box Embeddings and T-Box

We would like our energy function to be a representation which is tractable and trainable via gradient-descent, which requires the space \(Z\) to have differentiable structure. _Box embeddings_(Vilnis et al., 2018) are a trainable region-based embedding method which demonstrate strong performance for representing digraphs (Boratko et al., 2021; Zhang et al., 2022). We provide the requisite background on box embeddings and define the specific model that we will use for our experiments.

As introduced in Vilnis et al. (2018), box embeddings represent entities using a _box_ or _hyperrectangle_ in \(^{D}\), _i.e._, a Cartesian product of intervals

\[_{d=1}^{D}[x_{d}^{},x_{d}^{}]=[x_{1}^{ },x_{1}^{}][x_{D}^{},x_{D}^{}]^{D},\]

where \(x_{d}^{}<x_{d}^{}\) for \(d[\![D]\!]\). Vilnis et al. (2018) proposed modeling a directed graph such that boxes of parents contain their children with an energy function

\[}}(u,v;)-_{d=1}^{D}F_{ }((u)_{d},(v)_{d}),\]

where the per-dimension parameters are endpoints of an interval, _i.e._\((u)_{d}=[(u)_{d}^{},(u)_{d}^{}]\), and the per-dimension score is defined as

\[F_{}((x^{},x^{}),(y^{},y ^{})),x^{}] [y^{},y^{}]|}{|[y^{},y^{ }]|}=,y^{})- (x^{},y^{}),0)}{(y^{}-y^{ },0)}.\]

Previous works have highlighted the difficulty of optimizing an objective including these hard \(\) and \(\) functions (Li et al., 2019; Dasgupta et al., 2020). We use the Global T-Box model, the most recent solution to this issue, introduced in Boratko et al. (2021). Global T-Box (or GT-Box) softens the volume calculation by replacing the hard \(\) and \(\) operators with a smooth approximation \(_{t}() t(_{i}e^{x_{i}/t})\). The per-dimension score function is then given by

\[F_{}((x^{},x^{}),(y^{}, y^{});,)_{}( _{-}(x^{},y^{})- _{}(x^{},y^{}),0)}{ _{}(y^{}-y^{},0)},\]

which approximates \(F_{}\) for sufficiently small \(,>0\). These \(,\) are additional (global) trainable parameters of the model.

### Transitivity Bias of Box Embeddings

While the motivation and formulation of the energy for box embedding functions is quite different than that of the naive bit vector model in Example 1, it actually is more similar than it may at first appear.

**Remark 1**.: As observed in Boratko et al. (2021), the box intersection volume calculation can actually be viewed as the \(L^{2}\) inner-product of characteristic functions of boxes, in which case it takes an identical form to the energy function for bit vectors. For any functions \(f,g L^{2}(^{D})\), the standard inner product is \( f,g=_{^{D}}f(x)g(x)\,dx\). Now, for any box \(B^{D}\), we let \(_{B}(x)\) be the characteristic function, which is \(1\) when \(x B\) and \(0\) otherwise. Then the volume of intersection of boxes is

\[(A B)=_{A}(x),_{B}(x),\]

which is also valid if \(A=B\). Thus, we can write the energy function

\[_{}(u,v;)=-_{ (u)},_{(v)}}{_{(v)}, _{(v)}}.\]

**Remark 2**.: Box embeddings can be quantized into bit vectors in such a way that applying the bit vector energy function to the resulting quantizations preserves the set of node pairs which have zero energy. Given a box embedding \(\), for each \(d D\) let \(T_{d}\) be the endpoints of boxes in dimensions \(d\), _i.e._\(T_{d}:=_{v V}\{(v)_{d}^{-},(v)_{d}^{+}\}\). Let \(M_{d} T_{d}\), and assign indices \(T_{d}\{t_{d,m}\}_{m=1}^{M_{d}}\) such that \(t_{d,m} t_{d,m+1}\) for \(m M_{d}-1\). Then, for each \(v V\) and \(d D\), form the \(M_{d}\)-dimensional vector \((v)_{d,m} F_{}((v)_{d},(t_{d,m},t_{d,m+1}))\). By construction, this value will be either \(0\) or \(1\). Letting \((v)\{0,1\}^{ M_{d}}\) be the concatenation of \(\{(v)_{d}\}_{d=1}^{D}\), we obtain a bit vector representation, for which the energy function \((u,v;)=-\) is such that \(E(u,v;)=0\) if and only if \((v)_{d,m}=1(u)_{d,m}=1\), which is true if and only if \([(v)_{d}^{+},(v)_{d}^{+}][(u)_{d}^{-},(u)_{ d}^{+}]\).

Most importantly for our purposes, however, is the following proposition.

**Proposition 6**.: _The energy function \(_{,}\) has a transitivity bias._

Apart from prior empirical observations that box embeddings work well to embed transitively-closed DAGs Boratko et al. (2021), Proposition 5 suggests it should be possible to train box embeddings on the output of FindMinDistinguisher to represent a transitively-closed DAG. In practice, we train the smooth approximation provided by GT-Box. As \(, 0^{+}\), which often happens naturally during training Boratko et al. (2021), we expect it to capture the transitivity even when trained only on the positive and negative edges of the distinguisher provided by the algorithm.

## 5 Hierarchy-Aware Sampling

In this section we formally how we sample edges for our loss function, which we term hierarchy-aware sampling. Based on the results in the preceding section, given a transitively-closed digraph \(G\), if \(_{}\) has a transitive bias and the sidigraph \(H=(V,E_{H}^{-},E_{H}^{+})\) can distinguish \(G\) among transitively-closed digraphs, then we can train using the following loss function:

\[_{}(;H)_{(u v) E_{H}^{}} ^{+}(_{}(u,v))+_{(u v) E_{H}^{-}}^{-} (_{}(u,v)).\]

In particular, for a transitively-closed DAG \(G\), we can use \(H^{*}\) as returned by Algorithm 1.

If the cardinality of \(H^{*}\) is small enough, this may make training using \(_{}\) feasible. In general, however, we still may need to sample negative edges using some noise distribution, as mentioned in Section 2.2. In practice, we will compare using the sidigraphs \(G^{}\) and \(H_{*}=(G)\), which means positives will be sampled from either \(E\) or \(E^{}\), and negatives will be sampled from \(\) or \(E_{H^{*}}^{-}\). Even with our reduced set of negatives, there are still far more negatives than positives, and so we adopt the common practice of sampling \(k\) negatives for every positive within a batch.

[MISSING_PAGE_FAIL:8]

Figure 5 demonstrates a striking trend observed consistently in our experiments. **While GT-Box with our reduced edge set performs on par or better than with the original edge set, in contrast, the performance of Sim-Vec degrades significantly.** We demonstrate this using a representative example in Figure 6, where, in stark contrast to GT-Box taking advantage of \(E^{-}_{H^{*}}\) and outperforming all Sim-Vec settings, both Sim-Vec settings that use \(E^{-}_{H^{*}}\) fail to converge even to 0.4 F1. This dichotomy hearkens back to Proposition 5, which underlines the non-trivial synergy between an energy function with transitivity bias and the hierarchy-aware output of FindMinDistinguisher.

Figure 5: Convergence measured using AUF1c on three graph types for Sim-Vec and GT-Box models with the full or reduced set of positive and negative edges provided during training, respectively. Pos. \(\) (resp.) implies usage of transitive closure (resp. transitive reduction). Analogously, for Neg., the symbols imply usage of the set of the edge complement \(\) and \(E^{-}_{H^{*}}\), respectively.

Figure 6: While GT-Box (black squares) with a transitivity bias takes full advantage of the hierarchy-aware \(E^{-}_{H^{*}}\), Sim-Vec, which does not have a transitivity bias, falls apart under this negative sampling procedure. The reduction in negative examples from using the pruned negative edges for this particular graph is \(95.83\%\). Negative ratio \(k=4\) throughout.

Figure 7: The plots show convergence of GT-Box for negative ratio \(k=4\). The top row shows the plots for balanced trees with branching factors \(b=2\), \(5\) and \(10\). The middle row for nCRP graphs with \(=10\), \(100\), and \(500\), respectively going left to right. The bottom row shows the plots for Price’s graph with \(c=0.1\), \(=1.0\) and three values of \(m=1\), \(5\), and \(10\). The number of vertices in each graph is \( 2^{13}\).

**Balanced tree.** Figure 11 Row 1 visualizes the impact on convergence of increasing the branching factor. As branching factor \(b\) increases \(2<5<10\), we first note that the balanced tree with fixed \(|V|\) becomes more and more shallow. While \(E_{H^{*}}^{-}\) performs well on all values of \(b\), it particularly stands apart from \(\) in the \(b=10\) setting.

**nCRP.**\(\) is the normalized "new table" probability, with a smaller \(\) implying more separate clusters with a deeper hierarchy, and a larger \(\) implying fewer clusters and a more shallow hierarchy. The relative improvement of using \(E_{H^{*}}^{-}\) is more significant for smaller \(\).

**Price.** Price's model is learnable by GT-Box especially quickly under any setting, as evidenced by the very high AUF1C scores in Figure 5. Higher \(c\) has the effect of making edge attachment more uniformly distributed among nodes. In Figure 11 Row 3 we examine convergence values for \(c=0.1\), noting that \(E_{H}^{-}\) typically underperforms \(\) but catches up in the limit. As \(m\) (out-degree of newly added vertices) increases \(1<5<10\), looking at Table 1 we note that hierarchy-aware negative pruning also gets more aggressive as \(52.35\%<89.62\%<93.77\%\).

**MeSH.** As seen in Figure 8, on the larger real-world MeSH taxonomy, not only does GT-Box outperform Sim-Vec, but our minimal negative edge set \(E_{H^{*}}\) within GT-Box outperforms sampling from the edge complement, while being a 99.78% reduction. Meanwhile, \(E_{H^{*}}\) with Sim-Vec plummets, consistently with the trend exhibited in Figure 6. This encouraging result on a graph with \( 2^{15}\) nodes points not only to the economy of our approach, but to the stability and improved performance on large real-world data.

## 7 Limitations

While the connection between hierarchies and transitivity bias allows us to capitalize on it in the form of hierarchy-aware sampling, we acknowledge that the properties demanded of both the data and model are restricted to transitivity. While this does encompass a large variety of relationships observed in real-world graphs, this specific algorithm does not extend easily to new combinations of graph properties and inductive biases, which is a goal of future work.

Another limitation is the extent to which this sort of method would break down for graphs which are, strictly speaking, not transitively-closed, but close (in edit distance) to being transitively-closed. Strictly speaking, our proofs do not apply in that setting, and the efficacy of the approach may vary depending on the type of structural changes a particular removal of an edge brings about.

## 8 Conclusion

In this work, we propose a novel framework for identifying a sufficient subset of entities in the adjacency matrix which unambiguously specify a digraph, given some prior structural knowledge. We demonstrate the usability of this framework for the property of transitively-closed DAGs, or hierarchies. We derive a characterization of the sufficient negative set for such type of graphs, and based on that we devise a novel hierarchy-aware sampling technique. Our approach is efficient and robust when used in conjunction with an energy-based node embedding model possessing the appropriate inductive bias, which, for hierarchies, is transitivity bias.

Figure 8: Convergence of GT-Box vs Sim-Vec for \(\) vs \(E_{H^{*}}\) on MeSH 2020. The minimal negative edge set \(E_{H^{*}}\) converges to the highest F1 when coupled with GT-Box, but falls apart when combined with Sim-Vec, consistently with our hypothesis about the requirement of transitivity bias for utilizing \(E_{H^{*}}\).

Acknowledgements

This work was supported in part by IBM Research AI through the AI Horizons Network, the Chan Zuckerberg initiative under the project Scientific Knowledge Base Construction, and National Science Foundation (NSF) grant number IIS-1922090. Hung Le was supported by an NSF CAREER Award No. CCF-223728, an NSF Small Grant No. CCF-2121952, and a Google Research Scholar Award.