# Accelerating AI Performance using

Anderson Extrapolation on GPUs

Saleem Abdul Fattah Ahmed Al Dajani1, David E. Keyes

Applied Physics Program, Physical Sciences and Engineering Division

Extreme Computing Research Center, Computer, Electrical and Mathematical Sciences and Engineering Division

King Abdullah University of Science and Technology (KAUST)

Thuwal, Makkah Province, Kingdom of Saudi Arabia (KSA) 23955-6900

saleem.aldajani@kaust.edu.sa, david.keyes@kaust.edu.sa

###### Abstract

We present a novel approach for accelerating AI performance by leveraging Anderson extrapolation, a vector-to-vector mapping technique based on a window of historical iterations. By identifying the crossover point (Fig. 1) where a mixing penalty is incurred, the method focuses on reducing iterations to convergence, with fewer more compute-intensive but generally cacheable iterations, balancing speed and memory usage with accuracy and algorithmic stability, respectively. We demonstrate significant improvements in both training and inference, motivated by scalability and efficiency extensions to the realm of high-performance computing (HPC).

## 1 Introduction

Anderson extrapolation  has recently been applied to deep equilibrium models (DEQs) . Kolter et al.  found the gains not substantial due to early termination with a loose convergence tolerance. They focused on Anderson extrapolation during training. Here, we show significant acceleration of AI performance with Anderson on GPUs for both the forward pass (running inferences faster) and training (generating models faster). We demonstrate acceleration of the forward pass with standard Anderson as a baseline for future work with stochastic variants  and accelerating the backward pass with Jacobian-free methods like Jacobian-Free Backpropagation (JFB) and Neumann series gradient approximations .

As AI demand grows, as shown in Fig. 2, high-performance computing (HPC) is becoming critical due to economic pressures from the growth of data and AI infrastructure . Low-memory acceleration techniques, like Anderson extrapolation, will be key to increasing HPC-based AI computational efficiency. This study investigates matrix-free Anderson extrapolation on GPUs, emphasizing gains from advanced computing architectures compared to CPUs. Ourgoal is to maximize computational efficiency while reducing iterations to convergence by reusing previous iterations to avoid unnecessary gradient calculations, gaining partial benefits expected from second-order methods (e.g., ) without manipulating Hessian matrices.

The environmental impact of AI is rapidly growing [3; 15; 25; 28]. By 2030, AI is projected to account for 2% of global electricity consumption. We aim to reduce this impact by up to 90%, saving 160 terawatt-hours per year by 2030. The carbon footprint of AI exceeds the 500-megaton annual benchmark set by initiatives like Bill Gates' Breakthrough Energy . Efficiency-enhancing technologies like GPU and Anderson acceleration can reduce AI's carbon emissions by 60 gigatons per year by 2030, as shown in Fig. 2.

### Leveraging extrapolation for AI and HPC advances

Anderson extrapolation, a windowing technique for accelerating nonlinear fixed point iterations diagrammed in Figs. 3 and 4, is widely applied in fields like density functional theory, kinetic theory, and climate spin-up. It is well-suited for distributed memory parallelization and GPU implementation. It is a staple of major open-source large-scale solver libraries, including PETSc [11; 12], SUNDIALS , Trilinos [19; 20; 21; 22], and deal.II [4; 5; 6; 13]. It can be applied to machine learning training, smoothing out standard forward iterations and achieving superior accuracy in training and testing error. Benchmarking results on CIFAR10 show expected robustness benefits and allow characterization of the temporal advantages or disadvantages from the higher cost per iteration, where a small residual minimization step is applied at each new function evaluation.

### Balancing memory and convergence rate

Fundamental tradeoffs exist between memory capacity, memory bandwidth, communication cost, and algorithmic characteristics of stability and convergence rate. The tradeoffs are generally resolved to minimize time to solution. GPUs attain high memory bandwidth advantages over CPUs at the cost of smaller memory capacity. Anderson extrapolation promotes fewer, more expensive steps, reusing cached state-vector data. In distributed memory implementations, it produces convergence with fewer interprocessor communication steps. It has tuning parameters such as window size and damping that can be tuned to application and architecture. We are assessing its utility in machine learning more broadly at a time of emergent CPU-GPU superchips.

### Deep equilibrium neural network models

Deep equilibrium models (DEQs) are the continuum limit of explicit neural networks as the number of layers approaches infinity , approximating many explicit layers with a single, implicit layer with exponentially fewer parameters using a backward pass including the output. This reduces the inverse problem in parameter space to a fixed point iteration problem, enabling the usage of nonlinear, vector-to-vector mapping techniques to compute the fixed point iterations that converge to the deep equilibrium state parameters by minimizing the loss function. With gains in memory and acceleration, DEQs are fit for large-scale computer vision and natural language processing tasks and benefit more from matrix-vector operation-optimized computing architectures like GPUs and CPU-GPU superchips.

The standard approach using forward iteration for fixed point iteration problems often does not efficiently converge to the fixed point and suffers from initially slow error reduction and local minimum trapping in nonlinear problems like deep neural networks. Anderson extrapolation outperforms

Figure 2: AI carbon footprint projected to consume >2% of global electricity demand [3; 15; 25; 28], amounting to >10% of global electricity demand for data centers and infrastructure.

standard forward iteration by combining information from previous iterations to span a searchable subspace to extrapolate the next iteration, enhancing convergence rates at the expense of memory usage in each iteration. When the original fixed point iteration is contractive and thus guaranteed to converge, Anderson is theoretically guaranteed not to be slower  and it experimentally observed to be considerably faster in numerous applications.

DEQs represent any neural network at arbitrary depths and connectivities with a single implicit layer consuming vastly fewer parameters with faster forward passes for accelerated training and inferences. The implicit function theorem shows how gradients can be computed in the DEQ framework, facilitating backpropagation through the equilibrium state [9; 35].

DEQs provide a framework for accelerating deep learning, extending the capacity of deep networks within a single-layer architecture through fixed point computations and advanced root-finding algorithms. Their amenability to convergence acceleration with techniques like Anderson positions DEQs as a robust method to reduce computation needed to build state-of-the-art models and scale up beyond current computational limitations.

## 2 Methods

This work demonstrates Anderson extrapolation to accelerate AI performance algorithmically without increasing processors. Since it does not require inverting matrices approximating Hessians of the dimension of the state space, but only matrices of the dimension of the Anderson window size, it benefits from hardware optimized for uniform vector operations, like GPUs. We benchmark Anderson acceleration against standard forward iteration on GPUs and CPUs.

### Mathematical formulation

Fixed point acceleration starts with the fixed point iteration formula \(z^{}=f(z^{},x)\). Forward iteration, \(z^{k+1}=f(z^{k},x)\), moves step-wise towards this fixed point.

Figure 4: Deep equilibrium neural network model architecture (Source: NeurIPS Tutorial, 2020 ). \(f(z,x)=((z+(x+W_{2}*(( (W_{1}*z))))))\). ”norm” here is a group norm, representing a statistical normalization .

Figure 3: Mathematical formulation and vector representation. Adapted from Y. He & H. De Sterck. ”Linear Asymptotic Convergence Analysis of Anderson Acceleration, with Krylov Formulation in the Linear Case” Copper Mountain Conference (2022), ICERM Workshop (2023). Available at: https://www.bilibili.com/video/BV1w4411i77y/ and https://icerm.brown.edu/video_archive/?play=3320

Anderson acceleration uses a linear combination of prior iterates, \(z^{k+1}=_{i=1}^{m}_{i}f(z^{k-i+1},x)\), optimizing \(_{i}\) to minimize the residual norm, \(,x)-z^{k}\|_{2}}{\|f(z^{k},x)\|_{2}+}\), leading to faster convergence. The coefficients must sum to unity, thus:

\[_{}\ \ \|G\|_{2}^{2},\ \ \ \ 1^{T}=1\] (1)

The matrix \(G\) is defined as:

\[G=[f(z^{k},x)-z^{k},,f(z^{k-m+1},x)-z^{k-m+1}]\] (2)

The Lagrangian incorporating the equality constraint is:

\[L(,)=\|G\|_{2}^{2}-(1^{T}-1)\] (3)

To solve for \(_{i}\), we set up and solve:

\[[0&1^{T}\\ 1&H]=[0&1^{T}\\ 1&G^{T}G+ I][\\ ]=[1\\ 0]\] (4)

Anderson acceleration generally includes a mixing parameter \(\), incorporating some inertia when \(<1\):

\[z^{k+1}=(1-)_{i=1}^{m}_{i}z^{k-1+1}+_{i=1}^{m}_{ i}f(z^{k-i+1},x)\] (5)

### Dataset description, compute environment, and training details

The CIFAR10 dataset, with 60,000 32x32 labeled images in 10 classes, is used for supervised learning and image classification tasks. Accuracy is the ratio of correctly predicted labels to the total images, using cross-entropy loss.

High-dimensional tensors in standard PyTorch format are used. The compute environment includes Google Colab Pro with NVIDIA Tesla V100 GPUs and Intel Xeon CPUs. Training uses default hyperparameters from Kolter et al.  for comparison with prior results [7; 8; 9; 10; 17; 24], with Anderson parameters \(m=5\) and \(=1\).

### Deep neural networks, deep equilibrium models, and fixed Point equations

Traditional neural networks use layer-wise transformations:

\[z_{1} =x\] \[z_{i+1} =(W_{i}z_{i}+b_{i}), i=1,,k-1\] \[h(x) =W_{k}z_{k}+b_{k}\]DEQs model a network as an infinitely deep system, finding a fixed point \(z^{}\) that satisfies:

\[z^{}=(Wz^{}+Ux+b)\] (6)

Here, \(W\), \(U\), and \(b\) are shared across all layers, and \(\) is the activation function. Solving for \(z^{}\) avoids computing individual layers, reducing computational cost.

### GPU Optimization and Parallelization

Anderson acceleration maps well to GPUs, suited for uniform tasks with high throughput.

## 3 Results

Anderson extrapolation has a higher cost per iteration, measured in function evaluations or epochs. The main benefit is that Anderson extrapolation exhibits less fluctuation in accuracy, as seen in the test accuracy, whereas forward iteration shows more significant ups and downs in both training and testing accuracy, potentially indicating overfitting. Anderson acceleration reaches a higher accuracy plateau for both training and test datasets, suggesting better generalization capability. We speculate that this is due to better avoidance of suboptimal local minima because of the wider window of trial vectors from which each step is drawn.

Anderson extrapolation is benchmarked against traditional forward iteration methods in DEQs to understand its role in AI and HPC. The computational demand of Anderson extrapolation correlates with the number of epochs, as shown in Fig. 5. A trade-off is shown between accuracy and computing time, whereas forward iteration maintains a more consistent computational time as the number of epochs increases.

Implicit neural network model architecture performance is analyzed with the goal of understanding how incorporating Anderson acceleration impacts model accuracy and performance. The stability of train and test accuracy is observed, and Anderson acceleration demonstrates higher consistency over numerous epochs, whereas forward iteration reveals significant swings in train and test accuracy. Initialization error with Anderson is lower than with forward iteration.

We observe Anderson acceleration reaches higher accuracies in training and testing in less time than forward iteration. Anderson acceleration is also superior with random inputs. Across testing with random inputs, Anderson acceleration consistently outperforms or matches forward iteration, depending on target relative residual accuracy.

## 4 Discussion

These results show that Anderson extrapolation can train DEQ networks to higher accuracy than forward iterations and reach a given high accuracy in less time. Anderson extrapolation is also efficiently implementable in GPU programming environments, utilizing memory austerity and operational uniformity attributes similar to the forward algorithm. For large-scale neural network training problems requiring distributed memory, this study motivates porting and testing on state-of-the-art GPU architectures, CPU-GPU superchips, and emerging computing hardware.

GPUs have been shown to accelerate Anderson extrapolation beyond what could be achieved with standard forward iterations or with Anderson on CPUs. This is notable before reaching the 'crossover point,' the trade-off between computation speed and accuracy, illustrated in Figs. 1 and 6.

   & Algorithm & **DEQ (ours)** & DEQ [Implicit]  & ResNet-18 [**Explicit**]  \\   & Standard & **64.842** & \(\)170,000 & \(\)170,000 \\  & Accelerated & **64.842** & - & - \\   & Standard & 64.7\% & - & - \\  & Accelerated & **96.3\%** & - & - \\   & Standard & 64.2\% & 82.2\% & 81.6\% \\  & Accelerated & **79.1\%** & - & - \\   & Standard & 1.2\(\)10\({}^{4}\) & - & - \\  & Accelerated & **1.4\(\)10\({}^{3}\)** & - & - \\   & Standard & 1 & - & - \\  & Accelerated & **0.5** & - & - \\   & Ratio & **2-8.6** & - & - \\  & Compute saved & **50-88\%** & - & - \\  

Table 1: Summary of algorithmic improvements to training and inference without augmentation.

The'mixing penalty' due to the additional computational cost associated with Anderson acceleration is offset by the parallel processing capabilities of GPUs, enabling faster convergence than with CPUs or standard forward iterations alone.

The increase in time per iteration with Anderson arises from the residual minimization process during each acceleration step. The higher plateau for accuracy with Anderson compared to forward iteration suggests more robust learning when taking previous iterations into account. Monitoring the slowing of Anderson acceleration and switching to approximate forms of Newton's method (e.g., quasi-Newton, modified Newton, or inexact Newton) can be beneficial.

The unstable behavior with forward iteration necessitates lower learning rates and more epochs for training, increasing the time needed to reach the same accuracies achieved with Anderson by up to an order of magnitude. The inconsistency in accuracy with forward iteration raises concerns about overfitting during training, undermining the model's ability to generalize for reliable predictions on new, unseen data.

These findings indicate that Anderson acceleration improves DEQ performance with more rapid error reduction at the outset, as shown in Fig. 6 and Fig. 7. The rate at which peak accuracy is reached with extrapolation enables peak neural network performance in a fraction of the time required for forward iterations to stabilize at comparable accuracy. This acceleration is beneficial in time-sensitive applications where rapid deployment of accurate AI models is essential.

## 5 Conclusion

The integration of Anderson acceleration within deep learning workflows presents substantial improvements in computational efficiency, accuracy, and generalizability of implicit neural networks. Porting and parallelizing matrix-free acceleration techniques onto emerging CPU-GPU hybrid architectures holds promise. The accuracy and speed of deep equilibrium neural network training and inferences could be improved further, making them more viable for real-world applications beyond the classification task demonstrated herein. Based on investigations of explicit and implicit memory requirements , optimizations based on an Anderson-accelerated, fixed-point iteration implicit memory approach  are effective in memory-intensive computer vision processing, reducing memory and bandwidth consumption without compromising performance .

Figure 5: Evaluating CIFAR10 dataset through deep equilibrium. Anderson is 1.2x more accurate at stable convergence above mixing penalty.

Figure 6: Evaluating relative residual, \(,x)-z^{k}\|_{2}}{\|f(z^{k},x)\|_{2}+}\), for a random input \(x\). A typical GPU is approximately 100-150x faster to target relative residual than a typical CPU using Anderson, with a mixing penalty that is approximately \(10^{-1}\) to \(10^{-2}\) lower.

These methods applied to implicit neural networks, particularly DEQs, reveal new directions for AI research, such as exploring further acceleration gains from stochastic variants of Anderson extrapolation . Exploiting the continuum limit of infinite explicit layers in implicit networks reduces memory usage and achieves favorable performance trade-offs , where gradient approximations, such as truncated backward gradient for backpropagation , can be applied for even more acceleration.

## 6 NeurIPS Limitation and Broader Impact Statements

These results do not comprehensively search the Anderson hyperparameter space, nor do they establish the multiprocessor scalability at which they are aimed. Saving training and inference time and energy is the broader impact envisioned for this work. Being algorithmic in nature, it has the same potential for applied use and misuse as neural networks in general.