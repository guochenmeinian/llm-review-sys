# DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning

Hao Bai\({}^{1,2}\) Yifei Zhou\({}^{1}\)1 Mert Cemri\({}^{1}\) Jiayi Pan\({}^{1}\)

Alane Suhr\({}^{1}\) Sergey Levine\({}^{1}\) Aviral Kumar\({}^{3,4}\)

\({}^{1}\)UC Berkeley UIUC \({}^{3}\) CMU \({}^{4}\)Google DeepMind

Equal contribution, listed in alphabetical order; work done at UC Berkeley. E-mails: haob2@illinois.edu, yifei_zhou@berkeley.edu, aviralku@andrew.cmu.edu. Project page: https://digitl-agent.github.io/Code available at https://github.com/DigiRL-agent/digitl.

###### Abstract

Training corpuses for vision language models (VLMs) typically lack sufficient amounts of decision-centric data. This renders off-the-shelf VLMs sub-optimal for decision-making tasks such as in-the-wild device control through graphical user interfaces (GUIs). While training with static demonstrations has shown some promise, we show that such methods fall short for controlling real GUIs due to their failure to deal with real world stochasticity and non-stationarity not captured in static observational data. This paper introduces a novel autonomous RL approach, called DigiRL, for training in-the-wild device control agents through fine-tuning a pre-trained VLM in two stages: offline RL to initialize the model, followed by offline-to-online RL. To do this, we build a scalable and parallelizable Android learning environment equipped with a VLM-based evaluator and develop a simple yet effective RL approach for learning in this domain. Our approach runs advantage-weighted RL with advantage estimators enhanced to account for stochasticity along with an automatic curriculum for deriving maximal learning signal. We demonstrate the effectiveness of DigiRL using the Android-in-the-Wild (AitW) dataset, where our 1.3B VLM trained with RL achieves a 49.5% absolute improvement - from 17.7 to 67.2% success rate - over supervised fine-tuning with static human demonstration data. These results significantly surpass not only the prior best agents, including AppAgent with GPT-4V (8.3% success rate) and the 17B CogAgent trained with AitW data (38.5%), but also the prior best autonomous RL approach based on filtered behavior cloning (57.8%), thereby establishing a new state-of-the-art for digital agents for in-the-wild device control.

## 1 Introduction

Advances in vision-language models (VLMs), especially in regards to their remarkable common-sense, reasoning, and generalization abilities imply that realizing a fully autonomous digital AI assistant, that can simplify human life by automating day-to-day activities on computer devices via natural language interfaces, is no longer a distant aspiration . An effective device-control AI assistant should be able to complete tasks in-the-wild through Graphical User Interfaces (GUIs) on digital devices: make travel plans; experiment with presentation designs; and operate a mobile device autonomously, all while running amidst stochasticity and distractors on the device, the Internet, and the tools it interacts with. However, enhanced reasoning or common-sense abilities do not directly transfer to intelligent assistant behavior: ultimately we want AI assistants to accomplishtasks, exhibit rational behavior, and recover from their mistakes as opposed to simply producing a plausible completion to a given observation based on the data seen during pre-training. This implies that a mechanism to channel abilities from pre-training into a deployable AI "agent" is lacking.

Even the strongest proprietary VLMs, such as GPT-4V  and Gemini 1.5 Pro 2, still struggle to produce the right actions when completing tasks on devices. While general-purpose vision-language abilities help these models still make meaningful abstract deductions about novel scenes when deployed, these deductions do not transfer to accurate reasoning for control [47; 45; 55; 44]. As a result, most prior work for building device agents construct complex wrappers around proprietary VLMs by combining them with prompting, search, or tool use [47; 44; 52; 51; 45]. While building prompting or retrieval wrappers to improve decision-making performance of existing VLMs enhances their performance in the short run, without updating the weights, the effectiveness of the resulting agent is inherently limited by the capabilities of the base model [49; 3]. For example, we found that off-the-shelf VLMs make reasoning failures that derail the agent (e.g., Figure 2 and Figure 17), as direct consequences of inability of the base model to reason with low-level device-control actions. A different solution is to fine-tune the model on demonstrations via imitation learning. However, the dynamic nature of the web and device means that models trained to mimic actions in stale data can result in sub-optimalilty as the eco-system changes . Agents trained in this way struggle to recover from the agents' own mistakes [8; 12].

If we can instead build an interactive approach to _train_ a VLM to directly adapt and learn _from its own experience_ on the device and the Internet, that can be used to build a robust and reliable device-control agent, without needing wrappers on top of proprietary models. However, this learning-based approach must satisfy some desiderata. First, it must make use of online interaction data since static demonstration data would not be representative of the task when the model is deployed: for instance, even in the setting of web navigation alone, dynamic nature of in-the-wild websites means that the agent will frequently encounter website versions that differ significantly from the scenarios seen during training and will need to behave reliably despite changes in visual appearance and distractions. Second, learning on-the-fly means the approach must learn from multi-turn interaction data from the model itself, a large of chunk of which would consist of failures. Proper mechanisms must be designed to automatically pick out the correct actions while filtering the wrong ones.

To this end, **our main contribution** is a novel autonomous RL approach, DigiRL (i.e., RL for Digital Agents), for training device control agents, as shown in Figure 1. The resulting agent attains state-of-the-art performance on a number of Android device-control tasks. To train this agent, our approach operates in two phases: an initial offline RL phase to initialize the agent using existing data, followed by an offline-to-online RL phase, that further fine-tunes the model obtained from offline RL on online rollout data. Online RL training requires access to an environment that the agent can interact with and obtain reliable reward signals, all in a reasonable amount of wall-clock time. To do so, we build a scalable and parallelizable Android learning environment equipped with a robust VLM-based general-purpose evaluator  (average error rate 2.8% against human judgement) that supports running up to 64 real Android emulators at the same time to make online RL real-time. Then, to effectively learn autonomously, we develop an online RL approach that retains the simplicity of supervised learning, but incorporates several key deep RL insights to enable fast fine-tuning. Concretely, our approach is a variant of advantage-weighted regression (AWR) , equipped with: **(i)** an automatic curriculum that uses an instruction-level value function to order tasks so as to extract

Figure 1: **DigiRL overview. DigiRL is built upon a VLM that has been pre-trained on extensive web data to develop fundamental skills such as common knowledge, reasoning, and visual grounding. Initially, we employ offline RL to fine-tune the VLM using stale task-specific data, which helps in eliciting goal-oriented behaviors. Subsequently, our agent engages with real-world graphical user interfaces, continuously enhancing its performance through online RL and autonomous performance evaluations.**

maximal learning signal, which is inspired by prioritized replay methods [11; 32; 23], and **(ii)** another step-level value function trained via effective cross-entropy loss [17; 5] to extract low-variance and less-biased learning signal amidst stochasticity and diverse tasks. This RL approach allows us to fine-tune VLMs on their own experience.

We evaluate our agent trained with DigiRL in carrying out diverse instructions from **Android in the Wild dataset** on real Android device emulators and find that our agent can achieve a **28.7% improvement** over the existing state-of-the-art agents (from 38.5% to 67.2% success rate) 18B CogAgent , and over 9% improvement over the prior best autonomous learning approach based on Filtered Behavior Cloning [18; 26]. The performance of our agent also significantly surpasses wrappers on top of state-of-the-art proprietary VLMs such as GPT-4V  and Gemini 1.5 Pro  (17.7% success rate), despite using a significantly smaller model (with 1.3B parameters). To our knowledge, _this is the **first** work to successfully build an autonomous offline-to-online RL approach to enable state-of-the-art performance on device-control problems._

## 2 Related Work

**Multi-modal digital agents.** In contrast to language-only agents that largely interact with both text or code inputs and outputs [33; 49; 3; 30; 46; 20; 13], training multi-modal agents capable of controlling devices presents different challenges: first, device control is done directly at the pixel-level and in a coordinate-based action space, instead of natural language [31; 44] that LLM is most familiar with, and second, the ecosystem of a device and the Internet tends to be quite stochastic and unpredictable, which is absent with high-level planning in language only. To handle these challenges, prior work largely builds on strong proprietary VLMs [24; 7], and designs complex rule-based wrappers [47; 51; 45; 52] to enhance the visual grounding capabilities of VLMs in GUI interfaces and convert text output into pixel interactions. However, without any form of fine-tuning, this limits the room for possible performance improvement [44; 47; 49; 3; 50], especially when pre-training corpora only present limited action-labeled data. A separate line of work fine-tunes VLMs with demonstration data [19; 15; 9; 53] via imitation learning, but maximizing single-step accuracy from scale demonstrations without accounting for consequences of these actions in subsequent steps may lead to poor solutions amidst stochasticity , as agents trained in such ways will struggle to recover from out-of-distribution states not included in the demonstration data [8; 12]. The third category, and perhaps the closest to us, are works that run filtered imitation learning on autonomously-collected data to directly maximize the episode success rate [26; 18]. In contrast, _ours is the first work to scale autonomous, offline-to-online RL_ for device control, producing an agent that outperforms prior agents built via imitation. Even when compared to prior work running on-policy RL in simplified web navigation settings (MiniWob++ [37; 10]), our approach is 1000x more sample efficient (around 1e3 trajectories compared to around 1e6 trajectories), and operates in real-world GUI navigation tasks.

**Environments for device control agents.** Recent works have introduced simulated environments for building device control agents [48; 56; 16; 54; 4; 44]. However, these environments are primarily designed for evaluation, and present only a limited range of tasks within fully deterministic and

Figure 2: **Qualitative comparison between DigiRL and other approaches.** AutoUI trained from static human demonstrations can easily get stuck in out-of-distribution states while GPT-4V often get on a wrong goal (searched “logitech g933bestbuy.com logitech g933” in Google instead of bestbuy.com). In contrast, DigiRL can recover from such states and complete complex instruction as requested.

stationary settings, infeasible for acquiring a diverse repertoire of skills needed for device control. Alternatively, other works use environments with a greater diversity of tasks [48; 37], but these environments often oversimplify the task complexity, thus failing to transfer to in-the-wild settings. Coversely, our training environment utilizes autonomous evaluation  with Gemini 1.5 Pro  to support diverse, open-ended tasks on parallel _actual_ Android devices, at full scale unlike prior environments. This also contrasts other prior works that use single-threaded Android emulators [26; 39; 19] and thus inefficient for support online RL at scale.

**Reinforcement learning for LLM/VLMs.** The majority of prior research employing RL for foundation models concentrates on tasks that must be solved in a single turn, such as preference optimization [25; 58; 2] or reasoning . However, optimizing for single-turn interaction from expert demonstrations may result in sub-optimal strategies for multi-step problems [57; 38; 42], especially amidst a high degree of stochasticity or non-stationarity. Therefore, we focus on building multi-turn RL algorithms that can learn from sub-optimal, online interaction data in this work. While prior works have developed value-based RL algorithms for LLMs [42; 38; 1; 57; 50], they typically require maintaining multiple models such as Q-networks, value-networks, and policy networks, along with their delayed target counterparts, and can be subjective to slow convergence and sensitivity to choices of hyper-parameters. In contrast, we focus on identifying the key design choices for instantiating a simple yet effective RL algorithm for practitioners to incorporate to substantially improve full-scale Android device control. Our approach can serve as a base model for future research.

## 3 Problem Setup and Preliminaries

**Problem formulation.** We are interested in pixel-based interaction with virtual devices. We scope our study in the control of Android devices: this is already significantly more challenging and more general than previous learning-based environments that focus solely on web navigation [16; 56; 4], where the web browser itself is merely one application within our broader environment, and link-based device controls [47; 51] are inadequate for tasks like games that do not support link inputs.

Each episode begins with the emulator initialized to the home screen. Subsequently, a task is selected from a predefined set of language instructions, some examples of which are shown in Appendix A.1. An agent is then tasked with manipulating the emulator to fulfill this instruction. At each time step, the agent receives a screenshot of the current screen as the observation. Following the action space in prior literature , the available actions include tapping and sliding based on normalized \((x,y)\) coordinates (ranging from 0 to 1 relative to the screen dimensions), typing text strings of variable length, and pressing special buttons such as HOME, BACK, and ENTER, as illustrated in Figure 3. Our train and test instructions comes from General and Web Shopping subsets in AitW . These tasks consist of information-gathering tasks like "What's on the menu of In-n-Out?", and shopping tasks on the web like "Go to newegg.com, search for razer kraken, and select the first entry".

**Challenges of stochasticity.** Real-world device contrl presents unique challenges of stochasticity absent in simulated environments [56; 37] such as: **(1)** the non-stationarity of websites and applications, which undergo frequent updates, causing the online observations to be different from stale offline data, **(2)** various unpredictable distractors such as pop-up advertisements, login requests, and the stochastic order of search results. **(3)** technical challenges and glitches such as incomplete webpage loading or temporary access restrictions to certain sites. Examples of scenarios with such stochasticity from

Figure 3: **Environment details.**_Top:_ actions space and dynamics of the environment. _Bottom:_ examples of the read-world non-stationarity and dynamism of the environment.

our experiments are shown in Figure 3. We observe that these stochastic elements pose significant challenges for pre-trained VLMs, including even those fine-tuned on device control data. As a concrete example, Figure 4 shows an experiment result that illustrates the necessity of continuously adapting the models to the non-stationarity of websites and applications. After obtaining a good checkpoint using our approach (DigiRL), that we will introduce in the next section, with autonomous data from June.1 to June.3, we compare the performance of a frozen policy and a continuously updating policy using fresh autonomous data from June.7 to June.11. We find that indeed the the performance of the frozen policy gradually degrades over time due to the changes on websites and applications, while continuous online updates plays a key role in preventing this degradation.

**Setup for reliable and scalable online RL.** As autonomous RL interleaves data collection and training, to maximize learning amidst stochasticity, it is crucial to have a real-time data collection pipeline to collect enough experience for gradient updates. While this is not possible in single-thread Android emulator environments [26; 39] due to latency, we parallelize our Android emulator using appropriate error handling as discussed in Appendix A.1. In addition, the environment must provide a reward signal by judging whether the current observation indicates the agent has successfully completed the task. To generalize our _evaluator_ to support a wide range of tasks, we extend Pan et al. 's end-to-end autonomous evaluator that does not require accessing the internal states of the emulator or human-written rules for each task. This contrasts previous works that manually write execution functions to verify the functional completeness of each task [16; 44; 37; 48]. We adopt Gemini 1.5 Pro [6; 7] as the backbone of the autonomous evaluator. We seed this model with few-shot rollouts and the associated human-labeled success indicators to guide evaluation of novel queries. This pipeline enables a single evaluator that can evaluate all AiTW tasks. The evaluator is highly aligned with human annotations (average error rate 2.8%), validated in Figure 8.

## 4 DigiRL: Autonomous RL for Building a Strong Device-Control Agent

We now present our autonomous RL framework for training device agents. We pose the device control problem as a Markov decision process (MDP) and develop RL methods for this MDP. The core of our approach is based on a simple and scalable off-policy RL method, advantage-weighted regression (AWR) , but we make crucial modifications to handle stochasticity and highly-variable task difficulty, through the use of value functions trained with appropriate losses, and an automatic curriculum, induced by an instruction-level value function to maximize learning.

**Device control and GUI navigation as a MDP.** We conceptualize device control guided by natural language instructions as a finite horizon Markov Decision Process (MDP) represented by \(=\{,,,_{0},,H\}\) and run policy gradient to solve this MDP. At the beginning, an initial state \(s_{0}\) and a natural language instruction \(c\) are sampled from the initial state distribution \(_{0}\). A reward of 1 is given at the end if the agent successfully fulfills the task per the evaluator, otherwise a reward of 0 is given. The trajectory terminates either when the agent accomplishes the task or when the maximum allowed number of interactions \(H\) is exceeded. States are represented using the last two screenshots. To explain our approach in detail, we also include several standard definitions used in reinforcement learning (RL). The Q function for a policy \(\) represents the expected long-term return from taking a specific action at the current step and then following policy \(\) thereafter: \(Q^{}(s_{h},a_{h},c)=_{}[_{t=h}^{H}r(s_{t},a_{t},c)]\). The value function \(V^{}(s_{h},c)\) is calculated by averaging the Q-value, \(Q^{}(s_{h},a_{h},c)\), over actions \(a_{h}\) drawn from the policy \(\). The advantage \(A^{}(s_{h},a_{h},c)\) for a state-action pair is computed by subtracting the state's value under the policy from its Q-value: \(A^{}(s_{h},a_{h},c)=Q^{}(s_{h},a_{h},c)-V^{}(s_{h},c)\).

Figure 4: **Performance of our approach (DigiRL) in different training modes** on the Webshop subset. When utilizing a stale checkpoint, i.e., “frozen” (black+blue curve) performance generally begins to degrade as time evolves, whereas autonomous online training (black+red curve) via DigiRL allows us to retain performance despite non-stationarity and stochasticity.

### Backbone of Our Approach: Off-Policy RL via Advantage-Weighted Regression

The starting point we choose to build our approach on is the advantage-weighted regression (AWR) algorithm , which says that we can improve the policy reliably by regressing the policy towards exponentiated advantages induced by the reward function, as a proxy for optimizing the policy gradient while staying close to the previous policy :

\[*{arg\,max}_{}_{}[(a|s,c) (A(s,a,c)/)],\] (4.1)

for some positive parameter \(\) and the distribution of past experience \(\), and \(A(s,a,c)\) denotes the advantage of a state-action pair \((s,a)\) given a context \(c\). To avoid tuning the hyperparameter \(\), we consider an alternative that does "hard filtering" on the advantages instead of computing \((A)\), similar to prior works . This leads to the following loss function for fine-tuning the model:

\[()=-_{()}[(a|s,c)].\] (4.2)

Typically, these advantages are computed by running Monte-Carlo (MC) rollouts in the environment to estimate the value of a given state-action pair, and subtracting from it an estimate of the value of the state given by a learned value estimator alone. However, this approach is likely to produce high-variance advantages given the stochasticity of the device eco-system that affects MC rollouts.

### Obtaining Reliable Advantage Estimates from Doubly-Robust Estimators

To reliably identify _advantageous_ actions given significant environment stochasticity, we construct a per-step advantage estimator, inspired by doubly-robust estimators :

\[A^{}(s_{h},a_{h},c):=^{H-h}r(s_{H},a_{H},c)+(1-^{H-h }r(s_{H},a_{H},c))(V^{}(s_{h+1},c)+r(s_{h},a_{h},c)-V^{} (s_{h},c)),\] (4.3)

where \(\) is a weighting hyper-parameter. This construction of the advantage estimator is a simplified version of Generalized Advantage Estimation (GAE)  using only the next-step advantage estimator and final-step advantage estimator as there are no intermediate rewards in our problem. This construction balances an advantage estimator with higher variance Monte-Carlo estimates \(^{H-h}r(s_{H},a_{H},c)\) (due to stochasticity) and an estimator with higher bias \(V^{}(s_{h+1},c)+r(s_{h},a_{h},c)-V^{}(s_{h},c)\) (due to imperfect fitting of the value function). We observed that combining both high-variance and high-bias estimators gave us a sweet-spot in terms of performance. To implement the step-level hard filtering, we simply threshold this doubly robust estimator as \(A^{}(s_{h},a_{h},c)>1/H\) to decide which actions progress towards the goal.

### Automatic Curriculum using an Instruction-Level Value Function

While the AWR update (Equation 4.1) coupled with a robust advantage estimator (Equation 4.3) is likely sufficient on standard RL tasks, we did not find it to be effective enough for device control in preliminary experiments. Often this was the case because the task set presents tasks with highly-variable difficulties that collecting more data on tasks that the agent was already proficient at affected sample efficiency negatively. In contrast, maximal learning signal can be derived by experiencing the most informative tasks for the agent during training. To this end, we design an instruction-level value function \(V^{}(c)\) to evaluate if a given rollout can provide an effective learning signal:

\[A^{}(s_{h},a_{h},c):=_{t=h}^{H}r(s_{t},a_{t},c)-V^{}(c)=r(s_{H},a_{H},c)-V^{}(c),\] (4.4)

where \(_{t=h}^{H}r(s_{t},a_{t},c)\) is a Monte-Carlo estimator of \(Q(s_{h},a_{h},c)\). The equality holds because the MDP formulation only provides rewards at the end of a rollout. Intuitively, if a rollout attains a high value of \(A^{}(s_{h},a_{h},c)\), it means the value function \(V^{}\) is small. Therefore, this rollout represents a valuable experience of the agent accomplishing a difficult task, and thus should be prioritized, akin to ideas pertaining to prioritized experience  or level replay . When training the actor with a buffer of historical off-policy data, we first perform a filtering step to identify the top-\(p\) datapoints with highest \(A^{}(s_{h},a_{h},c)\). Then, we use it for AWR (Equation 4.1) with the doubly-robust advantage estimator (Equation 4.3).

**Implementation details.** Inspired by the findings in some recent works  that modern deep learning architectures like transformers  are better trained with cross-entropy losses instead of mean-squared losses, we utilize a cross-entropy objective based on the Monte-Carlo estimate of the trajectory reward for training both of our value functions:

\[(V^{}) =-_{}[r(s_{H},a_{H},c) V^{}(c)+(1-r(s_{ H},a_{H},c))(1-V^{}(c))],\] (4.5) \[(V^{}) =-_{}[r(s_{H},a_{H},c) V^{}(s_{h},a_{h },c)+(1-r(s_{H},a_{H},c))(1-V^{}(s_{h},a_{h},c))].\] (4.6)

**Final algorithm.** The final practical algorithm is shown in Figure 5. The instruction-level value function estimates the values of the trajectories, which is trained with loss shown in Equation (4.5). The step-level value function estimates the values of states, which is trained with loss shown in Equation (4.6). When training the actor, we first filter out trajectories and states using the value functions as shown in Equation (4.4) and Equation (4.3), then train the actor with the MLE loss shown in Equation (4.2) on the filtered data.

## 5 Experimental Evaluation

The goal of our experiments is to evaluate the performance of DigiRL on challenging Android device control problems. Specifically, we are interested in understanding if DigiRL can produce agents that can effectively learn from autonomous interaction, while still being able to utilize offline data for learning. To this end, we perform a comparative analysis of DigiRL against several prior approaches, including state-of-the-art agents in Section 5.1. We also perform several ablation experiments to understand the necessity and sufficiency of various components of our approach in Section 5.2.

**Baselines and comparisons.** We compare DigiRL with: **(a)** state-of-the-art agents built around proprietary VLMs, with the use of several prompting and retrieval-style techniques; **(b)** running imitation learning on static human demonstrations with the same instruction distribution, and **(c)**a filtered BC approach . For proprietary VLMs, we evaluate **GPT-4V** and **Gemini 1.5 Pro** both zero-shot and when augmented with carefully-designed prompts. For the zero-shot setting, we use the prompt from Yang et al.  and augment the observation with Set-of-Marks . Set-of-Marks overlays a number for each interactable element over the screenshot, so that a VLM can directly output the number of the element to interact with in plain text instead of attempting to calculate pixel coordinates, which is typically significantly harder. We also compare with AppAgent , which first prompts the VLM to explore the environment, and appends the experience collected to the test-time prompt. We also compare with two state-of-the-art fine-tuning methods for Android device control: **AutoUI** (specifically AutoUI-Base ) and **CogAgent**. AutoUI-Base uses an LM with 200M parameters, and a a vision encoder with 1.1B parameters. CogAgent has 11B parameters for its vision encoder and 7B for its LM. The supervised training corpus for both AutoUI-Base and CogAgent contains AitW, including the instruction set and the emulator configuration we use.

**Base VLM and offline dataset.** Both **Filtered BC** and **DigiRL** use trained AutoUI-Base checkpoints with the image encoder frozen. The instruction and step-level value functions for DigiRL employ this same frozen image encoder. The visual features output from the encoder are concatenated with instruction features derived from RoBERTa . A two-layer MLP is then used to predict the value function. In the offline phase, the offline dataset is collected by rolling out the initial AutoUI-Base supervised trained checkpoint as policy. For fair comparisons, we keep the number of offline data collected in the pure offline training roughly the same as the total number of data collected in the offline-to-online training. Due to the dynamic nature of the Internet-device eco-system, our offline data was stale by the time we were able to run our offline-to-online experiments, and this presented additional challenge in offline-to-online learning. In both General and Web Shopping subsets, offline experiments make use of around 1500 trajectories while offline-to-online experiments start with

Figure 5: **Algorithm visualization. The two value function are first trained with original distribution of collected trajectories according to Equation (4.5) and Equation (4.6), then used to filter the trajectories for training the actor. We use the MLE loss (Maximum Likelihood Estimation loss) to train the actor.**around 500 offline trajectories and update with another 1000 online trajectories. In the offline phase, DigiRL skips instruction-level filtering and instead trains the actor with all successful trajectories to make full use of the offline data. See a detailed breakdown of our dataset in Appendix A.1.

### Main Results

Our main results are summarized in Table 1 and Figure 6. We find that on both AitW General and AitW Web Shopping subsets, the agent trained via DigiRL significantly outperforms prior state-of-the-art methods based on prompting and retrieval (AppAgent + GPT-4V/Gemini 1.5 Pro) or training on static demonstrations (CogAgent and AutoUI), by a large margin with more than **49.5% absolute improvement** (from 17.7% to 71.9% on the General subset and from 17.7% to 67.2% on the Web Shopping subset). Notably, this improvement from DigiRL is realized _fully autonomously without making use of human supervision_ (e.g. manually labeled rollouts or hand-written verifiers).

**Are inference-time prompting and retrieval techniques or supervised training enough for device control?** Delving into Table 1, we observe that off-the-shelf proprietary VLMs, even when supplemented with the set-of-marks mechanism, do not attain satisfactory performance: both GPT-4V and Gemini 1.5 Pro achieve success rates under 20%. One possible cause could be the under-representation of Android device data in the pre-training data. Moreover, inference-time adaptation strategies such as AppAgent  show minimal improvement, with gains not exceeding 5% for either model. All this evidence suggests a limited scope for improvement without fine-tuning of some sort. As illustrated in Figure 7, the primary failures of these VLMs stem from hallucinatory reasoning that lead the VLMs to land on a relevant but wrong page. This suggests that while state-of-the-art VLMs excel at reasoning problems in code and math, their reliability in less-familiar domains, such

    & &  &  \\   & & Train & Test & Train & Test \\   &  & GPT-4V & 5.2 & 13.5 & 3.1 & 8.3 \\  & & Gemini 1.5 Pro & 32.3 & 16.7 & 6.3 & 11.5 \\   &  & GPT-4V & \(13.5\) & 17.7 & 12.5 & 8.3 \\  & & Gemini 1.5 Pro & \(14.6\) & 16.7 & 5.2 & 8.3 \\   & Supervised & CogAgent & \(25.0\) & 25.0 & 31.3 & 38.5 \\  & Training & AutoUI & \(12.5\) & 14.6 & 14.6 & 17.7 \\    &  & Filtered BC & \(51.7 5.4\) & \(50.7 1.8\) & \(44.7 1.6\) & \(45.8 0.9\) \\  & & **Ours** & \(46.9 5.6\) & \(62.8 1.0\) & \(39.3 6.0\) & \(45.8 6.6\) \\   & &  & Filtered BC & \(53.5 0.8\) & \(61.5 1.1\) & \(53.6 4.7\) & \(57.8 2.6\) \\   & & **Ours** & \(63.5 0.0\) & \(\) & \(68.2 6.8\) & \(\) \\   

Table 1: **Main comparisons of different agents across various settings.** Each offline experiment is repeated three times and the mean and standard deviation are reported. Each online experiment is repeated two times. Results are evaluated with our autonomous evaluator with the first 96 instructions in the train and test set. Correlation of our correlation and human judgements can be found in Figure 8.

Figure 6: **Offline-to-online training curves for Filtered BC and DigiRL.** Curves are smoothed with exponential weighting over the x-axis. _Left:_ AitW General. _Right:_ AitW Web Shopping. Two runs for each model are started on two different dates with at least two days apart. Observe that DigiRL is able to improve faster with a fewer number of samples. Since the data collection frequency is the bottleneck, these performance trends directly reflect performance trends against wall-clock time as well.

as device control, remains inadequate. For example, for the instruction "Go to newegg.com, search for alienware area 51, and select the first entry", a GPT-4V based agent erroneously searched "alien area 51 ebay" in Google.com and decided that it had made progress towards the task (Figure 17).

Training on domain-specific human demonstrations, however, does boost performance, allowing the smaller, specialized VLM, AutoUI with 1.5 billion parameters, to match or surpass the larger, generalist VLMs like GPT-4V and Gemini 1.5 Pro. Nonetheless, this supervised imitation learning approach still fall short, with success rates on both subsets remaining below 20%. This shortcoming is not fundamentally addressed via enhancements in model scale or architecture, as evidenced by CogAgent , with 18 billion parameters still achieving performances below 40% success rate. As depicted in Figure 7, a predominant failure mode for these agents is an inability to rectify their own errors. An example trajectory that we observed is that for the instruction "what's on the menu of In-n-Out", the agent accidentally activated the voice input button, and failed to quit that page until the step limit. In contrast, DigiRL is able to recover from the errors more efficiently( Appendix C.2).

**Comparison of different RL approaches.** In Table 1 and Figure 6, we present a comparative analysis of various autonomous approaches. Notably, both offline and offline-to-online configurations demonstrate that our RL approach, when augmented with a continuous stream of autonomous interaction data and reward feedback, substantially improves performance. This improvement is evident from an increase in the success rate from under 20% to over 40%, as the agent learns to adapt to stochastic and non-stationary device interfaces. Moreover, although the total sample sizes for offline and offline-to-online settings are equivalent, the top-performing offline-to-online algorithm markedly surpasses its offline counterpart (75% versus 62.8% on the General subset). This highlights the efficacy of autonomous environment interaction, and establishes the efficacy of DigiRL in learning from such uncurated, sub-optimal data. Lastly, DigiRL consistently outperforms the state-of-the-art alternative, Filtered BC, across both the General and Web Shopping subsets, improving from 61.5% to 71.9% and 57.8% to 61.4%, respectively, highlighting DigiRL's performance and efficiency.

### Analysis and Ablations

**Failure modes analysis.** We conduct an additional user study to annotate the failure modes for each agent as shown in Figure 7, and a more fine-grained breakdown can be found in Appendix D. At a high level, we classify the major failure modes of all agents into the following three categories: **(1) Failure to recover from mistakes** refers to the scenario where the agent made a mistake that led it to states from which it failed to quickly recover and resume the task, such as a wrong search page. **(2) Getting stuck midway** refers to the failure mode where the agent gets distracted on the right track to completing the instruction and as a result fails to accomplish the task. For example, failing to click on the right link or failing to search after typing the key words. **(3) Arriving at wrong goal** refers to the failure mode where the agent arrives at a wrong page and mistakenly thinks that it had completed the task. For e.g, the agent finds a machook on costco.com instead of finding a machook on ebay.com.

While all the types of failure modes benefit from offline and offline-to-online RL training as shown in Figure 7, the most consistent and significant reduction is probably for the failure mode of failing to recover from mistakes. This is because while pre-trained models, generating plausible future tokens, can get distracted by the dynamic nature of the environment and, as a result, encounter at never-before-seen states. With no clue of how to escape such states, these methods are unable to recover and fail to solve the task. In contrast, by training on autonomously-collected rollouts, our agent DigiRL is able to learn from its own mistakes and reduces failures to recover over training.

Figure 7: **Failure modes for each approach** on both the AiTW General and Web Shopping subsets. We found that the failure mode RL training is most effective at reducing compared to model supervised trained on human data is “Fail to recover from mistakes”. A more fine-grained decomposition can be found in Appendix D.

**Ablation study of each component in DigiRL.** We conduct an ablation study on different components of DigiRL in Figure 9. We find that all the components used by our approach are necessary: **(1)** using cross-entropy for training the value functions boosts performance by around 12% (compare Ours and Ours w/Regression); **(2)** using step-level advantages improves efficiency by 12% (comparing Ours and Ours w/o step-level advantage); **(3)** the use of automatic curriculum improves the speed of learning by around 25% (comparing Ours w/o step-level advantage and Filtered BC); **(4)** Ours outperforms vanilla AWR that does not employ a doubly-robust advantage estimator or curriculum.

Additionally, we also observe no degradation in performance as a result of "hard-filtering", as show by nearly comparable performance of our approach and the best run of exponential filtering obtained via an extensive tuning of the temperature hyperparameter \(\) in naive AWR (comparing Ours and Ours w/ vanilla AWR reweighting), despite simplicity of implementation in the hard filtering approach. Putting together, these choices result in a new state-of-the-art RL approach for device control.

**Evaluation of our autonomous evaluator.** In Figure 8, we present the findings from a user study aimed at assessing the accuracy of our autonomous evaluator. Our results indicate that the success rates reported by our automatic evaluator are remarkably consistent with those assessed by human evaluators across almost all models, with differences less than 3%. Furthermore, we observed that evaluations on the Web Shopping subset are more precise compared to those on the General subset. This increased accuracy likely stems from the fact that tasks in the General subset are formulated in free-form language, which can introduce ambiguity, whereas the Web Shopping subset features a narrower range of language expressions, reducing potential variability.

## 6 Discussion and Limitations

In this paper, we propose a novel autonomous RL approach, DigiRL, for training in-the-wild, multimodal, device-control agents that establish a new state-of-the-art performance on a number of Android control tasks from Android-in-the-Wild dataset . To achieve this, we first build a scalable and parallelizable Android environment with a robust VLM-based general-purpose evaluator that supports fast online data collection. We then develop a system for offline RL pre-training, followed by autonomous RL fine-tuning to learn via interaction, admit the stochasticity of the real-world Internet and device eco-system. Our agent achieves a 280% improvement over the previous state-of-the-art agents (from 17.7% to 68.2% in terms of task success rate), including AppAgent based on GPT-4V and Gemini 1.5 Pro, and supervised trained models such as AutoUI and CogAgent.

Due to computational limitations, and despite the fact that the parallel emulator and autonomous evaluator can be easily extended to complicated tasks, our agent is trained only with tasks from AitW instead of a all possible tasks on the device. Our design of the DigiRL algorithm aims for maximal implementation simplicity, so we hope that our approach to serve as a base algorithm for future research to build on, including algorithmic research as well as expanding the space of tasks.

Figure 8: **Correlation between our autonomous evaluator and human judgements for all policy models** on General and Web Shopping subsets. For repeated offline and online runs, we report the correlation results for the run with the highest autonomous evaluation success rate.

Figure 9: **Ablation study results on the AitW Web Shopping subset.**