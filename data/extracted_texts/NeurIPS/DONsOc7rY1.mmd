# Horse: Hierarchical Representation for Large-Scale Neural Subset Selection

Binghui Xie, Yixuan Wang, Yongqiang Chen, Kaiwen Zhou, Yu Li, Wei Meng, James Cheng

Department of Computer Science and Engineering

The Chinese University of Hong Kong

###### Abstract

Subset selection tasks, such as anomaly detection and compound selection in AI-assisted drug discovery, are crucial for a wide range of applications. Learning subset-valued functions with neural networks has achieved great success by incorporating permutation invariance symmetry into the architecture. However, existing neural set architectures often struggle to either capture comprehensive information from the superset or address complex interactions within the input. Additionally, they often fail to perform in scenarios where superset sizes surpass available memory capacity. To address these challenges, we introduce the novel concept of the _Identity Property_, which requires models to integrate information from the originating set, resulting in the development of neural networks that excel at performing effective subset selection from large supersets. Moreover, we present the Hierarchical Representation of Neural Subset Selection (HORSE), an attention-based method that learns complex interactions and retains information from both the input set and the optimal subset supervision signal. Specifically, HORSE enables the partitioning of the input ground set into manageable chunks that can be processed independently and then aggregated, ensuring consistent outcomes across different partitions. Through extensive experimentation, we demonstrate that HORSE significantly enhances neural subset selection performance by capturing more complex information and surpasses the state-of-the-art methods in handling large-scale inputs by a margin of up to 20%.

## 1 Introduction

Set-valued functions are of great importance to a wide range of real-world applications. For example, anomaly detection aims to identify a set of outliers from a larger dataset that could be users or financial transactions (Zhang et al., 2020). Another example is the recommender system, where the objective is to identify a set of products that better satisfy customer preferences (Ou et al., 2022). In these scenarios, there is a need for implicitly learning a set function (Rezatofighi et al., 2017; Zaheer et al., 2017) that quantifies the usefulness of a given subset of the inputs. The set function assigns a utility value to each subset, and the subset with the highest utility corresponds to the most desired output.

To illustrate the concept, let us consider the task of a recommender system. In this task, we aim to recommend a subset of items \(S\) from a larger item pool \(V\), denoted as \(S V\), that maximizes the utility of \(S\) with respect to the satisfaction of the customers. The utility can be captured by a parameterized utility function, denoted as \(F_{}(S;V)\), and our goal is to optimize the following criteria:

\[S^{*}=*{arg\,max}_{S 2^{V}}F_{}(S;V).\] (1)

One straightforward method (Balcan and Harvey, 2018) involves explicitly modeling the utility by learning the function \(U=F_{}(S;V)\) using supervised data. This data consists of pairs\(\{(S_{i},V_{i}),U_{i}\}_{i=1}^{N}\), where \(U_{i}\) represents the actual utility value of subset \(S_{i}\) given the respective item pool \(V_{i}\). However, this training approach becomes challenging to implement due to the significant number of supervision signals required, which can be expensive and time-consuming to acquire (Ou et al., 2022). To overcome this limitation, an alternative approach is to tackle Eq. 1 using an implicit learning method from a probabilistic perspective (Tschiatschek et al., 2018). This approach requires utilizing data in the form of \(\{(V_{i},S_{i}^{*})\}_{i=1}^{N}\), where \(S_{i}^{*}\) represents the optimal subset corresponding to \(V_{i}\). The objective is to estimate the parameters \(\) such that Eq. 1 holds for all possible \((V_{i},S_{i})\). In practical training, with limited data sampled from the underlying distribution \(P(S,V)\), the empirical log-likelihood \(_{i=1}^{N}[ p_{}(S^{*}|V)]\) is maximized for all the data pairs \(\{S,V\}\), where \(p_{}(S|V)\) is proportional to \(F_{}(S;V)\) for all \(S 2^{V}\)(Ou et al., 2022; Xie et al., 2024). Additional details are available in Appendix D.3.

The crux of the matter lies in determining the structure of neural networks that can effectively model \(F_{}(S,V)\) throughout the entire process. One commonly employed approach in the literature is to utilize an encoder to generate feature vectors for each element in \(V\). These vectors are then inputted into DeepSets (Zaheer et al., 2017), along with the corresponding supervised subset \(S\), in order to learn the permutation invariant set function \(F(S)\). However, this methodology may neglect the interaction between \(S\) and \(V\), leading to a reduction in the expressive capacity of the models. In the previous study, Xie et al. (2024) suggest incorporating the sum-pooling representation of \(V\) into \(S\) to enhance the performance. Yet, the simple integration in Xie et al. (2024); Wang et al. (2024) limits its capacity to effectively model interactions among elements or subsets within these sets. Furthermore, the approach struggles with high-cardinality sets \(V\), as encoding the entire set into memory may not be feasible (Bruno et al., 2021).

To address these problems, and inspired by (Willette et al., 2023), we introduce the notion of the _Identity Property_, a desirable concept for the effective functioning of the model \(F(S,V)\). Identity Property requires \(F(S,V)\) to accurately reflect which set \(V\) the information \(S\) originates from. In order to capture the interplay between \(S\) and \(V\) by adhering to the Identity Property, we propose a subset-based attentive set encoder. Additionally, this encoder facilitates the division of a large set \(V\) into smaller and manageable subsets. These subsets can be processed independently and later aggregated, ensuring no loss of information from \(V\). Hence, our approach is able to efficiently handle large-scale subset representation learning. As depicted in Figure 1, our method is capable of modeling more complex information and managing large-scale inputs more effectively than the two state-of-the-art approaches in the field of Neural Subset Selection tasks, EquiVSet (Ou et al., 2022) and INSET (Xie et al., 2024).

In this work, we make several contributions to the field of neural subset selection, which can be summarized as follows:

* We introduce and rigorously define a critical concept termed as the _Identity Property_ for neural subset selection. This property requires that models can reliably determine the source

Figure 1: Comparison of HORSE to the state-of-the-arts EquiVSet and INSET in handling subsets. “S” represents the supervision, indicating the specific subset of interest. “+” refers to the aggregation of different vectors, which is implemented through concatenation in practice. Unlike EquiVSet and INSET, the HORSE model captures more complex information from \(V\) by employing attention mechanisms. Furthermore, HORSE facilitates the division of \(V\) into distinct partitions.

set \(V\) from which the information of the subset \(S\) is derived, which is a crucial requirement for neural subset selection tasks.
* To adhere to the Identity Property and model complex interaction, we present a subset-based attention mechanism. This mechanism is crafted to learn the Hierarchical Representation of Neural Subset Selection, denoted as HORSE. Our theoretical analysis confirms that HORSE not only upholds the Identity Property but also maintains Permutation Invariance.
* Through extensive empirical research, we validate the effectiveness of HORSE. Our experiments across a variety of datasets demonstrate the consistently superior performance of HORSE. Additionally, we specifically explore HORSE's capabilities in large set environments, further showcasing its practical applicability and efficiency compared with the baselines.

## 2 Related Work

### Set Encoding.

The exploration of network architectures tailored for set-structured inputs has become a vibrant area of research in recent years. A number of key studies (Ravanbakhsh et al., 2017; Edwards and Storkey, 2017; Zaheer et al., 2017; Qi et al., 2017; Horn et al., 2020; Bloem-Reddy and Teh, 2020; Wang et al., 2023) have laid the groundwork in this domain, primarily focusing on creating models that are permutation equivariant using conventional feed-forward neural networks. These foundational models have been successful in universally approximating continuous permutation-invariant functions, primarily through the application of set-pooling layers to aggregate information across different elements of a set regardless of their order.

However, these methodologies have primarily concentrated on learning representations at the aggregate set level, paying less attention to more nuanced interactions occurring at elements. Recognizing this gap, more recent research efforts have aimed at introducing more sophisticated interaction modeling within invariant set functions for various applications. A notable example is the work by (Lee et al., 2019), which incorporates self-attention mechanisms to facilitate the processing of elements within sets, thereby effectively capturing element-wise interactions. Moreover, the concept of Janossy pooling, proposed by (Murphy et al., 2018), introduced a novel approach to incorporate higher-order interactions within the pooling process. Since then, subsequent studies have built upon this advancement, leading to further refinements and innovations in the field, e.g., (Kim, 2021; Li et al., 2020; Bruno et al., 2021; Willette et al., 2023).

### Hierarchical Set Function.

The existing literature primarily concentrates on processing entire input sets, often overlooking the information provided by the sub-levels. Addressing this oversight, Maron et al. (2020) introduced an innovative approach that integrates the symmetry of elements to generate representations of an input set. This methodology was further expanded into a broader context by Wang et al. (2020). Moreover, Bevilacqua et al. (2022) proposed a novel framework aimed at enhancing graph representations by including whole-graph representations to encode each subgraph. Along similar lines, Xie et al. (2024) developed an information aggregation module designed to learn \(F(S,V)\) effectively.

Despite these advancements, a gap remains in the current research landscape. These methods tend to overlook more complex interactions between elements or subsets within sets. Furthermore, they often fall short in scenarios where the input set has a significantly large cardinality, indicating a need for more scalable and interaction-sensitive approaches in set processing. In Table 1, we compare our proposed method with importance baselines commonly used in subset selection tasks. Specifically, DeepSets

   Model & Attn & V & Large-Scale \\  DeepSets (Zaheer et al., 2017) & ✗ & ✗ & ✓ \\ Set Transformer (Lee et al., 2019) & ✓ & ✓ & ✗ \\ EquiVSet (Ou et al., 2022) & ✗ & ✗ & ✓ \\ INSET (Xie et al., 2024) & ✗ & ✓ & ✗ \\  HORSE & ✓ & ✓ & ✓ \\   

Table 1: Properties of Various Methods: “Attn” indicates the use of the attention mechanism, “V” signifies the explicit utilization of information from \(V\), and ‘Large-scale’ denotes the capability of the methods to generalize effortlessly to large-scale settings.

can handle large-scale settings but may lose complex information due to its simple pooling-based structure. Set Transformer excels at modeling complex information within sets but faces challenges with large input set sizes. Methods tailored for subset selection tasks, like EquiVSet and INSET, may struggle with learning intricate interactions and often overlook large-scale settings. HORSE is designed to address these drawbacks.

### Core Subset Selection

Recent work has focused on extracting subsets from training datasets to decrease cost and improve effectiveness (Wei et al., 2015; Mirzasoleiman et al., 2020; Yang et al., 2023). This research also highlights the importance of modeling the relationship between the original dataset and its subsets. Unlike neural subset selection, these core subsets are unlabeled, and typically, more data in the core subset enhances the performance of the models. Our approach differs in that our optimal subset is labeled within the training set, and its size is constrained.

## 3 Method

### Preliminaries

In this paper, we concentrate on the development of neural networks for the purpose of modeling the hierarchical set function \(F(S,V)\), a critical component for tasks involving sets, such as neural subset selection. For every ground set \(V\), assumed to consist of \(n\) elements represented as \(x_{i}\), that is, \(V=\{x_{1},x_{2},...,x_{n}\}\), each element \(x_{i}\) belonging to \(\) is characterized by a \(d\)-dimensional tensor. Typically, the ground set \(V\) can be conceptualized as an assembly of multiple disjoint subsets, explicitly \(V=S_{1} S_{2} S_{m}\), where \(S_{i} S_{j}=\) for \(i j\) and each \(S_{i}\) is a subset in \(^{n_{i} d}\). In this context, \(n_{i}\) denotes the number of elements in subset \(S_{i}\). Generally, \(S V\) acts as a supervisory signal in the form of a mask over \(V\) to indicate the elements to be selected. For the sake of clarity, we define \(S\) as the concrete subset derived from this mask.

In the context of neural subset selection, the task entails the encoding of subsets \(S_{i}\) into representative vectors to forecast the associated function value \(Y\). Traditional approaches, such as those documented in Zaheer et al. (2017) and Ou et al. (2022), involve directly selecting \(S_{i}\) based on the encoding embeddings of all elements within \(V\), subsequently feeding \(S_{i}\) into feed-forward networks. Nonetheless, these methods model the function \(F(S_{i},V)\) solely based on the explicit subsets \(S_{i}\), potentially leading to suboptimal results due to the omission of the broader context provided by the ground set \(V\). This section introduces a novel attention-based method for encoding subset representations, which distinctively incorporates information from the entire input set \(V\), thereby enhancing performance.

### Identity Property

To effectively model \(F(S,V)\), Xie et al. (2024) have proposed to combine the representations of \(V\) and \(S\). In practice, this involves utilizing two DeepSets architectures, as proposed by Zaheer et al. (2017), to independently process \(S\) and \(V\) before merging their outputs, as presented by Figure 1. Given that set pooling operations process each element independently, certain information about the interactions among elements is inevitably lost. This omission can render some problems more challenging than necessary. To address this issue and facilitate the learning of complex interactions within sets, we introduce the following principles:

**Property 3.1**.: _Consider \(V^{n d}\) and \(S V\) where \(S^{s d}\), assuming that \(V\) is partitioned into a random collection of disjoint subsets \(V=S_{1} S_{2} S_{m}\). Here, \(m\) varies within the range \([1,n]\), dependent on the chosen method of partitioning. The function \(F\) is said to satisfy the Identity Property if and only if there exist functions \(g\) and \(h\) such that_

\[F(h(S),h(V))=F(h(S),g(h(S_{1}),,h(S_{m}))),\] (2)

_where \(g\) serves as an aggregation function that effectively combines the encoded representations of the subsets, ensuring that \(F\) leverages both the specific subset \(S\) and the ground set \(V\) through the transformations applied by \(h\) and the aggregation by \(g\)._

The method introduced by Xie et al. (2024) is notable for satisfying the Identity Property through its utilization of sum-pooling to simultaneously process all elements. However, this approach may not be practical for scenarios involving large inputs and may struggle to capture more complex information. In response to these limitations, we propose an attention-based method designed to fulfill the requirements of the Identity Property. Additionally, our interpretation of this property accommodates scenarios where \(S\) differs from the union of subsets \(\{S_{1} S_{2} S_{m}\}\). In practice, \(S\) is often chosen to be \(S_{1}\) for simplicity. This nuanced approach allows for greater flexibility and effectiveness in encoding set information, especially in complex or large-scale settings.

### Attention-Based Set Representation

In this section, we introduce a formulation for an attention-based set encoding function \(F\), leveraging the concept of partitions (referred to as slots in (Bruno et al., 2021; Willette et al., 2023)). Given a ground set \(V^{n d}\), we randomly divide it into \(m\) subsets. For each subset, we allocate a unique embedding \(s_{i}^{d_{s}}\). Furthermore, we establish \(=[s_{1},,s_{m}]^{T}\) as a matrix in \(^{m d_{s}}\). Similar to (Willette et al., 2023), we initialize \(\) by sampling \(m\) embeddings \(s_{i}\) from a parameterized Gaussian distribution with random initialization. Following this setup, we calculate the unnormalized attention scores between \(\) and \(V\), facilitating a dynamic weighting of elements within \(V\) based on their relevance to each partition's embedding. This process aims to capture the nuanced interrelations within subsets and between elements and their corresponding subsets.

\[q =LN( W^{q}),\] (3) \[k =VW^{k},\] \[v =VW^{v},\] (4)

In this expression, "\(LN^{n}\) represents Layer Normalization, and the matrices \(W^{q}^{d_{s} d_{h}}\), \(W^{k}^{d d_{h}}\), and \(W^{v}^{d d_{h}}\) are introduced. These matrices serve to project \(V\) and \(\) into a shared dimensional space \(d_{h}\). Subsequently, we employ a dot product attention mechanism to assess the interactions between \(V\) and \(\). This process is governed by the specified formulation, strategically aligning elements of \(V\) with the embeddings in \(\) through dimensional congruence, thus enabling a nuanced, attention-driven analysis of set elements in relation to their partitioned subsets.

\[ =^{-1}} qk^{T},\] (5) \[ =()^{m n},\] (6)

where \(\) denotes an element-wise activation function. Utilizing the unnormalized attention scores, denoted by \(\), we proceed to define the following mapping operation:

\[=()v.\] (7)

In this context, \(\) signifies a transformation function mapping from \(^{n d}\) to \(^{m d_{h}}\). The term "nl" represents a normalization operation, defined as follows:

\[()_{i,j}=_{i,j}/_{i=1}^{m}_{i,j},\] (8)

which normalizes the column of \(\). Then, we can apply a pooling function (such as sum, mean, min, or max) across the columns of \((V)\) and select the sigmoid function for \(\), thereby establishing an attention mechanism akin to the SSE (Set Stream Embedding) method proposed by Bruno et al. (2021). However, this approach has its limitations. Given that the attention score \(()_{i,j}\) is calculated independently of the other \(n-1\) attention scores, it is not feasible for the rows of \(()\) to form convex coefficients, unlike the softmax outputs typically observed in conventional attention mechanisms, as described by Willette et al. (2023).

To address this issue, we follow Willette et al. (2023) to aggregate information across all rows of \(()\), thereby incorporating dependencies among different set elements into the attention mechanism. This is achieved through a specific normalization process, outlined as follows:

\[M=diag(()_{n})^{-1}\] (9)

where \(_{n}=(1,,1)^{n}\) represents a vector of ones of dimension \(n\), and \(M_{2}^{m}\) signifies a vector in an \(m\)-dimensional space. Subsequently, we can compute \(h(V)\) by applying the normalization term \(M\), as follows:

\[h(V)=M()VW^{v}.\] (10)

Since \(S_{i}\) is a subset of \(V\) based on a partition method, \(h_{V}(S_{i})\) can be derived from \(h(V)\). Detailed steps are provided in Algorithm 1. For simplicity, we omit \(V\) and use \(h(S_{i})\) going forward. This process ensures that \(h\) meets the criteria specified in Eq. 2. By constructing such an \(h\) function, we ensure that the model can recognize the input set \(V\) regardless of its partitioning, leading to the property that \(g(h(S_{1}),h(S_{2}),,h(S_{m}))\) yields the same value for any partition of \(V\).

Furthermore, it facilitates the learning of interactions among the partitioned segments of \(V\), essentially enabling the model to identify the characteristics of the input ground set \(V\). Specifically, we concatenate \(h(S)\) with \(g(h(S_{1}),h(S_{2}),,h(S_{m}))\). In practice, a Multilayer Perceptron (MLP) is utilized to process the concatenated tensor into a vector \(Z^{d_{a}},\) achieving the following:

\[Z=F(h(S),g(h(S_{1}),h(S_{2}), h(S_{m})))^{d_{a}}.\] (11)

Given that the aforementioned process delineates the entire calculation in a matrix format, which may be complex for some readers, we have taken steps to enhance comprehension. To better illustrate how our method establishes an attention map between subsets \(S_{i}\), we have detailed the procedural steps in the Appendix (see Algorithm 1), with a particular focus on the generation of \(h(S_{i})\). This will not only clarify the operational details but also emphasize the underlying methodology and thought process involved in constructing \(h(S_{i})\).

## 4 Theoretical Results

In the realm of machine learning, particularly within the scope of set-based tasks, a fundamental requirement is the invariance to the permutation of input set elements. This characteristic ensures that the computation or outcome of a task is unaffected by the order in which the set's elements are presented, a principle that is especially pertinent to neural subset selection tasks. To address and formalize this aspect within the context of our proposed method, we present a theorem that rigorously demonstrates the permutation invariance of our approach.

**Theorem 4.1**.: _Let \(_{n}\) denote the set of all permutations of a given set \(V\). Since \(V^{n d}\) is represented by a matrix, let \(_{V}^{n n}\) be a random permutation applied to \(V\). Given that \(S V\) represents a subset of \(V\), the permutation \(_{V}\) naturally induces a corresponding permutation \(_{S}\) on \(S\). Under these conditions, HORSE exhibits permutation invariance, which is defined as:_

\[F(h(S),h(V))=F(h(_{S} S),h(_{V} V))\] (12)

Theorem 4.1 assures that irrespective of how the elements in the input set \(V\) are ordered, the output generated by HORSE remains consistent. This property is crucial for ensuring the reliability and applicability of our method across a wide spectrum of set-based tasks, where the inherent order of data points should not influence the task outcome. Furthermore, an illustration of the underlying concept is provided in Figure 2.

Figure 2: This figure illustrates the HORSE model’s capability to achieve Permutation Invariance and satisfy the Identity Property in subset selection tasks. It demonstrates that HORSE maintains consistent output despite the permutation of input set elements and the partition if the ground set.

**Theorem 4.2**.: _If \(\) represents a strictly positive element-wise activation function, then HORSE satisfies Property 3.1._

By satisfying this property, HORSE ensures that its encoding captures both the individual characteristics of subsets within \(V\) and the overarching structure of the entire set, thereby facilitating a more nuanced and comprehensive understanding of set information. The proofs is inspired by (Willettte et al., 2023) and can be found in Appendix A.

## 5 Experiments

In this section, we aim to demonstrate that HORSE significantly outperforms baseline models in a suite of benchmarks tailored to Neural Subset Selection tasks. Subsequently, we extend our investigation to scenarios involving large-scale input settings. Due to the page limitation, we have included additional experiments in Appendix D.

**Evaluations.** To assess the performance of various methods, we employ the mean Jaccard coefficient (MJC) as the evaluation metric. This metric quantifies the similarity between the predicted subset \(S^{}\) and the true subset \(S^{*}\) for each data sample \((S^{*},V)\). The Jaccard coefficient is calculated as follows: \(JC(S^{*},S^{})= S^{}|}{|S^{} S^{ }|},\) where the intersection and union operations determine the size of the overlap and the total unique elements in both sets, respectively. The MJC is then derived by averaging the Jaccard coefficient across all test set samples. Please note that all the following reported performance metrics are presented in percentages, with a default multiplication factor of \(100\%\).

**Baselines.** We conducted experiments compared with several approaches: Random, PGM (Tschiatschek et al., 2018), DeepSet (Zaheer et al., 2017), Set Transformer (Lee et al., 2019), EquiVSet (Ou et al., 2022), and INSET (Xie et al., 2024). The Random approach represents the expected performance of a random guess. DeepSet and Set Transformer are well-known methods or frameworks that satisfy permutation invariance, making them suitable for Neural Subset Selection tasks. PGM, EquiVSet and INSET are specifically designed for subset selection tasks. More comprehensive details available in Appendix D.

   Categories & Random & PGM & DeepSet & Set-T & EquiVSet & INSET & HORSE \\   Gear & 7.7 & 47.1\(\)0.4 & 37.9\(\)0.5 & 64.7\(\)0.6 & 72.5\(\)1.1 & 80.8\(\)1.2 & **83.2\(\)1.3** \\ Bath & 7.6 & 56.4\(\)0.8 & 41.8\(\)0.7 & 71.6\(\)0.5 & 76.4\(\)2.0 & 86.2\(\)0.5 & **87.6\(\)1.0** \\ Toys & 8.3 & 4.41\(\)0.4 & 42.1\(\)0.5 & 62.5\(\)2.0 & 68.4\(\)0.4 & 76.9\(\)0.5 & **77.4\(\)0.9** \\ Media & 9.4 & 44.1\(\)0.9 & 42.6\(\)0.4 & 53.0\(\)2.0 & 55.4\(\)0.5 & 62.0\(\)2.3 & **65.2\(\)1.5** \\ Safety & 6.5 & 25.0\(\)0.6 & 22.1\(\)0.4 & 23.4\(\)0.9 & 23.1\(\)2.0 & 23.8\(\)1.5 & **26.9\(\)1.2** \\ Diaper & 8.4 & 58.3\(\)0.9 & 45.1\(\)0.3 & 78.9\(\)0.5 & 82.8\(\)0.7 & **88.3\(\)0.7** & 88.0\(\)0.8 \\ Health & 7.6 & 44.9\(\)0.2 & 45.2\(\)0.1 & 69.2\(\)1.2 & 70.5\(\)0.9 & 81.2\(\)0.5 & **81.6\(\)0.6** \\ Carseats & 6.6 & 23.1\(\)1.0 & 21.2\(\)0.8 & 22.0\(\)1.0 & 22.3\(\)1.9 & 23.0\(\)2.4 & **24.8\(\)2.2** \\ Bedding & 7.9 & 48.5\(\)0.6 & 48.1\(\)0.2 & 76.2\(\)2.2 & 76.2\(\)0.5 & 85.7\(\)1.1 & **87.1\(\)0.7** \\ Feeding & 9.3 & 56.3\(\)0.8 & 42.8\(\)0.2 & 75.3\(\)0.6 & 81.9\(\)0.9 & 88.5\(\)0.5 & **90.3\(\)1.1** \\ Apparel & 9.0 & 53.3\(\)0.5 & 50.8\(\)0.4 & 68.0\(\)2.0 & 76.4\(\)0.5 & 83.7\(\)0.3 & **85.4\(\)0.6** \\ Furniture & 6.5 & 17.5\(\)0.7 & 16.8\(\)0.2 & 17.6\(\)0.8 & 16.2\(\)2.0 & 16.7\(\)3.5 & **18.1\(\)1.5** \\   

Table 2: Product recommendation results for 12 different product categories. The best results are indicated in bold black, while the second-best results are highlighted in blue. Due to space limitation, we use Set-T to denote Set Transformer.

   Method & Two Moons & Gaussian Mixture \\  Random & 5.5 & 5.5 \\ PGM & 36.0 \(\) 2.0 & 43.8 \(\) 0.9 \\ DeepSet & 47.2 \(\) 0.3 & 44.6 \(\) 0.2 \\ Set Transformer & 57.4 \(\) 0.2 & 90.5 \(\) 0.2 \\ EquiVSet & 58.5 \(\) 0.3 & 90.7 \(\) 0.2 \\ INSET & 58.2 \(\) 0.3 & 90.9 \(\) 0.2 \\  HORSE & **60.2 \(\) 0.5** & **91.8 \(\) 0.2** \\   

Table 3: Performance results on the Two-Moons and Gaussian-Mixture datasets. Bolded numbers denote the best performance on each dataset

### Synthetic Experiments

Firstly, We validate the effectiveness of our models through experimental trials focused on learning set functions, using two synthetic datasets: the two-moons dataset (Pedregosa et al., 2011) with an added noise variance of \(^{2}=0.1\), and and a Gaussian mixture represented as \((_{0},)+(_{1},)\).

Take the Gaussian mixture as an example, the data generation procedure as follows: i) Initially, we select an index, denoted as \(b\), using a Bernoulli distribution with a probability of \(\). ii) Subsequently, we sample 10 points from the Gaussian distribution \((_{b},)\) to construct the set \(^{*}\). iii) Further, we sample 90 points for \(V S^{*}\) from the Gaussian distribution \((_{1-b},)\). We follow the procedure of Ou et al. (2022) to obtain 1,000 samples, subsequently divided into training, validation, and test sets. We effectively demonstrate the efficacy of our approach in mastering complex set functions. Detailed results can be found in Table 3.

### Product Recommendation

The task involves recommending the most suitable subset of 30 products to a customer within a specific category. For this experiment, we utilize the dataset from the Amazon baby registry, sourced from Gillenwater et al. (2014). This dataset includes numerous product subsets chosen by various customers, with Amazon categorizing each item on a baby registry into specific categories such as "Bath", "Health" and "Feeding". Detailed information can be found in Appendix D.

Table 2 presents the performance of all models across different categories. Notably, out of the twelve cases evaluated, HORSE outperforms other models in 11 of them. Even in the Diaper category, our method achieves results that are comparable to INSET, which is noteworthy. These significant improvements highlight the effectiveness and superiority of HORSE. Specifically, while EquiVSet and INSET struggle to surpass classical neural subset selection baselines in the Safety, Car Seats, and Furniture categories, HORSE consistently outperforms all baselines in a notable manner.

### Compound Selection in AI-aided Drug Discovery

In drug discovery, the screening of compounds with diverse biological activities and favorable ADME (absorption, distribution, metabolism, and excretion) properties is a critical step (Li et al., 2021; Ji et al., 2022; Gimeno et al., 2019). Virtual screening typically involves a sequential filtering process that employs multiple essential filters. However, neural networks encounter challenges when learning the complete screening process. This difficulty arises from the absence of intermediate supervision signals, which can be costly or impossible to obtain due to pharmaceutical protection policies. Therefore, we implement a single filter, namely, high bioactivity, to obtain the optimal subset of compound selection, following the methodology in (Ou et al., 2022). Our experiments are conducted on two datasets: PDBBind (Liu et al., 2015) and BindingDB (Liu et al., 2007). To be more practical, we further enhance our approach with the inclusion of _two filters_: the high bioactivity filter and the diversity filter. This extended analysis is denoted as PDBBind-2 and BindingDB-2, representing the two-stage filtering process, for a more practical perspective.

Table 4 demonstrates that our method outperforms the baselines and significantly surpasses random guessing, especially on the BindingDB-2 and PDBBind-2 datasets. However, the improvement on PDBBind and BindingDB is less significant. This marginal enhancement is due to the informative structural characteristics of complexes (the elements within a set), which inherently provide substantial information for this task. Consequently, the model can effectively predict the activity values of

    & Random & PGM & DeepSet & Set-T & EquiVSet & INSET & HORSE \\  PDBBind & 9.9 & 91.0\(\)1.0 & 90.1\(\)1.1 & 91.9\(\)1.5 & 92.4\(\)1.1 & 93.5\(\)0.8 & **94.1 \(\) 0.7** \\ BindingDB & 9.0 & 69.0\(\)2.0 & 71.0\(\)2.0 & 71.5\(\)1.0 & 72.1\(\)0.9 & 73.4\(\)1.0 & **74.2 \(\) 1.1** \\ PDBBind-2 & 7.3 & 35.0\(\)0.9 & 32.3\(\)0.4 & 35.5\(\)1.0 & 35.7\(\)0.5 & 37.1\(\)1.0 & **43.2 \(\) 0.6** \\ BindingDB-2 & 2.7 & 17.6\(\)0.6 & 16.5\(\)0.5 & 18.3\(\)0.4 & 18.8\(\)0.6 & 19.8\(\)0.5 & **21.3 \(\) 0.5** \\  Average & 7.23 & 53.15 & 52.48 & 54.30 & 54.75 & 55.95 & **58.20** \\   

Table 4: Empirical results of compound selection Tasks. Bolded numbers denote the best performance on each dataset. Due to space limitations, we use “Set-T” to denote Set Transformer.

complexes even without explicitly considering the interactions between the optimal subset and its complement in single filter scenarios. Nonetheless, our method still achieves superior results compared to other methods, confirming its effectiveness.

### Large-Scale Setting

Set encoding mechanisms, as introduced by Zaheer et al. (2017) and further explored by Lee et al. (2019), have fundamentally shifted the way neural networks perceive and process sets by emphasizing the importance of permutation invariance and the ability to handle variable-sized inputs. These models are designed to learn from the entire set in a single gradient step, ensuring that the learned representations encapsulate the holistic properties of the set. However, this approach encounters practical limitations when dealing with large-scale sets, where processing the entire set in a single step becomes computationally infeasible due to memory constraints or the sheer volume of data.

To circumvent these challenges, an effective strategy involves training models on partitions of the set, sampled dynamically at each iteration of the optimization process (Lee et al., 2019; Wang et al., 2024). This method allows for manageable subsets to be used for training, significantly reducing the computational load. However, this method will lose information (Bruno et al., 2021; Willette et al., 2023) since it does not process all the elements from \(V.\) Therefore, we instead partition the set elements into mini-batches, independently encode each batch, and aggregate them to obtain a single set encoding. By applying this methodology across both baseline models and HORSE in scenarios characterized by large-scale input sets, we can highlight the efficiency and scalability of our proposed solution. We conducted experiments on the Two-Moons and Gaussian-Mixture datasets. To ensure consistency, we set the size of the optimal subset \(S^{*}\) to be 10. Subsequently, we varied the size of the input ground set \(V\) within the range of \(\{200,400,600,800,1000\}\). Notably, the memory capacity of the GeForce RTX 3090 is insufficient when the size reaches 600. The ground set was divided into 5 disjoint partitions, with each partition containing one-fifth of the elements in \(V\). For the purpose of comparison, we selected INSER and Set Transformer as baselines alongside our proposed method, HORSE. INSET demonstrated the best performance among baselines, while Set Transformer is an alternative method that incorporates an attention mechanism. The results obtained from these experiments are presented in the left two subfigures of Figure 3. It is evident that HORSE outperforms both INSER and Set Transformer by a significant margin, demonstrating its superior effectiveness in handling large-scale sets.

Furthermore, to further enhance the practicality of our approach and investigate the potential impact of the partition numbers on the results, we conducted additional experiments on the BindingDB dataset. In this experiment, we set the size of the optimal subset \(S^{*}\) to be 15, while the size of the ground set remained fixed at 1000. We partitioned the ground set into a range of 2 to 50 partitions. The results of these experiments are presented in the right subfigure of Figure 3. Remarkably, it becomes evident that HORSE exhibits remarkable robustness with respect to the number of partitions considered. Regardless of the specific partitioning scheme employed, HORSE consistently delivers exceptional performance, which is more robust than our baselines.

Figure 3: The performance of the methods on the Two-Moons and Gaussian Datasets with respect to the set size is examined in the left two subfigures. These subfigures provide insights into how the performance of the methods varies as the size of the input sets changes. The right subfigure focuses on the influence of the number of partitions on the performance using the BindingDB dataset.

Conclusion

In this paper, we have addressed the limitations observed in existing methods for neural subset selection tasks. These methods often struggle to effectively model complex information and lack scalability when dealing with large-scale inputs. To overcome these challenges, we propose an innovative and scalable approach called HORSE, which leverages the power of the attention mechanism. Theoretically, we establish that HORSE satisfies the Identity Property and Permutation Invariance, ensuring its soundness and effectiveness. Empirically, we thoroughly evaluate the performance of HORSE against various baselines in both standard and large-scale settings.

Limitation and Future Work.Our theoretical and empirical results demonstrate how the attention mechanism can enhance models for neural subset selection tasks in both standard and large-scale settings. However, in large-scale scenarios, our support is currently limited to a theoretical framework for partitioning the set into different groups within a synthetic distributed setting, rather than practical experimentation in a real distributed environment. Moving forward, we plan to implement and test our model in more practical, real-world scenarios to further validate its effectiveness.

## 7 Acknowledgements

We thank all the reviewers for their valuable comments. This work was supported by Research Grants 8601116, 8601594, and 8601625 from the UGC of Hong Kong.