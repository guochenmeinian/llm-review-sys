# Analyzing Generalization of Neural Networks

through Loss Path Kernels

 Yilan Chen

UCSD CSE

yilan@ucsd.edu

&Wei Huang

RIEKN AIP

wei.huang.vr@riken.jp

&Hao Wang

MIT-IBM Watson AI Lab

hao@ibm.com

Charlotte Loh

MIT EECS

cloh@mit.edu

&Akash Srivastava

MIT-IBM Watson AI Lab

akash.srivastava@ibm.com

&Lam M. Nguyen

IBM Research

LamNguyen.MLTD@ibm.com

&Tsui-Wei Weng

UCSD HDSI

lweng@ucsd.edu

Correspondence to: Yilan Chen and Tsui-Wei Weng.

###### Abstract

Deep neural networks have been increasingly used in real-world applications, making it critical to ensure their ability to adapt to new, unseen data. In this paper, we study the generalization capability of neural networks trained with (stochastic) gradient flow. We establish a new connection between the loss dynamics of gradient flow and general kernel machines by proposing a new kernel, called loss path kernel. This kernel measures the similarity between two data points by evaluating the agreement between loss gradients along the path determined by the gradient flow. Based on this connection, we derive a new generalization upper bound that applies to general neural network architectures. This new bound is tight and strongly correlated with the true generalization error. We apply our results to guide the design of neural architecture search (NAS) and demonstrate favorable performance compared with state-of-the-art NAS algorithms through numerical experiments.

## 1 Introduction

Deep learning models have been increasingly used in applications with significant societal impact. Therefore, it is crucial to ensure that these models perform well not only on the training data but also on the new and unseen data. Classical learning theory attributes the generalization ability of machine learning (ML) models to the small complexity of the hypothesis class . However, modern ML models, such as deep neural networks (NNs), can have billions of parameters yet still exhibit strong generalization abilities . This is because various elements of the learning algorithms, including optimization methods, training data, and neural architectures, can all influence the inductive bias, which in turn shapes the generalization abilities of neural networks . While the overall hypothesis class may be large, the "effective domain" of this class, which ultimately determines the model's generalization abilities, is often much smaller . Hence, it is vital to develop algorithmic generalization bounds to capture this effective domain of the hypothesis class.

There has been significant work investigating the generalization of neural networks in their infinite-width regime through kernel methods . They showed that an infinite-width NNtrained by gradient flow and squared loss is equivalent to a kernel regression with neural tangent kernel (NTK) [28; 3]. Moreover, Arora et al. , Cao & Gu  further characterized the generalization behaviors of such ultra-wide NNs by deriving data-dependent and NTK-based generalization bounds. However, they only considered ultra-wide fully connected NNs with a square (or logistic) loss function. In practice, NNs are usually not ultra-wide and have more complex architectures such as CNNs. Hence, it is crucial to establish generalization bounds that hold in a more general setting.

In this paper, we analyze the generalization capability of NNs trained using (stochastic) gradient flow across a wide range of NN architectures. Our key technical contribution is to establish a new connection between the loss dynamics of (stochastic) gradient flow and general kernel machines with a special kernel that we named the _loss path kernel_. This new kernel calculates the similarity between two data points by integrating the inner product of the loss gradient evaluated at these points, along the path determined by the gradient flow. Based on this connection, we develop a novel generalization bound by analyzing the complexity of kernel machines induced by various training sets. Our generalization bound is tight and can be applied to a broad class of NN architectures, not restricted to ultra-wide NNs (see Table 1 for a comparison with existing results derived from NTK theory). Numerical experiments demonstrate that our bound maintains a strong correlation with the true generalization error of NNs trained with gradient descent (GD) (see Figure 1 and 2 in Sec. 6 & 7). Given this observation, we use our generalization bound to guide the design of neural architecture search (NAS) and demonstrate through numerical experiments that our approach can achieve a favorable performance compared with state-of-the-art training-free and minimum-training NAS algorithms [37; 13; 39]. In summary, our contributions are:

* In Sec. 4.1 and 5, we show for the first time that the loss of NNs trained by (stochastic) gradient flow is equivalent to a _general_ kernel machine. This result enables us to investigate the generalization capability of NNs from the perspective of kernel theory.
* In Sec. 4.2 and 5, we derive tight generalization bounds for NNs based on the aforementioned equivalence. Our result is very general as it holds for any continuously differentiable NN architectures including finite-width and infinite-width NNs. Experiments demonstrate that our bounds are tight (4690\(\) tighter than existing norm-based bounds and 55\(\) tighter than existing NTK-based bounds as shown in Appendix A.2) and highly correlated with the true generalization error.
* In Sec. 6 and 7, we apply our theory to study special cases including infinite-width NNs, stable algorithms, and norm-constrained NNs. We apply our bound to guide the design of NAS. Numerical experiments demonstrate that our approach achieves a favorable performance compared with state-of-the-art NAS algorithms on NAS-Bench-201 benchmark .

## 2 Related Work

**Generalization theory in deep learning.** Generalization is a crucial aspect of deep learning theory and various techniques have been proposed to study it. For example, Bartlett et al.  derived tight bounds for the VC dimension of NNs with ReLU activation functions. There is also a line of work that measures the capacity of NNs based on different norms, margins [5; 41; 6; 44], and sharpness-based measures [42; 43; 1] to explain the generalization behaviors of NNs. Additionally, there are theories studying the generalization of NNs from PAC-Bayes [35; 22] and information-theoretical approach

    & Arora et al.  & Cao \& Gu  & **Ours** \\  Bound & \(^{}(^{})^{-1}}{n}}\) & \((L^{}()^{-1}}{n}})\) & Theorem 3, Theorem 5 \\ Model & Ultra-wide two-layer FCNN & Ultra-wide FCNN & **General continuously differentiable NN** \\ Data & i.i.d. data with \(\|\|=1\) & i.i.d. data with \(\|\|=1\) & i.i.d. data with \(\|\|=1\) & i.i.d. data \\ Loss & Square loss & Logistic loss & **Continuously differentiable \& bounded loss** \\ During training & No & No & **Yes** \\ Multi-outputs & No & No & **Yes** \\ Training algorithm & GD & SGD & (Stochastic) gradient flow \\   

Table 1: Comparison with existing NTK-based generalization bounds. \(=(,)\) is the NTK on training samples and \(^{}\) is the NTK of the first layer. \(L\) represents the number of NN layers. We highlight some unique properties of our results in blue color. “During training” means our bound can be calculated at any time during training while existing NTK-based bounds only hold for the NNs at convergence. “Multi-outputs” means our bound holds for NNs with multiple outputs.

[48; 58]. For example, Dziugaite & Roy  numerically evaluated and optimized the PAC-Bayes bound of stochastic NN and obtained a non-vacuous generalization bound. In contrast, we study the generalization of NNs by building a new connection between NNs and kernel machines. We refer readers to Valle-Perez & Louis  for a more comprehensive review of the generalization theory of NNs.

**Neural tangent kernel (NTK).** NTK was first introduced in Jacot et al. , where the authors demonstrated that a fully-trained, infinite-width NN follows kernel gradient descent in the function space with respect to the NTK. Under gradient flow and squared loss, the fully-trained infinite-width NN is equivalent to kernel regression with the NTK [28; 3]. Chen et al.  further established the equivalence between infinite-width NNs and regularized kernel machines. Arora et al.  studied the generalization capacity of ultra-wide, two-layer NNs trained by GD and square loss, while Cao & Gu  examined the generalization of deep, ultra-wide NNs trained by stochastic gradient descent (SGD) and logistic loss. Both studies derived generalization bounds of converged NNs based on NTK. Besides, Huang et al.  studied the convergence and generalization of PAC-Bayesian learning for deep, ultra-wide NNs. Later, Domingos  showed that every model trained by gradient flow is a "kernel machine" with the weights and bias as functions of input data, which however can be much more complex than a typical kernel machine and our general kernel machine in Definition 2. Chen et al.  showed that every NN trained by gradient flow is a general kernel machine but their kernel is valid only in very limited cases - when the loss gradient of output is a constant. Otherwise, the kernel is not symmetric and not valid. In this paper, we consider an equivalence between the loss of NNs and general kernel machines, which resolves the previous asymmetric problem of the kernel function and also makes the generalization analysis of multi-outputs easier.

**Neural Architecture Search (NAS).** NAS aims to automate the discovery of top-performance neural networks to reduce human efforts. However, most existing NAS algorithms require heavy training of a supernet or intensive architecture evaluations, suffering from heavy resource consumption [47; 34; 18; 33]. Thus, it is crucial to develop training-free or minimum-training NAS algorithms to reduce the computational cost and select the best architecture at the same time [37; 13; 39]. Since our generalization bound has a strong correlation with the true generalization error, we apply it to design a new minimum-training NAS algorithm. We demonstrate in Table 2 that with a simple random search algorithm, our approach can achieve a favorable performance compared with state-of-the-art training-free and minimum-training NAS algorithms.

## 3 Kernel Machine and Loss Path Kernel

In this section, we define notation, provide a brief overview of kernel methods, and introduce the main concept of interest--the loss path kernel.

### Preliminaries

Consider a supervised learning problem where the task is to predict an output variable in \(^{k}\) using a vector of input variables in \(^{d}\). Let \(\). We denote the training set by \(\{_{i}\}_{i=1}^{n}\) with \(_{i}(_{i},_{i})\). We assume each point is drawn i.i.d. from an underlying distribution \(\). Let \(=[_{1},,_{n}]^{T}^{n d}\), \(=[_{1},,_{n}]^{T}^{n k}\), and \(=[,]^{n(d+k)}\).

We express a neural network in a general form \(f(,):^{p}^{d}^{k}\) where \(^{p}\) represents its parameters and \(^{d}\) is an input variable. The goal of a learning algorithm is to find a set of parameters that minimizes a population risk \(L_{}()=_{}[(,)]\) where \((,)(f(,),)\) is a loss function. Throughout this paper, we assume that \((,)\) and is continuously differentiable. In practice, the underlying distribution \(\) is unknown so the learning algorithm minimizes an empirical risk on the training set \(\) instead: \(L_{}()=_{i=1}^{n}(,_{i})\). The _generalization gap_ is defined as \(L_{}()-L_{}()\). The loss gradient with respect to the parameters \(\) is \(_{}(,)=_{}f(,)^{} _{f}(f(,),)^{p 1}\). Our analysis only requires that \((,)\) is continuously differentiable w.r.t. \(\) and \(\) so \(f\) can be either a fully connected neural network or a convolutional network or a residual network.

### Kernel Method

Kernel methods [16; 51; 53] search for linear relations in high-dimensional feature space by using kernel functions. Rather than computing the coordinates in the feature space explicitly, they only need to calculate the inner product between data pairs, making it computationally easier.

A kernel is a function \(K:\), such that for all \(,^{}\), \(K(,^{})=(),(^{})\), where \(:\) is a mapping from \(\) to an (inner product) feature space \(\).

**Proposition 1** (Shawe-Taylor et al. ).: _A function \(K:\), which is either continuous or has a finite domain, is a kernel function if and only if it is a symmetric function and, for any finite subset of \(\), \(_{1},,_{n}\), the matrix \(K(,)\) is positive semi-definite, where \(K(,)\) is a \(n n\) matrix whose \((i,j)\)-th entry is \(K(_{i},_{j})\)._

**Definition 1** (Kernel machine).: Let \(\) be the reproducing kernel Hilbert space (RKHS) corresponding to a kernel \(K(,^{})=(),(^{})\). A kernel machine \(g:\) is a linear function in \(\) such that its weight vector \(\) can be expressed as a linear combination of the training points, i.e. \(g()=,()+b=_{i=1}^{n}a_{i}K(_ {i},)+b\), where \(=_{i=1}^{n}a_{i}(_{i})\) and \(b\) is a constant. The RKHS norm of \(g\) is \(\|g\|_{}=\|_{i=1}^{n}a_{i}(_{i}) \|=a_{i}a_{j}K(_{i},_{j})}\).

Next, we introduce general kernel machine, which generalizes the concept of kernel machine.

**Definition 2** (General kernel machine).: A general kernel machine \(g:\) with a kernel \(K(,^{})\) is \(g()=_{i=1}^{n}a_{i}K(_{i},)+h()\), where \(h:\) is a function of \(\). When \(h()\) is a constant, \(g()\) reduces to a kernel machine in Definition 1.

### Neural Tangent Kernel and Loss Path Kernel

Neural tangent kernel (NTK) has been introduced by Jacot et al.  to establish an equivalence between infinite-width NNs and kernel regression. After then, there is a growing line of work applying NTK theory to study properties of over-parameterized NNs, such as optimization convergence [21; 20] and generalization capability [2; 11]. The neural tangent kernel  associated with a NN \(f(,)\) at \(\) is defined as \((;,^{})=_{}f(,) _{}f(,^{})^{}^{k k}\). Under certain conditions, such as infinite width limit and NTK parameterization, the NTK converges to a deterministic limit kernel \((,^{})_{k}\) that remains constant during training: \((;,^{})(,^{}) _{k}\), where \((,^{}):^{d}^{d}\) is a scalar kernel and \(_{k}\) is a \(k k\) identity matrix. Next, we introduce the main concepts of interest in this paper: the _loss tangent kernel_ and the _loss path kernel_. They are central to characterizing the generalization behaviors of NNs trained by (stochastic) gradient flow.

**Definition 3** (Loss Tangent Kernel (LTK) \(\)).: The loss tangent kernel associated with the loss function \((,)\) is defined as \(}(;,^{})=_{}( {w},),_{}(,^{})\).

The LTK \(}\) has a natural connection with the NTK \(\) by applying the chain rule:

\[}(;,^{})=_{f}(,)^ {}(;,^{})_{f}(, ^{}).\]

Next, we introduce the loss path kernel, which integrates the LTK along a given path of the parameters. Later, we will characterize this path via the gradient flow dynamics.

**Definition 4** (Loss Path Kernel (LPK) \(_{T}\)).: Suppose the weights follow a continuous path \((t):[0,T]^{p}\) in their domain with a starting point \((0)=_{0}\), where \(T\) is a predetermined constant. This path is determined by the training set \(\) and the training time \(T\). We define the loss path kernel associated with the loss function \((,)\) along the path as \(_{T}(,^{};)_{0}^{T}}((t);,^{})t\).

In Appendix B, we show LTK is Riemann integrable so the integral in the above definition is well-defined. Intuitively, the LTK \(}(;,^{})\) measures the similarity between data points \(\) and \(^{}\) by comparing their loss gradients when evaluated using a fixed neural network parameter \(\). The LPK \(_{T}(,^{};)\) measures the overall similarity during the entire training time.

### Rademacher Complexity

Rademacher complexity  measures the complexity of a hypothesis class. It takes into account the data distribution and is a central concept in statistical learning theory. Next, we recall its definition and a generalization upper bound via Rademacher complexity.

**Definition 5** (Empirical Rademacher complexity \(}_{}()\)).: Let \(=\{f:^{k}\}\) be a hypothesis class. We denote \(\) as the set of loss functions associated with each function in \(\), defined as \(=\{g:(,)(f(),),f\}\). The empirical Rademacher complexity of \(\) with respect to a sample set \(\) is defined as: \(}_{}()=}_{}[_{g}_{i=1}^{n}_{i}g(_{i})]\), where \(=(_{1},,_{n})\) and \(_{i}\) are independent uniform random variables taking values in \(\{+1,-1\}\).

**Theorem 1** (Theorem 3.3 in Mohri et al. ).: _Let \(\) be a family of functions mapping from \(\) to \(\). Then for any \((0,1)\), with probability at least \(1-\) over the draw of an i.i.d. sample set \(=\{_{1},,_{n}\}\), the following holds for all \(g\): \(}_{}[g()]-_{i= 1}^{n}g(_{i}) 2}_{}()+3}\)._

## 4 Gradient Flow

In this section, we establish a new connection between the loss dynamics of gradient flow and a general kernel machine equipped with the LPK. Using this result, we introduce a new generalization bound by analyzing the complexity of the collection of kernel machines induced by all possible training sets. Our analysis applies to a wide range of neural network architectures, as long as they are continuously differentiable. Our numerical experiments validate the tightness of our bound and its strong correlation with the true generalization error.

### Loss Dynamics of Gradient Flow and Its Equivalence with General Kernel Machine

Consider the gradient flow dynamics (gradient descent with infinitesimal step size):

\[(t)}{t}=-_{}L_{S}((t))=-_{i=1}^{n}_{}((t),_{i}).\] (1)

The above ODE is well-defined for a wide variety of conditions, e.g. local Lipschitz-continuity of the gradient or semi-convexity of the loss function [49; 23]. Next, we establish its connection with the general kernel machine (KM) in the following theorem.

**Theorem 2** (Equivalence with general KM.).: _Suppose \((T)=_{T}\) is a solution of (1) at time \(T\) with initialization \((0)=_{0}\). Then for any \(\),_

\[(_{T},)=_{i=1}^{n}-_{T}(, _{i};)+(_{0},),\]

_where \(_{T}\) is defined in Definition 4._

The above theorem demonstrates that the loss of the NN at a certain fixed time is a general kernel machine. Herein, \(_{T}\) is the LPK and we prove in Appendix C.1 that it is a valid kernel. Unlike previous NTK works that establish the equivalence between _infinite-width_ NNs and kernel machines, our equivalence is much more general and holds for any NN that is continuously differentiable. Based on this equivalence, we characterize the generalization of NNs from the perspective of kernels. Note that \(_{T}\) is a function of \(\) and this property enables us to establish a data-dependent generalization bound shortly.

### Generalization Bounds

We introduce the main result in this section: a generalization bound for NNs whose weights follow gradient flow in (1) at time \(T\). We derive this bound by analyzing the Rademacher complexity of the function class of kernel machines induced by different training sets with constrained RKHS norms. Recall that each training set yields a distinct LPK. We define the collection of all such LPKs by

\[_{T}\{_{T}(,;^{}): ^{}(^{ n}),}_{i, j}_{T}(_{i}^{},_{j}^{};^{})  B^{2}\},\] (2)

where \(B>0\) is some constant, \(^{}=\{_{1}^{},,_{n}^{}\}\), \(^{ n}\) is the joint distribution of \(n\) i.i.d. samples drawn from \(\), and \((^{ n})\) is the support set of \(^{ n}\). Recall that \(=\{_{1},,_{n}\}\) is the trainingset. Note the set in (2) includes the case of \(^{}=\) if \(}_{i,j}_{T}(_{i},_{j};) B^ {2}\). Then we introduce a class of general kernel machines, corresponding to all different kernels in \(_{T}\).

\[_{T}\{g()=_{i=1}^{n}- (,_{i}^{};^{})+(_{0},): (,;^{})_{T}\}.\]

Note that \(g()_{T}\) corresponds to \((_{T},)\) trained from one possible dataset \(^{}(^{ n})\). Next, we compute the Rademacher complexity of \(_{T}\) and use it to obtain a generalization bound.

**Theorem 3**.: \(}_{}(_{T})\{U_{1},U_{2}\}\)_. Here_

\[U_{1} =(,;^{ })_{T}}((,; ^{}))+_{i j}(_{i},_{j})},\] \[U_{2} =_{>0}(+(_{T}^{},,\|\|_{1})}{n}} ),\]

_where \(_{T}^{}=\{g()=(g(_{1}),,g(_{ n})):g_{T}\}\), \((_{T}^{},,\|\|_{1})\) is the covering number of \(_{T}^{}\) with the \(_{1}\)-norm and_

\[(_{i},_{j})=[_{(,; ^{})_{T}}(_{i},_{j}; ^{})-_{(,;^{}) _{T}}(_{i},_{j};^{})].\]

The term \(U_{1}\) is composed by two components \(_{(,;^{})_{T}}((,;^{}))\) and \((_{i},_{j})\). The first component, according to the definition of LPK, quantifies the maximum magnitude of the loss gradient in \(_{T}\) evaluated with the set \(\) throughout the training trajectory. The second component assesses the range of variation of LPK within the set \(_{T}\). The term \(U_{2}\) is obtained from analyzing the covering number of \(_{T}\). It shows that if the variation of the loss dynamics of gradient flow with different training data is small, then the complexity of \(_{T}\) will also be small. The norm constraint \(}_{i,j}_{T}(_{i}^{},_{j}^{ };^{}) B^{2}\) balances a tradeoff between the tightness of the bound and the expressiveness of the set \(_{T}\) (the number of datasets covered). Combining these two bounds with Theorem 1, we obtain the following generalization bound.

**Corollary 1** (Generalization bound for NN).: _Fix \(B>0\). Let \(}_{}^{gf}(_{T})=(U_{1},U_{2})\) where \(U_{1}\) and \(U_{2}\) are defined in Theorem 3. For any \((0,1)\), with probability at least \(1-\) over the draw of an i.i.d. sample set \(=\{_{i}\}_{i=1}^{n}\), if \(}_{i,j}_{T}(_{i},_{j};) B ^{2}\), the following holds for \((_{T},)\) that trained from \(\),_

\[L_{}(A_{T}())-L_{S}(A_{T}()) 2}_{ }^{gf}(_{T})+3},\]

_where \(_{T}=A_{T}()\) is the output from the gradient flow (1) at time \(T\) by using \(\) as input._

Our result owns many compelling properties.

* First, our bound holds in a general setting as it does not hedge on a special NN architecture. In contrast, existing works  only consider fully connected NNs and require NN to be ultra-wide.
* Our bound depends on the data distribution through the quantities in \(U_{1}\) and \(U_{2}\). This property not only significantly tightens our bound but can also help explain some empirical observations of NNs. For example, different from classical generalization theory, e.g. VC dimension, our complexity bounds depend on the labels directly, which helps explain the random label phenomenon  as shown in Figure 3 in Sec. 7.
* Our experiments in Sec. 7 (Figure 2) demonstrate the tightness of the generalization bound. Intuitively, our bound is tight because (1) instead of considering the entire hypothesis class, we focus on the subset of interest characterized by running gradient flow from a starting point \(_{0}\); (2) we get the bound from an equivalence between NNs and general kernel machines, whose generalization bounds are tighter. Finally, we compare our generalization bound with two existing NTK-based bounds in Table 1.

Stochastic Gradient Flow

In the previous section, we derived a generalization bound for NNs trained from full-batch gradient flow. Here we extend our analysis to stochastic gradient flow and derive a corresponding generalization bound. To start with, we recall the dynamics of stochastic gradient flow (SGD with infinitesimal step size). Let \(_{t}\{1,,n\}\) be the indices of batch data used in time interval \([t,t+1]\) and \(|_{t}|=m\) be the batch size. We establish a new connection between the loss dynamics of stochastic gradient flow and a general kernel machine. Then we investigate the complexity of the collection of such kernel machines that can be induced by various training sets.

**Theorem 4**.: _Suppose \((T)=_{T}\) is a solution of stochastic gradient flow at time \(T\) with initialization \((0)=_{0}\). Then for any \(\),_

\[(_{T},)=_{t=0}^{T-1}_{i_{t}}- _{t,t+1}(,_{i};)+(_{0},),\]

_where \(_{t,t+1}(,_{i};)=_{t}^{t+1}}((t);,_{i})dt\) with \(}\) defined in Definition 3._

The above theorem shows that the loss of the NN in stochastic gradient flow dynamics can be characterized by a sum of general kernel machines. In particular, when we use the full batch at each time interval (i.e., \(m=n\)), the above result recovers Theorem 2. To study its generalization behavior, we introduce the class of kernel machines induced by different training sets \(^{}(^{ n})\) with constrained RKHS norms. Specifically, given \(B_{t}>0\) for \(t=0,,T-1\), we define

\[_{T}=\{(_{0,1}(,;^{}),, _{T-1,T}(,;^{})):^{} (^{ n}),}_{i,j_{t}} _{t,t+1}(^{}_{i},^{}_{j};^{ }) B_{t}^{2}\}.\]

Note this set includes the kernel induced by the training set \(\) if it satisfies the constraints. Then \((_{T},)\) trained from all feasible \(^{}(^{ n})\) form a function class

\[_{T}_{t=0}^{T-1}_{i_{t}}- _{t,t+1}(,^{}_{i};^{} )+(_{0},):(,;^{}) _{T}}.\] (3)

Next, we upper bound the Rademacher complexity of the function class \(_{T}\). This bound can naturally translate into a generalization bound by equipping with Theorem 1.

**Theorem 5**.: _The Rademacher complexity of \(_{T}\) defined in (3) has an upper bound:_

\[}_{}(_{T})_{t=0}^{T-1}}{n}(,;^{}) _{T}}(_{t,t+1}(,);^{})+_{i  j}_{t}(_{i},_{j})}.\]

_where \(_{t}(_{i},_{j})=[_{(, ;^{})_{T}}_{t,t+1}(_{i}, _{j};^{})-\ _{(,;^{}) _{T}}_{t,t+1}(_{i},_{j};^{} )]\)._

We assumed that mini-batches indices \(_{t}\) are chosen before training. However, our analysis can be extended to accommodate random mini-batch selections of any sampling strategy by enumerating all potential \(_{t}\) in \(_{T}\).

## 6 Case Study & Use Case

In the previous sections, we derived generalization bounds for NNs trained with (stochastic) gradient flow. These bounds may initially appear complex due to their dependence on the training process. Here we show that these bounds can be significantly simplified by applying them to infinite-width NNs (and stable algorithms in Appendix E.2, norm-constraint NNs in Appendix E.3). Moreover, we demonstrate that our generalization bounds maintain a high correlation with the true generalization error. As a result, we use them to guide the design of NAS, and our experimental results demonstrate that this approach has a favorable performance compared with state-of-the-art algorithms.

### Infinite-width NN

In this subsection, we consider a special case of infinite-width NNs trained by gradient flow and derive pre-computed generalization bounds. We focus on gradient flow to simplify the presentations but our results can be directly extended to stochastic gradient flow. For an infinite-width NN, under certain conditions, the neural tangent kernel keeps unchanged during training: \((_{t};,^{})(,^{ })_{k}\). Consider a \(\)-Lipschitz loss function, i.e. \(\|_{f}(,)\|\). The Rademacher complexity in Theorem 3 can be bounded by \(}_{}(_{T}) U_{}\), where

\[U_{}=}{n}|(_{i},_{ j})|}.\] (4)

In this infinite-width regime, our bound has no dependence on the initialization \(_{0}\) since the NTK converges to a deterministic limit and has no dependence on the parameters. That means the bound holds for all possible \(_{0}\) of the width NNs trained by gradient flow from initialization. Compared with the bound \((L^{}()^{-1}}{n}})\) in , \(U_{}\) has several advantages: (1) it has no dependence on the number of layers \(L\); (2) it holds for NNs with multiple outputs.

### Correlation Analysis and NAS

As a practical application, we apply our generalization bounds to guide the design of NAS. We first introduce a quantity \(U_{}\) simplified from the bound in Theorem 5, defined as

\[U_{}=_{t=0}^{T-1}}_{i,j _{t}}_{t,t+1}(_{i},_{j};)}(_{t,t+1}(,);)}.\]

\(U_{}\) can be computed along with training a NN via SGD on a training set \(\). Combining it with the training loss, we define the following quantity as an estimate of the population loss:

\[(,)=L_{}()+2U_{}.\] (5)

We analyze the correlation between \((,)\) and the true generalization error by randomly sampling 100 NN architectures from NAS-Bench-201 . For each, we compute both \((,)\) and the true generalization error. Since solving the gradient flow ODE is computationally infeasible for the large NNs in NAS-Bench-201, we apply a trapezoidal rule to approximate the integration in LPK \(K_{t,t+1}\). This approximation enables us to compute \(U_{}\) efficiently. Figure 1 demonstrates the correlation between \((,)\) and the test error. The left figures plot the test error at epoch 1 or 2 against \((,)\) of the respective epochs, showing a strong positive correlation between them. The right figures plot the test error at convergence against \((,)\) at epoch 1 or 2, which also demonstrate a positive correlation. The outlier is caused by some architecture with large loss gradients. This experiment shows that \((,)\) at the initial training stage can predict the performance of NNs at convergence. Based on this observation, we use \((,)\) as a metric in NAS for selecting architectures at the initial training stage (see Table 2). This approach significantly reduces computational costs compared with training-based NAS algorithms [47; 34; 18; 33].

Figure 1: Correlation between \((,)\) and the test error on CIFAR-100 at epoch 1 and epoch 2. Kendall’s tau shows they have a strong positive correlation.

## 7 Numerical Experiments

We conduct comprehensive numerical experiments to demonstrate our generalization bounds. We observe that our complexity bounds are tight with respect to the generalization gap and can capture how noisy label influences the generalization behaviors of NNs. Moreover, we apply \((,)\) in (5) to NAS and demonstrate favorable performance compared with state-of-the-art algorithms.

**(I) Generalization bound in Corollary 1.** In Figure 2 (more detailed in Figure A.4), we use a logistic loss to train a two-layer NN with 100 hidden nodes for binary classification on MNIST 1 and 7  by full-batch gradient flow and compute its generalization bound. Due to the computational cost of solving the gradient flow ODE and computing the kernel, we only train and calculate the bound on \(n=1000\) training samples. The bound would be tighter with more training samples. The NN is initialized using the NTK parameterization . We use the Softplus activation function, defined as \((x)=(1+e^{ x})\). This function is continuously differentiable and serves as a smooth approximation to the ReLU activation function. In our experiments, we set \(=10\). To train the NN via gradient flow, we solve the gradient flow ODE given by (1) to decide the NN parameters. For the equivalent general KM, we compute the LTK using the NN parameters and integrate it to get the LPK \(_{T}\). These ODEs are computed with torchdiffeq . To estimate the generalization bound, we train the NN on (20, 50, 100) independently sampled training sets \(^{}\) to estimate the \(_{T}\) and \(_{T}\), and the supremum in the bound \(U_{1}\) is estimated by taking the maximum over the finite set \(_{T}\). For \(U_{2}\), we compute an upper bound of it by setting \(\) as the largest \(_{1}\) distance between any two \(g()_{T}^{}\) and \((_{T}^{},,\|-1.0pt\| -1.0pt-1.0pt_{1})=1\) because in this case any \(g()_{T}^{}\) will satisfy as an \(\)-cover. We run each experiment five times and plot the mean and standard deviation. The numerical experiments demonstrate that our complexity bound is tight and can capture the generalization gap well. As the number of \(^{}\) increases, our bound converges to the true supremum value. To estimate the true supremum value, we apply the extreme value theory in Appendix A.3 and show the gap between the finite maximum and supremum is small, validating using a finite maximum as an estimate for our bound.

We train the NN using GD with a finite learning rate \(=10\) to compare with the NN trained by gradient flow. The training time \(t=\) training steps. In Figure 2 (a) and Figure A.4 (a)(b), we observe that the loss of the NN trained by GD and that trained by gradient flow are consistently close throughout the entire training process. Consequently, while we established upper bounds for NNs trained by gradient flow, these results can serve as an (approximate) upper bound for NNs trained by GD with a finite learning rate.

Notably, for the NN in this experiment, the VC dimension  is \(55957.3\), the norm-based bound in  is \(140.7\) at \(T=1000\), and the NTK-based bound for an ultra-wide NN in  is 1.44, which are all vacuous (larger than 1), while our bound is tight (0.03 at \(T=1000\)). See a detailed comparison in Appendix A.2. We also conduct an experiment of three-layer NN (3072-100-10) trained on binary CIFAR-10 (cat and dog)  in Figure A.5, where there is a larger generalization gap.

Figure 2: **Experiment (I). (a) shows the dynamics of logistic loss for 5 randomly selected training samples for NN trained by gradient flow (NN GF), NN trained by GD (NN GD), and the equivalent general kernel machine (KM) in Theorem 2. The dynamics of NN GF and KM overlap and thus verify the equivalence in Theorem 2. The dynamics of NN GF and NN GD are consistently close throughout the training process. (b) shows NN GF’s training loss, test loss, test error, and upper bound for \(L_{}(_{T})\) in Corollary 1. (c) shows that the complexity bound \(}_{g}^{gf}(_{T})\) in Corollary 1 captures the generalization gap \(L_{}(_{T})-L_{}(_{T})\) well. It first increases and then converges after sufficient training time.**

**(II) Generalization bound with label noise.** The settings are similar to Experiment (I) except we corrupt the labels with different portions of noise and calculate the bound after training NN until \(T=20000\). We estimate the bound with 20 training sets \(^{}\). The results in Figure 3 show that our bound has a strong correlation with the generalization gap and increases with the portion of label noise. Unlike classical generalization theory, e.g. VC dimension, our generalization bound can help explain the random label phenomenon .

**(III) Neural architecture search (NAS).** We apply \((,)\) in Eq. (5) to guide the design of NAS. The results are shown in Table 2. We use a simple random search (RS) with \((,)\), where 100 architectures are sampled from the search space for evaluation, and the architecture with smallest \((,)\) is selected. \(U_{}\) is estimated with a batch of data of size 600. Build upon Sec. 6.2, we apply \((,)_{1}\) and \((,)_{2}\) (\((,)\) after training 1 and 2 epochs) to select NN architectures at the initial training stage in order to reduce computational costs. We compare our method with state-of-the-art training-free/minimum-training NAS algorithms [13; 39]. We run the experiments four times with different random seeds and report the mean and standard deviation. We reproduce the results in Chen et al.  using their released code and directly adopt the results reported in Mok et al.  as they did not release the code. The results show our approach of RS + \((,)\) can achieve favorable performance compared with state-of-the-art training-free/minimum-training NAS algorithms.

## 8 Conclusion and Future Work

In this paper, we establish a new connection between the loss dynamics of (stochastic) gradient flow and a general kernel machine. Building upon this result, we introduce generalization bounds for NNs trained from (stochastic) gradient flow. Our bounds hold for any continuously differentiable NN architectures (both finite-width and ultra-wide) and are generally tighter than existing bounds. Moreover, for infinite-width NNs, we obtain a pre-computed generalization bound for the whole training process. Finally, we apply our results to NAS and demonstrate favorable performance compared with state-of-the-art NAS algorithms.

There are several directions for future research. First, evaluating our generation bounds relies on the loss gradient, which may contain private and sensitive information. One potential fix would be accessing such information in a differentially private manner and it would be interesting to investigate how this "noisy" observation of gradient information influences our generalization bounds. Second, it is worth exploring how other optimization algorithms and different model architectures influence the generalization bounds. Finally, our bounds provide worst-case guarantees to the generalization of NNs and it would be interesting to extend our results to obtain expected bounds for further sharpening the results.

    &  &  \\  & Accuracy & Best & Accuracy & Best \\ 
**Baselines** & & & & \\ TENAS  & 93.08\(\)0.15 & 93.25 & 70.37\(\)2.40 & **73.16** \\ RS + LGA\({}_{3}\) & 93.64 & & 69.77 & \\ 
**Ours** & & & & \\ RS + \((,)_{1}\) & 93.68\(\)0.12 & 93.84 & 72.02\(\)1.43 & 73.15 \\ RS + \((,)_{2}\) & **93.79\(\)**0.18 & **94.02** & **72.76\(\)**0.33 & 73.15 \\  Optimal & 94.37 & & 73.51 & \\   

Table 2: **Experiment (III).** Comparison with state-of-the-art training-free/minimum-training NAS methods on NAS-Bench-201. Test accuracy with mean and deviation are reported. "Best" is the best accuracy over the four runs. “Optimal” indicates the best test accuracy achievable in NAS-Bench-201 search space. RS: randomly sample 100 architectures and select the one with the best metric value.

Figure 3: **Experiment (II).** Generalization bound with label noise at \(T=20000\).

Acknowledgement

We thank the anonymous reviewers for valuable suggestions to improve the paper. We also thank the San Diego Supercomputer Center and the MIT-IBM Watson AI Lab for computing resources. Y. Chen and T.-W. Weng are supported by National Science Foundation under Grant No. 2107189 and 2313105.