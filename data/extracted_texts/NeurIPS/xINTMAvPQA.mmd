# Efficient and Approximate Per-Example Gradient

Norms for Gradient Noise Scale

Gavia Gray

Cerebras Systems

Toronto, Canada

gngdb.labs@gmail.com &Anshul Samar

Cerebras Systems

Sunnyvale, CA

anshul@cerebras.net &Joel Hestness

Cerebras Systems

Sunnyvale, CA

joel@cerebras.net

###### Abstract

Gradient Noise Scale (GNS) is valuable to compute because it provides a suggestion for a compute efficient batch size during training: small enough to be compute efficient and large enough to take advantage of parallelism. While it can be a valuable tool, computing GNS is often cumbersome or expensive due to the difficulty of obtaining gradient norms over a small batch of examples (smaller than the training batch used). An existing trick for collecting "efficient" per-example gradient norms is inefficient in transformer or convolutional models. By assuming activations are normally distributed, we compute an approximate per-example gradient norm that tracks the true per-example gradient norm in practical settings. Using this approximation, we construct a Scaled Output Gradient Noise Scale (SOGNS) that is generally applicable at negligible cost and provides additional feedback to the practitioner during training.

## 1 Introduction

Gradient Noise Scale (GNS) correlates with the _critical batch size_, which is the point below which one may expect to linearly accelerate training by adding examples to the batch (McCandlish et al., 2018). For this reason, the batch size prescribed by GNS has been demonstrated to be useful while training GPT-3 (Brown et al., 2020).

Computing the GNS requires gradient norms from small and large batches (described in Section 2). However, in settings where we desire high performance compute, batch sizes typically need to be large, making it difficult or costly to sample small batch gradients. Goodfellow (2015) introduces a trick to access per-example gradient norms efficiently, but this trick cannot be applied in settings with tensor rank larger than 2. In particular, transformer language models have rank-3 tensor with batch, sequence and hidden dimensions. To address this problem, we construct an approximation that assumes normally distributed activations at layer inputs, which allows us to access per-example norms efficiently (described in Section 3.1), and provide a reference implementation: https://github.com/CerebrasResearch/nanoGNS.

## 2 Background

McCandlish et al. (2018) suggest using the "simple" GNS, \(_{}\)1, as a metric to inform the practitioner while training a model,

\[_{}=G}\]where \(G\) are the gradients and \(\) is their associated covariance matrix. To compute \(_{}\) McCandlish et al. (2018) further define the unbiased estimators \(\) and \(||^{2}\) shown in Equations 1 and 2, where \(B_{}\) and \(B_{}\) are the batch sizes used to compute the gradients.

\[||^{2} :=}-B_{}}(B_{} |G_{B_{}}|^{2}-B_{}|G_{B_{}}|^{2})  G^{T}G\] (1) \[ :=}-1/B_{}}(|G_{B_{ }}|^{2}-|G_{B_{}}|^{2}) tr().\] (2)

We can easily compute \(|G_{B_{}}|\) using the accumulated gradients immediately after the backward pass. However, the challenge in computing \(|G_{B_{}}|\) is that it requires the gradients for a batch size that is smaller than the batch size used for the optimizer step. McCandlish et al. (2018) propose using the gradients communicated between Distributed Data Parallel (DDP) nodes but this means that the variance of the resulting GNS estimate is tied to that DDP configuration. A taxonomy of the options for computing \(|G_{B_{}}|\) is presented in Appendix A.

As \(|G_{B_{}}|\) may be estimated as the mean over samples within the minibatch, in accordance with the law of large numbers, the variance of the estimate decreases with the number of observations of the gradient norm. As shown in Figure 1, this implies the small batch size should be as small as possible to obtain an estimate of \(|G_{B_{}}|\), and thus the GNS, with minimal variance. Further discussion of this result may be found in Appendix B and code in Appendix B.1.

## 3 Efficient Per-example Gradient Norms

Goodfellow (2015) proposes a trick to compute gradient norms for individual examples in a minibatch, which would provide the minimum variance estimate of the GNS as described in Section 2. He observes that the squared norm of the gradient is a sum of elements in an outer product that can be factored into two smaller sums on the input vectors, eliminating the need to calculate the full outer product. It may be stated as follows using Einstein and Lagrange notation2,

\[n_{b}^{2}=(w^{})^{2}_{bik}=x_{bi}x_{bi}y^{}_{bk}y^{}_{bk},\]

where \(x\) are the activations prior to a linear layer, \(y^{}\) are the gradients of the loss with respect to the outputs of the linear layer and \(w^{}\) are the gradients of the loss with respect to the weights of the linear layer.

For networks of only linear layers acting on 2D inputs, this trick is sufficient to provide accurate GNS estimates3. However, for networks with convolutional or 3D inputs to linear layers, such as

Figure 1: The variance of the GNS estimator for different \(B_{}\) (left) and \(B_{}\) (right) sizes. \(B_{}=l\) and \(B_{}=s\) in legends.

transformers, this trick is no longer efficient. For three dimensions, inputs \(^{B T I}\) and outputs \(^{B T K}\)(Li et al., 2022), the per-example gradient norm \(n_{b}\) is,

\[n_{b}^{2}=(w^{})_{bik}^{2}=(_{t}x_{bti}y_{btk}^{})^{2}=x_{bti}y_{ btk}^{}x_{bui}y_{buk}^{},\]

which has \(O(T^{2})\) complexity in the sequence length \(T\). In these cases, computing the \(w^{}\) explicitly, as the per-example gradient trick avoids, is more efficient. More details on this case are provided in Appendix C.1.

### Proposed Approximation

Assuming all entries of \(\) are IID Gaussian with a batch-dependent standard deviation \(_{b}\) and mean zero allows us to compute the following expectation in closed form:

\[E[_{i}x_{bi}x_{bi}]=_{i}E[x_{bi}x_{bi}]=_{i}^{2}(x_{bi})=I ^{2}(x_{bi}).\]

Applying this in the 3D case,

\[E[n_{b}^{2}]=E[y_{btk}^{}y_{btk}^{}x_{bti}x_{bui}]=y_{ btk}^{}y_{buk}^{}E[x_{bti}x_{bui}]=_{t,k}y_{btk}^{ }y_{buk}^{}_{i}_{b}^{2}=I_{b}^{2}_{t,k}y_{ btk}^{}y_{buk}^{}\]

and we know \(_{b}^{2}=_{t,i}x_{bti}x_{bti}\) in line with our assumptions above, assuming \(x_{bti}\) is zero-mean. Factorizing the quadratic in the \(t,u\) dimension produces

\[E[n_{b}^{2}]=I_{b}^{2}_{k}(_{t}y_{btk}^{})^{2}.\]

In practice, this says we can approximate \(n_{b}\) as follows to construct \(_{b}\), the approximate per-example gradient norm,

\[n_{b}^{2}_{b}^{2}=I_{b}^{2}_{k}(_{t}y_{btk}^{ })^{2}=(_{t,i}x_{bti}x_{bti})_{k} (_{t}y_{btk}^{})^{2},\]

and we can see that this is equal to the exact per-example gradient when \(T=1\):

\[_{b}^{2}=I_{b}^{2}_{k}(_{t}y_{btk}^{})^{2 }=I_{i}x_{bi}x_{bi}_{k}(y_{bk}^{})^{2}=x_ {bi}x_{bi}y_{bk}^{}y_{bk}^{}=n_{b}^{2}\]

Experiments in Section 4, along with simulations in Appendix D, confirm that this approximation is accurate. This approximation may also be extended to \(|G_{B_{}}|\) as described in Appendix E, but this observation is unnecessary for the results presented here, as we assume the exact \(|G_{B_{}}|\) is easy to access.

Substituting \(_{b}^{2}\) into Equations 1 and 2 yields \(_{}\), the Scaled Output Gradient Noise Scale (SOGNS). The analogous metric using the exact per-example norm is \(_{}\) the Per-Example Parameter Gradient Noise Scale (PEPGNS) (see Appendix C.2 for index of terms).

## 4 Experiments

### Approximate Per-Example Gradient Noise Scale

We investigate how well SOGNS correlates with the observed GNS by training a 1M parameter Convolutional Neural Network (CNN) on MNIST (code linked in Appendix F). Figure 1(a) shows the overall fit of SOGNS to PEPGNS at all points throughout training for only the convolutional layers (the remaining linear layers only process 2D tensors so the estimate is exact). Throughout training, the relationship between SOGNS and PEPGNS is extremely regular at different orders of magnitude.

We also demonstrate the overall performance of the approximation by comparing the relationship between observed GNS and training loss. In Figure 1(b), we replicate McCandlish et al. (2018) and plot \(_{}\) as the authors measured. We see that the correlation to the critical batch size is similar for both SOGNS and PEPGNS. Additional statistics are plotted in Appendix F.

### Large Scale Gradient Noise Scale

To verify that this method is useful in practice, a checkpoint from a 111M parameter language model (Dey et al., 2023) was tested. In Figure 3, SOGNS and PEPGNS are compared, showing that the approximation tracks the exact case but diverges for some layers in the network. McCandlish et al. (2018) observes that the GNS may diverge by an order of magnitude from the measured _critical batch size_ so the relationship we observe is within the margin of error.

## 5 Conclusion

Choosing a batch size is often achieved with reference to previous experiments or by hyperparameter search, which can be especially onerous in novel settings where a reasonable choice for batch size is not obvious. The GNS is a useful metric to navigate in such circumstances. In this paper, we observe that the per-example gradient norm trick (Goodfellow, 2015) could provide a useful shortcut for a minimal variance estimate of the GNS but it is inefficient in practical settings involving large transformer models (Li et al., 2022), requiring \(O(T^{2})\) operations in sequence length \(T\). To address this, we propose SOGNS, an approximation that operates in \(O(T)\), while correlating closely with the exact GNS. As practitioners now know that it is critical to log the gradient norms during training, we hope that this work can make GNS an accessible metric for large scale experiments.

Figure 3: Results of a 111M parameter language model experiment measuring GNS on a fixed checkpoint. On the left, the approximate small batch gradient norm is compared to the exact and on right, the approximate SOGNS is compared to the exact PEPGNS.

Figure 2: Investigation of the accuracy of the approximation from Section 3.1 on MNIST.