# Non-asymptotic Convergence of Training Transformers for Next-token Prediction

Ruiquan Huang

Penn State University

State College, PA, 16801

rzh5514@psu.edu

&Yingbin Liang

Ohio State University

Columbus, OH, 43210

liang.889@osu.edu

&Jing Yang

Penn State Univeristy

State College, PA, 16801

yangjing@psu.edu

###### Abstract

Transformers have achieved extraordinary success in modern machine learning due to their excellent ability to handle sequential data, especially in next-token prediction (NTP) tasks. However, the theoretical understanding of their performance in NTP is limited, with existing studies focusing mainly on asymptotic performance. This paper provides a fine-grained non-asymptotic analysis of the training dynamics of a one-layer transformer consisting of a self-attention module followed by a feed-forward layer. We first characterize the essential structural properties of training datasets for NTP using a mathematical framework based on partial orders. Then, we design a two-stage training algorithm, where the pre-processing stage for training the feed-forward layer and the main stage for training the attention layer exhibit fast convergence performance. Specifically, both layers converge _sub-linearly_ to the direction of their corresponding max-margin solutions. We also show that the cross-entropy loss enjoys a _linear_ convergence rate. Furthermore, we show that the trained transformer presents non-trivial prediction ability with dataset shift, which sheds light on the remarkable generalization performance of transformers. Our analysis technique involves the development of novel properties on the attention gradient and further in-depth analysis of how these properties contribute to the convergence of the training process. Our experiments further validate our theoretical findings.

## 1 Introduction

The transformer architecture (Vaswani et al., 2017) has revolutionized the field of machine learning, establishing itself as a foundation model for numerous applications, including natural language processing (NLP) (Devlin et al., 2018), computer vision (Dosovitskiy et al., 2020), and multi-modal signal processing (Tsai et al., 2019). In particular, transformers achieve tremendous empirical success in large language models (LLMs) such as GPT-3 (Brown et al., 2020). Despite the empirical success, limited theoretical understanding of transformers have caused a series of critical concerns about their robustness, interpretability, and bias issues (Bommasani et al., 2021; Belkin, 2024).

To overcome these issues, recent advances in transformer theory have investigated the convergence of training transformers under theoretically amenable setting such as linear regression (Mahankali et al., 2023; Zhang et al., 2023; Huang et al., 2023) and binary classification (Tarzanagh et al., 2023, 2024; Vasudeva et al., 2024; Li et al., 2023). Nevertheless, one of the fundamental task in LLMs and other generative models is next-token prediction (NTP), which involves predicting the next word or token in a sequence, given the previous tokens. In NTP, a few recent theoretical studies have started to investigate the training dynamics of transformers (Tian et al., 2023; Li et al., 2024). However, those works lack of fine-grained _non-asymptotic_ convergence analysis of the training process, posing the following open questions for further investigation:In addition, a pre-trained transformer empirically exhibits non-trivial generalization ability. A follow-up question from a theoretical point of view is that _Can we show the generalization capability of a trained transformer on unseen data?_

In this paper, we take a first step towards addressing the aforementioned questions by studying the training dynamics of a single layer transformer consisting of a self-attention layer and a feed-forward layer for NTP. We summarize our contribution as follows.

* We develop a mathematical framework based on partial order to formally characterize the essential structural properties of the training dataset for next-token prediction. In particular, we introduce a realizable setting for training datasets where the loss can be minimized to near zero, which admits a _collocation_ and _query-dependent partial orders_. A collocation is a set of token pairs where each token is directly paired with its subsequent token. Query-dependent partial orders is a set of partial orders where each partial order classifies tokens into three categories: optimal tokens, non-optimal tokens and non-comparable tokens. These structural properties define favorable max-margin problems on both the feed-forward layer and the self-attention layer.
* Second, we design a two-stage training algorithm based on normalized gradient descent. In stage 1 of pre-processing, we use the collocation to train the feed-forward layer. In stage 2, we use the entire dataset to train the self-attention layer. We show that the feed-forward layer and the query-key attention matrix converge sublinearly in direction respectively to the max-margin solution for classifying next token from all other tokens in the preprocessing dataset, and to the max-margin solution for classifying the optimal from non-optimal tokens. In addition, the norm of the transformer parameters grows linearly, which further yields a _linear_ convergence rate of the cross-entropy loss. Our two-stage algorithm decouples the training of the feed-forward and attention layers without losing optimality, as stage 1's max-margin solution is judiciously designed to facilitate stage 2's fine-grained classification for optimal token prediction.
* Third, we show that the trained transformer has generalization ability for making non-trivial prediction on unseen data. In particular, the transformer is trained to learn an extended query-dependent partial order, where the non-comparable tokens are inserted in between the optimal tokens and non-optimal tokens. Thus, the trained transformer will attend to non-comparable tokens if optimal tokens are not in a new sentence and further make desirable prediction.

## 2 Related Work

Inspired by Brown et al. (2020), who demonstrated that pre-trained transformers can learn in-context - i.e., learn new tasks during inference with only a few samples - a series of works focus on the expressiveness power of transformers (Akyurek et al., 2022; Bai et al., 2023; Von Oswald et al., 2023; Fu et al., 2023; Giannou et al., 2023; Lin et al., 2023). These studies have shown that there exist parameter configurations such that transformers can perform various algorithms such as gradient descent. Additionally, Edelman et al. (2022) showed that transformers can represent a sparse function.

Regarding the training dynamics and optimization of transformers under in-context learning, Ahn et al. (2024); Mahankali et al. (2023); Zhang et al. (2023); Huang et al. (2023) studied the dynamics of a single attention layer, single-head transformer for the in-context learning of linear regression tasks. Cui et al. (2024) proved that multi-head attention outperforms single-head attention. Cheng et al. (2023) showed that local optimal solutions in transformers can perform gradient descent in-context for non-linear functions. Kim and Suzuki (2024) studied the nonconvex mean-field dynamics of transformers, and Nichani et al. (2024) established a convergence rate of \((1/t)\) for the training loss in learning a causal graph. Additionally, Chen et al. (2024) investigated the gradient flow in training multi-head attention. Chen and Li (2024) proposed a supervised training algorithm for multi-head transformers.

Another line of research focuses on the training dynamics of transformers for binary classification problems. Tarzanagh et al. (2023b, a) demonstrated an equivalence between the optimization dynamics of a single attention layer and a certain SVM problem. While Tarzanagh et al. (2023b, a) only proved an asymptotic convergence result, Vasudeva et al. (2024) improved the convergence rate to \(t^{-3/4}\). Li et al. (2023) studied the training dynamics of vision transformers and showed that the generalization error can approach zero given sufficient training samples. Additionally, Deora et al. (2023) investigated the training and generalization error under the neural tangent kernel (NTK) regime.

For transformers trained on next-token prediction (NTP), Tian et al. (2023) analyzed the training dynamics of a single-layer transformer, while Tian et al. (2023) studied the joint training dynamics of multi-layer transformers. Li et al. (2024) demonstrated the asymptotic convergence of transformers trained with a logarithmic loss function for NTP. Although these works provided valuable insights into the training dynamics of transformers for NTP, they did not provide the finite-time convergence analysis, which is the focus of this paper. We remark that Thrampoulidis (2024) studied NTP without transformer structure.

Our work is also related to the classical implicit bias framework for training neural networks (NNs). In particular Soudry et al. (2018); Nacson et al. (2019); Ji and Telgarsky (2021); Ji et al. (2021) established convergence rate of gradient descent-based optimization. Phuong and Lampert (2020); Frei et al. (2022); Kou et al. (2024) studied the implicit bias of ReLU/Leaky-ReLU networks on orthogonal data. A comprehensive survey is provided in Vardi (2023). However, these works focused on classical neural networks, whereas we investigate the implicit bias of transformers for NTP.

## 3 Problem Setup

**Notations.** All vectors considered in this paper are column vectors. We use \(\{A\}\) to denote the indicator function of \(A\), i.e., \(\{A\}=1\) if \(A\) holds, and \(\{A\}=0\) otherwise. \(\|W\|\) represents the Frobenius norm of the matrix \(W\). For a vector \(v\), we use \([v]_{i}\) to denote the \(i\)-th coordinate of \(v\). We use \((v)\) to denote the softmax function, i.e., \([(v)]_{i}=(v_{i})/_{j}(e_{j}^{}v)\), which can be applied to any vector with arbitrary dimension. We use \(\{e_{i}\}_{i[||]}\) to denote the canonical basis of \(^{||}\), i.e., \([e_{i}]_{j}=\{i=j\}\). The inner product \( A,B\) of two matrices \(A,B\) equals to \((AB^{})\).

**Next-token prediction.** We consider the task of next-token prediction, which aims to predict the subsequent token in a token sequence given its preceding tokens. Formally, suppose that there exists a finite vocabulary set \(^{d}\) that consists of all possible tokens, where \(d\) is the dimension of the embedding. Each token \(x\) is associated with a unique index \((x)\{1,2,,||\}\), where \(\) is the index function. An \(L\)-length sentence \(X=[x_{1},,x_{L}]^{L}^{d L}\) is a sequence of \(L\) tokens, where \(L\) is an integer. We assume that the maximum length of sentences is \(L_{}\). The subsequent tokens in sentences are generated from a set of ground-truth model \(\{p_{L}^{*}:^{L}\}_{L<L_{}}\), where \(p_{L}^{*}\) generates the next token \(x_{L+1}\) given the sentence \(X\) for any \(1 L<L_{}\). The task of next-token prediction requires us to learn all models \(\{p_{L}^{*}\}_{L<L_{}}\) given a training dataset \(_{0}=\{(X,x_{L+1})|L<L_{},X^{L},x_{L+1} \}\). Notably, if \(X=[x_{1},,x_{L}]_{0}\), then for any \(<L\), \(([x_{1},,x_{}],x_{+1})\) is also a training sample, since it follows \(p_{}^{*}\) as well.

**Decoder-only transformer.** A decoder-only transformer is a stack of blocks consisting of a self-attention layer and a feed-forward layer. For simplicity, we consider one-layer transformer, where the self-attention layer is determined by three matrices: \(W_{}^{d d_{1}}\), \(W_{}^{d_{1} d}\) and \(W_{}^{d_{2} d}\), namely key, query, and value matrices, and the feed-forward layer is determined by \(W_{}^{|| d_{2}}\). Here \(d_{1},d_{2}\) are hidden dimensions. Mathematically, given the input \(X=[x_{1},x_{L}]\), we write the one-layer transformer as \(_{}(X):=(W_{}W_{}X(X^{}W_{ }W_{}x_{L}))^{||}\), where \(:=(W_{},W_{},W_{},W_{})\), and \(\) is the softmax function. We note that the inner softmax function \(\) is part of the attention model, and the outer softmax function \(\) is the decoder that generates a probability distribution over \(\) for token prediction.

**Reparameterization.** We reparameterize the transformer architecture by consolidating the key and query matrices into a unified matrix \(W_{}\), such that \(W_{}=W_{}W_{}\). Similarly, we reparameterize the product of the feed-forward (\(W_{}\)) and value (\(W_{}\)) matrices as a single matrix \(W_{}\), defined as \(W_{}=W_{}W_{}\). Such a reparameterization is commonly adopted in transformer theory works (Huang et al., 2023; Tian et al., 2023; Li et al., 2024; Nichani et al., 2024). Thus, the transformer under those reparameterization is given by \(_{}(X):=(W_{}X(X^{}W_{}x_{L }))^{||}\).

**Cross-entropy loss.** Given the training dataset \(_{0}\) and the transformer model, we seek to learn \(p_{*}\) by minimizing (training) the _cross-entropy_ loss \(()\) defined as follows:

\[()=-_{0}|}_{(X,x_{L+1})_{0}} e_{(x_{L+1})}^{}_{}(X),\]where \((x_{L+1})\) is the index of \(x_{L+1}\) in \(\).

## 4 Realizable Training Dataset and Two-Stage Algorithm

In this section, we first provide a mathematical framework based on partial order to formally characterize a realizable training dataset for next-token prediction. We will then describe a two-stage algorithm for next-token prediction that we study.

### Realizable Training Dataset

We characterize a realizable training dataset via two structural properties, where the training loss can be made arbitrarily close to zero. We first provide some intuitions about those two properties.

**Existence of "collocation".** First, we note that if a sentence \(X=[x_{1},,x_{L}]\) is a legal training sample, \(([x_{1}],x_{2})\) is also in the training dataset. In addition, the output of a transformer given one single input token only depends on \(W_{}\), i.e. the feed-forward layer. Since training loss can be arbitrarily close to 0, there exists a sequence \(\{W_{t}\}\) such that \(_{t}-_{x_{0}} e_{}(x)^{}(W_{t }x)=0\), where \((x)\) is the index of next token of \(x\), and the summation is over the case when \(x\) is the first token. Due to that \((W_{t}x)\) is a probability distribution, the equality holds only when \(\) is injective, since otherwise it is an entropy of some distribution which is strictly greater than 0. Therefore, there exists an injective map \(:\) such that every sentence starts with \(x\), must have a unique next token \((x)\). We call the set of pairs \(\{x,(x)\}_{x}\) a _collocation_. We remark that \(p_{1}^{*}=\).

**Existence of "order".** Second, let us consider the output of a transformer \(_{}\) given a legal sentence \(X=[x_{1},,x_{L}]\) with the next token \(x_{L+1}=p_{L}^{*}(X)\). The transformer first calculates a convex combination of \(x_{1},,x_{L}\) with corresponding weight \(_{}(x_{}^{}W_{}x_{L})\) for each \( L\). Then, the transformer outputs \((_{}W_{}x_{}_{})\). Recall that the collocation forces \(x_{}\) to map \((x_{})\), thus \((W_{}x_{})\) has a peak value at the coordinate equal to \(((x_{}))\) (the index of \((x_{})\)). Hence, \(_{}(X)\) can only have peak value at the coordinates within the set \(\{((x_{}))\}_{ L}\). If the training loss can be arbitrarily close to 0, it is desirable to have \(^{-1}(x_{L+1})\{x_{}\}_{ L}\). Therefore, for those \(x_{}\) with \((x_{})=x_{L+1}\), \(_{}\) must be larger than \(_{^{}}\) with \((x_{^{}}) x_{L+1}\). Finally, it worth noting that \(_{}\) depends on the final token \(x_{L}\). This observation motivates us to define _query-dependent partial orders_ on \(\).

**Definition 1** (\(x^{q}\)-partial order): _Fix a token \(x^{q}\). An \(x^{q}\)-partial order assigns an ordering relationship \(>_{x^{q}}\) for certain pairs of tokens in \(\), and is created as follows. Let \(_{0}^{x^{q}}\) be the set of all legal sentences in the training dataset that has the final token (query) \(x^{q}\). Then, for any pair of tokens \(x,x^{}\), we assign \(x>_{x^{q}}x^{}\) if there exists a sentence \(X=[x_{1},,x_{L}]_{0}^{x^{q}}\) and \(x,x^{}\) are tokens in \(X\) such that \((x)=x_{L+1}(x^{})\), where \(x_{L+1}\) is the next token of \(X\)._

Note that Definition 1 is a "constructive definition" which might not be well-defined. However, as we are under the setting when the training loss can be arbitrarily close to 0, the aforementioned discussion shows that if \(x>_{x^{q}}x^{}\), then \(_{}>_{^{}}\), where \(x=x_{}\) and \(x^{}=x_{^{}}\) in some sentence. Thus, \((xW_{}x^{q})>(x^{}W_{}x^{q})\), which indeed need to be well-defined. Otherwise, we will have contradictions such as \((xW_{}x^{q})>(x^{}W_{}x^{q})<(xW_{}x^{q})\). Mathematically, a well-defined (strict) partial order \(>\) on a set \(\) satisfies two axioms (Yannakakis, 1982): (i) there is no \(x>x\); (ii) if \(x>x^{}\) and \(x^{}>x^{}\), then \(x>x^{}\). Thus, \(x^{q}\)-partial order created by \(_{0}\) is well-defined for every \(x^{q}\).

Finally, let us discuss the impact of query-dependent partial orders on \(_{0}\). For a given query \(x^{q}\), the partial order \(>_{x^{q}}\) divides tokens in \(\) into four disjoint types.

* [leftmargin=*,noitemsep,topsep=0pt,parsep=0pt,itemsep=0pt]
* _(Strict) optimal tokens._ A token \(x\) is optimal, if there is no \(x^{}\) such that \(x^{}>_{x^{q}}x^{1}\).
* _Confused tokens._ A token \(x\) is confused, if there exists \(x^{},x^{}\) such that \(x^{}>_{x^{q}}x^{}\).
* _(Strict) non-optimal tokens._ A token \(x\) is non-optimal if there is no \(x^{}\) such that \(x>_{x^{q}}x^{ 2}\).
* _Non-comparable tokens._ A token \(x\) is non-comparable if there is no \(x^{}\) such that \(x>_{x^{q}}x^{}\) or \(x^{}>_{x^{q}}x\).

In this work, we assume that there are no confused tokens. This assumption simplifies the problem, making it tractable to provide explicit convergence in direction for training a transformer in Section 5. In summary, we make the following structural assumption on the training dataset.

**Assumption 1** (Realizable training dataset): \(_{0}\) _admits (i) a collocation \(\{x,(x)\}_{x}\); (ii) well-defined query-dependent partial orders, where every \(x^{q}\)-partial order has no confused tokens._

We remark that combining the collocation and query-dependent partial orders, we can regenerate the training dataset as follows. For any sentence with only one token \(X=[x]\), the next token is \((x)\). For other sentences \(X=[x_{1},,x_{L}]\), let \(x_{}\) be optimal under the partial order \(>_{x_{L}}\), and then the next token of \(X\) is \((x_{})\). We next provide a simple example that justifies Assumption 1.

**Example 1**: _Consider a language system where the vocabulary consists of four tokens {S, V, O, P}, where S,V,O,P respectively stand for subject, verb, object, and punctuation mark. This system admits the commonly adopted word order (Dryer, 1991): S, V, O, P. Let the training dataset be {SVOP, VOP, OPP, PSV}._

_Let us create the corresponding collocation and the query-dependent partial orders from the dataset. The collocation is \(\{(,),(,),(,),( ,)\}\). That is, if a sentence starts with a subject, then the next token is a verb. Similarly, if a sentence starts with a verb, then the next token is an object, and so on. The query-dependent partial orders are created as follows:_

 _Partial order under query S. S\(>_{}\)P._ & _Partial order under query V. V\(>_{}\)S._ \\ _Partial order under query O. O\(>_{}\)S, O\(>_{}\)V._ & _Partial order under query P. O\(>_{}\)P._ \\ 

_Therefore, if a sentence starts with S (subject), the next token is V (verb) according to the collocation. Then, for the sentence SV, since the query is V and V\(>_{}\)S, the next token of the sentence coincides with the next token of V, which is exactly O (object). Finally, for the sentence SVO, following similar argument, the next token is P (punctuation mark). This example satisfies Assumption 1 and aligns with real-world scenarios. An illustration is provided in Figure 1._

**Additional notations of training data.** It is worth noting that there are only finite number of distinct sentences. For ease of presentation, we introduce the following notations. Suppose there are \(N\) distinct sentences in the training dataset \(_{0}\) indexed by \(n\{1,,N\}\). For each distinct sentence \(X^{(n)}\), we calculate its frequency \(^{(n)}\) in dataset \(_{0}\) as \(^{(n)}=)_{0}}\{X=X^{(n )}\}}{|_{0}|}\).

Building upon this, with a little abuse of notation, we use \((X^{(n)})\) to denote the subsequent token of the sentence \(X^{(n)}\) and \((X^{(n)})\) to denote the index of \((X^{(n)})\).

We further denote \(X^{(n)}_{-1}\) as the final token of \(X^{(n)}\), and let \(}_{}(X)=W_{}X(X^{}W_{}X^{(n )}_{-1})\). Then, the loss function \(()\) can be rewritten as follows:

\[()=_{n}^{(n)}((_{v}(e_{v}^{ }}_{}(X^{(n)})))-e_{(X^{(n)}) }^{}}_{}(X^{(n)})).\] (1)

### Training Algorithm

For the realizable dataset satisfying Assumption 1, we propose a two-stage training algorithm using normalized gradient descent (NGD). The pseudo code of the algorithm is presented in Algorithm 1. In Section 5, we show that the two-stage algorithm decouples the training of the feed-forward and

Figure 1: The left plot shows the mapping from sentence to the next token. The red rectangle indicates the optimal token in the corresponding sentence. The right plot shows the collocation relationship.

attention layers without losing the optimality. This is because the training in stage 1 is designed to yield a suitable max-margin solution, which will enable the training of stage 2 to solve a fine-grained classifcation problem and identify the optimal token for prediction.

In the first stage of pre-processing, we use the collocation set to train the feed-forward layer \(W_{}\). For simplicity, we introduce the following notation for the training loss of the feed-forward layer. Given a collocation \(\{x,(x)\}_{x}\), which can be obtained through extracting all length-2 sentences in the training dataset \(_{0}\)3, we use normalized gradient descent to train \(W_{}\). Equivalently, the loss function can be written as

\[_{0}(W_{})=-_{x}(x)}^{}W_{}x)}{_{v||}(e_{v}^{ }W_{}x)},\]

where the self-attention elements are removed because the attention matrices are not trained here. Based on the above loss function, we initialize \(W_{}^{(0)}=0^{|| d}\), and subsequently take an update at each time \(t\) by NGD as in line 4 of Algorithm 1.

In the second stage, we fix the trained feed-forward layer and train the self-attention layer based on the loss function given in Equation (1) and using the entire dataset \(_{0}\). Specifically, we initialize \(W_{}=0^{d d}\), and subsequently take an update at each time \(t\) by NGD as in line 7 of Algorithm 1.

```
1:Initialization:\(W_{}^{(0)}=0^{|| d}\), \(W_{}=0^{d d}\).
2:Input: A collocation \(\{x,(x)\}_{x}\), and a training dataset \(_{0}\), learning rate \(_{0},\).
3:for\(t\{0,1,...,T-1\}\)do
4: Update \(W_{}^{(t+1)}\) as \(W_{}^{(t+1)}=W_{}^{(t)}-_{0}} }_{0}(W_{}^{(t)})}{\|_{W_{}}_{0 }(W_{}^{(t)})\|}\).
5:endfor
6:for\(t\{0,,T_{1}-1\}\)do
7: Update \(W_{}^{(t+1)}\) as \(W_{}^{(t+1)}=W_{}^{(t)}-}}(^{(t)})}{\|_{W_{}}(^{(t )})\|}\), where \(^{(t)}=(W_{}^{(T)},W_{}^{(t)})\).
8:endfor ```

**Algorithm 1** Two-stage Normalized Gradient Descent

## 5 Training Dynamics of the Transformer

In this section, we present the convergence result for Algorithm 1. Before we proceed, we first introduce the following technical assumption, which has been commonly adopted in the previous theoretical studies of transformers (Huang et al., 2023; Li et al., 2024; Tian et al., 2023a).

**Assumption 2**: _The vocabulary set is orthonormal. Namely, the embedding has unit norm, i.e., \(\|x\|=1\), and \(x^{}x^{}=0\) holds for any distinct tokens \(x\) and \(x^{}\)._

### Convergence of Training \(W_{}\)

To characterize the training dynamics of \(W_{}\), we observe that the collocation \(\{(x,(x))\}_{x}\) defines the following hard-margin problem:

\[W_{}^{*}=\|W\|,(e_{v^{*}}-e_{v})Wx 1, v^{*}=(x),v(x).\] (2)

It can be shown that \(_{B+}_{0}(BW_{}^{*})=0\). Thus, the loss function \(_{0}\) trains \(W_{}\) to be the max-margin solution with \(W_{}x\) distinguishing the next token \((x)\) from all other tokens in \(\).

Since \(_{0}()\) is convex, we have the following convergence result on the training of \(W_{}^{(t)}\).

**Proposition 1**: _Let \(W_{}^{*}\) be defined in Equation (2). Under Assumptions 1-2, let \(W_{}^{(t)}\) be updated by Algorithm 1. Then, for any \(t 2\), we have \(}{2\|W_{}^{(t)}\|}\|W_{}^{(t)}\| t_ {0}\) and the following bound holds:_

[MISSING_PAGE_FAIL:7]

_where \(C_{0}=}{4\|W^{*}_{}\|^{2}}\) and \(C_{1}=/(4L_{}\|W^{*}_{}\|^{2})\)._

Theorem 2 shows that the training loss converges to its minimum value at a linear convergence rate. Furthermore, for \(T=((1/_{0}))\)\(t=((1/))\), the optimal token weight is given by \(1/(1+)\) for any \(>0\), which is close to 1. This implies that the trained transformer attends to the optimal token and thus outputs the correct next token \((x^{(n)}_{_{*}})\) with probability \(1-O(_{0})\).

### Proof Sketch of Theorem 1

The proof consists of the following three main steps. The key proof step lies in carefully analyzing the projection of gradient \(_{W_{}}(^{(t)})\) onto the token-query outer product \(x^{(n)}_{}(X^{(n)}_{-1})^{}\), max-margin attention weight matrix \(W^{*}_{}\), and the trained attention weight matrix \(W^{(t)}_{}\).

**Step 1 (Lemma 5).** By analyzing \(_{W_{}}(^{(t)}),x^{(n)}_{} (X^{(n)}_{-1})^{}\), we characterize the dynamics of attention weights. Using mathematical induction, we show that the lower bound of optimal token weight is \(1/L_{}\).

**Step 2 (Lemma 6).** Then, we show that the cosine similarity between the negative gradient and \(W^{*}_{}\) is strictly larger than the minimum optimal token weight. Utilizing step 1, due to the NGD update, the norm of the key-query matrix \(W^{(t)}_{}\) can be shown to grow linearly.

**Step 3 (Lemma 7).** Finally, we carefully compare the difference between the projections from gradient to the trained attention matrix and max-margin attention matrix. By separately evaluating the impact of the optimal and non-optimal tokens on those projections, we can show the following inequality for some constant \(C_{0}\):

\[_{W_{}}(^{(t)}),W^{(t)}_{}(1+\|W^{(t)}_{}\|}{\|W^{(t)}_{ }\|})_{W_{}}(^ {(t)}),W^{*}_{}_{}\|}{\|W^{ *}_{}\|}.\]

Utilizing step 2's result that \(\|W^{(t)}_{}\|\) grows linearly, the dynamics of the attention layer can be shown to converge in direction to the max-margin solution in Equation (3).

## 6 Generalization Ability

In this section, we prove the generalization ability of the trained transformers. Recall that Theorem 1 shows that \(W^{(t)}_{}\) converges to \(W^{*}_{}\|W^{(t)}_{}\|/\|W^{*}_{}\|\). To characterize the generalization ability, it is desirable to use the property of \(W^{*}_{}\), which is given in the following result.

**Proposition 2**: _Under Assumptions 1-2, fix a query token \(x^{q}\), let \(_{x^{q}},_{x^{q}},_{x^{q}}\) be the set of optimal tokens, the set of non-optimal tokens, and the set of non-comparable tokens, under \(x^{q}\)-partial order, respectively. Then, the solution \(W^{*}_{}\) of Equation (3) satisfies \(x^{}_{0}W^{*}_{}x^{q}=0\) for \(x_{0}_{x^{q}}\), and_

\[x^{}_{*}W^{*}_{}x^{q}=_{x^{q}}|}{| _{x^{q}}|+|_{x^{q}}|}, x^{}W^{*}_{}x^{q}=- {|_{x^{q}}|}{|_{x^{q}}|+|_{x^{q}}|},  x_{*}_{x^{q}},x_{x^{q}}.\]

Recall that non-comparable tokens (see Section 4) under a query \(x^{q}\) never appears in any training sentence data with the same query \(x^{q}\). Thus, Proposition 2 implies an interesting generalization capability - each \(x^{q}\)-partial order can automatically incorporate more relationships to expand the query-dependent partial orders. Combining Proposition 2 with Theorem 1, we obtain the following theorem on \(W^{(t)}_{}\).

**Theorem 3**: _Under the conditions and notations in Proposition 2, let \(T=((1/))\), and \(t=((1/))\). Then there exists a constant \(C_{0}\) such that_

\[(x_{*}-x_{0})^{}W^{(t)}_{}x^{q} C_{0}t,(x_{0}-x)^{ }W^{(t)}_{}x^{q} C_{0}t, x_{*}_{ x^{q}},x_{0}_{x^{q}},x_{x^{q}}.\]

_Moreover, if the trained transformer takes input \(X\) with query \(x^{q}\) that consists of a non-comparable token \(x_{0}\) and non-optimal tokens, then the prediction made by \(_{^{(t)}}(X)\) is \((x_{0})\) with high probability._Theorem 3 suggests that a new partial order is created by the trained transformer. Specifically, it inserts the non-comparable tokens between the optimal and non-optimal tokens. The trained transformer can generalize the token prediction to such new sentences as given in Theorem 3.

We use Example 1 to illustrate the generalization ability described above.

**Example 2** (Generalization to unseen data in Example 1): _Recall that in Example 1, the training dataset consists of four sentences: SVOP, VOP, OPP, and PSV. Consider the partial order \(>_{P}\) under the punctuation mark P. We have that O\(>_{P}\)P and O is an optimal token, P is a non-optimal token, and S,V are non-comparable tokens. We then have the following non-trivial prediction by the trained transformer._

**Case 1.** _Non-comparable tokens are learned to be "larger" than non-optimal tokens._

_Consider a new (unseen) input sentence SP. Since S is non-comparable before training, but is "larger" than P under the trained key-query matrix \(W^{(t)}_{_{1}}\), the next predicted token is \((S)\) = V._

**Case 2.** _Optimal tokens remain optimal over all tokens after training._

_Consider a new (unseen) input OSP. O is optimal and S is still "smaller" than O under the trained P-partial order. The trained transformer will consistently predict P._

In both of the above cases, the trained transformer provides desirable prediction for the unseen sentences. We further note that the effectiveness of both cases can vary during the inference time of the trained transformer. For instance, if the input sequence is SP (subject-punctuation), the output is SPV (subject-P-verb), which follows a logical subject-verb order and is desirable. However, in cases where the input is VP (verb-punctuation), it may be preferable to terminate the sequence after the verb, i.e., VPP, as the verb alone can suffice to convey the intended meaning.

## 7 Experiment

In this section, we verify our theoretical findings via an experiment on a synthetic dataset. Specifically, we randomly generate a realizable dataset as described in Assumption 1 with \(||=20\). Then, we train \(W_{}\) and \(W_{}\) by Algorithm 1, each with 900 iterations. The parameters are chosen as \(d=||\), \(_{0}=0.2/\), and \(=0.05/\). In Figure 2, the first three plots show the dynamics of the training stage 1, which indicates the convergence of the loss \(_{0}(W^{(t)}_{})\) to its minimum value, the convergence of \(W^{(t)}_{}\) in direction to \(W^{*}_{}\), and the linear increase of the norm \(\|W^{(t)}_{}\|\), respectively. These results verify Proposition 1. The last three plots show the dynamics of the training stage 2, which indicates the convergence of the loss \((^{(t)})\), the convergence of \(W^{(t)}_{}\) in direction to \(W^{*}_{}\), and the linear increase of the norm \(\|W^{(t)}_{}\|\). These results verify Theorem 1 and Theorem 2. All experiments are conducted on a PC equipped with an i5-12400F processor and 16GB of memory.

## 8 Conclusion

In this work, we investigated the training dynamics of a single-layer transformer for NTP. We first characterized two structural properties of the training dataset under the realizable setting where the training loss can be made arbitrarily close to zero. These properties allow us to define two max-margin solutions for both the feed-forward layer and the self-attention layer. Then, we showed that both layers converge in direction to their corresponding max-margin solutions sub-linearly, which further yields a linear convergence of the training loss for NTP. We further showed that the well trained transformer can have non-trivial prediction ability on unseen data, which sheds light on the generalization capability of transformers. Our experiments verify our theoretical findings.

Figure 2: Training dynamics of single-layer transformer for NTP.