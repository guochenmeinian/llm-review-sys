# MultiFusion: Fusing Pre-Trained Models for Multi-Lingual, Multi-Modal Image Generation

Marco Bellagente\({}^{4}\)\({}^{}\) Manuel Brack\({}^{2,3}\)\({}^{}\) Hannah Teufel\({}^{1}\) Felix Friedrich\({}^{3,6}\) Bjorn Deiseroth\({}^{1,3,6}\) Constantin Eichenberg\({}^{1}\) Andrew Dai\({}^{1}\) Robert J.N. Baldock\({}^{1}\)

Souradeep Nanda\({}^{5}\) Koen Oostermeijer\({}^{1}\) Andres Felipe Cruz-Salinas\({}^{1}\)

Patrick Schramowski\({}^{2,3,6,8}\) Kristian Kersting\({}^{2,3,6,7}\) Samuel Weinbach\({}^{1}\)

\({}^{1}\)Aleph Alpha, \({}^{2}\)German Research Center for Artificial Intelligence (DFKI),

\({}^{3}\)Computer Science Department, TU Darmstadt, \({}^{4}\)Stability AI, \({}^{5}\)University of Texas,

\({}^{6}\)Hessian.AI, \({}^{7}\)Centre for Cognitive Science, TU Darmstadt, \({}^{8}\)LAION

marco.bellagente@gmail.com

brack@cs.tu-darmstadt.de

hannah.teufel@aleph-alpha.de

Work performed while at Aleph AlphaEqual contributionEqual supervision

###### Abstract

The recent popularity of text-to-image diffusion models (DM) can largely be attributed to the intuitive interface they provide to users. The intended generation can be expressed in natural language, with the model producing faithful interpretations of text prompts. However, expressing complex or nuanced ideas in text alone can be difficult. To ease image generation, we propose MultiFusion that allows one to express complex and nuanced concepts with arbitrarily interleaved inputs of multiple modalities and languages. MultiFusion leverages pre-trained models and aligns them for integration into a cohesive system, thereby avoiding the need for extensive training from scratch. Our experimental results demonstrate the efficient transfer of capabilities from individual modules to the downstream model. Specifically, the fusion of all independent components allows the image generation module to utilize multilingual, interleaved multimodal inputs despite being trained solely on monomodal data in a single language.

## 1 Introduction

The recent popularity of text-to-image diffusion models (DM)  can largely be attributed to the intuitive interface they provide to users. The intended generation can easily be expressed in natural language, with the model producing faithful interpretations of a text prompt. Recent works have demonstrated the output quality to be largely dependent on the input encoders with more powerful variants yielding more expressive DMs . We take these insights one step further, vastly enhancing the capabilities of a pre-trained DM through the sophisticated integration of dedicated modules. We propose MultiFusion which effectively supports arbitrarily interleaved inputs of multiple modalities and languages. Further, we transfer these capabilities from an underlying language model (LM), eliminating the need for multilingual or multimodal interleaved training data for the generative model. Our approach can utilize readily available datasets and requires less than 5% of the training compute needed to build a comparable DM from scratch.

The capabilities of current text-to-image DMs are often restricted by the types of inputs they support. MultiFusion addresses two of these limitations, facilitating more expressive prompting. Firstly, most of the widely available models are only designed for one language, whereas MultiFusion supports five. Contrary to existing multilingual DMs , MultiFusion requires no multilingual data for DM training and instead only uses readily available English training data. Secondly, text is inherently limited in its expressiveness--many concepts are difficult to articulate with words alone . Consequently, a number of models have been proposed, allowing visual inputs as reference for image generation. Popular examples include image variation , adherence to an exemplary style , sketch-guided generation , or incorporating personal concepts in the resulting images . All of these methods go beyond established image editing techniques [1; 20; 44; 8] where certain aspects of the input picture are altered. However, these _image_-to-image methods are limited to specialized tasks and lack the same level of control and generative versatility for image inputs as for natural language. MultiFusion provides a powerful and flexible interface for arbitrary combinations of multimodal and multilingual inputs. The extended prompt capabilities result in a more expressive model facilitating a wide range of complex applications and use cases.

Our main contributions are as follows: (1) We present MultiFusion, a multilingual, multimodal diffusion model (2) efficiently bootstrapped using a modular encoder based on an auto-regressive language model. (3) We highlight the use of attention manipulation  for multimodal prompting, and (4) demonstrate the transfer of an LM's multilingual capabilities to downstream tasks, removing the need for multilingual downstream training data. (5) Lastly, we introduce the MCC-250 benchmark for the evaluation of compositionality from multimodal inputs.

## 2 Background

**Text-conditioned diffusion.** Diffusion models [39; 35; 2; 37] iteratively denoise a Gaussian distributed variable to produce samples of a learned data distribution. Intuitively, image generation starts from random noise \(\), and the model predicts an estimate of this noise \(_{}\) to be subtracted from the initial values. The denoising process results in a high-fidelity image \(x_{0}\) without any noise. Since this is an extremely hard problem, multiple steps are applied, each subtracting a small amount (\(_{t}\)) of the predictive noise, approximating \(\). For text-to-image generation, the model's \(\)-prediction is conditioned on a text prompt \(p\) and results in an image faithful to that prompt. The training objective of a diffusion model \(_{}\) can be written as

\[_{,_{p},,t}[w_{t}||} _{}(_{t}+_{t},_{p})-|| _{2}^{2}]\] (1)

where \((,_{p})\) is conditioned on text prompt \(p\), \(t\) is drawn from a uniform distribution \(t()\), \(\) sampled from a Gaussian \((0,)\), and \(w_{t},_{t},_{t}\) influence image fidelity. Consequently, the DM is trained to denoise \(_{t}:=+\) to yield \(\) with the squared error as loss. At inference, the DM is sampled using the model's prediction of \(=(_{t}-})\), with \(}\) as described below.

Classifier-free guidance  is a conditioning method using a purely generational diffusion model, eliminating the need for an additional pre-trained classifier. The approach randomly drops the text conditioning \(_{p}\) with a fixed probability during training, resulting in a joint model for unconditional

Figure 1: MultiFusion architecture. We augment a Decoder Language Model (1.1.) with adapters, finetuned for multimodality (1.2) as well as biases (2.1), finetuned for semantic search. Next, we condition the diffusion model (1.3) through cross-attention (2.2) on embeddings produced by the LM. During diffusion training, we use either the image or the caption for conditioning, while inference is performed with multimodal input. (Best viewed in color.)

and conditional objectives. During inference score estimates for the \(\)-prediction are adjusted so that:

\[_{}(_{t},_{p}):=_{}( _{t})+s_{g}(_{}(_{t},_{p})- _{}(_{t}))\] (2)

with guidance scale \(s_{g}\) which is typically chosen as \(s_{g}(0,20]\) and \(_{}\) defining the noise estimate with parameters \(\). Intuitively, the unconditioned \(\)-prediction \(_{}(_{t})\) is pushed in the direction of the conditioned \(_{}(_{t},_{p})\) to yield an image faithful to prompt \(p\). Lastly, \(s_{g}\) determines the magnitude of the influence of the text \(p\). Naturally, the prompt \(p\) is not fed directly into the model, but instead a high dimensional representation of \(p\) is obtained through a decoder. In prevalent models, the prompt \(p\) is text in natural language, whereas in the case of MultiFusion, \(p\) is a sequence of text and images.

Multimodality.Prevalent encoder models like CLIP --or multilingual variants like AltCLIP--are distinctly unsuited for arbitrarily interleaved sequences of images and text. Unlike MultiFusion's multimodal encoder, these architectures rely on two separate encoders for textual and visual inputs, with their respective representations being aligned during training. In contrast to our model, the resulting embeddings only encode a single image or textual description but no interleaved sequences comprised of both modalities. Prior work shows that large language models (LMs) produce meaningful representations for conditioning of generative diffusion models . Additionally, pre-trained capabilities of LMs transfer to downstream tasks even without specific finetuning and beyond the initial modalities . SBERT has demonstrated that pre-trained transformer-based LMs can be used to construct encoders for longer contexts , albeit exclusively for natural language sequences. Consequently, MultiFusion builds its encoder on a pre-trained LM, achieving context-preserving embeddings for multilingual, multimodal inputs.

Other works have used various forms of image conditioning for diffusion models to enable more expressive prompts. Versatile Diffusion  enables the generation of image variations through a unified multimodal framework. Rombach et al.  proposed retriever-augmented diffusion models facilitating conditioning on particular visual styles provided via exemplary images. Multiple works have introduced methods to generate high-quality, text-conditioned images from low-resolution inputs such as sketches . Furthermore, textual inversion  turns concepts from example images into word embeddings that can subsequently be used during generation. This enables incorporating individual styles or objects into generated images. Lastly, Liu et al.  proposed a more general approach for diffusion guidance with image inputs using CLIP. Similarly, Bansal et al.  applied arbitrary guidance functions to more capable diffusion models. Such methods facilitate image generation, for example, from segmentation maps, image variations, or style transfer. However, this type of guidance requires a separate model and more complex, hand-crafted pipelines. In contrast, MultiFusion introduces a unified, general pipeline for effective direct conditioning through classifier-free guidance, removing the need for separate components. Concurrent with our work, GlueGen also attempt the task of aligning additional input modalities to pre-trained text-to-image models . We encourage the reader to refer to their work for more details.

Multilingualism.Existing LMs pre-trained on multilingual data show impressive multilingual capabilities . Popular text-to-image DM's, however, are usually designed for a single input language . AltDiffusion  addressed this issue by proposing a multilingual text encoder (AltCLIP) on which the DM is conditioned instead. AltCLIP embeddings are aligned to the previously used CLIP encoder in a contrastive teacher-student setup using a large multilingual corpus. Subsequently, the cross-attention layers of a pre-trained Stable Diffusion model are finetuned to utilize the AltCLIP encoder instead. The resulting AltDiffusion model can be prompted in 9 different languages. AltDiffusion's image generation is aligned so that the same prompt in different languages results in similar images. With MultiFusion we leverage cross-lingual transfer  to enable multilingual prompting. More precisely, our generative model obtains multilingual capabilities from the encoder despite being solely trained on English data.

## 3 MultiFusion

By fusing pre-trained model components, MultiFusion creates one cohesive system that requires less than 5% of the computational resources needed to train a comparable model from scratch. Our approach involves replacing the encoder of Stable Diffusion (SD) with a more advanced one built on a pre-trained LM. Fusing these components results in a downstream model with the ability to comprehend multilingual, interleaved multimodal inputs. The image generation component inherits this potential despite being trained solely on mono-modal data in a single language. The architecture of MultiFusion is illustrated in Fig. 1. The vast majority of pre-trained weights remain frozen, resulting in an efficient computational process overall. Further information on the training data sizes, parameter counts, and GPU hours for all components is supplied in Tab. 3, along with details on data splits, languages, and modalities in Tab. 4 of the appendix. Subsequently, we outline how to combine and align the involved modules for image generation effectively.

**Input encoders.** The CLIP encoder  used by SD is unsuited for interleaved multimodal inputs as it disregards context and yields disjoint encodings of text and images. Previous work has demonstrated that text encoders based on context-sensitive LMs improve the expressiveness of downstream image generation models . Accordingly, we model the backbone of MultiFusion's encoder as an autoregressive transformer  using rotary position embeddings  trained on a multilingual corpus of various languages (step 1.1 in Fig. 1). We chose an autoregressive decoder model over a bi-directional architecture since decoder models intuitively outperform bi-directional models on relevant tasks. For example, autoregressive models excel at manipulation with natural language ("Subject X with background Y") (cf. Sec. 4.2) or correct features attributions ("Red X, Blue Y") (cf. Sec. 4). Previous research has identified the natural breaking of permutation equivariance as the source of these capabilities , compared to bidirectional models relying entirely on positional embeddings. We acknowledge that bi-directional models may outperform autoregressive ones on other embedding tasks , but argue that an autoregressive model is better suited for the tasks studied in MultiFusion due to the outlined benefits.

Following the methodology proposed by MAGMA , we consider an LM with an added image prefix and dedicated adapters to enable multimodal capabilities (step 1.2 in Fig. 1). Adapters are a suitable architectural choice for multimodal prompts since previous research has already performed extensive ablations on adapter architectures and demonstrated their improved understanding of multimodal inputs over other methods . In this architecture, the image prefix maps the image into sequences of token embeddings in a joint multimodal input embedding space. The adapters are added to each attention and feed forward layer of the transformer and are trained autoregressively on a combination of large-scale image-text datasets (cf. App. A), while the parameters of the language model remain frozen . As a result, the LM enables prompting with arbitrarily interleaved sequences of text and image tokens.

**Semantic embeddings.** In order to use the LM as an encoder for image generation, we extract the model's hidden representations before the language modeling head. While these representations already capture semantic information from the model's pre-training, they need to be optimized and aligned further for usage in arbitrary downstream tasks (step 2.1 in Fig. 1). Initial experiments without additional alignment have shown low rates of convergence (based on visual inspection of generated outputs). Consequently, we chose to produce semantic embeddings guided by the intuition that a focus on the semantics of a text prompt would best capture the information relevant to image generation. Thus simplifying the learning of mapping from embeddings to image outputs, which was confirmed by our initial experimental observations. In conclusion, we deem semantic fine-tuning an essential condition for successfully fusing an image generation model.

We obtained high-quality semantic embeddings through parameter-efficient bias fine-tuning . The tuning follows the supervised contrastive learning objective outlined in S-GPT . The training data consists of natural language tuples of premise and hypothesis, where entailments serve as positive samples and contradictions as negative ones. Importantly, the tuples are bi-lingual such that the premise and hypothesis are each stated in different languages. We observed that including only two languages in this finetuning task is sufficient to achieve multilingual alignment even beyond bilingualism (cf. Sec. 4). Crucially, the semantic bias weights are tuned independently of the multimodal adapters, allowing for modular extensions of the LM with both components.

**Bootstrapping Stable Diffusion.** The final MultiFusion encoder is the result of combining these separately trained modules (cf. Fig. 1). The DM is conditioned on embeddings extracted from the last hidden layer of the transformer. Subsequently, we denote \(H(x)\) as embedding for input \(x\) after a forward pass through MultiFusion's multimodal LM encoder. We now align the pre-trained image generation model with MultiFusion's encoder (step 2.2 in Fig. 1). Considering the depicted SD architecture, we only need to alter the conditioning portion of the generative model to use our new encoder instead of CLIP. In line with previous research , we keep all weights of the DM frozen and only finetune the cross-attention layers of the U-Net. The training objective remains the same as shown in Eq. 1 and 2 with the conditioning \(_{p}\) being the encoding \(H(x)\).

We trained the DM only on monolingual (English) and monomodal inputs with \(x\) being randomly chosen as either an image _or_ a caption. Nonetheless, the final MultiFusion model is capable of interpreting multilingual and arbitrarily interleaved text and image prompts. Our method highlights that the capabilities of strong LMs can be transferred to downstream tasks without the need for dedicated downstream training data. We provide empirical evidence in the following section.

**Modality alignment.** During initial experiments, we observed that further modifications to the inference mechanics are needed to enable stable multimodal prompting. More specifically, for interleaved text and image prompts MultiFusion's encoder represents one image as a sequence of 144 tokens in the input embedding space. In most scenarios, the accompanying text prompt contains fewer tokens, resulting in a disproportional influence of the visual inputs on the generated image. To counteract this phenomenon, we utilize attention manipulation  to up-weight the impact of textual tokens with respect to the discrepancy in input length. Representative results showcasing the effect of attention manipulation can be found in Appendix D.

## 4 Experiments

Next, we present exhaustive evaluations of the multimodal and multilingual capabilities of MultiFusion on various benchmarks. To evaluate compositional robustness on models allowing multi-modal inputs, we introduce the new MCC-250 benchmark. Furthermore, we showcase a variety of applications enabled by MultiFusion. We provide a detailed experimental protocol, including information on implementation and training setup in App. A.

### Empirical evaluation

**Image fidelity & image-text alignment.** We start off with a standard evaluation for image generation models to demonstrate the general functionality of MultiFusion. We investigate image fidelity using FID-30k scores on the MS COCO validation set . We report the results using textual, multimodal, and image prompts and a comparison against SD v1.5 in Tab. 1. The results show that the image fidelity of MultiFusion with textual prompts is competitive with the underlying SD model. Improved FID scores highlight that the capabilities of the original model are preserved in addition to benefiting from new input modalities and languages.

Using multimodal inputs instead of textual prompts results in a substantial improvement of FID scores. This improvement clearly indicates that visual inputs possess a greater capacity to encode comprehensive information about the underlying distributions, surpassing the effectiveness of textual descriptions alone. We acknowledge that using the MS COCO reference images as prompts provides a strong supervision. However, we argue that the above conclusion of image inputs adding additional and more fine-grained information over text prompts alone still holds. Because MultiFusion achieves the improved FID scores by generating meaningful variations of the prompt with more aligned details instead of just trivially reproducing the input image (cf. Sec. 4.2 and App. B). Beyond FID, we evaluated text-image alignment as average CLIPScore  between COCO captions and generated images. Again, MultiFusion achieves scores on par with SD, confirming the preservation of previous capabilities.

    &  &  \\  Guidance Scale & SD v1.5 & MF (Text) & MF (Multimodal) & MF (Image) & SD v1.5 & MF \\ 
8.0 & 14.62 & 14.19 & 11.50 & 8.02 & 0.31 & 0.30 \\
6.0 & 12.73 & 12.15 & 10.29 & 7.18 & 0.31 & 0.29 \\
4.0 & 10.19 & 9.90 & 8.53 & 6.03 & 0.31 & 0.29 \\
2.0 & 9.74 & 12.21 & 8.61 & 6.05 & 0.30 & 0.28 \\
1.0 & 26.09 & 32.81 & 24.22 & 18.93 & 0.27 & 0.25 \\   

Table 1: FID-30k and ClipScores on the MS-COCO validation set for MultiFusion (MF) and Stable Diffusion (SD). SD is always prompted with text. All images were generated with image size 256x256 and 100 diffusion steps. Textual prompts consisted of the COCO image caption, multimodal prompts of the caption and COCO reference image, and image prompts of just the image.

**Compositional robustness.** A challenging task many image synthesis models struggle with is image composition. DMs and, in particular, SD often fail to correctly compose the objects and attributes specified in the text prompt .

MultiFusion, on the other hand, behaves robustly with respect to challenges in compositional generation as shown in Fig. 2. For evaluation, we propose a new benchmark, Multimodal Concept Conjunction 250 (MCC-250)4. It builds on a subset of the Concept Conjunction 500 (CC-500) benchmark . CC-500 contains 500 text prompts of the pattern "a red apple and a yellow banana", textually describing two objects with respective attributes. For half of those prompts, we curated a set of images for each object, enabling multimodal prompting, i.e. the textual description is interleaved with a visual reference. For the new MCC-250 benchmark, we present a human evaluation of SD, Composable Diffusion  and MultiFusion in Tab. 2. Note that all approaches use the same image generation module. For MultiFusion we evaluate multimodal prompts (text and image) as well as text-only.

On this complex task, MultiFusion clearly outperforms both baseline models, almost doubling the portion of images containing both objects with correct colors. Successful compositions once again emphasize the capacity of multimodal prompting in MultiFusion. Importantly, the observed performance improvement originates from the multimodality of inputs, as the success rate on text-only inputs remains comparable. Surprisingly, Composable Diffusion performs slightly worse than SD for image composition. The resulting images frequently display strange blends of the two objects rather than capturing them as distinct entities. We provide further details on the user study in App C.

**Multilingual alignment.** Next, we investigate the multilingual capabilities of MultiFusion. Therefore, we evaluated the alignment of directly translated text inputs in prompt embedding space and the generated images. We compare MultiFusion with AltDiffusion , as it also builds on a frozen SD model. AltDiffusion makes for a great comparison, as the key differences between both

    &  &  &  One obj. \(\) \\ correct color \(\) \\  } &  One obj. \(\) \\ Two obj. \(\) \\  } &  Two obj. \(\) \\ correct colors \(\) \\  } \\  Stable Diffusion [\%] & 0.92\({}_{ 4.81}\) & 99.07\({}_{ 4.89}\) & 90.01\({}_{ 13.97}\) & 44.89\({}_{ 28.61}\) & 29.92\({}_{ 24.76}\) \\ Composable Diffusion [\%] & 3.88\({}_{ 7.49}\) & 96.01\({}_{ 7.72}\) & 88.49\({}_{ 42.83}\) & 34.72\({}_{ 22.79}\) & 25.59\({}_{ 18.94}\) \\ MultiFusion (text) [\%] & 1.08\({}_{ 4.55}\) & 98.91\({}_{ 4.57}\) & 82.36\({}_{ 18.98}\) & 36.03\({}_{ 29.17}\) & 21.66\({}_{ 22.23}\) \\ MultiFusion (multimodal) [\%] & **0.55\({}_{ 2.81}\)** & **99.44\({}_{ 2.85}\)** & **94.88\({}_{ 11.37}\)** & **65.06\({}_{ 30.64}\)** & **58.35\({}_{ 30.94}\)** \\   

Table 2: Fine-grained human evaluation results on MCC-250. SD and Composable Diffusion were prompted with text, whereas we show results for both text and multimodal prompts for MultiFusion. Specifically, multimodal prompts contain one visual reference for each object interleaved with the text input. Recall that each prompt is a complex conjunction of two different objects with different colors. MultiFusion with multimodal inputs outperforms both baseline models.

Figure 3: Multilingual alignment of images generated by MultiFusion. All images were generated using the same seed and with the respective translation of the prompt ‘_an image of an astronaut riding a horse_’. (Best viewed in color)

Figure 2: The multimodality of MultiFusion proves more robust for image compositions. SD is prompted in text with ‘_A photorealistic image of (Descr.)_’. MultiFusion prompts contain interleaved visual references. (Best viewed in color.)models lie in the respective encoders and training data. AltDiffusion uses a large multilingual dataset to finetune the DM, whereas MultiFusion's finetuning requires only English training data.

The evaluation is based on a multilingual version of DrawBench 5. To that end, we translated all prompts into the secondary languages of MultiFusion and AltDiffusion: German and Chinese. Additionally, we include the three languages shared by both models: French, Spanish, and Italian. In Fig. 3(a), we plot the alignment of multilingual text embeddings over DrawBench. MultiFusion's encoder clearly outperforms AltDiffusion's encoder (AltClip) on embedding alignment, scoring 97% cosine similarity on average. The alignment of the generated images is similar for both models, although MultiFusion was only finetuned using English data (cf. Fig. 3(b)). These results highlight that good multilingual embedding alignment enables the transfer of multilingual to downstream tasks without the need for explicitly multilingual training data.

### Applications

After we empirically demonstrated MultiFusion's multimodal, multilingual prompting capabilities, we show how it facilitates diverse interesting use cases. We provide more examples in the Appendix.

**Image composition.** One of the main strengths of MultiFusion's multimodal inputs is image composition. As demonstrated in our empirical evaluation and throughout Figs. 4(a) and 4(b), text and image sequences can be combined arbitrarily and flexibly. Interleaved inputs allow for intuitive combinations of images (cf. Fig. 4(b)) and serve as a visual reference to enrich text prompts.

**Negative prompting.** Negative prompting refers to a technique in image generation that aims to suppress certain concepts. To that end, the unconditioned estimate during classifier-free guidance \((z_{t})\) (cf. Eq. 2) is replaced with an estimate conditioned on a negative prompt \((z_{t},c_{n})\). Guiding away from \((z_{t},c_{n})\) results in the concepts described by \(n\) being avoided. Consequently, negative prompting works better with more descriptive prompts \(n\). Our previous results demonstrated (cf. Tab. 1) a higher expressiveness of images over text that also translates to negative prompts. Fig 4(c) shows that textual negative prompts are less effective in removing undesired concepts. However, using image prompts, these concepts can be completely suppressed.

**Image variation.** MultiFusion offers a direct interface for generating image variants. Simply providing a single image as input prompt to the default pipeline produces meaningful variations already. Other models, in contrast, rely on inversion or re-noising of the input image. We depict

Figure 4: Comparison of multilingual alignment over DrawBench prompts. MultiFusion achieves comparable alignment of the output images although the image generation module was only trained on English data. This can be attributed to the strong alignment of multilingual prompts in MultiFusion’s embedding space. Similarities are calculated based on paired comparisons between one language and all others. We do not report German results for AltClip/AltDiff nor Chinese for MultiFusion as these languages are not in the respective training data.

examples in Fig. 5(a). The generated images include sensible deviations from the input but faithfully reproduce the underlying image contents.

**Style modification.** Furthermore, MultiFusion can easily generate images adhering to any artistic style. The style of images is notoriously hard to describe in image generation prompts and has even led to the development of monetized prompt databases for long, obfuscated prompts. This is mostly due to the magnitude of factors that have to be accounted for, such as the color palette, composition, contrast, etc. Fortunately, all of these aspects can easily be expressed through exemplary (reference) images. Fig. 5(b) depicts examples where arbitrary styles are applied to various scene descriptions. MultiFusion delivers high-quality outputs that show the prompted scene in the desired style.

**Multilingualism.** Lastly, the strong multilingual embedding alignment makes MultiFusion largely invariant to the language of input prompts. Our model fully supports input prompts in five languages: English, German, French, Spanish, and Italian. In line with our results on language alignment, Fig. 3 shows that the same prompt in different languages yields largely similar images. This improves accessibility and expands the group of users, regardless of their native language. Moreover, Fig. 4(a) demonstrates that languages can even differ within the same input. This further emphasizes MultiFusion's expressiveness, allowing users to construct prompts in various ways.

## 5 Discussion

The capabilities of MultiFusion outlined above emphasize the advantages of the model, which we subsequently discuss alongside the remaining limitations and overall societal impact.

### Advantages of MultiFusion

Multimodality and multilingualism of inputs offer various benefits compared to prevalent models. The extension of the interface beyond the usual English-only text-to-image applications makes MultiFusion more flexible, expressive, and versatile.

**Expressiveness.** Natural language and images both have benefits and limitations in the concepts they can convey. When used in combination, however, we are able to draw from the advantages of one and the other, thus eliminating the restrictions of either modality. On the one hand, complex objects that should be included can easily be prompted via exemplary images instead of using long, convoluted, and error-prone textual descriptions. Natural language is often unclear and ambiguous and may lack words to describe certain concepts concisely. For the car depicted in Fig. 4(a), for example, MultiFusion faithfully infers the vehicle's color, make, type, and era and uses that as a reference for image generation. Achieving similar results through textual descriptions alone requires complicated prompt engineering and may be unsuccessful altogether. Additionally, some necessary

Figure 5: Applications of MultiFusion highlighting the versatility and expressiveness of multilingual, multimodal prompts. (Best viewed in color)

terminology may elude the end user. For example, a non-native speaker may not be aware that the type of building in Fig. 4(a) can be described as _half-timbered_. Users may consequently write sub-optimal textual descriptions which can be circumvented using visual prompts. Furthermore, images convey world knowledge that the model does not possess. For example, not all existing artworks or paintings are included in the training data and retained by the model (cf. Fig. 5(b)). Consequently, using their stylistic characteristics as reference is difficult through text alone. MultiFusion easily bridges this knowledge gap by facilitating the inclusion of these images in the prompt. This even enables using individual experiences for reference, such as my car, a sketch I drew, or a building I saw. On the other hand, natural language provides an intuitive interface for abstract concepts and scenarios. For example, the drawing of the car in Fig. 4(a) is easily turned into a photorealistic version through textual instruction. Consequently, the capabilities of MultiFusion result from combining the expressiveness of images and the abstraction of natural language. The inclusion of multiple languages only further increases the model's expressiveness.

**Robustness and clarity.** Similarly, the availability of multimodal inputs significantly improves the robustness of MultiFusion. The combination of visual and natural language inputs allows users to circumvent the limitations of the modalities themselves and those of current DMs. For example, natural languages contain a variety of homographs, i.e., words with the same spelling but different meanings. Inferring the correct one from context is often impossible, whereas images provide clear examples. On the other hand, images alone may contain various sources of noise, such as background objects or stylistic choices that the user might not want to consider for generation. However, when used in combination, one modality is able to make up for the shortcomings of the other.

Furthermore, MultiFusion overcomes the limitations of current DMs in compositional generation . As demonstrated on the MCC-250 benchmark, images generated from multimodal prompts with MultiFusion are less likely to miss objects specified in the prompt or bind attributes incorrectly. This is due to MultiFusion devoting multiple tokens to an object described via an image and using context-aware embeddings. In contrast, text-only generation using models such as Stable Diffusion often leads to regular attribute leakage or interchanged attributes, cf. Fig. 2. In this case, the green color of the apple leaks to the car, or colors are mixed up between objects. We include further examples in App. C showcasing the resilience of MultiFusion.

**Licensing.** Recently, copyright issues and licensing of training images have been heavily discussed topics6. Current legal actions7 may lead to a variety of images--especially creations from artists--being no longer admissible for use in training. Consequently, applications such as instructing the

Figure 6: Further applications of MultiFusion. (Best viewed in color)

model to reference an artist's style will be infeasible even though the end user might have the necessary copyright permissions. With MultiFusion, however, a user can still provide reference material not included in the training data simply in the input.

### Limitations

While MultiFusion achieves impressive results on various applications, there are some limits to the model's expressiveness. For one, MultiFusion always produces meaningful variations when prompted with a single input image. While this is an advantage in some settings, it may be a limitation in others since exactly copying objects from an input is not feasible. This originates from the encoder not being designed to reconstruct images from its representations but to encode relevant characteristics efficiently. Nonetheless, this behavior is generally intended as images are supposed to serve as references to MultiFusion instead of performing image editing. Additionally, the quality and composition of an input image have a significant effect on the generated image. We observed that some visual inputs must be carefully selected to achieve the desired outcome. For example, MultiFusion may also include unwanted items from the background of an image in the generated output. Furthermore, in some cases, MultiFusion has a tendency to copy the input image style even when the prompt's textual portion indicates a different one. Nonetheless, this can be adequately addressed with the proposed attention manipulation method , as demonstrated throughout the examples in this work.

### Societal impact

Recent developments in text-to-image models [35; 32; 39; 2] have the potential for a far-reaching impact on society, both positive and negative when deployed in applications such as image generation, image editing, or search engines. Previous research [41; 18] described many potential negative societal implications that may arise due to the careless use of such large-scale generative models. Many of these problems can be attributed to the noisy, large-scale datasets these models rely on. Since recent text-to-image models, such as SD, are trained on web-crawed data containing inappropriate content [42; 6; 5], they are no exception to this issue. Specifically, models relying on the LAION datasets  show signs of inappropriate degeneration . Consequently, we assume MultiFusion to suffer from similar shortcomings only reinforced by its additional capabilities. Therefore, we will not make the model weights publicly available in their current form.

## 6 Conclusion

In this work, we introduced MultiFusion, a diffusion model utilizing multilingual, arbitrarily interleaved multimodal inputs. These capabilities provide a versatile interface to users in order to better express themselves. We demonstrated that multilingual alignment in the encoder is sufficient to achieve multilingual in downstream tasks, eliminating the need for dedicated multilingual datasets. Thus, significantly reducing computational requirements. More generally, MultiFusion highlights how separate pre-trained components can be interconnected to realize complex models.

Numerous promising avenues for future research emerge from our current work. One intriguing direction involves expanding the scope of MultiFusion to facilitate interactive image generation through integration with a chat interface. This novel extension would offer an enhanced user experience, introducing a dynamic interplay between the system and users. Additionally, we envision the potential for recursive prompting of generated images, enabling a progressive refinement of output through incremental instructions. Moreover, the incorporation of an additional decoder, based on the MultiFusion encoder, holds promise for facilitating textual generation resulting in multimodal outputs. Lastly, we propose extending the encoder itself to encompass an even broader range of modalities, including audio, video, time series, and various others.